Reinforcement Learning in Low-Rank MDPs
with Density Features
Audrey Huang*â€ 
Jinglin Chen*â€ 
Nan Jiangâ€ 
Abstract
MDPs with low-rank transitionsâ€”that is, the transition matrix can be factored into the product of
two matrices, left and rightâ€”is a highly representative structure that enables tractable learning. The
left matrix enables expressive function approximation for value-based learning and has been studied
extensively. In this work, we instead investigate sample-efï¬cient learning with density features, i.e.,
the right matrix, which induce powerful models for state-occupancy distributions. This setting not only
sheds light on leveraging unsupervised learning in RL, but also enables plug-in solutions for convex
RL. In the ofï¬‚ine setting, we propose an algorithm for off-policy estimation of occupancies that can
handle non-exploratory data. Using this as a subroutine, we further devise an online algorithm that
constructs exploratory data distributions in a level-by-level manner. As a central technical challenge,
the additive error of occupancy estimation is incompatible with the multiplicative deï¬nition of data
coverage. In the absence of strong assumptions like reachability, this incompatibility easily leads to
exponential error blow-up, which we overcome via novel technical tools. Our results also readily extend
to the representation learning setting, when the density features are unknown and must be learned from
an exponentially large candidate set.
1
Introduction
The theory of reinforcement learning (RL) in large state spaces has seen fast development. In the model-free
regime, how to use powerful function approximation to learn value functions has been extensively studied
in both the online and the ofï¬‚ine settings (Jiang et al., 2017; Jin et al., 2020b,c; Xie et al., 2021), which also
builds the theoretical foundations that connect RL with (discriminative) supervised learning. On the other
hand, generative models for unsupervised/self-supervised learningâ€”which deï¬ne a sampling distribution
explicitly or implicitlyâ€”are becoming increasingly powerful (Devlin et al., 2018; Goodfellow et al., 2020),
yet how to leverage them to address the key challenges in RL remains under-investigated. While prior works
on RL with unsupervised-learning oracles exist (Du et al., 2019; Feng et al., 2020), they often consider
models such as block MDPs, which are more restrictive than typical model structures considered in the
value-based setting such as low-rank MDPs.
In this paper, we study model-free RL in low-rank MDPs with density features for state occupancy
estimation. In a low-rank MDP, the transition matrix can be factored into the product of two matrices, and
the left matrix is known to serve as powerful features for value-based learning (Jin et al., 2020b), as it can be
used to approximate the Bellman backup of any function. On the other hand, the right matrix can be used to
represent the policiesâ€™ state-occupancy distributions, yet how to leverage such density features (without the
knowledge of the left matrix) in ofï¬‚ine or online RL is unknown. To this end, our main research question is:
*The two authors contributed equally to this work.
â€ Department of Computer Science, University of Illinois Urbana-Champaign.
Email:
audreyh5@illinois.edu,
jinglinc@illinois.edu, nanjiang@illinois.edu.
1
arXiv:2302.02252v1  [cs.LG]  4 Feb 2023

Is sample-efï¬cient ofï¬‚ine/online RL with density features possible in low-rank MDPs?
We answer this question in the positive, and below is a summary of our contributions:
1. Ofï¬‚ine:
Section 3 provides an algorithm for off-policy occupancy estimation. It bears similarity to
existing algorithms for estimating importance weights (Hallak and Mannor, 2017; Gelada and Bellemare,
2019), but our setting gives rise to a number of novel challenges. Most importantly, our algorithm enjoys
guarantees under arbitrary ofï¬‚ine data distributions, when the standard notion of importance weights
are not even well-deï¬ned. We introduce a novel notion of recursively clipped occupancy and show
that it can be learned in a sample-efï¬cient manner. The recursively clipped occupancy always lower
bounds the true occupancy, and the two notions coincide when the data is exploratory. Such a guarantee
immediately enables an ofï¬‚ine policy learning result that only requires â€œsingle-policy concentrabilityâ€,
which is comparable to the most recent advances in value-based ofï¬‚ine RL (Jin et al., 2020c; Xie et al.,
2021).
2. Online: Using the ofï¬‚ine algorithm as a subroutine, in Section 4, we design an online algorithm that
builds an exploratory data distribution (or â€œpolicy coverâ€ (Du et al., 2019)) from scratch in a level-by-
level manner. At each level, we estimate each policyâ€™s state-occupancy distribution and construct an
approximate cover by choosing the barycentric spanner of such distributions. A critical challenge here
is that the additive â„“1 error in occupancy estimation destroys the multiplicative coverage guarantee of the
barycentric spanner, so the constructed distribution is never perfectly exploratory. Worse still, standard
algorithm designs and analyses for handling such a mismatch easily lead to an exponential error blow-up.
We overcome this by a novel technique, where two inductive error terms are maintained and analyzed in
parallel, with delicate interdependence that still allows for a polynomial error accumulation (Figure 1).
3. Representation learning: We also extend our ofï¬‚ine and online results to the representation learning
setting (Agarwal et al., 2020), where the true density features are not given but must also be learned from
an exponentially large candidate feature set.
4. Implications: Our online algorithm is automatically reward-free (Jin et al., 2020a; Chen et al., 2022b)
and deployment-efï¬cient (Huang et al., 2022). Further, since we can accurately estimate the occupancy
distribution for all candidate policies, our results enable plug-in solutions for settings such as convex RL
(Mutti et al., 2022; Zahavy et al., 2021), where the objectives and/or constraints are functions over the
entire state distributions (see Appendix C).
2
Preliminaries
Markov Decision Processes (MDPs)
We consider a ï¬nite-horizon episodic MDP (without reward) de-
ï¬ned as M = (X, A, P, H), where X is the state space, A is the action space, P = (P0, . . . , PHâˆ’1) with
Ph : X Ã— A â†’âˆ†(X) is the transition dynamics, H is the horizon, and d0 âˆˆâˆ†(X) is the known ini-
tial state distribution.1 We assume that X is a measurable space with possibly inï¬nite number of elements
and A is ï¬nite with cardinality K. Each episode is a trajectory Ï„ = (x0, a0, x1, . . . , xHâˆ’1, aHâˆ’1, xH),
where x0 âˆ¼d0, the agent takes a sequence of actions a0, . . . , aHâˆ’1, and xh+1 âˆ¼Ph(Â· | xh, ah). We use
Ï€ = (Ï€0, . . . , Ï€Hâˆ’1) âˆˆ(X â†’âˆ†(A))H to denote a (non-stationary) H-step Markov policy, which chooses
ah âˆ¼Ï€h(Â·|xh). (We will also omit the subscript h and write Ï€(Â·|xh) when it is clear from context.) We use
Ï to refer to non-Markov policies that can choose ah based on the history x0:h, a0:hâˆ’1, which often arises
from the probability mixture of Markov policies at the beginning of an trajectory. Once a policy Ï€ is ï¬xed,
the MDP becomes an Markov chain, with dÏ€
h(xh) being its h-th step distribution. As a shorthand, we use
the notation [H] to denote {0, 1, . . . , H âˆ’1}.
1We assume the known initial state distribution for simplicity. Our results easily extend to the unknown version.
2

Low-rank MDPs
We consider learning in a low-rank MDP, deï¬ned as:
Assumption 1 (Low-rank MDP). M is a low-rank MDP with dimension d, that is, âˆ€h âˆˆ[H], there ex-
ist Ï†âˆ—
h : X Ã— A â†’Rd and Âµâˆ—
h : X â†’Rd such that âˆ€xh,xh+1 âˆˆX, ah âˆˆA : Ph(xh+1|xh, ah) =
âŸ¨Ï†âˆ—
h(xh, ah), Âµâˆ—
h(xh+1)âŸ©. Further,
R
âˆ¥Âµâˆ—
h(x)âˆ¥1(dx) â‰¤BÂµ and âˆ¥Ï†âˆ—
h(Â·)âˆ¥âˆâ‰¤1.2
Notation
We use the convention 0
0 = 0 when we deï¬ne the ratio between two functions. Deï¬ne a âˆ§
b = min(a, b), and we treat âˆ§as an operator with precedence between â€œÃ—/â€ and â€œ+âˆ’â€. When clear
from the context, {â–¡h} = {â–¡h}Hâˆ’1
h=0 , and we refer to state â€œoccupancies,â€ â€œdistributions,â€ and â€œdensitiesâ€
interchangeably. Finally, letter â€œdâ€ has a few different versions (with different fonts): d is the low-rank
dimension, d(x) is a density, and (dx) is the differential used in integration. Further, while dÏ€
h and dD
h refer
to true densities, dh (without superscripts) is often used for optimization variables.
Learning setups
We provide algorithms and guarantees under a number of different setups (e.g., ofï¬‚ine
vs. online). The result that connects all pieces together is the setting of online reward-free exploration with
known density features Âµâˆ—= (Âµâˆ—
0, . . . , Âµâˆ—
Hâˆ’1) and a policy class Î  âŠ†(X â†’âˆ†(A))H (Section 4). Here,
the learner must explore the MDP and form accurate estimations of dÏ€
h for all Ï€ âˆˆÎ  and h âˆˆ[H], that is,
output {bdÏ€
h}hâˆˆ[H],Ï€âˆˆÎ  such that with probability at least 1 âˆ’Î´, âˆ€Ï€ âˆˆÎ , h âˆˆ[H], âˆ¥bdÏ€
h âˆ’dÏ€
hâˆ¥1 â‰¤Îµ, by only
collecting poly(H, K, d, log(|Î |), 1/Îµ, log(1/Î´)) trajectories. Two remarks are in order:
1. Such a guarantee immediately leads to standard guarantees for return maximization when a reward func-
tion is speciï¬ed. More concretely (with proof in Appendix F.2),
Proposition 1. Given any policy Ï€ and reward function3 R = {Rh} with Rh : X Ã— A â†’[0, 1], deï¬ne
expected return as vÏ€
R := EÏ€[PHâˆ’1
h=0 Rh(xh, ah)] = PHâˆ’1
h=0
RR
dÏ€
h(xh)Rh(xh, ah)Ï€(ah|xh)(dxh)(dah).
Then for {bdÏ€
h} such that âˆ¥bdÏ€
h âˆ’dÏ€
hâˆ¥1 â‰¤Îµ/(2H) for all Ï€ âˆˆÎ  and h âˆˆ[H], we have vbÏ€R
R â‰¥maxÏ€âˆˆÎ  vÏ€
Râˆ’
Îµ, where bÏ€R = argmaxÏ€âˆˆÎ  bvÏ€
R, and bvÏ€
R is the expected return calculated using {bdÏ€
h}.
Moreover, the result can be extended to more general settings, where the optimization objective is some
function of the state (and action) distribution that cannot be written as cumulative expected rewards; e.g.,
entropy as in max-entropy exploration (Hazan et al., 2019), or âˆ¥dÏ€
h âˆ’dÏ€E
h âˆ¥2
2, where Ï€E is an expert policy,
used in imitation learning (Abbeel and Ng, 2004). A detailed discussion is deferred to Appendix C.
2. The introduction of Î  and the dependence on K = |A| are both necessary, since low-rank MDPs can
emulate general contextual bandits where the density features Âµâˆ—become useless; see Appendix B for
more details.
To enable such a result, a key component is to estimate dÏ€
h using ofï¬‚ine data (Section 3). Later in
Section 5, we also generalize our results to the representation-learning setting (Agarwal et al., 2020; Modi
et al., 2021; Uehara et al., 2021b), where Âµâˆ—is not known but must be learned from an exponentially large
candidate set.
2This is w.l.o.g. as the norm of Ï†âˆ—
h can be absorbed into BÂµ. In a natural special case of low-rank MDPs with â€œsimplex featuresâ€
(Jin et al., 2020b, Example 2.2), Assumption 1 holds with BÂµ = d. Our sample complexities only have polylogarithmic dependence
on BÂµ which will be suppressed by eO.
3We assume known and deterministic rewards, and can easily handle unknown/stochastic versions (Appendix D.2).
3

3
Off-policy occupancy estimation
In this section, we describe our algorithm, FORC, which estimates the occupancy distribution dÏ€
h of any
given policy Ï€ using an ofï¬‚ine dataset. Note that this section serves both as an important building block for
the online algorithm in Section 4 and a standalone ofï¬‚ine-learning result in its own right, so we will make
remarks from both perspectives.
We start by introducing our assumption on the ofï¬‚ine data.
Assumption 2 (Ofï¬‚ine data). Consider a dataset D0:Hâˆ’1 = D0
S . . . S DHâˆ’1, where Dh = {(x(i)
h , a(i)
h ,
x(i)
h+1)}n
i=1.
For any ï¬xed h, we assume that tuples in Dh are sampled i.i.d. from Ïhâˆ’1 â—¦Ï€D
h , where
a0, . . . , ahâˆ’1 âˆ¼Ïhâˆ’1 is an arbitrary (h âˆ’1)-step (possibly non-Markov) policy4 and ah âˆ¼Ï€D
h is a single-
step Markov policy. Further, Ïhâˆ’1, Ï€D
h can be a function of D0:hâˆ’1, and Ï€D
h is known to the learner.
The dataset consists of H parts, where the h-th part consists of (xh, ah, xh+1) tuples, allowing us to
reason about the transition dynamics at level h. In practice (as well as in Section 4), such tuples will be
extracted from trajectory data. We use dD
h (xh, ah, xh+1), dD
h (xh), dD,â€ 
h
(xh+1) to denote the joint and the
marginal distributions, respectively. Importantly, we do not assume that dD,â€ 
h
(xh+1) = dD
h+1(xh+1), i.e.,
the next-state distribution of Dh and the current-state distribution of Dh+1 (which are both over X) may
not be the same, as we will need this ï¬‚exibility in Section 4. The H parts can also sequentially depend on
each other, though samples within each part are i.i.d. While this setup is sufï¬cient for Section 4 and already
weaker than the fully i.i.d. setting commonly adopted in the ofï¬‚ine RL literature (Chen and Jiang, 2019;
Yin and Wang, 2021), in Appendix D we discuss how to relax it to handle more general situations in ofï¬‚ine
learning.
3.1
Occupancy estimation via importance weights
Recall that value functions satisfy the familiar Bellman equations, allowing us to learn them by approxi-
mating Bellman operators via squared-loss regression. The occupancy distributions {dÏ€
h} also satisfy the
Bellman ï¬‚ow equation: let PÏ€
h denote the Bellman ï¬‚ow operator, where for any given dh : X â†’R and
policy Ï€, (PÏ€
hdh)(xh+1) :=
RR
Ph(xh+1|xh, ah)Ï€(ah|xh)dh(xh)(dxh)(dah).5 dÏ€
h can be then recursively
deï¬ned via the Bellman ï¬‚ow equation dÏ€
h = PÏ€
hâˆ’1dÏ€
hâˆ’1, with the base case dÏ€
0 = d0. (One difference is
that value functions are deï¬ned bottom-up, whereas occupancies are deï¬ned top-down.) Furthermore, in a
low-rank MDP, PÏ€
hdh is always linear in Âµâˆ—
h (Lemma 16), just like the image of Bellman operators for value
is always in the linear span of Ï†âˆ—
h.
Given the similarity, one might think that we can also approximate PÏ€
hâˆ’1 by regressing directly onto the
occupancies, hoping to obtain dÏ€
h via
argmin
dh
EdD
hâˆ’1
ï£®
ï£°
 
dh(xh) âˆ’dÏ€
hâˆ’1(xhâˆ’1)Ï€hâˆ’1(ahâˆ’1|xhâˆ’1)
Ï€D
hâˆ’1(ahâˆ’1|xhâˆ’1)
!2ï£¹
ï£»,
(1)
where Ï€hâˆ’1(ahâˆ’1|xhâˆ’1)
Ï€D
hâˆ’1(ahâˆ’1|xhâˆ’1) is the standard importance weighting to correct the mismatch on actions between
Ï€hâˆ’1 and data policy Ï€D
hâˆ’1. Unfortunately, this does not work due to the â€œtime-reversedâ€ nature of ï¬‚ow
4h on the superscript of a policy distinguishes identities and does not refer to the h-th step component (which is indicated by
the subscript), that is, Ïh and Ïhâ€² for hâ€² Ì¸= h can be completely unrelated policies.
5In this deï¬nition, we do not require dh to be a valid distribution. Even Ï€ is allowed to be unnormalized; see the deï¬nition of
pseudo-policy in Deï¬nition 1.
4

operators (Liu et al., 2018). In fact, the Bayes-optimal solution of Eq. (1) is
dh(xh) = (PÏ€
hâˆ’1(dD
hâˆ’1dÏ€
hâˆ’1))(xh)
dD,â€ 
hâˆ’1(xh)
Ì¸= (PÏ€
hâˆ’1dÏ€
hâˆ’1)(xh).
However, the fractional form of the solution indicates that we may instead aim to learn a related functionâ€”
the importance weight, or density ratio (Hallak and Mannor, 2017). If we use wÏ€
hâˆ’1 = dÏ€
hâˆ’1/dD
hâˆ’1 to replace
dÏ€
hâˆ’1 as the regression target in Eq. (1), the population solution would be
(PÏ€
hâˆ’1dÏ€
hâˆ’1)(xh)
dD,â€ 
hâˆ’1(xh)
=
dÏ€
h(xh)
dD,â€ 
hâˆ’1(xh)
=: wÏ€
h(xh).
The occupancy can then be straightforwardly extracted from the weight via elementwise multiplication, i.e.,
dÏ€
h = wÏ€
h Â· dD,â€ 
hâˆ’1, where dD,â€ 
hâˆ’1 can be estimated via MLE from the dataset itself.
While this is promising, the approach uses importance weight wÏ€
h(xh) as an intermediate variable, whose
very existence and boundedness rely on the assumption that the data distribution dD,â€ 
hâˆ’1 is exploratory and
provides sufï¬cient coverage over dÏ€
h. We next consider the scenario where such an assumption does not
hold. Perhaps surprisingly, although we would like to construct exploratory datasets in Section 4 and feed
them into the ofï¬‚ine algorithm, being able to handle non-exploratory data turns out to be crucial to the online
setting, and also yields novel ofï¬‚ine guarantees of independent interest.
3.2
Handling insufï¬cient data coverage
Because we make no assumptions about data coverage, the true occupancy dÏ€
h may be completely unsup-
ported by data, in which case there is no hope to estimate it well. What kind of learning guarantees can we
still obtain?
To answer this question, we introduce one of our main conceptual contributions, a novel learning target
for occupancy estimation under arbitrary data distributions.
Deï¬nition 1 (Pseudo-policy and recursively clipped occupancy). Given a Markov policy Ï€, data distri-
butions {dD
h }, and state and action clipping thresholds {Cx
h}, {Ca
h}, the recursively clipped occupancy,
{d
Ï€
h}, is deï¬ned as follows. Let d
Ï€
0 := dÏ€
0 = d0. Deï¬ne Ï€h(ah|xh) := Ï€h(ah|xh) âˆ§Ca
hÏ€D
h (ah|xh) (or
Ï€h = Ï€h âˆ§Ca
hÏ€D
h for short), and for 1 â‰¤h â‰¤H âˆ’1, inductively set 6
d
Ï€
h(xh) :=

PÏ€
hâˆ’1

d
Ï€
hâˆ’1 âˆ§Cx
hâˆ’1dD
hâˆ’1

(xh).
(4)
We also call objects like Ï€ a pseudo-policy, which can yield unnormalized distributions over actions.
The above deï¬nition ï¬rst clips the previous-level d
Ï€
hâˆ’1 to have at most Cx
hâˆ’1 ratio over the data distri-
bution dD
hâˆ’1 and the policy Ï€ to have at most Ca
hâˆ’1 ratio over Ï€D
hâˆ’1, then applies the Bellman ï¬‚ow operator.
This guarantees that d
Ï€
h is always supported on the data distribution (unlike dÏ€
h), and d
Ï€
h â‰¤dÏ€
h because
poorly-supported mass is removed from every level (and hence d
Ï€
h is generally an unnormalized distribu-
tion). Further, when we do have data coverage and the original importance weights on states and actions are
always bounded by {Cx
h} and {Ca
h}, it is easy to see that d
Ï€
h = dÏ€
h, since the clipping operations will have
no effects and Deï¬nition 1 simply coincides with the Bellman ï¬‚ow equation for {dÏ€
h}.
6Note that d
Ï€
h depends on hyperparameters Cx
h and Ca
h, which is omitted in the notation. Appendix E.1 discusses the relationship
between Cx
h, Cx
a and the missingness error, namely, that âˆ¥dÏ€
h âˆ’d
Ï€
hâˆ¥1 is Lipschitz in, and thus insensitive to misspeciï¬cations of,
the clipping thresholds.
5

Algorithm 1 Fitted Occupancy Iteration with Clipping (FORC)
Input: policy Ï€, density feature Âµâˆ—, dataset D0:Hâˆ’1, sample sizes nmle and nreg, clipping thresholds {Cx
h}
and {Ca
h}.
1: Initialize bdÏ€
0 = d0.
2: for h = 1, . . . , H do
3:
Randomly split Dhâˆ’1 to two folds Dmle
hâˆ’1 and Dreg
hâˆ’1 with sizes nmle and nreg, respectively.
4:
Estimate marginal data distributions bdD
hâˆ’1(xhâˆ’1) and bd D,â€ 
hâˆ’1(xh) by MLE on dataset Dmle
hâˆ’1:
bdD
hâˆ’1 = argmax
dhâˆ’1âˆˆFhâˆ’1
1
nmle
nmle
X
i=1
log

dhâˆ’1(x(i)
hâˆ’1)

and bd D,â€ 
hâˆ’1 = argmax
dhâˆˆFh
1
nmle
nmle
X
i=1
log

dh(x(i)
h )

,
(2)
where Fh =

dh = âŸ¨Âµâˆ—
hâˆ’1, Î¸hâŸ©: dh âˆˆâˆ†(X), Î¸h âˆˆRd, âˆ¥Î¸hâˆ¥âˆâ‰¤1
	
.
# âˆ¥Î¸hâˆ¥âˆâ‰¤1 guarantees
dD
h âˆˆFh
5:
Deï¬ne LDreg
hâˆ’1(wh, whâˆ’1, Ï€hâˆ’1) :=
1
nreg
Pnreg
i=1

wh(x(i)
h ) âˆ’whâˆ’1(x(i)
hâˆ’1)
Ï€hâˆ’1(a(i)
hâˆ’1|x(i)
hâˆ’1)
Ï€D
hâˆ’1(a(i)
hâˆ’1|x(i)
hâˆ’1)
2
, and es-
timate
bwÏ€
h = argmin
whâˆˆWh
LDreg
hâˆ’1

wh,
bdÏ€
hâˆ’1âˆ§Cx
hâˆ’1 bdD
hâˆ’1
bdD
hâˆ’1
, Ï€hâˆ’1 âˆ§Ca
hâˆ’1Ï€D
hâˆ’1

,
(3)
where Wh =

wh =
âŸ¨Âµâˆ—
hâˆ’1,Î¸up
h âŸ©
âŸ¨Âµâˆ—
hâˆ’1,Î¸down
h
âŸ©: âˆ¥whâˆ¥âˆâ‰¤Cx
hâˆ’1Ca
hâˆ’1, Î¸up
h , Î¸down
h
âˆˆRd

.
6:
Set the estimate bdÏ€
h = bwÏ€
h bd D,â€ 
hâˆ’1.
7: end for
Output: estimated state occupancies {bdÏ€
h}hâˆˆ[H].
As we will see below in Section 3.3, {d
Ï€
h} becomes a learnable target and the â„“1 estimation error of our
algorithm goes to 0 when the sample size n â†’âˆ. The thresholds {Cx
h} and {Ca
h} reï¬‚ect a bias-variance
trade-off: higher thresholds ensure that less â€œmassâ€ is clipped away (i.e., d
Ï€
h will be closer to dÏ€
h), but result
in a worse sample complexity as the algorithm will need to deal with larger importance weights. Below we
provide more ï¬ne-grained characterization on the bias part, i.e., how d
Ï€
h is related to dÏ€
h, and the proof is
deferred to Appendix E.2.
Proposition 2 (Properties of d
Ï€
h).
1. d
Ï€
h â‰¤dÏ€
h.
2. d
Ï€
h = dÏ€
h when data covers Ï€ (i.e., âˆ€hâ€² < h we have dÏ€
hâ€² â‰¤Cx
hâ€²dD
hâ€² and Ï€hâ€² â‰¤Ca
hâ€²Ï€D
hâ€²).
3. âˆ¥d
Ï€
h âˆ’dÏ€
hâˆ¥1 â‰¤âˆ¥d
Ï€
hâˆ’1 âˆ’dÏ€
hâˆ’1âˆ¥1 + âˆ¥d
Ï€
hâˆ’1 âˆ’d
Ï€
hâˆ’1 âˆ§Cx
hâˆ’1dD
hâˆ’1âˆ¥1 + âˆ¥PÏ€
hâˆ’1dÏ€
hâˆ’1 âˆ’PÏ€
hâˆ’1dÏ€
hâˆ’1âˆ¥1.
The 3rd claim shows how the bias term âˆ¥d
Ï€
h âˆ’dÏ€
hâˆ¥1 (i.e., how much mass d
Ï€
h is missing from dÏ€
h)
accumulates over the horizon: the RHS of the bound consists of 3 terms, where the ï¬rst is missing mass
from the previous level, and the other terms correspond to the mass being clipped away from states and
actions, respectively, at the current level.
6

3.3
Algorithm and analyses
We are now ready to introduce our algorithm, FORC, with its analyses and guarantees. See pseudocode
in Algorithm 1. The overall structure of the algorithm largely follows the sketch in Section 3.1: we use
squared-loss regression to iteratively learn the importance weights (line 5), and convert them to densities by
multiplying with the data distributions (line 6) estimated via MLE (line 4).
The major difference is that we introduce clipping in line 5 (in the same way as Deï¬nition 1) to guarantee
that the regression target is always well-behaved and bounded, and below we show that this makes bdÏ€
h a good
estimation of d
Ï€
h. In particular, we will bound the regression error âˆ¥bdÏ€
h âˆ’d
Ï€
hâˆ¥1 as a function of sample size
nreg. A key lemma that enables such a guarantee is the following error propagation result:
Lemma 1. For every h âˆˆ[H], the error between estimates bdÏ€
h from Algorithm 1 and the clipped target d
Ï€
h
is decomposed recursively as
bdÏ€
h âˆ’d
Ï€
h

1 â‰¤
bdÏ€
hâˆ’1 âˆ’d
Ï€
hâˆ’1

1
+ 2Cx
hâˆ’1
bdD
hâˆ’1 âˆ’dD
hâˆ’1

1+ Cx
hâˆ’1Ca
hâˆ’1
bd D,â€ 
hâˆ’1 âˆ’dD,â€ 
hâˆ’1

1
+
âˆš
2
 bwÏ€
h âˆ’EÏ€
hâˆ’1

dD
hâˆ’1
bdÏ€
hâˆ’1âˆ§Cx
hâˆ’1 bdD
hâˆ’1
bdD
hâˆ’1

2,dD,â€ 
hâˆ’1
,
where (EÏ€
hdh) := (PÏ€
hdh)/dD,â€ 
h
.
The proof can be found in Appendix E.2. The bound consists of 3 parts: the ï¬rst line is the error at
the previous level h âˆ’1, showing that the regression error accumulatives linearly over the horizon. The
second line captures errors due to imperfect estimation of the data distributions, since we use the estimated
bdD
hâˆ’1 and bd D,â€ 
hâˆ’1, instead of the groundtruth distributions, to set up the weight regression problem and extract
the density; these errors can be reduced by simply using larger nmle. The last line represents the ï¬nite-
sample error in regression, which is the difference between the estimated weight bwÏ€
h and the Bayes-optimal
predictor. We set the constraints in the hypothesis class in a way to guarantee the Bayes-optimal predictor
is in the class (see the deï¬nition of Wh below Eq. (3)), so the regression is realizable.
Bounding the complexities of Fh and Wh
The last challenge is in controlling the statistical complexities
of the function classes used in learning, Fh and Wh, both of which are inï¬nite classes. For Fh, we construct
an optimistic covering to bound its covering number (Chen et al., 2022a). For Wh, however, its hypothesis
takes the form of ratio between linear functions,
âŸ¨Âµâˆ—
hâˆ’1,Î¸up
h âŸ©
âŸ¨Âµâˆ—
hâˆ’1,Î¸down
h
âŸ©, where standard covering arguments, which
discretize Î¸up
h and Î¸down
h
, run into sensitivity issues, as Î¸down
h
is on the denominator where small perturba-
tions can lead to large changes in the ratio. We overcome this by recalling a technique from Bartlett and
Tewari (2006): we bound the pseudo-dimension of Wh, which is equal to the VC-dimension of the corre-
sponding thresholding class. Then, using Goldberg and Jerrum (1993), the VC-dimension is bounded by
the syntactic complexity of the classiï¬cation rule, written as a Boolean formula of polynomial inequality
predicates. The pseudo-dimension of Wh further implies â„“1 covering number bounds, for which Dong et al.
(2020); Modi et al. (2021) provide fast-rate regression guarantees.
Sample complexity of FORC
We now provide the guarantee for FORC, with its proof deferred to Ap-
pendix E.2.
Theorem 2 (Ofï¬‚ine dÏ€ estimation). Fix Î´ âˆˆ(0, 1). Suppose Assumption 1 and Assumption 2 hold, and Âµâˆ—
is known. Then, given an evaluation policy Ï€, by setting nmle = ËœO(d(P
hâˆˆ[H] Cx
hCa
h)2 log(1/Î´)/Îµ2) and
7

nreg = ËœO(d(P
hâˆˆ[H] Cx
hCa
h)2 log(1/Î´)/Îµ2), with probability at least 1 âˆ’Î´, FORC (Algorithm 1) returns
state occupancy estimates {bdÏ€
h}Hâˆ’1
h=0 satisfying
âˆ¥bdÏ€
h âˆ’d
Ï€
hâˆ¥1 â‰¤Îµ, âˆ€h âˆˆ[H].
The total number of episodes required by the algorithm is
ËœO

dH
P
hâˆˆ[H] Cx
hCa
h
2
log(1/Î´)/Îµ2

.
This result can also be used to establish a guarantee for âˆ¥bdÏ€
h âˆ’dÏ€
hâˆ¥1, simply by decomposing âˆ¥bdÏ€
h âˆ’
dÏ€
hâˆ¥1 â‰¤âˆ¥bdÏ€
h âˆ’d
Ï€
hâˆ¥1 + âˆ¥d
Ï€
h âˆ’dÏ€
hâˆ¥1. The regression error in the ï¬rst term is controlled by Theorem 2. The
second term is a one-sided missingness error due to insufï¬cient coverage of data, which we have character-
ized in Proposition 2. Note that we split âˆ¥bdÏ€
h âˆ’dÏ€
hâˆ¥1 into two terms using d
Ï€
h as an intermediate quantity
and analyze how their errors accumulate over the horizon separately; alternatively, one can directly try to
analyze how âˆ¥bdÏ€
h âˆ’dÏ€
hâˆ¥1 depends on âˆ¥bdÏ€
hâˆ’1 âˆ’dÏ€
hâˆ’1âˆ¥1. In general, we ï¬nd the latter can yield signiï¬cantly
worse boundsâ€”in fact, exponentially worse, as will be seen in Section 4.
Ofï¬‚ine policy optimization
Theorem 2 provides learning guarantees for d
Ï€
h, which is a point-wise lower
bound of dÏ€
h. When we consider standard return maximization with a given reward function, having access
to bdÏ€
h â‰ˆd
Ï€
h immediately enables pessimistic policy evaluation (Jin et al., 2020c; Xie et al., 2021), and we
are only Îµ-suboptimal compared to the maximal value computed over covered parts of the data, i.e., with
respect to d
Ï€
h. The immediate implication is that we can compete with the best policy fully covered by data
(satisfying property 2 of Proposition 2); see Appendix E.3 for the full statement and proof.
Theorem 3 (Ofï¬‚ine policy optimization). Fix Î´ âˆˆ(0, 1) and suppose Assumption 1 and Assumption 2 hold,
and Âµâˆ—is known. Given a policy class Î , let {bdÏ€
h}hâˆˆ[H],Ï€âˆˆÎ  be the output of running Algorithm 1. Then
with probability at least 1 âˆ’Î´, for any reward function R and policy selected as bÏ€R = argmaxÏ€âˆˆÎ  bvÏ€
R, we
have
vbÏ€R
R â‰¥argmax
Ï€âˆˆÎ 
vÏ€
R âˆ’Îµ,
where vÏ€
R and bvÏ€
R are deï¬ned in Proposition 1, and vR is deï¬ned similarly for {d
Ï€
h}. The total number of
episodes required by the algorithm is
ËœO

dH3 P
hâˆˆ[H] Cx
hCa
h
2
log(|Î |/Î´)/Îµ2

.
Computation
We remark that our policy optimization result only enjoys statistical efï¬ciency and does not
guarantee computational efï¬ciency, as Theorem 3 assumes that we can enumerate over candidate policies
and run FORC for each of them; similar comments apply to our later online algorithm as well. Since the
optimization variable is a policy, the most promising approach is to come up with off-policy policy-gradient
(OPPG) algorithms to approximate the objective. However, existing model-free OPPG methods all rely
on value-function approximation (Nachum et al., 2019b; Liu et al., 2019), which is not available in our
setting. Studying OPPG with only density(-ratio) approximation will be a pre-requisite for investigating the
computational feasibility of our problem, which we leave for future work.
8

4
Online policy cover construction
We now consider the online setting where the learner explores the MDP to collect its own data. The hope is
that we will collect exploratory datasets that provide sufï¬cient coverage for all policies in Î  (so that we can
estimate their occupancies accurately), which is measured by the standard deï¬nition of concentrability.
Deï¬nition 2 (Concentrability Coefï¬cient (CC)). Given a policy class Î  and any distribution d âˆˆâˆ†(X),
the concentrability coefï¬cient at level h relative to d is
CCh(d) = inf
n
c âˆˆR : maxÏ€âˆˆÎ 

dÏ€
h
d

âˆâ‰¤c
o
.
To achieve this goal, we ï¬rst recall the following result, which shows the existence of an exploratory
data distribution that satisï¬es the above criterion and hints at how to construct it.
Proposition 3 (Adapted from Chen and Jiang (2019), Prop. 10). Given a policy class Î  and h, let {dÏ€h,i
âˆ—
h
}d
i=1
be the barycentric spanner (Deï¬nition 4 in Appendix I.2) of {dÏ€
h}Ï€âˆˆÎ . Then, CCh

1
d
Pd
i=1 dÏ€h,i
âˆ—
h

â‰¤d.
Proposition 3 shows that for each level h, an exploratory distribution that has d concentrability always
exists. It is simply the mixture of {dÏ€h,i
âˆ—
h
} for i âˆˆ[d], which can be identiï¬ed if we have access to dÏ€
h for
all Ï€ âˆˆÎ . Of course, we can only estimate dÏ€
h if we have exploratory data, so the estimation of dÏ€
h and the
identiï¬cation of {Ï€h,i
âˆ—} need to be interleaved to overcome this â€œchicken-and-eggâ€ problem (Agarwal et al.,
2020; Modi et al., 2021): suppose we have already constructed policy cover at h âˆ’1. We can construct it
for the next level as follows:
1. Collect a dataset Dhâˆ’1 by rolling in to level h âˆ’1 with the policy cover, with CChâˆ’1(dD
hâˆ’1) â‰¤d, then
taking a uniformly random action, thereby CCh(dD,â€ 
hâˆ’1) â‰¤dK.
2. Use FORC to estimate dÏ€
h for all Ï€ âˆˆÎ  based on Dhâˆ’1.
3. Choose their barycentric spanner as the policy cover for level h, with CCh(dD
h ) â‰¤d.
The idea is that, since we have an exploratory distribution at level h âˆ’1, taking a uniform action afterwards
will give us an exploratory distribution at level h, though the degree of exploration will be diluted by a factor
of K. We collect data from this distribution to estimate dÏ€
h and compute the barycentric spanner for level h,
which will bring the concentrability coefï¬cient back to d, so that the process can repeat inductively.
The above reasoning makes an idealized assumption that dÏ€
h can be estimated perfectly. In such a
case, the constructed distribution will provide perfect coverage, so that the clipping introduced in Section 3
becomes completely unnecessary: all clipping operations would be inactive (by setting Cx
h = d and Ca
h =
K), and d
Ï€
h â‰¡dÏ€
h. Unfortunately, when the estimation error of dÏ€
h is taken into consideration, the reasoning
breaks down seriously.
The ï¬rst problem is that our estimate bdÏ€
h from FORC is not necessarily linear due to its product form.
However, that is not a concern as we can linearize it (corresponding to line 7 in Algorithm 2); we also have
an alternative procedure for FORC that directly produces linear bdÏ€
h (see Appendix D.3), so in this section
we will ignore this issue and pretend that bdÏ€
h is linear (thus is the same as edÏ€
h in Algorithm 2) for ease of
presentation.
7MLE only needs to be done once and not for every Ï€ âˆˆÎ .
9

Algorithm 2 FORC-guided Exploration (FORCE)
Input: policy class Î , density feature Âµâˆ—, n = nmle + nreg.
1: Initialize bdÏ€
0 = d0 and edÏ€
0 = d0, âˆ€Ï€ âˆˆÎ .
2: for h = 1, . . . , H do
3:
Construct {edÏ€hâˆ’1,i
hâˆ’1
}d
i=1 as the barycentric spanner of {edÏ€
hâˆ’1}Ï€âˆˆÎ , and set Î expl
hâˆ’1 = {Ï€hâˆ’1,i}d
i=1.
4:
Draw a tuple dataset Dhâˆ’1 = {(x(i)
hâˆ’1, a(i)
hâˆ’1, x(i)
h )}n
i=1 using unif(Î expl
hâˆ’1) â—¦unif(A).
5:
for Ï€ âˆˆÎ  do
6:
Estimate bdÏ€
h using the h-level loop7 of Algorithm 1 (lines 4-6) with Dhâˆ’1, bdÏ€
hâˆ’1, Cx
hâˆ’1 = d,
Ca
hâˆ’1 = K.
7:
Find the closest linear approximation edÏ€
h = âŸ¨Âµâˆ—
hâˆ’1, eÎ¸hâŸ©where eÎ¸h = argminÎ¸hâˆˆRd âˆ¥âŸ¨Âµâˆ—
hâˆ’1, Î¸hâŸ©âˆ’
bdÏ€
hâˆ¥1.
8:
end for
9: end for
Output: estimated state occupancy measure {bdÏ€
h}hâˆˆ[H],Ï€âˆˆÎ .
4.1
Taming error exponentiation
Now that the issue of (non-)linear bdÏ€
h is out of the way, we are ready to see where the real trouble is: note
that the barycentric spanner computed from {bdÏ€
h}Ï€âˆˆÎ  satisï¬es

bdÏ€
h
1
d
Pd
i=1 bdÏ€h,i
h

âˆ
â‰¤d,
âˆ€Ï€ âˆˆÎ .
(5)
However, the actual distribution induced by the policy cover {Ï€h,i}d
i=1 is dD
h = 1
d
Pd
i=1 dÏ€h,i
h
. Suppose for
now we have nmle = âˆfor perfect estimation of dD
h ; even then, the regression target in Eq. (3) will no
longer be bounded without clipping, as the boundedness of bd/bd does not imply that of bd/d, and the latter
can be very large or even inï¬nite.
While the unbounded regression target can be easily controlled by clipping, analyzing the algorithm and
bounding its error still prove to be very challenging. A natural strategy is to inductively bound âˆ¥bdÏ€
h âˆ’dÏ€
hâˆ¥1
using âˆ¥bdÏ€
hâˆ’1 âˆ’dÏ€
hâˆ’1âˆ¥1. Unfortunately, this approach fails miserably, as directly analyzing âˆ¥bdÏ€
h âˆ’dÏ€
hâˆ¥1 yields
âˆ¥bdÏ€
h âˆ’dÏ€
hâˆ¥1 â‰¤(1 + d)âˆ¥bdÏ€
hâˆ’1 âˆ’dÏ€
hâˆ’1âˆ¥1 + Â· Â· Â· ,
(6)
implying an O(d)H exponential error blow-up. (The concrete reason for this failure will be made clear
shortly.) In Appendix D.4, we also discuss an alternative approach that â€œpretendsâ€ data to be perfectly ex-
ploratory, which only addresses the problem superï¬cially and still suffers O(d)H error exponentiation, just
in a different way. Issues that bear high-level similarities are commonly encountered in level-by-level ex-
ploration algorithms, which often demand the so-called reachability assumption (Du et al., 2019, Deï¬nition
2.1), which we do not need.
As all the earlier hints allude to, the key to breaking error exponentiation is to split the error using d
Ï€
h into
its two sources with very different natures: a â€œtwo-sidedâ€ regression error âˆ¥bdÏ€
h âˆ’d
Ï€
hâˆ¥1, and a â€œone-sidedâ€
missingness error âˆ¥d
Ï€
h âˆ’dÏ€
hâˆ¥1 (in the sense that d
Ï€
h â‰¤dÏ€
h). Because the ofï¬‚ine occupancy estimation module
of Algorithm 2 is the same as that of Algorithm 1, Lemma 1 still holds (left Ã—1 chain of Figure 1), implying
that âˆ¥bdÏ€
h âˆ’d
Ï€
hâˆ¥1 can be bounded irrespective of the data distribution.
This observation disentangles the regression error from the rest of the analysis, allowing us to focus on
bounding the missingness error. For the latter, Proposition 2 also exhibits linear error propagation, as it
takes the form of Ah â‰¤Ahâˆ’1 + Bhâˆ’1 where Ah = âˆ¥d
Ï€
h âˆ’dÏ€
hâˆ¥1. However, it still remains to show that the
10

dâ‡¡
h
A
AB6nicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE1ItQ9OKxov2ANpTNdtMu3WzC7kQoT/BiwdFvPqLvPlv3LY5aOuDgcd7M8zMCxIpDLrut1NYWV
1b3yhulra2d3b3yvsHTROnmvEGi2Ws2wE1XArFGyhQ8naiOY0CyVvB6Hbqt564NiJWjzhOuB/RgRKhYBSt9DC8dnvlilt1ZyDLxMtJBXLUe+
Wvbj9macQVMkmN6Xhugn5GNQom+aTUTQ1PKBvRAe9YqmjEjZ/NTp2QE6v0SRhrWwrJTP09kdHImHEU2M6I4tAselPxP6+TYnjlZ0IlKXLF5
ovCVBKMyfRv0heaM5RjSyjTwt5K2JBqytCmU7IheIsvL5PmWdW7qJ7fn1dqN3kcRTiCYzgFDy6hBndQhwYwGMAzvMKbI50X5935mLcWnHzmE
P7A+fwBwQGNdg=</latexit>h = 0
h = 1
h = 2
h = H âˆ’1
â‡¥1
â‡¥1
â‡¥1
â‡¥1
bdâ‡¡
h
V
CUIAWMFC2OR6ENqQuQ4TmvVsSPbAVWhn8LCAEKsfAkbf4PTZoCWI1k6Ouce3esTpowq7TjfVmVldW19o7pZ29re2d2z6/tdJTKJSQcLJmQ/RIowyklHU81IP5UEJSEjvXB8Xfi9ByIVFfxOT1LiJ2jIaUwx0kYK7LonjF2kYRS
M7r2UBnbDaTozwGXilqQBSrQD+8uLBM4SwjVmSKmB6Taz5HUFDMyrXmZIinCYzQkA0M5Sojy89npU3hslAjGQprHNZypvxM5SpSaJKGZTJAeqUWvEP/zBpmOL/2c8jThOP5ojhjUAtY9AjKgnWbGIwpKaWyEeIYmwNm3V
TAnu4peXSfe06Z43z27PGq2rso4qOARH4AS4AK0wA1ogw7A4BE8g1fwZj1ZL9a79TEfrVhl5gD8gfX5AziMk/w=</latexit>
d
â‡¡
h
kbdâ‡¡
h âˆ’d
â‡¡
hk1
kd
â‡¡
h âˆ’dâ‡¡
hk1
â‡¥O(d)
9
x40IRt/6HO/GaZqFth4YOJxzL/fM8RLOpLKsb2NpeWV1b20Ud7c2t7ZNf2zJOBaEtEvNYdD0sKWcRbSmO0mguLQ47Tja6nfueBCsni6F6NE+qGeBCxgBGstNQ3Dx3FQirRbdUJsRrKIPMnp32zYtWsHGiR2AWpQIFm3/xy/JikIY0U4VjKnm0lys2wUIxwOik7qaQJiM8oD1NI6xPulmefoJOtOKjIBb6RQrl6
u+NDIdSjkNPT+YR572p+J/XS1Vw6WYsSlJFIzI7FKQcqRhNq0A+E5QoPtYE8F0VkSGWGCidGFlXYI9/+VF0j6r2e1+l290rgq6ijBERxDFWy4gAbcQBNaQOARnuEV3own48V4Nz5mo0tGsXMAf2B8/gAEwpT1</latexit>â‡¥O(d)
Figure 1:
Error propagation diagram for FORCE. â€œâ€¢ â†’â€¢â€ with Ã—c means (â€¢) â‰¤c Ã— (â€¢) + (other
instantaneous errors that do not accumulate over horizon), and multiple incoming arrows imply sum of
errors. The left Ã—1 chain is from Lemma 1, the right Ã—1 chain from Proposition 2, and the Ã—O(d) edges
from Lemma 4.
additional error (â€œBhâˆ’1â€) has no dependence on the inductive error (â€œAhâˆ’1â€), otherwise we would still have
error exponentiation.8 This is shown in the following key lemma:
Lemma 4. For any h âˆˆ[H] and Ï€ âˆˆÎ  in Algorithm 2,
âˆ¥d
Ï€
h âˆ’dÏ€
hâˆ¥1 â‰¤âˆ¥d
Ï€
hâˆ’1 âˆ’dÏ€
hâˆ’1âˆ¥1 + 4d max
Ï€â€²âˆˆÎ  âˆ¥bdÏ€â€²
hâˆ’1 âˆ’d
Ï€â€²
hâˆ’1âˆ¥1.
To understand this lemma, recall that the additional error in Proposition 2 characterizes the mass clipped
away at the current level. This mass can be bounded by the regression error of the previous level (maxÏ€â€²âˆˆÎ  âˆ¥bdÏ€â€²
hâˆ’1âˆ’
d
Ï€â€²
hâˆ’1âˆ¥1): intuitively, had we had perfect estimation of bdÏ€â€²
hâˆ’1 = d
Ï€â€²
hâˆ’1, our barycentric spanner would also be
perfect and we would not need any clipping at all in level h, implying 0 additional error in the bound. More
generally, the closer bdÏ€â€²
hâˆ’1 is to d
Ï€â€²
hâˆ’1, the less mass we need to clip away.
That said, this term is not instantaneous and depends inductively on quantities in the previous time
step, still raising concerns of error exponentiation. To see why this is not a problem, we visualize error
propagation in Figure 1: it can be clearly seen that such a dependence corresponds to a â€œcross-edgeâ€, and
appears at most once along any long chain. This also explains the destined failure of directly analyzing
âˆ¥bdÏ€
h âˆ’dÏ€
hâˆ¥1 in Eq. (6), as that corresponds to merging the two chains into one, where every edge along the
only chain acquires an O(d) multiplicative factor.
With this, we can now state the formal guarantee for our algorithm, FORCE. See Algorithm 2 for its
pseudo-code, and the proof of the guarantee is deferred to Appendix F.1.
Theorem 5 (Online dÏ€ estimation). Fix Î´ âˆˆ(0, 1) and consider an MDP M that satisï¬es Assumption 1, and
Âµâˆ—is known. Then by setting nmle = eO
 d3K2H4 log(1/Î´)/Îµ2
,nreg = eO
 d5K2H4 log(|Î |/Î´)/Îµ2
, n =
nmle + nreg, with probability at least 1 âˆ’Î´, FORCE returns state occupancy estimates {bdÏ€
h}hâˆˆ[H],Ï€âˆˆÎ  satis-
fying that
bdÏ€
h âˆ’dÏ€
h

1 â‰¤Îµ, âˆ€h âˆˆ[H], Ï€ âˆˆÎ .
The total number of episodes required by the algorithm is
eO(nH) = eO
 d5K2H5 log(|Î |/Î´)/Îµ2
.
8For example, if Bhâˆ’1 can only be bounded as Bhâˆ’1 â‰¤Ahâˆ’1, we would still have Ah â‰¤2Ahâˆ’1.
11

Theorem 5 also immediately translates to a policy optimization guarantee when combined with Propo-
sition 1:
Theorem 6 (Online policy optimization). Fix Î´ âˆˆ(0, 1) and suppose Assumption 1 and Assumption 2 hold,
and Âµâˆ—is known. Given a policy class Î , let {bdÏ€
h}hâˆˆ[H],Ï€âˆˆÎ  be the output of running FORCE. Then with
probability at least 1 âˆ’Î´, for any reward function R and policy selected as bÏ€R = argmaxÏ€âˆˆÎ  bvÏ€
R, we have
vbÏ€R
R â‰¥argmax
Ï€âˆˆÎ 
vÏ€
R âˆ’Îµ,
where vÏ€
R and bvÏ€
R are deï¬ned in Proposition 1. The total number of episodes required by the algorithm is
ËœO
 d5K2H7 log(|Î |/Î´)/Îµ2
.
The proof is deferred to Appendix F.2. We remark that Theorem 6 is a reward-free learning guarantee
(Jin et al., 2020a; Chen et al., 2022b), and it is easy to see that Algorithm 2 is deployment efï¬cient (Huang
et al., 2022).
5
Representation learning
In this section, we extend the ofï¬‚ine (Section 3) and online (Section 4) results to the representation learning
setting. Here, the true density feature Âµâˆ—is unknown, but the learner has access to a realizable density
feature class Î¥, deï¬ned formally below. For simplicity, we consider ï¬nite and normalized Î¥, as is standard
in the literature (Agarwal et al., 2020; Modi et al., 2021; Uehara et al., 2021b).
Assumption 3. We have a ï¬nite density feature class Î¥ = S
hâˆˆ[H] Î¥h such that Âµâˆ—
h âˆˆÎ¥h for each h âˆˆ[H],
thus Âµâˆ—âˆˆÎ¥. Further, for any Âµh âˆˆÎ¥h, we have
R
âˆ¥Âµh(x)âˆ¥1(dx) â‰¤BÂµ.
The algorithms and analyses for the representation learning case mostly follow the same template as the
known feature case, so we restrict our discussion to their differences. Recall that, in order to have realizable
function classes for regression and MLE in Section 3, we constructed Fh, Wh using functions linear in the
known Âµâˆ—
hâˆ’1. In order to maintain this realizability when Âµâˆ—
hâˆ’1 is unknown, we instead construct Fh, Wh
using the union of all functions linear in some candidate Âµhâˆ’1 âˆˆÎ¥hâˆ’1, i.e., S
Âµhâˆ’1âˆˆÎ¥hâˆ’1{âŸ¨Âµhâˆ’1, Î¸hâŸ©, Î¸h âˆˆ
Rd} (see Eq. (26) and Eq. (27) for their formal deï¬nitions).
While such union classes allow most of Section 3 and Section 4 to straightforwardly extend to the
representation learning setting, a nontrivial modiï¬cation must be made to the online algorithm. Recall in
line 7 of Algorithm 2, we constructed our policy cover using the barycentric spanner of {edÏ€
h}Ï€âˆˆÎ , the set of
linearized approximations to the density estimates. Importantly, this guaranteed a concentrability coefï¬cient
of d because all edÏ€
h are linear in the same feature Âµâˆ—
hâˆ’1. This is no longer the case with unknown features
because, if linearized in the same way (but over all feasible Âµhâˆ’1 âˆˆÎ¥hâˆ’1), each edÏ€
h can be composed of a
different Âµhâˆ’1 feature, resulting in a CC linear in |Î |. To overcome this issue, we replace line 7 with the
following â€œjoint linearizationâ€ step (see line 8 in Algorithm 4):
bÂµhâˆ’1 =
min
Âµhâˆ’1âˆˆÎ¥hâˆ’1 max
Ï€âˆˆÎ  min
Î¸hâˆˆRd âˆ¥âŸ¨Âµhâˆ’1, Î¸hâŸ©âˆ’bdÏ€
hâˆ¥1,
where all density estimates are linearized using a single feature bÂµhâˆ’1, whose linear span approximates all
bdÏ€
h well. We provide theorems for ofï¬‚ine/online dÏ€ estimation with representation learning below.
12

Theorem 7 (Ofï¬‚ine dÏ€ estimation with representation learning). Fix Î´ âˆˆ(0, 1). Suppose Assumption 1,
Assumption 2, and Assumption 3 hold. Then, given an evaluation policy Ï€, by setting
nmle = ËœO(d(P
hâˆˆ[H] Cx
hCa
h)2 log(|Î¥|/Î´)/Îµ2) and nreg = ËœO(d(P
hâˆˆ[H] Cx
hCa
h)2 log(|Î¥|/Î´)/Îµ2), with prob-
ability at least 1 âˆ’Î´, FORCRL (Algorithm 3) returns state occupancy estimates {bdÏ€
h}Hâˆ’1
h=0 satisfying that
bdÏ€
h âˆ’d
Ï€
h

1 â‰¤Îµ, âˆ€h âˆˆ[H].
The total number of episodes required by the algorithm is
ËœO

dH
P
hâˆˆ[H] Cx
hCa
h
2
log(|Î¥|/Î´)/Îµ2

.
Theorem 8 (Online dÏ€ estimation with representation learning). Fix Î´ âˆˆ(0, 1) and suppose Assumption 1
and Assumption 3 hold. Then by setting nmle = eO(d3K2H4 log(|Î¥|/Î´)/Îµ2), nreg = eO(d5K2H4 log(|Î ||Î¥|/Î´)/Îµ2),
n = nmle +nreg, with probability at least 1âˆ’Î´, FORCRLE (Algorithm 4) returns state occupancy estimates
{bdÏ€
h}Hâˆ’1
h=0 satisfying that
âˆ¥bdÏ€
h âˆ’dÏ€
hâˆ¥1 â‰¤Îµ, âˆ€h âˆˆ[H], Ï€ âˆˆÎ .
The total number of episodes required by the algorithm is
eO
 d5K2H5 log(|Î ||Î¥|/Î´)/Îµ2
.
The detailed proofs of these two theorems are given in Appendix G. We also present the theorems and
proofs for ofï¬‚ine/online policy optimization with representation learning as well as the formal representation
learning algorithms in Appendix G.
6
Conclusion
We have shown how to leverage density features for statistically efï¬cient state occupancy estimation and
reward-free exploration in low-rank MDPs, culminating in policy optimization guarantees. An important
open problem lies in investigating the computational efï¬ciency of our algorithms (e.g., through off-policy
policy gradient).
Acknowledgements
The authors thank Akshay Krishnamurthy and Dylan Foster for discussions related to MLE generalization
error bounds. NJ acknowledges funding support from NSF IIS-2112471 and NSF CAREER IIS-2141781.
References
Pieter Abbeel and Andrew Y Ng. Apprenticeship Learning via Inverse Reinforcement Learning. In Pro-
ceedings of the 21st International Conference on Machine learning, page 1. ACM, 2004.
Alekh Agarwal, Daniel Hsu, Satyen Kale, John Langford, Lihong Li, and Robert Schapire. Taming the
monster: A fast and simple algorithm for contextual bandits. In International Conference on Machine
Learning, pages 1638â€“1646, 2014.
13

Alekh Agarwal, Sham Kakade, Akshay Krishnamurthy, and Wen Sun. Flambe: Structural complexity and
representation learning of low rank mdps. Advances in Neural Information Processing Systems, 2020.
Martin Anthony and Peter L Bartlett. Neural network learning: Theoretical foundations. cambridge univer-
sity press, 2009.
Baruch Awerbuch and Robert Kleinberg. Online linear optimization and adaptive routing. Journal of Com-
puter and System Sciences, 74(1):97â€“114, 2008.
Peter Bartlett and Ambuj Tewari. Sample complexity of policy search with known dynamics. Advances in
Neural Information Processing Systems, 19, 2006.
Fan Chen, Song Mei, and Yu Bai. Uniï¬ed algorithms for rl with decision-estimation coefï¬cients: No-regret,
pac, and reward-free learning. arXiv preprint arXiv:2209.11745, 2022a.
Jinglin Chen and Nan Jiang. Information-theoretic considerations in batch reinforcement learning. In Inter-
national Conference on Machine Learning, 2019.
Jinglin Chen and Nan Jiang. Ofï¬‚ine reinforcement learning under value and density-ratio realizability: The
power of gaps. In Conference on Uncertainty in Artiï¬cial Intelligence, 2022.
Jinglin Chen, Aditya Modi, Akshay Krishnamurthy, Nan Jiang, and Alekh Agarwal. On the statistical
efï¬ciency of reward-free exploration in non-linear rl. In Advances in Neural Information Processing
Systems, 2022b.
Christoph Dann and Emma Brunskill. Sample complexity of episodic ï¬xed-horizon reinforcement learning.
In Advances in Neural Information Processing Systems, pages 2818â€“2826, 2015.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirec-
tional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.
Kefan Dong, Jian Peng, Yining Wang, and Yuan Zhou. âˆšn-regret for learning in Markov decision processes
with function approximation and low Bellman rank. In Conference on Learning Theory, 2020.
Simon Du, Akshay Krishnamurthy, Nan Jiang, Alekh Agarwal, Miroslav Dudik, and John Langford. Prov-
ably efï¬cient rl with rich observations via latent state decoding. In International Conference on Machine
Learning, 2019.
Jianqing Fan, Zhaoran Wang, Yuchen Xie, and Zhuoran Yang. A theoretical analysis of deep q-learning. In
Learning for Dynamics and Control, pages 486â€“489. PMLR, 2020.
Fei Feng, Ruosong Wang, Wotao Yin, Simon S Du, and Lin Yang. Provably efï¬cient exploration for re-
inforcement learning using unsupervised learning. Advances in Neural Information Processing Systems,
33:22492â€“22504, 2020.
Carles Gelada and Marc G Bellemare. Off-policy deep reinforcement learning by bootstrapping the covariate
shift. In Proceedings of the AAAI Conference on Artiï¬cial Intelligence, volume 33, pages 3647â€“3655,
2019.
Paul Goldberg and Mark Jerrum. Bounding the vapnik-chervonenkis dimension of concept classes parame-
terized by real numbers. In Proceedings of the sixth annual conference on Computational learning theory,
pages 361â€“369, 1993.
14

Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron
Courville, and Yoshua Bengio. Generative adversarial networks. Communications of the ACM, 63(11):
139â€“144, 2020.
Assaf Hallak and Shie Mannor. Consistent on-line off-policy evaluation. In International Conference on
Machine Learning, pages 1372â€“1383. PMLR, 2017.
Elad Hazan, Sham M Kakade, Karan Singh, and Abby Van Soest. Provably efï¬cient maximum entropy
exploration. In International Conference on Machine Learning, 2019.
Audrey Huang and Nan Jiang. Beyond the return: Off-policy function estimation under user-speciï¬ed error-
measuring distributions. In Advances in Neural Information Processing Systems, 2022.
Jiawei Huang, Jinglin Chen, Li Zhao, Tao Qin, Nan Jiang, and Tie-Yan Liu. Towards deployment-efï¬cient
reinforcement learning: Lower bound and optimality. In International Conference on Learning Repre-
sentations, 2022.
Nan Jiang, Akshay Krishnamurthy, Alekh Agarwal, John Langford, and Robert E. Schapire. Contextual
decision processes with low Bellman rank are PAC-learnable. In International Conference on Machine
Learning, 2017.
Chi Jin, Akshay Krishnamurthy, Max Simchowitz, and Tiancheng Yu. Reward-free exploration for rein-
forcement learning. In International Conference on Machine Learning, 2020a.
Chi Jin, Zhuoran Yang, Zhaoran Wang, and Michael I Jordan. Provably efï¬cient reinforcement learning with
linear function approximation. In Conference on Learning Theory, pages 2137â€“2143. PMLR, 2020b.
Ying Jin, Zhuoran Yang, and Zhaoran Wang. Is pessimism provably efï¬cient for ofï¬‚ine rl? arXiv preprint
arXiv:2012.15085, 2020c.
Jongmin Lee, Wonseok Jeon, Byungjun Lee, Joelle Pineau, and Kee-Eung Kim. Optidice: Ofï¬‚ine policy
optimization via stationary distribution correction estimation. In International Conference on Machine
Learning, pages 6120â€“6130. PMLR, 2021.
Qiang Liu, Lihong Li, Ziyang Tang, and Dengyong Zhou. Breaking the curse of horizon: Inï¬nite-horizon
off-policy estimation. In Advances in Neural Information Processing Systems, pages 5356â€“5366, 2018.
Qinghua Liu, Alan Chung, Csaba SzepesvÂ´ari, and Chi Jin. When is partially observable reinforcement
learning not scary? In Conference on Learning Theory, pages 5175â€“5220. PMLR, 2022.
Yao Liu, Adith Swaminathan, Alekh Agarwal, and Emma Brunskill. Off-policy policy gradient with state
distribution correction. arXiv preprint arXiv:1904.08473, 2019.
Aditya Modi, Jinglin Chen, Akshay Krishnamurthy, Nan Jiang, and Alekh Agarwal. Model-free represen-
tation learning and exploration in low-rank mdps. arXiv:2102.07035, 2021.
Mehryar Mohri and Afshin Rostamizadeh. Rademacher complexity bounds for non-iid processes. Advances
in Neural Information Processing Systems, 21, 2008.
Mirco Mutti, Riccardo De Santi, Piersilvio De Bartolomeis, and Marcello Restelli. Challenging common
assumptions in convex reinforcement learning. arXiv preprint arXiv:2202.01511, 2022.
Oï¬r Nachum, Yinlam Chow, Bo Dai, and Lihong Li. Dualdice: Behavior-agnostic estimation of discounted
stationary distribution corrections. Advances in Neural Information Processing Systems, 32, 2019a.
15

Oï¬r Nachum, Bo Dai, Ilya Kostrikov, Yinlam Chow, Lihong Li, and Dale Schuurmans. Algaedice: Policy
gradient from arbitrary experience. arXiv preprint arXiv:1912.02074, 2019b.
Asuman Ozdaglar, Sarath Pattathil, Jiawei Zhang, and Kaiqing Zhang. Revisiting the linear-programming
framework for ofï¬‚ine rl with general function approximation. arXiv preprint arXiv:2212.13861, 2022.
Tongzheng Ren, Tianjun Zhang, Lisa Lee, Joseph E Gonzalez, Dale Schuurmans, and Bo Dai. Spectral
decomposition representation for reinforcement learning. arXiv preprint arXiv:2208.09515, 2022.
Masatoshi Uehara, Masaaki Imaizumi, Nan Jiang, Nathan Kallus, Wen Sun, and Tengyang Xie. Finite sam-
ple analysis of minimax ofï¬‚ine reinforcement learning: Completeness, fast rates and ï¬rst-order efï¬ciency.
arXiv preprint arXiv:2102.02981, 2021a.
Masatoshi Uehara, Xuezhou Zhang, and Wen Sun. Representation learning for online and ofï¬‚ine RL in
low-rank MDPs. In International Conference on Learning Representations, 2021b.
Sara A Van de Geer. Empirical Processes in M-estimation, volume 6. Cambridge university press, 2000.
Vladimir Vapnik. Statistical learning theory, volume 2. Wiley New York, 1998.
Tengyang Xie, Ching-An Cheng, Nan Jiang, Paul Mineiro, and Alekh Agarwal. Bellman-consistent pes-
simism for ofï¬‚ine reinforcement learning. Advances in neural information processing systems, 34, 2021.
Ming Yin and Yu-Xiang Wang. Towards instance-optimal ofï¬‚ine reinforcement learning with pessimism.
Advances in neural information processing systems, 34:4065â€“4078, 2021.
Tom Zahavy, Brendan Oâ€™Donoghue, Guillaume Desjardins, and Satinder Singh. Reward is enough for
convex mdps. Advances in Neural Information Processing Systems, 34:25746â€“25759, 2021.
Wenhao Zhan, Baihe Huang, Audrey Huang, Nan Jiang, and Jason Lee. Ofï¬‚ine reinforcement learning with
realizability and single-policy concentrability. In Conference on Learning Theory, pages 2730â€“2775.
PMLR, 2022.
Tong Zhang. From Îµ-entropy to kl-entropy: Analysis of minimum information complexity density estima-
tion. The Annals of Statistics, 34(5):2180â€“2210, 2006.
16

A
Related works
In this section, we discuss a few lines of related work in detail.
First, the closest related works involve RL with unsupervised-learning oracles (Du et al., 2019; Feng
et al., 2020). Instead of investigating low-rank MDPs, they consider more restricted block MDPs and need
stronger assumptions such as reachability, identiï¬ability, and separatability (we refer the reader to their
works for the deï¬nitions). Their notion of â€œdecoderâ€ looks like density features in low-rank MDPs, but
they are incomparable. The crucial property of â€œdecoderâ€ is that it is a map from the X space to the low
d dimensional space. This map itself no longer exists in low-rank MDPs. In addition, the density feature
serves a different purpose in our paper, as its primary purpose is for constructing the weight function class.
A second line of related work is model-based representation learning in low-rank MDPs (Agarwal et al.,
2020; Uehara et al., 2021b; Ren et al., 2022), which assumes that both a realizable left feature class Î¦ âˆ‹Ï†âˆ—
and realizable density (right) feature class Î¥ âˆ‹Âµâˆ—are given to the learner, essentially inducing a realizable
dynamics model class. The learned model (features) are subsequently used for downstream planning. In
comparison, we utilize a much weaker inductive bias as we only require a realizable density feature class Î¥,
and we do not try to learn a dynamics model. Though we additionally need a policy class Î , this is a very
basic and natural function class to include. It can be immediately obtained from the (Q-)value function class
in the value-based approach, and from the dynamics model class (given a reward function) in the model-
based approach above. In terms of the algorithm design, we also use MLE, but for a different objective (the
data distribution, instead of the dynamics model).
The importance weight (density-ratio) learning used within our algorithms is related to the marginalized
importance sampling of the ofï¬‚ine RL algorithms in Nachum et al. (2019a); Lee et al. (2021); Uehara et al.
(2021a); Zhan et al. (2022); Chen and Jiang (2022); Huang and Jiang (2022); Ozdaglar et al. (2022). These
works do not make the low-rank MDP assumption and study the problem in general MDPs, and require both
a weight function class and value function class for learning. We leverage the true density Âµâˆ—or density
feature class Î¥ to construct the realizable weight function class, allowing us to achieve statistically faster
rates in the low-rank MDP setting. We do not need a value function class and instead only need a weaker
(as discussed in the previous paragraph) policy class Î . Lastly, we note that the aforementioned works
all learn weights, while our goal is to learn the densities. Extracting the densities from the weights allows
us to efï¬ciently explore the MDP using its low-dimensional structure, and additionally enables our return
maximization guarantees of Proposition 1 by separating them from the underlying data distribution.
B
Hardness result without the policy class
In this section, we show that without policy class Î , learning in low-rank MDPs (or an easier simplex
feature setting) is provably hard even when the true density feature Âµâˆ—is known to the learner. The crux is
that low-rank MDPs can readily emulate a fully general contextual bandit problem, where Âµâˆ—is useless. For
the hardness result, we adapt Theorem 2 of Dann and Brunskill (2015) to our case by only keeping their
second to third level to get a contextual bandit problem.
To provide speciï¬cs for the reward and transition functions, we ï¬rst note that the subscript of the re-
ward/transition function denotes which level it applies to (e.g., P0 are the transitions to x1 from x0). Level
h = 0 is composed of |X| âˆ’3 states with zero reward, i.e., x0 âˆˆ{1, . . . , |X| âˆ’3} and R0(i) = 0, âˆ€i âˆˆ
{1, . . . , |X|âˆ’3}. Level h = 1 is composed of 2 states, i.e., x1 âˆˆ{+, âˆ’}, where R1(+) = 1 and R1(âˆ’) = 0.
Lastly, at level h = 2 we have a single null absorbing state x2.
For the transition functions, in level h = 0 the transitions P0 are Bernoulli distributions where for any
state i âˆˆ{1, . . . , |X| âˆ’3} and action a0 âˆˆA, we have P0(+|i, a0) =
1
2 + Îµâ€²
i(a0) and P0(âˆ’|i, a0) =
1
2 âˆ’Îµâ€²
i(a0). Here, Îµâ€²
i is deï¬ned in a per-state manner given a parameter Îµ. We have Îµâ€²
i(a0) = Îµ/2 if a0 = aâˆ—
0,
17

where aâˆ—
0 is a ï¬xed action; Îµâ€²
i(a0) = Îµ if a0 = ai,âˆ—
0 where ai,âˆ—
0 is an unknown action deï¬ned per state i; and
Îµâ€²
i(a0) = 0 otherwise. In level h = 1, the transitions P1 simply transmit deterministically to the absorbing
state x2, i.e., P1(x2|x1, a1) = 1 for all x1 âˆˆ{+, âˆ’} and a1 âˆˆA.
It is easy to see that the dynamics of this contextual bandit can be modeled using simplex features, thus
it is an instantiation of low-rank MDPs. Since we only have two levels (H = 2), we only need to verify
that P0 and P1 can be written in the desired form (Assumption 1). In level h = 0, we add two latent states
corresponding to the rewarding and non-rewarding state, thus d = 2. Then in level h = 0, we have right
features Âµâˆ—
0(+) = [1, 0] and Âµâˆ—
0(âˆ’) = [0, 1], and left features Ï†âˆ—
0(x0, a0) = [P1(+|x0, a0), P1(âˆ’|x0, a0)]
for any (x0, a0), corresponding to the original Bernoulli distribution. It is easy to see that this satisï¬es
Assumption 1, i.e., for any (x0, a0, x1) we have P0(x1|x0, a0) = âŸ¨Ï†âˆ—
0(x0, a0), Âµâˆ—
0(x1)âŸ©. In level h = 1 we
can simply set a single latent state representing the singleton x2, and observe that Assumption 1 is trivially
satisï¬ed with Âµâˆ—
1(x2) = 1, and Ï†âˆ—
1(x1, a1) = 1 for any (x1, a1).
Finally, from Theorem 2 of Dann and Brunskill (2015), we know that the sample complexity of learning
in this contextual bandit problem is â„¦(|X|), demonstrating that efï¬cient learning is impossible in low-rank
MDPs (or the simplex feature setting) given only Âµâˆ—.
The necessity of K = |A| dependence
It is well known that learning contextual bandits with just a
policy class requires a dependence on |A| in regret and sample complexity; see Agarwal et al. (2014) and
the references therein. This can also be reproduced in the above hardness result: ï¬rst, we can scale up the
construction by adding more actions, and show an â„¦(|X|K) lower bound. Second, we now provide the
learner with a policy class that contains all Markov deterministic policies. The size of the class is O(K|X|),
and the log-size is O(|X| log(K)). Given the logarithmic dependence on K, no polynomial dependence on
log(|Î |) can explain away the linear-in-K dependence in the lower bound, and we must introduce K as a
separate factor in the sample complexity.
C
RL with objectives on state distributions
Proposition 1 also extends to general optimization objectives f({dh}) that are Lipschitz in the input {dh}
(note the Lipschitz property does not require the input to be a valid distribution). This Lipschitzness property
is key for many recent results in convex RL (Zahavy et al., 2021; Mutti et al., 2022), and also holds for return
maximization where f({dÏ€
h}) = vÏ€
R, in which case the Lipschitz constant is related to the maximum reward
maxh,x,a Rh(x, a). While we write the objective f({dh}) using state densities dh(xh) as input for simplicity,
it is straightforward to instead use state-action densities dh(xh)Ï€(ah|xh) formed by directly composing the
state density dh with the policy Ï€. If f is Lipschitz in state-action densities, it will still be Lipschitz in the
state-action densities in the â„“1 norm, which is the exactly the case in return maximization, since any input
density will be composed with same Ï€. Lastly, we note that constraints can also be added to the objective
and to result in a similar statement.
Proposition 4. Suppose the optimization objective is f({dh}), where f is Lipschitz in {dh} under the â„“1
norm, i.e., there exists a constant L > 0 such that for any {dâ€²
h} and {dâ€²â€²
h}
f({dâ€²
h}) âˆ’f({dâ€²â€²
h})
 â‰¤L
X
hâˆˆ[H]
âˆ¥dâ€²
h âˆ’dâ€²â€²
hâˆ¥1.
Then for {bdÏ€
h} such that âˆ¥bdÏ€
h âˆ’dÏ€
hâˆ¥1 â‰¤
Îµ
2H for all Ï€ âˆˆÎ  and h âˆˆ[H], and bÏ€ maximizing the plug-in
estimate of the objective:
bÏ€ = argmax
Ï€âˆˆÎ 
f({bdÏ€
h}),
18

we have
f({dbÏ€
h}) â‰¥max
Ï€âˆˆÎ  f({dÏ€
h}) âˆ’LÎµ.
Proof. For any Ï€ âˆˆÎ , from the Lipschitz assumption,
f({dÏ€
h}) âˆ’f({bdÏ€
h})
 â‰¤L
X
hâˆˆ[H]
âˆ¥dÏ€
h âˆ’bdÏ€
hâˆ¥1 â‰¤LÎµ/2.
Then, letting Ï€âˆ—= argmaxÏ€âˆˆÎ  f({dÏ€
h}) denote the maximizer of the true objective and using the above
inequality,
f({dbÏ€
h}) âˆ’f({dÏ€âˆ—
h }) = f({dbÏ€
h}) âˆ’f({bdbÏ€
h}) + f({bdbÏ€
h}) âˆ’f({bdÏ€âˆ—
h }) + f({bdÏ€âˆ—
h }) âˆ’f({dÏ€âˆ—
h }) â‰¥âˆ’LÎµ.
On bdÏ€
h being invalid distributions
One potential issue is that some of the objective functions f considered
in the literature are only well deï¬ned for valid probability distributions (e.g., entropy). This is easy to deal
with in the online setting, as we can simply project bdÏ€
h onto the probability simplex, which picks up a
multiplicative factor of 2 in âˆ¥bdÏ€
h âˆ’dÏ€
hâˆ¥1 (c.f. the analysis of the linearization step in Algorithm 2).
For the ofï¬‚ine setting, however, the situation can be trickier. For example, the above projection idea is
clearly bad for return maximization, since after projection all bdÏ€
h satisfy âˆ¥bdÏ€
hâˆ¥1 = 1 and we lose pessimism.
From an analytical point of view, pessimistic approaches (e.g., Theorem 3) only pays one factor of the
missingness error âˆ¥d
Ï€
h âˆ’dÏ€
hâˆ¥1 by leveraging its one-sidedness, and a factor of 2 introduced by projection is
simply unacceptable. Therefore, the question is whether we can generalize the pessimism in Theorem 3 to
general objective functions. We only answer this question with a rough sketch and leave the full investigation
to future work: roughly speaking, since we know âˆ¥bdÏ€
h âˆ’d
Ï€
hâˆ¥1 â‰¤Îµâ€² (for some appropriate value of Îµâ€² from
our analysis), we can form a version space for dÏ€
h as:
dÏ€
h âˆˆ{dh : âˆƒdâ€²
h, s.t. dh â‰¥dâ€²
h and âˆ¥dâ€²
h âˆ’bdÏ€
hâˆ¥1 â‰¤Îµâ€²}.
Then we can simply come up with pessimistic evaluation of f({dÏ€
h}) by minimizing f({dh}) over the above
set. It is not hard to see that such an approach will provide similar guarantees to Theorem 3 when applied to
return maximization.
D
Alternative setups, algorithm designs, and analyses
D.1
Ofï¬‚ine data assumptions
As mentioned in Section 3, our ofï¬‚ine data assumption allows sequentially dependent batches, where in-
batch tuples are i.i.d. samples. This is already weaker than the standard fully i.i.d. settings considered in the
ofï¬‚ine RL literature, and here we further comment on how to handle various extensions.
Trajectory data
One simple setting is when data are i.i.d. trajectories sampled from a ï¬xed policy. (This
setting does not ï¬t our need for the online algorithm, but is a representative setup for the purpose of ofï¬‚ine
learning.) While our protocol directly handles it (we can simply split the data in H chunks and call them
D0, D1, . . .), it seems somewhat wasteful as we only extract 1 transition tuple per trajectory, potentially
worsening the sample complexity by a factor of H. This is because in our analysis of the regression step
(Algorithm 1, line 5), we treat the regression target (which depends on bdÏ€
h) as ï¬xed and independent of the
current dataset. If we want to use all the data, we would need to union bound over the target as well; see
19

similar considerations in the work of Fan et al. (2020). A slow-rate analysis follows straightforwardly, and
we leave the investigation of fast-rate analysis to future work. We also remark that our current ofï¬‚ine setup
(Assumption 2) is the most natural protocol for the data collected from the online algorithm (Section 4), and
using full trajectory data does not seem to improve the theoretical guarantees of the online setting.
Fully adaptive data
A more general setting than Assumption 2 is that the data is fully adaptive, i.e., each
trajectory is allowed to depend on all trajectories that before it. To handle such a case, we will need to
replace the i.i.d. concentration inequalities with their martingale versions. Some special treatment in the
concentration bounds will also be needed to handle the random data-splitting step in Algorithm 1, line 3
(c.f. Mohri and Rostamizadeh, 2008); alternatively, if we union bound over regression targets (see previous
paragraph), the data splitting step will no longer be needed.
Unknown and/or non-Markov Ï€D
In Assumption 2 we assume that the last-step policy in the data-
collecting policy is Markov and known, as we need it to form the importance weights on actions. When Ï€D
is still Markov and unknown, we can use behavior cloning to back it out from data, which would require
some additional assumptions (e.g., having access to a policy class that realizes Ï€D), and we do not further
expand on such an analysis. When Ï€D is non-Markov, it is well known that the action in the data tuple
(xh, ah, xh+1) can be still treated as if it were generated from a Markov policyâ€”one can compute the
state-action occupancy for (xh, ah) (which is well-deï¬ned even if Ï€D is non-Markov) and then obtain the
equivalent Markov policy by conditioning on xh. Incidentally, the algorithmic solution is the same as the
case of unknown Markov Ï€D, i.e., behavior cloning.
D.2
Stochastic and/or unknown reward functions
When the reward function is stochastic but still known, Proposition 1 and all policy optimization guarantees
extend straightforwardly, since we can still directly compute the return. The more nontrivial case is when
the reward function R is unknown and comes as part of the data, i.e., we have the usual format of data
tuples that include (possibly) stochastic reward signals, {(x(i)
h , a(i)
h , r(i)
h )}nret
i=1 âˆ¼dD
h . Then given estimates
{bdD
h } (from MLE) and {bdÏ€
h} (from Algorithm 1 or Algorithm 2), the expected return can be estimated by
reweighting the rewards according to the importance weight bdÏ€
h/bdD
h , and assuming this ratio is well-deï¬ned:
bvÏ€
R =
1
nret
nret
X
i=1
X
hâˆˆ[H]
bdÏ€
h(x(i)
h )
bdD
h (x(i)
h )
Ï€h(a(i)
h |x(i)
h )
Ï€D
h (a(i)
h |x(i)
h )
r(i)
h .
It can be shown that we then have |bvÏ€
R âˆ’vÏ€
R| â‰¤Îµ + (additive terms), where the additive terms correspond to
the statistical error of return and MLE estimation, which is O((nret)âˆ’1/2). If bdD
h does not cover bdÏ€
h, which
may generally be the case, clipping (e.g., according to thresholds Cx
h, Ca
h) can again be used, which will
lead to additional error corresponding to clipped mass.
D.3
Algorithm design and analyses
In this section, we discuss alternative designs of the ofï¬‚ine density learning algorithm (Algorithm 1), as
well as their downstream impacts on the online and representation learning algorithms, which use the ofï¬‚ine
module in their inner loops. For simplicity, most discussions are in the case of ofï¬‚ine density learning with
known representation Âµâˆ—.
20

Point estimate in denominator
First, we discuss alternative parameterizations of the weight function
class. To enable more â€œelementaryâ€ â„“âˆcovering arguments, one may consider instead parameterizing the
weight function class as a ratio of linear functions over a ï¬xed function vh : X â†’R, speciï¬cally
Wh(vh) =

wh = âŸ¨Âµâˆ—
hâˆ’1, Î¸hâŸ©
vh
: âˆ¥whâˆ¥âˆâ‰¤Cx
hâˆ’1Ca
hâˆ’1, Î¸h âˆˆRd

.
When Âµâˆ—consists of simplex features, it can be shown that an â„“âˆcovering with scale Î³ of size (1/Î³)d can
be constructed for Wh(vh), because it can be induced by an â„“âˆcovering of the low-dimensional parameter
space that has scale adaptively chosen according to how much the weight can be perturbed with respect to
the denominator, thus ï¬xed size. It is unclear how to construct such â„“âˆcoverings for â€œlinear-over-linearâ€
function classes such as Wh of Algorithm 1. One may consider compositions of standard â„“âˆcoverings gen-
erated separately for the linear numerator and denominator, but bounding the covering error is challenging
due to sensitivity of the denominator to perturbations.
As we will see, however, the key issue with such ï¬xed-denominator parameterizations is that the Bayes-
optimal solution is no longer realizable. To handle this in the analysis, we can introduce an additional
approximation error (similar to Chen and Jiang (2019, Assumption 3) in the value learning setting) that will
appear in the ï¬nal bound, corresponding to how well the Bayes-optimal solution is approximated by the
function class. Depending on the choice of denominator, the approximation error may not be controlled, or
may lead to a slower rate of estimation; loosely, it is deï¬ned as
Îµapprox
h
=
max
whâˆ’1:âˆ¥whâˆ’1âˆ¥âˆâ‰¤Cx
hâˆ’1
min
whâˆˆWh(vh)
wh âˆ’EÏ€
hâˆ’1(dD
hâˆ’1whâˆ’1)

2,dD,â€ 
hâˆ’1 .
One obvious choice for the ï¬xed denominator is vh = bd D,â€ 
hâˆ’1, since it is immediately available from the
MLE data estimation step, plus the linear numerator can then be extracted exactly through the elementwise
multiplication bdÏ€
h = bwÏ€
h bd D,â€ 
hâˆ’1. However, the Bayes-optimal predictor EÏ€
hâˆ’1(dhâˆ’1) is no longer realizable,
since EÏ€
hâˆ’1(dhâˆ’1) = PÏ€
hâˆ’1(dhâˆ’1)/dD,â€ 
hâˆ’1 is a linear function over the true data distribution dD,â€ 
hâˆ’1. In this case,
using Lemma 19 gives a more interpretable upper bound on the approximation error involves the difference
between the ratio of any linear dh covered on dD,â€ 
hâˆ’1 and the corresponding ratio over bd D,â€ 
hâˆ’1:
Îµapprox
h
â‰¤
max
dh=âŸ¨Âµâˆ—
hâˆ’1,Î¸hâŸ©:
dhâ‰¤Cx
hâˆ’1Ca
hâˆ’1dD,â€ 
hâˆ’1

dh
bd D,â€ 
hâˆ’1
âˆ’
dh
dD,â€ 
hâˆ’1

2,dD,â€ 
hâˆ’1
.
However such approximation error may be difï¬cult to control even with small data estimation error due to
sensitivity of the denominator (for example if âˆ¥bd D,â€ 
hâˆ’1 âˆ’dD,â€ 
hâˆ’1âˆ¥1 â‰¤Îµmle but they have disjoint support).
Barycentric spanner in denominator
To avoid the above support issue and control the approximation
error, we can instead consider a denominator function upon which dD,â€ 
hâˆ’1 is supported. This is satisï¬ed by
the barycentric spanner of the version space of the estimate bd D,â€ 
hâˆ’1,
Vh =
n
vh = âŸ¨Âµâˆ—
hâˆ’1, Î¸hâŸ©: âˆ¥vh âˆ’bd D,â€ 
hâˆ’1âˆ¥1 â‰¤Îµmle, Î¸h âˆˆRdo
,
noting that dD,â€ 
hâˆ’1 âˆˆVh with high probability due to the MLE guarantee. Then letting evh denote the spanner,
Lemma 15 guarantees that
dD,â€ 
hâˆ’1
evh
â‰¤d, and the approximation error of Wh(evh) can be controlled by the error
of MLE estimation, since for any dh â‰¤Cx
hâˆ’1Ca
hâˆ’1dD,â€ 
hâˆ’1 we have

dh
evh
âˆ’
dh
dD,â€ 
hâˆ’1

2
2,dD,â€ 
hâˆ’1
â‰¤(Cx
hâˆ’1Ca
hâˆ’1)2
Z dD,â€ 
hâˆ’1(x)
evh(x)
 
1 + dD,â€ 
hâˆ’1(x)
evh(x)
! evh(x) âˆ’dD,â€ 
hâˆ’1(x)
 (dx)
21

â‰¤2(Cx
hâˆ’1Ca
hâˆ’1d)2âˆ¥evh âˆ’dD,â€ 
hâˆ’1âˆ¥1
which implies that Îµapprox
h
â‰¤2Cx
hâˆ’1Ca
hâˆ’1dâˆšÎµmle by the deï¬nition of Vh. However, since Îµmle is O(nâˆ’1/2
mle ),
this results in a slow rate of 1/Îµ4 total sample complexity for ofï¬‚ine density estimation, and from a compu-
tational standpoint, introduces another barycentric spanner construction step in the algorithm which can be
expensive. The representation learning setting has the additional challenge that there will be approximation
error if the wrong representation bÂµhâˆ’1 âˆˆÎ¥hâˆ’1 is chosen for bd D,â€ 
hâˆ’1, since dD,â€ 
hâˆ’1 /âˆˆVh(bÂµh) (we extend the
deï¬nition to Vh(Âµhâˆ’1) =
n
vh = âŸ¨Âµhâˆ’1, Î¸hâŸ©: âˆ¥vh âˆ’bd D,â€ 
hâˆ’1âˆ¥1 â‰¤Îµmle, Î¸h âˆˆRdo
), which, as in the ï¬rst case
above, may be difï¬cult to bound.
Clipped function class with point estimate in denominator
Generalizing and improving upon the pre-
vious analyses, using a clipped version of the function class Wh(vh)
Wclip
h
(vh) =

wh = âŸ¨Âµâˆ—
hâˆ’1, Î¸hâŸ©âˆ§Cx
hâˆ’1Ca
hâˆ’1vh
vh
: Î¸h+1 âˆˆRd

will allow us to bound the approximation error for general denominator functions vh. For any dh such that
dh â‰¤Cx
hâˆ’1Ca
hâˆ’1dD,â€ 
hâˆ’1, we can approximate the ratio
dh
dD,â€ 
hâˆ’1
with
dhâˆ§Cx
hâˆ’1Ca
hâˆ’1vh
vh
âˆˆWclip
h
(vh), and separate
the approximation error into two terms, based on whether dD,â€ 
hâˆ’1 is covered by vh according to a threshold
C â‰¥1:

dh âˆ§Cx
hâˆ’1Ca
hâˆ’1vh
vh
âˆ’
dh
dD,â€ 
hâˆ’1

2
2,dD,â€ 
hâˆ’1
â‰¤

 
dh âˆ§Cx
hâˆ’1Ca
hâˆ’1vh
vh
âˆ’
dh
dD,â€ 
hâˆ’1
!
Â· 1
"
dD,â€ 
hâˆ’1(x)
vh(x)
â‰¤C
#
2
2,dD,â€ 
hâˆ’1
(â€œcoveredâ€)
+

 
dh âˆ§Cx
hâˆ’1Ca
hâˆ’1vh
vh
âˆ’
dh
dD,â€ 
hâˆ’1
!
Â· 1
"
dD,â€ 
hâˆ’1(x)
vh(x)
> C
#
2
2,dD,â€ 
hâˆ’1
(â€œnot coveredâ€)
Bounding the two terms individually, for the â€œcoveredâ€ term, we have
(â€œcoveredâ€) â‰¤
Z
x:
dD,â€ 
hâˆ’1(x)
vh(x) â‰¤C
dD,â€ 
hâˆ’1(x)
 
dh(x)
vh(x) âˆ’
dh(x)
dD,â€ 
hâˆ’1(x)
!2
(dx)
â‰¤(Cx
hâˆ’1Ca
hâˆ’1)2
Z
x:
dD,â€ 
hâˆ’1(x)
vh(x) â‰¤C
dD,â€ 
hâˆ’1(x)
vh(x)
(dD,â€ 
hâˆ’1(x) âˆ’vh(x))2
vh(x)
(dx)
â‰¤(Cx
hâˆ’1Ca
hâˆ’1)2C(1 + C)
Z
x:
dD,â€ 
hâˆ’1(x)
vh(x) â‰¤C
dD,â€ 
hâˆ’1(x) âˆ’vh(x)

â‰¤(Cx
hâˆ’1Ca
hâˆ’1)2C(1 + C)
dD,â€ 
hâˆ’1 âˆ’vh

1 .
For the â€œnot coveredâ€ term, noticing that both parenthesized ratios are bounded on [0, Cx
hâˆ’1Ca
hâˆ’1], we
have
(â€œnot coveredâ€) â‰¤(Cx
hâˆ’1Ca
hâˆ’1)2
Z
dD,â€ 
hâˆ’1(x) Â· 1
"
dD,â€ 
hâˆ’1(x)
vh(x)
> C
#
(dx)
22

â‰¤(Cx
hâˆ’1Ca
hâˆ’1)2

1 âˆ’1
C
âˆ’1 dD,â€ 
hâˆ’1 âˆ’vh

1 ,
where the second inequality is because

1 âˆ’1
C
 Z
x:
dD,â€ 
hâˆ’1(x)
vh(x) >C
dD,â€ 
hâˆ’1(x)(dx) <
Z
x:
dD,â€ 
hâˆ’1(x)
vh(x) >C
(dD,â€ 
hâˆ’1(x) âˆ’vh(x))(dx) â‰¤
dD,â€ 
hâˆ’1 âˆ’vh

1
since
dD,â€ 
hâˆ’1
C
> vh. Thus in total, we have
Îµapprox
h
â‰¤Cx
hâˆ’1Ca
hâˆ’1

C + C2 +
C
C âˆ’1
 rdD,â€ 
hâˆ’1 âˆ’vh

1.
The bound depends on how close the point estimate vh is to the true dD,â€ 
hâˆ’1, as well as the threshold C. In
the case where vh = bd D,â€ 
hâˆ’1 is the point estimate, we are now able to bound Îµapprox
h
â‰¤Cx
hâˆ’1Ca
hâˆ’1(C + C2 +
C
Câˆ’1)âˆšÎµmle, which results in a slower rate than our results in the main text. If vh = evh is the barycentric
spanner of the version space, then it sufï¬ces to set C = d, in which case only the â€œcoveredâ€ part of the error
is nonzero, and we recover the analysis in the previous paragraph.
In general, the best choice of threshold C is not obvious because dD,â€ 
hâˆ’1 is not known, and will trade off
between the two errors. When C is large, the â€œcoveredâ€ error will be large since it is proportional to C2,
while if C is too small (too close to 1), the â€œnot-coveredâ€ error will be large since it is proportional to
C
Câˆ’1.
Direct extraction of the estimate
Putting aside the discussion of point estimates in the denominator, we
now present an alternative to pointwise multiplication + linearization used to extract bdÏ€
h from Algorithm 1.
Instead, we can directly extract the numerator, which will already be a linear function (in Âµâˆ—), from weight
ratio and use it as the estimate for bdÏ€
h. The regression objective might then be (replacing line 5 in Algo-
rithm 1)
, bdÏ€
h = argmin
vhâˆˆVh
argmin
dhâˆˆFh(vh)
LDreg
hâˆ’1
 
dh
vh
,
bdÏ€
hâˆ’1 âˆ§Cx
hâˆ’1 bdD
hâˆ’1
bdD
hâˆ’1
, Ï€hâˆ’1 âˆ§Ca
hâˆ’1Ï€D
hâˆ’1
!
,
where the version space of denominator functions Vh is deï¬ned above, and Fh(vh) = {dh = âŸ¨Âµâˆ—
hâˆ’1, Î¸hâŸ©:
âˆ¥dh/vhâˆ¥âˆâ‰¤Cx
hâˆ’1Ca
hâˆ’1, Î¸h âˆˆRd} represents linear numerator functions covered by vh. It is necessary to
constrain the denominator functions to the version space in order to ensure that the numerator is close to the
true density, since regression only guarantees quality of estimated weight. For example, even if bwÏ€
h = wÏ€
h,
if the denominator function is c Â· dD,â€ 
hâˆ’1 then the numerator will be c Â· dÏ€
h, leading to large bdÏ€
h estimation error.
In terms of the analysis, this is quantiï¬ed as the error between the denominator and true dD,â€ 
hâˆ’1 in Eq. (9),
which is controlled by Îµmle when the denominator is constrained to the version space Vh, and will result in
the same guarantee as we have for Algorithm 1 and Algorithm 2 in the known feature setting. In the online
setting with known features, direct extraction has the advantage of no longer requiring the linearization step
(line 7 in Algorithm 2), though it is computationally more expensive because the function classes are jointly
optimized, and the version space must be maintained. This advantage is lost in the representation learning
setting because the estimates {bdÏ€
h}Ï€âˆˆÎ  must be jointly re-linearized with the same representation in order to
construct the policy cover (line 9 of Algorithm 4).
23

MLE instead of regression
An alternative to using regression to estimate the occupancy is instead using
MLE-type estimation. Along similar veins as the regression algorithm, (a clipped version of) the previous-
level estimate bdÏ€
hâˆ’1 must be reused to reweight the data distribution in order to estimate bdÏ€
h:
bdÏ€
h = argmin
fhâˆˆFh
1
n
n
X
i=1
bdÏ€
hâˆ’1 âˆ§Cx
hâˆ’1 bdD
hâˆ’1
bdD
hâˆ’1
Ï€hâˆ’1 âˆ§Ca
hâˆ’1Ï€D
hâˆ’1
Ï€D
hâˆ’1
log(fh).
where Fh is some linear function class. One possible advantage of such an approach is that a linear density
estimate can be directly learned, but establishing formal guarantees for an MLE-type algorithm remains
future work. After separating the missingness error âˆ¥dÏ€
h âˆ’d
Ï€
hâˆ¥1 in the same way as in Section 3, similar
methods as classical MLE analysis (Appendix H) might be used to control âˆ¥bdÏ€
h âˆ’d
Ï€
hâˆ¥1. The challenge is that
such MLE analyses require Fh to include only valid densities âˆˆâˆ†(X), but this is at odds with reweighted
MLE objectives such as the one above, since the weights
bdÏ€
hâˆ’1âˆ§Cx
hâˆ’1 bdD
hâˆ’1
bdD
hâˆ’1
generally will not induce a valid
density when multiplied with the data distribution.
D.4
Discussion of other approaches for controlling error exponentiation in the online setting
Barycentric spanner in regression target (without clipping)
In Section 4 we controlled the error ex-
ponentiation arising from having only approximately exploratory data by ï¬rst clipping the regression target
bdÏ€
h/bdD
h (since the MLE estimate bdD
h does not necessarily cover bdÏ€
h), then separating the error âˆ¥bdÏ€
h âˆ’dÏ€
hâˆ¥1 into
the â€œtwo-sided regression errorâ€ and â€œone-sided missingness errorâ€. It will be instructive to also look at an
alternative approach that avoids clipping and â€œpretendsâ€ that data is perfectly exploratory, which provides
interesting insights on the underlying issue and the delicacy of error propagation in our problem from a
different perspective.
The seemingly feasible solution is based on the observation that 1
d
Pd
i=1 bdÏ€h,i
h
, the barycentric spanner of
{bdÏ€
h}Ï€âˆˆÎ  in the denominator of Eq. (5), is a good approximation of dD
h . So instead of using MLE to estimate
dD
h = 1
d
Pd
i=1 dÏ€h,i
h
, we could simply use 1
d
Pd
i=1 bdÏ€h,i
h
, which will keep the regression target bounded in
Algorithm 1 without any clipping.
However, a closer look reveals that this only sweeps the issue under the rug. The problem does not
go away, and only appears in a different form: recall from Lemma 1 that the bound includes a term of
2d
bdD
h âˆ’dD
h

1, and when we use 1
d
Pd
i=1 bdÏ€h,i
h
to replace bdD
h , we obtain
bdD
h âˆ’dD
h

1 =

1
d
d
X
i=1
bdÏ€h,i
h
âˆ’1
d
d
X
i=1
dÏ€h,i
h

1
â‰¤max
Ï€âˆˆÎ  âˆ¥bdÏ€
h âˆ’dÏ€
hâˆ¥1
which, in addition to merging the two inductive chains, gives us âˆ¥bdÏ€
hâˆ’dÏ€
hâˆ¥1 â‰¤(1+d) maxÏ€âˆˆÎ  âˆ¥bdÏ€
hâˆ’dÏ€
hâˆ¥1+
. . ., resulting in O(d)H error. In other words, because the error of the denominator distribution depends on
the quality of regression, even with full coverage we will suffer the same error exponentiation issues.
Reachability-based approach
Error exponentiation can be avoided if a reachability assumption (Du et al.,
2019; Modi et al., 2021) is satisï¬ed in the underlying MDP. Formally, this assumption requires that there
exists a constant Î·min such that âˆ€h âˆˆ[H], z âˆˆZh+1 we have maxÏ€âˆˆÎ  PÏ€[zh+1 = z] â‰¥Î·min, where Zh+1
correspond to the latent states of the MDP. For example, in the case where Âµâˆ—
h is full-rank and composed of
simplex features, Zh+1 = {1, . . . , d} and Î¸h[i] directly corresponds to PÏ€[zh+1 = i] for i âˆˆ{1, . . . , d}. The
direct implication is that we can construct a fully exploratory policy cover that reaches all latent states (and
thus covers all Ï€ âˆˆÎ ) as long as we ï¬nd, for each latent state, the policy that reaches it with probability
24

at least Î·min. This policy can be found as long as bdÏ€
h is estimated sufï¬ciently well, which when backed up
implies the latent state visitation is estimated sufï¬ciently well.
Speciï¬cally, in the ofï¬‚ine module used in Algorithm 2, we can instead set nreg such that âˆ¥bdÏ€
h âˆ’dÏ€
hâˆ¥1 â‰¤
Ïƒmin(Âµâˆ—
hâˆ’1)Î·min/4 for all Ï€ âˆˆÎ , which implies that when backed up to latent states the error of estimation
is âˆ¥bÎ¸Ï€
h âˆ’Î¸Ï€
hâˆ¥âˆâ‰¤Î·min/4. Then the exploratory policy cover can be chosen as Î expl
h
= {Ï€h,i}d
i=1 where
for each i âˆˆ{1, . . . , d}, Ï€h,i is such that bÎ¸Ï€h,i
h
[i] â‰¥Î·min/4, which implies Î¸Ï€h,i
h
[i] â‰¥Î·min/2 with high
probability, and such a policy is guaranteed to exist from the reachability assumption. Since the policy
cover is fully exploratory, a single induction chain in the error analysis (instead of the two in Figure 1) will
sufï¬ce.
E
Off-policy occupancy estimation proofs (Section 3)
E.1
Discussion of clipping thresholds for Â¯dÏ€
As we have previously mentioned, the clipped occupancy d
Ï€
h depends on clipping thresholds {Cx
h} and
{Ca
h} that are hyperparameter inputs to the ofï¬‚ine estimation algorithm (Algorithm 1). To better understand
the effects of Cx
h, Ca
h on d
Ï€
h and downstream analysis, we highlight three properties below, which we have
written only for Cx
h (but that take analogous forms for Ca
h).
Importantly, property 3 shows that the missingness error âˆ¥d
Ï€
h âˆ’dÏ€
hâˆ¥1 is Lipschitz in the clipping thresh-
olds {Cx
h}, indicating that small changes in Cx
h will only lead to small changes in the missingness error, and
thus the result of Theorem 2. For practical purposes, this serves as a reassurance that, within some limit,
misspeciï¬cations of Cx
h, Ca
h in the algorithm do not have catastrophic consequences.
Proposition 5. For two sets of clipping thresholds {Cx
h}, {(Cx
h)â€²}, following Deï¬nition 1, for each h =
1, . . . , H let their corresponding clipped occupancies be deï¬ned recursively as
d
Ï€
h = PÏ€
hâˆ’1

d
Ï€
hâˆ’1 âˆ§Cx
hâˆ’1dD
hâˆ’1

(d
Ï€
h)â€² = PÏ€
hâˆ’1

(d
Ï€
hâˆ’1)â€² âˆ§(Cx
hâˆ’1)â€²dD
hâˆ’1

with d
Ï€
0 = (d
Ï€
0)â€² = d0. Then the following two properties hold for each h âˆˆ[H]:
1. (Monotonicity) d
Ï€
h â‰¤(d
Ï€
h)â€² if Cx
hâ€² â‰¤(Cx
hâ€²)â€² for all hâ€² < h. The relationship also holds in the other
direction, i.e., replacing â€œâ‰¤â€ with â€œ>â€.
2. (Clipped occupancy Lipschitz in thresholds) âˆ¥(d
Ï€
h)â€² âˆ’d
Ï€
hâˆ¥1 â‰¤P
hâ€²<h |(Cx
hâ€²)â€² âˆ’Cx
hâ€²|.
3. (Missingness error Lipschitz in thresholds)
âˆ¥dÏ€
h âˆ’(d
Ï€
h)â€²âˆ¥1 âˆ’âˆ¥dÏ€
h âˆ’d
Ï€
hâˆ¥1
 â‰¤P
hâ€²<h |(Cx
hâ€²)â€² âˆ’Cx
hâ€²|.
Proof. We prove these three claims one by one.
Proof of Claim 1
We will prove Claim 1 via induction. Suppose d
Ï€
hâ€²âˆ’1 â‰¤(d
Ï€
hâ€²âˆ’1)â€² for some hâ€² â‰¤h. This
holds for the base case hâ€² = 1 since d
Ï€
0 = (d
Ï€
0)â€². Then since Cx
hâ€²âˆ’1 â‰¤(Cx
hâ€²âˆ’1)â€²,
d
Ï€
hâ€² = PÏ€
hâ€²âˆ’1

d
Ï€
hâ€²âˆ’1 âˆ§Cx
hâ€²âˆ’1dD
hâ€²âˆ’1

â‰¤PÏ€
hâ€²âˆ’1

(d
Ï€
hâ€²âˆ’1)â€² âˆ§(Cx
hâ€²âˆ’1)â€²dD
hâ€²âˆ’1

= (d
Ï€
hâ€²)â€².
Then by induction we have that d
Ï€
h â‰¤(d
Ï€
h)â€².
25

Proof of Claim 2
For Claim 2, using Lemma 20, we have
âˆ¥(d
Ï€
h)â€² âˆ’d
Ï€
hâˆ¥1
â‰¤


d
Ï€
hâˆ’1 âˆ§Cx
hâˆ’1dD
hâˆ’1

âˆ’

(d
Ï€
hâˆ’1)â€² âˆ§(Cx
hâˆ’1)â€²dD
hâˆ’1

1
â‰¤


d
Ï€
hâˆ’1 âˆ§Cx
hâˆ’1dD
hâˆ’1

âˆ’

(d
Ï€
hâˆ’1)â€² âˆ§Cx
hâˆ’1dD
hâˆ’1

1 +


(d
Ï€
hâˆ’1)â€² âˆ§Cx
hâˆ’1dD
hâˆ’1

âˆ’

(d
Ï€
hâˆ’1)â€² âˆ§(Cx
hâˆ’1)â€²dD
hâˆ’1

1
â‰¤
d
Ï€
hâˆ’1 âˆ’(d
Ï€
hâˆ’1)â€²
1 +
Cx
hâˆ’1dD
hâˆ’1 âˆ’(Cx
hâˆ’1)â€²dD
hâˆ’1

1
=
d
Ï€
hâˆ’1 âˆ’(d
Ï€
hâˆ’1)â€²
1 +
Cx
hâˆ’1 âˆ’(Cx
hâˆ’1)â€² .
Unfolding this recursion from level h âˆ’1 through level 0 gives the result.
Proof of Claim 3
For Claim 3, we have
âˆ¥dÏ€
h âˆ’(d
Ï€
h)â€²âˆ¥1 âˆ’âˆ¥dÏ€
h âˆ’d
Ï€
hâˆ¥1
 =

Z
|dÏ€
h(x) âˆ’(d
Ï€
h)â€²(x)| âˆ’|dÏ€
h(x) âˆ’d
Ï€
h(x)|(dx)

â‰¤
Z |dÏ€
h(x) âˆ’(d
Ï€
h)â€²(x)| âˆ’|dÏ€
h(x) âˆ’d
Ï€
h(x)|
 (dx)
â‰¤
Z (d
Ï€
h)â€²(x) âˆ’d
Ï€
h(x)
 (dx)
(since ||x| âˆ’|y|| â‰¤|x âˆ’y|)
=
(d
Ï€
h)â€² âˆ’d
Ï€
h

1
Then applying Claim 2 gives the stated claim.
E.2
Proof of occupancy estimation
Proposition (Restatement of Proposition 2). We have the following properties for d
Ï€
h:
1. d
Ï€
h â‰¤dÏ€
h.
2. d
Ï€
h = dÏ€
h when data covers Ï€, i.e., âˆ€hâ€² < h we have dÏ€
hâ€² â‰¤Cx
hâ€²dD
hâ€² and Ï€hâ€² â‰¤Ca
hâ€²Ï€D
hâ€².
3. âˆ¥d
Ï€
h âˆ’dÏ€
hâˆ¥1 â‰¤âˆ¥d
Ï€
hâˆ’1 âˆ’dÏ€
hâˆ’1âˆ¥1 + âˆ¥d
Ï€
hâˆ’1 âˆ’d
Ï€
hâˆ’1 âˆ§Cx
hâˆ’1dD
hâˆ’1âˆ¥1 + âˆ¥PÏ€
hâˆ’1dÏ€
hâˆ’1 âˆ’PÏ€
hâˆ’1dÏ€
hâˆ’1âˆ¥1.
Proof. We prove these three claims one by one.
Proof of Claim 1
Firstly, we have d
Ï€
h = dÏ€
h = d0. Assuming the claim holds for hâ€² âˆ’1, then we have
d
Ï€
hâ€² = PÏ€
hâ€²âˆ’1(d
Ï€
hâ€²âˆ’1 âˆ§Cx
hâ€²âˆ’1dD
hâ€²âˆ’1) â‰¤PÏ€
hâ€²âˆ’1(d
Ï€
hâ€²âˆ’1 âˆ§Cx
hâ€²âˆ’1dD
hâ€²âˆ’1) â‰¤PÏ€
hâ€²âˆ’1(dÏ€
hâ€²âˆ’1 âˆ§Cx
hâ€²âˆ’1dD
hâ€²âˆ’1) â‰¤
PÏ€
hâ€²âˆ’1dÏ€
hâ€²âˆ’1 = dÏ€
hâ€². By induction, we complete the proof.
Proof of Claim 2
It is easy to see that dÏ€
hâ€² â‰¤Cx
hâ€²dD
hâ€² together with Claim 1 implies d
Ï€
hâ€² â‰¤Cx
hâ€²dD
hâ€², thus
âˆ¥d
Ï€
hâ€²âˆ’d
Ï€
hâ€²âˆ§Cx
hâ€²dD
hâ€²âˆ¥1 = 0. In addition, Ï€hâ€² â‰¤Ca
hâ€²Ï€D
hâ€² gives us Ï€hâ€² = Ï€hâ€², therefore
PÏ€
hâ€²âˆ’1dÏ€
hâ€²âˆ’1 âˆ’PÏ€
hâ€²âˆ’1dÏ€
hâ€²âˆ’1

1 =
0. Now we can prove Claim 2 inductively. For hâ€² = 0, we know the claim holds since d
Ï€
0 = dÏ€
0 = d0. As-
suming the claim holds for hâ€² âˆ’1, by Claim 3 we have that
0 â‰¤âˆ¥d
Ï€
hâ€²âˆ’dÏ€
hâ€²âˆ¥1 â‰¤âˆ¥d
Ï€
hâ€²âˆ’1âˆ’dÏ€
hâ€²âˆ’1âˆ¥1+âˆ¥d
Ï€
hâ€²âˆ’1âˆ’d
Ï€
hâ€²âˆ’1âˆ§Cx
hâ€²âˆ’1dD
hâ€²âˆ’1âˆ¥1+âˆ¥PÏ€
hâ€²âˆ’1dÏ€
hâ€²âˆ’1âˆ’PÏ€
hâ€²âˆ’1dÏ€
hâ€²âˆ’1âˆ¥1 = 0.
This means the claim holds for hâ€². By induction, we complete the proof.
26

Proof of Claim 3
For the third part, we have the following decomposition
âˆ¥d
Ï€
h âˆ’dÏ€
hâˆ¥1 =
PÏ€
hâˆ’1

d
Ï€
hâˆ’1 âˆ§Cx
hâˆ’1dD
hâˆ’1

âˆ’PÏ€
hdÏ€
hâˆ’1

1
â‰¤
PÏ€
hâˆ’1

d
Ï€
hâˆ’1 âˆ§Cx
hâˆ’1dD
hâˆ’1

âˆ’PÏ€
hâˆ’1dÏ€
hâˆ’1

1 +
PÏ€
hâˆ’1dÏ€
hâˆ’1 âˆ’PÏ€
hdÏ€
hâˆ’1

1
â‰¤
d
Ï€
hâˆ’1 âˆ§Cx
hâˆ’1dD
hâˆ’1 âˆ’dÏ€
hâˆ’1

1 +
PÏ€
hâˆ’1dÏ€
hâˆ’1 âˆ’PÏ€
hdÏ€
hâˆ’1

1
(Lemma 20)
â‰¤
d
Ï€
hâˆ’1 âˆ§Cx
hâˆ’1dD
hâˆ’1 âˆ’d
Ï€
hâˆ’1

1 +
d
Ï€
hâˆ’1 âˆ’dÏ€
hâˆ’1

1 +
PÏ€
hâˆ’1dÏ€
hâˆ’1 âˆ’PÏ€
hdÏ€
hâˆ’1

1 .
Lemma (Restatement of Lemma 1). For every h âˆˆ[H], the error between estimates bdÏ€
h from Algorithm 1
and the clipped target d
Ï€
h is decomposed recursively as
bdÏ€
h âˆ’d
Ï€
h

1 â‰¤
bdÏ€
hâˆ’1 âˆ’d
Ï€
hâˆ’1

1 + 2Cx
hâˆ’1
bdD
hâˆ’1 âˆ’dD
hâˆ’1

1 + Cx
hâˆ’1Ca
hâˆ’1
bd D,â€ 
hâˆ’1 âˆ’dD,â€ 
hâˆ’1

1
+
âˆš
2
 bwÏ€
h âˆ’EÏ€
hâˆ’1
 
dD
hâˆ’1
bdÏ€
hâˆ’1 âˆ§Cx
hâˆ’1 bdD
hâˆ’1
bdD
hâˆ’1
!
2,dD,â€ 
hâˆ’1
,
where (EÏ€
hdh) := (PÏ€
hdh)/dD,â€ 
h
.
Proof. We start by separating out the recursive term
bdÏ€
h âˆ’d
Ï€
h

1 =
bdÏ€
h âˆ’PÏ€
hâˆ’1

d
Ï€
hâˆ’1 âˆ§Cx
hâˆ’1dD
hâˆ’1

1
â‰¤
bdÏ€
h âˆ’PÏ€
hâˆ’1

bdÏ€
hâˆ’1 âˆ§Cx
hâˆ’1 bdD
hâˆ’1

1 +
PÏ€
hâˆ’1

bdÏ€
hâˆ’1 âˆ§Cx
hâˆ’1 bdD
hâˆ’1

âˆ’PÏ€
hâˆ’1

d
Ï€
hâˆ’1 âˆ§Cx
hâˆ’1 bdD
hâˆ’1

1
+
PÏ€
hâˆ’1

d
Ï€
hâˆ’1 âˆ§Cx
hâˆ’1 bdD
hâˆ’1

âˆ’PÏ€
hâˆ’1

d
Ï€
hâˆ’1 âˆ§Cx
hâˆ’1dD
hâˆ’1

1
â‰¤
bdÏ€
h âˆ’PÏ€
hâˆ’1

bdÏ€
hâˆ’1 âˆ§Cx
hâˆ’1 bdD
hâˆ’1

1 +
bdÏ€
hâˆ’1 âˆ§Cx
hâˆ’1 bdD
hâˆ’1 âˆ’d
Ï€
hâˆ’1 âˆ§Cx
hâˆ’1 bdD
hâˆ’1

1
+
d
Ï€
hâˆ’1 âˆ§Cx
hâˆ’1 bdD
hâˆ’1 âˆ’d
Ï€
hâˆ’1 âˆ§Cx
hâˆ’1dD
hâˆ’1

1
â‰¤
bdÏ€
h âˆ’PÏ€
hâˆ’1

bdÏ€
hâˆ’1 âˆ§Cx
hâˆ’1 bdD
hâˆ’1

1 +
bdÏ€
hâˆ’1 âˆ’d
Ï€
hâˆ’1

1 + Cx
hâˆ’1
bdD
hâˆ’1 âˆ’dD
hâˆ’1

1 .
(7)
Here, we apply Lemma 20 in the second inequality. The last inequality is due to | min(x, y) âˆ’min(x, z)| â‰¤
|y âˆ’z| for x, y, z âˆˆR.
Now, we consider the ï¬rst term in Eq. (7) and get
bdÏ€
h âˆ’PÏ€
hâˆ’1

bdÏ€
hâˆ’1 âˆ§Cx
hâˆ’1 bdD
hâˆ’1

1
â‰¤

bdÏ€
h âˆ’PÏ€
hâˆ’1
 bdÏ€
hâˆ’1 âˆ§Cx
hâˆ’1 bdD
hâˆ’1
bdD
hâˆ’1
dD
hâˆ’1
!
1
+
PÏ€
hâˆ’1
 bdÏ€
hâˆ’1 âˆ§Cx
hâˆ’1 bdD
hâˆ’1
bdD
hâˆ’1
dD
hâˆ’1
!
âˆ’PÏ€
hâˆ’1
 bdÏ€
hâˆ’1 âˆ§Cx
hâˆ’1 bdD
hâˆ’1
bdD
hâˆ’1
bdD
hâˆ’1
!
1
â‰¤

bdÏ€
h âˆ’PÏ€
hâˆ’1
 bdÏ€
hâˆ’1 âˆ§Cx
hâˆ’1 bdD
hâˆ’1
bdD
hâˆ’1
dD
hâˆ’1
!
1
+ Cx
hâˆ’1
dD
hâˆ’1 âˆ’bdD
hâˆ’1

1 .
(8)
In the last inequality, we notice

bdÏ€
hâˆ’1âˆ§Cx
hâˆ’1 bdD
hâˆ’1
bdD
hâˆ’1

âˆ
â‰¤Cx
hâˆ’1 by our convention 0
0 = 0 and apply Lemma 20
again.
27

Let ewhâˆ’1 :=
bdÏ€
hâˆ’1âˆ§Cx
hâˆ’1 bdD
hâˆ’1
bdD
hâˆ’1
for short. Since âˆ¥ewhâˆ’1âˆ¥âˆâ‰¤Cx
hâˆ’1, Lemma 19 guarantees (PÏ€
hâˆ’1(dD
hâˆ’1 ewhâˆ’1))
dD,â€ 
hâˆ’1
â‰¤
Cx
hâˆ’1Ca
hâˆ’1, thus the ratio is well-deï¬ned. Then we can further upper-bound the ï¬rst term in Eq. (8) as
bdÏ€
h âˆ’PÏ€
hâˆ’1
 dD
hâˆ’1 ewhâˆ’1

1 =
 bwÏ€
h bd D,â€ 
hâˆ’1 âˆ’PÏ€
hâˆ’1
 dD
hâˆ’1 ewhâˆ’1

dD,â€ 
hâˆ’1
dD,â€ 
hâˆ’1

1
â‰¤
 bwÏ€
h bd D,â€ 
hâˆ’1 âˆ’bwÏ€
h dD,â€ 
hâˆ’1

1 +
 bwÏ€
h dD,â€ 
hâˆ’1 âˆ’PÏ€
hâˆ’1
 dD
hâˆ’1 ewhâˆ’1

dD,â€ 
hâˆ’1
dD,â€ 
hâˆ’1

1
=
 bwÏ€
h bd D,â€ 
hâˆ’1 âˆ’bwÏ€
h dD,â€ 
hâˆ’1

1 +
 bwÏ€
h âˆ’PÏ€
hâˆ’1
 dD
hâˆ’1 ewhâˆ’1

dD,â€ 
hâˆ’1

1,dD,â€ 
hâˆ’1
â‰¤âˆ¥bwÏ€
hâˆ¥âˆ
bd D,â€ 
hâˆ’1 âˆ’dD,â€ 
hâˆ’1

1 +
 bwÏ€
h âˆ’PÏ€
hâˆ’1
 dD
hâˆ’1 ewhâˆ’1

dD,â€ 
hâˆ’1

1,dD,â€ 
hâˆ’1
â‰¤Cx
hCa
h
bd D,â€ 
hâˆ’1 âˆ’dD,â€ 
hâˆ’1

1 +
 bwÏ€
h âˆ’PÏ€
hâˆ’1
 dD
hâˆ’1 ewhâˆ’1

dD,â€ 
hâˆ’1

2,dD,â€ 
hâˆ’1
.
(9)
Combining Eq. (7), Eq. (8), and Eq. (9) and noticing the deï¬nition of EÏ€
h and ewhâˆ’1 completes the
proof.
Theorem (Restatement of Theorem 2). Fix Î´ âˆˆ(0, 1). Suppose Assumption 1 and Assumption 2 hold, and
Âµâˆ—is known. Then, given an evaluation policy Ï€, by setting
nmle = ËœO
ï£«
ï£­d
ï£«
ï£­X
hâˆˆ[H]
Cx
hCa
h
ï£¶
ï£¸
2
log(1/Î´)/Îµ2
ï£¶
ï£¸and nreg = ËœO
ï£«
ï£­d
ï£«
ï£­X
hâˆˆ[H]
Cx
hCa
h
ï£¶
ï£¸
2
log(1/Î´)/Îµ2
ï£¶
ï£¸,
with probability at least 1 âˆ’Î´, FORC (Algorithm 1) returns state occupancy estimates {bdÏ€
h}Hâˆ’1
h=0 satisfying
bdÏ€
h âˆ’d
Ï€
h

1 â‰¤Îµ, âˆ€h âˆˆ[H].
The total number of episodes required by the algorithm is
ËœO
ï£«
ï£­dH
ï£«
ï£­X
hâˆˆ[H]
Cx
hCa
h
ï£¶
ï£¸
2
log(1/Î´)/Îµ2
ï£¶
ï£¸.
Proof. We ï¬rst make two claims on MLE estimation and error propagation.
Claim 1
Our estimated data distributions satisfy that with probability 1 âˆ’Î´/2, for any h âˆˆ[H]
bdD
h âˆ’dD
h

1 â‰¤Îµmle and
bd D,â€ 
h
âˆ’dD,â€ 
h

1 â‰¤Îµmle,
(10)
where
Îµmle := 6
s
d log(16HBÂµnmle/Î´)
nmle
.
28

Claim 2
Under the high-probability event that Eq. (10) holds, we further have that with probability at least
1 âˆ’Î´/2, for any 1 â‰¤h â‰¤H,
bdÏ€
h âˆ’d
Ï€
h

1 â‰¤
bdÏ€
hâˆ’1 âˆ’d
Ï€
hâˆ’1

1 + 3Cx
hâˆ’1Ca
hâˆ’1Îµmle +
âˆš
2Îµreg,hâˆ’1,
(11)
where
Îµreg,hâˆ’1 :=
s
221184d(Cx
hâˆ’1Ca
hâˆ’1)2 log (2Hnreg/Î´)
nreg
.
Now we establish the ï¬nal error bound with these two claims. Notice that the total failure probability is
less than Î´. Unfolding Eq. (11) from hâ€² = h to hâ€² = 1 and noticing that bdÏ€
0 = d
Ï€
0 = d0 yields that for any
h âˆˆ[H]
bdÏ€
h âˆ’d
Ï€
h

1 â‰¤
hâˆ’1
X
hâ€²=0

3Cx
hâ€²Ca
hâ€²Îµmle +
âˆš
2Îµreg,hâ€²

.
(12)
Substituting in the expressions for Îµmle and Îµreg,, we have
bdÏ€
h âˆ’d
Ï€
h

1 â‰¤
hâˆ’1
X
hâ€²=0
ï£«
ï£­18Cx
hâ€²Ca
hâ€²
s
d log(16HBÂµnmle/Î´)
nmle
+ 666Cx
hâ€²Ca
hâ€²
s
d log (2Hnreg/Î´)
nreg
ï£¶
ï£¸.
(13)
It is easy to see that if we set
nmle = ËœO
ï£«
ï£­d
ï£«
ï£­X
hâˆˆ[H]
Cx
hCa
h
ï£¶
ï£¸
2
log(1/Î´)/Îµ2
ï£¶
ï£¸and nreg = ËœO
ï£«
ï£­d
ï£«
ï£­X
hâˆˆ[H]
Cx
hCa
h
ï£¶
ï£¸
2
log(1/Î´)/Îµ2
ï£¶
ï£¸,
then we have
bdÏ€
h âˆ’d
Ï€
h

1 â‰¤Îµ, âˆ€h âˆˆ[H].
In the following, we provide the proof of these two claims respectively.
Proof of Claim 1
We start with a ï¬xed h âˆˆ[H] and bounding âˆ¥bdD
h âˆ’dD
h âˆ¥1, where we recall that bdD
h is
the MLE solution in Eq. (2). By Lemma 22, we know that function class Fh has an â„“1 optimistic cover
with scale 1/nmle of size (2âŒˆBÂµnmleâŒ‰)d. It is easy to see that the true marginal distribution dD
h âˆˆFh
from Lemma 17 and any dh âˆˆFh is a valid probability distribution over X. From Assumption 2, we
know that once conditioned on prior dataset D0:hâˆ’1, the current dataset Dmle
h
is drawn i.i.d. from the ï¬xed
distribution denoted as dD
h . Thus, Lemma 12 tells us that when conditioned on D0:hâˆ’1, with probability at
least 1 âˆ’Î´/(4H)
âˆ¥bdD
h âˆ’dD
h âˆ¥1 â‰¤
1
nmle
+
s
12 log(4H (2âŒˆBÂµnmleâŒ‰)d /Î´)
nmle
+
6
nmle
(14)
â‰¤
1
nmle
+
s
12d log(16HBÂµnmle/Î´)
nmle
+
6
nmle
â‰¤6
s
d log(16HBÂµnmle/Î´)
nmle
= Îµmle.
(15)
29

Since Eq. (14) holds for any such ï¬xed D0:hâˆ’1, applying the law of total expectation gives us this that
Eq. (14) holds with probability 1 âˆ’Î´/(4H) without conditioning on D0:hâˆ’1.
Similarly, with probability at least 1 âˆ’Î´/(4H), for the MLE solution bd D,â€ 
h
we have âˆ¥bd D,â€ 
h
âˆ’dD,â€ 
h
âˆ¥1 â‰¤
Îµmle. Union bounding these two high-probability events and further union bounding over h âˆˆ[H] gives us
that Eq. (10) holds with probability 1 âˆ’Î´/2.
Proof of Claim 2
Notice that the proof in this part is under the high-probability event that Eq. (10) holds.
We consider a ï¬xed h âˆˆ[H]. From Lemma 1, we have the error propagation result that
bdÏ€
h âˆ’d
Ï€
h

1 â‰¤
bdÏ€
hâˆ’1 âˆ’d
Ï€
hâˆ’1

1 + 2Cx
hâˆ’1
bdD
hâˆ’1 âˆ’dD
hâˆ’1

1 + Cx
hâˆ’1Ca
hâˆ’1
bd D,â€ 
hâˆ’1 âˆ’dD,â€ 
hâˆ’1

1
+
âˆš
2
 bwÏ€
h âˆ’PÏ€
hâˆ’1
 dD
hâˆ’1 ewhâˆ’1

dD,â€ 
hâˆ’1

2,dD,â€ 
hâˆ’1
,
(16)
where ewhâˆ’1 :=
bdÏ€
hâˆ’1âˆ§Cx
hâˆ’1 bdD
hâˆ’1
bdD
hâˆ’1
.
Since bwÏ€
h âˆˆWh, we have âˆ¥bwÏ€
hâˆ¥âˆâ‰¤Cx
hCa
h. The last term on RHS isolates the ï¬nite-sample error
of regression, involving the difference between the empirical minimizer bwÏ€
h and the population minimizer
PÏ€
hâˆ’1(dD
hâˆ’1 ewhâˆ’1)
dD,â€ 
hâˆ’1
of the regression objective. To bound this error, we apply Lemma 13 and Lemma 14, which
give us that, with probability at least 1 âˆ’Î´/(2H),
 bwÏ€
h âˆ’PÏ€
hâˆ’1
 dD
hâˆ’1 ewhâˆ’1

dD,â€ 
hâˆ’1

2
2,dD,â€ 
hâˆ’1
= E
h
LDreg
hâˆ’1 ( bwÏ€
h, ewhâˆ’1, Ï€)
i
âˆ’E
"
LDreg
hâˆ’1
 
PÏ€
hâˆ’1
 dD
hâˆ’1 ewhâˆ’1

dD,â€ 
hâˆ’1
, ewhâˆ’1, Ï€
!#
â‰¤2
 
LDreg
hâˆ’1 ( bwÏ€
h, ewhâˆ’1, Ï€) âˆ’LDreg
hâˆ’1
 
PÏ€
hâˆ’1
 dD
hâˆ’1 ewhâˆ’1

dD,â€ 
hâˆ’1
, ewhâˆ’1, Ï€
!!
+ 2Îµ2
reg,hâˆ’1
(17)
where
Îµreg,hâˆ’1 :=
s
221184 Â· d(Cx
hâˆ’1Ca
hâˆ’1)2 log (2Hnreg/Î´)
nreg
The ï¬rst term in Eq. (17) compares the empirical regression loss of the empirical minimizer bwÏ€
h against the
population solution. In order to show that this is â‰¤0, we ï¬rst need to check that
PÏ€
hâˆ’1(dD
hâˆ’1 ewhâˆ’1)
dD,â€ 
hâˆ’1
âˆˆWh.
As we have previously seen, we have
PÏ€
hâˆ’1(dD
hâˆ’1 ewhâˆ’1)
dD,â€ 
hâˆ’1
â‰¤Cx
hâˆ’1Ca
hâˆ’1 from Lemma 19, thus satisfying the
norm constraints of Wh. Further, Lemma 16 guarantees that both the numerator and denominator are linear
functions of Âµâˆ—
hâˆ’1, i.e., PÏ€
hâˆ’1
 dD
hâˆ’1 ewhâˆ’1

= âŸ¨Âµâˆ—
hâˆ’1, Î¸up
h âŸ©and dD,â€ 
hâˆ’1 = âŸ¨Âµâˆ—
hâˆ’1, Î¸down
h
âŸ©for some Î¸up
h , Î¸down
h
âˆˆ
Rd. Then since bwÏ€
h minimzes the empirical regression loss Eq. (3), we have
LDreg
hâˆ’1
 bwÏ€
hâˆ’1, ewhâˆ’1, Ï€

âˆ’LDreg
hâˆ’1
 
PÏ€
hâˆ’1
 dD
hâˆ’1 ewhâˆ’1

dD,â€ 
hâˆ’1
, ewhâˆ’1, Ï€
!
â‰¤0.
(18)
Combining Eq. (16), Eq. (17), Eq. (18) with the MLE bound of Eq. (10), with probability at least
1 âˆ’Î´/(2H) we have
âˆ¥bdÏ€
h âˆ’d
Ï€
hâˆ¥1 â‰¤âˆ¥bdÏ€
hâˆ’1 âˆ’d
Ï€
hâˆ’1âˆ¥1 + 2Cx
hâˆ’1Îµmle + Cx
hâˆ’1Ca
hâˆ’1Îµmle +
âˆš
2Îµreg,hâˆ’1
30

â‰¤âˆ¥bdÏ€
hâˆ’1 âˆ’d
Ï€
hâˆ’1âˆ¥1 + 3Cx
hâˆ’1Ca
hâˆ’1Îµmle +
âˆš
2Îµreg,hâˆ’1.
Finally, union bounding over h âˆˆ[H], plugging in the deï¬nition of Îµmle, and rearranging gives that
Eq. (11) holds with probability at least 1 âˆ’Î´/2.
E.3
Proof of ofï¬‚ine policy optimization
Theorem (Restatement of Theorem 3). Fix Î´ âˆˆ(0, 1) and suppose Assumption 1 and Assumption 2 hold.
Given a policy class Î , let {bdÏ€
h}hâˆˆ[H],Ï€âˆˆÎ  be the output of running Algorithm 1. Then with probability at
least 1 âˆ’Î´, for any deterministic reward function R and policy selected as bÏ€R = argmaxÏ€âˆˆÎ  bvÏ€
R, we have
vbÏ€R
R â‰¥argmax
Ï€âˆˆÎ 
vÏ€
R âˆ’Îµ,
where bvÏ€
R := PHâˆ’1
h=0
RR bdÏ€
h(xh)R(xh, ah)Ï€(ah|xh)(dxh)(dah) and vR is deï¬ned similarly for {d
Ï€
h}. The
total number of episodes required by the algorithm is
ËœO
ï£«
ï£­dH3
ï£«
ï£­X
hâˆˆ[H]
Cx
hCa
h
ï£¶
ï£¸
2
log(|Î |/Î´)/Îµ2
ï£¶
ï£¸.
Additionally, deï¬ne the set of policies fully covered by the data to be
Î covered =
n
Ï€ âˆˆÎ  : dÏ€
h = d
Ï€
h, âˆ€h âˆˆ[H]
o
.
Then with the same total number of episodes required by the algorithm, for any reward function R and
policy selected as bÏ€R = argmaxÏ€âˆˆÎ covered bvÏ€
R, with probability at least 1 âˆ’Î´, we have
vbÏ€R
R â‰¥argmax
Ï€âˆˆÎ covered vÏ€
R âˆ’Îµ.
Proof. Firstly, Theorem 2 states that, with probability at least 1âˆ’Î´/|Î |, ËœO

dH3 P
hâˆˆ[H] Cx
hCa
h
2
log(|Î |/Î´)/Îµ2

samples are sufï¬cient for learning {bdÏ€
h} such that âˆ¥bdÏ€
h âˆ’d
Ï€
hâˆ¥1 â‰¤
Îµ
2H for all h âˆˆ[H] and each Ï€ âˆˆÎ . Taking
a union bound over Ï€ âˆˆÎ , with probability at least 1 âˆ’Î´, we have that for all h âˆˆ[H], Ï€ âˆˆÎ ,
âˆ¥bdÏ€
h âˆ’d
Ï€
hâˆ¥1 â‰¤
Îµ
2H .
Then since the R is bounded on [0, 1], for any Ï€ âˆˆÎ  we have
|bvÏ€
R âˆ’vÏ€
R| =
Hâˆ’1
X
h=0
ZZ
(bdÏ€
h(xh) âˆ’d
Ï€
h(xh))R(xh, ah)Ï€(ah|xh)(dxh)(dah)
â‰¤
Hâˆ’1
X
h=0
Z
|bdÏ€
h(xh) âˆ’d
Ï€
h(xh)|
Z
Ï€(ah|xh)(dah)

(dxh)
=
Hâˆ’1
X
h=0
âˆ¥bdÏ€
h âˆ’d
Ï€
hâˆ¥1 â‰¤Îµ/2.
Denote Ï€âˆ—
R = argmaxÏ€âˆˆÎ  vÏ€
R, and recall that we pick bÏ€R = argmaxÏ€âˆˆÎ  bvÏ€
R. Then
vbÏ€R
R âˆ’max
Ï€âˆˆÎ  vÏ€
R = vbÏ€R
R âˆ’v
Ï€âˆ—
R
R â‰¥vbÏ€R
R âˆ’v
Ï€âˆ—
R
R = vbÏ€R
R âˆ’bvbÏ€R
R + bvbÏ€R
R âˆ’bv
Ï€âˆ—
R
R + bv
Ï€âˆ—
R
R âˆ’v
Ï€âˆ—
R
R â‰¥âˆ’Îµ,
31

where the ï¬rst inequality follows from the fact that dÏ€
h â‰¥d
Ï€
h, thus vÏ€
R â‰¥vÏ€
R. The second inequality results
from the fact that bvbÏ€R
R â‰¥bv
Ï€âˆ—
R
R and |bvÏ€
R âˆ’vÏ€
R| â‰¤Îµ/2 for all Ï€ âˆˆÎ .
The result for Î covered is a straightforward from the observation that for each Ï€ âˆˆÎ covered, we have
dÏ€
h = d
Ï€
h for all h âˆˆ[H] and vÏ€
R = vÏ€
R.
F
Online policy cover construction proofs (Section 4)
F.1
Proof of occupancy estimation
Lemma (Restatement of Lemma 4). For any h âˆˆ[H] and Ï€ âˆˆÎ  in Algorithm 2,
d
Ï€
h âˆ’dÏ€
h

1 â‰¤
d
Ï€
hâˆ’1 âˆ’dÏ€
hâˆ’1

1 + 4d max
Ï€â€²âˆˆÎ 
bdÏ€â€²
hâˆ’1 âˆ’d
Ï€â€²
hâˆ’1

1 .
Proof. Firstly, from the third claim of Proposition 2, we have that for any h âˆˆ[H], Ï€ âˆˆÎ 
âˆ¥d
Ï€
h âˆ’dÏ€
hâˆ¥1 â‰¤âˆ¥d
Ï€
hâˆ’1 âˆ’dÏ€
hâˆ’1âˆ¥1 + âˆ¥d
Ï€
hâˆ’1 âˆ’d
Ï€
hâˆ’1 âˆ§Cx
hâˆ’1dD
hâˆ’1âˆ¥1 + âˆ¥PÏ€
hâˆ’1dÏ€
hâˆ’1 âˆ’PÏ€
hâˆ’1dÏ€
hâˆ’1âˆ¥1.
(19)
Now we further simplify the latter two error terms on the RHS of Eq. (19) by noticing that Cx
h = d and
Ca
h = K for all h âˆˆ[H]. For the last term, Ï€D = unif(A) gives us
Ï€(ahâˆ’1|xhâˆ’1) = min{Ï€(ahâˆ’1|xhâˆ’1), Ca
hâˆ’1Ï€D(ahâˆ’1|xhâˆ’1)} = min{Ï€(ahâˆ’1|xhâˆ’1), 1)} = Ï€(ahâˆ’1|xhâˆ’1)
and thus
PÏ€
hâˆ’1dÏ€
hâˆ’1 âˆ’PÏ€
hâˆ’1dÏ€
hâˆ’1

1 = 0. For the middle term, we expand the expression as
d
Ï€
hâˆ’1 âˆ’d
Ï€
hâˆ’1 âˆ§ddD
hâˆ’1

1 =
Z
d
Ï€
hâˆ’1(xhâˆ’1) âˆ’

d
Ï€
hâˆ’1 âˆ§ddD
hâˆ’1

(xhâˆ’1)(dxhâˆ’1).
Consider a ï¬xed xhâˆ’1 âˆˆX. Note that d
Ï€
hâˆ’1(xhâˆ’1) âˆ’

d
Ï€
hâˆ’1 âˆ§ddD
hâˆ’1

(xhâˆ’1) is nonzero only if
ddD
hâˆ’1(xhâˆ’1) < d
Ï€
hâˆ’1(xhâˆ’1), for which we have
d
Ï€
hâˆ’1(xhâˆ’1) âˆ’

d
Ï€
hâˆ’1 âˆ§ddD
hâˆ’1

(xhâˆ’1) = d
Ï€
hâˆ’1(xhâˆ’1) âˆ’ddD
hâˆ’1(xhâˆ’1)
â‰¤bdÏ€
hâˆ’1(xhâˆ’1) âˆ’ddD
hâˆ’1(xhâˆ’1) +
d
Ï€
hâˆ’1(xhâˆ’1) âˆ’bdÏ€
hâˆ’1(xhâˆ’1)
 .
To bound bdÏ€
hâˆ’1(xhâˆ’1) âˆ’ddD
hâˆ’1(xhâˆ’1), we have
bdÏ€
hâˆ’1(xhâˆ’1) âˆ’ddD
hâˆ’1(xhâˆ’1)
â‰¤edÏ€
hâˆ’1(xhâˆ’1) âˆ’ddD
hâˆ’1(xhâˆ’1) +
bdÏ€
hâˆ’1(xhâˆ’1) âˆ’edÏ€
hâˆ’1(xhâˆ’1)

â‰¤
d
X
i=1
edÏ€hâˆ’1,i
hâˆ’1
(xhâˆ’1)
 âˆ’ddD
hâˆ’1(xhâˆ’1) +
bdÏ€
hâˆ’1(xhâˆ’1) âˆ’edÏ€
hâˆ’1(xhâˆ’1)

â‰¤
d
X
i=1
bdÏ€hâˆ’1,i
hâˆ’1
(xhâˆ’1)
 âˆ’ddD
hâˆ’1(xhâˆ’1) + (d + 1) max
Ï€â€²âˆˆÎ 
bdÏ€â€²
hâˆ’1(xhâˆ’1) âˆ’edÏ€â€²
hâˆ’1(xhâˆ’1)

â‰¤
d
X
i=1
d
Ï€hâˆ’1,i
hâˆ’1
(xhâˆ’1)
 âˆ’ddD
hâˆ’1(xhâˆ’1) + (d + 1) max
Ï€â€²âˆˆÎ 
bdÏ€â€²
hâˆ’1(xhâˆ’1) âˆ’edÏ€â€²
hâˆ’1(xhâˆ’1)

+ d max
Ï€â€²âˆˆÎ 
bdÏ€â€²
hâˆ’1(xhâˆ’1) âˆ’d
Ï€â€²
hâˆ’1(xhâˆ’1)

32

=
d
X
i=1
d
Ï€hâˆ’1,i
hâˆ’1
(xhâˆ’1) âˆ’ddD
hâˆ’1(xhâˆ’1) + (d + 1) max
Ï€â€²âˆˆÎ 
bdÏ€â€²
hâˆ’1(xhâˆ’1) âˆ’edÏ€â€²
hâˆ’1(xhâˆ’1)

+ d max
Ï€â€²âˆˆÎ 
bdÏ€â€²
hâˆ’1(xhâˆ’1) âˆ’d
Ï€â€²
hâˆ’1(xhâˆ’1)

â‰¤
d
X
i=1
dÏ€hâˆ’1,i
hâˆ’1
(xhâˆ’1) âˆ’ddD
hâˆ’1(xhâˆ’1) + (d + 1) max
Ï€â€²âˆˆÎ 
bdÏ€â€²
hâˆ’1(xhâˆ’1) âˆ’edÏ€â€²
hâˆ’1(xhâˆ’1)

+ d max
Ï€â€²âˆˆÎ 
bdÏ€â€²
hâˆ’1(xhâˆ’1) âˆ’d
Ï€â€²
hâˆ’1(xhâˆ’1)

= (d + 1) max
Ï€â€²âˆˆÎ 
bdÏ€â€²
hâˆ’1(xhâˆ’1) âˆ’edÏ€â€²
hâˆ’1(xhâˆ’1)
 + d max
Ï€â€²âˆˆÎ 
bdÏ€â€²
hâˆ’1(xhâˆ’1) âˆ’d
Ï€â€²
hâˆ’1(xhâˆ’1)
 .
In the second inequality, we use that Î expl
hâˆ’1 = {Ï€hâˆ’1,1, . . . , Ï€hâˆ’1,d} are the policies corresponding to the
barycentric spanner, which Lemma 15 guarantees to be of cardinality no larger than d. The ï¬rst equality is
because d
Ï€
hâˆ’1(xhâˆ’1) â‰¥0, âˆ€Ï€, which can be seen by the induction deï¬nition in Eq. (4) and the non-negativity
of d0. The ï¬fth inequality is due to d
Ï€
hâˆ’1(xhâˆ’1) â‰¤dÏ€
hâˆ’1(xhâˆ’1), âˆ€Ï€, which can be shown inductively by
noticing d
Ï€
0 â‰¤dÏ€
0 and the deï¬nition of d
Ï€
h in Eq. (4). The last equality can be seen from that dD
hâˆ’1(xhâˆ’1) is
the marginal distribution of Dhâˆ’1 and Dhâˆ’1 is rolled in with unif(Î expl
hâˆ’1).
Integrating over xhâˆ’1 yields
d
Ï€
hâˆ’1 âˆ’d
Ï€
hâˆ’1 âˆ§ddD
hâˆ’1

1 â‰¤(d + 1) max
Ï€â€²âˆˆÎ 
bdÏ€â€²
hâˆ’1 âˆ’edÏ€â€²
hâˆ’1

1 + (d + 1) max
Ï€â€²âˆˆÎ 
bdÏ€â€²
hâˆ’1 âˆ’d
Ï€â€²
hâˆ’1

1 .
Since d
Ï€â€²
hâˆ’1 = PÏ€â€²
hâˆ’2(d
Ï€
hâˆ’2âˆ§Cx
hâˆ’2dD
hâˆ’2) = PÏ€â€²
hâˆ’2(d
Ï€
hâˆ’2âˆ§ddD
hâˆ’2) is linear in the features Âµâˆ—
hâˆ’2 (Lemma 16),
and edÏ€â€²
hâˆ’1 is the closest linear approximation in the â„“1 norm to bdÏ€â€²
hâˆ’1 (line 7), for any Ï€â€² âˆˆÎ  we have
bdÏ€â€²
hâˆ’1 âˆ’edÏ€â€²
hâˆ’1

1 â‰¤
bdÏ€â€²
hâˆ’1 âˆ’d
Ï€â€²
hâˆ’1

1
(20)
and thus
d
Ï€
hâˆ’1 âˆ’d
Ï€
hâˆ’1 âˆ§ddD
hâˆ’1

1 â‰¤2(d + 1) max
Ï€â€²âˆˆÎ 
bdÏ€â€²
hâˆ’1 âˆ’d
Ï€â€²
hâˆ’1

1 .
(21)
Then combining Eq. (19) with Eq. (21) gives
d
Ï€
h âˆ’dÏ€
h

1 â‰¤
d
Ï€
hâˆ’1 âˆ’dÏ€
hâˆ’1

1 + 4d max
Ï€â€²âˆˆÎ 
bdÏ€â€²
hâˆ’1 âˆ’d
Ï€â€²
hâˆ’1

1 .
Theorem (Restatement of Theorem 5). Fix Î´ âˆˆ(0, 1) and consider an MDP M that satisï¬es Assumption 1,
where the right feature Âµâˆ—is known. Then by setting
nmle = eO
d3K2H4 log(1/Î´)
Îµ2

, nreg = eO
d5K2H4 log(|Î |/Î´)
Îµ2

, n = nmle + nreg,
with probability at least 1 âˆ’Î´, FORCE returns state occupancy estimates {bdÏ€
h}Hâˆ’1
h=0 satisfying that
âˆ¥bdÏ€
h âˆ’dÏ€
hâˆ¥1 â‰¤Îµ, âˆ€h âˆˆ[H], Ï€ âˆˆÎ .
The total number of episodes required by the algorithm is
eO(nH) = eO
d5K2H5 log(|Î |/Î´)
Îµ2

.
33

Proof. From Algorithm 2, we know that dataset D0:Hâˆ’1 satisï¬es Assumption 2 and for each Ï€ âˆˆÎ , bdÏ€
h is
estimated in the same way as that in Algorithm 1. Therefore, we can follow the same steps as the proof of
Theorem 2. By setting Cx
h = d and Ca
h = K for all h âˆˆ[H] in Eq. (13), with probability at least 1 âˆ’Î´, for
any policy Ï€ âˆˆÎ , we get that
bdÏ€
h âˆ’d
Ï€
h

1 â‰¤18hd3/2K
s
log(16HBÂµnmle/Î´)
nmle
+ 666hd3/2K
s
log (2|Î |Hnreg/Î´)
nreg
.
(22)
The primary difference between the above results and the corresponding statements in Theorem 2 is
that the regression error in Eq. (22) includes an additional union bound over all Ï€ âˆˆÎ . This is because
Algorithm 2 performs estimation for all policies, while Algorithm 1 only concerns a single ï¬xed policy. We
note that this change in the proof occurs only through application of Lemma 14, which is stated generally
and already includes a union bound over all policies of interest. Because MLE estimation occurs only for the
data distribution and is policy-agnostic, the MLE error (second term) does not require such a union bound.
Next, to bound the missingness error, from Lemma 4, we have
d
Ï€
h âˆ’dÏ€
h

1 â‰¤
d
Ï€
hâˆ’1 âˆ’dÏ€
hâˆ’1

1 + 4d max
Ï€â€²âˆˆÎ 
bdÏ€â€²
hâˆ’1 âˆ’d
Ï€â€²
hâˆ’1

1 .
(23)
Unfolding Eq. (23) yields
d
Ï€
h âˆ’dÏ€
h

1 â‰¤4d
hâˆ’1
X
hâ€²=0
max
Ï€â€²âˆˆÎ 
bdÏ€â€²
hâ€² âˆ’d
Ï€â€²
hâ€²

1 .
(24)
Plugging the bound for
bdÏ€â€²
hâ€² âˆ’d
Ï€â€²
hâ€²

1 from Eq. (22) into Eq. (24) gives
d
Ï€
h âˆ’dÏ€
h

1 â‰¤72h2d3/2K
s
log(16HBÂµnmle/Î´)
nmle
+ 2664h2d5/2K
s
log (2|Î |Hnreg/Î´)
nreg
.
(25)
Combining Eq. (22) and Eq. (25) via triangle inequality and simplifying, we have
bdÏ€
h âˆ’dÏ€
h

1 â‰¤90h2d3/2K
s
log(16HBÂµnmle/Î´)
nmle
+ 3330h2d5/2K
s
log (2|Î |Hnreg/Î´)
nreg
.
Finally, noticing that nmle = eO

d3K2H4 log(1/Î´)
Îµ2

, nreg = eO

d5K2H4 log(|Î |/Î´)
Îµ2

, n = nmle + nreg
completes the proof.
F.2
Proof of online policy optimization
First, we prove Proposition 1, from which our online policy optimization guarantee (Theorem 6) follows
when combined with Theorem 5.
Proposition 6 (Restatement of Proposition 1). Given any policy Ï€ and reward function9 R = {Rh} with
Rh : XÃ—A â†’[0, 1], deï¬ne expected return as vÏ€
R := EÏ€[PHâˆ’1
h=0 Rh(xh, ah)] = PHâˆ’1
h=0
RR
dÏ€
h(xh)Rh(xh, ah)
Ï€(ah|xh)(dxh)(dah). Then for {bdÏ€
h} such that âˆ¥bdÏ€
h âˆ’dÏ€
hâˆ¥1 â‰¤Îµ/(2H) for all Ï€ âˆˆÎ  and h âˆˆ[H], and policy
chosen as
bÏ€R = argmax
Ï€âˆˆÎ 
bvÏ€
R,
9We assume known & deterministic rewards, and can easily handle unknown/stochastic versions (Appendix D.2).
34

we have
vbÏ€R
R â‰¥max
Ï€âˆˆÎ  vÏ€
R âˆ’Îµ,
where bvÏ€
R = PHâˆ’1
h=0
RR bdÏ€
h(xh)Rh(xh, ah)Ï€(ah|xh)(dxh)(dah) is the expected return calculated using
{bdÏ€
h}.
Proof. Since the R is bounded on [0, 1], for any Ï€ âˆˆÎ  we have
|bvÏ€
R âˆ’vÏ€
R| =
Hâˆ’1
X
h=0
ZZ
(bdÏ€
h(xh) âˆ’dÏ€
h(xh))R(xh, ah)Ï€(ah|xh)(dxh)(dah)
â‰¤
Hâˆ’1
X
h=0
Z
|bdÏ€
h(xh) âˆ’dÏ€
h(xh)|
Z
Ï€(ah|xh)(dah)

(dxh)
=
Hâˆ’1
X
h=0
âˆ¥dÏ€
h âˆ’bdÏ€
hâˆ¥1 â‰¤Îµ/2.
Next, recall we pick bÏ€R = argmaxÏ€âˆˆÎ  bvÏ€
R, and denote Ï€âˆ—
R = argmaxÏ€âˆˆÎ  bvÏ€
R. Then using the above
inequality, we have
vbÏ€R
R âˆ’max
Ï€âˆˆÎ  vÏ€
R = vbÏ€R
R âˆ’v
Ï€âˆ—
R
R = vbÏ€R
R âˆ’bvbÏ€R
R + bvbÏ€R
R âˆ’bv
Ï€âˆ—
R
R + bv
Ï€âˆ—
R
R âˆ’v
Ï€âˆ—
R
R â‰¥âˆ’Îµ
since bvbÏ€R
R â‰¥bv
Ï€âˆ—
R
R , completing the proof.
Theorem 9 (Restatement of Theorem 6). Fix Î´ âˆˆ(0, 1) and suppose Assumption 1 and Assumption 2 hold,
and Âµâˆ—is known. Given a policy class Î , let {bdÏ€
h}hâˆˆ[H],Ï€âˆˆÎ  be the output of running FORCE. Then with
probability at least 1 âˆ’Î´, for any reward function R and policy selected as bÏ€R = argmaxÏ€âˆˆÎ  bvÏ€
R, we have
vbÏ€R
R â‰¥argmax
Ï€âˆˆÎ 
vÏ€
R âˆ’Îµ,
where vÏ€
R and bvÏ€
R are deï¬ned in Proposition 1. The total number of episodes required by the algorithm is
ËœO
d5K2H7 log(|Î |/Î´)
Îµ2

.
Proof. The proof takes similar steps as the proof of Theorem 3. From Theorem 5, w.p. â‰¥1 âˆ’Î´, we
obtain estimates {bdÏ€
h} such that âˆ¥dÏ€
h âˆ’bdÏ€
hâˆ¥1 â‰¤
Îµ
2H for all Ï€ âˆˆÎ  with ËœO

d5K2H7 log(|Î |/Î´)
Îµ2

total number of
samples, where we use the union bound over Ï€ âˆˆÎ . Combining this with Proposition 1 gives the result.
G
Representation learning
In this section, we present the detailed algorithms and results for the representation learning setting (Sec-
tion 5), where the true density features are not given but must also be learned from an exponentially large
candidate feature set. The algorithms and analyses mostly follow that of the known density feature case
(Section 3 and Section 4), therefore, we mainly discuss the difference here.
35

G.1
Off-policy occupancy estimation
We start with describing our algorithm FORCRL (Algorithm 3), which estimates the occupancy distribution
dÏ€
h of any given policy Ï€ using an ofï¬‚ine dataset D0:Hâˆ’1 when the true density feature Âµâˆ—is unknown and
the learner is given a realizable density feature class Î¥ âˆ‹Âµâˆ—(see Assumption 3).
As discussed in Section 5, instead of using Âµâˆ—to construct the function classes, a natural choice here is
to use the union of all linear function classes. Since now the feature comes from candidate feature classes
Î¥hâˆ’2, Î¥hâˆ’1, in line 4 of Algorithm 3, we use different function classes Fhâˆ’1(Î¥hâˆ’2), Fh(Î¥hâˆ’1) as deï¬ned
in Eq. (26) for the MLE objective. In addition, in line 5 of Algorithm 3, now we run regression with a
different function class Wh(Î¥hâˆ’1) as deï¬ned in Eq. (27).
Algorithm 3 Fitted Occupancy Iteration with Clipping and Representation Learning (FORCRL)
Input: policy Ï€, density feature class Î¥, dataset D0:Hâˆ’1, sample sizes nmle and nreg, clipping thresholds
{Cx
h} and{Ca
h}.
1: Initialize bdÏ€
0 = d0, âˆ€Ï€ âˆˆÎ .
2: for h = 1, . . . , H do
3:
Randomly split Dhâˆ’1 to two folds Dmle
hâˆ’1 and Dreg
hâˆ’1 with sizes nmle and nreg respectively.
4:
Estimate marginal data distributions bdD
hâˆ’1(xhâˆ’1) and bd D,â€ 
hâˆ’1(xh) by MLE with dataset Dmle
hâˆ’1.
bdD
hâˆ’1 =
argmax
dhâˆ’1âˆˆFhâˆ’1(Î¥hâˆ’2)
1
nmle
nmle
X
i=1
log

dhâˆ’1(x(i)
hâˆ’1)

and bd D,â€ 
hâˆ’1 =
argmax
dhâˆˆFh(Î¥hâˆ’1)
1
nmle
nmle
X
i=1
log

dh(x(i)
h )

where
Fh(Î¥hâˆ’1) =
n
dh = âŸ¨Âµhâˆ’1, Î¸hâŸ©: dh âˆˆâˆ†(X), Âµhâˆ’1 âˆˆÎ¥hâˆ’1, Î¸h âˆˆRd, âˆ¥Î¸hâˆ¥âˆâ‰¤1
o
.
(26)
5:
Deï¬ne LDreg
hâˆ’1(wh, whâˆ’1, Ï€hâˆ’1) :=
1
nreg
Pnreg
i=1

wh(x(i)
h ) âˆ’whâˆ’1(x(i)
hâˆ’1)
Ï€hâˆ’1(a(i)
hâˆ’1|x(i)
hâˆ’1)
Ï€D
hâˆ’1(a(i)
hâˆ’1|x(i)
hâˆ’1)
2
and es-
timate
bwÏ€
h =
argmin
whâˆˆWh(Î¥hâˆ’1)
LDreg
hâˆ’1
 
wh,
bdÏ€
hâˆ’1 âˆ§Cx
hâˆ’1 bdD
hâˆ’1
bdD
hâˆ’1
, Ï€hâˆ’1 âˆ§Ca
hâˆ’1Ï€D
hâˆ’1
!
where
Wh(Î¥hâˆ’1) =

wh =
âŸ¨Âµhâˆ’1, Î¸up
h âŸ©
âŸ¨Âµhâˆ’1, Î¸down
h
âŸ©: âˆ¥whâˆ¥âˆâ‰¤Cx
hâˆ’1Ca
hâˆ’1, Âµhâˆ’1 âˆˆÎ¥hâˆ’1, Î¸up
h , Î¸down
h
âˆˆRd

.
(27)
6:
Set the estimate bdÏ€
h = bwÏ€
h bd D,â€ 
hâˆ’1.
7: end for
Output: estimated state occupancies {bdÏ€
h}hâˆˆ[H].
Similar as in the known feature case counterpart (Theorem 2), we have the following guarantee for
estimating dÏ€.
Theorem (Restatement of Theorem 7). Fix Î´ âˆˆ(0, 1). Suppose Assumption 1, Assumption 2, and Assump-
36

tion 3 hold. Then, given an evaluation policy Ï€, by setting
nmle = ËœO
ï£«
ï£­d
ï£«
ï£­X
hâˆˆ[H]
Cx
hCa
h
ï£¶
ï£¸
2
log(|Î¥|/Î´)/Îµ2
ï£¶
ï£¸and nreg = ËœO
ï£«
ï£­d
ï£«
ï£­X
hâˆˆ[H]
Cx
hCa
h
ï£¶
ï£¸
2
log(|Î¥|/Î´)/Îµ2
ï£¶
ï£¸,
with probability at least 1âˆ’Î´, FORCRL (Algorithm 3) returns state occupancy estimates {bdÏ€
h}Hâˆ’1
h=0 satisfying
that
bdÏ€
h âˆ’d
Ï€
h

1 â‰¤Îµ, âˆ€h âˆˆ[H].
The total number of episodes required by the algorithm is
ËœO
ï£«
ï£­dH
ï£«
ï£­X
hâˆˆ[H]
Cx
hCa
h
ï£¶
ï£¸
2
log(|Î¥|/Î´)/Îµ2
ï£¶
ï£¸.
Proof. The proof for this theorem largely follows its counterpart for the known feature case (Theorem 2),
and we mainly discuss the different steps here. We now make the following two slightly different claims on
MLE estimation and error propagation. Based on them, the ï¬nal error bound is obtained in the same way as
Theorem 2.
Claim 1
Our estimated data distributions satisfy that with probability 1 âˆ’Î´/2, for any h âˆˆ[H]
bdD
h âˆ’dD
h

1 â‰¤Îµmle and
bd D,â€ 
h
âˆ’dD,â€ 
h

1 â‰¤Îµmle,
(28)
where
Îµmle := 6
s
d log(16H|Î¥|BÂµnmle/Î´)
nmle
.
Claim 2
Under the high-probability event that Eq. (28) holds, we further have with probability at least
1 âˆ’Î´/2, for any 1 â‰¤h â‰¤H, we have
bdÏ€
h âˆ’d
Ï€
h

1 â‰¤
bdÏ€
hâˆ’1 âˆ’d
Ï€
hâˆ’1

1 + 3Cx
hâˆ’1Ca
hâˆ’1Îµmle +
âˆš
2Îµreg,hâˆ’1,
where
Îµreg,hâˆ’1 :=
s
221184d(Cx
hâˆ’1Ca
hâˆ’1)2 log (2H|Î¥|nreg/Î´)
nreg
.
(29)
Proof of Claim 1
Notice that for the term Îµmle in Eq. (28), we now have an additional |Î¥| factor inside
the log. The reason is that here we use Fhâˆ’1(Î¥hâˆ’2), Fh(Î¥hâˆ’1) instead of Fhâˆ’1, Fh. By Lemma 22, the
two function classes considered here have â„“1 optimistic covers with scale 1/nmle of size |Î¥| (2âŒˆBÂµnmleâŒ‰)d.
In addition, we still have that dD
hâˆ’1 âˆˆFhâˆ’1(Î¥hâˆ’2), dD,â€ 
hâˆ’1 âˆˆFh(Î¥hâˆ’1) from Lemma 18, and any dhâˆ’1 âˆˆ
Fhâˆ’1(Î¥hâˆ’2), Fh(Î¥hâˆ’1) is a valid probability distribution over X.
37

Proof of Claim 2
This proof mostly follows the proof of Claim 2 in Theorem 2. The difference is that
the function class Wh(Î¥hâˆ’1) now consists of all features in Î¥hâˆ’1 instead of only the true feature Âµâˆ—
hâˆ’1.
Therefore, in Eq. (29), the term Îµreg,hâˆ’1 has an additional |Î¥| inside the log, which is from the counterpart
of Eq. (17). It is also easy to see that
PÏ€
hâˆ’1(dD
hâˆ’1 ewhâˆ’1)
dD,â€ 
hâˆ’1
âˆˆWh(Î¥hâˆ’1) by following the same logic before.
Further noticing that Âµâˆ—
hâˆ’1 âˆˆÎ¥hâˆ’1, we again have Eq. (18) holds here.
Theorem 10 (Ofï¬‚ine policy optimization with representation learning). Fix Î´ âˆˆ(0, 1) and suppose As-
sumption 1, Assumption 2, and Assumption 3 hold. Given a policy class Î , let {bdÏ€
h}hâˆˆ[H],Ï€âˆˆÎ  be the output
of running Algorithm 3. Then with probability at least 1 âˆ’Î´, for any deterministic reward function R and
policy selected as bÏ€R = argmaxÏ€âˆˆÎ  bvÏ€
R, we have
vbÏ€R
R â‰¥argmax
Ï€âˆˆÎ 
vÏ€
R âˆ’Îµ,
where vÏ€
R and bvÏ€
R are deï¬ned in Proposition 1, and vR is deï¬ned similarly for {d
Ï€
h}. The total number of
episodes required by the algorithm is
ËœO
ï£«
ï£­dH3
ï£«
ï£­X
hâˆˆ[H]
Cx
hCa
h
ï£¶
ï£¸
2
log(|Î ||Î¥|/Î´)/Îµ2
ï£¶
ï£¸.
Additionally, deï¬ne the set of policies fully covered by the data to be
Î covered =
n
Ï€ âˆˆÎ  : dÏ€
h = d
Ï€
h, âˆ€h âˆˆ[H]
o
.
Then with the same total number of episodes required by the algorithm, for any reward function R and
policy selected as bÏ€R = argmaxÏ€âˆˆÎ covered bvÏ€
R, with probability at least 1 âˆ’Î´, we have
vbÏ€R
R â‰¥argmax
Ï€âˆˆÎ covered vÏ€
R âˆ’Îµ.
Proof. The proof follows the same steps as that of Theorem 3. Notice that now we will apply Theorem 7
rather than Theorem 2 to get the bound âˆ¥bdÏ€
h âˆ’d
Ï€
hâˆ¥1, which leads to the additional log(|Î¥|) factor.
G.2
Online policy cover construction
Now we present the algorithm FORCRLE (Algorithm 4), which estimates the occupancy distribution dÏ€
h of
any given policy Ï€ with the access of online interaction. Again the true density feature Âµâˆ—is unknown and
the learner is given a realizable density feature class Î¥ (Âµâˆ—âˆˆÎ¥).
Similar as the know feature case online algorithm (Algorithm 2), we use the ofï¬‚ine algorithm (Algo-
rithm 3) as a submodule. However, as discussed in the main text, the crucial different step is to select a
representation bÂµhâˆ’1 in Eq. (30) in line 8 before setting edÏ€
h. This guarantee the cardinality of the barycentric
spanner is at most d. Then the state occupancy edÏ€
h is set as the linear estimate using bÂµhâˆ’1 (rather than using
Âµâˆ—
hâˆ’1 in the known feature case) in line 9.
Similar as in the known feature case counterpart (Theorem 5), we have the following guarantee for
estimating dÏ€.
Theorem (Restatement of Theorem 8). Fix Î´ âˆˆ(0, 1) and suppose Assumption 1 and Assumption 3 hold.
Then by setting
nmle = eO
d3K2H4 log(|Î¥|/Î´)
Îµ2

, nreg = eO
d5K2H4 log(|Î ||Î¥|/Î´)
Îµ2

, n = nmle + nreg,
38

Algorithm 4 FORCRL-guided Exploration (FORCRLE)
Input: policy class Î , density feature class Î¥, n = nmle + nreg
1: Initialize bdÏ€
0 = d0 and edÏ€
0 = d0, âˆ€Ï€ âˆˆÎ .
2: for h = 1, . . . , H do
3:
Construct {edÏ€hâˆ’1,i
hâˆ’1
}d
i=1 as the barycentric spanner of {edÏ€
hâˆ’1}Ï€âˆˆÎ , and set Î expl
hâˆ’1 = {Ï€hâˆ’1,i}d
i=1.
4:
Draw a tuple dataset Dhâˆ’1 = {(x(i)
hâˆ’1, a(i)
hâˆ’1, x(i)
h )}n
i=1 using unif(Î expl
hâˆ’1) â—¦unif(A).
5:
for Ï€ âˆˆÎ  do
6:
Estimate bdÏ€
h using the h-level loop10of Algorithm 3 (lines 4-6) with Dh, bdÏ€
hâˆ’1, Cx
h = d, Ca
h = K.
7:
end for
8:
Select feature bÂµhâˆ’1 according to
bÂµhâˆ’1 =
min
Âµhâˆ’1âˆˆÎ¥hâˆ’1 max
Ï€âˆˆÎ  min
Î¸hâˆˆRd âˆ¥âŸ¨Âµhâˆ’1, Î¸hâŸ©âˆ’bdÏ€
hâˆ¥1.
(30)
9:
For all Ï€ âˆˆÎ , set the closest linear approximation to bdÏ€
h with feature bÂµhâˆ’1 as edÏ€
h = âŸ¨bÂµhâˆ’1, eÎ¸hâŸ©, where
eÎ¸h = argminÎ¸hâˆˆRd âˆ¥âŸ¨bÂµhâˆ’1, Î¸hâŸ©âˆ’bdÏ€
hâˆ¥1.
10: end for
Output: estimated state occupancy measure {bdÏ€
h}hâˆˆ[H],Ï€âˆˆÎ .
with probability at least 1 âˆ’Î´, FORCRLE (Algorithm 4) returns state occupancy estimates {bdÏ€
h}Hâˆ’1
h=0 satis-
fying that
âˆ¥bdÏ€
h âˆ’dÏ€
hâˆ¥1 â‰¤Îµ, âˆ€h âˆˆ[H], Ï€ âˆˆÎ .
The total number of episodes required by the algorithm is
eO(nH) = eO
d5K2H5 log(|Î ||Î¥|/Î´)
Îµ2

.
Proof. The proof for this theorem largely follows its counterpart for the known feature case (Theorem 5),
and we only discuss the different steps here.
Firstly, Lemma 4 still holds. However, since we use â€œjoint linearizationâ€ in line 8 and line 9, we need
to modify the proof of Eq. (20) as the following. Again, we have d
Ï€â€²
hâˆ’1 = PÏ€â€²
hâˆ’2(d
Ï€
hâˆ’2 âˆ§Cx
hâˆ’2dD
hâˆ’2) =
PÏ€â€²
hâˆ’2(d
Ï€
hâˆ’2 âˆ§ddD
hâˆ’2) is linear in the true feature Âµâˆ—
hâˆ’2 (Lemma 16). Together with the feature selection
criteria Eq. (30), we have that
max
Ï€â€²âˆˆÎ  âˆ¥edÏ€â€²
hâˆ’1 âˆ’bdÏ€â€²
hâˆ’1âˆ¥1 = max
Ï€â€²âˆˆÎ 
min
Î¸hâˆ’1âˆˆRd âˆ¥âŸ¨bÂµhâˆ’2, Î¸hâˆ’1âŸ©âˆ’bdÏ€â€²
hâˆ’1âˆ¥1
â‰¤max
Ï€â€²âˆˆÎ 
min
Î¸hâˆ’1âˆˆRd âˆ¥âŸ¨Âµâˆ—
hâˆ’2, Î¸hâˆ’1âŸ©âˆ’bdÏ€â€²
hâˆ’1âˆ¥1 â‰¤max
Ï€â€²âˆˆÎ  âˆ¥d
Ï€â€²
hâˆ’1 âˆ’bdÏ€â€²
hâˆ’1âˆ¥1.
For Eq. (22), we will have an additional |Î¥| factor inside the log as
Îµmle := 6
s
d log(16H|Î¥|BÂµnmle/Î´)
nmle
.
The reason is that here we use Fhâˆ’1(Î¥hâˆ’2), Fh(Î¥hâˆ’1) instead of Fhâˆ’1, Fh. By Lemma 22, the two
function classes considered here have â„“1 optimistic covers with scale 1/nmle of size |Î¥| (2âŒˆBÂµnmleâŒ‰)d.
In addition, we still have that dD
hâˆ’1 âˆˆFhâˆ’1(Î¥hâˆ’2), dD,â€ 
hâˆ’1 âˆˆFh(Î¥hâˆ’1) Lemma 18, and any dhâˆ’1 âˆˆ
Fhâˆ’1(Î¥hâˆ’2), Fh(Î¥hâˆ’1) is a valid probability distribution over X.
The remaining part of the proof is the same as that of Theorem 5.
39

Theorem 11 (Online policy optimization with representation learning). Fix Î´ âˆˆ(0, 1) and suppose As-
sumption 1 and Assumption 3 hold. Given a policy class Î , let {bdÏ€
h}hâˆˆ[H],Ï€âˆˆÎ  be the output of running
Algorithm 4. Then with probability at least 1 âˆ’Î´, for any deterministic reward function R (as per Proposi-
tion 1) and policy selected as bÏ€R = argmaxÏ€âˆˆÎ  bvÏ€
R, we have
vbÏ€R
R â‰¥argmax
Ï€âˆˆÎ 
vÏ€
R âˆ’Îµ,
where bvÏ€
R := PHâˆ’1
h=0
RR bdÏ€
h(xh)R(xh, ah)Ï€(ah|xh)(dxh)(dah). The total number of episodes required by
the algorithm is
ËœO
d5K2H7 log(|Î ||Î¥|/Î´)
Îµ2

.
Proof. The proof follows the same steps as that of Theorem 6. Notice that now we will apply Theorem 8
rather than Theorem 5 to get the bound âˆ¥dÏ€
h âˆ’bdÏ€
hâˆ¥, which leads to the additional log(|Î |) factor.
H
Maximum likelihood estimation
In this section, we adapt the standard i.i.d. results of maximum likelihood estimation (Van de Geer, 2000) to
our setting, and in particular, to our (inï¬nite) linear function class. We consider the problem of estimating a
probability distribution over the instance space X, and note that we abuse some notations (e.g., n, L, D, F)
in this section, as they have different meanings in other parts of the paper. Given an i.i.d. sampled dataset
D = {x(i)}n
i=1 and a function class F, we optimize the MLE objective
bf = argmin
fâˆˆF
1
n
n
X
i=1
log

f(x(i))

.
(31)
We consider the function class F to be inï¬nite, and as is common in statistical learning, our result will
depends on its structural complexity. In particular, this will be quantiï¬ed using the â„“1 optimistic cover,
deï¬ned below:
Deï¬nition 3 (â„“1 optimistic cover). For a function class F âŠ†(X â†’R), we call function class F an â„“âˆ
optimistic cover of F with scale Î³, if for any f âˆˆF there exists f âˆˆF, such that âˆ¥f âˆ’fâˆ¥1 â‰¤Î³ and
f(x) â‰¤f(x), âˆ€x âˆˆX. Notice that here we do not require the cover to be proper, i.e., we allow F Ì¸âŠ†F.
Now we are ready to state the MLE guarantee formally.
Lemma 12 (MLE guarantee). Let D = {x(i)}n
i=1 be a dataset, where x(i) are drawn i.i.d. from some ï¬xed
probability distribution fâˆ—over X. Consider a function class F that satisï¬es: (i) fâˆ—âˆˆF, (ii) each function
f âˆˆF is a valid probability distribution over X (i.e., f âˆˆâˆ†(X)), and (iii) F has a ï¬nite â„“1 optimistic cover
(Deï¬nition 3) F with scale Î³ and F âŠ†(X â†’Râ‰¥0). Then with probability at least 1 âˆ’Î´, the MLE solution
bf in Eq. (31) has an â„“1 error guarantee
âˆ¥bf âˆ’fâˆ—âˆ¥1 â‰¤Î³ +
s
12 log(|F|/Î´)
n
+ 6Î³.
Proof. Our proof is based on Zhang (2006); Agarwal et al. (2020); Liu et al. (2022) and is simpler since we
assume the D here is drawn i.i.d. instead of adaptively. We ï¬rst deï¬ne L(f, D) = 1
2
Pn
i=1 log

f(x(i))
fâˆ—(x(i))

.
By Chernoffâ€™s method, for a ï¬xed f âˆˆF we have that
P
 L(f, D) âˆ’log(ED[exp(L(f, D))]) â‰¥log(|F|/Î´)

40

â‰¤exp(âˆ’log(|F|/Î´))ED [exp (L(f, D) âˆ’log(ED[exp(L(f, D))]))]
= Î´/|F|.
Union bounding over f âˆˆF, with probability at least 1 âˆ’Î´, for any f âˆˆF we have
âˆ’log(ED[exp(L(f, D))]) â‰¤âˆ’L(f, D) + log(|F|/Î´).
(32)
Let f âˆˆF be the Î³-close â„“1 optimistic approximator of the MLE solution bf âˆˆF. Since f(x) â‰¥
bf(x), âˆ€x âˆˆX due to the optimistic covering construction and bf is the MLE estimator, for the RHS of
Eq. (32). we have
âˆ’L(f, D) = 1
2
n
X
i=1
log
 
fâˆ—(x(i))
f(x(i))
!
â‰¤1
2
n
X
i=1
log
 
fâˆ—(x(i))
bf(x(i))
!
= 1
2
 n
X
i=1
log(fâˆ—(x(i))) âˆ’
n
X
i=1
log( bf(x(i)))
!
â‰¤0.
Next, consider the LHS of Eq. (32). From the deï¬nition of dataset D and L(f, D), we get
âˆ’log(ED[exp(L(f, D))]) = âˆ’log
 
ED
"
exp
 
1
2
n
X
i=1
log
 
f(x(i))
fâˆ—(x(i))
!!#!
= âˆ’n log

ED

exp
1
2 log
 f(x)
fâˆ—(x)

= âˆ’n log
ï£«
ï£­ED
ï£®
ï£°
s
f(x)
fâˆ—(x)
ï£¹
ï£»
ï£¶
ï£¸.
Furthermore, by âˆ’log(y) â‰¥1 âˆ’y, â„“1 optimistic cover deï¬nition, and fâˆ—, bf are valid distributions over
x âˆˆX, we have
âˆ’n log
ï£«
ï£­ED
ï£®
ï£°
s
f(x)
fâˆ—(x)
ï£¹
ï£»
ï£¶
ï£¸â‰¥n
ï£«
ï£­1 âˆ’ED
ï£®
ï£°
s
f(x)
fâˆ—(x)
ï£¹
ï£»
ï£¶
ï£¸= n

1 âˆ’
Z q
f(x)fâˆ—(x)(dx)

= n
2
Z p
fâˆ—(x) âˆ’
q
f(x)
2
(dx) + n
2

1 âˆ’
Z
f(x)(dx)

= n
2
Z p
fâˆ—(x) âˆ’
q
f(x)
2
(dx) + n
2
Z 
bf(x) âˆ’f(x)

(dx)
â‰¥n
2
Z p
fâˆ—(x) âˆ’
q
f(x)
2
(dx) âˆ’nÎ³
2 .
Then notice that
R p
fâˆ—(x) +
q
f(x)
2
(dx) â‰¤2
R  fâˆ—(x) + f(x)

(dx) â‰¤2
R
(fâˆ—(x) + bf(x) +
|f(x) âˆ’bf(x)|)(dx) â‰¤6 and the Cauchy-Schwarz inequality, we obtain
n
2
Z p
fâˆ—(x) âˆ’
q
f(x)
2
(dx) âˆ’nÎ³
2
â‰¥n
12
 Z p
fâˆ—(x) âˆ’
q
f(x)
2
(dx)
!  Z p
fâˆ—(x) +
q
f(x)
2
(dx)
!
âˆ’nÎ³
2
â‰¥n
12
Z
|f(x) âˆ’fâˆ—(x)|(dx)
2
âˆ’nÎ³
2 = n
12âˆ¥f âˆ’fâˆ—âˆ¥2
1 âˆ’nÎ³
2 .
41

Combining the above inequalities and rearranging yields
âˆ¥f âˆ’fâˆ—âˆ¥2
1 â‰¤12 log(|F|/Î´)
n
+ 6Î³.
Finally, by the triangle inequality and the deï¬nition of the â„“1 optimistic cover, we get
âˆ¥bf âˆ’fâˆ—âˆ¥1 â‰¤âˆ¥bf âˆ’fâˆ¥1 + âˆ¥f âˆ’fâˆ—âˆ¥1 â‰¤Î³ +
s
12 log(|F|/Î´)
n
+ 6Î³ ,
which completes the proof.
I
Auxiliary lemmas
In this section, we provide detailed proofs for auxiliary lemmas.
I.1
Squared loss regression results
Lemma 13 (Squared loss decomposition). For any wh, wh+1 : X â†’R, dataset Dreg
h
= {(xh, ah, xh+1)} âˆ¼
dD
h , and a pseudo-policy Ï€, we have
wh+1 âˆ’PÏ€
h
 dD
h wh

dD,â€ 
h

2
2,dD,â€ 
h
= E
h
LDreg
h (wh+1, wh, Ï€)
i
âˆ’E
"
LDreg
h
 
PÏ€
h
 dD
h wh

dD,â€ 
h
, wh, Ï€
!#
.
(33)
Proof. We introduce a new notation
(EÏ€
hwh)(xh+1) :=
 PÏ€
h
 dD
h wh

(xh+1)
dD,â€ 
h
(xh+1)
=
RR
Ph(xh+1|xh, ah)Ï€(ah|xh)dD
h (xh)wh(xh)(dxh)(dah)
dD,â€ 
h
(xh+1)
,
(34)
which represents the conditional expectation. Then we have the decomposition
E
h
LDreg
h (wh+1, wh, Ï€)
i
=
ZZZ
dD
h (xh, ah, xh+1)

wh+1(xh+1) âˆ’Ï€(ah|xh)
Ï€D(ah|xh)wh(xh)
2
(dxh)(dah)(dxh+1)
=
ZZZ
dD
h (xh, ah, xh+1)

wh+1(xh+1) âˆ’(EÏ€
hwh)(xh+1) + (EÏ€
hwh)(xh+1) âˆ’Ï€(ah|xh)
Ï€D(ah|xh)wh(xh)
2
(dxh)(dah)(dxh+1)
=
Z
dD,â€ 
h
(xh+1)(wh+1(xh+1) âˆ’(EÏ€
hwh)(xh+1))2(dxh+1)
+
ZZZ
dD
h (xh, ah, xh+1)

(EÏ€
hwh)(xh+1) âˆ’Ï€(ah|xh)
Ï€D(ah|xh)wh(xh)
2
(dxh)(dah)(dxh+1)
+ 2
ZZZ
dD
h (xh, ah, xh+1)(wh+1(xh+1) âˆ’(EÏ€
hwh)(xh+1))

(EÏ€
hwh)(xh+1) âˆ’Ï€(ah|xh)
Ï€D(ah|xh)wh(xh)

(dxh)(dah)(dxh+1)
42

= âˆ¥wh+1 âˆ’(EÏ€
hwh)âˆ¥2
2,dD,â€ 
h
+ E
h
LDreg
h (EÏ€
hwh, wh, Ï€)
i
+ 2
Z
dD,â€ 
h
(xh+1)(wh+1(xh+1) âˆ’(EÏ€
hwh)(xh+1))(EÏ€
hwh)(xh+1)(dxh+1)
âˆ’2
Z
dD,â€ 
h
(xh+1)(wh+1(xh+1) âˆ’(EÏ€
hwh)(xh+1))
Â·
ZZ
dD
h (xh, ah|xh+1) Ï€(ah|xh)
Ï€D(ah|xh)wh(xh)(dxh)(dah)

(dxh+1)
= âˆ¥wh+1 âˆ’(EÏ€
hwh)âˆ¥2
2,dD,â€ 
h
+ E
h
LDreg
h (EÏ€
hwh, wh, Ï€)
i
+ 2
Z
dD,â€ 
h
(xh+1)(wh+1(xh+1) âˆ’(EÏ€
hwh)(xh+1))((EÏ€
hwh)(xh+1) âˆ’(EÏ€
hwh)(xh+1))(dxh+1)
= âˆ¥wh+1 âˆ’(EÏ€
hwh)âˆ¥2
2,dD,â€ 
h
+ E
h
LDreg
h (EÏ€
hwh, wh, Ï€)
i
.
Lemma 14 (Deviation bound for regression with squared loss). For h âˆˆ[H], consider a dataset D0:h that
satisï¬es Assumption 2 and a function wh : X â†’[0, Cx
h] that only depends on D0:hâˆ’1
S Dmle
h
. Consider a
ï¬nite feature class Î¥h and a ï¬nite policy class Î â€² such that any Ï€ âˆˆÎ â€² is a pseudo-policy (Deï¬nition 1)
satisfying Ï€h(ah|xh) â‰¤Ca
hÏ€D
h (ah|xh), âˆ€xh âˆˆX, ah âˆˆA. Then with probability 1 âˆ’Î´, for any wh+1 âˆˆ
Wh+1(Î¥h) and Ï€ âˆˆÎ â€², we have
E
h
LDreg
h (wh+1, wh, Ï€) âˆ’LDreg
h (EÏ€
hwh, wh, Ï€)
i
âˆ’

LDreg
h (wh+1, wh, Ï€) âˆ’LDreg
h (EÏ€
hwh, wh, Ï€)

â‰¤1
2E
h
LDreg
h (wh+1, wh, Ï€) âˆ’LDreg
h (EÏ€
hwh, wh, Ï€)
i
+ 221184d(Cx
hCa
h)2 log (nreg|Î â€²||Î¥h|/Î´)
nreg
where the function class Wh+1(Î¥h) is deï¬ned in Algorithm 1 as in Eq. (26) and the operator EÏ€
h is deï¬ned
in Eq. (34).
Proof. We ï¬rst ï¬x the datasets D0:hâˆ’1
S Dmle
h
and prove the desired bound when conditioned on these
datasets, in which case wh, dD,â€ 
h
, Ï€D are ï¬xed. In the following, the expectation E and variance V are
w.r.t. (xh, ah, xh+1) âˆ¼dD
h , i.e., the data distribution from which the samples in Dreg
h
are drawn i.i.d. from
(Assumption 2), when conditioned on D0:hâˆ’1
S Dmle
h
.
Consider a single Ï€ âˆˆÎ â€² and feature Âµh âˆˆÎ¥h, and consider the hypothesis class
Y(Wh+1(Âµh), wh, Ï€) = {Y (wh+1, wh, Ï€) : wh+1 âˆˆWh+1(Âµh)} .
where the random variable Y (wh+1, wh, Ï€) (suppressing the dependence on the (xh, ah, xh+1) tuple) is
deï¬ned for convenience as
Y (wh+1, wh, Ï€) :=

wh+1(xh+1) âˆ’wh(xh) Ï€(ah|xh)
Ï€D(ah|xh)
2
âˆ’

(EÏ€
hwh)(xh+1) âˆ’wh(xh) Ï€(ah|xh)
Ï€D(ah|xh)
2
,
and we use Yi(wh+1, wh, Ï€) to denote its realization on the i-th tuple data (x(i)
h , a(i)
h , x(i)
h+1) âˆˆDreg
h . The
function class Wh+1(Âµh) is deï¬ned as in Eq. (27), i.e.,
Wh+1(Âµh) =
(
wh+1 = âŸ¨Âµh, Î¸up
h+1âŸ©
âŸ¨Âµh, Î¸down
h+1 âŸ©: âˆ¥wh+1âˆ¥âˆâ‰¤Cx
hCa
h, Î¸up
h+1, Î¸down
h+1 âˆˆRd
)
.
It can be seen that |Y (wh+1, wh, Ï€)| â‰¤4(Cx
hCa
h)2 from the following. From their respective deï¬ni-
tions, we know âˆ¥whâˆ¥âˆâ‰¤Cx
h, âˆ¥Ï€
Ï€D âˆ¥âˆâ‰¤Ca
h, and âˆ¥wh+1âˆ¥âˆâ‰¤Cx
hCa
h. We also have (EÏ€
hwh)(xh+1) =
(PÏ€
h(dD
h wh))(xh+1)
dD,â€ 
h
(xh+1)
âˆˆ[0, Cx
hCa
h] from Lemma 19.
43

Further, for any Y (wh+1, wh, Ï€) âˆˆY(Wh+1(Âµh), wh, Ï€), we can bound the variance V[Y (wh+1, wh, Ï€)]
as
V[Y (wh+1, wh, Ï€)] â‰¤E

Y (wh+1, wh, Ï€)2
= E
ï£®
ï£°
 
wh+1(xh+1) âˆ’wh(xh) Ï€(ah|xh)
Ï€D(ah|xh)
2
âˆ’

(EÏ€
hwh)(xh+1) âˆ’wh(xh) Ï€(ah|xh)
Ï€D(ah|xh)
2!2ï£¹
ï£»
= E
"
(wh+1(xh+1) âˆ’(EÏ€
hwh)(xh+1))2

wh+1(xh+1) âˆ’2wh(xh) Ï€(ah|xh)
Ï€D(ah|xh) + (EÏ€
hwh)(xh+1)
2#
â‰¤4(Cx
hCa
h)2E

(wh+1(xh+1) âˆ’(EÏ€
hwh)(xh+1))2
= 4(Cx
hCa
h)2E [Y (wh+1, wh, Ï€)] .
(Lemma 13)
Next, we show that the uniform covering number N1(Î³, Y(Wh+1(Âµh), wh, Ï€), m) (see Deï¬nition 7) for
any Î³ âˆˆR, m âˆˆN can be bounded by the covering number of Wh+1(Âµh). Let Zm = (x(i)
h , a(i)
h , x(i)
h+1)m
i=1
denote m i.i.d. samples from dD
h , and denote Xm = (x(i)
h+1)m
i=1 the corresponding xh+1 samples. For any
Zm and Y (wh+1, wh, Ï€), Y (wâ€²
h+1, wh, Ï€) âˆˆY(Wh+1(Âµh), wh, Ï€),
1
m
m
X
i=1
Yi(wh+1, wh, Ï€) âˆ’Yi(wâ€²
h+1, wh, Ï€)

= 1
m
m
X
i=1

 
wh+1(x(i)
h+1) âˆ’wh(x(i)
h ) Ï€(a(i)
h |x(i)
h )
Ï€D(a(i)
h |x(i)
h )
!2
âˆ’
 
wâ€²
h+1(x(i)
h+1) âˆ’wh(x(i)
h ) Ï€(a(i)
h |x(i)
h )
Ï€D(a(i)
h |x(i)
h )
!2
= 1
m
m
X
i=1
wh+1(x(i)
h+1) âˆ’2wh(x(i)
h ) Ï€(a(i)
h |x(i)
h )
Ï€D(a(i)
h |x(i)
h )
+ wâ€²
h+1(x(i)
h+1)
 Â·
wh+1(x(i)
h+1) âˆ’wâ€²
h+1(x(i)
h+1)

â‰¤4Cx
hCa
h
m
m
X
i=1
wh+1(x(i)
h+1) âˆ’wâ€²
h+1(x(i)
h+1)
 .
Thus any Î³/(4Cx
hCa
h)-covering of Wh+1|Xm in â„“1 is a Î³-covering of Y (Wh+1, wh, Ï€)|Zm in â„“1, and
N1(Î³, Y (Wh+1(Âµh), wh, Ï€), Zm) â‰¤N1(Î³/(4Cx
hCa
h), Wh+1(Âµh), Xm)
which implies the same relationship for the uniform covering numbers:
N1(Î³, Y (Wh+1(Âµh), wh, Ï€), m) = max
Zm N1(Î³, Y (Wh+1(Âµh), wh, Ï€), Zm)
â‰¤max
Xm N1(Î³/(4Cx
hCa
h), Wh+1(Âµh), Xm) = N1(Î³/(4Cx
hCa
h), Wh+1(Âµh), m).
Then using this inequality and b = 4(Cx
hCa
h)2 in Lemma 26 and conditioning on D0:hâˆ’1
S Dmle
h
, for
any wh+1 âˆˆWh+1(Âµh), we have
P
 E[Y (wh+1, wh, Ï€)] âˆ’
1
nreg
n
X
i=1
Yi(wh+1, wh, Ï€)
 â‰¥Îµ
!
â‰¤36N1

Îµ3
10240(Cx
hCa
h)4 , Y(Wh+1(Âµh), wh, Ï€), 640nreg(Cx
hCa
h)4
Îµ2

Â· exp

âˆ’
nregÎµ2
128V[Y (wh+1, wh, Ï€)] + 2048Îµ(Cx
hCa
h)2

44

â‰¤36N1

Îµ3
40960(Cx
hCa
h)5 , Wh+1(Âµh), 640nreg(Cx
hCa
h)4
Îµ2

Â· exp

âˆ’
nregÎµ2
512(Cx
hCa
h)2E[Y (wh+1, wh, Ï€)] + 2048Îµ(Cx
hCa
h)2

.
Then setting the RHS equal to Î´â€², we have
nreg =
512(Cx
hCa
h)2 (E[Y (wh+1, wh, Ï€)] + 4Îµ) log

36N1

Îµ3
40960(Cx
hCa
h)5 , Wh+1(Âµh), 640nreg(Cx
hCa
h)4
Îµ2

/Î´â€²
Îµ2
implying
Îµ â‰¤
v
u
u
t512(Cx
hCa
h)2E[Y (wh+1, wh, Ï€)] log

36N1

Îµ3
40960(Cx
hCa
h)5 , Wh+1(Âµh), 640nreg(Cx
hCa
h)4
Îµ2

/Î´â€²

nreg
+
2048(Cx
hCa
h)2 log

36N1

Îµ3
40960(Cx
hCa
h)5 , Wh+1(Âµh), 640nreg(Cx
hCa
h)4
Îµ2

/Î´â€²
nreg
.
From Lemma 23 and Lemma 25, and noting that nreg â‰¥2048(Cx
hCa
h)2
Îµ
, we have that
log

36N1

Îµ3
40960(Cx
hCa
h)5 , Wh+1(Âµh), 640nreg(Cx
hCa
h)4
Îµ2

/Î´â€²

â‰¤4(d + 1) log(8e) log
655360e2(Cx
hCa
h)6
Îµ3Î´â€²

â‰¤96d log
nreg
Î´â€²

.
Thus with probability at least 1 âˆ’Î´â€²,
E[Y (wh+1, wh, Ï€)] âˆ’
1
nreg
nreg
X
i=1
Yi(wh+1, wh, Ï€)

â‰¤
s
49152d(Cx
hCa
h)2E[Y (wh+1, wh, Ï€)] log
  nreg
Î´â€²

nreg
+ 196608d(Cx
hCa
h)2 log
  nreg
Î´â€²

nreg
.
Then invoking the AM-GM inequality,
E[Y (wh+1, wh, Ï€)] âˆ’
1
nreg
nreg
X
i=1
Yi(wh+1, wh, Ï€)

â‰¤1
2E[Y (wh+1, wh, Ï€)] + 221184 Â· d(Cx
hCa
h)2 log
  nreg
Î´â€²

nreg
.
Recall that this result holds for a ï¬xed Ï€ and Wh+1(Âµh) deï¬ned using a ï¬xed Âµh. Then setting Î´â€² =
Î´
|Î â€²||Î¥h|
and taking a union bound over Î  and Î¥h, we have that with probability at least 1 âˆ’Î´ that for any Ï€ âˆˆÎ â€²
and wh+1 âˆˆWh+1(Î¥h) that
E[Y (wh+1, wh, Ï€)] âˆ’
1
nreg
nreg
X
i=1
Yi(wh+1, wh, Ï€)

45

â‰¤1
2E[Y (wh+1, wh, Ï€)] +
221184d(Cx
hCa
h)2 log

nreg|Î â€²||Î¥h|
Î´

nreg
.
Finally, since this result holds for any ï¬xed D0:hâˆ’1
S Dmle
h
, by the law of total expectation, it also holds
with probability at least 1âˆ’Î´â€² without conditioning on D0:hâˆ’1
S Dmle
h
. Using Lemma 13 with the deï¬nitions
of Y (wh+1, wh, Ï€) and Yi(wh+1, wh, Ï€) completes the proof.
I.2
Barycentric spanner
In this section we ï¬rst deï¬ne the barycentric spanner (Awerbuch and Kleinberg, 2008, Deï¬nition 2.1), then
prove that a spanner of size d always exists for a set of functions linear in a feature Âµhâˆ’1, from which Propo-
sition 3 follows straightforwardly. The proof is adapted from Awerbuch and Kleinberg (2008, Proposition
2.2), which only applies to square matrices, and we extend it to rectangular matrices for completeness. We
close with a discussion of the computational complexity of ï¬nding the barycentric spanner.
Deï¬nition 4 (Barycentric spanner). Let V be a vector space over the real numbers, and S âŠ†V a subset
whose linear span is a m-dimensional subspace of V . A set X = {x1, . . . , xm} âŠ†S is a barycentric
spanner of S if every x âˆˆS may be expressed as a linear combination of elements of X using coefï¬cients
in [âˆ’1, +1].
Lemma 15 (Barycentric spanner for linear functions). For a feature Âµhâˆ’1 âˆˆÎ¥hâˆ’1 with rank d, any set of
linear functions U âŠ†{âŸ¨Âµhâˆ’1, Î¸hâŸ©: Î¸h âˆˆRd} has a barycentric spanner of cardinality min(|U|, d).
Proof. We prove the proposition when rank(Âµhâˆ’1) = d is full rank (the argument should be the same when
rank(Âµhâˆ’1) < d), and |U| > d (otherwise we can satisfy the lemma statement by picking all of U to be the
spanner). First, U is a compact subset of R|X| because it is closed and bounded. Because U is linear in Âµhâˆ’1,
its linear span is a d-dimensional subspace of R|X|, and any u âˆˆU can be written as the linear combination
of a subspace basis.
We claim the barycentric spanner is any subset B = {b1, . . . , bd} âŠ†U with B âˆˆRdÃ—|X| that maximizes
the volume | det(BBâŠ¤)|. By compactness, the maximum is obtained by at least one subset of U. Since
det(BBâŠ¤) = (Qd
i=1 Ïƒi(B))2, the maximizing B will have d singular values and full row rank (otherwise
the determinant will be 0). As a result, any u âˆˆU will be a linear combination of the rows of B, i.e., there
exists {ci}d
i=1 such that u = Pd
i=1 cibi. We will prove that |ci| â‰¤1 by contradiction.
W.l.o.g, suppose there exists u with coefï¬cient |c1| > 1. Then consider a new matrix eB = {u, b2, . . . , bd},
which can be expressed as eB = CB, where C âˆˆRdÃ—d is the coefï¬cient matrix. Then eB has determinant
| det( eB eBâŠ¤)| = | det(C)|2| det(BBâŠ¤)| = |c1|2| det(BBâŠ¤)| â‰¥det(BBâŠ¤).
Then we have a contradiction because B was volume-maximizing, and |ci| â‰¤1.
Computation of barycentric spanner
Lastly, we discuss computation of the barycentric spanner. In the
main results of the paper we assume that we can perfectly compute the barycentric spanner in an efï¬cient
manner. When this is not the case, the algorithm in Figure 2 in Awerbuch and Kleinberg (2008) (with
similar adaptations to handle rectangular matrices as in the proof of Lemma 15) can be used to compute a
C-approximate barycentric spanner, where C > 1, with O(d2 logC d) calls to a linear optimization oracle
(Awerbuch and Kleinberg, 2008, Proposition 2.5). A C-approximate barycentric spanner is deï¬ned similarly
as Deï¬nition 4, except that the coefï¬cients are in the range [âˆ’C, +C]. This will only change our main results
by increasing them by a factor of C, and we may simply set C = 2 with minimal effects on our sample
complexity guarantees.
46

I.3
Properties of low-rank MDPs
Lemma 16. In the low-rank MDP (Assumption 1), for any h âˆˆ[H], function dhâˆ’1 : X â†’R, and pseudo-
policy Ï€ (Deï¬nition 1), we have
(PÏ€
hdh)(xh+1) =
ZZ
Ph(xh+1|xh, ah)Ï€h(ah|xh)dh(xh)(dxh)(dah) = âŸ¨Âµâˆ—
h(xh+1), Î¸h+1âŸ©
for some Î¸h+1 âˆˆRd with âˆ¥Î¸h+1âˆ¥âˆâ‰¤âˆ¥dhâˆ¥1.
Proof. By the deï¬nition of low-rank MDPs (Assumption 1), we have
PÏ€
hdh =
ZZ
Ph(xh+1|xh, ah)Ï€h(ah|xh)dh(xh)(dxh)(dah)
=
ZZ
âŸ¨Âµâˆ—
h(xh+1), Ï†âˆ—
h(xh, ah)âŸ©Ï€h(ah|xh)dh(xh)(dxh)(dah)
= âŸ¨Âµâˆ—
h(xh+1), Î¸h+1âŸ©,
where Î¸h+1 =
RR
Ï†âˆ—
h(xh, ah)Ï€h(ah|xh)dh(xh)(dxh)(dah) âˆˆRd. In addition,
âˆ¥Î¸h+1âˆ¥âˆâ‰¤
ZZ
âˆ¥Ï†âˆ—
h(xh, ah)âˆ¥âˆÏ€h(ah|xh)|dh(xh)|(dxh)(dah)
â‰¤
Z Z
Ï€h(ah|xh)(dah)

|dh(xh)|(dxh)
â‰¤
Z
|dh(xh)|(dxh) = âˆ¥dhâˆ¥1
where we use Lemma 21 in the last inequality.
Lemma 17. In low-rank MDPs (Assumption 1), given a dataset Dh satisfying Assumption 2 for h âˆˆ[H],
let dD
h and dD,â€ 
h
be the corresponding current-state and next-state data distributions. Then for the function
class
Fh =
n
dh = âŸ¨Âµâˆ—
hâˆ’1, Î¸hâŸ©: dh âˆˆâˆ†(X), Î¸h âˆˆRd, âˆ¥Î¸hâˆ¥âˆâ‰¤1
o
,
we have that dD
h âˆˆFh and dD,â€ 
h
âˆˆFh+1.
Proof. Recall that under Assumption 2, Dh is collected by Ïhâˆ’1 â—¦Ï€D
h where a0:hâˆ’1 âˆ¼Ïhâˆ’1, an (hâˆ’1)-step
non-Markov policy, and ah âˆ¼Ï€D
h , a Markov policy.
First we prove the lemma statement for dD,â€ 
h
. Since dD
h is a valid distribution and Ï€D
h is a valid Markov
policy, from Lemma 16 we know that dD,â€ 
h
= P
Ï€D
h
h (dD
h ) can be written as âŸ¨Âµâˆ—
h, Î¸h+1âŸ©with âˆ¥Î¸h+1âˆ¥âˆâ‰¤1.
Finally, since dD,â€ 
h
is a valid marginal distribution, dD,â€ 
h
âˆˆâˆ†(X), thus satisfying all constraints of Fh+1.
To prove the lemma statement for dD
h , we ï¬rst prove a variant of Lemma 16 for non-Markov policies.
With some overload of notation, let dD
hâˆ’1(xhâˆ’1) denote the marginal distribution of xhâˆ’1 induced by rolling
the non-Markov policy Ïhâˆ’1 to level h âˆ’1. Then
dD
h (xh) =
ZZ
Ph(xh|xhâˆ’1, ahâˆ’1)Ïhâˆ’1(ahâˆ’1|x0:hâˆ’1)dD
hâˆ’1(xhâˆ’1)(dxhâˆ’1)(dahâˆ’1).
Using similar steps as the proof of Lemma 16, we have that
dD
h (xh) =
ZZ
Ph(xh|xhâˆ’1, ahâˆ’1)Ïhâˆ’1(ahâˆ’1|x0:hâˆ’1)dD
hâˆ’1(xhâˆ’1)(dxhâˆ’1)(dahâˆ’1)
47

=
ZZ
âŸ¨Ï†âˆ—
hâˆ’1(xhâˆ’1, ahâˆ’1), Âµâˆ—
hâˆ’1(xh)âŸ©Ïhâˆ’1(ahâˆ’1|x0:hâˆ’1)dD
hâˆ’1(xhâˆ’1)(dxhâˆ’1)(dahâˆ’1)
= âŸ¨Âµâˆ—
hâˆ’1(xh), Î¸hâŸ©,
where Î¸h =
RR
Ï†âˆ—
hâˆ’1(xhâˆ’1, ahâˆ’1)Ïhâˆ’1(ahâˆ’1|x0:hâˆ’1)dD
hâˆ’1(xhâˆ’1)(dxhâˆ’1)(dahâˆ’1) âˆˆRd. Since dD
hâˆ’1 and
Ïhâˆ’1(Â·|x0:hâˆ’1) are valid probability distributions over states xh and actions ah, respectively, it is easy to see
that
âˆ¥Î¸hâˆ¥âˆâ‰¤
ZZ
âˆ¥Ï†âˆ—
hâˆ’1(xhâˆ’1, ahâˆ’1)âˆ¥âˆÏhâˆ’1(ahâˆ’1|x0:hâˆ’1)dD
hâˆ’1(xhâˆ’1)(dxhâˆ’1)(dahâˆ’1) â‰¤1
since âˆ¥Ï†âˆ—
hâˆ’1(Â·)âˆ¥âˆâ‰¤1 from Assumption 1. Finally, since dD
h is a valid distribution, we have dD
h âˆˆFh.
Lemma 18. In low-rank MDPs (Assumption 1), given a dataset Dh satisfying Assumption 2 for h âˆˆ[H],
let dD
h and dD,â€ 
h
be the corresponding current-state and next-state data distributions. Then for the function
class
Fh(Î¥hâˆ’1) =
n
dh = âŸ¨Âµhâˆ’1, Î¸hâŸ©: dh âˆˆâˆ†(X), Âµhâˆ’1 âˆˆÎ¥hâˆ’1, Î¸h âˆˆRd, âˆ¥Î¸hâˆ¥âˆâ‰¤1
o
,
we have that dD
h âˆˆFh(Î¥hâˆ’1) and dD,â€ 
h
âˆˆFh+1(Î¥h).
Proof. From Lemma 17 we know that dD
h âˆˆFh (where Fh is linear in the true features Âµâˆ—
hâˆ’1, as deï¬ned
in the Lemma 17), and dD,â€ 
h
âˆˆFh+1. Noting that Fh âŠ†Fh(Î¥hâˆ’1) and Fh+1 âŠ†Fh+1(Î¥h) completes the
proof.
Lemma 19. For h âˆˆ[H], suppose we have a dataset Dh satisfying Assumption 2, with corresponding data
distributions dD
h and dD,â€ 
h
. Given a function wh : X â†’[âˆ’Cx
h, Cx
h] and pseudo-policy Ï€ (Deï¬nition 1) with
Ï€h(a|x)
Ï€D
h (a|x) â‰¤Ca
h, âˆ€x âˆˆX, a âˆˆA, we have

PÏ€
h(dD
h wh)
dD,â€ 
h

âˆ
â‰¤Cx
hCa
h.
Proof. For any xh+1 âˆˆX, we have
 PÏ€
h
 dD
h wh

(xh+1) â‰¤Cx
h
 PÏ€
hdD
h

(xh+1)
= Cx
h
ZZ
Ph(xh+1|xh, ah)Ï€h(ah|xh)dD
h (xh)(dxh)(dah)
â‰¤Cx
hCa
h
ZZ
Ph(xh+1|xh, ah)Ï€D
h (ah|xh)dD
h (xh)(dxh)(dah)
= Cx
hCa
hdD,â€ 
h
(xh+1).
The last equality follows from the Bellman ï¬‚ow equation and Assumption 2. The convention that 0
0 = 0
gives the lemma statement.
Lemma 20. For any two state distributions dh, dâ€²
h and a pseudo-policy Ï€ (Deï¬nition 1), we have the fol-
lowing inequality
âˆ¥PÏ€
hdh âˆ’PÏ€
hdâ€²
hâˆ¥1 â‰¤âˆ¥dh âˆ’dâ€²
hâˆ¥1,
where we recall that (PÏ€
hdh)(xh+1) =
RR
Ph(xh+1|xh, ah)Ï€(ah|xh)dh(xh)(dxh)(dah).
48

Proof. From deï¬nition of PÏ€
h and Lemma 21, we have
âˆ¥PÏ€
hdh âˆ’PÏ€
hdâ€²
hâˆ¥1 =
ZZ Ph(xh+1|xh, ah)Ï€(ah|xh)
 dh(xh) âˆ’dâ€²
h(xh)

(dxh)(dah)
 (dxh+1).
â‰¤
Z 
|dh(xh) âˆ’dâ€²
h(xh)|
ZZ
Ï€(ah|xh)Ph(xh+1|xh, ah)(dxh+1)(dah)

(dxh)
â‰¤
Z
|dh(xh) âˆ’dâ€²
h(xh)|(dxh) = âˆ¥dh âˆ’dâ€²
hâˆ¥1.
Lemma 21. For any pseudo-policy Ï€ (Deï¬nition 1), we have
Z
Ï€h(ah|xh)(dah) â‰¤1
âˆ€xh âˆˆX, h âˆˆ[H].
Proof. Recall Ï€h(ah|xh) = min

Ï€h(ah|xh), Ca
hÏ€D
h (ah|xh)
	
where Ï€h is a valid Markov policy. Then
Z
Ï€h(ah|xh)(dah) =
Z
min

Ï€h(ah|xh), Ca
hÏ€D
h (ah|xh)
	
(dah) â‰¤
Z
Ï€h(ah|xh)(dah) = 1.
I.4
Covering lemmas
In this subsection, we provide the â„“1 optimistic cover lemma used in MLE (Lemma 22) and pseudo-
dimension bound for the weight function class (Lemma 23) respectively.
Lemma 22. Suppose Assumption 3 holds. Then for the function class
Fh(Î¥hâˆ’1) = {dh = âŸ¨Âµhâˆ’1, Î¸hâŸ©: Âµhâˆ’1 âˆˆÎ¥hâˆ’1, Î¸h âˆˆRd, âˆ¥Î¸hâˆ¥âˆâ‰¤1, dh âˆˆâˆ†(X)},
there exists an â„“1 optimistic cover Fh(Î¥hâˆ’1) (according to Deï¬nition 3) with scale Î³ of size |Î¥hâˆ’1| (2âŒˆBÂµ/Î³âŒ‰)d
and Fh(Î¥hâˆ’1) âŠ†(X â†’Râ‰¥0).
Proof. The ideas of this proof are adapted from the proof of Proposition H.15 in Chen et al. (2022a). Let
Î˜h = {Î¸h : âˆƒÂµhâˆ’1 âˆˆÎ¥hâˆ’1, s.t., âŸ¨Âµhâˆ’1, Î¸hâŸ©âˆˆFh(Î¥hâˆ’1)} âŠ†{Î¸h : Î¸h âˆˆRd, âˆ¥Î¸hâˆ¥âˆâ‰¤1} be the set of
Î¸h parameters associated with Fh(Î¥hâˆ’1). Then any dh âˆˆFh(Î¥hâˆ’1) can be written as âŸ¨Âµhâˆ’1, Î¸hâŸ©for some
Âµhâˆ’1 âˆˆÎ¥h and Î¸h âˆˆÎ˜h. Deï¬ne the Î³â€²-neighborhood of Î¸h to be B(Î¸h, Î³â€²) := Î³â€²âŒŠÎ¸h/Î³â€²âŒ‹+ [0, Î³â€²]d, and
construct the optimistic covering function for each dh = âŸ¨Âµhâˆ’1, Î¸hâŸ©as
fÂµhâˆ’1,Î¸h(x) =
max
Î¸âˆˆB(Î¸h,Î³â€²)
âŸ¨Âµhâˆ’1(x), Î¸âŸ©
âˆ€x âˆˆX.
Note that fÂµhâˆ’1,Î¸h â‰¥dh pointwise, thus fÂµhâˆ’1,Î¸h â‰¥0, though it is not necessarily a valid distribution.
Further,
âˆ¥fÂµhâˆ’1,Î¸h âˆ’dhâˆ¥1 â‰¤
Z
max
Î¸âˆˆB(Î¸h,Î³â€²)
|âŸ¨Î¸ âˆ’Î¸h, Âµhâˆ’1(x)âŸ©|(dx)
â‰¤
Z
max
Î¸âˆˆB(Î¸h,Î³â€²)
âˆ¥Î¸ âˆ’Î¸hâˆ¥âˆâˆ¥Âµhâˆ’1(x)âˆ¥1(dx)
â‰¤Î³â€²
Z
âˆ¥Âµhâˆ’1(x)âˆ¥1(dx)
â‰¤Î³â€²BÂµ
49

using Assumption 3 in the last line. Observe that there are at most (2âŒˆ1/Î³â€²âŒ‰)d unique Î³â€²-neighborhoods in
the set {B(Î¸h, Î³â€²)}Î¸hâˆˆÎ˜h. This implies that there are at most |Î¥hâˆ’1| (2âŒˆ1/Î³â€²âŒ‰)d unique functions in the set
{fÂµhâˆ’1,Î¸h}âŸ¨Âµhâˆ’1,Î¸hâŸ©âˆˆFh(Î¥hâˆ’1), which forms an â„“1-optimistic cover of Fh(Î¥hâˆ’1) of scale Î³â€². Finally, setting
Î³â€² = Î³/BÂµ gives us an â„“1-optimistic covering of Fh(Î¥hâˆ’1) of scale Î³ with size |Î¥hâˆ’1| (2âŒˆBÂµ/Î³âŒ‰)d.
Lemma 23. For any h âˆˆ[H] and density feature Âµhâˆ’1 âˆˆÎ¥hâˆ’1, the function class
Wh(Âµhâˆ’1) =

wh =
âŸ¨Âµhâˆ’1, Î¸up
h âŸ©
âŸ¨Âµhâˆ’1, Î¸down
h
âŸ©: âˆ¥whâˆ¥âˆâ‰¤Cx
hâˆ’1Ca
hâˆ’1, Î¸up
h , Î¸down
h
âˆˆRd

.
has pseudo-dimension (Deï¬nition 6) bounded as Pdim(Wh(Âµhâˆ’1)) â‰¤4(d + 1) log(8e).
Proof. For any h and Âµh, consider the unconstrained version Wâ€²
h(Âµhâˆ’1) of Wh(Âµhâˆ’1):
Wâ€²
h(Âµhâˆ’1) =

w =
âŸ¨Âµhâˆ’1, Î¸up
h âŸ©
âŸ¨Âµhâˆ’1, Î¸down
h
âŸ©: Î¸up
h , Î¸down
h
âˆˆRd

.
Clearly, Wh(Âµhâˆ’1) âŠ†Wâ€²
h(Âµhâˆ’1), thus Pdim(Wh(Âµhâˆ’1)) â‰¤Pdim(Wâ€²
h(Âµhâˆ’1)), and Pdim(Wâ€²
h(Âµhâˆ’1)) =
VCdim(HWâ€²
h(Âµhâˆ’1)), where HWâ€²
h(Âµhâˆ’1) = {h = sign(w âˆ’c) : w âˆˆWâ€²
h(Âµhâˆ’1), c âˆˆR}. We will use
Lemma 24 to bound VCdim(HWâ€²
h(Âµhâˆ’1)). Any h(x) âˆˆHWâ€²
h(Âµhâˆ’1) may be written as the following Boolean
formula
Î¦ = 1
 âŸ¨Âµhâˆ’1(x), Î¸up
h âŸ©
âŸ¨Âµhâˆ’1(x), Î¸down
h
âŸ©âˆ’c â‰¥0

=
 
1
" d
X
i=1
Âµhâˆ’1(x)[i]Î¸up
h [i] âˆ’c
d
X
i=1
Âµhâˆ’1(x)[i]Î¸down
h
[i] â‰¥0
#
1 âˆ§
" d
X
i=1
Âµhâˆ’1(x)[i]Î¸down
h
[i] â‰¥0
#!
âˆ¨
 
1
" d
X
i=1
Âµhâˆ’1(x)[i]Î¸up
h [i] âˆ’c
d
X
i=1
Âµhâˆ’1(x)[i]Î¸down
h
[i] â‰¤0
#
âˆ§1
" d
X
i=1
Âµhâˆ’1(x)[i]Î¸down
h
[i] < 0
#!
which involves k = 2d + 1 real variables, a polynomial degree of at most l = 1 in these variables, and
s = 4 atomic predicates. Then from Lemma 24, Pdim(Wh(Âµhâˆ’1))) â‰¤VCdim(HWâ€²
h(Âµhâˆ’1)) â‰¤4(d +
1) log(8e).
Lemma 24 (Theorem 2.2 of Goldberg and Jerrum (1993)). Let Ck,m be a concept class where concepts
and instances are represented by k and m real values, respectively. Suppose that the membership test for
any instance c in any concept C of Ck,m can be expressed as a Boolean formula Î¦k,m containing s distinct
atomic predicates, each predicate being a polynomial inequality over k + m variables of degree at most l.
Then the VC dimension of Ck,m is bounded as VCdim(Ck,m) â‰¤2k log(8els).
I.5
Probabilistic tools
In this section, we deï¬ne standard tools from statistical learning theory (Anthony and Bartlett, 2009; Vapnik,
1998) that we use in our proofs. We note that, for convenience, we may override some notations from the
main paper, e.g., Îµ does not refer to the same thing as in other sections.
Deï¬nition 5 (VC-dimension). Let F âŠ†{âˆ’1, +1}X and xm
1 = (x1, . . . , xm) âˆˆX m. We say xm
1 is shattered
by F if âˆ€b âˆˆ{âˆ’1, +1}m, âˆƒfb âˆˆF such that (fb(x1), . . . , fb(xm)) = (b1, . . . , bm) âˆˆRm. The Vapnik-
Chervonenkis (VC) dimension of F is the cardinality of the largest set of points in X that can be shattered
by F, that is, dim(F) = max{m âˆˆN | âˆƒxm
1 âˆˆX m, s.t. xm
1 is shattered by F}.
50

Deï¬nition 6 (Pseudo-dimension). Let F âŠ†RX and xm
1 = (x1, . . . , xm) âˆˆX m. We say xm
1 is pseudo-
shattered by F if âˆƒc = (c1, . . . , cm) âˆˆRm such that âˆ€y = (y1, . . . , ym) âˆˆ{âˆ’1, +1}m, âˆƒfy âˆˆF
such that sign(fy(xi âˆ’ci) = yi âˆ€i âˆˆ[m]. The pseudo-dimension of F is the cardinality of the largest
set of points in X that can be pseudo-shattered by F, that is, Pdim(F) = max{m âˆˆN | âˆƒxm
1
âˆˆ
X m, s.t. xm
1 is pseudo-shattered by F}.
Deï¬nition 7 (Uniform covering number). For p = 1, 2, âˆ, the uniform covering number of H w.r.t. the
norm âˆ¥Â· âˆ¥p is deï¬ne as
Np(Îµ, H, m) = max
xm
1 âˆˆX m Np(Îµ, H, xm
1 )
where Np(Îµ, H, xm
1 ) is the Îµ-covering number of H|xm
1 w.r.t. âˆ¥Â· âˆ¥p, that is, the cardinality of the smallest set
S such that for every h âˆˆH|xm
1 , âˆƒs âˆˆS such that âˆ¥h âˆ’sâˆ¥p < Îµ.
Lemma 25 (Bounding uniform covering number by pseudo-dimension, Corollary 42 of Modi et al. (2021)).
Given a hypothesis class H âŠ†(Z â†’[a, b]), for any m âˆˆN we have
N1(Îµ, H, m) â‰¤
4e2(b âˆ’a)
Îµ
Pdim(H)
.
Lemma 26 (Uniform deviation bound using covering number, adapted from Corollary 39 of Modi et al.
(2021)). For b â‰¥1, let H âŠ†(Z â†’[âˆ’b, b]) be a hypothesis class and Zn = (z1, . . . , zn) be i.i.d. samples
drawn from some distribution P(z) supported on Z. Then
P
 E[h(z)] âˆ’1
n
n
X
i=1
h(zi)
 â‰¥Îµ
!
â‰¤36N1

Îµ3
640b2 , H, 40nb2
Îµ2

exp

âˆ’
nÎµ2
128V[h(z)] + 512Îµb

.
51

