Sum-product networks: A survey
Iago París, Raquel Sánchez-Cauce, Francisco Javier Díez∗
Abstract—A sum-product network (SPN) is a proba-
bilistic model, based on a rooted acyclic directed graph,
in which terminal nodes represent univariate proba-
bility distributions and non-terminal nodes represent
convex combinations (weighted sums) and products of
probability functions. They are closely related to prob-
abilistic graphical models, in particular to Bayesian
networks with multiple context-speciﬁc independen-
cies. Their main advantage is the possibility of build-
ing tractable models from data, i.e., models that can
perform several inference tasks in time proportional
to the number of links in the graph. They are some-
what similar to neural networks and can address the
same kinds of problems, such as image processing and
natural language understanding. This paper oﬀers a
survey of SPNs, including their deﬁnition, the main
algorithms for inference and learning from data, the
main applications, a brief review of software libraries,
and a comparison with related models.
Index Terms—Sum-product networks, probabilistic
graphical models, Bayesian networks, machine learn-
ing, deep neural networks.
I. Introduction
S
UM-PRODUCT networks (SPNs) were proposed by
Poon and Domingos [1] in 2011 as a modiﬁcation of
Darwiche’s [2], [3] arithmetic circuits. Every SPN consists
of a directed graph that represents a probability distribu-
tion resulting from a hierarchy of distributions combined
in the form of mixtures (sum nodes) and factorizations
(product nodes), as shown in Figure 1. SPNs, like arith-
metic circuits, can be built by transforming a probabilistic
graphical model [4], such as a Bayesian network or a
Markov network, but they can also be learned from data.
The main advantage of SPNs is that several inference tasks
can be performed in time proportional to the number of
links in the graph.
In this decade there has been great progress: numerous
algorithms have been proposed for inference and learn-
ing, and SPNs have been successfully applied to many
problems, including computer vision and natural language
processing, in which probabilistic models could not com-
pete with neural networks. The understanding of SPNs has
also improved and some aspect can now be explained more
clearly than in the original publications. For example, the
ﬁrst two papers about SPNs [1], [5] presented them as an
eﬃcient representation of network polynomials, while most
of the later references, beginning with [6], deﬁne them as
the composition of probability distributions, which is, in
Department of Artiﬁcial Intelligence, UNED, 28040 Madrid, Spain
(e-mail: {iagoparis, rsanchez, fjdiez}dia.uned.es).
∗Corresponding
author.
Submitted to IEEE Transactions on Pattern Analysis and Machine
Intelligence. Comments are welcome.
our view, more intuitive and much easier to understand.
Consistency was initially one of the deﬁning properties
of SPNs, which made them more general than arithmetic
circuits, but it later became clear that decomposability, a
stronger but much more intuitive property, suﬃces to build
SPNs for practical applications. In contrast, selectivity
(called determinism in arithmetic circuits), which was not
mentioned in the original paper [1], proved to be relevant
for some inference tasks and for parameter learning [7],
[8]. Additionally, some of the algorithms for SPNs are only
sketched in [1], without much detail or formal proofs, and
one of them turned out to be correct only for selective
SPNs. Other basic algorithms are scattered over several
papers, each using a diﬀerent mathematical notation.
n1
+
n3
x
n2
x
n6
+
n7 +
n10
x
n9
x
n8
x
n11
x
n15 +
n16
+
n14 +
=
B
A
C
n4
n5
n12
n13
n18
n19
n17
+
P(¬a)
P(+a)
P(+b|¬a)
P(¬b|+a)
P(+b|+a)
P(¬b|¬a)
P(+c|+a,+b)
P(+c|+a,¬b)
P(+c|¬a, +b)
P(¬c|+a,+b)
P(¬c|+a,¬b)
P(¬c|¬a, +b)
P(¬c|¬a, ¬b)
P(+c|¬a, ¬b)
Figure 1. A Bayesian network (left) and an equivalent SPN (right).
The 6 terminal nodes in the SPN are indicators for the 3 variables
in the model, A, B, and C; they are the input of the SPN for every
conﬁguration of these variables, including partial conﬁgurations. The
root node, n1, computes the joint and marginal probabilities.
For these reasons we decided to write a survey ex-
plaining the main concepts and algorithms for SPNs with
mathematical rigor. We have intentionally avoided any
reference to network polynomials, which has forced us to
develop new proofs for some algorithms and propositions,
alternative to those found in other references, such as
[9]. We have also reviewed the literature on SPNs, with
arXiv:2004.01167v1  [cs.LG]  2 Apr 2020

2
especial emphasis on their applications. However, we are
aware that some relevant references may have been omit-
ted, especially those published recently in this thriving
research area.
The rest of this paper is structured as follows. The two
subsections of this introduction highlight the signiﬁcance
of SPNs by comparing them with probabilistic graphi-
cal models and neural networks respectively. After some
mathematical preliminaries (Sec. II), we introduce the
basic deﬁnitions of SPNs (Sec.III) and the main algorithms
for inference (Sec.IV), parameter learning (Sec. V), and
structural learning (Sec. VI). We then review some appli-
cations in several areas (Sec. VII), a few open-source pack-
ages (Sec. VIII), and some extensions of SPNs (Sec. IX).
Section X contains the conclusions, Appendix A compares
SPNs with arithmetic circuits, Appendix B analyzes the
interpretation of sum nodes as weighted averages of condi-
tional probabilities, and Appendix C contains the proofs
of all the propositions.
A. SPNs vs. probabilistic graphical models (PGMs)
SPNs are similar to PGMs, such as Bayesian networks
(BNs) and Markov networks (also called Markov ran-
dom ﬁelds) [10], [4], in their ability to compactly rep-
resent probability distributions. The main diﬀerence is
that in a PGM every node represents a variable and—
roughly speaking—links represent probabilistic dependen-
cies, sometimes due to causal inﬂuences, while in an SPN
every node represents a probability function. PGMs and
other factored probability distributions can be compiled
into arithmetic circuits or SPNs [11]. In general a PGM
is more compact than an equivalent SPN, as shown in
Figure 1. Inference in BNs and Markov networks is an
NP-problem and existing exact algorithms have worst-case
exponential complexity [4], while SPNs can do inference in
time proportional to the number of links in the graph. In
principle this would not be an advantage for SPNs, because
the conversion of a BN or a Markov network into an
SPN may create an exponential number of links. However,
context-speciﬁc independencies in BNs [12] can reduce the
size of the corresponding SPN, as shown in Figure 2.
In fact, any problem solvable in polynomial time with
tabular BNs (i.e., those whose conditional probabilities are
given by tables) can be solved in polynomial time with
arithmetic circuits or SPNs [3], but the converse is not
true—see the example in [1].1
More importantly, while PGMs learned from data are
usually intractable—except for small problems or for spe-
ciﬁc types of models with limited expressiveness, such as
the naïve Bayes—the algorithms presented in Section VI
can build tractable SPNs that yield excellent approxima-
tions both for generative and discriminative tasks.
1However, every SPN of ﬁnite-state variables can be converted
into a BN whose conditional probabilities are encoded as algebraic
decision diagrams (ADDs), in time and space proportional to the
network size, and it is possible to recover the original SPN with the
variable elimination algorithm [13].
n1
+
n3
x
n2
x
n6
+
n7
+
n10
x
n9
x
n8
x
n11
x
n15 +
n16
+
n14 +
n4
n5
n12
n13
n17
n18
0
1
0.7
0.2
0.9
0.9
0
0.7
0
0
0.35
0.36
0
0
1
1
0.36
0.108
0.7
0.3
0.5
0.6
0.4
0.5
0.1
0.8
0.3
0.9
0.2
0.7
Figure 2.
If P(c | b, +a) = P(c | b, ¬a) for every value of B and C
(context-speciﬁc independence), nodes n16 and n17 in Figure 1 can
be coalesced into node n16 in this ﬁgure. The numbers in red are the
values Si(v) for v = (+a, +b, ¬c).
In contrast, BNs can be built using causal knowledge
elicited from human experts and there is a large body of
recent research on building causal BNs from experimental
and/or observational data, under certain conditions [14].
It is also possible to combine causal knowledge and data,
and even to build BNs interactively [15]. All these options
are currently impossible with SPNs. Additionally, the
independencies in a BN or in a Markov model are easier to
read than those in an SPN. On the other hand, the graph
of an SPN can sometimes be built from human knowledge
to represent part/subpart and class/subclass hierarchies—
see [1, Sec. 5] for an example.
In conclusion, each type of model has advantages and
disadvantages, and the choice for a real-world application
must take into account the size of the problem, the amount
of knowledge and data available, and the explanations
required by the user.
B. SPNs vs. neural networks
SPNs can be seen as a particular type of feedforward
neural networks because there is a ﬂow of information from
the input nodes (the leaves) to the output node (the root),
but in this paper we reserve the term “neural network”
(NN) for the models structured in layers connected by the
standard operators: sigmoid, ReLU, softmax, etc.
The main diﬀerence is that SPNs have a probabilistic
interpretation while standard NNs do not. Inference is

3
also diﬀerent: computing a posterior probability requires
two passes, and ﬁnding a the most probable explanation
(MPE) requires a backtrack from the root to the leaves,
as explained in Section IV. Additionally, SPNs can do
inference with partial information (i.e., when the values
of some of the variables are unknown), while in a NN it is
necessary to assign a value to each input node.
From the point of view of parameter learning, NNs are
usually trained with gradient descent or variations thereof,
while SPNs can also be trained with several probabilistic
algorithms, such as EM and Bayesian methods, which are
much more eﬃcient and have lower risk of overﬁtting (cf.
Sec. V).
When building practical applications, the main diﬀer-
ence is the possibility of determining the structure of an
SPN from data, looking for a balance between model
complexity and accuracy. In contrast, NNs are usually
designed by hand and it is necessary to examine diﬀerent
architectures of diﬀerent sizes with diﬀerent hyperparam-
eters, in a trial-and-error approach, until a satisfactory
model is found. For this reason, NNs that have succeeded
in practical applications are usually very big and training
them requires huge computational power. There are some
proposals to learn the structure of NNs using evolutionary
computation, which yields more eﬃcient graphs, but this
also requires immense computational power [16], [17].
In spite of these advantages, NNs are still superior to
SPNs in many tasks. For example, in 2012 an SPN by Gens
and Domingos [1] achieved a classiﬁcation accuracy of 84%
for the CIFAR-10 image dataset, one of the highest scores
at the time. However, deep NNs have amply surpassed that
result, reaching an impressive 99.3% accuracy.2
Nevertheless, in Section VII we mention several ap-
plications in which SPNs are competitive with NNs and
superior in some aspects. For instance, random tensorized
SPNs (RAT-SPNs) [18] have recently attained a classi-
ﬁcation accuracy comparable to deep NNs for MNIST
and other image datasets, with the advantages of being
interpretable as a probabilistic generative model and much
more robust to missing features. For another recent exam-
ple, Stelzner et al. [19] proved that the attend-infer-repeat
(AIR) framework used for object detection and location
is much more eﬃcient when the variational autoencoders
(VAEs) are replaced by SPNs: they achieved an improve-
ment in speed of an order of magnitude, with slightly
higher accuracy, as well as robustness against noise. Other
examples can be found in Sections VII and IX.
For a more detailed comparison of SPNs with NNs,
VAEs, generative adversarial networks (GANs), and other
models, see [18].
II. Mathematical preliminaries
In this paper we assume that every variable either has a
ﬁnite set of possible values, called states, or is continuous,
i.e., takes values in R.
2See
https://paperswithcode.com/sota/image-classiﬁcation-on-
cifar-10.
A. Conﬁgurations of variables
We denote by a capital letter, V , a variable and by the
corresponding lowercase letter, v, any value of V . Similarly
a boldface capital letter denotes a set of variables, V =
{V1, . . . , Vn}, the corresponding lowercase letter denotes
any of its conﬁgurations, v = (v1, . . . , vn), and conf(V) is
the set of all the conﬁgurations of V. The empty set has
only one conﬁguration, represented by ♦.
We denote by conf∗(V) the set of all the conﬁgurations
of V and its subsets:
conf∗(V) = {x | X ⊆V} .
(1)
Put another way,
conf∗(V) =
[
X∈P(V)
conf(X)
(2)
We can think of conf∗(V) \ conf(V) as the set of partial
conﬁgurations of V, i.e., the conﬁgurations in which only
some of the variables in V have an assigned value.
If X ⊆V, the projection (sometimes called restriction)
of a conﬁguration v of V onto X, v↓X, is the conﬁguration
of X such that every variable V
∈X takes the same
value as in v; for example, (+x1, +x2, ¬x3)↓{X1,X3} =
(+x1, ¬x3) and (+x1, +x2, ¬x3)↓∅= ♦. In order to sim-
plify the notation, when X has a single variable, V , we
will write v instead of (v) and v↓V instead of v↓{V }; for
example, (+x1, +x2, ¬x3)↓X2 = +x2.
Given two conﬁgurations, x and y, of two disjoint
sets, X and Y, the composition of them, denoted by xy,
is the conﬁguration of X ∪Y such that (xy)↓X = x
and (xy)↓Y
=
y. For example, (+x1, +x2)(¬x3)
=
(+x1, +x2, ¬x3).
When X ⊆V, a conﬁguration x is compatible with
conﬁguration v if x = v↓X, i.e., if every variable V ∈X has
the same value in both conﬁgurations. All conﬁgurations
are compatible with ♦.
Deﬁnition 1. Given a value v of a ﬁnite-state vari-
able V
∈
V, we deﬁne the indicator function, Iv
:
conf(V) 7→{0, 1}, as follows:
Iv(x) =
(
1
if V /∈X ∨v = x↓V
0
otherwise .
(3)
If all the variables in V are binary, then there are 2n
indicator functions.
Example 2. If V = {V0, V1} and the domains of these
variables are {+v0, ¬v0} and {+v1, ¬v1} respectively, then
I+v0(+v0, +v1) = 1, I+v0(¬v0, +v1) = 0, I+v0(+v1) =
I+v0(¬v1) = I+v0(♦) = 1, etc.
B. Probability distributions and probability functions
Deﬁnition 3. A probability distribution deﬁned on V is
a function P : conf(V) 7→R such that:
P(v) ≥0 ,
(4)
X
v
P(v) = 1 .
(5)

4
This deﬁnition can be extended so that P represents
not only a probability distribution but also all its marginal
probabilities, as follows.
Deﬁnition 4. A probability function deﬁned on V is a
function P : conf∗(V) 7→R such that the restriction
of P to conf(V) is a probability distribution and for every
conﬁguration x such that X ⊂V,
P(x) =
X
v|v↓X=x
P(v) .
(6)
This equation is the deﬁnition of marginal probability:
P(x) is obtained by summing the probabilities of all the
conﬁgurations of V compatible with x.
Proposition 5. If P is a probability function deﬁned on V
and X ⊂V, the restriction of P to conf(X) is a probability
distribution for X.
It is possible to deﬁne a new probability function as the
sum or the product of other probability functions.
Proposition 6. Let us consider n probability functions
{P1, . . . , Pn} deﬁned on the same set of variables, V, and
n weights, {w1, . . . , wn}, with wj ≥0 for every j, and
Pn
j=1 wj = 1. The function P : conf∗(V) 7→R, such that
for every conﬁguration of X ⊆V
P(x) =
n
X
j=1
wj · Pj(x) ,
(7)
is a probability function. It is said to be a weighted average
or a convex combination of probability functions.
Proposition
7. Let {P1, . . . , Pn} be a set of prob-
ability functions deﬁned on n disjoint sets of vari-
ables, {V1, . . . , Vn}, respectively. Let V = V1 ∪. . . ∪Vn.
The function P : conf∗(V) 7→R, such that for every
conﬁguration of X ⊆V
P(x) =
n
Y
j=1
Pj(x) ,
(8)
is a probability function.
C. MAP, MPE, and MAX inference
In some inference tasks e denotes the evidence, i.e., the
values observed (for example, the symptoms and signs of a
medical examination or the pixels in an image) and X the
variables of interest (for example, the possible diagnostics
or the objects that may be present in the image), with
X∩E = ∅. In this context, P(x | e) is called the posterior
probability.
The maximum a-posteriori (MAP) conﬁguration is
MAP(e, X) = arg max
x
P(x | e) .
(9)
Therefore, MAP inference divides the variables into three
disjoint sets: observed variables (E), variables of interest
(X), and hidden variables (H = V \ (E ∪X)).
The most probable explanation (MPE) is the conﬁgura-
tion of X = V\E that maximizes the posterior probability:
MPE (e) = arg max
x
P(x | e) .
(10)
MPE is a special case of MAP in which H = ∅, i.e., every
variable that is not observed is a variable of interest. In
general, MAP inference is much harder than MPE [20].
Finally, MAX is a special case of MPE in which all the
variables are of interest, i.e., X = V and H = E = ∅.
The MAX conﬁguration is the conﬁguration of X that
maximizes the probability:
MAX (x) = arg max
x
P(x) .
(11)
MPE and MAP are relevant when we wish to know the
most probable conﬁguration of the variables of interest X
(for example, the possible diagnostics), which is diﬀerent
from ﬁnding the most probable value for each variable
in X, as we will see in Example 26. MAP is relevant
when some unobserved variables are not of interest; for
example, H may represent the tests not performed: these
variables are neither observed nor part of the diagnosis.
See also [4, Secs. 2.1.5.2 and 2.1.5.3], where MPE and
MAP are called “MAP” and “marginal MAP” respectively.
The deﬁnition of MAX will be useful in Section IV-C.
D. Basic deﬁnitions about graphs
Graphs have many applications in computer science. We
describe here the type of graph used to build SPNs.
A directed graph consists of a set of nodes and a set
of directed links. When there is a link ni →nj we say
that ni is a parent of nj and nj is a child of ni; there
cannot be another link from ni to nj. Given a node ni,
we denote by pa(i) the set of indices of its parents and
by ch(i) the set of indices of its children. For example, in
Figure 1, ch(1) = {2, 3}. Node nk is a descendant of ni if
it is a child of ni or a child of a descendant of ni; we also
say that ni is an ancestor of nk.
A cycle of length l consists of a set of l nodes and l links
{n1 →n2, n2 →n3, . . . , nl−1 →nl, nl →n1}. A graph
that contains no cycles, i.e., no node is a descendant of
itself, is acyclic. An acyclic directed graph (ADG) is rooted
if there is only one node (the root, denoted by nr) having
no parents. Terminal nodes, also called leaves, are those
that do not have children.
A directed tree is a rooted ADG in which every node has
one parent, except the root. In this paper when we say “a
tree” we mean “a directed tree”.
III. Basic definitions of SPNs
A. Structure of an SPN
An SPN S consists of a rooted acyclic directed graph
such that:
• every leaf node represents a univariate probability
distribution,
• all the other nodes are either of type sum or product,

5
• all the parents of a sum node (if any) are product
nodes, and vice versa, and
• every link ni →nj outgoing from a sum node has an
associated weight, wij ≥0.
Usually wij > 0. We will assume, unless otherwise stated,
that all SPNs are normalized, i.e.,
∀i,
X
j∈ch(i)
wij = 1 .
The probability distribution of each leaf node is deﬁned
on a variable V , which can have ﬁnite states or be continu-
ous. In the ﬁrst case, the distribution is usually degenerate,
i.e., there is a particular value v∗of V such that P(v∗) = 1
and P(v) = 0 otherwise. In the graphical representation
this leaf is denoted by the indicator Iv∗, sometimes written
I(v∗), as in Figures 1 and 2. When V
is continuous,
the probability distribution can be Gaussian [6], [21],
Poisson [22], piecewise polynomial [23], etc. (SPNs can
be further generalized by allowing each terminal node to
represent a multivariate probability density—for example,
a multivariate Gaussian [24], [25] or a Chow-Liu tree [26].)
An SPN can be built bottom-up beginning with sub-
SPNs of one node and joining them with sum and product
nodes. All the deﬁnitions of SPNs can be established
recursively, ﬁrst for one-node SPNs, and then for sum and
product nodes. Similarly, all the properties of SPNs can
be proved by structural induction.
If the probability distribution of a leaf node is deﬁned
on V , its scope is the set {V }. If ni is a sum or a product
node, its scope is the union of the scopes of its children:
sc(ni) =
[
j∈ch(i)
sc(nj) .
(12)
The scope of an SPN, denoted by sc(S), is the scope of
its root, sc(nr). The variables in the scope of an SPN are
sometimes called model variables—in contrast with latent
variables, which we present below. We deﬁne conf(S) =
conf(sc(S)) and conf∗(S) = conf∗(sc(S)).
A sum node is complete if all its children have the
same scope. An SPN is complete if all its sum nodes are
complete. (In arithmetic circuits this property is called
smoothness.)
A product node is decomposable if its children have
disjoint scopes. An SPN is decomposable if all its product
nodes are decomposable.
Proposition 8. A product node ni is decomposable if and
only if no node in the SPN is a descendant of two diﬀerent
children of ni.
In the rest of the paper we assume that all the SPNs
are complete and decomposable.
B. Node values and probability distributions
In the following expressions we assume that all the
variables have ﬁnite states. If V were continuous, we would
write p(v) instead of P(v).
Deﬁnition 9 (Value Si(x)). Let ni be a node of S and
x ∈conf∗(S). If ni is a leaf node that represents P(v) then
Si(x) =
(
P(x↓V )
if V ∈X
1
otherwise ;
(13)
if it is a sum node,
Si(x) =
X
j∈ch(i)
wij · Sj(x) ,
(14)
and if it is a product node,
Si(x) =
Y
j∈ch(i)
Sj(x) .
(15)
Because of Equations 3 and 13, if P(v) is degenerate,
with P(v∗) = 1, then Si(x) = Iv∗(x), which justiﬁes using
indicators as leaf nodes.
Deﬁnition 10 (Value S(x)). The value S(x) returned by
the SPN is the value of the root, Sr(x).
Theorem 11. For a node ni in an SPN, the function Pi :
conf(S) 7→R, such that
Pi(x) = Si(x) ,
(16)
is a probability function deﬁned on V = sc(ni).
Please note that Pi is deﬁned on the conﬁgurations of
the scope of the node, conf∗(sc(ni)), while Si is deﬁned
on the conﬁgurations of the scope of the whole network,
conf∗(S).
The probability function P for the SPN is P(x) =
Pr(x). The above theorem guarantees that the SPN
properly computes a probability distribution and all its
marginal probabilities. The proof of the theorem, in Ap-
pendix C, relies on the completeness and decomposability
of the SPN. We oﬀer there some counterexamples showing
that if an SPN is not complete or decomposable then S(x)
is not necessarily a probability function. This theorem
would still hold if we replaced decomposability with a
weaker condition, consistency [1], but this would compli-
cate the deﬁnition of SPN without oﬀering any practical
advantage.
C. Selective SPNs
We introduce now a particular type of SPNs that have
interesting properties for MPE inference and parameter
learning and for the interpretation of sum nodes.
When computing S(x) for a given x ∈conf∗(S), prob-
ability ﬂows from the leaves to the root (cf. Def. 9).
Equation 14 says that all the children of a sum node ni can
contribute to Si(x). However, ni may have the property
that for every conﬁguration v ∈conf(S) at most one child
makes a positive contribution, i.e., Sj(x) = 0 for the other
children of ni. We then say that ni is selective [27]. The
formal deﬁnition is as follows.
Deﬁnition 12. A sum node ni in an SPN is selective if
∀v ∈conf(S), ∃j∗∈ch(i) | j ∈ch(i), j ̸= j∗⇒Sj(v) = 0 .
(17)

6
Please note that this deﬁnition says “conf”, not “conf∗”.
Therefore even if ni is selective there may be a partial
conﬁguration x ∈conf∗(S) \ conf(V) such that several
children of ni make positive contributions to Si(x).
Deﬁnition 13. An SPN is selective if all its sum nodes
are selective.
Arithmetic circuits satisfying this property are said to
be deterministic.
Example 14. Given the SPN in Figure 2, we can check
that if v = (+a, +b, ¬c) then S2(v) = 0.36 and S3(v) = 0.
Only node n2 makes a positive contribution to S1(v), so
Property 17 holds for this v with j∗= 2. We can make the
same check for each of the 6 sum nodes and each of the
8 conﬁgurations of {A, B, C} in order to conclude that
this SPN is selective. However, instead of making these
48 checks, in the next section we show that for this SPN
selectivity is a consequence of the structure of its graph,
regardless of its numerical parameters (the weights).
D. Sum nodes that represent model variables
Several papers about SPNs say that sum nodes rep-
resent latent random variables. However, in this section
we show that in some cases sum nodes represent model
variables.
Deﬁnition 15. Let ni be a sum node having m children
and V
∈sc(ni) a variable with m states. Let σ be
a one-to-one function σ : {1, . . . , m} 7→ch(i). If for
every m ∈{1, . . . , m} either
Ivj is a child of nσ(j) (and
hence a grandchild of ni) or Ivj = nσ(j) (i.e., the indicator
itself is a child of ni), we then say that
ni represents
variable V .
Example 16. Node n14 in Figure 1 represents variable C,
with σ(1) = 17, σ(2) = 18, because Ic1 = I+c = n17 =
nσ(1), and Ic2 = I¬c = n18 = nσ(2). Nodes 15, 16, and 17
also represent C for the same reason.
Node n6 represents variable B with σ(1) = 8 and σ(2) =
9 because Ib1 = I+b is a child of nσ(1) = n8 and Ib2 = I¬b
is a child of nσ(2) = n9. For analogous reasons node n7
also represents B and n1 represents A.
In Appendix B we prove that when the root node of an
SPN represents a variable V , this node can be interpreted
as the weighted average of the conditional probabilities
given V , with wi,σ(j) = P(vj). Similarly, if every ancestor
of a sum node ni represents a variable, then ni can be
interpreted as the weighted average of the conditional
probabilities given the context deﬁned by the ancestors
of ni.
Proposition 17. If a sum node ni represents a model
variable V , then ni is selective.
This proposition is useful because it establishes a suf-
ﬁcient condition for concluding that an SPN is selective
by just observing its graph, without analyzing its weights.
For example, we can see that, according to Deﬁnition 15,
nl +
ni +
nk
x
nj x
nj'
x
nk'
x
...
...
...
S
...
nl +
ni +
nk
x
nj x
nj'
x
nk'
x
ni' +
...
...
...
S'
...
Figure 3. Augmentation of an SPN, assuming that ni is not selective
in S. This process adds an indicator Iz(j) for every child nj of ni.
Node ni′ is added to restore the completeness of nl in S′.
every node in Figures 1 and 2 represents a variable,
which implies that every node is selective and consequently
both SPNs are selective. In the next section we will use
this property to transform any non-selective SPN into
selective.
E. Augmented SPN
The goal of augmenting a non-selective SPN S [9], [7] is
to transform it into a selective SPN S′ that represents the
same probability function. For every non-selective node ni
in S the augmentation consists in adding a new ﬁnite-state
variable Z so that ni represents Z in S′. The process is as
follows. For every child nj we add a state z(j) to Z. If nj is
a product node, we add the indicator Iz(j) as a child of nj,
as shown in Figure 3; if nj is a terminal node, we insert
a product node, make nj a child of the new node (instead
of being a child of ni) and add Iz(j) as the second child
of the new node. In the resulting SPN, S′, ni represents
variable Z (because of Def. 15) and is therefore selective.
However this transformation of the SPN may cause
an undesirable side eﬀect. Let us assume, as shown in
Figure 3, that ni has a parent, nk, and nl is a parent
of both nk and nk′. Even though nl was complete in S,
the addition of Z has made this node incomplete in S′
because Z ∈sc(ni) and Z ∈sc(nk) but Z /∈sc(nk′).
It is then necessary to make Z ∈sc(nk′) in order to
restore the completeness of nl. So we create a new sum
node, ni′, and make it a parent of all the indicators of Z,
{Iz1, . . . , Izm} (see again Fig. 3); the weights for ni′ can be
chosen arbitrarily provided that they are all non-negative
and their sum is 1. If nk′ is a product node, then we
add ni′ as a child of nk′. If nk′ is a terminal node, we
insert a product node, making both ni′ and nk′ children
of this new node. If nl has other children, such as nk′′, or
ancestral sum nodes in S, then we must make each one of
them a parent or a grandparent of n′
i, as we did for n′
k.
Given that variable Z was not in the scope of the original
SPN, we can say that Z was latent in S and the augmen-
tation of ni has made it explicit. The SPN S′ obtained by

7
augmenting all the non-selective nodes is said to be the
augmented version of S. Therefore, sc(S′) = sc(S) ∪Z,
where Z contains one variable for each sum node that was
not selective in S.3
Proposition 18. If S′ is the augmented version of S,
then S′ is complete, decomposable, and selective, and
represents the same probability function for sc(S), i.e., if
x ∈conf∗(S), then P ′(x) = P(x).
F. Induced trees
This section is based on [27].
Deﬁnition 19. Let S be an SPN and v ∈conf(S) such
that S(v) ̸= 0. The sub-SPN induced by v, denoted by Sv,
is a non-normalized SPN obtained by (1) removing every
node ni such that Si(v) = 0 and the corresponding links,
(2) removing every link ni →nj such that wij = 0, and (3)
removing recursively all the nodes without parents, except
the root.
Proposition 20. If we denote by Sv(x) the value that Sv
returns for x, then Sv(v) = S(v).
Proposition 21. If S is selective, v ∈conf(S), and
S(v) ̸= 0, then Sv is a tree in which every sum node has
exactly one child. Following the literature, in this case we
will write Tv instead of Sv to remark that it is a tree.
Example 22. Given the SPN in Figure 2 and v =
(+a, +b, ¬c), Sv only contains the links drawn with thick
lines in that ﬁgure and the nodes connected by them. This
graph is a tree because the SPN is selective.
When an SPN is selective, the set of trees obtained for
all the conﬁgurations in conf(S) is similar to the the set
of trees obtained by recursively decomposing the SPN,
beginning from the root, as proposed by Zhao et al. [28].
Proposition 23. If S is selective, v ∈conf(S), and
S(v) ̸= 0 then
S(v) =
Y
(i,j)∈Tv
wij
Y
nk is terminal in Tv
Sk(v) ,
(18)
where (i, j) denotes a link.
Corollary 24. If all the terminal nodes in S are indica-
tors, then
S(v) =
Y
(i,j)∈Tv
wij .
Example 25. For the SPN in Figure 2, when v =
(+a, +b, ¬c) we have S(v) = w1,2 · w6,8 · w14,18 = 0.3 ·
0.4 · 0.9 = 0.108.
3The original algorithm, proposed by Peharz’s [9], augments every
node in S, even those that were already selective. In contrast, our
algorithm only processes the nodes that were not selective in S, so
that the augmentation of a selective SPN does not modify it.
IV. Inference
A. Marginal and posterior probabilities
As deﬁned in the previous section, P(x) = S(x) =
Sr(x). The value S(x) can be computed by an upward
pass from the leaves to the root in time proportional to
the number of links in the SPN. If X and E are two disjoint
subsets of V, then P(x | e) = S(xe)/S(e), where xe is the
composition of x and e. Therefore, any joint, marginal, or
conditional probability can be computed with at most two
upward passes. Partial propagation, which only propagates
from the nodes in X ∪E, can be signiﬁcantly faster [29].
B. MPE inference
The MPE conﬁguration for an SPN is (see Sec. II-C)
MPE (e) = arg max
x
P(x | e) = arg max
x
P(xe)
= arg max
x
S(xe) .
(19)
Let us assume that S is selective. Then X ∪E = sc(S)
implies that xe ∈conf(S) and, because of Proposition 21,
the sub-SPN induced by xe is a tree in which every sum
node has only one child. Therefore, the MPE can be found
by examining all the trees for the conﬁgurations xe in
which e is ﬁxed and x varies. It is possible to compare all
those trees at once with a single pass in S, by computing
Smax
i
(e) for each node as follows:
• if ni is a sum node, then
Smax
i
(e) = max
j∈ch(i) wij · Smax
j
(e) ;
(20)
• otherwise Smax
i
(e) = Si(e) (cf. Eqs. 13 and 15).
Then the algorithm backtracks from the root to the leaves,
selecting for each sum node the child that led to Smax(e)
and for each product node all its children. When arriving
at a terminal node for variable V , the algorithm selects the
value v = arg max
v′
P(v′). In particular, if the terminal node
is an indicator, Iv∗, then v = v∗. If V is continuous, then v
is the mode of p(v). A tie means that there are two or more
conﬁgurations having the same probability P(x, e); these
ties can be broken arbitrarily. The backtracking phase is
equivalent to pruning the SPN in order to obtain a tree
in which every sum node has only one child and there is
exactly one terminal node for each variable; the v values
selected at the terminal nodes make up the conﬁguration
ˆx = MPE (e).
Example 26. Figure 4 shows the MPE inference for
the SPN in Figure 2 when e
=
+c. The MPE is
obtained by backtracking from the root to the leaves:
ˆx = MPE (e) = (+a, ¬b). We can check that Smax
1
(e) =
P(ˆxe) = P(+a, ¬b, +c) = 0.144. For any other conﬁgura-
tion x of X = V \ E = {A, B}, we have S(xe) < S(ˆxe),
in accordance with Equation 19. We can also check that
the nodes selected by the backtracking phase are those of
the tree induced by ˆxe—see Def. 19 and Prop. 21.

8
n1
max
n3 
x
n2
x
n6
max
n7
max
n10
x
n9
x
n8
x
n11
x
n15 max
n16 
max
n14 max
n4
n5
n12
n13
n17 
n18 
1
0
0.3
0.8
0.1
0.1
1
0.48
0.144
0.15
0.15
0.48
1
1
1
0.8
0.3
0.3
0.7
0.3
0.5
0.6
0.4
0.5
0.1
0.8
0.3
0.9
0.2
0.7
Figure 4. MPE computation for the SPN in Figure 2. Sum nodes turn
into max nodes. The numbers in red are the values Smax
i
(e) when
the evidence is e = +c. The most probable explanation, MPE (e) =
(+a, ¬b), is found by backtracking from the root to the leaves (thick
lines).
Note that, as mentioned in Section II-C, the MPE
cannot be determined by selecting the most probable value
for each variable. In this example P(¬a | e) = 0.57 >
P(+a | e) and P(¬b | e) = 0.68 > P(+b | e), so we would
obtain the conﬁguration (¬a, ¬b), which is not the MPE.
This algorithm was proposed by Chan and Darwiche
[30] for arithmetic circuits, adapted to SPNs by Poon and
Domingos [1], and later called Best Tree (BT) in [31]. Pe-
harz [7, Theorem 2] proved that when an SPN is selective,
BT computes the true MPE. However, when a network is
not selective, the sub-SPN induced by a conﬁguration xe
is not necessarily a tree, so the value Smax(e) computed by
BT—which only considers the probability that ﬂows along
trees with one child for each sum node—may be diﬀer-
ent from maxx S(xe) and, consequently, the conﬁguration
returned by BT may be diﬀerent from the true MPE.
Therefore, even though the MPE can be found in time
proportional to the size of the graph for selective SPNs,
MPE is NP-complete for general SPNs [9, Theorem 5.3].4
4When an SPN S is not selective, it is possible ﬁnd an approxima-
tion to the MPE by augmenting it and then ﬁnding the MPE for S′
given e. The result is a conﬁguration y of Y = (V ∪Z) \ E = X ∪Z,
which we can then project onto X. However, Park [20] proved that
in general this method does not ﬁnd good approximations, i.e., the
posterior probability of the conﬁguration found by this method may
diﬀer signiﬁcantly from that of the true MPE.
C. MAX and MAP
Exact MAP inference for SPNs is NP-hard because it
includes as a particular case MPE (see Sec. II-C), which
is NP-complete. Nevertheless, Mei et al. [31] proposed
several algorithms that are very eﬃcient in practice. First,
they presented an algorithm for the MAX problem in gen-
eral SPNs. Second, they proved that every MAP problem
for SPNs can be reduced to a MAX problem for a new SPN
built in linear time. This way they were able to exactly
solve MAP problems for SPNs with up to 1, 000 variables
and 150, 000 links.
Third, they proposed several approximate MAP solvers
that trade accuracy for speed, obtaining excellent results.
In particular, they extended the BT method to the MAX
problem for non-selective SPNs. This extension, called K-
Best Tree (KBT), selects the K trees with the largest out-
put. Then, the corresponding conﬁgurations are obtained
(by backtracking) and evaluated in the SPN. The one with
the largest output is the approximate solution to the MAX
problem. Note that, for K = 1, KBT reduces to BT.
V. Parameter learning
Parameter learning consists in ﬁnding the optimal pa-
rameters for an SPN given its graph and a dataset. In
generative learning the most common optimality criterion
is to maximize the likelihood of the parameters of the
model given a dataset, while in discriminative learning
the goal is to maximize the conditional likelihood for each
value of a variable C, called the class.
A. Maximum likelihood estimation (MLE)
Let D = {v1, v2, . . . , vT } be a dataset of T independent
and identically distributed (i.i.d.) instances. We denote by
W the set of weights of the SPN and by Θ the parameters
of the probability distributions in the terminal nodes; both
of them act as conditioning variables for the probability of
the instances in the database. We deﬁne LD(w, θ) as the
logarithm of the likelihood:
LD(w, θ) = log P(D | w, θ)
=
T
X
t=1
log S(vt | w, θ) .
(21)
The goal is to ﬁnd the conﬁguration of W ∪Θ that
maximizes LD(w, θ). Given that there is no restriction
linking the parameters of one node (either sum, product,
or terminal) with those of others, the optimization can
be done independently for each node. For terminal nodes
representing univariate distributions, standard statistical
techniques can be applied. In this section we will focus
on the optimization of the weights, W, so we omit Θ
in the equations. The conﬁguration that maximizes the
likelihood is
bw = arg max
w
P(D | w) = arg max
w
LD(w)
(22)
subject to wij ≥0 and P
j∈ch(i) wij = 1.

9
1) MLE for selective SPNs: When the SPN is selective
and S(v) ̸= 0, then the weights of the sum nodes can be
estimated in closed form by applying MLE [27] as follows.
Proposition 27. When an SPN is selective and its weights
are all diﬀerent from 0,
LD(w) =
X
i
X
j∈ch(i)
nij · log wij + c ,
(23)
where nij is the number of instances in the dataset for
which (i, j) ∈Tvt and c is a value that does not depend
on w.
The nij’s can be computed by having a counter for every
link in the SPN. For each instance vt in the dataset, we
compute S(vt) and then backtrack from the root to the
leaves: for each product node we select all its children and
for each sum node ni we select the only child for which
Sj(vt) > 0 and increase by 1 the counter nij.
It is then necessary to obtain the conﬁguration bw that
maximizes the likelihood, deﬁned in Equation 22. The
only constraint is P
j∈ch(i) wij = 1 for every i, which
implies that the parameters for one node can be optimized
independently of those for other nodes. The values that
maximize the i-th term in Equation 23 are
bwij =
nij
P
j′∈ch(i) nij′ .
(24)
There is a special case in which P
j∈ch(i) nij = 0. This
occurs when Si(vt) = 0 for every t, i.e., when none of
the instances in the dataset propagates through the sum
node ni. In this case, the weights of this node can be set
uniformly:
∀j ∈ch(i), bwij =
1
|ch(i)| .
Alternatively, it is possible to use a Laplace-like smoothing
parameter α, so that
bwij =
nij + α
P
j′∈ch(i)(nij′ + α) ,
(25)
with 0 < α ≤1.
2) Partial derivatives of S: For every node ni we deﬁne
S∂
i (x) =
1
S(x) · ∂S
∂Si
(x) .
(26)
For the root node we have
S∂
r (x) =
1
S(x) · ∂S
∂Sr
(x) =
1
S(x) .
(27)
If nj is not the root,
S∂
j (x) =
1
S(x) · ∂S
∂Sj
(x)
=
1
S(x) ·
X
i∈pa(j)
∂S
∂Si
(x) · ∂Si
∂Sj
(x)
=
X
i∈pa(j)
S∂
i (x) · ∂Si
∂Sj
(x) ,
where pa(j) is the set of indices for the parents of nj. If ni
is a sum node,
S∂
j (x) =
X
i∈pa(j)
wij · S∂
i (x) ;
(28)
if it is a product node,
S∂
j (x) =
X
i∈pa(j)
S∂
i (x) ·
Y
j′∈ch(i)\{j}
Sj′(x) .
(29)
These equations mean that, for every node nj, S∂
j (x) can
be computed once we have the S∂
i (x)’s of its parents and
the Sj′(x)’s of its siblings. Therefore, after computing
Si(x) for every node with an upward pass, S∂
i (x) can
be computed with a downward pass, both in linear time.
This algorithm is similar to backpropagation for neural
networks and can be implemented using software packages
that support automatic diﬀerentiation.
3) Gradient descent:
a) Standard
gradient
descent:
Gradient
descent
(GD), a well known optimization method, was proposed
for SPNs for both generative and discriminative models
in [1] and [5], respectively.5 The algorithm is initialized
by assigning an arbitrary value to each parameter, bw(0)
ij .
and in every iteration, s, this value is updated, in order to
increase the likelihood of the model:
bw(s+1)
ij
= bw(s)
ij + γ ∂LD(w)
∂wij
,
(30)
where γ is the learning rate (a hyperparameter). It may
be necessary to renormalize the weights for each i after
each update.
The partial derivative for LD can be eﬃciently com-
puted using the S∂
i ’s deﬁned above, as follows. Because of
Equation 21,
∂LD(w)
∂wij
=
T
X
t=1
∂log S
∂wij
(vt) ,
and
∂log S
∂wij
(vt) =
1
S(vt) · ∂S
∂wij
(vt)
=
1
S(vt) · ∂S
∂Si
(vt) · ∂Si
∂wij
(vt) ,
which together with Equations 14 and 26 leads to
∂log S
∂wij
(vt) = S∂
i (vt) · Sj(vt)
and, ﬁnally,
∂LD(w)
∂wij
=
T
X
t=1
S∂
i (vt) · Sj(vt) .
(31)
This equation allows us to perform each iteration of GD in
time proportional to the size of the SPN and the number
of instances in the database.
5The method is commonly called “gradient descent” when its goal
is to minimize a quantity—for example, the classiﬁcation error in
neural networks. In our case it would be more appropriate to call
it “gradient ascent” because the goal is to maximize the likelihood.
However, in this paper we follow the standard terminology for SPNs.

10
b) Stochastic GD and mini-batch GD: In the stochas-
tic version of GD, each iteration s uses only one instance
of the database, randomly chosen, so that
bw(s+1)
ij
= w(s)
ij + γ ∂log S
∂wij
(vt)
until the algorithm converges.
Another possibility is to use mini-batches to update the
weights, that is, to divide the dataset in batches of L
randomly drawn instances, where L < T (usually L ≪T),
and update the weights as follows:
bw(s+1)
ij
= bw(s)
ij + γ
L
X
l=1
∂log S
∂wij
(vl) .
This version is the most popular when applying GD.
c) Hard GD: The application of GD to deep net-
works, either neural networks or SPNs, suﬀers from the
vanishing gradients problem: the deeper the layer, the
lower the contribution of its weights to the model output,
so the inﬂuence of the parameters in the deepest layers
may be imperceptible. The hard version of GD for SPNs
solves this problem by replacing the sum nodes with max
nodes and reparametrizing the weights so that the gradi-
ent of the log-likelihood function remains constant. This
method was introduced for SPNs by Gens and Domingos
[5] for discriminative learning.
4) Expectation-Maximization (EM):
a) Standard EM: We have seen how to learn the
parameters of a selective SPN from a complete dataset.
However, many real-world problems have missing values.
We denote by Ht the variables missing (hidden) in the t-
th instance of the database. Additionally, when learning
the parameters of S′, an augmented SPN, the database is
always incomplete, even if it contains all the values for the
the model variables in S, because it does not contain the
latent variables Z, added when augmenting the SPN, so
Z ⊆Ht for every t.
In
this
situation
we
can
apply
the
expectation-
maximization (EM) algorithm, designed to estimate the
parameters of probabilistic models from incomplete data-
sets. The problem is as follows. If we had a complete
database, we would be able to estimate its parameters as
explained in the previous section. Alternatively, if we knew
the parameters, we would be able to generate a complete
database by sampling from the probability distribution.
The EM algorithm proceeds by iteratively applying two
steps. The E-step (expectation) computes the probability
P(ht | vt) for each conﬁguration of the variables missing
in vt in order to impute the missing values. More precisely,
instead of assigning a single value to each missing cell, we
create a virtual database in which all the conﬁgurations
of Ht are present, each with probability P(ht | vt). The M-
step (maximization) uses this virtual complete database
to adjust the parameters of the model by MLE, as in
Section V-A1. The two steps are repeated until the pa-
rameters (the weights) converge.
The problem is that initially we have neither a complete
database nor parameters for sampling the values of the
missing variables. The algorithm can be initialized by as-
signing arbitrary values to the parameters or by assigning
arbitrary values to the variables in Z. Unfortunately, a
bad choice of the initial values may cause the algorithm
to converge to a local maximum of the likelihood, which
may be quite diﬀerent from the global maximum.
The nij’s required by the M-step are obtained by count-
ing the number of instances in the database for which the
link (i, j) belongs to the tree induced by vtht. These are
the nij’s introduced in Equation 23, which in the case of
the virtual database are
nij =
T
X
t=1
X
ht | (i,j)∈T ′
vtht
P ′(ht | vt)
(32)
and can be eﬃciently computed by applying the following
result:
Proposition 28. Given a database D with T instances
and a selective SPN, the nij’s in Equation 32 are
nij =
T
X
t=1
wij · S∂
i (vt) · Sj(vt) .
(33)
Once we have the nij’s, the weights can be updated
using Equation 24 or 25. The time required by each
iteration of EM is proportional to the size of the network
and the number of instances in the dataset.
b) Hard EM: The EM algorithm needs the value of
S∂
i , which is proportional to ∂S/∂wij and may thus be
very small when the link (ni, nj) is in a deep position,
i.e., far from the root. Therefore this algorithm may suﬀer
from the vanishing gradients problem in the same way
as GD. To avoid it, Poon and Domingos [1] proposed a
hard version of EM for SPNs that selects for each hidden
variable H ∈Ht the most probable state. Thus, in the
E-step of each iteration, every instance of the dataset
contributes to the update of just one weight per sum node,
instead of contributing to all of them proportionally.
Hsu et al. [25] proposed a variant of hard EM for SPNs
with Gaussian leaves. This method proceeds top-down. It
starts at the top sum node and distributes the instances
among its children by maximum likelihood. Every other
sum node receives only the instances distributed to its
parents and redistributes them in the same fashion. This
way it updates the weights of the sum nodes locally. The
process is similar to the automatic parameter learning
in LearnSPN (cf. Sec. VI-B). These authors also provide
formulas for updating the parameters of Gaussian leaves.
5) Comparison of MLE algorithms: The application of
the EM to SPNs has been justiﬁed with diﬀerent mathe-
matical arguments. Peharz [9] exploited the interpretation
of the sum nodes in the augmented network as the sum
of conditional probability functions (cf. Eqs. 39 and 40).
In turn, Zhao et al. [28], using a uniﬁed framework based
on signomial programming, designed two algorithms for
learning the parameters of SPNs: sequential monomial
approximations (SMA) and the concave-convex procedure
(CCCP). GD is a special case of SMA, while CCCP

11
coincides with EM in the case of SPNs, despite being
diﬀerent algorithms in general. Their experiments proved
that EM/CCCP converges much faster than the other
algorithms, including GD. In turn, Desana and Schnörr
[24] derived the EM algorithm for SPNs whose leaf nodes
may represent complex probability distributions.
In discriminative learning, neither EM nor CCCP have
a closed-form expression for updating the weights [5].
Rashwan et al. [32] addressed this problem with the ex-
tended Baum-Welch (EBW) algorithm, which updates the
parameters of the network using a transformation that in-
creases the value of the likelihood function monotonically.
In the generative case, this transformation coincides with
the update formula of EM/CCCP (the M-step), while in
the discriminative case it provides a method to maximize
the (conditional) likelihood function with a closed-form
formula. They also adapted this method to SPNs with
Gaussian leaves.
Both the algorithm of Desana and Schnörr and EBW
outperformed GD and EM in a wide variety of datasets.
B. Semi-supervised learning
Trapp et al. [33] introduced a safe semi-supervised
learning algorithm for SPNs. By “safe” they mean that the
model performance can be increased but never degraded
by adding unlabeled data. They extended the EM to gener-
ative semi-supervised learning and deﬁned a discriminative
semi-supervised learning approach. They also introduced
the maximum contrastive pessimistic algorithm (MCP-
SPN), based on [34], for learning safe semi-supervised
SPNs. Their results were competitive with those of purely
supervised algorithms.
C. Approximate Bayesian learning
There are alternative methods for learning the parame-
ters of an SPN based on approximate Bayesian techniques,
such as Bayesian moment matching [35] and collapsed
variational inference [36], which are not as exposed to over-
ﬁtting as GD or EM. Both Bayesian methods start with a
product of Dirichlet distributions as a prior; the posterior
distribution P(wij | D) is a mixture of products of Dirich-
lets, which is computationally intractable. In both works
the solution applied was to approximate that distribution
with a single product of Dirichlet distributions. Rash-
wan et al. [35] applied online Bayesian moment matching
(oBMM), which approximates the posterior distributions
of the weights by computing a subset of their moments and
ﬁnding another distribution from a tractable family that
matches those moments. In this case, it suﬃced to match
the ﬁrst and second order moments of the distribution.
The experiments showed that this approach outperforms
SGD and online EM. This method has also been adapted
to SPNs with Gaussian leaves by Jaini et al. [37]. In the
same vein, Zhao and Gordon [38] presented an optimal
linear time algorithm for computing the moments in SPNs
with general acyclic directed graph structures, based on
the mixture of trees interpretation of SPNs. This provides
an eﬀective method to apply Bayesian moment matching
to a broad family of SPNs.
As mentioned above, Zhao et al. [36] addressed the prob-
lem by applying collapsed variational Bayesian inference
(CVB-SPN). This approach treats the dataset as partial
evidence, whose missing values correspond to the latent
variables of the SPN. They assumed that the missing val-
ues of each instance are not independent of those missing
in other instances, and marginalized these variables out of
the joint posterior distribution (the “collapse” step). Then,
they approximated this distribution with the product of
the Dirichlets that maximizes some evidence lower bound
(ELBO) of the log-likelihood function of the dataset (the
“variational inference” step). The experiments showed
that the online version of CVB-SPN—i.e., the version that
receives a stream of data in real time—outperforms oBMM
in many datasets.
D. Deep learning approach
Peharz et al. [18] considered a special class of SPNs,
which they called random SPNs, and trained them with
automatic diﬀerentiation, stochastic GD, and dropout,
using GPU parallelization. The resulting model was called
RAT-SPN. Its classiﬁcation accuracy, measured on the
MNIST images and other databases, was comparable to
that of deep neural networks, with the advantages of a
probabilistic generative model, such as interpretability and
robustness to missing features.
VI. Structural learning
Structural learning consists in ﬁnding the optimal (or
near-optimal) graph of an SPN. Most of the algorithms for
this task require some computation of probabilities during
the process.
A. First structure learners
BuildSPN, by Dennis and Ventura [39] was the ﬁrst
algorithm of this kind. It looks for subsets of highly
correlated variables and introduces latent variables to
account for those dependencies. These variables generate
sum nodes and the process is repeated recursively looking
for the new latent variables.
BuildSPN and the hand-coded structure of Poon and
Domingos [1], both designed for image processing, as-
sumed neighborhood dependence. In order to overcome
that limitation, Peharz et al. [40] proposed an algorithm
that subsequently combines SPNs of few variables into
larger ones applying a statistical dependence test.
BuildSPN was also critiqued by Gens and Domingos [6]
because (1) the clustering process may separate highly
dependent variables, (2) the size of the SPN and the
time required can grow exponentially with the number of
variables, and (3) it requires an additional step to learn
the weights.

12
Figure 5.
The LearnSPN algorithm recursively creates a prod-
uct node when there are subsets of (approximately) independent
variables and a sum node otherwise, grouping similar instances.
(Reproduced from [6] with the authors’ permission.)
B. LearnSPN
It is common in machine learning to see a dataset as a
data matrix whose columns are attributes or variables and
whose rows are observations or instances. The LearnSPN
algorithm [6] recursively splits the variables into indepen-
dent subsets (thus “chopping” the data matrix, as shown
in Figure 5) and then clusters the instances (thus “slicing”
the matrix). Every “chopping” creates a product node and
every “slicing” a sum node, as indicated in Algorithm 1.
There are two base cases:
1) When the piece of the data matrix produced by
“chopping” contains a single column (i.e., one vari-
able), the algorithm creates a terminal node with a
univariate distribution using MLE.
2) When the piece of the data matrix produced by
“slicing” contains several columns with relatively few
rows, the algorithm applies a naïve Bayes factor-
ization over those variables. This is like “chopping”
that piece into individual columns, which will be
processed as in the base case 1.
LearnSPN can be seen as a framework algorithm in the
sense that it does not specify the procedures for splitting
independent subsets of variables (splitVariables in Algo-
rithm 1) and clustering similar instances (clusterInstances
in that algorithm). Originally Gens and Domingos [6]
chose the G-Test for splitting and hard incremental EM
for clustering.
Splitting the variables (“chopping”) only considers pair-
wise independencies. The process starts with a graph con-
taining a node for each variable and no links. It randomly
selects one variable and adds an edge to the ﬁrst other
variable deemed dependent by the G-test, then moves
to that variable, and iterates until no new variable can
be linked to this component of the graph. At the end,
Algorithm 1 LearnSPN(T, V, α, m)
Input: T: a data matrix of instances over the variables
in V; m: minimum number of instances to allow a split of
variables; α: Laplace smoothing parameter
Output: an SPN S with sc(S) = V
if |V| = 1 then
S ←univariateDistribution(T, V, α)
else if |T| < m then
S ←naïveFactorization(T, V, α)
else
{Vj}C
j=1 ←splitVariables(T, V, α)
if C > 1 then
S ←QC
j=1 LearnSPN (T, Vj, α, m)
else
{Ti}R
i=1 ←clusterInstances(T, V)
S ←PR
i=1
|Ti|
|T| LearnSPN (Ti, V, α, m)
return S
if this component has gathered all variables only one
component is generated; then the clustering concludes and
the algorithm clusters instances instead.
Clustering similar instances (“slicing”) is achieved by
the hard EM algorithm assuming a naïve Bayes mixture
model, where the variables are independent given the
cluster Ci: P(v) = P
i P (ci) Q
j P (vj | ci). This particular
model produces a clustering that can be chopped in the
next recursion. This version of LearnSPN forces a cluster-
ing in the ﬁrst step, without attempting a split.
C. ID-SPN
Rooshenas and Lowd [21] observed that PGM learners
usually analyze direct interactions (dependencies) between
variables while previous SPN learners analyze indirect
interactions (dependencies through a latent variable). The
indirect-direct SPN (ID-SPN) structure learner combines
both methods. Their initial idea is that any tractable
multivariate distribution that can be represented as an
arithmetic circuit or an SPN can be the leaf of an SPN
without losing tractability. With this idea they learned
arithmetic circuit Markov networks (ACMN) [41], which
are roughly Markov networks learned as arithmetic cir-
cuits. ID-SPN begins with a singular ACMN node and
tries to replace it with a mixture (yielding a sum node)
or a product (yielding a product node), similar to the
cluster and split operations in LearnSPN. If a replacement
increases the likelihood, it is saved and the algorithm
recurs on the new ACMN leaves, until the likelihood does
not increase. This top-down process represents the learn-
ing of indirect interactions, while the creation of ACMN
leaves represents the learning of direct interactions. This
algorithm outperformed all previous algorithms and is
currently the state of the art. However, ID-SPN is slower
and more complex than LearnSPN, and has many more
hyperparameters to tune, which requires a random search
in the space of hyperparameters instead of a grid search.

13
D. Other algorithms
Peharz et al. [27] proposed a structure learner that
searches within the space of selective SPNs and showed
that it is competitive with LearnSPN.
Adel et al. [42] pointed out that previous work had only
compared algorithms on binary datasets. They designed
SVD-SPN, which proceeds by ﬁnding rank-1 matrices.
This allows the algorithm to cluster and split at the same
time, producing optimal data matrix pieces. It operates
recursively, like LearnSPN, but constructs the SPN from
the rank-1 submatrices extracted. It also considers a mul-
tivariate base case when the variables in the pieces of
the data matrix are highly correlated. In this case a sum
node is created with as many children as instances in the
piece of the matrix; each child is a product node of all
the variables in the matrix. In their experiments, SVD-
SPN obtained results similar to those of LearnSPN and
ID-SPN for binary datasets, but outperformed them in
multiple-category datasets, such as Caltech-101, and is 5
times faster.
E. Improvements to LearnSPN
Even though LearnSPN is not the best performing
algorithm, it is still widely used owing to its simplicity
and modularity [29] and has led to several variants.
1) Algorithm of Vergari et al. [26]: It consists of three
modiﬁcations to LearnSPN:
1) Binary splits. Every split cuts the data matrix into
only two pieces. This avoids creating too complex
structures at early stages (which often occurs when
learning from noisy data) and favors deep structures
over shallow ones. This is not a limitation in the
number of children of product nodes because con-
secutive splits can be applied if necessary.
2) Chow-Liu trees (CLTs) in the leaves. The naïve Bayes
factorization used as the base case of LearnSPN (see
Algorithm 1) can be replaced by the creation of
Chow-Liu trees [43], which are equivalent to tree-
shaped Bayesian networks or Markov networks. Ev-
ery tree is built by linking the variables with higher
mutual information until there is a path between ev-
ery pair of variables. CLTs are more expressive than
the naïve Bayes factorization (which is a particular
case of CLT) without adding computational com-
plexity. LearnSPN stops earlier when using CLTs as
leaves because each tree can accommodate more in-
stances, thus yielding simpler SPN structures (with
fewer edges) with lower risk of overﬁtting.
3) Bagging. This technique, originally used to build ran-
dom forests [44], consists in taking several random
samples from a dataset, each consisting of several
instances, and building a classiﬁer for each sample.
The overall classiﬁcation can be the average of the
outputs of the individual classiﬁers (for continuous
variables) or the mode (for ﬁnite-state variables).
In SPN learning, it extracts—with replacement—
n samples of the dataset and produces a sum node
Figure 6. Learning SPNs with bagging.
with n children, setting every weight to 1/n, as
shown in Figure 6. Each child represents one of
the individual classiﬁers and the sum node averages
the n results. Since the network size would grow
exponentially if bagging were applied before every
clustering, it is applied only before the ﬁrst Learn-
SPN operation—which is a clustering—in order to
achieve the widest eﬀect on the resulting structure.
The experiments showed that (1) binary splits yield
deeper and simpler SPNs and generally reduce the number
of edges and parameters, (2) using Chow-Liu trees attains
the same eﬀect and generally increases the likelihood,
and (3) bagging also increases the likelihood, especially
in datasets with a low number of instances. With these
modiﬁcations, LearnSPN achieved the same performance
as ID-SPN.
2) Beyond tree SPNs: One of the main disadvantages of
both LearnSPN and ID-SPN is that they always produce
trees (except when the leaves are Markov networks). In
order to generate more compact SPNs, Dennis and Ven-
tura [45] designed SearchSPN, an algorithm that produces
SPNs in which nodes may have several parents. It selects
the product node that contributes less to the likelihood
and greedily searches for candidate structures using mod-
iﬁed versions of the clustering methods of LearnSPN. The
resulting likelihood is signiﬁcantly better than that of
LearnSPN for the majority of datasets and comparable
with that of ID-SPN, but on average the execution is 7
times faster and the number of nodes 10 times smaller.
In the same vein, Rahman and Gogate [46] created a
post-processing algorithm that, after applying LearnSPN
with CLTs in the leaves, merges similar sub-SPNs. Simi-
larity is measured with a Manhattan distance; if two sub-
SPNs are closer than a certain threshold, the pieces of the
data matrix from which they come are combined and the
algorithm chooses the sub-SPN with the higher likelihood
for the combined data. This modiﬁcation of LearnSPN
increases the likelihood and reduces the number of param-
eters of the SPN; additionally, it dramatically increases
the learning time for some datasets. In combination with
bagging, it outperformed other algorithms—including ID-
SPN [21]—for high-dimensional datasets.
3) Further improvements to LearnSPN: As mentioned
in Section V-A5, Zhao et al. [28] showed that learning

14
the parameters with the CCCP algorithm improves the
performance of LearnSPN.
Di Mauro et al. [47] proposed approximate splitting
methods to accelerate LearnSPN, thus trading speed for
quality (likelihood).
Butz et al. [29] studied the diﬀerent combinations of
algorithms for LearnSPN. They compared mutual infor-
mation and the G-test for splitting, and k-means and
Gaussian mixture models for clustering. The best results
were obtained when using the G-test and either k-means
or Gaussian mixture models, both for the standard Learn-
SPN and for the version that generates CLTs in the leaves.
Liu et al. [48] proposed a clustering method that decides
the number of instance clusters adaptively, i.e., depending
on each piece of data matrix evaluated. Their goal was
to generate more expressive SPNs, in particular deeper
ones with controlled widths. When compared previous
algorithms (namely, standard LearnSPN [6], LearnSPN
with binary splits [26], and LearnSPN with approximate
splitting [47]), their method achieved higher likelihood in
20 binary datasets and generated deeper networks (i.e.,
more expressive SPNs) while maintaining a reasonable
size.
4) LearnSPN with piecewise polynomial distributions:
Most learning algorithms assume that when a terminal
node represents a continuous variable, the univariate dis-
tribution belongs to a known family (Poisson, Gaussian,
etc.) and only the parameters must be optimized. However,
there are at least two variants of LearnSPN that, in
addition to having indicators for ﬁnite-state variables, use
a piecewise polynomial distribution for each leaf node
representing a numeric variable, instead of requiring the
user to specify a parametric family [49], [23].
In LearnWMISPN [49], which combines LearnSPN with
weighted model integration (WMI), the order of each
polynomial is determined using the Bayesian informa-
tion criterion (BIC) [50]. A preprocessing step transforms
ﬁnite-state, categorical, and continuous features into a
binary representation before applying LearnSPN. The
corresponding inference algorithm can answer complex
conditional queries involving both intervals for continuous
variables and values for discrete variables.
In mixed SPN (MSPNs) [23] the operations of decompo-
sition (splitting) and conditioning (clustering) are based
on the Hirschfeld-Gebelein-Rényi maximum correlation
coeﬃcient [51].
F. Online structural learning
The algorithms presented so far need the complete
dataset to produce a structure. However, sometimes the
dataset is so big that the computer does not have enough
memory to store it at once. In other situations, e.g., in
recommender systems, the data arrive constantly. In these
cases the learning algorithm must be able to update the
structure instead of learning it from scratch every time
new data arrives.
In this context, Lee et al. [52] designed a version of
LearnSPN where clustering (slicing) is replaced by online
clustering, so that new sum children can be added when
new data arrive, while product nodes are unmodiﬁed.
Later Dennis and Ventura [53] extended their Search-
SPN algorithm [45] to the online setting. This online
version is as fast as the oﬄine version that works only
on the current batch and the quality of the resulting SPN
is the same.
Hsu et al. [25] created oSLRAU, an online structure
learner for Gaussian leaves (oSLRAU) which begins with
a completely uncorrelated SPN structure that is updated
when the arriving data reveals a new correlation. The
update consists in replacing a leaf with a multivariate
Gaussian leaf or a mixture over its scope.
Jaini et al. [54] proposed an algorithm, Prometheus,
whose ﬁrst concern is to avoid the parameter that decides
when two subsets of variables are independent in order
to perform a LearnSPN split. So instead of creating a
product node, it creates a mixture of them, representing
diﬀerent subset partitions. The way the partitions are
created allows them to share subsets, which is reﬂected
in the structure by common children, thus overcoming
the restriction to trees on the way. This is in some sense
similar to bagging in sum nodes (cf. Sec. VI-E) and makes
the algorithm robust in low data regimes. However, the
complexity of the algorithm grows with the square of
the number of variables. In order to extend it to high-
dimensional datasets, the authors created a version that
samples in each step from the set of variables instead
of using all of them. This algorithm can treat discrete,
continuous, and mixed datasets. Their experiments showed
that this algorithm surpasses both LearnSPN and ID-SPN
in the three types of datasets. It is also robust in low data
regimes, achieving the same performance as oSLRAU with
only 30-40% of the data.
G. Learning with dynamic data
Data are said to be dynamic when all the variables (or
at least some of them) have diﬀerent values in diﬀerent
time points—for example, Income-at-year-1, Income-at-
year-2, etc. The set of variables for a speciﬁc time point is
usually called a slice. The slice structure, called template,
is replicated and chained to accommodate as many time
points as necessary. The length of the chain is called the
horizon.
For this problem Melibari et al. [55] proposed dynamic
SPNs (DSPNs), which extend SPNs in the same way that
dynamic Bayesian networks (DBNs) [56] extend BNs. A
local-search structure learner generates an initial template
SPN and searches for neighboring structures, trying to
maximize the likelihood. Every neighbor, which comes
from replacing a product node, represents a speciﬁc choice
of factorization of the variables in its scope. The algorithm
searches over other choices of factorizations and updates
the structure if a better one is found. This method out-
performs non-dynamic algorithms, such as LearnSPN, and
other models, such as dynamic Bayesian networks and
recurrent neural networks.

15
Later, Kalra et al. [57] extended oSLRAU to the dy-
namic setting by unrolling the SPN to match the length of
the chain to the horizon, with shared weights and a shared
covariance matrix, to decide when a new correlation re-
quires a change in the template. This algorithm surpassed
that of Melibari et al. [55] and hidden Markov models in
5 sequential datasets, and recurrent neural networks in 4
of those datasets.
H. Relational data learning
Nath and Domingos [58] introduced relational SPNs
(RSPNs), which generalize SPNs by modeling a set of
instances jointly, allowing them to inﬂuence each other’s
probability distributions, and modeling probabilities of
relations between objects. Their LearnRSPN algorithm
outperformed Markov Logic Networks in both running
time and predictive accuracy when tested on three data-
sets.
I. Bayesian structure learning
Lee et al. [59] designed a Bayesian non-parametric
extension of SPNs. Trapp et al. [60] criticized this work
for neglecting induced trees in their posterior construc-
tion, corrected it by introducing inﬁnite sum-product tree,
and showed that it yields higher likelihood than inﬁnite
Gaussian mixture models.
A common problem of structural learning algorithms
is the lack of a principled criterion for deciding what a
“good” structure is. For this reason, Trapp et al. [61]
proposed an alternative Bayesian approach that decom-
poses the problem into two phases: ﬁrst ﬁnding a graph
and then learning the scope-function, ψ, which assigns a
scope to each node. The function ψ and the parameters
of the model are learned jointly using Gibbs sampling.
The Bayesian nature of this approach reduces the risk of
overﬁtting, avoids the need for a separate validation set to
adjust the hyperparameters of the algorithm, and enables
robust learning of SPN structures under missing data.
VII. Applications
SPNs have been used for a wide variety of applications,
from toy problems to real-world challenges.
A. Image processing
1) Image reconstruction and classiﬁcation: Poon and
Domingos, in their seminal paper about SPNs [1], ap-
plied them to image reconstruction, using a hand-designed
structure that took into account the local structure of the
image data. They tested their method on the datasets
Caltech-101 and Olivetti. Then Gens and Domingos [5]
used a diﬀerent hand-made structure for image classiﬁ-
cation on the datasets CIFAR-10 and STL-10, obtaining
excellent results for that time, as mentioned in Section I-A.
2) Image segmentation: Image segmentation consists
in labeling every pixel with the object it belongs to.
Yuan et al. [62] developed an algorithm that scales down
every image recursively to diﬀerent sizes and generates
object tags and unary potentials for every scale. Then,
it builds a multi-stacked SPN where every stack has a
bottom and a top SPN. The bottom SPN works on a pixel
and its vicinity, going from the pixel to bigger patches.
Product nodes model correlations between patches while
sum nodes combine them into a feature of a bigger patch.
When the patch is as big as the pixel in the next scaled
image, the results are introduced in the top SPN alongside
the unary potentials and the tags of that scale. This
process is stacked until the “patch” treated is the whole
image. Multi-stacked SPNs have been especially eﬀective
for handling occlusions in scenes.
Rathke et al. [63] applied SPNs to segmentation OCT
scans of retinal tissue. They ﬁrst built a segmentation
model for the health model and for every pathology and
then added to those models typical shape variations of
the retina tissue for some pathology-speciﬁc regions. The
resulting SPN extracts candidate regions (either healthy
or unhealthy) and selects the combination that maximizes
the likelihood. After a smoothing step, they obtain a
complete segmentation of the retina tissue as well as the
diagnosis and the aﬀected regions. This method achieved
state-of-the-art performance without needing images la-
beled by pathologies.
3) Activity recognition: Wang and Wang [64] addressed
activity recognition on still images. They used unsuper-
vised learning and a convolutional neural network to
isolate parts of the images, such as a hand or a glass,
and designed a spatial SPN including the spatial indi-
cator nodes “above”, “below”, “left”, and “right” for the
product nodes to encode spatial relations between pairs of
these parts. They ﬁrst partitioned the image to consider
only local part conﬁgurations. Its SPN structure has two
components: the top layers represent a partitioning of
the image into sub-images where product nodes act as
partitions and sum nodes as combinations of diﬀerent
partitions, while the bottom layers represent the parts
included in each sub-image and their relative position
using the spatial indicator nodes. In this sense the SPN
ﬁrst learns spatial relations of isolated parts in sub-images
and then learns correlations between sub-images. Spatial
SPNs outperformed other activity recognition algorithms
and were able to discover discriminant pairs of parts for
each class.
Amer and Todorovic [65] worked on activity localization
and recognition in videos. In their work, a visual word
is a meaningful piece of an image, previously extracted
with a neural network. Visual words lie in a grid with
three dimensions: height, width, and time. Every grid
position has a histogram of associated visual words, called
bag of words. To construct an SPN, each bag of words
is treated as a variable with two states: foreground and
background. Product nodes represent a combination of
sub-activities into a more complex activity (for example,

16
“join hands + separate hands = clap”) and sum nodes
represent variations of the same activity. An SPN is
trained for every activity in a supervised context, in which
the foreground and the background values are known, and
in a weakly supervised context, in which only the activity
is known. The structure is a near-completely connected
graph, pruned after parameter learning, which proceeds
iteratively: it learns—with GD—the weights of the SPN
from the parameters of the bag of words and then learns—
with variational methods—the parameters of the bag of
words from the weights of the SPN. The accuracy of
this weakly supervised setting was only 1.6 to 3% worse
than that of the supervised setting. This approach in
general achieved better performance than state-of-the-art
algorithms on several action-recognition datasets.
4) Object detection: Stelzner et al. [19] proved that
the attend-infer-repeat (AIR) framework used for object
detection and location is much more eﬃcient when the
variational autoencoders (VAEs) that model individual
objects are replaced by SPNs: they achieved an improve-
ment in speed of an order of magnitude, with slightly
higher accuracy, as well as robustness against noise.
B. Robotics
Sguerra and Cozman [66] used SPNs for aerial robots
navigation. Micro aerial vehicles need a set of sensors
that must comply with two criteria: light weight and real-
time response. Optical recognition with cameras satisﬁes
the former while fast inference with SPNs ensures the
latter, as they were able to classify—in real time—what
the camera sees into pilot commands, such as “turn right”,
and obtaining 75% of accuracy with just 66 images.
Pronobis et al. [67] designed a probabilistic representa-
tion of spatial knowledge called “deep spatial aﬀordance
hierarchy” (DASH), which encodes several levels of ab-
stractions using a deep model of spatial concepts. It mod-
els knowledge gaps and aﬀordances by a deep generative
spatial model (DGSM) which uses SPNs for inference
across diﬀerent levels of abstractions. SPNs ﬁt naturally
with DGSM because latent variables of the former are
internal descriptors in the latter. The authors tested it
in a robot equipped with a laser-range sensor.
Zheng et al. [68] designed graph-structured SPNs
(GraphSPNs) for structured prediction. Their algorithm
learns template SPNs and makes a mixture over them
(a template distribution), which can be applied to graphs
of varying size re-using the same templates. The authors
applied them to model large-scale global semantic maps
of oﬃce environments with a exploring robot, obtaining
better results than with the classical approach based on
undirected graphical models (Markov networks).
The authors joined both models into an end-to-end deep
model for semantic mapping in large-scale environments
with multiple levels of abstraction, called TopoNets [69].
These can perform inference about unknown spatial in-
formation, are useful for novelty detection, and achieve
real-time performance.
C. NLP and sequence data analysis
Peharz et al. [70] applied SPNs to modeling speech by re-
trieving the lost frequencies of telephone communications
(artiﬁcial bandwidth extension). In this problem tractable
and quick (real-time) inference is essential. They used a
hidden Markov model (HMM) to represent the temporal
evolution of the log-spectrum, clustered the data using the
Linde–Buzo–Gray algorithm and trained an SPN for each
cluster. The SPNs model each cluster and can be used
to retrieve the lost frequencies by MPE inference. This
model has achieved better results than state-of-the-art
algorithms both objectively, with a measure of log-spectral
distortion, and subjectively, through listening tests.
In language modeling, Cheng et al. [71] used a dis-
criminative SPN [5], whose leaf nodes represent vectors
with information about previous words. This SPN was
able to compute the probability of the next word more
accurately than classic methods for language modeling,
such as feedforward neural networks and recurrent neural
networks.
Later, Melibari et al. [55] used dynamic SPNs (DSPNs)
to analyze diﬀerent sequence datasets. Unlike dynamic
Bayesian networks, for which inference is generally expo-
nential in the number of variables per time slice, inference
in DSPNs has linear complexity. In a comparative study
with ﬁve other methods, including HMMs and neural
networks with long short-term memory (LSTM), DSPNs
were superior in 4 of the 5 datasets examined.
Ratajczak et al. [72] replaced the local factors of higher-
order linear-chain conditional random ﬁelds (HO-LC-
CRFs) and maximum entropy Markov models (MEMMs)
with SPNs. These outperformed other state-of-the-art
methods in optical character recognition and achieved
competitive results in phoneme classiﬁcation and hand-
writing recognition.
D. Other applications
Vergari et al. [73] used SPNs as autoencoders (SPAEs)
for feature extraction. They trained the SPNs with Learn-
SPN and used the values of the internal nodes or the
states of the latent variables associated to sum nodes as
the codiﬁcation variables. Although this model was not
trained to reconstruct its inputs, experiments showed that
SPAEs are competitive with state-of-the-art autoencoder
architectures for several multilabel classiﬁcation problems.
Butz et al. [74] used Bayesian networks to recognize
independencies in 3,500 datasets of soil bacteria and com-
bined them into an SPN in order to eﬃciently compute
conditional probabilities and the MPE.
Nath and Domingos [75] used relational SPNs (cf.
Sec. VI-H) for fault localization, i.e. ﬁnding the most
probable location of bugs in computer source code. The
networks, trained on a corpus of previously diagnosed
buggy programs, learned to identify recurring patterns of
bugs. They could also receive clues about bug suspicion
from other bug detectors, such as Tarantula.

17
Hilprecht et al. [76] proposed learning database man-
agement systems from data instead of queries, using en-
sembles of relational SPNs. This approach provides better
accuracy and better generalization to unseen queries than
diﬀerent state of the art methods.
Vergari et al. [77] also evaluated SPNs for representa-
tion learning [78]. SPNs encode a hierarchy of part-based
representations which can be ordered by scope length.
When compared with other representation learners, such
as VAEs or random Boltzmann machines (RBMs), they
provided competitive results both in supervised and semi-
supervised settings. Moreover, the model trained for ex-
tracting representations can be used as-is for inference.
Vergari et al. [79] designed a tool for automatic ex-
ploratory data analysis without the need for expert sta-
tistical knowledge. It leverages Gibbs sampling and a
modiﬁcation of mixed SPNs [23] to model the data, and
provides functionalities such as data type recognition,
missing values imputation, anomaly detection, and others.
Roy et al. [80] addressed the explanation of activity
recognition and localization in videos. A deep convolu-
tional neural network is used for localization and its output
is introduced to an SPN. Both models are learned jointly.
The explainability of the system was evaluated by the
subjective user trust in the explanations that the SPN
provides about the criteria of the neural net.
VIII. Software for SPNs
Every publication about SPNs presents some experi-
ments, and in many cases the source code is publicly avail-
able. The web page https://github.com/arranger1044/
awesome-spn contains many references about SPNs, classi-
ﬁed by year and by topic; the section “Resources” includes
links to talks and tutorials, the source code for some of
those publications, and several datasets commonly used
for the experiments. Most of the software is written in
Python or C++.
In particular, there are two projects that aim to develop
comprehensive, simple, and extensible libraries for SPNs.
Both of them are written in Python and use TensorFlow as
a backend for speeding up some operations. LibSPN,6 initi-
ated by Andrzej Pronobis at the University of Washington,
Seattle, WA [81], implements several methods for inference
(marginal and conditional probabilities, and approximate
MPE), parameter learning (batch and online, with GD and
hard EM), and visualization of SPNs. It lacks algorithms
for structural learning, but it allows building convolutional
SPNs with a layer-oriented interface [82]. The SPNs,
stored as Python structures, are compiled into Tensor-
Flow graphs for parameter learning and inference; for this
purpose LibSPN has implemented in C++ and CUDA
some operations that cannot be performed eﬃciently with
native TensorFlow operations. Several tutorials in Jupyter
Notebook are available at its website. It has been used
mainly for computer vision and robotics [67], [68], [82].
The other library, SPFlow7, is developed by Alejandro
6https://www.libspn.org.
7https://github.com/SPFlow/SPFlow.
Molina at the University of Darmstadt, Germany, with
contributors from several countries [83]. It implements
methods for inference (marginal and conditional proba-
bilities, and approximate MPE), parameter learning (with
GD) and several structural learning algorithms, and can
be extended and customized to implement new algorithms.
SPNs are usually compiled into TensorFlow for fast com-
putation, but they can also be compiled into C, CUDA,
or FPGA code.
There are also some smaller libraries of interest, such
as SumProductNetworks.jl for Julia,8 which implements
inference and parameter learning, and the Libra Toolkit
[84],9 a collection of algorithms written in OCaml for
learning several types of probabilistic models, such as BNs,
SPNs, and others, including the ID-SPN algorithm [21].
IX. Extensions of SPNs
In recent years there have been several extensions of
SPNs to more general models. In this section we brieﬂy
comment on some of them.
Sum-product-max networks (SPMNs) [85] generalize
SPNs to the class of decision making problems by including
two new types of nodes, max and utility, like in inﬂu-
ence diagrams. These networks can compute the expected
utility and the optimal policy in time proportional to the
number of links.
Autoencoder SPNs (AESPNs) [86] combine two SPNs
with an autoencoder between them. This model produces
better samples than SPNs by themselves.
Tan and Peharz [87] designed a mixture model of VAE
pieces over local subsets of variables combined via an SPN.
This combination yields better density estimates, smaller
models and improved data eﬃciency with respect to VAEs.
In credal sum-product networks (CSPNs) [88] the
weights of each sum node do not have a ﬁxed value, but
vary inside a set (a product of probability simplexes) in
such a way that each choice of the weights deﬁnes an SPN.
Sum-product graphical models (SPGMs) [89] join the
semantics of probabilistic graphical models with the eval-
uation eﬃciency and expressiveness of SPNs by allowing
the nodes associated to variables to appear in any part of
the network, not only in the leaf nodes. Their LearnSPGM
algorithm outperformed both LearnSPN and ID-SPN on
the 20 real-world datasets previously used in [90].
Sum-product-quotient networks (SPQNs) [91] introduce
quotient nodes, which take two inputs and output their
quotient, allowing these models to represent conditional
probabilities explicitly.
Tensor SPNs (tSPNs) [92] are an alternative represen-
tation of SPNs. Their main advantage is an important
reduction in the number of parameters—between 24 and
569 times in the experiments—with little loss of modeling
accuracy. Additionally, tSPNs allow for faster inference
and a deeper and more narrow neural-network architec-
ture.
8https://github.com/trappmartin/SumProductNetworks.jl.
9http://libra.cs.uoregon.edu.

18
Convolutional SPNs (ConvSPNs or CSPN) were de-
veloped independently in [82] and [93]. The ﬁrst model
exploits the inherent structure of spatial data in a similar
way to convolutional neural networks by using the sum
and product operations of SPNs. The second one ﬁnds a
criterion for seeing when a convolutional neural network
is an SPN.
Submodular SPNs (SSPNs) [94] are an extension of
SPNs for scene understanding, whose weights can be
deﬁned by submodular energy functions.
Compositional kernel machines (CKMs) [95] are an
instance-based method closely related to SPNs. They
have been successfully applied to image processing tasks,
mainly to object recognition.
Conditional SPNs (CSPNs) [96] extend SPNs to condi-
tional probability distributions. They include a new type
of node, called a gating node, which computes a convex
combination of the conditional probability of its children
with non-ﬁxed weights.
The tutorial by Vergari, Choi, Peharz and Van den
Broek at AAAI-2020 oﬀers an excellent review of proba-
bilistic circuits, including arithmetic circuits, SPNs, cutset
networks (CNets), probabilistic sentential decision dia-
grams (PSDDs), with many references for inference, learn-
ing, applications, hardware implementations, etc.10
X. Conclusion
SPNs are closely related to probabilistic graphical mod-
els (PGMs), such as Bayesian networks and Markov net-
works, but have the advantage of allowing the construction
of tractable models from data. SPNs have been applied
to the same tasks as neural networks, mainly image and
natural language processing, which exceeded by far the
capabilities of PGMs. Even though deep neural networks
yield in general better results, SPNs have the possibility
of automatically building the structure from data and
learning the parameters with either gradient descent or
some of the algorithms developed for probabilistic models.
In this paper we have tried to oﬀer a gentle intro-
duction to SPNs, collecting information that is spread
in many publications and presenting it in a coherent
framework, trying to keep the mathematical complexity
to the minimum necessary for describing with rigor the
main properties of SPNs, from their deﬁnition to the
algorithms for parametric and structural learning. We have
intentionally avoided presenting SPNs as representations
of network polynomials; readers interested in them can
consult [9] and references therein. We have then reviewed
several applications of SPNs in diﬀerent domains, some
extensions, and the main software libraries for SPNs.
Given the rapid growth of the literature about SPNs, some
sections of this paper might become obsolete soon, but we
still hope it will be useful for those researchers who wish
to get acquainted with this fascinating topic.
10http://starai.cs.ucla.edu/slides/AAAI20.pdf.
Acknowledgments
We thank Pascal Poupart for convincing us about the
advantages of SPNs, and Concha Bielza, Adnan Darwiche,
Pedro Larrañaga, Alejandro Molina, Andrzej Pronobis,
Martin Trapp, Jos van de Wolfshaar, Antonio Vergari and
Kaiyu Zheng for many useful comments. However, all the
possible mistakes and omissions are our sole responsibility.
This research has been supported by grant TIN2016-
77206-R from the Spanish Government, co-ﬁnanced by
the European Regional Development Fund. IP received
a predoctoral grant and RSC a postdoctoral grant from
UNED, both co-ﬁnanced by the Regional Government of
Madrid with funds from the Youth Employment Initiative
(YEI) of the European Union.
Appendices
Appendix A. From arithmetic circuits to SPNs
SPNs originated as a modiﬁcation of arithmetic circuits
(ACs) [3]. Both models consist of rooted ADG with three
types of nodes: sum, product, and leaves, but a leaf node
in an AC can only be a non-negative parameter or an
indicator for a Boolean variable. In contrast, the numeric
parameters of SPNs are placed in the links outgoing from
sum nodes and leaf nodes can be any univariate distri-
bution, not only indicators.11 ACs are smooth (i.e., com-
plete), decomposable, and deterministic (i.e., selective).
SPN are complete and decomposable; decomposability
might be replaced by a weaker condition, consistency
[1], but this generalization has no practical utility. So
the main diﬀerence between both models is that SPNs
are not necessarily selective (i.e., deterministic), which
makes them capable of representing some problems with
exponentially smaller networks than ACs [97, Sec. 5], but
at the price of making much harder both exact MPE (cf.
Sec. IV-B and [97, Sec. 4]) and parameter learning (cf.
Sec. V).
ACs were initially proposed as an eﬃcient represen-
tation of the polynomials of Bayesian networks (BNs),
useful for some inference tasks [3]. Later Chavira and
Darwiche [11] developed an alternative method for com-
piling BNss that may yield exponentially smaller ACs
in some cases, and Choi and Darwiche [97] showed that
ACs can represent arbitrary factors (called “potentials” in
other publications), not only the conditional probability
distributions of BNs.
As we said in the introduction, the ﬁrst papers about
SPNs [1], [5] presented them as representations of network
polynomials of probability distributions, just as in the case
of ACs, while most posterior references, beginning with [6],
present them as combinations of probability distributions,
which is a signiﬁcant diﬀerence with respect to ACs.
An important novelty in the area of ACs was the
algorithm by Lowd and Domingos [90] for learning ACs
directly from data, instead of compiling them from BNs.
11Section III-B explains why it makes sense to use indicators in
the place of univariate distributions.

19
In contrast, SPNs were proposed from their start [1] as
a method for building probabilistic models from data.
Nowadays there are many algorithms for learning ACs
and other types of probabilistic circuits. The references
can be found in the tutorial by Vergari et al. that we
recommended at the end of Section IX.
Appendix B. Interpretation of sum nodes as weighted av-
erages of conditional probabilities
In this appendix we show that when a sum node repre-
sents a a variable V , this node can be interpreted as the
average of the conditional probabilities given V , weighted
by the probabilities P(v). We study ﬁrst the case in which
this node is the root.
Proposition 29. Let S be an SPN, whose scope contains
at least two variables, such that its root is a sum node that
represents a variable V having m values. For every j, let
nσ(j) be the product node associated with value vj, as in
Deﬁnition 15. Then
P(vj) = wi,σ(j) .
(34)
Additionally, we can deﬁne eX = sc(ni) \ {V }, so that if
wi,σ(j) ̸= 0 then for every conﬁguration ex we have
P(ex | vj) =
Y
k∈ch(σ(j))\nk̸=Ivj
Sk(x) .
(35)
This proposition is especially interesting when each child
of the root, nσ(j), has only two children, namely Ivj and
another node, say nk(j)—as in Figures 1 and 2. In this case
the latter equation reduces to
P(ex | vj) = Sk(j)(x) .
(36)
Given that sc(nk(j)) = eX for every j, Theorem 11 implies
that Sk(j)(ex) = Pk(j)(ex). Therefore, the probability com-
puted by the root,
P(ex) = S(ex) =
X
j
wi,σ(j) · Sk(j)(ex) ,
(37)
can be interpreted as a weighted average of the probabil-
ities Pk(j)(ex),
P(ex) =
X
j
wi,σ(j) · Pk(j)(ex) ,
(38)
or as a particular case of the law of total probability:
P(ex) = P(ex | vj) · P(vj) .
(39)
Example 30. The root node in Figure 2 represents
variable A, so its children represent the probabilities P(a):
P(a1) = P(+a) = w1,σ(1) = w1,2 = 0.3
P(a2) = P(¬a) = w1,σ(2) = w1,3 = 0.7 .
For every ex ∈conf∗( eV) = {B, C}—for example, (+b, +c),
(+b), or (+c)—we have
P(ex | +a) =
Y
k∈ch(2)\nk̸=I+a
Sk(x) = S6(ex)
P(ex | ¬a) =
Y
k∈ch(3)\nk̸=I¬a
Sk(x) = S7(ex)
and
P(ex) = P(ex | +a)
|
{z
}
S6(ex)
· P(+a)
| {z }
w1,2
+ P(ex | ¬a)
|
{z
}
S7(ex)
· P(¬a)
| {z }
w1,3
.
If every ancestor of a sum node ni represents a variable,
then the above interpretation is still valid for the context
deﬁned by the ancestors of ni, A. In this case Equation 40
becomes
P(ex | a) =
m
X
j=1
P(ex | vj, a)
|
{z
}
Sk(j)(ex)
· P(vj | a)
|
{z
}
wi,σ(j)
.
(40)
Example 31. Node n6 in Figure 1 represents variable B.
Its only ancestor sum node, n1, represents variable A. The
path from the root to n6 deﬁnes the scenario {A = +a}.
We have sc(n6) = {B, C} and eV = sc(n6) \ {B} = {C}.
In this example Equation 40 instantiates into
P(c | +a) = P(c | +b, +a)
|
{z
}
S14(ex)
· P(+b | +a)
|
{z
}
w6,8
+
P(c | ¬b, +a)
|
{z
}
S15(ex)
· P(¬b | +a)
|
{z
}
w6,9
.
Similarly, n7 corresponds to the scenario {A = ¬a}, so
P(c | ¬a) = P(c | +b, ¬a)
|
{z
}
S16(ex)
· P(+b | ¬a)
|
{z
}
w7,10
+
P(c | ¬b, ¬a)
|
{z
}
S17(ex)
· P(¬b | ¬a)
|
{z
}
w7,11
.
Appendix C. Proofs
This appendix contains the proofs of all the propositions
and the theorem.
Proof of Proposition 5:
It follows from Equation 6
that P(x) ≥0 and
X
x
P(x) =
X
x
X
v|v↓X=x
P(v) .
Given that each conﬁguration of V is compatible with
exactly one conﬁguration of X, we have
X
x
P(x) =
X
v
P(v) = 1 .
Before proving Propositions 6 and 7, we introduce a new
result.
Proposition 32. Let P be a function P : conf∗(V) 7→R
that satisﬁes Equation 6. If P(♦) = 1, then P
v P(v) = 1.

20
Proof: Because of Equation 6, with X = ∅, we have
P(♦) =
X
v|v↓∅=♦
P(v) .
Taking into account that the empty conﬁguration is com-
patible with every conﬁguration of every set,
P(♦) =
X
v
P(v) .
Proof of Proposition 6: We ﬁrst prove that P satisﬁes
Equation 6 and then that it is a probability distribution:
P(x) =
n
X
j=1
wj · Pj(x) =
n
X
j=1
wj ·
X
v|v↓X=x
Pj(v)
=
X
v|v↓X=x
n
X
j=1
wj · Pj(v) =
X
v|v↓X=x
P(v) ,
It is clear that P(v) ≥0 for all v ∈conf(V) and, because
of Proposition 32,
X
v
P(v) = P(♦) =
n
X
j=1
wj · Pj(♦) =
n
X
j=1
wj = 1 .
Proof of Proposition 7:
When n = 1 the proof is
trivial because P = P1. When n = 2 we have, for every
conﬁguration x, with X ⊆V,
X
v|v↓X=x
P(v) =
X
v|v↓X=x
P1(v↓V1) · P2(v↓V2) .
Given that V1 ∪V2 = V and V1 ∩V2 = ∅, every
conﬁguration of V can be obtained from a composition
of two conﬁgurations, v = v1v2, where v1 = v↓V1 and
v2 = v↓V1. The condition v↓X = x—the compatibility
of x with v—can be decomposed into two conditions:
v↓X1
1
= x↓X1 and v↓X2
2
= x↓X2, where X1 = V1 ∩X
and X2 = V2 ∩X . Therefore,
X
v|v↓X=x
P(v) =
X
v1|v↓X1=x↓X1
X
v2|v↓X2=x↓X2
P1(v1) · P2(v2) .
The property V1 ∩V2 = ∅also implies that P1(v1) does
not depend on v2 and vice versa, so
X
v|v↓X=x
P(v) =


X
v1|v1=x↓X1
P1(v1)

·
X
v2|v2=x↓X2
P2(v2) .
The fact that P1 is a probability function implies that
P1(x↓X1) =
X
v1|v1=x↓X1
P1(v1)
and the deﬁnition X1 = V1∩X implies that x↓X1 = x↓V1.
Therefore
X
v|v↓X=x
P(v) = P1(x↓V1) · P2(x↓V2) = P(x) ,
which proves that P satisﬁes Equation 6. It is clear that
P(v) ≥0 for all v ∈conf(V) and, because of Proposi-
tion 32,
X
v
P(v) = P(♦) = P1(♦) · P2(♦) = 1 ,
which completes the proof for n = 2. As a consequence, if
P1(x↓V1)·. . .·Pn−1(x↓Vn−1) and Pn(x↓Vn) are probability
functions for disjoint Vj’s, then P1(x↓V1) · . . . · Pn(x↓Vn)
is also a probability function, which proves Proposition 7
for any value of n.
Proof of Proposition 8: Let nj and nj′ be two diﬀerent
children of ni. If descendants(nj) ∩descendants(nj′) = ∅,
then sc(nj) ∩sc(nj′) = ∅, i.e., their scopes are disjoint.
Reciprocally, if a node nk is a descendant of both nj
and nj′ then sc(nk) ⊆sc(nj) ∩sc(nj′), which implies that
the scopes are not disjoint because, by the deﬁnition of
scope, sc(nk) ̸= ∅.
Proof of Theorem 11:
Let ni be a terminal node
representing P(v). In this case the set V in Deﬁnition 4
is {V }, and Pi(v) = Si(v) = P(v) is a probability
distribution. If X ⊂V then X = ∅and Equation 6 holds
because, on the one hand, Pi(♦) = Si(♦) = 1 (Eqs. 13
and 16) and, on the other,
X
v|v↓∅=♦
Pi(v) =
X
v
Pi(v) = 1 .
Let ni be a non-terminal node. We assume that Pj
is a probability distribution for each of its children, nj.
If ni is a sum node then Pi is a probability functiojn
because of Proposition 6, with V = sc(ni) = sc(nj). The
completeness of the SPN guarantees that ni and all its
children have the same scope. If ni is a product node then
Pi is a probability function because of Proposition 7, with
Vj = sc(nj). The decomposability of the SPN guarantees
that the Vj’s are disjoint and the deﬁnition of sc(ni)
ensures that S
j Vj = V = sc(ni).
The following example shows that if an SPN is not
complete it may overestimate the probability of some
conﬁgurations.
Example 33. Let S be an SPN whose root, n0, is a sum
node with two children, n1 and n2, such that w0,1 = 0.6
and w0,2 = 0.4. These children represent two probability
distributions, P1 and P2, deﬁned on V1 and V2, respec-
tively, with P1(+v1) = P1(¬v1) = P2(+v2) = P2(¬v2) =
0.5. Then S(+v1) = w0,1 · S1(+v1) + w0,2 · S2(+v1) =
0.6 · 0.5 + 0.4 · 1 = 0.7 and S(¬v1) = 0.7. S(x) is not a
probability function because S(+v1) + S(¬v1) > 1.
The following example shows that if an SPN is not
decomposable it may underestimate the probability of
some conﬁgurations.
Example 34. Let S be an SPN whose root, n0, is a prod-
uct node with two children, n1 and n2, which represent two
probability distributions, P1 and P2 respectively, both de-
ﬁned on V , with P1(+v) = P1(¬v) = P2(+v) = P2(¬v) =
0.5. Then S(+v) = S1(+v) · S2(+v) = 0.5 · 0.5 = 0.25 and
S(¬v) = 0.25. S(x) is not a probability function because

21
S(+v)+S(¬v) < 1. In this case it is not possible to invoke
Proposition 7 because P1 and P2 are not deﬁned on disjoint
subsets of variables.
This is the reason why SPNs are required to be complete
and decomposable.
Proof of Proposition 17:
Let v ∈conf(S) and j∗
the integer in {1, . . . , m} such that vj∗
=
v↓V . Let
j ∈{j1,. . . , m} with j ̸= j∗; then Ivj(v) = 0 because
vj ̸= v↓V . If nσ(j) = Ivj (the indicator is a child of ni)
then Sσ(j)(v) = Ivj(v) = 0. If nσ(j) = Ivj (the indicator
is a grandchild of ni) then nσ(j) is a product node and
the contributions of its other children are multiplied by 0,
which implies that Sσ(j)(v) = 0.
Proof of Proposition 18:
As the augmentation pro-
cesses the nodes one by one, it suﬃces to prove the
proposition for the SPN that results from augmenting one
non-selective node, ni. The addition of the indicators for Z
makes this node selective in S′ because of Deﬁnition 15
and Proposition 17, without aﬀecting its completeness.
The addition of n′
k makes all its ancestors complete and
decomposable, without aﬀecting selectivity—see Figure 3.
Let x ∈conf∗(S). Given that Z /∈sc(S), Iz(j) = 1 for
every j. If we denote by S′
h(x) the value of node nh in S′,
we have S′
j(x) = Sj(x), S′
j′(x) = Sj′(x), S′
i(x) = Si(x),
S′
k(x) = Sk(x), S′
i′(x) = 1, S′
k′(x) = Sk(x), S′
l(x) = Sl(x),
etc. Therefore, for any node nh ∈S we have S′
h(x) =
Sh(x) and, consequently, P ′(x) = P(x).
Proof of Proposition 20:
Sv contains all the paths
making positive contributions to S(v).
Proof of Proposition 21: Selectivity implies that if ni
is a sum node in S and Si(v) ̸= 0, then it has exactly one
child nj such that Sj(v) ̸= 0 and wij > 0 (which implies
that this link has not been removed), so in Sv every sum
node ni has exactly one child. Every node in Sv other than
the root has at least one parent—otherwise, it would have
been removed. We now prove that no node can have more
than one parents. Let us assume that a certain node nj has
two parents, ni and ni′. They have at least one common
ancestor, the root. Let nk an ancestor of both ni and ni′
such that no descendant of nk is an ancestor of both of
them. Node nk cannot be a sum node because each sum
node has only one child. It cannot be a product node either
because then nk would have two children with a common
descendant, nj, which is impossible in a decomposable
SPN (cf. Prop. 8). Therefore assuming that a node has
two or more parents leads to a contradiction.
Proof of Proposition 23: The proposition holds triv-
ially when the SPN contains only one node. Let us consider
a non-terminal node ni. For each child nj the sub-SPN
rooted at nj is also selective. We assume that the proposi-
tion holds for each of these sub-SPNs. If ni is a sum node,
the proposition holds for ni because it only has only one
child in Tv and Sv(v) = S(v). If ni is a product node, the
proposition follows from Equation 15.
Proof of Corollary 24: Tv only contains the indicators
for which Sk(v) = 1.
Proof of Proposition 27: Equation 18 implies that
log S(vt | w) =
X
(i,j)∈Tvt
log wij + ct ,
(41)
where
ct =
(
log Sk(vt)
if nk is terminal in Tvt
0
otherwise .
Whether nk is terminal in Tvt does not depend on the
actual values of the weights, provided that they are all
diﬀerent from 0. If we deﬁne
c =
T
X
t=1
ct
we have
LD(w) =
T
X
t=1
log S(vt | w) =
X
wij∈W
nij · log wij + c ,
where nij is the number of instances in the dataset for
which (i, j) ∈Tvt and c does not depend on w.
Now, before proving Proposition 28, we introduce an
auxiliary result, adapted from [3].
Proposition 35. If S is selective, v ∈conf(S), S(v) ̸= 0,
and (i, j) ∈Sv, then
S(v) = wij · ∂S(v)
∂wij
.
(42)
Proof: Equation 18 implies wij ̸= 0 (because S(v) ̸=
0) and
∂S(v)
∂wij
= S(v)
wij
.
Proof of Proposition 28: Given that vtht is a complete
conﬁguration of sc(S′), i.e., vtht ∈conf(S′), and S′ is
selective, Proposition 35 implies that
S′(vtht) = wij · ∂S′(vtht)
∂wij
and
P ′(ht | vt) = S′(vtht)
S′(vt)
= wij ·
1
S′(vt) · ∂S′(vtht)
∂wij
.
So Equation 32 can be rewritten as
nij =
T
X
t=1
wij ·
1
S′(vt) ·
X
ht | (i,j)∈T ′
vtht
∂S′(vtht)
∂wij
.
If link (i, j) does not belong to the tree induced by
vtht then ∂S′(vtht)/∂wij = 0, so the inner summation
in the previous expression can be extended to all the
conﬁgurations of Ht:
nij =
T
X
t=1
wij ·
1
S′(vt) ·
X
ht
∂S′(vtht)
∂wij
.

22
Given that S′(vt) = P
ht S′(vtht), we have
nij =
T
X
t=1
wij ·
1
S′(vt) · ∂S′(vt)
∂wij
.
Because of Proposition 18, S′(vt) = S(vt) and
nij =
T
X
t=1
wij ·
1
S(vt) · ∂S(vt)
∂wij
=
T
X
t=1
wij ·
1
S(vt) · ∂S(vt)
∂Si
· ∂Si(vt)
∂wij
.
This result, together with Equations 26 and 14, leads to
Equation 33.
Proof of Proposition 29: Let j ∈{1, . . . m}. Since nr
represents variable V , either Ivj = nσ(j) or Ivj is a child
of nσ(j)—see Deﬁnition 15. In the ﬁrst case, we would
have sc(nσ(j)) = {V } and the completeness of S would
imply that sc(nr) = sc(nr) = {V }, in contradiction
with the assumption that sc(S) has at least to variables.
Therefore Ivj must be a child of nσ(j), a product node,
and
Sσ(j)(x) = Ivj(x) ·
Y
k∈ch(σ(j))\nk̸=Ivj
Sk(x) ,
for every x ∈conf∗(S). Let x = vjex, i.e., the composition
of vj and any ex ∈conf∗( eV). When j′ ̸= j we have
Ivj(vjex) = Iv′
j(vj) = 0 and Sσ(j′)(vjex) = 0. On the other
hand,
Sσ(j)(vjex) = Ivj(vj) ·
Y
k∈ch(σ(j))\nk̸=Ivj
Sk(ex) ,
=
Y
k∈ch(σ(j))\nk̸=Ivj
Sk(ex) .
Since nr is a sum node,
P(vjex) =
m
X
j′=1
wi,σ(j′) · Sσ(j′)(vjex)
= wi,σ(j) ·
Y
k∈ch(σ(j))\nk̸=Ivj
Sk(ex) .
In particular, if ex = ♦then Sk(♦) = 1 for every k and
P(vj) = wi,σ(j) .
If wi,σ(j) ̸= 0,
P(vj | ex) = P(vjex)
P(vj) =
Y
k∈ch(σ(j))\nk̸=Ivj
Sk(ex) .
References
[1] H. Poon and P. Domingos, “Sum-product networks: a new
deep architecture,” in Proceedings of the 12th Conference on
Uncertainty in Artiﬁcial Intelligence (UAI), pp. 337–346, 2011.
[2] A. Darwiche, “A logical approach to factoring belief networks,”
in 8th International Conference on Principles and Knowledge
Representation and Reasoning (KR), 2002.
[3] A. Darwiche, “A diﬀerential approach to inference in Bayesian
networks,” Journal of the ACM, vol. 50, pp. 280–305, 2003.
[4] D. Koller and N. Friedman, Probabilistic Graphical Models:
Principles and Techniques. Cambridge, MA: The MIT Press,
2009.
[5] R. Gens and P. Domingos, “Discriminative learning of sum-
product networks,” in Proceedings of the 26th Conference on
Neural Information Processing Systems (NIPS), pp. 3239–3247,
2012.
[6] R. Gens and P. Domingos, “Learning the structure of sum-
product networks,” in Proceedings of the 30th International
Conference on Machine Learning (ICML), pp. 873–880, 2013.
[7] R. Peharz, R. Gens, F. Pernkopf, and P. M. Domingos, “On the
latent variable interpretation in sum-product networks,” IEEE
Transactions on Pattern Analysis and Machine Intelligence,
vol. 39, pp. 2030–2044, 2017.
[8] R. Peharz, S. Tschiatschek, F. Pernkopf, and P. Domingos,
“On theoretical properties of sum-product networks,” in In-
ternational Conference on Artiﬁcial Intelligence and Statistics
(AISTATS), 2015.
[9] R. Peharz, Foundations of sum-product networks for probabilis-
tic modeling. PhD thesis, 2015.
[10] J. Pearl, Probabilistic Reasoning in Intelligent Systems: Net-
works of Plausible Inference.
San Mateo, CA: Morgan Kauf-
mann, 1988.
[11] M. Chavira and A. Darwiche, “Compiling Bayesian networks
using variable elimination,” in Proceedings of the 20th Inter-
national Joint Conference on Artiﬁcial Intelligence (IJCAI),
pp. 2443–2449, 2007.
[12] C. Boutilier, N. Friedman, M. Goldszmidt, and D. Koller,
“Context-speciﬁc independence in Bayesian networks,” in Pro-
ceedings of the 12th Conference on Uncertainty in Artiﬁcial
Intelligence (UAI), pp. 115–123, 1996.
[13] H. Zhao, M. Melibari, and P. Poupart, “On the relationship
between sum-product networks and Bayesian networks,” in
Proceedings of the 32nd International Conference on Machine
Learning (ICML), pp. 116–124, 2015.
[14] J. Pearl, Causality. Models, Reasoning, and Inference.
Cam-
bridge, UK: Cambridge University Press, 2000.
[15] I. Bermejo, J. Oliva, F. J. Díez, and M. Arias, “Interactive learn-
ing of Bayesian networks with OpenMarkov,” in Proceedings of
the 6th Conference on Probabilistic Graphical Models (PGM),
pp. 27–34, 2012.
[16] K. O. Stanley and R. Miikkulainen, “Evolving neural networks
through augmenting topologies,” Evolutionary Computation,
vol. 10, pp. 99–127, 2002.
[17] S. Ding, H. Li, C. Su, J. Yu, and F. Jin, “Evolutionary artiﬁ-
cial neural networks: a review,” Artiﬁcial Intelligence Review,
vol. 39, pp. 251–260, 2013.
[18] R. Peharz, A. Vergari, K. Stelzner, A. Molina, M. Trapp, K. Ker-
sting, and Z. Ghahramani, “Probabilistic deep learning using
random sum-product networks,” arXiv:1806.01910, 2018.
[19] K. Stelzner, R. Peharz, and K. Kersting, “Faster Attend-Infer-
Repeat with tractable probabilistic models,” in Proceedings
of the 36th International Conference on Machine Learning
(ICML), pp. 5966–5975, 2019.
[20] J. D. Park, “MAP complexity results and approximation meth-
ods,” in Proceedings of the 9th Conference on Uncertainty in
Artiﬁcial Intelligence (UAI), pp. 388–396, 2002.
[21] A. Rooshenas and D. Lowd, “Learning sum-product networks
with direct and indirect variable interactions,” in Proceedings
of the 31st International Conference on Machine Learning
(ICML), pp. 710–718, 2014.
[22] A. Molina, S. Natarajan, and K. Kersting, “Poisson sum-
product networks: A deep architecture for tractable multivariate
poisson distributions,” in Proceedings of the 31st AAAI Confer-
ence on Artiﬁcial Intelligence, pp. 2357–2363, 2017.

23
[23] A. Molina, A. Vergari, N. Di Mauro, S. Natarajan, F. Espos-
ito, and K. Kersting, “Mixed sum-product networks: A deep
architecture for hybrid domains,” in 32nd AAAI Conference on
Artiﬁcial Intelligence, 2018.
[24] M.
Desana
and
C.
Schnörr,
“Learning
arbitrary
sum-
product
network
leaves
with
Expectation-Maximization,”
arXiv:1604.07243, 2017.
[25] W. Hsu, A. Kalra, and P. Poupart, “Online structure learning
for sum-product networks with Gaussian leaves,” in 5th In-
ternational Conference on Learning Representations (ICLR) -
Workshop Track, 2017.
[26] A. Vergari, N. Di Mauro, and F. Esposito, “Simplifying, regular-
izing and strengthening sum-product network structure learn-
ing,” in Proceedings of the European Conference on Machine
Learning and Principles and Practice of Knowledge Discovery
in Databases (ECMLPKDD), pp. 343–358, 2015.
[27] R. Peharz, R. Gens, and P. Domingos, “Learning selective
sum-product networks,” in 31st International Conference on
Machine Learning (ICML) - Learning Tractable Probabilistic
Models (LTPM) Workshop, 2014.
[28] H. Zhao, P. Poupart, and G. J. Gordon, “A uniﬁed approach for
learning the parameters of sum-product networks,” in Proceed-
ings of the 30th Conference on Neural Information Processing
Systems (NIPS), pp. 433–441, 2016.
[29] C. J. Butz, J. S. Oliveira, A. E. dos Santos, A. L. Teixeira,
P. Poupart, and A. Kalra, “An empirical study of methods for
SPN learning and inference,” in Proceedings of the 9th Inter-
national Conference on Probabilistic Graphical Models (PGM),
pp. 49–60, 2018.
[30] H. Chan and A. Darwiche, “On the robustness of most probable
explanations,” in 22nd Conference on Uncertainty in Artiﬁcial
Intelligence (UAI), 2006.
[31] J. Mei, Y. Jiang, and K. Tu, “Maximum a posteriori inference in
sum-product networks,” in 32nd AAAI Conference on Artiﬁcial
Intelligence, 2018.
[32] A. Rashwan, P. Poupart, and C. Zhitang, “Discriminative train-
ing of sum-product networks by extended Baum-Welch,” in
Proceedings of the 9th International Conference on Probabilistic
Graphical Models (PGM), pp. 356–367, 2018.
[33] M. Trapp, T. Madl, R. Peharz, F. Pernkopf, and R. Trappl,
“Safe semi-supervised learning of sum-product networks,” in
33rd Conference Uncertainty in Artiﬁcial Intelligence (UAI),
2017.
[34] M. Loog, “Contrastive pessimistic likelihood estimation for
semi-supervised classiﬁcation,” IEEE Transactions on Pattern
Analysis and Maching Intelligence, vol. 38, pp. 462–475, 2016.
[35] A. Rashwan, H. Zhao, and P. Poupart, “Online and distributed
Bayesian moment matching for parameter learning in sum-
product networks,” in Proceedings of the 19th International
Conference on Artiﬁcial Intelligence and Statistics (AISTATS),
pp. 1469–1477, 2016.
[36] H. Zhao, T. Adel, G. Gordon, and B. Amos, “Collapsed varia-
tional inference for sum-product networks,” in Proceedings of the
33rd International Conference on Machine Learning (ICML),
pp. 1310–1318, 2016.
[37] P. Jaini, A. Rashwan, H. Zhao, Y. Liu, E. Banijamali, Z. Chen,
and P. Poupart, “Online algorithms for sum-product networks
with continuous variables,” in Proceedings of the 8th Interna-
tional Conference on Probabilistic Graphical Models (PGM),
pp. 228–239, 2016.
[38] H. Zhao and G. J. Gordon, “Linear time computation of mo-
ments in sum-product networks,” in Proceedings of the 26th
Conference on Neural Information Processing Systems (NIPS),
pp. 6894–6903, 2017.
[39] A. Dennis and D. Ventura, “Learning the architecture of sum-
product networks using clustering on variables,” in Proceedings
of the 26th Conference on Neural Information Processing Sys-
tems (NIPS), pp. 3239–3247, 2012.
[40] R. Peharz, B. C. Geiger, and F. Pernkopf, “Greedy part-wise
learning of sum-product networks,” in Proceedings of the Eu-
ropean Conference on Machine Learning and Principles and
Practice of Knowledge Discovery in Databases (ECMLPKDD),
pp. 612–627, 2013.
[41] D. Lowd and A. Rooshenas, “Learning Markov networks with
arithmetic circuits,” in Proceedings of the 16th International
Conference on Artiﬁcial Intelligence and Statistics (AISTATS),
pp. 406–414, 2013.
[42] T. Adel, D. Balduzzi, and A. Ghodsi, “Learning the structure
of sum-product networks via an SVD-based algorithm,” in Pro-
ceedings of the 31sr Conference on Uncertainty in Artiﬁcial
Intelligence (UAI), pp. 32–41, 2015.
[43] C. Chow and C. Liu, “Approximating discrete probability dis-
tributions with dependence trees,” IEEE Transactions on In-
formation Theory, vol. 14, pp. 462–467, 1968.
[44] L. Breiman, “Random forests,” Machine learning, vol. 45, pp. 5–
32, 2001.
[45] A. Dennis and D. Ventura, “Greedy structure search for sum-
product networks,” in Proceedings of the 24th International
Joint Conference on Artiﬁcial Intelligence (IJCAI), pp. 932–
938, 2015.
[46] T. Rahman and V. Gogate, “Merging strategies for sum-product
networks: from trees to graphs,” in Proceedings of the 32nd
Conference on Uncertainty in Artiﬁcial Intelligence (UAI),
pp. 617–626, 2016.
[47] N. Di Mauro, F. Esposito, F. Ventola, and A. Vergari, “Alterna-
tive variable splitting methods to learn sum-product networks,”
in AI*IA Advances in Artiﬁcial Intelligence: 16th International
Conference of the Italian Association for Artiﬁcial Intelligence,
2017.
[48] Y. Liu and T. Luo, “The optimization of sum-product network
structure learning,” Journal of Visual Communication and Im-
age Representation, vol. 60, pp. 391–397, 2019.
[49] A. Bueﬀ, S. Speichert, and V. Belle, “Tractable querying
and learning in hybrid domains via sum-product networks,”
arXiv:1807.05464, 2018.
[50] G. Schwarz, “Estimating the dimension of a model,” The Annals
of Statistics, vol. 17, pp. 461–464, 1978.
[51] H. Gebelein, “Das statistische Problem der Korrelation als
Variations-und Eigenwertproblem und sein Zusammenhang mit
der Ausgleichsrechnung,” Zeitschrift für Angewandte Mathe-
matik und Mechanik, vol. 21, pp. 364–379, 1941.
[52] S. Lee, M. Heo, and B. Zhang, “Online incremental structure
learning of sum-product networks,” in Proceedings of the 20th
International Conference on Neural Information Processing
(ICONIP), pp. 220–227, 2013.
[53] A. Dennis and D. Ventura, “Online structure-search for sum-
product networks,” in Proceedings of the 16th IEEE Inter-
national Conference on Machine Learning and Applications
(ICMLA), pp. 155–160, 2017.
[54] P. Jaini, A. Ghose, and P. Poupart, “Prometheus: directly
learning acyclic directed graph structures for sum-product net-
works,” in Proceedings of the 9th International Conference on
Probabilistic Graphical Models (PGM), pp. 181–192, 2018.
[55] M. Melibari, P. Poupart, P. Doshi, and G. Trimponias, “Dy-
namic sum-product networks for tractable inference on sequence
data,” in Proceedings of the 8th Conference on Probabilistic
Graphical Models (PGM), pp. 345–356, 2016.
[56] T. Dean and K. Kanazawa, “A model for reasoning about
persistence and causation,” Computational Intelligence, vol. 5,
pp. 142–150, 1989.
[57] A. Kalra, A. Rashwan, W.-S. Hsu, P. Poupart, P. Doshi, and
G. Trimponias, “Online structure learning for feed-forward and
recurrent sum-product networks,” in Advances in Neural Infor-
mation Processing Systems 31, pp. 6944–6954, 2018.
[58] A. Nath and P. Domingos, “Learning relational sum-product
networks,” in Proceedings of the 29th AAAI Conference on
Artiﬁcial Intelligence, pp. 2878–2886, 2015.
[59] C. Lee, S. Watkins and B. Zhang, “Non-parametric Bayesian
sum-product networks.,” in International Conference on Ma-
chine Learning (ICML). Learning Tractable Probabilistic Mod-
els Workshop, 2014.
[60] M. Trapp, R. Peharz, M. Skowron, T. Madl, F. Pernkopf,
and R. Trappl, “Structure inference in sum-product networks
using inﬁnite sum-product trees,” in 30th Conference on Neural
Information Processing Systems (NIPS), 2016.
[61] M. Trapp, R. Peharz, H. Ge, F. Pernkopf, and Z. Ghahramani,
“Bayesian learning of sum-product networks,” in Proceedings of
the 33rd Conference on Neural Information Processing Systems
(NeurIPS), pp. 6344–6355, 2019.
[62] Z. Yuan, H. Wang, L. Wang, T. Lu, S. Palaiahnakote, and C. L.
Tan, “Modeling spatial layout for scene image understanding via
a novel multiscale sum-product network,” Expert Systems with
Applications, vol. 63, pp. 231–240, 2016.

24
[63] F. Rathke, M. Desana, and C. Schnörr, “Locally adaptive prob-
abilistic models for global segmentation of pathological OCT
scans,” in Proceedings of the 20th International Conference on
Medical Image Computing and Computer Assisted Intervention
(MICCAI), pp. 177–184, 2017.
[64] J. Wang and G. Wang, “Hierarchical spatial sum-product net-
works for action recognition in still images,” IEEE Transactions
on Circuits and Systems for Video Technology, vol. 28, pp. 90–
100, 2016.
[65] M. R. Amer and S. Todorovic, “Sum product networks for
activity recognition,” IEEE Transactions on Pattern Analysis
and Machine Intelligence, vol. 38, pp. 800–813, 2016.
[66] B. M. Sguerra and F. G. Cozman, “Image classiﬁcation using
sum-product networks for autonomous ﬂight of micro aerial
vehicles,” in Proceedings of the 5th Brazilian Conference on
Intelligent Systems (BRACIS), pp. 139–144, 2016.
[67] A. Pronobis, F. Riccio, and R. P. N. Rao, “Deep spatial aﬀor-
dance hierarchy: spatial knowledge representation for planning
in large-scale environments,” in 27th International Conference
on Automated Planning and Scheduling (ICAPS) - Workshop
on Planning and Robotics, 2017.
[68] K. Zheng, A. Pronobis, and R. P. N. Rao, “Learning semantic
maps with topological spatial relations using graph-structured
sum-product networks,” in AAAI Conference on Artiﬁcial In-
telligence, 2017.
[69] K. Zheng and A. Pronobis, “From pixels to buildings: end-to-end
probabilistic deep networks for large-scale semantic mapping,”
in IEEE/RSJ International Conference on Intelligent Robots
and Systems (IROS), pp. 3511–3518, 2019.
[70] R. Peharz, G. Kapeller, P. Mowlaee, and F. Pernkopf, “Modeling
speech with sum-product networks: application to bandwidth
extension,” in Proceedings of the IEEE International Confer-
ence on Acoustics, Speech and Signal Processing (ICASSP),
pp. 3699–3703, 2014.
[71] W.-C. Cheng, S. Kok, H. V. Pham, H. L. Chieu, and K. M. A.
Chai, “Language modeling with sum-product networks,” in
Proccedings of the 15th Annual Conference of the International
Speech Communication Association (ISCA), pp. 2098–2102,
2014.
[72] M. Ratajczak, S. Tschiatschek, and F. Pernkopf, “Sum-product
networks for sequence labeling,” arXiv:1807.02324, 2018.
[73] A. Vergari, R. Peharz, N. Di Mauro, A. Molina, K. Kersting,
and F. Esposito, “Sum-product autoencoding: Encoding and
decoding representations using sum-product networks,” in 32nd
AAAI Conference on Artiﬁcial Intelligence, 2018.
[74] C. J. Butz, A. E. dos Santos, J. S. Oliveira, and J. Stavrinides,
“Eﬃcient examination of soil bacteria using probabilistic graph-
ical models,” in Proceedings of the 31st International Conference
on Industrial Engineering and Other Applications of Applied
Intelligent Systems (IEA/AIE), pp. 315–326, 2018.
[75] A. Nath and P. Domingos, “Learning tractable probabilistic
models for fault localization,” in Proceedings of the 30th AAAI
Conference on Artiﬁcial Intelligence, pp. 1294–1301, 2016.
[76] B. Hilprecht, A. Schmidt, M. Kulessa, A. Molina, K. Kersting,
and C. Binnig, “DeepDB: Learn from data, not from queries!,”
arXiv:1909.00607, 2019.
[77] A. Vergari, N. Di Mauro, and F. Esposito, “Visualizing
and understanding sum-product networks,” Machine Learning,
vol. 108, pp. 551–573, 2019.
[78] Y. Bengio, A. Courville, and P. Vincent, “Representation learn-
ing: A review and new perspectives,” IEEE Transactions on
Pattern Analysis and Machine Intelligence, vol. 35, pp. 1798–
1828, 2013.
[79] A. Vergari, A. Molina, R. Peharz, Z. Ghahramani, K. Kersting,
and I. Valera, “Automatic Bayesian density analysis,” in Pro-
ceedings of the 33rd AAAI Conference on Artiﬁcial Intelligence,
vol. 33, pp. 5207–5215, 2019.
[80] C. Roy, M. Shanbhag, M. Nourani, T. Rahman, S. Kabir,
V. Gogate, N. Ruozzi, and E. D. Ragan, “Explainable activity
recognition in videos,” in 24th International Conference on
Intelligent User Interfaces (IUI) Workshops, 2019.
[81] A. Pronobis, A. Ranganath, and R. P. N. Rao, “LibSPN: A
library for learning and inference with sum-product networks
and TensorFlow,” in 34th International Conference on Machine
Learning (ICLM) - Principled Approaches to Deep Learning
Workshop, 2017.
[82] J. van de Wolfshaar and A. Pronobis, “Deep generalized convo-
lutional sum-product networks for probabilistic image represen-
tations,” arXiv:1902.06155, 2019.
[83] A. Molina, A. Vergari, K. Stelzner, R. Peharz, P. Subramani,
N. Di Mauro, P. Poupart, and K. Kersting, “SPFlow: An easy
and extensible library for deep probabilistic learning using sum-
product networks,” arXiv:1901.03704, 2019.
[84] D. Lowd and A. Rooshenas, “The Libra toolkit for probabilis-
tic models,” Journal of Machine Learning Research, vol. 16,
pp. 2459–2463, 2015.
[85] M. Melibari, P. Poupart, and P. Doshi, “Sum-product-max
networks for tractable decision making,” in Proceedings of the
25th International Joint Conference on Artiﬁcial Intelligence
(IJCAI), pp. 1846–1852, 2016.
[86] A.
Dennis
and
D.
Ventura,
“Autoencoder-enhanced
sum-
product networks,” in Proceedings of the 16th IEEE Inter-
national Conference on Machine Learning and Applications
(ICMLA), pp. 1041–1044, 2017.
[87] P.
L.
Tan
and
R.
Peharz,
“Hierarchical
decompositional
mixtures of variational autoencoders,” in Proceedings of the
36th International Conference on Machine Learning (ICML),
pp. 6115–6124, 2019.
[88] D. D. Mauá, F. G. Cozman, D. Conaty, and C. P. Campos,
“Credal sum-product networks,” in Proceedings of the 10th
International Symposium on Imprecise Probability: Theories
and Applications (ISIPTA), pp. 205–216, 2017.
[89] M. Desana and C. Schnörr, “Sum-product graphical models,”
Machine Learning, vol. 109, pp. 135—-173, 2020.
[90] D. Lowd and P. Domingos, “Learning arithmetic circuits,” in
Proceedings of the 24th Conference on Uncertainty in Artiﬁcial
Intelligence (UAI), pp. 383–392, 2008.
[91] O. Sharir and A. Shashua, “Sum-product-quotient networks,”
in Proceedings of the 21st International Conference on Artiﬁcial
Intelligence and Statistics (AISTATS), pp. 529–537, 2018.
[92] C. Y. Ko, C. Chen, Y. Zhang, K. Batselier, and N. Wong, “Deep
compression of sum-product networks on tensor networks,”
arXiv:1811.03963, 2018.
[93] C. J. Butz, J. S. Oliveira, A. E. dos Santos, and A. L. Teixeira,
“Deep convolutional sum-product networks,” in 33rd AAAI
Conference on Artiﬁcial Intelligence, 2019.
[94] A. L. Friesen and P. Domingos, “Unifying sum-product networks
and submodular ﬁelds,” in Proceedings of the 34th Interna-
tional Conference on Machine Learning (ICML). Principled
Approaches to Deep Learning Workshop, 2017.
[95] R. Gens and P. Domingos, “Compositional kernel machines,”
in 5th International Conference on Learning Representations
(ICLR) - Workshop Track, 2017.
[96] X. Shao, A. Molina, A. Vergari, K. Stelzner, R. Peharz,
T. Liebig, and K. Kersting, “Conditional sum-product net-
works: Imposing structure on deep probabilistic architectures,”
arXiv:1905.08550, 2019.
[97] A. Choi and A. Darwiche, “On relaxing determinism in arith-
metic circuits,” in 34th International Conference on Machine
Learning (ICLM), 2017.

