When Deep Learners Change Their Mind:
Learning Dynamics for Active Learning⋆
Javad Zolfaghari Bengar1,2[0000−0002−2502−8419],
Bogdan Raducanu1,2[0000−0003−2207−6260], and
Joost van de Weijer1,2[0000−0001−9843−3143]
1 Computer Vision Center (CVC)
2 Univ. Aut`onoma of Barcelona (UAB)
{jzolfaghari,bogdan,joost}@cvc.uab.es
Abstract. Active learning aims to select samples to be annotated that
yield the largest performance improvement for the learning algorithm.
Many methods approach this problem by measuring the informativeness
of samples and do this based on the certainty of the network predic-
tions for samples. However, it is well-known that neural networks are
overly conﬁdent about their prediction and are therefore an untrustwor-
thy source to assess sample informativeness. In this paper, we propose a
new informativeness-based active learning method. Our measure is de-
rived from the learning dynamics of a neural network. More precisely we
track the label assignment of the unlabeled data pool during the train-
ing of the algorithm. We capture the learning dynamics with a metric
called label-dispersion, which is low when the network consistently as-
signs the same label to the sample during the training of the network
and high when the assigned label changes frequently. We show that label-
dispersion is a promising predictor of the uncertainty of the network, and
show on two benchmark datasets that an active learning algorithm based
on label-dispersion obtains excellent results.
Keywords: Active learning · Deep Learning · Image Classiﬁcation.
1
Introduction
Deep learning methods obtain excellent results for many tasks where large anno-
tated dataset are available [14]. However, collecting annotations is both time and
labor expensive. Active Learning(AL) methods [22] aim to tackle this problem
by reducing the required annotation eﬀort. The key idea behind active learning
is that a machine learning model can achieve a satisfactory performance with a
subset of the training samples if it is allowed to choose which samples to label.
In AL, the model is trained on a small initial set of labeled data called initial
label pool. An acquisition function selects the samples to be annotated by an
external oracle. The newly labeled samples are added to the labeled pool and
⋆We acknowledge the support of the Spanish Ministry of Science and Innovation for
funding projects PID2019-104174GB-I00.
arXiv:2107.14707v1  [cs.LG]  30 Jul 2021

2
Javad Zolfaghari Bengar et al.
the model is retrained on the updated training set. This process is repeated until
the labeling budget is exhausted.
One of the main groups of approaches for active learning use the network
uncertainty, as contained in its prediction, to select data for labelling [5,25,22].
However, it is known that neural networks are overly conﬁdent about their pre-
dictions; making wrong predictions with high certainty [19]. In this paper, we
present a new approach to active learning. Our method is based on recent work of
Toneva et al. [24], who study the learning dynamics during the training process
of a neural network. They track for each training sample the transitions from
being classiﬁed correctly to incorrectly (or vice-versa) over the course of learning.
Based on these learning dynamics, they characterize a sample of being ’forget-
table’ (if its class label changes from subsequent presentation) or ’unforgettable’
(if the class label assigned is consistent during subsequent presentations). Their
method is only applicable for labeled data (and therefore not applicable to ac-
tive learning) and was applied to show that redundant (forgettable) training
data could be removed without hurting network performance.
Inspired by this work, we propose a new uncertainty-based active learning
method which is based on the learning dynamics of a neural network. With
learning dynamics, we refer to the variations in the predictions of the neural
network during training. Speciﬁcally, we keep track of the model predictions on
every unlabeled sample during the various epochs of training. Based on the vari-
ations of the predicted label of samples, we propose a new active learning metric
called label-dispersion. This way, we can indirectly estimate the uncertainty of
the model based on the unlabeled samples. We will directly use this metric as
the acquisition function to select the samples to be labeled in the active learning
cycles. Other than the forgetfulness measure proposed in [24], we do not require
any label information.
Experimental results show that label-dispersion better resemble the true un-
certainty of the neural networks, i.e. samples with low dispersion were found to
have a correct label prediction, whereas those with high dispersion often had a
wrong prediction. Furthermore, in experiments on two standard datasets (CIFAR
10 and CIFAR 100) we show that our method outperforms the state-of-the-art
methods in active learning.
2
Related work
The most important aspect for an active learner is the strategy used to query the
next sample to be annotated. These strategies have been successfully applied to
a series of traditional computer vision tasks, such as image classiﬁcation [11,9],
object detection [1,2], image retrieval [31], remote sensing [6], and regression
[13].
Pool based methods are grouped into three main query strategies relying
mostly on heuristics: informativeness [29,10,4], representativeness [21], and hy-
brid [12,28], a comprehensive survey of these frameworks and a detailed discus-
sion can be found in [22].

Learning Dynamics for Active Learning
3
Predicted class last epochs:
dog, bird, airplane, bird, bird
Prediction confidence: 
Label-dispersion:
Predicted class last epochs:
car, car, car, car, car
Prediction confidence:
Label-dispersion: 
Predicted class last epochs:
cat, cat, frog, bird, bird, frog
Prediction confidence:
Label-dispersion:
Predicted class last epochs:
bird, deer, deer, bird, cat
Prediction confidence:
Label-dispersion:
0.97
0.72
0.71
0.91
0.01
0.99
0.71
0.99
(a)
(c)
(b)
(d)
Fig. 1. Comparison between the dispersion and conﬁdence scores. We show
four examples images together with the predicted label for the last ﬁve epochs of
training. The last predicted label is the network prediction when training is ﬁnished.
We also report the prediction conﬁdence and our label-dispersion measure. (a) Shows
an example which is consistently and correctly classiﬁed as car. The conﬁdence of model
is 0.99 and the consistent predictions every epoch result in low dispersion score of 0.01.
(b-d) present examples on which the model is highly conﬁdent despite a wrong ﬁnal
prediction and constant changes of predictions across the last epochs. This network
uncertainty is much better reﬂected by the high label-dispersion scores.
Informativeness-based methods: Among all the aforementioned strategies,
the informativeness-based approaches are the most successful ones, with un-
certainty being the most used selection criteria in both bayesian [10] and non-
bayesian frameworks [29]. In [30,15], the authors employed a loss module to learn
the loss of a target model and select the images based on their output loss. More
recently, query-synthesizing approaches have used generative models to generate
informative samples [17,18,32].
Representativeness-based methods: In [23] the authors rely on selecting
few examples by increasing diversity in a given batch. The Core-set technique
was shown to be an eﬀective representation learning method for large scale im-
age classiﬁcation tasks [21] and was theoretically proven to work best when the
number of classes is small. However, as the number of classes grows, its perfor-
mance deteriorates. Moreover, for high-dimensional data, using distance-based
representation methods, like Core-set, is ineﬀective because in high-dimensions
p-norms suﬀer from the curse of dimensionality which is referred to as the dis-
tance concentration phenomenon in the computational learning literature [7].
Hybrid methods: Methods that aim to combine uncertainty and representa-
tiveness use a two-step process to select the points with high uncertainty as
of the most representative points in a batch [16]. A weakly supervised learning
strategy was introduced in [25] that trains the model with pseudo labels obtained
for instances with high conﬁdence in predictions. While most of the hybrid ap-
proaches are based on a two-step process, in [26] they propose a method to select
the samples in a single step, based on a generative adversarial framework. An
image selector acts as an acquisition function to ﬁnd a subset of representative
samples which also have high uncertainty.

4
Javad Zolfaghari Bengar et al.
3
Active learning for image classiﬁcation
We describe here the general process of active learning for the image classiﬁcation
task. Given a large pool of unlabeled data U and an annotation budget B, the
goal of active learning is to select a subset of B samples to be annotated as
to maximize the performance of an image classiﬁcation model. Active learning
methods generally proceed sequentially by splitting the budget in several cycles.
Here we consider the batch-mode variant [21], which annotates multiple samples
per cycle, since this is the only feasible option for CNN training. At the beginning
of each cycle, the model is trained on the initial labeled set of samples. After
training, the model is used to select a new set of samples to be annotated at the
end of the cycle via an acquisition function. The selected samples are added to
the labeled set DL for the next cycle and the process is repeated until the total
annotation budget is spent.
3.1
Label-dispersion acquisition function
In this section, we present a new acquisition function for active learning. The ac-
quisition function is the most crucial component and the main diﬀerence between
active learning methods in the literature. In general, an acquisition function re-
ceives a sample and outputs a score indicating how valuable the sample is for
training the current model. Most of informativeness-based active learning ap-
proaches consider to assess the certainty of the network on the unlabeled data
pool which is obtained after training on the labeled data [5,25,22].
In contrast, we propose to track the labels of the unlabeled samples during
the course of training. We hypothesize that if the network frequently changes
the assigned label, it is unsure about the sample, therefore the sample is an
appropriate candidate to be labeled. In ﬁgure 1 we depict the main idea behind
our method and compare it to network conﬁdence. While the conﬁdence score is
used to assign the label based on the certainty of the last epoch, the dispersion
uses the prediction over all epochs in order to assess the certainty. The ﬁrst
example shows the case of a correct label prediction when both conﬁdence score
and dispersion agree. However, in the other three examples, we depict situa-
tions where the system predicts the wrong label with high certainty. However,
a large dispersion value (i.e. high uncertainty) is the indication of an erroneous
prediction.
This idea is based on the concept of forgettable samples recently introduced
by [24]. [24] states that there exist a large number of unforgettable samples that
are never forgotten once learnt. It is shown that they can be omitted from the
training set while the generalization performance is maintained. Therefore it
suﬃces to learn the forgettable samples in the train set. However to identify for-
gettable samples the ground-truth labels is needed. Since we do not have access
to the labels in active learning, we propose to use a measure called the label-
dispersion. The dispersion of a nominal variable is calculated as the proportion
of predicted class labels that are not the modal class prediction [8]. It estimates

Learning Dynamics for Active Learning
5
Forward pass on unlabeled 
set and
save predicted labels
Repeat until 
convergence
Sort unlabeled set 
based on 
dispersion. Select 
and label b samples 
with highest 
dispersion.
Add selection to 
labeled set. 
Remove selection 
from unlabeled set.
Evaluate on 
test set
Train the model on 
labeled set 
(n iterations)
Input: k labeled 
samples as initial 
labeled set
Active learning cycles
Fig. 2. Active learning framework using Dispersion. Active learning cycles start
with initial labeled pool. The model trained on labeled pool is used to output the
predictions and compute dispersion for each sample. The samples with highest dis-
persion are queried for labeling and added to labeled set. This cycle repeats until the
annotation budget is exhausted.
the uncertainty of the model by measuring the changes in the predicted class as
following:
Dispersion(x) := 1 −fx
T ,
(1)
with
fx =
X
t
1[yt = c∗],
c∗= arg max
c=1,...,C
X
t
1[yt = c],
(2)
where fx is the number of predictions falling into the modal class for sample
x and C is the number of classes. Larger values for dispersion means more
uncertainty in model outputs. Similar to forgettable samples, we are interested
in samples for which the model doesn’t persistently output the same class.
Fig. 2 presents the active learning framework with our acquisition function.
During the training of a network at regular intervals we will save the label pre-
dictions for all samples in the unlabeled pool (green block in Fig. 2). In practice,
we will perform this operation at every epoch. These saved label predictions
allow us to compute the label-dispersion with Eq. 1. We then select the samples
with highest dispersion to be annotated and continue to the next active learning
cycle until the total label budget is used.
3.2
Informativeness Analysis
To assess the informativeness of methods, we compute the scores assigned to
the unlabeled samples and sort the samples accordingly. Then we select several
portions of the most informative samples (according to their score) and run the
model to infer their labels. We argue that annotating the correctly classiﬁed
samples would not provide much information for the model because the model
already knows their label. In contrast, the model can learn from misclassiﬁed
samples if labeled. We use the accuracy to implicitly measure the informativeness

6
Javad Zolfaghari Bengar et al.
0
10
20
30
40
50
60
70
80
90
100
% of unlabeled data
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
Accuracy
Dispersion
BALD-Dropout
MarginSampling
Random
Oracle
0
10
20
30
40
50
60
70
80
90
100
% of unlabeled data
0.00
0.05
0.10
0.15
0.20
0.25
0.30
0.35
Accuracy
Dispersion
BALD-Dropout
MarginSampling
Random
Oracle
(a)
(b)
Fig. 3. Informativeness analysis of AL methods on CIFAR10(a) and CI-
FAR100(b) datasets. The model is used to infer the label of samples selected by AL
methods before labeling and the accuracy is measured. For any amount of unlabeled
samples, dispersion oﬀers samples with lower accuracy and hence more informative for
the model.
of unlabeled samples. The lower the accuracy, the more informative the samples
will be if labeled. Fig. 3 shows the accuracy of model on the unlabeled samples
queried by each method. The model used in this analysis is trained on the initial
labeled set. The accuracy of samples selected randomly remains almost constant
regardless of the amount of unlabeled samples. In this analysis, the oracle method
by deﬁnition uses groundtruth and queries samples that the model misclassiﬁed
and therefore the accuracy of the model is zero. Among the active learning
methods, on both CIFAR10 and CIFAR100 datasets, and for any amount of
unlabeled samples, dispersion queries misclassiﬁed samples the most, showing
that high dispersion correlates well with network uncertainty. These samples
can potentially increase the performance of the model if labeled.
4
Experimental Results
4.1
Experimental Setup
We start with model trained on initial labeled set from scratch and employ
Resnet-18 as the model architecture. The initial labeled set consists of 10%
of train dataset that is selected randomly once for all the methods. At each
cycle, we use the model with the corresponding acquisition function to select
b samples, which are then labeled and added to DL. We continue for 4 cycles
until the total budget is completely exhausted. In all experiments, the budget
per cycle is 5% and total budget is 30% of the entire dataset. Eventually for each
cycle, we evaluate the model on the test set. To evaluate our method, we use
CIFAR10 and CIFAR100 [14] datasets with 50K images for training and 10K for
test. CIFAR10 and CIFAR100 have 10 and 100 object categories respectively and
image size of 32×32. During training, we apply a standard augmentation scheme
including random crop from zero-padded images, random horizontal ﬂip, and

Learning Dynamics for Active Learning
7
image normalization using the channel mean and standard deviation estimated
over the training set.
Dispersion is computed from the most probable class in the output of the
model. During training we do an inference on the unlabeled pool at every epoch
and save the model predictions. Based on these predictions we compute the
label-dispersion for each sample speciﬁcally.
Implementation details. Our method is implemented in PyTorch3 [20]. We
trained all models with the momentum optimizer with value 0.9 and the ini-
tial learning rates 0.02. We train for 100 epochs and reduce the learning rate
by a factor of 5 once after 60 epochs and again at 80 epochs. Finally, to ob-
tain more stable results we repeat the experiments 3 times and report the mean
and standard deviation in our results. Baselines. We compare our method with
several informative and representative-based approaches. Random sampling: se-
lects an arbitrary subset of samples from all unlabeled samples. BALD [10]:
method chooses samples that are expected to maximise the information gained
about the model parameters. In particular, it select samples that maximise the
mutual information between predictions and model posterior via dropout tech-
nique. Margin sampling [3]: uses the diﬀerence between the two classes with
the highest scores as a measure of proximity to the decision boundary. KCen-
terGreedy [21]: is a greedy approximation of KCenter problem also known as
min-max facility location problem [27]. Samples having maximum distance from
their nearest labeled samples in the embedding space are queried for labeling.
CoreSet [21]: ﬁnds samples as the 2-Opt greedy solution of Kcenter problem
in the form of Mixed Integer Programming (MIP) problem. VAAL [23]: learns
a latent space using a Variational Autoencoder (VAE) and an adversarial net-
work trained to discriminate between unlabeled and labeled data. The unlabeled
samples which the discriminator classiﬁes with lowest certainty as belonging to
the labeled pool are considered to be the most representative and queried for
labeling. Oracle method: An acquisition function using ground-truth that selects
samples that the model miss-classiﬁed. In order to study the potential of active
learning, we evaluate oracle-based acquisition function. Note this is not a useful
active learning function in practice, as we would not have access to the ground-
truth annotations in a real scenario. In order to make a fair comparison with the
baselines, we used their oﬃcial code and adapted them into our code to ensure
an identical setting.
4.2
Results
Results on CIFAR10: A comparison with several active learning methods,
including both informativeness and representativeness, is provided in Fig. 4. As
can be seen in Fig. 4(a) dispersion outperforms the other methods across all
the cycles on CIFAR10, only the BALD-Dropout method obtains similar re-
sults at 30%. The active learning gain of dispersion against Random sampling is
3 Upon acceptance, we will release the code for our method.

8
Javad Zolfaghari Bengar et al.
10
15
20
25
30
% of labeled images
0.76
0.78
0.8
0.82
0.84
0.86
0.88
0.9
Accuracy
Dispersion
Margin Sampling
BALD-Dropout
KCenterGreedy
CoreSet
VAAL
Random
10
15
20
25
30
% of labeled images
0.3
0.35
0.4
0.45
0.5
0.55
0.6
Accuracy
Dispersion
Margin Sampling
BALD-Dropout
KCenterGreedy
CoreSet
VAAL
Random
(a)
(b)
Fig. 4. Performance Evaluation. Results for several active learning methods on
CIFAR10 (a) and CIFAR100 (b) datasets. All curves are average of 3 runs.
around 7.5% at cycle 4, equivalent to annotating 4000 samples less. The informa-
tive methods such as Margin Sampling and BALD lie above the representative
methods including KCenterGreedy, CoreSet, VAAL and Random highlighting
the importance of informativeness on CIFAR10 where the number of classes is
limited and each class is well-represented by many samples.
Results on CIFAR100: Fig. 4(b) shows the performance of active learning
methods on CIFAR100. As can be seen, the methods are closer and the over-
all performance of Dispersion, Margin sampling and CoreSet are comparable.
However, the addition of labeled samples at cycle 3 and 4 makes the disper-
sion superior in performance to others. The smaller gap between the informative
based methods and Random emphasizes the importance of representativeness
on CIFAR100 dataset which has more diverse classes that are underrepresented
with few samples in small budget size.
Additionally, Table 1 illustrates the full performance of models that are
trained on the entire datasets. Dispersion manages to attain almost 97% and
82% of full performance on CIFAR10 and CIFAR100 respectively by using only
30% of the data, which is a signiﬁcant reduction in the labeling eﬀort.
5
Conclusion
We proposed an informativeness-based active learning algorithm based on the
learning dynamics of neural networks. We introduced the label-dispersion met-
ric, which measures label-consistency during the training process. We showed
that this measure obtains excellent results when used for active learning on a
variety of benchmark datasets. For future work, we are interested in exploring

Learning Dynamics for Active Learning
9
Table 1. Active learning results. Performance of AL methods using 30% of dataset
both in absolute performance and relative to using all data.
Methods
CIFAR 10
CIFAR 100
Acc.
Rel.
Acc.
Rel.
All data
93.61
100%
74.61
100%
Dispersion
90.74
96.93%
60.66
81.97%
Margin sampling [3]
90.44
96.61%
59.78
80.78%
BALD [10]
90.66
96.85%
59.54
80.46%
KCenterGreedy [21]
89.57
95.69%
59.64
80.59%
CoreSet [21]
89.45
95.56%
59.87
80.91%
VAAL [23]
87.88
93.88%
58.42
78.95%
Random sampling
87.65
93.63%
58.47
79.02%
label-dispersion for other research ﬁelds such as out-of-distribution detection and
within the context of lifelong learning.
References
1. Aghdam, H.H., Gonzalez-Garcia, A., Van de Weijer, J., L´opez, A.M.: Active learn-
ing for deep detection neural networks. In: ICCV. pp. 3672–3680 (2019)
2. Bengar, J.Z., Gonzalez-Garcia, A., Villalonga, G., Raducanu, B., Aghdam, H.H.,
Mozerov, M., Lopez, A.M., Van de Weijer, J.: Temporal coherence for active learn-
ing in videos. In: ICCV-W. pp. 914–923 (2019)
3. Brust, C.A., K¨ading, C., Denzler, J.: Active learning for deep object detection. In:
VISAPP (2019)
4. Cai, W., Zhang, Y., Zhou, S., Wang, W., Ding, C., Gu, X.: Active learning for
support vector machines with maximum model change. In: Machine Learning and
Knowledge Discovery in Databases. pp. 211–226. Springer (2014)
5. Chitta, K., Alvarez, J.M., Lesnikowski, A.: Large-scale visual active learning with
deep probabilistic ensembles. arXiv preprint arXiv:1811.03575v3 (2019)
6. Deng, C., Liu, X., Li, C., Tao, D.: Active multi-kernel domain adaptation for
hyperspectral image classiﬁcation. Pattern Recognition 77, 306–315 (2018)
7. Donoho, D.L., et al.: High-dimensional data analysis: The curses and blessings of
dimensionality. AMS math challenges lecture 1(2000), 32 (2000)
8. Freeman, L.: Elementary applied statistics: for students in behavioral science. Wi-
ley (1965), https://books.google.es/books?id=r4VRAAAAMAAJ
9. Fu, W., Wang, M., Hao, S., Wu, X.: Scalable active learning by approximated error
reduction. In: KDD. pp. 1396–1405 (2018)
10. Gal, Y., Islam, R., Ghahramani, Z.: Deep bayesian active learning with image data.
In: ICML. pp. 1183–1192 (2017)
11. Gavves, E., Mensink, T.E.J., Tommasi, T., Snoek, C. G. M.and Tuytelaars, T.:
Active transfer learning with zero-shot priors: Reusing past datasets for future
tasks. In: ICCV. pp. 1–9 (2015)
12. Huang, S.J., Jin, R., Zhou, Z.H.: Active learning by querying informative and
representative examples. IEEE Trans. on PAMI 10(36), 1936–1949 (2014)

10
Javad Zolfaghari Bengar et al.
13. K¨ading, C., Rodner, E., Freytag, A., Mothes, O., Barz, B., Denzler, J.: Active
learning for regression tasks with expected model output changes. In: BMVC. pp.
1–15 (2018)
14. Krizhevsky, A.: Learning multiple layers of features from tiny images. Ph.D. thesis,
University of Toronto (2012)
15. Li, M., Liu, X., van de Weijer, J., Raducanu, B.: Learning to rank for active
learning: A listwise approach. In: ICPR. pp. 5587–5594 (2020)
16. Li, X., Guo, Y.: Adaptive active learning for image classiﬁcation. In: CVPR. pp.
859–866 (2013)
17. Mahapatra, D., Bozorgtabar, B., Thiran, J.P., Reyes, M.: Eﬃcient active learning
for image classiﬁcation and segmentation using a sample selection and conditional
generative adversarial network. In: International Conference on Medical Image
Computing and Computer-Assisted Intervention. pp. 580–588. Springer (2018)
18. Mayer, C., Timofte, R.: Adversarial sampling for active learning. In: WACV. pp.
3071–3079 (2020)
19. Ovadia, Y., Fertig, E., Ren, J., Nado, Z., Sculley, D., Nowozin, S., Dillon, J.V., Lak-
shminarayanan, B., Snoek, J.: Can you trust your model’s uncertainty? evaluating
predictive uncertainty under dataset shift. In: NeurIPS (2019)
20. Paszke, A., Gross, S., Chintala, S., Chanan, G., Yang, E., DeVito, Z., Lin, Z.,
Desmaison, A., Antiga, L., Lerer, A.: Automatic diﬀerentiation in pytorch. In:
NIPS-W (2017)
21. Sener, O., Savarese, S.: Active learning for convolutional neural networks: A core-
set approach. In: International Conference on Learning Representations (2018),
https://openreview.net/forum?id=H1aIuk-RW
22. Settles, B.: Active learning. Morgan Claypool (2012)
23. Sinha, S., Ebrahimi, S., Darrell, T.: Variational adversarial active learning. In:
ICCV (2019)
24. Toneva, M., Sordoni, A., des Combes, R.T., Trischler, A., Bengio, Y., Gordon, G.J.:
An empirical study of example forgetting during deep neural network learning. In:
ICLR (2019)
25. Wang, K., Zhang, D., Li, Y., Zhang, R., Lin, L.: Cost-eﬀective active learning for
deep image classiﬁcation. IEEE Transactions on Circuits and Systems for Video
Technology 27(12), 2591–2600 (2016)
26. Wang, S., Li, Y., Ma, K., Ma, R., Guan, H., Zheng, Y.: Dual adversarial network
for deep active learning. In: ECCV. pp. 1–17 (2020)
27. Wolf, G.W.: Facility location: concepts, models, algorithms and case studies. In-
ternational Journal of Geographical Information Science 25(2), 331–333 (2011)
28. Yang, Y., Loog, M.: A variance maximization criterion for active learning. Pattern
Recognition 78, 358–370 (2018)
29. Yang, Y., Ma, Z., Nie, F., Chang, X., Hauptmann, A.G.: Multi-class active learn-
ing by uncertainty sampling with diversity maximization. IJCV 113(2), 113–127
(2015)
30. Yoo, D., Kweon, I.S.: Learning loss for active learning. In: CVPR. pp. 93–102
(2019)
31. Zhang, D., Wang, F., Shi, Z., Zhang, C.: Interactive localized content based image
retrieval with multiple-instance active learning. Pattern Recognition 43(2), 478–
484 (2010)
32. Zhu, J.J., Bento, J.: Generative adversarial active learning. arXiv preprint
arXiv:1702.07956 (2017)

