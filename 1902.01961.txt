RESEARCH ARTICLE
Faster Remainder by Direct Computation
Applications to Compilers and Software Libraries
Daniel Lemire*1 | Owen Kaser2 | Nathan Kurz3
1TELUQ, Université du Québec, Quebec,
Canada
2Computer Science Department, UNB Saint
John, New Brunswick, Canada
3Vermont, USA
Correspondence
*Daniel Lemire, 5800 Saint-Denis, Oﬃce
1105, Montreal, Quebec, H2S 3L5 Canada.
Email: lemire@gmail.com
Summary
On common processors, integer multiplication is many times faster than integer
division. Dividing a numerator 푛by a divisor 푑is mathematically equivalent to mul-
tiplication by the inverse of the divisor (푛∕푑= 푛∗1∕푑). If the divisor is known in
advance—or if repeated integer divisions will be performed with the same divisor—it
can be beneﬁcial to substitute a less costly multiplication for an expensive division.
Currently, the remainder of the division by a constant is computed from the quo-
tient by a multiplication and a subtraction. But if just the remainder is desired and
the quotient is unneeded, this may be suboptimal. We present a generally applicable
algorithm to compute the remainder more directly. Speciﬁcally, we use the fractional
portion of the product of the numerator and the inverse of the divisor. On this basis,
we also present a new, simpler divisibility algorithm to detect nonzero remainders.
We also derive new tight bounds on the precision required when representing the
inverse of the divisor. Furthermore, we present simple C implementations that beat
the optimized code produced by state-of-art C compilers on recent x64 proces-
sors (e.g., Intel Skylake and AMD Ryzen), sometimes by more than 25%. On all
tested platforms including 64-bit ARM and POWER8, our divisibility-test functions
are faster than state-of-the-art Granlund-Montgomery divisibility-test functions,
sometimes by more than 50%.
KEYWORDS:
Integer Division, Bit Manipulation, Divisibility
1
INTRODUCTION
Integer division often refers to two closely related concepts, the actual division and the modulus. Given an integer numerator 푛
and a non-zero integer divisor 푑, the integer division, written div, gives the integer quotient (푛div 푑= 푞). The modulus, written
mod, gives the integer remainder (푛mod 푑= 푟). Given an integer numerator 푛and an integer divisor 푑, the quotient (푞) and the
remainder (푟) are always integers even when the fraction 푛∕푑is not an integer. It always holds that the quotient multiplied by
the divisor plus the remainder gives back the numerator: 푛= 푞∗푑+ 푟.
Depending on the context, ‘integer division’ might refer solely to the computation of the quotient, but might also refer to the
computation of both the integer quotient and the remainder. The integer division instructions on x64 processors compute both
arXiv:1902.01961v3  [cs.MS]  20 Nov 2019

2
D. LEMIRE, O. KASER, N. KURZ
the quotient and the remainder.a In most programming languages, they are distinct operations: the C programming language
uses / for division (div) and % for modulo (mod).
Let us work through a simple example to illustrate how we can replace an integer division by a multiplication. Assume we
have a pile of 23 items, and we want to know how many piles of 4 items we can divide it into and how many will be left over
(푛= 23, 푑= 4; ﬁnd 푞and 푟). Working in base 10, we can calculate the quotient 23 div 4 = 5 and the remainder 23 mod 4 = 3,
which means that there will be 5 complete piles of 4 items with 3 items left over (23 = 5 ∗4 + 3).
If for some reason, we do not have a runtime integer division operator (or if it is too expensive for our purpose), we can
instead precompute the multiplicative inverse of 4 once (푐= 1∕푑= 1∕4 = 0.25) and then calculate the same result using
a multiplication (23 ∗0.25 = 5.75). The quotient is the integer portion of the product to the left of the decimal point (푞=
⌊5.75⌋= 5), and the remainder can be obtained by multiplying the fractional portion 푓= 0.75 of the product by the divisor 푑:
푟= 푓∗푑= 0.75 ∗4 = 3.
The binary registers in our computers do not have a built-in concept of a fractional portion, but we can adopt a ﬁxed-point
convention. Assume we have chosen a convention where 1∕푑has 5 bits of whole integer value and 3 bits of ‘fraction’. The
numerator 23 and divisor 4 would still be represented as standard 8-bit binary values (00010111 and 00000100, respectively),
but 1∕푑would be 00000.010. From the processor’s viewpoint, the rules for arithmetic are still the same as if we did not have
a binary point—it is only our interpretation of the units that has changed. Thus we can use the standard (fast) instructions for
multiplication (00010111 ∗00000010 = 00101110) and then mentally put the ‘binary point’ in the correct position, which in
this case is 3 from the right: 00101.110. The quotient 푞is the integer portion (leftmost 5 bits) of this result: 00101 in binary
(푞= 5 in decimal). In eﬀect, we can compute the quotient 푞with a multiplication (to get 00101.110) followed by a right shift
(by three bits, to get 000101). To ﬁnd the remainder, we can multiply the fractional portion (rightmost 3 bits) of the result by
the divisor: 00000.110 ∗00000100 = 00011.000 (푟= 3 in decimal). To quickly check whether a number is divisible by 4 (푛
mod 4 = 0?) without computing the remainder it suﬃces to check whether the fractional portion of the product is zero.
But what if instead of dividing by 4, we wanted to divide by 6? While the reciprocal of 4 can be represented exactly with
two digits of ﬁxed-point fraction in both binary and decimal, 1∕6 cannot be exactly represented in either. As a decimal fraction,
1∕6 is equal to the repeating fraction 0.1666...(with a repeating 6), and in binary it is 0.0010101...(with a repeating 01). Can
the same technique work if we have a suﬃciently close approximation to the reciprocal for any divisor, using enough fractional
bits? Yes!
For example, consider a convention where the approximate reciprocal 푐has 8 bits, all of which are fractional. We can use the
value 0.00101011 as our approximate reciprocal of 6. To divide 23 by 6, we can multiply the numerator (10111 in binary) by the
approximate reciprocal: 푛∗푐= 00010111 ∗0.00101011 = 11.11011101. As before, the decimal point is merely a convention,
the computer need only multiply ﬁxed-bit integers. From the product, the quotient of the division is 11 in binary (푞= 3 in
decimal); and indeed 23 div 6 = 3. To get the remainder, we multiply the fractional portion of the product by the divisor
(푓∗푑= 0.11011101 ∗00000110 = 101.00101110), and then right shift by 8 bits, to get 101 in binary (푟= 5 in decimal). See
Table 1 for other examples.
While the use of the approximate reciprocal 푐prevents us from conﬁrming divisibility by 6 by checking whether the fractional
portion is exactly zero, we can still quickly determine whether a number is divisible by 6 (푛mod 6 = 0?) by checking whether
the fractional portion is less than the approximate reciprocal (푓< 푐?). Indeed, if 푛= 푞∗푑+ 푟then the fractional portion of the
product of 푛with the approximate reciprocal should be close to 푟∕푑: it makes intuitive sense that comparing 푟∕푑with 푐≈1∕푑
determines whether the remainder 푟is zero. For example, consider 푛= 42 (101010 in binary). We have that our numerator times
the approximate reciprocal of 6 is 101010 ∗0.00101011 = 111.00001110. We see that the quotient is 111 in binary (푞= 7 in
decimal), while the fractional portion 푓is smaller than the approximate reciprocal 푐(0.00001110 < 0.00101011), indicating
that 42 is a multiple of 6.
In our example with 6 as the divisor, we used 8 fractional bits. The more fractional bits we use, the larger the numerator we
can handle. An insuﬃciency of fractional bits can lead to incorrect results when 푛grows. For instance, with 푛= 131 (10000011
in binary) and only 8 fractional bits, 푛times the approximate reciprocal of 6 is 10000011 ∗0.00101011 = 10110.00000001, or
22 in decimal. Yet in actuality, 131 div 6 = 21; using an 8 bit approximation of the reciprocal was inadequate.
How close does the approximation need to be?—that is, what is the minimum number of fractional bits needed for the
approximate reciprocal 푐such that the remainder is exactly correct for all numerators? We derive the answer in § 3.
aWe use x64 to refer to the commodity Intel and AMD processors supporting the 64-bit version of the x86 instruction set. It is also known as x86-64, x86_64, AMD64
and Intel 64.

D. LEMIRE, O. KASER, N. KURZ
3
TABLE 1 Division by 6 (푑= 110 in binary) using a multiplication by the approximate reciprocal (푐= 0.00101011 in binary).
The numerator 푛is an 푁-bit value, with 푁= 6. The approximate reciprocal uses 퐹= 8 fractional bits. The integer portion of
the product (푁bits) gives the quotient. Multiplying the fractional portion of the product (퐹bits) by the divisor (푁bits) and
keeping only the integer portion (푁bits), we get the remainder (two last columns). The integer portion in bold (column 2) is
equal to the quotient (column 3) in binary. The integer portion in bold (column 4) is equal to the remainder (column 5) in binary.
푛
numerator times the approx. reciprocal (푛∗푐)
quotient
fractional portion ∗divisor (푓∗푑)
remainder
푁bits ∗퐹bits →푁+ 퐹bits
푁bits
퐹bits ∗푁bits →푁+ 퐹bits
푁bits
0
000000 ∗0.00101011 = 000000.00000000
0
0.00000000 ∗000110 = 000000.00000000
0
1
000001 ∗0.00101011 = 000000.00101011
0
0.00101011 ∗000110 = 000001.00000010
1
2
000010 ∗0.00101011 = 000000.01010110
0
0.01010110 ∗000110 = 000010.00000100
2
3
000011 ∗0.00101011 = 000000.10000001
0
0.10000001 ∗000110 = 000011.00000110
3
4
000100 ∗0.00101011 = 000000.10101100
0
0.10101100 ∗000110 = 000100.00001000
4
5
000101 ∗0.00101011 = 000000.11010111
0
0.11010111 ∗000110 = 000101.00001010
5
6
000110 ∗0.00101011 = 000001.00000010
1
0.00000010 ∗000110 = 000000.00001100
0
⋮
⋮
⋮
⋮
⋮
17
010001 ∗0.00101011 = 000010.11011011
2
0.11011011 ∗000110 = 000101.00100010
5
18
010010 ∗0.00101011 = 000011.00000110
3
0.00000110 ∗000110 = 000000.00100100
0
19
010011 ∗0.00101011 = 000011.00110001
3
0.00110001 ∗000110 = 000001.00100110
1
20
010100 ∗0.00101011 = 000011.01011100
3
0.01011100 ∗000110 = 000010.00101000
2
21
010101 ∗0.00101011 = 000011.10000111
3
0.10000111 ∗000110 = 000011.00101010
3
22
010110 ∗0.00101011 = 000011.10110010
3
0.10110010 ∗000110 = 000100.00101100
4
23
010111 ∗0.00101011 = 000011.11011101
3
0.11011101 ∗000110 = 000101.00101110
5
24
011000 ∗0.00101011 = 000100.00001000
4
0.00001000 ∗000110 = 000000.00110000
0
⋮
⋮
⋮
⋮
⋮
63
111111 ∗0.00101011 = 001010.10010101
10
0.10010101 ∗000110 = 000011.01111110
3
The scenario we describe with an expensive division applies to current processors. Indeed, integer division instructions on
recent x64 processors have a latency of 26 cycles for 32-bit registers and at least 35 cycles for 64-bit registers1. We ﬁnd sim-
ilar latencies in the popular ARM processors2. Thus, most optimizing compilers replace integer divisions by constants 푑that
are known at compile time with the equivalent of a multiplication by a scaled approximate reciprocal 푐followed by a shift. To
compute the remainder by a constant (푛mod 푑), an optimizing compiler might ﬁrst compute the quotient 푛div 푑as a multipli-
cation by 푐followed by a logical shift by 퐹bits (푐∗푛) div 2퐹, and then use the fact that the remainder can be derived using a
multiplication and a subtraction as 푛mod 푑= 푛−(푛div 푑) ∗푑.
Current optimizing compilers discard the fractional portion of the multiplication ((푐∗푛) mod 2퐹). Yet using the fractional bits
to compute the remainder or test the divisibility in software has merit. It can be faster (e.g., by more than 25%) to compute the
remainder using the fractional bits compared to the code produced for some processors (e.g., x64 processors) by a state-of-the-art
optimizing compiler.
2
RELATED WORK
Jacobsohn3 derives an integer division method for unsigned integers by constant divisors. After observing that any integer divisor
can be written as an odd integer multiplied by a power of two, he focuses on the division by an odd divisor. He ﬁnds that we
can divide by an odd integer by multiplying by a fractional inverse, followed by some rounding. He presents an implementation
solely with full adders, suitable for hardware implementations. He observes that we can get the remainder from the fractional
portion with rounding, but he does not specify the necessary rounding or the number of bits needed.
In the special case where we divide by 10, Vowels4 describes the computation of both the quotient and remainder. In contrast
with other related work, Vowels presents the computation of the remainder directly from the fractional portion. Multiplications
are replaced by additions and shifts. He does not extend the work beyond the division by 10.

4
D. LEMIRE, O. KASER, N. KURZ
Granlund and Montgomery5 present the ﬁrst general-purpose algorithms to divide unsigned and signed integers by constants.
Their approach relies on a multiplication followed by a division by a power of two which is implemented as an logical shift
((⌈2퐹∕푑⌉∗푛) div 2퐹). They implemented their approach in the GNU GCC compiler, where it can still be found today (e.g., up
to GCC version 7). Given any non-zero 32-bit divisor known at compile time, the optimizing compiler can (and usually will)
replace the division by a multiplication followed by a shift. Following Cavagnino and Werbrouck6, Warren7 ﬁnds that Granlund
and Montgomery choose a slightly suboptimal number of fractional bits for some divisors. Warren’s slightly better approach is
found in LLVM’s Clang compiler. See Algorithm 1.
Algorithm 1 Granlund-Montgomery-Warren5,7 division algorithm by a constant using unsigned integers.
1: Require: ﬁxed integer divisor 푑∈[1, 2푁)
2: Require: runtime integer numerator 푛∈[0, 2푁)
3: Compute: the integer 푛div 푑
4: if 푑is a power of two (푑= 2퐾) then
5:
return 푛div 2퐾
⊳Implemented with a bitwise shift
6: else if for 퐿= ⌊log2(푑)⌋and 푐= ⌈2푁+퐿∕푑⌉, we have 푐∗(2푁−(2푁mod 푑) −1) < (2푁div 푑)2푁+퐿then
7:
return (푐∗푛) div 2푁+퐿
⊳푐∈[0, 2푁)
8: else if 푑= 2퐾푑′ for 퐾> 0 then
9:
let 퐿= ⌈log2(푑′)⌉and 푐= ⌈2푁−퐾+퐿∕푑′⌉
⊳푐∈[0, 2푁)
10:
return 푐∗(푛div 2퐾) div 2푁−퐾+퐿
11: else
12:
let 퐿= ⌈log2 푑⌉and 푐= ⌈2푁+퐿∕푑⌉
⊳푐> 2푁
13:
let 푐′ be such that 푐= 2푁+ 푐′
⊳푐′ ∈[0, 2푁)
14:
return (⌊푐′ ∗푛∕2푁⌋+ ((푛−⌊푐′ ∗푛∕2푁⌋) div 2)) div 2퐿−1
15: end if
Following Artzy et al.8, Granlund and Montgomery5 describe how to check that an integer is a multiple of a constant divisor
more cheaply than by the computation of the remainder. Yet, to our knowledge, no compiler uses this optimization. Instead, all
compilers that we tested compute the remainder 푟by a constant using the formula 푟= 푛−푞∗푑and then compare against zero.
That is, they use a constant to compute the quotient, multiply the quotient by the original divisor, subtract from the original
numerator, and only ﬁnally check whether the remainder is zero.
In support of this approach, Granlund and Montgomery5 state that the remainder, if desired, can be computed by an additional
multiplication and subtraction. Warren7 covers the computation of the remainder without computing the quotient, but only for
divisors that are a power of two 2퐾, or for small divisors that are nearly a power of two (2퐾+ 1, 2퐾−1).
In software, to our knowledge, no authors except Jacobsohn3 and Vowels4 described using the fractional portion to com-
pute the remainder or test the divisibility, and neither of these considered the general case. In contrast, the computation of the
remainder directly, without ﬁrst computing the quotient, has received some attention in the hardware and circuit literature9,10,11.
Moreover, many researchers12,13 consider the computation of the remainder of unsigned division by a small divisor to be useful
when working with big integers (e.g., in cryptography).
3
COMPUTING THE REMAINDER DIRECTLY
Instead of discarding the least signiﬁcant bits resulting from the multiplication in the Granlund-Montgomery-Warren algorithm,
we can use them to compute the remainder without ever computing the quotient. We formalize this observation by Theorem 1,
which we believe to be a novel mathematical result.
In general, we expect that it takes at least 푁fractional bits for the approximate reciprocal 푐to provide exact computation of
the remainder for all non-negative numerators less than 2푁. Let us say we use 퐹= 푁+ 퐿fractional bits for some non-negative
integer value 퐿to be determined. We want to pick 퐿so that approximate reciprocal 푐= ⌈2퐹∕푑⌉= ⌈2푁+퐿∕푑⌉allows exact
computation of the remainder as 푟= ((
(푐∗푛) mod 2퐹) ∗푑) div 2퐹where 퐹= 푁+ 퐿.

D. LEMIRE, O. KASER, N. KURZ
5
We illustrate our notation using the division of 63 by 6 as in the last row of Table 1. We have that 63 is 111111 in binary and
that the approximate reciprocal 푐of 6 is ⌈28∕6⌉= 00101011 in binary. We can compute the quotient as the integer part of the
product of the reciprocal by
푛∗푐=
푁=6
⏞⏞⏞
111111 ∗0.
푁=6
⏞⏞⏞
001010
퐿=2
⏞⏞⏞
11
⏟⏞⏞⏞⏞⏞⏞⏞⏟⏞⏞⏞⏞⏞⏞⏞⏟
퐹=푁+퐿=8
=
quotient∶푁bits
⏞⏞⏞⏞⏞
0001010
.
(푐∗푛) mod 2퐹
⏞⏞⏞⏞⏞⏞⏞
10010101 .
Taking the 퐹-bit fractional portion ((푐∗푛) mod 2퐹), and multiplying it by the divisor 6 (110 in binary), we get the remainder
as the integer portion of the result:
((푐∗푛) mod 2퐹) ∗푑=
(푐∗푛) mod 2퐹
⏞⏞⏞⏞⏞⏞⏞
10010101 ∗
푑∶푁bits
⏞⏞⏞
000110 =
remainder∶푁bits
⏞⏞⏞⏞⏞
0000011
. ⋯.
The fractional portion (푐∗푛) mod 2퐹given by 10010101 is relatively close to the product of the reciprocal by the remainder
(00101011 ∗11) given by 10000001: as we shall see, this is not an accident.
Indeed, we begin by showing that the 퐹= 푁+ 퐿least signiﬁcant bits of the product ((푐∗푛) mod 2푁+퐿) are approximately
equal to the scaled approximate reciprocal 푐times the remainder we seek (푐∗(푛mod 푑)), in a way made precise by Lemma 1.
Intuitively, this intermediate result is useful because we only need to multiply this product by 푑and divide by 2퐹to cancel out
푐(since 푐∗푑≈2퐹) and get the remainder 푛mod 푑.
Lemma 1. Given 푑∈[1, 2푁), and non-negative integers 푐, 퐿such that
2푁+퐿≤푐∗푑≤2푁+퐿+ 2퐿
then
푐∗(푛mod 푑) ≤(푐∗푛) mod 2푁+퐿≤푐∗(푛mod 푑) + 2퐿(푛div 푑) < 2푁+퐿
for all 푛∈[0, 2푁).
Proof. We can write 푛uniquely as 푛= 푞∗푑+ 푟for some integers 푞and 푟where 푞≥0 and 푟∈[0, 푑). We assume that
2푁+퐿≤푐∗푑≤2푁+퐿+ 2퐿.
We begin by showing that 푐∗푟+ 2퐿푞< 2푁+퐿. Because 푐∗푑≤2푁+퐿+ 2퐿, we have that
푐∗푟+ 2퐿푞≤2푁+퐿
푑
푟+ 2퐿
푑푟+ 2퐿푞
= 2퐿
푑
(2푁푟+ 푟+ 푑∗푞)
= 2퐿
푑
(푛+ 2푁푟) .
Because 푛< 2푁and 푟< 푑, we have that 푛+ 2푁푟< 2푁푑which shows that
푐∗푟+ 2퐿푞< 2푁+퐿.
(1)
We can rewrite our assumption 2푁+퐿≤푐∗푑≤2푁+퐿+ 2퐿as 0 ≤푐∗푑−2푁+퐿≤2퐿. Multiplying throughout by the
non-negative integer 푞, we get
0 ≤푐∗푑∗푞−2푁+퐿푞≤2퐿푞.
After adding 푐∗푟throughout, we get
푐∗푟≤푐∗푛−2푁+퐿푞≤2퐿푞+ 푐∗푟
where we used the fact that 푐∗푑∗푞+ 푐∗푟= 푐∗푛. So we have that 푐∗푛−2푁+퐿푞∈[푐∗푟, 2퐿푞+ 푐∗푟]. We already showed
(see Equation 1) that 2퐿푞+ 푐∗푟is less than 2푁+퐿so that 푐∗푛−2푁+퐿푞∈[0, 2푁+퐿). Thus we have that 푐∗푛−2푁+퐿푞=
(푐∗푛) mod 2푁+퐿because (in general and by deﬁnition) if 푝−푘푄∈[0, 푦) for some 푦≤푄, then 푝mod 푄= 푝−푘푄. Hence,
we have that (푐∗푛) mod 2푁+퐿∈[푐∗푟, 2퐿푞+ 푐∗푟]. This completes the proof.
Lemma 1 tells us that (푐∗푛) mod 2푁+퐿is close to 푐∗(푛mod 푑) when 푐is close to 2푁+퐿∕푑. Thus it should make intuitive
sense that (푐∗푛) mod 2푁+퐿multiplied by 푑∕2푁+퐿should give us 푛mod 푑. The following theorem makes the result precise.

6
D. LEMIRE, O. KASER, N. KURZ
Theorem 1. Given 푑∈[1, 2푁), and non-negative integers 푐, 퐿such that
1
푑≤
푐
2푁+퐿≤1
푑+ 1∕푑
2푁
then
푛mod 푑= (((푐∗푛) mod 2푁+퐿) ∗푑) div 2푁+퐿
for all 푛∈[0, 2푁).
Proof. We can write 푛uniquely as 푛= 푞∗푑+ 푟where 푞≥0 and 푟∈[0, 푑). By Lemma 1, we have that 푐∗푛mod 2푁+퐿∈
[푐∗푟, 푐∗푟+ 2퐿푞] for all 푛∈[0, 2푁).
We want to show that if we multiply any value in [푐∗푟, 푐∗푟+ 2퐿푞] by 푑and divide it by 2푁+퐿, then we get 푟. That is, if
푦∈[푐∗푟, 푐∗푟+ 2퐿푞], then 푑∗푦∈[2푁+퐿푟, 2푁+퐿(푟+ 1)). We can check this inclusion using two inequalities:
• (푑∗푦≥2푁+퐿푟) It is enough to show that 푐∗푑∗푟≥2푁+퐿푟which follows since 푐∗푑≥2푁+퐿by one of our assumptions.
• (푑∗푦< 2푁+퐿(푟+ 1)) It is enough to show that 푑∗(푐∗푟+ 2퐿푞) < 2푁+퐿(푟+ 1). Using the assumption that 푐∗푑≤
2푁+퐿+ 2퐿, we have that 푑∗(푐∗푟+ 2퐿푞) ≤2푁+퐿푟+ 2퐿푟+ 2퐿푑∗푞= 2푁+퐿푟+ 2퐿푛. Since 푛< 2푁, we ﬁnally have
푑∗(푐∗푟+ 2퐿푞) < 2푁+퐿(푟+ 1) as required.
This concludes the proof.
Consider the constraint 2푁+퐿≤푐∗푑≤2푁+퐿+ 2퐿given by Theorem 1.
• We have that 푐= ⌈2푁+퐿∕푑⌉is the smallest value of 푐satisfying 2푁+퐿≤푐∗푑.
• Furthermore, when 푑does not divide 2푁+퐿, we have that ⌈2푁+퐿∕푑⌉∗푑= 2푁+퐿+ 푑−(2푁+퐿mod 푑) and so 푐∗푑≤
2푁+퐿+ 2퐿implies 푑≤(2푁+퐿mod 푑) + 2퐿. See Lemma 2.
On this basis, Algorithm 2 gives the minimal number of fractional bits 퐹. It is suﬃcient to pick 퐹≥푁+ log2(푑).
Lemma 2. Given a divisor 푑∈[1, 2푁), if we set 푐= ⌈2푁+퐿∕푑⌉, then
• 푐∗푑= 2푁+퐿+ 푑−(2푁+퐿mod 푑) when 푑is not a power of two,
• and 푐∗푑= 2푁+퐿when 푑is a power of two.
Proof. The case when 푑is a power of two follows by inspection, so suppose that 푑is not a power of two. We seek to round
2푁+퐿up to the next multiple of 푑. The previous multiple of 푑is smaller than 2푁+퐿by 2푁+퐿mod 푑. Thus we need to add
푑−2푁+퐿mod 푑to 2푁+퐿to get the next multiple of 푑.
Example. Consider Table 1 where we divide by 푑= 6 and we want to support the numerators 푛between 0 and 64 = 26 so that
푁= 6. It is enough to pick 퐹≥푁+ log2(푑) ≈8.58 or 퐹= 9 but we can do better. According to Algorithm 2, the number
of fractional bits 퐹= 푁+ 퐿= 6 + 퐿must satisfy 푑≤(26+퐿mod 푑) + 2퐿. Picking 퐿= 0 and 퐹= 6 does not work since
26 mod 6 + 1 = 5. Picking 퐿= 1 also does not work since 27 mod 6 + 2 = 4. Thus we need 퐿= 2 and 퐹= 8, at least. So we
can pick 푐= ⌈28∕6⌉= 43. In binary, representing 43 with 8 fractional bits gives 0.00101011, as in Table 1 . Let us divide 63
by 6. The quotient is (63 ∗43) div 28 = 10. The remainder is (((63 ∗43) mod 28) ∗6) div 28 = 3.
It is not always best to use the smallest number of fractional bits. For example, we can always conveniently pick 퐹= 2 ∗푁
(meaning 퐿= 푁) and 푐= ⌈22푁∕푑⌉, since 푑≤2퐹mod 푑+ 2푁clearly holds (given 푑≤2푁).
3.1
Directly Computing Signed Remainders
Having established that we could compute the remainder in the unsigned case directly, without ﬁrst computing the quotient, we
proceed to establish the same in the signed integer case. We assume throughout that the processor represents signed integers in
[−2푁−1, 2푁−1) using the two’s complement notation. We assume that the integer 푁≥1.
Though the quotient and the remainder have a unique deﬁnition over positive integers, there are several valid ways to deﬁne
them over signed integers. We adopt a convention regarding signed integer arithmetic that is widespread among modern computer

D. LEMIRE, O. KASER, N. KURZ
7
Algorithm 2 Algorithm to select the number of fractional bits and the scaled approximate reciprocal in the case of unsigned
integers.
1: Require: ﬁxed integer divisor 푑∈[1, 2푁)
2: We seek the smallest number of fractional bits 퐹such that for any integer numerator 푛∈[0, 2푁), the remainder is 푟=
((
(푐∗푛) mod 2퐹) ∗푑) div 2퐹for some scaled approximate reciprocal 푐.
3: We can always choose the scaled approximate reciprocal 푐←⌈2퐹∕푑⌉.
4: if 푑is a power of two then
5:
Let 퐹←log2(푑) and 푐= 1.
6: else
7:
Let 퐹←푁+ 퐿where 퐿is the smallest integer such that 푑≤(2푁+퐿mod 푑) + 2퐿.
8: end if
languages, including C99, Java, C#, Swift, Go, and Rust. Following Granlund and Montgomery5, we let trunc(푣) be 푣rounded
towards zero: it is ⌊푣⌋if 푣≥0 and ⌈푣⌉otherwise. We use “div” to denote the signed integer division deﬁned as 푛div 푑≡
trunc(푛∕푑) and “mod” to denote the signed integer remainder deﬁned by the identity 푛mod 푑≡푛−trunc(푛∕푑) ∗푑. Changing the
sign of the divisor changes the sign of the quotient but the remainder is insensitive to the sign of the divisor. Changing the sign of
the numerator changes the sign of both the quotient and the remainder: (−푛) div 푑= −(푛div 푑) and (−푛) mod 푑= −(푛mod 푑).
Let lsb퐾(푛) be the function that selects the 퐾least signiﬁcant bits of an integer 푛, zeroing others. The result is always non-
negative (in [0, 2퐾)) in our work. Whenever 2퐾divides 푛, whether 푛is positive or not, we have lsb퐾(푛) = 0 = 푛mod 2퐾.
Remark 1. Suppose 2퐾does not divide 푛, then 푛mod 2퐾= 2퐾−lsb퐾(푛) when the integer 푛is negative, and 푛mod 2퐾= lsb퐾(푛)
when it is positive. Thus we have lsb퐾(푛) + lsb퐾(−푛) = 2퐾whenever 2퐾does not divide 푛.
We establish a few technical results before proving Theorem 2. We believe it is novel.
Lemma 3. Given 푑∈[1, 2푁−1), and non-negative integers 푐, 퐿such that
2푁−1+퐿< 푐∗푑< 2푁−1+퐿+ 2퐿,
we have that 2푁−1+퐿cannot divide 푐∗푛for any 푛∈[−2푁−1, 0).
Proof. First, we prove that 2퐿cannot divide 푐. When 2퐿divides 푐, setting 푐= 훼2퐿for some integer 훼, we have that 2푁−1 <
훼푑< 2푁−1 + 1, but there is no integer between 2푁−1 and 2푁−1 + 1.
Next, suppose that 푛∈[−2푁−1, 0) and 2푁−1+퐿divides 푐∗푛. Since 2푁−1+퐿divides 푐∗푛, we know that the prime factorization
of 푐∗푛has at least 푁−1 + 퐿copies of 2. Within the range of 푛([−2푁−1, 0)) at most 푁−1 copies of 2 can be provided.
Obtaining the required 푁−1 + 퐿copies of 2 is only possible when 푛= −2푁−1 and 푐provides the remaining copies—so 2퐿
divides 푐. But that is impossible.
Lemma 4. Given 푑∈[1, 2푁−1), and non-negative integers 푐, 퐿such that
2푁−1+퐿< 푐∗푑< 2푁−1+퐿+ 2퐿,
for all integers 푛∈(0, 2푁−1] and letting 푦= lsb푁−1+퐿(푐∗푛), we have that (푦∗푑) mod 2푁−1+퐿> 0.
Proof. Write 푛= 푞∗푑+ 푟where 푟∈[0, 푑).
Since 푐∗푛is positive we have that 푦= (푐∗푛) mod 2푁−1+퐿= lsb푁−1+퐿(푐∗푛). (See Remark 1.)
Lemma 1 is applicable, replacing 푁with 푁−1. We have a stronger constraint on 푐∗푑, but that is not harmful. Thus we have
푦∈[푐∗푟, 푐∗푟+ 2퐿푞].
We proceed as in the proof of Theorem 1. We want to show that 푑∗푦∈(2푁−1+퐿푟, 2푁−1+퐿(푟+ 1)).
• (푑∗푦> 2푁−1+퐿푟) We have 푦≥푐∗푟. Multiplying throughout by 푑, we get 푑∗푦≥푐∗푑∗푟> 2푁−1+퐿푟, where we used
푐∗푑> 2푁−1+퐿in the last inequality.
• (푑∗푦< 2푁−1+퐿(푟+ 1)) We have 푦≤푐∗푟+ 2퐿푞. Multiplying throughout by 푑, we get 푑∗푦≤푐∗푑∗푟+ 2퐿푞∗푑<
2푁−1+퐿푟+ 2퐿푛. Because 푛≤2푁−1, we have the result 푑∗푦< 2푁−1+퐿(푟+ 1).
Thus we have 푑∗푦∈(2푁−1+퐿푟, 2푁−1+퐿(푟+ 1)), which shows that (푦∗푑) mod 2푁−1+퐿> 0. This completes the proof.

8
D. LEMIRE, O. KASER, N. KURZ
Lemma 5. Given positive integers 푎, 푏, 푑, we have that ⌊(푏−푎) ∗푑∕푏⌋= 푑−1 −⌊푎∗푑∕푏⌋if 푏does not divide 푎∗푑.
Proof. Deﬁne 푝
=
푎∗푑∕푏−⌊푎∗푑∕푏⌋. We have 푝
∈
(0, 1). We have that ⌊(푏−푎) ∗푑∕푏⌋
=
⌊푑−푎∗푑∕푏⌋
=
⌊푑−⌊푎∗푑∕푏⌋−푝⌋= 푑−1 −⌊푎∗푑∕푏⌋.
Theorem 2. Given 푑∈[1, 2푁−1), and non-negative integers 푐, 퐿such that
2푁−1+퐿< 푐∗푑< 2푁−1+퐿+ 2퐿,
let 휇= ⌊(lsb푁−1+퐿(푐∗푛) ∗푑)∕2푁−1+퐿⌋then
• 푛mod 푑= 휇for all 푛∈[0, 2푁−1)
• and 푛mod 푑= 휇−푑+ 1 for all 푛∈[−2푁−1, 0).
Proof. When 푛is non-negative, then so is 푐∗푛, and lsb푁−1+퐿(푐∗푛) is equal to (푐∗푛) mod 2푁−1+퐿; there is no distinction
between signed and unsigned mod, so the result follows by Theorem 1, replacing 푁by 푁−1. (Theorem 1 has a weaker
constraint on 푐∗푑.)
Suppose that 푛is negative (푛∈[−2푁−1, 0)). By Lemma 3, 2푁−1+퐿cannot divide 푐∗푛. Hence, we have that lsb푁−1+퐿(푐∗푛) =
2푁−1+퐿−lsb푁−1+퐿(푐∗(−푛)) by Remark 1. Thus we have
휇=
⌊
lsb
푁−1+퐿(푐∗푛) ∗푑∕2푁−1+퐿⌋
=
⌊(
2푁−1+퐿−
lsb
푁−1+퐿(푐∗(−푛))
)
∗푑∕2푁−1+퐿⌋
by Remark 1
= 푑−1 −
⌊
lsb
푁−1+퐿(푐∗(−푛)) ∗푑∕2푁−1+퐿⌋
by Lemmata 4 and 5
= 푑−1 −⌊((푐∗(−푛)) mod 2푁−1+퐿) ∗푑∕2푁−1+퐿⌋
= 푑−1 −((−푛) mod 푑)
by Theorem 1.
Hence we have 휇−푑+ 1 = −((−푛) mod 푑) = 푛mod 푑, which concludes the proof.
We do not need to be concerned with negative divisors since 푛mod 푑= 푛mod −푑for all integers 푛.
We can pick 푐, 퐿in a manner similar to the unsigned case. We can choose 푐= ⌊2퐹∕푑⌋+ 1 and let 퐹= 푁−1 + 퐿where 퐿is
an integer such that 2푁−1+퐿< 푐∗푑< 2푁−1+퐿+ 2퐿. With this choice of 푐, we have that 푐∗푑= 2푁−1+퐿−(2푁−1+퐿mod 푑) + 푑.
Thus we have the constraint −(2푁−1+퐿mod 푑) + 푑< 2퐿on 퐿. Because −(2푁−1+퐿mod 푑) + 푑∈[1, 푑], it suﬃces to pick 퐿
large enough so that 2퐿> 푑. Thus any 퐿> log2 푑would do, and hence 퐹> 푁+ log2(푑) is suﬃcient. It is not always best to
pick 퐿to be minimal: it could be convenient to pick 퐿= 푁+ 1.
3.2
Fast Divisibility Check with a Single Multiplication
Following earlier work by Artzy et al.8, Granlund and Montgomery5 describe how we can check quickly whether an unsigned
integer is divisible by a constant, without computing the remainder. We summarize their approach before providing an alternative.
Given an odd divisor 푑, we can ﬁnd its (unique) multiplicative inverse ̄푑deﬁned as 푑∗̄푑mod 2푁= 1. The existence of a
multiplicative inverse ̄푑allows us to quickly divide an integer 푛by 푑when it is divisible by 푑, if 푑is odd. It suﬃces to multiply
푛= 푎∗푑by ̄푑: 푛∗̄푑mod 2푁= 푎∗(푑∗̄푑) mod 2푁= 푎mod 2푁= 푛div 푑. When the divisor is 2퐾푑for 푑odd and 푛is divisible
by 2퐾푑, then we can write 푛div(2퐾푑) = (푛div 2퐾) ∗̄푑mod 2푁. As pointed out by Granlund and Montgomery, this observation
can also enable us to quickly check whether a number is divisible by 푑. If 푑is odd and 푛∈[0, 2푁) is divisible by 푑, then
푛∗̄푑mod 2푁∈[0, ⌊(2푁−1)∕푑⌋]. Otherwise 푛is not divisible by 푑. Thus, when 푑is odd, we can check whether any integer in
[0, 2푁) is divisible by 푑with a multiplication followed by a comparison. When the divisor is even (2퐾푑), then we have to check
that 푛∗̄푑mod 2푁∈[0, 2퐾∗⌊(2푁−1)∕푑⌋] and that 푛is divisible by 2퐾(i.e., 푛mod 2퐾= 0). We can achieve the desired result
by computing 푛∗̄푑, rotating the resulting word by 퐾bits and comparing the result with ⌊(2푁−1)∕푑⌋.
Granlund and Montgomery can check that an unsigned integer is divisible by another using as little as one multiplication
and comparison when the divisor is odd, and a few more instructions when the divisor is even. Yet we can always check the
divisibility with a single multiplication and a modulo reduction to a power of two—even when the divisor is even because of
the following proposition. Moreover, a single precomputed constant (푐) is required.

D. LEMIRE, O. KASER, N. KURZ
9
uint32_t d = ...; // your
divisor > 0
// c = ceil( (1<<64) / d ) ; we take L = N
uint64_t c = UINT64_C (0 xFFFFFFFFFFFFFFFF ) / d + 1;
// fastmod
computes (n mod d) given
precomputed c
uint32_t
fastmod(uint32_t n /* , uint64_t c, uint32_t d */) {
uint64_t
lowbits = c * n;
return (( __uint128_t)lowbits * d) >> 64;
}
FIGURE 1 C code implementing a fast unsigned remainder function using the __uint128_t type extension.
Proposition 1. Given 푑∈[1, 2푁), and non-negative integers 푐, 퐿such that 2푁+퐿≤푐∗푑≤2푁+퐿+ 2퐿then given some
푛∈[0, 2푁), we have that 푑divides 푛if and only if (푐∗푛) mod 2푁+퐿< 푐.
Proof. We have that 푑divides 푛if and only if 푛mod 푑= 0. By Lemma 1, we have that 푐∗(푛mod 푑) ≤(푐∗푛) mod 2푁+퐿≤
푐∗(푛mod 푑) + 2퐿(푛div 푑). We want to show that 푛mod 푑= 0 is equivalent to (푐∗푛) mod 2푁+퐿< 푐.
Suppose that 푛mod 푑= 0, then we have that (푐∗푛) mod 2푁+퐿≤2퐿(푛div 푑). However, by our constraints on 푐, we have that
푐≥2푁+퐿∕푑> 2퐿(푛div 푑). Thus, if 푛mod 푑= 0 then (푐∗푛) mod 2푁+퐿< 푐.
Suppose that (푐∗푛) mod 2푁+퐿< 푐, then because 푐∗(푛mod 푑) ≤(푐∗푛) mod 2푁+퐿, we have that 푐∗(푛mod 푑) < 푐which
implies that 푛mod 푑= 0. This completes the proof.
Thus if we have a reciprocal 푐= ⌈2퐹∕푑⌉with 퐹= 푁+ 퐿large enough to compute the remainder exactly (see Algorithm 2),
then (푐∗푛) mod 2퐹< 푐if and only if 푛is divisible by 푑. We do not need to pick 퐹as small as possible. In particular, if we set
푐= ⌈22푁∕푑⌉, then (푐∗푛) mod 22푁< 푐if and only if 푛is divisible by 푑.
Remark 2. We can extend our fast divisibility check to the signed case. Indeed, we have that 푑divides 푛if and only if |푑| divides
|푛|. Moreover, the absolute value of any 푁-bit negative integer can be represented as an 푁-bit unsigned integer.
4
SOFTWARE IMPLEMENTATION
Using the C language, we provide our implementations of the 32-bit remainder computation (i.e., a % d) in Figs. 1 and 2 for
unsigned and signed integers. In both case, the programmer is expected to precompute the constant c. For simplicity, the code
shown here explicitly does not handle the divisors 푑∈{−1, 0, 1, −231}.
For the x64 platforms, we provide the instruction sequences in assembly code produced by GCC and Clang for computing
푛mod 95 in Fig. 3; in the third column, we provide the x64 code produced with our approach after constant folding. Our approach
generates about half as many instructions.
In Fig. 4, we make the same comparison on the 64-bit ARM platform, putting side-by-side compiler-generated code for the
Granlund-Montgomery-Warren approach with code generated from our approach. As a RISC processor, ARM does not handle
most large constants in a single machine instruction, but typically assembles them from 16-bit quantities. Since the Granlund-
Montgomery-Warren algorithm requires only 32-bit constants, two 16-bit values are suﬃcient whereas our approach relies
on 64-bit quantities and thus needs four 16-bit values. The ARM processor also has a “multiply-subtract” instruction that is
particularly convenient for computing the remainder from the quotient. Unlike the case with x64, our approach does not translate
into signiﬁcantly fewer instructions on the ARM platform.
These code fragments show that a code-size saving is achieved by our approach on x64 processors, compared to the approach
taken by the compilers. We verify in § 5 that there is also a runtime advantage.
4.1
Divisibility
We are interested in determining quickly whether a 32-bit integer 푑divides a 32-bit integer 푛—faster than by checking whether
the remainder is zero. To the best of our knowledge, no compiler includes such an optimization, though some software libraries

10
D. LEMIRE, O. KASER, N. KURZ
int32_t d = ...; // your non -zero
divisor in
[ -2147483647 ,2147483647]
uint32_t pd =
d < 0 ? -d : d; // absolute value , abs(d)
// c = floor( (1<<64) / pd ) + 1; Take L = N + 1
uint64_t c = UINT64_C (0 xFFFFFFFFFFFFFFFF ) / pd
+ 1 + ((pd & (pd -1))==0 ? 1 : 0);
// fastmod
computes (n mod d) given
precomputed c
int32_t
fastmod(int32_t n /* , uint64_t c, uint32_t pd */) {
uint64_t
lowbits = c * n;
int32_t
highbits = (( __uint128_t) lowbits * pd) >> 64;
// answer is equivalent to (n<0) ? highbits - 1 + d : highbits
return
highbits - ((pd - 1) & (n >> 31));
}
FIGURE 2 C code implementing a fast signed remainder function using the __uint128_t type extension.
// GCC 6.2
mov
eax , edi
mov
edx , 1491936009
mul
edx
mov
eax , edi
sub
eax , edx
shr
eax
add
eax , edx
shr
eax , 6
imul
eax , eax , 95
sub
edi , eax
mov
eax
// Clang 4.0
mov
eax , edi
imul
rax , rax , 1491936009
shr
rax , 32
mov
ecx , edi
sub
ecx , eax
shr
ecx
add
ecx , eax
shr
ecx , 6
imul
eax , ecx , 95
sub
edi , eax
mov
eax , edi
// our fast
version
// + GCC 6.2
movabs
rax ,
194176253407468965
mov
edi , edi
imul
rdi , rax
mov
eax , 95
mul
rdi
mov
rax , rdx
//
//
//
//
FIGURE 3 Comparison between the x64 code generated by GCC 6.2 for unsigned 푎mod 95 (left) and our version (right).
Clang 4.0 generated the middle code, and when compiling our version (not shown) used a mulx instruction to place the high
bits of the product directly into the return register, saving one instruction over GCC.
provide related fast functions.b We present the code for our approach (LKK) in Fig. 5, and our implementation of the Granlund-
Montgomery approach (GM) in Fig. 6.
5
EXPERIMENTS
Superiority over the Granlund-Montgomery-Warren approach might depend on such CPU characteristics as the relative speeds
of instructions for integer division, 32-bit integer multiplication and 64-bit integer division. Therefore, we tested our software on
several x64 platforms and on ARMc and POWER8 servers, and relevant details are given in Table 2. The choice of multiplication
instructions and instruction scheduling can vary by compiler, and thus we tested using various versions of GNU GCC and
LLVM’s Clang. For brevity we primarily report results from the Skylake platform, with comments on points where the other
bhttps://gmplib.com
cWith GCC 4.8 on the ARM platform we observed that, for many constant divisors, the compiler chose to generate a udiv instruction instead of using the Granlund-
Montgomery code sequence. This is not seen for GCC 6.2.

D. LEMIRE, O. KASER, N. KURZ
11
// GCC 6.2 for a % 95 on ARM
mov
w1 , 8969
mov
w3 , 95
movk
w1 , 0x58ed , lsl 16
umull
x1 , w0 , w1
lsr
x1 , x1 , 32
sub
w2 , w0 , w1
add
w1 , w1 , w2 , lsr 1
lsr
w1 , w1 , 6
msub
w0 , w1 , w3 , w0
// our
version of a % 95 + GCC 6.2
mov
x2 , 7589
uxtw
x0 , w0
movk
x2 , 0x102b , lsl 16
mov
x1 , 95
movk
x2 , 0xda46 , lsl 32
movk
x2 , 0x2b1 , lsl 48
mul
x0 , x0 , x2
umulh
x0 , x0 , x1
//
FIGURE 4 Comparison between the ARM code generated by GNU GCC 6.2 for 푎mod 95 (left) and our word-aligned version
(right). In both cases, except for instruction order, Clang’s code was similar to GCC’s.
// calculate c for use in lkk_divisible
uint64_t
lkk_cvalue(uint32_t d) {
return 1 + UINT64_C (0 xffffffffffffffff ) / d;
}
// given
precomputed c, checks
whether n % d == 0
bool
lkk_divisible(uint32_t n,
uint64_t c) {
// rhs is large
when c==0
return n * c <= c - 1;
}
FIGURE 5 Unsigned divisibility test, our approach.
platforms were signiﬁcantly diﬀerent. For the Granlund-Montgomery-Warren approach with compile-time constants, we use
the optimized divide and remainder operations built into GCC and Clang.
We sometimes need to repeatedly divide by a constant that is known only at runtime. In such instances, an optimizing compiler
may not be helpful. Instead a programmer might rely on a library oﬀering fast division functions. For runtime constants on x64
processors, we use the libdivide libraryd as it provides a well-tested and optimized implementation.
On x64 platforms, we use the compiler ﬂags -O3 -march=native; on ARM we use -O3 -march=armv8-a and on POWER8
we use -O3 -mcpu=power8. Some tests have results reported in wall-clock time, whereas in other tests, the Linux perf
stat command was used to obtain the total number of processor cycles spent doing an entire benchmark program. To ease
reproducibility, we make our benchmarking software and scripts freely available.e
5.1
Beating the Compiler
We implement a 32-bit linear congruential generator14 that generates random numbers according to the function 푋푛+1 =
(푎∗푋푛+ 푏) mod 푑, starting from a given seed 푋0. Somewhat arbitrarily, we set the seed to 1234, we use 31 as the multi-
plier (푎= 31) and the additive constant is set to 27961 (푏= 27961). We call the function 100 million times, thus generating
100 million random numbers. The divisor 푑is set at compile time. See Fig. 7 . In the signed case, we use a negative multiplier
(푎= −31).
dhttp://libdivide.com
ehttps://github.com/lemire/constantdivisionbenchmarks

12
D. LEMIRE, O. KASER, N. KURZ
// rotate n by e bits , avoiding
undefined
behaviors
// cf https :// blog.regehr.org/archives /1063
uint32_t
rotr32(uint32_t n, uint32_t e) {
return (n >> e) | ( n << ( (-e)&31) );
}
// does d divide n?
// d = 2**e * d_odd; dbar =
multiplicative_inverse (d_odd)
// thresh = 0xffffffff / d
bool
gm_divisible(uint32_t n,
uint32_t e, uint32_t dbar ,
uint32_t
thresh) {
return
rotr32(n * dbar , e) <= thresh;
}
// Newton ’s method per Warren ,
// Hacker ’s Delight
pp. 246 - -247
uint32_t
multiplicative_inverse (uint32_t d) {
uint32_t x0 = d + 2 * ((d+1) & 4);
uint32_t x1 = x0 * (2 - d * x0);
uint32_t x2 = x1 * (2 - d * x1);
return
x2 * (2 - d * x2);
}
FIGURE 6 Unsigned divisibility test, Granlund-Montgomery approach.
TABLE 2 Systems Tested
Processor
Microarchitecture
Compilers
Intel i7-6700
Skylake (x64)
GCC 6.2; Clang 4.0
default platform
Intel i7-4770
Haswell (x64)
GCC 5.4; Clang 3.8
AMD Ryzen 7 1700X
Zen (x64)
GCC 7.2; Clang 4.0
POWER8
POWER8 Murano
GCC 5.4; Clang 3.8
AMD Opteron A1100
ARM Cortex A57 (Aarch64)
GCC 6.2; Clang 4.0
Because the divisor is a constant, compilers can optimize the integer division using the Granlund-Montgomery approach. We
refer to this scenario as the compiler case. To prevent the compiler from proceeding with such an optimization and force it to
repeatedly use the division instruction, we can declare the variable holding the modulus to be volatile (as per the C standard).
We refer to this scenario as the division instruction case. In such cases, the compiler is not allowed to assume that the modulus is
constant—even though it is. We veriﬁed that the assembly generated by the compiler includes the division instruction and does
not include expensive operations such as memory barriers or cache ﬂushes. We veriﬁed that our wall-clock times are highly
repeatablef.
We present our results in Fig. 8 where we compare with our alternative. In all cases, our approach is superior to the code
produced by the compiler, except for powers of two in the case of GCC. The beneﬁt of our functions can reach 30%.
fFor instance, we repeated tests 20 times for 9 divisors in Figs. 8 abcd and 9 ab, and we observed maximum diﬀerences among the 20 trials of 4.8 %, 0.3 %, 0.7 %,
0.0 %, 0.8 % and 0.9 %, respectively.

D. LEMIRE, O. KASER, N. KURZ
13
uint32_t x = 1234;
for(size_t i = 0; i < 100000000; i++) {
// d may be set at compile
time
x = (32 * x + 27961) % d;
}
FIGURE 7 Code for a linear congruential generator used to benchmark division by a constant.
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 10
 20
 30
 40
 50
 60
time (s)
divisor
division instruction
compiler
our approach (LKK)
(a) GNU GCC (unsigned)
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 10
 20
 30
 40
 50
 60
time (s)
divisor
division instruction
compiler
our approach (LKK)
(b) GNU GCC (signed)
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 10
 20
 30
 40
 50
 60
time (s)
divisor
division instruction
compiler
our approach (LKK)
(c) LLVM’s Clang (unsigned)
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 1
 10
 20
 30
 40
 50
 60
time (s)
divisor
division instruction
compiler
our approach (LKK)
(d) LLVM’s Clang (signed)
FIGURE 8 Wall-clock time to compute 100 million random integers using a linear congruential generator with various divisors
set at compile time (Skylake x64)
The performance of the compiler (labelled as compiler) depends on the divisor for both GCC and Clang, though Clang has
greater variance. The performance of our approach is insensitive to the divisor, except when the divisor is a power of two.
We observe that in the unsigned case, Clang optimizes very eﬀectively when the divisor is a small power of two. This remains
true even when we disable loop unrolling (using the -fno-unroll-loops compiler ﬂag). By inspecting the produced code,
we ﬁnd that Clang (but not GCC) optimizes away the multiplication entirely in the sense that, for example, 푋푛+1 = (31 ∗푋푛+
27961) mod 16 is transformed into 푋푛+1 = lsb4(9 −푋푛). We ﬁnd it interesting that these optimizations are applied both in the
compiler functions as well as in our functions. Continuing with the unsigned case, we ﬁnd that Clang often produces slightly
more eﬃcient compiled code than GCC for our functions, even when the divisor is not a power of two: compare Fig. 8 a with
Fig. 8 c. However, these small diﬀerences disappear if we disable loop unrolling.
Yet, GCC seems preferable in the signed benchmark: in Figs. 8 b and 8 d, Clang is slightly less eﬃcient than GCC, sometimes
requiring 0.5 s to complete the computation whereas GCC never noticeably exceeds 0.4 s.

14
D. LEMIRE, O. KASER, N. KURZ
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 10
 20
 30
 40
 50
 60
time (s)
divisor
division instruction
compiler
our approach (LKK)
(a) Ryzen (GCC, unsigned)
 0
 0.2
 0.4
 0.6
 0.8
 1
 1.2
 10
 20
 30
 40
 50
 60
time (s)
divisor
division instruction
compiler
our approach (LKK)
(b) ARM (GCC, unsigned)
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 10
 20
 30
 40
 50
 60
time (s)
divisor
division instruction
compiler
our approach (LKK)
(c) POWER8 (Clang, unsigned)
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 10
 20
 30
 40
 50
 60
time (s)
divisor
division instruction
compiler
our approach (LKK)
(d) POWER8 (Clang, signed)
FIGURE 9 Ryzen, ARM and POWER8 results for small divisors.
For comparison, Fig. 9 shows how the Ryzen, POWER8 and ARM processors perform on unsigned computations.The speed
of the hardware integer-division instruction varies, speeding up at 푑= 8 and again at 푑= 32 for Ryzen and 푑= 4, 16, 64,
256 and 1024 for ARM. The gap between hardware integer division and Granlund-Montgomery (compiler) is less on Ryzen,
POWER8 and ARM than on Skylake; for some divisors, there is little beneﬁt to using compiler on POWER8 and ARM. On x64
platforms, our approach continues to be signiﬁcantly faster than hardware integer division for all divisors.
On ARM, the performance is limited when computing remainders using our approach. Unlike x64 processors, ARM proces-
sors require more than one instruction to load a constant such as the reciprocal (푐), but that is not a concern in this instance since
the compiler loads 푐into a register outside of the loop. We believe that the reduced speed has to do with the performance of the
multiplication instructions of our Cortex A57 processor2. To compute the most signiﬁcant 64 bits of a 64-bit product as needed
by our functions, we must use the multiply-high instructions (umulh and smulh), but they require six cycles of latency and they
prevent the execution of other multi-cycle instructions for an additional three cycles. In contrast, multiplication instructions on
x64 Skylake processors produce the full 128-bit product in three cycles. Furthermore, our ARM processor has a multiply-and-
subtract instruction with a latency of three cycles. Thus it is advantageous to rely on the multiply-and-subtract instruction instead
of the multiply-high instruction. Hence, it is faster to compute the remainder from the quotient by multiplying and subtracting
(푟= 푛−(푛div 푑) ∗푑). Furthermore, our ARM processor has fast division instructions: the ARM optimization manual for Cortex
A57 processors indicates that both signed and unsigned division require between 4 and 20 cycles of latency2 whereas integer
division instructions on Skylake processors (idiv and div) have 26 cycles of latency for 32-bit registers1. Even if we take into
account that division instructions on ARM computes solely the quotient, as opposed to both the quotient and remainder on x64,
it seems that the ARM platform has a competitive division latency. Empirically, the division instruction on ARM is often within
10% of the Granlund-Montgomery compiler optimization (Fig. 9 b) whereas the compiler optimization is consistently more
than twice as fast as the division instruction on a Skylake processor (see Fig. 8 a).

D. LEMIRE, O. KASER, N. KURZ
15
 0
 0.05
 0.1
 0.15
 0.2
 0.25
 0.3
 0.35
 0.4
 0.45
 0.5
 100
 10000
 1x106
 1x108
time (s)
divisor
division instruction
compiler
our approach (LKK)
(a) Ryzen (GCC)
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 100
 10000
 1x106
 1x108
time (s)
divisor
division instruction
compiler
our approach (LKK)
(b) ARM (Clang)
FIGURE 10
Ryzen and ARM results for 28 larger divisors (using unsigned arithmetic). Our approach performed slightly
worse when compiled by GCC on ARM, but the Ryzen results were not sensitive to the choice of the compiler. On Skylake (not
shown), the division instruction behaved similarly for large and small divisors, as did compiler and our approach.
Results for POWER8 are shown in Figs. 9 c and 9 d. Our unsigned approach is better than the compiler’s; indeed the compiler
would sometimes have done better to generate a divide instruction than use the Granlund-Montgomery approach. For our signed
approach, both GCC and Clang had trouble generating eﬃcient code for many divisors.
As with ARM, code generated for POWER8 also deals with 64-bit constants less directly than x64 processors. If not in
registers, POWER8 code loads 64-bit constants from memory, using two operations to construct a 32-bit address that is then used
with a load instruction. In this benchmark, however, the compiler keeps 64-bit constants in registers. Like ARM, POWER8 has
instructions that compute the upper 64 bits of a 64-bit product. The POWER8 microarchitecture15 has good support for integer
division: it has two ﬁxed-point pipelines, each containing a multiplier unit and a divider unit. When the multi-cycle divider unit
is operating, ﬁxed-point operations can usually be issued to other units in its pipeline. In our benchmark, dependencies between
successive division instructions prevent the processor from using more than one divider. Though we have not seen published data
on the actual latency and throughput of division and multiplication on this processor, we did not observe the divisor aﬀecting
the division instruction’s speed, at least within the range of 3 to 4096.
Our results suggest that the gap between multiplication and division performance on the POWER8 lies between that of
ARM and Intel; the fact that our approach (using 64-bit multiplications) outperforms the compiler’s approach (using 32-bit
multiplications) seems to indicate that, unlike ARM, the instruction to compute the most signiﬁcant bits of a 64-bit product is
not much slower than the instruction to compute a 32-bit product.
Looking at Fig. 10, we see how the approaches compare for larger divisors. The division instruction is sometimes the fastest
approach on ARM, and sometimes it can be faster than the compiler approach on Ryzen. Overall, our approach is preferred on
Ryzen (as well as Skylake and POWER8), but not on ARM.
5.2
Beating the libdivide Library
There are instances when the divisor might not be known at compile time. In such instances, we might use a library such as a
libdivide. We once again use our benchmark based on a linear congruential generator using the algorithms, but this time, we
provide the divisor as a program parameter.
The libdivide library does not have functions to compute the remainder, so we use its functions to compute the quotient. It
has two types of functions: regular "branchful" ones, those that include some branches that depend on the divisor, and branch-
less ones. In this benchmark, the invariant divisor makes the branches perfectly predictable, and thus the libdivide branchless
functions were always slower. Consequently we omit the branchless results.

16
D. LEMIRE, O. KASER, N. KURZ
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 10
 20
 30
 40
 50
 60
time (s)
divisor
division instruction
libdivide
our approach (LKK)
(a) GNU GCC (unsigned)
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 10
 20
 30
 40
 50
 60
time (s)
divisor
division instruction
libdivide
our approach (LKK)
(b) GNU GCC (signed)
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 10
 20
 30
 40
 50
 60
time (s)
divisor
division instruction
libdivide
our approach (LKK)
(c) LLVM’s Clang (unsigned)
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 1
 10
 20
 30
 40
 50
 60
time (s)
divisor
division instruction
libdivide
our approach (LKK)
(d) LLVM’s Clang (signed)
FIGURE 11 Wall-clock time to compute 100 million random integers using a linear congruential generator with various
divisors passed as a program parameter (Skylake x64).
We present our results in Fig. 11. The performance levels of our functionsg are insensitive to the divisor, and our performance
levels are always superior to those of the libdivide functions (by about 15%), except for powers of two in the unsigned case. In
these cases, libdivide is faster, but this is explained by a fast conditional code path for powers of two.
5.3
Competing for Divisibility
We adapted a prime-counting benchmark distributed with libdivide, specialized to 32-bit operands. The code determines the
number of primes in [2, 40000) using a simplistic approach: odd numbers in this range are checked for divisibility by any smaller
number that has already been determined to be prime. See Fig. 12. When a number is identiﬁed as a prime, we compute its
scaled approximate reciprocal (푐) value, which is repeatedly used in future iterations. In this manner, the computation of 푐is
only done once per prime, and not once per trial division by the prime. A major diﬀerence from the benchmark using the linear-
congruential generator is that we cycle rapidly between diﬀerent divisors, making it much more diﬃcult to predict branches in
the libdivide functions.
In these tests, we compare libdivide against LKK and GM, the fast divisibility tests whose implementations are shown in
§ 4.1; see Fig. 5 for LKK and Fig. 6 for GM. Divisibility of a candidate prime is checked either using
• libdivide to divide, followed by multiplication and subtraction to determine whether the remainder is nonzero;
• the Granlund-Montgomery (GM) divisibility check, as in Fig. 6;
gWhen 20 test runs were made for 9 divisors, timing results among the 20 never diﬀered by more than 1%.

D. LEMIRE, O. KASER, N. KURZ
17
int
count_primes_under_N () {
int
primectr =0;
static
uint64_t
prime_cvals[N];
for (uint32_t n=3; n < N; n += 2) {
bool
isprime=true;
for (int j=0; j < primectr; ++j) {
if (lkk_divisible(n, prime_cvals[j])) {
isprime = false;
break;
}
}
if (isprime)
prime_cvals[primectr ++] = lkk_cvalue(n);
}
return (1+ primectr);
// 2 is also
prime.
}
FIGURE 12 Prime-counting benchmark for the unsigned divisibility test. The code shown is for the LKK approach, similar
code is used for other strategies.
• the C % operation, which uses a division instruction;
• our LKK divisibility check (Fig. 5).
LKK stores 64 bits for each prime; GM requires an additional 5-bit rotation amount. The division-instruction version of the
benchmark only needs to store 32 bits per prime. The libdivide approach requires 72 bits per prime, because we explicitly store
the primes.
Instruction counts and execution speed are both important. All else being equal, we would prefer that compilers emit smaller
instruction sequences. Using a hardware integer division will yield the smallest code, but this might give up too much speed. In
the unsigned case, our LKK has a signiﬁcant code-size advantage over GM—approximately 3 arithmetic instructions to compute
our 푐versus about 11 to compute their required constant. Both fast approaches use a multiplication and comparison for each
subsequent divisibility check. GM requires an additional instruction to rotate the result of the multiplication.
Performance results for the unsigned case are shown in Table 3, showing the total number of processor cycles on each platform
from 1000 repetitions of the benchmark. On Skylake, 20 repeated computations yielded cycle-count results within 0.3% of each
other. For ARM, results were always within 4%. Initially, Ryzen results would sometimes diﬀer by up to 10% within 20 attempts,
even after we attempted to control such factors as dynamic frequency scaling. Thus, rather than reporting the ﬁrst measurement
for each benchmark, the given Ryzen results are the average of 11 consecutive attempts (the basic benchmark was essentially
executed 11 000 times). Our POWER8 results (except one outlier) were within 7% of one another over multiple trials and so we
averaged several attempts (3 for GCC and 7 for Clang) to obtain each data point. Due to platform constraints, POWER8 results
are user-CPU times that matched the wall-clock times.
LKK has a clear speed advantage in all cases, including the POWER8 and ARM platforms. LKK is between 15% to 80%
faster than GM. Both GM and LKK always are much faster than using an integer division instruction (up to 7× for Ryzen) and
they also outperform the best algorithm in libdivide.
6
CONCLUSION
To our knowledge, we present the ﬁrst general-purpose algorithms to compute the remainder of the division by unsigned or
signed constant divisors directly, using the fractional portion of the product of the numerator with the approximate reciprocal3,4.
On popular x64 processors (and to a lesser extent on POWER), we can produce code for the remainder of the integer division

18
D. LEMIRE, O. KASER, N. KURZ
TABLE 3 Processor cycles (in gigacycles) to determine the number of primes less than 40000, 1000 times, using unsigned 32-
bit computations. Branchful and branchless are libdivide alternatives. Note that libdivide was only available for the x64 systems
as it uses platform-speciﬁc optimizations. POWER8 results are in user CPU seconds. Boldfacing indicates the fastest approach.
Algorithm
Skylake
Haswell
Ryzen
ARM
POWER8
GCC
Clang
GCC
Clang
GCC
Clang
GCC
Clang
GCC
Clang
division instruction
72
72
107
107
131
131
65
65
18
17
branchful
46
88
56
98
59
71
–
–
–
–
branchless
35
35
36
37
34
37
–
–
–
–
LKK
18
18
18
18
17
18
27
27
8.7
8.0
GM
24
27
27
28
27
32
36
37
10
11
GM/LKK
1.33
1.50
1.50
1.55
1.59
1.77
1.33
1.37
1.15
1.38
that is faster than the code produced by well regarded compilers (GNU GCC and LLVM’s Clang) when the divisor constant is
known at compile time, using small C functions. Our functions are up to 30% faster and with only half the number of instructions
for most divisors. Similarly, when the divisor is reused, but is not a compile-time constant, we can surpass a popular library
(libdivide) by about 15% for most divisors.
We can also speed up a test for divisibility. Our approach (LKK) is several times faster than the code produced by popular
compilers. It is faster than the Granlund-Montgomery divisibility check5, sometimes nearly twice as fast. It is advantageous on
all tested platforms (x64, POWER8 and 64-bit ARM).
Though compilers already produce eﬃcient code, we show that additional gains are possible. As future work, we could
compare against more compilers and other libraries. Moreover, various additional optimizations are possible, such as for division
by powers of two.
ACKNOWLEDGMENTS
The work is supported by the Natural Sciences and Engineering Research Council of Canada under grant RGPIN-2017-03910.
The authors are grateful to IBM’s Centre for Advanced Studies — Atlantic and Kenneth Kent for access to the POWER 8 system.
References
1. Fog A. Instruction tables: Lists of instruction latencies, throughputs and micro-operation breakdowns for Intel, AMD and
VIA CPUs. Copenhagen University College of Engineering Copenhagen, Denmark; 2016. http://www.agner.org/optimize/
instruction_tables.pdf. Accessed May 31, 2018.
2. Cortex-A57 Software Optimization Guide. ARM Holdings; 2016.
http://infocenter.arm.com/help/topic/com.arm.doc.
uan0015b/Cortex_A57_Software_Optimization_Guide_external.pdf. Accessed May 31, 2018.
3. Jacobsohn DH. A combinatoric division algorithm for ﬁxed-integer divisors. IEEE T. Comput.. 1973;100(6):608–610.
4. Vowels RA. Division by 10. Aust. Comput. J.. 1992;24(3):81–85.
5. Granlund T, Montgomery PL. Division by invariant integers using multiplication. SIGPLAN Not.. 1994;29(6):61–72.
6. Cavagnino D, Werbrouck AE. Eﬃcient algorithms for integer division by constants using multiplication. Comput. J..
2008;51(4):470–480.
7. Warren HS. Hacker’s Delight. Boston: Addison-Wesley; 2nd ed.2013.

D. LEMIRE, O. KASER, N. KURZ
19
8. Artzy E, Hinds JA, Saal HJ. A fast division technique for constant divisors. Commun. ACM. 1976;19(2):98–101.
9. Raghuram PS, Petry FE. Constant-division algorithms. IEE P-Comput. Dig. T.. 1994;141(6):334-340.
10. Doran RW. Special cases of division. J. Univers. Comput. Sci.. 1995;1(3):176–194.
11. Ugurdag F, Dinechin F De, Gener YS, Gören S, Didier LS. Hardware division by small integer constants. IEEE T. Comput..
2017;66(12):2097–2110.
12. Rutten L, Van Eekelen M. Eﬃcient and formally proven reduction of large integers by small moduli. ACM Trans. Math.
Softw.. 2010;37(2):16:1–16:21.
13. Moller N, Granlund T. Improved division by invariant integers. IEEE T. Comput.. 2011;60(2):165–175.
14. Knuth DE. Seminumerical Algorithms. The Art of Computer ProgrammingReading, MA: Addison-Wesley; 2nd ed.1981.
15. Sinharoy B, Van Norstrand J.A., Eickemeyer R.J., et al. IBM POWER8 processor core microarchitecture. IBM Journal of
Research and Development. 2015;59.
How to cite this article: D. Lemire, O. Kaser, and N. Kurz, Faster Remainder by Direct Computation, 2018.

