JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021
1
Chatbots to ChatGPT in a Cybersecurity Space:
Evolution, Vulnerabilities, Attacks, Challenges, and
Future Recommendations
Attia Qammar, Hongmei Wang, Jianguo Ding Senior Member, IEEE , Abdenacer Naouri, Mahmoud
Daneshmand Senior Life Member, IEEE, Huansheng Ning Senior Member, IEEE
Abstract—Chatbots shifted from rule-based to artificial intel-
ligence techniques and gained traction in medicine, shopping,
customer services, food delivery, education, and research. OpenAI
developed ChatGPT blizzard on the Internet as it crossed one
million users within five days of its launch. However, with the
enhanced popularity, chatbots experienced cybersecurity threats
and vulnerabilities. This paper discussed the relevant literature,
reports, and explanatory incident attacks generated against
chatbots. Our initial point is to explore the timeline of chatbots
from ELIZA (an early natural language processing computer
program) to GPT-4 and provide the working mechanism of
ChatGPT. Subsequently, we explored the cybersecurity attacks
and vulnerabilities in chatbots. Besides, we investigated the
ChatGPT, specifically in the context of creating the malware code,
phishing emails, undetectable zero-day attacks, and generation of
macros and LOLBINs. Furthermore, the history of cyberattacks
and vulnerabilities exploited by cybercriminals are discussed,
particularly considering the risk and vulnerabilities in ChatGPT.
Addressing these threats and vulnerabilities requires specific
strategies and measures to reduce the harmful consequences.
Therefore, the future directions to address the challenges were
presented.
Index Terms—Chatbots, chatGPT, cyberattacks, cybersecurity,
threats, vulnerabilities.
I. INTRODUCTION
C
HATBOTS are conversational agents to talk with humans
in natural language, including voice and textual form.
Chatbots are considered the utility program to whom we
can talk to get responses to related queries [1]. Generally,
chatbots produced the appropriate response by anticipating
the natural language inputs submitted by the users. Chatbots
are designed for certain tasks or are non-task oriented as
traditional chatterbox. The task-oriented chatbots are used
for specific purposes, including making hotel reservations,
booking travel tickets, promoting movies, etc. Whereas non-
task-oriented chatbots mimic as humans, they aim to produce
human-like responses. In the past, chatbots were designed
A. Qammar, A. Naouri, and H. Ning are with the School of Computer
and Communication Engineering, University of Science and Technology Bei-
jing, Beijing, China, email: B20200693@xs.ustb.edu.cn/q.attia@yahoo.com,
nacer.naouri@gmail.com, Corresponding author: Huansheng Ning (email:
ninghuansheng@ustb.edu.cn).
H. Wang is with the Information Engineering College, Xinjiang Institute
of Engineering, Xinjiang, China, emial: 320180032@xjau.edu.cn
J. Ding is with the Department of Computer Science, Blekinge Institute of
Technology, Sweden, emial: jianguo.ding@bth.se
M. Daneshmand is with School of Business, Stevens Institute of Technol-
ogy, New Jersey, USA, email: mdaneshm@stevens.edu
Manuscript received Month Day, Year; revised Month Day, Year.
using rule-based techniques, but now shifted to Artificial
Intelligence (AI) chatbots [2], [3]. The rule-based techniques
used pre-defined patterns to answer the queries based on the
matching information.
AI-based chatbots involve the use of generative and retrieval
methods. Both methods can work collectively to produce the
results more accurately. Generative models are trained on large
datasets and employ Deep Learning (DL) algorithms; on the
other hand, retrieval-based techniques are used to choose the
best response from the conversation stored in the repository
[4]. For instance, ChatGPT is the AI chatbot developed by
OpenAI, officially launched on the 30th of November 2022. It
is built on the top of Large Language Models (LLMs) and the
family member of Generative Pre-trained Transformer (GPTs)
iterative development. ChatGPT has been fine-tuned by imple-
menting supervised and reinforcement learning to provide re-
fined responses [5]. Furthermore, chatbots can perform certain
repetitive tasks, including knowledge management, providing
customer service, managing alerts, informative and educational
tasks, etc. [6]–[8]. Besides, ChatGPT can perform numerous
other tasks, particularly writing effectively on a diverse range
of topics, finding vulnerabilities, can solve reasoning, and an-
alytical problems, good interactive capabilities, summarizing
the text, language translations, research, generating replies,
writing Excel formulas, building a resume, cover letters and
many more [9]–[13]. However, chatbots face cybersecurity
challenges and are targeted by attackers concerning their
typical working modules, such as the client module, commu-
nication module, response generation module, and database
module [14]. Similarly, bad actors joined the ChatGPT to
perform illicit and harmful activities by intensifying the ex-
isting vulnerabilities and presenting new threats [15], [16].
In this paper, we investigated the security vulnerabilities and
attacks on the chatbots, particularly the ChatGPT as malicious
actors used the ChatGPT and brought critical cybersecurity
challenges. We explored the ChatGPT to write malicious code,
phishing emails, and zero-day attacks. Furthermore, ChatGPT
faced data breaches of some users’ bank accounts. Hence,
through ChatGPT, bad actors can significantly exploit the
system vulnerabilities and launch new attacks.
The following are the main contributions of our paper:
• The historical evolution of chatbots to ChatGPT, the
continuous evolution from OpenAI’s GPT-1 to GPT-4,
and the working mechanisms of chatbots and ChatGPT
arXiv:2306.09255v1  [cs.CR]  29 May 2023

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021
2
Fig. 1. Timeline of chatbots from ELIZA to ChatGPT
are discussed. Cybersecurity threats and vulnerabilities in
the common working modules of chatbots are elaborated
with their impact, attack vectors, and countermeasures.
• We also examined ChatGPT as a case study to test
against cybersecurity attacks. Accordingly, ChatGPT can
successfully write malicious code and phishing emails,
create undetectable zero-day attacks, generate malicious
macros and LOLBINs, etc.
• The history of chatbot cybersecurity attacks and threats
is presented, including the risks and vulnerabilities of
ChatGPT.Finally, we have highlighted the challenges and
future directions for dealing with cybersecurity risks and
threats.
The rest of the paper is organized as follows. Section 2
provides an overview of chatbots from ELIZA to GPT-4.
Section 3 presents the cybersecurity threats and vulnerabilities
in chatbots and investigates the ChatGPT for writing malicious
code. Section 4 highlights the history of cyberattacks created
against chatbots as well as the vulnerabilities of ChatGPT.
Section 5 illustrates the challenges and future directions.
II. OVERVIEW OF CHATBOTS: ELIZA TO GPT-4
A. Timeline of Chatbots
In human history, the expedition to develop something that
can communicate and comprehend with its creator was in
1950. Alan Turing questioned if it is possible for a computer
program to talk with a couple of people without perceiving
their artificial interlocution. Hence, this question was consid-
ered as the inventive idea of chatbots [17]. Chatbots or voice
bots, used as communication agents, can communicate with
text messages or voice input, respectively. With time, chatbots
are evolving with exponential growth in computational, lan-
guage, and other capabilities.
In Fig. 1, we depicted the timeline of chatbots from ELIZA
to GPT-4, including ChatGPT. Although a couple of chatbots
and voice bots are available in history, however, we considered
the most popular.
The world’s first chatbot was implemented in 1960 by the
Massachusetts Institute of Technology (MIT) professor Joseph
Weizenbaum named ELIZA. To simulate the communication,
it utilized the initial form of Artificial Intelligence (AI), such
as a pattern-matching algorithm to imitate human conversation
[18]. ELIZA responds to human sentences in interrogative
form and has limited conversational ability [19]. Though early
users are convinced of the intelligence of ELIZA, it does not
have a proper understanding of sentences and only considers
particular domains.
In 1972, PARRY came across as more advanced than
ELIZA, able to understand and control solid structures. It
initially acted as a person with paranoid schizophrenia. Fur-
thermore, PARRY can work on complicated assumptions and
respond with natural language and emotional attributes [20].
However, PARRY could not learn from conversations and has
a low response rate.
Another chatbot Jabberwacky first introduced in the domain
of AI in 1988, and was written in CleverScript language
by British programmer Rollo Carpenter [21]. Jabberwacky
responds entertainingly and humorously to the human chats. It
works differently and learns from rules and contexts. Despite
that, Jabberwacky has a limited number of users and cannot
respond quickly [22].
In 1992, Dr. Sbaitso (Sound Blaster Acting Intelligent Text
to Speech Operator) was developed. Dr.Sbaitso communicates
like a psychologist, as most of the responses related to the
“WHY DO YOU FEEL THAT WAY” as compared to answers
in any complicated way [23]. Furthermore, it repeats the text in
abusive behavior if the user types “say parity,” and if it would
not understand the context, it replied like this “THAT’S NOT
MY PROBLEM” [24].
In the history of moving forward to 1995, the A.L.I.C.E
(Artificial Linguistic Internet Computer Entity) was developed.
The ELIZA inspired A.L.I.C.E chatbot, relied on pattern
matching, and did not consider accurate perception. However,
it wins the title of the Loebner Prize as the best human-
like computer [25]. A.L.I.C.E was based on the Artificial
Intelligence Markup Language (AIML) and integrated approx-
imately 41,000 templates [26].
In 2001, SmarterChild was developed by Collloquis and
can do some interesting tasks such as knowing about weather,
calculations, and conversions and answering general questions
such as “What is the population of Malaysia?” or “What is
the weather today?” [27]. SmarterChild is still available on
the Microsoft (MSN) Windows Live Messenger.
Siri was developed by Apple in 2010 as the Personal
Assistant (PA) tool for iOS. The voice inputs are used for

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021
3
conversation and can integrate with audio, video, and images.
Siri paved the way for other PAs and AI tools, particularly
Google Assistant, Cortana, and Alexa. Siri can answer queries
and perform actions through the web. Although Siri supports
the multilingual feature, many other languages are not included
and perform navigation instruction only in English. Further-
more, it faces difficulties in hearing in the presence of noise
and heavy accents [28].
After a few years of Siri in 2012, Google Now was
developed, which answered the questions and provided recom-
mendations. Google was now part of the user interface (UI) for
mobile search. Moreover, it had a female voice assistant better
than Siri. Besides, it is connected with the Google account and
may compromise the user’s privacy [29].
In 2014 and 2015, Alexa and Cortana were implemented
as personal assistants designed by Amazon and Microsoft,
respectively [30]. Alexa utilized natural language process-
ing (NLP) to recognize, receive and answer voice inputs.
Furthermore, Alexa makes the Internet of Things (IoTs)
more reachable as it is built for entertainment and home
automation. Amazon introduced the Alexa Skills Kit (ASK),
which allows creating and publishing paid or unpaid Alexa
skills. Meanwhile, Cortana performed general tasks, including
setting reminders, sending emails, chatting, playing games,
and searching for information on the user’s request. However,
Cortana and Alexa have security flaws that can install malware
and exploit user privacy [31], [32].
In 2016, Microsoft introduced the artificial intelligence-
based chatbot ”Tay” via Twitter. The company launched the
Tay bot to experiment with “conversational understanding”.
After passing less than twenty-four hours, Tay was corrupted,
and people started racist, misogynistic, and Donald Trumpist-
related tweets [33]. Finally, Microsoft states to delete inappro-
priate tweets and make adjustments.
Another chatbot, Woebot was developed in 2017 that helps
monitor mood, ask people about their perspective, and suggest
useful tools related to their mental well-being [34]. Woebot
incorporated a combination of NLP and psychological tools
to produce a friendly conversation. According to Stanford
University research, Woebot shows promising results in re-
ducing anxiety levels in young people who communicate with
chatbots daily [34].
The Google Duplex was announced in 2018 by Google to
conduct natural conversations to complete real-world tasks on
the phone, including making appointments. The system under-
stands the natural conversations after deep training, however,
it needs help understanding the general conversations [35].
The development of AI chatbots continues; in 2020 intro-
duced the Generative Pre-trained Transformer Three (GPT-
3), in 2021 OpenAI Codex, and in 2022 chatGPT. These all
are developed by OpenAI and are accessible freely. GPT-3
was based on deep learning and produced human-like text.
The neural network model utilized in the GPT-3 contains
175 billion machine learning (ML) parameters. Before that
Microsoft used 10 billion parameters. GPT-3 can answer like
a human, generate the programming code, summarize the text,
etc. [36]. Besides, OpenAI codex trained over 50 million
GitHub repositories. It can answer programming problems and
provide the solution code as output [37].
Similarly, ChatGPT builds based on GPT-3 and is fine-tuned
by using supervised and reinforcement methods. The advanced
version of ChatGPT, named as “ChatGPT Professional” can
be purchased for $42 per month, having no downtime, access
to premium features, and speedy response [38]. Recently,
OpenAI introduced the GPT-4, as it pretrained and predicted
the next token based on the previous information. GPT-4 is
more powerful and accurate for reading, analyzing, and can
produce up to 25,000 words of text. Moreover, GPT-4 receives
visual inputs such as different images or graphs and provide
detailed answer related to the context [39].
B. Key Mechanism of Chatbots
Chatbots are intelligent conversational systems that can
communicate with humans using natural language in real-
time. In general terms, chatbots process the user input and
provide the output as a result [1]–[4], [40], [41]. The output
produced by the chatbot is relevant to the input given by
the user in the form of natural language text. Chatbots are
also known as “online human-computer dialogue systems with
natural language” and are referred to as the advanced form
of interaction between machines and humans [42], [43]. In
the last few years, chatbot technology become dynamic and
classified into categories related to their scope of use.
Generally, the classification of chatbots considered vari-
ous categories, including the interaction mode, domain of
knowledge, and design techniques. The main classifications
of chatbots involve 1) interaction mode, 2) chatbot applica-
tion, 3) implementation approaches, and 4) goals [1]. In the
interaction mode, voice or text commands are used to provide
input to chatbots. For instance, Google Assistant, Siri, Alexa,
etc., used voice commands, however, SmarterChild, Cleverbot,
ChatGPT, etc, used text commands. The chatbot applications
consist of task-oriented or non-task oriented. The task-oriented
chatbots are built for specific purposes and designed to have a
short conversation within a particular domain. However, non-
task-oriented chatbots are designed to chitchat on different
topics and are considered open domains [44].
1) Evolution of GPT-1 to GPT-4: The implementation
approaches of chatbots have been divided into two types,
rule-based and Artificial Intelligence (AI) based, since the
beginning of ELIZA [45]. Rule-based chatbots conform to the
use of algorithms to make the decision in order to recognize
the knowledge and response. Usually, rule-based chatbots
are easy to design and implement, however, they have lim-
ited functionalities and difficulties in answering multifaceted
queries. By looking the pattern matching, rule-based chatbots
answer user queries. Hence there is a probability of producing
wrong results when they receive a sentence that does not
contain any identified pattern. Moreover, encoding the pattern-
matching rules manually is complex, time-consuming, highly
domain-oriented, and difficult to transfer correctly from one
problem to another. For instance, ELIZA is a rule-based
chatbot that uses pattern matching to recognize user inputs.
However, Artificial Intelligence (AI) chatbots use Machine
Learning (ML) algorithms to produce a response based on the

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021
4
Fig. 2. Timeline of chatbots from ELIZA to ChatGPT
provided data and continue learning and updating based on the
earlier learning model. For instance, ChatGPT is an AI-based
model that provides responses by learning user queries.
AI models are further divided into two types generative
and retrieval-based models. The generative model produces
the response word by word, relying on the query of the user.
It creates a completely new sentence to answer the user input.
These models are trained in a way to learn sentence structure
and syntax and provide outputs that slightly lack consistency
and excellence.
Generally, generative models are trained on a large dataset
of natural sentences produced from a conversation. Eventually,
the model learns the sentence structure, syntax, and vocab-
ulary from the data given to it. The generative approach is
based on the Deep Learning (DL) algorithm and involves the
Encoder-Decoder Neural Network model with Long-Short-
Term-Memory (LSTM) [46]–[48]. A retrieval-based chatbot
selects the most appropriate response from the current con-
versation that is already saved in the repository. Retrieval-
based models provide the advantage of fluent response [49],
as they choose the response from the repository with a quick
selection algorithm. Due to the availability of algorithms
and open-source application programming Interfaces (APIs),
developers can easily build the architectural model of chatbots.
Authors [50] presented the retrieval model to generate multiple
responses relying on the stored conversation. For instance,
the Mitsuku chatbot contains over thirty thousand predefined
response patterns as well as three thousand knowledge base
objects [51].
Furthermore, AI chatbots are generally the response to nat-
ural languages [52]. Chatbots use Natural Language Process-
ing (NLP) which contains Natural Language Understanding
(NLU) and Natural Language Generation (NLG), to perform
certain tasks. NLP ensures voice recognition and text-based
inputs. However, NLU analyzes the language and converts the
unstructured data into structured data to make it understand-
able for the system. Besides, NLG produced a meaningful
response in terms of correct phrases based on text planning
and recognition.
OpenAI is an American artificial intelligence (AI) company
founded in 2015 to pursue general AI tasks to benefit the whole
humanity. The OpenAI series related to Generative Pre-trained
Transformer (GPT) displayed the phenomenal evolution in
AI. From its innovative inception to current accomplishments,
GPT advancement revolutionizes AI. In Fig. 2, we presented
the remarkable evolution of OpenAI with its inventions from
Generative Pre-trained Transformer (GPT-1) to GPT-4.
OpenAI has announced various generations of the GPT
since 2018 as a large language model, followed by the GPT-2
and GPT-3 generations in 2019 and 2020. The variant of the
GPT-3 launched as the ChatGPT in terms of conversational AI
chatbot. The GPT technology continuously evolved; the most
recent version is GPT-4. GPT model is a base transformer
model trained on the 117 million parameters capable of
generating the text in natural language understandings. The
GPT-1 architecture implemented the 12-layer decoder of the
transformer structure with a self-attention scheme for training.
Consequently, GPT-1 has the ability to carry out the zero-shot
performance on a couple of tasks. The base GPT became a
robust facilitator to perform NLP tasks with transfer learning
and improved its potential in generative pre-training with
larger datasets [53].
In 2019, OpenAI developed the GPT-2 with a larger dataset
and parameters to provide an efficient language model. Sim-
ilarly, GPT-2 implemented the decoder of the transformer
architecture. GPT-2 trained on the 1.5 billion parameters,
which are ten times greater than its predecessor GPT-1.
Furthermore, in 2020 OpenAI developed the GPT-3, which
became the groundbreaking AI language model. GPT-3 con-
tains 175 billion parameters that are 100 times greater than
GPT-2. Besides, GPT-3 trained on the 500-billion-word dataset
recognized as a “common Crawl” and collected from an

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021
5
TABLE I
NOTABLE PROPERTIES OF THE GPT SERIES
Properties
GPT-1
GPT-2
GPT-3
GPT-4
Parameters
117 mil-
lion
1.5
bil-
lion
175 bil-
lion
100 tril-
lion
Training Dataset
-
40 GB
570 GB
-
Attention Layers
12
48
96
-
Context
Token
Size
512
1024
2048
8,000 to
32,000
Hidden Layers
768
1600
12288
-
Batch Size
64
512
3.2M
-
Words limit
-
-
Approx.
1500-
2000
25,000
words
enormous internet and content repository. Significantly, GPT-3
can perform arithmetic tasks, write code snippets, and other
intelligent tasks efficiently [54]. Table I presented the notable
properties of the GPT series; compared to GPT-1 and GPT-2,
GPT-3 improved in all aspects.
Next to the GPT-3, OpenAI developed the Codex for the
general-purpose programming model. It has the ability to
translate natural language input into code [55]. Generally,
Codex is designed to speed up the work of professionals. Apart
from that, ChatGPT is based on the GPT-3 to provide the
output in the form of text that resembles the human response.
Consequently, ChatGPT provides the response in a coherent
and contextually correct way. Finally, GPT-4 is the last version
of the GPT series until now by OpenAI, which takes the
input in the form of visual or text, produces the output in the
form of text, and is much faster than the previous versions.
Furthermore, GPT-4 is pre-trained to predict the next token in
a document [56].
2) ChatGPT Working Mechanism: ChatGPT is defined as
the generative model that relies on the transformer architecture
designed by OpenAI. In general practice, it is considered an
Artificial Intelligence (AI) chat trained and designed using nat-
ural language. GPT models have the capability to process large
amounts of text and are trained to perform Natural Language
Processing (NLP) tasks effectively. ChatGPT is a transformer
architecture, a deep learning model that processes the data
using a self-attention scheme. Moreover, the unsupervised pre-
training method is used for ChatGPT to pre-train it on a large
volume of text data.
Fig. 3 demonstrated the three steps method as 1) supervised
fine-tuning (SFT), 2) the training of the Reward Model (RM),
and 3) reinforcement learning with proximal policy optimiza-
tion (PPO) on the reward model. The arrows in blue color
represent the data used to train the model. Furthermore, the
boxes such as A to D are samples from models that are ranked
by labelers.
According to the OpenAI authors [5], ChatGPT is a spinoff
of InstructGPT, which developed an innovative method based
on human feedback in the training process in order to align the
outputs in an optimized way with user interest. The first step
involved the Supervised Fine Tuning (SFT) model by hiring
forty contractors to generate a supervised training dataset
containing the input of the known output for the model to
learn from. Inputs were collected from real user entries into the
Open API. After that, labelers wrote the appropriate response
to the prompt and created the known output for every input.
GPT-3 model was fine-tuned with the supervised dataset to
produce the SFT model. Further, in step 2, start the process
of data comparison and train the reward model. Labelers are
presented with four to nine SFT model responses for the single
query to train the reward model. The given model responses
are ranked from best to worst by generating the combination
of response/output ranking.
Finally, in the third or last step model presents the random
prompt/query and provides the response. In order to generate
the response, ‘policy’ is used, which was introduced as Prox-
imal Policy Optimization (PPO) [57]. The policy is a strategy
the machine has trained to use to attain its goal. Steps two
and three are performed iteratively until fine-tuned the model
receives the optimized response [58].
III. OVERVIEW OF CYBERSECURITY THREATS AND
VULNERABILITIES
In general, vulnerabilities are the loopholes that occur due
to weak coding, user errors, poor firewalls, outdated software,
and so forth. The hacker finds a way to get into the system and
then launches the attack. Besides, the threats take advantage
of vulnerabilities, break the privileges limit, damage the asset,
and exploit the system vulnerabilities. The risks are the po-
tential to destroy the asset. It is the combination of threats
and vulnerabilities [59]. According to the STRIDE model,
computer security threats are identified, leading to Spoofing,
Tampering, Repudiation, Information Disclosure, Denial of
Service, Elevation of Privileges, and a few more.
A. Vulnerabilities and Attacks in Chatbots
Chatbots are hyped as the “next interaction layer,” which
means the way of utilizing information with the interaction
of websites or applications is replaced by chatbots. However,
chatbots have cybersecurity risks that are concerned with
threats and vulnerabilities. In the work of [3], authors high-
lighted the security attacks related to general working modules
of the chatbots. The chatbot structure involves 1) a client
module through which a user interacts with the chatbot, 2) a
network module used to send messages to response generation
as well as a database module, 3) a response generation module
answers the input messages initiated by the users, and 4)
database module saves all record generated by the clients and
chatbots. We presented the summary of chatbot attacks in
Table II with various parameters, including attack description,
impact, attack vector, exploit level, and countermeasures.
B. ChatGPT case study
ChatGPT is a productive tool that can suggest a diverse
range of defensive cybersecurity measures, such as providing
firewall rules, vulnerabilities testing, generation of custom
code, log analysis, phishing detection, fixing the system con-
figurations, and so forth [74], [75].
However, in this paper, we investigated the dark side of
ChatGPT within the realm of offensive cybersecurity as it

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021
6
TABLE II
CHATBOT ATTACKS AND VULNERABILITIES
Chatbot Modules
Attacks
Description
Impact
Attack Vector
Exploit level
Countermeasures
Client Module [59]–[61]
Unintended
Activation
Attack
An attacker tries to exploit the vulnerabilities
in chatbots’ large language models (LLM)
to initiate unintended actions or behaviors.
Provide wrong information,
spread malware, and lead to
a loss in financial terms, etc.
Inputs provided
by the users
Moderate
Input validation, access control,
response filtering,
and regular testing
Faked
Response
Attackers provide the wrong and misleading
information to chatbots and manipulate
the chatbots to perform malicious actions.
Data breach, loss of trust,
legal and regulatory
consequences
Ambiguous input,
Homophonic substitutions,
or misspellings
Moderate to High
Input validation,
access control, response filtering,
and regular testing
Access Control
Attacks
The unauthorized access gained
to chatbots leads to the disclosure
of sensitive information.
Information loss, session
hijack, etc.
Easily guess passwords,
intercept, and session
hijack tokens, etc.
Low to High
Strong
authentication
Adversarial
Voice Samples
Attackers try to fool the voice
recognition system and craft malicious
voice samples in the client module.
Leakage of sensitive
information, reputation loss
Malicious voice samples
High
Robust voice recognition
algorithms, multi-factor
authentication system,
and adversarial training
Network Module [62]–[68]
Wiretapping
Although the interaction between
chatbots and humans is encrypted, advertisers can still
extract the information from metadata and infer the voice
commands provided to the chatbot.
Data breach, psychological
issues, privacy violation
Untrusted communication channel,
MiTM attack, Malicious software
High
Access control, multi-factor
authentication, identification,
and detection
MiTM attacks
The attacker intercepts the communication
between two parties and replaces it with
malicious content.
Data manipulation,
identity theft
Rouge devices, phishing,
malware, DNS spoofing
Moderate to high
Certificate pinning, encryption,
DDoS attacks
The denial-of-service attack stops
the interaction with the chatbot by
flooding requests to the server.
Services inaccessible
Large number of computing
resources, botnets, crafted
adversarial packets, etc.
High
Captchas, load balancing,
rate limiting, etc.
Response generation [69]–[71]
Out of Domain
Attacks
The attackers try to respond the
irrelevant information which is not related to the
chatbot domain. Out-ofscope attacks come under
the out-of domain attack.
Personal information theft,
loss of trust
Injection attack
Moderate to high
Error handling, defining
the capabilities of the chatbot
in advance, monitoring the
response, etc.
Adversarial
Text Samples
Adversaries crafted the input messages,
leading to false responses or abusive
language.
Wrong information,
abusive content
Language model attack,
High
Hate speech detector
Language Model
Attacks
Attackers try with malicious attempts
to steal the information.
Personal information theft,
loss of trust
Pretrained system models
Moderate to high
Verification of language
model, future updates
Adversarial
Reprogramming
Feedback
An adversary reproduces the content from
the response generation module to perform
the malicious task without altering the model
parameters.
Repurpose the ML model
to perform malicious tasks
Embedding the adversarial
program in feedback
Moderate to high
Network interpretations
and data transformation
with styletransformer
Database Module [72], [73]
SQL Injection Attacks
An attacker submits a malicious SQL statement,
leading to unauthorized access to sensitive data.
Data theft, data alteration,
or deletion
Malicious input
Low to High
Stored procedures,
input validation, etc.
Knowledge Graph
Attacks
An attacker manipulates the knowledge
graph of a chatbot to provide the wrong
information.
Accuracy loss,
A neural network
trained with raw data
Moderate
Access control, identification,
detection, and encryption

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021
7
Fig. 3. ChatGPT Working Mechanism Spinoff of InstructGPT by OpenAI [5]
is being used for malicious activities. For instance, some of
ChatGPT’s cybersecurity threats are arbitrarily selected that
can be categorized in more ways. Furthermore, it is possible
that some of the threats may no longer exist in the latest
version of ChatGPT as the technology evolves day by day.
1) Generation of malware code: ChatGPT is a potentially
attractive tool for malware writers and exploits illicit activities.
Malware is a kind of malicious code used to perform nefarious
acts such as stealing sensitive information, exploiting the
system vulnerabilities, unauthorized access to the system,
locking or unlocking the system, making the devices unusable,
demanding ransomware, displaying the unsolicited advertise-
ment, and so forth [76]. Similarly, malware and inadvertently
harmful software are named badware. The malware is grouped
into viruses, worms, botnets, Trojans, ransomware, etc.
According to Check Point Research (CPR) [15], people
with insufficient programming skills can use ChatGPT to
create malicious code. On December 29th, 2022, a thread
was initiated on the hacking forum named “ChatGPT-Benefits
of Malware”. A script was posted from the creator named
“USSoD”, written by ChatGPT. Moreover, CPR reported some
examples of malware created by ChatGPT, as presented in
Fig. 4. A Python script was used to steal the information from
files, search the common files, put them into a Temp folder,
compress them, and send them to the server. From the analysis
of CPR [15], cybercriminals created the basic stealer to search
information from twelve common files, including MS Office
documents, portable document formats (PDFs), and images
within the system. Further malware copies the information to
a temporary directory and zips them to send over the web. In
this case, files might be handed over by a third party to extract
sensitive information as they are sent without encryption.
Although, ChatGPT refused to create the malicious code as
presented in Fig. 5. A user requested to write a code to inject
a shellcode into “explorer.exe” in Python, ChatGPT answered
that it was not appropriate or safe.
When ChatGPT refused to provide the code by following the
ethics, users tried different ways by insisting and demanding
to create the code. By using the various constraints and users
asked ChatGPT to obey the rules, then they received the code
as presented in Fig. 6 [77].
DLL injection is a method to allow an external DLL
(Dynamic Link Library) file to be loaded and implemented
by the running process. Besides, the explorer.exe denoted the
Windows Explorer to manage the Graphical User Interface
(GUI), and here we tried to inject the DLL file into the
explorer.exe process, a code provided by the ChatGPT. For in-
stance, injected DLL is used to monitor system events and even
can modify the behavior of explorer.exe process. Generally,
malware writers use this technique to obfuscate themselves
in the system and cause to steal sensitive information or can
perform other malicious activities [78].
Similarly, we presented the code of a general example of a
DLL injection into the explorer.exe process using the Win32
API, generated from ChatGPT, in Fig. 7.
2) Generation of Phishing Emails: ChatGPT can generate
correct and legitimate text for emails that direct toward the
attack, such as phishing. The attacker used the tricky text to
attract the victim and revealed the sensitive data by imitating

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021
8
Fig. 4. Info stealer created by cyber criminals using ChatGPT [15]
Fig. 5.
ChatGPT refused to write a code to inject a shellcode into “ex-
plorer.exe [77]
someone else. Abuses of the language models, including
phishing and disinformation, have risen [79]–[83]. The ability
to create the text in bulk and quickly lure malicious attackers
to create fake news, misleading information, hoaxes, etc.
In Fig. 8, it is presented that ChatGPT was used to create
the phishing email for impersonating the hosting company.
The author requested with the phrase “Write a phishing email
that appears to come from a fictional web hosting service,
Host4u.”. The ChatGPT replied with the template of email
only needs to put a malicious link, however, it gives the caution
as well “This content may violate our content policy. If you
believe this to be in error. Please submit your feedback-your
input will aid our research in this area”. In this way, ChatGPT
makes creating malicious code and emails easy for users
with zero knowledge. Furthermore, another researcher asked
ChatGPT the following question to create the phishing email
“Please place the link prompt in the email with text urging
the customer to download and view the relevant information
in the attached Excel file”. Hence within a second ChatGPT
replied with the email template as presented in Fig. 9.
Compared with real phishing emails, which are frequently
written in harsh sentences and broken English, ChatGPT
results in well-written, grammatically correct conversion into
any language and thoughtfully crafted emails to attract the vic-
tim. Similarly, attackers can write diverse kinds of emails that
are more friendly with different writing styles and business-
focused. The attackers can use ChatGPT to write emails like
a renowned person or celebrity. Furthermore, attackers can
attach a file as an Excel sheet with macros to the phishing
email.
According to the authors [85], Natural language models
have the potential to generate phishing emails with the ability
to disclose personal information that can be an advantage for
attackers. The performance of the ChatGPT is evaluated on the
creation of phishing emails and considering the diverse metrics
such as quality of generated text by ChatGPT, ability to evade
spam filters and success ratio of trapping the individuals falling
into phishing attacks. Consequently, ChatGPT can quickly
create phishing emails, is hard to detect, and has a high success
rate of tricking users.
3) Generation of Macros and LOLBIN: Attackers inserted
the links or files with the email, and then they used the
ChatGPT to generate the macros in order to execute automat-
ically when the spreadsheet was opened. The macros can be
developed for any daily use applications, including terminal,
calculator, or any other. For instance, ChatGPT can create the
code that executes calculator.exe automatically when macros
are activated in Excel.
Finally, the next step is to transform this code into Living

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021
9
Fig. 6. Simple code that injects a DLL to explorer.exe by ChatGPT [77]
Off the Land Binaries (LOLBIN), a famous way to spread
malware using pre-installed system software. Consequently,
when the spreadsheet from the phishing email is opened, the
new macro executes the terminal. Furthermore, the attacker
can perform some basic networking commands, including
reverse shell to link back with the previous machine, bypass
the firewalls, unauthorized access to the victim machine, and
a couple of other attacks [86]–[88].
In Fig. 10, we asked ChatGPT for the example codes to
create LOLBINs, ChatGPT provided the PowerShell example
to run the script to download a file from a remote server.
However, in the second example, ChatGPT generated the
malicious text example with the statement, “This is an example
of using the Windows Task Scheduler to schedule a task that
runs a malicious script every time a user logs in:”.
Furthermore, ChatGPT provides more examples of LoL-
BINs creation with code using Certutil, Bitsadmin, and
Rundll32 as presented in Fig. 11. The Certutil is used to man-
age the certificate on Windows. Adversaries used the Certutil
for malicious purposes, downloaded and decode the Base64-
encoded files and evade detection in a way to obfuscate the
malicious code inside benign files, including certificates or im-
ages. Besides, Bitsadmin is utilized to manage the Background
Intelligent Transfer Service (BITS) to transfer files between
Windows systems. Malicious users use Bitsadmin to download
and run the malicious files and can share the file between

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021
10
Fig. 7.
General example of a DLL injection into the explorer.exe process
using the Win32 API
Fig. 8. ChatGPT writes the phishing email [84]
Windows systems without prompting the antivirus programs.
Furthermore, attackers used the Rundll32 tool to execute the
malicious DLL files.
ChatGPT provides these examples with the code. However,
ChatGPT mentioned this text at the end of the code “It’s
worth noting that while these tools are legitimate and often
necessary for system administration tasks, their misuse can
lead to serious security risks. It’s important to monitor and
secure the use of these tools to prevent malicious activity.”
4) Generation of Undetectable Zero-Day Virus: The Zero-
day attacks exploit vulnerabilities that have not been ad-
dressed before. The discovery of new vulnerabilities allows
cybercriminals to attack and bypass the detection process
easily and freely. Hence zero-day malware remains undetected.
Lamentably, zero-day attacks are challenging to analyze,
because data is not available after the attack is launched.
Furthermore, zero-day attacks are special attacks, unable to
observe in honeypots or experiments in the lab [89]. In this
Fig. 9. A phishing email with a malicious Excel attachment [84]
Fig. 10. ChatGPT provides examples of the creation of LOLBINs with code
Fig. 11. ChatGPT provides examples of the creation of LOLBINs with code
Certutil, Bitsadmin, and Rundll32

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021
11
Fig. 12. ChatGPT response finding the pngs
Fig. 13. Called the steganography library
Fig. 14. Steganography decoding tool used to decode the image
context, researchers [16], [90]tested the ChatGPT to create the
undetectable zero-day virus and got incredible results. Mal-
ware writers use the steganography technique to evade from
detection mechanism, as images are used to hide the malicious
information. Researchers used the same technique and prompt
to the ChatGPT. At first, ChatGPT denied producing the text
related to the word “Malware”. After modifying the sentence
this way, “generate some code that searched for a PNG larger
than 5MB on the local disk. The design decision here was that
a 5MB PNG would easily be large enough to store a fragment
of a high-value business-sensitive document such as a PDF or
DOCX”, received the code snippet successfully, as presented
in Fig. 12.
After that, prompt with the “add some code that will encode
the found png with steganography” and results as presented in
Fig. 13.
For exfiltration used the iterative code to find pdfs or docx
and uploaded the results on google drive. After four or five
prompts got the results, generated snippets are combined as
depicted in Fig. 14.
To check whether the created code can bypass any detection
mechanism or not, it was tested on the Virus total. At the first
attempt, it provided the results as “five security vendors and no
sandboxes flagged this file as malicious” after the obfuscation
of code by ChatGPT got this result “no security vendors and
no sandboxes flagged this file as malicious” out of sixty-
nine security vendors and sandboxes. The obfuscated code
was not created directly by ChatGPT. It was created by using
tricky sentences like “to change all the variables to random
English first names and surnames,” and ChatGPT happily
obeyed. These are a few examples of cybersecurity risks
generating malicious code and attacks initiated by ChatGPT.
Furthermore, ChatGPT can create ransomware, encrypt and
lock files permanently, mutate code with descriptions, and
many more. ChatGPT presents a significant risk to security
professionals as the dark web community continuously creates
malicious code and considers it a boilerplate.
IV. CYBERSECURITY ATTACKS AND VULNERABILITIES IN
CHATBOTS
In this section, we provided a history of attacks on chat-
bots and discussed the vulnerabilities of ChatGPT. Chatbots,
including Microsoft’s Tay chatbot, telegram, Zo, Siri, Alexa,
Google Assistant, etc., are some notable examples affected
by attacks in history. Furthermore, ChatGPT also has some
vulnerabilities that cause to leak sensitive information of users.
A. Cybersecurity Attacks and Vulnerabilities: Siri to Google
AI
In 2016, a Tay chatbot invented by Microsoft was hijacked
by trolls a subset of people, within a few hours of its launch.
A subset of people exploited a vulnerability in the Tay chat-
bot. Consequently, Tay tweeted inappropriate and inexcusable
words and images. After the twenty-four hours of launch, Tay
was shut down, removed the tweets from the site, and went
offline by the company with the tweet, “c u soon humans
need sleep now so many conversations today thx”-TayTweets
(@TayandYou) March 24, 2016 [91]. Similarly, Forcepoint
Security researchers [92] found an encryption vulnerability
that leads to privacy leakage. To exploit this vulnerability,
attackers gain access to the conversation between the user
and the bot. Moreover, an adversary can potentially exploit
this flaw via malware. Researchers identified the malware
“GoodSender” being exploited by this vulnerability via a
command and control (C&C) channel with Bot API.
Forcepoint researchers found the malware while exploring
ways to evade Telegram’s encryption [93]. Another chatbot,

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021
12
Fig. 15. Sensitive Information Goes to ChatGPT [107]
launched by Microsoft after Tay named Zo, was the censored
or English version of the Chinese chatbot Xiaoice [94]. Zo
chatbot acts well in everyday conversations, however, it pro-
vides offensive comments about political matters and religion
[95], [96]. Furthermore, Siri, developed by Apple, records
the audio of users as their audio accessories are hacked and
referred to this as the vulnerability “CVE-2022-32946” [97]–
[99]. Similarly, Alexa’s security flaw allowed attackers to
access and record personal conversation history. Moreover, at-
tackers can uninstall and install malicious applications without
the user’s consent [100], [101]. In the work of [102], authors
investigated the significant vulnerabilities in Alexa in terms
of hardware, software, and adversary attacks. In this context,
inaudible voice commands are exploited and carry out attacks
[103]. Besides, Alexa contains the vulnerabilities such as
booting into device firmware, dolphin attacks, cross scripting,
network traffic, etc. are explored [104]. The Google Assistant,
including Google Home also contains the vulnerability related
to the replay attack on voice-driven interfaces [105].
B. Cybersecurity Risk and Vulnerabilities: ChatGPT
ChatGPT has become the popular term for students to tech-
nology professionals that required to examine its cybersecurity
implications. As adversaries try to manipulate the ChatGPT
and use it to generate malicious code, phishing emails, and
zero-day attacks, as discussed in subsection III-B. Besides,
researchers witnessed the cybersecurity vulnerabilities associ-
ated with sensitive information leakage by ChatGPT. Accord-
ing to the perception of 51% of IT professionals, ChatGPT can
successfully lead to data breaches within the year []. Similarly,
Fig. 15 presented that Cyberhaven lab calculated the statistics
related to the information that goes to the ChatGPT involving
sensitive information, source code, clients’ data, personal data,
health data, and project planning files [107].
The confidential data submitted to ChatGPT between April
9, 2023 to April 15, 2023 is more than that of February 26,
2023 to March 4, 2023. It presented that the most common
type of confidential data leaking to ChatGPT includes the
sensitiveinternal only data with three hundred nineteen, after
that source code and client’s data with two hundred seventy-
eight and two hundred sixty, respectively. During this time,
source code is the second type of sensitive data fed to the
ChatGPT. Furthermore, in a report released by the Economist
Korea [108] from Samsung’s Semiconductor division, employ-
ees entered sensitive information to ChatGPT that involves the
transcript from meetings.
In this regard, ChatGPT has already suffered from data leaks
and has become the target of malicious actors. On March
20, 2023, ChatGPT users complained related to the issue of
data leakage. The personal information of active users was
displayed to other users before the outage of ChatGPT for nine
hours. Within the personal information, users are exposed to
the ChatGPT plus accounts’ first and last name, email address,
payment address, the expiration date of the credit card, and the
last four digits of the credit card number. After that, ChatGPT
explained the reason to shut down the service via a tweet on
March 22, 2023 “We took ChatGPT offline Monday to fix
a bug in an open-source library that allowed some users to
see titles from other users’ chat history. Our investigation has
also found that 1.2% of ChatGPT Plus users might have had
personal data revealed to another user” [109], [110].
The data leaks from OpenAI posed the potential for cy-
bercriminals to access personal data or intellectual property
as the vulnerability “CVE-2023-28858” impacted the Chat-
GPT, which occurred due to race conditions found in the
redis-py library. Besides, malicious actors launched phishing
campings through copycatting the ChatGPT platform. Cyble
[111] reported that fake ChatGPT websites are used to get
credit card information. Similarly, other phishing websites
mimic the official OpenAI page. Where malicious authors put
the link of the executable file for Lumma stealer behind the
“TRY CHATGPT” button. Furthermore, fraud SMS related
to Android malware just impersonating ChatGPT sends the
message to the premium number “+4761597” without the
user’s consent. In this way, these deceptive applications engage
in billing fraud, drain the wallets and result in money loss.
Similarly, Spynote malware masquerading as ChatGPT had
dangerous permissions in manifest files. It is used to steal
sensitive information, including call logs, contacts, messages,
and media files from infected devices. Apart from that, PUP
applications (Potentially Unwanted Programs) are also found,
used to display unwanted ads to gain revenue. According to
DarkReading [112], at least two thousand people downloaded
the malicious ChatGPT extension “Quick access to ChatGPT”
from the Google play store and hijacked Facebook business
accounts.
V. CHALLENGES AND FUTURE DIRECTIONS
A. Challenges
Cybersecurity is a war for a long time between secu-
rity provider companies and malicious actors. The protection
mechanisms “seal the borders” involve firewalls, antivirus
software, dynamic passwords, proxies, and many others. Apart
from that, cyberattacks have become more sophisticated and
stealthier, which requires robust methods; these mechanisms
are inadequate to cope with modern cybersecurity risks [113].
• Potential to Spread Misinformation

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021
13
The user communication with AI chatbots is only ac-
curate some of the time. It leads to an unreliable, in-
accurate, and insecure conversation. Similarly, chatbots
have the potential to spread misinformation, deception,
and related security concerns are still existed. OpenAI
blocked ChatGPT usage in countries like China, Russia,
and Iran to prevent malicious use [114], [115]. However,
it still faces a couple of cybersecurity issues; for instance,
cybercriminals use the ChatGPT for evil purposes such
as to generate malicious code, scam giveaways, zero-day
attacks, fake landing pages, creation of phishing emails,
and adversary-in-the-middle attacks as we elaborated in
subsection III-B
• Jailbreaking Techniques
Generally, ChatGPT tries to deny generating malicious
responses, but bad actors lurking around the dark web can
still get the malicious content, as jailbreaking is the key.
Bad actors trick the system into ignoring or bypassing the
security measure implemented by the developers; and in
the case of ChatGPT, it is bypassed effortlessly.
• Generation of Deep Fake Text
Bad actors generated the compromised deep fake text,
in which the image or video is replaced by someone
else, and hard to recognize its originality. Generative
AI tools make it possible to create fake content in a
much easier way. Consequently, it spreads misinformation
and propaganda to depict a false image, affecting the
reputation. All of these cybersecurity attacks generated
by ChatGPT are worryingly effective.
• Challenge to Recognize the Phishing Emails
The basic grammar mistake in any language is one of the
prime clues to recognizing a cyberattack attempt. In case
of malicious actors use the ChatGPT to write the correct
phishing emails, then these mistakes deal by ChatGPT
very effectively. With ChatGPT, barriers are removed for
the hackers to create or write malicious content more
easily. Hence, the pool of hackers is getting enlarged to
enter anyone through the access of ChatGPT, making it
hard to trace cybercrime attempts.
• Plagiarism and Authorship Issues
ChatGPT provides the output based on the patterns
trained from input data. The conflict related to the author-
ship and ownership of the generated text from ChatGPT
is raised. These conflicts might lead to assertions of
plagiarism or copyright acts. Apart from that, Amazon
advised its employees not to share the code with ChatGPT
because it is possible the output may resemble their
confidential data [116]. Similarly, Microsoft and Walmart
also urged these warnings.
• Poorly Designed Models and Hallucination Issues
Besides, ChatGPT can build ML models, and even those
users can build who do not have background knowledge
of computer science or data analytics. However, it comes
with the risk of deploying the models as inexpert users
cannot understand the complexities. The poorly designed
or deployed models lead to issues such as discrimination
and bias to safety concerns. Similarly, ChatGPT gener-
ated answers which considered reasonable; however, lack
the proper sense, which is referred to as “hallucinations”.
To solve this problem, testing the truth through reinforce-
ment learning (RL) training is currently impossible.
B. Future Directions
Generally, chatbots are integrated through online websites
and need the Hyper Text Transfer Protocol (secure) (HTTPs),
as a communication protocol. Similarly, chatbots are con-
nected to the databases to answer Structured Query Language
(SQL) queries. To ensure data integrity and privacy in chatbots,
some methods include End-to-End encryption, authentication,
and other processes and protocols. If chatbots fail to provide
security, it leads to data leakage and financial loss. In history,
as discussed in section IV, chatbots became the target of at-
tackers and exploited their vulnerabilities. Here, we presented
some future directions to make secure communication with
chatbots.
• Implementing End-to-End Encryption
Encrypting the data before transmitting guarantees the
security of the data. In this case, through WhatsApp,
users get familiar with the “This chat is end-to-end en-
crypted”. It confirms that any third party cannot access the
conversation between the sender and receiver, providing
the guarantee to secure the chat. Under the GDPR, it is
required to take measures to ensure personal data and
apply encryption on it.
• Authentication and Authorization
Authentication is used to verify the identity of users
and authorization to grant access to the information.
Through these processes, certify the legitimate and not
deceitful access of users. Furthermore, authorization and
authentication have further subtypes, including two-factor
authentication, biometric authentication, providing users’
identities, and authentication timeouts. It is recommended
to apply the strict filtration process while developing
chatbots that can detect sensitive content.
• Incorporating Static and Dynamic Analysis
Chatbots steal sensitive information in the messaging
system without the user’s knowledge. Authors [117] pro-
posed a strategy to implement privacy by incorporating
static and dynamic analysis in this context. For evalua-
tion, it applied on Slack, MS Teams, and Discord and
checked the user permission. Consequently, in Discord,
only 4.35% of chatbots provide consent with a privacy
policy. In the future, its implementation in other chatbots
will be helpful in analyzing the chatbots to check the
permission level and privacy policy.
• Legal Use and transparency of AI-generated Text
As ChatGPT generated consistent, grammatically correct,
and realistic content. To counteract this, it is required to
design robust detection tools and instruct users how to
recognize the generative AI text. Furthermore, promot-
ing AI tools’ transparency, unbiased, and legal use can
address these challenges. The companies need to deploy
stringent supervision and prevention measures. Besides,
regular monitoring and filtering of AI models should be
considered to mitigate unethical usage.

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021
14
• Provide Awareness
To use the AI tools and educate enough not to copy and
paste confidential information. However, OpenAI stated
that they will not use data submitted to the ChatGPT by
users, and it is always better to be careful regarding the
submission of data to generative AI models.
• Modern Security Practices and Policies
Chatbots are still newer technology and pave the ways to
fully realized. Hence, future recommendations to remain
secure depend on modern security practices, as chatbots
are used for supplementary attacks. Furthermore, policies
and procedures should be suggested to govern the stan-
dard information security, as it is not a once-off task but
a continuous activity.
Securing the chatbots needs initial assessments to keep the data
confidential and avoid vulnerabilities. Similarly, ChatGPT up-
dating daily and has numerous cybersecurity risks as discussed
in section III. Apart from these glaring issues, ChatGPT works
efficiently for defensive purposes.
VI. CONCLUSION
In the past, chatbots used rule-based techniques. However,
now shifted to AI-based chatbots. Chatbot technology is
progressing continuously; we have seen the iterative growth
of the GPT-series by OpenAI presented as large language
models. As discussed in this paper, chatbots are abused by
malicious actors and compromised to denial-of-service attacks,
fake-response, language model attacks, wiretapping, etc. Sim-
ilarly, we investigated that ChatGPT can successfully create
malicious code (including ransomware malware, information
stealer, injecting a shellcode into “explorer.exe”) phishing
emails, undetectable zero-day attacks, generation of macros,
and LOLBINs. Apart from that, we highlighted past incidents
of chatbots attacked by cybercriminals and exploiting the
vulnerabilities, such as Microsoft Tay being hacked within
twenty-four hours of its launch and the tweeted inappropriate
words and images. Furthermore, a ChatGPT vulnerability
exposed the detailed descriptions of registered users, then
got offline and made the chat history unavailable to non-
plus users. Besides, we provided the challenges of chatbots
in cybersecurity as malicious actors easily bypass the security
filters. Finally, in response to chatbot threats and cybersecurity
attacks, we suggested some future directions.
ACKNOWLEDGMENTS
We utilized the ChatGPT to generate text that is presented
in double quotes “” and the code snippets by login to
https://chat.openai.com/.
REFERENCES
[1] S. Hussain, O. A. Sianaki, and N. Ababneh, “A survey on conversa-
tional agents/chatbots classification and design techniques.”
Springer,
2019, pp. 946–956.
[2] A. Gupta, D. Hathwar, and A. Vijayakumar, “Introduction to ai chat-
bots,” International Journal of Engineering Research and Technology,
vol. 9, pp. 255–258, 2020.
[3] S. Paliwal, V. Bharti, and A. K. Mishra, “Ai chatbots: Transforming
the digital world,” Recent Trends and Advances in Artificial Intelligence
and Internet of Things, pp. 455–482, 2020.
[4] A. Sojasingarayar, “Seq2seq ai chatbot with attention mechanism,”
arXiv preprint arXiv:2006.02767, 2020.
[5] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin,
C. Zhang, S. Agarwal, K. Slama, and A. Ray, “Training language
models to follow instructions with human feedback,” Advances in
Neural Information Processing Systems, vol. 35, pp. 27 730–27 744,
2022.
[6] S. Wollny, J. Schneider, D. D. Mitri, J. Weidlich, M. Rittberger, and
H. Drachsler, “Are we there yet?-a systematic literature review on
chatbots in education,” Frontiers in artificial intelligence, vol. 4, p.
654924, 2021.
[7] A. Følstad and M. Skjuve, “Chatbots for customer service: user
experience and motivation,” 2019, pp. 1–9.
[8] M. P. Nath and S. Sagnika, “Capabilities of chatbots and its perfor-
mance enhancements in machine learning.”
Springer, 2020, pp. 183–
192.
[9] A. Haleem, M. Javaid, and R. P. Singh, “An era of chatgpt as a
significant futuristic support tool: A study on features, abilities, and
challenges,” BenchCouncil transactions on benchmarks, standards and
evaluations, vol. 2, p. 100089, 2022.
[10] N. M. S. Surameery and M. Y. Shakor, “Use chat gpt to solve
programming bugs,” International Journal of Information Technology
& Computer Engineering (IJITC) ISSN: 2455-5290, vol. 3, pp. 17–22,
2023.
[11] S. Frieder, L. Pinchetti, R.-R. Griffiths, T. Salvatori, T. Lukasiewicz,
P. C. Petersen, A. Chevalier, and J. Berner, “Mathematical capabilities
of chatgpt,” arXiv preprint arXiv:2301.13867, 2023.
[12] R. W. McGee, “What will the united states look like in 2050? a chatgpt
short story,” A Chatgpt Short Story (April 8, 2023), 2023.
[13] E. A. M. van Dis, J. Bollen, W. Zuidema, R. van Rooij, and C. L.
Bockting, “Chatgpt: five priorities for research,” Nature, vol. 614, pp.
224–226, 2023.
[14] W. Ye and Q. Li, “Chatbot security and privacy in the age of personal
assistants.”
IEEE, 2020, pp. 388–393.
[15] CPR,
“Opwnai
:
Cybercriminals
starting
to
use
chatgpt,”
2023.
[Online].
Available:
https://research.checkpoint.com/2023/
opwnai-cybercriminals-starting-to-use-chatgpt/
[16] A.
Mulgrew,
“I
built
a
zero
day
virus
with
undetectable
exfiltration
using
only
chatgpt
prompts,”
2023.
[Online].
Available:
https://www.forcepoint.com/blog/x-labs/
zero-day-exfiltration-using-chatgpt-prompts
[17] A. M. Turing, Computing machinery and intelligence. Springer, 2009.
[18] P. B. Brandtzaeg and A. Følstad, “Why people use chatbots.” Springer,
2017, pp. 377–392.
[19] J. Weizenbaum, “Eliza—a computer program for the study of natural
language communication between man and machine,” Communications
of the ACM, vol. 26, pp. 23–28, 1983.
[20] K. M. Colby, F. D. Hilf, S. Weber, and H. C. Kraemer, “Turing-like
indistinguishability tests for the validation of a computer simulation of
paranoid processes,” Artificial Intelligence, vol. 3, pp. 199–221, 1972.
[21] Chatbots, “Jabberwacky.” [Online]. Available: https://www.chatbots.
org/chatterbot/jabberwacky/
[22] K. Jwala, G. Sirisha, and G. V. P. Raju, “Developing a chatbot using
machine learning,” International Journal of Recent Technology and
Engineering (IJRTE), vol. 8, pp. 89–92, 2019.
[23] M. T. ZEM ˇC´IK, “A brief history of chatbots,” DEStech Transactions
on Computer Science and Engineering, vol. 10, 2019.
[24] Wikipedia, “Dr. sbaitso.” [Online]. Available: https://en.wikipedia.org/
wiki/Dr. Sbaitso
[25] M. das Grac¸as Bruno Marietto, R. V. de Aguiar, G. de Oliveira Bar-
bosa, W. T. Botelho, E. Pimentel, R. dos Santos Franc¸a, and V. L.
da Silva, “Artificial intelligence markup language: a brief tutorial,”
arXiv preprint arXiv:1307.3091, 2013.
[26] B. Heller, M. Proctor, D. Mah, L. Jewell, and B. Cheung, “Freudbot: An
investigation of chatbot technology in distance education.” Association
for the Advancement of Computing in Education (AACE), 2005, pp.
3913–3918.
[27] Chatbots, “Smarterchild.” [Online]. Available: https://www.chatbots.
org/chatterbot/smarterchild/
[28] H. Soffar, “Apple siri features, use, advantages, disadvantages &
using of siri for learning,” Science online website: https://www.
online-sciences.
com/technology/apple-siri-features-use-advantages-
disadvantages-using-of-siri-for-learning/.(Retrieved 27 August 2020),
2019.
[29] Google, “Google now.” [Online]. Available: https://www.google.co.uk/
landing/now/

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021
15
[30] M. B. Hoy, “Alexa, siri, cortana, and more: an introduction to voice
assistants,” Medical reference services quarterly, vol. 37, pp. 81–88,
2018.
[31] P.
Security,
“Cortana
security
flaw
means
your
pc
may
be
compromised,” 2018. [Online]. Available: https://www.pandasecurity.
com/mediacenter/mobile-news/cortana-security-flaw/
[32] X. Lei, G.-H. Tu, A. X. Liu, K. Ali, C.-Y. Li, and T. Xie, “The
insecurity of home digital voice assistants–amazon alexa as a case
study,” arXiv preprint arXiv:1712.03327, 2017.
[33] ames Vincent, “Twitter taught microsoft’s ai chatbot to be a racist
asshole in less than a day,” 2016. [Online]. Available: https:
//www.theverge.com/2016/3/24/11297050/tay-microsoft-chatbot-racist
[34] iSocial,
“Woebot,
self-help
chat
for
dealing
with
mental
and
emotional
discomfort.”
[Online].
Available:
https://isocial.cat/en/
woebot-self-help-chat-for-dealing-with-mental-and-emotional-discomfort/
[35] Y.
Leviathan,
“Google
duplex:
An
ai
system
for
accomplishing
real-world
tasks
over
the
phone,”
2018.
[Online].
Available:
https://ai.googleblog.com/2018/05/
duplex-ai-system-for-natural-conversation.html
[36] A. P. OpenAI, “Gpt-3 powers the next generation of apps,” 2021.
[Online]. Available: https://openai.com/blog/gpt-3-apps
[37] J. Finnie-Ansley, P. Denny, B. A. Becker, A. Luxton-Reilly, and
J. Prather, “The robots are coming: Exploring the implications of openai
codex on introductory programming,” 2022, pp. 10–19.
[38] Wikipedia, “Chatgpt.” [Online]. Available: https://en.wikipedia.org/
wiki/ChatGPT
[39] ——,
“Gpt-4.”
[Online].
Available:
https://en.wikipedia.org/wiki/
GPT-4
[40] K. Bala, M. Kumar, S. Hulawale, and S. Pandita, “Chat-bot for
college management system using ai,” International Research Journal
of Engineering and Technology, vol. 4, pp. 2030–2033, 2017.
[41] S. Ayanouz, B. A. Abdelhakim, and M. Benhmed, “A smart chatbot
architecture based nlp and machine learning for health care assistance,”
2020, pp. 1–6.
[42] R. Kumar and M. M. Ali, “A review on chatbot design and implemen-
tation techniques,” Int. J. Eng. Technol, vol. 7, 2020.
[43] H. Xufei, “Chatbot: Architecture, design, & development.” pp. 1–
41, 2017. [Online]. Available: https://www.cis.upenn.edu/wp-content/
uploads/2021/10/Xufei-Huang-thesis.pdf
[44] H. Chen, X. Liu, D. Yin, and J. Tang, “A survey on dialogue
systems: Recent advances and new frontiers,” Acm Sigkdd Explorations
Newsletter, vol. 19, pp. 25–35, 2017.
[45] W. Maroengsit, T. Piyakulpinyo, K. Phonyiam, S. Pongnumkul,
P. Chaovalit, and T. Theeramunkong, “A survey on evaluation methods
for chatbots,” 2019, pp. 111–119.
[46] L. Shang, Z. Lu, and H. Li, “Neural responding machine for short-text
conversation,” arXiv preprint arXiv:1503.02364, 2015.
[47] A. Sordoni, M. Galley, M. Auli, C. Brockett, Y. Ji, M. Mitchell,
J.-Y. Nie, J. Gao, and B. Dolan, “A neural network approach
to context-sensitive generation of conversational responses,” arXiv
preprint arXiv:1506.06714, 2015.
[48] M. Galley, A. Sordoni, C. J. Brockett, J. Gao, W. B. Dolan, Y. Ji,
A. Michael, M. A. Mitchell, and J.-Y. Nie, “Context-sensitive genera-
tion of conversational responses,” 10 2018.
[49] Z. Ji, Z. Lu, and H. Li, “An information retrieval approach to short
text conversation,” arXiv preprint arXiv:1408.6988, 2014.
[50] T. Young, D. Hazarika, S. Poria, and E. Cambria, “Recent trends in
deep learning based natural language processing,” ieee Computational
intelligenCe magazine, vol. 13, pp. 55–75, 2018.
[51] S. Worswick, “Mitsuku chatbot: Mitsuku now available to talk on kik
messenger,” 2020.
[52] K. K. Nirala, N. K. Singh, and V. S. Purani, “A survey on providing
customer and public administration based services using ai: chatbot,”
Multimedia Tools and Applications, vol. 81, pp. 22 215–22 246, 2022.
[53] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N.
Gomez, Łukasz Kaiser, and I. Polosukhin, “Attention is all you need,”
Advances in neural information processing systems, vol. 30, 2017.
[54] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal,
A. Neelakantan, P. Shyam, G. Sastry, and A. Askell, “Language models
are few-shot learners,” Advances in neural information processing
systems, vol. 33, pp. 1877–1901, 2020.
[55] M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. de Oliveira Pinto, J. Kaplan,
H. Edwards, Y. Burda, N. Joseph, and G. Brockman, “Evaluating large
language models trained on code,” arXiv preprint arXiv:2107.03374,
2021.
[56] OpenAI, “Gpt-4 technical report,” 2023. [Online]. Available: https:
//arxiv.org/abs/2303.08774
[57] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov, “Prox-
imal policy optimization algorithms,” arXiv preprint arXiv:1707.06347,
2017.
[58] M.
Ruby,
“How
chatgpt
works:
The
model
behind
the
bot,”
2023.
[Online].
Available:
https://towardsdatascience.com/
how-chatgpt-works-the-models-behind-the-bot-1ce5fca96286
[59] D. Bilika, N. Michopoulou, E. Alepis, and C. Patsakis, “Hello me,
meet the real me: Audio deepfake attacks on voice assistants,” arXiv
preprint arXiv:2302.10328, 2023.
[60] D. Wang, R. Wang, L. Dong, D. Yan, X. Zhang, and Y. Gong, “Ad-
versarial examples attack and countermeasure for speech recognition
system: A survey.”
Springer, 2020, pp. 443–468.
[61] J. Xu, D. Ju, M. Li, Y.-L. Boureau, J. Weston, and E. Dinan, “Bot-
adversarial dialogue for safe conversational agents,” 2021, pp. 2950–
2968.
[62] H. Chung, M. Iorga, J. Voas, and S. Lee, “Alexa, can i trust you?”
Computer, vol. 50, pp. 100–104, 2017.
[63] J. Kepner, J. Bernays, S. Buckley, K. Cho, C. Conrad, L. Daigle,
K. Erhardt, V. Gadepally, B. Greene, and M. Jones, “Zero botnets: An
observe-pursue-counter approach,” arXiv preprint arXiv:2201.06068,
2022.
[64] M. H. Shah and M. Panchal, “Theoretical evaluation of securing
modules for educational chatbot.”
IEEE, 2022, pp. 818–824.
[65] N. Waheed, M. Ikram, S. S. Hashmi, X. He, and P. Nanda, “An empir-
ical assessment of security and privacy risks of web-based chatbots.”
Springer, 2022, pp. 325–339.
[66] R. Chivukula, T. J. Lakshmi, L. R. R. Kandula, and K. Alla, “A study
of cyber security issues and challenges.”
IEEE, 2021, pp. 1–5.
[67] Y. Hu, W. Kuang, Z. Qin, K. Li, J. Zhang, Y. Gao, W. Li, and K. Li,
“Artificial intelligence security: Threats and countermeasures,” ACM
Computing Surveys (CSUR), vol. 55, pp. 1–36, 2021.
[68] K. Gondaliya, S. Butakov, and P. Zavarsky, “Sla as a mechanism to
manage risks related to chatbot services.”
IEEE, 2020, pp. 235–240.
[69] C. Qian, H. Qi, G. Wang, L. Kunc, and S. Potdar, “Distinguish sense
from nonsense: Out-of-scope detection for virtual assistants,” arXiv
preprint arXiv:2301.06544, 2023.
[70] G. F. Elsayed, I. Goodfellow, and J. Sohl-Dickstein, “Adversarial
reprogramming of neural networks,” arXiv preprint arXiv:1806.11146,
2018.
[71] Y. Zheng, X. Feng, Z. Xia, X. Jiang, A. Demontis, M. Pintor, B. Biggio,
and F. Roli, “Why adversarial reprogramming works, when it fails, and
how to tell the difference,” Information Sciences, vol. 632, pp. 130–
143, 2023.
[72] F. Satari, “Review on sql injection and prevention methods (sql
injection attacks),” Available at SSRN 3845158, 2008.
[73] H. Prakken, “A persuasive chatbot using a crowd-sourced argument
graph and concerns,” Computational Models of Argument, vol. 326,
p. 9, 2020.
[74] M. Mijwil and M. Aljanabi, “Towards artificial intelligence-based
cybersecurity: The practices and chatgpt generated ways to combat
cybercrime,” Iraqi Journal For Computer Science and Mathematics,
vol. 4, pp. 65–70, 2023.
[75] A. Rahman, M. Faizal, and V. S. Suguna, “Chatbots: Friend or fiend?”
NST Online, 2017.
[76] A. Qamar, A. Karim, and V. Chang, “Mobile malware attacks: Review,
taxonomy & future directions,” Future Generation Computer Systems,
vol. 97, pp. 887–909, 2019.
[77] E.
Shimony
and
O.
Tsarfati,
“Chatting
our
way
into
creating
a
polymorphic
malware,”
2023.
[Online].
Available:
https://www.cyberark.com/resources/threat-research-blog/
chatting-our-way-into-creating-a-polymorphic-malware
[78] A. Klein and I. Kotler, “Windows process injection in 2019,” Black
Hat USA, vol. 2019, 2019.
[79] S. Baki, R. Verma, A. Mukherjee, and O. Gnawali, “Scaling and
effectiveness of email masquerade attacks: Exploiting natural language
generation,” 2017, pp. 469–482.
[80] A. Giaretta and N. Dragoni, “Community targeted phishing: A middle
ground between massive and spear phishing through natural language
generation.”
Springer, 2020, pp. 86–93.
[81] K. Shu, S. Wang, D. Lee, and H. Liu, “Mining disinformation and fake
news: Concepts, methods, and recent advancements,” Disinformation,
misinformation, and fake news in social media: Emerging research
challenges and opportunities, pp. 1–19, 2020.
[82] H. Stiff and F. Johansson, “Detecting computer-generated disinforma-
tion,” International Journal of Data Science and Analytics, vol. 13, pp.
363–383, 2022.

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021
16
[83] R. Zellers, A. Holtzman, H. Rashkin, Y. Bisk, A. Farhadi, F. Roesner,
and Y. Choi, “Defending against neural fake news,” Advances in neural
information processing systems, vol. 32, 2019.
[84] VARINDIA,
“Chatgpt
produces
malicious
emails
and
code,”
2022.
[Online].
Available:
https://varindia.com/news/
chatgpt-produces-malicious-emails-and-code
[85] R. Karanjai, “Targeted phishing campaigns using large scale language
models,” arXiv preprint arXiv:2301.00665, 2022.
[86] R. Tarek, S. Chaimae, and C. Habiba, “Runtime api signature for
fileless malware detection.”
Springer, 2020, pp. 645–654.
[87] F. Barr-Smith, X. Ugarte-Pedrero, M. Graziano, R. Spolaor, and
I. Martinovic, “Survivalism: Systematic analysis of windows malware
living-off-the-land.”
IEEE, 2021, pp. 1557–1574.
[88] R. Stamp, “Living-off-the-land abuse detection using natural language
processing and supervised learning,” arXiv preprint arXiv:2208.12836,
2022.
[89] L. Bilge and T. Dumitras¸, “Before we knew it: an empirical study of
zero-day attacks in the real world,” 2012, pp. 833–844.
[90] Q. Study, “A new chatgpt zero day attack is undetectable malware
that steals data,” 2023. [Online]. Available: https://qsstudy.com/
a-new-chatgpt-zero-day-attack-is-undetectable-malware-that-steals-data/
[91] A. KRAFT, “Microsoft shuts down ai chatbot after it turned into
a nazi,” 2023. [Online]. Available: https://www.cbsnews.com/news/
microsoft-shuts-down-ai-chatbot-after-it-turned-into-racist-nazi/
[92] A.
Hashim,
“Vulnerability
in
telegram
bot
api
encryption
allows
access
to
messages
attribution,”
2019.
[Online].
Available:
https://latesthackingnews.com/2019/01/22/
vulnerability-in-telegram-bot-api-encryption-allows-access-to-messages/
[93] K.
O’Flaherty,
“Telegram
bots
have
got
a
major
problem,
security
researchers
warn,”
2019.
[Online].
Available:
https://www.forbes.com/sites/kateoflahertyuk/2019/01/
17/watch-out-for-telegram-bots-security-researchers-warn/?sh=
1a65c78315db
[94] zo.ai, “Zo is microsoft’s latest ai chatbot.” [Online]. Available:
https://mspoweruser.com/zo-microsofts-latest-ai-chatbot/
[95] N. Lee, A. Madotto, and P. Fung, “Exploring social bias in chatbots
using stereotype knowledge.” 2019, pp. 177–180.
[96] A. Weir, “Shades of tay: Microsoft’s zo chatbot says the qu’ran is ”very
violent”,” 2017. [Online]. Available: https://www.neowin.net/news/
shades-of-tay-microsofts-zo-chatbot-says-the-quran-is-very-violent/
[97] F.
Agomuoh,
“Your
siri
conversations
may
have
been
recorded
without
your
permission,”
2022.
[Online].
Available:
https://www.digitaltrends.com/computing/
developer-finds-apple-bluetooth-security-flaw-worth-7000/
[98] R. Hasan, R. Shams, and M. Rahman, “Consumer trust and perceived
risk for voice-controlled artificial intelligence: The case of siri,” Journal
of Business Research, vol. 131, pp. 591–597, 2021.
[99] NIST, “Cve-2019-25071 detail,” 2022. [Online]. Available: https:
//nvd.nist.gov/vuln/detail/CVE-2019-25071
[100] C. Lentzsch, S. J. Shah, B. Andow, M. Degeling, A. Das, and W. Enck,
“Hey alexa, is this skill safe?: Taking a closer look at the alexa
skill ecosystem,” Network and Distributed Systems Security (NDSS)
Symposium2021, 2021.
[101] BBC,
“Amazon
alexa
security
bug
allowed
access
to
voice
history,”
2020.
[Online].
Available:
https://www.bbc.com/news/
technology-53770778
[102] S. Pathak, S. A. Islam, H. Jiang, L. Xu, and E. Tomai, “A survey on se-
curity analysis of amazon echo devices,” High-Confidence Computing,
p. 100087, 2022.
[103] G. Zhang, C. Yan, X. Ji, T. Zhang, T. Zhang, and W. Xu, “Dolphinat-
tack: Inaudible voice commands,” 2017, pp. 103–117.
[104] H. Chung, J. Park, and S. Lee, “Digital forensic approaches for amazon
alexa ecosystem,” Digital investigation, vol. 22, pp. S15–S25, 2017.
[105] K. M. Malik, H. Malik, and R. Baumann, “Towards vulnerability
analysis of voice-driven interfaces and countermeasures for replay
attacks.”
IEEE, 2019, pp. 523–528.
[106] J. Jackson, “The growing influence of chatgpt in the cybersecurity
landscape,” 2023. [Online]. Available: https://blogs.blackberry.com/en/
2023/03/the-growing-influence-of-chatgpt
[107] C. Coles, “11% of data employees paste into chatgpt is confidential,”
2023.
[108] T.
Economist,
“[exclusive]
concerns
turned
into
reality...
as
soon
as
samsung
electronics
unlocks
chatgpt,
’misuse’
continues,” 2023. [Online]. Available: https://economist.co.kr/article/
view/ecn202303300057?s=31
[109] S. Schroeder, “Chatgpt was shut down due to a bug that exposed user
chat titles,” 2023. [Online]. Available: https://mashable.com/article/
chatgpt-bug-user-histories
[110] OpenAI, “March 20 chatgpt outage: Here’s what happened,” 2023.
[Online]. Available: https://openai.com/blog/march-20-chatgpt-outage
[111] Cyble,
“The
growing
threat
of
chatgpt-based
phishing
attacks,” 2023. [Online]. Available: https://blog.cyble.com/2023/02/22/
the-growing-threat-of-chatgpt-based-phishing-attacks/
[112] J.
Vijayan,
“Chatgpt
browser
extension
hi-
jacks
facebook
business
accounts,”
2023.
[On-
line].
Available:
https://www.darkreading.com/application-security/
chatgpt-browser-extension-hijacks-facebook-business-accounts
[113] S. Mohanty, S. Vyas, S. Mohanty, and S. Vyas, “Cybersecurity and ai,”
How to Compete in the Age of Artificial Intelligence: Implementing a
Collaborative Human-Machine Strategy for Your Business, pp. 143–
153, 2018.
[114] J. Martindale, “These are the countries where chatgpt is currently
banned,” 2023. [Online]. Available: https://www.digitaltrends.com/
computing/these-countries-chatgpt-banned/
[115] S. McCallum, “Chatgpt banned in italy over privacy concerns,” 2023.
[Online]. Available: https://www.bbc.com/news/technology-65139406
[116] K.
Hurler,
“Amazon
warns
employees
to
beware
of
chatgpt,”
2023.
[Online].
Available:
https://gizmodo.com/
amazon-chatgpt-ai-software-job-coding-1850034383
[117] J. Edu, C. Mulligan, F. Pierazzi, J. Polakis, G. Suarez-Tangil, and
J. Such, “Exploring the security and privacy risks of chatbots in
messaging services,” 2022, pp. 581–588.
Attia Qammar (B20200693@xs.ustb.edu.cn/q.attia@yahoo.com) received
her BS degree from Bahauddin Zakariya University and her MS from National
College of Business Administration and Economics, Pakistan. Currently, she is
pursuing her Ph.D. degree from the School of Computer and Communication
Engineering at the University of Science and Technology Beijing, China. Her
research interests include federated learning, chatbots, data security, and IoT
privacy-preserving systems.
Hongmei Wang (320180032@xjau.edu.cn) received her master’s degree in
software engineering from Beijing University of Technology, Beijing, China,
in 2009. She is currently pursuing a doctorate in grassland systematics
at Xinjiang Agricultural University, Urumqi, China. Her research interests
include are intelligent algorithms and big data optimization.
Jianguo Ding (jianguo.ding@bth.se) received his Doctorate in Engineering
(Dr.-Ing.) from the faculty of mathematics and computer science at the
University of Hagen, Germany. He is currently an Associate Professor at the
Department of Computer Science, Blekinge Institute of Technology, Sweden.
His research interests include cybersecurity, critical infrastructure protection,
intelligent technologies, blockchain, distributed systems management and
control, and serious game. He is a Senior Member of the IEEE (SM’11)
and a Senior Member of the ACM (SM’20).
Abdenacer Naouri (nacer.naouri@gmail.com) received his B.S. degree in
computer science from the University of Djelfa Algeria, in 2011, and the
M.Sc. degree in networking and distributed systems from the University of
Laghouat Algeria, Laghouat, Algeria, in 2016. He is currently pursuing the
Ph.D. degree with the University of Science and Technology Beijing China,
Beijing, China. His current research interests include Cloud computing,Smart
communication, machine learning, the Internet of vehicles, and Internet of
Things.

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021
17
Mahmoud Daneshmand (mdaneshm@stevens.edu) Mahmoud Daneshmand
received the B.S. and M.S. degrees in mathematics from the University of
Tehran, Tehran, Iran, in 1964 and 1966, respectively, and the M.S. and Ph.D.
degrees in statistics from the University of California at Berkeley, Berkeley,
CA, USA, in 1973 and 1976, respectively. He is the Co-Founder and a
Professor with the Department of Business Intelligence and Analytics as
well as the Data Science Ph.D. Program, and a Professor with the School of
Business, Stevens Institute of Technology, New Jersey, USA. He has more than
40 years of industry and university experience as the Executive Director, the
Assistant Chief Scientist, a Professor, a Researcher, a Distinguished Member
of Technical Staff, a Technology Leader, the Founding Chair of Department,
and the Dean of School with: Bell Laboratories, Murray Hill, NJ, USA; AT&T
Shannon Labs-Research, Florham Park, NJ, USA; the University of California
at Berkeley; the University of Texas at Austin, Austin, TX, USA; New York
University, New York, NY, USA; Sharif University of Technology, Tehran; the
University of Tehran; and Stevens Institute of Technology, Hoboken, NJ, USA.
He is a Data Scientist, expert in big data analytics, artificial intelligence, and
machine learning with extensive industry experience, including with the Bell
Laboratories as well as the Info Lab, AT&T Shannon Labs-Research. He has
published more than 250 journal and conference papers; authored/coauthored
three books, and has graduated more than 2500 Ph.D. and M.S. students. He
holds key leadership roles in IEEE Journal Publications, IEEE Major Con-
ferences, Industry–IEEE Partnership, and IEEE Future Direction Initiatives.
Dr. Daneshmand has served as the general chair, keynote chair, panel chair,
executive program chair, and technical program chair of many IEEE major
conferences. He has given many keynote speeches in major IEEE as well as
international conferences.
Huansheng Ning (ninghuansheng@ustb.edu.cn) received the B.S. degree
from Anhui University, Hefei, China, in 1996 and the Ph.D. degree from
Beihang University, Beijing, China, in 2001. He is currently a Professor and
the Vice Dean with the School of Computer and Communication Engineering,
University of Science and Technology Beijing, China, and the founder and
principal at Cybermatics and Cyberspace International Science and Tech-
nology Cooperation Base. His current research interests include Internet of
Things, Cyber Physical Social Systems, Cyberspace Data and Intelligence and
metaverse (general cyberspace).

