Neural population geometry and optimal coding of tasks
with shared latent structure
Albert J. Wakhloo1,2, Will Slatton2,3, and SueYeon Chung2,3
1Center for Theoretical Neuroscience, Columbia University
2Center for Computational Neuroscience, Flatiron Institute
3Center for Neural Science, New York University
1
Abstract
Humans and animals can recognize latent structures in their environment and apply this
information to efficiently navigate the world. However, it remains unclear what aspects of
neural activity contribute to these computational capabilities. Here, we develop an ana-
lytical theory linking the geometry of a neural population’s activity to the generalization
performance of a linear readout on a set of tasks that depend on a common latent struc-
ture. We show that four geometric measures of the activity determine performance across
tasks. Using this theory, we find that experimentally observed disentangled representations
naturally emerge as an optimal solution to the multi-task learning problem. When data
is scarce, these optimal neural codes compress less informative latent variables, and when
data is abundant, they expand these variables in the state space. We validate our theory
using macaque ventral stream recordings. Our results therefore tie population geometry to
multi-task learning.
2
Introduction
Humans constantly solve different instances of similar problems. We brake at stop signs,
stop at red lights, and slow down in crowded streets. We do these things effortlessly and
efficiently learn to use new sensory cues to regulate our behavior. This is possible because we
are able to recognize overt symbols like road signs, as well as more abstract visual cues like
the crowdedness of a street. More generally, humans and animals learn to recognize latent
variables in the environment and use them to guide their behavior across contexts and tasks.
Recent experimental findings have described coding strategies that may underlie this
ability. In particular, several studies have described cases where independent variables in
the environment are represented along distinct directions of variation in the neuronal activity
space [1, 2, 3, 4, 5, 6, 7]–e.g., in orthogonal subspaces of the firing rates of a collection of
neurons [2, 7, 8, 9, 10]. These independent factors have ranged from the contacts of distinct
1
arXiv:2402.16770v2  [q-bio.NC]  11 Apr 2024

mouse whiskers [2], to more abstract latent variables such as the values of different choices in
a decision making task [3]. Neural representations in which distinct environmental variables
are represented along independent or orthogonal directions of variation are referred to as
factorized or disentangled.
Factorized representations were recently shown to emerge in
artificial networks trained on multiple tasks [8] and are thought to support generalization to
new contexts as well as efficient learning of new tasks that depend on shared latent variables
[4, 11].
Similarly, other studies have argued that the brain makes widespread use of cognitive
maps to solve these problems [12]. These are coding strategies in which environmental vari-
ables are represented in the population code in a way that preserves task-relevant relations
between them. This idea is supported by a range of findings, for example, studies in which
structurally similar tuning profiles emerge in a neural population for distinct types of envi-
ronmental variables [13, 14, 15, 16, 17, 18]. These variables have ranged from an animal’s
position [19], to the frequency of an auditory stimulus [13], to more abstract quantities like
the amount of evidence accumulated in a decision making task [15]. Here and in the find-
ings referenced above, neural populations represent latent structure in the environment in
a way that supports a target behavior. However, defining measures for and understanding
why certain neural activity patterns represent latent variables in task-efficient ways remains
challenging [20].
A promising approach to tying neural activity patterns to such computational goals is
to study the geometry of the neural responses [21]. Here, the overarching idea is to find
which mesoscopic statistics of the population activity contribute to a macroscopic target
computation or behavior. In this way, we can gain insight into neural computation with-
out having to give a detailed account of microscopic single unit activity. For example, in
the domain of invariant object recognition, recent works analytically tied coding efficiency
[22, 23, 24, 25] and few-shot generalization performance [26] to measurable statistics of the
population activity. Recent works have also studied a wide range of motor [27, 28, 29, 30, 31],
sensory [2, 6, 16, 32, 33, 34], and decision making [3, 4, 14, 15] computations and behaviors
by analyzing the geometry of neural population responses.
In this study, we develop an analytical theory for learning binary decision making tasks
depending on a common latent structure that directly ties the statistics of neural population
responses to generalization performance. While several authors have used linear probes or
heuristic geometric measures to analyze population activity in such tasks (e.g. [2, 4]), to the
best of our knowledge, there is no cohesive theory that directly ties mesoscopic statistical
features of neural activities to task performance in these settings. To fill this gap, we analyt-
ically calculate how neural population geometry shapes the generalization error of an agent
learning multiple tasks that depend on a common latent structure. To do this, we use a
flexible model in which binary classification tasks are formed in an unobserved latent space,
and an agent makes decisions by applying a linear readout to a set of neural population
activities [8, 35, 36, 37, 38].
Using these results, we show how the error is completely determined by a small set of
geometric statistics summarizing the relationship between the latent variables and neural
responses. We leverage this theory to derive normative predictions describing the optimal
geometry that a set of population responses should take. In this way, we show that disen-
tangled representations naturally emerge as an optimal solution to the multi-task learning
2

problem. Furthermore, we show how optimal codes compress less useful information when
data is scarce and expand this information in the activity space when data is abundant.
We go on to describe how this strategy would be reflected in measurable properties of the
eigenspectrum of neuronal responses. Finally, we show that our theory accurately predicts
the generalization error of readouts trained on artificial neural network (ANN) data as well
as multi-unit recordings taken from the macaque ventral stream [39], and we demonstrate
the use of our geometric measures as a data-analytic tool.
3
Results
3.1
Theory of multi-task learning
We study the ability of a neural population to support downstream learning of tasks that
depend on a common latent structure. Specifically, we consider a setting in which an agent
learns binary classification tasks using a set of p training stimuli. These stimuli are formed
from a set of unobserved d−dimensional latent variables, z ∈Rd (Fig 1a). The task labels for
a given binary classification task are formed by linearly separating the latent space into two
pieces using a hyperplane with a normal vector, T. Note that while the separation is linear
in the latent space, it may be highly non-linear in the stimulus space. Alongside the latent
variables, we consider the activity patterns of n neurons for each stimulus. We denote the
vector of activity patterns for each stimuli as x ∈Rn. We quantify how well these neuronal
activity patterns support downstream classification by calculating the generalization error
of an agent that makes predictions using a linear readout of these activities, which is learned
using a supervised Hebb rule (Methods; Fig. 1(e-f)) [38]. As described in the Supplemental
Material (SM), we calculate the generalization error of this readout both for a single, fixed
task as well as the average generalization error across all possible tasks. In this way, we
connect the statistical properties of the neural code to the multi-task learning problem.
While the generalization error for arbitrary distributions of neuronal activities and latent
variables is analytically intractable, we show that in many cases, the error only depends on
a few key statistics. To do this, we draw from recent work in deep learning theory showing
that the generalization error of linear readouts trained on complex distributions can in many
cases be well approximated by studying simpler Gaussian models (Methods) [36, 37, 40, 41].
Thus, we analytically calculate the generalization error using a Gaussian model and show
empirically that our theory accurately predicts the generalization error of the linear readout
rule when applied to the activations of non-linear multi-layer perceptrons (MLPs; Sec. 3.3)
as well as real neural responses from V4 and IT on visual classification tasks (Sec. 3.4).
Using this simplified model, we derive a formula for the average generalization error.
As described in the Supplemental Material, we prove that under certain conditions, the
generalization error can be written as a strictly decreasing function of four geometric terms,
which we schematize in Fig. 2. We now discuss each of these one by one:
• Neural-latent correlation: c. This term is a normalized sum of all covariances be-
tween neurons and latent variables. Geometrically, the neural-latent correlation mea-
sures how sensitive the population responses are to variations in the latent space.
3

Figure 1: Schematic of the task and model setup using images from the d-sprites dataset as
an example [42]. (a) Visual stimuli are generated from points in a latent space, and binary
discrimination tasks are formed by linearly separating the latent space using a hyperplane
with normal T1. (b) Each stimulus elicits a neuronal activity pattern, visualized as points
in an activity space. (c) A new binary discrimination task can be formed by separating
the latent space using a different hyperplane with normal T2. (d) As before, stimuli elicit
neural activity patterns. (e-f) Schematic of the Hebbian learning rule we consider. Given
a set of p = 12 training stimuli, subsequent decisions are made using a linear readout of
the neural activity patterns, in this case the firing rates of 3 neurons. When the number of
positive (red) and negative (blue) labels is balanced, this linear readout corresponds to using
a hyperplane whose normal points in the direction of the difference in the means of positive
and negative examples .
4

Figure 2: Schematic of the geometric terms. We visualize different possible neuronal activity
patterns elicited by the same set of stimuli.
(a) A small slice of the latent space from
which stimuli are generated. (b) Visualization of neural activity patterns with low (left) and
high (right) total correlation. When the correlation is high, the relative distances between
points in the latent space are approximately preserved in the neural state space. (c) Signal-
signal factorization (SSF). When the SSF is low, different latent variables are represented
along overlapping directions, and when it is high, independent directions in the latent space
are represented along approximately orthogonal directions in the neural state space. (d)
Signal-noise factorization (SNF). When the SNF is low, the noise distribution (grey ellipses)
around a point in the firing rate space falls along the coding directions. When it is high,
the noise distribution is orthogonal to these directions. (e) Neural dimension. In higher
dimensional representations, the neural activity and associated noise distribution occupies
more directions in the state space, shown here as 2d (left) vs. 3d (right) noise distributions.
As the dimension increases, the projection of a sample of neural activity onto a given direction
becomes increasingly concentrated, supporting generalization performance [26].
5

• Signal-signal factorization (SSF): f. This term measures the alignment between
the coding directions of distinct latent variables. The signal-signal factorization term
favors neural coding schemes that represent distinct latent variables along orthogonal
directions in the state space. Moreover, this term encourages these representations to
devote equal variance to distinct coding directions–i.e., to form a whitened represen-
tation of the latents.
• Signal-noise factorization (SNF): s. This term measures the magnitude of the
noise that lies along the coding directions of the latent variables. Ideally, any noise
present in the neural responses should lie in directions which are orthogonal to the
directions representing the latent variables.
• Neural dimension: PR . The participation ratio of the neural responses measures
the effective number of dimensions that the population activity spans. When all else is
equal, higher dimensional responses are preferred, as neuronal noise is less correlated
from trial to trial [26].
Finally, we note that the contribution to the error of the neural dimension and neural-
latent correlation decays to 0 with the number of training samples (Methods). On the other
hand, the error stemming from the signal-signal and signal-noise factorization represents an
irreducible error that does not depend on the number of training samples (SM). Intuitively,
in the few-shot regime, the main concern is maximizing the amount of total signal in the
neural code, while minimizing the impact of noise. These two aspects are primarily controlled
by the total correlation and dimension, respectively. On the other hand, in the many-shot
regime the main concern becomes keeping the representations of distinct latent variables
separate from one another and separate from noise directions. These features of the code
are respectively controlled by the two factorization terms.
We begin by validating our theory numerically on data drawn from a Gaussian model.
Here, we find that our theory yields an excellent agreement with numerical simulations for
a wide range of values (Methods). In these simulations, we sample the latents zµ and neural
responses xµ from a multivariate Gaussian. We set the covariance matrix of the latents to
have a spectrum that decays as a power law. Since the proof of our main theorem assumes
that the spectra of the covariances decays slowly, this allows us to parametrically study
violations of our main theorem’s assumptions (Fig. 3a-e; SM). After sampling the latents,
we form the neural responses either by taking a random high dimensional projection of the
latent variables, or by applying a whitening transform to the latent variables (Fig. 3b-d;
see Methods). We then calculate the empirical generalization error across a set of binary
classification tasks which are formed by shattering the latent space as above.
As shown in Fig. 3f-h, our theory yields an excellent fit to numerical simulations. Im-
portantly, the theoretical prediction holds all the way down to the few shot learning regime
in which the number of training samples p is small, despite the theory being derived in the
limit of large p. Furthermore, we find that the theory predicts the empirical generalization
error well for a relatively small number of neurons n and latent dimensionality d, as well as
latent variables whose covariance has an eigenspectrum that decays relatively quickly [32]
(Methods). These results suggest that our theory can be applied to a wide range of datasets
and is informative of the few shot learning regime.
6

Figure 3: Theory predicts empirical generalization error in Gaussian model with power law
covariance spectra. (a) Schematic illustrating simulation setup. Gaussian latent variables z
are used to generate task labels, and predictions are formed using a linear transformation of
the latent variables, x = Az. (b-d) Two typical units for the (b) latent variables, (c) random
high dimensional projection, and (d) whitened transform for various values of the spectral
decay exponent, α. (e) Eigenvalues of the latent covariance, Ω, for different decay rates, α.
(f-h) Multi-task generalization error as a function of training samples, p, for (f) the latent
variables themselves, (g) the random projection, and (h) the whitened transform.
3.2
Optimal representation of latent variables
Our theory provides an ideal framework to pose normative questions regarding multi-task
learning. In our model, latent variables which have less variance contain, on average, less
information about the task labels (Fig. 4a). This allows us to probe how task structure
and the number of available training samples determine which neural representations are
optimal in the sense of achieving the lowest possible generalization error. In this section,
we study the geometry of optimal neural representations, given some latent variables and a
fixed training dataset size.
We show analytically that the optimal representation disentangles latent variables into
orthogonal subspaces. More precisely, we show that each orthogonal direction of variation
in the latent space maps onto an orthogonal direction of variation in the neural state space.
Thus, disentangled representations emerge as the optimal solution to the multi-task learning
problem.
In addition to being disentangled, we find that the optimal code compresses less infor-
mative variables when data is scarce and expands these variables when data is abundant.
Importantly, while each latent is represented in an orthogonal direction, the variance along
each direction is not equal. As shown in Fig. 4a, when training data is limited, less in-
formative latent variables correspond to directions in the neuronal activity space that have
small variance. With increasing training data (i.e., as p increases), the amount of variance
dedicated to these less informative latent variables grows. This reflects a strategy in which
7

Figure 4: Optimal representational geometry as a function of training samples and latent
structure. (a) In our task setup, directions in the latent space that have little variance are,
on average, less informative of the task labels (Methods). Optimal neuronal representations
of these latent variables map independent directions in the latent space to independent
directions in the neuronal space. The amount of variance corresponding to less informative
directions in the latent space is small when data is scarce and is large when data is abundant.
(b) Eigenvalues of the optimal neural covariance as a function of the number of samples vs.
latent dimension, p/d. We show the eigenvalues of the latent variables’ covariance in black.
Markers correspond to results obtained by optimizing our formula for the generalization
error numerically, and solid lines correspond to our formula for the optimal code’s spectrum
(Methods). As the number of samples increases, the spectrum of the optimal neural code
becomes increasingly flat, indicating the expansion strategy described above. (c-e) Geometric
terms of the optimal neural representation for various values of p, given the same latent
covariance. (Note that we do not plot the signal-noise factorization, as it diverges for the
optimal representation for all p.) Surprisingly, the total correlation decreases with p.
8

the optimal code compresses less informative variables at small training set sizes p, while
at large sample sizes, these variables are expanded in the state space (Methods). Speak-
ing informally, the optimal code only starts paying attention to the less informative latent
variables when there are enough samples to learn their relevance to a given task.
We trace these features of the optimal neural code back to the eigenvalues of the neuronal
covariance matrix and our geometric terms. As shown in Fig 4b, the eigenspectrum of the
optimal code becomes increasingly flat as p increases. This reflects the fact that more and
more variance is being dedicated to the less informative directions in the latent space (i.e.,
directions with smaller variance). Turning to the geometric terms, we can see that this trade
off appears as a trend in which the participation ratio and signal-signal factorization term
of the optimal code both increase with p, while the correlation decreases (Fig. 4c-e). That
is, optimal neural codes become increasingly high dimensional as the number of available
training samples increases. A key normative prediction from our theory is therefore that
the task-related variability of neural responses becomes increasingly high dimensional as an
agent learns to perform tasks that depend on a complex latent structure.
3.3
Geometry of multi-task learning in MLPs
In practice, neural data is complex and may be non-Gaussian. To test our theory in this
regime, we now apply our theory to both random and trained non-linear multi-layer per-
ceptrons (MLPs). As shown in Fig. 5a, we first sample a set of zero-mean Gaussian latent
variables z. We then feed these latent variables through a random MLP with increasing
hidden layer sizes. After generating this set of stimuli, we sample 500 random T vectors,
and use these to generate a set of task labels (Methods). These labels and data points from
the random MLP are then used to train a downstream 4-hidden-layer MLP, which predicts
the label for each task using a linear readout of its penultimate layer. This is reminiscent
of the hidden manifold modeling framework [36]. Finally, we sample a new set of latent
variables along with a new set of tasks and calculate the generalization error of the Hebb
rule when applied to the representations at different layers of the trained and untrained
MLPs (Methods). This setup allows us to validate our theory on non-linear transformations
of Gaussian latent variables.
As shown in Fig. 5b-c, we find good agreement between our theoretical predictions and
the empirical generalization error. Examining the layer-by-layer generalization errors we find
that the random MLP successfully tangles neural representations of the latents, from the
perspective of the downstream Hebbian readout, as the generalization error increases through
the random MLP. On the other hand, we find that the trained MLP successfully manages
to learn the latent structure of the task, as demonstrated by the fact that the generalization
error of the Hebb rule drops sharply through the layers of the network. Overall, these results
provide compelling evidence that our analytical formula holds well beyond settings in which
the neural responses are Gaussian variables.
Turning to our geometric decomposition of the error, we find several interesting trends
across layers and non-linearities (Fig. 5d-g). In the random MLP, we see that the correlation
decreases each time the non-linearity is applied, while the signal-noise and signal-signal
factorization terms gradually decrease through the network. We also find that the dimension
of the responses stays more or less constant through the network.
9

Figure 5: Theory predicts generalization error of the Hebb rule in trained and random
MLPs.
(a) Schematic of the experiment.
Latent variables z are randomly shattered to
generate task labels. These latents are passed through a random MLP (light blue) and are
then used as inputs to train a 3 hidden layer MLP (dark blue) on the multi-task binary
classification problem using stochastic gradient descent. (b-c) After training, we sample a
new set of latents and teacher vectors and calculate the generalization error of the Hebb rule
on each layer of the (b) random and (c) trained network. Theoretical predictions closely
track empirical errors, and the trained network achieves a lower error in intermediate layers.
(d-g) Geometric terms across layers for the random (light blue line) and trained (dark blue
line) networks. Linear layers are marked by circles and relu layers by squares. Interestingly,
the error only slightly changes across linear and relu layers of the same model stage, in spite
of sharp changes in the geometry. (h-k) Average change in the geometry before and after
the application of relu. Here we show the mean and standard deviation of the difference
between each geometric term before and after the relu nonlinearity is applied. In the trained
network, the application of relu consistently causes increases in the dimension and signal-
signal factorization, as well as decreases in the correlation.
10

In the trained network, we find that linear and relu layers orchestrate a trade off between
the geometric terms that together lead to an overall decrease in multi-task generalization
error.
As shown in Fig 5h-k, the neural dimension spikes each time the nonlinearity is
applied, at the cost of the total correlation. Conversely, at each linear layer, the correlation
sharply rises while the dimension falls. On the other hand, we find that both the signal-signal
and signal-noise factorization terms monotonically increase through the penultimate layer
of the network. Importantly, this pattern is not present in the random network. Thus, the
trained MLP learns to use the nonlinearity to increase the dimension of the representation as
well as the factorization of the latent variables, while simultaneously squashing particularly
harmful noisy directions of variability in the input. Intuitively, the linear layers learn to orient
signal-unrelated features into parts of the state space that are zeroed out by the non-linearity
[43]. While these are sharp changes in the geometry, the overall generalization error across
layers exhibits only minor fluctuations between most linear and relu layers, highlighting the
limitations of a generalization error-based approach to analyzing network activity.
We now study the dynamics of these geometric terms through training. To do this, we
track the layer-wise generalization error and geometry through a single epoch of training
(Methods). As shown in Fig. 6, we find that the generalization error in this context decays
through training across layers. In the linear layers, we find that improved generalization is
driven by a steady rise in the signal-signal and signal-noise factorization as well as small
increases in the total correlation and dimension. On the other hand, in relu layers, the cor-
relation decreases through training, with improved generalization being driven by increases
in the dimension as well as the signal-signal and signal-noise factorizations across both early
and late layers. We find similar trends for MLPs with a tanh non-linearity (SM Figs. 2&3).
These geometric trends mirror the effects of adding more samples on the optimal representa-
tion: As more samples become available, the factorization terms and dimension grow while
the correlation shrinks. In other words, the dynamics of MLP representations in non-linear
layers qualitatively mimic the behavior of the optimal neural code described by our theory.
3.4
Predicting readout performance of macaque visual represen-
tations
Having considered non-linear artificial neural networks, we now apply our theory to biological
neural data. To do this, we draw from pre-existing multi-unit recordings from macaque V4
and IT taken while 2 monkeys viewed visual stimuli (Methods; Fig. 7(a-c)) [39]. The stimuli
used in these experiments included images of 64 objects taken from 8 categories and were
generated by modifying d = 6 continuous latent variables, that included the size, position,
and angle of the object. This allows us to form binary classification task labels by shattering
this 6-dimensional latent space on subsets of the data corresponding to individual object
categories–e.g., a given task could involve separating images of chairs on the left side of the
screen from those on the right. We test our theory by calculating the generalization error of
the Hebbian readout rule applied to the V4 and IT neural responses.
We find that our formula accurately predicts the generalization error of the monkey neu-
ral responses across both recording sites. Furthermore, as shown in Fig. 7d, both V4 and
IT achieve better generalization error than the raw pixels, and IT achieves a better gener-
11

Figure 6: Evolution of generalization error and representational geometry through training.
(a) Generalization error of the Hebb rule applied to four different layers on a previously
unseen set of tasks. We examine layers from the early stages of the MLP (black) as well as
late stages (green) for both linear (solid line) and relu (dashed line) layers. We can see that
the error decreases across all layers with the number of training samples, p. (b) Dimension
term. While the dimension of the neural activations increases for all layers through training,
the differences are most pronounced in the relu layers. (c) Correlation term. This term
decreases through training for relu layers and slightly increases for linear layers. (d) Signal-
signal factorization term. (e) Signal-noise factorization term. Both of these terms uniformly
increase through training across the network, though the former begins to plateau at the end
of training. These findings mirror the effect of increasing the number of training samples on
the optimal neural code.
alization error than V4. [44]. Applying our geometric decomposition of the generalization
error to these data, we find that the dimension is lower in V4 than in either IT or the pixel
data [26] (Fig. 7e). Furthermore, we find that the correlation steadily increases from the
raw pixels to V4 and IT, indicating an increased overall signal. We also find that the level
of signal-signal factorization improves from V4 to IT, in line with previous results [45]. In-
terestingly, we find that the signal-signal factorization is higher in the pixel space than in
either brain region. Finally, we find that the signal-noise factorization is far lower in the
pixels and V4 than in IT. This suggests that in IT, latent-unrelated variability overlaps less
with the coding directions than in V4 or the pixels (Fig. 7e-h). Taken together, these results
provide strong support for our theory and demonstrate the applicability of our metrics as a
tool for tying the geometry of neuronal population responses to the computational objective
of multi-task learning.
12

Figure 7: Theory predicts multi-task error in macaque V4 and IT data. (a-b) Example
stimuli and tasks. Visual stimuli included 64 objects grouped into 8 categories and were
generated by modifying d = 6 continuous latent variables that included object size, position,
and angle. We form binary classification tasks on subsets of the data coming from the same
category–e.g. all images from the ”Tables” category. (c) Probe placement. Figure adapted
with permission from [39]. (d) Generalization error across pixels and brain regions calculated
empirically (markers) and using our formula for the generalization error (solid line). (e-h)
Geometric terms across pixels and neural responses. We can see that V4 representations are
lower dimensional than either the pixels or IT, that the correlation improves throughout,
and that the signal-signal factorization is highest in the pixel space, though it improves from
V4 to IT. Note too that the signal-noise factorization sharply rises from V4 to IT, suggesting
that signal-unrelated variability overlaps less with the coding directions in the latter region.
13

4
Discussion
In this work, we analyzed a model of learning multiple tasks that share a common latent
structure, connecting neural population geometry to the multi-task learning problem. In
this model, binary classification tasks are generated by shattering a latent space uniformly.
By varying the spectrum of the latent covariance matrix, we were able to model situations
in which certain latent variables are, on average, more useful for downstream tasks than
others. We then calculated the generalization error of a linear readout applied to a set of
neural activations responding to stimuli that were generated from these latent variables. In
this way, we evaluated how well a neural representation is able to extract information about
latent variables from the stimuli and use this for arbitrary downstream tasks.
To connect the mesoscopic statistics of neural activation patterns to generalization per-
formance, we decomposed our formula for the generalization error into four geometric terms.
These terms summarize the relationship between population activity and the latent variables
and completely determine generalization performance in our task. First is the dimension of
the entire population activity, as measured by the participation ratio [46]. Second, we intro-
duced the total correlation between neuronal activity and the latent variables, representing
the overall sensitivity of neuronal responses to changes in the latent space. Third, we intro-
duced the signal-signal factorization, measuring the degree to which each latent variable is
coded in an orthogonal direction in the neural state space, with adequate variance devoted to
each coding direction. Finally, we defined the signal-noise factorization, which summarizes
the amount of signal-unrelated variability that lies along the directions in state space used
to represent the latent variables. Hence, this term summarizes the degree to which noise
correlations affect the readout considered here [47, 48, 49, 50]. Together, these four terms
completely characterize the generalization performance of the linear readout applied to the
neural population responses.
We next analytically calculated which geometries minimize the generalization error, given
a fixed latent structure and number of available training samples. This falls in line with stud-
ies on efficient coding in which predictions of sensory physiology are derived from statistics of
the natural environment [51, 52, 53]. In this way, we found that disentangled representations
naturally emerge as an optimal solution to the multi-task learning problem. Specifically, we
found that the eigenvectors of the neuronal and latent covariances match the singular vectors
of the cross-covariance (Methods, SM). This is reminiscent of work done on the successor
representation and grid cells [54, 55, 56, 57, 58], and it would be interesting to see if there
are additional constraints under which grid cells or other coding properties emerge as an
optimal code in our model [59]. Future work can also extend our theory to consider sets of
tasks that compete with one another as in previous studies of multi-task learning [60, 61].
Moreover, given recent findings regarding multi-task learning and dynamical motif reuse in
recurrent networks [62, 63], it would be interesting to extend our theory to a dynamical
setting in which predictions are formed using an appropriate readout of time-varying firing
rates.
We then described how the optimal neuronal code shifts from being low to high di-
mensional as the number of available samples increases. This finding reflects a strategy of
compressing less useful information when data is scarce and expanding it when data is abun-
dant. It would be very interesting to test this normative prediction in behaving animals.
14

This could be done, for example, using multi-unit recordings taken while an animal learns to
perform decision making tasks in which multiple latent variables (e.g. distinct context cues)
must be used to make a decision. Our theory predicts that task-related neural variability
in the early stages of training should primarily track highly informative latent variables. As
training progresses, we predict that task-relevant neural variability will increase in dimension
as it begins to track the less informative latent variables in the task.
It is interesting to compare these findings to previous work reporting that higher dimen-
sional representations of individual object classes are preferred for invariant object recog-
nition in the few shot learning setting [26]. Although the contribution to the error of the
neural dimension scales as 1/ p in our formula just as in this work, the additional contri-
bution to the error of the signal-signal and signal-noise factorization terms together with
the constraint of of the covariance matrix remaining positive semi-definite, leads to optimal
representations with lower dimension in the few shot regime. These results highlight the fact
that different geometric terms may compete with each other in ways that cannot be directly
read off from generalization error equations when there are additional constraints imposed
on the system. Another related line of work has considered the role of neural dimension in
random pattern separation [64, 65, 66]. In these settings, the optimal dimension can de-
crease when neural noise increases, and future work can examine whether a similar relation
holds in the multi-task learning setting considered here. Finally, we note that theories of
least squares estimation suggest that similar phenomena occur for regression tasks with more
complex readouts [67, 68, 69, 70]. We suspect that a similar spectral expansion trend of op-
timal coding schemes holds for other common readout mechanisms in both classification and
regression tasks, and it would be interesting to investigate these questions in future work.
To test our theory, we next considered a setting in which a set of Gaussian latents are
fed through a random network, and task labels are generated as above in the latent space.
A four hidden layer MLP was then trained on the outputs of the random network on these
tasks [36]. We then evaluated each layer of the trained network on previously unseen tasks
using the scheme described above. Here, we found that intermediate layers of the trained
network successfully learned to represent the latent variables in a way that could be used for
downstream tasks, in line with previous work [8]. Furthermore, we found a strong correspon-
dence between our theoretical predictions and the empirical generalization errors. Applying
our geometric decomposition of the error, we found that linear and relu layers obtained sim-
ilar generalization errors but with sharply different underlying geometries. Specifically, relu
layers yielded a large boost to the dimension at the cost of the total correlation, while the
signal-signal and signal-noise factorizations steadily improved through the network. This
suggests a strategy in which linear layers attempt to place neuronal noise in regions of the
state space that are then squashed by the non-linearity [43]. Tracking the evolution of these
terms through training, we found that the correlation in relu layers decreases through train-
ing, while the dimension, signal-signal factorization, and signal-noise factorization increase.
This mirrors the behavior of an optimal code as more samples are presented as well as the
behavior of deep linear networks trained on certain regression tasks [71, 72]. It would be in-
teresting to apply our geometric decomposition to analyze disentanglement in more complex
artificial networks.
Finally, we applied our theory to electrophysiological recordings from macaque V4 and
IT [39]. Specifically, we used our theory to predict the generalization error of a linear readout
15

of neural responses trained on a series of binary classification tasks. We formed task labels
using latent variables describing object position, size, and orientation. As before, our theory
showed excellent agreement with empirical generalization errors of linear readouts applied to
the neural responses. We found that both regions showed better readout performance than
the pixels themselves [44]. While we found a low signal-signal factorization in these neural
responses relative to the pixels, we did find that signal-signal factorization improved from
V4 to IT [45]. Moreover, we found a sharp increase in the signal-noise factorization from the
pixels and V4 to IT, suggesting that latent-unrelated variability may become increasingly
orthogonal to the coding directions through the ventral stream. It would be very interesting
to repeat such an analysis using data from visual regions specialized for localization. Overall,
these results demonstrate the applicability of our theory as a data-analytic tool.
While we focused on tasks that came from shattering a continuous latent space, future
work could extend this theory to consider distributions of latents and tasks that more closely
mirror common experimental settings. Specifically, one could repeat our general calculation,
while restricting the distribution of task vectors to a handful of relevant directions or re-
stricting the distribution of latent variables to be fixed to discrete values. This would be
particularly interesting in settings where only particular groups of tasks or latent variable
values are relevant for an animal to consider. Such a calculation would allow one to tie
the population geometry to the linear readout performance on families of tasks and latent
distributions which are tailored to specific experimental settings. We hope to pursue this
line of research in subsequent work.
5
Methods
5.1
Model of multi-task learning
We model a setting in which an agent learns a set of binary classification tasks by performing
a linear readout on a set of neural population activities. Formally, we assume that each
stimulus is associated with a d-dimensional latent variable zµ ∈Rd, with 1 ≤µ ≤p denoting
the sample index. The labels for a specific task are formed by shattering the latent space
using a hyperplane with a normal vector, T. Thus, we have that the binary classification
task labels, yµ, satisfy the relation, yµ = sign(T · zµ), so that each T vector defines a
specific classification task.
Associated with each latent, we also consider n-dimensional
neural activity patterns, xµ ∈Rn. From these data, the agent then forms predictions of
new data points using a supervised Hebbian readout rule [38]. More precisely, given a new
stimulus associated with latent variables z+ and firing rates x+, the agent forms a prediction
ˆy+ using the rule
ˆy+ = sign(w · x+),
w = 1
p
X
µ
yµxµ.
(1)
When the labels are balanced, this corresponds to using the difference in the mean activities
for positively and negatively labeled examples (Fig. 1f). We evaluate how well the neural
code can support downstream classification by calculating the generalization error of these
16

predictions, averaged across different tasks (i.e., different T vectors), though we also calculate
the error for a fixed T along the way (SM).
While we validate our theory on a wide range of data, we obtain our analytical results
using a simplified Gaussian model. This is inspired by work on the Gaussian Equivalence
Principle (GEP) in deep learning theory [40, 36, 37]. Formally, we assume that each pair of
neural responses and latent variables, (x, z) are jointly zero-mean Gaussians with covariance
matrices:
E[xx⊤] = Ψ,
E[xz⊤] = Φ,
E[zz⊤] = Ω.
(2)
We can see that Ψ describes the neuron-neuron covariances, Φ contains the covariances be-
tween single-unit responses and variations in the latent variables, and Ωdescribes covariances
between latent variables in the dataset. Note that since the T vectors are chosen randomly
from a Gaussian distribution, the latent covariance determines which directions in the latent
space are, on average, most informative of the task labels. Directions in the latent space
which have a significant amount of variance are typically more informative of the task labels
in this setup. This model corresponds to a variant of the popular student-teacher model
[35, 37]. Insofar as the GEP holds, the neuronal and latent (cross-)covariances fully specify
the generalization error of the linear readout.
5.2
Geometric decomposition of generalization error across tasks
We prove a formula for the generalization error of the linear readout, averaged across different
binary classification tasks (i.e., different T vectors).
To do so, we consider the limiting
case in which the number of neurons, latent dimensions, and training samples are all large
and of comparable size. Furthermore, we assume that the covariance matrices given above
satisfy certain spectral properties (SM). Under these assumptions, we show that the average
generalization error, Eg, is a decreasing function of the four geometric terms. Formally, we
have
Eg = 1
π tan−1
s
π
2pc2PR(Ψ) + 1
f + 1
s −1

,
(3)
where we have introduced the total neural-latent correlation c, the signal-signal factorization
f, the signal-noise factorization s, and the dimension of the population responses as measured
by the participation ratio, PR(Ψ). These terms correspond to statistics of the covariance
matrices specified above:
17

c =
Tr(ΦΦ⊤)
Tr(Ψ)Tr(Ω)
(4)
PR(Ψ) = Tr(Ψ)2
Tr(Ψ2)
(5)
f =
Tr(ΦΦ⊤)2
Tr(Ω)Tr(Φ⊤ΦΩ−1Φ⊤Φ)
(6)
s =
Tr(ΦΦ⊤)2
Tr(Ω)Tr(Φ⊤(Ψ −ΦΩ−1Φ⊤)Φ)
(7)
Since the function F(w) = 1
π tan−1  √w −1

is strictly increasing, we can see that each of
these geometric terms should be made as large as possible. As discussed in the SM, the term
f measures the overall degree of orthogonality between coding directions, while s measures
the amount of noise that lies along the signal directions. The neural noise is described by
the noise covariance matrix, cov(x|z) = Ψ−ΦΩ−1Φ⊤, which appears in the definition above.
Importantly, this neural noise may include variability that is related to latent variables that
are not measured experimentally. Motivated by this observation, we describe how these two
factorization terms can be collapsed into a single factorization term in the SM. Note that in
writing Eq. (3), we have separated a term that depends on the number of training samples,
1/[c2 PR(Ψ)] from a term which is independent of the sample size, 1/f + 1/s. Thus we can
see that the correlation and dimension terms become less important as p grows.
5.3
Gaussian simulations
To test our theory on data points which violate the assumptions of our theory, we began
by sampling from a finite Gaussian model. Specifically, we drew latent variables, zµ, from
a multivariate Gaussian distribution whose covariance matrix had eigenvalues that decayed
as a power law with rate α. Specifically, we set: ωi = 5i−α, where α was the power law of
the spectrum, and ωi is the ith eigenvalue of the latent covariance. The neural responses
were then given by the formula xµ = Azµ for a random, n × d Gaussian matrix A with
i.i.d. elements, or by applying a whitening transform xµ = Ω−1/2zµ. We set n = 80 and
d = 40 for these simulations. For a fixed training set, we sampled Ntask = 300 task T vectors
and calculated the generalization error across all tasks using a set of new latent variables.
Finally, we averaged over this entire procedure 30 times to generate the markers in Fig 3.
5.4
Optimal codes
We derive which neuronal codes achieve the lowest generalization error, given a fixed num-
ber of samples to train on and a fixed latent structure. Specifically, we calculate which
neuron-neuron and neuron-latent covariance matrices, Ψ, Φ achieve the lowest multi-task
generalization error, given fixed a fixed latent covariance Ωand training set size p (SM). We
do this by optimizing Eq (3) with respect to Ψ and Φ subject to the constraint that the
entire covariance matrix between neurons and latents be positive semi-definite, a necessary
condition for the code to be realizable. Using this approach, we find that the left and right
18

singular vectors of Φ are the eigenvectors of Ψ and Ω, respectively for the optimal code.
This shows that independent directions in the latent space map directly onto independent
directions in the firing rate space (SM). Furthermore, we obtain the following simple formula
for the eigenvalues of the optimal neural code. Denoting the eigenvalues of Ψ and Ωas ψi
and ωi respectively, we have up to a permutation symmetry:
ψi = C
ωi
2pωi + π P
k ωk
,
(8)
where C is an arbitrary constant. As p grows, we can see that the spectrum becomes flatter,
reflecting the expansion strategy, while as p shrinks, the spectrum decays faster and faster,
reflecting the fact that less informative directions are compressed in the state space (Fig.
4c).
To validate our calculation, we numerically calculated the optimal code by optimizing
on the space of positive semi-definite matrices. Since the full covariance matrix is positive
semi-definite, there must exist matrices X1 and X2 such that
L =

Ω1/2
0
X1
X2

,
LLT =
Ω
ΦT
Φ
Ψ

(9)
The space of possible X matrices is unconstrained, so we simply optimize Eq (3) with respect
to the Xi matrices and calculate Ψ and Φ after the fact.
5.5
MLP Experiments
We used random and trained MLPs to test several predictions from our theory using explic-
itly non-Gaussian artificial neural response data. To generate these data, we first sampled
a set of d = 40 dimensional latent variables from a multivariate Gaussian distribution with
eigenvalues ωk = k−0.2. For these experiments, we sampled 5 · 105 latent variables indepen-
dently from this distribution as a training dataset. Using these latent variables we generated
a set of Nt = 500 tasks by randomly shattering the latent space. Latent variables were then
passed through a 3 layer perceptron with randomly initialized weights. We chose the size of
each intermediate layer to be twice the size of the previous one to ensure the dimension of the
representation did not decrease. Each layer was composed of a linear transform, followed by
a relu non-linearity (see SM Figs. 2&3 for results using a tanh non-linearity). After sampling
the task labels and passing the latents through the random MLP, we trained a 4 hidden-layer
MLP to predict task labels from the random MLP responses. The trained network was sim-
ilarly composed of linear-batchnorm-relu blocks, and we used the adam optimizer to train
the network [73] through a single epoch. This setup corresponds to a multi-task version of
the hidden manifold modeling framework studied in deep learning theory [36].
We evaluated the generalization error through layers and training using a new set of
latents and tasks. Specifically, we sampled 103 latent variables from the same distribution as
well as a new set of 300 randomly selected tasks. To calculate the theoretical generalization
error , we then used the network representations of these new latent variables to calculate
the geometric metrics and evaluate the theoretical generalization errors using Eq. (3) with
p = 300. To evaluate the empirical generalization error, we randomly split the new set of
19

latents into a set of 300 training points and 700 test points. This process was repeated to
calculate the average test error of the Hebb rule across each layer and time point in training.
Finally, we averaged over train/test splits to generate the markers shown in Fig. 5.
5.6
Macaque analyses
We drew from a publicly available dataset containing multi-unit recordings from V4 and IT
taken from 2 monkeys [39]. These recordings were taken as the monkeys viewed visual stimuli
as described in [39] and contained 88 V4 and 168 IT neural sites, though we reproduce all
results projecting IT and pixel responses down to 88 randomly chosen dimensions (SM Fig.
4). The stimuli for this task were drawn from a generative model in which a total of 64
objects coming from 8 categories were displayed against varying backgrounds. These images
were generated by varying d = 6 continuous latent variables that controlled the object size,
angle, and position. Latent variables were drawn iid from a mean zero uniform distribution.
We tested the linear decodability of these latent variables from the neural firing rates
using the scheme described in Sec. 5.1. For the raw pixels, we first carried out a Gaussian
random projection onto a 5, 000 dimensional space before applying this scheme. Specifically,
for each task condition and for each of the 8 object category types, we formed task labels for
Nt = 300 tasks by randomly shattering the latent space. For each condition and category
type, there were 320 examples per condition and object category type. We z-scored the
latent variables to ensure that there was no especially informative latent variable. We then
formed predictions using the supervised Hebbian readout described in Sec. 5.1 after mean
centering the neuronal firing rates. This procedure was repeated over 15 different train/test
data splits. To generate the results in Fig. 7, we then averaged the generalization error
across the Nt classification tasks, data splits, conditions, and image categories. Error bars
denote the standard error of the mean for the estimated generalization error across the c = 8
category types. In SM, we also show the geometry and generalization errors for each of the
categories individually (SM Figs. 5&6), as well as the error obtained by pooling all categories
together (SM Fig. 7).
Acknowledgements
We thank Thomas Yerxa, Chi-Ning Chou, Arnav Raha, and Jenelle Feather for helpful dis-
cussions and comments on an earlier version of this manuscript. This work was funded by the
Center for Computational Neuroscience at the Flatiron Institute of the Simons Foundation.
S.C. is also partially supported by a Sloan Research Fellowship, and a Klingenstein-Simons
Award. All experiments were performed on the high-performance computing cluster at the
Flatiron Institute.
References
[1] Hristos S. Courellis, Juri Mixha, Araceli R. Cardenas, Daniel Kimmel, Chrystal M.
Reed, Taufik A. Valiante, C. Daniel Salzman, Adam N. Mamelak, Stefano Fusi, and Ueli
20

Rutishauser. Abstract representations emerge in human hippocampal neurons during
inference behavior. bioRxiv, 2023.
[2] Ramon Nogueira, Chris C Rodgers, Randy M Bruno, and Stefano Fusi. The geometry of
cortical representations of touch in rodents. Nature Neuroscience, 26(2):239–250, 2023.
[3] W Jeffrey Johnston, Justin M Fine, Seng Bum Michael Yoo, R Becket Ebitz, and
Benjamin Y Hayden. Semi-orthogonal subspaces for value mediate a tradeoff between
binding and generalization. ArXiv, 2023.
[4] Silvia Bernardi, Marcus K Benna, Mattia Rigotti, J´erˆome Munuera, Stefano Fusi, and
C Daniel Salzman. The geometry of abstraction in the hippocampus and prefrontal
cortex. Cell, 183(4):954–967, 2020.
[5] Irina Higgins, Le Chang, Victoria Langston, Demis Hassabis, Christopher Summerfield,
Doris Tsao, and Matthew Botvinick. Unsupervised deep learning identifies semantic
disentanglement in single inferotemporal face patch neurons. Nature communications,
12(1):6456, 2021.
[6] Le Chang and Doris Y Tsao. The code for facial identity in the primate brain. Cell,
169(6):1013–1028, 2017.
[7] Timo Flesch, Keno Juechems, Tsvetomira Dumbalska, Andrew Saxe, and Christopher
Summerfield.
Orthogonal representations for robust context-dependent task perfor-
mance in brains and neural networks. Neuron, 110(7):1258–1270, 2022.
[8] W Jeffrey Johnston and Stefano Fusi. Abstract representations emerge naturally in
neural networks trained to perform multiple tasks. Nature Communications, 14(1):1040,
2023.
[9] Alexandra Libby and Timothy J Buschman. Rotational dynamics reduce interference
between sensory and memory representations.
Nature neuroscience, 24(5):715–726,
2021.
[10] Ramanujan Srinath, Amy M Ni, Claire Marucci, Marlene R Cohen, and David H
Brainard. Orthogonal neural representations support perceptual judgements of natural
stimuli. bioRxiv, pages 2024–02, 2024.
[11] Irina Higgins, David Amos, David Pfau, Sebastien Racaniere, Loic Matthey, Danilo
Rezende, and Alexander Lerchner. Towards a definition of disentangled representations.
arXiv preprint arXiv:1812.02230, 2018.
[12] Timothy EJ Behrens, Timothy H Muller, James CR Whittington, Shirley Mark, Alon B
Baram, Kimberly L Stachenfeld, and Zeb Kurth-Nelson. What is a cognitive map?
organizing knowledge for flexible behavior. Neuron, 100(2):490–509, 2018.
[13] Dmitriy Aronov, Rhino Nevers, and David W. Tank. Mapping of a non-spatial dimension
by the hippocampal–entorhinal circuit. Nature, 543(7647):719–722, March 2017.
21

[14] Eric B. Knudsen and Joni D. Wallis.
Hippocampal neurons construct a map of an
abstract value space. Cell, 184(18):4640–4650.e10, September 2021.
[15] Edward H. Nieh, Manuel Schottdorf, Nicolas W. Freeman, Ryan J. Low, Sam Lewallen,
Sue Ann Koay, Lucas Pinto, Jeffrey L. Gauthier, Carlos D. Brody, and David W. Tank.
Geometry of abstract learned knowledge in the hippocampus. Nature, 595(7865):80–84,
July 2021.
[16] Xiaojun Bao, Eva Gjorgieva, Laura K. Shanahan, James D. Howard, Thorsten Kahnt,
and Jay A. Gottfried. Grid-like Neural Representations Support Olfactory Navigation
of a Two-Dimensional Odor Space. Neuron, 102(5):1066–1075.e5, June 2019.
[17] Alexandra O. Constantinescu, Jill X. O’Reilly, and Timothy E. J. Behrens. Organizing
conceptual knowledge in humans with a gridlike code. Science, 352(6292):1464–1468,
June 2016.
[18] Paul S Muhle-Karbe, Hannah Sheahan, Giovanni Pezzulo, Hugo J Spiers, Samson Chien,
Nicolas W Schuck, and Christopher Summerfield. Goal-seeking compresses neural codes
for space in the human hippocampus and orbitofrontal cortex. Neuron, 111(23):3885–
3899, 2023.
[19] Edvard I Moser, Emilio Kropff, and May-Britt Moser. Place cells, grid cells, and the
brain’s spatial representation system. Annu. Rev. Neurosci., 31:69–89, 2008.
[20] Anne E Urai, Brent Doiron, Andrew M Leifer, and Anne K Churchland. Large-scale
neural recordings call for new insights to link brain and behavior. Nature neuroscience,
25(1):11–19, 2022.
[21] SueYeon Chung and LF Abbott. Neural population geometry: An approach for un-
derstanding biological and artificial neural networks. Current opinion in neurobiology,
70:137–144, 2021.
[22] SueYeon Chung, Daniel D Lee, and Haim Sompolinsky. Classification and geometry of
general perceptual manifolds. Physical Review X, 8(3):031003, 2018.
[23] Uri Cohen, SueYeon Chung, Daniel D Lee, and Haim Sompolinsky. Separability and ge-
ometry of object manifolds in deep neural networks. Nature communications, 11(1):746,
2020.
[24] Albert J Wakhloo, Tamara J Sussman, and SueYeon Chung. Linear classification of
neural manifolds with correlated variability. Physical Review Letters, 131(2):027301,
2023.
[25] Chi-Ning Chou, Luke Arend, Albert J Wakhloo, Royoung Kim, Will Slatton, and
SueYeon Chung.
Neural manifold capacity captures representation geometry, corre-
lations, and task-efficiency across species and behaviors. bioRxiv, pages 2024–02, 2024.
22

[26] Ben Sorscher, Surya Ganguli, and Haim Sompolinsky. Neural representational geometry
underlies few-shot concept learning. Proceedings of the National Academy of Sciences,
119(43):e2200800119, 2022.
[27] Gamaleldin F Elsayed, Antonio H Lara, Matthew T Kaufman, Mark M Churchland, and
John P Cunningham. Reorganization between preparatory and movement population
responses in motor cortex. Nature communications, 7(1):13239, 2016.
[28] Abigail A Russo, Sean R Bittner, Sean M Perkins, Jeffrey S Seely, Brian M London,
Antonio H Lara, Andrew Miri, Najja J Marshall, Adam Kohn, Thomas M Jessell,
et al. Motor cortex embeds muscle-like commands in an untangled population response.
Neuron, 97(4):953–966, 2018.
[29] Matthew G Perich, Juan A Gallego, and Lee E Miller. A neural population mechanism
for rapid learning. Neuron, 100(4):964–976, 2018.
[30] Frederic Lanore, N Alex Cayco-Gajic, Harsha Gurnani, Diccon Coyle, and R Angus
Silver. Cerebellar granule cell axons support high-dimensional representations. Nature
neuroscience, 24(8):1142–1150, 2021.
[31] Jason Manley, Sihao Lu, Kevin Barber, Jeffrey Demas, Hyewon Kim, David Meyer,
Francisca Mart´ınez Traub, and Alipasha Vaziri. Simultaneous, cortex-wide dynamics of
up to 1 million neurons reveal unbounded scaling of dimensionality with neuron number.
Neuron, 2024.
[32] Carsen Stringer, Marius Pachitariu, Nicholas Steinmetz, Matteo Carandini, and Ken-
neth D Harris. High-dimensional geometry of population responses in visual cortex.
Nature, 571(7765):361–365, 2019.
[33] Liang She, Marcus K Benna, Yuelin Shi, Stefano Fusi, and Doris Y Tsao. The neural
code for face memory. BioRxiv, pages 2021–03, 2021.
[34] Emmanouil Froudarakis, Uri Cohen, Maria Diamantaki, Edgar Y Walker, Jacob Reimer,
Philipp Berens, Haim Sompolinsky, and Andreas S Tolias. Object manifold geometry
across the mouse cortical visual hierarchy. BioRxiv, pages 2020–08, 2020.
[35] Elizabeth Gardner and Bernard Derrida. Three unfinished works on the optimal storage
capacity of networks. Journal of Physics A: Mathematical and General, 22(12):1983,
1989.
[36] Sebastian Goldt, Marc M´ezard, Florent Krzakala, and Lenka Zdeborov´a. Modeling the
influence of data structure on learning in neural networks: The hidden manifold model.
Physical Review X, 10(4):041044, 2020.
[37] Bruno Loureiro, Cedric Gerbelot, Hugo Cui, Sebastian Goldt, Florent Krzakala, Marc
Mezard, and Lenka Zdeborov´a. Learning curves of generic features maps for realistic
datasets with a teacher-student model.
Advances in Neural Information Processing
Systems, 34:18137–18151, 2021.
23

[38] A Engel and C Van den Broeck. Statistical Mechanics of Learning. Cambridge University
Press, 2005.
[39] Najib J Majaj, Ha Hong, Ethan A Solomon, and James J DiCarlo. Simple learned
weighted sums of inferior temporal neuronal firing rates accurately predict human core
object recognition performance. Journal of Neuroscience, 35(39):13402–13418, 2015.
[40] Sebastian Goldt, Bruno Loureiro, Galen Reeves, Florent Krzakala, Marc M´ezard, and
Lenka Zdeborov´a.
The gaussian equivalence of generative models for learning with
shallow neural networks. In Mathematical and Scientific Machine Learning, pages 426–
471. PMLR, 2022.
[41] Andrea Montanari, Feng Ruan, Basil Saeed, and Youngtak Sohn. Universality of max-
margin classifiers. arXiv preprint arXiv:2310.00176, 2023.
[42] Loic Matthey, Irina Higgins, Demis Hassabis, and Alexander Lerchner. dsprites: Dis-
entanglement testing sprites dataset. https://github.com/deepmind/dsprites-dataset/,
2017.
[43] Christian Keup and Moritz Helias. Origami in n dimensions: How feed-forward networks
manufacture linear separability, 2022.
[44] Ha Hong, Daniel LK Yamins, Najib J Majaj, and James J DiCarlo. Explicit information
for category-orthogonal object properties increases along the ventral stream. Nature
neuroscience, 19(4):613–622, 2016.
[45] Jack W Lindsey and Elias B Issa. Factorized visual representations in the primate visual
system and deep neural networks. bioRxiv, pages 2023–04, 2023.
[46] Mehrdad Jazayeri and Srdjan Ostojic. Interpreting neural computations by examining
intrinsic and embedding dimensionality of neural activity. Current opinion in neurobi-
ology, 70:113–120, 2021.
[47] Bruno B Averbeck, Peter E Latham, and Alexandre Pouget. Neural correlations, pop-
ulation coding and computation. Nature reviews neuroscience, 7(5):358–366, 2006.
[48] Ramon Bartolo, Richard C Saunders, Andrew R Mitz, and Bruno B Averbeck.
Information-limiting correlations in large neural populations. Journal of Neuroscience,
40(8):1668–1678, 2020.
[49] Marlene R Cohen and John HR Maunsell. Attention improves performance primarily
by reducing interneuronal correlations. Nature neuroscience, 12(12):1594–1600, 2009.
[50] Amy M Ni, Douglas A Ruff, Joshua J Alberts, Jen Symmonds, and Marlene R Cohen.
Learning and attention reveal a general relationship between population activity and
behavior. Science, 359(6374):463–465, 2018.
[51] Deep Ganguli and Eero P Simoncelli. Efficient sensory encoding and bayesian inference
with heterogeneous neural populations. Neural computation, 26(10):2103–2134, 2014.
24

[52] Joseph J Atick and A Norman Redlich. Towards a theory of early visual processing.
Neural computation, 2(3):308–320, 1990.
[53] Eizaburo Doi, Jeffrey L Gauthier, Greg D Field, Jonathon Shlens, Alexander Sher,
Martin Greschner, Timothy A Machado, Lauren H Jepson, Keith Mathieson, Deborah E
Gunning, et al. Efficient coding of spatial information in the primate retina. Journal of
Neuroscience, 32(46):16256–16264, 2012.
[54] Kimberly L Stachenfeld, Matthew M Botvinick, and Samuel J Gershman. The hip-
pocampus as a predictive map. Nature neuroscience, 20(11):1643–1653, 2017.
[55] Ching Fang, Dmitriy Aronov, LF Abbott, and Emily L Mackevicius. Neural learning
rules for generating flexible predictions and computing the successor representation.
Elife, 12:e80680, 2023.
[56] Ben Sorscher, Gabriel C Mel, Samuel A Ocko, Lisa M Giocomo, and Surya Ganguli.
A unified theory for the computational and mechanistic origins of grid cells. Neuron,
111(1):121–137, 2023.
[57] Ida Momennejad, Evan M Russek, Jin H Cheong, Matthew M Botvinick, Nathaniel Dou-
glass Daw, and Samuel J Gershman. The successor representation in human reinforce-
ment learning. Nature human behaviour, 1(9):680–692, 2017.
[58] Ida Momennejad. Learning structures: predictive representations, replay, and general-
ization. Current Opinion in Behavioral Sciences, 32:155–166, 2020.
[59] Marjorie Xie, Samuel P Muscinelli, Kameron Decker Harris, and Ashok Litwin-Kumar.
Task-dependent optimal representations for cerebellar learning. Elife, 12:e82914, 2023.
[60] Yotam Sagiv, Sebastian Musslick, Yael Niv, and Jonathan Cohen. Efficiency of learning
vs. processing: Towards a normative theory of multitasking. In Proceedings of the 40th
Annual Meeting of the Cognitive Science Society, 08 2018.
[61] Giovanni Petri, Sebastian Musslick, Biswadip Dey, Kayhan ¨Ozcimder, David Turner,
Nesreen K Ahmed, Theodore L Willke, and Jonathan D Cohen. Topological limits to the
parallel processing capability of network architectures. Nature Physics, 17(5):646–651,
2021.
[62] Guangyu Robert Yang, Madhura R Joglekar, H Francis Song, William T Newsome,
and Xiao-Jing Wang. Task representations in neural networks trained to perform many
cognitive tasks. Nature neuroscience, 22(2):297–306, 2019.
[63] Laura Driscoll, Krishna Shenoy, and David Sussillo. Flexible multitask computation in
recurrent networks utilizes shared dynamical motifs. bioRxiv, pages 2022–08, 2022.
[64] Baktash Babadi and Haim Sompolinsky. Sparseness and expansion in sensory represen-
tations. Neuron, 83(5):1213–1226, 2014.
25

[65] Ashok Litwin-Kumar, Kameron Decker Harris, Richard Axel, Haim Sompolinsky, and
LF Abbott. Optimal degrees of synaptic connectivity. Neuron, 93(5):1153–1164, 2017.
[66] Samuel P Muscinelli, Mark J Wagner, and Ashok Litwin-Kumar. Optimal routing to
cerebellum-like structures. Nature neuroscience, 26(9):1630–1641, 2023.
[67] Blake Bordelon, Abdulkadir Canatar, and Cengiz Pehlevan. Spectrum dependent learn-
ing curves in kernel regression and wide neural networks. In International Conference
on Machine Learning, pages 1024–1034. PMLR, 2020.
[68] Abdulkadir Canatar, Blake Bordelon, and Cengiz Pehlevan. Spectral bias and task-
model alignment explain generalization in kernel regression and infinitely wide neural
networks. Nature communications, 12(1):2914, 2021.
[69] Blake Bordelon and Cengiz Pehlevan. Population codes enable learning from few exam-
ples by shaping inductive bias. Elife, 11:e78606, 2022.
[70] Abdulkadir Canatar, Jenelle Feather, Albert J Wakhloo, and SueYeon Chung. A spec-
tral theory of neural prediction and alignment. In Thirty-seventh Conference on Neural
Information Processing Systems, 2023.
[71] Andrew M Saxe, James L McClelland, and Surya Ganguli. Exact solutions to the nonlin-
ear dynamics of learning in deep linear neural networks. arXiv preprint arXiv:1312.6120,
2013.
[72] Andrew M Saxe, James L McClelland, and Surya Ganguli. A mathematical theory of
semantic development in deep neural networks. Proceedings of the National Academy
of Sciences, 116(23):11537–11546, 2019.
[73] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization, 2017.
26

Supplemental material
Albert J. Wakhloo,
Will Slatton,
SueYeon Chung
Contents
1
Proof of generalization error formula
1
2
Decomposition of generalization error into geometric terms
8
3
Optimal geometry
11
4
Additional MLP analyses
14
5
Additional macaque analyses
17
1
Proof of generalization error formula
Here we state and prove our formula for the generalization error.
Theorem 1. Let xn,µ ∈Rn, zn,µ ∈Rd with 1 ≤µ ≤p be a sequence of random variables
drawn independently from a Gaussian distribution with mean zero and covariance matrices:
Cn =
Ψn
Φn
Φ⊤
n
Ωn

,
(1)
where Cn is a positive-definite matrix with sub-matrices Ψn ∈Rn×n, Ωn ∈Rd×d, Φn ∈Rn×d,
and the ratios d/n = α and p/n = β are held fixed. In addition, let xn,+, zn,+ be a pair
of samples drawn independently from the same distribution, and let, rn be a sequence of d-
dimensional vectors drawn uniformly from the surface of the sphere of radius
√
d. Assume
that there exist positive constants c1, c2 such that
c1
n ≤ψn,i, ωn,i, ϕn,i ≤c2
n ,
(2)
where we use lower-case Greek letters to denote the eigenvalues or singular values of the
respective matrices. Define the Hebbian readout w := 1
p
P
µ sgn(⟨zn,µ, rn⟩)⟨xn,µ, xn,+⟩, and let
Θ(·) denote the Heaviside step function. Under these assumptions the generalization error
is given by
1
arXiv:2402.16770v2  [q-bio.NC]  11 Apr 2024

Exn,zn,rn

Θ(−⟨rn, zn,+⟩⟨w, xn,+⟩)
	
(3)
= 1
π tan−1
s
Tr(Ωn)
π
pTr(Ψ2
n)Tr(Ωn) + 2Tr(Φ⊤
n ΨnΦn)]
2Tr(Φ⊤
n Φn)2
−1

+ O(n−1/2).
Proof. We start by defining the random variables
γµ
n = sgn(⟨zn,µ, rn⟩)⟨xn,+, xn,µ⟩,
(4)
so that the generalization error may be written
ErnExn,+,zn,+Exn,µ,zn,µΘ

−sgn(⟨zn,+rn⟩)
X
µ
γµ
n

.
(5)
Note that we have used Fubini’s theorem to separate the expectations over the training and
test points, as well as the teacher vector rn.
Our goal is now to show that the distribution of the random variable P
µ γµ
n is approxi-
mately Gaussian. We do this using the Berry-Esseen theorem, which allows us to carry out
the inner expectation in Eq. 5. We can obtain the relevant moments needed to apply this
theorem using the identity
Ep,q∼N(0,Σ)sgn(q)p =
r
2
πσκ,
Σ :=
ζ
κ
κ
σ

,
(6)
together with
Eh∼N(0,v)|h|3 = 2
√
2
√π v3/2.
(7)
The mean and variance are simply
Exn,µ,zn,µγµ
n =
s
2
πr⊤
n Ωnrn
x⊤
n,+Φnrn,
(8)
Exn,µ(γµ
n −Exn,µ,zn,µγµ
n)2 = x⊤
n,+Ψnxn,+ −2(x⊤
n,+Φnrn)2
πr⊤
n Ωnrn
.
(9)
It suffices to bound the third central moment as
Exn,µ,zn,µ|γµ
n −Exn,µ,zn,µγµ
n|3 ≤2
√
2
√π
 x⊤
n,+Ψnxn,+
3/2 + |Exn,µ,zn,µγµ
n|3.
(10)
2

We now define the following random variables:
σn := r⊤
n Ωnrn,
κn := x⊤
n,+Φnrn,
ζn := x⊤
n,+Ψnxn,+,
ηn := z⊤
n rn.
(11)
If we now let Fn(s) denote the cumulative distribution fnction of the standardized sum
s =
P
µ(γµ
n −Exn,µγµ
n)
q
p(ζn −2κ2n
πσn)
(12)
and denote by N(s) the standard normal c.d.f., the Berry-Esseen theorem [1] gives
Fn(s) −N(s)
 ≤
K
√p
 1 −
2κ2n
πσnζn
3/2
 1 + (x⊤
n,+Ψnxn,+)−3/2|Exn,µ,zn,µγµ
n|3
,
(13)
where K is a constant. We can see that the rightmost term contributes at most a constant
factor from
|Exn,µ,zn,µγµ
n|3
(x⊤
n,+Ψnxn,+)3/2 ≤
M|x⊤
n,+Φnrn|3
(r⊤
n Ωnrn)3/2(x⊤
n,+Ψnxn,+)3/2 ≤M ′
ϕ3
n,max
ω3/2
n,minψ3/2
n,min
= O(1),
(14)
for some positive constants M, M ′. Next we show that the entire right hand side of Eq. (13)
is O(n−1/2) using the following Lemma.
Lemma 2. Let C by a positive definite matrix with submatrices
C =
 Ψ
Φ
Φ⊤
Ω

,
(15)
with Ψ ∈Rn×n, Ω∈Rd×d, and Φ ∈Rn×d. Then for all h ∈Rn and r ∈Rd
(h⊤Φr)2
h⊤Ψhr⊤Ωr ≤1.
(16)
Proof. Let (x, z) be mean-zero jointly Gaussian vectors with covariance matrix C. It follows
that
(h⊤Φr)2 =
 E[⟨x, h⟩⟨r, z⟩])2
(17)
≤E[⟨x, h⟩2]E[⟨z, r⟩2]
(18)
= (h⊤Ψh)(r⊤Ωr).
(19)
3

We therefore obtain
2κ2
n
πσnζn
≤2
π,
(20)
so that the right hand side of Eq. (13) is O(n−1/2). Keeping only the leading order terms,
we obtain the following expression for the generalization error
1
2ErnExn,znerfc

sgn(ηn)κn
q
π
pσnζn(1 −ϵ)

+ O(n−1/2),
(21)
ϵ :=
2κ2
n
πσnζn
.
(22)
Note that we have dropped the + subscript from the test point and latent pair. We can deal
with the term ϵ ≤2/π by noting that
Exn,rn
2κ2
n
πσnζn
≤Es,rn
2(s⊤Ψ1/2Φnrn)2
πdψn,minωn,min
= O(n−1),
(23)
where ψn,min, ωn,min are the smallest eigenvalues of Ωn, Ψn, and s is a random vector drawn
uniformly from the surface of the unit sphere. From here we can simply expand to first order
to see that the ϵ term contributes at most O(n−1).
We now show that we may replace the quadratic form involving xn with its expected value.
To do this, we first note that we can focus on estimating the expectation over a region which
in which |ζn −Tr(Ψ2
n)| > cTr(Ψ2
n) for some 0 < c < 1 by applying the Hanson-Wright
inequality. This gives
Ern,xn,zn

erfc

sgn(ηn)κn
q
π
pσnTr(Ψ2
n)

−erfc
sgn(ηn)κn
q
π
pσnζn

(24)
≤Ern,xn,zn1{|ζn−Tr(Ψ2n)|<cTr(Ψ2n)}
erfc

sgn(ηn)κn
q
π
pσnTr(Ψ2
n)

−erfc
sgn(ηn)κn
q
π
pσnζn
 + O(e−g√n),
for some constant g that depends on the choice of c. We can then obtain the following bound
for the integrand in the region where |ζn −Tr(Ψ2
n)| < cTr(Ψ2
n) by the mean value theorem:
Ern,xn,zn1{|ζn−Tr(Ψ2n)|<cTr(Ψ2n)}
erfc

sgn(ηn)κn
q
π
pσnTr(Ψ2
n)

−erfc
sgn(ηn)κn
q
π
pσnζn

(25)
≤E
√pC|κn|
Tr(Ψ2
n)3/2
ζn −Tr(Ψ2
n)
 ≤
√pC
p
E[κ2
n]E|ζn −Tr(Ψ2
n)|2
Tr(Ψ2
n)3/2
.
(26)
for some constant C. We can now use the following Lemma to show that the right hand side
is O(n−1/2) in expectation:
4

Lemma 3. Let r ∈Rd be a vector distributed either uniformly on the surface of the sphere
of radius
√
d or according to a mean-zero Gaussian with covariance Id. Then for any matrix
A ∈Rd×d we have
E|r⊤Ar −Tr(A)| = O(||A||op) + O(||A||F),
(27)
E|r⊤Ar −Tr(A)|2 = O(||A||2
op) + O(||A||2
F)
(28)
where || · ||op is the operator norm induced by the Euclidean norm.
Proof. Applying the Hanson-Wright inequality for random vectors satisfying the convex
concentration property [2, 3] we obtain the first identity
Z
p(dr)|r⊤Ar −Tr(A)| =
Z ∞
0
dtP(|r⊤Ar −Tr(A)| > t)
(29)
≤
Z ∞
0
dt exp

−Ct2
||A||2
F

+ exp

−
ct
||A||op

= O(||A||F) + O(||A||op),
(30)
where C, c are positive constants. Similarly we have
Z
p(dr)|r⊤Ar −Tr(A)|2 = 2
Z ∞
0
dtP(|r⊤Ar −Tr(A)| > t)t = O(||A||2
F) + O(||A||2
op), (31)
yielding the second identity.
Applying this estimate and using the fact that Eκ2
n = Tr(Φ⊤
n ΨnΦn) = O(n−2) then shows
that the right hand side is O(n−1/2). We are therefore left with
1
2ErnEηn,κnerfc

sgn(ηn)κn
q
π
pσnTr(Ψ2
n)

+ O(n−1/2).
(32)
From our definitions, the variables ηn, κn follow a bivariate Gaussian distribution:
ηn, κn ∼N

0,

r⊤
n Ωnr
r⊤
n Φ⊤
n Φnrn
r⊤
n Φ⊤
n Φnrn
r⊤
n Φ⊤
n ΨnΦnrn
 
.
(33)
The inner expectation can now be carried out analytically. We start by integrating over
the conditional distribution κn|ηn, which is Gaussian with mean ηnr⊤
n Φ⊤
n Φnr/σn and vari-
ance r⊤
n Φ⊤
n ΨnΦnrn −(r⊤
n Φ⊤
n Φnrn)2/σn. To do this, we need the Gaussian integral over the
complementary error function
1
2
Z
dx
√
2πve−(x−m)2/(2v)erfc(cx) = 1
2erfc

mc
√
1 + 2vc2

.
(34)
This formula can be derived by considering the function
5

I(a, b) :=
Z
Dxerfc(ax + b),
(35)
where Dx is a standard Gaussian measure. This integral can be solved by differentiating
under the integral with respect to b, leading to Eq. (34). Applying this identity to Eq. (32)
we obtain
1
2ErnEηnerfc

|ηn|r⊤
n Φ⊤
n Φnrn
r⊤
n Ωnrn
q
π
pσnTr(Ψ2
n) + 2

r⊤
n Φ⊤
n ΨnΦnrn −(r⊤
n Φ⊤
n Φnrn)2
r⊤
n Ωnrn


.
(36)
We can now split the integral over the ηn into half Gaussian integrals over complementary
error functions. This allows us to invoke the relation
1
2
Z ∞
0
√
2dx
√π e−x2/2erfc(cx) = 1
π tan−1
 1
√
2c

.
(37)
This formula can similarly be derived by considering the function
J(c) :=
Z ∞
0
Dxerfc(cx)
(38)
and differentiating with respect to c. This leaves
1
πErn tan−1
s
r⊤
n Ωnrn
π
pTr(Ψ2
n)r⊤
n Ωnrn + 2r⊤
n Φ⊤
n ΨnΦnrn

2(r⊤
n Φ⊤
n Φnrn)2
−1

+ O(n−1/2).
(39)
Note that this gives the generalization error for a fixed task vector rn.
To carry out the final expectation, we expand these quadratic forms around their mean
values. To do this, let us introduce the O(1) variables
σn
1 = r⊤
n Ωnrn,
σn
2 = n2r⊤
n Φ⊤
n ΨnΦnrn,
σn
3 = nr⊤
n Φ⊤
n Φnrn,
(40)
and the function
G(σn) = tan−1
s
σn
1 (cσn
1 + 2n−2σn
2 )
2n−2(σn
3 )2
−1

,
(41)
where c := π
pTr(Ψn)2 = O(n−2). We start by noting that we can focus on estimating the
expectation over a region in which ||σn −Eσn|| < t for t > 0. To see this, note that by the
Hanson-Wright inequality:
ErnG(σn) = Ern1{||σn−Eσn||<t}G(σn) + O(e−c√n).
(42)
6

We can now bound the remaining error incurred by replacing σn with its expected value as:
|G(σn) −G(Eσn)| ≤
X
i
sup
||ξ−Eσn||<t
|∂iG(ξ)||σn
i −Eσn
i |
(43)
=
X
i
sup
||ξ−Eσn||<t

ξ1(cξ1 + 2n−2ξ2)
2n−2(ξ3)2
−1
−1/2ξ1(cξ1 + 2n−2ξ2)
2n−2(ξ3)2
−1
(44)
(45)
∂i
ξ1(cξ1 + 2n−2ξ2)
2n−2(ξ3)2
|σn
i −Eσn
i |
(46)
≤M
sup
||ξ−Eσn||<t

ξ1(cξ1 + 2n−2ξ2)
2n−2(ξ3)2
−1

−1/2 X
i
|σn
i −Eσn
i |,
(47)
for some constant M. We can now use the following lemma to give a uniform bound on the
remaining term:
Lemma 4. Let C by a positive definite matrix with submatrices
C =
 Ψ
Φ
Φ⊤
Ω

,
(48)
with Ψ ∈Rn×n, Ω∈Rd×d, and Φ ∈Rn×d. Then
Tr(Ω)Tr(Φ⊤ΨΦ)
Tr(Φ⊤Φ)2
≥1.
(49)
Proof. The claim follows from an application of the Cauchy-Schwarz inequality:
Tr(Φ⊤Φ)2 = (E⟨Φ⊤x, z⟩)2 ≤E||Φ⊤x||2E||z||2 = Tr(Φ⊤ΨΦ)Tr(Ω),
(50)
where the expectation is taken over zero-mean, jointly Gaussian vectors (x, z) which have a
covariance matrix C.
From this lemma and our assumptions on the spectrum, it follows that there exists a constant
M ′ > 0 that does not depend on n such that
⟨σn
1 ⟩(c⟨σn
1 ⟩+ 2n−2⟨σn
2 ⟩)
2n−2⟨σn
1 ⟩2
≥1 + M ′,
(51)
where we use brackets to denote the expectation. Hence if we choose t sufficiently small, we
obtain
|G(σn) −G(Eσn)| ≤M ′′ X
i
|σn
i −Eσn
i |,
(52)
where M ′′ is a positive constant. By Lemma 3, the right hand side is O(n−1/2).
7

Replacing these quadratic forms with their means, we obtain the final result:
1
π tan−1
s
Tr(Ωn)
π
pTr(Ψ2
n)Tr(Ωn) + 2Tr(Φ⊤
n ΨnΦn)]
2Tr(Φ⊤
n Φn)2
−1

+ O(n−1/2).
(53)
2
Decomposition of generalization error into geometric
terms
Having obtained a closed-form solution for generalization error in the thermodynamic limit,
we now seek to rewrite this formula as a monotone function of interpretable geometric terms.
We start by splitting Eq. (53) into a p-dependent and p-independent term:
1
π tan−1
 s
π
2p
Tr(Ωn)Tr(Ψ2
n)Tr(Ωn)
(Tr(ΦnΦ⊤
n ))2
+ Tr(Ωn)Tr(Φ⊤
n ΨnΦn)
(Tr(ΦnΦ⊤
n ))2
−1
!
.
(54)
If we re-write the p-dependent term as
π
2p
Tr(Ωn) Tr(Ψn)
Tr(ΦnΦ⊤
n )
2
· Tr(Ψ2
n)
Tr(Ψn)2,
(55)
we can notice the participation ratio of the neural activity
PR(Ψ) = Tr(Ψn)2
Tr(Ψ2
n) ,
(56)
which is a well-known measure of the dimensionality of neural activity. We collect the rest
of the p-dependent term into our correlation term
c =
Tr(ΦnΦ⊤
n )
Tr(Ωn) Tr(Ψn),
(57)
and the generalization error now reduces to
1
π tan−1
 s
π
2pc2 PR(Ψ) + Tr(Ωn)Tr(Φ⊤
n ΨnΦn)
(Tr(ΦnΦ⊤
n ))2
−1
!
.
(58)
To interpret the correlation term, note that the numerator Tr(ΦnΦ⊤
n ) can be expanded as
Tr(ΦnΦ⊤
n ) =
d
X
i=1
n
X
j=1
E[zixj]2,
(59)
which is a sum-of-squares of the covariance between all pairs of latent variables zi and all
components xj of neural activity. The denominator of c is just the total latent variance
8

Tr(Ωn) times the total variance Tr(Ψn) of neural activity. The correlation term c can there-
fore be viewed as a generalization of the well-known Pearson correlation between two scalar
variables to capture the total correlation strength between the d-dimensional latent variable
z and the n-dimensional neural activity x. In particular, when n, d = 1, the total correlation
c reduces to the square of the standard Pearson correlation between z and x.
We call (the inverse of) the remaining p-independent term the alignment term a:
a =
(Tr(ΦnΦ⊤
n ))2
Tr(Ωn) Tr(Φ⊤
n ΨnΦn).
In the main text (and as described below), the alignment term is subdivided further, but we
can also interpret a as a whole. We can expand the main term in the denominator as
Tr(Φ⊤
n ΨnΦn) =
d
X
i=1
ϕ⊤
i Ψnϕi =
d
X
i=1
∥ϕi∥2 Var(ˆϕi · x),
(60)
where ϕi is the ith column of Φn, and Var(ˆϕi ·x) is the variance of the total neural response x
projected onto the direction of ϕi. Using the fact that the average neural activity, conditioned
on a specific value of the latents is given by E[x|z] = ΦΩ−1z, we can see that for a diagonal
Ω, the column ˆϕi corresponds to the coding direction of the latent variable zi. Additionally,
the norm ∥ϕi∥reflects the coding strength of the latent variable zi. The numerator of a can
be written as
(Tr(ΦnΦ⊤
n ))2 =
 
d
X
i=1
∥ϕi∥2
!
,
(61)
and therefore depends only on the norms of the coding directions ϕi but not on their angles.
Holding the norms of each ϕi fixed, we can see that the alignment term a prefers that
each coding direction ˆϕi be positioned along a direction of minimal response variance in the
neural state space. In other words, the only variance along a direction ˆϕi should be caused by
variations in the corresponding latent variable. Intuitively, the alignment term a encourages
arranging the coding directions of independent latent variables in such a way that the signal
for these variables do not interfere with one another and so that all latents are coded along
directions with low intrinsic noise. We make this notion more formal below.
Now let us subdivide the alignment term a as in the main text. We partition the total neu-
ral variance Ψ into stimulus-driven variance and stimulus-independent variance. Recall that
in our setup, latents z and neural responses x are drawn from a joint Gaussian distribution
(x, z) ∼N

0,
Ω
Φ⊤
Φ
Ψ

.
(62)
This is equivalent to a model in which we first sample z ∼N(0, Ω) and then sample x as
x = ΦΩ−1z + ε, where ε is stimulus-independent Gaussian noise with a covariance given by
Ψ −ΦΩ−1Φ⊤,
(63)
9

which is the covariance of the neural responses conditional on the latents, cov(x|z). This
suggests a partitioning of the total neural variance Ψ into stimulus-independent variance,
H := Ψ −ΦΩ−1Φ⊤, and stimulus-driven variance, cov(x) −H = ΦΩ−1Φ⊤.
We can now decompose the p-independent term of our generalization error formula as
Tr(Ωn) Tr(Φ⊤
n ΨnΦn)
(Tr(ΦnΦ⊤
n ))2
= Tr(Ωn) Tr(Φ⊤
n HΦn)
(Tr(ΦnΦ⊤
n ))2
+ Tr(Ωn) Tr(Φ⊤
n (ΦΩ−1Φ⊤)Φn)
(Tr(ΦnΦ⊤
n ))2
.
(64)
We call the inverse of the first term signal-noise factorization s:
1
s = Tr(Ωn) Tr(Φ⊤
n HΦn)
(Tr(ΦnΦ⊤
n ))2
.
(65)
This interpretation can be understood by noting that
Tr(Φ⊤
n HΦn) =
d
X
i=1
ϕ⊤
i Hϕi =
d
X
i=1
∥ϕi∥2 · ˆϕ⊤
i H ˆϕi.
(66)
We can see that ˆϕ⊤
i H ˆϕi gives the projection of stimulus-independent noise corrupting the
signal for the ith latent variable, so Tr(Φ⊤
n HΦn) measures the amount of stimulus-independent
noise corrupting the signal directions for each latent variable. In particular, note that s is
maximized when there is no stimulus-independent noise in the neural dimensions used for
coding the latent variables.
Finally, we call the inverse of the second part of the p-independent term the signal-signal
factorization:
1
f = Tr(Ωn) Tr(Φ⊤
n (ΦΩ−1Φ⊤)Φn)
(Tr(ΦnΦ⊤
n ))2
.
(67)
To understand this term, assume without loss of generality that
Ω= diag(ω1, . . . , ωD),
(68)
which can be accomplished by making an orthogonal change of variables. Now observe that
Tr(Φ⊤
n ΦnΩ−1Φ⊤
n Φn) =
d
X
i=1
d
X
j=1
1
ωi
⟨ϕi, ϕj⟩2.
(69)
The numerator of 1/f is a weighted sum of the dot products between all pairs of coding
vectors ϕi and ϕj for latents zi and zj. By contrast, the denominator (Tr(ΦnΦ⊤
n ))2 can be
expanded to
(Tr(ΦnΦ⊤
n ))2 =
 
d
X
i=1
⟨ϕi, ϕi⟩
!2
,
(70)
which only captures the signal strengths ∥ϕi∥without regard for the angles between pairs of
distinct signal directions. With the norms ∥ϕi∥fixed, f is maximized by making all coding
directions orthogonal. That is, f is maximized by a factorized code.
10

The interpretation of our factorization term f is somewhat complicated by the fact that it
depends on the relative norms ∥ϕi∥of the columns of Φ in addition to the coding directions
ˆϕi. Some such dependence is inevitable. For example, if one coding direction ϕi has norm
very near zero (i.e., has near zero signal strength to begin with), then the angle it makes
with other coding directions is irrelevant to generalization error and should not enter into f.
However, we will demonstrate numerically that practically all of the variation we observe in
f between the layers of artificial neural networks (Fig. 5 and SM Fig. 2), during training
of artificial neural networks (Fig. 6 and SM Fig. 3), and between brain regions (Fig. 7 and
SM Fig. 7) is driven by changes in the angles between coding directions. To this end, we
obtain a simplified analog of 1/f by assuming that all signal directions have the same norm
and assuming that Ωhas a flat spectrum. This yields the simplified quantity
Pd
i=1
Pd
j=1⟨ˆϕi, ˆϕj⟩
d
,
(71)
which manifestly only depends on the angles between coding directions. In fact, this simpli-
fied version of 1/f is linearly related to the mean-squared cosine similarity between all pairs
of distinct coding directions:
P
i̸=j⟨ˆϕi, ˆϕj⟩
d(d −1)
.
(72)
We therefore compare the quantity 1/f with the mean-squared cosine similarity between all
pairs of distinct coding directions (Eq. 72) and show that the two are almost exactly linearly
related for all representations we analyzed in both artificial neural networks and neural data
(SM Fig. 1).
3
Optimal geometry
Here we derive our formula for the spectrum of the optimal neural representation and show
that it is disentangled. The argument is not fully rigorous, but it matches numerical results
very well. Our goal is to calculate
arg min
Ψ,Φ
1
π tan−1
q
Tr(Ω)
 π
P Tr(Ψ2)Tr(Ω) + 2
 Tr(Φ⊤ΨΦ) −(Tr[ΦΦ⊤])2
Tr(Ω)

√
2Tr(ΦΦ⊤)

,
(73)
subject to
 Ω
Φ
Φ⊤
Ψ

≻0.
(74)
For a positive matrix Ω, this condition is equivalent to
Ψ ≻0,
Ψ −ΦΩ−1Φ⊤≻0.
(75)
11

Figure 1: Almost all variation in our factorization term is driven by changing angles between
coding directions. We compare the reciprocal 1/f of factorization to the mean-square cosine
angle between the coding directions for all pairs of distinct latent variables (see Eq. (72)),
which only depends on the angles between coding directions. Despite our factorization term
f nominally depending on the norms ∥ϕi∥of the coding directions, the almost perfect linear
relationship between 1/f and mean-square cosine similarity shows that we can effectively
consider f to only depend on the angles between coding directions across all the experiments
presented in this work: (a) Representations across layers of random and trained ReLU
networks (Fig. 5). (b) Representations across layers of random and trained tanh networks
(SM Fig. 2). (c) Representations taken across training epochs in trained ReLU networks
(Fig. 6). (d) Representations across training epochs in trained tanh networks (SM Fig. 3).
(e) Representations across pixels, V4, and IT in macaque electrophysiological data (measures
computed separately for each object category, Fig. 7). (f) Representations across pixels, V4,
and IT in macaque electrophysiological data (measures pooled across object categories, SM
Fig. 7).
12

Rewriting the argument of the tan−1 function, we can see that the objective may be written
as
arg min
Ψ,Φ
π
P Tr(Ψ2) Tr(Ω) + 2 Tr(Φ⊤ΨΦ)
Tr(ΦΦ⊤)2
.
(76)
Our approach is to first minimize Eq.
(76) holding Tr(ΦΦ⊤) = γ fixed and show that
the objective is ultimately invariant to the choice of γ. The optimization can be done by
introducing the Lagrangian
L(Ψ, Φ, ρ, ζ) = cTr(Ψ2) + 2Tr(Φ⊤ΨΦ) + ζ(γ −Tr(ΦΦ⊤))
(77)
−
Z
dnxρ(x)Tr(xx⊤(Ψ −ΦΩ−1Φ⊤)),
(78)
where the ρ(x) are KKT multipliers enforcing the positive definite constraint, ζ is a Lagrange
multiplier, and c := π
P Tr(Ω) . Note that we do not explicitly enforce the positivity constraint
on Ψ, as we find that this is unnecessary. The KKT equations are
∂ΨL = 2cΨ + 2ΦΦ⊤−⟨xx⊤⟩ρ = 0,
(79)
∂ΦL = 4ΨΦ + 2⟨xx⊤⟩ΦΩ−1 + ζΦ = 0,
(80)
δρL = Tr(xx⊤(Ψ −ΦΩ−1Φ⊤)) ≥0,
(81)
ρ(x)Tr(xx⊤(Ψ −ΦΩ−1Φ⊤)) = 0,
(82)
Tr(ΦΦ⊤) = γ.
(83)
We try for a solution with Ψ = ΦΩ−1Φ⊤. Since the variance of x conditional on z is given
by Ψ −ΦΩ−1Φ⊤, this is equivalent to looking for solutions in which the neural code has no
signal-unrelated noise. Then Eq. (79) gives ⟨xx⊤⟩ρ = 2Φ(cΩ−1 + I)Φ⊤. Using these two
formulae, Eq. (80) gives the condition
Φ(4Ω−1Φ⊤Φ + 4(cΩ−1 + I)Φ⊤ΦΩ−1 + ζI) = 0.
(84)
This suggests looking for a Φ whose right singular vectors are aligned with those of Ω. The
eigenvalues of Φ are then given by:
ϕ2
i =
γ
P
i
ω2
i
2ωi+ π
P Tr(Ω)
ω2
i
2ωi + π
pTr(Ω).
(85)
From the assumption that Ψ = Φ⊤Ω−1Φ, we can see that the eigenvectors of Ψ and the left
singular vectors of Φ can be chosen however we want, so long as they are the same. The
eigenvalues of Ψ are then simply
13

ψi =
γ
P
i
ω2
i
2ωi+ π
p Tr(Ω)
ωi
2ωi + π
pTr(Ω).
(86)
Plugging this solution into the objective, we indeed find that γ drops out of the picture
entirely. Since we are free to choose γ, we obtain that the optimal representation satisfies
the following properties: (1) The eigenvectors of Ψ are the left singular vectors of Φ and
(2) The eigenvectors of Ωare the right singular vectors of Φ. This implies that the optimal
representation is disentangled–i.e., that independent directions in the latent space map onto
independent directions in the neural space.
To see why, consider the covariance between an independent direction in the neural space
and an independent direction in the latent space. If we let o be the eigenvector of Ψ cor-
responding to the neural direction and u an eigenvector of Ωcorresponding to the latent
direction, we have
E[⟨o, x⟩⟨u, z⟩] = u⊤Φo.
(87)
For the optimal representation, this is either 0 or equal to a singular value of Φ. Since for
Gaussian variables, uncorrelated variables are independent, this argument shows that each
independent direction in the neural space is independent from all but one latent dimension.
Thus the optimal representation described here is disentangled.
Finally, the eigen/singular-values of the matrices are given by
ϕ2
i ∝
ω2
i
2pωi + πTr(Ω),
(88)
ψi ∝
ωi
2pωi + πTr(Ω).
(89)
This gives the optimal representation’s spectrum. As stated in the main text, we can see
that as p grows relative to d, the optimal representation’s spectrum becomes increasingly
flat, indicating that more variance in the neural state space is being allocated to the less
informative directions in the latent space.
4
Additional MLP analyses
Here we redo the analyses presented in the main text using a tanh non-linearity in the
random and trained networks.
14

Figure 2: Generalization error and geometry of the tanh MLPs. (a-b) Generalization error for
the random and trained network. (c-f) Layer-wise geometry of the networks. (g-j) Average
change in the geometry when passing from a linear to a tanh layer. We find similar trends
as with the relu non-linearity.
15

Figure 3: Dynamics of generalization error (a) and geometry (b-e) through training using
networks with a tanh non-linearity. In analogy to the main text, we show 2 early and 2 late
layers. At each of the early/late stages, we show a linear and tanh layer.
16

5
Additional macaque analyses
Figure 4: Geometry and generalization error using the same number of units across regions
[4]. Here, we repeat the analysis presented in the main text, only we project the pixels and
IT data down to 88 dimensions using Gaussian random projection. (a) Generalization error
for the three representations. (b-e) Geometric terms. We find qualitatively similar trends to
those presented in the main text.
17

Figure 5: Match between theoretical and empirical generalization error across each of the
8 individual categories. In the main text, we presented the theoretical and empirical gen-
eralization errors averaged over individual object categories. Here we show that the theory
predicts the empirical generalization error well across individual object categories.
Figure 6: Geometry across individual object categories. Here, we present the distribution
of geometric terms, calculated on subsets of the data corresponding to each of the 8 object
categories. Note that the terms in the main text correspond to averages over these 8 values.
18

Figure 7: Generalization error and geometry of the pooled monkey data. Here, we pool data
from all categories together, rather than considering data subsets corresponding to stimuli
coming from the same object category as done in the main text. We can see that the trends
are largely the same and that the theory predicts the empirical error. (a) Theoretical and
empirical generalization error. (b-e) Geometry of the pooled data.
19

References
[1] Martin Raiˇc. A multivariate Berry–Esseen theorem with explicit constants. Bernoulli,
25(4A):2824 – 2853, 2019.
[2] Radoslaw Adamczak. A note on the hanson-wright inequality for random vectors with
dependencies. 2015.
[3] Roman Vershynin. High-dimensional probability: An introduction with applications in
data science, volume 47. Cambridge university press, 2018.
[4] Najib J Majaj, Ha Hong, Ethan A Solomon, and James J DiCarlo.
Simple learned
weighted sums of inferior temporal neuronal firing rates accurately predict human core
object recognition performance. Journal of Neuroscience, 35(39):13402–13418, 2015.
20

