Bayesian Nonparametrics in Real-World Applications:
Statistical Machine Translation and Language Modelling
on Big Datasets
Yarin Gal
yg279@cam.ac.uk
16th of May 2013
Based in part on the lecture notes by Dr. Phil Blunsom

Outline
Introduction
Parallel corpora
Models of translation
Word Alignment
Basic Bayesian approaches
Bayesian Nonparametric approaches
Conclusions
2 of 70

Outline
Introduction
Parallel corpora
Models of translation
Word Alignment
Basic Bayesian approaches
Bayesian Nonparametric approaches
Conclusions
3 of 70

Introduction
The confusion of tongues:
4 of 70

Introduction
Task: make sense of foreign text like:
▶AI-hard: ultimately reasoning and world knowledge required
▶Statistical machine translation: Learn how to translate from data
5 of 70

Introduction
Warren Weaver memorandum, July 1949:
Thus it may be true that the way to
translate from Chinese to Arabic, or from
Russian to Portuguese, is not to attempt
the direct route, shouting from tower to
tower. Perhaps the way is to descend,
from each language, down to the common
base of human communication—the real
but as yet undiscovered universal
language—and—then re-emerge by
whatever particular route is convenient.
6 of 70

Introduction
The Machine Translation Pyramid:
German
English
German 
Syntax
English
Syntax
German 
Semantics
English 
Semantics
Interlingua
7 of 70

Introduction
The Machine Translation Pyramid:
German
English
German 
Syntax
English
Syntax
German 
Semantics
English 
Semantics
Interlingua
7 of 70

Introduction
The Machine Translation Pyramid:
German
English
German 
Syntax
English
Syntax
German 
Semantics
English 
Semantics
Interlingua
7 of 70

Introduction
Rule Based Machine Translation (RBMT):
taken from www.linguatec.net
8 of 70

Introduction
Warren Weaver memorandum, July 1949:
It is very tempting to say that a book
written in Chinese is simply a book written
in English which was coded into the
“Chinese code.” If we have useful methods
for solving almost any cryptographic
problem, may it not be that with proper
interpretation we already have useful
methods for translation?
9 of 70

Introduction
The Machine Translation Pyramid:
German
English
German 
Syntax
English
Syntax
German 
Semantics
English 
Semantics
Interlingua
10 of 70

Introduction: Fire the linguists
Fred Jelinek, 1988:
Every time I ﬁre a linguist, the
performance of the recognizer goes up.
11 of 70

Outline
Introduction
Parallel corpora
Models of translation
Word Alignment
Basic Bayesian approaches
Bayesian Nonparametric approaches
Conclusions
12 of 70

Parallel Corpora
Rosetta Stone:
13 of 70

Parallel Corpora
Iliad:
14 of 70

Parallel Corpora
UN Website:
15 of 70

Outline
Introduction
Parallel corpora
Models of translation
Word Alignment
Basic Bayesian approaches
Bayesian Nonparametric approaches
Conclusions
16 of 70

Models of translation
Given an input sentence, we have to predict an output translation
Natuerlich hat John spass am Spiel.
⇓
Of course John has fun with the game.
▶Since the set of possible output sentences is too large, we need to
construct the translation according to some decomposition of the
translation process
17 of 70

Models of translation
The Noisy Channel Model
P(English|French) = P(English) × P(French|English)
P(French)
arg max
e
P(e|f) = arg max
e
[P(e) × P(f|e)]
▶Bayes’ rule is used to reverse the translation probabilities
▶the analogy is that the French is English transmitted over a noisy
channel
▶we can then use techniques from statistical signal processing and
decryption to translate
18 of 70

Models of translation
The Noisy Channel Model
French
English
Statistical
Translation table
Statistical
Language Model
Bilingual Corpora
French/English
Monolingual Corpora
English
Je ne veux pas travailler
I not work
I do not work
I don't want to work
I no will work
I don't want to work
...
19 of 70

Language models
(Bi-gram) Language Modelling
P(e) = P(e0, e1, . . . , e|e|)
≈
|e|
∏
i=0
P(ei|ei−1)
▶We can approximate the probability of seeing an English word ei
conditioned only on the previous word ei−1.
▶These conditional probabilities can be estimated from monolingual
corpora.
20 of 70

Language models
(Bi-gram) Language Modelling: The Iliad
Sing, O goddess, the anger of Achilles son of Peleus,
that brought countless ills upon the Achaeans. Many a
brave soul did it send hurrying down to Hades, and
many a hero did it yield a prey to dogs and vultures, for
so were the counsels of Jove fulﬁlled from the day on
which the son of Atreus, king of men, and great
Achilles, ﬁrst fell out with one another.
▶P(x|y) = count(x,y)
count(y)
21 of 70

Language models
(Bi-gram) Language Modelling: The Iliad
Sing, O goddess, the anger of Achilles son of Peleus,
that brought countless ills upon the Achaeans. Many a
brave soul did it send hurrying down to Hades, and
many a hero did it yield a prey to dogs and vultures, for
so were the counsels of Jove fulﬁlled from the day on
which the son of Atreus, king of men, and great
Achilles, ﬁrst fell out with one another.
▶P(x|the) = count(x,the)
count(the)
▶P(son|the) = count(son,the)
count(the)
= 1
5
21 of 70

Language models
(Bi-gram) Language Modelling: The Iliad
Sing, O goddess, the anger of Achilles son of Peleus,
that brought countless ills upon the Achaeans. Many a
brave soul did it send hurrying down to Hades, and
many a hero did it yield a prey to dogs and vultures, for
so were the counsels of Jove fulﬁlled from the day on
which the son of Atreus, king of men, and great
Achilles, ﬁrst fell out with one another.
▶P(x|the) = count(x,the)
count(the)
▶P(son|the) = count(son,the)
count(the)
= 1
5
▶P(king|the) = count(king,the)
count(the)
= 0
5 ?
21 of 70

Language models
Solution: smoothing and interpolation with shorter sequences of words
Interpolated Kneser-Ney discounting language model
u - a sequence of l words
cuw - number of observations of the sequence u followed by the word w
π(u) - u without the left most word
Pu(w) = max(0, cuw −dl)
cu
+ dltu
cu
Pπ(u)(w)
where cu is the number of observations of the sequence u, tu is the
number of unique words following the sequence u, P∅is a uniform
distribution over all words, and dl depends on length l.
▶Shorter sequences of words will have higher weight in the
interpolation if uw is sparse
22 of 70

Language models
(Bi-gram) Interpolated Kneser-Ney: The Iliad
Sing, O goddess, the anger of Achilles son of Peleus,
that brought countless ills upon the Achaeans. Many a
brave soul did it send hurrying down to Hades, and
many a hero did it yield a prey to dogs and vultures, for
so were the counsels of Jove fulﬁlled from the day on
which the son of Atreus, king of men, and great
Achilles, ﬁrst fell out with one another.
▶Py(x) = max(0,cy,x−d1)
cy
+ d1ty
cy Pπ(y)(x)
23 of 70

Language models
(Bi-gram) Interpolated Kneser-Ney: The Iliad
Sing, O goddess, the anger of Achilles son of Peleus,
that brought countless ills upon the Achaeans. Many a
brave soul did it send hurrying down to Hades, and
many a hero did it yield a prey to dogs and vultures, for
so were the counsels of Jove fulﬁlled from the day on
which the son of Atreus, king of men, and great
Achilles, ﬁrst fell out with one another.
▶Pthe(x) = max(0,cthe,x−d1)
cthe
+ d1tthe
cthe Pπ(the)(x)
▶Pthe(son) = max(0,cthe,son−d1)
cthe
+ d1tthe
cthe Pπ(the)(son)
23 of 70

Language models
(Bi-gram) Interpolated Kneser-Ney: The Iliad
Sing, O goddess, the anger of Achilles son of Peleus,
that brought countless ills upon the Achaeans. Many a
brave soul did it send hurrying down to Hades, and
many a hero did it yield a prey to dogs and vultures, for
so were the counsels of Jove fulﬁlled from the day on
which the son of Atreus, king of men, and great
Achilles, ﬁrst fell out with one another.
▶Pthe(x) = max(0,cthe,x−d1)
cthe
+ d1tthe
cthe Pπ(the)(x)
▶Pthe(son) = 1−d1
5
+ 5d1
5 Pϵ(son)
23 of 70

Language models
(Bi-gram) Interpolated Kneser-Ney: The Iliad
Sing, O goddess, the anger of Achilles son of Peleus,
that brought countless ills upon the Achaeans. Many a
brave soul did it send hurrying down to Hades, and
many a hero did it yield a prey to dogs and vultures, for
so were the counsels of Jove fulﬁlled from the day on
which the son of Atreus, king of men, and great
Achilles, ﬁrst fell out with one another.
▶Pthe(x) = max(0,cthe,x−d1)
cthe
+ d1tthe
cthe Pπ(the)(x)
▶Pthe(son) = 1−d1
5
+ 5d1
5 (max(0,cson−d0)
c
+ P∅(son))
23 of 70

Language models
(Bi-gram) Interpolated Kneser-Ney: The Iliad
Sing, O goddess, the anger of Achilles son of Peleus,
that brought countless ills upon the Achaeans. Many a
brave soul did it send hurrying down to Hades, and
many a hero did it yield a prey to dogs and vultures, for
so were the counsels of Jove fulﬁlled from the day on
which the son of Atreus, king of men, and great
Achilles, ﬁrst fell out with one another.
▶Pthe(x) = max(0,cthe,x−d1)
cthe
+ d1tthe
cthe Pπ(the)(x)
▶Pthe(son) = 1−d1
5
+ 5d1
5 (2−d0
72 + 1
T)
23 of 70

Language models
(Bi-gram) Interpolated Kneser-Ney: The Iliad
Sing, O goddess, the anger of Achilles son of Peleus,
that brought countless ills upon the Achaeans. Many a
brave soul did it send hurrying down to Hades, and
many a hero did it yield a prey to dogs and vultures, for
so were the counsels of Jove fulﬁlled from the day on
which the son of Atreus, king of men, and great
Achilles, ﬁrst fell out with one another.
▶Pthe(x) = max(0,cthe,x−d1)
cthe
+ d1tthe
cthe Pπ(the)(x)
▶Pthe(king) = 0
5 + 5d1
5 Pϵ(king) = 5d1
5 (1−d0
72 + 1
T)
23 of 70

Language models
Language modelling in Machine Translation:
▶very important!
▶5-gram models are now commonplace
▶Such models require lots of data to estimate; we routinely use
billions of words of English
▶Smoothing is crucial for these higher order n-gram models
24 of 70

Models of translation
Word-based translation
Original statistical machine translation models (1990s):
break down translation to the word level
25 of 70

Models of translation
Phrase-based translation
Morgen
ﬂiege
ich
nach
Kanada
zur
Konferenz
Current state of the art:
map larger chunks of words (huge mapping tables)
26 of 70

Models of translation
Phrase-based translation
Morgen
ﬂiege
ich
nach
Kanada
zur
Tomorrow
Konferenz
Current state of the art:
map larger chunks of words (huge mapping tables)
26 of 70

Models of translation
Phrase-based translation
Morgen
ﬂiege
ich
nach
Kanada
zur
Tomorrow
I
Konferenz
Current state of the art:
map larger chunks of words (huge mapping tables)
26 of 70

Models of translation
Phrase-based translation
Morgen
ﬂiege
ich
nach
Kanada
zur
Tomorrow
I
will
ﬂy
Konferenz
Current state of the art:
map larger chunks of words (huge mapping tables)
26 of 70

Models of translation
Phrase-based translation
Morgen
ﬂiege
ich
nach
Kanada
zur
Tomorrow
I
will
ﬂy
to
the
conference
Konferenz
Current state of the art:
map larger chunks of words (huge mapping tables)
26 of 70

Models of translation
Phrase-based translation
Morgen
ﬂiege
ich
nach
Kanada
zur
Tomorrow
I
will
ﬂy
to
the
conference
Konferenz
in
Canada
Current state of the art:
map larger chunks of words (huge mapping tables)
26 of 70

Models of translation
Advantages of phrase-based approach:
▶improved modelling of multi-word translation units
▶increased context
▶permits idioms and non-compositional phrases
▶eases search and reliance on the language model
27 of 70

Models of translation
Phrase extraction:
Je
ne
veux
pas
travailler
I
do
not
want
to
work
28 of 70

Models of translation
Phrase extraction:
Je
ne
veux
pas
travailler
I
do
not
want
to
work
▶Use a word-based translation model to annotate the parallel corpus
with word-alignments
28 of 70

Models of translation
Phrase extraction:
Je
ne
veux
pas
travailler
I
do
not
want
to
work
▶⟨Je, I ⟩, ⟨veux, want to ⟩, ⟨travailler, work ⟩
28 of 70

Models of translation
Phrase extraction:
Je
ne
veux
pas
travailler
I
do
not
want
to
work
▶⟨Je, I ⟩, ⟨veux, want to ⟩, ⟨travailler, work ⟩, ⟨ne veux pas, do
not want to ⟩
28 of 70

Models of translation
Phrase extraction:
Je
ne
veux
pas
travailler
I
do
not
want
to
work
▶⟨Je, I ⟩, ⟨veux, want to ⟩, ⟨travailler, work ⟩, ⟨ne veux pas, do
not want to ⟩, ⟨ne veux pas travailler, do not want to work ⟩
28 of 70

Models of translation
Phrase extraction:
Je
ne
veux
pas
travailler
I
do
not
want
to
work
▶⟨Je, I ⟩, ⟨veux, want to ⟩, ⟨travailler, work ⟩, ⟨ne veux pas, do
not want to ⟩, ⟨ne veux pas travailler, do not want to work ⟩, ⟨Je
ne veux pas, I do not want to ⟩
28 of 70

Models of translation
Phrase extraction:
Je
ne
veux
pas
travailler
I
do
not
want
to
work
▶⟨Je, I ⟩, ⟨veux, want to ⟩, ⟨travailler, work ⟩, ⟨ne veux pas, do
not want to ⟩, ⟨ne veux pas travailler, do not want to work ⟩, ⟨Je
ne veux pas, I do not want to ⟩, ⟨Je ne veux pas travailler, I do
not want to work ⟩
28 of 70

Outline
Introduction
Parallel corpora
Models of translation
Word Alignment
Basic Bayesian approaches
Bayesian Nonparametric approaches
Conclusions
29 of 70

Word Alignment (IBM Model 1)
A simple generative model for p(F|E) is derived by introducing a latent
variable A into the conditional probability:
p(F, A|E) =
p(J|I)
(I + 1)J
J
∏
j=1
p(fj|eaj),
▶F and E are the input (source) and output (target) sentences of
length J and I respectively,
▶A is a vector of length J consisting of integer indexes into the
target sentence, known as the alignment.
To learn this model the EM algorithm is used to ﬁnd the MLE values
for the parameters p(fj|eaj).
▶For the EM update we need to calculate the conditional probability
of an alignment p(A|E, F) = p(F,A|E)
p(F|E)
30 of 70

Word Alignment (IBM Model 1)
Marginalising out A in p(F, A|E) gives the required denominator:
p(F|E) =
∑
A
p(F, A|E)
=
I
∑
a1=0
I
∑
a2=0
· · ·
I
∑
aJ=0
p(F, A|E)
=
p(J|I)
(I + 1)J
I
∑
a1=0
I
∑
a2=0
· · ·
I
∑
aJ=0
J
∏
j=1
p(fj|eaj)
Rather conveniently we can swap the sum and product in the last line
to get an equation that is tractable to compute:
=
p(J|I)
(I + 1)J
J
∏
j=1
I
∑
i=0
p(fj|ai).
The result is that we can calculate the counts in O(J × I) rather than
O(I + 1)J.
31 of 70

Word Alignment (IBM Model 1)
Limitations of this simple word alignment model:
▶The structure of sentences is not modelled, words align independently of
each other,
▶The position of words with a sentence is not modelled, obviously words
near the start of the source sentence are more likely to align to words
near the start of the target sentence,
▶The alignment is asymmetric, a target word may align to multiple source
words, but a source word may only align to a single target,
▶and many others ...
These limitations mean that this model does not work well as a translation
model on it’s own, however it is currently used as the ﬁrst step in learning
more complicated models by online translation providers such as Google and
Microsoft.
32 of 70

Word Alignment (HMM Model)
A more accurate generative model for p(F|E) is derived by introducing
dependencies between alignment positions.
The HMM alignment model
P(F, A|E)
=
P(l|m) ×
l∏
j=1
(P(aj|aj−1, l) × P(fj|eaj))
▶We can use the Forward-Backward algorithm for tractable training
of this model
33 of 70

Word Alignment (HMM Model)
Alignment in this model can be found by jumping over the English
sentence words and emitting foreign words.
HMM alignment model
.
.
.
Mary (1)
.
.
slapped (2)
.
P(2|1, l = 5)
.
.
slapped (2)
.
P(2|2, l = 5)
.
.
slapped (2)
.
P(2|2, l = 5)
.
.
the (3)
.
P(3|2, l = 5)
.
.
the (3)
.
P(3|3, l = 5)
.
.
witch (5)
.
P(5|3, l = 5)
.
.
green (4)
.
P(4|5, l = 5)
.
Maria
.
P(Maria|Mary)
.
dio
.
P(dio|slapped)
.
una
.
P(una|slapped)
.
botefada
.
P(botefada|slapped)
.
a
.
P(a|the)
.
la
.
P(la|the)
.
bruja
.
P(bruja|witch)
.
verde
.
P(verde|green)
Aligning the sentence pair (“Mary slapped the green witch”, “Maria dio
una bofetada a la bruja verde”)
34 of 70

Word Alignment (Fertility based models)
Another class of alignment models is the fertility based models.
These models follow more of a linguistic approach than the previous
ones that used mathematical conveniences.
▶We treat the alignment as a function from the source sentence
positions i to Bi ⊂{1, ..., m} where the Bi’s form a partition of the
set {1, ..., m},
▶We deﬁne the fertility of the English word i to be φi = |Bi|, the
number of foreign words it generated,
▶And Bi,k refers to the kth word of Bi from left to right.
We allow for additional, spurious, words to be generated by introducing
the NULL word at the beginning of the English sentence.
35 of 70

Word Alignment (Fertility based models)
Probability model:
P(F, A|E) =p(B0|B1, ..., Bl) ×
l∏
i=1
p(Bi|Bi−1, ei)
×
l∏
i=0
∏
j∈Bi
p(fj|ei)
2 main models belong to this class: IBM model 3 and IBM model 4.
For model 3 the dependence on previous alignment sets is ignored and
the probability p(Bi|Bi−1, ei) is modelled as
p(Bi|Bi−1, ei) = p(φi|ei)φi!
∏
j∈Bi
p(j|i, m),
36 of 70

Word Alignment (Fertility based models)
Whereas for model 4 it is modelled using two HMMs:
p(Bi|Bi−1, ei) =p(φi|ei) × p=1(Bi,1 −⊙(Bi−1)|·)
×
φi
∏
k=2
p>1(Bi,k −Bi,k−1|·)
For both these models the spurious word generation is controlled by a
binomial distribution:
p(B0|B1, ..., Bl) =
(m −φ0
φ0
)
(1 −p0)m−2φ0pφ0
1
1
φ0!
for some parameters p0 and p1.
37 of 70

Word Alignment (Fertility based models)
Models 3 and 4 word alignment
38 of 70

Word Alignment (Fertility based models)
Limitations:
▶Inference in models 3 and 4 is intractable
▶We know of no eﬃcient way to avoid the explicit summation over
all alignments in the EM algorithm in the fertility-based alignment
models
▶To circumvent this, the counts are collected only over a small
neighbourhood of good alignments
▶To keep the training fast, we consider only a small fraction of all
alignments.
▶Sparsity is not handled
39 of 70

Outline
Introduction
Parallel corpora
Models of translation
Word Alignment
Basic Bayesian approaches
Bayesian Nonparametric approaches
Conclusions
40 of 70

Basic Bayesian approaches
The alignment models mentioned have underpinned the majority of
statistical machine translation systems for almost twenty years.
▶They oﬀer principled probabilistic formulation and (mostly)
tractable inference
▶There are many open source packages implementing them
▶Giza++ – one of the dominant implementations,
▶employs a variety of exact and approximate EM algorithms
However –
41 of 70

Basic Bayesian approaches
However –
▶The parametric approach results in a signiﬁcant number of
parameters to be tuned
▶Intractable summations over alignments for models 3 and 4
▶Usually approximated using restricted alignment neighbourhoods
▶Shown to return alignments with probabilities well below the true
maxima
▶Sparse contexts are not handled
Many alternative approaches to word alignment have been proposed,
and largely failed to dislodge the IBM approach.
42 of 70

Basic Bayesian approaches
However –
▶The parametric approach results in a signiﬁcant number of
parameters to be tuned
▶Intractable summations over alignments for models 3 and 4
▶Usually approximated using restricted alignment neighbourhoods
▶Shown to return alignments with probabilities well below the true
maxima
▶Sparse contexts are not handled
Many alternative approaches to word alignment have been proposed,
and largely failed to dislodge the IBM approach.
What can we do instead?
42 of 70

Basic Bayesian approaches
One possible solution:
Dirichlet prior
Put a Dirichlet prior over the categorical distribution for the word
translation and alignment transition probabilities of the HMMs used in
the diﬀerent models:
te ∼Dir(Θe)
fj|a, e, T ∼Categorical(teaj)
aj|α ∼Dir(α)
aj+1|aj, aj ∼Categorical(aj)
▶Captures sparsity by using small values for the hyper-parameter
43 of 70

Basic Bayesian approaches
Several Bayesian inference mechanisms have also been recently adapted
for the word alignment models training:
▶Variational Bayes (2012)
▶Collapse Variational Bayes (2013)
Using the BLEU metric, we can see the improvement in translation
quality as more advanced techniques are used.
44 of 70

Basic Bayesian approaches
Several Bayesian inference mechanisms have also been recently adapted
for the word alignment models training:
▶Variational Bayes (2012)
▶Collapse Variational Bayes (2013)
Using the BLEU metric, we can see the improvement in translation
quality as more advanced techniques are used.
We can use the real-world application of Statistical
Machine Translation for the assessment of diﬀerent
inference techniques!
44 of 70

Basic Bayesian approaches
  EM (5,5)   
  VB (5,5)   
  CVB (3,3)  
27
27.25
27.5
27.75
28
28.25
28.5
28.75
29
29.25
29.5
29.75
30
BLEU
BLEU scores of diﬀerent systems translating from Chinese to English
45 of 70

Basic Bayesian approaches
Limitations of the variational approaches:
▶Still have a signiﬁcant number of parameters to tune
▶In the case of models 3 and 4, still approximating using alignment
neighbourhoods
Can we marginalise over all parameters?
46 of 70

Basic Bayesian approaches
Limitations of the variational approaches:
▶Still have a signiﬁcant number of parameters to tune
▶In the case of models 3 and 4, still approximating using alignment
neighbourhoods
Can we marginalise over all parameters?
▶Gibbs sampling has been implemented in 2011 for IBM model 1 to
use a fully Bayesian approach.
46 of 70

Basic Bayesian approaches
Limitations of the variational approaches:
▶Still have a signiﬁcant number of parameters to tune
▶In the case of models 3 and 4, still approximating using alignment
neighbourhoods
Can we marginalise over all parameters?
▶Gibbs sampling has been implemented in 2011 for IBM model 1 to
use a fully Bayesian approach.
We can still do better.
46 of 70

Outline
Introduction
Parallel corpora
Models of translation
Word Alignment
Basic Bayesian approaches
Bayesian Nonparametric approaches
Conclusions
47 of 70

Bayesian Nonparametric approaches
Several smoothing techniques have been proposed in language
modelling over the years.
▶Add-one smoothing (1920) – adds one to all counts
48 of 70

Bayesian Nonparametric approaches
Several smoothing techniques have been proposed in language
modelling over the years.
▶Add-one smoothing (1920) – adds one to all counts
▶Good-Turing smoothing (1953) – improves over this by using the
frequency of singletons to estimate the frequency of zero-count
bigrams
48 of 70

Bayesian Nonparametric approaches
Several smoothing techniques have been proposed in language
modelling over the years.
▶Add-one smoothing (1920) – adds one to all counts
▶Good-Turing smoothing (1953) – improves over this by using the
frequency of singletons to estimate the frequency of zero-count
bigrams
▶Interpolated Kneser-Ney (1995) – a further improvement that
includes absolute discounting
Pu(w) = max(0, cuw −d|u|)
cu
+ d|u|tu
cu
Pπ(u)(w)
48 of 70

Bayesian Nonparametric approaches
Several smoothing techniques have been proposed in language
modelling over the years.
▶Add-one smoothing (1920) – adds one to all counts
▶Good-Turing smoothing (1953) – improves over this by using the
frequency of singletons to estimate the frequency of zero-count
bigrams
▶Interpolated Kneser-Ney (1995) – a further improvement that
includes absolute discounting
Pu(w) = max(0, cuw −d|u|)
cu
+ d|u|tu
cu
Pπ(u)(w)
▶one of the most commonly used modern N-gram smoothing
methods in the NLP community
48 of 70

Bayesian Nonparametric approaches
Several smoothing techniques have been proposed in language
modelling over the years.
▶Add-one smoothing (1920) – adds one to all counts
▶Good-Turing smoothing (1953) – improves over this by using the
frequency of singletons to estimate the frequency of zero-count
bigrams
▶Interpolated Kneser-Ney (1995) – a further improvement that
includes absolute discounting
Pu(w) = max(0, cuw −d|u|)
cu
+ d|u|tu
cu
Pπ(u)(w)
▶one of the most commonly used modern N-gram smoothing
methods in the NLP community
▶Was discovered in 2006 to corresponds exactly to a well-know
stochastic process in the Bayesian Nonparametric community: the
hierarchical Pitman-Yor process.
48 of 70

Bayesian Nonparametric approaches
We can deﬁne the Pitman-Yor process by describing how to draw from
this process:
The Pitman-Yor process
Draws from the Pitman-Yor process G1 ∼PY(d, θ, G0) with a discount
parameter 0 ≤d < 1, a strength parameter θ > −d, and a base
distribution G0, are constructed using a Chinese restaurant process as
follows:
Xc+1|X1, ..., Xc ∼
t
∑
k=1
ck −d
θ + c
δyk + θ + td
θ + c
G0
Where ck denotes the number of Xis (tokens) assigned to yk (a type)
and t is the total number of yks drawn from G0.
49 of 70

Bayesian Nonparametric approaches
The hierarchical Pitman-Yor process is simply a Pitman-Yor process
where the base distribution is itself a Pitman-Yor process.
The hierarchical Pitman-Yor process
Denoting a context of atoms u as (wi−l, ..., wi−1), the hierarchical
Pitman-Yor process is deﬁned using the above deﬁnition of the
Pitman-Yor process by:
wi ∼Gu
Gu ∼PY(d|u|, θ|u|, Gπ(u))
...
G(wi−1) ∼PY(d1, θ1, G∅)
G∅∼PY(d0, θ0, G0)
where π(u) = (wi−l+1, ..., wi−1) is the suﬃx of u, |u| denotes the
length of context u, and G0 is a base distribution (usually uniform over
all words).
50 of 70

Bayesian Nonparametric approaches
Comparing this to interpolated Kneser-Ney discounting language model,
we see that Kneser-Ney is simply a hierarchical Pitman-Yor process
with parameter θ set to zero and a constraint of one table tuw = 1:
Interpolated Kneser-Ney discounting language model
Pu(w) = max(0, cuw −d|u|)
cu
+ d|u|tu
cu
Pπ(u)(w)
where cuw is the number of observations of the sequence u followed by
the word w, cu is the number of observations of the sequence u itself,
and tu is the number of unique words following the sequence u.
The hierarchical Pitman-Yor process
Pu(w) = cuw −d|u|tuw
θ + cu
+ θ + d|u|tu
θ + cu
Pπ(u)(w)
▶Modiﬁed Kneser-Ney uses diﬀerent values of discounts for diﬀerent
counts
51 of 70

Bayesian Nonparametric approaches
Bayesian Nonparametric approaches have been in use in
NLP since the 90’s!
52 of 70

Bayesian Nonparametric approaches
We can take advantage of the smoothing and interpolation with shorter
contexts properties of the hierarchical Pitman-Yor (PY) process, and
use it in word alignment as well.
Reminder: Model 1 generative story
P(F, A|E) = p(m|l) ×
m
∏
i=1
p(ai)p(fi|eai)
Where p(ai) =
1
l+1 is uniform over all alignments and
p(fi|eai) ∼Categorical.
▶F and E are the input (source) and output (target) sentences of
length J and I respectively,
▶A is a vector of length J consisting of integer indexes into the
target sentence – the alignment.
53 of 70

Bayesian Nonparametric approaches
Re-formulating the model to use the hierarchical PY process instead of
the categorical distributions, we get:
PY Model 1 generative story
ai|m ∼Gm
0
fi|eai ∼Heai
Heai ∼PY(H∅)
H∅∼PY(H0)
▶fi and ai are the i’th foreign word and its alignment position,
▶eai is the English word corresponding to alignment position ai,
▶m is the lengths of the foreign sentence.
54 of 70

Bayesian Nonparametric approaches
Following this approach, we can re-formulate the HMM alignment
model as well to use the hierarchical PY process instead of the
categorical distributions.
Reminder: HMM alignment model generative story
P(F,A|E) =
p(m|l) ×
m
∏
i=1
p(ai|ai−1, m) × p(fi|eai)
▶fi and ai are the i’th foreign word and its alignment position,
▶eai is the English word corresponding to alignment position ai,
▶m and l are the lengths of the foreign and English sentences
respectively.
55 of 70

Bayesian Nonparametric approaches
We replace the categorical distribution for the transition p(ai|ai−1, m)
with a hierarchical PY process with a longer sequence of alignment
positions in the conditional
PY HMM alignment model generative story
ai|ai−1, m ∼Gm
ai−1
Gm
ai−1 ∼PY(Gm
∅)
Gm
∅∼PY(Gm
0 )
▶Unique distribution for each foreign sentence length
▶Condition the position on the previous alignment position,
backing-oﬀto the HMM’s stationary distribution over alignment
positions
56 of 70

Bayesian Nonparametric approaches
Unlike previous approaches that ran into diﬃculties extending models 3
and 4, we can extend them rather easily by just replacing the
categorical distributions.
▶The inference method that we use, Gibbs sampling, circumvents
the intractable sum approximation of other inference methods
▶The use of the hierarchical PY process allows us to incorporate
phrasal dependencies into the distribution
57 of 70

Bayesian Nonparametric approaches
Reminder: Models 3 and 4 generative story
P(F, A|E) =p(B0|B1, ..., Bl) ×
l∏
i=1
p(Bi|Bi−1, ei) ×
l∏
i=0
∏
j∈Bi
p(fj|ei)
For model 3 the dependence on previous alignment sets is ignored and
the probability p(Bi|Bi−1, ei) is modelled as
p(Bi|Bi−1, ei) = p(φi|ei)φi!
∏
j∈Bi
p(j|i, m),
whereas in model 4 it is modelled using two HMMs:
p(Bi|Bi−1, ei) =p(φi|ei) × p=1(Bi,1 −⊙(Bi−1)|·)
×
φi
∏
k=2
p>1(Bi,k −Bi,k−1|·)
58 of 70

Bayesian Nonparametric approaches
Replacing the categorical priors with hierarchical PY process ones, we
set the translation and fertility probabilities p(φi|ei) ∏
j∈Bi p(fj|ei) using
a common prior that generates translation sequences.
PY models 3 and 4 generative story
(f1, ..., fφi)|ei ∼Hei
Hei ∼PY(HFT
ei )
HFT
ei ((f1, ..., fφi)) = HF
ei(φi)
∏
j
HT
(fj−1,ei)(fj)
HF
ei ∼PY(HF
∅)
HF
∅∼PY(HF
0)
HT
(fj−1,ei) ∼PY(HT
ei)
HT
ei ∼PY(HT
∅)
HT
∅∼PY(HT
0 )
▶We used superscripts for the indexing of words which do not have
to occur sequentially in the sentence
59 of 70

Bayesian Nonparametric approaches
We generate sequences instead of individual words and fertilities, and
fall-back onto these only in sparse cases.
Example
Aligning the English sentence “I don’t speak French” to its French
translation “Je ne parle pas français”, the word “not” will generate the
phrase (“ne”, “pas”), which will later on be distorted into its place
around the verb.
▶The distortion probability for model 3, p(j|i, m), is modelled as
depending on the position of the source word i and its class
▶Interpolating for sparsity
▶The same way the HMM model backs-oﬀto shorter sequences
▶Similarly for the two HMMs in model 4.
60 of 70

Bayesian Nonparametric approaches
How does this model compare to the EM trained models?
1
1>H
1>H>3 1>H>3>4
26.0
26.5
27.0
27.5
28.0
28.5
29.0
29.5
BLEU
Chinese -> English Pipeline
PY-IBM
Giza++
Figure: BLEU scores of pipelined
Giza++ and pipelined PY-IBM
translating from Chinese into
English
HMM Model
Model 4
20
21
22
23
24
25
26
27
28
BLEU
Chinese -> English
PY-IBM
Giza++ 10 iter.
Giza++
Figure: BLEU scores of Giza++’s
and PY-IBM’s HMM model and
model 4 translating from Chinese
into English
61 of 70

Bayesian Nonparametric approaches
Limitations
▶The use of Gibbs sampling for inference in this model is slow
▶On bi-corpora limited in size (∼500K sentence pairs) the training
takes 12 hours, compared to one hour for the EM model
▶More suitable for language pairs with high divergence – captures
information that is otherwise lost
62 of 70

Outline
Introduction
Parallel corpora
Models of translation
Word Alignment
Basic Bayesian approaches
Bayesian Nonparametric approaches
Conclusions
63 of 70

Conclusions
Arabic →English
ﺑﻐﺪﺍﺩ 
1
-
1
 ) ﺍﻓﺐ ( - ﺫﻛﺮﺕ ﻭﻛﺎﻟﺔ ﺍﻻﻧﺒﺎﺀ ﺍﻟﻌﺮﺍﻗﻴﺔ ﺍﻟﺮﺳﻤﻴﺔ ﺍﻥ ﻧﺎﺋﺐ ﺭﺋﻴﺲ
 ﻣﺠﻠﺲ ﻗﻴﺎﺩﺓ ﺍﻟﺜﻮﺭﺓ ﻓﻲ ﺍﻟﻌﺮﺍﻕ ﻋﺰﺓ ﺍﺑﺮﺍﻫﻴﻢ ﺍﺳﺘﻘﺒﻞ ﺍﻟﻴﻮﻡ ﺍﻻﺭﺑﻌﺎﺀ ﻓﻲ ﺑﻐﺪﺍﺩ
 ﺭﺋﻴﺲ ﻣﺠﻠﺲ ﺍﺩﺍﺭﺓ ﺍﳌﺮﻛﺰ ﺍﻟﺴﻌﻮﺩﻱ ﻝ- ﺗﻄﻮﻳﺮ ﺍﻟﺼﺎﺩﺭﺍﺕ ﻋﺒﺪ ﺍﻟﺮﺣﻤﻦ ﺍﻟﺰﺍﻣﻞ .
?
64 of 70

Conclusions
Arabic →English
ﺑﻐﺪﺍﺩ 
1
-
1
 ) ﺍﻓﺐ ( - ﺫﻛﺮﺕ ﻭﻛﺎﻟﺔ ﺍﻻﻧﺒﺎﺀ ﺍﻟﻌﺮﺍﻗﻴﺔ ﺍﻟﺮﺳﻤﻴﺔ ﺍﻥ ﻧﺎﺋﺐ ﺭﺋﻴﺲ
 ﻣﺠﻠﺲ ﻗﻴﺎﺩﺓ ﺍﻟﺜﻮﺭﺓ ﻓﻲ ﺍﻟﻌﺮﺍﻕ ﻋﺰﺓ ﺍﺑﺮﺍﻫﻴﻢ ﺍﺳﺘﻘﺒﻞ ﺍﻟﻴﻮﻡ ﺍﻻﺭﺑﻌﺎﺀ ﻓﻲ ﺑﻐﺪﺍﺩ
 ﺭﺋﻴﺲ ﻣﺠﻠﺲ ﺍﺩﺍﺭﺓ ﺍﳌﺮﻛﺰ ﺍﻟﺴﻌﻮﺩﻱ ﻝ- ﺗﻄﻮﻳﺮ ﺍﻟﺼﺎﺩﺭﺍﺕ ﻋﺒﺪ ﺍﻟﺮﺣﻤﻦ ﺍﻟﺰﺍﻣﻞ .
Baghdad 1-1 (AFP) - ofﬁcial Iraqi news agency reported that vice-chairman of 
the revolution command council Izzat Ibrahim received in Iraq on Wednesday 
in Baghdad, board chairman of the Saudi center for developing exports Abdel 
Rahman Al-Zamil.
▶Statistical machine translation works!
65 of 70

Conclusions
Chinese →English
加拿大与欧盟和澳洲一样 都在十一月二十八日关闭它们 
的大使馆,并在本周稍早重新开放。
Canada and the EU and Australia have closed on 28 November at the same 
as the Chinese embassy in their earlier this week, and re-opening up.
▶Statistical machine translation works … sometimes!
66 of 70

Conclusions
▶Statistical machine translation is a fully functional commercial
technology
▶Lots of linguistic challenges remain:
▶long distance reordering
▶complex morphology
▶underspeciﬁcation
▶Lots of theoretical and engineering challenges to be explored:
▶approximate search for intractable models
▶automatic learning of syntactic and semantic structures
▶eﬃciently dealing with massive quantities of data
▶Lots of room for improvement with latest Bayesian research
▶Many potential research projects!
▶Real-world application for the evaluation of new techniques!
67 of 70

References 1:
▶Daniel Jurafsky, James H. Martin, An Introduction to Natural
Language Processing, Computational Linguistics and Speech
Recognition, Second edition.
▶Brown, P. F., Della Pietra, S. A., Della Pietra, V. J., and Mercer,
R. L., The mathematics of statistical machine translation:
Parameter estimation, Computational Linguistics, 19(2), 263–311
(1993).
▶Stephan Vogel, Hermann Ney, Christoph Tillmann, HMM-based
word alignment in statistical translation, Proceedings of the 16th
conference on Computational linguistics, August 05-09, 1996,
Copenhagen, Denmark.
▶Franz Josef Och, Hermann Ney, A Systematic Comparison of
Various Statistical Alignment Models, Computational Linguistics,
v.29 n.1, p.19-51, March 2003.
▶Coskun Mermer, Murat Saraclar Bayesian Word Alignment for
Statistical Machine Translation, In Proceedings of ACL HLT, 2011.
68 of 70

References 2:
▶Yee Whye Teh, A hierarchical Bayesian language model based on
Pitman-Yor processes, Proceedings of the 21st International
Conference on Computational Linguistics and the 44th annual
meeting of the Association for Computational Linguistics,
p.985-992, Sydney, Australia, July 17-18, 2006.
▶Y. W. Teh, A Bayesian Interpretation of Interpolated Kneser-Ney
NUS School of Computing Technical Report TRA2/06, Technical
Report TRA2/06, School of Computing, National University of
Singapore, 2006.
▶Riley, Darcey and Gildea, Daniel, Improving the IBM alignment
models using variational Bayes, Proceedings of the 50th Annual
Meeting of the Association for Computational Linguistics: Short
Papers - Volume 2, ACL ’12, 2012
▶Gal, Yarin and Blunsom, Phil, A Systematic Bayesian Treatment of
the IBM Alignment Models, Proceedings of the 2013 Conference of
the North American Chapter of the Association for Computational
Linguistics: Human Language Technologies, June, 2013
69 of 70

Questions?
Questions?
70 of 70

Human Evaluation
There are two dominant approaches to the subjective evaluation of
automatic translation:
▶Scoring (1–5) individual sentences based on:
▶adequacy: does it preserve the meaning?
▶ﬂuency: is it real language?
▶Comparing sentences produced by two diﬀerent systems
▶binary comparison: is the sentence output from system A better
than that from system B?
▶ranking: rank the outputs of X systems?
Eliciting such evaluations is slow, expensive, and because human judges
often don’t agree, unreliable. However human evaluation remains the
gold standard for comparing translation models.
71 of 70

Human Evaluation
How would you rank this translation?
Source: 欧盟办事处与澳洲大使馆在同一建筑内
Candidate: the chinese embassy in australia and the eu representative
oﬃce in the same building
Reference Translations:
1. the eu oﬃce and the australian embassy are housed in the same
building
72 of 70

Automatic Evaluation
Ngram overlap metrics:
Source: 欧盟办事处与澳洲大使馆在同一建筑内
Candidate: the chinese embassy in australia and the eu representative
oﬃce in the same building
Reference Translations:
1. the eu oﬃce and the australian embassy are housed in the same
building
2. the european union oﬃce is in the same building as the australian
embassy
3. the european union ’s oﬃce and the australian embassy are both
located in the same building
4. the eu ’s mission is in the same building with the australian
embassy
73 of 70

Automatic Evaluation
Ngram overlap metrics: 1-gram precision p1 = 11
14
Source: 欧盟办事处与澳洲大使馆在同一建筑内
Candidate: the chinese embassy in australia and the eu representative
oﬃce in the same building
Reference Translations:
1. the eu oﬃce and the australian embassy are housed in the same
building
2. the european union oﬃce is in the same building as the australian
embassy
3. the european union ’s oﬃce and the australian embassy are both
located in the same building
4. the eu ’s mission is in the same building with the australian
embassy
73 of 70

Automatic Evaluation
Ngram overlap metrics: 2-gram precision p2 =
5
13
Source: 欧盟办事处与澳洲大使馆在同一建筑内
Candidate: the chinese embassy in australia and the eu representative
oﬃce in the same building
Reference Translations:
1. the eu oﬃce and the australian embassy are housed in the same
building
2. the european union oﬃce is in the same building as the australian
embassy
3. the european union ’s oﬃce and the australian embassy are both
located in the same building
4. the eu ’s mission is in the same building with the australian
embassy
73 of 70

Automatic Evaluation
Ngram overlap metrics: 3-gram precision p3 =
2
12
Source: 欧盟办事处与澳洲大使馆在同一建筑内
Candidate: the chinese embassy in australia and the eu representative
oﬃce in the same building
Reference Translations:
1. the eu oﬃce and the australian embassy are housed in the same
building
2. the european union oﬃce is in the same building as the australian
embassy
3. the european union ’s oﬃce and the australian embassy are both
located in the same building
4. the eu ’s mission is in the same building with the australian
embassy
73 of 70

Automatic Evaluation
Ngram overlap metrics: 4-gram precision p4 =
1
11
Source: 欧盟办事处与澳洲大使馆在同一建筑内
Candidate: the chinese embassy in australia and the eu representative
oﬃce in the same building
Reference Translations:
1. the eu oﬃce and the australian embassy are housed in the same
building
2. the european union oﬃce is in the same building as the australian
embassy
3. the european union ’s oﬃce and the australian embassy are both
located in the same building
4. the eu ’s mission is in the same building with the australian
embassy
73 of 70

Automatic Evaluation
Numerous automatic evaluation functions have been proposed, however
the dominant metric is BLEU:
BLEU
BLEUn = BP × exp
( N
∑
n=1
wn log pn
)
BP =
{ 1
if c > r
exp (1 −R′
C′) if c <= r
▶BP is the Brevity Penalty, wn is the ngram length weights (usually
1
n), pn is precision of ngram predictions, R′ is the total length of all
references and C′ is the sum of the best matching candidates.
▶statistics are calculate over the whole document, i.e. all the
sentences.
74 of 70

Questions?
Questions?
75 of 70

Bayesian Nonparametric approaches
We can take advantage of the smoothing and interpolation with shorter
contexts properties of the hierarchical Pitman-Yor (PY) process, and
use it in word alignment as well.
Reminder: Model 1 generative story
P(F, A|E) = p(m|l) ×
m
∏
i=1
p(ai)p(fi|eai)
Where p(ai) =
1
l+1 is uniform over all alignments and
p(fi|eai) ∼Categorical.
76 of 70

Bayesian Nonparametric approaches
Re-formulating the model to use the hierarchical PY process instead of
the categorical distributions, we get:
PY Model 1 generative story
ai|m ∼Gm
0
fi|eai ∼Heai
Heai ∼PY(H∅)
H∅∼PY(H0)
77 of 70

Bayesian Nonparametric approaches
Following this approach, we can re-formulate the HMM alignment
model as well to use the hierarchical PY process instead of the
categorical distributions.
Reminder: HMM alignment model generative story
P(F,A|E) =
p(m|l) ×
m
∏
i=1
p(ai|ai−1, m) × p(fi|eai)
78 of 70

Bayesian Nonparametric approaches
We replace the categorical distribution for the transition p(ai|ai−1, m)
with a hierarchical PY process with a longer sequence of alignment
positions in the conditional
PY HMM alignment model generative story
ai|ai−1, m ∼Gm
ai−1
Gm
ai−1 ∼PY(Gm
∅)
Gm
∅∼PY(Gm
0 )
▶Unique distribution for each foreign sentence length
▶Condition the position on the previous alignment position,
backing-oﬀto the HMM’s stationary distribution over alignment
positions
79 of 70

Bayesian Nonparametric approaches
Unlike previous approaches that ran into diﬃculties extending models 3
and 4, we can extend them rather easily by just replacing the
categorical distributions.
▶The inference method that we use, Gibbs sampling, circumvents
the intractable sum approximation of other inference methods
▶The use of the hierarchical PY process allows us to incorporate
phrasal dependencies into the distribution
Reminder: Models 3 and 4 generative story
P(F, A|E) =p(B0|B1, ..., Bl) ×
l∏
i=1
p(Bi|Bi−1, ei)
×
l∏
i=0
∏
j∈Bi
p(fj|ei)
80 of 70

Bayesian Nonparametric approaches
Reminder: Models 3 and 4 generative story – cont.
For model 3 the dependence on previous alignment sets is ignored and
the probability p(Bi|Bi−1, ei) is modelled as
p(Bi|Bi−1, ei) = p(φi|ei)φi!
∏
j∈Bi
p(j|i, m),
whereas in model 4 it is modelled using two HMMs:
p(Bi|Bi−1, ei) =p(φi|ei) × p=1(Bi,1 −⊙(Bi−1)|·)
×
φi
∏
k=2
p>1(Bi,k −Bi,k−1|·)
81 of 70

Bayesian Nonparametric approaches
Reminder: Models 3 and 4 generative story – cont.
For both these models the spurious word generation is controlled by a
binomial distribution:
p(B0|B1, ..., Bl) =
(m −φ0
φ0
)
(1 −p0)m−2φ0pφ0
1
1
φ0!
for some parameters p0 and p1.
82 of 70

Bayesian Nonparametric approaches
Replacing the categorical priors with hierarchical PY process ones, we
set the translation and fertility probabilities p(φi|ei) ∏
j∈Bi p(fj|ei) using
a common prior that generates translation sequences.
PY models 3 and 4 generative story
(f1, ..., fφi)|ei ∼Hei
Hei ∼PY(HFT
ei )
HFT
ei ((f1, ..., fφi)) = HF
ei(φi)
∏
j
HT
(fj−1,ei)(fj)
HF
ei ∼PY(HF
∅)
HF
∅∼PY(HF
0)
HT
(fj−1,ei) ∼PY(HT
ei)
HT
ei ∼PY(HT
∅)
HT
∅∼PY(HT
0 )
▶We used superscripts for the indexing of words which do not have
to occur sequentially in the sentence
83 of 70

Bayesian Nonparametric approaches
We generate sequences instead of individual words and fertilities, and
fall-back onto these only in sparse cases.
Example
Aligning the English sentence “I don’t speak French” to its French
translation “Je ne parle pas français”, the word “not” will generate the
phrase (“ne”, “pas”), which will later on be distorted into its place
around the verb.
84 of 70

Bayesian Nonparametric approaches
The distortion probability for model 3, p(j|i, m), is modelled simply as
depending on the position of the source word i and its class:
PY models 3 and 4 generative story – cont.
j|(C(ei), i), m ∼Gm
(C(ei),i)
Gm
(C(ei),i) ∼PY(Gm
i )
Gm
i ∼PY(Gm
∅)
Gm
∅∼PY(Gm
0 )
where we back-oﬀto the source word position and then to the
frequencies of the alignment positions.
85 of 70

Bayesian Nonparametric approaches
Distortion probability for IBM model 4
▶First probability distribution p=1 controls the head distortion
PY models 3 and 4 generative story – cont.
Bi,1 −⊙(Bi−1) | (C(ei), C(fBi,1)), m
∼Gm
(C(ei),C(fBi,1))
Gm
(C(ei),C(fBi,1)) ∼PY(Gm
C(fBi,1))
Gm
C(fBi,1) ∼PY(Gm
∅)
Gm
∅∼PY(Gm
0 )
86 of 70

Bayesian Nonparametric approaches
▶Second probability distribution p>1 controls the distortion within
the set of words
PY models 3 and 4 generative story – cont.
Bi,j −Bi,j−1|C(fBi,j), m ∼Hm
C(fBi,j)
Hm
C(fBi,j) ∼PY(Hm
∅)
Hm
∅∼PY(Hm
0 )
Again we model the jump size as depending on the word class for the
proposed foreign word, backing-oﬀto the relative jump frequencies.
87 of 70

Bayesian Nonparametric approaches
Fertility and translation of NULL words
▶Follows the idea of the original model, where the number of
spurious words is determined by a binomial distribution created
from a set of Bernoulli experiments, each one performed after the
translation of a non-spurious word
▶We use an indicator function I to signal whether a spurious word
was generated after a non-spurious word (I = 1) or not (I = 0)
PY models 3 and 4 generative story – cont.
I = 0, 1|l ∼HNF
l
HNF
l
∼PY(HNF
∅)
HNF
∅
∼PY(HNF
0 )
fi ∼HNT
∅
HNT
∅
∼PY(HNT
0 )
88 of 70

Questions?
Questions?
89 of 70

Word Alignment (IBM Model 1)
A simple generative model for p(s|t) is derived by introducing a latent
variable a into the conditional probability:
p(s, a|t) =
p(J|I)
(I + 1)J
J
∏
j=1
p(sj|taj),
where:
▶s and t are the input (source) and output (target) sentences of
length J and I respectively,
▶a is a vector of length J consisting of integer indexes into the
target sentence, known as the alignment,
▶p(J|I) is not important for training the model and we’ll treat it as
a constant ϵ.
To learn this model the EM algorithm is used to ﬁnd the MLE values
for the parameters p(sj|taj).
90 of 70

EM for Word Alignment (IBM Model 1)
To derive an EM update for this model we need to calculate the
expected values for the alignment vectors for each sentence. The
conditional probability of an alignment is:
p(a|s, t) = p(s, a|t)
p(s|t)
Marginalising out a in p(s, a|t) gives the required denominator:
p(s|t) =
∑
a
p(s, a|t),
=
I
∑
a1=0
I
∑
a2=0
· · ·
I
∑
aJ=0
p(s, a|t),
=
ϵ
(I + 1)J
I
∑
a1=0
I
∑
a2=0
· · ·
I
∑
aJ=0
J
∏
j=1
p(sj|taj).
91 of 70

EM for Word Alignment (IBM Model 1)
Rather conveniently we can swap the sum and product to get an
equation that is tractable to compute:
p(s|t) =
ϵ
(I + 1)J
I
∑
a1=0
I
∑
a2=0
· · ·
I
∑
aJ=0
J
∏
j=1
p(sj|taj)
=
ϵ
(I + 1)J
J
∏
j=1
I
∑
i=0
p(sj|ti).
92 of 70

EM for Word Alignment (IBM Model 1)
Now we can state the conditional probabilities for the alignments:
p(a|s, t) = p(s, a|t)
p(s|t) ,
=
ϵ
(I+1)J
∏J
j=1 p(sj|taj)
ϵ
(I+1)J
∏J
j=1
∑I
i=0 p(sj|ti)
,
=
J
∏
j=1
p(sj|taj)
∑I
i=0 p(sj|ti)
93 of 70

EM for Word Alignment (IBM Model 1)
The next step is to derive the expected counts c(s|t, s, t) for a single
pair on sentences of a source word s aligning with a target word t:
c(s|t, s, t) =
∑
a
p(a|s, t)
J
∑
j=1
δ(s, sj)δ(t, taj)
=
p(s|t)
∑I
i=0 p(s|ti)
J
∑
j=1
δ(s, sj)
I
∑
i=1
δ(t, ti)
where we’ve used a similar trick to that used earlier to rearrange the
sums. The result is that we can calculate the counts in O(J × I) rather
than O(I + 1)J.
94 of 70

EM for Word Alignment (IBM Model 1)
Finally by collecting the counts for all sentence pairs in our training
corpus (s, t) and normalising we can derive the EM update for the
translation probabilities p(s|t):
pi+1(s|t) =
∑
s,t ci(s|t, s, t)
∑
t
∑
s,t ci(s|t, s, t).
95 of 70

EM for Word Alignment (IBM Model 1)
Algorithm outline:
1 Initialise the translation probabilities p(s|t) to uniform,
2 E Step: For each pair of sentences in the training corpus, calculate
ci(s|t, s, t), keeping a running sum of ci(s|t) and ∑
t p(s|t),
3 M Step: Calculate the new probabilities p(s|t) using the normalised
counts,
4 Repeat from 2 until the log likelihood of the data (∑
s,t log p(s|t))
stops increasing
(up to a small tolerance).
96 of 70

Word Alignment (IBM Model 1)
Limitations of this simple word alignment model:
▶The structure of sentences is not modelled, words align independently of
each other,
▶The position of words with a sentence is not modelled, obviously words
near the start of the source sentence are more likely to align to words
near the start of the target sentence,
▶The alignment is asymmetric, a target word may align to multiple source
words, but a source word may only align to a single target,
▶and many others ...
These limitations mean that this model does not work well as a translation
model on it’s own, however it is currently used as the ﬁrst step in learning
more complicated models by online translation providers such as Google and
Microsoft.
97 of 70

Questions?
Questions?
98 of 70

