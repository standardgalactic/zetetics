LECTURE NOTES ON ENTROPY AND MARKOV CHAINS
PIETRO CAPUTO
Abstract. Notes for lectures at UC Santa Barbara (Summer 2022) and UC Berkeley
(Fall 2022). In progress
Contents
1.
Setup and preliminaries
1
1.1.
Relative entropy
2
1.2.
The Gibbs measure
3
1.3.
Conditional expectations
3
1.4.
Variance, covariance, entropy
4
2.
Entropy and Markov chain mixing
4
2.1.
Markov chains and mixing time
4
2.2.
Entropy contractions and mixing time
6
2.3.
Dirichlet forms, spectral gap
7
2.4.
Log-Sobolev inequalities
9
3.
Entropy decompositions and Block dynamics
11
3.1.
Heat bath chains
11
3.2.
Useful decompositions
11
4.
Product measures
12
4.1.
Subadditivity and tensorization
12
4.2.
Shearer inequality
13
4.3.
Shearer inequality for sub-modular functions
14
5.
Approximate versions in the non-product case
14
5.1.
Approximate tensorization and approximate subadditivity
14
5.2.
Block factorization and mixing time of block dynamics
15
6.
Block factorizations under spectral independence
15
6.1.
Spectral independence
16
6.2.
Approximate subadditivity and uniform factorizations under spectral
independence
16
6.3.
Setting up the recursion
17
6.4.
Estimating the local coeﬃcients
18
6.5.
Proof of Theorem 6.4
20
6.6.
Proof of Theorem 6.3
20
6.7.
Some applications
20
7.
Entropy inequalities for permutations
22
7.1.
A permanent bound using entropy subadditivity for permutations
23
8.
Open problems
24
References
24
1. Setup and preliminaries
Throughout, Ωdenotes a ﬁnite space, and P(Ω) is the set of probability measures on Ω.
1

2
PIETRO CAPUTO
1.1. Relative entropy. Given ν, µ ∈P(Ω), the relative entropy of ν with respect to µ is
deﬁned as
H(ν | µ) =
X
σ∈Ω
ν(σ) log
ν(σ)
µ(σ)

,
with the conventions 0 log(0) = 0 log(0/0) = 0.
This deﬁnition makes sense whenever
µ(σ) = 0 implies ν(σ) = 0, that is when ν is absolutely continuous with respect to µ.
Otherwise, one deﬁnes H(ν | µ) = +∞. Relative entropy is one of the most common and
eﬀective ways of measuring the “distance” between probability measures. It is also known
as Kullback-Leibler divergence. If H(X) denotes the Shannon entropy of a random variable
X taking values in Ωand with distribution ν ∈P(Ω) then
H(ν | µ) = −H(X) −ν[log µ].
(1.1)
Note that we are using only natural logarithms here, whereas Shannon entropy is often
deﬁned using base 2 logarithms. Recall that the total variation distance between µ, ν ∈
P(Ω) is deﬁned as
∥µ −ν∥TV = 1
2
X
σ∈Ω
|µ(σ) −ν(σ)|.
The following lemma summarizes the main properties of the relative entropy that we shall
need.
Lemma 1.1. For all ν, µ ∈P(Ω),
1) H(ν | µ) ⩾0 and H(ν | µ) = 0
⇐⇒
ν = µ.
2) Convexity: for any collection of probability measures {νi}, and all αi ⩾0 with P
i αi =
1,
H
 X
i
αiνi
 µ

⩽
X
i
αiH (νi | µ) .
3) Pinsker inequality:
∥µ −ν∥2
TV ⩽1
2 H(ν | µ).
(1.2)
4) Variational principle:
H(ν | µ) = sup
g:Ω7→R
{ν[g] −log µ [eg]} .
(1.3)
Proof. Deﬁne h(σ) = ν(σ)/µ(σ). Then µ[h] = 1 and H(ν | µ) = µ[h log h]. By Jensen
inequality and the convexity of x 7→x log x, x ⩾0, µ[h log h] ⩾µ[h] log µ[h], with equality
if and only if h is constant. This proves 1.
To prove 2, if hi = νi(σ)/µ(σ), h = P
i αihi, then convexity of x 7→x log x, x ⩾0,
implies h log h ⩽P
i αihi log hi and
H
 X
i
αiνi
 µ

= µ[h log h] ⩽
X
i
αiµ[hi log hi] =
X
i
αiH (νi | µ) .
To prove 3 one checks that u log u ⩾u −1 for all u ⩾0, and that
3(u −1)2 ⩽(2u + 4)(u log u −u + 1) ,
u ⩾0.
Taking the square root, applying this to u = h(σ) and integrating one ﬁnds
∥µ −ν∥TV = 1
2
Z
|h −1| dµ ⩽
1
2
√
3
Z p
(2h + 4)(h log h −h + 1) dµ
⩽
1
2
√
3
Z
(2h + 4) dµ
1/2 Z
(h log h −h + 1) dµ
1/2
=
1
√
2
p
H(ν | µ) .

LECTURE NOTES ON ENTROPY AND MARKOV CHAINS
3
To prove the variational principle (1.3), note that for any g : Ω7→R,
ν[g] = H(ν | µ) + µ[h log(eg/h)] ⩽H(ν | µ) + log µ[eg],
where we use the concavity of the logarithm and Jensen’s inequality applied to ν = µh ∈
P(Ω). The inequality is saturated at g = log h, which proves (1.3).
□
1.2. The Gibbs measure. Consider a product space Ω= Ω1 × Ω2 · · · × Ωn, where Ωi
are ﬁnite sets. We ﬁx a reference probability measure µ ∈P(Ω). The measure µ is often
referred to as the Gibbs measure. Some key examples to keep in mind for later applications
are as follows.
Example 1.2 (Ising model). Set Ω= {−1, +1}n. Fix β ∈R and let G = (V, E) be a graph
with V = [n] = {1, . . . , n}. The Ising model µ = µG,β is the Gibbs measure on Ωsuch that
µ(σ) ∝exp

β
X
xy∈E
σxσy

,
(1.4)
where σ = (σx, x ∈V ) ∈Ωdenotes a spin conﬁguration.
Example 1.3 (Potts model). Set Ω= {1, . . . , q}n, where q ∈N, q ⩾2. Fix β ∈R and let
G = (V, E) be a graph with V = [n]. The Potts model µ = µG,β is the Gibbs measure on Ω
such that
µ(σ) ∝exp

β
X
xy∈E
1σx=σy

,
(1.5)
where σ = (σx, x ∈V ) ∈Ω. If q = 2 this is the Ising model with β replaced by β/2.
The models above are classical examples of spin systems from Statistical Mechanics, see
the book [26] for background.
Example 1.4 (Proper colorings of a graph). Set Ω= {1, . . . , q}n, where q ∈N, q ⩾2 and
and let G = (V, E) be a graph with V = [n]. Take µ as uniform probability measure over all
proper colorings of the graph G with q colors. This can be obtained as the limit β →−∞
of (1.5).
Example 1.5 (Hard core model). Set Ω= {0, 1}n, and let G = (V, E) be a graph with
V = [n].
For any λ > 0 deﬁne the hard core model with fugacity λ as the probability
measure µ = µG,λ such that
µ(σ) ∝λ
P
x∈V σx1σ∈Ω0
where Ω0 = {σ ∈Ω: σxσy = 0 , ∀xy ∈E} is the set of independent sets of G.
Example 1.6 (Uniform permutations). Set Ω= [n]n, and let µ be the uniform distribution
on the symmetric group Sn ⊂Ωof permutations of [n], that is
Sn = {σ ∈Ω: σx ̸= σy, ∀x, y ∈[n], x ̸= y}.
We note that in the last three examples the measure µ is supported on a strict subset of
conﬁgurations, that is the system has hard constraints.
1.3. Conditional expectations. The elements of Ωare referred to as spin conﬁgurations.
Given σ ∈Ω, a subset A ⊂[n], σA = (σx, x ∈A) denotes the spin conﬁguration restricted
to A. The conditional distribution given the spins τ ∈ΩAc in Ac is denoted µτ
A:
µτ
A(η) = µ(σ = η | σAc = τ)1ηAc=τ ,
(1.6)
The deﬁnition (1.6) is interpreted as a probability measure on Ωand, with a slight abuse
of notation, sometimes we think of it as a probability on ΩA. Given a function f : Ω7→R,

4
PIETRO CAPUTO
τ ∈ΩAc, we write µτ
Af for the expectation of f under µτ
A, and write µAf for the function
ΩAc ∋τ 7→µτ
Af. Therefore, µAf satisﬁes
[µAf](τ) = µτ
Af =
X
ξ∈Ω
µ(σA = ξA | σAc = τ)f(ξ) 1ξAc=τ .
We refer to the function µAf as the conditional expectation, and to the conﬁguration τ
in µτ
A as the boundary condition, or pinning.
The elementary properties of conditional
probabilities imply the relations
µAµBf = µAf ,
B ⊂A ⊂[n],
valid for all all functions f : Ω7→R. Another property that will be useful is the fact that if
f is a probability density w.r.t. µ, that is f ⩾0 and µ(f) = 1, then µAf is the probability
density of the marginal of ν := fµ on ΩAc. Indeed, setting ηAc = τ,
X
ηA
ν(ηA, ηAc) =
X
ηA
f(ηA, τ)µ(ηA, τ)
= µ(σAc = τ)
X
ηA
f(ηA, τ)µ(ηA|σAc = τ) = µ(σAc = τ)µτ
Af.
1.4. Variance, covariance, entropy. The following functionals associated to µ are com-
monly used in our analysis. For functions f, g : Ω7→R, any A ⊂[n] and boundary condition
τ, the covariance of f, g with respect to µτ
A is deﬁned as
Covτ
A(f, g) = µτ
A[fg] −µτ
A[f]µτ
A[g].
We also write CovA(f, g) for the function ΩAc ∋τ 7→Covτ
A(f, g). Note that
µ [CovA(f, g)] = µ [(f −µτ
A[f])(g −µτ
A[g])] .
When f = g we write Varτ
Af = Covτ
A(f, f) and VarAf = CovA(f, f) for the variance. For
any nonnegative function f : Ω7→R+, the entropy of f with respect to µτ
A is deﬁned as
Entτ
Af = µτ
A[f log f] −µτ
A[f] log µτ
A[f].
Again, we write EntAf for the function ΩAc ∋τ 7→Entτ
Af. Note that this function satisﬁes
µ [EntAf] = µ [f log (f/µA[f])] .
When A = [n] we simply write Varf, Entf for the functionals Var[n]f, Ent[n]f.
Up to
normalization, the entropy functional coincides with the relative entropy: if ν = fµ/µ[f]
then
Entf = µ[f] H(ν | µ),
or, equivalently, Ent(ν/µ) = H(ν | µ).
2. Entropy and Markov chain mixing
2.1. Markov chains and mixing time. Let Ω0 denote the support of µ. Let P be a
stochastic matrix on Ω0, that is P = {P(σ, η), σ, η ∈Ω0} with P(σ, η) ⩾0 and P
η∈Ω0 = 1.
We assume that P is regular, that is there exists k ∈N such that P k(σ, η) > 0 for all
σ, η ∈Ω0. We also assume that µ is stationary under P, that is
X
σ∈Ω0
µ(σ)P(σ, η) = µ(η) ,
η ∈Ω0 .
Under these assumptions it is a standard fact that µ is the unique stationary distribution
and that
P k(σ, η) −→µ(η) ,
k →∞,
∀σ, η ∈Ω0 .

LECTURE NOTES ON ENTROPY AND MARKOV CHAINS
5
We say that the pair (P, µ) is reversible if
µ(σ)P(σ, η) = µ(η)P(η, σ) ,
σ, η ∈Ω0.
(2.1)
Clearly, if (P, µ) is reversible then µP = µ and therefore µ is stationary. Note that (2.1)
is equivalent to the condition that P is self-adjoint in L2(µ) (exercise). Most examples of
Markov chains considered below are reversible. However, unless otherwise stated, we do
not assume reversibility in what follows.
Example 2.1 (Glauber dynamics for the Ising model on a graph). Let G = (V, E) be a
graph with V = [n], and let Ω0 = Ω= {−1, +1}n. Given σ ∈Ω, x ∈[n], deﬁne
px(σ) = 1
2

1 + tanh

β
X
y: xy∈E
σy



Deﬁne P(σ, η), σ, η ∈Ω, by
P(σ, η) = 1
n
X
x∈[n]
1ηy=σy ∀y̸=x (px(σ)1ηx=+1 + (1 −px(σ))1ηx=−1) .
In words, we pick a vertex x uniformly at random, remove its spin σx and replace it by
a random spin ηx such that ηx = +1 with probability px(σ) and ηx = −1 with probability
1 −px(σ). The matrix P satisﬁes (2.1) if µ is the Ising model deﬁned in (1.4) (exercise).
The chain is regular (exercise).
Example 2.2 (Sampling proper colorings of a graph). Let G = (V, E) be a graph with
V = [n], and let Ω0 denote the set of all proper q-colorings of G, where q ∈N, q ⩾2.
Given σ ∈Ω0, x ∈[n], let ωx(σ) ⊂{1, . . . , q} denote the set of colors ax ∈{1, . . . , q} such
that ax ̸= σy for all y neighboring x. Deﬁne P by
P(σ, η) = 1
n
X
x∈[n]
1ηx∈ωx(σ), ηy=σy ∀y̸=x
|ωx(σ)|
.
In words, we pick a vertex uniformly at random, remove its color and replace it by uniformly
random color among the ones that are compatible with the neighbors. The matrix P is
symmetric and the stationary distribution µ is uniform over Ω0. If q is suﬃciently large
then one can check that P is regular (exercise).
Example 2.3 (Interchange process on a graph). Let G = (V, E) be a graph with V = [n],
and let Ω0 = Sn ⊂[n]n denote the set of permutations of [n]. Deﬁne P by
P(σ, η) =
1
2|E|
X
xy∈E
1ηz=σz, ∀z /∈xy .
In words, we pick an edge uniformly at random, then with probability 1/2 we swap the labels
at xy and with probability 1/2 we stay put. Note that for any σ ∈Ω0, any edge xy ∈E,
there are exactly two conﬁgurations η ∈Ωsuch that 1ηz=σz, ∀z /∈xy, that is either η = σ
or η is σ swapped at xy. The matrix P is symmetric and the stationary distribution µ is
uniform over Ω0. If the graph G is connected, then one checks that the matrix P is regular
(exercise).
The ε-mixing time Tmix(P, ε) of the chain is deﬁned as
Tmix(P, ε) = max
σ∈Ω0 min
n
k ∈N : ∥P k(σ, ·) −µ∥TV ≤ε
o
.
We write Tmix(P) = Tmix(P, 1/4), and call it the mixing time of the chain.
Lemma 2.4. For any k ∈N,
max
σ∈Ω0 ∥P k(σ, ·) −µ∥TV ⩽2−⌊k/Tmix(P)⌋.

6
PIETRO CAPUTO
Proof. Deﬁne
d(k) = max
σ∈Ω0 ∥P k(σ, ·) −µ∥TV ,
¯d(k) = max
σ,η∈Ω0 ∥P k(σ, ·) −P k(η, ·)∥TV .
Then d(k) ⩽¯d(k) ⩽2d(k). Moreover, the Markov property implies d(ℓk) ⩽¯d(k)ℓfor all
ℓ, k ∈N. The monotonicity d(k + 1) ⩽d(k) then shows that
d(k) ⩽d(Tmix⌊k/Tmix(P)⌋) ⩽¯d(Tmix⌊k/Tmix(P)⌋)
⩽(2d(Tmix))⌊k/Tmix(P)⌋⩽2−⌊k/Tmix(P)⌋.
□
We refer to e.g. [37] ans [31] for more background on Markov chain mixing.
2.2. Entropy contractions and mixing time. A well established approach to the proof
of upper bounds on the mixing time uses the following relations between mixing time and
relative entropy contractions.
Deﬁnition 2.5. A Markov chain with transition matrix P and stationary distribution µ
has relative entropy contraction with constant δ ∈(0, 1) if for all distributions ν ∈P(Ω0),
H(νP | µ) ⩽(1 −δ)H(ν | µ).
(2.2)
Lemma 2.6. If a Markov chain with transition matrix P and stationary distribution µ has
relative entropy contraction with constant δ ∈(0, 1), then its ε-mixing time satisﬁes
Tmix(P, ε) ⩽1 + 1
δ
h
log
  1
2ε2

+ log log

1
µ∗
i
.
(2.3)
where µ∗= minσ µ(σ).
Proof. Pinsker’s inequality (1.2) says that
∥δσP k −µ∥2
TV ⩽1
2H(δσP k | µ),
where δσ(τ) = 1(τ = σ) is the Dirac mass at σ. Iterating (2.2),
∥δσP k −µ∥2
TV ⩽1
2(1 −δ)kH(δσ | µ).
Since H(δσ | µ) = −log µ(σ) and (1 −δ)k ⩽e−δk we obtain
max
σ
∥δσP k −µ∥TV ⩽ε,
as soon as k is an integer such that k ⩾δ−1 log[(2ε2)−1 log(1/µ∗)].
□
Remark 2.7. If ν has density f with respect to µ, that is ν = fµ, then νP has density P ∗f
with respect to µ, where P ∗is the adjoint or time-reversal matrix P ∗(σ, σ′) = µ(σ′)
µ(σ) P(σ′, σ).
Thus, (2.2) is equivalent to
Ent(P ∗f) ⩽(1 −δ)Ent(f),
(2.4)
for all f ⩾0 such that µ[f] = 1. By homogeneity, this is equivalent to (2.4) for all f ⩾0.
When P is reversible, that is when P = P ∗, (2.2) is equivalent to Ent(Pf) ⩽(1 −δ)Ent(f)
for all f ⩾0.

LECTURE NOTES ON ENTROPY AND MARKOV CHAINS
7
2.3. Dirichlet forms, spectral gap. Consider a stochastic matrix P with stationary
distribution µ. The Dirichlet form associated to the pair (P, µ) is deﬁned as
DP (f, g) = ⟨f, (1 −P)g⟩,
where f, g are real functions, and ⟨f, g⟩= µ[fg] denotes the scalar product in L2(µ). Since
f is real we also have
DP (f, f) = ⟨(1 −Q)f, f⟩= 1
2
X
x,y
µ(x)Q(x, y)(f(x) −f(y))2,
where Q = 1
2(P + P ∗). Moreover, if P = P ∗one has
DP (f, g) = 1
2
X
x,y
µ(x)P(x, y)(f(x) −f(y))(g(x) −g(y)),
for all f, g. The spectral gap gap(P) is deﬁned as
gap(P) =
inf
f: Varf̸=0
DP (f, f)
Varf
.
The main use of spectral gap to bound convergence to equilibrium is as follows. Consider
the continuous time kernel Kt = e(P−1)t, t ⩾0. This is a stochastic matrix for each t,
which deﬁnes the Markov chain at time t, that is νKt is the distribution at time t when
started at ν at time 0. A sample of the conﬁguration at time t can be obtained by using
the updates from P at the arrival times of a Poisson process with rate 1.
Lemma 2.8. For any ν ∈P(Ω0), for any t ⩾0,
∥νKt −µ∥TV ⩽
1
2√µ∗
e−gap(P) t.
Proof. Let ft = νKt/µ, and observe that if f = ν/µ, then ft = K∗
t f, where
K∗
t = e(P ∗−1)t,
is the adjoint of Kt in L2(µ). Then a simple computation shows that
d
dtVarft = −2DP (ft, ft) ⩽−2 gap(P) Varft ,
t ⩾0.
Therefore, integrating,
Varft ⩽e−2 gap(P) t Varf ,
t ⩾0.
By convexity of Var(·) one has that Varf is maximized when ν = δσ∗with σ∗∈Ωsuch
that µ(σ∗) = µ∗and therefore Varf ⩽Var(δσ∗/µ) = µ−1
∗(1 −µ∗) ⩽µ−1
∗. Therefore, using
Schwarz’ inequality,
∥νKt −µ∥TV ⩽1
2
p
Varft ⩽
1
2√µ∗
e−gap(P) t.
□
Note that gap(P) = gap(P ∗) is the smallest nonzero eigenvalue of the reversible matrix
1 −(P + P ∗)/2 (exercise).
In particular, when P = P ∗and all eigenvalues of P are
nonnegative then gap(P) = 1 −λ2(P) where λ2(P) is the maximum eigenvalue of P that
is less than 1. Note that if P is reversible but its eigenvalues are not all nonnegative one
can always replace P by its lazy version ˆP = (1 + P)/2, to obtain a reversible chain with
nonnegative eigenvalues.

8
PIETRO CAPUTO
Lemma 2.9. Suppose that P is reversible. Then
Tmix(P, ε) ⩽1 +
1
gap∗(P)
h
log
  1
2ε

+ 1
2 log

1
µ∗
i
,
(2.5)
where gap∗(P) = 1 −max{|λ| : λ ̸= 1 eigenvalue of P}.
Proof. For any k ∈N, the spectral decomposition of P shows that Var(Pkf) ⩽λ2k
∗Varf,
for any f, where λ∗= max{|λ| : λ ̸= 1 eigenvalue of P}. Therefore taking f = ν/µ
∥νP k −µ∥TV ⩽1
2
q
Var(P kf) ⩽1
2
p
Varf λk
∗⩽e−gap∗(P) k
2√µ∗
,
where we use λ∗= 1−gap∗(P) ⩽e−gap∗(P) and Varf ⩽µ−1
∗. Thus, if k is an integer larger
than
1
gap∗(P)
h
log
  1
2ε

+ 1
2 log

1
µ∗
i
, we have ∥νP k −µ∥TV ⩽ε.
□
Clearly, when P is also positive semideﬁnite we also have gap∗(P) = gap(P). For the
reverse inequality, we note that when P is reversible, one can check that the mixing time is
at least the inverse gap up to constants. More precisely, for all irreducible aperiodic chains,
Th. 12.5 in [31] shows that
Tmix(P, ε) ⩾

1
gap∗(P) −1

log
  1
2ε

.
(2.6)
This does not require reversibility. Indeed the proof goes as follows. Let f be such that
Pf = λf with λ ̸= 1 and let σ ∈Ωbe such that |f(σ)| = ∥f∥∞. Note that µ(f) = 0, since
µ(f) = µ(Pf) = λµ(f). Then
|λ|k∥f∥∞= |λkf(σ)| = |P kf(σ)| = |P kf(σ) −µ(f)| ⩽2∥f∥∞d(k).
Therefore
d(k) ⩾1
2 |λ|k,
and |λ|Tmix(P,ε) ⩽2ε, which implies the claim.
The inequality (2.5) can be compared with (2.3). The latter is much better when δ is
comparable with gap(P). Indeed, for all examples considered here log log(1/µ∗) ∼log n
and thus when δ, gap(P) have the same order, going from (2.5) to (2.3) improves the bound
by a factor 1
n log n.
In non-reversible cases the spectral gap is not necessarily useful. For instance one can
ﬁnd stochastic matrices P which mix much faster than the symmetrized (P + P ∗)/2. The
following is an example.
Example 2.10. Consider a random sequence Z = {Z1, Z2, . . . } of iid fair coins {0, 1},
and ﬁx an integer L ∈N. The Markov chian on ΩL = {0, 1}L is deﬁned as follows. Given
an initial condition σ = (σ1, . . . , σL) ∈ΩL, deﬁne the sequence
Xj =
(
σj
j ⩽L
Zj−L
j > L
and let Y k denote the conﬁguration (Xk+1, . . . , Xk+L). Then Y k ∈ΩL, k = 0, 1I, . . . is a
sequence of random variables which deﬁnes a Markov chain on Ω= {0, 1}L, with Y 0 = σ.
The associated stochastic matrix P has uniform stationary distribution, it is non-reversible,
and has all eigenvalues equal to zero except the trivial eigenvalue equal to 1 (exercise). Note
that Y k has uniform distribution on ΩL as soon as k ⩾L, so that Tmix(P, ε) ⩽L for all
ε > 0. However, the symmetrized matrix ¯P = (P + P ∗)/2 has spectral gap proportional to
1/L2 (exercise) and therefore Tmix( ¯P, ε) ⩾cεL2.

LECTURE NOTES ON ENTROPY AND MARKOV CHAINS
9
2.4. Log-Sobolev inequalities. The inequality (2.2) can be considered as a discrete time
analogue of the so-called modiﬁed log-Sobolev inequality characterizing the relative entropy
decay for continuous time Markov chains; see, e.g. [9]. Below we discuss some basic relations
among (2.2), the standard log-Sobolev inequality and the modiﬁed log-Sobolev inequality.
Deﬁnition 2.11. The pair (P, µ) is said to satisfy the (standard) log-Sobolev inequality
(LSI) with constant α if for all f ⩾0:
DP (
p
f,
p
f) ⩾α Entµf.
It is said to satisfy the modiﬁed log-Sobolev inequality (MLSI) with constant ϱ if for all
f ⩾0:
DP (f, log f) ⩾ϱ Entµf.
(2.7)
It is well known that the Log-Sobolev inequality characterizes the so-called hypercon-
tractivity for the continuous time kernel Kt = e(P−1)t, t ⩾0 (see [24, Theorem 3.5]), that
is
∥Kt∥2→τ(t) ⩽1 ,
τ(t) = 1 + e2αt ,
(2.8)
where ∥Kt∥2→p = sup∥f∥2 ⩽1 ∥Ktf∥p and ∥· ∥p denotes the Lp(µ) norm. In the reversible
case one has (2.8) with τ(t) = 1 + e4αt. The reverse implication, namely that (2.8) with
τ(t) = 1 + e4ct implies α ⩾c always holds.
The modiﬁed Log-Sobolev inequality (2.7), on the other hand characterizes the expo-
nential decay of the relative entropy in continuous time for the kernel Kt (see [24, Theorem
3.6]).
Lemma 2.12. For any ν ∈P(Ω0), for any t ⩾0,
H(νKt | µ) ⩽H(ν | µ) e−ϱ t.
(2.9)
Moreover, if (2.9) holds for all ν ∈P(Ω0), t ⩾0, then (2.7) holds.
Proof. We proceed as in the proof of Lemma 2.8. Let ft = νKt/µ = K∗
t f, where f = ν/µ.
Then
d
dt H(νKt | µ) = d
dtEnt(K∗
t f) = −DP (K∗
t f, log K∗
t f) ⩽−ϱ Ent(K∗
t f).
Therefore, integrating,
Ent(K∗
t f) ⩽e−ϱ t Entf ,
t ⩾0.
To prove the converse, note that if (2.9) holds then
Ent(K∗
t f) −Entf ⩽(e−ϱ t −1) Entf ,
t > 0.
Dividing by t and passing to t →0+, one ﬁnds (2.7).
□
Next, we observe that the bound (2.4) is stronger than the MLSI in (2.7).
Lemma 2.13. If the relative entropy contraction (2.2) holds with constant δ, then (2.7)
holds with the same constant ϱ = δ.
Proof. We show that (2.4) implies the MLSI with the same constant δ. By homogeneity, it
is suﬃcient to restrict to the case µ[f] = 1. Suppose that
Ent(P ∗f) ⩽(1 −δ)Entf.
Since µ[P ∗f] = µ[f] = 1, from the variational principle (1.3) it follows that for any f ⩾0
with µ[f] = 1,
µ[(P ∗f) log f] ⩽µ[(P ∗f) log(P ∗f)] = EntP ∗f.

10
PIETRO CAPUTO
Therefore,
DP (f, log f) = µ[((1 −P ∗)f) log f] ⩾Entf −EntP ∗f ⩾δ Entf.
□
It is well known that MLSI implies spectral gap with gap ⩾ϱ/2.
Lemma 2.14. For any Markov chain, ϱ(P) ⩽2 gap(P).
Proof. This is a standard linearization argument. Take fε = 1 + εg with µ[g] = 0, and
observe that Entfε = ε2
2 Varg + o(ε2). Similarly, DP (log fε, fε) = ε2 DP (g, g) + o(ε2). Since
g is arbitrary with µ[g] = 0, this implies ϱ(P) ⩽2 gap(P).
□
It is also known that the standard LSI with constant α implies the MLSI ϱ = 2α, since
DP (f, log f) ⩾2DP (√f, √f) for all f ⩾0, and this can be improved to ϱ = 4α in the
reversible case; see [24, Lemma 2.7]. Here we recall a result of Miclo [36] showing in what
sense the LSI implies the discrete time entropy decay.
Lemma 2.15. If the pair (P ∗P, µ) satisﬁes the standard LSI with constant α, then the
relative entropy contraction (2.2) holds for (P, µ) with constant ϱ = α. In particular, when
P is reversible and positive semideﬁnite, if (P, µ) satisﬁes the LSI with constant α, then
for all f ⩾0:
EntµPf ⩽(1 −α)Entµf.
Proof. The ﬁrst assertion is proved in [36, Proposition 6]. The second assertion follows from
the ﬁrst and the simple observation that if P = P ∗and P is positive semideﬁnite, then the
LSI for (P, µ) implies the LSI for (P ∗P, µ) with the same constant since P ∗P = P 2 ⩽P as
quadratic forms in L2(µ) in this case.
□
Summarizing, for reversible chains one has
4αLSI ⩽ϱMLSI ,
δEC ⩽ϱMLSI ⩽2 gap,
and αLSI ⩽δEC when P is also positive semideﬁnite.
Example 2.16. Consider the trivial Markov chain P(σ, η) = µ(η) for all σ, η ∈Ω. Then
P0f = µ[f] and therefore δEC(P0) = gap(P0) = 1. It follows that ϱMLSI(P0) ⩾1, but exact
value is not explicitly known. On the other hand it is known that
αLSI(P0) =
1 −2µ∗
log

1
µ∗−1
 ,
(2.10)
see [24, Theorem A.2], and the minimum is attained at the indicator function of the conﬁg-
uration σ∗such that µ(σ∗) = µ∗. As a special case, the simple random walk on the complete
graph Kn has αLSI ∼log n.
We remark that (2.10) shows that for any Markov chain P,
αLSI(P0)Entf ⩽Var
p
f ⩽gap(P)−1DP (
p
f,
p
f) .
Therefore,
αLSI(P) ⩾(1 −2µ∗) gap(P)
log

1
µ∗−1

.

LECTURE NOTES ON ENTROPY AND MARKOV CHAINS
11
3. Entropy decompositions and Block dynamics
3.1. Heat bath chains. We now consider a class of Markov chains where at each step a
random region A ⊂[n] is chosen according to some distribution αA and the spins σA =
(σx, x ∈A) are updated by taking a sample from the conditional distribution µτ
A, where
τ = σAc is the current conﬁguration outside of A. This type of chain is usually called a
heat bath dynamics or Gibbs sampler. More formally, given nonnegative weightes αA such
that P
A⊂[n] αA = 1, we write P = Pα where
Pα(σ, η) =
X
A⊂V
αA µσAc
A
(η),
or in a more compact form,
Pαf =
X
A⊂V
αA µAf
We call the associated Markov chain the α-weighted block dynamics. Some examples:
• When αA = 1
n 1|A|=1 this is a single site Gibbs sampler, usually called the Glauber
dynamics.
Example 2.1 coincides with the Glauber dynamics for the Ising model,
while Example 2.2 is the Glauber dynamics for proper colorings.
• Consider the Potts model on a bipartite graph. The even/odd chain is obtained by
taking αE = αO = 1/2 where E ⊂[n] is the subset of even vertices and O ⊂[n] is the
subset of odd vertices.
• In the case of Example 2.3 instead we have binary blocks (edges) with weight given
by αA =
1
|E| 1A∈E.
Exercise 3.1. For any distribution α as above, the matrix Pα deﬁnes a reversible Markov
chain with invariant measure µ and Dirichlet form given by
DPα(f, g) = ⟨f, (1 −Pα)g⟩=
X
A⊂V
αAµ [CovA(f, g)] .
In particular, Pα is positive semi-deﬁnite for any α.
By convexity, it follows that
EntPαf ⩽
X
A⊂V
αA EntµAf.
(3.1)
If we are interested in controlling the entropy contraction of Pα we can look for an upper
bound on the right hand side in (3.1). More precisely, if
X
A⊂V
αA EntµAf ⩽(1 −δ)Entf,
then the relative entropy contraction (2.2) for P = Pα holds with constant δ.
3.2. Useful decompositions. The following decompositions are commonly used.
Lemma 3.2. For all A ⊂[n],
Entf = µ [EntAf] + Ent µA[f].
(3.2)
More generally, for any A1 ⊂· · · ⊂Ak ⊂[n],
EntAkf =
k
X
i=1
µAk

EntAiµAi−1f

,
(3.3)
where we set A0 = ∅and µA0f = f.
The same decompositions apply to the variance
functional, that is when Ent is replaced by Var.

12
PIETRO CAPUTO
Proof. Note that (3.2) is a special case of (3.3) when k = 2, A1 = A, A2 = [n]. To prove
(3.3) we start by noting that
EntAkf = µAk [f log(f/µAkf)]
= µAk

f log(f/µAk−1f)

+ µAk

f log(µAk−1f/µAkf)

= µAk

EntAk−1f

+ EntAkµAk−1f .
(3.4)
We may apply this decomposition again to the ﬁrst term in (3.4) to obtain
EntAkf = µAk

EntAk−2f

+ µAk

EntAk−1µAk−2f

+ EntAkµAk−1f .
Iterating, noting that µAk [EntA0f] = 0, we obtain (3.3). A similar argument proves the
statement for the variance.
□
4. Product measures
When there is no interaction the spin system takes the form of a product measure. It is
instructive to see the form taken by our inequalities in this simple case.
4.1. Subadditivity and tensorization. When µ is a product measure, the functionals
Var and Ent satisfy a family of simple inequalities. Suppose µ = ⊗x∈[n]µx is a product
measure for some µx ∈P(Ωx). Then, for any ν ∈P(Ω), letting νx ∈P(Ωx) denote the
marginal of ν on Ωx, and taking g = P
x∈[n] log(νx/µx) in the variational principle (1.3),
one obtains the subadditivity of relative entropy
H(ν | µ) ⩾ν[g] −log µ [eg] =
X
x∈[n]
νx [log(νx/µx)] =
X
x∈[n]
H(νx | µx) .
(4.1)
In terms of the entropy functional Ent, this says that for any f : Ω7→R+, deﬁning
fx = µ[n]\{x}f, one has the subadditivity statement for product measures
X
x∈[n]
Entfx ⩽Entf.
(4.2)
We note that using (1.1) and translating (4.1) one obtains the familiar subadditivity of the
Shannon entropy of an arbitrary random vector X = (X1, . . . , Xn), that is
H(X) ⩽
n
X
i=1
H(Xi).
Using the decomposition (3.2) for A = {x} and averaging over x ∈[n] one obtains
Entf = 1
n
X
x∈[n]
µ

Ent[n]\{x}f

+ 1
n
X
x∈[n]
Entfx .
Therefore, subadditivity in the form (4.2) is equivalent to
Entf ⩽
1
n −1
X
x∈[n]
µ

Ent[n]\{x}f

,
(4.3)
for all f : Ω7→R+. Applying the same bound with [n] replaced by [n] \ {x}, and then
iterating, shows that
Entf ⩽
1
(n −1)(n −2)
X
x1∈[n]
X
x2∈[n]\{x1}
µ

Ent[n]\{x1,x2}f

⩽
1
(n −1)!
X
x1∈[n]
X
x2∈[n]\{x1}
· · ·
X
xn−1∈[n]\{x1,...,xn−2}
µ

Ent[n]\{x1,...,xn−1}f

.

LECTURE NOTES ON ENTROPY AND MARKOV CHAINS
13
Equivalently,
Entf ⩽
X
x∈[n]
µ

Ent{x}f

,
(4.4)
which is known as the tensorization property of entropy for product measures.
4.2. Shearer inequality. The inequalities (4.2), (4.3) and (4.4) are special cases of the
following much more general estimate.
Lemma 4.1 (Shearer inequality). Suppose µ = ⊗x∈[n]µx is a product measure for some
µx ∈P(Ωx), x ∈[n]. For all α = {αA, A ⊂[n]}, such that αA ⩾0, for all f : Ω7→R+,
γ(α) Entf ⩽
X
A⊂[n]
αA µ [EntAf] ,
where γ(α) = minx∈[n]
P
A∋x αA. The same inequality applies to the variance functional,
that is when Ent is replaced by Var.
Proof. For any A, deﬁne Ai = {x ∈A , x ⩽i} and A−
i = {x ∈A , x < i}. Then (3.3)
implies
EntAf =
X
i∈A
µA
h
EntAiµA−
i f
i
.
Since µ is a product measure one has µAi = µ{i} ⊗µA−
i , and therefore
µA
h
EntAiµA−
i f
i
= µA
h
Ent{i}µA−
i f
i
.
(4.5)
Next, we claim that
µ
h
Ent{i}µA−
i f
i
⩾µ

Ent{i}µ[i−1]f

,
(4.6)
where [i −1] = {1, . . . , i −1} for i ⩾1 and [i −1] = ∅for i = 1. The inequality is actually a
consequence of the more general statement that whenever U, V ⊂[n], with U ∩V = ∅and
µUµV = µV µU then for all f : Ω7→R+ one has
µ [EntUµV f] ⩽µ [EntUf] .
(4.7)
Note that (4.6) follows by applying (4.7) with U = {i}, V = [i −1] \ A−
i and f replaced by
µA−
i f. To prove (4.7), we write
µ [EntUµV f] = µ [(µV f) log(µV f/µUµV f)]
= µ [(µV f) log(µV f/µV µUf)]
= µ [f log(µV f/µV µUf)] .
Taking g = log(µV f/µV µUf), ν = fµU, and observing that µU[eg] = 1, the variational
principle (1.3) shows that
µU [f log(µV f/µV µUf)] = ν[g] ⩽H(ν | µU) = µU [f log(f/µUf)] .
Integrating one concludes µ [EntUµV f] ⩽µ [EntUf].
This ends the proof of (4.7).
To
conclude the lemma, observe that by (3.3) we know that
Entf =
X
i∈[n]
µ

Ent[i]µ[i−1]f

=
X
i∈[n]
µ

Ent{i}µ[i−1]f

.
Therefore, summing over A in (4.5) one obtains
X
A⊂[n]
αA µ [EntAf] ⩾
X
A⊂[n]
αA
X
i∈A
µ

Ent{i}µ[i−1]f

⩾γ(α) Entf.
The proof of the same statement for the variance is left as an exercise.
□
Exercise 4.2. Prove the estimate of Lemma 4.1 for the variance functional.

14
PIETRO CAPUTO
4.3. Shearer inequality for sub-modular functions. The statement of Lemma 4.1 can
be extended as follows. Let Pn denote the set of all subsets of [n] = {1, . . . , n}. A function
h : Pn 7→R is called monotone if h(A) ⩽h(B) whenever A ⊂B and it is called sub-modular
if for all A, B ∈Pn,
h(A) + h(B) ⩾h(A ∩B) + h(A ∪B).
Exercise 4.3. Fix f : Ω7→R+. Show that both h(A) = µ [EntAf] and h(A) = µ [VarAf]
are monotone and sub-modular.
Lemma 4.4. For any monotone and sub-modular function h such that h(∅) = 0, for any
choice of non-negative weights α = {αA, A ⊂[n]}:
γ(α)h([n]) ⩽
X
A
αAh(A),
where γ(α) = mini∈[n]
P
A∋i αA.
Exercise 4.5. Prove Lemma 4.4.
5. Approximate versions in the non-product case
5.1. Approximate tensorization and approximate subadditivity. It is natural to
apply an approximate version of the above arguments to weakly interacting Gibbs measures,
e.g. high temperature Potts models or high q colorings, in which case the distribution µ
is not product but somewhat close to a product measure. One can indeed show that such
systems with weak interactions satisfy an approximate tensorization of the form
Entf ⩽C
X
x∈[n]
µ

Ent{x}f

,
(5.1)
where C ⩾1 is a constant. This recursive approach was initiated in the 90’s with the
works of Martinelli, Olivieri, Stroock, H.T. Yau, Zegarlinski [44, 43, 42, 32, 34]. Broadly
speaking, the main results of these works can be summarized with the statement that for
spin systems on Zd, with ﬁnite or compact spin space, if the system satisﬁes the strong
spatial mixing condition, then the approximate tensorization (5.1) holds, see also [13, 35]
for related results.
A related problem is the validity of an approximate version of the inequality (4.2), that
is the approximate subadditivity statement
X
x∈[n]
Entfx ⩽C Entf,
(5.2)
where fx = µ[n]\{x}f as above and C ⩾1 is a constant. In the case C = 1, as we have seen
in Section 4.1, (5.2) and (5.1) are equivalent. However, in the general case they are not,
and one cannot be deduced from the other even by modifying the constants C involved. A
theorem of Carlen and Cordero shows that (5.2) for all f ⩾0 is equivalent to
µ
" Y
x∈V
ϕx(σx)
#
⩽
Y
x∈V
µ

ϕx(σx)C1/C ,
(5.3)
for any collection of functions ϕx : V 7→R+, where C is the same constant appearing in
(5.2). This follows by an application of the variational principle (1.3) for entropy, see [18,
Theorem 2.1]. It is interesting to recall a result of Carlen, Lieb, Loss [16] establishing that
if µ is the uniform measure on the sphere
Sn−1 = {σ = (σ1, . . . , σn) ∈[−1, 1]n :
X
i
σ2
i = 1},

LECTURE NOTES ON ENTROPY AND MARKOV CHAINS
15
then (5.2) holds with C = 2, and the constant 2 is optimal. We refer to Section 7 below for
a discussion of the analogous problem for permutations. In Section 6 we show that under a
suitable weak dependence assumption a spin system satisﬁes the approximate subadditivity
(5.2) with C = O(1).
5.2. Block factorization and mixing time of block dynamics. For any collection of
weights α we deﬁne Cα as the optimal constant Cα > 0 such that for all f ⩾0,
γ(α) Entf ⩽Cα
X
A⊂[n]
αA µ [EntAf] ,
(5.4)
where γ(α) = minx∈[n]
P
A∋x αA. From Lemma 4.1 we know that Cα = 1 if µ is a product
measure.
Remark 5.1. For any Gibbs measure µ, and for any α with γ(α) > 0, the constant Cα must
be at least 1. Indeed, let f be a function of σx only, where x is such that γ(α) = P
A∋x αA.
Then µ [EntAf] = 0 if A ̸∋x, and µ [EntAf] ⩽Entf if A ∋x. Therefore for this function
one has P
A⊂[n] αA µ [EntAf] ⩽γ(α)Entf, which implies Cα ⩾1.
Remark 5.2. Note that if αA = 1
n1|A|=1 then Cα is the best constant C in the approximate
tensorization statement (5.1), while taking αA = 1
n1|A|=n−1 we obtain that
Csubadd = n −n −1
Cα
is the best constant C in the approximate subadditivity statement (5.2).
Deﬁnition 5.3. We say that µ satisﬁes the block factorization estimate with constant C,
for short BF(C) if Cα ⩽C for all weights α.
Note that BF(1) holds when µ is a product measure.
Lemma 5.4. For all weights α, for all f ⩾0,
Ent(Pαf) ⩽(1 −γ(α)
Cα )Ent(f) ,
and
Tmix(Pα) = O( Cα
γ(α)) log log(1/µ∗)).
In particular, if BF(C) holds, then Tmix(Pα) = O(C/γ(α)) log log(1/µ∗)) for all weights α.
Proof. By deﬁnition of Cα,
P
Λ αΛµ[EntΛ(f)] ⩾γ(α)
Cα Ent(f).
By convexity and Lemma 3.2
Ent(Pαf) ⩽P
Λ αΛ µ[Ent(µΛ(f))]
= Ent(f) −P
Λ αΛµ[EntΛ(f)] ⩽(1 −γ(α)
Cα )Ent(f).
□
Exercise 5.5. For any α, prove that gap(Pα) ⩽γ(α). Thus, using (2.6) one has that the
bound on Tmix(Pα) is tight up to O(Cα log log(1/µ∗))).
6. Block factorizations under spectral independence
The concept of entropy or variance factorization is at the heart of many results on rapid
mixing for Markov chains, see e.g. [33, 5, 19, 23]. Roughly speaking these works obtained
bounds on spectral gaps or mixing times for the Glauber dynamics (single site updates) by
using suitable recursive arguments based on repeated two-blocks factorizations. Here we
investigate much more general factorizations in order to obtain bounds on arbitrary block
dynamics.
The notion of BF(C) presented above was introduced in [14], where we proved that for
any spin system on a graph G ⊂Zd, d ⩾1, if the strong spatial mixing (SSM) holds, then

16
PIETRO CAPUTO
BF(C) holds for some constant C ⩾1. The results were then extended to obtain optimal
mixing for the Swendsen-Wang dynamics on Zd in [6].
The block factorization result of [14] was recently reﬁned in [7], where we consider ar-
bitrary graphs with bounded degrees and assume only spectral independence, a property
weaker than SSM. Spectral independence was recently introduced in [2]. Under the spectral
independence assumption, in [7] a tight mixing time bound was obtained for the Swendsen-
Wang dynamics of the ferromagnetic Potts model on arbitrary graphs with bounded degree.
We shall review the main results of [7].
6.1. Spectral independence. The following matrix captures the pairwise inﬂuence be-
tween vertices. For a pair of vertices x, y ∈[n] and a pair of single spin values a, a′ ∈X, it
measures the inﬂuence of the spin a at x on a′ at y. We deﬁne X = {(x, a), x ∈[n], a ∈X}
as the set of possible pairs of sites and single spin values, that is X = [n] × X.
Deﬁnition 6.1 (Inﬂuence matrix [2]). The ALO inﬂuence matrix J ∈RX×X is deﬁned by
J(x, a; x, a′) = 0 for all x, a, a′ and
J(x, a; y, a′) = µ(σy = a′ | σx = a) −µ(σy = a′)
for x ̸= y.
For a boundary condition τ ∈XU on some set of vertices U (also referred to as a pinning
on U), the matrix Jτ is deﬁned as above when µ is replaced by µτ := µ(· | σU = τ), that the
Gibbs measure µ conditioned on having the spin conﬁguration τ on the set U.
For any pinning τ, all row sums of Jτ vanish, and therefore the matrix Jτ has always
the eigenvalue zero. We denote by λmax(Jτ) ⩾0 the maximal eigenvalue of Jτ.
Deﬁnition 6.2 (Spectral independence [2]). We say that a spin system is η-spectrally
independent if for all possible pinnings τ we have λmax(Jτ) ⩽η.
Given b > 0, we say that the Gibbs measure µ is b-marginally bounded if for any
pinning τ and for any a ∈X such that µτ(σx = a) > 0 one has µτ(σx = a) ⩾b. All
systems we consider here are b-marginally bounded for some b > 0 uniformly in n, and
have an underlying interaction graph with bounded degrees, except for the uniform random
permutation model in Example 2.3 which has b = 1/n and mean ﬁeld type interaction
(degree of size n), therefore the following theorem is not interesting in that case.
See
however Section 7 for speciﬁc results in the permutation model case. The following was
proved in [7].
Theorem 6.3. Consider a Gibbs measure µ on a graph G = ([n], E) with maximum de-
gree ∆, and assume that µ is b-marginally bounded for some b > 0. If µ is η-spectrally
independent, then BF(C) holds with constant C = C(η, ∆, b).
Before getting to the proof of Theorem 6.3 we establish some preliminary facts.
6.2. Approximate subadditivity and uniform factorizations under spectral in-
dependence. The following theorem was obtained in [7]; see also [21] for an earlier sim-
ilar result. We use the notation Av |U|=ℓto denote the uniform average over all subsets
U ⊂V := [n] such that |U| = ℓ.
Theorem 6.4. If the spin system is η-spectrally independent and b-marginally bounded
then there exists a constant C = O(1 + η
b ) such that for any ℓ= {1, . . . , n −1} and for all
f ⩾0:
n
ℓAv |U|=ℓEnt(µV \Uf) ⩽C Entf.
(6.1)
In particular, (5.2) holds with C = O(1 + η
b ). Moreover, for any θ ∈(0, 1], there exists
C =
  1
θ
O( η
b ) such that for ℓ= ⌈θn⌉:
ℓ
n Entf ⩽C Av |Λ|=ℓµ [EntΛf] .
(6.2)

LECTURE NOTES ON ENTROPY AND MARKOV CHAINS
17
Note that (6.2) is equivalent to saying that for all α of the form αΛ =
 n
ℓ
−11|Λ|=ℓ, for
some ℓ, one has Cα = C.
We articulate the proof in two steps. The ﬁrst step deﬁnes the recursive scheme, which
allows one to go from a local inequality to a global one; see Lemma 6.7. The second step is a
control of the local inequality; see Lemma 6.8. The main line of attack is inspired by several
recent papers, among which [1, 3, 25, 22, 21]. This local to global procedure is reminiscent
of the recursive approach developed in [15, 12, 11], where similar ideas were used to derive
spectral gap estimates for a class of conservative spin systems. The argument here seems
to be more robust and, unlike the one in [15, 12, 11], it does not rely on symmetries of the
underlying measures.
6.3. Setting up the recursion. If U ⊂V , and τ = τU a conﬁguration of spins on U,
recall that we use notation µτ = µ(· | σU = τ) for the conditional distribution µV \U when
the spins on U are given by τ. Moreover, we write µτ,x = µ(· | τ ∪σx) if we additionally
condition on the spin σx at vertex x /∈U and similarly for µτ,x,y = µ(· | τ ∪σx ∪σy) for
x, y /∈U, so that e.g. the expression µτ [Entµτ,x,yf] indicates the entropy of f with respect
to µ(· | τ ∪σx ∪σy), that is
Entµτ,x,yf = µτ,x,y[f log(f/µτ,x,y(f))] ,
averaged over the two spins σx, σy sampled according to µτ.
Deﬁne the constants αk,
k = 0, . . . , n −2, as the largest numbers such that the inequalities
(1 + αk) Av x/∈U Entµτ (µτ,x(f)) ⩽Av x,y /∈U Entµτ (µτ,x,y(f)) ,
(6.3)
hold for all k = 0, . . . , n −2, for all U ⊂[n] with |U| = k, for all conﬁgurations τ on U
and for all functions f ⩾0. The symbol Av x/∈U denotes the uniform average over all n −k
vertices x /∈U, and Av x,y /∈U stands for the uniform average over all (n −k)(n −k −1)
pairs (x, y) with x, y /∈U and x ̸= y. We refer to (6.3) as the local inequality, since for each
choice of x, y, the distributions involved are concerned with the spins at two vertices only.
Exercise 6.5. If µ is a product measure, show that
Entµτ (µτ,x,y(f)) ⩾Entµτ (µτ,x(f)) + Entµτ (µτ,y(f)).
(6.4)
Remark 6.6. Fix x, y /∈U. Using µτ,xf = µτ,xµτ,x,yf, from Lemma 3.2 we have the
decomposition
Entµτ (µτ,x,y(f)) = Entµτ (µτ,x(f)) + µτ [Entµτ,x(µτ,x,y(f)] .
In particular, Entµτ (µτ,x,y(f)) ⩾Entµτ (µτ,x(f)) and therefore (6.3) is always true with
αk = 0. If µ is a product measure then the subadditivity (6.4) implies the validity of (6.3)
with αk = 1 for all k = 0, . . . , n −2. In the general case one has αk ∈[0, 1]
The recursion is based on the following statement, which rephrases [21, Theorem 5.4].
Lemma 6.7. Let αk, k = 0, . . . , n −2, be deﬁned by (6.3). Then, for all functions f ⩾0,
Av |U|=jEnt(µV \Uf) ⩽(1 −κj)Ent(f),
j = 1, . . . , n −1,
(6.5)
where
κj =
Pn−1
i=j Γi
Pn−1
i=0 Γi
,
Γi =
i−1
Y
k=0
αk ,
Γ0 = 1.
Proof. The claim (6.5) follows from the fact that for all k = 1, . . . , n −1:
Av |U|=k Ent(µV \Uf) ⩽δk Av |U|=k+1 Ent(µV \Uf) ,
δk =
Pk−1
i=0 Γi
Pk
i=0 Γi
,
(6.6)
since Av |U|=n Ent(µV \Uf) = Ent(f), and δjδj+1 · · · δn−1 = (1 −κj).

18
PIETRO CAPUTO
To prove (6.6), note that it holds for k = 1 with δ1 = 1/(1 + α0) = Γ0/(Γ0 + Γ1) by the
assumption (6.3) at τ = ∅. Next, we suppose it holds for 0 < k −1 < n −1 and show it for
k. For any |U| = k + 1 and U′ ⊂U with |U′| = k −1, setting {x, y} = U \ U′ and letting
τ = τU′ be the conﬁguration on U′, as in Lemma 3.2 we have the decomposition
Ent(µV \Uf) = Ent(µ(µV \Uf | τU′)) + µ

Ent(µV \Uf | τU′)

= Ent(µV \U′f) + µ [Entµτ (µτ,x,yf)] .
Averaging we obtain
Av |U|=k+1Ent(µV \Uf) = Av |U′|=k−1Ent(µV \U′f)
+ Av |U′|=k−1 Av x,y /∈U′µ [Entµτ (µτ,x,yf)] .
In the same way
Av |U|=kEnt(µV \Uf) = Av |U′|=k−1Ent(µV \U′f)
+ Av |U′|=k−1 Av x/∈U′µ [Entµτ (µτ,xf)] .
From (6.3),
Av |U|=k+1Ent(µV \Uf) −Av |U′|=k−1Ent(µV \U′f)
⩾(1 + αk−1) Av |U′|=k−1 Av x/∈U′µ [Entµτ (µτ,xf)]
= (1 + αk−1)

Av |U|=kEnt(µV \Uf) −Av |U′|=k−1Ent(µV \U′f)

.
Therefore,
Av |U|=k+1Ent(µV \Uf) ⩾(1 + αk−1) Av |U|=kEnt(µV \Uf) −αk−1 Av |U′|=k−1Ent(µV \U′f).
By the inductive assumption (6.6) at k −1 we have
Av |U|=k+1Ent(µV \Uf) ⩾(1 + αk−1 −αk−1δk−1) Av |U|=kEnt(µV \Uf)
= δ−1
k
Av |U|=kEnt(µV \Uf).
□
6.4. Estimating the local coeﬃcients. The next step is an estimate on the coeﬃcients
αk appearing in (6.3).
Lemma 6.8. If the spin system is η-spectrally independent and b-marginally bounded then
the local inequality (6.3) holds with
αk ⩾1 −
2η
b(n −k −1).
Proof. Fix U ⊂V , |U| = k ⩽n −2 and τ = τU. We may assume µτ(f) = 1, which implies
µτ(µτ,x,y(f)) = µτ(µτ,x(f)) = 1 for all x, y /∈U. For simplicity, we write Av x,y and Av x
for the averages Av x,y /∈U and Av x/∈U. Observe that
Av x,y Entµτ (µτ,x,y(f)) −2 Av x Entµτ (µτ,x(f))
= Av x,y µτ [µτ,x,y(f) log µτ,x,y(f) −µτ,x(f) log µτ,x(f) −µτ,y(f) log µτ,y(f)]
= Av x,y µτ

µτ,x,y(f) log
µτ,x,y(f)
µτ,x(f)µτ,y(f)

.
Using a log(a/b) ⩾a −b for all a, b ⩾0,
Av x,y Entµτ (µτ,x,y(f)) −2 Av x Entµτ (µτ,x(f))
⩾1 −Av x,y µτ [µτ,x(f)µτ,y(f)]
= −Av x,y µτ [(µτ,x(f) −1)(µτ,y(f) −1)] .
(6.7)

LECTURE NOTES ON ENTROPY AND MARKOV CHAINS
19
We may rewrite
Av x,y µτ [(µτ,x(f) −1)(µτ,y(f) −1)]
=
1
n −k −1
X
(x,a)
ν(x, a)ϕ(x, a)[Jτϕ](x, a),
(6.8)
where
ϕ(x, a) = µτ(f | σx = a) −1 = [µτ,x(f)](a) −1,
X is the set of all pairs (x, a) where x ∈V \ U (if U is the set where τ = τU is speciﬁed)
and a ∈[q], ν denotes the probability measure on X obtained by setting
ν(x, a) =
1
n −k µτ(σx = a),
and Jτ : X × X 7→R denotes the inﬂuence matrix from Deﬁnition 6.1. Note that in the
derivation of (6.8) we have used the fact that for each ﬁxed y /∈U one has
X
a′∈[q]
ν(y, a′)ϕ(y, a′) =
1
n −k µτ(µτ,y(f) −1) = 0.
Observe that Jτ is self-adjoint in L2(X, ν):
ν(x, a)Jτ(x, a; y, a′) = ν(y, a′)Jτ(y, a′; x, a).
In particular, its eigenvalues are real. Let η ⩾0 denote its largest eigenvalue (the eigenvalue
zero always exists since all row sums of Jτ vanish). Letting ⟨·, ·⟩denote the scalar product
in L2(X, ν) we have ⟨ψ, Jτψ⟩⩽η⟨ψ, ψ⟩for all ψ ∈L2(X, ν). Therefore,
Av x,y µτ [(µτ,x(f) −1)(µτ,y(f) −1)]
=
1
n −k −1⟨ϕ, Jτϕ⟩⩽
η
n −k −1⟨ϕ, ϕ⟩
=
η
n −k −1 Av x µτ 
(µτ,x(f) −1)2
=
η
n −k −1 Av xVarµτ (µτ,x(f)).
Recalling (6.7) we have shown
Av x,y Entµτ (µτ,x,y(f)) −2 Av x Entµτ (µτ,x(f))
⩾−
η
n −k −1 Av xVarµτ (µτ,x(f)).
(6.9)
Next, observe that for every ﬁxed x /∈U, setting hτ(σx) = [µτ,x(f)](σx):
Varµτ (µτ,x(f)) =
X
a
µτ(σx = a)(hτ(a) −1)2
⩽1
b
 X
a
µτ(σx = a)|hτ(a) −1|
!2
where b = minx/∈U mina µτ(σx = a), with the minimum over a restricted to spin values that
are allowed at x, that is such that µτ(σx = a) > 0, and we have used P
i a2
i ⩽(P
i ai)2 for
all ai ⩾0. Pinsker’s inequality shows that
X
a
µτ(σx = a)|hτ(a) −1| ⩽
q
2 Entµτ (µτ,x(f)).
It follows that
Varµτ (µτ,x(f)) ⩽2
b Entµτ (µτ,x(f)).
(6.10)
Inserting (6.10) into (6.9) concludes the proof.
□

20
PIETRO CAPUTO
6.5. Proof of Theorem 6.4. From Lemma 6.7, we see that (6.1) holds with C = n
ℓ(1−κℓ).
From Lemma 6.8 if follows that
αk ⩾max{1 −R/(n −k −1), 0},
R = ⌈2η/b⌉.
Using this bound in the deﬁnition of the coeﬃcients κℓand rearranging, see Section 2.2 of
[21], it is not hard to see that for any 1 ⩽ℓ⩽n −1:
κℓ⩾(n −ℓ−1) · · · (n −ℓ−R)
(n −1) · · · (n −R)
.
(6.11)
In particular,
n
ℓ(1 −κℓ) ⩽n
ℓ

1 −(n −ℓ−1) · · · (n −ℓ−R)
(n −1) · · · (n −R)

.
Remarkably, the expression in the right hand side above is decreasing with ℓ, and therefore
it is always less than R+1, its value at ℓ= 1. This shows that (6.1) holds with C ⩽R+1 =
O(1 + η
b ).
To prove (6.2), we start with the decomposition
Av |Λ|=ℓµ [EntΛf] = Ent(f) −Av |U|=n−ℓEnt

µV \Uf

,
which follows from Lemma 3.2. Therefore Lemma 6.7 implies that (6.2) holds with C =
ℓ
n κn−ℓ. Using (6.11) we see that
ℓ
n κn−ℓ
⩽(n −1) · · · (n −R)
(ℓ−1) · · · (ℓ−R) .
In particular, if ℓ= ⌈θn⌉with θ ∈(0, 1] ﬁxed, then for all suﬃciently large n one has
ℓ
n κn−ℓ⩽(1
θ)O(R). This ends the proof of Theorem 6.4.
6.6. Proof of Theorem 6.3. We brieﬂy sketch the main ideas involved in the proof.
1) Note that G is k-partite for some k ⩽∆+1 and let V1, . . . , Vk be disjoint independent
sets such that [n] = ∪k
i=1Vi. The following lemma reduces the general BF problem to
a factorization into the independent sets.
Lemma 6.9. If µ satisﬁes
Entf ⩽C 1
k
k
X
i=1
µ [EntVif] ,
(6.12)
then BF(C) holds with the same constant C.
2) Use Theorem 6.3 to obtain the ℓ-uniform block factorization (UBF) with ℓ= ⌊θn⌋:
ℓ
n Entf ⩽CUBF
1
 n
ℓ

X
|S|=ℓ
µ [EntSf] ,
where CUBF = CUBF(η, b, θ, ∆).
3) Prove that if θ is small enough, θ ⩽θ0(∆), then the ⌊θn⌋-UBF implies the k-partite
factorization (6.12) with constant C = C(θ, ∆). This is a delicate part of the argument,
see [7] for the details.
6.7. Some applications. We turn to some applications of Theorem 6.3.

LECTURE NOTES ON ENTROPY AND MARKOV CHAINS
21
6.7.1. Ising and Potts models. Consider the Ising Gibbs measure from Example 1.2, in the
ferromagnetic case, that is β ⩾0. We deﬁne
βc(∆) := log

∆
∆−2

.
This parameter is known to be the threshold of the uniqueness/non-uniqueness phase tran-
sition on the ∆-regular tree, see Mossel and Sly [38] and references therein for more details.
Theorem 6.10. For any ∆⩾3, 0 ⩽β < βc(∆), there exists a constant C = C(β, ∆) > 0
depending only on β, ∆such that the ferromagnetic Ising model on any graph of maxi-
mum degree ∆satisﬁes the block factorization BF(C). In particular, any α-weighted block
dynamics has mixing time Tmix = O(γ(α)−1 log n).
In the case of Glauber dynamics αA = 1
n1|A|=1 the above theorem predicts the correct
O(n log n) mixing time bound, which was already established in [38]. In light of Lemma
5.4 and Theorem 6.3, to prove Theorem 6.10 all we need to check is that under these
assumptions there exists η > 0 such that the ferromagnetic Ising model on any graph of
maximum degree ∆is η-spectrally independent. For a proof of this fact we refer to [20].
In fact, these results up to the critical threshold hold for antiferromagnetic Ising model as
well and for the hard-core model in Example 1.5. Again it suﬃces to prove the spectral
independence statement and the rest follows from Lemma 5.4 and Theorem 6.3. We refer
to [2, 21] and references therein for the validity of the spectral independence in these cases.
The results can also be extended to the ferromagnetic Potts model, that is Example 1.3,
with q ⩾3. However, here the spectral independence is known to hold for suﬃciently high
temperature only (not up to the critical tree threshold as in the case q = 2), see [7, Theorem
4.13] for more details.
6.7.2. Swendsen-Wang dynamics. In [8, 7] we extended Theorem 6.10 to obtain optimal
mixing bounds for the Swendsen-Wang (SW) dynamics. The latter is deﬁned as follows.
Let µ be the Potts distribution on G with conﬁguration space Ω= [q]V . The SW dynamics
takes a spin conﬁguration, transforms it into a “joint” spin-edge conﬁguration, performs
a step in the joint space, and then drops the edges to obtain a new Potts conﬁguration.
Formally, from a Potts conﬁguration σt ∈[q]V , a transition σt →σt+1 of the SW dynamics
is deﬁned as follows:
(1) Let Mt = M(σt) denote the set of monochromatic edges in σt.
(2) Independently for each edge e ∈Mt, keep e with probability p = 1 −exp(−β) and
remove e with probability 1 −p. Let At ⊂Mt denote the resulting subset.
(3) In the subgraph (V, At), independently for each connected component C (including
isolated vertices), choose a spin sC uniformly at random from [q] and assign to each
vertex in C the spin sC. This spin assignment deﬁnes σt+1.
We refer to these works for more background and for the details of the proof of the
following statement.
Theorem 6.11. In all cases covered by Theorem 6.10, the Swendsen-Wang dynamics has
mixing time Tmix = Θ(log n). The same conclusions hold for the ferromagnetic Potts model
with q ⩾3, provided β is suﬃciently small.
The proof is based on the concept of spin-edge factorization of entropy described as
follows. Consider the “joint” Edwards-Sokal distribution for G with parameters p ∈[0, 1]
and integer q ≥2. This is the probability measure ν on ΩJ = Ω×{0, 1}E, the set of “joint”
spin-edge conﬁgurations (σ, A) consisting of a spin assignment to the vertices σ ∈Ωand a
subset of edges A ⊂E, such that
ν(σ, A) = 1
Zj
p|A|(1 −p)|E|−|A|1(σ ∼A),

22
PIETRO CAPUTO
where σ ∼A means that A ⊂M(σ) (i.e., every edge in A is monochromatic in σ) and Zj
is the corresponding normalizing constant or partition function. When p = 1 −e−β, the
“spin marginal” of ν is precisely the Potts distribution µ, while the “edge marginal” of ν
corresponds to the random-cluster measure. For a ﬁxed conﬁguration σ ∈Ωand subset
of edges A ⊂E, Entν(f | σ) and Entν(f | A) denote the entropy of f with respect to the
conditional measures ν(· | σ) and ν(· | A), respectively. More precisely, for a given σ ∈Ω,
ν(· | σ) is the measure ν conditioned on the event that the spin conﬁguration is equal to σ,
and for a given A ⊂E, ν(· | A) is the measure ν conditioned on the event that the edge
conﬁguration is equal to A. In this way, Entν(f | σ) and Entν(f | A) are functions of σ and
A, respectively, and ν [Entν(f | σ)], ν [Entν(f | A)] denote the corresponding expectations
with respect to ν.
We say that ν satisﬁes the spin-edge factorization of entropy with
constant C if for all f : ΩJ 7→R+
Entν(f) ⩽C (ν [Entν(f | σ)] + ν[Entν(f | A)]) .
(6.13)
It is possible to show that once (6.13) is available with C = O(1) then the mixing time
of SW is O(log n). Thus, the proof of Theorem 6.11 is reduced to proving (6.13). On the
other hand, as shown in [6, 7] one can prove that BF(C), C = O(1), implies the spin-edge
factorization of entropy, with possibly diﬀerent constant from C′ = O(1).
6.7.3. Colorings. Finally, let us consider proper colorings of a graph as in Example 1.4. An
application of Theorem 6.3 in this case yields the following statement.
Theorem 6.12. If q > (11
6 −ϵ0)∆, where ϵ0 ≈10−5 > 0 is a ﬁxed constant, there exists
a constant C = C(q, ∆) depending only on q, ∆, such that the uniform distribution over
q-colorings of any graph of maximum degree ∆satisﬁes the block factorization BF(C). In
particular, any α-weighted block dynamics has mixing time Tmix = O(γ(α)−1 log n).
The reason for the condition q > (11
6 −ϵ0)∆is that under this assumption it is known
that an auxiliary dynamics on proper q-colorings, known as the ﬂip dynamics, has good
contraction properties. The latter, in turn, can be shown to imply spectral independence,
and therefore the above theorem follows again from Lemma 5.4 and Theorem 6.3. We refer
to [7, Section 4] for the details. It is a major open problem in the ﬁeld to obtain such
results under less restrictive conditions on q.
7. Entropy inequalities for permutations
Consider the uniform distribution over the symmetric group Sn, as in Example (1.6).
With A. Bristiel we prove the following sharp ℓ-UBF factorization with explicit constant
in this case, see [10].
Theorem 7.1. For any ℓ= 1, . . . , n, for any f : Sn 7→R+
ℓ
n Entf ⩽K(n, ℓ)
 n
ℓ

X
|A|=ℓ
µ [EntAf] ,
K(n, ℓ) = ℓlog(n!)
n log(ℓ!).
The inequality is saturated at any multiple of a Dirac mass at a single permutation, and
there are no other extremal functions.
Note that ℓ= 1 is trivial since ﬁxing all labels except x determines the label at x.
Similarly, the case ℓ= n is trivial with K(n, n) = 1. Using Lemma 3.2, the above theorem
is equivalent to the statement that for any ℓ= 1, . . . , n, for any f : Sn 7→R+
1
 n
ℓ

X
|A|=ℓ
Ent µAf ⩽

1 −log(ℓ!)
log(n!)

Entf.

LECTURE NOTES ON ENTROPY AND MARKOV CHAINS
23
In particular, taking ℓ= n −1 shows that for all f : Sn 7→R+, setting fx = µ[n]\{x},
X
x∈[n]
Ent fx ⩽n log n
log(n!) Entf,
(7.1)
Note that this establishes the subadditivity (5.2) with constant C given by
n log n
log(n!) ∼1 +
1
log n.
A previous bound by Carlen, Lieb, and Loss has 2 instead of n log n
log(n!), see [17]. Proving the
optimal bound (7.1) had been an open problem for many years, see Section 7.1 below.
The proof is obtained by using a suitable recursive technique, based on the fact that when
a single label σx is ﬁxed then the conditional distribution is uniform over permutations of
[n −1]. The usual decomposition gives
Entf = 1
n
X
x∈[n]
µ

Ent[n]\{x}f

+ 1
n
X
x∈[n]
Entfx ,
and the ﬁrst term can be estimated with the inductive hypothesis. The second term however
requires work. A similar approach was used in the case ℓ= 2, that is Random Transposi-
tions, see Lee,Yau [30] for the LSI, and Goel and Guo,Quastel for the MLSI [28, 27]. Note
however that none of these works obtains an exact constant as in our estimate. This shows
that there is an advantage in considering block factorization constants instead of LSI or
MLSI.
7.1. A permanent bound using entropy subadditivity for permutations. As a
combinatorial application of this result we mention the following sharp upper bound on
the permanent of a matrix with arbitrary nonnegative entries, which was independently
conjectured by the author, by Carlen, Lieb, Loss [17] and by Samorodnitsky [40].
Let
A = (ai,j) denote an n × n matrix, and write
perm(A) =
X
σ∈Sn
n
Y
i=1
ai,σi,
for the permanent of A.
Theorem 7.2. For any p ⩾1, for any n × n nonnegative matrix A,
perm(A) ⩽max

1, n!
nn/p
 n
Y
i=1
∥Ri∥p,
(7.2)
where Ri denotes the i−th row of A and ∥·∥p denotes the ℓp-norm of a vector, and equality
is uniquely achieved at either the identity matrix or the all - 1 matrix (up to permutation of
rows and multiplication by a scalar).
Note that pc := n log n
log(n!) is the value at which the increasing function
p 7→
n!
nn/p
takes the value 1, and that the values 1 and
n!
nn/p correspond to the case where A is the
identity matrix or A is the all-1 matrix respectively, and thus (7.2) is optimal.
The proof is based on the sharp subadditivity result (7.1). The main observation is that
(7.1), as we discussed above, is equivalent to (5.3) with C = pc. Consider now the matrix
A = (ax,y) such that ax,y = ϕx(y). The left hand side in (5.3) equals (1/n!)perm(A), while
for every x:
µ [ϕx(σx)pc]1/pc = n−1/pc∥Rx∥pc =
 1
n!
1/n
∥Rx∥pc,

24
PIETRO CAPUTO
where Rx denotes the x-th row of A. Therefore (5.3) proves the theorem at p = pc. As
already noted in [40, Lemma 1], this is suﬃcient to prove the desired statement for all
p ⩾1.
The use of entropy to prove upper bounds on the permanent goes back to [41, 39]. We
refer to [29, 4] for further varations of the Bregman-Minc theorem.
8. Open problems
Problem 8.1. Can one remove the assumption of bounded degree in Theorem 6.3, that is
prove BF for Gibbs measures on arbitrary graphs assuming spectral independence only ?
For example, consider the sub-critical Curie-Weiss model (Ising model on complete graph
in the uniqueness regime).
Problem 8.2. Theorem 7.1 computes, in the case of uniform permutations, the optimal
constant Cα as deﬁned in (5.4) for all α of the form αA =
 n
ℓ
−11|A|=ℓ. What can be said
about other weights α ? What is a relevant version of Shearer inequality for uniformly
random permutations ?
References
[1] Vedat Levi Alev and Lap Chi Lau. Improved analysis of higher order random walks and applications. In
Proceedings of the 52nd Annual ACM SIGACT Symposium on Theory of Computing, pages 1198–1211,
2020. 17
[2] N. Anari, K. Liu, and S. Oveis Gharan. Spectral independence in high-dimensional expanders and
applications to the hardcore model. In Proceedings of the 61st Annual IEEE Symposium on Foundations
of Computer Science (FOCS), pages 1319–1330, 2020. 16, 21
[3] Nima Anari, Kuikui Liu, Shayan Oveis Gharan, and Cynthia Vinzant. Log-concave polynomials ii:
high-dimensional walks and an fpras for counting bases of a matroid. In Proceedings of the 51st Annual
ACM SIGACT Symposium on Theory of Computing, pages 1–12, 2019. 17
[4] Nima Anari and Alireza Rezaei. A tight analysis of Bethe approximation for permanent. SIAM Journal
on Computing, (0):FOCS19–81, 2021. 24
[5] Lorenzo Bertini, Nicoletta Cancrini, and Filippo Cesi. The spectral gap for a Glauber-type dynamics
in a continuous gas. In Annales de l’IHP Probabilit´es et statistiques, volume 38, pages 91–108, 2002. 15
[6] A. Blanca, P. Caputo, D. Parisi, A. Sinclair, and E. Vigoda. Entropy decay in the Swendsen-Wang
dynamics. In Proceedings of the 53rd Annual ACM Symposium on Theory of Computing (STOC), 2021.
16, 22
[7] Antonio Blanca, Pietro Caputo, Zongchen Chen, Daniel Parisi, Daniel ˇStefankoviˇc, and Eric Vigoda. On
mixing of markov chains: Coupling, spectral independence, and entropy factorization. In Proceedings
of the 2022 Annual ACM-SIAM Symposium on Discrete Algorithms (SODA), pages 3670–3692. SIAM,
2022. 16, 20, 21, 22
[8] Antonio Blanca, Pietro Caputo, Daniel Parisi, Alistair Sinclair, and Eric Vigoda. Entropy decay in the
Swendsen–Wang dynamics on Zd. The Annals of Applied Probability, 32(2):1018–1057, 2022. 21
[9] Sergey G. Bobkov and Prasad Tetali. Modiﬁed logarithmic Sobolev inequalities in discrete settings. J.
Theoret. Probab., 19(2):289–336, 2006. 9
[10] Alexandre Bristiel and Pietro Caputo. Entropy inequalities for random walks and permutations. arXiv
preprint arXiv:2109.06009, 2021. 22
[11] P. Caputo. Spectral gap inequalities in product spaces with conservation laws. Stochastic analysis on
large scale interacting systems, 39:53–88, 2004. 17
[12] P. Caputo and F. Martinelli. Relaxation time of anisotropic simple exclusion processes and quantum
Heisenberg models. The Annals of Applied Probability, 13(2):691–721, 2003. 17
[13] Pietro Caputo, Georg Menz, and Prasad Tetali. Approximate tensorization of entropy at high temper-
ature. In Annales de la Facult´e des sciences de Toulouse: Math´ematiques, volume 24, pages 691–716,
2015. 14
[14] Pietro Caputo and Daniel Parisi. Block factorization of the relative entropy via spatial mixing. Com-
munications in Mathematical Physics, 388(2):793–818, 2021. 15, 16
[15] E. A. Carlen, M. C. Carvalho, and M. Loss. Determination of the spectral gap for Kac’s master equation
and related stochastic evolution. Acta Mathematica, 191:1–54, 2003. 17
[16] E. A. Carlen, E. H. Lieb, and M. Loss. A sharp analog of Young’s inequality on SN and related entropy
inequalities. The Journal of Geometric Analysis, 14:487–520, 2004. 14

LECTURE NOTES ON ENTROPY AND MARKOV CHAINS
25
[17] Eric Carlen, Elliott H Lieb, and Michael Loss. An inequality of Hadamard type for permanents. Methods
and Applications of Analysis, 13(1):1–18, 2006. 23
[18] Eric A Carlen and Dario Cordero-Erausquin. Subadditivity of the entropy and its relation to brascamp–
lieb type inequalities. Geometric and Functional Analysis, 19(2):373–405, 2009. 14
[19] Filippo Cesi. Quasi-factorization of the entropy and logarithmic Sobolev inequalities for Gibbs random
ﬁelds. Probab. Theory Related Fields, 120(4):569–584, 2001. 15
[20] Z. Chen, K. Liu, and E. Vigoda. Rapid mixing of Glauber dynamics up to uniqueness via contraction.
In Proceedings of the 61st Annual IEEE Symposium on Foundations of Computer Science (FOCS),
pages 1307–1318, 2020. 21
[21] Z. Chen, K. Liu, and E. Vigoda. Optimal mixing of Glauber dynamics: Entropy factorization via high-
dimensional expansion. In Proceedings of the 53rd Annual ACM Symposium on Theory of Computing
(STOC), 2021. 16, 17, 20, 21
[22] M. Cryan, H. Guo, and G. Mousa. Modiﬁed log-Sobolev inequalities for strongly log-concave distribu-
tions. The Annals of Probability, 49(1):506–525, 2021. 17
[23] Paolo Dai Pra, Anna Maria Paganoni, and Gustavo Posta. Entropy inequalities for unbounded spin
systems. Ann. Probab., 30(4):1959–1976, 2002. 15
[24] P. Diaconis and L. Saloﬀ-Coste. Logarithmic Sobolev inequalities for ﬁnite Markov chains. The Annals
of Applied Probability, 9(3):695–750, 1996. 9, 10
[25] Irit Dinur and Tali Kaufman. High dimensional expanders imply agreement expanders. In 2017 IEEE
58th Annual Symposium on Foundations of Computer Science (FOCS), pages 974–985. IEEE, 2017. 17
[26] Sacha Friedli and Yvan Velenik. Statistical mechanics of lattice systems: a concrete mathematical
introduction. Cambridge University Press, 2017. 3
[27] Fuqing Gao and Jeremy Quastel. Exponential decay of entropy in the random transposition and
Bernoulli-Laplace models. Ann. Appl. Probab., 13(4):1591–1600, 2003. 23
[28] Sharad Goel. Modiﬁed logarithmic Sobolev inequalities for some models of random walk. Stochastic
Process. Appl., 114(1):51–79, 2004. 23
[29] Leonid Gurvits and Alex Samorodnitsky. Bounds on the permanent and some applications. In 2014
IEEE 55th Annual Symposium on Foundations of Computer Science, pages 90–99. IEEE, 2014. 24
[30] Tzong-Yow Lee and Horng-Tzer Yau. Logarithmic Sobolev inequality for some models of random walks.
The Annals of Probability, 26(4):1855–1873, 1998. 23
[31] D. A. Levin and Y. Peres. Markov chains and mixing times. American Mathematical Society, 2017. 6,
8
[32] Sheng Lin Lu and Horng-Tzer Yau. Spectral gap and logarithmic Sobolev inequality for Kawasaki and
Glauber dynamics. Comm. Math. Phys., 156(2):399–433, 1993. 14
[33] Fabio Martinelli. Lectures on Glauber dynamics for discrete spin models. In Lectures on probability the-
ory and statistics (Saint-Flour, 1997), volume 1717 of Lecture Notes in Math., pages 93–191. Springer,
Berlin, 1999. 15
[34] Fabio Martinelli and Enzo Olivieri. Approach to equilibrium of Glauber dynamics in the one phase
region. II. The general case. Comm. Math. Phys., 161(3):487–514, 1994. 14
[35] Katalin Marton. An inequality for relative entropy and logarithmic Sobolev inequalities in Euclidean
spaces. J. Funct. Anal., 264(1):34–61, 2013. 14
[36] Laurent Miclo. Remarques sur l?hypercontractivit´e et l?´evolution de l?entropie pour des chaˆınes de
markov ﬁnies. pages 136–167, 1997. 10
[37] Ravi Montenegro and Prasad Tetali. Mathematical aspects of mixing times in Markov chains. Found.
Trends Theor. Comput. Sci., 1(3):x+121, 2006. 6
[38] E. Mossel and A. Sly. Exact thresholds for Ising–Gibbs samplers on general graphs. The Annals of
Probability, 41(1):294–328, 2013. 21
[39] Jaikumar Radhakrishnan. An entropy proof of Bregman’s theorem. Journal of combinatorial theory,
Series A, 77(1):161–164, 1997. 24
[40] Alex Samorodnitsky. An upper bound for permanents of nonnegative matrices. Journal of Combinato-
rial Theory, Series A, 115(2):279–292, 2008. 23, 24
[41] Alexander Schrijver. A short proof of Minc’s conjecture. Journal of combinatorial theory, Series A,
25(1):80–83, 1978. 24
[42] Daniel W. Stroock and Boguslaw Zegarli´nski. The logarithmic Sobolev inequality for continuous spin
systems on a lattice. J. Funct. Anal., 104(2):299–326, 1992. 14
[43] Daniel W. Stroock and Boguslaw Zegarli´nski. The logarithmic Sobolev inequality for discrete spin
systems on a lattice. Comm. Math. Phys., 149(1):175–193, 1992. 14
[44] Boguslaw Zegarlinski. Dobrushin uniqueness theorem and logarithmic Sobolev inequalities. J. Funct.
Anal., 105(1):77–111, 1992. 14

26
PIETRO CAPUTO
Department of Mathematics and Physics, Roma Tre University, Largo San Murialdo 1,
00146 Roma, Italy.
E-mail address: pietro.caputo@uniroma3.it

