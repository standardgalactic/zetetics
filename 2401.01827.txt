MoonShot: Towards Controllable Video Generation and Editing with
Multimodal Conditions
David Junhao Zhang♦♢, Dongxu Li♦, Hung Le♦, Mike Zheng Shou♢*, Caiming Xiong♦, Doyen Sahoo♦
♦Salesforce Research
♢Show Lab, National University of Singapore
https://showlab.github.io/Moonshot/
Zero-Shot Subject Customized Video Generation
A robot is landing to fly
A robot is walking
Image Animation
Given Image
Given Image
A can running
with heavy snow
Wearing a suit
Directly Use Image ControlNet
Video Editing
Figure 1. Our foundational video diffusion model, underpinned by multimodal conditioning, effectively facilitates image animation, the
creation of customized videos, and precise control over geometric structures through the image ControlNet. Additionally, it enables video
editing guided by text and image inputs, producing better results than recent state-of-the-art methods in these applications.
Abstract
Most existing video diffusion models (VDMs) are limited
to mere text conditions. Thereby, they are usually lacking
in control over visual appearance and geometry structure
of the generated videos.
This work presents MoonShot,
a new video generation model that conditions simultane-
ously on multimodal inputs of image and text. The model
builts upon a core module, called multimodal video block
(MVB), which consists of conventional spatialtemporal lay-
ers for representing video features, and a decoupled cross-
attention layer to address image and text inputs for ap-
pearance conditioning.
In addition, we carefully design
the model architecture such that it can optionally integrate
with pre-trained image ControlNet modules for geometry
visual conditions, without needing of extra training over-
head as opposed to prior methods. Experiments show that
with versatile multimodal conditioning mechanisms, Moon-
Shot demonstrates significant improvement on visual qual-
ity and temporal consistency compared to existing models.
In addition, the model can be easily repurposed for a vari-
ety of generative applications, such as personalized video
∗Corresponding Author
generation, image animation and video editing, unveiling
its potential to serve as a fundamental architecture for con-
trollable video generation. Models will be made public on
https://github.com/salesforce/LAVIS.
1. Introduction
Recently, text-to-video diffusion models (VDMs) [5, 12, 15,
21, 24, 50, 60, 78, 82, 83] have developed significantly,
allowing creation of high-quality visual appealing videos.
However, most existing VDMs are limited to mere text con-
ditional control, which is not always sufficient to precisely
describe visual content. Specifically, these methods are usu-
ally lacking in control over the visual appearance and ge-
ometry structure of the generated videos, rendering video
generation largely reliant on chance or randomness.
It is well acknowledged that text prompts are not suffi-
cient to describe precisely the appearance of generations,
as illustrated in Fig. 2. To address this issue, in the con-
text of text-to-image generation, efforts are made to achieve
personalized generation [9, 31, 34, 46, 75] by fine-tuning
diffusion models on input images. Similarly for video gen-
eration, AnimateDiff relies on customized model weights
to inject conditional visual content, either via LoRA [27] or
arXiv:2401.01827v1  [cs.CV]  3 Jan 2024

What does the
car look like?
Text prompt
A car running
in the desert
…
A car running
in the desert
Image prompt
Text prompt
…
(a) Video with text only 
(b) Video with text and image
Figure 2. A single text prompt (a) lacks the precision and detail for
accurate subject description. However, a picture (b) conveys much
more, worth a thousand words. By combining a picture with text,
we can produce videos that more closely match user requirements.
DreamBooth tuning [46]. Nonetheless, such an approach
incurs repetitive and tedious fine-tuning for each individ-
ual visual conditional inputs, hindering it from efficiently
scaling to wider applications. This inefficiency, as hinted
by the prior work [34], stems from the fact that most pre-
trained text-to-video models are not able to condition both
on images and text inputs. To overcome this issue, we in-
troduce a decoupled multimodal cross-attention module to
simultaneously condition the generation on both image and
text inputs, thus facilitating to better control the visual ap-
pearance, while minimizing required fine-tuning efforts and
unlocking zero-shot subject customized video generation.
In terms of geometric structure control, despite methods
such as ControlNet [79] and T2I-Adapter [38] are devel-
oped to leverage depth, edge maps as visual conditions for
image generation, analogous strategies for video synthesis
remain indeterminate. Among the few existing attempts,
VideoComposer [63] adds video-specific ControlNet mod-
ules to video diffusion models (VDMs) and subsequently
re-trains the added modules from scratch, incurring substan-
tial extra training overhead. In contrast, alternative meth-
ods [10, 81] reuse pre-trained ControlNet modules for im-
ages. However, they require adapting text-to-image mod-
els for video generation via frame propagation [16, 74] or
cross-frame attention [29, 68], resulting in subpar temporal
consistency compared to those based on VDMs.
Building upon the aforementioned observations, our ob-
jective is to explore a model architecture that combines the
strengths of both realms.
Namely, we expect the model
to be a high quality VDM that produces consistent video
frames, while also able to directly leverage pre-trained im-
age ControlNet to condition on geometry visual inputs. In
this regard, we observe that as shown in Fig. 3(b), prior
work [13, 60, 68, 83] typically insert temporal modules,
such as temporal convolutions layers, in between of spa-
tial modules, usually before self-attention and after spatial
convolution layers. This design modifies the spatial feature
visual quality
image ControlNet
spatial layers
resnet2d
self-attn
text-attn
image-attn
temporal layers
insert
temp-conv
temp-attn
frozen/train
(a) original
(b) composed
(c) decomposed
(d) MVB
Figure 3. Different designs of spatial-temporal modules include:
(a) the original spatial module from U-Net, (b) a temporal module
added within the spatial module, which hinders the image control-
net, and (c) a temporal module appended after the spatial module,
allowing for image control network functionality but failing to pro-
duce high-quality videos with text-only conditioning. In contrast,
our MVB block, conditioned on both image and text, enables the
image controlnet and can generate high-quality videos.
distribution, thereby making direct integration with image
ControlNet not feasible. In contrast, AnimateDiff [19] dis-
cards temporal convolution layers and only inserts temporal
attention layers after spatial layers, as shown in Fig. 3(c).
Such design preserves spatial feature distribution thus fa-
cilitating immediate reuse of image ControlNet. However,
it depends exclusively on text conditions, which do not of-
fer sufficient visual cues. Consequently, temporal modules
have to learn extra spatial information for compensation,
which diminishes the focus on temporal consistency, caus-
ing increased flickering and a decline in video quality.
To address these issues, we introduce MoonShot, a video
generation model that consumes both image and text con-
ditional inputs.
The foundation of the model is a new
backbone module for video generation, called multimodal
video block (MVB). Specifically, each MVB highlights
three main design considerations:
• a conventional spatial-temporal module for video genera-
tion, which in order consists of a spatial convolution layer,
a self-attention layer and a temporal attention layer that
aggregates spatial features. Such a design allows reuse of
pre-trained weights from text-to-image generation mod-
els without altering its spatial feature distribution, thus
subsuming its generation quality.
• a decoupled multimodal cross-attention layer that condi-
tions the generation on both text and image inputs. These
two conditions complement each other to guide the gen-
eration. In addition, image input offers reference visual
cues, allowing temporal modules to focus on video con-
sistency. This improves overall generation quality and

frame coherence, as evidenced experimentally;
• optionally, since spatial feature distribution is preserved,
pre-trained image ControlNet modules can be immedi-
ately integrated to control the geometric structure of the
generation, without needing of extra training overhead.
As a result, our model generates highly consistent videos
based on multimodal inputs, and can further utilize geom-
etry inputs, such as depth and edge maps, to control the
compositional layout of the generation. Moreover, thanks
to its generic architecture with versatile conditioning mech-
anisms, we show that MoonShot can be easily repurposed
for a variety of generative applications, such as image an-
imation and video editing. Qualitative and quantitative re-
sults show that MoonShot obtains superior performance on
personalized video generation, image animation and video
editing. When provided with a video frame as the image
condition, MoonShot demonstrates competitive or better re-
sults with state-of-the-art foundation VDMs, validating the
effectiveness of the model.
2. Related Work
Text to Video Generation. Previous research in this field
has leveraged a diversity of generative models, such as
GANs [47, 49, 54, 55, 59], Autoregressive models [14, 25,
33, 53, 72], and implicit neural representations [51, 77].
Driven by the diffusion model’s significant achievements in
image synthesis, a number of recent studies have explored
the application of diffusion models in both conditional and
unconditional video synthesis [1, 6, 20, 26, 28, 36, 40, 57,
57, 62, 67, 68, 73, 83]. Most existing approaches focus
on conditioning a single modality. For instance, Imagen
Video [24] and Make-A-Video are conditioned exclusively
on text, while I2VGen-XL [80] relies solely on images.
In contrast, our method supports multimodal conditions
with both image and text, enabling more precise control.
Video Editing and ControlNet. Beginning with Tune-A-
Video [68], numerous works [7, 8, 16, 18, 29, 44, 61, 74]
adapt the Text to Image (T2I) Model [45] for video edit-
ing. These methods introduce additional mechanisms like
cross-frame attention, frame propagation, or token flow to
maintain temporal consistency. While Dreammix [37] uses
the VDM for video editing, it necessitates source video fine-
tuning. In contrast, our method directly employs VDM for
video editing, achieving temporally consistent videos with-
out the need for fine-tuning or complex designs.
ControlNet [79] and T2I-Adapter [38] utilize structural
signals for image generation.
While ControlVideo [81]
applies this approach with a T2I model for video genera-
tion, resulting in limited temporal consistency. Control-A-
Video [10] integrates a temporal layer into both the base
VDM and ControlNet, training them jointly. Gen-1 [12]
concatenates structural signals with noise as the UNet input.
However, without these signals, the base VDMs in Control-
A-Video and Gen-1 will fail. Videocomposer [63] incurs
high training costs by adding a video controlnet after VDM
training. Our method stands out as our VDM independently
produces high-quality videos and directlu uses image Con-
trolNets without extra training.
Generation Model Customization.
Customizing large
pre-trained foundation models [11, 17, 32, 46, 52, 65]
enhances user-specific preferences while retaining robust
generative capabilities.
Dreambooth [46] leads the way
in this field, though it requires extensive fine-tuning. IP
adapter [76] and BLIP-Diffusion [34] achieve zero-shot
customization using an additional image cross-attention
layers and text-aligned image embeddings.
Animated-
iff [19] pioneers subject customization in videos but needs
additional Dreambooth training for the new subject.
Our method differs from IP-adapter in image domains by
introducing a decoupled image-text cross-attention layer in
the video block. We explore its effectiveness in facilitating
high-quality and smooth video creation and further apply it
to video tasks involving image animation and video editing.
Unlike Animatediff, our approach enables zero-shot subject
customization directly in videos.
Image Animation. This task aims to generate subsequent
frames given an initial image. Existing methods [2, 30, 30,
35, 35, 39, 41, 70] have focused on specific domains like hu-
man actions and nature scenes, while recent advancements
like I2VGen-XL [80], DynamicCrafter [69], and Video-
composer [63] target open-domain image animation with
video diffusion models. However, they often struggle to
maintain the appearance of the conditioning image.
In contrast, our approach, using masked condition and
decoupled image-text attention, effectively aligns the first
frame of the animated video with the provided image, en-
suring a more accurate retention of its original identity. Our
work is concurrent with DynamicCrafter [69] (Oct.,2023)
and we make comparisons in Fig. 7.
3. Model Architecture and Adaptations
Text-to-video latent diffusion models generate videos by
denoising a sequence of Gaussian noises with the guidance
of text prompts. The denoising network θ is usually a U-
Net-like model, optimized by a noise prediction loss:
L = Ez0,y,ϵ∼N(0,I ),t∼U(0,T)

∥ϵ −ϵθ(zt, t, y))∥2
2

,
(1)
where z0 is the latent code of training videos from VAE en-
coder, y is the text prompt, ϵ is the Gaussian noise added
to the latent code, t is the time step and ϵθ is the noise pre-
diction by the model. In the following, we first introduce
the MoonShot model architecture including its core com-
ponent, multimodal video block. Then, we showcase the
capabilities of the model by describing methods to repur-
pose the model for various video generation tasks, such as

Latent
Feature
W!
"
#"
#
#$
#
W"
W$
Image 
Feature
$#
%#
%
$
Text 
Feature
…
A corgi running on the grass
Image     
Encoder
Text
Encoder
ℰ
!%%
!%&
Video
Latents
…
!%'
!(%
!(&
…
Noise
UNet
Down Block
Up Block
Image Cross-Attn
Text  Cross-Attn
!('
…
…
Prediction
Figure 4. The overall workflow and the structure of our decoupled multimodal cross-attention layers. In the training phase, we use the
initial frame of the video as the image condition. For inference, the model accepts any image along with accompanying text.
0
z !"
1
0
0
0
z"
"
Image animation
noise
condition
condition  
mask
z !
#
z"
"
Figure 5. Masked condition for image animation.
geometry controlled video generation, image animation and
video editing.
3.1. Multimodal Video Block
Our model architecture builds upon multimodal video
blocks (MVB). There are three primary objectives under-
lying the design of this key module. First, we aim for the
model to consistently produce video frames of high qual-
ity. Second, it is desired for immediate integration of pre-
trained image ControlNet. In this way, we can facilitate the
use of geometric images for controlling the compositional
layout without extra training. Third, the model is expected
to accommodate multimodal text and image inputs for bet-
ter visual appearance conditioning. To this end, each MVB
consists of two groups of layers, spatialtemporal U-Net lay-
ers and decoupled multimodal cross-attention layers as de-
tailed in order below.
Spatialtemporal U-Net Layers.
Typical U-Net in text-
to-image models consists in order of a spatial convolu-
tion layer (ResNet2D), a self-attention layer, and a cross-
attention layer that conditions the generation on texts. Prior
work [5, 50, 60] adapts these models for video generation
by introducing additional temporal convolution and atten-
tion layers. As shown in Fig. 3(b), the temporal convolution
layer is usually inserted before each self-attention layer and
after the spatial convolution layer. While this architecture
may enhance temporal consistency, it alters the distribution
of the spatial feature baked in the pre-trained text-to-image
generation models. As a result, the model not only loses
the ability of text-to-image generation, but also becomes in-
compatible with established techniques developed for text-
to-image models, such as ControlNet, making direct inte-
gration of these techniques infeasible.
Differently, we observe that the addition of temporal at-
tention layers after the cross-attention layer does not signif-
icantly modify the spatial feature distribution, while con-
tributing effectively for temporal feature aggregation. By
freezing the spatial layers during training, we can reuse
ControlNet to condition the generation on geometry visual
inputs by broadcasting it along the temporal axis, as shown
in Fig. 3(c). In particular, we use space-time attention sim-
ilar to [4], where each patch attends to those at the same
spatial location and across frames.
Decoupled Multimodal Cross-attention Layers. Most ex-
isting video generation models use a cross-attention module
to condition the generation on texts. Given fy the embed-
ding of text prompts, diffusion models condition on it to
enhance the U-Net features fx via cross-attention layers,
where the query Q is obtained from U-Net features fx, key
K and value V are from the text embedding fy. The cross-
attention operation is then defined as:
(
Q = WQ · fx; K = WK · fy; V = WV · fy;
CrossAttention(Q, K, V) = softmax( QKT
√
d ) · V,
(2)
where Q ∈RBN×H×W ×C, K, V ∈RBN×L×C, with B
the batch size, N the number of frames, H the height, W
the width and C the number of channels, L the number of
text tokens, d the hidden size. Note that text embeddings
are duplicated for video frames.
There are two issues with this design.
First, relying
solely on text prompts usually proves inadequate in accu-
rately describing highly customized visual concepts for the
desired generation. Second, specifically for video genera-
tion, the absence of visual conditioning mechanism places
excessive burden on the temporal attention layers. They

must simultaneously ensure the consistency across frames
and also preserve high-quality spatial features, often re-
sulting in compromises on both fronts with flickered low-
quality videos.
To address these issues, we introduce decoupled multi-
modal cross-attention, where one extra key and value trans-
formation are optimized for image conditions, denoted as
KI, V I ∈RBN×L×C. The attention is formulated as:
CrossAttention(Q, K, V) + CrossAttention(Q, KI, VI).
(3)
This approach enables the model to effectively manage both
the image and text conditions. Additionally, conditioning
on visual cues allows the subsequent temporal modules to
focus more on maintaining temporal consistency, resulting
in smoother and higher-quality video outputs. With the ad-
ditional image condition, the training loss is thereby refor-
mulated as:
L = Ez0,y,y′,ϵ∼N(0,I ),t∼U(0,T)

∥ϵ −ϵθ(zt, t, y, y′))∥2
2

,
(4)
where y′ is the image condition.
3.2. Adapting for Video Generation Applications
Masked Condition for Image Animation. With the addi-
tional conditioning on image input, our model is a natural
fit for the task of image animation, which aims to trans-
form an input image into a short video clip with consistent
content. To enhance the content consistency, we adapt the
mask-conditioning mechanism introduced by [5] for image
animation. In particular, we use the first frame as an addi-
tional input condition for the U-Net. Apart from the orig-
inal four latent channels, as illustrated in Fig. 5, we add
five more channels to the U-Net’s input. Among them, four
channels represent the replicated first frame latent z0
0, and
one binary channel is used to denote the masked frames.
This approach encourages that the identity of the subject in
the animated video remains identical to that in the condi-
tioning image. We observe that incorporating an extra im-
age cross attention layer is essential for image animation. It
helps significantly to prevent sudden changes in appearance
and reduce temporal flickering, which are seen commonly
in models driven merely by text conditions.
Video Editing with Video Diffusion Models.
Dream-
Mix [37] shows that VDM can be repurposed for video
editing, which however requires extensive fine-tuning. Our
model is for general-purpose video generation, yet can
be used for video editing without needing of fine-tuning.
Specifically, for a selected encoded source video z0, we
add Gaussian noise using the DDPM [23] forward process.
Next, we employ diffusion directly with VDM, conditioned
on both text and image. This process effectively replaces
the subject in the original video with that from the image
Wearing a glass
Image
ControlNet
Text-Image to Video
Given Image
A bear astronaut
Depth Map
Wear a hat on the beach
Figure 6. Subject customized video generation results with the
option of utilizing an image controlnet network or not.
condition and incorporates visually appealing elements as
described in the text, resulting in a smooth edited video.
Geometry Controlled Generation. Since our model pre-
serves the spatial features of the pre-trained text-to-image
models, we can directly integrate the image ControlNet for
geometry conditioning.
To achieve this, we attach pre-
trained ControlNet modules to the model. Then condition
features for each frame is added to the corresponding fea-
ture maps via residues. Due to the careful design of spatial-
temporal U-Net layers, we observe satisfactory geometry
control effect without needing of video-specific fine-tuning.
4. Experiments
4.1. Implementation Details
Our spatial weights are initialized using SDXL [42], which
are fixed throughout the training process. Initially, follow-
ing IP-Adpter [76], we train the image cross-attention layers
using the LAION [48] dataset at a resolution of 512 × 320.
Subsequently, we keep the spatial weights unchanged and
proceed to train only the temporal attention layers. This
training step utilizes the WebVid10M [3] dataset, each clip
sampled 16 frames at a 512×320 resolution, with condition-
ing on video captions as text conditions and the first frame
as image conditions. Further refinement is carried out on
a set of 1000 videos from the InternVideo [64] collection,
aimed at removing watermarks. We use 16 A100 40G GPUs
for training. More details can be found in supplementary
material.
4.2. Human Evaluations
We follow Make-a-Video [50] to perform human evalu-
ations on Amazon Mechanical Turk.
In the realm of
video editing tasks (Tab. 3), in line with FateZero [44] and

Given Image
First Frame
Ours
DynamiCrafter
(VideoCrafter)
Videocomposer Touch the helmet
DynamiCrafter
(VideoCrafter)
Ours
First Frame
Big waves
Videocomposer
Robot is walking
I2VGen-XL
I2VGen-XL
Given Image
Figure 7. Image animation. Comparing our method with I2VGEN-XL [80], DynamiCrafter [69], and VideoComposer [63], it stands out
in its ability to control the animated video’s first frame to match the provided image exactly, maintaining the identity of the given image
more precisely and animating the image according to text prompts. In contrast, I2VGen-XL and DynamicCrafter show relatively weaker
identity preservation in their animated videos, and their response to text prompts is less effective.
Model
DINO
CLIP-I
CLIP-T
Non-Customized T2V
0.283
0.594
0.296
I2VGen-XL [80]
0.542
0.737
0.218
AnimateDiff [19]
0.582
0.784
0.243
300 finetune steps
Ours (zero-shot)
0.556
0.763
0.292
Ours (80 finetune steps)
0.624
0.802
0.292
Table 1.
Subject customized video generation performance on
Dreambooth dataset [46]. Higher metrics are better.
Render-A-Video [74], we direct annotators to identify the
most superior outcomes from five methods, judging by three
standards: 1) the precision of prompt-to-edited-frame align-
ment, 2) temporal coherence of the video, and 3) the overall
quality. In addition, for ControlNet evaluations (see Tab. 5),
we request annotators to judge whether the created video
adheres to the control signal. Furthermore, regarding the
text-to-video ablation studies (Tab. 6), we invite annotators
to evaluate the overall video quality, the accuracy of text-
video alignment and motion fidelity.
4.3. Subject Customized Generation
Quantitative Results. To evaluate our method for subject-
customized video generation, we perform experiments on
the DreamBooth [46] dataset, which includes 30 subjects,
each with 4-7 text prompts. We utilize DINO and CLIP-I
scores to assess subject alignment, and CLIP-T for video-
text alignment, calculating average scores for all frames. As
shown in Tab. 1, our method achieves strong zero-shot cus-
tomization, surpassing non-customized text-to-video (T2V)
models by a large margin.
Different from AnimateDiff,
which requires repetitive re-training for new subjects, our
method utilizes pre-trained decoupled multimodal attention
layers, achieving zero-shot customization with compared
DINO (First)
DINO (Avg)
CLIP-T (Avg)
GT
0.781
0.644
–
I2VGen-XL [80]
0.624
0.573
0.232
VideoComposer [63]
0.751
0.285
0.269
Ours
0.765
0.614
0.284
Table 2. Image animation results. Our model shows better visual
and textual alignment than competing methods.
performance. If fine-tuned with as few as 80 steps, our ap-
proach further surpasses AnimateDiff by a significant mar-
gin, demonstrating the effectiveness of our model.
Qualitative Results. As shown in Fig. 6, our model pro-
duces customized videos that align with both the subject of
image condition and the text condition. Additionally, the
image ControlNet can be directly integrated to realize con-
trol over geometric structures.
4.4. Image Animation
Quantitative Results. To assess image animation capabil-
ities, we select 128 video-text pairs from the Webvid eval-
uation set, covering diverse themes. We use DINO (First)
to measure the similarity between the first frame of the an-
imated video and the conditioning image, DINO (Average)
for the average similarity across all video frames compared
to the conditioning image, and CLIP-T for the overall align-
ment between text prompts and the animated video across
frames. As shown in Tab. 2, our method outperforms others
in all metrics, demonstrating superior identity preservation,
temporal consistence and text alignment.
Qualitative Results. We compare our results qualitatively
with I2VGEN-XL [80], DynamiCrafter [69], and Video-
Composer [63] as shown in Fig. 7, it’s seen that the iden-
tity or appearance in animated videos from I2VGEN-XL
and DynamiCrafter is different from the original image.

Render a Video
FateZero + Tune a Video
Ours
A car running on the road, Van Gogh style.    +
A  blue Lamborghini running on the road, Van Gogh style.
A  blue Lamborghini running on the road, Van Gogh style.
Figure 8. Visual comparisons with SOTA video editing methods.
Metric
FateZ [44]
Pix2V [7]
T2V-Z [29]
Render-A-V [74]
Ours
Fram-Acc (↑)
0.534
0.978
0.943
0.959%
0.976
Tem-Con (↑)
0.953
0.942
0.963
0.965
0.986
Pixel-MSE (↓)
0.092
0.256
0.091
0.073
0.064
User-Balance
4.4%
6.2%
7.4%
21.4%
60.6%
User-Temporal
3.6%
2.0%
3.8%
18.2%
72.4%
User-Overall
3.1%
3.1%
7.0%
24.6%
62.2%
Table 3. Quantitative comparisons and user preference rates for
video editing task.
While VideoComposer replicates the conditioning image as
the first frame, subsequent frames show abrupt changes in
appearance, as indicated by its high DINO (First) and low
DINO (Average) scores in Tab. 2. This issue may stem from
its limited capacity to extract visual cues. Our method, in
contrast, utilizes multimodal cross-attention layers and con-
dition masks, and excels by promoting the similarity be-
tween the first frame of the animated video and the con-
ditioning image, maintaining more effectively appearance,
and enabling animation in line with text prompts.
4.5. Video Editing
Quantitative Results. We compare with four video edit-
ing methods: FateZero [44], Pix2Video [7], Text2Video-
Zero [29], and Render-A-Video [74].
Notably, Render-
A-Video and Text2Video-Zero employ customized models
that incorporate ControlNet. In contrast, our approach uti-
lizes the base VDM model without integrating ControlNet.
Following the FateZero and Pix2Video, we utilize 72 videos
from Davis [43] and various in-the-wild sources. We report
three metrics in Tab. 3: Fram-Acc, a CLIP-based measure
of frame-wise editing accuracy; Tmp-Con, assessing the co-
sine similarity between consecutive frames using CLIP; and
Models
FID-vid (↓)
FVD (↓)
CLIP-T (↑)
N ¨UWA [67]
47.68
-
0.2439
CogVideo (Chinese) [25]
24.78
-
0.2614
CogVideo (English) [25]
23.59
1294
0.2631
MagicVideo [83]
-
1290
-
Video LDM [5]
-
-
0.2929
Make-A-Video [50]
13.17
-
0.3049
ModelScopeT2V [60]
11.09
550
0.2930
Ours
10.98
542
0.3068
Table 4. Quantitative comparisons on MSR-VTT [71].
Pixel-MSE, the averaged mean-squared pixel error between
aligned consecutive frames. Our method excels in tempo-
ral consistency and ranks second in frame editing accuracy.
And our method achieves better human preference on all
evaluation metrics (Sec. 4.2). In fact, prior methods typi-
cally utilize image models with frame propagation or cross-
frame attention mechanisms, which tend to yield worse tem-
poral consistency compared to our approach. This demon-
strates the clear advantage of using foundation VDMs for
video editing, compared to those relying on image models.
Qualitative Results.
As shown in Fig. 8, FateZero re-
constructs the input sports car frame well but the result is
not aligned with the prompt. Render-A-Video struggles to
swap the jeep to a sports car, facing challenges with shape
changes. Conversely, our method adeptly replaces the jeep
with a sports car as specified in the conditioning image
while also adhering to the text prompts.
4.6. Text to Video Generation
Since the spatial layers are frozen during training, we first
generate an image according to the text prompt. The image
is later combined with text for multimodal conditioned gen-
eration. We use MSR-VTT dataset [71] to evaluate qual-
ity of zero-shot generation, whose test set contains 2,990
videos of 320x240 resolution, accompanied by 59,794 cap-
tions in total. We compare our model with state-of-the-art
methods as shown in Tab. 4. Our method achieves the best
results across FID-vid [22], FVD [56], and CLIP-T [66],
demonstrating better visual quality and text alignment.
4.7. Ablation Studies
Spatial temporal modules designs. We investigate the de-
sign of temporal modules that allows direct integration of
the image ControlNet. As indicated in Tab 5, the design
which inserts the temporal convolution within spatial mod-
ules (Fig.3b) alters the original spatial features, rendering
the image ControlNet ineffective, even with fixed spatial
(fs) weights during video training. Conversely, placing the
temporal attention after all spatial modules in each block
and fixing spatial weight during video training renders the
image ControlNet feasible.
Impact of image condition on video consistency and
quality. In Tab. 6, we explore the impact of mutlimodal

only text
cross-attn
decoupled
multimodal
cross-attn
An astronaut walking on the moon
appearance flickering and suddenly change 
appearance and smooth motion
two consecutive frames
two consecutive frames
Figure 9. Comparison between text-only and multimodal conditioned VDMs. We compare MoonShot with AnimateDiff-XL [19], which
uses only text conditions. Despite trained on high-quality internal data, AnimateDiff-XL suffers appearance flickering and abrupt changes.
In contrast, our multimodal-conditioned model trained on a public dataset shows improved temporal consistency and visual quality.
only text
cross-attn
A rabbit is turning around
decoupled
multimodal
cross-attn
Figure 10. Impact of multimodal conditions for image animation.
condition on video generation. We freeze the spatial layers
and only train the temporal ones. Animatediff-XL also em-
ploys fixed SDXL spatial weights conditioned only by text
and is included in this ablation. However, since it’s trained
on internal high-quality data, we’ve also trained a model
without image conditions on the same data used for our final
model, to ensure a fair ablation. Text-Video (T-V) Align-
ment and Motion Fidelity are agreement scores from human
evaluation (Sec 4.2). We find that using only text condition
leads to weaker temporal consistency, motion fidelity, and
visual quality compared to multimodal conditions. Fig. 9
also shows that with only text condition, the resulting video
experiences significant temporal flickering and sudden ap-
pearance changes. These outcomes validate that when train-
ing temporal modules only, the additional image cross at-
tention provides effective visual signals, thereby allowing
the temporal module to focus on video consistency, leading
to reduced flickering and improved video quality.
Impact of image condition and masked condition on im-
age animation. Tab. 7 shows that masked condition helps
composed
composed-fs
decomposed-fs
MVB
rate
0%
13%
97%
97%
Table 5. Image controlent successful rates with different spatial
temporal designs in VDMs.
FVD(↓)
T-V alignment
Motion-Fidelty
Quality
Text Only
Ours w/o image condition
602
8%
4%
0%
Animatediff-XL
589
40%
12%
30%
MVB
Ours
542
52%
84%
70%
Table 6. Impact of multi-modal condition for the VDM.
DINO (First)
DINO (Avg)
CLIP-T (Avg)
text only
0.264
0.262
0.285
+ masked condition
0.760
0.296
0.210
+ image condtion
0.638
0.562
0.282
+ both
0.765
0.614
0.284
Table 7. Impact of conditions masks and decoupled cross attention
for image animation. Higher metrics are better.
to produces the first animation frame that matches the con-
ditioning image, yet cannot guarantee the temporal consis-
tency, as suggested by a high DINO (First) score and low
DINO (Avg) score. Adding the image condition improves
temporal consistence and subject identify, as evidenced by
the high DINO (Avg) score and confirmed in Fig. 10. With
both masked condition and image condition, our model pro-
duces the first frame that highly preserves the conditioning
image, as well as visually consistent animations.
5. Conclusion
We present MoonShot, a new video generation model that
conditions on both image and text inputs using the Mul-
timodal Video Block (MVB). Our model stands out in
producing high-quality videos with controllable visual ap-

pearance. In the meantime, our model is able to harvest
the pre-trained image ControlNet to control over geome-
try without extra training overhead.
The model offers a
generic architecture and versatile conditioning mechanisms,
which allows it to be easily adapted for various video gen-
eration tasks, such as image animation, video editing and
subject-customized video generation, with superior genera-
tion quality than previous methods, revealing its great po-
tential to serve as a foundation model for video generation
research and application.
6. Ethic
Our model operates with both image and text inputs, and
similarly to the Blip-Diffusion [34], we aim to ensure a
strong relevance of the generation samples to the supplied
image. This image can originate from two sources: either
directly uploaded by human users or generated by a text-
to-image model like DALL-E 3 or Stable Diffusion XL.
When the image is produced by a text-to-image model, the
bias, fairness, and toxicity levels of our model are inherent
and comparable to those of the text-to-image model. Con-
versely, when the image is provided by the user, these as-
pects are mostly governed by the user, not our model. To
mitigate harmful content that may result from inappropri-
ate image/text inputs, we plan to implement an NSFW (Not
Safe For Work) detector, which is open-sourced in diffusers
library [58] and can be directly integrated into our model.
This detector will strengthen the model governance layer to
control the input contents, either from human users or from
the text-to-image model, thereby reducing harmful compo-
nents. In addition to the above measures, we are conducting
red-teaming experiments to understand and minimize harm-
ful generation contents as a result of red-teaming attacks.
Note that by default, our model is designed to generate non-
toxic elements as long as the provided inputs do not elicit
harmful or biased content. Regardless, we are committed to
eliminating any toxicity or biases in our model and we will
not open source or make our model publicly available until
the safety measures are properly in place.
References
[1] Jie An, Songyang Zhang, Harry Yang, Sonal Gupta, Jia-Bin
Huang, Jiebo Luo, and Xi Yin. Latent-shift: Latent diffu-
sion with temporal shift for efficient text-to-video genera-
tion. arXiv preprint arXiv:2304.08477, 2023. 3
[2] Mohammad Babaeizadeh, Chelsea Finn, Dumitru Erhan,
Roy H Campbell, and Sergey Levine. Stochastic variational
video prediction. arXiv preprint arXiv:1710.11252, 2017. 3
[3] Max Bain, Arsha Nagrani, G¨ul Varol, and Andrew Zisser-
man. Frozen in time: A joint video and image encoder for
end-to-end retrieval. In Proceedings of the IEEE/CVF Inter-
national Conference on Computer Vision, pages 1728–1738,
2021. 5
[4] Gedas Bertasius, Heng Wang, and Lorenzo Torresani.
Is
space-time attention all you need for video understanding?
In Proceedings of the International Conference on Machine
Learning (ICML), 2021. 4
[5] Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dock-
horn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis.
Align your latents: High-resolution video synthesis with la-
tent diffusion models. In Proceedings of the IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition, pages
22563–22575, 2023. 1, 4, 5, 7
[6] Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dock-
horn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis.
Align your latents: High-resolution video synthesis with la-
tent diffusion models. In CVPR, 2023. 3
[7] Duygu Ceylan,
Chun-Hao Huang,
and Niloy J. Mi-
tra.
Pix2video:
Video editing using image diffusion.
arXiv:2303.12688, 2023. 3, 7
[8] Wenhao Chai, Xun Guo, Gaoang Wang, and Yan Lu. Stable-
video: Text-driven consistency-aware diffusion video edit-
ing. arXiv preprint arXiv:2308.09592, 2023. 3
[9] Li Chen,
Mengyi Zhao,
Yiheng Liu,
Mingxu Ding,
Yangyang Song, Shizun Wang, Xu Wang, Hao Yang, Jing
Liu, Kang Du, et al.
Photoverse:
Tuning-free image
customization with text-to-image diffusion models.
arXiv
preprint arXiv:2309.05793, 2023. 1
[10] Weifeng Chen, Jie Wu, Pan Xie, Hefeng Wu, Jiashi Li,
Xin Xia, Xuefeng Xiao, and Liang Lin.
Control-a-video:
Controllable text-to-video generation with diffusion models,
2023. 2, 3
[11] Xi Chen, Lianghua Huang, Yu Liu, Yujun Shen, Deli Zhao,
and Hengshuang Zhao. Anydoor: Zero-shot object-level im-
age customization. arXiv preprint arXiv:2307.09481, 2023.
3
[12] Patrick Esser,
Johnathan Chiu,
Parmida Atighehchian,
Jonathan Granskog, and Anastasis Germanidis.
Structure
and content-guided video synthesis with diffusion models.
arXiv preprint arXiv:2302.03011, 2023. 1, 3
[13] Oran Gafni, Adam Polyak, Oron Ashual, Shelly Sheynin,
Devi Parikh, and Yaniv Taigman.
Make-a-scene: Scene-
based text-to-image generation with human priors. In Com-
puter Vision–ECCV 2022: 17th European Conference, Tel
Aviv, Israel, October 23–27, 2022, Proceedings, Part XV,
pages 89–106. Springer, 2022. 2
[14] Songwei Ge, Thomas Hayes, Harry Yang, Xi Yin, Guan
Pang, David Jacobs, Jia-Bin Huang, and Devi Parikh.
Long video generation with time-agnostic vqgan and time-
sensitive transformer.
arXiv preprint arXiv:2204.03638,
2022. 3
[15] Songwei Ge, Seungjun Nah, Guilin Liu, Tyler Poon, An-
drew Tao, Bryan Catanzaro, David Jacobs, Jia-Bin Huang,
Ming-Yu Liu, and Yogesh Balaji. Preserve your own cor-
relation: A noise prior for video diffusion models. arXiv
preprint arXiv:2305.10474, 2023. 1
[16] Michal Geyer, Omer Bar-Tal, Shai Bagon, and Tali Dekel.
Tokenflow: Consistent diffusion features for consistent video
editing. arXiv preprint arxiv:2307.10373, 2023. 2, 3
[17] Yuchao Gu, Xintao Wang, Jay Zhangjie Wu, Yujun Shi, Yun-
peng Chen, Zihan Fan, Wuyou Xiao, Rui Zhao, Shuning

Chang, Weijia Wu, et al. Mix-of-show: Decentralized low-
rank adaptation for multi-concept customization of diffusion
models. arXiv preprint arXiv:2305.18292, 2023. 3
[18] Yuchao Gu, Yipin Zhou, Bichen Wu, Licheng Yu, Jia-Wei
Liu, Rui Zhao, Jay Zhangjie Wu, David Junhao Zhang,
Mike Zheng Shou, and Kevin Tang. Videoswap: Customized
video subject swapping with interactive semantic point cor-
respondence. arXiv preprint arXiv:2312.02087, 2023. 3
[19] Yuwei Guo, Ceyuan Yang, Anyi Rao, Yaohui Wang, Yu
Qiao, Dahua Lin, and Bo Dai. Animatediff: Animate your
personalized text-to-image diffusion models without specific
tuning. arXiv preprint arXiv:2307.04725, 2023. 2, 3, 6, 8
[20] William Harvey, Saeid Naderiparizi, Vaden Masrani, Chris-
tian Weilbach, and Frank Wood. Flexible diffusion modeling
of long videos. arXiv preprint arXiv:2205.11495, 2022. 3
[21] Yingqing He, Tianyu Yang, Yong Zhang, Ying Shan, and
Qifeng Chen. Latent video diffusion models for high-fidelity
video generation with arbitrary lengths.
arXiv preprint
arXiv:2211.13221, 2022. 1
[22] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner,
Bernhard Nessler, and Sepp Hochreiter. Gans trained by a
two time-scale update rule converge to a local nash equilib-
rium. Advances in neural information processing systems,
30, 2017. 7
[23] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising dif-
fusion probabilistic models. NeurIPS, 33:6840–6851, 2020.
5
[24] Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang,
Ruiqi Gao, Alexey Gritsenko, Diederik P Kingma, Ben
Poole, Mohammad Norouzi, David J Fleet, et al. Imagen
video: High definition video generation with diffusion mod-
els. arXiv preprint arXiv:2210.02303, 2022. 1, 3
[25] Wenyi Hong, Ming Ding, Wendi Zheng, Xinghan Liu,
and Jie Tang.
Cogvideo:
Large-scale pretraining for
text-to-video generation via transformers.
arXiv preprint
arXiv:2205.15868, 2022. 3, 7
[26] Tobias H¨oppe, Arash Mehrjou, Stefan Bauer, Didrik Nielsen,
and Andrea Dittadi. Diffusion models for video prediction
and infilling. arXiv preprint arXiv:2206.07696, 2022. 3
[27] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-
Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen.
Lora: Low-rank adaptation of large language models. arXiv
preprint arXiv:2106.09685, 2021. 1
[28] Levon Khachatryan, Andranik Movsisyan, Vahram Tade-
vosyan,
Roberto
Henschel,
Zhangyang
Wang,
Shant
Navasardyan, and Humphrey Shi. Text2video-zero: Text-to-
image diffusion models are zero-shot video generators. arXiv
preprint arXiv:2303.13439, 2023. 3
[29] Levon Khachatryan, Andranik Movsisyan, Vahram Tade-
vosyan,
Roberto
Henschel,
Zhangyang
Wang,
Shant
Navasardyan, and Humphrey Shi. Text2video-zero: Text-to-
image diffusion models are zero-shot video generators. arXiv
preprint arXiv:2303.13439, 2023. 2, 3, 7
[30] Yunji Kim, Seonghyeon Nam, In Cho, and Seon Joo
Kim.
Unsupervised keypoint learning for guiding class-
conditional video prediction. Advances in neural informa-
tion processing systems, 32, 2019. 3
[31] Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli
Shechtman, and Jun-Yan Zhu. Multi-concept customization
of text-to-image diffusion. In CVPR, 2023. 1
[32] Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli
Shechtman, and Jun-Yan Zhu. Multi-concept customization
of text-to-image diffusion. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition,
pages 1931–1941, 2023. 3
[33] Guillaume Le Moing, Jean Ponce, and Cordelia Schmid.
Ccvs: Context-aware controllable video synthesis. NeurIPS,
2021. 3
[34] Dongxu Li, Junnan Li, and Steven CH Hoi.
Blip-
diffusion:
Pre-trained subject representation for control-
lable text-to-image generation and editing. arXiv preprint
arXiv:2305.14720, 2023. 1, 2, 3, 9
[35] Yijun Li, Chen Fang, Jimei Yang, Zhaowen Wang, Xin
Lu, and Ming-Hsuan Yang. Flow-grounded spatial-temporal
video prediction from still images. In Proceedings of the Eu-
ropean Conference on Computer Vision (ECCV), pages 600–
615, 2018. 3
[36] Zhengxiong Luo, Dayou Chen, Yingya Zhang, Yan Huang,
Liang Wang, Yujun Shen, Deli Zhao, Jingren Zhou, and Tie-
niu Tan.
Videofusion: Decomposed diffusion models for
high-quality video generation. In CVPR, 2023. 3
[37] Eyal Molad, Eliahu Horwitz, Dani Valevski, Alex Rav
Acha, Yossi Matias, Yael Pritch, Yaniv Leviathan, and Yedid
Hoshen. Dreamix: Video diffusion models are general video
editors. arXiv preprint arXiv:2302.01329, 2023. 3, 5
[38] Chong Mou, Xintao Wang, Liangbin Xie, Jian Zhang, Zhon-
gang Qi, Ying Shan, and Xiaohu Qie. T2i-adapter: Learning
adapters to dig out more controllable ability for text-to-image
diffusion models. arXiv preprint arXiv:2302.08453, 2023. 2,
3
[39] Haomiao Ni, Changhao Shi, Kai Li, Sharon X Huang, and
Martin Renqiang Min. Conditional image-to-video gener-
ation with latent flow diffusion models. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition, pages 18444–18455, 2023. 3
[40] Yaniv Nikankin, Niv Haim, and Michal Irani.
Sinfusion:
Training diffusion models on a single image or video. arXiv
preprint arXiv:2211.11743, 2022. 3
[41] Junting Pan, Chengyu Wang, Xu Jia, Jing Shao, Lu Sheng,
Junjie Yan, and Xiaogang Wang. Video generation from sin-
gle semantic label map. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition,
pages 3733–3742, 2019. 3
[42] Dustin
Podell,
Zion
English,
Kyle
Lacey,
Andreas
Blattmann, Tim Dockhorn, Jonas M¨uller, Joe Penna, and
Robin Rombach.
Sdxl: Improving latent diffusion mod-
els for high-resolution image synthesis.
arXiv preprint
arXiv:2307.01952, 2023. 5
[43] Jordi Pont-Tuset, Federico Perazzi, Sergi Caelles, Pablo Ar-
bel´aez, Alex Sorkine-Hornung, and Luc Van Gool. The 2017
davis challenge on video object segmentation. arXiv preprint
arXiv:1704.00675, 2017. 7
[44] Chenyang Qi, Xiaodong Cun, Yong Zhang, Chenyang Lei,
Xintao Wang, Ying Shan, and Qifeng Chen. Fatezero: Fus-

ing attentions for zero-shot text-based video editing. arXiv
preprint arXiv:2303.09535, 2023. 3, 5, 7
[45] Robin Rombach, Andreas Blattmann, Dominik Lorenz,
Patrick Esser, and Bj¨orn Ommer. High-resolution image syn-
thesis with latent diffusion models. In CVPR, pages 10684–
10695, 2022. 3
[46] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch,
Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine
tuning text-to-image diffusion models for subject-driven
generation. 2022. 1, 2, 3, 6
[47] Masaki Saito, Eiichi Matsumoto, and Shunta Saito. Tempo-
ral generative adversarial nets with singular value clipping.
In ICCV, 2017. 3
[48] Christoph Schuhmann, Richard Vencu, Romain Beaumont,
Theo Coombes, Cade Gordon, Aarush Katta, Robert Kacz-
marczyk, and Jenia Jitsev.
LAION-5B: laion-5b: A new
era of open large-scale multi-modal datasets.
https:
//laion.ai/laion-5b-a-new-era-of-open-
large-scale-multi-modal-datasets/, 2022. 5
[49] Xiaoqian Shen,
Xiang Li,
and Mohamed Elhoseiny.
Mostgan-v: Video generation with temporal motion styles.
In CVPR, 2023. 3
[50] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An,
Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual,
Oran Gafni, et al. Make-a-video: Text-to-video generation
without text-video data. arXiv preprint arXiv:2209.14792,
2022. 1, 4, 5, 7
[51] Ivan Skorokhodov, Sergey Tulyakov, and Mohamed Elho-
seiny. Stylegan-v: A continuous video generator with the
price, image quality and perks of stylegan2. arXiv preprint
arXiv:2112.14683, 2021. 3
[52] James Seale Smith, Yen-Chang Hsu, Lingyu Zhang, Ting
Hua, Zsolt Kira, Yilin Shen, and Hongxia Jin. Continual dif-
fusion: Continual customization of text-to-image diffusion
with c-lora. arXiv preprint arXiv:2304.06027, 2023. 3
[53] Nitish Srivastava, Elman Mansimov, and Ruslan Salakhudi-
nov. Unsupervised learning of video representations using
lstms. In ICML, 2015. 3
[54] Yu Tian, Jian Ren, Menglei Chai, Kyle Olszewski, Xi Peng,
Dimitris N. Metaxas, and Sergey Tulyakov. A good image
generator is what you need for high-resolution video synthe-
sis. In ICLR, 2021. 3
[55] Sergey Tulyakov, Ming-Yu Liu, Xiaodong Yang, and Jan
Kautz.
Mocogan: Decomposing motion and content for
video generation. In CVPR, 2018. 3
[56] Thomas Unterthiner, Sjoerd Van Steenkiste, Karol Kurach,
Raphael Marinier, Marcin Michalski, and Sylvain Gelly. To-
wards accurate generative models of video: A new metric &
challenges. arXiv preprint arXiv:1812.01717, 2018. 7
[57] Vikram Voleti, Alexia Jolicoeur-Martineau, and Christopher
Pal. Masked conditional video diffusion for prediction, gen-
eration, and interpolation. arXiv preprint arXiv:2205.09853,
2022. 3
[58] Patrick von Platen, Suraj Patil, Anton Lozhkov, Pedro
Cuenca, Nathan Lambert, Kashif Rasul, Mishig Davaadorj,
and Thomas Wolf.
Diffusers:
State-of-the-art diffusion
models.
https://github.com/huggingface/
diffusers, 2022. 9
[59] Carl Vondrick, Hamed Pirsiavash, and Antonio Torralba.
Generating videos with scene dynamics. NIPS, 2016. 3
[60] Jiuniu Wang, Hangjie Yuan, Dayou Chen, Yingya Zhang,
Xiang Wang, and Shiwei Zhang. Modelscope text-to-video
technical report. arXiv preprint arXiv:2308.06571, 2023. 1,
2, 4, 7
[61] Wen Wang, kangyang Xie, Zide Liu, Hao Chen, Yue Cao,
Xinlong Wang, and Chunhua Shen. Zero-shot video editing
using off-the-shelf image diffusion models. arXiv preprint
arXiv:2303.17599, 2023. 3
[62] Wenjing Wang, Huan Yang, Zixi Tuo, Huiguo He, Junchen
Zhu, Jianlong Fu, and Jiaying Liu. Videofactory: Swap at-
tention in spatiotemporal diffusions for text-to-video gener-
ation. arXiv preprint arXiv:2305.10874, 2023. 3
[63] Xiang Wang, Hangjie Yuan, Shiwei Zhang, Dayou Chen,
Jiuniu Wang, Yingya Zhang, Yujun Shen, Deli Zhao,
and Jingren Zhou.
Videocomposer: Compositional video
synthesis with motion controllability.
arXiv preprint
arXiv:2306.02018, 2023. 2, 3, 6
[64] Yi Wang, Kunchang Li, Yizhuo Li, Yinan He, Bingkun
Huang, Zhiyu Zhao, Hongjie Zhang, Jilan Xu, Yi Liu, Zun
Wang, Sen Xing, Guo Chen, Junting Pan, Jiashuo Yu, Yali
Wang, Limin Wang, and Yu Qiao.
Internvideo: General
video foundation models via generative and discriminative
learning. arXiv preprint arXiv:2212.03191, 2022. 5
[65] Yuxiang Wei, Yabo Zhang, Zhilong Ji, Jinfeng Bai, Lei
Zhang, and Wangmeng Zuo. Elite: Encoding visual con-
cepts into textual embeddings for customized text-to-image
generation. arXiv preprint arXiv:2302.13848, 2023. 3
[66] Chenfei Wu, Lun Huang, Qianxi Zhang, Binyang Li, Lei Ji,
Fan Yang, Guillermo Sapiro, and Nan Duan. Godiva: Gen-
erating open-domain videos from natural descriptions. arXiv
preprint arXiv:2104.14806, 2021. 7
[67] Chenfei Wu, Jian Liang, Lei Ji, Fan Yang, Yuejian Fang,
Daxin Jiang, and Nan Duan. N¨uwa: Visual synthesis pre-
training for neural visual world creation. In ECCV, pages
720–736. Springer, 2022. 3, 7
[68] Jay Zhangjie Wu, Yixiao Ge, Xintao Wang, Weixian Lei,
Yuchao Gu, Wynne Hsu, Ying Shan, Xiaohu Qie, and
Mike Zheng Shou. Tune-a-video: One-shot tuning of image
diffusion models for text-to-video generation. arXiv preprint
arXiv:2212.11565, 2022. 2, 3
[69] Jinbo Xing, Menghan Xia, Yong Zhang, Haoxin Chen, Xin-
tao Wang, Tien-Tsin Wong, and Ying Shan. Dynamicrafter:
Animating open-domain images with video diffusion priors.
2023. 3, 6
[70] Wei Xiong, Wenhan Luo, Lin Ma, Wei Liu, and Jiebo Luo.
Learning to generate time-lapse videos using multi-stage dy-
namic generative adversarial networks. In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recogni-
tion, pages 2364–2373, 2018. 3
[71] Jun Xu, Tao Mei, Ting Yao, and Yong Rui. Msr-vtt: A large
video description dataset for bridging video and language. In
CVPR, 2016. 7
[72] Wilson Yan, Yunzhi Zhang, Pieter Abbeel, and Aravind
Srinivas. Videogpt: Video generation using vq-vae and trans-
formers. arXiv preprint arXiv:2104.10157, 2021. 3

[73] Ruihan Yang, Prakhar Srivastava, and Stephan Mandt. Dif-
fusion probabilistic modeling for video generation.
arXiv
preprint arXiv:2203.09481, 2022. 3
[74] Shuai Yang, Yifan Zhou, Ziwei Liu, , and Chen Change
Loy. Rerender a video: Zero-shot text-guided video-to-video
translation. In ACM SIGGRAPH Asia Conference Proceed-
ings, 2023. 2, 3, 6, 7
[75] Hu Ye, Jun Zhang, Sibo Liu, Xiao Han, and Wei Yang. Ip-
adapter: Text compatible image prompt adapter for text-to-
image diffusion models. 2023. 1
[76] Hu Ye, Jun Zhang, Sibo Liu, Xiao Han, and Wei Yang. Ip-
adapter: Text compatible image prompt adapter for text-to-
image diffusion models. 2023. 3, 5
[77] Sihyun Yu, Jihoon Tack, Sangwoo Mo, Hyunsu Kim, Junho
Kim, Jung-Woo Ha, and Jinwoo Shin. Generating videos
with dynamics-aware implicit generative adversarial net-
works. In ICLR, 2021. 3
[78] David Junhao Zhang, Jay Zhangjie Wu, Jia-Wei Liu,
Rui Zhao, Lingmin Ran, Yuchao Gu, Difei Gao, and
Mike Zheng Shou. Show-1: Marrying pixel and latent dif-
fusion models for text-to-video generation. arXiv preprint
arXiv:2309.15818, 2023. 1
[79] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding
conditional control to text-to-image diffusion models, 2023.
2, 3
[80] Shiwei* Zhang, Jiayu* Wang, Yingya* Zhang, Kang Zhao,
Hangjie Yuan, Zhiwu Qing, Xiang Wang, Deli Zhao, and
Jingren Zhou. I2vgen-xl: High-quality image-to-video syn-
thesis via cascaded diffusion models. 2023. 3, 6
[81] Yabo Zhang, Yuxiang Wei, Dongsheng Jiang, Xiaopeng
Zhang, Wangmeng Zuo, and Qi Tian.
Controlvideo:
Training-free controllable text-to-video generation.
arXiv
preprint arXiv:2305.13077, 2023. 2, 3
[82] Rui Zhao, Yuchao Gu, Jay Zhangjie Wu, David Junhao
Zhang, Jiawei Liu, Weijia Wu, Jussi Keppo, and Mike Zheng
Shou.
Motiondirector: Motion customization of text-to-
video diffusion models. arXiv preprint arXiv:2310.08465,
2023. 1
[83] Daquan Zhou, Weimin Wang, Hanshu Yan, Weiwei Lv,
Yizhe Zhu, and Jiashi Feng. Magicvideo: Efficient video
generation with latent diffusion models.
arXiv preprint
arXiv:2211.11018, 2022. 1, 2, 3, 7

