Meta-Learning through Hebbian Plasticity in
Random Networks
Elias Najarro and Sebastian Risi
IT University of Copenhagen
2300 Copenhagen, Denmark
enaj@itu.dk, sebr@itu.dk
Abstract
Lifelong learning and adaptability are two deﬁning aspects of biological agents.
Modern reinforcement learning (RL) approaches have shown signiﬁcant progress
in solving complex tasks, however once training is concluded, the found solutions
are typically static and incapable of adapting to new information or perturbations.
While it is still not completely understood how biological brains learn and adapt so
efﬁciently from experience, it is believed that synaptic plasticity plays a prominent
role in this process. Inspired by this biological mechanism, we propose a search
method that, instead of optimizing the weight parameters of neural networks
directly, only searches for synapse-speciﬁc Hebbian learning rules that allow
the network to continuously self-organize its weights during the lifetime of the
agent. We demonstrate our approach on several reinforcement learning tasks with
different sensory modalities and more than 450K trainable plasticity parameters.
We ﬁnd that starting from completely random weights, the discovered Hebbian
rules enable an agent to navigate a dynamical 2D-pixel environment; likewise
they allow a simulated 3D quadrupedal robot to learn how to walk while adapting
to morphological damage not seen during training and in the absence of any
explicit reward or error signal in less than 100 timesteps. Code is available at
https://github.com/enajx/HebbianMetaLearning.
1
Introduction
Agents controlled by neural networks and trained through reinforcement learning (RL) have proven to
be capable of solving complex tasks [1–3]. However once trained, the neural network weights of these
agents are typically static, thus their behaviour remains mostly inﬂexible, showing limited adaptability
to unseen conditions or information. These solutions, whether found by gradient-based methods or
black-box optimization algorithms, are often immutable and overly speciﬁc for the problem they have
been trained to solve [4, 5]. When applied to a different tasks, these networks need to be retrained,
requiring many extra iterations.
Unlike artiﬁcial neural networks, biological agents display remarkable levels of adaptive behavior
and can learn rapidly [6, 7]. Although the underlying mechanisms are not fully understood, it is well
established that synaptic plasticity plays a fundamental role [8, 9]. For example, many animals can
quickly walk after being born without any explicit supervision or reward signals, seamlessly adapting
to their bodies of origin. Different plasticity-regulating mechanisms have been suggested which can
be encompassed in two main ideal-type families: end-to-end mechanisms which involve top-down
feedback propagating errors [10] and local mechanisms, which solely rely on local activity in order
to regulate the dynamics of the synaptic connections. The earliest proposed version of a purely local
mechanism is known as Hebbian plasticity, which in its simplest form states that the synaptic strength
between neurons changes proportionally to the correlation of activity between them [11].
34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
arXiv:2007.02686v5  [cs.NE]  19 Apr 2022

Timesteps
Weights
t = 0
t = 100
t = 200
Reward
A
B
C
A
B
C
1
Undamaged morphology
A
B
C
Front-left leg damage (not seen during training)
Front-right leg damage 
Figure 1: Hebbian Learning in Random Networks. Starting from random weights, the discovered
learning rules allow fast adaptation to different morphological damage without an explicit reward
signal. The ﬁgure shows the weights of the network at three different timesteps (A, B, C) during the
lifetime of the robot with standard morphology (top-left). Each column represents the weights of each
of the network layers at the different timesteps. At t=0 (A) the weights of the network are initialised
randomly by sampling from an uniform distribution w ∈U[-0.1, 0.1], thereafter their dynamics are
determined by the evolved Hebbian rules and the sensory input from the environment. After a few
timesteps the quadruped starts to move which reﬂects in an increase in the episodic reward (bottom
row). The network with the same Hebbian rules is able to adapt to robots with varying morphological
damage, even ones not seen during training (top right).
The rigidity of non-plastic networks and their inability to keep learning once trained can partially
be attributed to them traditionally having both a ﬁxed neural architecture and a static set of synaptic
weights. In this work we are therefore interested in algorithms that search for plasticity mechanisms
that allow agents to adapt during their lifetime [12–15]. While recent work in this area has focused
on determining both the weights of the network and the plasticity parameters, we are particularly
intrigued by the interesting properties of randomly-initialised networks in both machine learning
[16–18] and neuroscience [19]. Therefore, we propose to search for plasticity rules that work with
randomly initialised networks purely based on a process of self-organisation.
To accomplish this, we optimize for connection-speciﬁc Hebbian learning rules that allow an agent to
ﬁnd high-performing weights for non-trivial reinforcement learning tasks without any explicit reward
during its lifetime. We demonstrate our approach on two continuous control tasks and show that such
a network reaches a higher performance than a ﬁxed-weight network in a vision-based RL task. In a
3-D locomotion task, the Hebbian network is able to adapt to damages in the morphology of a
simulated quadrupedal robot which has not been seen during training, while a ﬁxed-weight network
fails to do so. In contrast to ﬁxed-weight networks, the weights of the Hebbian networks continuously
vary during the lifetime of the agent; the evolved plasticity rules give rise to the emergence of
2

an attractor in the weight phase-space, which results in the network quickly converging to high-
performing dynamical weights.
We hope that our demonstration of random Hebbian networks will inspire more work in neural
plasticity that challenges current assumptions in reinforcement learning; instead of agents starting
deployment with ﬁnely-tuned and frozen weights, we advocate for the use of more dynamical neural
networks, which might display dynamics closer to their biological counterparts. Interestingly, we
ﬁnd that the discovered Hebbian networks are remarkably robust and can even recover from having a
large part of their weights zeroed out.
In this paper we focus on exploring the potential of Hebbian plasticity to master reinforcement learning
problems. Meanwhile, artiﬁcial neural networks (ANNs) have been the object of great interest by
neuroscientists for being capable of explaining some neurobiological data [20], while at the same
time being able to perform certain visual cognitive tasks at a human-level. Likewise, demonstrating
how random networks – solely optimised through local rules – are capable of reaching competitive
performance in complex tasks may contribute to the pool of plausible models for understanding
how learning occurs in the brain. Finally, we hope this line of research will further help promoting
ANN-based RL frameworks to study how biological agents learn [21].
2
Related work
Meta-learning. The aim in meta-learning or learning-to-learn [22, 23] is to create agents that can
learn quickly from ongoing experience. A variety of different methods for meta-learning already
exist [24–29]. For example, Wang et al. [27] showed that a recurrent LSTM network [30] can learn
to reinforcement learn. In their work, the policy network connections stay ﬁxed during the agent’s
lifetime and learning is achieved through changes in the hidden state of the LSTM. While most
approaches, such as the work by Wang et al. [27], take the environment’s reward as input in the inner
loop of the meta-learning algorithms (either as input to the neural network or to adjust the network’s
weights), we do not give explicit rewards during the agent’s lifetime in the work presented here.
Typically, during meta-training, networks are trained on a number of different tasks and then tested
on their ability to learn new tasks. A recent trend in meta-learning is to ﬁnd good initial weights
(e.g. through gradient descent [28] or evolution [29]), from which adaptation can be performed in a
few iterations. One such approach is Model-Agnostic Meta-Learning (MAML) [28], which allows
simulated robots to quickly adapt to different goal directions. Hybrid approaches bringing together
gradient-based learning with an unsupervised Hebbian rules have also proven to improve performance
on supervised-learning tasks [31].
A less explored meta-learning approach is the evolution of plastic networks that undergo changes at
various timescales, such as in their neural connectivity while experiencing sensory feedback. These
evolving plastic networks are motivated by the promise of discovering principles of neural adaptation,
learning, and memory [13]. They enable agents to perform a type of meta-learning by adapting
during their lifetime through evolving recurrent networks that can store activation patterns [32] or
by evolving forms of local Hebbian learning rules that change the network’s weights based on the
correlated activation of neurons (“what ﬁres together wires together”). Instead of relying on Hebbian
learning rules, early work [14] tried to explore the optimization of the parameters of a parameterised
learning rule that is applied to all connections in the network. Most related to our approach is early
work by Floreano and Urzelai [33], who explored the idea of starting networks with random weights
and then applying Hebbian learning. This approach demonstrated the promise of evolving Hebbian
rules but was restricted to only four different types of Hebbian rules and small networks (12 neurons,
144 connections) applied to a simple robot navigation task.
Instead of training local learning rules through evolutionary optimization, recent work showed
it is also possible to optimize the plasticity of individual synaptic connections through gradient
descent [15]. However, while the trainable parameters in their work only determine how plastic each
connection is, the black-box optimization approach employed in this paper allows each connection to
implement its own Hebbian learning rule.
Self-Organization. Self-organization plays a critical role in many natural systems [34] and is an
active area of research in complex systems. It also recently gaining more prominence in machine
learning, with graph neural networks being a noteworthy example [35]. The recent work by Mord-
3

vintsev et al. [36] on growing cellular automata through local rules encoded by a neural network
has interesting parallels to the work we present here; in their work the growth of 2D images relies
on self-organization while in our work it is the network’s weights themselves that self-organize. A
beneﬁt of self-organizing systems is that they are very robust and adaptive. The goal in our proposed
approach is to take a step towards similar levels of robustness for neural network-based RL agents.
Neuroscience. In biological nervous systems, the weakening and strengthening of synapses through
synaptic plasticity is assumed to be one of the key mechanisms for long-term learning [8, 9]. Evolution
shaped these learning mechanisms over long timescales, allowing efﬁcient learning during our lives.
What is clear is that the brain can rewire itself based on experiences we undergo during our lifetime
[37]. Additionally, animals are born with a highly structured brain connectivity that allows them to
learn quickly form birth [38]. However, the importance of random connectivity in biological brains is
less well understood. For example, random connectivity seems to play a critical role in the prefrontal
cortex [39], allowing an increase in the dimensionality of neural representations. Interestingly, it was
only recently shown that these theoretical models matched experimental data better when random
networks were combined with simple Hebbian learning rules [19].
The most well-known form of synaptic plasticity occurring in biological spiking networks is spike-
timing-dependent plasticity (STDP). On the other hand, artiﬁcial neural networks have continuous
outputs which are usually interpreted as an abstraction of spiking networks in which the continuous
output of each neuron represents a spike-rate coding average –instead of spike-timing coding– of
a neuron over a long time window or, equivalent, of a subset of spiking neurons over a short time
window; in this scenario, the relative timing of the pre and post-synaptic activity does not play
a central role anymore [40, 41]. Spike-rate-dependent plasticity (SRDP) is a well documented
phenomena in biological brains [42, 43]. We take inspiration from this work, showing that random
networks combined with Hebbian learning can also enable more robust meta-learning approaches.
3
Meta-learning through Evolved Local Learning Rules
The main steps of our approach can be summarized as follows: (1) An initial population of neural
networks with random synapse-speciﬁc learning rules is created, (2) each network is initialised with
random weights and evaluated on a task based on its accumulated episodic reward, with the network
weights changing at each timestep following the discovered learning rules, and (3) a new population
is created through an evolution strategy [44], moving the learning-rule parameters towards rules with
higher cumulative rewards. The algorithm then starts again at (2), with the goal to progressively
discover more and more efﬁcient learning rules that can work with arbitrary initialised networks.
In more detail, the synapse-speciﬁc learning rules in this paper are inspired by biological Hebbian
mechanisms. We use a generalized Hebbian ABCD model [45, 46] to control the synaptic strength
between the artiﬁcial neurons of relatively simple feedforward networks. Speciﬁcally, the weights of
the agent are randomly initialized and updated during its lifetime at each timestep following:
∆wij = ηw · (Awoioj + Bwoi + Cwoj + Dw),
(1)
where wij is the weight between neuron i and j, ηw is the evolved learning rates, evolved correlation
terms Aw, evolved presynaptic terms Bw, evolved postsynaptic terms Cw, with oi and oj being
the presynaptic and postsynaptic activations respectively. While the coefﬁcients A, B, C explicitly
determine the local dynamics of the network weights, the evolved coefﬁcient D can be interpreted as
an individual inhibitory/excitatory bias of each connection in the network. In contrast to previous
work, our approach is not limited to uniform plasticity [47, 48] (i.e. each connection has the same
amount of plasticity) or being restricted to only optimizing a connection-speciﬁc plasticity value [15].
Instead, building on the ability of recent evolution strategy implementations to scale to a large number
of parameters [44], our approach allows each connection in the network to have both a different
learning rule and learning rate.
We hypothesize that this Hebbian plasticity mechanism should give rise to the emergence of an
attractor in weight phase-space, which leads the randomly-initialised weights of the policy network to
quickly converge towards high-performing values, guided by sensory feedback from the environment.
4

3.1
Optimization details
The particular population-based optimization algorithm that we are employing is an evolution
strategy (ES) [49, 50]. ES have recently shown to reach competitive performance compared to other
deep reinforcement learning approaches across a variety of different tasks [44]. These black-box
optimization methods have the beneﬁt of not requiring the backpropagation of gradients and can deal
with both sparse and dense rewards. Here, we adapt the ES algorithm by Salimans et al. [44] to not
optimize the weights directly but instead ﬁnding the set of Hebbian coefﬁcients that will dynamically
control the weights of the network during its lifetime based on the input from the environment.
In order to evolve the optimal local learning rules, we randomly initialise both the policy network’s
weights w and the Hebbian coefﬁcients h by sampling from an uniform distribution w ∈U[-0.1,
0.1] and h ∈U[-1, 1] respectively. Subsequently we let the ES algorithm evolve h, which in turn
determines the updates to the policy network’s weights at each timestep through Equation 1.
At each evolutionary step t we compute the task-dependent ﬁtness of the agent F(ht), we populate
a new set of n candidate solutions by sampling normal noise ϵi = N(0, 1) and adding it to the
current best solution ht, subsequently we update the parameters of the solution based on the ﬁtness
evaluation of each of the i ∈n candidate solutions:
ht+1 = ht + α
nσ
n
X
i=1
F(ht + σϵi) · ϵi,
where α modulates how much the parameters are updated at each generation and σ modulates the
amount of noise introduced in the candidate solutions. It is important to note that during its lifetime
the agent does not have access to this reward.
We compare our Hebbian approach to a standard ﬁxed-weight approach, using the same ES algorithm
to optimise either directly the weights or learning rule parameters respectively. All the code necessary
to evolve both the Hebbian networks as well as the static networks with the ES algorithm is available
at https://github.com/enajx/HebbianMetaLearning.
Figure 2: Test domains. The random Hebbian network approach introduced in this paper is tested on
the CarRacing-v0 environment [51] and a quadruped locomotion task. In the robot tasks, the same
network has to adapt to three morphologies while only seeing two of them during the training phase
(standard Ant-v0 morphology, morphology with damaged right front leg and unseen morphology
with damaged left front leg) without any explicit reward feedback.
4
Experimental Setups
We demonstrate our approach on two continuous control environments with different sensory modali-
ties (Fig. 2). The ﬁrst is a challenging vision-based RL task, in which the goal is to drive a racing
car through procedurally generated tracks as fast possible. While not appearing too complicated,
the tasks was only recently solved (achieving a score of more than 900 averaged over 100 random
rollouts) [52–54]. The second domain is a complex 3-D locomotion task that controls a four-legged
robot [55]. Here the information of the environment is represented as a one-dimensional state vector.
Vision-based environment As a vision-based environment, we use the CarRacing-v0 domain [51],
build with the Box2D physics engine. The output state of the environment is resized and normalised,
resulting in a observational space of 3 channels (RGB) of 84×84 pixels each. The policy network
consists of two convolutional layers, activated by hyperbolic tangent and interposed by pooling
layers which feed a 3-layers feedforward network with [128, 64, 3] nodes per layer with no bias.
This network has 92,690 weight parameters, 1,362 corresponding to the convolutional layers and
5

91,328 to the fully connected ones. The three network outputs control three continuous actions
(left/right steering, acceleration, break). Under the ABCD mechanism this results in 456,640 Hebbian
coefﬁcients including the lifetime learning rates η.
In this environment, only the weights of the fully connected layers are controlled by the Hebbian
plasticity mechanism, while the 1,362 parameters of the convolutional layers remain static during
the lifetime of the agent. The reason being that there is no natural deﬁnition of what the presynaptic
and postsynaptic activity of a convolution ﬁlter may be, hence making the interpretation of Hebbian
plasticity for convolutional layers challenging. Furthermore, previous research on the human visual
cortex indicates that the representation of visual stimuli in the early regions of the ventral stream
are compatible with the representations of convolutional layers trained for image recognition [56],
therefore suggesting that the variability of the parameters of convolutional layers should be limited.
The evolutionary ﬁtness is calculated as -0.1 every frame and +1000/N for every track tile visited,
where N is the total number of tiles in the generated track.
3-D Locomotion Task For the quadruped, we use a 3-layer feedforward network with [128, 64, 8]
nodes per layer, no bias and hyperbolic tangent as activation function. This architectural choice leads
to a network with 12,288 synapses. Under the ABCD plastic mechanism, which has 5 coefﬁcients per
synapse, this translates to a set of 61,440 Hebbian coefﬁcients including the lifetime learning rates
η. For the state-vector environment we use the open-source Bullet physics engine and its pyBullet
python wrapper [57] that includes the “Ant” robot, a quadruped with 13 rigid links, including four
legs and a torso, along with 8 actuated joints [58]. It is modeled after the ant robot in the MuJoCo
simulator [59] and constitutes a common benchmark in RL [28]. The robot has an input size of 28,
comprising the positional and velocity information of the agent and an action space of 8 dimensions,
controlling the motion of each of the 8 joints. The ﬁtness function of the quadruped agent selects for
distance travelled during a period of 1,000 timesteps along a ﬁxed axis.
The parameters used for the ES algorithm to optimize both the Hebbian and static networks are the
following: a population size 200 for the CarRacing-v0 domain and size 500 for the quadruped,
reﬂecting the higher complexity of this domain. Other parameters were the same for both domains and
reﬂect typical ES settings (ES algorithms are typically more robust to different hyperparameters than
other RL approaches [44]), with a learning rate α=0.2, α decay=0.995, σ=0.1, and σ decay=0.999.
These hyperparameters were found by trial-and-error and worked best in prior experiments.
4.1
Results
For each of the two domains, we performed three independent evolutionary runs (with different
random seeds) for both the static and Hebbian approach. We performed additional ablation studies on
restricted forms of the generalised Hebbian rule, which can be found in the Appendix.
Vision-based Environment To test how well the evolved solutions generalize, we compare the
cumulative rewards averaged over 100 rollouts for the highest-performing Hebbian-based approach
and traditional ﬁxed-weight approach. The set of local learning rules found by the ES algorithm
yield a reward of 872±11, while the static-weights solution only reached a performance of 711±16.
The numbers for the Hebbian network are slightly below the performance of the state-of-the-art
approaches in this domain which rely on additional neural attention mechanisms (914±15 [54]),
but on par with deep RL approaches such as PPO (865±159 [54]). The competitive performance
of the Hebbian learning agent is rather surprising, since it starts every one of the 100 rollouts with
completely different random weights but through the tuned learning rules it is able to adapt quickly.
While the Hebbian network takes slightly longer to reach a high training performance, likely because
of the increased parameter space (see Appendix), the beneﬁts are a higher generality when tested on
procedurally generated tracks not seen during training.
3-D Locomotion Task For the locomotion task, we created three variations of a 4-legged robot such
as to mimic the effect of partial damage to one of its legs (Fig. 2). The choice of these morphologies is
intended to create a task that would be difﬁcult to master for a neural network that is not able to adapt.
During training, both the static-weights and the Hebbian plastic networks follow the same set-up: at
each training step the policy is optimised following the ES algorithm described in Section 3.1 where
the ﬁtness function consists of the average distance walked of two morphologies, the standard one
and the one with damage on the right front leg. The third morphology (damaged on left front leg) is
left out of training loop in order to subsequently evaluate the generalisation of the networks.
6

Quadruped Damage
Seen / Unseen during training
Learning Rule
Distance travelled
Solved
No Damage
Seen
Hebbian
1051 ± 113
True
No Damage
Seen
static weights
1604 ± 171
True
Right front leg
Seen
Hebbian
1019 ± 116
True
Right front leg
Seen
static weights
1431 ± 54
True
Left front leg
Unseen
Hebbian
452 ± 95
True
Left front leg
Unseen
static weights
68 ± 56
False
Table 1: Average distance travelled by the highest-performing quadrupeds evolved with both local
rules (Hebbian) and static weights, across 100 rollouts. While the Hebbian learning approach ﬁnds
a solution for the seen and unseen morphologies (deﬁned as moving away from the initial start
position at least 100 units of length), the static-weights agent can only develop locomotion for the
two morphologies that were present during training.
For the quadruped, we deﬁne solving the task as monotonically moving away from its initial position
at least 100 units of length along a ﬁxed axis. Out of the ﬁve evolutionary runs, both the Hebbian
network and the static-network found solutions for the seen morphologies in all runs. On the other
hand, the static-weights network was incapable of ﬁnding a single solution that would solve the unseen
damaged morphology while the Hebbian network did manage to ﬁnd solutions for the damaged
unseen morphology. However, the performances of the Hebbian networks evaluated on the unseen
morphology have a high variance. Understanding why some Hebbian solution generalise and other
do not paves the way for further research; we hypothesize that in order to obtain a solution capable
of generalizing robustly the agent would need to be trained on a diverse set of morphologies with
randomized damages. To test how well the evolved solutions generalize, we compare the distance
walked averaged over 100 rollouts for the Hebbian and the static-weights networks. We report the
highest-performing solutions on each of the morphologies from a single evolutionary run (Table 1).
Since the static-weights network can not adapt to the environment, it solves efﬁciently the morpholo-
gies that has seen during training but fails at the unseen one. On the other hand, the Hebbian network
is capable of adapting to the new morphologies leading to an efﬁcient self-organization of network’s
synaptic weights (Fig. 1). Furthermore, we found that the initial random weights of the network
can even be sampled from other distributions than the one used during the discovery of the Hebbian
coefﬁcients, such as N(0, 0.1), and the agent still reaches a comparable performance.
Interestingly, even without the presence of any reward feedback during its lifetime, the Hebbian-based
network is able to ﬁnd well-performing weights for each of the three morphologies. The incoming
activation patterns alone are enough for the network to adapt without explicitly knowing which is the
morphology currently being simulated. However, for the morphologies that the static-weight network
did solve, it reached a higher reward than the Hebbian-based approach. Several reasons may explain
this, including the need of extra time to learn or the lager size of the parameters space, which could
require longer training times to ﬁnd even more efﬁcient plasticity rules.
In order to determine the minimum number of timesteps the weights need to converge from random
to optimal during an agent’s lifetime, we investigated freezing the Hebbian update mechanism of
the weights after a different number of timesteps and examining the resulting episode’s cumulative
reward. We observe that the weights only need between 30 and 80 timesteps (i.e. Hebbian updates), to
converge to a set of optimal values (Fig. 3, left). Furthermore, we tested the resilience of the network
to external perturbations by saturating all its outputs to 1.0 for 100 timesteps, effectively freezing the
agent in place. Fig. 3, right shows that the evolved Hebbian rules allow the network to recover to
optimal weights within a few timesteps. Furthermore, the Hebbian network is able to recover from
a partial loss of its connections, which we simulate by zeroing out a subset of the synaptic weights
during one timestep (Fig. 4, left). We observe a brief disruption in the behavior of the agent, however,
the network is able to reconverge towards an optimal solution in a few timesteps (Fig. 4, upper-right).
In order to get a better insight into the effect of the discovered plasticity rules and the development of
the weight patterns during the Hebbian learning, we performed a dimensionality reduction through
principal component analysis (PCA) which projects the high-dimensional space where the network
weights live to a 3-dimensional representation at each timestep such that most of the variance
is best explained by this lower dimensional representation (Fig. 5). For the car environment the
7

0
200
400
600
800
1000
Freezing timestep
200
0
200
400
600
800
1000
1200
1400
Episode reward
0
200
400
600
800
1000
Episode timestep
0.5
0.0
0.5
1.0
1.5
Timestep reward
Figure 3: Learning efﬁciency and robustness to actuator perturbations. Left: The cumulative reward
for the quadruped whose weights are frozen at different timesteps. The Hebbian network only needs
in the order of 30–80 timesteps to converge to high-performing weights. Right: The performance of
a quadruped whose actuators are frozen during 100 timesteps (from t=300 to t=400). The robot is
able to quickly recover from this perturbation in around 50 timesteps.
10 timesteps later
A
B
A B
Figure 4: Resilience to weights perturbations. A: Visualisation of the network’s weights at the
timestep when a third of its weights are zeroed out, shown as a black band. B: Visualisation of
the network’s weights 10 timesteps after the zeroing; the network’s weights recovered from the
perturbation. Right: Performance of the quadruped when we zero out a subset of the synaptic weights
quickly recovers after an initial drop. The purple line indicates the timestep of the weight zeroing.
weights span ubiquitously across the three main components of the reduced PCA space, this contrasts
with the dynamics of a network in which we set the Hebbian coefﬁcient (Eq.1) to random values;
here the weight trajectory lacks any structure and oscillates around zero. In the case of the three
quadruped morphologies, the trajectories of the Hebbian network follow a 3-dimensional curve, with
an oscillatory signature; with random Hebbian coefﬁcients the network does not give rise to any
apparent structure in its weights trajectory.
5
Discussion and Future Work
In this work we introduced a novel approach that allows agents with random weights to adapt quickly
to a task. It is interesting to note that lifetime adaptation happens without any explicitly provided
reward signal, and is only based on the evolved Hebbian local learning rules. In contrast to typical
static network approaches, in which the weights of the network do not change during the lifetime of
the agent, the weights in the Hebbian-based networks self-organize and converge to an attractor in
weight space during their lifetime.
The ability to adapt weights quickly is shown to be important for tasks such as adapting to damaged
robot morphologies, which could be useful for tasks such as continual learning [60]. The ability to
converge to high-performing weights from initially random weights is surprisingly robust and the
best networks manage to do this for each of the 100 rollouts in the CarRacing domain. That the
Hebbian networks are more general but performance for a particular task/robot morphology can be
less is maybe not surprising: learning generally takes time but can result in greater generalisation [61].
8

PC 1
200
0
200
400
600
800
1000
PC 2
100
0
100
200
300
PC 3
0
200
400
600
800
Random coefficients
Evolve coefficients
PC 1
200
150
100
50
0
50
100
150
200
PC 2
40
20
0
20
40
60
PC 3
30
20
10
0
10
20
30
Random coefficients
Standard morphology
Damaged-left morphology
Damaged-right morphology
Figure 5: Discovered Weight Attractors. Low dimensional representations of the weights dynamics
(each dot represents a timestep, ﬁrst timestep indicated with a star marker). The plotted trajectory
represents the evolution of the ﬁrst 3 principal components (PCA) of the synaptic weights controlled
by the Hebbian plasticity mechanism with the evolved coefﬁcients over 1,000 timesteps. Left: Pixel-
based CarRacing-v0 agent. Right: The three quadruped agent morphologies: Bullet’s AntBulletEnv-
v0, the two damaged morphologies [2].
Interestingly, randomly initialised networks have recently shown particularly interesting properties in
different domains [16–18]. We add to this recent trend by demonstrating that random weights are all
you need to adapt quickly to some complex RL domains, given that they are paired with expressive
neural plasticity mechanisms.
An interesting future work direction is to extend the approach with neuromodulated plasticity, which
has shown to improve the performance of evolving plastic neural networks [62] and plastic network
trained through backpropagation [63]. Among other properties, neuromodulation allows certain
neurons to modulate the level of plasticity of the connections in the neural network. Additionally, a
complex system of neuromodulation seems critical in animal brains for more elaborated forms of
learning [64]. Such an ability could be particularly important when giving the network an additional
reward signal as input for goal-based adaptation. The approach presented here opens up other
interesting research areas such as also evolving the agents neural architecture [65] or encoding the
learning rules through a more indirect genotype-to-phenotype mapping [66, 38].
In the neuroscience community, the question of which parts of animal behaviors are already innate
and which parts are acquired through learning is hotly debated [38]. Interestingly, randomness in
the connectivity of these biological networks potentially plays a more important part than previously
recognized. For example, random feedback connections could allow biological brains to perform
a type of backpropagation [67], and there is recent evidence suggesting that the prefrontal cortex
might in effect employ a combination of random connectivity and Hebbian learning [19]. To the best
of our knowledge, this is the ﬁrst time the combination of random networks and Hebbian learning
has been applied to a complex reinforcement learning problem, which we hope could inspire further
cross-pollination of ideas between neuroscience and machine learning in the future [20].
In contrast to current reinforcement learning algorithms that try to be as general as possible, evolution
biased animal nervous system to be able to quickly learn by restricting their learning to what is
important for their survival [38]. The results presented in this paper, in which the innate agent’s
knowledge is the evolved learning rules, take a step in this direction. The presented approach opens
up interesting future research direction that suggest to demphasize the role played by the network’s
weights, and focus more on the learning rules themselves. The results on two complex and different
reinforcement learning tasks suggest that such an approach is worth exploring further.
Acknowledgements
This work was supported by a DFF-Danish ERC-programme grant and an Amazon Research Award.
9

Broader Impact
The ethical and future societal consequences of this work are hard to predict but likely similar to other
work dealing with more adaptive agents and robots. In particular, by giving robots the ability to still
function when injured could make it easier for them being deployed in areas that have both a positive
and negative impact on society. In the very long term, robots that can adapt could help in industrial
automation or help to care for the elderly. On the other hand, more adaptive robots could also be
more easily used for military applications. The approach presented in this paper is far from being
deployed in these areas, but it its important to discuss its potential long-term consequences early on.
References
[1] Oriol Vinyals, Igor Babuschkin, Wojciech M Czarnecki, Michaël Mathieu, Andrew Dudzik, Jun-
young Chung, David H Choi, Richard Powell, Timo Ewalds, Petko Georgiev, et al. Grandmaster
level in StarCraft II using multi-agent reinforcement learning. Nature, 575(7782):350–354,
2019.
[2] Christopher Berner, Greg Brockman, Brooke Chan, Vicki Cheung, Przemysław D˛ebiak, Christy
Dennison, David Farhi, Quirin Fischer, Shariq Hashme, Chris Hesse, et al. Dota 2 with large
scale deep reinforcement learning. arXiv preprint arXiv:1912.06680, 2019.
[3] Kai Arulkumaran, Marc Peter Deisenroth, Miles Brundage, and Anil Anthony Bharath. A brief
survey of deep reinforcement learning. arXiv preprint arXiv:1708.05866, 2017.
[4] Chiyuan Zhang, Oriol Vinyals, Remi Munos, and Samy Bengio. A study on overﬁtting in deep
reinforcement learning. arXiv preprint arXiv:1804.06893, 2018.
[5] Niels Justesen, Ruben Rodriguez Torrado, Philip Bontrager, Ahmed Khalifa, Julian Togelius, and
Sebastian Risi. Illuminating generalization in deep reinforcement learning through procedural
level generation. NeurIPS 2018 Workshop on Deep Reinforcement Learning, 2018.
[6] C. Lloyd Morgan. Animal Intelligence 1. Nature, 26(674):523–524, Sep 1882. ISSN 1476-4687.
doi: 10.1038/026523b0.
[7] Euan M Macphail. Brain and intelligence in vertebrates. Oxford University Press, USA, 1982.
[8] Xu Liu, Steve Ramirez, Petti T Pang, Corey B Puryear, Arvind Govindarajan, Karl Deisseroth,
and Susumu Tonegawa. Optogenetic stimulation of a hippocampal engram activates fear
memory recall. Nature, 484(7394):381–385, 2012.
[9] Stephen J Martin, Paul D Grimwood, and Richard GM Morris. Synaptic plasticity and memory:
an evaluation of the hypothesis. Annual review of neuroscience, 23(1):649–711, 2000.
[10] João Sacramento, Rui Ponte Costa, Yoshua Bengio, and Walter Senn. Dendritic cortical
microcircuits approximate the backpropagation algorithm. ArXiv e-prints, Oct 2018. URL
https://arxiv.org/abs/1810.11393.
[11] D. O. Hebb. The organization of behavior; a neuropsychological theory. Wiley, 1949.
[12] Eseoghene Ben-Iwhiwhu, Pawel Ladosz, Jeffery Dick, Wen-Hua Chen, Praveen Pilly, and
Andrea Soltoggio. Evolving inborn knowledge for fast adaptation in dynamic pomdp problems.
arXiv preprint arXiv:2004.12846, 2020.
[13] Andrea Soltoggio, Kenneth O Stanley, and Sebastian Risi. Born to learn: the inspiration,
progress, and future of evolved plastic artiﬁcial neural networks. Neural Networks, 108:48–67,
2018.
[14] Yoshua Bengio, Samy Bengio, and Jocelyn Cloutier. Learning a synaptic learning rule. Neural
Networks, 1991.
[15] Thomas Miconi, Jeff Clune, and Kenneth O. Stanley. Differentiable plasticity: training plastic
neural networks with backpropagation. ArXiv e-prints, Apr 2018. URL https://arxiv.org/
abs/1804.02464.
10

[16] Herbert Jaeger and Harald Haas. Harnessing nonlinearity: Predicting chaotic systems and
saving energy in wireless communication. science, 304(5667):78–80, 2004.
[17] Jürgen Schmidhuber, Daan Wierstra, Matteo Gagliolo, and Faustino Gomez. Training recurrent
networks by evolino. Neural computation, 19(3):757–779, 2007.
[18] Dmitry Ulyanov, Andrea Vedaldi, and Victor Lempitsky. Deep image prior. In Proceedings of
the IEEE Conference on Computer Vision and Pattern Recognition, pages 9446–9454, 2018.
[19] Grace W Lindsay, Mattia Rigotti, Melissa R Warden, Earl K Miller, and Stefano Fusi. Hebbian
learning in a random network captures selectivity properties of the prefrontal cortex. Journal of
Neuroscience, 37(45):11021–11036, 2017.
[20] Blake A Richards, Timothy P Lillicrap, Philippe Beaudoin, Yoshua Bengio, Rafal Bogacz,
Amelia Christensen, Claudia Clopath, Rui Ponte Costa, Archy de Berker, Surya Ganguli, et al.
A deep learning framework for neuroscience. Nature neuroscience, 22(11):1761–1770, 2019.
[21] Isabella Pozzi, Sander Bohté, and Pieter Roelfsema. A biologically plausible learning rule for
deep learning in the brain. 2018.
[22] Sebastian Thrun and Lorien Pratt. Learning to learn: Introduction and overview. In Learning to
learn, pages 3–17. Springer, 1998.
[23] Jürgen Schmidhuber. Learning to control fast-weight memories: an alternative to dynamic
recurrent networks. Neural Computation, 4(1):131–139, 1992.
[24] Sachin Ravi and Hugo Larochelle. Optimization as a model for few-shot learning. In Interna-
tional Conference on Learning Representations (ICLR 2018), 2017.
[25] Barret Zoph and Quoc V Le. Neural architecture search with reinforcement learning. arXiv
preprint arXiv:1611.01578, 2016.
[26] Jürgen Schmidhuber. A ‘self-referential’weight matrix. In International Conference on Artiﬁcial
Neural Networks, pages 446–450. Springer, 1993.
[27] Jane X Wang, Zeb Kurth-Nelson, Dhruva Tirumala, Hubert Soyer, Joel Z Leibo, Remi Munos,
Charles Blundell, Dharshan Kumaran, and Matt Botvinick. Learning to reinforcement learn.
arXiv preprint arXiv:1611.05763, 2016.
[28] Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adap-
tation of deep networks. In Proceedings of the 34th International Conference on Machine
Learning-Volume 70, pages 1126–1135. JMLR. org, 2017.
[29] Chrisantha Thomas Fernando, Jakub Sygnowski, Simon Osindero, Jane Wang, Tom Schaul,
Denis Teplyashin, Pablo Sprechmann, Alexander Pritzel, and Andrei A Rusu. Meta learning by
the baldwin effect. arXiv preprint arXiv:1806.07917, 2018.
[30] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation, 9(8):
1735–1780, 1997.
[31] Jeffrey Cheng, Ari Benjamin, Benjamin Lansdell, and Konrad Paul Kording. Augmenting
Supervised Learning by Meta-learning Unsupervised Local Rules. NeurIPS 2019 Workshop
Neuro AI, Sep 2019. URL https://openreview.net/pdf?id=HJlKNmFIUB.
[32] Randall D Beer and John C Gallagher. Evolving dynamical neural networks for adaptive
behavior. Adaptive behavior, 1(1):91–122, 1992.
[33] Dario Floreano and Joseba Urzelai. Evolutionary robots with on-line self-organization and
behavioral ﬁtness. Neural Networks, 13(4-5):431–443, 2000.
[34] Scott Camazine, Jean-Louis Deneubourg, Nigel R Franks, James Sneyd, Eric Bonabeau, and
Guy Theraula. Self-organization in biological systems, volume 7. Princeton university press,
2003.
11

[35] Zonghan Wu, Shirui Pan, Fengwen Chen, Guodong Long, Chengqi Zhang, and S Yu Philip. A
comprehensive survey on graph neural networks. IEEE Transactions on Neural Networks and
Learning Systems, 2020.
[36] Alexander Mordvintsev, Ettore Randazzo, Eyvind Niklasson, and Michael Levin. Growing neu-
ral cellular automata. Distill, 2020. doi: 10.23915/distill.00023. https://distill.pub/2020/growing-
ca.
[37] Stephen T Grossberg. Studies of mind and brain: Neural principles of learning, perception,
development, cognition, and motor control, volume 70. Springer Science & Business Media,
2012.
[38] Anthony M Zador. A critique of pure learning and what artiﬁcial neural networks can learn
from animal brains. Nature communications, 10(1):1–7, 2019.
[39] Wolfgang Maass, Thomas Natschläger, and Henry Markram. Real-time computing without
stable states: A new framework for neural computation based on perturbations. Neural compu-
tation, 14(11):2531–2560, 2002.
[40] Steven A. Prescott and Terrence J. Sejnowski. Spike-Rate Coding and Spike-Time Coding Are
Affected Oppositely by Different Adaptation Mechanisms. Journal of Neuroscience, 28(50):
13649, Dec 2008. doi: 10.1523/JNEUROSCI.1792-08.2008.
[41] Romain Brette. Philosophy of the Spike: Rate-Based vs. Spike-Based Theories of the Brain.
Frontiers in Systems Neuroscience, 9, Nov 2015. ISSN 1662-5137. doi: 10.3389/fnsys.2015.
00151.
[42] P. J. Sjöström, G. G. Turrigiano, and S. B. Nelson. Rate, timing, and cooperativity jointly
determine cortical synaptic plasticity. Neuron, 32(6):1149–1164, Dec 2001. ISSN 0896-6273.
doi: 10.1016/s0896-6273(01)00542-6.
[43] Luca A. Finelli, Seth Haney, Maxim Bazhenov, Mark Stopfer, and Terrence J. Sejnowski.
Synaptic Learning Rules and Sparse Coding in a Model Sensory System. PLOS Computational
Biology, 4(4):e1000062, Apr 2008. ISSN 1553-7358. doi: 10.1371/journal.pcbi.1000062.
[44] Tim Salimans, Jonathan Ho, Xi Chen, Szymon Sidor, and Ilya Sutskever. Evolution Strategies
as a Scalable Alternative to Reinforcement Learning. ArXiv e-prints, Mar 2017. URL https:
//arxiv.org/abs/1703.03864.
[45] Andrea Soltoggio, Peter Durr, Claudio Mattiussi, and Dario Floreano. Evolving neuromod-
ulatory topologies for reinforcement learning-like problems.
In 2007 IEEE Congress on
Evolutionary Computation, pages 2471–2478. IEEE, 2007.
[46] Yael Niv, Daphna Joel, Isaac Meilijson, and Eytan Ruppin. Evolution of reinforcement learning
in uncertain environments: Emergence of risk-aversion and matching. In European Conference
on Artiﬁcial Life, pages 252–261. Springer, 2001.
[47] Jimmy Ba, Geoffrey E Hinton, Volodymyr Mnih, Joel Z Leibo, and Catalin Ionescu. Using fast
weights to attend to the recent past. In Advances in Neural Information Processing Systems,
pages 4331–4339, 2016.
[48] Jürgen Schmidhuber. Reducing the ratio between learning complexity and number of time
varying variables in fully recurrent nets. In International Conference on Artiﬁcial Neural
Networks, pages 460–463. Springer, 1993.
[49] Hans-Georg Beyer and Hans-Paul Schwefel. Evolution strategies–a comprehensive introduction.
Natural computing, 1(1):3–52, 2002.
[50] Daan Wierstra, Tom Schaul, Jan Peters, and Juergen Schmidhuber. Natural evolution strategies.
In 2008 IEEE Congress on Evolutionary Computation (IEEE World Congress on Computational
Intelligence), pages 3381–3387. IEEE, 2008.
[51] Oleg Klimov. Carracing-v0. 2016. URL https://gym.openai.com/envs/CarRacing-v0/.
12

[52] Sebastian Risi and Kenneth O Stanley. Deep neuroevolution of recurrent and discrete world
models. In Proceedings of the Genetic and Evolutionary Computation Conference, pages
456–462, 2019.
[53] David Ha and Jürgen Schmidhuber. Recurrent world models facilitate policy evolution. In
Advances in Neural Information Processing Systems, pages 2450–2462, 2018.
[54] Yujin Tang, Duong Nguyen, and David Ha. Neuroevolution of self-interpretable agents. arXiv
preprint arXiv:2003.08165, 2020.
[55] John Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, and Pieter Abbeel. High-
Dimensional Continuous Control Using Generalized Advantage Estimation. ArXiv e-prints, Jun
2015. URL https://arxiv.org/abs/1506.02438.
[56] Daniel L. K. Yamins and James J. DiCarlo. Using goal-driven deep learning models to under-
stand sensory cortex. Nature Neuroscience, 19(3):356–365, Feb 2016. ISSN 1546-1726. doi:
10.1038/nn.4244.
[57] Erwin Coumans and Yunfei Bai. Pybullet, a python module for physics simulation for games,
robotics and machine learning. http://pybullet.org, 2016–2019.
[58] Yan Duan, Xi Chen, Rein Houthooft, John Schulman, and Pieter Abbeel. Benchmarking
Deep Reinforcement Learning for Continuous Control. ArXiv e-prints, Apr 2016. URL
https://arxiv.org/abs/1604.06778v3.
[59] Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based
control. In 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems, pages
5026–5033. IEEE, 2012.
[60] German I Parisi, Ronald Kemker, Jose L Part, Christopher Kanan, and Stefan Wermter. Continual
lifelong learning with neural networks: A review. Neural Networks, 2019.
[61] George Gaylord Simpson. The baldwin effect. Evolution, 7(2):110–117, 1953.
[62] Andrea Soltoggio, John A Bullinaria, Claudio Mattiussi, Peter Dürr, and Dario Floreano.
Evolutionary advantages of neuromodulated plasticity in dynamic, reward-based scenarios. In
Proceedings of the 11th international conference on artiﬁcial life (Alife XI), number CONF,
pages 569–576. MIT Press, 2008.
[63] Thomas Miconi, Aditya Rawal, Jeff Clune, and Kenneth O Stanley. Backpropamine: training
self-modifying neural networks with differentiable neuromodulated plasticity. arXiv preprint
arXiv:2002.10585, 2020.
[64] Michael J Frank, Lauren C Seeberger, and Randall C O’reilly. By carrot or by stick: cognitive
reinforcement learning in parkinsonism. Science, 306(5703):1940–1943, 2004.
[65] Adam Gaier and David Ha. Weight Agnostic Neural Networks. ArXiv e-prints, Jun 2019. URL
https://arxiv.org/abs/1906.04358.
[66] Sebastian Risi and Kenneth O. Stanley. Indirectly Encoding Neural Plasticity as a Pattern of
Local Rules. SpringerLink, pages 533–543, Aug 2010. doi: 10.1007/978-3-642-15193-4_50.
[67] Timothy P Lillicrap, Daniel Cownden, Douglas B Tweed, and Colin J Akerman. Random
feedback weights support learning in deep neural networks. arXiv preprint arXiv:1411.0247,
2014.
[68] Jonathan Frankle and Michael Carbin. The Lottery Ticket Hypothesis: Finding Sparse, Train-
able Neural Networks. ArXiv e-prints, Mar 2018. URL https://arxiv.org/abs/1803.
03635v5.
13

6
Appendix
6.1
Network Weight Visualizations
Fig. 6 shows an example of how we visualize the weights of the network for a particular timestep.
Each pixel represents the weight value wij of each synaptic connection. We represent the weights of
each of the three fully connected layers FC layer 1, FC layer 2, FC layer 3 separately: the quadruped’s
network has an input space of dimension 28 and three fully connected layers with [128, 64, 8] neurons
respectively, hence the rectangle above FC layer 1 has an horizontal dimension of 28 and a vertical
one of 128, the 2nd layer FC layer 2 has an horizontal dimension of 64 and a vertical one of 128
while the last layer’s FC layer 3 dimension is 64 vertical and 8 horizontally, which corresponds to the
dimension of the action space. Darker pixels indicate negative values while white pixels are positive
values. In the case of the CarRacing environment the weights are normalised to the interval [-1,+1],
while the quadruped agents have unbounded weights.
FC layer 1
FC layer 2
FC layer 3
Figure 6: Network Weights Visualizations. Visualisation of a random initial state of the network’s
weights. Each column represents the weights of each of the three layers while each pixel represents
the value of a weight between two neurons.
6.2
Training efﬁciency
We show the training over generations for both approaches and both domains in Fig. 7. Even though
the Hebbian method has to optimize a signiﬁcant larger number of parameters, training performance
increases similarly fast for both approaches.
(a) Training curves for the Car environment
0
100
200
300
400
500
Generation
0
200
400
600
800
1000
1200
1400
1600
Distance Walked
Hebbian
Static
(b) Training curves for the quadrupeds
Figure 7: Left: Training curve for the car environment. Right: Training curve of the quadrupeds for
both the static network and the Hebbian one. Curves are averaged over three evolutionary runs.
14

6.3
Hebbian rules
We analyze the different ﬂavours of Hebbian rules derived from Eq. 2 in the car racing environment.
For this experiment, we do not evolve the parameters of the convolutional layers and instead they
are randomly ﬁxed at initialisation; we solely evolve the Hebbian coefﬁcients controlling the feed
forward layers. From the simplest one where all but the A coefﬁcients are zero, to its most general
form where all the four A, B, C, D coefﬁcient and the intra-life learning rate η are present (Fig. 8):
∆wij = ηw · (Awoioj + Bwoi + Cwoj + Dw),
(2)
The static, and all the generalised Hebbian models can solve pixel-based task, only the Hebbian
version with a single unique coefﬁcient per synapse A is incapable of solving the task. The slower
convergence of the Hebbian models with more coefﬁcients can be explained by the fact that larger
parameter spaces need more generations to be explored by the ES algorithm.
0
50
100
150
200
250
300
Generation
0
100
200
300
400
500
600
700
800
900
Cumulative Reward
A
AD+lr
ABC
ABC+lr
ABCD+lr
Figure 8: Hebbian rules ablations. Training curves for the car racing agent with ﬁve different Hebbian
rule variations. Curves are averaged over three evolutionary runs.
We also show the distribution of coefﬁcients of the most general ABCD+η version (Fig. 10), which
shows a normal distribution. We hypothesise that this distribution is potentially necessary to allow
the self-organization of weights to not grow to extreme values. Analysing the resulting weight
distributions and evolved rules opens up many interesting future research directions.
6.4
Evolving initial weights and learning rules
We experimented with evolving –alongside the Hebbian coefﬁcients– the initial weights of the
network rather than randomly initializing them at each episode. We do this by sampling normal
noise twice (Algorithm 2, Step 5 from [44]) and computing the ﬁtness of the resulting solution pairs
(Hebbian coefﬁcients, initial weights). Surprisingly, this does not increase the training efﬁciency of
the agents (Fig. 9). Furthermore, we ﬁnd that runs for the CarRacing environment where we co-evolve
the initial conditions are more likely to stall on local optima: 2 out of 3 runs found a network with
good performance (at least 800 reward), while the third run stalled on low performance (a reward of
less than 100). This ﬁnding may be explained by the extra difﬁculty that co-evolution introduces in
the ES algorithm as well as the extra lottery-ticket initialisation of both the initial weights and the
Hebbian coefﬁcients [68]. However, other possible implementations of this system may yield better
results and evolving both the connections’ Hebbian coefﬁcients and learning rules has shown promise
in smaller networks [45, 13, 66].
15

0
100
200
300
400
500
Generation
0
200
400
600
800
1000
1200
1400
1600
Distance Walked
Hebbian
Coevolved initial weights
Figure 9: Training curves of the Hebbian networks for the quadruped environments. We show that
initializing the network with random weights at each episode and co-evolving the initial weights lead
to similar results. Curves are averaged over three evolutionary runs.
6
4
2
0
2
4
6
Coefficient value
0
200
400
600
800
Count
w
Aw
Bw
Cw
Dw
7.5
5.0
2.5
0.0
2.5
5.0
7.5
Coefficient value
0
1000
2000
3000
4000
5000
6000
Count
w
Aw
Bw
Cw
Dw
Figure 10: Distribution of Hebbian coefﬁcients for the Hebbian network solutions of the quadrupeds
(left) and the racing car (right).
16

