Intellige: A User-Facing Model Explainer for Narrative Explanations
Jilei Yang, LinkedIn Corporation, jlyang@linkedin.com
Diana Negoescu, LinkedIn Corporation, dnegoescu@linkedin.com
Parvez Ahammad, LinkedIn Corporation, pahammad@linkedin.com
Abstract
Predictive machine learning models often lack interpretability, resulting in low trust from model end users despite
having high predictive performance. While many model interpretation approaches return top important features to
help interpret model predictions, these top features may not be well-organized or intuitive to end users, which
limits model adoption rates. In this paper, we propose Intellige, a user-facing model explainer that creates
user-digestible interpretations and insights reﬂecting the rationale behind model predictions. Intellige builds an
end-to-end pipeline from machine learning platforms to end user platforms, and provides users with an interface
for implementing model interpretation approaches and for customizing narrative insights. Intellige is a platform
consisting of four components: Model Importer, Model Interpreter, Narrative Generator, and Narrative Exporter.
We describe these components, and then demonstrate the effectiveness of Intellige through use cases at LinkedIn.
Quantitative performance analyses indicate that Intellige’s narrative insights lead to lifts in adoption rates of
predictive model recommendations, as well as to increases in downstream key metrics such as revenue when
compared to previous approaches, while qualitative analyses indicate positive feedback from end users.
Keywords: model interpretation, user-digestible narrative explanation, sales & marketing insights.
1
arXiv:2105.12941v1  [stat.ML]  27 May 2021

1. Introduction
Predictive machine learning models are widely used in a va-
riety of areas in industry. For example, in sales and market-
ing, predictive models can help determine which potential
customers are likely to purchase a product, and in healthcare,
they can assist clinicians in detecting the risks of certain
diseases. Complex predictive models such as random for-
est, gradient boosted trees, and deep neural networks can
produce more accurate predictions than simple models such
as linear regression and decision trees, and are therefore
preferred in many use cases where prediction accuracy is
of utmost importance. However, one important challenge is
explaining model predictions to end users who are experts
in their domains, using application-speciﬁc platforms and
language. Previous literature points out that users can be re-
luctant to use the predictive models if they do not understand
how and why these models make predictions [1], [2]. There-
fore, building a user-facing model explainer that provides
model interpretation and feature reasoning becomes cru-
cial for engendering trust in prediction results and creating
meaningful insights based on them.
Unfortunately, most complex predictive models with high
predictive performance are intrinsically opaque, causing
difﬁculties in intuitive interpretations. Even though some
models output a list of globally important features to inter-
pret the overall model prediction, usually no interpretations
at individual sample level are produced. For example, in
sales prediction, it may be that for customer A, browsing
time is the most important feature whereas for customer
B, discount is the most important. A sales team may
strategize different customers individually by learning each
customer’s own top features. Therefore, developing a user-
facing model explainer which provides feature reasoning at
individual sample level is of critical need [3].
There exist several state-of-the-art model interpretation ap-
proaches that enable sample-level feature reasoning, e.g.,
LIME [3], KernelSHAP [4], and TreeSHAP [5]. These
approaches produce feature importance scores for each sam-
ple, indicating how much each feature has contributed to
the model prediction. A typical example of model pre-
diction and interpretation results using LIME for a jobs
upsell model in LinkedIn sales prediction is shown in Table
1. Here, a random forest model predicts how likely each
LinkedIn customer is to purchase more job slot products at
contract renewal by using over 100 features covering areas
such as job slots usage, job seeker activity, and company
level attributes.
The left panel of Table 1 displays the model outputs with
the interpretation results for a speciﬁc customer in jobs
upsell prediction. Here, even though we have conducted
sample-level feature reasoning by providing top important
feature lists, there still exist several challenges when surfac-
ing model interpretation results to end users such as sales
teams:
1. Feature names in top important feature list may not be
easily understood by sales teams. For example, feature
names such as hire cntr s3 and conn cmp s4
may be too abstract for sales team to extract meaningful
information.
2. Top important feature lists may not be well-organized.
For example, some features are closely related to each
other and can be grouped, some features contain too
many details for sales team to digest, and some fea-
tures are not very meaningful to sales team and can be
removed.
3. Top important feature lists may not be easily inte-
grated into sales management platforms which use
sales-friendly language, resulting in low adoption rates
by sales teams on predictive modeling intelligence.
A desired resolution is to convert these non-intuitive model
interpretation results into user-digestible narrative insights
as shown in the right panel of Table 1.
Two common approaches for creating narratives in the lit-
erature are generation-based and template-based [6]. The
former relies on neural-network-based natural language gen-
eration techniques, and can generate narrative explanations
in an automatic way to effectively save human effort. How-
ever, the generation-based approach usually requires a large
training data set, which may not exist in many use cases
– for example, in sales prediction, our main use case, ex-
isting narratives for sales recommendations are very lim-
ited, as sales teams usually do not write logs of how they
reached decisions about their customers. On the other hand,
template-based approaches can achieve a much higher gen-
eralizability than generation-based approaches, as they do
not rely on such training data. For this reason, we choose the
template-based approach for creating narrative explanations.
In this paper, we propose a template-based, user-facing
model explainer - Intellige, which aims to create user-
understandable interpretation and insights, and to reﬂect
the rationale behind machine learning models. In contrast
with sample-level model interpretation approaches, Intellige
has added follow-up steps to convert non-intuitive model
interpretation results into user-digestible narrative insights.
In contrast with other approaches, such as rule-based narra-
tive generation systems, Intellige is more scalable as it has
leveraged machine learning models to conduct automatic
narrative ranking.
To our knowledge, Intellige is the ﬁrst user-facing model
explainer employed in industry that provides a user-friendly
interface to conduct model interpretation and narrative cus-

Table 1. Model prediction & interpretation result (left panel) and narrative insights (right panel).
Model Prediction & Interpretation (Non-Intuitive)
Narrative Insights (User-Friendly)
Propensity score: 0.85 (Top 2%)
Top important features (with importance score):
• paid job s4: 0.030
• job view s4: 0.013
• hire cntr s3: 0.011
• conn cmp s4: 0.009
• · · ·
This account is extremely likely to upsell. Its upsell like-
lihood is larger than 98% of all accounts, which is driven
by:
• Paid job posts changed from 10 to 15 (+50%) in the
last month.
• Views per job changed from 200 to 300 (+50%) in the
last month.
• · · ·
tomization. Besides sales and marketing, Intellige is appli-
cable in a variety of use cases in industry. For example, in
healthcare, Intellige can help identify top signals in disease
risk prediction, and then translate them into clinical reports
which are digestible to clinicians. In credit card applica-
tions, narratives containing main reasons why applicants get
rejected can be generated by Intellige from credit risk pre-
diction models, and then surfaced to credit card applicants
for their reference.
The rest of the paper is organized as follows. Section 2 lists
related work in the area of model interpretation and narrative
generation; Section 3 describes the design of Intellige; Sec-
tion 4 presents use cases of Intellige at LinkedIn, including
performance evaluation results; Section 5 points out some
limitations of Intellige and discusses future directions; and
Section 6 concludes our work.
2. Related Work
Model interpretation approaches that focus on sample-level
feature reasoning have been widely explored in recent years.
Examples include Shapley Value [7][8], Local Gradient
[9], Integrated Gradient [10], Quantitative Input Inﬂuence
(QII) [11], Leave-One-Covariate-Out (LOCO) [12], LIME
[3], KernelSHAP [4], and TreeSHAP [5]. Moreover, many
model interpretation platforms have also been developed
to facilitate the implementation of these approaches in a
uniﬁed way, e.g., Microsoft InterpretML [13] and Machine
Learning Interpretability (MLI) in H2O Driverless AI [14].
All these model interpretation approaches and platforms can
easily suffer from one challenge when interpretation results
are presented to end users: feature importance scores in
tabular/bar-chart format may not be very intuitive, resulting
in low adoption rates.
To overcome this limitation, user-digestible narrative-based
model interpretations have been proposed [15][16]. Two
common approaches for such interpretations are generation-
based and template-based. Examples of neural network
generation-based approaches include synthesizing explana-
tions triggered by the word “because” [17][18], leveraging
LSTM to generate explanation sentences [19], creating tips
for Yelp restaurants based on GRU [20], and developing a
multi-task recommendation model which performs rating
prediction and recommendation explanation simultaneously
[21]. However, generation-based approaches highly depend
on the quality and quantity of training data, thus are less
generalizable than template-based approaches.
Recent work on creating narrative explanations via template-
based approaches includes imputing the predeﬁned narra-
tive templates with the most important features to explain
the recommendation models [22][23][24]. In [25], a Java
package provides narrative justiﬁcations for logistic/linear
regression models, [26] propose a way to generate narra-
tive explanations using logical knowledge translated from
a decision tree model, and [27] introduce a rule-based ex-
plainer for a GDPR automated decision which applies to
explainable models. However, all these aforementioned
templated-based approaches are only applicable to a subset
of machine learning models, and can easily fail when facing
a more complex model such as a random forest. More-
over, the templates used in these approaches are predeﬁned,
with limited variations, and as a result, the generated nar-
ratives can become repetitive and hard to customize. In
Intellige, we overcome these limitations by implementing
model-agnostic interpretation approaches which apply to
arbitrary predictive machine learning models, and by pro-
viding a user-friendly interface that allows customizing an
unlimited number of narrative templates.
3. Intellige Design
3.1. Overview
We propose Intellige as a self-service platform for user-
facing explanation. Intellige demystiﬁes the outputs of pre-

dictive models by assigning feature importance scores, and
converts non-intuitive model predictions and top important
features into user-understandable narratives. This enables
end users to obtain insights into model predictions, and to
build trust in model recommendations.
Intellige is designed to support all the commonly-used black-
box supervised machine learning models, including but not
limited to support vector machines, bagging, random forests,
gradient boosted trees, and deep neural networks.
Several challenges existed in the design and deployment of
Intellige:
1. How to consume outputs from a range of machine
learning platforms implementing machine learning
models?
2. How to enable ﬂexibility in choosing model interpreta-
tion approaches for different use cases?
3. How to efﬁciently generate template-based narratives
while allowing narrative customization?
4. How to produce narratives compatible with a range of
end user platforms?
To address the above challenges, we designed Intellige as
a ﬂexible platform consisting of four components: Model
Importer, Model Interpreter, Narrative Generator and Nar-
rative Exporter. These four components resolved the above
challenges in sequential order. Figure 1 shows these four
components:
1. Model Importer: Consumes model output from ma-
jor machine learning platforms and transforms it into
standardized machine learning model output.
2. Model Interpreter: Implements a collection of model
interpretation approaches to process the standardized
machine learning model output, and produces sample-
level top important feature lists.
3. Narrative Generator: Creates user-digestible narra-
tives via a template-based approach, based on the stan-
dardized machine learning model output, and addi-
tional feature information and narrative templates pro-
vided by Intellige users; Selects top narratives by using
top important feature list for each sample.
4. Narrative Exporter: Surfaces sample-level top nar-
ratives onto major end user platforms with necessary
format adjustments.
In the following sections, we introduce these four compo-
nents in more detail.
3.2. Model Importer
As we move toward machine learning platforms such as
ProML from LinkedIn,1 AutoML from Google,2 and Create
ML from Apple,3 it is very likely that different platforms
produce model outputs in very different formats, resulting in
low efﬁciency of developing model explainers speciﬁcally
for each platform. A natural resolution is to ﬁrst convert
these model outputs into standardized format. This leads to
the development of Model Importer.
The Model Importer takes the model output from a set of
machine learning platforms as its input, and produces stan-
dardized machine learning model output, which will be used
in the following Model Interpreter and Narrative Generator.
For use cases at LinkedIn, the set of machine learning plat-
forms includes ProML and other internal platforms built by
data science teams. A typical standardized machine learn-
ing model output consists of feature vectors and prediction
scores of all the samples, and optionally the predictive model
itself with a uniﬁed interface (i.e., the interface should take
standardized input (feature vectors) and produce standard-
ized output (prediction scores)). We set the predictive model
to be optional, since the following Model Interpreter can
sometimes work well even without access to the original
model, and the Narrative Generator does not depend on the
original model.
3.3. Model Interpreter
Model Interpreter is the second component of Intellige, aim-
ing to reveal insights behind machine learning model rec-
ommendations. It takes the output of Model Importer as its
input, and produces sample-level top important feature lists
by calculating feature importance scores for each sample,
which are then conveyed to the Narrative Generator as one
of its inputs.
The Model Interpreter consists of a collection of model
interpretation approaches with a uniﬁed input format (i.e.,
standardized machine learning model output) and a uni-
ﬁed output format (i.e., sample-level top important feature
lists). The collection of model interpretation approaches
includes state-of-the-art methods that produce sample-level
feature importance scores, e.g., LIME [3], KernelSHAP &
DeepSHAP [4], and TreeSHAP [5]. The Model Interpreter
need not have the model itself accessible – the model may be
trained in a separate system/device or there exist privacy/se-
curity concerns. If that is the case, high-performing model
interpretation approaches, such as K-LIME, are available
[14].
1https://engineering.linkedin.com/blog/2019/01/scaling-
machine-learning-productivity-at-linkedin
2https://cloud.google.com/automl
3https://developer.apple.com/machine-learning/create-ml

Figure 1. Intellige Components.
Intellige users have the ﬂexibility to choose the appropriate
interpretation approach in their use cases: For example, if
the input machine learning platform implements a restricted
set of machine learning algorithms (e.g., only tree-based
algorithms or neural-network-based algorithms), then In-
tellige users can choose model-speciﬁc interpretation ap-
proaches such as TreeSHAP and DeepSHAP as they are
usually more computationally-efﬁcient than model-agnostic
ones; On the contrary, if the original platform keeps a large
set of candidate algorithms, then model-agnostic interpreta-
tion approaches such as LIME and KernelSHAP are recom-
mended.
3.4. Narrative Generator
Narrative Generator is the key innovative part of Intellige, as
it generates human-understandable narratives for interpret-
ing model predictions. Its input consists of the outputs from
Model Importer and Model Interpreter, as well as Insights
Design, which in turn consists of a Feature Info File and
Narrative Templates (shown in Figure 1, details provided in
Section 3.4.1-3.4.4). The design of Narrative Generator is
challenging: Information solely from the model itself such
as feature name, feature value, and feature importance score
may not be comprehensive enough for narrative construc-
tion. Additional information such as feature descriptions
and narrative templates are needed as well.
To address the above challenge, we propose the Narrative
Generator, with the goal of minimizing the human effort in
narrative generation while keeping the ﬂexibility in narrative
customization. Many heavy tasks, such as feature value
extraction, template imputation, and narrative ranking are
handled inside the Narrative Generator in an automated way.
To enable narrative customization, we introduce Insights
Design, an additional input to Narrative Generator provided
by Intellige users. Insights Design contains information
from domain knowledge owners which cannot be directly
extracted from the model itself, but is essential to narrative
construction. Insights Design has two components: Fea-
ture Info File and Narrative Templates. The Feature Info
File contains additional information for each model feature,
including feature hierarchy information, detailed feature
descriptions, and narrative template imputation rules. The
Narrative Templates ﬁle contains a collection of templates
for imputing appropriate feature values.
In the following sections, we describe key features of Narra-
tive Generator in detail.
3.4.1. FOUR-LAYER FEATURE HIERARCHY
An intuitive way to understand feature meaning and the in-
trinsic relationship between different features is to construct
a hierarchical structure for these features. In Intellige, we
propose a four-layer feature hierarchy, where the original
features are set to be the ﬁrst layer. This feature hierarchy is
manually constructed with the help of model owners, and is
speciﬁed by Feature Info File in Insights Design. In prac-
tice, feature correlation analysis can also help in ﬁnding
the appropriate hierarchical structure by providing feature
grouping information.
Table 2 shows a sample feature hierarchy for 8 selected
features from the jobs upsell model introduced in Section 1.
There are four hierarchical layers for features in Table 2:
• Original-feature layer (1st layer): This layer con-
tains all the original features used in the model.

Table 2. Feature hierarchy for selected features from jobs upsell model.
Original-Feature
Super-Feature
Ultra-Feature
Category
job qty
job slots
job slots
product booking
job dprice usd
job slots
job slots
product booking
job view s3
views per job
job view
product performance
job view s4
views per job
job view
product performance
job viewer s3
viewers per job
job view
product performance
job viewer s4
viewers per job
job view
product performance
job applicant s3
applicants per job
job applicant
product performance
job applicant s4
applicants per job
job applicant
product performance
• Super-feature layer (2nd layer): This layer is used
to group closely related original-features for narra-
tive construction. For each super-feature, one nar-
rative will be constructed, and every original-feature
under this super-feature will have the opportunity to
appear in this narrative. Another important function-
ality of this layer is to conduct feature name explana-
tion. As we can see from Table 2, the super-feature
names are much more understandable than the original-
feature names, and they themselves are likely to ap-
pear in the narrative as well. For example, one narra-
tive will be constructed for the super-feature views
per job, where the values of the original-features
job view s3 and job view s4 as well as the su-
per feature name views per job are likely to be
incorporated into the narrative. We discuss the details
of how to construct narratives in Section 3.4.2.
• Ultra-feature layer (3rd layer): This layer is used
to identify and ﬁlter out super-features with (almost)
duplicated information to end users. For example,
in Table 2, both super-features views per job
and viewers per job are under ultra-feature job
view as they are very likely to contain overlapped in-
formation. If we show one narrative of views per
job to end users, then the marginal beneﬁt to show the
other narrative of viewers per job will be low.
We discuss how to ﬁlter out super-features under one
ultra-feature in Section 3.4.3.
• Category layer (4th layer): This layer is used to
group relevant narratives and concatenate them into
paragraphs.
For example, in Table 2, both ultra-
features job view and job applicant belong
to category product performance, and their cor-
responding narratives can then be concatenated into
one paragraph by using conjunction phrases. We dis-
cuss the details of narrative concatenation in Section
3.4.4.
We note that when the Narrative Generator was initially
designed, it only consumed the original-feature and the
super-feature layers as its input, as these two layers seemed
necessary and sufﬁcient in narrative generation. However,
as more and more requests for narrative deduplication and
narrative concatenation came in from Intellige users, we
decided to add the ultra-feature and the category layers,
in order to realize these two additional functionalities. In
practice, Intellige users can choose whether to specify the
ultra-feature and category layers based on their own use
cases. For example, specifying the ultra-feature layer is
recommended if the generated narratives contain too much
redundant information, and specifying the category layer
is recommended if end users prefer reading paragraphs. If
Intellige users ﬁnd it unnecessary to specify either of these
two layers, they can simply set them the same as the super-
feature layer to reduce preparation effort.
3.4.2. NARRATIVE TEMPLATE IMPUTATION
An important prerequisite for narrative construction is build-
ing the narrative templates in Insights Design. Narrative
templates are manually constructed with the help of model
owners, and then translated into appropriate code for imput-
ing feature values in Narrative Generator. An example of
how to translate narrative templates into code can be found
in Section A.1 in the Appendix.
Table 3 shows sample narrative templates for the jobs upsell
use case. Here, we refer to value change as an “insight
type”:
Each super-feature corresponds to one insight
type, which determines the speciﬁc narrative template to
use. prev value and current value in the template
value change are “insight items”: each original-feature
under one super-feature corresponds to one insight item,
which determines the position to impute the original-feature
value into the narrative template. For example, as shown
in Table 4, the original-features job view s3 and
job view s4 under the super-feature views per
job correspond to insight items prev value and
current value respectively, where prev value and
current value can be identiﬁed as two positions in the
template value change in Table 3. percent change
is an example of an “extra insight item” whose value

may not be directly extracted from original-features
but can be derived by extra calculations on the existing
insight items. For example, here percent change =
(current value-prev value)/prev value*100.
We note that the appearance of the original-features within
a given narrative is solely determined by the design of the
narrative template, rather than the importance scores of the
original-features – we discuss the usage of these importance
scores in Section 3.4.3. We also mention that, by default,
all the original-features under one super-feature will appear
in its corresponding narrative.
The introduction of “insight type” and “insight item” en-
ables the reusability of narrative templates. For example,
both super-features views per job and applicants
per job share the same insight type, and thus their narra-
tives are constructed based on the same template. Moreover,
“insight item” enables the construction of sample-speciﬁc
narratives, as each sample has its own feature values to be
imputed.
In addition to Narrative Templates, we are now able to
specify the complete version of Feature Info File within
Insights Design: The Feature Info File consists of all the
columns in Table 2 and 4: Original-Feature, Super-Feature,
Ultra-Feature, Category, Insight Type, and Insight Item,
which contains essential feature information for narrative
construction. Three additional columns: Insight Threshold,
Insight Weight, and Source, can also be incorporated as
optional columns to make the narrative generation process
more customizable. The detailed introduction of these three
columns can be found in Section A.2 in the Appendix.
3.4.3. NARRATIVE RANKING
By introducing the feature info ﬁle and narrative templates,
we are able to construct a collection of narratives for each
sample. However, too many narratives may overwhelm end
users, so instead we aim to present them with a few selected
ones which show the strongest signals to support the model
recommendations.
To select the most important narratives in a scalable way, we
leverage the sample-level top important feature list created
from Model Interpreter to rank all the narratives, and then
present the end users with the top ones. To this end, we
introduce the narrative importance score, which reﬂects
how large the contribution of each narrative is to the model
prediction. We set the score to be the maximum importance
score among all the original-features under the super-feature
corresponding to the narrative. The intuition behind this
setting is that one narrative is important as long as at least
one of its corresponding original-features is important.
We conduct narrative deduplication by retaining only the top
K narratives with the largest narrative importance scores
under each ultra-feature (Recall that the ultra-feature is in a
higher hierarchy of the super-feature, which groups those
super-features sharing the overlapped information). We can
set K = 1 to make the generated narratives most concise.
One example of conducting narrative ranking and dedupli-
cation in jobs upsell use case can be found in Section A.3 in
the Appendix (Table 8).
Finally, we point out that narrative ranking has inherited a
good property from feature ranking in Model Interpreter:
narrative ranking is sample-speciﬁc. A narrative with the
same content, e.g., views per job, can be ranked as No.
1 for customer A but No. 5 for customer B, indicating its
different contributions in supporting the recommendations
for different customers.
3.4.4. NARRATIVE CONCATENATION
Narrative concatenation is enabled by the category layer of
the four-layer feature hierarchy, where relevant narratives
under the same category can be concatenated as a paragraph
rather than a bullet-point list. The major goal is to make the
narratives better-organized so that the narratives focusing
on different aspects of the sample will not be mixed. In
Intellige, narrative concatenation is optional.
We conduct narrative concatenation by using conjunction
phrases such as “and”, “moreover” and “what’s more”. For
example, for narratives corresponding to super-features
views per job and applicants per job, the
paragraph after narrative concatenation is “Views per job
changed ..., and applicants per job changed ...”. We also
introduce paragraph importance score to rank these para-
graphs. Similar to narrative importance score, the paragraph
importance score is determined as the largest narrative im-
portance score among all the narratives incorporated in the
paragraph.
3.4.5. NARRATIVE GENERATOR DESIGN
We now describe the design of Narrative Generator. Figure
2 shows the six major steps in Narrative Generator:
I Construct super-feature mapping based on feature info
ﬁle and feature vectors: For each super-feature, this
mapping records its corresponding original-feature ids
(i.e., positions in feature vector), ultra-feature, category,
insight type and insight items.
II Collect information of all super-features for each sam-
ple: For each super-feature in one sample, we extract
its corresponding original-feature values from feature
vectors according to super-feature mapping from Step
I.
III Obtain top super-feature list for each sample: Based on
sample-level top feature lists from Model Interpreter

Table 3. Narrative templates for interpreting jobs upsell model.
Insight Type
Narrative Template
quantity
Purchased
{quantity num}
{super name} for
${total price}.
value change
{super name} changed from {prev value} to
{current value} ({percent change}%) in the last
month.
Table 4. Insight type and insight item for selected features from jobs upsell model.
Original-Feature
Super-Feature
Insight Type
Insight Item
job qty
job slots
quantity
quantity num
job dprice usd
job slots
quantity
total price
job view s3
views per job
value change
prev value
job view s4
views per job
value change
current value
job viewer s3
viewers per job
value change
prev value
job viewer s4
viewers per job
value change
current value
job applicant s3
applicants per job
value change
prev value
job applicant s4
applicants per job
value change
current value
and super-feature mapping from Step I, we rank each
sample’s top super-features by calculating narrative im-
portance scores, and then use ultra-features to conduct
deduplication (Section 3.4.3).
IV Obtain information of top super-features for each sam-
ple: For each sample, we join the information of all
super-features from Step II onto the top super-feature
list from Step III.
V Construct top narratives for each sample: For each
sample, we conduct narrative template imputation for
each top super-feature (Section 3.4.2).
VI (Optional) Construct top paragraphs for each sample:
For each sample, we conduct narrative concatenation
according to category name (Section 3.4.4).
3.5. Narrative Exporter
The generation of user-digestible narratives may not be
the last step of a user-facing model explainer, instead the
narratives should be further surfaced to various end user
platforms such as sales/marketing intelligence platforms,
Tableau dashboards and emails. Our solution is to incorpo-
rate an extra step called Narrative Exporter after the Nar-
rative Generator, to unify the narrative surfacing process.
Speciﬁcally, Narrative Exporter takes top narratives from
Narrative Generator as its input, and converts them into a
few speciﬁc formats of choice, such as html or email format.
This step completes the end-to-end pipeline from machine
learning platforms to end user platforms in Intellige.
4. Use Cases at LinkedIn
LinkedIn leverages data to empower decision making in
every area. One such area is sales, where data scientists
built predictive machine learning models for account rec-
ommendation, covering the entire business landscape from
customer acquisition to existing customer retention. Most
of these predictive models are black-box models, making it
challenging for data scientists to surface model outputs to
sales teams in an intuitive way.
Furthermore, LinkedIn sales teams use multiple internal
intelligence platforms. One typical platform, Merlin, aims
to help sales representatives close deals faster by providing
personalized and actionable sales recommendations/alerts.
Before Intellige, all these sales recommendations were rule-
based. A typical example of rule-based recommendations
is based on exploratory data analysis: Recommend the jobs
upsell opportunity if views per job increased more than 10%,
or the number of job posts increased more than 20% in the
past month. As we can see, these rule-based recommenda-
tions were neither very accurate as model predictions nor
scalable in their generation process.
Intellige has assisted LinkedIn data scientists in converting
machine intelligence from business predictive models into
sales recommendations on platforms such as Merlin, where
LinkedIn data scientists are typically both model owners
and Intellige users, while sales teams are the end users
applying Intellige’s narrative insights to their work. One
typical example of Intellige-based sales recommendations
on Merlin is with jobs upsell alerts. As introduced in Section
1, the jobs upsell model predicts how likely each account is
to purchase more job slots.

Figure 2. Narrative Generator - Major Steps.
Figure 3 shows how the jobs upsell alerts appear on Mer-
lin. When a sales representative logs into Merlin, a list
of account alerts including jobs upsell alerts are displayed
on the Merlin homepage (Figure 3(a)). On the summary
page of the account, we see a sentence describing its
propensity score.
To learn more about the underlying
reasons behind its recommendation, sales representatives
can click the “Job Slots Upsell” button which will direct
them to the account status page with more account de-
tails (Figure 3(b)).
In the Account Status section, top
narrative insights are listed, e.g., both viewers per
job and distinct countries that job posts
seek talents from largely increased in the last
month, which serve as strong signals of upsell propensity.
Besides Merlin, Intellige-based sales recommendations have
also been surfaced onto other sales platforms for different
audiences and use cases with the help of Narrative Exporter.
By the end of 2020, six Intellige-based sales recommenda-
tions across four lines of LinkedIn business - Talent Solu-
tions (LTS), Marketing Solutions (LMS), Sales Solutions
(LSS) and Learning Solutions (LLS) have been on-boarded
onto four internal sales intelligence platforms, which have
been surfaced to more than ﬁve thousand sales representa-
tives overseeing more than three million accounts.
4.1. Evaluation Results
To understand how helpful Intellige-based sales recommen-
dations are to sales representatives, we turned to qualitative
and quantitative evaluations of Intellige performance.
In our qualitative evaluation, we collected feedback from
sales representatives via questionnaires, interviews, and
other feedback channels. Similar approaches have been
proposed in [15] and [27], where the authors argued that
“subjective satisfaction is the only reasonable metrics to
evaluate success in explanation”. We have conducted a sur-
vey within a small group of sales representatives on the
helpfulness of Intellige-based sales recommendations (rat-
ings from 1 - not helpful at all to 5 - couldn’t do my job
without them). Ten responses have been received with aver-
age satisfaction rating of 3.5 (standard error 0.4). We have
also collected positive feedback from a broader group of
sales representatives, which we summarized into three main
points (A collection of feedback in the original can be found
in Section A.4 in the Appendix):
1. Top narrative insights are clear to understand and ef-
fectively help sales representatives build trust in the
account recommendations. These narrative insights
bring important metrics to their attention, and prompt
them to work on accounts that they may have not con-
sidered otherwise.
2. Intellige-based sales recommendations serve as a com-
prehensive information center. Sales representatives
appreciate that the top narrative insights are consoli-
dated all in one place, to save their time of gathering
information from different sources.
3. Intellige-based sales recommendations provide a di-
rectional guidance for next steps. The top narrative
insights allow sales representatives to act strategically,
e.g., prepare customer-speciﬁc conversations.
Another way to evaluate the performance of Intellige-based
sales recommendations is via quantitative evaluation, which
we conducted in two phases:

(a)
(b)
Figure 3. Jobs Upsell Alerts on Merlin.
1. Phase I: Compare the adoption rate between Intellige-
based recommendations and rule-based recommenda-
tions. Table 5 shows the interaction rates on all the
Intellige-based and rule-based Merlin Alerts across
sales representatives in LTS and LLS respectively in
the same 3-month time period, where the interaction
rate is deﬁned as # clicks / # impressions. Intellige-
based alerts have a signiﬁcantly higher interaction rate
than rule-based alerts, indicating that sales representa-
tives are more engaged with Intellige-based alerts. We
note that potential confounding factors in this compari-
son may exist, e.g., the novelty of new alerts may lead
to increased interactions. To address this novelty effect,
we started our measurements of # impressions and #
clicks one month after the new alerts launch date, in the
hope that most of the sales representatives have already
been familiarized with them. We have also extended
the time period of measurements to three months to
further reduce this potential novelty effect.
2. Phase II: Identify the differences with/without Intellige-
based recommendations via A/B testing. In the A/B
testing design, for each sales representative, we ran-
domly split his/her account book into treatment/control
groups, and we show the Intellige-based recommenda-
tions to all the eligible accounts in the treatment group
only. We then compare key metrics between the treat-
ment and control groups, e.g., upsell rate and revenue
for upsell recommendations (upsell rate = # success-
ful upsell opportunities / # sales opportunites created),
and churn rate and revenue lost for churn risk notiﬁ-
cations. Table 6 shows the A/B testing results of jobs
upsell alerts and recruiters upsell alerts after a 3-month
testing period, where “recruiters” is another LinkedIn
product. We observe boosts in both key metrics: up-
sell rate and average spend per account, indicating that
the Intellige-based sales recommendations work effec-
tively in driving the right sales decisions and bringing
in revenue to the company. We note that the A/B test-
ing period of recruiters upsell alerts was during the
COVID-19 crisis, where severe hiring freezes likely
negatively impacted the market of recruiters products
[28], leading to lower than expected numbers of cre-
ated opportunities and upsell opportunities in Table
6. As a result, the corresponding upsell rate lift of
recruiters product is not statistically signiﬁcant, and its
average spend per account possesses a relatively large
standard error.
4.2. Lessons from Deployment
We list key lessons we have learned from the deployment of
Intellige at LinkedIn:
1. Initial feedback we received after the launch of our
ﬁrst-ever Intellige-based sales recommendations - jobs
upsell alerts, was from sales representatives who found
some narratives confusing. For example, one narrative
was “Distinct countries of job posts changed value”,
which was identiﬁed as a vague statement: Did this
mean “distinct countries where job post viewers come
from” or “distinct countries where job posts seek tal-
ents from”?
To resolve the above issue, we worked with data scien-
tists who built sales predictive models to host multiple
working sessions with sales representatives. During
working sessions, we walked through the top narrative
insights with sales representatives, asked them about
meaningfulness of feature descriptions and whether
more granular information was needed, and then re-
vised the feature descriptions and narrative templates
in Insights Design accordingly. For example, when we
identiﬁed which description was correct for “distinct
countries of job posts”, we updated the correspond-
ing super-feature in Insights Design, which solved the
above issue. Overall, we found it useful to iterate
through several rounds of improvements and feedback
from our end users.

Table 5. Interaction rate (# clicks / # impressions) of Merlin Alerts, Intellige-based vs rule-based (standard error in parathesis).
LLS
LTS
Intellige-based Alerts
# impressions
694
7,188
# clicks
41
167
interaction rate (s.e.)
5.9% (0.9%)
2.3% (0.2%)
Rule-based Alerts
# impressions
1,031
5,445
# clicks
25
91
interaction rate (s.e.)
2.4% (0.5%)
1.7% (0.2%)
Interaction Rate Lift
lift (%)
+141%
+39%
lift (p-value)
<0.001***
0.012**
Table 6. A/B testing results of jobs upsell alerts and recruiters upsell alerts (upsell rate = # upsell opportunities / # opportunities created)
(standard error in parathesis).
Alerts Type
Treatment/
# Opportunities
# Upsell
Upsell
Avg Spend
Control
Created
Opportunities
Rate (s.e.)
Per Account (s.e.)
Jobs Upsell
With Alerts
1259
214
17.0% (1.1%)
$65,733 ($4,703)
Alerts
No Alerts
422
59
14.0% (1.7%)
$32,626 ($4,500)
Lift (%)
-
-
+21%
+101%
Lift (p-value)
-
-
0.084*
0.074*
Recruiters
With Alerts
115
11
9.6% (2.7%)
$25,863 ($14,209)
Upsell Alerts
No Alerts
118
7
5.9% (2.2%)
$2,908 ($1,236)
Lift (%)
-
-
+63%
+789%
Lift (p-value)
-
-
0.214
0.071*
*signiﬁcance codes: 0 ‘***’ 0.01 ‘**’ 0.05 ‘*’ 0.1 ‘’ 1
**treatment/control split: 70/30 in jobs upsell alerts, 50/50 in recruiters upsell alerts
2. Making the narrative templates reusable is strongly rec-
ommended to Intellige users, in order to save efforts
in template construction and maintenance. A certain
group of super-features can be narrated by one tem-
plate, while others cannot. For example, the template
value change in Table 3 can only be applied to a
super-feature with its original-feature pair in chronolog-
ical order, such as job view s3 and job view s4
in Table 4 (s3 and s4 stand for “two months ago” and
“last month” respectively). However, this template can-
not be used for the original-feature pair job qty and
job dprice usd, and instead we need to design a
new template for it, such as template quantity in
Table 3.
Therefore, when constructing narrative templates, we
recommend that Intellige users start with templates
that accommodate as many of the super-features as
possible, and then make new templates as needed until
the rest of the super-features are covered. We found
that if a narrative template is generalizable enough, it
can also be used in other model interpretation use cases
as well. In the Merlin use case, for example, over 80%
of narrative templates used in jobs upsell alerts were
reused in recruiters upsell alerts, which shortened the
preparation time of recruiters upsell alerts by more than
half. Frequently used narrative templates in Intellige
can be stored for potential future usage.
5. Limitations and Future Work
Here, we list several limitations of Intellige and discuss
future work:
• Intellige only supports supervised machine learning
models whose input features are in tabular data format.
Future work will aim to extend Intellige to support a
broader range of supervised learning models such as
image classiﬁcation and natural language processing
models, as well as other types of models including
unsupervised learning, semi-supervised learning and
time series models.
• The Insights Design input, including the feature info
ﬁle and narrative templates, is mostly manually created.
We plan to investigate ways to auto-generate parts of
Insights Design to further reduce manual efforts from
Intellige users.
• Translation of narrative templates into code is manually
conducted. As future work, we will try to automate this
process by identifying symbols and characters in nar-
rative templates and converting them into appropriate
code automatically.

6. Conclusion
In recent years, requests from end users of predictive models
of understandable model outputs have become widespread,
motivating the development of user-facing model explainers.
In this paper, we proposed Intellige, a novel user-facing
model interpretation and narrative generation tool, which
produces user-digestible narrative insights and reveals the
rationale behind predictive models. The evaluation results
in LinkedIn’s use cases demonstrate that the narrative in-
sights produced by Intellige boost the adoption rate of model
recommendations and improve key metrics such as revenue.
Acknowledgements
We would like to express our special thanks to our col-
leagues at LinkedIn that put this work together, including
Saad Eddin Al Orjany, Harry Shah, Yu Liu, Fangfang Tan,
Jiang Zhu, Jimmy Wong, Jessica Li, Jiaxing Huang, Kunal
Chopra, Durgam Vahia, Suvendu Jena, Ying Zhou, Rodrigo
Aramayo, William Ernster, Eric Anderson, Nisha Rao, An-
gel Tramontin, Zean Ng, Ishita Shah, Juanyan Li, Rachit
Arora, Tiger Zhang, Wei Di, Sean Huang, Burcu Baran,
Yingxi Yu, Sofus Macskassy, Rahul Todkar and Ya Xu. We
particularly thank Justin Dyer, Ryan Rogers and Adrian
Rivera Cardoso for their helpful comments and feedback.
References
[1] D. Martens and F. Provost, “Explaining data-driven
document classiﬁcations,” Mis Quarterly, vol. 38,
no. 1, pp. 73–100, 2014.
[2] J. Moeyersoms, B. d’Alessandro, F. Provost, and
D. Martens, “Explaining classiﬁcation models built
on high-dimensional sparse data,” arXiv preprint
arXiv:1607.06280, 2016.
[3] M. T. Ribeiro, S. Singh, and C. Guestrin, “Why should
i trust you?: Explaining the predictions of any clas-
siﬁer,” in Proceedings of the 22nd ACM SIGKDD In-
ternational Conference on Knowledge Discovery and
Data Mining, pp. 1135–1144, ACM, 2016.
[4] S. M. Lundberg and S.-I. Lee, “A uniﬁed approach to
interpreting model predictions,” in Advances in neural
information processing systems, pp. 4765–4774, 2017.
[5] S. M. Lundberg, G. Erion, H. Chen, A. DeGrave, J. M.
Prutkin, B. Nair, R. Katz, J. Himmelfarb, N. Bansal,
and S.-I. Lee, “From local explanations to global un-
derstanding with explainable ai for trees,” Nature ma-
chine intelligence, vol. 2, no. 1, pp. 2522–5839, 2020.
[6] Y. Zhang and X. Chen, “Explainable recommenda-
tion: A survey and new perspectives,” arXiv preprint
arXiv:1804.11192, 2018.
[7] E. Strumbelj and I. Kononenko, “An efﬁcient explana-
tion of individual classiﬁcations using game theory,”
The Journal of Machine Learning Research, vol. 11,
pp. 1–18, 2010.
[8] E. ˇStrumbelj and I. Kononenko, “Explaining predic-
tion models and individual predictions with feature
contributions,” Knowledge and information systems,
vol. 41, no. 3, pp. 647–665, 2014.
[9] D. Baehrens, T. Schroeter, S. Harmeling, M. Kawan-
abe, K. Hansen, and K.-R. M¨uller, “How to explain
individual classiﬁcation decisions,” The Journal of
Machine Learning Research, vol. 11, pp. 1803–1831,
2010.
[10] M. Sundararajan,
A. Taly,
and Q. Yan,
“Ax-
iomatic attribution for deep networks,” arXiv preprint
arXiv:1703.01365, 2017.
[11] A. Datta, S. Sen, and Y. Zick, “Algorithmic trans-
parency via quantitative input inﬂuence: Theory and
experiments with learning systems,” in 2016 IEEE
symposium on security and privacy (SP), pp. 598–617,
IEEE, 2016.

[12] J. Lei, M. G’Sell, A. Rinaldo, R. J. Tibshirani, and
L. Wasserman, “Distribution-free predictive inference
for regression,” Journal of the American Statistical
Association, vol. 113, no. 523, pp. 1094–1111, 2018.
[13] H. Nori, S. Jenkins, P. Koch, and R. Caruana, “In-
terpretml: A uniﬁed framework for machine learn-
ing interpretability,” arXiv preprint arXiv:1909.09223,
2019.
[14] P. Hall, An introduction to machine learning inter-
pretability. O’Reilly Media, Incorporated, 2019.
[15] E. Reiter, “Natural language generation challenges
for explainable ai,” arXiv preprint arXiv:1911.08794,
2019.
[16] I. Baaj, J.-P. Poli, and W. Ouerdane, “Some insights
towards a uniﬁed semantic representation of expla-
nation for explainable artiﬁcial intelligence,” in Pro-
ceedings of the 1st Workshop on Interactive Natural
Language Technology for Explainable Artiﬁcial Intel-
ligence (NL4XAI 2019), pp. 14–19, 2019.
[17] S. Antol, A. Agrawal, J. Lu, M. Mitchell, D. Batra,
C. Lawrence Zitnick, and D. Parikh, “Vqa: Visual
question answering,” in Proceedings of the IEEE in-
ternational conference on computer vision, pp. 2425–
2433, 2015.
[18] L. A. Hendricks, Z. Akata, M. Rohrbach, J. Donahue,
B. Schiele, and T. Darrell, “Generating visual explana-
tions,” in European Conference on Computer Vision,
pp. 3–19, Springer, 2016.
[19] F. Costa, S. Ouyang, P. Dolog, and A. Lawlor, “Auto-
matic generation of natural language explanations,” in
Proceedings of the 23rd International Conference on
Intelligent User Interfaces Companion, pp. 1–2, 2018.
[20] P. Li, Z. Wang, Z. Ren, L. Bing, and W. Lam, “Neu-
ral rating regression with abstractive tips generation
for recommendation,” in Proceedings of the 40th In-
ternational ACM SIGIR conference on Research and
Development in Information Retrieval, pp. 345–354,
2017.
[21] Y. Lu, R. Dong, and B. Smyth, “Why i like it: multi-
task learning for recommendation and explanation,” in
Proceedings of the 12th ACM Conference on Recom-
mender Systems, pp. 4–12, 2018.
[22] Y. Zhang, G. Lai, M. Zhang, Y. Zhang, Y. Liu, and
S. Ma, “Explicit factor models for explainable recom-
mendation based on phrase-level sentiment analysis,”
in Proceedings of the 37th international ACM SIGIR
conference on Research & development in information
retrieval, pp. 83–92, 2014.
[23] N. Wang, H. Wang, Y. Jia, and Y. Yin, “Explainable
recommendation via multi-task learning in opinion-
ated text data,” in The 41st International ACM SIGIR
Conference on Research & Development in Informa-
tion Retrieval, pp. 165–174, 2018.
[24] Y. Tao, Y. Jia, N. Wang, and H. Wang, “The fact:
Taming latent factor models for explainability with
factorization trees,” in Proceedings of the 42nd In-
ternational ACM SIGIR Conference on Research and
Development in Information Retrieval, pp. 295–304,
2019.
[25] O. Biran and K. McKeown, “Justiﬁcation narratives
for individual classiﬁcations,” in Proceedings of the
AutoML workshop at ICML, vol. 2014, pp. 1–7, 2014.
[26] R. Calegari, G. Ciatto, J. Dellaluce, and A. Omicini,
“Interpretable narrative explanation for ml predictors
with lp: A case study for xai.,” in WOA, pp. 105–112,
2019.
[27] F. Sovrano, F. Vitali, and M. Palmirani, “The differ-
ence between explainable and explaining: require-
ments and challenges under the gdpr,” 2019.
[28] K. Kimbrough, “Measuring the ﬁrst full month of
covid-19’s impact on hiring in the u.s..” https:
//www.linkedin.com/pulse/measuring
-first-full-month-covid-19s-impact
-hiring-us-karin-kimbrough, 2020.

A. Appendix
A.1. Translate Narrative Templates into Code
We use the narrative template value change as an exam-
ple to show how we can translate it into Scala code. This
example can be easily generalized to other narrative tem-
plates and programming languages. Just to recap, the narra-
tive template value change is: “{super name} changed
from {prev value} to {current value} ({percent change}%)
in the last month”.
To calculate the extra insight item percent change,
we can ﬁrst build a helper function changePercent in
Scala:
1
def changePercent(current_value: Double,
previous_value: Double): String = {
2
val change_percent =
3
(current_value, previous_value) match {
4
case (0, 0) => 0.0
5
case (a, b) => (a / b - 1) * 100
6
}
7
change_percent match {
8
case x if x.isInfinity => " "
9
case x if x >= 0 => " (+" ++
change_percent ++ "%) "
10
case _ => " (" ++ change_percent ++ "
%) "
11
}
12
}
This helper function is used to ﬁrst calculate the percent
change from prev value to current value as
change percent, and then convert change percent
into
a
more
user-friendly
format:
Empty
if
change percent is inﬁnite (e.g., changed from 0 to 4),
(+X%) if change percent is positive (e.g., changed
from 2 to 4 (+100%)), and (-X%) if change percent is
negative (e.g., changed from 4 to 2 (-50%)).
With the help of this helper function, we then build the
Scala function valueChangeInsight for the narrative
template value change which enables feature value im-
putation:
1
def valueChangeInsight(super_name: String
, insight_item: Map[String, Double]):
String = {
2
val change_percent_desc: String =
3
changePercent(insight_item("
current_value"),
4
insight_item("prev_value"))
5
(super_name.capitalize ++ " changed
from "
6
++ insight_item("prev_value") ++ " to "
7
++ insight_item("current_value")
8
++ change_percent_desc ++ " in the last
month.")
9
}
This Scala function has two inputs:
1. super name:
The name of super-feature.
E.g.,
views per job.
2. insight item: A Scala Map which maps all the in-
sight items in the narrative template to their correspond-
ing feature values for each sample. E.g., for customer
A, insight item = Map("prev value" ->
100, "current value" -> 150).
In practice, we can conduct narrative template imputation by
simply calling this Scala function. For example, we can run
the following Scala code if we want to construct narrative
insight of super-feature views per job for customer A:
1
val narrativeInsightA =
valueChangeInsight("views per job", Map
("prev_value" -> 100, "current_value"
-> 150))
The output will be “Views per job changed from 100 to 150
(+50%) in the last month”.
A.2. Additional Columns in Feature Info File
We brieﬂy introduce three additional columns Insight
Threshold, Insight Weight and Source in Feature Info File.
A sample Feature Info File for the jobs upsell use case with
these three columns included is shown in Table 7 (Due to
space limitations, we omit columns that already exist in
Table 2 and 4). Intellige users can work with model owners
to ﬁll in these three columns, and adjust their values based
on feedback collected from end users:
Table 7. Feature Info File for selected features from jobs upsell
model.
Original-
· · ·
Insight
Insight
Source
Feature
Threshold
Weight
job qty
· · ·
0.8
model
job dprice usd
· · ·
0.8
user
job view s3
· · ·
percent
1
model
change>10
job view s4
· · ·
percent
1
model
change>10
job viewer s3
· · ·
percent
1
model
change>10
job viewer s4
· · ·
percent
1
model
change>10
job applicant s3
· · ·
percent
1
model
change>5
job applicant s4
· · ·
percent
1
model
change>5
• Insight threshold:
This threshold can be used
to ﬁlter out those narratives not meeting certain

criteria, so that the remaining narratives can be
more relevant to end users.
For example, the in-
sight threshold for super-feature views per job
is percent change>10, thus only the narratives
with the increment of job views larger than 10(%) will
be shown to end users.
• Insight weight: This weight can be used to make
adjustments to the narrative ranking.
It takes val-
ues between 0 and 1 (default is 1), and is multiplied
with the feature importance score from Model Inter-
preter to determine the ﬁnal importance score for each
original-feature. The motivation is to incorporate do-
main knowledge into narrative ranking: If we believe
some original-features are predictive in modeling but
not that informative to end users, we can lower their
insight weights to prevent prioritizing their correspond-
ing narratives.
• Source: Sometimes additional features from external
data sources can also be used in narrative construction
(together with model features). These additional fea-
tures are usually in the formats incompatible with the
predictive models (e.g., name, date and url), however,
they can help make the generated narratives more infor-
mative. For example, a narrative can be “This customer
spent 15 hours browsing websites last week, with the
most visited website xyz.com”, where 15 is a model
feature, and xyz.com is an additional feature which
can not be fed into the model directly. We can set
“source” to be model or user to specify the source
of each original-feature. Note that one narrative cannot
be constructed by using only the additional features,
i.e., the additional features must be paired with at least
one model features under the same super-feature, to
make sure that a valid narrative importance score can
be assigned for narrative ranking.
A.3. Narrative Ranking Example
Table 8 shows one example of conducting narrative ranking
and deduplication in jobs upsell use case for customer A.
We set K = 1 in narrative deduplication.
Table 8. Example of narrative ranking and deduplication in jobs
upsell use case.
Original-
Super-
Ultra-
Feature
Feature
Feature
Feature
Imp. Score
job qty
job slots
job slots
0.3
job dprice usd
job slots
job slots
0.4
job view s3
views
job view
0.2
per job
job view s4
views
job view
0.6
per job
job viewer s3
viewers
job view
0.3
per job
job viewer s4
viewers
job view
0.2
per job
⇓Narrative Importance Score Calculation
Super-
Ultra-
Narrative
Narrative
Feature
Feature
Imp. Score
job slots
job slots
Purchased 30
0.4
job slots ...
views
job view
Views per job
0.6
per job
changed ...
viewers
job view
Viewers per job
0.3
per job
changed ...
⇓Narrative Deduplication
Top Ultra-
Top Narrative
Narrative
Feature
Imp. Score
job view
Views per job changed ...
0.6
job slots
Purchased 30 job slots ...
0.4
A.4. Feedback from Sales Representatives
We list several comments from sales representatives in their
original words:
• “These are awesome. I LOVE that you’ve called out
the % of employees. All of these are SUPER helpful.
The top insights are clear and concise. It would have
taken me a lot of time to ﬁnd all of that information, so
I love that it is all laid out. It also prompts me to think
more strategically, which I love.”
• “Most accounts were on my radar, but perhaps not for
the reasons highlighted in the insights—so calling that
out provides a new perspective on potential entry point
into an account/ways to actively engage with relevant
insight/reason.”

• “This SAR is clear to understand and is valuable by
bringing important metrics to my attention. As I dig
into SARs a bit more, time will tell which insights are
most valuable to me.”
• “As someone new to LI, this is incredibly helpful. It
points me into the right direction and allows me to
take action quickly and in a way that correlates to an
activity we are seeing in Sales Navigator.”
• “I love that the insights are consolidated all in one
place, versus needing to run different reports in Mer-
lin to gather the same information. I love the piece
that highlights how many days reps are logging onto
LinkedIn and searching on the platform.”
• “Yes it’s very valuable and useful for conversations. It
helps with next steps and the insights will help it lots of
different ways to tell a story to a customer depending
on where that conversation is at or to help prospect in.”

