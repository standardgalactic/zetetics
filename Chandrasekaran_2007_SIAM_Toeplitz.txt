 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Copyright Â© by SIAM. Unauthorized reproduction of this article is prohibited. 
SIAM J. MATRIX ANAL. APPL.
c
âƒ2007 Society for Industrial and Applied Mathematics
Vol. 29, No. 4, pp. 1247â€“1266
A SUPERFAST ALGORITHM FOR TOEPLITZ SYSTEMS OF
LINEAR EQUATIONSâˆ—
S. CHANDRASEKARANâ€ , M. GUâ€¡, X. SUNÂ§, J. XIAÂ¶, AND J. ZHUâ€¡
Abstract. In this paper we develop a new superfast solver for Toeplitz systems of linear equa-
tions. To solve Toeplitz systems many people use displacement equation methods. With displacement
structures, Toeplitz matrices can be transformed into Cauchy-like matrices using the FFT or other
trigonometric transformations. These Cauchy-like matrices have a special property, that is, their
oï¬€-diagonal blocks have small numerical ranks. This low-rank property plays a central role in our
superfast Toeplitz solver. It enables us to quickly approximate the Cauchy-like matrices by structured
matrices called sequentially semiseparable (SSS) matrices. The major work of the constructions of
these SSS forms can be done in precomputations (independent of the Toeplitz matrix entries). These
SSS representations are compact because of the low-rank property. The SSS Cauchy-like systems
can be solved in linear time with linear storage. Excluding precomputations the main operations are
the FFT and SSS system solve, which are both very eï¬ƒcient. Our new Toeplitz solver is stable in
practice. Numerical examples are presented to illustrate the eï¬ƒciency and the practical stability.
Key words. displacement equation, SSS structure, superfast algorithm, Toeplitz matrix
AMS subject classiï¬cations. 15A06, 65F05, 65G05
DOI. 10.1137/040617200
1. Introduction. Toeplitz systems of linear equations arise in many applica-
tions, including PDE solving, signal processing, time series analysis, orthogonal poly-
nomials, and many others. A Toeplitz system is a linear system
(1.1)
Tx = b
with a coeï¬ƒcient matrix to be a Toeplitz matrix
(1.2)
T =
â›
âœ
âœ
âœ
âœ
âœ
â
t0
tâˆ’1
tâˆ’2
Â· Â· Â·
tâˆ’(Nâˆ’1)
t1
t0
tâˆ’1
Â· Â· Â·
tâˆ’(Nâˆ’2)
t2
t1
t0
Â· Â· Â·
tâˆ’(Nâˆ’3)
...
...
...
...
...
tNâˆ’1
tNâˆ’2
tNâˆ’3
Â· Â· Â·
Â· Â· Â·
â
âŸ
âŸ
âŸ
âŸ
âŸ
â 
,
that is, its entries are constant along every diagonal (a matrix whose entries are con-
stant along every antidiagonal is called a Hankel matrix). The vector t = (tâˆ’(Nâˆ’1) Â· Â· Â·
tâˆ’1 t0 t1 Â· Â· Â· tNâˆ’1) is called the Toeplitz-vector that generates T.
There are both direct and iterative methods for solving (1.1). Direct solvers are
said to be fast if they cost O(N 2) operations; examples include Schur-type methods,
âˆ—Received by the editors October 19, 2004; accepted for publication (in revised form) by D. A.
Bini May 8, 2007; published electronically December 13, 2007.
http://www.siam.org/journals/simax/29-4/61720.html
â€ Department of Electrical and Computer Engineering, University of California at Santa Barbara,
Santa Barbara, CA 93106 (shiv@ece.ucsb.edu).
â€¡Department of Mathematics, University of California at Berkeley, Berkeley, CA 94720 (mgu@
math.berkeley.edu, zhujiang.cal@gmail.com).
Â§Department of Mechanical Engineering, University of California at Berkeley, Berkeley, CA 94720
(sunxt@cal.berkeley.edu).
Â¶Department of Mathematics, University of California at Los Angeles, Los Angeles, CA 90095
(jxia@math.ucla.edu).
1247

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Copyright Â© by SIAM. Unauthorized reproduction of this article is prohibited. 
1248
CHANDRASEKARAN, GU, SUN, XIA, AND ZHU
Levinson-type methods, and others [32]. An important type of direct solver is the
displacement equationâ€“type fast solver based on Gaussian eliminations. Some known
displacement equationâ€“type methods are the Heinig [28], GKO [22], and Gu [26]
methods. Those methods have complexity O(N 2). Methods with complexity less than
O(N 2) are called superfast. In this paper we will present a displacement equationâ€“
type superfast algorithm.
1.1. Fast and superfast methods. Many fast and superfast methods that have
been developed are numerically unstable [5, 15, 16, 7, 37]. References [5] and [15]
showed that the Schur algorithm and the Levinson algorithm are weakly stable in
some cases, but both may be highly unstable in the case of an indeï¬nite and non-
symmetric matrix.
Stable generalized Schur algorithms [32] and look-ahead algo-
rithms were developed in [9, 10].
High-performance look-ahead Schur algorithms
were presented [20].
Many other solvers use the FFT or other trigonometric transforms to convert the
Toeplitz (or even Hankel or Toeplitz-plus-Hankel) matrices into generalized Cauchy
or Vandermonde matrices, which can be done stably in O(N log N) operations. This
is also the approach that we will use in this paper, with the aid of a displacement
structure.
The concept of displacement structure was ï¬rst introduced in [31]. The Sylvester-
type displacement equation for a matrix Ë†C âˆˆRNÃ—N [29] is
(1.3)
Î© Ë†C âˆ’Ë†CÎ› = UV,
where Î©, Î› âˆˆRNÃ—N, U âˆˆRNÃ—Î±, V âˆˆRÎ±Ã—N, and Î± â‰¤N is the displacement rank
with respect to Î© and Î› if rank(UV ) = Î±. The matrix Ë†C is considered to possess a
displacement structure with respect to Î© and Î› if Î± â‰ªN.
With displacement structures it was shown in [19, 24, 36, 22, 28] that Toeplitz
and Hankel matrices can be transformed into Cauchy-like matrices of the following
form:
Ë†C =
 uT
i Â· vj
Î·i âˆ’Î»j
	
1â‰¤i,jâ‰¤N
(ui, vj âˆˆRÎ±) ,
where we assume that Î·i Ì¸= Î»j for 1 â‰¤i, j â‰¤N. Equivalently, a Cauchy-like matrix is
the unique solution to the displacement equation (1.3) with
Î© = diag(Î·1, . . . , Î·n), Î› = diag(Î»1, . . . , Î»n), U =
â›
âœ
â
uT
1
...
uT
n
â
âŸ
â , and V = (v1, . . . , vn) .
In particular, Ë†C is a Cauchy matrix if uT
i vj = 1 for all i and j. The displacement
rank of Ë†C is at most Î±.
To solve Toeplitz systems through Cauchy-like matrices, many people have uti-
lized matrix factorizations. Gohberg and Olshevsky [23] presented a fast variation of
the straightforward Gaussian elimination with partial pivoting (GEPP) procedure to
solve a Cauchy-like linear system of equations in O(N 2) operations. Among other re-
sults, Gohberg, Kailath, and Olshevsky [22] developed algorithm GKO, an improved
version of Heinigâ€™s algorithm [28], and demonstrated numerically that it is stable.
In their algorithm, the Hankel matrix and the Toeplitz-plus-Hankel matrix are also
transformed via fast trigonometric transforms into Cauchy-like matrices.

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Copyright Â© by SIAM. Unauthorized reproduction of this article is prohibited. 
SUPERFAST ALGORITHM FOR TOEPLITZ SYSTEMS
1249
Gu presented a modiï¬ed algorithm in [26] to avoid extra error growth.
This
algorithm is numerically stable, provided that the element growth in the computed
factorization is not large. The algorithm takes O(N 2) operations and is a fast stable
method.
Superfast algorithms appeared in [34, 4, 6, 17, 35, 1, 2, 21] and many others.
Superfast algorithms use divide-and-conquer strategies. Morf developed the ï¬rst idea
in [34]. These methods are unstable for nonsymmetric systems as they cannot deal
with nearly singular leading principal submatrices.
Van Barel and Kravanja presented a superfast method for rational interpolation
at roots of unity [39]. A similar idea was then applied to Toeplitz systems [38]. It
provided an explicit formula for the inverse of a Toeplitz matrix. Additional tech-
niques such as iterative reï¬nement and downdating were still required to stabilize
their algorithm.
1.2. Main results. Our new Toeplitz solver is also of the displacement equation
type. Given a Toeplitz linear system, we ï¬rst use the FFT to transform the associated
Toeplitz matrix into a Cauchy-like matrix. Then instead of using matrix factorizations
which often cost O(N 2) or more, we exploit a special low-rank property of Cauchy-like
matrices, that is, every oï¬€-diagonal block of a Cauchy-like matrix has a low numerical
rank. Using this low-rank property, we then approximate the Cauchy-like matrix by a
low-rank matrix structure called the sequentially semiseparable (SSS) matrix proposed
by Chandrasekaran et al. [12, 13]. A system with the coeï¬ƒcient matrix in compact SSS
form can be solved with only O(p2N) operations, where N is the matrix dimension,
and p is the complexity of the semiseparable description. The SSS solver is practically
stable in our numerical tests and those in [12, 13].
The SSS structure was developed to capture the low-rank property of the oï¬€-
diagonal blocks of a matrix and to maintain stability or practical stability in the
mean time. It is a matrix analog of semiseparable integral kernels in Kailathâ€™s pa-
per [30]. Matrix operations with compact form SSS representations are very eï¬ƒcient,
provided that such compact representations exist or can be easily computed. This
turns out to be true for our case, as the Cauchy-like matrices are transformed from
Toeplitz matrices. We use a recursive compression scheme with a shifting strategy to
construct compact SSS forms for those Cauchy-like matrices. The major work of the
compressions can be precomputed on some Cauchy matrices which are independent
of the actual Toeplitz matrix entries.
The overall algorithm thus has the following stages:
(1) Precompute compressions of oï¬€-diagonal blocks of Cauchy matrices.
(2) Transform the Toeplitz matrix into a Cauchy-like matrix in O(N log N)
operations.
(3) Construct a compact SSS representation from precomputed compressions and
solve the Cauchy-like matrix system O(p2N) operations.
(4) Recover the solution of the Toepliz system in O(N log N) operations.
The stages above are either stable or practically stable. Our numerical results
indicate that the overall algorithm is stable in practice. The Toeplitz matrix does not
have to be symmetric or positive deï¬nite, and no extra stabilizing step is necessary.
After the precomputations, the total cost for the algorithm is O(N log N) + O(p2N).
This indicates that the entire algorithm is superfast.
We also point out that similar techniques are used in [33], where the low-rank
property is exploited through the block columns without diagonals (called neutered
block columns in [33]), in contrast with the oï¬€-diagonal blocks here. The compres-

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Copyright Â© by SIAM. Unauthorized reproduction of this article is prohibited. 
1250
CHANDRASEKARAN, GU, SUN, XIA, AND ZHU
sions of either the neutered blocks or the oï¬€-diagonal blocks both give data-sparse
representations which enable fast factorizations of the Cauchy-like matrices. In fact,
corresponding to neutered block rows or columns, there are also matrix representa-
tions called hierarchically semiseparable (HSS) matrices [14] which are usually more
complicated structures than SSS matrices.
1.3. Overview. We will discuss the displacement structure and the transforma-
tion from a Toeplitz problem to a Cauchy-like problem in section 2. The low-rank
property of this Cauchy-like problem is then exploited. Section 3 then gives a linear
complexity solver using the SSS structure. In section 4, we will present an algorithm
for fast construction of SSS structures. We will then analyze the complexity in sec-
tion 5 and use some numerical experiments to demonstrate the eï¬ƒciency and the
practical stability. All algorithms have been implemented in Fortran 90. Section 6
draws some conclusions.
2. Displacement structures and low-rank property.
2.1. Cauchy-like systems. Given a Toeplitz system (1.1), we can use a dis-
placement structure to transform it into a Cauchy-like system. Deï¬ne
ZÎ´ =
â›
âœ
âœ
âœ
âœ
âœ
âœ
â
0
0
Â· Â· Â·
0
Î´
1
0
Â· Â· Â·
Â· Â· Â·
0
0
1
...
...
...
...
...
...
0
Â· Â· Â·
0
1
0
â
âŸ
âŸ
âŸ
âŸ
âŸ
âŸ
â 
,
and let Î© = Z1 and Î› = Zâˆ’1 in (1.3). Kailath, Kung, and Morf [31] have shown that
every Toeplitz matrix satisï¬es the displacement equation (1.3) with A Â· B, having
nonzero entries only in its ï¬rst row and last column, to be a matrix of rank at most
2. Hence the displacement rank of a Toeplitz matrix is at most 2 with respect to Z1
and Zâˆ’1. The following result can be found in [28].
Proposition 2.1. Let Ë†C âˆˆRNÃ—N be a matrix satisfying the displacement equa-
tion
(2.1)
Z1 Ë†C âˆ’Ë†CZâˆ’1 = UV,
where U âˆˆRnÃ—Î± and V âˆˆRÎ±Ã—n. Then F Ë†CDâˆ’1
0 FH is a Cauchy-like matrix satisfying
(2.2)
D1(F Ë†CDâˆ’1
0 FH) âˆ’(F Ë†CDâˆ’1
0 FH)Dâˆ’1 = (FU)

V DH
0 FH
,
where F =

1
N (Ï‰2(kâˆ’1)(jâˆ’1))1â‰¤k,jâ‰¤N is the normalized inverse discrete Fourier trans-
form matrix, Ï‰ = e
Ï€i
N , and
D1 = diag(1, Ï‰2, . . . , Ï‰2(Nâˆ’1)),
Dâˆ’1 = diag(Ï‰, Ï‰3, . . . , Ï‰2Nâˆ’1),
D0 = diag(1, Ï‰, . . . , Ï‰Nâˆ’1).
Here Î± â‰¤2. This proposition suggests that for a Toeplitz matrix T, one can
convert it into the Cauchy-like matrix in (2.2). Therefore the Toeplitz system (1.1)
can be readily transformed into a new system
(2.3)
CËœx = Ëœb,

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Copyright Â© by SIAM. Unauthorized reproduction of this article is prohibited. 
SUPERFAST ALGORITHM FOR TOEPLITZ SYSTEMS
1251
where C has the form
(2.4)
C =

uT
i vj
Ï‰2i âˆ’Ï‰2j+1
	
1â‰¤i,jâ‰¤N
(ui, vj âˆˆRÎ±) .
In section 3 we will present a fast solver for (2.3).
After obtaining Ëœx we will
then recover x with an FFT again.
All the stages involving FFT are stable and
cost O(N log N). The solver for (2.3) has a linear complexity and turns out to be
practically stable. Thus the total cost of our algorithm is bounded by O(N log N) +
O(Np2), where p is some parameter that will be described below. This indicates our
method is a superfast one with practical stability.
2.2. Low-rank property of Cauchy-like matrices. In this section, we will
show a low-rank property of C, i.e., every oï¬€-diagonal block of C has a low numerical
rank. This property is the basis of the superfast SSS solver in section 3.
First, a simple numerical experiment can give us an idea of the low-rank property
of C. To ï¬nd out the numerical ranks we can use one of the following tools:
(1) Ï„-accurate SVD: singular values less than Ï„ are dropped if Ï„ is an absolute
tolerance, or singular values less than Ï„ times the largest singular value are
dropped if Ï„ is a relative tolerance.
(2) Ï„-accurate QR:
A â‰ˆQR, A : m Ã— n, Q : m Ã— k, R : k Ã— k, k â‰¤l â‰¡min(m, n),
which is obtained in the following way. Compute the exact QR factorization
of matrix A = Ë†Q Ë†R, where Ë†Q is m Ã— l and Ë†R is l Ã— n with diagonal entries
satisfying Ë†R11 â‰¥Ë†R22 â‰¥Â· Â· Â· â‰¥Ë†Rll. Then obtain R by dropping all rows of Ë†R
with diagonal entries less than Ï„ if Ï„ is an absolute tolerance, or with diagonal
entries less than Ï„ Ë†R11 if Ï„ is a relative tolerance. Drop relevant columns of
Ë†Q accordingly to obtain Q.
Later, by ranks we mean numerical ranks. Here we take some random Toeplitz
matrices in diï¬€erent sizes. Then we transform them into Cauchy-like matrices C and
compute the numerical ranks of their oï¬€-diagonal blocks. For simplicity, we compute
the ranks for blocks C(1 : d, d + 1 : N), C(1 : 2d, 2d + 1 : N), . . ., C(1 : kd, kd + 1 :
N), . . ., where d is a ï¬xed integer, and these blocks are numbered as block numbers
1, 2, . . . , k as shown in Figure 2.1.
Here, k = 8 oï¬€-diagonal blocks for each of three N Ã— N Cauchy-like matrices are
considered. See Table 2.1 for the numerical ranks, where we use the Ï„-accurate SVD
with Ï„ to be an absolute tolerance.
We can see that the numerical ranks are relatively small as compared to the block
sizes. And when we double the dimension of the matrix, the numerical ranks do not
increase much. This is more signiï¬cant when a larger Ï„ is used.
Fig. 2.1. Oï¬€-diagonal blocks (numbered as 1, 2, 3). Upper triangular part only.

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Copyright Â© by SIAM. Unauthorized reproduction of this article is prohibited. 
1252
CHANDRASEKARAN, GU, SUN, XIA, AND ZHU
Table 2.1
Oï¬€-diagonal numerical ranks of Cauchy-like matrices transformed from some Toeplitz matrices
with absolute tolerance Ï„ = 10âˆ’9.
N = 640
N = 1280
N = 2560
Block #
Block size
Rank
Block size
Rank
Block size
Rank
1
80 Ã— 560
37
160 Ã— 1120
44
320 Ã— 2240
52
2
160 Ã— 480
43
320 Ã— 960
50
640 Ã— 1920
57
3
240 Ã— 400
45
480 Ã— 800
53
960 Ã— 1600
60
4
320 Ã— 320
46
640 Ã— 640
53
1280 Ã— 1280
61
5
400 Ã— 240
46
800 Ã— 480
52
1600 Ã— 960
60
6
480 Ã— 160
43
960 Ã— 320
50
1920 Ã— 640
58
7
560 Ã— 80
37
1120 Ã— 160
44
2240 Ã— 320
52
The low-rank property can be veriï¬ed theoretically in the following way. We ï¬rst
consider a special case of (2.4) where all uT
i vj = 1. We show that the following Cauchy
matrix has low-rank oï¬€-diagonal blocks:
(2.5)
C0 =

1
Ï‰2i âˆ’Ï‰2j+1
	
1â‰¤i,jâ‰¤N
The central idea is similar to that in the fast multipole method [25, 8], which im-
plies that with well-separated points (see, e.g., Figure 2.2) an interaction matrix is
numerically low-rank.
Here we introduce two sets of points {Î»k}N
k=1 â‰¡{Ï‰2k}N
k=1 and {Î·k}N
k=1 â‰¡{Ï‰2k+1}N
k=1
on the unit circle. When we consider an oï¬€-diagonal block of C0 as follows:
G =

1
Î»i âˆ’Î·j
	
1â‰¤iâ‰¤p, p+1â‰¤jâ‰¤N
we can show G is (numerically) low-rank. In fact G corresponds to two well-separated
sets {Î»k}p
k=1 and {Î·j}N
k=p+1; that is, there exists a point c âˆˆC such that
(2.6)
|Î»i âˆ’c|
>
d + e,
i = 1, . . . , p,
|Î·j âˆ’c|
<
d,
j = p + 1, . . . , N,
where d, e are positive constants. Consider the expansion
1
Î»i âˆ’Î·j
=
1
Î»i âˆ’c
1
1 âˆ’Î·jâˆ’c
Î»iâˆ’c
=
r

k=0
(Î·j âˆ’c)k
(Î»i âˆ’c)k+1 + O
Î·j âˆ’c
Î»i âˆ’c
	r+1
(2.7)
=
r

k=0
(Î·j âˆ’c)k
(Î»i âˆ’c)k+1 + Îµ,
(2.8)
Fig. 2.2. Well-separated sets in the plane.

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Copyright Â© by SIAM. Unauthorized reproduction of this article is prohibited. 
SUPERFAST ALGORITHM FOR TOEPLITZ SYSTEMS
1253
where r is a number such that the error term |Îµ| = |O(( Î·jâˆ’c
Î»iâˆ’c)r+1)| is bounded by a
given tolerance. We have the estimate

Î·j âˆ’c
Î»i âˆ’c

r+1
<

d
d + e
	r+1
,
which enables us to ï¬nd an appropriate r according to the tolerance. Thus
G =
 r

k=0
(Î·j âˆ’c)k
(Î»i âˆ’c)k+1

1â‰¤iâ‰¤p, p+1â‰¤jâ‰¤N
+ Ë†Îµ
(2.9)
=
â›
âœ
âœ
âœ
â
1
Î»1âˆ’c
1
(Î»1âˆ’c)2
Â· Â· Â·
1
(Î»1âˆ’c)r+1
1
Î»2âˆ’c
1
(Î»2âˆ’c)2
Â· Â· Â·
1
(Î»2âˆ’c)r+1
...
...
Â· Â· Â·
...
1
Î»pâˆ’c
1
(Î»pâˆ’c)2
Â· Â· Â·
1
(Î»pâˆ’c)r+1
â
âŸ
âŸ
âŸ
â 
â›
âœ
â
1
1
Â· Â· Â·
1
(Î·p+1âˆ’c)
(Î·p+2âˆ’c)
Â· Â· Â·
(Î·N âˆ’c)
...
...
Â· Â· Â·
...
(Î·p+1âˆ’c)r
(Î·p+2âˆ’c)r
Â· Â· Â·
(Î·N âˆ’c)r
â
âŸ
â + Ë†Îµ.
Therefore the numerical rank of G is at most r + 1, up to an error Ë†Îµ.
Now we can return to the Cauchy-like matrix (2.4). A similar argument shows
that any oï¬€-diagonal block of C satisï¬es
Ë†Gâ‰ˆ
 r

k=0
(uT
i vj) (Î·j âˆ’c)k
(Î»i âˆ’c)k+1

1â‰¤iâ‰¤p, p+1â‰¤jâ‰¤N
=
â›
âœ
âœ
âœ
âœ
âœ
â
uT
1
Î»1âˆ’c
uT
1
(Î»1âˆ’c)2
Â· Â· Â·
uT
1
(Î»1âˆ’c)r+1
uT
2
Î»2âˆ’c
uT
2
(Î»2âˆ’c)2
Â· Â· Â·
uT
2
(Î»2âˆ’c)r+1
...
...
Â· Â· Â·
...
uT
p
Î»pâˆ’c
uT
p
(Î»pâˆ’c)2
Â· Â· Â·
uT
p
(Î»pâˆ’c)r+1
â
âŸ
âŸ
âŸ
âŸ
âŸ
â 
â›
âœ
â
v1
v2
Â· Â· Â·
vq
(Î·p+1âˆ’c) v1
(Î·p+2âˆ’c) v2
Â· Â· Â·
(Î·N âˆ’c) vN
...
...
Â· Â· Â·
...
(Î·p+1âˆ’c)rv1
(Î·p+2âˆ’c)rv2
Â· Â· Â·
(Î·N âˆ’c)rvN
â
âŸ
â .
That is, we replace the entries of the two matrix factors in (2.9) with appropriate
vectors. Thus the numerical rank of Ë†G will be no larger than Î±(r + 1), which will be
relatively small as compared to N.
3. SSS structures and superfast SSS solver.
3.1. SSS representations. To take advantage of the low-rank property of the
Cauchy-like matrix C, we can use SSS structures introduced by Chandrasekaran
et al. [12, 13]. The SSS structure nicely captures the ranks of oï¬€-diagonal blocks
of a matrix such as shown in Figure 2.1.
A matrix A âˆˆCMÃ— Ëœ
M satisï¬es the SSS structure if there exist 2n positive integers
m1, . . . , mn, and Ëœm1, . . . , Ëœmn with M = m1 + Â· Â· Â· + mn and Ëœ
M = Ëœm1 + Â· Â· Â· + Ëœmn to
block-partition A as A = (Ai,j)kÃ—k , where Aij âˆˆCmiÃ— Ëœmj satisï¬es
(3.1)
Aij =
â§
âª
â¨
âª
â©
Di
if i = j,
UiWi+1 Â· Â· Â· Wjâˆ’1V H
j
if i < j,
PiRiâˆ’1 Â· Â· Â· Rj+1QH
j
if i > j.
Here the superscript H denotes the Hermitian transpose and empty products are
deï¬ned to be identity matrices. The matrices {Ui}nâˆ’1
i=1 , {Vi}n
i=2, {Wi}nâˆ’1
i=2 , {Pi}n
i=2,

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Copyright Â© by SIAM. Unauthorized reproduction of this article is prohibited. 
1254
CHANDRASEKARAN, GU, SUN, XIA, AND ZHU
Table 3.1
Dimensions of matrices in (3.1).
Matrix
Ui
Vi
Wi
Pi
Qi
Ri
Dimensions
mi Ã— ki
Ëœmi Ã— kiâˆ’1
kiâˆ’1 Ã— ki
mi Ã— li
Ëœmi Ã— li+1
li+1 Ã— li
{Qi}nâˆ’1
i=1 , {Ri}nâˆ’1
i=2 , and {Di}n
i=1 are called generators for the SSS structure and their
dimensions are deï¬ned in Table 3.1.
As an example, the matrix A with n = 4 has the form
(3.2)
A =
â›
âœ
âœ
â
D1
U1V H
2
U1W2V H
3
U1W2W3V H
4
P2QH
1
D2
U2V H
3
U2W3V H
4
P3R2QH
1
P3QH
2
D3
U3V H
4
P4R3R2QH
1
P4R3QH
2
P4QH
3
D4
â
âŸ
âŸ
â .
The SSS representation (3.2) is related to the oï¬€-diagonal blocks in Figure 2.1 in
the way that the upper oï¬€-diagonal block numbers 1, 2, and 3 are
U1

V H
2
W2V H
3
W2W3V H
4

,

U1W2
U2
	 
V H
3
W3V H
4

,
â›
â
U1W2W3
U2W3
U3
â
â V H
4 .
Appropriate row and column bases of the oï¬€-diagonal blocks are clearly reï¬‚ected.
The SSS structure depends on the sequences {mi} and { Ëœmi} and the SSS genera-
tion scheme. If A is a square matrix (M = Ëœ
M), then we can have a simpler situation
mi = Ëœmi, i = 1, . . . , n. SSS matrices are closed under addition, multiplication, inver-
sion, etc., although the sizes of the generators may increase.
While any matrix can be represented in this form for suï¬ƒciently large kiâ€™s and
liâ€™s, the column dimensions of Uiâ€™s and Piâ€™s, respectively, our main focus will be
on SSS matrices which have low-rank oï¬€-diagonal blocks and have generators with
kiâ€™s and liâ€™s to be close to those ranks.
We say these SSS matrices are compact.
Particularly true for Cauchy-like matrices, they can have compact SSS forms. Using
SSS structures, we can take advantage of the superfast SSS system solver in [12, 13] to
solve the Cauchy-like systems. The solver is eï¬ƒcient when the SSS form is compact,
and is practically stable. The solver shares similar ideas with that for banded plus
semiseparable systems in [11].
Here we brieï¬‚y describe the main ideas of the solver in [12, 13]. We consider
solving the linear system Ax = b, where A âˆˆCNÃ—N satisï¬es (3.1) and b itself is an
unstructured matrix. The solver computes an implicit ULV H decomposition of A,
where U and V are orthogonal matrices.
Before we present the formal algorithm, we demonstrate the key ideas on a 4 Ã— 4
block matrix example.
3.2. SSS solver: 4 Ã— 4 example. Let the initial system Ax = b be partitioned
as follows:
(3.3)
â›
âœ
âœ
â
D1
U1V H
2
U1W2V H
3
U1W2W3V H
4
P2QH
1
D2
U2V H
3
U2W3V H
4
P3R2QH
1
P3QH
2
D3
U3V H
4
P4R3R2QH
1
P4R3QH
2
P4QH
3
D4
â
âŸ
âŸ
â 
â›
âœ
âœ
â
x1
x2
x3
x4
â
âŸ
âŸ
â =
â›
âœ
âœ
â
b1
b2
b3
b4
â
âŸ
âŸ
â âˆ’
â›
âœ
âœ
â
0
P2
P3R2
P4R3R2
â
âŸ
âŸ
â Î¾,

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Copyright Â© by SIAM. Unauthorized reproduction of this article is prohibited. 
SUPERFAST ALGORITHM FOR TOEPLITZ SYSTEMS
1255
where the dimensions of the generators follow those in Table 3.1 with mi = Ëœmi and
the vector Î¾ = 0. The extra zero vector Î¾ on the right-hand side of (3.3) has been
added for the purpose of a general recursive pattern.
The algorithm has two main stages, compression (or elimination) and merging,
depending on the relationship between ki (li) and mi in the intermediate procedure.
3.2.1. Compression. At the beginning, k1 < m1 because of the low-rank prop-
erty described earlier. We apply a unitary transformation qH
1 to U1 so that the ï¬rst
m1 âˆ’k1 rows of U1 become zeros:
(3.4)
qH
1 U1 =

0
Ë†U1
	
m1 âˆ’k1
k1
.
Now we multiply qH
1 to the ï¬rst m1 equations of the system

qH
1
0
0
I
	
Ax =

qH
1
0
0
I
	
b âˆ’
â›
âœ
âœ
â
0
P2
P3R2
P4R3R2
â
âŸ
âŸ
â Î¾.
We pick another unitary transformation wH
1 to lower-triangularize qH
1 D1, the (1, 1)
diagonal block A, i.e.,

qH
1 D1

wH
1 =
 m1 âˆ’k1
k1
m1 âˆ’k1
D11
0
k1
D21
D22
	
.
Then system (3.3) becomes

qH
1
0
0
I
	
A

wH
1
0
0
I
	 
w1
0
0
I
	
x =

qH
1
0
0
I
	
b âˆ’
â›
âœ
âœ
â
0
P2
P3R2
P4R3R2
â
âŸ
âŸ
â Î¾,
which can be rewritten as
â›
âœ
âœ
âœ
âœ
â
D11
0
0
0
0
D21
D22
Ë†U1V H
2
Ë†U1W2V H
3
Ë†U1W2W3V H
4
P2QH
11
P2 Ë†QH
1
D2
U2V H
3
U2W3V H
4
P3R2QH
11
P3R2 Ë†QH
1
P3QH
2
D3
U3V H
4
P4R3R2QH
11
P4R3R2 Ë†QH
1
P4R3QH
2
P4QH
3
D4
â
âŸ
âŸ
âŸ
âŸ
â 
â›
âœ
âœ
âœ
âœ
â
z1
Ë†x1
x2
x3
x4
â
âŸ
âŸ
âŸ
âŸ
â 
=
â›
âœ
âœ
âœ
âœ
â
Î²1
Î³1
b2
b3
b4
â
âŸ
âŸ
âŸ
âŸ
â 
âˆ’
â›
âœ
âœ
âœ
âœ
â
0
0
P2
P3R2
P4R3R2
â
âŸ
âŸ
âŸ
âŸ
â 
Î¾,
where we have used the partitions
w1x1 = m1 âˆ’k1
k1
z1
Ë†x1
	
,
qH
1 b1 =

m1 âˆ’k1
Î²1
k1
Î³1
	
,
and
w1Q1 =

m1 âˆ’k1
Q11
k1
Ë†Q1
	
.

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Copyright Â© by SIAM. Unauthorized reproduction of this article is prohibited. 
1256
CHANDRASEKARAN, GU, SUN, XIA, AND ZHU
At this point, we can solve for z1 from the system of equations D11z1 = Î²1. We
also subtract D21z1 from the right-hand side to obtain Ë†b1 = Î³1 âˆ’D21z1. Then we can
discard the ï¬rst m1 âˆ’k1 rows and columns of the coeï¬ƒcient matrix of the system to
obtain
â›
âœ
âœ
â
D22
Ë†U1V H
2
Ë†U1W2V H
3
Ë†U1W2W3V H
4
P2 Ë†QH
1
D2
U2V H
3
U2W3V H
4
P3R2 Ë†QH
1
P3QH
2
D3
U3V H
4
P4R3R2 Ë†QH
1
P4R3QH
2
P4QH
3
D4
â
âŸ
âŸ
â 
â›
âœ
âœ
â
Ë†x1
x2
x3
x4
â
âŸ
âŸ
â =
â›
âœ
âœ
â
Ë†b1
b2
b3
b4
â
âŸ
âŸ
â âˆ’
â›
âœ
âœ
â
0
P2
P3R2
P4R3R2
â
âŸ
âŸ
â Ë†Î¾,
where Ë†Î¾ = Î¾ + QH
11z1. This new system has a similar structure to the original one
but with smaller dimension.
We can continue to solve it by recursion, if further
compressions of the blocks such as (3.4) are possible. Note the actual solution, say,
x1, can be recovered by
x1 = wH
1

z1
Ë†x1
	
.
3.2.2. Merging. During the recursive eliminations there are situations when
ki is no longer smaller than mi and no further compression is possible.
We are
then unable to introduce more zeros into the system. Now we proceed by merging
appropriate block rows and columns of the matrix. As an example we can merge the
ï¬rst two block rows and columns and rewrite the system of equations as follows:
â›
âœ
âœ
âœ
âœ
âœ
âœ
âœ
âœ
â

D1
U1V H
2
P2QH
1
D2
	

U1W2
U2
	
V H
3

U1W2
U2
	
W3V H
4
P3

Q1RH
2
Q2
	H
D3
U3V H
4
P4R3

Q1RH
2
Q2
	H
P4QH
3
D4
â
âŸ
âŸ
âŸ
âŸ
âŸ
âŸ
âŸ
âŸ
â 
â›
âœ
âœ
â

x1
x2
	
x3
x4
â
âŸ
âŸ
â 
=
â›
âœ
âœ
â

b1
b2 âˆ’P2 Ë†Î¾
	
b3
b4
â
âŸ
âŸ
â âˆ’
â›
âœ
âœ
â

0
0
	
P3
P4R3
â
âŸ
âŸ
â (R2 Ë†Î¾).
Hence the system becomes
â›
â
Ë†D1
Ë†U1V H
3
Ë†U1W3V H
4
P3 Ë†QH
1
D3
U3V H
4
P4R3 Ë†QH
1
P4QH
3
D4
â
â 
â›
â
Ë†x1
x3
x4
â
â =
â›
â
Ë†b1
b3
b4
â
â âˆ’
â›
â
0
P3
P4R3
â
â (ËœÎ¾),
where
Ë†D1 =

D1
U1V H
2
P2QH
1
D2
	
,
Ë†U1 =
 U1W2
U2
	
,
Ë†Q1 =

Q1RH
2
Q2
	
,
Ë†x1 =

x1
x2
	
,
Ë†b1 =

b1
b2 âˆ’P2Ï„
	
,
ËœÎ¾ = R2 Ë†Î¾.
The number of block rows/columns is reduced by one. Further compressions become
possible and we can proceed to solve the system recursively. In the case n = 1, we
have the system D1x1 = b1 âˆ’0Î¾, which is solved directly.

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Copyright Â© by SIAM. Unauthorized reproduction of this article is prohibited. 
SUPERFAST ALGORITHM FOR TOEPLITZ SYSTEMS
1257
3.3. General solve algorithm. We now present a short description of the gen-
eral algorithm. The procedure in the 4 Ã— 4 example can be directly extended to a
general system. We assume that the matrix A is in compact SSS form represented by
the generators {Ui}nâˆ’1
i=1 , {Vi}n
i=2, {Wi}nâˆ’1
i=2 , {Pi}n
i=2, {Qi}nâˆ’1
i=1 , {Ri}nâˆ’1
i=2 , and {Di}n
i=1
as in (3.1). We also partition x = (xi) and b = (bj) such that xi and bi have mi rows.
As in the 4 Ã— 4 example, there are two stages at each step of the recursion.
In the compression stage, we perform orthogonal eliminations on both sides of A
to create an (m1 âˆ’k1) Ã— (m1 âˆ’k1) lower triangular submatrix at the top left corner
of A. Then we solve a small triangular system and obtain the ï¬rst few components of
the solution vector. At this stage, we are left with a new system with less unknowns;
hence we can carry out a recursion.
In the merging stage, we merge the ï¬rst two block rows and columns of A while
still maintaining the SSS structure.
The numbers of block rows and columns are
reduced by one.
Combining these two stages, we can proceed with recursion to solve the system.
When n = 1 we can solve the linear system directly with standard solvers.
The SSS solver has a complexity O(Np2) [12, 13], where p is the maximum
numerical rank of the oï¬€-diagonal blocks of A, as compared to the traditional O(N 3)
cost for a general dense N Ã— N matrix. We use only orthogonal transformations and
a single substitution in the SSS solver. Although a formal proof for the backward
stability is not yet available, the solver is shown to be practically stable. The reader
is referred to [12, 13] for more discussions on the stability.
4. Fast construction of SSS representation for C. According to section 3,
a system in compact SSS form can be solved very eï¬ƒciently. We can thus use that
algorithm to solve the Cauchy-like system (2.3), provided that C can be quickly
written in SSS form. Therefore we try to ï¬nd an eï¬ƒcient construction scheme. Here we
provide a divide-and-conquer SSS construction scheme by using the fast merging and
splitting strategy in [13]. If we know further that the matrix has low-rank oï¬€-diagonal
blocks and it is easy to compress the oï¬€-diagonal blocks, then the construction can
be superfast. Here we will concentrate on this situation as Cauchy-like matrices have
this low-rank property.
We ï¬rst present a general divide-and-conquer construction algorithm and then
describe a fast shifting strategy to compress the oï¬€-diagonal blocks of C.
4.1. General divide-and-conquer construction algorithm. In this section,
we discuss the construction of the SSS structure of a matrix A, when the partition
sequence {mi}n
i=1 is given. The general construction methods can be applied to any
unstructured matrix, thus proving that any matrix has an SSS structure. (Of course,
ki and li will usually be large in this case, precluding any speed-ups.) These methods
can be viewed as speciï¬c ways to make the realization algorithm of [18] more eï¬ƒcient.
Suppose we are given an N Ã— N matrix A and a partition sequence {mi}n
i=1 with
n
i=1 mi = N. Starting with an appropriate sequence { Ëœm1, Ëœm2}, where k
i=1 mi =
Ëœm1 and n
i=k+1 mi = Ëœm2, we can ï¬rst partition A into a 2 Ã— 2 block matrix and then
construct a simple SSS form
(4.1)
A =
 Ëœm1
Ëœm2
Ëœm1
D1
B
Ëœm2
E
D2
	
=

Ëœm1
Ëœm2
D1
U1V H
2
P2QH
1
D2
	
,
where
(4.2)
B = U1V H
2
â‰¡U1

Î£1F H
1

, F = P2QH
1 â‰¡P2

Î£1F H
1


 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Copyright Â© by SIAM. Unauthorized reproduction of this article is prohibited. 
1258
CHANDRASEKARAN, GU, SUN, XIA, AND ZHU
are the low-rank SVDs of the oï¬€-diagonal blocks B and E. Note if the compressions
are done by Ï„-accurate SVD approximations or rank-revealing QR decompositions,
then appropriate â€œ=â€ signs should be replaced by â€œâ‰ˆâ€ signs. We now split either the
(1, 1) block or the (2, 2) block to obtain a 3 Ã— 3 block SSS matrix. For instance, we
can split the (1, 1) block according to an appropriate new sequence { Ë†m1, Ë†m2, Ë†m3} as
follows, where Ë†m1 + Ë†m2 = m1, Ë†m3 = m2:

Dnew
1
U new
1
(V new
2
)H
P new
2
(Qnew
1
)H
Dnew
2
	
= D1,

U new
1
W new
2
U new
2
	
= U1, (V new
3
)H = V H
2 ,
(Rnew
2
(Qnew
1
)H(Qnew
2
)H) = QH
1 , P new
3
= P2, Dnew
3
= D2,
where the new generators (marked by the superscript new) introduced based on (4.1)
can be determined in the following way. First, we partition the matrices for the old
ï¬rst block conformally with the two new blocks as

D11
1
D12
1
D21
1
D22
1
	
= D1,

U 1
1
U 2
1
	
= U1, ((Q1
1)H(Q2
1)H) = QH
1 .
We can then identify from these and the previous equations that
Dnew
1
= D11
1 , Dnew
2
= D22
1 , U new
2
= U 2
1 , V new
3
= V2, Qnew
2
= Q2
1, P new
3
= P2, Dnew
3
= D2.
The remaining matrices satisfy
(4.3)
(D12
1 U 1
1 ) = U new
1
((V new
2
)HW new
2
),
 D21
1
(Q1
1)H
	
=

P new
2
Rnew
2
	
(Qnew
1
)H.
By factorizing the left-hand side matrices using numerical tools such as the SVD and
rank-revealing QR, these two equations allow us to compute those remaining matrices
for the new blocks.
A is thus in the new form
A =
â›
â
Ë†m1
Ë†m2
Ë†m3
Ë†m1
Dnew
1
U new
1
(V new
2
)H
U new
1
W new
2
(V new
3
)H
Ë†m2
P new
2
(Qnew
1
)H
Dnew
2
U new
2
(V new
3
)H
Ë†m3
P new
2
Rnew
2
(Qnew
1
)H
P new
3
(Qnew
2
)H
Dnew
3
â
â .
We can use similar techniques if we want to split the second row and column
of (4.1).
We can continue this by either splitting the ï¬rst block row and block column or
the last ones using the above techniques, or splitting any middle block row and block
column similarly. Then we will be able to construct the desired SSS representation
according to the given sequence {mi, i = 1, 2, . . . , n}.
The general construction can be organized with bisection. The major cost is in
compressions of oï¬€-diagonal blocks of the form
(4.4)
D12
i
= XiY H
i ,
where Xi and Yi are tall and thin, and D12
i
is an oï¬€-diagonal block of
Di =
D11
i
D12
i
D21
i
D22
i
	
.
The compression (4.4) can be achieved by a Ï„-accurate QR factorization.
The construction is also practically stable in our implementation.

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Copyright Â© by SIAM. Unauthorized reproduction of this article is prohibited. 
SUPERFAST ALGORITHM FOR TOEPLITZ SYSTEMS
1259
4.2. Compression of oï¬€-diagonal blocks. For a general matrix with low-rank
oï¬€-diagonal blocks, the SSS construction can cost O(N 2p) as a compression such
as (4.2), (4.3), and (4.4) can take O(K2p), where K is the dimension of the block
being compressed. However, for the Cauchy-like matrix C in (2.3) the compressions
can be precomputed.
Theorem 4.1. The compression of the oï¬€-diagonal block
(4.5)
G =
 uT
i Â· vj
Î»i âˆ’Î·j
	
1â‰¤iâ‰¤p, qâ‰¤jâ‰¤N
= XY H
of C can be obtained by the compression of the corresponding oï¬€-diagonal block
G =

1
Î»i âˆ’Î·j
	
1â‰¤iâ‰¤p, qâ‰¤jâ‰¤N
= X0Y H
0
of C0, where the column dimension of X is no larger than twice the size of the column
dimension of X0.
Proof. Assume the oï¬€-diagonal block G0 = (
1
Î»iâˆ’Î·j )1â‰¤iâ‰¤p, qâ‰¤jâ‰¤N is in compressed
form X0Y H
0 , and the (i, j) entry is
1
Î»iâˆ’Î·j = Xi,:Y H
j,: , where Xi,: (Yj,:) denotes the ith
row of X (Y ). As Î± â‰¤2 for simplicity we ï¬x Î± = 2 in (2.1). Then the corresponding
oï¬€-diagonal block in C is
G =
 uT
i Â· vj
Î»i âˆ’Î·j
	
1â‰¤iâ‰¤p, qâ‰¤jâ‰¤N
=
ui1vj1 + ui2vj2
Î»i âˆ’Î·j
	
1â‰¤iâ‰¤p, qâ‰¤jâ‰¤N
=

(ui1vj1 + ui2vj2) Xi,:Y H
j,:

1â‰¤iâ‰¤p, qâ‰¤jâ‰¤N
=

(ui1Xi,:
ui2Xi,:)
vj1Y H
j,:
vj2Y H
j,:
		
1â‰¤iâ‰¤p, qâ‰¤jâ‰¤N
=
â›
âœ
â
u11X1,:
u12X1,:
...
...
up1Xi,:
up2Xi,:
â
âŸ
â 
vq1Y H
q,:
Â· Â· Â·
vN1Y H
N,:
vq2Y H
q,:
Â· Â· Â·
vN2Y H
N,:
	
â‰¡XY H.
That is, we get a compression of G.
Theorem 4.1 indicates that we can convert the compressions of the oï¬€-diagonal
blocks of C to be the compressions of those of C0 which is independent of the actual
entries of the Toeplitz matrix T. This means the compressions of oï¬€-diagonal blocks
of C0 can be precomputed. The precomputation can be done in O(N 2p) ï¬‚ops by a
rank-revealing QR factorization such as in [27]. It is possible to reduce the cost to
O(N log N) due to the fact that the compression of a large oï¬€-diagonal block can
be obtained by that of small ones. This can be seen implicitly from the following
subsection.
4.3. Compressions of oï¬€-diagonal blocks in precomputation. We further
present a shifting strategy to reduce the cost of the compressions in the precomputa-
tion. The signiï¬cance of this shifting strategy is to relate the compressions of large
oï¬€-diagonal block of C0 to those of small ones. That is, in diï¬€erent splitting stages
of the SSS construction of C, the compressions of oï¬€-diagonal blocks with diï¬€erent
sizes can be related.

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Copyright Â© by SIAM. Unauthorized reproduction of this article is prohibited. 
1260
CHANDRASEKARAN, GU, SUN, XIA, AND ZHU
For simplicity, we look at the example of partitioning C0 into 4Ã—4 blocks. Assume
in the ï¬rst cut that we can partition C0 into 2 Ã— 2 blocks with equal dimensions as in
the following:
C0 =

1
Ï‰2i âˆ’Ï‰2j+1
	
1â‰¤i, jâ‰¤N
=
C0;1,1
C0;1,2
C0;2,1
C0;2,2
	
=
â›
âœ
âœ
âœ
âœ
âœ
âœ
âœ
â
1
Ï‰2âˆ’Ï‰3
Â· Â· Â·
1
Ï‰2âˆ’Ï‰4k+1
1
Ï‰2âˆ’Ï‰4k+3
Â· Â· Â·
1
Ï‰2âˆ’Ï‰2N+1
...
...
...
Â· Â· Â·
...
1
Ï‰4kâˆ’Ï‰3
Â· Â· Â·
1
Ï‰4kâˆ’Ï‰4k+1
1
Ï‰4kâˆ’Ï‰4k+3
Â· Â· Â·
1
Ï‰4kâˆ’Ï‰2N+1
C0;2,1
C0;2,2
â
âŸ
âŸ
âŸ
âŸ
âŸ
âŸ
âŸ
â 
,
where k = N
4 , and without loss of generality, we consider only the block upper tri-
angular part. Assume that we have obtained a compression X1Y H
1
of C0;1,2 such
that
C0 =
C0;1,1
X1Y H
1
C0;2,1
C0;2,2
	
,
where X1 and Y1 are tall and thin and can be computed with Ï„-accurate SVD or
rank-revealing QR factorization. Next, we split C0;1,1, the (1, 1) block of C0, into
2 Ã— 2 blocks and compress its oï¬€-diagonal block. Suppose the resulting oï¬€-diagonal
block of C0;1,1 has size k Ã— ( N
2 âˆ’l). We will compress it by shifting certain parts of
X1 and Y1. That is, we partition X1 and Y1 conformally as
X1 =

X1,1
X1,2
	
, Y1 =

Y1,1
Y1,2
	
,
and also pick a k Ã— ( N
2 âˆ’l) block from the lower left corner of C0;1,2 = X1Y H
1
(that
is, X1,2Y H
1,1)
C0 =
C0;1,1
X1Y H
1
C0;2,1
C0;2,2
	
=
â›
âœ
âœ
âœ
âœ
âœ
âœ
âœ
âœ
âœ
âœ
âœ
âœ
âœ
â
1
Ï‰2âˆ’Ï‰2l+3
Â· Â· Â·
1
Ï‰2âˆ’Ï‰N+1
...
Â· Â· Â·
...
1
Ï‰2kâˆ’Ï‰2l+3
Â· Â· Â·
1
Ï‰2kâˆ’Ï‰N+1
1
Ï‰2k+2âˆ’Ï‰N+3
Â· Â· Â·
1
Ï‰2k+2âˆ’Ï‰2(Nâˆ’l)+1
...
Â· Â· Â·
...
1
Ï‰4kâˆ’Ï‰N+3
Â· Â· Â·
1
Ï‰N âˆ’Ï‰2(Nâˆ’l)+1
C0;2,1
C0;2,2
â
âŸ
âŸ
âŸ
âŸ
âŸ
âŸ
âŸ
âŸ
âŸ
âŸ
âŸ
âŸ
âŸ
â 
=
â›
âœ
â
X2Y H
2
X1,1Y H
1,1
X1,1Y H
1,2
X1,2Y H
1,1
X1,2Y H
1,2
C0;2,1
C0;2,2
â
âŸ
â ,

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Copyright Â© by SIAM. Unauthorized reproduction of this article is prohibited. 
SUPERFAST ALGORITHM FOR TOEPLITZ SYSTEMS
1261
where X2Y H
2
is an unknown compression of the upper right submatrix of C0;1,1, and
the blocks that donâ€™t concern us are left blank. At this point we do not need another
factorization to get X2Y H
2 ; instead we can directly derive the compression X2Y H
2
of
C0;1,1 from X1,2 and Y1,1. Clearly we have
X2Y H
2
=
â›
âœ
â
1
Ï‰2âˆ’Ï‰2l+3
Â· Â· Â·
1
Ï‰2âˆ’Ï‰N+1
...
Â· Â· Â·
...
1
Ï‰2kâˆ’Ï‰2l+3
Â· Â· Â·
1
Ï‰2kâˆ’Ï‰N+1
â
âŸ
â ,
X1,2Y H
1,1
=
â›
âœ
â
1
Ï‰2k+2âˆ’Ï‰N+3
Â· Â· Â·
1
Ï‰2k+2âˆ’Ï‰2(Nâˆ’l)+1
...
Â· Â· Â·
...
1
Ï‰4kâˆ’Ï‰N+3
Â· Â· Â·
1
Ï‰Nâˆ’Ï‰2(Nâˆ’l)+1
â
âŸ
â =
1
Ï‰2k X2Y H
2 .
That means we can get X2Y H
2
by shifting a subblock of X1Y H
1 . A similar situation
holds for the splitting of the (2, 2) block of C after the ï¬rst splitting. For the successive
compressions in later splittings, a similar shifting can be used. For splitting with
general block sizes the shifting is also similar.
The shifting scheme indicates that, in diï¬€erent levels of divide-and-conquer SSS
constructions, the compressions of large blocks and small blocks are related. This can
be used to further save the compression cost.
5. Performance and numerical experiments. It is well known that the FFT
transformation of the Toeplitz matrix T to a Cauchy-like matrix C, and the recovery
from the solution Ëœx of the Cauchy-like system to the solution x of the Toeplitz system,
are stable. In addition, the fast divide-and-conquer construction and the SSS system
solver in section 3 are both practically stable as we will see in our numerical results.
Thus our overall algorithm is stable in practice. Here no extra steps are needed to
stabilize the algorithm as required in some other superfast methods [38].
After the precomputations discussed in sections 4.2 and 4.3 are ï¬nished, the cost
of each SSS construction is only O(Np2), where p is the maximum of the oï¬€-diagonal
ranks of the Cauchy matrix C0. The SSS solver also costs O(Np2). The total cost
is thus no more than O(N log N) + O(Np2) ï¬‚ops.
The total storage requirement
is O(Np). For the convenience of coding, we use an O(N 2p) cost precomputation
routine as discussed in subsection 4.2.
A preliminary implementation of our superfast solver in Fortran 90 is available
at http://www.math.ucla.edu/Ëœjxia/work/toep. We did experiments on an Itanium 2
1.4 GHz SGI Altix 350 server with a 64-bit Linux operating system and Intel MKL
BLAS. Our method (denoted by NEW) is compared to Gaussian elimination with partial
pivoting (denoted by GEPP) (via the LAPACK linear system solver ZGESV [3]). We
consider real N Ã— N Toeplitz matrices T whose entries are random and uniformly
distributed in [0, 1].
The right-hand sides b are obtained by b = Tx, where the
exact solution x has random entries uniformly distributed in [âˆ’1, 1].
Matrices of
size N = 2k Ã— 100, k = 2, 3, . . . , are considered. These matrices are moderately ill-
conditioned. For example, for N = 2k Ã—100 with k = 2, . . . , 7, the one-norm condition
numbers of the matrices increase from the order of about 103 to 106.
For the NEW solver two important parameters are involved, the SSS block size d
(Figure 2.1) and the tolerance Ï„ in the compressions of oï¬€-diagonal blocks. Here we
use the Ï„-accurate QR factorization with Ï„ as a relative tolerance (section 2.2).
Figure 5.1 shows the execution time (in seconds) for GEPP and our NEW solver
with diï¬€erent d and Ï„. For the NEW solver only the time for solving the Cauchy-like

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Copyright Â© by SIAM. Unauthorized reproduction of this article is prohibited. 
1262
CHANDRASEKARAN, GU, SUN, XIA, AND ZHU
2
4
6
8
10
12
10
âˆ’2
10
âˆ’1
10
0
10
1
10
2
10
3
10
4
Execution time
k
time (seconds)
GEPP
NEW(Ï„=10âˆ’9, d=100)
NEW(Ï„=10âˆ’4, d=50)
Fig. 5.1. Computation time (in seconds) versus N = 2k Ã— 100. For the NEW solver, time for the
precomputation is excluded.
2
4
6
8
10
12
10
âˆ’2
10
âˆ’1
10
0
10
1
10
2
10
3
10
4
Precomputation time
k
time (seconds)
NEW(Ï„=10âˆ’9, d=100)
NEW(Ï„=10âˆ’4, d=50)
Fig. 5.2. Time (in seconds) for precomputations in the NEW solver.
system (2.3) is reported, as the compressions of oï¬€-diagonal blocks can be done in the
precomputation, as described in section 4.3, and the SSS construction time is nearly
the precomputation time. The precomputation time is shown in Figure 5.2, although
the precomputation only needs to be done once for a particular matrix size N.
We also consider the time scaling factor, that is, the factor by which the time is
multiplied when the matrix size doubles; see Figure 5.3. We observe that the time
scaling factors for the NEW solver are near 2, that is to say the NEW solver is close to
being a linear time solver.
Figures 5.4 and 5.5 present the errors Îµ1 = ||Ë†xâˆ’x||2
||x||2
and Îµ2 =
||T Ë†xâˆ’b||2
|||T ||Ë†x|+|b|||2 , respec-
tively, where Ë†x denotes the numerical solution.
A signiï¬cant point of the new solver is that we can use a relatively large tolerance
Ï„ in the precomputation and solving, and then use iterative reï¬nement to improve the
accuracy. A relatively large Ï„ leads to relatively small oï¬€-diagonal ranks (and thus

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Copyright Â© by SIAM. Unauthorized reproduction of this article is prohibited. 
SUPERFAST ALGORITHM FOR TOEPLITZ SYSTEMS
1263
2
4
6
8
10
12
1
2
3
4
5
6
7
8
9
10
Time scaling factor
k
time scaling factor
GEPP
NEW(Ï„=10âˆ’9, d=100)
NEW(Ï„=10âˆ’4, d=50)
Fig. 5.3. Time scaling factors.
1
2
3
4
5
6
7
8
9
10
11
10
âˆ’16
10
âˆ’14
10
âˆ’12
10
âˆ’10
10
âˆ’8
10
âˆ’6
10
âˆ’4
10
âˆ’2
Îµ1
k
relative error
NEW(Ï„=10âˆ’9, d=100)
GEPP
Fig. 5.4. Îµ1 = ||Ë†xâˆ’x||2
||x||2
versus N = 2k Ã— 100.
1
2
3
4
5
6
7
8
9
10
11
10
âˆ’16
10
âˆ’14
10
âˆ’12
10
âˆ’10
10
âˆ’8
10
âˆ’6
10
âˆ’4
10
âˆ’2
Îµ2
k
relative residual error
NEW(Ï„=10âˆ’9, d=100)
GEPP
Fig. 5.5. Îµ2 =
||T Ë†xâˆ’b||2
|| |T | |Ë†x|+|b| ||2 versus N = 2k Ã— 100.

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Copyright Â© by SIAM. Unauthorized reproduction of this article is prohibited. 
1264
CHANDRASEKARAN, GU, SUN, XIA, AND ZHU
small p in the operation counts above). Iterative reï¬nements are very cheap due to the
facts that no precomputation is needed and the solver itself is very fast (Figure 5.1).
For Ï„ = 10âˆ’4 and d = 50 the accuracy results after 2 to 8 steps of iterative reï¬nement
are displayed in Figure 5.6. In fact the number of iterative reï¬nement steps required
to reach Îµ2 < 10âˆ’13 is listed in Table 5.1.
Thus it is also clear that our NEW solver can also perform well as a preconditioner.
The block size d also aï¬€ects the performance. Figure 5.7 indicates that for a
1
2
3
4
5
6
7
8
9
10
10
âˆ’18
10
âˆ’16
10
âˆ’14
10
âˆ’12
10
âˆ’10
10
âˆ’8
10
âˆ’6
10
âˆ’4
10
âˆ’2
Îµ2
k
relative residual error
initial
2 steps
4 steps
6 steps
8 steps
Fig. 5.6. Îµ2 =
||T Ë†xâˆ’b||2
|| |T | |Ë†x|+|b| ||2 , initial values (before iterative reï¬nements), and after some steps
of iterative reï¬nement.
0
1
2
3
4
5
6
0
10
20
30
40
50
60
70
80
Computation time for NEW solver with different d=2jÃ—25
j
time (seconds)
precomputation + solver time
solver time
Fig. 5.7. Computation time of the NEW solver with N = 12800, Ï„ = 10âˆ’9, and diï¬€erent block
size d = 2j Ã— 25.
Table 5.1
Number of iterative reï¬nement steps required to reach Îµ2 < 10âˆ’13 for diï¬€erent matrix dimen-
sions N = 2k Ã— 100.
k
2
3
4
5
6
7
8
9
Number of steps
4
4
5
6
7
15
9
21

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Copyright Â© by SIAM. Unauthorized reproduction of this article is prohibited. 
SUPERFAST ALGORITHM FOR TOEPLITZ SYSTEMS
1265
given N if d = 2j Ã— 25 is too large or too small, then both the solver time and the
precomputation time can be relatively large. In fact by counting the operations in the
algorithm it is possible to determine an optimal d; see [12] for the derivation of the
optimal choice of d in the SSS solver. We can do similar derivations for both the SSS
construction and the SSS solver. We just point out that when Ï„ gets larger, the oï¬€-
diagonal ranks decrease and a smaller d should be chosen. In the previous experiments
we used two sets of parameters: (Ï„ = 10âˆ’9, d = 100) and (Ï„ = 10âˆ’4, d = 50).
Finally, it turns out that our current preliminary implementation of the solver is
slower than the implementation of the algorithm in [38], likely due to our ineï¬ƒcient
data structure and nonoptimized codes for SSS matrices. The complicated nature
of SSS matrices needs more careful coding and memory management. An improved
software implementation is under construction.
6. Conclusions and future work. In this paper we have presented a super-
fast and practically stable solver for Toeplitz systems of linear equations. A Toeplitz
matrix is ï¬rst transformed into a Cauchy-like matrix, which has a nice low-rank prop-
erty, and then the Cauchy-like system is solved by a superfast solver. This superfast
solver utilizes this low-rank property and makes use of an SSS representation of the
Cauchy-like matrix. A fast construction procedure for SSS structures is presented.
After a one-time precomputation the solver is very eï¬ƒcient (O(N log N) + O(Np2)
complexity). Also the algorithm is eï¬ƒcient in that only linear storage is required. In
future work we hope to further reduce the precomputation cost and ï¬nish a better-
designed version of the current Fortran 90 codes in both the computations and the
coding.
Acknowledgments. Many thanks to the referees for their valuable comments
which have greatly improved the development and presentation of this paper. The
authors are also grateful to the editors and the editorial assistant for their patience
and thoughtful suggestions.
REFERENCES
[1] G. S. Ammar and W. B. Gragg, Superfast solution of real positive deï¬nite Toeplitz systems,
SIAM J. Matrix Anal. Appl., 9 (1988), pp. 61â€“76.
[2] G. S. Ammar and W. B. Gragg, Numerical experience with a superfast real Toeplitz solver,
Linear Algebra Appl., 121 (1989), pp. 185â€“206.
[3] E. Anderson, Z. Bai, C. Bischof, J. Demmel, J. Dongarra, J. Du Croz, A. Greenbaum,
S. Hammarling, A. McKenney, S. Ostrouchov, and D. Sorensen, LAPACK Usersâ€™
Guide, 2nd ed., SIAM, Philadelphia, PA, 1994.
[4] R. R. Bitmead and B. D. O. Anderson, Asymptotically fast solution of Toeplitz and related
systems of linear equations, Linear Algebra Appl., 34 (1980), pp. 103â€“116.
[5] A. W. Bojanczyk, R. P. Brent, F. R. de Hoog, and D. R. Sweet, On the stability of
the Bareiss and related Toeplitz factorization algorithms, SIAM J. Matrix Anal. Appl., 16
(1995), pp. 40â€“57.
[6] R. P. Brent, F. G. Gustavson, and D. Y. Y. Yun, Fast solution of Toeplitz systems of
equations and computation of PadÂ´e approximants, J. Algorithms, 1 (1980), pp. 259â€“295.
[7] J. R. Bunch, Stability of methods for solving Toeplitz systems of equations, SIAM J. Sci.
Statist. Comput., 6 (1985), pp. 349â€“364.
[8] J. Carrier, L. Greengard, and V. Rokhlin, A fast adaptive multipole algorithm for particle
simulations, SIAM J. Sci. Statist. Comput., 9 (1988), pp. 669â€“686.
[9] T. F. Chan and P. C. Hansen, A look-ahead Levinson algorithm for general Toeplitz systems,
IEEE Trans. Signal Process., 40 (1992), pp. 1079â€“1090.
[10] T. F. Chan and P. C. Hansen, A look-ahead Levinson algorithm for indeï¬nite Toeplitz sys-
tems, SIAM J. Matrix Anal. Appl., 13 (1992), pp. 490â€“506.
[11] S. Chandrasekaran and M. Gu, Fast and stable algorithms for banded plus semiseparable
matrices, SIAM J. Matrix Anal. Appl., 25 (2003), pp. 373â€“384.

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Copyright Â© by SIAM. Unauthorized reproduction of this article is prohibited. 
1266
CHANDRASEKARAN, GU, SUN, XIA, AND ZHU
[12] S. Chandrasekaran, P. Dewilde, M. Gu, T. Pals, X. Sun, A.-J. van der Veen, and
D. White, Fast Stable Solvers for Sequentially Semi-Separable Linear Systems of Equa-
tions and Least Squares Problems, Technical report, University of California, Berkeley,
CA, 2003.
[13] S. Chandrasekaran, P. Dewilde, M. Gu, T. Pals, X. Sun, A.-J. van der Veen, and
D. White, Some fast algorithms for sequentially semiseparable representations, SIAM J.
Matrix Anal. Appl., 27 (2005), pp. 341â€“364.
[14] S. Chandrasekaran, M. Gu, and W. Lyons, A Fast and Stable Adaptive Solver for Hierarchi-
cally Semi-Separable Representations, Technical report, UCSB Math 2004-20, University
of California, Santa Barbara, CA, 2004.
[15] G. Cybenko, The numerical stability of the Levinson-Durbin algorithm for Toeplitz systems
of equations, SIAM J. Sci. Statist. Comput., 1 (1980), pp. 303â€“319.
[16] G. Cybenko, Error Analysis of Some Signal Processing Algorithms, Ph.D. thesis, Princeton
University, Princeton, NJ, 1978.
[17] F. R. deHoog, On the solution of Toeplitz systems, Linear Algebra Appl., 88/89 (1987),
pp. 123â€“138.
[18] P. Dewilde and A. van der Veen, Time-Varying Systems and Computations, Kluwer
Academic Publishers, Boston, MA, 1998.
[19] M. Fiedler, Hankel and Loewner matrices, Linear Algebra Appl., 58 (1984), pp. 75â€“95.
[20] K. A. Gallivan, S. Thirumalai, P. Van Dooren, and V. Vermaut, High performance
algorithms for Toeplitz and block Toeplitz matrices, Linear Algebra Appl., 241/243 (1996),
pp. 343â€“388.
[21] L. Gemignani, Schur complements of Bezoutians and the inversion of block Hankel and block
Toeplitz matrices, Linear Algebra Appl., 253 (1997), pp. 39â€“59.
[22] I. Gohberg, T. Kailath, and V. Olshevsky, Fast Gaussian elimination with partial pivoting
for matrices with displacement structure, Math. Comp., 64 (1995), pp. 1557â€“1576.
[23] I. Gohberg and V. Olshevsky, Fast state space algorithms for matrix Nehari and Nehari-
Takagi interpolation problems, Integral Equations Operator Theory, 20 (1994), pp. 44â€“83.
[24] I. Gohberg and V. Olshevsky, Complexity of multiplication with vectors for structured ma-
trices, Linear Algebra Appl., 202 (1994), pp. 163â€“192.
[25] L. Greengard and V. Rokhlin, A fast algorithm for particle simulations, J. Comput. Phys.,
73 (1987), pp. 325â€“348.
[26] M. Gu, Stable and eï¬ƒcient algorithms for structured systems of linear equations, SIAM J.
Matrix Anal. Appl., 19 (1998), pp. 279â€“306.
[27] M. Gu and S. C. Eisenstat, Eï¬ƒcient algorithms for computing a strong rank-revealing QR
factorization, SIAM J. Sci. Comput., 17 (1996), pp. 848â€“869.
[28] G. Heinig, Inversion of generalized Cauchy matrices and other classes of structured matrices,
in Linear Algebra for Signal Processing, IMA Vol. Math. Appl. 69, Springer, New York,
1995, pp. 63â€“81.
[29] G. Heinig and K. Rost, Algebraic Methods for Toeplitz-like Matrices and Operators, Oper.
Theory Adv. Appl. 13, BirkhÂ¨auser Verlag, Basel, 1984, pp. 109â€“127.
[30] T. Kailath, Fredholm resolvents, Wiener-Hopf equations, and Riccati diï¬€erential equations,
IEEE Trans. Inform. Theory, 15 (1969), pp. 665â€“672.
[31] T. Kailath, S. Kung, and M. Morf, Displacement ranks of matrices and linear equations,
J. Math. Anal. Appl., 68 (1979), pp. 395â€“407.
[32] T. Kailath and A. H. Sayed, eds., Fast Reliable Algorithms for Matrices with Structure,
SIAM, Philadelphia, 1999.
[33] P. G. Martinsson, V. Rokhlin, and M. Tygert, A fast algorithm for the inversion of general
Toeplitz matrices, Comput. Math. Appl., 50 (2005), pp. 741â€“752.
[34] M. Morf, Fast Algorithms for Multivariable Systems, Ph.D. thesis, Department of Electrical
Engineering, Stanford University, Stanford, CA, 1974.
[35] B. R. Musicus, Levinson and Fast Choleski Algorithms for Toeplitz and Almost Toeplitz
Matrices, Technical report, Res. Lab. of Electronics, M.I.T., Cambridge, MA, 1984.
[36] V.
Pan,
On computations with dense structured matrices,
Math. Comp.,
55 (1990),
pp. 179â€“190.
[37] D. R. Sweet, The use of pivoting to improve the numerical performance of algorithms for
Toeplitz matrices, SIAM J. Matrix Anal. Appl., 14 (1993), pp. 468â€“493.
[38] M. Van Barel, G. Heinig, and P. Kravanja, A stabilized superfast solver for nonsymmetric
Toeplitz systems, SIAM J. Matrix Anal. Appl., 23 (2001), pp. 494â€“510.
[39] M. Van Barel and P. Kravanja, A stabilized superfast solver for indeï¬nite Hankel systems,
Linear Algebra Appl., 284 (1998), pp. 335â€“355.

