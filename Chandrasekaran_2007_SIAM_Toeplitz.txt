 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Copyright © by SIAM. Unauthorized reproduction of this article is prohibited. 
SIAM J. MATRIX ANAL. APPL.
c
⃝2007 Society for Industrial and Applied Mathematics
Vol. 29, No. 4, pp. 1247–1266
A SUPERFAST ALGORITHM FOR TOEPLITZ SYSTEMS OF
LINEAR EQUATIONS∗
S. CHANDRASEKARAN†, M. GU‡, X. SUN§, J. XIA¶, AND J. ZHU‡
Abstract. In this paper we develop a new superfast solver for Toeplitz systems of linear equa-
tions. To solve Toeplitz systems many people use displacement equation methods. With displacement
structures, Toeplitz matrices can be transformed into Cauchy-like matrices using the FFT or other
trigonometric transformations. These Cauchy-like matrices have a special property, that is, their
oﬀ-diagonal blocks have small numerical ranks. This low-rank property plays a central role in our
superfast Toeplitz solver. It enables us to quickly approximate the Cauchy-like matrices by structured
matrices called sequentially semiseparable (SSS) matrices. The major work of the constructions of
these SSS forms can be done in precomputations (independent of the Toeplitz matrix entries). These
SSS representations are compact because of the low-rank property. The SSS Cauchy-like systems
can be solved in linear time with linear storage. Excluding precomputations the main operations are
the FFT and SSS system solve, which are both very eﬃcient. Our new Toeplitz solver is stable in
practice. Numerical examples are presented to illustrate the eﬃciency and the practical stability.
Key words. displacement equation, SSS structure, superfast algorithm, Toeplitz matrix
AMS subject classiﬁcations. 15A06, 65F05, 65G05
DOI. 10.1137/040617200
1. Introduction. Toeplitz systems of linear equations arise in many applica-
tions, including PDE solving, signal processing, time series analysis, orthogonal poly-
nomials, and many others. A Toeplitz system is a linear system
(1.1)
Tx = b
with a coeﬃcient matrix to be a Toeplitz matrix
(1.2)
T =
⎛
⎜
⎜
⎜
⎜
⎜
⎝
t0
t−1
t−2
· · ·
t−(N−1)
t1
t0
t−1
· · ·
t−(N−2)
t2
t1
t0
· · ·
t−(N−3)
...
...
...
...
...
tN−1
tN−2
tN−3
· · ·
· · ·
⎞
⎟
⎟
⎟
⎟
⎟
⎠
,
that is, its entries are constant along every diagonal (a matrix whose entries are con-
stant along every antidiagonal is called a Hankel matrix). The vector t = (t−(N−1) · · ·
t−1 t0 t1 · · · tN−1) is called the Toeplitz-vector that generates T.
There are both direct and iterative methods for solving (1.1). Direct solvers are
said to be fast if they cost O(N 2) operations; examples include Schur-type methods,
∗Received by the editors October 19, 2004; accepted for publication (in revised form) by D. A.
Bini May 8, 2007; published electronically December 13, 2007.
http://www.siam.org/journals/simax/29-4/61720.html
†Department of Electrical and Computer Engineering, University of California at Santa Barbara,
Santa Barbara, CA 93106 (shiv@ece.ucsb.edu).
‡Department of Mathematics, University of California at Berkeley, Berkeley, CA 94720 (mgu@
math.berkeley.edu, zhujiang.cal@gmail.com).
§Department of Mechanical Engineering, University of California at Berkeley, Berkeley, CA 94720
(sunxt@cal.berkeley.edu).
¶Department of Mathematics, University of California at Los Angeles, Los Angeles, CA 90095
(jxia@math.ucla.edu).
1247

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Copyright © by SIAM. Unauthorized reproduction of this article is prohibited. 
1248
CHANDRASEKARAN, GU, SUN, XIA, AND ZHU
Levinson-type methods, and others [32]. An important type of direct solver is the
displacement equation–type fast solver based on Gaussian eliminations. Some known
displacement equation–type methods are the Heinig [28], GKO [22], and Gu [26]
methods. Those methods have complexity O(N 2). Methods with complexity less than
O(N 2) are called superfast. In this paper we will present a displacement equation–
type superfast algorithm.
1.1. Fast and superfast methods. Many fast and superfast methods that have
been developed are numerically unstable [5, 15, 16, 7, 37]. References [5] and [15]
showed that the Schur algorithm and the Levinson algorithm are weakly stable in
some cases, but both may be highly unstable in the case of an indeﬁnite and non-
symmetric matrix.
Stable generalized Schur algorithms [32] and look-ahead algo-
rithms were developed in [9, 10].
High-performance look-ahead Schur algorithms
were presented [20].
Many other solvers use the FFT or other trigonometric transforms to convert the
Toeplitz (or even Hankel or Toeplitz-plus-Hankel) matrices into generalized Cauchy
or Vandermonde matrices, which can be done stably in O(N log N) operations. This
is also the approach that we will use in this paper, with the aid of a displacement
structure.
The concept of displacement structure was ﬁrst introduced in [31]. The Sylvester-
type displacement equation for a matrix ˆC ∈RN×N [29] is
(1.3)
Ω ˆC −ˆCΛ = UV,
where Ω, Λ ∈RN×N, U ∈RN×α, V ∈Rα×N, and α ≤N is the displacement rank
with respect to Ω and Λ if rank(UV ) = α. The matrix ˆC is considered to possess a
displacement structure with respect to Ω and Λ if α ≪N.
With displacement structures it was shown in [19, 24, 36, 22, 28] that Toeplitz
and Hankel matrices can be transformed into Cauchy-like matrices of the following
form:
ˆC =
 uT
i · vj
ηi −λj
	
1≤i,j≤N
(ui, vj ∈Rα) ,
where we assume that ηi ̸= λj for 1 ≤i, j ≤N. Equivalently, a Cauchy-like matrix is
the unique solution to the displacement equation (1.3) with
Ω = diag(η1, . . . , ηn), Λ = diag(λ1, . . . , λn), U =
⎛
⎜
⎝
uT
1
...
uT
n
⎞
⎟
⎠, and V = (v1, . . . , vn) .
In particular, ˆC is a Cauchy matrix if uT
i vj = 1 for all i and j. The displacement
rank of ˆC is at most α.
To solve Toeplitz systems through Cauchy-like matrices, many people have uti-
lized matrix factorizations. Gohberg and Olshevsky [23] presented a fast variation of
the straightforward Gaussian elimination with partial pivoting (GEPP) procedure to
solve a Cauchy-like linear system of equations in O(N 2) operations. Among other re-
sults, Gohberg, Kailath, and Olshevsky [22] developed algorithm GKO, an improved
version of Heinig’s algorithm [28], and demonstrated numerically that it is stable.
In their algorithm, the Hankel matrix and the Toeplitz-plus-Hankel matrix are also
transformed via fast trigonometric transforms into Cauchy-like matrices.

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Copyright © by SIAM. Unauthorized reproduction of this article is prohibited. 
SUPERFAST ALGORITHM FOR TOEPLITZ SYSTEMS
1249
Gu presented a modiﬁed algorithm in [26] to avoid extra error growth.
This
algorithm is numerically stable, provided that the element growth in the computed
factorization is not large. The algorithm takes O(N 2) operations and is a fast stable
method.
Superfast algorithms appeared in [34, 4, 6, 17, 35, 1, 2, 21] and many others.
Superfast algorithms use divide-and-conquer strategies. Morf developed the ﬁrst idea
in [34]. These methods are unstable for nonsymmetric systems as they cannot deal
with nearly singular leading principal submatrices.
Van Barel and Kravanja presented a superfast method for rational interpolation
at roots of unity [39]. A similar idea was then applied to Toeplitz systems [38]. It
provided an explicit formula for the inverse of a Toeplitz matrix. Additional tech-
niques such as iterative reﬁnement and downdating were still required to stabilize
their algorithm.
1.2. Main results. Our new Toeplitz solver is also of the displacement equation
type. Given a Toeplitz linear system, we ﬁrst use the FFT to transform the associated
Toeplitz matrix into a Cauchy-like matrix. Then instead of using matrix factorizations
which often cost O(N 2) or more, we exploit a special low-rank property of Cauchy-like
matrices, that is, every oﬀ-diagonal block of a Cauchy-like matrix has a low numerical
rank. Using this low-rank property, we then approximate the Cauchy-like matrix by a
low-rank matrix structure called the sequentially semiseparable (SSS) matrix proposed
by Chandrasekaran et al. [12, 13]. A system with the coeﬃcient matrix in compact SSS
form can be solved with only O(p2N) operations, where N is the matrix dimension,
and p is the complexity of the semiseparable description. The SSS solver is practically
stable in our numerical tests and those in [12, 13].
The SSS structure was developed to capture the low-rank property of the oﬀ-
diagonal blocks of a matrix and to maintain stability or practical stability in the
mean time. It is a matrix analog of semiseparable integral kernels in Kailath’s pa-
per [30]. Matrix operations with compact form SSS representations are very eﬃcient,
provided that such compact representations exist or can be easily computed. This
turns out to be true for our case, as the Cauchy-like matrices are transformed from
Toeplitz matrices. We use a recursive compression scheme with a shifting strategy to
construct compact SSS forms for those Cauchy-like matrices. The major work of the
compressions can be precomputed on some Cauchy matrices which are independent
of the actual Toeplitz matrix entries.
The overall algorithm thus has the following stages:
(1) Precompute compressions of oﬀ-diagonal blocks of Cauchy matrices.
(2) Transform the Toeplitz matrix into a Cauchy-like matrix in O(N log N)
operations.
(3) Construct a compact SSS representation from precomputed compressions and
solve the Cauchy-like matrix system O(p2N) operations.
(4) Recover the solution of the Toepliz system in O(N log N) operations.
The stages above are either stable or practically stable. Our numerical results
indicate that the overall algorithm is stable in practice. The Toeplitz matrix does not
have to be symmetric or positive deﬁnite, and no extra stabilizing step is necessary.
After the precomputations, the total cost for the algorithm is O(N log N) + O(p2N).
This indicates that the entire algorithm is superfast.
We also point out that similar techniques are used in [33], where the low-rank
property is exploited through the block columns without diagonals (called neutered
block columns in [33]), in contrast with the oﬀ-diagonal blocks here. The compres-

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Copyright © by SIAM. Unauthorized reproduction of this article is prohibited. 
1250
CHANDRASEKARAN, GU, SUN, XIA, AND ZHU
sions of either the neutered blocks or the oﬀ-diagonal blocks both give data-sparse
representations which enable fast factorizations of the Cauchy-like matrices. In fact,
corresponding to neutered block rows or columns, there are also matrix representa-
tions called hierarchically semiseparable (HSS) matrices [14] which are usually more
complicated structures than SSS matrices.
1.3. Overview. We will discuss the displacement structure and the transforma-
tion from a Toeplitz problem to a Cauchy-like problem in section 2. The low-rank
property of this Cauchy-like problem is then exploited. Section 3 then gives a linear
complexity solver using the SSS structure. In section 4, we will present an algorithm
for fast construction of SSS structures. We will then analyze the complexity in sec-
tion 5 and use some numerical experiments to demonstrate the eﬃciency and the
practical stability. All algorithms have been implemented in Fortran 90. Section 6
draws some conclusions.
2. Displacement structures and low-rank property.
2.1. Cauchy-like systems. Given a Toeplitz system (1.1), we can use a dis-
placement structure to transform it into a Cauchy-like system. Deﬁne
Zδ =
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎝
0
0
· · ·
0
δ
1
0
· · ·
· · ·
0
0
1
...
...
...
...
...
...
0
· · ·
0
1
0
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎠
,
and let Ω = Z1 and Λ = Z−1 in (1.3). Kailath, Kung, and Morf [31] have shown that
every Toeplitz matrix satisﬁes the displacement equation (1.3) with A · B, having
nonzero entries only in its ﬁrst row and last column, to be a matrix of rank at most
2. Hence the displacement rank of a Toeplitz matrix is at most 2 with respect to Z1
and Z−1. The following result can be found in [28].
Proposition 2.1. Let ˆC ∈RN×N be a matrix satisfying the displacement equa-
tion
(2.1)
Z1 ˆC −ˆCZ−1 = UV,
where U ∈Rn×α and V ∈Rα×n. Then F ˆCD−1
0 FH is a Cauchy-like matrix satisfying
(2.2)
D1(F ˆCD−1
0 FH) −(F ˆCD−1
0 FH)D−1 = (FU)

V DH
0 FH
,
where F =

1
N (ω2(k−1)(j−1))1≤k,j≤N is the normalized inverse discrete Fourier trans-
form matrix, ω = e
πi
N , and
D1 = diag(1, ω2, . . . , ω2(N−1)),
D−1 = diag(ω, ω3, . . . , ω2N−1),
D0 = diag(1, ω, . . . , ωN−1).
Here α ≤2. This proposition suggests that for a Toeplitz matrix T, one can
convert it into the Cauchy-like matrix in (2.2). Therefore the Toeplitz system (1.1)
can be readily transformed into a new system
(2.3)
C˜x = ˜b,

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Copyright © by SIAM. Unauthorized reproduction of this article is prohibited. 
SUPERFAST ALGORITHM FOR TOEPLITZ SYSTEMS
1251
where C has the form
(2.4)
C =

uT
i vj
ω2i −ω2j+1
	
1≤i,j≤N
(ui, vj ∈Rα) .
In section 3 we will present a fast solver for (2.3).
After obtaining ˜x we will
then recover x with an FFT again.
All the stages involving FFT are stable and
cost O(N log N). The solver for (2.3) has a linear complexity and turns out to be
practically stable. Thus the total cost of our algorithm is bounded by O(N log N) +
O(Np2), where p is some parameter that will be described below. This indicates our
method is a superfast one with practical stability.
2.2. Low-rank property of Cauchy-like matrices. In this section, we will
show a low-rank property of C, i.e., every oﬀ-diagonal block of C has a low numerical
rank. This property is the basis of the superfast SSS solver in section 3.
First, a simple numerical experiment can give us an idea of the low-rank property
of C. To ﬁnd out the numerical ranks we can use one of the following tools:
(1) τ-accurate SVD: singular values less than τ are dropped if τ is an absolute
tolerance, or singular values less than τ times the largest singular value are
dropped if τ is a relative tolerance.
(2) τ-accurate QR:
A ≈QR, A : m × n, Q : m × k, R : k × k, k ≤l ≡min(m, n),
which is obtained in the following way. Compute the exact QR factorization
of matrix A = ˆQ ˆR, where ˆQ is m × l and ˆR is l × n with diagonal entries
satisfying ˆR11 ≥ˆR22 ≥· · · ≥ˆRll. Then obtain R by dropping all rows of ˆR
with diagonal entries less than τ if τ is an absolute tolerance, or with diagonal
entries less than τ ˆR11 if τ is a relative tolerance. Drop relevant columns of
ˆQ accordingly to obtain Q.
Later, by ranks we mean numerical ranks. Here we take some random Toeplitz
matrices in diﬀerent sizes. Then we transform them into Cauchy-like matrices C and
compute the numerical ranks of their oﬀ-diagonal blocks. For simplicity, we compute
the ranks for blocks C(1 : d, d + 1 : N), C(1 : 2d, 2d + 1 : N), . . ., C(1 : kd, kd + 1 :
N), . . ., where d is a ﬁxed integer, and these blocks are numbered as block numbers
1, 2, . . . , k as shown in Figure 2.1.
Here, k = 8 oﬀ-diagonal blocks for each of three N × N Cauchy-like matrices are
considered. See Table 2.1 for the numerical ranks, where we use the τ-accurate SVD
with τ to be an absolute tolerance.
We can see that the numerical ranks are relatively small as compared to the block
sizes. And when we double the dimension of the matrix, the numerical ranks do not
increase much. This is more signiﬁcant when a larger τ is used.
Fig. 2.1. Oﬀ-diagonal blocks (numbered as 1, 2, 3). Upper triangular part only.

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Copyright © by SIAM. Unauthorized reproduction of this article is prohibited. 
1252
CHANDRASEKARAN, GU, SUN, XIA, AND ZHU
Table 2.1
Oﬀ-diagonal numerical ranks of Cauchy-like matrices transformed from some Toeplitz matrices
with absolute tolerance τ = 10−9.
N = 640
N = 1280
N = 2560
Block #
Block size
Rank
Block size
Rank
Block size
Rank
1
80 × 560
37
160 × 1120
44
320 × 2240
52
2
160 × 480
43
320 × 960
50
640 × 1920
57
3
240 × 400
45
480 × 800
53
960 × 1600
60
4
320 × 320
46
640 × 640
53
1280 × 1280
61
5
400 × 240
46
800 × 480
52
1600 × 960
60
6
480 × 160
43
960 × 320
50
1920 × 640
58
7
560 × 80
37
1120 × 160
44
2240 × 320
52
The low-rank property can be veriﬁed theoretically in the following way. We ﬁrst
consider a special case of (2.4) where all uT
i vj = 1. We show that the following Cauchy
matrix has low-rank oﬀ-diagonal blocks:
(2.5)
C0 =

1
ω2i −ω2j+1
	
1≤i,j≤N
The central idea is similar to that in the fast multipole method [25, 8], which im-
plies that with well-separated points (see, e.g., Figure 2.2) an interaction matrix is
numerically low-rank.
Here we introduce two sets of points {λk}N
k=1 ≡{ω2k}N
k=1 and {ηk}N
k=1 ≡{ω2k+1}N
k=1
on the unit circle. When we consider an oﬀ-diagonal block of C0 as follows:
G =

1
λi −ηj
	
1≤i≤p, p+1≤j≤N
we can show G is (numerically) low-rank. In fact G corresponds to two well-separated
sets {λk}p
k=1 and {ηj}N
k=p+1; that is, there exists a point c ∈C such that
(2.6)
|λi −c|
>
d + e,
i = 1, . . . , p,
|ηj −c|
<
d,
j = p + 1, . . . , N,
where d, e are positive constants. Consider the expansion
1
λi −ηj
=
1
λi −c
1
1 −ηj−c
λi−c
=
r

k=0
(ηj −c)k
(λi −c)k+1 + O
ηj −c
λi −c
	r+1
(2.7)
=
r

k=0
(ηj −c)k
(λi −c)k+1 + ε,
(2.8)
Fig. 2.2. Well-separated sets in the plane.

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Copyright © by SIAM. Unauthorized reproduction of this article is prohibited. 
SUPERFAST ALGORITHM FOR TOEPLITZ SYSTEMS
1253
where r is a number such that the error term |ε| = |O(( ηj−c
λi−c)r+1)| is bounded by a
given tolerance. We have the estimate

ηj −c
λi −c

r+1
<

d
d + e
	r+1
,
which enables us to ﬁnd an appropriate r according to the tolerance. Thus
G =
 r

k=0
(ηj −c)k
(λi −c)k+1

1≤i≤p, p+1≤j≤N
+ ˆε
(2.9)
=
⎛
⎜
⎜
⎜
⎝
1
λ1−c
1
(λ1−c)2
· · ·
1
(λ1−c)r+1
1
λ2−c
1
(λ2−c)2
· · ·
1
(λ2−c)r+1
...
...
· · ·
...
1
λp−c
1
(λp−c)2
· · ·
1
(λp−c)r+1
⎞
⎟
⎟
⎟
⎠
⎛
⎜
⎝
1
1
· · ·
1
(ηp+1−c)
(ηp+2−c)
· · ·
(ηN −c)
...
...
· · ·
...
(ηp+1−c)r
(ηp+2−c)r
· · ·
(ηN −c)r
⎞
⎟
⎠+ ˆε.
Therefore the numerical rank of G is at most r + 1, up to an error ˆε.
Now we can return to the Cauchy-like matrix (2.4). A similar argument shows
that any oﬀ-diagonal block of C satisﬁes
ˆG≈
 r

k=0
(uT
i vj) (ηj −c)k
(λi −c)k+1

1≤i≤p, p+1≤j≤N
=
⎛
⎜
⎜
⎜
⎜
⎜
⎝
uT
1
λ1−c
uT
1
(λ1−c)2
· · ·
uT
1
(λ1−c)r+1
uT
2
λ2−c
uT
2
(λ2−c)2
· · ·
uT
2
(λ2−c)r+1
...
...
· · ·
...
uT
p
λp−c
uT
p
(λp−c)2
· · ·
uT
p
(λp−c)r+1
⎞
⎟
⎟
⎟
⎟
⎟
⎠
⎛
⎜
⎝
v1
v2
· · ·
vq
(ηp+1−c) v1
(ηp+2−c) v2
· · ·
(ηN −c) vN
...
...
· · ·
...
(ηp+1−c)rv1
(ηp+2−c)rv2
· · ·
(ηN −c)rvN
⎞
⎟
⎠.
That is, we replace the entries of the two matrix factors in (2.9) with appropriate
vectors. Thus the numerical rank of ˆG will be no larger than α(r + 1), which will be
relatively small as compared to N.
3. SSS structures and superfast SSS solver.
3.1. SSS representations. To take advantage of the low-rank property of the
Cauchy-like matrix C, we can use SSS structures introduced by Chandrasekaran
et al. [12, 13]. The SSS structure nicely captures the ranks of oﬀ-diagonal blocks
of a matrix such as shown in Figure 2.1.
A matrix A ∈CM× ˜
M satisﬁes the SSS structure if there exist 2n positive integers
m1, . . . , mn, and ˜m1, . . . , ˜mn with M = m1 + · · · + mn and ˜
M = ˜m1 + · · · + ˜mn to
block-partition A as A = (Ai,j)k×k , where Aij ∈Cmi× ˜mj satisﬁes
(3.1)
Aij =
⎧
⎪
⎨
⎪
⎩
Di
if i = j,
UiWi+1 · · · Wj−1V H
j
if i < j,
PiRi−1 · · · Rj+1QH
j
if i > j.
Here the superscript H denotes the Hermitian transpose and empty products are
deﬁned to be identity matrices. The matrices {Ui}n−1
i=1 , {Vi}n
i=2, {Wi}n−1
i=2 , {Pi}n
i=2,

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Copyright © by SIAM. Unauthorized reproduction of this article is prohibited. 
1254
CHANDRASEKARAN, GU, SUN, XIA, AND ZHU
Table 3.1
Dimensions of matrices in (3.1).
Matrix
Ui
Vi
Wi
Pi
Qi
Ri
Dimensions
mi × ki
˜mi × ki−1
ki−1 × ki
mi × li
˜mi × li+1
li+1 × li
{Qi}n−1
i=1 , {Ri}n−1
i=2 , and {Di}n
i=1 are called generators for the SSS structure and their
dimensions are deﬁned in Table 3.1.
As an example, the matrix A with n = 4 has the form
(3.2)
A =
⎛
⎜
⎜
⎝
D1
U1V H
2
U1W2V H
3
U1W2W3V H
4
P2QH
1
D2
U2V H
3
U2W3V H
4
P3R2QH
1
P3QH
2
D3
U3V H
4
P4R3R2QH
1
P4R3QH
2
P4QH
3
D4
⎞
⎟
⎟
⎠.
The SSS representation (3.2) is related to the oﬀ-diagonal blocks in Figure 2.1 in
the way that the upper oﬀ-diagonal block numbers 1, 2, and 3 are
U1

V H
2
W2V H
3
W2W3V H
4

,

U1W2
U2
	 
V H
3
W3V H
4

,
⎛
⎝
U1W2W3
U2W3
U3
⎞
⎠V H
4 .
Appropriate row and column bases of the oﬀ-diagonal blocks are clearly reﬂected.
The SSS structure depends on the sequences {mi} and { ˜mi} and the SSS genera-
tion scheme. If A is a square matrix (M = ˜
M), then we can have a simpler situation
mi = ˜mi, i = 1, . . . , n. SSS matrices are closed under addition, multiplication, inver-
sion, etc., although the sizes of the generators may increase.
While any matrix can be represented in this form for suﬃciently large ki’s and
li’s, the column dimensions of Ui’s and Pi’s, respectively, our main focus will be
on SSS matrices which have low-rank oﬀ-diagonal blocks and have generators with
ki’s and li’s to be close to those ranks.
We say these SSS matrices are compact.
Particularly true for Cauchy-like matrices, they can have compact SSS forms. Using
SSS structures, we can take advantage of the superfast SSS system solver in [12, 13] to
solve the Cauchy-like systems. The solver is eﬃcient when the SSS form is compact,
and is practically stable. The solver shares similar ideas with that for banded plus
semiseparable systems in [11].
Here we brieﬂy describe the main ideas of the solver in [12, 13]. We consider
solving the linear system Ax = b, where A ∈CN×N satisﬁes (3.1) and b itself is an
unstructured matrix. The solver computes an implicit ULV H decomposition of A,
where U and V are orthogonal matrices.
Before we present the formal algorithm, we demonstrate the key ideas on a 4 × 4
block matrix example.
3.2. SSS solver: 4 × 4 example. Let the initial system Ax = b be partitioned
as follows:
(3.3)
⎛
⎜
⎜
⎝
D1
U1V H
2
U1W2V H
3
U1W2W3V H
4
P2QH
1
D2
U2V H
3
U2W3V H
4
P3R2QH
1
P3QH
2
D3
U3V H
4
P4R3R2QH
1
P4R3QH
2
P4QH
3
D4
⎞
⎟
⎟
⎠
⎛
⎜
⎜
⎝
x1
x2
x3
x4
⎞
⎟
⎟
⎠=
⎛
⎜
⎜
⎝
b1
b2
b3
b4
⎞
⎟
⎟
⎠−
⎛
⎜
⎜
⎝
0
P2
P3R2
P4R3R2
⎞
⎟
⎟
⎠ξ,

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Copyright © by SIAM. Unauthorized reproduction of this article is prohibited. 
SUPERFAST ALGORITHM FOR TOEPLITZ SYSTEMS
1255
where the dimensions of the generators follow those in Table 3.1 with mi = ˜mi and
the vector ξ = 0. The extra zero vector ξ on the right-hand side of (3.3) has been
added for the purpose of a general recursive pattern.
The algorithm has two main stages, compression (or elimination) and merging,
depending on the relationship between ki (li) and mi in the intermediate procedure.
3.2.1. Compression. At the beginning, k1 < m1 because of the low-rank prop-
erty described earlier. We apply a unitary transformation qH
1 to U1 so that the ﬁrst
m1 −k1 rows of U1 become zeros:
(3.4)
qH
1 U1 =

0
ˆU1
	
m1 −k1
k1
.
Now we multiply qH
1 to the ﬁrst m1 equations of the system

qH
1
0
0
I
	
Ax =

qH
1
0
0
I
	
b −
⎛
⎜
⎜
⎝
0
P2
P3R2
P4R3R2
⎞
⎟
⎟
⎠ξ.
We pick another unitary transformation wH
1 to lower-triangularize qH
1 D1, the (1, 1)
diagonal block A, i.e.,

qH
1 D1

wH
1 =
 m1 −k1
k1
m1 −k1
D11
0
k1
D21
D22
	
.
Then system (3.3) becomes

qH
1
0
0
I
	
A

wH
1
0
0
I
	 
w1
0
0
I
	
x =

qH
1
0
0
I
	
b −
⎛
⎜
⎜
⎝
0
P2
P3R2
P4R3R2
⎞
⎟
⎟
⎠ξ,
which can be rewritten as
⎛
⎜
⎜
⎜
⎜
⎝
D11
0
0
0
0
D21
D22
ˆU1V H
2
ˆU1W2V H
3
ˆU1W2W3V H
4
P2QH
11
P2 ˆQH
1
D2
U2V H
3
U2W3V H
4
P3R2QH
11
P3R2 ˆQH
1
P3QH
2
D3
U3V H
4
P4R3R2QH
11
P4R3R2 ˆQH
1
P4R3QH
2
P4QH
3
D4
⎞
⎟
⎟
⎟
⎟
⎠
⎛
⎜
⎜
⎜
⎜
⎝
z1
ˆx1
x2
x3
x4
⎞
⎟
⎟
⎟
⎟
⎠
=
⎛
⎜
⎜
⎜
⎜
⎝
β1
γ1
b2
b3
b4
⎞
⎟
⎟
⎟
⎟
⎠
−
⎛
⎜
⎜
⎜
⎜
⎝
0
0
P2
P3R2
P4R3R2
⎞
⎟
⎟
⎟
⎟
⎠
ξ,
where we have used the partitions
w1x1 = m1 −k1
k1
z1
ˆx1
	
,
qH
1 b1 =

m1 −k1
β1
k1
γ1
	
,
and
w1Q1 =

m1 −k1
Q11
k1
ˆQ1
	
.

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Copyright © by SIAM. Unauthorized reproduction of this article is prohibited. 
1256
CHANDRASEKARAN, GU, SUN, XIA, AND ZHU
At this point, we can solve for z1 from the system of equations D11z1 = β1. We
also subtract D21z1 from the right-hand side to obtain ˆb1 = γ1 −D21z1. Then we can
discard the ﬁrst m1 −k1 rows and columns of the coeﬃcient matrix of the system to
obtain
⎛
⎜
⎜
⎝
D22
ˆU1V H
2
ˆU1W2V H
3
ˆU1W2W3V H
4
P2 ˆQH
1
D2
U2V H
3
U2W3V H
4
P3R2 ˆQH
1
P3QH
2
D3
U3V H
4
P4R3R2 ˆQH
1
P4R3QH
2
P4QH
3
D4
⎞
⎟
⎟
⎠
⎛
⎜
⎜
⎝
ˆx1
x2
x3
x4
⎞
⎟
⎟
⎠=
⎛
⎜
⎜
⎝
ˆb1
b2
b3
b4
⎞
⎟
⎟
⎠−
⎛
⎜
⎜
⎝
0
P2
P3R2
P4R3R2
⎞
⎟
⎟
⎠ˆξ,
where ˆξ = ξ + QH
11z1. This new system has a similar structure to the original one
but with smaller dimension.
We can continue to solve it by recursion, if further
compressions of the blocks such as (3.4) are possible. Note the actual solution, say,
x1, can be recovered by
x1 = wH
1

z1
ˆx1
	
.
3.2.2. Merging. During the recursive eliminations there are situations when
ki is no longer smaller than mi and no further compression is possible.
We are
then unable to introduce more zeros into the system. Now we proceed by merging
appropriate block rows and columns of the matrix. As an example we can merge the
ﬁrst two block rows and columns and rewrite the system of equations as follows:
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝

D1
U1V H
2
P2QH
1
D2
	

U1W2
U2
	
V H
3

U1W2
U2
	
W3V H
4
P3

Q1RH
2
Q2
	H
D3
U3V H
4
P4R3

Q1RH
2
Q2
	H
P4QH
3
D4
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠
⎛
⎜
⎜
⎝

x1
x2
	
x3
x4
⎞
⎟
⎟
⎠
=
⎛
⎜
⎜
⎝

b1
b2 −P2 ˆξ
	
b3
b4
⎞
⎟
⎟
⎠−
⎛
⎜
⎜
⎝

0
0
	
P3
P4R3
⎞
⎟
⎟
⎠(R2 ˆξ).
Hence the system becomes
⎛
⎝
ˆD1
ˆU1V H
3
ˆU1W3V H
4
P3 ˆQH
1
D3
U3V H
4
P4R3 ˆQH
1
P4QH
3
D4
⎞
⎠
⎛
⎝
ˆx1
x3
x4
⎞
⎠=
⎛
⎝
ˆb1
b3
b4
⎞
⎠−
⎛
⎝
0
P3
P4R3
⎞
⎠(˜ξ),
where
ˆD1 =

D1
U1V H
2
P2QH
1
D2
	
,
ˆU1 =
 U1W2
U2
	
,
ˆQ1 =

Q1RH
2
Q2
	
,
ˆx1 =

x1
x2
	
,
ˆb1 =

b1
b2 −P2τ
	
,
˜ξ = R2 ˆξ.
The number of block rows/columns is reduced by one. Further compressions become
possible and we can proceed to solve the system recursively. In the case n = 1, we
have the system D1x1 = b1 −0ξ, which is solved directly.

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Copyright © by SIAM. Unauthorized reproduction of this article is prohibited. 
SUPERFAST ALGORITHM FOR TOEPLITZ SYSTEMS
1257
3.3. General solve algorithm. We now present a short description of the gen-
eral algorithm. The procedure in the 4 × 4 example can be directly extended to a
general system. We assume that the matrix A is in compact SSS form represented by
the generators {Ui}n−1
i=1 , {Vi}n
i=2, {Wi}n−1
i=2 , {Pi}n
i=2, {Qi}n−1
i=1 , {Ri}n−1
i=2 , and {Di}n
i=1
as in (3.1). We also partition x = (xi) and b = (bj) such that xi and bi have mi rows.
As in the 4 × 4 example, there are two stages at each step of the recursion.
In the compression stage, we perform orthogonal eliminations on both sides of A
to create an (m1 −k1) × (m1 −k1) lower triangular submatrix at the top left corner
of A. Then we solve a small triangular system and obtain the ﬁrst few components of
the solution vector. At this stage, we are left with a new system with less unknowns;
hence we can carry out a recursion.
In the merging stage, we merge the ﬁrst two block rows and columns of A while
still maintaining the SSS structure.
The numbers of block rows and columns are
reduced by one.
Combining these two stages, we can proceed with recursion to solve the system.
When n = 1 we can solve the linear system directly with standard solvers.
The SSS solver has a complexity O(Np2) [12, 13], where p is the maximum
numerical rank of the oﬀ-diagonal blocks of A, as compared to the traditional O(N 3)
cost for a general dense N × N matrix. We use only orthogonal transformations and
a single substitution in the SSS solver. Although a formal proof for the backward
stability is not yet available, the solver is shown to be practically stable. The reader
is referred to [12, 13] for more discussions on the stability.
4. Fast construction of SSS representation for C. According to section 3,
a system in compact SSS form can be solved very eﬃciently. We can thus use that
algorithm to solve the Cauchy-like system (2.3), provided that C can be quickly
written in SSS form. Therefore we try to ﬁnd an eﬃcient construction scheme. Here we
provide a divide-and-conquer SSS construction scheme by using the fast merging and
splitting strategy in [13]. If we know further that the matrix has low-rank oﬀ-diagonal
blocks and it is easy to compress the oﬀ-diagonal blocks, then the construction can
be superfast. Here we will concentrate on this situation as Cauchy-like matrices have
this low-rank property.
We ﬁrst present a general divide-and-conquer construction algorithm and then
describe a fast shifting strategy to compress the oﬀ-diagonal blocks of C.
4.1. General divide-and-conquer construction algorithm. In this section,
we discuss the construction of the SSS structure of a matrix A, when the partition
sequence {mi}n
i=1 is given. The general construction methods can be applied to any
unstructured matrix, thus proving that any matrix has an SSS structure. (Of course,
ki and li will usually be large in this case, precluding any speed-ups.) These methods
can be viewed as speciﬁc ways to make the realization algorithm of [18] more eﬃcient.
Suppose we are given an N × N matrix A and a partition sequence {mi}n
i=1 with
n
i=1 mi = N. Starting with an appropriate sequence { ˜m1, ˜m2}, where k
i=1 mi =
˜m1 and n
i=k+1 mi = ˜m2, we can ﬁrst partition A into a 2 × 2 block matrix and then
construct a simple SSS form
(4.1)
A =
 ˜m1
˜m2
˜m1
D1
B
˜m2
E
D2
	
=

˜m1
˜m2
D1
U1V H
2
P2QH
1
D2
	
,
where
(4.2)
B = U1V H
2
≡U1

Σ1F H
1

, F = P2QH
1 ≡P2

Σ1F H
1


 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Copyright © by SIAM. Unauthorized reproduction of this article is prohibited. 
1258
CHANDRASEKARAN, GU, SUN, XIA, AND ZHU
are the low-rank SVDs of the oﬀ-diagonal blocks B and E. Note if the compressions
are done by τ-accurate SVD approximations or rank-revealing QR decompositions,
then appropriate “=” signs should be replaced by “≈” signs. We now split either the
(1, 1) block or the (2, 2) block to obtain a 3 × 3 block SSS matrix. For instance, we
can split the (1, 1) block according to an appropriate new sequence { ˆm1, ˆm2, ˆm3} as
follows, where ˆm1 + ˆm2 = m1, ˆm3 = m2:

Dnew
1
U new
1
(V new
2
)H
P new
2
(Qnew
1
)H
Dnew
2
	
= D1,

U new
1
W new
2
U new
2
	
= U1, (V new
3
)H = V H
2 ,
(Rnew
2
(Qnew
1
)H(Qnew
2
)H) = QH
1 , P new
3
= P2, Dnew
3
= D2,
where the new generators (marked by the superscript new) introduced based on (4.1)
can be determined in the following way. First, we partition the matrices for the old
ﬁrst block conformally with the two new blocks as

D11
1
D12
1
D21
1
D22
1
	
= D1,

U 1
1
U 2
1
	
= U1, ((Q1
1)H(Q2
1)H) = QH
1 .
We can then identify from these and the previous equations that
Dnew
1
= D11
1 , Dnew
2
= D22
1 , U new
2
= U 2
1 , V new
3
= V2, Qnew
2
= Q2
1, P new
3
= P2, Dnew
3
= D2.
The remaining matrices satisfy
(4.3)
(D12
1 U 1
1 ) = U new
1
((V new
2
)HW new
2
),
 D21
1
(Q1
1)H
	
=

P new
2
Rnew
2
	
(Qnew
1
)H.
By factorizing the left-hand side matrices using numerical tools such as the SVD and
rank-revealing QR, these two equations allow us to compute those remaining matrices
for the new blocks.
A is thus in the new form
A =
⎛
⎝
ˆm1
ˆm2
ˆm3
ˆm1
Dnew
1
U new
1
(V new
2
)H
U new
1
W new
2
(V new
3
)H
ˆm2
P new
2
(Qnew
1
)H
Dnew
2
U new
2
(V new
3
)H
ˆm3
P new
2
Rnew
2
(Qnew
1
)H
P new
3
(Qnew
2
)H
Dnew
3
⎞
⎠.
We can use similar techniques if we want to split the second row and column
of (4.1).
We can continue this by either splitting the ﬁrst block row and block column or
the last ones using the above techniques, or splitting any middle block row and block
column similarly. Then we will be able to construct the desired SSS representation
according to the given sequence {mi, i = 1, 2, . . . , n}.
The general construction can be organized with bisection. The major cost is in
compressions of oﬀ-diagonal blocks of the form
(4.4)
D12
i
= XiY H
i ,
where Xi and Yi are tall and thin, and D12
i
is an oﬀ-diagonal block of
Di =
D11
i
D12
i
D21
i
D22
i
	
.
The compression (4.4) can be achieved by a τ-accurate QR factorization.
The construction is also practically stable in our implementation.

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Copyright © by SIAM. Unauthorized reproduction of this article is prohibited. 
SUPERFAST ALGORITHM FOR TOEPLITZ SYSTEMS
1259
4.2. Compression of oﬀ-diagonal blocks. For a general matrix with low-rank
oﬀ-diagonal blocks, the SSS construction can cost O(N 2p) as a compression such
as (4.2), (4.3), and (4.4) can take O(K2p), where K is the dimension of the block
being compressed. However, for the Cauchy-like matrix C in (2.3) the compressions
can be precomputed.
Theorem 4.1. The compression of the oﬀ-diagonal block
(4.5)
G =
 uT
i · vj
λi −ηj
	
1≤i≤p, q≤j≤N
= XY H
of C can be obtained by the compression of the corresponding oﬀ-diagonal block
G =

1
λi −ηj
	
1≤i≤p, q≤j≤N
= X0Y H
0
of C0, where the column dimension of X is no larger than twice the size of the column
dimension of X0.
Proof. Assume the oﬀ-diagonal block G0 = (
1
λi−ηj )1≤i≤p, q≤j≤N is in compressed
form X0Y H
0 , and the (i, j) entry is
1
λi−ηj = Xi,:Y H
j,: , where Xi,: (Yj,:) denotes the ith
row of X (Y ). As α ≤2 for simplicity we ﬁx α = 2 in (2.1). Then the corresponding
oﬀ-diagonal block in C is
G =
 uT
i · vj
λi −ηj
	
1≤i≤p, q≤j≤N
=
ui1vj1 + ui2vj2
λi −ηj
	
1≤i≤p, q≤j≤N
=

(ui1vj1 + ui2vj2) Xi,:Y H
j,:

1≤i≤p, q≤j≤N
=

(ui1Xi,:
ui2Xi,:)
vj1Y H
j,:
vj2Y H
j,:
		
1≤i≤p, q≤j≤N
=
⎛
⎜
⎝
u11X1,:
u12X1,:
...
...
up1Xi,:
up2Xi,:
⎞
⎟
⎠
vq1Y H
q,:
· · ·
vN1Y H
N,:
vq2Y H
q,:
· · ·
vN2Y H
N,:
	
≡XY H.
That is, we get a compression of G.
Theorem 4.1 indicates that we can convert the compressions of the oﬀ-diagonal
blocks of C to be the compressions of those of C0 which is independent of the actual
entries of the Toeplitz matrix T. This means the compressions of oﬀ-diagonal blocks
of C0 can be precomputed. The precomputation can be done in O(N 2p) ﬂops by a
rank-revealing QR factorization such as in [27]. It is possible to reduce the cost to
O(N log N) due to the fact that the compression of a large oﬀ-diagonal block can
be obtained by that of small ones. This can be seen implicitly from the following
subsection.
4.3. Compressions of oﬀ-diagonal blocks in precomputation. We further
present a shifting strategy to reduce the cost of the compressions in the precomputa-
tion. The signiﬁcance of this shifting strategy is to relate the compressions of large
oﬀ-diagonal block of C0 to those of small ones. That is, in diﬀerent splitting stages
of the SSS construction of C, the compressions of oﬀ-diagonal blocks with diﬀerent
sizes can be related.

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Copyright © by SIAM. Unauthorized reproduction of this article is prohibited. 
1260
CHANDRASEKARAN, GU, SUN, XIA, AND ZHU
For simplicity, we look at the example of partitioning C0 into 4×4 blocks. Assume
in the ﬁrst cut that we can partition C0 into 2 × 2 blocks with equal dimensions as in
the following:
C0 =

1
ω2i −ω2j+1
	
1≤i, j≤N
=
C0;1,1
C0;1,2
C0;2,1
C0;2,2
	
=
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
1
ω2−ω3
· · ·
1
ω2−ω4k+1
1
ω2−ω4k+3
· · ·
1
ω2−ω2N+1
...
...
...
· · ·
...
1
ω4k−ω3
· · ·
1
ω4k−ω4k+1
1
ω4k−ω4k+3
· · ·
1
ω4k−ω2N+1
C0;2,1
C0;2,2
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠
,
where k = N
4 , and without loss of generality, we consider only the block upper tri-
angular part. Assume that we have obtained a compression X1Y H
1
of C0;1,2 such
that
C0 =
C0;1,1
X1Y H
1
C0;2,1
C0;2,2
	
,
where X1 and Y1 are tall and thin and can be computed with τ-accurate SVD or
rank-revealing QR factorization. Next, we split C0;1,1, the (1, 1) block of C0, into
2 × 2 blocks and compress its oﬀ-diagonal block. Suppose the resulting oﬀ-diagonal
block of C0;1,1 has size k × ( N
2 −l). We will compress it by shifting certain parts of
X1 and Y1. That is, we partition X1 and Y1 conformally as
X1 =

X1,1
X1,2
	
, Y1 =

Y1,1
Y1,2
	
,
and also pick a k × ( N
2 −l) block from the lower left corner of C0;1,2 = X1Y H
1
(that
is, X1,2Y H
1,1)
C0 =
C0;1,1
X1Y H
1
C0;2,1
C0;2,2
	
=
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
1
ω2−ω2l+3
· · ·
1
ω2−ωN+1
...
· · ·
...
1
ω2k−ω2l+3
· · ·
1
ω2k−ωN+1
1
ω2k+2−ωN+3
· · ·
1
ω2k+2−ω2(N−l)+1
...
· · ·
...
1
ω4k−ωN+3
· · ·
1
ωN −ω2(N−l)+1
C0;2,1
C0;2,2
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠
=
⎛
⎜
⎝
X2Y H
2
X1,1Y H
1,1
X1,1Y H
1,2
X1,2Y H
1,1
X1,2Y H
1,2
C0;2,1
C0;2,2
⎞
⎟
⎠,

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Copyright © by SIAM. Unauthorized reproduction of this article is prohibited. 
SUPERFAST ALGORITHM FOR TOEPLITZ SYSTEMS
1261
where X2Y H
2
is an unknown compression of the upper right submatrix of C0;1,1, and
the blocks that don’t concern us are left blank. At this point we do not need another
factorization to get X2Y H
2 ; instead we can directly derive the compression X2Y H
2
of
C0;1,1 from X1,2 and Y1,1. Clearly we have
X2Y H
2
=
⎛
⎜
⎝
1
ω2−ω2l+3
· · ·
1
ω2−ωN+1
...
· · ·
...
1
ω2k−ω2l+3
· · ·
1
ω2k−ωN+1
⎞
⎟
⎠,
X1,2Y H
1,1
=
⎛
⎜
⎝
1
ω2k+2−ωN+3
· · ·
1
ω2k+2−ω2(N−l)+1
...
· · ·
...
1
ω4k−ωN+3
· · ·
1
ωN−ω2(N−l)+1
⎞
⎟
⎠=
1
ω2k X2Y H
2 .
That means we can get X2Y H
2
by shifting a subblock of X1Y H
1 . A similar situation
holds for the splitting of the (2, 2) block of C after the ﬁrst splitting. For the successive
compressions in later splittings, a similar shifting can be used. For splitting with
general block sizes the shifting is also similar.
The shifting scheme indicates that, in diﬀerent levels of divide-and-conquer SSS
constructions, the compressions of large blocks and small blocks are related. This can
be used to further save the compression cost.
5. Performance and numerical experiments. It is well known that the FFT
transformation of the Toeplitz matrix T to a Cauchy-like matrix C, and the recovery
from the solution ˜x of the Cauchy-like system to the solution x of the Toeplitz system,
are stable. In addition, the fast divide-and-conquer construction and the SSS system
solver in section 3 are both practically stable as we will see in our numerical results.
Thus our overall algorithm is stable in practice. Here no extra steps are needed to
stabilize the algorithm as required in some other superfast methods [38].
After the precomputations discussed in sections 4.2 and 4.3 are ﬁnished, the cost
of each SSS construction is only O(Np2), where p is the maximum of the oﬀ-diagonal
ranks of the Cauchy matrix C0. The SSS solver also costs O(Np2). The total cost
is thus no more than O(N log N) + O(Np2) ﬂops.
The total storage requirement
is O(Np). For the convenience of coding, we use an O(N 2p) cost precomputation
routine as discussed in subsection 4.2.
A preliminary implementation of our superfast solver in Fortran 90 is available
at http://www.math.ucla.edu/˜jxia/work/toep. We did experiments on an Itanium 2
1.4 GHz SGI Altix 350 server with a 64-bit Linux operating system and Intel MKL
BLAS. Our method (denoted by NEW) is compared to Gaussian elimination with partial
pivoting (denoted by GEPP) (via the LAPACK linear system solver ZGESV [3]). We
consider real N × N Toeplitz matrices T whose entries are random and uniformly
distributed in [0, 1].
The right-hand sides b are obtained by b = Tx, where the
exact solution x has random entries uniformly distributed in [−1, 1].
Matrices of
size N = 2k × 100, k = 2, 3, . . . , are considered. These matrices are moderately ill-
conditioned. For example, for N = 2k ×100 with k = 2, . . . , 7, the one-norm condition
numbers of the matrices increase from the order of about 103 to 106.
For the NEW solver two important parameters are involved, the SSS block size d
(Figure 2.1) and the tolerance τ in the compressions of oﬀ-diagonal blocks. Here we
use the τ-accurate QR factorization with τ as a relative tolerance (section 2.2).
Figure 5.1 shows the execution time (in seconds) for GEPP and our NEW solver
with diﬀerent d and τ. For the NEW solver only the time for solving the Cauchy-like

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Copyright © by SIAM. Unauthorized reproduction of this article is prohibited. 
1262
CHANDRASEKARAN, GU, SUN, XIA, AND ZHU
2
4
6
8
10
12
10
−2
10
−1
10
0
10
1
10
2
10
3
10
4
Execution time
k
time (seconds)
GEPP
NEW(τ=10−9, d=100)
NEW(τ=10−4, d=50)
Fig. 5.1. Computation time (in seconds) versus N = 2k × 100. For the NEW solver, time for the
precomputation is excluded.
2
4
6
8
10
12
10
−2
10
−1
10
0
10
1
10
2
10
3
10
4
Precomputation time
k
time (seconds)
NEW(τ=10−9, d=100)
NEW(τ=10−4, d=50)
Fig. 5.2. Time (in seconds) for precomputations in the NEW solver.
system (2.3) is reported, as the compressions of oﬀ-diagonal blocks can be done in the
precomputation, as described in section 4.3, and the SSS construction time is nearly
the precomputation time. The precomputation time is shown in Figure 5.2, although
the precomputation only needs to be done once for a particular matrix size N.
We also consider the time scaling factor, that is, the factor by which the time is
multiplied when the matrix size doubles; see Figure 5.3. We observe that the time
scaling factors for the NEW solver are near 2, that is to say the NEW solver is close to
being a linear time solver.
Figures 5.4 and 5.5 present the errors ε1 = ||ˆx−x||2
||x||2
and ε2 =
||T ˆx−b||2
|||T ||ˆx|+|b|||2 , respec-
tively, where ˆx denotes the numerical solution.
A signiﬁcant point of the new solver is that we can use a relatively large tolerance
τ in the precomputation and solving, and then use iterative reﬁnement to improve the
accuracy. A relatively large τ leads to relatively small oﬀ-diagonal ranks (and thus

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Copyright © by SIAM. Unauthorized reproduction of this article is prohibited. 
SUPERFAST ALGORITHM FOR TOEPLITZ SYSTEMS
1263
2
4
6
8
10
12
1
2
3
4
5
6
7
8
9
10
Time scaling factor
k
time scaling factor
GEPP
NEW(τ=10−9, d=100)
NEW(τ=10−4, d=50)
Fig. 5.3. Time scaling factors.
1
2
3
4
5
6
7
8
9
10
11
10
−16
10
−14
10
−12
10
−10
10
−8
10
−6
10
−4
10
−2
ε1
k
relative error
NEW(τ=10−9, d=100)
GEPP
Fig. 5.4. ε1 = ||ˆx−x||2
||x||2
versus N = 2k × 100.
1
2
3
4
5
6
7
8
9
10
11
10
−16
10
−14
10
−12
10
−10
10
−8
10
−6
10
−4
10
−2
ε2
k
relative residual error
NEW(τ=10−9, d=100)
GEPP
Fig. 5.5. ε2 =
||T ˆx−b||2
|| |T | |ˆx|+|b| ||2 versus N = 2k × 100.

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Copyright © by SIAM. Unauthorized reproduction of this article is prohibited. 
1264
CHANDRASEKARAN, GU, SUN, XIA, AND ZHU
small p in the operation counts above). Iterative reﬁnements are very cheap due to the
facts that no precomputation is needed and the solver itself is very fast (Figure 5.1).
For τ = 10−4 and d = 50 the accuracy results after 2 to 8 steps of iterative reﬁnement
are displayed in Figure 5.6. In fact the number of iterative reﬁnement steps required
to reach ε2 < 10−13 is listed in Table 5.1.
Thus it is also clear that our NEW solver can also perform well as a preconditioner.
The block size d also aﬀects the performance. Figure 5.7 indicates that for a
1
2
3
4
5
6
7
8
9
10
10
−18
10
−16
10
−14
10
−12
10
−10
10
−8
10
−6
10
−4
10
−2
ε2
k
relative residual error
initial
2 steps
4 steps
6 steps
8 steps
Fig. 5.6. ε2 =
||T ˆx−b||2
|| |T | |ˆx|+|b| ||2 , initial values (before iterative reﬁnements), and after some steps
of iterative reﬁnement.
0
1
2
3
4
5
6
0
10
20
30
40
50
60
70
80
Computation time for NEW solver with different d=2j×25
j
time (seconds)
precomputation + solver time
solver time
Fig. 5.7. Computation time of the NEW solver with N = 12800, τ = 10−9, and diﬀerent block
size d = 2j × 25.
Table 5.1
Number of iterative reﬁnement steps required to reach ε2 < 10−13 for diﬀerent matrix dimen-
sions N = 2k × 100.
k
2
3
4
5
6
7
8
9
Number of steps
4
4
5
6
7
15
9
21

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Copyright © by SIAM. Unauthorized reproduction of this article is prohibited. 
SUPERFAST ALGORITHM FOR TOEPLITZ SYSTEMS
1265
given N if d = 2j × 25 is too large or too small, then both the solver time and the
precomputation time can be relatively large. In fact by counting the operations in the
algorithm it is possible to determine an optimal d; see [12] for the derivation of the
optimal choice of d in the SSS solver. We can do similar derivations for both the SSS
construction and the SSS solver. We just point out that when τ gets larger, the oﬀ-
diagonal ranks decrease and a smaller d should be chosen. In the previous experiments
we used two sets of parameters: (τ = 10−9, d = 100) and (τ = 10−4, d = 50).
Finally, it turns out that our current preliminary implementation of the solver is
slower than the implementation of the algorithm in [38], likely due to our ineﬃcient
data structure and nonoptimized codes for SSS matrices. The complicated nature
of SSS matrices needs more careful coding and memory management. An improved
software implementation is under construction.
6. Conclusions and future work. In this paper we have presented a super-
fast and practically stable solver for Toeplitz systems of linear equations. A Toeplitz
matrix is ﬁrst transformed into a Cauchy-like matrix, which has a nice low-rank prop-
erty, and then the Cauchy-like system is solved by a superfast solver. This superfast
solver utilizes this low-rank property and makes use of an SSS representation of the
Cauchy-like matrix. A fast construction procedure for SSS structures is presented.
After a one-time precomputation the solver is very eﬃcient (O(N log N) + O(Np2)
complexity). Also the algorithm is eﬃcient in that only linear storage is required. In
future work we hope to further reduce the precomputation cost and ﬁnish a better-
designed version of the current Fortran 90 codes in both the computations and the
coding.
Acknowledgments. Many thanks to the referees for their valuable comments
which have greatly improved the development and presentation of this paper. The
authors are also grateful to the editors and the editorial assistant for their patience
and thoughtful suggestions.
REFERENCES
[1] G. S. Ammar and W. B. Gragg, Superfast solution of real positive deﬁnite Toeplitz systems,
SIAM J. Matrix Anal. Appl., 9 (1988), pp. 61–76.
[2] G. S. Ammar and W. B. Gragg, Numerical experience with a superfast real Toeplitz solver,
Linear Algebra Appl., 121 (1989), pp. 185–206.
[3] E. Anderson, Z. Bai, C. Bischof, J. Demmel, J. Dongarra, J. Du Croz, A. Greenbaum,
S. Hammarling, A. McKenney, S. Ostrouchov, and D. Sorensen, LAPACK Users’
Guide, 2nd ed., SIAM, Philadelphia, PA, 1994.
[4] R. R. Bitmead and B. D. O. Anderson, Asymptotically fast solution of Toeplitz and related
systems of linear equations, Linear Algebra Appl., 34 (1980), pp. 103–116.
[5] A. W. Bojanczyk, R. P. Brent, F. R. de Hoog, and D. R. Sweet, On the stability of
the Bareiss and related Toeplitz factorization algorithms, SIAM J. Matrix Anal. Appl., 16
(1995), pp. 40–57.
[6] R. P. Brent, F. G. Gustavson, and D. Y. Y. Yun, Fast solution of Toeplitz systems of
equations and computation of Pad´e approximants, J. Algorithms, 1 (1980), pp. 259–295.
[7] J. R. Bunch, Stability of methods for solving Toeplitz systems of equations, SIAM J. Sci.
Statist. Comput., 6 (1985), pp. 349–364.
[8] J. Carrier, L. Greengard, and V. Rokhlin, A fast adaptive multipole algorithm for particle
simulations, SIAM J. Sci. Statist. Comput., 9 (1988), pp. 669–686.
[9] T. F. Chan and P. C. Hansen, A look-ahead Levinson algorithm for general Toeplitz systems,
IEEE Trans. Signal Process., 40 (1992), pp. 1079–1090.
[10] T. F. Chan and P. C. Hansen, A look-ahead Levinson algorithm for indeﬁnite Toeplitz sys-
tems, SIAM J. Matrix Anal. Appl., 13 (1992), pp. 490–506.
[11] S. Chandrasekaran and M. Gu, Fast and stable algorithms for banded plus semiseparable
matrices, SIAM J. Matrix Anal. Appl., 25 (2003), pp. 373–384.

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Copyright © by SIAM. Unauthorized reproduction of this article is prohibited. 
1266
CHANDRASEKARAN, GU, SUN, XIA, AND ZHU
[12] S. Chandrasekaran, P. Dewilde, M. Gu, T. Pals, X. Sun, A.-J. van der Veen, and
D. White, Fast Stable Solvers for Sequentially Semi-Separable Linear Systems of Equa-
tions and Least Squares Problems, Technical report, University of California, Berkeley,
CA, 2003.
[13] S. Chandrasekaran, P. Dewilde, M. Gu, T. Pals, X. Sun, A.-J. van der Veen, and
D. White, Some fast algorithms for sequentially semiseparable representations, SIAM J.
Matrix Anal. Appl., 27 (2005), pp. 341–364.
[14] S. Chandrasekaran, M. Gu, and W. Lyons, A Fast and Stable Adaptive Solver for Hierarchi-
cally Semi-Separable Representations, Technical report, UCSB Math 2004-20, University
of California, Santa Barbara, CA, 2004.
[15] G. Cybenko, The numerical stability of the Levinson-Durbin algorithm for Toeplitz systems
of equations, SIAM J. Sci. Statist. Comput., 1 (1980), pp. 303–319.
[16] G. Cybenko, Error Analysis of Some Signal Processing Algorithms, Ph.D. thesis, Princeton
University, Princeton, NJ, 1978.
[17] F. R. deHoog, On the solution of Toeplitz systems, Linear Algebra Appl., 88/89 (1987),
pp. 123–138.
[18] P. Dewilde and A. van der Veen, Time-Varying Systems and Computations, Kluwer
Academic Publishers, Boston, MA, 1998.
[19] M. Fiedler, Hankel and Loewner matrices, Linear Algebra Appl., 58 (1984), pp. 75–95.
[20] K. A. Gallivan, S. Thirumalai, P. Van Dooren, and V. Vermaut, High performance
algorithms for Toeplitz and block Toeplitz matrices, Linear Algebra Appl., 241/243 (1996),
pp. 343–388.
[21] L. Gemignani, Schur complements of Bezoutians and the inversion of block Hankel and block
Toeplitz matrices, Linear Algebra Appl., 253 (1997), pp. 39–59.
[22] I. Gohberg, T. Kailath, and V. Olshevsky, Fast Gaussian elimination with partial pivoting
for matrices with displacement structure, Math. Comp., 64 (1995), pp. 1557–1576.
[23] I. Gohberg and V. Olshevsky, Fast state space algorithms for matrix Nehari and Nehari-
Takagi interpolation problems, Integral Equations Operator Theory, 20 (1994), pp. 44–83.
[24] I. Gohberg and V. Olshevsky, Complexity of multiplication with vectors for structured ma-
trices, Linear Algebra Appl., 202 (1994), pp. 163–192.
[25] L. Greengard and V. Rokhlin, A fast algorithm for particle simulations, J. Comput. Phys.,
73 (1987), pp. 325–348.
[26] M. Gu, Stable and eﬃcient algorithms for structured systems of linear equations, SIAM J.
Matrix Anal. Appl., 19 (1998), pp. 279–306.
[27] M. Gu and S. C. Eisenstat, Eﬃcient algorithms for computing a strong rank-revealing QR
factorization, SIAM J. Sci. Comput., 17 (1996), pp. 848–869.
[28] G. Heinig, Inversion of generalized Cauchy matrices and other classes of structured matrices,
in Linear Algebra for Signal Processing, IMA Vol. Math. Appl. 69, Springer, New York,
1995, pp. 63–81.
[29] G. Heinig and K. Rost, Algebraic Methods for Toeplitz-like Matrices and Operators, Oper.
Theory Adv. Appl. 13, Birkh¨auser Verlag, Basel, 1984, pp. 109–127.
[30] T. Kailath, Fredholm resolvents, Wiener-Hopf equations, and Riccati diﬀerential equations,
IEEE Trans. Inform. Theory, 15 (1969), pp. 665–672.
[31] T. Kailath, S. Kung, and M. Morf, Displacement ranks of matrices and linear equations,
J. Math. Anal. Appl., 68 (1979), pp. 395–407.
[32] T. Kailath and A. H. Sayed, eds., Fast Reliable Algorithms for Matrices with Structure,
SIAM, Philadelphia, 1999.
[33] P. G. Martinsson, V. Rokhlin, and M. Tygert, A fast algorithm for the inversion of general
Toeplitz matrices, Comput. Math. Appl., 50 (2005), pp. 741–752.
[34] M. Morf, Fast Algorithms for Multivariable Systems, Ph.D. thesis, Department of Electrical
Engineering, Stanford University, Stanford, CA, 1974.
[35] B. R. Musicus, Levinson and Fast Choleski Algorithms for Toeplitz and Almost Toeplitz
Matrices, Technical report, Res. Lab. of Electronics, M.I.T., Cambridge, MA, 1984.
[36] V.
Pan,
On computations with dense structured matrices,
Math. Comp.,
55 (1990),
pp. 179–190.
[37] D. R. Sweet, The use of pivoting to improve the numerical performance of algorithms for
Toeplitz matrices, SIAM J. Matrix Anal. Appl., 14 (1993), pp. 468–493.
[38] M. Van Barel, G. Heinig, and P. Kravanja, A stabilized superfast solver for nonsymmetric
Toeplitz systems, SIAM J. Matrix Anal. Appl., 23 (2001), pp. 494–510.
[39] M. Van Barel and P. Kravanja, A stabilized superfast solver for indeﬁnite Hankel systems,
Linear Algebra Appl., 284 (1998), pp. 335–355.

