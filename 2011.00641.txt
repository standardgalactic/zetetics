Active Structure Learning of Causal DAGs via
Directed Clique Trees
Chandler Squires
LIDS, MIT
MIT-IBM Watson AI Lab
csquires@mit.edu
Sara Magliacane
MIT-IBM Watson AI Lab
IBM Research
sara.magliacane@gmail.com
Kristjan Greenewald
MIT-IBM Watson AI Lab
IBM Research
kristjan.h.greenewald@ibm.com
Dmitriy Katz
MIT-IBM Watson AI Lab
IBM Research
dkatzrog@us.ibm.com
Murat Kocaoglu
MIT-IBM Watson AI Lab
IBM Research
murat@ibm.com
Karthikeyan Shanmugam
MIT-IBM Watson AI Lab
IBM Research
karthikeyan.shanmugam2@ibm.com
Abstract
A growing body of work has begun to study intervention design for efﬁcient
structure learning of causal directed acyclic graphs (DAGs). A typical setting is
a causally sufﬁcient setting, i.e. a system with no latent confounders, selection
bias, or feedback, when the essential graph of the observational equivalence class
(EC) is given as an input and interventions are assumed to be noiseless. Most
existing works focus on worst-case or average-case lower bounds for the number
of interventions required to orient a DAG. These worst-case lower bounds only
establish that the largest clique in the essential graph could make it difﬁcult to
learn the true DAG. In this work, we develop a universal lower bound for single-
node interventions that establishes that the largest clique is always a fundamental
impediment to structure learning. Speciﬁcally, we present a decomposition of a
DAG into independently orientable components through directed clique trees and
use it to prove that the number of single-node interventions necessary to orient any
DAG in an EC is at least the sum of half the size of the largest cliques in each chain
component of the essential graph. Moreover, we present a two-phase intervention
design algorithm that, under certain conditions on the chordal skeleton, matches
the optimal number of interventions up to a multiplicative logarithmic factor in the
number of maximal cliques. We show via synthetic experiments that our algorithm
can scale to much larger graphs than most of the related work and achieves better
worst-case performance than other scalable approaches. 1
1
Introduction
Causal modeling is an important tool in medicine, biology and econometrics, allowing practitioners to
predict the effect of actions on a system and the behavior of a system if its causal mechanisms change
due to external factors (Pearl, 2009; Spirtes et al., 2000; Peters et al., 2017). A commonly-used model
1A code base to recreate these results can be found at https://github.com/csquires/dct-policy.
34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
arXiv:2011.00641v1  [stat.ME]  1 Nov 2020

is the directed acyclic graph (DAG), which is capable of modeling causally sufﬁcient systems, i.e.
systems with no latent confounders, selection bias, or feedback. However, even in this favorable setup,
a causal model cannot (in general) be fully identiﬁed from observational data alone; in these cases
experimental (“interventional”) data is necessary to resolve ambiguities about causal relationships.
In many real-world applications, interventions may be time-consuming or expensive, e.g. randomized
controlled trials or gene knockout experiments. These settings crucially rely on intervention design,
i.e. ﬁnding a cost-optimal set of interventions that can fully identify a causal model. Recently, many
methods have been developed for intervention design under different assumptions (He & Geng, 2008;
Hyttinen et al., 2013; Shanmugam et al., 2015; Kocaoglu et al., 2017; Lindgren et al., 2018).
In this work we extend the Central Node algorithm of Greenewald et al. (2019) to learn the structure of
causal graphs in a causally sufﬁcient setting from interventions on single variables for both noiseless
and noisy interventions. Noiseless interventions are able to deterministically orient a set of edges,
while noisy interventions result in a posterior update over a set of compatible graphs. We also focus
only on interventions with a single target variable, i.e. single-node interventions, but as opposed
to (Greenewald et al., 2019) which focuses on limited types of graphs, we allow for general DAGs
but only consider noiseless interventions. In particular, we focus on adaptive intervention design,
also known as sequential or active (He & Geng, 2008), where the result of each intervention is
incorporated into the decision-making process for later interventions. This contrasts with passive
intervention design, for which all interventions are decided beforehand.
Universal lower bound. Our key contribution is to show that the problem of fully orienting a DAG
with single-node interventions is equivalent to fully orienting special induced subgraphs of the DAG,
called residuals (Theorem 1 below). Given this decomposition, we prove a universal lower bound
on the minimum number of single-node interventions necessary to fully orient any DAG in a given
Markov Equivalence Class (MEC), the set of graphs that ﬁt the observational distribution. This lower
bound is equal to the sum of half the size of the largest cliques in each chain component of the
essential graph (Theorem 2). This result has a surprising consequence: the largest clique is always
a fundamental impediment to structure learning. In comparison, prior work (Hauser & Bühlmann,
2014; Shanmugam et al., 2015) established worst-case lower bounds based on the maximum clique
size, which only implied that the largest clique in each chain component of the essential graph could
make it difﬁcult to learn the true DAG.
Intervention policy. We also propose a novel two-phase single-node intervention policy. The ﬁrst
phase, based on the Central Node algorithm, uses properties of directed clique trees (Deﬁnition 2) to
reduce the identiﬁcation problem to identiﬁcation within the (DAG dependent) residuals. The second
phase then completes the orientations within each residual. We cover the condition of intersection-
incomparability for the chordal skeleton of a DAG (Kumar & Madhavan (2002) introduce this
condition in the context of graph theory) . We show that under this condition, our policy uses at most
O(log Cmax) times as many interventions as are used by the (DAG dependent) optimal intervention
set, where Cmax is the greatest number of maximal cliques in any chain component (Theorem 3).
Finally, we evaluate our policy on general synthetic DAGs. We ﬁnd that our intervention policy
performs comparably to intervention policies in previous work, while being much more scalable than
most policies and adapting more effectively to the difﬁculty of the underlying identiﬁcation problem.
2
Preliminaries
We brieﬂy review our notation and terminology for graphs. A mixed graph G is a tuple of vertices
V (G), directed edges D(G), bidirected edges B(G), and undirected edges U(G). Directed, bidi-
rected, and undirected edges between vertices i and j in G are denoted i →G j, i ↔G j, and i −G j,
respectively. We use asterisks as wildcards for edge endpoints, e.g., i∗→Gj denotes either i →G j or
i ↔G j. A directed cycle in a mixed graph is a sequence of edges i∗→G . . . ∗→G i with at least one
directed edge. A mixed graph is a chain graph if it has no directed cycles and B(G) = ∅, and a chain
graph is called a directed acyclic graph (DAG) if we also have U(G) = ∅. An undirected graph is a
mixed graph with B(G) = ∅and D(G) = ∅.
DAGs and (I-)Markov equivalence. DAGs are used to represent causal models (Pearl, 2009). Each
vertex i is associated with a random variable Xi. The skeleton of graph D, skel(D), is the undirected
graph with the same vertices and adjacencies as D. A distribution f is Markov w.r.t. a DAG D if it
2

factors as f(X) = Q
i∈V (D) f(Xi | XpaD(i)). Two DAGs D1 and D2 are called Markov equivalent
if all positive distributions f which are Markov to D1 are also Markov to D2 and vice versa. The set
of DAGs that are Markov equivalent to D is the Markov equivalence class (MEC), denoted as [D].
[D] is represented by a chain graph called the essential graph E(D), which has the same skeleton
as D, with directed edges i →E(D) j if i →D′ j for all D′ ∈[D], and undirected edges otherwise.
Given an intervention I ⊆V (D), the distributions (f obs, f I) are I-Markov to D if f obs is Markov to
D and f I factors as
f I(X) =
Y
i̸∈I
f obs(Xi | XpaD(i))
Y
i∈I
f I(Xi | XpaD(i))
where paD(i) represents the set of parents of vertex i in the DAG D. Given a list of interventions I =
[I1, . . . , IM], the set of distributions {f obs, f I1, . . . , f IM } is I-Markov to a DAG D if (f obs, f Im)
is Im-Markov to D for ∀m = 1 . . . M. The I-Markov equivalence class of D (I-MEC), denoted
as [D]I, can be represented by the I-essential graph EI(D) with the same adjacencies as D and
i →EI(D) j if i →D′ j for all D′ ∈[D]I.
The edges which are undirected in the essential graph E(D), but directed in the I-essential graph
EI(D), are the edges which are learned from performing the interventions in I. In the special case of
a single-node intervention, the edges learned are all of those incident to the intervened node, along
with any edges learned via the set of logical constraints known as Meek rules Appendix A.
Structure of essential graphs. We now report a known result that proves that any intervention policy
can split essential graphs in components that can be oriented independently. The chain components
of a chain graph G, denoted CC(G), are the connected components of the graph after removing its
directed edges. These chain components are then clearly undirected graphs. An undirected graph is
chordal if every cycle of length greater than 3 has a chord, i.e., an edge between two non-consecutive
vertices in the cycle.
Lemma 1 (Hauser & Bühlmann (2014)). Every I-essential graph is a chain graph with chordal chain
components. Orientations in one chain component do not affect orientations in other components.
Deﬁnition 1. A DAG whose essential graph has a single chain component is called a moral DAG.
In many of the following results we will consider moral DAGs, since once we can orient moral DAGs
we can easily generalize to general DAGs through these results.
Intervention Policies. An intervention policy π is a (possibly randomized) map from (I-)essential
graphs to interventions. An intervention policy is adaptive if each intervention Im is decided based
on information gained from previous interventions, and passive if the whole set of interventions I is
decided prior to any interventions being performed. An intervention is noiseless if the intervention
set I collapses the set of compatible graphs exactly to the I-MEC, while noisy interventions simply
induce a posterior update on the distribution over compatible graphs. Most policies assume that the
MEC is known (e.g., it has been estimated from observational data) and interventions are noiseless;
this is true of our policy too. Moreover, we focus only on interventions on a single target variable, i.e.
single-node interventions. We discuss previous work on intervention policies in Section 6.
3
Universal lower-bound in the number of single-node interventions
In this section we prove a lower-bound on any possible single-node policy (Theorem 2) by decompos-
ing the complete orientation of a DAG in terms of the complete orientation of smaller independent
subgraphs, called residuals (Theorem 1), deﬁned on a novel graphical structure, directed clique trees
(DCTs). We provide all proofs in the Appendix.
First, we review the standard deﬁnitions of clique trees and clique graphs for undirected chordal
graphs (see also (Galinier et al., 1995)). A clique C ⊆V (G) is a subset of the nodes with an edge
between each pair of nodes. A clique C is maximal if C ∪{v} is not a clique for any v ∈V (G) \ C.
The set of maximal cliques of G is denoted C(G). The clique number of G is ω(G) = maxC∈C |C|.
A clique tree (aka a junction tree) TG of a chordal graph is a tree with vertices C(G) that satisﬁes
the induced subtree property, i.e., for any v ∈V (G), the induced subgraph on the set of cliques
containing v is a tree. A chordal graph can have multiple clique trees, so we denote the set of all clique
trees of G as T (G). A clique graph ΓG is the graph union of all clique trees, i.e. the undirected graph
3

2
6
5
7
4
3
1
(a)
(b)
(c)
(d)
(e)
Figure 1: A moral DAG (a), one of its clique trees (b), its two DCTs (c-d) and the DCG (e).
2



2


4
3
Figure 2: Examples of edge orientations.
b)
2
4
5
6
3
1
a)
Figure 3: A DAG and its CDCTs with (using
only edge a) and without arrow-meets (edge b).
with V (ΓG) = C(G) and U(ΓG) = ∪T ∈T (G)U(T). A useful characterization of the clique trees of
G are as the max-weight spanning trees of the weighted clique graph WG (Koller & Friedman, 2009),
which is a complete graph over vertices C(G), with the edge C1 −WG C2 having weight |C1 ∩C2|.
Given a moral DAG D, we can trivially deﬁne its clique trees T (D) as the clique trees of its skeleton
G = skel(D), i.e. T (G). For example, in Fig. 1 (a) we show a DAG, where we have chosen a color
for each of the cliques, while in Fig. 1 (b) we show one of its clique trees. We now deﬁne a directed
counterpart to clique trees based on the orientations in the underlying DAG:
Deﬁnition 2. A directed clique tree TD of a moral DAG D has the same vertices and adjacencies as
a clique tree TG of G = skel(D). For each ordered pair of adjacent cliques C1 ∗−∗C2 we orient the
edge mark of C2 as:
• C1 ∗→C2, if ∀v12 ∈C1 ∩C2 and ∀v2 ∈C2 \ C1, we have v12 →D v2 in the DAG D;
• C1 ∗−C2 otherwise, i.e. if there exists at least one incoming edge from C2 \C1 into C1 ∩C2,
where we recall that ∗denotes a wildcard for an edge. Thus, the above conditions only decide the
presence or absence of an arrowhead at C2; the presence or absence of an arrowhead at C1 is
decided when considering the reversed order.
A DAG can have multiple directed clique trees (DCTs), as shown in Fig. 1 (c) and (d). In ﬁgures, we
annotate edges with the intersection between cliques. Fig. 1 (c) represents the directed clique tree
corresponding to the standard clique tree in Fig. 1 (b). In Fig. 2 we show in detail the orientations
for two of the directed clique edges following Deﬁnition 2, the red edges are outcoming from the
clique intersection, while the blue edge is incoming in the intersection. Deﬁnition 2 also implies each
edge that is shared between two different clique trees has a unique orientation (since it is based on
the underlying DAG), so we can deﬁne the directed clique graph (DCG) ΓD of a moral DAG D as
the graph union of all directed clique trees of D. We show an example of a DCG in Fig. 1(e). As can
be seen in the examples in Fig. 1, DCTs can contain directed and bidirected edges, and, as we prove
in Appendix C, no undirected edgees. We deﬁne the bidirected components of a DCT as:
Deﬁnition 3. The bidirected components of TD, B(TD), are the connected components of TD after
removing directed edges.
Another structure that can happen in a DCT is when two arrows meet at the same clique. To avoid
confusing associations with colliders in DAGs, we call these structures in DCTs arrow-meets. Arrow-
meets will prove to be challenging for our algorithms, so we introduce intersection incomparability
and prove that in case it holds there can be no arrow-meets:
4

1
2
3
5
4
7
6
8
1
2
3
5
4
7
6
8
Figure 4: A DAG, its CDCT and its residuals.
2
4
3
1
2
4
3
1
Figure 5: DAGs in the same MEC with
m(D1) ̸= m(D2).
Deﬁnition 4. A pair of edges C1 −TG C2 and C2 −TG C3 are intersection comparable if C1 ∩C2 ⊆
C2 ∩C3 or C1 ∩C2 ⊇C2 ∩C3. Otherwise they are intersection incomparable.
For example, in Fig. 1 (e), the edges {2, 5, 6}↔{2, 3, 4} and {2, 3, 4}←{1, 2, 3} are intersection
comparable, since {2} ⊂{1, 2}, while {2, 5, 6}↔{2, 3, 4} and {2, 5, 6}→{5, 7} are intersection
incomparable, since {2} ̸⊆{5} and {5} ̸⊆{2}.
Proposition 1. Suppose C1 ∗→TDC2 and C2 ←∗TDC3 in TD. Then these edges are intersection
comparable. Equivalently in the contrapositve, if C1 ∗→TDC2 and C2 ∗−∗TDC4 are intersection
incomparable, we can immediately deduce that C2 →TD C4.
Bidirected components do not have a clear ordering, so we contract them into single nodes in a
contracted DCT (CDCT), and prove we can always construct a tree-like CDCT for any moral DAG:
Deﬁnition 5. The contracted directed clique tree (CDCT) ˜TD of a DCT TD is a graph on the vertex
set B1, B2 . . . BK ∈B(TD) with B1 →˜TD B2 if C1 →TD C2 for any clique C1 ∈B1 and C2 ∈B2.
Lemma 2. For any moral DAG D, one can always construct a CDCT with no arrow-meets.
In particular, one can adapt Kruskal’s algorithm for ﬁnding a max-weight spanning tree to construct
a DCT from the weighted clique graph and then contract it, as shown in detail in Algorithm 3 in
Appendix D. In Fig. 3 we show an example of a CDCT with arrow-meets (represented by the black
edge and the edge labelled “a”) and its equivalent no arrow-meets version (represented by the black
edge and the edge “b”) . Since we can always construct a CDCT with no arrow-meets, we assume
w.l.o.g. that the CDCT is a tree. The CDCT allows us to deﬁne a decomposition of a moral DAG into
independently orientable components. We call these components residuals, since they extend the
notion of residuals in rooted, undirected clique trees (Vandenberghe et al., 2015). Formally:
Deﬁnition 6. For a tree-like CDCT ˜TD of a moral DAG D, the residual of its node B is deﬁned as
Res ˜TD(B) = D|B\P , where P is its parent in ˜TD (or if there is none, P = ∅) and D|B\P is the
induced subgraph of D over the subset of V (D) that are assigned to B but not to P. We denote the
set of all residuals of ˜TD by R( ˜TD).
Intuitively this describes the subgraphs in which we cut all edges that are captured in the CDCT, as
shown in Fig. 4. We now generalize our results from a moral DAG to a general DAG. Surprisingly,
we show that orienting all of the residuals for all chain components in the essential graph is both
necessary and sufﬁcient to completely orient any DAG. We start by introducing a VIS:
Deﬁnition 7. Given a general DAG D, a verifying intervention set (VIS) is a set of single-node
interventions I that fully orients the DAG starting from an essential graph, i.e. EI(D) = D. A
minimal VIS (MVIS) is a VIS of minimal size. We denote the size of the minimal VIS for D as m(D).
For each DAG there are many possible VISes. A trivial VIS for any DAG is just the set of all of its
nodes. In general, we are more interested in MVISes, which are also not necessarily unique for a
DAG. For example, the DAG in Fig. 4 has four MVISes: {1, 3, 5}, {1, 3, 6}, {2, 4, 5}, and {2, 4, 6}.
We now show that ﬁnding a VIS for any DAG D can be decomposed twice: ﬁrst we can create a
separate task of ﬁnding a VIS for each of the chain components G of its essential graph E(D), and
then for each G we can create a tree-like CDCT and ﬁnd independently a VIS for each of its residuals:
Theorem 1. A single-node intervention set is a VIS for any general DAG D iff it contains VISes for
each residual R ∈R( ˜TG) for all chain components G ∈CC(E(D)) of its essential graph E(D).
5

An MVIS of D will then contain only the MVISes of each residual of each chain component. An
algorithm using this decomposition to compute an MVIS is given in Appendix F. In general, the size
of an MVIS of D cannot be calculated from just its essential graph, as shown by the two graphs in
Fig. 5. Instead, we propose a universal lower bound that holds for all DAGs in the same MEC:
Theorem 2. Let D be any DAG. Then m(D) ≥P
G∈CC(E(D))
j
ω(G)
2
k
, where ω(G) is the size of the
largest clique in each of the chain components G of the essential graph E(D).
We reiterate how this bound is different from previous work. For a ﬁxed MEC [D] with essential
graph E, it is easy to construct D∗∈[D] such that m(D∗) ≥P
G∈CC(E(D))
j
ω(G)
2
k
by picking the
largest clique in each chain component to be the upstream-most clique. The bound in Theorem 2 gives
a much stronger result: any choice of DAG in the MEC requires this many single-node interventions.
4
A two-phase intervention policy based on DCTs
While in the previous section we started from a known DAG D to construct a CDCT and then
proved an universal lower bound on m(D), in this section we focus on intervention design to learn
the orientations of an unknown DAG starting from its observational essential graph. Theorem 1
proves that to orient a DAG D, we only need to orient the residuals for each of its essential graph
chain components. The deﬁnition of residuals requires the knowledge of a tree-like CDCT for
each component, which can be easily derived from the directed clique graph (DCG) (e.g. through
Algorithm 3 in Appendix D). So, we propose a two phase policy, in which the ﬁrst phase uses
interventions to identify the DCG of each chain components, while the second phase uses interventions
to orient each of the residuals, as described in Algorithm 1. We now focus on describing the ﬁrst
phase of the algorithm and start by introducing two types of abstract, higher-level interventions.
Deﬁnition 8. A clique-intervention on a clique C is a series of single-node interventions that sufﬁces
to learn the orientation of all edges in ΓD that are incident on C. An edge-intervention on an edge
C1 −TG C2 is a series of single-node interventions that sufﬁces to learn the orientation of C1 −TD C2.
A trivial clique-intervention is intervening on all of C, and a trivial edge-intervention is intervening
on all of C1 ∩C2. The clique- and edge- interventions we use in practice are outlined in Appendix H.
Algorithm 1 DCT POLICY
1: Input: essential graph E(D)
2: for component G in CC(E(D)) do
3:
create clique graph ΓG
4:
ΓD = FINDDCG(ΓG)
5:
convert ΓD to a CDCT ˜TD
6:
for clique C in ˜TD do
7:
Let R = Res ˜TD(C)
8:
Intervene on nodes in V (R) un-
til R is fully oriented
9:
end for
10: end for
11: return completely oriented D
Algorithm 2 FINDDCG
1: Input: clique graph ΓG
2: let ΓD = ΓG
3: while ΓD has undirected edges do
4:
let T be a maximum-weight spanning tree of the
undirected component of ΓD
5:
let C be a central node of T
6:
perform a clique-intervention on C
7:
let Pup(C) = IDENTIFYUPSTREAM(C)
8:
let S = V (BC:Pup(C)
T
)
9:
while ΓD has unoriented incident to C \ S do
10:
propagate edges in ΓD
11:
perform an edge-intervention on an edge C1 −ΓG
C2 with C1 ∈C \ S
12:
end while
13: end while
14: return ΓD
The ﬁrst phase of our algorithm, described in Algorithm 2, is inspired by the Central Node algorithm
(Greenewald et al., 2019). This algorithm operates over a tree, so we will have to use a spanning tree:
Deﬁnition 9. (Greenewald et al., 2019) Given a tree T and a node v ∈V (T), we divide T into
branches w.r.t. v. For a node w adjacent to v, the branch B(v:w)
T
is the connected component of
T −{v} that contains w. A central node c is a node for which ∀w adjacent to c : |B(c:w)
T
| ≤| V (T )
2
|.
6

While our algorithm works for general graphs, it will help our intuition to ﬁrst assume that ΓG is
intersection-incomparable. In this case, there are no arrow-meets in ΓD by Prop. 1, nor in any of the
directed clique trees. Thus, after each clique-intervention on a central node C, there will be only one
parent clique upstream and the algorithm will orient at least half of the remaining unoriented edges by
repeated application of Prop. 1. For the intersection-comparable case, two steps can go wrong. First,
after a clique-intervention on C, we may ﬁnd that C has multiple parents in ΓD (i.e. C is at an arrows-
meet). We can prove that even in this case, there is always a single “upstream” branch, identiﬁed
via the IdentifyUpstream procedure, described in Appendix I, which performs edge-interventions
on a subset of the parents. A second step which may go wrong is in the propagation of orientations
along the downstream branches, which halts when encountering intersection-incomparable edges. In
this case, we simply kickstart further propagation by performing an edge-intervention.
The size of the problem is cut in half after each clique-intervention, so that we use at most
P
G∈CC(E(D))⌈log2(|C(G)|)⌉clique-interventions, where C(G) is the set of maximal cliques for
G. Furthermore, if ΓG is intersection-incomparable we use no edge-interventions (see Lemma 8
in Appendix J). The second phase of the algorithm then orients the residuals and uses at most
P
G∈CC(E(D))
P
C∈C(G) | Res ˜TG(C)| −1 single-node interventions (see Lemma 9 in Appendix J).
Theorem 3. Assuming ΓG is intersection-incomparable, Algorithm 1 uses at most (3⌈log2 Cmax⌉+
2)m(D) single-node interventions, where Cmax = maxG∈CC(E(D)) |C(G)|.
In the extreme case in which the essential graph is a tree, a single intervention on the root node
can orient the tree, so m(D) = 1, and |C| = |V (D)| −1, so Theorem 3 says that Algorithm 1 uses
O(log(p)) interventions, which is the scaling of the Bayes-optimal policy for the uniform prior as
discussed in Greenewald et al. (2019).
Remark on intersection-incomparability. Intersection-incomparable chordal graphs were intro-
duced as “uniquely representable chordal graphs” in Kumar & Madhavan (2002). This class was
shown to include familiar classes of graphs such as proper interval graphs. While the assumption of
intersection-incomparability is necessary for our analysis of the DCT policy, the policy still performs
well on intersection-comparable graphs as demonstrated in Section 5. This suggests that the restriction
may be an artifact of our analysis, and the result of Theorem 3 may hold more generally.
5
Experimental Results
We evaluate our policy on synthetic graphs of varying size. To evaluate the performance of a policy
on a speciﬁc DAG D, relative to m(D), the size of its smallest VIS (MVIS), we adapt the notion
of competitive ratio from online algorithms (Borodin et al., 1992; Daniely & Mansour, 2019). We
use ιD(π) to denote the expected size of the VIS found by policy π for the DAG D, and deﬁne our
evaluation metric as:
Deﬁnition 10. The instance-wise competitive ratio (ic-ratio) of an intervention policy π on D is
R(π, D) = ιD′(π)
m(D′). The competitive ratio on an MEC [D] is R(π) = maxD′∈[D]
ιD′(π)
m(D′).
The instance-wise competitive ratio of a policy on a DAG D simply measures the number of
interventions used by the policy relative to the number of interventions used by the best policy for
that DAG, i.e., the policy which guesses that D is the true DAG and uses exactly a MVIS of D to
verify this guess. Thus, a lower ic-ratio is better, and an ic-ratio of 1 is the best possible. In order to
compute the ic-ratio on D, we must compute m(D), the size of a MVIS for D. In our experiments,
we use our DCT characterization of VIS’s from Theorem 1 to decompose the DAG into its residuals,
each of whose MVIS’s can be computed efﬁciently. We describe this procedure in Appendix F.
Smaller graphs. For our evaluation on smaller graphs, we generate random connected moral DAGs
using the following procedure, which is a modiﬁcation of Erdös-Rényi sampling that guarantees
that the graph is connected. We ﬁrst generate a random ordering σ over vertices. Then, for the n-th
node in the order, we set its indegree to be Xn = max(1, Bin(n −1, ρ)), and sample Xn parents
uniformly from the nodes earlier in the ordering. Finally, we chordalize the graph by running the
elimination algorithm (Koller & Friedman, 2009) with elimination ordering equal to the reverse of σ.
We compare the OptSingle policy (Hauser & Bühlmann, 2014), the Minmax and Entropy strategy
of He & Geng (2008), called MinmaxMEC and MinmaxEntropy, respectively, and the coloring-based
strategy of Shanmugam et al. (2015), called Coloring. We also introduce a baseline that picks
7

(a) Average ic-Ratio (small graphs)
(b) Average Time (small graphs)
(c) Average ic-Ratio (larger graphs)
(d) Maximum ic-Ratio (larger graphs)
Figure 6: Comparison of intervention policies over 100 synthetic DAGs.
randomly among non-dominated2 nodes in the I-essential graph, called the non-dominated random
(ND-Random) strategy. As the name suggests, dominated nodes are easily proven to be non-optimal
interventions, so ND-Random is a more fair baseline than simply picking randomly amongst nodes.
In Fig. 6a and Fig. 6b, we show the average ic-ratio and the average run-time for each of the algorithms.
In terms of average ic-ratio, all algorithms aside from ND-Random perform comparably, using on
average 1.4-1.7x more interventions than the smallest MVIS. However, the computation time grows
quite quickly for GreedyMEC, GreedyEntropy, and OptSingle. This is because, when scoring a
node as a potential intervention target, each of these algorithms iterates over all possible parent sets
of the node. Moreover, the GreedyMEC and GreedyEntropy policies then compute the sizes of the
resulting interventional MECs, which can grow superexponentially in the number of nodes (Gillispie
& Perlman, 2013). In Appendix K, we show that in the same setting, OptSingle takes >10 seconds
per graph for just 25 nodes, whereas Coloring, DCT, and Random remain under .1 seconds per graph.
Larger graphs. For our evaluation on large tree-like graphs, we create random moral DAGs of
n = 100, . . . , 300 nodes using the following procedure. We generate a complete directed 4-ary tree
on n nodes. Then, we sample an integer R ∼U(2, 5) and add R edges to the tree. Finally, we ﬁnd a
topological order of the graph by DFS and triangulate the graph using that order. This ensures that
the graph retains a nearly tree-like structure, making m(D) small compared to the overall number of
nodes. In Fig. 6c and Fig. 6d, we show the average and maximum competitive ratio (computation
time is given in Appendix K). For the average graph, our DCT policy and the Coloring policy use
only 2-3 times as many interventions as the theoretical lower bound. Moreover, the worst competitive
ratio experienced by the DCT algorithm is signiﬁcantly smaller than the worst ratio experienced by
the Coloring policy, which suggests that our policy is more adaptive to the underlying difﬁculty of
the identiﬁcation problem.
2A node is dominated if all incident edges are directed, or if it has only a single incident edge to a neighbor
with more than one incident undirected edges
8

6
Related Work
Intervention policies fall under two distinct, but related goals. The ﬁrst is: given a ﬁxed number of
interventions, learn as much as possible about the underlying DAG. This goal is explored in Ghassami
et al. (2017, 2018) and Hauser & Bühlmann (2014). The second goal, which is the one considered
in this paper, is minimum-cost identiﬁcation: completely learn the underlying DAG using the least
number of interventions. We review previous work on policies operating under this objective. As
before, we use ιD(π) to denote the expected size of the VIS found by policy π.
We deﬁne ΠK as the set of policies using interventions with at most K target variables, i.e., |Im| ≤K
for Im ∈I. We use Π∞to represent policies allowing for interventions of unbounded size. A policy
π is K-node minimax optimal for an MEC [D] if π ∈arg minπ′∈ΠK maxD′∈[D] ιD′(π′). Informally,
this is the policy π that in the worst-case scenario (the DAG in the MEC that requires the most
interventions under π) ends up requiring the least interventions. A policy is K-node Bayes-optimal
for an MEC [D] and a prior PD supported only on the MEC [D] if π ∈arg minπ′∈ΠK EPD[ιD(π′)]
In the special cases of K = 1 and K = ∞, we replace K-node by single-node and unbounded,
respectively. Much recent work explores intervention policies under a variety of objectives and con-
straints. Eberhardt (2007) introduced passive, minimax-optimal intervention policies for single-node,
K-node, and unbounded interventions in both the causally sufﬁcient and causally insufﬁcient case,
when the MEC is not known. They also give a passive, unbounded intervention policy when the MEC
is known, and conjectures a minimax lower bound of ⌈ω(E(D))
2
⌉on ιD(π) for such policies. Hauser
& Bühlmann (2014) prove this bound by developing a passive, unbounded minimax-optimal policy.
Shanmugam et al. (2015) develop a K-node minimax lower bound of ω(E(D))
K
log ω(E(D))
K
ω(E(D))
based on separating systems. Kocaoglu et al. (2017) develop a passive, unbounded minimax-optimal
policy when interventions have distinct costs (where ιD(π) is replaced by the total cost of all in-
terventions.) Greenewald et al. (2019) develop an adaptive K-node intervention policy for noisy
interventions which is within a small constant factor of the Bayes-optimal intervention policy, but
the policy is limited to the case in which the chain components of the essential graph are trees. It
is important to note that all of these previous works give minimax optimal policies, i.e. they focus
on minimizing the interventions used in the worst case over the MEC. In contrast, our result in
Theorem 3 is competitive, holding for every DAG in the MEC, and shows that the largest clique is
still a fundamental impediment to structure learning. However, the current result holds only in the
single-node case, whereas previous work allows for larger interventions.
Finally, we note an interesting conceptual connection to Ghassami et al. (2019), which uses undirected
clique trees as a tool for counting and sampling from MECs, suggesting that clique trees and their
variants, such as DCTs, may be broadly useful for a variety of DAG-related tasks.
7
Discussion
We presented a decomposition of a moral DAG into residuals, each of which must be oriented
independently of one another. We use this decomposition to prove that for any DAG D in a MEC
with essential graph E, at least P
G∈CC(E)
j
ω(G)
2
k
interventions are necessary to orient D, where
CC(E) denotes the chain components of E and ω(G) denotes the clique number of G. We introduced
a novel two-phase intervention policy, which ﬁrst uses a variant of the Central-Node algorithm to
obtain orientations for the directed clique graph ΓD, then orients within each residual. We showed
that under certain conditions on the chain components of E, this intervention policy uses at most
(3 log2 Cmax + 2) times as many interventions as the optimal intervention set. Finally, we showed
on synthetic graphs that our intervention policy is more scalable than most existing policies, with
comparable performance to the coloring-based policy of Shanmugam et al. (2015) in terms of average
ic-ratio and better performance in terms of worst-case ic-ratio.
Preliminary results (Appendix K) suggest that the DCT policy is more computationally efﬁcient
than the coloring-based policy on large, dense graphs, but is slightly worse in terms of performance.
Further analysis of these results and possible improvements are left to future work. Our results,
especially the residual decomposition of the VIS, provide a foundation for further on intervention
design in more general settings.
9

Funding transparency statement
Chandler Squires was supported by an NSF Graduate Research Fellowship and an MIT Presidential
Fellowship and part of the work was performed during an internship at IBM Research. The work was
supported by the MIT-IBM Watson AI Lab,
Broader impact statement
Causality is an important concern in medicine, biology, econometrics and science in general (Pearl,
2009; Spirtes et al., 2000; Peters et al., 2017). A causal understanding of the world is required
to correctly predict the effect of actions or external factors on a system, but also to develop fair
algorithms. It is well-known that learning causal relations from observational data alone is not possible
in general (except in special cases or under very strong assumptions); in these cases experimental
(“interventional”) data is necessary to resolve ambiguities.
In many real-world applications, interventions may be time-consuming or expensive, e.g. randomized
controlled trials to develop a new drug or gene knockout experiments. These settings crucially
rely on experiment design, or more precisely intervention design, i.e. ﬁnding a cost-optimal set
of interventions that can fully identify a causal model. The ultimate goal of intervention design is
accelerating scientiﬁc discovery by decreasing its costs, both in terms of actual costs of performing
the experiments and in terms of automation of new discoveries.
Our work focuses on intervention design for learning causal DAGs, which have been notably
employed as models in system biology, e.g. for gene regulatory networks (Friedman et al., 2000) or
for protein signalling networks (Sachs et al., 2005). Protein signalling networks represent the way
cells communicate with each other, and having reliable models of cell signalling is crucial to develop
new treatments for many diseases, including cancer. Understanding how genes inﬂuence each other
has also important healthcare applications, but is also crucial in other ﬁelds, e.g. agriculture or the
food industry. Since even the genome of a simple organism as the common yeast contains 6275
genes, interventions like gene knockouts have to be carefully planned. Moreover, experimental design
algorithms may prove to be a useful tool for driving down the time and cost of investigating the
impact of cell type, drug exposure, and other factors on gene expression. These beneﬁts suggest that
there is a potential for experimental design algorithms such as ours to be a commonplace component
of the future biological workﬂow.
In particular, our work establishes a number of new theoretical tools and results that 1) may drive
development of new experimental design algorithms, 2) allow practitioners to estimate, prior to
beginning experimentation, how costly their task may be, 3) offer an intervention policy that is able
to run on much larger graphs than most of the related work, and provides more efﬁcient intervention
schedules than the rest.
Importantly, our work and in general intervention design algorithms have some limitations. In
particular, as we have mentioned in the main paper, all these algorithms have relatively strong
assumptions (e.g. no latent confounders or selection bias, inﬁnite observational data, noiseless
interventions, or in some case limitations on the graph structure (Greenewald et al., 2019)). If
these assumptions are not satisﬁed in the data, or the practitioner does not realize their importance,
the outcome of these algorithms could be misinterpreted or over-interpreted, leading to wasteful
experiments or overconﬁdent causal conclusions. Wrong causal conclusions may lead to potentially
severe unintended side effects or unintended perpetuation of bias in algorithms.
Even in case of correct causal conclusions, the actualized impact of experimental design depends on
the experiments in which it is used. Potential positive uses cases include decreasing the cost of drug
development, in turn leading to better and cheaper medicine for consumers.
10

References
Borodin, A., Linial, N., and Saks, M. E. An optimal on-line algorithm for metrical task system.
Journal of the ACM (JACM), 39(4):745–763, 1992.
Daniely, A. and Mansour, Y. Competitive ratio versus regret minimization: achieving the best of both
worlds. arXiv preprint arXiv:1904.03602, 2019.
Eberhardt, F. Causation and intervention. Unpublished doctoral dissertation, Carnegie Mellon
University, pp. 93, 2007.
Eberhardt, F., Glymour, C., and Scheines, R. N-1 experiments sufﬁce to determine the causal relations
among n variables. In Innovations in machine learning, pp. 97–112. Springer, 2006.
Friedman, N., Linial, M., Nachman, I., and Pe’er, D. Using bayesian networks to analyze expression
data. Journal of computational biology, 7(3-4):601–620, 2000.
Galinier, P., Habib, M., and Paul, C. Chordal graphs and their clique graphs. In International
Workshop on Graph-Theoretic Concepts in Computer Science, pp. 358–371. Springer, 1995.
Ghassami, A., Salehkaleybar, S., and Kiyavash, N. Optimal experiment design for causal discovery
from ﬁxed number of experiments. arXiv preprint arXiv:1702.08567, 2017.
Ghassami, A., Salehkaleybar, S., Kiyavash, N., and Bareinboim, E. Budgeted experiment design
for causal structure learning. In International Conference on Machine Learning, pp. 1724–1733.
PMLR, 2018.
Ghassami, A., Salehkaleybar, S., Kiyavash, N., and Zhang, K. Counting and sampling from markov
equivalent dags using clique trees. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence,
volume 33, pp. 3664–3671, 2019.
Gillispie, S. B. and Perlman, M. D. Enumerating markov equivalence classes of acyclic digraph
models. arXiv preprint arXiv:1301.2272, 2013.
Greenewald, K., Katz, D., Shanmugam, K., Magliacane, S., Kocaoglu, M., Adsera, E. B., and Bresler,
G. Sample efﬁcient active learning of causal trees. In Advances in Neural Information Processing
Systems, 2019.
Hauser, A. and Bühlmann, P. Two optimal strategies for active learning of causal models from
interventional data. International Journal of Approximate Reasoning, 55(4):926–939, 2014.
He, Y.-B. and Geng, Z. Active learning of causal networks with intervention experiments and optimal
designs. Journal of Machine Learning Research, 9(Nov):2523–2547, 2008.
Hyttinen, A., Eberhardt, F., and Hoyer, P. O. Experiment selection for causal discovery. The Journal
of Machine Learning Research, 14(1):3041–3071, 2013.
Kocaoglu, M., Dimakis, A., and Vishwanath, S. Cost-optimal learning of causal graphs. In Proceed-
ings of the 34th International Conference on Machine Learning-Volume 70, pp. 1875–1884. JMLR.
org, 2017.
Koller, D. and Friedman, N. Probabilistic graphical models: principles and techniques. MIT press,
2009.
Kumar, P. S. and Madhavan, C. V. Clique tree generalization and new subclasses of chordal graphs.
Discrete Applied Mathematics, 117(1-3):109–131, 2002.
Lindgren, E., Kocaoglu, M., Dimakis, A. G., and Vishwanath, S. Experimental design for cost-aware
learning of causal graphs. In Advances in Neural Information Processing Systems, pp. 5279–5289,
2018.
Maathuis, M., Drton, M., Lauritzen, S., and Wainwright, M. Handbook of graphical models. CRC
Press, 2018.
11

Meek, C. Causal inference and causal explanation with background knowledge. In Proceedings of
the Eleventh Conference on Uncertainty in Artiﬁcial Intelligence, pp. 403–410, San Francisco, CA,
USA, 1995. Morgan Kaufmann Publishers Inc. ISBN 1558603859.
Pearl, J. Causality: Models, Reasoning and Inference. Cambridge University Press, New York, NY,
USA, 2nd edition, 2009. ISBN 052189560X, 9780521895606.
Peters, J., Janzing, D., and Schölkopf, B. Elements of Causal Inference - Foundations and Learning
Algorithms. Adaptive Computation and Machine Learning Series. The MIT Press, Cambridge,
MA, USA, 2017.
Sachs, K., Perez, O., Pe’er, D., Lauffenburger, D. A., and Nolan, G. P. Causal protein-signaling
networks derived from multiparameter single-cell data. Science, 308(5721):523–529, 2005.
Shanmugam, K., Kocaoglu, M., Dimakis, A. G., and Vishwanath, S. Learning causal graphs with
small interventions. In Advances in Neural Information Processing Systems, pp. 3195–3203, 2015.
Spirtes, P., Glymour, C., and Scheines, R. Causation, Prediction, and Search. MIT press, 2nd edition,
2000.
Vandenberghe, L., Andersen, M. S., et al. Chordal graphs and semideﬁnite optimization. Foundations
and Trends® in Optimization, 1(4):241–433, 2015.
12

Supplementary material for: Active Structure Learning of Causal DAGs via
Directed Clique Trees
A
Meek Rules
In this section, we recall the Meek rules (Meek, 1995) for propagating orientations in DAGs. Of the
standard four Meek rules, two of them only apply when the DAG contains v-structures. Since all
DAGs that we need to consider do not have v-structures, we include only the ﬁrst two rules here.
Proposition 2 (Meek Rules under no v-structures).
1. No colliders: If a →G b −G c and a is not adjacent to c, then b →G c.
2. Acyclicity: If a →G b →G c and a is adjacent to c, then a →G c.
B
The running intersection property
A useful and well-known property of clique trees, used throughout proofs in the remainder of the
appendix, is the following:
Prop. (Running intersection property). Let γ = ⟨C1, . . . , CK⟩be the path between C1 and CK in
the clique tree TG. Then C1 ∩CK ⊆Ck for all Ck ∈γ.
We refer the interested reader to Maathuis et al. (2018).
C
Proof of Proposition 1
This proposition describes the connection between arrow-meets and intersection comparability. In
order to prove this proposition, we begin by establishing the following propositions:
Proposition 3. Suppose C1 and C2 are adjacent in TG. Then for all v1 ∈C1 \ C2, v2 ∈C2 \ C1, v1
and v2 are not adjacent in G.
Proof. We prove the contrapositive. Suppose v1 ∈C1 \ C2 and v2 ∈C2 \ C1 are adjacent. Then
C′
3 = (C1 ∩C2) ∪{v1, v2} is a clique and belongs to some maximal clique C3. For the induced
subtree property to hold, C3 must lie between C1 and C2, i.e., C1 and C2 are not adjacent.
Proposition 4. Let D be a moral DAG, there are no undirected edges in any of its directed clique
trees TD, and therefore neither in its directed clique graph ΓD.
Proof. (By contradiction). Suppose v1 →D v12 for v1 ∈C1 \ C2 and v12 ∈C1 ∩C2. Suppose
v2 →D v′
12 for v2 ∈C2 \ C1, and v′
12 ∈C1 ∩C2. By the assumption that D does not have
v-structures and by Prop. 3, v12 ̸= v′
12. Similarly, since v12 →D v2 (otherwise there would be a
v-structure with v1 →D v12) and v′
12 →D v1 (otherwise there would be a collider with v2 →D v′
12).
However, this induces a cycle v1 →D v12 →D v2 →D v′
12 →D v1.
Now we can ﬁnally prove the ﬁnal proposition:
Proposition 1. Suppose C1 ∗→TDC2 and C2 ←∗TDC3 in TD. Then these edges are intersection
comparable. Equivalently in the contrapositve, if C1 ∗→TDC2 and C2 ∗−∗TDC4 are intersection
incomparable, we can immediately deduce that C2 →TD C4.
Proof. We prove the contrapositive. If C1 ∩C2 ̸⊆C2 ∩C3 and C1 ∩C2 ̸⊇C2 ∩C3, then there
exist nodes v12 ∈(C1 ∩C2) \ C3 and v23 ∈(C2 ∩C3) \ C1. Since v12 and v23 are both in the same
clique C2 they are adjacent in the underlying DAG D, i.e. v12 −D v23. Moreover since C1 ∗→TDC2
by the deﬁnition of a directed clique graph, this edge is oriented as v12 →D v23. Then by Prop. 4,
C2 →TD C3.
13

{1,2}
{2,4,5}
{2,3,4}
{2,4,6}
{1,2}
{2,4,5}
{2,3,4}
{2,4,6}
1
2
4
5
6
3
Figure 7: A DAG, its DCT with a conﬂicting source, and its DCG without a conﬂicting source.
D
Proof of Lemma 2
Lemma 2. For any moral DAG D, one can always construct a CDCT with no arrow-meets.
Proof. To construct a CDCT with no arrow-meets, our approach is to ﬁrst construct the DCT in a
special way, so that after contraction, there are no arrow-meets. In particular, we need a DCT such
that each bidirected component has at most one incoming edge. A DCT in which this does not hold is
said to have conﬂicting sources, formally:
Deﬁnition 11. A directed clique tree TD has two conﬂicting sources C0 and CK+1, if C0 →TD C1
and CK ←TD CK+1, and C1 and CK are part of the same bidirected component B ∈B(TD), i.e.
C1, CK ∈B, possibly with C1 = CK.
An example of a clique tree with conﬂicting sources is given in Fig. 7. The ﬁrst DCT has conﬂicting
sources {1, 2} and {2, 3, 4}, while the second DCT does not have conﬂicting sources.
We will now show that Algorithm 3 constructs a DCT with no conﬂicting sources. This is sufﬁcient
to prove Lemma 2, since after contraction, the resulting CDCT will have no arrow-meets.
First, Algorithm 3 constructs a weighted clique graph WG, which is a complete graph over vertices
C(G), with the edge C1 −WG C2 having weight |C1 ∩C2|. We will show that at each iteration i,
there are no conﬂicting sources in TD. This is clearly true for i = 0 since TD has no edges to begin.
At a given iteration i, suppose that the candidate edge e = C1 ∗→C2 is a maximum-weight edge that
does not create a cycle, i.e. e ∈E, but that it will induce conﬂicting sources. That is, the current
TD already contains C2 ←∗C3 ←∗. . . ←∗CK−1 ←CK, where we choose CK that has no parents.
Note that we can do this by following any directed/bidirected edges upstream (away from C2), which
must terminate since TD is a tree and thus does not have cycles.
By Prop. 1, C1 ∩C2 ⋚C2 ∩C3. In this case, C1 ∩C2 ⊆C2 ∩C3, since C2 ←∗C3 was already picked
as an edge and thus cannot have less weight (in other words, it cannot have a smaller intersection)
than C1 ∗→C2. Furthermore, since C1 −C2 −C3 is a valid subgraph of the clique tree, we must
have C1 ∩C3 ⊆C2 by the running intersection property of clique trees (see Appendix B). Combined
with C1 ∩C2 ⊆C2 ∩C3, we have C1 ∩C3 = C1 ∩C2. This means that C1 −C3 is also a valid edge
in the weighted clique graph and it has the same weight (C1 ∩C3) as the C1 −C2 edge (C1 ∩C2).
Moreover since C1 ∗→C2 then this edge will also preserve the same orientations C1 ∗→C3. Thus,
C1 ∗→C3 is another candidate maximum-weight edge that does not create a cycle. We may continue
this argument, replacing C2 by Ck, to show that C1 ∗→CK is a maximum weight edge that does not
create a cycle. Since CK has no parents, there are still no conﬂicting sources after adding C1 ∗→CK.
Since we always pick a maximum-weight edge that does not create a cycle, this algorithm creates
a maximum-weight spanning tree of WG (Koller & Friedman, 2009), which is guaranteed to be a
clique tree of G Koller & Friedman (2009).
E
Proof of Theorem 1
We restate the theorem here:
Theorem 1. A single-node intervention set is a VIS for any general DAG D iff it contains VISes for
each residual R ∈R( ˜TG) for all chain components G ∈CC(E(D)) of its essential graph E(D).
14

Algorithm 3 CONSTRUCT_DCT
1: Input: DAG D
2: let WG be the weighted clique graph of G = skel(D)
3: let TD be the empty graph over V (WG)
4: for i = 1, . . . , |V (WG)| −1 do
5:
let E be the set of maximum-weight edges of WG that do not create a cycle when added to TD
6:
select e ∈E s.t. there are no conﬂicting sources
7:
add e to TD
8: end for
9: Contract the bidirected components of TD and create the CDCT ˜TD
10: Return ˜TD
{5,7}
{3,5,6}
{6,8}
{1,2,3}
{1,4}
1
2
3
5
4
7
6
8
1
2
3
5
4
7
6
8
1
2
3
5
4
7
6
8
Figure 8: A DAG, its contracted directed clique tree, its residuals, and its residual essential graph.
In order to prove the following theorem we start by introducing a few useful concepts and results.
E.1
Residual essential graphs
The residuals decompose the DAG into parts which must be separately oriented. Intuitively, after
adding orientations between all pairs of residuals, the inside of one residual is cut off from the insides
of other residuals. The following deﬁnition and lemmas formalize this intuition.
Deﬁnition 12. The residual essential graph Eres(D) of D has the same skeleton as D, with v1 →Eres(D)
v2 iff v1 →D v2 and v1 and v2 are in different residuals of ˜TD.
The following lemma establishes that after ﬁnding the orientations of edges in the DCT, the only
remaining unoriented edges are in the residuals.
Lemma 3. The oriented edges of Eres(D) can be inferred directly from the oriented edges of TD.
Proof. In order to prove this theorem, we ﬁrst introduce an alternative characterization of the
residual essential graph deﬁned only in terms of the orientations in the contracted DCT and prove its
equivalence to Deﬁnition 12. Let E′
res(D) have the same skeleton as D, with i →E′res(D)j if and only if
j ∈Res ˜TD(B) and i ∈P, for some B ∈B(TD) and its unique parent P.
Suppose v1 →D v2 for v1 ∈R1 and v2 ∈R2, with R1, R2 ∈R( ˜TD) and R1 ̸= R2. Let
R1 = Res ˜TD(B1) and R2 = Res ˜TD(B2) for B1, B2 ∈B( ˜TD). There must be at least one clique
C1 ∈B1 that contains v1, and likewise one clique C2 ∈B2 that contains v2. Since v1 and v2 are
adjacent, by the induced subtree property there must be some maximal clique on the path between
C1 and C2 which contains v1 and v2. Let C12 be the clique on this path containing v1 and v2 that
is closest to C1. Then, the next closest clique to C1 must not contain v2, so we will call this clique
C1\2. Since v1 →D v2, we know that C1\2 →TD C12, hence C1\2 and C12 are in different bidirected
components, and thus v1 →Eres(D) v2.
Lemma 4. The Eres(D) is complete under Meek’s rules (Meek, 1995).
Proof. Since Meek rules are sound and complete rules for orienting PDAGs (Meek, 1995), and in our
setting only two of the Meek rules apply (see Prop. 2 in Appendix A), it sufﬁces to show that neither
applies for residual essential graphs.
15

First, suppose i →Eres(D) j and j →Eres(D) k. We must show that if i and k are adjacent, then
i →Eres(D) k, i.e. the acyclicity Meek rule does not need to be invoked.
We use the alternative characterization of Eres(D) from the proof of Lemma 3, which establishes that
i →E j iff. j ∈ResTD(B) and i ∈P for some B ∈B(TD) and its unique parent P.
Since j →Eres(D) k, there must exist some component Bjk ∈B(TD) containing j and k whose parent
component Bj\k contains j but not k, i.e. Bj\k →˜TD Bjk. Likewise, there must be a component Bij
containing i and j whose parent component Bi\j contains i but not j, i.e. Bi\j →˜TD Bij. Moreover,
since there is a clique on {i, j, k}, there must be at least one component Bijk containing i, j and k.
We will prove that Bjk and Bj\k both contain i, which implies i →˜TD k.
Let γ be the path in ˜TD between Bi\j and Bjk. This path must contain the edge Bj\k →Bjk, since
Bi\j is upstream of Bjk, and TD is a tree. By the induced subtree property on k, no component on
the path other than Bjk can contain k. Now consider the path between Bijk and Bi\j. By the induced
subtree property on k, this path must pass through Bjk. Finally, by the induced subtree property on i,
Bjk and Bj\k must both contain i.
Now, we prove that also the ﬁrst Meek rule is not invoked. Suppose i →Eres(D) j, and j is adjacent to
k. We must show that if i is not adjacent to k, then j →Eres(D) k.
Since {i, j, k} do not form a clique, there must be distinct components containing i →j and j →k.
Let Bij and Bjk denote the closest such components in ˜TD, which are uniquely deﬁned since ˜TD is a
tree. Since i is upstream of k, Bij must be upstream of Bjk. Let P := pa ˜TD(Bjk), we know j ∈P
since it is on the path between Bij and Bjk (it is possible that P = Bij). Since we picked Bjk to be
the closest component to Bij containing {j, k}, we must have k ̸∈P, so indeed j →G k.
For an example of the residual essential graph, see Fig. 8. Lemma 4 implies that the residuals must be
oriented separately, since the orientations in one do not impact the orientations in others.
E.2
Proof for a moral DAG
We then prove the result for a moral DAG D:
Lemma 5 (VIS Decomposition). An intervention set is a VIS for a moral DAG D iff it contains VISes
for each residual of ˜TD. This implies that ﬁnding a VIS for D can be decomposed in several smaller
tasks, in which we ﬁnd a VIS for each of the residuals in R( ˜TD).
Proof.
VISes of residuals are necessary. We ﬁrst prove that any VIS I of D must contain VISes for each
residual of D. Consider the residual essential graph Eres(D) of D. We show that if we intervene on a
node c1 in the residual R1 = Res ˜TD(B1) of some B1 ∈B( ˜TD), then the only new orientations are
between nodes in R1, or in other words, each residual needs to be oriented independently.
By Deﬁnition 12, all edges between nodes in different residuals are already oriented in Eres(D). A
new orientation between nodes in R1 will not have any impact for the nodes in the other residuals,
which we can show by proving that Meek rules described in Prop. 2 would not apply outside of the
residual. In particular, Meek Rule 1 does not apply at all, since b and c must be in the same residual
since the edge is undirected, but then a is adjacent to c since it’s a clique. Likewise, a −Eres(d) c, then
a and b are in the same residual, so Meek Rule 2 only orients edges with both endpoints in the same
residual.
VISes of residuals are sufﬁcient. Now, we show that if I contains VISes for each residual of D,
then it is a VIS for D, i.e. that orienting the residuals will orient the whole graph by applying
recursively Meek rules. We will accomplish this by inductively showing that all edges in each
bidirected component are oriented. Let γ = ⟨B1, . . . , Bn⟩be a path from the root of ˜TD to a leaf of
˜TD. As our base case, all edges in B1 are oriented, since B1 = Res ˜TD(B1). Now, as our induction
hypothesis, suppose that all edges in Bi−1 are oriented.
The edges between nodes in Bi are partitioned into three categories: edges with both endpoints also
in Bi−1, edges with both endpoints in Res ˜TD(Bi), and edges with one endpoint in Bi−1 and one
16

Algorithm 4 FIND_MVIS_DCT
1: Input: Moral DAG D
2: let ˜TD be the contracted directed clique tree of D
3: let S = ∅
4: for component B of TD do
5:
let R = Res ˜TD(B)
6:
let S′ = FIND_MVIS_ENUMERATION(G[R])
7:
let S = S ∪S′
8: end for
9: Return S
Algorithm 5 FIND_MVIS_ENUMERATION
1: Input: DAG D
2: if D is a clique then
3:
Let π be a topological ordering of D
4:
Let S include even-indexed element of π
5:
Return S
6: end if
7: for s = 1, . . . , |V (D)| do
8:
for S ⊆V (D) with |S| = s do
9:
if S fully orients D then
10:
Return S
11:
end if
12:
end for
13: end for
endpoint in Res ˜TD(Bi). The ﬁrst category of edges are directed by the induction hypothesis, and
the second category of edges are directed by the assumption that I contains VISes for each residual.
It remains to show that all edges in the third category are oriented. Each of these edges has one
endpoint in some Ci−1 ∈Bi−1 and one endpoint in some Ci in Bi, so we can ﬁx some Ci−1 and Ci
and argue that all edges from Ci−1 ∩Ci to Ci \ Ci−1 are oriented.
Since Ci−1 →RD Ci, there exists some ci−1 ∈Ci−1 \ Ci and c′ ∈Ci ∩Ci−1 such that ci−1 →D c′.
By Prop. 3, ci−1 is not adjacent to any ci ∈Ci \ Ci−1, so Meek Rule 1 ensures that c′ →D ci is
oriented. For any other node c′′ ∈Ci−1 ∩Ci, either c′ →D c′′, in which case Meek Rule 2 ensures
that ci−1 →D c′′ and the same argument applies, or c′′ →D c′, in which case Meek Rule 2 ensures
that c′′ →D ci.
E.3
Proof for a general DAG
We can now easily prove the theorem for any DAG D:
Theorem 1. A single-node intervention set is a VIS for any general DAG D iff it contains VISes for
each residual R ∈R( ˜TG) for all chain components G ∈CC(E(D)) of its essential graph E(D).
Proof. By the previous result (Lemma 5) and Lemma 1 from (Hauser & Bühlmann, 2014).
F
Algorithm for ﬁnding an MVIS
An algorithm using the decomposition into residuals to compute a minimal verifying intervention set
(MVIS) is described in Algorithms 4 and 5. Compared to running Algorithm 5 on any moral DAG,
using Algorithm 4 ensures that we only have to enumerate over subsets of the nodes in each residual,
which in general require far fewer interventions. Moreover, the residual of any component containing
a single clique is itself a clique, which have easily characterized MVISes, and Algorithm 5 efﬁciently
computes.
17

G
Proof of Theorem 2
First, we prove the following proposition:
Proposition 5. Let D be a moral DAG, E = E(D) and let ˜TD contain a single bidirected component.
Then m(D) ≥
j
ω(E)
2
k
.
Proof. Let C1 ∈arg maxC∈C(E) |C|. By the running intersection property (see Appendix B), for
any clique C2, C1 ∩C2 ⊆C2 ∩Cadj for Cadj adjacent to C2 in TD. Since Cadj ↔TD C2, we have
v12 →D v2\1 for all v12 ∈C1 ∩C2 and v2\1 ∈C2 \ C1, i.e. there is no node in D outside of C1
that points into C1. Thus, since the Meek rules only propagate downward, intervening on any nodes
outside of C1 does not orient any edges within C1. Finally, since C1 is a clique, each consecutive
pair of nodes in the topological order of C1 must have at least one of the nodes intervened in order
to establish the orientation of the edge between them. This requires at least
j
|C1|
2
k
interventions,
achieved by intervening on the even-numbered nodes in the topological ordering.
Now we can prove the following result for a moral DAG D:
Lemma 6. Let D be a moral DAG and let G = skel(D). Then m(D) ≥
j
ω(G)
2
k
, where ω(G) is the
size of the largest clique in G.
Consider a path γ from the source of ˜TD to the bidirected component containing the largest clique, i.e.,
γ = ⟨B1, . . . , BZ⟩. For each component, pick C∗
i ∈arg maxC∈Bi |C|. Also, let Ri = Res ˜TD(Bi).
We will prove by induction that Pz
i=1 m(D[Ri]) ≥maxz
i=1
j
|C∗
i |
2
k
for any z = 1, . . . , Z. As a base
case, it is true for z = 1, since R1 = B1 and by Prop. 5.
Suppose the lower bound holds for z −1. If C∗
z is not the unique maximizer of
j
|C∗
z |
2
k
over
i = 1, . . . , z, the lower bound already holds. Thus, we consider only the case where Bz is the unique
maximizer.
Let Sz = C∗
z ∩Bz−1. By the running intersection property (see Appendix B), Sz is contained in the
clique Cadj in Bz−1 which is adjacent to C∗
z in TD. Since Cadj is distinct from C∗
z , |C∗
adj| ≥|Sz| + 1,
and by the induction hypothesis we have that
z−1
X
i=1
m(D[Ri]) ≥
max
i=1,...,z−1
|C∗
i |
2

≥
|C∗
z−1|
2

≥
|Cadj|
2

≥
|Sz| + 1
2

Finally, applying Prop. 5,
|Sz + 1|
2

+ m(D[Rz]) ≥
|Sz| + 1
2

+
|C∗
z ∩Rz|
2

≥
|C∗
z|
2

where the last equality holds since |Sz| + |C∗
z ∩Rz| = |C∗
z| and by the property of the ﬂoor function
that
 a+1
2

+
 b
2

≥
 a+b
2

, which can be easily checked.
Finally we can prove the theorem:
18

Algorithm 6 CLIQUEINTERVENTION
1: Input: Clique C
2: while C −ΓD C′ unoriented for some C′ do
3:
if ∃v non-dominated in C then
4:
Pick v ∈C at random among non-dominated nodes.
5:
else
6:
Pick v ∈C at random.
7:
end if
8:
Intervene on v.
9: end while
10: Output: Pup(C)
Algorithm 7 EDGEINTERVENTION
1: Input: Adjacent cliques C, C′
2: while C −ΓD C′ unoriented do
3:
Pick v ∈C ∩C′ at random.
4:
Intervene on v.
5: end while
6: Output: Pup(C)
Theorem 2. Let D be any DAG. Then m(D) ≥P
G∈CC(E(D))
j
ω(G)
2
k
, where ω(G) is the size of the
largest clique in each of the chain components G of the essential graph E(D).
Proof. By Lemma 6 and Lemma 1 in Hauser & Bühlmann (2014).
H
Clique and Edge Interventions
We present the procedures that we use for clique- and edge-interventions in Algorithm 6 and Algo-
rithm 7, respectively.
I
Identify-Upstream Algorithm
Given the clique graph, a simple algorithm to identify the upstream branch consists of performing an
edge-intervention on each pair of parents of C to discover which is the most upstream. However, if
the number of parents of C is large, this may consist of many interventions. The following lemma
establishes that the only parents which are candidates for being the most upstream are those whose
intersection with C is the smallest:
Proposition 6. Let Pup(C) ∈paΓD(C) be the parent of C which is upstream of all other parents.
Then Pup(C) ∈PΓD(C), where PΓD(C) is the set of parents of C in ΓD with the smallest intersection
size, i.e., P ∈PΓD(C) if and only if P →ΓD C and |P ∩C| ≤|P ′ ∩C| for all P ′ ∈paΓD(C).
Proof. We begin by citing a useful result on the relationship between clique trees and clique graphs
when the clique contains an intersection-comparable edge:
Lemma 7 (Galinier et al. (1995)). If C1 −TG C2 −TG C3 and C1 ∩C2 ⊆C2 ∩C3, then C1 −ΓG C3.
Corollary 1. If C1 −TG C2 −TG C3 and C1 ∩C2 ⊆C2 ∩C3, then C1 ∩C3 = C1 ∩C2.
Proof. By the running intersection property of clique trees (see Appendix B), C1 ∩C3 ⊆C2.
Combined with C1 ∩C2 ⊆C2 ∩C3 and simple set logic, the result is obtained.
Every parent of C is adjacent in ΓD to every other parent of C by Prop. 1 and Lemma 7, and since
every edge has at least one arrowhead, there can be at most one parent of C that does not have an
incident arrowhead.
19

Algorithm 8 IDENTIFYUPSTREAM
1: Input: Clique C
2: for P1, P2 ∈PΓD(C) do
3:
perform an edge-intervention on P1 −ΓD P2
4: end for
5: Output: Pup(C)
Now we show that this parent must be in PΓD(C). Corollary 1 implies that for any triangle in ΓG, two
of the edge labels (corresponding to intersections of their endpoints) must be equal. If P ∈PΓD(C)
and P ′ ∈paTD(C) \ PΓD(C), then the labels of P →ΓD C and P ′ →ΓD C are of different size
and thus cannot match. Therefore, the label of P ∩P ′ = P ∩C. Finally, since we already know
P →ΓD C, it must also be the case that P →ΓD P ′.
J
Proof of Theorem 3
We start by proving bounds for each of the two phases:
Lemma 8. Algorithm 2 uses at most ⌈log2 |C|⌉clique-interventions. Moreover, assuming TG is
intersection-incomparable, Algorithm 2 uses no edge-interventions.
Proof. Since TG is intersection-incomparable, after a clique-intervention on C, orientations propagate
in all but at most one branch of TG out of C. By the deﬁnition of a central node, the one possible
remaining branch has at most half of the nodes from the previous time step, so the number of edges
in TG reduces by at least half after each clique-intervention. Thus, there can be at most ⌈log2 |C|⌉
clique-interventions.
For ease of notation, we will overload the symbol CC for the chain components of a chain graph G to
take a DAG as an argument, and return the subgraphs corresponding to the chain components of its
essential graph. Formally, CC(D) = {D[V (G)] | G ∈CC(E(D))}.
Lemma 9. The second phase of Algorithm 1 (line 6-8) uses at most P
C∈C(D′) | Res ˜TD′(C)| −1
single-node interventions for the moral DAG D′ ∈CC(D).
Proof. Eberhardt et al. (2006) show that n −1 single-node interventions sufﬁce to determine the
orientations of all edges between n nodes. We sum this value over all residuals.
Theorem 3. Assuming ΓG is intersection-incomparable, Algorithm 1 uses at most (3⌈log2 Cmax⌉+
2)m(D) single-node interventions, where Cmax = maxG∈CC(E(D)) |C(G)|.
Proof. Consider a moral DAG D′ ∈CC(D).
We will show that Algorithm 1 uses at most
(3⌈log2 |C(E(D))|⌉+ 2)m(D′) single-node interventions. The result then follows since m(D) =
P
D′∈CC(D) m(D′), the total number of interventions used by Algorithm 1 is the sum over the number
interventions used for each chain component, and Cmax ≥|C(E(D))| for all D′.
Assume that for each clique-intervention in Algorithm 2, we intervene on every node in the clique.
Then, the number of single-node interventions used by each clique intervention is upper-bounded
by ω(G). By Theorem 2 and the simple algebraic fact that ∀a ∈N, a ≤3⌊a
2⌋(which can be
proven simply by noting that if a is even a ≤3 a
2 and if a is odd a ≤3 a−1
2 ., ω(G) ≤3m(D),
Algorithm 2 uses at most 3m(D) single-node interventions. Next, by Lemma 5 and Lemma 9,
and the fact that ∀a ∈N, a −1 ≤2⌊a
2⌋, the second phase of Algorithm 1 uses at most 2m(D)
single-interventions.
K
Additional Experimental Results
K.1
Scalability of OptSingle
We use the same graph generation procedure as outlined in Section 5. We compare OptSingle,
Coloring, DCT, and ND-Random on graphs of up to 25 nodes in Fig. 9. We observe that at 25 nodes,
20

(a) Average ic-ratio
(b) Average Computation Time
Figure 9: Comparison (over 100 random synthetic DAGs)
(a) Average Computation Time
OptSingle already takes more than 2 orders of magnitude longer than either the Coloring or DCT
policies to select its interventions, while achieving comparable performance in terms of average
competitive ratio.
K.2
Computation time for large tree-like graphs
In this section, we report the results on average computation time associated with Fig. 6c from
Section 5. We ﬁnd similar scaling for our DCT policy and the Coloring policy, both taking about
5-10 seconds for graphs of up to 500 nodes, as seen in Fig. 10a.
K.3
Comparison on large dense graphs
In this section, we generate dense graphs via the same Erdös-Rényi-based procedure described in
Section 5. We show in Fig. 11 that the DCTpolicy is more scalable to dense graphs than the Coloring
policy, but that our performance becomes slightly worse than even ND-Random. Since the size of the
MVIS is already large for such dense graphs, this suggests that the two-phase nature of the DCTpolicy
may be too restrictive for such a setting. Further analysis of the graphs on which different policies do
well is left to future work.
21

(a) Average ic-ratio
(b) Average Computation Time
Figure 11: Comparison (over 100 random synthetic DAGs)
22

