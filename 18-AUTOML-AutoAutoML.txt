ICML 2018 AutoML Workshop
Towards Further Automation in AutoML
Matthias Feurer
feurerm@cs.uni-freiburg.de
Frank Hutter
fh@cs.uni-freiburg.de
University of Freiburg
Abstract
Even though recent AutoML systems have been successful in various applications, they
introduce new hyper-hyperparameters of their own, including the choice of the evaluation
strategy used in the loss function, time budgets to use and the optimization strategy with
its hyper-hyperparameters. We study whether it is possible to make these choices in a
data-driven way for a dataset at hand. Using 437 datasets from OpenML, we demonstrate
the possibility of automating these choices, that this improves over picking a ﬁxed strategy
and that for diﬀerent time horizons diﬀerent strategies are necessary.
Keywords: Automated Machine Learning, Meta-Learning
1. Introduction
Recent advancements in hyperparameter optimization (Snoek et al., 2012, 2014, 2015; Hut-
ter et al., 2011; Shahriari et al., 2016) triggered the development of new AutoML systems
based on them, e.g., Auto-WEKA (Thornton et al., 2013), hyperopt-sklearn (Komer et al.,
2014) and Auto-sklearn (Feurer et al., 2015a). These systems deﬁne a suitable space of
machine learning (ML) pipelines and perform a combined optimization of all of its choices
(diﬀerent preprocessors, classiﬁers, hyperparameter settings, etc). As a side-product, cur-
rent AutoML systems introduce new hyper-hyperparameters regarding the choice of the
evaluation strategy used in the loss function, time budgets to use and the optimization
strategy. In this paper we investigate whether we can automate these choices in a data-
driven way to avoid overwhelming users with too many options. As a ﬁrst step, we propose
a solution based on greedy submodular function minimization and algorithm selection.
Speciﬁcally, we construct several combinations of evaluation strategies and portfolios
of ML pipelines, both independently and complementary to each other, and then use a
selector to choose the most appropriate one given a new data set. Using 437 datasets from
OpenML (Vanschoren et al., 2014), we demonstrate that it is possible to improve over
picking a ﬁxed combination of evaluation strategy and portfolio of ML pipelines (which we
will refer to as policy). Moreover, we show that it is possible to build a portfolio of policies
with the greedy algorithm which performs up to 10% better than choosing the single best
policy for a time horizon from the test set.
2. Automation in Current AutoML Systems
In this section we brieﬂy review the state of automation of the AutoML toolkit Auto-
sklearn (Feurer et al., 2015a) under the assumption of having access to tabular data and
c⃝2018 M. Feurer & F. Hutter.

Towards Further Automation in AutoML
a given target metric. Our ﬁndings also apply to other AutoML systems, such as Auto-
WEKA (Thornton et al., 2013) and TPOT (Olson et al., 2016). We also summarize an
updated version of Auto-sklearn for the 2nd AutoML challenge, which is the system we aim
to automate further.
Auto-sklearn is a plugin-in replacement for any scikit-learn estimator (Pedregosa et al.,
2011). However, users are faced with several design decisions that inﬂuence Auto-sklearn’s
internal optimization process. More speciﬁcally, users need to specify an evaluation strat-
egy and its arguments, a maximum budget for the evaluation of individual ML pipelines,
hyper-hyperparameters for an ensembling strategy (which replaces the model selection pro-
cess inside Auto-sklearn), and ﬁnally which classiﬁers and pipeline components to search
over. While we have not formally studied this, for example, we expect the default holdout
evaluation strategy to be a bad choice for small datasets (< 5000 data points) due to a
high risk of overﬁtting the hyperparameters to the holdout data. For advanced users it
is also possible to change hyper-hyperparameters of the internal optimization procedure
SMAC (Hutter et al., 2011).
Based on our experience in the ﬁrst AutoML challenge (Guyon et al., 2015, 2016), we
adapted Auto-sklearn for the second AutoML challenge. We found that for large datasets,
sophisticated algorithms were likely to run over the allocated time for individual evaluations,
even when using the simple holdout strategy (rather than cross validation). Therefore, we
replaced holdout with successive halving (Jamieson and Talwalkar, 2016) (see also Appendix
B), but included a fallback to cross-validation for datasets with less than 1000 datapoints.
Furthermore, resembling Feurer et al. (2015b,a) we constructed a portfolio of complimentary
ML pipelines to warmstart the Bayesian optimization procedure which we used to ﬁnd
performant ML pipelines. We also made several other manual decisions, such as imposing
an upper limit on the amount of datapoints and the number of features we use for training.
This work is mostly inspired by the surprisingly large number of manual decisions we had
to make for the updated version of Auto-sklearn.
3. Related Work
Our proposed methodology draws inspiration from several previous works and we reference
those where appropriate. The idea of optimizing the target function of hyperparameter
optimization in ML has not yet been tackled. To the best of our knowledge, only Guyon
et al. (2015) acknowledge the lack of such an approach; more speciﬁcally, they mention
that “there is no attempt to treat the problem [of full model selection] as the optimization
of a regularized functional J2 with respect to both (1) modeling choices and (2) data split.”
We aim to solve a more general problem, which is not limited to avoiding overﬁtting.
For example, our approach also supports determining a reasonable subsampling strategy
(e.g., depending on the given time budgets, there may not be enough time to perform
hyperparameter optimization using a holdout strategy).
Several existing works compare diﬀerent evaluation strategies (Kohavi, 1995; Petrak,
2000; Yadav and Shukla, 2016). Our goal is not to compare diﬀerent strategies per se, but
to automate the choice between them based on meta-data.
Most similar in spirit is Lindauer et al.’s (2015) AutoFolio, which automatically con-
ﬁgures an extensive algorithm selection framework for hard combinatorial problems. Our
2

Towards Further Automation in AutoML
proposed approach applies similar ideas to the domain of AutoML, but algorithm selection
is only one part of it.
4. Methodology
As highlighted in Section 2, current AutoML systems still have several adjustable knobs
(hyper-hyperparameters) which aﬀect the global optimization strategy used to ﬁnd ML
pipelines (a pipeline consisting of data cleaning such as missing value imputation and feature
normalization as well a classiﬁer, see for example Feurer et al. (2015a). Extending Feurer
et al. (2015a), we aim to create an AutoML system by optimizing:
V∗, ω∗∈argmin
V∈V,ω∈Ω
X
D∈D
π(V, ω, D, b),
(1)
where V is an evaluation strategy, such as holdout or cross-validation, π is a global optimiza-
tion algorithm such as SMAC (Hutter et al., 2011) with hyper-hyperparameter searchspace
Ω, which is instantiated with a hyper-hyperparameter vector ω. The remaining variables
need to be provided by the user – D is a set of meta training datasets which we use to
optimize the AutoML-system and b is a budget the AutoML system must abide (such as
total runtime).
4.1. Optimizing Average-Case Performance for a Set of Datasets
We now describe a way to construct an AutoML system by optimizing Equation 1; we
note that this is similar to our submission to the second AutoML challenge discussed in
Section 2. It constructs a portfolio of ML pipelines and chooses the evaluation strategy for
these pipelines.
In our setting, a portfolio P is an ordered set of untrained ML pipelines, which is se-
lected oﬄine. The portfolio aims to provide a diverse set of ML pipelines to be applied to
new datasets. Together with an evaluation strategy V (see Appendix B for the evaluation
strategies we consider), it provides a policy π(P, V) of how to tackle a new dataset. Our
base strategy for portfolio building is greedy submodular function minimization (Krause
and Golovin, 2014), which has already been successfully applied to combinatorial optimiza-
tion (Xu et al., 2010, 2011; Seipp et al., 2015).
Given a set of ML pipelines S, a set of meta training datasets D, the performances of
all ML pipelines in S on all training datasets, and the empty portfolio P, for each s ∈S
we evaluate the performance of P′ = P ∪{s}.
We continue with P′ which minimizes
L(P′) = P
D∈D L(argmins∈P′ L(s, Dtest), Dtest), remove the selected s from S, and repeat
until |P| reaches a pre-deﬁned limit. In the naive setting we only look at the test error,
but to reduce the risk of overﬁtting, we use the following alternative strategy: For each
extended portfolio P′ = P ∪{s} and each dataset D ∈D we select the pipeline s with the
lowest validation error on D. The loss of P′ is calculated by applying the pipelines selected
on the validation set to the test set: L(P′) = P
D∈D L(argmins∈P′ L(s, Dval), Dtest).
As the losses for datasets can live on diﬀerent scales, we employ the simple regret scaled
between zero and one for each dataset. We compute the statistics for zero-one scaling by
looking at all evaluation strategies we consider.
3

Towards Further Automation in AutoML
Because we need to choose a maximal time budget for invoking each conﬁguration, we
also propose a strategy to learn these budgets with the greedy algorithm: When calculating
the gain of adding a conﬁguration to the portfolio, we pick the time limit resulting in the
lowest loss (in case of ties, we pick the shorter time limit).
4.2. Constructing a Portfolio of specialized AutoML Systems
Our approach for constructing an AutoML system presented in the previous subsection
only optimizes the hyper-hyperparameters Ωby constructing a portfolio of ML pipelines,
but has no measure to optimize the evaluation strategy. Since the evaluation strategy and
its parametrization is one of the most important choices and highly dataset-dependent,
we actually want to construct multiple complementary AutoML systems by optimizing
Equation 1 for diﬀerent subsets of D and decide on a per-dataset basis which one to use.
We propose to optimize the evaluation strategy by enumerating all diﬀerent choices;
for each evaluation strategy V, we greedily build a portfolio P and compute a score for
V by simulating the validation performance that P would achieve using V. After having
chosen one (V, P) combination, we continue to use the same approach, but use the marginal
improvement over the existing set of policies as our loss function (both for adding new ML
pipelines to a portfolio and selecting a policy). Marginal improvement is the improvement
of the policy under construction over the current set of policies; this can be computed using
an oracle selector1 to always choose the correct policy for each training dataset. While this
procedure could be repeated until the training loss is zero, we stop the construction after
the third policy. We will refer to this set of policies as complementary policies. As a
baseline, we construct a set of policies by constructing one portfolio for each evaluation
strategy using the greedy algorithm described in Section 4.1 (we will refer to these as
independent policies).
Given a set of AutoML policies, we can use dataset meta-features to select, on a per-
dataset basis, which policy to use. For this, we loosely follow the selector design of Hy-
dra (Xu et al., 2010, 2011): for each pair of AutoML policies, we ﬁt a random forest to predict
whether playing policy πA outperforms playing policy πB given dataset meta-features.
We collect training targets by simply applying the policies to the training datasets,
measuring their real-valued performance values and using their diﬀerence as the label for a
cost-sensitive pairwise binary classiﬁcation. At prediction time (i.e., when tackling a new
dataset), we evaluate all pairwise models, for each policy π add up the probabilities for π
to outperform each other policies, and select the policy with the highest score.
5. Experiments
5.1. Setup
We brieﬂy describe our experimental setup and refer to Appendix C for a detailed de-
scription. For this paper we restrict ourselves to constructing ML pipelines based on XG-
Boost (Chen and Guestrin, 2016); speciﬁcally, we optimize 13 hyperparameters of XGBoost
and 9 hyperparameters of an additional data preprocessing pipeline, which we give in Table
2 in Appendix A. We restrict our study to binary classiﬁcation.
1. We refer to oracle as a policy selector which selects the policy with the lowest test loss.
4

Towards Further Automation in AutoML
Time limit (s):
60
600
3600
No limit
mean
std
mean
std
mean
std
mean
std
CV
0.2052
0.0021
0.1178
0.0012
0.0799
0.0020
0.0797
0.0021
Holdout
0.0874
0.0009
0.0841
0.0019
0.0829
0.0020
0.0829
0.0020
SH
0.0916
0.0008
0.0865
0.0028
0.0860
0.0017
0.0859
0.0018
Independent+Selector
0.1017
0.0046
0.0951
0.0041
0.0852
0.0022
0.0852
0.0022
Independent+Oracle
0.0640
0.0013
0.0593
0.0010
0.0598
0.0013
0.0597
0.0012
Complementary+Selector
0.0799
0.0012
0.0728
0.0020
0.0710
0.0023
0.0712
0.0020
Complementary+Oracle
0.0593
0.0009
0.0549
0.0009
0.0542
0.0011
0.0548
0.0018
Table 1: Results of executing the diﬀerent policies for diﬀerent time horizons. For each time
horizon we report the average normalized regret and standard deviation. Oracle
models are printed with a grey background.
We print the best strategy using
multiple policies in bold, and the best strategy using a single policy in bold italic.
Abbreviations are CV: cross-validation, SH: successive halving.
We use a total of 437 binary datasets from OpenML.org (Vanschoren et al., 2014). For
each dataset we optimized the hyperparameters of our pipeline with SMAC (Hutter et al.,
2011), with both a holdout strategy with a 33%-validation set, and a 5-fold cross-validation,
which resulted in 871 diﬀerent ML pipelines. As target metric we used balanced error rate
as deﬁned by Guyon et al. (2015). We limited each call to the ML pipeline to 10 minutes
and restricted the available memory to 6000MB. For each ML pipeline (and learning curve
step) we stored the time taken so far, the validation and the test loss.
We then ran the cross-product of all 871 ML pipelines and 437 datasets with both
holdout and 5-fold cross-validation to obtain four performance matrices.2 Having access to
these, we were able to run all experiments as table lookups. To obtain learning curve data for
the holdout strategy, we saved the predictions for XGBoost following a geometric schedule
(2, 4, 8, 16, . . . 512 iterations). For the successive halving strategy (see Appendix B) we had
to set values for the following hyper-hyperparameters: minimum budget, maximum budget
and discard ratio η. We used 16, 256, and 2.
All experiments were conducted with 10-fold cross-validation.
We used 90% of the
datasets as training data for our AutoML algorithms and tested them on the remaining
10% of the datasets. We repeated this procedure ten times to account for randomness due
to the shuﬄing and report both the mean and standard deviation over these repetitions.
Furthermore, we ran our procedure with four alternative (simulated) total time budgets:
60s, 600s, 3600s seconds, and no limit.
5.2. Results
We present the results of our experiments in Table 1. The ﬁrst three rows (CV, Holdout and
SH) are results for using a single policy for all datasets and the last four rows (Independent
and Complementary) are results for using a selector or oracle to choose between diﬀerent
policies on a per-dataset basis. Strategies using an oracle are printed with a grey back-
ground. We print the best strategy using multiple policies in bold, and the best strategy
using a single policy in bold italic.
2. For both resampling strategies we measure the validation and test loss.
5

Towards Further Automation in AutoML
We report the normalized average regret of evaluating a portfolio of size 16 until the
speciﬁed total time budget is reached. The regret of each dataset is scaled between zero
and one. As a proxy for the lowest achievable loss for a dataset, we use the lowest observed
loss (of all evaluation strategies). Reaching a regret of zero is only possible when selecting
the correct evaluation strategy and the ML pipeline which has the lowest test loss; this is
most likely not possible in practice because we choose the ﬁnal ML pipeline to apply to the
test set based on the validation set.
Best single policy.
(Top three rows.) The best single policy is diﬀerent for diﬀerent time
horizons. For 60s and 600s, it is on average best to use the holdout strategy, while for 3600s
and no limit cross-validation results in a lower loss. Cross-validation does not perform well
for small time horizons because there is not enough time to run it on the larger datasets.
Successive Halving can make use of the iterative nature of XGBoost and therefore perform
well on large datasets, but it is not the best evaluation strategy for small datasets and
therefore only has a mediocre performance on average across datasets.
Multiple policies with a selector.
(Rows four and six.) When using a selector, we
have a clear winner: Complementary policies. They clearly outperform selecting a policy
from independently constructed policies.
Using multiple policies.
Using multiple policies with a selector can outperform a single
policy if the policies to choose from are constructed correctly. Simply ﬁtting a selector to
independently constructed policies does not outperform the single best independent policy
for any of the time budgets. (We note, though, that the performance with a selector is
not much worse than the single best policy, so this can still be useful since it frees us
from choosing the best policy.) The complementary policies perform much better than the
independent ones, yielding the best performance for all four time budgets; we can therefore
conclude that using multiple policies is clearly better than using only a single policy for all
datasets.
Complementary policies.
(Rows ﬁve and seven) To answer the question whether we
can build complementary policies we need to remove the selector as a potential source of
error. By comparing the oracle performance of the independent policies with the oracle
performance of complementary policies, we ﬁnd that the complementary policies always
result in a lower regret. Furthermore, the correct selection appears to be easier for the
complementary policies than for independently constructed policies, as using a selector on
complementary policies results in an improvement over the single best policy, while using a
selector on independently constructed policies hurts performance.
6. Conclusion
We presented a novel methodology for automatically constructing AutoML strategies that
are optimized to perform well for a given set of representative datasets, a speciﬁc budget,
and with a speciﬁc set of base level classiﬁers. Our method also supports selecting between
specialized AutoML strategies on a per-dataset basis using simple dataset metafeatures.
Through extensive experiments on 437 datasets we have shown that our solution not only
matches, but outperforms the single best AutoML system from our set.
6

Towards Further Automation in AutoML
Although our approach shows promising results we are keen on studying the eﬀect of
jointly learning all portfolios and the selector to see if we can reduce the rather high average
regret of the oracle selector, and also whether we can improve the selector to be closer to
the oracle selector performance.
With an improved optimization and selection strategy
we also expect that it will be straightforward to optimize additional evaluation strategy
hyper-hyperparameters, such as the number of cross-validation folds and the choice among
more ML models.
Finally, as the method highly depends on having a large amount of
representative meta training datasets, we want to study the eﬀect of the training set size
and whether we can learn the optimal number of complementary policies for diﬀerent meta
training datasets D.
Acknowledgements
The authors acknowledge support by the state of Baden-W¨urttemberg through bwHPC, the
German Research Foundation (DFG) through grant no. INST 39/963-1 FUGG and Emmy
Noether grant HU 1900/2-1, the European Research Council (ERC) under the European
Union’s Horizon 2020 research and innovation programme under grant no. 716721.
References
J. Bergstra and Y. Bengio. Random search for hyper-parameter optimization. Journal of
Machine Learning Research, 13:281–305, 2012.
B. Bonet and S. Koenig, editors. Proceedings of the Twenty-nineth National Conference on
Artiﬁcial Intelligence (AAAI’15), 2015. AAAI Press.
T. Chen and C. Guestrin. XGBoost: A scalable tree boosting system. In B. Krishnapuram,
M. Shah, A. Smola, C. Aggarwal, D. Shen, and R. Rastogi, editors, Proceedings of the
22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining
(KDD), pages 785–794. ACM Press, 2016.
M. Feurer, A. Klein, K. Eggensperger, J. T. Springenberg, M. Blum, and F. Hutter. Eﬃcient
and robust automated machine learning. In C. Cortes, N. Lawrence, D. Lee, M. Sugiyama,
and R. Garnett, editors, Proceedings of the 29th International Conference on Advances
in Neural Information Processing Systems (NIPS’15), pages 2962–2970, 2015a.
M. Feurer, T. Springenberg, and F. Hutter. Initializing Bayesian hyperparameter optimiza-
tion via meta-learning. In Bonet and Koenig (2015), pages 1128–1135.
I. Guyon, K. Bennett, G. Cawley, H. J. Escalante, S. Escalera, Tin Kam Ho, N. Maci`a,
B. Ray, M. Saeed, A. Statnikov, and E. Viegas. Design of the 2015 chalearn automl
challenge. In 2015 International Joint Conference on Neural Networks (IJCNN), pages
1–8, July 2015. doi: 10.1109/IJCNN.2015.7280767.
I. Guyon, I. Chaabane, H. J. Escalante, S. Escalera, D. Jajetic, J. R. Lloyd, N. Maci`a,
B. Ray, L. Romaszko, M. Sebag, A. Statnikov, S. Treguer, and E. Viegas. A brief review
of the chalearn automl challenge: Any-time any-dataset learning without human inter-
vention. In Frank Hutter, Lars Kotthoﬀ, and Joaquin Vanschoren, editors, Proceedings
7

Towards Further Automation in AutoML
of the Workshop on Automatic Machine Learning, volume 64 of Proceedings of Machine
Learning Research, pages 21–30, New York, New York, USA, 24 Jun 2016. PMLR.
F. Hutter, H. Hoos, and K. Leyton-Brown. Sequential model-based optimization for gen-
eral algorithm conﬁguration. In C. Coello, editor, Proceedings of the Fifth International
Conference on Learning and Intelligent Optimization (LION’11), volume 6683 of Lecture
Notes in Computer Science, pages 507–523. Springer-Verlag, 2011.
K. Jamieson and A. Talwalkar. Non-stochastic best arm identiﬁcation and hyperparameter
optimization. In Proceedings of the Seventeenth International Conference on Artiﬁcial
Intelligence and Statistics (AISTATS), 2016.
Ron Kohavi. A study of cross-validation and bootstrap for accuracy estimation and model
selection. In Proceedings of the 14th International Joint Conference on Artiﬁcial Intelli-
gence - Volume 2, IJCAI’95, pages 1137–1143, 1995.
B. Komer, J. Bergstra, and C. Eliasmith. Hyperopt-sklearn: Automatic hyperparameter
conﬁguration for scikit-learn.
In F. Hutter, R. Caruana, R. Bardenet, M. Bilenko I.
Guyon, B. K´egl, , and H. Larochelle, editors, ICML workshop on Automated Machine
Learning (AutoML workshop 2014), 2014.
A. Krause and D. Golovin. Submodular function maximization. In L. Bordeaux, Y. Hamadi,
and P. Kohli, editors, Tractability: Practical Approaches to Hard Problems, pages 71–104.
Cambridge University Press, 2014.
M. Lindauer, H. Hoos, F. Hutter, and T. Schaub. Autofolio: An automatically conﬁgured
algorithm selector. Journal of Artiﬁcial Intelligence Research, 53:745–778, August 2015.
Randal S. Olson, Ryan J. Urbanowicz, Peter C. Andrews, Nicole A. Lavender, La Creis
Kidd, and Jason H. Moore. Applications of Evolutionary Computation: 19th European
Conference, EvoApplications 2016, Porto, Portugal, March 30 – April 1, 2016, Proceed-
ings, Part I, chapter Automating Biomedical Data Science Through Tree-Based Pipeline
Optimization, pages 123–137. Springer International Publishing, 2016.
F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blon-
del, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau,
M. Brucher, M. Perrot, and E. Duchesnay. Scikit-learn: Machine learning in Python.
Journal of Machine Learning Research, 12:2825–2830, 2011.
Johann Petrak. Fast Subsampling Performance Estimates for Classiﬁcation Algorithm Se-
lection. In Proceedings of the ECML-00 Workshop on Meta-Learning: Building Automatic
Advice Strategies for Model Selection and Method Combination, pages 3–14, 2000.
J. Seipp, S. Sievers, M. Helmert, and F. Hutter. Automatic conﬁguration of sequential
planning portfolios. In Bonet and Koenig (2015).
B. Shahriari, K. Swersky, Z. Wang, R. Adams, and N. de Freitas. Taking the human out of
the loop: A review of Bayesian optimization. Proceedings of the IEEE, 104(1):148–175,
2016.
8

Towards Further Automation in AutoML
J. Snoek, H. Larochelle, and R. P. Adams. Practical Bayesian optimization of machine
learning algorithms.
In P. Bartlett, F. Pereira, C. Burges, L. Bottou, and K. Wein-
berger, editors, Proceedings of the 26th International Conference on Advances in Neural
Information Processing Systems (NIPS’12), pages 2960–2968, 2012.
J. Snoek, K. Swersky, R. Zemel, and R. Adams. Input warping for Bayesian optimization
of non-stationary functions. In E. Xing and T. Jebara, editors, Proceedings of the 31th
International Conference on Machine Learning, (ICML’14), pages 1674–1682. Omnipress,
2014.
J. Snoek, O. Rippel, K. Swersky, R. Kiros, N. Satish, N. Sundaram, M. Patwary, Prabhat,
and R. Adams. Scalable Bayesian optimization using deep neural networks. In F. Bach
and D. Blei, editors, Proceedings of the 32nd International Conference on Machine Learn-
ing (ICML’15), volume 37, pages 2171–2180. Omnipress, 2015.
C. Thornton, F. Hutter, H. Hoos, and K. Leyton-Brown. Auto-WEKA: combined selection
and hyperparameter optimization of classiﬁcation algorithms. In I. Dhillon, Y. Koren,
R. Ghani, T. Senator, P. Bradley, R. Parekh, J. He, R. Grossman, and R. Uthurusamy,
editors, The 19th ACM SIGKDD International Conference on Knowledge Discovery and
Data Mining (KDD’13), pages 847–855. ACM Press, 2013.
J. Vanschoren, J. van Rijn, B. Bischl, and L. Torgo. OpenML: Networked science in machine
learning. SIGKDD Explorations, 15(2):49–60, 2014.
R. Vinayak and R. Gilad-Bachrach. DART: Dropouts meet Multiple Additive Regression
Trees. In Guy Lebanon and S. V. N. Vishwanathan, editors, Proceedings of the Eighteenth
International Conference on Artiﬁcial Intelligence and Statistics, volume 38 of Proceed-
ings of Machine Learning Research, pages 489–497, San Diego, California, USA, 09–12
May 2015. PMLR.
L. Xu, H. Hoos, and K. Leyton-Brown. Hydra: Automatically conﬁguring algorithms for
portfolio-based selection. In M. Fox and D. Poole, editors, Proceedings of the Twenty-
fourth National Conference on Artiﬁcial Intelligence (AAAI’10), pages 210–216. AAAI
Press, 2010.
L. Xu, F. Hutter, H. Hoos, and K. Leyton-Brown.
Hydra-MIP: Automated algorithm
conﬁguration and selection for mixed integer programming. In Proc. of RCRA workshop
at IJCAI, 2011.
S. Yadav and S. Shukla.
Analysis of k-fold cross-validation over hold-out validation on
colossal datasets for quality classiﬁcation. In 2016 IEEE 6th International Conference on
Advanced Computing (IACC), pages 78–83, Feb 2016.
9

Towards Further Automation in AutoML
Appendix A. Conﬁguration space
Name
Values
Default
Log
Max depth
[1, 10]
3,
Learning rate
[0.01, 1]
0.1
✓
Booster
GBTree, DART
GBTree
Subsample
[0.01, 1.0]
1.0,
Min child weight
[1e−10, 20]
1
Column subsample by tree
[0.1, 1.0]
1
Column subsample by level
[0.1, 1.0]
1
α
[1e −10, 1e −1]
1e −10
✓
λ
[1e −10, 1e −1]
1e −10
✓
Balancing
Oﬀ, On
Oﬀ
Sample type
{uniform, weighted}
uniform
Normalization type
tree, forest
tree
Dropout rate
[1e −10, 1 −1e −10]
0.5
Imputation strategy
{mean, median, most frequent}
mean
One Hot Encoding
{On, Oﬀ}
On
OHE use minimum fraction
{On, Oﬀ}
On
OHE mimimum fraction
[0.0001, 0.5]
0.01
✓
Rescaling strategy
{Standardize, None, MinMax,
Standardize
Normalize, Quantile Transformer,
Percentile MinMax}
# Quantiles
[10, 2000]
1000
Quantile distribution
{uniform, normal }
uniform
Lower percentile
[0.001, 0.3]
0.25
Upper percentile
[0.7, 0.999]
0.75
Table 2: Conﬁguration Space of Extreme Gradient Boosting (Chen and Guestrin, 2016).
Sample type, Normalization type and dropout rate are only active if the Dropout
Additive Regression Trees (DART)-Booster (Vinayak and Gilad-Bachrach, 2015)
is chosen.
10

Towards Further Automation in AutoML
Appendix B. Evaluation strategies
Holdout.
This is the default evaluation strategy in Auto-sklearn.
Conﬁgurations are
executed one after the other, and the conﬁguration with the lowest loss on the validation
set is chosen. For iterative algorithms, we keep track of the learning curve and use the
lowest validation loss to select a conﬁguration (and a time step) in the end. If we enforce a
time limit, we take the learning curve up to that limit into account.
Cross-validation (CV).
We expect this to perform best for small datasets, because
it results in a more robust estimate of the validation loss and thus reduces overﬁtting.
However, it comes at the cost of increasing the time to evaluate individual ML pipelines by
roughly k times. Similar to holdout, cross-validation executes one conﬁguration after the
other and uses the validation loss to choose the best conﬁguration.
Successive Halving (SH).
This bandit strategy by Jamieson and Talwalkar (2016)
makes use of partial evaluations of a conﬁguration. Given a minimal and maximal budget
per conﬁguration, successive halving starts by running a ﬁxed number of conﬁgurations on
the smallest budget. Then, it iteratively selects the best half of conﬁgurations with best
validation loss, doubles their budget, and re-evaluates. This process is continued until only
one conﬁguration is left or the maximal budget is reached.3 We expect successive halving
to work best for large datasets, for which there is not enough time to train multiple conﬁg-
urations for the full budget, but for which running conﬁgurations on a small budget already
gives good indications of the validation performance.
In our implementation, successive halving uses the same holdout set as the holdout
strategy to compute the loss of an ML pipeline.
The diﬀerence to the regular holdout
strategy is that all ML pipelines are compared only on a very small budget, while the holdout
strategy compares the ML pipelines on the full budget. For increasing budgets, successive
halving only compares a decreasing number of ML pipelines to each other, thereby saving
a lot of time. We note that a the budget of successive halving could also be changed to be
the number of cross-validation folds, which would result in a strategy speeding up CV.
Diﬀerent holdout strategies could be ignored from an optimization point of view as often
done in the research of meta-learning and algorithm selection. For AutoML systems this is
highly relevant as we are not interested in the optimization performance (of some subpart)
of the system, but the generalization performance when applied to a dataset for a certain
amount of time.
Appendix C. Experimental details
For this paper we restrict ourselves to XGBoost (Chen and Guestrin, 2016), a versatile
and performent implementation of gradient boosting. Additionally, we use a preprocessing
pipeline consisting of 1) One-Hot-Encoding (OHE) of categorical features, 2) imputation of
missing values, 3) rescaling of features and 4) removal of constant features. While step 1)
and 2) could be handled by XGBoost, we follow the pipeline implementation of Auto-sklearn
which allows us to easily add further models later on. We optimize 13 hyperparameters for
3. In practice, the fraction of conﬁgurations to continue and by how much to increase the budget is governed
by a hyper-hyperparameter called the discard ratio η.
11

Towards Further Automation in AutoML
XGBoost and 9 hyperparameters for our preprocessing pipeline which we give in Table 2
in Appendix A. As gradient boosting requires to build a tree for each class in iteration if
there are more than two classes, we restrict ourselves to binary classiﬁcation.
We use a total of 437 binary datasets from OpenML.org (Vanschoren et al., 2014) with
at least 150 instances each. While our pipeline could handle sparse data, we found only 3
datasets fulﬁlling our criteria and therefore dropped them to not run into any downstream
issues. The meta-features we used as inputs are: number of missing values, majority class
size, dimensionality, majority class percentage, number of binary features, number of classes,
number of features, number of instances, number of instances with missing values, number of
numeric features, number of symbolic features, percentage of binary features, percentage of
instances with missing values, percentage of missing values, percentage of numeric features
and the percentage of symbolic features and are available on OpenML (Vanschoren et al.,
2014). We give rudimentary details on the datasets in Table 3.
Meta-Feature
Min
Max
# Features
2
61359
# Instances
151
1496391
Majority class percentage
53.06%
99.84 %
Table 3: Dataset statistics.
For each dataset we optimized the hyperparameters of our pipeline with SMAC (Hutter
et al., 2011), once with a holdout strategy with a 33%-validation set, and once with 5-fold
cross-validation, which resulted in 871 diﬀerent conﬁgurations for our ML pipeline.
As
target metric we used balanced accuracy as deﬁned by Guyon et al. (2015).
To obtain learning curve data for the holdout strategy, we saved the predictions for
XGBoost with a geometric schedule, i.e. after 2, 4, 8, 16, . . . 512 iterations. Moreover, we
used 10% of the training data for XGBoost’s early stopping. We limited each call to the
machine learning pipeline to 10 minutes and restricted the available memory to 6000MB. For
each conﬁguration (and learning curve step) we stored the time taken so far, the validation
and the test loss. The test loss for the holdout strategy can simply be obtained by applying
the trained model to the test set. For cross-validation we apply all models to the test set
and use the average predictions (of the probabilities) to compute the loss.
After gathering 871 conﬁgurations, we ran the cross-product of all 871 conﬁgurations
and 437 datasets with both holdout and 5-fold cross-validation to obtain four performance
matrices (two evaluation strategies, each with the validation and test loss). Having access
to these, we were able to run all experiments as table lookups. For the successive halving
strategy we had to set values for the hyper-hyperparameters minimum budget, maximum
budget and discard ratio η, without any special intention we set these to 16, 256 and 2.
Initial optimization and computation of the performance matrices was done on a com-
pute cluster using CentOS 7.4.1708. Each node is equipped with two Intel(R) Xeon(R)
CPU E5-2630 v4 @ 2.20GHz (20 cores) and 120 GB of RAM.
All experiments were conducted in a 10-fold cross-validation fashion. We used 90% of
the datasets as training data for our AutoML algorithms and tested them on the remaining
10% of the datasets. We repeated this procedure ten times to account for randomness due
12

Towards Further Automation in AutoML
to the shuﬄing and report both the mean and standard deviation over these repetitions. As
we are interested in the performance for diﬀerent time horizons, we ran our procedure with
(a simulated) total time budget of 60 (1m), 600 (10m) and 3600 (1h) seconds and without
any total time budget.
To maximize the performance of our policy selector, we also tune its hyperparameters
(minimal number of samples to create a new split, minimal number of samples required for
a leaf, number of features considered at each split). For of each pairwise classiﬁer inside
the selector we run 30 iterations of random search (Bergstra and Bengio, 2012) on the
cost-weighted out-of-bag score, using a total of 512 trees inside the random forest.
13

