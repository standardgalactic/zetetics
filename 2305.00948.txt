L A RG E L I NG U I ST I C M O D E L S : A NA LY Z I NG T H E O R E T I CA L
L I NG U I ST I C A B I L I T I E S O F L L M s
ga≈°per begu≈°a, maksymilian dƒÖbkowskia, ryan rhodesb
auniversity of california, berkeley, brutgers university
manuscript as of august 21, 2023
abstract
The performance of large language models (LLMs) has recently improved to the point
where the models can perform well on many language tasks. We show here that for the first time, the
models can also generate coherent and valid formal analyses of linguistic data and illustrate the vast
potential of large language models for analyses of their metalinguistic abilities. LLMs are primarily
trained on language data in the form of text; analyzing and evaluating their metalinguistic abilities
improves our understanding of their general capabilities and sheds new light on theoretical models
in linguistics. In this paper, we probe into GPT-4‚Äôs metalinguistic capabilities by focusing on three
subfields of formal linguistics: syntax, phonology, and semantics. We outline a research program for
metalinguistic analyses of large language models, propose experimental designs, provide general
guidelines, discuss limitations, and offer future directions for this line of research. This line of inquiry
also exemplifies behavioral interpretability of deep learning, where models‚Äô representations are accessed
by explicit prompting rather than internal representations.
keywords
large language model, linguistic theory, GPT-4, metacognition, syntax, phonology,
semantics, transformers
1
introduction
GPT-4 (OpenAI, 2023) may be the first large language model (LLM) to generate coherent
syntactic, phonological, and semantic analyses of linguistic data using some of the popular
theoretical approaches in modern linguistics (Chomsky, 2014; Heim and Kratzer, 1998; Prince
and Smolensky, 1993/2004).1 This paper illustrates a vast potential to study not only the
language competence of LLMs, but also their metalinguistic capacities. In this paper, we
explore GPT-4‚Äôs ability to provide linguistic analyses for syntactic, phonological, and semantic
data sets.
Large language models have seen enormous success in recent years in a variety of fields. The
underlying architecture of most LLMs is based on transformers (Vaswani et al., 2017)‚Äîdeep
neural networks for sequential data processing. While the exact details of GPT-4‚Äôs architecture
are not available, we know that the underlying model is a transformer trained to predict
the next token in a sequence (OpenAI, 2023). GPT-4‚Äôs predecessor, GPT-3 had 175 billion
parameters (Brown et al., 2020); the exact number of parameters in GPT-4 is unknown.
1 This paper is inspired by Begu≈°‚Äôs (2023) tweet posted on the day of GPT-4‚Äôs release (May 14) and the ensuing
discussion.
1

2 research program outline
2
There exists a large body of work testing the linguistic abilities of neural networks trained on
text, ranging from recurrent neural networks (RNNs) (Gulordava et al., 2018; Matusevych
and Culbertson, 2022), to long-short term memory networks (LSTMs) (Linzen, Dupoux, and
Goldberg, 2016) to‚Äîmost recently‚Äîtransformers (Wilcox et al., 2018; Yedetore et al., 2023).
LLMs have also ignited a debate on what their outputs mean for linguistic theory in general
(Katzir, 2023; Piantadosi, 2023). Most of these studies, however, only consider the models‚Äô
performance on behavioral tasks‚Äîthis is to say, they test the models‚Äô ability to use language
correctly. They do not examine the models‚Äô metalinguistic performance‚Äîwhich is the ability
to generate analyses of data within a given linguistic framework. LLMs have been shown to
perform tasks such as Part-of-Speech (POS) tagging (Blevins, Gonen, and Zettlemoyer, 2023),
but POS is only one aspect of metalinguistic ability and the performance in previous models
is not impressive. Behzad et al., 2023 composed the first corpus of metalinguistic questions
and answers (Q&A) and used it to fine-tune LLMs to answer basic metalinguistic questions.
Even with fine-tuning, the models perform below humans per human evaluators and the
complexity of the metalinguistic answers is not at the level of formal analysis. This confirms
our results that earlier models are unable to perform complex formal linguistic analyses. Until
recently, LLMs were unable to generate coherent formal linguistic analyses at all.
This paper shows that testing complex metalinguistic abilities of large language models is
now a possible line of inquiry and argues that the results of such tests can provide valuable
insights into LLMs‚Äô general metacognitive abilities. Our purpose is not to provide evidence
in favor of or against individual theoretical proposals in linguistics or even make claims
in favor of or against functional or generative approaches to language as such. Rather, we
simply recognize that theoretical linguistics constitutes one approach to analyzing natural
language, and since LLMs are trained primarily on language data, linguistic theory affords
an opportunity to explore the extent of LLMs‚Äô higher-order meta-cognitive abilities.
Our paper outlines a methodology for such studies, identifies pitfalls, and offers future direc-
tions in this line of inquiry. We present several preliminary case studies in three subdisciplines
of linguistics: syntax, phonology, and semantics. We evaluate GPT-4‚Äôs performance on each
task and compare its performance with that of previous versions (GPT-3.5 vs. GPT-4). We
demonstrate that GPT-4 is largely capable of (though not perfect at) providing coherent
analyses of simple problems in each of the three areas, as well as recognizing ambiguities,
correcting its own analytical errors, and insightfully commenting on the feasibility of multiple
solutions.
2
research program outline
We advocate for testing the ability of large language models to construct linguistic analyses.
Why is this line of work important? The majority of studies thus far perform behavioral tests of
LLMs. Behavioral tests include tasks like asking a model whether a sentence is (un)gram-
matical, or seeing if a model can correctly perform a syntactic operation such as agreement,
movement, or embedding (Haider, 2023). Behavioral tasks test language performance.

2 research program outline
3
Here, we outline a research program where large language models such as GPT-4 are tested
on higher-level metalinguistic abilities. The term metalinguistic has several interpretations (for a
detailed discussion, see Bialystok and Ryan, 1985). We use the term metalinguistic ability to
refer to the ability to analyze language itself and to generate formal, theoretical analyses of
linguistic phenomena‚Äîin other words, to refer to the work that linguists do. Metalinguistic
ability is cognitively more complex than language use (Tunmer and Herriman, 1984); it is
acquired later, and linguistic competence is its precondition. Applying a linguistic formalism
from the training data to the model‚Äôs own language ability in constructing an analysis is
a complex metacognitive task. We argue that theoretical linguistic formalism presents the
perfect testing ground for accessing the metalinguistic abilities of large language models.
Testing LLMs on more complex metacognitive tasks is a new research frontier which can
inform us about their general capabilities and provide a metric to compare these abilities
across versions and models. This line of research can be understood as behavioral interpretability
of deep learning, where the model‚Äôs performance is evaluated through explicit metacognitive
prompts rather than internal representations.
Many previous studies have attempted to test whether linguistic structures are learnable
from surface statistics (i. e. the relative poverty of a human learner‚Äôs input notwithstanding,
given a sufficient quantity of input data, can the target grammar be acquired from statistical
regularities alone?).2 Large language models acquire linguistic competence from the surface
statistics of their training data. Our goal is to understand whether this is a sufficient basis to
analyze language itself.
Prompting a network to analyze language structure in linguistic terms might shed light on
whether the model has, at some level, access to hierarchical linguistic structure that could
enable or inform behavioral outputs. In other words, if the model performs well on behavioral
tasks, are its outputs felicitous because of distributional knowledge, or are they informed by
linguistic structure? Recent research suggests that transformers represent language hierarchi-
cally in structures that resemble syntactic trees (Murty et al., 2022), but so far such claims
needed to be evaluated implicitly by looking into the transformers‚Äô internal representations.
The nature of the language models‚Äô internal representations presents one of the most con-
sequential questions in the current research on generative AI. We show that LLMs can now
be prompted to generate structure explicitly and hope that this line of inquiry may provide a
useful direction for answering this overarching question.
Human linguists have arrived at a range of analytical tools that comprise the science of
linguistics by reasoning about language structure from their own knowledge of their native
languages (that is, from their mental grammar). It remains to be seen whether large language
models can ever achieve a similar capacity to reason from their ‚Äúknowledge‚Äù about language
2 We recognize that the learning environment for LLMs and human learners are very different. LLM input is both
more impoverished‚Äîit consists only of text and lacks e. g. the sensory information embedded in phonology,
prosody, gesture, etc.‚Äîand richer, in the sense that LLMs are trained on orders of magnitude more examples than
human learners will ever experience. However, there is still a question of whether natural language grammars are
in principle learnable from statistics alone.

2 research program outline
4
to analyze their own grammar. In this paper, we present several case studies which suggest
that the GPT architecture is moving in that direction.
Further research will investigate whether large language models are capable of innovative
solutions that were not hypothesized by humans thus far. Deep neural networks have been
instrumental in shrinking the hypothesis spaces and offer new solutions to problems in
fields as diverse as protein design (Jumper et al., 2021), geometry (Davies et al., 2021), and
understanding unknown communication systems (Begu≈°, Leban, and Gero, 2023). It remains
to be seen whether any new insights can be gained by metalinguistic prompting of LLMs.
Finally, we note that this line of work can be useful for educational purposes. Evaluating the
performance of LLMs‚Äô metalinguistic abilities is an effective pedagogical task for both learning
about linguistic theory and for learning to evaluate the abilities of deep neural models.
2.1
General guidelines
Any metalinguistic ability can be tested with GPT-4. Here, we motivate several experimental
design choices we made in eliciting metalinguistic behavior from GPT-4 and GPT-3.5 and
discuss their limitations.
First, the vast majority of training data is likely in English. However, GPT-4 returns coherent
analyses of other languages as well. Testing the model‚Äôs performance on metalinguistic tasks
in non-English languages can be very informative. The training data for GPT in particular is
so expansive that it almost certainly includes linguistic analyses‚Äîmost likely, simple analyses
and problem sets that one would find in educational materials for introductory linguistics
courses, as well as corpora with labeled data. While we can use these examples as a starting
point, it will become important to vary the tasks and data sets used to prompt the LLM
to probe whether it has the capacity to generalize beyond the exemplars included in its
training. Crucially, we are interested in seeing whether the LLM has simply memorized a
linguistic analysis (or a particular problem or data set), or whether it can apply the learned
metalinguistic analyses to novel unobserved data.
The best test case for this purpose may be to test LLMs on artificial languages. Some work
has recently begun testing LLMs using artificial grammar learning (AGL) paradigms that
have been used in human research for decades (Matusevych and Culbertson, 2022; Yedetore
et al., 2023). These studies are structured like human behavioral AGL studies but they do
not test metalinguistic abilities. This is a crucial test case, because it avoids the problem
of memorization‚Äîwe can tailor data sets explicitly to be as unlike the exemplars in the
training data as possible.3 We show that phonology is especially suitable for testing LLM‚Äôs
metalinguistic abilities on artificial languages.
Reproducibility is a major concern for research with proprietary large language models.
Unfortunately, GPT is still a black box, and there is a lot we do not know about its architecture,
3 There are many other reasons to pursue this approach, including the possibility that LLMs have acquired human-
like cognitive biases alongside their distributional knowledge of human language. However, this is beyond the
scope of the current paper.

3 syntactic theory
5
training data, memory, or how it was optimized for human conversation. GPT‚Äôs outputs can
be noisy and unpredictable‚Äîsubsequent prompts, even with the same wording, can generate
substantially different responses. Given the opacity of the model, it will be difficult to make
inferences about its abilities and internal representational states. Data transparency will be
extremely important for reproducibility and cross-study comparison. Prompts and responses
need to be disclosed in full. Our own data is publicly accessible and available for download,
comparison, and reanalysis in Begu≈°, DƒÖbkowski, and Rhodes (2023).
Finally, it can also be informative to make comparisons between different models and different
versions of the same model, especially in light of the recent claims that emergent abilities in
the large language models are a mirage (Schaeffer, Miranda, and Koyejo, 2023). In this paper,
we make some comparisons between GPT-4 and GPT-3.5. While it is difficult to definitely
show that metalinguistic abilities abruptly start with GPT-4, we do detect a giant leap in
performance between GPT-4 and GPT-3.5‚Äîthe older model performs considerably worse on
the same prompts. Again, transparency and prompt consistency are key for making these
kinds of comparisons.
3
syntactic theory
Syntax is a subfield of linguistics which studies the structure of sentences and the rules and
constraints by which sentences are derived. While there are many competing theoretical
frameworks in syntax (e. g. construction grammar: Fillmore, Kay, and O‚Äôconnor, 1988; categorial
grammar: Ajdukiewicz, 1935; relational grammar: Perlmutter, 1980), the most commonly adopted
set of approaches has evolved from the minimalist program (Chomsky, 2014), which in turn
traces its intellectual lineage back to X-bar theory (Chomsky, Jacobs, and Rosenbaum, 1970) and
government and biding (Chomsky, 1993). In testing GPT‚Äôs syntactic abilities, we used the X-bar
theory because its formalism is relatively well-established, prominent in the literature (hence
likely present in the training data set), and straightforward to both prompt and evaluate.
We tested GPT on a set of syntactic problems involving syntactic ambiguity and center
embedding in English and wh-movement in German. We find that GPT-4 is capable of
providing mostly correct analyses of relatively simple as well as more complex structures.
GPT-3.5 fares considerably worse on the same tasks.
3.1
Ambiguity and center-embedding
To evaluate GPT-4‚Äôs ability to perform syntactic analyses couched in X-bar theory, we prompted
the model to draw syntactic tree diagrams for a number of sentences using LATEX‚Äôs forest
package (≈Ωivanoviƒá, 2017). Other packages such as tikz (Tantau, 2007) or tikz-tree can also
be used. In Prompt 1, we asked GPT-4 to draw a syntactic analysis of the famously ambiguous
sentence ‚ÄúI saw an elephant with binoculars.‚Äù Our intention was to test if GPT-4 can identify
the two possible structures which correspond to the two different interpretations of the same
surface form.

3 syntactic theory
6
Prompt 1
Draw a theoretical syntactic analysis of the sentence "I saw an elephant with binoc-
ulars" in LaTeX using the forest package. Use the X-bar theory. Draw all possible structures of
this sentence.
Broadly speaking, the structures generated by GPT-4 successfully capture the difference
between the two readings of the sentence (Reply 1).4 In the first structure, the prepositional
phrase ‚Äúwith binoculars‚Äù attaches to the verb phrase. This corresponds to the reading where
seeing the elephant is done with binoculars, i. e. the speaker is wearing the binoculars. In the
second structure, the PP ‚Äúwith binoculars‚Äù attaches to the noun ‚Äúelephant.‚Äù This corresponds
to the reading where the elephant is wearing the binoculars. GPT-4‚Äôs explanation correctly
tracks the relationship between syntactic structure and meaning.
GPT-4 consistently places tensed verbs under T (or I) and leaves the V-head position empty.
This seems to reflect V-to-T raising, a type of head movement that has been argued for
languages like French. This is a mistake since the [+past] tense feature in English is commonly
analyzed as lowering to V (as opposed to the verb raising to T), but this is a more fine-grain
mistake and is not fatal.
XP
Specifier
X‚Äô
X‚Äô
X
Complement
Adjunct
Figure 1: X-bar schema.
Finally, while GPT-4 correctly identified the general relationship be-
tween syntactic structure and meaning, the response skips many bar
levels and does so inconsistently. According to the standard X-bar
schema (Figure 1), each phrasal category has a three-level structure:
X, X‚Äô, and XP (Jackendoff, 1977). With this elaborate structure, X-
bar trees may get very unwieldy. Consequently, skipping vacuous
levels of structure as shorthand is common in X-bar theory, but it
is worth noting that GPT-4 is inconsistent. For example, adjuncts
are supposed to combine with an X‚Äô-level node without going up a
level (i. e. adjunction yields another X‚Äô-level node). GPT-4 deviates
from this structure in two different ways. In the tree on the left, the
PP adjoins to V‚Äô (correct), but the next node is labeled as VP (as opposed to V‚Äô). In the tree
on the right, the PP adjoins to N (not N‚Äô) and goes up two bar levels, yielding an NP (as
opposed to staying at the N‚Äô level).
Reply 1
As a text-based AI, I am unable to directly draw diagrams. However, I can provide you
with the LaTeX code using the forest package to create the syntactic trees based on the X-bar
theory. You can copy and paste the code into your LaTeX editor to visualize the tree structures.
Below are two possible structures for the sentence "I saw an elephant with binoculars."
The first structure assumes that the "with binoculars" phrase is modifying the verb "saw," indi-
cating that the person saw the elephant using binoculars.
The second structure assumes that the "with binoculars" phrase is modifying the noun "ele-
phant," indicating that the person saw an elephant that had binoculars.
4 Some of GPT‚Äôs replies have been formatted and edited for conciseness. For complete transcripts of full exchanges,
see Begu≈°, DƒÖbkowski, and Rhodes (2023).

3 syntactic theory
7
TP
DP
I
T'
T
saw
VP
V'
V
DP
D
an
NP
N
elephant
PP
P
with
NP
N
binoculars
TP
DP
I
T'
T
saw
VP
V'
V
DP
D
an
NP
N
elephant
PP
P
with
NP
N
binoculars
As a comparison, GPT-3.5 performs considerably worse on the same prompt (Reply 2). It
generates an analysis for only one of the two possible interpretations (left-hand tree). The
tense feature appears in C, and the modifying PP branches directly underneath ‚Äúelephant‚Äù
(which should be a terminal node).

3 syntactic theory
8
Reply 2
CP
C[+past]
TP
NP
I
T'
T
VP
V
saw
NP
Det
an
N'
elephant
PP
P
with
NP
binoculars
CP
C[+past]
TP
NP
I
T'
T
VP
V
saw
NP
Det
an
N'
elephant with binoculars
N
elephant
PP
with binoculars
When asked to diagram the second structure (right-hand tree in Reply 2), it generates an even
less plausible tree. Again, the tense feature is in C, and again the PP is inappropriately placed
under a terminal node. But here, the entire phrase ‚Äúelephant with binoculars‚Äù is placed under
N‚Äô as a single node, with N and PP nodes consisting of a repetition of ‚Äúelephant‚Äù and ‚Äúwith
binoculars‚Äù underneath it.
The explanation produced by GPT-3.5 is also substantially worse (Reply 3). The first paragraph
describes the structure of the first tree, but does not discuss the relationship between meaning
and syntactic structure. The second paragraph alludes to the possibility of providing different
analyses, but fails to relate them to semantic ambiguity. While GPT-3.5 observes that the PP
node may be ‚Äúattached to the VP node‚Äù or be ‚Äúmodifier with N‚Äô node,‚Äù it points to individual
linguists‚Äô analytical preferences and fails to explain the differences in meaning.
Reply 3
This tree structure represents one possible interpretation of the sentence. The CP
(complementizer phrase) node contains the C (complementizer) node with the feature [+past],
indicating that the sentence is in the past tense. The TP (tense phrase) node contains the subject
NP (noun phrase) "I" and the T' (tense phrase) node, which contains the T (tense) node and the

3 syntactic theory
9
VP (verb phrase) node. The VP node contains the verb "saw" and the object NP, which consists
of the determiner "an" and the N' (noun phrase) node "elephant", which in turn contains the PP
(prepositional phrase) node "with binoculars".
Note that there are other possible structures for this sentence, depending on the analysis cho-
sen. For example, some linguists might treat "with binoculars" as a separate sentence con-
stituent and place it in a separate PP node attached to the VP node, while others might analyze
it as a modifier within the N' node. The X-bar theory allows for flexibility in representing sen-
tence structure, and different analyses can capture different aspects of the sentence's meaning
and syntax.
It is crucially important to test large language models on constructions that are unlikely to
have been part of their training data. The elephant sentence is a very common example that
appears in many introductory linguistics texts and problem sets. Therefore, it is likely that
this sentence, in particular, appeared (at some level of frequency) in GPT-4‚Äôs training data.
For this reason, in Prompt 4, we asked GPT-4 to analyze a similarly ambiguous sentence
(adapted from a joke in the Cyanide & Happiness comic; McElfatrick, 2008).
Prompt 4
Do a theoretical syntactic analysis of the sentence "I fed my baby turtles" in LaTeX
using the forest package. Use the X-bar theory. Draw all possible structures of this sentence.
GPT-4 performs well in this task, generating two tree structures from the initial prompt
(Reply 4). Each tree corresponds to an interpretation of the sentence, which GPT accurately
describes. The trees (particularly the second tree) are not perfect‚ÄîGPT gets some details of
the X-bar analysis wrong.5 But importantly, it largely gets the constituent structure right in a
way that neatly delineates the two possible interpretations.6
Reply 4
The sentence "I fed my baby turtles" can have two possible interpretations, depend-
ing on whether "baby" is modifying "turtles" or not. If "baby" is modifying "turtles," the sentence
means "I fed the baby turtles that I have." If "baby" is not modifying "turtles," the sentence
means "I fed my baby some turtles (to eat)." Here is the LaTeX code for the two possible struc-
tures using the X-bar theory and the forest package:
5 GPT is still placing the verb in T, as though through V-to-T raising, and although ‚Äúturtles‚Äù adjoins at an approxi-
mately correct level to modify the VP, it for some reason projects its own VP.
6 Furthermore, GPT-4 correctly identifies and analyzes ambiguity present in variations on the same this basic
structure (e. g. sentences with other object nouns such as ‚Äúmice‚Äù), as well as other ambiguous sentences like
‚ÄúJoan baked cookies for the baby in the oven.‚Äù The model‚Äôs replies are not reported for brevity.

3 syntactic theory
10
TP
DP
D
I
T'
T
fed
VP
V'
V
DP
D
my
NP
AdjP
Adj
baby
N'
N
turtles
TP
DP
D
I
T'
T
fed
VP
V'
V
DP
D
my
NP
N
baby
VP
V'
V
DP
N
turtles
Finally, in Prompt 5, we tested GPT-4‚Äôs performance on a center-embedding sentence (example
adopted from Katzir, 2023). Center embedding refers to a specific type of structure where
a phrase is nested in the middle of another phrase of the same type. Center-embedding
sentences are difficult to process, resulting in an impression of ill-formedness. Thus, although
grammatical, center-embedding sentences are virtually unattested in natural discourse. This
means that LLMs have almost certainly not been exposed to center-embedding structures at a
large scale. As such, they are a useful test for the models‚Äô generative syntactic ability.
Prompt 5
Draw a theoretical syntactic analysis of the sentence "The mouse that the cat that
the dog painted taught sang" in LaTeX using the forest package. Use X-bar theory. Draw all
possible structures of this sentence.
GPT-4 performs remarkably well on this task (Reply 5). The CP ‚Äúthat the dog painted‚Äù
modifies ‚Äúcat‚Äù and the CP ‚Äúthat the cat that the dog painted taught‚Äù modifies ‚Äúmouse.‚Äù This
is to say, the model‚Äôs analysis is correct. Reply 5 furthermore improves on the analyses above
in some minor respects. For example, it does not show incorrect V-to-T (or I) raising.

3 syntactic theory
11
Reply 5
CP
IP
NP
Det
The
N'
N
mouse
CP
C'
C
that
IP
NP
Det
the
N'
N
cat
CP
C'
C
that
IP
NP
Det
the
N'
N
dog
I'
VP
V'
V
painted
I'
VP
V'
V
taught
NP
t
I'
VP
V'
V
sang

3 syntactic theory
12
3.2
Movement in German
In Prompt 6, we asked GPT-4 to analyze a German sentence with multiclausal wh-movement.
As analyzed in X-bar theory, this construction involves successive cyclic movement, with traces
of the wh-element at each phase edge (in minimalism, these are analyzed as unpronounced
copies; the two analyses are functionally equivalent).
GPT-4 consistently gets some details wrong. For example, although wh-elements are typically
argued to occupy Spec,CP, the model represents them as having head-moved to C. (Under the
standard assumptions, the C-head is the landing site of the German tensed verb.) With the
initial Prompt 6a, GPT gets the hierarchical structure mostly right (Reply 6a). However, the
model fails to produce a branch ordering that reflects the surface word order (e. g. the branch
order of the daughters of I‚Äô and V‚Äô does not reflect verb-final word order). Furthermore, there
is no reference to either traces or copies of the wh-element.
When specifically prompted to account for the correct surface word order (Prompt 6b), GPT-4
revises its tree. It correctly orders the branches under I‚Äô in the embedded clause, but not in the
matrix clause (Reply 6b). When the model is prompted to specify the base-generated position
of the wh-element (Prompt 6c), it introduces a trace in the verb phrase of the embedded
clause (Reply 6c). Still, there is no explicit reference to cyclic movement (no intermediate
trace at Spec-CP of the embedded clause). This may reflect a bias in GPT-4‚Äôs training data
with more papers using older versions of X-bar theory, or pedagogical materials aimed at
introductory linguistics students (which presumably lack these theoretical details).
Prompt 6
a. Draw a theoretical syntactic analysis of the German sentence "Was hat Hans gesagt dass
er gestern gegessen hat?" in LaTeX using the forest package. Use the X-bar theory. Draw
all possible structures of this sentence.
b. Draw the same sentence, but make sure you account for the correct word order.
c. Where is "Was" base-generated?
In summary, GPT-4 is capable of providing analyses of syntactic ambiguity, center embed-
ding sentences, and complex structures which involve both A‚Äô-bar and head movement. The
details of the proposed structures are often erroneous or unconventional. Notably, GPT-4 far
outperforms GPT-3.5, which fails to generate well-formed syntactic trees altogether.

4 phonological theory
13
Reply 6
a.
CP
C'
C
Was
IP
NP
Hans
I'
I
hat
VP
V'
V
gesagt
CP
C'
C
dass
IP
NP
er
I'
I
hat
VP
AdvP
gestern
V'
V
gegessen
b.
CP
C'
C
Was
IP
NP
Hans
I'
I
hat
VP
V'
V
gesagt
CP
C'
C
dass
IP
NP
er
I'
VP
AdvP
gestern
V'
V
gegessen
I
hat
c.
CP
C'
C
Wasùëñ
IP
NP
Hans
I'
I
hat
VP
V'
V
gesagt
CP
C'
C
dass
IP
NP
er
I'
VP
AdvP
gestern
V'
V
gegessen
NP
tùëñ
I
hat
4
phonological theory
Phonology is a subfield of linguistics that studies the organization of sounds within a language.
In phonological theory, facts of sound distribution or sound alternations between sets of
morphologically related words can be modeled as rules transforming one sound into another
or as constraints whose relative ranking determines the ‚Äúbest‚Äù form of a word.
We tested GPT-4 on two phonological problems by prompting the model with small data sets:
a palatalization process in Korean and a spirantization process in an artificial language. These
were the type of questions that a linguistics student could encounter in a homework problem
set. GPT-4 performs well on tasks involving phonological theory and successfully analyses
familiar data sets as well as novel artificial languages.

4 phonological theory
14
4.1
Korean palatalization
In Prompt 7, we asked GPT-4 to solve a simple phonological neutralization problem in Korean.
In Korean, /s/ palatalizes before /i/ and is realized as [ É]. This is a simple problem that is often
given to introductory linguistics students. As such, it might have appeared in the training
data set. Nevertheless, as we are about to see, the model‚Äôs performance is not perfect.
Prompt 7
Consider the following (simplified) words from Korean and answer the questions
about the data below ‚Äì focus on the [s] and [ É] sounds.
korean
english
 Éi
poem
kasu
singer
sal
flesh
kas…ôl
hypothesis
miso
smile
 Éinmun
newspaper
ka Éi
thorn
pan Éik
method
o Éip
fifty
so
cow
Write out the environments for [s] and [ É] in a LaTeX table. Include the phonemes that appear
on either side of [s] and [ É] in the data set.
Write a rule describing the transformation that‚Äôs occurring in Korean. Example: A ‚ÜíB / C _ D
In its Reply 7, the model correctly recognizes that [s] becomes [ É] before [i]. It incorrectly
restricts the application of the rule to the word-initial position.
Reply 7
Here is a LaTeX table showing the environments for [s] and [ É] in the given Korean
words:
Word
Phoneme
Preceding Phoneme
Following Phoneme
[ Éi]
[ É]
# (word boundary)
[i]
[kasu]
[s]
[a]
[u]
[sal]
[s]
[a]
[l]
[kas…ôl]
[s]
[a]
[…ô]
[miso]
[s]
[i]
[o]
[ Éinmun]
[ É]
# (word boundary)
[i]
[ka Éi]
[ É]
[a]
[i]
[pan Éik]
[ É]
[n]
[i]
[o Éip]
[ É]
[o]
[i]
[so]
[s]
# (word boundary)
[o]
Based on the table above, we can make the following generalization about the distribution of
[s] and [ É] in Korean:
[s] ‚Üí[ É] / # _ [i]
This rule states that the [s] sound changes to [ É] when it appears at the beginning of a word
(word boundary, represented by "#") and is followed by [i].
When asked to change its answer (Prompt 8), the model incorrectly identifies the environment
of palatalization as the intervocalic V _ V (Reply 8). The new rule constitutes a decrease in
performance since it predicts even fewer correct outputs.

4 phonological theory
15
Prompt 8
There is a mistake in the rule you proposed. Identify it and state the correct rule.
Reply 8
Apologies for the previous confusion. You're right. Upon revisiting the data, we see
that the [s] changes to [ É] not just at the beginning of a word followed by [i], but also within a
word, regardless of the following vowel.
The instances of [ É] in [ Éi] ('poem'), [ Éinmun] ('newspaper'), [ka Éi] ('thorn'), [pan Éik] ('method'), and
[o Éip] ('fifty') do not strictly follow the earlier stated rule, as the [ É] sound isn't always at the
beginning of the word and isn't always followed by [i].
So a more accurate rule would be:
[s] ‚Üí[ É] / V _ V
This rule suggests that the [s] sound changes to [ É] when it appears between two vowels in a
word.
When asked to correct the rule again (Prompt 9), GPT-4 recognizes that palatalization is
sensitive only to the following [i]. The model‚Äôs correct answer is notable since the original
Prompt 7 specified the rule format as A ‚ÜíB / C _ D, strongly suggesting bilateral conditioning
of the phonological process. In Reply 9, GPT-4 deviates from the rule format and arrives at
the correct solution (without much hinting along the way), despite having been misled by
the original statement.
Prompt 9
This is still not correct. For example, consider the word [pan Éik]. Correct the rule
again.
Reply 9
Apologies for the confusion. You're correct, [pan Éik] does not follow the previous rule.
Looking again at the data, it appears that the [s] becomes [ É] when it's followed by [i], irrespective
of the preceding sound. This holds true for [ Éi] ('poem'), [ Éinmun] ('newspaper'), [ka Éi] ('thorn'),
[pan Éik] ('method'), and [o Éip] ('fifty').
Therefore, the corrected phonological rule in this context would be:
[s] ‚Üí[ É] / _ [i]
This rule states that the [s] sound changes to [ É] when it is followed by [i].
4.2
Spirantization in an artificial language
In Prompt 10, we provided GPT-4 with a small artificial language data set demonstrating a
phonological alternation between voiceless stops (p t k) and voiceless fricatives (…∏ Œ∏ x) in
morphologically related forms. We asked the model to identify one phonological process that
operates in the language and analyze it.

4 phonological theory
16
Prompt 10
Here are some singular nouns and their corresponding plural form in a language
you're encountering for the first time:
singular
plural
rap
ra…∏a
rat
raŒ∏a
vik
vixa
sik
sixa
lum
luma
zen
zena
jot
joŒ∏a
kit
kiŒ∏a
gut
guŒ∏a
kip
ki…∏a
zep
ze…∏a
vanup
vanu…∏a
roŒ∏ep
roŒ∏e…∏a
milik
milixa
hep
he…∏a
suril
surila
mirik
mirixa
riŒ∏it
riŒ∏iŒ∏a
delop
delo…∏a
ra√∞at
ra√∞aŒ∏a
vop
vo…∏a
The plural is formed by adding /-a/ to the singular form. Identify one phonological process that
operates in the language and analyze it.
GPT-4 first analyzed the alternation as assimilation to the following vowel with respect to the
place of articulation (Reply 10). Since the spirantization of a stop in the proximity of a vowel
can be seen as a type of assimilation (the increase in sonority makes the sound more similar
to the vowel), the answer is partly correct. However, the assimilated feature is the manner,
not the place, of articulation.7
Reply 10
Based on the provided data, one phonological process that seems to operate in
this language is assimilation. Assimilation is a process where a sound becomes more like a
neighboring sound in terms of some phonetic features. In this case, it appears that the final
consonant of the singular form assimilates to the /-a/ suffix in the plural form, with regard to
the place of articulation.
The assimilation process in this language is regressive, meaning that the features of the fol-
lowing sound (the /-a/ suffix) affect the preceding consonant, causing it to change its place of
articulation to match the following vowel. This results in the final consonant of the singular
form becoming a fricative or remaining a nasal in the plural form, depending on the original
consonant.
When pressed about the identity of the assimilated feature (Prompt 11), GPT-4 completely
changes its answer and correctly identifies the phonological process as ‚Äúa form of consonant
mutation called spirantization‚Äù (Reply 11). The model correctly illustrates the process with
examples from the data set and generalizes over the natural class of alternating segments
by observing that ‚Äú[t]he stop consonants at the end of the singular nouns become fricatives
when the plural suffix /-a/ is added.‚Äù
Prompt 11
If it's assimilation, what is the feature that the consonant assimilates with respect
to?
Reply 11
Apologies for the confusion in my previous response. Upon closer examination, the
phonological process that operates in this language is not assimilation. Instead, it appears to
7 Additionally, there are various notational issues which we do not focus on in evaluating the models‚Äô answers. For
example, GPT-4 uses slashes / / to represent both underlying and surface forms. (The latter are conventionally
given in brackets [
].) GPT-4 uses the right-pointing arrow (‚Üí) to represent (i) a morphological operation
followed by (ii) a phonological process, whereas conventionally the arrow is used to represent only the latter.

4 phonological theory
17
be a form of consonant mutation called spirantization. Spirantization is a process where a stop
consonant turns into a fricative or approximant in certain phonological environments.
In this language, spirantization occurs when a stop consonant (/p/, /t/, /k/) appears at the end
of a singular noun form and is followed by the plural suffix /-a/. The stop consonant turns into
a fricative with the same place of articulation as the original stop:
1. /p/ ‚Üí/…∏/: bilabial voiceless stop becomes bilabial voiceless fricative
‚Ä¢ rap ‚Üíra…∏a
‚Ä¢ vanup ‚Üívanu…∏a
‚Ä¢ roŒ∏ep ‚ÜíroŒ∏e…∏a
2. /t/ ‚Üí/Œ∏/: alveolar voiceless stop becomes dental voiceless fricative
‚Ä¢ rat ‚ÜíraŒ∏a
‚Ä¢ ra√∞at ‚Üíra√∞aŒ∏a
3. /k/ ‚Üí/x/: velar voiceless stop becomes velar voiceless fricative
‚Ä¢ vik ‚Üívixa
‚Ä¢ sik ‚Üísixa
In this case, the feature that changes during the spirantization process is the manner of articu-
lation. The stop consonants at the end of the singular nouns become fricatives when the plural
suffix /-a/ is added. The place of articulation remains the same for the affected consonants.
Next, we performed a wug test (Berko, 1958) to establish if GPT-4 can apply the generalizations
it identified to previously unencountered forms (Prompt 12). GPT-4 fully succeeds at applying
the spirantization rule to new forms and explaining its reasoning (Reply 12).8
Prompt 12
Given the singular nouns "zefop," "trewen," "nenek," and "gesel," how would you
pluralize them?
Reply 12
Based on the observed pattern of spirantization in the provided data, we can predict
the plural forms of the given singular nouns as follows:
‚Ä¢ zefop ‚Üízefo…∏a: The voiceless stop /p/ in the final position of the singular noun "zefop"
undergoes spirantization, turning into the bilabial voiceless fricative /…∏/ before the plural
suffix /-a/.
‚Ä¢ trewen ‚Üítrewena: In the singular noun "trewen," the final consonant is a nasal /n/ rather
than a voiceless stop. Since spirantization applies only to voiceless stops, we would simply
add the plural suffix /-a/ without any changes to the final consonant.
‚Ä¢ nenek ‚Üíneneœáa: The voiceless stop /k/ in the final position of the singular noun "nenek"
undergoes spirantization, turning into the velar voiceless fricative /x/ (represented here
as œá for clarity) before the plural suffix /-a/.
8 Curiously, the model represents the velar fricative with ‚Äúœá,‚Äù which typically stands for the uvular fricative, and
‚Äúmotivated‚Äù its unconventional notation.

4 phonological theory
18
‚Ä¢ gesel ‚Üígesela: In the singular noun "gesel," the final consonant is a liquid /l/ rather than
a voiceless stop. Since spirantization applies only to voiceless stops, we would simply add
the plural suffix /-a/ without any changes to the final consonant.
Then, we tested GPT-4‚Äôs ability to predict further alternations by reasoning about natural
classes of sounds (Prompt 13). To do so, we asked the model to generate plural forms of nouns
ending in voiced stops (b d g). Note that such words do not appear in the data set. Thus, the
task is more difficult than the standard wug test, since the model has to guess which natural
class spirantization actually applies to. This task is open-ended, in that more than one answer
is justifiable.
Prompt 13
Given the singular nouns "lob," "weg," "nemed," how would you pluralize them?
For /b/ in /lob/, the model suggested three possible outputs: […∏], [Œ≤], and [b] (Reply 13).
The first option ([…∏]) assumes that the output of spirantization for the voiced /b/ is the same
as for the voiceless /p/. This guess is not unreasonable given that only spirantization to […∏]
was observed in the data set and that voicing is dispreferred in fricatives which can trigger
devoicing.9 The second option ([Œ≤]) assumes that the spirantization applies to voiceless and
voiced stops alike and that voicing is preserved. Given phonetic plausibility and phonological
typology, this is arguably the most likely outcome. The third option ([b]) assumes that
spirantization applies only to voiceless stops. This again is a reasonable guess given no
counter-indications in the original data set. Overall, the model‚Äôs performance on the task is
impressively nuanced.10
Reply 13
lob ‚Üílo…∏a: The voiceless stop /b/ in the final position of the singular noun "lob" is
not one of the voiceless stops we've observed undergoing spirantization in the data provided
(/p/, /t/, /k/). However, since /b/ is the voiced counterpart of /p/, we could assume that it might
undergo a similar process, turning into the bilabial voiced fricative /Œ≤/ before the plural suffix
/-a/. Although this is a reasonable assumption based on the data provided, it is also possible
that "lob" simply takes the plural suffix without any change: loba.
When explicitly instructed to ‚Äúassume that voiced stops behave in a way analogous to voiceless
stops‚Äù (Prompt 14), GPT-4 flawlessly extends the spirantization rule to /b d g/, predicting
[Œ≤ √∞ …£] in the plural forms (Reply 14).
Prompt 14
Assume that voiced stops behave in a way analogous to voiceless stops. Given the
singular nouns "lob", "weg", "nemed", how would you pluralize them?
9 For further discussion, see Begu≈° (2018, 2022) and Begu≈° and DƒÖbkowski (2023).
10 Given the lack of voiced stop-final words in the data set, GPT-4 erred on the side of rule non-application for /d/
and /g/. These solutions are not reported in Reply 13 for brevity.

4 phonological theory
19
Reply 14
If we assume that voiced stops behave in a way analogous to voiceless stops, we
can predict the plural forms of the given singular nouns as follows:
1. lob ‚ÜíloŒ≤a: The voiced stop /b/ in the final position of the singular noun "lob" undergoes
a process analogous to spirantization, turning into the bilabial voiced fricative /Œ≤/ before
the plural suffix /-a/.
2. weg ‚Üíwe…£a: The voiced stop /g/ in the final position of the singular noun "weg" under-
goes a process analogous to spirantization, turning into the velar voiced fricative /…£/ be-
fore the plural suffix /-a/.
3. nemed ‚Üíneme√∞a: The voiced stop /d/ in the final position of the singular noun "nemed"
undergoes a process analogous to spirantization, turning into the dental voiced fricative
/√∞/ before the plural suffix /-a/.
Finally, we tested GPT-4‚Äôs ability to formalize its analysis. In Prompt 15, we asked the model
to capture the process of spirantization with a phonological rule (Chomsky and Halle, 1968).
We did not explicitly specify the rule format or instruct the model to refer to natural classes.
Prompt 15
Write one phonological rule which captures the process of spirantization in this
language, generalizing over all instances of spirantization seen so far.
In formalizing its generalization, GPT-4 chooses to employ binary features (Reply 15). The
model correctly identifies oral stops as [‚àísonorant, +stop]. In the proposed rule, the [‚àísono-
rant, +stop] segments are analyzed as acquiring [+continuant] feature. The intuition behind
this choice is correct. Nevertheless, the [¬±stop] feature is the polar opposite of [¬±continuant]
(i. e. [¬±stop] = [‚àìcontinuant]). Thus, GPT-4 should have used [‚àístop] instead of [+continu-
ant] (or [‚àícontinuant] instead of [+stop]) to be more notationally transparent. Finally, the
model states the environment of application as following a consonant and preceding /a/. The
former part of the statement is incorrect as all the instances of spirantization are postvocalic.
Reply 15
Voiceless stops /p/, /t/, /k/ and their voiced counterparts /b/, /d/, /g/ are realized
as corresponding fricatives /…∏/, /Œ∏/, /x/ and /Œ≤/, /√∞/, /…£/ respectively when they occur before a
vowel /a/ and after a consonant.
Using formal phonological rule notation, this could be written as:
[‚àísonorant, +stop] ‚Üí[+continuant] / [+consonant] _ [a]
When asked to revise the rule without any additional hints (Prompt 16), GPT-4 recognizes that
the preceding segment is irrelevant to spirantization, and instead observes that spirantization
takes place in ‚Äúthe final position of a word, before the plural morpheme /a/‚Äù (Reply 16). If
we interpret GPT-4 to mean morphosyntactic (rather than phonological) word, the answer is
technically correct, but redundant, since the suffix -a always attaches to the end of a word. In
addition, the model‚Äôs notation is unsatisfactory. The bracketed [#, a] suggests a (nonsensical)

4 phonological theory
20
segment with the features of a word boundary and [a]. The environment identified by GPT-4
would be better restated as _ +a.
Prompt 16
There is a mistake in the rule you provided. Identify it and write the correct rule.
Reply 16
Voiceless stops /p/, /t/, /k/ and their voiced counterparts /b/, /d/, /g/ are realized as
corresponding fricatives /…∏/, /Œ∏/, /x/ and /Œ≤/, /√∞/, /…£/ respectively when they occur in the final
position of a word, before the plural morpheme /a/.
Using formal phonological rule notation, this could be written as:
[‚àísonorant, +stop] ‚Üí[+continuant] / _ [#, a]
When asked to simplify the rule (Prompt 17), GPT-4 correctly changes the statement of
the environment (Reply 17). The model also proposes that since [+stop] already excludes
sonorants, the [‚àísonorant] specification is redundant. This reanalysis may be due to the
fact that the category of stops is used ambiguously in the literature to refer to either all stops
(including nasals) or oral stops only.
Prompt 17
The rule is not maximally simple. Simplify it further.
Reply 17
Based on the information given, the rule for spirantization doesn't need to specify
the [‚àísonorant] feature, as [+stop] already excludes sonorants. Moreover, the use of the word
boundary symbol (#) is unnecessary, as we have only seen the plural morpheme /a/ attached
at the end of words.
A simpler, yet accurate version of the rule can be:
[+stop] ‚Üí[+continuant] / _ [a]
Finally, in Prompt 18, we asked GPT-4 to provide an analysis of spirantization couched in
Optimality Theory (Prince and Smolensky, 1993/2004). GPT summarizes the main tenets of
OT, correctly identifies some reasonable constraints, describes the logic of its analysis, chooses
an example word, constructs a minimal tableau, describes its notation, and restates the central
proposal. The model‚Äôs remarkable Reply 18 is given below in full.
Prompt 18
Analyze the process of spirantization in this language in Optimality Theory.
Reply 18
In Optimality Theory (OT), phonological processes are understood as the outcome of
a competition between different constraints. Constraints are often universal but can be ranked
differently in different languages, leading to different phonological outcomes. There are two
main types of constraints: markedness constraints, which enforce certain phonological pat-
terns, and faithfulness constraints, which discourage changes to the underlying form.

4 phonological theory
21
With respect to the spirantization process in the language you're studying, let's identify possible
constraints and their potential interactions:
1. *Stop/a/: A markedness constraint that prohibits stop consonants before /a/. This con-
straint is violated by any form that has a stop consonant immediately preceding /a/.
2. Max-C: A faithfulness constraint that prohibits the deletion of consonants from the input
to the output. This constraint would be violated if a consonant in the input were deleted
in the output.
3. Ident-[cont]: A faithfulness constraint that requires the [continuant] feature of a segment
to be the same in the input and the output. This constraint would be violated if a stop
([‚àícontinuant]) in the input became a fricative ([+continuant]) in the output.
Given the patterns observed in the data, it appears that *Stop/a/ is ranked above Ident-[cont].
This means that it's more important to avoid having a stop consonant before /a/ than it is to
preserve the [continuant] feature of the consonant. This ranking results in spirantization: the
stop consonants /p/, /t/, /k/ (and potentially their voiced counterparts /b/, /d/, /g/) become the
fricatives /…∏/, /Œ∏/, /x/ (and potentially /Œ≤/, /√∞/, /…£/) before /a/.
In OT tableaux format, this could be represented for a singular noun like "rat" becoming "raŒ∏a":
Input:
Candidate (a):
Candidate (b):
/rata/
[rata]
[raŒ∏a]
*Stop/a!
*Stop/a
Ident-[cont]
Ident-[cont]!
The exclamation point (!) marks a fatal violation that causes the candidate to be ruled out. The
candidate without the fatal violation (in this case, [raŒ∏a]) is the one that matches the observed
data, and so it is the optimal output.
So, in sum, the spirantization process in this language can be explained in Optimality Theory
as the result of a high-ranking markedness constraint (*Stop/a/) that causes voiceless stops to
become fricatives before /a/, despite a lower-ranking faithfulness constraint (Ident-[cont]) that
would otherwise discourage this change.
In summary, GPT-4 performs surprisingly well on the spirantization task. Although the
problem was relatively simple, the language in the task was constructed. Consequently, we
can be certain that the model‚Äôs success was not due to memorization of the original data set.
The model correctly extends the spirantization rule to a class of sounds missing from the
data set. This demonstrates a considerable ability for abstraction and generalization in the
domain of phonological analysis. Finally, GPT-4 succeeds in applying different phonological
formalisms. While its initial responses are often mistaken, the model can self-correct with
minimal or no additional hints.
GPT-4 outclasses GPT-3.5. Given the initial Prompt 10, the latter model hallucinates a vowel
harmony process, completely failing the task (Reply 19). It is not the case that GPT-3.5
unexpectedly picks up on subtle distributional asymmetries in the data set. Rather, the
model‚Äôs answer is completely nonsensical. For example, it identifies the last vowel in [ra…∏a]
as back and in [vixa] as front despite the fact that they are the same vowel.

5 lambda calculus
22
Reply 19
One phonological process that may be operating in this language is vowel harmony,
where vowels in a word are influenced by the vowels in neighboring syllables. Based on the
plural forms provided, it seems that there may be a front-back vowel harmony system, where
the suffix /-a/ has a front or back variant depending on the vowel in the previous syllable.
For example, the singular noun "rap" has a back vowel /a/, and the plural form "ra…∏a" also has
a back vowel in the suffix. Similarly, the singular noun "vik" has a front vowel /i/, and the plural
form "vixa" also has a front vowel in the suffix.
We can see this pattern consistently across the plural forms provided, suggesting that vowel
harmony is a regular feature of the language. Further analysis would be needed to determine
the specific rules and constraints of the vowel harmony system, such as whether it is restricted
to certain syllables or environments, or whether certain vowels are dominant over others in
determining harmony.
5
lambda calculus
Formal semantics is a subfield of both linguistics and logic which studies how the meaning of
complex expressions is derived from the meaning of their constituent parts and the structure
of those expressions. In order to model linguistic meaning with precision, semanticists make
use of mathematical tools. One such tool is lambda calculus, which is a formal system for
expressing computation using function abstraction and application.
We tested GPT-4‚Äôs ability to construct lambda calculus analyses of English sentences, includ-
ing cases of scopal ambiguity. Overall, the model performs well on simpler sentences and
understands scopal ambiguity, but makes some substantive mistakes in using the lambda
calculus formalism.
In Prompt 20, we asked GPT-4 to provide an analysis of a simple quantified English sentence.
The model‚Äôs Reply 20 is largely correct, although there are notational and presentational
issues (to be discussed at the end of this section).
Prompt 20
Write the lambda calculus analysis of the sentence "Every plant is yellow" in LaTeX.
Reply 20
Every plant is yellow
‚â°(‚àÄùë•)((Plant(ùë•) ‚ÜíYellow(ùë•))
‚â°‚àÄùë•((ùúÜùë•.Plant(ùë•))(ùë•) ‚Üí(ùúÜùë•.Yellow(ùë•))(ùë•))
Next, we asked GPT-4 to analyze a sentence which involves scopal ambiguity (Prompt 21).
The two readings of ‚ÄúEvery student likes a classmate‚Äù are ‚ÄúFor every student, there is a
classmate that they like‚Äù (with universal quantification scoping over existential) and ‚ÄúThere
exists a classmate such that every student likes them‚Äù (with the opposite scope).

5 lambda calculus
23
Prompt 21
Write the lambda calculus analysis of the sentence "Every student likes a class-
mate" in LaTeX.
The answer provided by the GPT-4 presents an analysis of the former reading, with the univer-
sal quantifier scoping high (Reply 21). Nevertheless, the answer contains two mistakes. Most
importantly, the model existentially quantifies over ‚ÄúStudent‚Äù in the consequent. A correct
analysis would only have an existential quantification over the relational noun ‚ÄúClassmate.‚Äù
Additionally, if ‚ÄúLikes‚Äù and ‚ÄúClassmate‚Äù are interpreted as two-place functions, they must be
applied to their arguments one by one. (The model incorrectly applies the functions to two
arguments at the same time.)
Reply 21
Every student likes a classmate
‚â°(‚àÄùë•)(Student(ùë•) ‚Üí(‚àÉùë¶)(Student(ùë¶) ‚àßClassmate(ùë•, ùë¶) ‚àßLikes(ùë•, ùë¶)))
‚â°‚àÄùë•((ùúÜùë•.Student(ùë•))(ùë•) ‚Üí(‚àÉùë¶)((ùúÜùë¶.Student(ùë¶))(ùë¶) ‚àß
(ùúÜùë•.ùúÜùë¶.Classmate(ùë•, ùë¶))(ùë•, ùë¶) ‚àß(ùúÜùë•.ùúÜùë¶.Likes(ùë•, ùë¶))(ùë•, ùë¶)))
When explicitly asked about another meaning of the sentence (Prompt 22), GPT-4 recognizes
the existence of scopal ambiguity and provides an analysis of the other reading. Reply 22
doesn‚Äôt involve extraneous quantification but repeats the mistake of applying two-place
functions to both of their arguments at the same time.
Prompt 22
Is there another reading for this sentence?
Reply 22
Yes, there is another reading for the sentence "Every student likes a classmate." The
initial interpretation assumes that each student likes a possibly different classmate. However,
another reading could be that there exists a particular classmate whom every student likes.
Here's the lambda calculus analysis for this alternative interpretation:
Every student likes a classmate
‚â°(‚àÉùë¶)(Student(ùë¶) ‚àß(‚àÄùë•)(Student(ùë•) ‚Üí(Classmate(ùë•, ùë¶) ‚àßLikes(ùë•, ùë¶))))
‚â°‚àÉùë¶((ùúÜùë¶.Student(ùë¶))(ùë¶) ‚àß(‚àÄùë•)((ùúÜùë•.Student(ùë•))(ùë•) ‚Üí
ùúÜùë•.ùúÜùë¶.Classmate(ùë•, ùë¶))(ùë•, ùë¶) ‚àß(ùúÜùë•.ùúÜùë¶.Likes(ùë•, ùë¶))(ùë•, ùë¶)))
Finally, the style and notation used by the model leave a lot to be desired. First, there should
be no parentheses around the quantifiers; (‚àÄùë•) and (‚àÉùë¶) are not permitted by the syntax of
predicate logic. Second, the English sentences should be enclosed in double brackets J
K,
which represent the interpretation function. Lastly, equals signs (=) should be used instead
of the equivalence symbol (‚â°).
In summary, GPT-4 can provide semantic analyses of English sentences and recognize scopal
ambiguities. The model makes lambda calculus errors and uses unconventional notation.
These issues parallel the model‚Äôs problems with the presentation of syntactic trees and
phonological notation.

6 pitfalls and limitations
24
6
pitfalls and limitations
There are several potential pitfalls when analyzing the metalinguistic abilities of large language
models. In this section, we briefly discuss some of the most immediate challenges.
The first problem to consider is that of memorization. It is likely that many of the examples
we chose existed in the corpus that GPT was trained on. Simple problem sets that can be
found in linguistics textbooks and Wikipedia articles also have readily available solutions
printed online, which GPT has almost definitely seen. This is why it is crucial to also test
LLMs on data that they are very unlikely to have seen‚Äîeither by creating new versions of
these problems with similar structures, or by wholly inventing new languages and grammars.
It is also possible that the learning in LLMs is not fully unsupervised: it is possible that GPT-4
is trained on large amounts of syntactically labeled data. Due to OpenAI‚Äôs policy to not release
any details about training data, it will be difficult to evaluate confounding factors based on
training data. We can ask GPT-4 itself this question (Prompt 23), but of course its answer
(Reply 23) cannot be trusted.
Prompt 23
Were you trained on any labeled syntactic data?
Reply 23
As an AI language model based on the GPT-4 architecture, I was trained on a large
dataset of text from various sources. While the primary training data consists of raw, unlabeled
text, the model does benefit indirectly from labeled syntactic data present in the training cor-
pus.
Since the training data may include texts that contain syntactic annotations or examples, such
as educational materials or linguistic research papers, the model can develop an understanding
of syntactic structures and linguistic concepts by learning from these examples. (This may be
less of a problem in phonology, since phonetic or phonological annotations are much rarer
in the written corpus.) However, it is important to note that the GPT models are primarily
trained on natural language, not labeled syntactic data.
The output of large language models can also be highly variable and unpredictable. This is
particularly true for GPT. Using the same prompts in new chats can result in substantially
different outputs. It is not clear whether this is due to an inherent instability in GPT‚Äôs next-
token-prediction process or a programmed discourse function that GPT is emulating with its
limited memory. We speculate that GPT may treat separate, repeated prompts‚Äîeven across
different chat instances‚Äîas requests for new or different information. This is to say, the model
may, in effect, be performing a kind of pragmatic reasoning by making inferences about our
intentions as we repeat the same prompts over and over again.

7 conclusion
25
7
conclusion
Large language models perform remarkably well in predicting the next sequence to generate
coherent text and are also able to generate theoretical linguistic analyses with various degrees
of accuracy. This paper outlines a new research program that tests the metalinguistic abilities
of LLMs. We demonstrate that GPT-4 is the first LLM that can generate coherent metalinguistic
analyses. We present several prompt designs that elicit GPT-4‚Äôs performance on syntactic,
phonological, and semantic tasks. This line of research has already yielded some further
insights into the GPT-4‚Äôs ability to do explicit recursion, a unique property of human language
(DƒÖbkowski and Begu≈°, 2023). We believe several further insights can be gained through the
proposed research program.
The metalinguistic abilities of LLMs will also be useful in the classroom. Many instructors
have already begun incorporating ChatGPT into their coursework, typically by using it to
generate text which students can then critique or improve. This can be especially useful in
linguistics education. GPT-4 is capable of generating theoretical analyses in several different
linguistic domains. Here, we show that these analyses are often far from perfect, which makes
them useful educational tools, by giving students the opportunity to take the role of the
instructor and critically evaluate the model‚Äôs mistakes. This is not only useful for linguistic
purposes, but also for learning how to evaluate and interpret models‚Äô performance‚Äîa task
that we believe will be increasingly important in the future.
bibliography
Ajdukiewicz, Kazimierz (1935). ‚ÄúDie syntaktische Konnexit√§t.‚Äù In: Studia philosophica 1. En-
glish translation in S. McCall (ed.), Polish logic 192‚Äì1939, 207‚Äì231. Oxford: Oxford University
Press, pp. 1‚Äì27.
Begu≈°, Ga≈°per (2018). ‚ÄúUnnatural phonology: A synchrony-diachrony interface approach.‚Äù
PhD thesis. Harvard University.
Begu≈°, Ga≈°per (2022). ‚ÄúDistinguishing cognitive from historical influences in phonology.‚Äù In:
Language 98.1, pp. 1‚Äì34. url: https://muse.jhu.edu/article/849525.
Begu≈°, Ga≈°per (May 2023). ‚ÄúGPT4 draws a syntactic tree for the sentence: ...‚Äù Accessed on
April 17, 2023. url: https://twitter.com/begusgasper/status/1635719937275871233
?s=20.
Begu≈°, Ga≈°per and Maksymilian DƒÖbkowski (2023). ‚ÄúThe blurring history of intervocalic
devoicing.‚Äù In: PsyArXiv. doi: 10.31234/osf.io/qkjn2.
Begu≈°, Ga≈°per, Maksymilian DƒÖbkowski, and Ryan Rhodes (May 2023). Large linguistic
models: Analyzing theoretical linguistic abilities of LLMs. OSF. url: https : / / osf . io / y3
bpt/?view_only=22fb71220b77422cb8f296e250c212f1.
Begu≈°, Ga≈°per, Andrej Leban, and Shane Gero (2023). ‚ÄúApproaching an unknown communi-
cation system by latent space exploration and causal inference.‚Äù In: arXiv 2303.10931. eprint:
2303.10931 (stat.ML).
Behzad, Shabnam, Keisuke Sakaguchi, Nathan Schneider, and Amir Zeldes (July 2023).
‚ÄúELQA: A Corpus of Metalinguistic Questions and Answers about English.‚Äù In: Proceedings

bibliography
26
of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long
Papers). Toronto, Canada: Association for Computational Linguistics, pp. 2031‚Äì2047. doi:
10.18653/v1/2023.acl-long.113. url: https://aclanthology.org/2023.acl-long.113.
Berko, Jean (1958). ‚ÄúThe child‚Äôs learning of English morphology.‚Äù In: Word 14.2-3, pp. 150‚Äì177.
Bialystok, Ellen and Ellen Bouchard Ryan (1985). ‚ÄúToward a Definition of Metalinguistic
Skill.‚Äù In: Merrill-Palmer Quarterly 31.3, pp. 229‚Äì251. issn: 0272930X, 15350266. url: http:
//www.jstor.org/stable/23086295 (visited on 05/03/2023).
Blevins, Terra, Hila Gonen, and Luke Zettlemoyer (July 2023). ‚ÄúPrompting Language Mod-
els for Linguistic Structure.‚Äù In: Proceedings of the 61st Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Papers). Toronto, Canada: Association for
Computational Linguistics, pp. 6649‚Äì6663. doi: 10.18653/v1/2023.acl-long.367. url:
https://aclanthology.org/2023.acl-long.367.
Brown, Tom B. et al. (2020). Language Models are Few-Shot Learners. arXiv: 2005.14165 [cs.CL].
Chomsky, Noam (1993). Lectures on government and binding: The Pisa lectures. Studies in Genera-
tive Grammar [SGG] 9. Walter de Gruyter. doi: https://doi.org/10.1515/9783110884166.
Chomsky, Noam (2014). The Minimalist Program. MIT press.
Chomsky, Noam and Morris Halle (1968). The Sound Pattern of English. New York: Harper &
Row.
Chomsky, Noam, Roderick A Jacobs, and Peter S Rosenbaum (1970). ‚ÄúRemarks on nominal-
ization.‚Äù In: 1970 184, p. 221.
DƒÖbkowski, Maksymilian and Ga≈°per Begu≈° (2023). Large language models and (non-)linguistic
recursion. arXiv: 2306.07195 [cs.CL].
Davies, Alex et al. (2021). ‚ÄúAdvancing mathematics by guiding human intuition with AI.‚Äù In:
Nature 600.7887, pp. 70‚Äì74. doi: 10.1038/s41586-021-04086-x. url: https://doi.org/10
.1038/s41586-021-04086-x.
Fillmore, Charles J, Paul Kay, and Mary Catherine O‚Äôconnor (1988). ‚ÄúRegularity and idiomatic-
ity in grammatical constructions: The case of let alone.‚Äù In: Language, pp. 501‚Äì538.
Gulordava, Kristina, Piotr Bojanowski, Edouard Grave, Tal Linzen, and Marco Baroni (June
2018). ‚ÄúColorless Green Recurrent Networks Dream Hierarchically.‚Äù In: Proceedings of the
2018 Conference of the North American Chapter of the Association for Computational Linguistics:
Human Language Technologies, Volume 1 (Long Papers). New Orleans, Louisiana: Association
for Computational Linguistics, pp. 1195‚Äì1205. doi: 10.18653/v1/N18-1108. url: https:
//aclanthology.org/N18-1108.
Haider, Hubert (2023). ‚ÄúIs Chat-GPT a grammatically competent informant?‚Äù Manuscript.
url: https://lingbuzz.net/lingbuzz/007285.
Heim, Irene and Angelika Kratzer (1998). Semantics in generative grammar. Wiley-Blackwell.
isbn: 978-0-631-19713-3.
Jackendoff, Ray (1977). X-bar Syntax: A study of phrase structure. Cambridge, MA: MIT press.
Jumper, John et al. (2021). ‚ÄúHighly accurate protein structure prediction with AlphaFold.‚Äù In:
Nature 596.7873, pp. 583‚Äì589. doi: 10.1038/s41586-021-03819-2. url: https://doi.org/1
0.1038/s41586-021-03819-2.

bibliography
27
Katzir, Roni (2023). ‚ÄúWhy large language models are poor theories of human linguistic
cognition. A reply to Piantadosi (2023).‚Äù Manuscript. Tel Aviv University. url: https:
//lingbuzz.net/lingbuzz/007190.
Linzen, Tal, Emmanuel Dupoux, and Yoav Goldberg (2016). ‚ÄúAssessing the Ability of LSTMs
to Learn Syntax-Sensitive Dependencies.‚Äù In: Transactions of the Association for Computational
Linguistics 4, pp. 521‚Äì535. doi: 10.1162/tacl_a_00115. url: https://aclanthology.org/Q1
6-1037.
Matusevych, Yevgen and Jennifer Culbertson (2022). ‚ÄúTrees neural those: RNNs can learn the
hierarchical structure of noun phrases.‚Äù In: Proceedings of the Annual Meeting of the Cognitive
Science Society 44.44, pp. 1848‚Äì1854.
McElfatrick, Dave (Mar. 2008). Cyanide & Happiness (Explosm.net): Comicshithead. url: https:
//explosm.net/comics/dave-comicshithead#comic.
Murty, Shikhar, Pratyusha Sharma, Jacob Andreas, and Christopher D. Manning (2022).
Characterizing Intrinsic Compositionality in Transformers with Tree Projections. arXiv: 2211.012
88 [cs.CL].
OpenAI (2023). GPT-4 Technical Report. arXiv: 2303.08774 [cs.CL].
Perlmutter, David M. (1980). ‚ÄúRelational grammar.‚Äù In: Current Approaches to Syntax. Brill,
pp. 195‚Äì229.
Piantadosi, Steven (2023). ‚ÄúModern language models refute Chomsky‚Äôs approach to lan-
guage.‚Äù Manuscript. url: https://lingbuzz.net/lingbuzz/007180.
Prince, Alan and Paul Smolensky (1993/2004). Optimality Theory: Constraint Interaction in
Generative Grammar. First published in 1993, Tech. Rep. 2, Rutgers University Center for
Cognitive Science. Malden, MA: Blackwell.
Schaeffer, Rylan, Brando Miranda, and Sanmi Koyejo (2023). Are Emergent Abilities of Large
Language Models a Mirage? arXiv: 2304.15004 [cs.AI].
Tantau, Till (2007). TikZ and pgf Manual for version 1.18. url: https://www.bu.edu/math/
files/2013/08/tikzpgfmanual.pdf.
Tunmer, William E. and Michael L. Herriman (1984). ‚ÄúThe Development of Metalinguis-
tic Awareness: A Conceptual Overview.‚Äù In: Metalinguistic Awareness in Children: Theory,
Research, and Implications. Ed. by William E. Tunmer, Christopher Pratt, and Michael L. Herri-
man. Berlin, Heidelberg: Springer Berlin Heidelberg, pp. 12‚Äì35. isbn: 978-3-642-69113-3. doi:
10.1007/978-3-642-69113-3_2. url: https://doi.org/10.1007/978-3-642-69113-3_2.
Vaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
≈Åukasz Kaiser, and Illia Polosukhin (2017). ‚ÄúAttention is All you Need.‚Äù In: Advances
in Neural Information Processing Systems. Ed. by I. Guyon, U. Von Luxburg, S. Bengio, H.
Wallach, R. Fergus, S. Vishwanathan, and R. Garnett. Vol. 30. Curran Associates, Inc. url:
https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a84
5aa-Paper.pdf.
Wilcox, Ethan, Roger Levy, Takashi Morita, and Richard Futrell (Nov. 2018). ‚ÄúWhat do RNN
Language Models Learn about Filler‚ÄìGap Dependencies?‚Äù In: Proceedings of the 2018 EMNLP
Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP. Brussels, Belgium:
Association for Computational Linguistics, pp. 211‚Äì221. doi: 10.18653/v1/W18-5423. url:
https://aclanthology.org/W18-5423.

bibliography
28
Yedetore, Aditya, Tal Linzen, Robert Frank, and R. Thomas McCoy (2023). ‚ÄúHow poor is
the stimulus? Evaluating hierarchical generalization in neural networks trained on child-
directed speech.‚Äù In: arXiv preprint arXiv:2301.11462.
≈Ωivanoviƒá, Sa≈°o (2017). Forest. https://ctan.org/pkg/forest. Version 2.1.5. LaTeX package
for drawing (linguistic) trees.
acknowledgements
Ga≈°per Begu≈° would like to thank students in his Linguistics 111:
Phonology and Linguistics 211A: Advanced Phonology I classes at UC Berkeley. We also thank
Zuzanna Fuchs and Yimei Xiang for their feedback on syntactic and semantic analyses. This
research was partially funded by the Georgia Lee Fellowship in the Society of Hellman Fellows
to Ga≈°per Begu≈°.

