RydbergGPT
David Fitzek,1, 2 Yi Hong Teoh,3 Hin Pok Fung,3 Gebremedhin A. Dagnew,3
Ejaaz Merali,3 M. Schuyler Moss,3 Benjamin MacLellan,3 and Roger G. Melko3, 4
1Department of Microtechnology and Nanoscience,
Chalmers University of Technology, 412 96 Gothenburg, Sweden
2Volvo Group Trucks Technology, 405 08 Gothenburg, Sweden
3Department of Physics and Astronomy, University of Waterloo,
200 University Ave.
West, Waterloo, Ontario N2L 3G1, Canada
4Perimeter Institute for Theoretical Physics, Waterloo, Ontario N2L 2Y5, Canada
(Dated: June 3, 2024)
We introduce a generative pretained transformer (GPT) designed to learn the measurement out-
comes of a neutral atom array quantum computer. Based on a vanilla transformer, our encoder-
decoder architecture takes as input the interacting Hamiltonian, and outputs an autoregressive
sequence of qubit measurement probabilities. Its performance is studied in the vicinity of a quan-
tum phase transition in Rydberg atoms in a square lattice array. We explore the ability of the
architecture to generalize, by producing groundstate measurements for Hamiltonian parameters not
seen in the training set. We focus on examples of physical observables obtained from inference on
three different models, trained in fixed compute time on a single NVIDIA A100 GPU. These can act
as benchmarks for the scaling of larger RydbergGPT models in the future. Finally, we provide Ry-
dbergGPT open source, to aid in the development of foundation models based off of a wide variety
of quantum computer interactions and data sets in the future.
I.
INTRODUCTION
Generative models have emerged as a key technology
for predicting the probabilistic behavior of a quantum
system.
Their most basic task is to produce a tar-
get sequence representing qubit measurement outcomes;
e.g. projective measurements distributed according to the
Born rule of quantum mechanics. A wide variety of ar-
chitectures have been explored for this purpose, from
restricted Boltzmann machines [1–9], to recurrent neu-
ral networks [5, 10–16] and transformers [17–24]. While
the above models have been trained to output target
sequences drawn from a single parameterized distribu-
tion, the utility of generative models can be expanded by
adopting modern encoder-decoder architectures. In this
case, the encoder takes a source sequence and maps it
to a context vector. This is then passed to the decoder,
which combines it with other inputs and maps it to the
target sequence.
The most powerful architecture to emerge recently is
the attention-based transformer [25], variations of which
underlie most of the large language models (LLMs) be-
ing developed by academia and industry today [26–31].
A key challenge in physics that can be tackled by the
vanilla encoder-decoder transformer structure is the task
of predicting the output of a quantum computer given
its experimental settings. In the simplest example, the
source sequence is a description of an interacting Hamil-
tonian, and the target sequence is the conditional proba-
bility of each qubit measurement outcome, selected from
a dictionary of size two. Viewing the task as a language
problem, the self-attention mechanism encodes context,
or correlations, between qubit measurement outcomes.
Self-attention has been shown to be a suitable mech-
anism of encoding the possible long-range correlations
– including entanglement – found in quantum systems
without the need to pass hidden or latent vectors along
the sequence [17, 19, 20]. The impressive demonstrations
of scaling in large language models gives hope that the
transformer architecture will benefit quantum computers
as they continue to grow in size and capability [32].
In this work, we train an attention-based transformer
to learn the distribution of qubit measurement outcomes
corresponding to a Hamiltonian describing an array of
interacting Rydberg atoms. Using the encoder-decoder
architecture, we use simulated Rydberg occupation data
drawn from a two-dimensional qubit array to train three
different models. Once trained, each model can produce
new data via autoregressive sampling of the decoder. In
the case of a Rydberg atom array in its groundstate, the
autoregressive output can be interpreted as the complete
quantum wavefunction, allowing for the calculation of
many physical observables of interest. We report results
for estimators of the energy and other physical observ-
ables. By varying the Hamiltonian that is input to the en-
coder, we find that the transformer is able to accurately
predict qubit measurement outcomes in regions outside
of the training regime. Our results give optimism that
attention-based transformer models are well-positioned
to provide a powerful approach for predicting the prop-
erties of quantum many-body systems.
II.
RYDBERG ATOM ARRAY PHYSICS
Rydberg atoms arrays are a powerful platform for
quantum information processing where interacting qubits
are encoded with the electronic ground state |g⟩and ex-
cited (Rydberg) state |r⟩for each atom [33–37]. We con-
sider a system of N = L × L atoms arranged on a square
arXiv:2405.21052v1  [quant-ph]  31 May 2024

2
lattice. The governing Hamiltonian defining the Rydberg
atom array interactions has the following form:
ˆH =
X
i<j
C6
∥ri −rj∥6 ˆniˆnj −δ
N
X
i=1
ˆni −Ω
2
N
X
i=1
ˆσx
i ,
(1)
C6 = Ω
Rb
a
6
,
Vij =
a6
∥ri −rj∥6 ,
(2)
where ˆσx
i = |g⟩i⟨r|i + |r⟩i⟨g|i, the occupation number
operator ˆni = 1
2 (ˆσi + 1) = |r⟩i⟨r|i and ˆσi = |r⟩i⟨r|i −
|g⟩i⟨g|i. The experimental settings of a Rydberg atom ar-
ray are controlled by the detuning from resonance δ, Rabi
frequency Ω, lattice length scale a and the positions of the
atoms {ri}N
i . From Eq. (2), we obtain a symmetric ma-
trix V, that encapsulates the relevant information about
the lattice geometry, and derive the Rydberg blockade
radius Rb, within which simultaneous excitations are pe-
nalized [33, 34]. Finally, for the purposes of our study,
the atom array is considered to be affected by thermal
noise, in equilibrium at a temperature T = 1/β. The
experimental settings are thus captured by the set of pa-
rameters x = (Ω, δ/Ω, Rb/a, V, βΩ). By adjusting the
free parameters in the Rydberg Hamiltonian, the system
can be prepared in various phases of matter, separated
by distinct phase transitions [37–42].
Rydberg atom array experiments are able to perform
projective measurements of the qubit states in the oc-
cupation basis. These measurements are informationally
complete at T = 0 due to the positive real-valued ground
state wavefunction [37, 43–45]. In the next section, we
consider these projective measurements as training vec-
tors for an unsupervised learning strategy for our gen-
erative model. The goal of the generative model after
training will be to predict, given a Hamiltonian, the dis-
tribution of projective measurements in the quantum sys-
tem.
III.
TRANSFORMER ARCHITECTURE
In order to learn the qubit measurement distribu-
tion of the Rydberg atom array, we employ the trans-
former encoder-decoder architecture [25]. These genera-
tive models are universal sequence-to-sequence function
approximators and, granted sufficient tunable parame-
ters θ, can encode arbitrarily complex functions [46].
Transformers belong to the class of generative models
known as autoregressive models.
Autoregressive mod-
els learn conditional probability distributions over the N
individual variables, such as qubit states σ = {σi}N
i ,
pθ(σi|σi−1, . . . , σ1).
The joint distribution that arises
from these conditionals can be used to represent the like-
lihood of measurement outcomes,
pθ(σ) =
n
Y
i=1
pθ (σi | σi−1, . . . , σ1) .
(3)
Rb
x = {Ω, δ/Ω, Rb/a, V, βΩ}
↑
↓
↑
↓
↓
↑
↓
↑
↑
↓
↑
↓
↓
↑
↓
↑
σ = {σ1, σ1, ..., σN}
GraphNN
PosEmb
Transformer Encoder
MHA (Self)
FFNN
Embedding
PosEmb
Transformer Decoder
MHA (Self, Causal)
MHA (Cross)
FFNN
pθ(σ; x) = {pθ(σ1; x), pθ(σ2|σ1; x), ..., pθ(σN|σ<N; x)}
context
Measure
Figure 1. Overview of RydbergGPT. The bottom section rep-
resents the data from the physical system used in the architec-
ture. On the left is a diagram representing the experimental
settings x of the system and on the right is a corresponding
projective measurement σ. The experimental settings are in-
puts to the encoder portion of the model, composed of a graph
neural network and the transformer encoder, and is processed
to form a context vector. This context vector is combined with
the sample to obtain a chain of conditional probabilities from
the transformer decoder, the product of which is the probabil-
ity of obtaining the sample from the system. The sequence of
conditional probabilities is mapped onto the 2D lattice with
a “snake” path as illustrated.
This is the decoder output, which can be sampled to in-
fer any number of qubit measurements. This output can
be related to a pure quantum wavefunction for example
through the Born rule pθ(σ) = |Ψθ(σ)|2. Additionally,
under the assumption of a positive real-valued wavefunc-
tion, the transformer’s output captures the full quantum
state, i.e. Ψθ(σ) =
p
pθ(σ) [10, 33], where θ denotes the
model parameters.
An encoder that maps the experimental Hamiltonian
settings to a context vector can be used to condition this
decoder. The encoder-decoder architecture that we use
is illustrated in Fig. 1 and follows the overall design in-
troduced by Vaswani et al. [25].
Note that the source sequence for our encoder, the ex-
perimental Hamiltonian settings x, have a natural graph
representation. The interactions V can be considered as
the edge attributes and the remaining variables node at-
tributes. The field of machine learning has proposed a
multitude of models for processing such graphs [47]. In

3
our architecture, we choose to process the graph with a
convolutional graph neural network [47, 48]. This allow
us to map the graph, which scales quadratically in the
number of atoms N, to a sequence representation that
scales linearly. To further capture correlations, we feed
the sequence to the vanilla transformer encoder, consist-
ing of multiple self multi-head attention and feedforward
blocks, and obtain the context vector. The decoder au-
toregressively parameterizes the equilibrium state of the
system with respect to the context via an attention mech-
anism between the context and the intermediate state
within the decoder. The choice of hyperparameters for
the architecture is outlined in Table I.
Table I. The transformer encoder-decoder architecture and
training parameters. Including the dimension of the feedfor-
ward network layer (dff), the model dimension (dmodel), the
graph dimension (dgraph) as well as the total number of train-
able parameters.
Parameter
Value
Neural network architecture
dff
128
dmodel
32
dgraph
64
num heads
8
num blocks encoder
1
num blocks decoder
3
num graph layers
2
trainable params
66562
Training hyperparameters
batch size
1024
optimizer
AdamW
dropout
0.1
learning rate
0.001
learning rate schedule Cosine annealing warm start
T0
1
Tmult
2
ηmin
0.00001
dataset buffer
50
A powerful property of autoregressive models is the
ability to perform independent and identically dis-
tributed sampling without the use of Markov chain
Monte Carlo (MCMC). To perform sampling, we input
the experimental settings of interest to our encoder and
obtain the context vector as output. Following that, the
decoder, conditioned on this context, takes as input a
starting token and samples the first element of the se-
quence. Then, we iteratively sample the ith token condi-
tioned on all tokens preceding it. We traverse the lattice
of atoms systematically with the snake path convention,
shown in the bottom right of Fig. 1.
IV.
TRAINING TRANSFORMER MODELS
We follow a data-driven training procedure, where we
train transformers on a synthetic dataset D composed of
pairs of source and target sequences. The former corre-
sponds to experimental settings x, input to the encoder,
and the latter corresponds to binary qubit measurement
data, input to the decoder.
Our target sequences are
simulated Rydberg occupation measurements, produced
via Stochastic Series Expansion quantum Monte Carlo
(QMC) method described in Ref. [44].
We employ our QMC to produce uncorrelated samples,
which we collect across a wide range of settings. Specif-
ically, we choose values of the inverse temperature βΩ
within the set {0.5, 1, 2, 4, 8, 16}, the detuning parameter
δ/Ωover the range [−0.364, 3.173], the Rydberg blockade
Rb/a within {1.05, 1.15, 1.3}, and the linear system size L
for the values {5, 6, 11, 12, 15, 16}. Without loss of gener-
ality, we take Ω= 1 henceforth. For each configuration,
we generate a dataset containing |D| = 105 entries, each
corresponding to a binary measurement sampled from
the QMC simulation. The datasets are selected to ex-
plore δ/Ωvalues near a quantum phase transition in the
square-lattice Rydberg atom arrays, which occurs at ap-
proximately δ/Ω= 1.1 [44].
A transformer architecture with parameters θ is
trained in an unsupervised manner with a dataset D by
minimizing the Kullback-Leibler (KL) divergence, result-
ing in the loss function:
L(θ) ≈−1
|D|
X
σ∈D
ln pθ(σ).
(4)
This loss is minimized when the decoder output and the
target distributions are equal. We initialize each model
using the method introduced by Glorot et al. [49] We
use the AdamW optimizer [50], which is an extension
to stochastic gradient descent with an adaptive learning
rate and weight regularization, in combination with a co-
sine annealing warm restart learning rate scheduler [51].
Details for the training hyperparameters can be found in
Table I.
In this manuscript, we train three models with differ-
ent subsets of the simulated data. Specifically, the mod-
els are all trained on data at Rb/a = 1.15 and across the
values for δ/Ωand βΩ. However, each model is trained
with different sets of linear size L. Model M1 is trained
with data for systems of size L = 5, 6, model M2 with
L = 5, 6, 11, 12 and model M3 with L = 5, 6, 11, 12, 15, 16,
refer to Table II for the total number of tokens, i.e. mea-
surement outcome of a single qubit, for each lattice size.
As we generate 105 samples for each lattice size, the
larger lattice sizes have more tokens compared to the
smaller ones.
We conducted training on a single A100 GPU for 85
hours. The training datasets for all three models are of
the order of 108, while during training, all models pro-
cessed tokens on the order of 1010. We summarize the
relevant training information in Table III.

4
Table II. Number of tokens, in units of 108, contained in our
dataset for various sizes L. Each token corresponds to the
measurement outcome of a single Rydberg qubit. For each
configuration, i.e. size and experimental settings, we have a
total of 105 samples, with each sample containing a total of
N = L × L measurements.
Lattice size, L
5
6
11
12
15
16
Tokens (108)
1.5 2.16 7.26 8.64 13.5 15.4
Table III.
Details on the training of each model, including
epochs trained, the amount of tokens in the training dataset,
in units of 108, and the total number of tokens processed dur-
ing training, in units of 1010. The models were each trained
for a constant amount of time, 85 hours, on an Nvidia A100.
Model
M1
M2
M3
Epochs
116
40
17
Tokens in dataset (108) 3.66 19.6 48.4
Tokens processed (1010) 4.25 7.82 8.23
V.
INFERENCE WITH TRAINED MODELS
In this section, we perform inference using the trained
model and study the disordered-to-checkerboard phase
transition of the Rydberg atom array, governed by the
dimensionless detuning δ/Ω. First, samples are drawn
from the model in accordance to the method described
at the end of Section III. The samples are then used to ob-
tain predictions of physical observables. We compare our
model’s predictions with estimates obtained from QMC
simulations [44]. For Rydberg atom arrays, QMC is able
to obtain an estimate of the observables that can be con-
sidered exact, within statistical errors. In showing that
our model’s predictions of these observables are in good
agreement with the QMC values, we illustrate the effi-
cacy of data-driven training as a technique for predicting
Rydberg atom array measurement outcomes for ground
states. Further, we explore our trained models’ ability to
generalize away from the ground state and also to system
sizes not represented in the training data.
In order to construct our estimators below, we draw
samples from our model decoder. In our implementation,
the complexity of sampling with respect to the system
size N = L × L is of order O(N 3). However, this com-
plexity can theoretically be reduced to O(N 2) by caching
the appropriate intermediate values in the attention cal-
culation.
A.
Physical observables
Physical quantities that are diagonal in the occupa-
tion basis can be directly computed from the samples
drawn from the model. One such quantity is the stag-
gered magnetization for the square-lattice Rydberg atom
array. The staggered magnetization is significant as it
is the order parameter for the disorder-to-checkerboard
quantum phase transition [52], and it can be calculated
with:
⟨ˆσstag⟩≈1
Ns
X
σ∼pθ(σ)
1
N

N
X
i=1
(−1)i (ni(σ) −1/2)
 ,
(5)
where i runs over all N = L × L atoms using the snake
path convention of Fig. 1. Here, ni(σ) = ⟨σ|ri⟩⟨ri|σ⟩is
the occupation number operator acting on atom i in a
given configuration σ.
In contrast, we consider an off-diagonal observable,
where we must make use of the ground state wave func-
tion amplitudes of the inferred samples Ψ(σ) =
p
pθ(σ).
As an example, we examine the spatially averaged expec-
tation value of ˆσx, which is defined as
⟨ˆσx⟩≈1
Ns
X
σ∼pθ(σ)
1
N
X
σ′∈SSF(σ)
Ψθ(σ′)
Ψθ(σ) ,
(6)
where SSF(σ) is the set of configurations that are con-
nected to σ by a single spin flip (SSF).
Finally, we consider an estimate of the ground state
energy ⟨E⟩, which is defined as
⟨E⟩≈1
Ns
X
σ∼pθ(σ)
⟨σ| ˆH|Ψθ⟩
⟨σ|Ψθ⟩.
(7)
This observable can be efficiently evaluated for local non-
diagonal operators [3, 53–55]. More detailed derivations
of Eq. (6) and Eq. (7) can be found in Appendix A.
It is important to emphasize that these estimates of
off-diagonal observables are appropriate only when the
output of the decoder can be interpreted as the normal-
ized wavefunction amplitude, Ψθ(σ) =
p
pθ(σ). In par-
ticular, this correspondence is valid when the system is
a pure state and the wavefunction can be assumed to be
real and positive in the occupation basis, as is the case for
the Rydberg Hamiltonian in its T = 0 ground state. Im-
portantly, the use of the decoder output as a wave func-
tion amplitude is no longer valid for finite temperature
states. While the above estimates can still be computed,
Ψθ(σ) is not able to capture the physics of the thermal
state, as is demonstrated in our results which follow.
B.
Results
We now demonstrate the results of the trained models
across parameter regimes and observables. In Fig. 2 we
illustrate the performance of model M1 on the three ob-
servables introduced in Section V A as a function of δ/Ω,
for Rb/a = 1.15, βΩ= 16.0. The estimators are calcu-
lated over Ns = 105 samples. The predictions are bench-
marked against the QMC observables calculated from 105
decorrelated samples, which we take as the ground truth.
At βΩ= 16, the temperature is sufficiently low for the

5
−1.54
−0.94
−0.34
⟨E⟩/N
(a)
Exact
M1
0.31
0.53
0.75
⟨ˆσx⟩/N
(b)
−0.5 0.0
0.5
1.0
1.5
2.0
2.5
3.0
δ/Ω
0.04
0.20
0.36
⟨ˆσstag⟩/N
(c)
Figure 2.
Model predictions of multiple observables across
the disordered-to-checkerboard phase transition, which is gov-
erned by the dimensionless detuning, δ/Ω. The observables
include, (a) energy, (b) site-averaged x-magnetization and (c)
the staggered magnetization. Here, blue points correspond to
observables estimated from projective measurements sampled
from the trained model (i.e., out-of-distribution), red points
indicate parameters contained within the training dataset
(i.e., in-distribution), and dotted black lines correspond to
the exact observables.
Rydberg atom array to be in its groundstate to a good
approximation. The model shows good agreement with
the QMC estimates for the energy and staggered magne-
tization. As for the x-magnetization, we observe a slight
deviation from the QMC estimates in the tails. We con-
clude that the trained model is able to generalize well
to dimensionless detunings, δ/Ω, that lie outside of the
training data set, when the system is restricted to the
groundstate.
Next, we investigate how well the model generalizes
away from the groundstate. In Fig. 3 we illustrate pre-
dictions made by model M1 with the same strategy, ex-
cept temperature T = 1/β is varied instead of the detun-
ing. We see a strong disagreement between the exact ob-
servables and the model-obtained estimates at high tem-
peratures. In our architecture, the decoder represents a
probability distribution over configurations in the Ryd-
berg occupation basis, which can be mapped one-to-one
onto a positive wavefunction of a pure state. As men-
tioned, the T = 0 ground state of the Rydberg Hamil-
tonian Eq. (1) is known to take such a form, and thus
the model performs well for the ground state, e.g. low
temperatures. However, as the temperature is increased,
the system becomes a well-mixed Gibbs state, which can
no longer be accurately represented with a positive wave-
function. As such, the output of the decoder is incapable
of accurately representing this state. The model is merely
approximating a probability distribution that results in
identical occupation-basis measurement likelihoods. This
results in the incorrect estimate of off-diagonal observ-
ables, which require the wave function interpretation of
the decoder output Ψθ(σ) =
p
pθ(σ), as in Eq. (6). To
appropriately capture such high temperature states, one
might employ more sophisticated methods such as a com-
plex density matrix [56] or minimally entangled typical
thermal states (METTS) [57, 58].
This is particularly evident in Fig. 3 at the limit of
large temperature. Here, the system tends to the fully-
mixed state where all states are equi-probable. The pos-
itive wavefunction that shares the same computational
basis probability as this state is the equi-superposition
of all states.
This state corresponds to the maximum
eigenstate of the x-magnetization, leading to the diver-
gence in Fig. 3 between the exact QMC and the trained
model estimators at high temperatures, where ⟨ˆσx⟩be-
comes significantly over-estimated. This is further sup-
ported by the fact that the high-temperature estimates
of the staggered magnetization are somewhat accurate.
Again, this behavior is due to the fact that this diagonal
estimator relies only on the computational basis prob-
abilities with which certain configurations are sampled.
Finally, we assess the ability of our transformer to gen-
eralize to larger system sizes beyond those included in the
training set. For this we use all three trained models, M1,
M2 and M3. Figure 4 presents the energy evaluations
from each of our three trained models, averaged over 104
samples.
These models were trained on consecutively
larger datasets as previously described in Table III. We
first observe that the model M1 trained on the smallest
dataset, containing only data from lattice sizes L = 5, 6,
captures the L = 5 energy accurately but struggles to
generalize to larger system sizes. Interestingly, the mod-
els trained on more extensive datasets do not accurately
capture the properties of their respective training sets,
with the energy estimates deviating significantly from ex-
pected values.
These results illustrate how physical estimators are
affected by bias in the training datasets.
As shown
in Table II, we always accumulate 105 samples for each
grid size. Therefore, the number of training tokens or
measurements varies for each lattice size; more precisely,
the dataset is biased towards larger lattice sizes. This ex-
plains why models M2 and M3 perform worse than M1 for
the lattice sizes L = 5, 6, since the fraction of measure-
ments for smaller sizes is less represented in their train-
ing dataset. However, they perform better for larger grid
sizes, where a larger relative number of training tokens
are available.

6
−0.47
−0.35
−0.23
⟨E⟩/N
(a)
Exact
M1
0.25
0.50
0.75
⟨ˆσx⟩/N
(b)
10−1
100
T/Ω
0.08
0.18
0.28
⟨ˆσstag⟩/N
(c)
Figure 3.
Model predictions of multiple observables as the
dimensionless temperature T/Ωis increased. The observables
include, (a) energy, (b) site-averaged x-magnetization and
(c) the staggered magnetization.
Again, blue points corre-
spond to observables estimated from projective measurements
sampled from the trained model (i.e., out-of-distribution),
red points indicate parameters contained within the training
dataset (i.e., in-distribution), and dotted black lines corre-
spond to the exact observables.
6
8
10
12
L
−0.55
−0.50
−0.45
−0.40
⟨E⟩/N
outside
training
data
Exact
M1
M2
M3
Figure 4. Extrapolation over lattice size L. The ground-state
energy, as estimated by quantum Monte Carlo simulations
and the models trained on three different datasets, demon-
strates how training data affect the ability to extrapolate to
system sizes not included in the training.
VI.
CONCLUSION
In this paper we have explored the training of
attention-based transformer models for the problem of
mapping a quantum Hamiltonian to the qubit measure-
ment outcomes of the corresponding quantum state. We
construct an encoder-decoder architecture suitable for
Rydberg atom arrays, where the encoder input is a repre-
sentation of the interacting Hamiltonian, and the decoder
output represents measurements of the Rydberg occupa-
tion.
Since transformers have been developed as autoregres-
sive models for natural language processing, they natu-
rally represent probability distributions, such as Born-
rule-distributed measurement outcomes.
In the case
where a quantum state can be assumed to be pure, real
and positive, this probability distribution has a one-to-
one mapping to the wavefunction, giving the output of
the decoder a powerful interpretation.
In the ground-
state of Rydberg atom arrays governed by Hamiltonian
Eq. (1), this assumption is valid, and we use it to demon-
strate that a trained transformer model can accurately
reproduce physical estimators in this case.
We have further explored the behavior of the trans-
former model when the quantum state is no longer real
and positive, i.e. in a thermally mixed state at higher
temperature. We clearly see the breakdown in our abil-
ity to interpret the output of the decoder as the corre-
sponding quantum state, as the physical estimators sig-
nificantly deviate from their exact values for large T.
However, we propose that our decoder architecture could
be modified to represent a density matrix [56] or METTS
[57, 58], which would be able to represent noisy, thermal
and dynamical states, extending the capabilities of the
model.
Finally, we attempt to demonstrate the scaling po-
tential of the encoder-decoder transformer.
Namely,
whether the transformer model, when trained on a
dataset composed of smaller many-body system, is able
to make predictions for larger system sizes. Our results
shows signatures of generalization, however, it also sug-
gest that we have not attained the necessary scale in
our transformer to exploit its full potential. Hopefully,
by training models with more data on a diverse set of
lattice sizes, systematic improvements in model perfor-
mance will be observed. Indeed, such systematic scaling
is the hallmark of large transformers used in industry,
giving reason for optimism [32].
More generally, just like in modern LLMs, our trans-
former architecture should allow for vastly improved per-
formance with increases in the number of trainable pa-
rameters θ, larger data sets, and more computational re-
sources. In addition, since these models are being de-
signed to represent physical qubit systems, a number
of architectural improvements should be explored. For
example, there may be more efficient ways to use to-
kens than to represent a dictionary of only two individ-
ual qubit measurements. In particular, transformers de-

7
signed for natural language processing typically manage
dictionaries comprising tens to hundreds of thousands
of tokens [29].
An alternative approach involves using
patches of qubits as tokens, which can reduce some com-
putational demands [20].
However, this method intro-
duces constraints on the system geometries, potentially
limiting the applicability to arbitrary quantum array con-
figurations.
This balance between computational effi-
ciency and system flexibility remains a critical area for
future research.
Notably, all three models discussed in this study were
trained using only a single NVIDIA A100 GPU over a
duration of 85 hours. Looking forward, we can use this
as a benchmark as the size and capability of such trans-
former models is increased towards the goal of predicting
the behavior of larger quantum systems. Inspired by the
scaling observed in large language models within industry
today, we imagine that significantly larger transformers
trained on vastly more data may be able to make highly
useful predictions, e.g. about the behavior of a quantum
computer in parameter regimes for which no data is cur-
rently available.
Finally, while our models in this study learned from
synthetic data, our transformer architecture could in-
stead be trained on qubit measurement data obtained
from real experimental devices. Such would be the be-
ginning of a new type of foundation model, trained on real
quantum computer output to generate predictions on a
wide variety of unseen input settings. Like today’s LLMs,
the capabilities of such large quantum models could in-
crease dramatically with increased number of parame-
ters, data set size, and compute, potentially ushering in
a new era of scaling in AI and quantum computer co-
design.
The RydbergGPT code and model weights for the
models discussed in this study are available at [59].
ACKNOWLEDGMENTS
We
acknowledge
support
from
the
Natural
Sci-
ences and Engineering Research Council of Canada
(NSERC) and the Perimeter Institute for Theoreti-
cal Physics.
This research was supported in part
by grant NSF PHY-2309135 to the Kavli Institute
for Theoretical Physics (KITP). Computational support
was provided by the facilities of the Shared Hierar-
chical Academic Research Computing Network (SHAR-
CNET:www.sharcnet.ca) and Compute/Calcul Canada.
Research at Perimeter Institute is supported in part by
the Government of Canada through the Department of
Innovation, Science and Economic Development Canada
and by the Province of Ontario through the Ministry of
Economic Development, Job Creation and Trade. D.F.
acknowledges the Knut and Alice Wallenberg (KAW)
Foundation for funding through the Wallenberg Cen-
tre for Quantum Technology (WACQT). The computa-
tions were enabled by resources provided by Chalmers
e-Commons at Chalmers University of Technology.
[1] Y. Nomura, A. S. Darmawan, Y. Yamaji, and M. Imada,
Restricted
boltzmann
machine
learning
for
solving
strongly correlated quantum systems, Phys. Rev. B 96,
205152 (2017).
[2] X. Gao and L.-M. Duan, Efficient representation of quan-
tum many-body states with deep neural networks, Na-
ture Communications 8, 662 (2017).
[3] G. Carleo and M. Troyer, Solving the quantum many-
body problem with artificial neural networks, Science
355, 602 (2017).
[4] G. Torlai and R. G. Melko, Latent space purification via
neural density operators, Phys. Rev. Lett. 120, 240503
(2018).
[5] J. Carrasquilla and G. Torlai, How to use neural net-
works to investigate quantum many-body physics, PRX
Quantum 2, 040201 (2021).
[6] L. L. Viteritti, F. Ferrari, and F. Becca, Accuracy of
restricted Boltzmann machines for the one-dimensional
J1 −J2 Heisenberg model, SciPost Phys. 12, 166 (2022).
[7] G. Torlai, B. Timar, E. P. L. van Nieuwenburg, H. Levine,
A.
Omran,
A.
Keesling,
H.
Bernien,
M.
Greiner,
V. Vuleti´c, M. D. Lukin, R. G. Melko, and M. Endres,
Integrating neural networks with a quantum simulator
for state reconstruction, Phys. Rev. Lett. 123, 230504
(2019).
[8] J. Carrasquilla, G. Torlai, R. G. Melko, and L. Aolita,
Reconstructing quantum states with generative models,
Nat. Mach. Intell. 1, 155 (2019).
[9] S. Czischek, M. G¨arttner, and T. Gasenzer, Quenches
near ising quantum criticality as a challenge for artificial
neural networks, Phys. Rev. B 98, 024311 (2018).
[10] M. S. Moss, S. Ebadi, T. T. Wang, G. Semeghini,
A. Bohrdt, M. D. Lukin, and R. G. Melko, Enhanc-
ing variational monte carlo simulations using a pro-
grammable quantum simulator, Phys. Rev. A 109,
032410 (2024).
[11] S. Czischek, M. S. Moss, M. Radzihovsky, E. Merali, and
R. G. Melko, Data-Enhanced Variational Monte Carlo
Simulations for Rydberg Atom Arrays, Physical Review
B 105, 205108 (2022), arxiv:2203.04988.
[12] J. Carrasquilla and R. G. Melko, Machine learning phases
of matter, Nature Physics 13, 431 (2017).
[13] S. Morawetz, I. J. S. De Vlugt, J. Carrasquilla, and
R. G. Melko, U(1)-symmetric recurrent neural networks
for quantum state reconstruction, Phys. Rev. A 104,
012401 (2021).
[14] M. Hibat-Allah, R. G. Melko, and J. Carrasquilla, In-
vestigating topological order using recurrent neural net-
works, Phys. Rev. B 108, 075152 (2023).
[15] M. Hibat-Allah, E. M. Inack, R. Wiersema, R. G. Melko,
and J. Carrasquilla, Variational neural annealing, Nature
Machine Intelligence 3, 952–961 (2021).
[16] M. Hibat-Allah, R. G. Melko, and J. Carrasquilla, Sup-
plementing recurrent neural network wave functions with

8
symmetry and annealing to improve accuracy (2024),
arXiv:2207.14314 [cond-mat.dis-nn].
[17] H. Wang, M. Weber, J. Izaac, and C. Y.-Y. Lin, Pre-
dicting Properties of Quantum Systems with Conditional
Generative Models (2022), arxiv:2211.16943 [quant-ph].
[18] J. Carrasquilla, D. Luo, F. P´erez, A. Milsted, B. K.
Clark, M. Volkovs, and L. Aolita, Probabilistic simula-
tion of quantum circuits using a deep-learning architec-
ture, Physical Review A 104, 032610 (2021).
[19] Y.-H. Zhang and M. Di Ventra, Transformer quantum
state: A multipurpose model for quantum many-body
problems, Physical Review B 107 (2023).
[20] K.
Sprague
and
S.
Czischek,
Variational
Monte
Carlo with large patched transformers, Communications
Physics 7, 90 (2024).
[21] P. Cha, P. Ginsparg, F. Wu, J. Carrasquilla, P. L. McMa-
hon, and E.-A. Kim, Attention-based Quantum Tomog-
raphy, Machine Learning:
Science and Technology 3,
01LT01 (2022).
[22] L. L. Viteritti, R. Rende, and F. Becca, Transformer vari-
ational wave functions for frustrated quantum spin sys-
tems, Phys. Rev. Lett. 130, 236401 (2023).
[23] L. L. Viteritti, R. Rende, A. Parola, S. Goldt, and
F. Becca, Transformer wave function for the shastry-
sutherland model:
emergence of a spin-liquid phase
(2024), arXiv:2311.16889 [cond-mat.str-el].
[24] H. Lange, G. Bornet, G. Emperauger, C. Chen, T. La-
haye, S. Kienle, A. Browaeys, and A. Bohrdt, Trans-
former neural networks and quantum simulators: a hy-
brid approach for simulating strongly correlated systems,
(2024).
[25] A. Vaswani,
N. Shazeer,
N. Parmar,
J. Uszkoreit,
L. Jones, A. N. Gomez,  L. Kaiser, and I. Polosukhin,
Attention is all you need, in Advances in Neural Infor-
mation Processing Systems, Vol. 2017-Decem (2017) pp.
5999–6009, arxiv:1706.03762.
[26] C. Zhang, C. Zhang, S. Zheng, Y. Qiao, C. Li, M. Zhang,
S. K. Dam, C. M. Thwal, Y. L. Tun, L. L. Huy, D. kim, S.-
H. Bae, L.-H. Lee, Y. Yang, H. T. Shen, I. S. Kweon, and
C. S. Hong, A complete survey on generative ai (aigc):
Is chatgpt from gpt-4 to gpt-5 all you need?
(2023),
arXiv:2303.11717 [cs.AI].
[27] R. Gozalo-Brizuela and E. C. Garrido-Merchan, Chatgpt
is not all you need. a state of the art review of large
generative ai models (2023), arXiv:2301.04655 [cs.LG].
[28] S. Wu, O. Irsoy, S. Lu, V. Dabravolski, M. Dredze,
S.
Gehrmann,
P.
Kambadur,
D.
Rosenberg,
and
G. Mann, BloombergGPT: A Large Language Model for
Finance (2023), arxiv:2303.17564 [cs, q-fin].
[29] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Ka-
plan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sas-
try, A. Askell, S. Agarwal, A. Herbert-Voss, G. Krueger,
T. Henighan, R. Child, A. Ramesh, D. Ziegler, J. Wu,
C. Winter, C. Hesse, M. Chen, E. Sigler, M. Litwin,
S. Gray, B. Chess, J. Clark, C. Berner, S. McCandlish,
A. Radford, I. Sutskever, and D. Amodei, Language mod-
els are few-shot learners, in Advances in Neural Informa-
tion Processing Systems, Vol. 33, edited by H. Larochelle,
M. Ranzato, R. Hadsell, M. Balcan, and H. Lin (Curran
Associates, Inc., 2020) pp. 1877–1901.
[30] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A.
Lachaux, T. Lacroix, B. Rozi`ere, N. Goyal, E. Ham-
bro, F. Azhar, A. Rodriguez, A. Joulin, E. Grave, and
G. Lample, LLaMA: Open and Efficient Foundation Lan-
guage Models (2023), arxiv:2302.13971 [cs].
[31] K. Nakaji, L. B. Kristensen, J. A. Campos-Gonzalez-
Angulo, M. G. Vakili, H. Huang, M. Bagherimehrab,
C.
Gorgulla,
F.
Wong,
A.
McCaskey,
J.-S.
Kim,
T. Nguyen, P. Rao, and A. Aspuru-Guzik, The gener-
ative quantum eigensolver (gqe) and its application for
ground state search (2024), arXiv:2401.09253 [quant-ph].
[32] J. Kaplan, S. McCandlish, T. Henighan, T. B. Brown,
B. Chess, R. Child, S. Gray, A. Radford, J. Wu, and
D. Amodei, Scaling laws for neural language models
(2020), arXiv:2001.08361 [cs.LG].
[33] X. Wu, X. Liang, Y. Tian, F. Yang, C. Chen, Y.-C. Liu,
M. K. Tey, and L. You, A concise review of Rydberg atom
based quantum computation and quantum simulation,
Chinese Physics B 30, 020305 (2021).
[34] M. D. Lukin, M. Fleischhauer, R. Cote, L. M. Duan,
D. Jaksch, J. I. Cirac, and P. Zoller, Dipole Blockade and
Quantum Information Processing in Mesoscopic Atomic
Ensembles, Physical Review Letters 87, 037901 (2001).
[35] A. Browaeys and T. Lahaye, Many-body physics with
individually controlled Rydberg atoms, Nature Physics
16, 132 (2020).
[36] D. Jaksch, J. I. Cirac, P. Zoller, S. L. Rolston, R. Cˆot´e,
and M. D. Lukin, Fast Quantum Gates for Neutral
Atoms, Physical Review Letters 85, 2208 (2000).
[37] M. Endres, H. Bernien, A. Keesling, H. Levine, E. R.
Anschuetz,
A.
Krajenbrink,
C.
Senko,
V.
Vuletic,
M. Greiner, and M. D. Lukin, Atom-by-atom assembly
of defect-free one-dimensional cold atom arrays, Science
354, 1024 (2016).
[38] S. Ebadi, T. T. Wang, H. Levine, A. Keesling, G. Se-
meghini, A. Omran, D. Bluvstein, R. Samajdar, H. Pich-
ler, W. W. Ho, S. Choi, S. Sachdev, M. Greiner,
V. Vuleti´c, and M. D. Lukin, Quantum phases of matter
on a 256-atom programmable quantum simulator, Nature
595, 227 (2021).
[39] W. Xu, A. V. Venkatramani, S. H. Cant´u, T. ˇSumarac,
V. Kl¨usener, M. D. Lukin, and V. Vuleti´c, Fast Prepa-
ration and Detection of a Rydberg Qubit Using Atomic
Ensembles, Physical Review Letters 127, 050501 (2021).
[40] R. Samajdar, W. W. Ho, H. Pichler, M. D. Lukin, and
S. Sachdev, Quantum phases of rydberg atoms on a
kagome lattice, Proceedings of the National Academy of
Sciences 118 (2021).
[41] C. Miles, R. Samajdar, S. Ebadi, T. T. Wang, H. Pich-
ler, S. Sachdev, M. D. Lukin, M. Greiner, K. Q. Wein-
berger, and E.-A. Kim, Machine learning discovery of
new phases in programmable quantum simulator snap-
shots, Phys. Rev. Res. 5, 013026 (2023).
[42] M. Kalinowski, R. Samajdar, R. G. Melko, M. D. Lukin,
S. Sachdev, and S. Choi, Bulk and boundary quantum
phase transitions in a square rydberg atom array, Phys.
Rev. B 105, 174417 (2022).
[43] S. Bravyi, D. P. DiVincenzo, R. I. Oliveira, and B. M.
Terhal, The complexity of stoquastic local hamiltonian
problems (2007), arXiv:quant-ph/0606140 [quant-ph].
[44] E. Merali, I. J. S. De Vlugt, and R. G. Melko, Stochas-
tic Series Expansion Quantum Monte Carlo for Rydberg
Arrays (2023), arxiv:2107.00766.
[45] W. Xu, A. V. Venkatramani, S. H. Cant´u, T. ˇSumarac,
V. Kl¨usener, M. D. Lukin, and V. Vuleti´c, Fast prepara-
tion and detection of a rydberg qubit using atomic en-
sembles, Phys. Rev. Lett. 127, 050501 (2021).

9
[46] C. Yun, S. Bhojanapalli, A. S. Rawat, S. J. Reddi,
and S. Kumar, Are Transformers universal approx-
imators of sequence-to-sequence functions?
(2020),
arxiv:1912.10077 [cs, stat].
[47] T. N. Kipf and M. Welling, Semi-supervised classification
with graph convolutional networks, in 5th International
Conference on Learning Representations, ICLR 2017 -
Conference Track Proceedings (2017) arxiv:1609.02907.
[48] Y. Wang, Y. Sun, Z. Liu, S. E. Sarma, M. M. Bronstein,
and J. M. Solomon, Dynamic graph cnn for learning on
point clouds, ACM Trans. Graph. 38 (2019).
[49] X. Glorot and Y. Bengio, Understanding the difficulty
of training deep feedforward neural networks, in Pro-
ceedings of the Thirteenth International Conference on
Artificial Intelligence and Statistics, Proceedings of Ma-
chine Learning Research, Vol. 9, edited by Y. W. Teh and
M. Titterington (PMLR, Chia Laguna Resort, Sardinia,
Italy, 2010) pp. 249–256.
[50] I. Loshchilov and F. Hutter, Decoupled weight decay
regularization, in International Conference on Learning
Representations (2019).
[51] I. Loshchilov and F. Hutter, SGDR: Stochastic gradient
descent with warm restarts, in International Conference
on Learning Representations (2017).
[52] R. Samajdar, W. W. Ho, H. Pichler, M. D. Lukin, and
S. Sachdev, Complex density wave orders and quantum
phase transitions in a model of square-lattice rydberg
atom arrays, Phys. Rev. Lett. 124, 103601 (2020).
[53] G. Torlai, G. Mazzola, J. Carrasquilla, M. Troyer,
R. Melko, and G. Carleo, Neural-network quantum state
tomography, Nature Physics 14, 447 (2018).
[54] M. Hibat-Allah, M. Ganahl, L. E. Hayward, R. G. Melko,
and J. Carrasquilla, Recurrent neural network wave func-
tions, Physical Review Research 2, 023358 (2020).
[55] R. G. Melko, G. Carleo, J. Carrasquilla, and J. I. Cirac,
Restricted Boltzmann machines in quantum physics, Na-
ture Physics 15, 887 (2019).
[56] S.
Kothe
and
P.
Kirton,
Liouville
space
neural
network
representation
of
density
matrices
(2023),
arXiv:2305.13992 [quant-ph].
[57] S. R. White, Minimally entangled typical quantum states
at finite temperature, Phys. Rev. Lett. 102, 190601
(2009).
[58] D. Hendry, H. Chen, and A. Feiguin, Neural network
representation for minimally entangled typical thermal
states, Phys. Rev. B 106, 165111 (2022).
[59] D. Fitzek and Y. H. Teoh, Rydberggpt (2024), code
repository available at:
https://github.com/PIQuIL/
RydbergGPT/tree/main.
Appendix A: Estimating expectation values of
off-diagonal operators
In this work, we estimate the ground state expectation
values of two off-diagonal operators, ⟨ˆσx⟩and ⟨E⟩, using
samples inferred from our trained GPT model. Here, we
provide more detail for how to arrive at equations Eq. (6)
and Eq. (7).
Considering a general, off-diagonal operator ˆO, we can
write down its expectation value with respect to the prob-
ability distribution encoded in the trained GPT model,
⟨ˆO⟩= ⟨Ψθ| ˆO|Ψθ⟩
⟨Ψθ|Ψθ⟩,
where Ψθ(σ) =
p
pθ(σ) is the wave function interpre-
tation of the joint probability distribution. Inserting the
identity in the numerator and the denominator, and mul-
tiplying by 1, the above expectation value can be equiv-
alently written as
⟨ˆO⟩=
P
σ |⟨Ψθ|σ⟩|2 × ⟨σ| ˆ
O|Ψθ⟩
⟨σ|Ψθ⟩
P
σ′ |⟨Ψθ|σ′⟩|2
.
One can recognize
|⟨Ψθ|σ⟩|2
P
σ′ |⟨Ψθ|σ′⟩|2 as some normalized prob-
ability P(σ) which can be approximated using impor-
tance sampling. Here we also define the local estimator
of our operator ˆO,
ˆOloc(σ) = ⟨σ| ˆO|Ψθ⟩
⟨σ|Ψθ⟩,
such that the expectation value of ˆO can be estimated as
⟨ˆO⟩≈1
Ns
X
σ∼pθ(σ)
ˆOloc(σ).
If we take ˆO = ˆH, then we have exactly recovered equa-
tion Eq. (7).
In order to arrive at equation Eq. (6), we will consider
what happens when the off-diagonal operator ˆO acts on
the bra ⟨σ| in the definition of ˆOloc(σ).
If our off-diagonal operator ˆO is purely off-diagonal, it
will map a given configuration configuration σ to a lin-
ear combination of some set of other configurations {σ′},
that is ˆO|σ⟩= P
{σ′} CO(σ′)|σ′⟩, and we can rewrite our
expression for ˆOloc(σ) as,
ˆOloc(σ) =
X
σ′∈SSF(σ)
CO(σ′)Ψθ(σ′)
Ψθ(σ) ,
where we have used the fact that Ψθ(σ) = ⟨σ|Ψθ⟩.
We can note that if ˆO = ˆσx, then SSF(σ) is the set of
configurations connected to σ by a single spin flip (SSF)
and CO(σ′) = 1. Then it is clear that the inner sum in
equation Eq. (6) takes the form of the equation above.

