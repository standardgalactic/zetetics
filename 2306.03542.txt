Masked Autoencoders are Efficient
Continual Federated Learners
Subarnaduti Paul1,2∗, Lars-Joel Frey1, Roshni Kamath1,2, Kristian Kersting1,2,3,4, Martin Mundt1,2*
1Department of Computer Science, TU Darmstadt, Darmstadt, Germany
2Hessian Center for AI (hessian.AI), Darmstadt, Germany
3German Research Center for Artificial Intelligence (DFKI), Darmstadt, Germany
4Centre for Cognitive Science, TU Darmstadt, Darmstadt, Germany
*{subarnaduti.paul, martin.mundt}@tu-darmstadt.de
Abstract
Machine learning is typically framed from a perspective of i.i.d., and more impor-
tantly, isolated data. In parts, federated learning lifts this assumption, as it sets
out to solve the real-world challenge of collaboratively learning a shared model
from data distributed across clients. However, motivated primarily by privacy and
computational constraints, the fact that data may change, distributions drift, or even
tasks advance individually on clients, is seldom taken into account. The field of
continual learning addresses this separate challenge and first steps have recently
been taken to leverage synergies in distributed supervised settings, in which several
clients learn to solve changing classification tasks over time without forgetting
previously seen ones. Motivated by these prior works, we posit that such federated
continual learning should be grounded in unsupervised learning of representations
that are shared across clients; in the loose spirit of how humans can indirectly
leverage others’ experience without exposure to a specific task. For this purpose,
we demonstrate that masked autoencoders for distribution estimation are particu-
larly amenable to this setup. Specifically, their masking strategy can be seamlessly
integrated with task attention mechanisms to enable selective knowledge transfer
between clients. We empirically corroborate the latter statement through several
continual federated scenarios on both image and binary datasets.
1
Introduction
Whether from a purely practical or from a biologically plausible perspective, building machines
that mirror the capabilities of humans (Lake et al., 2017) requires the ability to continue learning
throughout their lifetime (Chen & Liu, 2018). The key challenge is often framed as the sequential
learning problem, where, much in contrast to traditional static train-validation-test dataset splits,
neural models suffer from catastrophic interference (McCloskey & Cohen, 1989). They tend to
forget what they have previously seen if revisits are disallowed. When surveying the literature
landscape, several biological underpinnings (Kudithipudi et al., 2022) and practical pillars (Hadsell
et al., 2020; Mundt et al., 2023) to alleviate the catastrophic forgetting phenomenon are typically at
the focus of attention. Considerations span from regularization, dynamic architecture, and popular
episodic memory buffer techniques (Hayes et al., 2021) to tangible quantities to measure compute,
parameter growth, and various other performance assessments surrounding knowledge transfer and
accumulation over time (Mundt et al., 2022a).
However, the primary objective of continual machine learning (CL) seems to be predominantly
centered around maintaining a single model, heavily inspired by an individual human’s ability to
learn throughout their lifetime. At the same time, well-known research from social and cognitive
sciences suggests that humans also heavily benefit not only from recalling their own experiences
arXiv:2306.03542v1  [cs.LG]  6 Jun 2023

C1
C2
C2
D
Federated Learning
Continual Learning
Continual Federated Learning
C
D1
C3
D2
D3
D1'
D1
D2
D4
D1''
D2
D1
D2''
D2'
C2
direct experience
indirect experiences
Figure 1: Set-up schematic. As data (on clients) may drift individually over time, models need to mitigate
forgetting. In distributed scenarios, being additionally informed through indirect experience (dashed red arrows)
provides further learning benefit, as similar data may appear on other clients at different points in time.
but also leveraging the knowledge of their fellow humans. In other words, they learn from indirect
experiences. Take for instance an organization working together, where every member partially profits
from the growing know-how simply by listening to the report of others (Argote & Miron-Spektor,
2011; Gino et al., 2010). In addition to each individual’s episodic memory, there thus exists the
notion of a transactive memory (Wegner et al., 1985; Wegner, 1987). Relating back to machine
learning, an intuitive parallel can be found in the concept of federated learning (FL) (McMahan
et al., 2017; Kairouz et al., 2021), where several learning clients attempt to share and consolidate
their knowledge, most commonly with a central server. Alas, FL comes with its own frequent focus,
motivated primarily by computational efficiency and preservation of privacy by avoiding the explicit
communication of data. As such, a recent survey (Criado et al., 2022) argues that FL has, quoting
the original authors, a ”long road ahead“. The latter is attributed to the fact that while FL seems to
distribute data and minimize communication, it seldom considers scenarios where data distributions
on clients drift or individual tasks change over time. Speaking informally, although continual learning
and federated learning share similar properties in data being distributed across ”time“ or ”space“
respectively, their joint consideration remains largely open. We point to figure 1 for visual intuition.
Thus, inspired by the notion of transactive memory and the necessity to combine federated and
continual learning, we propose an unsupervised continual federated learner (CFL), focused on
selectively sharing acquired representations. More specifically, we build on the previous work
of federated weighted inter-client transfer (FedWeIT) (Yoon et al., 2021), which has proposed an
attention based approach to CFL for supervised classification scenarios. In particular, we leverage
their proposed parameter decomposition to intuitively integrate it with a masked autoencoder for
distribution estimation (Germain et al., 2015). We demonstrate that MADE is a model that is
particularly amenable to unsupervised CFL, due to its inherent masking strategy sharing a common
denominator with attention based masking to avoid forgetting. In summary, our contributions are:
• We draw inspiration from the supervised FedWeIT and extend it to our unsupervised
Continual Federated MAsked autoencoders for Density Estimation (CONFEDMADE); an
unsupervised continual federated learner based on masking to enable selective knowledge
transfer between clients and reduce forgetting.
• We highlight that MADE is a model particularly amenable to CFL and investigate several
non-trivial considerations, such as connectivity and masking strategy, beyond a trivial
application of federated averaging and FedWeIT to the unsupervised setting.
• We extensively evaluate our approach on several CFL scenarios on both image and numerical
data. Overall, CONFEDMADE consistently reduces forgetting while sparsifying parameters
and reducing communication costs with respect to a naive unsupervised CFL approach.
2
Background and Related Work
We briefly introduce the key principles behind typical FL and CL set-ups, as well as their conjunction
into CFL, illustrated in figure 1. In the process, we provide a concise overview of existing approaches,
with a particular focus on an unsupervised perspective to motivate our proposed CONFEDMADE.
2

(Unsupervised) Federated Learning: A standard FL set-up collaboratively trains C number of
clients {c1, c2, . . . , cC}, typically aggregated through a server module to yield a global model
(Kairouz et al., 2021). Here, each client learns on its privately accessible dataset, with the key
objective that this data is to remain private and it is prohibited to share it explicitly. Instead, the
pioneering and to date most popular approach is to communicate parameters or gradient signals and
average them: so called federated averaging (FedAvg) (McMahan et al., 2017). In practice, the focus
of FL seems to generally lie on preserving privacy while lowering communication and computation
costs. Whereas data heterogeneity becomes a challenge, it is predominantly the case because a single
task t comprised of dataset D = {x1, x2, . . . , xn} is distributed across the C clients. As each client
only sees a fraction N/C of the data, it can no longer be assumed that data is i.i.d. overall.
Different FL algorithms have been proposed to deal with this challenge. For instance, FedProx (Li
et al., 2020) introduces an additional proximity term to deal with deviations in data distribution across
clients. FedBN (Li et al., 2021b) improves convergence speed by leveraging batch normalization
layers, in a similar spirit to various works that have focused primarily on computational reduction
(Li & Wang, 2019; Li et al., 2021b; Cheng et al., 2021). Analogously, select works (Singhal et al.,
2021; Li et al., 2021a) aim to reduce computation and memory overhead, e.g. through reconstructing
local parameters or formulating contrastive losses. Most of these approaches could intuitively find
application in unsupervised approaches, where far fewer works have formulated explicitly tailored
mechanisms. Here, dedicated techniques often frame the problem from a clustering perspective
(Lubana et al., 2022; Lu et al., 2022; Chung et al., 2022; Zhang et al., 2020), reducing communi-
cation overheads in the process. Other works once more employ contrastive techniques to align
representations through distillation (Han et al., 2022) or by fragmenting training into multiple stages
(van Berlo et al., 2020). However, none of these works consider the case where each client’s data
distribution may severely shift over time, or where client-specific tasks may change independently. In
other words, although data heterogeneity is a prominent theme, continual learning seldom is.
(Unsupervised) Continual Learning: In continual learning, data heterogeneity is typically addressed
from a different, complimentary perspective to the above FL set-up. A standard CL setup trains a
single (client) model on a sequence of T tasks. This can be expressed through observing x(t)
n of N (t)
individual instances in dataset D(t), where generally p(x(t)) ̸= p(x(t+1)). The earlier mentioned
surveys in the introduction (Chen & Liu, 2018; Kudithipudi et al., 2022; Hadsell et al., 2020; Mundt
et al., 2023) highlight how the focus is then typically on designing mechanisms to avoid forgetting in
this single model, falling into general categories of regularization (e.g. elastic weight consolidation
(Kirkpatrick et al., 2017), rehearsal (e.g. gradient episodic memory (Lopez-Paz & Ranzato, 2017)),
or dynamic architectures (e.g. dynamically expandable architectures (Yoon et al., 2018)).
Unsupervised continual learning approaches can correspondingly also be attributed to these pillars.
Apart from works on lifelong generative adversarial networks (Goodfellow et al., 2014; Ramapuram
et al., 2020), autoencoders (Ballard, 1987) trained with variational inference (Kingma & Welling,
2013) present a frequent choice. Both are popular due to their ability to rehearse previously observed
examples through a generative model (Shin et al., 2017). Several unsupervised CL follow-ups (Achille
et al., 2018; Rao et al., 2019; Mundt et al., 2022b) additionally propose how latent space structuring
techniques aid in removing assumptions that have resulted in the majority of CL algorithms being
tailored to incremental classifiers (Farquhar & Gal, 2018; Mundt et al., 2022a), shifting towards
considerations on data distributions. However, unifying unsupervised CL with distributing data across
multiple clients with changing tasks is yet to be considered by these works.
(Unsupervised) Continual Federated Learning: Formally, a continual federated learner will
contain C clients {c1, c2, . . . , cC}, where each client will individually observe a sequence of Tc tasks
consisting of datasets Dt
c = {x1, x2, . . . , xn}t
c. A single task for a single client at any time step t
thus contains N (t)
c
individual instances x(t)
c,n that the local client trains for an overall amount of e
epochs with r communication rounds to the outside. Following figure 1, a client could observe a task
a different client has observed at a previous time step, i.e. observing instances originating from the
same distribution a separate client has seen, or alternatively all data at all times can be distinct.
Intuitively, one could attempt to apply any of the three pillars of CL to this distributed scenario,
as algorithms such as FedAvg can directly be applied to any unsupervised learner. In fact, Park
et al. (2021) follows this idea and couples training with a rehearsal strategy, whereas the supervised
Usmanova et al. (2021) proposes the use of distillation. However, the latter is tailored to supervised
scenarios and the former seems to be challenging to integrate into the typical federated learning
desideratum of protecting privacy (by employing an explicit memory buffer). Similarly, we could
3

conjecture the use of the variational CL procedures outlined in the former subsection, but note
that they do not easily allow for explicit data distribution estimates. Conceptually, our work thus
follows a different approach, referred to as federated weighted inter-client transfer (FedWeIT) (Yoon
et al., 2021). FedWeIT is a supervised framework that is cohesive with early postulated principles
of modularity through reduction of parameter overlap (French, 1992). Specifically, it decomposes
parameters into generic and task-specific ones and proceeds to learn attention maps to share and
preserve appropriate subsets. In our work, we show that this prior art can be extended to the
unsupervised scenario when integrated together with an efficient distribution estimator based on
masked autoencoders (MADE) (Germain et al., 2015) and the effective FedAvg.
3
Unsupervised Continual Federated Learning with Masked Autoencoders
We now formally introduce our unsupervised CFL approach with the aim for a client to learn
meaningful representations continually throughout their lifetime while leveraging other clients’
indirect experience. To mirror realistic real-world set-ups, the summarized CFL key elements are 1.)
each client holds its sequence t of privately accessible task data; 2) the distribution of tasks across the
clients may be different; 3) depicting a common continual learning assumption, a particular client is
practically unaware of the ordering of the set of tasks and a particular set of data is only seen once in
each client’s lifetime. Before introducing why masked auto-encoders are particular suitable to operate
in CFL scenarios, we briefly introduce required preliminaries and analyze important considerations
when first lifting masked autoencoders to a federated scenario (without the extra continual element).
3.1
Preliminaries: Federated Weighted Inter-client Transfer & Masked Autoencoding
FedWeIT (Yoon et al., 2021) has been proposed to tackle data heterogeneity in supervised continual
federated classification. Here, with data distribution varied across clients, a traditional FedAvg
(McMahan et al., 2017) will suffer from negative interference from other clients, typically leading to
the global model averaging an improper solution. Unlike traditional CL strategies in earlier cited
surveys, FedWeIT decomposes parameters and selectively transmits previously acquired knowledge
across clients. A generic global parameter (Wc) is first decomposed into two sets of parameters: one
assigned to capture generic knowledge for each client (Bc) and another assigned to capture task-
specific knowledge (Ac) for each individual task per client. The Server in turn stores the task-adaptive
parameters into a “knowledge base” (Kb) to combat catastrophic forgetting while learning the future
task sequences. To selectively utilize the knowledge stored in Kb and reduce inter-client interference,
each client learns to allocate a weighted attention α along with a threshold mask mc:
W t
c = Bt
c ∗mt
c + At
c +
X
iϵC/c
X
j<|t|
αt
(i,j) ∗A(j)
i
.
(1)
In order to efficiently decompose and limit forgetting, two additional loss terms are then optimized:
min
B(t)
c
,A(1:t)
c
,α(t)
c ,m(t)
c
L(W t
c, T (t)
c ) + λ1Ω({m(t)
c , A(1:t)
c
}) + λ2
t−1
X
i=1
∥∆B(t)
c
⊙m(i)
c
+ ∆A(i)
c
∥2
2 (2)
Here, in the first term Ωdenotes a sparsity-inducing regularizer, whereas the second regularizer term
is used to reduce catastrophic forgetting with respect to prior time steps by limiting the change in
relevant parameters. λ1 and λ2 are hyperparameters to weigh the individual terms.
Masked Autoencoders Germain et al. (2015) have been originally proposed for the purpose of
distribution estimation (MADE). We will later highlight why they are particularly suitable for CFL
beyond their demonstrated capability for unsupervised learning, after delving into some important
considerations and caveats. In particular, MADE adheres to the autoregressive property through
the use of masking, where an output unit ˆxd only depends on previous input units x<d, assuming a
d-dimensional input. Hence, the autoencoder outputs take the form ˆxd = p(xd|x<d), where p(xd) is
the probability of observing xd at dth dimension, acting both as a data distribution estimator as well
as a synthetic data generator. The respective structure of MADE with hidden layers l is given as:
4

Table 1: Practical considerations for federated MADE in terms of obtained negative log-likehood (↓is better).
(a) MADE with synchronized & distinct masks for
federated MNIST. In the former, random masks (or the
respective seed) gets communicated to all clients from
the server, in the latter masks are drawn independently.
Clients
Synchronized Mask
Distinct Mask
1
75.23 ± 0.27
75.23 ± 0.27
2
76.93 ± 0.35
97.27 ± 0.21
5
83.51 ± 0.83
170.7 ± 0.66
10
86.23 ± 0.99
219.1 ± 1.54
20
87.89 ± 1.11
264.0 ± 3.57
40
89.89 ± 1.47
301.2 ± 2.15
(b) Federated MADE performance for architectural
choices, including direct connections, connectivity- and
order-agnostic training, as well as their combinations.
MADE configuration
Client
Server
Baseline
89.66 ± 0.61
100.8 ± 0.66
+Direct Connection
75.66 ± 0.57
84.10 ± 0.59
+Connectivity Agnostic
98.32 ± 0.22
110.7 ± 0.60
+Order Agnostic
81.18 ± 0.64
84.78 ± 2.37
+Direct+Order
62.83 ± 0.30
71.42 ± 1.19
+Direct+Connectivity
78.64 ± 1.74
87.12 ± 1.76
+DC+CA+OA
103.6 ± 0.26
111.5 ± 0.97
hl(x) = g(bl + (W l ⊙M W l)hl−1(x))
ˆx = sigm(c + (V ⊙M V )hL(x) + (D ⊙M D)x)
(3)
Here, ˆx is the reconstructed output for a given data input x. W, V, D denote the connection matrices
for input to hidden layer, hidden to output layer, and a direct (skip) connection (DC) between the
input and output layer respectively. The respective binary mask matrices M W , M V for hidden layer,
and output layer connections are defined as:
M W
k,d = 1m(k)≥d =
(
1
if m(k) ≥d
0
otherwise,
M V
k,d = 1d>m(k) =
(
1
if d > m(k)
0
otherwise,
(4)
where k ∈{1..K} refers to hidden layer units. Following equation 4, MADE assigns binary masks
as we determine m(k) for the k-th unit by sampling random integers less than D to the hidden units.
Instead of learning a single deterministic model, we can further opt for order-agnostic training (OA)
(resampling input ordering after each mini-batch, thereby reproducing the mask for the first hidden
layer) or connectivity-agnostic training (CA) (resampling the hidden layers units). We point to the
left side of figure 2 for an illustration of a MADE network and proceed to exploit the masking in
MADE as a part of our CFL framework, thereby significantly reducing communication costs.
3.2
Embedding a Masked Autoencoder into the Federated-Averaging Framework
Before establishing MADE as an efficient continual federated learner, we first analyze traditional
FL. Hence, we first take a step back from CL and distribute data from the same distribution across
multiple clients and perform FedAvg through a central server. For each client, we define a MADE
model based on the principles discussed in the previous section. In particular, we wish to investigate
how a federated MADE (FEDMADE) performs when i) an increasing amount of clients get a smaller
percentage of data, ii) we consider different training choices such as OA, CA, etc, iii) FEDMADE is
confronted with only a (skewed) subset of data, to anticipate the later continual learning set-ups iv)
and most importantly, whether and how the masking in different clients may interfere with each other.
To analyze and answer these questions, we report the performance in terms of obtained negative
log-likelihoods, where smaller numbers are better, for an unsupervised federated MNIST (Deng,
2012) set-up. Each of the C clients receives 1/Cth of the data. In table 1a, we answer the above
questions i) and iv) simultaneously and highlight the most important aspect of moving MADE into a
federated scenario. Namely, we observe that sampling random integers for the masks, as suggested in
the original MADE framework, independently on each client network, is detrimental to the obtained
performance. The rationale is quite intuitive, as MNIST data gets distributed across an increasing
amount of clients, at each step of federated communication and averaging different MADE clients
presently update different sub-models. As a crucial step to enable effective FEDMADE and thus
also later continual FEDMADE, we are required to communicate the (sequence of) mask(s) from
the server as well. Although masks are small, this in fact incurs an extra communication expense,
which can however be almost fully alleviated if we know that the different clients operate on similar
5

   
Central Server                           
                                                 
                                     
Server              Client
3
2
1
2
2
1
2
1
2
2
2
t-1
+
t
{1:r}
Client 1
{1:r}
+
t-1
+
t
+
Client 2
Server              Client
Server              Client
End of round r
End of task t
Weighted Averaging Module
LOCAL CLIENT NETWORK
Parameter decomposition
Parameter decomposition
Transmission  of local training 
 parameters (r times)
learning through
Indirect experiences
MASKED AUTOENCODER
Knowledge Base
3
1
1
Figure 2: Proposed CONFEDMADE framework. MADE clients (left) send highly sparsified decomposed
parameters following a masked subset and a server redistributes averaged global base parameters and stores
task-adaptive parameters in a knowledge base. Clients selectively leverage the latter through attention masks.
devices and operating systems that can interpret the same random seed in the same manner, in which
case communicating the latter is sufficient. The respective synchronized mask column in table 1a
demonstrates how this almost fully alleviates performance drops, even with a large number of clients.
Table 1b complements this insight with answers to the remaining two questions, how different MADE
choices affect performance, and how a skewed subset of data affects learned representations. For this
purpose, we emulate the first step of a CL scenario and train a federated MADE on distributed MNIST
with three classes. We can observe how this results in a perhaps expected loss in performance of the
baseline without any of the optional MADE components. Naively, one would however equip MADE
with all components that have initially been hypothesized to yield advantages in its original paper.
Here, we observe a pattern that is consistent with our previous observation on masking. In fact, direct
connections provide a cohesive benefit, but this is not necessarily the case for order and connectivity-
agnostic training. The former’s resampled input ordering after each mini-batch independently on
clients, as posited in the original work, leads to more robust models. The latter’s resampling on the
level of hidden units however massively degrades performance due to interference with FedAvg,
following the same intuition as with the earlier independent masking. As these considerations are
imperative, we take them into account in our ensuing design of continual federated MADE.
3.3
CONFEDMADE: Learning Representations Continually through Indirect Experience
In the previous section, we laid out the foundation to employ MADE in federated scenarios. A critical
next step is to inherit a form of continual learning strategy that can handle catastrophic forgetting
under data distribution shifts. The most direct way would be to fully embed MADE into the earlier
outlined FedWeIT framework. We will refer to this strategy as FedWeIT-MADE. However, we can
move beyond such a combination. To this end, we propose CONFEDMADE, which on the one hand
includes our earlier findings on federated MADE and on the other hand, incorporates its masking
strategy based on the auto-regressive property in a more meaningful way into continual learning. This
will outperform the naive combination while simultaneously cutting down communication costs.
Figure 2 provides a detailed overview of CONFEDMADE. First, note that the server serves a dual
purpose: 1) averaging the learned representations by the clients and communicating global weights
WG back along with the masking matrices; 2) building a memory buffer as knowledge base for
storing the task-specific representations learned by all the clients in previous time steps.
Second, we decompose the parameters of the clients’ MADE models, fusing earlier equations 3 and
1. Importantly, to further guarantee that the auto-regressive property is satisfied, we apply MADE’s
masking to the additively decomposed parameters at any step:
6

WC ⊙M W = Bc ⊙mc ⊙M W + Ac ⊙M W +
X
iϵC/c
X
j<|t|
α(i,j) ∗A(j)
i
⊙M W
.
(5)
Following our prior analysis, we strictly ensure that an identical auto-regressive masking strategy is
imposed on each client network. Finally, the straightforward FedWeIT MADE strategy results in two
masks for Bc, that despite masking the same set of parameters, would be treated independently. We
could then treat MADE as the client architecture choice and optimize earlier equation 2.
However, once more, this is not only practically wasteful by not exploiting synergies, but can also hurt
performance through interference effects when employing FedWeIT’s regularizers to pull towards
a set of parameters that could presently be masked differently by the autoencoder. To both combat
this interference of two independent masks (to clarify, for task adaptive parameters and for MADE)
and make use of the sparsity induced by the MADE mask at any point in time, we thus optimize the
decomposed MADE client parameters through a modified loss function:
min
B,A,α,mL(W t
dec, T (t)
c ) + λ1(Ω({m(t)
c , A(1:t)
c
⊙M W })) + λ2
t−1
X
i=1
∥∆B(t)
c
⊙m(i)
c + ∆A(i)
c
∥2
2 (6)
We have primarily modified the sparsity-inducing term Ω, which in practice is an L1 norm (Yoon
et al., 2021), to yield as small as possible amount of task-adaptive (Ac) parameters to populate the
knowledge base. Here, at any point in time, we include a client’s present MADE mask M W in n
element-wise product with the Ac parameters. In this way, we avoid regularizing parameters that are
currently not contributing to the MADE prediction and thus also explicitly further encourage task-
adaptive parameters to be even sparser, subject to the empirically chosen masking ratio of the MADE
architecture. We analyse this trade-off in the later experimental section. The second, perhaps implicit
modification, is to refrain from adding the mask to the L2 regularizer, as would be intuitive according
to earlier equation 5, where the MADE mask operates on the Base parameters in conjunction with the
FedWeIT mask. The reasoning here is two-fold. First, the sum operates over previous time steps,
and as such a current steps’ MADE mask would mask a different portion of parameters and thus
interfere. Naturally, this could be solved by also storing respective MADE masks in the knowledge
base, at the expense of larger memory and communication overheads. However, and interestingly,
one can realize that we also do not need the MADE masking in this term. On the one hand, the Ac
parameters up to t −1 have already been largely sparsified through the Ω-term. On the other hand,
before computing the actual L2 term between the base and task-adaptive parameters,a difference in
the level of Bc and Ac is taken individually first (∆Bc & ∆Ac). By definition, masking a value to be
zero does not contribute to differences or sums and vice-versa if a respective weight had indeed been
masked and received no update, its difference between time steps would be zero independently of its
exact value. Overall, CONFEDMADE thus involves the following communication:
Cost of communication at end of round r: Participating clients communicate the local base
parameters to the server and the server sends back the averaged global parameter. The client’s cost is
( ˜
Bc ∗|r|), where |r| is the total number of communication rounds (linked to SGD steps) and ˜
Bc
is a masked subset (|Bc ⊙mc ⊙M W |) for each client. In reference to the original FedWeIT, the
amount of communicated (non-zero) parameters is thus reduced through MADE. The server’s cost
is ( ¯
WG∗|r|∗|C|), where ¯
WG denotes the averaged local base parameter and |C| the number of clients.
Cost of communication at end of round t: After completion of r “inner” communication rounds,
clients communicate the learned task-specific representations Ac, and the server maintains a memory,
which it communicates to the clients at the beginning of task t+1. Total communication cost accounts
to |C| ∗(|R| ∗|WG| + Ac). For reference, a communication overhead of a typical FL approach is
|C| ∗(|R| ∗θ) where θ is the full set of model parameters without decomposition or subset masking.
4
Experimental Analysis
We empirically corroborate the advantages of CONFEDMADE in three different dataset sequences
and respectively contrast it with baseline approaches. Specifically, we quantify how i) forgetting is
mitigated ii) adaptive task knowledge is attended to, and iii) communication costs are reduced.
7

Table 2: Average final negative log-likelihoods and forgetting values (lower is better) for different learning
settings and models. CONFEDMADE improves substantially through the symbiosis of auto-regressive masking
and parameter decomposition, approaching the upper-bound more closely than an unsupervised FedWeIT.
MNIST
Binary
MNIST, EMNIST
Learning Setting
FL
CL
NLL (↓)
Forgetting (↓)
NLL (↓)
Forgetting (↓)
NLL (↓)
Forgetting (↓)
Offline
×
×
72.68 ± 1.68
−
38.45 ± 1.33
−
84.98 ± 1.21
−
Federated Offline
✓
×
79.23 ± 1.11
−
40.33 ± 0.88
−
89.66 ± 3.12
−
CL-UB
×
✓
73.32 ± 1.23
0.00 ± 0.00
39.45 ± 0.37
0.00 ± 0.00
86.66 ± 3.12
0.00 ± 0.00
CL-LB
×
✓
126.1 ± 3.32
38.62 ± 2.76
81.32 ± 0.97
28.32 ± 1.23
125.3 ± 2.76
30.98 ± 0.43
FedCL-UB
✓
✓
74.47 ± 0.57
0.00 ± 0.00
41.67 ± 1.26
0.00 ± 0.00
87.34 ± 2.24
0.00 ± 0.00
FedCL-LB
✓
✓
115.3 ± 5.67
31.43 ± 1.21
84.45 ± 0.45
29.56 ± 2.54
129.8 ± 3.21
36.98 ± 1.02
FedWeIT-MADE
✓
✓
99.32 ± 1.97
19.43 ± 1.11
69.23 ± 0.66
18.02 ± 0.97
114.6 ± 0.65
20.24 ± 0.37
FedWeIT-MADE∗
✓
✓
93.32 ± 2.70
14.43 ± 0.87
63.83 ± 1.12
12.40 ± 0.87
109.3 ± 3.54
15.11 ± 0.71
CONFEDMADE
✓
✓
87.12 ± 2.76
8.32 ± 0.76
59.15 ± 0.67
8.12 ± 0.43
104.2 ± 3.10
9.76 ± 0.87
0
1
2
3
4
# Number of tasks
70
80
90
100
110
120
Negative log-likelihood
Mnist Average Task Loss
0
1
2
3
4
# Number of tasks
50
60
70
80
90
100
110
120
Mnist Base Task Loss
0
1
2
3
4
# Number of tasks
60
70
80
90
100
110
120
130
Mnist New Task Loss
CONFEDMADE
FEDWeIT-MADE
FEDWeIT-MADE*
FEDCL-LB
FEDCL-UB
Figure 3: Decomposed negative log-likelihood in FCL to showcase: (left) average of tasks seen so far, (center)
the “base” loss, i.e the value for only initial task in evolution over time to assess forgetting, (right) the “new”
loss, i.e. the value for only the newest task to gauge encoding of new knowledge. Lower values are better.
4.1
Learning settings and compared approaches
Dataset sequences. We construct three sequences of unsupervised tasks, ranging from images
to numerical data. A) Sequential MNIST: comprised of 10 distinct digit classes, we consider 5
consecutive tasks per client to each consist of images with a single random digit. B) Numerical binary
data: we create sequences across four popular binary datasets from distribution estimation literature,
namely “RCV1”, “Adult”, “Connect4”, and “Tretail” Uria et al. (2016); Dua & Graff (2017) with
100000 data instances in total. At each timestep, a client is trained on only one of these numerical
datasets. C) MNIST + EMNIST: With 36 distinct subsets, 10 digits and 26 letters, and a total of
215,600 data samples, a client observes mutually exclusive samples from either digits or letters as a
task. We emphasize that we refer to labels only for the sake of intuition in formulating sequences of
tasks for clients, not for the fully unsupervised optimization (no task conditioning, labels, etc.). We
defer to the supplementary material for detailed descriptions of constructed sequences.
Learning settings and respective models. We contrast CONFEDMADE with FedWeIT in the
continual federated setting and relate it to offline scenarios and respectively achievable lower- and
upper-bound baselines. Specifically, we consider 1) Offline: a standard MADE Germain et al. (2015),
as optimized on the entire dataset in a traditional training regime. 2) Federated Offline: similar to
the before scenario, but with all data being evenly distributed across clients at the start, following
the FedAvg MADE introduced in section 3.2 (with ablation study insights). 3) Continual lower-
and upper-bounds (CL-LB & CL-UB): in CL-LB data is presented incrementally without revisits
or employing any strategy for continual updates, expecting full catastrophic forgetting in a standard
MADE model (with only 1 client). CL-UB represents the best-case continual learning scenario, where
data is introduced incrementally but accumulated over time, hence without incurring any forgetting.
4) Continual Federated lower- and upper-bounds (FedCL-LB & FedCL-UB): Combines continual
and federated scenarios, where the upper-bound again accumulates all data individually per MADE
client and the lower-bound swaps out all data without. Again, in the spirit of bounds, the best model
of section 3.2 is used. 5) FedWeIT-MADE and CONFEDMADE in CFL: full continual federated
learning (without data revisits) and mitigated forgetting through a) a naive combination of FedWeIT
and MADE, without insights of section 3.2 and naive application of eq. 2, b) FedWeIT-MADE∗, now
including the ablation insights of section 3.2 to highlight their importance, c) our full CONFED-
8

C1 : [6]
C2 : [5]
C3 : [3]
C4 : [1]
C1 : [1]
C2 : [3]
C3 : [5]
C4 : [6]
0.19 0.25 0.36 0.56
0.18 0.28 0.57 0.36
0.22
0.6
0.28 0.26
0.52 0.34 0.28 0.31
0.2
0.3
0.4
0.5
0.6
0.7
0.8
C1 : [6, 8, 9]
C2 : [4, 5, 9]
C3 : [3, 5, 9]
C4 : [1, 4, 7]
C1 : [5, 8, 9]
C2 : [0, 1, 3]
C3 : [6, 7, 8]
C4 : [0, 1, 4]
0.54 0.36 0.56 0.21
0.43 0.39 0.57 0.39
0.67 0.41 0.29 0.36
0.41 0.49 0.35 0.55
0.2
0.3
0.4
0.5
0.6
0.7
0.8
Figure 4: Heatmaps for values of α (range 0 to 1) to highlight inter-
client knowledge transfer when other clients have observed the same
tasks (left) or have partial overlap (right). Two tasks for individual
clients are denoted in brackets on x and y-axes respectively.
Table 3: Sparsification/Performance trade-
off to reduce communication through
CONFEDMADE’s M W and increased
cut-off for FedWeIT mc. Whereas mc
by itself lightly improves communication
at minor performance expense, CONFED-
MADE lowers communication by almost
50% at virtually no drop in NLL.
Comm. strategy
% of Bc
NLL
w/o M W & mc > 0.1
88.6
85.89 ± 0.67
w/ M W & mc > 0.1
44.5
87.12 ± 0.83
w/ M W & mc > 0.2
39.4
88.52 ± 0.44
w/ M W & mc > 0.3
34.7
90.08 ± 0.57
w/ M W & mc > 0.4
30.0
91.12 ± 0.89
-MADE approach, based on proposed eq. 6. We typically assume 5 clients and clients’ participation
in each communication round in all federated (continual) scenarios. For simplicity, each consecutive
task is trained for 50 communication rounds, where each round is equivalent to one epoch.
4.2
Efficiently mitigating forgetting with CONFEDMADE across three data scenarios
We present the final averaged log-likelihoods and forgetting values for all of our three sketched data
sequences in experimental table 2. Starting with the full data learning scenarios, we can see that a
single client (offline) is only marginally better than the federated and continual learning upper-bounds.
Without constraints imposed on data seen by clients, it is understandable that these set the benchmark
on achievable results with a MADE architecture.
We continue to notice the non-surprising opposite trend with MADE in incremental and federated
learning scenarios in their lower-bounds (CL-UB and FedCL-UB), where previously seen data
samples are no longer continuously repeated with newly encountered data. Here, the frameworks
naturally succumb to large catastrophic forgetting, as observed in the NLL-losses deteriorating to
115.31 and 126.12 compared to 74.47 and 73.32 respectively. The naive FedWeIt-MADE already
shows significant signs of improvement in the performance, highlighting earlier intuition that masked
autoencoders are effective together with parameter decomposition in CFL scenarios. Most importantly,
however, upon enforcing the strategies discussed in section 3.3 we see an additional improvement in
the catastrophic forgetting as conceptualized in FedWeIT-MADE∗. With our full CONFEDMADE,
pertaining to the modifications in the sparsity-inducing regularizer, the impact of the knowledge base
is further boosted and performance over its two counterparts improved by roughly 6.0 and 10.0 nats.
To further nuance these statements, figure 3 shows the evolution of a decomposed loss after each task,
separately quantifying average performance, forgetting the first task, and encoding new knowledge.
We can observe that FedMADE forgets more of the learned representations of task 0 as we progress to
learn. Although CONFEDMADE is yet to reach the full upper bounds, it certainly reduces the gap to
what can be achieved, while widening the gap with lower bounds with a big difference of up to 20.00
in forgetting. Overall the choices in section 3 (in the initial FedMADE) and the CONFEDMADE
loss both meaningfully and significantly improve upon a FedWeIT baseline.
CONFEDMADE attends to overlapping knowledge of other clients: To illustrate the effect of
the inter-client transfer of learned representations, we plot the ’attention’ values α at the end of two
tasks (one task on each axis) in the form of heatmaps in figure 4. In the figure’s left panel, the tasks
are set up in a way that there is a complete overlap of data subsets, whereas the right panel contains
partial overlap among the clients. As such, we can investigate the strength of a CONFEDMADE
client to decide when the shared knowledge across the network can aid its own training. Overall, we
can observe higher values of α (0.60 and 0.67) in the respective plots when there is overlap in the data
subsets and thus later mitigated forgetting, and values as low as 0.19 or 0.21 when there is no overlap.
CONFEDMADE reduces the cost of communication by actively masking model parameters: As
base parameters Bc largely account for the communication cost of CONFEDMADE (Ac is generally
small), we investigate the sparsification/performance trade-off by ablating the auto-regressive MADE
mask (M W ) and modulating the cut-off value for the FedWeIT mask (mc) on MNIST in table 3.
Intuitively, we first quantify how much sparsification we can achieve at which cost through our
inclusion of the auto-regressive masking. Here, we observe that we gain a substantial, almost half,
9

reduction in the communicated amount of Bc at virtually no performance drop. In contrast, setting
higher cut-offs on the FedWeIT mask (0-1 range) , results in only a small further reduction of
communication cost, with similar minor drops in performance. Despite this advantage seeming
almost negligible in relation to the major gains obtained through MADE in CONFEDMADE, we can
nevertheless observe that both masking strategies together are symbiotic.
5
Conclusion
We have shown that masked autoencoders are effective continual federated learners by synergizing
the benefits of auto-regressive masking with FedWeIT’s parameter decomposition framework. Em-
pirical evaluations on a range of image to numerical data have demonstrated that our introduced
CONFEDMADE approach mitigates forgetting while lowering communication costs. However, our
investigation has presently been limited to MLPs, to highlight the framework’s key elements. One
natural next step is thus to scale to large-scale transformer architectures, where recent work has
corroborated masked autoencoding as an effective strategy on a token basis He et al. (2022). The
respective approach can seamlessly be integrated with CONFEDMADE, allowing future investigation
into clients that may additionally observe not only different distributions but also multiple modalities.
Acknowledgements
This work was supported by the project “safeFBDC - Financial Big Data Cluster (FKZ:
01MK21002K)”, funded by the German Federal Ministry for Economics Affairs and Energy as
part of the GAIA-x initiative. It benefited from the Hessian Ministry of Higher Education, Research,
Science and the Arts (HMWK; project “The Third Wave of AI”), the Federal Ministry of Educa-
tion and Research (BMBF) and the State of Hesse Project collaborative center “High-Performance
Computing for Computational Engineering Sciences (NHR4CES)” as part of the NHR Program.
References
Alessandro Achille, Tom Eccles, Loic Matthey, Christopher P. Burgess, Nick Watters, Alexander
Lerchner, and Irina Higgins. Life-Long Disentangled Representation Learning with Cross-Domain
Latent Homologies. Neural Information Processing Systems (NeurIPS), 2018.
Linda Argote and Ella Miron-Spektor. Organizational learning: From experience to knowledge. New
Perspectives in Organization Science, 2011.
Dana H. Ballard. Modular learning in neural networks. In AAAI Conference on Artificial Intelligence,
1987.
Zhiyuan Chen and Bing Liu. Lifelong machine learning. Synthesis Lectures on Artificial Intelligence
and Machine Learning, 2018.
Sijie Cheng, Jingwen Wu, Yanghua Xiao, Yang Liu, and Yang Liu. Fedgems: Federated learning of
larger server models via selective knowledge fusion. arXiv preprint arXiv:2110.11027, 2021.
Jichan Chung, Kangwook Lee, and Kannan Ramchandran. Federated unsupervised clustering with
generative models. International Workshop on Trustable, Verifiable and Auditable Federated
Learning in Conjunction with AAAI 2022, 2022.
Gregory Cohen, Saeed Afshar, Jonathan Tapson, and André van Schaik. Emnist: an extension of
mnist to handwritten letters. arXiv preprint arXiv:1702.05373, 2017.
Marcos F. Criado, Fernando E. Casado, Roberto Iglesias, Carlos V. Regueiro b, and Senen Barro. Non-
iid data and continual learning processes in federated learning: A long road ahead. Information
Fusion, 2022.
Li Deng. The mnist database of handwritten digit images for machine learning research. IEEE Signal
Processing Magazine, 2012.
Dheeru Dua and Casey Graff. UCI machine learning repository. "University of California, Irvine,
School of Information and Computer Sciences", 2017.
10

Sebastian Farquhar and Yarin Gal. Towards Robust Evaluations of Continual Learning. International
Conference on Machine Learning (ICML), Lifelong Learning: A Reinforcement Learning Approach
Workshop, 2018.
Robert M. French. Semi-distributed representations and catastrophic forgetting in connectionist
networks. Connection Science, 1992.
Mathieu Germain, Karol Gregor, Iain Murray, and Hugo Larochelle. Made: Masked autoencoder for
distribution estimation. Proceedings of the 32nd International Conference on Machine Learning,
07–09 Jul 2015.
Francesca Gino, Linda Argote, Ella Miron-Spektor, and Gergana Todorova. First, get your feet wet:
The effects of learning from direct and indirect experience on team creativity. Organizational
Behavior and Human Decision Processes, 2010.
Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative Adversarial Nets. Neural Information Processing
Systems (NeurIPS), 2014.
Raia Hadsell, Dushyant Rao, Andrei A Rusu, and Razvan Pascanu. Embracing change: Continual
learning in deep neural networks. Trends in cognitive sciences, 2020.
Sungwon Han, Sungwon Park, Fangzhao Wu, . Sundong Kim Kim, Chuhan Wu, Xing Xie, and Meey-
oung Cha. Fedx: Unsupervised federated learning with cross knowledge distillation. European
Conference on Computer Vision, 2022.
Tyler L. Hayes, Giri P. Krishnan, Maxim Bazhenov, Hava T. Siegelmann, Terrence J. Sejnowski, and
Christopher Kanan. Replay in deep learning: Current approaches and missing biological elements.
Neural Computation, 2021.
Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, and Ross Girshick. Masked
autoencoders are scalable vision learners.
In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition (CVPR), June 2022.
Peter Kairouz, H. Brendan McMahan McMahan, et al. Advances and open problems in federated
learning. Foundations and Trends in Machine Learning, 2021.
Ronald Kemker, Marc McClure, Angelina Abitino, Tyler Hayes, and Christopher Kanan. Measuring
catastrophic forgetting in neural networks. arXiv preprint arXiv:1708.02072, 2017.
Diederik P. Kingma and Max Welling. Auto-Encoding Variational Bayes. International Conference
on Learning Representations (ICLR), 2013.
James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A.
Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, Demis Hassabis,
Claudia Clopath, Dharshan Kumaran, and Raia Hadsell. Overcoming catastrophic forgetting in
neural networks. Proceedings of the National Academy of Sciences, 2017.
Dhireesha Kudithipudi, Mario Aguilar-Simon, Jonathan Babb, Maxim Bazhenov, et al. Biological
underpinnings for lifelong learning machines. Nature Machine Intelligence, 2022.
Brenden M Lake, Tomer D Ullman, Joshua B Tenenbaum, and Samuel J Gershman. Building
machines that learn and think like people. Behavioral and brain sciences, 2017.
Daliang Li and Junpu Wang. Fedmd: Heterogenous federated learning via model distillation. arXiv
preprint arXiv:1910.03581, 2019.
Qinbin Li, Bingsheng He, and Dawn Song. Model-contrastive federated learning. Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2021a.
Tian Li, Anit Kumar Sahu, Manzil Zaheer, Maziar Sanjabi, Ameet Talwalkar, and Virginia Smith.
Federated optimization in heterogeneous networks. arXiv preprint arXiv:1812.06127, 2020.
Xiaoxiao Li, Meirui Jiang, Xiaofei Zhang, Michael Kamp, and Qi Dou. Fedbn: Federated learning
on non-iid features via local batch normalization. arXiv preprint arXiv:2102.07623, 2021b.
11

David Lopez-Paz and Marc' Aurelio Ranzato. Gradient episodic memory for continual learning.
Advances in Neural Information Processing Systems, 2017.
Nan Lu, Zhao Wang, Xiaoxiao Li, Gang Niu, Qi Dou, and Masashi Sugiyama. Federated learning
from only unlabeled data with class-conditional-sharing clients. International Conference on
Learning Representations, 2022.
Ekdeep Singh Lubana, Chi Ian Tang, Fahim Kawsar, Robert P. Dick, and Akhil Mathur. Orchestra:
Unsupervised federated learning via globally consistent clustering. International Conference on
Machine Learning, 2022.
M. McCloskey and N. J. Cohen. Catastrophic Interference in Connectionist Networks : The Sequential
Learning Problem. Psychology of Learning and Motivation - Advances in Research and Theory,
1989.
H. Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas.
Communication-efficient learning of deep networks from decentralized data. Proceedings of the
20th International Conference on Artificial Intelligence and Statistics (AISTATS), 2017.
Martin Mundt, Steven Lang, Quentin Delfosse, and Kristian Kersting. Cleva-compass: A continual
learning evaluation assessment compass to promote research transparency and comparability. In
International Conference on Learning Representations, 2022a.
Martin Mundt, Iuliia Pliushch, Sagnik Majumder, Yongwon Hong, and Visvanathan Ramesh. Unified
probabilistic deep continual learning through generative replay and open set recognition. Journal
of Imaging, 2022b.
Martin Mundt, Yongwon Hong, Iuliia Pliushch, and Visvanathan Ramesh. A wholistic view of
continual learning with deep neural networks: Forgotten lessons and the bridge to active and open
world learning. Neural Networks, 2023.
Tae Jin Park, Kenichi Kumatani, and Dimitrios Dimitriadis. Tackling dynamics in federated incre-
mental learning with variational embedding rehearsal. arXiv preprint arXiv:2110.09695, 2021.
Jason Ramapuram, Magda Gregorova, and Alexandros Kalousis. Lifelong generative modeling.
Neurocomputing, 2020.
Dushyant Rao, Francesco Visin, Andrei Rusu, Razvan Pascanu, Yee Whye Teh, and Raia Hadsell.
Continual unsupervised representation learning. Advances in Neural Information Processing
Systems, 2019.
Hanul Shin, Jung K. Lee, Jaehong J. Kim, and Jiwon Kim. Continual Learning with Deep Generative
Replay. Neural Information Processing Systems (NeurIPS), 2017.
Karan Singhal, Hakim Sidahmed, Zachary Garrett, Shanshan Wu, John Rush, and Sushant Prakash.
Federated reconstruction: Partially local federated learning. Advances in Neural Information
Processing Systems, 2021.
Benigno Uria, Marc-Alexandre Côté, Karol Gregor, Iain Murray, and Hugo Larochelle. Neural
autoregressive distribution estimation. arXiv preprint arXiv:1605.02226, 2016.
Anastasiia Usmanova, Francois Portet, Philippe Lalanda, and German Vega. A distillation-based
approach integrating continual learning and federated learning for pervasive services. Third
Workshop on Continual and Multimodal Learning for Internet of Things at IJCAI, 2021.
Bram van Berlo, Aaqib Saeed, and Tanir Ozcelebi. Towards federated unsupervised representation
learning. EdgeSys 2020: Proceedings of the Third ACM International Workshop on Edge Systems,
Analytics and Networking, 2020.
Daniel M. Wegner. Transactive memory: A contemporary analysis of the group mind. Theories of
Group Behavior, Springer Series in Social Psychology, 1987.
Daniel M. Wegner, Toni Giuliano, and Paula T. Hertel. Cognitive interdependence in close rela-
tionships. Compatible and Incompatible Relationships, Springer Series in Social Psychology,
1985.
12

Jaehong Yoon, Eunho Yang, Jeongtae Lee, and Sung Ju Hwang. Lifelong learning with dynamically
expandable networks. arXiv preprint arXiv:1708.01547, 2018.
Jaehong Yoon, Wonyong Jeong, Giwoong Lee, Eunho Yang, and Sung Ju Hwang. Federated continual
learning with weighted inter-client transfer. Proceedings of the 38th International Conference on
Machine Learning, 18–24 Jul 2021.
Fengda Zhang, Kun Kuang, Zhaoyang You, Tao Shen, Jun Xiao, Yin Zhang, Chao Wu, Yuet-
ing Zhuang, and Xiaolin Li. Federated unsupervised representation learning. arXiv preprint
arXiv:2010.08982, 2020.
13

APPENDIX
In the appendix, we complement the main body with Training setup details and hyperparameters in
part A and further empirical visualization in part B.
A
Training Conditions and Hyperparameters
A.1
Training Configuration:
To evaluate our various experimental setups, we have used NVIDIA TESLA V100 - SXM3. From
the architectural point of view, a client model comprises of multi-layer perceptrons (MLP) with
varied hidden layer capacities depending on the complexity of the task to be learned. To put a
constraint on the the model complexity, we have largely set the depth of the hidden layers between
[350, 500] for images and [90, 110] for numerical data. For the sake of simplicity, the number of
clients typically ranges from 3 to 5 for our federated setups (FedCL-LB & -UB), whereas only a
single client is used for continual setups (CL-LB & -UB). The duration of the experiments span from
several hours to a max of 24 hours. We optimize our decomposed MADE client parameters using
Adam or AdamW optimizers with the learning rate starting at 0.001 alongside a weight decay of
0.0001. We collaboratively learn individual task for 50 rounds of communication where each round
resonates to only one epoch. After extensive experimental evaluation, We have opted to use 0.0001
and 100 for the hyperparameters λ1, λ2 respectively.
Code is available at: https://github.com/ml-research/CONFEDMADE
A.2
Task set Formulation
Table 4 provides the detailed description of the dataset as used in the main body. Recall, it is implied
that the offline scenario uses the entire dataset, whereas continual lower and upper-bounds makes use
of the respectively sketched tasks. The reader is reminded that the sequences of tasks are presented in
isolation for the lower bound and concatenated (accumulated) for the upper bound.
A.3
Evaluation Measures
In our main body’s experiments, we have considered two sets of evaluation measures to compare our
techniques. The first is based on the decomposition of the loss to measure the old and the new task
Kemker et al. (2017). The second is based on the notion of forgetting Lopez-Paz & Ranzato (2017).
Average per-task Negative log-likelihood loss: We measure the reconstruction loss at the end
of the task t averaged over all the previous tasks. Nt = 1/t Pt
i=1 nt,i where nt,i is the Negative
log-likelihood (NLL) loss of task i after the completion of t.
Base Task loss: With this measure, we measure the ability of our frameworks to retain the information
learned during the first task set at a future time step. Base task loss can be defined as Bt = nt,0
where nt,0 is the Negative log-likelihood of task 0 after the completion of task t.
New Task loss: We also measure the model’s capability to genaralize on newly seen data samples. The
New task loss for a client model can be defined as Nt = nt,t where ntt is the Negative log-likelihood
for task t after the completion of task t.
Average forgetting: We measure the forgetting in terms of the difference in the negative log-
likelihood loss for the task i after the completion of task t and its base task loss. For T tasks, Average
forgetting can be defined as Ft = 1/(t −1) Pt−1
i=1 maxtϵ1..T −1(0, nt,i −nT,i) where nt,i is the
negative log-likelihood for task i after the completion of task t.
14

Table 4: Details of the various dataset used in this work. The datasets range from MNIST, EMNIST to numerical
datasets such as Adult, Connect4, etc. We have also stated total no of distinct subsets in each dataset (No.
Classes), Sequence of tasks created (No. Tasks), number of distinct data subsets used per task (Classes per task).
The train-validation-test splits can be visualized in the following Train, Validation, and Test columns.
DATASETS
Dataset
No. Tasks
No. Classes
Classes per task
Train
Validation
Test
MNIST Deng (2012)
5
10
3
15620
3121
3120
MNIST Deng (2012)
5
10
1
5626
1126
1127
EMNIST Cohen et al. (2017)
5
10
13
42817
8564
8563
Adult Dua & Graff (2017)
4
1
1
23257
4653
4651
Connect4 Dua & Graff (2017)
4
1
1
48255
9651
9651
Tretail Dua & Graff (2017)
4
1
1
20990
4199
4198
RCV1 Dua & Graff (2017)
4
1
1
34285
6858
6857
0
1
2
3
# Number of tasks
40
50
60
70
80
90
Negative log-likelihood
Binary Averaged Task Loss
0
1
2
3
# Number of tasks
30
40
50
60
70
80
90
Binary Base Task Loss
0
1
2
3
# Number of tasks
35
40
45
50
55
60
Binary New Task Loss
Type
CONFEDMADE
FEDWeIT-MADE
FEDWeIT-MADE*
FEDCL-LB
FEDCL-UB
Figure 5: Decomposed negative log-likelihood in FCL for binary datasets to showcase: (left) average of tasks
seen so far, (center) the “base” loss, i.e the value for only initial task in evolution over time to assess forgetting,
(right) the “new” loss, i.e. the value for only the newest task to gauge encoding of new knowledge. Lower values
are better.
A.4
Empirical Visualization
Recall in the main body, we have visualized the effectiveness of CONFEDMADE against other
approaches for Sequential MNIST dataset Deng (2012). To further validate that we can scale
our proposed approach against different data distributions, we produce the three similar plots for
Binary datasets based on Average task loss, Base task loss and New task loss. If we refer to the
first plot on the left (Average task loss), CONFEDMADE can be seen to retain previously learned
representations much more efficiently compared to its other counterparts under similar continual
lower-bound scenario. Thereby, we can establish CONFEDMADE as effective continual federated
learner, pertaining to our continual and auto-regressive masking strategies.
15

