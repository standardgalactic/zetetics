Birth of a Transformer: A Memory Viewpoint
Alberto Bietti, Vivien Cabannes, Diane Bouchacourt, Hervé Jegou, Léon Bottou
Meta AI
June 2, 2023
Abstract
Large language models based on transformers have achieved great empirical successes. However, as
they are deployed more widely, there is a growing need to better understand their internal mechanisms in
order to make them more reliable. These models appear to store vast amounts of knowledge from their
training data, and to adapt quickly to new information provided in their context or prompt. We study
how transformers balance these two types of knowledge by considering a synthetic setup where tokens are
generated from either global or context-specific bigram distributions. By a careful empirical analysis of
the training process on a simplified two-layer transformer, we illustrate the fast learning of global bigrams
and the slower development of an “induction head” mechanism for the in-context bigrams. We highlight
the role of weight matrices as associative memories, provide theoretical insights on how gradients enable
their learning during training, and study the role of data-distributional properties.
1
Introduction
As large language models (LLMs) are growing in usage and deployment, it is increasingly important to open
the black box and understand how they work. A better understanding can help with interpretability of how
these models make decisions, and will be crucial to improve these models and mitigate their failure cases,
such as hallucinations or reasoning errors.
An important ingredient in the success of recent LLMs is their ability to learn and reason from information
present in their context [5]. These “in-context” learning capabilities are often attributed to the transformer
architecture [46], in particular its self-attention blocks, which are able to carefully select parts of the input
sequence in order to infer plausible next tokens. Additionally, predictions may require “global” knowledge,
such as syntactic rules or general facts, which may not appear in the context and thus needs to be stored in
the model.
In order to better understand how transformers develop these capabilities during training, we introduce
a synthetic dataset that exhibits both aspects. It consists of sequences generated from a bigram language
model, but where some of the bigrams are specific to each sequence. Then, the model needs to rely on
in-context learning for good prediction on the sequence-specific bigrams, while the global bigrams can be
guessed from global statistics conditioned on the current token. While one-layer transformers fail to reliably
predict the in-context bigrams, we find that two-layer transformers succeed by developing an induction head
mechanism [14, 36], namely a “circuit” of two attention heads that allows the transformer to predict b from a
context [· · · , a, b, · · · , a], and which appears to be ubiquitous in transformer language models [36, 48].
In order to obtain a fine-grained understanding of how this in-context mechanism emerges during training,
we further simplify the two-layer architecture by freezing some of the layers at random initialization, including
embeddings and value matrices. This focuses our study on attention and feed-forward mechanisms, while
avoiding the difficulty of learning representations, which may require complex nonlinear dynamics [13, 29, 40].
This simplification also allows us to introduce a natural model for individual weight matrices as associative
memories, which store input-output or key-value pairs of embeddings through their outer products. Random
high-dimensional embeddings are particularly well-suited to this viewpoint thanks to their near-orthogonality.
We provide a detailed empirical study of the training dynamics, by measuring how quickly each weight matrix
1
arXiv:2306.00802v1  [stat.ML]  1 Jun 2023

learns to behave as the desired associative memory, studying how this is affected by data-distributional
properties, and investigate the order in which layers are learned: the model first finds the right output
associations from the current token and from uniform attention patterns, then the attention heads learn to
focus on the correct key-value pairs. We then present theoretical insights on this top-down learning process
through population gradient dynamics. Despite its simplicity, our setup already provides useful insights on
the structure and evolution of transformer language models throughout training, paving the way for a better
understanding of LLMs.
In summary, we make the following contributions:
• We introduce a new synthetic setup to study global vs in-context learning: sequences follow bigram
language models, where some bigrams change across sequences and others do not.
• We view the transformer’s weight matrices as associative memories that learn to store specific pairs of
embeddings, and use this to derive a simplified but more interpretable model for our task.
• We empirically study the training dynamics with careful probing: global bigrams are learned first, then
the induction head is formed by learning appropriate memories in a top-down fashion.
• We give theoretical insights on training dynamics, showing how a few top-down gradient steps on the
population loss can recover the desired associative memories by finding signal in noisy inputs.
Related work.
After the success of transformer language models for in-context learning was found [5], several
works have studied how in-context learning may arise in various contexts [6, 34, 39, 43, 51]. Multiple recent
papers have introduced synthetic tasks in order to better understand and interpret transformers [7, 29, 35, 53].
Several works have attempted to understand internal mechanisms in transformers that are responsible for
certain behaviors, an area known as “mechanistic interpretability” [14, 15, 31, 35, 36, 48]. Memory and neural
networks have a long history of connections [4, 16, 18, 23, 24, 26, 30, 45, 49, 50]. The associative memories
we consider bear similarity to Willshaw networks [50], though we use continuous input/outputs. The reader
may also be interested in Fast Weight programmers [41, 42]. The use of random vectors for storing memories
is related to [20]. Our approach to probing based on memory recall is related to techniques in [11, 15], though
motivated differently. [12, 28, 32] study statistical and approximation properties of transformers, highlighting
benefits of sparse attention patterns, but do not consider training dynamics. [22, 27, 44] provide theoretical
analyses of learning dynamics in transformers and other attention models, but consider different data setups
and focus on single-layer architectures, while we focus on two-layer models and take a different viewpoint
based on associative memories.
2
Background
This section provides background on transformer architectures and induction head mechanisms.
Transformer architecture.
Transformers [46] operate on sequences of embeddings by alternating self-
attention operations and token-wise feed-forward layers. We focus on decoder-only, auto-regressive architec-
tures with a causal attention mask, which are commonly used in large language models trained for next-token
prediction [5, 9, 37, 38]. We ignore normalization layers in order to simplify the architecture, since its stability
benefits are not as crucial in the small models we consider. Given an input sequence of tokens z1:T ∈[N]T of
length T, where N is the vocabulary size, the transformer operates as follows:
• Token embeddings: each discrete token is mapped to a d-dimensional embedding via an embedding
map WE ∈Rd×N. We will denote the embeddings of tokens zt by xt := wE(zt), where wE(j) is the j-th
column of WE.
• Positional embeddings: the positional embeddings pt = wP (t) ∈Rd are added to each token
embedding depending on its position in the sequence, leading to the following input embeddings:
xt := xt + pt = wE(zt) + wP (t).
(1)
2

Layer 0
Sequence
Layer 1
Layer 2
pt−1
wE(a)
pt
wE(b)
pT −1
wE(a)
a
b
a
b
[· · · ]
∗
wE(a)
w1(a)
wE(b)
∗
wE(a)
∗
∗
∗
∗
wU(b)
wE(a)
Attn1: P
s ps−1p⊤
s
W 1
OW 1
V
Residual
Attn2: P
k w1(k)wE(k)⊤
W 2
OW 2
V : P
k wU(k)wE(k)⊤
Prediction
Figure 1: Induction head mechanism. Induction heads are a two-layer mechanism that can predict b from
a context [. . . , a, b, . . . , a]. The first layer is a previous token head, which attends to the previous token based
on positional embeddings (pt →pt−1) and copies it after a remapping (wE(a) →w1(a) := W 1
OW 1
V wE(a)).
The second layer is the induction head, which attends based on the output of the previous token head
(wE(a) →w1(a)) and outputs the attended token, remapped to output embeddings (wE(b) →wU(b)). Boxes
in the diagram represent sets of embeddings in superposition on each token’s residual stream, and attention
and output associations are shown with the associative memory viewpoint presented in Section 4.
• Attention blocks: given an input sequence x1:T ∈Rd×T of embeddings, the causal attention block
computes, for WK, WQ, WV , WO ∈Rd×d (key, query, value, output), and for each t,
x′
t := WOWV x1:tσ(d−1/2x⊤
1:tW ⊤
KWQxt) ∈Rd,
(2)
where σ takes the softmax of its elements, leading to an attention of the “values” WV xt with weights pro-
portional to exp(d−1/2(WKxs)⊤(WQxt)). Note that the attention operation usually considers multiple
“heads” that each projects the input to a lower dimension. Here we stick to a single head for simplicity,
since it will be sufficient for our purposes. Rewriting (2) on each t as x′
1:T = A(x1:T ; WK, WQ, WV , WO),
the ℓ-th layer of the transformer applies attention with layer-specific parameters along with a residual
connection as follows:1
x1:T := x1:T + A(x1:T ; W ℓ
K, W ℓ
Q, W ℓ
V , W ℓ
O)
• Feed-forward blocks: feed-forward blocks operate on individual token embeddings after each attention
block, typically by applying a one-hidden-layer MLP to each token, denoted F(·; WF ), with a residual
connection: at layer ℓ, we have
xt := xt + F(xt; WF ).
Our simplified setup will linear feed-forward layers: F(xt; WF ) = WF xt.
• Unembedding: After the last transformer layer, the embeddings are mapped back to the vocabulary
space RN through a linear “unembedding” layer WU = [wU(1), . . . , wU(N)]⊤∈RN×d, where we refer
to the wU(j) as “output embeddings”. The output of this layer is then fed into a cross-entropy loss for
predicting of zt+1 from each xt.
We will sometimes refer to the representations xt for a given token t throughout layers as its residual
stream [14], since they consist of sums of embeddings and layer outputs due to residual connections.
Induction head mechanism.
Induction heads [14, 36] are a particular type of mechanism (or “circuit”) in
transformers that allows basic in-context prediction of the form [· · · , a, b, · · · , a] →b. These were found to be
1We omit layer indices for simplicity of notation, and use the assignment operator := instead.
3

ubiquitous in transformer language models, playing a key role in enabling various forms of in-context learning.
The basic mechanism consist of two attention heads in separate layers (see Figure 1 for an illustration): (i)
the first is a previous token head which attends to the previous token using positional information and copies
its embedding to the next token; (ii) the second is the induction head itself, which attends using the output of
the previous token head, and outputs the original token. Our work focuses on this basic copy mechanism, but
we note that richer behaviors are possible, particularly when combining multiple such mechanisms (e.g., [48]).
3
Synthetic Setup
In this section, we introduce our synthetic data setup, which allows us to carefully study how the induction
head mechanism develops during training, and how transformers learn to use information from the context vs
simple associations from the training data.
Bigram data model.
Our model for sequences consists of a generic bigram language model (i.e., Markov
chain), but where the transitions for a few trigger tokens denoted qk are modified in each sequence to always be
followed by some output tokens ok. Let K be the number of trigger tokens, and fix the following distributions
over the vocabulary [N]: πb(·|i), πu, πo(·|i) and πq, for i ∈[N]. πb(·|i) are the global bigram conditionals, πu
the global unigram distribution, while πo is used to sample output tokens at each sequence. The triggers
are either fixed to some predefined set of tokens Q, or sampled from πq. Each sequence zn
1:T is generated
as follows:
• (optional) Sample q1, . . . , qK ∼πq, i.i.d. without replacement (random triggers)
• Sample ok ∼πo(·|qk), i.i.d. with replacement.
• Sample zn
1 ∼πu and zn
t |zn
t−1 ∼pn(·|zn
t−1) for t = 2, . . . , T, where
pn(j|i) =
(
πb(j|i),
if i /∈{qk}k
1{j = ok},
if i = qk.
Experimental setup and initial experiment.
Our experiments take πu and πb to be unigram and
bigram character-level distributions estimated from the tiny Shakespeare dataset, with vocabulary size N = 65.
We generally sample triggers from πq = πu or fix them to the K most frequent tokens. We sample uniform
outputs ok in most cases, but also experiment with πo = πb in Section 5.
As a preliminary experiment, we train a two-layer vanilla transformer with single-head attention layers
and MLP feed-forward layers, following the training setup described in Section 5. On our synthetic data,
with fixed (resp. random) triggers and uniform outputs, the model achieves over 99% accuracy (resp. 95%)
on output tokens after the first occurrence, versus around 55% for one layer. We visualize attention maps
on test sequences in Figure 2, which shows that the model has learned an induction head mechanism. The
sequence in the middle figure has (qk, ok) ∈{(a, b), (t, s)}. For fixed triggers, the induction head is only
active for the triggers used in training, which suggests the presence of a “memory” in the attention layer. For
random triggers, it is active on every repeated token, so that the model then needs to disambiguate between
in-context and global predictions. For instance, the model may choose to use the retrieved token when it is
unlikely to be sampled from the global bigram distribution, something which we found to often be the case
in practice.
4
The Associative Memory Viewpoint
In this section, we present our associative memory view on transformers: with nearly orthogonal embeddings,
the weight matrices behave as associative memories which store pairs of embeddings as a weighted sum of
their outer products. We then introduce a simplified transformer model with fixed random embeddings that
will yield a precise understanding of learning dynamics using this viewpoint.
4

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
r s a b t s L a b t s L , a b
t h b n t L & C L & C a b t h
Figure 2: Induction head behavior in attention maps observed on a 2-layer transformer trained on
two variants of our synthetic dataset. Each row shows the attention pattern for predicting the next token.
(left) The first layer head always attends to the previous token. (center) For fixed triggers Q = {a, t}, the
second layer head mainly attends to tokens following such triggers. (right) For random triggers, the induction
head mechanism is active for any repeated token (here the only trigger is L). Red and green boxes highlight
tokens following previous occurrences of the query, with red boxes corresponding to “correct” output tokens ok
following trigger tokens qk.
4.1
Weight matrices as associative memories
While intermediate representations in the transformer consist of high-dimensional vectors in residual streams,
they are often “collapsed” down to scalar measurements by testing against other representations, using
operations of the form v⊤
j Wui for some matrix W. For instance, ui and vj could be key and query vectors
in an attention head, or input and output embeddings for predicting the next token. If (ui)i and (vj)j
are orthonormal (or nearly-orthonormal) sets of embeddings, a natural way to store desired input-output
associations i, j is through the following associative memory:
W =
X
i,j
αijvju⊤
i ,
(3)
so that the scores v⊤
j Wui ≈αij may be used to assess the relevance of the (i, j) pair, e.g., as part of a
softmax operation in attention or next token prediction.
Random embeddings.
A simple way to ensure that embeddings (ui)i and (vj)j are nearly-orthonormal
is to set them to be random high-dimensional vectors, such as Gaussian vectors with variance 1/d in d
dimensions. Indeed, these are known to satisfy [20, 47]
u⊤
i ui ≈1
and
u⊤
i uj ≈O
 1
√
d

,
so that (3) is a reasonable way to define an associative memory, without requiring an explicit activation
function as employed in end-to-end memory networks [45]. We may also easily create a “remapping” of
an existing embedding ui by multiplying it by a random matrix W0 ∈Rd×d with Gaussian entries with
variance 1/d, which is commonly used for initializing neural network parameters.
The new remapped
embedding W0ui is near-unit norm, and is near-orthogonal to ui in addition to the other uj. Note that this
fact implies that attention scores at initialization are near-uniform. See Appendix A for more details.
Learning associative memories.
We now show that learning associations of input-output embeddings
via gradient descent leads to a weighted associative memory of a form similar to (3).
Lemma 1 (Gradients and associative memories). Let p be a data distribution over input-output tokens, and
consider the following loss, where the input and output embeddings WE and WU are fixed:
L(W) = E(z,y)∼p[ℓ(y, WUWwE(z))],
(4)
5

with ℓthe cross-entropy loss. The gradients of the population loss L then take the form
∇W L(W) =
N
X
k=1
Ez[(ˆpW (y = k|z) −p(y = k|z))wU(k)wE(z)⊤],
(5)
where ˆpW (y=k|x) = σ(WUWwE(z))k are the model’s predicted probabilities. Running gradient descent (with
or without weight decay) from initialization W0 then leads to estimates of the following form, for some α0
and αij that vary with the number of iterations:
ˆW = α0W0 +
X
i,j
αijwU(j)wE(i)⊤.
(6)
Note that (4) is a convex problem in W, thus with appropriate step-size and large enough number of steps
(with no weight decay) we can expect gradient descent to be close to the global minimum. At the optimum, if
the embeddings are nearly orthogonal, then (5) implies ˆpW (y = k|z) ≈p(y = k|z). We remark that if W0 is a
Gaussian random matrix, as if often the case for neural network layers, the first term in (6) plays a minor
role: testing W0 against an input-output pair (i, j) with αij ̸= 0 will concentrate around zero when d is large,
while the (i, j) term in the sum will concentrate around αij.
Handling superposition.
In Lemma 1, we assumed that inputs to the matrix W are embeddings of a
single token. Yet, in transformer models, the inputs to weight matrices are often sums, or superpositions
of embeddings. For instance, the initial representations of each token are sums of token and positional
embeddings, and representations at later layers are sums of the outputs of each previous block, due to residual
connections. Outputs of attention layers are also weighted sums of potentially many embeddings, at least
initially when attention patterns are spread out. By linearity, associative memories of the form (6) simply
operate individually on each embedding of a superposition, and return a new superposition (up to additional
noise due to near-orthogonality). In practice, we will see that learned memories tend to focus on a single
embedding and filter out the rest as noise when irrelevant (see also Section 6). We note that linearity can
also be limiting, since it makes it difficult to map sets to specific output embeddings: u{i,j} := ui + uj needs
to map to Wui + Wuj, and thus cannot map to a new embedding v{i,j}. Such mappings of sets thus require
non-linear associative memories, for instance by leveraging a sparse decoding of which elements are actually
present (e.g., using compressed sensing), or by using MLPs with non-linear activations [13].
4.2
A simplified two-layer transformer architecture
We consider a simpler two-layer transformer which is more interpretable with the memory viewpoint, and
will help us analyze learning dynamics both empirically and theoretically.
• We freeze input, output and positional embeddings (WE, WU, WP ) to their random initialization
throughout training. This brings us to the Gaussian random vector setup presented above.
• We fix W 1
Q = W 2
Q = Id, so that W 1
K and W 2
K play the role of both key and query matrices. This changes
the gradient dynamics, but simplifies the model by avoiding the redundancy in (2). The pre-softmax
attention scores then take the form x⊤
q W ℓ
Kxk, with xq (resp. xk) the query (resp. key) embeddings,
which now directly resembles an associative memory lookup.
• We freeze W 1
V , W 1
O, and W 2
V to random initialization. These play the role of remapping attended
tokens into new tokens, since for random W and large d, Wx is nearly orthogonal to x and to any other
random embeddings independent of x.
• We train W 2
O, since the outputs of the induction head need to be mapped back into appropriate output
embeddings in order to predict the output tokens ok correctly.
6

• We use a single linear feedforward layer after the second attention block, with weight matrix WF . This
is plausibly the layer responsible for learning the global bigram distributions.
We remark that while this model freezes some parameters at initialization, it is richer than a “lazy” or neural
tangent kernel approximation [8, 19, 21] since the model is still highly non-linear in its parameters and, as we
will see, induces rich non-linear learning dynamics.
Solving the bigram problem with associative memories.
We now show how the above architecture
can solve the synthetic bigram problem from Section 3 with well-chosen weight matrices. While this is only a
hypothetical model, we show in Section 5 that it is surprisingly faithful to the learned model.
Recall that due to residual connections, the inputs to the weight matrices typically consist of superpositions
of various embeddings including token embeddings, positional embeddings, or “remapped” versions thereof.
These may be viewed as sets, as illustrated in Figure 1, and associative memories can easily ignore certain
elements of the set, e.g., ignore token embeddings by only focusing on positional embeddings. The induction
head mechanism can be obtained by setting:
W 1
K =
T
X
t=2
ptp⊤
t−1,
W 2
K =
X
k∈Q
wE(k)(W 1
OW 1
V wE(k))⊤,
W 2
O =
N
X
k=1
wU(k)(W 2
V wE(k))⊤,
(7)
where Q is the set of triggers when they are fixed, or the support of πq when they are random. In words, the
first attention layer matches a token to the previous tokens using positional embeddings. The second layer
matches the trigger token to a remapping of itself by W 1
OW 1
V , and the output matches a remapping of the
input token by W 2
V to the corresponding output token. The global bigram statistics can be encoded in the
feed-forward layer as follows:
WF =
N
X
i=1
N
X
j=1
log πb(j|i)wU(j)wE(i)⊤.
(8)
The question remains of how the model could trade-off predictions from the induction head and from the
feed-forward layer, which are added together due to residual connections. With fixed triggers Q, we may
simply remove all i ∈Q from the summation in (8), so that the model exclusively relies on the attention head
for all triggers (indeed, the output of W 2
O is in the span of output embeddings, which are nearly orthogonal to
the row space of WF ). When the triggers can vary across different sequences, choosing between the induction
head and the feed-forward layer is more ambiguous as it depends on context, and WF may try to learn more
complex mappings that also use the outputs of W 2
O. In practice, we observe that the model often prefers the
induction head, unless its output agrees with one of the top predictions from the global bigram, in which case
it tends to prefer those.
5
Empirical Study
In this section, we present our empirical analysis of learning dynamics on the bigram data defined in Section 3,
for the simplified architecture defined in Section 4.2. See Appendix D for additional results.
Experimental setup.
We train our models using mini-batch SGD with momentum, where each batch
consists of 512 fresh sequences of length T = 256 sampled from our synthetic model. We use a fixed learning
rate and weight decay. Hyperparameters are given in Appendix D. Unless otherwise noted, we use d = 128,
random triggers with πq = πu and uniform output tokens. The reported accuracies and losses are computed
over each fresh batch before it is used for optimization, and are averaged over relevant tokens: “in-context
accuracy/loss” numbers only consider predictions of output tokens on triggers starting at the second occurrence
(the first is non-deterministic), while “global loss” refers to average loss on non-trigger tokens.
7

0
200
400
600
800
1000
0.0
0.2
0.4
0.6
0.8
1.0
in-context accuracy
freeze W2
O
0
200
400
600
800
1000
0.0
0.2
0.4
0.6
0.8
1.0
freeze W1
K and W2
K
0
200
400
600
800
1000
0.0
0.2
0.4
0.6
0.8
1.0
freeze W2
K
0
200
400
600
800
1000
0.0
0.2
0.4
0.6
0.8
1.0
freeze W1
K
0
200
400
600
800
1000
iteration
0.0
0.2
0.4
0.6
0.8
1.0
memory recall
0
200
400
600
800
1000
iteration
0.0
0.2
0.4
0.6
0.8
1.0
0
200
400
600
800
1000
iteration
0.0
0.2
0.4
0.6
0.8
1.0
0
200
400
600
800
1000
iteration
0.0
0.2
0.4
0.6
0.8
1.0
Wo2
Wk2
Wk1 (t<64)
Wk1
Figure 3: Learning the induction head alone: in-context accuracy (top) and recall probes
(bottom) with some layers frozen until iteration 300. The output matrix W 2
O can and must be learned before
the key-query matrices, but does not suffice for good accuracy. It is easier to learn W 2
K before W 1
K, and W 1
K
stores initial context positions (t < 64) much faster than late positions.
Memory recall probes.
In addition to loss and accuracy, we consider metrics to check whether individual
matrices have learned the desired associative memories: for a desired target memory W∗= P
(i,j)∈M vju⊤
i ,
the corresponding recall metric is computed from the empirical estimate ˆW as
R( ˆW, W∗) =
1
|M|
X
(i,j)∈M
1{arg max
j′
v⊤
j′ ˆWui = j}.
(9)
We use this for each matrix in (7) as target, and additionally test the previous token matrix W 1
K on smaller
time windows. For the final feed-forward layer, we measure the average KL divergence between the predicted
softmax distribution using only WF and the global bigram distribution πb:
dKL(WF , πb) := 1
N
N
X
k=1
dKL(σ(WUWF wE(k)), πb(·|k)).
(10)
Emergence of the induction head via top-down learning.
We begin our study by only training to
minimize the loss on trigger-output token predictions after their first occurrence. This should be predictable
with 100% accuracy using the two-layer induction head mechanism according to Section 4. We also remove
the feed-forward layer, in order to focus on the learning of attention matrices W 1
K, W 2
K and W 2
O in isolation.
Figure 3 studies the effect of freezing different layers until iteration 300 on the training dynamics. By
looking at memory recall probes, we see that training key-query matrices does not lead to any learning
unless W 2
O is learned first, and that W 2
O can learn the correct associations even when trained by itself
with key-value matrices at random initialization. Recall that the attention weights are essentially uniform
when WK are at random initialization, so that training W 2
O alone resembles a bag-of-words models that
aggregates representations throughout the sequence. While such a model has poor prediction accuracy, it is
nevertheless sufficient to recover the correct associations in W 2
O (a similar observation was made in [44] in a
different setup).
Then, these associations enable learning key-query matrices that focus the attention on relevant tokens,
by storing relevant key-query pairs in the form of associative memories, which eventually recovers the desired
induction head behavior and leads to near-perfect accuracy. The two rightmost plots suggest that the second
layer is learned before the first, in the sense that W 2
K is easier to learn when W 1
K is frozen compared to the
reverse, yet learning them together seems beneficial, possibly due to helpful feedback loops [1]. We also
8

0
100
200
300
400
500
iteration
0
1
2
3
4
5
loss
in-context vs global loss
K=1, fix q frequent
K=5, fix q rare
K=1, random q
K=5, random q
0
100
200
300
400
500
iteration
0.0
0.2
0.4
0.6
0.8
in-context accuracy
out of distribution
unif (train unif)
bigram (train unif)
unif (train bigram)
bigram (train bigram)
0
100
200
300
400
500
iteration
0.0
0.2
0.4
0.6
0.8
memory recall
Wo2 recall
Wk2 recall
Wk1 recall (t<64)
1.0
1.2
1.4
1.6
1.8
2.0
2.2
KL(WF,
b)
attention and feed-forward probes
Wf KL
Figure 4: Global vs in-context learning and data-distributional effects. (left) Loss on global (dashed)
vs in-context (solid) tokens throughout training, for fixed or random trigger tokens qk. The red curves fixes
the trigger q1 to the most frequent token, while the fixed triggers in blue curves are less common. (center)
In-context accuracy with different training and test distributions πo for output tokens. Uniform leads to better
generalization than global bigrams πb. (right) Probe metrics throughout training: W 2
O and WF eventually
compete and deviate from our natural estimates.
observe that W 1
K fits previous token associations for early positions much faster than later positions (purple
vs gray line). This is likely due to the fact that it should be enough for the previous token head to attend to
the first appearance of each trigger qk, which is typically early in the sequence, so that most of the gradient
will focus on early positions.
Overall, this provides a fine-grained understanding of the learning dynamics of induction heads. In
Section 6, we analyze how a few gradient steps in a top-down fashion may suffice to recover appropriate
associative memories in high dimension and with enough data. See Appendix D for additional experiments,
including on the role of dimensionality.
Global vs in-context learning.
Figure 4(left/right) shows that when training all layers jointly, the global
bigram statistics tend to be learned more quickly than the induction head, as seen from the quick drop in
loss and KL in early iterations. The W 2
O probe also seems to improve quickly initially, but only leads to mild
improvements to in-context predictions. The full learning of the in-context mechanism takes longer, likely
due to slower dynamics of the key-query matrices. We also observe a tension between W 2
O and WF later in
training, leading to slight degradations of our probe metrics. This may be due to the fact that the input
to WF now contains additional signal from the induction head which may be leveraged for better predictions,
in particular for disambiguation in the case of random triggers, so that our guess of memories in Section 4.2
may no longer be accurate.
Role of the data distribution.
We can see in Figure 4(left) that changes to the data distribution can
have a significant effect on the speed of learning the in-context mechanism. We observe that the following
may slow down in-context learning: (i) a smaller number of triggers K, (ii) using only rare fixed triggers,
and (iii) using random triggers instead of fixed triggers. By inspecting the individual memory probes (see
Figure 5 in Appendix D), we hypothesize that (i) and (ii) are due to slow learning of W 2
O, while (iii) is more
related to slow learning of key-query matrices. This is reasonable since (i-ii) reduce the number of overall
output tokens in the data, while (iii) increases the number of possible trigger tokens that should be stored
in W 2
K, thus increasing the data requirements in order to learn the full associative memory. We also show in
Figure 4(center) that changing the output token distribution to bigram distributions at training time reduces
the in-context accuracy when using out-of-distribution output tokens, while training on uniform outputs
performs well on both distributions. This highlights that using a more diverse training distribution can lead
to models with better generalization accuracy, with little additional training cost.
9

6
Theoretical Insights on Learning Dynamics
In this section, we provide theoretical insights on how gradients near initialization may allow the emergence
of induction heads, and how this behavior is affected by data-distributional properties.
Finding signal in noisy inputs.
In Lemma 1, we showed how gradient dynamics on a simple classification
task with fixed embeddings of the inputs and outputs lead to associative memories. We now show that
when inputs consist of superpositions of multiple embeddings, as is the case in the transformer residual
streams, gradients may learn associative memories that filter out irrelevant components of these superpositions,
focusing on useful signal instead.
Lemma 2 (Gradient associative memory with noisy inputs). Let p be a data distribution on (x, y) ∈Rd ×[N],
and consider the following classification problem, with fixed output embeddings WU:
L(W) = E(x,y)∼p[ℓ(y, WUWx)].
The gradients take the following form: denoting µk := E[x|y = k] and ˆµk := Ex[ ˆpW (k|x)
p(y=k) x],
∇W L(W) =
N
X
k=1
p(y = k)wU(k)(ˆµk −µk)⊤.
The key takeaway from this lemma is that with enough data (here infinite data), the associative memory
arising from gradients can learn to filter out noise from inputs, since it only depends on its expectations or
conditional expectations. In particular, µk can isolate relevant parts of x that are predictive of a label k, and
thus can lead to the right associations.
An illustrative example.
To gain more intuition about this result, consider the following example: we
would like to predict y from x = wE(y) + pt, where pt is a positional embedding at a random position t ∈[T],
which we would like to ignore. Further assume that y is uniformly distributed with p(y = k) = 1/N,
and consider the matrix obtained after one population gradient step with step-size η starting from an
initialization W0 = 0 (so that ˆpW0(k|x) = 1/N):
W1 = η
N
N
X
k=1
wU(k)(µk −¯µ)⊤,
with ¯µ = E[x]. We show in Appendix B that when d is large enough to ensure near-orthonormal embeddings,
we have
wU(k)⊤W1(wE(y) + pt) ≈η
N 1{k = y} + O
 1
N 2

,
so that for large enough N and T, we obtain a near-perfect classifier that ignores the positional embedding,
after just one gradient step (but a highly idealized one). Understanding how this translates to the finite
dimension and finite sample regime is an important theoretical question that we leave for future work. We
note that data models related to the above have been useful to study gradient dynamics of neural networks
on continuous data [2, 22, 25]. Using a single gradient step to learn representations has also been fruitful in
other contexts [3, 10].
Learning the induction head with gradients.
In Appendix B, we use Lemma 2 in a similar manner
to show how training W 2
O by itself at initialization, i.e., when the attention patterns are near-uniform, can
recover the desired associative memory. This is possible because when predicting an output token at later
occurrences of a trigger, the same output token is guaranteed to be present in the context, while other tokens
need not appear more relative to other sequences. See also Figure 8 in Appendix D for numerical experiments
verifying this for finite data and dimension. Once W 2
O has learned the correct associations, we show that the
10

gradient with respect to the key-value matrix W 2
K at zero initialization can leverage the correctness of W 2
O to
find the right associative memory that focuses the attention on correct triggers. Finally, by linearizing the
second-layer attention around W 2
K = 0, we show how gradients w.r.t. W 1
K may learn correct associations for
the previous token head.
7
Discussion
In this paper, we studied the question of how transformers develop in-context learning abilities, using a
simplified setup that allows a fine-grained understanding the model and its training dynamics. While our
model already captures rich phenomena at play in the bigram task we consider, more elaborate models are
likely needed to understand transformers trained on more complex tasks like language modeling. This includes
learning embeddings that are more adapted to the data and more structured (e.g., word embeddings [33, 27],
or grokking [29, 35]), factorized key-query and value-output matrices that may induce additional regularization
effects [17], and non-linear feedforward layers, which may provide richer associative memories between sets of
embeddings. Understanding how transformers leverage such aspects to learn in richer settings is an important
next step.
Acknowledgements
The authors thank Sainbayar Sukhbaatar and Shubham Toshniwal for helpful discussions.
References
[1] Z. Allen-Zhu and Y. Li. Backward feature correction: How deep learning performs deep learning. arXiv
preprint arXiv:2001.04413, 2020.
[2] Z. Allen-Zhu and Y. Li. Towards understanding ensemble, knowledge distillation and self-distillation in
deep learning. In Proceedings of the International Conference on Learning Representations (ICLR), 2023.
[3] J. Ba, M. A. Erdogdu, T. Suzuki, Z. Wang, D. Wu, and G. Yang. High-dimensional asymptotics of
feature learning: How one gradient step improves the representation. Advances in Neural Information
Processing Systems (NeurIPS), 2022.
[4] T. Bricken and C. Pehlevan. Attention approximates sparse distributed memory. Advances in Neural
Information Processing Systems (NeurIPS), 2021.
[5] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam,
G. Sastry, A. Askell, et al. Language models are few-shot learners. In Advances in Neural Information
Processing Systems (NeurIPS), 2020.
[6] S. Chan, A. Santoro, A. Lampinen, J. Wang, A. Singh, P. Richemond, J. McClelland, and F. Hill. Data
distributional properties drive emergent in-context learning in transformers. In Advances in Neural
Information Processing Systems (NeurIPS), 2022.
[7] F. Charton. What is my math transformer doing?–three results on interpretability and generalization.
arXiv preprint arXiv:2211.00170, 2022.
[8] L. Chizat, E. Oyallon, and F. Bach. On lazy training in differentiable programming. Advances in Neural
Information Processing Systems (NeurIPS), 2019.
[9] A. Chowdhery, S. Narang, J. Devlin, M. Bosma, G. Mishra, A. Roberts, P. Barham, H. W. Chung,
C. Sutton, S. Gehrmann, et al. Palm: Scaling language modeling with pathways. arXiv preprint
arXiv:2204.02311, 2022.
11

[10] A. Damian, J. Lee, and M. Soltanolkotabi. Neural networks can learn representations with gradient
descent. In Conference on Learning Theory (COLT), 2022.
[11] G. Dar, M. Geva, A. Gupta, and J. Berant. Analyzing transformers in embedding space. arXiv preprint
arXiv:2209.02535, 2022.
[12] B. L. Edelman, S. Goel, S. Kakade, and C. Zhang. Inductive biases and variable creation in self-attention
mechanisms. In Proceedings of the International Conference on Machine Learning (ICML), 2022.
[13] N. Elhage, T. Hume, C. Olsson, N. Schiefer, T. Henighan, S. Kravec, Z. Hatfield-Dodds, R. Lasenby,
D. Drain, C. Chen, R. Grosse, S. McCandlish, J. Kaplan, D. Amodei, M. Wattenberg, and C. Olah. Toy
models of superposition. Transformer Circuits Thread, 2022.
[14] N. Elhage, N. Nanda, C. Olsson, T. Henighan, N. Joseph, B. Mann, A. Askell, Y. Bai, A. Chen,
T. Conerly, N. DasSarma, D. Drain, D. Ganguli, Z. Hatfield-Dodds, D. Hernandez, A. Jones, J. Kernion,
L. Lovitt, K. Ndousse, D. Amodei, T. Brown, J. Clark, J. Kaplan, S. McCandlish, and C. Olah. A
mathematical framework for transformer circuits. Transformer Circuits Thread, 2021.
[15] M. Geva, J. Bastings, K. Filippova, and A. Globerson. Dissecting recall of factual associations in
auto-regressive language models. arXiv preprint arXiv:2304.14767, 2023.
[16] A. Graves, G. Wayne, and I. Danihelka. Neural turing machines. arXiv preprint arXiv:1410.5401, 2014.
[17] S. Gunasekar, B. E. Woodworth, S. Bhojanapalli, B. Neyshabur, and N. Srebro. Implicit regularization
in matrix factorization. Advances in Neural Information Processing Systems (NIPS), 2017.
[18] J. J. Hopfield. Neural networks and physical systems with emergent collective computational abilities.
Proceedings of the national academy of sciences, 79(8):2554–2558, 1982.
[19] J. Hron, Y. Bahri, J. Sohl-Dickstein, and R. Novak. Infinite attention: Nngp and ntk for deep attention
networks. In Proceedings of the International Conference on Machine Learning (ICML), 2020.
[20] A. Iscen, T. Furon, V. Gripon, M. Rabbat, and H. Jégou. Memory vectors for similarity search in
high-dimensional spaces. IEEE transactions on big data, 4(1):65–77, 2017.
[21] A. Jacot, F. Gabriel, and C. Hongler. Neural tangent kernel: Convergence and generalization in neural
networks. Advances in Neural Information Processing Systems (NeurIPS), 2018.
[22] S. Jelassi, M. Sander, and Y. Li. Vision transformers provably learn spatial structure. In Advances in
Neural Information Processing Systems (NeurIPS), 2022.
[23] Y. Jiang and C. Pehlevan. Associative memory in iterated overparameterized sigmoid autoencoders. In
Proceedings of the International Conference on Machine Learning (ICML), 2020.
[24] A. Joulin and T. Mikolov. Inferring algorithmic patterns with stack-augmented recurrent nets. Advances
in Neural Information Processing Systems (NIPS), 2015.
[25] S. Karp, E. Winston, Y. Li, and A. Singh. Local signal adaptivity: Provable feature learning in neural
networks beyond kernels. Advances in Neural Information Processing Systems (NeurIPS), 2021.
[26] D. Krotov and J. J. Hopfield. Dense associative memory for pattern recognition. Advances in Neural
Information Processing Systems (NIPS), 29, 2016.
[27] Y. Li, Y. Li, and A. Risteski. How do transformers learn topic structure: Towards a mechanistic
understanding. arXiv preprint arXiv:2303.04245, 2023.
[28] B. Liu, J. T. Ash, S. Goel, A. Krishnamurthy, and C. Zhang. Transformers learn shortcuts to automata.
In Proceedings of the International Conference on Learning Representations (ICLR), 2023.
12

[29] Z. Liu, O. Kitouni, N. S. Nolte, E. Michaud, M. Tegmark, and M. Williams. Towards understanding
grokking: An effective theory of representation learning. In Advances in Neural Information Processing
Systems (NeurIPS), 2022.
[30] R. McEliece, E. Posner, E. Rodemich, and S. Venkatesh. The capacity of the hopfield associative memory.
IEEE transactions on Information Theory, 33(4):461–482, 1987.
[31] K. Meng, D. Bau, A. Andonian, and Y. Belinkov. Locating and editing factual associations in gpt. In
Advances in Neural Information Processing Systems (NeurIPS), 2022.
[32] W. Merrill, A. Sabharwal, and N. A. Smith. Saturated transformers are constant-depth threshold circuits.
Transactions of the Association for Computational Linguistics, 10:843–856, 2022.
[33] T. Mikolov, K. Chen, G. Corrado, and J. Dean. Efficient estimation of word representations in vector
space. In Proceedings of the International Conference on Learning Representations (ICLR), 2013.
[34] S. Min, X. Lyu, A. Holtzman, M. Artetxe, M. Lewis, H. Hajishirzi, and L. Zettlemoyer. Rethinking the
role of demonstrations: What makes in-context learning work? In Conference on Empirical Methods in
Natural Language Processing (EMNLP), 2022.
[35] N. Nanda, L. Chan, T. Liberum, J. Smith, and J. Steinhardt. Progress measures for grokking via
mechanistic interpretability. In Proceedings of the International Conference on Learning Representations
(ICLR), 2023.
[36] C. Olsson, N. Elhage, N. Nanda, N. Joseph, N. DasSarma, T. Henighan, B. Mann, A. Askell, Y. Bai,
A. Chen, T. Conerly, D. Drain, D. Ganguli, Z. Hatfield-Dodds, D. Hernandez, S. Johnston, A. Jones,
J. Kernion, L. Lovitt, K. Ndousse, D. Amodei, T. Brown, J. Clark, J. Kaplan, S. McCandlish, and
C. Olah. In-context learning and induction heads. Transformer Circuits Thread, 2022.
[37] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever, et al. Language models are unsupervised
multitask learners. Technical report, OpenAI, 2019.
[38] J. W. Rae, S. Borgeaud, T. Cai, K. Millican, J. Hoffmann, F. Song, J. Aslanides, S. Henderson, R. Ring,
S. Young, et al. Scaling language models: Methods, analysis & insights from training gopher. arXiv
preprint arXiv:2112.11446, 2021.
[39] Y. Razeghi, R. L. Logan IV, M. Gardner, and S. Singh. Impact of pretraining term frequencies on
few-shot reasoning. In Conference on Empirical Methods in Natural Language Processing (EMNLP),
2022.
[40] A. M. Saxe, J. L. McClelland, and S. Ganguli. Exact solutions to the nonlinear dynamics of learning in
deep linear neural networks. arXiv preprint arXiv:1312.6120, 2013.
[41] I. Schlag, K. Irie, and J. Schmidhuber. Linear transformers are secretly fast weight programmers. In
Proceedings of the International Conference on Machine Learning (ICML), 2021.
[42] J. Schmidhuber. Learning to control fast-weight memories: An alternative to dynamic recurrent networks.
Neural Computation, 4(1):131–139, 1992.
[43] S. Shin, S.-W. Lee, H. Ahn, S. Kim, H. Kim, B. Kim, K. Cho, G. Lee, W. Park, J.-W. Ha, et al. On the
effect of pretraining corpora on in-context learning by a large-scale language model. In North American
Chapter of the Association for Computational Linguistics (NAACL), 2022.
[44] C. Snell, R. Zhong, D. Klein, and J. Steinhardt. Approximating how single head attention learns. arXiv
preprint arXiv:2103.07601, 2021.
[45] S. Sukhbaatar, A. Szlam, J. Weston, and R. Fergus. End-to-end memory networks. Advances in Neural
Information Processing Systems (NIPS), 2015.
13

[46] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin.
Attention is all you need. In Advances in Neural Information Processing Systems (NIPS), 2017.
[47] R. Vershynin. High-dimensional probability: An introduction with applications in data science, volume 47.
Cambridge university press, 2018.
[48] K. Wang, A. Variengien, A. Conmy, B. Shlegeris, and J. Steinhardt. Interpretability in the wild: a
circuit for indirect object identification in gpt-2 small. In Proceedings of the International Conference on
Learning Representations (ICLR), 2023.
[49] J. Weston, S. Chopra, and A. Bordes. Memory networks. In Proceedings of the International Conference
on Learning Representations (ICLR), 2015.
[50] D. J. Willshaw, O. P. Buneman, and H. C. Longuet-Higgins. Non-holographic associative memory.
Nature, 222(5197):960–962, 1969.
[51] S. M. Xie, A. Raghunathan, P. Liang, and T. Ma. An explanation of in-context learning as implicit
bayesian inference. In Proceedings of the International Conference on Learning Representations (ICLR),
2022.
[52] G. Yang and E. J. Hu. Tensor programs iv: Feature learning in infinite-width neural networks. In
Proceedings of the International Conference on Machine Learning (ICML), 2021.
[53] Y. Zhang, A. Backurs, S. Bubeck, R. Eldan, S. Gunasekar, and T. Wagner. Unveiling transformers with
lego: a synthetic reasoning task. arXiv preprint arXiv:2206.04301, 2022.
14

A
Associative Memories with Random Vectors
In this section, we provide basic properties of associative memories based on outer products of random
Gaussian embeddings, as described in Section 4.
We consider embeddings uk ∈Rd with i.i.d. Gaussian N(0, 1
d) entries.
We recall a few facts:
• (Norm) We have u⊤
i ui ≈1. This is standard from the concentration of random vectors in high dimension
(see, e.g., [47, Theorem 3.1.1]).
• (Near-orthogonality) For i ̸= j, we have u⊤
i uj = O(1/
√
d). To see this, denoting ui = d−1/2(˜uik)k,
where ˜uik are the unnormalized entries of ui, note that we have
√
du⊤
i uj =
1
√
d
d
X
k=1
˜uik˜ujk →N(0, 1),
by the central limit theorem, since for each k, the quantities ˜uik˜ujk are zero-mean, unit-variance,
i.i.d. random variables.
• (Remapping: norm) If W is a Gaussian random matrix with i.i.d. N(0, 1
d) entries, then for any fixed x we
have ∥Wx∥≈∥x∥. This follows from Johnson-Lindenstrauss (see, e.g., [47, Lemma 5.3.2 and Exercise
5.3.3]). In particular, if x is a normalized Gaussian embedding as above, then ∥Wx∥≈1.
• (Remapping: near-orthogonality) Consider a random vector x =
1
√
d ˜x and a random matrix W =
1
√
d ˜W,
where the entries of ˜x and ˜W are i.i.d. N(0, 1). Then x and Wx are nearly orthogonal. To see this,
note that E[x⊤Wx] = E[x⊤E[W]x] = 0, and the variance is
E(x⊤Wx)2 = E
X
i,j
x2
i W 2
ijx2
j
= 1
d3 E
X
i
˜x4
i ˜W 2
ii + E
X
i̸=j
˜x2
i ˜x2
j ˜W 2
ij
= 1
d3

dM4M2 + d(d −1)
2
M 3
2

= O
1
d

,
where M2 and M4 denote the second and fourth moments of the standard Gaussian, respectively. Then,
Chebyshev’s inequality implies that |x⊤Wx| = O(1/
√
d) with high probability.
Ensuring appropriate memory lookups then requires such properties to hold for many embeddings and
pairs of embeddings, with errors that are small enough to ensure correct associations. This may be achieved
with careful union bounds or more powerful concentration results. We do not attempt to do this in a precise
manner in this paper, and will generally assume d large enough to satisfy the desired associative memory
behaviors, noting that a precise analysis is an important direction for future work.
B
Theoretical Insights on Gradient Dynamics
In this section, we provide additional details on the theoretical insights from Section 6, including details
on the illustrative example (Section B.1), derivations of gradients w.r.t. key-query matrices at initialization
(Section B.2), as well as a study of how the induction head mechanism may develop in a simplified setup,
using a sequence of single layer-wise gradient steps in a top-down manner (Section B.3).
15

B.1
Details on illustrative example
Consider the example discussed in Section 6: we would like to predict y from x = wE(y) + pt, where pt is a
positional embedding at a random position t ∈[T], which we would like to ignore. Further assume that y is
uniformly distributed (p(y = k) = 1/N) and consider the matrix obtained after one population gradient step
with step-size η starting from an initialization W0 = 0 (so that ˆpW0(k|x) = 1/N):
W1 = η
N
N
X
k=1
wU(k)(µk −¯µ)⊤,
(11)
with ¯µ = E[x].
Note that we have µk = wE(k) + 1
T
P
t pt and ¯µ = 1
N
P
k wE(k) + 1
T
P
t pt, so that (11) becomes
W1 = η
N
N
X
k=1
wU(k)(wE(k) −¯wE)⊤,
(12)
with ¯wE :=
1
N
PN
k=1 wE(k). When d is large enough to ensure near-orthonormal embeddings, we have for
any y and t,
W1(wE(y) + pt) ≈η
N wU(y) + O
 1
N 2

.
This implies
wU(k)⊤W1(wE(y) + pt) ≈η
N 1{k = y} + O
 1
N 2

,
as claimed in the main text. The classifier ˆy = arg maxk wU(k)⊤W1(wE(y) + pt) then has essentially perfect
accuracy, and has learned to ignore the spurious positional embeddings, which are simply exogenous noise.
B.2
Gradients on key-query matrices at initialization
We now derive expressions for population gradients of the attention key-query matrices at zero initialization,
noting that random initialization behaves similarly to zero initialization. Although the optimization problems
involving these matrices are non-convex, these gradients at initialization lead to associative memories, similar
to Lemma 2. When output matrices of the previous layer already encode the desired associations, these
gradients can lead to associative memories that focus the attention on the correct key-value pairs.
We begin with the following lemma, which gives the gradient of the loss w.r.t. W = W 2
K at zero
initialization. For simplicity, we drop the d−1/2 factor from the softmax, which only changes gradients by a
multiplicative factor, and thus does not change its form.
Lemma 3 (Gradient of second attention layer). Consider the following loss for predicting the next token y
from an attention layer with inputs X = [x1, . . . , xT ], and value-output matrix Φ2 := W 2
OW 2
V :
L(W) = E(X,y)[ℓ(y, ξ(X))],
ξ(X) = WUΦ2Xσ(X⊤WxT ),
(13)
with ℓthe cross-entropy loss and σ(u)t =
eut
P
s eus for u ∈RT is the softmax.
The gradient at W = 0 is given by
∇W L(W)

W =0 =
N
X
k=1
E(X,y)
"
(ˆpW (k|X) −1{y=k}) 1
T
T
X
t=1
wU(k)⊤Φ2xt · (xt −¯x1:T )x⊤
T
#
= 1
T
N
X
k=1
T
X
t=1
EX[ˆpW (k|X)wU(k)⊤Φ2xt · (xt −¯x1:T )x⊤
T ]
−1
T
N
X
k=1
T
X
t=1
p(y = k) EX[wU(k)⊤Φ2xt · (xt −¯x1:T )x⊤
T | y = k]
16

with ¯x1:T = 1
T
PT
t=1 xt.
Now we consider the gradient w.r.t. W = W 1
K at zero initialization, and consier a simplification of the
second layer attention to its linearization around W 2
K = 0. We will see that this still provides first-order
information that is sufficient for W 1
K to be learned.
Lemma 4 (Gradient of first attention layer). Consider the following loss for predicting the next token y from
a stack of two attention layers, with all parameters fixed except for W = W 1
K, the key-query matrix at the
first attention layer:
L(W) = E(X,y)[ℓ(y, ξ(X))],
ξ(X) = WUΦ2X¯σ(Z(W)⊤W2xT ).
(14)
Here, ¯σ(u1:T )t =
1
T (1 + ut −1
T
PT
s=1 us) is the linearization of the softmax around 0, and Z(W) =
[z1(W), . . . , zT (W)] with
zt(W) =
t
X
s=1
Φ1xsσ(p⊤
1:tWpt)s,
and Φℓ= W ℓ
OW ℓ
V for ℓ= 1, 2.
The gradient at W = 0 is given by
∇W L(W)

W =0
=
N
X
k=1
EX
"
ˆpW (k|X) 1
T
T
X
t=1
wU(k)⊤Φ2xt · 1
t
t
X
s=1
(Φ1xs)⊤W2xT (ps −¯p1:t)p⊤
t
#
−
N
X
k=1
p(y = k) EX
"
1
T
T
X
t=1
wU(k)⊤Φ2xt · 1
t
t
X
s=1
(Φ1xs)⊤W2xT (ps −¯p1:t)p⊤
t |y = k
#
−
N
X
k=1
EX
"
ˆpW (k|X)wU(k)⊤Φ2¯x1:T · 1
T
T
X
t=1
1
t
t
X
s=1
(Φ1xs)⊤W2xT (ps −¯p1:t)p⊤
t
#
+
N
X
k=1
p(y = k) EX
"
wU(k)⊤Φ2¯x1:T · 1
T
T
X
t=1
1
t
t
X
s=1
(Φ1xs)⊤W2xT (ps −¯p1:t)p⊤
t |y = k
#
B.3
Learning the induction head mechanism
In this section, we analyze the training dynamics of the induction head mechanism, in the following simplified
setup: we consider a single trigger (K = 1), and assume that πu, πq, πo and πb(·|i) are uniform over [N] for
any i.
To further simplify the analysis, we consider a loss that only considers sequences of length T where the
last input zT is the second occurrence of the trigger token, and the label y = zT +1 is the corresponding output
token. We note that this may be easily extended to later occurrencies of the trigger. This is similar to the
setup of Figure 1, where the loss is only taken on triggers after the second occurrence: in that case, the loss
may be written as a weighted sum of the one we consider here, weighted by the probability of the second (or
later) trigger appearing at the given position T.
In practice, when the loss is on all tokens and WF is also learned, we may expect that WF quickly learns
the global bigram statistics, as we saw empirically in Section 5. Indeed, the current token embedding, which
is included in the input superposition, has strong predictive signal compared to the attention layers, which
initially mainly appear as noise. This is then similar to the setup of Lemma 1, which provides recovery
of bigram statistics when d is large (though we note that the other information from attention layers in
the inputs may eventually be used and bias away from perfect recovery, see Figure 4(right)). Once such
global estimates are obtained, the expected loss will be mainly dominated by trigger tokens, leading to the
setup above.
17

For simplicity, we thus drop the feed-forward layer WF in the remainder of this section, focusing on
the learning of W 2
O, W 2
K and W 1
K, in this top-down order. We will consider zero-initialization a single
gradient steps, noting that random initialization should lead to similar associative memory behaviors when
the dimension is large enough, since it leads to a remapping of input embeddings which is near-orthogonal to
any output embedding (see Appendix A).
B.3.1
Learning W 2
O
We begin by studying the learning of the second output matrix W 2
O. In the above data model, we may
consider a loss as in Lemma 2 with input-outputs (x, y), where y is the output token of the sequence, and x
depends on the random sequence z1:T as
x = 1
T
T
X
t=1
W 2
V (wE(zt) + εt),
where εt = pt + 1
t
Pt
s=1 Φ1(wE(zs) + ps) with Φ1 = W 1
OW 1
V , is a “noise” vector from the residual streams,
containing positional embeddings as well as an average attention output from the first layer. In practice, the
logit predictions are of the form WU(W 2
Ox + εT ) due to residual connections, but we ignore the term WUεT
for simplicity, noting that it is near-zero when d is large.
After a gradient step on W 2
O with step-size η, starting from zero-initialization (so that ˆp(k|x) = p(y =
k) = 1/N for all x), Lemma 2 yields
W 2
O = η
N
N
X
k=1
wU(k)(E[x|y = k] −E[x])⊤.
(15)
Now, consider the random variables q (trigger token), o (output token), to (position of the first occurrence
of the output token). In our simplified data model, q and to have the same distribution regardless of the
conditioning on y = k, while o is equal to k when y = k, while it is uniform in [N] without this condition.
The sequence z1:T has the same distribution in either p(·) or p(·|y = k), except for the token zto.
Then, we may write:
E[x|y = k] −E[x] = 1
T
 E[W 2
V wE(zto)|y = k] −E[W 2
V wE(zto)]

+ 1
T
 
E
" T
X
t=to
W 2
V εt|y = k
#
−E
" T
X
t=to
W 2
V εt
#!
,
since εt is independent of o when t < to. Noting that εt only depends on zto and thus on y through the first
layer attention, we have
E[x|y = k] −E[x] = 1
T W 2
V (wE(k) −¯wE)
+ 1
T
 
E
" T
X
t=to
1
t W 2
V Φ1wE(zto)|y = k
#
−E
" T
X
t=to
1
t W 2
V Φ1wE(zto)
#!
= 1
T W 2
V (wE(k) −¯wE) + τ
T W 2
V Φ1(wE(k) −¯wE),
where τ := E
hPT
t=to
1
t
i
, and ¯wE = 1
N
PN
k=1 wE(k). Thus, (15) becomes
W 2
O =
η
NT
N
X
k=1
wU(k)(W 2
V (wE(k) −¯wE))⊤+ ητ
NT
N
X
k=1
wU(k)(W 2
V Φ1(wE(k) −¯wE))⊤,
(16)
18

so that when d is learn enough to ensure near-orthonormal embeddings, we have
wU(k)⊤W 2
OW 2
V wE(j) ≈
η
NT 1{k = j} + O

η
N 2T

wU(k)⊤W 2
OW 2
V Φ1wE(j) ≈ητ
NT 1{k = j} + O
 ητ
N 2T

,
where the O(·) terms are due to the ¯wE elements. The first line yields a behavior that matches desired
associative memory in (7) of Section 4.2 when N is large.
The second line shows additional spurious
associations that are stored in W 2
O due to the output of the first layer attention, but which may be “cleaned
up” once the attention layers start focusing on the correct tokens.
Finally, we note that despite the recovery of these useful associations after one gradient step, the predictions
with this estimate W 2
O are still near-random, since in the bag-of-words setup with average attention, the
output token cannot be distinguished from any other token in the sequence in our model (except perhaps the
trigger token, which is guaranteed to appear twice, but does not provide any signal to infer the output token,
since the two are independent).
B.3.2
Learning W 2
K
Now assume that W 2
O is as in (16). As argued above, the predictions ˆp(k|x) are essentially random 1/N in
our model for W 2
K = 0, so that after one gradient step on W 2
K with learning rate η, Lemma 3 yields:
W 2
K =
η
TN
X
k,t
 E[wU(k)⊤Φ2xt · (xt −¯x)x⊤
T | y = k] −E[wU(k)⊤Φ2xt · (xt −¯x)x⊤
T ]

,
(17)
where xt are the inputs to the second attention layer, given by
xt = xt,0 + xt,1
(18)
xt,0 = wE(zt) + pt
(19)
xt,1 = 1
t
t
X
s=1
Φ1(wE(zs) + ps).
(20)
From now on, we consider a simplified architecture where only xt,0 are fed as queries and values, while
only xt,1 are fed as keys. Using the fact that trigger tokens q are sampled uniformly (i.e., πq = 1/N), we have
W 2
K =
η
TN
N
X
k=1
T
X
t=1
(E[At,k | y = k] −EX[At,k])
(21)
=
η
TN 2
N
X
k=1
T
X
t=1
N
X
j=1
(E[At,k | y = k, q = j] −E[At,k | q = j])
(22)
where
At,k = wU(k)⊤Φ2xt,0 · (xt,1 −¯x1)x⊤
T,0,
(23)
with ¯x1 =
1
T
P
t xt,1. Now, note that we have wU(k)⊤Φ2xt,0 ≈α 1{zt = k} with α = η/TN by (16),
and xT,0 = wE(q) + pT . This yields
W 2
K ≈
αη
TN 2
N
X
j=1
N
X
k=1
∆k,j(wE(j) + pT )⊤,
(24)
with
∆k,j := E
" T
X
t=1
1{zt = k}(xt,1 −¯x1)|y = k, q = j
#
−E
" T
X
t=1
1{zt = k}(xt,1 −¯x1)|q = j
#
= ∆o
k,j + ∆q
k,j + ∆r
k,j,
19

where the three terms split the sum inside the expectation
∆o
k,j := E [1{zto = k}(xto,1 −¯x1)|y = k, q = j] −E [1{zto = k}(xto,1 −¯x1)|q = j]
∆q
k,j := E

X
t∈Tq
1{zt =k}(xt,1 −¯x1)|y=k, q=j

−E

X
t∈Tq
1{zt =k}(xt,1 −¯x1)|q=j


∆r
k,j := E
"X
t∈Tr
1{zt =k}(xt,1 −¯x1)|y=k, q=j
#
−E
"X
t∈Tr
1{zt =k}(xt,1 −¯x1)|q=j
#
,
where Tq = {to −1, T} and Tr = [T] \ {to, to −1, T} (recall that to is a random variable, corresponding to the
first occurrence of the output token, so that these sets are random).
We will now show that ∆o
k,j carries the desired signal for the appropriate induction head associative
memory, while ∆q
k,j and ∆r
k,j are negligible, for N large enough.
Controlling ∆o
k,j.
For t = to, noting that zto = y, we have
∆o
k,j = E [1{y = k}(xto,1 −¯x1)|y = k, q = j] −E [1{y = k}(xto,1 −¯x1)|q = j]
=

1 −1
N

E [xto,1 −¯x1|y = k, q = j]
= N −1
N
 
¯p +
N
X
i=1
ak,j,iΦ1wE(i)
!
,
with ak,j,i ≈(Φ1wE(i))⊤E [xto,1 −¯x1|y = k, q = j] thanks to near-orthonormality, and
¯p = Eto
"
1
to
to
X
s=1
ps −1
T
T
X
t=1
1
t
t
X
s=1
ps
#
is a spurious positional mixture.
We then distinguish the following cases:
• If j ̸= k and i = j, since the trigger token j only appears at positions to −1 and T, we have
ak,j,i ≈Eto
"
1
to
−1
T
T
X
t=to−1
1
t −1
T 2
#
=: γT .
We may expect to to be concentrated around T/2, in which case γT ≳2
T −1
T −
1
T 2 ≥C
T > 0 for T larger
than a small constant.
• If j = k = i, the two occurrences of the trigger happen one after the other, so it must be that to = T. Then
ak,j,i ≈2
T −
1
T(T −1) −2
T 2 = 2
T + O
 1
T 2

,
for T larger than a small constant.
• If i ̸= j = k, all tokens up to position to −2 = T −2 are i.i.d. uniform in [N] \ {j}, so that
ak,j,i ≈
T −2
T(N −1) −1
T

(T −2) ·
1
N −1 +
T −2
(T −1)(N −1) +
T −2
T(N −1)

= O
 1
N

20

• If i ̸= j and j ̸= k, all tokens except at positions to −1, to and T (we have to < T) are uniform in [N] \ {j}.
The triggers do not contribute anything to ak,j,i since i ̸= j, and the output token may be also randomized
by taking the average over k ∈[N] \ {j}. We thus obtain
1
N −1
X
k̸=j
ak,j,i ≈O
 1
N

.
In summary, we obtain
1
N
N
X
k=1
ak,j,i ≈
(
O
  1
N

,
if i ̸= j
Ω
  1
T

,
if i = j.
Thus, when N is large, while T is moderate, the above sum leads to more signal in the i = j terms compared
to i ̸= j. In particular, this yields
(Φ1wE(i))⊤
 
1
N
N
X
k=1
∆o
k,j
!
≈
(
O
  1
N

,
if i ̸= j
Ω
  1
T

,
if i = j,
so that this component in (24) acts precisely like the desired associative memory in (7).
It remains to show that the other components are negligible compared to this. It then suffices to show:
1
N
N
X
k=1
(∆q
k,j + ∆r
k,j) ≈o
 1
T

.
Controlling ∆q
k,j.
For t ∈Tq, note that we always have zt = j in the expectations, so that ∆q
k,j = 0
unless k = j. For k = j, we have ∆q
k,j = O(1), so that
1
N
N
X
k=1
∆q
k,j = O
 1
N

.
Controlling ∆r
k,j.
Using that ∥xt,1 −¯x1∥≤C = O(1) for all t, we provide the following crude bound via
the triangle inequality and Hölder inequality:
∆r
k,j = E
"X
t∈Tr
1{zt =k}(xt,1 −¯x1)|y=k, q=j
#
−E
"X
t∈Tr
1{zt =k}(xt,1 −¯x1)|q=j
#
∥∆r
k,j∥≤C
 
E
"X
t∈Tr
1{zt =k}|y=k, q=j
#
+ E
"X
t∈Tr
1{zt =k}|q=j
#!
≤2CT
N ,
since zt is independent of y given t ∈Tr and thus is uniform in [N] \ {j}, and |Tr| ≤T. We note, however,
that ∆r
k,j may be controlled much more finely by leveraging the similarities between the distributions
of zt, t ∈Tr with or without conditioning on y.
Overall, we have shown that up to some spurious positional embeddings, W 2
K behaves as the desired
associative memory from (7) when N is large enough, satisfying:
(Φ1wE(i))⊤W 2
KwE(j) ≈αη
TN

Ω
 1
T

1{i = j} + O
 T
N

(25)
21

B.3.3
Learning W 1
K
We now assume that W 2
O and W 2
K have learned the correct associations, and consider one gradient step away
from zero-initialization on W 1
K. Note that when W 1
K = 0, the predictions of the model are still often near
random chance. Indeed, the second layer attention will attend to all tokens starting at the first occurrence of
the trigger, since all such tokens contain Φ1wE(q) in their average attention, which activates the second-layer
attention head. Then the output is likely to predict the trigger itself, which will be an incorrect prediction
most of the time.
We may thus consider ˆp(k|X) = 1/N at this stage as well. We also consider a simplified architecture where
the first layer attention only uses positional embeddings in the key-query matrix, and only token embeddings
in the value-output matrix. In particular, we have xt = wE(zt). Lemma 4 then gives the following form
for W 1
K after one gradient step of step-size η:
W 1
K = η
N
N
X
k=1
EX
"
1
T
T
X
t=1
wU(k)⊤Φ2xt · 1
t
t
X
s=1
(Φ1xs)⊤W 2
KxT (ps −¯p1:t)p⊤
t |y = k
#
−η
N
N
X
k=1
EX
"
1
T
T
X
t=1
wU(k)⊤Φ2xt · 1
t
t
X
s=1
(Φ1xs)⊤W 2
KxT (ps −¯p1:t)p⊤
t
#
−η
N
N
X
k=1
EX
"
wU(k)⊤Φ2¯x1:T · 1
T
T
X
t=1
1
t
t
X
s=1
(Φ1xs)⊤W 2
KxT (ps −¯p1:t)p⊤
t |y = k
#
+ η
N
N
X
k=1
EX
"
wU(k)⊤Φ2¯x1:T · 1
T
T
X
t=1
1
t
t
X
s=1
(Φ1xs)⊤W 2
KxT (ps −¯p1:t)p⊤
t
#
.
Note that since W 2
O and W 2
K already captured the desired associations at this stage, we have
wU(k)⊤Φ2xt ≈α 1{zt = k}
and
(Φ1xs)⊤W 2
KxT ≈α′ 1{zs = zT },
for some α, α′ > 0. Recall that in our model, we have zT = q with probability one (q is the trigger token),
and that q only appears twice: once at position tq := to −1 < T and once at position T. We then have, for
any t > 1,
W 1
Kpt ≈ηαα′
NTt
N
X
k=1
(At,k −Bt,k −Ct,k + Dt,k),
with
At,k = E[1{zt =k} 1{tq ≤t}(ptq −¯p1:t)|y=k]
(26)
Bt,k = E[1{zt =k} 1{tq ≤t}(ptq −¯p1:t)]
(27)
Ct,k = E[rk 1{tq ≤t}(ptq −¯p1:t)|y=k]
(28)
Dt,k = E[rk 1{tq ≤t}(ptq −¯p1:t)],
(29)
where rk := 1
T
PT
t=1 1{zt = k}. We have
At,k = E[1{zt =k}(1{tq = t −1} + 1{tq ∈[t −2] ∪{t}})(ptq −¯p1:t)|y=k]
= P(tq = t −1|y = k)(pt−1 −¯p1:t) + 1
N
X
s∈[t−2]∪{t}
P(tq = s|y = k)(ps −¯p1:t)
= P(tq = t −1)(pt−1 −¯p1:t) + 1
N
X
s∈[t−2]∪{t}
P(tq = s)(ps −¯p1:t)
= P(tq = t −1)(pt−1 −¯p1:t) + O
 1
N

,
22

since the distribution of tq is the same regardless of y. We proceed similarly for the other quantities and
obtain the following:
Bt,k = O
 1
N

Ct,k = P(tq = t −1)
T
(pt−1 −¯p1:t) + O
 1
N

Dt,k = O
 1
N

.
This yields the following associative memory behavior, for t > 1:
p⊤
s W 1
Kpt ≈ηαα′(T −1)
T 2t

P(tq = t −1)

1{s = t −1} −1
t 1{s ∈[t]}

+ O
 1
N

,
which matches the desired “previous token head” behavior from (7) when N is large.
C
Other Proofs
C.1
Proof of Lemma 1
Proof. Recall the form of the cross-entropy loss for classification with K classes:
ℓ(y, ξ) = −
N
X
k=1
1{y = k} log
eξk
P
j eξj .
Its derivatives take the form
∂ℓ
∂ξk
(y, ξ) = s(ξ)k −1{y = k},
with s(ξ)k =
eξk
P
j eξj the softmax.
The gradient of L is then given by
∇W L(W) = E(z,y)
" N
X
k=1
∂ℓ
∂ξk
(y, WUWwE(z))∇W (wU(k)⊤WwE(z))
#
= E(z,y)
" N
X
k=1
(ˆpW (k|z) −1{y = k})wU(k)wE(z)⊤
#
=
N
X
k=1
Ez[Ey[(ˆpW (k|z) −1{y = k})wU(k)wE(z)⊤| z]]
=
N
X
k=1
Ez[(ˆpW (k|z) −Ey[1{y = k}|z])wU(k)wE(z)⊤],
which yields the desired result.
23

C.2
Proof of Lemma 2
Proof. Using similar steps as the proof of Lemma 1, we have
∇W L(W) = E(x,y)
" N
X
k=1
∂ℓ
∂ξk
(y, WUWx)∇W (wU(k)⊤Wx)
#
= E(x,y)
" N
X
k=1
(ˆpW (k|x) −1{y = k})wU(k)x⊤
#
=
N
X
k=1
wU(k) Ex[ˆpW (k|x)x]⊤−
N
X
k=1
Ey[1{y = k}wU(k) E[x|y]⊤]
=
N
X
k=1
wU(k) Ex[ˆpW (k|x)x]⊤−
N
X
k,j=1
p(y = j) 1{j = k}wU(k) E[x|y = j]⊤
=
N
X
k=1
p(y = k)wU(k)(ˆµk −µk)⊤,
with ˆµk = p(y = k)−1 Ex[ˆpW (k|x)x] and µk = E[x|y = k].
C.3
Proof of Lemma 3
Proof. To better isolate the role of keys from values, we denote the keys that are fed into the matrix W
by Z = [z1, . . . , zT ] ∈Rd×T , while the query is simply xT . In practice we have Z = X, and both are
superpositions of potentially multiple embeddings (if W is part of the second attention layer, these are the
token embedding, positional embedding, and the output of the first attention layer).
The gradient of the loss at W = 0 writes:
∇W L(W)

W =0 = E(X,Z,y)
" N
X
k=1
∂ℓ
∂ξk
(y, ξ) · ∇W (wU(k)⊤Φ2Xσ(Z⊤WxT ))

W =0
#
(30)
= E(X,Z,y)
" N
X
k=1
(ˆpW (k|X, Z) −1{y = k}) · ∇W (wU(k)⊤Φ2Xσ(Z⊤WxT ))

W =0
#
.
(31)
We have
∇W (wU(k)⊤Φ2Xσ(Z⊤WxT ))

W =0 =
T
X
t=1
wU(k)⊤Φ2xt · ∇W (σ(Z⊤WxT )t)
= 1
T
T
X
t=1
wU(k)⊤Φ2xt · (zt −¯z1:T )x⊤
T ,
where ¯z1:T = 1
T
P
t zt, and we used the fact that
∂
∂us
σ(u)t

u=0 = 1
T 1{t = s} −1
T 2 .
(32)
The gradient (31) now writes
∇W L(W)

W =0 =
N
X
k=1
E(X,Z)[(ˆpW (k|X, Z) −1{y = k}) 1
T
T
X
t=1
wU(k)⊤Φ2xt · (zt −¯z1:T )x⊤
T ],
and the result follows.
24

C.4
Proof of Lemma 4
Proof. The linearization of the second layer softmax around zero takes the following form:
¯σ(Z⊤W2xT )t = 1
T (1 + z⊤
t W2xT −¯z⊤
1:T W2xT ),
with zt = Pt
s=1 Φ1xsσ(p⊤
1:tWpt)s the output of the first attention layer.
ξk =
T
X
t=1
wU(k)⊤Φ2xt¯σ(Z⊤W2xT )
= 1
T
T
X
t=1
wU(k)⊤Φ2xt + 1
T
T
X
t=1
wU(k)⊤Φ2xt
t
X
s=1
(Φ1xs)⊤W2xT σ(p⊤
1:tWpt)s
−wu(k)⊤Φ2¯x1:T · 1
T
T
X
t=1
t
X
s=1
(Φ1xs)⊤W2xT σ(p⊤
1:tWpt)s.
Then,
∇W L(W)

W =0
(33)
= E(X,y)
" N
X
k=1
∂ℓ
∂ξk
(y, ξ) · ∇W ξk

W =0
#
(34)
= E(X,y)
" N
X
k=1
∂ℓ
∂ξk
(y, ξ) 1
T
T
X
t=1
wU(k)⊤Φ2xt
1
t
t
X
s=1
(Φ1xs)⊤W2xT (ps −¯p1:t)p⊤
t
#
(35)
−E(X,y)
" N
X
k=1
∂ℓ
∂ξk
(y, ξ)wU(k)⊤Φ2¯x1:T · 1
T
T
X
t=1
1
t
t
X
s=1
(Φ1xs)⊤W2xT (ps −¯p1:t)p⊤
t
#
,
(36)
using (32). The result follows by using
∂ℓ
∂ξk (y, ξ) = ˆp(k|ξ) −1{y = k}.
D
Experiment Details and Additional Experiments
In this section, we present additional details on the experiments, as well as additional results.
Computing setup.
We use Pytorch and each run uses a single GPU, along with 60 CPU cores for real-time
data generation. We will make our code available upon publication.
Hyperparameters.
We now provide the hyperparameters used in each figure. The SGD step-size is
denoted η. We fix the momentum parameter to 0.9 and the weight decay parameter to 10−4. U denotes the
uniform distribution over [N].
• Figure 2: K = 3, πq = πu (random triggers) or Q is the K most likely elements of πu, πo = U, d = 128,
dhidden = 4 × 128 (hidden dimension of the feed-forward MLPs), η = 0.2.
• Figure 3: K = 5, πq = πu (random triggers), πo = U, d = 128, η = 0.2.
• Figure 4(left) and Figure 5: πo = U, d = 128, η = 1. For random triggers we use πq = πu. For K = 1
with fixed frequent trigger, the only trigger is the most probable token according to πu, while for K = 5
with fixed rare triggers, the five triggers are the 6-th to 10-th most probable tokens according to πu.
25

0
100
200
300
400
500
iteration
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
memory recall
Wo2
K=1, fix q frequent
K=5, fix q rare
K=1, random q
K=5, random q
0
100
200
300
400
500
iteration
0.0
0.2
0.4
0.6
0.8
1.0
memory recall
Wk2
K=1, fix q frequent
K=5, fix q rare
K=1, random q
K=5, random q
0
100
200
300
400
500
iteration
0.0
0.2
0.4
0.6
0.8
memory recall
Wk1 (t<64)
K=1, fix q frequent
K=5, fix q rare
K=1, random q
K=5, random q
Figure 5: Memory recall probes for the setting of Figure 4(left).
0
100
200
300
400
500
iteration
0.0
0.1
0.2
0.3
0.4
0.5
0.6
memory recall
Wo2
train unif
train bigram
0
100
200
300
400
500
iteration
0.0
0.2
0.4
0.6
0.8
memory recall
Wk2
train unif
train bigram
0
100
200
300
400
500
iteration
0.0
0.2
0.4
0.6
0.8
memory recall
Wk1 (t<64)
train unif
train bigram
Figure 6: Memory recall probes for the setting of Figure 4(center).
• Figure 4(center): K = 3, πq = πu (random triggers), πo = U or πo = πb (conditioned on the trigger),
d = 128, η = 1.
• Figure 4(right): K = 3, πq = πu (random triggers), πo = U, d = 128, η = 1.
Memory recall probes and data-distributional properties.
Figure 5 and Figure 6 show the evolution
of the different memory probes for the settings considered in Figure 4(left,center). Figure 5 highlights that
associative memories for the induction head are slower to learn when using few triggers (small K), rare fixed
triggers, or random triggers (note that the probe for W 2
K with fixed triggers only shows recall accuracy on
the set of triggers Q, which is an easier task). Figure 6 shows that using uniform output tokens can lead to
better fitting of W 2
O and W 2
K compared to using output tokens sampled from bigrams. In addition to the
increased diversity when using uniform outputs, this may also be due to the fact that bigram outputs are
already well predicted using global statistics with just the feed-forward layer, hence the gradient signal on
such well-predicted tokens may not propagate through the induction head mechanism. In contrast, the recall
accuracy for W 1
K is comparable for both settings, since the previous token head is useful at all positions
regardless of the output token distribution.
Effect of dimension.
Recall that our study of associative memories with random embeddings requires
large dimension d in order to ensure near-orthogonality, and thus store input-output pairs more effectively. In
Figure 7, we evaluate the recall accuracy for W 2
O for varying dimension, when training it by itself, and only
on the output tokens (as in Figure 3). We see that higher dimension leads to faster learning of the memory,
in particular d = 128 seems sufficient for fast learning after just a few iterations with a tuned learning rate. If
the learning rate isn’t tuned, we notice that there is a further slowdown for low dimension, which might be
due to issues with standard parameterization of neural networks at initialization [52]. Note that learning W 2
O
alone is a convex optimization problem, and we hypothesize that higher dimension makes the problem better
26

0
50
100
150
200
250
iteration
0.0
0.2
0.4
0.6
0.8
1.0
Wo2 recall, fixed lr
32
64
128
256
512
1024
0
20
40
60
80
iteration
0.2
0.4
0.6
0.8
1.0
Wo2 recall, tuned lr
32
64
128
256
512
1024
Figure 7: Effect of dimension on learning W 2
O alone, with fixed or tuned learning rate.
0
10
20
30
40
batches
0.0
0.2
0.4
0.6
0.8
1.0
accuracy
one-step classifier
d = 32
d = 64
d = 128
d = 256
d = 512
d = 1024
d = 2048
102
103
dimension d
0.2
0.4
0.6
0.8
accuracy
one-step classifier
5 batches
10 batches
20 batches
Figure 8: Accuracy of one-step estimate of W 2
O with varying dimension and number of batches used for
computing expectations. Each batch consists of 32 sequences of 256 tokens for a total of 8 192 tokens,
with K = 5 random triggers and uniform outputs.
conditioned, and hence easier to learn. In Figure 8, we show “one-step” recall accuracies for classifying output
tokens from the average attention input to W 2
O, given by
R1 = 1
N
N
X
k=1
1

k = arg max
k′ (W 2
V wE(k′))⊤(µk −µ)

,
where µk = E[x|y = k] and µ = E[x], for x = 1
t
Pt
s=1 W 2
V wE(zs) and y = zt+1, when zt is a trigger token
after its first occurrence. Expectations are computed over batches of data of varying sizes and in different
dimensions. We call this “one-step” since it is related to the classifier obtained after performing a single
gradient step on W 2
O from zero initialization (see Lemma 2 and Appendix B.3.1). The plots illustrate that
this simple one-step model is already able to extract relevant signal from the noisy average attention, after a
handful of batches of data, corresponding to tens of thousands of tokens, and that this gets easier as the
dimension increases.
27

