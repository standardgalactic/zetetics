C H A P T E R  7
UNIFICATION
Cardano knew how to make inferences from chance to frequencies. 
Bayes and Laplace then taught us how to make inferences from 
observed frequencies to chance— all within the framework of proba-
bilistic degrees- of- belief. But what, after all, is chance?
We know how to deal with it computationally, but we don’t seem 
to know what it is.
Bruno de Finetti tells us not to worry: Chance does not exist.
Yet, it is perfectly legitimate to carry on just as if it did! How is 
this possible? This is de Finetti’s great idea, and the story of this 
chapter.
That probability as a physical quantity— physical propensity, objec-
tive chance— does not exist is a position that has been taken by many 
philosophers, most notably David Hume. But de Finetti did more than 
just take this philosophical stance. He showed that, in a certain 
Bruno de Finetti

 
UNIFICATION 
123
precise sense, that if we dispense with objective chance nothing is lost. 
The mathematics of inductive reasoning remains exactly the same.
Consider flipping a coin of fixed bias. The trials are assumed to be 
independent, and as a result, they exhibit another important prop-
erty. Order doesn’t matter. To say that order doesn’t matter is to 
say that if you take any finite sequence of heads and tails and per-
mute the outcomes any way you please, and the resulting sequence has 
the same probability. We say that this probability is invariant under 
permutations.
Or, to put it another way, the only thing that does matter is rela-
tive frequency. Outcome sequences that have the same frequencies of 
heads and tails have the same probability. Frequency is said to be a suf-
ficient statistic. To say that order doesn’t matter or to say that the only 
thing that does matter is frequency are two ways of saying exactly the 
same thing. This property is called exchangeability by de Finetti.
Now suppose a coin with unknown bias is flipped 3 times with a 
uniform prior over bias, as in Bayes and Laplace. An outcome sequence 
with two heads and a tail can be gotten three ways:
HHT,
HTH,
THH.
Initial probabilities on the first trial are 1
2, and probabilities on subse-
quent trials are gotten by using Laplace’s rule of succession (chapter 6): 
P(HHT) = (1
2) (2
3) 1
4( ),
P(HTH) = (1
2) 1
3( ) 2
4( ),
P(THH) = (1
2) 1
3( ) 2
4( ).
These are the same. The denominators, 2, 3, and 4, have to be the 
same because of the rule of succession. The numerators have to be 
the same (though not in the same order) because of the same frequen-
cies of heads and tails. The point is that here we have an example of 
exchangeability without independence. In fact, the uniform prior was 
not essential. Any uncertainty about the bias of the coin in indepen-
dent trials gives exchangeable degrees of belief.

124 
CHAPTeR 7
De Finetti proved the converse. Suppose your degrees of belief— 
the judgmental probabilities of chapter 2— about outcome sequences 
are exchangeable. Call an infinite sequence of trials exchangeable if 
all of its finite initial segments are. De Finetti proved that every such 
exchangeable sequence can be gotten in just this way. It is just as if 
you had independence in the chances and uncertainty about the bias. 
It is just as if you were Thomas Bayes.
What the prior over the bias would be in Bayes is determined in the 
representation. Call this the imputed prior probability over chances 
the de Finetti prior. If your degrees of belief about outcome sequences 
have a particular symmetry, exchangeability, they behave just as if they 
are gotten from a chance model of coin flipping with an unknown 
bias and with the de Finetti prior over the bias.
So it is perfectly legitimate to use Bayes’ mathematics even if we 
believe that chance does not exist, as long as our degrees of belief are 
exchangeable.
De Finetti’s theorem helps dispel the mystery of where the prior 
belief over the chances comes from. From exchangeable degrees of 
belief, de Finetti recovers both the chance statistical model of coin flip-
ping and the Bayesian prior probability over the chances. The math-
ematics of inductive inference is just the same. If you were worried 
about where Bayes’ priors came from, if you were worried about 
whether chances exist, you can forget your worries. De Finetti has 
replaced them with a symmetry condition on degrees of belief. This is, we 
think you will agree, a philosophically sensational result.
READING DE FINETTI
Suppose you are interested enough to want to go to the source. We 
recommend chapter 9 of de Finetti’s Probability, Induction, and Sta-
tistics1 as the best stand- alone account of his ideas and his program. 
This includes a historical and comparative survey, which moves 
from the beginnings to the crisis of the classical formulation, the 
rise of objectivistic concepts, the erosion of the objectivistic po-
sitions, critical examination of controversial aspects, and recon-
struction of the classical formulation according to the subjectivistic 

 
UNIFICATION 
125
viewpoint. Although de Finetti is never easy to read, this is the best 
we know.
A longer, readable essay is his summing up of earlier work in his 
1937 “La Prévision,” which is available in a translation that he him-
self oversaw: “Foresight: Its Logical Laws, Its Subjective Sources.”2 
Just a year later he published our favorite paper, “Sur la condition 
d’équivalence partielle,” giving wide generalizations of his earlier work 
in the context of a deep philosophical discussion of inductive reason-
ing.3, 4 We will discuss these generalizations later in this chapter.
Returning to de Finetti’s original theorem, it is possible to ap-
proach it in a more simple and transparent way if we start with finite 
sequences. This is next up.
FINITE EXCHANGEABLE SEQUENCES
One thing one might worry about— de Finetti certainly did— is the 
reliance on infinity. De Finetti’s theorem is a limiting result; it is not 
true for exchangeable probabilities on finite sequences.5 But it can be 
approximated on finite sequences. This fact can be used to prove de 
Finetti’s theorem for infinite sequences.
Suppose that a coin is flipped twice. Outcomes can be HH, HT, 
TH, or TT. The set of possible probabilities over these outcome se-
quences consists of the points in a tetrahedron, with vertices giving 
each outcome probability 1. exchangeability requires that HT and 
TH have the same probability and thus determines a plane slicing 
through this tetrahedron, as shown in figure 7.1. The probability (0, 1
2, 
1
2, 0)— indicated on the upper right in the figure— that gives HT and 
TH each the probability 1
2 is not independent. If you see a head on 
the first toss, you will be sure of a tail on the second. This is just the 
probability that you would get for sampling from an urn with one red 
and one black ball without replacement.
The exchangeable probabilities that make the tosses independent 
are gotten from the surface of independence (called the Wright man-
ifold in population genetics) within the plane of exchangeability. The 
points on the plane and beneath the curve in figure 7.2 are exchange-
able probabilities that can be represented as averages over independent 

126 
CHAPTeR 7
probabilities. Those above the curve on the plane are exchangeable 
probabilities that cannot be so represented.
The exchangeable sequences above the curve can, however, be rep-
resented as averages of the kinds of probabilities we get from sampling 
from an urn of known composition, with two balls and without re-
placement. These are the vertices of the exchangeable triangle: 2 red 
balls (2 heads), 2 black balls (2 tails), and 1 of each. The vertices are 
extreme points. They cannot be represented as nontrivial averages of 
other points. every other point in the triangle can be represented as 
an average of them.
This generalizes to longer sequences and higher- dimensional geo-
metric objects. You can take it on faith and skip this paragraph or 
persist, as you please. Consider the vertices of the analogue to the 
Figure 7.2. Averages of independent probabilities
(0, ½, ½, 0)
(0, 0, 0, 1)
(1, 0, 0, 0)
Figure 7.1. exchangeable sequences

 
UNIFICATION 
127
triangle when there are more trials. (This is called the exchangeable 
simplex.) The vertices of this simplex are probabilities that repre-
sent sampling from an urn of the appropriate size without replace-
ment. Suppose that there are M red balls and L black balls in an urn. 
Sampling without replacement must give a sequence M red balls 
and L black balls in some order. All these outcome sequences have the 
same probability, so the probability is exchangeable. Sampling without 
replacement gives exchangeability. Call the exchangeable probability 
just gotten e, and suppose for reductio that it is a nontrivial mixture of 
two other probabilities, a and b. Both a and b must give probability 0 
to outcome sequences with other than M red and L black balls. But a 
and b must give each outcome sequence with M red and L black balls 
the same probability because they are exchangeable. So a = b = e. It is 
not a nontrivial average. It is a vertex of the exchangeable simplex.
Now, what if you think that there might be another trial— there might 
be another ball in the urn? And what if you maintain your belief that 
order doesn’t matter under that supposition? That is to say that your 
beliefs should be extendable to exchangeable beliefs on three trials. 
Then the point (0, 1
2, 1
2, 0) at the top vertex of our triangle is no longer 
an option. [Why? It gives probability 0 to HH and to TT. Then its ex-
tension to three trials must give probability 0 to HHH, HHT, TTH, 
and TTT. If it were exchangeable, it would have to give probability 0 
to any sequence with 2 heads or 2 tails in any position. But how am I 
to preserve probability of HT being 1
2, when both its extensions, HTH 
and HTT must have probability zero?]
extendability to an exchangeable probability on 3 tosses cuts off 
nonindependent probabilities near the top vertex of the triangle, as 
shown in figure 7.3.
extendability to more tosses further constrains the exchangeable 
probabilities, with them approximating the mixtures of independent 
ones, just as sampling without replacement from a large urn approxi-
mates sampling with replacement. The appendix to this chapter gives 
a careful statement (with an error term) for this finite version of the 
theorem.
What does this show? If your judgment of symmetry— that probabili-
ties don’t depend on order of trials— doesn’t itself depend on the number 
of trials, then you essentially have de Finetti’s theorem.

128 
CHAPTeR 7
DE FINETTI’S THEOREM FOR  
MORE GENERAL OBSERVABLES
The discussion above has been for binary observables— things like 
heads and tails, men and women, or 0s and 1s. The whole picture gen-
eralizes to essentially arbitrary observables: {red, white, blue}, or real 
values {successive measurements of length}, or a day- to- day weather 
maps. Consider successive measurements in an arbitrary space, Χ. Sup-
pose that we have assigned probabilities to possible outcomes. Thus 
for A1, A2, A3, subsets of Χ,
P(A1, A2, A3)
stands for our probability that the first outcome is in A1, the second, 
in A2, and the third, in A3. P is exchangeable if order doesn’t matter. 
Thus,
P(A1, A2, A3) = P(A2, A1, A3) = P(A2, A3, A1)
= P(A1, A3, A2) = P(A3, A1, A2) = P(A3, A2, A1).
This kind of symmetry is supposed to hold for any number of obser-
vations. Suppressing measure theoretic niceties, de Finetti’s theorem 
holds: 
P(A1, A2, . . . , An) =
P(A1) P(A2
∫
). . . P(An) µ(dP).
Figure 7.3. exchangeability extendable to three trials

 
UNIFICATION 
129
Moreover, μ is determined as the limiting proportion of outcomes in 
a given set.
When Χ is a 2- point set, for example, Χ = {0, 1}, this theorem spe-
cializes to the binary version of de Finetti’s theorem stated before. In 
general, the integral is over a terrifyingly large set, the set of all prob-
abilities on Χ. When Χ = {0, 1}, a probability is specified by a number 
p (the chance of 1), and the integral becomes
pk(1−p)n−k µ(dp).
0
1
∫
For 3- valued observables, {r, w, b}, for example, we get the probability 
of an outcome sequence:
P(r, r, w, r, b, b, w, r) =
pr
4pw
2
∫
pb
2µ(dp),
where the integral is over the set of pr , pw, pb, which are nonnegative 
and sum to 1.
The general form of de Finetti’s theorem looks marvelous until you 
think about it a bit. It says that we have to think about a probability 
μ on the set of all probabilities. If Χ is large, this can be very large 
indeed. There is a large, esoteric literature about this, but it is not easy 
to do.6
This leads to the question: what do we have to add to exchangeabil-
ity to get down to mixtures of the day-to-day families of statistics— 
the normal Poisson, uniform and other standard families? This, 
too, is a healthy research area. We will give a brief account for the 
normal next.7
DE FINETTI’S THEOREM FOR  
THE NORMAL DISTRIBUTION
The normal distribution is the most widely used family of distribu-
tions in applied statistical work. It has two parameters, the mean, μ, 
and the variance, σ  2.
f (x|µ,σ 2) =
1
σ
2π
e−(x−µ)2 /2σ 2.

130 
CHAPTeR 7
Its probability density is the familiar bell- shaped curve shown in fig-
ure 7.4. Standard statistics problems begin: let X1, X2, . . . , Xn be inde-
pendent normal variables with mean μ and variance σ2. A Bayesian 
treatment needs a prior density for μ and σ  2. Then
P(X1 ≤x1, . . . , Xn ≤xn) =
Φµ,σ 2(x1). . .Φµ,σ 2
∫∫
(xn)υ(dµ,dσ 2)
with Φ(x) being the area to the left of x under the normal curve.
Can we find a symmetry characteristic of this setup, without ever 
mentioning the bell- shaped curve? The answer is a bit mathematical. 
Here is a leading special case. Suppose we know that the mean is 0, 
so that the only parameter left is the variance, σ  2. For this case, de 
Finetti’s theorem (proved by David Freedman) is as follows.
Theorem: Let X1, X2, X3, . . . be an infinite sequence of real- valued 
random variables that are exchangeable and satisfy the following, 
somewhat less than transparent, symmetry:
P(X1 ≤x1, . . . , Xn ≤xn) = P (X1 + X2)
2
≤x1,
⎛
⎝⎜
(X1 −X2)
2
≤x2, X3 ≤x3, . . . Xn ≤xn
⎞
⎠⎟.
f(x)
x
The normal distribution
µ
Figure 7.4. The normal distribution

 
UNIFICATION 
131
Then there exists a unique probability ν such that
P(X1 ≤x1, . . . , Xn ≤xn) = Φσ 2 (X1
0
∞
∫
). . .Φσ 2 (Xn) v(dσ 2).
Similar characterizations are available for most standard families. 
The challenge of finding more natural symmetry characterizations is 
ongoing. It seems a lot to ask. The usual justification of the normal 
distribution is via the central limit theorem. There are Bayesian ver-
sions of the central limit theorem, but blending these with de Finetti 
theory is still for the future.
MARKOV CHAINS
So far, so good, but you might have another worry— which de Finetti 
also shared. What if your degrees of belief are not exchangeable? What 
if order makes a difference? The simplest sort of patterns in the data 
stream that we might want to project are those in which the outcome 
of a trial tends to depend on that of the preceding trial. exchangeable 
degrees of belief cannot project such patterns.
Already in 1938, de Finetti suggested that exchangeability needed 
to be extended to a more general notion of partial exchangeability. 
The idea was that where full exchangeability fails, we might still have 
some version of conditional exchangeability. With respect to the kind 
of pattern just considered, the relevant condition would consist of 
the outcome of the preceding trial. The notion desired here is that of 
Markov exchangeability.
We are considering a loosening of the assumption of pattern-
lessness in the data stream to one where the simplest types of pat-
terns can occur; that is, those where the probability of an outcome 
can  depend on the probability of the previous outcome. One way 
to say this is that we loosen the assumption that the imputed 
chances make the trials independent to the assumption that the 
true chances make the trials Markov dependent. Here we can replace 
the example of the coin flip with that of the Markov thumbtack. 
A thumbtack is repeatedly flicked as it lies. It can come to rest in ei-
ther of two positions: point up (PU) or point down (PD). The chance 

132 
CHAPTeR 7
of the next state may well depend on the previous one. Thus there are 
unknown transition probabilities:
PU PD
PU P(PU | PU) P(PD | PU)
PD P(PU | PD) P(PD | PD),
which an adequate account of inductive inference should allow us to 
learn.
A stochastic process is Markov exchangeable if sequences of the same 
length having the same transition counts and the same initial state 
are equiprobable. David Freedman showed that any stationary Mar-
kov exchangeable process is representable as a mixture of stationary 
Markov chains.8 For another form of de Finetti’s theorem for Markov 
chains, we need the notion of a recurrent stochastic process. A state 
of a stochastic process is called recurrent if the probability that it is vis-
ited an infinite number of times is 1. The process is recurrent if all 
its states are recurrent. Diaconis and Freedman9 show that a recurrent 
Markov exchangeable stochastic process has a unique representation 
as a mixture of Markov chains.
MORE PARTIAL EXCHANGEABILITY
De Finetti also had other sorts of cases of partial exchangeability in 
mind in 1938. He conceptualizes the question of partial exchangeabil-
ity in general as modeling degree of analogy between events:
But the case of exchangeability can only be considered as a lim-
iting case: the case in which this “analogy” is, in a certain sense, 
absolute for all the events under consideration. . . . To get from 
the case of exchangeability to other cases which are more gen-
eral but still tractable, we must take up the case where we still 
encounter “analogies” among the events under consideration, 
but without attaining the limiting case of exchangeability.10
For his simplest example, he takes the case in which two odd- 
looking coins are flipped. If the coins look exactly alike, we may take 

 
UNIFICATION 
133
a sequence consisting of tosses of both to be exchangeable. If they look 
similar, we will want an appropriate form of partial exchangeability, 
where a toss of coin A may give us some information about coin 
B, but not as much as we would get from a toss of coin B. Later, he 
discusses a more interesting, but essentially similar, case in which 
animal trials of a new drug are partially exchangeable with human 
trials.
The trials of coin A are exchangeable, as those are of coin B, but 
they are not exchangeable with each other. One way to say this is to 
say that for a mixed sequence of trials of both coins, initial segments 
of the same length that have both the same numbers of heads on coin 
A and the same number of heads on coin B are equiprobable.
This case, that of Markov exchangeability, and many others can be 
brought under a general theory of partial exchangeability in which 
the concept of a sufficient statistic plays a key role. Sequences with 
the same value of a sufficient statistic have the same probability. For 
simple coin tossing, the statistic is the frequency of heads; for tossing 
several coins, it is the vector of frequencies of heads of the respective 
coins; for the Markov case, it is the vector of initial state and transi-
tion counts. In each case, under appropriate conditions we have (1) a 
de Finetti– type representation of degrees of belief as mixtures of the ex-
treme points of the convex set of probabilities for which that is a suf-
ficient statistic and (2) a convergence result to the effect that learning 
from experience will with probability one converge to one of these 
extreme points.
SUMMING UP
Bayes was the father of parametric Bayesian analysis. He analyzed a 
chance model— the sequence of independent and identically distrib-
uted coin flips. There is an unknown parameter— the bias of the 
coin. One puts a degree of belief prior on the parameter, and then one 
can reason inversely from data to a posterior degree of belief and pre-
dictively to anticipated new data. Although he, himself, did not put 
it in this way, he showed how chance, frequency, and degree of belief 
all interact to give statistical inference.

134 
CHAPTeR 7
De Finetti was the father of subjective Bayesian analysis. He took the 
old idea of symmetry and applied it to degrees of belief. He showed 
how the elements of Bayes’ chance model could all be seen as artifacts 
of such a symmetry, exchangeability. For cases where we do not have 
exchangeability, de Finetti showed the same idea applied to weaker 
symmetries: Markov exchangeability and more general forms of par-
tial exchangeability. Chance functions as a kind of place marker for 
the appropriate symmetry.
De Finetti showed us how to reason about chance if chance does 
not exist.
APPENDIX 1. ERGODIC THEORY AS  
A GENERALIZATION OF DE FINETTI
In an appendix to chapter 4 we briefly visited the 1931 ergodic theorem 
of George Birkhoff 11 as a general way of establishing a connection be-
tween probability and frequency. It is now appropriate to revisit it as a 
far- reaching generalization of de Finetti’s theorem.
What is a symmetry? It is a feature that is invariant under a group 
of transformations. This is given a lucid explanation in a classical 
book by Hermann Weyl.12 Think of invariance under rotations or 
reflections as marks of familiar physical symmetries. More generally, 
we have a set of states and a set of measurable subsets of it. A prob-
ability measure is invariant with respect to a group (or semigroup) of 
transformations, if every measurable set, A, has the same probability 
as its inverse image, T -1(A). As an especially important case of sym-
metry, stochastic processes invariant with respect to time shifts are 
said to be stationary.
A measurable set is invariant with respect to the transformations 
if the transformations take it into itself (except of a set of probability 
zero.) The probability measure is ergodic with respect to the transfor-
mations if the invariant sets either have probability one or probabil-
ity zero.
The ergodic decomposition theorem says that every invariant 
probability can be represented as an average (mixture) of ergodic 
ones. In particular, the ergodic decomposition tells us that stationary 

 
UNIFICATION 
135
processes are representable as mixtures of ergodic processes. An 
exchangeable probability on a sequence of two- valued random 
variables— one that is invariant under finite permutations of trials— is 
stationary. The ergodic probabilities of the decomposition make the 
coin flips independent and identically distributed. This is the de Finetti 
representation theorem all over again.
David Freedman’s 1962 generalization of de Finetti’s theorem 
to stationary Markov- exchangeable processes also uses ergodic 
representation.13
David Freedman
Freedman, in fact, proves a much more general result. A stationary 
stochastic process characterized by a kind of sufficient statistic is a 
mixture of ergodic measures characterized by that statistic.
We see now de Finetti’s point of view has a far- reaching general-
ization to probabilistic symmetries. The ergodic measures can be 
thought of as surrogates for the physical chance hypotheses. Just as 
in the coin- flipping case, we reason just as if we were reasoning about 
objective chance— but this is all cashed out as symmetry in our de-
grees of belief.

136 
CHAPTeR 7
APPENDIX 2. DE FINETTI’S THEOREM
ON EXCHANGEABILITY
De Finetti’s ideas connecting exchangeability and long-term fre-
quency are such a basic part of the foundations of chance that it
behooves us to work them out in more detail. This appendix gives
a careful statement of de Finetti’s theorem in both the 0/1 case
and the most general case. De Finetti’s theorem is a limit theorem;
inﬁnite exchangeable sequences are required. However, there are per-
fectly useful versions for ﬁnite exchangeable sequences and useful
quantitative estimates of the accuracy of the inﬁnite limits for ﬁnite
sequences. These ﬁnite-to-inﬁnite results give proofs for the most
general version of de Finetti’s theorem. It all boils down to a sim-
ple idea: if you have an urn with many balls, some red and some
white, and you take a sample of a small number, the results will
hardly depend on whether your sample is with or without replace-
ment. Making this precise takes a bit of notation and a bit of math.
However, all that is needed is the classical birthday problem, which
we treated in chapter 1.
DE FINETTI’S THEOREM FOR TWO-VALUED OUTCOMES
Consider binary outcomes. We will call them 0/1, but they might be
H/T for heads and tails or M/W for men and women (as the out-
comes of successive births in a family or hospital). We assume, after
reﬂection to be sure, that probabilities have been coherently assigned
to potential sequences, for example, P(0), P(1), P(0, 1), P(0, 1, 0), . . . .
The assignment P(·) is exchangeable if for any r ≥1 and all possible
sequences of r 0s and 1s, e1, e2, . . . , er, say, P doesn’t change if the ei are
permuted. Thus
P(01) = P(10)
when r = 2,
P(011) = P(101) = P(110),
P(001) = P(010) = P(100)
when r = 3,
and so on. The order doesn’t matter.

 
UNIFICATION 
137
It might be that P is only deﬁned for a ﬁxed r (e.g., r = 100). Say P is
extendable if for any R > r there is a ˜P deﬁned on sequences of length
R which is exchangeable and restricts to P for its ﬁrst r coordinates.
Thus more data of similar type can be contemplated. De Finetti’s
theorem now applies:
Theorem (de Finetti’s theorem for two values): Let P be an exchange-
able and extendable probability assignment to sequences of two
values. Then there exists a unique prior probability μ on [0, 1] such
that for every n and all sequences e1, e2, . . . , en,
P(e1, . . . , en) =
 1
0
θs(1 −θ)n−s μ(dθ),
(1)
with s = e1 + · · · + en, the number of 1s among {e1, e2, . . . , en}. More-
over P assigns probability one to the event
{proportion of ones in the ﬁrst n places has a limit θ}
(2)
and
P(θ ≤x) = μ(0, x].
(3)
Informally, a Bayesian with an exchangeable P is sure that long-
term frequencies exist (2) and that P may be represented as a mixture
of coin tossing (1). The mixing distribution μ is uniquely identiﬁed as
the long-term frequency distribution (3). An explanation and proof
of de Finetti’s theorem is given later in this appendix. It follows as a
special case of the general de Finetti theorem, our next topic.
GENERAL FORM OF DE FINETTI’S THEOREM
Observables of interest can be much richer than 0s and 1s. Rolling a
die leads to outcomes {1, 2, 3, 4, 5, 6}. The lengths of successive new-
borns can be any real number. Yearly temperature proﬁles give rise
to random curves. In general, let X be any set of possible outcomes.
A sequence of X-valued outcomes can be assigned probabilities. For
example, if A, B, C are subsets of X, P(A, B, C) is interpreted as the
probability assigned to the ﬁrst outcome is in A, the second outcome
is in B, and the third outcome is in C. An exchangeable assignment

138 
CHAPTeR 7
entails assigning the same probability to all arrangements. In the
example,
P(A, B, C) = P(A, C, B) = P(B, A, C) = P(B, C, A)
= P(C, A, B) = P(C, B, A).
Such assignments are often natural. Think about the lengths of the
ﬁrst three births in a large hospital (starting just past midnight) with
A = {length ≤20 inches},
B = {length > 18 inches},
C = {length between 16 and 25 inches}.
De Finetti’s theorem applies to essentially all such probabilities.∗
Theorem (General version of de Finetti’s theorem): Let P be an
exchangeable, extendable probability on sequences of outcomes in
a set X. Then there exists a unique prior distribution μ such that for
any n and any sequence of subsets A1, A2, . . . , An,
P(A1, A2, . . . , An) =

P
F(A1)F(A2) · · · F(An) μ(dF).
(4)
Moreover, for any subset A, P assigns probability one to the outcome
{proportion of ﬁrst n trials in A has a limit }
(5)
with
μ(L) = chance assigned by P to the limiting ratio  being in L. (6)
Remark: Let us try to explain some of the symbols in (4), (5), and (6).
It might help to compare with the parallel equations (1), (2), and (3).
To begin with, P on the left side of (4) is the given exchangeable
probability. On the right side of (4), P is the set of all probabilities
on X and μ is a probability distribution on P (a probability on prob-
abilities!). In the 0/1 case of the ﬁrst section, the set of probabilities
on X = {0, 1} can be identiﬁed with [0, 1], with θ in [0, 1] being the
chance of 1 on the next trial.
∗Technically, X must be a complete, separable metric space but such details will be
suppressed.

 
UNIFICATION 
139
In the present scenario, F is a ﬁxed probability on X and (4) says
that any exchangable P can be uniquely represented as a mixture of
independent and indentically distributed trials with common distri-
bution F. The mixing measure μ is a prior on F. It depends on P, as
(5), (6) show. In nonparametric Bayesian statistics, one often deﬁnes
P by specifying μ, hopefully after a good deal of reﬂection. This can
be a serious undertaking, but it is a hot topic on both the research
and applied frontiers.∗,†
In this chapter, we showed that theorems such as (4), (5), and (6)
can fail if only ﬁnite exchangeable sequences are considered. It turns
out that there is a perfectly useful ﬁnite version of de Finetti’s the-
orem; this is developed in the next section. The following sections
show how to take limits in the ﬁnite theorems to get the inﬁnite
theorems.
FINITE VERSIONS OF DE FINETTI’S THEOREM
Begin with the two-valued case. Fix n ≥2 and consider an exchange-
able assignment,
P(e1, e2, . . . , en),
ei 0 or 1.
The following argument shows that P can be uniquely represented
as a mixture of “urn measures.” Consider an urn containing n balls
with r balls labeled 1 and n −r balls labeled 0. If the urn is well mixed
and the balls are removed one by one in random order, a sequence
of length n results, containing r 0s and n −r 1s. Call this way of
generating sequences Pr: thus
Pr(e1, . . . , en) =
⎧
⎨
⎩
1
(n
r)
if e1 + · · · + en = r
0
otherwise.
∗J. K. Ghosh and R. V. Ramamoorthi, Bayesian Nonparametrics (Berlin: Springer, 2003).
†P. Diaconis and D. Freedman (1986). “On the Consistency of Bayes Estimates,” Annals of
Statistics 14 (1986): 1–26.

140 
CHAPTeR 7
Theorem (de Finetti’s theorem for ﬁnite binary sequences): Let P be
an exchangeable probability on binary sequences of length n. Then
there exists a unique prior distribution μ on {0, 1, . . . , n} such that for
any e1, . . . , en,
P(e1, . . . , en) =
n

r=0
Pr(e1, . . . , en) μ(r).
(7)
Moreover,
μ( j) = P{ j ones}.
(8)
Proof: The argument is so simple, we can give it in a line; it’s just the
law of total probability. For any set of sequences A,
P(A) =

P(A | no. 1s = r)P{no. 1s = r}.
By exchangeability, given r heads out of n, P assigns equal probability
to all sequences with r ones. Thus
P(A | no. 1s = r) = Pr(A).
The proof is completed by simply deﬁning μ(j) = P{no. 1s = j}, giv-
ing (7) and (8).
QED
There is nothing special about the balls in the urn being labeled
0/1. Let X be any set; now we really mean any set, real numbers, vec-
tors, curves, any set. Suppose an urn u contains n balls, each marked
by one or another of the set X (repeats allowed). Let Hu be the
probability distribution of n draws made at random without replace-
ment from u and Mu be the probability distribution of n draws made
at random with replacement; H stands for hypergeometric, M for
multinomial. These multinomials will enter in the following section.
Theorem (Finite form of de Finetti’s theorem, general case): Let X
be a set and P an exchangeable probability on sequences of length n
from X. Then there exists a unique probability μ(u) over urns such
that
P(A) =

u
Hu(A)μ(u),
A any set of sequences.
(9)

 
UNIFICATION 
141
In (9),
μ(u) is the chance that P assigns to sequences resulting in u.
(10)
The proof is precisely as in the binary case, using the law of total
probability. Technically, the sum should be replaced by an integral,
if X is inﬁnite. Again we suppress such niceties; see Diaconis and
Freedman∗for more details.
These theorems give unique representations à la de Finetti: the
most general exchangeable probability is a mixture of simple sam-
pling distributions. Because samples with and without replacement
are close, this will lead to exchangeable probabilities being close to a
mixture of multinomials. We turn to making this precise.
EXPLICIT BOUNDS IN FINITE THEOREMS
Let X be any set and P the set of all probabilities on X (for
measure-theoretic details, see Diaconis and Freedman). For F ∈P,
let Fk be the independent probability on sequences of length k, so
Fk(A1, . . . , Ak) = F(A1)F(A2) · · · F(Ak) for Ai subsets of X. If μ is a
probability on P, let
Pμk(A) =

P
Fk(A) μ(dF).
(11)
Let P be an exchangeable probability on sequences of length n.
For 1 ≤k ≤n, let Pk be the marginal distribution on the ﬁrst k
coordinates. Thus
Pk(A1, . . . , An) = P(A1, . . . , Ak, X, . . . , X).
Theorem: Let X be a set and P an exchangeable probability on
sequences of length n. Then there exists a probability μ on P such
that for all k ≤n and any set A,
Pk(A) −Pμk(A)
 ≤k(k −1)
2n
.
∗P. Diaconis and D. Freedman, “Finite Exchangeable Sequences,” Annals of Probability
8 (1980): 45–64.

142 
CHAPTeR 7
Thus, if n is large with respect to k2, the ﬁrst k coordinates
of an exchangeable probability are uniformly well approximated
by a mixture of independent identically distributed probabilities.
De Finetti’s theorem holds approximately! The result is often used
in reverse: suppose P is an exchangeable probability on sequences
of length k that can be extended to an exchangeable probability on
sequences of length n (we can imagine getting more, similar data).
Then P is almost a mixture of independent identically distributed
probabilities.
The proof is easy and instinctive. Recall the multinomial and
hypergeometric probabilities Mu and Hu generated by drawing with
or without replacement from an urn u containing n balls labeled with
various elements of X (repeats allowed). For 1 ≤k ≤n, let Muk, Huk be
the probabilities induced on sequences of length k.
Lemma: For any set A and any urn u,
|Muk(A) −Huk(A)| ≤k(k −1)
2n
.
Proof: It is without loss of generality to take X = {1, 2, . . . , n} and u =
{1, 2, . . . , n}. Then for any sequence x = (x1, . . . , xk) in X,
Muk(x) = 1
nk ,
Huk(x) =
⎧
⎪⎨
⎪⎩
1
n(n −1) · · · (n −k + 1)
if all xi distinct
0
otherwise.
It follows that the worst case for A is A = {x : all xi distinct}. Since
Huk(A) = 1 and Muk(A) = n(n −1) . . . (n −k + 1)/nk. For this A, direct
computation shows
|Muk(A) −Huk(A)| = 1 −n(n −1) · · · (n −k + 1)
nk
.

 
UNIFICATION 
143
Using (1 −x)(1 −y) = 1 −x −y + xy ≥1 −x −y (if x, y > 0), we see
n(n −1) . . . (n −k + 1)
nk
=

1 −1
n
 
1 −2
n

. . .

1 −k −1
n

≥1 −1 + · · · + k −1
n
= 1 −k(k −1)
2n
.
Thus, for any A,
|Muk(A) −Huk(A)| ≤k(k −1)
2n
.
QED
Remark: Note that the preceding calculations simply involve cal-
culating the chance that a sample of size k from {1, 2, . . . , n} with
replacement yields all distinct elements. This is just the birthday
problem of chapter 1. It is straightforward to show that for the cho-
sen A, |Muk(A) −Huk(A)| ≥1 −e−k(k−1)/2n. Thus our analysis is sharp;
k2/n must be small to have the two distributions close.
To conclude, let P be an exchangeable probability on sequences
of length n. We saw that P can be exactly represented as a mixture of
urn measures Hu. The mixing measure μ is simply induced by P: pick
from P, what’s the chance that the induced sample gives the urn u?
Now,
Pk(A) −Mμk(A)
 =


Huk(A) μ(du) −

Muk(A) μ(du)

≤

|Huk(A) −Muk(A)| μ(du) ≤k(k −1)
2n
.
This concludes the proof of the general de Finetti theorem.
Remark: Of course, the version of de Finetti’s theorem for arbitrary
X yields a theorem when X = {0, 1}. For this case, something sharper
can be said. By using a sharper bound for sampling with and with-
out replacement for urns with only two kinds of balls, Diaconis and
Freedman proved the following theorem.
Theorem: Let P be an exchangeable probability on binary sequences
of length n. Then there exists a probability μ on [0, 1] such that
|Pk(A) −Pμk(A)| ≤4k
n
for any set A.
Thus only k/n needs to be small (rather than k2/n being small).

144 
CHAPTeR 7
FROM FINITE TO INFINITE
The last step in deriving the usual version of de Finetti’s theorem for
inﬁnite exchangeable sequences, as before, requires taking limits in
the theorem of the last section. Let P be an exchangeable probability
on inﬁnite sequences of elements of a set X. Thus for any n and sets
A1, A2, . . . , An of X, P(A1, A2, . . . , An) is deﬁned, exchangeable, and
extendable. From the main theorem of the last section, there exists
a mixing measure, call it μn (because it depends on n), such that for
any k ≤n,
|P(A1, . . . , Ak) −Pμnk| ≤k(k −1)
2n
.
Fixing k, let n tend to inﬁnity. The μns have a limit μ in an appropri-
ate sense, so
P(A1, . . . , Ak) = Pμk.
Since this holds for all k with the same μ, P = Pμ follows.
The existence of the limit μn in the space of all probabilities
P is a classical part of functional analysis. Details are in Diaconis
and Freedman (Section 3). This limit is what forces mild restric-
tions on X. Indeed Dubins and Freedman∗show that P = Pμ can fail
for completely general X. The failure leans on exotic set-theoretic
notions such as the axiom of choice and nonmeasurable sets. For any
concievable real-world situation, de Finetti’s theorem holds.
There is a ﬁnitely additive version of the theorem which holds for
any X. See Diaconis and Freedman (Theorem 29).
∗L. Dubins and D. Freedman, “Exchangeable Sequences Need not Be Mixtures of Indepen-
dent, Identically Distributed Random Variables,” Zeitschrift für Wahrscheinlichkeits theorie und
verwandte Gebiete 48 (1979): 115–32.

