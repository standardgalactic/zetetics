Causal Structure Discovery
from Distributions Arising from Mixtures of DAGs
Basil Saeed 1 Snigdha Panigrahi 2 Caroline Uhler 1 3
Abstract
We consider distributions arising from a mixture
of causal models, where each model is repre-
sented by a directed acyclic graph (DAG). We
provide a graphical representation of such mixture
distributions and prove that this representation en-
codes the conditional independence relations of
the mixture distribution. We then consider the
problem of structure learning based on samples
from such distributions. Since the mixing variable
is latent, we consider causal structure discovery
algorithms such as FCI that can deal with latent
variables. We show that such algorithms recover a
“union” of the component DAGs and can identify
variables whose conditional distribution across
the component DAGs vary. We demonstrate our
results on synthetic and real data showing that the
inferred graph identiﬁes nodes that vary between
the different mixture components. As an imme-
diate application, we demonstrate how retrieval
of this causal information can be used to cluster
samples according to each mixture component.
1. INTRODUCTION
Determining causal structure from data is a central task
in many applications. (Friedman et al., 2000; Heckerman
et al., 1995) Causal structure is often modeled using a di-
rected acyclic graph (DAG), where the nodes represent the
variables of interest, and the directed edges represent the
direct causal effects between these variables (Pearl, 2009).
Assuming that the generating distribution of the data fac-
tors according to the DAG provides a way to relate the
conditional independence relations in the distribution to
separation statements in the DAG (known as d-separation)
1Laboratory for Information and Decision Systems and Institute
for Data, Systems and Society, Massachusetts Institute of Technol-
ogy, Cambridge, MA, USA 2Department of Statistics, University
of Michigan, Ann Arbor, MI, USA 3Department of Biosystems Sci-
ence and Engineering, ETH Zurich, Switzerland. Correspondence
to: Caroline Uhler <cuhler@mit.edu>.
through the Markov property (Lauritzen, 1996). When not
all variables of interest can be measured, DAGs are not
sufﬁcient to represent the observed distribution, since la-
tent variables may introduce confounding effects between
the observed variables. Instead, a family of mixed graphs
known as maximal ancestral graphs (MAGs) can be used
to model the observed variables by depicting the presence
of latent confounders between pairs of variables through
bidirected edges (Richardson and Spirtes, 2002).
With respect to learning the causal graph from data, the
most ubiquitous methods infer d-separation relations by es-
timating conditional independence relations from the data;
examples are the PC and GSP algorithms in the fully ob-
served setting, and the FCI algorithm in the presence of la-
tent variables (Spirtes et al., 2000; Solus et al., 2017; Zhang,
2008). These algorithms are consistent under the faithful-
ness assumption, which asserts that every conditional inde-
pendence relation in the distribution corresponds to a d-
separation relation in the graph. Note that even under faith-
fulness, the causal graph is in general not fully identiﬁable
from observational data; it can in general only be identiﬁed
up to its Markov equivalence class (Spirtes et al., 2000).
In various applications, data used for causal structure discov-
ery is heterogeneous in that it stems from different causal
models on the same set of variables (Gates and Molenaar,
2012; Chu et al., 2003; Ramsey et al., 2011). This is relevant
for example in biomedical applications, where the goal is
to learn a gene regulatory network based on gene expres-
sion data from a disease that consists of multiple not well
characterized subtypes (as is the case for many neurologi-
cal diseases). In such scenarios, the samples stem from a
mixture of different causal models on the same set of vari-
ables, and the causal effects of the mixture distribution can
in general not be faithfully represented by a single DAG.
Furthermore, a single DAG inferred from such samples can-
not identify differences between the component DAGs in the
mixture, which may be critical for personalized biomedical
interventions, and may lead to ﬂawed conclusions down-
stream.
In this work, we consider distributions arising as mixtures
of causal DAGs. Our main contributions are as follows:
arXiv:2001.11940v2  [stat.ML]  9 Aug 2020

Causal Structure Discovery from Distributions Arising from Mixtures of DAGs
• We introduce the mixture graph to represent such mix-
ture distributions. We prove that this graph encodes the
conditional independence relations in the mixture dis-
tribution through separation statements (Theorem 3.2)
and show that the separation statements in every such
graph can be realized by independence relations in
some mixture distribution (Proposition 3.5).
• We introduce the union graph, a graph deﬁned from
the mixture graph. We prove that, under a faithfulness
and ordering assumption on the DAGs in the mixture,
the FCI algorithm applied to data from a mixture of
DAGs outputs the union graph (Theorem 4.4).
• We prove that the union graph can be used to iden-
tify variables whose conditional distribution across
the component DAGs changes (Proposition 4.6). We
demonstrate the implication of this result for identify-
ing critical nodes and for clustering samples according
to their mixture component on synthetic data and data
from genomics.
2. PRELIMINARIES & RELATED WORK
2.1. Graphical representations: DAGs and MAGs
In this paper, we consider two types of graphs: directed
acyclic graphs (DAGs) and mixed graphs with directed (→)
and bidirected (↔) edges. We denote the former by D =
(V, E) and the latter by M = (V, D, B), where V denotes
the set of vertices, E and D denote the set of directed edges
and B denotes the set of bidirected edges. A mixed graph is
said to be ancestral if it has no directed cycles, and whenever
there is a bidirected edge u ↔v, then there is no directed
path from u to v (Richardson and Spirtes, 2002). While
ancestral graphs have been deﬁned more generally to allow
also for undirected edges, in this work we will only make
use of graphs with directed and bidirected edges.
Throughout, we will use the notation chM(v), paM(v) and
anM(v) to denote the children, parents and ancestors, re-
spectively, of a node v in the graph M. Furthermore, we
use the standard deﬁnitions of path and directed path in a
graph; for these deﬁnitions, see e.g. Lauritzen (1996). We
will use the notation v ↔M u as a shorthand to denote “the
edge v ↔u between nodes u, v in M”, and use similar
notations for other types of edges.
The notion of d-separation from DAGs can be generalized
to ancestral graphs by accounting for the new possible ways
to obtain a collider from bidirected edges (Richardson and
Spirtes, 2002). In ancestral graphs, unlike in DAGs, it is
possible to have a pair of nodes that are not adjacent, but can-
not be d-separated given any subset of nodes. An ancestral
graph where any non-adjacent pair of nodes is d-separated
given some subset of nodes is called maximal, and a non-
maximal ancestral graph can be made maximal by adding a
bidirected edge between all such pairs. An ancestral graph
that is maximal is called a Maximal Ancestral Graph (MAG)
(Richardson and Spirtes, 2002).
Ancestral graphs are a useful representation of DAGs with
unobserved nodes.
Speciﬁcally, Richardson and Spirtes (2002) showed that
given a DAG D = (V ∪L, E), with observed nodes V
and unobserved nodes L, satisfying a set of d-separation
statements of the form “A d-separated from B given C”
for disjoint A, B, C ⊆V , there exists an ancestral graph
M = (V, D, B) with the same d-separation statements,
called the marginal ancestral graph of D with respect to L.
Sadeghi et al. (2013) gave a local criterion to construct this
graph from D. Throughout our paper, we will make use of
this in the special case where L consists of a single node of
in-degree 0. The specialization of Sadeghi’s algorithm to
this case is provided in Algorithm 1.
Algorithm 1: Construction of the marginal ancestral graph
Input: DAG D = (V ∪{y}, E), where y has in-degree 0.
Output: the marginal ancestral graph of D w.r.t. y.
(0) Initialize D = ∅, B = ∅
(1) For u, v ∈chD(y): add u ↔v to B.
(2) For t, u, v such that (t →u) ∈E and (u ↔v) ∈B:
if u ∈anD(v), then add t →v to D.
(3) For u, v such that u ↔v ∈B: if u ∈anD(v), then
remove u ↔v from B and add u →v to D.
(4) Return the ancestral graph M = (V, D, B).
Although, in general, the ancestral graph constructed using
Sadeghi’s criterion is not maximal, the relevant restriction
considered here, i.e., when L consists of a single node with
in-degree 0, is always a MAG. The following proposition
states this; a proof is provided in section A of the Appendix
Proposition 2.1. The output of Algorithm 1 is a MAG.
2.2. Markov Properties
Given a graph M with nodes V , we associate to each node
v ∈V a random variable Xv and denote the joint distri-
bution of XV := (xv : v ∈V ) by pXV . The Markov
property associates missing edges in M with conditional
independence statements in pXV : a distribution pXV is
said to satisfy the Markov property with respect to M if
for any disjoint A, B, C ⊆V such that A and B are d-
separated given C in M, it holds that XA ⊥⊥XB | XC
in pXV (Lauritzen, 1996). For DAGs, an equivalent con-
dition to the Markov property is for pXV to factorize as
pXV (xV ) = Q
v∈V p(xv|xpaG(v)); see Lauritzen (1996).
Considering latent variables XL, Richardson and Spirtes
(2002) showed that given a distribution pXV ,XL that is

Causal Structure Discovery from Distributions Arising from Mixtures of DAGs
Markov with respect to a DAG D over V ∪L, the marginal
pXV (xV ) = P
xL pXV ,XL(xV , xL) is Markov with respect
to the marginal ancestral graph of D with respect to L.
It is possible for two different DAGs D1, D2 over the same
set of nodes to satisfy the same set of d-separation state-
ments. In this case, D1 and D2 are said to be Markov equiv-
alent, and the set of all DAGs that are Markov equivalent
to a DAG D is called the Markov equivalence Class of D.
These deﬁnitions trivially extend to MAGs. The Markov
equivalence class of a MAG can be represented by a partial
ancestral graph (PAG): the edges in such a graph have three
types of tips: arrowheads (←), tails (−) and circles ◦−,
where arrowhead (tail) signiﬁes that this arrowhead exists in
all graphs in the Markov equivalence class (Zhang, 2008).
2.3. Causal Structure Discovery
The goal of structure learning is to recover the graph D
or M from data generated from the distribution pXV . This
task often requires assumptions beyond the Markov property.
One common such assumption is the so-called faithfulness
assumption which states that for any disjoint A, B, C ⊆V ,
it holds that A and B are d-separated given C whenever
XA ⊥⊥XB | XC in pXV (Spirtes et al., 2000). The faithful-
ness assumption allows making inference about the structure
of D or M from conditional independence tests on the data.
Various algorithms have been proposed for this task that
are provably consistent, such as the PC, GES or GSP algo-
rithms for learning DAGs (Spirtes et al., 2000; Chickering,
2002; Solus et al., 2017), and the FCI algorithm for learning
MAGs (Spirtes et al., 2000). Note that even under the faith-
fulness assumption, it is in general only possible to retrieve
the Markov equivalence class of a graph D or M from data;
this is the output of the above algorithms. For example,
FCI in general does not return a speciﬁc MAG, but a PAG
representing a Markov equivalence class of MAGs.
2.4. Causal Inference from Mixtures of DAGs
While the problem of learning appropriate representations
from data of DAG mixtures arises in various applications,
little work has been done on theory and methodology in
this direction. Spirtes (1994) investigated the conditional
independence properties of such mixture distributions; he
deﬁned a cyclic graphical model derivable from the com-
ponent DAGs and proved that the mixture distribution
is Markov with respect to it. However, this graph does not
capture the full set of conditional independence relations
for any reasonable mixture. In fact, as we discuss later,
this graph is similar to the representation we deﬁne in Sec-
tion 4, which also only provides partial information about
the structure of the component DAGs. To capture the full
set of independences in the mixture distribution, a represen-
tation sparser than that of Spirtes (1994) is necessary. Strobl
(2019a;b) built on this work to deﬁne a sparser graph. How-
ever, we provide examples in Section B of the Appendix
showing that the Markov condition in general does not hold
for this graph, i.e., there can be d-separation statements in
the graph that do not correspond to conditional indepen-
dence relations in the mixture distribution. Finally, Ramsey
et al. (2011) provided conditions for the mixture distribu-
tion to be representable by a graph that is a union of the
component DAGs.
To learn the component DAGs from mixture data, a sim-
ple approach is to cluster the data using, for example, the
Expectation-Maximization (EM) algorithm and then learn
a DAG from each cluster. This, however, uses a reduced
sample size to learn each DAG (corresponding to the size of
the associated cluster). In the case where the cluster labels
are known and the DAGs are related, Wang et al. (2020)
showed that learning each DAG separately can lead to loss
in accuracy compared to when the full sample size is used
to learn the DAGs jointly. When the expectation in the EM
algorithm can be computed, as e.g. for Gaussians, Thiesson
et al. (1997) proposed a heuristic approach based on the EM
algorithm to directly learn the component DAGs from the
mixture data. In this work, we consider a different prob-
lem. Instead of learning the component DAGs we provide
a graphical representation of the mixture distribution and
identify critical aspects of the component DAGs that are cap-
tured by this graph and can be identiﬁed by algorithms such
as FCI when applied directly to the mixture distribution.
3. MIXTURE DAG AND MARKOV
PROPERTY
In this section, we provide our ﬁrst main result: after for-
mally introducing distributions that arise as mixtures of
DAGs, we deﬁne the mixture DAG and prove in Theorem 3.2
and Proposition 3.5 that it is a valid representation of the
model, i.e., the DAG encodes the conditional independence
relations of the mixture distributions. More precisely, not
only is the Markov condition satisﬁed (i.e., all separation
statements in the mixture DAG correspond to conditional
independence relations in the mixture distribution), but in
addition, every mixture DAG is also realizable by a mixture
distribution (meaning that the mixture DAG cannot be made
sparser without losing the Markov property).
3.1. Mixture of Causal DAGs
To introduce the mixture model, we consider K DAGs
{D(1), . . . , D(K)} with D(j) = (V, E(j)) for 1 ≤j ≤K,
i.e., these K DAGs are deﬁned on the same set of nodes.
Associated with each component DAG D(j) is a random vec-
tor XV with distribution p(j)(xV ). Let V INV denote the set
of nodes that are invariant across the K component DAGs,

Causal Structure Discovery from Distributions Arising from Mixtures of DAGs
i.e., nodes whose conditional distribution in the factorization
does not vary across D(1), . . . , D(K); that is
V INV=
n
v ∈V : p(j)(xv|xpaD(j)(v)) = p(k)(xv|xpaD(k)(v))
for all j, k ∈{1, 2, · · · , K}
o
.
(1)
Assuming that each distribution p(j)(xV ) admits a factor-
ization according to DAG D(j), we then obtain:
p(j)(xV ) =
Y
v∈V \V INV
p(j)(xv|xpaD(j)(v))
Y
v∈V INV
p(j)(xv|xpaD(j)(v))
=
Y
v∈V \V INV
p(j)(xv|xpaD(j)(v))
Y
v∈V INV
p(1)(xv|xpaD(1)(v))
for all 1 ≤j ≤K, i.e., each distribution decouples into two
components: one over the variables associated with V INV
that remains constant across all K distributions, and another
over the remaining variables which may differ with j.
Let J be a discrete variable taking values in {1, . . . , K}
with probabilities pJ(j) for each j ∈{1, . . . , K}. Deﬁning
a joint distribution pµ over XV ∪J by
pµ(xV , j) := pJ(j) · p(j)(xV ),
(2)
this joint distribution satisﬁes p(j)(xV ) = pµ(xV |J = j)
and the observed mixture distribution is obtained by
marginalizing pµ over the unobserved index variable J.
With a slight abuse of notation, we denote the resulting
mixture distribution also by pµ. Given samples from this
distribution, i.e., without knowledge of the membership of
each sample to its generating DAG, we analyze what can
still be inferred regarding the structure of D(1), . . . , D(K).
3.2. Mixture DAG and Markov Property
We now present the mixture DAG, a DAG that is represen-
tative of the independence relations induced amongst the
observed variables after marginalizing over the index vari-
able J in (2). Denoting the number of vertices in V by |V |,
the mixture DAG is a graph on K ·|V |+1 nodes constructed
by placing the K component DAGs next to each other, giv-
ing rise to a DAG on K · |V | nodes, and using an additional
node to represent J. We now provide the precise deﬁnition.
Deﬁnition 3.1 (Mixture DAG). Let v(j) denote vertex v in
DAG j and let [V ] := ∪1≤j≤KV (j) denote the vertices of
the K component DAGs. The mixture DAG, denoted by Dµ,
has nodes [V ] ∪{y} and edges Eµ consisting of edges in
each component DAG, namely
K
[
j=1
n
v(j) →˜v(j) : v, ˜v ∈V, v →˜v ∈E(j)o
,
and additional edges from node y to some nodes in [V ],
namely those corresponding to variables that have condi-
tionals that are not the same for all j, i.e.,
K
[
j=1
n
y →v(j) : v ∈V \ VINV}.
Figure 1 provides an example of the mixture DAG arising
from a mixture with K = 2 and |V | = 4. Note that, while
the results of this section hold even when
the DAGs D(j) have no common topological ordering
(meaning that there exists no ordering π such that v < u
in π only if u ̸∈anD(j)(v) for all 1 ≤j ≤K), the
mixture DAG is sparsest, and hence provides informa-
tion about the component DAGs through separation state-
ments, when a common topological ordering exists (as in
Figure 1). When there is no common ordering, the set
VINV is generally smaller, since paD(j) ̸= paD(k) implies
p(j)(xv|xpaD(j)(v)) ̸= p(k)(xv|xpaD(k)(v)), which implies a
denser mixture DAG.
We emphasize here that the DAG in Deﬁnition 3.1 is not a
graphical model representation of the mixture distribution
in the standard sense. This is already clear from the fact that
the mixture DAG has K ·|V |+1 nodes, whereas the mixture
distribution is only |V |-dimensional. Yet, in the following
theorem we show that it is possible to read off conditional
independence relations that hold in the mixture distribution
pµ from the mixture graph in an intuitive manner.
For A ⊂V , we use the notation [A] to denote all K copies
of the nodes in A, i.e., A = ∪1≤j≤KA(j).
Theorem 3.2 (Markov Property). Let A, B, C ⊆V be dis-
joint. If [A] and [B] are d-separated given [C] in the mixture
DAG Dµ, then XA⊥⊥XB|XC in the mixture distribution pµ.
To illustrate this result, consider the example in Figure E.1.
Since [1] = {1(1), 1(2)} and [4] = {4(2), 4(2)} are d-
separated given ∅in the mixture DAG, then the mixture
distribution pµ(x1, x2, x3, x4) satisﬁes X1 ⊥⊥X4.
We note that while the graphical representation provided
by Strobl (2019b) (the mother graph) is similar to the mix-
ture DAG, it critically differs in how the component DAGs
are connected via the node y. Importantly, we show in Sec-
tion B in the Appendix that the mixture distribution pµ is
not Markov with respect to the mother graph1.
In the following, we provide a proof for Theorem 3.2. For
each 1 ≤j ≤K, let eD(j) be the sub-DAG induced by
Dµ on the vertices V (j) ∪{y}.The main ingredient of the
proof is the following lemma, which connects d-separation
statements in the mixture DAG to conditional independence
1Strobl (2019a;b) provides two different constructions; we
show that the Markov property does not hold in either.

Causal Structure Discovery from Distributions Arising from Mixtures of DAGs
(a) D(1)
(b) D(2)
(c) Dµ
(d) M(1)
(e) M(2)
(f) M∪
Figure 1: (a)-(b): component DAGs for a mixture model with K = 2; (c): corresponding mixture DAG (see Deﬁnition 3.1);
(d)-(e): associated component MAGs (see Section 4); (f): associated union graph (see Deﬁnition 4.2).
relations in the mixture distribution via d-separation in eD(j).
Lemma 3.3. Let A, B, C ⊆V be disjoint. If for all 1 ≤
j ≤K it holds that
(a) A(j) and B(j) are d-separated given C(j), and;
(b) A(j) and y are d-separated given C(j) in eD(j),
then XA ⊥⊥J | XC in pµ, implying the factorization
p(j)(xA, xB|xC) = p(1)(xA|xC)p(j)(xB|xC)
for all 1 ≤j ≤K.
We now provide the proof for Theorem 3.2.
Proof of Theorem 3.2. We start by showing that the condi-
tions of Lemma 3.3 are satisﬁed. First, note that [A] and [B]
are d-separated given [C] in Dµ implies that A(j) and B(j)
are d-separated given C(j) in D(j) for all 1 ≤j ≤K. Sec-
ond, note that since y has in-degree 0, we cannot have both
a d-connecting path given [C] between [A] and y and one
between [B] and y in Dµ. Hence, we may assume without
loss of generality that [A] and y are d-separated given [C]
(otherwise, [B] and y are d-separated given [C]).
We now use Lemma 3.3 to show that pµ(xA, xB|xC) fac-
torizes as fA(xA, xC)fB(xB, xC), which would prove that
XA ⊥⊥XB|XC in pµ. By deﬁnition of pµ in (2),
pµ(xA, xB|xC) =
K
X
j=1
p(j)(xA, xB|xC)pJ(j),
and hence as a consequence of Lemma 3.3 we obtain
pµ(xA, xB|xC) =
K
X
j=1
p(1)(xA|xC)p(j)(xB|xC)pJ(j)
= p(1)(xA|xC)
K
X
j=1
p(j)(xB|xC)pJ(j),
providing a factorization of the desired form.
In Theorem 3.2, we established that every separation state-
ment in the mixture DAG Dµ corresponds to a conditional
independence relation in the mixture distribution pµ. Next,
we show that every mixture DAG is realizable, i.e., that
for any mixture DAG Dµ, there exists a pµ whose condi-
tional independence relations are faithfully represented by
the separation statements of Dµ. This implies that Dµ is the
“correct” graphical representation of a mixture of DAGs and
cannot be made sparser without losing the Markov property.
3.3. Faithfulness
We deﬁne faithfulness of a mixture distribution pµ with re-
spect to a mixture DAG Dµ analogously to how faithfulness
is deﬁned for a distribution with respect to a DAG model.
Deﬁnition 3.4 (Mixture Faithfulness). The mixture distri-
bution pµ is faithful with respect to a mixture DAG Dµ if
for any disjoint A, B, C ⊆V with XA ⊥⊥XB|XC in pµ it
holds that [A] and [B] are d-separated given [C].
We next provide an example showing that mixture faith-
fulness is not implied by faithfulness of each component
distribution p(j) with respect to the corresponding DAG
D(j). Hence, to establish realizability of the mixture graph,
it is not sufﬁcient to rely on the fact that for every DAG
D(j), there exists a distribution p(j) that is faithful to it.
Example 1. Consider the distributions p(1)(xV ), p(2)(xV )
on V = {1, 2, 3, 4} that factor according to the DAGs
D(1), D(2), respectively, shown in Figure 1. Namely
p(1)(xV ) = p(1)(x1)p(1)(x2|x1)p(1)(x3)p(1)(x4),
p(2)(xV ) = p(2)(x1)p(2)(x2)p(2)(x3|x4)p(2)(x4),
where
p(1)(x1) = N(x1; 0, 1),
p(2)(x1) = N(x1; 0, 1),
p(1)(x2|x1) = N(x2; x1, 1),
p(2)(x2) = N(x2; 0, 2),
p(1)(x3) = N(x3; 0, 1),
p(2)(x3|x4) = N(x3; x4, 1),
p(1)(x4) = N(x4; 0, 1),
p(2)(x4) = N(x4; 0, 1).

Causal Structure Discovery from Distributions Arising from Mixtures of DAGs
Then, deﬁning pµ(xV ) := P2
j=1 p(j)(xV )pJ(j), for some
J ∼pJ(j), we obtain that
pµ(x2, x3) =
Z
pµ(xV )dx1dx4
=
Z
pJ(1)p(1)(x1)p(1)(x2|x1)p(1)(x3)p(1)(x4)dx1dx4
+
Z
pJ(2)p(2)(x1)p(2)(x2)p(2)(x3|x4)p(2)(x4)dx1dx4
= pJ(1) N(x2; 0, 2) N(x3; 0, 1)
+ pJ(2) N(x2; 0, 2) N(x3; 0, 2)
= N(x2; 0, 2)

pJ(1)N(x3; 0, 1) + pJ(2)N(x3; 0, 2)

= f(x2)g(x3),
which implies that X2 ⊥⊥X3 in pµ, although in the mixture
DAG corresponding to pµ shown in Figure 1 the nodes 2
and 3 are d-connected via the path through y.
This example was carefully crafted; even a slight per-
turbation such as choosing p(2)(x2) = N(x2; 0, 2.001)
would have meant that pµ(x2, x3) does not factor, indi-
cating that mixture-faithfulness violations are rare. More
precisely, consider the family of Gaussian mixture mod-
els where each p(j) is a Gaussian distribution that is
faithful with respect to D(j).
A violation of mixture-
faithfulness occurs if and only if P
j p(j)(xA, xB|xC) fac-
tors as pµ(xA|xC)pµ(xB|xC), i.e.,
X
j
p(j)(xA, xB|xC) =
X
i
p(i)(xA|xC)
X
j
p(j)(xB|xC),
when [A] and [B] are d-connected given [C] in Dµ. This
represents an equality constraint on the parameters of the
Gaussians p(j) for 1 ≤j ≤K. As a consequence, mixture-
faithfulness holds almost surely and any Dµ is realizable by
a mixture of Gaussians, thereby proving the following.
Proposition 3.5 (Realizability of Dµ). For any mixture
DAG Dµ, there exists a mixture distribution pµ that is faith-
ful with respect to Dµ.
4. LEARNING FROM MIXTURE DATA
Without knowing the membership of each sample to a com-
ponent DAG, we cannot generally learn the structure of
D(j) for each j from the data. Since the mixing variable is
latent, an intuitive approach is to apply FCI to learn a MAG
representation of pµ. In this section, we will characterize
the output of FCI. In particular, we will show that FCI iden-
tiﬁes critical nodes in the component DAGs: those whose
conditionals across the component DAGs vary.
A difﬁculty for structure discovery using MAG-based learn-
ing algorithms such as FCI, is that even under the mixture-
faithfulness assumption the conditional independence rela-
tions in a mixture distribution pµ may not be representable
by any MAG. We illustrate this in the following example
and then provide conditions to avoid this phenomenon.
Example 2. Consider Dµ shown in Figure 2a. We show
that there does not exist any MAG f
M over the variables
V = {1, . . . , 5} that satisﬁes: A d-sep from B given C in
f
M if and only if [A] d-sep from [B] given [C] in Dµ. First,
note that such a MAG would need to have the same skeleton
as the graph in Figure 2b to respect the adjacencies in Mµ.
Otherwise it would have an extra or missing d-separation
with no analog in Mµ. In addition, f
M would also need
to contain the colliders 4 →5 ←2 and 1 →2 ←5
to respect the d-separation relations resulting from 4(2) →
5(2) ←y →2(2) and 1(1) →2(1) ←y →5(1) respectively.
This implies the existence of 2 ↔f
M 5. Further note that
conditioning on either [2], [3] or [4] (or any subset of these)
connects [5] and [1] in Dµ which are d-separated given ∅.
The only orientation of arrowheads compatible with both
the skeleton and these separation/connection relations is
2 →3 →4. Hence, 4 ∈de f
M(2). Finally, the existence
of an arrowhead 4←∗5 would violate the separation: [5]
d-separated from [1] given ∅. Hence, 2 ↔f
M 5 and 2 ∈
an f
M(5), violating the ancestral property.
We now identify a class of mixture models for which
the d-separations in the mixture DAG are equivalent to d-
separation statements in a MAG.
Deﬁnition 4.1. Let M(j) be the MAG constructed via Al-
gorithm 1 from the induced sub-DAG eD(j) deﬁned in Sec-
tion 3.2. The MAGs M(1), . . . , M(K) are said to be com-
patible with the same poset if there exists a partial order
π on V such that for all 1 ≤j ≤K it holds that (a) u ∈
anM(j)(v) ⇒u <π v; and (b) u ↔M(j) v ⇒u ̸≶π v.
Figures 1d and 1e show examples of MAGs M(j) that sat-
isfy this poset compatibility condition. One can further
check that the MAGs M(1) and M(2) associated with the
mixture DAG in Figure 2a do not satisfy this condition. This
example shows that there exist DAGs D(1), . . . , D(K) with a
common topological ordering whose corresponding MAGs
M(1), . . . , M(K) do not satisfy the poset compatibility con-
dition 4.1. On the other hand, it can be readily veriﬁed that
the compatibility assumption on M(1), . . . , M(K) implies
that D(1), . . . , D(K) have a common topological ordering.
In the following, we show that poset compatibility ensures
that d-separation relations in Dµ are representable by a
MAG, which we call the union graph since it is obtained as
a union of the edges of M(1) . . . , M(K).
Deﬁnition 4.2 (Union Graph). The union graph M∪:=
(V, D∪, B∪) has vertices V , directed edges
D∪= {v →u : u, v ∈V, ∃jvj →M(j) uj},
and bidirected edges
B∪= {v ↔u : v, u ∈V, ∃jvj ↔M(j) uj}.

Causal Structure Discovery from Distributions Arising from Mixtures of DAGs
We remark that Spirtes (1994) studied a similar graph and
proved the Markov property for a DAG with vertices V ∪{y}
and directed edges given by the union of D(1), . . . , D(K).
An example of a union graph M∪is given in Figure 1f. In
general, M∪may neither be maximal nor ancestral (see
Figure 2b for an example). However, the following lemma
states that under poset compatibility it is guaranteed to be
both. The proof is given in Section D of the Appendix,
Lemma 4.3. Under the assumption that M(1) . . . , M(K)
are compatible with the same poset, M∪is a MAG.
We now state the main results of this section, characterizing
the output of FCI when run on mixtures of DAGs.
Theorem 4.4. Let A, B, C ⊆V be disjoint. If the compo-
nent MAGs satisfy the poset compatibility assumption, then
A and B are d-separated given C in M∪if and only if [A]
and [B] are d-separated given [C] in Dµ.
The proof is provided in Section E in the Appendix. The
following corollary follows directly from the asymptotic
consistency of FCI (Spirtes et al., 2000).
Corollary 4.5. If the distribution pµ is faithful with respect
to a mixture DAG whose component MAGs satisfy the poset
compatibility assumption 4.1, then FCI outputs the Markov
equivalence class of the corresponding union MAG M∪.
We end this section by pointing out an important structural
property of M∪, which can be used to recover key informa-
tion about the component distributions in the mixture. We
leave the proof to Section F of the Appendix.
Proposition 4.6. A bidirected edge u ↔v in the union
graph M∪implies that u ∈V \ VINV. Additionally, this
implies that p(j)(xu|xpaD(j)(u)) ̸= p(i)(xu|xpaD(i)(u)).
Hence bidirected edges identify nodes in the component
DAGs whose conditional distribution varies across mixture
components. As we show in the following section, these
nodes are natural candidates for features when clustering.
5. EXPERIMENTS
5.1. Synthetic Data
In the following, we demonstrate the effectiveness of learn-
ing the union graph from mixture data, analyze the perfor-
mance when estimating V \ VINV using Proposition 4.6,
and investigate the performance of clustering using mixture
data when V \ VINV are used as features.
We generated K component DAGs each with |V | = 10
nodes and the same topological ordering from an Erd¨os-
R´enyi model with expected degree d = 1.5/K so that the
nodes in the M∪have expected degree less than 1.5. From
these DAGs, the corresponding MAGs M(j) were computed
using Algorithm 1. If the MAGs were not compatible with
the same poset, the DAGs were discarded to ensure poset-
compatibility (2 out of 270 graphs were discarded).
Data was sampled from each DAG based on a linear
structural equation model with additive Gaussian noise,
where each edge weight (u, v) was sampled uniformly in
[−2, −0.25] ∪[0.25, 2] (to ensure that it was bounded away
from zero) and set to be equal for the edges (u(j), v(j))
for all 1 ≤j ≤K if this edge existed in DAG D(j). In
this case, v ∈VINV if and only if the parents of Xv are
the same across all K DAGs. The mean for the Gaus-
sian noise was sampled uniformly in [−2, 2] with stan-
dard deviation 1. From each DAG D(j), we generated
n pj observations where PK
j=1 pj = 1 yielding a total
of n samples. For the plots in the main paper, we chose
pj = 1/K. We present additional plots in Appendix G for
when p = (pk : 1 ≤k ≤K) is sampled from a Dirichlet
distribution.
Learning the Union MAG. To evaluate Corollary 4.5, we
ran the R implementation of FCI from the pcalg library
on this synthetic data using Gaussian conditional indepen-
dence tests (despite the true distribution being a mixture of
Gaussians) with threshold α. The output is a PAG bP∪repre-
senting the Markov equivalence class of the union graph. As
comparison, we computed the true union graph M∪based
on the MAGs M(j), generated n samples from this graph
(using a structural equation model with the same parameters
as in the mixture) and ran FCI on these samples to obtain an
estimate eP∪for the PAG of the union graph. This offsets the
estimation errors that are intrinsic to FCI. The difference be-
tween the PAGs bP∪and eP∪was measured via a normalized
structural Hamming distance; the structural Hamming dis-
tance (SHD) between PAGs counts the occurrences of ∗→
in one of the PAGs versus ∗−in the other, plus the number
of adjacencies present in one graph but not the other. The
normalization is done by dividing over the possible number
of errors for the realization at hand to keep the value in [0, 1]
and make the numbers comparable. Figure 2e shows the
normalized SHD averaged over 30 realizations of synthetic
datasets. We used K = 4 and n = 5000 in this plot; in
Section G in the Appendix, we provide plots for K ∈{2, 6}
and n ∈{1000, 10000}.
Identifying Nodes in V \VINV. To evaluate Proposition 4.6,
we estimated V \ VINV by determining all nodes incident
to bidirected edges in the PAG bP∪estimated using FCI.
This set was compared to the ground truth; Figure 2f shows
true positive and false positive rates for varying signiﬁcance
levels2, averaged over 30 realizations. We used K = 4 and
2We do not use ROC plots since while increasing the threshold
monotonically increases the true positive rate of the estimated ad-
jacencies, it generally does not monotonically increase the number
of correctly inferred edge orientations.

Causal Structure Discovery from Distributions Arising from Mixtures of DAGs
(a)
(b)
(c)
(d)
(e)
(f)
(g)
(h)
Figure 2: (a) shows a mixture DAG Dµ and (b) shows the associated union graph M∪. In this model, each DAG D(j) has
a common topological ordering, however, the union MAG is not ancestral; (c) shows the output of FCI on genes in the
apoptosis pathway using mixture data without knowledge of the cluster membership for each sample, while (d) shows the
difference graph of Wang et al. (2018) on the same genes learned when cluster membership of each sample is known; (e)
shows the average normalized SHD between the PAG bP∪estimated using the mixture data, and eP∪estimated using data
sampled from M∪; (f) shows the true and false positive rate in estimating V \ VINV; (g) shows the performance of clustering
when the set [V \ VINV] has no descendants in Dµ, while (h) shows the same plot when [V \ VINV] has descendants in Dµ.
n = 5000 in this plot. In Section G in the Appendix, we
show plots for K ∈{2, 6} and n ∈{1000, 10000}.
Clustering. Under mixture-faithfulness, XV \VINV repre-
sents the set of nodes whose conditionals vary across the
component DAGs. This motivates using the nodes XV \VINV
and their descendents as features for clustering since these
are the only nodes with different marginals across the mix-
ture components. Since FCI generally cannot identify all the
descendents of XV \VINV, we used only XV \VINV for cluster-
ing. As a proof-of-concept demonstrating that these features
can be useful, we considered two settings, one in which
[V \ VINV] has no descendants in Dµ (see Figure 2g), and
another one in which this set has descendants (Figure 2h).
In both settings, we used eK-means clustering for various
values of ˜K.
To compare the quality of clustering us-
ing [V \ VINV] versus all nodes as features, we used the
V-measure score from Rosenberg and Hirschberg (2007)
which is based on ground truth cluster assignments; a higher
score represents better performance. As per what is expected
from our theoretical results, Figure 2g shows that clustering
based on the reduced number of features [V \VINV] results in
higher quality clusters as compared to using all features for
clustering in the setting where [V \VINV] has no descendants
in Dµ, while otherwise both feature sets perform equally.
5.2. Real Data
Ovarian Cancer. We applied this framework to gene ex-
pression data from ovarian cancer in K = 2 patient groups
(with 93 and 168 observations, respectively) with different
survival rates (Tothill et al., 2008). We followed the analysis
of Wang et al. (2018), where the difference-DAG was esti-
mated for the two groups based on the apoptosis pathway
consisting of |V | = 10 genes. The resulting difference-
DAG is shown in Figure 2d. While the difference-DAG
can identify edges that are different between the two DAGs
D(1) and D(2) and hence provides more information than
the union graph, computing the difference-DAG requires
knowledge of the membership of each observation to the
two disease subgroups, which is not available for many
diseases. The estimated PAG eP∪based on the combined
samples from the two patient groups is shown in Figure 2c.
It was estimated using FCI with stability selection. FCI
identiﬁed BIRC3 as the node with the highest number of
incident bidirected edges; BIRC3 is known to be one of the
major disregulated genes in ovarian cancer and an inhibitor
of apoptosis (Johnstone et al., 2008; J¨onsson et al., 2014).
T cell activation. We also applied our framework to single-
cell gene expression data of naive and activated T cells
(i.e. K = 2, with 298 and 377 samples, respectively)

Causal Structure Discovery from Distributions Arising from Mixtures of DAGs
from Singer et al. (2016). Following the analysis in Wang
et al. (2018), we performed the analysis on 60 genes that had
a fold expression change above 10. The FCI output on these
60 nodes is shown in Section G.2 in the Appendix. The fol-
lowing nodes have the highest number of incident bidirected
edges, indicating that they may play important roles in T cell
activation: CDC6, CDC20, SHCBP1, NKG2A, GZMB4 and
KIF2C. All these genes have been discribed before as criti-
cal: CDC6 and CDC20 are essential regulators of the cell
division cycle. Shorter cell cycle time for increased prolifer-
ation is a hallmark of T cell activation (Qiao et al., 2016;
Borlado and M´endez, 2008). SHCBP1 has been shown to
be tightly linked to cell proliferation and strongly correlates
with proliferative stages of T cell development (Schmandt
et al., 1999; Buckley et al., 2014). NKG2A functions to
limit excessive activation, prevent apoptosis, and preserve
the speciﬁc T cell response (Rapaport et al., 2015). GZMB4
has been shown to regulate antiviral T cell response (Salti
et al., 2011). Finally, the gene KIF2C encodes a Kinesin-like
protein that functions as a microtubule-dependent molecular
motor. It is over-expressed in a variety of solid tumors and
induces frequent T cell responses (Gnjatic et al., 2010).
6. DISCUSSION
In this paper, we provided a graphical representation (via
the mixture DAG) of distributions that arise as mixtures of
causal DAGs. We showed that the mixture DAG not only
satisﬁes the Markov property with respect to such mixture
distributions, but is also always realizable by a mixture dis-
tribution, meaning that it cannot be made sparser without
losing the Markov property. In addition, we characterized
the output of the prominent FCI algorithm when applied
to data from such mixture distributions. FCI is a natural
candidate in this setting due to the presence of the latent
mixing variable. We proved that FCI can identify variables
whose conditionals vary across the different components
and showed how this property can be used to infer cluster
membership of samples. This is relevant for many applica-
tions, as for example when studying diseases consisting of
multiple not well characterized subtypes. In such studies, ge-
nomic perturbation experiments can now be performed rela-
tively routinely, leading to high-throughput interventional
data. In future work it would be interesting to study how in-
terventional data could be used to enhance causal inference
based on mixtures of DAGs or which interventions to per-
form in order to enhance identiﬁability of pathways that are
shared among the different subtypes as well as those that are
different across the subtypes for personalized interventions.
ACKNOWLEDGEMENTS
Basil Saeed was partially supported by the Abdul Latif
Jameel Clinic for Machine Learning in Health at MIT. Caro-
line Uhler was partially supported by NSF (DMS-1651995),
ONR (N00014-17-1-2147 and N00014-18-1-2765), IBM,
and a Simons Investigator Award.
REFERENCES
Borlado, L. R. and M´endez, J. (2008). CDC6: from DNA
replication to cell cycle checkpoints and oncogenesis.
Carcinogenesis, 29(2):237–243.
Buckley, M. W., Arandjelovic, S., Trampont, P. C., Kim,
T. S., Braciale, T. J., and Ravichandran, K. S. (2014). Un-
expected phenotype of mice lacking SHCBP1, a protein
induced during T cell proliferation. PloS ONE, 9(8).
Chickering, D. M. (2002). Optimal structure identiﬁca-
tion with greedy search. Journal of Machine Learning
Research, 3(Nov):507–554.
Chu, T., Glymour, C., Scheines, R., and Spirtes, P. (2003).
A statistical problem for inference to regulatory structure
from associations of gene expression measurements with
microarrays. Bioinformatics, 19(9):1147–1152.
Friedman, N., Linial, M., Nachman, I., and Pe’er, D. (2000).
Using Bayesian networks to analyze expression data.
Journal of Computational Biology, 7(3-4):601–620.
Gates, K. M. and Molenaar, P. C. (2012). Group search
algorithm recovers effective connectivity maps for in-
dividuals in homogeneous and heterogeneous samples.
NeuroImage, 63(1):310–319.
Gnjatic, S., Cao, Y., Reichelt, U., Yekebas, E. F., N¨olker,
C., Marx, A. H., Erbersdobler, A., Nishikawa, H., Hilde-
brandt, Y., Bartels, K., et al. (2010). NY-CO-58/KIF2C is
overexpressed in a variety of solid tumors and induces fre-
quent T cell responses in patients with colorectal cancer.
International Journal of Cancer, 127(2):381–393.
Heckerman, D., Mamdani, A., and Wellman, M. P. (1995).
Real-world applications of Bayesian networks. Commu-
nications of the ACM, 38(3):24–26.
Johnstone, R. W., Frew, A. J., and Smyth, M. J. (2008). The
TRAIL apoptotic pathway in cancer onset, progression
and therapy. Nature Reviews Cancer, 8(10):782–798.
J¨onsson, J.-M., Bartuma, K., Dominguez-Valentin, M.,
Harbst, K., Ketabi, Z., Malander, S., J¨onsson, M.,
Carneiro, A., M˚asb¨ack, A., J¨onsson, G., et al. (2014).
Distinct gene expression proﬁles in ovarian cancer linked
to Lynch syndrome. Familial Cancer, 13(4):537–545.
Lauritzen, S. L. (1996). Graphical Models, volume 17.
Clarendon Press.
Pearl, J. (2009). Causality. Cambridge University Press.

Causal Structure Discovery from Distributions Arising from Mixtures of DAGs
Qiao, R., Weissmann, F., Yamaguchi, M., Brown, N. G.,
VanderLinden, R., Imre, R., Jarvis, M. A., Brunner,
M. R., Davidson, I. F., Litos, G., et al. (2016). Mecha-
nism of APC/CCDC20 activation by mitotic phosphory-
lation. Proceedings of the National Academy of Sciences,
113(19):E2570–E2578.
Ramsey, J., Spirtes, P., and Glymour, C. (2011). On meta-
analyses of imaging data and the mixture of records. Neu-
roImage, 57(2):323–330.
Rapaport, A. S., Schriewer, J., Gilﬁllan, S., Hembrador,
E., Crump, R., Plougastel, B. F., Wang, Y., Le Friec, G.,
Gao, J., Cella, M., et al. (2015). The inhibitory receptor
NKG2A sustains virus-speciﬁc CD8+ T cells in response
to a lethal poxvirus infection. Immunity, 43(6):1112–
1124.
Richardson, T. and Spirtes, P. (2002).
Ancestral graph
Markov models. The Annals of Statistics, 30(4):962–
1030.
Rosenberg, A. and Hirschberg, J. (2007). V-measure: A
conditional entropy-based external cluster evaluation mea-
sure. In Proceedings of the 2007 Joint Conference on
Empirical Methods in Natural Language Processing and
Computational Natural Language Learning (EMNLP-
CoNLL), pages 410–420.
Sadeghi, K. et al. (2013). Stable mixed graphs. Bernoulli,
19(5B):2330–2358.
Sadeghi, K. and Lauritzen, S. (2014). Markov properties for
mixed graphs. Bernoulli, 20(2):676–696.
Salti, S. M., Hammelev, E. M., Grewal, J. L., Reddy, S. T.,
Zemple, S. J., Grossman, W. J., Grayson, M. H., and
Verbsky, J. W. (2011). Granzyme B regulates antiviral
CD8+ T cell responses. The Journal of Immunology,
187(12):6301–6309.
Schmandt, R., Liu, S. K., and McGlade, C. J. (1999).
Cloning and characterization of mPAL, a novel Shc SH2
domain-binding protein expressed in proliferating cells.
Oncogene, 18(10):1867–1879.
Singer, M., Wang, C., Cong, L., Marjanovic, N. D., Kowal-
czyk, M. S., Zhang, H., Nyman, J., Sakuishi, K., Kurtu-
lus, S., Gennert, D., et al. (2016). A distinct gene mod-
ule for dysfunction uncoupled from activation in tumor-
inﬁltrating T cells. Cell, 166(6):1500–1511.
Solus, L., Wang, Y., Matejovicova, L., and Uhler, C. (2017).
Consistency guarantees for permutation-based causal in-
ference algorithms. arXiv preprint arXiv:1702.03530.
Spirtes, P. (1994). Conditional independence properties in
directed cyclic graphical models for feedback. Technical
report, Carnegie Mellon University.
Spirtes, P., Glymour, C. N., and Scheines, R. (2000). Cau-
sation, Prediction, and Search. MIT press.
Strobl, E. V. (2019a). The global Markov property for a
mixture of DAGs. arXiv preprint arXiv:1909.05418.
Strobl, E. V. (2019b). Improved causal discovery from
longitudinal data using a mixture of DAGs. In Proceed-
ings of Machine Learning Research, volume 104, pages
100–133.
Thiesson, B., Meek, C., Chickering, D. M., and Heckerman,
D. (1997). Learning mixtures of DAG models. In Pro-
ceedings of the Fourteenth Conference on Uncertainty in
Artiﬁcial Intelligence, pages 504–513.
Tothill, R. W., Tinker, A. V., George, J., Brown, R., Fox,
S. B., Lade, S., Johnson, D. S., Trivett, M. K., Etemad-
moghadam, D., Locandro, B., et al. (2008). Novel molec-
ular subtypes of serous and endometrioid ovarian cancer
linked to clinical outcome. Clinical Cancer Research,
14(16):5198–5208.
Wang, Y., Segarra, S., and Uhler, C. (2020).
High-
dimensional joint estimation of multiple directed Gaus-
sian graphical models. Electronic Journal of Statistics.
Wang, Y., Squires, C., Belyaeva, A., and Uhler, C. (2018).
Direct estimation of differences in causal graphs. In Ad-
vances in Neural Information Processing Systems, pages
3770–3781.
Zhang, J. (2008). Causal reasoning with ancestral graphs.
Journal of Machine Learning Research, 9(Jul):1437–
1474.

Causal Structure Discovery from Distributions Arising from Mixtures of DAGs
APPENDIX
A. Proof of Proposition 2.1
We begin by recalling the deﬁnition of an inducing path from Richardson and Spirtes (2002), specialized to ancestral graphs.
Deﬁnition A.1. A path v1, . . . , vn in an ancestral graph G is inducing if v1 and vn are not adjacent in G and for all
i ∈{2, . . . , n −1}, we have
vi−1 ↔vi ↔vi+1
and
vi ∈anG({v1, vn}).
Richardson and Spirtes (2002) showed the following condition for an ancestral graph to be maximal.
Lemma A.2 ((Richardson and Spirtes, 2002)). An ancestral graph M is maximal if and only if G does not contain any
inducing paths.
This allows us to prove Proposition 2.1.
Proof of Proposition 2.1. We show that the graph resulting from Algorithm 1 does not contain inducing paths. Let M be
the output of the algorithm. Suppose we have vertices v1, . . . , vn where vi−1 ↔vi ↔vi+1 for all i ∈{2, . . . , n −1} in
M. Then by step 1 of the algorithm, we must have v1, . . . , vn ∈chDµ(y), implying that v1 ↔M vn, and hence the path is
not inducing.
B. Counter-example for the Markov property of the mother graph
In the following, we provide a counter-example for the Markov property of the mother-graph representation introduced
by Strobl (2019b;a). We ﬁrst remark that the Markov property in Strobl (2019a) generalizes that of Strobl (2019b) in the
following sense: if the Markov property of the latter is satisﬁed, then the former is satisﬁed. Hence, we here provide a
counter-example for the former, which can serve as a counter-example for both.
(a) D(1)
(b) D(2)
(c) Dm
Figure B.1: (c) shows the mother graph Dm associated with the DAGs D(1) and D(2) in (a) and (b).
We start by recalling a few deﬁnitions from Strobl (2019b) using notation native to our development. Given a mixture
of DAGs with distribution pµ where p(j) factorizes according to D(j), the mother graph Dm = (Vm, Dm) has nodes
Vm := [V ] ∪{y(1), . . . , y(K)} and directed edges
Dm :=
[
1≤j≤K
{y(j) →v(j) : v ∈V \ VINV} ∪{u(j) →v(j) : u →D(j) v}.
An example of the mother graph is shown in Figure B.1. A variable c(j) ∈[V ] in the mother graph is called an m-collider if
and only if at least one of the following conditions hold:
• a(j) →c(j) ←b(j), where a, b ∈V ∪{y}
• a(j) →c(j) ←y(j) and y(k) →c(k) ←b(k) where a, b ∈V .
An m-path exists between [A] and [B] in the mother graph if and only if there exists a sequence of triples between [A] and
[B] such that at least one of the following two conditions is true for each triple in the sequence:

Causal Structure Discovery from Distributions Arising from Mixtures of DAGs
• a(j)∗−∗c(j)∗−∗b(j) with a, b, c ∈V ∪{y}
• a(j) →c(j) ←y(j) and y(k) →c(k) ←b(k) where a, b, c ∈V .
Finally, [A] and [B] are said to be m-d-connected given [C] if and only if there exists an m-path between [A] and [B] such
that the following two conditions hold:
• c(j) ∈[C] for every m-collider on the path, where c ∈V
• a(j) ̸∈[C] for every non-m-collider on the path, where c ∈V ∪{y}.
Now, the Markov property for the mother graph states that if [A] and [B] are not m-d connected given [C] in the mother
graph, then XA ⊥⊥XB | XC in pµ (Strobl, 2019b;a).
We now provide a counter example for this Markov property. For this, consider the mother graph in Figure B.1c over
V = {1, 2, 3, 4}. Note that according to the deﬁnition of m-d-connection, [{1}] and [{4}] are not m-d-connected given
[{2, 3}]. Hence, the Markov property should imply that X1 ⊥⊥X4|X2, X3 in any mixture distribution whose mother graph
is as shown. In the following, construct a mixture distribution where this is not satisﬁed.
For simplicity, let pJ(1) = pJ(2) = 1
2. Deﬁne p(1)(xV ) as
p(1)(x1) = N(x1; 0, 1),
p(1)(x2|x1) = N(x2; x1, 1),
p(1)(x3) = N(x3; 0, 1),
p(1)(x4) = N(x4; 0, 1),
and p(2)(xV ) as
p(2)(x1) = N(x1; 0, 1),
p(2)(x2) = N(x2; 0, 1),
p(2)(x3|x4) = N(x3; x4, 1),
p(2)(x4) = N(x4; 0, 1).
Clearly, p(1)(xV ) and p(2)(xV ) factorize according to D(1) of Figure B.1a and D(2) of Figure B.1b, respectively. Now,
pµ(x1, x2, x3, x4) =
X
j∈{1,2}
pJ(j)p(j)(x1, x2, x3, x4)
= 1
2
1
(2π)2

e−
x2
1
2 e−
x2
3
2 e−
x2
4
2 e−(x2−x1)2
2
+ e−
x2
1
2 e−
x2
2
2 e−
x2
4
2 e−(x3−x4)2
2

= 1
2
1
(2π)2 e−
x2
1
2 e−
x2
2
2 e−
x2
3
2 e−
x2
4
2

ex2x1e−
x2
1
2 + ex3x4e−
x2
4
2

,
which cannot be written as
f(x1, x2, x3)g(x2, x3, x4)
for any f, g, implying that X1 ̸⊥⊥X4 | X2, X3 in pµ.
C. Proof of Lemma 3.3
Proof of Lemma 3.3. By the assumption, p(j1)(xV ) factors according to D(j1). Hence, it is sufﬁcient to deﬁne a distribution
epXV ,J(xv, j) over XV ∪{J} that factors according to eD(j), with J ∈{j1, j2} for an arbitrarily chosen j2 ∈{1, . . . , K} \
{j1}, such that
epXV |J(xV |j1) = p(j1)(xV ).

Causal Structure Discovery from Distributions Arising from Mixtures of DAGs
Then, the factorization with respect to eD(j1) along with the two d-separation statements in the hypothesis of the lemma
would imply
p(j1)(xA, xB|xC) =
P
xV \(A∪B∪C) p(j1)(xV )
P
xV \C p(j1)(xV )
=
P
xV \(A∪B∪C) ep(xV |j1)
P
xV \C ep(xV |j1)
= ep(xA, xB|xC, j1)
= ep(xA|xC)ep(xB|xC, j1).
To complete the proof, we deﬁne such a distribution ep. First let Vy := ch e
D(j1)(y) and note that
ep(xV , j) = epJ(j)
Y
v∈V
ep(xv|xpa e
D(j1)(v), j)
= epJ(j)
Y
v∈Vy
ep(xv|xpaD(j1)(v), j)
Y
v∈V \Vy
ep(xv|xpaD(j1)(v)).
Deﬁne
epJ(j) :=
(
pJ(j1)
j = j1
1 −pJ(j1)
j = j2
,
ep(xv|xpaD(j)(v)) := p(xv|xpaD(j)(v))
∀v ∈V \ Vy.
Now, for each v ∈Vy, deﬁne
U(v) := paD(j1)(v) ∩paD(j2)(v)
and
D(v) := paD(j2)(v) \ paD(j1)(v),
and choose an arbitrary ﬁxed value for xpaD(i)(v) \ paD(j)(v) and denote it by x′
d(v).
Then deﬁne for all v ∈Vy,
ep(xv|xpaD(j)(v), j) :=
(
pXv|Xpa
D(j1) (v),J(xv|xpaD(j1)(v), j1)
j = j1
pXv|XU(v),XD(v),J(xv|xU(v), x′
d(v), j2)
j = j2
.
Now, one easily checks that this distribution indeed satisﬁes the factorization property, which completes the proof.
D. Proof of Lemma 4.3
The ancestral property follows directly since we impose the order compatibility assumption of Deﬁnition 4.1. In the
following, we show maximality using the deﬁnition of inducing path and the associated maximality condition in Section A.
Proof of Lemma 4.3. Suppose we have a path v1 ↔v2 ↔. . . vn−1 ↔vn in M∪. Then, for all m ∈{1, . . . , n −1},
we must have some j ∈{1, . . . , K} such that v(j)
m ↔v(j)
m+1 in M(j), implying that for all m, we must have a j such
that v(j)
m , v(j)
m+1 ∈chD(j)(y) and hence a j such that v(j)
m , v(j)
m+1 ∈chDµ(y). But by construction of Dµ, this implies
that v(j)
m v(j)
m+1 ∈chDµ(y) for all j ∈{1, . . . , K}. Therefore, for any j, we have v(j)
1
· · · , v(j)
n
∈chD(j)(y), and hence
Algorithm 1 adds an edge between v(j)
1
and v(j)
n
in M(j), resulting in an edge between v1 and vn in M∪. Therefore, the
path v1, . . . , vn is not inducing in M∪.

Causal Structure Discovery from Distributions Arising from Mixtures of DAGs
(a) Mµ
(b) M(1)
(c) M(2)
(d) M∪
Figure E.1
E. Proof of Theorem 4.4
Since we assume that A, B, C ⊆V , i.e., these sets do not contain y, then [A] and [B] are d-separated in Dµ given [C] if and
only if they are d-separated in the marginal MAG of Dµ w.r.t. {y} obtained from Algorithm 1. We refer to this MAG as the
mixture MAG and denote it by Mµ. We will make use of this MAG in parts of the following proof since it simpliﬁes the
arguments.
One thing to note about Mµ is that if we remove the edges of the form u(j)◦−◦v(i) for u, v ∈V and i ̸= j, then we obtain
a bijection between the edges of Mµ and the union of all the edges of M(j) for all j. Figure E.1 illustrates this for an
example. Hence, we can alternatively think of the union graph as having directed edges
D∪:= {u →v : u, v ∈V, ∃i u(i) →Mµ v(i)},
and bidirected edges
B∪:= {u ↔v : u, v ∈V, ∃i u(i) ↔Mµ v(i)}.
We prove Theorem 4.4 in 3 main steps. First, in Lemma E.5 we show that for any d-connecting path between a and b given
C in M∪, we can ﬁnd a d-connecting path between a(i) and b(k) given [C] in Mµ. Second, in Lemma E.7 we show the
converse: that for any d-connecting path a(i) and b(k) given [C] in Mµ, we can ﬁnd a d-connecting path between a and b
given C in M∪. Finally, in Lemma E.8 we show that this equivalence implies that for any disjoint sets A, B, C ⊆V , A and
B are d-separated in M∪if and only if [A] and [B] are d-separated in Mµ given [C].
The proof strategy in Lemmas E.5 and E.7 relies on concatenating d-connecting paths given C of the form P1 = ⟨v1, . . . , vn⟩
and P2 = ⟨vn . . . , vm⟩together to create longer d-connecting paths given C of the form P = ⟨v1, . . . , vm⟩. When doing so,
we must take care to ensure that vn is active on the longer path, i.e., we must ensure that vn is a collider on the path P if and
only if vn ∈C.
E.1. A connecting path in M∪implies an analogus one in Mµ
We begin by proving some auxiliary results for step 1.
Lemma E.1 (Bidirected Connections). If a(i) ↔Mµ b(k) for any i, k ∈{1, . . . , K}, then a(i) ↔Mµ b(j) for all j ∈
{1, . . . , K} \ {i}.
Proof. a(i) ↔Mµ b(k) implies that a(i), b(k) ∈chDµ(y). By construction of Dµ, this implies a(j), b(j) ∈chDµ(y) for
all j ∈{1, . . . , K}, and hence step 1 of Algorithm 1 will add the bidirected edges a(i) ↔b(j) for all j ∈{1, . . . , K}.
Step 3 will only remove it if a(i) and a(j) are ancestors of one another in Dµ, which could happen only if j = i. Hence,
a(i) ↔Mµ b(j) for all j ∈{1, . . . , K} \ {i}.
Lemma E.2 (Bidirected district). Assume a(i) ↔Mµ b(j) and c(k) ↔Mµ d(l).
• If i ̸= l, then a(i) ↔Mµ d(l).
• If i = l, then
– a(i) ↔Mµ d(l) if neither a(i), d(l) is an ancestor of another in Mµ,

Causal Structure Discovery from Distributions Arising from Mixtures of DAGs
– a(i) →Mµ d(l) if a(i) ∈anMµ(d(l)); or
– a(i) ←Mµ d(l) if d(l) ∈anMµ(a(i)).
Proof. a(i) ↔Mµ b(j) and c(k) ↔Mµ d(l) implies that a(ι), b(ι), c(ι), d(ι) ∈chDµ(y) for all ι ∈{1, . . . , K}. Hence, step 1
of Algorithm 1 will add a(i) ↔Mµ d(l). If i ̸= l, then a(i) and d(l) cannot be ancestors of one another, implying that step 3
will not remove this bidirected edge. If i = l, then the edge will be removed and replaced with the appropriate directed edge
if one of a(i) or d(l) is an ancestor of the other. Otherwise, the bidirected edge will remain.
Lemma E.3 (Arrow tip lemma). Under the ordering assumption in Deﬁnition 4.1, if a directed edge a →M∪b exists in
M∪, then we must have aj →Mµ bj for some j in Mµ. If a bidirected edge a ↔M∪b exists in M∪, then we must have
aj ↔Mµ bj for some j in Mµ.
Proof. The proof follows directly from the deﬁnition of the union graph.
Lemma E.4 (Changing Arrowtips Lemma). Under the ordering assumption in Deﬁnition 4.1, if a(j)∗→Mµb(j) but not
a(k)∗→Mµb(k) (same type of edge) for some j ̸= k, then we must have b(j) ↔b(k).
Proof. The ordering assumption does not allow a(j) →Mµ b(j) and a(k) ↔Mµ b(k) (and vice versa). Hence, we must only
look at the existence of a(j)∗→b(j) and the in-existence of an edge between a(k) and b(k).
First, we note that if step 1 of Algorithm 1 deﬁning Mµ adds b(j) ↔b(k), then it will remain since step 2 does not
modify edges but only adds them, while step 3 will never remove an edge b(j) ↔b(k) since neither can be an ancestor or a
descendant of the other in Dµ.
Now, if a(j) →D(j) b(j) but not a(k) →D(k) b(k) for some k, then we must have b ∈V \ V I and hence b(ι) ∈chDµ(y) for
all ι ∈{1, . . . , K} by construction of Dµ. Therefore, step 1 of Algorithm 1 will add b(j) ↔b(k).
For the other case we must check that a(j)◦→b(j) was added by the algorithm that created Mµ. In all steps, the algorithm
will only add such an edge if b ∈V \ VINV and hence b(j) ↔b(k) must have been added in step 1.
Lemma E.5 (Step 1). Under the ordering compatibility assumption in Deﬁnition 4.1, if there is a connecting path between
a and b given some C ⊆V \ {a, b} in M∪ending in an arrow head (or tail respectively) incident to b, then there is a
connecting path between a(i) and b(k) given [C] in Mµ for some i, k ∈{1, . . . , K} that also ends in an arrow head (or tail
respectively) towards b(k).
Proof. We use induction on the number of edges in the connecting path in M∪. The base case for 1 edge follows directly
from Lemma E.3.
Now assume we have a d-connecting path given C consisting of m + 1 edges in M∪: P∪= ⟨a, . . . , d, b⟩ending in an
arrow head (or tail respectively). Consider the sub-path ⟨a, . . . , d⟩with m edges. By the inductive hypothesis, there is a
path Pµ = ⟨a(i), . . . , d(j)⟩in Mµ that is d-connecting given [C], for some i, j, ending in the same tip. In the following, we
show that we can always ﬁnd a path of the form ⟨d(j), . . . , b(k)⟩for some k that can be joined together with Pµ to create a
path ⟨a(i), b(k)⟩that is d-connecting given [C]. We do this by considering all the different cases for the tips of the edges
c∗−∗M∪d and d∗−∗M∪b.
Before discussing the different cases, note that if the edge d(j)∗−∗Mµb(k) exists and is of the same type as the edge
d∗−∗M∪b, then we can create the desired d-connecting path ePµ from a(j) to b(i) given [C] by concatenating this edge with
Pµ, since:
d is active on Q∪⇒
 d is a collider on Q∪⇔d ∈C

⇒
 d(j) is a collider on ePµ ⇔d(j) ∈[C]

⇒d(j) is active on ePµ,

Causal Structure Discovery from Distributions Arising from Mixtures of DAGs
where the second implication follows because the path Pµ ends in the correct type of arrow tip by the inductive hypothesis
(I.H.). Hence, in what follows, it is sufﬁcient to
assume either d(j)∗−∗b(k) is not in Mµ or is not the same edge type as d∗−∗b in M∪.
(S.3)
(i) case c∗→d←∗b in M∪:
Since d is a collider on the path Q∪, we must have d ∈C, and hence d(j) ∈[C] for all j. Hence the path Pµ is of
the form ⟨a(i), . . . , γ(ι), d(j)⟩, where γ(ι)∗→d(j) for some γ ∈V and ι ∈{1, . . . , K} by the I.H. Furthermore, by
Lemma E.3, we must have d(ι)←∗Mµb(k) for some ι, k ∈{1, . . . , K}. Since we assumed in (S.3) that this isn’t true
for ι = j, then by Lemma E.4 we must have d(j) ↔d(ι), creating the path ∗→d(j) ↔d(ι)←∗b(k) that is d-connected
given [C] (recall d(j), d(ι) ∈[C]). Concatenating d(j) ↔d(ι)←◦b(k) to Pµ gives the desired d-connecting path
⟨a(i), . . . , d(j), d(ι), b(k)⟩.
For the remaining cases, we begin by recalling that the edge d∗−∗M∪b must exist since d(k)∗−∗Mµb(k) for some k by
Lemma E.3. Now, let α(j) be the node on the path Pµ closest to a(i) such that all nodes between α(j) and d(j) have the
same index j, i.e., all of these are contained in the same MAG M(j). This means that the node preceding α(j) on this path,
call it γ(κ), either has a different index (i.e., a part of a different M(κ)), or α(j) = a(i).
Call P (j)
µ
= ⟨α(j), . . . , d(j)⟩the subpath of Pµ from α(j) to d(j). This path is completely contained in M(j). If it is possible
to ﬁnd a path P (k)
µ
= ⟨α(k), . . . , d(k)⟩in M(k) that is analogous to P (j)
µ
(same types of edges), then we can replace the
segment P (j)
µ
of Pµ with P (k)
µ
to obtain a connecting path between a(i) and d(k) given [C]. Then, concatenating d(k)∗−∗b(k)
gives us the desired connecting path from a(i) to b(k) given [C] in Mµ.
Hence, in checking the remaining cases, we further
assume that it is not possible to ﬁnd a path P (k)
µ
in M(k).
(S.4)
Therefore, walking along the path P (j)
µ
backwards starting at d(j) until α(j), we will eventually ﬁnd an edge β(j)∗−∗δ(j)
such that β(k)∗−∗δ(k) is not an edge. Take the ﬁrst such edge. Now, if this edge was β(j) ↔δ(j), then by Lemma E.1, we
must have β(j) ↔δ(k), implying that we can concatenate the subpath of Pµ of the form ⟨a(i), . . . , β(j)⟩with β(j) ↔δ(k)
and the subpath of P (k)
µ
of the form ⟨δ(k), . . . , b(k)⟩to create the desired d-connecting path given [C]. Next we look at the
situations where we do not have β(j) ↔δ(j), considering each remaining case on the arrowheads of c∗−∗d∗−∗b in M∪
separately.
(ii) case c ←d →b in M∪: This case is depicted in Figure E.2a. If the ﬁrst edge found is of the form β(j) ←δ(j) where
β(k) ←δ(k) is not present (see Figure E.2b), then by Lemmas E.4 and E.2, we must have β(j) ↔b(k) (Figure E.2d).
Replacing the segment ⟨β(j), . . . , d(j)⟩of Pµ with β(j) ↔b(k) gives the desired path.
Otherwise, if we have β(j) →δ(j) instead (Figure E.2c), then Lemmas E.4 and E.2 again say that we must have
δ(j) ↔b(k) (Figure E.2e). The subpath of Pµ of the form ⟨δ(j), c(j)⟩shown in Figure E.2e is connecting given [C]
by the I.H. Starting at δ(j) and walking towards c(j), we can ﬁnd a collider that is in [C] (shown in Figure E.2f).
This collider must be a descendant of δ(j) Hence, δ(j) is active given [C] on the path β(j) →δ(j) ↔b(k) since it is
a collider whose descendant is in [C]. Replacing the segment ⟨β(j), . . . , d(j)⟩in Pµ with this path gives the desired
connecting path given [C].
(iii) case c →d →b in M∪: Proceeding similarly, if the edge found is of the form β(j) ←δ(j), then we must have
β(j) ↔b(k) similar to before and for the same reasons. Furthermore, we can ﬁnd a d-connecting path by performing a
concatenation similar to the one we did before: replace the segment ⟨β(j), . . . , d(j)⟩of Pµ with β(j) ↔Mµ b(k). This
is illustrated in Figure E.3a,
If, otherwise, the edge found is of the form β(j) →δ(j). We can conclude that we have the bidirected edge δ(j) ↔d(k)
by applying the Lemmas E.2 and E.4 again.

Causal Structure Discovery from Distributions Arising from Mixtures of DAGs
If there is a collider on the subpath between ⟨δ(j), . . . , c(j)⟩, then any such collider must be in [C] since Pµ is d-
connecting given [C] (see Figure E.3b). Furthermore, one of these colliders will be a descendant of δ(j), and we can
apply similar logic to that in Case (ii) to show that the path obtained by replacing the segment ⟨β(j), . . . , d(j)⟩of Pµ
with δ(j) ↔d(k) is d-connecting given [C].
Otherwise, no such collider exists between δ(j) and c(j) and hence c(j) is a descendant of δ(j) (see Figure E.3c).
Therefore, b(k) is a descendant of δ(k) by the ordering compatibility assumption, and Algorithm 1 adds the directed
edge δ(k) →b(k) since δ(k) and b(k) will both be in chDµ(y). This further implies that δ(j), b(j) ∈chDµ(y), so
Algorithm 1 will add an edge between these two nodes. The ordering assumption once again ensures that this edge is
of the form δ(j) →b(j).
(iv) case c ←d ←b in M∪. Proceeding similarly, if we have the edge β(j) →δ(j), then we can follow the same logic to
create the d-connecting path (see Figure E.4a).
Otherwise, β(j) ←δ(j), and we have the bidirected edge β(j) ↔d(k), and we again check for colliders between β(j)
and d(j).
If there is a collider, it will be both in [C] and a descendant of d(k) inMµ, and we can ﬁnd the desired d-connecting
path with the same logic followed previously (see Figure E.4b).
If there is no such collider, then β(j) will be a descendant of d(j), and using a similar argument to that used for
Figure E.3c, we can conclude that we have directed edges β(j) ←Mµ d(j) and β(k) ←Mµ d(k) (see Figure E.4c). In
such a scenario, we can repeat the logic for the node β in place of the node c: we continue walking along the path
P (j)
µ
starting from β(j) until α(j) is reached or until we ﬁnd another edge along this path that does not exist on P (k)
µ . If
the former happens ﬁrst, we deal with the case like we would have if P (k)
µ
and P (j)
µ
had identical edges. If the latter
happens ﬁrst, then we recursively repeat the logic of case (iv).
This completes the proof.
E.2. A d-connecting path in Mµ implies an analogous d-connecting path in M∪
Again, we begin with some auxiliary results.
(a)
(b)
(c)
(d)
(e)
(f)
Figure E.2: An illustration of the logic in the proof of Lemma E.5, case (ii). We do not plot all possible edges in order
to reduce clutter. Instead, we plot non-edges using an x superimposed on a dashed line. Furthermore, we indicate paths
between two nodes with a squiggly line. (a), (b) and (c) show the relevant segment of the path Pµ in blue; (d), (e) and (f)
show the segment that replaces ⟨β(j), . . . , d(j)⟩on Pµ to create the desired d-connecting path in blue.

Causal Structure Discovery from Distributions Arising from Mixtures of DAGs
(a)
(b)
(c)
Figure E.3: An illustration of the d-connecting paths constructed by following the logic of case (iii) in the proof of
Lemma E.5. In each of (a), (b) and (c), the segment that replaces ⟨β(j), . . . , d(j)⟩on Pµ to create the desired d-connecting
path is colored in blue.
(a)
(b)
(c)
Figure E.4: An illustration of the d-connecting paths constructed by following the logic of case (iv) in the proof of lemma E.5.
In each of (a) and (b), the segment that replaces ⟨β(j), . . . , d(j)⟩on Pµ to create the desired d-connecting path is colored in
blue.
Lemma E.6 (At most 1 bidirected edge). If there exists a connecting path between a(i) and b(k) given some [C] where
a, b ∈V and C ⊆V \ {a, b} in Mµ, then there must exist a path ePµ between a(i) and b(k) that is also connecting given
[C] that contains at most one bidirected edge.
Proof. Since a(i) and b(k) are connected given [C] in Mµ, then they must also be connected given [C] in Dµ. Let Pµ denote
the path connecting a(i) to b(k) given [C] in Dµ. Let Pµ = ⟨a(i), u1, . . . , u(l)⟩and let ux, uz be the ﬁrst and last occurrences
of the vertex y on Pµ, respectively, if any. Since y has an in-degree of 0, neither ux nor uz can be a collider. Hence, we can
concatentate the paths P1 = ⟨a(i), . . . , ux⟩and P2 = ⟨uz, . . . , b(k)⟩to get a connecting path given [C] in Dµ.
Now,
if ux−1 is neither an ancestor nor a descendant of dz+1,
then in Mµ,
we will have the path
a(i), . . . , ux−1, uz+1, . . . , b(k) by virtue of Algorithm 1, since it adds a bidirected edge between any pair of children
of y. This is a path from a(i) to b(k) that is also connected given [C] that contains only 1 bidirected edge.
Otherwise, (W.L.O.G) ux−1 ∈anDµ(uz+1), i.e., there is a directed path from ux−1 to uz+1 in Dµ. Step 3 of Algorithm 1
adds the edge ux−1 →uz+1 to Mµ to create the path ePµ := ⟨a(i), . . . , ux−1, uz+1, . . . , b(k)⟩. This path is from a(i)
to b(k) and passes through no bidirected edges. If this path is active, then we are done. If this path is not active, then,
since ⟨a(i) . . . , ux−1⟩and ⟨uz+1, . . . , b(k) are active, ePµ must be inactive by virtue of ux−1 ∈[C]. But since Pµ in Dµ is
connecting, this implies that ux−1 must have been a collider on that path, hence we have the edge ux−2 →ux−1 in Dµ
and Mµ. Step 2 of Algorithm 1 adds ux−2 →uz+1 in such a case. Then, the path ⟨a(i), . . . , ux−1, uz+1, . . . , b(k)⟩must be
connecting from a(i) to b(k) given [C], which completes the proof.
Lemma E.7 (A Connecting Path in Mµ implies a connecting path in M∪). Under the assumptionin Deﬁntion 4.1, if there
is a connecting path between a(i) and b(k) given some [C] in Mµ for some i, k ∈{1, . . . , K}, where C ⊆V \ {a, b}, then
there is a connecting path between a and b given C in M∪.
Proof. By Lemma E.6, we must have a connecting path in Mµ between a(i) and b(k) given [C] that passes through at most
1 bidirected edge. If there exist paths that pass through no bidirected edges, take any such path. Otherwise, take any path

Causal Structure Discovery from Distributions Arising from Mixtures of DAGs
(a)
(b)
(c)
(d)
Figure E.5: An illustration of the logic for case (i) for the proof of Lemma E.7. In (a) and (c), we color in blue the relevant
segments of the d-connecting path in Mµ, while in (b) and (d), we color in blue the relevant segments of the constructed
d-connecting path in M∪.
that passes through 1 bidirected edge. Call this path Pµ = ⟨a(i) =: u(i)
0 , u(i)
1 , . . . , u(k)
m := b(k)⟩.
By the structure of Mµ discussed in the beginning of this section, only a bidirected edge can connect a node u(i)
x to a node
u(k)
x+1 in Mµ for i ̸= k. Hence, if there is no bidirected edge on this path, then all the nodes u(i)
0 , . . . , u(i)
m will be contained
in the same MAG M(i). Each edge along this d-connecting path given [C] will show up in M∪, and hence we can create a
path ⟨u0, . . . , um⟩that is d-connecting given C in M∪.
In the case where Pµ contains a bidirected edge, let us label the nodes incident as u(i)
x ↔u(k)
x+1. The segments ⟨u(i)
0 , . . . , u(i)
x ⟩
and ⟨u(k)
x+1, . . . , u(k)
m ⟩will each be contained in M(i) and M(k) respectively, and hence we can ﬁnd d-connecting paths
⟨u0, . . . , ux⟩and ⟨ux+1, . . . , um⟩in M∪that are each d-connecting given C. We must now show that we can connect these
paths to create a d-connecting path given C from u0 = a to um = b in M∪.
Of course, there is no difﬁculty if the bidirected edge ux ↔ux+1 appears in M∪, since we can connect these two subpaths
with this bidirected edge and have the desired connecting path. The difﬁculty is when this edge does not appear. From the
deﬁnition of M∪, we can see that this only happens when the bidirected edge connects u(i)
x and u(k)
x+1 for i ̸= k, i.e., the
bidirected edge is not contained in any MAG M(j) for any j. We split the remainder into two cases.
(i) case ux = ux+1. If u(i)
x and u(k)
x+1 are both colliders on Pµ, then we must have ux, ux+1 ∈C. Then c = d will be an
active collider given C in M∪on the path obtained by concatenating ⟨u0, . . . , ux⟩and ⟨ux+1, . . . , um⟩in M∪, and
hence we have our d-connecting path given C. We therefore assume, W.L.O.G., that u(i)
x is not a collider on Pµ.
If there is a path ⟨u(k)
0 , . . . , u(k)
x ⟩in Mµ where every pair of adjacent vertices u(k)
n , u(k)
n+1 on this path are connected by
the same edge type as the pair u(i)
n , u(i)
n+1 in Pµ, then we can replace the segment of ⟨u(i)
0 , . . . , u(i)
x ⟩of Pµ with ⟨u(k)
0
to
u(k)
x ⟩to obtain a path that is d-connecting given [C] and contained completely in M(k), meaning that we can ﬁnd the
desired d-connecting path given C in M∪. If no such path exists in Mµ, then starting at u(i)
x and walking backwards
along Pµ towards u(i)
0 , we will ﬁnd an edge u(i)
z ∗−∗Mµu(i)
z+1 where u(k)
z ∗−∗Mµu(k)
z+1 is not an edge. Take the ﬁrst such
edge found (i.e., the edge closest to u(i)
x that satisﬁes this; see Figure E.5a).
If u(i)
z
→Mµ u(i)
z+1, then by Lemmas E.4 and E.2, there is a bidirected edge u(i)
z+1 ↔Mµ u(k)
x , implying that step 1
of Algorithm 1 adds another bidirected edge u(k)
z+1 ↔u(k)
x . If u(i)
z+1 is not a descendant of u(i)
x , then the bidirected
edge u(k)
z+1 ↔Mµ u(k)
x
would not be removed by step 3 of Algorithm 1 and hence will appear in Mµ. Furthermore,
we will have collliders α(i) and γ(i) between u(i)
z+1 and u(i)
x that are in [C] that will be descendants of u(i)
z+1 and u(i)
x
respectively. The ordering assumption ensures that α and γ are descendants of uz+1 and ux in M∪, respectively.
Hence, the path ⟨u0, . . . , uz+1, ux, . . . , um⟩in M∪is d-connecting in M∪given C. Figures E.5a and E.5b illustrate
this.
Now we check the case where u(i)
z
←u(i)
z+1. If u(i)
z
is not a descendant of u(i)
x , then we can construct a path in
M∪by a similar argument to the above. If u(i)
z
is a descendant of u(i)
x , then by Lemma E.2, there is a directed edge
u(i)
z
←Mµ u(i)
x , which appears as uz ←M∪ux. We can use this to construct a path in M∪as shown in Figures E.5c

Causal Structure Discovery from Distributions Arising from Mixtures of DAGs
(a)
(b)
(c)
(d)
Figure E.6: An illustration of the logic for case (ii) for the proof of Lemma E.7. In (a) and (c), we color in blue the relevant
segments of the d-connecting path in Mµ, while in (b) and (d), we color in blue the relevant segments of the constructed
d-connecting path in M∪.
and E.5d. This path is active since u(i)
x is not a collider, and hence u(i)
x ̸∈[C], implying that ux ̸∈C.
(ii) case ux ̸= ux+1: Step 1 of Algorithm 1 adds the bidirected edge u(k)
x
↔u(k)
x+1, which will show up in M∪as an edge
ux ↔ux+1 unless it is removed by step 3; so this is the only case we must check. Assume W.L.O.G. that this edge is
removed by step 3 because u(k)
x
is a descendant of u(k)
x+1 in Dµ and therefore in Mµ. Then a directed edge u(i)
x ←u(i)
x+1
will be added instead, which appears in M∪as ux ←ux+1. The only case where we cannot join ⟨u0, . . . , ux⟩and
⟨ux+1, . . . , um in M∪together using this directed edge ux ←ux+1 to create a d-connected path given C is when
u(k)
x+1 is in [C], and hence is a collider on Pµ. This implies that we have u(k)
x+1 ←Mµ u(k)
x+2. in which case step 2 of
Algorithm 1 would have added the edge u(k)
x+1 ←u(k)
x+2, which appears as ux+1 ←M∪ux+2. This edge can be used to
create the d-connecting path given C given by ⟨u0, . . . , ux, ux+2, . . . , um⟩in M∪. This is illustrated in Figure E.6
and completes the proof.
E.3. The main result
Finally, we use the results of the ﬁrst two steps to prove the following.
Theorem E.8. Under the assumption in Deﬁnition 4.1, for any disjoint A, B, C ⊆V , [A] and [B] are d-separated given
[C] in Dµ if and only if A and B are d-separated given C in M∪.
Proof. Since Mµ is the marginal MAG in Dµ with respect to the vertex y, the d-separation statements involving subsets not
including y are the same in both. By proposition 2.1, Mµ is a MAG, hence d-separation in Mµ is compositional (Sadeghi
and Lauritzen, 2014); therefore for A, B, C ⊆V disjoint it holds that
n
[A] sep from [B] in Mµ given [C]
o
⇔
n
ai sep from bk in Mµ given [C] for all ai ∈[A], bk ∈[B]
o
.
Now Lemmas E.5 and E.7 imply
n
ai sep from bk in Mµ given [C] for all ai ∈[A], bk ∈[B]
o
⇔
n
a sep from b given C for all a ∈A, b ∈B
o
.
Finally, since M∪is a MAG, applying compositionality gives
n
a sep from bin M∪given C for all a ∈A, b ∈B
o
⇔
n
A sep from B given C in M∪
o
,
which completes the proof.

Causal Structure Discovery from Distributions Arising from Mixtures of DAGs
F. Proof of Proposition 4.6
u ↔M∪v implies u(j) ↔M(j) v(j) for some j, which implies u ←y →v in Dµ. Hence, u, v ∈V \ VINV. By deﬁnition of
VINV, this implies the claim.
G. Additional Experimental Results
G.1. Synthetic Data
In the following, we present ﬁgures for the experiments described in Section 5 for additional values of K and n, and when
p(j) is not uniform over the mixture components. Figures G.1 and G.2 shows the normalized SHD plot in evaluating the
union graph as described in the main paper, while ﬁgures G.3 and G.4 shows the true and false positives in predicted
V \ VINV. Finally, Figure G.5 shows the result of K-means clustering.

Causal Structure Discovery from Distributions Arising from Mixtures of DAGs
(a)
(b)
(c)
(d)
(e)
(f)
(g)
(h)
(i)
Figure G.1: Normalized SHD evaluating the estimation of the union graph from mixture data using FCI for K ∈{2, 4, 6}
and n ∈{1000, 5000, 10000}. We take p(j) uniform over the mixture components.

Causal Structure Discovery from Distributions Arising from Mixtures of DAGs
(a)
(b)
(c)
(d)
(e)
(f)
(g)
(h)
(i)
Figure G.2: Normalized SHD evaluating the estimation of the union graph from mixture data using FCI for K ∈{2, 4, 6}
and n ∈{1000, 5000, 10000}. We take p(j) to be Dirichlet with parameter 2.

Causal Structure Discovery from Distributions Arising from Mixtures of DAGs
(a)
(b)
(c)
(d)
(e)
(f)
(g)
(h)
(i)
Figure G.3: True and false positive rates in estimating V \ VINV using Proposition 4.6 applied to the PAG bP∪estimated by
running FCI on the mxiture data. The ﬁgures show the results for K ∈{2, 4, 6} and n ∈{1000, 5000, 10000}. We take
p(j) to be uniform.

Causal Structure Discovery from Distributions Arising from Mixtures of DAGs
(a)
(b)
(c)
(d)
(e)
(f)
(g)
(h)
(i)
Figure G.4: True and false positive rates in estimating V \ VINV using Proposition 4.6 applied to the PAG bP∪estimated by
running FCI on the mxiture data. The ﬁgures show the results for K ∈{2, 4, 6} and n ∈{1000, 5000, 10000}. We take
p(j) to be Dirichlet with parameter 2

Causal Structure Discovery from Distributions Arising from Mixtures of DAGs
(a)
(b)
(c)
(d)
(e)
(f)
Figure G.5: A comparison of clustering when all the variables are used as features vs. when only the variables in the
estimated set V \ VINV are used as features. In generating ﬁgures (a), (b) and (c), V \ VINV has descendants in the generating
model, while in ﬁgures (d), (e) and (f), V \ VINV has no descendants.

Causal Structure Discovery from Distributions Arising from Mixtures of DAGs
G.2. Real Data
Here, we present the output of FCI on the T cell mixture data referenced in section 5.2.
Figure G.6: The PAG learned using FCI on the T cell mixture data. The inferred arrowheads are shown in red, while the
inferred arrowtails are shown as blue brackets.

