Solving Bayesian Network Structure Learning
Problem with Integer Linear Programming
Ronald Seoh
A Dissertation Submitted to the Department of Management
of the London School of Economics and Political Science
for the Degree of Master of Science
01 Sep 2015
arXiv:2007.02829v1  [stat.ML]  6 Jul 2020

Abstract
This dissertation investigates integer linear programming (ILP) formulation of
Bayesian Network structure learning problem. We review the deÔ¨Ånition and key
properties of Bayesian network and explain score metrics used to measure how
well certain Bayesian network structure Ô¨Åts the dataset. We outline the integer
linear programming formulation based on the decomposability of score metrics.
In order to ensure acyclicity of the structure, we add ‚Äúcluster constraints‚Äù
developed speciÔ¨Åcally for Bayesian network, in addition to cycle constraints ap-
plicable to directed acyclic graphs in general. Since there would be exponential
number of these constraints if we specify them fully, we explain the methods to
add them as cutting planes without declaring them all in the initial model. Also,
we develop a heuristic algorithm that Ô¨Ånds a feasible solution based on the idea
of sink node on directed acyclic graphs.
We implemented the ILP formulation and cutting planes as a Python pack-
age, and present the results of experiments with diÔ¨Äerent settings on reference
datasets.

Contents
1
Introduction
1
2
Preliminaries
3
2.1
Bayesian Network . . . . . . . . . . . . . . . . . . . . . . . . . . .
3
2.1.1
Overview
. . . . . . . . . . . . . . . . . . . . . . . . . . .
3
2.1.2
Key Characteristics of Bayesian Network . . . . . . . . . .
5
2.2
Integer Linear Programming . . . . . . . . . . . . . . . . . . . . .
8
2.2.1
Overview
. . . . . . . . . . . . . . . . . . . . . . . . . . .
8
2.2.2
Branch-and-Bound . . . . . . . . . . . . . . . . . . . . . .
8
2.2.3
Cutting Planes
. . . . . . . . . . . . . . . . . . . . . . . .
10
2.2.4
Branch-and-Cut . . . . . . . . . . . . . . . . . . . . . . . .
10
3
Learning the Structure
12
3.1
Score Metrics . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
12
3.1.1
Bayesian Dirichlet . . . . . . . . . . . . . . . . . . . . . . .
13
3.2
ILP Formulation
. . . . . . . . . . . . . . . . . . . . . . . . . . .
18
3.2.1
Sets, Parameters, Variables
. . . . . . . . . . . . . . . . .
18
3.2.2
Objective
. . . . . . . . . . . . . . . . . . . . . . . . . . .
18
3.2.3
Constraints
. . . . . . . . . . . . . . . . . . . . . . . . . .
19
4
Finding Solutions
21
4.1
Cluster Cuts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
21
4.2
Cycle Cuts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
23
4.3
Sink Finding Heuristic . . . . . . . . . . . . . . . . . . . . . . . .
23
5
Implementation and Experiments
26
5.1
Implementation Details . . . . . . . . . . . . . . . . . . . . . . . .
26
5.1.1
Notes on Branch-and-Cut
. . . . . . . . . . . . . . . . . .
27
5.2
Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
28
5.2.1
Setup
. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
28
5.2.2
Experiment 1: All Features Turned On . . . . . . . . . . .
29
5.2.3
Experiment 2: Without Cycle Cuts . . . . . . . . . . . . .
30
5.2.4
Experiment 3: Without Gomory Cuts . . . . . . . . . . . .
31
5.2.5
Performance of Sink Finding Heuristic
. . . . . . . . . . .
35

6
Conclusion
36
6.1
Future Directions . . . . . . . . . . . . . . . . . . . . . . . . . . .
36
6.1.1
Statistical Modelling . . . . . . . . . . . . . . . . . . . . .
36
6.1.2
Optimisation
. . . . . . . . . . . . . . . . . . . . . . . . .
37
Appendices
39
A Software Instructions
39

Chapter 1
Introduction
Bayesian network is a probabilistic graphical model using directed acyclic graphs
to express joint probability distributions and conditional dependencies between
diÔ¨Äerent random variables. Nodes represent random variables and directed arcs
are drawn from parent nodes to child nodes to show that the child node is condi-
tionally dependent to its parent nodes. Aside from its mathematical properties,
Bayesian network‚Äôs visual presentation make them easily perceivable, and many
researchers in diÔ¨Äerent Ô¨Åelds have used it to model and study their systems.
Constructing a Bayesian network requires two major components: its graph
topology, and parameters for the joint probability distribution. In some cases,
the structure of the graph gets gets speciÔ¨Åed in advance by ‚Äúexperts‚Äù and we
Ô¨Ånd the values for the parameters that Ô¨Åt the given data, more speciÔ¨Åcally by
using maximum likelihood approach.
The problem gets more complicated when we do not know the graph structure
and have to learn it from the given data.
This would be the case when the
problem domain is too large and it is extremely diÔ¨Écult or impractical for humans
to manually deÔ¨Åne the structure. Learning the structure of the network from data
have been proven NP-hard, and there have been several diÔ¨Äerent approaches over
the years to tackle this problem.
Early methods were approximate searches, where the algorithm searches
through the candidates to look for the most probable one based on their strate-
gies but usually does not provide any guarantee of optimality. Later on, there
have developments on exact searches based on conditional independence testing
or dynamic programming. While these did provide some level of optimality, their
real world applicability was limited as the amount of computation became infea-
sible for larger number of datasets, or the underlying assumptions could not be
easily met in reality.
In this dissertation, we discuss another method of conducting exact search of
Bayesian network structure using integer linear programming (ILP). While this
approach is relatively new in the Ô¨Åeld, it achieves fast search process based on
various integer programming techniques and state-of-the-art solvers, and allows
users to incorporate prior knowledge easily as constraints.
1

2
One of the main challenges in Bayesian network structure learning is on how
to enforce acyclicity of the resulting structure. While acyclicity constraint devel-
oped for general DAGs also applies to Bayesian network, it is not tight enough for
our ILP formulation due to the fact that we select the set of parent nodes for each
variables rather than individual edges in the graph. We study so-called ‚Äúcluster
constraints‚Äù developed by Jaakkola et al.[27] that provides stronger enforcement
of acyclicity on Bayesian network structure. Since there will be exponential num-
ber of such constraints if we specify them fully, we also cover how we can add
them as cutting planes when needed during the solution process.
We also implemented a test computer program based on this ILP formulation
and examined their performance on some of the sample datasets. There are few
other implementation publicily available, but we provide a clean object-oriented
version written in Python programming language.
The structure of this disseration will be as follows: Chapter 2 will review the
concept of integer linear programming and Bayesian network needed to under-
stand the problem. Chapter 3 will examine score metrics used for BN structures,
and present the ILP formulation. Chapter 4 will explain our approach on adding
cluster constraints as cutting planes to the ILP model, and a heuristic algorithm
to obtain good feasible solutions. Chapter 5 will present our implementation
details and benchmark results. Lastly, Chapter 6 will provide pointers to further
development of our methods.

Chapter 2
Preliminaries
This chapter reviews two theoretical and conceptual foundations behind the topic
of this dissertation: Bayesian Network (BN) and Integer Linear Programming
(ILP).
2.1
Bayesian Network
2.1.1
Overview
Consider a dataset D that contains n predictor variables x1, x2, x3, ..., xn, the
class variable y that can take m classes y1, y2, y3, ..., ym and k samples s1, s2, s3, ..., sk.
Let‚Äôs suppose we want to Ô¨Ånd probabilities for each possible values of y, given
the observations of all the predictor variables. In other words, if we want to
calculate the probabilities of y = y1, it can written as
P(y = y1 | x1, x2, ..., xn) = P(y = y1) √ó P(x1, x2, ..., xn | y)
P(x1, x2, ..., xn)
(2.1)
where the left hand side shows posterior probabilities, P(y = y1) on the right
hand side prior proabibilities of y = y1, P(x1, x2, ..., xn | y) support data provides
for y = y1, and P(x1, x2, ..., xn) on the denominator normalising constant.
One of the simplest and most popular approaches to the task stated above
is Naive Bayes[35], where we simply assume that all the predictor variables are
indepedent from each other. Then Equation 2.1 becomes
P(y = y1 | x1, x2, ..., xn) = P(y = y1) √ó Q
i=1 P(xi | y)
P(x1, x2, ..., xn)
(2.2)
The question is, what if these predictors are actually not completely inde-
pendent as Naive Bayes assumed? Depending on the subject domain, it might
be the case that some of the predictors have dependency relationships. If we
assume that all the variables are binary and we need to store all the arbitrary
3

4
dependencies without any additional assumptions, that means we need to store
2n ‚àí1 values in the memory - which would become too much computational
burden even for relatively small number of variables.
This is where Bayesian network comes in - it leverages a graph structure to
provide more intuitive representation of conditional dependencies in the domain
and allow the user to perform inference tasks in reasonable amount of time and
resources.
DeÔ¨Ånition 2.1.1. A Bayesian Network G = (N, A) is a probabilistic graphical
model in a directed acyclic graph (DAG) where each node n ‚ààN represents a
variable in the dataset and each arc (i, j) ‚ààA indicates the variable j being
probabilistically dependent on i.1
We can therefore perceive Bayesian network as consisting of two components:
1. Structure, which refers to the directed acyclic graph itself: nodes and
arcs that specify dependencies between the variables,
2. Parameters, corresponding conditional probabilities of each node (vari-
able) given its parents.
Figure 2.1: ASIA Bayesian Network Structure
One example BN constructed from the ASIA dataset by Lauritzen and Spiegel-
halter is presented in Figure 2.1.[18]
1We are using the neutral expression ‚Äòprobabilistically dependent‚Äô as the interpretation of
the arcs might become based on the assumptions on the domain. Some researchers interpret i
to be direct cause of j, which might not be valid in other cases.

5
2.1.2
Key Characteristics of Bayesian Network
Markov Condition
The key assumption behind Bayesian network is that one node (variable) is
conditionally independent on its non-descendants, given its parent nodes. This
assumption signiÔ¨Åcantly reduces the space required for inference tasks, since one
would need to have only the values of the parents.
To be precise, let‚Äôs say our Bayesian network tell us that x2 is a parent node
of x1, but not others. That means x1 is independent of the rest of the variables
given x2. Then we can change Equation 2.1 into
P(y = y1 | x1, x2, ..., xn) = P(y = y1) √ó P(x1|x2, y) √ó P(x2, ..., xn | y)
P(x1, x2, ..., xn)
(2.3)
If we can Ô¨Ånd other conditional independencies like this using Bayesian net-
work, we can keep changing Equation 2.3 above into hopefully smaller number
of terms than what we would have had by assuming arbitrary dependencies and
independencies.
The assumption explained above is called Markov condition, which the formal
deÔ¨Ånition[18] is stated below:
DeÔ¨Ånition 2.1.2. Given a graph G = (N, A) and a joint probability distribu-
tion P deÔ¨Åned over variables represented by the nodes n ‚ààN. If the following
statement is also true
‚àÄn ‚ààN, n ‚ä•‚ä•p ND(v) | Pa(v)
where ND(v) refers to the non-descendants and Pa(v) parent nodes of v,
then we can say that G satisÔ¨Åes the Markov condition with P, and (G,P) is a
Bayesian network.
d-separation
As stated in DeÔ¨Ånition 2.1.1, the structure of Bayesian network is a directed
acyclic graph: main motivation for using it was to make use of conditional in-
dependence to store uncertain information eÔ¨Éciently by associating dependence
with connectedness and independence with un-connectedness in graphs. By ex-
ploiting the paths on DAG, Judea Pearl introduced a graphical test called d-
separation in 1988 that can discover all the conditional independencies that are

6
implied from the structure (or equivalently Markov condition stated in DeÔ¨Åni-
tion 2.1.2):
DeÔ¨Ånition 2.1.3. A trail in a directed acyclic graph G is an undirected path in
G, which is a connected sequence of edges in G‚Ä≤ where all the directed edges in
G are replaced with undirected edges.
DeÔ¨Ånition 2.1.4. A head-to-head node with respect to trail t is a node x in t
where there are consecutive edges (Œ±, x), (x, Œ≤) for some nodes Œ± and Œ≤ in t.
DeÔ¨Ånition 2.1.5. (Pearl 1988) If J, K, L are three disjoint subsets of nodes
in a DAG D, then L is said to d-separate J from K, denoted I(J, L, K)D, iÔ¨Ä
there is no trail t between a node in J and a node in K along which (1) every
head-to-head node (w.r.t. t) either is or has a descent in L and (2) every node
that delivers an arrow along t is outside L. A trail satisfying the two conditions
above is said to be active, otherwise it is said to be blocked (by L).[20]
Expanding DeÔ¨Ånition 2.1.5, we can summarise the types of dependency rela-
tionships extractable from Bayesian network into following[8][31][17]:
Figure 2.2: DiÔ¨Äerent Connection Types in DAG
‚Ä¢ Indirect cause: Consider the connection type ‚ÄòLinear‚Äô in Figure 2.2. We
can say that a is independent of c given b. Although a might explain c up to
some degree, it becomes conditionally independent when we have b. In light
of d-separation deÔ¨Ånition, we can say that a c is d-connected and active
when we condition against something other than a, b, c. However, when we
consider b as a condition (member of L in the deÔ¨Ånition), b d-separates a
and c.
‚Ä¢ Common eÔ¨Äect: For the ‚ÄòConverging‚Äô connection type, we can say that
a and c becomes dependent given b. a and c are independent without b.
However, once we condition on b, the two become dependent - once we

7
know b has occurred, either one of a and c would explains away the other
since b is probabilistically dependent on a and c.
‚Ä¢ Common cause: For the ‚ÄòDiverging‚Äô connection type, a is independent of
c given b.
Markov Equivalent Structures
Another charcteristic that arise from the structure of Bayesian network is that
it might be possible to Ô¨Ånd another DAG that encodes same set of conditional
independencies as original. Two directed acyclic graphs are considered Markov
equivalent[33] iÔ¨Äthey have
‚Ä¢ same skeletons, which is a graph which all the directed edges of the DAG
are replaced with undirected ones, and
‚Ä¢ same v-structures, which refers to all the head-to-head meetings of di-
rected edges without unjoined tails in the DAG.
To represent this equivalence class, we use Partially Directed Acyclic Graph
(PDAG) with every directed edge compelled and undirected edge reversible.

8
2.2
Integer Linear Programming
2.2.1
Overview
Integer Linear Programming (ILP) refers to the special group of Linear Program-
ming (LP) problems where the domains of some or all the variables are conÔ¨Åned
to integers, instead of real numbers in LP. Such problems can be written in the
form
max c‚ä§x
subject to Ax ‚â§b
(2.4)
x ‚ààZ
If the problem have both non-integer and integer variables, we call it a Mixed
Integer Linear Programming (MILP) problem. While ILP obviously can be used
in situations where only integral quantities make sense, for example, number
of cars or people, more eÔ¨Äective uses of integer programming often incorporate
binary variables to represent logical conditions like yes or no decisions.[34]
The major issue involved with solving ILPs is that they are usually harder
to solve than non-integer LP problems. Unlike conventional LPs which Simplex
algorithm can directly produce the solution by checking extreme points of feasible
region,2 ILP need additional steps to Ô¨Ånd a speciÔ¨Åc solution that are strictly
integers. There are mainly two approaches to solving ILP - 1) Branch-and-Bound
and 2) Cutting Planes. In fact, these two are not mutually exclusive to be precise
but rather can be combined and used together to solve ILP, the approach called
3) Branch-and-Cut. The following subsections will brieÔ¨Çy describe the Ô¨Årst two
and outline the details of the third, which is actually used in the main part of
this dissertation.
2.2.2
Branch-and-Bound
The term ‚ÄúBranch-and-Bound‚Äù (BnB) itself refers to an algorithm design paradigm
that was Ô¨Årst introduced to solve discrete programming problems, developed by
Alisa H. Land and Alison G. Doig of the London School of Economics in 1960.[29]
BnB have been the basis of solving a variety of discrete and combinatorial opti-
misation problems,[11] most notably integer programming.
2It is therefore possible that such solution happens to be integers; however, it is generally
known from past experience that such case rarely appears in practice.

9
Figure 2.3: Branch-and-Bound example
BnB in IP can be best described as a divide-and-conquer approach to sys-
tematically explore feasible regions of the problem. Graphical representation of
this process is shown in Figure 2.3.[12] We start from the solution of LP-relaxed
version of the original problem, and choose one of the variables with non-integer
solutions - let‚Äôs call this with variable xi with non-integer solution f. Then we
create two additional sub-problems (branching) by having
‚Ä¢ one of them with additional constraint that xi ‚â§‚åäf‚åã,
‚Ä¢ another with xi ‚â•‚åàf‚åâ.
We choose and solve one of the two problems. If the solutions still aren‚Äôt
integers, branch on that as above and solve one of the new nodes (problems),
and so on.
‚Ä¢ If one of the new problems return integer solutions, we don‚Äôt have to branch
any more on that node (pruned by integrality), and this solution is called
incumbent solution - the best one yet.
‚Ä¢ If a new problem is infeasible, we don‚Äôt have to branch on that as well
(pruned by infeasibility).
‚Ä¢ If the problem returns integer solutions but the objective value is smaller
than the incumbent, then we stop branching on that node (pruned by
bound).
When branching terminates on certain node, we go back to the direct parent
node and start exploring the branches that haven‚Äôt been explored yet. After we
keep following these rules and there are no more branches to explore, than the
incumbent solution at that moment is the optimal solution.

10
2.2.3
Cutting Planes
Cutting planes is another approach for solving ILP that was developed before
BnB. The basic idea is that we generate constraints (which would be a hyper-
plane) that can cut out the currunt non-integer solution and tighten the feasible
region. If we solve the problem again with the new constraint and get integer
solutions, then the process ends. If not, we continue adding cutting planes and
solve the problem until we get integer solutions.
The question is how we generate the one that can cut out the region as much
as possible. There are a number of diÔ¨Äerent strategies available for this task,
but the most representative and the one implemented for this project is called
Gomory Fractional Cut:
n
X
j=1
(aj ‚àí‚åäaj‚åã)xj >= a0 ‚àí‚åäao‚åã
(2.5)
The equation above is called Gomory fractional cut,[21][12] where aj, xj comes
from the row of the optimal tableau. Since Pn
j=1 ajxj = a0, there must be a
k ‚ààZ that satisÔ¨Åes Pn
j=1(aj ‚àí‚åäaj‚åã)xj = a0 ‚àí‚åäao‚åã+ k. Also, k is non-negative
since Pn
j=1(aj ‚àí‚åäaj‚åã)xj is non-negative. Therefore Equation 2.5 holds. As a
result, Gomory factional cut can cut out the current fractional solution from the
feasible region.
In additional to general purpose cuts like Gomory cuts that can be applied to
all the IP problems, we might be able to generate cutting planes that are speciÔ¨Åc
to the problem. This dissertation also examines some domain-speciÔ¨Åc cutting
planes for the BN structure learning problem, which is done in chapter 4.
2.2.4
Branch-and-Cut
Branch-and-Cut combines branch-and-bound and cutting planes into one algo-
rithm, and it is the most successful way of solving ILP problems to date. Es-
sentially following the structure of BnB to explore the solution space, we add
cutting planes whenever possible on LP-relaxed problems before branching to
tighten the upper bound, so that we can hopefully branch less than the stan-
dard BnB. The decision of whether to add cutting planes depends on the speciÔ¨Åc
problems and success of previously added cuts. Pseudocode of Branch-and-Cut

11
algorithm is presented in algorithm 1. The criteria used for our problem will also
be discussed in chapter 4.
Algorithm 1: Branch-and-Cut Algorithm
Require: initial problem, problem list, objective upper bound, best solution
Ensure: best solution ‚ààZ
objective lower bound ‚áê‚àí‚àû
problem list ‚áêproblem list ‚à™initial problem
while problem list Ã∏= ‚àÖdo
current problem ‚áêp ‚ààproblem list
Solve LP-Relaxed current problem
if infeasible then
Go back to the beginning of the loop
else
z ‚áêcurrent objective, x ‚áêcurrent solution
end if
if z ‚â§objective upper bound then
Go back to the beginning of the loop
end if
if x ‚ààZ then
best solution ‚áêx
objective upper bound ‚áêz
Go back to the beginning of the loop
end if
if cutting planes applicable then
Find cutting planes violated by x
add the cutting plane and go back to ‚ÄòSolve LP-Relaxed
current problem‚Äô
else
Branch using non-integer solutions in x
problem list ‚áêproblem list ‚à™new problems
end if
end while
return best solution

Chapter 3
Learning the Structure
In some cases, it might be possible to create a Bayesian network manually if
there‚Äôs a subject expert who already have a knowledge about the relationships
between the variables. Many statistical softwares such as OpenBUGS1 and Net-
ica2 allow the users to specify BN models.
However, it would be impossible to create such models in other cases, either
because we simply do not have such domain knowledge or there are too many
variables to consider. Therefore, there have been constant interests in ways to
automatically learn the Bayesian network structure that best explains the data.
Chickering (1996) have showed that the structure learning problem of Bayesian
network is NP-hard.[9] Even with additional conditions such as having an inde-
pendence oracle or the number of parents limited to 2, the problem still remains
intractable.[10][19] Therefore, the eÔ¨Äorts have been focused on making the com-
putation feasible using various assumptions and constraints while attempting to
provide some guarantee of optimality.
Before going into the actual solutions, the Ô¨Årst thing we need to consider
is how we should deÔ¨Åne the best structure. Since Bayesian network is about
revealing conditional independencies in the domain, the best Bayesian network
should be able to identify as many dependencies as possible that are highly likely
in terms of probability.
Researchers formalised this notion as score-and-search, where we search through
the space of structure candidates with scores for each of them, and select the
highest scoring one. Section 3.1 explains how such score metric works. After ex-
plaining the score metric, our integer linear programming formulation of Ô¨Ånding
the best Bayesian network structure is presented in section 3.2.
3.1
Score Metrics
By scoring a candidate BN structure, we are measuring a probability of the can-
didate being the BN structure representing joint probability distribution (JPD)
1http://www.openbugs.net
2https://www.norsys.com/netica.html
12

13
that our training data is sampled from. Following Bayes‚Äô Theorem, we want to
calculate the posterior probability
P(Bh
S | D) = c √ó P(Bh
S) √ó P(D | Bh
S)
(3.1)
where Bh
S is a hypothesis that the candidate structure Bs represents the JPD
that the dataset D was sampled from, P(Bh
S) being the prior probability of
the hypothesis, P(D | Bh
S) support D provides for the hypothesis, and c is a
normalising constant.
For the priors of each candidate hypothesis, a uniform prior is usually as-
sumed, even though it might be possible to assign diÔ¨Äerent priors based on
expert knowledge.[25] The main problem here is how to measure the support
P(D | Bh
S) - more generally, how we can measure the goodness of Ô¨Åt of the
candidate structure to the dataset. There are a number of score metrics based
on diÔ¨Äerent theoretical foundations, but this dissertation uses BDeu metric, the
special type of Bayesian Dirichlet-based metrics which their inner workings are
explained below:
3.1.1
Bayesian Dirichlet
Bayesian Dirichlet (BD)-based score metrics are Bayesian approaches to scoring
using Dirichlet distribution to calculate the posterior probability of the structure.
These score metrics have the following assumptions in common[7]:
Notation
‚Ä¢ ŒòG = {Œòi} for i = 1, ..., n: Set of parameters of all the variables i = 1, ..., n
in Bayesian Network DAG G
‚Ä¢ Œòi = {Œòij} for j = 1, ..., qi: Set of parameters of all the qi parent conÔ¨Ågu-
rations j = 1, ..., n for just one variable i
‚Ä¢ Œòij = {Œ∏ijk} for k = 1, ..., ri: Set of parameters (physical probabilities) of i
taking each ri number of values, given one parent conÔ¨Åguration j
Assumptions
1. Multinomial Samples: Dataset D have multinomial samples with yet
unknown physical probabilities Œ∏ijk.

14
2. Dirichlet Distribution: Set of physical probabilities in Œòij = Œ∏ij1, Œ∏ij2, ..., Œ∏ijri
follows Dirichlet distribution.
3. Parameter Independence: All the Œòi are independent with each other
(global parameter independence), and all the Œòij are independent with each
other as well (local parameter independence).
4. Parameter Modularity: Given two BN structures G and G‚Ä≤, if i have
same set of parents in both structures, then G and G have identical Œòij.
Before going deeper, let‚Äôs brieÔ¨Çy go over the rationale behind using a Dirich-
let distribution. If we were certain about the values of all the Œ∏ijk, we could
simply just express them as a multinomial distribution. However, since we don‚Äôt
have such information, we estimate the behaviour of such distribution by using
Dirichlet distribution, which allows us to reÔ¨Çect our beliefs about each Œ∏ijk using
corresponding parameters Œ±ijk, which are constructed before taking our data D
into account.
While it is theoretically possible to use a distribution other than Dirichlet, we
use them as it is algebraically straightforward to calculate posterior probabilities.
First, since Œòij follows Dirichlet distribution, we know
p(Œòij | Bh
S, Œ±) = Œì(Pri
k=1 Œ±ijk)
Qri
k=1
√ó
ri
Y
k=1
Œ∏
Œ±ijk‚àí1
x=k
(3.2)
Also, expected value E(Œ∏ijk) is Œ±ijk
Œ±0 , where Œ±0 = P
ijk Œ±ijk.
With multinomial sample D, We want to calculate
p(Œòij | D, Bh
S, Œ±) = c √ó
ri
Y
k=1
p(Œòij | Bh
S, Œ±) √ó Œ∏
Nijk
ijk
(3.3)
Since we have Equation 3.2, we can rewrite Equation 3.3 as
p(Œòij | D, Bh
S, Œ±) = c √ó
ri
Y
k=1
Œ∏
(Œ±ijk+Nijk)‚àí1
ijk
(3.4)
where Nijk is a number of times ijk appeared in D.
So Equation 3.4 shows that the posterior distribution of Œòij given D is also
a Dirichlet distribution. We say Dirichlet distribution is a conjugate prior to

15
multinomial samples, where both prior and posterior distributions are Dirichlet.
Also, we now have
E(Œ∏ijk | D, Bh
S, Œ±) = Œ±ijk + Nijk
Œ±ij + Nij
, where Nij =
X
ijk
Nijk
(3.5)
BD metric
With Equation 3.5, we can also calculate the probability of seeing a certain
combination of all the variable values Cm+1, which can be written as
p(Cm+1 | D, Bh
S, Œ±) =
qY
i=1
qi
Y
j=1
Œ±ijk + Nijk
Œ±ij + Nij
(3.6)
Equation 3.6 makes our original task of calculating the support of dataset
to candidate hypothesis P(D | Bh
S, Œ±) algebraically convenient. Let‚Äôs say our
dataset D have m instances, then
P(D | Bh
S, Œ±) =
m
Y
d=1
p(Cd | C1, ..., cd‚àí1, Bh
S, Œ±)
(3.7)
where each Cd represents an instance in D with certain combination of vari-
able values. Expanding 3.6, Equation 3.7 can be calculated by
P(D | Bh
S, Œ±) =
n
Y
i=1
qi
Y
j=1
Œ±ij1
Œ±ij
√ó Œ±ij1 + 1
Œ±ij + 1 √ó ... √ó Œ±ij1 + (Nij1 ‚àí1)
Œ±ij + (Nij1 ‚àí1)

(3.8)
√ó

Œ±ij2
Œ±ij + Nij1
√ó
Œ±ij2 + 1
Œ±ij + Nij1 + 1 √ó ... √ó
Œ±ij2 + (Nij2 ‚àí1)
Œ±ij + Nij1 + (Nij2 ‚àí1)

√ó

Œ±ijri
Œ±ij + Pri‚àí1
k=1 Nijk
√ó
Œ±ijri + 1
Œ±ij + Pri‚àí1
k=1 Nijk + 1
√ó ... √ó Œ±ijri + (Nijri ‚àí1)
Œ±ij + (Nij ‚àí1)

=
n
Y
i=1
qi
Y
j=1
Œì(Œ±ij)
Œì(Œ±ij + Nij) √ó
ri
Y
k=1
Œì(Œ±ijk + Nijk)
Œì(Œ±ijk)
Therefore, the probability of the hypothesis P(D | Bh
S, Œ±) can be calculcated
using 3.8, and if we log-transform it we get

16
BD(B, D) = log(P(Bh
S | Œ±)) +
n
X
i=1
qi
X
j=1

log

Œì(Œ±ij)
Œì(Œ±ij + Nij)

+
ri
X
k=1
log
Œì(Œ±ijk + Nijk)
Œì(Œ±ijk)

(3.9)
Equation 3.9 is called BD scoring function, introduced by Heckerman, Geiger
and Chickering.[26]
K2, BDe and BDeu
While BD metric is logically sound, it is practically unusable as we need to have
all the Œ±ijk in hand to calculate the total score. K2 metric in Equation 3.10 by
Cooper and Herskovits (1992)[13], which was actually developed before BD met-
ric, simply assigns Œ±ijk = 1 for all ijk, assuming uniform Dirichlet distribution
(not uniform distribution) on the prior.
K2(B, D) = log(P(B)) +
n
X
i=1
qi
X
j=1

log

(ri ‚àí1)!
(Nij + ri ‚àí1)!

+
ri
X
k=1
log(Nijk! )

(3.10)
BDe metric[26] attempts to reduce the number of Œ±ijk that needs to be spec-
iÔ¨Åed by introducing likelihood equivalence. Let‚Äôs suppose there‚Äôs a complete BN
structure G that speciÔ¨Åes all the ‚Äòtrue‚Äô conditional dependencies without any
missing edges, and our candidate structure Bh
S is Markov equivalent to it. Then
their parameters for Dirichlet distribution and hence their likelihood should be
the same as they represent same joint probability distribution. Then we can get
Œ±ijk = Œ±‚Ä≤ √ó P(Xi = xik, Q
Xi = wij | G), where Œ±‚Ä≤ represent the level of belief
on the prior and wij is j-th conÔ¨Åguration of parents for i in G. In other words,
we only need Œ±ijk for the conÔ¨Åguration that are actually represented by edges in
BS.
BDeu metric[5] goes one step further from BDe by simply assuming that
P(Xi = xik, Q
Xi = wij | G) =
1
ri√óqi, which assigns uniform probabilities to all
combinations of xi values and its parent conÔ¨Ågurations. This allows the user to
calculate the scores with limited prior knowledge, while maintaining the property
of likelihood equivalence which K2 metric cannot.

17
Decomposability of Score Metrics
As seen from Equation 3.9, these score metrics are log-transformed to provide
decomposability. Since the outermost sum is summed over each variable i, we
can say that BDeu score is a sum of local scores for each variable given their
parent node conÔ¨Ågurations. Therefore, we can say that we are trying to Ô¨Ånd
the best structure with the highest score by choosing the parent nodes for each
variable that maximises their local scores. This goal is expressed as the objective
in Equation 3.11.

18
3.2
ILP Formulation
Based on the decomposable score metrics in section 3.1, we present integer linear
programming formulation of Ô¨Ånding the best Bayesian network structure. One
of the early works on the ILP formulation have been done by Cussens[15] for
reconstructing pedigrees (‚Äòfamily tree‚Äô) as a special type of BN.
The work independently done by Jaakkola et al.[27] provides proof that the
well known acylic subgraph polytope Pdag is not tight enough for the BN struc-
ture learning problem and presents tighter constraint of maintaining acyclicity
dubbed cluster constraint, along with the algorithm to approximate the tightened
polytope using duals.
Cussens incorporated the Ô¨Åndings of [27] into branch-and-cut algorithm by
adding cluster constraints as cutting planes, along with general purpose cutting
planes and heuristics algorithm for speeding up the performance.[14][2]. This
dissertation largely follows the work of Barlett and Cussens[2], but with slightly
diÔ¨Äerent strategies for Ô¨Ånding cutting planes and object-oriented implementation
of experiment program from scratch using Python language and Gurobi solver,
which is completely independent from Cussens‚Äôs computer program GOBNILP
written in C language[16]3.
3.2.1
Sets, Parameters, Variables
‚Ä¢ v = node in BN
‚Ä¢ W = parent set candidate for v
‚Ä¢ c(v, W) = local scores for v having W as parents
‚Ä¢ I(W ‚Üív) = binary variable for W being selected for v.
3.2.2
Objective
maximise total score:
X
v,W
c(v, W) √ó I(W ‚Üív)
(3.11)
3https://www.cs.york.ac.uk/aig/sw/gobnilp/

19
3.2.3
Constraints
Only One Parent Set Constraint
‚àÄv ‚ààV :
X
W
I(W ‚Üív) = 1
(3.12)
Acyclicity Constraint
Since Bayesian network takes a form of DAG, the constraint to ensure acyclicity is
the primary challenge of this ILP formulation. Acyclicity constraints on directed
acyclic graphs for use in linear programming have been studied quite extensively
as facets of acylic subgraph polytope Pdag. Typically, such constraints can be
expressed in the form
X
(i,j)‚ààC
xij ‚â§|C|‚àí1
(3.13)
where xij is a binary variable for each directed edge (i, j) and C is any subset of
nodes.
While this constraint is obviously valid for Bayesian network structures as
well and actually used as cutting planes in section 4.2, this is not tight enough
for our model for two reasons. First, it is already known that 3.13 still allows
edge selections that are actually not valid except for some special cases of planar
DAG[22].
More importantly however, our binary variable selects a group of parent nodes
but not individual edges. This creates another problem as simplying having just
3.13 would allow a situation where the choice of parent nodes for one variable
gets divided over two candidate sets.
Proposition 3.2.1. There exists some cases of dicycles in Bayesian network G
that cannot be cut oÔ¨Äby the constraint in Equation 3.13.
Proof. We prove by counterexample. Let‚Äôs suppose we have variable nodes N =
A, B, C. If the solver assigned 0.5 to the parent set choices (A | B, C) (B | A, C),
(C | A, B) and 0.5 to (A | ‚àÖ), (B | ‚àÖ), (C | ‚àÖ), then these still satisiÔ¨Åes 3.12 by
0.5+0.5 = 1 and 3.13 by 0.5+0.5+0.5 = 1.5 < 3 for each node, but this solution
does not represent any valid BN structure.

20
‚àÄcluster C ‚äÜV :
X
v‚ààC
X
W‚à©C=‚àÖ
I(W ‚Üív) ‚â•1
(3.14)
Equation 3.14 is a constraint developed by Jaakkola et al.[27] to overcome
this issue. The basic idea is that for every possible cluster (subset of nodes) in the
graph, there should be at least one node that does not have any of its parents
in the same cluster, or have no parents at all. This would be able to enforce
acyclicity as there would be no edges at all that points to such node within the
cluster (but might have nodes that start from this node to the other node in the
cluster). Since this constraint applies to clusters of every size in the graph, we
can see that this constraint is much tighter than 3.13.
Since both Equation 3.13 and Equation 3.14 would require exponential num-
ber of constraints if we add every possible cases of them at once into our model,
we instead add them as cutting planes if needed. We Ô¨Årst solve LP relaxation
without them, search for constraints that are violated by the current solution,
then add those to the model and solve again. The details of how we search for
these cutting planes are explained in chapter 4.

Chapter 4
Finding Solutions
Based on the formulation outlined in section 3.2, we present the actual process
developed to solve the problem. Due to the number of acyclicity constraints
that grows exponentially with respect to the number of variables, we do not add
them directly to the formulation as that will complicate the shape of our feasible
region and make the solution process diÔ¨Écult, let alone the time of generating all
the constraint statements. Instead, we add them as cutting planes on demand
- we Ô¨Årst solve the problem without those constraints and search for only the
constraints that are actually violated by the current solution. In addition, we
considered some heuristic algorithm that takes advantage of the relaxed solution
to make our solving process faster.
4.1
Cluster Cuts
To add Equation 3.14 as cutting planes, we need to Ô¨Ånd a cluster or set of nodes
that violates 3.14 under the solution yet without this constraint. Since cutting
planes allow us to tighten our feasible region and speed up the solution process,
we want to look for the cluster that violates the constraint the most, i.e. the
one that has the most members with their parents within the same cluster. To
express this idea formally, let‚Äôs Ô¨Årst see that Equation 3.14 can be rewritten as
‚àÄC ‚äÜV :
X
v‚ààC
X
W:|W‚à©C|‚â•1
I(W ‚Üív) ‚â§|C|‚àí1
(4.1)
This is because we have Equation 3.12, where we should have exactly one parent
set choices for each node. There should be exactly |C| parent set choices, but
at least one of them should choose a parent set that is completely outside C.
Therefore, The number of I(W ‚Üív) that chooses W with one or more of its
members inside C (|W ‚à©C|‚â•1) should be limited to |C|‚àí1.
So we are looking for a cluster C that makes the LHS of 4.1 exceed the RHS
the most. Cussens[16] have suggested one way of achieving this by formulating
21

22
this as another small IP problem based on the current relaxed solution. For
each non-zero I(W ‚Üív) in current relaxed solution, we create corresponding
binary variable J(W ‚Üív). We also put these non-zero solutions as coeÔ¨Écients
of J(W ‚Üív), denoted as x(W ‚Üív) below. Lastly, we create binary variables
M(v ‚ààC) for all the nodes v, which will indicate that each node is chosen to be
included.
Objective:
maximise
X
v,W
x(W ‚Üív) √ó J(W ‚Üív) ‚àí
X
v‚ààV
M(v ‚ààC)
(4.2)
Constraint:
M(v ‚ààC) = 1 for each J(W ‚Üív) = 1
(4.3)
M(w ‚ààC) = 1 for at least one w ‚ààW for each J(W ‚Üív) = 1
(4.4)
We are having x(W ‚Üív) as coeÔ¨Écients here because we want to choose the
cluster that is supported by the current relaxed solution.
That is, we want
the choice of I(W ‚Üív) to be the one that cuts out the feasible region the
most including the current relaxed solution, not just some arbitrary space in
the feasible region. We also have P
v‚ààV M(v ‚ààC) that represents the size of
the found cluster C. Since we are trying to get a single cluster that have more
I(W ‚Üív) > 0 than the total number of nodes, we want P x(W ‚Üív)√óJ(W ‚Üí
v) to exceed P
v‚ààV M(v ‚ààC) as much as possible. In addition, we are ruling out
any solution with the objective value ‚â§‚àí1 to avoid any unviolated clusters.
The two constraints are the representation of I(W ‚Üív) as node in the clus-
ter. For any cluster with I(W ‚Üív), then v must be inside such cluster. For
members of W, at least one of them should be in the same cluster. [16] have im-
plemented these two constraints by using their SCIP solver‚Äôs logicor functionality,
which is based on constraint programming. In more generic linear programming
convention, we rather express these by
‚àÄJ(W ‚Üív) : J(W ‚Üív) ‚àíM(v ‚ààC) = 0
(4.5)
‚àÄJ(W ‚Üív) : J(W ‚Üív) ‚â§
X
w‚ààW
M(w ‚ààC)
(4.6)
where the Ô¨Årst constraint simply says M(v ‚ààC) must be 1 if J(W ‚Üív) = 1, and
the second constraint forces at least one M(w ‚ààC) to be 1 if J(W ‚Üív) = 1.

23
We pass this formulation to the solver to get the best cluster cuts possible.
When the solver returns the solution with the values of each M(v ‚ààC), we now
have a cluster C to add to the main model. We add additional constraint as
expressed in Equation 4.1, but only for the particular cluster found by the above
model.
Given that this is a relatively simple ILP problem, we can obtain the cluster
cut in a very short amount of time in conjunction with a fast solver. While there
is an alternative approach that formulates this problem as an all-pairs shortest
path algorithm[27] based on the same ideas explained here, we chose to use ILP
solvers as it was more practical to achieve faster solving process with the solver
than trying to solve this problem with an algorithm written in Python.
4.2
Cycle Cuts
We add exactly one cluster cuts each time we solve the sub-IP problem. However,
it might be also helpful to rule out all the cycles that can be directly detected
in the current solution and tighten the feasible region. That is, we want to add
Equation 3.13 as cutting planes as well. In fact, all the cycles can be directly
converted as cluster cuts, since all the clusters simply refer to any set of nodes
with a limit on the number of edges. However, the converse is not true since
there might be cluster cuts that might not be shown as cycles, as proved in
Proposition 3.2.1.
In order to add cycle cuts, we Ô¨Årst need to Ô¨Ånd all the existing cycles in the
current solution. There are several diÔ¨Äerent approaches to acheive this, but we
used the algorithm developed by Johnson[28], which is currently one of the best
known universally applicable version. After we get all the unique elementary
cycles, we go over each of them to get Equation 4.1 over the members of each
cycle.
4.3
Sink Finding Heuristic
With cluster cuts and cycle cuts, we now can obtain optimal bayesian networks
by solving the ILP model. However, depending on the size of dataset and the
number of variables, it might be too time consuming to wait until the program
reaches the optimal solution. Rather, we might want to get sub-optimal but
feasible solutions that might be reasonably close to the optimal one. This also
allows us to get a better lower bound on the problem and prevent excessive
branchings on the solver.

24
[16] have suggested an idea for a heuristic algorithm to acquire such solution,
which is based on the fact that every DAG has at least one sink node, or the
node that has no outgoing edge. Such sink node in a Bayesian network structure
would indicate that the variable would not directly inÔ¨Çuence others, and we can
freely choose a parent set for that variable node without creating any cycles.
Let‚Äôs suppose that we remove that node and its incoming edges. In the resulting
DAG, there should be another sink node to maintain acyclicity, which we can
again choose a parent set for it. If we keep following this rule and decide on
all the variables we have, a feasible DAG is constructed. We illustrate this idea
graphically in Figure 4.1.
So we are essentially adding nodes one by one to construct a DAG. We want
to add them in an order that maximises the total score. During the Ô¨Årst time
we decide on the node to assign parents, we check the local scores of all the
parents and pick the highest scoring set for each node. We then look at the
current relaxed solution and rank these node/parent combinations based on their
closeness to 1. We assign 1 to the variable representing the combination that
are closest to 1. Since this node is a sink node, we must make sure that there‚Äôs
no other nodes where this node becomes a parent.
We check all the parent
candidates of other nodes and assign 0 to all the variables representing parents
with this node.
All the following iterations would be identical as the Ô¨Årst except we rule out
all the parents that are already assigned 0 in previous iterations when picking
the highest scoring parent candidate.
These procedures are fully outlined in
algorithm 2.
Figure 4.1: Graphical Illustration of Using Sink Nodes to Construct a DAG.

25
Algorithm 2: Sink Finding Heuristic Algorithm
Require: current solution
Ensure: nodes to decide ‚áê0
heuristic BN ‚áêempty list
while nodes to decide > 0 do
best parents ‚áêempty list
Get all the local scores for this node sorted in descending order
for each sorted parent candidates W of this node do
if W not in heuristic BN then
best parent for this node ‚áêW
break
end if
end for
best parents ‚à™best parent for this node
best distance ‚áê‚àû
for chosen W ‚ààbest parents do
distance of W ‚áê(1.0 ‚àícurrent solution(W))
if distance of W < best distance then
best distance ‚áêdistance of W
end if
end for
heuristic BN(best distance) ‚áê1
for all other nodes do
heuristic BN(parent) ‚áê0 for all parents with W
end for
for all other parents of this node do
heuristic BN(other parents) ‚áê0
end for
end while
return heuristic BN

Chapter 5
Implementation and Experiments
Based on the ILP formulation with problem-speciÔ¨Åc cutting planes and heuristic
algorithm developed in chapter 3 and 4, we present the details of our computer
program implementation written in Python language with Gurobi solver and ex-
amine its performance on the reference datasets.
5.1
Implementation Details
Figure 5.1: Bayene Program Design
We created a Python package named Bayene (Bayesian network) that discovers
an optimal Bayesian network structure based on our ILP formulation given the
data input. Refer to Figure 5.1 for the overall structural design of Bayene. We
have solution controller that takes charge of controlling the overall solving process
and facilitating communication between the model writer and solver. Two model
writers, one for the main model and another for the cluster cut Ô¨Ånding model,
transforms ILP problems as programming objects that can be transferred to the
solver and always modiÔ¨Åable through dedicated functions when needed. Model
writers and interfaces to solvers are written with Pyomo package[24]. In addition,
solution controller includes our own sink-Ô¨Ånding heurisitic algorithm, and a cycle
detector based on the elementary cycle Ô¨Ånding algorithm implementation from
NetworkX package[23].
26

27
Figure 5.2: Comparison of DiÔ¨Äerent ILP Solvers from SCIP website
GOBNILP makes an extensive use of APIs provided by the SCIP framework[1]:
SCIP leverages constraint programming (CP) techniques to solve integer pro-
gramming problems, making it one of the fastest non-commercial MIP solvers
in the world. Please see Figure 5.2 for the speed comparison between SCIP and
other existing MIP solvers.1 One thing we were curious about during the imple-
mentation was whether we could achieve even faster speeds by formulating our
problem in a more generic ILP manner and implementing simpler layers between
our program and more conventional but still faster solvers such as CPLEX and
Gurobi. Also, we wanted Bayene to be more Ô¨Çexibly designed to allow easier
adaptation of future developments in our ILP formulation and portability across
diÔ¨Äerent solvers.
5.1.1
Notes on Branch-and-Cut
The biggest challenge we faced during the implementation was to modify the
solver‚Äôs branch-and-cut process to our needs. Since we do not specify all the
cluster constraints all at once when we transfer our model to the solver, we
add them as cutting planes whenever the current relaxed solution violates the
conditions. The main problem was that due to the limitations in Pyomo package,
we were allowed to add these constraints only when the solver Ô¨Ånishes its Branch-
and-Cut process completely. This was problematic as this meant that the solver
would have to go over branch-and-bound tree to get the integer solution, only to
Ô¨Ånd that it violates the cluster constraint and needs to cut oÔ¨Ä. Also, the solver
will have to completely restart the branch-and-cut tree from scratch, which would
add even more time to the solution process. We have tried adjusting several
parameters of the solver regarding Branch-and-Cut such as limiting the time
spent or number of nodes explored, but these tunings were eventually abandoned
1http://scip.zib.de/

28
as they fail to gurantee any optimality and often terminated the process too early
without returning any feasible solution.
Even with the direct interface to the solvers however, there are some issues
with the way the solvers handle user constraints. They distinguish two type of
cuts users can add to the model, one being user cuts and another being lazy
constraints. User cuts refer to the cutting planes that are implied by the model
but cannot be directly inferred by the solver. These constraints tightens the
feasible region of the LP relaxation but does not cut oÔ¨Äany of the IP region. On
the other hand, lazy constraints are the ones that are actually required to get
the correct solution but cannot be added all at once because there are too many
of them or simply impossible to specify them all in the beginning.
Our cluster constraints fall into the second category - lazy constraints. The
problem is that these lazy constraints can only be added to the solver when they
reaches integer feasible solution, which can take signiÔ¨Åcantly more time than
checking lazy constraints at non-integer solution node. Reference manuals of
the solvers do not fully specify the reasoning behind this, but our guess is that
adding violated cutting planes at every nodes of BnB tree might complicate the
solution process too much with excessive branching.
We have eventually settled on the Pyomo-based implementation due to time
constraint. We also attempted implementing branch-and-cut outside the solver
and control the solution process by ourselves, but this showed to be extremely
diÔ¨Écult as branchings occurred very often early in the process and number of
nodes on BnB tree grew rapidly beyond our memory control despite diÔ¨Äerent
branching strategies we tried.
Understanding various behaviours and techniques of ILP solvers and adjust-
ing them for the best performance on our speciÔ¨Åc problem requires more thorough
investigation on their own, and call for more attention in the future research.
5.2
Experiments
We experimented with Bayene to see how they perform in practice using ref-
erence datasets of diÔ¨Äerent conditions. We also examined how turning on and
oÔ¨ÄBayene‚Äôs diÔ¨Äerent features - cycle cuts, sink-Ô¨Ånding heuristic, and gomory
fractional cuts - changes its behaviour or performance.
5.2.1
Setup
‚Ä¢ We used pre-calculated BDeu local score Ô¨Åles provided by Cussens, avail-
able on his GOBNILP website. These score Ô¨Åles are based on the reference

29
datasets used for benchmarks in past Bayesian Network literatures, which
the original versions can be obtained from the website Bayesian Network
Repository by Marco Scutari2 with the orignal source information.
‚Ä¢ Each dataset was sampled to have 100, 1000, and 10000 instances, and two
scores Ô¨Åles were created for every dataset, with parent set size limit of 2
and 3 respectively.
‚Ä¢ Cussens[16] have indicated that some of the parent candidate sets have
been pruned using the methods published by de Campos and Ji.[6]
‚Ä¢ Our benchmark have been conducted on Apple Mid 2012 MacBook Air
machine with Intel Core i5 3317U 1.7Ghz, 4GB RAM, and OS X 10.11
operating system.
‚Ä¢ While Bayene works on any ILP solver Pyomo supports, we used Gurobi as
it was one of the fastest commercial solvers without the problem size limit
on academic use.
‚Ä¢ We turned oÔ¨Äall the general purpose MIP cuts by setting Gurobi parameter
Cuts to 0, except for Gomory fractional cuts by setting GomoryPasses to
20000000 (unlimited).
5.2.2
Experiment 1: All Features Turned On
For the Ô¨Årst experiment, we applied both cluster cuts and cycle cuts, along with
Gomory fractional cuts by the solver. Please see Table 5.1 and Table 5.2 for the
detailed results. We were able to solve most of the ILP problems within 1-hour
limit, ranging from less than a second for asia dataset (8 attributes, 118 ILP
variables) to over 20 minutes for alarm (37 attributes, 2736 ILP variables).
Please see Figure 5.3a for example BN structure generated by Bayene for the
water dataset. Figure 5.3b shows how objective values have progressed during
the solution process. Each dots are plotted whenever we add cluster cuts and the
solver returns the current solution. ILP objectives are shown as blue dots, and
sink heuristic objectives as red dotted lines. Note that sink heuristic objective
value do not get changed unless we get bigger objective value.
In the beginning without most of cluster and cycle constraints, we begin with
a quite high objective value, but falls rapidly on the next two iterations. The
objective value then changes very little until we reach the optimum solution.
2http://www.bnlearn.com/bnrepository/

30
(a) Resulting BN structure
(b) Progression of Objective Values
Figure 5.3: Results from insurance with 1000 instances, parent size limit = 2.
Interestingly enough, the objective value of sink heuristic reaches near the range
of the actual optimum in the very beginning and do not change until the end.
Although cluster cuts and cycle cuts have been invoked same number of
times for almost all cases, the total number of cycles that have been ruled out
through cycle cuts exceeds the number of cluster cuts by a huge margin. These
large number of cuts allow the solver to reach the range of valid solutions more
quickly and eventually the optimum.
5.2.3
Experiment 2: Without Cycle Cuts
(a) insurance with 1000 instances
(b) water with 100 instances
Figure 5.4: Progression of Objective Values from DiÔ¨Äerent Datasets in the Second
Experiment. Both problems did not reach optimal solution within the time limit.
For the second experiment, we applied just the cluster cuts and kept the
Gomory cuts. Please see Table 5.3 and Table 5.4 for the detailed results. We were

31
not able to solve most of the problems within the time limit, as the objective value
progressed really slowly as seen from Figure 5.4a and Figure 5.4b. Moreover, we
observed that the speed of the solver started to slow down over each iteration,
while still having the wide gap between the sink heuristic objective value. It
seems that adding few cluster cuts already complicates the shape of the feasible
region heavily, while they don‚Äôt cut enough to reach the area with valid solutions.
Things get worse since we are restarting the branch-and-bound tree every time
we add cluster cuts. We could see that cycle cuts we add by detecting all the
elementary cycles serve a signiÔ¨Åcant role in getting the optimal solutions in a
reasonable amount of time.
5.2.4
Experiment 3: Without Gomory Cuts
(a) Resulting BN structure
(b) Progression of Objective Values
Figure 5.5: Results from water with 10000 instances, parent size limit = 3.
For the third experiment, we applied both cluster cuts and cycle cuts, but
completely disabled Gomory frational cuts from the solver. Please see Table 5.5
and Table 5.6 for the detailed results. While the third experiment was able to
solve most of the problems as the Ô¨Årst experiment did, disabling Gomory cuts
improved the solution time signiÔ¨Åcantly in many cases, especially the hardest
ones in the Ô¨Årst experiment where it took 22 minutes to solve alarm with 10000
instances but 12 minutes in the third experiment. Instead, a little more cluster
and cycle cuts were added to solve the problems than the Ô¨Årst experiment. This
implies that general purpose cutting planes like Gomory cuts that are used to
tighten the bound on BnB tree can be countereÔ¨Äective in our cases. Patterns of
objective value changes were not diÔ¨Äerent from the Ô¨Årst experiment.

Title
# Attributes
# Instances
# ILP Variables
BDeu Score
Time Elapsed (in sec)
# Cluster Cut Iterations
# Cycle Cut Iterations
# Cycle Cut Count
asia
8
100
41
‚àí245.644264
0.1605529785
3
3
11
asia
8
1000
88
‚àí2317.411506
0.493724823
8
8
25
asia
8
10000
118
‚àí22466.396546
0.5763838291
8
8
28
insurance
27
100
266
‚àí1687.683853
1.1246800423
9
9
59
insurance
27
1000
702
‚àí13892.798172
31.290997982
20
20
295
insurance
27
10000
2082
‚àí133111.964488
531.714365005
28
28
780
water
32
100
356
‚àí1501.644722
3.298566103
17
17
147
water
32
1000
507
‚àí13263.115737
5.2235310078
16
16
227
water
32
10000
813
‚àí128810.974528
10.9444692135
16
16
248
alarm
37
100
591
‚àí1362.995568
33.3234539032
23
23
446
alarm
37
1000
1309
‚àí11248.39992
258.938903093
46
46
532
alarm
37
10000
2736
‚àí105486.499123
1331.78003311
44
44
885
hailÔ¨Ånder
56
100
214
‚àí6021.269394
1.3409891129
10
10
63
hailÔ¨Ånder
56
1000
671
‚àí52473.926982
10.9388239384
27
27
194
hailÔ¨Ånder
56
10000
2260
‚àí498383.409915
942.131913185
68
69
639
carpo
60
100
2139
‚àí‚àí
‚àí‚àí
‚àí‚àí
‚àí‚àí
‚àí‚àí
carpo
60
1000
2208
‚àí‚àí
‚àí‚àí
‚àí‚àí
‚àí‚àí
‚àí‚àí
carpo
60
10000
4354
‚àí‚àí
‚àí‚àí
‚àí‚àí
‚àí‚àí
‚àí‚àí
Table 5.1: Experiment 1 with parent set size limit = 2. ‚Äò‚àí‚àí‚Äô indicates that the problem was not solved within 1 hour.
Title
# Attributes
# Instances
# ILP Variables
BDeu Score
Time Elapsed (in sec)
# Cluster Cut Iterations
# Cycle Cut Iterations
# Cycle Cut Count
asia
8
100
41
‚àí245.644264
0.2108130455
4
4
11
asia
8
1000
107
‚àí2317.411506
0.4743950367
7
7
27
asia
8
10000
161
‚àí22466.396548
1.2923400402
10
10
41
insurance
27
100
279
‚àí1686.225878
1.3009831905
10
10
66
insurance
27
1000
774
‚àí13887.350147
30.3841409683
17
17
269
insurance
27
10000
3652
‚àí‚àí
‚àí‚àí
‚àí‚àí
‚àí‚àí
‚àí‚àí
water
32
100
482
‚àí1500.988471
8.4975321293
15
15
228
water
32
1000
573
‚àí13262.367639
4.0370209217
12
12
210
water
32
10000
961
‚àí128705.656236
29.74208498
16
16
399
alarm
37
100
907
‚àí1349.227422
132.99851799
19
19
871
alarm
37
1000
1928
‚àí11240.347094
947.912580967
47
47
841
alarm
37
10000
2736
‚àí‚àí
‚àí‚àí
‚àí‚àí
‚àí‚àí
‚àí‚àí
hailÔ¨Ånder
56
100
244
‚àí6019.469926
1.100315094
7
7
78
hailÔ¨Ånder
56
1000
761
‚àí52473.24561
21.7998039722
22
22
247
hailÔ¨Ånder
56
10000
3768
‚àí‚àí
‚àí‚àí
‚àí‚àí
‚àí‚àí
‚àí‚àí
carpo
60
100
2139
‚àí‚àí
‚àí‚àí
‚àí‚àí
‚àí‚àí
‚àí‚àí
carpo
60
1000
2208
‚àí‚àí
‚àí‚àí
‚àí‚àí
‚àí‚àí
‚àí‚àí
carpo
60
10000
4354
‚àí‚àí
‚àí‚àí
‚àí‚àí
‚àí‚àí
‚àí‚àí
Table 5.2: Experiment 1 with parent set size limit = 3. ‚Äò‚àí‚àí‚Äô indicates that the problem was not solved within 1 hour.

Title
# Attributes
# Instances
# ILP Variables
BDeu Score
Time Elapsed (in sec)
# Cluster Cut Iterations
# Cycle Cut Iterations
# Cycle Cut Count
asia
8
100
41
‚àí245.644264
1.2105300427
30
NA
NA
asia
8
1000
88
‚àí2317.411506
3.3687419891
60
NA
NA
asia
8
10000
118
‚àí22466.396546
4.5231909752
64
NA
NA
insurance
27
100
266
‚àí‚àí
‚àí‚àí
‚àí‚àí
NA
NA
insurance
27
1000
702
‚àí‚àí
‚àí‚àí
‚àí‚àí
NA
NA
insurance
27
10000
2082
‚àí‚àí
‚àí‚àí
‚àí‚àí
NA
NA
water
32
100
356
‚àí‚àí
‚àí‚àí
‚àí‚àí
NA
NA
water
32
1000
507
‚àí‚àí
‚àí‚àí
‚àí‚àí
NA
NA
water
32
10000
813
‚àí‚àí
‚àí‚àí
‚àí‚àí
NA
NA
alarm
37
100
591
‚àí‚àí
‚àí‚àí
‚àí‚àí
NA
NA
alarm
37
1000
1309
‚àí‚àí
‚àí‚àí
‚àí‚àí
NA
NA
alarm
37
10000
2736
‚àí‚àí
‚àí‚àí
‚àí‚àí
NA
NA
hailÔ¨Ånder
56
100
214
‚àí‚àí
‚àí‚àí
‚àí‚àí
NA
NA
hailÔ¨Ånder
56
1000
671
‚àí‚àí
‚àí‚àí
‚àí‚àí
NA
NA
hailÔ¨Ånder
56
10000
2260
‚àí‚àí
‚àí‚àí
‚àí‚àí
NA
NA
carpo
60
100
2139
‚àí‚àí
‚àí‚àí
‚àí‚àí
NA
NA
carpo
60
1000
2208
‚àí‚àí
‚àí‚àí
‚àí‚àí
NA
NA
carpo
60
10000
4354
‚àí‚àí
‚àí‚àí
‚àí‚àí
NA
NA
Table 5.3: Experiment 2 with parent set size limit = 2. ‚Äò‚àí‚àí‚Äô indicates that the problem was not solved within 1 hour.
Title
# Attributes
# Instances
# ILP Variables
BDeu Score
Time Elapsed (in sec)
# Cluster Cut Iterations
# Cycle Cut Iterations
# Cycle Cut Count
asia
8
100
41
‚àí245.644264
1.2677178383
29
NA
NA
asia
8
1000
107
‚àí2317.411506
2.9705760479
50
NA
NA
asia
8
10000
161
‚àí22466.396547
5.4340219498
59
NA
NA
insurance
27
100
279
‚àí‚àí
‚àí‚àí
‚àí‚àí
NA
NA
insurance
27
1000
774
‚àí‚àí
‚àí‚àí
‚àí‚àí
NA
NA
insurance
27
10000
3652
‚àí‚àí
‚àí‚àí
‚àí‚àí
NA
NA
water
32
100
482
‚àí‚àí
‚àí‚àí
‚àí‚àí
NA
NA
water
32
1000
573
‚àí‚àí
‚àí‚àí
‚àí‚àí
NA
NA
water
32
10000
961
‚àí‚àí
‚àí‚àí
‚àí‚àí
NA
NA
alarm
37
100
907
‚àí‚àí
‚àí‚àí
‚àí‚àí
NA
NA
alarm
37
1000
1928
‚àí‚àí
‚àí‚àí
‚àí‚àí
NA
NA
alarm
37
10000
2736
‚àí‚àí
‚àí‚àí
‚àí‚àí
NA
NA
hailÔ¨Ånder
56
100
244
‚àí‚àí
‚àí‚àí
‚àí‚àí
NA
NA
hailÔ¨Ånder
56
1000
761
‚àí‚àí
‚àí‚àí
‚àí‚àí
NA
NA
hailÔ¨Ånder
56
10000
3768
‚àí‚àí
‚àí‚àí
‚àí‚àí
NA
NA
carpo
60
100
5068
‚àí‚àí
‚àí‚àí
‚àí‚àí
NA
NA
carpo
60
1000
3827
‚àí‚àí
‚àí‚àí
‚àí‚àí
NA
NA
carpo
60
10000
16391
‚àí‚àí
‚àí‚àí
‚àí‚àí
NA
NA
Table 5.4: Experiment 2 with parent set size limit = 3. ‚Äò‚àí‚àí‚Äô indicates that the problem was not solved within 1 hour.

Title
# Attributes
# Instances
# ILP Variables
BDeu Score
Time Elapsed (in sec)
# Cluster Cut Iterations
# Cycle Cut Iterations
# Cycle Cut Count
asia
8
100
41
‚àí245.644264
0.229927063
4
4
11
asia
8
1000
88
‚àí2317.411506
0.5296328068
8
8
25
asia
8
10000
118
‚àí22466.396546
0.6273970604
8
8
28
insurance
27
100
266
‚àí1687.683853
1.0839219093
9
9
59
insurance
27
1000
702
‚àí13892.798172
26.9869270325
21
21
318
insurance
27
10000
2082
‚àí133111.964488
319.534489155
28
28
776
water
32
100
356
‚àí1501.644722
6.8530170918
20
20
183
water
32
1000
507
‚àí13263.115737
4.921047926
16
16
186
water
32
10000
813
‚àí128810.974528
48.8902380466
19
19
981
alarm
37
100
591
‚àí1362.995568
28.4560148716
23
23
434
alarm
37
1000
1309
‚àí11248.39992
173.517156839
47
47
548
alarm
37
10000
2736
‚àí105486.499123
721.979949951
45
45
869
hailÔ¨Ånder
56
100
214
‚àí6021.269394
1.3437559605
10
10
63
hailÔ¨Ånder
56
1000
671
‚àí52473.926982
9.6990509033
24
24
191
hailÔ¨Ånder
56
10000
2260
‚àí‚àí
‚àí‚àí
‚àí‚àí
‚àí‚àí
‚àí‚àí
carpo
60
100
2139
‚àí‚àí
‚àí‚àí
‚àí‚àí
‚àí‚àí
‚àí‚àí
carpo
60
1000
2208
‚àí‚àí
‚àí‚àí
‚àí‚àí
‚àí‚àí
‚àí‚àí
carpo
60
10000
4354
‚àí‚àí
‚àí‚àí
‚àí‚àí
‚àí‚àí
‚àí‚àí
Table 5.5: Experiment 3 with parent set size limit = 2. ‚Äò‚àí‚àí‚Äô indicates that the problem was not solved within 1 hour.
Title
# Attributes
# Instances
# ILP Variables
BDeu Score
Time Elapsed (in sec)
# Cluster Cut Iterations
# Cycle Cut Iterations
# Cycle Cut Count
asia
8
100
41
‚àí245.644264
0.2282519341
4
4
11
asia
8
1000
107
‚àí2317.411506
0.5901920795
7
7
27
asia
8
10000
161
‚àí22466.396546
1.231169939
10
10
41
insurance
27
100
279
‚àí1686.225878
1.3578009605
10
10
66
insurance
27
1000
774
‚àí13887.350147
20.3226950169
17
17
269
insurance
27
10000
3652
‚àí‚àí
‚àí‚àí
‚àí‚àí
‚àí‚àí
‚àí‚àí
water
32
100
482
‚àí1500.968391
8.6199500561
13
13
206
water
32
1000
573
‚àí13262.465272
7.5233428478
13
13
227
water
32
10000
961
‚àí128705.731312
28.2011299133
13
13
456
alarm
37
100
907
‚àí1349.227422
121.306695938
19
19
871
alarm
37
1000
1928
‚àí11240.347094
754.878758907
44
44
854
alarm
37
10000
6473
‚àí‚àí
‚àí‚àí
‚àí‚àí
‚àí‚àí
‚àí‚àí
hailÔ¨Ånder
56
100
244
‚àí6019.469926
1.1171729565
7
7
78
hailÔ¨Ånder
56
1000
761
‚àí52473.24561
14.2832479477
22
22
258
hailÔ¨Ånder
56
10000
3768
‚àí‚àí
‚àí‚àí
‚àí‚àí
‚àí‚àí
‚àí‚àí
carpo
60
100
5068
‚àí‚àí
‚àí‚àí
‚àí‚àí
‚àí‚àí
‚àí‚àí
carpo
60
1000
3827
‚àí‚àí
‚àí‚àí
‚àí‚àí
‚àí‚àí
‚àí‚àí
carpo
60
10000
16391
‚àí‚àí
‚àí‚àí
‚àí‚àí
‚àí‚àí
‚àí‚àí
Table 5.6: Experiment 3 with parent set size limit = 3. ‚Äò‚àí‚àí‚Äô indicates that the problem was not solved within 1 hour.

35
5.2.5
Performance of Sink Finding Heuristic
Figure 5.6: Percentage diÔ¨Äerence between best heuristic objective at each itera-
tion and the Ô¨Ånal optimal objective value, on insurance dataset of 1000 instances
and parent size limit = 3.
One interesting aspect of our sink Ô¨Ånding heuristic is the proximity of its
output to the Ô¨Ånal optimal solution. For the case of insurance dataset as seen
on Figure 5.6, the heuristic algorithm was able to produce a solution with an
objective value that diÔ¨Äered around 6% from the Ô¨Ånal optimal objective value,
even for the Ô¨Årst iteration. It soon reached around 1% in next few iterations.
While we only have limited knowledge about the shape of polytopes for BN
structures, we can see that our sink Ô¨Ånding heuristic can reach the vicinity of
the optimal solution quite well. Although Bayene sends the solutions generated
by the heuristic to the solver for warm starts, it seems that the solver does not
actually make much use of them since the solutions for the relaxed problem yet
without all the necessary cuts are bigger than the sink Ô¨Ånding heuristic solution.
Further study of BN structure polytopes and solutions generated by the sink
Ô¨Ånding heuristic would help us to get optimal solutions faster.

Chapter 6
Conclusion
This dissertation reviewed the conceptual foundation behind Bayesian network
and studied formulating the problem of learning the BN structure from data
as an integer linear programming problem. We went over the inner workings
of score metrics used to measure the statistical Ô¨Åt of the BN structure to the
dataset, and presented the ILP formulation based on the decomposability of the
score metric. In order to deal with the exponential number of constraints, we
investigated diÔ¨Äerent ways to add constraints to the model on the Ô¨Çy as cutting
planes rather than fully specifying them initially.
We implemented the ILP formulation and cutting planes as a computer soft-
ware and conducted a benchmark on various reference datasets. We saw that
ruling out all the cycles found in the solution at each iteration is critical to
reaching the optimality in a reasonable amount of time. We also found out that
general purpose cutting planes such as Gomory fractional cuts that are used to
tighten the bound on BnB tree can backÔ¨Åre in some cases such as ours. Lastly,
we discovered that our sink Ô¨Ånding heuristic algorithm returns solutions that are
quite close to the Ô¨Ånal optimal solution very early on in the process.
6.1
Future Directions
This section will present some of the ideas for the further development of Bayene
from two aspects: its statistical modelling capabilities and mathematical opti-
misation techniques employed.
6.1.1
Statistical Modelling
Alternate Scoring Functions
For this dissertation, we focused on using Bayesian Dirichlet-based score met-
ric, which is based on the assumption of multinomial data and Dirichlet prior.
We used BDeu score metric, which adds additional assumptions of likelihood
equivalence and uniform prior probabilities. In addition to Dirichlet-based score
36

37
metric, there are a number of diÔ¨Äerent information theoretic score functions
such as MDL and BIC used in BN literatures. Also, there have been some recent
development on score metrics such as SparsityBoost by Brenner and Sontag[4]
that reduces computational burden and attempts to incorporate aspects of con-
ditional independence testing. Understanding diÔ¨Äerences between these score
metrics would be important in making the eÔ¨Äectiveness of our Bayesian network
structure learning as a statistical model.
Other Types of Statistical Distribution
Adding on to alternate score metrics, versatility of Bayesian network and its
structure learning can be expanded by making it applicable to diÔ¨Äerent types of
distribution. There already have been done for learning BN structure on con-
tinous distribution, but mostly based on conditional independence information.
Figuring out ways to allow more types of distribution, especially for the ILP
formulation, would be an interesting and worthwhile challenge.
6.1.2
Optimisation
Leveraging the Graph Structure
While we did not directly make much use of the fact that the structure of
Bayesian network is DAG, there were few researches that did in the last few
years, including the one by Studeny et al.[32] that introduced the concept of char-
acteristic imset, which stems from the property of Markov equivalent structures
described in subsection 2.1.2, and another with a set of additional treewidth con-
straints by Parviainen et al.[30]. Empirical results on these approaches showed
to be signiÔ¨Åcantly slower than ours, but it would be interesting to go further
with these leads from combinatorial optimisation perspective.
Advanced Modelling Techniques
We benchmarked on the pre-calculated local score Ô¨Åles that have parent set
size limit of either 2 or 3. While our formulation theoretically works on bigger
parent set sizes, we currently haven‚Äôt employed more advanced techniques such
as column generation that could make it possible to deal with extremely large
number of variables. Further development of incorporating such techniques to
Bayene would allow us to handle larger datasets.

38
Alternate Optimisation Paradigm
Aside from the integer linear programming, there have been eÔ¨Äorts to use alter-
nate optimsation scheme such as constraint programming.[3] While they were
not decisively better than our ILP approach, they did show some promising re-
sults. It would be worthwhile to examine the inner workings of their approach
in order to improve our formulation.
Deeper Integration with Branch-and-Cut
As seen from section 5.1, we had some issues with adjusting the solver‚Äôs branch-
and-cut algorithm to our needs, as there were complications resulting from var-
ious techniques and restrictions involved with the solver programs. In order to
make Bayene suitable for more learning tasks, thorough inspection of how the ILP
solvers perform optimisation would be needed to prevent ineÔ¨Écent operations.

Appendix A
Software Instructions
Bayene can be downloaded from the following link: https://link.iamblogger.
net/4khv1. Bayene itself is not a standalone program but rather a Python library,
so the user needs to inherit the class from the package to his or her application.
For evaluation purposes, we provide a test script Ô¨Åle sampleÀôscript.py that allows
the user to testdrive Bayene.
There‚Äôs no formal install functionality yet on Bayene, so the user needs to
install all the dependencies manually. Bayene was written for CPython 2.7 series,
and will not work on any other implementation of Python such as Python 3 or
PyPy. Please download the appropriate version of Python 2.7 for your platform
from https://www.python.org/downloads/.
If your system does not have pip, please refer to https://pip.pypa.io/en/
latest/installing.html for install instructions. After installing pip, please
turn on Command Prompt on Windows or Terminal on OS X or Linux with admin-
istrator or root access and type pip install pyomo numpy scipy, which will install
all the required libaries for Bayene. In addition, you need to install gurobipy
package included with the installation of Gurobi solver, which the instructions
are provided in http://www.gurobi.com/documentation/.
After installing all the dependencies, please open sampleÀôscript.py with a plain
text editor.
Please edit the string on line 12 to specify the score Ô¨Åles that
needs to be tested. Please note that all the score Ô¨Åles used for this disserta-
tion is available in http://www-users.cs.york.ac.uk/~jc/research/uai11/
ua11_scores.tgz.
Lastly, go back to Command Prompt or Terminal, navigate to the directory
where sampleÀôscript.py is located and type python sampleÀôscript.py. Please refer to
the source code for further information.
39

Bibliography
[1]
Tobias Achterberg et al. ‚ÄúConstraint integer programming: A new approach
to integrate CP and MIP‚Äù. In: Integration of AI and OR techniques in
constraint programming for combinatorial optimization problems. Springer,
2008, pp. 6‚Äì20.
[2]
Mark Bartlett and James Cussens. ‚ÄúInteger Linear Programming for the
Bayesian network structure learning problem‚Äù. In: ArtiÔ¨Åcial Intelligence
(2015).
[3]
Peter van Beek and Hella-Franziska HoÔ¨Ämann. ‚ÄúMachine learning of Bayesian
networks using constraint programming‚Äù. In: ().
[4]
Eliot Brenner and David Sontag. ‚ÄúSparsityBoost: A New Scoring Func-
tion for Learning Bayesian Network Structure‚Äù. In: CoRR abs/1309.6820
(2013). url: http://arxiv.org/abs/1309.6820.
[5]
Wray Buntine. ‚ÄúTheory reÔ¨Ånement on Bayesian networks‚Äù. In: Proceedings
of the Seventh conference on Uncertainty in ArtiÔ¨Åcial Intelligence. Morgan
Kaufmann Publishers Inc. 1991, pp. 52‚Äì60.
[6]
Cassio Polpo de Campos and Qiang Ji. ‚ÄúProperties of Bayesian Dirichlet
Scores to Learn Bayesian Network Structures.‚Äù In: AAAI. 2010, pp. 431‚Äì
436.
[7]
Alexandra M Carvalho. ‚ÄúScoring functions for learning Bayesian networks‚Äù.
In: ().
[8]
Eugene Charniak. ‚ÄúBayesian networks without tears.‚Äù In: AI magazine 12.4
(1991), p. 50.
[9]
David Maxwell Chickering. ‚ÄúLearning Bayesian networks is NP-complete‚Äù.
In: Learning from data. Springer, 1996, pp. 121‚Äì130.
[10]
David Maxwell Chickering, David Heckerman, and Christopher Meek. ‚ÄúLarge-
sample learning of Bayesian networks is NP-hard‚Äù. In: The Journal of Ma-
chine Learning Research 5 (2004), pp. 1287‚Äì1330.
[11]
Jens Clausen. ‚ÄúBranch and bound algorithms-principles and examples‚Äù.
In: Department of Computer Science, University of Copenhagen (1999),
pp. 1‚Äì30.

BIBLIOGRAPHY
[12]
M. Conforti, G. Cornuejols, and G. Zambelli. Integer Programming. Grad-
uate Texts in Mathematics. Springer International Publishing, 2014. isbn:
9783319110080.
[13]
Gregory F Cooper and Edward Herskovits. ‚ÄúA Bayesian method for the
induction of probabilistic networks from data‚Äù. In: Machine learning 9.4
(1992), pp. 309‚Äì347.
[14]
James Cussens. ‚ÄúBayesian network learning with cutting planes‚Äù. In: arXiv
preprint arXiv:1202.3713 (2012).
[15]
James Cussens. ‚ÄúMaximum likelihood pedigree reconstruction using integer
programming‚Äù. In: Proceedings of WCB 2010 (), p. 9.
[16]
James Cussens and Mark Bartlett. ‚ÄúGOBNILP 1.6.1 User/Developer Man-
ual‚Äù. In: (2015).
[17]
James Cussens, Brandon Malone, and Changhe Yuan. ‚ÄúTutorial on Opti-
mal Algorithms for Learning Bayesian Networks‚Äù. In: ().
[18]
R¬¥on¬¥an Daly, Qiang Shen, and Stuart Aitken. ‚ÄúLearning Bayesian net-
works: approaches and issues‚Äù. In: The Knowledge Engineering Review
26.02 (2011), pp. 99‚Äì157.
[19]
Sanjoy Dasgupta. ‚ÄúLearning polytrees‚Äù. In: Proceedings of the Fifteenth
conference on Uncertainty in artiÔ¨Åcial intelligence. Morgan Kaufmann Pub-
lishers Inc. 1999, pp. 134‚Äì141.
[20]
Dan Geiger, Tom S Verma, and Judea Pearl. ‚Äúd-separation: From theorems
to algorithms‚Äù. In: arXiv preprint arXiv:1304.1505 (2013).
[21]
Ralph E Gomory. ‚ÄúAn algorithm for integer solutions to linear programs‚Äù.
In: Recent advances in mathematical programming 64 (1963), pp. 260‚Äì302.
[22]
Martin Gr¬®otschel, Michael J¬®unger, and Gerhard Reinelt. ‚ÄúOn the acyclic
subgraph polytope‚Äù. In: Mathematical Programming 33.1 (1985), pp. 28‚Äì
42.
[23]
Aric A. Hagberg, Daniel A. Schult, and Pieter J. Swart. ‚ÄúExploring network
structure, dynamics, and function using NetworkX‚Äù. In: Proceedings of the
7th Python in Science Conference (SciPy2008). Pasadena, CA USA, Aug.
2008, pp. 11‚Äì15.
[24]
William E Hart, Jean-Paul Watson, and David L WoodruÔ¨Ä. ‚ÄúPyomo: mod-
eling and solving mathematical programs in Python‚Äù. In: Mathematical
Programming Computation 3.3 (2011), pp. 219‚Äì260.
[25]
David Heckerman and Dan Geiger. Likelihoods and parameter priors for
Bayesian networks. Tech. rep. Citeseer, 1995.

BIBLIOGRAPHY
[26]
David Heckerman, Dan Geiger, and David M Chickering. ‚ÄúLearning Bayesian
networks: The combination of knowledge and statistical data‚Äù. In: Machine
learning 20.3 (1995), pp. 197‚Äì243.
[27]
Tommi Jaakkola et al. ‚ÄúLearning Bayesian network structure using LP
relaxations‚Äù. In: International Conference on ArtiÔ¨Åcial Intelligence and
Statistics. 2010, pp. 358‚Äì365.
[28]
Donald B Johnson. ‚ÄúFinding all the elementary circuits of a directed graph‚Äù.
In: SIAM Journal on Computing 4.1 (1975), pp. 77‚Äì84.
[29]
Ailsa H Land and Alison G Doig. ‚ÄúAn automatic method of solving discrete
programming problems‚Äù. In: Econometrica: Journal of the Econometric
Society (1960), pp. 497‚Äì520.
[30]
Pekka Parviainen, Hossein Shahrabi Farahani, and Jens Lagergren. ‚ÄúLearn-
ing bounded tree-width Bayesian networks using integer linear program-
ming‚Äù. In: Proc. 17th Int. Conf. on AI and Stat. 2014, pp. 751‚Äì759.
[31]
Judea Pearl. d-SEPARATION WITHOUT TEARS (At the request of many
readers). url: http://bayes.cs.ucla.edu/BOOK-2K/d-sep.html.
[32]
Milan Studen¬¥y and David Haws. ‚ÄúLearning Bayesian network structure:
Towards the essential graph by integer linear programming tools‚Äù. In: In-
ternational Journal of Approximate Reasoning 55.4 (2014), pp. 1043‚Äì1071.
[33]
Thomas Verma and Judea Pearl. ‚ÄúEquivalence and synthesis of causal
models‚Äù. In: Proceedings of the Sixth Annual Conference on Uncertainty
in ArtiÔ¨Åcial Intelligence. Elsevier Science Inc. 1990, pp. 255‚Äì270.
[34]
H Paul Williams. ‚ÄúModel building in mathematical programming‚Äù. In:
(1999).
[35]
Harry Zhang. ‚ÄúThe optimality of naive Bayes‚Äù. In: AA 1.2 (2004), p. 3.

