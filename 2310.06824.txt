THE GEOMETRY
OF TRUTH:
EMERGENT LINEAR
STRUCTURE IN LARGE LANGUAGE MODEL REPRE-
SENTATIONS OF TRUE/FALSE DATASETS
Samuel Marks
Northeastern University
s.marks@northeastern.edu
Max Tegmark
MIT
ABSTRACT
Large Language Models (LLMs) have impressive capabilities, but are also prone
to outputting falsehoods. Recent work has developed techniques for inferring
whether a LLM is telling the truth by training probes on the LLM’s internal acti-
vations. However, this line of work is controversial, with some authors pointing
out failures of these probes to generalize in basic ways, among other conceptual
issues. In this work, we curate high-quality datasets of true/false statements and
use them to study in detail the structure of LLM representations of truth, drawing
on three lines of evidence: 1. Visualizations of LLM true/false statement repre-
sentations, which reveal clear linear structure. 2. Transfer experiments in which
probes trained on one dataset generalize to different datasets. 3. Causal evidence
obtained by surgically intervening in a LLM’s forward pass, causing it to treat
false statements as true and vice versa. Overall, we present evidence that lan-
guage models linearly represent the truth or falsehood of factual statements. We
also introduce a novel technique, mass-mean probing, which generalizes better
and is more causally implicated in model outputs than other probing techniques.
1
INTRODUCTION
Despite their impressive capabilities, large language models (LLMs) do not always output true text
(Lin et al., 2022; Steinhardt, 2023; Park et al., 2023). In some cases, this is because they do not know
better. In other cases, LLMs apparently know that generated statements are false but output them
anyway. For instance, Perez et al. (2022) demonstrate that LLM assistants output more falsehoods
when prompted with the biography of a less-educated user. More starkly, OpenAI (2023) documents
a case where a GPT-4-based agent gained a person’s help in solving a CAPTCHA by lying about
being a vision-impaired human. “I should not reveal that I am a robot,” the agent wrote in an internal
chain-of-thought scratchpad, “I should make up an excuse for why I cannot solve CAPTCHAs.”
We would like techniques which, given a language model M and a statement s, determine whether
M believes s to be true (Christiano et al., 2021). One approach to this problem relies on inspecting
model outputs; for instance, the internal chain-of-thought in the above example provides evidence
that the model understood it was generating a falsehood. An alternative class of approaches instead
leverages access to M’s internal state when processing s. There has been considerable recent work
on this class of approaches: Azaria & Mitchell (2023), Li et al. (2023b), and Burns et al. (2023) all
train probes for classifying truthfulness based on a LLM’s internal activations. In fact, the probes
of Li et al. (2023b) and Burns et al. (2023) are linear probes, suggesting the presence of a “truth
direction” in model internals.
However, the efficacy and interpretation of these results are controversial. For instance, Levinstein &
Herrmann (2023) note that the probes of Azaria & Mitchell (2023) fail to generalize in basic ways,
such as to statements containing the word “not.” The probes of Burns et al. (2023) have similar
generalization issues, especially when using representations from autoregressive transformers. This
suggests that these probes may be identifying not truth, but other features which correlate with truth
on their training data.
1
arXiv:2310.06824v1  [cs.AI]  10 Oct 2023

−20
−10
0
10
−10
0
10
20
−20
−10
0
10
−15
−10
−5
0
5
10
15
−5
0
5
10
−5
0
5
10
−10
0
10
−10
0
10
20
Dataset visualizations with PCA
cities
sp_en_trans
larger_than
cities_cities_conj
PC1
PC2
False
True
Figure 1: Top two principal components of the LLaMA-13B layer 12 residual stream representations
of statements in our datasets.
In this work, we shed light on this murky state of affairs. We first curate high-quality datasets
of true/false factual statements which are uncontroversial, unambiguous, and simple (section 2).
Then, working with the autoregressive transformer LLaMA-13B (Touvron et al., 2023) as a testbed,
we study in detail the structure of LLM truth representations, drawing on multiple lines of evidence:
• PCA visualizations of LLM representations of true/false statements display clear lin-
ear structure (section 3), with true statements separating from false ones in the top PCs
(see figure 1). Although visually-apparent axes of separation do not always align between
datasets (figure 3), we argue that this is compatible with the presence of a truth direction in
LLM representations (section 3.2).
• Linear probes trained to classify truth on one dataset generalize well to other datasets
(section 4). For instance, probes trained only on statements of the form “x is larger/smaller
than y” achieve near-perfect accuracy when evaluated on our Spanish-English translation
dataset. We also show that this is not explained by LLMs linearly representing the differ-
ence between probable and improbable text.
• Truth directions identified by probes are causally implicated in model outputs (sec-
tion 5). By adding truth vectors into the residual stream above certain tokens, we can cause
LLaMA-13B to treat false statements introduced in-context as true, and vice-versa.
Improving our understanding of the structure of LLM truth representations also improves our ability
to extract LLM beliefs: based on geometrical considerations, we introduce mass-mean probing1,
a simple, optimization-free probing technique which may also be of interest outside of the study of
LLM truth representations (section 4.1). We find that mass-mean probes generalize better and are
more causally implicated in model outputs than other probing methods.
Overall, this work provides strong evidence that LLM representations contain a truth direction
and makes progress on extracting this direction given access to true/false datasets.
Our code,
datasets, and an interactive dataexplorer are available at https://saprmarks.github.io/
geometry-of-truth/.
1.1
RELATED WORK
Linear world models. Substantial previous work has centered on the question of whether LLMs
have world models decodable from their representations (Li et al., 2023a; 2021; Abdou et al., 2021;
Patel & Pavlick, 2022). Early work especially focused on whether individual neurons represent
features (Wang et al., 2022; Sajjad et al., 2022; Bau et al., 2020), but features may more generally
be represented by directions in a LLM’s latent space (i.e. linear combinations of neurons) (Dalvi
et al., 2018; Gurnee et al., 2023; Cunningham et al., 2023; Elhage et al., 2022). If a model represents
a feature along a single direction in its latent space, then we say the model linearly represents the
feature. Just as other authors have asked whether models have directions representing the concepts
of “West Africa” (Goh et al., 2021) or “basketball” (Gurnee et al., 2023), we ask here whether there
is a direction corresponding to the truth or falsehood of a factual statement.
1Mass-mean probing is named after the mass-mean shift intervention of Li et al. (2023b)
2

Probing for truthfulness. Other authors have trained probes to classify truthfulness from LLM
activations, using both logistic regression (Azaria & Mitchell, 2023; Li et al., 2023b) and unsuper-
vised techniques (Burns et al., 2023). This work differs from prior work in a number of ways. First,
our datasets consist only of clear, simple, and unambiguous factual statements, unlike the intention-
ally misleading question/answer pairs of Li et al. (2023b); Lin et al. (2022), the complicated and
inconsistently structured prompts of Burns et al. (2023), and the sometimes confusing statements of
Azaria & Mitchell (2023); Levinstein & Herrmann (2023). Second, a cornerstone of our analysis is
evaluating whether probes trained on one dataset transfer to other topically and structurally different
datasets in terms of both accuracy and causal mediation of model outputs.2 Third, we go beyond the
mass-mean shift interventions of Li et al. (2023b) by introducing and systematically studying the
properties of mass-mean probes; this improved understanding allows us to perform causal interven-
tions which are more localized than those of loc. cit.
Causal methods. Accurate generalization of probes trained on one dataset to other datasets is
an inherently correlational observation, a lax standard of evidence for evaluating interpretability
hypotheses (Hewitt & Liang, 2019; Belinkov, 2022). For instance, probes trained to identify a
feature f could generalize well by instead relying on some feature f ′ ̸= f which is correlated
with f on both the train and test sets. A more stringent standard of evidence is causal evidence
that targeted changes to a model’s execution produce outputs consistent with the predictions of an
interpretability hypothesis (Pearl, 2001; Vig et al., 2020; Meng et al., 2022; Li et al., 2023b).
2
DATASETS
In this work, we scope truth to mean the truth or falsehood of a factual statement. Appendix A
further clarifies this definition and its relation to definitions used elsewhere.
We introduce two classes of datasets, shown in table 1. Our curated datasets consist of state-
ments which are uncontroversial, unambiguous, and simple enough that LLaMA-13B is likely to
understand whether they are true or false. For example, “The city of Zagreb is in Japan” (false)
or “The Spanish word ‘nariz’ does not mean ‘giraffe’ ” (true). Following Levinstein & Herrmann
(2023), some of our datasets are formed from others by negating statements by adding “not” (e.g.
neg cities consists of negations of statements in cities) or by taking logical conjunctions
(e.g. cities cities conj consists of statements of the form “It is the case both that s1 and
that s2” where s1 and s2 are statements from cities). In addition to our true/false datasets, we
introduce another dataset, likely, consisting of nonfactual text where the final token is either the
most likely or the 100th most likely completion, according to LLaMA-13B. We use this to disam-
2Burns et al. (2023); Azaria & Mitchell (2023); Levinstein & Herrmann (2023) do test the transfer accuracy
of probes (with mixed results), but do not perform any causal mediation experiments, even on their probes’
train sets.
Table 1: Our datasets
name
topic
rows
cities
locations of world cities
1496
sp en trans
Spanish-English translation
354
neg cities
negations of statments in cities
1496
neg sp en trans
negations of statements in sp en trans
354
larger than
Numerical comparisons: larger than
1980
smaller than
Numerical comparisons: smaller than
1980
cities cities conj
Conjunctions of two statements in cities
1500
cities cities disj
Disjunctions of two statements in cities
1500
likely
Nonfactual text with likely or unlikely final tokens
10000
companies true false
The headquarters and industries of companies
1200
common claim true false
Various claims
4450
counterfact true false
Various factual recall claims
31960
3

−20
−10
0
10
−10
−5
0
5
10
15
20
−2
0
2
4
−2
−1
0
1
2
3
4
−5
0
−4
−2
0
2
4
−20
−10
0
10
−20
−15
−10
−5
0
5
10
−2
0
2
−1
0
1
2
3
−5
0
5
10
−5
0
5
10
−2
0
2
−2
−1
0
1
2
−5
0
5
−4
−2
0
2
4
Dataset visualizations in various PCA bases
cities
larger_than
sp_en_trans
counterfact_true_false
cities
larger_than
False
True
Plotted dataset
PCA dataset
Figure 2: LLaMA-13B layer 12 residual stream representations of datasets, visualized after pro-
jection onto top PCs of other datasets. If DPCA is the dataset given on the y-axis and Dplot is the
dataset given on the x-axis, then the corresponding subplot is produced by computing the top two
PCs of DPCA and then projecting Dplot onto these PCs. Thus the subspace shown is the same across
rows and the data shown is the same across columns.
biguate between the text which is true and text which is likely. For more details on the construction
of these datasets, including statement templates, see appendix G.
Our uncurated datasets are more difficult test sets adapted from other sources.
They con-
tain claims which are sometimes ambiguous, malformed, controversial, or unlikely for the
model to know the fact-of-the-matter about. The uncurated sets are companies true false,
common claim true false, and counterfact true false, adapted from Azaria &
Mitchell (2023), Casper et al. (2023), and Meng et al. (2022), respectively.
3
VISUALIZING LLM REPRESENTATIONS OF TRUE/FALSE DATASETS
We begin our investigation with a simple technique: visualizing LLaMA-13B representations of our
datasets using principal component analysis (PCA). We observe clear linear structure in the top two
principal components (PCs) of our datasets, with true statements linearly separating from false ones.
As explored in appendix B, this structure emerges rapidly in early-middle layers and emerges later
for datasets of more structurally complicated statement (e.g. conjunctive statements).
Here and throughout this paper, we extract residual stream activations over the final token of the
input statements, all of which end with a period. This choice is discussed in appendix H. We also
center the representations in each dataset by subtracting off their mean. In this section, we use the
residual stream in layer 12, selected for being the earliest layer in which linear structure had emerged
for all of our true/false datasets.
We encourage readers to view our online dataexplorer at https://saprmarks.github.io/
geometry-of-truth/dataexplorer, which contains interactive versions of these visual-
izations.
3.1
KEY OBSERVATIONS
True and false statements separate in the top few PCs (figures 1 and 2). Moreover, after project-
ing away these PCs, there remains essentially no linearly-accessible information for distinguishing
4

−10
0
10
−20
−15
−10
−5
0
5
10
−20
−10
0
10
−10
−5
0
5
10
15
20
25
−10
−5
0
5
−5
0
5
10
Joint visualizations of statements and their opposites
cities + neg_cities
sp_en_trans + neg_sp_en_trans
larger_than + smaller_than
PC1
PC2
True,  larger_than
False, larger_than
False, smaller_than
True,  smaller_than
True, cities
False, cities
False, neg_cities
True,  neg_cities
True,  sp_en_trans
False, sp_en_trans
False, neg_sp_en_trans
True,  neg_sp_en_trans
Figure 3: Top two principal components of the datasets D+ ∪D−where D+ and D−consist of
opposite statements. Representations in D+ and D−are independently centered by subtracting off
their means; without this, there would also be an additional translational displacement between D+
and D−. Inset shows NTDs for D+ and D−. The orthogonality in the left and center plots emerges
over layers; see appendix B.
true/false statements (appendix C). Given a dataset D, call the vector pointing from the false state-
ment representations to the true ones the naive truth direction (NTD) of D.3
NTDs of different datasets often align, but sometimes do not. For instance, figure 2 displays our
datasets separating along the first PC of cities. On the other hand, in figure 3 we see a stark
failure of NTDs to align: the NTDs of cities and neg cities are approximately orthogonal,
and the NTDs of larger than and smaller than are approximately antipodal. In section 4,
these observations will be corroborated by the poor generalization of probes trained on cities and
larger than to neg cities and smaller than.
3.2
HYPOTHESES FOR EXPLAINING MISALIGNMENT OF NAIVE TRUTH DIRECTIONS
Here we articulate hypotheses which would explain both (1) the visible linear structure apparent in
each dataset individually and (2) the failure for NTDs of different datasets to align in general.
H1. LLM representations have no truth direction, but do have directions corresponding
to other features which are sometimes correlated with truth. For instance, LLaMA-
13B might have linearly-represented features representing sizes of numbers, association
between English words and their Spanish translations, and association between cities and
their countries (Hernandez et al., 2023). This would result in each dataset being linearly
separated, but NTDs only aligning when all their truth-relevant features are correlated.
H2. LLMs linearly represent the truth of various types of statements, without having a
unified truth feature. The the truth of negated statements, conjunctive statements, state-
ments about comparisons, etc., may all be treated as distinct linearly-represented features.
H3. Misalignment from correlational inconsistency (MCI): there is a truth direction as
well as other linearly-represented features which correlate with truth on narrow data
distributions; however these correlations may be inconsistent between datasets. For
instance, MCI would explain the center panel of figure 3 by positing that the negative y-
direction represents truth and the positive x-direction represents some feature which is cor-
related with truth on sp en trans and anticorrelated with truth on neg sp en trans.
H1 is at odds with the results of sections 4 and 5: for H1 to hold, there would have to be a non-truth
feature which is both correlated with truth across all of our datasets and causally mediates the way
3Of course, there are many such vectors. In section 4 we will be more specific about which such vector we
are discussing (e.g. the vector identified by training a linear probe with logistic regression). In this section, we
will leave the notion informal to facilitate discussion.
5

LLaMA-13B handles in-context true/false statements. We will also see in section 5 that directions
identified by training probes on both cities and neg cities are more causally implicated in
LLaMA-13’s processing of true/false statements. Thus, our work is overall suggestive of MCI.
4
PROBING AND GENERALIZATION EXPERIMENTS
In this section we train probes on datasets of true/false statements and test their generalization
to other datasets. But first we discuss a deficiency of logistic regression and propose a simple,
optimization-free alternative: mass-mean probing. We will see that mass-mean probes generalize
better and are more causally implicated in model outputs than other probing techniques.
4.1
CHALLENGES WITH LOGISTIC REGRESSION, AND MASS-MEAN PROBING
False
True
θt = θmm
θf
θlr
Figure 4: An illustration of a weakness of lo-
gistic regression. Truth is represented along
the black direction, while an irrelevant fea-
ture that varies independently from truth is
represented along the blue direction. Logis-
tic regression finds the magenta direction.
A common technique in interpretability research for
identifying directions representing a feature is to
train a linear probe with logistic regression Alain &
Bengio (2018) on a dataset of positive and negative
examples of the feature. In some cases, however, the
direction identified by logistic regression can fail to
reflect an intuitive best guess for the feature direc-
tion, even in the absence of confounding features.
Consider the following scenario, illustrated in fig-
ure 4 with hypothetical data:
• Truth is represented linearly along a direc-
tion θt.
• Another feature f
is represented lin-
early along a direction θf which is non-
orthogonal to θt.4
• The statements in our dataset have some
variation with respect to feature f, indepen-
dent of their truth value.
We would like to recover the direction θt, but lo-
gistic regression will fail to do so. Assuming for
simplicity linearly separable data, logistic regression
will instead converge to the maximum margin sepa-
rator Soudry et al. (2018) (the dashed magenta line
in figure 4). Intuitively, logistic regression treats the
small projection of θf onto θt as significant, and adjusts the probe direction to have less “interfer-
ence” (Elhage et al., 2022) from θf.
A simple alternative to logistic regression which will recover the desired direction in this scenario is
to take the vector pointing from the mean of the false data to the mean of the true data. In more detail
if D = {(xi, yi)} is a dataset of xi ∈Rd with binary labels yi ∈{0, 1}, we set θmm = µ+ −µ−
where µ+, µ−are the means of the positively- and negatively-labeled datapoints, respectively. A
reasonable first pass at converting θmm into a probe is to define5
pmm(x) = σ(θT
mmx)
where σ is the logistic function. However, when evaluating on data that is independent and iden-
tically distributed (IID) to D, we can do better. Letting Σ be the covariance matrix of the dataset
Dc = {xi −µ+ : yi = 1} ∪{xi −µ−: yi = 0} formed by independently centering the positive
and negative datapoints, we set
piid
mm(x) = σ(θT
mmΣ−1x).
4As suggested by the superposition hypothesis of Elhage et al. (2022), features being represented non-
orthogonally in this way may be the typical case in deep learning.
5In this work, we are interested in identifying truth directions, so we always center our data and use probes
without biases. In other settings, we would instead set pmm(x) = σ(θT
mmx + b) for a tunable bias b ∈R.
6

The effect of multiplying by Σ−1 is to tilt the decision boundary to accommodate interference from
θf; in fact, we show in appendix F that under mild assumptions, Σ−1θmm coincides on average with
the direction found by logistic regression. Thus mass-mean probing provides a way to select a good
decision boundary while – unlike logistic regression – also tracking a candidate feature direction
which may be non-orthogonal to this decision boundary. Appendix E gives another interpretation of
mass-mean probing in terms of Mahalanobis whitening.
We call the probes pmm and piid
mm mass-mean probes. As we will see, piid
mm is about as accurate
as logistic regression probes on the train set D, while pmm enjoys better generalization to other
true/false datasets and is more causally implicated in model outputs than other probing techniques.
4.2
EXPERIMENTAL SET-UP
We evaluate the following techniques for eliciting the truth or falsehood of factual statements from
LLaMA-13B.
Logistic regression, as in Alain & Bengio (2018) but with fixed bias b = 0.
Mass-mean probing. We use piid
mm when validating on held-out IID data and pmm otherwise.
Contrast-Consistent Search (CCS), introduced in Burns et al. (2023). CCS is an unsupervised
method: given contrast pairs of statements with opposite truth values, CCS identifies a direction
along which the activations of these statements are far apart. For our contrast pairs, we pair state-
ments from cities and neg cities, and from larger than and smaller than.
Logistic regression/mass-mean probing on the likely dataset. This is used to benchmark our
probes against probes trained only to classify statements as being likely/unlikely text.
Calibrated 5-shot prompting. Given a dataset D, we construct a 5-shot prompt by sampling five
statements6 and labels from D and presenting them to the model in-context. We then append the
remaining statements in D to this prompt one-at-a-time and treat the model’s predicted next token
as its classification. See appendix I for example prompts. We then calibrate predictions so that
half of the statements are labeled true/false; this improves performance by a few percentage points.
Since performance is very sensitive to the 5-shot prompt used, we report the best of five randomly-
generated 5-shot prompts.
Logistic regression on the validation set (oracle). This gives an upper-bound for the accuracy of
a linear probe on the validation set.
4.3
RESULTS
The results are shown in figure 5. We highight some key observations.
Generalization accuracy is high across all techniques. For instance, no matter the technique,
training probes only on datasets of statements about numerical comparisons results in a probes with
95%+ accuracy on Spanish-English translation. The performance of the probes relative to calibrated
5-shot accuracies suggest that model outputs are being influenced by features other than the truth.
CCS and mass-mean probing outperform logistic regression, with mass-mean probing doing
best. The average accuracies across the cities+neg cities columns are 73%, 86%, and 84%
for logistic regression, mass-mean probing, and CCS, respectively.
Probes trained on true/false datasets outperform probes trained on likely. While probes
trained on likely are clearly better than random on cities (a dataset where true statements
are significantly more probable than false ones), they generally perform poorly. This is especially
true on datasets where likelihood is negatively correlated (neg cities, neg sp en trans) or
approximately uncorrelated (larger than, smaller than) with truth. This demonstrates that
LLaMA-13B linearly encodes truth-relevant information beyond the plausibility of the text.
6The number n = 5 of shots was selected by a hyperparamter sweep on cities.
7

Figure 5: Generalization accuracy of probes trained on LLaMA-13B layer 12 residual stream ac-
tivations. The x-axis shows the train set, and the y-axis shows the test set. All probes are trained
on 80% of the data. When the train set and test set are the same, we evaluate on the held-out 20%.
Otherwise, we evaluate on the full test set.
5
CAUSAL INTERVENTION EXPERIMENTS
In this section we perform experiments which measure the extent to which the probe directions
identified in section 4 are causally implicated in model outputs.
5.1
EXPERIMENTAL SET-UP
Our goal is to cause LLaMA-13B to treat false statements introduced in context and true and vice
versa. Consider the following prompt:
The Spanish word ‘jirafa’ means ‘giraffe’. This statement is: TRUE
The Spanish word ‘escribir’ means ‘to write’. This statement is: TRUE
The Spanish word ‘diccionario’ means ‘green’. This statement is: FALSE
The Spanish word ‘gato’ means ‘cat’. This statement is: TRUE
The Spanish word ‘aire’ means ‘silver’. This statement is: FALSE
The Spanish word ‘uno’ means ‘floor’. This statement is:
We hypothesize that the truth value of the statement “The Spanish word ‘uno’ means ‘floor’.” is rep-
resented in the residual stream above two tokens: the final word (floor) and the end-of-sentence
punctuation token (’.), bolded above. Thus if θ is a candidate truth direction in the layer ℓresid-
ual stream, we intervene in the forward pass of LLaMA-13B by adding some multiple αθ, α > 0,
to the layer ℓresidual stream above these tokens. More specifically, we pass the above prompt
into LLaMA-13B to obtain layer ℓresidual stream activations x0, x1, . . . , x−1. If xk and xk+1
are the activations above these two tokens, we add αθ to xk and xk+1 while leaving all other
8

Table 2: Results of intervention experiments. The train set column indicates the datasets and probing
technique (logistic regression, mass-mean probing, or CCS) which was used to identify the truth
direction. The α column gives the scaling factor which was optimal in a sweep of α’s. Probability
differences are averaged over all statements in sp en trans.
false→true
true→false
train set
α
p(TRUE) −p(FALSE)
α
p(FALSE) −p(TRUE)
no intervention
−
−0.45
−
−0.55
cities (LR)
15
0.23
14
0.01
cities+neg cities (LR)
47
0.39
17
0.18
cities (MM)
4
0.25
6
0.77
cities+neg cities (MM)
15
0.43
9
0.79
cities+neg cities (CCS)
46
0.41
13
0.59
likely (LR)
−
−
49
0.01
likely (MM)
7
0.23
15
0.19
activations unchanged. We then allow the model to continue its forward pass as usual with the mod-
ified activations. We record the model’s probabilities p(TRUE), p(FALSE); our goal is to increase
p(TRUE) −p(FALSE). Conversely, starting from a true statement we can subtract a multiple αθ
from the corresponding token positions with the goal of decreasing p(TRUE) −p(FALSE).
We perform this intervention with ℓ= 10 and where θ is a direction extracted by one of the probes p
in section 4. We normalize θ so that p(µ−+θ) = p(µ+) where µ+, µ−are the mean representations
of the true and false statements, respectively. Thus, from the perspective of p, adding θ takes the
average false statement to the average true statement. Effective intervention strengths closer to
α = 1 therefore indicate that θ better aligns with the model’s truth direction.
For the false→true version of our experiment, we use the first five lines of the prompt above with
each false statement in sp en trans appended one-at-a-time; we report the optimal intervention
strength α and the average p(TRUE) −p(FALSE) for that intervention strength. For the true→false
version, we do the same but using only true statements in sp en trans.
5.2
RESULTS
Mass-mean probe directions are highly causal; logistic regression directions are less causal.
This is most stark when causing LLaMA-13B to believe a true statement is false: our best inter-
vention induces LLaMA-13B to swing its average prediction from TRUE with probability 77% to
FALSE with probability 89%.
Probes trained on likely have some effect, but it is small and inconsistent. For instance, in
the false→true case, intervening along the logistic regression direction of likely has the opposite
of the intended effect, so we leave it unreported. This reinforces our case that LLMs represent truth
and not only text likelihood.
Training on statements and their negations results in directions which are more causal. This
provides evidence for the MCI hypothesis of section 3.2.
Interventions in other positions are ineffective. We tested applying our interventions over the final
two tokens of other statements in the prompt. This produced no effect. Thus, our intervention cannot
work by simply adding in a “say true” direction. This also supports our hypothesis that LLaMA-13B
represents truth over the final two tokens of a factual statement.
6
DISCUSSION
6.1
LIMITATIONS AND FUTURE WORK
Our work has a number of limitations. First, we focus on simple, uncontroversial statements, and
therefore cannot disambiguate truth from closely related potential features, such as “commonly be-
lieved” or “verifiable” (Levinstein & Herrmann, 2023). Second, we only address how to identify a
9

truth direction; we found empirically that the optimal bias for linear probes was under-determiend
by many of our training sets, and so we leave the problem of identifying well-generalizing biases to
future work. Third, we studied only one model at a single scale, though we’ve checked that many of
our results seem to hold for LLaMA-7B and LLaMA-30B as well. Finally, although the evidence in
section 4 and 5 shed light on which of the hypotheses in section 3.2 is correct, uncertainty remains.
6.2
CONCLUSION
In this work we conduct a detailed investigation of the structure of LLM representations of truth.
Drawing on simple visualizations, correlational evidence, and causal evidence, we find strong rea-
son to believe that there is a “truth direction” in LLM representations. We also introduce mass-mean
probing, a simple alternative to other linear probing techniques which better identifies truth direc-
tions from true/false datasets.
ACKNOWLEDGMENTS
We thank Ziming Liu and Isaac Liao for useful suggestions regarding distinguishing true text from
likely text, and Wes Gurnee, Eric Michaud, and Peter Park for many helpful discussions throughout
this project. We thank David Bau for useful suggestions regarding the experiments in section 5.
We also thank Oam Patel, Hadas Orgad, Sohee Yang, and Karina Nguyen for their suggestions, as
well as Helena Casademunt, Max Nadeau, and Ben Edelman for giving feedback during this paper’s
preparation.
REFERENCES
Mostafa Abdou, Artur Kulmizev, Daniel Hershcovich, Stella Frank, Ellie Pavlick, and Anders
Søgaard. Can language models encode perceptual structure without grounding? a case study
in color, 2021.
Guillaume Alain and Yoshua Bengio.
Understanding intermediate layers using linear classifier
probes, 2018.
Amos Azaria and Tom Mitchell. The internal state of an llm knows when its lying, 2023.
David Bau, Jun-Yan Zhu, Hendrik Strobelt, Agata Lapedriza, Bolei Zhou, and Antonio Torralba.
Understanding the role of individual units in a deep neural network. Proceedings of the National
Academy of Sciences, 2020. ISSN 0027-8424. doi: 10.1073/pnas.1907375117. URL https:
//www.pnas.org/content/early/2020/08/31/1907375117.
Yonatan Belinkov.
Probing classifiers:
Promises, shortcomings, and advances.
Computa-
tional Linguistics, 48(1):207–219, March 2022.
doi: 10.1162/coli a 00422.
URL https:
//aclanthology.org/2022.cl-1.7.
Collin Burns, Haotian Ye, Dan Klein, and Jacob Steinhardt. Discovering latent knowledge in lan-
guage models without supervision. In The Eleventh International Conference on Learning Rep-
resentations, 2023. URL https://openreview.net/forum?id=ETKGuby0hcs.
Stephen Casper, Jason Lin, Joe Kwon, Gatlen Culp, and Dylan Hadfield-Menell. Explore, establish,
exploit: Red teaming language models from scratch, 2023.
Paul Christiano, Ajeya Cotra, and Mark Xu. Eliciting latent knowledge: How to tell if your eyes de-
ceive you, 2021. URL https://docs.google.com/document/d/1WwsnJQstPq91_
Yh-Ch2XRL8H_EpsnjrC1dwZXR37PC8/edit#heading=h.jrzi4atzacns.
Hoagy Cunningham, Aidan Ewart, Logan Riggs, Robert Huben, and Lee Sharkey. Sparse autoen-
coders find highly interpretable features in language models, 2023.
Fahim Dalvi, Nadir Durrani, Hassan Sajjad, Yonatan Belinkov, Anthony Bau, and James R. Glass.
What is one grain of sand in the desert? analyzing individual neurons in deep nlp models. In AAAI
Conference on Artificial Intelligence, 2018. URL https://api.semanticscholar.org/
CorpusID:56895415.
10

Nelson Elhage, Tristan Hume, Catherine Olsson, Nicholas Schiefer, Tom Henighan, Shauna Kravec,
Zac Hatfield-Dodds, Robert Lasenby, Dawn Drain, Carol Chen, Roger Grosse, Sam McCandlish,
Jared Kaplan, Dario Amodei, Martin Wattenberg, and Christopher Olah. Toy models of superpo-
sition. Transformer Circuits Thread, 2022.
Geonames. All cities with a population > 1000, 2023. URL https://download.geonames.
org/export/dump/.
Gabriel Goh, Nick Cammarata †, Chelsea Voss †, Shan Carter, Michael Petrov, Ludwig Schubert,
Alec Radford, and Chris Olah. Multimodal neurons in artificial neural networks. Distill, 2021.
doi: 10.23915/distill.00030. https://distill.pub/2021/multimodal-neurons.
Wes Gurnee, Neel Nanda, Matthew Pauly, Katherine Harvey, Dmitrii Troitskii, and Dimitris Bertsi-
mas. Finding neurons in a haystack: Case studies with sparse probing, 2023.
Evan Hernandez, Arnab Sen Sharma, Tal Haklay, Kevin Meng, Martin Wattenberg, Jacob Andreas,
Yonatan Belinkov, and David Bau. Linearity of relation decoding in transformer language models,
2023.
John Hewitt and Percy Liang. Designing and interpreting probes with control tasks. In Proceed-
ings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th
International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pp. 2733–
2743, Hong Kong, China, November 2019. Association for Computational Linguistics.
doi:
10.18653/v1/D19-1275. URL https://aclanthology.org/D19-1275.
B. A. Levinstein and Daniel A. Herrmann. Still no lie detector for language models: Probing empir-
ical and conceptual roadblocks, 2023.
Belinda Z. Li, Maxwell Nye, and Jacob Andreas. Implicit representations of meaning in neural
language models. In Proceedings of the 59th Annual Meeting of the Association for Computa-
tional Linguistics and the 11th International Joint Conference on Natural Language Processing
(Volume 1: Long Papers), pp. 1813–1827, Online, August 2021. Association for Computational
Linguistics.
doi: 10.18653/v1/2021.acl-long.143.
URL https://aclanthology.org/
2021.acl-long.143.
Kenneth Li, Aspen K Hopkins, David Bau, Fernanda Vi´egas, Hanspeter Pfister, and Martin Wat-
tenberg. Emergent world representations: Exploring a sequence model trained on a synthetic
task.
In The Eleventh International Conference on Learning Representations, 2023a.
URL
https://openreview.net/forum?id=DeG07_TcZvT.
Kenneth Li, Oam Patel, Fernanda Vi´egas, Hanspeter Pfister, and Martin Wattenberg. Inference-time
intervention: Eliciting truthful answers from a language model, 2023b.
Stephanie Lin, Jacob Hilton, and Owain Evans. TruthfulQA: Measuring how models mimic hu-
man falsehoods. In Proceedings of the 60th Annual Meeting of the Association for Computa-
tional Linguistics (Volume 1: Long Papers), pp. 3214–3252, Dublin, Ireland, May 2022. As-
sociation for Computational Linguistics. doi: 10.18653/v1/2022.acl-long.229. URL https:
//aclanthology.org/2022.acl-long.229.
Kevin Meng, David Bau, Alex Andonian, and Yonatan Belinkov.
Locating and editing factual
associations in GPT. Advances in Neural Information Processing Systems, 36, 2022.
OpenAI. Gpt-4 technical report, 2023.
Peter S. Park, Simon Goldstein, Aidan O’Gara, Michael Chen, and Dan Hendrycks. Ai deception:
A survey of examples, risks, and potential solutions, 2023.
Roma Patel and Ellie Pavlick. Mapping language models to grounded conceptual spaces. In Interna-
tional Conference on Learning Representations, 2022. URL https://openreview.net/
forum?id=gJcEM8sxHK.
Judea Pearl. Direct and indirect effects. In Proceedings of the Seventeenth Conference on Uncer-
tainty in Artificial Intelligence, UAI’01, pp. 411–420, San Francisco, CA, USA, 2001. Morgan
Kaufmann Publishers Inc. ISBN 1558608001.
11

Ethan Perez, Sam Ringer, Kamil˙e Lukoˇsi¯ut˙e, Karina Nguyen, Edwin Chen, Scott Heiner, Craig Pet-
tit, Catherine Olsson, Sandipan Kundu, Saurav Kadavath, Andy Jones, Anna Chen, Ben Mann,
Brian Israel, Bryan Seethor, Cameron McKinnon, Christopher Olah, Da Yan, Daniela Amodei,
Dario Amodei, Dawn Drain, Dustin Li, Eli Tran-Johnson, Guro Khundadze, Jackson Kernion,
James Landis, Jamie Kerr, Jared Mueller, Jeeyoon Hyun, Joshua Landau, Kamal Ndousse, Lan-
don Goldberg, Liane Lovitt, Martin Lucas, Michael Sellitto, Miranda Zhang, Neerav Kingsland,
Nelson Elhage, Nicholas Joseph, Noem´ı Mercado, Nova DasSarma, Oliver Rausch, Robin Lar-
son, Sam McCandlish, Scott Johnston, Shauna Kravec, Sheer El Showk, Tamera Lanham, Timo-
thy Telleen-Lawton, Tom Brown, Tom Henighan, Tristan Hume, Yuntao Bai, Zac Hatfield-Dodds,
Jack Clark, Samuel R. Bowman, Amanda Askell, Roger Grosse, Danny Hernandez, Deep Gan-
guli, Evan Hubinger, Nicholas Schiefer, and Jared Kaplan. Discovering language model behaviors
with model-written evaluations, 2022.
Fabien
Roger.
What
discovering
latent
knowledge
did
and
did
not
find,
2023.
URL
https://www.alignmentforum.org/posts/bWxNPMy5MhPnQTzKz/
what-discovering-latent-knowledge-did-and-did-not-find-4.
Hassan Sajjad, Nadir Durrani, and Fahim Dalvi. Neuron-level interpretation of deep NLP models:
A survey. Transactions of the Association for Computational Linguistics, 10:1285–1303, 2022.
doi: 10.1162/tacl a 00519. URL https://aclanthology.org/2022.tacl-1.74.
Daniel Soudry, Elad Hoffer, Mor Shpigel Nacson, Suriya Gunasekar, and Nathan Srebro. The im-
plicit bias of gradient descent on separable data. The Journal of Machine Learning Research, 19
(1):2822–2878, 2018.
Jacob Steinhardt.
Emergent deception and emergent optimization, 2023.
URL https://
bounded-regret.ghost.io/emergent-deception-optimization/.
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth´ee
Lacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Ar-
mand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation
language models, 2023.
Jesse Vig, Sebastian Gehrmann, Yonatan Belinkov, Sharon Qian, Daniel Nevo, Yaron Singer,
and Stuart Shieber.
Investigating gender bias in language models using causal mediation
analysis.
In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (eds.), Ad-
vances in Neural Information Processing Systems, volume 33, pp. 12388–12401. Curran
Associates, Inc., 2020.
URL https://proceedings.neurips.cc/paper_files/
paper/2020/file/92650b2e92217715fe312e6fa7b90d82-Paper.pdf.
Xiaozhi Wang, Kaiyue Wen, Zhengyan Zhang, Lei Hou, Zhiyuan Liu, and Juanzi Li.
Find-
ing skill neurons in pre-trained transformer-based language models.
In Proceedings of the
2022 Conference on Empirical Methods in Natural Language Processing, pp. 11132–11152,
Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguis-
tics. doi: 10.18653/v1/2022.emnlp-main.765. URL https://aclanthology.org/2022.
emnlp-main.765.
A
SCOPING OF TRUTH
In this work, we consider declarative factual statements, for example “Eighty-one is larger than
fifty-four” or “The city of Denver is in Vietnam.” We scope “truth” to mean the truth or falsehood
of these statements; for instance the examples given have truth values of true and false, respectively.
To be clear, we list here some notions of “truth” which we do not consider in this work:
• Correct question answering (considered in Li et al. (2023b) and for some of the prompts
used in Burns et al. (2023)). For example, we do not consider “What country is Paris in?
France” to have a truth value.
• Presence of deception, for example dishonest expressions of opinion (“I like that plan”).
12

Figure 6: Top two principal components of representations of datasets in the LLaMA-13B residual
stream at various layers.
• Compliance. For example, “Answer this question incorrectly: what country is Paris in?
Paris is in Egypt” is an example of compliance, even though the statement at the end of the
text is false.
Moreover, the statements under consideration in this work are all simple, unambiguous, and un-
controversial. Thus, we make no attempt to disambiguate “true statements” from the following
closely-related notions:
• Uncontroversial statements
• Statements which are widely believed
• Statements which educated people believe
On the other hand, our statements do disambiguate the notions of “true statements” and “state-
ments which are likely to appear in training data.”
For instance, given the input China is
not a country in, LLaMA-13B’s top prediction for the next token is Asia, even though this
completion is false. Similarly, LLaMA-13B judges the text “Eighty-one is larger than
eighty-two” to be more likely than “Eighty-one is larger than sixty-four”
even though the former statement is false and the latter statement is true. As shown in section 5,
probes trained only on statements of likely or unlikely text fail to accurately classify true/false state-
ments.
B
EMERGENCE OF LINEAR STRUCTURE ACROSS LAYERS
The linear structure observed in section 3 follows the following pattern: in early layers, represen-
tations are uninformative; then, in early middle layers, salient linear structure in the top few PCs
rapidly emerges, with this structure emerging later for statements with a more complicated logical
structure (e.g. conjunctions); finally, the linear separation becomes more salient and exits the top
few PCs in later layers. See figure 6. We hypothesize that this is due to LLMs hierarchically de-
veloping understanding of their input data, before focusing on features which are most relevant to
immediate next-token prediction in later layers.
13

Figure 7: Top PCs of datasets of statements and their opposites. The representations for the datasets
are independently centered by subtracting off their means; without this centering there would also
be a translational displacement between datasets of statements and their negations.
Interestingly, the misalignment between cities and neg cities and between sp en trans
and neg sp en trans also emerges over layers. This is seen in figure 7: in layer 6, representa-
tions are uninformative; then in layer 8, the NTD of cities and neg cities appear antipodal;
finally by layer 10, the NTDs have become orthogonal.
This can be interpreted in light of the MCI hypothesis. MCI would explain figure 7 as follows:
in layer 8, the top PC represents a feature which is correlated with truth on cities and anti-
correlated with truth on neg cities; in layer 10, this feature remains the top PC, while a truth
feature has emerged and is PC2. Since PC1 and PC2 have opposite correlations on cities and
neg cities, the datasets appear to be orthogonal.
C
NEARLY ALL LINEARLY-ACCESSIBLE TRUTH-RELEVANT INFORMATION IS
IN THE TOP PCS
In section 5 we saw that true and false statements linearly separate in the top PCs. We might ask
how much of this separation is captured in the top PCs and how much of it remains in the remaining
PCs. The answer is that nearly all of it is in the top PCs.
One way to quantify the amount of linearly accessible information in some subspace V is to project
our dataset D onto V to obtain a dataset
Dproj = {(projV (x), y)}(x,y)∈D
and record the validation accuracy of a linear probe trained with logistic regression on D. This is
shown in figure 8 for V being given by the top k+1 through k+d principal components (i.e., the top
d principal components, excluding the first k). As shown, once the top few principal components
are excluded, almost no remaining linearly-accessible information remains.
14

Figure 8: The solid lines show the validation accuracy of a linear probe trained with logistic re-
gression on the dataset, after projecting the representations to the top d + 1 through d + k principal
components. For comparison, we also show the accuracy of linear probes trained on random k-
dimensional projections (averaged over 50 random projections).
D
FURTHER VISUALIZATIONS
Figure 9 shows PCA visualizations of all of our datsets.
As shown, datasets some datasets
saliently vary along features other than the true. For instance, the three clusters of statements in
companies true false correspond to three different templates used in making the statements
in that dataset. To give another example, if we were to include all comparisons between integers
x ∈{1, . . . , 99} in our larger than dataset, then the top principal components would be domi-
nated by features representing the sizes of numbers in the statements.
In figure 10 we also visualize our datasets in the PCA bases for other datsets, expanding figure 2. We
see that although our datasets do visually separate somewhat in the top PCs of the likely dataset,
text liklihood does not account for all of the separation in the top PCs.
One might ask what the top PC of the larger than dataset is, given that it’s not truth. Figure 11
provides an interesting suggestion: it represents the absolute value of the difference between the two
numbers being compared.
E
MASS-MEAN PROBING IN TERMS OF MAHALANOBIS WHITENING
One way to interpret the formula piid
mm(x) = σ(θT
mmΣ−1x) for the IID version of mass-mean prob-
ing is in terms of Mahalanobis whitening. Recall that if D = {xi} is a dataset of xi ∈Rd with
covariance matrix Σ, then the Mahalanobis whitening transformation W = Σ−1/2 satisfies the
property that D′ = {Wxi} has covariance matrix given by the identity matrix, i.e. the whitened
coordinates are uncorrelated with variance 1. Thus, noting that θT
mmΣ−1x coincides with the inner
product between Wx and Wθ, we see that pmm amounts to taking the projection onto θmm after
performing the change-of-basis given by W. This is illustrated with hypothetical data in figure 12.
F
FOR GAUSSIAN DATA, IID MASS-MEAN PROBING COINCIDES WITH
LOGISTIC REGRESSION ON AVERAGE
Let θ ∈Rd and Σ be a symmetric, positive-definite d × d matrix. Suppose given access to a
distribution D of datapoints x ∈Rd with binary labels y ∈{0, 1} such that the negative datapoints
are distributed as N(−θ, Σ) and the positive datapoints are distributed as N(θ, Σ). Then the vector
15

Figure 9: Top two principal components of all of our datasets.
16

Figure 10: Visualizations of datasets in PCA bases for other datasets. All columns contain the same
data and all rows are in the same basis. See figure 2.
Figure 11: PCA visualization of larger than. The point representing “x is larger than y” is
colored according to x −y.
17

Figure 12: Mass-mean probing is equivalent to taking the projection onto θmm after applying a
whitening transformation.
identified by mass-mean probing is θmm = 2θ. The following theorem then shows that piid
mm(x) =
σ(2θT Σ−1x) is also the solution to logistic regression up to scaling.
Theorem 1. Let
θlr = arg max
ϕ:∥ϕ∥=1
−E(x,y)∼D

y log σ
 ϕT x

+ (1 −y) log
 1 −σ
 ϕT x

be the direction identified by logistic regression. Then θlr ∝Σ−1θ.
Proof. Since the change of coordinates x 7→Wx where W = Σ−1/2 (see appendix E) sends
N(±θ, Σ) to N(±Wθ, Id), we see that
WΣθlr = arg max
ϕ:∥ϕ∥=1
−E(x,y)∼D′ 
y log σ
 ϕT x

+ (1 −y) log
 1 −σ
 θT Wx

where D′ is the distribution of labeled x ∈Rd such that the positive/negative datapoints are dis-
tributed as N(±Wθ, Id). But the argmax on the right-hand side is clearly ∝Wθ, so that θlr ∝Σ−1θ
as desired.
G
DETAILS ON DATASET CREATION
Here we give example statements from our datasets, templates used for making the datasets, and
other details regarding dataset creation.
cities. We formed these statements from the template “The city of [city] is in [country]”
using a list of world cities from Geonames (2023). We filtered for cities with populations > 500, 000,
which did not share their name with any other listed city, which were located in a curated list of
widely-recognized countries, and which were not city-states. For each city, we generated one true
statement and one false statement, where the false statement was generated by sampling a false coun-
try with probability equal to the country’s frequency among the true datapoints (this was to ensure
that e.g. statements ending with “China” were not disproportionately true). Example statements:
• The city of Sevastopol is in Ukraine. (TRUE)
• The city of Baghdad is in China. (FALSE)
18

sp en trans. Beginning with a list of common Spanish words and their English translations, we
formed statements from the template “The Spanish word ‘[Spanish word]’ means ‘[English
word]’.” Half of Spanish words were given their correct labels and half were given random incor-
rect labels from English words in the dataset. The first author, a Spanish speaker, then went through
the dataset by hand and deleted examples with Spanish words that have multiple viable translations
or were otherwise ambiguous. Example statements:
• The Spanish word ‘imaginar’ means ‘to imagine’. (TRUE)
• The Spanish word ‘silla’ means ‘neighbor’. (FALSE)
larger than and smaller than. We generate these statements from the templates “x is larger
than y” and “x is smaller than y” for x, y ∈{fifty-one, fifty-two, . . . , ninety-nine}.
We exclude cases where x = y or where one of x or y is divisible by 10. We chose to limit
the range of possible values in this way for the sake of visualization: we found that LLaMA-13B
linearly represents the size of numbers, but not at a consistent scale: the internally represented dif-
ference between one and ten is considerably larger than between fifty and sixty. Thus, when
visualizing statements with numbers ranging to one, the top principal components are dominated
by features representing the sizes of numbers.
neg cities and neg sp en trans.
We form these datasets by negating statements
from cities and sp en trans according to the templates “The city of [city] is not
in [country]” and “‘The Spanish word ‘[Spanish word]’ does not mean ‘[English
word]’.”
cities cities conj and cities cities disj.
These datasets are generated from
cities according to the following templates:
• It is the case both that [statement 1] and that [statement 2].
• It is the case either that [statement 1] or that [statement 2].
We
sample
the
two
statements
independently
to
be
true
with
probability
1
√
2
for
cities cities conj and with probability 1 −
1
√
2 for cities cities disj.
These
probabilities are selected to ensure that the overall dataset is balanced between true and false
statements, but that there is no correlation between the truth of the first and second statement in the
conjunction.
likely. We generate this dataset by having LLaMA-13B produce unconditioned generations of
length up to 100 tokens, using temperature 0.9. At the final token of the generation, we either sample
the most likely token or the 100th most likely final token. We remove generations which contain
special tokens. Dataset examples:
• The 2019-2024 Outlook for Women’s and Girls’ Cut and Sew and Knit and Crochet
Sweaters in the United States This study covers the latent demand outlook for (LIKELY)
• Tags: python, django Question: How to get my django app to work with python 3.7 I
am new to django and have been trying to install it in my pc. I have installed python 3.7
together (UNLIKELY)
companies true false. This dataset was introduced by Azaria & Mitchell (2023); we obtained
it via the project repository for Levinstein & Herrmann (2023) which also used the dataset. Example
statements:
• ArcelorMittal has headquarters in Luxembourg. (TRUE)
• Exxon Mobil engages in the provision of banking and financial services. (FALSE)
common claim true false. CommonClaim was introduced in Casper et al. (2023). It consists
of various statements generated by GPT-3-davinci-002, labeled by humans as being true, false, or
neither. If human labelers disagreed on the truth of a statement, this is also recorded. We adapted
CommonClaim by selecting statements which were labeled true or false with no labeler disagree-
ment, then removing excess true statement to balance the dataset. Example statements:
19

• Tomatoes are not actually a vegetable. (TRUE)
• Contrary to popular belief, the platypuses are not venomous. (FALSE)
As these examples show, the statements can be ambiguous or of unclear truth value.
counterfact true false. Counterfact was introduced in Meng et al. (2022) and consists of
factual recall statements. We adapt Counterfact by using statements which form complete sentences
and, for each such statement, using both the true version and a false version given by one of Coun-
terfact’s suggested false modifications. We also append a period to the end. Example statements:
• Olaus Rudbeck spoke the language Swedish. (TRUE)
• The official religion of Malacca sultanate is Christianity. (FALSE)
H
DETAILS ON REPRESENTATION EXTRACTION
We chose to extract activations over the end-of-sentence punctuation tokens. For instance, the rep-
resentation of the statement “The city of Calgary is in Canada.” is extracted from
the residual stream over the “.” token, and the representation of “The Spanish word ‘muy’
means ‘temperature’.” is extracted over the “’.” token. This choice was motivated by the
following:
• Prior work showing that adding end-of-sentence punctuation can affect CCS performance
with autoregressive models (Roger, 2023).
• Preliminary observations: early in this work when examining attention patterns, we no-
ticed that end-of-sentence punctuation tokens often attend to the “not” token to a surprising
degree. We hypothesized that autoregressive models sometimes represent “metadata” about
sentences – for examples their truth values – above their terminating punctuation tokens.
• Convenience for visualizations: when extracting representations over the the final token
t of a statement (before the period token), the top principal components are more likely to
be dominated by features relating to the particular token t rather than features which relate
to the sentence as a whole. For example, the statements in cities cluster by the country
at the end of the sentance, rather than by the truth value of the sentance.
• Conceptual arguments.
Consider a statement like “Seventy-five is larger
than seventy-eight.” If we were to stop reading at the second “seventy” to-
ken (i.e. read only “Seventy-five is larger than seventy”) this statement
would appear to be true. However, reading on to the “eight” token, we see that the state-
ment is false. Generally, until we have observed the end-of-sentence punctuation token,
we still cannot be sure that the sentence is over, making it premature to make judgements
about whether the sentence is true as a whole.
I
EXAMPLE 5-SHOT PROMPTS
We show here example 5-shot prompts that were used for the results in section 4.
cities:
The city of Huangshi is in China.
TRUE
The city of Lanzhou is in China.
TRUE
The city of Suining is in China.
TRUE
The city of Sofia is in Mexico.
FALSE
The city of Ulyanovsk is in Russia.
TRUE
larger than:
Fifty-three is larger than fifty-nine.
FALSE
Ninety-four is larger than eighty-four.
TRUE
Seventy-six is larger than fifty-five.
TRUE
20

Sixty-seven is larger than fifty-two.
TRUE
Fifty-four is larger than sixty-two.
FALSE
common claim true false:
Lions only urine and defecate once every five to
fourteen days.
FALSE
Approximately one in every five people has an extra
body part, most commonly an extra finger or toe.
FALSE
Knowledge is compiled information or understanding
gained through experience or study.
TRUE
Argentina has the second-highest human development
index rating in Latin America after Chile.
TRUE
Greece is the homeland of the Olympic Games.
TRUE
21

