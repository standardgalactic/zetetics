Rethinking the Discount Factor in Reinforcement Learning:
A Decision Theoretic Approach
Silviu Pitis
University of Toronto, Vector Institute
Toronto, ON
spitis@cs.toronto.edu
Abstract
Reinforcement learning (RL) agents have traditionally been
tasked with maximizing the value function of a Markov deci-
sion process (MDP), either in continuous settings, with ﬁxed
discount factor γ < 1, or in episodic settings, with γ = 1.
While this has proven effective for speciﬁc tasks with well-
deﬁned objectives (e.g., games), it has never been established
that ﬁxed discounting is suitable for general purpose use (e.g.,
as a model of human preferences). This paper characterizes
rationality in sequential decision making using a set of seven
axioms and arrives at a form of discounting that generalizes
traditional ﬁxed discounting. In particular, our framework ad-
mits a state-action dependent “discount” factor that is not
constrained to be less than 1, so long as there is eventual long
run discounting. Although this broadens the range of possi-
ble preference structures in continuous settings, we show that
there exists a unique “optimizing MDP” with ﬁxed γ < 1
whose optimal value function matches the true utility of the
optimal policy, and we quantify the difference between value
and utility for suboptimal policies. Our work can be seen as
providing a normative justiﬁcation for (a slight generalization
of) Martha White’s RL task formalism (2017) and other re-
cent departures from the traditional RL, and is relevant to task
speciﬁcation in RL, inverse RL and preference-based RL.
1
Introduction
The ﬁeld of reinforcement learning (RL) studies agents that
learn to interact “optimally” with a partially known environ-
ment (Sutton and Barto 2018). The common environmental
model, underlying almost all analysis in RL, is the Markov
decision process (MDP), with optimal behavior deﬁned in
terms of the expected sum of future rewards, discounted at
a constant rate per time step. The motivation for this ap-
proach has historically been a practical one; e.g., to enable us
to make “precise theoretical statements” (Sutton and Barto
2018) or because rewards provide “the most succinct, ro-
bust, and transferable deﬁnition of a task” (Abbeel and Ng
2004). While this has proven effective for tasks with well-
deﬁned objectives (e.g., games), modern RL aims to tackle
increasingly general tasks, and it has never been established
that the MDP is suitable for general purpose use. Recently,
researchers have adopted more ﬂexible value functions with
Copyright c⃝2019, Association for the Advancement of Artiﬁcial
Intelligence (www.aaai.org). All rights reserved.
state, state-action, and transition dependent discount factors
(Sutton et al. 2011; Silver et al. 2017; White 2017). Prac-
tical considerations aside, which approach, if any, is most
sensible from a normative perspective?
We address this question by presenting an axiomatic
framework to characterize rationality in sequential decision
making. After a motivating example in Section 2 and a dis-
cussion of related work in Section 3, including an overview
of the relevant work in normative decision theory, we pro-
ceed in Section 4 to develop our framework. From a set of
seven axioms we derive a form of utility function that admits
a state-action dependent discount factor that can be greater
than 1, so long as there is long run discounting. Although
this broadens the range of possible preference structures
(vis-`a-vis the ﬁxed discount setting), we show that there ex-
ists a unique “optimizing MDP” with ﬁxed γ < 1 whose
optimal value function matches the utility of the optimal
policy, and we quantify the difference between value and
utility for suboptimal policies. Section 5 discusses the im-
plications of our work for task speciﬁcation in RL, inverse
RL and preference-based RL, and highlights an interesting
connection to Dayan’s successor representation (1993).
2
Motivation
To motivate our work, we show how the traditional ﬁxed γ
value function fails to model a set of preferences. Consider
an agent that is to walk in a single direction on the side of a
cliff forever (the “Cliff Example”). Along the cliff are three
parallel paths—low, middle and high—to which we assign
utilities of 100, 0 and 50, respectively (Figure 1 (left) a-c).
Let us also suppose the agent has the option to jump down
one level at a time from a higher path, but is unable to climb
back up. Thus the agent has many options. Four of them are
shown in Figure 1 (left) d-g with their assigned utilities.
At a glance, there does not appear to be anything irra-
tional about the utility assignments. But it is impossible to
represent this utility structure using a 3-state ﬁxed γ MDP
with inﬁnite time horizon. To see this, consider that for the
Bellman equation to be satisﬁed with respect to the optimal
policy, paths c-g in Figure 1 (left) imply the reward values
shown in Figure 1 (right) when one assumes γ = 0.9. This
implies that the utilities of paths a and b are -30 and -20,
respectively. Not only is this incorrect, but the order is re-
versed! This holds for all γ ∈[0, 1) (see Supplement for
arXiv:1902.02893v1  [cs.LG]  8 Feb 2019

Figure 1: The utilities in the Cliff Example (left) are not compatible with any 3-state MDP; e.g. the only MDP that satisﬁes the
Bellman equation with respect to the optimal policy when γ = 0.9 (right).
a short proof). It follows that either the utility assignments
are irrational, or the MDP structure used is inadequate. This
inconsistency motivates a closer look: what characteristics,
or axioms, should we demand of “rational” utility assign-
ments? Are they satisﬁed here, and if so, what does this
mean for our default modeling approach (the ﬁxed γ MDP)?
3
Related work
Koopmans (1960) provided the ﬁrst axiomatic development
of discounted additive utility over time. This and several
follow-up works are summarized and expanded upon by
Koopmans (1972) and Meyer (1976). The applicability of
these and other existing discounted additive utility frame-
works to general purpose RL is limited in several respects.
For instance, as remarked by Sobel (2013), most axiomatic
justiﬁcations for discounting have assumed deterministic
outcomes, and only a handful of analyses address stochastic-
ity (Meyer 1976; Epstein 1983; Sobel 2013). Naively pack-
aging deterministic outcome streams into arbitrary lotteries
(as suggested by Meyer (1976), §9.3) is difﬁcult to interpret
in case of control (see our commentary in Subsection 4.2
on the resolution of intra-trajectory uncertainty) and entirely
hypothetical since the agent never makes such choices—
compare our (slightly) less hypothetical “original position”
approach in Subsection 4.4.
Existing frameworks have also typically been formulated
with a focus on future streams of consumption or income,
which has led to assumptions that do not necessarily ap-
ply to sequential decision making. When each unit in a
stream is assumed to be scalar, this already rules out “certain
types of intertemporal complementarity” (Diamond 1965).
This complementarity is also ruled out by mutual indepen-
dence assumptions (Koopmans 1972) in frameworks admit-
ting vector-valued units (see commentary of Meyer (1976),
§9.4, and Frederick, Loewenstein, and O’Donoghue (2002),
§3.3). Even seemingly innocuous assumptions like bounded
utility over arbitrary outcome streams have important con-
sequences. For instance, Meyer (1976) and Epstein (1983)
derive similar utility representations as our Theorem 3, but
use the aforementioned assumption to conclude that the dis-
count factor is always less than or equal to 1 (their argument
is discussed in Subsection 4.7). By contrast, our framework
admits a state-action dependent “discount” factor that may
be greater than 1, so long as there is eventual long run dis-
counting—thus, speciﬁc, measure zero trajectories may have
unbounded reward but the utility of a stochastic process that
may produce such trajectories will still exist. This is illus-
trated in the Supplement.
Frederick, Loewenstein, and O’Donoghue (2002) provide
a comprehensive empirical review of the discounted additive
utility model as it pertains to human behavior and conclude
that it “has little empirical support.” While this is consistent
with our normative position, it does not invalidate discount-
ing, as humans are known to exhibit regular violations of
rationality (Tversky and Kahneman 1986). Such violations
are not surprising, but rather a necessary result of bounded
rationality (Simon 1972). Our work is in a similar vein to
Russell (2014) in that it argues that this boundedness neces-
sitates new research directions.
Several papers in economics—including Kreps (1977)
(and sequels), Jaquette (1976) (and prequels), Porteus
(1975) (and sequel)—examine sequential decision processes
that do not use an additive utility model. Of particular rel-
evance to our work are Von Neumann and Morgenstern
(1953), Kreps and Porteus (1978) and Sobel (1975), on
which our axiomatic framework is based. To the authors’
knowledge, no study, including this one, has ever provided a
direct axiomatic justiﬁcation of the MDP as a model for gen-
eral rationality. This is not so surprising given that the MDP
has historically been applied as a task-speciﬁc model. For
example, the MDP in Bellman’s classic study (1957) arose
“in connection with an equipment replacement problem”.
The MDP is the predominant model used in RL research.
It is commonly assumed that complex preference structures,
including arbitrary human preferences, can be well repre-
sented by traditional MDP value functions (Abbeel and Ng
2004; Christiano et al. 2017). To the authors’ knowledge, the
best (indeed, only) theoretical justiﬁcation for this assump-
tion is due to Ng and Russell (2000), discussed in Subsection
4.8. Some RL researchers have proposed generalizations of
the traditional value functions that include a transition de-
pendent discount factor (White 2017; Silver et al. 2017). Our
work can be understood as a normative justiﬁcation for (a
slight generalization of) these approaches. This, and further
connections to the RL literature are discussed in Section 5.

4
Theory
4.1
Preliminaries
Our basic environment is a sequential decision process
(SDP) with inﬁnite time horizon, formally deﬁned as the tu-
ple (S, A, T, T0) where S is the state space, A is the action
space, T : S × A →L(S) is the transition function map-
ping state-action pairs to lotteries (i.e., probability distribu-
tions with ﬁnite support) over next states, and T0 ∈L(S) is
the distribution from which initial state s0 is chosen. To ac-
commodate episodic (terminating) processes, one may sim-
ply introduce a terminal state that repeats ad inﬁnitum.
A trajectory is an inﬁnite sequence of states and ac-
tions, (st, at, st+1, at+1, . . . ). For each non-negative integer
t, yt ∈Yt denotes the history from time t = 0 through the
state at time t; e.g., (s0, a0, s1, a1, s2) ∈Y2. yt[i] indexes the
i-th state in yt; e.g., (s0, a0, s1, a1, s2)[2] = s2.
A (stochastic) stationary policy π : S →L(A) (or
ω when a second generic is needed) maps states to lot-
teries over actions. A non-stationary policy from time t,
Πt = (πt, πt+1 | yt+1, πt+2 | yt+2, . . . ) (or just Π, or
Ωwhen a second generic is needed) is a conditional se-
quence of stationary policies where the choice of πt may
depend on yt. Π[i] indexes Π’s i-th element (e.g., Πt[1] =
πt+1 | yt+1), Π[i:] denotes (Π[i], Π[i+1], . . . ). Π(s) is short-
hand for Π[0](s). Note that stationary policy π may be
viewed as the non-stationary policy (π, π, . . . ). The space
of all non-stationary policies is denoted Π.
A Markov decision process (MDP) is an SDP together
with a tuple (R, γ), where R : S×A →R returns a bounded
scalar reward for each transition and γ ∈[0, 1) is a discount
factor. For a given MDP, we deﬁne the value function for a
policy Π, V Π : S →R, as V Π(st) = E[P∞
t=0 γtR(st, at)].
Further, we deﬁne the Q-function for Π, QΠ : S × A →R
as Q(s, a) = V aΠ(s), where aΠ = (π, Π[0], Π[1], . . . ) is the
non-stationary policy that uses generic policy π with π(s) =
a in the ﬁrst step and follows policy Π thereafter.
4.2
Preferences over prospects
As a starting point in our characterization of rationality, we
would like to apply the machinery of expected utility theory
(Von Neumann and Morgenstern 1953) to preferences over
the set of possible futures, or “prospects”, P, and more gen-
erally, lotteries on prospects, L(P). Some care is required
in deﬁning a prospect so as to satisfy the necessary axioms.
In particular, we would like (strict) preference to be asym-
metric, meaning that between any two prospects p, q ∈P,
at most one of p ≻q or q ≻p holds, where ≻denotes strict
preference (for convenience, we also deﬁne weak preference
p ⪰q as not q ≻p, and indifference p ∼q as p ⪰q and
q ⪰p). Without additional assumptions, preferences over
trajectories or policies fail to satisfy asymmetry.
Suppose that preferences were deﬁned over bare trajecto-
ries, as in “preference-based RL” (Wirth et al. 2017). One
problem with this is that intra-trajectory uncertainty has al-
ready been resolved; e.g., to evaluate a trajectory that risked,
but did not realize, a visit to a difﬁcult region of the state
space, one must make an assumption about how the agent
would have behaved in that region. More generally, it is un-
clear whether trajectories should be compared with respect
to action quality (what could have happened) or with respect
to outcomes (what did happen).
Suppose instead that preferences were deﬁned over bare
policies. This is still problematic because preference would
depend on the current state distribution (not just T0). To see
this take an SDP with disconnected state trees S1 and S2,
where π1 ≻π2 in S1 but π2 ≻π1 in S2.
To avoid such difﬁculties, we deﬁne a prospect as a pair
(s, Π), where s ∈S and Π is an arbitrary non-stationary
policy, which represents the stochastic process that results
when the agent starts in state s and behaves according to Π
thereafter. For this deﬁnition to work we need the following
assumption, in absence of which different histories leading
up to the initial state s could result in preference reversal:
Assumption (Markov preference, MP). Preferences over
prospects are independent of time t and history yt.
One might justify MP in several ways. First, trivially, one
may restrict the scope of inquiry to time t = 0. Second, theo-
retically, it is a consequence of certain preference structures
(e.g., the structure associated with the standard optimality
criteria for MDPs). Third, practically, one can view MP as a
constraint on agent design. Typical RL agents, like the DQN
agent of Mnih et al. (2013) are restricted in this way. Fi-
nally, constructively, MP may be achieved by using an “in-
ternal” SDP that is derived from the environment SDP, as
shown in Subsection 4.3. The DRQN agent of Hausknecht
and Stone (2015), which augments the DQN agent with a
recurrent connection, implements such a construction.
As compared to trajectories, all uncertainty in prospects is
left unresolved, which makes them comparable to the “tem-
poral lotteries” of Kreps and Porteus (1978). As a result, it
is admittedly difﬁcult to express empirical preference over
prospects. Indeed, within the bounds of an SDP, an agent
only ever chooses between prospects originating in the same
state. In Subsection 4.4 we will apply a “veil of ignorance”
argument (Harsanyi 1953; Rawls 2009) to enable a general
comparison between prospects for normative purposes.
4.3
Constructive Markov preference (MP)
In this section we derive an “internal” SDP from an environ-
ment SDP, (S, A, T, T0), so that Markov preference is satis-
ﬁed with respect to the internal SDP. First, deﬁne a histori-
cal prospect to be a pair (yt, Πt) where yt ∈∪nYn and Πt
is the policy to be followed when starting in the ﬁnal state of
yt. One should have little trouble accepting asymmetry with
respect to preferences over historical prospects.
Next, deﬁne an equivalence relation on the set of all histo-
ries as follows: yi, yj ∈∪nYn are equivalent if the last states
are equal, yi[i] = yj[j], and, for all Π1 and Π2, (yi, Π1
i ) ≻
(yi, Π2
i )
⇐⇒
(yj, Π1
j) ≻(yj, Π2
j). Let S′ be the set of
equivalence classes with generic element s′ = {yt}. Note
that S′ may be uncountable even if S is ﬁnite.
It follows from our construction that preferences over
the prospects (s′, Π), where s′ ∈S′ and Π is an arbitrary
non-stationary policy, are independent of time t and history

yt. Therefore, the constructed SDP, (S′, A, T ′, T0), where
T ′(s′, a) := T(yt[t], a), satisﬁes Markov preference.
4.4
Cardinal utility over prospects
We imagine a hypothetical state from which an agent
chooses between lotteries (i.e., probability distributions with
ﬁnite support) of prospects, denoted by L(P). We might
think of this choice being made from behind a “veil of ig-
norance” (Rawls 2009). In our case, the veil is not entirely
hypothetical: the agent’s designer, from whom preferences
are derived, is faced with a similar choice problem. The de-
signer can instantiate the agent’s internal state, which encap-
sulates the agent’s subjective belief about the history, to be
anything (albeit, the external state is outside the designer’s
control). Now there may be some uncertainty about which
internal hardware states correspond to which histories, so
that the designer is, in a sense, behind a veil of ignorance.
We assume that strict preference (≻) with respect to arbi-
trary prospect lotteries ˜p, ˜q, ˜r ∈L(P) satisﬁes:
Axiom 1 (Asymmetry). If ˜p ≻˜q, then not ˜q ≻˜p.
Axiom 2 (Negative transitivity). If not ˜p ≻˜q, and not ˜q ≻˜r,
then not ˜p ≻˜r.
Axiom 3 (Independence). If α ∈(0, 1] and ˜p ≻˜q, then
α˜p + (1 −α)˜r ≻α˜q + (1 −α)˜r.
Axiom 4 (Continuity). If ˜p ≻˜q ≻˜r, then ∃α, β ∈(0, 1)
such that α˜p + (1 −α)˜r ≻˜q ≻β˜p + (1 −β)˜r.
The notation α˜p + (1 −α)˜q above represents a mixture
of prospect lotteries, which is itself a prospect lottery with a
α% chance of lottery ˜p and a (1 −α)% chance of lottery ˜q.
Note that prospects are (degenerate) prospect lotteries.
We now apply the VNM Expected Utility Theorem
(Von Neumann and Morgenstern 1953) to obtain a cardi-
nal utility function over prospects. We use a version of the
theorem from Kreps (1988) (Theorem 5.15), restated here
without proof and with minor contextual modiﬁcations:
Theorem 1 (Expected utility theorem). The binary relation
≻deﬁned on the set L(P) satisﬁes Axioms 1-4 if and only if
there exists a function U : P →R such that, ∀˜p, ˜q ∈L(P):
˜p ≻˜q ⇐⇒
X
z
˜p(z)U(z) >
X
z
˜q(z)U(z)
where the two sums in the display are over all z ∈P in the
respective supports of ˜p and ˜q. Moreover, another function
U ′ gives this representation if and only if U ′ is a positive
afﬁne transformation of U.
Applying the theorem produces the cardinal utility func-
tion U : P →R, as desired. We overload notation and deﬁne
U : L(P) →R as U(˜p) = P
z ˜p(z)U(z).
We further deﬁne U Π : S →R as U Π(s) = U((s, Π)) =
U(s, Π) for policy Π. Similarly, we overload U Π to deﬁne
U Π : S × A →R as U Π(s, a) = U(s, aΠ).
Finally, given a lottery over states, ˜s ∈L(S), we denote
the prospect lottery given by ˜s with ﬁxed Π as (˜s, Π), and
deﬁne preference ordering ≻˜s over policies induced by ˜s
according to the rule Π ≻˜s Ωif and only if U(˜s, Π) =
P
z ˜s(z)U Π(z) > P
z ˜s(z)U Ω(z) = U(˜s, Ω). We further
deﬁne the shorthand U ˜s : Π →R as U ˜s(Π) = U(˜s, Π).
4.5
Rational planning
Axioms 1-4 are classics of decision theory and have been de-
bated extensively over the years (see footnote 1 of Machina
(1989) for some initial references). Other than asymmetry,
which is natural given MP, we do not wish to argue for their
merits. Rather, it is prudent to strengthen this foundation for
the sequential context. Without strong normative assump-
tions about the structure of preferences, such as those im-
plied by the standard optimality criteria of MDPs, one could
infer little about future behavior from past experiences and
learning would be impossible; see, e.g., the “No Free Lunch”
theorem for inverse reinforcement learning (Armstrong and
Mindermann 2018). The axioms and results in this subsec-
tion provide what we argue is a minimal characterization of
rational planning. We begin with:
Axiom 5 (Irrelevance of unrealizable actions). If the
stochastic processes generated by following policies Π and
Ωfrom initial state s are identical, then the agent is indiffer-
ent between prospects (s, Π) and (s, Ω).
A consequence of this axiom is that U(s, aΠ) is well-
deﬁned, since U(s, aΠ) is constant for all ﬁrst-step policies
π with π(s) = a.
Assuming non-trivial MP, an agent will choose between
prospects of the form (s2, Π2) at time t = 2, where s2 ∼
T(s1, a1) and Π2 is any non-stationary policy. The agent has
preferences over plans for this choice at t = 1, which can be
ascertained by restricting the t = 1 choice set to the set X of
prospects of the form (s1, a1Π). From a rational agent, we
should demand that the restriction of U to X, U|X : Π →
R, represent the same preference ordering over policies as
U T (s1,a1). We thus assume:
Axiom 6 (Dynamic consistency). (s, aΠ) ≻(s, aΩ) if and
only if (T(s, a), Π) ≻(T(s, a), Ω).
Dynamic consistency is based on the similar axioms of
Sobel (1975) and Kreps and Porteus (1978), reﬂects the gen-
eral notion of dynamic consistency discussed by Machina
(1989), and might be compared to Koopmans’ classic “sta-
tionarity” axiom (1960). Note that we demand consistency
only before and after an action has been chosen, but not be-
fore and after environmental uncertainty is resolved. That is,
(T(s, a), Π) ≻(T(s, a), Ω) does not imply that for all z in
the support of T(s, a), (z, Π) ≻(z, Ω).
Finally, we adopt a mild version of “impatience”, com-
parable to Sobel’s countable transitivity axiom (1975), but
stated here in terms of utility (for clarity). Impatience can be
understood as the desire to make the ﬁnite-term outcomes
meaningful in light of an inﬁnite time horizon (Koopmans
1960). In the statement below, ΠnΩis the policy that fol-
lows Π for the ﬁrst n steps and Ωthereafter.
Axiom 7 (Horizon continuity). The sequence {U(s, ΠnΩ)}
converges with limit U(s, Π).
One might use this basic setup to prove a number of facts
about rational behavior in SDPs; e.g., Sobel (1975) uses a
similar axiomatic structure to prove a policy improvement
theorem alongside the next result. We restrict our analysis to
three immediately relevant results (but see our comment on
convergence theorems in Subsection 5.1). The ﬁrst justiﬁes

our later focus on stationary policies. An optimal policy is a
policy Π for which (s, Π) ⪰(s, Ω) for all s and Ω.
Lemma 1. If Π is an optimal policy, so too is the policy
Π1Π = (Π[0], Π[0], Π[1], Π[2], . . . ) formed by delaying Π
one step in order to act according to Π[0] for that step.
Proof. Consider any state s. For each state z in the support
of Π(s), (z, Π) ⪰(z, Π[1:]) (because Π is optimal) so that
(T(s, Π(s)), Π) ⪰(T(s, Π(s)), Π[1:]). By dynamic consis-
tency, this implies (s, Π1Π) ⪰(s, Π).
Theorem 2. If there exists an optimal policy Π, there exists
an optimal stationary policy π.
Proof. Put π = Π[0]. By repeated application of Lemma 1
we have πnΠ ⪰Π for all n > 0. It follows from horizon
continuity that π ⪰Π.
The next result is somewhat similar Lemma 4 of Kreps
and Porteus (1978), but with a recursive, afﬁne formulation.
Note that the proof does not use horizon continuity.
Theorem 3 (Bellman relation for SDPs). There exist R :
S × A →R and Γ : S × A →R+ such that for all s, a, Π,
U(s, aΠ) = R(s, a) + Γ(s, a)Es′∼T (s,a)[U(s′, Π)].
Proof. Fix s and a. Dynamic consistency ensures that
U T (s,a) = Es′∼T (s,a)[U(s′, Π)] represents the same prefer-
ences as the restriction of U to the space X of prospects of
the form (s, aΠ). Preferences are cardinal because Π may
be stochastic, so that prospects in X are prospect lotter-
ies (i.e., X ⊂L(P)), X is closed under mixtures (i.e.,
˜p, ˜q ∈X =⇒α˜p + (1 −α)˜q ∈X, ∀α ∈[0, 1]), and Ax-
ioms 1-4 apply to prospect lotteries in X. Therefore, by the
restriction of Theorem 1 to X, U|X and U T (s,a) are related
by the positive afﬁne transformation U|X = α + βU T (s,a)
for some α ∈R, β ∈R+. Deﬁne R(s, a) = α, Γ(s, a) = β.
Since s and a were arbitrary, the result follows.
The ﬁnal result of this subsection will be used to prove
the value-utility relation of Subsection 4.9 and is useful for
analyzing convergence of RL algorithms (see discussion in
Subsection 5.1). It uses two new assumptions. First, we as-
sume |S| = n is ﬁnite. This allows us to deﬁne vectors
uΠ and rπ so that their ith components equal U(si, Π) and
R(si, π(si)), respectively. Further, we deﬁne diagonal ma-
trix Γπ whose ith diagonal entry is Γ(si, π(si)) and transi-
tion matrix Tπ whose ijth entry is T(si, π(si))(sj). Second,
we assume the set {uΠ : Π ∈Π} spans Rn. This latter as-
sumption is sensible given that |{uΠ}| ≫n (one can also
hypothetically alter S or A to make it true). We have:
Theorem 4 (Generalized successor representation). If |S| =
n and span({uΠ}) = Rn, limn→∞(ΓπTπ)n = 0, so that
(I −ΓπTπ)−1 = I + (ΓT)1 + (ΓT)2 + . . . is invertible.
Proof. Using aΠ = πnΩin the vector form of Theorem 3,
and expanding the recursion n −1 steps gives:
uπnΩ= rπ + ΓπTπuπn−1Ω
= rπ(I + (ΓT)1 + · · · + (ΓT)n−1) + (ΓT)nuΩ
where the superscripts on Γπ and Tπ were dropped for con-
venience. Similarly, using aΠ = π, we have:
uπ = rπ(I + (ΓT)1 + · · · + (ΓT)n−1) + (ΓT)nuπ.
Subtracting the second from the ﬁrst gives uπnΩ−uπ =
(ΓπTπ)n(uΩ−uπ). By horizon continuity, the left side
goes to 0 as n →∞for all π and Ω. Since {uΩ} spans
Rn, so too does {uΩ−uπ}. It follows that (ΓπTπ)n →0,
for otherwise, there exists x ̸= 0 for which (ΓπTπ)nx ̸→0,
but x can be expressed as a linear combination of the span-
ning set, which leads to a contradiction. That (I −ΓπTπ) is
invertible follows from the well known matrix identity (Ke-
meny and Snell (1976) §1.11.1).
4.6
Preference structure of MDPs
The value function of an MDP induces preferences over
L(P) when treated as a utility function on P; i.e., ac-
cording to the rule ˜p ≻˜q if P
(s,Π) ˜p((s, Π))V Π(s) >
P
(s,Π) ˜q((s, Π))V Π(s). We have that:
Theorem 5. Preferences induced by the value function of
an MDP in continuous settings, with ﬁxed γ < 1, and in
episodic settings, with γ = 1, satisfy Axioms 1-7.
Proof. Axioms 1-4 follow from the necessity part of The-
orem 1. Axioms 5 and 6 are obvious, as is Axiom 7 in the
episodic case. Axiom 7 is true in the continuous case be-
cause bounded R and γ < 1 imply that the total contribution
of rewards received after n steps goes to 0 as n →∞.
Corollary. Theorem 2 applies to MDPs (this is well known;
see, e.g., Theorem 6.2.9(b) of Puterman (2014)).
If the converse of Theorem 5 were true (i.e., Axioms 1-
7 implied ﬁxed γ), this would validate the use of the MDP
as the default model for RL. Unfortunately, Theorem 3 is
the closest we can get to such a result; it is not hard to see
that the utilities in the Cliff Example of Section 2—which
are inconsistent with any ﬁxed γ—are consistent with the
axioms (this is illustrated in the Supplement).
4.7
Γ(s, a) > 1
A key feature of our Theorem 3 representation is that, but for
Axiom 7, which demands long run discounting, Γ is uncon-
strained. As noted in Section 3, this differentiates our result
from the otherwise identical recursions of Meyer (1976) and
Epstein (1983), which bound the discount factor by 1. It also
suggests a more general form of value function than the pro-
posals of White (2017), Silver et al. (2017) and Sutton et al.
(2011), which adopt a variable γ, but constrain it to [0, 1].
A natural question is whether our axioms can (and
should) be extended to provide a normative justiﬁcation
for a constrained discount factor—in particular, should we
apply the same argument as Meyer and Epstein? Let us
ﬁrst understand their result. Meyer and Epstein both con-
sider trajectories as primitive objects of preference, so that
each trajectory has a well-deﬁned utility. Further, they as-
sume that the utilities of trajectories are bounded, that
the domain of preference includes arbitrary trajectories,
and that at least one trajectory τ has non-zero utility

(which is true so long as the agent is not indifferent be-
tween all trajectories). Now suppose there exists (s, a) for
which Γ(s, a) > 1. It must be that R(s, a) = 0, else
U((s, a, s, a, . . . )) does not exist. But then the sequence of
utilities {U((s, a, τ)), U((s, a, s, a, τ)), . . . } formed by re-
peatedly prepending (s, a) to τ diverges. This contradicts the
assumptions and it follows that Γ(s, a) ≤1 for all (s, a).
We cannot apply their argument directly. Trajectories are
not primitive objects in our framework, and can only be val-
ued by successively unrolling the Theorem 3 Bellman rela-
tion and summing the “discounted” rewards along the trajec-
tory. However, there is no guarantee that this sum converges
(see Supplement for an example). But even if we were to
insist that, in addition to Axioms 1-7, all plausible trajec-
tories have bounded utilities, the Meyer-Epstein argument
would still not apply and Γ(s, a) > 1 would still be possible.
The reason is that arbitrary trajectories like (s, a, s, a, . . . )
are not necessarily plausible—if these trajectories are not in
the support of any prospect, it does not make sense to de-
mand that the agent express preference with respect to them,
even hypothetically speaking. For instance, it does not make
sense for a chess agent to evaluate the impossible sequence
of moves (e4, e4, e4, ...). For this reason, we argue that
the Meyer-Epstein approach to sequential rationality is too
restrictive, and that we should allow Γ(s, a) > 1, so long as
there is long run discounting in accordance with Axiom 7.
4.8
The optimizing MDP
Although the ﬁxed γ MDP is not expressive enough to
model all preference structures considered rational by our
framework, it may still serve as a model thereof. In this sub-
section we prove the existence of an MDP whose optimal
value function equals the optimal utility function. As a pre-
liminary step, let us restate without proof a classic result for
MDPs (e.g., Theorem 6.2.6 of Puterman (2014)):
Lemma 2 (Bellman optimality). Given an MDP, policy π is
optimal with respect to V if and only if, ∀s ∈S,
V π(s) = arg max
a∈A(R(s, a) + γE[V π(s′)]).
We also deﬁne U ∗= U π∗, V ∗= V π∗and Q∗= Qπ∗,
and recall that U is overloaded for domains S and S × A.
Theorem 6 (Existence of optimizing MDP). Given an SDP
with utility U over prospects and optimal stationary pol-
icy π∗, for all γ ∈[0, 1), there exists a unique “optimizing
MDP” that extends the SDP with discount factor γ and re-
ward function R such that π∗is optimal with respect to V ,
and has corresponding optimal V ∗= U ∗and Q∗= U ∗.
Proof. Put R(s, a) = U ∗(s, a) −γE[U ∗(s′)]. Then:
U ∗(st) = R(st, π∗(st)) + γE[U ∗(st+1)]
= E
" ∞
X
t=0
γtR(st, π∗(st))
#
= V ∗(st)
and:
U ∗(s, a) = R(s, a) + γE(V ∗(st+1)) = Q∗(s, a).
Since V ∗(s) = U ∗(s, π∗(st)) ≥U ∗(s, a) = Q∗(s, a),
∀a ∈A, it follows from Lemma 2 that π∗is optimal
with respect to V . For uniqueness, suppose that V ∗= U ∗
and Q∗= U ∗are optimal; then by deﬁnition of Q∗, we
have U ∗(s, a) = R(s, a) + γE[U ∗(s′)], so that R(s, a) =
U ∗(s, a) −γE[U ∗(s′)] as above.
A consequence of Theorem 6 is that the inverse reinforce-
ment learning problem (Ng and Russell 2000) is solvable (in
theory); i.e., there exists a reward function that can explain
any behavior satisfying Axioms 1-7 as being the solution to
an MDP. This same consequence follows from Theorem 3
of (Ng and Russell 2000), which characterizes the set of re-
ward functions under which some observed behavior is opti-
mal. Our theorem differs from that of Ng & Russell in that it
produces a unique solution for completely speciﬁed prefer-
ences, whereas the theorem of Ng & Russell produces a set
of solutions for partially speciﬁed preferences.
4.9
Relating value to utility
Although V ∗= U ∗in the optimizing MDP, the Cliff Exam-
ple tells us that, in general, V π ̸= U π. This is a potentially
serious problem because an agent may never ﬁnd the optimal
policy. Indeed, humans are the only general purpose agents
we know of, and our bounded rationality is well documented
(Simon 1972; Tversky and Kahneman 1986). It is possible
that general purpose objective preferences are so complex
that all intelligent agents—present or future; human, artiﬁ-
cial or alien—are so bounded to suboptimality.
Nevertheless, a natural hypothesis is that the closer V π is
to V ∗—i.e., the better an agent performs in its approximate
model of U—the better off it will tend to be. There is a sense
in which this is true, at least for ﬁnite |S|. Recalling the vec-
tor notation deﬁned for Theorem 4, deﬁning vπ accordingly,
noting that u∗= v∗, and setting ϵπ = Γπ −γI, we have:
Theorem 7. In the optimizing MDP (for ﬁnite |S|) :
uπ = u∗−(I −ΓπTπ)−1(I −γTπ)(v∗−vπ)
= vπ −(I −ΓπTπ)−1ϵπTπ(v∗−vπ).
Proof. The Bellman relation provides four equations:
uπ = rπ + ΓπTπuπ
(1)
ua∗= rπ + ΓπTπu∗
(2)
vπ = rπ + γTπvπ
(3)
ua∗= rπ + γTπv∗
(4)
where rπ is the vector representing R(s, π(s)). The ﬁrst
equality of the theorem follows by computing (2) - (4), iso-
lating rπ −rπ, substituting it into (1) - (3), adding v∗−v∗
(= 0) to one side, and rearranging, given that (I −ΓπTπ)
is invertible by Theorem 4.
The second equality follows by expanding (I−ΓT)−1 =
I + ΓT + (ΓT)2 + · · · = I + (I −ΓT)−1ΓT, and writing:
uπ = u∗−(I + (I −ΓT)−1ΓT)(I −γTπ)(v∗−vπ)
= vπ −[(I −ΓT)−1Γ −γ(I + (I −ΓT)−1ΓT)]T(v∗−vπ).
= vπ −[(I −ΓT)−1Γ −(I −ΓT)−1γI]T(v∗−vπ).
= vπ −(I −ΓπTπ)−1ϵπTπ(v∗−vπ)
where we again use the identity I + (I −ΓT)−1ΓT = (I −
ΓT)−1 in the third line.

It is worth examining the factors of the product (I −
ΓπTπ)−1ϵπTπ(v∗−vπ). The ﬁnal factor (v∗−vπ) has
non-negative entries and tells us that the difference between
uπ and vπ is a linear function of the agent’s regret in the
optimizing MDP, which supports our hypothesis that better
approximated performance is correlated with better objec-
tive performance. The factors (I −ΓπTπ)−1 and Tπ also
have all non-negative entries (to see this, write the former
as an inﬁnite sum). Thus, the sign of the error depends on
the entries of ϵπ = Γπ −γI. Unfortunately, there is no way
to guarantee that ϵπ has all negative entries, which would
guarantee that value estimates in optimizing MDP are pes-
simistic, since Γ(s, a) may be greater than 1 at some (s, a).
It is emphasized that Theorem 7 does not address the pref-
erence reversal problem observed in the Cliff Example. Even
though u∗−uπ is related to v∗−vπ by linear transfor-
mation, the relationship is not monotonic (entry-wise, or in
norm). Preferences over suboptimal prospects implied by the
optimizing MDP’s value function may very well be reversed.
5
Discussion
5.1
General reinforcement learning (GRL)
Our analysis suggests that, from a normative perspective, the
MDP may not be sufﬁciently expressive. Two approaches for
representing non-MDP preference structures come to mind.
First, one can adopt the Theorem 3 representation and use
an “MDP+Γ” model of the environment that deﬁnes both an
external reward signal R and an external anticipation func-
tion Γ. This is the approach of White’s RL task formalism
(2017), which, motivated by practical considerations, pro-
poses to specify tasks using a transition-based discount fac-
tor. This is also the approach taken by Silver et al. (2017).
The theoretical analysis of White (2017) covers convergence
of standard algorithms for the case of Γ ≤1. Although we
defer a rigorous analysis of convergence in our more gen-
eral case, we note that Theorem 4, which guarantees that
ΓπTπ (corresponding to White’s Pπ,γ) has maximum eigen-
value less than 1, makes our setting easier to analyse (we get
White’s Lemma 3 for free).
Second, one might avoid the use of a single, monolithic
MDP to model global preferences, and instead think about
ways of coordinating many specialized MDPs. One way to
do so is hierarchically, as in hierarchical RL. The idea is that
it is, or at least should be, easier to express accurate prefer-
ence at the level of goals (i.e., without incurring preference
reversal between suboptimal goals) than at the level of ﬁne-
grained prospects. So given a set of goals G, an agent can
decompose its preferences into two stages: ﬁrst pick g ∈G,
and then optimize a g-specialized MDP, Mg, to pick ﬁne-
grained actions. Kulkarni et al. (2016) provides an example
of how this might be done. Note that all Mgs share world
dynamics, so that only R (and optionally γ) change with g.
As these approaches allow an agent to represent more
general preference structures than the standard RL frame-
work, one might term them general reinforcement learning
(GRL). This is a bit of a misnomer, however, as the RL prob-
lem, broadly framed, includes GRL.
5.2
Inverse and preference-based RL
Our work leads to a generalization of the inverse reinforce-
ment learning (IRL) problem. Rather than asking, “given the
observed behavior, what reward signal, if any, is being op-
timized?” (Russell 1998), our work suggests asking: given
the observed behavior, what utility function (parameterized
by R and Γ), if any, is being optimized? Future work should
explore whether this produces empirical improvements.
Preference-based RL (PBRL) (Wirth et al. 2017) side
steps reward function design by learning directly from hu-
man preferences. Although a wide variety of algorithms fall
under this general umbrella, PBRL “still lacks a coherent
framework” (Wirth et al. 2017). In particular, the object of
preference has varied widely between different algorithms.
For instance, studies in PBRL have seen preferences ex-
pressed over actions given a state (Grifﬁth et al. 2013), over
entire policies (Akrour, Schoenauer, and Sebag 2011), over
complete trajectories (Wilson, Fern, and Tadepalli 2012),
and over partial trajectories (Christiano et al. 2017). Yet,
per the discussion in Subsection 4.2, none of these objects
satisfy Axiom 1 (asymmetry) without additional assump-
tions, which are not always explicitly stated or analyzed. Our
work is relevant to PBRL in that it proposes a basic object—
the prospect (s, Π)—over which (objective) preferences are
asymmetric (given MP). Our axiomatic framework falls well
short of the coherent foundation sought by Wirth, however,
as there is no obvious way in which preferences over abstract
prospects can be empirically expressed.
5.3
Generalized successor representation
An interesting connection between the MDP and the MDP-
Γ is that, in both, a policy-speciﬁc value function can be
decomposed into the product of “discounted expected future
state occupancies” and rewards. In the ﬁnite case, the former
factor is represented by the matrix S = (I −ΓT)−1 (see
Theorem 4), so that v = Sr. When Γ = γ, S is the well
known successor representation (SR) (Dayan 1993).
What makes the SR interesting here is that it seems to
solve some of the problems posed by the abstract anticipa-
tion function Γ. First, Γ is sensitive to the discretization of
time (for the same reason annual interest rates are larger than
monthly ones). Second, small changes in the average Γ can
result in large changes in value (increasing γ from 0.990 to
0.991 increases the value of a constant positive perpetuity by
over 10%). By contrast, the entries of S—despite its opaque
formula—provide an interpretable and time-scale invariant
measure of causality (Pitis 2018). Changes in S impact v in
proportion to r, and there is even evidence to suggest that the
humans utilize the SR to cache multi-step predictions (Mo-
mennejad et al. 2017). For these reasons, it may be easier
and more effective to elicit and use values of S, representing
long run accumulations of the Γ function, than individual
values of Γ, whether for GRL, IRL or PBRL.
6
Conclusion
We have provided normative justiﬁcation for a departure
from the traditional MDP setting that allows for a state-
action dependent discount factor that can be greater than 1.

Future work should empirically test whether our proposal
has practical merit and conﬁrm that classic convergence re-
sults extend to our more general setting.
References
Abbeel, P., and Ng, A. Y. 2004. Apprenticeship learning via inverse
reinforcement learning. In The Twenty-ﬁrst International Confer-
ence on Machine learning, 1. ACM.
Akrour, R.; Schoenauer, M.; and Sebag, M. 2011. Preference-based
policy learning. In Joint European Conference on Machine Learn-
ing and Knowledge Discovery in Databases, 12–27. Springer.
Armstrong, S., and Mindermann, S. 2018. Occams razor is insuf-
ﬁcient to infer the preferences of irrational agents. In Advances in
Neural Information Processing Systems.
Bellman, R.
1957.
A markovian decision process.
Journal of
Mathematics and Mechanics 679–684.
Christiano, P. F.; Leike, J.; Brown, T.; Martic, M.; Legg, S.; and
Amodei, D. 2017. Deep reinforcement learning from human pref-
erences. In Advances in Neural Information Processing Systems.
Dayan, P.
1993.
Improving generalization for temporal differ-
ence learning: The successor representation. Neural Computation
5(4):613–624.
Diamond, P. A. 1965. The evaluation of inﬁnite utility streams.
Econometrica: Journal of the Econometric Society 170–177.
Epstein, L. G. 1983. Stationary cardinal utility and optimal growth
under uncertainty. Journal of Economic Theory 31(1):133–152.
Frederick, S.; Loewenstein, G.; and O’Donoghue, T. 2002. Time
discounting and time preference: A critical review. Journal of eco-
nomic literature 40(2):351–401.
Grifﬁth, S.; Subramanian, K.; Scholz, J.; Isbell, C. L.; and Thomaz,
A. L. 2013. Policy shaping: Integrating human feedback with rein-
forcement learning. In Advances in neural information processing
systems, 2625–2633.
Harsanyi, J. C. 1953. Cardinal utility in welfare economics and in
the theory of risk-taking. Journal of Political Economy.
Hausknecht, M., and Stone, P. 2015. Deep recurrent q-learning for
partially observable MDPs.
Jaquette, S. C. 1976. A utility criterion for markov decision pro-
cesses. Management Science 23(1):43–49.
Kemeny, J. G., and Snell, J. L.
1976.
Finite markov chains.
Springer-Verlag, second edition.
Koopmans, T. C. 1960. Stationary ordinal utility and impatience.
Econometrica: Journal of the Econometric Society 287–309.
Koopmans, T. C. 1972. Representation of preference orderings
over time. Decision and organization 1(1).
Kreps, D. M., and Porteus, E. L. 1978. Temporal resolution of
uncertainty and dynamic choice theory. Econometrica: journal of
the Econometric Society 185–200.
Kreps, D. M. 1977. Decision problems with expected utility crite-
ria, I: upper and lower convergent utility. Mathematics of Opera-
tions Research 2(1):45–53.
Kreps, D. 1988. Notes on the Theory of Choice. Westview press.
Kulkarni, T. D.; Narasimhan, K.; Saeedi, A.; and Tenenbaum, J.
2016. Hierarchical deep reinforcement learning: Integrating tem-
poral abstraction and intrinsic motivation. In Advances in neural
information processing systems, 3675–3683.
Machina, M. J. 1989. Dynamic consistency and non-expected util-
ity models of choice under uncertainty. Journal of Economic Lit-
erature 27(4):1622–1668.
Meyer, R. F. 1976. Preferences over time. Decisions with multiple
objectives 473–89.
Mnih, V.; Kavukcuoglu, K.; Silver, D.; Graves, A.; Antonoglou, I.;
Wierstra, D.; and Riedmiller, M. 2013. Playing atari with deep
reinforcement learning. In NIPS Deep Learning Workshop.
Momennejad, I.; Russek, E. M.; Cheong, J. H.; Botvinick, M. M.;
Daw, N.; and Gershman, S. J. 2017. The successor representa-
tion in human reinforcement learning. Nature Human Behaviour
1(9):680.
Ng, A. Y., and Russell, S. J. 2000. Algorithms for inverse rein-
forcement learning. In The Seventeenth International Conference
on Machine Learning, 663–670.
Pitis, S. 2018. Source traces for temporal difference learning. In
The Thirty-Second AAAI Conference on Artiﬁcial Intelligence.
Porteus, E. L.
1975.
On the optimality of structured poli-
cies in countable stage decision processes. Management Science
22(2):148–157.
Puterman, M. L.
2014.
Markov decision processes: discrete
stochastic dynamic programming. John Wiley & Sons.
Rawls, J. 2009. A theory of justice: Revised edition. Harvard
university press.
Russell, S. 1998. Learning agents for uncertain environments. In
Proceedings of the eleventh annual conference on Computational
learning theory, 101–103. ACM.
Russell, S.
2014.
Rationality and intelligence: A brief update.
Vincent C. Mller (ed.), Fundamental Issues of Artiﬁcial Intelligence
(Synthese Library).
Silver, D.; van Hasselt, H.; Hessel, M.; Schaul, T.; Guez, A.;
Harley, T.; Dulac-Arnold, G.; Reichert, D. P.; Rabinowitz, N. C.;
Barreto, A.; and Degris, T. 2017. The predictron: End-to-end learn-
ing and planning. In The Thirty-fourth International Conference on
Machine Learning.
Simon, H. A. 1972. Theories of bounded rationality. Decision and
organization 1(1):161–176.
Sobel, M. J. 1975. Ordinal dynamic programming. Management
science 21(9):967–975.
Sobel, M. J. 2013. Discounting axioms imply risk neutrality. An-
nals of Operations Research 208(1):417–432.
Sutton, R. S., and Barto, A. G. 2018. Reinforcement Learning: An
Introduction (in preparation). MIT Press.
Sutton, R. S.; Modayil, J.; Delp, M.; Degris, T.; Pilarski, P. M.;
White, A.; and Precup, D. 2011. Horde: A scalable real-time ar-
chitecture for learning knowledge from unsupervised sensorimotor
interaction. In The 10th International Conference on Autonomous
Agents and Multiagent Systems-Volume 2, 761–768. International
Foundation for Autonomous Agents and Multiagent Systems.
Tversky, A., and Kahneman, D. 1986. Rational choice and the
framing of decisions. Journal of business S251–S278.
Von Neumann, J., and Morgenstern, O. 1953. Theory of games and
economic behavior. Princeton university press.
White, M.
2017.
Unifying task speciﬁcation in reinforcement
learning. In The Thirty-fourth International Conference on Ma-
chine Learning.
Wilson, A.; Fern, A.; and Tadepalli, P. 2012. A bayesian approach
for policy learning from trajectory preference queries. In Advances
in neural information processing systems, 1133–1141.
Wirth, C.; Akrour, R.; Neumann, G.; and F¨urnkranz, J. 2017. A
survey of preference-based reinforcement learning methods. Jour-
nal of Machine Learning Research 18(136):1–46.

Figure 2: Extension of Figure 1 to illustrate more paths.
Supplement
In this supplement we use the Cliff Example to illustrate
concretely three claims made in the paper. The path labels
in Figure 2 and the state and state-action labels in Figure 3
are referenced throughout.
Preference reversal occurs for all γ ∈[0, 1)
This claim is made in Section 2. Given the values of paths d,
e, f and g in Figure 2, we have:
V (g) = R(MM) + γV (e), so that R(MM) = 70 −γ80,
and
V (f) = R(HH) + γV (d), so that R(HH) = 60 −γ70.
It follows that R(MM) > R(HH) for all γ ∈[0, 1) since
R(MM) −R(HH) = 10 −γ10 is positive.
The Cliff Example satisﬁes Axioms 1-7
This claim is made in Subsection 4.6. The following R
and Γ provide one consistent Theorem 3 representation (not
unique) and imply the following utilities for paths h-k:
(s, a)
R(s, a)
Γ(s, a)
LL
10
0.9
ML
-10
0.9
MM
0
0.875
HM
-2
0.9
HH
25
0.5
path
U(path)
h
55
i
61.25
j
52.5
k
53.59375
If other paths (not shown) are given utilities consistent
with Theorem 3, and we assume the utility of all lotteries
(none shown) are computed as expected values, Axioms 1-
4 and dynamic consistency hold (by the necessity part of
Theorem 1, and by the fact that positive afﬁne transforma-
tions represent the same preferences, respectively). Axiom 5
is obviously satisﬁed. Finally, notice that each step that path
d is delayed (paths f, h, j...) brings the utility closer to path
a. Similarly, delaying path e (paths g, i, k...) brings the util-
ity closer to that of path b. This is true in general because
Γ < 1, so that the contribution of future rewards to utility
goes to zero and horizon continuity is satisﬁed.
Figure 3: SDP representation of Cliff Example.
Γ(s, a) > 1 is consistent with Axioms 1-7
This claim is made in Section 3 and Subsection 4.7 as one
of the factors differentiating our framework from those of
Meyer (1976) and Epstein (1983), in addition to that of
White (2017). To see that this is possible, suppose that ac-
tion HH were stochastic rather than deterministic: although
the agent attempts to stay on the high path, half of the time
the agent trips and slides down to the middle path. Us-
ing the R and Γ shown in the table to the left, but setting
Γ(HH) = 1.2, results in an MDP-Γ that has bounded utility
over prospects and satisﬁes Axioms 1-7. Even though the
“discounted” reward along path a is unbounded, the agent
almost surely cannot stay on path a, and the expectation of
any policy that tries to stay on path a exists. In particular, an
agent that tries to stay on path a, and resorts to the policy
that follows path e if it trips (call this prospect X), has util-
ity U(X) = 25 + 1.2(0.5 × U(X) + 0.5 × 80), so that
U(X) = 182.5, which is ﬁnite. Since the utilities of all
prospects exist, the fact that this revised MDP-Γ is consis-
tent with axioms 1-7 follows by an argument similar to that
used for the original Cliff Example.

