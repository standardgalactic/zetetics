Two Heads are Better than One:
Towards Better Adversarial Robustness by Combining
Transduction and Rejection
Nils Palumbo âˆ—1, Yang Guo âˆ—1, Xi Wu 2, Jiefeng Chen 1, Yingyu Liang 1, Somesh Jha 1
1 University of Wisconsin-Madison, 2 Google
npalumbo@wisc.edu, yguo@cs.wisc.edu, wu.andrew.xi@gmail.com,
jchen662@wisc.edu, yliang@cs.wisc.edu, jha@cs.wisc.edu
Abstract
Both transduction and rejection have emerged as important techniques for defending
against adversarial perturbations. A recent work by TramÃ¨r [Tra22] showed that, in
the rejection-only case (no transduction), a strong rejection-solution can be turned
into a strong (but computationally inefficient) non-rejection solution. This detector-
to-classifier reduction has been mostly applied to give evidence that certain claims
of strong selective-model solutions are susceptible, leaving the benefits of rejection
unclear. On the other hand, a recent work by Goldwasser et al. [GKKM20] showed
that rejection combined with transduction can give provable guarantees (for certain
problems) that cannot be achieved otherwise. Nevertheless, under recent strong
adversarial attacks (GMSA [CWG+22], which has been shown to be much more
effective than AutoAttack against transduction), Goldwasser et al.â€™s work was
shown to have low performance in a practical deep-learning setting. In this paper,
we take a step towards realizing the promise of transduction+rejection in more
realistic scenarios. Theoretically, we show that a novel application of TramÃ¨râ€™s
classifier-to-detector technique in the transductive setting can give significantly
improved sample-complexity for robust generalization. While our theoretical
construction is computationally inefficient, it guides us to identify an efficient
transductive algorithm to learn a selective model. Extensive experiments using
state of the art attacks (AutoAttack, GMSA) show that our solutions provide
significantly better robust accuracy.
1
Introduction
A recent line of research [GKKM20, MHS21, Goo19, WJS+21, WYW20a] has investigated aug-
menting models with transduction or rejection to defend against adversarial perturbations. However,
the results of leveraging these new options have been mixed. For example, a recent work by
TramÃ¨r [Tra22] gives an equivalence between classification-only and classification-with-rejection; the
major application of the authorâ€™s results has been to provide bounds on the performance of defenses
with rejection, which can be used to show that the robustness of defenses with rejection may be lower
than the authors originally claimed, casting doubt on the benefits of rejection.
On the other hand, some recent work in theory has demonstrated that transduction, that is leveraging
the unlabeled test-time input for learning the model, may have significant impact on defending
against adversarial robustness. Specifically, Montasser et al. [MHS21] studied the setting of trans-
duction (without rejection), and show that robust learning with transduction allows for significant
âˆ—Equal contribution.
Preprint. Under review.
arXiv:2305.17528v1  [cs.LG]  27 May 2023

improvemnents in sample complexity, reducing dependency on VC dimension from exponential to
linear; however, this comes at the cost of significantly greater assumptions on the data (OPTU2 for the
realizable case rather than the OPTU of the inductive setting 2). Goldwasser et al. [GKKM20] studied
transduction and rejection, and show even more surprising results, not achievable with transduction
or rejection alone. However, one prominent limitation of these works seems to be that none has yet
resulted in practical robust learning mechanisms in the deep learning setting typically considered.
Realizable
Agnostic Generalization Bound
Condition
Generalization Bound
Induction [MHS19]
OPTU = 0
O

2VC(H) log(n)+log(1/Î´)
n

OPTU + O
 q
2VC(H)+log(1/Î´)
n
!
Transduction [MHS21]
OPTU2 = 0
O
 VC(H) log(n)+log(1/Î´)
n

2OPTU2 + O
 q
VC(H)+log(1/Î´)
n
!
Rejection (Theorem 4.2)
OPTrej
U = 0
O

2VC(T(H)) log(n)+log(1/Î´)
n

OPTrej
U + O
 q
2VC(T(H))+log(1/Î´)
n
!
Transduction + Rejection [GKKM20]
OPTU = 0
O
 q
VC(H) log(n)
n
+ log(1/Î´)
n
!
2 OPTU +2 âˆš2 OPTI + O
 q
VC(H) log n+log(1/Î´)
n
!
Transduction + Rejection (ours) (Theorem 4.1)
OPTU2/3 = 0
O
 VC(H) log(n)+log(1/Î´)
n

2OPTU2/3 + O
 q
VC(H)+log(1/Î´)
n
!
Table 1: Summary of generalization bounds for the four settings. Compared to transduction alone
and [GKKM20], our defense weakens the necessary conditions in the realizable case and improves the asymp-
totic error in the agnostic case. Compared to induction and rejection alone, sample complexity has a linear rather
than exponential dependence on the VC dimension. Compared to [GKKM20], the dependence on the error
bound Ïµ improves from inverse quadratic to inverse linear in the realizable case.
Specifically, compared to Goldwasser et al., which considered arbitrary perturbations, we focus on the
classic and practical scenairo of bounded perturbations for deep learning. Somewhat surprisingly, we
show that a novel application of TramÃ¨râ€™s classifier-to-detector technique in the transductive setting
can give significantly improved sample-complexity for robust generalization, noting that bounded
perturbations are critical for the construction to work. To obtain these improvements, we do not
require stronger assumptions on the data, as with [MHS21]; in the realizable case, we only need
to assume OPTU2/3 = 0, which is even better than the OPTU = 0 assumption in the inductive case.
Table 1 gives more details; the notation is described in Section 3. Our results give a first constructive
application of TramÃ¨râ€™s classifier-to-detector reduction which leads to improved sample complexity.
While our theoretical construction is computationally inefficient due to the use of TramÃ¨râ€™s reduction,
it guides us to identify a practical transductive algorithm for learning a robust selective model. In
addition, we present an objective for general adaptive attacks targeting selective classifiers based
on our algorithm. Our transductive defense algorithm gives strong empirical performance on image
classification tasks, both against our adaptive attack and against existing state-of-the-art attacks such
as AutoAttack and standard GMSA. On CIFAR-10, we obtain 73.9% transductive robust accuracy
with rejection, a significant improvement on the current state-of-the-art result of 66.6% [CAS+20] for
robust accuracy up to the perturbation considered (lâˆžwith budget Ïµ = 8/255).
The rest of the paper is organized as follows. Section 2 reviews main related work, and Section 3
presents some necessary background. We develop our theory results in Section 4. Guided by our
theory, Section 5 develops a practical robust learning algorithm, leveraging both transduction and
rejection. We provide systematic experiments in Section 6, and conclude in Section 7.
2
Related Work
In recent years, there have been extensive studies on adversarial robustness in the traditional in-
ductive learning setting, where the model is fixed during the evaluation phase [CW17, GSS14,
MDFF16]. Most popular and effective methods are adversarial training, such as PGD [MMS+17],
TRADES [ZYJ+19]. These methods are effective against adversaries on small dataset like MNIST, but
still ineffective on complex dataset like CIFAR-10 or ImageNet [CAS+20]. Defenses beyond adver-
sarial training have been proposed but most are broken by strong adaptive attacks [CH20, TCBM20].
To break this robust bottleneck, recent work has proposed alternative settings with relaxed yet realistic
assumptions, particularly by allowing rejection and transduction. In robust learning with rejection
2The optimal robust risk is OPTU = infhâˆˆH Pr(x,y)âˆ¼D
âˆƒz âˆˆU(x) : h(z) , y.
2

(a.k.a., abstain), we allow rejection of adversarial examples instead of correctly classifying all of
them [Tra22]. Variants of adversarial training with rejection option have been considered [LF19,
PZH+22, CRC+21, KCF20, SDM+20, HYC+22], also different generalizations such as [SHS20]
(unseen attacks), [SLK20, BSRK22, SLM+] (certified robustness). [Tra22] proves an equivalence
between robust learning with rejection and standard robust learning in the indcutive setting and shows
that the evaluation of past defenses with rejection was unreliable.
The other approach is to define an alternative notion of adversarial robustness via transductive
learning, i.e. "dynamically" ensuring robustness on the particular given test samples rather than on
the whole distribution. Similar settings have been studied but under the view of "test-time defense"
or "dynamic defense" [Goo19, WJS+21, WYW20a]. [GKKM20] is the first paper to formalize
transductive learning for robust learning, and the first to consider transduction+rejection. It considers
general adversaries on test data and presents novel theoretical guarantees. [CWG+22] formally
defines the notion of transductive robustness as a maximin problem and presents a principled adaptive
attack, GMSA. [MHS21] discusses robust transductive learning against bounded perturbation from a
learning theory perspective and obtains corresponding sample complexity.
3
Preliminaries
Robust Error
Robust Error (with Rejection)
Inductive
errU(h; x, y) := supzâˆˆU(x) 1{h(z) , y}
errrej
U (h; x, y) := supzâˆˆU(x) 1{h(z) < {y, âŠ¥} âˆ¨h(x) , y}
Transductive
err(h; x, y, Ëœx, Ëœz, Ëœy) := 1
m
Pm
i=1 1 {h (Ëœzi) , Ëœyi}
errrej(h; x, y, Ëœx, Ëœz, Ëœy) := 1
m
Pm
i=1 1
(
(h (Ëœzi) < { Ëœyi} âˆ§Ëœzi = Ëœxi)
âˆ¨(h (Ëœzi) < { Ëœyi, âŠ¥} âˆ§Ëœzi , Ëœxi)
)
Table 2: Summary of the robust error in all settings. Note that transductive error of the learner A is
the corresponding notion of error where h = A(x, y, Ëœz).
Let X denote the input space, Y the label space, D the clean data distribution over X Ã— Y. We
will assume binary classification for our theoretical analysis: Y = {Â±1}. Let U(x) denote the set
of possible perturbations of an input x, e.g., for â„“p norm perturbation of budget Ïµ, U is the â„“p ball
of radius Ïµ: U(x) = {z : âˆ¥z âˆ’xâˆ¥p â‰¤Ïµ}. We assume U satisfies âˆ€x âˆˆX, x âˆˆU(x); essentially
all interesting perturbations satisfy this. Let U2(x) := {z : âˆƒt âˆˆU(x), such that z âˆˆU(t)}, and
Uâˆ’1(x) := {z : x âˆˆU(z)}. If a perturbation set Î› satisfies Î›2 = U, then we say Î› = U1/2. When U
is the â„“p ball of radius Ïµ, U2 is that of radius 2Ïµ, Uâˆ’1 = U, and U1/2 is that of radius Ïµ/2; we define
U3 and U1/3 similarly.
All learners are provided with n i.i.d. training samples 3 (x, y) = (xi, yi)n
i=1 âˆ¼Dn. There are m i.i.d.
test samples (Ëœx, Ëœy) âˆ¼Dm, and the adversary can perturb Ëœx to Ëœz âˆˆU(Ëœx). We describe the main settings
below; the corresponding notions of error are in Table 2. For each setting, we define risk as the
expected worst-case error up to the perturbation U, and empirical risk similarly.
Induction.
In the traditional robust classification setting (e.g., [MMS+19]; also called the inductive
setting or simply induction), the learning algorithm (the defender) is given training set (x, y), learns a
classifier h : X 7â†’Y from some hypothesis class H.
Rejection.
In the setting of robust classification with rejection, the classifier has the extra power of
abstaining (i.e., outputting a rejection option denoted by âŠ¥), and furthermore, rejecting a perturbed
input does not incur an error. The learning algorithm is given training set (x, y) and learns a selective
classifier h : X 7â†’Y âˆª{âŠ¥} from some hypothesis class H. An error occurs only when h rejects a
clean input, or accepts and misclassifies. We define additionally OPTrej
U := infhâˆˆH Rrej
U (h; D).
Transduction.
In the setting of robust classification with transduction (e.g., [MHS21]), the learning
algorithm (the transductive learner) has access to the unlabeled test input data; the goal is to predict
labels only for these given test inputs (a transductive learner need not generalize). The learner A
is given the training data (x, y) and the (potentially perturbed) test inputs Ëœz, and outputs m labels
3Here x = (xi)n
i=1 and similarly with y, Ëœx, Ëœy, etc. We will also overload the notation U, e.g., U(x) := {u âˆˆ
Xn : ui âˆˆU(xi)}.
3

h(Ëœz) = (h(Ëœzi))m
i=1 as predictions for Ëœz. That is, the learner is a mapping A : (X Ã— Y)n Ã— Xm 7â†’Ym. A
special case is when A learns a classifier h and use it to label Ëœz; the labels are also denoted as h(Ëœz).
Our setting: Transduction+Rejection.
A transductive learner for selective classifiers A is given
(x, y, Ëœz), and outputs rejection or a label for each input in Ëœz. That is, the learner is a mapping
A : (X Ã— Y)n Ã— Xm 7â†’(Y âˆª{âŠ¥})m. An error occurs when it rejects a clean test input or accepts and
misclassifies.
4
Theoretical Analysis
In this section, we present the theorem statements and proof sketches for the realizable case in two
settings: transduction+rejection, and rejection only. The proof details and the agnostic case results
are in Appendix A.
4.1
Transduction + Rejection: Realizable Case
We first present the result for our main focus, the setting with both transduction and rejection.
For comparison with existing results in the inductive-only and transduction-only settings [MHS19,
MHS21], we follow their setup: assume there exists a classifier (without rejection) with 0 robust
error from a hypothesis class H of VC-dimension VC(H), and the learner constructs a selective
classifier for labeling the test inputs (or constructs a set of selective classifiers and uses any of them
for labeling). The goal is to design a learner with a small robust error.
Theorem 4.1. For any n âˆˆN, Î´ > 0, hypothesis class H of classifiers without rejection, perturbation
set U such that U = Uâˆ’1 and U1/3 exists, and distribution D over X Ã— Y satisfying OPTU2/3 = 0,
there exists a transductive learner A that constructs a set of selective classifiers âˆ†s.t. the following is
true: with probability â‰¥1 âˆ’Î´ over (x, y) âˆ¼Dn and (Ëœx, Ëœy) âˆ¼Dn, if âˆ†, âˆ…, then for any h âˆˆâˆ†,
errrej
U (h; x, y, Ëœx, Ëœy) â‰¤VC(H) log(2n) + log(1/Î´)
n
.
For U satisfying our conditions (including lp balls), we obtain a stronger guarantee than those
using only transduction or only rejection. First, compared to the guarantee for transduction without
rejection [MHS21] (see Table 1), our result requires weaker assumptions on the data: we need
OPTU2/3 = 0 rather than OPTU2 = 0. For example, consider the â„“p norm perturbation: U(x) = {z :
âˆ¥z âˆ’xâˆ¥p â‰¤Ïµ}. Then using transduction alone requires that there exists a classifier with 0 robust error
for perturbations U2(x) which are â„“p norm perturbations of adversarial budget 2Ïµ. In contrast, our
result shows that using both transduction and rejection only requires there exists a classifier with
0 robust error for perturbations U(x) which are â„“p norm perturbations of adversarial budget 2Ïµ/3.
Equivalently, for a data distribution with a margin 2Ïµ, transduction without rejection can only handle
adversarial perturbations with budget Ïµ, while combining transduction and rejection can handle
adversarial perturbations with budget 3Ïµ, tolerating three times the adversarial magnitude. Second,
compared to rejection only (see Table 1), this bound has a linear sample complexity rather than
exponential. Therefore, combining transduction and rejection has the benefits of both techniques.
This result, while potentially very strong, comes with the caveat that the defense is not guaranteed
to find a nonempty âˆ†(i.e., the defense is sound but may not be complete). Consider an adversarial
budget Ïµ, and suppose Ëœz is the given potentially perturbed test input and Ëœx is the corresponding clean
test input. To obtain the guarantee, we need to find a model which is Ïµ/3-robust at q = Ëœx + (Ëœz âˆ’Ëœx)/3.
Such a model always exists when OPTU2/3 = 0. However, given only Ëœz without knowing q or Ëœx, our
algorithm finds a model Ïµ/3-robust at every perturbation within 2Ïµ/3 of Ëœz and thus âˆ†may be empty.
While weaker conditions donâ€™t guarantee that we find a model satisfying the conditions, the result
still provides intuition for the success of our derived empirical defense. For typical data distributions
and hypothesis classes, it might be expected that, if we fail to find a Ïµ-robust hypothesis at the
fully-perturbed data, we will nevertheless be more likely to find a model which is robust nearer the
clean data distribution (i.e. where the condition is required by the theory) rather than further away.
Determining conditions for this is an interesting direction for future research.
Proof Sketch.
For intuition, think of U as the â„“p norm perturbation with adversarial budget Ïµ. We
omit technical details; see Appendix A.3 for the complete proof.
4

(a)
(b)
Figure 1: (a) h is Ïµ/3-robust at Ëœz; Ë†h correctly classifies Ëœz.(b) h is not Ïµ/3-robust at Ëœz; Ë†h rejects Ëœz.
Consider some clean training set x, y, clean test set Ëœx, Ëœy, with perturbed test data Ëœz with Ëœzi within Ïµ of
Ëœxi. Let Ëœzâ€² = Ëœx + (Ëœz âˆ’Ëœx)/3 be the intermediate perturbation a third of the way between Ëœx and Ëœz.
First, following Montasser et al. [MHS21], define the set of robust hypotheses âˆ†U1/3
H
(x, y, Ëœzâ€²) as
âˆ†U1/3
H
(x, y, Ëœzâ€²) = {RU1/3(h; x, y) = 0 âˆ§RU1/3(h; Ëœzâ€²) = 0}. That is, we find those classifiers that satisfy:
(1) they are Ïµ/3-robustly correct (i.e., correct and robust to perturbations of budget Ïµ/3) on the
training data (x, y); (2) they have Ïµ/3 margin on the intermediate perturbations Ëœzâ€² (i.e., have the same
prediction for all perturbations of budget Ïµ/3).
This then guarantees, as shown in [MHS21], that with high probability, for any h âˆˆâˆ†U1/3
H
(x, y, Ëœzâ€²) the
robust error facing perturbation of budget Ïµ/3 is bounded by VC(H) log(2n)+log(1/Î´)
n
if OPTU2/3 = 0.
Following TramÃ¨r [Tra22], we can define a transformation FU1/3 that maps a classifier without
rejection, h, to the selective classifier Ë†h = FU1/3(h): Ë†h(x) =
(h(x)
if âˆ€xâ€² âˆˆUâˆ’1/3(x) , h(xâ€²) = h(x)
âŠ¥
otherwise
.
That is, Ë†h rejects x if it is within Ïµ/3 from hâ€™s decision boundary, otherwise accepts and predicts h(x).
Now, continuing with the proof, consider a clean test sample (Ëœx, Ëœy) with adversarial perturbation Ëœz.
The corresponding intermediate perturbation is Ëœzâ€² = Ëœx + (Ëœz âˆ’Ëœx)/3. We will show that if h is correct at
Ëœzâ€², then Ë†h makes no error at Ëœz.
If Ëœz = Ëœx, then Ëœzâ€² = Ëœx = Ëœz. Since h is Ïµ/3-robust at Ëœzâ€², h(Ëœz) = h(Ëœzâ€²) = Ëœy and so Ë†h(Ëœz) = Ëœy which is correct.
Otherwise, we need to consider two cases: (a) h is Ïµ/3-robust at Ëœz; (b) h is not. See visualization
in Figure 1. In both cases, the Ïµ/3-balls about Ëœz and Ëœzâ€² intersect. Let Ëœzâ€²â€² be 5some point in the
intersection. Since h is Ïµ/3-robust at Ëœzâ€², h(Ëœzâ€²â€²) = h(Ëœzâ€²) = Ëœy. Now, in case (a) where h is Ïµ/3-robust at Ëœz,
h(Ëœz) = h(Ëœzâ€²â€²) = Ëœy, which is correct. In case (b) where h is not Ïµ/3-robust at Ëœz, Ë†h rejects Ëœz and makes no
error.
Hence the error of Ë†h on Ëœz is less than the error of h on Ëœzâ€². So the error bound for h implies the desired
error bound for any Ë†h in the set âˆ†â€² =
nË†h = FU1/3(h) : h âˆˆâˆ†U1/3
H
(x, y, Ëœzâ€²)
o
.
As we have access only to the adversarial test data Ëœz, we need to ensure Ïµ/3-robustness at any possible
Ëœzâ€² (i.e. Ëœzâ€² within 2Ïµ/3 of Ëœz). This is equivalent to ensuring Ïµ-robustness at Ëœz in the realizable case. We
then output âˆ†:=
nË†h = FU1/3(h) : h âˆˆT
Ëœzâ€²âˆˆUâˆ’2/3(Ëœz) âˆ†U1/3
H
(x, y, Ëœzâ€²)
o
. By the above, as âˆ†âŠ†âˆ†â€², any Ë†h in âˆ†
achieves the desired bound, leading to the theorem statement.
Remark:
More direct approaches may seem possible, but have surprising pitfalls. At first glance,
this approach may seem less natural than simply applying the analysis of [MHS21] to a potential
Ëœzâ€² âˆˆU1/2(Ëœx) with the condition of OPTU, obtaining a U1/2-robust classifer hâ€², and deriving an
Ïµ-robust selective classifier by the transformation FU1/2. While this seems possible at first, as
TramÃ¨r [Tra22] shows that applying this transformation results in doubled robustness, this isnâ€™t
possible in this situation, as hâ€² is only guaranteed to be U1/2-robust at Ëœzâ€², not at every Ïµ/2 perturbation
of Ëœx as needed by the analysis. Similarly, it might seem possible to obtain an Ïµ/2-robust classifier at
Ëœz using [MHS21], and derive the desired Ïµ-robust classifier from FU1/2; this, however, requires the
condition OPTU2, as the analysis of [MHS21] only applies on perturbations up to half the margin;
hence, this approach gains no advantage from rejection.
5

4.2
Rejection Only: Realizable Case
Theorem 4.2. For any n âˆˆN, Î´ âˆˆ(0, 1/2), hypothesis class H of selective classifiers, perturbation
set U, and distribution D over X Ã— Y satisfying OPTrej
U = 0, there exists an algorithm that outputs
h âˆˆH such that with probability â‰¥1 âˆ’Î´ over (x, y) âˆ¼Dn,
Rrej
U (h; D) â‰¤2VC(T(H)) log(n) + log(1/Î´)
n
where T(H) := {T(h) : h âˆˆH} denotes the transformed hypothesis class with transformation
T(h)(x, xâ€², y) := 1{h(x) , y âˆ¨h(xâ€²) < {y, âŠ¥}}.
Compared to the traditional setting [MHS19] (see Table 1), The guarantee still requires a sample
complexity exponentially large, though the requirement on data is weaker. In contrast, combining
transduction and rejection can reduce the sample complexity to linearly large.
Proof Sketch.
Our proof adapts the classical sample compression argument [LW86] with im-
provements based on [MHS19, HKS19, MY16]. The key argument is to construct the algorithm
that compresses the finite training samples into another finite compressed dataset Ë†S U, where the
data inflation and discretization subroutine uses the dual space of T(H). Then, we perform the
classical PAC learning and followed by the Î±-boosting procedure to construct the final classifier (and
corresponding rejector) with a small robust loss under rejection. Since this is not our main focus, the
proof details are provided in Appendix A.1.
5
Defense by Transduction and Rejection
The analysis of Theorem 4.1 suggests the following defense algorithm: (1) first obtain a classifier h
that are robust and correct on the training data and also robust on the test inputs, (2) then transform h
to a selective classifier Ë†h by rejecting inputs too close to the decision boundary of h. We describe the
resulting defense below, which we refer to as TLDR (Transductive Learning Defense with Rejection).
Step (1) To get h, we perform adversarial training on both the training set and the test set, using a ro-
bust cross-entropy objective. As in TADV [CWG+22] we train with private randomness. Specifically,
we train a model with softmax output as the class prediction probabilities hs and the class prediction
is h(x) = arg maxyâˆˆY hs
y(x). Given the labeled training data (x, y) and the test inputs Ëœz, we optimize
the following objective:
min
h
1
n
X
(x,y)âˆˆ(x,y)
"
LCE(hs(x), y) + max
xâ€²âˆˆU(x) LCE
 hs(xâ€²), y#
+ Î»
m
X
ËœzâˆˆËœz
"
max
Ëœzâ€²âˆˆU(Ëœz) LCE
 hs(Ëœzâ€²), h(Ëœz)#
(1)
where LCE is the cross-entropy loss and Î» > 0 is a hyper-parameter.
Step (2) Having learned h, we now turn h into a selective classifier Ë†h. Recall that Ë†h rejects the input x
if there exists xâ€² âˆˆU1/3(x) with h(x) , h(xâ€²); otherwise accepts and predicts the label h(x). So we
only need to determine the existence of xâ€² âˆˆU1/3(x) with h(x) , h(xâ€²). We use a standard inductive
attack, PGD, for this by solving:
arg max
xâ€²âˆˆU1/3(x)
LCE(hs(xâ€²), h(x)).
(2)
When U is â„“p norm ball of radius Ïµ, the constraint is then âˆ¥xâ€² âˆ’xâˆ¥â‰¤Ïµ/3. In practice, we can
generalize to a constraint âˆ¥xâ€² âˆ’xâˆ¥â‰¤Ïµdefense where where Ïµdefense is a hyper-parameter we call the
rejection radius.
5.1
Adaptive Attacks
Since no strong adaptive attacks exist for the new transduction+rejection setting to our knowledge, we
design one here. Our attack is based on GMSA in [CWG+22], which has been shown to be a strong
attack for transductive defense (without rejection). The goal of the attack is to find perturbations Ëœz of
the clean test inputs Ëœx such that the transductive learner has a large error when given (x, y, Ëœz). GMSA
runs in stages; in each stage t, it simulates the transductive learner on the current data set (x, y, Ëœzt)
6

to get a classifier ht, and then maximizes the minimum or average loss of {hi}t
i=1 to get the updated
perturbations of the test inputs Ëœzt+1 (called GMSAMIN and GMSAAVG, respectively). See [CWG+22]
for the details.
GMSA does not directly apply to our setting since we have selective classifiers Ë†h with a rejection
option which is not considered in GMSA. Our contribution is to design a method to get the updated
perturbations Ëœz of the test inputs in each stage such that the selective classifier incurs a large error.
Recall that Ë†h constructed from h incurs error in two cases: (1) it accepts Ëœz and misclassifies with
h(Ëœz) , y; (2) Ëœz = Ëœx and it rejects Ëœz. We consider the two cases below.
Case (1) We will propose a novel loss measuring the loss of a selective classifier Ë†h on a perturbation
(Ëœz, y) from a clean test point (Ëœx, y) for such kind of error; maximizing this loss gives the desired
Ëœz. Recall that we need Ëœz to be accepted and also the prediction h(Ëœz) , y. For the latter, we can
maximize LCE(hs(Ëœx), y) where hs is the class probabilities of h (i.e., its softmax output). The former
is equivalent to minh(Ëœzâ€²),h(Ëœz) âˆ¥Ëœz âˆ’Ëœzâ€²âˆ¥â‰¥Ïµdefense.
Now, suppose LDB,h(Ëœzâ€²) is a surrogate loss function on the closeness to the decision boundary; it
increases when Ëœzâ€² gets closer to the decision boundary of h. Then the condition is equivalent to
âˆ¥Ëœz âˆ’p(Ëœz)âˆ¥= Ïµdefense where p(Ëœz) = arg maxâˆ¥Ëœzâ€²âˆ’Ëœzâˆ¥â‰¤Ïµdefense LDB,h(Ëœzâ€²). Now, as the maximum value of
âˆ¥Ëœz âˆ’p(Ëœz)âˆ¥is exactly Ïµdefense. So to satisfy the condition, we would like to maximize âˆ¥Ëœz âˆ’p(Ëœz)âˆ¥.
Summing up, for this case, we would like to maximize:
LREJ(Ëœz, y) := LCE(hs(Ëœz), y) + Î»â€² âˆ¥Ëœz âˆ’p(Ëœz)âˆ¥, where p(Ëœz)
= arg max
âˆ¥Ëœzâ€²âˆ’Ëœzâˆ¥â‰¤Ïµdefense
LDB,h(Ëœzâ€²)
(3)
and Î»â€² > 0 is a hyper-parameter. Finally, for LDB,h, the following definition works well in our
experiments: LDB,h(Ëœzâ€²) := rank2 hs(Ëœzâ€²) âˆ’max hs(Ëœzâ€²), which is minimized at the decision boundary as
the top-two class probabilities are equal.
Case (2) A critical step in an effective application of LREJ to a transductive attack is the selection
of which points to perturb. To do this, we apply a post-processing step after finding Ëœz in (1). We
must predict whether Ë†h is more likely to incur error on Ëœz or on the clean input Ëœx (i.e., Ë†h(Ëœx) , y). If we
expect that the clean point is likely to be incorrectly classified or rejected, then we update Ëœz to Ëœx. In
GMSA, we have access to a series of models trained on previous attack iterations; we estimate the
likelihood of success at Ëœz and Ëœx by the fraction of previous models which fail at each point.
Summing up the two cases and combining with GMSA gives our final attack (details in Algorithm 1
in Appendix B.5).
6
Experiments
This section performs experiments to evaluate the proposed method TLDR and compare it with
baseline methods (e.g., those using only rejection or transduction). Our main findings are: 1)
TLDR outperforms the baselines significantly in robustness, confirming the advantage of combining
transduction and rejection. 2) Our adaptive attack is significantly stronger than existing attacks which
were not designed for the new setting, providing a strong evaluation. 3) Rejection rates rise steadily
with the rejection radius, but few clean samples are rejected and the robust accuracy remains stable.
6.1
Robustness of TLDR
Baselines.
(1) AT: adversarial training [MMS+18]; (2) AT (with rejection): adversarial training
(AT) with rejection; (3) RMC [WYW20b]; (4) DANN [AGL+15]; (5) TADV [CWG+22]; (6) Re-
jectron [GKKM20]. Among them, (1) is in the traditional induction setting, (2) is rejection only,
(3)(4)(5) are transduction only, and (6) incorporates both transduction and rejection.
Evaluation.
We attack the defenses and report the robust accuracy (1 - the robust error defined
in Section 3). To attack inductive classifiers, we use AutoAttack [CH20]. For inductive selective
classifiers, we use PGD on the rejection-aware loss LREJ from Eqn (3). For transductive classifiers,
we use GMSA which has been shown to be a strong adaptive attack on transduction [CWG+22].
Finally, for our transductive selective classifiers, we use our adaptive attack in Section 5.1 (roughly
GMSA with LREJ). For Rejectron [GKKM20] we use GMSA with a loss function LDISC targeting
their defense; see Appendix B.6 for the details.
7

Setting
Defense
Attacker
MNIST
CIFAR-10
pREJ
Robust accuracy
pREJ
Robust accuracy
Induction
AT [MMS+18]
AutoAttack
â€“
0.897
â€“
0.448
Rejection only
AT (with rejection)
PGD (LREJ)
0.852
0.968
0.384
0.634
Transduction only
RMC [WYW20b]
GMSA (LCE)
â€“
0.588
â€“
0.396
DANN [AGL+15]
GMSA (LCE)
â€“
0.062
â€“
0.055
TADV [CWG+22]
GMSA (LCE)
â€“
0.943
â€“
0.541
Transduction+Rejection
URejectron [GKKM20]
GMSA (LDISC)
0.274
0.721
0.000
0.145
Transduction+Rejection
TLDR (ours)
GMSA (LREJ)
0.126
0.972
0.208
0.739
Table 3: Results on MNIST and CIFAR-10. Robust accuracy is 1 - robust error; see Section 3. pREJ
is the percentage of inputs rejected. The baseline results are from [CWG+22]. The strongest attack
against each defense is shown. The best result is boldfaced.
Attack
MNIST
CIFAR-10
PGD (LCE)
0.991
0.794
PGD (LREJ)
0.988
0.781
AutoAttack
0.989
0.756
GMSA (LCE)
0.988
0.853
GMSA (LREJ)
0.972
0.739
Table 4: Robust accuracy by different attacks on
TLDR. The strongest attack is boldfaced.
Loss
MNIST
CIFAR-10
AutoAttack [CH20]
0.980
0.592
LCE
0.977
0.524
LREJ(LCE)
0.974
0.470
LREJ
0.973
0.458
Table 5: Robust accuracy under different attack
losses on a fixed adversarially trained model
with rejection, AutoAttack for comparison. The
strongest attack is boldfaced.
TLDR Components
Attacker
MNIST
CIFAR-10
Rejection
Ltest
pREJ
Robust accuracy
pREJ
Robust accuracy
âœ“
âœ“
GMSA (LREJ)
0.588
0.967
0.208
0.739
âœ“
Ã—
GMSA (LREJ)
0.646
0.975
0.179
0.725
Ã—
âœ“
GMSA (LCE)
â€“
0.900
â€“
0.516
Ã—
Ã—
GMSA (LCE)
â€“
0.935
â€“
0.516
Table 6: Ablation study of TLDR. The best result is boldfaced.
For transductive models, we report the stronger of GMSAMIN and GMSAAVG. Inductive models are
trained with standard adversarial training [GSS15], and transductive models with the TLDR loss in
Eqn (1). As Rejectron depends heavily on a key hyperparameter determining confidence needed to
reject, we report the results for the parameter value strongest against our attack. The best-performing
value on CIFAR-10 effectively eliminated the possibility of rejection (hence the rejection rate of 0);
other choices resulted in near-0 robust accuracy.
Datasets and Defense/Attack Setup. We evaluate on MNIST [LeC98] and CIFAR-10 [KH+09]. We
consider an adversarial budget of Ïµ = 0.3 in lâˆžon MNIST and Ïµ = 8/255 in lâˆžon CIFAR-10. For
defense, on MNIST, we use a LeNet architecture; on CIFAR-10 we use a ResNet-20 architecture.
In both cases, we train for 40 epochs with a learning rate of 0.001 using ADAM for optimization.
On MNIST, we use 40 iterations of PGD during training with a step size of 0.01. On CIFAR-10, we
use 10 iterations of PGD in training with a step size of 2/255. In training TLDR, we put 85% of the
weight on Ltrain, equivalent to Î» = 0.176 after a warm start period epochs in which Î» = 0. We use a
rejection radius of Ïµ/4 for selective classifiers. For attack, we use 10 iterations of GMSA on both
datasets. On MNIST, we use 200 steps of PGD with a stepsize of 0.01 while generating adversarial
examples. On CIFAR-10, the PGD attacks use 100 steps with a stepsize of 1/255. Defense settings
used while training models in GMSA (including internal PGD settings) are the standard defense
settings. Internal optimizations in the calculation of LREJ use 10 steps of PGD with a stepsize of 15%
of the rejection radius. We use Î»â€² = 1 in LREJ; we observe little sensitivity to the parameter.
8

Results. Table 3 shows the robust accuracy and rejection rate of different methods. We observe
that either transduction or rejection can improve the performance, while combining both techniques
leads to the best results. In particular, our defense outperforms existing transductive defenses such as
RMC and DANN. It also outperforms the strongest existing baseline of 66.56% robust accuracy on
CIFAR-10 [CAS+20] (note that 66.56% is for the classic setting without rejection and transduction).
Finally, note that our defense passes the sanity check of [Tra22] (i.e. we do not exceed the theoretical
upper bound on robust accuracy of 79%), providing evidence that our evaluation is reliable. These
results provide positive support for the benefit of combining transduction and rejection for robustness.
6.2
Ablation Studies
Different Attacks on TLDR. Table 4 shows the results of different attack methods on TLDR.
Previous work [CWG+22] shows that transduction-aware attacks are necessary against transductive
defenses; we observe that attacks (PGD on LCE or LREJ and AutoAttack) from the traditional setting
perform poorly against our defense. We can also see that GMSA significantly outperforms even a
rejection-aware transfer attack (referred to as PGD targeting LREJ; note that PGD and AutoAttack do
not target the final model in this case, given the transductive setting, but instead target a proxy trained
by the adversary); see Algorithm 2 in Appendix B.5 for the full details. This shows that GMSA
is critical for attacking a transductive defender; while PGD and AutoAttack are strong against an
inductive model, they performs poorly facing transduction. Finally, we observe that GMSA with LCE
is much weaker than GMSA with LREJ. This shows another key component in our adaptive attack,
the loss LREJ, is also critical to get a strong attack against our defense.
Ablation on LREJ. To further investigate the importance of LREJ, we attack an adversarially trained
model with rejection (i.e., the AT+Rejection model), with PGD on different losses: LREJ, cross-
entropy LCE, and LREJ with LDB,h replaced by LCE, with AutoAttack given for comparison. We
also consider a multitargeted attack LMULTI which attempts to find, for each incorrect label, the
perturbation with the highest robust confidence up to the rejection radius. More precisely, given a
point (x, y) and a base classifier h with adversarial budget Ïµ and rejection radius Ïµdefense, for each
target label yâ€² , y, we find a perturbation z(yâ€²) such that z(yâ€²) and its Ïµdefense-ball around it all get the
label yâ€² (and thus z(yâ€²) will be accepted and misclassified): we let
z(yâ€²) = arg min
p:âˆ¥pâˆ’xâˆ¥â‰¤Ïµ
LMULTI(p, yâ€²), where LMULTI(p, yâ€²) := LCE(p, yâ€²) +
max
âˆ¥zâˆ’pâˆ¥â‰¤Ïµdefense LCE(z, yâ€²).
(4)
Finally, the attack outputs the strongest perturbation,
z = arg min
z(yâ€²):yâ€²,y
LMULTI(z(yâ€²), yâ€²).
(5)
Table 5 shows the robust accuracy under these different attacks; note that, as with transudction,
AutoAttack is unable to find perturbations which evade defenses with rejection. LREJ leads to the
strongest attack. In particular, it can be significantly better than LCE and LMULTI, demonstrating its
importance for attacking models with rejection.
Key Components of TLDR. Compared to traditional defenses, TLDR has two novel components:
using the given test inputs in training the classifier (the second term in Equation (1), referred to as
Ltest), and transforming the trained classifier into one with rejection. Table 6 shows the results of the
ablation study on these two components. In all cases, rejection significantly improves results. The
use of transduction is helpful on CIFAR-10, but reduces performance on MNIST probably since itâ€™s
easier to get robust predictions on MNIST and thus knowing test inputs does not help.
7
Conclusion
Existing works on leveraging transduction and rejection gave mixed results on their benefits for
adversarial robustness. In this work we take a step in realizing their promise in practical deep
learning settings. Theoretically, we show that a novel application of Tram`â€™erâ€™s results give improved
sample complexity for robust learning in the bounded perturbations setting. Guided by our theory, we
identified a practical robust learning algorithm leveraging both transduction and rejection. Systematic
experiments confirm the benefits of our constructions. There are many future avenues to explore,
such as improving the theoretical bounds, and improving the efficiency of our algorithms.
9

References
[AGL+15]
Hana Ajakan, Pascal Germain, Hugo Larochelle, FranÃ§ois Laviolette, and Mario Marc-
hand. Domain-adversarial neural networks, 2015.
[Ass83]
Patrick Assouad. DensitÃ© et dimension. In Annales de lâ€™Institut Fourier, volume 33,
pages 233â€“282, 1983.
[BEHW89] Anselm Blumer, Andrzej Ehrenfeucht, David Haussler, and Manfred K Warmuth.
Learnability and the vapnik-chervonenkis dimension. Journal of the ACM (JACM),
36(4):929â€“965, 1989.
[BSRK22]
Sina Baharlouei, Fatemeh Sheikholeslami, Meisam Razaviyayn, and Zico Kolter. Im-
proving adversarial robustness via joint classification and multiple explicit detection
classes. arXiv preprint arXiv:2210.14410, 2022.
[CAS+20]
Francesco Croce, Maksym Andriushchenko, Vikash Sehwag, Edoardo Debenedetti,
Nicolas Flammarion, Mung Chiang, Prateek Mittal, and Matthias Hein. Robustbench:
a standardized adversarial robustness benchmark. arXiv preprint arXiv:2010.09670,
2020.
[CH20]
Francesco Croce and Matthias Hein. Reliable evaluation of adversarial robustness with
an ensemble of diverse parameter-free attacks. In International conference on machine
learning, pages 2206â€“2216. PMLR, 2020.
[CRC+21]
Jiefeng Chen, Jayaram Raghuram, Jihye Choi, Xi Wu, Yingyu Liang, and Somesh Jha.
Revisiting adversarial robustness of classifiers with a reject option. In The AAAI-22
Workshop on Adversarial Machine Learning and Beyond, 2021.
[CW17]
Nicholas Carlini and David Wagner. Towards evaluating the robustness of neural
networks. In 2017 ieee symposium on security and privacy (sp), pages 39â€“57. Ieee,
2017.
[CWG+22] Jiefeng Chen, Xi Wu, Yang Guo, Yingyu Liang, and Somesh Jha. Towards evaluating
the robustness of neural networks learned by transduction. In International Conference
on Learning Representations, 2022.
[GKKM20] Shafi Goldwasser, Adam Tauman Kalai, Yael Kalai, and Omar Montasser. Beyond
perturbations: Learning guarantees with arbitrary adversarial test examples. Advances
in Neural Information Processing Systems, 33:15859â€“15870, 2020.
[Goo19]
Ian Goodfellow. A research agenda: Dynamic models to defend against correlated
attacks. arXiv preprint arXiv:1903.06293, 2019.
[GSS14]
Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing
adversarial examples. arXiv preprint arXiv:1412.6572, 2014.
[GSS15]
Ian J. Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing
adversarial examples, 2015.
[HKS19]
Steve Hanneke, Aryeh Kontorovich, and Menachem Sadigurschi. Sample compression
for real-valued learners. In Algorithmic Learning Theory, pages 466â€“488. PMLR, 2019.
[HYC+22]
Zhiyuan He, Yijun Yang, Pin-Yu Chen, Qiang Xu, and Tsung-Yi Ho. Be your own
neighborhood: Detecting adversarial example by the neighborhood relations built on
self-supervised learning. arXiv preprint arXiv:2209.00005, 2022.
[KCF20]
Masahiro Kato, Zhenghang Cui, and Yoshihiro Fukuhara. Atro: Adversarial training
with a rejection option. arXiv preprint arXiv:2010.12905, 2020.
[KH+09]
Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny
images. 2009.
[LeC98]
Yann LeCun. The MNIST database of handwritten digits. 1998.
[LF19]
Cassidy Laidlaw and Soheil Feizi. Playing it safe: Adversarial robustness with an
abstain option. arXiv preprint arXiv:1911.11253, 2019.
[LW86]
Nick Littlestone and Manfred Warmuth. Relating data compression and learnability.
1986.
10

[MDFF16]
Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, and Pascal Frossard. Deepfool: a
simple and accurate method to fool deep neural networks. In Proceedings of the IEEE
conference on computer vision and pattern recognition, pages 2574â€“2582, 2016.
[MHS19]
Omar Montasser, Steve Hanneke, and Nathan Srebro. Vc classes are adversarially
robustly learnable, but only improperly. In Conference on Learning Theory, pages
2512â€“2530. PMLR, 2019.
[MHS21]
Omar Montasser, Steve Hanneke, and Nathan Srebro. Transductive robust learning
guarantees. arXiv preprint arXiv:2110.10602, 2021.
[MMS+17] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian
Vladu. Towards deep learning models resistant to adversarial attacks. arXiv preprint
arXiv:1706.06083, 2017.
[MMS+18] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian
Vladu. Towards deep learning models resistant to adversarial attacks. In 6th Inter-
national Conference on Learning Representations, Conference Track Proceedings.
OpenReview.net, 2018.
[MMS+19] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian
Vladu. Towards deep learning models resistant to adversarial attacks, 2019.
[MY16]
Shay Moran and Amir Yehudayoff. Sample compression schemes for vc classes. Journal
of the ACM (JACM), 63(3):1â€“10, 2016.
[PZH+22]
Tianyu Pang, Huishuai Zhang, Di He, Yinpeng Dong, Hang Su, Wei Chen, Jun Zhu,
and Tie-Yan Liu. Two coupled rejection metrics can tell adversarial examples apart. In
Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,
pages 15223â€“15233, 2022.
[SDM+20]
Angelo Sotgiu, Ambra Demontis, Marco Melis, Battista Biggio, Giorgio Fumera, Xiaoyi
Feng, and Fabio Roli. Deep neural rejection against adversarial examples. EURASIP
Journal on Information Security, 2020(1):1â€“10, 2020.
[SF12]
Robert E Schapire and Yoav Freund. Boosting. adaptive computation and machine
learning. MIT Press, Cambridge, MA, 1(1.2):9, 2012.
[SHS20]
David Stutz, Matthias Hein, and Bernt Schiele. Confidence-calibrated adversarial
training: Generalizing to unseen attacks. In International Conference on Machine
Learning, pages 9155â€“9166. PMLR, 2020.
[SLK20]
Fatemeh Sheikholeslami, Ali Lotfi, and J Zico Kolter. Provably robust classification
of adversarial examples with detection. In International Conference on Learning
Representations, 2020.
[SLM+]
Fatemeh Sheikholeslami, Wan-Yi Lin, Jan Hendrik Metzen, Huan Zhang, and J Zico
Kolter. Denoised smoothing with sample rejection for robustifying pretrained classifiers.
In Workshop on Trustworthy and Socially Responsible Machine Learning, NeurIPS
2022.
[SSBD14]
Shai Shalev-Shwartz and Shai Ben-David. Understanding machine learning: From
theory to algorithms. Cambridge university press, 2014.
[TCBM20] Florian Tramer, Nicholas Carlini, Wieland Brendel, and Aleksander Madry. On adaptive
attacks to adversarial example defenses. Advances in Neural Information Processing
Systems, 33:1633â€“1645, 2020.
[Tra22]
Florian Tramer. Detecting adversarial examples is (nearly) as hard as classifying them.
In International Conference on Machine Learning, pages 21692â€“21702. PMLR, 2022.
[WJS+21]
Dequan Wang, An Ju, Evan Shelhamer, David Wagner, and Trevor Darrell. Fighting
gradients with gradients: Dynamic defenses against adversarial attacks. arXiv preprint
arXiv:2105.08714, 2021.
[WYW20a] Yi-Hsuan Wu, Chia-Hung Yuan, and Shan-Hung Wu. Adversarial robustness via
runtime masking and cleansing. In International Conference on Machine Learning,
pages 10399â€“10409. PMLR, 2020.
11

[WYW20b] Yi-Hsuan Wu, Chia-Hung Yuan, and Shan-Hung Wu. Adversarial robustness via runtime
masking and cleansing. In Hal DaumÃƒÂ© III and Aarti Singh, editors, Proceedings of
the 37th International Conference on Machine Learning, volume 119 of Proceedings of
Machine Learning Research, pages 10399â€“10409. PMLR, 13â€“18 Jul 2020.
[ZYJ+19]
Hongyang Zhang, Yaodong Yu, Jiantao Jiao, Eric Xing, Laurent El Ghaoui, and Michael
Jordan. Theoretically principled trade-off between robustness and accuracy. In Interna-
tional conference on machine learning, pages 7472â€“7482. PMLR, 2019.
12

Supplementary Material
A
Proof Details
Before introducing the proof for the generalization results, we first need to make some additional
definitions. We define the empirical robust risk as
Ë†RU(h; S ) =
X
(x,y)âˆˆS
ï£®ï£¯ï£¯ï£¯ï£¯ï£°sup
zâˆˆU(x)
1{h(z) , y}
ï£¹ï£ºï£ºï£ºï£ºï£»
And we can define the empirical robust risk under rejection accordingly:
Ë†Rrej
U (h; S ) =
X
(x,y)âˆˆS
ï£®ï£¯ï£¯ï£¯ï£¯ï£°sup
zâˆˆU(x)
1{h(x) , y âˆ¨h(z) < {y, âŠ¥}}
ï£¹ï£ºï£ºï£ºï£ºï£»
And we can define the corresponding robust empirical risk minimization procedure (under rejection)
as follows:
RERMH(S ) := argmin
hâˆˆH
Ë†RU(h; S )
RERMrej
H (S ) := argmin
hâˆˆH
Ë†Rrej
U (h; S )
A.1
Rejection Only: Realizable Case
Definition A.1 (Realizable Robust PAC Learnability under Rejection). For Y = {0, 1}, âˆ€Ïµ, Î´ âˆˆ
(0, 1), H = HcÃ—Hr, the sample complexity of realizable robust (Ïµ, Î´) - PAC learning of H with respect
adversary U under rejection, denoted as MRE(Ïµ, Î´; H, U), is defined as the smallest m âˆˆN âˆª{0} for
which there exists a learning rule A : (X Ã— Y)m 7âˆ’â†’(Y âˆª{âŠ¥})X s.t. for every data distribution D over
(X Ã— Y)m where there exists a predictor with rejection option hâˆ—âˆˆH with 0 risk, RU,rej(hâˆ—; D) = 0
with probability at least 1 âˆ’Î´ over S âˆ¼Dm,
Rrej
U (A(S ); D) â‰¤Ïµ
If no such m exists, MRE(Ïµ, Î´; H, U) = âˆž. We say that H is robustly PAC learnable under rejection
in the realizable setting with respect to adversary U if âˆ€Ïµ, Î´ âˆˆ(0, 1), MRE(Ïµ, Î´; H, U) is finite.
Theorem A.2 (Sample Complexity for Realizable Robust PAC Learning under Rejection). In the
realizable setting, for any H = Hc Ã— Hr and U, and any Ïµ, Î´ âˆˆ(0, 1/2),
MRE(Ïµ, Î´; H, U) = 2O((dr+dc) log(dr+dc)) 1
Îµ log
 1
Îµ
!
+ O
 1
Îµ log
 1
Î´
!!
(6)
where dr = VC(Hr), dc = VC(Hc).
The idea of the proof is to adapt the classical sample compression argument [LW86] with im-
provements based on [MHS19, HKS19, MY16]. The generalization result in the inductive case
(Theorem 4.2) directly comes from Equation (31).
Proof. First, we define the concept of sample compression scheme and sample compression algorithm.
Definition A.3 (Sample Compression Scheme). Given âˆ€m âˆˆN samples, S âˆ¼Dm, a sample
compression scheme of size k is defined by the following pair of functions:
1. Compression function Îº : (X Ã— Y)m 7â†’(X Ã— Y)â‰¤k.
2. Reconstruction function: Ï : (X Ã— Y)â‰¤k 7â†’H.
An algorithm A is a sample compression algorithm if âˆƒÎº, Ï s.t. A(S ) = (Îº â—¦Ï)(S ).
Fix Ïµ, Î´
âˆˆ
(0, 1), m
>
2(dr + dc) log(dr + dc).
Let the compression parameter, n
=
O  (dr + dc) log (dr + dc).
Let D be any distribution, then by realizability of the learner,
infhâˆˆH Rrej
U (h; D) = 0. Thus, âˆ€S sampled from D, we have Ë†R
rej
U (RERMrej
H (S ); S ) = 0.
13

Compression
First, we define a compression function Îº as through the following inflation and
discretization procedure. Given the training data S := {(xi, yi)}iâˆˆ[m], we define the following index
mapping:
I(x) = min{i âˆˆ[m] : x âˆˆU(xi)},
âˆ€x âˆˆ
[
iâˆˆ[m]
U(xi).
(7)
In another word, this index function outputs the first indexed training sample to include x in its
neighborhood.
Then, we consider the set of RERM mapping learned by a size n subset of the training data:
Ë†H = {RERMrej
H (L) : L âŠ†S, |L| = n}.
(8)
Note that
| Ë†H| â‰¤|{L : L âŠ†S, |L| = n}| =
 
m
n
!
â‰¤
em
n
n
.
(9)
Then, we inflate the data in the following way:
S U =
[
iâˆˆ[m]
 xI(x), x, yI(x)
 : x âˆˆU (xi)	 .
(10)
Note that xI(x) can be different from xi.
Letâ€™s define the following transformation T:
T(h)(x, xâ€², y) := 1{h(x) , y âˆ¨h(xâ€²) < {y, âŠ¥}}, h âˆˆH.
(11)
And we can obtain the transformed hypothesis class T(H) := {T(h)|h âˆˆH}.
Now, we proceed to define the dual space G of T(H) as the following set of functions.
G := {g(x,xâ€²,y)|g(x,xâ€²,y)(t) = t(x, xâ€², y), t âˆˆT(H)}.
(12)
We denote the VC dimension of the dual space as VCâˆ—(T(H)) := VC(G).
By Lemma Appendix A.1,
VC(T(H)) = O  (dr + dc) log (dr + dc) .
(13)
By the classic result in [Ass83], the VC dimension of the dual space satisfies the following inequality:
VCâˆ—(T(H)) < 2VC(T(H))+1.
(14)
Now, we can construct the compressed dataset Ë†S U as the following. For each (x, xâ€², y) âˆˆS U,
{g(x,xâ€²,y)(t)}tâˆˆT( Ë†H) gives a labeling. When ranging over (x, xâ€², y) âˆˆS U, the labeling may not be unique.
So for each unique labeling, we choose a representative (x, xâ€², y) âˆˆS U, and let Ë†S U be the set of the
representatives. That is:
Ë†S U =

(x, xâ€², y) âˆˆS U
 {g(x,xâ€²,y)(t)}tâˆˆT( Ë†H) provides a unique labeling

.
(15)
Intuitively, Ë†S U split the infinite size dataset S U into finite size according to the labeling of T( Ë†U) on
the dual space. Thus, Ë†S U is not necessarily unique but always exists. And | Ë†S U| equals the number of
possible labeling for T( Ë†H).
Let dâˆ—:= VC(G) = VCâˆ—(T(H)) denote the VC-dimension of G, the dual hypothesis class of
T( Ë†H) [Ass83]. By applying Sauerâ€™s Lemma, we obtain that for |T( Ë†H)| > dâˆ—,
| Ë†S U| â‰¤
ï£«ï£¬ï£¬ï£¬ï£¬ï£­
e|T( Ë†H)|
dâˆ—
ï£¶ï£·ï£·ï£·ï£·ï£¸
dâˆ—
.
(16)
14

Let n = Î˜ (VC (T (H))). For m â‰¥n, we have
| Ë†S U| â‰¤

e|T( Ë†H)|
dâˆ—
(17)
â‰¤

e| Ë†H|
dâˆ—
(18)
â‰¤

e
em
n
ndâˆ—
(19)
â‰¤
 e2m
n
!ndâˆ—
(20)
=
 
e2m
VC(T(H))
!Î˜(VC(T(H))Â·VC(T(Hâˆ—)))
.
(21)
Now we have obtain the compression map: Îº(S ) = Ë†S U.
Reconstruction
Now, we want to reconstruct a hypothesis from Ë†S U. First, suppose we have a
data distribution over Ë†S U, denoted as P. This distribution P over samples will be later used in the
Î±âˆ’boosting procedure.
Then, we sample the set of n i.i.d. samples from P and obtain S â€² âˆˆË†S U. By classic PAC learn-
ing guarantee [BEHW89], for n = Î˜(VC(T(H))) = Î˜(dr + dc) log(dr + dc), we have with non-
zero probability âˆ€t âˆˆT(H) with P
(x,xâ€²,y)âˆˆS â€² t(x, xâ€², y) = 0 implies E(x,xâ€²,y)âˆ¼Pt(x, xâ€², y) < 1/9. Let
L = {(x, y) : (x, xâ€², y) âˆˆS â€²} âŠ†S , and tP = T(RERMrej
H (L)). Since Ë†R
rej
U (RERMrej
H (L); L) = 0,
âˆ€(x, xâ€², y) âˆˆS â€², tP(x, xâ€², y) = 0. Thus, âˆ€P over Ë†S U, there exists a weak learner tP âˆˆT( Ë†H), s.t.
E(x,xâ€²,y)âˆ¼P tP(x, xâ€², y) < 1/9.
Now, we use tP as a weak hypothesis in a boosting algorithm, specifically Î±âˆ’boost algorithm
from [SF12] with Ë†S U as the dataset and Pk generated at each round of the algorithm. Then with
appropriate choice of Î±, running Î±âˆ’boosting for K = O(log(| Ë†S U|)) rounds gives a sequence of
hypothesis h1, . . . , hK âˆˆË†H and the corresponding ti = T(hi) such that âˆ€(x, xâ€², y) âˆˆË†S U,
1
K
K
X
k=1
1{hk(x) , y âˆ¨hk(xâ€²) < {y, âŠ¥}}
(22)
= 1
K
K
X
k=1
tk(x, xâ€², y)
(23)
< 2
9 < 1
3.
(24)
Since Ë†S U includes all the unique labellings, 1
K
PK
k=1 tk(x, xâ€², y) < 1
3, âˆ€(x, xâ€², y) âˆˆË†S U implies
1
K
K
X
k=1
tk(x, xâ€², y) < 1
3, âˆ€(x, xâ€², y) âˆˆS U.
(25)
Let Â¯h := Majority(h1, . . . , hK), i.e., Â¯h outputs the prediction in Y âˆª{âŠ¥} that receives the most votes
from {h1, . . . , hK}. Then âˆ€(x, xâ€², y) âˆˆË†S U,
1{Â¯h(x) , y âˆ¨Â¯h(xâ€²) < {y, âŠ¥}} = 0.
(26)
This is because: (1) on x, less than 1/3 of hiâ€™s do not output y, so Â¯h(x) = y; (2) on xâ€², less than 1/3 of
hiâ€™s do not output y or âŠ¥, so the majority vote must be in y or âŠ¥, i.e., Â¯h(x) âˆˆ{y, âŠ¥}.
In summary, given the same m training samples, we can simply find a Â¯h with 0 robust error on S :
Ë†R
rej
U (Â¯h; D) =
m
X
i=1
ï£®ï£¯ï£¯ï£¯ï£¯ï£°sup
zâˆˆU(x)
1{Â¯h(x) , y âˆ¨Â¯h(z) < {y, âŠ¥}}
ï£¹ï£ºï£ºï£ºï£ºï£»= 0.
(27)
15

Now we have the compression set with size:
nK = O(VC(T(H)) log(| Ë†S U|)) = O(VC(T(H))2 VCâˆ—(T(H)) log(m/ VC(T(H))))
Then, we apply Lemma 11 of [MHS19] (Replacing RU with Rrej
U still holds), we obtain for sufficiently
large m, with probability at least 1 âˆ’Î´,
Rrej
U (Â¯h; D) â‰¤O
 
VC(T(H))2 VCâˆ—(T(H)) 1
m log(m/ VC(T(H))) log(m) + 1
m log(1/Î´)
!
.
(28)
We then can extend the sparsification procedure from [MY16, MHS19] to the rejection scenario.
Since t1, . . . , tK âˆˆT( Ë†H), the classic uniform convergence results [SSBD14] implies that we can
sample N = O(VCâˆ—(T(H))) i.i.d. indices i1, . . . , iN âˆ¼Uniform([K]) and obtain:
sup
(x,xâ€²,y)âˆˆS U

1
N
N
X
j=1
ti j(x, xâ€², y) âˆ’1
K
T
X
i=1
ti(x, xâ€², y)

< 1
18
(29)
And thus, we can combine Equation (22) with Equation (29) and obtain:
âˆ€(x, xâ€², y) âˆˆS U, 1
N
N
X
j=1
ti j(x, xâ€², y) â‰¤âˆ’1
18 + 1
K
K
X
i=1
tk(x, xâ€², y) < âˆ’1
18 + 4
9 = 1
2
we can further obtain an improved hypothesis Â¯tâ€² := Majority(ti1, . . . tiN) with
Â¯tâ€²(x, xâ€², y) = 0, âˆ€(x, xâ€², y) âˆˆS U
Thus, the compression set has a reduced size:
nN = O(VC(T(H)) Â· VCâˆ—(T(H)))
Now, we apply Lemma 11 of [MHS19] and can obtain the following improved bound. Applying
similar strategy from Equation (26), we can obtain
Â¯h
â€² := Majority(hi1, . . . hiN) = Ï( Ë†S U) = A(S )
(30)
which is our full reconstruction map.
Then, for large sample size m â‰¥c VC(T(H)) VCâˆ—(T(H)) (c is a sufficiently large constant), with
probability at least 1 âˆ’Î´,
RU,rej(Â¯hâ€²; D) â‰¤O
 
VC(T(H)) VCâˆ—(H) 1
m log(m) + 1
m log(1/Î´)
!
(31)
Plugging in Lemma Appendix A.1 and solving for m gives
MRE(Ïµ, Î´; H, U) = 2O(VC(T(H))) 1
Îµ log
 1
Îµ
!
+ O
 1
Îµ log
 1
Î´
!!
(32)
= 2O((dr+dc) log(dr+dc)) 1
Îµ log
 1
Îµ
!
+ O
 1
Îµ log
 1
Î´
!!
(33)
â–¡
Lemma
[VC dimension of robust loss with rejection] Let VC(Hc) = dc, and VC(Hr) = dr. Then,
VC(T(H)) = O  (dr + dc) log (dr + dc).
Proof. Suppose d > dr + dc.
By definition of VC dimension, the max number of labeling of d points is 2d on h âˆˆT(H). And since
the label of h is a deterministic function of hc and hr, by Sauerâ€™s Lemma, the number of labeling of h
is at most O(ddr) Ã— O(ddc) = O(ddr+dc).
Thus, 2d = O(ddr+dc). And d = O((dr + dc) log(dr + dc)).
If d < dr + dc, d = O(dr + dc) log(dr + dc) by definition.
â–¡
16

A.2
Rejection Only: Agnostic Case
Now, we define notion of PAC learnability in the agnostic case under rejection setting as the follows:
Definition A.4 (Robust PAC Learnability under Rejection). For Y = {0, 1}, âˆ€Ïµ, Î´ âˆˆ(0, 1), H =
Hc Ã— Hr, the sample complexity of robust (Ïµ, Î´) - PAC learning of H with respect to perturbation U
under rejection, denoted as MAG(Ïµ, Î´; H, U), is defined as the smallest m âˆˆN âˆª{0} for which there
exists a learning rule A : (X Ã— Y)m 7âˆ’â†’(Y âˆª{âŠ¥})X s.t. for every data distribution D over (X Ã— Y)m,
Rrej
U (A(S ); D) â‰¤OPTrej
U + Ïµ
with probability at least 1 âˆ’Î´ over S âˆ¼Dm. If no such m exists, MAG(Ïµ, Î´; H, U) = âˆž. We say that
H is robustly PAC learnable under rejection if MAG(Ïµ, Î´; H, U) is finite for all Ïµ, Î´ âˆˆ(0, 1).
Lemma A.5. Let MRE = MRE(1/3, 1/3; H, U). Then,
MAG(Ïµ, Î´; H, U) = O
 MRE
Ïµ2
log2
 MRE
Ïµ
!
+ 1
Ïµ2 log
 1
Î´
!!
(34)
Proof. The proof detail follows exactly the same from the Proof of Theorem 8 from [MHS19] with
the loss replaced.
â–¡
Theorem A.6 (Sample Complexity for Agnostic Robust PAC Learning under Rejection). In the
agnostic setting, for any H = Hc Ã— Hr and U, and any Ïµ, Î´ âˆˆ(0, 1/2),
MAG(Ïµ, Î´; H, U) = O

VC(T(H)) VCâˆ—(T(H)) log (VC(T(H)) VCâˆ—(T(H)))
(35)
1
Îµ2 log2
 VC(T(H)) VCâˆ—(T(H))
Îµ
!
+ 1
Îµ2 log
 1
Î´
!
(36)
= 2O(VC(H)) 1
Îµ2 log2
 1
Îµ
!
+ O
 1
Îµ2 log
 1
Î´
!!
(37)
= 2O((dr+dc) log(dr+dc)) 1
Îµ2 log2
 1
Îµ
!
+ O
 1
Îµ2 log
 1
Î´
!!
(38)
where dr = VC(Hr), dc = VC(Hc).
Proof. Combining results from Lemma Lemma A.5 and Theorem A.2 gives the complexity result.
Solving Equation (37) gives the following generalization result given in Table 1
Pr
(x,y)âˆ¼Dn
h
Rrej
U (A(x, y); D) â‰¤Ïµ
i
â‰¥1 âˆ’Î´
where Ïµ = O
 q
2VC(T(H))+log(1/Î´)
n
!
.
â–¡
A.3
Transduction+Rejection: Realizable Case
We will prove a more general result which then implies Theorem 4.1. First, the training data can also
be perturbed, i.e., the adversary perturbs z âˆˆU(x) and Ëœz âˆˆU(Ëœx), and the learner A are given (z, y, Ëœz)
instead of (x, y, Ëœz). The criterion in the transductive rejection error (see Table 2) is then the worst case
over both z âˆˆU(x) and Ëœz âˆˆU(Ëœx). Second, we will consider OPTU3 = 0 and prove the guarantee
tolerating U2. This then implies the guarantee tolerating U when OPTU3/2 = 0.
In general the set of optimally learned classifiers âˆ†is defined as follows [MHS21]:
âˆ†U
H(z, y, Ëœz) =
ï£±ï£´ï£´ï£´ï£²ï£´ï£´ï£´ï£³
{h âˆˆH : RUâˆ’1(h; z, y) = 0 âˆ§RUâˆ’1(h; Ëœz) = 0}
(Realizable Case)
arg min
hâˆˆH
max {RUâˆ’1(h; z, y), RUâˆ’1(h; Ëœz)}
(Agnostic Case)
where
RU(h; z, y) = sup
ËœxâˆˆU(z)
1
n
n
X
i=1
1{h(Ëœxi) , yi}
17

and
RU(h; z) = RU(h; z, h(z)).
Recall the transformation F which we define following TramÃ¨r [Tra22] in Section 4.
Then, we define the relaxed robust shattering dimension following [MHS21]:
Definition A.7 (Relaxed Robust Shattering Dimension). A sequence z1, . . . , zk âˆˆX is relaxed U-
robustly shattered by H, if âˆ€y1, . . . , yk âˆˆ{Â±1}: âˆƒxy1
1 , . . . , xyk
k âˆˆX and âˆƒh âˆˆH such that zi âˆˆU(xyi
i )
and h(U(xyi
i )) = yi, âˆ€1 â‰¤i â‰¤k. The relaxed U-robust shattering dimension rdimU(H) is defined as
the largest k for which there exist k points that are relaxed U-robustly shattered by H.
Define the set of intermediate perturbations as follows:
Definition A.8 (Intermediate Perturbations). Given x and z and perturbations U1 and U2, the set of
possible intermediate perturbations between x and z is
ipU1,U2(x, z) =
({x}
if x = z
U1(x) âˆ©Uâˆ’1
2 (z)
otherwise
Theorem A.9. For any n âˆˆN, Î´ > 0, class H, perturbation set U, and distribution D over X Ã— Y
satisfying OPTUâˆ’1U = 0:
Pr
(x,y)âˆ¼Dn
(Ëœx,Ëœy)âˆ¼Dn
" âˆ€z âˆˆU3(x), âˆ€z0 âˆˆipU,U2(x, z), âˆ€Ëœz âˆˆU3(Ëœx), âˆ€Ëœz0 âˆˆipU,U2(Ëœx, Ëœz),
âˆ€Ë†h âˆˆFU

âˆ†U
H(z0, y, Ëœz0)

: errrej(Ë†h; x, y, Ëœx, Ëœz, Ëœy) â‰¤Ïµ
#
â‰¥1 âˆ’Î´
where Ïµ =
rdimUâˆ’1(H) log(2n)+log(1/Î´)
n
â‰¤VC(H) log(2n)+log(1/Î´)
n
.
Proof. We adapt the strategy of Theorem 5 of [Tra22] for the rejection scenario.
By setting z = z0, Ëœz = Ëœz0 and applying Theorem 1 of [MHS21], we obtain the following
Pr
(x,y)âˆ¼Dn
(Ëœx,Ëœy)âˆ¼Dn
h
âˆ€z0 âˆˆU(x), âˆ€Ëœz0 âˆˆU(Ëœx), âˆ€h âˆˆâˆ†U
H(z0, y, Ëœz0) : errËœz0,Ëœy(h) â‰¤Ïµ
i
â‰¥1 âˆ’Î´
(39)
as OPTUâˆ’1(U) = 0.
Suppose (x, y), (Ëœx, Ëœy) âˆ¼Dn. Now, let z âˆˆU3(x), Ëœz âˆˆU3(Ëœx) and take some z0 âˆˆipU,U2(x, z), Ëœz0 âˆˆ
ipU,U2(Ëœx, Ëœz), both of which are necessarily nonempty as U3 = U2U, and Ë†h âˆˆFU

âˆ†U
H(z0, y, Ëœz0)

.
Write Ë†h = FU(h) for some h âˆˆâˆ†U
H(z0, y, Ëœz0).
From Equation (39) (replacing z with z0 and Ëœz with Ëœz0), it is enough to show that
errrej(Ë†h; x, y, Ëœx, Ëœz, Ëœy) â‰¤errËœz0,Ëœy(h).
Suppose that Ë†h incurs an error under rejection at point Ëœzi; it is enough to show that h incurs an
error at Ëœz0i. Furthermore, note that because h âˆˆâˆ†U
H(z0, y, Ëœz0), we have that h(Uâˆ’1(Ëœz0i)) = {h(Ëœz0i)} as
Ëœz0i âˆˆUâˆ’1(Ëœz0i). Write h(Ëœz0i) = Ë†yi.
We have one of the following:
1. Ë†h(Ëœzi) , Ëœyi and Ëœzi = Ëœxi
2. Ë†h(Ëœzi) < {Ëœyi, âŠ¥} and Ëœzi , Ëœxi
In the first case, we must have Ëœz0i = Ëœxi as well as Ëœz0i is an intermediate perturbation between Ëœxi and Ëœzi,
so, as h(Uâˆ’1(Ëœzi)) = h(Uâˆ’1(Ëœz0i)) = Ë†yi, Ë†h does not reject Ëœz0i and Ë†h(Ëœz0i) = Ë†yi. Hence, h(Ëœz0i) = Ë†yi as well
so, as Ë†h makes an error at Ëœzi, Ë†yi , y and so h makes an error at Ëœz0i.
In the second case, if h(Uâˆ’1(Ëœzi)) , {h(Ëœzi)}, then Ë†h would reject Ëœzi and hence not incur an error.
So h(Uâˆ’1(Ëœzi)) = {h(Ëœzi)} and so Ë†h(Ëœzi) = h(Ëœzi). Since Ëœz0i âˆˆU(Ëœxi) âˆ©Uâˆ’2(Ëœzi), there exists some
Ëœzâ€²
0iâˆˆU(Ëœz0i) âˆ©Uâˆ’1(Ëœzi) and so, h(Ëœz0i) = h(Ëœzâ€²
0i) = h(Ëœzi) = Ë†h(Ëœzi) = Ë†yi, so h incurs an error at Ëœz0i.
In either case, we have that h makes an error at Ëœz0i, showing the result.
â–¡
18

Sample Complexity
Given Ïµ and Î´, we need
rdimUâˆ’1(H) log(2n) + log(1/Î´)
n
â‰¤Ïµ
for the result to hold.
Now, noting that log(2n) = 1 + log n â‰¤1 + âˆšn for n â‰¥16; hence we need to solve for the n such that
rdimUâˆ’1(H)(1 + âˆšn) + log(1/Î´)
n
= Ïµ
or, equivalently
rdimUâˆ’1(H) + log( 1
Î´) + âˆšn
n
= Ïµ
or
âˆšn = nÏµ âˆ’rdimUâˆ’1(H) âˆ’log(1
Î´)
or
n = n2Ïµ2 âˆ’2Ïµ
 
rdimUâˆ’1(H) + log(1
Î´)
!
n +
 
rdimUâˆ’1(H) + log(1
Î´)
!2
or
n2Ïµ2 âˆ’
 
2Ïµ
 
rdimUâˆ’1(H) + log(1
Î´)
!
+ 1
!
n +
 
rdimUâˆ’1(H) + log(1
Î´)
!2
= 0.
Solving, the result holds if
n â‰¥
2Ïµ

rdimUâˆ’1(H) + log( 1
Î´)

+ 1 +
q
(2Ïµ

rdimUâˆ’1(H) + log( 1
Î´)

+ 1)2 âˆ’4

rdimUâˆ’1(H) + log( 1
Î´)
2 Ïµ2
2Ïµ2
= O
ï£«ï£¬ï£¬ï£¬ï£¬ï£¬ï£¬ï£¬ï£¬ï£¬ï£¬ï£­
rdimUâˆ’1(H) + log( 1
Î´)
Ïµ
+
q
rdimUâˆ’1(H) + log( 1
Î´)
Ïµ
3
2
ï£¶ï£·ï£·ï£·ï£·ï£·ï£·ï£·ï£·ï£·ï£·ï£¸
and, similarly, using
rdimUâˆ’1(H) log(2n) + log(1/Î´)
n
â‰¤VC(H) log(2n) + log(1/Î´)
n
we have the result if
n = O
ï£«ï£¬ï£¬ï£¬ï£¬ï£¬ï£¬ï£¬ï£¬ï£¬ï£¬ï£­
VC(H) + log( 1
Î´)
Ïµ
+
q
VC(H) + log( 1
Î´)
Ïµ
3
2
ï£¶ï£·ï£·ï£·ï£·ï£·ï£·ï£·ï£·ï£·ï£·ï£¸
Remark:
If OPTUâˆ’1U = 0, we can guarantee the existence of an Ë†h which satisfies our conditions,
but we canâ€™t guarantee that we will find it, as we cannot find âˆ†U
H(z0, y, Ëœz0) without z0 and Ëœz0. We can,
however, construct that an algorithm which, if it returns a model, always returns on which meets the
conditions.
Simplified Result
To obtain a bound which does not involve an intermediate perturbation step, we
may let
âˆ†U
rej,H(z, y, Ëœz) :=
\
zâ€²âˆˆUâˆ’2(z), Ëœzâ€²âˆˆUâˆ’2(Ëœz)
âˆ†U
H(zâ€², y, Ëœzâ€²)
Note that for common classes of perturbations, we can simplify the definition of âˆ†rej. Note that the
conditions of the theorem hold for perturbations defined via Ïµ-balls in a metric.
Lemma A.10. In the realizable case, if U = Uâˆ’1,
âˆ†U
rej,H(z, y, Ëœz) = âˆ†U3
H (z, y, Ëœz)
19

Proof. Suppose h âˆˆâˆ†U
rej,H(z, y, Ëœz). Then by the definitions of âˆ†rej and âˆ†, for any zâ€² âˆˆUâˆ’2(z), Ëœzâ€² âˆˆ
Uâˆ’2(Ëœz), we have that, for any x âˆˆUâˆ’1(zâ€²) and Ëœx âˆˆUâˆ’1(Ëœzâ€²), h(xi) = h(zâ€²
i) and h(Ëœxi) = h(Ëœzâ€²
i). Now, as
there exists some zâ€²â€² âˆˆU(zâ€²) âˆ©Uâˆ’1(bz) and h(x) = h(zâ€²) = h(zâ€²â€²) = h(z) by an argument similar
to that in Theorem A.9 and similarly for Ëœx and Ëœz, we have that for any x âˆˆUâˆ’3(z) and Ëœx âˆˆUâˆ’3(Ëœz),
h(xi) = h(zi) and h(Ëœxi) = h(Ëœzi), and so
âˆ†U
rej,H(z, y, Ëœz) âŠ†âˆ†U3
H (z, y, Ëœz)
Now, if h âˆˆâˆ†U3
H (z, y, Ëœz), we have that, for any x âˆˆUâˆ’3(z) and Ëœx âˆˆUâˆ’3(Ëœz), h(xi) = h(zi) and h(Ëœxi) =
h(Ëœzi). Now, suppose zâ€² âˆˆUâˆ’2(z), Ëœzâ€² âˆˆUâˆ’2(Ëœz). Since x âˆˆU(x) for all x, zâ€² âˆˆUâˆ’3(z), Ëœzâ€² âˆˆUâˆ’3(Ëœz)
as well. Hence, h(zâ€²
i) = h(zi) and h(Ëœzâ€²
i) = h(Ëœzi). Now, if x âˆˆUâˆ’1(zâ€²) and Ëœx âˆˆUâˆ’1(Ëœzâ€²), we have
x âˆˆUâˆ’3(z) and Ëœx âˆˆUâˆ’3(Ëœz) and so h(xi) = h(zi) and h(Ëœxi) = h(Ëœzi). But then h(xi) = h(zâ€²
i) and
h(Ëœxi) = h(Ëœzâ€²
i). Hence, we have that
âˆ†U3
H (z, y, Ëœz) âŠ†âˆ†U
rej,H(z, y, Ëœz)
and the result follows.
â–¡
Now, by the above and from Theorem A.9 we may immediately derive Theorem 4.1 by noting
that if U = Uâˆ’1, Uâˆ’1U = U2, and if Ë†h âˆˆFU(âˆ†U
H(z, y, Ëœz)) = FU1/3(âˆ†U1/3
rej,H(z, y, Ëœz)) then we have
Ë†h âˆˆFU1/3

âˆ†U1/3
H
(z0, y, Ëœz0)

for some z0 âˆˆipU1/3,U2/3(x, z) and Ëœz0 âˆˆipU1/3,U2/3(Ëœx, Ëœz).
A.4
Transduction+Rejection: Agnostic Case
Note that, if U can be decomposed into a form U = (U1/3)3 where U1/3 = Uâˆ’1/3 (as with standard
perturbations in lp), we obtain a bound which depends on OPTU2/3 rather than OPTU2, enabling, for Ë†h
satisfying the conditions, much stronger guarantees if OPTU2/3 << OPTU2. Note that as âˆ€x x âˆˆU(x),
âˆ€x U2/3(x) âŠ†U2(x), and so OPTU2/3 â‰¤OPTU2.
Theorem A.11. For any n âˆˆN, Î´ > 0, class H, perturbation set U, and distribution D over X Ã— Y:
Pr
(x,y)âˆ¼Dn
(Ëœx,Ëœy)âˆ¼Dn
" âˆ€z âˆˆU3(x), âˆ€z0 âˆˆipU,U2(x, z), âˆ€Ëœz âˆˆU3(Ëœx), âˆ€Ëœz0 âˆˆipU,U2(Ëœx, Ëœz),
âˆ€Ë†h âˆˆFU

âˆ†U
H(z0, y, Ëœz0)

: errrej(Ë†h; x, y, Ëœx, Ëœz, Ëœy) â‰¤Ïµ
#
â‰¥1 âˆ’Î´
where
Ïµ = min
ï£±ï£´ï£´ï£²ï£´ï£´ï£³2 OPTUâˆ’1U +O
ï£«ï£¬ï£¬ï£¬ï£¬ï£¬ï£­
r
VC(H) + log(1/Î´)
n
ï£¶ï£·ï£·ï£·ï£·ï£·ï£¸, 3OPTUâˆ’1U + O
ï£«ï£¬ï£¬ï£¬ï£¬ï£¬ï£­
r
rdim U(H) ln(2n) + ln(1/Î´)
n
ï£¶ï£·ï£·ï£·ï£·ï£·ï£¸
ï£¼ï£´ï£´ï£½ï£´ï£´ï£¾.
Proof. Suppose (x, y), (Ëœx, Ëœy) âˆ¼Dn.
Now, let z âˆˆU3(x), Ëœz âˆˆU3(Ëœx) and take some z0 âˆˆ
ipU,U2(x, z), Ëœz0 âˆˆipU,U2(Ëœx, Ëœz), both of which are necessarily nonempty, and Ë†h âˆˆFU

âˆ†U
H(z0, y, Ëœz0)

.
Write Ë†h = FU(h) for some h âˆˆâˆ†U
H(z0, y, Ëœz0).
We will begin as in Theorem A.9. As before, there are two cases in which Ë†h can incur an error at Ëœzi:
1. Ë†h(Ëœzi) , Ëœyi and Ëœzi = Ëœxi
2. Ë†h(Ëœzi) < {Ëœyi, âŠ¥} and Ëœzi , Ëœxi
Now, if Ëœzi = Ëœxi, an error occurs if Ë†h rejects Ëœzi or if h robustly predicts some Ë†yi , Ëœyi; hence an error
occurs if h is not Uâˆ’1-robust at Ëœz0i or if h(Ëœz0i) , Ëœyi.
Otherwise, h must be Uâˆ’1-robust at Ëœzi, as, otherwise, Ë†h would reject Ëœzi. Hence, as there exists some
Ëœzâ€²
0i âˆˆU(Ëœz0i) âˆ©Uâˆ’1(Ëœzi), if h is U-robust at Ëœz0i, we must have h(Ëœzi) = h(Ëœz0i), and so, if Ë†h makes an error,
h is not Uâˆ’1-robust at Ëœz0i or h(Ëœz0i) , Ëœyi.
20

Now, in both cases, errors only occur if h is not Uâˆ’1-robust at Ëœz0i or h(Ëœz0i) , Ëœyi. As Ëœxi âˆˆUâˆ’1(Ëœz0i), we
have, equivalently, that an error occurs if h is not Uâˆ’1-robust at Ëœz0i or h(Ëœxi) , Ëœyi.
Hence,
errrej(Ë†h; x, y, Ëœx, Ëœz, Ëœy) â‰¤errrej(h; Ëœx, Ëœy) + RUâˆ’1(h; Ëœz0)
Now, the right hand is exactly what is bounded in Theorem 2 of [MHS21]; as we have h âˆˆ
âˆ†U
H(z0, y, Ëœz0), we have
errrej(Ë†h; x, y, Ëœx, Ëœz, Ëœy) â‰¤errrej(h; Ëœx, Ëœy) + RUâˆ’1(h; Ëœz0) â‰¤Ïµ
where
Ïµ = min
ï£±ï£´ï£´ï£²ï£´ï£´ï£³2 OPTUâˆ’1U +O
ï£«ï£¬ï£¬ï£¬ï£¬ï£¬ï£­
r
VC(H) + log(1/Î´)
n
ï£¶ï£·ï£·ï£·ï£·ï£·ï£¸, 3OPTUâˆ’1U + O
ï£«ï£¬ï£¬ï£¬ï£¬ï£¬ï£­
r
rdim U(H) ln(2n) + ln(1/Î´)
n
ï£¶ï£·ï£·ï£·ï£·ï£·ï£¸
ï£¼ï£´ï£´ï£½ï£´ï£´ï£¾
with probability â‰¥1 âˆ’Î´ by its proof.
â–¡
As in the realizable case, we can immediately derive the following corollary. However, we cannot
simplify the definition of âˆ†rej as before; see Lemma A.13.
Corollary A.12. For any n âˆˆN, Î´ > 0, class H, perturbation set U where U = Uâˆ’1, and distribution
D over X Ã— Y:
Pr
(x,y)âˆ¼Dn
(Ëœx,Ëœy)âˆ¼Dn
ï£®ï£¯ï£¯ï£¯ï£¯ï£°âˆ€z âˆˆU3(x), âˆ€Ëœz âˆˆU3(Ëœx), âˆ€Ë†h âˆˆFU

âˆ†U
rej,H(z, y, Ëœz)

:
errrej(Ë†h; x, y, Ëœx, Ëœz, Ëœy) â‰¤Ïµ
ï£¹ï£ºï£ºï£ºï£ºï£»â‰¥1 âˆ’Î´
where
Ïµ = min
ï£±ï£´ï£´ï£²ï£´ï£´ï£³2 OPTUâˆ’1U +O
ï£«ï£¬ï£¬ï£¬ï£¬ï£¬ï£­
r
VC(H) + log(1/Î´)
n
ï£¶ï£·ï£·ï£·ï£·ï£·ï£¸, 3OPTUâˆ’1U + O
ï£«ï£¬ï£¬ï£¬ï£¬ï£¬ï£­
r
rdim U(H) ln(2n) + ln(1/Î´)
n
ï£¶ï£·ï£·ï£·ï£·ï£·ï£¸
ï£¼ï£´ï£´ï£½ï£´ï£´ï£¾.
Lemma A.13. In the agnostic case, we have that if U = Uâˆ’1,
âˆ†U
rej,H(z, y, Ëœz) âŠ†âˆ†U3
H (z, y, Ëœz)
Proof. By the definition of R, we have
RUâˆ’3(h; Ëœz) = 1
n
n
X
i=1
1
n
âˆƒËœxi âˆˆUâˆ’3 (Ëœzi) : h (Ëœxi) , h (Ëœzi)
o
= 1
n
n
X
i=1
1
n
âˆƒËœzâ€²
i âˆˆUâˆ’2 (Ëœzi) âˆƒËœxi âˆˆUâˆ’1 
Ëœzâ€²
i

: h (Ëœxi) , h (Ëœzi)
o
=
max
Ëœzâ€²
iâˆˆUâˆ’2(Ëœzi)
1
n
n
X
i=1
1
n
âˆƒËœxi âˆˆUâˆ’1  Ëœzâ€²
i
 : h (Ëœxi) , h (Ëœzi)
o
=
max
Ëœzâ€²
iâˆˆUâˆ’2(Ëœzi)RUâˆ’1(h; Ëœzâ€²)
where the last equality holds as x âˆˆU(x) for all x and as U = Uâˆ’1, which together show that if for
some Ëœzi and Ëœzâ€²
i âˆˆUâˆ’2(Ëœzi) we have that h(Ëœzâ€²
i) , h(Ëœzi), that either there exists some Ëœzâ€²â€²
i âˆˆU = Uâˆ’1(Ëœzâ€²
i)
such that h(Ëœzâ€²â€²
i ) , h(Ëœzâ€²
i) or there exists some Ëœzâ€²â€²
i âˆˆU = Uâˆ’1(Ëœzi) such that h(Ëœzâ€²â€²
i ) , h(Ëœzi) (as before, note
that Ëœzi = U(Ëœzâ€²â€²
i ) for some Ëœzâ€²â€²
i âˆˆU(Ëœzâ€²
i) by the definition of U3); the reverse is similar.
We can derive a result for RUâˆ’3(h; z, y) similarly.
Suppose h âˆˆâˆ†U
rej,H(z, y, Ëœz).
Then, h minimizes max {RUâˆ’1(h; zâ€², y), RUâˆ’1(h; Ëœzâ€²)} for all zâ€² âˆˆ
Uâˆ’2(z), Ëœzâ€² âˆˆUâˆ’2(Ëœz), so by the above, h must also minimize
max
zâ€²âˆˆUâˆ’2(z),Ëœzâ€²âˆˆUâˆ’2(Ëœz) max RUâˆ’1(h; zâ€², y), RUâˆ’1(h; Ëœzâ€²)	
= max
(
max
zâ€²âˆˆUâˆ’2(z)RUâˆ’1(h; zâ€², y),
max
Ëœzâ€²âˆˆUâˆ’2(Ëœz)RUâˆ’1(h; Ëœzâ€²)
)
= max {RUâˆ’3(h; Ëœz), RUâˆ’3(h; z, y)}
21

and so h âˆˆâˆ†U3
H (z, y, Ëœz).
However, minimizing
max
zâ€²âˆˆUâˆ’2(z),Ëœzâ€²âˆˆUâˆ’2(Ëœz) max RUâˆ’1(h; zâ€², y), RUâˆ’1(h; Ëœzâ€²)	
does not necessarily imply that h minimizes max {RUâˆ’1(h; zâ€², y), RUâˆ’1(h; Ëœzâ€²)} for all zâ€² âˆˆUâˆ’2(z), Ëœzâ€² âˆˆ
Uâˆ’2(Ëœz), so the reverse may not hold.
â–¡
A.5
Extension to Unbalanced Training and Test Data
We provide a sketch of a proof that allows extending Theorem 1 of [MHS21] to unbalanced training
and test sets; however, for simplicity, we will work with the original form. The assumptions are the
same, except that we have n training points and m test points.
The proof is exactly as before up to the "Finite robust labelings" portion (which points are and are
not labelled donâ€™t matter up to then and the symmetry arguments still apply). The basic idea of
determining the probability of zero loss on the training and test sets and error > Ïµ on the test examples
with permutation still applies. Let EÏƒ,x be the event that there exists a labelling Ë†h(xÏƒ(1:n+m)) in the
allowable set where this occurs.
We have
Pr
Ïƒ
EÏƒ,x
 â‰¤Pr
Ïƒ
h
âˆƒË†h âˆˆÎ U
H(x1, . . . , xn+m) : errxÏƒ(1:n),yÏƒ(1:n)(Ë†h) = 0 âˆ§errxÏƒ(n:n+m),yÏƒ(n:n+m)(Ë†h) > Ïµ
i
and, as in [MHS21], note the probability of choosing such a perturbation Ïƒ for a fixed Ë†h is at most

m
n + m
s
â‰¤

m
n + m
âŒˆÏµmâŒ‰
=
n + m
m
âˆ’âŒˆÏµmâŒ‰
â‰¤
n + m
m
âŒˆâˆ’ÏµmâŒ‰
if we assume the number of total errors s â‰¥âŒˆÏµmâŒ‰without loss of generality (otherwise, err > Ïµ would
be impossible).
Hence, by a union bound,
Pr
Ïƒ
EÏƒ,x
 â‰¤
Î U
H(x1, . . . , xn+m)

n + m
m
âŒˆâˆ’ÏµmâŒ‰
and so
Pr
Ïƒ
EÏƒ,x
 â‰¤(n + m)rdimUâˆ’1(H)n + m
m
âŒˆâˆ’ÏµmâŒ‰
by Sauerâ€™s Lemma (in the form of Lemma 3 of [MHS21]).
Now, we bound the probability by Î´, we need
(n + m)rdimUâˆ’1(H)n + m
m
âŒˆâˆ’ÏµmâŒ‰
â‰¤Î´
which, solving, gives us
Ïµ â‰¥
rdimUâˆ’1(H) log n+m
m (n + m) + log n+m
m
1
Î´
m
=
rdimUâˆ’1(H) log(n + m) + log 1
Î´
m log

1 + m
n

Which reduces to the original result if n = m (note that the logarithms are base-2).
Corollary
If we fix n + m, H, and Î´, the guarantee is strongest (i.e. we minimize Ïµ) when n = m.
To see this, consider the denominator. Write Î± = m
n . Then, we wish to maximize nÎ± log(1 + Î±) (or
equivalently f(Î±) = Î± log(1 + Î±) subject to Î± â‰¥0. Now, note that f â€²(Î±) = log(1 + Î±) âˆ’1 = 0 when
Î± = 1, i.e. when m = n.
Also, we can see from the result above, that if we fix m and Î´, then the minimum value of Ïµ tends
towards âˆžas n â†’âˆž, so there does not necessarily exist a labelled training set sampled from D
which provides a guarantee with high probability of arbitrarily low error on a fixed test set.
22

B
Experimental Details
B.1
Computing Infrastructure
We used a SLURM cluster with A100 GPUs to run our experiments.
B.2
Baseline Details
The baselines are trained with standard adversarial training [GSS14] [MMS+18]. Attacks against AT
without rejection use standard PGD with a cross-entropy objective, while attacks against AT with
rejection use PGD targeting LREJ as described in algorithm 3. In all cases, the parameters for PGD in
training are the same as those used in TLDRâ€™s training process for the same dataset.
B.3
Defense
In our implementation, we begin to incorporate the transductive term in our objective (see Equa-
tion (1)) after initially training the model with the inductive loss term only; this allows learning a
better baseline before we begin to enforce robustness about the test points. In our experiments, we
use the transductive loss in the final half of the training epochs, and put 85% of the weight on the
inductive term afterwards.
B.4
Adaptive Attack
Solving for the perturbation Ëœx by iteratively optimizing LREJ poses several difficulties.
First, the rejection-avoidance term
Ëœx âˆ’arg max||xâ€²âˆ’Ëœx||â‰¤Ïµ LDB,h(xâ€²)
 is not differentiable with respect to
Ëœx. While it is possible to approximate the derivative with the derivative of a proxy (e.g. differentiating
though some fixed number of PGD steps, necessitating second-order optimization), this is extremely
expensive and does not improve results in our experiments (see below).
Intuitively, we might see that this would be the case: if the decision boundary is smooth, we might
expect the maximizers in U(x + âˆ†) and U(x) to be the same for small âˆ†unless xâ€² is near the border
of U(x) given that U(x + âˆ†) â‰ˆU(x). In this case, approximating xâ€² as constant with respect to x is
reasonable.
In addition, note that if h(x) = y, the adversary must find a Ëœx where h(Ëœx) , y which is not rejected: if
maximizing LREJ with PGD, the rejection-avoidance term penalizes moving Ëœx towards the decision
boundary. As this is necessary to find a valid attack (when h(Ëœx) = y at initialization), we adjust Î»
adaptively during optimization by setting it to zero when h(Ëœx) = y.
B.5
Transductive Attack Details
We present two rejection-aware transductive attacks: a stronger but more computationally intensive
rejection-aware GMSA (Algorithm 1) and a weaker but faster rejection-aware transfer attack which
takes the transductive robust rejection risk into account (Algorithm 2).
Finally, note the attack with LREJ, without GMSA, is effective against selective classifiers based on
the transformation F (and via TramÃ¨râ€™s equivalency, selective classifiers in general). So we summarize
this attack on a fixed model in Algorithm 3.
B.6
Rejectron Experiments
Goldwasser et al.â€™s implementation of Rejectron [GKKM20] trains a classifier (call it hc) on the
training set and a discriminator (hd) to distinguish between the (clean) training and (potentially-
perturbed) test data. Samples are rejected if the discriminator classifies them as test data; otherwise,
the classifierâ€™s prediction is returned. Our adaptive attack is then very simple: we follow the approach
of Algorithm 1 but with a loss function LDISC which targets the defense.
Given a sample (x, y), the attackerâ€™s goal is to flip the label, and, simultaneously, to avoid rejection;
hence, we maximize the following loss:
LDISC(x, y) = LCE(hs
c(x), y) + Î»LCE(hs
d(x), 1)
23

Algorithm 1 Rejection-Aware GMSA
Require: A clean training set T, a clean test set E, a transductive learning algorithm for classifiers A, an
adversarial budget of Ïµ, mode either MIN or AVG, a radius used for rejection Ïµdefense, and a maximum
number of iterations N â‰¥1. E|X refers to the projection on the feature space for E.
1: Search for a perturbation of the test set which fools the model space induced by (T, U(E|X)).
2: Eâ€² = E
3: Ë†E = E
4: errmax = âˆ’inf
5: for i=0,...,N-1 do
6:
Train a transductive model on the perturbed data.
7:
h(i) = A(T, Eâ€²|X)
8:
err =
1
|Eâ€²|
|Eâ€²|
X
i=1
1
n
F(h(i)) (Ëœxi) < { Ëœyi} âˆ§Ëœxi = xi

âˆ¨

F(h(i)) (Ëœxi) < { Ëœyi, âŠ¥} âˆ§Ëœxi , xi
o
{The Ëœxi and the xi are the ith datapoints of Eâ€² and E, repectively; yi is the true label.}
9:
if errmax < err then
10:
Ë†E = Eâ€²
11:
end if
12:
for j = 1, . . . , |E| do
13:
if mode = MIN then
14:
Ëœx j = arg max
âˆ¥Ëœxâˆ’x jâˆ¥â‰¤Ïµ min
1â‰¤kâ‰¤i LREJh(k)(Ëœx, y j)
15:
else
16:
Ëœxj = arg max
âˆ¥Ëœxâˆ’xjâˆ¥â‰¤Ïµ
1
i
iX
k=1
LREJh(k)(Ëœx, y j)
17:
end if
{Select whether to perturb by comparing success rates against past models for the clean and perturbed
samples.}
18:
errclean = 1
i
X
0â‰¤kâ‰¤i
1
h
F

h(k)
(xj) , y j
i
19:
errperturbed = 1
i
X
0â‰¤kâ‰¤i
1
h
F

h(k)
(Ëœxj) < {y j, âŠ¥}
i
{Do not perturb if the perturbation reduces robust rejection accuracy less on average than leaving the
points unchanged.}
20:
if errperturbed < errclean then
21:
Ëœxj = xj
22:
end if
23:
Eâ€²
j = Ëœxj, yi
24:
end for
25: end for
26: Return: Ë†E
where class 1 for hd corresponds to test data, signalling rejection, and where hs returns the softmax
activations of h. Maximizing LDISC then minimizes the confidence in the true label and the probability
of rejection.
Figures 2 and 3 show our adaptive attackâ€™s performance on MNIST and CIFAR-10. Ï„ is a key
hyperparameter of Rejectron, which determines the confidence needed by hd to reject a sample; to
evaluate Rejectron fairly, we report the results on best-performing value of Ï„, based on (transductive)
robust rejection accuracy; see Table 3. On CIFAR-10, performance is near-zero and rejection rate
is near 100% for small values of Ï„. The best-performing value of Ï„ is 1 (effectively eliminating the
possibility of rejection), leading to a rejection rate of 0; this behavior on CIFAR-10 illustrates the
algorithmâ€™s struggles with the practical high-complexity deep learning setting.
24

Algorithm 2 Transductive Rejection-Aware Transfer
Require: A model h, a clean labelled test point (x, y), an adversarial budget of Ïµ, and a radius used for rejection
Ïµdefense.
{Search for a perturbation Ëœx of x for which h predicts Ë†y , y robustly.}
1:
Ëœx = arg max
âˆ¥Ëœxâˆ’xâˆ¥â‰¤Ïµ
"
LCE(hs(Ëœx), y) + Î»
Ëœx âˆ’arg
max
âˆ¥xâ€²âˆ’Ëœxâˆ¥â‰¤Ïµdefense LDB,h(xâ€²)

#
where LCE is the cross-entropy loss, hs returns the softmax activations of h and where
LDB,h(x) = rank2hs(x) âˆ’max hs(x).
{If the attack did not succeed against h (in other words, if h does not robustly predict Ë†y , y), check whether
to leave x unperturbed.}
2:
xâ€² = arg
max
âˆ¥xâ€²âˆ’Ëœxâˆ¥â‰¤Ïµdefense LCE(hs(xâ€²), h(Ëœx))
3: if h(xâ€²) , h(Ëœx) âˆ¨h(Ëœx) = y then
4:
Leave x unperturbed if F(h) rejects it, or if h(x) , y.
5:
xâ€²â€² = arg
max
âˆ¥xâ€²â€²âˆ’xâˆ¥â‰¤Ïµdefense LCE(hs(xâ€²â€²), h(x))
6:
if h(x) , y âˆ¨h(xâ€²â€²) , h(x) then
7:
Ëœx = x
8:
end if
9: end if
10: Return: Ëœx
Algorithm 3 Inductive Rejection-Aware Attack
Require: A model h, and a clean labelled test point (x, y), an adversarial budget of Ïµ, and a radius used for
rejection Ïµdefense.
1: Search for a perturbation Ëœx of x for which h predicts Ë†y , y robustly.
Ëœx = arg max
âˆ¥Ëœxâˆ’xâˆ¥â‰¤Ïµ
"
LCE(hs(Ëœx), y) + Î»
Ëœx âˆ’arg
max
âˆ¥xâ€²âˆ’Ëœxâˆ¥â‰¤Ïµdefense LDB,h(xâ€²)

#
where LCE is the cross-entropy loss, hs returns the softmax activations of h and where
LDB,h(xâ€²) = rank2hs(xâ€²) âˆ’max hs(xâ€²)
2: Return: Ëœx
C
Ablation Studies
C.1
Warm Start in TLDR
Warm start (epochs)
Rejection Rate
Robust Rejection Accuracy
0
0.813
0.153
500
0.531
0.177
1000
0.830
0.171
Here we perform experiments showing that in training TLDR, it is best to first trains a baseline model
without transductive regularization Ltest in the early stage (warm start) and then add transductive
regularization for later training.
We generate the data with 100 Gaussians (one per class) equally spaced in lâˆžwith a separation of
3 units between means. The adversarial budget is 2 units, and we ensure that the data is sparse by
generating 10 samples per class. The models are 10 layer feedforward networks with skip connections.
The synthetic models are trained for 1000 epochs total; we see the best performance when the model
has transductive regularization but is allowed to learn an initial baseline model before transductive
regularization is used in training. Doing so reduces the risk of the regularization term harming
performance.
25

Figure 2: Effects of Ï„ on performance of Rejec-
tron on MNIST with attacker GMSA (LDISC).
Figure 3: Effects of Ï„ on performance of Rejec-
tron on CIFAR-10 with attacker GMSA (LDISC).
TLDR Components
Attacker
MNIST
CIFAR-10
Rejection
Transductive Regularization
pREJ
Robust accuracy
pREJ
Robust accuracy
âœ“
âœ“
GMSAAVG (LREJ)
0.796
0.968
0.195
0.744
âœ“
âœ“
GMSAMIN (LREJ)
0.588
0.967
0.208
0.739
âœ“
Ã—
GMSAAVG (LREJ)
0.646
0.975
0.179
0.725
âœ“
Ã—
GMSAMIN (LREJ)
0.202
0.980
0.182
0.733
Ã—
âœ“
GMSAAVG (LCE)
â€“
0.900
â€“
0.516
Ã—
âœ“
GMSAMIN (LCE)
â€“
0.914
â€“
0.601
Ã—
Ã—
GMSAAVG (LCE)
â€“
0.935
â€“
0.516
Ã—
Ã—
GMSAMIN (LCE)
â€“
0.942
â€“
0.556
Table 7: Full ablation results of TLDR.
C.2
GMSA Method
We present extended results of our defense ablation and compare the results of GMSAAVG, which
optimizes the average loss of past iterations, and GMSAMIN, which optimizes the worst-case loss.
See [CWG+22]. We can see that while the two perform about the same on the full TLDR defense
(GMSAMIN performs slightly better), GMSAAVG is much stronger for models not incorporating both
components.
C.3
Rejection Radius
0.2
0.4
0.6
0.8
1.0
Ïµdefense/Ïµ
0.0
0.2
0.4
0.6
0.8
1.0
Robust Accuracy
0.2
0.4
0.6
0.8
1.0
Ïµdefense/Ïµ
0.0
0.2
0.4
0.6
0.8
1.0
Adversarial Rejection Rate
0.2
0.4
0.6
0.8
1.0
Ïµdefense/Ïµ
0.0
0.2
0.4
0.6
0.8
1.0
Clean Rejection Rate
Figure 4: Effects of rejection radius Ïµdefense on MNIST (inductive) with attacker PGD (LREJ).
The rejection radius Ïµdefense is an important hyper-parameter for TLDR; however, the modelâ€™s perfor-
mance is not very sensitive to it. Figure 4 shows the trend of robust accuracy, the rejection rate on
adversarial test data, and the rejection rate on clean test data, for the inductive classifier on MNIST;
Figure 5 shows those for TLDR. The robust accuracy remains stable. The theoretical analysis suggests
setting the radius to Ïµ/3 where Ïµ is the adversarial budget. Given TLDRâ€™s low sensitivity to the
parameter, we use Ïµ/4 for consistency as the inductive case performs best with that setting. The
rejection rate on the adversarial test data rises rapidly with the rejection radius (reaching 0.949 for
TLDR for Ïµdefense = Ïµ), but the rejection rate on clean data increases much more slowly (0.108 when
Ïµdefense = Ïµ). So among all rejected inputs only a few are clean inputs, leading to low errors as desired.
26

0.2
0.4
0.6
0.8
1.0
Ïµdefense/Ïµ
0.0
0.2
0.4
0.6
0.8
1.0
Robust Accuracy
0.2
0.4
0.6
0.8
1.0
Ïµdefense/Ïµ
0.0
0.2
0.4
0.6
0.8
1.0
Adversarial Rejection Rate
0.2
0.4
0.6
0.8
1.0
Ïµdefense/Ïµ
0.0
0.2
0.4
0.6
0.8
1.0
Clean Rejection Rate
Figure 5: Effects of rejection radius Ïµdefense on MNIST (TLDR) with attacker GMSA (LREJ).
Figure 6: Robustness scaling with adversarial budget Ïµ on MNIST
The rejection rate on clean inputs is presented for the transductive case in order to illustrate the
difference in effects on clean and perturbed data, but, as the adversary may select to perturb, some
clean points were not in the training set, and, hence, the clean rejection rates should not be considered
reliable. The rejection rates rise with the rejection radius: adversarial rejection rates increase rapidly
as the rejection radius increases, while clean rejection rates increase only slowly. In all cases, far
more perturbed samples are rejected than clean samples.
C.4
Ablation on Attacks: Attack Radius
The theory suggests that incorporating rejection can allow a transductive learner to tolerate perturba-
tions twice as large; we investigate how transduction and rejection affects the robustness as Ïµ grows
(models are adversarially trained with the corresponding Ïµ and the selective classifiers use a rejection
radius of Ïµ/2). The results are shown for the natural choice of adversary, as in the experiment section
(e.g. GMSA with LREJ for the transduction+rejection). For selective classifiers, the rejection rate
scaling is shown.
We see that the combination of rejection and transduction does indeed maintain high accuracy for
larger Ïµ; at Ïµ = 0.6, it has 96.2% of the robust accuracy that transduction alone had for Ïµ = 0.3. This
aligns with the theory, given the increased constant factors of OPTU2 in Corollary A.12 compared to
the results for classifiers in [MHS21].
Note also the behavior of the inductive classifier: accuracy improves past Ïµ = 0.6. To see why,
note that a model adversarially trained for Ïµ â‰¥1 will return near-uniform predictions for all classes
(resulting in a robust accuracy of approximately 10%, as seen), making finding adversarial examples
slightly more difficult than for smaller Ïµ where this does not occur. The decline in rejection rate for
very large Ïµ is a similar phenomenon.
C.5
Weighting of LREJ
We examine the effect of the hyperparameter Î»â€² between the cross-entropy and rejection-avoidance
terms in LREJ on MNIST; see Equation 3. In the inductive case, as shown in Figure 8, there is little
sensitivity to Î»â€² in either attack success rate or rejection rate. When targeting TLDR, there is little
27

Figure 7: Rejection rate scaling with adversarial budget Ïµ on MNIST.
0.0
0.5
1.0
1.5
2.0
â€²
0.0
0.2
0.4
0.6
0.8
1.0
Robust Accuracy
0.0
0.5
1.0
1.5
2.0
â€²
0.0
0.2
0.4
0.6
0.8
1.0
Adversarial Rejection Rate
Figure 8: Effects of Î»â€² on results of PGD optimizing LREJ targeting adversarial training with rejection
on MNIST.
sensitivity in terms of attack success rate as seen in Figure 9; rejection rate is highest for intermediate
values of Î»â€² but, as expected, rejection rate declines with Î»â€² beyond that.
D
Limitations
While our framework is theoretical-sound with lower sampled complexity than the rejection-only
case and with more relaxed optimality condition than the transductive-only case, our sample com-
plexity proof under the transductive rejection case requires the non-emptiness of âˆ†in Theorem 4.1.
While weaker conditions donâ€™t guarantee that we find a model satisfying the conditions, the result
demonstrate that empirical defense incorporating both transduction and rejection have the potential to
outperform others. Our proposed defense algorithm TLDR, though effective at improving the robust
accuracy under rejection, incurs a high computational cost relative to standard adversarial training
due to the joint training with the unlabeled data. If it is possible to delay evaluation until a sufficiently
large batch of samples arrives, the cost can be made insignificant via amortization. The need to
perform a full training process prior to evaluation means, however, that the defense is not suitable
for latency-sensitive applications. Our adaptive attack is even more costly, as effectively attacking
this defense using GMSA requires multiple iterations of the full transductive training process; hence,
adversaries attacking TLDR require substantial resources.
28

0.0
0.5
1.0
1.5
2.0
â€²
0.0
0.2
0.4
0.6
0.8
1.0
Robust Accuracy
0.0
0.5
1.0
1.5
2.0
â€²
0.0
0.2
0.4
0.6
0.8
1.0
Adversarial Rejection Rate
Figure 9: Effects of Î»â€² on results of GMSA optimizing LREJ targeting TLDR on MNIST.
29

