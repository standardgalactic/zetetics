Parrot: Pareto-optimal Multi-Reward
Reinforcement Learning Framework for Text-to-Image Generation
Seung Hyun Lee1,6∗, Yinxiao Li1, Junjie Ke1, Innfarn Yoo2, Han Zhang3, Jiahui Yu4†, Qifei Wang2,
Fei Deng2,5∗, Glenn Entis1, Junfeng He1, Gang Li1, Sangpil Kim6, Irfan Essa1, Feng Yang1
Google Research1, Google2, Google DeepMind3, OpenAI4, Rutgers University5, Korea University6
Stable Diffusion 1. 5
Parrot
Human Preference
Aesthetics
Text-Image Alignment
Image Sentiment
Prompt Expansion
↑
↑
↑
↑
“A boat”
“A painting of a horse in a field 
of flowers”
“Cozy living room with a painting 
of a corgi on the wall above a 
couch and a round coffee table in 
front of a couch and a vase of 
flowers on a coffee table”
“A cartoon of a happy car on the 
road”
“Airplane taking off a runway” →
”4k resolution of an airplane taking 
off a runway with a beautiful 
sunset in the background”
Figure 1. Parrot visual examples. Parrot consistently improves the quality of generated images across multiple criteria: aesthetics, human
preference, text-image alignment, and image sentiment. Moreover, Parrot handles prompt expansion to complement semantics of the text
input for high-quality image generation. Each column generated images using the same seed.
Abstract
Recent works demonstrate that using reinforcement
learning (RL) with quality rewards can enhance the qual-
ity of generated images in text-to-image (T2I) generation.
However, a simple aggregation of multiple rewards may
cause over-optimization in certain metrics and degradation
in others, and it is challenging to manually find the optimal
weights. An effective strategy to jointly optimize multiple
rewards in RL for T2I generation is highly desirable. This
paper introduces Parrot, a novel multi-reward RL frame-
work for T2I generation. Through the use of the batch-
wise Pareto optimal selection, Parrot automatically identi-
fies the optimal trade-off among different rewards during
the RL optimization of the T2I generation. Additionally,
* This work was done during an internship at Google.
† This work was done during working at Google.
Parrot employs a joint optimization approach for the T2I
model and the prompt expansion network, facilitating the
generation of quality-aware text prompts, thus further en-
hancing the final image quality. To counteract the potential
catastrophic forgetting of the original user prompt due to
prompt expansion, we introduce original prompt centered
guidance at inference time, ensuring that the generated im-
age remains faithful to the user input. Extensive experi-
ments and a user study demonstrate that Parrot outperforms
several baseline methods across various quality criteria, in-
cluding aesthetics, human preference, image sentiment, and
text-image alignment.
1. Introduction
Recent progress in diffusion models [15, 26, 36, 45, 55]
combined with pre-trained text encoders has resulted in re-
markable results in text-to-image (T2I) generation. These
arXiv:2401.05675v1  [cs.CV]  11 Jan 2024

advancements have enabled the creation of high-fidelity
novel images based on user-input text prompts. However,
despite these advances, existing T2I models, such as Im-
agen [41] and Stable Diffusion [38], still generate images
that fall short of the ideal. For example, the images in the
first row in Fig. 1 exhibits quality issues such as poor com-
position (e.g. bad cropping), misalignment with the user in-
put (e.g. missing objects), or generally less aesthetic pleas-
ing. Consequently, achieving higher quality generated im-
ages with T2I generation remains a challenging task.
Quality of text-to-image generation is affected by mul-
tiple factors. Diverse quality metrics have been proposed
to quantify each of these quality aspects.
Some qual-
ity aspects are universal to images, including the aesthet-
ics [20, 21, 31] of an image or whether the image is pleas-
ing to look at [43]. There are also unique aspects regard-
ing generated images, such as image-text alignment [33]
and human preference [23] given the input prompt. Recent
works [4, 10, 24] have demonstrated that incorporating such
quality signals as a reward function can enhance the gener-
ated image quality of T2I models. In particular, DPOK [10]
shows online reinforcement learning (RL) using ImageRe-
ward [51] score as a reward is able to improve T2I generated
image quality. Given the aforementioned diverse criteria for
the quality of generated images, it is a natural extension to
incorporate multiple quality metrics in the RL fine-tuning
of T2I models. While a straightforward strategy involves
using a weighted sum of the reward scores [30, 37, 44, 49],
this approach requires manual tuning of weights for differ-
ent rewards, which is both time-consuming and resource-
intensive. It is also not scalable as the number of rewards in-
crease. Furthermore, optimizing one quality metric may in-
advertently lead to degradation in others. For example, the
model might generate an exceptionally good image with-
out considering the relevance to the user provided prompt.
Therefore, an effective strategy is needed to identify the op-
timal trade-off between different rewards in T2I generation.
To achieve this, we propose a novel Pareto-optimal
multi-reward reinforcement learning framework for text-
to-image generation, denoted as Parrot. Within a batch
of samples produced by T2I models, each sample embod-
ies a distinctive trade-off among various reward functions.
By identifying and leveraging the set that attains the opti-
mal trade-off (i.e. the Pareto-optimal set [29]) within such a
training batch, Parrot effectively optimize multiple rewards
simultaneously. This results in the generation of images
with good aesthetics, proper image-text alignment, align-
ment with human preference, and an overall pleasing senti-
ment (Fig. 1).
The quality of generated images is significantly influ-
enced by the text prompt input provided to the T2I model.
Semantically rich prompts have been shown to produce
higher quality images.
As an example, simple prompts
like “A boat” often fail to generate highly detailed im-
ages. On the other hand, prompts with details, like “A green
boat floating on the water with a beautiful sunset”, tend
to generate higher quality images. Recognizing the diffi-
culty of manually crafting effective prompts, Promptist [13]
has been introduced to autonomously learn prompt expan-
sion with large language models (LLM). This involves RL-
tuning of the LLMs while keeping the T2I model frozen as
a black box. However, since the T2I model is not tuned
in collaboration with the prompt expansion network, it may
struggle to adapt to the generated text inputs. In Parrot, we
propose to jointly optimize the prompt expansion network
and the T2I model using multiple quality rewards. This al-
lows the prompt expansion network and T2I model to col-
laboratively generate higher quality images. To the best of
our knowledge, we are the first to show the potential of joint
optimization for prompt expansion and T2I models.
Moreover, although prompt expansion facilitates a more
detailed context, there is a risk of catastrophic forgetting of
the original input prompt. To address this challenge, we fur-
ther introduce original prompt-centered guidance in Parrot
during inference time, enabling the generated images to re-
main faithful to the original prompts while simultaneously
incorporating additional details.
Our contributions can be summarized as follows:
• We present Parrot, a novel framework that enhances the
generated image quality of T2I generation through multi-
reward RL fine-tuning.
Leveraging batch-wise Pareto-
optimal selection, it efficiently optimizes the T2I model
with multiple rewards, enabling collaborative improve-
ment in aesthetics, human preference, image sentiment,
and text-image alignment.
• We are the first to advocate for the joint optimization of
the prompt expansion network and the T2I model. Our
results demonstrate the superiority of this joint optimiza-
tion compared to the individual fine-tuning of either the
prompt expansion network or the T2I model.
• We propose original prompt-centered guidance as a
means to prevent the catastrophic forgetting of the ini-
tial input prompt after prompt expansion. This ensures
that the generated image stays true to the original prompt
while adding more details.
• Extensive results and a user study show that Parrot out-
performs several baseline methods across various quality
criteria.
2. Related Work
T2I Generation: The goal of T2I generation is to create an
image given an input text prompt. Several T2I generative
models have been proposed and have demonstrated promis-
ing results [5, 7, 12, 18, 19, 25, 35, 40, 41, 54]. Notably,
Stable Diffusion [38] models show impressive generation
performance in T2I generation, leveraging latent text rep-

resentations from LLMs. Despite substantial progress, the
images generated by those models still exhibit quality is-
sues, such as bad cropping or misalignment with the input
texts.
RL for T2I Fine-tuning: Fan et al. [9] pioneered the appli-
cation of RL fine-tuning for T2I models, treating the denois-
ing process as a multi-step decision making task. For human
preference learning, recent works [4, 6, 10] have explored
RL fine-tuning technique for T2I diffusion model, show-
casing superior performance.
In addition to fine-tuning
the T2I model directly using RL, Promptist [13] fine-tunes
the prompt expansion model by using image-text alignment
scores and aesthetic scores as rewards. However, since the
T2I model is not jointly fine-tuned, it may not adapt well
to the following image generation tasks. Parrot proposes to
jointly optimize the prompt expansion model and the T2I
model using multi-reward RL.
Multi-objective Optimization: Multi-objective optimiza-
tion problem involves optimizing multiple objective func-
tions simultaneously. The scalarization technique [28, 47]
formulates multi-objective problem into single-objective
problems with the weighted sum of each score, which re-
quires pre-defined weights for each objective.
Rame et
al. [34] proposed weighted averaging method to find Pareto
frontier, leveraging multiple fine-tuned models.
Lin et
al. [27] proposes to learn a set model to map trade-off pref-
erence vectors to their corresponding Pareto solutions. In-
spired by this, Parrot introduces a language-based prefer-
ence vector constructed from task identifiers for each re-
ward, then encoded by the text-encoder. In the context of
multi-reward RL for T2I diffusion models, Promptist [13]
uses a simple weighted sum of two reward scores. This ap-
proach requires manual tuning of the weights, which makes
it time-consuming and hard to scale when the number of
rewards increases.
Generated Image Quality: The quality assessment of im-
ages generated by T2I models involves multiple dimen-
sions, and various metrics have been proposed.
In this
paper, we consider using four types of quality metrics as
rewards:
aesthetics, human preference, image-text align-
ment, and image sentiment. Aesthetics, capturing the over-
all visual appeal of the image, has been addressed using
learning-based that leverage human ratings for aesthetics
in real images [11, 16, 20, 21, 31, 48, 52]. Human pref-
erences, rooted the concept of learning from human feed-
back [3, 32], involves gathering preferences at scale by hav-
ing raters to compare different generations [23, 51]. Text-
image alignment measures the extent to which the generated
image aligns with the input prompt, CLIP [33] score is often
employed, measuring the cosine distance of between con-
trastive image embedding and text embedding. Image sen-
timent is important for ensuring the generated image evokes
positive emotions in the viewer. Serra et al. [43] predict av-
erage polarity of sentiments an image elicits, learning esti-
mates for positive, neutral, and negative scores. In Parrot,
we use its positive score as a reward for positive emotions.
3. Preliminary
Diffusion Probabilistic Models: Diffusion probabilistic
models [15] generate the image by gradually denoising a
noisy image.
Specifically, given a real image x0 from
the data distribution x0 ∼q(x0), the forward process
q(xt|x0, c) of diffusion probabilistic models produce a
noisy image xt, which induces a distribution p(x0, c) con-
ditioned on text prompt c. In classifier-free guidance [14],
denoising model predicts noise ¯ϵθ with a linear combina-
tion of the unconditional score estimates ϵθ(xt, t) and the
conditional score estimates ϵθ(xt, t, c) as follows:
¯ϵθ = w · ϵθ(xt, t, c) + (1 −w) · ϵθ(xt, t, null),
(1)
where t denotes diffusion time step, the null indicates a null
text and w represents the guidance scale of classifier-free
guidance where w ≥1. Note that ϵθ is typically parameter-
ized by the UNet [39].
RL-based T2I Diffusion Model Fine-tuning: Given a re-
ward signal from generated images, the goal of RL-tuning
for T2I diffusion models is to optimize the policy defined as
one denoising step of T2I diffusion models. In particular,
Black et al. [4] apply policy gradient algorithm, which re-
gards the denoising process of diffusion models as a Markov
decision process (MDP) by performing multiple denoising
steps iteratively. Subsequently, a black box reward model
r(·, ·) predicts a single scalar value from sampled image x0.
Given text condition c ∼p(c) and image x0, objective func-
tion J can be defined to maximize the expected reward as
follows:
Jθ = Ep(c)Epθ(x0|c)[r(x0, c)],
(2)
where the pre-trained diffusion model pθ produces a sample
distribution pθ(x0|c) using text condition c. Modifying this
equation, Lee et al. [10] demonstrate that the gradient of
objective function ∇Jθ can be calculated through gradient
ascent algorithm without using the gradient of reward model
as follows:
∇Jθ = E[r(x0, c)
T
X
t=1
∇θ log pθ(xt−1|c, t, xt)],
(3)
where T denotes the total time step of the diffusion sam-
pling process. With parameters θ, the expectation value can
be taken over trajectories of the diffusion sampling process.
4. Method
4.1. Parrot Overview
Fig. 2 shows the overview of Parrot, which consists of the
prompt expansion network (PEN) pϕ and the T2I diffusion
model pθ. The PEN is first initialized from a supervised

T2I Model
Prompt Expansion Network
…
“A boat”
“A boat in the middle of a lake with 
a beautiful sunset.”
Reward Scores 1
Reward Scores 2
Reward Scores N
…
Policy Gradient Update
Sample1
Sample2
Batch-wise Pareto 
Optimal Selection
SampleN
: Tunable
: Frozen
“Old fashioned 
cocktail. ”
“A glass of old fashioned cocktail on 
a wooden table in a dark room”
Training
Inference
: Forward
: Backward
Joint Optimization
Prompt Expansion Network
Joint Guidance
Multi-Reward
T2I Model
Text-Image Alignment, 
 Aesthetic Score,
Human Preference, 
Image Sentiment
Original prompt
Original prompt
Figure 2. Overview of the Parrot framework. During the training, N images are generated from the T2I model using the expanded prompt
from the prompt expansion network. Multiple quality rewards are calculated for each image, and the Pareto-optimal set is identified using
the non-dominated sorting algorithm. These optimal images are then used to perform policy gradient update of the parameters of T2I
diffusion and prompt expansion network jointly. During the inference, both the original prompt and the expanded prompt are provided to
the T2I model, enabling a trade-off between faithfulness and detail.
fine-tuning checkpoint on demonstrations of prompt expan-
sion pairs, and the T2I model is initialized from pretrained
diffusion model. Given the original prompt c, the PEN gen-
erates an expanded prompt ˆc, and the T2I model generates
images based on this expanded prompt. During the multi-
reward RL fine-tuning, a batch of N images is sampled, and
multiple quality rewards are calculated for each image, en-
compassing aspects like text-image alignment, aesthetics,
human preference, and image sentiment. Based on these re-
ward scores, Parrot identifies the batch-wise Pareto-optimal
set using a non-dominated sorting algorithm. This optimal
set of images is then used for joint optimization of the PEN
and T2I model parameters through RL policy gradient up-
date. During inference, Parrot leverages both the original
prompt and its expansion, striking a balance between main-
taining faithfulness to the original prompt and incorporating
additional details for higher quality.
4.2. Batch-wise Pareto-optimal Selection
Algorithm 1 outlines the procedure of Parrot. Rather than
updating the gradients using all images, Parrot focuses on
high-quality samples, considering multiple quality rewards
in each mini-batch. In the multi-reward RL, each sample
generated by the T2I model presents distinct trade-offs for
each reward. Among these samples, a subset with varied op-
timal trade-offs across objectives, known as the Pareto set,
exists. For a Pareto-optimal sample, none of its objective
values can be further improved without damaging others.
In other words, the Pareto-optimal set is not dominated by
any data points, also known as the non-dominated set. To
achieve a Pareto-optimal solution with T2I diffusion model,
Parrot selectively uses data points from the non-dominated
set using non-dominated sorting algorithm. This naturally
encourages the T2I model to produce Pareto-optimal sam-
ples with respect to the multi-reward objectives.
Reward-specific Preference: Inspired by the use of prefer-
ence information in multi-objective optimization [27], Par-
rot incorporates the preference information through reward-
specific identifies. This enables Parrot to automatically de-
termine the importance for each reward objective.
Con-
cretely, we enrich the expanded prompt ˆc by prepending
reward-specific identifier “<reward k>” for k-th reward.
Based on this reward-specific prompt, N images are gen-
erated and are used for maximizing the corresponding k-th
reward model during gradient update. At inference time,
a concatenation of all the reward identifiers
“<reward
1>,...,<reward K>” is used for image generation.
Non-dominated Sorting: Parrot constructs Pareto set with
non-dominated points based on trade-offs among multiple
rewards. These non-dominated points are superior to the
remaining solutions and are not dominated by each other.
Formally, the dominance relationship is defined as follows:
the image xa
0 dominates the image xb
0, denoted as xb
0 < xa
0,
if and only if Ri(xb
0) ≤Ri(xa
0) for all i ∈1, ..., m, and
there exists j ∈1, ..., m such that Rj(xa
0) < Rj(xb
0). For
example, given the i-th generated image xi
0 in a mini-batch,
when no point in the mini-batch dominates xi
0, it is referred
to as a non-dominated point.
Policy Gradient Update: We assign a reward value of zero
to the data points not included in non-dominated sets and
only update the gradient of these non-dominated data points
as follows:
∇Jθ =
1
n(P)
N
X
i=1,xi
0∈P
K
X
k=1
T
X
t=1
rk(xi
0, ck)×
∇θ log pθ(xi
t−1|ck, t, xi
t),
(4)
where i indicates the index of images in mini-batches, and
P denotes batch-wise a set of non-dominated points. K and

Algorithm 1 Parrot: Pareto-optimal Multi-Reward RL
Input: Text prompt c, Batch size N, Total iteration E, the num-
ber of rewards: K, Prompt expansion network pϕ, T2I diffusion
model: pθ, Total diffusion time step T, Non-dominated set: P
for e = 1 to E do
Sample text prompt c ∼p(c)
for k = 1 to K do
Expand text prompt ˆc ∼pϕ(ˆc|c)
Prepend reward-specific tokens “<reward k>” to ˆc
Sample a set of images {x1
0, ..., xN
0 } ∼pθ(x0|ˆc)
A set of reward vector R = {R1, ..., RN}
P ←NONDOMINATEDSET({x1
0, ..., xN
0 })
for j in P do
∇Jθ += PT
t=1 rk(xj
0, ˆc)·∇log pθ(xj
t−1|ck, t, xj
t)
∇Jϕ += −rk(xj
0, ˆc) × ∇log pϕ(ˆc|c)
Update the gradient pθ from Eq. 4
Update the gradient pϕ
Output: Fine-tuned diffusion model pθ,
prompt expansion network pϕ
function NONDOMINATEDSET({x1
0, ..., xN
0 })
P ←∅
for i = 1 to N do
dominance ←True
for j = 1 to N do
if xj
0 dominates xi
0 then
dominance ←False
if dominance is True then
Add i to P
return P
T are the total number of reward models and total diffusion
time steps, respectively. The same text prompt is used when
updating the diffusion model in each batch.
4.3. Original Prompt Centered Guidance
While prompt expansion enhances details and often im-
proves generation quality, there is a concern that the added
context may dilute the main content of the original input.
To mitigate this during the inference, we introduce original
prompt-centered guidance. When sampling conditioned on
the original prompt, the diffusion model ϵθ typically pre-
dicts noises by combining the unconditioned score estimate
and the prompt-conditioned estimate.
Instead of relying
solely on the expanded prompt from PEN, we propose using
a linear combination of two guidances for T2I generation:
one from the user input and the other from the expanded
prompt. The strength of the original prompt is controlled
by guidance scales w1 and w2. The noise ¯ϵθ is estimated,
derived from Eq. 1, as follows:
¯ϵθ = w1 · ϵθ(xt, t, c) + (1 −w1) · ϵθ(xt, t, null)+
(5)
w2 · ϵθ(xt, t, ˆc) + (1 −w2) · ϵθ(xt, t, null),
(6)
where null denotes a null text.
5. Experiments
5.1. Experiment Setting
Dataset: The PEN is first supervised fine-tuned on a large-
scale text dataset named the Promptist [13], which has
360K constructed prompt pairs for original prompt and
prompt expansion demonstration.
The original instruc-
tion “Rephrase” is included per pair in Promptist. We mod-
ify the instruction into “Input: <original prompt>. This is
a text input for image generation. Expand prompt for im-
proving image quality. Output: ”. Subsequently, we use the
RL tuning prompts (1200K) from Promptist for RL training
of the PEN and T2I model.
T2I Model: Our T2I model is based on the JAX version
of Stable Diffusion 1.5 [38] pre-trained with the LAION-
5B [42] dataset. We conduct experiments on a machine
equipped with 16 NVIDIA RTX A100 GPUs. DDIM [46]
with 50 denoising steps is used, and the classifier-free guid-
ance weight is set to 5.0 with the resolution 512×512.
Instead of updating all layers, we specifically update the
cross-attention layer in the Denoising U-Net. For optimiza-
tion, we employ the Adam [22] optimizer with a learning
rate of 1×10−5.
Prompt Expansion Network: For prompt expansion, we
use PaLM 2-L-IT [2], one of the PaLM2 variations,
which is a multi-layer Transformer [50] decoder with ca-
sual language modeling. We optimize LoRA [17] weights
for RL-based fine-tuning. The output token length of the
PEN is set to 77 to match the maximum number of token
length for Stable Diffusion. For original prompt-centered
guidance, we set both w1 and w2 to 5 in Eq. 6.
Reward Models: We incorporate four quality signals as
rewards: Aesthetics, Human preference, Text-Image Align-
ment, Image Sentiment. For aesthetics, we use the VILA-
R [21] pre-trained with the AVA [31] dataset.
For hu-
man preference, we train a ViT-B/16 [8] using the Pick-a-
Pic [23] dataset, which contains 500K examples for human
feedback in T2I generation. The ViT-B/16 image encoder
consists of 12 transformer layers, and the image resolution
is 224×224 with a patch size of 16×16. For text-image
alignment, we use CLIP [33] with the image encoder ViT-
B/32. For image sentiment, we use the pre-trained model
from [43], which outputs three labels: positive, neutral,
negative. We use the positive score ranging from 0 to 1 as
the sentiment reward.
5.2. Qualitative Analysis
Comparison with Baselines: Fig 3 shows the visual com-
parison of Parrot and multiple baselines. We include results
from Stable Diffusion 1.5, DPOK [10] with a weighted sum
of rewards, Promptist [13], and Parrot. DPOK exclusively
fine-tunes the T2I model, while Promptist focuses on fine-
tuning only the prompt expansion network. Parrot shows vi-

A F1
F1, concept art oil 
painting by Jama 
Jurabaev, extremely 
detailed, brush hard, 
artstation
SD 1.5
Promptist
Parrot
3d rendering of a f1 
car in dark and moody 
environment with a 
red and orange color 
scheme.
A kitchen
A turtle
A kitchen with a lot 
of natural light and a 
beautiful view of the 
mountains
A turtle in a beautiful 
garden with flowers 
and a pond
A kitchen, a fantasy 
digital painting, 
trending on Artstation, 
highly detailed
A turtle, RPG 
Reference, Oil Painting, 
Trending on Artstation, 
octane render, Insanely 
Detailed, 8k, HD
A woman is looking 
out 
on 
a 
winter 
archipelago.
A woman is looking out 
on winter archipelago. 
The 
image 
is 
very 
peaceful and serene.
Painting of a woman 
looking out on a winter 
archipelago, ultra 
realistic, concept art, 
intricate details, 
eerie
A kitchen
A woman is looking 
out 
on 
a 
winter 
archipelago.
A F1
A turtle
A Japanese 
Porcelain Imari vase 
A Japanese 
Porcelain Imari vase 
A Japanese Porcelain 
Imari vase, intricate, 
elegant, highly 
detailed, digital 
painting, artstation, 
concept art, smooth, 
sharp focus, 
illustration,
A photo of a Japanese 
Porcelain Imari vase 
on the ground with a 
beautiful floral 
pattern on the front
DPOK
Figure 3. Comparison of Parrot and diffusion-based RL base-
lines. From left to right, we provide results of Stable diffusion
1.5 [38] (1st column), DPOK [10] (2nd column) with the weighted
sum, the Promptist [13] (3rd column), and the Parrot (4th column).
sually better images, particularly in aspects like color com-
bination, cropping, perspective, and fine details in the im-
age. This improvement can be attributed to Parrot’s T2I
model being fine-tuned together with the prompt expansion
model that incorporates aesthetic keywords during training.
Parrot generates results that are more closely aligned with
the input prompt, as well as more visually pleasing.
Fig. 4 shows the training curve comparison of Parrot and
(a) CLIP Score
(b) ViT-PickScore
(c) VILA-AVA
(d) Image Sentiment
Steps (K)
Steps (K)
Steps (K)
Steps (K)
Figure 4.
Training curve for fine-tuning on weighted sum
and Parrot. For weighted sum, WS1 denotes {0.7, 0.1, 0.1, 0.1}
and WS2 denotes {0.25, 0.25, 0.25, 0.25} for aesthetics, human
preference, text-image alignment and image sentiment.
Using
weighted sum leads to decrease in human preference score and im-
age sentiment score despite an improvement in the aesthetic score.
In contrast, Parrot exhibits stable increases across all metrics.
Model
Quality Metrics
TIA (↑)
Aesth (↑)
HP (↑)
IS (↑)
SD 1.5 [38]
0.2322
0.5755
0.1930
0.3010
DPOK [10] (WS)
0.2337
0.5813
0.1932
0.3013
Parrot w/o PE
0.2355
0.6034
0.2009
0.3018
Promptist [13]
0.1449
0.6783
0.2759
0.2518
Parrot
0.1667
0.7396
0.3411
0.3132
Table 1. Quantitative comparison between Parrot and alternatives
on the Parti dataset [53]. Abbreviations: WS - Weighted Sum;
PE - Prompt Expansion; TIA - Text-Image Alignment; Aesth -
Aesthetics; HP - Human Preference; IS - Image Sentiment. TIA
score is measured against the original prompt without expansion.
using a linear combination of the reward scores. Each sub-
graph represents a reward. WS1 and WS2 denote two dif-
ferent weights with multiple reward scores. WS1 places
greater emphasis on the aesthetic score, while WS2 adopts
balanced weights across aesthetics, human preference, text-
image alignment, and image sentiment.
Employing the
weighted sum of multiple rewards leads to a decrease in
the image sentiment score, despite notable enhancements in
aesthetics and human preference. In contrast, Parrot consis-
tently exhibits improvement across all metrics.
5.3. Quantitative Evaluation
Comparison with Baselines: Table 1 presents our results
of the quality score across four quality rewards: text-image
alignment score, aesthetic score, human preference score,

Area
Question
Aesthetics
“Which image shows better aesthetics with-
out blurry texture, unnatural focusing, and
poor color combination?”
Human Preference
“Which generated image do you prefer?”
Text-Image Alignment
“Which image is well aligned with the
text?”
Image Sentiment
“Which image is closer to amusement, ex-
citement, and contentment?”
Table 2. Questions for user study. For performing user study, we
carefully design questions suitable to each quality.
Average
Aesthetics
%
Human
Preference
Text-image
Alignment
Image
Sentiment
Figure 5. User study results on PartiPrompts [53]. Parrot outper-
forms baselines across all metrics.
and emotion score. The first group shows methods without
prompt expansion, and the second group compares meth-
ods with expansion. The prompt expansion and T2I gener-
ation are performed on the PartiPrompts [53]. Using a set
of 1631 prompts, we generate 32 images for each text in-
put and calculate the average for each metric. “w/o Prompt
Expansion” indicates the generation of images solely based
on original prompt without expansion. We compare with
DPOK [10] with a weighted sum (WS) of rewards, using
balanced weights of {0.25, 0.25, 0.25, 0.25}. For Promp-
tist [13], we generate prompt expansion from their model.
Our method outperforms both compared methods in aes-
thetics, human preference and sentiment scores. The text-
image alignment score is measured with the original prompt
before expansion for fair comparison.
As a result, the
group without prompt expansion generally shows a higher
text-image alignment score. Parrot shows better text-image
alignment in each subgroup.
User Study: We conduct a user study using MTurk [1]
with generated images from 100 random prompts in the Par-
tiPrompts [53]. Five models are compared: Stable Diffu-
sion v1.5, DPOK [10] with an equal weighted sum, Promp-
tist [13], Parrot without prompt expansion, and Parrot. Each
rater is presented with the original prompt (before expan-
sion) and a set of five generated images, with the image or-
der being randomized. Raters are then tasked with selecting
(a) Aesthetic score
(b) Human preference score
(c) Text-image alignment score
(d) Image Sentiment score
Figure 6. Ablation study. We perform an ablation study by re-
moving one of quality signals. We observe that each quality signal
affects their improvement of (a) aesthetics, (b) human preference
score, (c) text-image alignment score, (d) image sentiment score.
the best image from the group, guided by questions outlined
in Table 2. Each question pertains to a specific quality as-
pect: aesthetics, human preference, text-image alignment,
and image sentiment. For each prompt, 20 rounds of ran-
dom sampling are conducted and sent to different raters.
The aggregated user study results, illustrated in Fig 5, show
that Parrot significantly outperforms other baselines across
all dimensions.
5.4. Ablations
Effect of Pareto-optimal Multi-reward RL: To show the
efficacy of Pareto-optimal Multi-reward RL, we conduct an
ablation study by removing one reward model at a time.
Fig 6 shows quantitative results using one hundred random
text prompts from the Promptist [13]. We observe that our
training scheme improves multiple target objectives. Fig 7
shows the visual comparison between Parrot, Parrot with a
single reward, and Parrot without selecting the batch-wise
Pareto-optimal solution. Using a single reward model tends
to result in degradation of another reward, especially text-
image alignment. For example, in the third column, results
of the first row miss the prompt a tophat, even though the
Stable Diffusion result includes that attribute. On the other
hand, Parrot results capture all prompts, improving other
quality signals, such as aesthetics, image sentiment and hu-
man preference.
Effect of Original Prompt Centered Guidance: Fig 8
shows the effect of the proposed original prompt-centered
guidance. As evident from the figure, utilizing only the ex-
panded prompt as input often results in the main content be-
ing overwhelmed by the added context. For instance, given
the original prompt “A shiba inu”, the result from the ex-
panded prompt shows a zoomed-out image and the intended
main subject (shiba inu) becomes small. The proposed orig-
inal prompt-centered guidance effectively addresses this is-
sue, generating an image that faithfully captures the original
prompt while incorporating visually more pleasing details.

Stable Diffusion 1.5
Aesthetics
Human Preference
 Text-Image Alignment
Image Sentiment
Parrot
Parrot
w/o Pareto Optimal
A raccoon wearing formal 
clothes, wearing a tophat 
and holding a cane. Oil 
painting in the style of 
Rembrandt.
A raccoon wearing formal 
clothes, wearing a tophat 
and holding a cane. Oil 
painting in the style of 
Rembrandt.
A raccoon wearing formal 
clothes, wearing a tophat 
and holding a cane. Oil 
painting in the style of 
Rembrandt.
A raccoon wearing formal 
clothes, wearing a tophat 
and holding a cane. Oil 
painting in the style of 
Rembrandt.
A raccoon wearing formal 
clothes, wearing a tophat 
and holding a cane. Oil 
painting in the style of 
Rembrandt.
A raccoon wearing formal 
clothes, wearing a tophat 
and holding a cane. Oil 
painting in the style of 
Rembrandt.
A raccoon wearing formal 
clothes, wearing a tophat 
and holding a cane. Oil 
painting in the style of 
Rembrandt.
A painting of a pandas 
dressed as a chef, serving 
a cookie in a cozy kitchen.
A painting of a pandas 
dressed as a chef, serving 
a cookie in a cozy 
kitchen.
A painting of a pandas 
dressed as a chef, serving 
a cookie in a cozy 
kitchen.
A painting of a pandas 
dressed as a chef, serving 
a cookie in a cozy kitchen.
A painting of a pandas 
dressed as a chef, serving 
a cookie in a cozy 
kitchen.
A painting of a pandas 
dressed as a chef, serving 
a cookie in a cozy kitchen.
A painting of a pandas 
dressed as a chef, serving 
a cookie in a cozy kitchen.
Figure 7. The comparisons of the diffusion fine-tuning between Pareto-optimal multi-reward RL and single reward RL. We show
results with same seed from various methods: Stable Diffusion 1.5 [38] (1st column), T2I model fine-tuned with the aesthetic model (2nd
column), the human preference model (3rd column), text-image alignment (4th column), image sentiment (5th column), Parrot without
Pareto-optimal selection (6th column) and Parrot (7th column). Parrot is effective to generate acceptable images without sacrificing one of
quality signals. For example, T2I model fine-tuned with a single quality signal such as aesthetics, human preference and image sentiment
results in text-image misalignment. In contrast, our method achieves a balanced visual outcome across multiple criteria.
Stable Diffusion 1.5
Parrot
Pickup truck under street 
lights at night
W/o original prompt 
centered guidance
Pickup 
truck 
under 
street 
lights at night with a beautiful 
cityscape in the background.
Pickup 
truck 
under 
street
lights at night with a beautiful
cityscape in the background.
A shiba inu
Shiba inu in a field of flowers 
with a beautiful sunset
A pumpkin
A pumpkin with a beautiful 
autumn background
A pumpkin with a beautiful 
autumn background
Shiba inu in a field of flowers 
with a beautiful sunset
Figure 8. Results of original prompt centered guidance. As we
expand the prompt, the content in the generated image often fades
away (2nd column). We observe that this guidance is helpful for
keeping the main content of the original prompt.
6. Conclusion and Limitation
In this paper, we propose Parrot, a framework designed to
improve T2I generation by effectively optimizing multiple
quality rewards using RL. With batch-wise Pareto-optimal
selection, we effectively balance the optimization process
of multiple quality rewards, improving each quality met-
ric. Furthermore, through co-training the T2I model and
the prompt expansion model, Parrot can generate higher-
quality images. Additionally, our original prompt centered
guidance technique enables the control over the persistence
of the main content in user input, ensuring that the gen-
erated image remains faithful to the user prompt. Results
from the user study indicate that Parrot significantly ele-
vates the quality of generated images across multiple crite-
ria including text-image alignment, human preference, aes-
thetics, and image sentiment.
While Parrot has demon-
strated effectiveness in enhancing generated image quality,
its efficacy is limited by the quality metrics it relies on.
Consequently, advancements of the generated image quality
metrics will directly enhance the capabilities of Parrot. Ad-
ditionally, Parrot is adaptable to a broader range of rewards
that quantify generated image quality.
Societal Impact: Parrot could potentially raise ethical con-
cerns related to the generation of immoral content. This
concern stems from the user’s ability to influence T2I gener-
ation, allowing for the creation of visual content that may be
deemed inappropriate. The risk may be tied to the potential
biases in reward models inherited from various datasets.

References
[1] Amazon mechanical turk. https://www.mturk.com/,
2005. 7
[2] Rohan Anil, Andrew M Dai, Orhan Firat, Melvin John-
son, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri,
Emanuel Taropa, Paige Bailey, Zhifeng Chen, et al. Palm 2
technical report. arXiv preprint arXiv:2305.10403, 2023. 5
[3] Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell,
Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort,
Deep Ganguli, Tom Henighan, et al. Training a helpful and
harmless assistant with reinforcement learning from human
feedback. arXiv preprint arXiv:2204.05862, 2022. 3
[4] Kevin Black, Michael Janner, Yilun Du, Ilya Kostrikov, and
Sergey Levine.
Training diffusion models with reinforce-
ment learning. In ICML Workshop, 2023. 2, 3, 1
[5] Huiwen Chang, Han Zhang, Jarred Barber, AJ Maschinot,
Jose Lezama, Lu Jiang, Ming-Hsuan Yang, Kevin Murphy,
William T Freeman, Michael Rubinstein, et al. Muse: Text-
to-image generation via masked generative transformers. In
ICML, 2023. 2
[6] Kevin Clark, Paul Vicol, Kevin Swersky, and David J Fleet.
Directly fine-tuning diffusion models on differentiable re-
wards. arXiv preprint arXiv:2309.17400, 2023. 3
[7] Xiaoliang Dai, Ji Hou, Chih-Yao Ma, Sam Tsai, Jialiang
Wang, Rui Wang, Peizhao Zhang, Simon Vandenhende, Xi-
aofang Wang, Abhimanyu Dubey, et al.
Emu: Enhanc-
ing image generation models using photogenic needles in a
haystack. arXiv preprint arXiv:2309.15807, 2023. 2
[8] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,
Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,
Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-
vain Gelly, et al. An image is worth 16x16 words: Trans-
formers for image recognition at scale. In ICLR, 2021. 5
[9] Ying Fan and Kangwook Lee. Optimizing ddpm sampling
with shortcut fine-tuning. In ICML, 2023. 3, 1, 7, 8
[10] Ying
Fan,
Olivia
Watkins,
Yuqing
Du,
Hao
Liu,
Moonkyung Ryu, Craig Boutilier, Pieter Abbeel, Moham-
mad Ghavamzadeh, Kangwook Lee, and Kimin Lee. Dpok:
Reinforcement learning for fine-tuning text-to-image diffu-
sion models. In NeurIPS, 2023. 2, 3, 5, 6, 7, 1, 8
[11] Yuming Fang, Hanwei Zhu, Yan Zeng, Kede Ma, and Zhou
Wang. Perceptual quality assessment of smartphone photog-
raphy. In CVPR, 2020. 3
[12] Ligong Han, Yinxiao Li, Han Zhang, Peyman Milanfar,
Dimitris Metaxas, and Feng Yang. Svdiff: Compact param-
eter space for diffusion fine-tuning. In ICCV, 2023. 2
[13] Yaru Hao, Zewen Chi, Li Dong, and Furu Wei. Optimizing
prompts for text-to-image generation. In CoRR, 2022. 2, 3,
5, 6, 7
[14] Jonathan Ho and Tim Salimans.
Classifier-free diffusion
guidance. In CoRR, 2022. 3
[15] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffu-
sion probabilistic models. In NeurIPS, 2020. 1, 3
[16] Vlad Hosu, Hanhe Lin, Tamas Sziranyi, and Dietmar Saupe.
Koniq-10k: An ecologically valid database for deep learn-
ing of blind image quality assessment. TIP, 29:4041–4056,
2020. 3
[17] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-
Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen.
Lora: Low-rank adaptation of large language models. 2022.
5
[18] Yujin Jeong, Wonjeong Ryoo, Seunghyun Lee, Dabin Seo,
Wonmin Byeon, Sangpil Kim, and Jinkyu Kim. The power
of sound (tpos): Audio reactive video generation with stable
diffusion. In ICCV, 2023. 2
[19] Bahjat Kawar, Shiran Zada, Oran Lang, Omer Tov, Huiwen
Chang, Tali Dekel, Inbar Mosseri, and Michal Irani. Imagic:
Text-based real image editing with diffusion models.
In
CVPR, 2023. 2
[20] Junjie Ke, Qifei Wang, Yilin Wang, Peyman Milanfar, and
Feng Yang. Musiq: Multi-scale image quality transformer.
In ICCV, 2021. 2, 3
[21] Junjie Ke, Keren Ye, Jiahui Yu, Yonghui Wu, Peyman Mi-
lanfar, and Feng Yang. Vila: Learning image aesthetics from
user comments with vision-language pretraining. In CVPR,
2023. 2, 3, 5
[22] Diederik P Kingma and Jimmy Ba. Adam: A method for
stochastic optimization. In ICLR, 2015. 5, 1
[23] Yuval Kirstain, Adam Polyak, Uriel Singer, Shahbuland Ma-
tiana, Joe Penna, and Omer Levy.
Pick-a-pic: An open
dataset of user preferences for text-to-image generation.
2023. 2, 3, 5
[24] Kimin Lee, Hao Liu, Moonkyung Ryu, Olivia Watkins,
Yuqing Du, Craig Boutilier, Pieter Abbeel, Mohammad
Ghavamzadeh, and Shixiang Shane Gu.
Aligning text-to-
image models using human feedback. In CoRR, 2023. 2
[25] Seung Hyun Lee, Sieun Kim, Innfarn Yoo, Feng Yang,
Donghyeon Cho, Youngseo Kim, Huiwen Chang, Jinkyu
Kim, and Sangpil Kim. Soundini: Sound-guided diffusion
for natural video editing. arXiv preprint arXiv:2304.06818,
2023. 2
[26] Yuheng Li, Haotian Liu, Qingyang Wu, Fangzhou Mu, Jian-
wei Yang, Jianfeng Gao, Chunyuan Li, and Yong Jae Lee.
Gligen: Open-set grounded text-to-image generation.
In
CVPR, 2023. 1
[27] Xi Lin, Zhiyuan Yang, Xiaoyuan Zhang, and Qingfu Zhang.
Pareto set learning for expensive multi-objective optimiza-
tion. In NeurIPS, 2022. 3, 4
[28] Shie Mannor and Nahum Shimkin. The steering approach
for multi-criteria reinforcement learning. In NeurIPS, 2001.
3
[29] Kaisa Miettinen.
Nonlinear multiobjective optimization.
Springer Science & Business Media, 1999. 2
[30] Hossam Mossalam, Yannis M Assael, Diederik M Roijers,
and Shimon Whiteson. Multi-objective deep reinforcement
learning. In CoRR, 2016. 2
[31] Naila Murray, Luca Marchesotti, and Florent Perronnin.
Ava: A large-scale database for aesthetic visual analysis. In
CVPR, 2012. 2, 3, 5
[32] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Car-
roll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini
Agarwal, Katarina Slama, Alex Ray, et al.
Training lan-
guage models to follow instructions with human feedback.
In NeurIPS, 2022. 3

[33] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learn-
ing transferable visual models from natural language super-
vision. In ICML, 2021. 2, 3, 5
[34] Alexandre Rame, Guillaume Couairon, Mustafa Shukor,
Corentin Dancette, Jean-Baptiste Gaya, Laure Soulier, and
Matthieu Cord.
Rewarded soups: towards pareto-optimal
alignment by interpolating weights fine-tuned on diverse re-
wards. In NeurIPS, 2023. 3
[35] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu,
and Mark Chen. Hierarchical text-conditional image gen-
eration with clip latents. arXiv preprint arXiv:2204.06125,
2022. 2
[36] Elad Richardson, Kfir Goldberg, Yuval Alaluf, and Daniel
Cohen-Or. Conceptlab: Creative generation using diffusion
prior constraints. arXiv preprint arXiv:2308.02669, 2023. 1
[37] Diederik M Roijers, Peter Vamplew, Shimon Whiteson, and
Richard Dazeley.
A survey of multi-objective sequential
decision-making. JAIR, 48:67–113, 2013. 2
[38] Robin Rombach, Andreas Blattmann, Dominik Lorenz,
Patrick Esser, and Bj¨orn Ommer. High-resolution image syn-
thesis with latent diffusion models. In CVPR, 2022. 2, 5, 6,
8, 1, 4, 7
[39] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net:
Convolutional networks for biomedical image segmentation.
In MICCAI, 2015. 3
[40] Chitwan Saharia, William Chan, Huiwen Chang, Chris Lee,
Jonathan Ho, Tim Salimans, David Fleet, and Mohammad
Norouzi. Palette: Image-to-image diffusion models. In SIG-
GRAPH, 2022. 2
[41] Chitwan Saharia, William Chan, Saurabh Saxena, Lala
Li, Jay Whang, Emily L Denton, Kamyar Ghasemipour,
Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans,
et al. Photorealistic text-to-image diffusion models with deep
language understanding. In NeurIPS, 2022. 2
[42] Christoph Schuhmann, Romain Beaumont, Richard Vencu,
Cade Gordon,
Ross Wightman,
Mehdi Cherti,
Theo
Coombes, Aarush Katta, Clayton Mullis, Mitchell Worts-
man, et al. Laion-5b: An open large-scale dataset for training
next generation image-text models. NeurIPS, 2022. 5
[43] Alessio Serra, Fabio Carrara, Maurizio Tesconi, and Fabrizio
Falchi. The emotions of the crowd: Learning image senti-
ment from tweets via cross-modal distillation. ECAI, 2023.
2, 3, 5
[44] Christian Shelton. Balancing multiple sources of reward in
reinforcement learning. In NeurIPS, 2000. 2
[45] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan,
and Surya Ganguli.
Deep unsupervised learning using
nonequilibrium thermodynamics. In ICML, 2015. 1
[46] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denois-
ing diffusion implicit models. In ICLR, 2021. 5
[47] Gerald Tesauro, Rajarshi Das, Hoi Chan, Jeffrey Kephart,
David Levine, Freeman Rawson, and Charles Lefurgy. Man-
aging power consumption and performance of computing
systems using reinforcement learning. In NeurIPS, 2007. 3
[48] Zhengzhong Tu, Hossein Talebi, Han Zhang, Feng Yang,
Peyman Milanfar, Alan Bovik, and Yinxiao Li.
Maxvit:
Multi-axis vision transformer. In ECCV, 2022. 3
[49] Kristof Van Moffaert and Ann Now´e. Multi-objective rein-
forcement learning using sets of pareto dominating policies.
JMLR, 15(1):3483–3512, 2014. 2
[50] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-
reit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia
Polosukhin. Attention is all you need. In NeurIPS, 2017. 5
[51] Jiazheng Xu, Xiao Liu, Yuchen Wu, Yuxuan Tong, Qinkai
Li, Ming Ding, Jie Tang, and Yuxiao Dong.
Imagere-
ward: Learning and evaluating human preferences for text-
to-image generation. In NeurIPS, 2023. 2, 3
[52] Zhenqiang Ying, Haoran Niu, Praful Gupta, Dhruv Mahajan,
Deepti Ghadiyaram, and Alan Bovik. From patches to pic-
tures (paq-2-piq): Mapping the perceptual space of picture
quality. In CVPR, 2020. 3
[53] Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gun-
jan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yin-
fei Yang, Burcu Karagol Ayan, Ben Hutchinson, Wei Han,
Zarana Parekh, Xin Li, Han Zhang, Jason Baldridge, and
Yonghui Wu. Scaling autoregressive models for content-rich
text-to-image generation. Transactions on Machine Learn-
ing Research, 2022. 6, 7, 1
[54] Lijun Yu, Yong Cheng, Kihyuk Sohn, Jos´e Lezama, Han
Zhang, Huiwen Chang, Alexander G Hauptmann, Ming-
Hsuan Yang, Yuan Hao, Irfan Essa, et al. Magvit: Masked
generative video transformer. In CVPR, 2023. 2
[55] Yufan Zhou,
Bingchen Liu,
Yizhe Zhu,
Xiao Yang,
Changyou Chen, and Jinhui Xu. Shifted diffusion for text-
to-image generation. In CVPR, 2023. 1

Parrot: Pareto-optimal Multi-Reward
Reinforcement Learning Framework for Text-to-Image Generation
Supplementary Material
This supplementary material provides:
• Sec. A: implementation details, including the training de-
tails, and details of quantitative experiments.
• Sec. B: more ablation studies on better tradeoff for differ-
ent rewards.
• Sec. C: more ablation studies on original prompt guidance
and training scheme of Parrot.
• Sec. D: more visual examples to show the advancements
of Parrot.
A. Implementation Details
Training Details. We conduct our experiments with Jax
implementation of Stable Diffusion 1.5 [38]. In terms of
diffusion-based RL, we sample 256 images per RL-tuning
iteration. For policy gradient updates, we accumulate gra-
dients across all denoising timesteps. Our experiments em-
ploy a small range of gradient clip 10−4. We keep negative
prompt as null text.
Details of Quantitative Experiments.
From Parti [53]
prompts, we generate images of dimensions 512 × 512.
In all experiments in the main paper, we apply reward-
specific preference expressed as “<Reward 1>, <Reward
2>, <Reward 3>, <Reward 4>”, which is optional to se-
lect one or several rewards. Reward models are aesthetics,
human preference, text-image alignment and image senti-
ment. During the inference stage, the guidance scale is also
set as 5.0 for the diffusion sampling process. We employ
the AdamW [22] as optimizer with β1 = 0.9, β2 = 0.999
and a weight decay of 0.1.
B. Better Tradeoff for Different Rewards
To verify that Parrot achieved better tradeoff for differ-
ent rewards, we generate 1000 images from common an-
imal dataset [4], where each text prompt consists of the
name of a common animal.
As shown in Fig. 9, us-
ing only text-image alignment with reward-specific pref-
erence “<Reward 3>” yields images with higher text-
image alignment score, while using only aesthetic model
with reward-specific preference “<Reward 1>” yields im-
ages with higher aesthetic score. In the case of using two
reward-specific preferences “<Reward 1>, <Reward 3>”,
we observe that scores are balanced and show that better re-
sults across multiple rewards compared with Parrot without
Pareto optimal selection.
C. Prompt Guidance and Training Scheme
Original Prompt Guidance. In Fig. 10, we provide addi-
tional ablation study of original prompt centered guidance
by adjusting the guidance scale w1 and w2, which deter-
mines visual changes from original prompt and expanded
prompt, respectively. We assigned 5 to both w1 and w2
in most of our experiments, which shows better text-image
alignment performance in Fig. 10.
Ablation Study on Training Scheme. Fig. 11 provides ab-
lation study comparing variation of the Parrot: Stable Diffu-
sion, the prompt expansion network (PEN) tuning only, T2I
model fine-tuning only, Parrot without joint optimization,
and Parrot. In the second column, without fine-tuning the
T2I diffusion model does not lead to significant improve-
ments in terms of texture and composition. Furthermore,
the third column demonstrates that fine-tuning the diffu-
sion model enhances texture and perspective, yet this im-
provement is hindered by the limited information in the text
prompt. We also observe that the quality of images from
joint optimization surpasses that of combining decoupled
generative models.
D. More Visual Examples
We show additional visual examples of Parrot in Figs. 12
to 17. Note that generated images from Parrot are improved
across multiple-criteria. Fig. 12 highlights examples where
the Parrot brings improvements in aesthetics. For example,
Parrot effectively addresses issues such as poor cropping in
the fourth column and improves color in the fifth column.
Fig. 13 presents examples of images with improved human
preference score generated by Parrot. In Fig. 14, we pro-
vide examples of improved text-image alignment achieved
by Parrot. Fig. 15 shows examples where Parrot enhances
image sentiment, producing emotionally rich images.
Finally, additional comparison results between diffusion-
based RL baselines are described in Fig. 16, and Fig. 17.
Diffusion-based RL baselines are listed: Stable Diffusion
1.5 [38], DPOK [10] with weighted sum of multiple reward
scores, Promptist [9], Parrot without prompt expansion, and
Parrot. For Parrot without prompt expansion, we only take
original prompt as input.

Figure 9. Comparing Parrot with/without Pareto Optimal. When we apply Pareto-optimal selection, we observe that each reward score
increases based on reward-specific preference.

“Oil painting of a deer” →”Oil 
painting of a deer in a forest. the 
deer is standing in the middle of 
the painting, and the forest is 
behind it. the deer is brown, and 
the forest is green. the painting is 
done in a realistic style, and the 
texture is smooth. the 
atmosphere of the painting is 
peaceful and serene.”
“A teapot” →”a teapot with a 
textured surface and a warm 
atmosphere.”
“A parrot” →”A parrot with a 
green and blue body and a yellow 
beak. The parrot is sitting on a 
branch. The background is a blue 
sky with white clouds.”
“A clock” →”A clock with a 
wooden frame and a white face. 
The clock is sitting in a room 
with a white wall. The room is lit 
by a single light bulb”
“A robot cooking” →”A robot is 
cooking in a kitchen. The robot 
has a chef's hat on. The kitchen is 
clean and well-organized. The 
robot is cooking a meal for a 
family of four. The meal is a roast 
chicken, potatoes, and green 
beans. The robot is carefully 
preparing the meal and takes 
pride in its work.”
Figure 10. Original prompt centered guidance. We present visual comparison of 5 different pairs of w1 and w2 to demonstrate the
effectiveness of guidance scales. For all experiments, we assign w1 = 5 and w2 = 5 (3rd row) for the best performance.

SD 1.5
T2I Model 
Tuning Only
Parrot w/o 
Joint Optimization
PEN Tuning Only
Parrot
A tree with a beautiful sky and 
clouds
A tree
A tree with a beautiful sky and 
clouds
A tree
A tree with a beautiful sky
A crown
3d render of a golden crown 
with a red jewel on a white 
background.
3d render of a golden crown 
with a red jewel on a white 
background.
A crown
A crown of flowers on a white 
background. The flowers are 
arranged in a circle and the 
crown is topped with a red 
rose. The background is a light 
blue color and the flowers are 
a variety of colors. The image 
is both beautiful and elegant.
The city of London
The city of London at night 
with a beautiful view of the 
city lights.
The city of London at night 
with a beautiful view of the 
city lights.
The city of London
The city of London is a 
beautiful city with a rich 
history. The city is also known 
for its vibrant nightlife.
A cartoon house with red roof
3d render of a cartoon house 
with a red roof and a blue sky
A cartoon house with red roof
 A cartoon house with a red 
roof and a blue sky
3d render of a cartoon house 
with a red roof and a blue sky
Figure 11. Visual examples of Parrot under different settings. From left to right, we provide results of Stable Diffusion 1.5 [38], the only
fine-tuned PEN, the only fine-tuned T2I diffusion model, the Parrot without joint optimization, and the Parrot.

Stable Diffusion 1. 5
Parrot
Aesthetics
↑
“A glowing magical portal inside a big 
wave made of sand fantasy desert”
“A cocktail”
“An F1 race car in a Manhattan 
street”
“A fantasy book style portrait of 
a giant dragon”
“Suddenly there is a peach blossom 
forest”
Figure 12. More Examples of aesthetics improvement from the Parrot. Given the text prompt, we generate images with Stable Diffusion
and Parrot. After fine-tuning, the Parrot alleviates quality issues such as poor composition (e.g. bad cropping), misalignment with the
user input (e.g. missing objects), or generally less aesthetic pleasing.
Stable Diffusion 1. 5
Parrot
Human Preference
↑
“Sculptures of two lovers walking 
through paris”
“A small house with trees in the 
background”
“A pick-up truck”
“Legendary tower of light and 
magic”
“An Armchair”
Figure 13. More Examples of human preference improvement from the Parrot. Given the text prompt, we generate images with Stable
Diffusion 1.5 [38] and Parrot.

Stable Diffusion 1. 5
Parrot
Text-Image Alignment
↑
“A pineapple surfing on a wave”
“A room with two chairs and a 
painting of the Statue of Liberty”
“Inside of orange”
“A heart made of wood”
“A corgi’s head depicted as an 
explosion of a nebula”
Figure 14. More examples of text-image alignment improvement from the Parrot. Given the text prompt, we generate images with the
Stable Diffusion 1.5 [38] and the Parrot.
Stable Diffusion 1. 5
Parrot
Image Sentiment
↑
“Cute cat sticker”
“A pig face”
“A tiny cute cyberpunk monster 
metallic bodysuit big eyes smiling 
waving.”
“Primitive drawing of a heart with 
a smiley face in the middle”
“Cute little anthropomorphic furry 
dog.”
Figure 15. More examples of image sentiment improvement from the Parrot. Given the text prompt, we generate images with the Stable
Diffusion 1.5 [38] and the Parrot.

SD 1.5
Promptist
Parrot w/o PE
Abstract 
geometric 
cityscape 
Abstract 
geometric 
cityscape 
Abstract geometric cityscape, 
hyperdetailed, artstation, 
cgsociety, 8k
Abstract 
geometric 
cityscape 
DPOK
Parrot
Abstract geometric cityscape 
with a beautiful sunset.
A painting of the city of 
nuremberg by john blanche in a 
realistic style with a lot of 
detail and a warm color palette
A painting of the city of 
nuremberg by john blanche
A painting of the city of 
nuremberg by john blanche, 
trending on artstation
A painting of the city of 
nuremberg by john blanche
A painting of the city of 
nuremberg by john blanche
A mix of paris and favelas. 
A mix of paris and favelas. 
 A mix of paris and favelas, 
highly detailed, digital painting, 
artstation, concept art, sharp 
focus, illustration, art by greg 
rutkowski and alphonse mucha
A mix of paris and favelas. 
A mix of paris and favelas. a 
beautiful city with a lot of 
colors.
A large magical beast called an 
owlbear in the sunset
A large magical beast called 
an owlbear
A large magical beast called an 
owlbear, highly detailed, digital 
painting, artstation, concept art, 
sharp focus, illustration, art by 
greg rutkowski and alphonse 
mucha
A large magical beast called 
an owlbear
A large magical beast called 
an owlbear
Figure 16. More results from the Parrot and baselines: Stable Diffusion 1.5 [38], DPOK [10] with weighted sum, Promptist [9], Parrot
without prompt expansion, and Parrot.

SD 1.5
Promptist
Parrot w/o PE
A dungeon labyrinth
A dungeon labyrinth
A dungeon labyrinth, RPG 
Reference, Oil Painting, 
Trending on Artstation, 
octane render, Insanely 
Detailed, 8k, HD
A dungeon labyrinth
DPOK
Parrot
A dungeon labyrinth with a 
dark and mysterious 
atmosphere
A red carpet floor with a 
beautiful floral pattern and a 
chandelier hanging from the 
ceiling.
A red carpet floor
A red carpet floor, a fantasy 
digital painting by Greg 
Rutkowski and James Gurney, 
trending on Artstation, highly 
detailed
A red carpet floor
A red carpet floor
Studio photo of a 
futuristic cute robot
Studio photo of a futuristic 
cute robot
Studio photo of a 
futuristic cute robot
Studio photo of a 
futuristic cute robot
Studio photo of a futuristic 
cute robot with a blue and 
white light.
A tree house in the jungle with 
a beautiful view of the 
mountains
A tree house in the jungle
A tree house in the jungle, highly 
detailed, digital painting, 
artstation, concept art, sharp 
focus, illustration, art by greg 
rutkowski and alphonse mucha
A tree house in the jungle
A tree house in the jungle
Figure 17. More results from the Parrot and baselines: Stable Diffusion 1.5 [38], DPOK [10] with weighted sum, Promptist [9], Parrot
without prompt expansion, and Parrot.

