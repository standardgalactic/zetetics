Monte Carlo Methods and Applications, 223–233
© De Gruyter 2013
Chapter 24
Exact Sampling for the Ising Model at All
Temperatures
Mario Ullrich
Abstract. The Ising model is often referred to as the most studied model of statistical physics.
It describes the behavior of ferromagnetic material at different temperatures. It is also an in-
teresting model for mathematicians, because although the Gibbs distribution is continuous in
the temperature parameter, the behavior of the usual single-spin dynamics to sample from this
measure varies extremely. Namely, there is a critical temperature where we get rapid mixing
above and slow mixing below this value. In this chapter we give a survey of the known results
on mixing time of Glauber dynamics for the Ising model on the square lattice and present a
technique that makes exact sampling of the Ising model at all temperatures possible in poly-
nomial time. At high temperatures this is well known, but we have not found any reference
where exact sampling for the Ising model at low temperatures in polynomial time is described.
Keywords. Ising Model, Exact Sampling, Random Cluster Model.
Mathematics Subject Classiﬁcation 2010. 60J10, 82C44, 65C40.
24.1
Introduction
In this chapter we summarize the known results on the mixing time of the heat bath dy-
namics for the Ising model and combine them with some graph theoretic results to an
algorithm for exact sampling from the Ising model in polynomial time. By time (or run-
ning time) we always mean the number of steps of the underlying Markov chain. The
algorithm that will be analyzed (Algorithm 24.2, given in Section 24.5) is at high tem-
peratures simply the Coupling from the past algorithm (see Propp and Wilson [18]).
At low temperatures we have to produce a sample at the dual graph, but this can be
traced back to sampling on the initial graph with constant boundary condition.
The main theorem of this article is stated as follows.
Theorem 24.1. Let GL be the square lattice with N D L2 vertices. Then, Algo-
rithm 24.2 outputs an exactly distributed Ising conﬁguration with respect to GL
ˇ
in
expected time smaller than
 cˇ N .log N/2
for ˇ ¤ ˇc D log.1 C
p
2/ and some cˇ > 0
 16 N C log N
for ˇ D ˇc and some C > 0.

224
Mario Ullrich
As a consequence we get that one can estimate the expectation of arbitrary functions
with respect to the Gibbs distribution in polynomial time. Namely, if we use the sim-
ple Monte Carlo method to approximate the expectation of a function f on the Ising
model, we need 
2kf  Eˇf k2
2 exact samples from ˇ (i. e., Algorithm 24.2) to
reach a mean square error of at most 
2. Therefore, if we denote the bounds from The-
orem 24.9 by Tˇ, we need on average Tˇ 
2kf Eˇf k2
2 steps of the Markov chain
that will be deﬁned in Section 24.2.
The ﬁrst polynomial-time algorithm (FPRAS) was shown by Jerrum and Sinclair
[10]. There they present an algorithm to approximate the partition function Zˇ of the
Ising model on arbitrary graphs and, as a consequence, approximate expectations of
functions that are given in terms of the partition function in polynomial time at all
temperatures ˇ.
24.2
The Ising Model
In this section we introduce the two-dimensional Ising model. Let G D .V; E/ be a
graph with ﬁnite vertex set V 
 Z2 and edge set E D ¹¹u; vº 2
V
2

W ju  vj D 1º,
where
V
2

is the set of all subsets of V with 2 elements. From now, N WD jV j. We
are interested in the square lattice, i. e., V D ¹1; : : : ; Lº2 for some L D
p
N 2 N,
because it is the most widely used case. We denote the induced graph by GL.
The Ising model on GL is now deﬁned as the set of possible conﬁgurations IS D
¹1; 1ºV , where  2 IS is an assignment of 1 or 1 to each vertex in V , together
with the probability measure
ˇ./ WD GL
ˇ
./ D
1
Zˇ
exp
²
ˇ
X
u;vW u$v 1

.u/ D .v/
³
;
where u $ v means u and v are neighbors in GL, Z is the normalization constant and
ˇ 	 0 is the inverse temperature. This measure is called the Gibbs distribution with
free boundary condition.
Additionally we need the notion of boundary conditions, but we restrict ourself here
to the “all plus” and “all minus” case.
Let V c D Z2 n V . Then we denote the lattice GL together with the probability
measure
˙
ˇ ./ WD GL;˙
ˇ
./ D
1
eZˇ
GL
ˇ
./  exp
²
ˇ
X
v2V; u2V cW
u$v
1

.v/ D ˙1
³
by the Ising model with plus/minus boundary condition, respectively. One can imagine
that this corresponds to the Ising model on GL with a strip of ﬁxed spins around, so
every vertex in GL has the same number of neighbors.

Chapter 24 Exact Sampling for the Ising Model at All Temperatures
225
In 1944 Onsager [17] proved that there is a phase transition at ˇ D ˇc WD ln.1C
p
2/
in the case where V D Z2, and we will see that this value is also important for ﬁnite
lattices. Namely, the dynamics deﬁned below is rapidly mixing if and only if ˇ  ˇc.
We will use the so-called heat bath dynamics. These dynamics deﬁne a irreducible,
aperiodic and reversible Markov chain Xˇ D .Xˇ
i /i2N with stationary distribution
ˇ by the transition matrix
P.; v;
/ D
1
N
 
1 C
ˇ./
ˇ.v;
/
!1
 2 IS;
v 2 V;
where v;
 with  2 ¹1; 1º is deﬁned by v;
.v/ D  and v;
.u/ D .u/, u ¤ v.
The interpretation of this algorithm is very simple. In each step, choose a random
v 2 V and assign a new value to v according to ˇ conditioned on all neighbors of v.
Note that the results of this chapter hold in general for all Glauber dynamics as deﬁned
in [6] which admit a monotone Coupling (see Section 24.3). For a general introduction
to Markov chains see, e. g., [11], or [13] in the context of spin systems.
In the following we want to estimate how fast such a Markov chain converges to
its stationary distribution. Therefore we ﬁrst introduce the total variation distance to
measure the distance between two probability measures  and , which is deﬁned by
k  kTV D 1
2
X
	2IS
j./  ./j :
Now we can deﬁne the mixing time of the Markov chain with transition matrix P and
stationary distribution ˇ by
ˇ D min
²
n W max
	2IS
kP n.; /  ˇ./kTV  1
2e
³
:
This is the expected time the Markov chain needs to get close to its stationary dis-
tribution. In fact, one can bound the spectral gap of the transition matrix P in either
direction in terms of the mixing time (see, e. g., [11, Theorems 12.3 and 12.4]), so one
can bound the error of a MCMC algorithm to integrate functions over IS, as one can
read in [19]. Furthermore, if the Markov chain is rapidly mixing (i. e., the mixing time
is bounded by a polynomial in the logarithm of size of the state space IS) we get
that the problem of integration (with an unnormalized density) on the Ising model is
tractable (see also [16]). Unfortunately there is no Markov chain that is proven to be
rapidly mixing at all temperatures.
However, in this chapter we are interested in sampling exactly from the station-
ary distribution, but ﬁrst we present the known mixing-time results for the Glauber
dynamics for the Ising model. For proofs or further details we refer to the particular
articles or the survey of Martinelli [13]. Of course, we can only give a small selection
of references, because there are many papers leading to the results given below.

226
Mario Ullrich
Theorem 24.2 ([14]). Let ˇ < ˇc. Then there exists a constant cˇ > 0 such that
the mixing time of the Glauber dynamics for the Ising model with arbitrary boundary
condition on GL satisﬁes
ˇ  cˇ N log N:
Theorem 24.3 ([2]). Let ˇ > ˇc. Then there exists a constant cˇ > 0 such that the
mixing time of the Glauber dynamics for the Ising model on GL satisﬁes
ˇ 	 ecˇL:
The results above can be obtained by observing that some spatial mixing property
of the measure ˇ is equivalent to the mixing in time of the Glauber dynamics. For
details for this interesting fact, see [3].
The constant cˇ of Theorem 24.2 is widely believed to diverge if ˇ tends to ˇc.
Determining the mixing time in the case ˇ D ˇc was a challenging problem for a long
time. It was solved by Lubetzky and Sly in their recent paper [12].
Theorem 24.4 ([12]). There exists a constant C > 0 such that the mixing time of the
Glauber dynamics for the Ising model on GL at the critical temperature satisﬁes
ˇc  4 N C :
Remark 24.5. We give here only a brief description of the constant C, which can be
given explicitly. For more details see [12, p.19].
However, numerical experiments on the “true” exponent suggest that C  3:08
(see, e. g., [15,20] and note the explanation below).
The constant C in Theorem 24.4 is given by
C D 2 C log3=2

2
1  pC

:
(24.1)
Here, pC is the limiting vertical crossing probability in the random cluster model on
a fully-wired rectangle, where the width of the lattice is 3 its height. The C, as given
here, differs from the one given in [12] by eliminating a factor of 2 in front of the log
term and by the additional 2. The reason is that we state their result in terms of N
and not in the side length L of the lattice (therefore without factor 2) and that we are
interested in the discrete time single-spin algorithms. Therefore we get an additional

Chapter 24 Exact Sampling for the Ising Model at All Temperatures
227
factor N in their spectral gap result ( [12, Thm. 1]) and a factor N by (see, e. g., [11])
ˇc  log

e
min	 ˇc./

gap.Xˇc/1  4 N gap.Xˇc/1;
because min	 ˇc./ 	 exp.3N/.
The results of this section show that the Glauber dynamics is mixing rapidly for
ˇ  ˇc, but very slowly mixing for larger ˇ. In Section 24.4 we will see how to avoid
this problem.
24.3
Exact Sampling
In this section we brieﬂy describe the so-called Coupling from the past algorithm
(CFTP) to sample exactly from the stationary distribution of a Markov chain.
This algorithm works under weak assumptions on the Markov chain for every ﬁnite
state space and every distribution, but to guarantee that the algorithm is efﬁcient we
need some monotonicity property of the model and that the chain is rapidly mixing.
For a detailed description of CFTP and the proof of correctness see [18].
We restrict ourself to the heat bath dynamics for the Ising model. First note that the
heat bath dynamics, as deﬁned above, admits a monotone Coupling, that is, given two
realizations of the heat bath chain X D .Xt/t2N and Y D .Yt/t2N, there exists a
Coupling .X; Y / (i. e., using the same random numbers) such that
Xt  Yt
H)
XtC1  YtC1
for all t 2 N;
where  means smaller or equal at each vertex.
Additionally we know that 1    1 for all  2 IS, where 1 D .1/V and
1 D .1/V . Therefore if we set X0 D 1 and Y0 D 1 we know that X0    Y0
for all  and so Xt  Zt  Yt for the realization Z D .Zt/t2N with Z0 D . Since
this holds for all , one can choose Z0  ˇ and we get that whenever Xt and Yt
coalesce, they also coalesce with Zt which has the right distribution.
Having presented the idea of the algorithm, we will now state the algorithm in detail.
Note that the algorithm is called Coupling from the past, because we run the chains
from the past to the present. The algorithm CFTP.G; ˇ/ to sample from the distribution
G
ˇ works as described in Algorithm 1.
We denote the algorithm by CFTP˙.G; ˇ/ if we sample with respect to ˙
ˇ , i. e.,
with plus/minus boundary condition.
See [9] for examples that show that it is necessary to go from the past in the future
and that we have to reuse the random numbers.
Now we state the connection between the expected running time of the CFTP algo-
rithm and the mixing time of the Markov chain.

228
Mario Ullrich
Algorithm 24.1. Coupling from the past.
Input: The graph G D .V; E/ and the value of ˇ
Output: An Ising conﬁguration   ˇ
1:
procedureCFTPG; ˇ
2:
Set t D 0
3:
Set X0 D 1 and Y0 D 1
4:
while X0 ¤ Y0
5:
t D t C 1
6:
Generate random numbers U2tC1; : : : ; U2t1 that are sufﬁcient to run
the Markov chain. (e. g., Ui  Uniform ¹V  Œ0; 1º)
7:
Set X2tC1 D 0 and Y2tC1 D 1 and run the chains until time 0 by using
only the random numbers U2tC1; : : : ; U1
8:
end while
9:
return  D X0
10:
end procedure
Proposition 24.6 ([18]). Let Tˇ be the expected running time of CFTP.G; ˇ/ from
Algorithm 24.1 with G D .V; E/ and jV j D N. Then
Tˇ  4 ˇ log N;
where ˇ is the mixing time of the underlying Markov chain.
We see that exact sampling from the Gibbs distribution is efﬁcient whenever the
Markov chain is rapidly mixing. By the results of Section 24.2 we know that this is
the case for ˇ  ˇc. In the case ˇ > ˇc we need a different technique to generate
exact samples. Therefore we need essentially the so called random cluster model, as
we will see in the next section.
24.4
The Random Cluster Model
The random cluster model (also known as the FK model) was introduced by Fortuin
and Kasteleyn in [5] to study lattice spin systems with a graph structure. It is deﬁned
on a graph G D .V; E/ by its state space RC D ¹! W !  Eº and the RC measure
p.!/ D
1
Z pj!j .1  p/jEjj!j 2C.!/;
where p 2 .0; 1/, Z is the normalization constant and C.!/ is the number of connected
components in the graph .V; !/. For a detailed introduction and related topics see [7].

Chapter 24 Exact Sampling for the Ising Model at All Temperatures
229
×
×
×
×
×
×
×
×
×
×
Figure 24.1. Left: The graph G3 (solid lines) and its dual (dashed lines). Right: A RC state on
G3 (solid lines) and its dual conﬁguration (dashed lines).
There is a tight connection between the Ising model and the random cluster model.
Namely, if we set p D 1  eˇ, we can translate an Ising conﬁguration   ˇ to a
random cluster state !  p and vice versa. To get an Ising conﬁguration  2 IS
from ! 2 RC assign independent and uniformly random spins to each connected
component of !. For the reverse way include all edges e D ¹e1; e2º 2 E with .e1/ D
.e2/ to ! with probability p. For details see [4].
Therefore sampling an Ising conﬁguration according to ˇ is equivalent to sampling
a RC state from p whenever both models are deﬁned on the same graph G and p D
1  eˇ.
Another important concept in connection with the RC model is the duality of graphs
(see, e. g., [8]). Let G D .V; E/ be a ﬁnite, planar graph, i. e., without intersecting
edges if we draw it in the plane (like our GL). The dual graph G D .V ; E/ of G
is constructed as follows. Put a vertex in each face (including the inﬁnite outer one) of
the graph and connect two vertices by an edge if and only if the corresponding faces
of G share a boundary edge. It is clear, that the number of vertices can differ in the
dual graph, but we have the same number of edges.
Additionally we deﬁne a dual conﬁguration !  E in G to a RC state !  E
in G by
e 2 ! ” e … !;
where e is the edge in E that “crosses” e. (By the construction, this edge is unique.)
See Figure 24.1 for the graph GL with L D 3 and its dual graph G
L together with
2 corresponding RC states.
Now we can state the following theorem about the relation of the distribution of a
RC state and its dual (see [8]).
Proposition 24.7 ([8, p. 164]). Let G D .V; E/ be a ﬁnite, planar graph and p be
the random cluster measure on G. Furthermore let G D .V ; E/ be the dual graph
of G and 
p be the random cluster measure on G.

230
Mario Ullrich
Then
!  p
”
!  
p;
where
p D 1 
p
2  p :
(24.2)
Obviously, .p/ D p. By Proposition 24.7 one can see that sampling from p and
sampling from 
p is equivalent. It is straightforward to get the following proposition.
Proposition 24.8. Sampling from the Gibbs distribution G
ˇ is equivalent to sampling
from the Gibbs distribution G
ˇ  , where
ˇ D log

coth ˇ
2

:
(24.3)
Additionally,
ˇ > ˇc
”
ˇ < ˇc:
Note that the relations of (24.2) and (24.3) can be alternatively stated as
0 D 2  2p  2p C pp
and
0 D 1 C eˇ C eˇ  C eˇCˇ :
Proof. The equivalence was shown by the above procedure, i. e., if we want to sample
from G
ˇ , we can sample from G
ˇ  , generate a RC state with respect to 
p, go to
the dual lattice with measure p, and ﬁnally generate a state according to G
ˇ . Since
p./ D 1  eˇ ./, the formula for ˇ comes from
ˇ D  log.1  p/ (24.2)
D
log
2  p
p

D log

coth ˇ
2

:
This proves the statement.
24.5
Exact Sampling for the Ising Model
In this section we present an algorithm to sample exactly from the a Gibbs distribution.
But, before we prove that it is efﬁcient, we state our sampling algorithm.
Therefore we ﬁrst have to explain what the graph G
L looks like. It is easy to obtain
(see Figure 24.1) that G
L D .V 
L ; E
L/ is also a square lattice with .L  1/2 vertices
and an additional auxiliary vertex v, which is connected to every vertex on the bound-
ary of it. We denote the operation of adding a vertex to a graph and connect it to all
boundary vertices by [b. So G
L D GL1 [b v.

Chapter 24 Exact Sampling for the Ising Model at All Temperatures
231
Algorithm 24.2. Sampling from the Ising model on the square lattice.
Input: An integer L and the value of ˇ
Output: An Ising conﬁguration   GL
ˇ
1:
if ˇ  ˇc then
2:
 D CFTP.GL; ˇ/
3:
else
4:
e D CFTPC.GL1; ˇ/, where ˇ is given in (24.3)
5:
Deﬁne a Ising conﬁguration  on G
L D GL1 [b v by .v/ D e.v/ on
V.GL1/ and .v/ D 1.
6:
Generate a RC state ! from 
7:
Take the dual RC state ! D .!/
8:
Generate an Ising conﬁguration  from !
9:
end if
10:
return 
Theorem 24.9. Let GL be the square lattice with N D L2 vertices. Then, the al-
gorithm from above outputs an exactly distributed Ising conﬁguration with respect to
GL
ˇ
in expected time smaller than
 cˇ N .log N/2
for ˇ ¤ ˇc D log.1 C
p
2/ and some cˇ > 0I
 16 N C log N
for ˇ D ˇc, where C is given in (24.1).
Proof. The running time of the algorithm follows directly from Theorems 24.2
and 24.4 and Proposition 24.6. Therefore we only have to prove that the output 
of the algorithm has the right distribution. In the case of ˇ  ˇc this is obvious. For
ˇ > ˇc we know from Proposition 24.8 that   GL
ˇ
, if the dual conﬁguration 
on G
L (line 5 of Algorithm 24.2) is distributed according to ˇ  WD 
G
L
ˇ  . But by
the construction of lines 4 and 5 of Algorithm 24.2, this is true. For this, note that
ˇ./ D ˇ./ for all  2 IS. We get that for each vertex v 2 V (especially for
v)
ˇ./ D ˇ

 \ ¹ W .v/ D 1º

C ˇ

 \ ¹ W .v/ D 1º

D ˇ

¹ W .v/ D 1º

ˇ

 ¹ W .v/ D 1º

C ˇ

¹ W .v/ D 1º

ˇ

 ¹ W .v/ D 1º

D 1
2
h
ˇ

 ¹ W .v/ D 1º

C ˇ

 ¹ W .v/ D 1º
i
D 1
2 ˇ

¹; º ¹ W .v/ D 1º

:

232
Mario Ullrich
The last equality comes from the fact that
ˇ

 ¹ W .v/ D 1º

D ˇ

 ¹ W .v/ D 1º

:
Therefore we can sample from ˇ on G
L by sampling  from the conditional measure
ˇ. j ¹ W .v/ D 1º/ and then choose with probability 1
2 either  or . If we
now use that G
L D GL1 [b v one can see that sampling on G
L with respect to
ˇ. j ¹ W .v/ D 1º/ is the same as sampling e from GL1;C
ˇ
and setting
.v/ D
´
e.v/
v 2 V.GL1/;
1
v D v:
Note that we omit the step of choosing  or  with probability 1
2, because the RC
state that will be generated would be the same.
This completes the proof.
Remark 24.10. Note that the same technique works also for the q-state Potts model.
This model consists of the state space P D ¹1; : : : ; qºV and the same measure ˇ.
In this case we consider the random cluster measure
p;q.!/ D
1
Z pj!j .1  p/jEjj!j qC.!/
and the connection of the models is again given by p D 1  eˇ.
A recent result of Beffara and Duminil-Copin [1] shows that the self-dual point of
the RC model corresponds to the critical temperature of the Potts model ˇc.q/ D
ln.1 C pq/ in the same way as in the case q D 2 (i. e., the Ising case). Therefore,
a sampling algorithm for the Potts model above (and at) the critical temperature is
enough to sample at all temperatures.
Acknowledgments. The author was supported by the DFG GK 1523.
References
[1] V. Beffara and H. Duminil-Copin, The self-dual point of the two-dimensional random-
cluster model is critical for q 	 1, Probab. Theory Related Fields 153 (2012), 511–542.
[2] F. Cesi, G. Guadagni, F. Martinelli, and R. H. Schonmann, On the two-dimensional
stochastic Ising model in the phase coexistence region near the critical point, J. Statist.
Phys. 85 (1996), 55–102.
[3] M. Dyer, A. Sinclair, E. Vigoda, and D. Weitz, Mixing in time and space for lattice spin
systems: a combinatorial view, Random Structures Algorithms 24 (2004), 461–479.
[4] S. F. Edwards and A. D. Sokal, Generalization of the Fortuin-Kasteleyn-Swendsen-Wang
representation and Monte Carlo algorithm, Phys. Rev. D 38 (1988), 2009–2012.

Chapter 24 Exact Sampling for the Ising Model at All Temperatures
233
[5] C. M. Fortuin and P. W. Kasteleyn, On the random-cluster model. I. Introduction and
relation to other models, Physica 57 (1972), 536–564.
[6] R. J. Glauber, Time-dependent statistics of the Ising model, J. Mathematical Phys. 4
(1963), 294–307.
[7] G. Grimmett, The random-cluster model, Grundlehren der Mathematischen Wis-
senschaften 333, Springer, Berlin, 2006.
[8] G. Grimmett, Probability on Graphs, IMS Textbooks Series 1, Cambridge University
Press, Cambridge, 2010.
[9] O, Häggström, Finite Markov chains and algorithmic applications, London Mathemati-
cal Society Student Texts 52, Cambridge University Press, Cambridge, 2002.
[10] M. Jerrum and A. Sinclair, Polynomial-time approximation algorithms for the Ising
model, SIAM J. Comput. 22 (1993), 1087–1116.
[11] D. A. Levin, Y. Peres, and E. L. Wilmer, Markov chains and mixing times. Ameri-
can Mathematical Society, Providence, RI, 2009, With a chapter by J. G. Propp and
D. B. Wilson.
[12] E. Lubetzky and A. Sly, Critical Ising on the square lattice mixes in polynomial time,
Comm. Math. Phys. 313 (2012), 815–836.
[13] F. Martinelli, Lectures on Glauber dynamics for discrete spin models, in: Lectures on
probability theory and statistics (Saint-Flour, 1997), pp. 93–191, Lecture Notes in Math-
ematics 1717, Springer, Berlin, 1999.
[14] F. Martinelli and E. Olivieri, Approach to equilibrium of Glauber dynamics in the one
phase region. I. The attractive case, Comm. Math. Phys. 161 (1994), 447–486.
[15] M. P. Nightingale and H. W. J. Blöte, Dynamic Exponent of the Two-Dimensional Ising
Model and Monte Carlo Computation of the Subdominant Eigenvalue of the Stochastic
Matrix, Physical Review Letters 76 (1996), 4548–4551.
[16] E. Novak and H. Wo´zniakowski, Tractability of multivariate problems. Volume II: Stan-
dard information for functionals, EMS Tracts in Mathematics 12. European Mathemati-
cal Society (EMS), Zürich, 2010.
[17] L. Onsager, Crystal statistics. I. A two-dimensional model with an order-disorder transi-
tion, Phys. Rev. 65 (1944), 117–149.
[18] J. G. Propp and D. B. Wilson, Exact sampling with coupled Markov chains and applica-
tions to statistical mechanics, Random Structures Algorithms 9 (1996), 223–252.
[19] D. Rudolf, Explicit error bounds for lazy reversible Markov chain Monte Carlo, J. Com-
plexity 25 (2009), 11–24.
[20] F. Wang, Naomichi Hatano and Masuo Suzuki, Study on dynamical critical exponents of
the Ising model using the damage spreading method, J. Phys. A: Math. Gen. 28 (1995),
4543–4552.
Author information
Mario Ullrich, Friedrich Schiller University, Jena, Germany.
Email: mario.ullrich@uni-jena.de


