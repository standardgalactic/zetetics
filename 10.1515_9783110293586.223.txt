Monte Carlo Methods and Applications, 223â€“233
Â© De Gruyter 2013
Chapter 24
Exact Sampling for the Ising Model at All
Temperatures
Mario Ullrich
Abstract. The Ising model is often referred to as the most studied model of statistical physics.
It describes the behavior of ferromagnetic material at different temperatures. It is also an in-
teresting model for mathematicians, because although the Gibbs distribution is continuous in
the temperature parameter, the behavior of the usual single-spin dynamics to sample from this
measure varies extremely. Namely, there is a critical temperature where we get rapid mixing
above and slow mixing below this value. In this chapter we give a survey of the known results
on mixing time of Glauber dynamics for the Ising model on the square lattice and present a
technique that makes exact sampling of the Ising model at all temperatures possible in poly-
nomial time. At high temperatures this is well known, but we have not found any reference
where exact sampling for the Ising model at low temperatures in polynomial time is described.
Keywords. Ising Model, Exact Sampling, Random Cluster Model.
Mathematics Subject Classiï¬cation 2010. 60J10, 82C44, 65C40.
24.1
Introduction
In this chapter we summarize the known results on the mixing time of the heat bath dy-
namics for the Ising model and combine them with some graph theoretic results to an
algorithm for exact sampling from the Ising model in polynomial time. By time (or run-
ning time) we always mean the number of steps of the underlying Markov chain. The
algorithm that will be analyzed (Algorithm 24.2, given in Section 24.5) is at high tem-
peratures simply the Coupling from the past algorithm (see Propp and Wilson [18]).
At low temperatures we have to produce a sample at the dual graph, but this can be
traced back to sampling on the initial graph with constant boundary condition.
The main theorem of this article is stated as follows.
Theorem 24.1. Let GL be the square lattice with N D L2 vertices. Then, Algo-
rithm 24.2 outputs an exactly distributed Ising conï¬guration with respect to GL
Ë‡
in
expected time smaller than
 cË‡ N .log N/2
for Ë‡ Â¤ Ë‡c D log.1 C
p
2/ and some cË‡ > 0
 16 N C log N
for Ë‡ D Ë‡c and some C > 0.

224
Mario Ullrich
As a consequence we get that one can estimate the expectation of arbitrary functions
with respect to the Gibbs distribution in polynomial time. Namely, if we use the sim-
ple Monte Carlo method to approximate the expectation of a function f on the Ising
model, we need 
2kf  EË‡f k2
2 exact samples from Ë‡ (i. e., Algorithm 24.2) to
reach a mean square error of at most 
2. Therefore, if we denote the bounds from The-
orem 24.9 by TË‡, we need on average TË‡ 
2kf EË‡f k2
2 steps of the Markov chain
that will be deï¬ned in Section 24.2.
The ï¬rst polynomial-time algorithm (FPRAS) was shown by Jerrum and Sinclair
[10]. There they present an algorithm to approximate the partition function ZË‡ of the
Ising model on arbitrary graphs and, as a consequence, approximate expectations of
functions that are given in terms of the partition function in polynomial time at all
temperatures Ë‡.
24.2
The Ising Model
In this section we introduce the two-dimensional Ising model. Let G D .V; E/ be a
graph with ï¬nite vertex set V 
 Z2 and edge set E D Â¹Â¹u; vÂº 2
V
2

W ju  vj D 1Âº,
where
V
2

is the set of all subsets of V with 2 elements. From now, N WD jV j. We
are interested in the square lattice, i. e., V D Â¹1; : : : ; LÂº2 for some L D
p
N 2 N,
because it is the most widely used case. We denote the induced graph by GL.
The Ising model on GL is now deï¬ned as the set of possible conï¬gurations IS D
Â¹1; 1ÂºV , where  2 IS is an assignment of 1 or 1 to each vertex in V , together
with the probability measure
Ë‡./ WD GL
Ë‡
./ D
1
ZË‡
exp
Â²
Ë‡
X
u;vW u$v 1

.u/ D .v/
Â³
;
where u $ v means u and v are neighbors in GL, Z is the normalization constant and
Ë‡ 	 0 is the inverse temperature. This measure is called the Gibbs distribution with
free boundary condition.
Additionally we need the notion of boundary conditions, but we restrict ourself here
to the â€œall plusâ€ and â€œall minusâ€ case.
Let V c D Z2 n V . Then we denote the lattice GL together with the probability
measure
Ë™
Ë‡ ./ WD GL;Ë™
Ë‡
./ D
1
eZË‡
GL
Ë‡
./  exp
Â²
Ë‡
X
v2V; u2V cW
u$v
1

.v/ D Ë™1
Â³
by the Ising model with plus/minus boundary condition, respectively. One can imagine
that this corresponds to the Ising model on GL with a strip of ï¬xed spins around, so
every vertex in GL has the same number of neighbors.

Chapter 24 Exact Sampling for the Ising Model at All Temperatures
225
In 1944 Onsager [17] proved that there is a phase transition at Ë‡ D Ë‡c WD ln.1C
p
2/
in the case where V D Z2, and we will see that this value is also important for ï¬nite
lattices. Namely, the dynamics deï¬ned below is rapidly mixing if and only if Ë‡  Ë‡c.
We will use the so-called heat bath dynamics. These dynamics deï¬ne a irreducible,
aperiodic and reversible Markov chain XË‡ D .XË‡
i /i2N with stationary distribution
Ë‡ by the transition matrix
P.; v;
/ D
1
N
 
1 C
Ë‡./
Ë‡.v;
/
!1
 2 IS;
v 2 V;
where v;
 with  2 Â¹1; 1Âº is deï¬ned by v;
.v/ D  and v;
.u/ D .u/, u Â¤ v.
The interpretation of this algorithm is very simple. In each step, choose a random
v 2 V and assign a new value to v according to Ë‡ conditioned on all neighbors of v.
Note that the results of this chapter hold in general for all Glauber dynamics as deï¬ned
in [6] which admit a monotone Coupling (see Section 24.3). For a general introduction
to Markov chains see, e. g., [11], or [13] in the context of spin systems.
In the following we want to estimate how fast such a Markov chain converges to
its stationary distribution. Therefore we ï¬rst introduce the total variation distance to
measure the distance between two probability measures  and , which is deï¬ned by
k  kTV D 1
2
X
	2IS
j./  ./j :
Now we can deï¬ne the mixing time of the Markov chain with transition matrix P and
stationary distribution Ë‡ by
Ë‡ D min
Â²
n W max
	2IS
kP n.; /  Ë‡./kTV  1
2e
Â³
:
This is the expected time the Markov chain needs to get close to its stationary dis-
tribution. In fact, one can bound the spectral gap of the transition matrix P in either
direction in terms of the mixing time (see, e. g., [11, Theorems 12.3 and 12.4]), so one
can bound the error of a MCMC algorithm to integrate functions over IS, as one can
read in [19]. Furthermore, if the Markov chain is rapidly mixing (i. e., the mixing time
is bounded by a polynomial in the logarithm of size of the state space IS) we get
that the problem of integration (with an unnormalized density) on the Ising model is
tractable (see also [16]). Unfortunately there is no Markov chain that is proven to be
rapidly mixing at all temperatures.
However, in this chapter we are interested in sampling exactly from the station-
ary distribution, but ï¬rst we present the known mixing-time results for the Glauber
dynamics for the Ising model. For proofs or further details we refer to the particular
articles or the survey of Martinelli [13]. Of course, we can only give a small selection
of references, because there are many papers leading to the results given below.

226
Mario Ullrich
Theorem 24.2 ([14]). Let Ë‡ < Ë‡c. Then there exists a constant cË‡ > 0 such that
the mixing time of the Glauber dynamics for the Ising model with arbitrary boundary
condition on GL satisï¬es
Ë‡  cË‡ N log N:
Theorem 24.3 ([2]). Let Ë‡ > Ë‡c. Then there exists a constant cË‡ > 0 such that the
mixing time of the Glauber dynamics for the Ising model on GL satisï¬es
Ë‡ 	 ecË‡L:
The results above can be obtained by observing that some spatial mixing property
of the measure Ë‡ is equivalent to the mixing in time of the Glauber dynamics. For
details for this interesting fact, see [3].
The constant cË‡ of Theorem 24.2 is widely believed to diverge if Ë‡ tends to Ë‡c.
Determining the mixing time in the case Ë‡ D Ë‡c was a challenging problem for a long
time. It was solved by Lubetzky and Sly in their recent paper [12].
Theorem 24.4 ([12]). There exists a constant C > 0 such that the mixing time of the
Glauber dynamics for the Ising model on GL at the critical temperature satisï¬es
Ë‡c  4 N C :
Remark 24.5. We give here only a brief description of the constant C, which can be
given explicitly. For more details see [12, p.19].
However, numerical experiments on the â€œtrueâ€ exponent suggest that C  3:08
(see, e. g., [15,20] and note the explanation below).
The constant C in Theorem 24.4 is given by
C D 2 C log3=2

2
1  pC

:
(24.1)
Here, pC is the limiting vertical crossing probability in the random cluster model on
a fully-wired rectangle, where the width of the lattice is 3 its height. The C, as given
here, differs from the one given in [12] by eliminating a factor of 2 in front of the log
term and by the additional 2. The reason is that we state their result in terms of N
and not in the side length L of the lattice (therefore without factor 2) and that we are
interested in the discrete time single-spin algorithms. Therefore we get an additional

Chapter 24 Exact Sampling for the Ising Model at All Temperatures
227
factor N in their spectral gap result ( [12, Thm. 1]) and a factor N by (see, e. g., [11])
Ë‡c  log

e
min	 Ë‡c./

gap.XË‡c/1  4 N gap.XË‡c/1;
because min	 Ë‡c./ 	 exp.3N/.
The results of this section show that the Glauber dynamics is mixing rapidly for
Ë‡  Ë‡c, but very slowly mixing for larger Ë‡. In Section 24.4 we will see how to avoid
this problem.
24.3
Exact Sampling
In this section we brieï¬‚y describe the so-called Coupling from the past algorithm
(CFTP) to sample exactly from the stationary distribution of a Markov chain.
This algorithm works under weak assumptions on the Markov chain for every ï¬nite
state space and every distribution, but to guarantee that the algorithm is efï¬cient we
need some monotonicity property of the model and that the chain is rapidly mixing.
For a detailed description of CFTP and the proof of correctness see [18].
We restrict ourself to the heat bath dynamics for the Ising model. First note that the
heat bath dynamics, as deï¬ned above, admits a monotone Coupling, that is, given two
realizations of the heat bath chain X D .Xt/t2N and Y D .Yt/t2N, there exists a
Coupling .X; Y / (i. e., using the same random numbers) such that
Xt  Yt
H)
XtC1  YtC1
for all t 2 N;
where  means smaller or equal at each vertex.
Additionally we know that 1    1 for all  2 IS, where 1 D .1/V and
1 D .1/V . Therefore if we set X0 D 1 and Y0 D 1 we know that X0    Y0
for all  and so Xt  Zt  Yt for the realization Z D .Zt/t2N with Z0 D . Since
this holds for all , one can choose Z0  Ë‡ and we get that whenever Xt and Yt
coalesce, they also coalesce with Zt which has the right distribution.
Having presented the idea of the algorithm, we will now state the algorithm in detail.
Note that the algorithm is called Coupling from the past, because we run the chains
from the past to the present. The algorithm CFTP.G; Ë‡/ to sample from the distribution
G
Ë‡ works as described in Algorithm 1.
We denote the algorithm by CFTPË™.G; Ë‡/ if we sample with respect to Ë™
Ë‡ , i. e.,
with plus/minus boundary condition.
See [9] for examples that show that it is necessary to go from the past in the future
and that we have to reuse the random numbers.
Now we state the connection between the expected running time of the CFTP algo-
rithm and the mixing time of the Markov chain.

228
Mario Ullrich
Algorithm 24.1. Coupling from the past.
Input: The graph G D .V; E/ and the value of Ë‡
Output: An Ising conï¬guration   Ë‡
1:
procedureCFTPG; Ë‡
2:
Set t D 0
3:
Set X0 D 1 and Y0 D 1
4:
while X0 Â¤ Y0
5:
t D t C 1
6:
Generate random numbers U2tC1; : : : ; U2t1 that are sufï¬cient to run
the Markov chain. (e. g., Ui  Uniform Â¹V  Å’0; 1Âº)
7:
Set X2tC1 D 0 and Y2tC1 D 1 and run the chains until time 0 by using
only the random numbers U2tC1; : : : ; U1
8:
end while
9:
return  D X0
10:
end procedure
Proposition 24.6 ([18]). Let TË‡ be the expected running time of CFTP.G; Ë‡/ from
Algorithm 24.1 with G D .V; E/ and jV j D N. Then
TË‡  4 Ë‡ log N;
where Ë‡ is the mixing time of the underlying Markov chain.
We see that exact sampling from the Gibbs distribution is efï¬cient whenever the
Markov chain is rapidly mixing. By the results of Section 24.2 we know that this is
the case for Ë‡  Ë‡c. In the case Ë‡ > Ë‡c we need a different technique to generate
exact samples. Therefore we need essentially the so called random cluster model, as
we will see in the next section.
24.4
The Random Cluster Model
The random cluster model (also known as the FK model) was introduced by Fortuin
and Kasteleyn in [5] to study lattice spin systems with a graph structure. It is deï¬ned
on a graph G D .V; E/ by its state space RC D Â¹! W !  EÂº and the RC measure
p.!/ D
1
Z pj!j .1  p/jEjj!j 2C.!/;
where p 2 .0; 1/, Z is the normalization constant and C.!/ is the number of connected
components in the graph .V; !/. For a detailed introduction and related topics see [7].

Chapter 24 Exact Sampling for the Ising Model at All Temperatures
229
Ã—
Ã—
Ã—
Ã—
Ã—
Ã—
Ã—
Ã—
Ã—
Ã—
Figure 24.1. Left: The graph G3 (solid lines) and its dual (dashed lines). Right: A RC state on
G3 (solid lines) and its dual conï¬guration (dashed lines).
There is a tight connection between the Ising model and the random cluster model.
Namely, if we set p D 1  eË‡, we can translate an Ising conï¬guration   Ë‡ to a
random cluster state !  p and vice versa. To get an Ising conï¬guration  2 IS
from ! 2 RC assign independent and uniformly random spins to each connected
component of !. For the reverse way include all edges e D Â¹e1; e2Âº 2 E with .e1/ D
.e2/ to ! with probability p. For details see [4].
Therefore sampling an Ising conï¬guration according to Ë‡ is equivalent to sampling
a RC state from p whenever both models are deï¬ned on the same graph G and p D
1  eË‡.
Another important concept in connection with the RC model is the duality of graphs
(see, e. g., [8]). Let G D .V; E/ be a ï¬nite, planar graph, i. e., without intersecting
edges if we draw it in the plane (like our GL). The dual graph G D .V ; E/ of G
is constructed as follows. Put a vertex in each face (including the inï¬nite outer one) of
the graph and connect two vertices by an edge if and only if the corresponding faces
of G share a boundary edge. It is clear, that the number of vertices can differ in the
dual graph, but we have the same number of edges.
Additionally we deï¬ne a dual conï¬guration !  E in G to a RC state !  E
in G by
e 2 ! â€ e â€¦ !;
where e is the edge in E that â€œcrossesâ€ e. (By the construction, this edge is unique.)
See Figure 24.1 for the graph GL with L D 3 and its dual graph G
L together with
2 corresponding RC states.
Now we can state the following theorem about the relation of the distribution of a
RC state and its dual (see [8]).
Proposition 24.7 ([8, p. 164]). Let G D .V; E/ be a ï¬nite, planar graph and p be
the random cluster measure on G. Furthermore let G D .V ; E/ be the dual graph
of G and 
p be the random cluster measure on G.

230
Mario Ullrich
Then
!  p
â€
!  
p;
where
p D 1 
p
2  p :
(24.2)
Obviously, .p/ D p. By Proposition 24.7 one can see that sampling from p and
sampling from 
p is equivalent. It is straightforward to get the following proposition.
Proposition 24.8. Sampling from the Gibbs distribution G
Ë‡ is equivalent to sampling
from the Gibbs distribution G
Ë‡  , where
Ë‡ D log

coth Ë‡
2

:
(24.3)
Additionally,
Ë‡ > Ë‡c
â€
Ë‡ < Ë‡c:
Note that the relations of (24.2) and (24.3) can be alternatively stated as
0 D 2  2p  2p C pp
and
0 D 1 C eË‡ C eË‡  C eË‡CË‡ :
Proof. The equivalence was shown by the above procedure, i. e., if we want to sample
from G
Ë‡ , we can sample from G
Ë‡  , generate a RC state with respect to 
p, go to
the dual lattice with measure p, and ï¬nally generate a state according to G
Ë‡ . Since
p./ D 1  eË‡ ./, the formula for Ë‡ comes from
Ë‡ D  log.1  p/ (24.2)
D
log
2  p
p

D log

coth Ë‡
2

:
This proves the statement.
24.5
Exact Sampling for the Ising Model
In this section we present an algorithm to sample exactly from the a Gibbs distribution.
But, before we prove that it is efï¬cient, we state our sampling algorithm.
Therefore we ï¬rst have to explain what the graph G
L looks like. It is easy to obtain
(see Figure 24.1) that G
L D .V 
L ; E
L/ is also a square lattice with .L  1/2 vertices
and an additional auxiliary vertex v, which is connected to every vertex on the bound-
ary of it. We denote the operation of adding a vertex to a graph and connect it to all
boundary vertices by [b. So G
L D GL1 [b v.

Chapter 24 Exact Sampling for the Ising Model at All Temperatures
231
Algorithm 24.2. Sampling from the Ising model on the square lattice.
Input: An integer L and the value of Ë‡
Output: An Ising conï¬guration   GL
Ë‡
1:
if Ë‡  Ë‡c then
2:
 D CFTP.GL; Ë‡/
3:
else
4:
e D CFTPC.GL1; Ë‡/, where Ë‡ is given in (24.3)
5:
Deï¬ne a Ising conï¬guration  on G
L D GL1 [b v by .v/ D e.v/ on
V.GL1/ and .v/ D 1.
6:
Generate a RC state ! from 
7:
Take the dual RC state ! D .!/
8:
Generate an Ising conï¬guration  from !
9:
end if
10:
return 
Theorem 24.9. Let GL be the square lattice with N D L2 vertices. Then, the al-
gorithm from above outputs an exactly distributed Ising conï¬guration with respect to
GL
Ë‡
in expected time smaller than
 cË‡ N .log N/2
for Ë‡ Â¤ Ë‡c D log.1 C
p
2/ and some cË‡ > 0I
 16 N C log N
for Ë‡ D Ë‡c, where C is given in (24.1).
Proof. The running time of the algorithm follows directly from Theorems 24.2
and 24.4 and Proposition 24.6. Therefore we only have to prove that the output 
of the algorithm has the right distribution. In the case of Ë‡  Ë‡c this is obvious. For
Ë‡ > Ë‡c we know from Proposition 24.8 that   GL
Ë‡
, if the dual conï¬guration 
on G
L (line 5 of Algorithm 24.2) is distributed according to Ë‡  WD 
G
L
Ë‡  . But by
the construction of lines 4 and 5 of Algorithm 24.2, this is true. For this, note that
Ë‡./ D Ë‡./ for all  2 IS. We get that for each vertex v 2 V (especially for
v)
Ë‡./ D Ë‡

 \ Â¹ W .v/ D 1Âº

C Ë‡

 \ Â¹ W .v/ D 1Âº

D Ë‡

Â¹ W .v/ D 1Âº

Ë‡

 Â¹ W .v/ D 1Âº

C Ë‡

Â¹ W .v/ D 1Âº

Ë‡

 Â¹ W .v/ D 1Âº

D 1
2
h
Ë‡

 Â¹ W .v/ D 1Âº

C Ë‡

 Â¹ W .v/ D 1Âº
i
D 1
2 Ë‡

Â¹; Âº Â¹ W .v/ D 1Âº

:

232
Mario Ullrich
The last equality comes from the fact that
Ë‡

 Â¹ W .v/ D 1Âº

D Ë‡

 Â¹ W .v/ D 1Âº

:
Therefore we can sample from Ë‡ on G
L by sampling  from the conditional measure
Ë‡. j Â¹ W .v/ D 1Âº/ and then choose with probability 1
2 either  or . If we
now use that G
L D GL1 [b v one can see that sampling on G
L with respect to
Ë‡. j Â¹ W .v/ D 1Âº/ is the same as sampling e from GL1;C
Ë‡
and setting
.v/ D
Â´
e.v/
v 2 V.GL1/;
1
v D v:
Note that we omit the step of choosing  or  with probability 1
2, because the RC
state that will be generated would be the same.
This completes the proof.
Remark 24.10. Note that the same technique works also for the q-state Potts model.
This model consists of the state space P D Â¹1; : : : ; qÂºV and the same measure Ë‡.
In this case we consider the random cluster measure
p;q.!/ D
1
Z pj!j .1  p/jEjj!j qC.!/
and the connection of the models is again given by p D 1  eË‡.
A recent result of Beffara and Duminil-Copin [1] shows that the self-dual point of
the RC model corresponds to the critical temperature of the Potts model Ë‡c.q/ D
ln.1 C pq/ in the same way as in the case q D 2 (i. e., the Ising case). Therefore,
a sampling algorithm for the Potts model above (and at) the critical temperature is
enough to sample at all temperatures.
Acknowledgments. The author was supported by the DFG GK 1523.
References
[1] V. Beffara and H. Duminil-Copin, The self-dual point of the two-dimensional random-
cluster model is critical for q 	 1, Probab. Theory Related Fields 153 (2012), 511â€“542.
[2] F. Cesi, G. Guadagni, F. Martinelli, and R. H. Schonmann, On the two-dimensional
stochastic Ising model in the phase coexistence region near the critical point, J. Statist.
Phys. 85 (1996), 55â€“102.
[3] M. Dyer, A. Sinclair, E. Vigoda, and D. Weitz, Mixing in time and space for lattice spin
systems: a combinatorial view, Random Structures Algorithms 24 (2004), 461â€“479.
[4] S. F. Edwards and A. D. Sokal, Generalization of the Fortuin-Kasteleyn-Swendsen-Wang
representation and Monte Carlo algorithm, Phys. Rev. D 38 (1988), 2009â€“2012.

Chapter 24 Exact Sampling for the Ising Model at All Temperatures
233
[5] C. M. Fortuin and P. W. Kasteleyn, On the random-cluster model. I. Introduction and
relation to other models, Physica 57 (1972), 536â€“564.
[6] R. J. Glauber, Time-dependent statistics of the Ising model, J. Mathematical Phys. 4
(1963), 294â€“307.
[7] G. Grimmett, The random-cluster model, Grundlehren der Mathematischen Wis-
senschaften 333, Springer, Berlin, 2006.
[8] G. Grimmett, Probability on Graphs, IMS Textbooks Series 1, Cambridge University
Press, Cambridge, 2010.
[9] O, HÃ¤ggstrÃ¶m, Finite Markov chains and algorithmic applications, London Mathemati-
cal Society Student Texts 52, Cambridge University Press, Cambridge, 2002.
[10] M. Jerrum and A. Sinclair, Polynomial-time approximation algorithms for the Ising
model, SIAM J. Comput. 22 (1993), 1087â€“1116.
[11] D. A. Levin, Y. Peres, and E. L. Wilmer, Markov chains and mixing times. Ameri-
can Mathematical Society, Providence, RI, 2009, With a chapter by J. G. Propp and
D. B. Wilson.
[12] E. Lubetzky and A. Sly, Critical Ising on the square lattice mixes in polynomial time,
Comm. Math. Phys. 313 (2012), 815â€“836.
[13] F. Martinelli, Lectures on Glauber dynamics for discrete spin models, in: Lectures on
probability theory and statistics (Saint-Flour, 1997), pp. 93â€“191, Lecture Notes in Math-
ematics 1717, Springer, Berlin, 1999.
[14] F. Martinelli and E. Olivieri, Approach to equilibrium of Glauber dynamics in the one
phase region. I. The attractive case, Comm. Math. Phys. 161 (1994), 447â€“486.
[15] M. P. Nightingale and H. W. J. BlÃ¶te, Dynamic Exponent of the Two-Dimensional Ising
Model and Monte Carlo Computation of the Subdominant Eigenvalue of the Stochastic
Matrix, Physical Review Letters 76 (1996), 4548â€“4551.
[16] E. Novak and H. WoÂ´zniakowski, Tractability of multivariate problems. Volume II: Stan-
dard information for functionals, EMS Tracts in Mathematics 12. European Mathemati-
cal Society (EMS), ZÃ¼rich, 2010.
[17] L. Onsager, Crystal statistics. I. A two-dimensional model with an order-disorder transi-
tion, Phys. Rev. 65 (1944), 117â€“149.
[18] J. G. Propp and D. B. Wilson, Exact sampling with coupled Markov chains and applica-
tions to statistical mechanics, Random Structures Algorithms 9 (1996), 223â€“252.
[19] D. Rudolf, Explicit error bounds for lazy reversible Markov chain Monte Carlo, J. Com-
plexity 25 (2009), 11â€“24.
[20] F. Wang, Naomichi Hatano and Masuo Suzuki, Study on dynamical critical exponents of
the Ising model using the damage spreading method, J. Phys. A: Math. Gen. 28 (1995),
4543â€“4552.
Author information
Mario Ullrich, Friedrich Schiller University, Jena, Germany.
Email: mario.ullrich@uni-jena.de


