
Data Analysis

Data Analysis
A Bayesian Tutorial
Second Edition
D.S.Sivia
Rutherford Appleton Laboratory and St. Catherine’s College, Oxford
with
J.Skilling
Maximum Entropy Data Consultants, Cambridge
1

Data Analysis
A Bayesian Tutorial
D.S.Sivia
Rutherford Appleton Laboratory and St. Catherine’s College, Oxford
with
J.Skilling
Maximum Entropy Data Consultants, Cambridge
1

3
Great Clarendon Street, Oxford OX2 6DP
Oxford University Press is a department of the University of Oxford.
It furthers the University’s objective of excellence in research, scholarship,
and education by publishing worldwide in
Oxford New York
Auckland Cape Town Dar es Salaam Hong Kong Karachi
Kuala Lumpur Madrid Melbourne Mexico City Nairobi
New Delhi Shanghai Taipei Toronto
With ofﬁces in
Argentina Austria Brazil Chile Czech Republic France Greece
Guatemala Hungary Italy Japan Poland Portugal Singapore
South Korea Switzerland Thailand Turkey Ukraine Vietnam
Oxford is a registered trade mark of Oxford University Press
in the UK and in certain other countries
Published in the United States
by Oxford University Press Inc., New York
© D. S. Sivia and J. Skilling 2006
The moral rights of the authors have been asserted
Database right Oxford University Press (maker)
First published 2006
All rights reserved. No part of this publication may be reproduced,
stored in a retrieval system, or transmitted, in any form or by any means,
without the prior permission in writing of Oxford University Press,
or as expressly permitted by law, or under terms agreed with the appropriate
reprographics rights organization. Enquiries concerning reproduction
outside the scope of the above should be sent to the Rights Department,
Oxford University Press, at the address above
You must not circulate this book in any other binding or cover
and you must impose the same condition on any acquirer
British Library Cataloguing in Publication Data
Data available
Library of Congress Cataloging in Publication Data
Data available
Typeset by the author in LaTeX
Printed in Great Britain
on acid-free paper by
Biddles Ltd., King’s Lynn, Norfolk
ISBN 0–19–856831–2
978–0–19–856831–5
ISBN 0–19–856832–0 (Pbk.)
978–0–19–856832–2 (Pbk.)
1 3 5 7 9 10 8 6 4 2

To Joyce and Jennifer

This page intentionally left blank 

Preface
As an undergraduate, I always found the subject of statistics to be rather mysterious.
This topic wasn’t entirely new to me, as we had been taught a little bit about probability
earlier at high school; for example, I was already familiar with the binomial, Poisson
and normal distributions. Most of this made sense, but only seemed to relate to things
like rolling dice, ﬂipping coins, shufﬂing cards and so on. However, having aspirations
of becoming a scientist, what I really wanted to know was how to analyse experimental
data. Thus, I eagerly looked forward to the lectures on statistics. Sadly, they were a great
disappointment. Although many of the tests and procedures expounded were intuitively
reasonable, there was something deeply unsatisfactory about the whole affair: there
didn’t seem to be any underlying basic principles! Hence, the course on ‘probability
and statistics’ had led to an unfortunate dichotomy: probability made sense, but was
just a game; statistics was important, but it was a bewildering collection of tests with
little obvious rhyme or reason. While not happy with this situation, I decided to put
aside the subject and concentrate on real science. After all, the predicament was just a
reﬂection of my own inadequacies and I’d just have to work at it when the time came to
really analyse my data.
The story above is not just my own, but is the all too common experience of many
scientists. Fortunately, it doesn’t have to be like this. What we were not told in our un-
dergraduate lectures is that there is an alternative approach to the whole subject of data
analysis which uses only probability theory. In one sense, it makes the topic of statis-
tics entirely superﬂuous. In another, it provides the logical justiﬁcation for many of the
prevalent statistical tests and procedures, making explicit the conditions and approxi-
mations implicitly assumed in their use.
This book is intended to be a tutorial guide to this alternative Bayesian approach,
including modern developments such as maximum entropy. It is not designed to be a
comprehensive or authoritative text, but is aimed at showing the reader how simple
and straightforward it is to use probability theory directly for data analysis. As such, the
examples are to be taken as illustrative rather than exhaustive; to attempt the latter would
encourage the cook-book recipe mentality which bedevils conventional statistics. It is
hoped that the emphasis on going from a few basic principles to a concrete statistical
prescription will provide the reader with the conﬁdence to tackle problems which are
not found in text books. If he, or she, comes away with the feeling: ‘Is that all it is? It’s
easy — I can do that’, then the enterprise can be deemed to have been successful.
This book should be accessible to all students who have completed a standard ﬁrst
year undergraduate course in Mathematical Methods, covering topics such as partial
derivatives, multiple integrals, matrices, vectors and so on; this should certainly encom-
pass most physicists, chemists and engineers, and quite a few others besides. The ﬁrst
three chapters form the core material, and deal with the basic principles and the subject
of parameter estimation; in fact, most data analysis problems require no more than this.
Chapter 4 provides a valuable insight into hypothesis testing and Chapter 5 gives an al-
ternative rationale for many familiar probability distributions. Chapter 6, which looks at
non-parametric estimation, is perhaps the most demanding and is also liable to change
signiﬁcantly with future developments; along with Chapter 7 on experimental design, it

viii
Preface
is really for the enthusiast.
Finally, it is my pleasure to acknowledge the invaluable suggestions and comments
given to me by several friends and colleagues: in particular, Phil Gregory, Steve Gull,
Kevin Knight, Sabine K¨onig, Angus Lawson, Jerry Mayers, Vincent Macaulay, Toby
Perring, Roger Pynn, Steve Rawlings, John Skilling and Dave Waymont. While I may
yet regret not taking all their advice, this text has been improved greatly because of
them; I thank them for their patience and help.
Rutherford Appleton Laboratory
October, 1995
D.S.S.
Preface to the Second Edition
A couple of years ago, I was approached by some colleagues about the possibility
of updating my book. My initial reaction was to think why should I? For what I had
intended, I would still write it pretty much the same way. Upon reﬂection, however,
the merits of a Second Edition became apparent. For example, it would allow me to
include the simple ideas for robust estimation that I had explored since the original
text; namely, ways of dealing with ‘outliers’, and unknown correlated noise, within the
framework of the ubiquitous least-squares procedure. More importantly, though, John
Skilling had kindly offered to contribute a couple of chapters on modern numerical
techniques for Bayesian computation. The latter have taken longer to materialize than
anticipated, because John’s thoughts on the topic have been continually evolving over
the period. Indeed, the subject is at the forefront of his current research.
The strengths of the original book have been preserved by keeping the existing text
largely unaltered, but it has been enhanced with the addition of Chapters 8, 9 and 10.
Even with the extra material, including a new Appendix, the aim has been to keep the
volume slim. This has been achieved through the type-setting efﬁciency of LATEX, and
some culling of content now deemed less useful.
It goes without saying that I am greatly indebted to John Skilling, my co-author,
for Chapters 9 and 10, Appendix B, and for suggestions on changes to the original
text. John has included examples of ‘C’ computer code in his chapters to illustrate
the main ideas. These short programs are freely available in the public domain, for
use without warranty, under the terms of the GNU General Public License; for details
see http://www.gnu.org/copyleft/gpl.html. Courtesy suggests, of course, that
their use be acknowledged by reference to this volume (D. S. Sivia with J. Skilling, ‘Data
Analysis — a Bayesian tutorial’, 2nd Edition, 2006, Oxford University Press). Related material
should be available at http://www.inference.phy.cam.ac.uk/bayesys/, and I
am very grateful to David MacKay for this use of his website.
Finally, I’d like to acknowledge the valuable feedback I was given on some of the
new material by Richard Bailey, Bruce Henning, Steve Rawlings, Stephen Stokes and
David Waymont, and the help and patience of my Editor, S¨onke Adlung, at OUP.
Oxford
October, 2005
D.S.S.

Contents
PART I
THE ESSENTIALS
1.
The basics
3
1.1
Introduction: deductive logic versus plausible reasoning
3
1.2
Probability: Cox and the rules for consistent reasoning
4
1.3
Corollaries: Bayes’ theorem and marginalization
5
1.4
Some history: Bayes, Laplace and orthodox statistics
8
1.5
Outline of book
12
2.
Parameter estimation I
14
2.1
Example 1: is this a fair coin?
14
2.1.1
Different priors
17
2.1.2
Sequential or one-step data analysis?
19
2.2
Reliabilities: best estimates, error-bars and conﬁdence intervals
20
2.2.1
The coin example
23
2.2.2
Asymmetric posterior pdfs
24
2.2.3
Multimodal posterior pdfs
25
2.3
Example 2: Gaussian noise and averages
26
2.3.1
Data with different-sized error-bars
29
2.4
Example 3: the lighthouse problem
29
2.4.1
The central limit theorem
33
3.
Parameter estimation II
35
3.1
Example 4: amplitude of a signal in the presence of background
35
3.1.1
Marginal distributions
39
3.1.2
Binning the data
42
3.2
Reliabilities: best estimates, correlations and error-bars
43
3.2.1
Generalization of the quadratic approximation
49
3.2.2
Asymmetric and multimodal posterior pdfs
50
3.3
Example 5: Gaussian noise revisited
52
3.3.1
The Student-t and χ2 distributions
54
3.4
Algorithms: a numerical interlude
55
3.4.1
Brute force and ignorance
56
3.4.2
The joys of linearity
57
3.4.3
Iterative linearization
58
3.4.4
Hard problems
60
3.5
Approximations: maximum likelihood and least-squares
61
3.5.1
Fitting a straight line
65

x
Contents
3.6
Error-propagation: changing variables
68
3.6.1
A useful short cut
73
3.6.2
Taking the square root of a number
74
4.
Model selection
78
4.1
Introduction: the story of Mr A and Mr B
78
4.1.1
Comparison with parameter estimation
83
4.1.2
Hypothesis testing
84
4.2
Example 6: how many lines are there?
85
4.2.1
An algorithm
89
4.2.2
Simulated data
91
4.2.3
Real data
93
4.3
Other examples: means, variance, dating and so on
94
4.3.1
The analysis of means and variance
94
4.3.2
Luminescence dating
98
4.3.3
Interlude: what not to compute
100
5.
Assigning probabilities
103
5.1
Ignorance: indifference and transformation groups
103
5.1.1
The binomial distribution
107
5.1.2
Location and scale parameters
108
5.2
Testable information: the principle of maximum entropy
110
5.2.1
The monkey argument
113
5.2.2
The Lebesgue measure
115
5.3
MaxEnt examples: some common pdfs
117
5.3.1
Averages and exponentials
117
5.3.2
Variance and the Gaussian distribution
118
5.3.3
MaxEnt and the binomial distribution
120
5.3.4
Counting and Poisson statistics
121
5.4
Approximations: interconnections and simpliﬁcations
121
5.5
Hangups: priors versus likelihoods
124
5.5.1
Improper pdfs
124
5.5.2
Conjugate and reference priors
125
PART II
ADVANCED TOPICS
6.
Non-parametric estimation
129
6.1
Introduction: free-form solutions
129
6.1.1
Singular value decomposition
130
6.1.2
A parametric free-form solution?
135
6.2
MaxEnt: images, monkeys and a non-uniform prior
136
6.2.1
Regularization
138
6.3
Smoothness: fuzzy pixels and spatial correlations
140
6.3.1
Interpolation
141
6.4
Generalizations: some extensions and comments
142

Contents
xi
6.4.1
Summary of the basic strategy
144
6.4.2
Inference or inversion?
145
6.4.3
Advanced examples
148
7.
Experimental design
149
7.1
Introduction: general issues
149
7.2
Example 7: optimizing resolution functions
151
7.2.1
An isolated sharp peak
152
7.2.2
A free-form solution
156
7.3
Calibration, model selection and binning
161
7.4
Information gain: quantifying the worth of an experiment
163
8.
Least-squares extensions
165
8.1
Introduction: constraints and restraints
165
8.2
Noise scaling: a simple global adjustment
166
8.3
Outliers: dealing with erratic data
167
8.3.1
A conservative formulation
168
8.3.2
The good-and-bad data model
171
8.3.3
The Cauchy formulation
172
8.4
Background removal
173
8.5
Correlated noise: avoiding over-counting
174
8.5.1
Nearest-neighbour correlations
175
8.5.2
An elementary example
176
8.5.3
Time series
177
8.6
Log-normal: least-squares for magnitude data
179
9.
Nested sampling
181
9.1
Introduction: the computational problem
181
9.1.1
Evidence and posterior
182
9.2
Nested sampling: the basic idea
184
9.2.1
Iterating a sequence of objects
185
9.2.2
Terminating the iterations
186
9.2.3
Numerical uncertainty of computed results
187
9.2.4
Programming nested sampling in ‘C’
188
9.3
Generating a new object by random sampling
190
9.3.1
Markov chain Monte Carlo (MCMC) exploration
191
9.3.2
Programming the lighthouse problem in ‘C’
192
9.4
Monte Carlo sampling of the posterior
195
9.4.1
Posterior distribution
196
9.4.2
Equally-weighted posterior samples: staircase sampling
197
9.4.3
The lighthouse posterior
198
9.4.4
Metropolis exploration of the posterior
199
9.5
How many objects are needed?
200
9.5.1
Bi-modal likelihood with a single ‘gate’
200
9.5.2
Multi-modal likelihoods with several ‘gates’
201

xii
Contents
9.6
Simulated annealing
203
9.6.1
The problem of phase changes
203
9.6.2
Example: order/disorder in a pseudo-crystal
204
9.6.3
Programming the pseudo-crystal in ‘C’
206
10. Quantiﬁcation
209
10.1 Exploring an intrinsically non-uniform prior
209
10.1.1 Binary trees for controlling MCMC transitions
210
10.2 Example: ON/OFF switching
212
10.2.1 The master engine: ﬂipping switches individually
212
10.2.2 Programming which components are present
212
10.2.3 Another engine: exchanging neighbouring switches
215
10.2.4 The control of multiple engines
216
10.3 Estimating quantities
216
10.3.1 Programming the estimation of quantities in ‘C’
218
10.4 Final remarks
223
A. Gaussian integrals
224
A.1
The univariate case
224
A.2
The bivariate extension
225
A.3
The multivariate generalization
226
B. Cox’s derivation of probability
229
B.1
Lemma 1: associativity equation
232
B.2
Lemma 2: negation
235
Bibliography
237
Index
241

Part I
The essentials
‘La th´eorie des probabilit´es n’est que le bon sens reduit au calcul.’
— Pierre Simon, Marquis de Laplace

This page intentionally left blank 

1
The basics
‘There are three kinds of lies: lies, damned lies and statistics.’
Mark Twain (1924) probably had politicians in mind when he reiterated Disraeli’s fa-
mous remark. Scientists, we hope, would never use data in such a selective manner to
suit their own ends. But, alas, the analysis of data is often the source of some exasper-
ation even in an academic context. On hearing comments like ‘the result of this experi-
ment was inconclusive, so we had to use statistics’, we are frequently left wondering as
to what strange tricks have been played on the data.
The sense of unease which many of us have towards the subject of statistics is largely
a reﬂection of the inadequacies of the ‘cook-book’ approach to data analysis that we
are taught as undergraduates. Rather than being offered a few clear principles, we are
usually presented with a maze of tests and procedures; while most seem to be intuitively
reasonable individually, their interrelations are not obvious. This apparent lack of a
coherent rationale leads to considerable apprehension because we have little feeling for
which test to use or, more importantly, why.
Fortunately, data analysis does not have to be like this! A more uniﬁed and logi-
cal approach to the whole subject is provided by the probability formulations of Bayes
and Laplace. Bayes’ ideas (published in 1763) were used very successfully by Laplace
(1812), but were then allegedly discredited and largely forgotten until they were redis-
covered by Jeffreys (1939). In more recent times they have been expounded by Jaynes
(1983, 2003) and others. This book is intended to be an introductory tutorial to the
Bayesian approach, including modern developments such as maximum entropy.
1.1
Introduction: deductive logic versus plausible reasoning
Let us begin by trying to get a general feel for the nature of the problem. A schematic
representation of deductive logic is shown in Fig. 1.1(a): given a cause, we can work out
its consequences. The sort of reasoning used in pure mathematics is of this type: that is
to say, we can derive many complicated and useful results as the logical consequence of
a few well-deﬁned axioms. Everyday games of chance also fall into this category. For
example, if we are told that a fair coin is to be ﬂipped ten times, we can calculate the
chances that all ten tosses will produce heads, or that there will be nine heads and one
tail, and so on.
Most scientists, however, face the reverse of the above situation: Given that certain
effects have been observed, what is (are) the underlying cause(s)? To take a simple
example, suppose that ten ﬂips of a coin yielded seven heads: Is it a fair coin or a
biased one? This type of question has to do with inductive logic, or plausible reasoning,
and is illustrated schematically in Fig. 1.1(b); the greater complexity of this diagram is

4
The basics
Fig. 1.1 A schematic representation of (a) deductive logic, or pure mathematics, and (b) plausible
reasoning, or inductive logic.
designed to indicate that it is a much harder problem. The most we can hope to do is to
make the best inference based on the experimental data and any prior knowledge that
we have available, reserving the right to revise our position if new information comes to
light. Around 500 BC, Herodotus said much the same thing: ‘A decision was wise, even
though it led to disastrous consequences, if the evidence at hand indicated it was the
best one to make; and a decision was foolish, even though it led to the happiest possible
consequences, if it was unreasonable to expect those consequences.’
Even though plausible reasoning is rather open-ended, are there any general quanti-
tative rules which apply for such inductive logic? After all, this issue is central to data
analysis.
1.2
Probability: Cox and the rules for consistent reasoning
In 1946, Richard Cox pondered the quantitative rules necessary for logical and con-
sistent reasoning. He started by considering how we might express our relative beliefs
in the truth of various propositions. For example: (a) it will rain tomorrow; (b) King
Harold died by being hit in the eye with an arrow at the battle of Hastings in 1066 AD;
(c) this is a fair coin; (d) this coin is twice as likely to come up heads as tails; and so
on. The minimum requirement for expressing our relative beliefs in the truth of these
propositions in a consistent fashion is that we rank them in a transitive manner. In other
words, if we believe (a) more than (b), and (b) more than (c), then we must necessarily
believe (a) more than (c); if this were not so, we would continue to argue in circles.
Such a transitive ranking can easily be obtained by assigning a real number to each of
the propositions in a manner so that the larger the numerical value associated with a
proposition, the more we believe it.

Corollaries: Bayes’ theorem and marginalization
5
Cox actually took this much for granted— as being obvious— and wondered what
rules these numbers had to obey in order to satisfy some simple requirements of logical
consistency. He began by making two assertions. The ﬁrst is very straightforward: if
we specify how much we believe that something is true, then we must have implicitly
speciﬁed how much we believe it’s false. He didn’t assume any particular form for this
relationship, but took it as being reasonable that one existed. The second assertion is
slightly more complicated: if we ﬁrst specify how much we believe that (proposition) Y
is true, and then state how much we believe that X is true given that Y is true, then we
must implicitly have speciﬁed how much we believe that both X and Y are true. Again,
he only asserted that these quantities were related but did not specify how. To work
out the actual form of the relationships, Cox used the rules of Boolean logic, ordinary
algebra, and the constraint that if there were several different ways of using the same
information then we should always arrive at the same conclusions irrespective of the
particular analysis-path chosen. He found that this consistency could only be ensured if
the real numbers we had attached to our beliefs in the various propositions obeyed the
usual rules of probability theory:
prob(X|I) + prob

X|I

= 1
(1.1)
and
prob(X, Y |I) = prob(X|Y, I) × prob(Y |I) ,
(1.2)
with 0 = prob(false) and 1 = prob(true) deﬁning certainty; a proof is given in Ap-
pendix B. Here X denotes the proposition that X is false, the vertical bar ‘ | ’ means
‘given’ (so that all items to the right of this conditioning symbol are taken as being true)
and the comma is read as the conjunction ‘and’.
We have made the probabilities conditional on I, to denote the relevant background
information at hand, because there is no such thing as an absolute probability. For exam-
ple, the probability we assign to the proposition ‘it will rain this afternoon’ will depend
on whether there are dark clouds or a clear blue sky in the morning; it will also be af-
fected by whether or not we saw the weather forecast. Although the conditioning on
I is often omitted in calculations, to reduce algebraic cluttering, we must never forget
its existence. A failure to state explicitly all the relevant background information, and
assumptions, is frequently the real cause of heated debates about data analysis.
Equation (1.1) is called the sum rule, and states that the probability that X is true
plus the probability that X is false is equal to one. Equation (1.2) is called the product
rule. It states that the probability that both X and Y are true is equal to the probability
that X is true given that Y is true times the probability that Y is true (irrespective
of X). We can change the description of probability, as when we multiply by 100 to
obtain percentages, but we are not allowed to change the content of eqns (1.1) and
(1.2). Probability calculus uses the unique scale in which the rules take the form of
un-adorned addition and multiplication.
1.3
Corollaries: Bayes’ theorem and marginalization
The sum and product rules of eqns (1.1) and (1.2) form the basic algebra of probability
theory. Many other results can be derived from them. Amongst the most useful are two

6
The basics
known as Bayes’ theorem and marginalization:
prob(X|Y, I) = prob(Y |X, I)× prob(X|I)
prob(Y |I)
(1.3)
and
prob(X|I) =
+∞

−∞
prob(X, Y |I) dY .
(1.4)
Bayes’ theorem, or eqn (1.3), follows directly from the product rule. To see this,
let’s rewrite eqn (1.2) with X and Y transposed (or interchanged):
prob(Y, X|I) = prob(Y |X, I) × prob(X|I) .
Since the statement that ‘Y and X are both true’ is the same as ‘X and Y are both true’,
so that prob(Y, X|I)=prob(X, Y |I), the right-hand side of the above can be equated
to that of eqn (1.2); hence, we obtain eqn (1.3). It is invaluable because it enables us
to turn things around with respect to the conditioning symbol: it relates prob(X|Y, I)
to prob(Y |X, I). The importance of this property to data analysis becomes apparent if
we replace X and Y by hypothesis and data:
prob(hypothesis|data, I) ∝prob(data|hypothesis, I)× prob(hypothesis|I) .
The power of Bayes’ theorem lies in the fact that it relates the quantity of interest, the
probability that the hypothesis is true given the data, to the term we have a better chance
of being able to assign, the probability that we would have observed the measured data
if the hypothesis was true.
The various terms in Bayes’ theorem have formal names. The quantity on the far
right, prob(hypothesis|I), is called the prior probability; it represents our state of
knowledge (or ignorance) about the truth of the hypothesis before we have analysed
the current data. This is modiﬁed by the experimental measurements through the like-
lihood function, or prob(data|hypothesis, I), and yields the posterior probability,
prob(hypothesis|data, I), representing our state of knowledge about the truth of the
hypothesis in the light of the data. In a sense, Bayes’ theorem encapsulates the process
of learning. We should note, however, that the equality of eqn (1.3) has been replaced
with a proportionality, because the term prob(data|I) has been omitted. This is ﬁne for
many data analysis problems, such as those involving parameter estimation, since the
missing denominator is simply a normalization constant (not depending explicitly on
the hypothesis). In some situations, like model selection, this term plays a crucial rˆole.
For that reason, it is sometimes given the special name of evidence. This crisp single
word captures the signiﬁcance of the entity, as opposed to older names, such as prior
predictive and marginal likelihood, which describe how it tends to be used or calculated.
Such a central quantity ought to have a simple name, and ‘evidence’ has been assigned
no other technical meaning (apart from as a colloquial synonym of data).
The marginalization equation, (1.4), should seem a bit peculiar: up to now Y has
stood for a given proposition, so how can we integrate over it? Before we answer that

Corollaries: Bayes’ theorem and marginalization
7
question, let us ﬁrst consider the marginalization equation for our standard X and Y
propositions. It would take the form
prob(X|I) = prob(X, Y |I) + prob

X, Y |I

.
(1.5)
This can be derived by expanding prob(X, Y |I) with the product rule of eqn (1.2):
prob(X, Y |I) = prob(Y, X|I) = prob(Y |X, I)× prob(X|I) ,
and adding a similar expression for prob

X, Y |I

to the left- and right-hand sides,
respectively, to give
prob(X, Y |I) + prob

X, Y |I

=

prob(Y |X, I) + prob

Y |X, I

× prob(X|I) .
Since eqn (1.1) ensures that the quantity in square brackets on the right is equal to unity,
we obtain eqn (1.5). Stated verbally, eqn (1.5) says that the probability that X is true,
irrespective of whether or not Y is true, is equal to the sum of the probability that both
X and Y are true and the probability that X is true and Y is false.
Suppose that instead of having a proposition Y , and its negative counterpart Y , we
have a whole set of alternative possibilities: Y1, Y2, . . . , YM = {Yk}. For example, let’s
imagine that there are M (say ﬁve) candidates in a presidential election; then Y1 could
be the proposition that the ﬁrst candidate will win, Y2 the proposition that the second
candidate will win, and so on. The probability that X is true, for example that unem-
ployment will be lower in a year’s time, irrespective of whoever becomes president, is
then given by
prob(X|I) =
M

k=1
prob(X, Yk|I) .
(1.6)
This is just a generalization of eqn (1.5), and can be derived in an analogous manner, by
putting prob(X, Yk|I) = prob(Yk|X, I) × prob(X|I) in the right-hand side of eqn
(1.6), as long as
M

k=1
prob(Yk|X, I) = 1 .
(1.7)
This normalization requirement is satisﬁed if the {Yk} form a mutually exclusive and
exhaustive set of possibilities. That is to say, if one of the Yk’s is true then all the others
must be false, but one of them has to be true.
The actual form of the marginalization equation in (1.4) applies when we go to
the continuum limit. For example, when we consider an arbitrarily large number of
propositions about the range in which (say) the Hubble constant H0 might lie. As long
as we choose the intervals in a contiguous fashion, and cover a big enough range of
values for H0, we will have a mutually exclusive and exhaustive set of possibilities.
Equation (1.4) is then just a generalization of eqn (1.6), with M→∞, where we have
used the usual shorthand notation of calculus. In this context, Y now represents the
numerical value of a parameter of interest (such as H0) and the integrand prob(X, Y |I)

8
The basics
is technically a probability density function rather than a probability. Strictly speaking,
therefore, we should denote it by a different symbol, such as pdf(X, Y |I), where
pdf(X, Y = y |I) =
lim
δy →0
prob(X, y ⩽Y < y + δy |I)
δy
,
(1.8)
and the probability that the value of Y lies in a ﬁnite range between y1 and y2 (and X
is also true) is given by
prob(X, y1 ⩽Y < y2 |I) =
y2

y1
pdf(X, Y |I) dY .
(1.9)
Since ‘pdf’ is also a common abbreviation for probability distribution function, which
can pertain to a discrete set of possibilities, we will simply use ‘prob’ for anything
related to probabilities; this has the advantage of preserving a uniformity of notation
between the continuous and discrete cases. Thus, in the continuum limit, the normaliza-
tion condition of eqn (1.7) takes the form
+∞

−∞
prob(Y |X, I) dY = 1 .
(1.10)
Marginalization is a very powerful device in data analysis because it enables us to
deal with nuisance parameters; that is, quantities which necessarily enter the analysis
but are of no intrinsic interest. The unwanted background signal present in many exper-
imental measurements, and instrumental parameters which are difﬁcult to calibrate, are
examples of nuisance parameters. Before going on to see how the rules of probability
can be used to address data analysis problems, let’s take a brief look at the history of
the subject.
1.4
Some history: Bayes, Laplace and orthodox statistics
About three hundred years ago, people started to give serious thought to the question
of how to reason in situations where it is not possible to argue with certainty. James
Bernoulli (1713) was perhaps the ﬁrst to articulate the problem, perceiving the differ-
ence between the deductive logic applicable to games of chance and the inductive logic
required for everyday life. The open question for him was how the mechanics of the
former might help to tackle the inference problems of the latter.
Reverend Thomas Bayes is credited with providing an answer to Bernoulli’s ques-
tion, in a paper published posthumously by a friend (1763). The present-day form of the
theorem which bears his name is actually due to Laplace (1812). Not only did Laplace
rediscover Bayes’ theorem for himself, in far more clarity than did Bayes, he also put
it to good use in solving problems in celestial mechanics, medical statistics and even
jurisprudence. Despite Laplace’s numerous successes, his development of probability
theory was rejected by many soon after his death.

Some history: Bayes, Laplace and orthodox statistics
9
Fig. 1.2 A schematic illustration of the result of Laplace’ analysis of the mass of Saturn.
The problem was not really one of substance but concept. To the pioneers such as
Bernoulli, Bayes and Laplace, a probability represented a degree-of-belief or plausibil-
ity: how much they thought that something was true, based on the evidence at hand. To
the 19th century scholars, however, this seemed too vague and subjective an idea to be
the basis of a rigorous mathematical theory. So they redeﬁned probability as the long-
run relative frequency with which an event occurred, given (inﬁnitely) many repeated
(experimental) trials. Since frequencies can be measured, probability was now seen as
an objective tool for dealing with random phenomena.
Although the frequency deﬁnition appears to be more objective, its range of va-
lidity is also far more limited. For example, Laplace used (his) probability theory to
estimate the mass of Saturn, given orbital data that were available to him from various
astronomical observatories. In essence, he computed the posterior pdf for the mass M,
given the data and all the relevant background information I (such as a knowledge of
the laws of classical mechanics): prob(M |{data}, I) ; this is shown schematically in
Fig. 1.2. To Laplace, the (shaded) area under the posterior pdf curve between m1 and
m2 was a measure of how much he believed that the mass of Saturn lay in the range
m1 ⩽M <m2. As such, the position of the maximum of the posterior pdf represents a
best estimate of the mass; its width, or spread, about this optimal value gives an indi-
cation of the uncertainty in the estimate. Laplace stated that: ‘ . . . it is a bet of 11,000
to 1 that the error of this result is not 1/100th of its value.’ He would have won the bet,
as another 150 years’ accumulation of data has changed the estimate by only 0.63%!
According to the frequency deﬁnition, however, we are not permitted to use probability
theory to tackle this problem. This is because the mass of Saturn is a constant and not
a random variable; therefore, it has no frequency distribution and so probability theory
cannot be used.
If the pdf of Fig. 1.2 had to be interpreted in terms of the frequency deﬁnition,
we would have to imagine a large ensemble of universes in which everything remains
constant apart from the mass of Saturn. As this scenario appears quite far-fetched, we
might be inclined to think of Fig. 1.2 in terms of the distribution of the measurements of
the mass in many repetitions of the experiment. Although we are at liberty to think about
a problem in any way that facilitates its solution, or our understanding of it, having to
seek a frequency interpretation for every data analysis problem seems rather perverse.

10
The basics
For example, what do we mean by the ‘measurement of the mass’ when the data consist
of orbital periods? Besides, why should we have to think about many repetitions of an
experiment that never happened? What we really want to do is to make the best inference
of the mass given the (few) data that we actually have; this is precisely the Bayes and
Laplace view of probability.
Faced with the realization that the frequency deﬁnition of probability theory did not
permit most real-life scientiﬁc problems to be addressed, a new subject was invented—
statistics! To estimate the mass of Saturn, for example, one has to relate the mass to the
data through some function called the statistic; since the data are subject to ‘random’
noise, the statistic becomes the random variable to which the rules of probability the-
ory can be applied. But now the question arises: How should we choose the statistic?
The frequentist approach does not yield a natural way of doing this and has, therefore,
led to the development of several alternative schools of orthodox or conventional statis-
tics. The masters, such as Fisher, Neyman and Pearson, provided a variety of different
principles, which has merely resulted in a plethora of tests and procedures without any
clear underlying rationale. This lack of unifying principles is, perhaps, at the heart of
the shortcomings of the cook-book approach to statistics that students are often taught
even today.
The frequency deﬁnition of probability merely gives the impression of a more objec-
tive theory. In reality it just makes life more complicated by hiding the difﬁculties under
the rug, only for them to resurface in a less obvious guise. Indeed, it is not even clear
that the concept of ‘randomness’ central to orthodox statistics is any better-deﬁned than
the idea of ‘uncertainty’ inherent in Bayesian probability theory. For example, we might
think that the numbers generated by a call to a function like rand on a computer con-
stitutes a random process: the frequency of the numbers will be distributed uniformly
between 0 and 1, and their sequential order will appear haphazard. The illusory nature
of this randomness would become obvious, however, if we knew the algorithm and the
seed for the function rand (for then we could predict the sequence of numbers output
by the computer). At this juncture, some might argue that, in contrast to our simple il-
lustration above, chaotic and quantum systems provide examples of physical situations
which are intrinsically random. In fact, chaos theory merely underlines the point that we
are trying to make: the apparent randomness in the long-term behaviour of a classical
system arises because we do not, or cannot, know its initial conditions well enough;
the actual temporal evolution is entirely deterministic, and obeys Newton’s Second Law
of Motion. The quantum case is more difﬁcult to address, since its interpretation (as
opposed to its technical success) is still an open question for many people, but we re-
fer the interested reader to Caves et al. (2002) for a very insightful viewpoint. Either
way, ‘randomness’ is what we call our inability to predict things which, in turn, reﬂects
our lack of knowledge about the system of interest. This is again consistent with the
Bayes and Laplace view of probability, rather than the asserted physical objectivity of
the frequentist approach.
To emphasize this last point, that a probability represents a state of knowledge rather
than a physically real entity, consider the following example of Jaynes (1989). We are
told that a dark bag contains ﬁve red balls and seven green ones. If this bag is shaken

Some history: Bayes, Laplace and orthodox statistics
11
well, and a ball selected at ‘random’, then most of us would agree that the probability of
drawing a red ball is 5/12 and the probability of drawing a green one is 7/12. If the ball
is not returned to the bag, then it seems reasonable that the probability of obtaining a red
or green ball on the second draw will depend on the outcome of the ﬁrst (because there
will be one less red or green ball left in the bag). Now suppose that we are not told the
outcome of the ﬁrst draw, but are given the result of the second one. Does the probability
of the ﬁrst draw being red or green change with the knowledge of the second? Initially,
many of us would be inclined to say ‘no’: at the time of the ﬁrst draw, there were still
ﬁve red balls and seven green ones in the bag; so, the probabilities for red and green
should still be 5/12 and 7/12 irrespective of the outcome of the second draw. The error
in this argument becomes obvious if we consider the extreme case of a bag containing
only one red and one green ball. Although the second draw cannot affect the ﬁrst in a
physical sense, a knowledge of the second result does inﬂuence what we can infer about
the outcome of the ﬁrst one: if the second was green, then the ﬁrst one must have been
red; and vice versa. Thus (conditional) probabilities represent logical connections rather
than causal ones.
The concerns about the subjectivity of the Bayesian view of probability are under-
standable, and the aim of creating an objective theory is quite laudable. Unfortunately,
the frequentist approach does not achieve this goal: neither does its concept of random-
ness appear very rigorous, or fundamental, under scrutiny and nor does the arbitrariness
of the choice of the statistic make it seem objective. In fact, the presumed shortcomings
of the Bayesian approach merely reﬂect a confusion between subjectivity and the difﬁ-
cult technical question of how probabilities should be assigned. The popular argument
goes that if a probability represents a degree-of-belief, then it must be subjective be-
cause my belief could be different from yours. The Bayesian view is that a probability
does indeed represent how much we believe that something is true, but that this belief
should be based on all the relevant information available. While this makes the assign-
ment of probabilities an open-ended question, because the information at my disposal
may not be the same as that accessible to you, it is not the same as subjectivity. It sim-
ply means that probabilities are always conditional, and this conditioning must be stated
explicitly. As Jaynes has pointed out, objectivity demands only that two people having
the same information should assign the same probability; this principle has played a key
rˆole in the modern development of the (objective) Bayesian approach.
In 1946, Richard Cox tried to get away from the controversy of the Bayesian versus
frequentist view of probability. He decided to look at the question of plausible reasoning
afresh, from the perspective of logical consistency. He found that the only rules which
met his requirements were those of probability theory. Although the sum and product
rules of probability are easy to prove for frequencies (with the aid of a Venn diagram),
Cox’s work shows that their range of validity goes much further. Rather than being
restricted to just frequencies, probability theory constitutes the basic calculus for logical
and consistent plausible reasoning; for us, that means scientiﬁc inference (which is the
purpose of data analysis). So, Laplace was right: ‘It is remarkable that a science, which
commenced with a consideration of games of chance, should be elevated to the rank of
the most important subjects of human knowledge.’

12
The basics
1.5
Outline of book
The aim of this book is to show how probability theory can be used directly to address
data analysis problems in a straightforward manner. We will start, in Chapter 2, with the
simplest type of examples: namely, those involving the estimation of the value of a sin-
gle parameter. They serve as a good ﬁrst encounter with Bayes’ theorem in action, and
allow for a useful discussion about error-bars and conﬁdence intervals. The examples
are extended to two, and then several, parameters in Chapter 3, enabling us to introduce
the additional concepts of correlation and marginalization. In Chapter 4, we will see
how the same principles used for parameter estimation can be applied to the problem of
model selection.
Although Cox’s work shows that plausibilities, represented by real numbers, should
be manipulated according to the rules of probability theory, it does not tell us how to
assign them in the ﬁrst place. We turn to this basic question of assigning probabilities in
Chapter 5, where we will meet the important principle of maximum entropy (MaxEnt).
It may seem peculiar that we leave so fundamental a question to such a late stage, but it
is intentional. People often have the impression that Bayesian analysis relies heavily on
the use of clever probability assignments and is, therefore, not generally applicable if
these are not available. Our aim is to show that even when armed only with a knowledge
of pdfs familiar from high school (binomial, Poisson and Gaussian), and na¨ıvet´e (a ﬂat,
or uniform, pdf), probability theory still provides a powerful tool for obtaining useful
results for many data analysis problems. Indeed, we will ﬁnd that most conventional
statistical procedures implicitly assume such elementary assignments. Of course we
might do better by thinking deeply about a more appropriate pdf for any given problem,
but the point is that it is not usually crucial in practice.
In Chapter 6, we consider non-parametric estimation; that is to say, problems where
we know so little about the object of interest that we are unable to describe it adequately
in terms of a few parameters. Here we will encounter MaxEnt once again, but in the
slightly different guise of image processing.
In Chapter 7, we focus our attention on the subject of experimental design. This
concerns the question of ‘what are the best data to collect’, in contrast to most of this
book, which deals with ‘what is the optimal way of analysing these data’. This recip-
rocal question can also be addressed by probability theory, and is of great importance
because the beneﬁts of good experimental (or instrumental) design can far outweigh the
rewards of the sophisticated analysis of poorer data.
Chapters 8 shows how one of the most widely used data analysis procedures in
the physical sciences, least-squares, can be extended to deal with more troublesome
measurements, such as those with ‘outliers’.
Chapters 9 and 10 are concerned with modern numerical techniques for carrying out
out Bayesian calculations, when analytical approximations are inadequate and a brute-
force approach is impractical. In particular, they provide the ﬁrst introductory account
of the novel idea of nested sampling.
Most of the examples used in this book involve continuous pdfs, since this is usually
the nature of parameters in real life. As mentioned in Section 1.3, this can simply be
considered as the limiting case of an arbitrarily large number of discrete propositions.

Outline of book
13
Jaynes (2003) correctly warns us, however, that difﬁculties can sometimes arise if we are
not careful in carrying out the limiting procedure explicitly; this is often the underlying
cause of so-called paradoxes of probability theory. Such problems also occur in other
mathematical calculations, which are quite unrelated to probability. All that one really
needs to avoid them is basic professional care allied to common sense. Indeed, we hope
that the reader will come to share Laplace’s more general view that ‘Probability theory
is nothing but common sense reduced to calculation’.

2
Parameter estimation I
Parameter estimation is a common data analysis problem. Like Laplace, for example,
we may be interested in knowing the mass of Saturn; or, like Millikan, the charge of
the electron. In the simplest case, we are only concerned with the value of a single
parameter; such elementary examples are the focus of this chapter. They serve as a
good introduction to the use of Bayes’ theorem, and allow for a discussion of error-bars
and conﬁdence intervals.
2.1
Example 1: is this a fair coin?
Let us begin with the analysis of data from a simple coin-tossing experiment. Suppose
I told you that I had been to Las Vegas for my holidays, and had come across a very
strange coin in one of the casinos; given that I had observed 4 heads in 11 ﬂips, do you
think it was a fair coin? By fair, we mean that we would be prepared to lay an even
50 : 50 bet on the outcome of a ﬂip being a head or a tail. In ascribing the property of
fairness (solely) to the coin we are, of course, assuming that the coin-tosser was not
skilled enough to be able to control the initial conditions of the ﬂip (such as the angular
and linear velocities). If we decide that the coin was fair, the question which follows
naturally is how sure are we that this was so; if it was not fair, how unfair do we think it
was?
A sensible way of formulating this problem is to consider a large number of con-
tiguous propositions, or hypotheses, about the range in which the bias-weighting of the
coin might lie. If we denote the bias-weighting by H, then H = 0 and H = 1 can
represent a coin which produces a tail or a head on every ﬂip, respectively. There is a
continuum of possibilities for the value of H between these limits, with H = 1/2 indi-
cating a fair coin. The propositions could then be, for example: (a) 0.00 ⩽H < 0.01;
(b) 0.01 ⩽H < 0.02; (c) 0.02 ⩽H < 0.03; and so on. Our state of knowledge about
the fairness, or the degree of unfairness, of the coin is then completely summarized by
specifying how much we believe these various propositions to be true. If we assign a
high probability to one (or a closely-grouped few) of these propositions, compared to
the others, then this would indicate that we were conﬁdent in our estimate of the bias-
weighting. If there was no such strong distinction, then it would reﬂect a high level of
ignorance about the nature of the coin.
In the light of the data, and the above discussion, our inference about the fairness of
this coin is summarized by the conditional pdf: prob(H|{data}, I). This is, of course,
shorthand for the limiting case of a continuum of propositions for the value of H; that
is to say, the probability that H lies in an inﬁnitesimally narrow range is given by
prob(H|{data}, I) dH. To estimate this posterior pdf, we need to use Bayes’ theo-
rem (eqn 1.3); it relates the pdf of interest to two others, which are easier to assign:

Example 1: is this a fair coin?
15
prob(H|{data}, I) ∝prob({data}|H, I) × prob(H|I) .
(2.1)
Note that we have omitted the denominator prob({data}|I), as it does not involve
bias-weighting explicitly, and replaced the equality by a proportionality. If required, we
can evaluate the missing constant subsequently from the normalization condition of eqn
(1.10):
1

0
prob(H|{data}, I) dH = 1 .
(2.2)
The prior pdf, prob(H|I), on the far right-hand side of eqn (2.1), represents what
we know about the coin given only the information I that we are dealing with a ‘strange
coin from Las Vegas’. Since casinos can be rather dubious places, we should keep a very
open mind about the nature of the coin; a simple probability assignment which reﬂects
this is a uniform, or ﬂat, pdf:
prob(H|I) =

1
0 ⩽H ⩽1 ,
0
otherwise .
(2.3)
This prior state of knowledge, or ignorance, is modiﬁed by the data through the
likelihood function: prob({data}|H, I). It is a measure of the chance that we would
have obtained the data that we actually observed, if the value of the bias-weighting was
given (as known). If, in the conditioning information I, we assume that the ﬂips of
the coin were independent events, so that the outcome of one did not inﬂuence that of
another, then the probability of obtaining the data ‘R heads in N tosses’ is given by the
binomial distribution:
prob({data}|H, I) ∝HR (1−H)N−R .
(2.4)
We leave a formal derivation of this pdf to Chapter 5, but point out that eqn (2.4) seems
reasonable because H is the chance of obtaining a head on any ﬂip, and there were
R of them, and 1−H is the corresponding probability for a tail, of which there were
N −R . For simplicity, an equality has again been replaced by a proportionality; this
is permissible since the omitted terms contain factors of only R and N, rather than the
quantity of interest H.
According to eqn (2.1), the product of eqns (2.3) and (2.4) yields the posterior pdf
we require; it represents our state of knowledge about the nature of the coin in the light
of the data. To get a feel for this result, it is instructive to see how this pdf evolves as
we obtain more and more data pertaining to the coin. This is done with the aid of data
generated in a computer simulation, and the results of their analyses is shown in Fig.
2.1. The panel in the top left-hand corner shows the posterior pdf for H given no data;
it is, of course, the same as the prior pdf of eqn (2.3). It indicates that we have no more
reason to believe that the coin is fair than we have to think that it is double-headed,
double-tailed, or of any other intermediate bias-weighting.
Suppose that the coin is ﬂipped once and it comes up heads; what can we now say
about the value of H? The resulting posterior pdf, shown in the second panel of Fig. 2.1,

16
Parameter estimation I
Fig. 2.1 The evolution of the posterior pdf for the bias-weighting of a coin, prob(H|{data}, I),
as the number of data available increases. The ﬁgure on the top right-hand corner of each panel
shows the number of data analysed; in the early panels, the H or T in the top left-hand corner
shows whether the result of that (last) ﬂip was a head or a tail.

Example 1: is this a fair coin?
17
goes to zero for H = 0 and rises linearly to having the greatest value at H = 1. Based
purely on this single datum, it is most probable that the (strange) coin has two heads;
after all, we don’t have any empirical evidence that it even has a tail yet. Although
H = 1 is in some ways our ‘best’ estimate so far, the posterior pdf indicates that this
value is not much more probable than many others. The only thing we can really be sure
about is that the coin is not double-tailed; hence, the posterior pdf is zero at H = 0.
If the coin is ﬂipped for a second time and again comes up heads, the posterior pdf
becomes slightly more peaked towards H = 1 (proportional to H 2); this is plotted in
the third panel of Fig. 2.1. Since we still haven’t seen a tail, our inclinations following
the ﬁrst datum are just reinforced. As soon as a tail is obtained, however, the posterior
pdf for H = 1 also drops to zero; we are then sure that the coin is not double-headed
either. The next panel shows the resultant pdf, given that the third ﬂip produced a tail
(proportional to (1−H)H 2). If the fourth ﬂip also comes up tails, then the maximum of
the posterior pdf is at H = 0.5. It then becomes most probable that the coin is fair, but
there is still a large degree of uncertainty in this estimate; this is indicated graphically
in panel 5 of Fig. 2.1.
The remainder of Fig. 2.1 shows how the posterior pdf for H evolves as the number
of data analysed becomes larger and larger. We see that the position of the maximum
wobbles around, but that the amount by which it does so decreases with the increasing
number of observations. The width of the posterior pdf also becomes narrower with
more data, indicating that we are becoming increasingly conﬁdent in our estimate of
the bias-weighting. For the coin in this example, the best estimate of H eventually
converges to 0.25. This was, of course, the value chosen to simulate the ﬂips; they can
be thought of as coming from a tetrahedral coin with a head on one face and tails on the
other three!
2.1.1
Different priors
Most people tend to be happy with the binomial distribution for the likelihood function,
but worry about the prior pdf. The uniform assignment of eqn (2.3) was chosen mostly
for its simplicity; it is just a na¨ıve way of encoding a lot of initial ignorance about the
coin. How would our inference about the fairness of the coin have changed if we had
chosen a different prior?
To address this question, let’s repeat the analysis of the data above with two alter-
native prior pdfs; the results are shown in Fig. 2.2. The solid line is for the uniform pdf
used in Fig. 2.1, and is included for ease of comparison. One of the alternative priors
is peaked around H = 0.5 and reﬂects our background information that most coins
are fair (even in Las Vegas); it is plotted with a dashed line. It has a width which is
broad enough to comfortably accommodate the possibility that H might be as low as
0.35, or as high as 0.65, but expresses considerable doubt about greater levels of biased
behaviour. The second alternative, shown with a dotted line, is very sharply peaked at
H = 0 and H = 1; it indicates that we would expect a ‘strange coin from a casino’ to
be heavily biased, one way or another.
Figure 2.2 shows how the posterior pdfs for the different priors evolve, as more
and more data become available; they have all been scaled vertically to have the same

18
Parameter estimation I
Fig. 2.2 The effect of different priors, prob(H|I), on the posterior pdf for the bias-weighting of
a coin. The solid line is the same as in Fig. 2.1, and is included for ease of comparison. The case
for two alternative priors, reﬂecting slightly different assumptions in the conditioning information
I, are shown with dashed and dotted lines.

Example 1: is this a fair coin?
19
maximum value to aid comparison between them. We ﬁnd that when there are few
data, the resulting posteriors are different in detail; as the number of data increases,
they all become more sharply peaked and converge to the same answer. This seems
quite reasonable. The outcome of only a few ﬂips tells us little about the fairness of
the coin. Our state of knowledge after the analysis of these data is, therefore, strongly
dependent on what we knew or assumed before the results; hence, the posteriors are
somewhat different. As the empirical evidence grows, we are eventually led to the same
conclusions irrespective of our initial beliefs; the posterior pdf is then dominated by the
likelihood function, and the choice of the prior becomes largely irrelevant.
Two further curious points may be noted: (i) it takes quite a lot of ﬂips to be able
to estimate the bias-weighting with some degree of conﬁdence (about a thousand to
pin it down between 0.2 and 0.3); (ii) the posterior pdfs for the solid and dotted lines
converge together quite quickly, but the dashed case takes much longer. The answer to
the ﬁrst observation is that it just does, but the number of ﬂips required will depend on
the actual bias-weighting of the coin. If the coin was tossed ten times and came up tails
on every occasion, we would rapidly conclude that it was biased; on the other hand,
the result of 45 heads and 55 tails in a hundred ﬂips would still leave us somewhat
uncertain as to whether the coin was fair. With regard to the second point, both the ﬂat
(solid) and the spiky (dotted) priors encode a large degree of ignorance about the nature
of the coin. Despite the peculiar shape of the latter, it is fairly ﬂat for most values of
H; the strange-looking spikes at H = 0 and H = 1 disappear as soon as a head and
a tail have been observed. The ‘fair-minded’ prior (dashed line), however, claims to
be moderately well-informed about the character of the coin regardless of the data. It,
therefore, takes much more to be convinced that the coin is not fair. Even though the
prior probability for H =0.5 was about a million times greater than that of H =0.25, a
thousand ﬂips were enough to drag it (kicking and screaming, perhaps) to the conclusion
that the value of H was less than 0.3 (but more than 0.2).
2.1.2
Sequential or one-step data analysis?
If we have a set of data {Dk} comprising the outcome of N ﬂips of a coin (k =
1, 2, . . . , N), then Bayes’ theorem tells us that the posterior pdf for H is given by
prob(H|{Dk}, I) ∝prob({Dk}|H, I) × prob(H|I) .
(2.5)
This is a one-step process in that we consider the data collectively, as a whole. As
an alternative, we could also think of analysing the data sequentially (as they arrive).
That is to say, we start by computing the posterior pdf based on the ﬁrst datum D1,
prob(H|D1, I), and use it as the prior for the analysis of the second datum D2; the
new posterior could then be used as the prior for the third datum, and so on. If we
continue this procedure, will we obtain the same result as the one-step approach?
To simplify matters, let us consider the case of just two data. The posterior pdf for
H, based on both, is merely a special case of eqn (2.5) for N =2:
prob(H|D2, D1, I) ∝prob(D2, D1|H, I) × prob(H|I) .
(2.6)
Equally well, we could use Bayes’ theorem to express the posterior in terms of pdfs
conditional on D1 throughout (like the background information I):

20
Parameter estimation I
prob(H|D2, D1, I) ∝prob(D2|H, D1, I) × prob(H|D1, I) .
(2.7)
This shows that the prior in eqn (2.6) can certainly be replaced by the posterior pdf based
on the ﬁrst datum, but the other term doesn’t quite look like the likelihood function for
the second datum. Implicit in the assignment of the binomial pdf of eqn (2.4), however,
was the assumption (subsumed in I) that the data were independent. This means that,
given the value of H, the result of one ﬂip does not inﬂuence what we can infer about
the outcome of another; mathematically, we write this as
prob(D2|H, D1, I) = prob(D2|H, I) .
Substituting this in eqn (2.7) yields the desired relationship, showing that the two data
can either be analysed together or one after the other. This argument can be extended to
the third datum D3, giving
prob(H|D3, D2, D1, I) ∝prob(D3|H, I) × prob(H|D2, D1, I) ,
and repeated until all the data have been included. We should not really be surprised
that both the one-step and sequential methods of analysis give the same answer; after
all, the requirements of such consistency was what led Cox to the rules of probability
theory in the ﬁrst place.
When one ﬁrst learns about Bayes’ theorem, and sees how the data modify the prior
through the likelihood function, there is occasionally a temptation to use the resulting
posterior pdf as the prior for a re-analysis of the same data. It would be erroneous to
do this, and the results quite misleading. In order to justify any data analysis procedure,
we must be able to relate the pdf of interest to others used in its calculation through
the sum and product rules of probability (or their corollaries). If we cannot do this then
the analysis will be suspect at best, and open to logical inconsistencies. For the case of
this proposed ‘boot-strapping’ we would, in fact, be trying to relate the posterior pdf to
itself; this can only be done by an equality, and not through the likelihood function. If
we persist in our folly, and keep repeating it, the resulting posterior pdfs will become
sharper and sharper; we will just fool ourselves into thinking that the quantity of interest
can be estimated far more accurately than is warranted by the data.
2.2
Reliabilities: best estimates, error-bars and conﬁdence
intervals
We have seen how the posterior pdf encodes our inference about the value of a parame-
ter, given the data and the relevant background information. Often, however, we wish to
summarize this with just two numbers: the best estimate and a measure of its reliability.
Since the probability (density) associated with any particular value of the parameter is
a measure of how much we believe that it lies in the neighbourhood of that point, our
best estimate is given by the maximum of the posterior pdf. If we denote the quantity of
interest by X, with a posterior pdf P = prob(X|{data}, I) , then the best estimate of
its value Xo is given by the condition

Reliabilities: best estimates, error-bars and conﬁdence intervals
21
dP
dX

Xo
= 0 .
(2.8)
Strictly speaking, we should also check the sign of the second derivative to ensure that
Xo represents a maximum rather than a minimum (or a point of inﬂexion):
d2P
dX2

Xo
< 0 .
(2.9)
In writing the derivatives of P with respect to X we are, of course, assuming that X
is a continuous parameter. If it could only take discrete values, our best estimate would
still be that which gave the greatest posterior probability; it’s just that we couldn’t then
use the calculus notation of eqns (2.8) and (2.9), because the gradients are not deﬁned
(since the increment in X cannot be inﬁnitesimally small).
To obtain a measure of the reliability of this best estimate, we need to look at the
width or spread of the posterior pdf about Xo. When considering the behaviour of any
function in the neighbourhood of a particular point, it is often helpful to carry out a
Taylor series expansion; this is simply a standard tool for (locally) approximating a
complicated function by a low-order polynomial. Rather than dealing directly with the
posterior pdf P, which is a ‘peaky’ and positive function, it is better to work with its
logarithm L,
L = loge

prob(X|{data}, I)

,
(2.10)
since this varies much more slowly with X. Expanding L about the point X =Xo , we
have
L = L(Xo) + 1
2
d2L
dX2

Xo
(X−Xo)2 + · · · ,
(2.11)
where the best estimate of X is given by the condition
dL
dX

Xo
= 0 ,
(2.12)
which is equivalent to eqn (2.8) because L is a monotonic function of P.
The ﬁrst term in the Taylor series, L(Xo), is a constant and tells us nothing about the
shape of the posterior pdf. The linear term, which would be proportional to X−Xo , is
missing because we are expanding about the maximum (as indicated by eqn 2.12). The
quadratic term is, therefore, the dominant factor determining the width of the posterior
pdf and plays a central rˆole in the reliability analysis. Ignoring all the higher-order
contributions, the exponential of eqn (2.11) yields
prob(X|{data}, I) ≈A exp
	
1
2
d2L
dX2

Xo
(X−Xo)2

,
(2.13)
where A is a normalization constant. Although this expression looks a little weird, what
we have really done is to approximate the posterior pdf by the ubiquitous Gaussian
distribution. Also known as the normal distribution, it is usually written as

22
Parameter estimation I
Fig. 2.3 The Gaussian, or normal, distribution. It is symmetric with respect to the maximum, at
x=µ, and has a full width at half maximum (FWHM) of about 2.35σ.
prob(x|µ,σ) =
1
σ
√
2π exp
	
−(x −µ)2
2 σ2

,
(2.14)
and is plotted in Fig. 2.3; this function is symmetric about the maximum, at x=µ, and
has a width which is proportional to σ. Comparing the exponents of eqns (2.13) and
(2.14), we are reassured to ﬁnd that the posterior pdf for X has a maximum at X =Xo;
its width, characterized by the parameter σ, is inversely related to the square root of the
second derivative of L at X =Xo:
σ =

−d2L
dX2

Xo
−1/2
,
(2.15)
where d2L/d X2 is necessarily negative following eqn (2.9). Our inference about the
quantity of interest is conveyed very concisely, therefore, by the statement
X = Xo ± σ ,
(2.16)
where Xo is the best estimate of the value of X, and σ is a measure of its reliability;
the parameter σ is usually referred to as the error-bar.
The expression of eqn (2.16) is, of course, just shorthand for the Gaussian approx-
imation of eqn (2.13). The integral properties of the normal distribution tell us that the
probability that the true value of X lies within ±1σ of X =Xo is 67%:
prob(Xo−σ ⩽X < Xo+σ |{data}, I) =
Xo+σ

Xo−σ
prob(X, {data}|I) dX ≈0.67 .
Similarly, the probability that X lies within ±2σ of Xo is 95%; we would be quite
surprised, however, if our best estimate of X was wrong by more than about 3σ.

Reliabilities: best estimates, error-bars and conﬁdence intervals
23
2.2.1
The coin example
As a concrete example of the above analysis, let’s consider the case of the coin-tossing
experiment of the previous section. Multiplying the uniform prior of eqn (2.3) with
the binomial likelihood function of eqn (2.4), we obtain the posterior pdf for the bias-
weighting:
prob(H|{data}, I) ∝HR (1−H)N−R ,
where 0 ⩽H ⩽1 . Taking its natural logarithm, according to eqn (2.10), gives
L = constant + R log e(H) + (N −R) log e(1−H).
(2.17)
For the best estimate of H and its error-bar, we need both the ﬁrst and the second
derivative of L; differentiating twice, with respect to H, we have
dL
dH = R
H −(N −R)
(1−H)
and
d2L
dH2 = −R
H2 −(N −R)
(1−H)2 .
(2.18)
However, eqn (2.12) states that, at the optimal value for the bias-weighting Ho, the ﬁrst
derivative is equal to zero:
dL
dH

Ho
=
R
Ho
−(N −R)
(1−Ho) = 0 .
Hence, algebraic rearrangement of this equation tells us that the best estimate of the
bias-weighting is given by the relative frequency of outcomes of heads:
Ho = R
N .
(2.19)
According to eqn (2.15), the associated error-bar is related to the second derivative of L
evaluated at H = Ho . Substituting for R from eqn (2.19), into the right-hand formula
in eqn (2.18), and after some rearrangement and cancellation, we obtain
d2L
dH2

Ho
= −
N
Ho (1−Ho) .
Equation (2.15) then tells us that the error-bar for the estimate of the bias-weighting is
given by
σ =

Ho (1−Ho)
N
.
(2.20)
Since Ho does not vary a lot after a moderate amount of data have been analysed, the
numerator tends to a constant value; thus the width of the posterior becomes inversely
proportional to the square root of the number of data, as can be seen in Fig. 2.1. This
formula for the error-bar also conﬁrms our earlier assertion that it is easier to identify a
highly biased coin than it is to be conﬁdent that it’s fair (because the numerator in eqn
(2.20) has its greatest value when Ho= 0.5). The posterior pdf for H, given the data 9
heads in 32 ﬂips, and the Gaussian approximation to it resulting from the above analysis
(H = 0.28 ± 0.08), are shown in Fig. 2.4.

24
Parameter estimation I
Fig. 2.4 The Gaussian, or quadratic, approximation (dashed line) to the posterior pdf for the
bias-weighting of the coin, given 9 heads in 32 ﬂips.
2.2.2
Asymmetric posterior pdfs
The preceding analysis, leading to the error-bar formula of eqn (2.15), relies on the
validity of the quadratic expansion of eqn (2.11). Figure 2.4 illustrates that this is usu-
ally a reasonable approximation; in fact, the agreement with the Gaussian pdf tends to
improve steadily as the number of data increases. There are times, however, when the
posterior pdf is markedly asymmetric; this can be seen in the early panels of Figs. 2.1
and 2.2. While the maximum of the posterior (Xo) can still be regarded as giving the
best estimate, the true value is now more likely to be on one side of this rather than the
other. The concept of an error-bar does not seem appropriate in this case, as it implicitly
entails the idea of symmetry.
A good way of expressing the reliability with which a parameter can be inferred, for
an asymmetric posterior pdf, is through a conﬁdence interval. Since the area under the
posterior pdf between X1 and X2 is proportional to how much we believe that X lies
in that range, the shortest interval that encloses 95% of the area represents a sensible
measure of the uncertainty of the estimate. Assuming that the posterior pdf has been
normalized, to have unit area, we need to ﬁnd X1 and X2 such that
prob(X1 ⩽X < X2 |{data}, I) =
X2

X1
prob(X|{data}, I) dX ≈0.95 ,
(2.21)
where the difference X2 −X1 is as small as possible. The region X1 ⩽X <X2 is called
the shortest 95% conﬁdence interval, and is illustrated in Fig. 2.5.
A natural question which arises is ‘Why have we chosen the 95% conﬁdence level?’
The answer is that this is traditionally seen as a reasonable value, as it provides a re-
spectably conservative estimate of the reliability. There is nothing to stop us from giving
the shortest 50%, 70%, 99%, or any other, conﬁdence interval. Indeed, there is some-
thing to be said for listing a whole set of nested intervals since this provides a more

Reliabilities: best estimates, error-bars and conﬁdence intervals
25
Fig. 2.5 The shortest 95% conﬁdence interval, shown by the shaded region.
complete picture of the reliability analysis; by doing so, however, we are merely recon-
structing the posterior pdf!
When dealing with a highly asymmetric posterior pdf, the question of what we mean
by the ‘best’ estimate is rather open-ended. The maximum still indicates the single most
probable value for the parameter of interest, but the mean, or expectation, can be thought
of as being more representative as it takes into account the skewness of the pdf. For a
normalized posterior, this weighted average ⟨X⟩is given by
⟨X⟩=

X prob(X|{data}, I) dX ,
(2.22)
and is sometimes denoted by E(X) or X (the latter not to be confused with ‘not X’);
as usual, the integral is replaced by a sum when X can only take discrete values. If
the posterior pdf has not been normalized then, of course, the right-hand side must
be divided by

prob(X|{data}, I) dX. For the case of the pdf shown in Fig. 2.5,
the mean lies slightly to the left of Xo. If the posterior pdf is symmetric about the
maximum, as in the Gaussian distribution, then the mean and the maximum become
coincident (⟨X⟩=Xo).
2.2.3
Multimodal posterior pdfs
So far, we have only considered posterior pdfs which have a single maximum. Depend-
ing on the nature of the experimental data to be analysed, we can sometimes obtain
posteriors which are multimodal; this is depicted schematically in Fig. 2.6. There is no
difﬁculty when one of the maxima is very much larger than the others: we can sim-
ply ignore the subsidiary solutions, to a good approximation, and concentrate on the
global maximum. The problem arises when there are several maxima of comparable
magnitude. What do we now mean by a best estimate, and how should we quantify its
reliability?
Well, to a large extent, the difﬁculty is of our own making. The posterior pdf gives
a complete description of what we can infer about the value of the parameter in the
light of the data, and our relevant prior knowledge. The idea of a best estimate and an
error-bar, or even a conﬁdence interval, is merely an attempt to summarize the posterior
with just two or three numbers; sometimes this just can’t be done, and so these concepts

26
Parameter estimation I
Fig. 2.6 A multimodal posterior pdf. Since its shape cannot be summarized by just a couple of
numbers, the concept of a best estimate and an error-bar is inappropriate.
are not valid. The posterior pdf still exists, and we are free to draw from it whatever
conclusions are appropriate.
Consider the special case of a multimodal distribution having two (roughly) equal-
sized probability bumps; such a bimodal pdf is seen in Fig. 2.6, if the structure to the
right of X= 20 is ignored. This posterior conveys the message that the value of X is
either −10 , give or take a little bit, or about +10 ; we could write it as: ‘X =−10 ± 2
or X = +10 ± 2’. Since the mean of the posterior pdf is still unique, it is sometimes
suggested that this should be used as a (single) best estimate. The problem with that
argument is that the expectation ⟨X⟩≈0, a value which the posterior pdf indicates is
very improbable! Even if we turned a blind eye to this difﬁculty, and used the mean
as the best estimate, we would have to assign a sizeable error-bar to encompass the
most probable values of X; this would again be somewhat misleading, as it is not a
good reﬂection of the information inherent in the posterior pdf. For the bimodal case
we can characterize the posterior in terms of a few numbers: two best estimates and
their associated error-bars, or disjoint conﬁdence intervals. For a general multimodal
pdf, the most honest thing we can do is just display the posterior itself.
2.3
Example 2: Gaussian noise and averages
For our second example, let us consider the problem of estimating the mean of a Gaus-
sian process. The normal distribution, which was deﬁned in eqn (2.14), and plotted in
Fig. 2.3, is often used as a theoretical model for describing the noise (or imperfections)
associated with experimental data. We leave its formal derivation with maximum en-
tropy to Chapter 5, but note that its use is traditionally justiﬁed by appealing to the
central limit theorem. For our present purposes, we need merely say that the probability
of the kth datum having a value xk is given by
prob(xk|µ, σ) =
1
σ
√
2π exp
	
−(xk−µ)2
2 σ2

,
(2.23)
where µ is the true value of the parameter of interest, and σ is a measure of the error
in its measurement. Given a set of data {xk}, what is the best estimate of µ and how

Example 2: Gaussian noise and averages
27
conﬁdent are we of this prediction?
In this example, we will restrict ourselves to the relatively easy case when the value
of σ is known; the more realistic situation, when this condition is relaxed, is left to
Section 3.3. Our inference about the value of µ is expressed, therefore, by the posterior
pdf prob(µ|{xk}, σ, I); to help us calculate it, we use Bayes’ theorem:
prob(µ|{xk}, σ, I) ∝prob({xk}|µ, σ, I) × prob(µ|σ, I) .
(2.24)
Although a knowledge of σ could be subsumed into the background information I, we
will continue to state this conditioning explicitly (for future reference). If we assume
that the data are independent, so that the measurement of one datum does not inﬂuence
what we can infer about the outcome of another (when given the values of µ and σ),
then the likelihood function is given by the product of the probabilities of obtaining the
N individual data:
prob({xk}|µ, σ, I) =
N

k=1
prob(xk|µ, σ, I) .
(2.25)
This follows from the repeated use of eqn (1.2), which reduces to
prob(X, Y |I) = prob(X|I) × prob(Y |I) ,
since the notion of independence implies that
prob(X|Y, I) = prob(X|I) .
As a knowledge of the width of a Gaussian tells us nothing about the position of its
centre, let us assign a simple uniform pdf for the prior:
prob(µ|σ, I) = prob(µ|I) =

A
µmin ⩽µ ⩽µmax ,
0
otherwise ,
(2.26)
where the normalization constant A is given by the reciprocal of the range µmax−µmin,
determined by the relevant background information I. According to eqn (2.24), if we
multiply this prior by the likelihood function resulting from eqns (2.23) and (2.25), we
obtain the following for the logarithm of the posterior pdf L:
L = loge

prob(µ|{xk}, σ, I)

= constant −
N

k=1
(xk−µ)2
2 σ2
,
(2.27)
where the constant includes all terms not involving µ; the posterior pdf is, of course,
zero outside the range µmin to µmax.

28
Parameter estimation I
To ﬁnd the best estimate µo , eqn (2.12) tells us that we need to differentiate L with
respect to µ and set the derivative to zero:
dL
dµ

µo
=
N

k=1
xk−µo
σ2
= 0 .
Since σ is a constant which is independent of k, we can take it outside the summation
sign and rearrange the equation as follows:
N

k=1
xk =
N

k=1
µo = Nµo .
Thus the best estimate of the value of µ is given by the simple arithmetic average of the
sample of measurements {xk}:
µo = 1
N
N

k=1
xk ,
(2.28)
irrespective of the magnitude of the error in the measurement process. Our conﬁdence in
the reliability of this best estimate, however, does depend on the size of σ, as indicated
by the second derivative of L:
d2L
dµ2

µo
= −
N

k=1
1
σ2 = −N
σ2 .
According to eqn (2.15), the error-bar we should associate with µo is given by the square
root of the inverse of (minus) this quantity; thus we can summarize our inference about
the value of µ by the statement
µ = µo ±
σ
√
N
.
(2.29)
Just as in the case of the coin-tossing experiment, we encounter the familiar result that
the reliability of the estimate is proportional to the square root of the number of data.
In Section 2.2, we noted that the error-bar analysis relies on the validity of the
quadratic expansion of eqn (2.11). For the case of Gaussian noise considered above,
this is not only a reasonable approximation but an exact identity (because all higher
derivatives of L are zero). Thus, the posterior pdf is completely deﬁned by the param-
eters of eqn (2.29). The only proviso concerns the values of µmin and µmax. We could,
in principle, express an inordinate amount of prior ignorance by letting them tend to
±∞(although not at the outset!) but, as long as the range they span is large enough,
their actual value has no effect on the posterior pdf. If the best estimate and error-bar
allow for values of µ outside the range µmin to µmax, then it is most honest simply to
display the posterior itself with its cut-offs; in that case, probability theory would just
be warning us that our prior knowledge was at least as important as the data.

Example 3: the lighthouse problem
29
2.3.1
Data with different-sized error-bars
In the above analysis, we assumed that the magnitude of the error-bar for each datum
was the same; this may well be reasonable if all the measurements were made with a
given experimental set-up. Suppose, however, that the data were obtained from several
laboratories using equipment of varying sophistication. How should we then combine
the evidence from observations of differing quality?
Let us assume that the measurement error can still be modelled through a Gaussian
pdf, so that the probability of the kth datum having a value xk is
prob(xk|µ,σk) =
1
σk
√
2π exp
	
−(xk−µ)2
2 σk2

,
where the error-bar for each datum σk need not be of the same size. Following the
arguments between eqns (2.23) and (2.27), we ﬁnd that the logarithm of the posterior
pdf for µ is now given by
L = loge

prob(µ|{xk}, {σk}, I)

= constant −
N

k=1
(xk−µ)2
2 σk2
.
As usual, we obtain the expression for the best estimate by setting the ﬁrst derivative of
L equal to zero; since the resulting equation does not simplify as much as before, the
formula for µo is somewhat more complicated:
µo =
N

k=1
wkxk
 N

k=1
wk ,
where wk = 1
σk2 .
(2.30)
Rather than the best estimate being given by the arithmetic mean of the data, we must
now calculate their weighted average. Less reliable data will have larger error-bars σk
and correspondingly smaller weights wk. The second derivative of L yields the error-bar
for the best estimate and allows us to summarize our inference about µ as
µ = µo ±
 N

k=1
wk
−1/2
.
(2.31)
Note that if all the data were of comparable quality, so that σk=σ, then eqns (2.30) and
(2.31) would reduce to the simpler forms of eqns (2.28) and (2.29).
2.4
Example 3: the lighthouse problem
For the third example, we follow Gull (1988) in considering a very instructive problem
found on a problems sheet for ﬁrst-year undergraduates at Cambridge: ‘A lighthouse is
somewhere off a piece of straight coastline at a position α along the shore and a distance
β out at sea. It emits a series of short highly collimated ﬂashes at random intervals and
hence at random azimuths. These pulses are intercepted on the coast by photo-detectors

30
Parameter estimation I
Fig. 2.7 A schematic illustration of the geometry of the lighthouse problem.
that record only the fact that a ﬂash has occurred, but not the angle from which it came.
N ﬂashes have so far been recorded at positions {xk}. Where is the lighthouse?’
The geometry of the problem above is illustrated in Fig. 2.7. Given the nature of the
lighthouse emissions, it seems reasonable to assign a uniform pdf for the azimuth of the
kth datum θk:
prob(θk|α, β, I) = 1
π ,
(2.32)
where the angle must lie between ±π/2 radians (or ±90◦) to have been detected. Since
the photo-detectors are only sensitive to position along the coast rather than direction,
we must relate θk to xk. An inspection of Fig. 2.7, and elementary trigonometry, allows
us to write:
β tan θk = xk−α .
(2.33)
As we shall see in Section 3.6, when dealing with the subject of changing variables, the
relationship of eqn (2.33) can be used to rewrite the pdf of eqn (2.32) as
prob(xk|α, β, I) =
β
π

β 2 + (xk−α)2 .
(2.34)
This tells us that the probability that the kth ﬂash will be recorded at position xk, know-
ing the coordinates of the lighthouse (α, β), is given by a Cauchy distribution. The
functional form of this pdf is met frequently in physics and is often called a Lorentzian.
It is symmetric about the maximum, at xk = α, and has a FWHM of 2β; the distribution
is plotted in Fig. 2.8.
Inferring the position of the lighthouse from the data involves the estimation of
both α and β; as a two-parameter problem, this is outside the scope of the present
chapter. We will therefore assume that the distance out to sea is known and reduce it to
a single parameter example; the reader should have no difﬁculty, however, in tackling
the full problem after the material of the next chapter. Our inference about the position

Example 3: the lighthouse problem
31
Fig. 2.8 The Cauchy, or Lorentzian, distribution. It is symmetric with respect to the maximum,
at x=α, and has a FWHM of twice β.
of the lighthouse is then expressed by the posterior pdf prob(α|{xk}, β, I); to help us
calculate it, we begin by writing down Bayes’ theorem:
prob(α|{xk}, β, I) ∝prob({xk}|α, β, I) × prob(α|β, I) .
(2.35)
As a knowledge of β, without the data, tells us nothing new about the position of the
lighthouse along the shore, let us assign a simple uniform pdf for the prior:
prob(α|β, I) = prob(α|I) =

A
αmin ⩽α ⩽αmax ,
0
otherwise,
(2.36)
where αmin and αmax could represent the limits of the coastline, if these were known,
or be made arbitrarily large (up to the size of the Earth!) if we were very ignorant; the
normalization constant A is, of course, just the reciprocal of this prior range. Since the
recording of a signal at one photo-detector does not inﬂuence what we can infer about
the position of another measurement (when given the location of the lighthouse), the
likelihood function for these independent data is just the product of the probabilities for
obtaining the N individual detections:
prob({xk}|α, β, I) =
N

k=1
prob(xk|α, β, I) .
(2.37)
Substituting the prior of eqn (2.36) and the likelihood function resulting from eqns
(2.34) and (2.37) into Bayes’ theorem of eqn (2.34), allows us to write the logarithm of
the posterior pdf as
L = loge

prob(α|{xk}, β, I)

= constant −
N

k=1
loge

β 2 + (xk−α)2
, (2.38)
where the constant includes all terms not involving α. For the purposes of this example,
we will assume that the prior range is so large that we need not worry about any cut-offs
imposed on the posterior.

32
Parameter estimation I
The best estimate of the position αo is given by the maximum of the posterior pdf;
differentiating eqn (2.38) once, with respect to α, we obtain the condition
dL
dα

αo
= 2
N

k=1
xk−αo
β 2 + (xk−αo)2 = 0 .
(2.39)
Unfortunately, it is difﬁcult to rearrange this equation so that αo is expressed in terms
of {xk} and β. Although an analytical solution may confound us, there is nothing to
stop us tackling the problem numerically. The most straightforward method is to use
brute force and ignorance: simply evaluate L, from eqn (2.38), for a whole series of
different possible values of α; the number giving rise to the largest L will be the best
estimate. This is a tedious procedure if carried out manually, but can be performed
painlessly if automated through a short computer program. If we plot the exponential
of L, exp(L), on the vertical axis, as a function of α, on the horizontal axis, then we
obtain the posterior pdf for the position of the lighthouse. Not only does this give us a
complete visual representation of our inference, it has the advantage that we don’t need
to worry whether the posterior pdf is asymmetric or multimodal.
Before illustrating the use of such an analysis, it is worth making a brief practical
point: it is better to calculate L ﬁrst, as a function of α (on a uniform grid), and then take
exponentials, rather than working directly with the posterior pdf itself. The reason is
purely numerical and stems from the limit on the size of the number which can be stored
in a computer; typically, anything smaller than 10−300 tends to be indistinguishable
from zero and numbers larger than 10+300 are effectively inﬁnite. The reader may be
forgiven for failing to see why this should be a problem, since 600 orders of magnitude
is an enormous range; after all, probabilities which are more than a factor of 1000 down
(10−3) on the maximum are almost imperceptible on a graph and of little real interest
anyway. This is certainly true, but it assumes that the maximum value of the posterior
probability has already been scaled to a sensible number. In practice, this is often not
the case as we usually ignore the normalization constant until the end of the calculation.
In terms of eqn (2.38), we tend to omit the constant term from the sum because it
affects neither the best estimate nor its error-bar; its value does, however, translate into
a multiplicative scale factor for the posterior pdf. A good procedure, therefore, is to
evaluate L for a series of different α’s, ﬁnd the maximum Lmax and subtract it from all
the L’s; taking the exponential of these numbers will give a sensible maximum value
of one. As well as being useful for plotting purposes, it then enables us to calculate the
normalization constant safely, if required, without fear of underﬂows or overﬂows in the
execution of the program.
Figure 2.9 shows how the posterior pdf for the position of the lighthouse evolves as
the number of ﬂashes detected on the coast increases. A random number generator was
used to create a uniform sample of azimuths {θk}, according to eqn (2.32), which were
converted into data positions {xk} with eqn (2.33); the lighthouse was taken to be 1 km
out at sea (β = 1), and assumed to be known. In the early panels, the positions of the
ﬂashes are marked by small open circles at the top of the graphs; the number of data
analysed is shown in the top right-hand corner. Not only is the posterior pdf very broad

Example 3: the lighthouse problem
33
Fig. 2.9 The posterior pdf for the position of the lighthouse along the coast, given that it is 1 km
out to sea. The number of data available is shown in the top right-hand corner of the graphs, and
their positions are marked by small open circles at the top of the ﬁrst four panels. The average
value of the data is indicated by a long vertical line.
when there are only a few data, it can easily be multimodal if their locations are suf-
ﬁciently discordant. By about a dozen measurements, however, the posterior becomes
a well behaved Gaussian-like pdf. As the experimental evidence mounts, the posterior
pdf becomes narrower, and narrower (inversely proportional to
√
N ), with the peak con-
verging towards a value of α = 1; this is as expected, and quite reassuring, as the data
were generated with the lighthouse 1 km along the shore.
2.4.1
The central limit theorem
Before leaving this problem, it is worth pointing out a curious feature about the mean
value of the data. In the previous section, we found that the best estimate of the pa-

34
Parameter estimation I
rameter µ of a normal distribution, as in eqn (2.23), was given by the simple arithmetic
average of samples drawn from it; stated formally in eqn (2.28), this result seems intu-
itively reasonable as µ is the mean of the Gaussian pdf. In the present case, the data are
drawn from a Cauchy pdf; since this too is symmetric about the point of interest α, we
might think that the average value of the data would also provide a good estimate of the
position of the lighthouse. We should be a little concerned by this proposal, however,
as the sample mean is not the solution to eqn (2.39) for the best estimate αo. Our suspi-
cions are conﬁrmed in Fig. 2.9, where the average value of the data is indicated by the
long vertical line. We see that the sample mean is not a good estimator for this problem,
since it often lies well outside the range allowed by the posterior pdf.
At ﬁrst sight, the observation above seems very peculiar; it appears to conﬂict with
our training if not experience. The everyday respectability afforded to averages hinges,
formally, on the central limit theorem: this states that if samples are drawn randomly
from (almost) any pdf with mean µ, then in the limit of many data N, their average will
tend to this value; the error-bar for the difference between µ and the sample mean will
also go down like
√
N (as in eqn 2.29). Technically, it can be shown that the Cauchy
distribution is an exception to this rule because it violates one of the underlying assump-
tions of the central limit theorem. This follows from the fact that means µ and variances
σ2 are additive. On dividing by N to recover averages, the error-bar on the mean dimin-
ishes like
√
N. But for this to hold, µ and σ have to exist. For a Cauchy distribution, its
very wide wings make σ2 inﬁnite and µ undeﬁned. In fact, the variability of the mean
of the data does not decrease with increasing number of measurements, and is likely to
be as ‘wrong’ after a thousand, or a million, data as it is after just one!
Although the central limit theorem is not applicable to the Cauchy likelihood, this
did not prevent us from being able to estimate the position of the lighthouse. To infer
the value of α, given β and the detection of ﬂashes at {xk}, we needed to consider the
posterior pdf prob(α|{xk}, β, I); as can be seen from Fig. 2.9, the latter continues to
get sharper as the number of data increases. The point is that the central limit theorem is
a statement about the pdf for the sample mean: prob(x|α, β, I), where x =  xk/N.
If the maximum of the posterior pdf happens to be equal to the average value of the data,
then this might be relevant; otherwise, it’s of little concern. The moral of this tale is that,
despite its everyday respectability, the sample mean is not always a useful number— let
the posterior pdf decide what is best.

3
Parameter estimation II
In the previous chapter, we were concerned with the most elementary type of data anal-
ysis problem: parameter estimation involving just one unknown variable. Now let us
progress to the more common case in which there are several parameters; some of these
will be of interest to us, but others merely a nuisance. As well as being a more challeng-
ing optimization problem, we will be led to generalize the idea of error-bars to include
correlations and to the use of marginalization to deal with the unwanted variables. We
will also see how certain approximations naturally give rise to some of the most fre-
quently used analysis procedures, and discuss the so-called propagation of errors.
3.1
Example 4: amplitude of a signal in the presence of
background
In many branches of science, we are often faced with the task of having to estimate the
amplitude of a signal in the presence of a background. For example, an X-ray diffrac-
tion pattern from a crystalline sample will contain both the distinctive Bragg peaks of
interest and a general contribution from diffuse scattering processes; in an astronomi-
cal setting, the emission spectrum of a galaxy may be contaminated by stray light from
the night sky. An idealized one-dimensional situation is illustrated in Fig. 3.1, where
the horizontal x-axis represents the measurement variable (such as the scattering an-
gle, wavelength, or whatever). In the simplest case, the background can be taken as
being ﬂat, and of unknown magnitude B, and the signal of interest assumed to be the
amplitude A of a peak of known shape and position. The data for these problems are
frequently integer-valued, and correspond to things like the number of photons detected
from a quasar at a certain wavelength or the number of neutrons scattered by a sample
in a given direction. Given such a set of counts {Nk}, measured at experimental settings
{xk}, what is our best estimate of the amplitude of the signal peak and the background?
Fig. 3.1 A signal peak of amplitude A sitting on a ﬂat background of magnitude B.

36
Parameter estimation II
Let us begin by thinking about the nature of the data a little more carefully. It seems
reasonable that we should expect the number of counts in the kth data-channel to be
proportional to the sum of the signal and the background at xk; taking the peak to be
Gaussian in shape, for the sake of argument, with a width of w and centred at xo, the
ideal datum Dk ought to be given by:
Dk = no

A e−(xk−xo)2/2w2 + B

,
(3.1)
where no is a constant related to the amount of time for which the measurements were
made. Unlike the number of counts Nk, however, the Dk in eqn (3.1) will not generally
be a whole number. Therefore, the actual datum will be an integer (⩾0) in the vicinity
of this expected value. A pdf which incorporates this property, and is usually invoked in
such counting experiments, is a Poisson distribution:
prob(N |D) = DN e−D
N!
,
(3.2)
and is illustrated in Fig. 3.2. We leave the derivation of this strange-looking formula to
Chapter 5 but note the result that its formal expectation value, as deﬁned by the discrete
version of eqn (2.22), is indeed equal to D:
⟨N⟩=
∞

N=0
N prob(N |D) = D .
(3.3)
Equation (3.2) is, of course, an assignment for the likelihood of the datum Nk:
prob(Nk|A, B, I) = DNk
k
e−Dk
Nk!
,
(3.4)
where the background information I includes a knowledge of the relationship between
the expected number of counts Dk and the parameters of interest A and B; for the case
of the Gaussian peak-shape model of eqn (3.1), this means that xo, w and no are taken
as given (as well as xk). If the data are independent, so that, when given the values of
Fig. 3.2 The Poisson distribution prob(N |D) for: (a) D=1.7, and (b) D=12.5.

Example 4: amplitude of a signal in the presence of background
37
A and B, the number of counts observed in one channel does not inﬂuence what we
would expect to ﬁnd in another, then the likelihood function is just the product of the
probabilities for the individual measurements:
prob({Nk}|A, B, I) =
M

k=1
prob(Nk|A, B, I) ,
(3.5)
where there are M data.
Our inference about the amplitude of the signal and the background is embodied
in the posterior pdf prob(A, B|{Nk}, I); as usual, we use Bayes’ theorem to help us
calculate it:
prob(A, B|{Nk}, I) ∝prob({Nk}|A, B, I) × prob(A, B|I) .
(3.6)
Having already dealt with the likelihood function above, all that remains is the assign-
ment of the prior pdf. Irrespective of the data, the one thing we do know is that the
amplitude of neither the signal nor the background can be negative; the most na¨ıve way
of encoding this is through a uniform pdf in the positive quadrant:
prob(A, B|I) =

constant
for A ⩾0 and B ⩾0 ,
0
otherwise .
(3.7)
Later, in Chapter 5, we will see that this simple ﬂat pdf is not necessarily the one that
represents the most prior ignorance for this problem, but it is quite adequate for our
present needs. Although we should formally have upper bounds Amax and Bmax, which
determine the normalization constant in eqn (3.7), we will just assume they are sufﬁ-
ciently large as not to impose a cut-off on the posterior pdf; that is to say, in eqn (3.6),
prob({Nk}|A, B, I) will have diminished (gradually) to a negligibly small value before
these limits are reached.
Multiplying the Poisson likelihood resulting from eqns (3.4) and (3.5) by the prior
of eqn (3.7), according to Bayes’ theorem of eqn (3.6), yields the posterior pdf; its
logarithm L is given by
L = loge

prob(A, B|{Nk}, I)

= constant +
M

k=1

Nk loge(Dk) −Dk

, (3.8)
where the constant includes all terms not involving A and B, and the latter are both
positive. Our best estimate of the amplitude of the signal and the background is given
by the values of A and B which maximize L; its reliability is indicated by the width, or
sharpness, of the posterior pdf about this optimal point. Let us illustrate the use of this
analysis with some computer examples.
Four sets of data and the posterior pdfs which result from them are shown in Fig. 3.3.
The data were generated with a Poisson random number generator, in accordance with
eqn (3.4), from the simple ‘Gaussian peak on ﬂat background model’ of eqn (3.1). They

38
Parameter estimation II
Fig. 3.3 Poisson data and the resulting posterior pdfs for the amplitude A of a Gaussian signal
peak, centred at the origin with a FWHM of 5 units, and the ﬂat background B, for four different
experimental set-ups.

Example 4: amplitude of a signal in the presence of background
39
are plotted as histograms, as experimental measurements would normally correspond to
the number of counts detected in ﬁnite-sized channels; for simplicity, we have chosen
the bin-width to be unity. In all cases, the underlying signal is centred at the origin, so
that xo = 0, and has a FWHM of 5 units; this is assumed to be known for the analysis.
The posterior pdf is now two-dimensional, since it is a function of both A and B. We
can display it using contours, which are lines joining points of equal probability density,
just like hills and valleys are represented on a topographic map. In Fig. 3.3, the contours
correspond to 10%, 30%, 50%, 70% and 90% of the maximum probability.
The ﬁrst panel shows the number of counts detected in 15 data-bins, where the
parameter no was chosen to give a maximum expectation of 100 counts; its value is
taken as given. The corresponding posterior pdf, resulting from the exponential of eqn
(3.8), is plotted in the second panel on the top; it indicates that our best estimate of the
amplitude of the signal is approximately equal to one, and is about half the magnitude of
the background. The second panel down shows the data for the same set-up, but where
the experiment was conducted for only one-tenth of the time; the number of counts is
down by a factor of ten, and the data appear more noisy. The corresponding posterior
pdf is about three times as broad as before, in both directions, and is in line with the
√
10 that we might have guessed, on the basis of our experience from the examples of
the previous chapter. The posterior is truncated (or suddenly equal to zero) for negative
values of A and reﬂects the importance of the prior when the data are of poor quality.
In the third panel down we return to the original count-rate, but have 31 data spread
over twice the measurement range. With the doubling of the number of data, we might
expect the reliability with which A and B can be estimated would improve by a factor of
√
2; although a little hard to tell from these diagrams, this only seems to be true for the
background. Some further thought reveals this to be reasonable: measurements far away
from the origin only tell us about the background, since they contain no information
about the signal peak. The last two panels illustrate the case when there are just 7 data
spread over half the original range in x; the posterior pdf is noticeably broadened, by
more than a simple factor of
√
2 , and is distinctly skewed. These features are indicative
of a strong correlation between our estimates of A and B: as the range of x over which
the data are collected is severely restricted, it becomes difﬁcult to tell the signal apart
from the background!
3.1.1
Marginal distributions
The two-dimensional posterior pdf above describes completely our joint inference about
the values of A and B. Often, however, we are not interested in the background. It has
to be included in our calculation, because it is needed for the likelihood function, but we
have no intrinsic interest in knowing its value: its presence is merely a nuisance. What
we would really like to estimate is just the amplitude of the signal, irrespective of the
size of the background;in other words, we require the pdf prob(A|{Nk}, I). According
to the marginalization rule of eqn (1.4), this can be obtained simply by integrating the
joint posterior pdf with respect to B:

40
Parameter estimation II
prob(A|{Nk}, I) =
∞

0
prob(A, B|{Nk}, I) dB .
(3.9)
There may, of course, be situations where the background is of primary interest and the
presence of a peak is a nuisance. This seemingly perverse case does sometimes occur,
because the phenomena that give rise to broad features in the data may be of as much
scientiﬁc importance as the ones resulting in a sharp signal. All we need to do then, to
obtain the pdf prob(B|{Nk}, I), is integrate the joint posterior pdf with respect to A:
prob(B|{Nk}, I) =
∞

0
prob(A, B|{Nk}, I) dA .
(3.10)
The four pairs of marginal distributions corresponding to the data-sets, and posterior
pdfs, of Fig. 3.3 are plotted in Fig. 3.4; it is now much easier to see the effects of the
different experimental set-ups on the reliability of the inferred values A and B, which
were mentioned a little earlier.
To avoid any confusion, we should emphasize that the marginal distribution for A,
prob(A|{Nk}, I), is not the same as the conditional pdf prob(A|{Nk}, B, I); although
both describe our inference about the amplitude of the signal peak, they correspond to
different circumstances. The former pdf takes into account our prior ignorance of the
value of B, while the latter is appropriate when the magnitude of the background has
already been determined reliably from a calibration experiment. This conditional pdf,
given that B = 2, is plotted as a dotted line in Fig. 3.4; compared with its marginal
counterpart, we see that there is a signiﬁcant narrowing of the posterior probability for
the last data-set, and least difference for the case when measurements have been made
far beyond the tails of the signal peak. When we are able to distinguish between the
background and the signal peak fairly well, there is little to be gained from a separate
calibration experiment for B; if the data-set is severely truncated, or the background is
highly structured, this additional information can be very beneﬁcial.
For the above analysis, we have assumed that the shape and position of the signal
peak is known; in the context of the Gaussian model of eqn (3.1), we took the val-
ues of w and xo as implicitly given in the background information I. How could this
condition be relaxed if we were not quite so fortunate? Well, from the discussion about
marginalization,it follows that we should integrate out the relevant variables as nuisance
parameters:
prob(A, B|{Nk}, I) =

prob(A, B, w, xo|{Nk}, I) dw dxo ,
(3.11)
where I still assumes that the model of eqn (3.1) is adequate, but does not necessarily
include a knowledge of the width and position of the Gaussian peak. The four-parameter
posterior pdf under the double integral can itself be related to two others which are easier
to assign by using Bayes’ theorem:
prob(A, B, w, xo|{Nk}, I) ∝prob({Nk}|A, B, w, xo, I) × prob(A, B, w, xo|I) .

Example 4: amplitude of a signal in the presence of background
41
Fig. 3.4 The marginal distributions for the amplitude A and the background B corresponding to
the Poisson data, and the resulting posterior pdfs, for the four experimental set-ups in Fig. 3.3.
The dotted line shows the posterior pdf for A conditional on knowing the true value of B.

42
Parameter estimation II
The ﬁrst term on the right-hand side is equivalent to the likelihood function of eqns (3.4)
and (3.5); the second is the prior pdf for A, B, w and xo, which can be decomposed as:
prob(A, B, w, xo|I) = prob(A, B|I) × prob(w, xo|I) .
If we already knew the width and location of the Gaussian peak, then the prior pdf for
w and xo would be very sharp. In the limit of absolute certainty, it would become
prob(w, xo|I) = δ(w−2.12) δ(xo) ,
where the two δ-functions are equal to zero unless w=2.12 (i.e. FWHM=5) and xo=0
respectively; in this case, the integral of eqn (3.11) is very easy to evaluate and yields
prob(A, B|{Nk}, I) ∝prob({Nk}|A, B, w=2.12, xo=0, I) × prob(A, B|I) .
Thus, as expected, the analysis reduces to that of eqn (3.6). If we do not know the
values of w and xo, however, we must assign a suitably broad prior for these parameters
(as well as for A and B). The marginalization integral will then require more work,
but can either be computed numerically or approximated analytically, as we shall see
in Section 3.2. Although probability theory allows us to deal with such experimental
uncertainties, we do, of course, expect the marginal distribution prob(A, B|{Nk}, I)
to be broader than the conditional pdf prob(A, B|{Nk}, w, xo, I); this would merely
reﬂect the corresponding lack of relevant information.
3.1.2
Binning the data
When the data were plotted in Fig. 3.3 using histograms, we said that this was because
experimental measurements often corresponded to the number of counts detected in
channels of ﬁnite width. This means that eqn (3.1), for the expected value of the datum
Dk, should actually be written as an integral over the kth data-bin:
Dk =
xk+∆/2

xk−∆/2
no

A e−(x−xo)2/2w2 + B

dx ,
(3.12)
where we have taken all the measurement channels to have the same width ∆. As long
as the bin-width is not too large, so that the integrand varies only linearly across it, the
integral of eqn (3.12) can be approximated well by the area of a trapezoidal column:
Dk ≈no

A e−(xk−xo)2/2w2 + B

∆.
(3.13)
Thus, eqn (3.1) is justiﬁed because the ﬁxed number ∆can be absorbed into no. This
new (redeﬁned) constant reﬂects both the amount of time for which the experimental
measurements were made and the size of the ‘collecting area’. The bin-width ∆is not
always determined by the physical size of the detectors, however, but is often chosen
to be large enough so that there are a reasonable number of counts in the resulting
composite data-channels. Is there anything to be gained, or lost, in this binning process?

Reliabilities: best estimates, correlations and error-bars
43
Fig. 3.5 The analysis of data corresponding to the experimental set-up of the ﬁrst panel in Fig.
3.3, but with bins which are four times narrower.
Figure 3.5 shows the data for the same experimental set-up as in the ﬁrst panel of
Fig. 3.3, but where the bins are four times narrower than before; this data-set looks
noisier because, on average, each channel has only a quarter of the previous number of
counts. Figure 3.5 also shows the resulting posterior pdf for A and B, and its marginal
distributions, given that w = 2.12 and xo = 0. Compared with the corresponding pdfs
in Figs 3.3 and 3.4, the reliability of the inferred parameters is virtually identical. This
illustrates that the perceived improvement in the data quality afforded by the binning
procedure is largely cosmetic. While this visual gain is not without value, and there
are computational advantages in dealing with fewer ‘measurements’, we must always
be aware that too coarse a binning can destroy useful information in the data. To take
an extreme case, if all the counts were added together into a single number, we would
completely lose all our ability to distinguish the signal from the background! At a more
mundane level, if the data-channels are too wide, we could also get into trouble with
the integral approximation of eqn (3.13). Some of the issues related to optimal binning
strategies are discussed further in Chapter 7.
3.2
Reliabilities: best estimates, correlations and error-bars
We have now seen an example of an estimation problem involving more than one param-
eter. Although the posterior pdf is of a higher dimensionality, being a function of several
variables, it still encodes our inference about the values of the parameters, given the
data and the relevant background information. As before, we often wish to summarize

44
Parameter estimation II
it with just a few numbers: the best estimates and a measure of their reliabilities. Since
the probability (density) associated with any particular set of values for the parameters
is a measure of how much we believe that they (really) lie in the neighbourhoodof those
values, our optimal estimate is given by the maximum of the posterior pdf. If we denote
the quantities of interest by {Xj}, with a posterior pdf P = prob({Xj}|{data}, I),
then the best estimate of their values, {Xoj}, is given by the solution to the set of si-
multaneous equations:
∂P
∂Xi

{Xoj}
= 0 ,
(3.14)
where i = 1, 2, . . . , up to the number of parameters to be inferred. Strictly speaking,
we also need a further test, analogous to eqn (2.9), to ensure that we are dealing with a
maximum and not a minimum or a saddle-point; we’ll say more about that shortly. In
writing the partial derivatives of P with respect to the Xi’s we are, of course, assuming
that they can all lie in a continuous range. If some could only take discrete values, our
best estimate would still correspond to the one with greatest posterior probability but
we couldn’t then use the calculus notation of eqn (3.14).
As in Section 2.2, it is more convenient to work with the logarithm of P rather than
use the posterior pdf itself:
L = log e

prob({Xj}|{data}, I)

.
(3.15)
Since the logarithm is a monotonic function, the maximum of L occurs at the same
place as that of P ; hence, eqn (3.14) can be written with L substituted for P. Rather
than pursuing this analysis in generality, with several parameters, let us ﬁrst consider the
speciﬁc case of just two variables; we will denote them by X and Y , instead of X1 and
X2, to reduce the multiplicity of subscripts. The pair of simultaneous equations which
we must now solve, to obtain our best estimates Xo and Yo, are given by
∂L
∂X

Xo,Yo
= 0
and
∂L
∂Y

Xo,Yo
= 0 ,
(3.16)
where L = log e

prob(X, Y |{data}, I)

.
To obtain a measure of the reliability of this best estimate, we need to look at the
spread of the two-dimensional posterior pdf about the point (Xo, Yo). As in Section 2.2,
we can analyse the local behaviour of a (potentially) complicated function by using a
Taylor series expansion:
L = L(Xo, Yo) + 1
2
	
∂2L
∂X2

Xo,Yo
(X−Xo)2 + ∂2L
∂Y 2

Xo,Yo
(Y −Yo)2
+ 2
∂2L
∂X ∂Y

Xo,Yo
(X−Xo) (Y −Yo)

+ · · · ,
(3.17)
where Xo and Yo are given by the condition of eqn (3.16). Although this expression
looks quite horrendous, it is simply the two-dimensional version of eqn (2.11); it’s just

Reliabilities: best estimates, correlations and error-bars
45
that there are now four second (partial) derivatives to deal with, instead of only one!
We have, in fact, reduced this tally of terms to three by using the equality of the mixed
derivatives: ∂2L/∂X ∂Y = ∂2L/∂Y ∂X. The ﬁrst term in the Taylor series, L(Xo, Yo),
is a constant and tells us nothing about the shape of the posterior pdf. The two linear
terms, which would be proportional to X−Xo and Y −Yo, are missing because we are
expanding about the maximum (as indicated by eqn 3.16). The three quadratic terms
are, therefore, the dominant factors determining the width of the posterior pdf, and play
a central rˆole in the reliability analysis; let us study them more closely.
To aid the generalization of this discussion to the case of several variables a lit-
tle later, let us rewrite the quadratic part of eqn (3.17) in matrix notation; calling the
quantity in the square brackets Q, we have
Q =

X−Xo
Y −Yo

A
C
C
B

X−Xo
Y −Yo

,
(3.18)
where the components of the 2×2 symmetric matrix in the middle are given by the
second derivatives of L, evaluated at the maximum (Xo, Yo):
A = ∂2L
∂X2

Xo,Yo
,
B = ∂2L
∂Y 2

Xo,Yo
,
C =
∂2L
∂X ∂Y

Xo,Yo
.
(3.19)
Figure 3.6 shows a contour of Q in the X–Y plane; within our quadratic approximation,
it is also a line along which the posterior pdf is constant. It is an ellipse, centred at
(Xo , Yo), the orientation and eccentricity of which are determined by the values of A,
B and C; for a given contour-level (Q = k), they also govern its size. The directions
of the principal axes formally correspond to the eigenvectors of the second-derivative
Fig. 3.6 The contour in the X–Y parameter space along which Q = k, a constant. It is an el-
lipse, centred at (Xo , Yo), the characteristics of which are determined by the eigenvalues λ and
eigenvectors e of the second-derivative matrix deﬁned in eqns (3.18)–(3.20).

46
Parameter estimation II
matrix of eqns (3.18) and (3.19); that is to say, the (x, y) components of e1 and e2 in
Fig. 3.6 are given by the solutions of the equation

A
C
C
B

x
y

= λ

x
y

.
(3.20)
The two eigenvalues λ1 and λ2 which satisfy eqn (3.20) are, in turn, inversely related to
the square of the widths of the ellipse along its principal directions. Furthermore, if the
point (Xo , Yo) in eqn (3.16) is to be a maximum rather than a minimum or a saddle-
point, both λ1 and λ2 must be negative; in terms of A, B and C, this requirement takes
the form
A < 0 ,
B < 0 ,
AB > C2.
Returning to the question of the reliability of our best estimates, this would be easy to
deﬁne if the ellipse in Fig. 3.6 was aligned with the X and Y coordinate axes; this is so
when C =0. In that case, the error-bars for Xo and Yo will just be inversely proportional
to the square root of (the modulus of) the appropriate eigenvalues (which are then simply
A and B). What should we do if the ellipse is skewed?
If we were only interested in knowing the value of X, then we would integrate out
Y as a nuisance parameter:
prob(X|{data}, I) =
+∞

−∞
prob(X, Y |{data}, I) dY .
This integral can be done analytically within the quadratic approximationof eqns (3.17)–
(3.19): prob(X, Y |{data}, I) = exp(L) ∝exp(Q/2). Assuming that the joint poste-
rior pdf is not (signiﬁcantly) truncated by the bounds of the prior, Appendix A shows
the result to be
prob(X|{data}, I) ∝exp
 1
2
AB−C2
B

(X−Xo)2

,
where we have omitted the normalization constant. Comparing this to eqns (2.13) and
(2.14), we see that the marginal distribution for X is just a simple one-dimensional
Gaussian pdf; our best estimate is still Xo, and its associated error-bar σX is given by
σX =

−B
AB−C2 .
(3.21)
We can obtain an analogous result for Y , yielding a marginal error-bar σY:
σY =

−A
AB−C2 .
(3.22)
While the expressions for σX and σY above provide a useful measure of the reliability
of our best estimates, in general, they paint an incomplete picture. To understand this,

Reliabilities: best estimates, correlations and error-bars
47
we must consider the denominator of eqns (3.21) and (3.22) in a bit more detail. The
term AB −C2 is, in fact, the determinant of the matrix in eqn (3.20); for such a real
symmetric matrix, this is equal to the product of its eigenvalues. Thus if either λ1 or
λ2 becomes very small, so that the ellipse in Fig. 3.6 is extremely elongated in one of
its principal directions, then AB−C2 →0; consequently, apart from the special case
when C =0, both σX and σY will be huge. Although this correctly warns us that neither
X nor Y can be inferred reliably on the basis of the current data, it fails to tell us that
there could still be some joint aspect of the two which can be determined well, because
the posterior pdf might be very sharp in one direction while being extremely broad in
the other. To see how this information can be conveyed, let us ﬁrst look at the concept
of error-bars again in a slightly different way.
So far, from eqn (2.14), we have thought of the error-bar as representing the width
of a Gaussian pdf: FWHM ≈2.35σ. Another way to think about it is in terms of the
variance of the posterior, which also gives a measure of its spread. It is formally deﬁned
to be the expectation value of the square of the deviations from the mean; assuming a
normalized pdf, this is given by
Var(X) =

(X−µ)2
=

(X−µ)2 prob(X|{data}, I) dX ,
(3.23)
where µ is the average value ⟨X⟩, as in eqn (2.22). For the one-dimensional normal
distribution of eqn (2.14), this integral yields the result

(X−µ)2
= σ2 .
(3.24)
The square root of the variance is called the standard deviation, or the root-mean-square
(r.m.s.) error, of X. This deﬁnition of the error-bar can be extended to pdfs of more than
one variable. Explicitly, for the two-dimensional case we have been considering,
σX2 =

(X−Xo)2
=

(X−Xo)2 prob(X, Y |{data}, I) dX dY ,
(3.25)
where σX is the same as in eqn (3.21) when the double integral is evaluated within the
quadratic approximation of eqns (3.17)–(3.19). The corresponding expression for σY is
similar, with the X’s and Y ’s swapped around.
The idea of variance can be broadened to consider the simultaneous deviations of
both X and Y ; this covariance, which we will denote as σXY
2 , is given by
σXY
2
=

(X−Xo)(Y −Yo)

=

(X−Xo)(Y −Yo) prob(X, Y |{data}, I) dX dY ,
(3.26)
and is a measure of the correlation between the inferred parameters. If an over-estimate
of one usually leads to a larger than average value for the other, then the difference

48
Parameter estimation II
Y −Yo will tend to be positive when X−Xo is positive; if the same is true for under-
estimates, so that Y −Yo is usually negative when X −Xo is as well, the expectation
value of the product of the deviations will be positive: the covariance will then be greater
than zero. If there is an anti-correlation, so that an over-estimate of one is accompanied
by an under-estimation of the other, then the covariance will be negative. When our
estimate of one parameter has little, or no, inﬂuence on the inferred value of the other,
then the magnitude of the covariance will be negligible in comparison to the variance
terms; in other words, |σXY
2 |≪
√
σX2 σY2 .
When the double integral of eqn (3.26) is evaluated within the quadratic approxima-
tion of eqns (3.17)–(3.19), it yields the result
σXY
2
=
C
AB−C2 .
(3.27)
In conjunction with eqns (3.21) and (3.22), therefore, we see that both the variance
and covariance terms are given by (minus) the elements of the inverse of the second-
derivative matrix of eqn (3.20):

σX2
σXY
2
σXY
2
σY2

=
1
AB−C2

−B
C
C
−A

= −

A
C
C
B
−1
.
(3.28)
This table of ‘error-bar products’ is called the covariance matrix. When C = 0, σXY
2
also equals zero and means that the inferred values of the parameters are uncorrelated.
In that case, the principal directions of the corresponding posterior pdf will be parallel
to the coordinates axes; this situation is illustrated in Fig. 3.7(a). As the magnitude of
C increases, relative to A and B, the posterior pdf becomes more and more skew and
elongated; this reﬂects the growing strength of the correlation between our estimates
Fig. 3.7 A schematic illustration of covariance and correlation. (a) The contours of a posterior
pdf with zero covariance, where the inferred values of X and Y are uncorrelated. (b) The corre-
sponding plot when the covariance is large and negative; Y + mX = constant along the dotted
line (where m>0), emphasizing that only this sum of the two parameters can be inferred reliably.
(c) The case of positive correlation, where we learn most about the difference Y −mX; this is
constant along the dotted line.

Reliabilities: best estimates, correlations and error-bars
49
of X and Y , and is shown schematically in Figs 3.7(b) and (c). In the extreme case,
when C =±
√
AB, the elliptical contours will be inﬁnitely wide in one direction (with
only the information in the prior preventing this catastrophe!) and oriented at an angle
± tan−1
A/B with respect to the X-axis. Although the error-bars σX and σY will be
huge, saying that our individual estimates of X and Y are completely unreliable, the
large off-diagonal elements of the covariance matrix tells us that we can still infer a
linear combination of these parameters quite well. If the covariance is negative, then
the posterior pdf will be very broad in the direction Y = −mX, where m =

A/B,
but fairly narrow perpendicular to it; this bunching of the probability contours along
the lines of ‘Y + mX = a constant’ indicates that the data contain a lot of information
about the sum Y + mX, but little about the difference Y −X/m. Similarly, when the
covariance is positive, we can infer the difference Y −mX but not the sum Y +X/m.
3.2.1
Generalization of the quadratic approximation
Let us generalize the bivariate analysis of the last few pages to the case when there are
several variables, and use the opportunity to summarize the main results. The condition
for obtaining our best estimate was given in eqn (3.14), and was chosen to be the one
which maximized the posterior pdf for the set of M parameters {Xj}. In terms of its
logarithm L, of eqn (3.15), this requirement for the optimal values {Xoj}, or Xo for
simplicity, can be expressed by the simultaneous equations
∂L
∂Xi

Xo
= 0 ,
(3.29)
where i = 1, 2, . . ., M. For the multivariate case, the extension of the Taylor series
expansion of eqn (3.17) takes the form
L = L(Xo) + 1
2
M

i=1
M

j=1
∂2L
∂Xi ∂Xj

Xo
(Xi−Xoi)

Xj −Xoj

+ · · · .
(3.30)
If the elements of a column matrix (M ×1) are regarded as the components of a vector,
and we ignore higher-order terms in the Taylor series expansion, then the exponential
of eqn (3.30) yields the following approximation for the posterior pdf:
prob(X|{data}, I) ∝exp
 1
2

X−Xo
T ∇∇L(Xo)

X−Xo

,
(3.31)
where ∇∇L is the symmetric M×M matrix of second-derivatives, whose ijth element
is ∂2L/∂Xi ∂Xj, and the transpose T of X−Xo is a row vector. As such, the quadratic
exponent is just a generalization of Q in eqn (3.18) and we can think of Fig. 3.6 as
being a two-dimensional slice through it. The pdf of eqn (3.31) is called a multivariate
Gaussian, since it is a multidimensional version of eqns (2.13) and (2.14). Assuming
that the bounds of the prior do not cause a signiﬁcant truncation of this M-dimensional
probability ‘bump’, Appendix A shows the associated constant of proportionality to be
given by (2π)−M/2 times the square root of the determinant of the ∇∇L matrix; this

50
Parameter estimation II
Fig. 3.8 An illustration of the difference between the (correct) marginal error-bar σii and its
(misleading) ‘best-ﬁt’ counterpart indicated by the bold line in the middle. The former is given
by the ith diagonal element of (∇∇L)−1, while the latter is related to the inverse of ∂2L/∂Xi2.
ensures the correct normalization, so that the multidimensional integral of the posterior
pdf with respect to all the parameters {Xj} is equal to unity.
It will come as no surprise that the maximum of the multivariate Gaussian is de-
ﬁned by the vector Xo; the condition for ﬁnding its components, in eqn (3.29), can be
written compactly as: ∇L(Xo)=0. By comparison with the standard one-dimensional
Gaussian of eqn (2.14), we see that ∇∇L is analogous to −1/σ2; this suggests that
the spread (or ‘width’) of the posterior should be related to the inverse of the second-
derivative matrix. Indeed, as shown in Appendix A, the covariance matrix σ2 is given
by minus the inverse of ∇∇L (evaluated at Xo):

σ2 
ij =

Xi−Xoi

Xj −Xoj

= −

∇∇L
−1 
ij ,
(3.32)
and is a generalization of eqns (3.26) and (3.28). The square root of the diagonal el-
ements (i = j) corresponds to the (marginal) error-bars for the associated parameters;
the off-diagonal components (i ̸= j) tell us about the correlations between the inferred
values of Xi and Xj.
Before concluding our discussion of the quadratic approximation, it is worth empha-
sizing the fact that the inverse of the diagonal elements of a matrix are not, in general,
equal to the diagonal elements of its inverse. Stated in this direct way, few of us would
expect to make the mistake. We would be pursuing this folly inadvertently, however, if
we were foolishly tempted to estimate the reliability of one parameter in a multivariate
problem by holding all the others ﬁxed at their optimal values. The situation is illus-
trated schematically in Fig. 3.8 and shows that the estimate of the error-bars can be
misleadingly small if we try to avoid the marginalization procedure.
3.2.2
Asymmetric and multimodal posterior pdfs
The above analysis, leading to the approximation of the posterior pdf by a multivariate
Gaussian, relies on the validity of the quadratic expansion of eqn (3.30). The elliptical

Reliabilities: best estimates, correlations and error-bars
51
Fig. 3.9 A schematic illustration of two posterior pdfs for which the multivariate Gaussian is not
a good approximation: (a) a ‘lop-sided’ pdf, analogous to the asymmetric case of Section 2.2;
(b) a multimodal pdf.
contours in Fig. 3.3 indicate that this is often a reasonable assumption, although we
must be careful about their potentially truncated nature when there are few data. There
are situations, however, when the posterior can have a very lop-sided shape; such a pdf
is sketched in Fig. 3.9(a). The word ‘asymmetric’ may not be appropriate here, because
there might still be a high degree of symmetry involved, but our difﬁculty is akin to
the case discussed earlier in Section 2.2: namely, the concepts of the error-bar and lin-
ear correlation associated with the covariance matrix do not seem suitable; at the very
least, the correspondence between the integral deﬁnition of covariance and the inverse
of ∇∇L will be doubtful. We might think of generalizing the idea of conﬁdence inter-
vals to a multidimensional space, but it will usually be hard to describe the surface of
the (smallest) hypervolume containing 90% of the probability (say) in just a few num-
bers. The prescriptions of Section 2.2 can, of course, be used for the one-dimensional
marginal distributions prob(Xj |{data}, I) for each of the parameters; unfortunately,
the relevant multiple integrals over the joint pdf will often be difﬁcult to carry out (an-
alytically), and only tell part of the story. Without the convenient properties of the mul-
tivariate Gaussian, the maximum of the marginal pdf for Xj may not be the same as the
jth component of the vector Xo either; thus we will have to think carefully whether
we are interested in the best estimate of all the parameters simultaneously, or just the
optimal value of one irrespective of the others.
The situation may be even worse, because the posterior might have many maxima;
such a pdf is shown schematically in Fig. 3.9(b). For some types of problems, this
multimodal nature can persist no matter how good the quality of the data. There is no
difﬁculty in deﬁning the best estimate, and expressing its reliability, when one of the
probability bumps is much bigger than the others: we simply make a (local) quadratic
expansion about the global maximum, and ignore the subsidiary solutions. If several of
the maxima are of comparable magnitude, then we could carry out this procedure for
all of them and provide a list of the alternative (almost) optimal values and their associ-
ated covariance matrices; if the number of signiﬁcant solutions is very large, however,
then we could soon be in trouble. Actually, we tend to have serious difﬁculties with

52
Parameter estimation II
multimodal pdfs even when there is an obvious single best estimate: the problem is the
practical one of how (to write a computer program) to ﬁnd the global solution in a large
multidimensional parameter space when there are many (small) local ‘hillocks’ to trip
us up along the way. We will return to this topic in Section 3.4, when discussing some
aspects about the computer implementation of our Bayesian analyses. First, though, let
us brieﬂy take another look at the Gaussian noise problem in the light of the multivariate
character of this chapter.
3.3
Example 5: Gaussian noise revisited
In Section 2.3, we were concerned with estimating the mean µ of a Gaussian process.
Since that chapter was restricted to a consideration of one-parameter examples, we as-
sumed that the magnitude of the error-bar σ for the experimental measurements {xk}
was already known. Now we are in a position to be able to relax that condition. What we
require is the pdf prob(µ|{xk}, I), rather than the corresponding posterior conditional
on being given the variance prob(µ|{xk}, σ, I). As a Gaussian process is deﬁned by
both µ and σ, we must integrate out the standard deviation as a nuisance parameter from
their joint posterior pdf:
prob(µ|{xk}, I) =
∞

0
prob(µ, σ|{xk}, I) dσ,
(3.33)
where the integrand can be expressed as a product of the likelihood function and the
prior, for the two parameters, by using Bayes’ theorem:
prob(µ, σ|{xk}, I) ∝prob({xk}|µ, σ, I) × prob(µ, σ|I) .
(3.34)
As in eqn (2.25), if the data are independent, the likelihood function is simply the prod-
uct of the probabilities for obtaining the individual measurements; using the deﬁnition
of a Gaussian distribution stated in eqn (2.23), we obtain
prob({xk}|µ, σ, I) =

σ
√
2π
−N
exp

−
1
2 σ2
N

k=1
(xk−µ)2

,
(3.35)
where there are N data.
What about the pdf prob(µ, σ|I)? To express complete prior ignorance about these
parameters we should assign a pdf that is uniform with respect to µ and log σ ; the
reason for this peculiar choice will become apparent in Chapter 5, but stems from the
fact that the position µ is associated with an additive uncertainty whereas the width σ
is a multiplicative scale-factor. For the moment, however, let us pursue our policy of
na¨ıvet´e and use a simple ﬂat prior:
prob(µ, σ|I) =

constant
for σ > 0 ,
0
otherwise.
(3.36)
We will indicate how the results would be different if we had made the preferred assign-
ment mentioned above, but our conclusions will be unaffected for all practical purposes.

Example 5: Gaussian noise revisited
53
Multiplying the prior of eqn (3.36) by the likelihood function of eqn (3.35), as in
eqn (3.34), allows us to write the marginal distribution of eqn (3.33) as
prob(µ|{xk}, I) ∝
∞

0
tN−2 exp

−t2
2
N

k=1
(xk−µ)2

dt ,
(3.37)
where we have made the substitution σ = 1/t (so that dσ = −dt/t2). A rescaling of t
through the substitution τ = t

Σ (xk−µ)2 then reduces eqn (3.37) to
prob(µ|{xk}, I) ∝

N

k=1
(xk−µ)2
−(N−1)/2
,
(3.38)
because a deﬁnite integral involving only τ can be absorbed into the proportionality.The
analysis of Section 2.2 shows that we can obtain our best estimate µo and a measure of
its reliability from the ﬁrst and second derivatives of the logarithm of this (marginal)
posterior pdf. Accordingly,
dL
dµ

µo
= (N −1)  (xk−µo)
 (xk−µo)2
= 0 ,
where L = log e[prob(µ|{xk}, I)] and the summations are from k = 1 to N; this can
only be satisﬁed if the numerator is equal to zero, and yields the result of eqn (2.28):
µo = 1
N
N

k=1
xk .
(3.39)
In other words, the optimal estimate of µ is still given by the arithmetic average of the
data. Differentiating L for a second time, and evaluating it at the maximum, we have
d2L
dµ2

µo
= −
N (N −1)
 (xk−µo)2 .
Since eqn (2.15) tells us that the error-bar for the best estimate is given by the inverse
of the square root of (minus) the second derivative, we can summarize our inference of
the mean by
µ = µo ±
S
√
N
,
where S2 =
1
N −1
N

k=1
(xk−µ)2 .
(3.40)
Comparing this to eqn (2.29), we see that the only difference with the case of Section
2.3 is that the formerly (assumed) known value of σ has been replaced by an estimate
derived from the data.

54
Parameter estimation II
3.3.1
The Student-t and χ2 distributions
In Section 2.3, we noted that the posterior pdf prob(µ|{xk}, σ, I) was deﬁned com-
pletely by the best estimate and its associated error-bar; this was because the quadratic
termination of the Taylor series in eqn (2.11) was exact for that case. When σ is not
known beforehand, this is no longer true: the summary of eqn (3.40) just represents
a useful approximation to the (marginal) posterior prob(µ|{xk}, I). The actual pdf is
given in eqn (3.38); its shape is easier to ascertain if we rewrite the sum (xk−µ)2 in
terms of two parameters, x and V , derived from the data:
N

k=1
(xk−µ)2 = N ( x −µ)2 + V ,
(3.41)
where x is equal to the sample-mean of eqn (3.39) and V is given by
V =
N

k=1
(xk−x )2 .
(3.42)
Substituting for the sum from eqn (3.41) into eqn (3.38) yields
prob(µ|{xk}, I) ∝

N ( x −µ)2 + V
−(N−1)/2
,
(3.43)
and is called a Student-t distribution. When N = 3, this pdf has the same form as the
Cauchy distribution of eqn (2.34) which was plotted in Fig. 2.8. It has a maximum at
µ = x, a FWHM which is proportional to
√
V and very long tails. As the number of
data increases, this type of function is multiplied by itself many times over; as a result,
the wide wings of the pdf are soon killed off to leave a more Gaussian-like distribution
centred about x. Hence, the optimal value µo is always equal to the sample-average;
its error-bar, which is related to V through eqn (3.40), becomes more meaningful as N
gets larger (⩾10).
In our short discussion about the prior earlier, we promised to indicate how the
results would differ if we had assigned a pdf which was uniform with respect to log σ.
We would have been led to the posterior pdf
prob(µ|{xk}, I) ∝

N ( x −µ)2 + V
−N/2
,
(3.44)
which is identical to eqn (3.43) except that the power of the N/2-term has increased
by one; that is, we would have obtained a Student-t distribution with N −1 degrees-
of-freedom, instead of N −2. Their shapes are very similar, with a maximum at x, but
the pdf of eqn (3.44) is a little narrower than that of eqn (3.43). In terms of eqn (3.40),
S2 would be given by V/N rather than V/(N −1) ; this difference is negligible if N
is moderately large, which is also the r´egime when the concept of an error-bar is most
appropriate. Thus the simple-minded assignment of eqn (3.36) gives us a slightly more
conservative estimate of the reliability of the optimal value µo, when there are few data,
but our conclusions remain essentially unchanged.

Algorithms: a numerical interlude
55
Before leaving this section, let us consider what we can learn from the data about
the magnitude of the expected error in the measurements. Our inference about σ is
described by the (marginal) posterior pdf prob(σ|{data}, I):
prob(σ|{xk}, I) =
+∞

−∞
prob(µ, σ|{xk}, I) dµ .
(3.45)
Using the likelihood function and prior of eqns (3.35) and (3.36), along with the substi-
tution of eqn (3.41), Bayes’ theorem allows to write this as
prob(σ|{xk}, I) ∝σ−N exp

−V
2 σ2

+∞

−∞
exp

−N ( x −µ)2
2 σ2

dµ ,
(3.46)
where we have taken all the terms not involving µ outside the integral, and σ > 0.
Since the area under the one-dimensional normal distribution on the right-hand side is
proportional to σ, we obtain the result
prob(σ|{xk}, I) ∝σ1−N exp

−V
2 σ2

.
(3.47)
In the formal language of statistics, this is related to the χ2 distribution (through the
substitution X = V/σ2). As usual, according to the analysis of Section 2.2, the best
estimate and its error-bar can be derived from the ﬁrst and second derivatives of the
logarithm of eqn (3.47). This allows us to summarize our inference about σ by
σ = σo ±
σo

2(N −1)
,
(3.48)
where the optimal value is σo =

V/(N −1). Although eqn (3.48) permits negative
values of σ, this simply indicates that the quadratic expansion of eqn (2.11) gives a
poor approximation to the asymmetric posterior pdf when the number of data is small;
it would then be better to give an estimate and error-bar for log σ instead, which we will
learn how to do in Section 3.6.
3.4
Algorithms: a numerical interlude
We have now seen several examples of how the rules of probability theory can be used
to obtain the posterior pdf for the quantities of interest; in order to summarize the infer-
ence, we usually need to ﬁnd its maximum. Sometimes this optimization can be done
analytically, but often we are forced to turn to the computer. The virtue of this electronic
machine lies, of course, in its ability to do numerical calculations (and graphical plot-
ting) considerably more quickly, and painlessly, than we could do ourselves by hand.
Since computer programming and optimization are large subjects in their own right,
our aim here is merely to give a ﬂavour of the range of problems that are likely to

56
Parameter estimation II
be encountered and to give an indication of the sort of procedures that might be used
to tackle them. A more advanced account of a novel Monte Carlo method for doing
Bayesian computation is given in Chapters 9 and 10.
We should point out that the material in this section is to do with algorithms, rather
than the fundamentals of data analysis. That is to say, the techniques employed to ﬁnd
(as opposed to deﬁne) the optimal solution are based purely on practical considerations;
the fact that we are using them to solve probabilistic problems is essentially coinciden-
tal. Details of the procedures mentioned below can be found in numerous good books;
of particular note is Numerical Recipes, by Press et al. (1986), which includes source-
code for computer programs (in FORTRAN, C and Pascal). Practical Optimization, by
Gill et al. (1981), is also excellent, as is the old gem by Acton (1970) entitled Numerical
Methods That (usually) Work.
3.4.1
Brute force and ignorance
For a one-parameter problem, the most robust way of ﬁnding the maximum of the pos-
terior pdf is to plot it out and have a look! On a computer, this brute force and ignorance
method entails the division of the horizontal axis into a ﬁnite number of grid points,
representing the possible values of the parameter, and the evaluation of the posterior
probability at each of them. Plotting the latter along the vertical axis, we simply ﬁnd the
biggest value and read off the corresponding optimal estimate of the quantity of interest.
As mentioned in Section 2.4, it is numerically better to calculate the logarithm of the
posterior probability for all the grid points ﬁrst; and then subtract the largest number
from each and exponentiate. If normalization is required, this pdf can be multiplied by
a constant so that the area under the curve is equal to unity.
The greatest advantage of this elementary algorithm is that it gives a complete pic-
ture of our inference about the value of the parameter. As long as there are enough
grid points (a few hundred usually being quite adequate), which cover a sufﬁciently
large range, this procedure will always work. It doesn’t matter whether the posterior
pdf is asymmetric, multimodal or whatever; even the question of differentiability is of
no concern. This brute force method generalizes readily to two-parameter problems: we
just plot the posterior probability vertically against a two-dimensional grid of points,
often with contours. This is what was done in Fig. 3.3. As well as all the advantages
mentioned above, it is very easy to obtain the marginal distributions: simply add up the
probabilities in the ‘X’ or ‘Y ’ directions, as appropriate. Even sharp cut-offs imposed
by the prior pdf cause no difﬁculty for this numerical integration; this can be seen from
the low-count example in Figs 3.3 and 3.4.
Unfortunately,this direct approach, of evaluating the probability everywhere,rapidly
becomes impractical after the two-parameter case. Apart from the problem of how to
display a function of several variables on a piece of paper, the computational cost of
calculating the probabilities is soon prohibitive. If each axis was divided into just 10 dis-
crete points, an M-parameter problem would then require 10M evaluations. While the
calculations for two variables might only take a second, a whole day will be required for
seven and the age of the universe for twenty! Super-computers, and ridiculously coarse
grids, can buy a little more leeway, but not much. For multivariate analysis, we must

Algorithms: a numerical interlude
57
look for more efﬁcient algorithms.
3.4.2
The joys of linearity
In Section 3.2, we represented a set of M parameters {Xj} by the components of a
(column) vector X; the condition for our best estimate of their values, denoted by Xo,
can be written as
∇L(Xo) = 0 ,
(3.49)
where the jth element of ∇L is given by the partial derivative of the logarithm of the
posterior pdf ∂L/∂Xj (evaluated at X =Xo). Equation (3.49) is very compact notation
for a set of M simultaneous equations; they will be quite difﬁcult to solve, in general,
unless they are linear. That is to say, if we are fortunate enough to be able to rearrange
∇L into a form resembling a straight line (‘y = mx + c’),
∇L = HX + C ,
(3.50)
where the components of the vector C, and the square matrix H, are all constant, then
the solution to eqn (3.49) is trivial:
Xo = −H−1C .
(3.51)
Differentiating eqn (3.50), we ﬁnd that the important ∇∇L matrix is invariant with
respect to the parameters {Xj}:
∇∇L = H ,
(3.52)
and so all the higher derivatives are identically zero. The covariance matrix σ2, given
by minus the inverse of ∇∇L, therefore provides a complete description of the shape
of the posterior pdf:

σ2 
ij =

Xi−Xoi

Xj −Xoj

= −

H−1 
ij ,
where the {Xoj} are the components of the vector Xo in eqn (3.51).
Although it is easy to write down the optimal solution in the form of eqn (3.51),
actually evaluating it can still entail a fair amount of computational effort. Luckily,
however, there are plenty of good procedures, or subroutines, available for matrix alge-
bra; algorithmic details can be found in the references mentioned earlier. If we solved
the set of simultaneous equations HXo = −C directly, with a Cholesky decomposi-
tion for example, it would avoid the need to invert the matrix H; this should also be
somewhat faster numerically for obtaining the best estimate Xo. Whichever route we
decide to follow, it will be most efﬁcient if the algorithm used exploits the known sym-
metry of ∇∇L ( ∂2L/∂Xi ∂Xj = ∂2L/∂Xj ∂Xi ). The computational time taken
for such matrix manipulations tends to scale like the cube of the number of variables
M 3; the speed of personal computers is sufﬁcient to tackle most reasonable parameter
estimation problems in this way.
We will, of course, have difﬁculty in solving the equations for the optimal estimate
if the determinant of H is zero, or extremely small; in that case, our answers will be

58
Parameter estimation II
highly sensitive to small changes in the data and the corresponding (marginal) error-
bars will be huge. In terms of Fig. 3.6, this will happen if the ellipsoid of Q = k , where
Q =

X−Xo
T ∇∇L(Xo)

X−Xo

, is (almost) inﬁnitely long in any of its principal
directions. In this situation, it is useful to analyse the eigenvalues and eigenvectors of H:
the principal axes indicate which linear combinations of the parameters can be inferred
independently of each other and the eigenvalues give the reliability with which they can
be estimated. The only real cure for the predicament is to improve the characteristics of
the posterior pdf; this can be done by obtaining more relevant data, or by supplementing
them with cogent prior information.
3.4.3
Iterative linearization
The linear problem is so convenient, both analytically and computationally, that it is
worth trying to make use of it even when ∇L cannot quite be written in the form of eqn
(3.50). To see how we might do this, consider the Taylor series expansion of L about
some arbitrary point in parameter space X1:
L = L(X1) +

X−X1
T ∇L(X1) + 1
2

X−X1
T ∇∇L(X1)

X−X1

+ · · · .
In the past, the ﬁrst-derivative term has been missing because we’ve always expanded
about the optimal estimate Xo. Differentiating this with respect to the {Xj}, we obtain
∇L = ∇L(X1) + ∇∇L(X1)

X−X1

+ · · · ,
(3.53)
which is just the Taylor series for ∇L. If we ignore the higher-order terms on the right-
hand side, so that eqn (3.53) can be rearranged into the linear form of eqn (3.50), then
the solution to eqn (3.49) is given by
Xo ≈X1 −

∇∇L(X1)
−1
∇L(X1) .
(3.54)
This relationship will be exact when X1 = Xo, or if ∇L is truly linear, but it will be
a reasonable approximation as long as X1 is close enough to the optimal estimate. As
such, it lends itself to suggest an iterative algorithm: (i) start with a good guess of the
optimal solution X1; (ii) evaluate the gradient-vector ∇L, and the second-derivative
matrix ∇∇L, at X =X1; (iii) calculate an improved estimate X2, by equating it to the
right-hand side of eqn (3.54); (iv) repeat this process until ∇L = 0.
The procedure outlined above is called the Newton–Raphson algorithm. It is a gener-
alization of the numerical technique often used to ﬁnd the roots of a function f(xo)=0;
in this case, the function is multivariate: ∇L(Xo) = 0. We can summarize it with the
recursion relationship
XN+1 = XN −

∇∇L(XN)
−1
∇L(XN ) ,
(3.55)
where XN is our estimate of the solution after N −1 iterations. If ∇L is really linear,
then only one iteration will be required: X2 will equal Xo, irrespective of the initial

Algorithms: a numerical interlude
59
guess X1. In general, the algorithm will rapidly converge to Xo as long as the starting
point is ‘close enough’ to the optimal solution; we will say more about this shortly, but
ﬁrst let us brieﬂy mention an important practical point.
The stability of iterative procedures can usually be improved by slowing them down
a little. This means that it can be advantageous to make a slightly smaller change in
going from XN to XN+1 than that recommended by eqn (3.55). Although this can
easily be done by multiplying the matrix-vector product on the right of eqn (3.55) by
a fractional constant, it tends to be better to achieve a similar effect by adding a small
(negative) number c to all the diagonal elements of ∇∇L:
XN+1 = XN −

∇∇L(XN) + c I
−1
∇L(XN ) ,
(3.56)
where I is the identity matrix, with ones along the diagonal and zeros everywhere else.
The reason for this curious suggestion is best understood from the quadratic form in
Fig. 3.6, where the nature of a matrix is characterized by its eigenvalues {λj} and
eigenvectors {ej}; explicitly, for ∇∇L, these are the solutions of the equation

∇∇L

ej = λj ej ,
(3.57)
where j = 1, 2, . . . , M. If we add a multiple of the identity matrix to ∇∇L, then we
ﬁnd that this new matrix has the same eigenvectors but different eigenvalues:

∇∇L + c I

ej =

λj + c

ej ,
(3.58)
Thus the ‘beeﬁng-up of the diagonal’ has no effect on the orientation of the ellipsoid
and leaves the correlations between the parameters in place; with a suitable choice of c,
however, it does cause a signiﬁcant narrowing in those principal directions in which it
was very elongated. Since these small eigenvalues are associated with large uncertain-
ties, eqn (3.56) stabilizes the iterative algorithm by selectively reducing their inﬂuence.
To put it another way, the inverse of a matrix is related to the reciprocal of its deter-
minant; as this is given by the product of the eigenvalues, the small ones will cause
(∇∇L)−1 to blow up. By adding a small (negative) multiple of the identity matrix to
∇∇L, we can ensure that the magnitude of the determinant is safely greater than zero;
this results in a reduced (ﬁnite) value for the inverse of the hybrid second-derivative ma-
trix and gives a correspondingly smaller change in our estimate of the solution between
iterations.
If the number of parameters becomes too large, so that the ∇∇L matrix is difﬁcult
to store or invert, then the Newton–Raphson procedure can be implemented through a
conjugate-gradient algorithm; details may be found in Numerical Recipes (Press et al.
1986), for example. The more serious problem is that the solution to eqn (3.49) will
not necessarily represent the position of the maximum, even when the posterior pdf is
unimodal. The requirement ∇L(Xo) = 0 is really a condition for ﬁnding a stationary
point and can sometimes be satisﬁed in the extreme tails of the posterior probability.
The Newton–Raphson procedure will diverge (to inﬁnity), therefore, if the initial guess
X1 is not close enough to the optimal solution. The situation is illustrated schematically
in Fig. 3.10.

60
Parameter estimation II
Fig. 3.10 An illustration of the logarithm of two posterior pdfs: (a) a well-behaved (linear-like)
problem for which the Newton–Raphson procedure will converge to Xo from any starting point;
(b) a unimodal pdf where the Newton–Raphson algorithm will diverge unless the initial guess
(X1) lies within the two dotted lines.
For the case of Fig. 3.10(b), one way to ensure that our computer program will
converge towards Xo is to rely predominantly on the direction of the gradient ∇L. A
good alternative to such ﬁrst-derivative methods is provided by the approach of Nelder
and Mead (1965): their ‘up-hill’ simplex search algorithm works with the function L
directly and is very robust for unimodal pdfs; it is practical to use for up to a couple
of dozen parameters. Being gradient-free, it also allows (any) sharp cut-offs imposed
by the prior to be encoded in a straightforward manner. Nevertheless, since it lacks the
efﬁciency of Newton–Raphson, it’s best used just as a ﬁrst step to get close to the opti-
mal solution. Besides, we need to evaluate the second-derivative matrix to estimate the
error-bars for the parameters; if necessary, the required gradients can even be calculated
by ﬁnite-differences ( ∂L/∂Xj ≈

L(X̸=j, Xj+ δ/2) −L(X̸=j, Xj−δ/2)

/δ, etc.).
3.4.4
Hard problems
The most difﬁcult optimization task occurs for the case when the posterior pdf is multi-
modal. For up to a couple of parameters, or so, the best thing to do is to use brute force
and ignorance: simply ‘display’ the entire pdf, by evaluating it on a discrete grid, and
look for the maximum(s). Unfortunately, as mentioned earlier, this becomes hopelessly
impractical for more than a handful of variables. While gradient-based algorithms are
the most efﬁcient for multivariate analysis, they cannot help for the multimodal case: a
knowledge of the local slope sheds no light on the position of the global maximum. To
date, all procedures designed to address this problem entail a large element of inspired
trial-and-error. Simulated annealing is perhaps the most well-established of these tech-
niques and was proposed by Kirkpatrick et al. (1983) on the basis of a thermodynamic
analogy. The basic scheme involves an exploration of the parameter space by a series of
trial random changes in the current estimate of the solution; in terms of eqn (3.55), this
could be written as XN+1 = XN + ∆X, where ∆X is chosen by a random number
generator. The proposed update of the solution is always considered advantageous if it
yields a higher value of L, but bad moves are also sometimes accepted. This occasional

Approximations: maximum likelihood and least-squares
61
Fig. 3.11 A schematic illustration of the logarithm of two multimodal posterior pdfs. Simulated
annealing should work well for case (a), but will not be very helpful for (b).
allowance of retrograde steps provides a mechanism for escaping entrapment in local
maxima. The shape of L for two multimodal posterior pdfs is shown in Fig. 3.11. Simu-
lated annealing should work like a treat for case (a), where there is an underlying trend
towards a global maximum, but is unlikely to be much more efﬁcient than brute force
and ignorance for (b).
The real difﬁculty with the multimodal case is that it is almost impossible to guar-
antee that the optimal solution has been found, in a ﬁnite amount of time, in general. As
such, the ﬁeld remains wide open. Ingenious new schemes, such as genetic algorithms
(Michalewicz 1992), are always being put forward, but none can claim universal suc-
cess or applicability. We must also remember that it doesn’t really make sense to talk
about a single ‘best’ answer, even in principle, if the posterior pdf has several maxima
of comparable magnitude. In these cases, Monte Carlo methods that can simulate ex-
ploration of all the important regions of parameter space will have considerable merit;
they are the focus of a more detailed discussion in Chapters 9 and 10.
3.5
Approximations: maximum likelihood and least-squares
There has been one common theme to all our discussions, and examples, so far: namely,
the use of Bayes’ theorem. Its great virtue lies in the fact that it helps us to relate the
pdf required to describe our inference, to others which we have a better chance of being
able to assign. Using the components of the vectors X and D to denote the set of
M parameters of interest and the N measured data, respectively, we can write Bayes’
theorem compactly as
prob(X|D, I) ∝prob(D|X, I) × prob(X|I) ,
(3.59)
where I represents all the relevant background information. The prior pdf should reﬂect
everything we know about X before the analysis of the current data; if we are largely
ignorant, then we might indicate this na¨ıvely with a uniform, or ﬂat, pdf (as a simple
limiting case of a very broad one):
prob(X|I) = constant ,
(3.60)

62
Parameter estimation II
for, effectively, all values of X. Since this uniform assignment can be absorbed into
the (omitted) normalization constant of eqn (3.59), the posterior pdf becomes directly
proportional to the likelihood function:
prob(X|D, I) ∝prob(D|X, I) .
(3.61)
Thus our best estimate Xo, given by the maximum of the posterior, is equivalent to the
solution which yields the greatest value for the probability of the observed data; in this
context, it is usually referred to as the maximum likelihood estimate.
We can obtain further simpliﬁcation by making suitable approximations with regard
to the likelihood function itself. If we assume that the data are independent, for example,
then their joint pdf prob(D|X, I) is given by the product of the probabilities for the
individual measurements:
prob(D|X, I) =
N

k=1
prob(Dk|X, I) ,
(3.62)
where Dk is the kth datum. Although we have used this result several times before, it
is worth emphasizing that it follows immediately from (the repeated application of) the
product rule of eqn (1.2),
prob(Dk, Dl|X, I) = prob(Dk|Dl, X, I) × prob(Dl|X, I) ,
and the assertion that our knowledge of one datum has no inﬂuence on our ability to
predict the outcome of another, if we are already given X:
prob(Dk|Dl, X, I) = prob(Dk|X, I) .
If we also assume that the noise associated with the experimental measurements can
reasonably be represented as a Gaussian process, then the probability of an individual
datum can be written as:
prob(Dk|X, I) =
1
σk
√
2π exp
	
−(Fk−Dk)2
2 σk2

,
(3.63)
where I implicitly includes a knowledge of both the expected size of the error-bars
{σk}, and an adequate model of the functional relationship f between the parameters
X and the ideal (noiseless) data F :
Fk = f (X, k) .
(3.64)
Equations (3.62) and (3.63) then allow us to approximate the likelihood function by
prob(D|X, I) ∝exp

−χ2
2

,
(3.65)
where χ2 is the sum of the squares of the normalized residuals (Rk = (Fk−Dk)/σk):

Approximations: maximum likelihood and least-squares
63
χ2 =
N

k=1
Fk−Dk
σk
2
.
(3.66)
With the uniform prior of eqn (3.60), according to eqn (3.61), the logarithm L of the
posterior pdf is simply given by
L = loge

prob(X|D, I)

= constant −χ2
2 .
(3.67)
Since the maximum of the posterior will occur when χ2 is smallest, the corresponding
optimal solution Xo is usually called the least-squares estimate.
Maximum likelihood and least-squares are amongst the most frequently used pro-
cedures in data analysis; as we have just seen, they can easily be justiﬁed through the
Bayesian approach. In this context, however, neither of these methods is seen as fun-
damental or sacrosanct: they are merely what Bayes’ theorem reduces to when certain
simplifying approximations are deemed to be suitable. If the results from a least-squares
analysis seem unsatisfactory, for example, we don’t need to throw-up our hands in hor-
ror and say ‘Least-squares doesn’t work!’. Since we understand the assumptions which
underlie its use, it’s much more fruitful to ponder whether these were appropriate for the
problem being considered; if not, we should start again from Bayes’ theorem and derive
a better statistical prescription for that case. Some examples of the latter are given in
Chapter 8.
Perhaps one of the main reasons for the popularity of the least-squares method is
that it is simple to apply. This is particularly true if the functional relationship of eqn
(3.64) is linear, because then ∇L can be written in the convenient form of eqn (3.50);
as mentioned in Section 3.4, this makes the optimization problem very easy to handle.
To see that this is so, let us write the equation for the kth ideal datum as
Fk =
M

j=1
Tkj Xj + Ck ,
(3.68)
where the values of both Tkj and Ck are independent of the parameters {Xj}; in matrix-
vector notation, this would just be F = TX+C. Using the chain-rule of differentiation,
the jth component of ∇L is given by
∂L
∂Xj
= −1
2
∂χ2
∂Xj
= −
N

k=1
(Fk−Dk)
σk2
∂Fk
∂Xj
.
(3.69)
Although we could substitute for Fk, and ∂Fk/∂Xj = Tkj, from eqn (3.68) in eqn (3.69),
and rearrange it into the form of eqn (3.50), it’s slightly messy to do this algebraically.
An easier way to verify the linearity of ∇L is to differentiate eqn (3.69) with respect to
Xi and notice that the elements of the second-derivative matrix are all constant:
∂2L
∂Xi ∂Xj
= −
N

k=1
Tki Tkj
σk2
.
(3.70)

64
Parameter estimation II
Since all the higher derivatives of L are identically zero, the posterior pdf is deﬁned
completely by the optimal solution Xo and its covariance matrix; the components of
the latter are related to twice the inverse of the ∇∇χ2, or the Hessian, matrix:

Xi−Xoi

Xj−Xoj

= −

∇∇L
−1 
ij = 2

∇∇χ2−1 
ij .
(3.71)
The useful property that the ∇∇L matrix will be constant if the parameters X
are related linearly to the data is not generally true without eqn (3.67); that is why the
least-squares approximation is very convenient. For example, the functional model of
eqn (3.1), where we are given the shape and position of the signal peak, is linear with
respect to the amplitude A and the background B. Nevertheless, the optimal solution
(Ao , Bo) is difﬁcult to write down analytically because the gradient-vector of the pos-
terior pdf in eqn (3.8) cannot be rearranged into the form of eqn (3.50); this was not a
serious impediment, of course, because we just did the problem numerically (with brute
force and ignorance). Even for that case of the Poisson likelihood, however, we could
have obtained a fairly good estimate by using the least-squares approximation. This is
because eqn (3.2) starts to take on Gaussian characteristics if the expected number of
counts is more than a handful (⩾10); this behaviour can be seen in Fig. 3.2(b), despite
the fact that it’s a function of a discrete variable. In the limit of large numbers, it can
formally be shown that eqn (3.2) is well-represented by a normal distribution:
prob(N |D) = DN e−D
N!
∝exp
	
−(N −D)2
2 D

,
In our usual shorthand notation, we could summarize this by saying N ≈D ±
√
D.
As the number of measured counts will be roughly equal to the expected value D, we
can make the denominator of the exponent independent of the parameters A and B by
replacing the
√
D error-bar with
√
N ; this helps to linearize the optimization problem.
With a uniform prior, the logarithm of the posterior pdf is then approximated well by
eqn (3.67); in terms of our present notation, the corresponding χ2 statistic is given by
χ2 =
N

k=1
(Fk−Dk)2
Dk
,
where Dk is now the number of counts measured in the kth data-channel and Fk is
our estimate of their expected number based on the linear relationship of eqn (3.1).
This result, where the error-bar in eqn (3.66) is replaced by the square root of the datum
(σk
2 = Dk, or more often Fk), tallies with the deﬁnition of χ2 found in many elementary
textbooks; it suggests that the assumption of a Poisson process is implicit in its use. This
would not be appropriate, for example, if the uncertainties related to an experiment were
dominated by thermal (Johnson) noise in a piece of electronic equipment; in that case,
σk = a constant (proportional to the temperature) would be more suitable.
Despite the practical beneﬁts of least-squares, we should emphasize that the real
justiﬁcation for its use hinges on the assignments of a Gaussian likelihood function and

Approximations: maximum likelihood and least-squares
65
uniform prior. Otherwise, what reason do we have for using the sum of the squares of
the residuals as a misﬁt statistic as opposed to anything else? Indeed, there is something
to be said for the robustness of the l1-norm:
l1-norm =
N

k=1

Fk−Dk
σk
 .
(3.72)
This sum of the moduli of the (normalized) residuals has the advantage that it is far
less susceptible to the effects of ‘freak data’ than is χ2 (which is called the l2-norm,
in this terminology). Although the least-squares prescription was obtained by adopting
the traditional assumptions of independent, additive, Gaussian noise in this section, we
will see an alternative derivation in Chapter 5; there we will use the maximum entropy
principle to assign the relevant uniform, Gaussian and exponential pdfs needed to justify
the maximum likelihood, least-squares and l1-norm estimates.
Before giving a simple example of the use of least-squares, we should warn against
the unfortunate connotations conjured up by the words ‘maximum likelihood estimate’.
In terms of everyday parlance, it suggests that we have obtained the most probable
values for the parameters of interest; without qualiﬁcation, this is not so! By maximizing
the likelihood function, we have found the values which make the measured data most
probable; although we expect this to be relevant to our real question, ‘What are the most
probable values of the parameters, given the data?’, it is not the same thing. To put it
more plainly: prob(A|B) ̸= prob(B|A), in general. For example, the probability that
it will rain given that there are clouds overhead is quite different from the probability of
there being clouds overhead given that it’s raining. The required relationship between
these distinct conceptual entities is, of course, provided by Bayes’ theorem. At this
juncture, the reader may be forgiven for thinking that this is purely a philosophical point;
after all, we have already noted, in eqn (3.61), that the posterior pdf becomes directly
proportional to the likelihood function if we assign a uniform prior (which we often
do). This belabouring is important, however, because a recognition of the qualiﬁcation
can have practical consequences. Even with a ﬂat prior, a due consideration of the range
over which it is valid can lead to a better estimate of the parameters involved and enable
us to tackle problems which would otherwise be inaccessible; we will see examples of
this in both Section 3.6 and in Chapter 4.
3.5.1
Fitting a straight line
One of the most frequently encountered problems in data analysis is the ﬁtting of a
straight line to graphical data. Suppose we are given a set of N data {Yk}, with associ-
ated error-bars {σk}, measured at ‘positions’ {xk}. What is the best estimate of the two
parameters of a straight line which describes them?
Let us do this problem as a simple exercise in the use of least-squares. The situation
is illustrated schematically in Fig. 3.12. For the straight-line model, the kth ideal datum
yk is given by
yk = mxk + c ,
where m is the slope and c is the intercept. Substituting Fk = yk, and Dk = Yk, into
eqn (3.66), we obtain

66
Parameter estimation II
Fig. 3.12 Fitting a straight line to noisy graphical data.
χ2 =
N

k=1
(mxk + c −Yk)2
σk2
.
According to eqn (3.69), the components of ∇L are given by (−1/2 times) the partial
derivatives of χ2:
∂χ2
∂m =
N

k=1
2 (mxk + c −Yk) xk
σk2
and
∂χ2
∂c
=
N

k=1
2 (mxk + c −Yk)
σk2
.
To make the subsequent algebraic manipulations easier, let us rewrite ∇∇χ2 in the
form of eqn (3.50):
∇χ2 =

α
γ
γ
β

m
c

−

p
q

,
(3.73)
where the constants α, β, γ, p and q are related to the data through
α =  wkxk
2 ,
β =  wk ,
γ =  wkxk ,
p =  wkxkYk ,
q =  wkYk ,
in which wk = 2/σk
2 and the summations are from k = 1 to N. The best estimate
(mo, co), given by the minimum of χ2, is deﬁned by the requirement that ∇χ2 =0; the
pair of linear simultaneous equations resulting from equating eqn (3.73) to zero can be
solved directly, or by matrix inversion as in eqn (3.51), and yield
mo = β p −γ q
αβ −γ2
and
co = αq −γ p
αβ −γ2 .
(3.74)
From eqn (3.71), the corresponding covariance matrix for these parameters is given by
twice the inverse of ∇∇χ2. By explicit differentiation, or comparison with eqns (3.50)
and (3.52), the latter is the same as the 2×2 matrix in eqn (3.73); thus

Approximations: maximum likelihood and least-squares
67

σmm
2
σmc
2
σmc
2
σcc
2

= 2

α
γ
γ
β
−1
=
2
αβ −γ2

β
−γ
−γ
α

,
(3.75)
where the square root of the diagonal elements gives the (marginal) error-bars for the
inferred values of m and c, and the γ-term describes how they are correlated.
If the error-bars for the data are not known, then we might do the analysis by assum-
ing that they are all of the same size: σk=σ. By setting wk= a constant = 2/σ2 in the
deﬁnitions of α, β, γ, p and q above, we would ﬁnd that the values of mo and co were
independent of the magnitude of σ; however, this would not be true of our estimate of
the reliability of the optimal solution. As in Section 3.3, we must take the uncertainty
about σ into account by marginalizing over it as a nuisance parameter:
prob(m, c|{Yk}, I) =
∞

0
prob(m, c, σ|{Yk}, I) dσ
∝
∞

0
prob({Yk}|m, c, σ, I) × prob(m, c, σ|I) dσ ,
where we have used Bayes’ theorem to express the joint posterior pdf on the right as a
product of the likelihood function and a prior pdf. With the assumptions of independent,
additive, Gaussian noise, the likelihood function is still of the form exp(−χ2/2):
prob({Yk}|m, c, σ, I) ∝σ−N exp

−
1
2 σ2
N

k=1
(mxk + c −Yk)2

,
but we now have to be careful to explicitly retain all the factors which depend on σ. If
we combine this with the assignment of a uniform prior, then the integral over σ can be
evaluated with the same algebra as in Section 3.3; this results in a Student-t distribution,
very similar to that of eqn (3.38):
prob(m, c|{Yk}, I) ∝

N

k=1
(mxk + c −Yk)2
−(N−1)/2
.
As usual, the best estimate of m and c, and their related covariance matrix, is given
by the partial derivatives of the logarithm of this (marginal) posterior pdf. This leads to
the recovery of the formulae of eqns (3.74) and (3.75), with the unknown value of the
error-bar σ being replaced by an estimate S derived from the data:
S2 =
1
N −1
N

k=1
(mo xk + co −Yk)2 .
In all the preceding analysis, we have implicitly assumed (in I) that the positions {xk}
are known exactly. The case in which there is also a signiﬁcant uncertainty with regard
to the x-coordinate is more challenging, and we refer the avid reader to Gull (1989b)
for a discussion of the subtleties involved.

68
Parameter estimation II
3.6
Error-propagation: changing variables
The last topic in this chapter is often called the propagation of errors; despite the some-
what alarming title, suggestive of an unfortunate ampliﬁcation of mistakes, it’s quite
innocuous. Here we are concerned with the question of how uncertainties in our esti-
mate of a set of parameters translate into reliabilities of quantities derived from them.
For example, suppose we are told that X =10 ± 3 and Y =7 ± 2; what can we say about
the difference X−Y , or the ratio X/Y , or the sum of their squares X 2 + Y 2, and so
on? In essence, the problem is nothing more than an exercise in the change of variables:
given the pdf prob(X, Y |I), where the relevant information I includes a knowledge
of the data if dealing with the posterior, we need the corresponding pdf prob(Z|I),
where Z = X−Y , or Z = X/Y , or whatever, as appropriate.
Let us start with the easiest type of transformation; namely, one that involves a single
variable and some function of it. Given that Y = f(X), how is the pdf prob(X|I)
related to prob(Y |I)? Imagine taking a very small interval δX about some arbitrary
point X =X∗; the probability that X lies in the range X∗−δX/2 to X∗+ δX/2 is
given by
prob

X∗−δX
2
⩽X < X∗+ δX
2
I

≈prob(X =X∗|I) δX ,
(3.76)
where the equality becomes exact in the limit δX →0. Now suppose that we view this
pdf as a function of another quantity Y , related (monotonically) to X by Y = f(X);
then, f will map the point X = X∗(uniquely) to Y = Y ∗= f(X∗) and the interval
δX to the corresponding region δY . The situation is shown schematically in Fig. 3.13.
Since the range of Y values spanned by Y ∗± δY/2 is equivalent to a variation in X
between X∗± δX/2, the area under the pdf prob(Y |I) should equal the probability
represented by eqn (3.76). Therefore, we require that
prob(X =X∗|I) δX = prob(Y =Y ∗|I) δY .
As this must be true for any point in X-space then, in the limit of inﬁnitesimally small
intervals, we obtain the relationship
Fig. 3.13 Changing variables in one dimension: the function f maps the point X∗to Y ∗=f(X∗)
and the small interval δX to the corresponding region δY .

Error-propagation: changing variables
69
prob(X|I) = prob(Y |I) ×

dY
dX
 ,
(3.77)
where the term on the far right, given by the modulus of df/dX, is called the Jacobian.
The reason for taking the magnitude of the derivative is to ensure that it represents a
ratio of lengths even when positive increments in X map to negative ones for Y . If
Y = f(X) is not a one-to-one transformation, then eqn (3.77) has to be extended to a
summation over all the Y values which correspond to a given X.
As a concrete example of this procedure, let’s return to the lighthouse problem of
Section 2.4. There, in eqn (2.32), we had assigned a uniform pdf for the azimuth θ:
prob(θ|α, β, I) = 1/π, where the angle θ had to lie between ± π/2 radians. In eqn
(2.33), we also had the connection between the azimuth and the position along the coast
x: β tanθ = x −α. Differentiating both sides of eqn (2.33) with respect to x, we have
β sec2θ × dθ
dx = 1 .
Using the trigonometric identity tan2θ + 1 ≡sec2θ, and substituting for tanθ from
eqn (2.33), the Jacobian can be written as
dθ
dx =

β

1 + tan2θ
−1
=

β

1 +
x −α
β
2 −1
.
Finally, we can use eqn (3.77) to transform the pdf for θ in eqn (2.32) to its equivalent
form in terms of x; after a little algebraic rearrangement, we obtain (as promised) the
Cauchy distribution of eqn (2.34):
prob(x|α, β, I) = prob(θ|α, β, I) ×

dθ
dx
 =
β
π

β 2 + (x−α)2 .
The result in eqn (3.77) can be generalized to the case of several variables. Although
it becomes more difﬁcult to give a simple pictorial representation, like the one in Fig.
3.13, the central theme of the argument is the same: if we want to write the pdf for M
parameters {Xj} in terms of the same number of quantities {Yj} related to them, then
we must ensure that
prob({Xj}|I) δX1 δX2 · · · δXM = prob({Yj}|I) δMVol

{Yj}

,
where δMVol

{Yj}

is the M-dimensional volume in Y-space mapped out by the small
hypercube region δX1 δX2 · · · δXM in X-space. After some effort, most textbooks on
mathematical methods (for scientists) derive the formula
δMVol

{Yj}

=

∂(Y1 , Y2 , · · · , YM)
∂(X1 , X2 , · · · , XM)
 δX1 δX2 · · · δXM ,
where the strange-looking quantity in the modulus sign is the multivariate Jacobian; it
is given by the determinant of the M×M matrix of partial derivatives ∂Yi/∂Xj. Thus
the general form of eqn (3.77) can be written as

70
Parameter estimation II
prob({Xj}|I) = prob({Yj}|I) ×

∂(Y1 , Y2 , · · · , YM)
∂(X1 , X2 , · · · , XM)
 .
(3.78)
To illustrate the use of eqn (3.78), let’s consider the transformation of a pdf deﬁned
on a two-dimensional Cartesian grid (x, y) to its equivalent form in polar coordinates
(R, θ). From Fig. 3.14(a), it can be seen that the functional relationship between the two
sets of parameters is given by
x = R cosθ
and
y = R sinθ .
Taking the partial derivatives of x and y with respect to R and θ, we can easily evaluate
the determinant of the resulting 2×2 matrix for the Jacobian:

∂(x, y)
∂(R, θ)
 =

cosθ
−R sinθ
sinθ
R cosθ
 = R

cos2θ + sin2θ

= R ,
where the ﬁnal simpliﬁcation is obtained from the identity sin2 θ + cos2 θ ≡1. Accord-
ing to eqn (3.78), therefore, the pdf prob(R, θ|I) is related to prob(x, y|I) by
prob(R, θ|I) = prob(x, y|I) × R .
(3.79)
Thus if the pdf for x and y was an isotropic, bivariate, Gaussian,
prob(x, y|I) =
1
2πσ2 exp
	
−

x2+ y2
2 σ2

,
(3.80)
then the corresponding pdf for R and θ would take the form
prob(R, θ|I) =
R
2πσ2 exp

−R2
2 σ2

.
(3.81)
Rather than working through the formal Jacobian analysis given above, we could
have obtained eqn (3.79) directly from a simple geometrical argument: the probability
Fig. 3.14 Changing variables from Cartesian to polar coordinates.

Error-propagation: changing variables
71
that the polar parameters lie in the small range R±δR/2, and θ ±δθ/2, is given by the
product of the value of the pdf prob(x, y|I) at the point corresponding to (R, θ) and
the shaded element of area R δθ δR in Fig. 3.14(a); by the deﬁnition of the pdf for R
and θ, of course, this is equal to prob(R, θ|I) δθδR. Hence, the desired result follows:
prob(R, θ|I) δR δθ = prob(x, y|I) R δR δθ.
Having transformed prob(x, y|I)to polar coordinates, it becomes a straightforward
task to obtain the pdf for the radius R=

x2+y2: we just need to marginalize the joint
pdf prob(R, θ|I) overθ. For the case of the bivariate Gaussian in eqn (3.80), eqn (3.81)
is integrated readily with respect to θ:
prob(R|I) =
2π

0
prob(R, θ|I) dθ =
R
σ2 exp

−R2
2 σ2

.
(3.82)
Again, we could also have arrived at this result pictorially: since the value of the pdf
in eqn (3.80) depends only on the distance from the origin, and not the direction, the
probability that R lies in a narrow range δR is given by the product of the magnitude
of the Gaussian pdf at that radius and the corresponding area of the shaded ring shown
in Fig. 3.14(b); hence, prob(R|I) δR = prob(x, y|I) 2πR δR and eqn (3.82) follows
immediately with the substitution of x2+ y2 =R2 in eqn (3.80).
A multidimensional generalization of this last argument allows us to derive the χ2
distribution mentioned towards the end of Section 3.3. The likelihood function of eqns
(3.65) and (3.66) is an N-dimensional, isotropic, Gaussian when viewed in terms of the
normalized residuals:
prob(D|X, I) ∝exp

−r2
1 + r2
2 + · · · + r2
N
2

,
where rk = (Fk −Dk)/σk. As in the case above, the value of the pdf depends only on
the distance from the origin (rk = 0); this radius R =

Σ rk2 is, of course, just the
square root of χ2. The probability that R lies in a narrow range δR, prob(R|X, I) δR,
will equal the product of the magnitude of the likelihood function at that radius and the
hypervolume of the associated spherical shell; since the latter is proportional to RN−1,
the marginal distribution is just an extension of eqn (3.82):
prob(R|X, I) ∝RN−1 exp

−R2/2

.
To convert this into a pdf for χ2, we just need to carry out a one-to-one transformation
according to eqn (3.77); using the functional relationship χ2 =R2, we obtain
prob

χ2 |X, I

∝

χ2 N/2−1 exp

−χ2/2

.
(3.83)
Technically, this is called a χ2 distribution with N degrees of freedom. For N ⩾2, it
has a maximum at N−2; if the number of data is large, its shape is well-described by a
Gaussian pdf summarized by χ2 ≈N ±
√
2N.
We have now seen the basic ingredients required for the propagation of errors: it
either involves a transformation in the sense of eqn (3.78), or an integration such as eqn

72
Parameter estimation II
Fig. 3.15 The elements of area over which the joint pdf for the two parameters prob(X, Y |I)
must be integrated to obtain an estimate of: (a) the sum Z =X+Y ; and (b) the ratio Z =X/Y .
(3.82), or a combination of the two. For the common problem of wanting to estimate the
sum Z = X +Y , or the ratio Z = X/Y , of two parameters X and Y , we simply need
to integrate the joint pdf prob(X, Y |I) along the shaded strips in Fig. 3.15. If this is
not intuitively obvious, we can justify it analytically by using marginalization and the
product rule:
prob(Z|I) =

prob(Z|X, Y, I) prob(X, Y |I) dX dY
=

δ

Z−f(X, Y )

prob(X, Y |I) dX dY ,
(3.84)
where the Dirac δ-function in the second line is equal to zero unless Z =f(X, Y ), and
indicates that Z is determined unambiguously (by the function f ) when X and Y are
known. Let’s explicitly work through this procedure for the case Z =X+Y .
Following eqn (3.84), the integral along the diagonal strip in Fig. 3.15(a) can be
written as
prob(Z|I) =

prob(X, Y |I) δ

Z−(X+Y )

dX dY .
(3.85)
If the information I only tells us that X = xo ± σx, and Y = yo ± σy , then it is
reasonable to assume that these parameters are not correlated; prob(X, Y |I) is then
just the product of the individual pdfs for X and Y , so that eqn (3.85) becomes
prob(Z|I) =

dX prob(X|I)

prob(Y |I) δ

Z−X−Y

dY .
Since the δ-function is inﬁnitely sharp (but has unit area), the Y integrand is zero unless
Y = Z −X; the integral with respect to Y is trivial, therefore, and the pdf for the sum
reduces to a convolution of the pdfs for X and Y :

Error-propagation: changing variables
73
prob(Z|I) =

prob(X|I) prob(Y =Z−X |I) dX .
(3.86)
The nature of the information I also suggests that we should assign Gaussian pdfs for
the two parameters, with maxima at xo and yo, and widths σx and σy, respectively;
substituting these into eqn (3.86), we have
prob(Z|I) =
1
2πσxσy
+∞

−∞
exp

−(X−xo)2
2 σx2

exp

−(Z−X−yo)2
2 σy2

dX .
After some fairly tedious algebra, consisting largely of completing the square for X in
the exponent and simplifying the resultant expression, we obtain
prob(Z|I) =
1
σz
√
2π exp
	
−(Z−zo)2
2 σz2

,
(3.87a)
where
zo = xo + yo
and
σz
2 = σx
2 + σy
2 .
(3.87b)
Thus the pdf for the sum is a Gaussian, with a maximum at zo and width of σz; in fact,
the pdf for the difference Z =X−Y is the same except that zo is then given by xo−yo.
3.6.1
A useful short cut
The calculation above seems rather long-winded for such a simple problem. Could we
have got there by a simpler route? After all, we are not usually interested in knowing
the ﬁne detail of the shapes of the pdfs; for practical purposes, we’re often satisﬁed to
approximate them with Gaussians. Within such limits, the answer is ‘Yes, there is an
easier method!’
Intuitively, we might have guessed that the best estimate for the difference of the
two parameters was xo−yo ; the corresponding error-bar, given σx and σy, requires a
little more thought. Suppose we perturb Z =X−Y :
δZ = δX −δY .
(3.88)
This would then tell us how small changes in the values of X and Y would affect our
estimate of their difference. For the error-bars, of course, we are interested in studying
the deviations from the optimal solution: δX = X −xo, and so on. According to the
integral deﬁnitions of eqns (3.25) and (3.26), and the information I, we know that

δX 2
= σx
2 ,

δY 2
= σy
2 ,

δX δY

= 0 .
(3.89)
Note that by δX 2 we mean (δX)2, not δ

X2
, and so on. Squaring both sides of eqn
(3.88) and taking expectation values, we have

δZ 2
=

δX 2 + δY 2 −2 δX δY

=

δX 2
+

δY 2
−2

δX δY

,

74
Parameter estimation II
where we have used the linear property that the integral of a sum of terms is equal to
the sum of the integrals of those terms, in separating the expectations in the last step.
Substituting from eqn (3.89), we obtain the error-bar of the difference X−Y :
σz =

δZ 2
=

σx2 + σy2 .
As promised, this is the same as for the sum X+Y in eqn (3.87b).
For a second example of this procedure, let’s work out the formula for the error-
bar of the ratio of two parameters. Perturbing Z = X/Y , using the quotient rule of
differentiation, we have
δZ = Y δX −X δY
Y 2
.
This expression can be simpliﬁed slightly by dividing through by the ratio itself:
δZ
Z
= δX
X −δY
Y
.
Squaring both sides and taking expectation values, we obtain

δZ 2
zo2
=

δX 2
xo2
+

δY 2
yo2
−2

δX δY

xo yo
,
where the X, Y and Z in the denominator have been replaced by the constants xo, yo
and zo (= xo/yo) because we are interested in the deviations from the optimal solution.
Finally, substituting from eqn (3.89) and taking square roots gives the desired formula
for the error-bar of the ratio:
σz
zo =
 σx
xo
2
+
σy
yo
2
.
(3.90)
Equivalently, changing variables to logarithms through eqn (3.77) gives us
σlog Z =

σ2
log X + σ2
log Y ,
which illustrates the variability of log Z = log X −log Y . Despite its virtues, let us end
our discussion of error-propagation with a salutary warning against the blind use of this
nifty short cut.
3.6.2
Taking the square root of a number
Data of the type shown in Fig. 3.3 are often collected in diffraction studies of crystalline
materials. With reference to the associated model of Fig. 3.1, the amplitude of the Bragg
(or signal) peak is usually estimated using a least-squares ﬁtting program and the results
stated as A = Ao ± σA ; any uncertainties about the background and the peak shape
do, of course, give rise to a correspondingly larger (marginal) error-bar. Since this

Error-propagation: changing variables
75
amplitude is related to the modulus squared of the complex structure factor, A = |F|2,
crystallographers need to take square roots. Writing f =|F|, therefore, we have
A = f 2.
(3.91)
For the purposes of this simple example, we are merely interested in obtaining the best
estimate fo and a measure of its reliability σf.
This seems like a fairly easy problem for the short cut procedure: the optimal so-
lution is obviously fo = √Ao; to propagate the error, we just differentiate eqn (3.91),
square both sides, and take the expectation values:

δA2
= 4 fo2 
δf 2
= 4 Ao

δf 2
,
Substituting σA2 =

δA2
, and rearranging this equation for the error-bar σf =

⟨δf 2⟩,
we quickly arrive at the result
f =

Ao ±
σA
2√Ao
.
(3.92)
Unfortunately, this elementary analysis breaks down when Ao is negative! Such an
occurrence is not unusual, particularly for weak and strongly overlapping reﬂections (or
signal peaks). So, what has gone wrong?
Well, we have made two mistakes — both are quite common. The ﬁrst stems from
our failure to distinguish clearly between the posterior pdf and the likelihood function.
The least-squares ﬁt is just a shorthand way of saying that, as a function of the am-
plitude, prob({data}|A, I) is approximately Gaussian, with a maximum at Ao and a
width of σA:
prob({data}|A, I) ∝exp
	
−(A−Ao)2
2 σA2

.
(3.93)
Our inference about the value of A, however, is described by prob(A|{data}, I). To
relate these two pdfs, we need to use Bayes’ theorem:
prob(A|{data}, I) ∝prob({data}|A, I) × prob(A|I) .
(3.94)
Since we know physically that the amplitude must be positive, we should, at least, assign
a prior which is zero for negative values of A ; a na¨ıve choice which incorporates this
information is
prob(A|I) =

constant
for A ⩾0 ,
0
otherwise .
(3.95)
Multiplying eqns (3.93) and (3.95), we see that the best estimate of A will always be
positive. This is true even when the amplitude Ao which gives the closest agreement
with the data is negative; in that case, the posterior pdf will be a severely truncated
Gaussian. The short cut procedure for the error-propagation will be ﬂawed, therefore,
because it implicitly relies on an expansion about a centrally located maximum; this

76
Parameter estimation II
was our second mistake. Since there are no such limitations on the formal change of
variables, from prob(A|{data}, I) to prob(f |{data}, I), we should do the proper
calculation according to eqn (3.77).
Having obtained the posterior pdf for the amplitude of the Bragg peak, the transfor-
mation we require to describe our inference about the modulus of the structure factor is
given by
prob(f |{data}, I) = prob(A|{data}, I) ×

dA
df
 ,
where the derivative of eqn (3.91) yields the Jacobian |dA/df| = 2f, with f =|F|⩾0.
This allows us to write the posterior pdf for f as
prob(f |{data}, I) ∝f × exp
	
−

f 2−Ao
2
2 σA2

,
(3.96)
for f ⩾0, and zero otherwise. As usual, the strange-looking expression of eqn (3.96)
can be approximated by the more familiar Gaussian pdf:
prob(f |{data}, I) ≈
1
σf
√
2π exp
	
−(f −fo)2
2 σf2

,
(3.97)
where, according to eqns (2.12) and (2.15), the parameters fo and σf are given by the
ﬁrst and second derivatives of the logarithm L = log e

prob(f |{data}, I)

:
2 fo2 = Ao +

Ao
2 + 2σA21/2
,
(3.98a)
where
σf
−2 =
1
fo2 + 2

3fo2 −Ao

σA2
.
(3.98b)
It can easily be shown that these formulae for the best estimate of f, and its error-bar,
reduce to the form of eqn (3.92) in the limit Ao ≫σA. For those reﬂections where the
amplitude of the Bragg peak is poorly determined by the data, the answers can be very
different. Let us illustrate this graphically with a few examples.
Consider ﬁrst of all a good reﬂection, A=9±1, as depicted in the top panels in Fig.
3.16. It conﬁrms that, in this case, the result of the short cut procedure agrees with the
proper solution. Moving on to the next pair of pdfs, which are for a poorly determined
amplitude, A = 1 ± 9, we notice an interesting divergence. The solution of eqn (3.96),
which is plotted with a solid line, indicates that the magnitude of the structure factor
could be vanishingly small but is very unlikely to be larger than 6; the optimal estimate
is about 2.6. By contrast, the result of eqn (3.92), shown as a dashed line, gives 1.0 for
the best estimate and does not exclude values of up to about 12 as being unreasonable.
It can also be seen that the Gaussian pdf of eqns (3.97) and (3.98), plotted with a dotted
line, is a respectable approximation to eqn (3.96). Finally, the most striking case occurs
when Ao is negative; for example, A=−20 ± 9. The last two panels in Fig. 3.16 show
that the proper probabilistic solution is perfectly well-behaved, whereas the short cut

Error-propagation: changing variables
77
Fig. 3.16 The left-hand panels show the posterior pdf for the amplitude of the Bragg peak,
given the na¨ıve prior of eqn (3.95) and the least-squares likelihood function of eqn (3.93), with
A = 9 ± 1, 1 ± 9 and −20 ± 9. The right-hand plots are the corresponding pdfs for the modulus
of the structure factor (f =
√
A): the solid, dotted and (when appropriate) dashed lines are for
eqns (3.96), (3.98) and (3.92), respectively.
procedure breaks down completely because eqn (3.92) does not yield a real value for f.
Thus we see how a simple recognition of the relevance of the prior, and strict adherence
to the rules of probability theory, can offer a signiﬁcant advantage over a ‘cook-book’
approach to least-squares and error-propagation. This analysis can be extended to the
case of strongly overlapping peaks, which often occurs in powder diffraction; details
can be found in Sivia and David (1994).

4
Model selection
So far, we have been concerned with the problem of parameter estimation. In studying
the linear relationship between two quantities, for example, we discussed how to in-
fer the gradient and intercept of the associated straight line. Often, however, there is a
question as to whether a quadratic, or even cubic, function might be more appropriate.
In this chapter, we go on to consider such cases when there is uncertainty as to which
one of a set of alternative models is most suitable.
4.1
Introduction: the story of Mr A and Mr B
In the analysis of the data in Fig. 3.3, we took the signal peak to be Gaussian in shape
and the background as ﬂat; this was reasonable since the experimental counts were gen-
erated in a computer simulation designed to conform to this description. In real life, of
course, we would have to choose a functional form based on the relevant background
information available. This could include theoretical considerations, the results of cali-
bration measurements or merely an approximation to simplify the algebra; in all cases,
the underlying assumptions need to be stated clearly (in the conditioning on I ). Sup-
pose, however, there was real debate as to whether the signal peak should be Gaussian
or Lorentzian. How can we decide which is better?
The type of question posed above is often called model selection, or model compar-
ison; we will see several different examples of it in this chapter. Na¨ıvely, we might think
that a choice between proposed alternatives can be made on the basis of how well they
ﬁt the data. A little reﬂection soon reveals a potential difﬁculty in that more compli-
cated models, deﬁned by many parameters, will always be able to give better agreement
with the experimental measurements. Although a tenth-order polynomial might yield
a closer match with one-dimensional graphical data than a simple straight line, most
people would prefer the latter unless the discrepancy was very large. Since an analysis
of such problems soon becomes cluttered with algebraic detail, to take into account the
varying degrees of ﬂexibility allowed by the different models, let us begin with an el-
ementary formulation due to Jeffreys (1939); following Gull’s presentation (1988), we
call it the story of Mr A and Mr B.
Mr A has a theory; Mr B also has a theory, but with an adjustable parameter λ.
Whose theory should we prefer on the basis of data D?
Despite its humorous overtones, this represents the bare bones of the model selection
question. In the context of our graph-ﬁtting example, Mr A could be the person who
thinks that the (noisy) measurements of y against x are described by y = 0 ; Mr B
believes that they pertain to y = a, but is not sure about the value of the constant a;
there could also be a Mr C who is willing to allow the possibility of a non-zero slope,

Introduction: the story of Mr A and Mr B
79
so that y = a + b x, and therefore has two adjustable coefﬁcients; and so on. The
easiest comparison is that between Mr A and Mr B, since it involves only one unknown
parameter; the extension to other problems entails a multivariate generalization of the
arguments below, but the basic principles are the same.
From our discussion in Chapter 1, it is clear that we need to evaluate the posterior
probabilities for A and B being correct to ascertain the relative merit of the two theories.
If the ratio of the posteriors,
posterior ratio = prob(A|D, I)
prob(B|D, I) ,
(4.1)
is very much greater than one, then we will prefer A’s theory; if it is very much less
than one, then we prefer that of B; and if it is of order unity, then the current data are
insufﬁcient to make an informed judgement.
To estimate the odds in eqn (4.1), let us start by applying Bayes’ theorem to both
the numerator and the denominator; this gives
prob(A|D, I)
prob(B|D, I) = prob(D|A, I)
prob(D|B, I) × prob(A|I)
prob(B|I) ,
(4.2)
because the term prob(D|I) cancels out, top and bottom. As usual, probability theory
warns us immediately that the answer to our question depends partly on what we thought
about the two theories before the analysis of the data. To be fair, we might take the ratio
of the prior terms, on the far right of eqn (4.2), to be unity; a harsher assignment could
be based on the past track records of the theorists! To assign the probabilities involving
the experimental measurements, prob(D|A, I) and prob(D|B, I), we need to be able
to compare the data with the predictions of A and B: the larger the mismatch, the lower
the corresponding probability. This calculation is straightforward for Mr A, but not for
Mr B; the latter cannot make predictions without a value for λ.
To circumvent this difﬁculty, we can use the sum and product rule to relate the prob-
ability we require to other pdfs which might be easier to assign. In particular, marginal-
ization and the product rule allow us to express prob(D|B, I) as
prob(D|B, I) =

prob(D, λ|B, I) dλ
=

prob(D|λ, B, I) prob(λ|B, I) dλ .
(4.3)
The ﬁrst term in the integral prob(D|λ, B, I), where the value of λ is given, is now just
an ordinary likelihood function; as such, it is on a par with prob(D|A, I). The second
term is B’s prior pdf for λ; the onus is, therefore, on that theorist to articulate his state
of knowledge, or ignorance, before he is given access to the data.
To proceed further analytically, let us make some simplifying approximations. As-
sume that, a priori, Mr B is only prepared to say that λ must lie between the limits λmin
and λmax; we can then na¨ıvely assign a uniform prior within this range:

80
Model selection
prob(λ|B, I) =
1
λmax −λmin
for λmin ⩽λ ⩽λmax ,
(4.4)
and zero otherwise. Let us also take it that there is a value λo which yields the closest
agreement with the measurements; the corresponding probability prob(D|λo, B, I)
will be the maximum of B’s likelihood function. As long as this adjustable parameter
lies in the neighbourhood of the optimal value, λo ± δλ, we would expect a reasonable
ﬁt to the data; this can be represented by the Gaussian pdf
prob(D|λ, B, I) = prob(D|λo, B, I) × exp
	
−(λ −λo)2
2 δλ2

.
(4.5)
The assignments of eqns (4.4) and (4.5) are illustrated in Fig. 4.1. We may note that,
unlike the prior pdf prob(λ|B, I), B’s likelihood function need not be normalized with
respect to λ; in other words, prob(D|λo, B, I) need not equal 1/

δλ
√
2π

. This is
because the λ in prob(D|λ, B, I) appears in the conditioning statement, whereas the
normalization requirement applies to quantities to the left of the ‘ | ’ symbol.
In the evaluation of prob(D|B, I), we can make use of the fact that the prior of eqn
(4.4) does not depend explicitly on λ; this enables us to take prob(λ|B, I) outside the
integral in eqn (4.3):
prob(D|B, I) =
1
λmax −λmin
λmax

λmin
prob(D|λ, B, I) dλ ,
(4.6)
having set the limits according to the speciﬁed range. Assuming that the sharp cut-offs
at λmin and λmax do not cause a signiﬁcant truncation of the Gaussian pdf in eqn (4.5),
its integral will be equal to δλ
√
2π times prob(D|λo, B, I). The troublesome term
then reduces to
Fig. 4.1 A schematic representation of the prior pdf (dashed line) and the likelihood function
(solid line) for the parameter λ in Mr B’s theory.

Introduction: the story of Mr A and Mr B
81
prob(D|B, I) = prob(D|λo, B, I) × δλ
√
2π
λmax −λmin
.
(4.7)
Substituting this into eqn (4.2), we ﬁnally see that the ratio of the posteriors required to
answer our original question decomposes into the product of three terms:
prob(A|D, I)
prob(B|D, I) = prob(A|I)
prob(B|I) ×
prob(D|A, I)
prob(D|λo, B, I) × λmax −λmin
δλ
√
2π
.
(4.8)
The ﬁrst term on the right-hand side reﬂects our relative prior preference for the
alternative theories; to be fair, we can take it to be unity. The second term is a measure
of how well the best predictions from each of the models agree with the data; with the
added ﬂexibility of his adjustable parameter, this maximum likelihood ratio can only
favour B. The goodness-of-ﬁt, however, cannot be the only thing that matters; if it was,
we would always prefer more complicated explanations. Probability theory tells us that
there is, indeed, another term to be considered. As assumed earlier in the evaluation of
the marginal integral of eqn (4.3), the prior range λmax −λmin will generally be much
larger than the uncertainty ±δλ permitted by the data. As such, the ﬁnal term in eqn
(4.8) acts to penalize B for the additional parameter; for this reason, it is often called an
Ockham factor. That is to say, we have naturally encompassed the spirit of Ockham’s
Razor: ‘Frustra ﬁt per plura quod potest ﬁeri per pauciora’ or, in English, ‘it is vain to
do with more what can be done with fewer’.
Although it is satisfying to quantify the everyday guiding principle attributed to
the thirteenth-century Franciscan monk William of Ockham (or Occam, in Latin), that
we should prefer the simplest theory which agrees with the empirical evidence, we
should not get too carried away by it. After all, what do we mean by the simpler theory
if alternative models have the same number of adjustable parameters? In the choice
between Gaussian and Lorentzian peak shapes, for example, both are deﬁned by the
position of the maximum and their width. All that we are obliged to do, and have done,
in addressing such questions is to adhere to the rules of probability.
While accepting the clear logic leading to eqn (4.8), many people rightly worry
about the question of the limits λmin and λmax. Jeffreys (1939) himself was concerned
and pointed out that there would be an inﬁnite penalty for any new parameter if the
range was allowed to go to ±∞. Stated in the abstract, this would appear to be a severe
limitation. In practice, however, it is not generally such a problem: since the analysis
is always used in speciﬁc contexts, a suitable choice can usually be made on the basis
of the relevant background information. Even in uncharted territory, a small amount of
thought soon reveals that our state of ignorance is always far from the ±∞scenario.
If λ was the coupling constant (or strength) for a possible ﬁfth force, for example, then
we could put an upper bound on its magnitude because everybody would have noticed
it by now if it had been large enough! We should also not lose sight of the fact that the
precise form of eqn (4.8) stems from our stated simplifying approximations; if these are
not appropriate, then eqns (4.2) and (4.3) will lead us to a somewhat different formula.
In most cases, our relative preference for A or B is dominated by the goodness of
the ﬁt to the data; that is to say, the maximum likelihood ratio in eqn (4.8) tends to

82
Model selection
overwhelm the contributions of the other two terms. The Ockham factor can play a
crucial rˆole, however, when both theories give comparably good agreement with the
measurements. Indeed, it becomes increasingly important if B’s theory fails to give a
signiﬁcantly better ﬁt as the quality of the data improves. In that case, δλ continues to
become smaller but the ratio of best-ﬁt likelihoods remains close to unity; according to
eqn (4.8), therefore, A’s theory is favoured ever more strongly. By the same token, the
Ockham effect disappears if the data are either few in number, of poor quality or just
fail to shed new light on the problem at hand. This is simply because the posterior ratio
of eqn (4.1) is then roughly equal to the complementary prior one, since the empirical
evidence is very weak; hence, there is no inherent preference for A’s theory unless it
is explicitly encoded in prob(A|I)/prob(B|I). This property can be veriﬁed formally
by going back to eqns (4.2), (4.5) and (4.6), and considering the poor-data limit in which
δλ ≫λmax −λmin and prob(D|λo, B, I) ≈prob(D|A, I).
Some further interesting features arise when we consider the case where Mr A also
has one adjustable parameter; call it µ. If we make the same sort of probability assign-
ments, and simplifying approximations, as for Mr B, then we ﬁnd that
prob(A|D, I)
prob(B|D, I) = prob(A|I)
prob(B|I) × prob(D|µo, A, I)
prob(D|λo, B, I) × δµ (λmax −λmin)
δλ (µmax −µmin) .
(4.9)
This could represent the situation where we have to choose between a Gaussian and
Lorentzian shape for a signal peak, but one associated parameter is not known. The
position of the maximum may be ﬁxed at the origin by theory, for example, and the
amplitude constrained by the normalization of the data; A and B could then be the
hypotheses favouring the alternative lineshapes, where µ and λ are their related full-
width-half-maxima. If we give equal weight to A and B before the analysis, and assign
a similar large prior range for both µ and λ, then eqn (4.9) reduces to
prob(A|D, I)
prob(B|D, I) ≈prob(D|µo, A, I)
prob(D|λo, B, I) × δµ
δλ .
For data of good quality, the dominant factor will tend to be the best-ﬁt likelihood ratio.
If both give comparable agreement with the measurements, however, then the shape
with the larger error-bar for its associated parameter will be favoured. At ﬁrst sight, it
might seem rather odd that the less discriminating theory can gain the upper hand. It
appears less strange once we realize that, in the context of model selection, a larger
‘error-bar’ means that more parameter values are consistent with the given hypothesis;
hence its preferential treatment.
Finally, we can also consider the situation where Mr A and Mr B have the same
physical theory but assign a different prior range for λ (or µ). Although eqn (4.8) can
be seen as representing the case when µmax −µmin is inﬁnitesimally small, so that A
has no ﬂexibility, eqn (4.9) is more appropriate when the limits set by both theorists are
large enough to encompass all the parameter values giving a reasonable ﬁt to the data.
With equal initial weighting towards A and B, the latter reduces to
prob(A|D, I)
prob(B|D, I) = λmax −λmin
µmax −µmin
,

Introduction: the story of Mr A and Mr B
83
because the best-ﬁt likelihood ratio will be unity (since λo = µo) and δλ = δµ. Thus,
our analysis will lead us to prefer the theorist who gives the narrower prior range; this is
not unreasonable as he must have had some additional insight to be able to predict the
value of the parameter more accurately.
4.1.1
Comparison with parameter estimation
The dependence of the result in eqn (4.8) on the prior range λmax −λmin can seem a
little strange, since we haven’t encountered such behaviour in the preceding chapters.
It is instructive, therefore, to compare the model selection analysis with parameter es-
timation. To infer the value of λ from the data, given that B’s theory is correct, we use
Bayes’ theorem:
prob(λ|D, B, I) = prob(D|λ, B, I) × prob(λ|B, I)
prob(D|B, I)
.
(4.10)
The numerator is the familiar product of a prior and likelihood, and the denominator is
usually omitted since it does not depend explicitly on λ; hence this relationship is often
written as a proportionality. From the story of Mr A and Mr B, however, we ﬁnd that
the neglected term on the bottom plays a crucial rˆole in ascertaining the merit of B’s
theory relative to a competing alternative. In recognition of its new-found importance,
the denominator in Bayes’ theorem is sometimes called the ‘evidence’ for B; it is also
referred to as the ‘marginal likelihood’, the ‘global likelihood’ and the ‘prior predictive’.
Since all the components necessary for both parameter estimation and model selection
appear in eqn (4.10), we are not dealing with any new principles; the only thing that sets
them apart is that we are asking different questions of the data.
A simple way to think about the difference between parameter estimation and model
selection is to note that, to a good approximation, the former requires the location
of the maximum of the likelihood function whereas the latter entails the calculation
of its average value. As long as λmin and λmax encompass the signiﬁcant region of
prob(D|λ, B, I) around λo, the precise bounds do not matter for estimating the opti-
mal parameter and need not be speciﬁed. Since the prior range deﬁnes the domain over
which the mean likelihood is computed, due thought is necessary when dealing with
model selection. Indeed, it is precisely this act of comparing ‘average’ likelihoods rather
than ‘maximum’ ones which introduces the desired Ockham balance to the goodness-
of-ﬁt criterion. Any likelihood gain from a better agreement with the data, allowed by
the greater ﬂexibility of a more complicated model, has to be weighed against the addi-
tional cost of averaging it over a larger parameter space.
It is important to remember that the discussion of the rˆole of λmin and λmax in our
illustrative example stems speciﬁcally from the assignment of eqn (4.4). In practice, a
more suitable prior might be
prob(λ|B, I) = e−λ/b
b
,
where b is an initial estimate of the magnitude of the parameter λ, with 0<λ<∞, but
the probabilistic evidence for Mr. B’s theory is still given by the prior-weighted average

84
Model selection
of his likelihood as in eqn (4.3). For clarity of exposition, however, we will continue to
use eqn (4.4) and its multivariate generalization.
4.1.2
Hypothesis testing
The phrasing of the story of Mr A and Mr B suggests that we are dealing with the
problem conventionally called hypothesis testing. Although this is usually treated as a
separate topic in the orthodox literature, our purpose is to emphasize that the principles
required are no different from those used in parameter estimation: it’s all just a matter
of applying the sum and product rules of probability theory.
Suppose we have a hypothesis H1: the shape of the signal peak is Gaussian, for
example. To quantify how much we believe that this proposition is true, based on the
data D and all the relevant background information I, we need to evaluate the posterior
probability prob(H1|D, I); to help us do this, we can use Bayes’ theorem:
prob(H1|D, I) = prob(D|H1, I) × prob(H1|I)
prob(D|I)
.
(4.11)
The numerator is simply the product of the prior and the likelihood for H1, but we
also require prob(D|I) to put the posterior probability on a meaningful scale. The
denominator can be ignored, of course, if we are only interested in the relative merit of
H1 compared to another hypothesis H2; then, applying Bayes’ theorem to the second
proposition, and dividing by the expression above, we obtain the odds-ratio:
prob(H1|D, I)
prob(H2|D, I) = prob(D|H1, I)
prob(D|H2, I) × prob(H1|I)
prob(H2|I) ,
which is the same as eqn (4.2), with H1 = A and H2 = B. To assess the intrinsic truth
of H1, we could think of letting H2 be the hypothesis that H1 is false: H2 =H1. Using
marginalization and the product rule, we would then write prob(D|I) as
prob(D|I) = prob(D|H1, I)×prob(H1|I) + prob

D|H1, I

×prob

H1 |I

,
where the two priors would be related by the sum rule of eqn (1.1):
prob(H1|I) + prob

H1 |I

= 1 .
In conjunction with eqn (4.11), we see that the difﬁcult term is the likelihood function
for H1: prob

D|H1, I

. This is because, in general, we cannot compare the predictions
of the hypothesis with the data given only that H1 is false: we need well-deﬁned alter-
natives! If the peak shape is not Gaussian, for example, could it be Lorentzian? With a
speciﬁc set of possibilities, {Hj}, the problem boils down to one of model selection —
simply compare their evidences, prob(D|Hj, I).
It is often said that a hypothesis should be rejected if it gives a poor ﬁt to the data,
even though agreement does not assure its truth. As such, traditional hypothesis testing
often involves the use of procedures designed to asses the signiﬁcance of the mismatch
between theory and experiment; many entail the χ2 statistic. As shown by eqn (3.83), it

Example 6: how many lines are there?
85
is certainly true that if we know the nature of the object of interest, and the related data
are subject to independent Gaussian noise, then the expected value of χ2 will be about
equal to the number of measurements N; we would be quite surprised by deviations
of more than a few times
√
N. Nevertheless, it is a bold step to go from this statement
to rejecting a hypothesis because χ2 is too large. The point is that the misﬁt statistic
is a measure of the likelihood function prob(data|hypothesis, I); to reject (or accept)
a theory, however, we need the posterior probability prob(hypothesis|data, I). Al-
though a larger value of χ2 will give a smaller likelihood for the data, it is only one
of the ingredients which makes up the posterior: we also need prob(data|I), as well
as an assignment for the prior. Despite the conventional practice of quoting P-values
for assessing the level of rejection of a hypothesis, the basis for this formal quantiﬁca-
tion seems rather doubtful. The misﬁt statistic can serve a useful (qualitative) purpose,
nonetheless, if a poor quality-of-ﬁt prompts us to think about alternative hypotheses;
probability theory then provides us with the tools for quantitatively choosing which one
is best.
Even if we could come up with a convincing argument to disregard a theory, the
job is only half-done until we have something better to take its place. To quote Jeffreys
(1939, Section 7.2.2), ‘If there is no clearly stated alternative, and the null hypothesis
is rejected, we are left without any rule at all, whereas the null hypothesis, though not
satisfactory, may at any rate show some sort of correspondence with the facts.’ He points
out that, while there was never a time when Newton’s theory of gravity would not have
failed a P-test, ‘The success of Newton was not that he explained all the variation of the
observed positions of the planets, but that he explained most of it.’
4.2
Example 6: how many lines are there?
To illustrate the practical use of the analysis discussed above, let us consider a case
which occurs frequently in spectroscopy, crystallography and many other areas of sci-
ence: namely, the question of assessing how many signal peaks there is most evidence
for in a pertinent set of data. Here we are primarily concerned with inferring the ampli-
tudes and positions of a set of a few discrete excitation lines, but are not sure as to their
exact number.
For simplicity, let’s assume that we are dealing with a one-dimensional problem
where the shape of the signal peaks f(x) is known; then, the ideal spectrum G(x) can
be expressed as:
G(x) =
M

j=1
Aj f(x, xj) ,
(4.12)
where Aj is the magnitude of the jth line and xj represents its location. If all the exci-
tations were Gaussian with width W, for example, then
f(x, xj) = exp
	
−

x −xj
2
2 W 2

.
The situation is shown in Fig. 4.2(a). Most experimental set-ups give rise to a blurred
and noisy version of the spectrum in eqn (4.12), and tend to entail a slowly varying

86
Model selection
Fig. 4.2 (a) An ideal spectrum consisting of the sum of a few signal peaks; in the model selection
problem, we’re not sure as to their number. (b) The corresponding data are usually a blurred and
noisy version of (a), and are often corrupted by a slowly varying background (dotted line).
background signal B(x); such measurements are illustrated in Fig. 4.2(b). In this case,
the ideal data {Fk} are related to the parameters {Aj, xj} by
Fk =

G(x) R(xk−x) dx + B(xk) ,
(4.13)
where we have implicitly assumed that the shape of the resolution function R(x) does
not vary with position in writing the blurring process as a convolution integral. If we
make the approximation that the experimental measurements {Dk} are subject to in-
dependent additive Gaussian noise {σk}, then we obtain the least-squares likelihood
function of Section 3.5:
prob

{Dk}|{Aj, xj}, M, I

∝exp

−χ2
2

,
(4.14)
where χ2 is deﬁned in eqn (3.66) and the normalization factor, involving the product of
terms like σk
√
2π, has been omitted since its value is independent of the parameters
to be inferred. Note that we have stated the conditioning on the number of excitation
lines M explicitly, whereas R, B and {σk} have been subsumed in I. This is because
its value is not known, but is of interest to us: hence, it’s a model selection problem.
To estimate the number of signal peaks, we need to evaluate the posterior pdf for
M, prob(M |{Dk}, I). As always, it’s best to start with Bayes’ theorem:
prob(M |{Dk}, I) = prob({Dk}|M, I) × prob(M |I)
prob({Dk}|I)
.
If we assign a simple uniform prior for M = 1, 2, . . ., up to a few (say 20), then the
posterior becomes proportional to the evidence (or marginal likelihood):
prob(M |{Dk}, I) ∝prob({Dk}|M, I) ,
(4.15)
where the omitted normalization constant can be evaluated at the end, if required, from
the condition  prob(M |{Dk}, I) = 1. As in the case of Mr B in the previous section,

Example 6: how many lines are there?
87
the evidence can be expressed as a marginal integral over the product of the prior and
likelihood function for the parameters of the M-line model:
prob({Dk}|M, I) =

· · ·

prob

{Dk}, {Aj, xj}|M, I

dMAj dMxj ,
(4.16)
where
prob

{Dk}, {Aj, xj}|M, I

= prob

{Dk}|{Aj, xj}, M, I

prob

{Aj, xj}|M, I

.
Since we’ve already settled on a least-squares likelihood in eqn (4.14), all we need
now is an assignment for the prior. In keeping with our policy of na¨ıvet´e, let us take this
to be a simple uniform pdf within a suitably bounded region:
xmin ⩽xj ⩽xmax
and
0 ⩽Aj ⩽Amax ,
(4.17)
and zero otherwise. This means that, to be properly normalized, the prior is given by the
reciprocal of the volume of the hypercube deﬁned by eqn (4.17):
prob

{Aj, xj}|M, I

=

(xmax −xmin) Amax
−M ,
(4.18)
and is just a multidimensional generalization of Mr B’s prior for λ in eqn (4.4). The
parameters xmin and xmax can be set equal to the x-limits over which the data have
been measured; if we had thought that the lines of interest could lie outside this range,
we would presumably have conducted a different experiment. If we were not already
given it, we could take an upper bound for the amplitudes of the peaks Amax from the
integrated intensity of the data; we’d effectively be making use of the known product of
the overall experimental count-rate and the length of time for which the measurements
were made.
Substituting the product of the likelihood and prior of eqns (4.14) and (4.18) into
eqn (4.16), we see that the posterior in eqn (4.15) is given by
prob(M |{Dk}, I) ∝

(xmax −xmin)Amax
−M
· · ·

exp

−χ2
2

dMAj dMxj ,
(4.19)
where the multiple integral is over the region deﬁned by eqn (4.17). Since the pdf
exp(−χ2/2) can have a very complicated multimodal topology, it is safest to carry out
this integration numerically with a Monte Carlo algorithm. Alternatively, we can pursue
the calculation further analytically by making some gross simplifying approximations.
Suppose that there is a set of 2M parameters Xo = {Aoj, xoj} which yield the best
least-squares ﬁt to the data χ2
min; then, a quadratic Taylor series expansion about this
point gives
χ2 ≈χ2
min + 1
2

X−Xo
T ∇∇χ2(Xo)

X−Xo

+ · · · ,
as in eqn (3.30). Assuming that the signiﬁcant portion of the pdf exp(−χ2/2), around
the maximum at Xo, lies within the region permitted by eqn (4.17), the contribution

88
Model selection
of this solution towards eqn (4.19) is equal to exp(−χ2
min/2) times the integral of the
related 2M-dimensional multivariate Gaussian:

· · ·

exp

−1
4

X−Xo
T ∇∇χ2(Xo)

X−Xo

d2MXj =
(4π)M

det(∇∇χ2)
,
where det

∇∇χ2
is the determinant of the Hessian matrix, evaluated at Xo. As the
labelling of the M lines in our model of the spectrum is arbitrary (which one we call 3
and which one 4 etc.), there will be M! equivalent maxima in the likelihood function;
this is just the number of ways we can permute the indices associated with the various
signal peaks. Hence, we can approximate the posterior pdf of eqn (4.19) by
prob(M |{Dk}, I) ∝
M! (4π)M

(xmax−xmin)Amax
M
det(∇∇χ2)
exp

−χ2
min
2

.
(4.20)
As mentioned earlier, the omitted constant of proportionality can be determined from
the normalization requirement:  prob(M |{Dk}, I) = 1.
Although eqn (4.20) looks rather horrendous, there is a close correspondence with
the decomposition of eqn (4.8) in the analysis of Mr A and Mr B. The prior term
prob(M |I) is missing because we originally had no (strong) preference for any par-
ticular number of lines; therefore, it has been absorbed into the normalization constant.
With the assumption that the data are subject to independent Gaussian noise, the best-ﬁt
likelihood is proportional to exp(−χ2
min/2). The remaining terms constitute the mul-
tivariate equivalent of the Ockham factor: they are the ratio of the 2M-dimensional
hypervolumes, in the parameter space {Aj , xj}, permitted by the prior and the poste-
rior. We should emphasize that the speciﬁc form of eqn (4.20) relies on the validity of
our simplifying approximations. They will tend to be reasonable in our stated (prior)
r´egime of small M, but we must return to a more robust evaluation of the multiple inte-
gral in eqn (4.19) if they are not adequate. Indeed, if the assignment of the least-squares
likelihood, or the uniform prior, is not appropriate, then we have to take a step further
back to eqn (4.16).
In the above analysis, we have implicitly assumed (in I) that the exact shape of the
signal peak, and the background, is known. This is often the case, as they are usually
determined fairly accurately from calibration experiments. If we are not quite so fortu-
nate, however, then we must characterize their nature by a small number of parameters
and marginalize over them. For example, a slowly varying background can be approx-
imated by a straight line if the x-range of the data is not too large; B(x) is, therefore,
deﬁned by just two unknown variables b1 and b2. The conditioning of the pdfs on these
parameters must now be stated explicitly, so that eqn (4.16) becomes
prob({Dk}|M, I) =

· · ·

prob

{Dk}, {Aj, xj}, b1, b2|M, I

dMAj dMxj db1db2.
Proceeding as before, we will be led to the formula in eqn (4.20); the only difference
is that b1 and b2 will affect both the best-ﬁt possible χ2
min and contribute towards the

Example 6: how many lines are there?
89
determinant of the ∇∇χ2 matrix (now 2M +2 square). Since the prior pdf for b1 and
b2 does not depend on the number of lines, prob(b1, b2|M, I) = prob(b1, b2|I), its
normalization factor can be incorporated into the existing constant of proportionality.
Uncertainty about the width of excitation lines, or the size of the error-bars for the
data, or any other systematic effect, can be dealt with in the same way through marginal-
ization; the relevant integrations can be done by optimization, within the quadratic ap-
proximation. If there is debate as to whether the shape of the signal peaks is Gaussian
(G) or Lorentzian (L), for example, then we can compare their evidence:
prob({Dk}|G, I)
prob({Dk}|L, I) =

prob({Dk}|W, G, I) prob(W |G, I) dW

prob({Dk}|W, L, I) prob(W |L, I) dW ,
where we have assumed that all the lines have the same, but unknown, width W. If
we take the priors prob(W |G, I) and prob(W |L, I) to be uniform over a comparable
range (Wmin to Wmax), then these terms will cancel. The remaining pdfs on the right
can be expressed as marginal distributions over the number of lines:
prob({Dk}|W, G, I) =

M
prob({Dk}|M, W, G, I) prob(M |W, G, I) ,
and there will be an identical equation with L’s instead of G’s. Since we don’t expect a
knowledge of the line shape to inﬂuence what we can say about M before the analysis
of the data, we can assign prob(M |W, G, I) = prob(M |W, L, I) = prob(M |I). As
the order of an integration and summation can be interchanged, we ﬁnd that the required
probability is given by
prob({Dk}|G, I) =
few

M=1
Wmax

Wmin
prob({Dk}|M, W, G, I) dW ,
where the integrand is just the evidence for the number of lines calculated earlier, when
we were given the exact shape of the peaks. Additionally, we could evaluate the evidence
for M by marginalizing over the two candidate line shapes:
prob({Dk}|M, I) = prob({Dk}|G, M, I) × prob(G|M, I)
+ prob({Dk}|L, M, I) × prob(L|M, I) ,
and so on.
4.2.1
An algorithm
In order to make use of the preceding analysis, we need an algorithm for its practical
implementation. To this end, we will work with the approximation of eqn (4.20). This
formula for calculating the posterior probability for the number of peaks requires us to
ﬁnd the value of χ2
min, and the determinant of ∇∇χ2 (evaluated at that optimal point
in parameter space), for an M-line model. Both quantities should be readily available in
standard least-squares programs because the minimum value of χ2 deﬁnes the best-ﬁt

90
Model selection
parameters and, from eqn (3.71), twice the inverse of ∇∇χ2 gives an estimate of their
reliability. Thus we have another example where a basic understanding of probability
theory allows us to extract more potency from a widely-used procedure than a cook-
book approach to least-squares would have suggested was possible. The main difference
is that we must now give some thought to a suitable range for the uniform prior for the
model variables, whereas previously it had been sufﬁcient to just say that it was large.
The most difﬁcult task for using eqn (4.20) is ﬁnding the set of parameters {Aj, xj}
(and possibly b1, b2 and W) which yield the best-ﬁt χ2
min. This is because the positions
of the peaks, unlike their amplitudes, are not linearly related to the data and so χ2
can be multimodal; as discussed in Section 3.4, it leads to a (potentially) hard problem
in multidimensional optimization. Most least-squares programs avoid this difﬁculty by
shifting the onus on to the user to provide an initial guess which is good enough to
reﬁne to (what is hopefully) the global minimum. As a somewhat ambitious alternative,
we outline below an algorithm based on the combination of one-dimensional ‘brute
force and ignorance’ searches and linearized multidimensional optimization. Although
far from being foolproof, it’s computationally efﬁcient and has been used with a fair
amount of success.
Let us start by considering the range parameters xmin , xmax and Amax . Ideally,
these should be determined from previous measurements, or theory, since they reﬂect
our background information; in practice, a reasonable estimate can be obtained from a
cursory look at the current data. As mentioned earlier, xmin and xmax can be set equal
to the limits of the x-range over which the data have been measured. If the candidate
signal peaks f(x) in eqn (4.12), and the resolution function R(x) in eqn (4.13), have
been normalized with respect to x, then the mathematical properties of a convolution en-
sure that the integrated intensity of the data provides an upper bound for the amplitudes:
Amax ≈ Dk.
To ﬁnd the best-ﬁt solution for any speciﬁed number of lines, let’s begin with the
case M = 0 and work upwards. If there are no signal peaks, then the data are simply
described by a background B(x). Assuming that this varies sufﬁciently slowly, it can
be taken to be linear over the range xmin to xmax: B(x) = b1x + b2. It is then easy
to reﬁne b1 and b2 to obtain both χ2
min and the determinant of the 2×2 ∇∇χ2 matrix;
prob(M = 0|{Dk}, I) is just proportional to exp

−χ2
min/2

/

det(∇∇χ2) . Next,
we need the best-ﬁt parameters for a one-line model. To circumvent the problem of non-
linearity associated with the position x1, we can conduct an explicit search for its value:
that is, we divide the x-range into a couple of hundred discrete points and optimize
the linear parameters A1, b1 and b2 as we step through the ﬁnite set of possibilities
for x1. Taking the values which yield the smallest χ2 as a good initial guess, we can
carry out a four-dimensional search for the best-ﬁt solution for the one-line model. To
improve robustness, we can use a simplex routine before completing the reﬁnement
with Newton–Raphson. The former has the additional advantage that, since it works
directly with χ2, it allows for the encoding of the sharp bounds of the uniform prior in
eqn (4.17); nevertheless, the efﬁciency of Newton–Raphson ensures that it is a useful
last step. In any case, we need the gradient information in ∇∇χ2 to estimate both the
posterior probability for the one-line model and the error-bars for its parameters; since

Example 6: how many lines are there?
91
analytical differentiation with respect to x1 is awkward, ﬁnite differences can be used
to obtain the derivatives.
The rest of the algorithm is essentially a repetition of this recipe. For the second
line, we ﬁrst carry out an explicit search for x2 on a one-dimensional grid; during this
procedure, we can reﬁne all the linear parameters (such as A1, A2, b1 and b2) but it is
best to hold the others (like x1) ﬁxed at their previous values. This is then followed by a
simultaneous optimization of all the parameters of the two-line model, with simplex and
Newton–Raphson. Having evaluated χ2 and ∇∇χ2 at the optimal solution, we can cal-
culate the posterior prob(M = 2|{Dk}, I) from eqn (4.20). This process of combining
one-dimensional brute force and ignorance searches with linearized multidimensional
reﬁnement can be continued until we reach the upper limit of our prior prob(M |I).
Before illustrating this algorithm with a couple of examples, we should make some
additional remarks. It is helpful to use the prior ranges Amax and xmax−xmin (and even
bmax, for the background) to scale the parameters to be optimized. This is because then
all the variables, such as Aj/Amax, will be dimensionless and of comparable magnitude.
Another advantage of working in this scaled space is that we can improve the stability of
the matrix calculations by adding a small multiple of the identity matrix to the Hessian:
∇∇χ2 →∇∇χ2 + c I. As shown in eqn (3.58), this has no effect on the eigenvectors
of the Hessian and so does not change the pattern of correlations between the inferred
parameters. It does, however, put a lower bound on the eigenvalues (⩾c), thereby en-
coding our prior knowledge that the error-bar for any parameter cannot be larger than
the prior range assigned to it. Finally, if the width W of the peaks is not known then it
too must be marginalized like the background. Because of its highly non-linear nature,
it is often best done by running the program several times over using different given
widths. We can then plot the two-dimensional posterior pdf for the number of lines and
their width, and integrate with respect to W to obtain prob(M |{Dk}, I). If desired, we
can sum over M to yield prob(W |{Dk}, I).
4.2.2
Simulated data
Let us begin our illustration of model selection with the aid of data generated in a
computer simulation. The measurements are shown in Fig. 4.3(a) and result from the
convolution of a spectrum consisting of the sum of a ‘few’ excitation lines, all having
the same Gaussian proﬁle, with a Gaussian resolution function of FWHM 2.0 µeV; they
are also corrupted by a linear background and (Poisson) noise. Given this information
alone, how many lines is there most evidence for in the data?
Carrying out the analysis with the algorithm described above, we obtain the poste-
rior pdf for M indicated by the triangles in Fig. 4.3(b). Note that, to highlight its shape,
the pdf has been plotted on a logarithmic scale and a continuous solid line drawn be-
tween the discrete points. The position of the maximum indicates that there is most evi-
dence for two signal peaks. Their width W (FWHM) is estimated to be 1.03 ± 0.08µeV,
with locations at 13.98±0.03µeV and 15.47±0.02µeV. The spectrum used to generate
the data in Fig. 4.3(a) did indeed contain two lines, with a FWHM of 1.0µeV, centred at
14.0 and 15.5 µeV. The amplitudes of both were the same and were inferred correctly
to within 5%.

92
Model selection
Fig. 4.3 (a) Simulated data from a spectrum of a few excitation lines; the width of the instru-
mental resolution function is 2.0 µeV. (b) The triangles, and solid line, mark the corresponding
posterior probability for the number of lines M. The dashed line shows the result for data of
poorer quality, with this shortfall supplemented by increased prior knowledge in the analysis giv-
ing the dotted line.
The shape of the posterior pdf for M is characteristic of this type of model selection
analysis: (i) there is a sharp fall-off from the maximum on the left, because there is not
enough structure in the proposed spectrum to adequately account for the data; (ii) there
is a slow decline on the right, as the models become increasingly, and unnecessarily,
complicated. As discussed in Section 4.1, we have captured the essence of Ockham’s
Razor: our best estimate of the spectrum is the one with the least number of lines that is
consistent with the experimental measurements. Although this is exactly what we would
have done using our ‘common sense’, the value of the analysis is that it sharpens and
reﬁnes it far beyond our qualitative intuition. It would otherwise be difﬁcult for us to
state that the two-line model was ten orders of magnitude more probable than the one-
line alternative, or that the empirical evidence for two peaks was thirty times greater
than that for three.
We should emphasize, however, that the conclusions are always conditional on both
the quality of the data and our prior knowledge. For example, if the error-bars for the
data of Fig. 4.3(a) had been three times larger (e.g. experiment run for only one-tenth of
the time), we would have obtained the posterior pdf marked by the squares, and dashed
line, in Fig. 4.3(b). There is then most evidence for only one line, at ∼14.8 µeV, with
a FWHM of about 2 µeV, although two could not be ruled out at the 90% conﬁdence
level. This is because the poorer measurements can be more simply, but sufﬁciently,
explained by a broader single peak than with two narrower ones. Indeed, the maximum
of the posterior would occur at M =0 if the error-bars were so large that the data could
be ﬁt adequately by just a linear background! In that case the pdf prob(M |{data}, I)
would also be very ﬂat, closely resembling the prior prob(M |I); consequently, proba-
bility theory would be warning us that it was unwise to make too decisive a judgement
based on such ﬂimsy measurements. Even with the poorer data, with three times more
noise than in Fig. 4.3(a), we would still ﬁnd most evidence for two lines if it was al-
ready known that the peaks comprising the spectrum had a FWHM of 1.0 µeV; the
corresponding posterior is shown by the circles, and dotted line, in Fig. 4.3(b).

Example 6: how many lines are there?
93
4.2.3
Real data
An interesting series of illustrations of the (real-life) use of the theory and algorithm
outlined above, as applied to molecular tunnelling spectroscopy with neutron scattering
data, can be found in Sivia and Carlile (1992). Here we present just one example from
crystallography: the analysis of part of the X-ray diffraction pattern from a powdered
sample of a zeolite. The experimental data were collected at the synchrotron source
at Daresbury, U.K., and are shown in Fig. 4.4(a). The shape of the Bragg peaks, in
Fig. 4.4(b), is known from calibration measurements; it does not vary with scattering
angle θ. It is the effective resolution function R(θ) for the experiment and includes the
contribution from both the intrinsic width of the lines and the instrumental blurring.
The ﬁrst step in solving the structure of a crystal involves the determination of the
size of the unit cell and its space-group symmetry. To tackle both these questions, we
need to estimate the positions and amplitudes of the Bragg lines; before we can do that,
however, we need to know how many there are. Carrying out the analysis as described
earlier, we obtain the posterior pdf for the number of Bragg peaks given in Fig. 4.4(c).
There is most evidence for ﬁve lines, therefore, and the optimal estimates of their am-
plitudes {Aj} and positions {θj} are shown in Fig. 4.4(d); the (1-σ) error-bars for these
parameters are also indicated, but the horizontal discrepancies for the locations are too
Fig. 4.4 (a) Part of the X-ray diffraction data from a zeolite. (b) The calibrated proﬁle, or resolu-
tion function, of the Bragg peaks. (c) The logarithm of the posterior pdf for the number of lines.
(d) The inferred amplitudes and positions of the Bragg peaks, and their estimated error-bars.

94
Model selection
small to be seen. The ability to infer, with a high degree of conﬁdence, that one of the
visible peaks is a doublet, whereas the others are single, can be crucial to the correct as-
signment of the space-group and thence the successful solution of the crystal structure.
4.3
Other examples: means, variance, dating and so on
The model selection procedure described in this chapter is applicable to many prob-
lems in data analysis. Bretthorst (1988, 1990), for example, has been using it in NMR
spectroscopy; atmospheric variations of 14C were analysed by Sonett (1990); and Gull
(1988) has addressed the perennial question of the optimal expansion-order (of a poly-
nomial function) for ﬁtting graphical data. Let us conclude this discussion with some
further illustrations.
4.3.1
The analysis of means and variance
A common problem in science is one of classiﬁcation. An archaeologist, for example,
might have found two sets of humanoid skeletons whose ages differ by a million years.
A question which could arise is whether variations in physical features, such as their
brain size, show evidence for signiﬁcant evolutionary change. Suppose that the two sites
yield N1 and N2 measurements, represented by data-vectors D1 and D2, respectively;
we could then consider the relative merit of the following hypotheses:
A: there is no change over this period of time, and so both sets of data can be charac-
terized by the same (unknown) mean µ and standard deviation σ.
B: there is a change, with the two sites having (unknown) means µ1 and µ2 , and
standard deviations σ1 and σ2, respectively.
In principle, there is also the possibility that the mean remains the same while the stan-
dard deviation changes and vice versa; unless we have good reason to expect such pecu-
liar one-sided evolution, however, we would tend to assign a fairly low prior probability
to these propositions. Thus, it will generally be adequate to consider only the case of A
and B above; a more complete analysis, and an outline of the history of the problem,
can be found in Bretthorst (1993). Essentially, we are saying that our task here is one of
deciding whether all the skulls belong to one class or two distinctly different ones.
In Section 4.1, we saw that the data-dependent term in model selection was the
evidence; therefore, we need to evaluate prob(D1, D2|A, I) and prob(D1, D2|B, I).
Beginning with the former, marginalization and the product rule allow us to write
prob(D1, D2|A, I) =

prob(D1, D2|µ, σ, A, I) prob(µ, σ|A, I) dµ dσ, (4.21)
For the prior for A’s parameters, we can assign a simple uniform pdf in the region
µmin ⩽µ ⩽µmax and 0 ⩽σ ⩽σmax ,
prob(µ, σ|A, I) =

(µmax −µmin) σmax
−1,
(4.22)
and zero otherwise; we could even let µmin =0 and µmax =σmax, and set them equal
to the maximum extent of the measuring equipment. Since we believe that the variations

Other examples: means, variance, dating and so on
95
in the brain size of the specimens can be characterized by just a mean and a standard
deviation, it is reasonable to assign a Gaussian pdf for the likelihood function; we will
formally see why this is so in Chapter 5. Treating the data as a single set of N =N1+N2
independent measurements {xk}, as in eqn (3.35), we have
prob(D1, D2|µ, σ, A, I) =

σ
√
2π
−N
exp

−
1
2 σ2
N

k=1
(xk−µ)2

.
(4.23)
The double integral of eqn (4.21) can be calculated numerically, by evaluating the
pdf of eqn (4.23) on a rectangular grid deﬁned by the prior of eqn (4.22) and summing
the contributions. It can also be approximated analytically, with a quadratic Taylor series
expansion of the logarithm of A’s likelihood function:
L = L (µo, σo) −1
2

µ−µo
σ−σo

α
γ
γ
β

µ−µo
σ−σo

+ · · · ,
(4.24)
where L = log e[prob(D1, D2|µ, σ, A, I)], with a maximum at (µo, σo). Accordingly,
the parameters µo and σo are determined by the conditions ∂L/∂µ=0 and ∂L/∂σ=0:
µo = 1
N
N

k=1
xk
and
σo2 = 1
N
N

k=1
(xk−µo)2 ,
(4.25)
and the elements of the 2×2 matrix are given by the second partial derivatives of L,
evaluated at µo and σo:
α = N/σo2 ,
β = 2 N/σo2 ,
γ = 0 .
(4.26)
Exponentiating the log-likelihood of eqn (4.24), and using the prior of eqn (4.22), we
see that eqn (4.21) can be approximated by the product of
exp

L(µo, σo)

(µmax−µmin) σmax
= prob(D1, D2|µo, σo, A, I)
(µmax−µmin) σmax
,
(4.27)
and the integral of a bivariate Gaussian:
µmax

µmin
σmax

0
exp

−1
2

α (µ−µo)2 + β (σ−σo)2
dµ dσ ≈
2π
√αβ .
(4.28)
Substituting for the parameters α, β, µo and σo from eqns (4.25) and (4.26) into eqns
(4.23), (4.27) and (4.28), the evidence for hypothesis A reduces to
prob(D1, D2|A, I) ≈

σo
√
2π
2−N exp(−N/2)
(µmax−µmin) σmax N
√
2 .
(4.29)

96
Model selection
The other quantity we need for our classiﬁcation problem is prob(D1, D2|B, I).
Since hypothesis B asserts that the data-sets D1 and D2 pertain to distinctly differ-
ent classes, knowledge of one tells us nothing about the other; in conjunction with the
product rule of probability, this independence takes the form
prob(D1, D2|B, I) = prob(D1|B, I) × prob(D2|B, I) .
(4.30)
Both terms on the right-hand side can be written as marginal integrals like eqn (4.21):
prob(Dj|B, I) =

prob

Dj |µj, σj, B, I

prob

µj, σj|B, I

dµj dσj ,
where j = 1, 2. The priors for the two means and standard deviations can each be set
equal to eqn (4.22), since we suspect they might even be the same; the likelihoods for
D1 and D2 will be similar to eqn (4.23), with the relevant summation over the respective
N1 and N2 data. A quadratic Taylor series expansion then allows us to approximate the
probabilities on the right of eqn (4.30) by expressions like eqn (4.29):
prob(Dj |B, I) ≈

σoj
√
2π
2−Nj exp(−Nj/2)
(µmax−µmin) σmax Nj
√
2
,
where σo1 and σo2 are given by the appropriate summations in eqn (4.25), over D1 and
D2 respectively. Finally, dividing eqn (4.29) by the resultant product for the evidence
for hypothesis B, we obtain
prob(D1, D2|A, I)
prob(D1, D2|B, I) ≈(µmax−µmin)σmax
π
√
2
×
N1 N2 (σo)2−N
N (σo1)2−N1 (σo2)2−N2 .
(4.31)
Since Bayes’ theorem tells us that the ratio of the posterior probabilities for A and B is
given by eqn (4.31) times prob(A|I)/prob(B|I), which is usually taken as ∼1, this
formula provides a useful analytical solution to our classiﬁcation problem; it should be
fairly accurate as long as we have a reasonable amount of data (N1 ≫1 and N2 ≫1).
The preceding analysis can be generalized to the case when there are several sets of
data {Dj}, with j =1, 2, . . ., M. The evidence that all the measurements (N =  Nj)
belong to the same classiﬁcation-group is given by a double integral, like eqn (4.21),
where the joint pdf prob({Dj}, µ, σ|A, I) is marginalized with respect to the unknown
mean µ and standard deviation σ . The corresponding probability that each data-set
relates to a different class takes the form of a product of the terms prob(Dj|B, I),
which is just an extension of eqn (4.30):
prob({Dj}|B, I) =
M

j=1
prob(Dj|B, I) .
Making the same simplifying approximations as before, eqn (4.31) becomes:
prob({Dj}|A, I)
prob({Dj}|B, I) ≈
(µmax−µmin)σmax
π
√
2
M−1
× (σo)2−N
N
×
M

j=1
Nj
(σoj)2−Nj ,
where σoj is the standard deviation, in eqn (4.25), of the Nj measurements Dj.

Other examples: means, variance, dating and so on
97
Returning to the case of just two data-sets, there is an important alternative ques-
tion which we could ask. Suppose, for example, that we have to choose between two
manufacturers of light bulbs on the basis of samples provided by them. Although we’re
interested in assessing the signiﬁcance of the difference between their expected life-
times (per unit cost), it’s not really a classiﬁcation problem. We already know that the
products come from competing factories and, hence, will not have the same mean and
standard deviation; the question is whether bulbs from one will last longer then those
from the other. Thus, we need the posterior probability of the hypothesis µ1 > µ2; this
can be evaluated from the joint pdf for the two means:
prob(µ1>µ2|D1, D2, I) =
∞

0
dµ1
µ1

0
dµ2 prob(µ1, µ2|D1, D2, I) .
(4.32)
If this probability was very close to unity, then we would buy from manufacturer 1;
if it was almost zero, then the competitor would be better; a value of order 0.5 would
indicate no strong preference for either.
Since a knowledge of the expected lifetime from one factory does not tell us much
about the average performance of the other, the product rule of probability allows us to
write the integrand in eqn (4.32) as
prob(µ1, µ2|D1, D2, I) = prob(µ1|D1, I) × prob(µ2|D2, I) .
(4.33)
From our formulation of the problem in terms of means and standard deviations, the
two pdfs on the right are just the marginal integrals of prob

µj, σj |Dj, I

with respect
to σj; according to the analysis in Section 3.3, they yield the Student-t distribution of
eqn (3.43). Substituting these into eqn (4.33), the double integral of eqn (4.32) can then
be calculated numerically. If the number of samples provided is moderately large, the
Student-t distributions will tend towards Gaussians, so that the joint pdf for the means
can be approximated by
prob(µ1, µ2|D1, D2, I) ≈
√N1N2
2πS1S2
exp

−1
2
	
N1(µ1−µo1)2
S 2
1
+ N2 (µ2−µo2)2
S 2
2


,
where µo1, µo2, S1 and S2 are given by the appropriate summations over D1 and D2,
respectively, in eqns (3.39) and (3.40). With a change of variables to Z = µ1−µ2 , as
discussed in Section 3.6, the integral of eqn (4.32) reduces to
prob(µ1>µ2|D1, D2, I) ≈
1
Sz
√
2π
∞

0
exp
	
−(Z−zo)2
2 Sz2

dZ ,
(4.34)
where zo = µo1 −µo2 and Sz2 = S 2
1 /N1 + S 2
2 /N2. Thus, the integral properties of the
Gaussian pdf tells us that the required probability will be about 0.84 if the difference
between the two sample means zo equals Sz, almost 0.98 for zo = 2 Sz, and so on; it
will be one minus these numbers if zo is negative, but of the same magnitude.

98
Model selection
It is interesting to note that eqn (4.34), unlike eqn (4.31), does not have an explicit
dependence on the prior range µmax−µmin and σmax. In fact, we’ve already seen this
type of behaviour in Section 4.1. In the story of Mr A and Mr B, the posterior pdf for the
parameter λ, prob(λ|D, B, I), is unaffected by the choice of λmin and λmax as long as
they don’t cause a signiﬁcant truncation of the likelihood function; even if this is so, the
prior range still plays an important rˆole in the assessment of the relative merit of the two
theories. The point is that in eqns (4.31) and (4.34) we are asking different questions of
the data; we should not be surprised, therefore, if the answers are not the same.
Before moving on, let’s consider how the analysis would have to be modiﬁed if the
data {xk} were subject to uncertainties described by error-bars {ϵk}. These are often
neglected because they are small compared with the underlying variation deﬁned by σ;
for example, whereas the lifetime of a particular light bulb can be measured to a frac-
tion of a second, it can differ from others within the sample by many hours. For the case
of our intrepid archaeologist, however, the estimation of the brain size from skull frag-
ments will be a less precise operation. This can be taken into account by expressing the
likelihood function for the datum xk as a marginal integral over the exact, but unknown,
value of the sample ˆxk:
prob(xk|ϵk, µ, σ, I) =

prob(xk, ˆxk|ϵk, µ, σ, I) dˆxk
=

prob(xk|ˆxk, ϵk, I) prob(ˆxk|µ, σ, I) dˆxk ,
(4.35)
where unnecessary conditioning statements have been omitted in the second line. As-
signing Gaussian pdfs for both the terms on the right-hand side, we obtain
prob(xk|ϵk, µ, σ, I) =
1

2π(ϵk2 + σ2)
exp
	
−(xk−µ)2
2 (ϵk2 + σ2)

.
(4.36)
The likelihood function for a whole set of independent measurements will then be a
product of such terms; eqn (4.23) can be regarded as a special (but common) case,
when ϵk ≪σ. With this modiﬁcation, the rest of the analysis proceeds as before. The
problem can easily be tackled numerically, or even approximated analytically if all the
error-bars are of the same size (ϵk = ϵ, for all k).
4.3.2
Luminescence dating
The next example comes from a dating technique, involving the optically stimulated
luminescence from minerals, which is suitable for the study of sedimentary deposition
and other geomorphological processes (Aitken 1998, Stokes 1999). For our purposes,
we need say only that the data consist of N measurements {xk}, of the ‘equivalent dose’
of laboratory radiation De, with error-bars {ϵk}, such as those shown in Fig. 4.5(a). Our
task is to infer the age structure of the sediment sample, F(x). A subsequent conversion
from x, or De, to an absolute chronology requires the additional collection of environ-
mental radiation information (the annual dose rate).

Other examples: means, variance, dating and so on
99
This problem takes a model selection form if F(x) can reasonably be approximated
as the sum of a few, say M, discrete age components:
F(x) =
M

j=1
Aj fj(x) ,
(4.37)
where Aj is the relative contribution from the jth era of sediment deposition, fj(x),
so that  Aj = 1. In the simplest case, which we adopt here, the fj’s will just be
δ-functions: fj(x) = δ(x−xoj). The prior for the amplitudes and equivalent doses (or
locations) of the M components, prob({Aj, xoj}|M, I), can be taken as being uniform
in the range 0 ⩽Aj ⩽1, subject to normalization, and 0 ⩽xoj ⩽xmax (where xmax
could be set by the largest datum).
As above, the likelihood for an individual measurement can be written as a marginal
integral over its exact, but unknown, value ˆxk:
prob(xk|ϵk, F(x), I) =

prob(xk|ˆxk, ϵk, I) F(ˆxk) dˆxk ,
(4.38)
Fig. 4.5 (a) Luminescence dating measurements from 65 aliquots. (b) The logarithm of the pos-
terior pdf for the number of age components. (c) The inferred proportions and equivalent doses
of the optimal three age components, and their 1-σ error-bars. (d) Samples of proportions and
equivalent doses drawn from their posterior pdf, generated by a Monte Carlo algorithm, where
the number of age components has been marginalized out.

100
Model selection
where we have used the deﬁnition of F(x), F(ˆxk) = prob(ˆxk|F(x), I), on the right.
With a Gaussian assignment for the noise, prob(xk|ˆxk, ϵk, I), and a δ-function model
for the fj’s in eqn (4.37), the likelihood function for xk becomes
prob(xk|ϵk, F(x), I) =
M

j=1
Aj
ϵk
√
2π
exp
	
−(xk−xoj)2
2 ϵk2

.
(4.39)
The generalization to the case where each age component has a Gaussian width wj is
straightforward, and leads to a similar formula with the terms in the summation resem-
bling those in eqn (4.36) with σ=wj (instead of zero). Making the usual assumption of
independence, the joint likelihood of all the data is just a product of N such factors.
As noted in Section 4.1, the probabilistic evidence for a given model is determined
principally by the mean value of the resulting likelihood function (averaged over the
prior pdf). Its evaluation, within the quadratic approximations of Section 4.2, yields the
posterior pdf for M shown in Fig. 4.5(b); the corresponding estimates of the proportions
and equivalent doses of the optimal three age components are displayed in Fig. 4.5(c).
If the same calculation was carried out with a Monte Carlo algorithm, such as that
discussed in Chapter 9, with M marginalized out (using a Poisson prior with ⟨M⟩=1),
then we would obtain estimates of {Aj, xoj} indicated in Fig. 4.5(d). There is strong
evidence, therefore, that the given sediment sample was a mixture of grains from three
different ages. One possibility that could explain such post-depositional mixing is insect
activity.
4.3.3
Interlude: what not to compute
The ﬁnal example in this chapter comes from quasi-elastic neutron scattering, which
is used to study the rotational and diffusive motions of atoms and molecules. The one-
dimensional data, such as those shown in Fig. 4.6(a), are often analysed in terms of the
sum of a few Lorentzian components, all centred at the origin; using the notation of eqn
(4.12), the ideal underlying spectrum G(x) takes the form
G(x) = Ao δ(x) +
M

j=1
Aj
wj
π

x2 + wj2 ,
(4.40)
where Aj and wj are the amplitude and width of the jth line, and Ao is the δ-function
contribution from elastic scattering. As in eqn (4.13), the measured neutron counts are
a blurred and noisy version of this model and subject to a slowly varying background;
the instrumental resolution function is shown in Fig. 4.6(b). Since each Lorentzian is
associated with a particular type of molecular motion, it is important to ascertain how
many components there is most evidence for in the data.
The problem of obtaining the best estimate of M is essentially the same as that
addressed in Section 4.2; the main difference is that it is now the widths, rather than
positions, of the excitations which are not known. By making very similar simplifying
approximations, we are led to a formula just like eqn (4.20) except that xmax−xmin is
replaced with wmax; a related algorithm is given in Sivia et al. (1992). Following this

Other examples: means, variance, dating and so on
101
procedure yields the posterior pdf for M shown by the ﬁlled-in squares (and solid line)
in Fig. 4.6(c); there is, therefore, most evidence for two quasi-elastic components. The
best estimates of the amplitudes and widths of the Lorentzians, and their (1-σ) error-
bars, are given in Fig. 4.6(d).
This illustration seems fairly straightforward and not unlike our earlier example. A
Fourier transform of eqn (4.40), however, reveals that it is equivalent to the notorious
problem of characterizing the sum of decaying exponentials. The difﬁculty is described
graphically halfway through Acton’s 1970 book, in a section entitled: ‘Interlude: what
not to compute’. He considers the analysis of radioactive decay. In the ﬁrst instance,
the abundance A and B of two known substances, with decay rates a and b, is to be
estimated from the data
y(t) = A e−at + B e−bt ,
where y is proportional to the number of counts recorded at time t. Although this is
easily dealt with through least-squares ﬁtting, there is a closely-related situation which
is far more troublesome than it looks. In the second case, the substances are not known;
Fig. 4.6 (a) Data from a quasi-elastic neutron scattering experiment. (b) The instrumental reso-
lution, or blurring, function. (c) The logarithm of the posterior pdf for the number of Lorentzian
lines; the solid markers resulted from the use of a formula akin to eqn (4.20), whereas the open
circles were obtained with a Monte Carlo algorithm. (d) The inferred amplitudes and widths of
the quasi-elastic components, and their error-bars; w=0 corresponds to elastic scattering.

102
Model selection
therefore, all four parameters (a, b, A and B) have to be inferred. Acton states that the
solution to this problem lies in the chemical, rather than computer, laboratory because:
‘... it is well known that an exponential equation of this type in which all four parame-
ters are to be ﬁtted is extremely ill conditioned. That is, there are many combinations of
(a, b, A, B) that will ﬁt most exact data quite well indeed (will you believe four signif-
icant ﬁgures?) and when experimental noise is thrown into the pot, the entire operation
becomes hopeless.’ He concludes: ‘But those with Faith in Science do not always read
the Book— and must be spanked or counselled.’
The sobering message of this anecdote is already contained in the solid line of Fig.
4.6(c): in contrast to the example of Fig. 4.4, the posterior pdf for the number of quasi-
elastic components is much ﬂatter after the maximum (at M = 2). Thus the analysis
is automatically warning us that the most we can really say is that there are at least
two Lorentzians. Indeed, even if we knew that there were more than two broadened
lines, the (marginal) error-bars for their inferred parameters would be very large. The
precise form of the solid line in Fig. 4.6(c) is itself suspect, because the simplifying
approximations which underlie eqn (4.20) break down when the spread of the likeli-
hood function becomes comparable to the prior range; a more reliable evaluation of the
evidence integral is then given by a Monte Carlo algorithm, as illustrated by the dotted
line in Fig. 4.6(c). Our conclusions are in general agreement with the folklore that it is
difﬁcult to characterize quasi-elastic scattering reliably if there are more than a couple
of Lorentzian components. Once again, we see that Laplace’s contention is borne out:
probability theory is nothing but common sense reduced to calculation.

5
Assigning probabilities
In the preceding three chapters, we have seen how pdfs can be manipulated, with the
sum and product rule of probability, to address data analysis problems; little was said,
however, about their assignment in the ﬁrst place. We now turn to this basic question. In
addition to justifying the earlier use of the binomial, Gaussian and Poisson distributions,
we will have our ﬁrst encounter with the principle of maximum entropy.
5.1
Ignorance: indiﬀerence and transformation groups
In Section 1.2, we outlined how Cox showed that any method of plausible reasoning,
which satisﬁes elementary requirements of logical consistency, must be equivalent to
the use of probability theory. While the sum and product rule specify the relationship
between pdfs, they do not tell us how to assign them. Are there any general principles
to help us do this?
The oldest idea dates back to Bernoulli (1713), in what he called the ‘principle of
insufﬁcient reason’; Keynes (1921) later renamed it the ‘principle of indifference’. It
states that if we can enumerate a set of basic, mutually exclusive, possibilities, and have
no reason to believe that any one of these is more likely to be true than another, then we
should assign the same probability to all. If we consider an ordinary die, for example,
we can list the six potential outcomes of a roll,
Xi ≡the face on top has i dots ,
for i =1, 2, . . ., 6; according to Bernoulli, therefore, we have
prob(Xi|I) = 1/6 ,
(5.1)
where the background information I consists of nothing more than the enumeration of
the possibilities. Although our everyday intuition tells us that this assignment is very
reasonable, can we justify it in a more fundamental way? The reason for asking this
question is that a better understanding of this easy problem might shed light on how to
deal with more complicated situations later.
The most open-ended aspect of the statement of Bernoulli’s principle above is what
we mean by not having any reason to believe that one possibility is more likely to be
true than another; it’s worth trying to elaborate on this. Let’s imagine that the potential
outcomes of the die-roll were denoted by the ﬁrst six letters of the alphabet; then, we
need to assign the probabilities prob(A|I), prob(B|I), . . . , prob(F|I). This is, of
course, nothing but a change of nomenclature: A could be equivalent to X1, B to X2,
and so on. Having ascribed relative truth-values to propositions A to F, suppose that
we were told that a mistake had been made and that A really stood for X6, and B

104
Assigning probabilities
for X5, etc.: Should such a reordering make any difference? If the conditioning on I
represents a gross ignorance about the details of the situation, then the answer is no;
the contrary would indicate that we were in possession of cogent information other than
the simple enumeration of the possibilities. Consistency demands, therefore, that our
probability assignment should not change if the order in which the propositions are
listed is rearranged; the only way to satisfy this requirement is through eqn (5.1). This
justiﬁcation of Bernoulli’s principle has led Jaynes (1978) to suggest that it is more apt
to think of it as a consequence of the ‘desideratum of consistency’; we will ﬁnd that this
type of argument forms the basis of the central theme in this chapter.
Bernoulli, himself, appreciated that the principle of insufﬁcient reason could only
be applied to a very limited number of problems; mainly, those involving games of
chance. And, indeed, the scope of possible problems is far wider than those for which
probability assignments are essentially deﬁnitive. Nevertheless, we can still obtain some
important elementary results by combining the principle with the sum and product rules
of probability. Consider, for example, the case of coloured balls being drawn randomly
from an urn. If we knew that the contents consisted of W white balls and R red ones,
and nothing more, then the principle of indifference tells us that we should assign the
uniform pdf
prob(j|I) =
1
R + W ,
(5.2)
for the proposition that any particular ball, denoted by the index j, will be chosen.
Marginalization and the product rule then allow us to express the probability that the
colour drawn will be red as
prob(red|I) =
R+W

j=1
prob(red, j|I) =
1
R + W
R+W

j=1
prob(red|j, I) ,
where we have substituted for prob(j|I) from eqn (5.2) on the far right. The term
prob(red|j, I) will be equal to one if the jth ball is red and zero if it’s white; the
known contents of the urn, therefore, tell us that the summation will be equal to R.
Hence, we obtain the anticipated result:
prob(red|I) =
R
R + W .
(5.3)
We have derived eqn (5.3) from the assignment in eqn (5.2) with the sum and product
rule, and it justiﬁes the common notion of probability as
prob(red|I) =
number of cases favourable to red
total number of equally possible cases .
Other familiar relationships emerge when we consider the result of repeating this ball-
drawing procedure many times over. For algebraic simplicity, let us restrict ourselves to
the case of ‘sampling with replacement’ where the contents of the urn are the same each

Ignorance: indiﬀerence and transformation groups
105
time. What is then the probability that N such trials will result in r red balls? Using
marginalization and the product rule, we can express this as
prob(r|N, I) =

k
prob(r, Sk|N, I)
=

k
prob(r|Sk, N, I) prob(Sk|N, I) ,
(5.4)
where the summation is over the 2N possible sequences of red–white outcomes {Sk}
of N draws. The ﬁrst term in the second line, prob(r|Sk, N, I), will be equal to one if
Sk contains exactly r red balls and zero otherwise; thus, we need only consider those
sequences which have precisely r red outcomes for prob(Sk|N, I). Since I assumes
a general ignorance about the situation, other than a knowledge of the contents of the
urn, the result of one draw does not inﬂuence what we can infer about the outcome
of another; the probability of drawing any particular sequence Sk depends only on the
total number of red (and complementary white) balls obtained, therefore, and not on
their order. Speciﬁcally, for the {Sk} which matter, we have
prob(Sk|N, I) =

prob(red|I)
r ×

prob(white|I)
N−r.
Substituting for prob(red|I) from eqn (5.3), and for the corresponding probability of
getting a white ball, this gives
prob(Sk|N, I) = Rr W N−r

R +W
N .
(5.5)
Hence, the summation of eqn (5.4) reduces to this term times the number of possible
sequences of N-draws which contain exactly r red balls. To evaluate the latter, we must
make a brief digression to the topic of permutations and combinations.
Let us begin this short foray into counting exercises with the easiest problem: In
how many ways can n different objects be arranged in a straight line? Well, there are n
choices for the ﬁrst item, leaving n−1 possibilities for the second; this means that two
objects can be picked in n×(n−1) ways, where their order matters. Continuing along
this path, there are n−2 choices for the third item, n−3 for the fourth, and so on; the
total number of permutations is then given by the product
n × (n−1) × (n−2) × · · · × 3 × 2 × 1 = n! .
(5.6)
A closely related question is as follows: In how many ways can we sequentially pick
m objects from n different ones? We have, in fact, covered this case as an intermediate
step in the calculation above; we just stop the product when m items have been chosen:
n × (n−1) × (n−2) × · · · × (n−m+2) × (n−m+1) .
This number of permutations is often written as nPm and can be expressed as a ratio of
two factorials by using eqn (5.6):

106
Assigning probabilities
nPm =
n!
(n−m)! .
(5.7)
The special case of m = n does reduce to n!, as expected, because 0! = 1; the latter
follows from the substitution of n = 1 in n! = (n−1)! × n. If we are not interested
in the order in which the m objects are picked, then this can be taken into account by
dividing eqn (5.7) by the number of ways that m items can be permuted; the resulting
number of combinations is often denoted by nCm:
nCm =
n!
m! (n−m)! .
(5.8)
This formula also arises in the context of the binomial expansion for integer powers,
where it appears as the coefﬁcient in the relevant summation:
(a +b)N =
N

j=0
N!
j! (N −j)! aj bN−j .
(5.9)
It is useful to note that the sum on the right is equal to unity if a + b =1; this result will
be helpful in some of the following analysis. With a = b =1,  nCm = 2n, where m
goes from 0 to n, which represents the number of relevant possibilities.
Returning to the task of evaluating prob(r|N, I), the ﬁnal ingredient we need is the
number of different ways in which exactly r red balls can be drawn in N trials. To see
how we can use the results derived above to calculate this, let’s think of the problem
in the following terms: imagine that the integers 1 to N have been written on separate
small pieces of paper; if we select r of them, then the numbers chosen can be thought
of as representing the draws on which a red ball was obtained. Thus, the sequences we
require correspond to the number of ways of selecting r integers out of N where their
order is irrelevant: hence, there are NCr of them. From our discussion of eqn (5.4),
therefore, the results in eqns (5.5) and (5.8) combine to yield
prob(r|N, I) =
N!
r! (N −r)! × Rr W N−r

R +W
N .
(5.10)
We can easily check that this pdf is normalized because the associated summation can
be written in the form of eqn (5.9):
N

r=0
prob(r|N, I) =
N

r=0
N!
r! (N −r)! pr qN−r = (p +q)N ,
where p +q =1 since
p =
R
R + W
and
q =
W
R + W .
(5.11)
The pdf of eqn (5.10) allows us to compute the frequency, r/N , with which we
expect to observe red balls:

Ignorance: indiﬀerence and transformation groups
107
 r
N

=
N

r=0
r
N prob(r|N, I) =
N

r=1
(N −1)!
(r−1)! (N −r)! pr qN−r,
where the lower limit of the sum has been changed to r=1 because the contribution of
the r=0 term is zero. Taking a factor of p outside the summation, and letting j = r −1,
we ﬁnd that
 r
N

= p
N−1

j=0
(N −1)!
j! (N −1−j)! pj qN−1−j = p (p +q)N−1 .
Substituting for p and q from eqn (5.11), we again obtain the anticipated result,
 r
N

=
R
R + W ,
(5.12)
where the expected frequency of red balls, in repetitions of the urn ‘experiment’, is
equal to the probability of obtaining one in a single trial. A similar calculation for the
variance of r/N shows that the mean-square deviation from eqn (5.12) is given by
! r
N −p
2"
= p q
N .
(5.13)
Since this becomes zero in the limit of inﬁnite N, it veriﬁes that Bernoulli’s famous
theorem of large numbers is obeyed:
lim
N→∞
 r
N

= prob(red|I) .
(5.14)
Although Bernoulli was able to derive this relationship for predicting the long-run fre-
quency of occurrence from the probability assignment of eqn (5.3), his unfulﬁlled quest
lay in the reverse process: What could one say about the probability of obtaining a
red ball, in a single draw, given a ﬁnite number of observed outcomes? The answer to
that question had to await Bayes and Laplace; as we have already seen in the ﬁrst four
chapters, it lies at the very heart of data analysis.
5.1.1
The binomial distribution
As in the case of the urn with red and white balls, or the coin-ﬂipping example of Section
2.1, the outcome of some experiments can only take one of two values; for generality,
we can call them ‘success’ and ‘failure’. Even if there were also yellow, green and
blue balls in the urn, we could still formulate the problem in such terms as long as our
primary interest lay in just one colour; ‘white’ would then stand for ‘not red’. Following
the nomenclature of the preceding analysis, we can deﬁne:
prob(success|I) = p
and
prob(failure|I) = q = 1−p ,
for the probability of success, and failure, in a single trial. The formulae of eqns (5.10)
and (5.11), therefore, give the pdf of obtaining r successes in N trials as

108
Assigning probabilities
Fig. 5.1 The binomial distribution, prob(r|N, p), or the probability of obtaining r favourable
outcomes in N trials where p is the chance of success on any given attempt.
prob(r|N, I) =
N!
r! (N −r)! pr (1−p)N−r ,
(5.15)
where r = 0, 1, 2, . . ., N. This is called the binomial distribution and is illustrated in
Fig. 5.1 for two different values of p and N. The expected number of successes, ⟨r⟩, and
the mean-square deviation from this average value, follows from eqns (5.11)–(5.13):
⟨r⟩= Np
and

(r−Np)2 
= Np(1−p) .
(5.16)
These results can be veriﬁed pictorially from the examples shown in Fig. 5.1.
5.1.2
Location and scale parameters
Having seen how some familiar elementary results can be derived from the principle of
indifference, and sum and product rule of probability, let us return to the central topic
of this chapter: namely, the assignment of probabilities. As stated earlier, Bernoulli’s
principle of insufﬁcient reason can be used when we are able to enumerate a set of basic
possibilities (and no more); this implicitly assumes that the quantity of interest, X, is
restricted to certain discrete values, as in eqn (5.1). Can we generalize the argument to
cover the case of continuous parameters?

Ignorance: indiﬀerence and transformation groups
109
Suppose that X represented the position of a lighthouse along a straight coast, as in
Section 2.4. Then, given the information I, the probability that X lies in the inﬁnitesi-
mally small range between x and x+δx is given by
prob(X = x|I) dX =
lim
δx→0 prob(x ⩽X < x + δx|I) .
Although we treat continuous pdfs as the limiting case of discrete ones, the concept
of enumerating the possibilities is rather awkward. Nevertheless, we can still make use
the idea of consistency which underlies the principle of indifference. For example, if
we were told that a mistake had been made in deﬁning the origin, so that the position
previously quoted as x was actually x+xo, should this make any difference to the pdf
assigned for X? If I indicates gross ignorance about the details of the situation, other
than a knowledge that X pertains to a location (and some idea of its possible range),
then the answer is ‘not much’; the contrary would indicate that we were in possession of
other cogent information regarding the position of the lighthouse. Consistency demands,
therefore, that the pdf for X should change very little with the value of the offset xo;
this requirement can be written mathematically as
prob(X |I) dX ≈prob(X + xo|I) d(X+xo) .
Since xo is a constant, d(X+xo) = dX; this leads to the solution
prob(X |I) ≈constant in the allowed range ,
(5.17)
and zero otherwise. Thus complete ignorance about a location parameter is represented
by the assignment of a uniform pdf.
Another common problem concerns quantities which are associated with a size or
magnitude, whereby it is the relative or fractional change that is important (rather than
the absolute one, as in the case of a location parameter); these are often called scale pa-
rameters. For example, we could be interested in the length L of a biological molecule.
What should we assign for the pdf prob(L|I), where I represents gross prior ignorance
about the value of L? Well, if we really had little idea about the length scale involved,
then the graph of the pdf should be essentially invariant with respect to a stretching,
or shrinking, of the horizontal L-axis. In other words, if we were told that a mistake
had been made in the units of length quoted, so that they should have been ˚angstroms
instead of nanometres, then this should not make much difference to the pdf we assign.
This requirement of consistency can be written as
prob(L|I) dL ≈prob(βL|I) d(βL) ,
where β is a positive constant; since d(βL) = β dL, it can only be satisﬁed by
prob(L|I) ∝1/L in the allowed range ,
(5.18)
and zero otherwise. This pdf is often called a Jeffreys’ prior, since it was ﬁrst suggested
by him (1939); it represents complete ignorance about the value of a scale parameter.

110
Assigning probabilities
This result seems less weird once we realize that it’s equivalent to a uniform pdf for
the logarithm of L, prob(logL|I) = constant, as can be veriﬁed with a change of
variables according to eqn (3.77). In essence, it conﬁrms our practical experience that
problems involving magnitudes are best done using logarithmic graph paper!
5.2
Testable information: the principle of maximum entropy
We have now seen how some pdfs can be assigned when given only the nature of the
quantities involved. The method hinges on the use of consistency arguments and re-
quires a consideration of the transformation groups which characterize ignorance for
the given situation. For the case of a ﬁnite set of discrete possibilities, this means that
the related pdf has to be invariant with respect to any permutation of the relevant propo-
sitions. Continuous parameters are treated in the same way, with the appropriate trans-
formations being an origin shift or an axis stretch for the two most common cases of
location and scale variables. Having dealt with ignorance, let’s move on to a more en-
lightened situation.
Suppose that a die, with the usual six faces, was rolled a very large number of times;
if we were only told that the average result was 4.5, what probability should we assign
for the various possible outcomes {Xi} that the face on top had i dots? The information
I provided can be written as a simple constraint:
6

i=1
i prob(Xi|I) = 4.5 .
(5.19)
Since the uniform pdf of eqn (5.1) predicts an average value of 3.5, instead of 4.5, it
can be ruled out as a valid assignment. However, there are still many pdfs which are
consistent with eqn (5.19); which one is best?
The constraint of eqn (5.19) is an example of testable information; with such a
condition, we can either accept or reject (outright) any proposed pdf. Jaynes (1957) has
suggested that, in this type of situation, we should make the assignment by using the
principle of maximum entropy (MaxEnt): that is, we should choose that pdf which has
the most entropy S while satisfying all the available constraints. Explicitly, for the case
of the die above, we need to maximize
S = −
6

i=1
pi loge[pi] ,
(5.20)
where pi =prob(Xi|I), subject to normalization and the condition of eqn (5.19):
6

i=1
pi = 1
and
6

i=1
i pi = 4.5 .
Such a constrained optimization can be done with the method of Lagrange multipliers
(and some numerical analysis) and yields the pdf shown in Fig. 5.2.
At this juncture, it is reasonable to ask why the function in eqn (5.20) should be par-
ticularly favoured as a selection criterion. A good qualitative discussion of this question,

Testable information: the principle of maximum entropy
111
Fig. 5.2 The MaxEnt assignment for the pdf for the outcomes of a die roll, given only that it has
the usual six faces and yields an average result of 4.5.
with reference to the die problem above, is given in Jaynes (1963); there he shows how
the MaxEnt assignment reﬂects many of the properties we would expect from common
sense. In a more general context, the entropy function can be justiﬁed in a variety of
different ways: arguments ranging from information theory (Shannon 1948) to logical
consistency (Shore and Johnson 1980), and several others besides, all lead to the con-
clusion that the ‘− pi log[pi]’ criterion is highly desirable. To get a feel for why this
is so, let us consider two simple examples. The ﬁrst is the kangaroo problem of Gull
and Skilling (1984), which they describe as a physicists’ perversion of the formal math-
ematical analysis of Shore and Johnson; the second is a combinatorial argument, often
phrased in terms of a hypothetical team of monkeys.
The kangaroo problem is as follows:
Information: A third of all kangaroos have blue eyes, and a quarter of all kangaroos
are left-handed.
Question:
On the basis of this information alone, what proportion of kangaroos
are both blue-eyed and left-handed?
Well, for any given kangaroo, there are four distinct possibilities. Namely, that it is
(1) blue-eyed and left-handed, (2) blue-eyed and right-handed, (3) not blue-eyed but
left-handed or (4) not blue-eyed and right-handed. Following Bernoulli’s law of large
numbers, the expected values of the fraction of kangaroos with traits (1)–(4) will be
equal to the probabilities we assign to each of these propositions. Denoting the latter by
p1, p2, p3 and p4, respectively, we can represent the situation pictorially by using a
2 × 2 truth, or contingency, table shown in Fig. 5.3(a).
Although there are four possible combinations of eye-colour and handedness to be
considered, the related probabilities are not independent of each other. In addition to the
usual normalization requirement, p1 + p2 + p3 + p4 = 1, we also have two conditions
on the marginal probabilities: p1 + p2 = 1/3 and p1 + p3 = 1/4. This means that the
2 × 2 contingency table of Fig. 5.3(a) can be parameterized by a single variable x, or
p1 (say), as is shown in Fig. 5.3(b). All such solutions, where 0 ⩽x ⩽1/4, satisfy the
constraints of the testable information: Which one is ‘best’?

112
Assigning probabilities
Fig. 5.3 The 2×2 truth table for the kangaroo problem: (a) the general case, where the probabil-
ities for the four possibilities are denoted by p1, p2, p3, p4 ; (b) the corresponding parameteriza-
tion in terms of a single variable x, which takes into account the information provided.
If a choice had to be made between the permissible pdfs, our common sense would
draw us towards the assignment based on independence: x = 1/12 . This is simply
because any other value would indicate that a knowledge of the kangaroo’s eye-colour
told us something about its handedness; since we have no information to ascertain even
the sign of any potential correlation, let alone its magnitude, this would seem to be
somewhat foolhardy. There might well be a gene-linkage, of course, but we would be
wise to shy away from assuming any strong connections (x →0 or x →1/4) without
some pertinent evidence to support it.
The virtue of this little example is that it’s easy enough to decide which is the most
sensible pdf assignment, in the face of the inadequate information. This allows us to
ask whether there is some function of the {pi} which, when maximized subject to the
known constraints, yields the same preferred solution. If so, then it would be a good
candidate for a general variational principle which could be used in situations that were
too complicated for our common sense. To this end, Skilling (1988) has shown that the
only functions which give uncorrelated assignments in general are those related mono-
tonically to the entropy S = − pi log[pi]. The point is illustrated in Table 5.1, where
the results obtained by using three proposed alternatives are listed; as expected, they
all return optimal solutions with varying degrees of implied correlation between hand-
Table 5.1 The solutions to the kangaroo problem given by maximizing four different
functions, subject to the constraints of the available information.
Variational function
Optimal x
Implied correlation
− pi log[pi]
0.0833
None
− p2
i
0.0417
Negative
 log[pi]
0.1060
Positive
 √pi
0.0967
Positive

Testable information: the principle of maximum entropy
113
edness and eye-colour. This indicates that there is intrinsic merit in using the principle
of maximum entropy for assigning pdfs; further support is provided by the so-called
monkey argument.
5.2.1
The monkey argument
Suppose there are M distinct possibilities {Xi} to be considered; our task is to ascribe
truth-values to them, given some testable information I: prob(Xi|I)= pi. How can we
do this in the most honest and fair way? Well, we could imagine playing a little game:
the various propositions could be represented by different boxes, all of the same size,
into which pennies are thrown at random; this job is often delegated to a hypothetical
team of monkeys, to denote the fact that there should be no underlying bias in the pro-
cedure. After a very large number of coins have been distributed, the fraction found in
each of the boxes gives a possible assignment for the probability for the corresponding
{Xi}. The resulting pdf may not be consistent with the constraints of I, of course, in
which case it should be rejected as a potential candidate; if it is in agreement, then it’s a
viable option. The boxes can then be emptied and the monkeys allowed to have another
go at scattering the pennies. After many such trials, some distributions will be found
to come up more often than others; the one that occurs most frequently (and satisﬁes
I ) would be a sensible choice for prob({Xi}|I). This is because our ideal monkeys
have no particular axe to grind, and so this favoured solution can be regarded as the one
that best represents our state of knowledge: it agrees with all the testable information
available while being as non-committal as possible towards everything else. Let’s see
how this corresponds to the pdf with the greatest value of ‘− pi log[pi]’.
After the monkeys have scattered all the pennies given to them, suppose that we
ﬁnd n1 in the ﬁrst box, n2 in the second, and so on. The total number of coins N is then
deﬁned by the condition
N =
M

i=1
ni ,
(5.21)
and we will assume that it is very large, and certainly much greater than the number of
boxes: N ≫M. This distribution {ni} will give rise to a corresponding candidate pdf
{pi} for the possibilities {Xi}:
pi = ni/N ,
(5.22)
where i = 1, 2, . . ., M. Since every penny can land in any of the boxes, there are
MN number of different ways of scattering the coins amongst them; by design, each
is equally likely to occur. All of these basic sequences are not distinct, however, as
many yield the same distribution {ni}. The expected frequency F with which a {pi}
will arise, therefore, is given by
F

{pi}

= number of ways of obtaining {ni}
MN
.
(5.23)
To evaluate the numerator, we can break up the calculation into a series of simpler
steps. Let’s start with box1 and ask: In how many different ways can n1 coins be chosen

114
Assigning probabilities
from a total of N? Following our earlier discussion of permutations and combinations,
in Section 5.1, the answer is just the binomial coefﬁcient NCn1. Next, moving on to box
2, we can ask: In how many different ways can n2 coins be selected from the remaining
N −n1? The answer is N−n1Cn2. Continuing in this vein, we ﬁnd that the numerator
in eqn (5.23) is given by the product of M such binomial terms; using eqns (5.8) and
(5.21), the required number reduces to
NCn1 × N−n1Cn2 × N−n1−n2Cn3 × · · · × nMCnM =
N!
n1! n2! · · · nM! .
Substituting this expression for the numerator in eqn (5.23), we obtain
log[F ] = −N log[M ] + log[N!] −
M

i=1
log[ni!] ,
(5.24)
where we have taken the logarithm of both sides to help deal with the excessively large
numbers involved. In fact, the right-hand side can be simpliﬁed by using the Stirling
approximation:
loge[n!] ≈n loge[n] −n ,
which is valid as n→∞. As we approach this limit, eqn (5.24) reduces to
log[F ] = −N log[M ] + N log[N ] −
M

i=1
ni log[ni ] ,
because two of the terms cancel out according to eqn (5.21). Finally, substituting for ni
from eqn (5.22), and using the fact that  pi = 1, we are led to the result
log[F ] = −N log[M ] −N
M

i=1
pi log[pi] .
(5.25)
Since this is related monotonically to the expected frequency with which the monkeys
will come up with the candidate pdf {pi}, the assignment prob({Xi}|I) which best
represents our state of knowledge is the one that gives the greatest value for log[F ]
while being consistent with the testable information I. As M and N are constants, this
is equivalent to the constrained maximization of the function S:
S = −
M

i=1
pi log[pi] .
(5.26)
Any monotonic function of S will, of course, lead to the same result. Therefore, neither
the base of the logarithm, nor the precise value of N in eqn (5.25), is of any great
concern and need not be speciﬁed.
The functional form of eqn (5.26) is the same as the quantity called ‘entropy’ in
thermodynamics; for this reason, it too is referred to by that name. The similarity is

Testable information: the principle of maximum entropy
115
far from being purely coincidental, however, as Jaynes has pointed out in many of his
papers (e.g. 1983). He has shown how the familiar relationships of thermodynamics
can easily be derived by treating the subject as an exercise in inferential calculus, with
MaxEnt playing the central rˆole of a variational principle for assigning pdfs subject
to the macroscopic constraints like temperature, pressure and volume. This approach
provides a fundamentally different perspective for several branches of physics.
5.2.2
The Lebesgue measure
In the monkey argument above, we took all the boxes to be of the same size; this seems
reasonable, since we wanted to be fair towards all of the propositions that they rep-
resented. If we were dealing with an ordinary die, for example, then the principle of
indifference would naturally lead us to assign equal prior weight to the six possible out-
comes. Suppose that, for some perverse reason, the problem is posed in terms of just
three hypotheses:
Xi ≡the face on top has

i dots
for i = 1, 2 ,
3, 4, 5 or 6 dots
for i = 3 .
Given (only) the six-sided nature of the die, we would then be inclined to make the box
for X3 four times as large as that for X1 and X2. How does this unevenness affect the
preceding analysis?
Let’s go back to the case of the M distinct possibilities, but adjust the size of the
boxes so that the chances that a monkey will throw a penny into the ith one is mi. We
always have the condition that
M

i=1
mi = 1 ,
but the mi are not necessarily equal; if they were, then the mi would all be 1/M. What
is now the expected frequency F that the monkeys will throw n1 coins in the ﬁrst box,
n 2 in the second, and so on? Well, it will be given by the number of different ways
of scattering the N pennies which yield the distribution {ni} times the probability of
obtaining such a sequence of throws:
F

{pi}

=
N!
n1! n2! · · · nM! × m1n1 m2n2 · · · mM
nM .
(5.27)
This is called a multinomial distribution, and is a generalization of its binomial (M =2)
counterpart given in eqn (5.15). If all the mi are equal, then the product of the M terms
of the far right reduce to the reciprocal of MN and, as required, we recover eqn (5.23).
Taking the logarithm of eqn (5.27), and using Stirling’s approximation, we ﬁnd that eqn
(5.25) now becomes
log[F ] =
M

i=1
ni log[mi] −N
M

i=1
pi log[pi] .
Substituting for ni from eqn (5.22), we ﬁnally obtain

116
Assigning probabilities
1
N log[F ] = −
M

i=1
pi log
 pi
mi

= S .
(5.28)
In other words, we can regard the entropy formula of eqn (5.26) as a special case of
this more general form (where mi =1/M). This is known by various names, including
the Shannon–Jaynes entropy, the Kullback number and the cross-entropy; in the last
context, it is often written with the opposite sign so that it has to be minimized!
Although the need for boxes of uneven size might seem rather contrived in our die
example, Jaynes (1963) has pointed out that the generalization to the form of eqn (5.28)
is necessary when we consider the limit of continuous parameters:
S = −

p(x) log
 p(x)
m(x)

dx .
(5.29)
The Lebesgue measure, m(x), ensures that the entropy expression is invariant under
a change of variables, x →y = f(x), because both p(x) and m(x) transform in the
same way. Essentially, the measure takes into account how the (uniform) bin-widths in
x-space translate to a corresponding set of (variable) box-sizes in an alternative y-space.
To obtain a better understanding of the nature of m(x), let’s consider maximizing
the entropy of eqn (5.29) subject to only the normalization condition

p(x) dx = 1.
This type of calculation is done most easily as the limit of its discrete counterpart; as
such, the method of Lagrange multipliers tells us that we must maximize Q with respect
to the {pi}, treated as independent variables:
Q = −

i
pi loge
 pi
mi

+ λ

1 −

i
pi

,
and subsequently determine λ from the condition  pi =1. Since the partial derivative
∂pi/∂pj = 0 if i ̸= j, we have
∂Q
∂pj
= −1 −log e
 pj
mj

−λ = 0 ,
for all j; this leads to the trivial solution
pj = mj e−(1+λ) .
The normalization requirement for {pi} ﬁxes the Lagrange multiplier, and in the con-
tinuum limit
p(x) = prob(x|normalization) ∝m(x) .
(5.30)
In other words, m(x) is any multiple of the pdf which expresses complete ignorance
about the value of x; thus, the transformation-group (invariance) arguments of Section
5.1 are appropriate for ascertaining the measure.

MaxEnt examples: some common pdfs
117
5.3
MaxEnt examples: some common pdfs
Having seen two of the ways by which we are led to MaxEnt, let us illustrate its use
with a few examples. We have, in fact, just met the simplest situation where the testable
information consists purely of the normalization condition. If the measure is uniform,
then eqn (5.30) tells us that we should assign prob(x|I) = constant. For the case of
M discrete possibilities {Xi}, this reduces to prob(Xi|I) = 1/M, in accordance with
Bernoulli’s principle of insufﬁcient reason.
The uniform pdf itself gives rise to others, when manipulated through the sum and
product rules of probability. We saw this in Section 5.1, when considering the repeated
drawing of red and white balls from an urn. Assuming the contents of the container, and
our state of ignorance, to be the same on each trial, we were led to the binomial distribu-
tion; sampling without replacement, on the other hand, would yield the hypergeometric
distribution. For the continuous case of the lighthouse problem in Section 2.4, we used
the fact that the uniform pdf for the azimuth angle of the emitted beam translates to
a Cauchy distribution for the position of the ﬂashes detected on the coast; the relevant
transformation was derived in Section 3.6. Now let’s see how MaxEnt gives rise to some
other commonly met pdfs.
5.3.1
Averages and exponentials
Suppose that the testable information, pertaining to some quantity x, consisted of a
knowledge of the expectation value µ; this can be written as the constraint
⟨x⟩=

x prob(x|I) dx = µ .
(5.31)
What should we now assign for prob(x|I)? According to the principle of MaxEnt, we
need that pdf which has the most entropy while satisfying the conditions of eqn (5.31)
and normalization. As we said earlier, this optimization is best done as the limiting case
of a discrete problem; explicitly, we need to ﬁnd the maximum of Q:
Q = −

i
pi loge
 pi
mi

+ λ0

1 −

i
pi

+ λ1

µ −

i
xi pi

,
where λ0 and λ1 are Lagrange multipliers. Setting ∂Q/∂pj = 0, we obtain
pj = mj e−(1+λ0) e−λ1xj .
(5.32)
In fact, with mi = 1/6 and xi = i, this is the solution to the die problem plotted in
Fig. 5.2; λ0 and λ1 were calculated numerically, so that the resultant pdf satisﬁed the
requirements  i pi = 4.5 and  pi =1. Generalizing to the continuous case, with a
uniform measure, eqn (5.32) becomes a simple exponential function:
prob(x|I) ∝exp

−λ1 x

.
The normalization constant and λ1 can easily be evaluated if the limits of integration,
in eqn (5.31), are 0 and ∞; it results in the assignment

118
Assigning probabilities
prob(x|µ) = 1
µ exp

−x
µ

for x ⩾0 .
(5.33)
A uniform measure is not always the most appropriate, and this gives rise to other
important pdfs; we’ll illustrate this with the binomial and Poisson distributions shortly.
Another example is given in Sivia and David (1994), where they show how both the
Wilson distributions of crystallography emerge from the same constraint on the mean;
it simply requires a careful consideration of the relevant measures. Now let’s see what
happens as we obtain some more cogent testable information.
5.3.2
Variance and the Gaussian distribution
Suppose that we know not only a value µ, but also the variance σ2 about that value:

(x−µ)2
=

(x−µ)2 prob(x|I) dx = σ2.
(5.34)
To assign prob(x|I), we need to maximize its entropy subject to normalization and eqn
(5.34). For the discrete case, this is equivalent to ﬁnding the extremum of Q:
Q = −

i
pi loge
 pi
mi

+ λ0

1 −

i
pi

+ λ1

σ2 −

i
(xi −µ)2 pi

,
where λ0 and λ1 are Lagrange multipliers. Setting ∂Q/∂pj = 0, we obtain
pj = mj e−(1+λ0) e−λ1(xj−µ)2 .
With a uniform measure, this generalizes into the continuum assignment
prob(x|I) ∝exp

−λ1(x −µ)2 
.
The normalization constant and λ1 are readily evaluated if the limits of integration, in
eqn (5.34), are ±∞; it results in the standard Gaussian pdf
prob(x|µ,σ) =
1
σ
√
2π exp
	
−(x −µ)2
2 σ2

,
(5.35)
where σ is deﬁned in eqn (5.34), and µ is seen to be the mean as deﬁned in eqn (5.31).
This indicates why there is a close link between the normal distribution and quantities
characterized solely by their mean and variance: the former is the most honest descrip-
tion of our state of knowledge, when given nothing more than this information about
the latter.
So far, we have only considered pdfs of a single variable; the MaxEnt analysis is
easily extended to the case of several parameters by expressing the entropy of eqn (5.29)
as a multi-dimensional integral:
S = −

· · ·

p(x) log
 p(x)
m(x)

dNx ,
(5.36)

MaxEnt examples: some common pdfs
119
where p(x)=prob(x1, x2, . . . , xN |I), and so on. If the testable information pertaining
to the quantities {xk} consists of a knowledge of just their individual variances,

(xk−µk)2
=

· · ·

(xk−µk)2 p(x) dNx = σk
2 ,
(5.37)
where k =1, 2, . . ., N, then it can be shown that the maximization of eqn (5.36), with
a uniform measure, yields a simple product of Gaussian pdfs:
prob

{xk}
{µk, σk}

=
N

k=1
1
σk
√
2π exp
	
−(xk −µk)2
2 σk2

.
(5.38)
This is, in fact, the same pdf as a least-squares likelihood function! If we identify the
{xk} as the data {Dk}, with error-bars {σk}, and the {µk} as the predictions {Fk} based
on some given model, then eqn (5.38) corresponds to eqns (3.65) and (3.66).
From the MaxEnt point of view, the least-squares likelihood does not imply the ex-
istence (or assumption) of a mechanism for generating independent, additive, Gaussian
noise. It is just seen as the pdf which best represents our state of knowledge given only
the value of the expected square-deviation between our predictions and the data, as in
eqn (5.37). This potential mismatch could stem from uncertainties in the measurement
process, perhaps due to a poor calibration, but it may simply reﬂect the inadequacies of
the simplifying approximations in the modelling of the situation. If we had cogent in-
formation about the covariance

(xi −µi)(xj −µj)

, where i ̸= j, then MaxEnt would
assign a correlated multivariate Gaussian pdf for prob({xk}|I).
In Section 3.5, we mentioned the possibility of using the l1-norm of eqn (3.72) as a
criterion for ﬁtting functional models to data. This involves the minimization of the sum
of the moduli of the misﬁt residuals, rather than their squares; the latter is, of course,
just χ2 (or the l2-norm) of eqn (3.66). This procedure follows naturally from MaxEnt
if the testable information consists only of the expected value of the modulus of the
discrepancy between theory and experiment for the individual data:

|xk−µk|

=

· · ·

|xk−µk| p(x) dNx = ϵk ,
(5.39)
where k =1, 2, . . ., N. Then, it can be shown that the maximization of eqn (5.36), with
a uniform measure, yields the product of symmetric exponential pdfs:
prob

{xk}
{µk, ϵk}

=
N

k=1
1
2ϵk
exp

−|xk−µk|
ϵk

.
(5.40)
The logarithm of the likelihood function is, therefore, given by the l1-norm (plus an
additive constant). Having seen how the precise nature of the testable constraints can
inﬂuence the pdf we assign, let’s look at a couple of examples where the measure plays
an equally important rˆole.

120
Assigning probabilities
5.3.3
MaxEnt and the binomial distribution
Although we have already derived the binomial distribution from elementary consider-
ations in Section 5.1, it’s instructive to see how it emerges from the use of the MaxEnt
principle. Suppose that we are given (only) the expected number of success in M trials,
⟨N ⟩=µ. What should we assign for the probability of a speciﬁc number of favourable
outcomes, prob(N |M,µ)?
According to the MaxEnt principle, we need to maximize the entropy S of eqn
(5.28) subject to the testable information
⟨N ⟩=
M

N=0
N prob(N |M,µ) = µ
(5.41)
and normalization. Following an earlier calculation, this optimization yields the pdf of
eqn (5.32); speciﬁcally, we obtain
prob(N |M,µ) ∝m(N) e−λN ,
(5.42)
where λ is a Lagrange multiplier and m(N) is the measure. The former is, of course,
determined by the constraint on the mean, but we must ﬁrst assign m(N).
From our discussion of the Lebesgue measure in Section 5.2.2, m(N) is propor-
tional to the pdf which reﬂects gross ignorance about the details of the situation. Given
only that there are M trials, the principle of indifference tells us to assign equal proba-
bility to each of the 2M possible outcomes. The number of different ways of obtaining
N successes in M trials, or MCN in eqn (5.8), is therefore an appropriate measure for
this problem:
m(N) =
M!
N! (M −N)! .
(5.43)
All we need to do now is to substitute this into eqn (5.42), and to impose the constraints
of normalization and eqn (5.41). The related algebra is simpliﬁed by noting that
M

N=0
m(N) e−λN =

e−λ + 1
M ,
(5.44)
which follows from eqn (5.9) on putting a = e−λ and b = 1. Its reciprocal yields the
constant of proportionality in eqn (5.42), and its implicit differentiation with respect to
λ, giving
M

N=0
N m(N) e−λN = M

e−λ + 1
M−1e−λ ,
allows eqn (5.41) to be reduced to M

1 + eλ−1 = µ. A little algebraic rearrangement
then shows eqn (5.42) to be a binomial pdf:
prob(N |M,µ) =
M!
N! (M −N)!
 µ
M
N 
1 −µ
M
M−N
.
(5.45)

Approximations: interconnections and simpliﬁcations
121
5.3.4
Counting and Poisson statistics
Many problems in science involve the counting of discrete events in a ﬁnite interval.
These can include a temporal aspect, such as the number of neutrons (or X-ray photons,
or buses, or accidents) detected in a given amount of time, or a spatial component, like
the number of quasars in a certain patch of the sky. What should we assign for the
probability of observing N events, given only the expected value ⟨N ⟩=µ?
This case is actually very similar to the preceding one, because the situation can
be regarded as the M →∞limit of Section 5.3.3. That is to say, we can imagine the
ﬁnite interval of interest to be made up of a huge number of microscopic sub-intervals
within each of which only a single event could occur. We will show how the binomial
pdf of eqn (5.45) tends to a Poisson pdf as M→∞in the next section, but let’s see how
the result can be obtained directly through MaxEnt by using the large-M limit of the
measure in eqn (5.43):
m(N) = M N
N! .
(5.46)
The substitution of this measure into eqn (5.42) gives
prob(N |µ) = A

M e−λN
N!
,
(5.47)
where A is a normalization constant; the imposition of this constraint gives
∞

N=0
prob(N |µ) = A eM e−λ = 1 ,
where the summation has been carried out by noting that it is equivalent to the Taylor
series expansion of exp(X), for X =M e−λ. This allows eqn (5.47) to be written as
prob(N |µ) =
1
eM e−λ

M e−λN
N!
.
(5.48)
We must also satisfy the requirement that ⟨N ⟩= µ, and this leads to the condition
M e−λ = µ. Substituting this into eqn (5.48), we obtain the Poisson distribution
prob(N |µ) = µN e−µ
N!
,
(5.49)
which is independent of the exact degree of subdivision of the original interval; in using
the measure of eqn (5.46), we have implicitly assumed that it is large. The pdf of eqn
(5.49) was illustrated in Fig. 3.2, for two different values of µ.
5.4
Approximations: interconnections and simpliﬁcations
The Poisson distribution can also be derived as the limiting form of a binomial pdf,
where the probability of success in any given trial is very small but the number of
attempts is extremely large. The probability of obtaining N successes in M trials, where

122
Assigning probabilities
the expected number of favourable outcomes ⟨N ⟩= µ, is given in eqn (5.45). As M
becomes very large, M!/(M−N)! can be approximated by M N and the M−N in the
exponent of (1−µ/M) can be replaced by just M:
prob(N |M,µ) ≈M N
N!
 µ
M
N 
1 −µ
M
M
.
After cancelling the factors of M N, the term on the far right can be recognized as one
which tends to e−µ as M →∞; this is easily veriﬁed by considering its logarithm:
loge

1 −µ
M
M 
= M loge

1 −µ
M

= −µ −µ2
2M −
µ3
3M 2 −· · · ,
where we have used the Taylor series expansion of log e(1−X), for X =µ/M, in the
last step. Thus, we obtain the Poisson distribution of eqn (5.49).
This alternative derivation is useful because it tells us that the binomial pdf can be
approximated by the Poisson formula
prob(r|N, p) =
N!
r! (N −r)! pr (1−p)N−r ≈(Np)r e−Np
r!
,
(5.50)
if p is small but N is large. As a simple example, consider the case of a fruit grower
who ﬁnds that 1 in 50 apples picked are bad: If 100 are sent out without sorting, what
is the probability that r will be rotten? Since an individual apple can either be good or
bad, this situation can be treated as a binomial problem with N = 100 and p = 0.02;
from eqn (5.50), it can also be approximated by a Poisson pdf with µ = Np = 2. The
corresponding results given by the two pdfs, for r = 0, 1, 2, . . ., 7, are shown in Table
5.2; as expected, there is good agreement between the two formulae.
Pursuing the theme of simpliﬁcations a little further, we ﬁnd that the Poisson pdf
can be approximated by a Gaussian expression if the average value µ is large. To see
this, consider the logarithm of eqn (5.49):
Table 5.2 The Poisson approximation to the binomial pdf: the probability of ﬁnding r rotten
apples in a carton of 100, given that 1 in 50 tend to be bad.
Number of rotten apples
Binomial probability
Poisson approximation
r
prob(r|N =100, p = 0.02)
prob(r|µ = 2)
0
0.1326
0.1353
1
0.2707
0.2707
2
0.2734
0.2707
3
0.1823
0.1804
4
0.0902
0.0902
5
0.0353
0.0361
6
0.0114
0.0120
7
0.0031
0.0034

Approximations: interconnections and simpliﬁcations
123
L = loge

prob(N |µ)

= N loge(µ) −µ −loge(N!) .
(5.51)
Since the most probable values of N will be in the vicinity of µ, it is appropriate to use
Stirling’s formula if µ ≫1:
loge(N!) ≈N log e(N ) −N + 1
2 loge(2πN) .
The last term on the right can be ignored in the limit N→∞, as we did earlier, but its
inclusion ensures a good approximation even for fairly small N; substituting this for
loge(N!) in eqn (5.51), we obtain
L ≈N −µ −N log e(N/µ) −1
2 loge(2πN) .
(5.52)
To investigate the behaviour of L in the neighbourhood of µ, let’s put N = µ + ϵ and
consider the case |ϵ| ≪µ. After a little rearrangement, L can be written as
L ≈−1
2 loge(2πµ) + ϵ −

µ + ϵ + 1
2

loge

1 + ϵ
µ

.
A Taylor series expansion of log e(1 + ϵ/µ) then yields
L ≈−1
2 loge(2πµ) −ϵ2
2µ −
ϵ
2µ + · · · ,
where only the most signiﬁcant terms have been retained. The variation of L in the
vicinity of N ≈µ can ascertained by completing the square for ϵ; its exponential yields
the desired Gaussian expression:
prob(N |µ) = µN e−µ
N!
≈
1
√2πµ exp
	
−

N −µ + 1
2
2
2µ

,
(5.53)
for large µ. This tendency towards a normal distribution is evident in Fig. 3.2, even
for µ = 12.5; the correspondence improves as the average value gets bigger. If we had
carried out the analysis above by substituting for µ instead of N in eqn (5.52), then we
would have been led to an alternative formula:
µN e−µ
N!
≈
1
√
2πN
exp
	
−(µ −N )2
2N

,
(5.54)
which too is valid in the limit of a large number of counts (N ≈µ ≫1).
In the Gaussian approximation to the Poisson pdf of eqn (5.53), it can be seen that
the variance of N is equal to µ; this is, in fact, a general property of the Poisson distri-
bution. That is to say, it can be shown that prob(N |µ) in eqn (5.49) satisﬁes

(N −µ)2
=
∞

N=0
(N −µ)2 prob(N |µ) = µ ,
(5.55)

124
Assigning probabilities
irrespective of the magnitude of µ. A calculation similar to that leading to eqn (5.53),
but starting from eqn (5.15), yields the result
prob(r|N ) =
N!
r! (N −r)! pr(1−p)N−r ≈
1
σ
√
2π exp
	
−(r −µ)2
2 σ2

,
(5.56)
where µ =Np≫1 and σ2 =Np(1−p)≫1. In other words, a binomial pdf can also
be approximated by a Gaussian if both the mean and variance in eqn (5.16) are large.
This was ﬁrst shown by de Moivre (1733), for the case p = 1/2, and later by Laplace
in greater generality; it can be veriﬁed pictorially from Fig. 5.1. The validity of eqns
(5.53), (5.54) and (5.55) is often the justiﬁcation for the use of a least-squares likelihood
function, even when the most appropriate pdf is not intrinsically Gaussian.
5.5
Hangups: priors versus likelihoods
In Section 1.4, we noted that Laplace’s approach to probability was rejected by many,
soon after his death, on the grounds that a degree of belief was too vague a concept
to be the basis of a rigorous theory. Even today, a discussion of Bayesian methods is
usually qualiﬁed by the words ‘subjective probabilities’. Like Jaynes, we believe that
this notion is misguided. To explain this, let us try to get to the heart of the matter.
The main point of concern tends to be the choice of the prior pdf: What should we
do if this is not known? This issue is often regarded as a major obstacle for the gen-
eral use of the Bayesian approach and is cited as proof of its subjective nature. From
our point of view, however, the question is itself rather strange. No probability, whether
prior, likelihood or whatever, is ever ‘known’; it is simply an assignment which reﬂects
the relevant information that is available. Thus prob(x|I1) ̸= prob(x|I2), in general,
where the conditioning statements I1 and I2 are different. Nevertheless, objectivity can,
and ought to, be introduced by demanding that two people with the same information I
should assign the same pdf; this requirement of consistency has been the central theme
of this chapter. We have seen how invariance arguments, under the appropriate transfor-
mation groups, can often determine a pdf uniquely, when given only the nature of the
quantities concerned; MaxEnt provides a powerful extension, and allows us to deal with
the case of testable constraints. The point is that nowhere in Sections 5.1–5.4 have we
explicitly differentiated between a prior and a likelihood; we have merely considered
how to assign prob(X|I) for different types of I. If X pertains to data, then we can
call prob(X|I) a likelihood; if neither X nor I refer to (new) measurements, then we
may say it’s a prior. The distinction between the two cases is one of nomenclature, and
not that of objectivity or subjectivity. If it appears otherwise, then this is because we are
usually prepared to state the conditioning assumptions for the likelihood function but
shy away from doing likewise for the prior pdf.
5.5.1
Improper pdfs
At a technical level, the issue of improper priors is often quoted as a serious impediment
for the general use of the Bayesian approach; that is to say, some pdfs which represent
complete ignorance cannot be normalized. The Jeffreys’ prior is most frequently cited

Hangups: priors versus likelihoods
125
as an example of the difﬁculty: given that a scale parameter can lie in the range 0 to ∞,
there is no proportionality constant which will make the integral of the pdf in eqn (5.18)
equal to unity. How can we get around this problem?
When faced with such a dilemma in an ordinary mathematical setting, we tend to
excise the source of the difﬁculty and consider how the result from the remaining part
evolves as the excluded quantity is gradually readmitted. We can do the same thing here:
carry out the calculation for a proper prior, with a lower and upper bound of A and B,
and investigate how the posterior pdf is affected as these limits approach the improper
values. For the case of a Jeffreys’ prior, for example, we would use
prob(L|I) =





1
loge(B/A) × 1
L
for A ⩽L < B ,
0
otherwise ,
where A > 0 and B is ﬁnite, multiply it by the likelihood prob(data|L, I), normalize
the resulting posterior pdf, and consider what happens as A →0 and B →∞. While
the prior itself might diverge with these limits, the posterior prob(L|data, I) is usually
well-behaved. If the conclusions drawn from the posterior do depend strongly on the
values of A and B, then probability theory is warning us that there is very little relevant
information in the data; it is then not surprising that our prior knowledge, which must
deﬁne these limits, should play a crucial rˆole in the inference procedure.
After going through the formalities for handling improper priors, we often ﬁnd that
the same result would have been obtained if the unnormalized pdf (e.g. 1/L) had been
substituted directly into Bayes’ theorem; while not wishing to encourage such compla-
cency, the temptation for taking this short cut is understandable. In fact, we do this every
time the maximum likelihood method, or the least-squares procedure, is used: both rely
on the implicit use of a uniform prior, which cannot be normalized without deﬁning a
suitable range. In general, for parameter estimation, improper priors merely constitute
a technical inconvenience rather than a serious difﬁculty. For model selection, however,
the situation is quite different. As we saw in Chapter 4, the prior pdf must be normalized
properly in that case (by setting suitable bounds if necessary) because the evaluation of
the probabilistic evidence entails an averaging of the likelihood function over it.
5.5.2
Conjugate and reference priors
In order to address the concerns voiced over the subjectivity of the prior pdf, several
procedures have been proposed for their ‘automatic’ assignment based on the nature of
the likelihood function. For example, conjugate priors, which ensure that the functional
form of the resultant posterior pdf is the same as that of the prior (e.g. see Bernardo and
Smith 1994), have been advocated. While these are motivated by the pragmatic desire
to have simpliﬁed algebra, an information-based criterion, leading to reference priors,
has been put forward by Bernardo (1979). The idea is that, ‘even for small sample sizes,
the information provided by the data should dominate the prior information, because of
the “vague” nature of the prior knowledge.’
Although the development of conjugate and reference priors can seem attractive,
because they provide ‘off the shelf’ pdfs, we have some basic misgivings about them.

126
Assigning probabilities
The principal concern is that they perpetuate the myth that the likelihood function, un-
like the prior pdf, is inherently objective. From our point of view, this is not so: both are
simply assignments that reﬂect states of knowledge. While we might expect our initial
understanding of the object of interest to have a bearing on the experiment we conduct,
it seems strange that the choice of the prior pdf should have to wait for, and depend in
detail upon, the likelihood function.

Part II
Advanced topics
‘Data analysis is simply a dialogue with the data.’
— Stephen F. Gull, Cambridge 1994

This page intentionally left blank 

6
Non-parametric estimation
In Chapters 2 and 3, we dealt with problems which entailed the estimation of the values
of a few well-deﬁned parameters; the analysis was then extended, in Chapter 4, to cover
the case where there was some doubt as to their optimal number. Now let’s move on
to the most difﬁcult situation, in which we want to draw inferences about an object of
interest but don’t have enough information, or conﬁdence, to be able to characterize it by
a speciﬁc type of functional model. This is a rather open-ended question that requires
us to think of both a suitable description and any weak constraints which might be
appropriate for a ﬂexible ‘free-form’ solution.
6.1
Introduction: free-form solutions
Many data analysis problems in science can be stated in the following general terms:
Given a set of data {Dk}, and the relevant background information I, what can we
say about the object of interest f(x)? Here the variable x is used rather loosely and
could refer to a coordinate that was discrete, continuous or even multidimensional. For
example, an astronomer might want to know the radio-brightness distribution of the
two-dimensional sky, having measured the (complex) visibilities with an interferometer;
a materials scientist could be faced with the task of estimating the three-dimensional
preferred-orientation distribution of crystallites in a processed metal, given the number
of neutrons that a sample scatters through various angles. As we have noted from the
outset, our inference is summarized by the posterior pdf prob[f(x)|{Dk}, I ]; before we
can even think about assigning the related prior and likelihood, however, we must ﬁrst
decide on a suitable way of describing f(x). There are essentially three possibilities; let
us consider each in turn.
If we know a lot about the object of interest, then we can often characterize it by a
few speciﬁc parameters. Taking f(x) to be a one-dimensional spectrum, for the sake of
argument, a simple example would be:
f(x) = a1 δ(x−x1) + a2 δ(x−x2) ,
(6.1)
where δ(x−x∗) is an inﬁnitely sharp spike, of unit area, at x=x∗. Our inference about
f(x) is, therefore, embodied in the posterior pdf for the positions and amplitudes of the
two δ-functions in eqn (6.1): prob(a1, a2, x1, x2|{Dk}, I). Such problems of parameter
estimation were the subject of Chapters 2 and 3; with the usual approximations for the
prior and likelihood, they frequently reduce to a least-squares analysis.
The next situation arises when we don’t know enough about f(x) to be able to
specify a particular functional model unambiguously, but can restrict ourselves to a
small set of possibilities. A chemist concerned with studying the rotational excitations

130
Non-parametric estimation
of a molecular compound, for example, could use quantum mechanical arguments to
decide that the spectrum of interest should consist of a few sharp peaks; nevertheless,
there may be some uncertainty as to their expected number. For illustrative purposes,
eqn (6.1) could then be generalized so that f(x) is deﬁned by a sum like
f(x) =
M

j=1
aj δ

x−xj

,
(6.2)
where it is only known that M is a small integer (⩽10, say). The problem of estimating
the optimal value of M requires the evaluation of the posterior pdf prob(M |{Dk}, I);
this task, and model selection in general, was the subject of Chapter 4.
The most difﬁcult case occurs when we know so little about the object of interest
that we are unable to characterize it by a functional model. This is a rather awkward
situation because we cannot communicate quantitatively about f(x) without some form
of parametric description. Given this limitation, we must try to choose a formulation
that ensures a lot of ﬂexibility in the allowed solutions. This usually means that f(x)
has to be deﬁned by a very large number of variables, which leads to problems of its
own. In particular, a na¨ıve uniform prior pdf is no longer adequate when there are more
parameters to be inferred than data. To take a trivial example, there is no unique best
estimate for X and Y when we are only told that X + 2 Y = 3; the ambiguity would
be reduced to a ﬁnite interval if we knew that both X and Y were positive, and could
be eliminated completely if we had reason to believe that they tended to be of the same
magnitude. Thus, in order to obtain a satisfactory free-form solution, we are forced to
think about any weak constraints that might be appropriate for f(x). These can either be
encoded through the use of a suitable non-uniform prior or implemented by a judicious
choice of the description of f(x), or a combination of the two; the bulk of this chapter is
devoted to illustrating how this can be done. Before that, however, let us examine a sim-
ple ‘non-parametric’ set-up more closely to get a better appreciation of the difﬁculties
associated with having too few data.
6.1.1
Singular value decomposition
Perhaps the most straightforward way of obtaining a free-form description of a one-
dimensional spectrum f(x) is to divide the x-axis into tiny pieces and consider the
(average) value of f in each of these regions. This procedure is shown schematically
in Fig. 6.1, and is equivalent to expressing f(x) as a sum of many, slightly displaced,
narrow rectangles. Ignoring the ﬁnite width of these columns, f(x) is deﬁned mathe-
matically as in eqn (6.2), except that M is now large and the locations of the δ-functions
{xj} are no longer unknown variables; they are ﬁxed by a relationship such as
xj = xmin +
xmax −xmin
M

j −1
2

,
(6.3)
for j = 1, 2, . . ., M. Our inference about f(x) is, therefore, encapsulated in the poste-
rior pdf for the amplitude parameters: prob

{aj}|{Dk}, I

. This division of a contin-
uous function into a set of discrete pixels can be generalized to two, three and higher

Introduction: free-form solutions
131
Fig. 6.1 A schematic illustration of the way a function f(x) can be described by M amplitudes
{aj} of a set of discrete pixels centred at {xj}.
dimensions: we just have (rectangular) elements of area and volume, instead of length.
In fact, this discretization is nothing more than the standard brute force and ignorance
way of doing calculations numerically on a computer.
Having set up a free-form description in Fig. 6.1, let’s turn our attention to the char-
acteristics of the likelihood function prob

{Dk}|{aj}, I

. If this has a well-deﬁned
maximum, when viewed in the M-dimensional space of the {aj}, then a na¨ıve uniform
prior will be sufﬁcient to yield a unique best estimate for f(x); if it does not, then we
will have to think harder about a suitable assignment for prob

{aj}|I

. To keep the
analysis simple, let us work with the exp(−χ2/2) likelihood function of Section 3.5:
loge

prob

{Dk}|{aj}, I

= constant −1
2
N

k=1
Fk−Dk
σk
2
,
(6.4)
where σk is the (known) error-bar for the kth datum Dk , and Fk is the corresponding
prediction based on the given amplitude parameters {aj}. We will also take it that the
data are related linearly to f(x) so that Fk can be written as
Fk =
M

j=1
Tkj aj + Ck ,
(6.5)
where the elements of the matrix T and vector C are determined by the nature of the
experiment but are independent of the values of the {aj}. For example, Tkj might be
equal to the sine, or cosine, of 2πjk/M for interferometric measurements and C could
represent a slowly varying background signal.
Within the congenial conditions provided by eqns (6.4) and (6.5), the shape of the
likelihood function is elliptical in nature. As such, the illustration of Fig. 3.6 represents
the special case of M = 2 (with X = a1 and Y = a 2). Although ellipsoids of higher
dimensions are difﬁcult to draw, they too are characterized completely by the direc-
tions and widths of their principal axes. These are formally given by the eigenvectors
{el} and eigenvalues {λl}, respectively, of the M ×M matrix of second-derivatives of
loge

prob

{Dk}|{aj}, I

; ignoring a factor of −1/2, the latter is just ∇∇χ2. Hence,
the orientation and spread of the likelihood function can be ascertained by (numerically)
solving the simultaneous equations

132
Non-parametric estimation
∇∇χ2 el = λl el ,
(6.6)
for l =1, 2, . . ., M, where the ijth element of the Hessian matrix is given by

∇∇χ2
ij =
∂2χ2
∂ai ∂aj
= 2
N

k=1
Tki Tkj
σk2
.
(6.7)
Since the width of the probability ellipsoid along its principal axes is inversely propor-
tional to the (square-root of the) corresponding eigenvalues, we would ideally like all
the λ’s to be large. By the same token, the presence of λ’s with small values warns us
that the maximum of the likelihood function is very shallow. Indeed, it is not deﬁned
(uniquely) if any of the eigenvalues is equal to zero, because there are then some di-
rections in which prob

{Dk}|{aj}, I

is absolutely ﬂat. As it can be shown that this
situation necessarily arises when the number of parameters to be inferred is greater that
the number of data (M >N), it follows that a uniform prior pdf is not generally adequate
for obtaining an unambiguous best solution for f(x) in that case.
The occurrence of extremely small eigenvalues (λ∼0) is really the hallmark of the
free-form solution problem, and can arise even when the number of data is large enough
that M ⩽N. The easiest way to check for them is to test whether the determinant of the
∇∇χ2 matrix is (almost) equal to zero, because the latter is given by the product of the
{λl}. Since a matrix whose determinant is zero is said to be singular, an analysis of its
eigenproperties is called singular value decomposition. This can be a useful exercise as
it enables us to ascertain which aspects of f(x) can, or cannot, be inferred reliably from
the data; let us elaborate on this further.
Consider the two-dimensional example of Fig. 3.6, with parameters X and Y . If the
principal directions of the ellipse lay along the coordinate axes, then the related eigen-
vectors would be: e1 = (1, 0) and e2 = (0 , 1). The corresponding widths of the pdf,
which are proportional to the reciprocal of √λ1 and √λ2 , would clearly indicate the
uncertainties associated with our best estimates of X and Y respectively. Now suppose
that the ellipse is skew, as drawn in Fig. 3.6. The description of the reliability of the
optimal inference is then not quite so straightforward, because the estimates of X and
Y are correlated. For example, the data may tell us a lot about the sum X +Y but almost
nothing about the difference X−Y ; in that case, the marginal error-bars σX and σY will
both be huge but the sum-constraint means that typical estimates X and Y will not be
independent of each other. While this behaviour can be conveyed through a covariance
matrix, as noted in Section 3.2, the same information is also contained in the eigenvec-
tors and eigenvalues. This is because, for such a negative correlation, e1=(1, −1) and
λ1∼0, indicating that the difference X−Y is poorly determined, and e2=(1, 1) and λ2
is large, showing that the sum X+Y can be estimated with a high degree of conﬁdence.
More generally, an eigenvector with components (α, β) refers to the quantity αX+βY ;
the corresponding eigenvalue tells us how reliably it can be inferred. For our multivari-
ate case, the M elements of the various el’s pertain to different linear combinations of
the {aj}; since the principal axes of an ellipse are mutually perpendicular, these are
the quantities which are uncorrelated and can be estimated independently of each other.

Introduction: free-form solutions
133
Fig. 6.2 Some of the eigenvectors of the ∇∇χ2 matrix for a simple convolution problem, pre-
sented in order of decreasing eigenvalues; the calculation was done on a grid of 64 points. Rather
than using the discrete ‘stick’ representation of Fig. 6.1, the vertices of adjacent amplitudes have
been joined together to give a continuum impression.

134
Non-parametric estimation
Hence, the eigenvectors with large λ’s represent aspects of f(x) which can be inferred
reliably from the data; those with small λ’s indicate areas of great uncertainty.
As a concrete example of a singular value analysis, let’s look at a convolution prob-
lem like the one illustrated in Fig. 4.2; in such a case, the experimental data are a blurred
and noisy version of the object of interest. If the shape of the resolution (or blurring)
function was Gaussian, with width w, then the elements of the T matrix in eqn (6.5)
would take the form
Tkj =
1
w
√
2π exp
	
−

xk −xj
2
2 w2

,
where xk and xj are the x-coordinates of the kth datum and jth pixel in the free-form
representation of f(x), respectively. Taking both the {aj} and the {Dk} to be deﬁned on
the same (uniform) grid of 64 points, so that M =N =64, and all the error-bars in eqn
(6.7) to be unity (σk=1), some of the eigenvectors of the corresponding ∇∇χ2 matrix
are shown in Fig. 6.2; they have been ordered by the magnitude of their eigenvalues,
with λ1 > λ2 > λ3 and so on. Each of the el’s is, of course, a collection of 64 numbers:
the components of a vector along the M parameter-axes, referring to a particular linear
combination of the {aj}. Rather than representing them by a set of discrete amplitudes
for the various pixels, as in Fig. 6.1, adjacent vertices have been joined together to
give a continuum impression in Fig. 6.2. Although the results plotted are for w = 1
pixel, the eigenvectors do not change very much with the blur width. In addition to their
general oscillatory nature, the most important feature of the el’s is that those associated
with large eigenvalues vary slowly with x while the ones having small λ’s are highly
structured. This indicates that it is the gross, or low-frequency, features of f(x) which
can be inferred most reliably and emphasizes that information about the ﬁne detail is
lost in a convolution process. The spectra of eigenvalues for w=1 and w=5 pixels are
shown in Fig. 6.3; since the latter has many more small λ’s, it conﬁrms our expectation
that the problem becomes more acute as the blur width increases.
Fig. 6.3 The eigenvalue spectrum {λl}, for the eigenvectors in Fig. 6.2, plotted on: (a) a linear
scale; (b) a logarithmic scale. The squares and solid line are for a blur-width of w = 1 pixel; the
circles and dotted line are for w=5 pixels.

Introduction: free-form solutions
135
6.1.2
A parametric free-form solution?
From our discussion so far, we have seen that a non-parametric solution for f(x) actu-
ally entails the inference of a very large number of parameters. Although this does not
pose any problem of principle, it does mean that we have to think much harder about
a suitable non-uniform prior in order to obtain a satisfactory solution. An easy way of
avoiding this inconvenience is to use a free-form description which uses fewer vari-
ables. We have already met one method of doing this in the guise of the Taylor series
expansion, whereby a potentially complicated function is approximated (locally) by a
low-order polynomial:
f(x) = c1 + c2(x−xo) + c3(x−xo)2 + c4(x−xo)3 + · · · .
(6.8)
As long as the number of coefﬁcients {cl} used is small enough, their estimation fre-
quently reduces to a straightforward exercise in least-squares ﬁtting; if necessary, we
could carry out a model selection analysis to determine the optimal expansion order (as
in Gull 1988). Even though a power series may not provide the best approximation for
the spectrum of interest, eqn (6.8) illustrates how an arbitrary function can be expressed
as a linear combination of Mb basis functions {ηl(x)}:
f(x) =
Mb

l=1
cl ηl(x) .
(6.9)
The Taylor series, therefore, represents the special case ηl(x) = (x−xo)l−1. Depend-
ing on the nature and the symmetry of the problem, Chebyshev polynomials, spherical
harmonics, sinusoids, wavelets and many other functions have been found to be useful.
The description of f(x) in eqns (6.2) and (6.3) is also a special case of eqn (6.9), with
ηl(x)=δ(x−xl); the trick here, however, is to choose a set of basis functions carefully
so that Mb can be made small. Although the aim is to reduce the analysis to simple
parameter estimation, it can still be considered a free-from solution in the sense that
the coefﬁcients {cl} don’t directly correspond to meaningful quantities in terms of a
particular physical model.
In many ways, the eigenvectors of the ∇∇χ2 matrix provide a natural choice for
the expansion of eqn (6.9); in the continuum limit, they are called eigenfunctions:
ηl(x)=el(x). Those associated with large eigenvalues represent aspects of f(x) which
are determined well by the experimental measurements, while the ones with small λ’s
make almost no contribution towards the ﬁtting of the data; this insensitivity of the latter
means that they can be omitted from the summation. In terms of our discrete formulation
of Fig. 6.1, the spectrum of interest then becomes
f(xj) = aj =
Mb

l=1
cl el(xj) ,
(6.10)
for j = 1, 2, . . . , M, where el(xj) is the jth component of the lth eigenvector in eqn
(6.6); the sum is only over those el’s that have large λl’s, so that typically Mb ≪M.

136
Non-parametric estimation
After some effort, it can be shown that the optimality condition ∇χ2 = 0 and the
covariance matrix 2

∇∇χ2−1 lead to the result
cl = 2
λl
N

k=1
Dk
σk2
M

j=1
Tkj el(xj) ±
 
2
λl
.
(6.11)
In the derivation of this formula for the coefﬁcients {cl}, it is useful to remember that
the eigenvectors of a real symmetric matrix (like the Hessian ∇∇χ2) are orthogonal;
by convention, they are also normalized so that
M

j=1
el(xj) em(xj) =

1
if l = m,
0
otherwise.
Although a singular value decomposition analysis allows us to circumvent the technical
difﬁculties of handling very small eigenvalues, it does not really address the central issue
of the inﬁnite uncertainties inherent in seeking a free-form solution. This is because the
addition of any multiple of an eigenvector with λ=0 in eqn (6.10) changes the estimate
of f(x), but not the ﬁt to the data prob[{Dk}|f(x), I ]. To break this ambiguity, and
have a unique maximum for the posterior pdf prob[f(x)|{Dk}, I ], we must ponder
over whatever little information I contains about f(x) and assign an appropriate non-
uniform prior prob[f(x)|I ].
6.2
MaxEnt: images, monkeys and a non-uniform prior
Suppose we knew that the object of interest constituted a positive and additive distribu-
tion, and almost nothing else. By this we mean that f(x) ⩾0 and its integral (or sum)
over any region represents a physically meaningful and important quantity. This is so
for the radio-ﬂux distribution of an astronomical source, the electron density of a crys-
tal, the intensity (but not amplitude) of incoherent light in an optical image and many
other situations. What prior best encodes our limited state of knowledge?
Well, since the properties under discussion also hold for pdfs, the principle of max-
imum entropy encountered in Chapter 5 may be helpful to us here as well. In particular,
an easy way of thinking about distributions that are positive and additive is to imagine
that they are the result of a team of monkeys scattering balls of ‘stuff’ at random into
a collection of boxes; if the latter are arranged in a two-dimensional grid, and the little
blobs being thrown are luminescent, then we could generically call the end product an
image. While no distribution of interest is actually generated in this manner, the mech-
anism is a good representation of our state of ignorance. As such, it seems reasonable
that we should assign the highest (initial) probability to those f(x)’s which our hypo-
thetical apes come up with most often, and the least to the ones that are rarest. Hence,
following the discrete formulation in Fig. 6.1, eqn (5.28) suggests that a suitable prior
pdf might be
prob

{aj}|{mj}, α, I

∝exp

αS

,
(6.12)

MaxEnt: images, monkeys and a non-uniform prior
137
Fig. 6.4 The contours along which exp(αS) is constant, as a function of the amplitude ai and
al in two pixels i and l; the Lebesgue measure is uniform, so that mi = ml. In (a) α =1, while
in (b) α =10.
where α is a constant and S is the entropy of the {aj} relative to the Lebesgue measure
{mj}. According to Skilling (1989, 1991), the Shannon–Jaynes expression for S should
be generalized to
S =
M

j=1

aj −mj −aj log
 aj
mj

,
(6.13)
if the distributions involved are not normalized; it reduces to the form in eqn (5.28)
when  aj =  mj = 1, and to the even more basic − aj log(aj) formula of eqn
(5.26) if the measure is also taken as being uniform (so that mj = 1/M). The pdf of
eqns (6.12) and (6.13) is illustrated in Fig. 6.4. It has a maximum at {aj}={mj}, and
decreases rapidly to zero as any of the {aj} tries to become negative. The width of the
pdf is controlled by the parameter α which, in terms of the analysis of Section 5.2, is
equivalent to the total number of balls (or pennies etc.) thrown by the monkeys: the
larger the value of α, the sharper the entropic prior.
Using the assignment in eqn (6.12), and the least-squares likelihood function of eqn
(6.4), we obtain the posterior pdf:
prob

{aj}|{Dk}, {mj}, α, I

∝exp

αS −χ2
2

.
(6.14)
This follows from Bayes’ theorem, with α and {mj} given throughout, as the data de-
pend only on the {aj}: prob

{Dk}|{aj}, {mj}, α , I

= prob

{Dk}|{aj}, I

. With
the extraneous factors in the conditioning statement, this isn’t quite the pdf we were af-
ter! Although Gull (1989a) tried to deal with α properly through marginalization, Sibisi
(1996) and Skilling (1998) have concluded that the MaxEnt prior of eqn (6.12) has an
inherent ﬂaw: the results are dependent of the degree of pixelation (or choice of M).
This shortcoming had been missed earlier due to a deceptive side-effect of the Gaussian
approximation made in the calculation, and because the quantitative answers from the

138
Non-parametric estimation
analysis (Skilling 1990) were generally sensible in practice (Sibisi 1990, Sivia and Web-
ster 1998). Rather than abandoning the idea of MaxEnt image processing altogether, we
can make use of its helpful properties by adopting the pragmatic, if qualitative, approach
of regularization for this difﬁcult problem; a probabilistic formulation is discussed in
Chapter 10, where more advanced computational techniques are described.
6.2.1
Regularization
Since the ‘most probable’ estimate for the {aj} in eqn (6.14) is given by the maximum
of α S −χ2/2 , this condition can be regarded as a constrained minimization of χ2
where α is a Lagrange multiplier. From this viewpoint, S is seen as a regularization
function which helps to stabilize the least-squares procedure for a free-form solution;
many alternatives have been proposed for this purpose (Tikhonov and Arsenin 1977).
The most popular choice is, perhaps, the quadratic penalty  (aj −mj)2, which offers
the advantages of analytical simplicity; others include nearest-neighbour smoothness
criteria, such as  (aj+1−aj)2, and the Burg (1967) entropy  log(aj/mj). Within
a probabilistic framework, the various functions would be equivalent to different non-
uniform priors with each reﬂecting whatever limited information there was available.
Some of the earliest examples of image processing with MaxEnt regularization were in
astronomy (Frieden 1972, Gull and Daniell 1978), and a classic case from a forensic
study is reproduced in Fig. 6.5 (Gull and Skilling 1984).
The task of solving the large set of simultaneous equations
∂
∂aj

αS −χ2
2

= 0 ,
(6.15)
for j = 1, 2, . . . , M, can be a demanding numerical exercise. This is particularly so if
the number of variables involved is much greater than a couple of hundred, since the
(a)
(b)
Fig. 6.5 (a) Data consisting of a photograph, where the picture of interest has been blurred by the
movement of the camera. (b) The MaxEnt estimate of the underlying scene.

MaxEnt: images, monkeys and a non-uniform prior
139
computation time for matrix manipulations tends to scale as the third power of the size
of the problem (∼M 3 ). Skilling and Bryan (1984) describe a good general-purpose
algorithm for this optimization, with an elaborate iterative scheme at its core. The pro-
cedure begins at the global maximum of entropy, {aj} = {mj}, which is the estimate
of f(x) returned in the absence of any data; in this context, the Lebesgue measure is
often called the default model. The local gradients of S and χ2 are then consulted, to
ascertain the best small change {∆aj} which will reduce the misﬁt with the data while
keeping the entropy as large as possible. This is done repeatedly until the desired solu-
tion is obtained, 2α∇S =∇χ2, with χ2 ≈N (the number of measurements) to ensure
a respectable agreement with the data; it is illustrated schematically in Fig. 6.6.
There are two important conditions that must be satisﬁed if the algorithm outlined
above is to converge satisfactorily. The ﬁrst is that the likelihood function must not be
multimodal. If it is, then the posterior pdf will inherit the unfortunate aspects of that
topology; and all the grief it entails (c.f. Sections 3.2 and 3.4)! A notorious example
of this situation concerns the case of phaseless Fourier data which bedevils crystallo-
graphers, amongst others. Luckily, the likelihood function for many problems is well-
behaved, as it can be characterized by eqns (6.4) and (6.5). This is implicitly assumed
in Fig. 6.6, because the contours of χ2 have been drawn as being elliptical. Given the
convex nature of both S and χ2 under these circumstances, so that they can only touch
each other at a single point, the MaxEnt solution is unique and ‘easy’ to ﬁnd.
The second condition for satisfactory convergence stems from a more practical con-
sideration. It concerns the maximum change in the image {∆aj} allowed between one
iteration and the next. This is because the local gradients of S and χ2, upon which the
update is based, are a poor guide to the long-range behaviour of αS −χ2/2. In order
to impose a limit on the size of the permissible change, we must ﬁrst decide on how
the ‘step-length’ ∆l is to be deﬁned. Skilling and Bryan (1984) have found that a good
Fig. 6.6 A schematic illustration of a MaxEnt trajectory. We start at the default model m, and
take small steps in such a way as to reduce the misﬁt with the data (χ2) while keeping the entropy
S as large as possible.

140
Non-parametric estimation
choice is
∆l 2 =
M

j=1
∆aj
2
aj
,
(6.16)
where ∆aj is the small increment in the amplitude of the jth pixel. A little reﬂection
soon reveals the beneﬁts of this non-Euclidean choice, with the extra aj in the denomi-
nator: a small change in a pixel with a low ﬂux (e.g. ∆aj =0.1 for aj =1) is comparable
to a large deviation for one with a big amplitude (e.g. ∆aj = 1 for aj = 100). This
property is useful for avoiding the inadvertent development of negative amplitudes. Al-
though higher powers of aj in the denominator would also have this effect, they tend to
be unnecessarily conservative and, hence, less efﬁcient.
For some types of problems, such as those where the object of interest is related to
the data through a Laplace transform, the general-purpose algorithm outlined above is
not very efﬁcient. Bryan (1990) has developed an alternative procedure speciﬁcally for
these cases, where the likelihood function only has a handful of good eigenvalues.
6.3
Smoothness: fuzzy pixels and spatial correlations
Maximum entropy regularization is a good way of encoding our prior knowledge that
f(x) is positive quantity, whose integral represents the amount of ‘important stuff’ in
any region. Often, however, we have reason to believe that the object of interest is
also locally smooth. How can we build this into the analysis? This question may seem
strange at ﬁrst, since MaxEnt images are frequently described as being the smoothest
solutions consistent with the data. While it is true that our hypothetical team of monkeys
will generate featureless maps far more often than highly structured ones, this is not
quite the same thing as local smoothness. In some ways, the concepts are incompatible:
the entropic formula arises from saying that the next ball is equally likely to be dropped
into any bin, irrespective of the number that are already there; smoothness implies that
the structure is spatially correlated, so that a high ﬂux in one pixel is accompanied by
comparably large values in its neighbours. How can these conﬂicting requirements be
accommodated?
A simple resolution presents itself if we return to Fig. 6.1. There we said that an arbi-
trary function f(x) could be expressed as a sum of many, slightly-displaced, δ-functions
(or extremely narrow rectangles); in terms of eqn (6.9), this corresponds to using a set
of basis functionsηj(x)=δ

x−xj

. If we want to describe a smooth function, in a free-
form manner, all we need to do is use a set of basis functions which have some spatial
extent. Using the familiar Gaussian form of width w, for example, eqn (6.2) would then
become
f(x) =
M

j=1
aj exp
	
−

x −xj
2
2 w2

.
(6.17)
This situation is illustrated in Fig. 6.7. For any given length-scale w, the problem re-
duces to the one of estimating the positive amplitudes {aj} that was met in the previous
section. Thus ‘fuzzy pixels’ (or fuzzy balls) provide an easy extension to the monkey
model which allows us to encode preference for local smoothness.

Smoothness: fuzzy pixels and spatial correlations
141
Fig. 6.7 A schematic illustration of the way a smooth function f(x) can be represented as the
sum of basis functions with spatial extent w.
There are two obvious questions for the above formulation: how do we choose w,
and do we have to use this speciﬁc basis function setup? Within the regularizationframe-
work, the pragmatic advice is to let w be as large as possible while still satisfying the
constraints of the data. This is a qualitative way of implementing Ockham’s razor: if w
is too small, unnecessary structure will become admissible; if w is too big, χ2 ≈N will
not be achievable. The answer to the second question is no: the use of Gaussian basis
functions, of ﬁxed width, is just a simple choice that is often found to be adequate; if it
isn’t, we’d have to try something a bit more elaborate.
Although the fuzzy pixel formulation and more traditional gradient constraints,
where (aj+1−aj)2 might be used as a regularizer, are essentially alternative ways of
thinking about local smoothness, the former does offer some advantages. First, it makes
it easy to combine smoothness with positivity: as long as both the basis functions and
their amplitudes are positive, this will also be true of f(x). Second, since the existence
of a derivative implies continuity, the use of gradient constraints is technically restricted
to such functions. By contrast, the basis functions of Fig. 6.7 can be either continuous
or discrete; as such, this idea of smoothness as a correlation between spatially-related
pixels is far more general. Indeed, this brings us to the last point: unlike derivatives,
the concept of fuzzy pixels can easily be extended to encode prior knowledge about
the non-local behaviour of f(x). For example, in a stellar spectrum, we might expect
that an emission line at frequency ν1 would be accompanied by a similar companion at
2ν1; in that situation, we could use basis functions which had two peaks separated by a
suitably large distance.
6.3.1
Interpolation
The importance of using prior knowledge about local smoothness is, perhaps, most
vividly demonstrated by the interpolation problem. Here the data consist of the actual
values of f(x) given at coarse intervals; we are then required to make inferences about
this function on a much ﬁner scale. An example of such data, with imperceptibly small
(but ﬁnite) error-bars, is shown in Fig. 6.8(a). If we only know that f(x) is a positive
distribution, and maximize the entropy as in Section 6.2, then we will obtain the solution
of Fig. 6.8(b). This answer looks extremely weird, but is easily explained. The data tell
us a lot about f(x) at the measured points, but they contain absolutely no information

142
Non-parametric estimation
Fig. 6.8 (a) Interpolation data. (b) The MaxEnt solution of Section 6.2, equivalent to w=0. (c) A
plot of the lowest achievable value of the normalized χ2 versus the spatial correlation length-scale
w. (d) The MaxEnt solution with w=4.
about its behaviour in the intervening space. Consequently, while the solution is highly
constrained at the data-points, the monkeys are free to even-out everything in between.
Hence, the very spiky appearance of Fig. 6.8(b).
Although understandable, the result in Fig. 6.8(b) is still disconcerting. The cause of
this unease is, of course, the lack of local smoothness. If we encode our prior expectation
of spatial correlations through the fuzzy pixel formulation of eqn (6.17), then we obtain
the graph of the lowest achievable χ2/N versus w shown in Fig. 6.8(c); values of w less
than about ﬁve are capable of giving adequate agreement with the data (χ2 ≈N). The
shape of the curve in Fig. 6.8(c) is highly atypical, being peculiar to the interpolation
problem; usually, it’s not easy to achieve χ2 ≪N even as w →0. The MaxEnt solution
with w=4 is given in Fig. 6.8(d).
6.4
Generalizations: some extensions and comments
In Section 6.2, we said that the entropic regularizer of eqn (6.13) was appropriate for the
case where it was only known that f(x) constituted a positive and additive distribution.
Although this is quite common, what can be done if (even) these conditions are not
valid? Well, we obviously need to use a regularization function for the {aj} that does
not exclude negative values; to overcome the non-uniqueness difﬁculties of free-form
solutions, it’s still helpful if it has a well-deﬁned maximum. A quadratic penalty factor

Generalizations: some extensions and comments
143
is a suitable candidate, and can be thought of as a multivariate Gaussian prior:
prob

{aj}|{mj}, {γj}, α, I

∝exp

−α
M

j=1

aj −mj
2
γ 2
j

.
(6.18)
The logarithm of this expression simpliﬁes to −α (aj −mj)2 if all the γj are equal,
and to just −α  aj
2 if all the mj are also zero. Not only is eqn (6.18) less constraining
than its entropic counterpart, it is much easier to handle analytically. When α is large
and all the mj are positive, the Gaussian and entropic priors are very similar; this can be
veriﬁed visually from Fig. 6.4(b), or derived formally by expanding the entropy of eqn
(6.13) as a (multivariate) quadratic Taylor series about its maximum at {aj}={mj}:
S ≈−
M

j=1

aj −mj
2
2mj
,
so that exp(αS) is just like eqn (6.18), with γj =2mj . Thus, the simpler regulariza-
tion function (aj−m)2 can often lead to results which closely resemble the entropic
ones. In accordance with its implied extra prior knowledge, the latter will be superior if
positivity is an important (rather than merely valid) constraint; this will be particularly
so if the object of interest has a large dynamic range (i.e. it has some very bright areas
on generally dim background).
Even for the case when f(x) is not necessarily positive, we might have cogent prior
information to assign a more elaborate pdf than eqn (6.18). One such situation arises
when the object of interest is known to be the difference of two distributions which
are each inherently positive and additive. For example, the nuclear-spin density in an
NMR experiment results from the combined affect of atoms with up and down spin;
denoting their distributions by fu(x) and fd(x), respectively, f(x) = fu(x) −fd(x).
Our inference about f(x) is, therefore, governed by what we can say about fu(x) and
fd(x). Given only the most basic characteristics of the up and down distributions, it is
appropriate to use a regularizer constructed from the sum of their individual entropies:
S = Su +Sd, where Su and Sd are given by expressions similar to eqn (6.13). Since our
primary interest tends to lie in f(x), rather than fu(x) and fd(x), it is more convenient
to work with it directly. This can be done, following some mathematical inspiration,
if there is a common default model. It relies on the fact that the maximum of the total
entropy Su+Sd , subject to a constraint on any function of f(x), or the data, is equivalent
to the optimization of Sud:
Sud =
M

j=1

ψj −2mj −aj log
ψj + aj
2mj

,
where ψ2
j = aj
2 + 4m2
j, and aj is not necessarily positive. The use of this regularizer,
and that of eqn (6.18), is illustrated in Fig. 6.9; the data consist of the 18 lowest-order
Fourier coefﬁcients of a ‘spin-density’ function deﬁned on a grid of 128 points. We see
that, by comparison with the simpler Gaussian prior, the best estimate of f(x) from the

144
Non-parametric estimation
Fig. 6.9 (a) Simulated data, resulting from the Fourier transform of a ‘spin-density’ function. (b)
The corresponding best estimates of f(x) obtained by using a hybrid positive–negative entropy
formulation (solid line) and a Gaussian prior (dotted line).
hybrid positive–negative entropy formulation has somewhat reduced rippling features
and sharper ‘structural’ peaks. Two examples of its practical use can be found in Laue
et al. (1985) and David (1990).
Returning to the case where the object of interest is positive, suppose we also knew
it had an upper bound fmax. How can we take this additional prior information into
account? Well, we obviously need a regularizer which is zero unless 0 ⩽f(x) ⩽fmax;
for a unique (best) free-form solution, it’s also helpful if it has a well-deﬁned maximum.
One such possibility is
S = −
M

j=1

aj log
 aj
mj

+

1−aj

log
 1−aj
1−mj

,
(6.19)
where we have set amax =1; it has a maximum at {aj}={mj}, and decays rapidly to
zero as any of the amplitudes aj approach 0 or 1. In fact, eqn (6.19) is also the sum of
the entropies of two distributions: {aj} relative to the default model {mj}, and {1−aj}
relative to {1−mj}.
6.4.1
Summary of the basic strategy
We’ve now seen several examples of how to deal with the free-form estimation of f(x).
In keeping with the spirit of ﬂexibility, the procedures were designed to encode differ-
ent types of weak assumptions about the object of interest: positivity, local smoothness,
bounds and so on. Every formulation corresponds to a particular set of conditioning
information and, in essence, represents a variation on the exact question that is being
asked of the data; as such, data analysis is nothing more than a dialogue with the exper-
imental measurements. If we don’t ‘like’ the results of an analysis, then the constructive
way forward is to reﬂect on the reasons for the dissatisfaction. This pondering usually
reveals that we’ve either made an assumption which isn’t valid or, as is more often the
case, we haven’t encoded a property of f(x) we believe to be true. With this additional
insight, the problem can then be recast so that we better answer the question which we
had really intended to ask. The great advantage of this adaptive approach is that we

Generalizations: some extensions and comments
145
don’t have to fear ‘failure’: as long as we are guided to a more satisfactory analysis,
we will have learnt something useful from the exercise. Indeed, it can be said that it’s
precisely when our initial expectations are not met, and we are forced to think more
deeply about a problem, that real (scientiﬁc) progress is made.
Pursuing the theme a little further, we should emphasize that there is nothing to
prevent us from carrying out a parametric analysis (as in Chapters 2 and 3) after a
free-form estimation. As we have said before, the latter is used in situations where we
know so little about the object of interest that we don’t have the conﬁdence to describe
f(x) by a speciﬁc functional model; its results, qualitative as they might be, can often
suggest one! For example, Fig. 6.9(b) seems to indicate that the ‘spin-density’ function
consists of just three sharp peaks, which could each be parameterized by an amplitude
and a position. Not only will these stronger prior assumptions allow us to estimate f(x)
in a more succinct and precise manner, it will also automatically remove the unsightly
rippling structure. We don’t have to go all the way to committing ourselves to exactly
three peaks, of course, but could simply say there were only a few; the model selection
analysis of Chapter 4 could then be used to tell us how many there was most evidence
for in the data. Although this transition from a free-from analysis to a model-based
one is quite common, there is no reason why it shouldn’t go the other way. This is
particularly so if we have serious doubts about the validity of the (quite) strong prior
knowledge which implicitly underlies any parametric estimation. In this case, we can
carry out a free-form analysis to see whether our conclusions change very much as the
initial constraints are relaxed; if they do, then we must lay more stress on the nature and
importance of the related assumptions.
6.4.2
Inference or inversion?
In our discussion about free-form solutions at the beginning of this chapter, we largely
restricted ourselves to the case where the data are related linearly to f(x). Such prob-
lems are the ones which are most generally tractable, and can be written in the form of
eqn (6.5); using matrix and vector notation, this becomes
F = T a + C,
(6.20)
where a is a digitized version of f(x), which gives rise to the ideal data F through an
experimental set-up described by T and C. Given eqn (6.20), it is tempting to think of
free-form analysis as an exercise in applying an inverse operator to the data:
a = T−1(F −C ) .
(6.21)
A little thought, however, soon reveals that this procedure is likely to be of limited use.
First of all, the inverse only exists if T is a square matrix; since the number of pixels M
in a is usually much larger than the number of data N, we are immediately in trouble.
Secondly, eqn (6.21) is inadequate because it makes no mention of the noise σ which
accompanies the experimental measurements. Finally, eqn (6.21) cannot be generalized
easily for the case of non-linear problems.
Despite the drawbacks, the thought of being able to ‘invert’ the data directly can
still hold considerable appeal; so, let’s examine this idea a bit further with the aid of

146
Non-parametric estimation
a simple convolution example. Suppose that the data D are related to f(x) through a
blurring with an invariant point-spread, or resolution, function R(x):
D(xk) =

f(x) R(xk−x) dx + B(xk) ± σ(xk) ,
(6.22)
for k =1, 2, . . ., N, where B is a slowly-varying background signal and σ is the noise
in the measurements; in fact, we met this relationship in eqn (4.13) and Fig. 4.2. This
equation cannot be inverted as it stands, but does become more amenable to analyti-
cal manipulation if we ignore σ and treat B and D like continuous functions. Then,
according to the convolution theorem, the Fourier transform of eqn (6.22) yields
ˆD(ω) ≈ˆf(ω) ˆR(ω) + ˆB(ω) ,
(6.23)
where the (complex) hat-ω functions are related to their (real) x counterparts by
ˆf(ω) =

f(x) exp(i2πωx) dx ,
(6.24)
and so on. By rearranging eqn (6.23), and then taking an inverse Fourier transform, the
object of interest can be expressed as
f(x) ≈
  ˆD(ω) −ˆB(ω)
ˆR(ω)

exp(−i2πωx) dω .
(6.25)
The use of this inversion formula is illustrated in Fig. 6.10, where it is assumed that
both B and R are known (exactly).
The data in Fig. 6.10(c) were simulated by convolving the test object of Fig. 6.10(a)
with the resolution function shown in Fig. 6.10(b); a ﬂat background signal and noise
were also added. Since both f(x) and {Dk} are deﬁned on the same grid of 128 points in
this example, there is no difﬁculty with the existence of the equivalent inverse operator
T−1. The result obtained by using eqn (6.25) is shown in Fig. 6.10(d); it provides an
atrocious estimate of the test object of Fig. 6.10(a). Indeed, the blurred version of f(x)
given by the data is far more informative than the deconvolution! The cause of this
disappointing performance is, of course, the failure of eqn (6.25) to take into account
the presence of the noise in the measurements. Retaining the σ-term in eqn (6.22), a
more accurate version of eqn (6.25) would be
  ˆD(ω) −ˆB(ω)
ˆR(ω)

exp(−i2πωx) dω = f(x) ± ϵ(x) ,
(6.26)
where the discrepancy ϵ is given by
ϵ(x) ≈
  ˆσ(ω)
ˆR(ω)

exp(−i2πωx) dω .
(6.27)
Hence, the error in the estimate of f(x) is dominated by the small Fourier components
of the resolution function; since the magnitude of ˆR(ω) tends to zero as ω becomes

Generalizations: some extensions and comments
147
Fig. 6.10 The data shown in (c) were generated by convolving the test object of (a) with the
resolution, or point-spread, function of (b); the simulation was carried out on a grid of 128 points,
and a ﬂat background signal and noise were also added. (d) is the result of using the ‘inversion’
formula of eqn (6.25), and (e) is the corresponding estimate of f(x) given by a simple Gaussian
ﬁlter. (f) is the MaxEnt solution.
large, most of the artefacts in Fig. 6.10(d) are high-frequency ripples. This suggests that
the result of the inverse transform can be improved upon by smoothing it out,
f(x) ≈
  ˆD(ω) −ˆB(ω)
ˆR(ω)

ˆΩ(ω) exp(−i2πωx) dω ,
(6.28)
where ˆΩ(ω) is mostly near 1 but tends to 0 as least as fast as ˆR(ω) when ˆR(ω) is

148
Non-parametric estimation
small, and the process is known as ﬁltering or windowing. Carrying out such a blurring
process, we obtain Fig. 6.10(e). This is clearly much better than Fig. 6.10(d): the long
tails of the resolution function seen in Fig. 6.10(c) have been removed, but the ‘true’
features are no longer obscured by large-magnitude rippling artefacts.
The example of Fig. 6.10 illustrates the point that the inverse transform, even when
it does exist, is usually not very helpful; additional procedures, such as ﬁltering, are
needed to make it usable. The crux of the problem is, of course, that data analysis is
not a question of mathematical inversion: it’s a matter of inference. The analysis that
is required is the one that will carefully weigh-up the evidence of both the current data
and our prior knowledge. The beneﬁts of such an approach can be seen in Fig. 6.10(f),
which shows the MaxEnt solution for f(x). At least in this case, prior knowledge about
positivity is a very powerful piece of information; consequently, it leads to a superior
result.
Although data analysis, particularly free-form estimation, is often referred to as an
‘inverse problem’, we should not take this too literally. The task is really one of infer-
ential calculus or plausible reasoning; while this is like the reverse of deductive logic,
it’s not the same as applying T−1 to the data. Having said that, a pseudo-inverse can be
a practical tool for a qualitative analysis: it’s certainly more computationally efﬁcient
than a constrained optimization, for example, and may well be adequate for the job at
hand. Indeed, ﬁltering can even be regarded as a kind of singular value decomposition
technique because the effect of damping the artefact-prone high-ω components in eqn
(6.28) is very similar to the omission of the small-λ eigenvectors in an expansion like
eqn (6.10). As also noted in earlier chapters, therefore, many seemingly ad hoc (but pop-
ular) analysis procedures can be justiﬁed by reformulating them in terms of probability
theory. This forces us to think more carefully about the assumptions which implicitly
underlie them and, thereby, opens up the possibility for further improvement.
6.4.3
Advanced examples
The cases where there is uncertainty about the background signal or resolution function
are more difﬁcult, but progress can sometimes be made by adopting a multiple channel
approach; we met two-channel entropy when discussing non-positive and bounded f(x)
a little earlier. A separation between background and signal can be attempted, for exam-
ple, if a clear distinction can be made between broad and sharp structure (Yethiraj et al.
1991). Although an impressive illustration of ‘blind deconvolution’ (where R(x) is un-
known) can be found in Newton (1985), this problem does not generally have a satisfac-
tory solution. Newton (1985) also has a somewhat exotic demonstration where a variety
of (unknown) moving objects are automatically sorted into velocity-groups based on
just two consecutive frames of a movie. A more practical application of multi-channel
estimation, concerning the multi-scale analysis of data from the infra-red satellite IRAS,
is reported in Bontekoe (1993).
An approach that has recently gained popularity is that of independent component
analysis, or ICA, which has been used to tackle the ‘blind source separation’ problem.
While details can be found in Roberts and Stephens (2001), for example, we recommend
Knuth (1999, 2005) for a throughly Bayesian perspective on the topic.

7
Experimental design
We have now seen many examples of how probability theory can be used to make quan-
titative inferences based on all the (new) empirical information, and (past) prior knowl-
edge, that is available. While this is the central task of data analysis, we have so far
assumed that there is little choice in the actual conduct of the experiments. Let us turn
our attention to this question, and consider how issues such as optimal instrumental
design, and procedure, can be addressed.
7.1
Introduction: general issues
In Chapter 2, we began by considering the analysis of a simple coin-ﬂipping experiment:
Given that N tosses yielded R heads, what is the best estimate of the bias-weighting H
and how conﬁdent are we in this prediction? Although the subsequent examples were of
increasing technical complexity, they did not differ in their basic logical character; the
problems were all to do with making quantitative inferences based on the relevant data,
and prior knowledge, that were available. An important complementary issue, which we
have not yet discussed, concerns the good design of experiments. This can include any-
thing from an assessment of the number of measurements needed to achieve a desired
level of conﬁdence, to questions of the best way to build apparatus or collect data. Let
us now see how probability theory can be used to guide us in such matters.
The simplest case is again provided by the coin-ﬂipping example of Section 2.1:
there is only one quantity to be inferred (the bias-weighting) and only one way of
changing the experiment (the number of tosses); what’s more, there are no nuisance
parameters. The sole design question open is then: how many ﬂips N will be required
to estimate H to a given degree of conﬁdence? Well, the reliability of the inference is
described by the width of the posterior pdf prob(H|N, R, I); as we saw in Section 2.2,
this can be approximated by the error-bar σ of eqn (2.20):
σ =

Ho(1−Ho)
N
,
(7.1)
where, according to eqn (2.19), Ho =R/N. Hence, the conﬁdence interval (a few times
σ) depends on both the number of tosses and the fraction of heads obtained. While a
few ﬂips may be enough to convince us that a coin is heavily-weighted, a much larger
number will be needed for us to be sure that it is reasonably fair. Thus we see that, even
in this elementary case, the answer to our question is not so straightforward: it depends
on the actual bias of the coin!
Although it’s difﬁcult to give a deﬁnitive answer for the number of ﬂips needed for
σ to be less than some small value σc , useful information can be still gleaned from

150
Experimental design
eqn (7.1). For example, it tells us that the error-bar will be largest, for a given N, if
Ho = 1/2. A plausible estimate will be provided, therefore, if the calculation is done
on this basis: N ⩾0.25/σ 2
c . In fact, this estimate will not be very pessimistic as long as
the coin is not heavily-biased (since we need N ⩾0.16/σ 2
c if Ho=0.2 or 0.8). Perhaps
the most important feature apparent from eqn (7.1) is that the size of the error-bar is
inversely proportional to
√
N; this can be veriﬁed pictorially from Fig. 2.1, and relies
on the fact that the fraction of heads obtained does not vary a lot after a modest number
of tosses. This means that if we evaluate σ on the basis of 50 ﬂips (say), then we can
calculate how many more will be needed to achieve the desired level of conﬁdence; a
four-fold increase will halve the error-bar, and so on.
Having illustrated the nature of experimental design questions with a very elemen-
tary example, let’s consider the problem from a more general viewpoint. We are aiming
to make the best possible inference about some object, or issue, of interest, based on
all the information that is available. As we have seen in the preceding chapters, this
usually entails the evaluation of the posterior pdf for a set of appropriate parameters:
prob({Xj}|{Dk}, I). According to Bayes’ theorem, this can be expressed as a product
of the likelihood and prior:
prob({Xj}|{Dk}, I) ∝prob({Dk}|{Xj}, I) × prob({Xj}|I) .
(7.2)
The term on the far right represents our state of knowledge about the {Xj} given only
the prior information I, while the new insight brought by the data is encoded in the like-
lihood prob({Dk}|{Xj}, I). If the likelihood function is much more sharply peaked
than the prior (with both being viewed in the space of the {Xj}), then it will dominate
the posterior and we will have learnt a great deal from the (new) experimental results;
if it is very broad, then the extra measurements add little to our existing position. Thus,
if we have a choice in such matters, the experimental set-up should be tailored to make
the likelihood function as sharply peaked as possible. In other words, the essence of
optimal design is to make the data most sensitive to the quantities of interest.
While the general criterion above is very easy to state, and seems eminently reason-
able, difﬁculties soon emerge when we consider its implementation in detail. For exam-
ple, a proposed improvement in the conduct of an experiment may lead the multivariate
likelihood function to become narrower in some directions but wider in others. An as-
sessment of its potential beneﬁts is then not so straightforward, and requires a careful
consideration of the relative importance of the accuracy with which the various param-
eters are inferred. Another problem, which we have already met in the coin-ﬂipping
case, is that the spread of the pdf prob({Dk}|{Xj}, I) may depend on the actual (true)
values of the Xj’s; since these are not known beforehand, but are to be estimated, the
planning of an optimized experiment can be rather awkward. In particular, it could mean
that an instrument designed to minimize the time taken to conﬁrm that the Xj’s were
close to some anticipated values may not be the best one for dealing with the situation
if our expectations turn out to be incorrect. Despite the many difﬁculties, however, it’s
still well worth thinking about the question of optimal experimental design: even if we
can only manage a rather crude probabilistic analysis, the insight it provides can help in
guiding us in the right direction.

Example 7: optimizing resolution functions
151
7.2
Example 7: optimizing resolution functions
To illustrate some of the issues raised in the preceding section, let’s consider the case
of a simple convolution problem. We’ve used this example earlier, in Figs 4.2 and 6.10,
where the data were related to the object of interest through a blurring process. While
this smearing is a generally undesirable feature, it is often quite difﬁcult to avoid in
practice. Nevertheless, it may be possible to change the shape and width of the resolu-
tion, or point-spread, function to some extent by altering the design of the experimental
set-up; how can this be done to our best advantage?
To make the analysis easier, let’s assume that we are dealing with a one-dimensional
problem (deﬁned by the parameter x); let’s also suppose that the nature of the blurring
does not depend on location of the structure, so that it is described by an invariant
resolution function R(x). In many situations, this point-spread function is (very nearly)
Gaussian in shape:
R(x) ∝exp

−x2
2w2

,
(7.3)
but it can be highly asymmetric, like a sharp-edged exponential:
R(x) ∝e−x/τ
for x ⩾0 ,
(7.4)
and zero otherwise. Although the truncated form of eqn (7.4) may seem extreme, vari-
ants of it are typical when x is time and R is a causal response: Charter (1990) gives an
example from drug absorption studies, and Fig. 6.10(b) resembles the resolution func-
tion for experiments at pulsed neutron sources. Whatever the shape of R(x), the ‘ideal’
data {Fk} are related to the spectrum of interest f(x) by the integral equation
Fk = T

f(x)R(xk−x) dx + B(xk) ,
(7.5)
for k = 1, 2, . . . , N, where we have added the term B(x) to allow for a background
signal. The parameter T is a scaling constant whose magnitude may be proportional to
the amount of time for which the experiment is conducted, but is independent of the
width of the point-spread function (e.g. w or τ); the size of the background signal may
also increase linearly with the recording time, but we will assume that this variation is
already incorporated in B(x).
In Section 7.1, we noted that the impact of the new data {Dk} becomes greater as
the corresponding likelihood function becomes narrower; as such, to optimize the set-
up, we must investigate how the spread of prob({Dk}|f(x), T, R(x), B(x), I) depends
on the experimental parameters. If the measurements involve a counting process (e.g.
the detection of neutrons or photons) then, as we saw in Chapter 5, the appropriate
likelihood function is typically a Poisson pdf:
prob({Dk}|{Fk}, I) =
N

k=1
F Dk
k
e−Fk
Dk!
,
(7.6)
where, for brevity, we have replaced f(x), T, R(x) and B(x) in the conditioning state-
ment by {Fk}, which are related to the former through eqn (7.5). When we talk about

152
Experimental design
looking at the width of the likelihood function, we really mean that we want to know
what range of different f(x)’s will give reasonable agreement with the data; to quantify
this, we must specify how f(x) is to be described. Hence, it’s not enough merely to
write down an expression such as eqn (7.6): as usual, we also have to think carefully
about what we know about the nature of the object of interest. Consequently, the opti-
mal strategy for one situation may not be the best one for another. Nevertheless, let’s
consider a couple of speciﬁc cases and see what we can learn.
7.2.1
An isolated sharp peak
Perhaps the simplest example that we can investigate concerns the amplitude and posi-
tion of a sharp, isolated, peak. That is to say, we believe that the spectrum of interest
consists of a single δ-function:
f(x) = A δ(x−µ) ,
(7.7)
so that f(x) is deﬁned completely by the two parameters A and µ. With this form, the
convolution integral of eqn (7.5) reduces to
Fk = A T R(xk−µ) + B(xk) .
(7.8)
Returning to our central task of assessing how the parameters of the experimental de-
sign affect the width of the likelihood function, the easiest way of proceeding is to use
a numerical brute force and ignorance method: (i) create a set of ‘mock’ data {Dk},
computer-generated to conform with the model of eqn (7.8) and the Poisson process of
eqn (7.6); (ii) evaluate the probability in eqn (7.6), resulting from these simulated mea-
surements, for different possible values of A and µ; (iii) display this two-dimensional
pdf in (A, µ)-space as a contour map, to give a vivid representation of the spread of the
likelihood function; (iv) examine how the range of A and µ values which yield reason-
able agreement with the data changes as the experimental parameters (such as w or τ or
T etc.) are varied.
The algorithm outlined above is actually akin to the procedure used to obtain the
results in Fig. 3.3, where we analysed simulated Poisson data (for a signal peak on a
ﬂat background) generated under various ‘experimental’ conditions. This led to a brief
discussion of how factors such as the number of counts, the spatial extent of the mea-
surements, and so on, inﬂuenced the reliability with which the parameters of interest
could be inferred. Although our observations were (naturally) based on the changes in
the width and orientation of the resultant posterior pdfs, our conclusions would have
been much the same had we considered the likelihood function instead; this is simply
because these pdfs are proportional to each other, with the assignment of a na¨ıve uni-
form prior (give or take the positivity cut-offs). An interesting feature of this approach
to tackling design issues is that we don’t need to create, and analyse, a large number of
‘mock’ data-sets for each set-up: just one will sufﬁce! The reason is that while different
realizations of random noise shift the maximum of the likelihood function, its spread
remains largely unchanged; since our concern here is purely with the latter, we need
only carry out one simulation for each situation.
Let’s begin by considering the case of a Gaussian resolution function, as in eqn
(7.3); if the background signal is also taken to be constant, so that B(x) = B, then the

Example 7: optimizing resolution functions
153
problem becomes very similar to that in Section 3.1. Rather than knowing µ (and w)
and wanting to infer A and B, however, we will now suppose that B is given and it is
A and µ that are to be estimated. The left-hand side of Fig. 7.1 shows four sets of data
which have been simulated to investigate the effect of varying the design parameters
T , w and B. The corresponding likelihood functions on the right-hand side are plotted
with contour-levels at 10, 30, 50, 70 and 90% of the maximum probability. The upper-
most panels represent a standard, or reference, against which other changes are to be
assessed; in arbitrary units, it has T = 20, w = 16 and B = T/10. In the second pair
of plots, the counting time has been increased ten-fold (so that T = 200, but w and B
are still 16 and T/10); as might have been expected, the likelihood function becomes
about three (or
√
10) times narrower in each direction. The next two panels show that
a more curious result is obtained when the width of the resolution function is reduced
to w = 4 (but T and B remain unchanged): the likelihood function shrinks by a fac-
tor of two with respect to µ, but expands to double its original size in the A-direction.
This can be understood qualitatively by noting that a narrower R(x) reduces both the
amount of blurring and the total number of counts measured; while the former leads
to an improvement in the positional discrimination provided by the data, the latter has
an adverse effect on our ability to infer the amplitude reliably. Thus, despite the overt
simpliﬁcations of our example, we are again faced with the realization that addressing
experimental design issues is not a straightforward problem: what do we really mean
by optimal, when we are forced to trade-off the accuracy of one parameter against an-
other? The answer to this question can only be found by deeper soul-searching about
our ultimate goal, and thence an assessment of the relative importance of the reliability
of the parameters; although this awkwardness can be unsettling, the likelihood analysis
serves a useful purpose in pointing us to matters which need more thought. Finally, the
last two plots in Fig. 7.1 show how an increase in the level of the background signal (to
B = T/2) degrades the quality of the data; this broadening of the likelihood function
might be even more pronounced if the value of B were not assumed to be known, and
had to be marginalized out of the problem.
Having seen how the effects of changing T, w and B can be investigated by the use
of a few computer simulations, and a numerical (and pictorial) analysis of the likelihood
function, let’s try to understand these results from a more theoretical point of view. As
in Fig. 7.1, suppose that the ‘true’ values are A=1 and µ=0; then, according to eqns
(7.3) and (7.8), the ideal data { ˆ
Dk} will be given by
ˆ
Dk = T exp

−xk
2
2w2

+ B ,
(7.9)
for k = 1, 2, . . . , N. Although the measured counts {Dk} will be related to these
through a Poisson process, it’s more convenient to approximate this corruption by an
additive Gaussian noise term {ϵk}:
Dk = ˆ
Dk + ϵk ,
(7.10)
where ⟨ϵk⟩= 0 and

ϵk
2
= σk
2 = ˆ
Dk. Within this context, the likelihood function of
eqn (7.6) reduces to the form

154
Experimental design
Fig. 7.1 Poisson data resulting from the convolution of an isolated sharp peak with a Gaussian
resolution function, and a background signal, simulated under different experimental conditions.
The corresponding likelihood functions are plotted with respect to the parameters to be inferred.

Example 7: optimizing resolution functions
155
loge

prob({Dk}|{Fk}, I)

≈constant −χ2/2 ,
(7.11)
where χ2 is the usual sum of the normalized-residual squareds,
χ2 =
N

k=1
(Fk−Dk)2
σk2
,
(7.12)
and the {Fk} are related to the ‘trial’ values of A and µ by eqns (7.3) and (7.8):
Fk = A T exp
	
−(xk−µ)2
2w2

+ B .
(7.13)
The maximum of prob({Dk}|{Fk}, I) is, of course, deﬁned by the condition that both
∂χ2/∂A and ∂χ2/∂µ are equal to zero; its position will be fairly close to the true input-
value, so that Ao ≈1 and µo ≈0. Our interest here, however, is with the spread of the
likelihood function about this optimal point; to study this, we must calculate the second
partial derivatives ∂2χ2/∂A2, ∂2χ2/∂µ 2 and ∂2χ2/∂A∂µ.
Substituting for Fk and Dk from eqns (7.9), (7.10) and (7.13) in the formula for χ2
of eqn (7.12), and then differentiating twice with respect to A while keeping µ constant,
we obtain
∂2χ2
∂A2
=
N

k=1
2T 2
σk2
exp
	
−(xk−µ)2
w2

.
(7.14)
This comprises one of the elements of the ∇∇χ2 matrix, but it must be evaluated at the
maximum of the likelihood function. Since we expect µo and Ao to be about zero and
unity, respectively, this component will be approximately given by
∂2χ2
∂A2

Ao,µo
≈
N

k=1
2T 2
σk2
exp

−xk
2
w2

.
(7.15)
After substituting σk
2 = ˆ
Dk from eqn (7.9), the behaviour of the resulting summation
can be ascertained by analogy with the integral
∞

−∞
exp

−x2/w2
T exp(−x2/2w2) + B dx ≈
w
√
2π
T + B
√
2 ,
(7.16)
where the expression on the right-hand side is asymptotically correct in the limits of
B →0 and B ≫T, and is accurate to within about 5% in the intermediate regime. This
means that, as long as the data are measured ﬁnely enough over the entire region of the
blurred peak, eqn (7.15) reduces to
∂2χ2
∂A2

Ao,µo
∝
w T 2
T + B
√
2 .
(7.17)

156
Experimental design
The other second partial derivatives of χ2 can be obtained in a similar manner, but the
corresponding algebra requires a little more effort and thought; to a reasonable approx-
imation, it can be shown that
∂2χ2
∂µ2

Ao,µo
∝
T 2
w

T + B
√
8

and
∂2χ2
∂A ∂µ

Ao,µo
≈0 .
(7.18)
Since the covariance matrix is given by (twice) the inverse of ∇∇χ2, we now have all
the ingredients necessary for investigating how T, w and B inﬂuence the width, and
orientation, of the likelihood function.
The ﬁrst thing to notice is that the off-diagonal term ∂2χ2/∂A ∂µ is negligible, as
expected from left–right symmetry. Not only does this make it very easy to invert the
∇∇χ2 matrix, it also tells us that the data impose no correlation between the amplitude
and position of the sharp peak. This property is conﬁrmed by Fig. 7.1, because the
principal directions of the likelihood ellipses are parallel to the coordinate axes (rather
than being skew). The covariance matrix for A and µ becomes particularly simple in the
limit of a small background signal (B ≪T), when it takes the form



δµ2
⟨δµ δA⟩
⟨δµ δA⟩

δA2

∝1
T

w
0
0
1/w

,
(7.19)
where δµ=µ−µo, and so on. Hence, as expected, the range of parameter values which
give reasonable agreement with the data is inversely proportional to
√
T; in other words,
a ten-fold increase in the counting time for the experiment causes the likelihood func-
tion to sharpen-up by a factor of
√
10. The peculiar behaviour of prob({Dk}|{Fk}, I)
with respect to changes in the width of the Gaussian resolution function can also be
understood from eqn (7.19):

δµ2
∝w, whereas

δA2
∝1/w. As we said earlier, this
opposite dependence of A and µ on w illustrates one of the dilemmas often faced when
trying to optimize an experimental design: we can only improve the accuracy with which
one of the parameters is determined at the cost of degrading the precision for another!
Finally, eqns (7.17) and (7.18) also indicate how an increase in the background signal
has an adverse effect on the quality of the data. A larger value of B reduces the magni-
tude of both ∂2χ2/∂A2 and ∂2χ2/∂µ2, by essentially raising the level of the (Poisson)
noise σk
2; the inverse of the ∇∇χ2 matrix then gives the corresponding increase in the
range of the A and µ values which become consistent with the measurements.
7.2.2
A free-form solution
An obvious way of extending the preceding analysis is to consider a spectrum having
two sharp peaks. Not only does this open up the possibility of investigating how dif-
ferent experimental conditions affect the reliability with which we can infer the four
relevant parameters (A1, A2, µ1 and µ2), it also enables a study of our ability to resolve
two closely spaced features if we’re not sure. Rather than pursuing this generalization,
and then moving on to several peaks, or speciﬁc types of broad structure, let’s proceed
straight to the (extreme) situation where we know very little about the spectrum f(x).

Example 7: optimizing resolution functions
157
In other words, while the experimental set-up is still deﬁned by eqns (7.5) and (7.6), we
need to use the free-form representation of f(x) discussed in Section 6.1:
f(x) =
M

j=1
aj δ

x−xj

,
(7.20)
where M is large and the {xj} are ﬁxed by eqn (6.3). With this formulation, our infer-
ence about f(x) is described by the posterior pdf for the M amplitude coefﬁcients {aj};
we must, therefore, view the likelihood function in the multidimensional space of these
parameters, when examining how its spread depends on the experimental conditions.
The main difference between the free-form situation and the earlier case is that we
are now dealing with a much larger multivariate problem; rather than just having the two
parameters A and µ, there may be hundreds of aj! Hence, in contrast to Fig. 7.1, it will
be impossible to present a direct picture of the likelihood function. Nevertheless, we can
still carry out a theoretical analysis within the quadratic approximation of eqns (3.30)–
(3.32); in that context, L is now the (natural) logarithm of prob({Dk}|{Fk}, I) and
the {Xj} are to be read as {aj}. Thus, with the simpliﬁcation afforded by eqn (7.11),
the width and orientation of the likelihood function is characterized by the second-
derivative matrix

∇∇χ2
ij =
∂2χ2
∂ai ∂aj
.
(7.21)
Substituting for f(x) from eqn (7.20) in eqn (7.5), and then differentiating the resultant
expression of eqn (7.12), we obtain:

∇∇χ2
ij =
N

k=1
2T 2
σk2 R(xk−xi)R(xk−xj) .
(7.22)
The inverse of ∇∇χ2 does, of course, yield the covariance matrix, which tells us the
range of parameter values

δaj
2
which give reasonable agreement with the data; its
off-diagonal terms give the related correlations

δai δaj

. Although this is the type of
information we need for addressing questions of experimental design, it is much more
convenient to ascertain this from the eigenvalues and eigenvectors of eqn (7.22). With
reference to Fig. 3.6, and our singular value decomposition discussion in Section 6.1,
this is simply because it’s a lot easier to think about the spread of a skew likelihood
function in terms of the widths along its principal axes.
As in eqn (6.6), the eigenvalues {λl} and eigenvectors {el(x)} of the ∇∇χ2 matrix
are given by the solutions of the standard equation:
M

i=1

∇∇χ2
ij el(xi) = λl el(xj) ,
(7.23)
for l = 1, 2, . . ., M. These can be obtained numerically, as in Figs 6.2 and 6.3, by
simulating a set of noisy data from a test spectrum, computing the elements of the

158
Experimental design
resulting ∇∇χ2 matrix in eqn (7.22) and solving eqn (7.23). Alternatively, we can
do the calculations analytically by making suitable simplifying approximations; as we
saw a little earlier, these are often aided by considering analogies with the continuum
limit. In this instance, for example, the summation of eqn (7.22) can be replaced by the
integral
∇∇χ2(x, x′) = 2T 2
σ2

R(y−x)R(y−x′) dy ,
(7.24)
as long as the data have been sampled fairly ﬁnely, and the size of the error-bars is
roughly constant (so that σk ≈σ). This is, in fact, just the auto-correlation function of
R(x), whose magnitude depends only on the separation between the points x and x′;
denoting this symmetric function by G, we have
∇∇χ2(x, x′) = 2T 2
σ2
G

|x−x′|

.
(7.25)
Substituting this into the continuum version of eqn (7.23), the eigenvalue equation then
takes the form
2T 2
σ2

G

|x−x′|

el(x′) dx′ = λl el(x) .
(7.26)
Since the left-hand side is a convolution integral, eqn (7.26) is best solved by taking its
Fourier transform. This allows us to write
2T 2
σ2
ˆG(ω) ˆel(ω) = λl ˆel(ω) ,
(7.27)
where the real-space x-functions are related to their (complex) ω-hat Fourier counter-
parts by eqn (6.24). For any arbitrary R(x), the only thing we can say about ˆG(ω) is
that it must be real and symmetric; hence, by inspection, the most general solutions of
eqn (7.27) have to be of the type
ˆel(ω) ∝δ(ω−ωl) ± δ(ω+ωl) ,
(7.28)
where ωl is any real number, the eigenvalues being proportional to ˆG(ωl). The (inverse)
Fourier transform of eqn (7.28) then tells us that the eigenfunctions for the convolution
problem are
el(x) ∝sin(ωlx) and cos(ωlx) .
(7.29)
The corresponding eigenvalues are given by eqn (7.27) as:
λl = 2T 2
σ2
| ˆR(ωl)|2 ,
(7.30)
where we have used a standard result about the Fourier transform of auto-correlation
functions to replace ˆG(ω) with | ˆR(ω)|2.
In Section 6.1, we noted that the eigenvectors of the ∇∇χ2 matrix represented the
linear combinations of the {aj} in eqn (7.20) which could be estimated independently

Example 7: optimizing resolution functions
159
of each other. Hence, if we express the spectrum of interest as a sum of the {el(x)} in
eqn (7.29):
f(x) =

l

sl sin(ωlx) + cl cos(ωlx)

,
(7.31)
and view prob({Dk}|f(x), I) in the (M-dimensional) space of the coefﬁcients {sl}
and {cl}, then the principal directions of the (elliptical) likelihood function will be par-
allel to these coordinate axes; the corresponding widths will be given by the reciprocal
of the eigenvalues in eqn (7.30):

δsl
2
=

δcl
2
= 2/λl .
(7.32)
Since the larger values of ωl give rise to more rapidly varying structure in f(x), these
frequencies are inversely related to the ﬁneness of the detail in the spectrum of interest
(so that δx ∝1/ω). Therefore, the error-bars for the various sine and cosine contribu-
tions in eqn (7.32) tell us how much information the data contain about features of f(x)
on different length-scales.
To explain the last point further, consider the two resolution functions shown in Fig.
7.2(a). They are just the Gaussian and sharp-edged exponential of eqns (7.3) and (7.4),
with w and τ chosen to yield the same full-width at half-maximum; their peak heights
have also been adjusted to give them equal integrated areas. According to eqn (7.30),
the resulting eigenvalue spectra

λ(ω) are proportional to the moduli of the Fourier
transforms of these point-spread functions | ˆR(ω)|; they are plotted in Figs 7.2(b) and
(c). The highest frequency, ω = 64, corresponds to pixel-scale structure δx ≈1, while
the lowest, ω=0, refers to features in f(x) which do not vary at all with position. Thus
the fall-off in the eigenvalue spectra, with increasing ω, conﬁrms our expectation that
information about the ﬁner aspects of f(x) is lost in a blurring process. In fact, judging
by the different decay-rates of the λ(ω), Fig. 7.2(b) tells us that this degradation is far
more severe for the Gaussian R(x) than it is for the exponential one; this can be seen
clearly from the logarithmic plot of Fig. 7.2(c). We should point out, however, that this
sharp contrast arises primarily from the presence of the hard edge in the exponential
R(x), and not from the exp(−x) and exp(−x2) decays.
The profound effect that the nature of a resolution function can have on the reliabil-
ity with which intricate detail can be inferred is illustrated in Fig. 7.3. It shows the best
estimates of f(x) that result from a MaxEnt analysis of two simulated data-sets, which
were generated by convolving a test spectrum with the functions of Fig. 7.2(a); as we
would have anticipated, the sharp features are (faithfully) recovered in the exponential
case but are lost with the Gaussian blur.
Before ﬁnally leaving this example, let’s see how changes in the background level,
the experimental time and the width of the resolution function affect the quality of the
data. To do this, we need to consider the noise term σ2 in eqn (7.30) a little more care-
fully. This quantity arose from making the simplifying approximation that all the {σk}
in eqn (7.12) were roughly equal; for Poisson data, its magnitude will be proportional
to the total number of counts measured. Thus, we would expect that
σ2 ∝T R0 (1+Bf) ,
(7.33)

160
Experimental design
Fig. 7.2 (a) A Gaussian and exponential resolution function having the same FWHM and area. (b)
The modulus of their Fourier transforms. (c) The same as (b), but on a logarithmic scale. (d) The
equivalent of (c), but for a Gaussian and exponential having only a quarter of the width in (a).
where Bf is the fraction of counts emanating from the background signal, as compared
to those from the spectrum of interest, and R0 is the integral of R(x). Substituting this
into eqn (7.30), we obtain the familiar result that λ ∝T; in other words, the likelihood
function sharpens up with the square root of the experimental time. The way in which a
large background level degrades the quality of the data, by contributing to the noise σ,
is also apparent from eqn (7.33). The strong inﬂuence that the shape of the resolution
function can have on the convolution problem has already been noted. This property
is formally encoded in the characteristics of the eigenvalue spectrum, with eqn (7.30)
showing that the latter is primarily governed by the Fourier transform of R(x); accord-
ing to eqn (7.33), there is also a weak dependence on the integrated area R0. The effect
of changing the width of the point-spread function is illustrated in Fig. 7.2(d), which
gives | ˆR(ω)|/√R0 corresponding to w and τ having a quarter of the values in Fig.
7.2(a). We see that while this four-fold narrowing of the resolution functions makes an
enormous difference to the λ(ω) for the Gaussian, there is little change in the exponen-
tial case. This acute sensitivity of the eigenvalue spectrum on w tells us that the loss in
resolving-power incurred from a broad Gaussian blur cannot easily be compensated for
by an increase in the measurement time; for that, we would have to boost T by many
orders of magnitude! This behaviour contrasts markedly with that of the exponential,
which seems to preserve ﬁne detail information largely irrespective of τ, and indicates
there is great (potential) merit in having sharp features in the point-spread function.

Calibration, model selection and binning
161
Fig. 7.3 (a) A test spectrum, on a grid of 128 pixels. (b) Simulated (noisy) data generated by
convolving (a) with the Gaussian in Fig. 7.2(a). (c) The same as (b), but using the exponential in
Fig. 7.2(a). (d) The (free-form) MaxEnt estimates of f(x) resulting from the data in (b) and (c),
plotted as a solid and dotted line respectively.
7.3
Calibration, model selection and binning
One of the problems which frequently plagues experimental studies is that of nuisance
parameters. Although we know how to deal with these quantities formally, through
marginalization, a relevant design issue is how much effort should be spent in trying
to calibrate them out. To take a speciﬁc example, consider the case of the (ﬂat) back-
ground signal in the preceding analysis; if the value of B was not known, what frac-
tion of our time should we devote towards its measurement? Well, given that we’re
interested in the parameters {Xj} and not B, we really need to look at the spread of
the marginal likelihood function prob({Dk}|{Xj}, I) rather than the conditional pdf
prob({Dk}|{Xj}, B, I); the two are related by
prob({Dk}|{Xj}, I) =

prob({Dk}|{Xj}, B, I) prob(B|I) dB ,
(7.34)
where we have dropped the conditioning of B on the {Xj} in the last term, because we
are concerned with the merit of obtaining an independent estimate of the background. If
the data {Dk} are collected for a time TD, then the width of prob({Dk}|{Xj}, B, I)
will tend to be proportional to 1/√TD ; similarly, if a time TB is spent on a background
calibration, then the spread of prob(B|I) will scale like 1/√TB . Since both these pdfs

162
Experimental design
contribute to the precision attainable for the parameters {Xj}, the best choice of the
ratio TB : TD is simply the one that makes the integral of eqn (7.34) as sharply peaked
as possible. While the optimal value of this temporal division will depend on the details
of the problem at hand, a simple illustration of what we are trying to do was provided
by Fig. 3.4. In that example, the {Xj} pertain to just one quantity; namely X1 =A, the
amplitude of the isolated peak. Figure 3.4 shows the comparison between the marginal
pdf for A, where prob(B|I) = constant (for B > 0), with the one conditional on
knowing the true value of B; this is akin to considering the extreme cases of TB = 0
and TB →∞, respectively, for a ﬁxed TD. Judging by the similarity of the widths of
these pdfs, we see that a separate measurement of the background signal is of limited
beneﬁt in three out of the four situations discussed. Our conclusions would be very
different, however, if B(x) was highly structured (rather than slowly varying); in such
circumstances, we have little choice but to spend an appreciable amount of time on a
calibration experiment.
So far, we have concentrated on the optimization of experimental design for param-
eter estimation problems. In other words, we’ve assumed a deﬁnitive model H1 which
allows us to characterize the object of interest by a speciﬁc set of variables {Xj}; al-
though their number can be rather large for a free-form solution, our central strategy has
been to make the likelihood function as sharply peaked as possible in order to maximize
the impact of the (new) data. What should we do, however, if our primary objective is
to choose between H1 and an alternative hypothesis H2? Well, we are then concerned
with the ratio of their posterior probabilities
prob(H1|{Dk}, I)
prob(H2|{Dk}, I) = prob(H1|I)
prob(H2|I) × prob({Dk}|H1, I)
prob({Dk}|H2, I) ,
(7.35)
where we have used Bayes’ theorem, top and bottom, to obtain the right-hand side. As
the ratio of the priors for H1 and H2 is likely to be of order unity, a well-designed exper-
iment will be one that makes the contrast between their evidences as large as possible.
Using marginalization and the product rule, and assigning a simple uniform prior for
their respective model parameters, the latter are given by
prob({Dk}|H1, I) =

· · ·

prob({Dk}|{Xj}, H1, I) dX1 dX2 · · · dXM , (7.36)
and a similar expression with H2 and Yj’s. Since the evidence is proportional to the
integral of the likelihood function, we need to consider its height as well as its spread;
in terms of eqn (7.11), for example, this means that both the minimum value of χ2
and the determinant of ∇∇χ2 are important. In fact, as the goodness-of-ﬁt term tends
to be dominant, a model selection experiment will be optimized if most of the data
are collected where the competing hypotheses give (drastically) different predictions.
Having said that, there might still be merit in the regions of commonality if we have an
intrinsic interest in the parameters of the favoured model; this will be particularly so if
it greatly sharpens up the likelihood function.
The discussion above serves as a useful reminder of how the nature of the data-
collection procedure can have a strong inﬂuence on what we can glean from an exper-
iment. That is to say, it’s not just the physical characteristics of the apparatus that are

Information gain: quantifying the worth of an experiment
163
important but so too are the questions of where, and for how long, different measure-
ments are taken. In Fig. 3.3, for example, we saw how changes in the x-range of the
data could affect the precision with which the amplitude of a peak was inferred: while a
very narrow window was bad because it led to a strong correlation with the background,
there was also little point in going too far beyond the tails of the signal. Similarly, a ﬁne
sampling-rate can be important for a sharp-edged resolution function but is not required
for a smooth one. This can be understood analytically by considering the impact that a
bin-width ∆x will have on the eigenvalue spectrum of Fig. 7.2. A discrete Fourier trans-
form of R(x), digitized on this length-scale, will yield an upper bound on the frequency
of ωmax ∼1/∆x; this will, in turn, place a limit on the ﬁneness of the detail in f(x) that
can be inferred reliably, unless λ(ω) has already decayed to almost zero by this cut-off.
Hence, a high data sampling-rate is far more crucial for a point-spread function with
sharp features than it is for a broad smooth one.
7.4
Information gain: quantifying the worth of an experiment
In the general discussion of Section 7.1, it was noted that a good experiment was one
that yielded a likelihood function which was much more sharply peaked than the prior;
otherwise, very little is learnt from the measurements. This basic idea can be quantiﬁed
through the notion of entropy that was introduced in Chapter 5, eqns (5.26)–(5.29).
Here, we reverse the sign, and deﬁne
H =

P(X) log2
P(X)
π(X)

dX .
(7.37)
This negative entropy is the (positive) amount of information in the posterior
P(X) = prob(X|D, I)
relative to the prior
π(X) = prob(X|I) ,
(7.38)
after acquiring data D. With logarithms to the base 2, information is measured in bits. If
X had just two equivalent states, for example, then the prior would be π(X) = ( 1
2, 1
2).
Full knowledge of X, on the other hand, would compress its posterior P(X) to either
(1, 0) or (0, 1). Either corresponds to an information of
H = 1 log2(2) + 0 log2(0) = 1 bit ,
as P log P →0 for small P. Similarly, with four equivalent states and prior π(X) =
( 1
4, 1
4, 1
4, 1
4), full knowledge that compresses the posterior into just one state has infor-
mation H = 2 bits. Knowing how a 6-face die landed gives H = log2 6 = 2.58 bits, and
so on. Equation (7.37) generalizes such examples to partial knowledge over an arbitrary
prior. It is the corner-stone of information theory, as founded by Shannon (1948).
Now suppose that we design an experiment to measure X (either a single scalar or a
vector of several parameters). The experiment is to produce data D (again, either scalar
or vector), and is speciﬁed by its likelihood function

164
Experimental design
L(D, X) = prob(D|X, I) .
The joint distribution prob(D, X|I) = L(D, X) π(X) yields, as usual, the evidence
Z(D) = prob(D|I) =

L(D, X) π(X) dX
(7.39)
and the posterior
P(X) = L(D, X) π(X)
Z(D)
.
(7.40)
Here, though, the formulae remain general: we are only designing the equipment and
have not yet acquired speciﬁc data. Nevertheless, we can use eqn (7.37) to discover the
amount of information
H(D) =
 L(D, X) π(X)
Z(D)
log2
L(D, X)
Z(D)

dX
that we would have about X if we did the experiment. Even before acquiring the data,
we can anticipate the results with eqn (7.39), which gives us the expected information
⟨H⟩=

H(D) Z(D) dD.
This is the beneﬁt of the experiment, quantifying the amount of information about X
which the experiment is expected to provide.
Sometimes we are not interested in X directly, but only in some subsidiary property
Q(X), perhaps with nuisance parameters eliminated. In that case, we use the prior and
posterior of X in eqns (7.38) and (7.40) to induce prior and posterior for Q instead,
through
prob(Q∗|. . .) =

δ

Q∗−Q(X)

prob(X|. . .) dX .
We can then obtain the beneﬁt of the proposed experiment in terms of expected infor-
mation about Q instead of X.
The subject of ‘Bayesian design’ is the study of such expectations, and of the trade-
offs that have to be made between beneﬁts and costs. It puts experimental design on a
ﬁrm, quantitative footing.

8
Least-squares extensions
Least-squares is probably the most widely used data analysis procedures in the physical
sciences, and we saw how it could be justiﬁed from a Bayesian viewpoint in Chapter 3.
The deeper understanding provided by the latter leads naturally to powerful extensions
of the basic recipe. A consideration of the uncertainties in the characteristics of the
implicit Gaussian likelihood, for example, yields simple prescriptions for dealing with
‘outliers’ and suspected noise correlations.
8.1
Introduction: constraints and restraints
The least-squares procedure for estimating the M parameters X of a suitable model
from a pertinent set of N data D is given by the minimum of the χ2 function:
χ2 =
N

k=1
R2
k ,
(8.1)
where Rk = (Fk−Dk)/σk is the normalized residual for the kth datum, being the
difference between the theoretical prediction Fk,
Fk = f (X, k) ,
(8.2)
and the corresponding datum Dk, relative to an estimate of the expected mismatch σk.
As we saw in Section 3.5, this prescription follows from the assignment of a uniform
prior for the X,
prob(X|I) = constant ,
(8.3)
and an uncorrelated Gaussian likelihood function for the D,
prob(D|X, I) =
N

k=1
1
σk
√
2π
exp

−R2
k
2

∝exp

−χ2
2

,
(8.4)
so that the logarithm of the posterior pdf becomes
L = log e

prob(X|D, I)

= constant −χ2
2 ,
(8.5)
where a knowledge of the {σk}, and the functional relationship f, is assumed in the
background information I. The least-squares procedure is fairly simple to apply and
often quite successful in practice. The reasons for this are three-fold: (i) if N ≫M, the
posterior pdf is usually dominated by the likelihood and the prior is largely irrelevant;

166
Least-squares extensions
(ii) the Gaussian pdf is a good approximation in many situations (e.g. for a Poisson pdf
where Dk≫1); (iii) the optimization task is particularly easy if the functional relation-
ship in eqn (8.2) is linear.
With this basic understanding of the assumptions that underlie the use of least-
squares, a straightforward extension is immediately obvious if there is cogent prior
information about the model parameters. If we already knew that Xj = xoj ± ϵj , for
example, where j = 1, 2, . . ., M, then the assignment of an uncorrelated Gaussian pdf
for the prior for X,
prob(X|I) =
M

j=1
1
ϵj
√
2π exp
	
−(Xj −xoj)2
2 ϵj2

∝exp

−C
2

,
(8.6)
where
C =
M

j=1
Xj −xoj
ϵj
2
,
(8.7)
leads to the following for the logarithm of the posterior pdf:
L = loge

prob(X|D, I)

= constant −1
2

χ2 + C

.
(8.8)
Hence, the best estimate of the X is given by those values which minimize χ2+C. This
additional constraint function C is negligible if the initial uncertainties {ϵj} are very
large, when the original least-squares procedure is recovered. The extra term becomes
important, however, if the measurements are insensitive to the values of some of the
Xj’s, or if they only tell us about a restricted linear combination of them. In the limit
of very strong prior information, when some of the ϵj →0, the constraints are called
restraints. It’s equivalent to holding the relevant Xj’s at their presumed values, with a
corresponding reduction in the number of model parameters M.
The speciﬁcation of a non-uniform prior is widely seen as the principal difference
between the Bayesian and the orthodox approaches to data analysis. While this is sig-
niﬁcant in itself, because the addition of the C term to χ2 above seems more natural
than an ad hoc ﬁx, the advantages over a conventional viewpoint are much broader.
Even with the stated prior and likelihood assumptions inherent in least-squares, useful
extensions still follow from a careful consideration of the underlying uncertainties.
8.2
Noise scaling: a simple global adjustment
Suppose we were faced with our generic parameter estimation problem, and were pre-
pared to make the assignments of eqns (8.3) and (8.4), but had no estimates for the {σk}.
How could we then proceed? Well, we’d have to make some suitable assumptions; the
simpler the better. For example, that all the {σk} were roughly the same and equal to
the unknown constant σ; or that they were proportional to the square root of the data
(Poisson-like), σk = σ√Dk ; or whatever seemed most appropriate. This is quite rou-
tine from our viewpoint, where probabilities represent states of knowledge, because all
analyses are conditional on the background information and assumptions that go into

Outliers: dealing with erratic data
167
them. If there was serious doubt about the validity of a proposed noise assignment,
then it could always be compared quantitatively with an alternative suggestion through
a comparison of their resultant probabilistic evidence, prob(data|assumptions, I), as
in Chapter 4.
Pursuing the most elementary hypothesis, deﬁned by just one extra parameter σ, the
conditional likelihood for the data is still of the form of eqn (8.4):
prob(D|σ, X, I) ∝
1
σN exp

−χ2
2 σ2

,
(8.9)
where all occurrences of σ have been highlighted explicitly. Having separated out the
noise scaling factor in this way, χ2 is simply given by eqn (8.1) with σk=1 if we have
no clues; σk=√Dk for Poisson-like situations; {σk} are equal to the supplied estimates
if they’re thought to be reasonable to within an overall multiplicative factor; and so on.
The marginal likelihood, with σ integrated out with a Jeffreys’ prior, prob(σ|X, I) =
prob(σ|I) ∝1/σ, is then given by
prob(D|X, I) =
∞

0
prob(D|σ, X, I) prob(σ|I) dσ ∝
∞

0
 2t
χ2
N
2 −1
e−t dt
χ2 ,
where we have substituted t = χ2/2σ2. The expression on the right is simply a deﬁnite
integral of t, which is just a ‘number’, times N/2 factors of χ2 in the denominator.
With the uniform prior of eqn (8.3), therefore, the logarithm of the posterior pdf reduces
to
L = loge

prob(X|D, I)

= constant −N
2 loge

χ2
.
(8.10)
In fact, we have already met this result in Chapter 3: it’s equivalent to eqn (3.38) and
the penultimate equation in Section 3.5.1, to within a Jeffreys’ prior, and to eqn (3.44).
While the best estimate of the model parameters, Xo, is still given by the X which yield
the smallest value of χ2, χ2
min, the quadratic measure of their covariance, (∇∇L)−1
evaluated at Xo, becomes

Xi−Xoi

Xj −Xoj

= 2

∇∇χ2−1 
ij
χ2
min
N
,
(8.11)
instead of just twice the inverse of ∇∇χ2 (at Xo), where i and j =1, 2, . . . , M.
8.3
Outliers: dealing with erratic data
The ﬁtting of a linear model to a pertinent set of data, such as those in Fig. 8.1(a),
is probably the most frequent use of least-squares; the relevant algebra is in Section
3.5.1. Sometimes this procedure is used ‘blindly’, and Fig. 8.1(b) shows an example
of what can happen if we are not careful. This illustration is so simple that it’s easy to
understand and rectify the problem: a few of the data do not conform to the straight line
hypothesis, to within their estimated error-bars, and their presence distorts the analysis

168
Least-squares extensions
Fig. 8.1 The problem of outliers: (a) a ‘well-behaved’ set of data; (b) a case where quirky things
occasionally happen. The least-squares estimate of the best straight lines is indicated by the dots,
whereas the corresponding results following the analysis in Section 8.3.1 is marked by the dashes.
badly; assuming that the fault lies in the quirkiness of the measurements, rather than in
the inadequacy of the model, the suspect data can be expunged. When the number of
measurements is very large, or they’re multidimensional in nature, or the ﬁtting function
is complicated, such a manual remedy may not be feasible. So, how can we deal with
‘outliers’ in an automatic fashion?
8.3.1
A conservative formulation
We have noted that the source of the difﬁculty is the presence of data which do not
conform to our presumed model to within the stated error-bars. Although we could try
to identify them, in some iterative procedure, let’s treat all the measurements on an equal
footing and adopt a more suspicious attitude towards all of them. That is to say, we’ll
treat the given {σk} as merely representing lower bounds on the noise; after all, they are
typically estimated under ideal conditions that sometimes fail to hold.
Consider a single datum, D. If all we know is that the expected mismatch with the
model prediction F is greater than or equal to σo, what should we assign for the pdf for
the error-bar σ? Since σ is a scale parameter, we might think of a Jeffreys’ pdf:
prob(σ|σo, σ1, I) =
1
loge(σ1/σo) × 1
σ ,
(8.12)
for σo ⩽σ < σ1, and zero otherwise. As this requires the speciﬁcation of a ﬁnite upper
bound, σ1, let’s use a variant that avoids this:
prob(σ|σo, I) = σo
σ2 ,
(8.13)
for σ ⩾σo, and zero otherwise; we will indicate the difference that this modiﬁcation
makes later. The marginal likelihood for D, with the unknown σ integrated out, is given
by
prob(D|F, σo, I) =
∞

0
prob(D|F, σ, I) prob(σ|σo, I) dσ ,
(8.14)

Outliers: dealing with erratic data
169
Fig. 8.2 (a) The likelihood contribution from a datum, prob(D|F, σo, I), plotted as a function
of the residual R = (F −D)/σo, when σo is believed (dashed) compared with when it’s used
as a lower bound (solid); a Jeffreys’ prior solution for the latter case, with σ1 = 100 σo, is also
shown (dotted). The dot–dashed line is the Cauchy variant of the Gaussian (dashed), as described
in Section 8.3.3. (b) The same curves plotted on a logarithmic axis.
where we have dropped the conditioning on σo in the ﬁrst term on the right-hand side,
and F in the second, as being unnecessary. Making the least-squares assumption of a
Gaussian pdf for prob(D|F, σ, I), and using the assignment of eqn (8.13), we obtain
prob(D|F, σo, I) =
σo
√
2π
1/σo

0
t e−t2(F−D)2/2 dt ,
where we have substituted σ = 1/t (so that dσ = −dt/t2). Hence, on evaluating this
easy integral, we ﬁnd that
prob(D|F, σo, I) =
1
σo
√
2π
	
1 −e−R2/2
R2

,
(8.15)
where R = (F −D)/σo is the residual for the datum; it is plotted as a solid line in
Fig. 8.2. Compared with the implicit Gaussian of a least-squares analysis, marked by a
dashed line, where we believe that σo is the error-bar, prob(σ|σo, I) = δ(σ−σo), the
pdf of eqn (8.15) is about 50% broader in the region of the central peak (where it’s also
lower by a factor of 2) and has slowly decaying Cauchy-like tails. It is the presence of
the latter that is crucial in reducing the skewing effect of outliers, because the likelihood
penalty for a large mismatch is then not so severe. For completeness, the dotted line in
Fig. 8.2 shows the pdf that would have resulted if the marginal integral of eqn (8.14)
had been evaluated with the Jeffreys’ prior of eqn (8.12) with σ1 = 100 σo.
If we treat all the data as above, and take the noise on the measurements as being
independent, then, with the assignment of a uniform prior for the model parameters, the
posterior pdf becomes
L = loge

prob(X|D, I)

= constant +
N

k=1
loge
	
1 −e−R 2
k/2
R2
k

.
(8.16)

170
Least-squares extensions
While this is slightly more complicated than the least-squares case of eqns (8.1) and
(8.5), it follows from all the same assumptions except that the quoted {σk} are treated
as lower bounds rather than being believed absolutely. The beneﬁt of the extra effort
involved in ﬁnding the X that maximizes the sum in eqn (8.16), over minimizing χ2, is
evident from the simple example of Fig. 8.1(b): the estimate of the gradient and intercept
of the straight line is 12.0±1.4 and 352.1±7.0, respectively, instead of −5.3±0.8 and
519.1 ± 3.8, and compares well with the true values of 10 and 350 used to generate the
test data (corrupted intermittently by the addition of a large random positive number).
The allowance for a global noise scaling in the least-squares framework, through the use
of eqn (8.10), leads to estimates of −5.3 ± 10.4 and 519.1 ± 50.7; while this reduces
the risk of over-interpreting the signiﬁcance of the skewed result, it doesn’t help in
mitigating the effect of the outliers.
One of the drawbacks of the ‘robust’ formulation here, with respect to least-squares,
is that the posterior pdf of eqn (8.16) is not guaranteed to be unimodal even when the
model parameters are linearly related to the data. This is illustrated in Fig. 8.3, where
the mean µ is estimated from a set of measurements (of the same quantity) containing
one outlier. The solid line shows the posterior pdf of eqn (8.15), with Fk =µ and σk=1,
while the dotted line gives the equivalent result from the standard form of eqn (8.5).
Fig. 8.3 The posterior pdf for the mean µ, after 2, 3, 5 and 100 data (marked by circles, and all
with unit error-bar), when there is one outlier. The solid line is for the conservative formulation
of eqn (8.16), whereas the dotted line is the result yielded by a standard least-squares analysis.

Outliers: dealing with erratic data
171
With just two data, our conservative formulation returns a bimodal distribution; this
is entirely reasonable as, at this stage, we don’t know which of the two inconsistent
measurements is the outlier. By contrast, least-squares suggests that the true value is
given by the arithmetic mean to a fair degree of reliability; this does not accord with
common sense. When a third measurement is made, the pdf becomes almost unimodal.
The small remnant peak is a warning that there might be two outliers! After all, we’re
treating all the data on an equal footing and haven’t built into our analysis that there’s
only one corrupt point. As expected, the skewing effect of the rogue datum on least-
squares is also seen to diminish with an increasing number of ‘good’ measurements.
8.3.2
The good-and-bad data model
Although eqn (8.16) provides valuable automatic protection against occasional quirky
data, there is a price to pay: the uncertainties on the inferred model parameters are about
50% larger than they would be if we were able to believe the {σk} as given. In the case
of Fig. 8.1(a), for example, where there are no outliers, the conservative formulation
returns estimates of 9.8 ± 1.2 and 351.4 ± 5.9 for the gradient and intercept of the
straight line, respectively, whereas least-squares yields 9.8 ± 0.8 and 351.2 ± 3.8. Can
we gain the beneﬁt of a robust analysis without paying the related penalty?
Let’s formulate the problem slightly differently. Rather than being very pessimistic,
and saying that the noise associated with a measurement could only be worse than
expected, we could allow just two possibilities: either everything behaved normally and
the quoted error-bar is reasonable, or something went seriously wrong and the noise
assessment should be scaled up by a large factor. In terms of eqn (8.13), this is equivalent
to the assignment
prob(σ|σo, β, γ, I) = β δ(σ−γσo) + (1−β) δ(σ−σo) ,
(8.17)
where 0 ⩽β ≪1 and γ ≫1. Substituting this into the marginal integral of eqn (8.15),
along with a Gaussian pdf for prob(D|F, σ, I), we obtain
prob(D|F, σo, β, γ, I) =
1
σo
√
2π
/β
γ exp

−R2
2γ2

+ (1−β) exp

−R2
2
0
, (8.18)
where R =(F −D)/σo. With a uniform prior and the usual assumption of independent
noise, the posterior pdf becomes
L = constant +
N

k=1
loge
 β
γ e−R 2
k/2γ2 + (1−β) e−R 2
k/2

,
(8.19)
where L = log e

prob(X|D, β, γ, I)

, and reduces to standard least-squares in the
limit β →0.
This formulation was ﬁrst put forward by Box and Tiao (1968) and is conditional
on a suitable choice for β and γ; namely, the frequency of the suspect measurements
and the severity of their quirkiness. Although Box and Tiao preferred to examine the
sensitivity of the results with respect to these parameters instead of marginalizing over

172
Least-squares extensions
them, estimates of 9.7±0.8 and 351.3±3.8 were obtained for the gradient and intercept
of the straight line in Fig. 8.1(a) when β and γ were integrated out within the quadratic
approximation of Section 3.2.1; thus, there appears to be no inferential penalty for en-
coding robustness in this way. The corresponding results from the data of Fig. 8.1(b)
are also reasonable: 12.1± 0.9 and 351.0 ± 4.7. While this suggests that the formula-
tion of eqn (8.19) is superior to that of eqn (8.16), we have found the latter to be more
amenable to optimization with elementary search algorithms (e.g. simplex following an
initial estimate from least-squares). The lack of extraneous parameters, such as β and
γ, is both an advantage and a limitation of eqn (8.16).
8.3.3
The Cauchy formulation
Another useful alternative to the conservative formulation of eqn (8.13) is given by the
assignment
prob(σ|σo, I) =
2 σo
√π σ2 exp

−σo2
σ2

,
(8.20)
which expresses an expectation that σ should be of the same order as σo, but could be
either narrower or wider. Marginalizing σ according to eqn (8.14), using the substitution
σ = 1/t, the likelihood contribution from datum D takes the Cauchy form
prob(D|F, σo, I) =
1
σo π
√
2

1 + (F −D)2/2σo2
 .
This curve is shown as the dot–dashed line in Fig. 8.2. Once again, the tails decrease
slowly to allow outliers, but the broadening of the central peak is reduced; in fact the
curvature at the peak mimics that of the original Gaussian of width σo, albeit at a height
reduced by a factor √π. With multiple independent data, and a uniform prior for the X,
the posterior pdf takes the form
L = loge

prob(X|D, I)

= constant −
N

k=1
loge

1 + R2
k
2

.
(8.21)
This is equivalent to the standard least-squares eqn (8.5) when the residuals are small.
For the data of Fig. 8.1(b), the gradient and intercept are estimated as 12.1±1.2 and
350.4 ± 5.5.
More generally, it is permissible to assign any pdf to the likelihood, for example
prob(D|F, σo, β, I) ∝

1 + (F −D)2
2 β σo2
−β
.
In this formula, β = 1 is the Cauchy assignment, whilst β →∞is the Gaussian. Instead
of guessing a functional form for the noise, experience with other datasets from the
same experimental apparatus might suggest, or even deﬁne, some other pdf speciﬁcally
tuned to that apparatus. Anything goes, though the evidence values guide one towards a
sensible choice.

Background removal
173
8.4
Background removal
Although the data of Fig. 8.1(b) were used as a simple example of the outlier problem,
they are reminiscent of the common case of a sharp signal sitting on a slowly varying
background (albeit just a straight line). The correct way to deal with this situation is
to evaluate the posterior pdf for the quantities of interest while marginalizing out the
nuisance parameters, as was done in Section 4.2. This calculation can sometimes seem
quite daunting, however, and we might seek a pragmatic separation between background
and signal as a ﬁrst step towards the analysis of the data. How can we use probability
theory to help us do this?
Let’s begin with the assumption of Gaussian noise, so that
prob(D|A, B, σ, I) =
1
σ
√
2π exp
	
−(A +B −D)2
2 σ2

,
(8.22)
where σ is the expected mismatch between a single datum D and the corresponding
sum of the background B and signal A. Since we cannot predict A without a model for
the spectrum, it has to be marginalized out with a suitable prior prob(A|I). If we knew
only that A was positive and had a mean µ, for example, then the MaxEnt principle
would lead to the exponential assignment of Section 5.3.1:
prob(A|µ, I) = 1
µ exp

−A
µ

for A ⩾0 ,
(8.23)
and zero otherwise. Substituting from eqns (8.22) and (8.23) into the A-integral of the
joint pdf for A and D, prob(D, A|B, µ, σ, I) = prob(D|A, B, σ, I)× prob(A|µ, I),
we obtain
prob(D|B, µ, σ, I) =
1
µ σ
√
2π
∞

0
e−A/µ e−(A+B−D)2/2σ2 dA
= eη(R + η/2)
2µ

1 −erf
η +R
√
2

,
(8.24)
where the second line follows from the completion of the square for A in the exponent
of the integrand and the deﬁnition of the error function in Appendix A.1, η = σ/µ and
R = (B−D)/σ is the background-based residual. This marginal likelihood is plotted
as a dashed line in Fig. 8.4 for the case of η = 0.1; it is asymmetric, with a long tail
allowing for large negative values of R.
Greater ﬂexibility can be incorporated into the analysis by invoking the analogue of
the good-and-bad data model, with 0<β <1 being the chance that a datum contains a
signal and 1−β that it’s pure background. A weighted combination of the pdfs of eqn
(8.22) with A=0, giving the dotted Gaussian in Fig. 8.4, and eqn (8.24) yields
prob(D|B, β, µ, σ, I) = β eη(R +η/2)
2µ

1−erf
η +R
√
2

+ (1−β)e−R2/2
σ
√
2π
,
(8.25)

174
Least-squares extensions
Fig. 8.4 (a) The likelihood contribution from a background-based residual R = (B−D)/σ. The
dotted Gaussian is conditional upon there being purely background, or A = 0, while the dashed
line is for the prior information ⟨A⟩/σ =10; their weighted average, assuming a 50% chance of
a signal, is shown by the solid curve. (b) The same pdfs plotted on a logarithmic axis.
and is marked by the solid line in Fig. 8.4 for β = 0.5 and η = 0.1. Assuming noise
independence, the likelihood for a set of data D, given a slowly-varying model for the
background described by a few parameters X, is a product of such terms for each of the
measurements: prob(D|X, β, µ, σ, I) = 1 prob(Dk|Bk, β, µ, σk, I) where R, σ and
η in eqn (8.25) are replaced by Rk =(Bk−Dk)/σk with Bk =f(X, k) and ηk =σk/µ.
The nuisance parameters β and µ do need to be marginalized out, of course, to evaluate
the posterior pdf for X. Carrying out the calculation for the data of Fig. 8.1 leads to
virtually the same results as in Section 8.3.2; while there is no gain over eqn (8.21) in
this case, we’d hope for an advantage in more trying circumstances. Examples of this
type of analysis applied to real data can be found in Fischer et al. (2000) and in David
and Sivia (2001).
8.5
Correlated noise: avoiding over-counting
One of the simplifying assumptions made throughout this book is that the uncertainty
associated with one measurement is independent from that of any other. In the context
of the Gaussian noise model, whose general multivariate form is given by
prob(D|X, I) =
1

(2π)N det(C)
exp

−1
2 (F −D)TC−1(F −D)

,
(8.26)
where F −D is a column-vector of misﬁts (Fk−Dk for k=1, 2, . . . N), it is equivalent
to taking the (N ×N) covariance matrix C as being diagonal:
Ckk′ =

σk
2 for k =k′,
0
otherwise ,
(8.27)
where Ckk′ is the kk′-element of C. The inverse of this matrix is trivial, and leads to eqn
(8.4) on substitution in eqn (8.26). When eqn (8.27) is not an adequate approximation

Correlated noise: avoiding over-counting
175
to C, the data analysis prescription is still straightforward if we have a good estimate of
its off-diagonal terms. All that changes is the deﬁnition of the misﬁt statistic:
χ2 =
N

k=1
N

k′=1
(Fk−Dk)

C−1
kk′ (Fk′ −Dk′) ,
(8.28)
which reduces to eqn (8.1) for the case of eqn (8.27). If we suspect that correlations are
not negligible but don’t know their nature, how can we proceed?
8.5.1
Nearest-neighbour correlations
When faced with the situation where we lack important information, progress has to be
made by invoking suitable assumptions; the simplest are best to begin with, but more
sophistication can be added later if necessary. In this spirit, let’s suppose that there is a
characteristic nearest-neighbour correlation strength ϵ. This means that the expectation
values of the pairwise products of misﬁts is constrained by

(Fk−Dk)(Fk′ −Dk′)

=

σk
2
for k =k′,
ϵ σkσk′
for |k−k′| =1,
(8.29)
where −1 < ϵ < 1. Independence is equivalent to ϵ = 0 and signiﬁes that a higher than
expected value of one measurement, for example, does not systematically correspond to
a higher or lower value for the next. A tendency for same-signed deviations is indicated
by ϵ>0, whereas oppositely-signed ones have ϵ< 0.
The speciﬁcation of eqn (8.29) does not deﬁne the covariance matrix completely,
and so we are left with the task of assigning prob(D|X, I) subject to limited informa-
tion. We saw how this could be done with the principle of MaxEnt in Chapter 5, when
given testable constraints: the sole condition that

(Fk−Dk)2 
=σk
2 leads to eqn (8.4),
while the additional knowledge of all the cross-correlations yields eqn (8.26). Maxi-
mizing the entropy of the likelihood function (relative to a uniform measure) subject to
eqn (8.29), we obtain the Gaussian assignment of eqn (8.26) with a covariance matrix
whose inverse is tridiagonal:

C−1
kk′ =

Λk
for k =k′,
λk
for |k−k′| =1,
(8.30)
where the 2N−1 Lagrange multipliers, Λ1 , Λ2 , . . . , ΛN , λ1 , λ2 , . . . , λN−1, have to
be chosen to fulﬁl the requirements of eqn (8.29). The solution is found to be
Λk =









1
(1−ϵ2) σk2
for k = 1 or N,
1+ ϵ2
(1−ϵ2) σk2
for 1 < k < N,
and
λk =
−ϵ
(1−ϵ2) σk σk+1
,
(8.31)
with the resulting covariance matrix taking the simple form

176
Least-squares extensions
Ckk′ = σk σk′ ϵ|k−k′| .
(8.32)
While this clearly satisﬁes eqn (8.29), its inverse relationship to eqns (8.30) and (8.31)
is best veriﬁed by explicit multiplication.
Substituting for C−1 in eqn (8.26), with
det(C) = (σ1σ2 · · · σN)2
1−ϵ2N−1 ,
(8.33)
and using the uniform prior of eqn (8.3), we ﬁnd that the logarithm of the posterior pdf,
loge

prob(X|D, ϵ, I)

, becomes
L = constant −1
2

(N −1) log e

1−ϵ2
+
Q
1−ϵ2

,
(8.34)
where Q consists of quantities quadratically related to the residuals Rk =(Fk−Dk)/σk.
Explicitly,
Q = χ2 + ϵ

ϵ

χ2−φ

−2ψ

,
(8.35)
where
χ2 =
N

k=1
R2
k ,
φ = R2
1 +R2
N
and
ψ =
N−1

k=1
Rk Rk+1 .
(8.36)
Reassuringly, eqn (8.34) reduces to the least-squares form of eqn (8.5) when ϵ=0. It is
interesting to note how MaxEnt has ﬁlled out the whole of the covariance matrix, with a
power-law decay ϵ|k−k′|, even though the constraints were only on the variance and ﬁrst
off-diagonal terms. Not only is the result of eqn (8.32) simple, but one that we might
have tried intuitively; indeed, Bernardo and Smith (1994) give it as a possibility without
offering any theoretical justiﬁcation.
When faced with the situation where the quoted error-bars ‘don’t look right’, our
ﬁrst inclination would be to suspect that they have been wrongly scaled: the σk should
really have been γσk , for all k, where the value of the constant γ is not known. If this
uncertainty is taken into account as well as allowing for the possibility of correlations,
then an analysis similar to that in Section 8.2 leads to a marginal posterior pdf (with γ
integrated out) analogous to eqn (8.10):
L = constant −1
2

N loge(Q) −log e

1−ϵ2
,
(8.37)
where L = log e

prob(X|D, ϵ, I)

, and Q is given by eqns (8.35) and (8.36).
8.5.2
An elementary example
The easiest case to analyse is the situation considered in Section 2.3: Given a set of N
measurements of a quantity µ, {xk} all with an error-bar σ, what is our best estimate
µo and how conﬁdent are we of this prediction? The answer derived was the arithmetic
mean of the data, with an uncertainty of σ/
√
N. How would eqns (2.28) and (2.29)
change if the measurements were subject to nearest-neighbour correlations of strength
ϵ? Well, substituting Fk = µ, Dk = xk and σk = σ in eqns (8.34)–(8.36), the optimality
condition dL/dµ|µo =0 yields the solution

Correlated noise: avoiding over-counting
177
µo =
1
N −ϵ(N −2)
	
x1 + xN + (1−ϵ)
N−1

k=2
xk

.
(8.38)
Its reliability, calculated by taking the square root of the inverse of d2L/dµ2, is given
by
µ = µo ± σ
 
1 + ϵ
N (1 −ϵ) + 2ϵ ≈µo ±
σ
√
N

1 + ϵ
1 −ϵ ,
(8.39)
where the simpliﬁcation on the right holds for moderately large N and ϵ not too close
to unity, when eqn (2.28) is also a good approximation to µo.
As required, eqns (8.38) and (8.39) reduce to eqns (2.28) and (2.29) when ϵ=0. In
the limit of positive correlation, when ϵ→1, all the data become identical. With xk=x,
for all k, we obtain µ = x ± σ; this is independent of N, as expected. In the limit of
anti-correlation, when ϵ →−1, the data alternate between two values spaced equally
around the true µ; eqns (8.38) and (8.39) then return the correct µ without uncertainty!
Having passed these basic tests, we can feel more conﬁdent that our analysis will yield
sensible results for intermediate levels of correlation. Even the strange asymmetry in
the weighting of the measurements in eqn (8.38) can be understood by recognizing that
the ﬁrst and last points have only one neighbour, whereas all the others have two.
Although we tend to think of correlations in the noise as a bad thing, because they
reduce the effective number of ‘independent’ measurements, quantiﬁed approximately
by the (1+ ϵ)/(1−ϵ) factor in eqn (8.39), the striking difference in the behaviour of
the reliability of µo in the two limits of ϵ →±1 warns us that this view may be overly
simplistic. If we were ﬁtting a straight line to a pertinent set of data where the noise was
subject to serious positive correlation, say, then this would have a detrimental effect on
our estimate of the intercept but a beneﬁcial one on the gradient. This is because, in the
limit of ϵ →1, all the measurements would deviate from the straight line by the same
fraction of their error-bars; with σk =σ, the data would exactly match the true gradient
but be displaced vertically by a random amount (determined by σ).
8.5.3
Time series
A treatment of correlated noise inherently assumes an ordering of the data; otherwise,
terms such as ‘nearest-neighbour’ have little meaning. Since measurements which are
a function of (increasing) time t are naturally sequential, Dk = D(tk) with tk > tk−1,
data where the ordering is important are often referred to as a time series. Dependence
of a measurement on the preceding ones is called a Markov process, and the length of
the backwards linkage is the ‘order of the chain’. Independence is equivalent to zeroth
order,
prob(Dk|Dk−1, Dk−2, . . . , D1, X, I) = prob(Dk|X, I) ,
(8.40)
where X is the vector of parameters for the model describing the systematic variation
of the data, and the (unqualiﬁed) term ‘Markov chain’ is most frequently used to denote
a ﬁrst order Brownian motion-like process:
prob(Dk|Dk−1, Dk−2, . . . , D1, X, I) = prob(Dk|Dk−1, X, I) .
(8.41)

178
Least-squares extensions
An example of a time series is shown in Fig. 8.5(a). It relates to a molecular dynam-
ics simulation of 256 water molecules at room temperature; the details can be found in
Refson (2000) and an introduction in Allen and Tildesley (1987). Many physical prop-
erties of the system, such as the potential energy, the rotational and translational kinetic
energies, the pressure, the dipole moment, the stress tensor and so on, can be computed
Fig. 8.5 (a) The evolution in the kinetic energy of a model water system, sampled at intervals of
0.0025 ps. (b) The joint posterior pdf for the correlation coefﬁcient, ϵ, and the standard deviation,
σ, of the ﬂuctuations: prob(ϵ, σ|{data}, I). (c)–(e) The marginal posterior pdfs for ϵ, σ and the
average kinetic energy µ; the dotted lines show the equivalent results assuming independence
(ϵ=0). (f) The same as (b), except that axes are logarithmic in σ and the ratio (1+ϵ)/(1−ϵ).

Log-normal: least-squares for magnitude data
179
as a function of time. Figure 8.5(a) shows 800 sequential samples of the instantaneous
kinetic energy, plotted in terms of equivalent degrees Kelvin, over a 2 picosecond inter-
val. If the data are regarded as ﬂuctuations about a uniform value µ, as in Section 8.5.2,
then the joint posterior pdf for the correlation strength ϵ, the error-bar σ and µ is given
by
L = Lp −1
2

(N−1) log e

1−ϵ2
+ 2N loge(σ) +
Q
σ2 (1−ϵ2)

,
(8.42)
where Rk = µ −Dk in eqn (8.36), Lp is a constant plus the logarithm of the prior pdf
for µ, σ and ϵ, and L = loge

prob(µ, σ, ϵ|D, I)

; this is just eqn (8.34) with all the
factors of σ retained explicitly. To obtain the posterior pdf for σ and ϵ, the exponential of
L has to be integrated with respect to µ. This marginalization can be done analytically
because, for a given σ and ϵ (and a uniform µ-prior), the task of optimizing µ is linear.
Hence,
prob(σ, ϵ|D, I) ∝prob(σ, ϵ|I) × exp(L0)

−d2L
dµ2
−1/2
,
(8.43)
where L0 is the value of eqn (8.42) when dL/dµ = 0. Carrying out this calculation
with a uniform prob(σ, ϵ|I) yields the pdf in Fig. 8.5(b); it has been contoured at 0.5,
2.5, 12.5, 30, 60 and 90% of the maximum. The integrations of Fig. 8.5(b) along the
two coordinate axes give the marginal posterior pdfs for σ and ϵ shown in Figs 8.5(c)
and (d). The latter conﬁrms the presence of signiﬁcant nearest-neighbour correlation,
with ϵ ≈0.94, whereas the dotted line in the former warns us about the danger of over
conﬁdence in assuming independence. This tendency can also be seen in Fig. 8.5(e),
where the error-bar for µ is about ﬁve times too small if we take ϵ as being zero rather
than marginalizing it out; this is consistent with the (1+ ϵ)/(1−ϵ) term in eqn (8.39).
As always, however, we must not forget that our conclusions are conditional on the
assumptions that underlie the analysis.
The assignment of prob(σ, ϵ|I) ∝[σ(1−ϵ2)]−1 might be a better choice than a
na¨ıve uniform prior. While the scale parameter argument for 1/σ is given in Section
5.1.2, (1−ϵ2)−1 is a suggestion based on the observation that the correlation ellipse
of Fig. 3.6 changes rapidly as ϵ →±1. The potential beneﬁt of our proposal, which is
equivalent to having a uniform prior in log(σ) and log[(1+ϵ)/(1−ϵ)], can be seen from
the more convenient shape of the posterior pdf in Fig. 8.5(f). When this prior was used
for the data of Fig. 8.5(a), the results were essentially unchanged.
8.6
Log-normal: least-squares for magnitude data
When discussing location and scale parameters in Section 5.1.2, we saw that it was
more natural to work in logarithmic coordinates for the latter. This simply reﬂects the
fact that it’s the relative change that is most important for such quantities. For example,
economic changes are usually given in percentage terms. Likewise, it may be preferable
to work with fractional reliabilities for magnitude-type data: a length is 50m to within
2%, say, instead of ±1m. Although this might sound strange, and is unnecessary for

180
Least-squares extensions
Fig. 8.6 (a) The Gaussian likelihood for the logarithm of datum D given an expected value of
F =1 and a relative error σ of 10% (dotted) and 50%. (b) The equivalent log-normal pdf for D.
many situations, it has a bearing on the precise choice of our likelihood function. If we
let y = loge(x), so that

(δy)2
=
2δx
x
23
,
then a constraint on the relative error in x is equivalent to an absolute one in y. The
MaxEnt principle then leads us to assign a Gaussian pdf for log e(x) or, through a change
of variables with eqn (3.77), a log-normal distribution in x:
prob(x|xo, σ) =
1
x σ
√
2π exp

−[loge(x/xo)]2
2σ2
4
,
(8.44)
for positive x, and zero otherwise, where xo >0 is the median value of x with relative
variance σ2 =

(δx/x)2
; two cases are illustrated in Fig. 8.6. Since the assumption
of Gaussian noise is central to the use of least-squares, and a poor approximation to
eqn (8.44) when σ is more than about 10%, it’s better to work with the logarithm of
magnitude data.

9
Nested sampling
The resurgence of interest in the Bayesian approach to data analysis has been driven, in
practical terms, partly by the rapid development of computer hardware and partly by the
advent of larger-scale problems. This chapter discusses some of the modern numerical
techniques that are useful for doing Bayesian calculations when analytical approxima-
tions are inadequate; in particular, it focuses on the novel idea of nested sampling.
9.1
Introduction: the computational problem
In this chapter we are concerned, not so much with the probabilistic formalism, but with
how to compute it once the hypothesis space and the data have been assigned. To orient
ourselves, let’s expand the joint probability for the parameters x of a given model and
the data D in the two different ways allowed by the product rule:
prob(D|x, I) prob(x|I) = prob(x, D|I) = prob(D|I) prob(x|D, I)
L(x)
π(x)
=
· · ·
=
Z
P(x)
Likelihood × Prior
=
Joint
=
Evidence × Posterior
inputs
=⇒
· · ·
=⇒
outputs
from which Bayes’ theorem follows trivially. The inputs to our computation are the
prior π(x) and the likelihood L(x), whilst the desired outputs are the evidence Z and
the posterior P(x); the compressed notation emphasizes that our focus is on algorithmic
methods rather than probabilistic foundation.
The normalization requirement for probability distributions means that the cumulant
masses of both the prior and the posterior are unity,

· · ·

π(x) dx = 1
and

· · ·

P(x) dx = 1 .
(9.1)
In the terminology of the subject, mass denotes an accumulated amount of probability,
and the pdf (probability density function) is its differential d(mass)/d(volume of x).
It is the second condition in eqn (9.1) that lets us separate the amount Z of joint distri-
bution from the shape P, through
Z =

· · ·

L(x) π(x) dx ,
(9.2)
whence
P(x) = L(x) π(x)
Z
.
(9.3)
Our aim is to evaluate these outputs for problems which are too large for brute-force
enumeration of ‘all’ x, or for adequate approximation by any convenient algebraic form.

182
Nested sampling
In this context, it is worth remembering that a space of high dimensionality has an
exponentially large number of constituent regions: at a resolution of 1 part in R, a space
of dimension N has RN elements. Hence even quite moderate N can defeat the brute-
force approach, and have too much freedom for useful analytical approximation.
9.1.1
Evidence and posterior
What we perhaps cannot do in practice, we can nevertheless contemplate doing in prin-
ciple. Just as we can think of evaluating Z in eqn (9.2) by the direct summation of small
elements, so we can imagine sorting these elements into decreasing order of likelihood
value. Taking the example of a two-dimensional model, where x = (x1, x2), the 4×4
table of likelihood values
L =
0
8 15
3
11 24 22 10
19 30 26 16
9 23 18
6
would sort into a 16-cell vector:
L = (30, 26, 24, 23, 22, 19, 18, 16, 15, 11, 10, 9, 8, 6, 3, 0) .
The accumulation of such elements in the context of a general x lets us acquire the
function
ξ(λ) = proportion of prior with likelihood greater than λ.
(9.4)
More formally,
ξ(λ) =

· · ·

L(x) >λ
π(x) dx ,
(9.5)
in which the element of prior mass is dξ = π(x) dx. Dimensionality is not a problem:
if each of the N components of x is stored to an accuracy of 1 part in R, then the single
Encloses everything
Awkward intermediate structure
States of large     are scarce
-ve                        log                              0
log
L
L
ξ
Fig. 9.1 A sorted likelihood function, on logarithmic axes: ξ is the proportion of the prior mass
in which the model parameters yield a likelihood value greater than a threshold L.

Introduction: the computational problem
183
coordinate ξ should be stored to 1 part in RN, requiring the same number N log2 R of
bits as for x, implying no loss of information. At any degree of decomposition, there is
a 1:1 correspondence between x and ξ. At least, that’s the theory. In practice, we can
store ξ to ordinary arithmetical precision, and in continuous problems leave ambiguous
ties of likelihood to look after themselves. In discrete problems, where a substantial
fraction of the prior mass can all be assigned exactly the same likelihood value, ties can
be resolved by adding a small amount of random jitter to L. This imposes a ranking
even when none existed before.
Because the restriction on likelihood becomes tighter as λ increases, ξ is a decreas-
ing (with jitter, strictly decreasing) function of likelihood limit λ, but it need not have
any other nice property.The extreme values are λ⩾0 at ξ =1, because likelihood values
cannot be negative, and λ = Lmax (if the maximum exists) at ξ = 0. The differential
−dξ/dλ, insofar as the small-scale limit exists, is the density of states; speciﬁcally, the
density of prior mass with respect to likelihood.
It’s actually more convenient to use the equivalent inverse form of eqn (9.4) or (9.5),
in which the enclosed prior mass ξ is the primary variable and L the subsidiary, L(ξ)
being deﬁned by
L

ξ(λ)

≡λ .
(9.6)
Note that L(ξ), having scalar argument ξ, should not be confused with L(x), having
vector argument x. In accordance with modern computing practice, we overload the
symbol L with formally different functions according to the argument type, and also
use L for the common value that these functions take. A typically awkward likelihood
is sketched in Fig. 9.1, using logarithmic axes because even after taking logs the scales
of these axes could easily be thousands or millions in a respectably large application.
The likelihood function L(ξ) underlies both the evidence Z and the posterior P. To
see this, remember that each small element dξ came from a source volume of that same
0
1
Area
0
1
(a)
(b)
L
L
Z
ξ
ξ
Fig. 9.2 (a) Likelihood function with area Z, not to scale since most of the area is likely to be
found at invisibly tiny values of ξ. (b) Posterior samples are randomly scattered over the area Z.

184
Nested sampling
prior mass π(x)dx. Hence the evidence of eqn (9.2), being a sum over these elements,
is simply the enclosed area as shown in Fig. 9.2(a):
Z =
1

0
L(ξ) dξ .
(9.7)
Any point taken randomly from this area, as illustrated in Fig. 9.2(b), yields a random
sample ˜ξ from the posterior distribution
P(ξ) = L(ξ)
Z
(9.8)
which, by the same argument, gives equivalently a random sample ˜x from the poste-
rior P(x). Hence the sorted likelihood function L(ξ) is the key to obtaining both the
evidence Z and the posterior P (as a set of random samples).
9.2
Nested sampling: the basic idea
The new technique of nested sampling (Skilling 2004) tabulates the sorted likelihood
function L(x) in a way that itself uses Monte Carlo methods. The technique uses a col-
lection of n objects x, randomly sampled with respect to the prior π, but also subject to
an evolving constraint L(x)>L∗preventing the likelihood from exceeding the current
limiting value L∗. We assume that we are able to generate such objects, and will discuss
the accomplishment of this in Section 9.3.
In terms of ξ, the objects are uniformly sampled subject to the constraint ξ < ξ∗,
where ξ∗corresponds to L∗; this is illustrated in Fig. 9.3. At the outset, sampling is
uniform over the entire prior, meaning that ξ∗=1 and L∗=0. The idea is then to iterate
inwards in ξ and correspondingly upwards in L, in order to locate and quantify the tiny
region of high likelihood where most of the joint distribution is to be found.
Uniform
0
1
L∗
ξ∗
Fig. 9.3 Four objects (n=4) sampled uniformly in ξ < ξ∗, or equivalently in L > L∗.

Nested sampling: the basic idea
185
0
1
new
0
1
0
1
worst
(a)
ξ∗
(b)
ξ∗
(c)
ξ∗
Fig. 9.4 An iteration replaces the worst object with a new one inside the shrunken domain.
9.2.1
Iterating a sequence of objects
On entry, an iteration holds n objects restricted to ξ < ξ∗, as shown in Fig. 9.4(a). The
worst of these, being the one with smallest likelihood and hence largest ξ, is selected.
Located at the largest of n numbers uniformly distributed in (0, ξ∗), it will lie about one
part in n less than ξ∗. More technically, the shrinkage ratio t = ξ/ξ∗is distributed as
prob(t) = n tn−1 ,
(9.9)
with mean and standard deviation
log t = (−1 ± 1)/n .
(9.10)
Iteration proceeds by using the worst object’s (ξ, L) as the new (ξ∗, L∗). Meanwhile,
the worst object, no longer obeying the constraint, is discarded. There are now n−1
surviving objects, still distributed uniformly over ξ but conﬁned to a shrunken domain
bounded by the new constraint ξ∗; this is illustrated in Fig. 9.4(b). The new domain
is nested within the old, hence the name ‘nested sampling’. The next step is to gener-
ate a replacement object, sampled uniformly over the prior but constrained within this
reduced domain. For now, we assume that we are able to do this. Having done it, the
iteration again holds n objects restricted to ξ < ξ∗, as in Fig. 9.4(c), just like on entry
except for the 1-part-in-n shrinkage. The loop is complete, and the next iteration can
be started.
Successive iterations generate a sequence of discarded objects on the edges of pro-
gressively smaller nested domains. At iterate k,
Lk = L∗
and
ξk = ξ∗=
k

j=1
tj ,
(9.11)
in which each shrinkage ratio tj is independently distributed with the pdf of eqn (9.9)
with the statistics of eqn (9.10). It follows that
log ξk = (−k ±
√
k )/n .
(9.12)

186
Nested sampling
0
1
factor e1/n
L
ξ
Fig. 9.5 The sorted likelihood function, L(ξ), is generated as a discrete sequence.
Ignoring uncertainty for a moment, we can proclaim each log t to be −1/n so that
ξk = exp(−k/n), and the sequence then tabulates L(ξ) just as we require. Everything
we want is now available, as shown in Fig. 9.5.
The evidence of eqn (9.7) is evaluated by associating with each object in the se-
quence a width h = ∆ξ, and hence a vertical strip of area A = hL, whence
Z ≈

k
Ak ,
where Ak = hk Lk .
(9.13)
Here, the simplest assignment of the width is
hk = ξk−1 −ξk .
(9.14)
One can try to be more accurate by using the trapezoid or such rule instead, but the
uncertainties in ξ tend to overwhelm these minor variations of implementation. Along
with the evidence, the information, or negative entropy,
H =

P(ξ) log

P(ξ)

dξ ≈

k
Ak
Z log
Lk
Z

is available as the natural logarithmic measure of the prior-to-posterior shrinkage (as
discussed in Section 7.4, but here using natural logarithms). Very roughly,
H ≈(# active components in data) × log(signal/noise ratio) .
(9.15)
9.2.2
Terminating the iterations
The usual behaviour of the areas A is that they start by rising, with the likelihood L in-
creasing faster than the widths h decrease. The more important regions are being found.
At some point, L ﬂattens off sufﬁciently that decreasing width dominates increasing
likelihood, so that the areas pass across a maximum and start to fall away. Most of the
total area is usually found in the region of this maximum, which occurs in the region of
ξ ≈e−H. Very roughly, the width of this region is

Nested sampling: the basic idea
187
∆(log ξ) ≈

# active components in data ,
(9.16)
in accordance with the ‘χ2 = N ±
√
2N ’ folklore surrounding Gaussian data from
eqn (3.83). Remembering that ξk ≈e−k/n suggests a termination condition
‘continue iterating until the count k signiﬁcantly exceeds nH’
which conveniently expresses the general aim that a nested-sampling calculation should
be continued until most of Z has been found. Of course, H is here the current evaluate
from the previous k iterates.
Unfortunately, we can offer no rigorous criterion to ensure the validity of any such
termination condition. It is perfectly possible for the accumulation of Z to ﬂatten off, ap-
parently approaching a ﬁnal value, whilst yet further inward there lurks a small domain
in which the likelihood is sufﬁciently large to dominate the eventual results. Exactly
this happens in Section 9.6.2, where termination is crudely imposed by stopping after a
sufﬁcient number of iterates. Termination remains a matter of user judgment about the
problem in hand, albeit with the aim of effectively completing the accumulation of Z.
9.2.3
Numerical uncertainty of computed results
The nH iterates taken to reach the dominating areas are random, and according to eqn
(9.12) are subject to a standard deviation uncertainty
√
nH. Correspondingly, the accu-
mulated values of log ξ are subject to a standard deviation uncertainty
√
nH/n. There
will be some internal variation as well, as the dominating region is traversed, but the
major uncertainty will be a scale factor caused by the shift in logarithm. This shift is
transmitted to the evidence Z through eqn (9.13), so that log Z too has standard devia-
tion uncertainty
√
nH/n. Putting the results together,
log Z ≈log

k
Ak

±

H
n .
(9.17)
Incidentally, this uncertainty range is best expressed, as shown, in terms of log Z;
translating it back to a mean and standard deviation of Z itself just looks misleading
whenever the range of the logarithm is substantial. The choice matters. For example,
if the evidence values for hypotheses C and D were quoted as log eZC = −1000 and
logeZD = −1010, one would be inclined to favour C by a factor of e10. Yet if the es-
timates were accompanied by uncertainties of ±100, the comparison would effectively
be just a toss-up. The difference would then be log eZC −logeZD = 10 ± 100
√
2, for
which the chance of C bettering D is a mere 53%. If the difference was important, more
computation would be needed to reduce the uncertainty.
Also, remember that Z is dimensional, having dimensions inverse to the data. For
example, if the relevant data were 157 measurements of length and 23 of time, the
dimension of Z would be m−157s−23, so that Z should be quoted in a form akin to
loge

Z / m−157s−23
= estimate ± numerical uncertainty .
A more sophisticated approach to the uncertainty uses the known distribution eqn
(9.9) of each shrinkage ratio t. If we knew the t’s, we would know all the ξ’s, thence

188
Nested sampling
all the widths, and (apart from minor systematic errors in the numerical integration) the
evidence and everything else. In other words, we know Z(t) as a function of the t’s:
(9.11)
(9.14)
(9.13)
(9.13)
t
=⇒
ξ
=⇒
h
=⇒
A
=⇒
Z .
So all we need to do is sample the t’s a dozen times or more in order to obtain a cor-
responding list of evidence values. From such a list, we extract statistics (mean and
standard deviation, quantiles or whatever) just as from any other set of Monte Carlo
samples. The illustrative approximation of eqn (9.17) should be good enough for many
purposes, but the sophisticated option is more defensible professionally.
Historically, it has not been usual to present an uncertainty range along with log Z.
There has, perhaps, been a feeling that numerical calculations should be accurate. How-
ever, there are inevitable uncertainties associated with nested sampling, as with any
other numerical method, and it is honest to acknowledge them. The uncertainty dimin-
ishes as the square root of the amount of computation (here n) that was allotted, just as
for other Monte Carlo methods.
9.2.4
Programming nested sampling in ‘C’
Although small, the following ‘C’ main program incorporates the above ideas and
should sufﬁce for many applications. It is protected against over/underﬂow of expo-
nential quantities such as the likelihood L∗by storing them as logarithms (as in the
variable logLstar), adding those values through the PLUS macro, and multiplying
them through summation. Rather than attempting greater sophistication, the program
uses the simple proclamation of steady compression by log(t) = −1/n each step. The
corresponding step-widths h are also stored as logarithms, in logwidth.
//
NESTED SAMPLING MAIN PROGRAM
//
(GNU GENERAL PUBLIC LICENSE software: c⃝Sivia and Skilling 2006)
#include <stdio.h>
#include <stdlib.h>
#include <math.h>
#include <float.h>
#define UNIFORM ((rand()+0.5)/(RAND MAX+1.0))
// Uniform inside (0,1)
#define PLUS(x,y) (x>y ? x+log(1+exp(y-x)) : y+log(1+exp(x-y)))
// logarithmic addition log(exp(x)+exp(y))
/*
*/
#include "apply.c"
// Application code, setting int n, int MAX,
// struct Object, void Prior,
// void Explore, void Results.
/*
*/
int main(void)
{
Object Obj[n];
// Collection of n objects
Object Samples[MAX];
// Objects stored for posterior results
double logwidth;
// ln(width in prior mass)
double logLstar;
// ln(Likelihood constraint)
double H
= 0.0;
// Information, initially 0
double logZ =-DBL MAX;// ln(Evidence Z, initially 0)

Nested sampling: the basic idea
189
double logZnew;
// Updated logZ
int
i;
// Object counter
int
copy;
// Duplicated object
int
worst;
// Worst object
int
nest;
// Nested sampling iteration count
// Set prior objects
for( i = 0; i < n; i++ )
Prior(&Obj[i]);
// Outermost interval of prior mass
logwidth = log(1.0 - exp(-1.0 / n));
// NESTED SAMPLING LOOP
for( nest = 0; nest < MAX; nest++ )
{
// Worst object in collection, with Weight = width * Likelihood
worst = 0;
for( i = 1; i < n; i++ )
if( Obj[i].logL < Obj[worst].logL )
worst = i;
Obj[worst].logWt = logwidth + Obj[worst].logL;
// Update Evidence Z and Information H
logZnew = PLUS(logZ, Obj[worst].logWt);
H = exp(Obj[worst].logWt - logZnew) * Obj[worst].logL
+ exp(logZ - logZnew) * (H + logZ) - logZnew;
logZ = logZnew;
// Posterior Samples (optional)
Samples[nest] = Obj[worst];
// Kill worst object in favour of copy of different survivor
do copy = (int)(n * UNIFORM) % n; // force 0 ⩽copy < n
while( copy == worst && n > 1 );
// don’t kill if n is only 1
logLstar = Obj[worst].logL;
// new likelihood constraint
Obj[worst] = Obj[copy];
// overwrite worst object
// Evolve copied object within constraint
Explore(&Obj[worst], logLstar);
// Shrink interval
logwidth -= 1.0 / n;
}
//
NESTED SAMPLING LOOP (might be ok to terminate early)
// Exit with evidence Z, information H, and optional posterior Samples
printf("# iterates = %d\n", nest);
printf("Evidence: ln(Z) = %g +- %g\n", logZ, sqrt(H/n));
printf("Information: H = %g nats = %g bits\n", H, H/log(2.));
Results(Samples, nest, logZ);
// optional
return 0;
}
Accompanying the main program, an application module is to be ‘#include’d as
ﬁle ‘apply.c’. This needs to #define the number n of objects, and the number MAX
of iterates. The application module also needs to deﬁne the Object structure needed

190
Nested sampling
to hold a sample, and procedures Prior for setting up a prior sample, Explore for
ﬁnding a new sample within the current likelihood constraint and optionally Results
for calculating such posterior results as might be wanted. It is open to the main program
to terminate early if some suitable condition is satisﬁed, though that is not implemented
here. With the application module programmed, nested sampling is complete. From
the user, it requires judgment about the number of objects needed and the termination
decision.
9.3
Generating a new object by random sampling
To complete the account of nested sampling, we must now discuss how to accomplish
the required task of generating a new object, and this too involves randomness.
Computers are causal devices that perform deﬁnitive reproducible calculations, so
are not random at all. What is described as a random number is actually a number
generated by an algorithm that is carefully constructed to give successive outputs that
are un-predictable in the absence of knowledge of the generator itself — that’s what
‘random’ means in Bayesian computation. Such algorithms (Press et al. 1986) provide
random integers in some range, most conveniently 0 to 232−1, or 0 to 264−1, whose
values are unpredictable by the ignorant. It is easy to rescale such an integer into the
standard range used in ﬂoating-point work:
u = Uniform(0, 1) ⇐⇒prob(u|ignorance of generator) =
/ 1 for 0 < u < 1,
0
otherwise.
As a professional detail, a good generator should produce odd multiples of 2−24 for
ordinary IEEE 23-bit-mantissa ﬂoating-point format, or odd multiples of 2−53 for a 52-
bit-mantissa format. Such numbers are a-priori-equivalent and symmetrically arranged
in the strict interior of (0,1), with no stray anomalies such as an exact 0 or 1.
Initializing a nested-sampling calculation involves acquiring random samples di-
rectly from the prior, with the likelihood constraint L > 0 not yet of consequence. We
suppose that the prior is of sufﬁciently simple algebraic form to be sampled without
much expense by using the computer’s random generator. For example, the exponential
prior prob(q) = a−1 exp(−q/a) for an intensity variable q of scale a is sampled by
q = −a log u, and textbooks contain a variety of transformation and related methods
for obtaining samples from standard distributions.
Continuing a nested-sampling calculation involves generating a new object from
the prior, subject to the likelihood constraint L > L∗. The prior domain obeying this
constraint shrinks geometrically as the calculation proceeds, and we do not expect to
be able to ﬁnd an object in this tiny domain ab initio. Instead, we need to learn from
experience so far, and use guidance from previous iterates. A sequence of iterates in
which each uses the previous one(s) is called a Markov chain, and methods that do this
in the context of random sampling are called Markov chain Monte Carlo, or MCMC.
In a nested-sampling iterate, there is obvious guidance built into the method itself.
Whenever we need another object ˜x sampled from the prior π(x) within L(x)>L∗,
we already have n−1 such objects available as the current survivors. This strongly

Generating a new object by random sampling
191
suggests that we should take one of the survivors (a random one in order to preserve
randomness within the constraint) and copy it. Call it X:
˜x(new) = X(old)
(as introductory estimate) .
At least, X satisﬁes the desired conditions of prior and likelihood, so it’s in the right
general place. However, we need an independently sampled new object, not just one
of the old ones over again. Hence we need to move away from X, not so far that we
disobey the constraint, but far enough to lose memory of the starting point.
9.3.1
Markov chain Monte Carlo (MCMC) exploration
First, we learn how to explore the prior alone, without reference to the likelihood. After
all, if we can’t do the unconstrained exploration, we certainly won’t be able to cope with
the constraint as well. The object we seek usually has a large number of possible states
A, B, C, . . . , which we let the computer explore randomly across a fully-connecting
pattern of permitted transitions. There can be considerable artistry involved here, using
a sympathetic understanding of the structure of the problem in hand.
As might be expected, exploring a uniform prior is particularly easy. The simplest
example has its states laid out linearly, with transitions allowed between neighbours.
A ←→B ←→C ←→· · · ←→Z
Starting from the current state, choose at random to move to the left or right neighbour,
and keep doing it. At the boundaries, use either wraparound conditions by linking A to
Z and Z to A, or reﬂection conditions by linking A back to itself and Z back to itself.
Similarly, in two dimensions transitions might be allowed to move randomly to any of
either 4 or 8 neighbours, again with wraparound or reﬂective boundaries.
A ←→B ←→C
↕
↕
↕
D ←→E ←→F
↕
↕
↕
G ←→H ←→I
A ←→B ←→C
↕
↗
↙
↖
↘↕
↗
↙
↖
↘↕
D ←→E ←→F
↕
↗
↙
↖
↘↕
↗
↙
↖
↘↕
G ←→H ←→I
Left for long enough, any such system will eventually be found in any speciﬁed state
with uniform probability, meaning that we cannot predict where it will be unless we
can model the random generator. However, in a large system with size s, it will take
a long time, O(s2), for knowledge of the original location to diffuse away. So, in the
continuum where the number of possible states is huge, transitions are usually taken
in random direction with ﬁnite distance, jumping across intermediate positions. The
appropriate magnitude of this distance depends on the application, as will be seen in the
‘lighthouse’ example below.
The simplicity of these exploration schemes leads to a recommendation that, where
possible, the original problem should be transformed into coordinates with respect to
which the prior is uniform, having convenient topology such as the wraparound unit
cube.

192
Nested sampling
Imposing the likelihood constraint is easy. All we need to do is reject any transi-
tion that would take an object (already obeying the constraint) to a state disobeying it.
Suppose, for example, that states F, H and I in the two-dimensional array above are
prohibited. Transitions to them are then blocked, as shown below:
A ←→B ←→C
↕
↕
D ←→E
F
↕
G
H
I
A ←→B ←→C
↕
↗
↙
↖
↘↕
↗
↙
D ←→E
F
↕
↗
↙
G
H
I
Formally, exploration continues, but with a null move if the trial transition is rejected.
9.3.2
Programming the lighthouse problem in ‘C’
The aim of this application module is to solve the ‘lighthouse’ problem of Section 2.4,
using the locations of ﬂashes observed along the coastline to locate the lighthouse that
emitted them in random directions. The lighthouse is here assumed to be somewhere in
the rectangle −2 < x < 2 , 0 < y < 2 , with uniform prior.
// apply.c
"LIGHTHOUSE" NESTED SAMPLING APPLICATION
//
(GNU GENERAL PUBLIC LICENSE software: c⃝Sivia and Skilling 2006)
//
u=0
u=1
//
-------------------------------------
//
y=2 |:::::::::::::::::::::::::::::::::::::| v=1
//
|::::::::::::::::::::::LIGHT::::::::::|
//
north|::::::::::::::::::::::HOUSE::::::::::|
//
|:::::::::::::::::::::::::::::::::::::|
//
|:::::::::::::::::::::::::::::::::::::|
//
y=0 |:::::::::::::::::::::::::::::::::::::| v=0
// --*--------------*----*--------*-**--**--*-*-------------*---
//
x=-2
coastline -->east
x=2
// Problem:
Lighthouse at (x,y) emitted n ﬂashes observed at D[.] on coast.
// Inputs:
//
Prior(u)
is uniform (=1) over (0,1), mapped to x = 4*u - 2; and
//
Prior(v)
is uniform (=1) over (0,1), mapped to y = 2*v; so that
//
Position
is 2-dimensional -2 <x <2 , 0 <y <2 with ﬂat prior
//
Likelihood
is L(x,y) = PRODUCT[k] (y/pi) / ((D[k] - x)ˆ2 + yˆ2)
// Outputs:
//
Evidence
is Z = INTEGRAL L(x,y) Prior(x,y) dxdy
//
Posterior
is P(x,y) = L(x,y) / Z estimating lighthouse position
//
Information
is H = INTEGRAL P(x,y) log(P(x,y)/Prior(x,y)) dxdy
/*
*/
#define n
100
// # Objects
#define MAX 1000
// # iterates
/*
*/
typedef struct
{
double
u;
// Uniform-prior controlling parameter for x

Generating a new object by random sampling
193
double
v;
// Uniform-prior controlling parameter for y
double
x;
// Geographical easterly position of lighthouse
double
y;
// Geographical northerly position of lighthouse
double
logL;
// logLikelihood = ln prob(data | position)
double
logWt; // log(Weight), adding to SUM(Wt) = Evidence Z
} Object;
/*
*/
double logLhood(
// logLikelihood function
double
x,
// Easterly position
double
y)
// Northerly position
{
int
N = 64;
// # arrival positions
double D[64] =
{ 4.73,
0.45, -1.73,
1.09,
2.19,
0.12,
1.31,
1.00,
1.32,
1.07,
0.86, -0.49, -2.59,
1.73,
2.11,
1.61,
4.98,
1.71,
2.23,-57.20,
0.96,
1.25, -1.56,
2.45,
1.19,
2.17,-10.66,
1.91, -4.16,
1.92,
0.10,
1.98, -2.51,
5.55, -0.47,
1.91,
0.95, -0.78, -0.84,
1.72, -0.01,
1.48,
2.70,
1.21,
4.41, -4.79,
1.33,
0.81,
0.20,
1.58,
1.29,
16.19,
2.75, -2.38, -1.79,
6.50,-18.53,
0.72,
0.94,
3.64,
1.94, -0.11,
1.57,
0.57};
// up to N=64 data
int
k;
// data index
double logL = 0;
// logLikelihood accumulator
for( k = 0; k < N; k++ )
logL += log((y/3.1416) / ((D[k]-x)*(D[k]-x) + y*y));
return logL;
}
/*
*/
void Prior(
// Set Object according to prior
Object* Obj)
// Object being set
{
Obj->u = UNIFORM;
// uniform in (0,1)
Obj->v = UNIFORM;
// uniform in (0,1)
Obj->x = 4.0 * Obj->u - 2.0;
// map to x
Obj->y = 2.0 * Obj->v;
// map to y
Obj->logL = logLhood(Obj->x, Obj->y);
}
/*
*/
void Explore(
// Evolve object within likelihood constraint
Object* Obj,
// Object being evolved
double
logLstar)
// Likelihood constraint L > Lstar
{
double
step = 0.1;
// Initial guess suitable step-size in (0,1)
int
m
= 20;
//
MCMC counter (pre-judged # steps)
int
accept = 0;
// # MCMC acceptances
int
reject = 0;
// # MCMC rejections
Object
Try;
// Trial object
for( ; m > 0; m-- )
{
// Trial object
Try.u = Obj->u + step * (2.*UNIFORM - 1.);
// |move| < step
Try.v = Obj->v + step * (2.*UNIFORM - 1.);
// |move| < step
Try.u -= floor(Try.u);
// wraparound to stay within (0,1)

194
Nested sampling
Try.v -= floor(Try.v);
// wraparound to stay within (0,1)
Try.x = 4.0 * Try.u - 2.0;
// map to x
Try.y = 2.0 * Try.v;
// map to y
Try.logL = logLhood(Try.x, Try.y);
// trial likelihood value
// Accept if and only if within hard likelihood constraint
if( Try.logL > logLstar )
{
*Obj = Try;
accept++;
}
else
reject++;
// Reﬁne step-size to let acceptance ratio converge around 50%
if( accept > reject )
step *= exp(1.0 / accept);
if( accept < reject )
step /= exp(1.0 / reject);
}
}
/*
*/
void Results(
// Posterior properties, here mean and stddev of x,y
Object* Samples,
// Objects deﬁning posterior
int
nest,
// # Samples
double
logZ)
// Evidence (= total weight = SUM[Samples] Weight)
{
double x = 0.0, xx = 0.0;
// 1st and 2nd moments of x
double y = 0.0, yy = 0.0;
// 1st and 2nd moments of y
double w;
// Proportional weight
int
i;
// Sample counter
for( i = 0; i < nest; i++ )
{
w = exp(Samples[i].logWt - logZ);
x
+= w * Samples[i].x;
xx += w * Samples[i].x * Samples[i].x;
y
+= w * Samples[i].y;
yy += w * Samples[i].y * Samples[i].y;
}
printf("mean(x) = %g, stddev(x) = %g\n", x, sqrt(xx-x*x));
printf("mean(y) = %g, stddev(y) = %g\n", y, sqrt(yy-y*y));
}
The Object structure that encodes a possible solution needs to contain a trial lo-
cation (x, y). In accordance with the recommendation to compute with a uniform prior
on a unit square, x and y are slaved to controlling variables u and v, also contained in
Object and each assigned uniform prior on (0,1). The transformation in this example
is simply
x = 4 u −2 ,
y = 2 v
(9.18)
because the lighthouse is supposedly known to be within that given rectangle. However,
we could equally well have encoded, say,
x = tan

π

u −1
2

,
y = v/(1−v)
which would have allowed the lighthouse to be located anywhere in y > 0 (north of
the coastline), though still with a preference for O(1) coordinate values. Any mapping

Monte Carlo sampling of the posterior
195
from the unit square to the upper half-plane would do. Object also contains the cor-
responding likelihood value L(x, y), as its logarithm logL for safety. The logLhood
function does the numerical evaluation, with reference to the N data supplied in the
vector D. Algebraically,
L(x, y) =
N

k=1
y/π
(Dk −x)2 + y 2
which is implicitly a function of the controlling variables u, v. Finally, Object con-
tains the area or weight A = h L, as calculated by the main program and stored in
logWt.
Procedure Prior merely assigns a random (u, v) within the unit square and trans-
forms that to (x, y) by eqn (9.18), before calculating the corresponding likelihood.
Procedure Explore takes a starting position (u, v) — in fact a copy of one of the
other positions— and generates a new and supposedly independent position from it,
subject to likelihood L exceeding the current limit L∗. It does this by adding (or sub-
tracting) a suitable increment to each of u and v, which are then mapped back within
(0,1) if they had escaped, before transforming to the desired (x, y). The increment is
chosen uniformly within some range (-step, step). (Actually, a Gaussian distribu-
tion would be more usual for such purpose, but standard ‘C’ only provides a uniform
generator.) Any trial position obeying the likelihood constraint is accepted, resulting
in movement. Otherwise, the trial is rejected and there is no movement. Thus step
should be reasonably large so that movement is reasonably fast, without being so large
that the likelihood constraint stops all movement. It is difﬁcult to pre-judge the appro-
priate size of step, which in any case tends to diminish as iterations proceed and the
likelihood constraint becomes tighter. Accordingly, Explore includes a toy learning
procedure which adjusts step to balance the number of accepted and rejected trials.
Any such procedure in which the step-size almost certainly tends to some long-term
limiting value is acceptable, and will allow the eventual position to be correctly dis-
tributed. In fact, Explore allows 20 of these MCMC steps of adjustable size, which
seems to be adequate.
Finally, procedure Results computes whatever results are needed from the se-
quence of weighted objects found by nested sampling. The particular results chosen are
the means and standard deviations of the lighthouse coordinates x and y.
9.4
Monte Carlo sampling of the posterior
At this stage, we may note that a set of ‘Monte Carlo’ random samples is, in fact, the
preferred speciﬁcation for a posterior distribution that is too complicated for brute-force
enumeration or convenient analytical approximation. Alternatives are often worse. For
example, it is very easy for the maximum of a multi-dimensional posterior to be way
off towards one corner, and atypical of the bulk of the distribution. Similarly, the mean
⟨x⟩may not be representative either, and can even be prohibited. Also it is quite hard
for a distribution to stay Gaussian, because in N dimensions there are 1
6N 3 third-order
differentials to corrupt the 1
2N 2 second-differential curvature coefﬁcients that might
deﬁne a Gaussian approximation.

196
Nested sampling
True, a relatively small number of Monte Carlo samples may offer only a minuscule
view of the entire space of available states. Nevertheless, they should usually sufﬁce to
quantify any desired scalar property Q(x). If the n samples {˜x} are correctly taken from
the posterior P(x), then their properties {Q(˜x)} will be correctly taken from the pos-
terior distribution P(Q) of that property. A dozen or more such scalar values should be
sufﬁcient to show the expected range of Q, usually as a mean and standard deviation. For
example, the 12 samples {Q(˜x)} = {−0.01, 0.14, 0.99, −1.50, −1.34, −0.52, −1.17,
0.35, −0.64, 0.80, −0.63, 0.84} happened to come from the unit Gaussian distribution.
While not perfect, their mean and standard deviation, µ ± σ = −0.22 ± 0.83, are in
fair accordance with the underlying truth, 0 ± 1. It doesn’t matter that the underlying
x may lie in a space of huge dimensionality, as long as we only seek low-dimensional
properties Q. In such sampling, the mean of (say) 12 samples should vary around the
true average by about ±1/
√
12 = ±0.3 true standard deviations, and should deviate by
more than one standard deviation only once in about 1800 trials. The expected error de-
creases as √n, whilst the chance of serious error decreases exponentially. Monte Carlo
sampling can seem a policy of despair, but actually it works quite well.
Historically, dating back ﬁfty years to the foundation paper of Metropolis et al.
(1953), most algorithmic development has focussed on repeatedly sampling the poste-
rior by some method of Monte Carlo exploration. Note, though, that posterior samples
alone cannot yield the evidence integral— there’s too much freedom in any plausible
interpolant. Passage from (easy) sampling of the prior to (difﬁcult) sampling of the pos-
terior has been accomplished by methods such as simulated annealing, which might
incidentally reveal the evidence as an optional by-product of the transition. In fact, the
evidence has often been ignored, and to this day many authors fail to report the value
of this fundamental quantity. Consequently, rational comparison of different analyses
of their data is damaged. It now seems more productive to aim for the sorted likelihood
function— equivalently the density of states — which underlies both evidence and pos-
terior. As it happens, the evidence (a scalar) is a simpler quantity than the posterior (a
function), which inverts the traditional view and may encourage better practice.
9.4.1
Posterior distribution
Posterior samples may be generated in proportion to the areas that contribute to Z in
Fig. 9.2(b), and those are exactly the A’s that we already have. The sequence of samples
{xk}, with proportional weights
wk = Ak
Z
in accordance with their associated areas, models the posterior. Any property Q is avail-
able too, from the weighted sequence of values {Q(xk)} that deﬁnes estimates of its
mean
mean(Q) =

k
wk Q(xk)
and standard deviation
dev(Q) =

k
wk

Q(xk)−mean(Q)
2
1/2
.

Monte Carlo sampling of the posterior
197
Nested sampling usually places a useful number of samples, according to eqn (9.16)
O

n

# active components in data

, across the posterior. That is usually enough to
make the estimates of Q insensitive to the details of the shrinkage factors t underlying
the weights w. Hence it sufﬁces to set the factors by simply proclaiming the central
values log(t) = −1/n. Even so, best practice would not rely on this, but would sample
the t’s a dozen times or more in order to obtain corresponding estimates of a mean
property, from which an average value could be derived along with its uncertainty;
mean(Q) = estimate ± numerical uncertainty.
If this numerical uncertainty were to exceed dev(Q), one might wish to consider re-
computing with more nested samples in order to reduce the numerical uncertainty to a
subsidiary level.
9.4.2
Equally-weighted posterior samples: staircase sampling
A single sample from the posterior pdf can be provided by choosing k according to
prob(k) = wk, and then giving the corresponding xk. A set of at least several equally-
weighted posterior samples can be used to show ‘typical’ ˜x, and thus develop an intu-
itive understanding of the posterior. They can also be used as a compressed represen-
tation of it, stored for reference in a future when the data and such details may have
been lost. Hence we now aim to draw ν samples from the sequence, ensuring a mean
multiplicity ⟨nk⟩= ν wk for each object. Again, proclaiming log(t) = −1/n usually
sufﬁces to set the weights, though best practice would be to average them over a dozen
or so samples of t. To minimize the reduction of information, it is desirable to keep the
actual (integer) number of copies nk as close as possible to its mean, being the integer
immediately below or immediately above ⟨nk⟩. Construct the cumulant staircase
Sequence number
Staircase height
1
2
3
4
5
6
7
8
0
1
2
3
4
w8
u+3
w7
w6
u+2
w5
u+1
w4
w3
w2
u
w1
Fig. 9.6 Cumulant staircase of weights, showing extraction of four equally-weighted samples (1,
4, 5, 7) from a sequence of eight.

198
Nested sampling
Sk = u + ν
k

j=1
wj ,
u = Uniform(0, 1) ,
as illustrated in Fig. 9.6. Whenever the staircase S rises above an integer, 1, 2, 3, . . .,
record the value of k. Each step has height ν wk, and is randomly offset vertically by
u. Hence the number nk of its intercepts has expectation ⟨nk⟩= ν wk as desired, and
can only be either of the integers immediately above or below that value. Moreover, the
total number of intercepts is exactly ν, also as sought. Of course, our number of nested
samples is limited, so that these posterior samples would tend to repeat if ν were too
large. To avoid this, we may restrict ourselves to
ν ⩽

max
k
wk
−1
.
Each sample can then appear at most once in the posterior list.
9.4.3
The lighthouse posterior
Progress of the nested sampling computation of Section 9.3.2, with n = 100 objects,
is shown in Fig. 9.7. Point k in the nested sequence is expected to be located roughly
on the likelihood contour enclosing the fraction ξ = e−k/n of the available area. Thus
the ﬁrst 20 points, as shown in the ﬁrst plot of Fig. 9.7, indicate the outermost
20
100 of
the area, −0.2 < log ξ < 0. This is the domain of least likelihood, lying furthest away
from the more probable locations. After 100 steps, the available area has compressed
by about a factor of ‘e’, so that points 101–120 as shown in the second plot correspond
-2
-1
0
1
2
0
1
2
Points 1 - 20
Distance y (km)
Position along coast x (km)
1
-2
-1
0
1
2
0
1
2
Points 101 - 120
Distance y (km)
Position along coast x (km)
1
-2
-1
0
1
2
0
1
2
Points 201 - 220
Distance y (km)
Position along coast x (km)
-2
-1
0
1
2
0
1
2
Points 301 - 320
Distance y (km)
Position along coast x (km)
Fig. 9.7 A nested sampling sequence climbs up likelihood contours towards the maximum.

Monte Carlo sampling of the posterior
199
-2
-1
0
1
2
0
1
2
Distance from shore y (km)
Position along coast x (km)
Fig. 9.8 200 posterior samples for the lighthouse problem, superposed on likelihood contours
drawn at 10%, 30%, 50%, 70%, 90% of maximum.
to the shell −1.2 < log ξ < −1. Points 201–220 as shown in the third plot lie in a shell
−2.2 < log ξ < −2 around a yet-higher likelihood contour, and so on. Eventually, the
shells shrink around the point of maximum likelihood at (1.26, 0.93), as the monotonic
sequence of points homes in.
Nested sampling proceeds according to the shape of the likelihood contours, irre-
spective of the actual values. Nevertheless, it is the likelihood values that deﬁne suc-
cessive objects’ weights, which deﬁne the posterior and sum to Z, and which suggest
when a run may be terminated. When these are used, it transpires that the evidence was
loge

Z/km−64
= −160.29 ± 0.16 (as the 64 data were given in kilometres), and the
speciﬁed 1000 iterates were enough. Meanwhile, the position of the lighthouse as esti-
mated from the given data was x = 1.24 ± 0.18, y = 1.00 ± 0.19. Figure 9.8 shows
200 posterior samples extracted from the nested sequence by staircase sampling.
9.4.4
Metropolis exploration of the posterior
As we have seen from eqns (9.15) and (9.16), nested sampling can take many iterations
to reach the bulk of a conﬁned posterior, but relatively few to cross it. Only these rel-
atively few are signiﬁcantly informative about the posterior, and are serious candidates
for the equally-weighted samples that are commonly used to represent it. If the principal
interest is in accurate estimation of the posterior, this imbalance can be wasteful.
Fortunately, it is almost as easy to move a sample around the posterior as it is to
move it within a hard likelihood constraint. Again, we avoid complication by assuming
that our transition scheme is faithful to the prior. But, instead of having a hard boundary,
we now wish the chain to reach state X according to the posterior probability P(X),
being the already-encoded prior modulated by likelihood L(X).
The trick is to accept a trial move with limited probability that is proportional to
L(destination). Consider transitions between two states A and B.

200
Nested sampling
A
B
-

acceptance ∝L(B)
acceptance ∝L(A)
These transitions are in detailed balance when A is populated proportionally to L(A)
as desired, and B proportionally to L(B), because the forward and backward ﬂuxes
then balance. By analogy with physical systems, the unique stable state that ensues
when every transition is in balance is called equilibrium. Left for long enough, a fully-
connected system with detailed balance will eventually reach equilibrium and be found
in state X with probability
lim
t→∞prob(X|ignorance of generator) = P(X) .
All that remains is to maximize the number of acceptances in the interest of efﬁciency,
which we do by scaling the larger of the two to the greatest possible value of 1. This
gives the ansatz
accept (A →B) with probability min

1, L(B)/L(A)

.
(9.19)
Even more simply, though mildly wasteful of random numbers,
accept (A →B) if L(B) > L(A) × Uniform(0, 1) .
(9.20)
For comparison, nested sampling accepted (A →B) if L(B) > L∗. So, the minor ex-
tension of eqn (9.19) or (9.20)— a single extra instruction in the exploration code —
allows us to explore the posterior with as many samples as we wish. Upward moves in
likelihood are always accepted, but downward moves are sometimes rejected and this
asymmetry gives just the required degree of modulation.
Actually, it is not strictly necessary to seed the exploratory Markov chains with pre-
computed posterior samples. Any seed would sufﬁce in principle, and the chain would
eventually lose memory of it (though this might take a long time if the seed was highly
improbable). Historically, this is how the whole subject of Bayesian computation started
ﬁfty years ago, with Metropolis et al. (1953).
9.5
How many objects are needed?
As we investigate the number of objects needed, an unexpected advantage of nested
sampling comes to light. Geometrically, the permitted domain shrinks by about 1 part
in n per iterate. This concentrates the n objects ever more tightly into regions of higher
likelihood, as illustrated in Fig. 9.9. At each iterate, the worst (outermost) object is
discarded in favour of a copy of an internal survivor, which is then re-equilibrated.
9.5.1
Bi-modal likelihood with a single ‘gate’
The likelihood function need not have the convenient single maximum shown in Fig.
9.9. Consider instead a bi-modal likelihood, having two maxima, such as that shown in
Fig. 9.10. One mode is dominant because it contains the bulk of the evidence

L dξ;

How many objects are needed?
201
Fig. 9.9 Nested likelihood contours,
L(x), with n=4 objects.
X1
X2
Fig. 9.10 The thick likelihood contour around the
dominant mode X1 is a ‘gate’. The subordinate
mode X2 is surrounded by a ‘trap’.
the other is subordinate. There is a critical likelihood gate below which the modes are
connected, and above which they are separate. Before the gate is reached, MCMC ex-
ploration can presumably diffuse freely around the volume enclosing both modes. After
the gate is passed, transitions between modes need to jump across to the other tiny
but relatively distant domain, so are essentially blocked and an object can diffuse only
within its own mode.
At the critical likelihood, let the accessible volumes be X1 for the dominant mode
and X2 for the subordinate. The chance of an exploratory object falling into the domi-
nant mode as the gate closes behind it is the proportional gate width
W =
X1
X1 + X2
.
Conversely, with chance 1−W, it falls into the subordinate mode, where it is essentially
trapped. With n independent objects, the chance of ‘success’ with at least one object in
the dominant mode is
prob(success|n objects) = 1 −(1−W )n .
Basically, we need rather more than W −1 objects to be reasonably sure of at least one
success. Thus, if the gate width is W =1/64 but we only supply n =10 objects, then
the chance of a success is less than 1/6.
9.5.2
Multi-modal likelihoods with several ‘gates’
It may be that a particular multi-modal problem has just one narrow gate. Eventually,
the likelihood will favour the dominant mode by a factor that more than compensates
for the narrow opening, but that’s not known as the gate is passed.
It is perhaps more likely that a complicated problem has several gates, perhaps six
gates of width 1/2 or so at different likelihood levels, instead of just one. After all, there

202
Nested sampling
log
log
(a)
ξ
W4
W3
W2
W1
(b)
ξ
W1 W2 W3 W4
Fig. 9.11 (a) Several wide gates, and (b) one narrow gate of equivalent aperture.
is a bigger parameter-space associated with this more general framework. If objects
are programmed to explore independently, half will fail at the ﬁrst gate, then half the
survivors will fail at the second and so on until the ﬁnal survival rate is only 1/64, just
as for a single narrow gate; this is shown schematically in Fig. 9.11.
At each iterate of nested sampling, though, the object with lowest (worst) likelihood
is eliminated in favour of a copy of one of the (better) others. After a gate is passed
and the likelihood constraint continues to climb to more-restrictive heights, the subor-
dinate mode should become progressively less populated. Indeed, after the constraint
has climbed above the subordinate maximum, that mode can have no surviving objects
at all. So, even if a gate is quite narrow, the dominant mode becomes re-populated pro-
vided at least one object manages to ﬁnd it.
With a gate width of 1/2, the chance of having at least one success from n objects
is 1 −2−n, nearly certain. Before the next gate is reached, it may well be that the
population along the dominant route has increased from the original n/2 back up to or
nearly to n. After 6 such gates, with a mere 2−n chance of failure at each, the chance
of success is (1 −2−n)6. Not only is this more than 99% for n = 10, but quite soon
afterwards most or all of the objects should be in the dominant mode. Nested sampling’s
‘copy’ operation has turned an expectation of total failure into a a high probability of
complete success.
Generally, with a series of well-separated gates of widths Wg, the expectations of
success are
prob(success|n objects) =



1 −

1 −1
gWg
n
for no copying,
1
g

1 −(1 −Wg)n 
with copying.
Basically, the number of objects needed to give a good chance of success is
nmin ≈



1
g

W −1
g

for no copying,
(Wmax)−1
with copying.
and copying always beats exploration by individually-preserved objects.

Simulated annealing
203
9.6
Simulated annealing
Nested sampling can be compared with the traditional method of simulated annealing,
which uses fractional powers Lβ of the likelihood to move gradually from the prior
(β = 0) to the posterior (β = 1). As the ‘coolness’ β increases, annealing gently com-
presses a set of points ˜x sampled from dPβ ∝Lβdξ, known as a thermal ensemble.
At stage β, the mean log-likelihood

log L

β =

log L dPβ =

Lβ log L dξ

Lβ dξ
=
d
dβ

log

Lβ dξ

is estimated by averaging over the corresponding ensemble. Summing this yields
1

0

log L

β dβ = log

L dξ

−log

dξ

= log Z
which is the thermodynamic integration formula for the evidence Z. The bulk of the
ensemble, with respect to log ξ, should follow the posterior dPβ ∝Lβ ξ dlog ξ and be
found around the maximum of Lβ ξ. Under the usual conditions of differentiability and
concavity ‘⌢’, this maximum occurs where
d log L
d log ξ
= −1
β .
Annealing over β thus tracks the log L/ log ξ slope, whereas nested sampling tracks
the underlying abscissa value log ξ.
9.6.1
The problem of phase changes
As β increases from 0 to 1, one hopes that the annealing maximum tracks steadily up in
L, so inward in ξ; this is shown schematically in Fig. 9.12(a). The annealing schedule
0
log
log
Anneal
0
log
log
A
B
C
D
E
F
water
steam
slope = -1
(a)
(b)
L
L
ξ
ξ
Fig. 9.12 Proper annealing needs log-likelihood to be concave like (a), not (b).

204
Nested sampling
that dictates how fast β is increased ought to allow successive posteriors Pβ to overlap
substantially— exactly how much is still a matter of some controversy. Yet it may not
be possible at all.
Suppose that Lβξ is not concave, as in Fig. 9.12(b). No matter what schedule is
adopted, annealing is supposed to follow the concave hull of the log-likelihood func-
tion as its tangential slope ﬂattens. But this will require jumping right across any con-
vex ‘⌣’ region that separates ordinary concave ‘phases’ where local maxima of Lβξ
are to be found. At β = 1, the bulk of the posterior should lie near a maximum of
L ξ, in one or other of these phases. Let us call the outer phase ‘steam’ and the inner
phase ‘water’, as suggested by the potentially large difference in volume. Annealing
to β = 1 will normally take the ensemble from the neighbourhood of A to the neigh-
bourhood of B, where the slope is dlog L/dlog ξ = −1/β = −1. Yet we actually
want samples to be found from the inner phase beyond D, ﬁnding which will be ex-
ponentially improbable unless the intervening convex valley is shallow. Alternatively,
annealing could be taken beyond β = 1 until, when the ensemble is near the point of
inﬂection C, the supercooled steam crashes inward to chilled water, somewhere near F.
It might then be possible to anneal back out to unit temperature, reaching the desired
water phase near E. However, annealing no longer bridges smoothly during the crash,
and the value of the evidence is lost. Along with it is lost the internal Bayes factor
prob(states near E)/prob(states near B) which might have enabled the program to
assess the relative importance of water and steam. If there were three phases instead of
just two, annealing might fail even more spectacularly. It would be quite possible for
supercooled steam to condense directly to cold ice, and superheated ice to sublime di-
rectly to hot steam, without settling in an intermediate water phase at all. The dominant
phase could be lost in the hysteresis, and inaccessible to annealing.
Phase change problems in general are well known to be difﬁcult to anneal. Nested
sampling, though, marches steadily down in prior mass ξ along ABCDEF· · · , regard-
less of whether the associated log-likelihood is concave or convex or even differentiable
at all. There is no analogue of temperature, so there is never any thermal catastrophe.
Nested points will pass through the steam phase to the supercooled region, then steadily
into superheated water until the ordinary water phase is reached, traversed and left be-
hind in an optional continued search for ice. All the internal Bayes factors are available,
so the dominant phase can be identiﬁed and quantiﬁed.
This, then, is a second illustration of the ability of nested sampling to solve multi-
phase problems.
9.6.2
Example: order/disorder in a pseudo-crystal
Consider the following elementary model of order/disorder based on ‘switches’, each
of which can be in either of two states, 0 or 1. A sequence of M switches is laid out along
a line, so there are 2M equally-weighted prior states. The switches deﬁne a sequence
of clusters c with widths hc across which the state is constant. For example, the ten
switches 0001111001 have four clusters, respectively three 0’s (width h1 =3), followed
by four 1’s (h2 = 4), then two 0’s (h3 = 2) and ﬁnally a single 1 (h4 = 1). Each cluster
has an energy beneﬁt (i.e. a log-likelihood gain) proportional to the number 1
2h(h−1)

Simulated annealing
205
of internal interactions permitted among its members, so that (with speciﬁc scaling)
log L =
2
M

c
1
2 hc (hc−1) ,

c
hc = M .
The example state 0001111001 has log L=2. Of the 1024 states of 10 switches, the top
two (0000000000 and 1111111111) are fully ordered with log L=9 and share 49% of
the posterior, the next four (0000000001, 0111111111, 1000000000, 1111111110) with
log L = 7.2 share another 16%, and so on down to the two lowest states (0101010101
and 1010101010) with log L=0 which share 0.006%.
Figure 9.13 shows the behaviour for M =1000, precisely calculated by recurrence
on M. An ‘order’ phase with wide clusters dominates, having the fully-ordered states
0000 · · · and 1111 · · · with log L = 999 sharing 71% of the posterior, the next four
with log L = 997.002 sharing 19% and so on. With M being quite large, there is also
a well-separated ‘disorder’ phase with most clusters narrow, and the ‘order’ phase is
favoured overall by a Bayes factor of e300.
An ensemble annealed to β = 1, though, has no chance (technically, about e−110
chance) of ﬁnding the tiny volume occupied by the ‘order’ states. It ought to transition to
the ordered phase at the freezing point β =0.69 where the two phases ought to become
equally populated, but it won’t. As expected, the simulation in Skilling (2004), which
started with a random state and evolved by inverting atoms at random according to
0
-200
-400
-600
-800
0
200
400
600
800
1000
300
110
0
-1
-2
0
1
2
Disorder
-685
-690
-695
 990
 995
1000
 Order
71%
19%
7%
2%
loge L
loge ξ
Fig. 9.13 Order/disorder example for 1000 switches. The upper sub-plot magniﬁes the ‘order’
phase, and the lower sub-plot magniﬁes the ‘disorder’ phase. The order phase is favoured by a
Bayes factor exp(300) but is hard to ﬁnd by a factor exp(110). Dashed lines enclose 75% of
posterior samples for each phase.

206
Nested sampling
the usual detailed balance, failed to move away from the ‘disorder’ phase in an allotted
trillion trial inversions. Even if by incredible luck it had succeeded in ﬁnding the ‘order’
phase, it could not have determined the evidence Z = e307.
Yet, nested sampling (with a fresh sample within the likelihood constraint approx-
imated by allowing trial MCMC inversions of each switch ten times per iterate, and a
collection of any size n) successfully estimates log L as a function of log ξ and reaches
the fully-ordered states steadily, in the expected nH ≈700n iterates.
9.6.3
Programming the pseudo-crystal in ‘C’
// apply.c
"PSEUDO-CRYSTAL" NESTED SAMPLING APPLICATION
//
(GNU GENERAL PUBLIC LICENSE software: c⃝Sivia and Skilling 2006)
// Problem:
M switches s = 0 or 1, grouped in clusters of widths h.
//
e.g. M=10, s = {0,0,0,1,1,1,1,0,0,1}
//
h = {
3
,
4
, 2 ,1}
// Inputs:
//
Prior(s)
is uniform, 1/2ˆM on each of 2ˆM states
//
Likelihood
is L(s) = exp( SUM h(h-1)/M )
// Outputs:
//
Evidence
is Z = SUM L(s) Prior(s)
//
Posterior
is P(s) = L(s) Prior(s) / Z
//
Information
is H = SUM P(s) log(P(s)/Prior(s))
/*
*/
#define n
1
// # Objects
#define MAX 800
// # iterates
#define M
1000
// # switches in this application
/*
*/
typedef struct
{
char
s[M];
// state of switches
double
logL;
// logLikelihood = ln prob(data | s)
double
logWt; // log(Weight), adding to SUM(Wt) = Evidence Z
} Object;
/*
*/
double logLhood(
// logLikelihood function
char*
s)
// switches
{
int
i, j;
// left and right counters
double logL = 0;
// logLikelihood accumulator
i = 0;
// L.H. boundary
for( j = 1; j < M; j++ )
if( s[j] != s[j-1] )
{
// R.H. boundary found
logL += (j - i) * (j - i - 1);
// cluster width h = j-i
i = j;
// reset L.H. boundary
}
logL += (j - i) * (j - i - 1);
// R.H. cluster
return logL / M + sqrt(DBL EPSILON) * UNIFORM;
// normalized
}
// jitter eliminates ties between likelihood values
/*
*/

Simulated annealing
207
void Prior(
// Set Object according to prior
Object* Obj)
// Object being set
{
int
j;
for( j = 0; j < M; j++ )
Obj->s[j] = (int)(2 * UNIFORM) % 2; // 0 or 1
Obj->logL = logLhood(Obj->s);
}
/*
*/
void Explore(
// Evolve object within likelihood constraint
Object* Obj,
// Object being evolved
double
logLstar)
// Likelihood constraint L > Lstar
{
int
m = 10 * M;
// MCMC counter (pre-judged # steps)
int
try;
// Try ﬂipping this switch
double
logLtry;
// Trial loglikelihood
for( ; m > 0; m-- )
{
try = (int)(M * UNIFORM) % M;
// random switch
Obj->s[try] = 1 - Obj->s[try];
// try ﬂipping
logLtry = logLhood(Obj->s);
// trial loglikelihood
if( logLtry > logLstar )
Obj->logL = logLtry;
// accept
else
Obj->s[try] = 1 - Obj->s[try]; // reject
}
}
/*
*/
void Results(
// Output nested sampling sequence
Object* Samples,
// Objects deﬁning posterior
int
nest,
// # Samples
double
logZ)
// Evidence (= total weight = SUM[Samples] Weight)
{
int
k;
// Sample counter
for( k = 0; k < nest; k++ )
printf("%7.2f %8.4f\n", -(k+1.) / n, Samples[k].logL);
}
// print log(enclosed prior mass), log(likelihood)
The aim of the above module is to estimate the density of states by plotting log L
as a function of log ξ, in order to recover the theoretical Fig. 9.13. Hence the Object
structure needs to contain a vector s of M = 1000 switches, as well as the usual log-
likelihood and logarithmic weight as used by the main program.
Procedure Prior just sets the switches at random, before calculating the corre-
sponding likelihood. This being a discrete problem, a small amount of random jitter is
added to each likelihood, in the logLhood function. After all, nested sampling is try-
ing to compress the available domain by a factor of exp(1/n) or so each step, and this
would be difﬁcult if a single likelihood value occupied more prior mass than that.
Procedure Explore takes a starting state s — in fact a copy of one of the other
positions — and generates a new and supposedly independent state from it, subject to
likelihood L exceeding the current limit L∗. It does this by trying to ﬂip each of the

208
Nested sampling
0
-200
-400
-600
-800
0
200
400
600
800
1000
loge L
loge ξ
Fig. 9.14 Nested sampling (line) recovers the theoretical shape (shaded) of Fig. 9.13.
1000 switches about 10 times. Any trial position obeying the likelihood constraint is
accepted, resulting in movement. Otherwise, the trial is rejected and there is no change.
It would be more efﬁcient to calculate only the local changes induced by ﬂipping a
switch, but for transparency the likelihood is here computed from scratch.
Finally, procedure Results prints out the nested sampling trajectory, ready for
plotting as Fig. 9.14.
Even though only one object was used in this run, the density curve is reproduced
well, with just the expected random drifts reﬂected in an evidence estimate log eZ =
280 ± 27 lying a plausible one standard deviation from the true value of 307. The two-
phase character of the system shows up clearly, as does its convex nature indicating the
difﬁculty of annealing it. Nested sampling took 720 iterates to reach the fully-ordered
states, consistent with the expected H ±
√
H with H = 692.

10
Quantiﬁcation
In this chapter, we extend the basic exploratory techniques of Chapter 9, aiming to
answer the perennial questions ‘What’s there, and how much?’. As so often in algorithm
design, there are some nice tricks to be found. Here are some of them.
10.1
Exploring an intrinsically non-uniform prior
We now consider problems in which the discrete states of a system are assigned different
prior probabilities, which cannot usefully be transformed to uniformity. It is possible to
choose potential transitions at random, as before. Indeed, many writers recommend just
that, using rejections to control the desired non-uniformity, as in Metropolis exploration
described in Section 9.4.4. Our view, though, is that most priors are sufﬁciently simple
to be explored efﬁciently without rejection, and that the insight required to accomplish
this is worthwhile.
Each transition, A→B and so on, is assigned a correspondingrate rBA. If the system
starts in state A it will evolve to B at rate rBA per unit time:
prob(B|A) = rBA dt
(10.1)
in a small interval dt. Then, if our knowledge of the system is uncertain, with probability
π(A) of being in A, the ﬂux from A to B in time dt will be π(A) rBA dt. Practical
schemes adopt the detailed balance convention
A
B
-

rBA
rAB
with
rBA
rAB
= π(B)
π(A)
(10.2)
in which forward and backward ﬂuxes become equal when the states are populated
according to their priors.
From current state X, the total rate to other states is R = 
Y ̸=X rYX, whence the
interval τ to the next transition away from X will be distributed as
prob(τ) = R exp(−Rτ) .
(10.3)
So we can jump directly to the next transition by sampling
τ = −R−1 log [Uniform(0, 1)] .
(10.4)
When a transition falls due, we list the rates rYX for all available destinations Y and
select Y according to
prob(Y |X) = rYX
R .
(10.5)

210
Quantiﬁcation
Finally, when the accumulated time would cross an assumed equilibration time, we
deem the current state to be the (sufﬁciently) independent new sample that we seek. In
this way, the prior is explored without any wasteful rejection.
Incorporating the likelihood constraint is easy. All we do is reject any transition that
would lead to a destination disobeying the constraint. Time is incremented as before,
but the source state is preserved. As before, the prior is explored freely for transitions
within the constraint, but transitions to outside are forbidden.
10.1.1
Binary trees for controlling MCMC transitions
As a technical trick, identifying the next transition from a choice of M can be done in
log(M) operations instead of the more obvious M, provided the transition scheme is
created with this in mind. Pad the M rates (say 5 of them, r1, r2, r3, r4, r5) with zeros
until the vector length M + is a power of 2 (here M + =23 =8). Then arrange a binary
tree of partial sums.
r1+r2+r3+r4+r5
r1+r2+r3+r4
r5
r1+r2
r3+r4
r5
0
r1
r2
r3
r4
r5
0
0
0
This involves doubling the power-of-2 storage but requires only a single O(M) pass to
set up. Thereafter, if a local transition from X to a nearby Y only affects one or a few
of the base rates, the tree can be kept up-to-date by updating only those few base cells
and the log2 M + cells above each. Selecting a particular rate involves ﬁnding the base
cell j at which the cumulant
r1 + r2 + . . . + rj > fR ,
R = r1 + r2 + . . . . . . + rm
(10.6)
ﬁrst exceeds some random fraction f of the total rate R. With the tree in place, this can
easily be done in log2 M + steps by using the partial sums to switch left or right at each
descending level. As a by-product, the total R is always available at the top of the tree.
Addressing within a binary tree is particularly slick if storage addresses are assigned
as follows, with the total at address #1.
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
The otherwise-unused address #0 holds M +, making the tree self-contained, of total
size 2 × M +.
The accompanying procedures PutRate and GetRate update and use the trees.
PutRate encodes the obvious pattern of additions consequent upon inserting a new
individual rate value. GetRate selects a random rate from the total, which involves
subtraction as the tree is descended from its apex to identify the component covering the

Exploring an intrinsically non-uniform prior
211
randomly-selected cumulant as in eqn (10.6). In ﬂoating-point arithmetic, subtraction
ampliﬁes relative rounding errors, leading to the possibility of selecting a component
known to have zero rate. Such anomalous behaviour is avoided by GetRate, the proof
of robustness being in the accompanying comments.
// tree.c
PROCEDURES FOR BINARY TREE
//
(GNU GENERAL PUBLIC LICENSE software: c⃝Sivia and Skilling 2006)
// Example: m=5 rates (r0,r1,r2,r3,r4) are stored using
//
mplus=8 cells, and are preceded by partial sums.
//
+-------------------------------------------------------+
//
T |1
r0+r1+r2+r3+r4
|
//
|-------------------------------------------------------|
//
R |2
r0+r1+r2+r3
|3
r4
|
//
|---------------------------|---------------------------|
//
E |4
r0+r1
|5
r2+r3
|6
r4
|7
0
|
//
|-------------|-------------|-------------|-------------|
//
E |8
r0 |9
r1 |10 r2 |11 r3 |12 r4 |13
0 |14
0 |15
0 |
//
+------+------+------+------+------+------+------+------+
//
Tree[0] holds the value of mplus, here 8.
/*
*/
void PutRate(
// Update binary tree with new rate
double* Tree,
// Binary tree of rates
int
j,
// Input cell (not including mplus)
double
r)
// New value of rate
{
j += (int)Tree[0];
Tree[j] = r;
for( ; j > 1; j >>= 1 )
Tree[j>>1] = Tree[j] + Tree[jˆ1];
}
/*
*/
int GetRate(
// Select cell (not including mplus) with random rate
double* Tree)
// Binary tree of rates
{
// This procedure is crafted never to select a cell having rate=0
double f;
int
j;
int
mplus = (int)Tree[0]; // Assuming Tree[1] > 0, then
f = Tree[1] * UNIFORM;
// 0 < f ⩽Tree[1] for any rounding.
for( j = 1; j < mplus; )
{
// Enter with
j <<= 1;
// f ⩽Tree[parent] = Tree[j]+Tree[j+1] ⩾Tree[j]
if( f > Tree[j] ) // Only possible if Tree[j] < Tree[parent],
{
// so that Tree[j+1] > 0,
f -= Tree[j++];
// hence j+1 is a safe destination.
if( f > Tree[j] )
// Rounding error occasionally
f = Tree[j];
// matters, so keep f ⩽Tree[j].
}
// Either way, f > 0 by construction.
}
// Else 0 < f ⩽Tree[j], so Tree[j] > 0,
return j - mplus;
// hence the entry j was a safe destination.
}

212
Quantiﬁcation
10.2
Example: ON/OFF switching
We now specialize to problems in which the object in question is a set of N binary
ON/OFF switches. This model is the kernel of a variety of problems involving mix-
tures or combinations of constituents. Each switch can be ON (1) or OFF (0) with pre-
assigned prior probability
prob

switch s is
5 ON
OFF
6
=
5 πs
1−πs
6
.
There are 2N states in all, and we need a transition scheme that reaches all of them.
10.2.1
The master engine: ﬂipping switches individually
The simplest, most obvious, scheme allows one switch at a time to ﬂip ON or OFF.
This is the master transition scheme, known as an engine, but other engines may also
be useful. Clearly all states are accessible, but not always in a single step. From any
particular state, any single switch can be ﬂipped, yielding N transitions. In accordance
with detailed balance, switching ON from OFF should have a rate proportional to π,
whilst OFF from ON should have 1−π instead.
OFF
ON
-

π
1−π
For example, the state 00110 in which the third and fourth switches out of ﬁve are ON
would have available transitions
r = κ ( π1 , π2 , 1−π3 , 1−π4 , π5 )
to states 10110, 01110, 00010, 00100, 00111 respectively, where κ is some normalizing
engine throttle setting. κ is also the rate at which each switch equilibrates. The natural
setting is κ = 1, for which equilibration times are O(1). We do not attempt to max-
imize the individual rates, Metropolis-style, because that would make some switches
equilibrate slower than others.
Finding the interval τ to the next transition involves the total rate R, where
R = κ

π1 + π2 + (1−π3) + (1−π4) + π5

.
Selecting this next transition involves sampling from r/R. Performing it involves ﬂip-
ping the switch ON/OFF and inverting its rate (π ←→1−π). All the book-keeping can
be done at cost log N by using a binary tree of the N rates available at the current time.
All transitions that fall within the likelihood constraint are accepted, because the rates
have been set to explore the prior faithfully.
10.2.2
Programming which components are present
The above ideas are coded into the following ‘C’ application module for estimating
which components of a mixture are present. In this little problem, there may be ﬁve

Example: ON/OFF switching
213
components, coded as M=5 switches s, each of which has its prior probability π, coded
as PrON, of being ON. The likelihood function has three factors, the ﬁrst of which
claims it unlikely (prob = 0.2) that either component 2 or component 4 is ON. On the
other hand, the second factor claims that either 3 or 4 is rather likely to be ON, with
probability 0.9. The third factor claims that the probability of both 1 and 4 being ON
is 0.8. Such data idealize the sort of information often available in spectrometry, where
individual lines relate ambiguously to components of a mixture.
In the code, the Object structure holds the 5 switch settings s, as well as their
log-likelihood logL and the weight logWt controlled by the main program. It also
holds the corresponding binary tree of weights, dimensioned through Mplus, being the
power-of-2 next above M (i.e. 8).
// apply.c
"ON/OFF SWITCHING" NESTED SAMPLING APPLICATION
//
(GNU GENERAL PUBLIC LICENSE software: c⃝Sivia and Skilling 2006)
// Problem:
5 switches s, with probabilistic data.
// Inputs:
//
Prior(s)
is {0.1, 0.2, 0.3, 0.2, 0.1} for the 5 switches.
//
Likelihood
is Pr(s[2] OR s[4]) = 0.2, Pr(s[3] OR s[4]) = 0.9,
//
Pr(s[1] AND s[4]) = 0.8.
// Outputs:
//
Evidence
is Z = SUM L(s) Prior(s)
//
Posterior
is P(s) = L(s) Prior(s) / Z
//
Information
is H = SUM P(s) log(P(s)/Prior(s))
/*
*/
#define n
100
// # Objects
#define MAX 1000
// # iterates
#define M
5
// # switches
#define Mplus
8
// power-of-2 ⩾M
#include "tree.c"
// binary tree procedures void PutRate, int GetRate
const double PrON[M] = {0.1, 0.2, 0.3, 0.2, 0.1};
// Prior(s is ON)
/*
*/
typedef struct
{
char
s[M];
// switches
double
Tree[2*Mplus]; // binary rate tree
double
logL;
// logLikelihood = ln prob(data | s)
double
logWt;
// log(Weight), with SUM(Wt) = Evidence Z
} Object;
/*
*/
double logLhood(
// logLikelihood function
char*
s)
// switches
{
double
L = 1.0;
L *= (s[2] | s[4]) ? 0.2 : 0.8;
L *= (s[3] | s[4]) ? 0.9 : 0.1;
L *= (s[1] & s[4]) ? 0.8 : 0.2;
return
log(L) + sqrt(DBL EPSILON) * UNIFORM;
}
// jitter eliminates ties between likelihood values
/*
*/

214
Quantiﬁcation
void Prior(
// Set Object according to prior
Object* Obj)
// Object being set
{
int
i;
// Initialize empty Tree of transition rates
Obj->Tree[0] = Mplus;
for( i = 1; i < 2*Mplus; i++ )
Obj->Tree[i] = 0.0;
// Initialize object
for( i = 0; i < M; i++ )
{
Obj->s[i] = (UNIFORM < PrON[i]);
PutRate(Obj->Tree, i, Obj->s[i] ? 1.-PrON[i] : PrON[i]);
}
Obj->logL = logLhood(Obj->s);
}
/*
*/
void Explore(
// Evolve object within likelihood constraint
Object* Obj,
// Object being evolved
double
logLstar)
// Likelihood constraint L > Lstar
{
double Interval = 30.0; // pre-judged time to equilibrate
double t = 0.0;
// evolution time, initialized 0
double logLtry;
// trial logLikelihood
int
i;
// switch being ﬂipped
while( (t += -log(UNIFORM) / Obj->Tree[i]) < Interval )
{
i = GetRate(Obj->Tree);
Obj->s[i] = 1 - Obj->s[i];
// trial state
logLtry = logLhood(Obj->s);
if( logLtry > logLstar )
{
// accept
Obj->logL = logLtry;
PutRate(Obj->Tree, i,
Obj->s[i] ? 1.-PrON[i] : PrON[i]);
}
else
Obj->s[i] = 1 - Obj->s[i];
// reject
}
}
/*
*/
void Results(
// Posterior properties, here mean of s
Object* Samples,
// Objects deﬁning posterior
int
nest,
// # Samples
double
logZ)
// Evidence (= total weight = SUM[Samples] Weight)
{
double post[M] = {0,0,0,0,0};
// posterior prob(ON)
double w;
// Proportional weight
int
i;
// Sequence counter
int
k;
// Sample counter
for( i = 0; i < nest; i++ )

Example: ON/OFF switching
215
{
w = exp(Samples[i].logWt - logZ);
for( k = 0; k < M; k++ )
if( Samples[i].s[k] )
post[k] += w;
}
for( k = 0; k < M; k++ )
printf("%d:
Posterior(ON)=%3.0f%%\n", k, 100.*post[k]);
}
Procedure Prior sets the switches at random, according to their prior, before calcu-
lating the corresponding likelihood— including the small random jitter recommended
for a discrete problem. It also initializes the binary tree of transition rates through
PutRate. Procedure Explore sets up a time Interval after which the evolving
sample is deemed to be sufﬁciently independent of its starting state. Meanwhile, the
clock-time t accumulates the sub-intervals between transitions as calculated by eqn
(10.4). At each transition moment, the appropriate switch is selected by GetRate ac-
cording to eqn (10.5), and ﬂipped. Then the likelihood constraint is applied to deter-
mine if the transition is to be accepted, in which case the new likelihood is adopted, or
whether it is to be rejected, in which case the ﬂipped switch is reset to its previous state.
Finally, the Results procedure accumulates the posterior probability of each switch
being ON.
This being such a small problem, correct results can be evaluated by brute force
addition over the 32 possible switch settings, and compared with the computed results
Correct
Computed
logeZ
−3.36
−3.32∗
H
0.63
0.61
prob(s0 is ON)
10%
6%
prob(s1 is ON)
25%
25%
prob(s2 is ON)
13%
14%
prob(s3 is ON)
61%
63%
prob(s4 is ON)
17%
14%
∗with ±0.08 numerical uncertainty
There is no signiﬁcant or systematic difference between correct and computed results.
10.2.3
Another engine: exchanging neighbouring switches
Neighbouring components in a mixture commonly have substantial similarities, so that
their switches might be correlated. For efﬁcient exploration, there could be explicit tran-
sitions between switches that are known to be related. For example, an ON setting could
move directly between the fourth and ﬁfth positions
· · · 10 · · ·· · ·
←→· · · 01 · · · · · ·
instead of having to move inefﬁciently in two steps via · · · 00 · · ·· · · or · · · 11 · · · · · ·
(and hoping that at least one of these intermediate states was consistent with the data,
so would not be prohibited).

216
Quantiﬁcation
To accord with detailed balance, the transition rate from the former state to the latter
should be
r···01······ , ···10······ = κ′ (1−π4) π5 .
(10.7)
· · · 10 · · ·· · ·
· · · 01 · · · · · ·
-

(1−π4) π5
π4 (1−π5)
This second transition engine has its own throttle setting κ′, which may differ from the
ﬁrst engine’s throttle. Indeed, there could be several such engines, each designed to pick
out transitions that are sympathetic to the particular application in hand.
10.2.4
The control of multiple engines
If there are several engines, each of them 1, 2, 3, . . . has its family of transition rates,
culminating in the total rate R(1), R(2), R(3), . . . at which it is supplying events. The
entire rate at which all the engines together are working is R, where
R = κ(1)R(1)+ κ(2)R(2)+ κ(3)R(3)+ · · ·
As above, ﬁnding the interval τ to the next transition involves this entire rate, and select-
ing the appropriate engine involves sampling from κ R/R, after which its appropriate
transition is selected.
Entire rate
R
· · ·
. . .
κ(1)R(1)
κ(2)R(2)
κ(3)R(3)
· · ·
throttled engine rates
engine 1
R(1)
· · ·
. . .
r11
r21
r31
· · ·
components
engine 2
R(2)
· · ·
. . .
r12
r22
r32
· · ·
components
engine 3
R(3)
· · ·
. . .
r13
r23
r33
· · ·
components
· · ·
Generally, it is recommended practice to tune the throttle settings κ in order to keep
the computational resources allotted to different engines more-or-less balanced. Then,
no individual engine will dominate the CPU, and even if one of them turns out to be
less efﬁcient for the particular problem in hand, the waste of resources will not be over-
whelming.
10.3
Estimating quantities
Commonly in applications, each ON switch is accompanied by an associated positive
quantity q, OFF being indistinguishable from q = 0. In spectrometry for example, q
might be the concentration of a component chemical that may or may not be present

Estimating quantities
217
in the source. If it is present, with prior probability assigned as π, it would have an
associated amount q, speciﬁed perhaps by an exponential prior or (often better)
prob(q) = a/(a + q)2
(10.8)
which expresses a preference for q being O(a), whilst not discriminating strongly against
occasional larger values. Presence and quantity could be combined into a single prior
prob(q) = (1 −π)δ(q) + πa/(a + q)2
but it’s computationally more efﬁcient to keep them separate. In accordance with the
recommendation to have such quantities slaved to controlling variables with respect to
which the prior is uniform, we let
q = a
u
1 −u ,
u =
q
a + q ,
(10.9)
where q is controlled by u having ﬂat prior over (0,1). Speciﬁcally, we sample q from
its prior by sampling
u = Uniform(0, 1)
(10.10)
and substituting back.
We saw in previous sections how to control switching by operating various engines.
Here, using the master engine which ﬂips one switch at a time, we extend this to include
quantities. Exploration of the prior is straightforward. We generate ON/OFF transitions
just as in Section 10.2.1. An ON-to-OFF transition is just that; the switch is turned OFF
and the quantity ignored. An OFF-to-ON transition requires an accompanying quantity,
which is best sampled anew with q re-sampled through eqns (10.10) and (10.9). That
completes exploration of the prior.
The likelihood function will involve the quantities q, often enough as a Gaussian
L = exp(−quadratic function of the q’s)
(10.11)
as from data linearly related to the quantities and subject to Gaussian noise. Spectro-
scopic data, for example, are very often of this form. Within nested sampling, the current
likelihood constraint L > L∗imposes a restriction on the permitted quantities. In par-
ticular, an individual q will be constrained within the interval between the roots q−and
q+ of the quadratic equation log L=log L∗. If the quadratic equation has no real roots,
or if the upper root q+ is negative, that particular q cannot be present at all — and must
already be OFF because the current state always obeys the constraint. Otherwise, the
quantity in question (which has to be positive) is restricted to
q0 < q < q+
(10.12)
where q0 = max(q−, 0), which is equivalent through eqn (10.9) to a related range
u0 < u < u+
(10.13)
for its controlling variable u.

218
Quantiﬁcation
Suppose ﬁrst that the range (q0, q+) exists and does not include q = 0 (i.e. q0 =
q−> 0), so that the OFF state is prohibited. Necessarily, the switch must currently be
ON, and a trial transition to OFF will certainly fail. Even so, we are still allowed to
re-sample q within its range, by setting its controller as
u = Uniform(u0, u+)
(10.14)
So at least the ‘rejected’ transition is not wasted; it can re-equilibrate its quantity q.
\/O \/F \/F
ON, new q
⊂−−−
−
−−−→
1−π
Now suppose that the range (q0, q+) does include q = 0 (i.e. q0 = 0), so that OFF
is a valid state. An ON-to-OFF transition would always be accepted. An OFF-to-ON
transition is accompanied by a new q, but this will only be accepted if it falls within the
range, for which the probability is
α = u+ −u0
Equivalently, compute α ﬁrst, then with probability α ﬂip the switch ON and re-sample
q through eqns (10.14) and (10.9).
OFF
ON, q
-

π, acceptance α
1−π
The only wasted computations here are the rejected OFF-to-ON transitions, which are
only frequent if the prior favours ON (large π) while the likelihood favours OFF (small
α, usually accompanying small q) — in practice a relatively unusual combination.
Other transition engines can be programmed similarly. For the transition of eqn
(10.7) involving neighbouring switches, the change should be accompanied by an ac-
ceptance probability proportional to the destination’s α
· · · 10 · · · · · ·
· · · 01 · · · · · ·
-

(1−π4) π5, acceptance α5/c
π4 (1−π5) , acceptance α4/c
where c = max(α4, α5). This preserves detailed balance in the usual way, whilst min-
imizing the number of rejected transitions.
10.3.1
Programming the estimation of quantities in ‘C’
As in Section 10.2.2, we allow ﬁve components, now coded as M =5 quantities q, each
of which has a prior probability, coded as PrON, of being ON. A component that is OFF
has its quantity q set to 0, while a component that is ON has positive quantity with prior
prob(q)=1/(1 + q)2, as in eqn (10.9) in units where a=1. The likelihood function is
Gaussian
L(q) = exp

−χ2/2

with normalization factor ignored for simplicity, where χ2 is the chi-squared misﬁt

Estimating quantities
219
χ2 =
N−1

k=0
(Fk −Dk)2 ,
scaled to unit noise σk=1, between the N actual data D and mock data F, namely
Fk =
M−1

j=0
Tkj qj ,
the response matrix T of these simulated measurements being coded as Expt. The
Object structure is unchanged, except that the binary switches s are up-graded to
ﬂoating-point quantities q. The binary-tree scheme of transition rates is also unchanged.
However, the logLhood function has to calculate the residuals resid = F −D, and
thence χ2, in order to reach the required log-likelihood value.
// apply.c
"QUANTIFICATION" NESTED SAMPLING APPLICATION
//
(GNU GENERAL PUBLIC LICENSE software: c⃝Sivia and Skilling 2006)
// Problem:
5 quantities q, which can be ON (+ve) or OFF (zero),
//
as measured by linear data with Gaussian noise.
// Inputs:
//
Prior(q)
is prob(ON) = PrON with prob(q | ON) = 1/(1+q)ˆ2
//
Likelihood
is exp(-chisquared/2)
//
chisquared = SUM residualˆ2
//
residual = mock data - actual data
//
mock data = [Expt response].[quantities q]
// Outputs:
//
Evidence
is Z = INTEGRAL Likelihood(q) * Prior(q) dq
//
Posterior
is P(q) = Likelihood(q) * Prior(q) / Z
//
Information
is H = INTEGRAL P(q) log(P(q)/Prior(q)) dq
/*
*/
#define n
100
// # Objects
#define MAX 1000
// # iterates
#define M
5
// # switches
#define Mplus
8
// power-of-2 ⩾M
#define N
3
// # data
#include "tree.c"
// binary tree procedures void PutRate, int GetRate
const double PrON[M] ={0.1, 0.2, 0.3, 0.2, 0.1}; // prior prob(ON)
const double Data[N] ={3,6,9};
// actual data
const double Expt[N][M] ={{0,1,2,3,4},
// 1st data response
{0,0,3,2,1},
// 2nd data response
{3,2,1,2,3}};
// 3rd data response
/*
*/
typedef struct
{
double
q[M];
// quantities, 0 = OFF else prob(q)=1/(1+q)ˆ2
double
Tree[2*Mplus]; // binary rate tree
double
logL;
// logLikelihood = ln prob(data | q)
double
logWt;
// log(Weight), with SUM(Wt) = Evidence Z
} Object;
/*
*/

220
Quantiﬁcation
double logLhood(
// logLikelihood function
double* q)
// quantities
{
double resid;
// residual = (mock - actual) data
int
j, k;
// component and data counters
double C = 0.0;
// chisquared
for( k = 0; k < N; k++ )
{
resid = -Data[k];
for( j = 0; j < M; j++ )
resid += Expt[k][j] * q[j];
C += resid * resid;
}
return
-0.5 * C + sqrt(DBL EPSILON) * UNIFORM;
}
// jitter eliminates ties between likelihood values
/*
*/
void Prior(
// Set Object according to prior
Object* Obj)
// Object being set
{
double u;
int
i;
// Initialize empty Tree of transition rates
Obj->Tree[0] = Mplus;
for( i = 1; i < 2*Mplus; i++ )
Obj->Tree[i] = 0.0;
// Initialize object
for( i = 0; i < M; i++ )
{
u = (UNIFORM < PrON[i]) ? UNIFORM : 0.0;
Obj->q[i] = u / (1.0 - u);
PutRate(Obj->Tree, i, (u > 0.0) ? 1.-PrON[i] : PrON[i]);
}
Obj->logL = logLhood(Obj->q);
}
/*
*/
double TryQ(
// revised trial q[i]
double* q,
// quantities
int
i,
// id of quantity q[i] to be varied
double
logLstar)
// constraint
{
double resid;
// residual = (mock - actual) data
int
j, k;
// component and data counters
double A, B, C, D;
// quadratic coeffs and discriminant
double u;
// controlling variable for q[i]
double min, max;
// range, of q[i] then of u
// “L > Lstar” is quadratic interval “A*x*x + 2*B*x + C ⩽0.0”
A = B = 0.0;
C = 2.0 * logLstar;
// minus chisquared on entry
for( k = 0; k < N; k++ )
{
resid = -Data[k];
for( j = 0; j < M; j++ )

Estimating quantities
221
resid += Expt[k][j] * q[j];
A += Expt[k][i] * Expt[k][i];
B += Expt[k][i] * resid;
C += resid * resid;
}
// Find controlling interval
if( A > 0.0 )
// q[i] does affect data
{
// Solve quadratic for (qmin,qmax) relative to q[i]
D = B * B - A * C;
// discriminant
if( D > 0.0 )
// distinct real roots
{
if( B > 0.0 )
{ min = -B - sqrt(D);
max = C / min;
min /= A; }
else
{ max = -B + sqrt(D);
min = C / max;
max /= A; }
// Reset (qmin,qmax) relative to origin q=0
min += q[i];
max += q[i];
// Restrict (qmin,qmax) to non-negative values
if( max <= 0.0 )
max = min = 0.0;
else if( min < 0.0 )
min = 0.0;
// Transform to controlling interval (umin,umax)
min /= 1.0 + min;
max /= 1.0 + max;
}
else
min = max = 0.0;
// no real roots, so null interval
}
else
{
// q[i] unmeasured, so
min = 0.0;
max = 1.0;
// all u are in range
}
// Accept/Reject
if( q[i] == 0.0 )
// entry state OFF
u = (UNIFORM < max - min)
// accept ON, sample u
? min + (max - min) * UNIFORM : 0.0;
else
// entry state ON
u = (min > 0.0)
// reject OFF, re-sample u
? min + (max - min) * UNIFORM : 0.0;
return
u / (1.0 - u);
// trial quantity
}
/*
*/
void Explore(
// Evolve object within likelihood constraint
Object* Obj,
// Object being evolved
double
logLstar)
// Likelihood constraint L > Lstar
{
double Interval = 30.0; // pre-judged time to equilibrate
double t = 0.0;
// evolution time, initialized 0
double qold;
// entry value
double logLtry;
// trial logLikelihood
int
i;
// quantity being changed
while( (t += -log(UNIFORM) / Obj->Tree[i]) < Interval )
{

222
Quantiﬁcation
i = GetRate(Obj->Tree);
qold = Obj->q[i];
// enable recovery of entry state
Obj->q[i] = TryQ(Obj->q, i, logLstar);
// trial state
logLtry = logLhood(Obj->q);
if( logLtry > logLstar )
{
Obj->logL = logLtry;
// accept
PutRate(Obj->Tree, i,
(Obj->q[i]>0.) ? 1.-PrON[i] : PrON[i]);
}
else
Obj->q[i] = qold;
// reject
}
}
/*
*/
void Results(
// Posterior properties, here statistics of s
Object* Samples,
// Objects deﬁning posterior
int
nest,
// # Samples
double
logZ)
// Evidence (= total weight = SUM[Samples] Weight)
{
double post[M] = {0,0,0,0,0}; // posterior prob(ON)
double mean[M] = {0,0,0,0,0}; // quantity mean (when ON)
double var[M]
= {0,0,0,0,0}; // variance (when ON)
double w;
// Proportional weight
int
i, k;
// Sequence and sample counters
for( i = 0; i < nest; i++ )
{
w = exp(Samples[i].logWt - logZ);
for( k = 0; k < M; k++ )
if( Samples[i].q[k] > 0.0 )
{
post[k] += w;
mean[k] += w * Samples[i].q[k];
var[k]
+= w * Samples[i].q[k]*Samples[i].q[k];
}
}
for( k = 0; k < M; k++ )
{
mean[k] /= post[k];
var[k] = var[k] / post[k] - mean[k] * mean[k];
printf("%d: Posterior(ON) =%3.0f%%,", k, 100.*post[k]);
printf(" q =%6.2f +-%6.2f\n", mean[k], sqrt(var[k]));
}
}
As in the earlier pseudo-crystal module, procedure Prior sets the quantities ac-
cording to their prior, before calculating the corresponding likelihood— including the
small jitter recommended for a discrete problem in which some transitions (speciﬁcally,
OFF-to-OFF) do not otherwise alter the likelihood. It also initializes the binary tree
of ON/OFF transition rates through PutRate. Procedure Explore is again slightly
up-graded. As well as ﬂipping an ON/OFF state, it has to control the corresponding
quantity, which has to be stored (as qold) on entry in case the transition is rejected and

Final remarks
223
the entry state has to be recovered. Also, the task of proposing a new trial quantity is
more complicated, so is delegated to a new procedure TryQ.
Procedure TryQ encodes the re-sampling suggested in Section 10.3 above. It starts
by generating the residuals, from which the quadratic eqn (10.11) is derived, initially in
terms of the deviation δq from the current value. Computation would be faster if the
residuals were stored as part of their Object structure, and updated when necessary,
but the code would be longer. The quadratic is solved to obtain the allowed q-range of
eqn (10.12), which is transformed to the controlling u-range of eqn (10.13). Then u is
appropriately re-sampled within its range, and the corresponding trial quantity returned.
This proposed q is designed to obey the likelihood constraint automatically, but round-
ing errors or (more pertinently) likelihood-jitter may break the constraint. Whatever the
cause, if the trial q results in an inadmissible likelihood, the trial is rejected by reverting
to the original entry state.
Finally, the Results procedure accumulates the posterior probability of each quan-
tity being ON (What’s there...?), and the mean and standard deviation of that quantity if
it’s present at all (...and how much?). Brute force integration gives the true answers for
comparison with the computed results.
Correct
Computed
loge Z
−9.17
−9.64∗
H
7.59
7.63
prob(q0 is ON)
99.6%, 2.28 ±0.39
99%, 2.20 ±0.43
prob(q1 is ON)
11.6%, 0.61 ±0.68
17%, 0.63 ±0.71
prob(q2 is ON)
99.6%, 1.76 ±0.32
99%, 1.76 ±0.37
prob(q3 is ON)
9.6%, 0.43 ±0.42
11%, 0.54 ±0.49
prob(q4 is ON)
2.4%, 0.19 ±0.16
4%, 0.14 ±0.15
∗with ±0.28 numerical uncertainty
The computed results are correct to within an unimportant random numerical deviation
that can be further reduced by using more than 100 objects (and iterates) in the nested
sampling.
10.4
Final remarks
In this book, we have tried to show that probability calculus is not just required for
rational inference, but is also straightforward to use. What could be simpler than the
sum and product rules? The algorithms with which we compute probabilistic results
can be similarly straightforward, as we have attempted to demonstrate with short but
powerful programs. Sadly, the research literature often seems a forbidding morass of
extreme technicalities of doubtful importance presented in impenetrable jargon. Yet all
this froth need not imply that important ideas are difﬁcult. Indeed, our experience sug-
gests the reverse, that the best ideas really are simple. We hope that in this book we have
illuminated this simplicity sufﬁciently that Bayesian theory and practice become, like
standard calculus, merely the automatic and un-regarded tools of our readers’ profes-
sional activities.

A
Gaussian integrals
The mathematical techniques and results used in this book should be familiar to most
scientists and engineers from their ﬁrst year undergraduate courses. If the Taylor series,
partial differentiation, multiple integrals, vectors, matrices and so on only bring back
hazy memories, then a browse through your favourite ‘Mathematical Methods’ text-
book is strongly recommended; information on the (slightly) more advanced material
used, such as Lagrange multipliers and Fourier transforms, is also likely to be contained
therein. One topic which is often not dealt with in much detail, but is very useful in
obtaining (at least approximate) analytical results in probability calculations, is that of
Gaussian integrals; it is to this that we devote the next few pages.
A.1
The univariate case
Let’s begin with the integral of a Gaussian function of just one variable
J =
b

a
exp

−x2
2σ2

dx ,
(A.1)
where we have taken the mean, or maximum, to be at the origin (µ = 0) for simplicity.
Despite its trivial look, this can only be evaluated easily when a and b are ±∞or zero.
In that instance, the non-obvious ﬁrst step is to consider the square of eqn (A.1):
J 2 =
b

a
b

a
exp
	
−

x2+ y2
2σ2

dx dy ,
where we have multiplied through by an equivalent expression with y as the dummy
variable (instead of x). If we now make the substitution x = R cos θ and y = R sin θ,
so that it’s like changing from Cartesian to polars coordinates in Fig. 3.14, we can
express J 2 as the product of two straightforward integrals:
J 2 =
θmax

0
dθ
∞

0
R exp

−R2
2σ2

dR ,
where θmax is 2π if a and b are ±∞, and π/2 if they are 0 and ∞, respectively. Since
the ﬁrst term on the right reduces to just θmax, and the second is equal to σ2, we obtain
+∞

−∞
exp

−x2
2σ2

dx = σ
√
2π ,
(A.2)

The bivariate extension
225
or half this value for the semi-inﬁnite range. This is the origin of the normalization
constant in eqn (2.14).
For situations where the limits are not ±∞or zero, the integral of eqn (A.1) has
to be evaluated numerically. We can either do the computation ourselves, perhaps by
simply pressing a button on a calculator, or look up the answer in a standard table of
results; the latter are listed in many books (e.g. Abramowitz and Stegun 1965) and often
formally pertain to the error function:
erf(z) =
2
√π
z

0
e−t2dt ,
(A.3)
which can be related to eqn (A.1) by
b

a
exp

−x2
2σ2

dx = σ
π
2
	
erf
 b
σ
√
2

−erf
 a
σ
√
2

.
(A.4)
If a=−σ and b=+σ, then the integral is equal to 68% of the inﬁnite case in eqn (A.2);
for ±2σ, the fraction rises to just over 95%. Once the limits lie outside ±3σ (from
x=µ), the result is virtually indistinguishable from σ
√
2π.
A.2
The bivariate extension
Having dealt with the one-variable case, let’s consider a two-dimensional Gaussian.
Again taking the maximum to be at the origin (xo = yo = 0), for simplicity, its most
general form is
G(x, y) = exp

−1
2

Ax2+ By2+ 2Cxy

,
(A.5)
where the three constants must satisfy the conditions A>0, B >0 and AB >C2. First
of all, let’s integrate G(x, y) with respect to just one of the variables; if we choose y,
then the resulting marginal function g(x) is
g(x) = exp

−1
2 Ax2 +∞

−∞
exp

−1
2

By2+ 2Cxy

dy ,
(A.6)
where we will always take the limits to be ±∞from now on, to facilitate an analytical
solution. The integral on the right is most easily evaluated by rewriting the exponent as
By2 + 2Cxy = B

y + Cx
B
2
−C 2
B x2 ,
a manipulation which is frequently called ‘completing the square’; substituting this in
eqn (A.6), we obtain
g(x) = exp

−1
2

A −C 2
B

x2

+∞

−∞
exp

−1
2 B (y + φ)2
dy ,

226
Gaussian integrals
where φ = Cx/B. Since the y-integral is now like the one in eqn (A.2), apart from the
unimportant offset φ, with σ2 =1/B, its value is equal to

2π/B ; hence, the marginal
function g(x) is just a Gaussian centred at the origin:
g(x) =
+∞

−∞
G(x, y) dy =

2π
B exp

−x2
2σx2

,
(A.7)
where the variance σx2 is given by
σx
2 =
B
AB −C 2 .
(A.8)
By essentially repeating the procedure above, or by appealing to the symmetry of the
problem, the corresponding integral of G(x, y) with respect to x can also be shown to
be a Gaussian centred at the origin:
+∞

−∞
G(x, y) dx =

2π
A exp

−y2
2σy2

,
(A.9)
where the variance σy2 is given by
σy
2 =
A
AB −C 2 .
(A.10)
Finally, by integrating eqn (A.7) with respect to x, or eqn (A.9) with respect to y, using
the result of eqn (A.2), we ﬁnd that
+∞

−∞
+∞

−∞
G(x, y) dx dy =
2π
√
AB −C 2 .
(A.11)
A.3
The multivariate generalization
To extend the preceding analysis to the case of a Gaussian function of many variables,
we will have to draw quite heavily on some standard results from linear algebra. Let’s
begin by writing the multivariate generalization of eqn (A.5) in matrix–vector notation:
G(x) = exp

−1
2 xTHx

,
(A.12)
where the transpose xT = (x1, x2, . . . , xN) and H is a (real) symmetric matrix, whose
N eigenvalues {λj} must all be positive. Thus eqn (A.5) represents the special case of
N =2, where the elements of H are given by: H11 =A, H22 =B and H12 =H21 =C.
To evaluate the N-dimensional integral of G(x)
Z =

· · ·

G(x) dx1 dx2 · · · dxN ,
(A.13)
let us rotate the axes by making the substitution

The multivariate generalization
227
x = Oy ,
(A.14)
where the columns of the O matrix are the normalized eigenvectors of H; since the latter
are orthogonal to each other, this means that:

OTO

ij = δij
and

OTHO

ij = λj δij ,
(A.15)
where δij is equal to one if i=j, and zero otherwise. With this change of variables, the
exponent in eqn (A.12) becomes
xTH x = (Oy)TH(Oy) = yT
OTHO

y =
N

j=1
λj yj
2 .
(A.16)
Taking determinants, the ﬁrst part of eqn (A.15) gives
1 = det

OTO

= det

OT
det

O

=

det

O
2 ,
where the manipulations on the right follow from the rules that the determinant of a
product of square matrices is equal to the product of their determinants, and that the
swapping around of all the rows and columns of a matrix leaves the determinant un-
changed; the Jacobian,
det

O
, of an orthogonal transformation is therefore unity.
Using this, the second part of eqn (A.15) gives
λ1 λ2 . . . λN = det

OTHO

= det

OT
det(H) det(O) = det(H) .
(A.17)
Accordingly, eqn (A.13) reduces to a simple product of one-dimensional integrals:
Z =
N

j=1

exp

−1
2 λj yj
2
dyj .
Hence, using eqn (A.2), we ﬁnd that
Z =
(2π)N/2
√λ1λ2 . . . λN
.
(A.18)
Finally, using eqn (A.17), the N-dimensional Gaussian integral of eqn (A.13) becomes
Z =

exp

−1
2 xTHx

dNx =
(2π)N/2

det(H)
.
(A.19)
As a simple check, this formula can easily be tested against the explicit N =2 result of
eqns (A.5) and (A.11); there, det(H) = H11 H22 −H12 H21 = AB −C 2.
Apart from the normalization constant, or partition function, of eqn (A.18), the other
quantity of interest is the covariance matrix σ2. Its ijth element is formally deﬁned by

σ2
ij =

(xi−xoi)(xj −xoj)

,
(A.20)

228
Gaussian integrals
where xoj =

xj

, and the expectation value of any function of the parameters f(x) is
given by the multiple integral

f(x)

= 1
Z

f(x) exp

−1
2 xTHx

dNx .
(A.21)
Since the maximum, or mean, of the multivariate Gaussian of eqn (A.12) is at the origin
(so that xoj =0 for all j), eqn (A.19) becomes

σ2
ij = 1
Z

xi xj exp

−1
2 xTHx

dNx .
(A.22)
By writing the exponent of the multivariate Gaussian in component form,
xTHx =
N

l=1
N

m=1
Hlm xl xm ,
we can see that the right-hand side of eqn (A.21) is related to the partial derivative of
the logarithm of the partition function
−2
∂
∂Hij
5
loge

Z
6
= 1
Z

xi xj exp

−1
2 xTHx

dNx .
Thus, in conjunction with eqns (A.18) and (A.21), we have

σ2
ij =
∂
∂Hij
5
loge

det(H)
6
.
(A.23)
This strange-looking quantity can be evaluated by remembering that the determinant of
a matrix is given by the scalar product of any row, or column, with its cofactors; this
means that
∂
∂Hij
5
det(H)
6
= hij ,
(A.24)
where hij is equal to (−1)j−i times the determinant of the (N −1)-squared matrix left
by striking out the ith row and jth column of H. Hence, eqn (A.22) becomes

σ2
ij =
hij
det(H) .
As we are dealing with a symmetric matrix, hij is also the ijth cofactor of the transpose
of H; therefore, we ﬁnally obtain the result
σ2 = adj(H)
det(H) = H−1,
(A.25)
where the adjoint of H in the numerator is a matrix consisting of the cofactors of HT.
This inverse relationship can again be easily checked for the N = 2 case of eqn (A.5),
by comparison with the results given in eqns (A.8) and (A.10).

B
Cox’s derivation of probability
Any general theory must apply to special cases. Following Cox (1946), we take this
mantra to heart and consider merely the simple, unambiguous, 8-state toy world of up
to three binary switches. We just want to be able to learn about it, and to distinguish
plausible settings from implausible. Remarkably, consideration of this tiny world suf-
ﬁces to deﬁne the rules of probability calculus. Any other calculus leads to contradiction
with how we wish to reason about it, and so no other calculus is acceptable to us. In his
bibliography of work in the ﬁeld, Jaynes (2003) remarks that in his view ‘this article
was the most important advance in the conceptual ... formulation of probability theory
since Laplace’. We concur, though we have re-ordered some of the material and mod-
ernized the approach. Cox wrote about general propositions. In modern idiom, we make
this more speciﬁc by considering the binary switches with which we nowadays encode
them.
Our belief about the state S of a system is always in a speciﬁc context X, and we
write π(S |X ) for it. Thus, in the 1-bit context I = {↓, ↑} of a single switch A, we
write our belief in A being ‘↑’ as
π(A|I) ,
where
(A|I) = {↑} | {↓, ↑} ,
and our belief in the converse ‘NOT A’ as
π(A|I) ,
where
(A|I) = {↓} | {↓, ↑} .
In the 2-bit context J = {↓↓, ↓↑, ↑↓, ↑↑} of two switches A, B, we have
(A|J ) = {↑↓, ↑↑} | {↓↓, ↓↑, ↑↓, ↑↑} ,
with belief π(A|J ) ,
for the ﬁrst bit A being ‘↑’, and similarly for the second bit B. There are also ‘A AND
B’ joint beliefs about two bits both being ‘↑’,
(AB |J ) = {↑↑} | {↓↓, ↓↑, ↑↓, ↑↑} ,
with belief π(AB |J ) ,
and other conditional assignments such as
(B |AJ ) = {↑↑} | {↑↓, ↑↑} ,
with belief π(B |AJ ) .
In the 3-bit context K = {↓↓↓, ↓↓↑, ↓↑↓, ↓↑↑, ↑↓↓, ↑↓↑, ↑↑↓, ↑↑↑} of A, B, C,
(A|K) = {↑↓↓, ↑↓↑, ↑↑↓, ↑↑↑} | {↓↓↓, ↓↓↑, ↓↑↓, ↓↑↑, ↑↓↓, ↑↓↑, ↑↑↓, ↑↑↑} ,
with belief π(A|K) ,
and similarly for other assignments.

230
Cox’s derivation of probability
We aim to develop a calculus for manipulating our beliefs about this system, and
start by asserting transitivity — if, in context K, we have more belief in A than B,
and more in B than C, then we assert that we have more belief in A than in C. To
do otherwise would lead us to argue in circles. A consequence is that we can map π
(whatever it was originally) onto real numbers, in which ‘more belief in’ is represented
by ‘>’. The transitivity assertion is
π(A|K) > π(B |K)
π(B |K) > π(C |K)
4
=⇒π(A|K) > π(C |K) .
So beliefs are real numbers— or at least they may as well be.
We now assert that knowing about A, and also about B given that knowledge, suf-
ﬁces to teach us about AB, all in the same overall context J. Some function F formal-
izes this inference:
(A|J ) = {↑↓, ↑↑} | {↓↓, ↓↑, ↑↓, ↑↑} ,
belief a = π(A|J ) ,
(B |AJ ) = {↑↑} | {↑↓, ↑↑} ,
belief b = π(B |AJ ) ,
(AB |J ) = {↑↑} | {↓↓, ↓↑, ↑↓, ↑↑} ,
belief π(AB |J ) = F(a, b) .
(B.1)
If F were independent of its second argument, while A was known to be ‘↑’, eqn (B.1)
would say π(AB |J)=F

π(certainty)

=constant. Arbitrary beliefs about B would
then all take the same value, which defeats our object. Likewise, if F were independent
of its ﬁrst argument, but B were known to be ‘↑’, all beliefs about A would take the
same value. Hence, for a usable calculus, we need F to depend on both its arguments.
The world of three bits allows sequential learning too. The three beliefs
(A|K) = {↑↓↓, ↑↓↑, ↑↑↓, ↑↑↑} | {↓↓↓, . . ., ↑↑↑} , belief x = π(A|K) ,
(B |AK) = {↑↑↓, ↑↑↑} | {↑↓↓, ↑↓↑, ↑↑↓, ↑↑↑} ,
belief y = π(B |AK) ,
(C |ABK) = {↑↑↑} | {↑↑↓, ↑↑↑} ,
belief z = π(C |ABK) ,
chain together to deﬁne our belief π(ABC |K ). In the chain, B could be combined
with A before linking with C, or with C before A. Hence
π(ABC |K) = F

F(x, y)
AB|K
,
z
C|ABK

= F

x
A|K
, F(y, z)
BC|AK

(B.2)
in which the arguments of the outer F’s are interpreted underneath. This is the ‘associa-
tivity equation’ and, following Lemma 1, it restricts F to be of the form
F(a, b) = w−1
w(a) + w(b)

where w is some invertable function of only one variable, instead of two. This is often
quoted multiplicatively as F(a, b) = w−1
w(a) × w(b)

, but here we delay the ﬁnal

Cox’s derivation of probability
231
exponentiation of w and let the arguments add instead of multiply. Remembering that π
was initially on an arbitrary scale, we can now upgrade to a less arbitrary scale of belief
φ(·) = w

π(·)

in which the sequential learning of eqn (B.1) proceeds by addition;
φ(AB |J ) = φ(A|J ) + φ(B |AJ ) .
(B.3)
If we wish, we can revert to any other π in which learning proceeds by the appropri-
ate modiﬁcation of addition, but the new scale φ is available as a common standard. Yet
there remains some arbitrariness, because φ could be rescaled by any constant factor
whilst still obeying eqn (B.3). To ﬁx the scale completely, we consider negation.
In the two-state context I = {↓, ↑} of a single bit, we assert that our belief about A
deﬁnes our belief about its converse A, formalized by some function f
φ(A|I) = f

φ(A|I)

.
Repeated negation is the identity, so
f

f(x)

= x .
(B.4)
Now consider the three-state context T = {↓↑, ↑↓, ↑↑} of two bits A and B in which
at least one is ‘↑’. With context T understood throughout,
φ(AB) = φ(A) + φ(B |A)
sequential learning
= φ(A) + f

φ(B |A)

deﬁnition of f
= φ(A) + f

φ(B, A) −φ(A)

sequential de-learning
= φ(A) + f

φ(B) −φ(A)

B = ↓state is unique in T
= φ(A) + f

f

φ(B)

−φ(A)

deﬁnition of f
= x + f

f(y) −x

name φ(A) = x, φ(B) = y .
Symmetry AB = BA then gives
x + f

f(y) −x

= y + f

f(x) −y

,
(B.5)
in which x and y are independent variables. Following Lemma 2, the functional eqns
(B.4) and (B.5) together require
f(ξ) = γ−1 log

1 −eγξ
,
where γ is a constant, and hence
exp

γ φ(A|I)

= 1 −exp

γ φ(A|I)

(B.6)
because ξ = φ(A|I) requires f(ξ) = φ(A|I). We now upgrade to a new, ﬁxed scale
by deﬁning prob(·) = exp

γ φ(·)

. Qualitatively, 0 ⩽prob(·) ⩽1 because neither
exponential in eqn (B.6) can be negative. Quantitatively, eqn (B.6) becomes

232
Cox’s derivation of probability
prob(A|I) + prob

A|I

= 1
(B.7)
which is the sum rule. Meanwhile, eqn (B.3) exponentiates to
prob(AB|J ) = prob(A|J ) × prob(B|AJ )
(B.8)
which is the product rule. We have derived the sum and product rules of probability
calculus, and there’s no scaling freedom left. With both rules obeyed, we are entitled to
call prob(S|X) the probability of state S in context X. This quantiﬁcation of belief is
what probability means.
Though the calculus is now ﬁxed, we are free to transform to other scales such as
percentages (100 × prob), odds prob/(1−prob), logarithms log(prob) or whatever
else might be convenient, provided that we transform the sum and product rules to
compensate. For example, percentages add to 100, not 1. There is, of course, no change
of content in any such reversible transformation.
For general inference, we use the simple switches A, B, C, . . . to encode arbitrary
propositions. Applying the product rule when we know B to be ‘↑’, and hence that
prob(A|J ) = prob(AB|J ) because ‘↑↓’ is excluded from our belief, shows that the
true statement (B |AJ ) has unit probability: prob(B|AJ)=1. In general context, the
unique true proposition thus has to be assigned prob(TRUE)=1. The negation of truth
being falsity, it follows from the sum rule that prob(FALSE)=0. Hence
prob(FALSE) = 0 ⩽prob(·) ⩽1 = prob(TRUE)
(B.9)
which identiﬁes the range of probability values.
The Cox derivation rests only upon elementary logic applied to very small worlds.
If there is a general theory of rational inference at all, it must apply in special cases,
so it can only be this probability calculus. Moreover, any deﬁned problem can be bro-
ken down into small steps. We have to use probability calculus in the small steps, and
this implies using it overall in the larger problem. This is the only globally-applicable
calculus we are ever going to have, so we should use it. And it does seem to be rather
successful.
B.1
Lemma 1: associativity equation
Our task is to solve eqn (B.2), namely
F

F(x, y), z

= F

x, F(y, z)

.
(B.10)
Here and hereafter we assume that our functions are differentiable.
The ﬁrst step is to ﬁnd a special relationship between the derivatives of F. Deﬁning
u = F(x, y)
and
v = F(y, z) ,
(B.11)
and substituting in eqn (B.10),
F(u, z) = F(x, v) .
(B.12)

Lemma 1: associativity equation
233
Taking the ratio ∂y ÷ ∂x of differentials of eqn (B.12), with sufﬁx r denoting derivative
with respect to the r’th argument,
F1(u, z) F2(x, y)
F1(u, z) F1(x, y) = F2(x, v) F1(y, z)
F1(x, v)
.
(B.13)
Such relationships are non-singular because F is required to depend on both its argu-
ments. Deﬁning
g(ξ, η) = log
F2(ξ, η)
 −log
F1(ξ, η)

(B.14)
and using it in eqn (B.13),
g(x, y) = g(x, v) + log
F1(y, z)
 .
(B.15)
Adding g(y, z), and using eqn (B.14),
g(x, y) + g(y, z) = g(x, v) + log
F2(y, z)
 .
(B.16)
Differentiating ∂z(B.15),
0 = g2(x, v) F2(y, z) + F12(y, z)
F1(y, z) .
(B.17)
Differentiating ∂y(B.16),
∂
∂y

g(x, y) + g(y, z)

= g2(x, v) F1(y, z) + F12(y, z)
F2(y, z) .
(B.18)
Eliminating F12 between eqns (B.17) and (B.18),
∂
∂y

g(x, y) + g(y, z)

= 0 .
(B.19)
The second step is to use this to express F in terms of functions of one variable only.
Differentiating ∂x(B.19),
∂2g(x, y)
∂x∂y
= 0 .
Solving in terms of arbitrary functions p and q of one variable,
g(x, y) = q(y) −p(x) .
(B.20)
Substituting eqn (B.20) in the exponential of eqn (B.14), using arbitrary functions P
and Q related to exponentials of p and q,
Q(y)
P(x) = ∂F(x, y)/∂y
∂F(x, y)/∂x .
(B.21)
Deﬁning new functions R and S related to P and Q, and re-expressing F as Φ,

234
Cox’s derivation of probability
dR(x) = P(x) dx ,
dS(y) = Q(y) dy ,
F(x, y) = Φ(R, S) .
(B.22)
Substituting from eqn (B.22) in eqn (B.21),
∂Φ(R, S)
∂R
= ∂Φ(R, S)
∂S
.
Solving in terms of (the inverse of) an arbitrary function W,
Φ(R, S) = W −1
R + S

.
(B.23)
In terms of x and y,
F(x, y) = W −1
R(x) + S(y)

.
(B.24)
The third step is to remove excess generality in F, which appears to depend on as
many as three functions of one variable. Substituting eqn (B.24) in eqn (B.12),
R(u) + S(z) = R(x) + S(v) .
(B.25)
Substituting eqn (B.24) in eqn (B.11),
W(u) = R(x) + S(y)
and
W(v) = R(y) + S(z) .
(B.26)
Eliminating R(x) and S(z) from eqns (B.25) and (B.26),
R(u) + W(v) −R(y) = W(u) −S(y) + S(v) .
(B.27)
Now u, y, v are independent variables derived from x, y, z, so the u- and v-dependences
in eqn (B.27) require
R(ξ) = W(ξ) + c
and
S(ξ) = W(ξ) + d ,
(B.28)
where c and d are constants. Substituting from eqn (B.28) in eqn (B.24),
W

F(x, y)

= W(x) + W(y) + c + d .
(B.29)
Deﬁning
w(ξ) = W(ξ) + c + d
to eliminate c + d, and substituting in eqn (B.29),
F(x, y) = w−1
w(x) + w(y)

.
(B.30)
Finally, eqn (B.30) satisﬁes eqn (B.10) without further restriction, so it is the general
solution.

Lemma 2: negation
235
B.2
Lemma 2: negation
We aim to solve for f obeying eqns (B.4) and (B.5), namely
f

f(x)

= x ,
(B.31)
x + f

f(y) −x

= y + f

f(x) −y

.
(B.32)
The ﬁrst step is to derive from eqn (B.32) a differential equation in only one variable.
Deﬁning
u = f(y) −x
and
v = f(x) −y ,
and substituting in eqn (B.32),
x + f(u) = y + f(v) .
(B.33)
Forming the ratio −∂2
xy ÷ (∂x × ∂y) of differentials of eqn (B.33) gives
f ′′(u)
f ′(u) [1 −f ′(u)] =
f ′′(v)
f ′(v) [1 −f ′(v)] = γ ,
(B.34)
because the separation of the u and v variables means that both expressions must equal
a constant γ.
The second step is to solve this for f, disallowing the singular possibilities f ′ = 0
and f ′ = 1 because in each case eqn (B.32) would force the supposedly independent
variables x and y to be equal.
f ′′(ξ)/f ′(ξ) = γ −γ f ′(ξ)
re-arrange
log|f ′(ξ)| = γ ξ −γ f(ξ) + a
integrate
f ′(ξ) = A exp

γ ξ −γ f(ξ)

exponentiate
eγf df = A eγξ dξ
separate
eγf = A eγξ + B
integrate
f(ξ) = γ−1 log

B + Aeγξ
re-arrange
(B.35)
The third step is to remove excess generality in the solution. Substituting for f from
eqn (B.35) in eqn (B.32),
γ−1 log

A2 eγy + AB + B eγx
= γ−1 log

A2 eγx + AB + B eγy
.
Since the x- and y-dependence both require B =A2, eqn (B.35) becomes
f(ξ) = γ−1 log

A2 + Aeγξ
.
(B.36)
This is the most general solution of eqn (B.32).

236
Cox’s derivation of probability
The fourth step is to remove the remaining spurious generality by using eqn (B.31).
Substituting eqn (B.36) in eqn (B.31),
γ−1 log

A2 + A3 + A2eγξ
= ξ .
(B.37)
Using the ξ-dependence in the exponential of eqn (B.37),
A2 + A3 = 0
and
A2 = 1 .
Hence A = −1 so that eqn (B.36) becomes
f(ξ) = γ−1 log

1 −eγξ
.
(B.38)
Finally, eqn (B.38) satisﬁes both eqns (B.31) and (B.32) without further restriction,
so it is their general solution.
QED

Bibliography
An extensive list of references can be found in several advanced textbooks, such as:
Bernardo, J.M. and Smith, A.F.M. (1994). Bayesian theory. John Wiley, New York.
Jaynes, E.T. (2003). Probability theory: the logic of science. Cambridge University
Press, Cambridge.
O’ Hagan, A. (1994). Kendall’s advanced theory of statistics, Vol. 2B: Bayesian infer-
ence. Edward Arnold, London.
Although still incomplete at his death, and edited to publication by G.L. Bretthorst,
we would strongly recommend Jaynes’ magniﬁcent treatise to any serious student of
the Bayesian approach; the tutorial paper by Loredo (1990) is also excellent. Two
recent textbooks of note are MacKay (2003) and Gregory (2005), which are written
from a scientiﬁc perspective similar to ours; by contrast, Bernardo and Smith (1994)
and O’ Hagan (1994), in common with many Bayesian texts, assume a more conven-
tional statistical background.
Other general sources of relevant literature include the proceedings of the annual
Maximum Entropy and Bayesian Methods workshops, published by Kluwer, and lat-
terly the American Institute of Physics, and the four-yearly Valencia International
Meeting on Bayesian Statistics, published by Oxford University Press. A list of the
references speciﬁcally mentioned in the present text is given below.
Abramowitz, M. and Stegun, I.A. (1965). Handbook of mathematical functions. Dover,
New York.
Acton, F.S. (1970). Numerical methods that work. Harper & Row, New York.
Aitken, M.J. (1998). An Introduction to optical dating. Clarendon Press, Oxford.
Allen, M.P. and Tildesley, D.J. (1987). Computer simulation of liquids. Oxford Uni-
versity Press, Oxford.
Bayes, T. (1763). An essay towards solving a problem in the doctrine of chances. Phil.
Trans. Roy. Soc., 53, 330–418.
Bernardo, J.M. (1979). Reference posterior distributions for Bayesian inference. J.
Roy. Stat. Soc. B, 41, 113–47.
Bernoulli, J. (1713). Ars conjectandi. Thurnisiorum, Basel.
Bontekoe, T.R. (1993). Pyramid images. In Maximum entropy and Bayesian methods
(ed. A. Mohammad–Djafari and G. Demoment). Kluwer, Dordrecht.
Box, G.E.P. and Tiao, G.C. (1968). A Bayesian approach to some outlier problems.
Biometrika, 55, 119–29.
Bretthorst, G.L. (1988). Bayesian spectrum analysis and parameter estimation. Lecture
Notes in Statistics, Vol. 48, Springer–Verlag, Berlin.

238
Bibliography
Bretthorst, G.L. (1990). Bayesian analysis II: model selection. J. Magn. Reson., 88,
552–70.
Bretthorst, G.L. (1993). On the difference in means. In Physics and probability (ed.
W.T. Grandy, Jr. and P.W. Milonni). Cambridge University Press, Cambridge.
Burg, J.P. (1967). Maximum entropy spectral analysis. In Proceedings of the 37th
Meeting of the Society of Exploration Geophysicists, Oklahoma City. Reprinted 1978,
in Modern spectral analysis (ed. D.G. Childers), IEEE press.
Bryan, R.K. (1990). Maximum entropy analysis of oversampled data problems. Eur.
Biophys. J., 18, 165–74.
Caves, C.M., Fuchs, C.A. and Schack, R. (2002). Quantum probabilities as Bayesian
probabilities. Phys. Rev A, 65, 22305–10.
Charter, M.K. (1990). Drug absorption in man, and its measurement by MaxEnt. In
Maximum entropy and Bayesian methods (ed. P.F. Foug`ere). Kluwer, Dordrecht.
Cox, R.T. (1946). Probability, frequency and reasonable expectation. Am. J. Phys., 14,
1–13.
David, W.I.F. (1990). Extending the power of powder diffraction for structure deter-
mination. Nature, 346, 731–4.
David, W.I.F. and Sivia, D.S. (2001). Background estimation using a robust Bayesian
analysis. J. Appl. Cryst., 34, 318–24.
de Moivre, A. (1733). Approximatio ad summam terminorium binomii (a + b)n in
seriem expansi. Reproduced in Archibald, A.C. (1926). Isis, 8, 671–83.
Fischer, R., Hanson, K.M., Dose, V. and von der Linden, W. (2000). Background esti-
mation in experimental spectra. Phys. Rev. E, 61, 1152–60.
Frieden, B.R. (1972). Restoring with maximum likelihood and maximum entropy. J.
Opt. Soc. Am., 62, 511–18.
Gill, P.E. , Murray, M. and Wright, H.W. (1981). Practical optimization. Academic
Press, London.
Gregory, P.C. (2005). Bayesian Logical Data Analysis for the Physical Sciences. Cam-
bridge University Press, Cambridge.
Gull, S.F. (1988). Bayesian inductive inference and maximum entropy. In Maximum
entropy and Bayesian methods in science and engineering, Vol. 1 (ed. G.J. Erickson
and C.R. Smith). Kluwer, Dordrecht.
Gull, S.F. (1989a), Developments in maximum entropy data analysis. In Maximum
entropy and Bayesian methods (ed. J. Skilling). Kluwer, Dordrecht.
Gull, S.F. (1989b). Bayesian data analysis: straight–line ﬁtting. In Maximum entropy
and Bayesian methods (ed. J. Skilling). Kluwer, Dordrecht.
Gull, S.F. and Daniell, G.J. (1978). Image reconstruction from incomplete and noisy
data. Nature, 272, 686–90.
Gull, S.F. and Skilling, J. (1984). Maximum entropy image reconstruction. IEE Proc.,
131F, 646–59.
Jaynes, E.T. (1957). Information theory and statistical mechanics. Phys. Rev., 106,
620–30; 108, 171–90.
Jaynes, E.T. (1963). Foundations of probability theory and statistical mechanics. In
Delaware seminar in foundations of physics (ed. M. Bunge). Springer–Verlag, Berlin.

Bibliography
239
Jaynes, E.T. (1978). Where do we stand on maximum entropy? In The maximum en-
tropy formalism (ed. R.D. Levine and M. Tribus). M.I.T. Press, Cambridge, MA.
Jaynes, E.T. (1983). Papers on probability statistics and statistical physics (ed. R.D.
Rosenkrantz). Reidel, Dordrecht.
Jaynes, E.T. (1986). Bayesian methods: an introductory tutorial. In Maximum entropy
and Bayesian methods in applied statistics (ed. J.H. Justice). Cambridge University
Press, Cambridge.
Jaynes, E.T. (1989). Clearing up mysteries — the original goal. In Maximum entropy
and Bayesian methods (ed. J. Skilling). Kluwer, Dordrecht.
Jeffreys, H. (1939). Theory of probability. Clarendon Press, Oxford.
Keynes, J.M. (1921). A treatise on probability. MacMillan, London.
Kirkpatrick, S., Gelatt, C.D. and Vecchi, M.P. (1983). Optimization by simulated an-
nealing. Science, 220, 671–80.
Knuth, K.H. (1999). A Bayesian approach to source separation. In Proceedings of the
ﬁrst international workshop on independent component analysis and signal separa-
tion: ICA’99 (ed. J.-F. Cardoso, C. Jutten and P. Loubaton), Aussios, France, Jan.
1999, pp. 283–8.
Knuth, K.H. (2005). Informed source separation: a Bayesian tutorial. In Proceedings of
the 13th European signal processing conference (EUSIPCO 2005) (ed. E. Kuruoglu),
Antalya, Turkey.
Laplace, P.S. de (1812). Th´eorie analytique des probabilit´es. Courcier Imprimeur,Paris.
Laplace, P.S. de (1814). Essai philosophique sur les probabilit´es. Courcier Imprimeur,
Paris.
Laue, E., Skilling, J. and Staunton, J. (1985). Maximum entropy reconstruction of
spectra containing antiphase peaks. J. Magn. Res., 63, 418–24.
Loredo, T.J. (1990). From Laplace to supernova 1987A: Bayesian inference in as-
trophysics. In Maximum entropy and Bayesian methods (ed. P.F. Foug`ere). Kluwer,
Dordrecht.
MacKay, D.J.C. (2003). Information Theory, Inference, and Learning Algorithms.
Cambridge University Press, Cambridge
Metropolis, N., Rosenbluth, A.W., Rosenbluth, M.N., Teller, A.H. and Teller, E. (1953).
Equation of state by fast computing machines. J. Chem. Phys., 21, 1087–92.
Michalewich, Z. (1992). Genetic algorithms + data structures = evolution programs.
Springer–Verlag, New York.
Nelder, J.A. and Mead, R. (1965). A simplex method for function minimization. Com-
put. J., 7, 308–13.
Newton, T.J. (1985). Blind deconvolution and related topics. Ph.D. Thesis, Cambridge
University.
Press, W.H., Flannery, B.P., Teukolsky, S.A. and Vetterling, W.T. (1986). Numerical
recipes: the art of scientiﬁc computing. Cambridge University Press, Cambridge.
Refson, K. (2000). MOLDY: a portable molecular dynamics simulation program for
serial and parallel computers. Comput. Phys. Comm., 126, 309–28.
Roberts, S. and Everson, R. (2001). Independent component analysis: principles and
practice. Cambridge University Press, Cambridge.

240
Bibliography
Shannon, C.E. (1948). A mathematical theory of communication. Bell Syst. Tech. J.,
27, 379–423 and 623–56.
Shore, J.E. and Johnson, R.W. (1980). Axiomatic derivation of the principle of maxi-
mum entropy and the principle of minimum cross-entropy. IEEE Trans., IT-26, 26–
37.
Sibisi, S. (1990). Quantiﬁed MaxEnt: an NMR application. In Maximum entropy and
Bayesian methods (ed. P.F. Foug`ere). Kluwer, Dordrecht.
Sibisi, S. (1996). Compound Poisson priors. In MaxEnt 96 (ed. M. Sears, V. Nedeljkovic,
N.E. Pendock, S. Sibisi).
Sivia, D.S. and Carlile, C.J. (1992). Molecular spectroscopy and Bayesian spectral
analysis — how many lines are there? J. Chem. Phys., 96, 170–8.
Sivia, D.S., Carlile, C.J., Howells, W.S. and K¨onig, S. (1992). Bayesian analysis of
quasielastic neutron scattering data. Physica B, 182, 341–8.
Sivia, D.S. and David, W.I.F. (1994). A Bayesian approach to extracting structure–
factor amplitudes from powder diffraction data. Acta Cryst. A, 50, 703–14.
Sivia, D.S. and Webster, J.R.P. (1998). The Bayesian approach to reﬂectivity data.
Physica B, 248, 327–37.
Skilling, J. (1988). The axioms of maximum entropy. In Maximum entropy and Bayesian
methods in science and engineering, Vol. 1 (ed. G.J. Erickson and C.R. Smith).
Kluwer, Dordrecht.
Skilling, J. (1989). Classic maximum entropy. In Maximum entropy and Bayesian
methods (ed. J. Skilling). Kluwer, Dordrecht.
Skilling, J. (1990). Quantiﬁed maximum entropy. In Maximum entropy and Bayesian
methods (ed. P.F. Foug`ere). Kluwer, Dordrecht.
Skilling, J. (1991). Fundamentals of MaxEnt in data analysis. In Maximum entropy in
action (ed. B. Buck and V.A. Macaulay). Clarendon Press, Oxford.
Skilling, J. (1998). Massive inference and maximum entropy. In Maximum entropy
and Bayesian methods (ed. R. Fischer, R. Preuss and U. von Toussaint). Kluwer,
Dordrecht.
Skilling, J. (2004). Nested sampling. In Maximum entropy and Bayesian methods in
science and engineering (ed. G. Erickson, J.T. Rychert, C.R. Smith). AIP Conf. Proc.,
735, 395–405.
Skilling, J. and Bryan, R.K. (1984). Maximum entropy image reconstruction: general
algorithm. Mon. Not. R. Astron. Soc., 211, 111–24.
Sonett, C.P. (1990). Atmospheric 14C variations: a Bayesian prospect. In Maximum
entropy and Bayesian methods (ed. P.F. Foug`ere). Kluwer, Dordrecht.
Stokes, S. (1999). Luminescence dating applications in geomorphological research.
Geomorphology, 29, 153–71.
Tikhonov, A.N. and Arsenin, V.Y. (1977). Solutions of ill–posed problems. Halsted
Press, New York.
Twain, M. (1924). Autobiography, Vol. 1, p. 246. Harper and Brothers, New York.
Yethiraj, M., Robinson, R.A., Sivia, D.S., Lynn, J.W. and Mook, H.A. (1991). A
neutron–scattering study of magnon energies and intensities in iron. Phys. Rev. B,
43, 2565–74.

Index
Acton, F.S. 56, 101–2
algorithms for optimization
brute force and ignorance 32, 56, 60–1, 90–1,
152–3
for free-from solutions 138–40
for hard problems 60–1
iterative linearization 58–60, 90–1, 139–40
for linear problems 57–8
practical points 32, 56, 59–60, 91
see also nested sampling
amplitude of signal with background 35–43,
151–6, 161–2
archaeology 94, 98
astronomy 7, 9–10, 35, 85, 121, 129, 136, 138,
148
auto-correlation function 158
average 28, 33-44, 53–4, 117–8, 120–1
weighted 29
see also expectation value and mean
background information 5, 15, 17, 19, 27, 36,
40, 61–2, 72–3, 78, 84, 90, 92, 124,
136, 140, 142–5, 150
background signal 35–43, 86, 88–9, 90, 100,
131, 146, 148, 152–6, 160–1
background removal 148, 173–4
basis functions 135, 140–1
Bayes, Rev. T. 3, 8, 107
Bayes’ theorem 6, 12, 14
derivation from product rule 6, 181
examples of use
coin ﬂipping 15, 19-20
experimental design 150, 162, 164
free-form solutions 137
Gaussian noise 27, 29, 52, 54, 174
hypothesis testing 84
lighthouse 31
model selection 79, 83, 86, 96
signal with background 37, 40
justiﬁcation for maximum likelihood and
least-squares 61–5, 165
warning against ‘boot-strapping’ 20
Bernardo, J.M. 125, 176, 237
Bernoulli, J. 8–9
law of large numbers 107, 111
principle of insufﬁcient reason 103–4, 108,
117
urn 104–7
best estimates 21–2,23, 28–9, 32, 34, 37, 44, 49,
53, 55, 57, 62–3, 66, 76, 138
limitations 25–6, 50–2
open endedness 25, 50–2
bimodal pdf 26, 33, 170–1
binary switches 204–8, 229-30
binary trees 210-1
bin-width and binning 39, 42–3, 163
binomial distribution 12, 15, 20, 23, 107–8,
117–8
derivation 104–8, 120
Gaussian approximation to 124
Poisson approximation to 121–2
properties 108
binomial expansion 106, 114
bit 163
blind deconvolution 148
blind source separation 148
Boolean logic 5
‘boot-strapping’ 20
Bretthorst, G.L. 94, 237
Burg entropy 112, 138
14C dating 94
calculus 7–8, 21, 44-5, 224
calibration experiments 40, 88, 161–2
Cauchy distribution 30–1, 34, 54, 100, 169, 172,
195
derivation from uniform pdf 69
causality versus logic 11
central limit theorem 26, 33–4
changing variables 30, 68–77
Cartesian and polar coordinates 70–1, 224
‘marginal’ transformations 71–3
‘one-to-one’ transformations
multivariate 69–70
univariate 68–9, 71, 76-7
see also propagation of errors
chaotic systems 10
Chebyshev polynomials 135
chi-squared (χ2) 62–4, 66, 86–91, 131–2,
137–9, 155–8, 162
distribution 54–5, 71
expectation value 71, 85, 139
signiﬁcance tests 84–5
Cholesky decomposition 57
classiﬁcation problem 94–8
coin example 3, 4, 14–20, 149–50

242
Index
bias-weighting and fairness 14
effect of different priors 17–9
number of ﬂips required 17, 23, 149–50
one-step or sequential analysis 19–20
summarizing inference 23–4
combinations 106, 113–4, 120
completing the square 73, 225
complex numbers 129, 139, 146, 158
computer code 56, 188-90, 192–5, 206–8, 211,
213–5, 219–23
conditional probabilities 5, 11, 124
logical versus causal connections 11
versus marginal probabilities 39–42, 50
conditioning information 5, 11, 15, 17–9
symbol (|) 5
see also background information and testable
information
conﬁdence interval 12, 14, 24–5, 51, 149
limitations 25–6
conjugate-gradient algorithm 59
continuum limit 7–8, 12–3, 14, 109, 116, 155,
158
contours 38–9, 56
conventional statistics 9–11, 84–5
convolution 72, 86, 133–4, 146–8, 151–2,
159–61
cook-book approach 3, 10, 77, 90
correlated noise 174–9
correlation 12, 35, 39, 47-50, 57, 64, 67, 72,
119, 132, 156, 157–9
counting exercises 105–6, 113–4
counting experiments 35–6, 121, 151
covariance 47-50, 57, 64, 67, 119, 132, 156,
157–9, 175, 227–8
Cox, R.T. 4–5, 11, 12, 20, 103, 229
assertions 4–5, 230–1
derivation of probability 229-36
cross-entropy 116
crystallography 35, 74–5, 85, 93–4, 118, 129,
136, 139
deconvolution 138, 146–8, 159–61
deductive logic 3–4, 8
default model 139
degree of belief 3–4, 9, 11, 14, 20
detailed balance 200, 209, 218
determinant, see matrices
de Moivre, A. 124
dice rolling 103, 110–1, 115
difference in means
evidence for 94–8
signiﬁcance of 97–8
Dirac δ-function 42, 72, 99, 100, 129–31, 135,
140, 152, 157, 158, 169, 217
drug-absorption studies 151
eigenvalue equation 46, 59, 132, 158
eigenfunctions 135, 158
eigenvalues 45–7, 58, 59, 91, 131–4, 135–6,
148, 157–60, 226–7
eigenvectors 45–6, 58, 59, 91, 131–4, 135–6,
148, 157–8, 226
entropy 110–6, 163, 186
error-bars 12, 14, 22, 23,28, 29, 46
dependence on number of data 23, 28, 53,
149–50
for free-form solutions 136, 158–9
limitations 24, 25, 55
marginal 46–8, 50, 54–5, 57, 67, 93, 99, 101,
132
relation to variance 47–8, 50
see also propagation of errors
error function 173, 225
evidence, probabilistic 6, 83, 86, 94, 96, 100,
125, 162, 164, 167, 181–4, 186,
numerical uncertainty 187–8, 215, 223
expectation values
continuous pdfs 25, 117–9, 228
discrete pdf 36, 107, 110, 117–21
multivariate pdfs 119, 228
see also mean
experimental design 12, 39, 40, 149–64
general issues 149–50, 161–4
information gain 163–4
resolution function example 151–61
for free-form solution 156–61
with an isolated sharp peak 152–6
exponential decays, sum of 101–2
exponential distribution 117–8, 120
factorials 88, 105–6
Stirling’s approximation 114, 115, 123
ﬁltering 147–8
ﬁnite-differences for gradients 60, 91
Fisher, R.A. 10
ﬂat prior, see uniform pdf
forensic deblurring 138
Fourier transform 101, 139, 143–4, 146–7,
158–60, 163, 224
free-form solutions
basic strategy 144–5
in experimental design 156–61
formulations of 130–1, 135, 140–2, 148
inference versus inversion 145–8
optimization algorithms for 139–40
priors for 131–2, 135, 136–7, 142–4
versus model selection 130, 144–5
versus parameter estimation 129–36, 44-5
frequency deﬁnition 9–11, 107
full-width-half-maximum (FWHM) 22, 30–1,
38–9, 47, 54, 91–2, 159–60
fuzzy pixels 140–2, 148
Gaussian approximation, see quadratic
approximation

Index
243
Gaussian distribution 12, 22, 46, 49–50, 62,
70–1, 73, 80,143
approximation to binomial pdf 124
approximations to Poisson pdf 64, 122–3,
153, 166–7
assumption for least-squares 62, 64–5, 128,
165
integration of 22, 71, 88, 95, 224–8
MaxEnt derivation 118–9
model of noise 26, 29, 52, 55, 86, 174, 187
versus Cauchy 30–1, 169, 172
genetic algorithms 61
goodness-of-ﬁt, see likelihood function
grad-grad (∇∇) matrix 49–50, 57–60, 63–4,66,
87–8, 89–91, 131–4, 135–6, 155-6,
157–8, 162, 167
beeﬁng-up of diagonal 59, 91
gradients
calculation of by ﬁnite-differences 60, 91
constraints for free-form solutions 141
optimization algorithms based on 57–60,
138–40
vector (∇) 50, 57–9, 63, 66, 136, 139
Gull, S.F. 29, 67, 78, 94, 111, 127, 135, 137–8
Herodotus 4
Hessian matrix, see grad-grad (∇∇) matrix
histograms 39, 43
history 8–11, 107
hypergeometric distribution 117
hypothesis 6, 14, 94, 96
hypothesis testing 84–5
image processing 11, 136, 138
see also free-form solutions
improper priors 124–5
independence 15, 27, 31, 36–7, 52, 62, 72–3, 95
formal statement of 20, 27, 62
inductive logic 3–4, 8
information theory 111, 163–4, 186–7
interpolation 141–2
invariance arguments 103–4, 108–10, 116, 124
inverse problems 148
inversion versus inference 145–8
Jacobian 69–71, 76, 227
Jaynes, E.T. 3, 10, 11, 13, 104, 110-11, 115, 116,
124, 229, 237
Jeffreys, H. 3, 78, 81, 85, 109
Jeffreys’ prior 109–10, 125, 167, 168
joint pdf, see marginalization
kangaroo argument 111–3
Keynes, J.M. 103
Kullback number 116
l1-norm and l2-norm 65, 119
Lagrange multipliers 110, 116–8, 120–1, 138,
175, 224
Laplace, P.S. de 1, 3, 8–11, 13, 102, 107, 124,
229
mass of Saturn 9–10, 14
Laplace transform 140
law of large numbers 107, 111
learning process 6
least-squares 61–7, 89, 101, 129, 135
extensions 165–80
background removal 173–4
constraints 166
correlated noise 174–79
for magnitude data 179–80
outliers 167–72
unknown noise 67, 166-7
ﬁtting of a straight line 65–7
implicit assumptions 61–5, 75, 119
l1-norm versus l2-norm 65, 119
likelihood function 62–3, 86, 119, 132, 137,
155
linear property 63–4, 166
MaxEnt derivation 117, 119
Lebesgue measure 115–6, 117–21, 137, 139
default model 139
light bulb manufacturers 97–8
lighthouse problem 29–33, 192–5, 198–9
likelihood function 6
average 83
best-ﬁt ratio 81–2
binomial 15, 107–8, 120
Cauchy 30–1, 34, 69, 172, 195
Gaussian 26, 29, 52, 62, 75, 80
least-squares 62–3, 86, 119, 132, 137, 155,
165
marginal 6, 83, 87, 99, 161–2, 167, 168–9
Poisson 36, 121–2, 151
robust analysis 165–80
role in experimental design 150–63
sorted 182–4, 186
uniform 30
versus posterior pdf 65, 75, 85
versus prior pdf 124–6
see also independence and maximum
likelihood estimate
linear problems 57–8, 63–4, 131–4, 145, 166
linearization 58–60, 64, 90–1
location parameter 108–9
logic
Boolean 5
deductive 3–4, 8
inductive 3–4, 8
versus causality 10–11
logical consistency 4–5, 11, 20, 103–4, 108–9,
111, 124
log-normal distribution 179–80
Lorentzian 30–1, 81, 89, 100-22, 195
luminescence dating 98–100

244
Index
marginalization 5–8, 12, 56, 72, 79, 87–9, 94,
96, 104–5, 161–2
derivation from sum and product rules 6–7
marginal distributions 39–42, 46, 52–5, 99-100
error-bars 46–8, 50, 53–5, 57, 66–7, 93,
101–2, 132
likelihood function 6, 83, 87, 99, 161–2, 167,
168–9
versus conditional pdf 40–2, 50
Markov process 177, 190–2
see also Monte Carlo
mass of Saturn 9–10, 14
matrices 224
beeﬁng-up diagonal elements 59, 91
covariance 48, 50, 51, 57, 64, 66–7, 132, 136,
156, 174–6, 227–8
determinant 47, 49, 57, 59, 69–70, 88, 90,
132, 162, 176, 227–8
eigenvalue equation 46, 59, 132, 147
identity 59, 91,
inverse 48, 50, 57–9, 136, 145, 175, 228
symmetric 45, 50, 57, 136, 226–8
see also grad-grad (∇∇)
matrix–vector notation 45–6, 49, 57–9, 87–8,
174,226–8
maximum entropy (MaxEnt) 12
derivation of some common pdfs 116–21,
175–6, 180
use in free-form solutions 136–44, 147–8,
159–61
Lebesgue measure 115–6, 117–21, 137, 139
principle 110–6, 124, 136–7
maximum likelihood estimate 61–2, 65, 131
best-ﬁt ratio 81–2
mean
best estimate for Gaussian noise 28, 29
difference between two data-sets
evidence for 94–8
signiﬁcance of 97–8
poor estimate for Cauchy data 33–4
representative estimate for asymmetric pdfs
25
silly estimate for multimodal pdfs 26
see also expectation value
median 180
Metropolis, N. 196, 199-200, 209
Millikan’s oil-drop experiment 14
model selection 6, 12, 78–102
classiﬁcation problem 94–8
comparison with parameter estimation 83–4,
89–90, 97–8, 125
elementary example 78–83
experimental design for 162
luminescence dating 98–100
number of exponential decays and
Lorentzians 100-2
number of peaks example 85–94
an algorithm 89–91
alternative lineshapes 82, 89
Ockham effect 81–2, 88, 92
range for uniform prior 79–80, 81–3, 87, 90,
94, 98, 99, 100
versus free-form solutions 129–30, 144-5
molecular spectroscopy 85, 93, 129–30
monkey model 111, 113–5, 136, 140, 142
Monte Carlo algorithms 56, 60–1, 87, 99–100,
101–2
Markov chain (MCMC) 190–2, 195, 200, 206
binary trees 210-1
sampling of posterior 195–200
see also nested sampling
multi-channel entropy 143–4, 148
multimodal pdfs 25–6, 33, 50–2, 56, 60–1, 87,
90, 139, 200-2
multinomial distribution 115
mutually exclusive and exhaustive 7
nested sampling 181-223
applications
lighthouse 192–5, 198–9
ON/OFF switching 212–6
pseudo-crystal 204–8
quantiﬁcation 216–23
basic idea 181–90
binary trees 210-1
copy operation 191, 202
detailed balance 200, 209, 218
Markov chain Monte Carlo (MCMC) 190–2,
195, 200, 206
Metropolis 196, 199-200, 209, 212
multimodal likelihoods 200–2
numerical uncertainty 187–8, 196–7, 215, 223
quantiﬁcation 209–23
random jitter 183, 206
sampling of posterior 195–200
shrinkage ratio 185–6, 187–8
simulated annealing 203–8
sorted likelihood 182–4, 186
nested sampling computer code
application procedures (apply.c)
lighthouse 192–5
ON/OFF switching 213–5
pseudo-crystal 206–8
quantiﬁcation 219–23
binary tree (tree.c) 211
main program 188-90
neutron scattering 35, 93, 100-1, 121, 129, 151
Newton–Raphson algorithm 58–60, 90–1
Neyman, J. 10
NMR spectroscopy 94, 143–4
noise in experimental data
Gaussian 26, 29, 62, 86
approximation to Poisson 64, 153, 166–7
correlated 174–9
different-sized error-bars 29
of unknown magnitude 52–5, 67, 166–7,
176

Index
245
Johnson 64
Poisson 36, 64, 91, 151, 166–7
non-parametric estimation 12, 129–48, 156–61
see also free-form solutions
normal distribution, see Gaussian distribution
normalization 7–8, 15, 27, 31, 32, 37, 50, 56, 80,
86–7, 106, 110, 116, 125, 136, 181,
225, 227
nuisance parameters 8, 35, 39–42, 46, 52, 161–2
objectivity 9, 11, 124, 126
Ockham’s Razor 81, 92
factor 81–2, 88
odds ratio 79, 81, 82, 84, 89, 96
optimization, see algorithms
orthodox statistics 9–11, 84–5, 166
orthogonality 136, 227
outliers 167–72
paradoxes 13
parameter estimation 6, 12, 14
bivariate 35–49, 52–5
multivariate 43–4, 49–52
univariate 14–34, 74–7, 176–7
versus free-form solutions 129–36, 144-5
versus model selection 83–4, 89–90, 97–8
see also best estimates and reliabilities
partition function 227
Pearson, K. 10
P -values 85
permutations 88, 105–6
phase problem 139
pixels 130–1, 137, 140–2
plausible reasoning 3–4, 9, 11, 103
point-spread function 146–7, 151–61
Poisson distribution 12, 36
Gaussian approximations to 64, 153, 122-3,
166
as limiting form of a binomial pdf 121–2
MaxEnt derivation 121
properties 36, 121, 123–4
polynomial 21, 78, 94, 135
positive and additive distributions 136, 140, 142
posterior probability 6, 15, 26–7, 29, 31–3, 34,
129, 181–4
asymmetric 24–5, 50–1, 56
bivariate 37–9, 44–9, 52–5, 65–7
cut-offs imposed by prior 28, 31, 37, 46, 49,
56, 75, 77, 80
dependence on experimental set-up 38–9, 40,
149–50
effect of prior 17–9, 52, 54, 165
evolution with increasing data 16–9, 23, 28,
32–3, 36, 92, 149–50
marginal 39–42, 46, 52–5, 87, 176
multimodal 25–6, 33, 50–2, 56, 60–1, 170
multivariate 43–4, 49–52, 130, 137
odds ratio 79, 81, 82, 84, 89, 96
practical points 21, 32, 44, 56
sampling 195–200
summarizing inference 20–6, 43–52, 195
univariate 14–34, 74–7, 176–7
versus likelihood 65, 75, 85
see also quadratic approximation
principal axes 45–6, 48, 131–4, 156, 157–9
principle of indifference 103–4, 108, 115, 117,
120
prior predictive 6, 83
prior probability 6, 77
effect on posterior 17–9, 52, 54, 165
ﬂat or uniform 15, 27, 31, 37, 52, 61–2, 65,
75, 79–80, 87, 94, 165, 176
for free-form solutions 130–2, 135, 136–7,
142–4
improper 124–5
Jeffreys’ 109–10, 124–5, 167, 168
versus likelihood 124, 125–6
probability
assignment of 11-2, 103–26
density function (pdf) 7–8, 14, 20–1
distribution function (pdf) 8
fundamental rules 5, 103, 229–36
corollaries 5–8
interpretations 8–11, 103, 229–32
mass 181–4
product rule 5, 6–7, 11, 72, 79, 87, 94, 97, 105,
161–2, 232
for independent quantities 27, 62, 96
propagation of errors 68–77
free-form solutions 136
ratios or quotients 72, 74
short-cut procedure 73–5
sums and differences 72–4
taking square-roots 74–7
propositions 4–5, 6–7, 14, 94
quadratic approximation 21–4, 28, 44-48, 53–5,
76–7, 143, 157, 195
evaluation of integrals 46, 48, 87–8, 95
generalization and summary of 49–50
quantum mechanics 10, 130
radioactive decay 98–9, 101–2
random
number 10, 32, 37, 190
phenomena 9, 10–11
variable 9–10
versus uncertainty 10-11
real numbers 4, 12, 77, 226, 230
regularization 138–40, 142–44
reliability analysis 20–6, 43–52
asymmetric posterior pdfs 24–5, 50–1, 56
conﬁdence intervals 24–5, 51
correlations and covariance 47–50, 57, 64,
66–7
dependence on number of data 23, 28, 33, 34

246
Index
difference between means 97–8
error-bars 22–6, 28–9, 46–8, 50–1, 53–5,
66-7, 93, 99, 101–2
free-form solutions 136, 144–5
multimodal posterior pdfs 25–6, 50–2, 170
see also best estimates, propagation of errors
and experimental design
residuals of ﬁt 62, 65, 71, 155, 165
resolution function 86, 91, 93, 100–1, 134,
146–8, 151–61
root-mean-square (rms) error 47
sampling rate of data 42–3, 158, 163
scale parameter 52, 109–10, 168, 179–80
sequential or one-step analysis 19–20
Shannon, C.E. 111, 163
Shannon–Jaynes entropy 116, 137
signiﬁcance tests 84–5
simplex search algorithm 60, 90
simulated annealing 60–1, 203–8
singular value decomposition 131–4, 135–6
Skilling, J. 111, 112, 137–9, 184, 188, 192, 205,
206, 211, 213, 219
smoothness 138, 140–2, 144, 148
spherical harmonics 135
standard deviation 47, 52, 94–8, 187, 196–7
state of knowledge 5, 11, 14–5, 19, 113, 136,
150
statistics 10
Stirling approximation 114, 115, 123
straight line, ﬁtting of 65–7
Student-t distribution 54, 67, 97
subjectivity 9, 11, 124, 125–6
sum rule 5, 7, 11, 232
Taylor series expansion 21, 135, 224
bivariate 44–5, 95
multivariate 49, 58, 87, 143
univariate 21, 122, 135
testable information 110, 113, 114, 116–21, 124
thermodynamics 60, 114–5, 203
time series 177-9
transformation groups, see invariance arguments
transitive ranking 4, 230
Twain, M. 3
uncertainty versus randomness 9–11
uniform pdf 12, 15, 27, 30, 31, 37, 52, 61–2, 65,
75, 79–80, 86, 87, 94, 103–4, 117,
130–1, 135, 165
variance 47, 107–8
evidence for signiﬁcant change 94–8
and the Gaussian distribution 95, 118–9
vector–matrix notation 49, 57–9, 87–8, 145, 174,
226–8
Venn diagram 11
wavelets 135
Wilson distributions 118
windowing 147–8
X-ray diffraction 35, 74–5, 85, 93–4, 136

