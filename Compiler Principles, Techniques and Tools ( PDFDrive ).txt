
Preface 
This bwk is a descendant of Prinrlpdes of Compiler Design by Alfred V, Aho 
and Jeffrey D. UNman. Like its ancestor, it is intended as a text for a first 
course in compiler design. The emphasis is on solving p b l c m s  universally 
cnwuntered in designing s language' translator, regardless of the source or tar- 
get machine. 
Although few p p l e  are likely to build or even maintain a compiler for a 
major programming language, the reader can profitably apply the ideas and 
techniques discussed in this book to general software design. Fwr example, 
the string matching techniques for building lexical analyzers have also been 
used in text editors, information retrieval systems, and pattern recognit ion 
programs. Curttext -free grammars and syn tax-d irected definitions have been 
u
d
 
to build many little languages such as the typesettin6 and figure drawing 
systems that prproduced this h k ,  The techniques of d
e
 optimization have 
been used in program verifitrs and in programs that prduce 'Structured" 
pdograms from unstructured ones. 
The m a p  topicn' in cornpib design are covered in depth. The first chapter 
intrduccs the basic structure of a compiler and is essential to the rest of the 
b Q k  
Chapter 2 presents a translator from infix to p t f i x  expressions, built using 
some of the basic techniques described in this book, Many of the remaining 
chapters amplify the material in Chapter 2. 
Chapter 3 covers lexical analysis, regular expressions, finitc-state machines, 
and scanner-generator tools. The maprial in this chapter i s  broadly applicabk 
to text-prcxx~ing* 
Chapter 4 cuvers the major parsing techniques in depth, ranging from tht 
recursiue&scent 
methods that are suitable for hand implementation to the 
mmputatianaly more intensive LR techniques that haw ken used in parser 
generators. 
Chapter 5 introduces the principal Meas in syntaxdirected translation. This 
chapter is used in the remainder of the h
k
 
for both specifying and implc- 
menting t rrrnslations. 
Chapter 6 presents the main ideas for pwforming static semantic checking, 
Type checking and unification are discuswd in detail, 

PREFACE 
Chapter 7 discusses storage organizations u d  to support the run-time 
environment of a program. 
Chapter 8 begins with a discussion of intermediate languages and then 
shows how common programming language constructs can be translated into 
intermediate d e .  
Chapter 9 covers target d e  
generation. Included are the basic "on-the- 
fly" d
e
 
generation mcthds, as well as optimal rnethds for generating d
t
 
for expressions, Peephole optimization and dt-generator generators arc also 
covered. 
Chapter 10 is a wmprehensivc treatment of d
t
 
optimization. Data-flow 
analysis methods are covered in detail, as well as the principal rnethds for 
global optirnhtiw. 
Chapter I 1  discusses some pragmatic issues that arise in implementing a 
compiler. Software engineering and teaing are particularly important in m- 
pller mnstxuctim. 
Chapter 12 presents case studies of wmpikrs that have been ms~nrctcd 
udng some of the techniques presented in this book. 
Appndix A dcscriks a simple language; a "subset" of Pascal, that can be 
used as the basis of an implementation project, 
The authors have taught both introductory and advanced courses, at the 
undergraduate and graduate levels, from the material in this b k  
at: AT&T 
&11 hbratories, Columbia, Princeton, and Stanford, 
An introductory mmpibr course might cover matmid from the following 
sections of this book: 
introduction 
lexical analysis 
symbl tables 
parsing 
syn tax4 ireded 
trawlat ion 
type checking 
run-time organization 
intermediate 
code generat ion 
d
e
 
generation 
d
e
 
optimization 
Chapter 1 and Sections 2.1-2.5 
2.6. 3.1-3.4 
2.7, 7-6 
2.4, 4.1-4,4 
Informmtbn needmi for a programming project like the one in Apptndix A is 
introduced in Chapter 2. 
A course stressing twls In compiler construction might include tbe dims- 
sion of lexical analyzer generators in Sections 3.5, of pmw generators in SIX- 
tions 4.8 and 4.9, of code-generator generators in Wim 9.12, and material 
on techniques for compiler constriction from Chapter I I. 
An advanced course might stress the algorithms used in lexica1 analyzer 
generators and parser gcneratms discussed in Chapters 3 and 4, the material 

PREFACE 
3 
on type equivalence, overloading, polymurphisrn, and unifica~ion In Chapter 
6 ,  the material on run-time storage organizalion in Chapter 7, the paitern- 
directed code generation methods discussed in Chapter 9, and material on 
code optimization from Chapter 10. 
Exercises 
As before: we rate exercises with stars. Exereism without stars test under- 
standing of definitions, singly starred exercises are intended for more 
advanced courses, and doubly starred exercises are fond for thought. 
Acknowledgments 
At various stages in the writing of this book, a number of people have given 
us invaluable comments on the manuscript. In this regard we owe a debt of 
gratitude to Bill Appelbe. Nelson Beebe, Jon Btntley, Lois Bngess, Rodney 
Farrow, Stu Feldman, Charles Fischer, Chris Fraser, Art Gittelman, Eric 
Grosse, Dave Hanson, Fritz Henglein, Robert Henry, Gerard Holzmann, 
Steve Johnson, Brian Kernighan, Ken Kubota, Daniel Lehmann, Dave Mac- 
Queen, Dtanne Maki, Alan Martin, Doug Mcllroy, Charles McLaughlin, John 
Mitchell, Elliott Organick, Roberr Paige, Phil Pfeiffer, Rob Pike, Kari-Jouko 
Riiiha, Dennis Rirchic. Srirarn Sankar, Paul Stwcker, Bjarne Strmlstrup, Tom 
Szyrnanskl. Kim Tracy. Peter Weinberger, Jennifer Widom. and Reinhard 
Wilhelra. 
This book was phototypeset by the authors using the cxcellenr software 
available on the UNlX system. The typesetting comnmd read 
picJk.s 
tbl 
eqn I t m f f  -ms 
p i c  is Brian Kernighan's language for typesetting figures; we owe Brian a 
special debt of gratirude for accommodating our special and extensive figure- 
drawing needs so cheerfully, tbl is Mike Lesk's language for laying out 
tables. eqn is Brian Kernighan a d  Lorinda Cherry's language for typesetting 
mathcrnatics. trofi is Joe Ossana's program for formarring text for a photo- 
typesetter, which in our case was a Mergenthakr Lino~ron 202M. The ms 
package of troff macros was written by Mike Lesk. in addition, we 
managed the lext using make due to Stu Feldman, Crass references wirhin 
the text.-were mainrained using awk crealed by A l  Aho, Brian Kernighan, and 
Peter Weinberger, and sed created bv Lee McMahon. 
The authors would par~icularly like to aekoowledp Patricia Solomon for 
heipin g prepare the manuscript for photocomposiiion. Her cheerfuhcss and 
expert typing were greatly appreciated. I. D. Ullrnan was supported by an 
Einstein Fellowship of the Israeli Academy of Arts and Sciences during part of 
the lime in which this book was written. Finally, the authors would like 
thank AT&T Bell Laboratories far ils suppurt during the preparation of the 
manuscript. 
A , V + A , . R . S . . J . D . U .  

Contents 
1.1 Compilers .............................................................. I 
1.2 Analysis of the source program ................................. 
4 
.......................................... 
1.3 The phasa of a compiler 
10 
1.4 Cousins of the compiler ............................................ 16 
1.5 The grouping of phases .............................,.....I......... 20 
....................................... 
1.6 Compiler-construction tools 
22 
Bibliographic noles .................................................. 
23 
Cbapkr 2 A Simple Ompass Cempiler 
1 
............................................................... 
2.1 Overview 
..................................................... 
2.2 Syntax definition 
......................................... 
2.3 Syntax-directed translation 
2.4 Parsing ................................................................ 
.............................. 
2.5 A translator for simple expressions 
2.6 Lexical analysis ....................................................... 
...................................... 
2.7 Incarprating a symbol table 
2.8 Abstract stack machines ............................ 
............... 
................................... 
2.9 Putting the techniques together 
................................. 
........................ 
Exercises 
... 
Bibliographic notes .................................................. 
Chapter 3 bid Analysis 
33 
3.1 The role of the bxical analyzer .................................. 
3.2 Input buffering ....................................................... 
3.3 Specification of tokens ......................... .. 
................. 
............................................... 
3.4 Recognition of tokens 
3.5 A language for specifying lexical analyzers .................... 
3. 6 Finite automata ................................. .. ................... 
3.7 From a regular expression to an NFA .......................... 
3.8 Design of a lexical analyzer generator .......................... 
................ 
3.9 Optimization of DFA-based pattern matchers 
Exercises ............................................................... 
.................................................. 
Bibliographic notes 

CONTENTS 
Chapter 4 Syntax A d y s b  
4.1 The role of the parser ............................................... 
4.2 Context-free grammars ............................................. 
4.3 Writing a grammar .................................................. 
4.4 Topdown parsing .................................................... 
................................. 
4.5 Bottom-up parsing 
..; ..-. .......... 
...................................... 
4.6 Operator-precedence parsing 
4.7 LR parsers ............................................................. 
....................................... 
4.8 Using ambiguous grammars 
................................................... 
4.9 Parser generators 
..........*.................... ............*.*........*.**** 
Exercises 
.. 
.................................................. 
Bibliographic notes 
Chapter 5 SyntsK-Dim Translation 
......................................... 
5.1 Synta~directed definitions ....................................... 
5.2 Construction of syntax trees 
............. 
5.3 Bottom-up evaluation of Sattributed definitions 
............................................. 
5.4 L-attributed definitions ............................................... 
5.5 Topdown translation 
.................. 
5.6 Bottom-up evaluation of inherited attributes 
5.7 Recursive evaluators ......................... ...... ................. 
..................... 
5.8 Space for attribute values at compile time 
................ 
5.9 Assigning spare at compiler-construction time 
......................... 
5 . LO Analysis of syntaxdirected definitions 
........*......................*.......**.........* 
.......... 
E ~ercises 
.'. 
.................................................. 
Bibliographic notes 
Chapter 6 Type khaklng 
.......................................................... 
6.1 Type systems 
.......................... 
6.2 Specification of a simple type checker 
.................................. 
6.3 Equivalence of type expressions 
..................................................... 
6.4 Type conversions 
6 3  Overloading of functions and operators ........................ 
.............................................. 
6.6 Polymorphic funclions 
6.7 An algorithm for unification ...................................... 
Exercises ............................................................... 
Bibliographic notes ................................................... 
7+1 Source language issues ...................................... 
-- ...... 
................................................. 
7.2 Storage organization 
.............................. 
7.3 Storage-allocation strategies 
... 
.... 
.......................................... 
7.4 Amss to nonlocal names 

CONTENTS 
3 
7.5 Parameter passing .................................................. 424 
7.6 Symbol tables ....................................................... 429 
........... 
7.7 Language facilities for dynamic storage allma tion 
440 
................... ... 
7. 8 Dynamic storage alkation techniques 
, 
442 
.................................. 
7.9 $orage allocation in Fortran 
..... 446 
............................................................... 
Exercises 
455 
................................................ 
Bibliographic notes 
461 
Chapter 8 Intermediate C& Generstba 
463 
8 . I Intcrmediatt languages ............................................. 
................... 
........**............**..,........ 
8.2 Declarations 
... 
............................................. 
8.3 Assignment slaternents ................... 
8.4 Boolean e~pressions 
.... 
...................... 
..........**................. .................*..... 
8.5 Case statements 
- 
.......................................................... 
8.6 Backpatching ....................................................... 
8.7 P r d u r e  calls 
Exercises ............................................................... 
............................... 
Bibliographic notes 
... 
............... 
........................ 
9.1 Issues in the design of a code generator 
.................................................. 
9.2 The target machine 
.................................... 
9.3 Run-time storage management 
..................................... 
9.4 Basic blocks and flow graphs 
9.5 Next-use information ................................................ 
........................................... 
9.6 A simple code generator 
............................... 
9. 7 Register allocation and assignment 
......................... 
9.8 The dag representation of basic blwks 
............................................... 
9.9 Peephole optimist ion 
...................... 
9.10 Generating code from dagg 
.. .............. 
.......... 
9.1 1 Dynamic programming code-generation algorithm 
......................................... 
9.12 Code-generator generators 
Exercises ............................................................... 
................................................. 
Bibliographic noles 
.......................... 
............................ 
10.1 Introduction 
I .. 
586 
10.2 The principal sources of optimization ........................... 592 
...................................... 
10.3 Optimization of basic blocks 
598 
10.4 Loops in flow graphs .................................... 
.- .......... 602 
....................... 
10.5 introduction to global data-flow analysis 
608 
10.6 l€erative mlutiosi of data-flow equations ....................... 624 
10.7 Cde-improving transformations ................................. 633 
................................................. 
10.8 Dealing with aliases 
648 

CONTENTS 
................. 
10.9 Data-flow analysis of structured flow graphs 
660 
.................................... 
10.10 Efficient data-flow algorithms 
671 
10.1 1 A tool for data-flow analysis ...................................... 680 
10.12 Estimation of typ 
...................................... 
+,.,. ....... 694 
10.13 Sy mblic debugging of optimized axle ......................... 703 
............................................................... 
Exercises 
711 
.................................................. 
Bibliographic notes 
718 
Chapter 11 Want to Write a Compiler? 
723 
................................................. 
11 . 1 Planning a compiler 
723 
11.2 Approaches to compiler development ........................... 725 
....................... 
I 1.3 The compilerdevelopment environment 
729 
1 L . 4 Testing and maintenance ........................................... 731 
........... 
12.1 BQN. a preproawr for typesetting mathematics 
733 
................................................ 
12.2 Compilers for Pascal 
734 
...................................................... 
12.3 The C compilers 
735 
........................... -........... 
12.4 The Fortran H compilers 
.... 
737 
12.5 The Bliss( l 1 compiler ............................................... 740 
12.6 Modula-2 optimizing compiler .................................... 
742 
........................................................... 
A.l Intrduction 
745 
A.2 A Pascalsubset .................................................... 
745 
.................................................... 
A.3 Program structure 
745 
A.4 Lexical conventions .................................................. 
743 
.............................. 
................... 
A . 5 Suggested exercises 
? 
749 
....................................... 
A.6 Evolution of the interpreter 
750 
................................... 
A.7 Extensions ......................... : 
751 

CHAPTER 
Introduction 
to Compiling 
The principles and techniques of compiler writing are so pervasive that the 
ideas found in this book will be used many times in the career of a cumputer 
scicn t is1 , Compiler writing spans programming languages, machine architec- 
ture, language theory, algorithms, and software engineering. Fortunately, a 
few basic mrnpikr-writ ing techniques can be used to construct translators for 
P wide variety of languages and machines. In this chapter, we intrduce the 
subject of cornpiiing by dewxibing the components of a compiler, the environ- 
ment in which compilers do their job, and some software tools that make it 
easier to build compilers. 
1.1 COMPILERS 
Simply stated, a mmpiltr is a program that reads a program written in oae 
language - the source Language - and translates it inm an equivalent prqgram 
in another language - the target language (see Fig. 1 .I). As an important part 
of this translation process, the compiler reports to its user the presence of 
errors in the murcc program. 
messages 
At first glance, the variety of mmpilers may appear overwhelming. There 
are thousands of source languages, ranging from traditional programming 
languages such as Fortran and Pascal to specialized languages (hat have arisen 
in vktually every area of computer application. Target languages are equally 
as varied; a target language may be another programming language, or the 
machine language of any computer between a microprocasor and a 

supercwmputcr , Compilers arc sometimes classified as ~ingle~pass, 
multi-pass, 
load-and-go, debugging, or optimizing, depending on how they have been con- 
structed or on what function they arc suppsed to pcrform. Uespitc this 
apparent complexity, the basic tasks that any compiler must perform arc 
essentially the same. By understanding thcse tasks, we can construct com- 
pilers h r  a wide variety of murcc languages and targct machines using the 
same basic techniques. 
Our knowlctlp about how to organim and write compilers has increased 
vastly sincc thc first compilers startcd to appcar in the carty 1950'~~ 
it is diffi- 
cult to give an exact date for the first compiler kcausc initially a great deal of 
experimentat ion and implementat ion was donc independently by several 
groups. Much of the early work on compiling deal1 with the translation of 
arithmetic formulas into machine cads. 
Throughout the lY501s, compilers were mnsidcred notoriously difficult pro- 
grams to write. The first Fortran ~Cimpller, for exampie, took f 8 staff-years 
to implement (Backus ct a[. 119571). We have since discovered systematic 
techniques for handling many of the imponant tasks that mcur during compi- 
lation. Good implementation languages, programming environments, and 
software twls have also been developed. With the% advances, a substantial 
compiler can be implemented even as a student projtxt in a onesemester 
wmpilar-design cuursc+ 
There are two puts to compilation: analysis and synthesis. The analysis part 
breaks up the source program into mnstitucnt pieces and creates an intermdi- 
ate representation of the sou'rce pmgram. Tbc synthesis part constructs the 
desired larget program from the intcrmcdiate representation. Of the I w e  
parts, synthesis requires the most specialized techniques, Wc shall msider 
analysis informally in Sxtion 1.2 and nuthe the way target cude is syn- 
thesized in a standard compiler in % d o n  1.3. 
During anaiysis, the operations implicd by thc source program are deter- 
mined and recorded in a hierarchical pltrlrcturc m l l d  a trcc. Oftcn, a special 
kind of tree called a syntax tree is used, in which cach nodc reprcscnts an 
operation and the children of a node represent the arguments of the operation. 
Fw example. a syntax tree for an assignment statemcnt is shown in Fig. 1.2. 
: 
e 
/ ' \  
gasit i o n  
+ 
/ '-. 
initial. 
+ 
/ \ 
rate 
60 
h. 
1.2, Syntax trtx lor @sit ion : 
+ initial + rate 60. 

EC. 1.1 
COMPILERS 3 
Many software tools that manipulate source programs first perform some 
kind of analysis. Some exampies of such tools include: 
Structure edit~m, A Structure editor takes as input a sequence of corn- 
mands to build a sour= program* The structure editor not ofil y performs 
the text-creation and mdification functions of an ordinary text editor, 
but it alw analyzes the program text, putting an appropriate hierarchical 
strudure on the source program. Thus, the structure editor can perform 
additional tasks that are useful in the preparation of programs. For 
example, it can check that the input is correctly formed, can supply kcy- 
words automatically (e-g.. when the user types while. the editor svpplics 
the mathing do and remidi the user tha# a conditional must come 
ktween them), and can jump from a begin or left parenthesis to its 
matching end or right parenihesis. Further, the output of such an editor 
i s  often similar to the output of the analysis phase of a compiler. 
Pretty printers. A pretty printer anaiyxs a program and prints it in wch 
a way that the structure of the program becomes clearly visible. For 
example, comments may appear in a spcial font, and statements may 
appear with an amount of indentation proportional to the depth of their 
nesting in the hierarchical organization of the stakments. 
Static checkers. A siatic checker reads a program, analyzes it, and 
attempts to dimver potential bugs without running the program, The 
analysis portion is often similar to that fmnd in optimizing compilers of 
the type discussed in Chapter 10. Fw example, a static checker may 
detect that parts of the source propam can never be errscutd, or that a 
certain variable might be used before bctg defined, In addition, it can 
catch Iogicai errors such as trying to use a real variable as a pintcr, 
employing the t ype-checking techniques discussed in Chapter 6. 
inrerpr~iers. Instead of producing a target program as a translation, an 
interpreter performs the operations implied by the murce program. For 
an assignment statement, for example, an interpreter might build a tree 
like Fig. 1.2, and then any out the operations at the nodes as it "walks" 
the tree. At the root it wwk! discover it bad an assignment to perform, 
so it would call a routine to evaluate the axprcssion on the right, and then 
store the resulting value in the Location asmiated with the identifiet 
position. At the right child of the rm, the routine would discover it 
had to compute the sum of two expressions. Ct would call itaclf recur- 
siwly to compute the value of the expression rate + 60. It would then 
add that value to the vaiue of the variable initial. 
Interpreters are hqueatly used to cxecute command languages, since 
each operator executed in a command language is usually an invmtim of 
a cornpk~ routine such as an editor or compiler. Similarly, some 'Wry 
high-level" Languages, like APL, are normally interpreted bause there 
are many things about the data, such as the site and shape of arrays, that 

4 
1NTRODUCTION TO COMPILING 
SEC. I .  
cannot be deduced at compile time. 
Traditionally, we think of a compiler as a program that translates a source 
language like Fortran into the assembly or machine ianguage of some com- 
puter. However, there are seemingly unrelated places where compiler technol- 
ogy is regularly used. The analysis portion in each of the following examples 
is similar to that of a conventional compiler. 
Text formrrers. 
A text farmatter takes input that is a stream uf sharac- 
ten, most of which is text to be typeset, but some of which includes com- 
mands to indicate paragraphs, figures. or mathematical structures like 
wbscripts and superscripts. We mention some of the analysis done by 
text formatters in the next section. 
Si1it-m ct~stylihrs. A silicon compiler has a source language that is similar 
or identical to a conventional programming language. However, the vari- 
ables of the language represent, not locations in memory, but, logical sig- 
nals (0 or 1) or groups of signals in a switching circuit. The output is a 
circuit design in an appropriate language. See Johnson 1 19831. Ullman 
1 19843, or Trickey 1 19BSJ for a discussion of silicon compilation. 
Qucry inrerpreters. A query interpreter translates a predicate containing 
relational and h l e a n  operators into commands to search s database for 
records satisfying [hat pmlicate. (See Ullman 119821 or Date 11986j+) 
The Context of a Compiler 
In addit ion to a compiler, several other programs may be required to create an 
executable target program, A source program may be divided into modules 
stored in separate files. The task of collecting the source program is some- 
times entrusted to a distinct program, called a preprocessor, The preprocessor 
may also expand shorthands, called macros, into source language staternenfs. 
Figure 1.3 shows a typical "compilation." 
The target program created by 
the compiler may require further processing before it can be run. The corn- 
piler in Fig, 1.3 creates assembly code that is translated by an assembler into 
machine code and then linked together with some library routines into thc 
code that actually runs on the machine, 
We shall consider the components of a compiler in the next two sccticsns; 
the remaining programs in Fig. 1.3 are discussed in Sec~ion 1.4. 
1,2 ANALYSIS OF THE SOURCE PROGRAM 
In this section, we introduce analysis and illustrate its use in some text- 
formatting languages, The subject is treated in more detail in Chapters 2-4 
and 6. In compiling, analysis consists of three phaxs: 
1. 
Lirtuar unu!ysh, in which the stream of characters making up the source 
program i s  read from left-to-right and grouped into wkms thar are 
sequences of characters having a collective meaning. 

ANALYSIS OF THE SOURCE PROGRAM 
5 
library. 
rclrmtabk objcct filcs 
absdutc machinc adc 
Fig. '1 -3. A language- praccsning systcm. 
2. 
Hi~rurc~htcu~ 
am/y,~i.s, 
in which characters or tokens are grouped hierarchi- 
cally into nested cdlcctiwnx with mlleclive meaning* 
3. 
Scmontic unuiysh, in which certain checks are performed to ensure that 
I he components of a program fit together meaningfully. 
In a compiler, linear analysis is called Irxicd anulysi,~ or srmnin#. For exam- 
ple, in lexical analysis the charaaers in the assignment statement 
'position := initial + rate * 6 0  
would be grouped into the fdlowmg tokens; 
1. 
The identifier go$ ition. 
2. 
The assignment symbol : =. 
3. 
Theidentifier i n i t i a l .  
4. 
The plus sim. 
5. The identifier rate. 
6. The multiplication sign. 
7. The number 6 0 ,  
The blanks separating the characters of these tokens would normally be elim- 
inated during lexical analysis. 

Syntax Analysis 
H ierarchical analysis is called pur.~ing or synm antiiyxix 
14 involves grouping 
the tokens of the source program into grammatical phrases that are used by 
the compiler to synthesize output. Usualty, the grammatical phrases of the 
source program are represented by a parse tree such as the one shown in Fig. 
1 -4. 
I '  
position 
Fig, 1.4. Pursc trcc for position : = initial + rate 60. 
In the expression i n i t i a l  + rate * 60, the phrase rate 60 is a hgi- 
cal unit bemuse the usual conventions of arithmetic expressions tell us that 
multiplicat ion 
is performed before addit ion. 
Because the 
expression 
5 n i t i a l  + rate is foilowed by a *. it is not grouped into a single phrase by 
itself in Fig. 1.4, 
The hierarchical structure of a program is usually expressed by recursive 
rules. For example, we might have the idlowing rules as part sf the defini- 
tion of expressions: 
I .  Any idmtijeris an expression. 
2, 
Any m m h r  is an expression. 
3. 
If txprc.rsioiz 1 and ~xpr~'ssiun are expressions, then so are 
Rules (I) and (2) are (noorecursive) basis rules, while (3) defines expressions 
in terms of operators applied to other expressions. Thus, by rule I I). i n i -  
t i a l  and rate are expressions. By rule (21, 60 is an expression, while by 
rule (31, we can first infer that rate * 60 is an expresxion and finally that 
initial + rate 60 is an expression. 
Similarly, many Ianguagei; define statements recursively by rules such as: 

SEC+ 1.2 
ANALYSIS OF THE SOURCE PROGRAM 7 
1. 
If identrfrer is an identifier, and c'xprc+s.~ion~ 
is an exyrcshn, then 
is a statement. 
2. 
If expremion I is an expression and siumncnr 2 is a statemen I, then 
are statements. 
The division between lexical and syntactic analysis is somewhat arbitrary. 
We usually choose a division that simplifies the overall task of analysis. One 
factor in determining the division is whether a source !anguage construct is 
inherently recursive or not. Lexical constructs do not require recursion, while 
syntactic conslructs often do. Context-free grammars are a formalization of 
recursive rules that can be used to guide syntactic analysis. They are intro- 
duced in Chapter 2 and studied extensivdy in Chapter 4, 
For example, recursion is not required to recognize identifiers, which are 
typically strings of letters and digits beginning with a letter. We would nor- 
mally recognize identifiers by a simple scan of the input stream. waiting unlil 
a character that was neither a letter nor a digit was found, and then grouping 
all the letters and digits found up to that point into an ideatifier token. The 
characters so grouped are recorded in a table, called a symbol table. and 
removed from the input so that processing of the next token can begin. 
On the other hand, this kind of linear scan is no1 powerful enough to 
analyze expressions or statements. For example, we cannot properly match 
parentheses in expressions, or begin and end in statements, without putting 
some kind of hierarchical or nesting structu~e on the input. 
. - 
/ -- \ 
position 
+ 
I \ 
i n i t i a l  
+ 
: = 
/ \  
vos i t ion 
+ 
/ \  
initial 
* 
/ ' \  
rate 
h#~resl 
I 
Fig. 1.5. Scmantic analysis inscrt s a conversion frnm intcgcr to real. 
The parse tree in Fig. 1.4 describes the syntactic siructure of the input. A 
more common internal representation of this syntactic structure is given by the 
syntax tree in Fig. L.5(a). A syntax tree is a compressed representation of the 
parse tree in which the operators appear as the interior nodes, a.nd the 
operands of an operator are the children of the node for that operator. The 
construction of trecs such as the one In Fig. 1 S(a) is discussed in Section 5.2. 

8 
INTRODUCTION TO COMPILING 
SEC. 1.2 
We shall take up in Chapter 2, and in more detail in Chapter 5 ,  the subject of 
~yntax-bireced trwtshriun, In which the compiler uses the hierarchical struc- 
ture on the input to help generate the output. 
Semantic Analysis 
The semantic analysis phase checks the source program for semantic errors 
and gathers type information for the subsequent de-generation phase. It 
uses the hierarchical structure determined by the syntax-analysis phase to 
identify the operators and operands of expressions and statements. 
An important compnent of semantic analysis i s  type checking. Here the 
compiler checks that each operator has operands that are permitted by the 
source language specificat ion. For example, many programming language 
definitions require a compiler to report an error every time a real number is 
used to index an array. However, the language specification may permit some 
operand coercions, for example, when a binary arithmetic operator is applied 
to an integer and real, [n this case, the compiler may need to convert the 
integer to a real. Type checking and semantic analysis are discused in 
Chapter 6. 
Example 1.1, Inside a machine, the bit pattern representing an integer is gen- 
erally different from the bit pattern for a real, even if the integer and the real 
number happen to have the same value, Suppse, for example, that all iden- 
tifiers in Fig. 1 3  have been declared to be reals and that 60 by itself is 
assumed to be an integer. Type checking of Fig. 1.5{a) reveals that + is 
applied to a real, rats, and an integer, 60. The general approach is to con- 
vert the integer into a real. This has been achieved in Fig. 1.5(b) by creating 
an extra node for the operator irltod that explicitly converts an integer into 
a real. Alternatively, since the operand of inttawd is a constant, the corn- 
piler may instead repla- the integer constant by an equivalent real constant. 
Analysis in Text Formatters 
It is useful to regard the input to a text formatter as specifying a hterarchy of 
hxcs that are rtaangular regions to be filled by some bit pattern, represent- 
ing light and dark pixels to be printed by the output device. 
For example, the 
system (Knuth [1984aj) views its input this way. 
Each character that is not part of a command represents a box containing the 
bit pattern for that character in the appropriate font and size. Consecutive 
characters not separated by "white space" (blanks or newline characters) are 
grouped into words, consisring of a sequence of horizontally arranged boxes, 
shown schematically in Fig, 1.6. The grouping of characters into words (or 
commands) is the linear or lexical aspect of analysis in a k x t  formatter. 
Boxes in 
may tx built from smaller boxes by arbitrary horizontal and 
vertical combinations. For example, 

ANALY SlS OF THE SDURCE PROGRAM 
9 
Fig. t .6. Grouping of characters and words into 
groups the list of boxes by juxtaposing them horizontally, while the \vbox 
operator similarly groups a list of bxes by vertical juxtaposition. Thus, if we 
say in 
we get the arrangement of boxes shown in Fig. 1.7. 
Determining the 
hierarchical arrangement of boxes implied by the input is part of syntax 
analysis in w. 
Fig. 1.7. Hierarchy of h x c s  in w. 
As another example, the preprocessor EQN for mathematics (Kernighan 
and Cherry 1 l975]), or the mathematical processor in m, builds mathemati- 
cal expsiofis from operators like sub and sup for subscripts and super- 
scripts. If EQN encounters an input text of the form 
BOX sub box 
it shrinks the size of h x  and attaches it to BOX near the lower right corner, 
as illustrated in Fig. 1.8. The sup uperator similarly attaches box at the 
upper right. 
Fig. 1.8. Building the subiscript structure in mathematical Icxt. 
These operators can be applied recursively, so, for example. the EQN text 

10 
INTRODUCTION TO COMPlLlNG 
a sub {i sup 2 )  
results in d , : .  Grouping the operators sub and sup into tokens is part of the 
lexical amalysts of EQN text, However, the syfitactic structure of the text is 
needed to determine the size and placement of a box. 
1,3 THE PHASES OF A COMPILER 
Conceptually, a compiler operates in p h s e s ,  each of which transforms the 
source program from one representation to another. A typical decompmition 
of a compiler is shown in Fig, 1.9, In practice, some of the phases may be 
grouped together, as mentioned in Sxtion 1.5, and the intermediate represen- 
tations between the grouped phases need not be explicitly constructed. 
wurcc program 
lcxical 
analyzcr . 
4 
syntax 
analyzer 
J. 
 erna antic 
analyzer 
symbol-tublc 
G 
error 
managcr 
handlcr 
intcrrncdiatc code 
gcncrator 
C-, 
c d c  
optimizer 
1 
codc 
gcncrat or 
4 
targct program 
Fig. 1.9. 
P h a m  d a mrnpilcr + 
The first three phases, forming the bulk of the analysis portion of a com- 
piler, were introduced in the last section. Two other activities, symbl-table 
management and error handling, are shown interacting with the six phases of 
lexical analysis, syntax analysis, semantic analysis, intermediate code genera- 
tion, code optimization, and code generation. Informally, we shall also call 
the symbol-table manager and the error handler "phases." 

THE PHASES OF A COMflLER 
I 
Sy mhl-Table Management 
An essential function of a compiler is to record the identifiers used in the 
source program and collect information about various attributes of each idcn- 
tifier. These attributes may provide information about the storage allocated 
for an identifier, its type, its scope (where in the program it is valid). and, in 
the case of procedure names, such things as the number and types of its argu- 
ments, the method of passing each argument (e.g+, by reference), and the type 
returned, if any. 
A ,~ymhl table is a data structure containing a record €or each identifier, 
with fields for the attributes uf the identifier. The data structure allows us 10 
find the record for each idenfifier quickly and to store or retrieve data from 
ihat record quickly. Symbol tables are discussed in Chapters 2 and 7. 
When an identifier in the source program is detected by the lexical 
analyzer, the identifier is entered into the symbol table. However, the attri- 
butes of an identifier cannot normally k determined during lexical analysis. 
For example, in n Pascal declaration like 
var position, i n i t i a l ,  rate : real ; 
the type real is not known when position, i n i t i a l ,  and rate are seen by 
the lexical analyzer + 
The remaining phases enter information abut identifiers into the symbol 
table and then use this information in various ways. 
For example, when 
doing semantic analysis and intermediate code generation, we need to know 
what the types of identifiers are, so we can check that thc source program 
uses them in valid ways, and so that we can generate the proper operations on 
them, The code generator typically enters and uses detailed information about 
the storage assigned to identifiers. 
Each phase can encounter errors. However, after detecting an error, a phase 
must mmchow deal with that error, so that compilation can proceed, allowing 
further errors in the source program to be detected. A compiler that stops 
when it finds the first error is not as helpful as it could be. 
The syntax and semantic analysis phases usually handle a large fraction of 
the errors detectable by the compiler The lexical phase ern detect errors 
where the characters remaining in the input do not form any token of the 
language. Errors where the token stream violates the structure rules Is)wW 
of the language are determined by the synlax analysis phase. During semantic 
analysis the compiler tries to detect constructs that have the right syntactic 
structure but no meaning to the operatibn involved, e . g ,  if we try to add two 
identifiers, me of which is the name of an array, and the other the name of a 
procedure, We discuss the handling of errors by each phase in the part of the 
book devoted to ihat phase. 

The Analysis Phases 
As translation progresses, the compiler's internal represintation of the source 
program changes. 
We ilh strate these representations by considering the 
translation of the statement 
position ;= initial + rate * &I 
( 1 . 1 )  
Figure 1.10 shows the rcprescntarion of this statement after each phase. 
The lexical analysis phase rcads the characters in the source program and 
groups them into a stream of tokens in which each token repre,sents a logically 
cohesive sequence of characters, such as an identifier, a keyword (if, while, 
etc,), a punctuation character, or a multi-character operator like :=. The 
character sequence forming a token is called the !mmr for the token, 
Certain tokens will lx augmented by a "lexical value." For example, when 
an identifier like rate is found, the lexical analyzer not only generates a 
token, say id, but also enters the lexemr rate into the symbol table, if it is 
not already there. The lexical value ass~iated with this occurrence of id 
points to rhe symbol-table entry for r a t e +  
In this sedion, we shall u.se id,, id,, and id:, for position, initial, and 
rate, respectively, to emphasize that the internal representation of an identif- 
ier is different from the character sequence forming the identifier. 
The 
representation of ( I. 1 )  after lexical analysis is therefore suggested by: 
We should also make up tokens for the multi-character operator : = and the 
number 60 to reflect their internal .representation, but we defer that until 
Chapter 2, Lexical analysis is covered in detail in Chapter 3. 
The second and third phases, syntax and semantic analysis, have also k e n  
inlroduced in Section 1.2. Syntax analysis imposes a hierarchical structure on 
the token stream, which we shall portray by syntax trees as in Fig. 1. I I (a). 
A 
typical data structure for thc tree is shown in Fig. 1.1 1(b) in which an interior 
node is a record with a field for the operator and two fields containing 
pointers to the records for the left and right children. A leaf is a record with 
two or more fields, one to identify the token at the leaf, and the others to 
record information a b u t  the token. Additional ihformarion about language 
constructs can be kepr by adding more' fields to thet records for nodes. We 
discuss syntax and semantic analysis in Chapters 4 and 6, respectively. 
Intermediate Cde Generation 
After syntax and semantic analysis, some compilers generate an explicit inter- 
mediate representation of the source program. We can think of this inter- 
mediate representation as a program for an abstract machine. This intermedi- 
ate representation should have two important properties; ir should be easy to 
produce, and easy to translate into the target program, 
The intermediate represenlation can have a variety d forms. In Chapter 8, 

THE PHASES OF A COMPILER 
13 
SYMBOL TABLE 
3 rate 
4 
position := i n i t i a l  + rate * 60 
id, : = idl + id] 
60 
* 
Q 
I 
syntax nnalyzcr 
templ := inttoreal(60) 
tenpl := id3 r 60.0 
id1 ;= id2 + templ 
WVF id3, R2 
HWLF X60.0, R2 
MOVF id2, R1 
ADDF R2, R1 
MOVF R1, i d l  
Fig. 1.10. Translation of u statcmcnt . 

14 
INTRODUCTION TO COMPILING 
SEC. 1.3 
Fig. 1.11. The data struclurc in (b) is for thc tree in (a). 
we consider an intermediate form catkd "three-address code," which is like 
the assembly language for a machine in &ich every manory location can aft 
like a registel.. Three-address code consists of a sequence of instructions, each 
of which has at most three operands. The source program in (1.1) might 
appear in three-address code as 
This inter mediate form has several properties. Fi tst , c a d  t hree-address 
instruction has at most one operator in addition to the assignment. Thus, 
when generating these iinstrunions, the compiler has to decide rm the order in 
which operations are to be done; the multiplication precedes the addition in 
the source program of (1.1). 
Second, the compiler must generate a temporary 
name to hold the value computed by each instruction* Third, some "three- 
address" instructions have fewer than three wrands, e.g., the first and last 
instructions in ( 1.3). 
In Chapter 8, we cover the principal intermediate representations used in 
compilers. in general, these representations must do more than compute 
expressions; they must also handle flow-of-control constructs and procedure 
calls. Chapters 5 and 8 present algorithms for generating intermediate wde 
for typical programming language constructs. 
The code optimization phase attempts to improve the intermediate code, so 
that faster-running machine code will result. h
e
 
optimizations are trivial. 
For example, a natural algorithm generates the intermediate d
e
 
(1.31, using 
an instruction for each oprator in the tree representation after semantic 
analysis, even though there is a better way to perform the same calculation, 
using 1he two, instructions 

There is nothing wrong with this simple algorithm, since the problem can be 
fixed during  he mdespti'mizatiua phase. That is, the compiler can deduce 
that the conversion of 60 from integer to real representation can be done once 
and for all at compik time, so the inttoreal operation can be eliminated. 
Besides, temp3 is used only once', to transmit its value to i d l .  It then 
becomes safe to substitute id1 for temp3, w~creupon the last statement of 
(1.3) is not needed and the code of (1.4) results. 
There is great variation in the amount of wde optimization different corn- 
pilers perform. In lhose that do the most. called "bptimizing cornpiters," a 
significant fraction of the time of the compiler is spent on this phase, How- 
ever, there are simple optimizations that sjgnificantly improve the running 
time of the target program without slowing down compilation too much. 
Many of these are discussed in Chapter 9, while Chapter 10 gives the technol- 
ogy used by the most powerful optimizing compilers. 
The final phase of the compiler is the generation of target code, consisting 
normally of relocatable machine code or assembly c d c ,  Memory locations 
are selected for each of the variables used by the program. Then, intermedi- 
ate inslructions are each translared into a sequence of machine instructions 
that perform the same task. A crucial aspect is the assignment of variables to 
registers. 
For example, using registers I and 2, the translation of the cude of ( 1.4) 
might become 
HOVF i d 3 ,  R2 
MULF #6O. 0, R2 
MOVF i d 2 ,  R1 
ADDF R2, R t  
HOVE' R l ,  id1 
The first and second operands of each ifistruaion specify a source and desttna- 
tion, respectively. The F in each insiruction tells us that instructions deal with 
floating-point numbers. This code moves the contents of the address' id3 
into register 2, then multiplies it with the realanstant 60.0. The # signifies 
that 6 0 . 0  is to be treated as a constant. The third instruction moves id2 into 
register I and adds to it the value previously computed in register 2. Finally, 
the value in register I is moved into the address of idl. rn the code imple- 
ments the assignment in Fig. 1.10. Chaptm 9 covers code generation. 

16 
INTRODUCTION TO COMPILING 
1.4 COUSlNS OF THE COMPILER 
As we saw in Fig. 1.3, the input to a compiler may be produced by one or 
more preprocessors, and further processing of h e  compiler's output may be 
needed before running machine code is obtained. In this section, we discuss 
the context in which a compiler typically operates. 
Preprocessors produce input to compikrs. They may perform the following 
functions: 
Aiurro processing. A preprocessor may allow a user to define macros that 
are shorthands for longer wnstrlrcts. 
File inclusion. A preprocessor may include header files into the program 
text. For example, the C preprocessor causes the contenls of the file 
<global. h> to replace the statement #include sglobal . h> when it 
processes a file containing this statement. 
"Rarionai" preprocew.ws. These processors augment older languages 
with more modern flow-of-contrd and data-structuring facilities. 
For 
example, such a preprocessor might provide the user with built-in macros 
for constructs like while-statements or if-statements, where none exist in 
the programming language itself. 
Lcmguage ext~nsiuns, These processors attempt to add capabilities to the 
language by what amounts to buih-in macros, For example. the language 
Equel (Stonebraker et a\. [19761) is a database query language embedded 
in C. Statements beginning with ## arc taken by the preprocessor to be 
databage-access statements, unrelated to C, and are translated into pro- 
cedure calls on routines that perform the database access. 
Macro processors deal with two kinds of statement: macro definition and 
macro use. Definitions are normally indicated by some unique character or 
keyword, like d ~ f  
ine or macro. They consist of a name for the macro 
being defined and a body, forming its definition. Often, macro processors 
permit form1 poramercrs in their definition, char is, symbols ro be replaced by 
values (a "value" is a string of characters, in this conlext). The use of a 
macro consists of naming the macro and supplying actual paramefers, that is* 
values for its formal parameters. The macro processor substitutes the actual 
parameters for the formal parameters in the body of the macro; the 
transformed body then replaces the macro use itself. 
Ikample 1.2. The 
typesetting system mentioned in Section 1 -2 contains a 
general macro facility, Macro definitions take the form 
\Bef inc <macro name> <template> 
{<body>] 
A mcrv name i s  any string sf letters preceded by a backslash. The template 

S C .  1.4 
COUSINS OF THE COMPILER 
17 
i s  any string of characters, with strings of the form 
#7, #2, . . . , #9 
regarded as formal parameters. These symbols may also appear in the body, 
any number of times. For example, Ihe following macro defines a citation for 
the Juurnd of the ACM. 
The macro name is \JACM, and the template i s  "#7 ;#2;#3."; sernicolms 
separate the parameters and the Iast parameter is followed by a period, A use 
of this macro must take the form of the template, except that arbitrary strings 
may be substituted for the formal pararncter~.~ 
Thus. we may write 
and expect to see 
J .  ACM 17:4, pp. 715-728. 
The portion of the body I \sl J. ACM) calls for an italicized ("slanted") "J, 
ACM". 
Expression {\bf X I  ) says that the first actual parameter is to be 
made boldface; this parameter is intended to be the volume n u m k r .  
TEX allows any punctuarion or string of texi to separate the volume, issue, 
and page numbers in the definition of the UACM macro. We could even have 
used no punctuation at all. in which case 'TEX would take each actual parame- 
ter to be a single character or a string surrounded by ( } 
o 
Assemblers 
Some compilers produce assembly d t ,  as in (1.5). that is passed to an 
assembler for further prassing, Other compilers perform the job of the 
assembler, producing relocatable machine code that can be passed directly to 
the loaderllink-editor. We assume the reader has same Familiarity with what 
an assembly language looks like and what an assembler does; here we shall 
review the relationship between assembly and machine code. 
Ass~mbly rude is a rnnernoaic venim of machine code, in which names are 
used instead of binary codes for operations, and names are also given to 
memory addresses. A typical sequence of assembly instrucrion~ might k 
MOV a, R1 
ADD #2, R1 
MOV Rl, b 
This code moves the contents of the address a into register I, then adds the 
constant 2 to it, treating the contents of register 1 as a fixed-point numkr, 
2 Well. almost arbilrary string*, sincc a simple kft-to-righl scan t$ thc macro usr: is mde. and as 
MW as a symbol matching ~ h c  
text fcNrrwinp a #i symbnl in thc lcrnplatc is fibund. thc prcccdinp 
string is docmed to march #i. Thus. if wc tried 10 hubsfilutc ab;cd for 41, wc would find thar 
only ab rnutchcd #I 
and cd was matchcd to #2. 

18 
INTRODUCTlON TO COMPILING 
SEC. 1.4 
and finally stores the result in the location named by b. Thus, it computes 
b : = a + 2 .  
It is customary for assembly languages to have macro facilities that are sirni- 
lar to those in the macro preprocessors discussed above. 
The simplest form of assembler makes two passes ever tile input, where a puss 
consists of reading an input file once. In the first pass, all the identifiers that 
denote storage locations are found and stored in a symhl table (separate from 
that of the compiler). Identifiers are assigned storage locations as they are 
encountered for the first time, so after reading ( I  .6), for example, the symbol 
table might contain the entries shown in Fig. 1.12. In that figure, we have 
assumed lhat a word, consisting of four bytes, is set aside for each identifier, 
and that addresses are assigned starting from byte 0. 
Fig. 1.12. An assembler's syrnbl tablc wilh Identifiers uf ( 1.8). 
In the second pass, the assembler scans the input again. This time, it 
rraaslates each operation code into the sequence of bits representing that 
operation in machine language, and it translates each identifier representing a 
location into the address given for that identifier in the symbol table. 
The output of the second pass is usually relocutable machine code, meaning 
that it can be loaded starting at any location L in memory; i-e., if L i s  added 
to all addresses in the d e ,  then all references will be correct. Thus, the out- 
put of the assembler must distinguish those portions of instructions that refer 
to addresses that can be relocated. 
Exampte Id. The following is a hypothetical machine mde into which the 
assembly instructions ( l A) might be translated. 
We envision a tiny instruction word, in which the first bur bits are the 
instruction code, with 000 1, 00 10, and 00 11 standing for load, store, and 
add, respectively, By had and store we mean moves from memory into a 
register and vice versa. The next two bits designale a register, and 01 refers 
to register I in each of the three above instructions. The two bits after that 
represent a "fag," with 00 standing for the ordinary address mode, where the 

COUSINS OF THE COMPILER 
19 
last eight bits refer to a memory address. The tag 10 stands for the "immedi- 
ate" mode, where the last eight bits are taken literally as the operand. This 
mode appears in the second instruct ion of ( 1.7). 
We also see in (1.71 a * associated wi'h the first and third instructions, 
This * represents the relocarion bir that is associated with each operand in 
relocatable machine code+ Suppose that the address space containing the data 
is to be loaded starting at location L, The presence of the 4 means that L 
must be added to the address of the instruction. Thus, if L - 0 000 1 1 1 1, 
i+e., 15, then a and b would be at locations 15 and 19, respectively, and the 
instructions of (1.7) would appear as 
in absoIuw, or unrelacatablc, machine code. Nole that there is no * associ- 
ated with the second instruction in (1.71, so L has not k e n  added to its 
address in I, I.$), which is exactly right because the bits represents the constant 
2, not the location 2. 
a 
Usualiy, a program called a iuadw performs the two functions of loading and 
lin k-editing . The prwess of loading consists of taking relocatable machine 
code, altering the reloatable addresses as discussed in Example 1.3, and plat- 
ing the altered instructions and data in memory at the proper locations. 
The link-editor allows us to make a single program from several files of 
relocatable machine code, These files may have been the resull of several dif- 
ferent compilations, and one or more may be library files of routines provided 
by the system and available to any program that needs them. 
If the files art to be u ~ e d  together in a useful way, there may be some 
gxterrtd references, in which the code of one file refers to a location in 
another file. This reference may be to a data location defined in one file and 
used in another, or it may be to the entry point of a procedure that appears in 
the code for one file and is called from another file. The relocatable machine 
code file must retain the information in the symbol table for each data I%a- 
lion or instruction label that is referred to externally. If we do not know in 
advance what might be referred to, we in effect must include the entire assem- 
bler symbol table as part of the relocatable machine code. 
For example, the code of (1.7) would be preceded by 
if a file loaded with (1.7) referred to b, then that reference would be replaced 
by 4 plus the offset by which the data iocatiuns in file (1.7) were relocated. 

1.5 THE GROUPING OF PHASES 
The discussion of phases in Section 1.3 deals with the logical organization of a 
compiler. 
I n  an impkmentatioo, activities from more than one phase are 
often grouped together. 
Front and Back Ends 
Often, the phases are collected into a front end and a buck end. The front end 
consists of those phases, or parts of phases, that depend primarily on the 
source language and are largely independent of the target machine. These 
normally include lexical and syntactic analysis, the creation of the symbol 
table, semantic analysis, and the generation of intermediate code. A certah 
amount of code optimization can be done by the front end as well. The front 
end also ~ncludes the error handling that goes along with each of these phases. 
The back end includes those portions of the compiler that depend on the 
target machine, and generally, these portions do not depend on the source 
Eanguage, just the intermediate language. In the back end, we find aspects of 
the code optimization phase, and we find code generation, along with the 
necessary error handling and symbol-table operations. 
It has become fairly routine to take the front end of a compiler and redo its 
associated back end to produce a compiler for the same source language on a 
different machine. Ef the back end is designed carefully, it may not even be 
necessary tu redesign too much of the back end; this matter is discussed in 
Chapter 9. It is also tempting to compile several different languages into the 
same intermediate language and use a common back end for the different. 
front ends, thereby obtaining several compilers for one machine. However. 
because of subtle differences in the viewpoints of different languages, there 
has been only limited success in this direction. 
Several phases of compilation are usually implemented in a single pass consist- 
ing of reading an input file and writing an ouiput file. In practice. there is 
great variation in the way the phases d a compiler are grouped into passeh. so 
we prefer to organize our discussion of compiling around phases rather than 
passes, Chapter 12 discusses some representative compilers and mentions the 
way they have structured the phases into passes. 
As we have mentioned, il is common for several phases to be grouped into 
one pass. and for the activity of these phases to be interleaved during the 
pass. For example, lexica1 analysis, syntax analysis, semantic analysis, and 
intermediate code generation might be grouped into one pass. If so, the token 
stream after lexical analysis may be translated directly inro intermediate code. 
In more detail, we may think of the syntax analyzer as being "in charge." It 
attempts to d i s ~ ~ ~ e t  
the grammatical structure on the tokens it sees; it obtains 
tokens as it needs them, by calling che lexical analyzer to find the next token. 
As the grammatical structure is discovered, the parser calls the intermediate 

SEC. 1.5 
THEGROUPING OF PHASES 21 
code generator to perform ,semantic analysis and generate a portion of the 
code. A compiler organized this way is presented in Chapter 2. 
Rducisg the Number of Passes 
It is desirable to have relatively few passes, since it takes time to read and 
write intermediate files. On thk other hand, if we group several phases inm 
one pass, we may be forced to keep the entire program in memory, because 
one phase may need information in a different order than a previous phase 
produces it. The internal form uf h e  program may be considerably larger 
than either the source program nr the target program, so this space may nor 
be a trivial matter. 
For some phases, grouping into one pass presents few problems. For exam- 
ple, as we mentioned above, the interface between the lexical and syntactic 
analyzers can often be limited to a single token, On the other hand. it is 
often very hard ro perform code generation until the inlerrnediate representa- 
tion has been completely generated. For example, languageh like PLtf and 
.41gol 68 permit variables to be used before they are declared. We cannot 
generak (he target code for a construct if we do not know the types of vari- 
ables involved in that construct. Similarly, most languages allow goto's that 
jump forward in the code. We cannot determine the target address of such a 
jump until we have seen the intervening source code and generated target 
code for it. 
In some cases, it is possible to leave a blank slot for missing information, 
and fill in the slot when the information becomes available, 
In particular, 
intermediate and target c d e  generation can often be merged into one pass 
using a technique called "backpatching." 
While wc cannot explain all the 
details until we have seen intermediate-code generation in Chapter 8, we a n  
illuskrate backpatching in terms of an assembler, Recall that in thc previuus 
secrion we discussed a two-pass assembler. where the first pass discovered all 
the identifiers that represent memory locations and deduced their addresses as 
they were discovered. Then a second pass substituted addresses for ideniif- 
iers. 
We can combine the action of thc passes as follows. On encountering an 
assembly statement that is a forward reference. say 
GOTO target 
we generate a skeletal instruction, with the machine operation cnde Tor GOTO 
and blanks for the address, All instructions with blanks for the address of 
target are kcpt in a list associated with the symbol-table entry for target. 
The blanks are filled in when we finally encuuntcr an instruction such as 
target: MOV foobar, R1 
and determine the value of target; it is the address of the current instruct 
tion. We then "backpatch," by going down the list for target of all the 
instruclions lhat need its address. substituting the address of target for [he 

22 
INTR ODUfllON TO COMPILING 
SEC. 1.5 
blanks in the address fields of those instructions. This approach is easy to 
implement if the instructions can be kept in memory until all target addresses 
can be determined, 
This approach is a reasonable one for an assembler that can keep a11 its out- 
put in memory. Since the intermediate and finat representations of d
e
 for 
an assembler are rolrghly the same, and surely of appr~ximately the same 
Iength. backpatching over the length of the entire assembly program is not 
infeasible. However, in a compiler, with a space-consuming intermediate 
code, we may need to be careful about the distance over which backpatching 
occurs. 
1,6 COM PILER-CONSTRUCTION TOOLS 
The compiler writer, like any programmer, can profitably use software ~ools 
such as debuggers, version managers, profilm, and so on. In Chapter 11, we 
shall see how some of these tools can be used to implement a compiler. In 
addition to these software-development tools, .othsr more specialized took 
have been devebptd for helping implement. various phases of a compiler + We 
men tion them briefly in this section; they are covered in detail in the appropri- 
ate chapters. 
Shortly after the first compilers were written, systems to help with the 
compiier-writing process appeared. These systems have often been referred GO 
as 
compiler-cot~pders, compiler-generators, or 
translcifur-wriiing systems, 
Largely, they are oriented around a particular model of languages, and they 
are most suitable for generating compilers of languages similar to the model. 
For, example, it is tempting to assume that lexical analyzers for all 
Languages are essentially the same, except for the particular keywords and 
signs remgn Ized . Many compiler-compilers do in fact produce fixed lexical 
analysis routines for use in the generated compiler. These routines differ only 
in the list of keywords recognized, and this list is all that needs to k supplied 
by the u
~
.
 
The approach is valid, but may be unworkable if it is required to 
recognize nonstandard tokens, such as identifiers that may include certain 
characters other than letters and digits. 
Some general tools have been created for the automatic design of specific 
compiler mrnpnen ts, 
These tools use specialized languages for specifying 
and implementing the mmpnent, and many use algorithms that arc quite 
sophisticated. The most successful tools are those that hide the details of the 
generation algorithm and produce cornpnents that can be easily integrated 
into the remainder of a compiler. The following is a list of some useful 
compiler-mstruc1ton tools: 
I. Parser generators. These produce syntax analyzers, normally from input 
that is based on a context-free grammar. In early compilers, syntax 
analysis.consurned not only a large fraction of the running time of a com- 
piler, but a large fraction of the intellectual effort of writing a compiler. 
This phase i s  now considered one of the easie-st to implement. Many of 

CHAPTER I 
BlBLlOGR APHIC NOTES 23 
the "little languages" used to typeset this book, such as PIC {Kernighan 
119821) and EQN, were implemented in s few days using the parser gen- 
erator described in Section 4+7. Many parser generators utilize powerful 
parsing algorithms that are too complex to be carried out by hand. 
Scanner ggenrmrors. These automatically generate lexical analyzers, nor- 
mally from a specificalion based on regular expressions, discussed in 
,Chapter 3. The basic organization of the resulting lexical analyzer I s  in 
effect a finite automaton. A typical scanner generator and irs implemen- 
tation are discussed in Sections 3+5 and 3.8. 
Synmx-dirwid mmdution engines. These produce collections of routines 
that walk the parsc Iree, such as Fig, 1.4, generating intermediate code, 
The basic idea is that one or more "translations" are associated with each 
node of the parse tree, and each translation is defined in terms of transla- 
tions at its neighbor nodes in the tree. Such engines are discussed in 
Chapter 5. 
Ausomarich code pneruturs. Such a tool takes a colkctlon o f  rules that 
define the translation of each operation of the intermediate language into 
the machine language for the target machine. The rules must include suf- 
ficient detail that we can handle the different possible access methods for 
data; e.g.. variables may be in registers, in a tixed (static) location in 
memory, or may be allocated a position on a stack. The basic technique 
i s  "template matching." The intermediate code statements are replaced 
by "templa~es" that represent sequences of machine instructions, in such 
a way that the assumptions a b u t  storage of variables match from tem- 
plate to template. Since there are usually many ophns regarding where 
variables are to be placed (e+g., in one of severa1 registers or in memory), 
there are many possible ways to "tile" intermediate code with a given set 
of templates, and it is necessary to select a g
d
 
filing without a cumbina- 
torial explosion in running time of the compiler, Twis of this nature are 
covered in Chapter 9. 
Dal+flow engines. Much of the information needed to perform g
d
 
code 
optimization involves "data-Row analysis," the gathering of information 
a b u t  how values are transmitted from one part of a program to each 
other part. Different tasks of this nature can be performed by essentially 
the same routine, with the user supplying detaiis of the relationship 
bet ween intermediate code statements and the information being gath- 
ered. A twl of this nature is described in Section 10.1 1. 
BIBLIOGRAPHIC NOTES 
Writing in 1%2 on the history of compiler writing, Knuch 119621 observed 
that, "ln this field there has k e n  an unusual amount of paraljel discovery of 
the same technique by people working independently." He continued by 
observing that several individuals had in fact dimvered "various aspects of a 

24 
INTRODUCT[ON TO COMPlLING 
CHAPTER 1 
technique, and it has been polished up through the years into a very pretty 
algorithm, which none of the originators fully realized," Ascribing credit for 
techniques remains a perilous task; the bibliographic notes in this b
k
 
are 
Intended merely as an aid for further study of the literature, 
Historical notes on the development of programming languages and com- 
pilers until the arrival of Fortran may be found in Knuth and Trabb Pardo 
1 19771. Wexelblat 1198 1 j contains historical recdections a b u t  several pro- 
gramming languages by participants in their development. 
Some fundamental early papers on compiling have been collected in Rosen 
1 l%71 and Pollack [1972]. The January 1%1 
issue of the Communir.utiurts qf 
the ACM provides a snapshot of the state of compiler writing at the time. A 
detailed account of an early Algol 60 compiler is given by Randell and 
Russell [l9641. 
Beginning in the early 1960's with the study of syntax, theoretical studies 
have had a profound influence on the development of compiler technology, 
perhaps, at least as much influence as in any other area of computer science. 
The fascination wilh syntax has long since waned, but compiling as a whole 
continues to be the subject OF lively research. The fruits of this research will 
become evident when we examine compiling in more detail in the following 
chapters. 

CHAPTER 2 
A Simple 
One-Pass 
Compiler 
This chapter i s  an introduction to the material in Chapters 3 through 8 of this 
book. It presents a number of basic compiling techniques that are illustrated . 
by denloping s working C program that trbns~ates infix expressions into post- 
fix form. Here, the .emphasis is on the front end of st compiler, that is, on 
lexical analysis, parsing, and intermediate code generation. Chapters 9 and 10 
cover code generation and optimization. 
2.1 OVERVIEW 
A programming language can be defined by describing what its programs look 
like (the svntax of the language) and what its programs mean (the semuntirLs of 
the language). For specifying r he syntax of a language, we present a widely 
used notation, called con text-free grammars or BNF for Backus-Naur Form). 
With the notations mrrenlly available, the semantics of a language is much 
more difficult to descrilx than the syntax. Consequenlly, for specifying the 
semantics of a language we shall use informal descriptions and suggestive 
< 
examples. 
Besides specifying the syntax of a language, a context-free grammar can be 
used to help guide the translation of programs. A grammar-oriented mrnpil- 
ing technique, known as symu-diwctd rranslutioa, is very helpful for organ is- 
ing a compiler front end and will bc used extensively throughout this chapter+ 
In the course of discussing syntax-directed  rans slat ion, we shall construct a 
compiler that translates infix expressions into postfix form, a notation in 
which the operators appear after their operands. For example, the postfix 
form of the expression 9-5+2 i s  95-2++ Postfix natation can be converted 
directly into code for a computer that performs all its computations using a 
stack. We begin by constructing a simple program to translate expressions 
consisting of digits separated by plus and minus signs into postfix form. 
AS 
the basic ideas become clear, we extend the program to handle more general 
proyamming language constructs. Each of our translators is formed by sys- 
tematically extending the previous one. 

26 
A SIMPLE COMPILER 
SEC. 2.2 
I n  our compiler, the k i d  nwlyrer converts the stream of input characters: 
into a stream of tokens that becomes the input to the following phase, as 
shown in Fig. 2.1. The "syntax-directed translator" in the figure is a combi- 
nation of a syntax analyzer and an intermediatecode generator. One reason 
for starting with expressions consisting of digits and operators is to make Iexb 
cal analysis initially very easy; each input character forms a single token. 
Later, we extend the language to include lexical constructs such as numbers, 
identifiers, and keywords. For this extended language we shall construct a 
lexical analyzer that collects consecutive input characters into the appropriate 
tokens. The construction of lexical analyzers will &e discussed in detail in 
Chapter 3. 
character 
intcrmdialc 
analyzer 
stream 
directed 
strcam 
rcprmcntirt ion 
t ransIa4or 
Fig. 2.1. Structure of our compiler front cnd. 
2-2 SYNTAX DEFINITION 
In this section, we introduce a notation, called a context-free grammar (gram- 
mar, for short), for specifying the syntax of a language, It will k used 
throughout this book as part of the specification of the front end of a corn- 
piler . 
A grammar naturally describes the hierarchical structure of many program- 
ming language constructs. For example, an ifelse statement in C has the 
form 
if ( expression ) statement dse statement 
That is, the statement is the concatenation of the keyword if, an opening 
parenthesis, an expression, a cbsing parenthesis, a statement. the key word 
else, and another statement. (In C, there is no keyword then.) Using the 
variable expr to denote an expression and the variable stmt to denole a state- 
ment, this structuring rule can te expressed as 
in which the arrow may be read as "can have the form ." Such a rule is called 
a prducriorz. In a production lexical dements like the keyword if and the 
parentheses are ailed tokens, 
Variables like expr and scml represent 
sequences of tokens and are called nontwminals. 
A cmrex~-frpe gramnear has four components: 
I .  A set of tokens, known as i ~ r m i n d  symbls. 

SEC. 2.2 
2. 
A set of nonterminals. 
.3. 
A set of productions where each production consists of a nmterminal,. 
called the Itft side of the production, an arrow, and a sequend of tokens 
and/or nonterminals, called the right side of the production. 
4. 
A designation of one of the nonterminals as the start symbol. 
We follow the convention of specifying grammars by listing their prduc- 
.[ions, with the productions for the start symbol listed first. We assume that 
digits, signs such as <=, and boldface strings such as while are terminals. An 
italicized name is a nontwminal and any nonitalicized name or symbol may be 
assumed to be a token.' For notational convenience, productions with the 
same nonterrninal on the left can have their right sides grouped, wiih the 
alterna~ive right sides kparatod by tha symbol 1 , which we read as '*or. " 
Example 2.1. Several examples in this chapter use expressions consisting of 
digits and plus and minus signs, e.g., 9-5+2, 3-1, and 7. Since a plus or 
minus sign must appear between two digits, we refer to such expressions as 
"lists of digits separated by plus or minus signs." The following grammar 
describes the syntax of these expressions. The productions are: 
The right sides of the three productions with nonterrninal list on the left 
side can equivalently be grouped: 
According to our conventions, the tokens of the grammar are the symbols 
The nonterminals are the italicized names list and digit, with Fis~ being the 
starting nonterminal because its productions are given first. 
u 
We say a production is for a nonterrninal If the nontcrminal appears on the 
left side of the production. A string of tokens is a sequence of zero OT more 
tokens. The string containing zero tokens, written as t, is called the empty 
string. 
A grammar derives strings by beginning with the start symbol and repeat- 
edly replacing a nonterminal by the right side of a prcduction for that 
' Individual italic letters will be used for additional purposes when gcarnrnars arc studied in dctril 
in Chaprer 4. For examplc, wc shall use X, Y, a d  Z to talk a h u t  a symbol that is ctrhcr a lnkcn 
or a nonktmind. HOW~YCT, 
any itabicized mamc mntaining two ur mure characters will mntinuc 
to rcprcsent a nonrc~minal. 

28 
A SIMPLE COMPILER 
SEC. 2+2 
nonterminal. The token strings that can be derived from the start symbol 
form the J U H ~ I I U R C ~  defined by the grammar. 
Elcampie 2.2. The Ianguagt defined by the grammar of Example 2+1 consists 
of lists of digits separated by plus and minus signs. 
The ten 
for the nonterminal &it allow it to stand for any of 
the tokens 0, 1, . + . , 9. From production (2.4), a single digit by itself is a 
list. Productions (2.2) and (2.3) express the fact that if we take any list and 
follow it by a plus or minus sign and then anurhec digit we have a new list. 
It turns out that prdunjons (2.2) to (2.5) are all we need to define the 
language we are interested in. For example, we can deduce that 9-5+2 is a 
lisl as follows. 
a) 
9 is a /is! by production (2.4), since 9 is a digir. 
b) 
9-5 is a Iisr by production (2+3), since 9 is a h r  and 5 is a digit. 
C) 9- 
5+2 is a h.rr by production (2.21, since 9- 
5 is a list and 2 is a dlgir. 
This reasoning i s  ilIustrated by the tree in Fig. 2.2. Each node in the rrce is 
labeled by a grammar symbol. An interim node and its children correspond 
to a production; the interior node corresponds to the left side of the praduc- 
tion, the children to rhe right side. Such trees are called parse trees and are 
discussed below. 
Fig. 2.2. 
Parsc trcc for 8- 5*2 according to the grammar in Example 2.1. 
Example 23. A somewhat different sort of tist is the sequence of statements 
separated by semicolons found in Pascal begin-end blocks. One nuance of 
such lists is that an empty lisl of statements may be found between the tokens 
begin and end. We may start to develop a grammar for begin-end blocks by 
induding the productions: 

SEC. 2.2 
SYNTAX DEFINITION 29 
Note that the second possible right side for u p ~ s t m ~ ~  
("optional statement 
list") is e, which stands for the empty string of symbuls. That is, opt-rtmrs 
can be replaced by the empty string, so a block can consist of the two-token 
string begin end. Notice that the productions for .stmt_liss are analogous to 
those for h r  in Example 2.1. with semicolon in place of the arithmetic Q p 3 -  
tor and srml in place of d i ~ i r .  We have not shown the productions for srmi, 
Shortly, we shall discuss the appropriate productions for the various kinds of 
statements, such as if-statements, assignment statements. and so on. 
0 
Parse Trees 
A parse tree pictorially shows how the start syrnhol or a grammar derives a 
string in the language, If nonterrninal A has a production A 
XYZ, then a 
parse tree may have an interior nude labeled A with three children labeled X, 
Y, and 2, from left to righc 
Formally, given a context-free grammar, a purse tree is a tree with the ful- 
towing pruperries: 
I .  The r w t  is Iabekd by the start symbol 
2. 
Each leaf is labeled by a token or by E. 
3. 
Eachinterior nodeislabeledby anonterminal. 
4. 
If A is the nonterrninal iabeling some interior node and XI, Xz, . . . . X,, 
are the labels of the children of that node from left to right, theq 
A - X I X I  . . + X,, is a production. Here, XI, Xz, . . . . X,, stand for a 
symbol that is either a terminal or a nonterrninal. As a special case, if 
A - E thcn a node labeled A may haw a single child labeled E+ 
Example 2.4. 
In Fig, 2.2, the root is labeled list, the start symbol of the 
grammar in Exsmple 2.1. The children of the root are labeled. from Left to 
right, lisr. +, and digii. Note that 
Iisf 
+ list + di#It 
is a production in the grammar of Example 2.1. The same pattern with - is 
repeated at the left child of the root, and the three nodes labeled digil each 
have one child that is labeled by a digit. 
a 
The leaves of a parse tree read from left to right Corm the ykM of the tree, 
which is thc string gmtwk~d or dcriv~d from the nonterminal at the root of 
the parse tree. In Fig. 2+2, the generated string is 9-5+2. 
l o  that figure, all 
the ieavcs arc shown at the bottom level. Henceforth, we shall not necessarily 

30 
A SIMPLE COMPlLER 
SEC. 2.2 
line up the leaves in this way. Any tree imparts a natural left-bright order 
to its leaves, based on the idea that if a and I, are two children with the same 
parent, and a is to the left of b, then all descendants of a are 40 the left of 
descendants of B. 
Another definition of the language generated by a grammar is as the set of 
strings that can be generated by some parse tree. The process of finding a 
parse tree for a given string of tokens i s  called pursing that string. 
Ambiguity 
We have to be careful in talking about the structure of a string according to a 
grammar, Whik it is clear that ea& parse tree derives exactly the string read 
off its leaves, a grammar can have more than cine parse tree generating a 
given string of tokens. Such a grammar is said to be ambiguous. To show 
that a grammar is ambiguous, d l  we need to do is fib a token string that has 
more than cine parse tree. Since a string with more than one parse tree usu- 
ally has more than one meaning, for cornpilifig applications we need to design 
unambiguous grammars, or to use ambiguous grammars with additional rules 
to resolve the ambiguities. 
Example 2.5. Suppose we did nut distinguish between digits and lists as in 
Example 2.1. We could have written the grammar 
Merging the oat ion of digit and iisr into the nonierminal string makes superfi- 
cial sense. because a sirigle digir is a special case of a list. 
However, Fig. 2.3 shows that an expression like 9-5+2 now has more than 
one parse tree. The two trees for 9-5+2 correspond to the two ways of 
parenthesizing the expression: 
(9-5 ) +2 and 9- 1 5+2 ) . 
This second 
parenthesizatim gives the expression the value 2 rather than the customary 
value 6. The grammar gf 'Example 2+1 did not permit this interpretation. 
a 
By convention, 9+5+2 is equivalent to I 9 + 5 ) + 2  and 9-5-2 Is equivalent to 
(9-51-2. 
When an operand like 5 has operators to its left and right, con- 
ventions are needed for deciding which operator takes that operand. We say 
that the operator + ussuciares ro h e  !eft kcause an oprand with plus signs on 
both sides of it is taken by the operator to its left. In most programming 
languages the four arithmetic operators, addition, su btraaion, multiplication, 
and division are left associative. 
Some common operators such as exponentiation are right associative. As 
another example, the assignment operator = in C is right associative; in C, 
the 
expression a=b=c is treated in the same way as the expression a=( b=cl. 
Strings like a=b=c with a right-associative qxrator are generated by the 
following grammar; 

string 
- 
s!rin# 
2 
Fig, 2.3. Two par= trws for 9-5+2 
The contrast between a p a w  tree for a left-associative operator like - and a 
parse tree for a right-associative operator like = is shown by Fig. 2.4. Note 
that the parse tree for 8-5-2 grows down towards the left, whereas the parse 
tree for a=b=c grows down towards the right. 
Fig, 2.4, Parse trees for left- and right-awiativc operators. 
Precedence of Operators 
Consider the expression 9+S+2. There ace two possible interpretat ions of this 
expression: t9+51+2 or 9+(5*2). The associativity of * and * do nut 
resolve this ambiguity. For this reason, we need to know the relative pre- 
cedence of operators when more than one kind of operator is present. 
We say that + has hi8ht.r precedence than + if * takes its operands before + 
does. In ordinary arithmetic, multiplication and division have higher pre- 
cedence than addition and subtraction. Therefore, 5 is taken: by * in both 
9+5*2 and 9*5+2; ie., the expressions are equivalent to 9+I5+2) and 
1 9 ~ 5  
1+2, respectively. 
Syrtiar of apressim, A grammar for arithmetic expressions a n  be 

32 
A SIMPLE COMPILER 
SEC. 2.2 
constructed from a table showing the a~wciativity and precedence of opera- 
tors. We start with the four common arithmetic operators and a precedence 
table, showing the operators in order of increasing precedence with operators 
at the same precedence level on the same line: 
left associative: 
+ - 
left associative: 
4 / 
We create two nonterminals u p r  and Vrm for the two levels of precedence. 
and an extra nonterminal frrctur for generating basic units in expreuions. The 
basic units in expressions are presently digits and parenthesized expressions. 
Now consider the binary operators, 
and 1, that have the highest pre- 
cedence. Since these operators associate to the left, the productions are simi- 
lar to those for lists that associate to the left. 
Similarly, vxpr generates lias of terms separated by the additive operators. 
The resulting grammar is therefore 
This grammar treats an expression as a list of terms separated by either + or - 
signs, and a term as a list of factors separated by + or / signs, Notice that 
any parenthesized expression is a factor, so with parentheses we can develop 
expressions that have arbitrarily deep nestiing (and also arbilrarily deep trees). 
Syrtrax uf xtaternents, Keywords allow us to recognize statements in most 
Languages. All Pascal statements begin with a keyword except assignments 
and procedure calls. Some Pascal statements are defined by the following 
(ambiguous) grammar in which the token id represents an. identifier. 
The nonrerminal vpr,rtrrs generates a possibly empty list of statements 
separated by semicolons using the productions in Example 2+3. 

sec. 2.3 
SYNTAK-DLRE~ED 
TRANSLATION 33 
2 2  SYNTAX-DIRECTED TRANSLATION 
To translate a programming language construct, a compiler may need to keep 
track of many quantities besides the oode generated for the construct. For 
exampk, the compiler may need to know the type or the construct, or the 
lmtion of Ihe first instruction in the target d e ,  or the number of instruc- 
 ions generated. We therefore talk abstractly abut atiribuies asswiated with 
mstructs. An attribute may represent any quantity, e.g., a type, a string, a 
memory hation, M whatever, 
In this section, we present a formaltsrn called a syntaxdirected definition 
for specifying translations for programming language mnst ructs. A syntax- 
directed definition s p e c i k s  the translation of a construct in terms of attributes 
associated with its syntactic mmpncnts , ln later chapters, syn tsrxdueded 
definitions are used to specify many of the translations that take piace in the 
f r a t  end of a compiler. 
We siw intruduw a more procedural notation, called a translation scheme, 
for specifying translations. Throughout this chapter, we use translation 
schemes for translating infix expressions into postfix notation, A more 
detailed discussion of syntaxdirected definitions and their implementation is 
contained in Chapter 5. 
The posft ncriutiurt for an expressim B can be defined inductively as follows: 
1. 
If E is a variable or constant, then the pstfix n~aticin for E is E itself. 
2. 
If E Is an expression of the form E l  op E Z +  where op is any binary opera- 
tor, then the postfix notation for E is El' Elr op, where El' and E2' are 
the postfix notations For E and E l ,  respectively. 
3. 
IF E is an expression of the form ( El ), then the postfix notation For E 1 
is aim the psifix notation for E. 
No parenthem are needed in postfix notation because the position and arity 
(number of arguments) of the operators permits only one decoding of a post- 
fu expression. For example, the pstfix notation for [9-5) +2 is 95-2+ and 
the p t f i x  mation fur 9- { 5+2 1 is 952+-. 
A syntax-direcid dcfirolrion u m  
a wntcrrt-frw grammar to specify the syntac- 
tic structw of the input. With each grammar symbl. it associates a set of 
attributes, and with each prcduction, a et of scmntic rules for computing 
value$ of the attributes wmiated with the symbols appearing in that produc- 
tian. The grammar and the e r  of semantic rules constitute the syntax- 
directed definition. 
A Iraodriim is an input-output mapping. The output for each input x is 
specified in the following manner. Fim, construct a parse tree for x. S u p p  

34 
A SIMPLE COMPILER 
SEC. 2.3 
a node n in the parse tree is labeled by the grammar symbol X. We write X.u 
to denote the value of attribute a of X at that node. The value of X.rr at n i s  
computed using the semantic rule for attribute a associated with the X- 
production used at node n. A parse tree showing the attribute values at each 
node is called an nltnoiuted par= tree. 
Synthesized Attributes 
An attribute i s  said to be synrkesized if its value at a par%-tree n d e  is deter* 
mined from attribute values at the children of the node. Synthesized attri- 
h t c s  have the desirable property that they can be evaluated during a single 
Bottom-up traversal of the parse tree. In this chapter, on ty synthesized attri- 
butes are used; "inherited" attributes are considered in Chapter 5 ,  
Example 2.6, A syntax-directed definition for translating expressions consist- 
ing of digits separated by plus or minus signs into postfix notation is shown in 
Fig. 2.5. Associated with each nonterminal is a stringvalued attribute r that 
represents the post fix notarisn for the expression generated by that nontermi- 
nal in a parse tree+ 
Fig. 2.5. Syntax-dircctcd definition for infix to postfix translation. 
The postfix form of a digit is the digit itselc e.g., the semantic rule associ- 
ated with the production term -* 9 defines term1 to be 9 whenever this pro- 
duction i s  uad at a node in a parse tree. When the production cxpr * term is 
applied, the value of term. t becomes the value of expr. I. 
The production expr .-, expr I + rerm derives an expression containing a plus 
operator (the subscript in expr 1 distinguishes the instance of expr an the right 
from that on the left side). The left operand of the plus operator i s  given by 
expr and the right operand by term. The semantic rule 
associated with this production defines the value of attribute expr.r by con- 
catenating the postfix forms expr j.t and !erm.i of the left and right operands, 
respectively, and then appending the plus sign. The operator 11 in semantic 

SYHTAX+DIRECTEO TRANSLATION 
35 
rules represents string concatenation. 
Figure 2.6 contains the annotated parse tree correspnding to the tree of 
Fig. 2.2. The value of the 1-attribute at each node has been computed using 
the semantic rule associated with the production used at that node. The vahe 
of the attribute at the root is the postfix notation for the string generated by 
the parse tree. 
o 
Fig. 2.6. 
Attribute values at nodes in a parse tree+ 
Example 2.7. Suppose a robot can be instructed to move one step east, north, 
west, or south from its current position. A sequence of such instructions is 
generated by the foUowing grammar: 
Changes in the position of the robot on input 
are shown in Fig, 2.7. 
south I 
I north 
Fig. 2,7. Kccping track of a robot's psition. 
In the figure, a position is marked by a pair (x,y), where x and y represent 
the number of steps to the eahi and north, respectively, from the starting 

36 
A SIMPLE COMPILER 
SEC. 2.3 
position. {If x is negative, then the robot is to the west of the starting pi- 
tion; similarly, if y is negative, then the robot is to the wuth of the starting 
position .) 
Let us construa a syntax-directed definition to translate. an instruction 
sequence into a robot position. We shall u.se two attributes, ,wy.x aod sc.y.y, 
to keep track of the position resulting from an instruction sequence generated 
by the nonterminal wq. Initially, scy generates begin, and s q . x  and sq.y are 
both initialized to 0, as shown at the leftmosr interior node of the parse tree 
for begin west south shown in Fig. 2.8. 
kgin 
west 
Fig. 2.8. 
Annotated pars trcc fur begin west south. 
The change in position due to an individual instruction derived from instr IS 
given by attributes inar,dx and instr.dy. For example, if instr derives west, 
then insrr.rlw = - I and r'nsrrdy = 0. Suppose a sequence . s q  is formed by 
following a sequence sty, by a new instruction inslr. The new psirion of the 
robot is [hen given by the rules 
A syntax-directed definition for translating an instruction sequence into a 
robot position is shown in Fig. 2.9. 
Depth-First Travemb 
A syntaxdirected definition does not impose any specific order for the evalua- 
tion of attributes on a parse tree; any evaluation order that computes an attri- 
bute a after all the other attributes that u depends on is acceptable. In gen- 
eral, we may have to evaluate some attributes when a node is first reached 
during a walk of the parse tree. others after all its children have been visited, 
ur at some point in between visits to the children of the node, Suitable 
evaluation orders are discussed in more detail in Chapter 5. 
The translations in this chapter can all be imp'lemented by evaluating the 
semantic rules for the attributes in a par,* tree in a predetermined order. A 
rrwersul of a tree starts at the root and visits each node of the tree in some 

SEC. 2.3 
SY NTAX-DIR ECTED TRANSLATION 37 
I 
h ~ i f . &  : = 0 
instr -r north 
imir.dy := I 
h.w - east 
Fig, 2.9. Synrax-dircctcd definition of thc rrht's pohi~im. 
instr.rlx :- I 
tns!r+dv : = O 
order, In this chapter, semantic rules will be evaluated using the depth-first 
traversal deiined in Fig, 2-10, It starts at the root and recursively visits the 
children of each node in left-to-right order, as shown in Fig. 2.11. The 
semantic rules at a given node are evaluated once all descertdants of that node 
have been visited, It is c a k d  "depth-first*' because it visits an unvisited child 
of a node whenever it can, so it tries 10 visit nodes as far away from the root 
as quickly 3s it can. 
procedure visit (n : nodc); 
begin 
- 
for cach child m nf n, from Icfr to righi do 
v i d  (#Y 1; 
cva[uufc scmanfic rules at nidc R 
end 
Fig, 2,10, A dcpth-first traversal of a trcc 
Translation Sc hems 
In the remainder of this chapter, we use n procedural specification for defining 
a translation. 
A irundutiun sthtmv is a context-free grammar in which pro- 
gram fragments called semunrirb urriuns are embedded within the right sides of 
product ions, A translation scheme i s  like a syntax-directed definition, except 
that the order of evaluation of the semantic rules is e~plicitly shown. Thc 

38 
A SIMPLE COMPILER 
Fig, 2.11. Examplc of a deplh-first traversal of a trec. 
position at which an action is to be executed is shown by enclosing it between 
braces and writing it within the right side of a production, as in 
rest + + term ( print ( ' + ' ) j resr 
A translation scheme generates an output for each sentence x generated by 
the underlying grammar by executing the actions in the order they appear dur- 
ing a depth-first traversal of a parse tree for x, For example, consider a parse 
tree with a node labeled rest representing this produchn, The action 
{prinr('+'l ) will be performed after the subtree for t
m
 is traversed but 
before the child for rest! is visited, 
Fig, 2.22, An cxtra Icaf is wnstructcd for a scrnantic action. 
When drawing a parse tree for a translation scheme, we indicate an action 
by constructing for it an extra child, connected by a dashed h e  to the node 
for its production, For example, the portion of the parse tree for the above 
production and action is drawn as in Fig. 2.12. The node for a semantic 
action has no children, so the action is performed when that node is first seen. 
Emitting s Translation 
In this chapter, the semantic actions in translations schemes will write the out- 
put of a translation into a file, a string or character at a time. For example, 
we translate 9-5+2 into 95-2+ by printing eacb character in 9-5+2 exactly 
once, without using any storage for the translation of subexpressions. When 
the output is created incrementally in this fashion, the order in which the 
characters are printed is important. 
Notice that the syntax-directed pefinitions mentioned so far have the follow- 
ing important property: the srrcng representing the translation of the rrantermi- 
nal on the left side of each production is the concatenation of the translations 

SEC. 2.3 
SY NTAX-DIRECTED TRANSLATION 
39 
of the nonterminals on the right; in the same wder as in the production, with 
some additional strings (perhaps none) interleaved. A syntaxdirtxted defini- 
tion with this property is termed simple. For example, consider the first pro- 
duction and semantic rule from the syntax-direded definition of Fig. 2-5; 
Here the translation expr+r is the concatenation of the translations of e q r  I and 
term, followed by the symbol t. Notice that exprl appears before term on the 
right side of the prduction. 
An additional string appears between t m . C  and r e ~ t  
1 .r in 
PRODUCTION 
SEMANTIC 
RULE 
rat 
+ rerm r s r  
rest.t := rerm.t 11 '+' 1 nstl.t 
~
7
)
 
but, again, the nonterminal k r m  appears before resr on the right side. 
Simple syntax-d irected definitions can be implemented with translation 
schemes in which actions print the additional strings in the order they appear 
in the definition. The actions In the Following productions print the additional 
strings in (2.6) and (2.7), respectively: 
Example 2-8. Figure 2.5 contained a simple definition for translating expres- 
sions into postfix form. A translation scheme derived from this definition is 
given in Fig. 2.23 and a parse tree with actions for 9-5+2 is shown in Fig, 
2,14. Note that although Figures 2.6 and 2.14 represent the -me input- 
output mapping, the translation in the two cases is constructed differently; 
Fig. 2.6 attaches the outpul to the root of the parse tree, while Fig. 2.14 prints 
the output incrementally. 
Fig. 2.13, Actions translating expressions into postfix aotation. 
The root of Fig. 2.14 represents the first production in Fig, 2.13, Jn a 
depth-first traversal, we first perform all the actions in the subtree for tht left 
operand txpr when we traverse the leftmost subtree of the root. We then visit 
the leaf + at which there is no action. We next prform the actions in the 

4 A SIMPLE COMPILER 
SEC. 2.3 
subtree for the right operand term and, finally, the semantic action 
{ prim {'+') ) at the extra node. 
Since the productions for term have only a digit on the right side, that digit 
is printed by the actions for the productions. No output is necessary for the 
production expr - term, and only the operator needs to be printed in the 
action for the first two productions. When executed during a depth-first 
traversal of the parse tree, the actions in Fig. 2.14 print 95-2+. 
o 
Fig. 2.14. 
Actions rranslaring 9-5+2 into 95-2+. 
As a general rule, most parsing methods prwess their input from left to 
right in a "greedy** fashion; that is, they construct as much of a parse tree as 
possible before reading the next input token. In a simple translation scheme 
(one derived from a simple syntax-directed definition), actions are also done 
in a left-bright order. Therefore, to implement a simple translation scheme 
we can execute the semantic actions while we parse; it is not necessary to can- 
struct the par= tree at all. 
2.4 PARSING 
Parsing is h e  process of determining if a string of tokens can be generated by 
a grammar. In discussing [his problem, it is helpful to think of a parse tree 
being constructed, even though a compiler may not actually construct such a 
tree. However. a parser must be capable of constructing the tree, or else the 
translation cannot be guaranteed wrrect. 
This m i o n  introduces a parsing method that can be applied to construct 
synta x-directed translators. A complete C program, irnplemen ting the transla- 
tion scheme of Fig. 2,13, appears in the next section. A viable alternative is 
to use a software tool to generate a translator directly from a translation 
scheme. See %dim 4.9 for the description of such a tool; it can implement 
the translation scheme of Fig. 2.13 without modification. 
A parser can be constructed for any grammar. Grammars used in practice, 
howtvcr, have a special form. For any context-free grammar there is a parser 
that takes at most ~ ( n "  
time to parse a string of n tokens. But cubic time is 

too expensive. Given a programming language, we can generally construct a 
grammar that can be parsed quickly. Linear algorithms suffice to parse essen- 
tially all languages that arise iin practice. Programming language parsers 
almost aiways make a single left-to-right scan over the input, hoking ahead 
one token at a time. 
Most parsing methods fall into cine of two classes, called the topdown and 
b u m - u p  methods. These terms refer to the order in which rides in the 
parse tree are constructed. In the former, wnstrudion starts at the root and 
proceeds towards the leaves, while, in the latter, construction starts at the 
leaves and proceeds towards the rod. The popularity of top-down parsers is 
due to the fact that efficient parsers can be constructed more easily by hand 
using topdown methods, Batom-up parsing, however, can handle a larger 
class OF grammars and translation, schemes, so software tools for generating 
parsers directly from grammars have tended to use bttom-up methods. 
Top-Down Parsing 
We introduce topdown parsing by considering a grammar that is well-suited 
for this class of methods. Later in this section, we consider the construction 
of top-down parsers in general. The following grammar generates a pubset of 
b 
the types of Pascal. We use  he token dotdot for ". ." to emphasize that the 
character sequence is treated as a unit. 
The top-dawn construction of a parse tree is done by starting with the root, 
labeled with the starting nonterminal, and repeatedly performing the followittg 
two steps (see Fig. 2.15 for an example). 
I. At node n ,  labeled with nonterminal A, select one of the prductions for 
A and wnstrua children at n for the symbols on the right side of the pro- 
d u ~ t  
ion. 
2. 
Pind the next node at which a subtree is to be constructed. 
For some grammars, the above steps can be implemented during a single Icft- 
bright scan of the input string. The current token being scanned in the input 
is frequently referred to as the Imkcrkeab symbol. Initially, the lookahead 
symbol is the first. i.e.. leftmost, token of the input string. Figure 2.16 illus- 
trates the parsing of the string 
m a y  [ nurn dotdot nurn 1 of Integer 
Initially, the token array is the lookahead symbol and the known part of the 

42 
A SIMPLE COMPILER 
SEC. 2.4 
mum 
Wd 
num 
num 
dddot 
num 
FPg. 2.15. 
Stcps in thc top-down mnstructim of a parw i r ~ .  
parse tree consists of the root, labeled with the starting nonterminal iype in 
Fig. 2.16(3). The objective is to construct the remainder of the parse tree in 
such a way that the string generated by the parse tree matches the input 
string. 
For a match to occur, nonterrninal fype in Fig. 2.16Ia) must derive a string 
that starts with the lookahead symbol array. In grammar (2,8), there is jusr 
one production for ~ p e  
that can derive such a string, so we select it, and con- 
struct the children of the root labeled with the symbols on the right side of the 
p~oduction . 
Each of the three snapshots in Fig+ 2.16 has arrows marking the lookahead 
symbol in the input and the node in the parse tree that is being considered. 
When children are constructed at a nobe, we next wtkider the leftmost child. 
In Fig. 2. Lqb), children have just been constructed at the root, .and the left- 
most child labeled with array is being considered. 
When the node being considered in the parse tree is for a terminal and the 

a n y  
[ 
nam dotdot nrm 
1 
@f integer 
INPUT t 
array 
t 
mum M d o t  num 
of integer 
array 
[ 
num dotdot mm 
I 
d integer 
INPUT 
t 
Fig. 2.16. Topdown parsing while scanning thc input from left to right. 
lerrninal matches the lookahead symbol, then we advance in both the parse 
tree and the input. The hext token in the input becomes the new lookahead 
symbol and the next child in the parse tree is considered. In Fig, 2,iqc), the 
arrow in the parse tree has advanced to the next child OC the root and the 
arrow in the input has advanced to the next token [. After the next advance. 
the arrow in the parse tree will point to the child labeled with rtonterminal 
simple. When a n d e  labeled with a nonterminal is considered, we repeat the 
process of selecting a production for the nonterminai, 
In general, the selection of a production for a nonterminal may involve 
triaband-error; that is, we may have to try a pruduction and backtrack to try 
another production if the first is found to be unsuitable, A production is 
unsuitable if, after using the prduction, we cannot complete the tree to match 
the input string. There is an important special case, however, called predic- 
tive parsing, in which backtracking does not occur. 

44 
A SIMPLE COMPILER 
Predictive Parsing 
Rcrwrsive-de,rt~e~$ 
pursing is a top-down met hod of syntax analysis in which we 
execute a set of recursive procedures to process the input, A procedure i s  
associated with each nonterminal of a grammar. Here, we consider a special 
form of recursive-descent parsing, called predictive parsing, in which the lmk- 
ahead symbol unam biguausly determines h e  procedure selected for each non- 
terminal. The sequence of procedures called in processing the input implicitly 
defines a parse tree for the input. 
h 
procedure rnrrft*h I r : klkon); 
begin 
if k w k d m d  = t lhen 
hkt1htYtd : = tt~Wtt>kM 
dae error 
end; 
The predictive parser in Fig. 2+17 c~nsists of prmedures for the ncsntermi- 
nals type and simpk uf grammar (2.8) and an additional procedure mufch, We 

SEC+ 2.4 
PARSING 
45 
use mrrrd to simplify the code for y p ~  
and simple; it advances to the next 
input token If its argument f matches the lookahcad symbol. Thus macrh 
changes the variable ikrrheud, which is the currently scanned input token. 
Parsing begins with a call of the procedure for the aarting nonterminal type 
in our grammar. With the same input as in Fig. 2+16, IonkaAtwb is initially 
the first token array. Procedure type executes the code 
corresponding to the right side of the production 
Note thar each terminal in the right side is matched with the lookahead sym- 
bd and that each nonterminal in the right side leads to a all of its p & d u r e .  
Wirh the input of Fig. 2.16, after the tokens array and I are marched. the 
Imkahead symbol is num. At this point procedure simple is called and the 
code 
mu&h (num) ; march (dotdot) ; match ( numl 
in its body is executed. 
The lookahead symbol guides the selection of the pruduction to be used. I f  
the right side of a prduction starts with a token, then the production csn be 
used when the Imkahead symbol matches the token. Now consider n right 
side starting with a nonterminal, as in 
This pruduction is used if the lookahead symbol can be generated from simpk. 
For exampk, during the execution of the code fragrnenl (2.91, suppose the 
lookahead symbl is integer when control reaches the procedure call type, 
There is no prduction for rypc that starts with token integer. However, a 
production for simple does. so production (2+10) is used by having gpt call 
procedure simple on look ahead integer. 
Predictive parsing relies on information about what first symbols can be 
generated by the right side of a production, More precisely, let a be the right 
side of a production for nonterminal A .  We define FIRST(a) to be the set of 
tokens that appear as the first symbols of one w more strings generated from 
a. I f  a is 6 or can generate c, then E is also in  FIRST{^).^ For example, 
, FlRST(simpk] = { integer, char, num 1 
FIRST{ t id) = ( f } 
FlRST{srray [ simpk 1 of type) = { array 
In practice, many production right sides start with tokens, simplifying the 
' Prrdwticms with r on fhc sight sidc complicalc the dctcrrninatirm of thc first y m h h  gcncruwd 
by ;l nonrcrrninal. Fur cxampk, il nmtcrmina'l B can dcrivc thc cmpty string and thcrc is a pru- 
Juctirm A -. BC, thcn the first symbol gcncretcd by C can u L ~ r  bc thc lirst xymbd gcncrard by A. 
If C can also gcncrrrtc r, thcn huh FlRSTIAl a d  FIRSTtBC) contain ti. 

46 A SIMPLE COMPILER 
SEC. 2.4 
construction of FIRST sets, An algorithm for computing FIRST'S is given in 
Section 4.4. 
The FIRST sets must be cmsidered if there are two productions A - u and 
A -. 9. Recursivedescent parsing without backtracking requires FlRST(a) 
and FIRSTIP) to be disjoint. The lookahead symbI can then be used to 
decide which production to use; if the lookahead symbol is in FIRST(a), then 
a is used. Otherwise, if the lookahead symbol is in FIRSTIP), then P is 
used. 
Productions with E on the right side require special treatment. The recursive- 
descent parser will use an E-production as a defauh when no other production 
can be used. For example, consider: 
While parsing opuimis, if the lookahead symbol is not in FIRST(stmt_Iisr), 
then the  production i s  used. This choice is exactly right if the bokahead 
symbol is end. Any lookahead symbol other than end will result in an error, 
detected during the parsing of stmi. 
A prebicrive parser is a program consisting of a procedure for every nontermi- 
nal. Each prcrcedure d a s  two things. 
1. 
It decides which production to use by looking at the lookahead symbol. 
The production with right side, a is used if the lookahead symbol is in 
FIRST(ot). If there is a conflict between two right sides for any look- 
ahead symbol, then we cannot use this parsing method on this grammar. 
A produdion with c on €he right side is used if the lookahead symbol is 
not in the FIRn set for any ather right hand side. 
2. 
The procedure uses a production by mimicking the right side. A nonter- 
rninal results in a call to the procedure for the nonterminal, and a token 
matching the lookahead symbol results in the next input token being read. 
lf at mme point the token in the prduction does not match the look- 
ahead symbol, an error is declared. Figure 2.17 is the result of applying 
these rules to grammar (2.3). 
Just as a translation scheme is formed by extending a grammar, a syntax- 
directed translator can be formed by extending a predictive parser. An algo- 
rithm for this purpose is given in Section 5-5. The following limited construc- 
tion suffices for the present because the translation schemes irnplernen ted in 
this chapter do not assmiate attributes with nonterminals: 
1. 
Construct a predictive parser, ignoring the actions in productions. 

SEC. 2.4 
PARSING 47 
2. 
Copy the aciicins from the translation scheme into the parser. I f  an action 
appears after grammar symbol X in prduction p, then it is copied after 
the code implementing X, Otherwise, if it appears at the beginning of the 
production, then it is copied just before the d
e
 implementing the pro- 
duction. 
We shall construct such a translator in the next section. 
Left Recursiom 
It is possible for a recursivedescent parser to Imp forever. A problem arises 
with kft-recursive productions like 
in which the leftmost symbol on the right side is the same as the nonterminal 
on the left side of the production. Suppose the procedure for expr decides to 
apply this prduction. The right side begins with cxpr so the procedu~e for 
e,yr is called recursively, and the parser Imps forever. Note that the look- 
ahead symbol changes only when a terminal in th; 
righr side is matched. 
Since the production begins with the nonterminal eqw, no changes to the input 
take place between recursive calls, causing the infinite loop. 
Fig+ 2.18. Lcft- and right-rccunivc ways of gcncrating a string. 
A left-recursive production can be eliminated by rewriting the offending 
production. Consider a nonterrninal A with two productions 
where a and Q are sequences of terminals and nonterrninals that do not start 
with A. For example, in 

48 
A slMPLE COMPILER 
SEC. 2.4 
A = expr, u = + m m ,  and p = term, 
The nonterminal A is 
reruniw because the prduction A -. A a  has A 
itself as the leftmost symbol on the right side. Repeated application of this 
production builds up a sequence of a's to the right of A, as in Fig. 2. L8(a). 
When A is finally replaced by 9, we hive a $ followed by a sequence of zero 
or more a's, 
The same effect can k achieved, as in Fig. 2.18(b), by rewriting the pro- 
ductions for A in the following manner. 
Here R is a new nonterminal. The prdudion R -t olR is right rewrsivt 
because this production for R has R itself as the last symbol on the right side. 
Right-recursive productions lead to trees that grow down towards the right, as 
in Fig. 2,18(b). Trees growing dowfi to the Tight make it harder to translate 
expressions containing left-associative operators, such as minus. I n  the next 
seaion, however, we shall see that the proper transhion of expressions into 
p t f i x  notation can still be attained by a careful design of the translation 
scheme based an a right-recursive grammar. 
In Chapter 4, we consider more general forms of kft recursion and show 
how all left recursion can be eliminated from a grammar. 
2,s A TRANSLATOR FOR SIMPLE EXPRESSIONS 
Using the techniques of the last three sections, we now construct a syntax- 
directed translator, in the form of a working C program, that translates arith- 
metic expressions into gastfix form, To keep the initial program manageably 
small, we start off with expressions consisting of digits separated by plus and 
minus signs. The language is extended in the next two sections to include 
numbers, identifiers, and other operators. Since expressions appear as a con- 
struct in so many languages, it is worth studying their t~anslation in detail. 
Fig. 2.19. Initial spification of infix-to-postfix translator. 
A syntaxdirected translation scheme can often serve as the specification for 
a transtator. We use the scheme in Fig. 2.19 (repeated from Fig. 2+13) as the 

SEC. 2.5 
A TRANSLATOR FOR SIMPLE EXPREWONS 49 
definition of the translation to be performed. As is often the case, the under- 
lying grammar of a given scheme has to be modified kfore it can be parsed 
with a predictive parser. In particular, the grammar underlying the scheme in 
Fig. 2.19 is left-recursive, and as we saw in the last section, a predictive 
parser cannot handle a !eft-recursive grammar. By eliminating the Itft- 
recursion, we can obtain a grammar suitable for use in a predictive recursive- 
descent translator. 
Abstract and Cmrete Syntax 
A useful starting pint for thinking abut the translation of an input string is 
an ubstruct syntax tree in which each node represents an operator and the chil- 
dren of the nude represent the operands. By contrast, a parse tree is called a 
rorrcrece synm m e ,  and the underlying grammar is called a concrete .~yitrux 
for the language. Abstract syntax trees, or simply synrsrx rrws, differ from 
parse trees because superficial distinctions of form. unirnptant for transla- 
t ion, do not appear in syntax trees. 
Fig, 2.20. Syntax trcc for 9-5t2. 
For example, the syntax tree for 9- 5+2' is shown in Fig. 2.20. Since + and 
- have the same precedence level, and operators at the same precedence level 
are evaluated left to right, the tree shows 9-5 grouped as a wbexpression. 
Comparing Fig. 2.20 with the correspnding parse tree of Fig. 2.2, we note 
that the syntax tree associates an operator with an interior node, rather than 
making the operator be one of the children, 
It is desirable for a translation scheme to be based on a grammar whose 
parse trees are as close to syntax trees as possible. The grouping of sukx- 
pressions by the grammar in Fig. 2.19 Is similar to their grouping in syntax 
trees, Unfortunately, the grammar of Fig. 2.19 is left-recursive, and hence 
not suitable for predictive parsing. It appears there is a conflict; on the one 
hand we need a grammar that facilitates parsing, on the other hand we need a 
radically different grammar for easy translation. The obvious solution is to 
eliminate the left-recursion. However, this must be done carefully as the fol- 
lowing example shows. 
Elrarnpk 2.9, The following grammar is unsuitable for translating expressions 
into postfix form, even though it generates exactly the =me language as the 
grammar in Fig, 2.19 and can be used for recursivedescent parsing. 

50 
A SlMPLE COMPILER 
This grammar has the problem that the operands of the operators generated 
by rest - + expr and rwf 
- expr are not obvious from the prduchns. 
Neither of the following choices for forming the translation re3t.r from that of 
expr.# is acceptable: 
{We have only shown the production and semantic action for the minus opera- 
tor.) The banslation of 9-5 is 95-, However, if we use the action in (2.12), 
then the minus sign appears before expr-r and 9-5 incorrectly remains 9-5 in 
translation. 
On the other hand, if we use (2. t 3) and the analogous rule for plus, the 
operators consistently move to the right end and 9 - 5 2  is translated 
incorrectly into 952+- (the mrred translation is 95-2+). 
0 
Adapting the Tm&tion 
Scheme 
The left-recursion elimination technique sketched in Fig. 2.18 can also be 
applied to product ions containing semantic actions. We extend the transfor- 
mation in 5ection 5.5 to take synthesized attributes into account. The tech- 
nique transforms the productions A * Aa \ A ft \ y into 
When semantic actions are emkdded in the productions, we w r y  them along 
in the transformation. Here, if we fet A = apr, a = + term {print('+') ), 
p = - term ( print ( - ') ), and y = term, the transformatim above produces 
the translatim scheme (2.14). The eqw productions in Fig. 2.19 have k e n  
transformed into the productions for expr and the new nonterminal rest in 
(2.14). The productions for rerm are repealed from Fig. 2.19. Notice that the 
underlying grammar is different from the one in Example 2.9 and the differ- 
ence makes the desired translation possible. 
resf 
+ term { prim('+') 
rest 1 - term { print('-') } mi 1 E 
term 
0 ( prinr('0') 
) 
term. -. 1 {  prim('^') } 
(2.14) 
Figure 2.21 shows how 9-5+2 i s  translated using the above grammar. 

A TRANSLATOR FOR SIMPLE EXPRESSIONS 5 1 
Fjg. 2-21. Trnnslation of 9-5t2 into 95-2 t. 
Prrrcedures for the Nonterminsls expr, term, and resr 
We now implement a translator in C using the syntax4itected translation 
scheme (2.14). The essence of the translator is the C code in Fig. 2.22 for the 
functions expr, term, and 
rest. These 
functions implement the 
corresponding nonterminals in (2.14). 
term( I 
I 
if (lsdigit(1ookahead~~ 
{ 
putcharIldokahead1; match(1ookahead); 
1 
else error ( 1 ; 
Fig. 2-22. Functions for thc nontcrminuls expr. r e s r , m d  twm. 
The function match, presented later, is the C counterpart of the d
e
 in 

52 
A SIMPLE COMPILER 
SEC+ 2.5 
Fig. 2.17 to match a token with the Iookahead symbol and advance through 
the input. Since each token is a single character in our language, match can 
be implemented by comparing and reading characters. 
For those unfamiliar with the programming language C, we mention the 
salient differences between C and other Algol derivatives such as Pascal, as 
we find uses fur those features of C. A program in C consists of a sequence 
of function definitions. wirh execution starting at a distinguished function 
called main. Function definitions cannot be nested. Parentheses enclosing 
function parameter lists are needed even if there are no parameters: hence we 
w r k  exprI ), term[ ), and rest( } +  Functions communicate either by pass- 
ing para'nietcrs "by value" or by accessing data global to all functions. For 
example, the functions t e r m {  ) and rest l 1 examine the lookahcad symbol 
using the global identifrer lcmkahead. 
C and hscai use the foliowing symbols for assignmknts and equality tests: 
The hncrions fur !he ncmtcrminals mimic the right sides of productions. 
For example. the production cxpr -. term resf is implemented by the calls 
term() and rest l 1 in the function expr l 1. 
As another example, function rest( ) uses the first probudion for rrsr in 
(2.14) if the lockahead symbol is a plus sign, the second prductim if the 
luukahead symbol is a minus sign, and the production rcxt + E by default. 
The first produclion for rexi is implemented by the first if-statement in Fig. 
2.22. I f  the lookahead symbol is +, the plus sign is matched by the call 
match( 't' ). 
After the call term( 1 .  the C standard library routine 
putchart '+' 1 implements the semanlic action by printing a plus character. 
Since the third production for rest has r as its right side, the last else in 
rest I 1 does nothing. 
The ten productions for r u m  generate the ten digits, In Fig. 2.22. the r w -  
tine i s d i g i t  tests if the Iookahead symbol is a digit. The digit is printed 
and matched if the test succeeds; otherwise, an error occurs. (Note that 
match changes the I d a h e a d  symbol, so the printing must wcur before the 
digit is matched.) Before sbowing n complete program, we shall make one 
speed-improving transformation to the code in Fig. 2.22. 
Optimizing the Translator 
Certain recursive calls can be replaced by iterations, When the last statement 
executed in a procedure body is a recursive call of the same procedure, the 
call is said to be toil recwrsiw. For example, the calls of rest I 
1 at the end 
+f the fourth and sevenfh lines of the function rest l 1 are tail recursive 

SIX. 2.5 
A TRANSLATOR FOR SIMPLE EXPRESSIONS 53 
because control flows to the end of the Function body after each of these calls, 
We can speed up a program by replacing tail recursion by iteration. For a 
procedure without parameters, a tail-recursive call can be simply replaced by a 
jump lo the beginning of the procedure. The code for rest can be rewritten 
as: 
rest ( 1 
I 
L: 
if Ilwkahead == ' + ' ) ' {  
match('+'); tcrml); putcharI'+'l; goto L; 
1 
else if (lookahead == '-'I 
match('-'); tern(); putchar('-'); 
got0 L; 
1 
else ; 
1 
As long as the lookahead symbol ts a plus or minus sign, procedure rest 
matches the sign, calls term to match a digit, and repeats the process. Note 
that since match removes the sign each time it is called, this cycle occurs only 
on alternating sequences of signs and digits. If this dmnge is made in Fig. 
2.22, the only remaining call of rest is from expr (see line 3). The two 
functions can therefore be integrated into one, as shown in Fig. 2.23. In C, a 
statement simr can be repeatedly executed by writing 
because the condition 1 is always true. We can exit from a loop by executing 
a break-statement. The stylized form of the code in Fig+ 2,23 allows other 
operators to be added conveniently. 
Fig, 2.23, Rcptaccmcnt for hnct ions expr and rest of Fig. 2.22. 

54 
A SIMPLE COMPILER 
SEC. 2.6 
The mmplete C program for our translator is shown in Fig. 2.24. The first 
line, beginning with #inelude, bads wtype . h> , a file of standard routines 
that contains the d
e
 
for the predicate isdigit, 
Tokens, consisting of single characters, are supplied by the standard library 
routine getehar that reads the next character from the input file. However, 
lookahead is declared to be an integer on line 2 of Fig. 2.24 to anticipate 
the additional tokens that are not single characters that will be introduced in 
later sect ions, Since lookahead i s  declared outside any of the functions, it is 
global to any functions that are defined after line 2 of Fig, 2.24, 
The function match checks tokens; it reads the next input token if the Imk- 
ahead symbol is matched and cakts the error routine otherwise. 
The function error uses the standard library function printf to print ihe 
message "synf ax error" and then terminates execution by the call 
cxi t { '! ) to another standard library function. 
2.6 LEXICAL ANALYSlS 
We shall now add to the translator of the previous section a lexical analyzer 
that reads and converts the input into a stream of tokens to be analyzed by the 
parser. Recall from the definition of a grammar in Section 2,2 that the sen- 
tences of a language consist of strings of tokens. A sequence d input charac- 
ters that wmprises a single token is called a lexeme. A lexical analyzer can 
insulate a parser from the lexeme representation of tokens. We begin by list- 
ing some of the functions we might want a lexical analyzer to perform. 
Removal of White Space and Camnrents 
The expression translator in the last section sees every character in the Input, 
so extraneous characters, such as blanks, will cause it to fail. Many languages 
allow "white space" (blanks, tabs, and newlines) to appear between tokens. 
Comments can likewise be ignored by the parser and translator, so they may 
also be treated as white space. 
if white space is eliminated by the lexical analyzer, the parser will never 
have to consider it. The alternative of modifying the grammar to incorporate 
white space into the syntax is not nearly as easy to implement. 
Anytime a single digit appears in an expression, it seems reasonable to allow 
an arbitrary integer mnstant in its place. Since an integer constant is a 
sequence of digits, integer constants an tK allowed either qy adding produc- 
tions to the grammar for expressions, or by creating a token for such con- 
stants. The job of collecting digits into integers is generally given to a lexical 
analyzer because nurnkrs can be treated as single units during translation. 
Let num be the token representing an integer. When a sequence of digits 

SEC. 2 h  
LEXICAL ANALYSIS 
55 
#include cctypc.h> f *  loads file with predicate i s d i g f t  */ 
int loohhead; 
main( 1 
I 
lookahead + getahar0; 
cxpr ( 1 ; 
putchar I 'In' 1 ; #+ adds trailing newline character */ 
1 
patch( t 1 
i n t  t; 
I 
if ( lookahead == t ) 
lookahead = getchar[ 1 ;  
else error( 1; 
1 
error( 1 
printf('syntax 
error\nml; /+ print error message * f  
exitlll; 
i+ then halt */ 
1 
Fig, 2J4. 
C program to t c a n ~ h t ~  
an infix erprwsion iota postfix form. 

% A SIMPLE COMPILER 
SEC. 2.6 
appears in the input stream, the lexicai analyzer will pass mum to the parser. 
The value of the integer will be passed abng as an attriblrte of the token num. 
Logically, the lexical analyzer passes both the token and the attribute to the 
parser. If we write a token and its attribute as a tuple endo& 
between < > , 
the input 
is transformed into the sequence of tuples 
The token + has no attribute. The semd components of the tuples, the attri- 
butes, play no role during parsing, but are needed during translation. 
Recognizing Identifirs and Keywads 
Languages use identifiers as names of variables, arrays, functions, and the 
tike. A grammar for a ianguage often treats an identifier as a token, A 
parser based on such a grammar wants to see the same token. say id, each 
time an identifier appears in the input. For example, the input 
would be converted by the ) e x i d  analyzer into the token stream 
This token scream is used for parsing. 
When talking about the Le~ical analysis of the input line (2.13, it is useful 
to distinguish between the token id and the Lexernes count and increment 
aswiated with instances of this token. The translator needs to know that the 
lexeme count forms the first two instances of id in (2.16) and that the kx- 
erne increment forms the third instance of id, 
When a lexerne forming an identifier is seen in the input, some mechanism 
is needed to determine if the lexeme has k e n  seen before. As mentioned in 
Chaprer I ,  a symM tabk is used as such a mechanism. The lexeme is stored 
in the symbd table and a pointer to this symbol-table entry becomes an attri- 
bute of the token id. 
Many lanpuages use fixed character strings such as begin, end, if, and so 
on, as punctuation marks or to identify certain constructs. These character 
strings, called keywords, generally satisfy the ruIes for forming identifiers, so a 
mechanism is needed for deciding when a heme forms a keyword and when 
it forms an identifier. ~ h c  
problem is easier to resolve if keywords arc 
reserved. i-e., if they cannot be used as identifiers. Then a character string 
forms an identifier only if it is not a keyword, 
The problem of isdating tokens a h  arises cf the same characters appear in 
the lexemes of more than one token, as in c, <=, and *> in Pascal. Tech- 
niques for recognizing such tokens efficiently are discussed In Chapter 3. 

When n lexical analyzer is inserkd between the parser and the input stream, it 
interacts with the two in the manner shown in Fig. 2.25. It reads characters 
from the input, groups them into lexernes, and passes the tokens formed by 
the kxemeu, together with their attribute values, to the later stages of the 
compiler. In some situations, the lexical analyzer has 10 rcad some charaaers 
ahead before it can decide OII the token to be returned to the parser. For 
example, a lexical analyzer for Pascal must read ahead after it sees the charac- 
ter >. If the next characlcr is -, then the character sequence > = is the lexeme 
forming the token for the "greater than or equal to" oprator. Otherwise 3 is 
the h e m e  forming the '+greater than" operator, and the lexical analyzer has 
read one character too many. The extra character bas to be pushed back onto 
the input, because it can be the beginning of the next lexeme in the input. 
Fig. 2-25 Inserting a lcxicat analyzcr bcbwcn thc input and thc parscr. 
rcad 
pas 
token and 
The lexical analyzer and parser form a prd~cer~omurner 
pair. The kxical 
analyzer produces tokens and the parser consumes them. Produced tokens can 
be held in a token buffer unt ii they are consumed. The inleraction between 
the two is cunstrained only by the size of the buffer, bemuse the lexical 
analyzer cannot proceed when the buffer is full and the parser cannot proceed 
when the buffer is empty, Commonly, the buffer holds just one token. In 
this case, the interaction can be implemented simply by making the kxical 
analyzer be a procedure called by the parser, returning tokens on demand. 
The implementation of reading and pushing back characters i s  usually done 
by setting up an input buffer. A block of characters is read into tht buffer at 
a time; a pointer keeps track of the portion of the input that has been 
analyzed, Pushing back a character is jrnplemmtcd by moving back the 
pointer. Input characters may a
h
 need to b saved for error reporting, since 
some indication has to be given of where in the input text the error occurred. 
The buffering of input characters can be justified on efficiency grounds alone. 
Fetching a block of characters is usually more efficient than fetching one char- 
acter at a time, Techniques for input buffering are discussed in Section 3.2. 
lexical 
analy zcr 
its aitrihtes - 
A 
parm 

58 
A SIMPLE COMPILER 
A Lexical Analyzer 
We now construct a rudimentary lexical analyzer for the expression translator 
of S e c t h  2.5. The purpose of the lexical analyzer is to allow white space and 
numbers to appear within expressions. In the next section, we extend the lexi- 
cal analyzer to allow identifiers as well, 
uses getchar I 
t- 
rcturns tokcn 
to read character 
1txanI 1 
to c a l h  
pushw back c using 
analyxr 
ungetc I 
c , stdin 1 
scts global variable 
to attribute valuc 
Fig. 2.26. 
lmplemcnting thc intcractiuns in Fig, 2.25. 
Figure 2.26 suggests how the lexical analyzer, written as the function 
lexan in C, 
implements the interactions in Fig, 2+25. The routines getchar 
and ungttc from the standard include-file cstdio, hr take care of inpul 
buffering: lcxan reads and pushes back input characters by calling thc m u -  
tines getchar and ungetc, respectively. With c declared to tK a character, 
the pair of statements 
leaves the input stream undisturbed. The call of getchar assigns the next 
input character to c; the call of ungetc pushes back the value of c onto the 
standard Input stdin, 
I f .  the implementation language does not allow data structures to be 
returned from functions. then tokens and their attributes have to be passed 
separately+ The function lexan returns an integer 'encoding of a token. The 
token for a character MR 
be any conventional integer encoding of that charac- 
ter, A token, such as nlrm, can then be encoded by an integer larger than any 
integer eowding a character, say 256. To allow the encodkg to be changed 
easily, we use a symbolic constant HUM to refer to the integer encoding of 
m m .  In Pascal, the asswiatirsn ktween NUM and the e n d i n g  can be done 
by a m
~
t
 
declaration; in C, W M  can be made to stand for 256 using a 
define-statement: 
Xdef ine NUM 256 
The function lexan returns NUM when a sequence of digits is seen in the 
input. A global variable tokenval is set to the value of the sequence of 
digits. Thus, if a 7 is foliowed immediately by a 6 in the inpur, tokenval is 
assigned the integer value 76. 

SEC. 2.6 
LEXICAL ANALYSIS 59 
Allowing numbers within expressions requires a change in the grammar in 
F I ~ .  
2.19. We replace the individual digits by the nonterminal factor and 
introduce the fdbwing productions and semantic actions: 
The C cude for facm in Fig. 2.27 is a direct implementation of the pruduc- 
tims above, When lookahead equals HUM, the value of attribute num.vuhc 
is given by the global variable tokenval. The action of printing this value is 
done by the standard library function print€. The first argument of 
printf is a string between double quotes specifying the format to be used for 
printing the remaining arguments;. Where BhB appears in tbe string, the 
decimal representation of the n&t argument is printed. Thus, the printf 
statement in Fig, 2.27 prints a blank followed by the decimal representation of 
t okenval followed by another blank. 
Fig. 2.27. C d
c
 
for Jaiwr when opcrands can be numbers. 
The implementation of function l c x m  is shown in Fig. 2.28. Every time 
the body of the while statement On lines 8-28 is executed, a character i s  read 
irito t on line 9. If the character i s  a blank or a tab (written *\tr), 
then no 
token is returned to the parser; we merely go around the while Imp again. If 
the character is a newline (written 'b'), 
then a global variable lineno is 
incremented, thereby keeping track of line numbers in the input, h i  again no 
token is returned. Supplying a line number with an error message helps pin- 
point errors. 
The wde for reading a sequence of digits is on lines 14-23. The prdicale 
i s d i g i t  It ) from the include-file qetype. hr is used on lines !4 and I7 to 
determine if an incoming character t is a digit, If it is, then its integer value 
is given by the expression t-' 0 ' in both ASCll and EBCDIC. With other 
character sets. the conversion may need to be done differently. In Section 
2.9, we incorporate this lexical analyar into our expression translator. 

60 A SIMPLE COMPILER 
SEC. 2.6 
I I) #include <stdio. hr 
(2) #include cctype. hr 
(3) int lineno = -I ; 
(4) i n t  tokenval = NONE; 
i n t  t; 
while(?) { 
t = getchar ( 1 ; 
if (1 5 =  ' ' I '  
I I t =a 'it') 
; 
/ r  strip out blanks and tabs */ 
else if (t == 'kn' 1 
lineno = lintno + 1; 
else if lisdigit(tl1 i 
tokenval = t - ' 0 ' ;  
t = getchari); 
while (isdigitIt11 { 
tokenval = tokcnval*lO + t - ' O r ;  
t = getchar(); 
1 
UAgctcIt, stdin); 
return HUM; 
1 
else { 
tokenval = NONE; 
return t; 
1 
1 
Fig. 2.28, C codc for Lcxiwl analyzcr eliminating whirc spacc and mllcding numbcrs. 
2.7 INCORPORATING A SYMBOL TABLE 
A data structure called a symbol table is generally used to store information 
about various source language constructs. The information is collected by the 
analysis phases of the compiler and used by the synthesis phases to generate 
the target code. For example, during lexical analysis, the character string, or 
lexeme, forming an identifier is saved in a symbol-table entry. Later phases 
of the compiler might add to this entry information such as the type of the 
identifier, its usage le.g., pradure, variable, or label), and its position in 
storage. The code generation phase would then use the information to gen- 
erate the proper code to store and access this variable. In Section 7+6, 
we dis- 
cuss the implementation and use of symbol tables in detail. In this seclion, we 

illustrate how the lexical analyzer of the previous section might interact with a 
symbol table. 
The Symbol-Table Interface 
The symbol-table routines are concerned primarily with saving and retrteving 
lexemes. When a Le~erne is saved, we also save the token associated with the 
lexeme. The following operations will be performed On the symbol table. 
insert{ s , t 1: 
Returns index of new entry for string s, token t 
lookupI s 1: 
Returns index of the entry for string s,. 
or 0 if s is not found. 
The lexical analyzer uses the lookup operatson to determine whether there is 
an entry for a h e m e  in the symbol table. If no entry exists, then it uses the 
insert operation to create one. We shall discuss an implementation in which 
the lexical analyzer and parser both know about the format of symbol-table 
entries. 
Handling Reserved Keywords 
The symbol-table routines above can handle any collection of reserved key- 
words. For example, consider tokens div and lnod with lexernes d i v  and 
mod, rexpecti~ely. We can initialize the symbol table using rhe calk 
insert ( "div" , div 1 ;- 
insert{"modar mad); 
Any subsequent call lookup{ "div" 1 returns the token div, so div cannot 
be used as an idcn t ifier . 
Any collection of reserved keywords can be handled in this way by 
appropriately initializing the symbol table. 
The data structure for a particular implementation of a symbol table is 
sketched in Fig. 2.29. We do not wish to set aside a fixed amount of space to 
hold kxemes forming identifiers; a fixed amount of space may not be large 
enough to hold a very lung identifier and may k wastciully Large for a short 
identifier, such as i. In Fig. 2.29, a separate array lexemes holds the char- 
acter string forming an identifier. The string is terminated by an end-of-string 
character, denoted by EOS, that may not appear in identifiers. Each m i r y  in 
the symbol-table array symtable is a record consisting of two fields, 
lexptr, pointing to the beginntng of a lexeme, and token. Additional fields 
can hold attribute values, although we shall not do su here. 
In Fig. 2.29, the 0th entry is kfi empty, because lookup returns O to indi- 
cate that there is no entry for a string. The 1st and 2nd entries are for the 
keywords div and mod. The 3rd and 4th entries are for identifters count 
and i. 

62 
A SIMPLE COMPILER 
ARRAY symtable 
lexptr 
token 
attributes 
I
l 
a 
div 
- 
m
d
 
M 
id 
PI. 2.29. Symbol tablc and array for storing strings. 
Pseudwode for a lexical analyzer that handles identifiers is shown in Fig. 
2.30; a C implementation appears in Section 2.9. White space and integer 
constants are handled by the lexical analyzer in the same manner as in Fig. 
2.28 in the last section, 
When our present lexical analyzer reads a letter, it starts saving Rtters and 
digits in a buffer lexbuf. The string collected in lexbuf is then lmktd up 
in the symbol table, using the lookup operation. Since the symbl table is 
initialized with entries for the keywords div and H, 
as shown in Fig. 2.29, 
the lookup operation will find these entries if lexbuf contains either div or 
mod. If there is no entry for the string in lexbuf. i.e., lcmkrrp returns 0, 
then lexbuf contains a lexeme for a new identifier. An entry for the new 
identifier is created using insert. After the insertion is made, p is the index 
of the symbol-table entry for the string in lexbuf, This index is communi- 
cated to the parser by setting tokenval to g, and the token in the token 
field of the entry is returned. 
The default action is to return the integer e n d i n g  of the character as a 
token. Sncc the single character tokens here have no attributes, tokenval is 
,Wt to NONE. 
2.8 ABSTRACT STACK MACHlNES 
The front end of a compiler constructs an intermediate representation of the 
source program from which the back end generates tbe target program. One 
popular form of intermediate representation is code for an abslract stack 
machine. As mentioned in Chapter 1, partitioning a compiler into a front end 
and a back end makes it easier to modify a compiler to run on a new machine. 
In this section, wt present an abstract stack machine and show how d e  

ABmRACT MACHINES 63 
fuwth kmtt: integer; 
w lexbd : 
army 10. - 1001 d char; 
C :  
char; 
w
n
 aoopw 
read a character into c; 
if c is a blank or a tab then 
do nothing 
el* If c is a newline then 
!inem := Ilneno + I 
else if c is a digit t
h
 
begin 
xt k n v a l  to the value of this and following digits; 
return NWM 
&e if c is a letter then b i n  
place c and successive letters and digits id0 kxbuf; 
p := h k r r p I i x x b ~ ) ;  
ifp=Oth#n 
p := i n ~ t ~ i ~ k ~ ~ .  
m); 
10ketl~d := p+ 
rtturn  he ~Dkm field of table entry p 
end 
e k  hgir 
token is a single character */ 
set rokdval to ~~; 
i r :  there is no attribute */ 
return integer e n d i n g  of character c 
end 
d 
tm! 
Fig. 2,N. Pscudo-cde for a kxical analyzer. 
can be gtnerated for it. The machine has separate instruction and data 
memories and all arithmetic operations are performed on values on a stack, 
The instrudions are quite limited and fall into three classes: integer arith- 
metic, stack rnanipu!ation, and control flow. Figure 2.32 illustrates the 
machine. The pointer pc indicates the instruction we are abut to execute. 
The meanings of the instructions shown will be discussed shortly. 
The abstraa machine must implement each operator in the intermediate 
language. A basic operation, such as addition or subtraction, is supprted 
directly by the abstract machine. A mre complex operation, however, may 
nted to tK implemented as a sequence of abstract machine instructions. We 
simplify the dtscriprion of the machine by assuming that  here is an 

64 
A SIMPLE COMPILER 
Fig. 2.31. Snapshot of the slack rnachiae after the first four instructions arc cxecutcd. 
instruction for each arithmetic operator, 
The abstract machine cude for an arithmetic expression simulates the 
evaluation of a postfix representation for that expression using a stack. The 
evaluation proceeds by prmssing the pxtfix representation from left to right, 
pushing each operand onto the stack as it is encountered. When a k-ary 
operator is encountered, its leftmost argument Is k - I positions below the top 
of the stack and its rightmost argument is at the top. The evaluation applies 
the operator to the top k values on the stack, pops the operands, and pushes 
the result onto the stack. For example, in the evaluation of the postfix expres- 
sion I 3 + 5 *, the following actions are performed: 
1 .  
Stack 1. 
2. 
Stack 3. 
3. Add the two topmost elements, pop them, and stack the result 4. 
4. 
Stack 5 .  
5 .  Multiply the twq topmost elements, pop them, and stack the result 20. 
The value on top of the stack at the end (here 20) is the value of the entire 
expression. 
In the intermediate language, all values will be integers, with 0 correspond- 
ing to false and nonzero integers carresponding to true. The bwlean 
operators and and or require both their arguments to be evaluated. 
There is a distinction between the meaning of identifiers on the left and right 
sides of an assignmen1 . In each of the a~qignments 
the right side specifies an integer value, while the kft side specifics where the 
value is to be stored. Similarly, if p and q are pointers to characters. and 

SEC. 2.8 
ABXRACT MACHINES 65 
the right side qt specifies a character, while pf specifies where the character 
is to be stored. The terms !-vahe and r-value refer to values that are 
appropriale on the kft and right sides o l  an assignment, respectively. That is, 
r-values are what we usually thirtk of as "values," while I-values are Iocations. 
S-k 
Manipulation 
Besides the obvious instruction for pushing an integer constant onto the stack 
and popping a value from the top of the stack, there are instructions to access 
data memory: 
push v 
push v onto the stack 
rualue l 
push contents of data location I 
lvalue I 
push address af data location f 
POP 
throw away value on top of the stack 
: = 
the r-value on top i s  placed in the l-value k b w  it 
and both are p y p d  
eoPY 
push a copy of the t q  value on the stack 
Ttanshtim of Expressions 
Code to evaluate an expression on a stack machine i s  closely related to p s t f i x  
notation for that expression. By definition, the postfix form of expression 
E + F is the concatenation of the postfix form of E, the postfix form of F ,  
and + . Similarly, stack-machine d
e
 
to evaluate E i- 
F is the concatenation 
of the code to evaluate E, the d
e
 
to evaluate F, and the instruction to add 
their values. The translation of expressions into stack-machine code can 
therefore k done by adapting the translators in Sctions 2.6 and 2.7. 
Here we generate stack d
e
 for e~pressions in which data Iwatims are 
addressed symbolically. (The allocation of data locations for identifiers is dis- 
cussed in Chapter 7.) The expression a+b translates into: 
rvalut a 
rvalue b 
+ 
In words: push the contents of the data locations for a and b onto the stack; 
then pop the top two values on the stack, add them, and push the result onto 
the stack. 
The transhtion of assignments into stack-machine code is done as follows: 
the I-value of the identifier assigned to is pushed onto the stack, the expres- 
sion is evaluated, and its r-value is assigned to the identifier. For example, 
the assignment 
day := I146q+y) d i v  4 + Il53*m + 2 )  div 5 + d 
12-17) 
translates into the axle in Fig, 2.32, 

66 A SIMPLE COMPILER 
lvalue day 
guah 1461 
rvalue y 
* 
push 4 
div 
push 153 
rvalue m 
* 
push 2 
+ 
push 5 
div 
+ 
rvalue d 
+ 
: = 
Fig. 2.32. Translation of day : = ( 146 1 c y  ) div 4 + t 1 S3*m + 2 1 div 5 + d 
These remarks can be expressed formalty as follows. Each nonterminal has 
an attribute t giving its translation. Attribute k e r n  of id gives the string 
representation of the identifier . 
Control Flow 
The stack machine executes instructions in numerical sequence unless told to 
do otherwise by a mditional or unconditional jump statement. Several 
options exist for specifying the targets of jumps: 
I, The instruction operand gives the target ication. 
2. 
The instruction operand specifies the relative distance, positive or nega- 
tive, to be jumped. 
3. 
The target is spcified symbolically; ix., the machine supprts labels. 
With the first two options there is the additional possibility of taking the 
operand from the t
q
 of the stack. 
We choose the third option for the abstract machine because ii is easier to 
generate such jumps. Moreover. symbolic addresses need not be changed if, 
after we generate code for the abstract machine, we make certain improve- 
ments in the d
e
 
that result In the insertion or deletion of instructions. 
The control-flow instructions for the stack machine are: 
label l 
target of jumps to E; has no other effect 
gota I 
next instruction is taken from statement with label 1 
gof alse i 
pop the top value: jump if it is zero 
gotrue l 
pop the top value; jump if it is nonzero 
halt 
stop execution 

ABSTRACT MACHINES 67 
The layout in Fig. 2.33 sketches the abstract-machine code for conditional and 
while statementfi. The fdlowing discussion concentrates on creating labels. 
Consider the code layout for if-statements in Fig. 2.33, There can only be 
one label o u t  instruction in the translation of a source program; otherwise, 
there will be confusion a b u l  where control flows to from a goto out state- 
ment. We therefore need some mechanism for consistently replacing out in 
the code layout by a unique label every time an if-statement is translated. 
Suppose newlabel is a procedure that returns a fresh label every time it is 
called. In the following semantic action, the label returned by a call of newla- 
be1 is r k r d e d  using a lwal variable our: 
gofalse out 
L===l 
I 
d
c
 
for srmr, I 
l a k l  out I 
gofalse out 
Fig. 2.33. Code layout for conditional and while staterncnts. 
Emitting a Translatim 
The expression translators in Section 2.5 used print statements to inmemen- 
tally generate the translation of an expression. Similar print statements can be 
used to emit the translation of statements. Instead of print statements, we use 
a procedure emit to hide printing details. For example, emit can worry about 
whether each abstract-machine instruction meeds to be on a separate line. 
Using the procedure emit, we can write the following instead of (2,18): 
emif('label', out); 1 
appear within a production, we consider the elements 

68 
A SIMPLE COMPILER 
SEC. 2.8 
on the right side of the production in a left-to-right order, For the above pro- 
duction, the order of actions is as follows: actions during the parsing of expr 
are done, out is set to the lab1 returned by newfdwb and the gofalse 
instruction is emitted, actions during the parsing of simt are done, and, 
finally, the label instruction i x  emitted. Assuming the actions during the 
parsing of expr and stmt I emit the code for these nonterminals, the above pro- 
duction implements the d
e
 
layout of Fig. 2.33. 
Fig. 234. Pscicudo-mdc for translating statcmcnts. 
Pseudo-code for translating assign rnent and conditional statements is shown 
in Fig. 2.34. Since variable orrt is local to procedure sirpot, its value is not 
affected by Ihe calls to procedures expr and .~#ms. The generation of labels 
requires some thought. Suppose that the labels in the translation are of the 
form L?, L2, . . . . The pseudo-code manipulates such I a M s  using the 
integer following L. Thus, out is declared to be an integer, newlabel returns 
an integer that becomes the value of out, and emit must be written to print a 
label given an integer. 
The d e  
layout for while statements in Fig. 2.33 can be converted into 
code in a similar fashion. The translation of a quence of statements is sim- 
ply the conatenation of the statements in the sequence, and is left ro the 
reader. 
The translation of most single-entry singleexit construns ic similar lo that 
of while statements. We illustrate by considering control flow in expressions. 
Example 2.10. The lexical analyzer in Section 2.7 contains a conditional of 

PUTTING THE TECHNIQUES TOGETHER 69 
the form: 
if r = blank or i - tab then . 
- 
If r is a blank, then clearly it is not necessary to test if r is a tab, because the 
first equality implies that the condition is true. The elrpression 
can therefore be implemented as 
if expr 
then true dse rxprz 
The reader can verify that the following code implements the or operator: 
code for rxpr 
COPY 
1, copy value of cxpr 
+/ 
gotrue out 
POP 
/* pop value of cxprl */ 
code for expr2 
label 
o u t  
Recall chat the gotrue and gofalse instructions pop the value on top of the 
stack to simplify code generation for conditional and while statements, By 
copying the value of cxpr I we ensure that the value on top of the stack is true 
if the gotrue instruction leads ti, a jump. 
2.9 PUTTING THE TECHNIQUES TOGETHER 
In this chapter, we have presented a number of syntaxdirected techniques for 
constructing a compiler front end. To summarize these techniques. in this 
section we put tugether a C program that functions as an infix-to-postfix trans- 
lator for a language consisting of sequences of expressions terminated by serni- 
colons. The expressions consist of numbers, identifiers, and the operators +, 
-. +, 1, div, and mod, The output of the translator is a postfix reprmeienta- 
tion for each expression. The translator is an extension OI 
the programs 
developed in Sections 2.5-2.7. A listing of the complete C program i s  given at 
the end of this section. 
Description d the Translator 
The translator i s  designed using the syntax-directed translation scheme in Fig. 
2.35. The token id represents a noncmpty sequence of letters and digits 
beginning with a letter, nurn a sequence of digits, and eof an end-of-file char- 
acter. 
Tokens are separated by sequences of blanks, tabs, and newlines 
("white space"). 
The attribute lexemt of the token id gives the character 
string forming the token; the attribuk vdw of the token num gives the 
integer represenkd by the mum. 
The code for the translator is arranged into seven modules, each stored in a 
separate file. Execution begins in the rncdute main. e that consists of a call 

70 
A SIMPLE COMPILER 
Fig. 2.35. Spxification for infix-to-postfix translator. 
infix cxprcsr;ions 
FI. 
2.36. 
M d u  lcs of infix-to-postfix translor or. 
I 
I 
+ 
, 
, , , . . . - . - . - , . 
, 
to i n i t  I 1 for initialization followed by a call to parse I 1 for the translation. 
The remaining six modules are shown in Fig, 236. There is also a global 
header file global .h that contains definitions common to more than one 
module; the first statement in every niodule 
symbol. c 
causes this header file to be included as part of the module. Before showing 
the code for the translator, we briefly describe each module and haw it was 
constructed. 
C 
A 
-  parser.^ - error c 

SEC. 2.9 
PUlTlNG THE TECHNIQUES TOGETHER 7 1 
The lexical analyzer is a routine called lexanI ) that is called by the parser to 
find tokens. Implemented from the p u d o d e  in Fig. 2,30, the routine 
reads the input one character at a time and returns to the parser the token it 
found. The value of the attribute associated with the token is assigned to a 
global variable tokenval . 
The- tblbwing tokens are expected by the parser: 
Here ID represents an identifier, NUM a number, and DONE the end-of-tila 
chara~er. White space is silently stripped out by the lexical analyzer, The 
table in Fig. 2-37 shows the token md attribute value p r o d u d  by the lexical 
analyzer for each soure language lexeme. 
Wnence of digits .......... I 
( numeric value of squcncc 
div ............................ 
LEXEME 
.................. 
white space 
Fig. 137. Description of tokcns. 
TOKEN 
othcr scguences of a letter 
then letters and digits ..... 
cnd-of-file character ....... 
any at her character ........ 
The lexical analyzer uses the symbol-table routine lookup to determine 
whether an identifier h e m e  has been previously seen and the routine 
insert to dore a new laerne into the symbd table. It also increments a 
global variable lineno every time it sees a newline character. 
ATTRIBUTE VALUE 
The Parser Module parser . c 
ID 
K w E  
that character 
The parser is constructed using the techniques of Section 2.5. We first elim- 
inate Left-recursion from the translation scheme of Fig. 2.35 so that the under- 
lying grammar can be parsed with 
a recurshedescent parser. 
The 
transformed scheme is shown in Fig. 2.38. 
We then construct functions for the nonterminals arpr, term, and factor as 
we did in Fig. 2.24. The function parse4 ) impkments the start 'symbol of 
the grammar; it calls lrxan whenever it needs a new token, The parser uses 
the function emit to generate the output and the function exror to report a 
syntax error. 
index into amtable 
NONE 
- 
.. 

72 
A SIMPLE COMPlLER 
Fig. 2.38. Sy ntax-directed translation scttcme after eliminating lcft-recursion. 
The Emitter Module emitter. c 
The emitter module consists of a single function e m i t  It , tval 1 that gen- 
erates the outpul for token t with attribute value tval. 
The Symbd-Table Moduks symbol. e a d  init. c 
The symbol-table module symbol. c implements the data structure shown in 
Fig. 2.29 of Section 2.7. The entries in the array symtable are pairs consist- 
ing of a pointer to the lexemoa array and an integer denoting the token 
stored thew. The operation insert I o , t 1 returns the symtable index for 
the lexemc s forming the token t. The function lmkupls) returns  he 
index of the entry in symtable for the lexerne s or Q if s is not there. 
The module i n i t  .c is used to prebad symtable with keywords. The 
lexerne and token representations for all the keywords are stored in the array 
keywords, which has the same type as the symtabla array, The funaim 
init{ ) gws sequentially through the keyword array, using the function 
insert to put the keywords in the symbol table. This arrangement albws us 
to change the representation of the tokens for keywords in a convenient way. 
The Errw Module error. c 
The error module manages the error reporting, which is extremely primitive. 
On encountering a syntm error, the compiler prints a message saying that an 
error has m r r e d  on the current input line and then halts. A better ewm 
recovery technique might skip to the next semimion and wntinue parsing; the 

SEC. 2.9 
PUTTLNG THE TECHNIQUES TOGETHER 
73 
reader is encouraged to make this modification to the transIator. More 
sophisticated error recovery techniques are presented in Chapter 4. 
The c d e  for the modules appears in seven files: lexer. c, parser. c, 
emitter + e, symbol. c, i n i t .  c ,  error. e, and main. c. The file main. c 
contains the main routine in the C program that calls i n i t  ( 1, then 
parse ( 1 ,  and upon successful completion exit I 0). 
Under the UNlX operating system, the compiler can be created by execut- 
ing the command 
or by separately compiling the files, using 
and linking the resulting @lemme. o files: 
The ec commandcreates a file a.out that contains the translator. The trans- 
lator can then be exercised by typing a. out followed by the expressions to be 
translated; e.g., 
2+3*5; 
72 div 5 m o d  2; 
w whatever other expressions you like. Try it. 
The Listing 
Here is a listing of the C program implementing the translator. Shown is the 
global header file global.h, followed by the seven source files. For clarity, 
the program has been written in an elementary C style. 
#include <stdio.hr 
load i / o  routines */ 
#include sctype. h* 
/+ load character t e s t  mutints */ 
Int tokenval; 
/+ 
value of token attribute +/ 

int lineno; 
struct entry I 
form of symbol table entry */ 
char *lexptr; 
i n t  token; 
I ;  
struet entry symtable E 1; 
f * 
symbol table *f 
Xinclude "globa1.hm 
char lexbuf [BSIZE]; 
int lineno 
1; 
int tokenval = WNE; 
int lexan0 
1 
lexical analyzer */ 
1 
i n t  t; 
whileIl) { 
t = getchar( I ;  
if ( t  i= 
' ' " 
a 
4 k z m  *\t*) 
else 
else 
1 
else 
1 
; /x 
strip out white space */ 
if [ t  += 'h*) 
lineno = lineno + 1 ;  
i f  IisBigitIt)) 
/+ t is a d i g i t  ./ 
ungetc[t, stdin); 
seanf("%dn, dtokenvall; 
return MIM; 
if Iisalpha4tl) I 
/+ t is a letter */ 
int p, b = 0 ;  
while IisalnumIt)) 4 
l* t i $  alphaameric *f 
lexbuflbl = t; 
t 
getchar E 1 ; 
b = b + A ;  
if Ib >= BSIZE) 
errarlncmpiler erxorn); 
t 
laxbuf[b] = EOS; 
i f  (t I =  EDF) 
ungetc ( t , stdin) ; 
p = lookup[lexbuf 1; 
if { p  'f 
0 )  
p = insert(lexbuf, ID); 
toktnval = p; 
return symmblt[pl.token; 
else i f  It == EOP) 
return DONE; 

PUTTING THE TECHNIQUES TOGETHER 75 
else I 
tokonval = NONE; 
return t; 
1 
8 
i n t  kdcahead ; 
parse [ 1 
I* parses and translates expression list */ 
I 
lookahcad = lexanI ) ; 
while (lookahead I =  DONE 1 I 
sxgrI1; matchI';'l; 
1 
1 
term( l 
{ 
int t; 
factor l 1; 
while{ 1) 
switch [lookahaadl { 
case '+': 
case '1': cast DIV: case MOD: 
t = hmkaheatt; 
mstehIlookahead); factor( 1; e m i t t t ,  NONEI; 
continue ; 
default : 
return ; 
1 

SEC. 2.9 
f astor l l 
I 
switch(lookahcad1 I 
case ' I ' : 
match('[']; expr(1; match['l'l; 
break; 
case NUH: 
emitINUM, tokenval); match(NUM1; break; 
case ID: 
emitIID, tokenvall; m t c h I D 1 ;  break; 
dtf ault : 
error I '-tax 
error" 1 ; 
match [ t 1 
i n t  t; 
E 
if (lookahead == t )  
lookahead + 1txanI 1; 
else arrorl'syntax 
error" 1; 
1 
emit(t, tval} 
/* 
generates output *i 
i n t  t, tval; 
I 
switchIt) E 
case '+' - . case ' - ' , case '*' : case '1' : 
printf[*Xc\nu, t); break; 
case DIV: 
printf I 'DIV\nC 1 ; break; 
caoe MOD: 
printfIUMOD\n'); break; 
ease HUM: 
printf I "%d\nlt, t v a l )  ; break; 
case ID: 
printf[m%s\n*, symtableitvall,lexptr); break; 
default : 
grintflwtoksn %d, tokcnval %d\nM, t, tval]; 
1 
1 
#define STRMAx 999 
/* s i z e  o f  lexemee array 
t/ 
#define SYMMCLX 100 
/+ size of symtabla */ 

SEC. 2.9 
PUTTING THE TECHNIQUES TOGETHER 77 
char lexemes[STRMAX 1 ; 
int lastchar = - 1; 
I* last used p ~ s i t i ~ n  
in lexemes * I  
rtruet entry s y m t a b l e [ S Y ~ 1 ;  
int lastentry = 0 ;  
i* last used position in symtable *I 
i n t  1ookupIsI 
1, returns position of entry for s */ 
char 3 1 3 ;  
{ 
i n t  p; 
far (p = lastentry; p r 0 ;  p = p - 1 )  
if (strcrnpIsptablelp].lcxptr~ 3) == 01 
return p; 
return 0 ; 
1 
int insertIs, tok) ,J'* returns position of entry for s *,' 
char s [ l ;  
i n t  tok; 
I 
int len; 
len = etrlenIa1; I* strlen computes length of s +I 
if (lastentry + 7 >= SYHMU) 
errorInsymbd table full" 1; 
if [lastchar + 1en + 1 r =  5TRMAX) 
error I "lexemas array full" ) ; 
lastentry = lastentry + 1; 
symtablellastentry3.token = tok; 
sptablellastentryJ.1exptr - ilexemesllastchar + 11; 
lastchar = lastchar + len + 1 ; 
strcpyIsymtable€lastentry].ltxptx, d l ;  
return Lastentry; 
1 
s t r u c t  entry keywords[] = { 
"div" , PfY, 
*moU1', MOD, 
0 ,  
0 
1 ; 
i n i t  I 1 
/ x  
loads keywords into aptable +l 
I 
struct entry * g ;  
for ( p  = keywords; p-rtoken; p++l 
insert ( p-rlexptr , p->token 1 ; 
I 

78 
A SlMPLE COMPILER 
SEC. 2.9 
t r r Q X  [ m ) 
f +  generates all error marages *i 
char *m; 
{ 
fprintfIstderr, 'line %d: %s\nh, lineno, m ) ;  
exit( 11; 
i 
unruccessful termination */ 
1 
main( l 
I 
i n i t (  1; 
parset 1 ; 
exit(0); 
{x 
successful ttrminstion +/ 
, 
1 
EXERCISES 
2.1 Consider the context-free grammar 
S + S S +  I S $ *  l a  
a) Show how the string aa+a* can be gtnaattd by this grammar. 
b) Construct a parse tree for this string. 
C) What language is generated by this grammar? Justify your 
answer. 
2.2 What language is generated by the following grammars? In each case 
justify your answer. 
a ) $ -  
O S  1 1 0  7 
b) $ 4  + S S  I - S S \  a 
E
)
$
~
 
$ ( 5  ) S i  E 
d ) S + a S b S  I b S a S I r  
e > S + a l s  + s ~ s s ~ S  
* I  ( S  1 
2.3 Which of the grammars in Exercise 2.2 are ambiguous? 
2.4 Construct unambiguous context-free grammars for each of the follow- 
ing languages. In each cage show that your grammar is mrrect, 

CHAPTER 2 
a) Arithmetic expressions in *stfix 
notation, 
b) Left-associative lists of identifiers separated by commas. 
C) Right-assdative lists of identifiers separaied by commas, 
d) Arithmetic expressions of integers and identifiers with the four 
binary operators +, -, *, /. 
e) Add unary plus and minus to the a~ithmetic operators of (d)+ 
*2,5 
a) %ow that all binary strings generated by the following grammar 
have values divisible by 3. Hint. U s e  induction on the number of 
nodes in a parse tree. 
num 
11 1 
1001 1 num O 
1 num num 
b) Does the grammar generate all binary strings with values divisibk 
by 31 
2.6 Construct a context-free grammar for roman numerals. 
2.7 Construct a syntax-birec~ed translation scheme that translates arith- 
metic expressions from infix notation into prefix notation in which an 
operator appears before its operands; e.g,, -xy is the prefix notation 
for x-y. 
Give annotated parse trees for the inputs 9-5+2 and 9- 
5*2. 
2.8 Construct a syntax-directed translation scheme that translates arith- 
metic expressions from postfix notation into infix notation. Give 
annotated parse trees for the inputs 95-2+ and 952+-. 
2.9 Construct a syn tax-directed translation scheme that translates integers 
into roman numerals. 
2.10 Construct a syntax-directed translation scheme that translates roman 
numerals into integers. 
2.1 1 Construct recursivedescent parsers for the grammars in Exercise 2.2 
(a), I b h  and {cl. 
2.12 Construct 
a 
syntax-directed translator 
tha~ verifies 
that 
the 
parentheses in an input string are properly balanced. 
The €allowing rules define the translation of an English word into pig 
Latin: 
a) If the word begins with a nonempty string of consonants, move the 
initial consonant string to the back of the word and add the suffix 
AY; e.g., gig becomes igpay. 
b) If the word begins with a vowel, add the suffix YAY; e,g.. owl 
becomes owlyay. 
c) U following a Q is a consonant. 
d) Y at the beginning of a word is a vowel if it is not followed by a 
vowd. 

80 A SIMPLE COMPILER 
CHAPTER 2 
e) One-letter words are not changed. 
Construct a syntax-directed translation scheme for pig Latin, 
2.14 In the programming language C the for-statement has the form: 
The first expression i s  executed before the Imp; it is typically used for 
initializing the Imp index, The second expression is a test made 
before each iteration of the loop; the Imp is exited if the expression 
becomes 0. The bop itself consists of the statement (srmt expr3;). 
The third e~pression is executed at the end of each iteration; it is typi- 
cally used to increment the loop index. The meaning of the for- 
statement is similar to 
Construct a syntax4 irected translation scheme to lranslate C for- 
statements into stack-machine code+ 
*2,15 Consider the foilowing for-statement: 
Three semantic definitions can be given for this statement. One pos- 
sible meaning i s  rhat the limit 10 * j and increment 10 - j are to be 
evaluated once before the Imp, as in PLk For example, if j = 5 
before the imp, we would run through the Imp ten times and exit. A 
second, completely different, meaning would ensue if we are required 
to evaluate the limit and increment every time through the hop. For 
example, if j = 5 before the loop, the bop would never terminate. A 
third 'meaning is given by languages such as Algal. When the incre- 
ment is negative, the test made for termination of the loop is 
i < 10 * j ,  rather than i > LO * j. For each of these three semantic 
I scheme to translate 
if-then- and if-t hen- 
definitions construct a syntax-directed translation 
these for-loops into stack-machine code. 
2.16 Consider the following grammar fragment for 
else-statements: 
s m t  + If vxpr then stmt 
) if expr tben stmr else srmr 
I ather 
where dher stands for the other statements in the language. 
a) Show that this grammar is ambiguous. 
b) Construcl an equivalent unambiguous grammar that asswiittes 
each else with the closest previous unmatched then. 

CHAPTER 2 
BlBLlOGRAPHE NOTES 8 1 
c) Construct a syntaxdiredd translation scheme based on this gram- 
mar to translate conditional statements into stack machine d e .  
*2.17 Construct a sy ntax-directed translation scheme that translates arith- 
metic expressions in infix notation into arithmetic expressions in infix 
notation having no redundant parentheses. Show the annotated parse 
tree forthe input ( ( ( 1  + 2 )  + I 3  + 4 ) )  + 5 ) .  
PROGRAMMING EXERCISES 
P2,1 Implement a translator from integers to roman numerals based on the 
syntax-directed translation scheme developed in Exercise 2.9. 
P2.2 Modify the trans la to^ in Section 2.9 to produce as output code for the 
abstract stack machine of Section 2.8. 
P2.3 Modify the error recovery module of the translator in Section 2.9 to 
skip to the next Input expression on encountering an error. 
P2,4 Extend the translator in Section 2.9 to handle all Pam1 expressions. 
P2.5 Extend the compiler of Section 2.9 to translate into stack-machine 
code statements generated by the following grammar: 
W . 6  Construct a set of test expressions for the compiler in Section 2.9, so 
that each production is used at least once in deriving some ted expres- 
sion. Construa a testing program that can be used as a general com- 
piler testing tw1. Use your testing program to run your compiler on 
these test expressions, 
P2.7 Construct a set of test statements for your compiler of Exercise P2.5 
rn that each production is used at least once to generate some lest 
statement. Use the testing program of Exercise P2A to run your 
compiler on these test expressions. 
BIBLIOGRAPHIC NOTES 
This introductory chapter touches on a number of subjects that are treated in 
more detail in the rest of the book. Pointers to the literature appear in the 
chapters wntaining further material. 
Context-free grammars were i n t r d u d  by Chornsky 11956) as part of a 

82 
A SIMPLE COMPILER 
CHAPTER 2 
study of natural languages. Their use in specifying the syntax of program- 
ming languages arose independently, While working with a draft of Algol 60, 
John Backus "hastily adapted [Emil Post's productions] to that use" (Wexel- 
blat 11981 , p+ l62]), The resulting notation was a variant of context-free gram- 
mars. The scholar Panini devised an equivalent syntactic nutation to specify 
the rules of Sanskrit grammar between 400 B.C. and 200 B.C. (lngerman 
119471)- 
The proposal that BNF, which began as an abbreviation of Backus Normal 
Form, be read as Backus-Naur Form, to recognize Naur's contributions as edi- 
tor of the Algol 60 report (Naur [19631), is contained in a letter by Knuth 
I1WL 
Syntax-directed definitions are a form of inductive definitions in which the 
induction i s  on the syntactic structure. As such they have long been used 
informally in mathematics. Their application to programming Languages came 
with the use of a grammar to structure the Algol 60 report. 
Shortly 
thereafter, Irons 11 961 ) constructed a syntax-directed compiler. 
Recursive-descent parsing has k
n
 
used since the early 19M)'s+ huer 
1197 61 attributes rhe method to Lucas 11% 11. bare [1962b, p. 1281 describes 
an Algol compiler organized as ''a set of p r d u r e s ,  each of which is capable 
of processing one of the syntactic units of the Algol 60 report. " Foster I 1%8l 
discusses the elimination of left recursion from productions containing seman- 
tic actions that do not affect attribute values. 
McCarthy 119631 advwated that rhe translation of a language be based on 
abstract syntax. In the same paper McCarrhy 11%3, p.241 left "the reader to 
convince himself" that a tail-recursive fotmu~atlon of the factorial function is 
equivalent to an iterative program. 
The benefits of partitioning a compiler into a front end and a back end were 
explored in a committee report by Strong et al. 119581, The report coined the 
name UNCOL (from universal computer oriented language) for a universal 
intermediate language. The concept has remaiaed an ideal. 
A good way to learn about implementation techniques Is to read the code of 
existing compilers. Unfortunately, code is not often published. Randell and 
Russel 119641 give a comprehensive account of an early Algol compiler. 
Compiler code may also be seen in McKeernan, Hwrning, and Wortman 
119701. Barron I198 11 is a collection of pagers on Pascal implementation, 
including implementation notes distributed with the Pascal P compiler (Nori et 
al. [1981]), code generation details (Ammann [1977]), and the code for an 
implementation of Pascal S, a Pascal subset designed by Wirth 119811 for stu- 
dent use. Knuth 119851 gives an unusually clear and detailed description of 
[he l&X translator. 
Kernighan and Pike [ 19841 d e w  ibe in detail how to build a desk calculator 
program around a syntax-directed translation scheme using the compiler- 
constmction tools available on the UNlX operating system. Equation (2.17) is 
from Taotzen 119631, 

CHAPTER 3 
Lexical 
Analysis 
This chapter deals with tech iques for specifying and implementing lexical 
analyxrs. A simple way to build a lexical analyzer is to construct a diagram 
that illustrates the structure of the tokens of the source language, and then to 
hand-translate the diagram into a program for finding tokens. Efficient lexi- 
cal analyzers can be produced in this manner, 
The techniques used to implement lexical analyzers can also ke applied to 
other areas such as query languages and information retrieval systems. In 
each application, the underlying problem is the specification and design of 
programs that execute actions triggered by patterns in strings. Since patttrn- 
directed programming is widely useful, we introduce a pattern-action language 
cakd LRX for specifying lexical analyzers. In this language, patterns are 
specified by regular expressions, and a compiler for L ~ K  
can generate an effi- 
cient finiteautomaton recognizer for the regular expressions. 
Several other languages use regular expressions to describe patterns, For 
example, the pattern-scanning language AWK uses regular expressions to 
select input lines for processing and the U N I X  system shell allows a user to 
refer to a set of file names by writing a regular expreshn, The UNIX cam- 
mand ra *. o, b r  instance, removes all files with names ending in ". o".' 
A software tool that automates the construction of lexical analyzers allows 
peoplc with different backgrounds to use pattern matching in their own applii- 
cation areas. For example, Jarvis 119761 used a lexical-analyzer generator to 
create a program that recognizes imperfections in printed circuit bards. The 
circuits are digitally scanned and converted into "strings" of line segments at 
different angles. The "lexical analyzer" lmked for patterns corresponding to 
imperfections in the string of line segments. A m a p  advantage of a lexical- 
analyzer generator is that it can utilize the kt-known pattern -matching algo- 
rithms and thereby create efficient lexical analyzers for people who are not 
experts in pattern-matching techniques. 
The cxprcssion . o is a variant of the usual notation for tcgular expressions. Ewrciscs 3.10 
and 3.14 rncntion wimc commonly used variants ut rcgular cxprciision notatiuns. 

84 
LEXICAL ANALYSIS 
3+1 THE ROLE OF THE LEXICAL ANALYZER 
The lexical analyzer is the first phase of a compiler. Its main task is to read 
the input characters and produce as output a sequence of tokens that the 
parser uses for syntax analysis. This interaction, summarized schematically in 
Fig. 3. I ,  is commonly implemented by making the kxical analyzer be a sub- 
routine or a coroutine of the parser. Upon receiving a "get next token" com- 
mand from the parser, the lexical analyzer reads input characters until it can 
identify the next token+ 
Fig. 3.1, lnlcraction of lexical rlnalyzcr with parser. 
Since the lexical analyzer is the part of the compiler that reads the source 
text, it may also perform certain secondary tasks at the user interface, One 
such task is stripping out From the scsurce program cwmments and white space 
in the form of blank, cab, and newline characters. 
Another is correlating 
error messages from the compiler with the source program. For example, rhe 
lexical analyzer may keep track of+ the number of newline characters seen, so 
that a line number can be associated with an error message. In some cm- 
pilers, the lexical analyzer is in charge of making a copy of the source pro- 
gram with the error messages marked in it. If the source language supports 
some macro preprocessor functions, then these preprocessor functions may 
also be implemented as lexical analysis takes place, 
Sometimes. lexical analyzers are divided into a cascade of two phases, the 
first called "scanning," and che second "lexical analysis." 
The scanner i s  
respnsibk for doing simple tasks, while rhe lexical a n a l p r  proper does the 
more complex operations. For example, a Fortran compikr might use a 
scanner to eliminate blanks from the input. 
Issues in .Lexical Analysis 
There are several reasons for separating the analysis phase of compiling into 
lexical analysis and parsing. 
1 .  
Simpler design is perhaps the most important consideration. The separa- 
tion of lexical analysis from syntax analysis often allows us to simplify 

SEC+ 3.1 
THE ROLE OF THE LEXICAL ANALYZER 
85 
one or the other of these phases. For example, a parser embodying the 
conventions for comments and white space is significantly more complex 
than one that can assume comments and white space have already been 
removed by a lexical analyzer. If we are designing a new language, 
separating the lexical and syntactic conventions can lead to a cleaner 
overall language design. 
Compiler efficiency is improved. A separate lexical analyzer allows us to 
construct a specialized and potentiaily more efficient prmssor for the 
task. A k g e  amount of time is spent reading the source program and 
partitioning it Into tokens. Speciahd buffering techniques for reading 
input characters and prwssing tokens can significantly speed up the per- 
formance of a compiler, 
Compiler portability is enhanced. Input alphabet peculiarities and other 
device-specific anomalies can be restricted to the !exical analyzer, The 
representation of special or non-standard symbols, such as 
in Pascal, 
can be isdated in the lexical analyzer . 
Specialized twls have been designed to help automate the construction of 
lexical analyzers and parsers when they are separated. We shall see several 
examples of such tools in this book. 
Tokens, Patterns, Lexemes 
When talking about lexical analysis, we use the terms "loken," "pattern," and 
"lexeme" with specific meanings. Examples of their use are shown in Fig. 
3.2. In general, there is a se[ of strings in the input for which the same token 
is produced as output. This set of strings is described by a rule called a put- 
wrn associated with the token. The pattern i s  said tr, mar& each string in the 
set. 
A lexerne is a sequence of characters in the source program that is 
matched by the pattern for a token. For example, in the Pascal statement 
the substring pi is a lexerne for the token "identifierA" 
p i .  count, P2 
lcttcr folli~wd by lcltcrs and digits 
3.1416.0,6.02B23 
anynumcriicorptant 
n ~ ~ r e  
dumped' 
I any characters bctwccn " and " cxccpl " 
Fig, 3.2. Exarnplcs of lokcns, 

86 LEXICAL ANALYSlS 
SEC. 3.1 
We treat tokens as terminal syrnbis in the grammar for the source 
language, using boldface names to represent tokens. The kxcmes matched by 
the pattern for the token represent strings of characters in the source program 
that can be treated together as a lexical unit. 
In mosl programming languages, the following constructs are treated as 
tokens: key words, operators, identifiers, constants, literal strings, and punc- 
tuation symbols such as parentheses, commas, and semicolons. In the cxam- 
pie above. when the character sequence pi appears in the source program, a 
token representing an identifier is returned to the parser. The returning of a 
token is often implemented by passing an integer corresponding to the token. 
It is this integer that is referred to in Fig, 3,2 as boldface M, 
A pattern is a rule describing the set of lexemcs that can represent a partic- 
ular taken in source programs. The pattern for the token mnst in Fig. 3.2 is 
just the single s~ring oonst that spells out the keyword. The pattern for the 
token r e l a t h  is the set of all six Pascal relational operators. To describe pre- 
cisely the patterns for more complex tokens like id (for identif~r) and mum 
(for number) we shall use the regularexpression notation developed in Section 
3.3. 
Certain language conventions impact the difficulty of lexical analysis. 
Languages such as Fortran require certain constructs in fixed positions on the 
input line. Thus the alignment of a lexerne may be important In determining 
the correctness of a wurce program. The trend in modern language design is 
toward free-format input, allowing constructs to be positioned anywhere on 
the input line, so this aspect of lexical analysis Is becoming less important. 
The treatment of blanks varies greatly from language to language In some 
ianguages, such as Fortran or Algol 68, blanks are not significant except in 
literal strings. They can be added at will to improve the readability of a pro- 
gram. The conventions regarding blanks can greatly complicate the task of 
identifying tokens. 
A popular e~ample that illustrates the potential difficulty of recognizing 
tokens i s  the DO statement of Fortran, In the statement 
we cannot tell until we have seen the decimal p i n t  that Do is not a keyword, 
but rather part of the identifier D05I. On the other hand, in the statement 
we have seven tokens, corresponding to the keyword DO, the statement label 
5, the identifier I, the operator =, the constant 1, the comma, and the con- 
stant 25. Here, we cannot be sure until we have seen the comma that DO i s  a 
keyword, To alleviate this uncertainty, Fortran 77 allows an optional mmma 
between the label and index of the DO statement. The use of this comma is 
encouraged because it helps make the M, statement clearer and more read- 
able. 
In many languages, certain strings are reserved; i.e., their meaning is 

SEC. 3.1 
THE ROLE OF THE LEXICAL ANALYZER 
87 
predefined and cannot be changed by the uuer. If keywords are not reserved, 
then the lexical analyzer must distinguish between a keyword and a user- 
defined identifier, In PWI, keywords are not reserved; thus, the rules for dis- 
tinguishing keywords from identifiers are quite complicated as the following 
PL/I statement illustrates; 
IF THEN THEN THEN = ELSE; ELSE ELSE = THEN; 
Attributes for Tokens 
When more than cine pattern matches a lexerne, the lexical analyzer must pro- 
vide additional information abut the partldar lexerne that matched to the 
subsequent phases of the compiler. For example, the pattern num matches 
both the strings 0 and 7 ,  but it is essential for the code generator to know 
what string was actually matched. 
The iexical analyzer collects information abut tokens into their associated 
attributes, The tokens influence parsing decisions; the attributes influence the 
translation of tokens. As a practical matter, a token has usually onIy a single 
attribute - 
a pointer to the symbl-table entry in which the information about 
the token is kept; the pointer becomes the attribute for the token. For diag- 
nostic purposes, we may Ix interested in both the lexerne for an identifier and 
the line number on which it was first seen. Both these items of information 
can k stored in the symbol-table entry for the identifier. 
Example 3-1, The tokens and asmciated attribute-values for the Fortran 
stalemen t 
are written below as a sequence of pairs: 
<id, pointer to symbol-table entry for E> 
<id, pointer to symbol-table entry for M> 
<id, pointer to symbol-table entry for C> 
<nuin+ integer value 2> 
Note that in certain pairs there is no need for an attribute value; the first am- 
portent i s  sufficient to identify the lexeme. [n this srnalt example, the token 
num has been given an integer-valued attribute. The compiler may store the 
character string that forms a number in a symbl table and let the atlrtbute of 
token mum ke B pointer to the table entry. 
a 

88 
LEXICAL ANALYSIS 
SEC. 3.1 
Few errors are discernible at the lexical level alone, because a lexical analyzer 
has a very localized view of a source program. If the string f i 
is encountered 
in a C program for the first time in the context 
a lexical analyzer cannot tell whether f i is a misspelling of the keyword if or 
an undeclared function identifier. Since f i is a v a M  identifier, the lexical 
analyzer must return the token for an identifier and let some other phase of 
the compiler handle any error. 
But, suppose a situation does arise in which the lexical analyzer is unable to 
proceed because none of the patterns for tokens matches a prefix of the 
remaining input. Perhaps Ihe simplest recovery strategy is "panic mode" 
recovery, We delete successive characters from the remaining input until the 
lexical analyzer can find a well-formed token. This recovery technique may 
occasionally confuse the parser, but in an interactive cornput ing environment it 
may be quite adequate. 
Other possible error-recovery actions are: 
1 + 
deleting an extraneous character 
2. 
inserting a missing character 
3. 
replacing an incorrect character by a correct character 
4, 
transposing two adjacent characters. 
Error transformations like these may be tried in an attempt to repair the 
input. The simplest such strategy is to see whether a prefix of the remaining 
input can be transformed into a valid lexerne by just a single error transforma- 
tion. This strategy assumes most lexical errors are the result of a single error 
transformation, an assumption usually, but not always, borne out in practice. 
One way of finding the errors in a program is to compute the minimum 
number of error transformations required to transform the erroneous program 
into one that is syntactically wcll-formed. We say that the erroneous program 
has k errors if the shortest sequence of errw transformatjons that will map it 
into some valid program has Length k. Minimum-distance error correction is a 
convenient theoretical yardstick, but it is not generally used in practice 
because it is too costly to irnplcmcnt. However, a few experimental compilers 
have used the minimum-distance criterion to make local corrections. 
3.2 INPUT BUFFERING 
This sect ion covers some efficiency issues concerned with the buffering of 
input, We first mention a two-buffer input scheme that is ~seful when lwk- 
ahead on the input is necessary to identify tokens. Then we intrduce some 
useful techniques for speeding up the iexical analyzer, such as the use of "sen- 
tinels" to mark the buffer end. 

SEC. 3.2 
iNPUT BUFFERING 89 
There arc three general approaches to the implementation of a lexical 
analyzer. 
I +  Use a lexical-analyzer generator, such as the Lex compiler discussed in 
Section 3.5, to product: the lexical analyzer from a regular-expression- 
based specification. In this case, the generator provides routines for read- 
ing and buffering the input. 
2. 
Write the lexical analyzer 
in 
a conventional systems-programming 
language, using the 110 facilities of that language to read the input+ 
3. 
Write the lexical analyzer in assembly language and explicitly manage the 
reading of input + 
The three choices art listed in order of increasing difficulty for the imple- 
mentor, Unfortunately. the harder-to-implement approaches often yield faster 
lexical analyzers. Since the lexicrrl analyzer is the only phase of the compiler 
that reads the source program character-bytcharacter, it is possible to spend a 
considerable amount of time in the le~ical analysis phase, even though the 
later phases are conceptually more complex. Thus, the speed of lexical 
analysis is a concern in compiler design. While the bulk. of the chapter is 
devoted to the first approach, the design and use of an automatkc generator, 
we also mnsid& techniques that are helpful in manual design. Section 3.4 
discusses tran sb ion diagrams, which are a useful concept for [he organization 
of a handdesigned lexical analyzer. 
For many source languages, there are times when the lexical analyzer needs 10 
look ahead sevcraf characters bq0nd the Iexerne for a pattern before a match 
can be announced, The lexical analyzers in Chapter 2 used a function 
ungetc to push lookahead characters back into [he input stream, Because a 
large amount of time can be consumed moving characters, specialized buffer- 
ing techniques have been developed to reduce the amount of overhead 
required to process an input character. Many buffering schemes can be used, 
but since the techniques are somewhat depndent on system parameters, we 
shall only outline the principles behind one class of schemes here+ 
We use a buffer divided inro two N-character halves, as shown in Fig. 3.3. 
Typically, N i s  the number of characters on one disk block, e+g., 1024 or 
4096. 
Fig. 3.3. An input buffer in two halvcs. 

90 LEXICAL ANALYSIS 
SEC. 3+2 
We read N input characters into each half of the buffer with one system 
read command, rather than invoking a read command for each input charac- 
ter+ If fewer than N characters remain in the input, then a special character 
mf is read into the buffer after the input characters, as in Fig. 3.3. That is, 
mf marks the end of the source file and is different from any input character, 
Two pointers to the input buffer are maintained. The string of characters 
between the two poinrers is the current lexeme. Initially, both pointers p i n t  
to the first character of the next lexcrnc to be found. One, called the forward 
pointer, scans ahead until a match for a pattern is found. Once the next lex- 
erne is determined, the forward pointer i s  set to the character at its right end. 
After the h e m e  is processed, both pointers are set to the character immedi- 
ately past the lexcme. With this scheme, comments and white space can be 
treated as patterns that yield no token. 
If the forward pointer is about to move past the halfway mark, the right 
half is filled with N new input characters. If the forward pointer is about to 
move past the right end of the buffer, the left half is filled with h' new charac- 
ters and the forward pointer wraps around to the beginning of the buffer. 
This buffering scheme works quite well most of the time, but with it the 
amount of lookahead is limited, and this limited lookahead may make it 
impossible lo recognize tokens in situations where the distance that the for- 
ward pointer must travei is more than the length of the buffer. For example, 
if we see 
DECLARE ( A w l ,  ARG2, 
. . . , ARGn ) 
in a PLtl program, we cannot determine whether DECLARE it; a keyword or an 
array name until we see the character that follows the right parenthesis. l n  
either case, the h e m e  ends at the second E, bul the amount of lookahead 
needed is proportional to the n u m b  of arguments, which in principle is 
unbounded. 
if Jomurd at cnd of first lxhalf then begin 
reload s m n d  half; 
jorwurd := ~ w w w d  + I 
end 
else if /owrrrd at cnd of sccond half then bgin 
rcload first half; 
m o w  J~mv~rd 
to beginning of first haw 
end 
el* fimwrd : = jirward + I ; 
Fig. 3.4. Codc to advancc forward pointcr. 

INPUT BUFFERING 91 
If we use the scheme of Fig. 3.3 exactly as shown, we must check each time 
we move the forward pointer that we have nor moved off one half of the 
buffer; if we do, thcn we must reload the other half. That is, our code for 
advancing the forward winter performs tests like those shown in Fig. 3.4. 
Except a1 the ends of the buffer halves, the code in Fig. 3.4 requires two 
tests for each advance of the forward pointer. We can reduce the two tests to 
one if we extend each buffer half to hold a sentinel character at the end. The 
sentinel is a special character that cannot be part of the mum program. A 
natural choice is &* 
Pig. 3.5 shows the same buffer arrangement as Fig. 3,3, 
with the sentinels added. 
Fig. 3.5, Sentinels at end of each buffer half. 
With the arrangement of Fig. 3.5, we can use the d
e
 
shown in Fig. 3.6 to 
advance the forward pointer (and test for the end of the source file). M a t  of 
the time the code performs only one test to see whether f m r d  pints to an 
eof. Only when we reach the end of a buffet half or the end of the file do we 
perform more tests. Since N input characters are encountered between mfs, 
the average number of tests per input character is very close to L. 
jonvarb : = furnard + I ; 
if jorward t = eof Uwn b i n  
if funuard at end of first half then kgjn 
reload seccmd balf; 
forward := forward + I 
ead 
else if Joward at end of second half thcn begin 
reload first half; 
mow fuwurd to beginning of firsl half 
end 
e k  /+ eof within a buffer signifying end of input */ 
terminate lexical analysis 
end 
Fig. 3.6. Lookahead d
c
 
with sentinels. 

92 
LEXICAL ANALYt'SIS 
SEC. 3.2 
We also need to decide how to prams the character scanned by the forward 
pointer; does it mark the end of a token, does it represent progress in firtding 
a particular keyword, or what'.' One way to structure these tests is to use a 
case statement, if the implementation language has one. The test 
can then be implemented as one of the different cases. 
3.3 SPECIFICATION OF TOKENS 
Regular expressions are an imporlant notation for specifying patterns, Each 
pattern matches a set of strings, so regular expressions will serve as names for 
sets of strings. 
Section 3.5 extends this notation into a pattern-directed 
language for lexical anajysis. 
Strings and Languages 
The term a/,hrrbel or chrucrer dass denotes any finite set of symbols. Typi- 
cal examples of symbols are letters and characters. The set {0,1} is the Mwry 
a/piusbe:. ASCII and EBCDIC are two examples of computer alphabets. 
A string over =me alphabet is a finite sequence of symbols drawn from that 
alphabet. In language theury, the terms sentewe and word arc often used as 
synonyms for the term "string." The length of a string 3, usually written Is[, 
i s  the number of occurrences of symbols in s. For example, banana is a 
string of iength six. The empty string. denoted t, i s  a special string of length 
zero. Some cornman terms associated with parts of a string are summarized 
in Fig. 3.7. 
The term #un#uqt. denotes any set of strings over some fixed alphabet. 
This definition is very broad. Abstract languages like DZI, 
the empty se4, or 
(€1, the set containing only the empty string, are languages under this defini- 
tion. So too are the set of all syntactically well-formed Pascal programs and 
the set of all grammatically correct English sentences, ahhmgh the latter two 
sets are much more difficult to specify, Also note that this definition does not 
ascribe any meaning to the strings in a language. Methods for ascribing 
meanings to strings are discussed in Chapter 5. 
If x and y are strings, then the cmrutmutim of x and y, written xy, Is the 
string formed by appending y to x, For example, if x = dog and y = house, 
then xy = doghouse. The empty string i s  the identity element under con- 
catenation. That is,sr = r s  = s. 
, 
If we think of concatenation as a "product", we can define string "exponen- 
tiation" as follows+ Define sU to be t, and for i >O define xi to be si -Is. 
Since r s  is s itself, s '  = s. Then, s2 = ss, s3 = sm, and m on. 

SEC. 3.3 
SPEClFlCATIOrJ OF TOKENS 93 
1 e.g., rtan is a substring of banana. Every prefix and every 
TERM 
pr@ of s 
s@i of $ 
s~bslrin8 of 3 
I suffix of s i s  a substring of s, but mt every subatring of s is 
D E F I N ~ o H  
A siring dained by removing zero or more trailing symbols 
of srring s; e. g., han is a prefix of banana. 
A string formed by deleting 2 . e ~  w more of the leading 
symbols of s; e.g.. hana is a suffix of banana, 
L 
( 
s ptefu or a suffix of r. For every string 1, both s and t are 
A string obtained by deleting a prefix and a suffix from J: 
Fig. $,7, Terms for parts of a strifig. 
pmpr prefix, suffix, 
or sutrstrhg of s 
sdwquersce of s 
There are several important operations that can be applkd to languages. Fm 
lexical analysis, we are interested primarily in union, concatenation, and clo- 
sure, which art defined in Fig. 3.8. We can also generalize the "expnentia- 
tion" operatar to languages by defming LO 
to be (€1, and L' to be Li-'L. 
Thus, L" is L concatenated with itself i - I times, 
Any nonernfiy mtng x that is, respectively, a prefix, suffix, 
or sutming of s such that s # x. 
Any string formed by deleting iwo M more not necessarily 
contiguous symbols from s; e+g+, baaa is a subquenm of 
banana+ 
Example 3.2, Let L be the set {A, 8, . . . , 2, a, b, , . . , t} and D the set 
(0, I, . . . , 9). We can think of L and D in two ways. We can think of L as 
the alphabet consisting of the set of upper and lower case letters, and D as the 
alphabet consisting of the set of the ten decimal digits. Alternatively, since a 
symbol can k 
regarded as a string ~f length m e ,  the sets L and D art each 
finite languages. Here are some emmpks of new languages created from L 
and D by applying the operators defined in Fig. 3,8, 
1 + 
L U D is the set of ktters and digits. 
2. W is the set of strings consisting of a letter followed by a digit. 
3. 
L' 
is the set of all four-later strings. 
4. 
L* is the set of all strings of letters, including E, the empty string, 
5. L{L V D)* is the set of 8H gtring~ of letters and digits beginning with a 
letter. 
6. 
D+ is the set of all strings of me or c n ~ e  
digits. 

94 
LEKICAL ANALYSIS 
SEC. 3.3 
uwim of L and M 
written LUM 
I 
written L * 
I 
L 
+ denotes "one w more wncatenatbns or' L. 
written L* 
positive c h u w  d L 
In Pawl, an identikr is a letttr followed by zero or more letters or digits; 
that is, an identifier is a member of the set defined in part (5) of Example 
3.2. In this twticm, we present a notation, called regular expressions, that 
allows us to define precisely sets such as this. With this notation, we might 
defme Pascal identifiers as 
L* denotes "zero as m e  
mncaten~tbs of' L. 
L t  = U L ~  
i = I 
The vertical bar here means "or," the parentheses are used to group subex- 
presshs, the star meus "zero a more instances of" the parenthesized 
expression, and the juxtapitim of ktttr with the remainder of the expres- 
sion means concatenation. 
A regular expression is built up out of simpler reguhr expressions using a 
set of defining rub. Each regular txpressicm r denotes a language L ( r ) .  The 
defining rules specify how L(r) is formed by combining in various ways the 
languages d e m d  by the subexpressions of r. 
Were arc the rubs that define the regdur expms$iotts m r  ~~# 
Z. 
Associated with each rule* is r specification of the language b e n d  by the 
regular expression king defined. 
1. 
t is a regular expression that denotes (€1, that is, the set containing the 
empty string. 
2. 
If o is a symbol in 2, then a is a regular expression that denotes {a), i .e., 
the we containing the string a. Although we use the same wtation for all 
three, technically, the regular expression a is diffemt from the string u 
or the symbol a. I t  will h clear from the context whether we are talking 
abut a as a regular expression, string, cw symbol. 

SEC. 3.3 
SPECIFICATION OF TOKENS 95 
3. 
Suppose r and s are regular expressions denoling the Languages L ( r )  and 
L(s). Then, 
a) 
( r )  I (s) is a regular expression denoting L (r) U L (s). 
) 
( r ) ( s )  is a regular expression denoting L (r)L(s). 
C) 
(r)* is a regular expression denoting (L(r))*. 
d) 
(r) i s  a regular expression denoting ~ ( r ) . '  
A language denoted by a regular expression is said to be a regidor set. 
The specificat ion of a regular expression is an example of a recursive defini- 
tion. Rules ( 1 )  and (2) form the basis of the definition; we use the term h s i r  
symbol to refer to 6 or a symbol in 2 appearing in a regular expression. Rule 
(3) provides the inductive step. 
Unnecessary parentheses can be avoided in regular expressions if we adopt 
the conventions that: 
I .  
the unary operator * has the highest precedence and is left associative, 
2. 
concatenat ion has the second highest precedence and is left associative, 
3. 
) has the lowest precedence and is left associative. 
Under these mnventions, (a) I(( b)*(c)) is equivalent to u 1 b * ~ .  Both expres- 
sions denote the set of strings that are either a single a or zero or more 6's 
followed by one c. 
Example 3.3. Let 1 = {a, b). 
1. 
The regular expression a \b denotes the set {a, b). 
2. The regular expression (a1 b)(a lb) denotes {na, ub, h, 
bb). the set of all 
strings of a's and b's of length two. Another regular expression for this 
same set is uu 1 aB I bu I bb. 
3. 
The regular expression a* denotes the set of all strings of zero or more 
a's, i.e., {E, u, au, ma, - 
). 
4. 
The regular expression (uIb)* denotes the set of all strings containing 
x r o  or more instances of an a or b, that is, the set of all strings of a's 
and b's. Another regular expression for this set is (a*b*)*, 
5 .  The regular expression a I u*b denotes the set containing the string a and 
all strings consisting of zero or more a's followed by a b. 
If two regular expressions r and s denote the same language, we say r and s 
are eyuivaht and write r = s. For example, (atb) = (bja). 
There are a numkr of algebraic laws obeyed by regular expressions and 
these can be used to manipulate regular expressions into equivalent forms. 
Figure 3.9 shows some algebraic laws that hold for regular expressions r, s, 
and t. 
' Tha ruk says that cxrra pairs nf prcnthcw may be p l a d  arutlfid mgulrr cxprcsbns if wc 
dcsirc. 

% LEXICAL ANALYSH 
SEC. 3.3 
EF = r 
I 
c is thc identity clcmcnt for concatenation 
r E  = r 
r ~ s l r )  = rslrt 
( s l t ) r  = srltr 
concatenation distributcs over [ 
Flg. 3,9. A lgcbraic propcrtics of rcgular expressions. 
r* = (rlc)* 
+" 
r** = p 
For notational convenience, we may wish to give names to regular expressions 
and to define regular expressions using these names as if they were symhls, 
lf E is an alphabet of basic symbols, then a regular definitlun is a sequence of 
defjnitims of the form 
relation between * and E 
* is idcmpotcmt 
where each bi is a distinct name, and each ri is a regular expression over the 
symbols in E U (dl, J 2 ,  + . . , diPl), i.e., the basic symbols and the previ- 
ously defined names. By restricting each ri to symbols of E and the previ- 
ously defined names, we can construct a regular expression over E for any rj 
by reptatedl y replacing regu lar-expression names by the expressions they 
denote. If ri used d, for some j r i, then ri might be recursively defined, and 
this substitution process would not terminate. 
To distinguish names from symbols, we print the names in regular defini- 
tions in hldface. 
Enarnple 3,4. 
As we have stated, the set of P a m i  identifiers is the set of 
strings of letters and digits beginning with a letter. Here is a regular defini- 
tion for this set. 
ExamNe 3.5, Unsigned nurnhrs in Pascal are strings such as 5280, 39,37, 

SEC. 3,3 
SPECiFKATiON OF TOKENS 97 
6 
+ 336E4, or 1.894E-4. The following regular definition provides a precise 
specification for this class of strings: 
This definition says that an opthnal-fradion is either a decimal point fob 
lowed by one or more digits, or i t  is missing (the empty string). 
An 
op4ionLexprrent, if it is not missing, is an E folbwed by an optional + or - 
sign, followed by one or more digits. Note that at least one digit must follow 
the period, so num does not match I .  but it does match 1 - 0 .  
o 
Notational Short bands 
Certain constructs occur so frequently in regular expressions that it is can- 
venient to intrduce notational short hands for them. 
+ 
1. 
Ow or more insramces+ The unary postfix operator 
means "one or 
more Instances of." If r is a regular expression that denotes the language 
L ( r ) ,  then (r)' 
is a reg~lar expression that denotes the language 
(L 
(r))+ . Thus, the regular expression a + denotes the set of a11 strings of 
one or more a's. The operator + has the same precedence and associa- 
tivity as the operator *. The two algebraic identities ri = r' Ir and 
r' = rr* relate the Kleene and positive closure operators. 
2. 
Zero or one ictrsce. The unary postfi~ operator ? means "zero or m e  
instance of." The notation r'? is a shorthand for rlt. If r is a regular 
expression, then (r)? is a regular expression that denotes the language 
L(r) U {E), For example, using the ' and :) operators, we can rewrite 
the regular definition for mum in Example 3.5 as 
3. 
Chacior c.lasses. The notation labcj where a. b, and c are alphakt 
symbols denotes the regular expression a I b I c. An abbreviated char- 
acter class such as la-zl denotes the regular expression a I b ( . - . 1 z. 
Using character classes, we can describe identifiers as king strings gtn- 
erafed by the regular expression 

98 
LEXICAL ANALYSIS 
SEC. 3+3 
Some languages cannot be described by any regular expression. To illustrate 
the limits of the descriptive power of regular expressions, here we give exam- 
ples of programming language constructs that cannot be described by regular 
cxpressians. Prmfs of these assertions can be found in the references. 
Regular expressions cannot be used to describe b a l a n d  or nested con- 
structs. Fw example, the sat of ail strings of balanced parentheses cannot be 
described by a reguiar expression. On the other hand, this set can be speci- 
fied by a context-free grammar. 
Repeating strings cannot be described by regular expressions+ The set 
{wcw I w is a string of a's and b's ) 
cannot be denoted by any regular expression, nor can it be described by a 
context-ffee grammar. 
Regular expressions can be used to denote only a fixed number of repeti- 
tions or an unspecified number of repetitions of a given construct, Two arbi- 
trary numbers cannot be compared to see ivhether they are the same. Thus, 
we cannot describe. Hollerith strings of the form rrHa luz + - 
4 u,~ from ea~iy 
versions of Fortran with a regular expression, because the number of charac- 
ters following H must m a t h  the decimal number before H. 
3.4 RECOGNITION OF TOKENS 
In the previous section, we considered the problem of how to specify tokens. 
In this section, we address the question of how to recognize them. 
Throughout chis wcticln, wc use the language generated by the following 
grammar as a running example. 
Example 3.6. Consider the following grammar fragment: 
where the terminals if, then, d*, relop, id, and nurn generate sets of strings 
given by the following regular definitions: 
i C  + if 
then -. then 
else -. else 
r e b o p + < [ < +  
I < > I > [ > '  
id + Mter ( Mer I digit )* 
mum + digit ' ( . digitt )'? ( E( + I - )? digit 
+ ) 1 

SEC. 3.4 
RECOGNITION OF TOKENS 99 
where letter and digit are as defined previously. 
For this language fragment the lexical analyzer will recognize the keywords 
if, then, else, as well as the kxernes denoted by relop, id, and mum. Ta 
simplify matters, we assume keywords are reserved; rhat is, they cannot be 
used as identifiers. As in Example 3.5, nurn represents the unsigned integer 
and real numbers of Pascal. 
In addition, we assume lexemes are separated by white space, consisting of 
nonnull sequences of blanks. tabs, and newlines. Our lexical analyzer will 
strip out white space. It will do so by comparing a string against the regular 
definition ws, below. 
delh -. blank I tab I newline 
ws -. delim' 
If a match for ws is found, the lexical analyzer does not return a token to the 
parser. Rather, it proceeds to find a token following the white space and 
returns that to the parser+ 
Our goaj i s  to construct a lexical analyzer rhat will isolate the lexeme for 
the next token in the input buffer and produce as output a pair mnsisting of 
the appropriate token and attribute-value, wing the translation table given in 
Fig. 3.10. The attribute-values for the relational operators arc given by the 
symblic constants LT, LE, EQ, NE, GT, GE. 
I3 
WS 
if 
then 
else 
id 
hum 
< 
< = 
- - 
< > 
> 
2 = 
pointer to table entry 
pointer to tabk entry 
LT 
GE 
EQ 
NE 
GT 
GE 
Fig. 3.1C Rcgularcxprcssion pattcrns for tokens. 
Transition Diagram 
As an intermediate step in the construction of a lexica! analyzer, we first pro- 
duce a stylized flowchart, cakd a rrumirion diugrum. Transition diagrams 

100 
LEXICAL ANALYSIS 
SEC. 3.4 
depict the actions that take place when a lexical analyzer is called by the 
parser to get the next token. as suggested by Fig. 3.1, Suppose the input 
buffer is as in Fig. 3.3 and the lexeme-beginning pointer points to the charac- 
ter rollowing the last lexeme found. 
We use a transition diagram to keep 
track of informarion about characters that are seen as the forward pointer 
scans the input+ We do so by moving from position to position in the diagram 
as characters are read. 
Positions in a transition diagram are drawn as circles and are called stures. 
The states are connected by arrows, called edg~s. Edges leaving state s have 
labels indicating the input characters that can next appear after the transition 
diagram has reached state S. The Iskl other refers to any character that is 
not indicated by any of the other edges leaving s+ 
We assume the transition diagrams of this section are deierminisric; that is, 
no symbol can match the labels of two edges leaving one state. Starting in 
Section 3+5, we shall relax this condition, making life much simpler for the 
designer of the lexical analyzer and, with proper tools, no harder for the 
implcmentw. 
One state is labeled the sturt ~tatc; it is the initial state of the transition 
diagram where control res~des when we begin to recognize a token. Certain 
states may have actions that are executed when the flow of control reaches 
that state. On entering a state we read the next input character. If there is 
an edge from the current state whose label matches this input character, we 
then go to the state pointed to by the edge. Otherwise, we indicate failure. 
Figure 3.1 1 shows a transition diagram for the patterns >= and z. The 
transition diagram works as follows+ Its start statc is state 0. In state 0. we 
read the next input character. The edge labeled r from state 0 is to be fol- 
lowed to state 6 if this input chars'cler is z. Otherwise, we have failed to 
remgnize either r or >=. 
Fig. 3-11. Transition diagram for >=. 
On reaching state 6 we read the next input character. The edge labeled = 
from state 6 is to be followed to state 7 if this input character is an =. Other- 
wise, the edge labeled other indicates that we are to go to statc 8. The double 
circle on statc 7 indicates that it is an accepting state, a state in which the 
token r = has k e n  found. 
Notice that the character > and another extra character are read as we fol- 
low the sequence of edge$ from the start state to the accepting state 8. Since 
the extra character is not a part of the relational operator r ,  we must retract 

SEC, 3.4 
RECOGNITION OF TOKENS 
101 
the forward pointer one character. We use a * to indicate slates on which this 
input retraction must take place, 
In general, there may be several transition diagrams, each specifying a 
group of tokens. If failure w a r s  while we are following one transition 
diagram, then we retract the forward pointer to where it was in the start state 
of this diagram, and activate the next transition diagram, Since the lexerne- 
beginning and forward pointers marked the same position in the start state of 
the diagram, the forward pointer is retracted to the position marked by the 
lexerne-beginning pointer. If failure occurs in all transition diagrams, then a 
lexical error has been detected and we invoke an error-recovery routine. 
Example 3,7. A transition diagram for the token &p 
is shown in Fig. 3.12. 
Not ice that Fig+ 3 + i  1 is a part of this more compla~ transition diagram, 
a 
Fig* 3.12. Transition diagram for rclalional opcrators. 
Example 3.8, Since keywords are sequences of letters, they are exceptions to 
the rule that a sequence of letters and digits starting wilh a letter is an idtnti- 
fier. Rather than e n d e  the exceptions into a transition diagram, a useful 
trick is to treat keywords as special identifiers, as in %ction 2.7, When the 
accepting slate in Fig. 3.13 is reached, we execute some c d e  to determine if 
the lexeme leading to the accepting state i s  a keyword or an identifier. 
Fig. 3.13. Transition diagram for idcntificrs and kcyword~. 

102 
LEXICAL ANALYSIS 
SEC. 3.4 
A simple technique for separating keywords from idehtifiets is to initialize 
appropriately the symbol table in which information abut identifiers is saved. 
For the tokens of Fig, 3.10 we need to enter the strings 5f, then, and else 
into the symbol table before any characters in the input are seen. We also 
make a note in the symbol table of the token to be returned when m e  of these 
strings is recognized. The return statement next to the accepting state in Fig. 
3.13 uses geitoktt() and i n m l U ( )  to obtain the taken and attribute-value, 
respectively, to be returned. The p r d u r c  instaUd() has access to the 
buffer, where the identifier lcxeme has been I w t e d .  The symbol table is 
examined and if the lexerne is found there marked as a keyword, instai_id() 
returns 0. if the Iexeme is found and is a program variable, imtdIid(l 
returns a pointer to the synhti table entry. If the laerne is not found in the 
symbol table, it is insrailed as a variable and a pointer to ,the newly created 
entry is returned. 
The procedure gerr&rr() similarly I d s  for the \exerne in rhe symbd table. 
If the lexeme is a keyword, the corresponding token i s  returned; otherwise, 
the token M is returned. 
Note that the transition diagram does not change if additional keywords are 
to be recognized; we simply initialize the symhl table with the strings and 
tokens of the additional keywords. 
o 
The technique OF placing keywords in the symbol table is almost essential if 
the kxical analyzer is mded by hand. Without doing so, the number of states 
in a lexical analyzer for a typical programming language is several huddred, 
while using the trick, fewer than a hundred states will probably suffloe. 
E 
digit 
digit 
digit 
Flg. 3.14, Transition diagrams for unsigned numbers in Pam!. 
Example 33. A number of issues arise when we construct a recognizer for 
unsigned numbers given by the regular definitidn 

SeC. 3.4 
RECOGNITION OF TOKENS 
103 
Note that the definition is of the form digits fraction'! exponent? in which 
fmtt3on and exponent are optional. 
The lexeme for a given token must be the longest possible, For example, 
the lexial analyzer must not stop after seeing 12 or even 12.3 when the 
input is 92.3E4, Starting at states 25, 20, and 12 in Fig. 3.14, accepting 
states will be reached after 12, 12.3, and 12.334 are seen, respctively, 
provided 12.3E4 is followed by a non-digit in the input. The transition 
diagrams with s t m  states 25, 20, and 12 are for did&, digits h d h ,  and 
digibfrachn? expomnt, respectively, so the start states must be trled in the 
reverse order 12, 20, 25. 
The action when any of tbe accepting states 19, 24, or 27 is reached is to 
call a procedure insiaLwn that enters the h e m e  into a table of numbers and 
returns a pointer to the created entry. The lexical analyzm returns the token 
mum with this pointer as the lexical vallre. 
13 
Information about the language that is not in the regular definitions of the 
tokens a n  be used to pinpoint errors in the input. For example, on input 
1. ex, we fail in states 34 and 22 in Fig. 3. t4 with next input character & +  
Rather than returning the numkr 1, we may wish to report an error and cvn- 
tinue as if the input were I . 0  x. Such knowledge can also be used to sim- 
plify the transition diagrams, because error-handling may be used to recover 
from some situations that would otherwise lead to failure. . 
There art several ways in which the redundant matching in the transition 
diagrams of Fig. 3.14 can be avoided. Ome approach is to rewrite the irami- 
tion diagrams by combining- them into one, a nontrivial task iin general. 
Anotber is to change the response to failure during the process of following a 
diagram, An approach explored later in this chapter allows us to pass through 
several acceptjng states; we revert back to the last accepting state that we 
passed through when failure occurs. 
Example 3.10, A sequence of transition diagrams for all tokens of Example 
3.8 is obtained if we put together the transition diagrams of Fig. 3.12, 3.13, 
and 3-14, Lower-numbered start stares are to be attempted before higher 
numbered states. 
The only remaining issue wncerns white space. The treatment af ws. 
representing white space, i s  different from that of the patterns discussed above 
because nothing is returned to the parser when white space is found in the 
input. A transition diagram recognizing ws by itself is 
Nothing is returned when the accepting state is reached; we merely go back to 
the start state of the first transition diagram to lmk for another pattern. 

104 
LEXlICAL ANALYSIS 
SEC. 3.4 
Whenever possible, it is ktter to look for frequently occurring tokens 
before less frequently occurring ones, because a transition diagram is reached 
only after we faif on all earlier diagrams. Since white space is expected to 
uccur frequently, putting the transition diagram for white space near the 
beginning should be an improvement over testing for white space at the end. EI 
Implementing a Transition Magrm 
A sequence of transhim diagrams can be converted into a program to look for 
the tokens specified by the diagrams. We adopt a systematic approach that 
works for all transition diagrams and constructs programs whose size i s  pro- 
portional to the n u m h  of states and edges in the diagrams. 
Each state gets a segment of code. If there are edges leaving a state, then 
its code reads a character and selects an tdge to fdlow, if possible, A func- 
tion nextchar l 1 is used to read the next character from the input buffer, 
advance the forward pointer at each all, and return the character read."i 
there is an edge labeled by the character read, or labeled by a character class 
containing the character read, then mntroI is transferrtd to the code for the 
state pointed to by that edge. If there is no such edge, and the current state i s  
not one 1h3t Indicates a token has been found, then a routine f a i l  I 1 is 
invoked to retract the forward pointer to the psirim of the beginning pointer 
and to initiate a search for a token specified by the next transition diagram. 
If there are no other transition diagrams to try, fail( 1 calls an error- 
recovery routine. 
To return tokens we use a global variable IexicaLvalue, which is 
assigned 
the 
pointers returned 
by 
functions 
instal L i d  I 1 
and 
i n s t a l l ~ u m I )  
when an identifier or number, respectively, is found. The 
token class is returned by the main procedure of the lexical analyzer, called 
nexttoken[ 1. 
We use a case statement to find the start state of the next transition 
diagram. tn the C implementation in Fig. 3.15, two variables state and 
start keep track of the present state and the starting state of the current 
transition diagram. The state numbers in the d
e
 are for the transition 
diagrams of Figures 3.12 - 3.14. 
Edges in transilion diagrams are traced by repeatedly selecting the code 
fragment for a state and executing the code fragment to determine the next 
state as shown in Fig. 3.16. We show the mbe for state 0, as modified in 
E~arnple 3.10 to handle white space, and the d
e
 for two of tht transition 
diagrams from Fig. 3.13 and 3.14. Note that the C construct 
repeats srmi "forever," i.e.+ until a return occurs. 
" A more cffwknl implemcnlation would uw aa in-line macro in p
h
 of tho function 
ntxtchaz ( I .  

SEC. 3.5 
A LANGUAGE FOR SPECIFYING LEXICAL ANALYZERS 105 
int state = 0, start = 0; 
int lexical-value ; 
/* to *returnn second component of token */ 
int f a i l  I ) 
i 
forward = tokedeginning; 
switch (start) { 
case 0: 
s t a r k  = 9; break; 
case 9: 
start a 12; break; 
case 12: 
start = 2Q; break; 
case 20: 
start = 25; break; 
case 25: 
recwer 1 ) ; break; 
default: 
J'* compiler error *I 
1 
return start; 
J 
Fig. 3.15. C d
c
 
IQ find ncxl start statc. 
Since C does not allow both a token and an attribute-ualuc to be returned. 
install-id l ) and i n s t a l l ~ u m  
( ) appropriately set some global variable 
to the attribute-value corresponding to the table entry for the id or num in 
quest ion. 
If the implementation language does not have a case statement, we can 
create an array for each slate, indexed by characters. I f  state 1 is such an 
array, then s m e  llcl is a pointer to a piece of d e  that must be executed 
whenever the lookahead character is c. This code would normally end with a 
gota to code for the next state. The array for state s is referred to as the 
indirect transfer table for s. 
3.5 A LANGUAGE FOR SPECIFYUNG LEXICAL ANALYZERS 
Several tools have h e n  built for constructing lexical analyzers from special- 
purpose notations based on regular expressions. We have already seen the use 
of regular expressims for specifying token pat terns. Befort we consider algo- 
rithms for compiling regular expressions into pattern-matching programs, we 
give an example of a tool hat might use such an algorithm. 
In this section, we describe a particular tool, called k x ,  that has been 
widely used to specify lexical analyzers for a variety of languages. We refer 
to the tool as the k
x
 iwnpikr, and to its input specification as [he Lex 
bngua~e. Discussion of an existing tool will allow us to show how the specifi- 
cation of patterns using regular expressions can be combined with act ions. 
e.g., making entries into a symbol table, that a lexical analyzer may be 
required to perform. 
Lex-like specifications can be used even if a Lex 

token nexttoken( 1 
{ 
whilell) I 
switch (statel { 
case 0: 
c = nextchar ( ) ; 
/+ c is lookahead character */ 
if Ic==blank ! !  c==tab 
e==newline) { 
state = 0 ;  
lexerneibeginning++; 
/r advance beginning of lexeme */ 
1 
else i f  ( e  == ' < ' I  
s t a t e  = 1; 
else if [ c  == '3'1 
s t a t e  = 5 ;  
else if {c == '>') 
state = 6; 
else state = fail( 1 ; 
break; 
.,. /+ eases 1-8 here */ 
ease 9: 
c = nextchart 1; 
if {isletterlc)) state = 10; 
else state = f a i l ( ) ;  
break; 
case 10: 
c = nextchar0; 
if [isletterIc) 
1 state = 10; 
else if (isdigit(c)) s t a t e  = 10; 
else state - 11; 
break; 
case 1 1 : retract { 1 1 ; h s t a l L 5 d  } ; 
return ( gettoken( ) 1; 
. . . /+ cases 12-24 here */ 
ease 25: 
c = nextchar0; 
if (isdigit(c)S s t a t e  = 26; 
else state = fail ( 1 ; 
break ; 
case 26: 
c = nextchartl; 
if IisdigitIc)) state = 26; 
else state = 27; 
break; 
case 27: 
r e t r a c t ( ? ] ;  i n s t a l l - n u l l ;  
return I NUM 1 ; 
1 
1 
1 
Fig. L16. C codc for Icxical snaly zer. 

SEC. 3.5 
A LANGUAGE FOR SPECIFYING LEXICAL ANALYZERS 
107 
compiler i s  not available; the specifications can be manually ttranswihd into a 
working program using the transiiion diagram techniques of the previous sec- 
tion. 
Lex is generally used in the manner depicted in Fig. 3. I f .  First, a specifi- 
cat ion of a lexical analyzer is prepred by creating a program lex . I  in tht 
Lex language. Then, lex. 1 is run through the Lex wmpiler to produce a C 
program lrx. yy . e. The program lex. yy . c consists of a tabular represen- 
tation of a transition diagram mnstructed from the regular expressions of 
l e x .  1, together with a standard routine that uses the table to recognize lex- 
ernes, The actions assmiat& with regular expressions in lcx . 1 are pieus of 
C code and are carried over directly to lex. yy . c. Finally, lex. yy . c i s  tun 
through the C compiler to produce an object program a.aut, which is the 
lexical analyzer that transforms an input stream into a sequence of tokens. 
Ler 
k l e x . y y . o  
program 
compiler 
lex*yy+c 
compiler 
Fig. 3.17. Creating a kxical analyzer with Lex. 
A Lex program consists of three parts: 
declarations 
%% 
translation rules 
%% 
auxiliary prwcdures 
The declarations sation includes declarations of vartables, manifest constants. 
and regular definitions. ( A  manifest constant is an identifier that IS deciarcd 
to represent a constant.) The regular definitions are statements similar to 
those given in Section 3.3 and are used as components of the regular expres- 
sions apparing In the translation rules. 

The translation rules of a Lex program are statements of the form 
p 
{ action I } 
p2 
{ action 2 ) 
. . -  
.
+
+
 
pn 
( action, ) 
where each p, is a regular expression and each ocriorri is a program fragment 
describing what action the lexical analyzer should take when pattern pi 
matches a Iexeme. In Lex, the actions are written in C; in general, however, 
they can be in any implementation language, 
The third section holds whatever auxiliary procedures are needed by the 
actions. 
Alternatively, these procedures can be compiled separately and 
loaded with the lexidal analyzer. 
A lexical analyzer created by Lex behaves in concert with a parser in the 
following manner. When activated by the parser, the lexical analyzer begins 
reading its remaining input, one character at a time, until it has found the 
Imgest prefix of the Input that is matched by one of the regular expressions 
pi, Then, it executes acrim;. Typically, actioni will return control to the 
parser. Hwcver, if it does not, then the lexical analyzer proceeds to find 
more lexernes, until an action causes control to return to the parser. The 
repeated search for lexemes un~il an explicit return allows the lexical analyzer 
to process white space and comments conveniently. 
The iexical analyzer returns a single quantity, the token, to the parser. To 
pass an attribute value with information about the kxerne, we can set a global 
variable called yylval . 
Emmpk 3J1, Figure 3.18 is a Lex program that recognizes the tokens of 
Fig. 3.10 and returns the token found. A few observations abut the code 
will introduce us co many of the important features of Lex. 
In the declarations section, we see (a place for) the declaration of certain 
manifest constants used by the translation rules.4 These declarations are sur- 
rounded by the special brackets %{ and % I .  Anything appearing between 
these brackets is copied directly into the lexical analyzer l s x  . yy . c, and is 
not treated- as part of the regular definitions or the translation rules. Exrctly 
the a m e  treatment is amrded the auxiliary procedures in the third seclion. 
In Fig. 3.18, there are two procedures, install-id and installxum, that 
are used by the translation rules; these procedures will k copied into 
1ex.yy.c verbalim. 
Also included in the definitions section are same regular definitions. Each 
such definition consists of 3 name and a regular expression denoted by that 
name. For example, the first name defined is dalim; it stands for the 
It i6 m m s n  for tho program 1cx.yy.c to be used as a sskoulint of a parser generated by 
Yam, a parser pneratoe to h discussed in Chapttr 4. In this case, the declaration of the manifst 
mmnts would be provided by.thc parser, whtn it is compiled with the program 1ex.y~. 
c .  

SEC. 3.5 
A LANGUAGE FOR SPECIFYING LEXICAL ANALYZERS 
109 
961 
/* definitions of manifest constants 
LT, LE, EQ, NE* GT, GEB 
IF, THEN, ELSE, ID, NUMBER, RELOP */ 
/+ regulax 
Uelim 
ws 
letter 
d i g i t  
id 
number 
definitions *I 
[ \th] 
idelim)+ 
[ A-Za-z ] 
10-91 
{letter)Iiletter)!{digit))+ 
Idigit)+{\.{digit)+)?IEC+\-]?Idigitl+)? 
I/* no action and no return *J') 
{return( IF 1 ; 1 
{rcturnITHBN);) 
{returnIELSE);} 
{yylval = i n s t a l L i d 0 ;  return(ID1;) 
{yylval = i n s t a l l ~ m (  
1; return(~~mE~);l 
{yylval = LT; r e t u r n ( ~ ~ ~ 0 ~ 1 ; )  
(yylval = LE; return[RELX)P);) 
{yylval = EQ; return(RELOP1; 
{yylval = NE; ~ ~ ~ U ~ ~ I R E L O P ) ; }  
{yylval = GT; return[~E~O~);) 
(yylval = GE; return[RELO~);) 
f nstalLidI 1 i 
I* procedure to install the lexeme, whose 
f i r s t  character i s  pointed to by yytext and 
whose length is yyltng, into the symbol table 
and return a pointer thereto */ 
1 
i n s t a l l ~ u m 0  E 
t'* 
similar procedure to i n s t a l l  a lexeme that 
is a number */ 
1 
Irig. 3.18. Lcx program for thc tokens of Fig. 3-10. 

Ila 
LEXICAL ANALYSIS 
SEC. 3.5 
character class I \t\n], that is, any of the three symbols blank, tab 
(represented by \t), or newline (represented by Xn). The second definitiort is 
of white space. denoted by the name ws. White space is any sequence of one 
or more delimiter characters+ Noti= that the word delim must be sur- 
rounded by braces in Lex, to distinguish it from the pattern consisting of the 
five letters delim. 
In the definition of letter, we see the use of a character class. The short- 
hand [ A-Za-z 1 means any of the capital letters A through z or the hwer- 
case letters a through z, The fifth definition, of i d ,  uses parentheses, which 
are metasymbols in Lex, with their natural meaning as groupers. Similarly, 
the vertical bar is a Lex metasymbol representing union. 
In the last regular definition, of number, we observe a few more details, 
We see ? used as a metasymhl, with its customary meaning of "zero or one 
occurrences of." We also note the backslash used as an escape, to let a char- 
acter that is a Lex metasymbol have its natural meaning, In particular, the 
decimal point in the definition of number is expressed by \. because a dot by 
itself represents the character dass of all characters except the newline, in Lex 
as in many UNlX system programs that deal with regular expressions. Irt the 
character class [ + \ - I ,  
we placed a backslash before the minus sign because 
the minus sign standing for itself could be confused with its use to denote a 
range, as in [A-=I.$ 
There is another way to cause characters to have their natural meaning, 
even if they arc metasymbb of Lex: surround them with quotes. We have 
shown an example of this convention in the translation rules section, where 
the six relational operators are surrounded by quotes+6 
Now, let us consider the translation rules in the section following the first 
%%. The first rule says that if we see ws, that is, any maximal sequmce of 
blanks, tabs, and newlines, we take no action. In particular, we do not return 
to the parser. Recall that the structure of the lexical analyzer is such that it 
keeps trying to recognize tokens, until the action associated with one found 
causes a return. 
The second rule says that if the letters if are wen, return the token IF, 
which is a manifest constant representing some integer understwd by the 
parser to be the token if. The next two rules handle keywords then and 
else similarly. 
In the rule for id, we see two statements in the associated action. First, the 
variable yylval is set to the value returned by procedure install-id; the 
definition of that procedure is In the third ,section. yylval is a variable 
Acrually, Lclr hodkti thc charactcr class [+- 1 mrccaly withut ihc ba&s!ash, 
~ C P U S C  fhc 
minus sign appearing a1 the cnd cannot represcnl a rangc. 
WC did SO bccaug ;C and r aw Lex mctasymbls; thcy surround lhc nemcs of '"sates," cnnbling 
Lcx to ~ h n g c  
state whcn c m a n t ~ i n g  
ccrhin tokens, mch as mmmcnts or guorcd strings, that 
must k trcatcd diffcrcntly from thc usual text. Thcrc is no n c d  ro surround thc qua1 sign by 
yurms, but ncitkr is it fimbiddcn. 

SEC. 3.5 
A LANGUAGE FOR SPECIFYING LEXICAL ANALYZERS 
11 1 
whose definition .appears in the Lex output 1ex.y-y. e, and which is also 
available to the parser. The purpose of yylval i s  to hold the lexical value 
returned, since the second statement of the action, return ( ID) . can only 
return a code for the [ k e n  class. 
W; do not show the details of the code for i n s t a l l i d .  However, we 
may suppose that it looks in the symbol table for the h e m e  matched by the 
pattern id. Lex makes the lexerne available to routines appearing in the third 
section through two variables yytext and yyleng. The variable yytext 
corresponds to the variable that we have been calling I~xemehcginnistg, that 
is, a pointer to the first character of the lexeme; yyleng is an integer telling 
how Long the heme is. For example, if ins tall-id faiis to find the identi- 
fier in the symbol table, it might create a new entry for it. The yyleng char- 
acters of the input, starting at yytext, might be copied into a character array 
and delimited by an end-of-string marker as in Section 2.7. The new symbl- 
table entry wwld point to the beginning of this copy. 
Numbers are treated similarly by the nexl rule, and for the last six rules, 
yylval is uwd to return a code for the particular relational operator found, 
while the actual return value is rhe code for token relop in each case. 
Suppose the lexical analyzer retiuking from the program of Fig. 3.18 is 
given an input consisting of two tabs, the letters i f ,  and a blank. The ~ w o  
tabs are the longst initial prefix of the input matched by a pattern, namely 
the pattern ws, The action for ws is to do nothing, so the iexical analyzer 
moves the lexemakginning pointer, yytcxt, to the i and begins to search 
for another token. 
The next lexerne to k matched is if. Note that the patterns if 
and {id) 
both match this Itxeme, and no pattern matches a longer string. Since the 
pattern for keyword if precedes the pattern for identifiers in the list of Fig. 
3,18, the mnflicr is resolved in favor of the keyword. In general, this 
ambiguity-resolving strategy makes it easy to reserve keywords by listing them 
ahead of the pattern for identifiers. 
For another example, suppose <= are  he first two characters read. While 
pattern < matches the first character, it is not the longest pattern matching a 
prefix of the input. Thus Lex's strategy of selecting the longest prefix 
matched by a pattern makes it easy to rewtve the conflict between < and <= 
in the expected manner - by choosing <= as the next token. 
0 
As we saw in Section 3.1, lexical analyzers for certain programming language 
constructs need to look ahead beyond the end of a lelterne before they can 
determine a token with certainty. Recall the example from Fortran of the pair 
of statements 
In Fortran, blanks are not significant outside of cornmenis and Hollerith 

1 12 
LEXICAL ANALYSIS 
SEC, 3.5 
strings, so suppose that all rernuvabic blanks are stripped before lexical 
analysis begins. The above statements then appear to the lexical anaiyzer as 
In the first statement, we cannot tell until we see the decimal p i n t  that the 
string W is part of the identifier D05I. lo the second statement, DO i s  a key- 
word by itself, 
In Lex, we can write a pattern of the form r ,/rZ, where r l  and r2 are reg- 
ular expressions, meaning match a string in r , ,  but only if followed by a 
string in r2. The regular expression rz after the lookahead operator / indi- 
cates the right context for a match; it is used only to restrict a match, not to 
be part of the match. For example, a Lex specification that recognizes the 
keyword IX> in the context above is 
With this specification, the lexical analyzer will look ahead in its input buffer 
for a sequence of letters and digits folkweb by an equal sign followed by 
letters and digits followed by a comma to be sure that it did not have an 
assignment statement. Then oniy the characters D and 0 ,  preceding the Imka- 
head operator / would be part of the heme that was matched. After a suc- 
cessful match, yytrxt points to the D and yyleng = 2. Note that this sim- 
ple Icmkahead pattern allows DO to k recognized when fotlowcd by garbage, 
like z 4 = 6 ~ ,  
but it will never recognize b0 that is part of an identifier. 
Example 3.12. The lookahead operator can be used to cope with another dif- 
ficult lexical analysis problem in Fortran: distinguishing keywords from identi- 
fiers. For example, the input 
is a perfectly. good Fortran assignment statement, not a logical if-statement. 
One way to specify the keyword IF using Lex is to define its possible right 
contexts using the lookahead operator. The simple form of the logical if- 
statement is 
Fortran 77 introduced another form of the logical if-statemen t: 
We note that every unlakled Fortran statement begins with a letter and that 
every right parenthesis u , d  for subscripting or operand grouping must be fol- 
lowed by an operator symbol such as =, +, or comma, mother right 

SEC. 3.4 
FINITE AUTOMATA 
1 13 
parenthesis, or the end of the statement. Such a right parenthesis cannot be 
followed by a letter. In this situation, to confirm that IF is a keyword rather 
than an array name we scan forward looking for a right parenthesis followed 
by a letter before seeing a newline character (we assume continuation cards 
"cancel" the previous newline character). This pattern for the keyword IF 
can be Witten as 
The dot stands for "any character but newline" and the backslashes in front of 
the parentheses tell Lcx to treat them literally, not as rnctasymbols for group- 
ing in regular expressions (see Exercise 3.10). 
a 
Another way to attack the problem posed by if-~tatements in Fortran is, 
after seeing IF(, to determine whether I F  has been ddared an array. We 
scan for the full pattern indicated a b v e  only if it has been so declared. Such 
tests make the automatic implementation of a lexical analyzer from a Lex 
specification harder, and they may even cost time in the long run, since fre- 
quent checks must be made by the program simulating a transition diagram to 
determine whether any such tests must be made. It should Ix noted that tok- 
enizing Fortran is such an irregular task that it is Rquentty easier to write an 
ad hoc lexical analyzer for Fortran in a conventional programming language 
than it is to use an automatic lexical analyzer generator, 
3.6 FINITE AUTOMATA 
A recognizer for a language is a program that takes as input a string x and 
answers "yes" if x is a sentence of the language and "no" otherwise. We 
compile a regular expression into a recognizer by constructing a generalized 
transition diagram called a finite automaton. A finite automaton can bc dcttr- 
ministic or nondeterministic, where "nondeterministic" means that more than 
one transition out of a state may be possible on the same input symbol. 
Both deterministic and nondeterministic finite automata are capable of 
recognizing precisely the regular sets. Thus they both can recognize exactly 
what regular expressions can denote. However, there is a time-space tradeoff; 
while deterministic finite automata can lead to faster recognizers than non- 
deterministic automata, a deterministic finite automaton can be much bigger 
than an equivalent nondeterministic automaton. In the next section, we 
present methods for cmwerting regular expressions into both kinds of finite 
automata, The conversion into a nondeterministic automaton i s  more direct 
so we discuss these automata first, 
The examples in this section and the next deal primarily with the language 
denoted by the regular expresSion (a lb)*abb, consisting of the rset of all 
strings of 4'6 and 6's ending in abb. Similar languages arise in practice. For 
example, a regular expression for the names of all files that end in +r, is of 
the furm ( . lolc)* .o, with c representing any character other than a dot or 
an o. As another example, after the opening /*, mntmertts in C consist of 

114 
LEXICAL ANALYSIS 
SEC. 3,6 
any sequence of characters ending in */, with the added requirement that no 
proper prefix ends in */. 
Nondeterministic Finite Automata 
A mndeterministic fiife a u t m t m  (N FA. for shwt) is a mathematical model 
that consim of 
I . 
a set of slcrtrs S 
2. 
a set of input symbls Z (the inpsrt .tymbl alp&#) 
3. 
a transition function move that maps state-symtml pairs to sets of states 
4. 
a state so that is distinguished as the start (or initial) state 
5 .  a set of states F distinguished as acwphtg (or find) slates 
An NFA can be represented diagrammatically by a labeled directed graph, 
called a ~ransibm gruph, in which the nodes art the states and the labeled 
edges represent the transition function. This graph loolrs like a transition 
diagram, but the same character can label two or more transitions out of one 
state, and edges can be labeled by the special symbol E as well as by input 
symbols. 
The transition graph for an NFA that recognizes the language (a Ib)*uM Is 
shown in Fig. 3.19. The set of states of the NFA is (0, 1, 2. 3) and the input 
symbol alphakt i s  {a, b). State 0 in Fig. 3.19 is distinguished as the start 
stale, and the accepting state 3 is indicated by a double circle. 
#ig. 3.19, A nondctcrministic finite automaton. 
When describing an NFA, we use the transition graph representation. In a 
computer, the transition fuwtion of an NFA can be impkmcnttd in several 
different ways, as we shall see. The easiest implementation is a trclwi~iom 
table in which there is a row for each state and r column for tach input sym- 
bol and c, if ileoessary. The entry for row i and symbol a in the table is the 
set of states (or more likely in practice, a pointer to the set of states) that can 
be reached by a transition from state i on input a. The transition table for the 
NFA of Fig. 3.19 is Bown in Fig. 3.20. 
The transition table representation has the advantage that it provides fasl 
access to the transitions of a given stare on a given character; its disadvantage 
is that it can take up a lot of space when tht input alphabet is large and most 
transitions are to the empty set. 
Adjacency list representations of the 

FINITE AUTOMATA 
15 
Flg. 3.20. Transition table for the finite automaton of Fig. 3.19. 
transition function provide more compact implementations, but access to a 
given transition is slower. It should k clear that we can easily cmvwt any 
one of these impltmentations of a finite automaton into another. 
An NFA accepts an input string x if and only if there is some path in the 
transition graph from the start state to some accepting state, such that the 
edge labels along this path spell out x. The NFA of Fig. 3+19 accepts the 
input strings abb, mM, hbb, aaabb, 
+ 
- . For example, mbb is accepted by 
the path from 0, fallowing the edge labeled u to state 0 again, jhen to states 1, 
2, and 3 via edges iabdeb a, b, and b, respectively. 
A path can be represented by a sequence of state transitims called moves. 
The following diagram shows the moves made in acmpting the input string 
aabb: 
In general, more than one sequence of m v e s  can lead 40 an accepting state. 
Notice that several otbcr sequences of moves may te made on the input string 
&, but none of the others happen to end in an accepting state. For exam- 
ple, another sequence of moves on input aabb keep reentering the non- 
accepting state 0: 
The h g u h g e  &fined by an NFA is the set of input strings it accepts. It is 
not hard to show that the NFA of Fig. 3.19 accepts (a Ib)*ubb. 
Example 3.13, In Fig. 3.21, we see an NFA to remgnk m* lbb*. String 
aaa is accepted by moving ,through skates 0, I, 2, 2, and 2. The lab& of 
thew edges are r, a, a,and a, whose concatenation i s  wa. Note that ~ ' s  
"disappear " in a concatenation. 
a 
A beierminisric firtite automaton (DFA, for short) is a special case of a non- 
deterministic finite autorn&n in which 
1. 
no state has an etransitim; i.e., a transition on input r, and 

1 !6 LEXICAL ANALYSIS 
start 
Fig. 3.21. NFA accepting art* ibh*. 
2. 
for each state s and input symbol u, there is at most one edge labeled a 
leaving s. 
A deterministic finite automaton has at most one transition from each state 
on any input. If we are using a transition table to represent the transition 
function of a DFA, then each entry in the transition table i s a  single state, As 
a consequence, it i s  very easy to determine whether a deterministic finite auto- 
maton accepts an input string, since there is at most one pet h from the start 
state labeled by that string. The following algorithm shows how to simulale 
the behavior of a DFA on an input string. 
Algorithm 3.1 + Simuht ing a DFA . 
hpr. An input string x terminated by ail end-uf-file character mf. A DFA D 
with slart state so and .set of accepting states F. 
Outpi. The answer "yes" if D accepts x; "w" otherwiw. 
Methud Apply the algorithm in Fig. 3,22 to the input string x. The function 
move{s, c )  gives the state to which there is a transition from state s on input 
character c. The function ncxtchar returns the next, character of rhe input 
string x+ 
13 
Fig. 322. Simulating a DFA . 

SEC. 3.6 
FINITE AUTOMATA 
I 17 
Example 3.14. In Fig. 3.23, we see the transition graph of a deterministic fin- 
ite automaton accepting the same language (a /b)*abb as that accepted by the 
NFA of Fig. 3-19. With this DFA and the input string a M b ,  Algorithm 3,l 
follows the sequence of states 0, 
1, 2, 1, 2, 3 and returns "yes". 
Fig. 3.27. DFA accepting {n Ih)*rrM. 
Convtrsion of an NFA Into a DFA 
Note that the NFA of Fig. 3.19 has two transitions from state 0 on input a; 
that is, it may go to state 0 or I .  Similarly, the NFA of Fig+ 3.21 has two 
transitions on r from stare 0. While we have not shown an example of it, a 
situation where we could choose a transition on E or on a real input symbol 
also causes ambiguity. These situations, in which the transition firnclion is 
multiualued, make it hard to simulate an NFA with a computer program. The 
definition of acceptance merely asserts that there must be some path labeled 
by the input siring in question leadiog from the start state to an accepting 
state. But if there are many paths that spell out the same input string, we 
may have to consider them all before we find one that leads to acceptance or 
discover that no path leads to an accepting state. 
We now present an algorithm for constructing from an NFA a DFA that 
recognizes the same language. This algorithm, often called the srrbsrt con- 
struction, is ,useful for simuiating an NFA by a cornpuler program. A closely 
related algorithm plays a fundamental role in the mnstructton of LR parsers 
in the next chapter. 
In the transition table of an NFA, each entry is a set of states; in the transi- 
t ion table of a DFA, 
each entry is just a single state. The general idea behind 
the NFA-to-DFA construction is that each DFA state corresponds to a set of 
NFA states. The DFA uses its state to keep track of all possible states the 
NFA can be in after reading each input symkl. That is to say, after reading 
input u l u 2  . . + a,, the DFA is in a state that represents the subset T of the 
srates of the NFA that are reachable from the NFA's start state along wmc 
path labeled u ,  a2 
+ a,+ 
The number of states of the DFA can be expnen- 
tial in the n u m b  of states of the NFA, but in practice this worst case occurs 
rarely. 

1 18 
LEXlCAL ANALYSIS 
SEC. 3.6 
Algorithm 3.2. {Subset constructwfi.) Constructing a DFA from an N FA. 
hpt. An NFA N. 
Oucpur. A DFA D accepting the same language. 
Mehod. Our algorithm constructs a transition table Dtran for D, Each DFA 
state is a set of NFA states and we construct Dtrm srr that D will simulate "in 
parallel" d l  possible moves N can make on a given input string. 
We use the operations in Fig. 3.24 to keep track of sets of NFA states (s 
represents an NFA state and T a set of NFA states). 
1 on r-transitions abne. 
1 
OPERATI~H 
c-cf,sure(x) 
mdT, a) I Set d NFA states to 'which there is a transitioo on input 
DESCRIITJOH 
Set of NFA states reachable from NFA state s m r- 
transitions alone, 
I symbl u from m
e
 
NFA state s in T. 
Fig. 324. Operations on NFA states. 
Before it sees the first input symbol, N can be in any of the states in the set 
r-closurIso), where so is the start state of N, Suppose that e~actty the states 
in set T are reachable from so an a given sequence of input symbols, and k t  a 
be the next input symbol. On seeing a, an move lo any of the states in the 
set m e ( T ,  a). When we allow for r-transition$, N can be in m y  of the states 
in E-clostrre(mve(T, a)), after seeing the a. 
initially, ~-ctossrr~(s~) 
is thc only statc in Dsmes and it is unmarked; 
whik there is an'unmarked state Tin D&ks do begin 
mark T; 
for each input symbol n do k g i i  
O := ~ - c ~ o s u c ~ ( ~ w ( T ,  
a)); 
iC U is not in Dstarcx them 
add U as an unmarked statc to Dstates, 
Dlrun IT, u ) : = U 
end 
end 
Fig. 325. The subset construction. 
We construct Dsrars, the set of states of D, and Dtran, the transition table 
for D, in the following manner. Each state of D corresponds to a set of NFA 

SEC. 3.6 
FINITE AUTOMATA 
1 19 
states that N could be in aficr reading some sequence of input symbols indud- 
ing all possible €-transitions before or after symbol9 are read. The start state 
of D is t-closure(sO}. Srates and transitions are added to D using the algo- 
rithm of Fig. 3.25. A state of D is an accepting state if it is a set of NFA 
states containing at least one accepting state of N. 
push all states in T onto mck; 
initialize s-c/sctre(T) to T; 
while $ruck is not cmpty do begin 
p o ~  
r, the top ckmcnt, off d srack* 
for each stak u with an cdge from t to u labclcd c da 
if u is not in c-cioxure(T) d+ &in 
add u to r-c-hurp(T); 
push u onto stud 
end 
end 
Fig. 3.26. Computation of &-cd~swe 
The computation of r-cdosure(T) is a typical prwss of searching a graph for 
ndes reachable from a given set of nodes, In this case the states of T are the 
given set of nodes, and the graph consists of just the r-labded edges of the 
NFA. A simple algorithm to compute e-clusure(T) uses a stack to hold states 
whose edges have not been checked for elabekd transitions. 
Such a pro- 
cedure is shown in Fig. 3.26. 
Example 3,15. 
Figure 3.27 shows another NFA N ampting the language 
(a Ib)*abb. 
(It happens to be the one in the next section, which will be 
mechanically mnstructd from the regular expression.) Let us apply Algo- 
rithm 3.2 to N. The start state of the equivahe DFA is E - c ~ ~ s Y P ~ ~ ) ,  
which i s  
A = (0, 1,2,4,7}, since these are exactly the states reachable from state 0 via 
a path in which every edge is labeled t. Note that a path can have no edges, 
so 0 is reached from itself by such a path. 
The input symbl alphabet here is {a, b}, The algorithm of Fig, 3.25 tells 
us to mark A and then to compute 
We first compute mave(A, u), the set of states of N having transitions on u 
from members of A, Among the states 0, I, 2, 4 and 7, only 2 and 7 have 
such transitions, to 3 and 8, so 
Let us call this set 3. Thus, D m n  lA, a J = B. 
Among the states in A, only 4 has a transition on b to 5 ,  so the DFA has a 
 rans sit ion on b from A to 

120 
LEXICAL ANALYSIS 
Fig. 3.n. NFA N for (a lb)*aM. 
Thus, Dtrm(A, B I = C. 
If we continue this p m c m  with the now unmarked sets B and C, we even- 
tually reach the point where all sets that are states of the DFA are marked. 
This is an& since thete are "only" 2" different subsets of a set of eleven 
states, and a set, once marked, i s  marked forever. The five different sets of 
states we actually wnstruct are: 
A = {O, 1, 2,4, 7) 
D = {I, 2, 4,5, 6, 7, 9) 
B = { 1 , 2 , 3 , 4 , 6 , 7 , 8 )  E={l,2,4,5,6,7, 10) 
C = {I, 2,4, 5, 6,7} 
State A is the start state, and state E is the only accepting state. The complete 
transition tabk Dtran is shown in Fig. 3.28. 
Fb. 3.28. Transition tablc Dtmt for DPA. 
- 
-ATE 
A 
B 
C 
D 
E 
Also, a transition graph for the resulting DFA i s  shown in Fig. 3.29. 
Ic 
should be noted that the DFA of Fig; 3.23 also accepts (a I b)*& and has one 
-- 
~ 
~~ 
INPUT SYMBOL 
P 
B 
3 
B 
B 
B 
6 
C 
D 
C 
E 
C 

FROM A REGULAR EXPRESSlON TO AN NFA 
121 
Fig. 3.29. R
w
 
It of applying thc subset mnMruction to Fig. 3.27. 
Fewer state. We discuss the question of minimjzation of the nunikr of states 
of a-DFA in Section 3.9. 
i l  
3.7 FROM A REGULAR EXPRESSiON TO AN NFA 
There are many strategies for building a rcmgnizer from a regular expression, 
each with its own strengths and weaknesses. One strategy that has been used 
in a number of text-editing programs is to construct an NFA from a regular 
expression and then to simulate the khavicir of the NFA on an input string 
using Algorithms 3,3 and 3+4 of this section. If run-time speed is essential, 
we can convert the NFA into a DFA using the subset construction of the pre- 
vious section. in Section 3.9, we see an alternative implementation of a DFA 
from a regular expression in which an intervening NFA is not explicitly con- 
strucied. This section concludes with a discussion of rime-space tradeoffs in 
the implementation of recognizers based on NFA and DFA, 
We now give an algorithm to construcr an NFA from a reguiar expression. 
There are many variants of this algorithm, but here we present a simple vtr- 
sion that is easy to implement. The algorithm is syntax-directed in that it uses 
the syntactic .structure of the regular expression to guide the canstruct'ron pro- 
cess. The cases in the algorithm follow the cases in the definition of a regular 
expression. We First show how to construct automata to recognize E and any 
symbol in the alphabet. Then, we show how to construa automata for cxpres- 
sions containing an. alternation, concatenation, or Kleene closure operator. 
For example, for the expression r Is, we construct an NFA inductively from 
the NFA's for r and s. 
As the construction proceeds, each step introdurns at most two new states, 
so the resulting NFA constructed for a regular expression has at most twice as 
many starts as there are symbols and operators in the regular expression. 

122 
LEXICAL ANALYSIS 
SEC. 3.7 
Algdthm 3,3, (Thompson's ro~tructwn+) An NFA from a regular expres- 
sion. 
Input. A regular expression r over an alphabet E. 
Output. An NFA N accepting L I r ) .  
Method. We first parse r into its mnstituent subexpressions, Then, using 
ruks ( 1 ) and (2) below, we construct NFA's for each of the basic symbols in r 
(those that are either r or an alphakt symbol) + The basic symbols wrrespond 
to parts ( 1) and (2) in the definition of s regular expression. It is important 
to understand that if a symbol a occurs several times in r, a separate NFA is 
constructed for each m
u
 
rrence. 
Then, guided by the syntactic structure of the regular expression r, we com- 
bine these NFA's inductively using rule (3) below until we obtain the NFA for 
the entire expression. Each intermediate NFA p r d u d  during the course of 
the construction corresponds to a subexpression of r and has several important 
properties: it has exactly m e  final state, no edge enters the start state, and no 
edge leaves the find state, 
For E, construct the NFA 
Here i is a new start state and f a new accepting state. Clearly, this NFA 
recognizes (€1. 
For u in Z, construct the NFA 
Again i i s  a new start state and f a new accepting state. This machine 
recap izes {a}. 
Suppse N Is) and Nit) are NFA's for regular expressions s and i. 
a) 
For the regular expression s lr, construct the following compsite 
NFA N(s]r): 

$EC. 3.7 
FROM A REGULAR EXPRESON TO AN NFA 
123 
Here i is a new start state and f a  new accepting state. There is a 
transition on E from I to the start states of N ( s )  and N(t). There is 
a transition on E from the accepting states of N ($1 and H ( r )  to the 
new accepting state f. The start and ampting slates of Nls) and 
. 
N{r) are not start or accepting states of H(slt). Note that any path 
from i to f must pass through either N{s) or N(c) exclusiveiy. Thus, 
we see that the compite NFA recclgnies L Is) U L (t ). 
b 
Far the regular expression sf, construct the composite NFA W r ) :  
The start state of N(s) becomes the start state of the composite NFA 
and the accepting state of N { t )  becomts the mepting state of the 
composite NFA, The ampting slate of N(s) is merged with the 
start state of N ( t ) ;  that is, all transitions from the start state of N(1) 
become transitions from tbe accepting state of N(s). The new 
merged date 1 0 ~ 4 s  its status as a start or accepting state in the corn- 
posite NFA. A path from I to f must go first through N(s) and then 
through N(r), so the label of that path will be a string in L(s)L(f). 
Since no edge enters the start state of N(t) or leaves the accepting 
state of N(s), there .en be no path from i to f that travels from N(i) 
back to H(s). Thus, the mmposiic NFA recognizes LIs)L(f l .  
C) 
For the regular expression s*, construct the campsite NFA hl(ss): 
' 
& 
Here i is a new start state and f a  new accepting state. In ihe corn- 
positc NFA, we can go from i to J directly, along an edge labeled t, 
representing the fact that c is in {L 
(s))*, 
or we can go from i to f 
passing through #(s) one or more times. Clearly. the composite 
NPA recognizes (L f s)) *. 
dl For the parenthesized regular expression ($1, use N (s) itself as the 
NFA. 

Every time we construct a new state, we give it rt distinct hame. In this way, 
no two states of any campnent NFA can have the same name, Even if the 
same symbol appears several times in r, we create for each instance of chat 
symbol a separate NFA with its own states. 
D 
We can verify that each step of the construction of Algorithm 3.3 produces 
an NFA that recognizes the correct language. in addition, the construction 
produces an NFA N l r )  with the folkwing properties. 
N { r )  has at most twice as many states as the number of symbols and 
operators in r. This fdbws from the fact each step of the construction 
creates at most two new states, 
N l r )  has exactly one start state and one accepting state. The accepting 
state has no outgoing thnsitions. This property holds for each of the 
constituent automata as well. 
Each state of N ( r )  has either one outgoing transition on a symbol in Z or 
at most two outgoing E-transitions. 
Example 3.16. Let us use Algorithm 3.3 to construct N(r) for the regular 
expression F = (a lb)*abb. Figure 3.30 shows a parse tree for r that is ando- 
gous to the parse trees cunstructcd for arithmetic txpressims in Section 2.2. 
For the constituent r,, the first a, we construct- the NFA 
We can now combine N(rl) and N ( r Z )  using the unicm rule to obtain the 

SEC. 3.7 
NFA for r~ = rl 1r2 
PROM A REGULAR EXPRB-N 
TO A N  NFA 125 
The NFA for (rJ) is the same as that for rj, The NFA for (rl)* is then: 
The NFA for rb= a i s  
To obtain the automaton for r 5 r 6 ,  we merge states 7 and 7', calling tht 
r&lting state 7. to obtain 
Continuing in this fashion we obtain the NFA for rll = (a]b)*& that was 
Frtst exhibited in Fig. 3.27. 
o 

126 LEXICAL ANALYSIS 
SEC. 3.7 
Two-Stack Sirnulatiam of an NFA 
! 
We now presaht an algorithm that, given an NFA N constructed by Algorithm 
3.3 and an input string x, determines whether N accepls x. The algorithm 
works by reading the input one character at a time and computing the corn- 
plete set of states thal N muld lx in after having read each prefix of the input. 
The algorithm takes advantage of the special properties of the NFA produced 
by Algorithm 3.3 to compute each set of nondeterministic states efficiently. It 
can be impkmented to run in time proportima1 to IN 1 x 1x1, where IN! i s  the 
number of states in N and 1x1 is the length of x. 
Alg~dthm 3,4, Simulating an NFA. 
. 
An NPA N ccmstructed by Algorithm 3.3 and an input string x. We 
assume x is terminated by an end-of-file character eat. N has start state so 
and set of accepting states F. 
Outpur. The answer "yes" if N accepts x; "no" otherwise. 
Medmf. Apply the algorithm sketched in Fig. 3.3 1 to the input string x. The 
algorithm in effect performs the subsel construction at run time. It computes 
a transition from the current set of states S to the next set of states in two 
stages. First, it determines mve(S, a ) ,  all states that a n  Ix reached from a 
state in $ by a transition on a, the current input character. Then, it computes 
the e-closure of move($, a), that is, all states thal can be reached from 
~ v e ( S ,  
a) by zero or more etransitions. The algorithm uses the function 
nextchr to read the characters of x, one at a time. When aii characters of x 
have h e n  seen, the algorithm returns 'yes" if an accepting state is in the set 
S of current states; "no", otherwise. 
o 
Fig. 3.31. Simuiating the NFA of Algorithm 3.3, 
Algorithm 3.4 can k efficiently implemented using two stacks and a bit 
vector indexed by NFA states. We use one stack to keep track of the current 
set of nondeterministic states and the other stack to compute the next set of 
nondeterministic slates. We can use the algorithm in Fig. 3.26 to compute the 
 dosu sure. The bit vedm can t>e used to determine in constant time whether a 

SEC. 3.7 
FROM A REGULAR EXPRESSION TO AN NFA 127 
nondeterministic state is already on a stack so rhat we do no1 add it twice, 
Once we have computed the next state on the second stack, we can intet- 
change the roles of the rwo stacks. Since each nondeterministic state has at 
most two out-transitions, each state can give rise to at most two new states in 
a transidion. Let us write I N  1 for the number of states of N. Since there can 
be at most IN[ states on a stack, the computation of the next set of states 
from the current set of states can be done in time proportional to INl. Thus, 
the total time needed to simulate the behavior of hl on input x is proportional 
to I N \  
x 1x1. 
Example 3.17. Let N be the NFA of Fig. 3.27 and let x be the string consist- 
ing of the single character cr. The start state is r-cio.swe({0)) = (0, I, 2, 4, 7). 
On input symbol II there is a transition from 2 to 3 and from 7 to 8. Thus, T 
i s  13, 8)- Taking the c-closure of T gives us the next state { I ,  2, 3,4, 6, 7, 8). 
Since none of these nondeterministic states is accepting, the algorithm returns 
'ho." 
Notice that Algorithm 3.4 does the subset construction at run-time, For 
example, compare the above transitions with the states of the DFA in Fig. 
3.29 constructed from the NFA of Fig, 3+27. The start and next state sets on 
input a correqmnd to states A and B of the DFA. 
o 
Time-Space Trsdmffs 
Given a regular expression r and an input string x, we now have two methods 
for determining whether x is in L ( r ) .  One approach is to use Atgmithm 3.3 
to construct an NFA N from r, This construction can IM done in O (  I r l )  time, 
where (r 1 is the length of r. N has at most twice as many states as (r(, and at 
most two transitions from each state, so a transition tabk for N can be stored 
in O ( l r ( )  space. We can then use Algorithm 3.4 to determine whether N 
accepts x in 0 ( 1 r 1 x Ix 1) time. Thus, using this approach, we can determine 
whether x is in L (r) in total time proportional to the length of r times the 
length of x. This approach has been used in a number of text editors to 
=arch for regular expression patterns when the target string x is generally not 
very long. 
A second approach is to construct a DFA from the regular expression r by 
applying Thompson's construction to r and then the subset wnstructioa, Algo- 
rithm 3.2, to the resulting NFA. (An implementation that avoids constructing 
the intermediate NFA explicitly is given m Section 3,9.) Implementing the 
transition function with a transition table, we can use Algorithm 3+ 1 to simu- 
late the DFA on input x in time proportional to the length of x, independent 
of the number of states in the DFA, This approach has often been used in 
pattern-matching programs that search text files for regular expression pat- 
terns.~ Once the finite automaton has been constructed, the mrching can 
proceed very rapidly, so this approach is advantageous when the target string 
x is very Long. 
There are, however, certain regular expressions whose smallest DFA has a 

128 
LEXICAL ANALYSIS 
SEC. 3.7 
numkr of states that is exponential in the size of the regular expression. For 
example, the regular expression Iu 1 b)*a (u 1 b)(a 1 b }  - . . iu 1 b ) ,  where there 
are tt - I (u Ibl's at the end, has no DFA with fewer than 2'' states, This reg- 
ular expression denotes any string of a's and &'s in which the nth character 
from the right end is an u, lt is not hard to prove that .any DFA for this 
expression must keep track of the last n characters it sees on .the input; other- 
wise, it may give an erroneous answer. Clearly, at least 2" states are required 
to keep track of all possible sequences of n a's and b's, Fortunately, expres- 
sions such as this do not occur frequently in lexical analysis applications, but 
there are applications where similar expressions do arise. 
A third approach is to use a DFA, but avoid constructing all of the transi- 
tian table by using a technique called "lazy transition evaluation." Here, 
transitions are computed at run time but a transition from a given state on a 
given character is not determined until it Is actually rrseded, The computed 
transitions are stored in a cache. Each time a transition is about to be made, 
the cache is consulted, If the transit im is na there, it i s  computed and stored 
in the cache. If the cache becomes full, we can erase some previously com- 
puted transition to make room for the new transition. 
Figure 3.32 sltmmarizes the worstcaw space and time requirements for 
determining whether an input string x is in the language denoted by a regular 
expression r using recognizers constructed from nondeterministic and deter- 
ministic finite automata. The "lazy" technique combines the space require- 
ment of the NFA method with the time requirement of the DFA approach. 
I t s  space requirement is the size of the regular expression plus the size of the 
cache; its observed running time is almost as fast as that of a deterministic 
recognizer. In same applications, the "lazy" technique is considerably faster 
than the DFA approach, because tsc~ time is wasted computing state transitions 
that are never used, 
 AUTO;^ 
SACE 1 
TIME 
a 
1.1 
1 
o( lrlx 1x1) 
DFA 
W l r 9  
O M )  
Fjg. 3.32. Spacc and timc takm to rwogniu: rcgulac cxpmssicms. 
3 3  DESIGN OF A LEXICAL ANALYZER GENERATOR 
In this section, we consider the design of a software t o 1  that automatically 
constructs a lexical analyzer from a program is the Lex language. Although 
we discuss several methods, and none is precisely identical to that used by the 
UNIX system Lex command, we refer to these programs for constructing lexi- 
cal analyzers as Lex compiler$. 
We assume that we have a specification of a lexical analyzer of the form 

EC. 3,8 
DESlGN OF A LEXICAL ANALYZER GENERATOR 
129 
where, as in M i o n  3.5, each pattern pi is a regular expression and each 
action nctimi is a program fragment that is to Be executed whenever a h e m e  
matched by pi is found in the input, 
Our problem is to construct a recognizer that looks for lexemes in the input 
buffer. If more than one pattern matches, the recognizer is to choose the 
longest lexcme matched. If there arc two or more patterns that match the 
longest lexeme, the firat-listed matching pattern is chosen. 
A finite automaton is a natural model arwnd which to buiM a lexical 
analyzer, and the one constructed by our Lelt compiler has the form shown in 
Fig. 3.33Ib). There is an input buffer with two pointers to it, a lexeme- 
beginning and a forward pointer, as discussed in Section 3,2. The Lex corn- 
piler constructs a transition table for a finite automaton from the regular 
expression patterns in the L e x  specification. The lexical analyzer itself con- 
sists of a finite automaton simulator that uses this transition table to Imk for 
the regular expression patterns in the input buffer. 
transition 
1 
table 
( 
(b) Wcmatic lexical analyzer. 
Fig. 3.33. Mdel of t e x  compikr. 
The remainder of this section shows that the irnpkmentation of a Lex 

130 
LEXICAL ANALYSIS 
SEC. 3.8 
compiler can be based on either norrdetermhistic or deterministic automata, 
At the end of the last section we saw that the transition table of an NFA for a 
regular expression pattern can be considerably smaller than that of a DFA, 
but the DFA has the decided advantage of being able to recognize patterns 
faster than the NFA. 
One method is to construct the transition table of a nondeterministic finite 
automaton N for the composite pattern p )p2 1 * . - lP,, . This can k done by 
first creating an NFA N(pi) for each pattern pi using Algorithm 3.3, then 
adding a new start state so, and finally linking so to the start state of each 
N (pi) with an e-transit ion, as shown in Fig. 3.34. 
Fig. 3.34. N FA mnstructcd from L a  spccificalion 
To simblate this NFA we can use a modification of Algorithm 3.4. The 
modification ensures that the combined NFA recognizes the longest prefix of 
the input that is matched by a pattern. In the m b i n t d  NFA, there is an 
accepting state for each pattern pi. When we simulate the NFA using Algo- 
rithm 3.4, we construct the sequence of sets of states that the combined NFA 
can b in after seeing each input character. Even if we find a set of states 
that contains an accepting state, to find the longest match we must continue to 
simulate the NFA until it reaches serrninuiiun, that is, a set of states from 
whkh there are no transitions on the current input symbol. 
We presume that the Lex specification is designed so that a valid source 
program cannot entirely fill the input buffer without having the NFA reach 
termination. For example, each compiler puts sow restriction on the length 
of an identifier, and violations of this limit will bc detected when the input 
buffer overflows, if not sooner, 
To find the correct match, we make two modifications to Algorithm 3,4. 
First, whenever we add an accepting state to the current set of stales, we 

SEC. 3.8 
DEsIGN OF A LEXICAL ANALYZER GENERATOR 
13 1 
record the current input position and the pattern pi corresponding to this 
accepting state. If the current set of states already contains en accepting state, 
then only the pattern that appears first in the Lex specification is recorded. 
Second, we continue making transitions until we reach termination. Upon ter- 
mination, we retract the forward painter to the psition at which the last 
match occurred. The pattern making this match identifies the token found, 
and the lexernc matched is the striqg between the lexerne-beginning and for- 
ward pointers. 
Usually, the Lex specification is such that some pattern, possibly an error 
pattern, will always match. If no pattern matches, however, we have an error 
condition for which no provision was made, and the lexical analyzer should 
transfer wntrol to some default error recovery routine. 
Example 3.18, 
A simple example illustrates the above ideas. Suppose we 
have the following Lex program consisting of three regular expressions and no 
reguiar definitions, 
a 
{ ) #* actions are omitted here */ 
d& 
{ 1 
0%' 
{ ) 
The three tokens above are remgnized by the automata of Fig. 3.35(3). We 
have simplified the third automaton somewhat from what would be prduwd 
by Algorithm 3.3. As indicated above, we can convert the NFA's of Fig. 
3.35(a) into one combined NFA W shown in 3.35(b). 
Let us now consider the behavior of H on the input string ouba using our 
modification of Algorithm 3.4. Figure 3,36 shows the sets of states and pat- 
terns that match as each character of the input aaba is prmssed. This figure 
shows that the initial: set of states is 10, l, 3, 7). States I ,  3, and 7 each haw 
a transition on a, to states 2, 4, and 7, respectively. Since state 2 is the 
accepting state for the first pattern, we record the fact that the first pattern 
matches after reading the first a. 
Yowever, there is a transition from state 7 to state 7 on the second input 
character, so we must continue making transitions. There is a transition from 
state 7 to state 8 on the input character b. State 8 is ehc accepting state for 
the third pattern. Once we reach state 8, there arc no transitions possible on 
the next input character a sr, we have reached termination. Sncc the last 
match occurred after we read the third input character, we repor1 that the 
third pattern has matched the lexetne aab. 
o 
The role af actionr associated with the pattern pi in ihe Lex specification is 
as follows. When an instance of pi is recognized, the lexical nnalyzr executes 
the associated program actioni. Note that action; is not executed just because 
the NFA enters a state that includes the accepting state for pi; ractioni is only 
executed if pi turns out to be the pattern yielding the longest rnatch. 

132 
LEXICAL ANALYSIS 
SEC. 3.8 
(a) NFA for a, ubb, and rr*b ' , 
(b) Combincd NFA. 
Fig. 3.X. Sequcncc of scts of statcs mtcrcd in prorx~5ing input aah. 
Another approach to the construction of a lexical analyzer from a Lex specifi- 
cation is to use 3 DFA to perform the pattern matching+ The only nuance is 
to make sure we find the pruper pattern matches. The situation is compkkly 
analogous to the modified iimulation of an NFA jug described. Whep wc 
convert an NFA to a DFA using the subset construction Aigorithm 3.2, there 

may be several accepting states in a given subset of nondeterministic states. 
In such a situation, the accepting stak corresponding to the pattern listed first 
in the Lex specification has priority. As in the NFA simu4ation, the only 
other modification we need to perform is to continue making state transitions 
until we reach a state with no next statc (i.e., the state 0) for the current 
input symbol, To find the \exerne matched, we return to the last input p i -  
tion at which the DFA entered an accepting slate. 
Fig. 337. Transition tab1c for a DFA+ 
01 37 
247 
8 
7 
5 8 
68 
Example 3.19, If we wnvert the NFA in Figure 3.35 to a DFA, we obtain 
the transition table in Fig. 3.37, where the states of the DFA have been 
named by lists of the states of the NFA. The last column in Fsg. 3.37 indi- 
cates one of the patterns recognized upm entering that DFA statc. For exam- 
ple, among NFA states 2,4, and 7, only 2 is accepting, and it is the accepting 
state of the automaton for regular expression n in Fig. 3.35Ca)- Thus, DFA 
state 247 remgnizes pattern o. 
Note that the string abb is marched by two patterns, ubb and u*b*, recog- 
nized at NFA states 6 and 8. DFA state 63, in the last line of the transition 
table, therefore includes two accepting states of the NFA. We note that abb 
appears before a*b+ in the translation rules of our Lex specification, so we 
announce that .ubb has been found in DFA state 68. 
,On input string ash, the DFA enters the states suggested by the NFA 
simulation shown in Fig. 3.36. Consider a second example, the input string 
a h .  The DFA of Fig- 3.37 starts off in state 0137. On input u it goes to 
statc 247. Then on input 6 it,progresses to state 58, and on input a it has no 
next state. We have thus reached terminat ion, progressing through the DFA 
states 0!37, then 247, then 58, The last of these includes the amping NFA 
state 8 from Fig. 3.35(a). Thus in state 58 the DFA announces that the pat- 
tern a*b" has been recognized, and selects ab, the prefix of the input that led 
to state 58, as the lex~rne. 
a 
247 
7 
7 
- 
8 
58 
8 
8 
68 
8 
nonc 
u 
n*b ' 
none 
u*b ' 
abb 

134 
LEXlCAL ANALYSIS 
Recall from Section 3.4 that the lookahead operator 1 is necessary in some 
situations, since the pattern that denotes a par~icular token may need to 
describe some trailing context for the actual lextme. When mnverting a pat- 
tern with / to an NFA, we car, treat the / as if it were c, so that we do not 
actually look for / on the input, However, i f  a string denoted by this regular 
expression is recognized in the input buffer, the end of the lexerne is not the 
position of the NFA's accepting state. Rather it Is at the la9 occurrence of 
the state of this NFA having a transition on the (imaginary) /. 
Example 3.M. The NFA recognizing the pattern for IF given in Example 
3.12 i s  shown in Fig, 3.38. State 6 indicates the presence of keyword IF; 
howevtr, we find the token IF by scanning backwards to the laa occurrence 
of state 2. 
o 
Fig. 3 3 .  NFA recognizing Fortran kcywrd IF. 
3+9 OPT1 MUATION OF DFA-BASED PATTERN MATCHERS 
In this section, we present three algorithms that have been used to implement 
and optimize pattern matchers constructed from regular expressions. The first 
algorithm is suitable for iac~udon in a Lex compiler teause it mnstrucis a 
DFA directly from a regular expression, without constructing an intermediate 
NFA along the way. 
The second algorithm minimizes the number of states of any DFA, so it can 
be used to redua the size of a DFA-based pattern matcher. The algorithm is 
efkient; its running time is 0 (nlogn), where n is the number of states in the 
DFA. The third algorithm cm be used to produrn fast but more compact 
representations for the transition table of a DFA than a straightforward two- 
dimensional table. 
Important States m f  m NFA 
Let us cal! a state of an NFA impmns if it has a none a~t-transition. The 
subset construction in Fig. 3.25 uses only the important states in a subset T 
when it determines r-clusure(mve(T, a ) ) ,  the set of states that is reachable 
from T on input a+ The set mvve(s, a )  is nonempty only if state s is impr- 
tant. During the mnstruction, two subsets can k identified if they have the 
same important states, and either both or neither include accepting states of 
the NFA. 

SEC. 3+9 
OPTIMIZATION OF DFA-BASED PATTERN MATCHERS 
135 
When the subset construction is applied to an NFA obtained from s regular 
expression by Algorithm 3.3, we can exploit the special properties of the NFA 
to combine the two wnstructioas. The mrnbined construction relates the 
important states of the NFA with the symbts in the regular expression. 
Thompson's mnstructim builds an impxiant state exactly when a symbol in 
the alphabet appears in a regular expression. For example, imprtant states 
will k constructed for each a and b in (a [b)*abb. 
Moreover, the resulting NFA has exactly one accepting state. but the 
accepting state i s  not important bemuse it has no transitions leaving it. By 
concatenating a unique right-end marker # to a regular expression r ,  we give 
the accepting state of r a transition on #, making it an important state of the 
NFA for r#. In other words, by using the augmented regular expression (r)# 
we can forget abut acceptiing states as the s u b $  construction proceeds; when 
the construction is complete, any DFA state with a transition on # must be an 
accepting state. 
We represent an augmented regular expression by a syntax tree with basic 
symbols at the h v e s  and operators at the interior fiodes, We refer to an ink- 
rim node as a cuc-node, ar-node, or star-node if it is labeled by a concatena- 
tion, 1, or * operator, respectively. Figure 3.3Na) shows a syntax tree for an 
augmented regular expression with cat-nodes marked by dots. The synlax tree 
for a regular expression can be constructed in the same manner as a syntax 
tree fw an arithmetic expression (see Chapter 2). 
Leaves in the syntax tree for a regular expression are labeled by alphabet 
symbls or by E+ To each leaf not labled by r we attach a unique integer and 
refer to this integer as tht psirim of the leaf and also as a position of its sym- 
bol. A repeated symbol therefore has several positions. Posi~ions are shown 
below the symbls in the syntax tree of Fig. 3.3qa). The numbered states in 
the NFA of Fig. 3.3%~) correspond to the positions of the leaves in the synlax 
tree in Fig. 3.39(a). It is no coincidence that these states art the imprtant 
states of the NFA. Non-important states are named by upper case letters in 
Fig. 3.39(c)+ 
The DFA in Fig. 3.39(b) can be obtained from the NFA in Fig. 3.39(c) if 
we apply the subset construction and identify subsets containing the same 
irnprtant states. The identification results in one fewer state being con- 
structed, as a comparison with Fig. 3.29 shows. 
In this section, we show how to construct a DFA directly from an augmented 
regular expression (r)#. We begin by constructing a syntax tree T for (r)X 
and then corn pu t ing four functions: nullable, firstpus, 1usrpLr, and fdluwps, 
by making traversals over T. Finally, we construa the DFA from folfuwps. 
The .functions nullable, firstpus, and iustpos are defined m the nodes of the 
syntax tree and are used to compute f o l h w p , ~ ,  which is defined on the set of 
pitions. 

136 LEXlCAL ANALYSIS 
I 
1 \ 
d 
/ \ 
6 
(a) Syntax tree for (a 1 b)*abb# 
a 
D 
(b) Resulting DFA. 
(c) U ndcrlying N FA. 
Rig. 3.39. DFA and NFA wnstrueted from (a lb)*abb#, 
Remembering the equivalence b e t w n  the inprtant NFA states and the 
pitions of the leaves in the synlsx tree of the regular expression, we can 
short-circuit the conslxudim of the NFA by building the DFA whose states 
correspond to sets of positions in the tree. The etransitions of the NFA 
represent some fairly complicated aructure of the psitions; in particular, they 
encode the information regarding when one position can f o h w  another. That 
is, each symbol in an input string to a DFA can be matched by certain psi- 
tions. An input symhl c can only be matched by pitions at which there is a 
c, bur not every position with a c can necessarily match a particular 
occurrence of c in the input stream. 
The notion of a position matching an input symbol will be defined in terms 

SEC. 3.9 
OPTlMlZATlON OF DFA-BASED PATTERN MATCHERS 137 
of the function fuibwpos on wsiticsns of the syntax tree. Cf i is a psition, 
then fdhwps(ij is the set of positions j such that there is some input string 
.+,--- . -  such that i corresponds to this occurrence of r and j to this 
occurrence of d. 
Example 3.21, In Fig. 3.39(a), followps(1) = { I ,  2, 3). The reasoning is 
that if we m an a corresponding to psition 1, then we have just sten an 
occurrence of alb in the closure (uIb)*. We could next see the first position 
of another wurrencc of alb, which explains why 1 and 2 are in fMowps(1). 
We could a h  next see the first psition of what follows (a \b)*, that is, p s i -  
tion 3. 
u 
In order to compute the function foilowpm, we need to know what positions 
can match the first or last symbl of a string generated by a given subexpres- 
sion of a regular expression. (kch information was used informally in E  am- 
ple 3.21 .) If r* is such a subexpression, then every position that can be first 
in r follows every position that can be last in r. Similarly, if rs ks a subexpres- 
sion, then every first position of s follows every last p i t i o n  of r. 
At each node n of the syntax tree of a regular expression, we define a func- 
tion firsfpos{n) that gives the set of positions that can match the firs1 symbol 
of a string generated by the subexpression rooted at n. Likewise, we define'a 
function hscpusIn) that gives the set of positions that can match the last sym- 
bol in such a string. For example, if n is the root of the whole tree in Fig. 
3,39(a), then firsfp ( n )  = { 1, 2, 3) and lassps (n) = (6). We give an algo- 
rithm for computing these functions momentarily. 
In order to carnpute firsrpos and Lustpus, we need to know which ndes are 
the roots of subexpressions that generate languages that include the empty 
string. Such nodas are called ndlnble, and we define dIa6le (n) to be true if 
node n is nullable, false otherwise, 
We can now give the rules to compute the functions nulhble, firstpus, hst- 
pos. and f d b w p x .  For the first three functions, we have a basis rule that 
tells abut expressions of a basic symbl, and then three inductive rules that 
allow us to determine the value of the functions working up the syntax tree 
from the bottom; in each case the inductive rules correspond to the three 
operators, union, concatenation, and closure. The rules for nulhble and fir$:- 
pos are given in Fig. 3,40. The rules for lasips(n) are the same as those for 
firsrp(n), but with cl and C* reversed, and are not S ~ Q W ~ .  
The first rule for mkbk states that if n is a leaf labeled t, then ndhble(n) 
is surely true, The s ~ x l n d  ruk states that if n is a leaf labeled by i n  alphakt 
symbol, then nulhbLe(n) is false. In this case, each leaf corresponds to a sin- 
gle input symbol, and therefore cannot generate t. The last rule for nullable 
states that if n is a star-ndc with child c , then ndhble (n) is true, because 
the closure of an expression generates a language t h i  includes E. 
As another example, the fourth rule for firslpw states that if n is a cat-node 
with left child cl and right child c2, and if nt~Ilubk(c ,) is true, then 

138 
LEXICAL ANALYSIS 
NODE m 
t~ is a leaf 
labeled E 
Fig. 3.40. Rules for computiag ndlobk and Prstpux. 
vuilabie IN) 
I 
true 
I 
0 
n is a leaf 
labeled with position i 
otherwise, $ r s f p s ( n )  = firs/pos(c,). What this rule says is that if in an 
expression rs, r generates r, then the first psitions of s "show through" r and 
are also first positions of rs; otherwise, only the first positions of r are first 
positions of rs. The reasoning behind the remaining rules for nuliable and 
firsrps are similar. 
The function f d u w p o ~ ( i )  tells us what positions can follow p i t  ion i in the 
syntax tree, Two rules define all the ways one position can follow another. 
firslpm (n ) 
fahe 
I .  
If n is a cat-node with left child c and right cbild cz, and i is a position 
in h t p o s  (cl ), then all. psitions in Jirsrps(c2) are in foI1uwpos (i). 
2. 
If n i s  a star-node, and i is a position in Iwrpos(n), then all positions in 
f i r s i p s ( n )  are in fof60wpos(i). 
If firsrpus and i m p s  have been computed for each node, fdlowpos of each 
psition can be computed by making one depth-first traversal of the syntax 
tree. 
Example 3.22. Figure 3.41 shows the values of firsips and Imps at all 
n d t s  in the tree of Fig. 3+39(a); firspm(n) appears to the left of node n and 
hstposIn) to the right. For example, firJipos at the leftmost leaf labeled a is 
(11, since this leaf is labeled with psitimi 1. Similarly, firspm of the second 
leaf is {2), since this leaf is labeled with position 2. By the third rule in Fig+ 
3.40, firsips of their parent is ( 1 ,  21, 

OPTIMIZATION OF DFA-BASED PATTERN MATCHERS 
139 
Fig. 3.41. j h j w s  and Ins~pos fw nodes in syntax trcc for (a ]b)*u&B. 
The node labeled * is the only nuIlable node. Thus, by the If-condition of 
the fourth rule, firsips for the parent of this n d e  (the one representing 
expression (a(b)*a) is the union of ( 1 ,  2) and (31, which are the firstpus's of 
its left and right children, On the other hand, the eke;e-oondition applies for 
iusrpas of this n d e ,  since the leaf at psition 3 is not nullable, Thus, the 
parent of the star-node has iasrp containing only 3. 
Let us now compute fo#iowps batom up for each .node of the syntax tree 
of Fig. 3Al. At the star-nde, we add both 1 and 2 ti, folluwpos(1) and to 
foIIowpos{2) using rule (2). At the parent of the stat-node, we add 3 to foL 
lowpm ( I) and folluwpus (2) using rule ( I). At the next cat-node, we add 4 to 
fcllluwpm(3) using rule (I). At the next two cat-nodes we add 5 to fol- 
iuwps(4) and 6 to fdbwpm(5) using the same rule. This completes the con- 
struction of ftdhwpus. Figurc 3+42 summarizes fdbowpm. 
We can illustrate the function fdluwpos by creating a directed graph with a 
node for each position and directed edge from nude i to node j if j is in 
fuUowpos(i). Figure 3.43 shows 1his directed graph for fdlowpos of Fig. 

140 
LEXlCAL ANALYSIS 
3.42. 
Fig. 3.43. Directed graph for the function jdowpos. 
It is interesting to note that this diagram would become an NFA 
transitions for the regular expression in question if we: 
I. make all positions in f i r q m s  of the r m t  be start states, 
2, 
label each directed edge t i ,  j )  by the symbol at position j, and 
3. make the position aswciated with B be the only accepting state 
without C- 
It should therefore come as no surprise that we can convert the fuhwpm 
graph into a DFA using the subset construction. The entire construction can 
be carried out an the positions, using the folhwing algorithm. 
D 
A&withrn 3.5, Construction of a DFA from a regular expression r. 
Input. A regular expression r. 
O~rpur. A DFA D that recognizes L ( r ) .  
1 . 
Construcr a syntax tree for the augmented regular expression ( r ) # ,  where 
# i s  a unique endmarker appended to (r). 
2. 
Construa the functions nullable, firsipus, lasrpos, and fdlawpos by mak- 
ing depth-first traversals of T .  
3. 
Construct Dsrates, the set of states of D. and Drran, the transition table 
for D by the procedure in Fig. 3.44. The states in Dsratcs are sets of 
positions; ini~ially, each state is "unmarked," and a state becomes 
"marked" just before we consider its our-transitions. The start stale of D 
is Jrstpus(ri), and the accepting states are all thost containing the p s i -  
tion assclciated with the endmarker #. 
Example 3.23. Let us construct a DFA for the regular expression lo(b)*abb. 
The syntax tree for ((a lb)*abb)# is shown in Fig. 3.39(a). dhbk i s  true 
only for the node labeled *. The functions firstpu~ and iastpm are shown in 
Fig. 3+41, and fullowps is shown in Fig. 3.42. 
From Fig. 3.41, Prstps of the root is {I, 2, 3). Let this set be A and 

OPTlMlZATlON OF DFA-BASED PATTERN MATCHERS 
141 
initially, the only unmarked state in Dsrutes is Jrsqw+r(rrwr), 
where rwt is the rmt of the syntax tree far (r)#: 
whUe thwc is an unnaikcd state T in Dstates da begin 
mark T: 
fmr each input symbol a ds begin 
Ict U IN the set of positions that are in {oiIowp~Cp) 
for somc psition p in T ,  
such that the symbol at position p is a; 
it U is not empty and is not in D S ~ P S  
then 
add U as an unmark4 aate to Dstnres; 
D~mn 
IT, cr p; = U 
md 
end 
Fig. 3.44. Construclion of DFA+ 
'consider input symbol a. Positions 1 and 3 are for a, so let B = 
folbwpus( 1) U followpus (31 = {I, 2, 3, 4). Since this sa has not yet been 
seen, we set DiranlA, a ]  := B. 
When we consider input b, we note that of the positions in A, only 2 is 
associated with b, so we must consider the set fduwpus(2) = [I, 2, 31, Since 
this set has already k
n
 
seen, we do not add it to Dxturts, but we add the 
transition Dfran [A, b 1 : = A. 
We now continue with B = 11, 2, 3, 4). The states and transitions we 
finally obtain are the same as those that wcre shown in Fig. 3.39(b). 
Minimizing the Number of State of a DFA 
An important theoretical result i s  that every regular set is recognized by a 
minimum-state DFA that is unique up to state names. In this section, we 
show how to construct this minimum-state DFA by reducing the number of 
states in a given DFA to the bare minimum without affecting the language 
that i s  being recognized. Suppose that we have a DFA M with set of states S 
and input symbol alphabet E. We assume that every state has a transition on 
every input symbol.. If that were not the case, we can introduce a new "dead 
state" d, with transitions from d to d on all inputs, and add a transition from 
state s to d on input a if there was no transit ion from s on u. 
We say that string w disrr'nguish~s state s from state r if, by starting with the 
DFA M in state s and feeding it input w ,  we end up in an accepting state, but 
starting in state r and feeding it input w, we end up in a nonaccepting state, or 
vice versa. For example, r distinguishes any accepting state from any nonac- 
cepting state, and in the DFA of Fig. 3.29, states A and B are disfinguished by 
the input bb, since A goes to the nonaccepting state C on input hb, while B 
goes to the accepting state E on that same input. 

142 
LEXICAL ANALY SlS 
SEC, 3.9 
Our algorithm for minimizing the number of states of a DFA works by 
finding all groups of states that can be distinguished by some input string. 
Each group of states that cannot be distinguished is then merged into a single 
state, The algorithm works by maintaining and refining a partition of the set 
of states. Each group of states within the partition consists of states that have 
not yet been distinguished from one another, and any pair of states chosen 
from differen1 groups have been found distinguishable by some input. 
Initially, the partition consists of two groups: the accepting states and the 
nonacceptiog states. The fundamental step is to take some group of states, 
say A = {s,,sz, . . . ,sk) and some input symbol a, and look at what transi- 
tions states s l ,  . . . ,.% have on input o. If these transitions are to states that 
fall into two or more different groups of the current partition, then we must 
split A so that the transitions from the subsets of A are all confined to a single 
group of the current partition. Suppse, For example, that s 
and s 2  go to 
states s, and 12 on input n, and t ,  and s2 are in different groups of  he parti- 
tion. Then we must split A into at least two subsets so that one subset con- 
tains s ,  and the other $ 2 .  
Note that t l  and t z  are distinguished by some 
string w, so sl and $2 are distinguished by string ow. 
We repeat this process of splitting goups in the current partilion until no 
more groups need to be split. While we have justified why states that have 
been split into different groups really can t~ distinguished, we have not indi- 
cated why states that are not split into different groups are certain not to be 
distinguishable by any input string. Such i s  the case, however, and we leave a 
proof of that fact to the reader interested in the theory (see, for example, 
Hopcroft and Ullman i1979)). Also left to the interested reader is a proof 
that the DFA constructed by taking one state for each group of the final parti- 
tion and then throwing away the dead state and states not reachabte from the 
start state has as few states as any DFA accepting the same language. 
Algorithm 3.6, Minimizing the number of states of a DFA. 
lsrput. A DFA M with set of states $, set of inputs X, transitions defined for 
all states and inputs, start state so+ and set of accepting states F. 
Owlpr. A DFA M' accepting the same language as M and having as few 
states as possible. 
Method. 
I .  Construct an initial partition Il OF the set of states with two groups: the 
accepting states F and the nonaccepting states .S -F. 
2. 
Apply 1he procedure of Fig, 3.45 to ll to construct a new partition n,, 
. 
3, 
If f,,, = n, let n,,,,, 
= n: and continue with step (41, Otherwise, repeat 
step (2) with fl := ll,,,. 
4. 
Choose one state in each group of the partition nhnar 
as the reprrstnmivp 

SEC. 3.9 
OPTIMlZATlON OF DFA-BASED PATTERN MATCHERS 
143 
for that group. The representatives will be the states of the reduced DFA 
M'+ Let s be a representative state, and suppose on input n there is a 
transition of M from s to 1. Let r be the representative of t's group (r 
may be t). Then M' has a transition from s to r on a. Let the start state 
of M' be the representative of the group containing the skirt state so of 
M, and let the accepting states of M' be the repre~ntatives that are in F. 
Nde that each group of IIr,,, 
either consists only of states in F or has no 
states in F. 
If M' has a dead state, that is, a statc d that is not accepting and that has 
transitions to itself on all input symbols, then remove d from M', Also 
remove any stares not reachable from the start state. Any transitions to d 
from other states become undefined. 
u 
for tach group G of I1 do begin 
partition G into subgroups such that two stales s and r 
of G arc in thc same subgroup if h d  only if for all 
input symbols n. states s and i have Iransitions un a 
to statcs in thc same group of I I: 
at worst, a statc will bc in a subgruup by i t d f  */ 
rcplacc G in II,, 
by thc wt of all subgruup formed 
end 
Fig. 3.45. Construction of II ,,,. 
Example 324, Let us reconsider the DFA represented in Fig. 3.29+ The ini- 
tial partition I1 consists of two groups: ( E ) ,  the accepting hiate: and (ABCD). 
the nonaccepting states. To construa n,, 
, the algorithm of Fig, 3.45 first 
considers (El. Since this group consists of a single state, it canno1 be split 
further, so (E) is placed in I[,,. 
The algorithm then considers the goup 
(ABCD). On input u, each of these states has a transition to 8, so they muld 
at1 remain in one group as far as input u is concerned. On input b, however, 
A, B, and C go to members of the group (ABCD) of 11, while D goes to E. a 
member of another group, Thus, in n,, 
the group (ABCD) must k splir inti, 
twu new groups (ABC) and (D); n,,, 
is thus (ABC)(D)(E). 
In the nexi pass through the algorithm of Fig. 3+45, we again have no split* 
ting on input a, but (ABC) must be split into two new grwps (AC)IB), since 
on input b, A and C each have a transition to C, while B has a transition to D, 
n memkr of a group of the partition different from that of C, Thus the next 
value of n is (AC)(B)(D)(E). 
In the next pass through the algorithm of Fig. 3.45, we cannor split m y  of 
the groups consisting of a single state, The only .possibility is to try to split 
(ACI. But A and C go the same statc B on input u, and they go to the same 
state C on input b. Hence, after this pass, n,,, 
= n. 
II,.jnab is thus 
IACNBNDME). 

144 
LEXICAL ANALYSIS 
SEC, 3.9 
if WE choose A as the representative for the group (AC), and choose B, D, 
and E for the singleton groups, then we obtain the reduced automaton whose 
transition table is shown in Fig. 3.46. State A is the start state and state E is 
the only accepthg stare. 
Fig. 3.46. Transition tabk of rcduccd DFA 
For example, in the reduced automaton, state E has a transition to state A 
on input b, since A is the representarive of the group for C and there is a tran- 
sition from E to C on input b in the original automaton. A similar change has 
taken place in the entry Fw A and input 6; otherwise, all other transitions are 
copied from Fig. 3.29. There Is no dead state in Fig. 3+46, and all states are 
reachable [rum the start state A .  
o 
State Miaimization in Lexical Analyzers 
To apply the state minimization procedure to the DFA's constructed in Section 
3.7, we must begin Algorithm 3.5 with an initial partition that places in dif- 
ferent groups all states indicating different tokens. 
Exampie 3.25. In the case of the DFA of Fig. 3+37, the initial partit ion would 
group 01 37 with 7, since they each gave no indication of a token recognized; 8 
and 58 would also be grouped, since they each indicated mken a*b+ . Other 
states would be in groups by themselves. Then we immediately discover that 
0137 and 7 k b n g  in different groups since they go to different groups on 
input a, Likewise, 8 and 58 do not belong together because of their transi- 
[ions on input b. Thus the DFA of Fig. 3.37 is the minimum-state automaton 
doing its job. 
u 
Table-Compression Methods 
As we have indicated, there are many ways to implement the transition func- 
tion of a finite automaton. The process of lexical analysis occupies a r e a m -  
able portion of the compiler's time, since it is the only process that must look 
at the input one character at B time. Therefore the lexical analyzer should 
minimize the number of operations it performs per input character, If a DFA 
is used to help implement the lexical analyzer, then an efficient representation 
of the transition function is desirable, A two-dim.cnsional array, indened by 

SEC. 3.9 
OPTIMIZATION OF DFA-BAED PATTERN MATCHERS 
145 
states and characters, provides the fastest awss, but it can take up too much 
space (say several hundred states by 128 characters). A more compact but 
slower scheme is to use a linked list to store the transitions out of each state, 
with a "default" transition at the end of the list. The most frequentiy occur- 
ing transition is m c  obvious choice for the default. 
There is a more subtle implementation that combines the fast access of the 
array representation with the compactness of the list structures. Here we use 
a data structure consisting of four arrays indexed by state numbers as depicted 
in Fig. 3.47.7 The bose array is used to determine the base location of the 
entries for each state stored in the next and check arrays. The defaulr array is 
used to determine an alternative base location in case [Re current base location 
is invalid. 
Flg. 3.47, Data structure for representing transition cables. 
To compute wxfsiute(s, a), the transition for state s on input symbol a, we 
first consult the pair of arrays nexr and check. In particular, we find their 
entries for state s in lqcation 1 = h
e
 
[s 
J t a, where character a is treated as 
an integer. We take next[C] to be the next state for s on input u if chckIl] = 
s, If rheckIl1 # s, we determine q = defwCtlsl and repeat the entire pro- 
cedure recursively, using q in place of s+ The praxdure is the following: 
The intended use of the structure of Fig. 3.47 is to make the nm-check 
arrays short, by taking advantage of the similarities among states. For 
7 There would k in practia another array indexed by s, giving ~ h c  
pattern that matchcs. if any, 
whcn state s is entered. This information IS derived from the NFA states making up DFA aatc s. 

example, state g, the default for state s, might be the state that says we are 
"working on an identifier," such as state 10 in Fig. 3.13. Perhaps s is entered 
after swing th a prefix of the keyword then as well as a prefix of an identif- 
ier. On input character e we must go to a specjal state that remembers we 
bave seen the, but otherwise state s behaves as state q does. Thus, we set 
check [ h e  1s ]+el to s and nexr[hse[s J+ el to the state for the. 
While we may not be able to chmse bust? values sr, that no m t - c k k  
entries remain unused, experience shows that the simple strategy of setting the 
base to the lowest number such that the special entries can be filled in without 
conflicting with existing entries is fairly g
d
 
and utilizes little more space 
than the minimum pssible. 
We can shorten check into an array indexed by slates if the DFA has the 
property that the incoming edges to each state t all have the same label a+ TO 
implement this scheme, we set check [ I  1 = a and repiace the test on line 2 of 
prmdu re nextstate by 
it c k c k  [nexr[base I$] fa I I = a tbcn 
EXERCISES 
What is the input alphabet of each of the following languages? 
a) Pascal 
b) C 
C) Fortran 77 
d) Ada 
e) Lisp 
What are the conventions regarding the use of blanks in each of the 
languages of Exercise 3. I? 
Identify the lexemes that make up the tokens in the following pro- 
grams. Give reasonable attribute values for the tokens. 
a) Pascal 
function nax I i, j : integer 1 : integer ; 
{ return maximum of integers i and j ) 
begin if 
i r j then mix := 
i 
else max := j 
end ; 
b) C 
int max I i, j ) int 
/* return maximum of 
i 
return i r j ? i :  j; 
1 
i, j; 
integers i and j 4 /  

CHAPTER 3 
EXERCISES 147 
F'WlUCTIONMAX ( I ,  Y 1 
RETURN MAXIMUM OF INTEGERS I AND J 
IF [I .GT, J )  THEN 
MAX = I 
ELSE 
M A X * J  
END IF 
RETURN 
3,4 Write a program h r  the function nextchar I 1 of Section 3.4 using 
the buffering scheme with sentinels described in Section 3.2. 
3.5 In a string of length n, how many of the kdlowing are rhare? 
a) p~efixes 
h) suffixes 
c) ~ubstrings 
d) proper prefixes 
e) subscquenccs 
*b7 Wrik regular definitions for the following languages. 
a) All strings of letters that contain thc five vowels in order. 
h) All strings of letters in which thc btters are in ascending icxiw- 
graphic order. 
C) Comments consisting of a string surrounded .by /+ and */ without 
an intervening */ unless it appears inside the quotes " and ". 
*dl All swings of digits with no repcated digit. 
C) All strhgs of digits with at most one repeated digit. 
f) All strings of 0's and 1's with an cvcn number ot 0's and an odd 
number of 1's. 
g) The set of chess moves, such as p - k4 or kbp x y ~ .  
h) All strings or O's and 1's that do nd contain the substring 01 1. 
i) All strings of 0's and I 's that do not crrntain the subsequence 01 1. 
3.8 Specify ihe lexical form of numeric constants in the languages of 
Exercise 3.1 . 
3.9 Specify the lercicnl form of identifiers and keywords in the Languages 
of Exercise 3.1. 

148 
LEXICAL ANALYSIS 
CHAPTER 3 
3,10 The regular expression constructs permitted by Lex are listed in Fig+ 
3.48 in decreasing order of precedence. In this table, c stands for any 
single character, r for a regular expression, and s for a string. 
nay non-opcratur character r I 
a 
character r literally 
string s literally 
any character but ncwlinc 
beginning nf linc 
cnd of line 
any charicter in 
any char~cter not in s 
7gro or more r's 
onc or more r's 
zcro o r  onc r 
rn to n wcurrenccs of r 
r ,  then n, 
rl or r l  
r 
r , when folkwed by r, 
n
~
~
'
t
 
a. *b 
"abc 
a h $  
[ "abc 1 
a * 
a + 
a? 
a{ l , S l  
ab 
atb 
(sib) 
abc/ 1 2 3  
Fig, 3.48. Le M regular cxpsessions. 
a) The special meaning of the operator symbols 
must be turned off if the operator symbol is to be used as a match- 
ing character, This can be done by quoting the character, using 
one of two styles of quotation. The expression *sW matches the 
string s literally, provided no " appears in o. For example, "**" 
matches the string **. We could also have matched this string 
with the expression \*I*+ 
Note that an unquoted + is an instance 
of the Kleene closure operator. Write a Lex regular expression 
that matches the string "\. 
b) In Lex, a mmpdemmtt.6 character class is a character class in which 
the first symbol is " +  
A complemented character class matches 
any character not in the class. Thus, [ "a] matches any character 
h a t  is na an a, ["A-za-z] matches any character that is not an 
upper or lower case letter, and so on. Show that for every regular 
definition with complernen~ed character 
classes there is an 
equivalent regular expression without complemented character 
classes. 

CHAPTER 3 
EXERCISES 149 
C) The regular expression r (m,n l malches from m to tt occurrences 
of rhe pattern r, For example, a{?, 51 matches a string of one to 
five a's+ Show that fur every regular expression containing repeti- 
tion operators there is an equivalent regular expression without 
repel it ion operators, 
dl The operator " matches the leftnmt end of a line, This is the 
same operator that introduces a mmplerneated character class, but 
the conkxi in which " appears will always determine a unique 
meaning for this operator. The operator $ matches the rightmoa 
end of a line. For example, "[ "aeiou] +$ matches any line that 
does nM contain a lower case vowel. For every regular expression 
containing the " and $ operators is there an equivalent regular 
expression without these aperalors? 
3.11 Write a Lex program that copies a file, replacing each nonnull 
sequence of white space by a single blank. 
3.12 Write a Lex program that copies a Fortran program, replacing all 
instances of DOUBLE PRECISION by REAL. 
3.13 Use your specification for keywords and identifiers for Fortran 77 
from Exercise 3.9 to identify the tokens in the following statements: 
fF[I) = TOKEN 
I F I X )  ASSIGNSTOKEN 
IFII) ?0,20,30 
IF(I1 GOT015 
IFII 1 THEN 
Can you write your specification for keywords and identifiers in Lex? 
3.14 In the UNIX system, the shell command ah uses the operators in Fig. 
3.49 in filename exprcssions to describe sets of filenames. For exam- 
ple, the filename expression *+o 
matches all filenames ending in .a; 
sort. ? matches dl filenames that are of the form sort-r where c is 
any charactcr, Character classes may k abbreviated as in [a-21. 
Show how shell filename expressions can be represented by regular 
expressions. 
3+15 Modify Algorithm 3.1 to find the longest prefix of the input that is 
accepted by the DFA. 
3J6 Construct nondeterministic finite automata for the foilowing regular 
expressions using Algorithm 3+3. Show the sequence of moves made 
by each in processing the input string ahbbub. 
al Ia [b)* 
b) (a* ib*)* 

150 
LEXICAL ANALYSIS 
CHAPTER 3 
EXPRESSION 
MATCHES 
EXAMPLE 
string s literdlly 
character c literally 
P 
any characte~ I 
sort1.7 
[ sl 
( any character in s I sort. [CSO] 
Fig. 3.49. Filcnamc cxprcssions in thc program sh. 
3.17 Convert the NFA's in Exercix 3.16 into DFA's using Algorithm 3.2. 
Show the sequence of moves made by each in processing the input 
string ab&kb. 
3.18 Construct DFA's for the regular expressions in Exercise 3.16 using 
Algorithm 3.5. Compare the size of the DFA's with those constructed 
in Exercise 3.17. 
3.19 Construct a deterministic finite automaton from the transition 
diagrams for the tokens in Fig. 3.10. 
3.28 Emnd the table of Fig. 3.40 to include the regular expression opera* 
tors ? and " . 
3.21 Minimize the states in the DFA's of Exercise 3.18 using Algorithm 
3.6. 
3.22 We can prove that two regular expressions are equivalent by showing 
that their minimum-state DFA's are the same, except for state names. 
Using this technique, show that the following regular expressions are 
alt equivalent. 
a) (alH* 
b) (a* [b*)* 
c) (I€ )db* )* 
3,23 Construct minimum-state DFA's for the fdbwing regular expressions. 
a) Iu \b)*a (a b) 
b) (a lb)*a (a l b ) b  lb) 
€1 fa 1b)*a@lb)Ia IbMu 
**d) Prove that any deterministic finite automaton for the regular 
expression (a lb)*a(u lb)(aib) - 
(a lb), where there are n - 1 
(u 1 bj's at the end, must have at least 2" states. 

CHAPTER 3 
EXERCISES 
151 
Construct the representation of Fig. 3.47 for the transition table of 
Exercise 3.19+ Pick defaub states and try the following two methods 
of mnstructing the next array and mmpare the amounts of space used: 
a) Starting with the densest states (those with the largest number of 
entries differing from their default states) first, place the entries 
for the states in the next array. 
b) Plaa the entries for the states in the next array in a random ord&+ 
A variant of the table compression scheme of Section 3.9 would be to 
avoid a recursive rzexrsmrrr procedure by using a fixed default location 
for each state, Construct the representation of Fig. 3.47 for the tran- 
sition table of Exercise 3.19 using this norirecursive technique, Com- 
pare the space requirements with those of Exercise 3.24. 
Let b bl . . b, be a pattern string, called a keyword. A rriv for a 
keyword is a transition diagram with m+ 1 states in which each state 
correspnds to a prefix of the keyword. For 1 5 s 5 ctr, there is a 
transition from state s - 1 to state s on symbol b,. The start and final 
states correspond to the emply string and the complete keyword, 
respectively, The trie for the keyword abuhu is; 
We now define a faiIurc fwtu~iun f on each state of the rranrrition 
diagram, except the start state. Suppose states s and 1 represent pre- 
fines u and v of the keyword, Then, we define f Is) = t if and only if 
v is the longest proper suffix of u h a t  is also the prefix 01 the key- 
word. The failure function f for the above trie is 
For example, states 3 and I rqresent prefixes a h  and u of the key- 
word a h h a .  f (3) = 1 bemu= u is the longest proper suffix of o h  
that is prefix of the keyword. 
Construct the failure function for the keyword ubabahub. 
Let the states in the tric be 0, 1, . . . , m, with 0 the start state. 
Show that the algorithm in Fig. 3.50 corrmly computes the failure 
function. 
Show that in the overall execution of the algorithm in Fig. 3.50 the 
assignment statement t := f [i) in the inner Imp is executed at 
most m times. 
Show that the algorithm runs in O(m) time. 

152 
LEXICAL ANALYSIS 
CHAPTER 3 
,* cornpure faiturc function f for b ,  . . . b, */ 
1 := 0;IIlI := 0; 
fors := 1 tom-1 dobegin 
while i > 0 and A, +, # b, , , do r := j ' ( t ) ;  
ilb.%,, = b , , ,  thenwin t : =  t + I ; J [ . v + l ) : -  
rend; 
~
j
~
s
+
~
)
:
=
 
n 
end 
Rg. 3.50. Algorithm to cornputc failure fundon for Exorcisc 3,26. 
3.27 Algorithm KM P in Fig. 3.5 1 uses the failure function f constructed as 
in Exercise 3.26 to determine whether keyword b l  + 
b, is a sub- 
string of a target string u l  . - - un. &ates in the trie for Bb . . . b,,, 
are numbered from O to m as in Exercise 3.26(b). 
/t dries rr, + 
. tr,, crmtain b ,  . . . b,,, as a substring *I 
,s := 41; 
furi := I to n do begin 
whik s > O and a, # b3 , , do s : = j ' f s ) ;  
itui = b y ,  I t h ~  
s := s + I 
if s = m then return "yes" 
end: 
return "no" 
Fig, 3S1. Algmithm KMP. 
a) Apply Algorithm KMP to determine whether ~ t h h a  
is a substring 
of ububub~.&. 
*b) Prove that Algorithm KMP returns "yes" if and only if b 
- . . b,, 
is a substring of a I 
4 a,,+ 
*c) Show that Algorithm KMP runs in O(m + n )  time. 
*d) Given a keyword y, show that the failure function can be used to 
construct, in O ( \ y / )  time, a DFA with ( y )  + 1 states for the rtgu- 
lar expression . *y . *, where . stands for any input character. 
Define the pcriod of a string s to be an integer p such that s can be 
expressed as (scv)", 
for same k 2 0, where Iuv 1 = p and v is noi the 
empty string. Fw example, 2 and 4 are periods of the string a h h h .  
a) Show that p is a period of a string s if and only if st - us for some 
strings i and u of length p .  
b) Show that if p and y are periods of a string s and if p +y I 
1x1 + gcd(p,y), then gcd(p.4) is a period of s, where gcd(p.q) is 
the greatest common divisor of p and y. 

CHAPTER 3 
EXERCISES I53 
C) Let sp (si) 
be the smallest perid of the prefix of length i of a 
string s. Show that the failure function f has the property that 
f(j1 = j - Vb,-d. 
5.29 Let the shorresi r~pearing prefu of a string s be the shortest prefix u of 
5 such that s = uk. for some k r I .  For example, a6 is the shortest 
repeating prefix of abuhbd and a h  is the shortest repeating prefix 
of &. Construct an algorithm that finds the shortest repeating pre- 
f i ~  
of a string s in O(Is1) time. Hint. Use the failure functian.of 
Exercise 3.26, 
3-30 A Fihrracci string is defined as follows: 
For example, s3 = ub, s4 = dm, and s 5 = abrrab. 
a) What is the length of s,? 
**b) What i s  the smallest perid of s,? 
C) Construct the failure function for sg. 
*d) Using induction, show that the fatlure function for s, 
can be 
expresssd 
by j 
= j - 1 
, where 
R 
is 
such 
that 
5 j+l 
\ s k + l i  for 1 5  j 5  IS,,^. 
C) Apply Algorithm KMP to determine whether $6 is a substring of 
the target string $7. 
0 Construct a DFA for the regular expression . *s6. *. 
**g) In Algorithm KMP, what is the maximum number of consecutive 
applications of the failure function executed in determining 
whether s* is a substring of the target string sk + ? 
3.31 We can extend the trie and failure function concepts of Exercise 3+26 
from a single keyword to a set of keywords as fdhws. Each state in 
the trie corresponds to a prefix of one or more keywords, The start 
state cwrespnbs to the empty string, and a state that corresponds to 
a complete keyword is a final state, Additional states may be made 
final during the computation of the Failure function. The transition 
diagram for the set of keywords {he, she, his, hers} is shown in 
Fig. 3.52. 
For the trie we define a transirion function g that maps state-symbol 
pairs to states such that g(s, b,,,) = s' if state s carresponds to a 
prefix b l  - - 
+ bj of some keyword and s' corresponds to a prefix 
b l  
+ 
b#j+l. Lf so is the start staie, we define g(so, a) = ro for 
all input symbols a that are not the initial symbol of any keyword. 
We then set g(s, a )  = fail for any transition not defined. Note that 
there are no fail transitions for the start stare. 

154 
LEXICAL ANALYSIS 
CHAPTER 3 
mb 
3.52, Tric for keywords {he. she, his, hers). 
Suppose states s and r represent prefixes u and v of some keywords. 
Then, wc define $(s) = r if and only if v i s  the longest proper suffix 
of u that is also the prefix of some keyword. The failure furmion f 
for the transition diagram above is 
For example, states 4 and I represent prefixes eh and h. f(4) = 1 
because h is the longest proper suffix of sh that is a prefix of some 
keyword. 
The failure function f can be computed for states of 
increasing depth using the algorithm in Fig. 3.53, The depth of a 
state is its distance from the start state. 
for cach state 3 of dcpth 1 do 
its) := sf,; 
for cach dcpth d 2 1 do 
for cach statc sd of depth d and character a 
such that ~(s,.,. u )  = s' do be@ 
S := J(sd); 
while 8 (s, a) = fad do s : = f(s); 
](J') := $IS, 
Q): 
eml 
Fi, 3.53. Algorithm lo mmputc failure function for trie of kcywords. 
Note that since #(so, c) # jail for any character c, the while-Imp in 
Fig. 3.53 is guaranteed to terminate. After setling f ( s t )  to g ( I ,  a 1, if 
g(c, a) is a final state, we also make s' a final state, if it is not 
dread y. 
a) Construct the failure function for the set of keywords {ma, crhaa, 
a&ubuuu). 

CHAPTER 3 
EXERCrsES 155 
*b) Show that the algorithm in Fig. 3.53 correctly m p u t e s  the failure 
fu fiction. 
*c) Show that the failure fundion can be computed in time propor- 
tional to tbe sum of the kngths of the keywords. 
3.32 Let g be the transition function and $the failure funcfion of Exercise 
3,3L for a set of keywords K = (y,, y2, 
+ . . , y*}. Algorithm AC in 
Fig. 3.54 uses g and J to determine whether a target string a . - . a, 
contains a substring that is a keyword. State so is the start state of 
the transition diagram for K ,  and F is lhe set of final states. 
f* does a ,  . . - o;, contain a keyword as a substring 4 1  
s:= so; 
for i := 1 b n do begin 
while ,q(s, a,') = iui! do s = {Is); 
s := g(s, a,); 
iP$isiaFtkenrttwlr "yes" 
. 
end; 
return "no" 
Fig. 3.54. Algorithm AC, 
a) Apply Algorithm AC to the input string ushers using the transi- 
tion and failure functions of Exercise 3.3 1. 
*b) Prove that Algorithm AC returns "yes" if and only if some key- 
word yi is a substring of u , . . . a,* 
*c) Show that Algorithm AC makes at most 2rt slate transilions in pro- 
cessing an inpar string of length n. 
*d) Show that from the transition diagram and failure function for a 
A 
set of keywords Cy,, y2, . . + . 
yi) a DFA with at most 
lyiI + I 
!a I 
states can be constructed in linear time for the regular expression 
**(YI ~
Y
Z
 
1 ' ' - lyk) + * +  
el Modify Algorithm AC to print out each keyword found in the tar- 
get string. 
3.33 Use the algorithm in Exercise 3.32 to construct a le~ical analyzer for 
the keywords in Pascal. 
3-34 Define h ( x ,  y), a bngesc cmmun subseqwnct of two strings x and 
y, to be a string that is a subsequence of both x and y and is as long 
as any such subsequence. For example, tie is a longest cammon 
subsequence of etriped and tiger, Define d(x, y), the d i ~ ~ u n w  
between x and y, to be the minimum number of inserrions and dek- 
tions required to transform x into y. For example, b(striped, 
tiger) = 6. 

156 
I-EXICALANAI,YSIS 
CHAPTER 3 
a) Show thar for any two strings x and y, the distance between -T and 
y and the length of their longest common subsequence are related 
by dIx. )I) = 1x1 + ]yl - O* I h k  v ) l h  
*b) Write an algorithm that takes two srrings x and y as input and pro- 
duces a longest common subs.equcnce of x and y as output. 
3-35 Define eIx, y ). the d i f  dsinncc between twn strings x and y, to be 
the minimum number of character insert ions, dcle~ions, and replacc- 
ments that are required to transform x into y. Let x = a ,  . . . t~,,, and 
v = bI + 
b,,. e(x, y) can be computed by a dynamic programming 
algorithm using a distance array d [ O . . m .  O..n I in which dli,, j ]  is the 
edtt distance between a ,  - - 
ai and b, . 4 . hi+ The algorithm in Fig- 
3.55 can be used to compute the d matrix. The funclion repI is just 
the mst of a character replacement: rep/(a,, hi) = 0 i f  a, - b,, I oth- 
erwise. 
Fig. 3.55. Algorithm to compute edit distance between two strings. 
a) What is the relation between the distance metric nf Exercise 3.34 
and edit distance? 
b) Use the algorithm in Fig. 3.55 to compute the edit distance 
between ahbb and Bahau. 
C) Construct an algorithm that prints out the minimal sequence of 
editing transformations required to transform x into y. 
3.36 Give an algorithm that takes as input a string x and a regular expres- 
sion r, and produces as output a string y in L ( r )  such that d{x, y) i s  
as small as possible, where d is the distance function in Exercise 3.34. 
PROGRAMMING EXERClSES 
P3.1 Write a lexical analyzer in Pascal or C for the tokens shown in Fig. 
3.11). 
P3.2 Write a specification for the tokens of Pascal and from this spccifica- 
tion construct transition diagrams. Use the transition diagrams to 
implement a lexical analyzer for Pascal in a language like C or Pascal. 

CHAPTER 3 
BlBLlOGRAPHlC NOTES 
157 
P3.3 Complete the Lex program in Fig, 3-18, Compare the size and speed 
of the resulting kxical analyzer produced by Lcx with the program 
written in Exercise P3.1. 
P3.4 Write a Lex specification for the tokens of Pascal and use the Lex 
compiler to construct a lexical analyzer for Pascal. 
P3.5 Write a program that takes as input a regular expression and the 
name of a file. and produces as output all lines of the file that contain 
a substring denoted by the regular expression. 
F3.6 Add an error recovery scheme to the Lex program in Fig. 3.18 to 
enable it to continue to look for iokens in the presence of errors, 
PA7 Program a lexical analyzer from the DFA constructed in Exercise 3.18 
and compare this lexical analyzer with that constructed in Exercises 
P3,l and P3+3. 
P3.8 Construct a tool that produces a lexical analyzer from a regular 
expression description of a set of tokens. 
BIBLIOGRAPHIC NOTES 
The restrictions imposed on the lexical aspects of a language are often deter- 
mined by thc environment in which the language was created. When Fortran 
was designed in 1954, punched cards were a common input medium. Blanks 
were ignored in Fortran partially because keypurrchers, who preparcd cards 
from handwritten notes, tended to miscount blanks; (Backus 11981 1). 
A l p !  
58's separation of the hardware reprexntation from the reference language 
was a compromise reached after a member of the design committee insisted, 
"No! I will never use a period for a decimal p i n t  ." (Wegstein [1981]). 
Knuth 11973a) presents additional techniques for buffering input. Feldman 
11 979b j discusses the practical difficulties of token recognition in Fortran 77. 
Regular expressions were first studied by Kkene I 19561, who was interest& 
in describing the events that could k represented by McCulloch and Pitts 
119431 finite automaton model of nervous activity, The minirnizatbn of finite 
automata was first studied by Huffman [I9541 and Moore 119561. The 
cqu ivalence of deterministic and nondeterministic automakt as far as their 
ability to recognize languages was shown by Rabin and Scott 119591. 
McNaughton and Yamada 119601 describe an algorithm to construct a DFA 
directly from a regular expression. More of the theory of regular expressions 
can be found in Hopcroft and Ullrnan 119791. 
11 was quickly appreciated that tools to build lexical analyzers from regular 
expression specifications wou Id be useful in the implementat ion of compilers. 
Johnson at al. 119681 discuss an early such system. Lex, the language dis- 
cussed in this chapter, is due to Lesk [1975], and has been used to construct 
lexical analyzers for many compilers using the UNlX system. The mmpact 
implementation scheme in Section 3+9 for transition tabks is due to S. C. 

158 
LEXICAL ANALYSIS 
CHAPTER 3 
Johnson, who first used it in the implementation of the Y ace parser generator 
(Johnson 119751). 
Other table-compression schemes are discussed and 
evalualed in Dencker, Diirre, and Heuft 1 19841. 
The problem of compact implementation of transition tables has k e n  
the~retiC3lly studied in a general setting by Tarjan and Yao 119791 and by 
Fredman, Kurnlbs, and 
Szerneredi 119841+ Cormack, Horspml, and 
Kaiserswerth 119851 present s perfect hashing algorithm based on this work. 
Regular expressions and finite automata have been used in many applica- 
tions other than compiling. Many text editors use regular expressions for coa- 
text searches. Thompson 1 19681, for example, describes the construction of an 
NFA from a regular expression (Algorithm 3.3) in the context of the QED 
text editor, The UNIX system has three general purpose regular expression 
searching programs: grep, egrep, and f greg. greg dws not allow union 
or parentheses for grouping in its regular expressions, but it does allow a lim- 
ited form of backreferencing as in Snobl. greg employs Algorithms 3.3 and 
3.4 to search for its regular expression patterns. The regular expressions in 
egrep are similar to those iin Lex, excepl for iteration and Imkahead. 
egrsp uses a DFA wich lazy state construction to look for its regular expres- 
sion patterns, as outlined in Section 3.7. fgrep Jmks for patterns consisting 
of sets of keywords using the algorithm in Ah0 and Corasick [1975j, which is 
discussed in Exercises 3.31 and 3.32. Aho 1 I9801 discusses the rehtive per- 
formance of these programs. 
Regular expressions have k e n  widely used in text retrieval systems, in 
daiabase query languages, and in file processing languages like AWK (Aho, 
Kernighan, and Weinberger 1 19791). Jarvis 19761 used regular expressions to 
describe imperfections in printed circuits. Cherry [I982 1 used the keyword- 
matching algorithm in Exercise 3.32 to look for poor diction in mmuscripts. 
The string pattern matching algorithm in Exercises 3.26 and 3.27 is from 
Knuth, Morris, and Pratt 1 19771. This paper also contains a g
d
 
discussim 
of periods in strings+ Another efficient algorithm for string matching was 
invented by Boyer and Moore 119771 who showed that a substring match can 
usuaily be determined without having to examine all characters of the target 
string. Hashing has also been proven as an effective technique for string pat- 
tern matching (Harrison [ 197 1 1). 
The notion of a longest common subsequence discussed in Exercise 3.34 has 
h e n  used in the design of the VNlX system file comparison program d i f f  
(Hunt and Mcllroy 119761). An efficient practical algorithm for computing 
longest common subsequences is describtd in Hunt and Szymanski [1977j+ 
The algorithm for computing minimum edit distances in Exercise 3.35 is horn 
Wagner and Fischcr 1 19741. Wagner 1 1974 1 contains a solution to Exercise 
3.36, 
&.nkoff and Krukal 11983) contains a fascinating discussion of the 
broad range of applications of minimum distance recugnition algorithms from 
the study of patterns in genetic sequences to problems in speech processing. 

CHAPTER 4 
Syntax 
Analysis 
Every programming language has rules that prescribe the syntactic structure of 
wefl-formed programs. tn Pascal, for example. a program is made out of 
blmks, a block out of statements, a statement out of expressions, an expres- 
sion wt of tokens, and so on. The syntax of programming language con- 
structs can be described by context-free grammars or BNF (Backus-Naur 
Form) notat ion, introduced in Sect ion 2.2. Grammars offer significant advan- 
tages to b t h  language designers and compiler writers. 
A grammar gives a precise, yet easy-to-understand, syntactic specificat ion 
of a programming language. 
From certain classes of grammars we can automatically construct an effi- 
cient parser that determines if a source program is syntactically well 
formed. As an additional benefit, the parser construction process can 
reveal syntactic ambiguities and other difficult-to-parse constructs that 
might otherwise go undeteded in the initial design phase of a language 
and its compiler. 
A properly designed grammar imparts a structure to a programming 
language that is useful for  he translation of source programs into correct 
object d e  and for the detection of errors. Tmls are available for con- 
verting grammar-based descriptions of translations into working pro- 
grams. 
Languages evolve over a perid of time, acquiring new constructs and 
performing additional kasks. These new slnstrurts can be added to a 
language more easily when there is an existing implementation based on a 
grammatical description of the language. 
The bulk of this chap~er is devoted to parsing methods that are typically 
used in compilers. We first present the basic concepts, [hen techniques that 
are suitable for hand implementation, and finally algorithms that have been 
used in automated tools, Since programs may contain syntactic errors, we 
extend the parsing methods so they recover from commonly wcurring errors. 

160 
SYNTAX ANALYSIS 
SEC. 4.1 
4.1 THE ROLE OF THE PARSER 
In our compiler m d d ,  the parser obtains a string of tokens from the lexical 
analyzer, as shown in Fig. 4.1, and verifies that the string can k generated by 
the grammar for the source language. We expect the parser to report any 
syntax errors in an intelligible fashion. It should also recover from commonly 
occurring errors w that it can continue prmssing the remainder of its input. 
Fig, 4.1. Position of pmcr in winpilcr mndcl. 
Thcre are three general types of parsers fur grammars. Universal parsing 
mcthds such as the Cwke-Younger-Kwarni algorithm and Earky's algorithm 
can parse any grammar (see the bibliographic notes). These methods, how- 
ever, are too inefficient to use in production compilers, The methods com- 
monly used in mmpilcrs are classified as being either topdown or bottom-up. 
As indicated by their names, topdown.parwrs build parse trees from the top 
(root) to the bottom (leaves), while bottom-up parsers start from the leaves 
and work up to rhe rmt. In both cases, the input to the parser is scanned 
from left to right, one symbol at a time. 
The most efficient top-down and bottom-up methods work only an sub* 
classes of grammars, but several of these sukla~ses, such as the LL and LR 
grammars, are expressive enough to describe most syntactic constructs in pro- 
gramming languages. Parsers implemented by hand often work with LL 
grammars; e,g,, the approach of Section 2.4 constructs parsers for LL gram- 
mars. Parsers for the larger chss of LR grammars are usually construcled by 
automated tools. 
In this chapter, we assume the output of the parser is some representation 
of the parse tree for the stream of tokens produced by the lexical analyzer. In 
practice, there are a number of tasks that might be conducted during parsing, 
such as collecting information about various tokens into the symbol table, per- 
forming r y p  checking and other kinds OF semantic analysis, and generating 
intermediate code as in Chapter 2. We have lumped all of these activities into 
the ''rest of front end" box in Fig. 4+1. We shall discuss these activities in 
detail in the next three chapters. 

SEC. 4.1 
THE ROLE OF THE PARSER 
161 
In the remainder of this section, we consider the nature of syntactic errors 
and gencnl strategies for error recovery. Two of these strategies, called 
pan ic-mode and phrase-level recovery, are discussed in mare detail together 
with the individual parsing methods. The implementation of each strategy 
calls upon the compiler writer's judgment, but we shall give some hints 
regarding approach. 
Syntax Error Handling 
If a compiler had to process only cwrect programs, its design and implementa- 
tion wou Id be greatly simplified. But programmers frequently write incorrect 
programs, and a good compiler should assist the programmer in identifying 
and locating errors. It is striking that although errors are so commonplace, 
few languages have been designed with error handling in mind. Our civiliza- 
tion would be radically different if spoken languages had the same require- 
men t for syntactic accuracy as computer languages, Most programming 
language specificahns do not describe how a compiler should respond to 
errors; the response i s  left to the compiler designer. Planning the errw han- 
dling right from the start can both simplify the structure of a compiler and 
improve its response to errors. 
We know that programs can contain errors at many different levels. For 
example, errors can be 
+ 
lexical, such as misspelling an identifier, keyword, or operator 
+ 
syntactic, such as an arithmetic expression with unbalanced parentheses 
+ 
semantic, such as an operator applied to an incompatible operand 
logical, such as an infinitely recursive call 
Often much of the error detection and recovery in a compiler is centered 
around the syntax analysis phase. One reason for this is thal many errors are 
syntactic in nature or are exposed when the stream of tokens coming from the 
lexical analyzer disobeys the grammatical ru ks defining the programming 
language. Another is the precision of modern parsing methods; they can 
detect the presence of syntactic errors in programs very efficiently. Accu- 
rately detecting the presence of semantic and logicel errors at compile time is 
a much more difficult task. In this section, we present a few basic techniques 
for recovering from syntax errors; their implementation is discussed in con- 
junction with the parstng methods in this chapter. 
The error handler in u parser has simple-to-state goals: 
It should ,report the presence of crrors clearly and accurately. 
It should recover from each error quickly enough to be able to detect sub- 
sequent errors. 
It should not significantly stow down the processing of correct programs, 
The effective realization of these goals presents difficult challenges. 
Fortunately, common errors are simple ones and a relatively straightforward 

162 
SYNTAX ANALYSIS 
SEC. 4.1 
error-handling mechanism often suffices. In some cases, however, an error 
may have occurred long More the position at which its presence is detected, 
and the precise nature of the error may be very difficuh to deduce. In diffi- 
cult cases. the error handler may have to guess what the programmer had in 
mind when the program was written. 
Several parsing methods, such as the LL and LR methods, detect an error 
as soon as possible. More precisely, they have the viable-prefa property. 
meaning they detect that an error has mcurred as soon as they see a prefix of 
the input that i s  not a p r e f i ~  of any string in the language. 
Example 4.1. To gain an appreciation of the kinds of errors that occur in 
practice, let us examine the errors Ripley and Druseikis 119'781 found in a 
sample of student Pascal programs. 
They diwmered that errors do not occur that frequently; 6Q% of the pro- 
grams compiled were syntactically and slemantically correct. 
Even when 
errors did occur, they were quite sparse; 80% of the statements having errors 
had only one error, \3% had two. Finally, mast errors were trivial; 90% 
were single token errors. 
Many of the errors could be classilied simply: 6U% were punctuation errors, 
20% operator and operand errors, 15% keyword errors, and the remaining 
five per cent other kinds. The bulk of the punctuation errors revolved around 
the incorrect use of ~micolons. 
Fw some concrete examples, consider the following Pascal program. 
(1) 
program prmaxIinput, output); 
0 1  
var 
(31 
x, y: integer; 
(4) 
function rnaxli:integer; j:integer) : integer; 
r 5 )  
{ return maximum of integers i and j } 
(bl 
Begin 
(7) 
if i > j then max := i 
If9 
else max := j 
19) 
end ; 
A common punctuation error is to use a comma in place of the semicolon in 
the argurnenl list of e function declaration (e.g., using a comma in place of 
the first sernbfon on line (4)); anolher is to leave out a mandatory semicolon 
at  he end of a line (e.g., the semicolon at the end of lint (4)); another is to 
put in an extraneous semicolon at the end of a line before a n  else (e.g., put- 
ting a semicolon at the end of line (7)). 
Perhaps one reason why semicolon errors are so common is that the use of 
semicolons varies greatly from cine language to another. 
In Pascal, a 

SEC. 4. 1 
THEROLEOFTHEPARSER 
163 
semicolon is a statement separator: in PL/l and C, it is a statement termina- 
o r .  Some studies have suggested that the latter usage is less error prone 
{Gannon and Horning 1 1975 1). 
A typical example of an operator error is to leave out the colon from : . 
Misspellings of keywords are usually rare, but leaving out the i from 
w r  i t t l n  would be a representative example, 
Maoy Pam1 compilers have no difficulty handling common insertion, dele- 
tion. and mutation errors. In fact, several Pascal compilers will correctly corn- 
pile the above program with a common punctuation or operator error; they 
will issue only a warning diagnostic, pinpointing the offending construct. 
However, another common type of error is much more difficult to repair 
correctly, This is a missing begin or end (c.g., line (9) missing). Most 
compilers would not try to repair this kind of error. 
o 
How should an error handler report the presence of an error'? At the very 
least, i t  should report the place in the pource program where an error is 
detected because there is a good chance that the actual error rxcurred within 
the previous few tokens. A common strategy employed' by many compilers is 
to print the offending line with a pointer to the position at which an error is 
detected. If there is a reasonable likelihood of what the error actually is, an 
informative, understandable diagnostic message is also included; e+g . , "sern i- 
colon missing at this position. 
I + 
Once an error is detected, how should the parser recover'? As we shall see, 
there are a number of general strategies, but no one method clearly dom- 
inates. In most cases, it is not adequate for the parser to quit after detecting 
the first error, because subsequent prciwssing of the input may reveal addi- 
tional errors. . Usually, there is some form of error ramvery in which the 
parxr attempts to restore itself to a state where processing of the input can 
continue with a reasonabk bope that correci input will be parsed and other- 
wise handled correctly by the compiler. 
An inadequate job of recovery may introduce an annoying avalanche of 
"spurious" errors, those that were not made by the programmer, but were 
inkroduced by the changes made to the parser state during error recovery. In 
a similar vein, syntactic error recovery may introduce spurious semantic errors 
that will later be detected by the semantic analysis or code generation phases. 
For example, in recovering from an error, the parser may skip a declaration 
of some variable, say zap. When zap is later encountered in expressions, 
there is nothing syntactically wrong, but since there is no symbul-table entry 
for zap, a message "zap undefined" is generated, 
A mnservative strategy for a compiler is to inhibit error messages that stem 
from errors uncovered too close together in the input stream. After discover- 
ing me syntax error, the compiler should require w c r a i  tokens to be pared 
successfully before pzrmitting another error message. In some cases, there 
may be too many errors For the compiler to continue sensible processing. {For 
example, how should s Pascal compiler respond to a Fortran program as 

SYNTAX ANALYSIS 
SEC. 4.1 
input?) It seems that an error-rccovery strategy has to be a carefully con- 
sidered compromise, raking into account the kinds of errors ihal are likely to 
occur and reasonable to process. 
As we have mentioned, mme compilers attempt error repair, a process in 
which the compiler attempts ro guess what the programmer intended to write. 
The PLK compiler (Conwag and Wilcox 119731) is an example of this type of 
compiler. Except possibly in an environment of short programs written by 
beginning students. exrensive error repair is not likely to be cost effective. In 
fact. with the increasing emphasis on interactive computing and good pro- 
gramming environments, the trend seems to be toward simple error-recovery 
mechanisms. 
Error-Recovery Strategies 
There are many different gcneral strategies that rr. parser can employ to 
recover from a syntactic error. Although no one strategy has proven itself to 
be universally acceptable, a few methods have broad applicability. Here we 
intrduce the following strategies; 
panic mode 
phrase level 
error productions 
global correction 
Panic-mode recrrvery. This is the simplest method to implement and can be 
used by most parsing methods. On discovering an error, the parser discards 
input symbols one at a time until one of a designaied set of synchronizing 
tokens is found. The synchronizing tokens are usuaiiy delimiters, such as 
semicdon or end, whose role in the source program is clear. The compiler 
designer must sclect the synchronizing tokens appropriate for the source 
language, of course. While panic-mode correction often skips a considerable 
amount of input without checking it for additional errors, i t  has the advantage 
of simplicity and, unlike some other methods to be considered later, it is 
guaranteed n a  to gct inio an infinite loop. in situations where multiple errors 
in the samc statement are rare, this method may tx quite adequate, 
Phrudcvel recuvury. On discovering an error, a parser may perform local 
correction on the remaining input; that is, it may replace a prefix of the 
remaining input by some h~ring that allows the parser to continue. A typical 
local correction would be to replace a comma by a semi~wlon. delete an 
extraneous semicolon, or insert a missing semicolon. The choice of the local 
correction is left to the compiler designer. Of cuurse. we must be carefu I to 
choose replacements that do not lead to infinite loops, as would be the case, 
for example, if we always inserted something on the input ahead of the 
current input symbd. 
This type of replacement can correct any input string and has been used in 
several error-repairing compilers. The method was first used with top-down 
parsing. Its majw drawback is the difficulty it has in coping with situations in 

SEC. 4.2 
CONTEXT-FREE GRAMMARS 165 
which the actual crror has occurred beforc thc p i n t  ot' detection. 
Error prudui-tinns. If wt: have a gwd Idca of thc common errors that might 
be encountered, wc can augment the grammar for the language at hand with 
productions that generatc the crroncous constructs, We thcn usc thc grammar 
augmented by these error prt~luctians to construct a parser, if an error pro- 
duction is u
d
 
by the parser, w~ can gcncratc appropriate error dhgnost ics to 
indicatc the erroneous construct that has hccn rccognI;rtd in the input. 
G
o
 
r
+
 
Ideally, we would like a cornpilcr to rnakc as fcw 
changes as possible in prwesing an inmrrca input st~iny. Therc are algo- 
rithms for choosing ii minimal sequencc of changes to obtain a globally least- 
wst correction. Given an incorrect input string x and grammar G. these algo- 
rithms will find a parse tree for u related string y, such that the number of 
insertions, deletions. and changcs of tokcns rcquircd to transform x into y is 
as small as possible. Unfortunately, thcsc methods are in general tm costly to 
implement in terms rrf time and spam, MI these techniques arc currently only 
of theoretical interest. 
We should point out that a closest correct program may nut tw what the 
programmer had in mind. Ncvcrthcless. thc notion of least -cost correct ion 
dws provide a yardstick for evaluating error-rcwvery techniques, and it has 
becn used for finding optimal rcplaccment strtngs for phraw-levcl recovery. 
4.2 CONTEXT-FREE GRAMMARS 
Many programming language crsnstructs haw an inhcrcntly rcuursive structure 
that can bc dcfined by contextdfrcc grammars. For example, wc might h a w  a 
conditional ~tatcrncnt dcfined by a rule such as 
If S, and S? are statements and E i s  nn cxpression. then 
(4.1) 
"id E then S ,  else S2" Its a statemcnt. 
This rorm aT cwnditirsnal statcnwnt cannot be specified using thc notation of 
regular expressions; in Chapter 3, we saw that regular cxpressirm can specify 
the lexical Slrudurt of tokens. On the nthcr hand, using the syntactic variable 
srmr to denote the clas of statemcnrs and Pxpr the class uf expressions, wc can 
readily exprcss (4. I) using the grammar production 
In this section, we review thc definition of a crmtcxt-Crcc grammar and 
introduce tcrminotogy for talking about parsing. From Section 2.2. a contcxt- 
free grammar (grammar for short) cunsists of terminals, nunterminals, a start 
symbol, and prmluct ions. 
1 +  Terminals are thc basic symbwls from which strings arc formed. The 
word "token" is a synonym for "terminal" when wc arc talking about 
grammars fur programming languigcs. in (4.21, tach of thc keywords if, 
then. and else is a terminal. 

166 SYNTAX A N A L W S  
SEC. 4.2 
2. 
Nonterminals are syntactic variables that denote sets of strings. In (4.21, 
srmt and q
r
 are nonterminds. The nonterrninals define sets of strings 
that help define the language generated by the grammar. They also 
impose a hierarchical structure on the language that is useful for Both 
syntax analysis and translation. 
3, 
Iln a grammar, m e  nonterminal is distinguished as the start symbol, and 
the set of strings it denotes is the language defined by the grammar. 
4. 
The productions of a grammar specify the manner in which the terminals 
and nonterminals can be combined to form strings. Each production con- 
sists of a nonterminal. fohwed by an arrow (sometimes the symbol ;:= 
is used in place of the arrow), followed by a string of nmterminals and 
terminals. 
Example 4.2, The grammar with the following prdudbns defines simple 
arithmetic expressions. 
In this grammar, the terminal symbols are 
The nonterminal symbols are vxpr and rtp, and expr is the start symbol. 
Nohticurd Conventions 
To avoid always having to state that "these are the terminals," "these are [he 
nonterminals," and so on, we shall employ the following notational ccinven- 
tims with regard to grammars throughout the remainder of this book. 
I .  
These symbls are terminals: 
i) Lower-case Ietters early in the alphabet such as u, b, c. 
ii) Operator symbols suchas +, -,etc. 
iii) Punctuatim symbols such as parentheses, comma, clc. 
iv) The digits 0, 1 ,  . . . , 9. 
V) 
Boldface strings such as id or if. 
2. These symbuls are nmterrninals: 
i) 
Upper-case letters early in the alphabet such as A, B, C. 

SEC. 4.2 
CONTEXT-FREE GRAMMARS 
t67 
ii) 
The letter S, which, when it appears, is usually the start symbol. 
iii) Lowcr-case italic names such as expr or ssms. 
3. 
Upper-case letters late in the alphabet, such as X, Y ,  2, represent gram- 
m r  symbls, that is, either nonterminals or terminals. 
4. 
Lower-case Ietters late in the alphabt, chiefly u, v ,  . . . , z, represent 
strings of terminals. 
5 .  Lower-case Greek letters, or, P, y, for example. represent strings of 
grammar symbols. Thus, a generic production could be written as 
A + a, indicating that there i s  a single nonterminal A on the left of the 
arrow (the Iefr side of the production) and a string of grammar symbols a 
to the right of rhe arrow (the rigkr side of the productim). 
6. I f A  + a , , A  +al, . . . , A - cq are all productions with A on the left 
(we call them A-producriom), we may write A -+ a, lal 1 . . . lak. We 
cell a, , a2, . . . , a* the olrer~arive~ 
for A .  
7. 
Unless otherwise stated, the lcft side of thc first production is the slart 
symbl. 
Example 4.3. Using these shorthands, we could write the grammar of Exam- 
ple 4.2 concisely as 
Our notational conventions tell us that. E and A are nonterminals, with E the 
start symbol. The remaining symbols are ~crminak 
D 
There are several ways to view the process by which a grammar defines a 
language. In Section 2.2, we viewed this process as one of building parse 
trees, but there is also a related derivational view that we frequently find use- 
ful. In f a d ,  this derivational view gives a precise description of the top-down 
construction of a parse !Fee, The central idea here is that a production is 
treated as a rewriting rule in which the nonterminal on the lcft i s  replaced by 
the string on the right side of the production. 
For example, consider the following grammar fur arhhrnetic expressions. 
with the nonterminal E representing an expression. 
The production E - - E signifies that an expression preceded by a minus sign 
is also an expression. This production can be used to generate more complex 
expressions from simpler expressions by allowing us to replace any instance of 
an E by - E+ In the simpkt case, we can replace a single E by - E+ We 
can describe this action by writing 

168 
SYNTAX ANALYSIS 
which is read '+E derives -E." 
The prduction E - I&) 
tells us that we could 
a
h
 replace one instance of an E in any string of grammar symbols by ( E ) ;  
e.g., E*E * 
( E ) & E  or E*E * 
E * ( E ) .  
We can take a single E and repatedly apply productions in any order to 
obtain a sequence of replacements. For example, 
We call such a sequence of replacements a durivurion af -(id) from E. This 
derivation provides a prwf that one particular instance of an cxpressian is the 
string -(id). 
In a more abstraa setting. we say that a A B  
aye if A - y is a produe- 
tion and a and p are 
arbitrary 
strings of grammar 
symbols. 
If 
a, * 
a2 
. . 3 u,, , we say a, derivus a,,. The symbd * means 
"derives in one stcp." Often we wish to say "derives in zero or more steps. 
1 * 
For this purpose we can use the symbol a. 
Thus. 
1 .  
a % a Tor any string a, 
and 
2. 
I f u h ~ a n d P * y . ~ b c n a % = ~ .  
Likewise. we uw +to 
mean "derives in one or more steps." 
Given a grammar G with start symbol S, we can use rhe 5 relarion ro 
define L IG j. the Iunguugd* ~(~nurutud 
by G. Strings in L(C) may contain only 
terminal symbols of G. We say a string of terminals w is in L ( G )  if and only 
if S % w. The string w is called a sentence of G. A language that can be 
generated by a grammar is said to be a chomw$ree Iunpuge. If two gram- 
mars generate the same language, the grammars are said to be uyuivulrrrt. 
If S &- a, where a may contain nonterminals, then we say that a is a sen- 
rrnrid.form nf G. A sentence is a sententid form with no nonterminals, 
E~ampk 4.4. The string -Iid+id) is a sentence of grammar (4.31 bccause 
there is thc derivatirm 
The strings E, - E ,  - ( E l ,  . . . , -(id+ id) appearing in this derivation are all 
sentential forms of this grammar. Wc write E % -(ld+id) to indicate thar 
- (id+id) can be derived from E. 
We can show by induction on the length of a derivation that every sentence 
in the language of grammar (4.3) is 'an arithmetic expression involving the 
binary operators + and *, [he unary operator -, parentheses, and the 
operand id. Similarly, we can show by induction on the length of an arith- 
metic expression that all such expressions can be generated by this grammar. 
Thus, grammar (4.31 generates precisely the set of ail arithmetic expressions 
involving binary + and *, unary - , parentheses, and the operand Id. 
o 
At each step in a ber ivation, there are two choices t be made. W e  need to 
choose which nonterrniaal to replace. and having made this choice. which 

SEC+ 4+2 
CONTEXT-FREE GRAMMARS 169 
alternative to use 'for that nunterminal. For example, derivation (4.4) of 
Example 4.4 could continue from - ( E  + E )  as follows 
Each nonkrminal in (4.5) is replaced by the same right side as in Example 
4.4, but the order of replacements is different. 
To understand how certain parsers work we need to consider derivations in 
which only the leftmost nonterminal in any sentential form is replaced at each 
step, Such derivations are termed kjmoss. If u * 
p by a step in which the 
leftmost nonterminal in a is replaced, we write ot j$ 
p. Since derivation 
(4.4) is leftmost, we can rcwrite it as; 
Using our notational conventions, every leftmost srep can be written 
WAY 3 
why whrrc w mnsisis of terminals only, A - S is the production 
applied, and y is a string of grammar symbols. To emphasize the fact that a 
derives p by a leftmost derivation. we write a 2 B .  If S 
u, then we say 
a is a kfb.wnkrrrirr/,f#rm of the grammar at hand. 
Analogous defmitipns hold for righrmtsr derivations in which the rightmost 
nonterminal is replaced at each step. Rightmost derivations are sometimes 
called ~.unnnirrr/ ber i vat ion s . 
Parse Trees and Derivations 
A parse tree may be viewed as u graphical representation for a derivation that 
filters out the choicc regarding replacement order. Recall from Section 2.2 
that each interior nde of a parse tree i s  labeled by some nortterminal A ,  and 
that the children of the node are labeled, from left to right, by the symbols in 
the right side of the production by which this A was replaced in the derivation. 
The lcaves of the parse tree are labeled by nanterminals or terminals and. 
read from kft tu right, rhey constitute a sentential form, called ihe yield or 
frontier of 'the tree, For example. the parse tree for - (idi- id) implied by 
derivation (4.4) is shown in Fig. 4.2. 
To see the relationship between derivations and parse trees, consider any 
dcrivation otl * 
a2 . . + + a , ,  where U, is a single nonttrmhal A+ For 
each sentcntial form ai in the derivation, we construct a parse tree whose 
yield is ai. The process is an induction on i, Fw the basis, rhe tree for 
al 
A is a singie node labeled A. To do thc induction, suppose we have 
already constructed a parse tree whose yield is a i - ,  = X !X2 * 
XI. 
IRecaL 
ling our conventions, each Xi is either sr nclnterminal or a terrninaI+) Suppose 
oli is derived from a, - by replacing X,, a nonterrninal, by 0 = Y Y . . . Y,. 
That is, at the ith step of the derivation, production X, -. P is applied to ui-1 
to derive ai = XIXI - 
+ 
+ Xj-IpXj+, 
- . X,. 
To model this step of the derivation, we find the jrh leaf from the left in 
the current parse tree. This leaf i s  labeled Xi, We give this leaf r children, 
!ahlcd Y ,  . YI. . . . . Y,, 
from the left. As a special caw, if r = 0, i.c., 

170 
SYNTAX ANALYSIS 
Flg. 4.2. P a m  trec for -(id + id). 
Q = r, then we give the jth leaf one child labeled r. 
Example 4,s. Consider de~ivation (4.4). The sequence of parse trees con- 
structed from rhis derivation is shown in Fig. 4,3. In the first step of the 
derivation, E * 
-E. To model this step, we add two children, labeled - 
and E, to the root E of the initial tree to create the second tree. 
fig. 4.3, Building thc par& tree from derivation (4.4). 
In the second step of the derivation, -E 
-(El, Consequently, we add 
three children, labeled (, E, and ), to the leaf labeled E of the second tree to 
obtain the third tree with yield -{El. Continuing in this fashion we obtain 
the complete parse tree as the sixth tree. 
o 
As we have mentioned, a parse tree ignores variations in the wder in which 
symbols in sentential forms are replaced. For elrampie, if derivation (4.4) 
were continued as in line (4.51, the same final parse tree of Fig. 4.3 would 
result. These variations in the order in which prductions are applied cart also 
be eliminated by considering only leftrnost (or rightmost) derivations. It is 

SEC. 4.2 
CONTEXT-FREE GRAMMARS 
171 
not hard to see that every parse tree has associated with it a unique Iefirnast 
and a unique right most derivalion. In what follows, we shall frequently parse 
by producing a tefmost or right most derivation, understanding that instead of 
this derivation we ~ ~ u l d  
produce the parse tree itself, However, we should 
not assume that every sentence necessarily has only one par% tree or only one 
leftmob or rightmost derivation. 
Example 4.6. Let us again consider the arithmetic expression grammar (4.3). 
The sentence id + idkid has the two distinct kftmosc derivations: 
with the two corresponding parse trees shown in Fig. 4.4. 
Fig. 4.4. Two parsc trccs for M+Jd*id. 
Note that the parse tree or Fig. 4.4(rl) refleas the commonly assumed pre- 
cedence of + and *, while the tree of Fig. 4.4(b) does not. That is, i t  is cus- 
tomary to treat operator 
as having higher precedence than +, corresponding 
ro the fad that we would normally evaluate an exprewion like 
+ b*c as 
a + ( b * ~ . ) ,  
rather than as: (a + b ) ~ .  
Ambiguity 
A grammar that produces more than one parse tree for some sentence is said 
lo be unrbigums, Put another way, an ambiguous grammar is one that pro- 
duces more than one leftmosr or more than one rightmost derivation for the 
same sentence, For certain types of parsers, it i s  desirable that the grammar 
be made unambiguous, for I T  it is not, we cannot uniquely determine which 
parse tree to select for a sentence. For some applications we shall also can- 
sider methods whereby we can use certain ambiguous grammars, together with 
disambiguuting ruks that "throw away" undesirable parse lrees, leaving us 
with only one tree for each sentence. 

172 
SYNTAX ANALYSIS 
4.3 WRITIfiG A GRAMMAR 
Grammars are capable of describing must. but not all, of the syntax of pro- 
gramming languages. A limited amount of syntax analysis is done by a lexical 
analyzer as it produces the sequence of tokens from the input characters. Cer- 
tain constrahts on the input, such as the requirement that identifiers k 
declared before they are used. cannot be described by a context-free grammar. 
Therefore, the gquences of tokens accepted by a parser form rl superset of a 
programming language; subsequent phases must analyze the output of the 
parser to cnsure compliance with rules that are not checked by thc parser (see 
Chapter 6). 
We begin this section by considering the division of work ktween a lexical 
analyzer and a parser, Because each parsing method can handle grammars 
only of a certain form, the initial grammar may hare to be rewritten lo make 
it parsable by the method cho+i.cn, Suitable grammars for expressions can 
often bc constructed using aswiativity and, precedence information, as in k c -  
tion 2.2. [n this section. we consider transfurmations h a t  arc u d u l  for 
rcwriring grammars so they become suitable for top-down parsing, Wc con- 
clude this scction by considering some programming language constructls that 
cannot bc described by any grammar. 
Regular Expressifins vs. Contex t-Free Grammars 
Every construct that can be describcd by a regular expression can also be 
described hy a grammar. For example. the regular expression ( u  1 b)*& 
and 
the grainmar 
describc h c  samc language, the set OE strings of o's and 6's ending in uhb. 
We can mechanically canverr a nondeterministic finite automaton ( NFA I 
into a grammar that generates the same language as recognized by the NFA. 
The grammar above was constructed from the NFA of Fig. 3.23 using the fol- 
lowing construction; For each slatc i of thc NFA, crcatc a nnntcrminal sym- 
bol A,. I f  state i has a transit ion to state j on symbol rr. intruducc the prwluc- 
tinn A, - aA,. If state i gws to state j on input e, introduce the production 
A, - A,. 
If i is an accepting stale, intruduse A, -. r .  IT i is the start state. 
mukc A, be thc start symbol of €he grammar. 
Since every regular sct is a context-free languiige, we may reasonably ask, 
"Why use regular expressions to detine the lexical syntax of a language'?" 
There are w c r a l  reasonsA 
I. The lexical rules uf n language are frquently quite sirnplc, and to 
describe them we do not need a notation as powcrful as grammars. 

SEC. 4.3 
WRITING AGRAMMAR 
173 
2. ReguIar expressions generally provide a more concise and easier to under- 
stand notation for tokens than grammars, 
3. 
More efficient lexical analyzers can be mnslructed automatically from 
regular expressions than from arbitrary grammars. 
4. 
Separating the syntactic structure of a language into lexical and nonlexical 
parts provides a mnvenknt way of rnodularizing the front end of a corn- 
piler into two manageable-sized components. 
There are no firm guidelines as to what to put into the lexical ruies, as 
opposed to the syntactic rules. Regutar expressions are most useful for 
describing the structure of lexical constructs such as - identifiers, wnstants, 
keywords, and so 'forth. Grammars, on the other hand, are most useful in 
describing nested structures such as balanccd parentheses, matching begin- 
end's, correspnding if-then-else's, and MI on. As we have noted, these 
nested structures cannot bc described by regular cxprcssions. 
Verifying the Language Generated by a Grammar 
A lthough compiler designers rarely do it fur a complete programming 
language grammar, it is important to be able to reason that a given sel of pro- 
ductions generates a particular language. Trou blewrne constructs a n  be stu- 
died by writing a concise. abstract grammar and studying the language that it 
generates. We shall construct such a grammar for conditionals below. 
A proof that s grammar G generates a language L has two parts: we mus1 
show that every string generated by G is in L. and conversely that every string 
in L can indeed be generated by G .  
Example 4.7, Consider the grammar (4.6) 
It may nor be initially apparent, but this simple grammar generates all strings 
of balanced parentheses, and only such strings. To see this, we shall show 
first that every sentencr: derivable [rum S is balanced, and then that every bal- 
anced string is dtrivable frnm S. To show that every sentence derivable from 
S is balanced, we use an inductive prod un the nurnhr of steps in a deriva- 
tion. For the basis step, we note that the only string of terminals derivabk 
from S in one step is the empty string, which surely is balanced. 
Now assume that all derivalions of fewer than n steps produce balanced sent 
fences, and consider a leftmost derivation of exactly n steps. Such a deriva- 
tion must be of the form 
The derivations of x and y Crom S take fewer than n steps SU, by [he inductive 
hypothesis. x and y are balanced, Therefore, rhc string Ix)y musl be bat- 
anced. 
We have thus shown that any string derivable from S is balanced. Wc ~llust 

next show that every balanced string is derivable from S. To do this we use 
induction on the lenglh of a string. For the basis step, the empty string is 
derivable from S+ 
Now smume that every balanced string of length less than 2n: is derivable 
from S, and consider a balanced string w of length 2n, n 2 I. Surely w 
kgins with a left parenthesis. Let {x) be the shortest prefix of w having an 
equal number of left and right parentheses. Then w can be written as (x)y 
where both x and y are balanced, Since x and y are of length less than 2n. 
they are derivable from S by the inductive hypothesis. Thus, we can find a 
derivation d the form 
proving that w = (x)y is also derivable from S, 
Eliminating Ambiguity 
Sometimes an ambiguous grammar can be rewritten to eliminate the ambi- 
guity. As an example, we shall eliminate the ambiguity from the ~ollowing 
"dangling-else" grammar; 
Here "other" stands for any other statement. According to this grammar, the 
compound conditional statement 
has the parse tree shown in Fig. 4.5. Grammar (4.7) is ambiguous since the 
string 
has the two parse trees shown in Fig, 4.6. 
Fig. 4.5. P u t s  trcc fm wnditiunal stotcrncnt. 

if 
expr 
tben 
srmr 
if 
x 
then 
. ~ t m  
Fig. 4.6. TWO parse trees for an ambiguous scntcncc. 
In all programming languages with conditional statements of this form, the 
first parse tree is preferred. The geoeral rule is, "Match each else with the 
chest previous unmatched them++' This disambiguating rule can be incw 
p ~ r a t d  directly into the grammar. For example, we can rewrite grammar 
(4,3) as the following unambiguous grammar. The idea is that a statement 
appearing betwecn r then and an dse must be "matched;" i.e., it mud not 
end with an unmatched then fdlc~wed by any statement, for the else would 
then ht forced to match this unmatched then+ A matched statement is either 
an if-then-else statement containing no unmatched statements or it i s  my other 
kind of unconditional statement. Thus, we may use the grammar 
This grammar generates the same set of strings as (4.71, but it allows only one 
parsing fur string (4.81, 
namely the m e  that associates each else with the 
closest previous unmatched then. 

176 
SYNTAX ANALYSIS 
Ellminatbn of Len Rccursh 
A grammar is kfI ~ P C U ~ J ' S I V P  
if it has a ntmterrninal A such that there is a 
derivation A 5 
~a for some siring a. Tup-duwn parsing methods cannot 
handlc left-recursive gram mars, so a transformation that eliminates left rwur- 
sion is needed. In Section 2,4, we discussed simple left recursion, whcrc there 
was one production of the fwm A - Am. Here we study the gencral cai;e, In 
M i o n  2.4, we showcd how the Icft-recursive pair of productions A -. A a  1 P 
could be replaced by the non-lcft-rccursivc productims 
A - $At 
A' 4 aA' [ r 
without changing the set of strings derivable from A. This rule by itself suf- 
fices in many grammars. 
Examplo 4,8. Consider thc following grammar for arithmetic expressions, 
E - E + T ( T  
T - T t F I F  
44-10) 
F - IE) 
1 id 
Eliminating the immediaie kft recursim (prductiuns oC the form A -. A a )  to 
the prductions for E and then for T, we obtain 
E - TE' 
E' -. +TE' I r 
T + FTt 
(4.1 1 )  
T' -. *FT' I E 
f - I0 I 
n 
No matter how many A-productions there are, we can eliminate immediate 
left tccursion from them by thc €r>llowing technique. First, we group the A- 
productions as 
A + A a ,  t A a 2  I .
*
.
 1
4
 
I P l  I P 1  I . . .  \ P a  
whcre no j3, begins with an A .  Then, wc replam the A-productions by 
A + b I A '  I PIAt I 
- . I P,A' 
A' - alA' 1 a2Ar I . . 
I a,A' 
I r 
Thc nontcrrninal A generates the same strings us before but is no kongcr left 
rccursivc. This prmdure eliminates all immediate left recursion from the A 
and A' prductirrns Iprwided nu ai i s  €1, but it dws not eliminate left recur- 
sion involving derivations of two or 
grammar 
S 4 A u  Ib 
A + A r  I W I r  
The nonterminal S is left-recursive 
more step. For example, consider the 
(4.12) 
kcause S 
Au * 
.Wu, 
but it is nor 

SEC. 4.3 
WPIT1NG A GRAMMAR 
177 
immediately left recursive. 
Algorithm 4.1, below, will systemarically eli~ninate left ~xmrsion from a 
grammar. It is guaranteed to work if [he grammar bas no q d c s  (derivations 
of the form A 5 A )  or t-productions (productions of the form A - E). 
Cyclcl; can be syskrnatically eliminated from a grammar as can r-productions 
{see Exercises 4.20 and 4.22). 
Algorithm 4.1. Eliminating left recursion. 
frrpur. Grammar G with no cycles or c-prc~ctuctions. 
Ourput. An equivalent grammar with no left recursion. 
Method. Apply the algorithm in Fig. 4.7 fci G, Notc that the resulting non- 
left-recursive grammar may have r-productions. 
13 
1. 
Arrange thc nonterminals in wme ordcr A , ,  A ?. . . , . A,, . 
[or i := I to rr do begin 
for j ;= I tu i - I do begin 
replace tach product ion of the lorn, A, 
A ,  y 
by the productions Ai - 6!y ( 6]y I . . , ! &Piq 
whcre A, - 6 ,  
1 8; ] . - 
- 
1 EA arc all Chcrurrcnt Ai-productions; 
end 
eliminate thc immediate left recursion among the A,-productions 
end 
Fig. 47, A lgorithnr to eliminak kft recursion from a grammar 
Thc reason the procedure in Fig. 4.7 works is that after the i -  I" ireration 
of the outer for loop in step (21, any production of the form AL --+ A p ,  where 
k < I, must have 1 > 4, As a result, on the next iteration, the inner hop (on ji 
progressively raises the lower limit on m in any production Ai + &,,a, until we 
must have m 2 i. Then, eliminating immediate lefr recursion for the Ai- 
prductions forces m to he greater than i. 
Example 4.9. Let us apply this procedure to yramniar (4.12). Technically, 
Algorithm 4.1 i s  nM guaranteed to work, because of the E-production. but in 
this case the production A - E turns out to be harmless. 
We order the nonterminals S, A. Therc is no immediate left recursion 
among the S-productions. so nothing happens during step 12) for the case i = 
I .  For i = 2, we substitute the S-productions in A - Sd to obtain the follow- 
ing A-productions. 
Eliminating the immediate left recurstan among the A-productions yields the 
following grammar. 

178 
SYNTAX ANALYSIS 
Left Factoring 
LRft facroring is a grammar transformittion that is useful for producing a 
grammar suitable for predictive parsing. The basic idea is that when it is not 
clear which of two alternative productions to use to expand a n~nttrrninal A, 
we may be able to rewrite the A-productions to defer the decision until we 
havc seen enough of rhe input to make the right choice. 
For example, if we havc the two productions 
on seeing the input token if, we cannot immediately tell whtch production to 
choose to expand stmt. In general, if A - clPl I upz are two A-productions, 
and the input begins with a' noncmpty string derived from a, we do not know 
whether to expand A ro apl or to c&. 
However, we may defer the decision 
by expanding A to &A'. Then, after seeing the input derived frnm a, we 
expand A' to PI or to p2- That is, left-factored, the original prwduchns 
become 
Algorithm 4+2. Left factoring a grammar, 
input. Grammar G. 
Ourput, An equivalent left-factored grammar. 
Methurl. For each nonterminal A find the bagest prefix a common to two or 
more of its alternatives. 
IF a + r. ix., there is a nontrivial common prefix, 
replace all the' A productions A + 
I orB2 I 
. . I up,, I r where y 
represents all alternatives thar do not begin with a by 
Here A' is a new nonterminal. Repeatedly apply this transformation until no 
rwo alternatives fr~r a nrsntermjnal have a common prefix. 
o 
Example 44.10, The following grammar abstracts the dangling-else problem: 
Here i, t, and u stand for if, then and else, E and $ for "expression" and 
"statement," Left -factored, this grammar becomes: 

S&C. 4.3 
WRITING A GRAMMAR 
179 
Thus, we may expand S to iEtSS' on input i, and wait until iEiS has been seen 
to decide whether to expand S' to d or to r. Of course, grammars (4.13) and 
(4,141 are both ambiguous, and on input c, it will not be dear which alterna- 
tive for S' should be chosen. Example 4+19 discusm a way out of this 
dilemma. 
o 
Ic should come as no surprise that some languages cannot be generated by any 
grammar. In fact. a few syntactic construds found in many programming 
languages cannot be specified using grammars alone. In this section, we shall 
present xveral of these constructs, using simple abstract languages to i llus- 
trate the difficulties. 
Exampie 4.11. Consider the abstract language L = {wcw I w is in (uib)*}. 
L , consists of all words m m p d  of a repeated string of u's and bps separated 
by a c ,  swh as aabcaab. I t  can be proven this language is not context free. 
This language abstracts the problem of checking that identifiers are declared 
before their use ia a program. That is, the first w in wcw represents the 
declaration of an identifier w, The second w represents its use. While it is 
kyond the scope of this book to prove it, the non-context-freedom of L 1  
direct1 y implies the non-context-freedom of programming languages like Algol 
and Pascal, which require declaration uf identifiers before their use, and 
which allow identifiers of arbitrary length. 
For this r e a m ,  a grammar for the syntax of Algol or Pascal does not 
specify the characters in an identifier. Instead, all identifiers are represented 
by a token such as id In the grammar. In a compiler for such a language, the 
semantic analysis phaw checks that identifier& haw been declared before their 
use, 
U 
Example 4.12. 
The language L = (Ll*'brnchP1 
( RZ 
I and m 2 I )  is not can- 
text k c .  That is, L2 consists of strings in the language generated by the reg- 
ular expression u*b*c*d* such thal the number of a's and c's are equal and 
the number of 15's and d's are equal. (Recall a" means u written n times.) L2 
abstracts the problem of checking that the number of formal parameters in the 
declaration of a procedure agrees with the number of actual parameters in a 
use of the procedure+ That is, rr" and b"' could represent the formal parame- 
ter tists in two procedures declared to have n and m arguments, respectively. 
Then r." and dm represent the actual parameter lists in calls to these two pro- 
cedures. 
Again note that the typical syntax of procedure definitions and uses does 
not concern itself with counting the number of parameters. For example, the 
CALL statement in a Fortran-like language might be dwcribed 

180 
SYNTAX ANALYSIS 
with suitable productions for expr. Checking that the nurnbr of actual 
parameters in the call is correct is usually done during the *mantic analysis 
phase. 
Example 4.13. 
The language L3 = {u"bwr"l nzO}, that 
is, strings in 
L ( n V * c * )  with equal numbers of u's, b's and c's, is not contexr free. An 
example of a problem that embeds L3 is the following. Typeset text uses ital- 
ics where ordinary typed text uses underlining. In converting a file of text 
destined to be printed on a line printer to text suitable for a phototypesetter, 
one has to replace underlined words by ilalics. An underlined word is a string 
of letters foilowed by an equal number of backspaces and an equal number of 
underscores. If we regard u as any h e r ,  6 as backspace, and c as under- 
score, the language L 3  represents underlined words. The conclusion is that 
we cannut use a grammar to dtscrik underhed words in this fashion. On 
the other hand, if we represent an underlined word as a sequence d lctter- 
backspace-underscore triples then we can represent underlined words with the 
regular expression (& )* . 
o 
I t  IS interesting to note rhal languages very similar to L a ,  L2, and L, ate 
context free. For example, L', = { w w R  I w is in (a 
1 bj*), where w" stands 
for w reversed, is context free. It i s  generated by the grammar 
The language L'* = {a"b"cmd" 1 ~2 I and m 2  I) is context free, with gram- 
mar 
Also, L;' - {~"b"c"d'~ 
I n? 1 and m 2 I) is context free, with grammar 
Finally, L') = { d b "  I n r  I )  is context free. with grammar 
It is worth noting that L:l, is the prototypical example of a language not defin- 
able by any regular expression, To see this, s u p p  LJ3 were the language 
defined by some regular expression. Equ ivalentiy, suppose we could construct 
a DFA D accepting L; 
D must have same finite number of states, say k. 
Consider the sequence of states s,), sl, s2, . . . , 
entered by D having read 
E, u. aa. . . . , a'. That is, xi is the stale entered by D having read i 0's. 

SEC. 4.4 
TOP-DOWN PARSING 
181 
path Lubclcd ui ' 
path labclcd b' 
. . . 
. . .  
Fig, 4.8, DFA L) acscpt ing rr'b' and rrihi 
Sincc D has only k different stares, at least two states in the sequence 
st,, s , ,  . . . , sk must be the same. say si and ,ti. From statc s, a sequence of 
i b's takes D 10 an acceprinp state J, since u'bi is in LI3+ Bul then thcrc is also 
e path from the initial statc s ~ ,  l o  xi to J' labelcd uJbi, as shown in Fig. 4.8, 
Thus, D also accepts rrih', which is not in Lt3, contradicting the assumption 
that LP3 is the language accepted hy D. 
Cdhquially, we say thi~t "a finite automaton cannot keep count," meaning 
that a finitc automaton canno1 accept a language like L'; which would require 
it to kccp count of the number ol 14's &fore it sees the b's. Similarly, we say 
"a graniniar can keep count of two items but nut three," since with a gram- 
mar we can define L'3 but not L j .  
4.4 TOP-DOWN PARSING 
In this scctim. we intrtduce the basic ideas behind top-down parsing and 
show how to c~nstruct an efficient non-backtracking form of top-down parser 
called ;t predictive parser. 
Wc define the class of LL(l} grammars from 
which predictive parsers can be cimstructed automatically. Besides formaliz- 
ing the disc.ussion of predictive parsers in Section 2.4, we consider nonrecur- 
sive predictivc parsers. This sectim concludes with a discussi~n of ermr 
rccovcry. Bottom-up pursers are Jiscusscd in kctions 4.5 - 4.7. 
'l'op-down parsing can be viewed a h  rrn uttcmpt to find a leftmost derivation 
Ihr an input string. Equiwlenlly, it can be viewed as an attempt to construct 
a parse tree for the input starring from the root and creating thc nodes of the 
parse tree in prmrder. In Section 2.4, we discussed the special case of 
recuwive-dcscent parsing, called predicrive p;wsing, where na backtracking is 
required, We now cnnsidcr a general form of lop-down parsing, called rccur- 
sive descent. that tnay involve backtracking. that is, making repeated scans of 
the input. Huwcvcr, backtracking parsers ;Ire nut seen I'reyuently. One rea- 
son is th'dt backtrackirhg is rawly needed tu prrse programming language con- 
structs. In situatims l ikc nalu ral language purhing, backtracking is still not 
very efficient, and tabular methods such as the dynamic programming nfgo- 
rithm of Excrcisc 4.63 or thc method of Earley [ 19701 are preferred. See Ahu 
and Ullrnan 11972bl for a description of general parsing methods. 

I82 
SYNTAX ANALYSIS 
$EC+ 4+4 
Backtracking is required in the next example, and we shall suggest a way of 
keeping track of the input when backtracking takes place. 
E m p l e  4.14. Consider the grammar 
- 
and the input string w = cab. To construct a parse tree for this string top- 
down. we initially create a tree consisting of a single node labeled $. An 
input pointer points to r. the first symbol of w. We then use the first prduc- 
ticln for S to expand the tree and obtain the tree of Fig, 4.9(a)+ 
Fig. 4.9. Steps in topdown parse. 
The leftmost leaf, labeled c, matches the first symM of w, so we now 
advance the input pointer to a. the second symbol of w, and consider the next 
leaf, labeled A. We can theq expand A using the first alternative for A to 
obtain the tree of Fig. 4.9(b). We now have a match for the second input 
symbol so we advance the input pointer to d. the third input symbol, and com- 
pare d against the next leaf, labeled b+ Since b does not match d ,  we report 
failure and go back to A to see whether there is another alternative for A rhal 
we have not tried but that might produrn a match, 
In going back to A ,  we must reset the input pointer to position 2, the psi- 
tion it had when we first came to A, which means that the pmedure for A 
(analogous to the procedure fw nonterminals in Fig. 2.17) must stare the 
input pointer in a local variable. We now try the second alternative for A to 
obtain the tree of Fig. 4.9(c). The leaf a matches the second symbol of w and 
the leaf d matches the third symbol. Since we have produced a parse tree for 
w ,  we halt and announce successful completion of parsing, 
o 
A left-recursive grammar can cause a recursive-descent parser, even one 
with backtracking, to go into an infinite Imp. That is, when we try to expand 
A, we may eventually find ourselves again trying to expand A without having 
consumed any input. 
I n  many cases, by carefully writing a grammar, eliminating left recursion from 
it, and lcft factoring the resulting grammar, we can obtain a grammar that can 

SEC, 4.4 
TOP-DOWN PARSING 
183 
k parsed by a recursive-dcscent parser that needs no backtracking, i.e., a 
predictive parser, as discussed in Section 2.4. Tu construct a predictive 
parser, we must know, given the current input symbol a and the nonterminal 
A 
to 
be 
expanded, 
which 
one 
0 
the alternatives of 
production 
A - orl (a2 
I - 
+ . 14, is the unique alternative that derives a string beginning 
with a. That is, the proper alternative must be detectable by looking at only 
the first symbol it derives. Flow-of-cmtrd constructs in most programming 
languages, with their distinguishing keywords. are usually detectable in this 
way. For example, if we have the productiolis 
stmr -. if a p r  then slmt else srmr 
I while expr do stsns 
1 begin firnr-list end 
then the keywords if. whik, and begin tel t us which alternative is the only one 
that could possibly succeed if we are to find a statement. 
Transition Diagrams for Predictive Parsers 
In %aim 2.4, we discussed the implementation of predictive parsers by recur- 
sive procedures, e.g,, those of Fig, 2.17, last as a transition diagram was 
seen in Section 3.4 to be a useful plan or flowchart'for a lexical analyzer, we 
can create a transition diagram as a plan for a predictive parser. 
Several differences between the transition diagrams for a lexical analyzer 
and a predictive p a r w  are immediately apparent. In the case of the parser, 
rhcre is one diagram for each nonterminal. The labels of edges are tokens 
and nontcrminals. A transition on a token (terminal) means we shu~tld take 
that transition if that token is the next input syrnbd. A transition on a n o w  
terminal ,4 is a call of the procedure for A. 
To construct the transition diagram of a predictive parser from e grammar, 
first eliminak left recursion from the grammar, and then left factor the gram- 
mar. Then for each nonterminal A do h e  following: 
I .  
Create an initial and Final (return) state. 
2. 
For each production A -. XtXl . . . X,, create a path from the initial to 
the final slate, with edges labeled X I  , X,, . . . ,x,. 
The predictive parser working off the transition diagrams behaves as id- 
lows. Lr begins in the start state for the start symbol. If after some actions it 
is in state s with an a g e  labeled by terminal a to state r, and if the next input 
symbol is a, then thc parser moves the input cursor one position right and 
goes 10 state t. If, on the other hand, the edge is labeled by a nonterminal A, 
the parser instead goes to the start state for A, without moving the input cur- 
sor. If it ever reaches the final state for A ,  it immediately goes to state I, in 
effect having "read" A from the input during the time it moved from state s 
to t .  Finally, if there is an edge from s to f labeled E, then from state s the 
parser immediately goes to state r, without advancing the input, 

184 
SYNTAX ANALVSlS 
SEC. 4.4 
A predictive parsing program based on a transition diagram attempts to 
match terminal symbols against the input. and makes a potentially recursive 
procedure call whenever i l  has to follow an edge labeled by a nonterminal. A 
nonrecursive implementation can be obtained by stacking the stater; s when 
there is a transition on a nonterminal out of s, and popping the stack when the 
final state for a nonterminal is reached+ We shall discuss the implementation 
of transition diagrams in more detail shortly. 
The above approach wwks if the given transition diagram does nol have 
nondeterminism, in the sense that there is more than one transition from a 
state on the same input. IC ambiguity occurs, we may be able lo rewlve it in 
ao ad-hoc way, as in the next example. 
If the nondeterminism cannot be 
eliminated, we cannut build a predictive parser, but we could build a 
recursive-descent parser using backtracking to systernatic;llly try all pussibili- 
ties, if that were the best persing strategy we could find. 
Example 4.15. 
Figure 4.10 contains a collection of transition diagram for 
grammar (4.11 ). The only ambiguities concern whether ur not 10 take an e- 
edge, If we interpret the edges out of the initial state Tor E' as saying take 
the transition on + whenever that is the next input and take the transition on 
r otherwise, and make the analogous assumption for T', then the ambiguity is 
removed, and we can write a predictive parsing program for grammar (4.1 I1.U 
Transition diagrams can be simplified by subsrituling diagrams in one 
another; thcse substitutions are similar to the transformations on grammars 
used in Section 2.5, For example, in Fig. 4.1 i(a), the call of E' on itself has 
h e n  replaced by a jump to the beginning of the diagram for E' . 
Fig. 4.10. Trunsilion diagrams i'or grammar (4.1 I). 

TOP-DOWN PARUNG 
185 
Fig. 4.1 1. Sirnplificb transition diiigrams, 
Figure 4.1 I @ )  
4
~
~
s
 
an equivalent transitbn diagram for I.:'. We may then 
substitute the diagram or Fig. 4.1 1(b) for the transit ion on E' in the diagram 
for E in Fig. 4.10, yielding the diagram of Fig, 4.lI(c). Lastly, we observe 
that the first and third nodes in Fig. 4.1 I(c) arc equivalent and we merge 
them. The result, Fig. 4,11(6), is repented as h e  first diagram in Fig. 4.12. 
The same techniques apply to the diagrams for T and T', The complete set of 
resulting diagrams is shown in Fig. 4.12. A C implementation of this predic- 
the parser runs 20-25% faster than a C implementation of Fig. 4. W. 
Fig. 4.12. Simplified transition diagrams for arithmetic cxprcxsi~lns. 

186 
SYNTAX ANALYSIS 
Nonmucsive Predictive Farsing 
It is possible to build a nonrecursive predictive parser by maintaining a stack 
explicitly, rather than implicitly via recursive calls. The key problem during 
predictive parsing is that of determining the production to be applied for a 
nonterminal. Thc nonrecursive parser in Fig. 4.13 looks up the production to 
be applied in a parsing table. ln what follows, we shall see how the table can 
be constructed dircctiy from certain grammars. 
- 
STACK 
x - 
Prcd ia ivc Parsing 
% OUTPUT 
- 
. Y  
Program 
- 
z - 
5 
Parsing Tablc 
M 
Hg, 4.13, Modcl of a nonrccursivc prcdictivc pparscr, 
A table-driven predictive parser has an input buffer, a stack, a parsing 
table, and an output stream. The input buffer contains the string to bc 
parsed, iollowcd by $, a symbol used as a right endmarker to indicatc the end 
of the input string. The stack contains a sequence of grammar synboIs with $ 
on the botiorn, indicating thc bottom of the stack. Initdly, the stack contains 
the start symbol of the grammar on top of 3. The parsing iable is a twa- 
dimensional array MIA, u I, where A is a nonterminal, and u is a terminal or 
the symbol $A 
The parser is controlled by a program that behaves as fok~ws, Thc pro- 
gram considers X, the symbol on tup of the stack, and a. thc current input 
symbol. These two symbds determine the action of the parscr. There are 
three possibilities, 
I .  If X = u = $, the parser halts and announces successful completion of 
parsing. 
2. 
If X = u + $. the parser pops X off the stack and advances the input 
winter to the next input symbol, 
3. 
If X is a nontcrrninal, the program consults entry MIX, rr I of the parsing 
table M. Thk entry will be either an X-production of the grammar ur an 
error entry. If, for example, M IX, u I = {X - UVW), the parser replaces 
X on top of the stack by WVU (with U on top). As output, we shall 

SEC. 4,4 
TOP-DOWN PARSING 
187 
assume that the parser just prints the production used; any other wde 
could k executed here. If MIX, u 1 = error, the parser calls an error 
recovery routine. 
The behavior of the parser can be described in terms of ils runturafions, 
which give the stack contents and the remaining input. 
Algorithm 4,3, Nonrecursive predictive parsing. 
Input. A string w and a parsing table M for grammar G. 
Output. If w is in L ( G ) ,  a leftmost derivation of w; otherwise, an error indi- 
cat ion. 
Merhob. Initially, the parser is in a configuration in which it has $8 on the 
stack with S, the start symbol of G on top, and w$ in the input buffer. The 
program that utilizes the prdiclive parsing table M to produce a parse for the 
input is shown in Fig. 4.14. 
xt ip to pint lo the first symhl of wS; 
m
t
 
let X be the lop aack symbol and fl the symbol pinted to by ip; 
if X is a terminal or $ then 
if X = a lhen 
pop X from the stack and advance ip 
el= rrrur(l 
else 
,-'* X is a nontcrminal a /  
ifM\X, ~j = X 
Y r Y 1  . . * Yk 
&h 
pop X from the stack; 
push FA, YL-, . 
+ 
+ . . l', onto thc stack, with Y ,  on top; 
output thc production X - Y, Y2 . . - Yk 
end 
else d!rror () 
until X = % 
/* stack is empty */ 
Fig. 4-14. Prcdiclivc parsing prugriim. 
Example 4.16, Consider the grammar (4.1 I) from Example 4.8. A predictive 
parsing table for this grammar i s  shown in Fig. 4.15. Blanks are crror entries; 
non-blanks indicate a production with which to expand the top nontermind on 
the stack. Note that we have not yet indicated how these entries could be 
selected, but we hall do so shortly. 
With input id + id 9: id the predictive parser makes the sequence of moves 
in Fig. 4.16. The input pointer points to the leftmost symbol of the string in 
the INPUT 
cdumn, If we observe the actions uf this parser carefully. we see 
that it is tracing out a Leftmost derivation for the Input, that is, the prduc- 
tions output are tho* of a leftmost derivation. The input symbols that have 

1158 SYNTAX ANALYSIS 
SEC. 4.4 
Fkg. 4.15. Parsing tablc M for grammar (4. 11). 
NDNTER - 
MINAL 
E 
Er 
T 
T 
F 
already been scanned, followed by the grammar syrnbh on the stack (from 
the top to bottom), make up the left-sentential forms in the derivation. 
D 
$E 
$E'T 
$EtT'F 
$E'r 
id 
$45' T' 
$E' 
W'T + 
SE'T 
SE'T'F 
W'T' id 
E'T' 
SE'T'F* 
%E'TfF 
$Ed T'id 
$ E r r  
SE' 
$ 
INPUT SYMBOL 
E -. TE' 
T - m  
F + M  
T' -. E 
E' - t TE' 
T - F r  
F - id 
T' -. +Fr 
P - M  
T'+r 
E' -C c 
' 
id 
E -TE ' 
T-FT' 
F-kl 
F@ 4.16. Movcs made by prcdictivc parwr on input M+M*M, 
FIRST and FOLLOW 
-k 
Et- + TE' 
Tr- 
The construction of a predictive parser is aided by two functions associated 
with a grammar C. Thcse functions, FIRST and FOLLOW, allow us to fill in 
the entries of a predictive parsing tabk for G, whenever possible. Sets of 
tokens yielded by the FOLLOW function can also be used as synchronizing 
tokens during pan ic-mde error recovery+ 
If or is any string of grammar symbols, let FIRSTIor) be the set of terminals 
* 
Tr+*Fr 
I 
E dTEr 
' 
T-.Fr' 
F - 0  
E'Y 
T -E 
5 
&#+E 
Tr-€ 

SEC. 4.4 
TOP-DOWN PARSING 
189 
that begin the strings derived horn a. If a & E .  
then r is illso in FIRST(a). 
Dcfine FOLLOW ( A  1, for nonterminai A, to IE the set uf terminals o that 
can appear immediatclg to the right of A in somc scntcntial form, that is, the 
set of terminals u such that there exists a derivation of the form $ &- UAOP 
for some a and p. Note that there may, at wmc time during the derivati~n, 
haw hen symbols between A and 0. but if so, they derivcd r and disap- 
pared. If A can hc the rightmost symhl in somc scntcntial h m ,  thcn S i s  in 
FOLLOWIA). 
To compute FIRSTIXI for all grammar symbols X, apply thc following rules 
until no more terminals or c clan k added to any FIRST set. 
1. 
If X is terminal, thcn FIRSTIX) is {X). 
2. 
1I'X 
r is a production, then add r to FIRSTIX). 
3. [I 
X is nnnterminal and X 
Y I Y2 . * * Yk is P production, then plam u iin 
FIK5TIX) if for some i, a is in FlRST(Y,), and r i s  in all of 
* 
FIRSTIY1), . . . . F[RSF(l',',_,); that is, Y 
- 
+ . Y,-) =S E. If 
is in 
FIRSTIY,) for all j = 1, 2, . . . , k, thcn add E to FIRSTIX). For exam- 
ple, cverything in FIRST( Y , )  i s  surely in FiRSIX). If Y, docs nut 
derive s, thcn wc add nothing more tu FIRSUX), but if YI % E. then we 
add FIRST(Y2) and so on. 
Now, we can computc FIRST Cur any string XIX2 . - 
Xa as follows. Add 
ta FIRSTIXJ2 
+ 
+ . X,) all the nun-r symbols of FIRST(XI). Also add the 
nonr symbds of FIRST(X*) if E is in FIRSTIX ,), thc nan-E symbols of 
FIRST(Xrl if r is in both FIRST(X , I  and FlRST(X2), and so on. Finally, add 
c to FlRST(XIX2 - 
+ 
X,) if, for all i, P1Rn(Xi) wntains c. 
To compute FOLLOWIA) for all nmterminals A, apply the following rules 
until mothing can be added 40 any FOLLOW set+ 
1. 
Plum $ in FOLLOWIS), whcrc S is the start symbol and $ is lhc input 
right endmarker. 
2. 
If there is a production A -. aS@, then everything in FIRST(P) except for 
E is placed in FOLLOWIB), 
3. 
If thcrc is a produdion A - a!?. or a production A + aB@ where 
FLRST(p) contains c (i.e+, P & E), tbco everything in FOLLOW(A) is in 
FOLLOW{B). 
Example 4.17. Considcr again grammar (4, l I), repeat& below: 
Then; 

190 
SYNTAX ANALYSIS 
FIRST(E} = FIRST(T) = FlRST(F) = {(, id). 
For exampk, M and lefl parenthesis are added to FIRSTIF) by rule (3) in 
the definition of FERST wdh i = I in each case, since FlRST(id) = {id) and 
FIRST{'(') = ( ( ) by rule ( I ) .  
Then by rule ( 3 )  with i = 1, the production 
T + FT' implies that id and left parenlhesis are in FIRST(T) as well. 
As 
another example, r is in FIRST(Et) by rule (2). 
To compute FOLLOW sets. we put $ in FOLLOW(E) by ruk ( 1 )  for FOL- 
LOW. By rule (2) applied to prduction F - [ E l ,  the right parenthesis is also 
in FOLLOWIEI. By rule (3) applied to production E -, TE' , TI and right 
parenthesis are 
in FOLLOW(E'). 
Since E' 
c, they are also 
in 
FOLLOW(7). For a last example of how the FOLLOW rules are applied, the 
production E + TE' implies, by rule (2), that everything other than E in 
FIRSTIE') must be placed in FOtLOW(7). We have already seen that $ is in 
FOLLOWC7'). 
Construction of P r d i t i v e  Parsing Tables 
The fokwing algo~ithrn a n  be used to construct a predictive parsing table for 
a grammar G. The idea behind the algorithm .is the following. Suppose 
A - a i s  a pduction with a in FIRST(a3. Then, the parser will expand A by 
a when the current input symbol is a. The only complication w u r s  when 
a = r or a %r. 
In this care, wc should againexpand A by u if the current 
input symbol is in FOLLOWIA), or if the $ on the input has ken reached and 
$ is in FOLLOW(A). 
Algorithm 4.4. Construcrion of a predictive parsing table. 
Input. Grammar G. 
Ouspr, Parsing tablc M. 
1 .  
For each production A -. a of the grammar, do steps 2 and 3. 
2. 
For each terminal u in FIRST(a), add A -. u to MIA, u 1. 
3. If t is in FlKST(a), add A + a  to MIA, b 1 for each termtnal b in 
FOLLOWIA), If c is in FIRSTlorj and $ is in FOLLOW(A), add A + a 
to M !A, $1, 
4. 
Make each undefined entry of M be e m +  

SEC. 4.4 
TOP-DOWN PAUSING 
19 1 
Example 4.18. 
Let us apply Algorithm 4.4 to grammar (4.11). Since 
FiRST(TEP) = FlRfl(T) = (I, id}, production E +TE' causes MIE, (1 and 
M [E, id] 40 acquire the entry E - TE'. 
Production E' - +TE' causes M IE', + ] lo acquire E' + + TE'. Prwluction 
E' -. t 
causes 
M [ E ' ,  )I 
and 
M [E', $1 
to 
acquire 
E' - c 
sincc 
FOLLOWE') = I), $1. 
The parsing table produced by Algorithm 4,4 for grammar (4.1 I) was 
shown in Fig. 4.15. 
IJ 
LL(I) Grammars 
Algorithm 4.4 can bc applied to any grammar G to prduce a parsing taMe M. 
For wme grammars, however, M may have somc entries that are rnulti~ly- 
defined. Fur example. if G is left recursive ur ambiguous, then M will have at 
least one multiply-detined entry. 
Example 4.19, Let us consider grammar 14.13) from Example 4,10 again; it 
Is repeated here for convenience. 
The parsing table for this grammar is shown in Fig. 4.17. 
Fig. 4.17. Parsing table M for grammar 44.13). 
The entry 
o r  MISr.e] contains 
both 
S ' - e S  
and S
J
,
 sincc 
FOLLOWIS') = { e ,  $1. 
The grammar is ambiguous and the ambiguity is 
manifcsteri by a choice in what production to use when an e (else) is seen. 
Wc can resolve the ambiguity if we choose St -. eS. This choice corresponds 
to associating else's with the closest previous then's. Note that the choice 
S' * r would prevent P from ever being put un the stack or removed from the 
input, and is therefore surely wrong. 
Q 
A grammar whose parsing table has no multiply-defined entries is said to be 
LL{ t). The first "L" in LL(1) stands for scanning the input from left to 
right, rhe second "L" for producing a leftmost derivation, and the "I " for 
using one input symbol of lookahead at each step to make parsing action 

192 
SYNTAX ANALYSlS 
SEC. 4.4 
deckions. It can be shown that Algorithm 4.4 produces for every LL(I) 
grammar G a parsing table that parses all and only the sentences of C+ 
LL(I) grammars have several distinctive properties. No ambiguous or Icft- 
recursive grammar can be LL( I). It can also be shown that a grammar G ih 
LLIL) if and only if whenever A -. a ( f! are two distinct productions uf G thc 
following conditions hold: 
I. 
For no terminal u do both cn and P derive strings beginning with u. 
2. 
At most one of a and p can derive the empty string. 
3. 
If B &= 
E, then a does not derive any string beginning with a terminal in 
FOLLOW (A). 
Clearly, grammar (4.1 1) for arithmetic expressions is LL4 I ). Griimmir 
14-13), modeling if-then-else statements, is nut. 
There remains the quesrion of what should be donc when a parsing table 
has multiply-defined entries. One recourse is to transform the grammar by 
eliminating all left recursion and then left factoring whenever possible. hoping 
to produce a grammar for which the parsing table has no multiply-defined 
entries. 
Unfortunately, there are some grammars for which no amount of 
alteration will yield an LLI I) grammar. Grammar (4.13) is one such exam- 
ple; its language has no LL(1) grammar at all. As we saw, we can srill parse 
(4.13) with a predictive parser by arbitrarily making MIS', e j  = (S' +d). 
In 
general, there are no universal rules by which multiply-defined entries can be 
made single-valued without affecting the language recognized by the parser. 
The main difficulty in using predictive parsing is in writing a grammar for 
the source language such that a predictive parscr can be constructed €rum the 
grammar. Although left-recursion elirninrrtion and left factoring are easy to 
do, they make the resulting grammar hard to read and difficult to use For 
translation purposes. To alleviate some of this difficulty, a common organiza- 
tion for a parser in a compiler is to use a predictive parser for control con- 
structs and to use operator precedence (discussed in 5kctim-i 4.6) for expres- 
sions. However, if an LR parser generator, as discussed in Section 4.9, is 
available, one can get all the benefhs of predictive parsing and operator pre- 
cedence automatically. 
Emr Retovery in M i c t i v e  Parsing 
The slack of a nnnrecursive predictive parser makes explicif the terminals and 
nnnterminals that the parser hopes to match with the remainder of the input. 
We shall therefore refer to symbols on the parser stack in the full~wing dis- 
cussion. An error is detected during predictive parsing when the terminal on 
top of the stack does not match the next input symbol or when nonlerrninal A 
is an top of the stack. a is the next input hymbd. and the parsing table entry 
M\A,aI is empty. 
Panic-mode error recovery i s  based on the idea of skipping symbls on the 
the input until a token in a selected set of synchronizing tokens appears. I t s  

SEC. 4.4 
TOP-DOWN PARSING 
193 
effectiveness depends on the choice of synchronizing set. The sets should be 
chosen so that the parser recovers quickly from errors that arc likely to occur 
in practice. %me heuristics are as follows: 
1 .  
As a starting p i n t ,  we can place all symbols in FOLLOWCA) into the 
synchronizing set for n~nterminal A. If we skip tokens until an element 
of FOLLO W(A) is seen and pop A from the stack. it i s  likely that parsing 
can continue. 
2. 
It is not enough to u e  FOLLOW(A) as the synchrortizing set for A. For 
example, if semicobns terminate, statements, as in C, then keywords that 
begin statements may not appear in the FOLLOW set of the nonterminal 
generating expressions. A missing serniwlon after an assignment may 
therefore result in the keyword beginning the next statement being 
skipped. Often, there is a hierarchical structure on constructs in a 
language; e,g., expressions appear within statements, which appear within 
blocks, and sc, on. We can add to the synchronizing set of a lower con- 
struct the symbols that begin higher conslructs. For example, we might 
add keywords that begin statements to the synchronizing sets for the non- 
terminals generating expressions. 
3, 
If wc add symbols in F[RST(A) to the synchronizing set for nonterminal 
A ,  then it may be pssibk to resume parsing according to A if a symbol in 
FIRST(A) appears in the input. 
4. 
If a nonterminal can generate the empty string, then the production deriv- 
ing r can be used as a default. Doing so may postpone some error detec- 
tion, but cannot cause an error to be mis.sd This approach reduces the 
num bcr of n~ntcrminals that have to be considered during error recovery. 
5 .  
If a terminal on top of the stack cannot be matched, a simple idea i s  to 
pop the terminal, issue a message saying that the terminal was inwrted, 
and continue parsing In effect, this approach takes the synchronizing set 
of a token to consist of all other tokens. 
Example 4.20. 
Using FOLLOW and FlRST symbds as synchronizing tokens 
works reasonably well when e~pressims are parsed according to grammar 
(4.1 I). The parsing table for this grammar in Fig. 4.15 is repeated in Fig. 
4+ 
13, with "synch" indicating synchronizing tokens obtained from the FOL- 
LOW set of the nonterminal in question. Thc FOLLOW sets for (he nonkr- 
rninal are obtained from Example 4.17. 
The table in Fig. 4.18 is to be used as follows. 4f the parser looks up entry 
MIA, a1 and finds that it is blank, then the input symbol a i s  skipped. Jf the 
entry is synch, then the nontcrminal on top of the stack is popped in an 
attempt to rcsume parsing. If a token on top of the stack does not match the 
input symbol. then we pop the token from the stack, as mentioned a b v e .  
On the erroneous inpu~ )id a + id the parser and error recovery mechanism 
of Fig. 4.18 behave as in Fig. 4.19. 

194 
SYNTAX ANALYSIS 
Fig. 4.18. Synchronizing tokcns added to parsing tablc of Fig. 4.15. 
MIHAL. 
E 
E' 
T 
T' 
F 
$E 
$E 
1SE' 7 
$Ef T'F 
$E'T id 
$EFT' 
S E T  F * 
$E'T'F 
SE'T' 
$E' 
$E'T t 
$E' T 
$E' T'F 
$E'7"id 
$E'T1 
$E 
$ 
)id*+id$ 
id r + id $ 
id* + id$ 
id* + M %  
id* + id$ 
;t; + id$ 
lk-f id$ 
+ id$ 
-t id$ 
+ id% 
+ ids 
id $ 
id S 
id $ 
$ 
$ 
$ 
crmr, skip ) 
id is in FIKST(E) 
id 
EdTE' 
T-FT' 
F -id 
crror, MII.'. + 1 = hynch 
F has bccn prippcd 
* 
TI 
-*FTI 
i 
synch 
f 
&'-+ 7.E' 
synch 
r -I 
sy nch 
Fig. 4.!9. 
Parsing and crror rcwvcry mows madc by prcdictivc parw. 
The a h v e  discussion of panic-mode recovery does not address the impor- 
tant issue of error messages, In general, informative error messages have to 
be supplied by the compiler designer. 
[ 
E -TE" 
'l-+'i" 
F 4 E )  
Phrus~-/cvd re<-uvcry. Phrase-level recovery is implemented by filling in the 
blank entries in the predictive parsing table with pointers to error routines. 
These routines may change, insert, or delete symbols on the input and issue 
appropriate error messages. They may also pop from the stack. It is ques- 
tionable whether we sh;hould permit alteration of stack symhh or the pushing 
of new symbols onto h e  stack, since then the steps carried nut by the parser 
might not corrcspnd to the derivation of any word in the language at ail. In 
synch 
E** 
sy nch 
r~ 
I 
synch 
$ - 
synch 
E'-€ 
hynch 
TIT 
synch 

any event, we must be sure that there is no possibility of an infinite Imp, 
Checking that any recovery action eventually results in an input symbol being 
consumed (or the stack being shortened if the end of the input has k
n
 
reachdl is a g
d
 
way to protect against such b p s .  
4,s WITOM-UP PARSING 
En this section, we intrduce a general style of btom-up syntax analysis, 
known as shift-reduce parsing. An easy-to-implement form of shi it-reduce 
parsing, called operator-precedence parsing, is presented in Section 4.6. A 
much mare general methud of shift-reduce parsing, called LR parsing, is dis- 
cussed in Section 4.7. LR parsing is used in a number of automatic parser 
genera tors. 
Shift-reduce parsing attempts to construct a parse tree for an input string 
beginning at the leaves (the bottom) and working up towards the r o d  (the 
top). We can think of this process as one of "reduciag" a string w to the start 
symbl of a grammar. At each reductio~ step a particular substring matching 
the right side of a production is replaced by the symtd on the left of that prw- 
ductim, and if the substring is chosen correctly at each step, a righimost 
derivation is traced out in reverse. 
EKPmple 4 A  Consider the grammar 
The sentence &kde can be redud to S by thc fdkwing steps: 
We scan a&& 
looking for a substring that matches the right side of m
e
 
production. The snbstrings b and d qualify, Let uschcmse the leftmost b and 
replace it by A, the left side of the production A - lq we thus obtain the string 
aAkde. Now the substrings A h ,  b, and d mat& the right side of some pro- 
d ~ t i o f i .  Although b is the leftmost substring that matches the right side of 
some prductkn, we choose to replace the substring A& by A, the left side of 
the prdudion A -. Ah-. We now obtain ddc. Then replacing d by 3, the 
left dde of the produdbn B - d, we obtain M e .  We can now replace this 
entire string by S. Thus, by a sequence of fwr reductions we are able to 
reduce abhdc to S. These reductions, in fact, trace wt the following right- 
most derivation in reverse: 

9 6  SYNTAX ANALYSIS 
Handles 
informally, a "handle" of a string is a substring that matches the right side of 
a productjoo, and whose reduction to the nonterminal on the left side of the 
production represents one step along the reverse of a rightmost dcrivatiun. In 
many cascs the lcftmnst substring p that matches the right side OC some pro- 
duction A + p is nor a handle, because a reduction by rhc production A - @ 
yields a string that cannot be reduced t c ~  the start symhl, In Example 4.21, if 
we rcplaccd b by A in the second string uA&& 
we wuuld vbtain the string 
trAArdt that cannot be subsequently reduced to S. For this reuson. we must 
give a more precise definition of a handle. 
Formally. a kcrndlc of a right-sentential form y is a production A - P and a 
pusition of y whcre the string p may be found and rcplaced by A [CI produce 
the previous right-sentential form in a rightmost derivation of y. 'That is, if 
S 9 d
w
 2 apw. then A - in !he position following a is a handle of 
a a w .  The smng w to the right of the handle contains only terminal symbols, 
Nute wc say "a handle" rather than "the handle" because the grammar could 
be ambiguous, with more than one rightmust derivation of a p w .  I f  a gram- 
mar is unambiguous, then every right-sententia\ form of the grammar has 
exactly one handle. 
In the cxample above, rrbbt.& 
is a right-xntential form whose handle i s  
A - b at position 2. Likewise, uAb& 
is a rtght-santenlial form whosc handle 
is A -. Abc at position 2. Sometimes we say "the substring 
is a handle of 
apw" if the position of p and the production 4 - @ we have in mind are 
clear. 
Figure 4.20 portrays the handle A -. P in the parse tree of a right-sentenrial 
form a p w .  The handle represents rhe leftmost. complete subtrce consisting of 
a node and all its children. ln Fig, 4.20, A is the bottommost leftmost interior 
node with all its children in the tree. Reducing @ to A in apw can be thought 
of as "pruning the handle." that is, removing the children of A from the parse 
tree. 
Example 4.22. Consider the following grammar 
and the rightmost derivation 

BOTTOM-UP PAR SING 
197 
Fig. 4,20. 'Fhc hmdk A -. P in rhc parw trcc for a@v. 
We have subscripted thc: id's for notational convenience and underlined a hsn- 
dle of each right-sentential form. For example, id, is a handle of  he right- 
scntencial form idl + idz * idJ because id is the right side uf thc production 
E + id. and replacing MI by E produces thc previous right-sentential form 
E 4 id2 * id?. Notc that the string appearing to the right or a handle con- 
rains only terminal symbols. 
Because grammar (4.16) i s  ambiguous. there is another righlmt~t derivation 
of I ~ C  
same string: 
Consider the right senrcntial form E + F * id3. In this deriuiion. E + E is 
a handle of E + E * id3 whcrcas id3 by itbelf is a handlc of this samc right- 
scntential Corm accurding to thc derivation above. 
The two rightmost ilerivatirms in this example are analogs uf thc two lefi- 
mmt derivations in Example 4.6. The first derivation gives * a higher pre- 
ccdcncc than + ,  whereas the second givcs + rhc higher precedence. 
n 
Handle Pruning 
A rightmost derivation in rcvcrsc can be obtained by "handk pruning," That 
is, we start with r string (sf terminals w that we wish tu parse. If w is a scn- 
tencc of the grammar at hand. then w=y,,, where y, Is the nth right-sentential 
form ol' surne iks yet unknown rightmost derivation 

198 
SYNTAX ANALYSIS 
SEC. 4.5 
To reconstruct this derivation in reverse order, we ltxa~e the handle P, in y, 
and replace 9, by the left side of some production A, 
-+ 9, ta obtain thr: 
{u - !)st right-sentential form y, -. , . Note that we do not yet know how han- 
dles are to be found, but we shall s;ee methods of doing so shortly. 
W e  then repeat this process. That is, we locate the handle p, - #  in y,,-, 
and reduce this handle to obtain the right-sentential form y,-2. 
If by continu- 
ing this process we produce a right-sentencia1 form consisting only of the start 
symbol $, then we halt and announce successful completion of parsing. The 
reverse of the sequence of productions used in the reductions i s  a rightmust 
derivation for the input string, 
Example 4.23. Consider the grammar (4.16) of Example 4.22 and the input 
string id, + id2 * idlA The sequence of reductions shown in Fig. 4.21 
reduces idi + idz * id3 to the start symbol E, The reader shouId observe that 
the sequence of right-sententid forms in this example is just the reverse of the 
sequence in the first rightmosr derivation in Exampk 4.22. 
o 
MI + idI * id3 
E + id, ;P id, 
id, 
id2 
id 3 
E + E  
E + E 
E --. id 
€ - i d  
E - M  
E - . E * E  
E + E + E  
Fig. 4.21. Reduct~ons made by shift-reduce parser 
Stack Implementation d Shift-Redm Parsing 
There are two problems that must be solved if we are to parse by handle prun- 
ing. The first is to locacc the substring to be reduced in a right-sentential 
form, and the sewnd is to determine what production to hmse in case there 
is mwe than one production with that substring on the right side. Before we 
get to these question:, let us first consider the type of data structures to use in 
a shift-reduce parser, 
A convenient way to implement a shift-reduce parser is to use a stack to 
hold grammar symbols and an input buffer to hold the string w to be parsed. 
We use $ to mark the bottom of the stack and also the right end of the input. 
Initially, the stack is emply, and the string w is on the input, as follows: 
The parser operates by shifting zero or more input symbols onto the stack 
until a handle 
is on top of the stack. The parser then reduces P to the left 

SEC. 4.5 
BI)'I'TOM-UP PARSING 
199 
side of the appropriate production. The parser repests this cycle until it has 
detected an error or until the stack contains the start symbol and thc input 1s 
empty : 
After entering this configuration, the parser halts and announces successful 
complc~ ion of parsing . 
Ewmpk 4-24. Lei us step through the actions a shift-reduce parser tnighi 
make in parsing [he input wing id, + id2 * id3 according to grammar I4.16). 
using the first derivation of Example 4.22. The sequence is shown in Fig. 
4.22. Note that because grammar (4.16) has two rightmost dcrivatinns for 
this input there is another sequence uf steps a shift-reduce parser might take. D 
id, + idl * id)$ 
t id, * idJ$ 
+ Id2 * M4S 
id2 * id)$ 
shift 
rcducc by E + id 
sh tCt 
shift 
rcducc by E - id 
shift 
shif~ 
rcduw by E - id 
rcducc By E 
E * E 
rcduw by E -. E + E 
I ucwp1 - 
Fig. 4.22. Configurarions of shift-rcducc p i w r  r m  input id, + id2*idj. 
While the primary operations of the parser arc shift and reduce. there are 
actually four possible actions u shift-reduce parscr can make: I I )  shill. (2) 
reduce. (3) accept, and (4) error. 
I .  In a .rhjff action, the next input symbol is shiftcd onto the top of the 
stack. 
2. 
[n a ruduce action, the parser knows the right end of the handk is at the 
top of the stack. It must then locatc  he left end elf the handle within the 
stack and decide with what nwnterminal to replace the handle. 
3. 
In an accept action. the parser announces successful con~pktitm r,l pars- 
ing+ 
4. 
In an error action, the parser discovers h a t  rr syntax errur has occurred 
and calls an error recovery routine. 

200 
SYNTAX ANAtYSl$ 
SEC. 4.5 
There is an important fact that justifies the use of a stack in shift-reduce 
parsing: the handle will a!ways eventually appear on top of the stack, never 
inside. This fact becomes obvious when we consider the pseible forms of two 
successive steps in any rightmost derivation. These two steps can be uf the 
form 
in case (I), A is replaced by PBy, and then the rightmost nonterminal B in 
that right side is replaced by y. In case (21, A i s  again replaced first, but this 
time the right side is a stringy of terminals only. The next rightmost nmter- 
minal B will be somewhere to the left of y. 
Let us consider case (I) in reverse, where a shift-reduce parser has just 
reached the configura~icin 
The parser now reduces the handle y to B to reach the configuration 
Since B is the rightmost nonterminal in apByr, the right end of the handle of 
afWyz cannot occur inside the stack. The parser can therefore shift the string 
y onto the stack to reach the configuration 
in which BBy is the handle, and it gets reduced to A. 
In case (21, in configuration 
the handle y is on top of the stack. After reducing the handle y to B, the 
parser can shift the string xy to get the next handle y on top of the stack: 
Now the parser reduces y to A. 
In boeh cases, after making a reduction the parser had to shift zero or more 
symbols to get the next handle onto the stack. 
It never had to go into the 
stack to find the handle, It is this aspect of handle pruning that makes a stack 
a particularly convenient dala structure for implementing a shift-reduce 
parser. We still must explain how choices of aclion are to be made so the 
shift-reduce parser works correctly. Operator precedence and LR parsers are 
two such techniques that we shall discuss shortly. 

ROTTOM-UP PAR SING 201 
Via b1e Prefixes 
The set of prefixes of right senlential f o r m  that can appear on the stack nf a 
shift-reduce parser are called viuh!e prifiirt~s. An equ tvalent definition af a 
viable prefix is thar it is a prefix of a right-senteotial form that does not con- 
tinue past the right end of the rightmost handle of' that sententid form. By 
this definition. it is always possible to add terminal symbols tp the tnd of a 
viable prefix to obtain a right-scntcntial form. Therefore, therc i s  apparently 
no er'mr as long as the portion of ~hr: input seen tu II given p i n t  can be 
reduced to a viable prefix. 
Conflicts During Shift-Reduce Parsing 
Thcre are context-free grammars for which shift-reduce parsing cannot bc 
used. Every shift-reduce parscr for such a grammar can reach a c~nfiguratim 
in which the parser, knowing rhe entire stack contents and the next input sym- 
bol, cannot decide whether to shift or to reduce (a , s h ~ f / r l d u w  
rwnflicr). or 
cannot decide which of several rcduaions to make (a rrdwdrrdttc*~ wnflict). 
We now give some examples of syntactic constructs that give rise to such 
grammars. Technicully. these grammars are not in the LRIk) cluss of gram- 
mars defined in Sectiun 4.7; we r&r 
to them as.non-LR grammars. The L in 
LR(k) refers to the number of symbols of Inokahead on the input. Grammars 
used in compiling usually fall in the LR( I) class, with one symbol lookahad. 
Example 4.25. An ambiguous grammar can never be L R .  For example. con- 
sider the dangling-else grammar (4.7) of Sectinn 4.3: 
If we haw a shift-reduce parser In configuratiun 
we cannot tell whether if cxpr then ~mr 
i s  the handle, no matter what appears 
btlow it on the stack. Here there is a shit'tJreduce conflict. Depcnbing on 
what 
follows Ihc else on thc 
input, 
ir 
might bc 
corrcct 
to reduce 
if txpr then sfmr to smr. or it might he currcct to shift else and then to Imk 
for anuthcr stmt to mrnplete the alternative if expr Ihen stmt dse sfmi. Thus, 
we canna tell whether to shift or reducc in lhis case. so  he grammar is not 
LHI I ) .  More generally, no ambiguous grammar, as this one certainly is. can 
be LR(R) For any k .  
We shuuld mention, however. that shift-rcducs parsing can be easily 
adapted to parst certain amhipunus grammars, such as thc if-then-else gram- 
mar above. Whcn we construci such a parser for a grammar containing the 
two productions above, there will be a shil'~lreduce contlict: on else, either 
shift, or reduce by srmr + if pxpr then stmt. If we rcsolue rhe conflict in f a w  

of shifting. the parser will behave naturally. We discuss parsers for such 
~lrnbigutlus gva~ntnim in .Stxbtirm 4.8. 
n 
Anrbthcr cunialon c:~~tso 
{ b f  nun-LR-mess ocurh when we know we have a 
handle, hut the stack ctmtcnts and the next input symbol are not sufficient to 
dctcrminc which production should be uscd in a reduction. 'The next example 
iihstrats> 1h1s si~utltion. 
Example 4.26. Suppnse we have a lexical analyzer that returns loken id for 
ail iilrntificrs. rcgardtcss i>t' usage. Suppose also that our language invokes 
prr~ccburcs hy giving thcir nameh, with parameters surrounded by parentheses, 
and that arrays arc referenced by thc same syntax, Since the translation of 
indices in ikrr;iy rcfcrcnccs and parameters in procedure calls are different. we 
W U I  
10 usc dilfurcnt productions to generate Iists of actual parameters and 
indices. Our grammar might thcrcforc have (among others) productions such 
as: 
h scalcnicnt beginning wirh A ( X ,  J 1 would appcar as the token .;iream 
idtid, id) ro the parscr. After shifting the first three tokens onto thc stack. a 
sh~l'[-reducc p m c r  wwld be in configuration 
I{ is wirlc.nt t ha1 I ht: id nn top of the stack must be reduced. but by which pm- 
ducriun'! 'l'hc correct chi~icc is prtlduction ( 5 )  if A is a procedure and prrduc- 
ttm r 7) if A is an array, Thc stack docs not tell which: infortnation in the 
sytnbrtl tahlr: obiainetl from thc declaration of A has to be uscd. 
Onc wlution i s  to chunge the token id in production (I) to pmid and to 
use a mow wphistirdtcd lc~ical analyzer that rcturns token prwid when i t  
rccognizcs an idcntificr which is the nsmr of a procedure. Doing this would 
rcquire thc lcxicsll analyxer to consult [he symbol table before returning a 
token. 
I f  we nudc this modification, then on processing AII., J )  the parser would 
hc either in the configuratirm 

SEC. 4.6 
OPERATOR -PRECEDENCE PARSING 
203 
or in [he configuration abovc. In rhe former case, we chwse rcduc~ion by 
production ( 5 ) ;  in the latter case by production (7). Notice how the symbol 
third f r t m  the top of the stack determines the reduction to be made, even 
thctq$~ it is not invdved in the reduction, Shift-reduce parsing can utilize 
information [ar down in the stack 
guide the parse. 
LI 
4.6 OPERATOR-PRECEDENCE PARSING 
The largest class of grammars for which shift-reduce parsers can be built suc- 
cessfully - the LR grammars - will h discussed in Section 4.7, However, for 
a sma!l but irnprtant class of grammars we can easily construct efficient 
shift-reduce parsers by hand. These grammars have ~ h c  
property (among 
oiher essential requirements) that no p~aduciion right side is r ur has two 
adjacent nonterminals. A grammar with the latter property i s  called an 
operutor grrrmmur . 
Example 4.27. The following grammar for expressions 
is nnt an operator grammar, because ~ h c  
right side EAE has two (in fact three) 
consecutive nonterminals, However, if we substitute for A each of its alterna- 
tives. we obtain the following operator grammar; 
We now describe an easy-to-implement p a r h g  technique ciilled opcrator- 
precedence parsing. H istorirally , the technique was first described as a mani- 
pulation on tokens without any reference to an underlying grammar. In fact, 
once wc finish building an operator-precedence parser from a grammar, we 
may effectively ignore the grammar. using the nonterminals on the stack only 
as placeholder\ for artributcs associated with the nontcrminsls. 
As a general parsing technique, operator-precedence parsing has a number 
af disadvantages. For example, it is hard to handle tokens like the rninus 
sign, which has two different pre~~dences 
(depending on whether it is unary 
or binary). W o r e ,  since the relationship between a grammar for the language 
being parsed and. the operator-precedence parar it sc If i s  tenuous. one cannot 
always be sure the parser accepts exactly the desired language. Finally, only u 
small class of grammars can be parsed using operator-precedence techniques. 
Nevertheless, because of its simplicity, numerous compilers using operator- 
precedence parsing techniques for expressions h a w  been built successfully. 
Often these parsers use recursivc descent, described in Section 4.4. for state- 
ments and higher-level cun struas, Opratur-precedence parsers have t ven 
been built for entirc languages. 
In operator-precedence parsing, we define ih ree disjoint prrrxdm-rj rdff- 
lions, <., , and .), between certain pairs of terminals. These precedence 
relations guide the selection uf handles and have the following meanings: 

rr 'yields prcccdcncc to" b 
rr "has thc surnc prcccdcncc as" b 
rr .> b 
tr "takcs prcccdcncc ovct" 
We shuuld caution the reader that while these relations may appear similar to 
the arithmetic relations "less than," *'equal to." and "greater than." the pre- 
cedence relarions have quite different properties. For example, we could have 
u <. b and cr *> b for the same language, or we might have none of u <. b, 
a 
h, and u 
+ >  6 holding for some terrnintlis u and b. 
There are two common ways of determining what precedence relations 
should hold between a pair of terminals. The first method we discuss is intui- 
tive and is based on the traditional notions of associativity and precedence of 
operatmi. For examplc, if * is to have higher precedence than t , we make 
+ <. * and * -> + , This approach will be %en to resolvc the mbiguitics of 
grammar (4.!7). 
and it enables us to write an operator-precedence parser for 
it (although the unary minus sign causes prublems), 
The second method of selecting operator-precedence relations is first to con- 
struct an unambiguous grammar for the language, a grammar that reflects the 
correct associativity and precedence in its parw trees. This job is not difficult 
fm expressions; the syntax of expressions in Section 2.2 provides the para- 
digm. For the other cmnmon sourcc of ambiguity, the dangling else, grammar 
(4.9) is a useful model, Having obtained an unambiguous grammar, there is a 
mechanical methob for constructing operator-precedence relations from it. 
These relations may not be disjoint, and they may parse a language other than 
that generated by the grammar, but wilh the standard sorts of arithmetic 
expressions, few problems are encountered in practice. We shall not discuss 
this mnstruction here; see Aho and Ullrnan I IY72b). 
Using Operator-Precedewe Relations 
The intention of the prewdcnce relations is to delimit the handle of a right- 
scntcntial form, with <. marking the left end, = appearing in the interior of 
the handle, and .> marking thz right end, To bc more precisc, suppose we 
have a right-sentential farm of an operator grammar. The fact that no adja- 
cent nonterminals a p p r  on the right sides of productions implies that no 
right-scntential form will have two adjacent nonterminals either, Thus, we 
may writc the right-scntential form as Pt,alpI 
. ur,Pf1, whcrc each p, is 
either E (the empty string) or a single nonterminal, and each u, is rl single ter- 
minal. 
Suppose that between ai and o;,, exactly one of the relations <., =, and 
*> holds. Further. Ict us use $ to mark each end of the string, and define 
$ <. b and b -> $ for all tcrmlnals b. Now suppme we remove the nuntzrmi- 
nafs from rhe string and place the correct relation <+, 
, or .>, bctwcen each 

EC. 4A 
OPER ATOR-PRECEDENCE PARSING 205 
pair of terminals and bctween the endmost terminais and the $'s marking the 
ends of the string+ For example, suppose we initially have the right-sen tential 
form id + id * Id and the precedence relations are those given in Fig. 4.23, 
These relations are ! m e  of those that we would choose to parse according to 
grammar (4.17). 
Fig. 4.23, 
Opcta~or-prcccdcncc rclaticsns. 
Then the string with the prccedence relations inserted is: 
$ <. id .> + <- id .> * < . i d . >  $ 
(4.18) 
For example. <. is inserted between the leftmost $ and id since <- is the 
entry in row $ and column id. The handle can be found by the fdlowing pro- 
cess. 
I .  %an the string from the left end until the first 
+ >  
is encountered. In 
(4.18) above, this occurs between the first id and +. 
2. 
Then scan backwards (to the left) over any A's until a C 
+ is encountered. 
In (4.181, we scan backwards to $. 
3. 
The handlecontains everythingco the left ofthe first +.> 
and tothe right 
of the <. encountered in step (21, including any intervening or surround- 
ing nantcrminals. (The inclusion of surrounding nonterminals is neces- 
sary so that two adjacent nonterminals do not appear in a right-sentential 
form.) In (4. IS), the handle is the first id. 
lf we are dealing with grammar (4.17). we then reduce id to E+ At this 
point we have the right-sentenrial Form E +id*id. After reducing the two 
remaining id's 10 E by the same steps, we obtain the right-seientential form 
E +E*E. Consider now the string $-I-*$ obtained by deleting the nontcrmi- 
nals. Inserting [he precedence relations, we get 
indicating that the left end of the handle lies between + and * and the right 
end between * and $+ Thcse precedence relations indicate that, in the right- 
scntential furm E +E*E, the handle is E*E. Note how the E's surrounding 
the * become part of the handle. 
Since the nonterminals do not influence the parse, we need nut worry about 
disringuishing among them. A single marker "nanterminal" can be kept on 

2~ 
SYNTAX ANALYSIS 
EC. 4.6 
the stack of a shift-reduce parser to indicate placeholders for attribute values. 
11 may appear from the discussion above that the entire right-sentential form 
must be scanned at each step to find the handle. Such is not the case if we 
use a stack to store the input symbols already seen and if the precedence rela- 
tions are used to guide the adions of a shift-reduce parser. If the pecedenm 
relation <. or = holds between the topmost terminal symbol on the stack and 
the next input symbol, the parser shifts; it has not yet found the right end of 
the handle, If the relatior .> holds, a reduction is called for, At this point 
the parser has found the right end of the handle, and the precedence relations 
can be used to find the left end of the handle in the stack, 
Ef no precedence relation holds between a pair of terminals (indicated by a 
blank entry in Fig. 4.23), then a syntactic error has k e n  detected and an 
error recovery routine must be invoked, as discussed later in this section. The 
above ideas can be formalized by the fobwing algorithm. 
Algorithm 4.5. Operator-precedence parsing algorithm. 
Input. An input string w and a table of precedence relations. 
Ourpttf. If w is well formed, a skrlrrd parse tree, with a placeholder nonter- 
minal E labeling all interior nodes; otherwise, an error indication. 
Method. Initially, ihe stack contains $ and the input buffer the string w$. To 
parse, wc execute the program of Fig. 4.24. 
o 
( I )  sct ip to pint to the first symbol of w$; 
( 2 )  romt f m v t r  
(31 
if$ is on top of thc stack and ip points to $ them 
(4) 
return 
e k  bgh 
( 5 )  
let a bc the topmost tcrminal symbl on thc slack 
and k t  b Ix thc symbol pointod to by ip; 
(bl 
I u < . b o r a  
bthenkgin 
(7) 
push ;h onto the slack; 
(8) 
advance jp to Ihc next input symbol; 
d; 
19) 
elwifu .> b then 
/ r r  reduce *, 
1101 
rePat 
i l l 1  
pop the stack 
! 12) 
until tbc top stack terminal is rclatd by <- 
to the terminal most reccntlg popped 
11 3) 
else errur I) 
cad 
Fig. 4.24. Opatvr-prcccdcncc parsing algorithm. 

SEC. 4.6 
OPERATOR-PRECEDENCE PARSING 207 
We are always free to create operator-precedence relations any way we see fit 
and hope that the operatorqmxdence parsing algorithm will work correctly 
when guided by them+ For a language of arithmctic expressions such as that 
generated by grammar (4.17) we can use the following heuristic to product a 
proper set of precedence relations. Note that grammar (4. t 7) is ambiguous, 
and right-sententla] forms could have many handles. Our rules are designed 
to select the "proper" handles to reflect a given set of associativity and pre- 
cedence rules for binary operators. 
If operator el has higher precedence than operator 9, 
make 0, +> 
and 
0 
. 
For example, if * has higher precedence than +, make 
* +> + and + <- *. These relatims ensure that, in an expccssion of the 
form E +E*E +E, the central E*E is the handle that will be reduced 
first. 
If O1 and €12 are operators of equal precedence (they may in fact be the 
same operator), then make 
.> B2 and e2 .> O I  if the operators are 
left-associative, or make tJ1 <. ti2 and e2 <. OI if they are right- 
associative. For example, if + and - are left-associative, then make 
+ -> +, + -> -, - .> -, and - .> +. lf 1 i s  right associative, then 
make t <, t. These relations ensure that E - E  + E will h a w  handle 
E - E  selected and E t E t E will have the last E t E selected. 
Make 8 <-id, id.> 8, t 3 < - ( ,  
<, 0, ) . >  0, 0 . >  1, 0 . >  $, and 
$ <. B fur all operators 0. Also, lel 
( = )  
S <. ( 
$ < +  id 
w-( 
id .> $ 
) *> $ 
( <. id 
id .> ) 
) .> 1 
Thcsc rules ensure that b t h  id and (15) 
will be rcduced to E. A h +  $ 
serves as bath the left and right endmarker, causing handles to be found 
between $'s 
wherever possible. 
Exampk 4.28. 
Figure 4.25 containx the opcrslor-precedence relations for 
grammar (4.171, assuming 
I. t is of highest precedence and right-associative, 
2. * and/lrre uf next highest precedence and left-associative, and 
3+ + and - are of lowest precedence and left-associative, 
(Blanks denote error entries,) The reader should try out the table lo see that 
it works correctly. ignoring problems with unary minus For the moment. Try 
the table on the input id * (id t id) -id1 id, for example. 

208 
$Y NTAX ANALYSIS 
Fig. 4.25, Operator-prcwdcncc rclalions . 
Handling Unary Operators 
If we have a unary operator such as - (logical negation), which is not also a 
binary operator, we can incorporate it into the above scheme for creating 
operator-precedence relations. S u p p i n g  - to be a unary prefix operator, we 
make 0 <. -. for any operator 0, whether unary or binary. We make - -> 0 
if - has higher precedence than 0 and - <* 0 if not. For example, if - has 
higher precedence than &, and & is left-associative, we would group 
E&-E&E 
as (E&(-E))&E, by these rules. The rule for unary postfix apera- 
tors is analogous. 
The situation changes when we have an operator like the minus sign - thal 
is both unary prefix and binary infix. Even if we give unary and binary 
minus the same precedence. the table o l  Fig, 4 .Z$ will fail to parse strings like 
Id*-id correctly. The best approach in this caw is to use the lexical analyzer 
to distinguish between unary and binary minus, by having It return a different 
token when it sees unary minus. Unfortunately, the lexical analyzer cannot 
use lookahead to distinguish the two; it must remember the previous token. 
In Fortran, for example, a minus sign is unary if the previous token was an 
owrator, a left parenthesis, a comma, or an assignment symbol. 
Compilers using operator-precedence parsers need not store the table of pre- 
cedence relations. In most cam, the table can be encodcd by two prcwdm.e 
functions $and #.that map terminal symbols to integers. We atlempt to select 
f and g so that, for symbols: a and 6, 

SEC. 4.6 
OPER ATOR-PRECEDENCE PARSING 
209 
numerical comparison between I (a) and g{b). Note, however, that error 
entries in the precedence matrix are obscured. since one of ( I ) ,  (21, or (3) 
holds no matter what f(u) and g ( b )  are. The loss of error detection capabil- 
ity is generally not considered serious enough to prevent the using of pre- 
cedence functions where possible; errors can still be caught when a reduction 
is called for and no handle can be found. 
Not every table of precedence relations has precedence functions to encode 
it, but in practical cases the functions usuaHy exist. 
Example 4.29. The precedence table of Fig. 4.25 has the following pair of 
precedence functions, 
For example, * <* id, and f 
(*) < #(id). Note that f (id) r  id) suggests 
that id .> id; but, in fact, no precedencc i-elation holds between id and id. 
Other error entries in Fig. 4.25 are similarly replaced by one or another pre- 
cedence relation. 
A simple method for finding precedence functions for a table, if such func- 
tions exist, is the following. 
Algorithm 4,6. 
Constructing precedence functions. 
Input, An operator precedence matrix. 
0 
Precedence functions representing the input matrix, ur an indication 
that none exist. 
I .  
Create symbols JI, and gd, for each a that is a terminal or %;. 
2. 
Partitiun the created symbols into as many groups as possible, in such a 
way that if u = 6, then f, and gh are in the =me group. Note that we 
may have to put symbols in the tiame group even if they are not related 
by G. For example, if u A 6 and r* A b, then S, and 1. must be in the 
same group, since they are both in the same group as XI,. 
If, in addition. 
r = J, then f;, and xd, are in the same group ewn though o + d may not 
hold. 
3. 
Create a directed graph whose nodes are the groups found in (2). For 
any a and b. if a <. 6, place an edgc from the goup af gh to the group 
of J,. If u + >  
b, place an edge from the group of Iu to that of %. Note 
that an edge or path from J, tu 
means that f ( u )  must exceed ~ ( b ) ;  
a 
path from gh to fu means that g ( b )  must exceed f'(u). 
4. 
If the graph cons~ructcd in 43) has a cycle. then no precedence functions 
exist. If there are no cycles, let .f (u) be the length of the longest path 

210 
SYNTAX ANALYSIS 
SEC. 4.6 
beginning at the group of A,; let g(a) be the length of the iongest path 
from the group of g,. 
0 
Example 4.30. Consider the matrix of Fig. 4.23. There are no = relation- 
ships, so each symbol is in a group by itself. Figure 4.26 shows the graph 
constructed using Algorithm 4+6. 
Fig. 4.26. Graph reprcsenting prmdcoce functions. 
Therc art no cycles, so precedence functions exist. As ff and gs have no 
out-edges, I($) 
= gI%) = 0. The hngest path from g+ has length 1 ,  HI 
# ( + I  = I .  There is a path from gm to f* to g, toJ+ t o g +  to fS, so 
# (id) = 5. The resulting precedence functions are: 
There are two pinls in the par,& 
process at which an operator-precedence 
parser can discover syntactic errors: 
I .  
If no precedence rdation holds between the terminal on top of the stack 
and the current input.' 
2. 
If a handle has h e n  found, but there is no production with this handle as 
a right side. 
Recall that the operator-precedence parsing algorithm (Algorithm 4.5) apptars 
to reduce handles composed of terminals only. However, while nonterminals 

SEC, 4.6 
OPERATOR-PRECEDENCE PARSING 
2 I 1 
are treated anonymously, they still have places heid for them on the parsing 
stack. Thus when we talk in (2) above about a handle matching a 
production's right side, we mean that the terminals are the same and the psi- 
tions mupied by mnterminals are the same. 
We should obwrve that, besides C I) and (2) above, there are no other 
points at which ertors auld be detected. When scanning down the stack to 
find the left end of the handle in steps ( 10-321 of Fig. 4.24, khe operator- 
precedence parsing algorithm, we are sure to find a <. relation, since $ marks 
the bottom of etack and is related by <* to any symbol that muld appear 
immediately a b v e  it on thc stack. Note also that we never allow adjacent 
symbols on the stack in Fig. 4.24 unless they are related by <. or =. Thus 
steps (10-12) must succeed in making a reduction. 
Just because we find a sequence of symbols a < 
+ b , = b2 = . . 
A Bk oo 
the   tack, however, does not mean that b 1 b 2  - . . bk is the string of terminal 
symbls on the rtght side of some production. We did not check for this con- 
dition in Fig. 4.24, but we clearly can do so, and in fact we must do so if we 
wish to associate semantic rules with reductions. Thus we have an opportm- 
ity to detect errors in Fig. 4.24, modified at steps ( 10- 12) to determine what 
prduction is the handle in a reduction. 
Hundii* 
Errors During R~dudrms 
We may divide the error detection and recovery routine into sevcral pieces. 
One pkce handles errors of type (2). For example, this routine might pop 
fiyrnbls off the stack just as In steps 10- t2) of Fig. 4.24, However, as there 
is no production to reduce by, no semantic actions are taken; a diagnostic mes- 
sage is printed instead, To determine what the diagnostic should say, the row 
tine handling case (2) must decide what production the right side being 
popped "looks like." For example, suppose ubc is popped, and there is no 
production ~ight side consisting of a, b and c together with zero or more non- 
terminals. Then we might consider if deletion of one of a. b, and c yields a 
legal right side (nonterminals omitted), For example, if there were a right 
side aEcE, we might issue the diagnostic 
illegal b on Line (line containing b) 
We might also consider changing or inserting a terminal. Thus if abEslc. were 
a right side, we might issue a diagnostic 
missing d on line (line containing c) 
We may also find that there is a right side with the proper sequence of ter- 
minals, but the wrong pattern of nonterminak For example, if ubc is pop@ 
off the stack with no intervening w surrounding nonterminals, and u& is not 
a right side but a E k  is, we might issue a diagnostic 
missing E on line (line containing b) 

212 
SYNTAX ANALYSIS 
SEC, 4h 
Here E stands for an appropr hte syntactic category represented by non terrni: 
nal E. Fur example, if 4. b, w c is an operator. we might say "expression:" if 
a is a keyword like if, we might say "conditionai." 
In general, the difficulty of determining appropriate diagrmt ics when no 
legal right side is found depends u p n  whether thcrc are a finite or infinite 
number of pssiMc strings that could be popped in lines 1 10- 12) of Fig. 4.24. 
Any such string b1b2 - 
+ 
+ bL must have = relations hdding between adjacent 
symbols, so b 1 = b 
+ 
- = b I .  If an operator precedence table tells us 
that there are only a finite number of sequences of terrninais relatcd by =, 
then wc can handle thefie strings on a case-by-case basis. For each such string 
x we can determine in advance a minimum-distance legal right side y and issue 
a diagnostic implying that x was found when y was intended. 
11 is easy to determine all strings that could be popped from rhe stack in 
steps ( 10-121 of Fig. 4.24, These are evident in the directed graph whose 
nodes represent the terminals, with an edge from u to b if and only if u = bA 
Then the possible strings are the labels of the nodes along paths in this graph- 
Paths wnsisting of a single node arc possible. Howwcr, in order for u path 
B I B l  
- . bk to be 'pppable" on wrne Input, thcrc must be a symbol u (pas- 
sibly %) such that ucmb,. Call such a bl inirid. A h ,  there must be a sym- 
bol c {possibly $1 such that bk->r. Call bi find: Only then could a reduction 
be called for and b l A z  + 
4 
+ bk he the sequence of symbols popped. If the 
graph has a path from an initial tu a final node containing a cycle, then thcrc 
arc an infinity of strings that might be popped; otherw'rsc, therc arc only a fin- 
ite number. 
Fig. 4.27. Graph fw prcccdcncc matrix of Fig. 4+25, 
Example 4,31, Let us reconsider grammar (4.17): 
E -+ E+E 1 E - E  I E*E ( E / E  I E f E 1 
I -E I id 
The precedence matrix for this grammar was shown in Fig. 4.25, and its 
graph is given in Fig. 4.27. There i s  only one edge, because Ihe only pair 
related by = is the left and right parenthesis. All but the right parenthesis 
are initial, and ali but the left parenthesis arc final. Thus the only paths from 
an Initial to a rind node are the paths +, -, *, I !  id, and t of length onc, and 
the path from ( to 1 of length two. There are but a finite numbcr, and each 
corresponds to the terrninirls of some productiun's right side in the grammar. 
Thus the error checker for reductions need only check that the p r o p  set of 

%c. 4.6 
OPERATOR*PRECEDENCE PARSING 21 3 
nontcrminal markers appears among the terminal strings being reduced. 
Specifically, the checker does the iollrswing; 
I .  
I f  -t , -, *, i ,  or 1 is reduced, it checks that nonterminals appear on both 
sides. If not, it issues the diagnostic 
missing operand 
2 .  
If id is reduced, i t  checks that therc is no nonterminal to the right or left. 
If there is, it can warn 
missing operator 
3, 
if ( ') 
is rebuctd. it checks that there is a nonterminal bctween the 
parentheses. if not, it can say 
no expression between parentheses 
Also i t  must check that no nonterminnl appears on either side of the 
parentheses. If one docs, it issues the same diagnm~ic as in 121, 
0 
if therc are an infinity of strings that may be popped, error messages cannot 
hc tabulated on a case-by-case basis. We might use a general routine co deter- 
mine whether some produdion right side is close (say distance 1 or 2. where 
distance is measured in terms' of tokens, rather than characters, inserted, 
dclcted, or changed) to [he popped string and if so. issuc a specific diugnostic 
on the assumption that that production was intended. 11' no production is c l i w  
to thc p p p c d  string, we can issue a general diagnostic to rhe effect that 
"something is wrong in the current line. 
7 ,  
Wc must now discuss the other way in which the o p e r a t r d p e  parser 
detects errors. When consulting the prcccdence matrix to decide whether to 
shift or reducts (lines (6) and (95 of Fig. 4.241, we may find thal nu relation 
holds between the top stack symbol and the first input symbol, Fur example, 
suppose u and b are [hc two top stack symbds ( b  is at thc lop), i4 
and d arc 
the next two inpu~ symbols, and there is no prcccdence relation between b 
and r .  'ru recover. wc must modify the stack, input or both. We may change 
symbols. tnserl symbols onto the input or stack, or delctc symbols from thc 
input ur slack. If wc inscrt or change, we must be careful that wc do not get 
into an infinite loop, where. for cxample. we perpetually insert symbols at the 
beginning of the input withut bcing able to reduce or to shift any uf the 
inserted symbds. 
One approach thal will assure us no infinite loops k to guarantee that after 
recovery !he current input symbol can bc shifted (if the current input is $, 
guarantee that no symtml is placcd on the input, and the stack is eventually 
shortened). For examplc. given ob on thc slack and rd on rhe input. if c r l  T' 

2 14 
SYNTAX ANALYSIS 
ZC. 4.6 
we might pop b from the stack. Another choice is to delete c from the input 
if b I 
.d. A third choice is to find a symbol e such that b i  .e ST and insert e 
in front of c on the input, More generally, we might insert a string of sym- 
bols such that 
~ I - ~ ~ s - c ~ I  
- 5 . e  
11 <.C - 
if a single symbol fur insertion could not be found. The exact action cho,se;en 
should reflea the compiler designer's intuition regarding what error i s  likely 
in each case. 
For each blank entry in the precedence matrix we must specify an error- 
recovery routine; the same routine could be used in several places. Then 
when the parser consuhs the entry for a and b in step (6) of Fig, 4.24, and no 
precedence relation holds between a and b. it finds a pointer to the error- 
recovery routine for this error. 
Emmple 432. Considcr thc precedence matrix of Fig. 4.25 again. In Fig, 
4.28, we show the rows and columns of this matrix that have one or more 
blank entries, and we have filled in these blanks with the names of error han- 
dling routines. 
r~~ 
4.28. Operator-precedence matrix with error entries. 
The substance of thew error handling routines is as Follows: 
el: 
.e2: 
e3: 
e4: 
/+ called when whole expression i s  missing */ 
insert id onto the input 
issue diagnostic: "missing operand" 
/* called when expression begins with a right parenthesis */ 
delete ) from the input 
issue diagnostic: "unbalanced right parenthesis" 
/* called when M w ) is followed by Id or ( +/ 
insert + onto the input 
issue diagnostic: "missing operator" 
/+ called when expression ends with a left parenthesis +/ 
pop ( from the stack 
issue diagnostic; "missing right parenthesis" 
Let us consider how this error-handling mechanism would treat the 

$EC. 4.7 
LR PARSERS 
215 
erroneous input M + 1. The first actions taken by the parser arc to shift id, 
reduce it to E (we again usc E for anonymous nonterminais on the stack), and 
then to shift the + . We now have configuration 
STACK 
INPUT 
$E + 
)$ 
Since + .> ) a reduction is called for, and the handle is +. The error 
checker for reductions is required to inspect for E ' s  to kft and right. Finding 
one missing, it issues the diagnostic 
missing operand 
and dws the reduction anyway. 
Our mfiguratim i s  now 
$E 
1 $ 
There is no precedence relation between $ and ), and the entry in Fig. 4.28 
for this pair of symbls is e2. Routine e2 causes diagnostic 
unbalanced right parenthesis 
to be printed and removes the right parenthesis from the input. We are now 
left with the final configuration for the parser. 
$E 
5 
0 
4 7  LR PARSERS 
This section presents an efficient, bottom-up syntax analysis technique that 
can be used to parse a large class of context-free grammars, The technique is 
callcd LRtk) parsing; the "L" is for left-to-right scanning of the input, the 
"R" for constructing a rightmost derivahn in reverse, and the k for the 
number of input symbds of lookahead that arc used in making parsing deci- 
sions. When (k) is omitted, k i s  assumed to be 1. LR 'parsing is attractive for 
a variety of reasons. 
LR parsers can be constructed to recognize virtually all programming- 
language construm for which context-free grammars can be wrilten. 
The LR parsing method is the most gcneral nonbxktracking shi ft-reduce 
parsing methud known, yct it can be implemented as efficiently as other 
shift-reduce methds. 
The class of grammars that can be parsed using LR methods is a proper 
superset of the class of grammars that can be p.~rsed with predidive 
parsers. 
J 
An LR parser can detect a syntactic error as soon as it is p.sib\c tu ciu rn 
on a kft-to-right scan of the input. 

The principal drawback of the merhod is that it is too much work to con-. 
struct an LR parser by Rand for a typical programming-language grammar. 
One needs a specialized tool - an LR parser generator. Fortunately. many 
such generators are available. and we shall discuss the design and use of one. 
Yacc, in Section 4.9. Wiih such a generator, one can write a context-free 
grammar and have the generator automatically produce a parser for that 
grammar. If the grammar contains ambiguities or other constructs that are 
difficult to parse in a left-to-right scan of the input, then the parser generator 
can locate thcw constructs and inform the compiler designer of their presence. 
Aftcr discussing thc operation of an LR parser, we present three techniques 
for constructing an LR parsing table for a grammar. The first method, called 
simple LR (SLR for short), is the easiest tu implement, but the least powerful 
of the three. It may fail to produce a parsing table for certain grammars on 
which the other methds succeed. The second method, called canonical LR, 
is the most powerful, and the most expensive. The third method, called Id- 
ahead LR ILALR for short ), is intermediate in power and cost between the 
uthcr two. The LALR method will work on most programming-language 
grammars and, with some.eff~rt, can be implemented efficiently+ Some tech- 
niques for compressing the size of an LR parsing table arc considered later in 
this scction. 
The LR Paning A l ~ r i t h m  
The whematic form of an LR p a r w  is shown in Fig+ 4.29. It consists of an 
input, an output, a stack, a driver program, and a parsing tabk that has two 
paris (artion and goto). The driver program is thc same for all LR pnrsers; 
only the parsing table changes from one parser to another. The parsing pro- 
gram reads characters from an input buffer one at a time. The program uses 
a sqack to store a string of the form .r,,X ,a ,Xzs2 . + 
+ X,S,~, where s ,  is on 
top. Each Xi is a grammar symbol and each si is a symbol called a stu~c., 
Each state symbol summarizcs the information contained in the stack below it, 
and the mmbina~ion of the state symbt an top of the stack and the current 
input symbol art used to index the parsing table and determine the shift- 
reduce parsing decision. In an implernentrrthn, the grammar symbols need 
not appear on the stack; however. we shall always include them in our discus- 
sions to help explain the khavior of an LR parser. 
Thc parsing table consists of two perk, a parsing uction function ~ ~ ~ f i r v r  
and 
a goto function p t n .  The program driving the LR parser behaves as fdlows, 
It determines s,, 
the stale currently on top uf the stack, and u;, the current 
input symbol. It then wnsults ucrionls,, ail, the parsing action table enlry For 
state s,,, and input dl,, which can have one of four values: 
I .  shift s, where s i s  a state, 
2. 
reduce by a grammar prduction A - p. 
3 ,  
accept + and 
4. 
error. 

LR PARSERS 
217 
Fig, 4.29, Model of an LR parser. 
The function goto takes a state and grammar symbol as arguments and pro- 
duces a state. We shall see that the f i r m  funktion of a parsing table con- 
struc~cb from a grammar G using the SLR, canonical LR. or LALR method is 
the transition function of a deterministic finite automaton that recognim the 
viable prcfixes of G .  Recall that rhe viable prefixes of G are those prefixes of 
right-sentential fnrms that can appear on the stack of a shifr-reduce parser, 
because they do not extend past the rightmost handle. The initia! state of this 
DFA is the state initially pat on top of the LR parw, stack. 
A configurariun of an LR parser is a pair whose first componcnr is the stack 
contents and whose second component is the unexpended input: 
I 
This configuration represents the right-sententid form 
. 
LR 
Parsing Program 
STACK 
in essentially the same way as a shift-reduce parser wuuld; only the presence 
of states On the stack i s  new. 
The next move of the parser is determined by reading a,, the current input 
symbd, and s,,, 
the state on top of the stack, and then consulting the parsing 
action table entry ourion Is,,, , ail, The configura!ions resulting after each of 
the four types of move are as follows; 
OUTPUT 
- 
s , 4  
Xr" 
. 
If a&fiunls,, ail = shift s, the parser executes a shift move. entering the 
configuration 
Here the parser has shifted both the current input symbol ai and the next 
slate s, which is given in acrton[s,, 
u,], onto the slack; rr,+ I kcrsmes the 
current input symbol. 

218 
SYNTAX ANALYSIS 
SEC. 4.7 
2. 
If urticln(s,, uil = reduct: A + f3, then the parser executes a reduce 
mow, entering the ~xmfiguration 
where s = gr~rssI.r.,~-,, A 1 and r is the length of 13, the right side of the 
prduction, Here the parser first popped 2r symbols off the stack (r slate 
symbols and r grammar symbols), exposing state s,,,-,. The parser then 
pushed buth A, the !eft side of the production, and s, the entry for 
gnro(s,-,, A 1, onto the stack. The current input symbol is not changed 
in 
a 
reduce move. 
For 
the LR parsers we shall construct, 
X, -, , 
, . . . X,, 
the sequencc of grammar symbols popped off the stack, 
will always match 8, the right sidc of the reducing production. 
The output or an LR parKcr is generated aftcr a reduce move by errecut- 
ing the semantic action associated with the reducing production. For the 
time king, we shall assume the output consists of jwii printing the reduc- 
ing product ion. 
3. 
If ~ i u i t I s , ~ .  
ai I = i l c ~ q t ,  
parsing is completed. 
4. 
If uchnls,, u,] = error, the parser has discovered an error and calls an 
error recovery routine. 
The LR parsing algorithm is summarized below. All LR parsers behave in 
this faahion; the only difference between onc LR parser and another is the 
information in the parsing action and goto fields of the parsing table. 
Algorithm 4.7, LR parsing algorithm. 
hpur. An input string w and an LR pursing tablc with functions uction and 
gutd~ for a grammar G. 
Ourput, If w is in LIG), a bottom-up parse for w; otherwise, an error indica- 
t ion. 
M ~ t h d .  Initially, the parser has so on its r;tack, where so is the initial state. 
and w$ in the input buffer. The parser then exccutes the program in Fig. 
4.30 until an accept or crror action is enwuntered, 
n 
Example 4,33. Figurc 4.31 shows the parsing action and goto functions of an 
LR parsing table for the following grammar for arithmetic expressions with 
binary operators + and *: 
The codes for the actions are: 

LR PARSERS 2 19 
set ip to p i n t  10 thc fi~st symbol of wS; 
rcpeat fopwer begJa 
Ict s be the state on top of the slack and 
a the symbol painted to by ip; 
P action [s, a 1 = shift s' UHn be& 
push a then s' on lop of the stack: 
advatrcc ip to the next input symbt 
d 
tJse If a d o n  Is. a 1 = rcduw A - fl t
h
 
bcgZn 
pop 2* ) @ j  q r n b d s  off thc stack; 
kr s' bc the state now on tw of thc stack; 
push A then gcrco Is', A 1 on top of the staclc; 
output the production A -- p 
end 
cLoe if a d m  is, n = acccpt then 
WtMl'm 
* 
error() 
end 
Fig. 4.30, LR parsing program. 
Fig. 431. Parsing fable fur cxprcuaim grammar. 
1. 
si means shift and stack state i, 
2. 
r j  means reduce by production numbered j, 
3. accmeansaaept, 
4. 
blank means error. 

Note that the value of ~otols, aj for terminal a is found in the action field 
connected with the shift action on input a for state s. The pto field gives 
gotols, A! for nonwrminals A. A h ,  bear in mind that we have not yet 
explained how the entries for Fig. 4.31 were selected; we shall deal with this 
issue shortly, 
On input id * id + id, the sequence of stack and input contents is shown in 
Fig. 4+32. For example, at line ( I) the LR parser is in state 0 with id the first 
hput symbol. The action in row 0 and column id of the action field of Fig. 
4.31 is s5, meaning shift and cover h e  stack with state 5 ,  That is what has 
happened at h e  (21: the first token M and the state symbol 5 h a w  both been 
pushed onto the stack, and id has been removed from the input + 
Then, * becomes the current input symbl. and the action of state 5 on 
input * is 10 reduce by F - id. Two symbols are popped off the stack (one 
state symhl and one grammar symbol). State O is then exposed. Since the 
goto of state 0 on F is 3, F and 3 are pushed onto the stack. We now have 
the configuraticm in line (3). Each of the remaining moves i s  determined 
similarly. 
ACTiW 
hi ft 
~cducc by F - kl 
rcduce by T - F 
hift 
shift 
reducc by F -c id 
rcducc by T + T*F 
rcducc by E 4 T 
shift 
shif~ 
rcducc by P - id 
reducc by T -+ F 
E -E+T 
acccpt 
Fig+ 4.32. Movcs of LR purscr on ZB * id + id. 
How do we construct an LR parsing taMe for a given grammar'! A grammar 
Fur which we can construct a parsing table is said to be an LR grmrnar, 
There are context-free grarnnlars that are nut LR, but these can generally be 
avoided fur typical programming-language constructs, Intuitively, in urder fur 
a grammar tu be LR it is sufficient that a left-twigfit shift-reduce parser be 
able to recognize handles when they appar on top of the stack, 

XC. 4.7 
LR PARSERS 221 
An LR parser does not have to scan the entire stack to know when the han- 
dle appears on top+ Rather, the state symbol on top of the stack contains all 
the Information it needs. It is a remarkable fact that if it is possible to recog- 
nize a handle knowing only the grammar symbds on the stack, then there is a 
finite automaton that can, by reading the grammar symbols on the stack from 
lop to bottom, determine what handle, if any, is on top of rhe stack. The 
goto function of an LR parsing table is essentially such a finite automatun. 
The automaton need nut, however, read the stack on every move. The state 
symbol stored on top of the stack is the state the handle-recognizing finite 
automaton would be in if it had read the grammar symbols of the stack from 
bottom to top, Thus, the LR parser can determine from  he state on top of 
the stack everything that i t  needs to know about what is in the stack, 
Another source of information that an LR parser can use to help make its 
shift-reduce decisions is the next k input symbols. The cases k =O or k = 1 are 
of practical interest, and we shall only consider LR parsers with k~ l here, 
Fdexampk, the action table in Fig. 4.31 uses one symbol of lmkahead. A 
grammar that can be parsed by an LR parser examining up to k input symbls 
on each move is called an M(k) Krummur. 
There is a significant difference between LL and LR grammars. For s 
grammar to be LR(k), we must be able to recognize the occurrence of the 
right side of a production, having seen all of what 1s derived from rhat right 
side with k inpui symbols of lmkaheab. This requirement is far less stringent 
than that b r  LLIk) grammars where we musl be able to recognize the use of 
a production seeing only the first k symbols of what its right side derives. 
Thus, LR grammars can describe more languages than t l  grammars. 
Constructing SLR Parsing Tables 
We now show how to construct from a grammar an LR parsing table. We 
shall give three methds. varying in their power and ease of implementation, 
The first, called "simple LR" or SLR for short, is the weakest of the three in 
terms of the number of grammars for which it succeeds, but is the easiest lo 
implement. We shall refer to the parsing table constructed by this method as 
an SLR table. and to an LR parser using an SLR parsing table as an SLR 
parser, A grammar for which an SLR parser can be constructed is said to be 
an SLR grammar. The other two methods augment the, SLR melhd with 
lookahead information, w the SLR method is a good starting point for study- 
ing LR parsing. 
- An L R ( ~ )  
item ( i t m  fur sho~t) of a grammar G is a production of G with a 
dot at wine position of the right side. Thus, production A -. XYZ yields the 
four items 

222 
SYNTAX ANALYSIS 
SEC. 4.7 
The production A + r generates only one item, A + .. An item can be 
represented by a pair of integers, the first giving the number of the production 
and the second the position of the dot. Intuitively, an item indicates how 
much of a production we have seen at a given point in the parsing process. 
For example, the first item above indicates that we hope to see a string deriv- 
able from XYZ next on the input. The smmd item indkat~s that we bave just 
seen on the input a string derivable from X and that we hope next to see a 
string derivable from E. 
The central idea in the SLR methd is first to construd from the grammar a 
deterministic finite ~utornaton to recognize viabk prefixes. We group items 
together into sets, which give rise to the states of the SLR parser. The items 
can be viewed as the states of an NFA recognizing viable prefixes, and the 
"grouping together" is really the subset construction discussed in Section 3.6. 
One collection of sets of LR@) items, which we call the mnonical LR(0) 
dlection, providts the basis for constructing SLR parsers. To construct the 
txnonical LR(O) dlection for a grammar, we define an augmented grammar 
and two functions, closure and gum. 
If G is s grammar with start symbol S, then G', the augmented grammar for 
G ,  is G with a new start symbol S' and production S' - S. Thc purpose of 
this new starting produdion is to indicate to the parser when it should stop 
parsing and announce acceptance of the input. That is, acceptance occurs 
when and only when the parser is about to reduce by $' 
$. 
If i is r set of items for a grammar G, then cbsw&(l) is the stt of items con- 
structed from 1 by the two rules: 
I 
+ 
Initially, every item in 1 is added to closure (1). 
2. 
If A - a-BB is in cLsur&) and B -. y is a production, then add the item 
B -. py to I, if it is not already there. We apply this ruk until no more 
new items can be added to closrrre ( l )  . 
Intuitively, A -. or.BP in ciswe{!) indicates that, at some point in the parsing 
prams. we think we might next see a substring derivable from B p  as input. 
If B -. y is a production, we also expect we might see a substring derivable 
from y at this point. For this reason we also include B -. -y in closure(I). 
Example 4.34, Consider the augmented expression grammar; 
If l is the set of one item {IE' - -El}, then cIsrcre(1) contains the items 

L R  PARSERS 223 
Here, E' -. WE is put in c~Iusurp(J) by rub (1). 
Since there is an E immedi- 
ately to the right of a dot, by rule 12) we add the E-productions with dots at 
the left end, that is, E -, .E + T and E - -T. Now there is a T immediately 
to the right of a dot, so we add T -. +TaF and T - *F. Next, the F to the 
right of a dot forces F -. . ( E )  and F -, .M to be added. No other items are 
put into chure (1) by rule (2). 
D 
The function ch~sure can be computed as in Fig. 4.33. A convenient way to 
implement the function closure is to keep a b b a n  array &ed, 
indexed by 
the nmtermiinalu of G, such that crrIde&3] is set to true if and when we add 
the items B - -y for each 8-production 3 -. y. 
b t b n  r l ~ ~ . ~ u r e  
[ 1 1; 
w 
J := 1; 
m w t  
for each itcm A - a.Bp in J end cwh production 
B -. y of C such that B -. .y is n d  in J dm 
add B -c +y to J 
d 
no mwc items can bc addcd to J; 
retwrn J 
end 
Fig. 4.33. Computation o f  cimm. 
Note that if one B-production is added to the closure of I with the dot at the 
left end, then all B-prductions will be similarly added to the closure. In fact, 
it is not necessary in some circumstances actually to list the items B -. -7 
added to 1 by clossrre, A list of the nonttminals B whose ;eprduct 
ions were so 
added will suffice. In fact, it turns out that we can divide all the sets of items 
we are interested in inlo two classes of items. 
1. 
Kernel iirms, which include the initial item, S' -. -S, and all items whose 
dots are nor at the left end. 
2. 
hronkerwl items, which have their dots at the left end. 
Moreover, each set of items we are interested in is formed by taking the GI+ 
sure of a set of kernel items; the items added in the closure can never be 

kernel items, of wurse, n u s ,  we can represent the sets of items we are 
really interested in with very little storage if we throw away all nonkernel 
items, knowing that they could k regenerated by the chure process, 
The Gao Operation 
The s e m d  useful function is gomll, X) where l is a set of items and X is a 
grammar symbol. gotdi, X) Is defined to be the closure of the set of all items 
[A -. aX$] such that [A + a.X@] is In I. Intuitively, if 1 is the wt of items 
that are valid for some viable prefix y, then gdo(1, X I  i s  the set of items that 
are valid fm the viable prefix yX. 
Exam* 
4.36. 
if f is the set of two items {[Ei+E-I, [E*E.+T]), then 
gofo(l, +) consists of 
We computed gom(l, +) by examining 1 for items with + immediately to the 
right of the dot. F -.E- i s  not such an item, but E +E++T is. We m v s d  
the dot over the + to get {E - E+ +T) 
and then took the c h r e  of this set. a 
The Sets-@-Items Cmsf~tion 
We are now ready to give the algorithm to construct C, the canonical mllec- 
tim of sets of LR(O) hems for an augmented grammar Gr: the algorithm is 
shown in Pig. 4.34. 
@urn 
iimIGt); 
w 
C := {closwe([[S' -. .S]})}; 
reped 
fw each set of items 1 in C and each @ammar symbol X 
such that gmo (I, X) is not empty and not in C d~ 
add goiou.. X) to c 
. d l  
a0 more sets of items can be added to C 
orl 
Example 4,X, The canonical cdlection of sets of LRCO) items for grammar 
(4.19) of Elrample 434 is shown in Fig. 4.35. The gau funaim for this set 
of items is shown as the transition diagram of a &termhistic finite automaton 
D in Fig. 4.36. 
n 

LR PARSERS 225 
Fig. 4.35. Canonid LRIO) wllsctim for grammar (4+19) 
If each state of D in Fig. 4.36 is a final staa and I .  is the initial state, then 
D recognizes exactly the viable prefixes of grammar (4.19). This is no 
accident. For every grammar G, the goto function of the canonical collection 
of sets of items defines a deterministic finite automaton that recognizes the 
viable prefixes of G. !n fact, me can visuaiize a nondeterministic finite auto- 
maton N whose states are the items themselves. There is a transition from 
A -. a . X p  to A -. d . 9  labdd X, and there is a transition from A -. ad?@ to 
B - -y laklcd E. Then closure(]) for set of items (states of N) I is exactly the 
E - C ~ O S U ~ P  of a set of NPA states defined in Section 3.6, Thus, gad!, X) gives 
the transition horn I on symbol X in the DFA mfistructed from N by the sub- 
set construction. Viewed in this way, the procedure itms(Gf) in Fig. 4.34 is 
'just the subset construction itself applied to the NFA N constructed from G' as 
we have described. 
Valid items. We say item A -. f3 -p2 
is valid for a viable prefix apI if there 
is a derivation S' % d w  2 c@,fllw. In general, an item will be valid for 
many viable prefixes. The fact that A + 8, .p2 is valid for apl teils us a iot 
about wheth~r to shift or reduce when we find apl on the parsing stack. In 

226 
SYNTAX ANALYSIS 
FIg. 4.36. Trausition diagram of DFA D for viablc prefixes, 
particular, if o2 # E, then it suggests lhat we haw not yet shifted the handle 
onto the stack, SO shift is our move. If P2 = E, then it Iwks as if A -. 9, 
i s  
the handle, and we sbould redrm by this production. Of course. two valid 
items may tell us to do different things for the same viable prefix. Some of 
these conflicts can be remlved by looking at the next input symbol, and others 
can be resolved by the methods of the next section, but we should not suppse 
that all parsing action conflicts can k resolved if the LR method is used to 
mnstruct a parsing table for an arbitrary grammar. 
We can easily compute the set of valid items for each viable prefix that can 
appear an the stack of an LR parser. In fact, it is a central theorem of LR 
parsing theory that the set of valid items for a viable prefix y is exactly the set 
of items reached from the initial state abng a pith labdeb y in the DFA con- 
structed from the canonical collsctiot~ of sets of items with transitions given by 
gofu. In essence, the set of valid items embvdies all the useful information 
that can be gleaned from the stack. While we shall not prove this thewern 
here, we shall give an example. 
Example 4.37. Let us consid~r the grammar (4.19) again, whose sets of items 

and goiu function are exhibited in Fig. 4.35 and 4.36. Clearly, the string 
E + T * is a viable prefix of (4.19). The automaton of Fig. 4.36 will k in 
state 1, after having read E + T *. State IT 
contains the items 
w h M  are precidy the items valid for E -t T * . To see this. consider the fol- 
lowing three rightmost derivations 
The first derivation shows the validity of T -.T * -F, the second the vdidity 
of F - *(El, and the third the validity of F + *id for the viable prefix 
E + T *. It can be shown that there are no other valid items fm E + T *, 
and we leave a proof to the interested reader. 
0 
Now we shall show how to construct t h ~  
SLR parsing action and goto func- 
tions from the deterministic finite automaton that recognizes viable prefixes. 
Our algorithm will not produce uniquely defined parsing action tables for all 
grammars, but it does s u W  om many grammars for programming 
languages, Given a grammar, G, we augment G to ptduce G', and from G' 
we construct C, the canonical mllectim of sets of items for G*. 
We mstruct 
action, the parsing action function, and goto, the gqto function, from C using 
the following algorithm. It requires us to know FOLLOWIA) for tach nmter- 
minal A of a grammar (see Secrtion 4.4). 
Algorithm 43, Constructing an SLR parsing table. 
h p t +  An augmented grammar G'- 
Ourprrr. The SLR parsing table functions action and goto for G' . 
C 
MetM. 
1. 
Conaruct C ={I0, 1 , . . . , I,}, the coibctbn of s t s  of LR(0) items for 
G'. 
2. 
State i is constructed from ii. The parsing actions for state i are deter- 
mined as follows: 
a) 
If [A + a*afl] is in It and guto(li, a) = 1,. then set m/um[S, a] to 
"shift j." 
Here u must be a terminal. 
b) If [A 
a-) 
is in ji, then set actiun[i, a] to "reduce A -. a" fw all o 

228 
WNTAX ANALYSIS 
ifi F;OLLOW(A); here A may not k $'. 
If any conflicting admx are generated by the abve ruks, we say the gram- 
mar is not SLRI I ) .  The algorithm fails to produce a parsr in this caw. 
3 .  The goto transitions f r  state i are constructed for all mnterminals A 
using the rule: If goru(Ii, A )  = I,, then goto[i, A ]  = j. 
4. 
All entries not defined by rules (2) and (3) are made "'error." 
5. The initial state of the parser i s  the one constructed from the et of items 
containing IS' + .S 1. 
o 
The parsing table consisting of the parsing adion and goto functions deter- 
mind by Algorithm 4.8 is called the SM(1) &kfw G. An LR parser using 
the %R( 9) table for G is called the SLR( I) parMr for G, and it grammar hav- 
ing an SLR(1) paning table is said to lx SU(1). We usually omit the "(I)" 
after the "SLR," since we shall not deal here with parsers having more than 
one symbol of lookahad. 
EtPmpk 4.38, Let us construct [he SLR table for grammar (4,191. The 
canonid oollmtion af wts of LR(O) items for (4.19) was Shown in Fig. 4+35. 
First mnsider the set of items Io: 
The item F - .(El gives rise €0 the entry mc#Sw[U, (1 = shift 4, the item 
F -. -id to the entry aclion[O; id] = shift 5. Other items in lo yield nr, 
actions. Now consider 1 I : 
The first: item yields acriusti I ,  $1 = accept, the second y iclds ac~iun[l, + 1 = 
shift 6. Next consider I * :  
Since FOLLOWIE) = {$, +, )), the first item makes uction[2, $1 = 
actiun[2, +] = acfiun[2, )I = reduce E - T. The second item makes 
aciionI2, *] = shift 7, Continuing in this fashion we obtain the parsing action 
and goto tables that were s h ~ w n  in Fig. 4.31. In that figure, the numbers of 
productions in reduct actions are the same aa the order in which they a p r  

SEC. 4.7 
LR PARSERS 229 
In the otiginal grammar (4+18). That is, E -. E+T is nurnter 1. E -. T is 2, 
and so on. 
a 
Example 4.39. 
Every SLR(I) grammar is unambiguous, but there are many 
unambiguous grammars that are not SLR(1). Consider the grammar with pro- 
duct ions 
We may think of L and R as standing for I-value and r-value, respectively, and 
* as an operator indicating "contents of." The canonical collection of sets of 
LR(0) items for grammar (4.20) is shown in Fig. 4.37. 
I * ;  
S + L - = R  
R - L m  
Fi. 4+37+ Canonical LRIO) wlkxtion for grammar (4+20). 
Consider the set of items l z .  The first item in this set makes acrionl2, = 1 
be "shift 6." Since FOLLOWIR) contains =, (to see why, consider 
$ "-. L = R * * R = R), the second item sets ut.tion\2, = ] to "reduce 
R - L." Thus entry acrionl2, = I is multiply defined. Since there is both a 
shift and a reduce entry in actim[2, =], state 2 has a shiftlreduce conflict on 
' Ao in Secliom 2.8, an I.valuc designates u ltuatim and an r-value is a valuc that can k stored in 
a l a w n .  

230 
SYNTAX ANALYSIS 
SEC. 4.7 
input symbol =. 
Grammar (4.20) is not ambiguous. This shift'educe 
conflict arises from 
the fact that the SLR parser construction method is not powerful enough to 
remember enough left context to decide what action the par,ser should take on 
input = having seen a string reducible to L. The canonical and LALR 
methods, to be discussed next, will succeed on a larger cdleaion of gram- 
mars, including grammar (4.20). 
!t should be pointed out, however, that 
there are unambiguous grammars for which every LR parser construction 
method will produce a parsing action table with parsing action conflicts. For- 
tunately, such grammars can generally be avoided in programming language 
applications. 
o 
Ccmtmdhg Camaical LR Parsing Tables 
We shall now present the most general technique €or constructing an LR pars- 
ing table from a grammar. Recall that in the SLR method, state i calls for 
reduction by A -. a if the set of items ii contains item !A * ot.1 and a is in 
FOLLOWEA). In some situations, however, when state i appears on top of 
the stack, the viable prefix Qa on the stack i s  such that 
cannot be followed 
by a in a right-sentential form. Thus, the reduction by A -. a would be 
invalid on input u. 
Example 4,&, 
Let us reconsider Example 4.39, where in state 2 we had item 
R -. L*, 
which wuld correspund to A -c a above, and rr could b the = sign, 
which is in FOLLOWIR). Thus, the SLR parser calls for reduction by R + L 
in state 2 with = as the next input (the shift action is also called for because 
of item S -. L- = R in state 2). However, there is no right-sententh1 form of 
the grammar in Example 4.39 that begins R = . ,, , Thus state 2, which is 
the state corresponding to viable prefix L only, should not really call for 
reduction of that L to R. 
It is possible to carry more information in the slate that will allow us to rule 
out some of these invalid reductions by A -. a. By splitting states when 
necessary, we can arrange to have each state of an LR parser indicate exactly 
which input symbols can follow a handle a for which there is a possible reduc- 
tion to A. 
T h e  extra information Is incorporated into the state by redefining items to 
include a terminal symbl as a wand component. The general form of an 
item becomes IA -. a-g , u 1, where A -. crp is a production and a is a terminal 
or the right endmarker S. We call such an object an LR(!) ifm. The I refus 
to the length of the second component, called the hkaheud of the Item+4 The 
lookahead has no effect in an item of the form IA -, a$, a 1, where B is not 
t, but an item of the form IA +a*, 
a1 calls for a red~ction by A + a only if 
Lwkahoadti that art: strinp of lcngkh grcatcr than one arc possBlc, of cwrtc, but we shall not 
consider w h  lookahcads hcrc. 

SEC. 4.7 
LR PARSERS 231 
the next input symbol is a. Thus, we are compelled to reduce by A - a only 
cm those input symbls a for which !A -. a*, 
a1 is an LR( 1) item in the state 
on top of the stack. The set of su& 
a's will always be a subset of 
FOLLOWIA), bet it could be a proper subset, as in Example 4.40. 
Formally, we say LR(I) item [A +a.p, a ]  is valid foi a viable prefix y if 
there is a derivation S % M w  
k g w ,  where 
I. y = Sa,and 
2, eitherais the first symhlof w,or w L r  mda is $. 
Exampk 4A1. Let us consider the grammar 
I 
There is a rightmost derivation S 9 oat?& * d a b .  
We see that item 
[B + wB, a ]  is valid for a viable prefix y = mu by letting 8 = uu, A = 8, 
w = ab, a = a, and $ = B in the a b v e  defmition. 
There is also a rightmost derivation S 3 
BoS 
B u d .  From this deriva- 
tion we see that item IB -. a -B, $1 is valid for viabk prefix Bao. 
u 
The method for constructing the collection of sets of valid LRII) items is 
essentially the same as the way we built the canonical alkction of sets of 
LRIO) items, We only need to modify the two procedures closure and #do. 
To appreciate the new definition of the closure operation. consider an item 
of the form [A + a.BQ, a ] in the set of items valid for some viable prefix y . 
Then there is a right- 
derivation S $Max 
?8crS$nr+ where y = h. 
Suppose par derives terminal string by. Then for each prductbn of the form 
B - q for some q. we have derivation S 
yWhy 9 yqby. 
Thus, 
[B - .q, b ]  is valid for l. Note thbt b can be the first 
&rived from 
@+ or it is possible that B derives E in the derivation 
b 
by+ and b can 
therefore be a. To summarize both possibilities we say that b can be any ter- 
minal in FIRST(Bm), where FIRST is the function from SBctiori 4.4. Note 
that x cannot m t a i n  the first terminal of by, so FIRST(@) = FERST(Po), 
We now give the LR(1) sets of items mstruction. 
A-m 
4.9. Construction of the sets of LR( 1) items. 
Input. An augmented grammar C' . 
O u p ~  
The sets of LR( 1) items that are the set of items valid for me or 
mare viable prefixes of G'. 
Method, The procedures clomre and gum and the main routine i t e m  fur con- 
structing the sets of items are shown in Fig. 4+38. 
o 
E~nmple 4.42, Consider the tollowing augmented grammar. 

3 2  SYNTAX ANALYSIS 
CClrctbn c~osure ({) ; 
besin 
repeed 
for each item [A 
u.Bp, a) in !. 
each production B + .v m G', 
and each terminal b in FIRST(@) 
such that 18 4 .y, b ]  i s  not in I do 
add IS + .y . b ]  to I; 
until no more items can be added to 1; 
wm I 
d; 
pm&m itemsIG'); 
begin c := {€hs&re({[S' + -S, 53))); 
v
t
 
hr each set of hems 1 in C and each grammar symhl X 
such that @oV, X) is not emply and not in C do 
add goto (1, X) to C 
unfJl no more sets of items can be added to C 
md 
Fi& 4.38. W
s
 of LR( 1 )  items construction for grammar G'. 
We begin by computing the closure of {[S' -. a s ,  $1). To close, we match the a 
item [S' - -S, 
$1 with the item [A -. a*BP, a ]  in the proadurc closure. That 
is. A = S', a = r, B = S, = r, and a = O. Function c h u m  tells us to add 
[B + +y, 61 for each production B - y and terminal b in FIRST(Pa), 
ln 
terms of the present grammar, B - y must be S + CC, and since fl is c and a 
is $, b may only be $. Thus we add [S 
-CC, 
$1. 
We continue to compute the closure by adding all items [C + *y, b 1 for b in 
FIR$T(C$). 
That is, matching [S -, C C ,  $1 against [A -. a*Bb, a] we have 
A = S, a = E, B = C, p = C, and a = $. Since C does not derive the empty 
string, FIRST(C$) = FIR$T(C). Since FIRSTIC) contains terminals c and dt 
we add items [C-.cC, c], [ C + &  
d l ,  [C+.d, c ]  and [ C - . d , d l .  
None of the new items has a nonterminal immediately to the right of the dot, 
so we have mmpletcd our first set of LR( I) items. The initial set of items is: 

sac. 4-7 
LR PARSERS 233 
The brackets have been omitted for notational convtnknce, and we use the 
notation [C * TC, dd] as a shorthand for tbc two items [C * -cC, r 1 and 
[C * -cC, 
dl. 
Now we compute g m  
(lo, X) for the varims values of X. For X = S we 
must close the item [S' -S*, 
$1. No additional closure is pssible, since the 
dot is at tbe right end. Thus we have the next set of items: 
For X = C we close IS + C C ,  $1. 
We add the C-productions with seaad 
component $ and then can add no more, yielding: 
Next, let X = c. We must dose {[C * r C ,  cfd]). We add the C-productions 
with second component cid, yielding: 
Finally, let X = ti, and we wind up with the set of Items: 
We have finished msiderilrg gum on lo. We get no new sets from I , ,  but 
j2 has gads on C, 
c, and b. On C we get: 
no closure bing needed, On c we take the closure of {[C +c-C, $11, to 
obtain; 
Note that I6 dit'fers from l3 
only in smmd components. We shall see that it 
is common for several sets of LRII) items for a grammar to have the same 
f~st components and differ in their second mmponenls. When we mstrud 
the wjlection of sets of LRIO) item for the same grammar, each set of LRIO) 
items will coincide with the set of first components of one or more sets of 
LR(1) items. We shall have more to say abut this phenomenon when we dis- 
atas LALR parsing, 
Continuing with the g m  function for I*, 
g t ~ u { i ~ ,  
d) is men to be: 

234 
SYNTAX ANALYSIS 
Turning now to 13, the goso's of 1 on c and d are l 3  and J4, respectively, 
and 
( i s ,  C) is: 
I g  and i5 have no gore's. The gads or 16 on c and b are It; 
and 17, respec- 
tively, and R O ~ U U ~ ~  
C) is; 
Thc remaining sets of items yield no .goiu's+ w we are done, Figure 4.39 
shows the ten sets of items with their goso's. 
We now give the rules whereby the LR( 1) parsing action and goto functions 
are constructed from the sets of LRC I) items. The action and goto functions 
are represented by a tablc as before. The only difference is in the values of 
thc entries. 
Algorithm 4.10. Construct ion of the canonical LR parsing table. 
hpur. An augmented grammar G'. 
Output. The canonical LR parsing table functions action and goto for G' . 
1 . 
Construct C = {lo, / I , . . . ,I,), the mllsction of sets of LR( 1) items for 
G'. 
2. 
State i uf thc parser is constructed from 1;. The parsing actions for state i 
are determined as follows: 
a) 
If {A - a > u P ,  & I i ~ i n / ~  
and~oto(li,a) =Ij,then set artionli. alto 
"shif~ j." Herc, rr is required to be a terminal. 
b 
If IA -a*, u J is in 1,. A f S'. then set ~ctionIl, a] to "reduce 
A - a." 
If a conflict results from the above rules, the grammar is said not to be 
LR( I ) ,  and the algorithm is said to fail. 
3. The 
goto transitions for 
state i are determined as follows: If 
gotu(li, A )  = j,, then #utali, A ]  = j. 
4. 
All entries not defined by rules (2) and (3) are made "error." 
5. Thc initial state of the parser is the one constructed from the set mntain- 
ing item IS' - -S, $1. 
n 
The table formed from the parsing action and goto functions produced by 
Algorithm 4.10 is called the canonical LRCI) parsing tabk. An LR parser 
using this table is d e d  a canonical LR(ll parser. If the parsing action 

SEC. 4.7 
LR PARSERS 235 
Fig. 4.39. The gutu graph for grammar (4.21) 
function has no multiply-defined entries, then the given grammar is called an 
LR(/) grammar. As before, we omit the "I I " if it is understood. 
Exam* 
4,43. 
The canonical parsing table for the grammar (4.2 1 ) is shown 
in Fig. 4.40. Product ions 1 , 2, and 3 are S - CC, C -, cC, and C -. d. 
Every SLR( 1) grammar is an LR(1) grammar, but for an SLR(I) grammar 
the canonical LR parser may have more states than the SLR parser for the 

236 
SYNTAX ANALYSIS 
Fig. 4.40. Canonical parsing tabk ftw grammar (4.21). 
- 
- 
0 
s3 
?A 
same grammar. The grammar of the previous examples is SLR and has art 
%R parser with seven states, compared with the ten of Fig. 4.40. 
I 
2 
3 
4 
5 
6 
7 
8 
9 
Comtrublng LALR Pming Tabks 
acc 
s6 
s7 
: s3 !d 
3 
r3 
r l  
s6 
s7 
r3 
r2 
r2 
r2 
We now introduce our last parser wnstruclion method, the LALR 
(lwhhmd-LR) technique. This method is'often used in practice because the 
tables obtained by it are considerably smalkr than the canonical LR tables, 
yet most common synladic constructs of programming languages can k 
expressed conveniently by an LALR grammar. The same is almost true for 
SLR 
bur there are a few constructs that cannot be conveniently 
handled by SLR techniques (see Example 4.39, for example). 
For a comparison of parser size, the SLR and LALR tables for a grammar 
always have the same numkr of states, and this numbtr is typically several 
hundred states for a language like Pascal, The canonical LR table would typi- 
cally have several thousand stares for the same size language. Thus, it is 
much easier and more economkal to construct SLR and LALR tables than the 
canonical LR tables. 
By way of i n t r d u c ~ i ~ n ,  
let us again consider the gleammar (4.211, whose 
sets of LR(1) items were shown in Fig. 4.39. Take a pair of similar looking 
states, such as i4 and j 7 .  Each of these states has only items with first com- 
ponent C + d*. In 14, the Iwkaheads are c* or d; in I , ,  $ is the only lmk- 
ahead + 
To see the difference between the roles of I4 and 1 in the parser, note that 
grammar (4.2i) generates the regular set c*k*b. When reading an input 
iu. - - 
r;dc.c 
+ 
+ . d, the parser shifts the first group of c's and their following 
d onto the stack, entering state 4 after reading the d. The parser then calls 
for a reduction by C - d, provided the next input symbd is c. or d, The 

SEC. 4.7 
LR PARSERS 237 
requirement that c. or d follow makes sense, since these are the symbls that 
could begin strings in e d .  If $ follows the first 6, we have an input like ccd, 
which is not in the language, and state 4 correctly declares an error if $ is the 
next input. 
The parser enters state 7 after reading the second 6. Then, the parser must 
see $ on the input,-or it started with a string not of the form c*dc*b. It thus 
makes sense tha~ state 7 should reduce by C + d on input $ and declare error 
on inputs c or b, 
Let us now replace i4 and l7 by ja7, the union of 14 and I,, consisting of 
the set of three items represented by [C -, ti., c/di$]+ The goto's on d to I4 
or I ,  from lo, [ 2 ,  i3, a n d i b  now enter 
The action of state 47 Is to 
reduce on any input. The revised parser behaves essentialiy like the originai, 
although it migbt reduce d to C in circumstances where the original would 
declare error, for example, on input like ccd or cdcbc. The error will eventu- 
ally be caught; in fact, i t  will be caught before any more input symbols are 
shifted, 
More generally, we can look for sets of LR( I) itcms having the same core, 
that is, set of first components, and we may merge these sets with common 
cores into one set of items. For example, in Fig. 4-39, 1, and l7 form such a 
pair, with core {C +d.). Similarly, 
and l6 
form another pair, with core 
{C - cC, 
C + .cC, C -. d]. There i s  one more pair, I s  and j9, with m e  
{C -. K.). Note that, in general, a core is a set of LRIO) ifems for the gram- 
mar at hand, and that an LR(L) grammar may produce more than two sets of 
items with the same core. 
Since the core of gum(/, X) depends only on the core of I, the goto's of 
merged sets can themselves k merged. Thus, there is no problem revising 
the goto function as we merge xts of items. The action functions are modi- 
fied to reflect the nm-error actions of all sets of items in the merger. 
Suppose we have an LR( I I grammar. that is, one whose sets of LR( 1) items 
produce no parsing action conflicts. If we replace all states having the same 
core with their union, it is possible that the resulting union will have a con- 
flict, but it i s  unlikely for the following reason: Suppose in rhe union there is a 
corrflia on lookahead o because there is an item [A -. a+, 
a ]  calling for a 
reduction by A -. a, and there is another item [B -t p a y ,  b ]  calling for a 
shift. Then some set of items from which rhe union was formed has item 
[A -. a+, 
a), and since the cores of all these states are the same, it must have 
an item [B -. B*ay, c ]  for some c. But then this state has the same 
shiftkduce conflict on a, and the grammar was not LR(I) as we assumed. 
Thus, the merging of states with common cores can never ,produce a 
shifthedue conflict that was n a  present in one of the original states, h a u s e  
shift actions depend only on the core, nut the lmkahead. 
It is possible, however, that a merger will produce a reducekeduce conflict, 
as the following example shows. 
Example 4.44. Consider the grammar 

238 
SYNTAX ANALYSIS 
which generates the four strings a d ,  ace, bcd, and h e .  The reader can 
check that the grammar is LR(I) by &nstructing the sets of items. Upon 
doing so, we find the set of items {IA - c., d ) ,  IB -. c . .  e J} valid for viable 
prefix ac and {IA -. s., s], IB -. c., b J) valid for bc, Neither of these sets 
generates a conflict, and their cores are the same. However, their union, 
which is 
generates a redudreduce conflict, since reductions by both A - c and B -. c 
are tailed for on inputs d and P. 
We art now prepared to give the first of two LALR table construdion algo- 
rithms. The general idea is to construct the sets of LR(1) items, and if no 
conflicts arise, merge sets with common cores. We then construct the parsing 
table from (he collection qf merged sets of items. The method we are a b u t  
to describe serves primarily as a definition of LALR( I )  grammars. Construct- 
ing the entire collection of LR(1) sets of items requires too much space and 
time to k useful in praaice. 
Algorithm 4.1 1. An easy, but apaceafisuming LALR table construction. 
Input. An augmented grammar G' 
Ourptr;. The LALR parsing table functions acrion and gom for G'. 
I .  
Construct C = {lo, 1 , . . . , I,,}, 
the collection of sets of LR( 1) items, 
2. 
For each core present among the set of LRIi) items, find all sets having 
that core, and replace these sets by their union+ 
3. 
Let C' = {Jo, J t ,  . . . , Jm) be the resulting sets of LR(1) items. The 
parsing actions for slate i are constructed from J, in the same manner as 
in Algorithm 4.10, If there is a parsing action mflin, the algorithm 
fails to produce a parser, and the grammar is said not to be LALR( I ) .  
4. 
The ~ O J O  
table is constructed as follows. If J b the union of one or more 
e t s  of LR( 1) items, that is, J = I I U 
U 
. U lk, then the axes of 
. 
p W , ,  X), guto(12, X ) ,  . . . , goto(Jl,, X) 
are 
the 
same, 
since 
f , ,  i2, . . - , Ik all have the same core+ Let K be the union of all sets of 
items having the same core as goto (1 ,, X). Then goru(d, X) = K. 
o 
The table produced by Algorithm 4.1 1 is called the L A M  parsing rabk for 
G. If there are no parsing action conflicts, then the given grammar is said to 

SEC. 4.7 
LR PARSERS 239 
be an L9LR( 1) grammar. The collection of sets of items constructed in step 
(3) is called the IALR I I ) collection. 
Example 4.45. 
Again consider the grammar (4.21) whose gum graph was 
shown in Fig. 4.39. As we mentiontd, there are three pairs of sets of items 
that can be merged, l3 
and i6 are replaced by their union: 
l 4  and IT 
are replaced by their union: 
and I s  and l9 are replaced by their union: 
The LALR action and goto functions for thc condensed sets of hems arc 
shown in Fig. 4.41. 
Fig. 4.41. LALR parsing table for grammar (4.21) 
#aim 
u 
d 
s36 
4 7  
acc 
536 
s47 
s36 
s47 
r3 
r3 
r3 
r l 
r2 
r2 
r2 
To see how the goto's are computed, consider gom(l%, C ) .  In the original 
set of LRI 1) items, gowIi3, C) = I s ,  and f is now part of I,, so we make 
g o t ~ t I ~ ~ ,  
C) be [rn. We could have arrived at the same conclusion if we con- 
sidered 
rhe orher part of !%. 
That is, g ~ r o ( l ~ ,  
C )  = f9. and i9 
is now 
part of 1 
For another example, consider gomIIl, c), an entry that is exer- 
cised after the shift action of lz on input c. In the original sets of LR(1) 
items, gut0 ( I 2 ,  1'1 = It,. Since Ib 
is now part of !36. goto ( I 2 ,  c') becomes I&. 
Thus, the entry in Fig. 4.4 1 for state 2 and input c i s  made s36, meaning shift 
and push state 36 onto the stack. 
o 
goto 
$
S
C
 
1
2
 
5 
89 
When presented with a string from the language c*dr*d, both the LR parser 
of Fig. 4.40 and the LALR parser of Fig. 4.41 make exactly the same 
sequence of shifts and reductions, although the names of the states on the 
stack may differ; Le., If [he LR parser puts i or I 
on the stack, the LALR 

. . 
240 
SYNTAX ANALYSIS 
SEC. 4.7 
parser will put 1* 
on the stack. This relationship holds in general for an 
LALR grammar. The LR and LALR parsers will mimic one another (M 
correct inputs. 
However, when prevented with erroneous input, the LALR parser may 
prmed to do some reductions after the LR parser has declared an error, 
although the LALR parser will never shift another symbol after the LR parser 
declares an error. For example, an input ccd followed by $, the LR parser of 
Fig. 4.34 will put 
on the stack, and in state 4 will discover an error, because $ is the next input 
symbol and state 4 has action error m $. In contrast, the LALR parser of 
Fig. 4.41 will make the corresponding moves, pulting 
on the stack. But state 47 on input $ has action reduce C + b. The LALR 
parser will thus change its stack to 
Now the action of state 89 on input $ is reduce C -. cC. The stack becomts 
whereupon a similar reduction is called for, obtaining stack 
Finally, state 2 has action error on input $, so the error is now discovered. 
0 
Efficimt Construchn of LALR Parsing T a k  
There are several modifications we can make to Algorithm 4.11 to avoid mn- 
strutting the full collection o i  sets of LR(I) hems in the prmss of creating an 
LALR( 1) parsing table. The first observation is that we can represent a set of 
items I by its kernel, that is, by those items that are either the initial item 
IS' - .S, $1, or that have the dot somewhere other than at the beginning of 
the right side. 
W n d ,  we can compute the parsing actions generated by i from the kernel 
alone. Any item calling for a reductian by A - a will be in the kernel unless 
a = r. Reduction by A -. E is called for on input a if and only if there is a 
kernel item 1B -. y+C& b l  such that C 5 d q  for some q, and 11 is in 
FIRST(T$&). The set of nontuminalt A such that C % A ~  
can be prccom- 
pured for each nonterminal C. 
The shift actions generated by 1 can be determined from the kernel of 1 as 
follows. We shift on input a if there is a kernel item [B * y*CB, 61 where 
C 
ru in a derivation in which the last step does not use an E-production. 
The set of such a's can also be prcmmputed for each C. 
Here is how the goto transitions for I can be computed from the kernel. If 

SEC. 4.7 
LR PARSERS 242 
[S + ysX6, b] is in the kernel of I, then [B - yX.6, b ]  is in the kernel of 
guto(I, X). Item [A +X$+ a ]  is a h  in the kernel of goro(l, X) if there is 
an item [B - y-C8. b 1 in the kernel of I. and C %AT 
for some q. If we 
precompute for each pair of nontnminah C and A whether C %drl for sane 
q, then computing sets of items from kernels only is just slightly less efficient 
than doing so with closed sets of items. 
To compute the LALRI I) sets of items for an augmented grammar G', we 
start with the kernel S' + *S of the initial set of items lo, Then, we compute 
the kernels of the goto transitions from I. as outlined above. We continue 
computing the goto transitions for each new kernel generated until we have 
the kernels of the entire collection of sels of LR(O) Items. 
Example 4.46. Let us again consider the augmented grammar 
The kernels of the sets of LRIO) items for this granirnar are shown in Fig. 
4.42. 
0 
I,: L + *R. 
IE: 
R -cL. 
lg: 
S - L = R .  
Fig. 4.42, 
Kernels of the sets of LRIO) items for grammar (4.20) 
Now we expand the kernels by attaching to each LR(O) item the proper 
lookahead symbols (second components). To see how lookahead symbols pro- 
pagate from a set of items I to gotu(l, X), consider an LR(0) item B + y C 6  
in the kernel of I. Suppose C $
A
~
 for some q (perhaps C = A and 
q = r), and A -. X f3 is a product ion. Then LR(O) item A + X . is in 
goto(i, X). 
Suppose now that we are computing not LR(0) items, but LR(I) items, and 
[B -. y+C6, b] is in the set 1. Then for what values of a will [A -. X+P, a 1 be 
in o
r
 
X ?  Certainly if some a is in FIRST(qS), then the derivation 
P 
C 
tells us that [A -X$, 
a ]  must be in guto(i, X). In this case, the 
value of b is irrelevant, and we say that a, as a lookahead for A -* X-P, is 
generated spntaneously. By definition, $ is generated spontaneously as a 
lookahead for the item S' -t 5 in the initial set of items. 
Bui there is mot her source of lookaheads for item A + X*p. If I$ 
E, 

242 
SYNTAX ANALYSIS 
SEC. 4.7 
then [A -. X - P ,  b ] wilt also be in goro(i, X). We say, in this case, that look- 
aheads propagate from B - yC6 to A -. X +, A simple method ro determine 
when an LR( I )  item in I generates a lookahad in gofo(1, X) spontaneously, 
and when lwkaheads propagate, is contained in the next algorithm. 
Algorithm 4.12. Determining lookahtads, 
h p r .  The kernel K of a set of LR(O) items I and a grammar symbol X. 
Qurpur+ The lookaheads spntaneoudy generated by items in 1 for kernel 
items In goto(!, X) and the items in I from which Icmkaheads are propagated 
to kernel Items in goro I!, X) . 
Merhd. The algorithm is grven in Fig. 4.43, 11 uses a dummy lookahead 
symbol # to detect situations in which Imkaheads propagate. 
o 
fw each item B -. y.6 in K do begin 
6' := riusure({IB - y,6. #I}): 
if [A 
a,Xp, Q ]  is in J' where a is no1 # then 
Imkahead a is generated spontaneously for item 
A - oX,P in goto(), X); 
if [A - a . X P .  #I is in J' then 
lookaheads propagate from B + y.6 in f to 
A + aX $ in goto (i, X) 
md 
Fig, 4.43. Discovering propagated and spontaneous Imkaheads. 
Now let us consider how we go a b u l  finding the Imkaheads associated 
with the items in the kernels of the sets of LRIO) items. First, we know that 
$ is a lookahead for St -. +S in the initial set of LR(0) items+ Algorithm 4,12 
gives us all the lookaheads generated spontaneously. After listing all those 
lookaheads, we must allow them to propagate unlil no further propagation is 
possible, There are ' many different approaches, all of which in some sense 
keep track of "new" lookaheads that have propagated to an item but which 
have not yet propagated out. The next algorithm describes one technique to 
propagate lookahads to all items. 
Algorithm 4.13. Efficient computation of the kernels of the LALR(1) collec- 
tion of sets of items. 
input. An augmented grammar G'. 
Ofirpur. The kernels of the LALR(1) collection of sets of items for 6'. 
I + 
Using the method wtlined above, construct the kernels of the sets of 
LR(O) items for G. 

SEC. 4.7 
LR PARSERS 243 
2. 
Apply Algorilhm 4.12 to the kernel of each set of LR(O) items and gram- 
mar symbol X to determine which Imkaheads are spontaneously gen- 
erated for kernel items in goto(\, X), and from which items in I lmk- 
aheads are propagated to kernel items in gore(!, X). 
3. Initialize a table that gives, for each kernel item in each set of items, the 
associated bokahcads. Initially, each item has associated with it only 
those bokaheads that we determined in (2) were generated spontane- 
ously. 
4. 
Make repateb passes over the kernel items in all sets, When we visit an 
item i, we look up the kernel items to which i pmpagates its looksheads, 
using information tabulated in (2). The current set of lookaheads for i is 
added to those alremdy associated with each of the items to which i pro- 
pagates its lookaheads, We continue making passes over the kernel items 
until no more new Imkaheads are propagated. 
0 
Example 4.47. 
Let us construct the kernels of the LALR( 1) items for the 
grammar in the previous example, The kernels of the LR(0) items were 
shown in Fig. 4.42. When we apply Algorithm 4.12 to the kernel of set of 
items lo, we compute ckwr~({lS' 
+ -S, # I}), which is 
Two items in this closure cause lookaheads to be generated spontaneously. 
Item [L -, +aR, = I  causes lookahead = to be spontaneously generated for 
kernel item L -. *.R in i4 and item 1L + .id, = j muses = to be spontane- 
ously generated for kernel item L + id. in 15+ 
The pattern of propagation of lmkaheads among the kernel items dtter- 
mined in step (2) of Algorithm 4.13 i s  summarized in Fig. 4.44. For example, 
the gotos of I ,  on symbols S ,  L, R, *, and id are respectively 1 1 .  f l ,  1 3 ,  I.+, 
and I s .  For 11, we computed only the ciosure of the lone kernel item 
I$' -. *$, # 1. 
Thus, 3' - -S propagates its lookahcad to each kernei item in 
I through 15+ 
In Fig. 4.45, we show steps ( 3 )  and (4) of Algorithm 4.13. The column 
labeled I N ~ T  shows the spuntaneously generated Iookaheads for each kernel 
item. On the first pas. the lookahead $ propagates from S' -. S in lo to the 
six items listed in Fig. 4.44, The lookahead = propagates from L -c *.R in I4 
to items L -, * R .  in 1, and R + L- in I x .  l t  also propagates to itself and to 
L -. id. in l s ,  but these lwkaheads are already present. In the second and 
' third passes, the only new lookahead propagated is $, discovered for the suc- 
mmrs of Iz and l o  on pass 2 and for the successor of l6 on pass 3. No new 
lookaheads are propagated bn pass 4, so the final set of lookaheads is shown 

2 
SYNTAX ANALYSIS 
SEC. 4.7 
Fig. 4 4 .  Propagation of Iookahcads. 
in the rightmost mlurnn of Fig. 4.45. 
Note that the shiftireduce conflid found in Exa'mple 4.39 using the SLR 
method has disappeared with the LALR technique. The reason is that only 
lookahead $ is associated with R -. L- in Iz, so there is no conflict with the 
parsing action of shift on = generated by item $ - L .  =R In 
D 
A typical programming language grammar with 50 to 100 terminals and LOO 
prductims may have an LALR parsing table with several hundred states. 
The action fundion may easily have 20,000 entries, each requiring at least 8 
bits to e n d e .  Clearly a more efficient encoding than a two-dimensionai 
array may bc important. We shall briefly mention a few techniques that have 
been used to compress the action and goto fields of an LR parsing table. 
One useful technique for compacting the action field is to recognize that 
usually many rows of the action table are identical. For example, in Fig. 
4.40, states 0 and 3 have identical actwn entries, and sr, do 2 and 6. We can 
therefore save considerable space, at little cost in time, if we create a pointer 
for each state into a onedimensional array. Pointers for states with the same 
adions p i n t  to the same location, To access information from this array, we 
assign each terminal a number from zero to one less than the number of ter- 
minals, and we use this integer as an offset from the pointer value for each 
state. In a given state, the parsing action for the ith terminal will te found i 
locations past the pointer value for that state, 

SEC. 4.7 
LR PARSERS 245 
Fig. 4.45. Computation of hkahcads. 
Further space efficiency can be achievd at the expense of a somewhat 
slower parser (generally considered a reasonable trade, since an LR-like 
parser consumes only a smalI fraction of the total cornpilarim time) by creat- 
ing a list for- the actions of each state. The list consists of (terminal-symbd, 
action) pairs. The most frequent action for a state can be placed at the end of 
the list, and in place of a terminal we may use the ndation "any," meaning 
.that if the current input symbol has not been found so far on the list, we 
should do that action no matter what the input is. Moreover, error entries 
can safely tK replaced by reduce actions, for further uniformity along a row. 
The errors will be detected later, before a shift move, 
Exampk 4 M .  Consider the parsing table of Fig. 4.3!. 
First, note that the 
actions for states 0, 4,6, and 7 agree, We can represent them ail by the list: 
State i has a similar list: 

246 WNTAX ANALYSIS 
SEC. 4.7 
In state 2, we can repbce the error entries by r2, so reduction by production 2 
will occur CMI any input but 1. Thus the list for state 2 is: 
State 3 has only error and r4 entries. We can replace the former by the latter, 
so the list for state 3 mnsist~ of only the pair (any, r4). States 5 ,  10, and 1 I 
can &e treated similarly. The list for state 8 is: 
and for state 9: 
We can alw c n d e  the goio table by a list, but here it appears more effi- 
cient to make a list of pairs for each nonterminal A. Each pair on the list for 
A is of the form (curre~r-srate, ncxt-s~ate), indicating 
This kchnique is useful h a u s e  there tend to be rather few states in any one 
mlumn of the pto table. The reason is that the goto on nonterminal A can 
only be a state derivable from a set of items in which some items have A 
immediately to [he left of a dot. No set has items with X and Y immediately 
to the left of a dot if X # Y. Thus, each state apvars in at most one goto 
column. 
For more spcx reduction, we note that the error entries In the goto tabk 
are never consulted. We can therefore replace each error entry by the most 
common non-error entry in its column. This entry becomes the default; it is 
represented in the list for each column by one pair with "any" in place of 
current-state. 
Example 4,49. 
Consider Fig, 4.31 again. The column for F has entry 10 tor 
state 7, and all other entries are either 3 or error. We may replace error by 3 
and create for wlurnn F the list: 
Qmilarly, a suitable list for column T is: 
For column E we may choose either 1 or 8 to be the, default; two entries are 
necessary in either caw. For example, we might create for column E the lid: 

SEC. 4,8 
USING AMBIGUOUS GRAMMARS 247 
If the reader totats up the number of entries in the lists created in this 
example and the previous one, and then adds the pointers from states to 
action iists and from nmterminals to next-state lists, he will not be impressed 
with the space savings over the matrix implementation of Fig. 4.3 1. 
We 
should not be misled by this small example, however. For practical gram- 
mars, the space needed for the list representation is typically less than ten per- 
cent of that needed for the matrix representaticm. 
We should also point out that the table-compression methods for finite auto- 
mata that were discussed in Section 3.9 can also be used to represent LR pars- 
ing tables. Application of these methods is discussed in the exercises. 
4.8 USING AMBIGUOUS GRAMMARS 
[t is a theorem that every ambiguous grammar fails to be LR, and thus is not 
in any of the classes of grammars discussed in the previous section. Certain 
types of ambiguous grammars, however, are useful in the specifica~ion and 
implementation of languages, as we shall see in this section. For language 
constructs like errpressions an ambiguous grammar provides a shorter, more 
natural specification than any equivalent unambiguous grammar. Another use 
af ambiguous grammars is in i.wlating commonly murring syntactic con- 
structs for special case optimizatbn, With an ambiguous grammar. we can 
spcify the special case constructs by carefully adding new productions to the 
grammar + 
We should emphasize that although the grammars we use are ambiguous, in 
a l l  cases we spcify disambiguating rules thal allow only one parse tree for 
each sentence. [n this way, the overall language specification still remains 
unambiguous. We also stress that ambiguous constructs should be used spar- 
ingly and io a strictly controlled fashion; otherwise, there can be no guarantee 
as to what language is recognized by a parser, 
Using Precedence and Associativity lo Resolve Parsing Action Conflicts 
Consider expressions in programming languages, The following grammar for 
arithmetic expressions with operators + and * 
is ambiguous because it does not specify the associativity or precedence of the 
operators + and *. The unambiguous grammar 
generates the same language, but gives + a lower precedence than *, and 
makes both operators left-associative. There are two reawns why we might 

248 
SYNTAX ANALYSls 
SEC. 4.8 
want to use grammar (4.22) instead of (4.23). First, as we shall see, we can 
easiiy change the associativities and precedence bvels of the operators + and 
* without disturbing the productions of (4.22) or tht number of statts in the 
resulting parser. Second, the parser for (4.23) will spend a substantial frac- 
tion of its time reducing by the prductions E -. T and T - F, whose &ole 
function is to enforce associativity and precedence, The parser for (4.22) will 
not waste time reducing by these single productions, as they art called. 
I,: E + E+E- 
E + E.+E 
E - E-*E 
Fig. 4.4, Sets of LR(O) items for augmented gfamnar (4.Z!), 
The scts of LR(0) items for (4.22) augmented by E' + E art shown in Fig. 
4.46. Since grammar (4.22) is ambiguous, parsing action conflicts will k 
gen- 
erated when we try to produce an LR parsing table from the sets of items+ 
The states corresponding to sets of items 1, and I s  generate these conflicts. 
Suppose we use the SLR approach to constructing the parsing adion table. 
The conflict generated by t r  bctween reduction by E -. E +E and shift on + 
and * cannot k resolved bemuse + and s are each in FOLLOW(@. Thus 
both actions would be called for on inputs + and *, A similar wnflla is gen- 
erated by I s ,  berween reduction by E -. E*E and shift on inputs + and *. Cn 
fact, ea& of our LR parsing table mstruction methods will generate these 
conflicts. 

SEC. 4.8 
USING AMBIGUOUS GRA WMARS 249 
However, these problems can be resolved using the precedence and associa- 
tivity information for + and *. Consider the input id + id )F id, which 
causes a parser based on Fig. 4.46 to enter state 7 after prmssing id + id; in 
particular the parser reaches a configuration 
Assuming that * takes precedence over + , we know the parser should shift 
* onto the stack, preparing to reduce the s and its surrounding id's to an 
expression. This is what the SLR parser of Fig. 4.3 ! for the same language 
would do, and it is what an operator-precedence parser would do. On the 
other hand, if + takes precedence over *, we know the parser should reduce 
E +E to E. Thus the relative precedence af + followed by * uniquely detcr- 
mines how the parsing action conflia between reducing E -- E + E  and shifting 
on * in state 7 should be resolved. 
If the input had k e n  M + M + id, instead, the parser would still reach a 
configuration in which it had stack OE 1+4E7 after processing input id + id, 
On input + there is again a shiftlreduce conflict in state 7. Now, however, 
the assmiativity of the + operator determines how this conflict should be 
rewlvd. If + i s  left-associative, the correct action i s  to reduce by E -. E +E, 
That is. the M's surrounding the first + must be grouped first+ Again this 
choice coincides with what the SLR or operator-precedence parsers would do 
for the grammar of Example 434. 
In summary, assuming + is left-associative, the aclion of state 7 on input + 
should be to reduce by E -. E C E, and assuming h a t  * takes precedence over 
+ , the action of state 7 on input * should bi to shift. Similarly, assuming 
that * i s  left-associative and takes precedence over +, we can argue that state 
8, which can appear om top of the stack only when E * E are the top three 
grammar symbols, should have a a i m  reduce E + E * E on both + and * 
inputs. In the case of input + , the reason is chat + takes precedence over +, 
while in the case of input *,  he rationale is that * is left-assmiathe. 
Proceeding in this way, we obtain the LR parsing table shown in Fig. 4+47. 
Productions 1-4 are E. -. E S E ,  E -. E * E, E -. ( E ) ,  and E - id, respec- 
tively, It is interesting that a similar parsing action table would be prduced 
by eliminating the reductions by the single productions E + T and T -5 F from 
the SLR table for grammar (4.23) shown in Fig. 4.31, Ambiguous grammars 
like (4.22) can k handled in a similar way in the context of LALR and 
canonical LR parsing. 
Tho "Danglhg-else" Ambiguity 
Consider again the following grammar for conditional statements: 

250 
SYNTAX ANALYSlS 
Fig. 4.47, Parsing tablc for grammar (4.22). 
As we noted in Section 4.3, this grammar is ambiguous because it does not 
resolve the dangling-else arrtbbiguity. To simplify the discussion, let us con- 
sider an abstracrim of the above grammar, wbere i stands for iCexpr then, e 
stands for else, and a stands for "all other productions." We can then write 
the grammar, with augmenting production $' - $. as: 
The sets of LR(0) items for grammar (4.24) are shown in Fig+ 4.43, The 
ambiguity in (4.24) gives rise to a shift/redace conflict in f 4 .  There, S - is-cS 
calk for a shift of a and, since FOLlOW(S) = {c, $1, item S + 8- 
calls for 
reduction by S -. iS on input c. 
Translating back to the if . . . then . . dse terminology, given 
on the stack and else as the first input symbol, should we shift else onto the 
stack Ii.e., shift r )  or reduce if uxpr then m
r
 to m r  (i .e, reduce by $ -. d1'2 
The answer is that we should shift else, because it is "associaled" with the 
previous then. In the terminology of grammar (4.241, the u on the input, 
standing for else, can only form part of the right side beginning with the iS on 
the top of the stack. If what follows I. on the input cannot be parsed as an S, 
completing right side i S d ,  then it can be shown that there is no other parse 
possible . 
We are drawn to the mnslusion that the shiftkeduce conflict in l4 
should bc 
resolved in favor of shif~ an input E. The SLR parsing table constructed from 
the sets of items of Fig. 4.48, using this resolution of the parsing action con- 
flict in /, 
on input e, is shown in Fig. 4.49. Productions I through 3 are 
S -. iSd, S -. is, and S - u, resptively. 

USING AMBIGUOUS GRAMMARS 251 
I , :  S' -c S .  
Fig. 4.48. LRIO) states for ahgmcntcd grammar (4.24), 
Fig. 4.49. LR parsing tablc for abstract "danglingcl,~" grammar, 
For exarnpk, on input iiued, the parser makes (he moves shown in Fig. 
4+SO, corresponding to the correct resolution of the "dangling-else." At line 
151, state 4 selects the shift action on input r, whereas at line (9) + state 4 calls 
for reduct ion by S + iS on $ input. 
By way of cornparism, if we are unable to use an ambiguous grammar to 
specify conditional statements, then we would have to use a bulkier grammar 
along the lines of (4.9). 
Ambiguities from SpedalXase Productbm 
Our final example suggesting the usefulness of ambiguous grammars occurs if 
we Introduce an addiitional production to specify a special case of a syntactic 
construct generated in a more general fashion by the rest of the grammar. 
When we add the extra production, we generate a parsing action conflict. We 
can often resolve the conflict satisfactorily by a disambiguating ruie that says 

252 
SYNTAX ANALYSIS 
fig. 4.50, Parsing actions rakon on input iiueu. 
reduce by the special-case product ion. The semantic action associated with the 
additional production then allows the special case to be handled by a more 
specific mechanism. 
An interesting use of special-case productions was made by Kernighan and 
Cherry 1 19'15 j in their equation-typesetting preproceuwr EQN, which was used 
to help typeset this book. In EQN, the syntaK of a mathematical expression is 
described by a grammar that uses a subscript operator sub and a superscript 
operalor sup, as shown In the grammar fragment (4.25). Braces are used by 
the preprocessor to bracket compound expressions, and c is usxi as a taken 
representing any string of text. 
Grammar (4.25) is ambiguous for several reasons. The grammar does not 
specify the associativity and pr~edence of the operators sub and sup. Even if 
we resolve the ambiguities arising from the associativity and precedence of the 
sub and sup, say by making these two operators of equal precedence and righ I 
associative, the grammar will still be ambiguous. This is because production 
( I) isolates a special case of expressions generated by productions (2) and (31, 
namely expressions of the form E w b  E sup E. The reason for creating 
expressions of this form specially is that many typesetters would prefer to 
typeset an expression like a sub i sup 2 as II? 
rather than as ui2. By merely 
adding a special caw production, Kernighan and Cherry were able to get EQN 
to produce this special case output, 
To see how h i s  kind of ambiguity can be treated in the LR setling, lel us 
construct an SLR parser for grammar (4.25). The. sets of LR(0) items for this 

USING AMBIGUOUS GRAMMARS 253 
I": E' -. -E 
E * .E sub E sup E 
E + .E sub E 
E -c .E sup E 
E - r . { E }  
E + +i. 
I T :  E -, E + s u ~  
E SUP E 
E - E sub &.sup E 
E -€.sub E 
E - c E m b E .  
E -. E.sup E 
I , ;  
E -. E a b  E sup E 
E -. E,sub E 
E -- E.sup E 
E -. E sup E .  
Ill: 
E 
E - ~ b  
E sup E 
E + E  mbE mpE. 
E - E-sub E 
E -. E+mp E 
E + E sup E* 
Hg4 4 S .  LRID) mts of items for grammar (4+25), 
grammar are shown in Fig+ 4.51. ln this colkction, three sets of items yield 
parsing anion conflicts. 1 ,, la, and 1 I I generate shiftkduce conflicts on the 
tokens sub and sup because the associativity and precedence of these operators 
have not ken specified. We resolve these parsing action conflicts when we 
make sub and sup of equal precedenw and right-amxiaiive. Thus, shift is 
preferred in each case. 

SEC. 4.8 
I , ,  aim generates a reducdreduce conflict on inputs ) and $ between the 
two productions 
E -. E subE sup E 
E -. E sup E 
State / 
will be on top of the stack when we have seen an input that has been 
reduced to E sub E sup E on the stack. If we resohe the redumlreduce con- 
flict in favor of production ( I), 
we shall treat an equation of the form form E 
sub E sup E as a special case. Using these disambiguating rules. we obtain 
the SLR parsing table show in Fig. 4.52. 
Fig. 432. Parsing table for grammar (4+25). 
STATE 
0 
I 
2 
3 
4 
5 
6 
7 
8 
9 
10 
I1 
Writing unambiguous grammars that factor out special case sy ntsctic coo- 
strum is very difficult. To appreciate how difficult this is, the reader is 
invited to construct an equivalent unambiguous grammar for (4,251 that iso- 
lates expressions of the form E sub E sup E, 
Error Retovery in LR Parsing 
r rct ion 
goto 
sub 
sup 
{ 
ch 
S I 
E 
An LR parser will detecq an error when it consults the parsing action table 
and finds an error entry, Errors are never detected by consulting the goto 
table. 
Unlike an operator-precedence parser, an LR parser will announce 
error as soon as there is no valid continuation for the portion of the input thus 
far scanned. A canonical LR parsing will never make even a single reduction 
before announcing an error. The SLR and LALR parsers may make xevcral 
reductions before announcing an error, but they will never shifi an erroneous 
input symbol onto the stack. 
In LR parsing, we can implement panic-made error recovery as follows. 
$2 
s3 
' s 4  d 
acc 
s2 
s3 
r5 
r5 
r5 
r5 
s 2 
s3 
s2 
s3 
I 
6 
7 
8 
54 
s5 
s9 
I 
s4 
si0 
r2 
r2 
s4 
$5 
r3 
r3 
r4 
r4 
r4 
r4 
s2 
s3 
s4 
s5 
r l 
r 1 
I I 

SEC. 4.8 
USING AMBIGUOUS GRAMMARS 255 
We scan down the stack until a state s with a goto on a particular nonterrninal 
A i s  found. Zero or more input symbols are then discarded until a symbol a is 
found that can legitimately follow A. The parser then stacks the state 
~ I O [ S ,  A ]  and resumes normal parsing. There might be more than one 
choice f o ~  the nonterminal A. 
Normally these would be nonterminals 
representing major program pieces, such as an expression, statement, or 
block. For example, if A is the nonterminal s ~ m t ,  a might be semicolon or 
end. 
This method of recovery attempts to isolate the phrase containing the syn- 
tactic error, The parser determines that a string derivable from A contains an 
error. Part of that string has already been prows&, 
and the result of this 
processing is a Mquence of states on top of the stack. The remainder of the 
string is still on the input, and the parser attempts to skip over the remainder 
of this string by looking for a symbol on the input that can legitimately follow 
A, By removing states from the stack. skipping over the input, and pushing 
~otuls, A I on the stack, the parser pretends that it has f o u ~ d  an instance uf A 
and resumes normal parsing. 
Phrase-level recovery is implemented by examining each error entry in the 
LR parsing table and deciding on the basis of language usage the most likely 
programmer error that would give rise to that error. An appropriate recovery 
procedure can then be constructed; presumably the top of the stack andbr first 
input symbols would be modified in a way deemed appropriate for each error 
entry. 
Compared with operator-precedence parsers, the design of specific error- 
handling routines for an LR parser is relatively easy. In particular, we do not 
have to worry about faulty reductions; any reduction called for by an LR 
parser is sureiy mrrecl. Thus we may fill In each blank entry in the action 
field with a pointer to an error routine that will take an appropriate action 
selected by the compiler designer. The actions may include insertion or dele- 
tion of symbols From the stack or the input or b t h ,  or alteration and transpo- 
sition of input symbols, just as for the operator-precedence parser. Like that 
parser, we must make our choices without allowing the possibility that the LR 
parser will get into an infinite Imp+ A strategy that assures at least one input 
symbol will be removed Or eventually shifted. or tha~ the stack will eventually 
shrink if the end of the input has been reached, is sufficient in this regard. 
Popping a stack state that covers a nonterminal should te avoided. because 
this modification eliminates from the stack a construct that has already been 
successfully parwi+ 
Example 4.9. Consider again the exp~ession grammar 
Figure 4.53 shows the LR parsing table from Fig. 4.47 for this grammar, 
mdified for error detection and recovery. We have changed each state that 
calls for a particular reduction on some input symbols by replacing error 
entries in that slate.by the reduction, This change has the effect of pstfponing 

the error dekctim until one or more reductions are made, but the error will 
stiH be caught before any shift move takes place. The remaining blank entries 
f r m  Fig+ 4.47 have been replaced by calls to error routines. 
Fig. 4.53. LR parsing table with errw routines, 
The error routines are as folbws. The similarity of these actions and the 
errors they represent to the error actions in Example 4-32 (operator pre- 
cedence) should bc noted. However, case el in the LR parser is frequently 
handled by the reduction processor of the operator-precedence parser. 
e 1: / r This routine is eallcd from states 0 ,  2, 4 and 5. all of which &n the 
beginning of an operand, either an id or a left parenthesis. Instead, an 
operator, + or *, or the end of the input was folrnd. */ 
push an imaginary id onto the stack and cover it with stale 3 (the goto of 
states 0, 2,4 and 5 on id)' 
issue diagnostic "missing operand" 
e 2  /* This routine is called from states 0, 1, 2, 4 and 5 on finding a right 
parenthesis. * I  
remove the right parenthesis from the input 
issue diagnostic "unbahced right parenthesis" 
e3;' i* 
This routine is called frm states I or 6 when expecting an oprator, 
and an id or right parenthesis is found. */ 
push + onto the stack and cover it with state 4. 
issue diagnostic 'missing operator" 
e4: /* This routine is called from state 6 when the end of the input is found. 
Note thai L practice grammar symbols are not placed on the stack. It is useful to irnagim them 
thew lo remind us of the symbols which the state "represent." 

SEC. 4,9 
PARSER GENERATORS 257 
State 6 expects an operator or a right parenthesis. */ 
push a right pamntkesis onto the stack and mver it with state 9. 
issue diagnostic "missing right parenthesis" 
On the erroneous input id + ) discussed in Example 4.32, the sequence of 
configurations entered by the parser is shown in Fig. 4.54. 
o 
E R R ~ P ~  
MESSAGE AND ACTION 
"unbalanced right parenthesis" 
, 
e2 removes tight parenthesis 
"miasing operand" 
el pushcs id 3 on stack 
FPg. 4.54. Parsing and error recovery mows made by LR parscr. 
4,9 PARSER GENERATORS 
This section haws how a parser generator can be used to facilitate the can- 
struction of the front end of a compiler. We shall use the LALR parser gen- 
erator Yacc as the basis of our discussion, since it implements many of the 
concepts discussed in the previous two redions and it is widdy available. 
Y acc stands for "yet another mmpilerampikr," rending the popularity of 
parser generators in the early 1970's when the first versiort of Yacc was 
created by S. C. Johnson. Yacc is available as a command on the UNIX sys- 
tem, and has been used to help implement hundreds of wmpilers+ 
A translator can be constructed using Yacc in the manner illustrated in Fig. 
4.55. First, a file, say tran~late . y, containing a Yacc specification of the 
translator is prepared. The UNIX system command 
transforms the file translate .y into a C program called y + tab. c using the 
LALR method outlined in Algorithm 4.13. - The program y, tab, c i s  a 
representation of an LALR parser written in C, along with other C routines 
that the user may have prepared. The LALR parsing table is compacted as 
described in SeFtion 4.7. By compiling y,tab.e along with the ly library, 

258 
SYNTAX ANALYSIS 
specification <-. 
corn p i h  
y.tab.c 
translate. y 
y, tab.c 
- 
a. out 
! compi~cr 
input 
a. 
out 
output 
Fig. 4.55, Crcat ing an inputhutput translator with Yacc. 
that contains the LR parsing program using the command 
we obtain the desired object program a. out that performs the translation 
specified by the original Yacc 
ff ather procedures are needed, they 
can be compiled or loaded with y . tab. c ,  just as with any C program. 
A Y acc source program has three parts: 
declarations 
%% 
translation rules 
%% 
supporting C-routines 
Exempk 4-51. To illustrate how to prepare a Yacc wurce program. let us 
construct a simple desk calculator that reads an arithmetic expression, evalu- 
ates it, and then prints ils numeric value. We sha!l build the desk calculator 
starting with the follawing grammar for arithmetic expressions: 
E - E + T l T  
T - T * F l F  
F - ( E )  digit 
The [oken digit is a single digit between 0 and 9. A Yacc desk calculator pro- 
gram derived From this grammar is shown in Fig. 4.56. 
~1 
The dedararions porr. There are two optional sections in the declaralions 
part of a Yacc program. In the first section, we put ordinary C declarations, -- 
delimited by %i 
and X I .  Here we place declarations of any temporaries used 
by the translation rules or procedures of the second and third sections. In 
%c 
name ly is system dcpcndcnl. 

PARSER GENERATORS 259 
%token DIGIT 
%% 
1 h e  
: expr '\nP 
4 printfC"?M\n", S t ) ;  1 
i 
expr 
: expr ' +' term 
{ $$ = $1 + $3; 1 
term 
* 
term 
: 
term '*' factor { $$ = $1 
$3; 1 
I 
factor 
factor : ' I ' expr ' ) ' 
i SS = $ 2 ;  1 
DIGIT 
9 
%% 
yylex0 { 
int c; 
c = getcharl); 
if Iisdigitlcl) { 
yylval = e-'0'; 
return DIGIT; 
1 
return c; 
1 
Fig. 4.56. Y acc specificat ion of a simple desk calcula~or 
. 
Fig. 4.56, this section contains only the include-statement 
that causes the C preprocessor to include the standard header file gctype . hs 
that contains the predicate i s d i g i t .  
Also in the declarations part are declarations of grammar tokens, In Fig. 
4.56, the statement 
%token DIGIT 
declares DIGIT to be a token. Tokens declared in this stction can then be 
used in the second and third parts of the Yacc specification. 
The trmbtion rules part. In the part of the Yacc specification after the 
first %% pair. we put the translation rules. Each rule consists of a grammar 

production and the associated semantic action. A set of prducthns that we 
have been writing 
would be written in Yacc as 
<left sider 
: dalt 1, 
{ semantic action 1 1 
calt 2r 
{ semantic action 2 1 
.
I
.
 
I 
I 
< a l t n >  
(semantic a e t i o n n )  
? 
[n a Yacc production, a quoted single character ' c '  i s  taken to be the termi- 
nal symbol. c, and unquotcd strings of letters and digits not declared to be 
tokens are taken to be nonterminals, Alternative right sides can be separated 
by a vertical bar, and a smicolm follows each left side with its alternatives 
and their semantic actions, The first left side is taken to be the start symbol. 
A Yacc semantic action is a sequence of C statements. In a semantic 
action. the symbol $S refers to the attribute value associated with the nonter- 
mind on the IeR, while $5. refers to the value associated with the ith grammar 
symbol (terminal or nonterrninal) on the right. The m a n t i c  action is per- 
formed whenever we reduce by the associated production, so normally the 
semantic action computes a value for $$ in terms of the Qi's. In the Yacc 
specification, we have written the two 'E-ptoduct ions 
and their associated semantic actions as 
exgr 
: expr ' + '  t e r m  
I SS = $1 + $3; 1 
! 
term 
* 
Note that the nonterrninal term in the first production is the third grammar 
symbol on the right, while ' + ' is the second. The semantic action associated 
with the first production adds tht value of the expr and the term on the 
right and assigns the result as the value for the nonterminal cxpr on the left, 
We have omitted the semantic adion for the second production altogether, 
since copying the value i s  the default action For productions with a single 
grammar symtd on the right. In general, { $$ = 81 ; t is the default 
semantic action, 
Notice that we have added a new starting production 
line 
: expr 'h' 
I printf["%a\nn, $ 1 ) ;  
) 
to the Yam specification. This production says that an input to the desk cal- 
culator is to be an expression follciwed by a ncwline character. The semantic 
action asmiated with this production prints the decimal value of h e  cxpes- 
sion followed by a ncwline character. 
' 

SEC. 4.9 
PARSER GENERATORS 261 
The s~pporrirrg C-rourirtes port, The third part of a Yacc specification con- 
sists of supporting C-routines. A lexical analyzer by the name yylex( ) must 
be provided. Other prmedures such as error recovery routines may be added 
as necessary. 
- The lexical analyzer yylexC 1 produces pairs consisting of a token and its 
associated attribute value. If a token such as DIGIT is returned, the token 
must be declared in the first section of the Yacc specification. The attribute 
value associated with a token is communicated to the parser through a Yacc- 
defined variable yylval. 
The lexical analyzer in Fig. 4.56 is very crude. it reads Input characters 
one at a time using the C-function getchar( 1. If the character is a digit, 
the value of the digit is stored in the variable yylval, and the token DIGIT 
is returned. Otherwise, the character itself is returned as the token. 
Using Y w c  with Ambiguous Grammars 
Let us now modify the Yacc specification so that the resulting desk calculator 
becomes more useful. First, we shall allow the desk calculator to evaluate a 
sequence of expressions, one to a line. We shall also albw blank lines 
between expressions. We do this by changing the first rule to 
lints 
: lines expr 'h' { printf("%g\nl', $ 2 ) ;  ) 
I 
I 
lines "a' 
I 
9 
In Y acc, an empty alternative, as the third line is, denotes E. 
Semnd, we shall enlarge the dass of expressions to inelude numbers instead 
of single digits and to include the arithmelic operators t , - (both binary and 
unary), *, and /. The easiest way to specify this class of expressions is to use 
the ambiguous grammar 
The resulting Yacc specification is shown in Fig. 4.57. 
Since the grammar in the Yacc specification in Fig. 4.57 is ambiguous, the 
LALR algorithm will generate parsing action conflicts. Yacc will ~tpt 
the 
number of parsing sclion conflicts that are generated. A description of the 
sets of items and the parsing action conflicts can be obtained by invoking Yacc 
with a -v option. This oplion generates an additional file y .output that 
contains the kernels of the sets of items found by the parsing, a description of 
the parsing action conflicts generated by the LALR algorithm, and a readable 
representation of the LR parsing table showing how the parsing action con- 
flicts were resolved. Whenever Yacc reports that it has found parsing action 
wnflkts, it is wise to create and wnsult the file y .output to see why the 
parsing action conflicts were generated and to see whether they were rewlved 
correctly. 

262 
SYNTAX ANALYBS 
w 
#include <ctype.h> 
#include <rtdio.h> 
Xdefine YYSTYPE double I+ double type for Yaee stack +J' 
% 1 
%token NUMBER 
Xleft '+' '-' 
Xlcft '+' '/' 
%right WINUS 
expr ' +' expr 
{ $$ = $1 + $3; 1 
expr ' - * * X F  
C SS = $1 - $3; 1 
rxpr 'a' 
expr 
( $$ = $1 u $3; 1 
txpr '/' exgr 
i 
$$ = $1 / $3; ) 
' [ *  fXPf * I '  
I $$ = $2; 1 
'-' expr 
Xpree mdZNWS 
I tL = - $2; 3 
NUMBER 
I 
i n t  c ;  
while [ I c = getchar0 1 -- ' 
1; 
i f  [ { c  == ' . ' I  !: <indigit{cll 1 { 
ungetcIc, atdin); 
scanf["%lfn, 6yylval); 
return NUMBER; 
3 
return c; 
1 
Unless otherwise instructed Yacc will resolve all parsing action conflicts 
using the following two ruks: 
1. 
A reducdreduce conflict i e  resolved by choosing the conflicting produc- 
tion listed first iin the Y m  specification. Thus, in order to make the 
correct resolution in the typesetting grammar (4.25), it is sufficient to Ikt 
production ( 1) ahead of production (31. 

SEC. 4.9 
PARsERGENERATORS 263 
2. 
A shiftlreduce conflict is resolved in favor of shift. This rule rewlves the 
shiftkeduce conflict arising from the dangling-else ambiguity correctly. 
Since these default rules may not always be what the compiler writer wants, 
Yacc p~ovides a general mechanism for resolving shiftlrebuce conflicts. In the 
declarations portion, we can assign precedences and associativities to termi- 
nals. The declaration 
X l e f t  ' + '  '-' 
makes + and - be of the same precedence and be left associative. We can 
declawan operator to t~ right associative by saying 
and we can force an operator to be a nonassociative binary operator (i .e., two 
occurrences of the operator cannot be combined at all) by saying 
The tokens are given precedences in the order in which they appear in the 
declarations part, lowest first. Tokens in the same declaralion have the same 
precedence. Thus, the declaration 
in Fig+ 4.57 gives the token UHINUS a precedence Icvel higher than that of the 
Five preceding terminals. 
Yaw rewlves shift/reduce conflicts by attaching a precedence and associa- 
tivity to each prcduction involved in a conflict, as well as to each terminal 
involved in a conflict. If it must choose between shifting input symbol a and 
reducing by production A -. a, Yscc reduces if the preoedena of the prduc- 
tion is greater than that of a, or if the precedences are the same and the asso* 
ciativity of the production is left. Otherwise, shift is the chosen action. 
Normally, the precedence of a production is taken to be the same as that of 
its rightmost terminal. This is the sensible decision in most cases. For exam- 
ple, given productions 
we would prefer to reduce by E + E + E  with lookahead +, because the + in 
the right side has the same precedence as the hkahead, but I s  left asmia- 
tive. With lookahead *, we would prefer to shift, kcause the lookahead has 
higher precedence than the + in the production. 
In those situations where the rightmost terminal does not supply the proper 
precedence to a prduction, we can force a precedence by appending to a pro- 
duction the tag 
The precedence and associativity of, the production will then be the same as 
that of the terminal, which presumably is defined in the declaration section. 

264 
SYNTAX ANALYSIS 
SEC. 4.9 
Y acc does not report shiftjrcduce conflicts that are resolved using this pre- 
cedence and associativity mechanism. 
This "terminal" can be a p!awhoIder, like ~JMINUS in Fig. 4.57; this termi- 
nal is not returned by the lexical analyzer, but is declared sdely to define a 
precedence for a production. In Fig. 4.57, the declaration 
assigns to the token WMI NLfS a p r d c n u  that is highc~ than that of r and /. 
in the translation rules part, the tag 
at the end of the prduction 
expr 
: 
I - '  
expr 
makes the unary minus operator in this production have a higher prectdence 
than any other operator. 
Creating YEKC 
Lexical Analyzers with liex 
Lex was designed to produce lexical analyzers that muid be u
d
 
with Yaw. 
The Lex library 11 will provide a driver program named yylrxI 1, the name 
required by Yacc for its bxical analyzer. If Lex is used to produce the lexical 
analyzer, we replace the routine yylext 1 in the third part of the Yacc specif- 
ication by the statement 
and we have each Lex action return a terminal known to Yam. By using the 
#include "lex.yy. c" statement, the program yykx has access to Yacc's 
names for tokens, since the Lex output file is compiled as part of the Yacc 
output file y . tab. c. 
Under the UNIX system, if the tex specification is in the file first. 1 and 
the Yacc specifica(ion in second. y, we can say 
lex first.1 
yace 8ccand.y 
cc y,tab,c -ly -11 
to obtain the desired translator. 
The Lex specificah in Fig. 4.58 can tx used in place of the lexical 
analyzer in Fig. 4.57. The last pattern is \nl since . in Lex matches any 
character but a newiine, 
Ermr Rmvery in Yacc 
In Yacc, error recovery can be prformed using a form of error prductions. 
First, the user decides what "major" nonterminals wjll have error recovery 
associated with them. Typical choices are some subset of the nonterminals 

PARSER GENERATORS 265 
number 
[o-9]+\.?l[0-9]+\.[0-9]+ 
%% 
I
3
 
{ I+ skip blanks +/ ) 
{number 1 
( sscanf{yytext. " Y l f  , Lyylval ) ; 
return NUMBER; 1 
\n: 
4 return yytcxt(0J; ) 
Fig. 4.58. Lex 6pecification for yyler I 1 in Fig. 4.57. 
generating expressions, statements, blocks, and procedures+ The user then 
adds to the grammar error productions of the form A -. error a 
+ where A is a 
major nontcrminal and a is a string of grammar symbls, perhaps the empty 
string; error is a Yacc reserved word. Y ~ C C  
will generate a parser from such 
a spifiaticm, treating the error product ions as ordinary productions. 
However, when the parser generated by Yacc encounters an error, i t  treats 
the states whose sets of items contain error productions in a special way, On 
encountering an error, Yam pops syrnbds from its stack until it finds the top- 
most state on its stack whotie underlying set of items includes an item of the 
form A + + error a. The parser then "shifts" a fictitious token error onto the 
stack, as though it saw the token ermr on its input. 
When a is E, a reduction to A occurs immediately and the semantic action 
associated with the product ion A -. error (which might be a user-specified 
error-recovery routine) i s  invqked. The parser then discards input symbls 
until it finds an input symbl on which normal parsing can proceed. 
If a is not empty, Yam skips ahead on the input looking for a substring that 
can Ix reduced to a. If a consists entirely of terminals, then it looks for this 
string of terminals on the input, and ''reduces" them by shifting them onto the 
stack. At this point, the parser will have ermr rr on tap OF its stack. The 
parser will then reduce e m r  a to A, and resume normal parsing. 
For example, an error production of the form 
would specify to the parser that it should skip just beyond the next semicolon 
on seeing an error, and assume that a statement had been found. The seman- 
tic routine for this error production would not need to manipulate the input, 
but could generate a diagnostic message and set a flag to inhibit generation of 
object code, for example. 
Example 4S2. Figure 4.59 shows the Y acc desk alculator of Fig. 4.57 with 
the error pruduct ion 
lines : error "w' 
This error production causes the desk calculator to suspend normal parsing 
when a syntax error is found on an input line. On encountering the error, the 

%I 
#fnclude qctype . h> 
#include eatdio*hz 
#define WSTYPE double /* double type for Yacc stack */ 
94 1 
%token NUMBER 
%left '+' 
%left ' * *  '/' 
%right UMINUS 
%% 
lines 
expr 
%% 
lines expr 'h* { pr5ntf("%g\nn, $2); 1 
l i n e s  "m' 
/* empty 
error 'In' { yyerrortMreenter last line:"); 
yytrrok; 1 
txpr '+' 
expr 
{ SS = $1 + $3; 1 
expr * - *  expr 
{ SL = $+l 
- $3; 1 
expr ' *' expr 
I $$ - $1 * $3; 1 
expr '/' expr 
( $$ = $1 / $3; 1 
' [ '  expr ' I '  
{ W = 92; 1 
'-* 
expr 
%prec UMINUS 
I 
$$ = - $2; 
NUMBMC 
Fig. 4.59. Desk calculator with error recovery. 
parser in the desk calculator starts popping symbols from its stack until it 
encounters a state that has a shift adbrt on the tokcn error+ State O is such a 
state (in this example, it's the only such state), since its items include 
Also, state 0 is ahay; on the bottom of the slack. The parser shifts the token 
e m r  onto the stack, and then proceeds to skip ahead in the input until it has 
found a newline character. At this point the parser shifts the newline onto the 
stack, reduces mw 'h' to lines, and emits the diagnostic message "reenter 
last line:''. The special Yacc routine yyerrok resets the parser to its normal 
mode of operation. 
a 

I 
CHAPTER 4 
EXERCISES 
4.1 Consider the grammar 
a) What are the terminals, nonterminals, and start symbl? 
b) Find par= trees for the following sentences: 
0 
( 0 .  a) 
ii) (a, la, 0 ) )  
iiil (a, 
{a7 dH 
C) Construct a leftmost derivation for tach of the sentences in (b). 
d) Construd a rightmost derivation for each of the senlences in Ib), 
*e) What language does this grammar generate? 
/ 
4.2 Consider the grammar 
a) Show that this grammar is ambiguous by wnstructing two different 
lcftmost derivations for the sentence ah&. 
b) Construct the corresponding rightmost derivations for a h b .  
C) Construct the corresponding parse trees for ubub. 
*d) What ,language does this grammar generate'! 
4.3 Consider the grammar 
a) Conshuct a parse tree for the sentence not (tmt ar f d ~ ) .  
b) Show that this grammar generates a11 h l e a n  expressions, 
*c) is this grammar ambiguous? W by? 
4-4 Consider the grammar 
Note that the first vertical bar is the '+or" symbl, not a separator 
between alternatives. 
a) SOHI 
that this grammar generates all regular expressions over 1he 
symbols a and B+ 
b} Show that this grammar is ambiguous. 
*c) Construct an equivalent unambiguous grammar that gives the 
operators *, concatenation, and I the precedences and associativi- 
ties defined in Section 3.3. 
d) Construct a parse tree in both grammars for the senten= a Ib*c. 

268 
SYNTAX ANALYSIS 
CHAPTER 4 
4.5 The following grammar for if-then=elst statements is proposed to 
remedy the dangling-else ambiguity: 
Show that this grammar is still ambiguous. 
*4*6 Try to design a grammar for each of the fdlowing languages, Which 
languages are regular'? 
a) The str of all strings of 0's and 1's such that every O is immedi- 
atdy followed by at least one 1, 
b) Strings of 0's and 1's with an equal number of 0's and 1's. 
c) Strings of O's and 1 's with an unequal number of O's and I 's. 
d) Strings of O's and 1's in which 01 1 does not appear aii a substring. 
e) Strings of 0's and 1 's of the form xy where x + y. 
f) Strings of O*s and 1's of the form a. 
4.7 Construct a grammar for the expressions of each of the following 
languages. 
a) Pascal 
b) C 
cl Fortran 77 
d) Ada 
e) Lisp 
4.8 Construct unambiguous grammars for the statements in each of the 
languages of Exwcix 4.7. 
, 4.9 We can use regular~xpression-like operators in the right sides of 
grammar produc~ions. Square brackets can k 
used to denote an 
optional part of a production. For example, we might write 
to denote an optional else-statement. In general, A -. a [ 
1 y i s  
equivalent to the two productions A -. aPr and A + ay. 
Curly brackets can be used to dense r phrase that can be repeated 
zero or more times. Fw example, 
denotes a list of semimlqn-separated stmt's enclosed between begir 
and end. In general, A + a { i3 ) y is equivalent to A - a8 y and 
B +PB I r .  
In a sense, I p 1 stands for the regular cxpresion P I E, and { P) 
stands for $*, We may generalize these notations to dlow any 

CHAPTER 4 
EXERCISES 269 
regular expression of grammar symbols on the right side of prabuc- 
tions. 
a) Modify the above smt-production so that a semicolon-terminated 
list of stmi's appears on the right side. 
b) Give a set of context-free productions generating the same set of 
strings as A + B*a(C ID). 
c) Show how to replace any production A -, r, where r is a regular 
expression, by a finite mlhtion of context-free productions, 
4.10 The following grammar generates declarations for a single identifier: 
a) Show how this grammar can be generalized to pxmit n options Ai, 
I I 
i 5 n, each of which can be eirher ai or bi. 
b) The above grammar permits redundant or contradictory declara- 
tions such as 
declare zap real fixed real floating 
We could insist that the syntax of the language forbid such 
declarations. 
We are thus left with a finite number of token 
sequences that are syntactically correct. Obviously these legal 
declarations form a context-free language, indeed a regular set. 
Write a grammar for declarations with n options, each option 
appearing at most once. 
*%I 
Show that a grammar for part (b) must have at least 2" symbols. 
d) Whal does {c) say about the feasibility of enforcing nonredun- 
dancy and noncontradiction among options In declarations via the 
syntactic definition of a language? 
a) Eliminale the left-recursion from the grammar in Exercise 4. I. 
b) ConHruct a predictive parser for the grammar in (a)+ Show the 
behavior of the parser on the sentences in Errercise 4. L C  b). 
Construct a recursive-descent parser with backtracking fo~. the gram- 
mar in Exercise 4.2. Can you construcl a predictive parser for this 
grammar? 
The grammar 
S - crsu I ua 
generates all even length strings of a's except for the empty string, 

270 
SYNTAX ANALYSIS 
CHAPTER 4 
a) Construct a recursivedescent parser with backtracking for this 
grammar that tries the alternative aSa before m. Show that rhe 
procedure for S succeeds on 2+4, or 8 a's, but fails on 6 a's. 
*b) What language does your parser recognize? 
4.14 Construct a predictive parser for the grammar in Exercise 4,3. 
4,15 Construct a predictive parser from the unambiguous grammar for reg- 
ular expressions in Exercise 4.4, 
*4,16 Show that no left-recursive grammar can be tL[ I ) ,  
4.18 Show that a grammar with no E-productions in which each alternative 
begins with a dislinct terminal is always LLI t ). 
4.19 A frammar symbol X is uselers if there is no derivation of the farm 
S 3 
wXy % w q +  That is, X can never appear in the derivation of 
some sentence. 
*a) Write an algorithm to eliminate all productions containing useless 
symbols from a grammar. 
b) Apply your algorithm to the grammar 
4,20 We say a grammar is r-free if either it has no e-productions or there 
is exactly one E-production S -. r and then the start symlml S does not 
appear on the right side of any production. 
a) Write an algorithm to convert a given grammar into an equivalent 
efree grammar. Hint, First determine all nunterminals that can 
generate the empty string. 
b) Apply your algorithm to the grammar in Exercise 4.2. 
4 
A single pruductwn is one with a single nonterminal as its right side. 
a) Write an algorithm to convert a grammar into an equivalent gram- 
mar with no single productions. 
b) Apply your algorithm to the expression grammar (4.10). 
4.22 A cycle-fife grammar has m, derivations of the form A h 
A for any 
nonterminal A .  
a) Write an algorithm to convert a grammar into an equivalent 
cycle-free grammar. 
b) Apply your algorithm to the grammar S -. SS ) ( S )  I E. 
4+23 a) Using the grammar in Exercise 4.1 construct a rightmost deriva- ' 
tion for ( 0 ,  (a, a)) and show the handle of each right-sententiai 
form. 

CHAPTER 4 
EXERCISES 27 1 
b) Show the steps of a shift-reduce parser corresponding to the right- 
most derivation of (a). 
c) Show the steps in the bottom-up construction of a parse tree dur- 
ing the shift-reduce parse of (b)+ 
4.24 Figure 4.m shows operator-precedence relations for the grammar of 
Exercise 4-1. Using rhese precedence relations, parse the sentences in 
Fig. 4,@, Operator-prcccdcncc rctatims fm thc grrmmar ijf E~crcisc 4.1. 
4-25 Find operator-precedence functions for the table of FigA 4.60. 
4.26 There is a mechanical way to produce operator-precedence relations 
from an operator grammar, including those with many different non- 
terminals. Define kudjng (A) for nonterminal A to be the set of ter- 
minals u such that a i s  the leftmost terminal in mme string derived 
from A, and define rrding ( A  ) to be the set of terminals that can be 
the rightmost in a string derived from A, Then for terminals a and b, 
we say cr 
b if there is a right side of the form aufiby, where p is 
either empty or a single nonterminal. and a and y are arbitrary. We 
say u <. b if there Is a right side of the form uaAf$, and b is in 
Ieuding ( A ) ;  we say u .> 6 if there is a right side of the form aAbp. 
and rr is in rrditt#(A). 
in both cases. a and P arc arbitrary strings. 
Alsu. $ <-b whenever b is in hudisr#(S)~ where S is the start symbl, 
and u .> $ whenever o is in t r d i n g ( S ) .  
a) For the grammar of Exercise 4.1 compute leuding and truilirrg for 
S and T. 
b) Verify [hat the precedence relations of Figure 4.60 are the ones 
derived from this grammar. 
4.27 Generate uperator-precedence relations for the following grammars. 
a) The grammar of Exercise 4.2. 
b) The grammar of Exercise 4,3. 
G) The expression grammar (4.10). 
4-28 Construct an fiperator-precedence parser for regular expressions. 

272 
SYNTAX AKALYSIS 
CHARER',~ 
4.29 A grammar is said to be a (uniquely invertible) operator-precedence 
grammar if it i s  an operator grammar with no two right sides that 
have the same pattern of terminals. and the method of Exercise 4.26 
yields at most one precedence relation between any pair of terminals. 
Which of the grammars of Exercise 4.27 are operator-precedence 
grammars? 
4+30 A grammar is said to be in Greibach normal form (GNF) is it is €-free 
and each production (except S -. E if it exists) is of the form A 
-+ atu , 
where a is a terminal and a is a string of nonterminals, possibly 
empty + 
**a) Write an algorithm to convert a grammar into an equivalent 
Grebach normal form grammar. 
bl Apply your algorithm to the expression grammar (4. lo), 
*4.31 Show that every grammar can be converted into an equivalent opera- 
tor grammar. Hifir. First transform the grammar into Greibach nor- 
mal form. 
'4.32 
Show that every grammar can be converted into an operator grammar 
in which each production is of one of the forms 
If r is in the language, then S + t is also a production. 
4,33 Consider the ambiguous grammar 
a) Construct the collection of sets of LRIO) items for this grammar. 
b) Construct an NFA in which each state is an LR(Ol item from (a). 
Show that the goto graph of the canonical mllection of LR(0) 
items for this grammar is the same as rhe DFA constructed from 
the NFA using the subset construction. 
C) Construct the parsing table using the SLR Algorithm 4.8. 
d) Show all moves allowed by the table from (c) on the input abab. 
e) Construct the canonical parsing table. 
f) Construct the parsing table using the LALR Algorithm 4.1 1. 
g) Construct the parsing table using the LALR Algorithm 4.13, 
434 Construct an SLR parsing table for the grammar in Exercise 4.3. 
4+35 Consider the following grammar 
E - E + T  I T 
T + T F  I F 
F - F *  
I o 1 B 

CHAPTER 4 
EXERCISE5 273 
a) Construct the SLR parsing tabb for this grammar. 
b) Construct the LALR parsing tabk. 
4 , s  Compact the parsing tables constructed in Exercises 4.33, 4.34, and 
4,35, according to the method of M i o n  4.7. 
4.37 a) %ow that !he fallowing grammar 
is LL(1) but not SLR(1) + 
**b) Show that every LL(1) grammar is an LR(L) grammar. 
*4.33 Show that no LR( 1) grammar can be ambiguous. 
4.39 Show that the folbwing grammar 
is LALR( 1) but not StR(1). 
4.40 Show that the following grammar 
is LR( I )  but not LALR( 1). 
*4.41 Consider the family of grammars G, defined by: 
S + Aibi 
I S ~ S J #  
A, - #,Ai I ui 
1 L i, j d n and j # i 
a) Show that G. has h 2 - n  produdions and 2' -+ n2 f n sets of 
LR(0) items. What does this result say about how big an LR 
parser can get compared to the size af the grammar? 
b) 1s G, SLR( I )? 
C) is G, LALRC l)? 
4.42 Write an algorithm ti, compute for each nonterrninal A in a grammar 
the set of nonterminals B such thu A h B a  for some string of gram- 
mar symbols a. 
4,43 Write an algorithm to compute for each nonterminal A in a grammar 
the set of terminals o such that A %ow for yome wing of terminals 
w, where the last step of the derivation 'does not use an *-production. 
4.44 Construct an SLR parsing table for the grammar of Exercise 4,4. 
Resolve the parsing action conflicts in such a way that r e p  lar expres- 
sions will be parsed normally. 

274 
SYNTAX ANALYSIS 
CHAPTER 4 
4.45 Construct an SLR parser for the dangling-else grammar (4.71, treating 
expr as a terminal. Resdve the parsing action conflict in the usual 
way. 
4-d6 a) Construct an SLR parsing table for the grammar 
Resolve the parsing action conflict so that expressions will be 
parsed in the same way as by the LR parser in Fig. 4.52. 
b) Can every reduceireduce conflict generated in the LR parsing table 
construction process be converted into a shifttreduce conflict by 
transforming the grammar'? 
*4.47 Construcl an equivalent LR grammar for the typesetting grammar 
(4.25) that factors out expressions of the form E sub E sup E as a 
special case. 
*4& 
Consider the following ambiguous grammar for n binary infix opera- 
tors: 
# 
Assume that all operators are left-associative and that 0, takes pre- 
cedence over O j  if i > j .  
a) Conslruct the SLR sets of items for this grammar. Haw many sets 
of items are there, as a function of n? 
b) Construct the SLR parsing table for this grammar and compact it 
using the list representation in Section 4.7. What is the total 
length of ali the lists used in the representation, as a function of 
n ? 
c) HOW many steps does it take to parse id Oi id 0, id? 
'4.49 
Repeat Exercise 4.48 for the unambiguous grammar 
What do your answers to Exercises 4.48 and 4+49 say abut the rela- 
tive efficiency OF parsers for equivalent ambiguous and unambiguous 
grammars? What about the relative efficiency of constructing the 
parser? 
4
9
 Write a Yacc program that will take arithmetic expressions as input 
md produce the corresponding pstfix expression as output. 

CHAPTER 4 
EXERCISES 275 
4.51 Wrtte a Yacc "desk calculator" program that will evaluate holean 
expressions. 
4.52 
Write a Yacc program that will take a regular expression as input and 
produce its parsetree as output. 
4 3 3  Trace out the moves that would be made by the predictive, operator- 
precedence, and LR parsers of Examples 4.20, 4.32, and 4.50 on the 
following erroneous inputs: 
a) ( M  + ( * i d )  
b) $ + id) + ( id * 
'4.54 
Construct error-correct ing operator-precedence and LR parsers for the 
following grammar: 
*4.55 The grammar in Exercise 4.54 can be made LL by replacing the pro- 
ductions for list by 
Construct an error-correcting predictive parser for the revised grarn- 
mar. 
4.56 
Show the behavior of your parsers of Exercises 4.54 and 4.55 on the 
erroneous inputs 
a) if e then s ; if e then s end 
b) while e do begin s ; if e then s ; end 
4.57 Write predictive, operator-precedence, and LR parsers with psnic- 
mode error recovery for the grammars of Exercises 4.54 and 4.55, 
using semicolon and end as synchronizing tokens. Show the &havim 
of your parsers on the erroneous inputs of Exercise 4.56. 
4.58 In Section 4+6, we proposed a graph-oriented method for determining 
the set of strings that could be popped from the nack in a reduce 
move of an operator-precedence parser. 
*a) Give an algorithm For finding a regular expression denoting all 
such strings. 
b) Give an algorithm to determine whether the set of such strings is 
finite w infinite, listing them if finite. 

276 
SYNTAX ANALYSIS 
CHAPrER 4 
c) Apply your algorithms from (a) and (b) to the grammar of Exer- 
cise 4-54, 
We made the claim for the error-correcting parsers of Figs. 4.18, 
4-28, and 4+53 that any error correction eventually resulted in at least 
one more symbol being removed from the input or the stack being 
shortened If the end of the input has ken reached. The corrections 
chosen, however, did not all cause an inpltt symbol to be consumed 
immediately. Can you prove that no infinite loops are possible for the 
parsers of Figs. 4.18, 4.28, and 4.53') Hint. It helps to observe that 
for the operator-precedence parser, consecutive terminals on the stack 
are related by s., 
even if there have been errors.  or the LR parser, 
the stack will still contain a viable prefix, even in the presence of 
errors, 
Give an algorithm for detecting unreachable entries in predictive, 
apator-precedence, and LR parsing tables. 
The LR parser of Fig. 4.53 handles the four situations in which the 
top state is 4 or 5 (which occur when + and * are on top of the 
stack, respectively) and the next input is + or * in exactly the same 
way: by calling the routine el, which inserts an id between them. We 
could easily envision an LR parser for expres;~ions involving the full 
set of arithmetic operators behaving in the same fashion: insert M 
between the adjacent operators. In certain languages (such as PWI or 
C but not Fortran or Pascal), it would k wise to treat, in a special 
way, the case in which 1 is on top of the stack and rk is the next input. 
Why? What would be a reasonable course of action for the error 
corrector to take'? 
4.62 A grammar is said to be in Chmsky normulfurm (CNP) if it is r-free 
and each non-c-production is of the form A 
BC or of the form 
A +a. 
*a) Give an algorithm to convert a grammar into an equivalent Chom- 
sky normal form grammar. 
b) Apply your algorithm to the expression grammar (4+10), 
4,63 Given a grammar G in Chomsky normal form and an input string w 
= ula2 + 
4 
u,,, write an algorithm to determine whether w is in 
L (GI. Hint. Using dynamic programming fill in an n x n [able T in 
which Tli, j I = {A I A 
aiq +, . - - u,}. 
The- input string w is in 
L(G) if and only if S is in T I ] ,  n]+ 
*4.64 a) Given a Chomsky normal form grammar G ,  show how to add pro- 
ductions for single insertion, deletion, and mutation errors to the 
grammar so that the enlarged grammar generates all possible token 
hirings. 

BIBLIOGRAPHIC NOTES 277 
Modify the parsing algorithm in Exercise 4.63 so that, given any 
string w ,  it will find a parse for w that uses the fewest number of 
error productions. 
4.65 Write a Yacc parser for arithmetic expressions that uses the error- 
recovery mechanism in Example 4.50. 
BIBLIOGRAPHIC NOTES 
The highly influential Algol 60 report ( N w r  119631) used Backus-Naur Form 
(BNF) to define the syntax of a major programming language. 
The 
equivalence of BNF and context-free grammars was quickly noted, and the 
theory of formal languages received a great deal of attention in the 1960's. 
Hopcroft and Ullman 119791 cover the basics of the field. 
Parsing methods became much more systematic after the development of 
context-free grammars. Several general techniques for parsing any context- 
free grammar were invented. One of the earliest is [he dynamic programming 
technique suggested in Exercise 4.63, which was independently discovered by 
J. Cocke, Younger I19671, and Kasarni 119651. As his Ph+ D+ 
thesis, Earky 
1 19701 also developed a universal parsing algorithm for all content-free gram- 
mars. Aho and Utlman [ 1972b and 1973aj discuss these and other parsing 
methods in detail. 
Many different parsing methods have been employed in compilers. Shcri- 
dan 119591 describes the parsing methcd used in the original Fortran compikr 
that introduced additional parentheses around operands in order to lx able to 
parse expressions. The idea of operator precedence and the use of precedence 
functions is from Floyd 119631. In the I W s ,  a large number of bottom-up 
parsing strategies were proposed, These include simple precedence (Wirth 
and W e k r  1 1%6]), bounded-context I Floyd 1 1 W1, Graham [ 1964]), mixed- 
strategy precedence (McKeeman, Horning, and Wortman 119701), and weak 
precedence (Ichbiah and Morse 1 19701). 
Recursi ve-descent and predict ivc parsing are widely used in practice. 
Because of its flexibility, recursive-descent parsing was used in many early 
compiler-writing systems such as META (Schorre 1 19641) and TMG ( McClure 
[1965]). A solution to Exercise 4.13 can be found in Birrnan and Ullrnan 
119731, along with some of the theory of this parsing method. Pratt 119731 
proposes a topdown operator-precedence parsing method. 
LL grammars were st~died by Lewis and Stearns [ 1%81 and their properties 
were developed in Rosenkrantz and Stearns 1 lWO1. Predictive parsers were 
studied extensively by ~ n u t h  1 197 la). 
Lewis, Rosentrantz, and Slesrns 
(19761 describe the use of predictive parsers in compilers. Algorithms for 
transforming grammars into LL( I) form are presented in Foster [1%81, Wood 
119691, Steams 119711, and Soisalon-Soininen and Ukkonen 119791. 
LR grammars and parsers were first introduced by Knuth 119651 who 
described the construction of canonical LR parsing tables. The LR method 
was not deemed practical until Korenjak 11%91 showed that with 
it 

278 
SYNTAX ANALYSIS 
CHAPTER 4 
reasonable-sized parsers could be produced for p~ograrnmi ng language gram- 
mars. When DeRemer 11969, 19711 devised the SLR and LALR methods, 
which are simpler than Korenjak's, the LR technique became the method of 
choice for *automatic parser generators. Today, LR parer generators are 
common in compiler-construct ion environments. 
A great deal of research went into the engineering of t R  parsers. The use 
of ambiguous grammars in LR parsing is due to A h a  Johnson, and Ullman 
[I9751 and Earley 11975al. The elimination of reductions by single produc- 
tions has been discussed in Anderson, Eve, and Horning [ 19731, Aho and Ull- 
man 11973b1, Derners 11 975 1, Backhouse 1 19761, loliat 119761, Pager 1 1977b1, 
Soisaian-Soininen I 19801, and Tokuda 1 198 1 1, 
Techniques for computing LALR( I I lookahead sels have been proposed by 
LaLonde 11971 1, Anderson, Eve, and Horning 119731, Pager 11977al, Kristen- 
sen and Madsen 1 198 1 j, DeRemer and Pennello [ 19821, and Park, Choe, and 
Chang j 19851 who also provide some experimental comparisons. 
Aha and Johnson ( 19741 give a general survey of LR parsing and discuss 
some of the algorithms underiying the Yam parser generator, including the 
use of error productions for error recoyery. Aha and Ullman 11872b and 
L973aj give an extensive treatmen1 of LR parsing and its theoretical underpin- 
nings. 
Many error-recovery techniques for parsers have &en proposed. Error- 
recovery techniques are surveyed by Ciesingcr ( 19791 and by Sippu 11981 1. 
Irons 11%3] proposed a grammar-based approach to syntactic error recovery. 
Error productions were employed by Wirth 1I968j for handling errors In a 
PL360 compiler. 
Leinius 1 l9fO] propwd the strategy of phrase-level 
recovery. Aho and Peterson 1 19721 show how global' least-cost error recovery 
can be achieved using error product ions in con~unction with general parsing 
algorithms for context-free grammars. Mauney and Fischer (19821 extend 
thew ideas to local least-cost repair for LL and LR parsers using the parsing 
technique of Graham, Harrison, and Ruzzo 119801. Graham and Rhdes 
1 1975 1 discuss error recovery in the context of precedence parsing. 
Horning 11 9763 discusses qualitks good error messages shw Id have. Sippu 
and %isalon&ininen 
119831 compare the performance of the error-recovery 
technique in the Helsinki Language Processor (Raihi et al. 119831) with the 
"forward move" recovery technique of Pennello and DeRemer 119781, the LR 
error-recovery technique of Graham, Haley, and Joy 119791, and the "global 
context'' recovery technique of Pai and Kieburtz 1 l98Oj. 
Error correction during parsing is discussed by Conway and Maxwell 
Il%3j,, Moulton and Muller 119671, Conway and Wilcox IW731, Levy 19751, 
Tai 1b9781, and Rhhrich 119801. Ahu and Peterson 1l972] contains a solution 
to Exercise 4.63, 

CHAPTER 5 
Syntax-Directed 
Translation 
This chapter develops the theme qf Section 2.3, the translation of languages 
guided by context-free grammars. We associate information with a program- 
ming language construct by attaching attributes to the grammar symbols 
representing the construct. Values for attributes are computed by "semantic 
rules" associated with the grammar productions. 
There are two ndat ions for associating semantic ruks with productions, 
syntaxdirected definitions and translation schemes. Syntax-directed defini- 
tions are high-level specifications for translations. They hide many implernen- 
tation details and free the user from having to spcify explicitly the order in 
which translation takes place. Translation schemes indicate the order in which 
semantic rules are to be evaluated, so they allow some implementation details 
rn be shown. We use both notations in Chapter 6 For specifying semantic 
checking, particularly the determination of types, and in Chapter 8 for gen- 
erating intermediate code. 
Conceptually. with both syntax-directed definitions and t ransbt ion schemes, 
we parse the input token stream, build the parse tree, and then traverse the 
rree as needed to evaluate the semantic rules at the ?arse-tree nodes (see Fig. 
5.1). Evaluation of the semantic rules may generate code, saw information in 
a symbl table, issue error messages, or perform any other activitiesA The 
translation or the token stream is the result obtained by evaluating the seman- 
tic rules, 
input 
parsc 
dcpcndcncy 
cvaluatiun ordcr 
- - - 
string 
trw 
graph 
for wmant ic rulcs 
Fig. 5.1. Conceptual vicw of syntax4 ircclcd translat inn, 
An implementation does not have to follow the outline in Fig. 5.1 literally. 
Special cases of syntax-directed definitions can be implemented in a single pass 
by evaluating semantic rules during parsing, without explicitly constructing a 
parse tree or a graph showing dependencies between attributes. Since single- 
pass implementation is important for compileti me efficiency. much d this 

280 SYNTAXwDIRECTED TRANSLATION 
SEC. 5.1 
chapter is devoted to studying such spcial cases. One important subclass, 
called the "L-attributed " definitions, encompasses virtually all translations 
that can be performed without explicit oonstruaioa of a parse tree. 
5.1 SY NTAX-DIRECTED DEFINlTlONS 
A syntax-directed definition is a generalization of a context-free grammar in 
which each grammar symbol has an associated set of attributes, partitioned 
into two subsets called the synthesized and inherited attributes of that gram- 
mar symbl. If we think of a node for the grammar symbd in a parse tree as 
a remd with fields for holding information, then an attribute corresponds to 
the name of a kZd. 
An attribute can represent anything we choose: a string, a number, a type, a 
memory location, or whatever. The value of an attribute at a parse-tree node 
is defined by a semaniic ruk associated with the prducth used at that node. 
The value of a synthesized attribute at a node is computed from the values of 
attributes at the children of that n d e  in the parse tree; the value of an inber- 
i t 4  aqtributc Is computed from the values of attributes at the siblings and 
parent oC that node+ 
Semantic rules *t 
up dependencies between attributes that will be 
represented by a graph. From the dependency graph, we derive an evaluation 
order for the semantic rules, Evatuation of the semantic rules defines the 
values of the attributes at the nudes in the parse tree for the input string. A 
semantic rule may alw have side effects, e.g., printing a value or updating a 
global variable. Of course, an implementation need not explicitly construct a 
parse tree or a dependency graph; it just has to produce the same output for 
each input string. 
A parse tree showing the values of attributes at each nude i s  called an anno- 
tated parse tree. The prooess of computing the attribute values at the nodes is 
called anmiuting or decora~irrg the parse tree. 
Jn a syntaxdirected definition, each grammar production A -. a has associ- 
ated with it a set of semantic rules of the form b := f ( c , ,  cz, . . . , c*) where 
f is a function, and either 
1. 
b is a synthesized attribute of A and c ~ , E ~ , . . . , c ~  
arc attributes 
belonging to the grammar symbls of the production, or 
2. b i s  an inherited attribute of one of the grammar symbols on the right 
side of the production, and c , c2, . . . , ck are attributes klonging to 
the grammar symbls of the production. 
In either wm, we say that-attribute b depends on attributes c , ,  czt . . . ct. 
An attrihre gramrtsar is a syntaxdirected definition in which the functions in 
semantic rules cannot have side effects. 

SEC. 5.1 
SY NTAX-DIRECTED DEFINITIONS 281 
Functions in semantic rules will often be written as expressions. Occasion- 
ally, the only purpose of a semantic rule in a syntax-directed definition is to 
create a side effect. Such semantic rules are written as procedure calls ar pro- 
gram f~agments. Tbey can be thought of as rules defining the values of 
dummy synthesized attributes of the nonkrminal on the left side of the ~ S S Q C ~ -  
ated production; the dummy attribute and the := sign in the semantic rule are 
not shown. 
Example 5.1. 
The syntax-directed definition in Fig. 5.2 is for a desk- 
calculator program, This definition associates an integer-valued synthesized 
attribute called vaI with each of the nmterminals E, T, and F. For each E, T ,  
and F-production. the semantic rule computes the value of attribute vnl for the 
nonterminal on the left side from the values of v d  for the nmtcrminals on the 
right side. 
L - E n  
E d E l  + T  
E A T  
T + T l + F  
T d F  
F + ( E )  
F + digit 
fig. 5,2. Syntax-directed definition of a simplc dcsk calculator 
The token digit has a synthesized attribute !em/ whose value is assumed to 
Be supplied by the kxical analyzer. The rule associated with the production 
L -t E n for the starting nonterminal L is just a p r d u r e  that prints as output 
the value of the arithmetic expression generated by E; we can think of this 
rule as defining a dummy attribute for the nanterminal L. A Y acc specifica- 
tion for this desk calculator was presented in Fig. 4-56 to illustrate translation 
during LR parsing. 
In a syntax-directed definition, te~minals are assumed to have synthesized 
attributes only, as the definition does not provide any semantic rules fix ter- 
minals. Values for attribules of terminals are usually supplied by the lexical 
analyzer, as discussed in Section 3.1. 
Furthermore. the start symbol is 
assumed not to have any inherited at tributes. unless at herwise stated. 
Sy nthmized Attributes 
Synthesized attributes are used extensively in practice. 
A syntax-directed 
definition that uses synthesized attributes exclusively is said to be an S- 
artribtlteb dejlnition. A p a w  tree for an Sattributed definition can always be 
annotated by evaluating the semantic rules for the attributes at each node 

bttom up, from the leaves to the root. Section 5+3 describes how an LR- 
parser generator can be adapted to mechanically implement an S-attributed 
definition based on an LR grammar. 
Example 5.2. The S-attributed definition in Example 5.2 specifies a desk cal- 
culator that reads an input line containing an arithmetic expression involving 
digits, parentheses, the operators + and *, followed by a newline character n, 
and prints the value of the expression. For e~ample, given the expression 
3+5+4 fdlowed by a newline, the program prints the value i9. Figure 5.3 
contains an annotated parse tree for the input 3+5+4n. The output, printed 
at the root of the tree, is the value of E.vai at the first child of the root. 
L 
1
'
 
n 
E.vuE = 19 
E.vd - 15 I
\
 
+ 
T,vd = 4 
1 
T . v d  = 15 
I 
F.vd = 4 
T.w/ = 3 
1
1
 
+ 
F. 
v d  = 5 
I 
digitkxvd = 4 
I 
1 
F.vd = 3 
digit. /xvai = 5 
1 
digit. k x v d  = 3 
Flg. 5.3. Annotated parse trec for 3* 5+4 n. 
To see how attribute values are computed, consider the icftrnost bttummost 
interior node, whish corresponds to the use of the product ion F + dl&. The 
corresponding semantic rule, F.vu€ := digit.iexuai, defines the attribute F. val 
at that node to have the value 3 because the value of digit.bexual at the child 
of this node is 3. Similarly, at the parent of this F-node, the attribute T.vu1 
bas the value 3. 
Now consider the node for the produdion T -. T + F. The value of the 
attribule T.va/ at this node is defined by 
When we apply the semantic rule at this node. T i  . v d  has the value 3 from the 
left child and F.vd the value 5 from the right child. Thus, T-vd acquires the 
value 15 at this node, 
The rule associated with the production for the starting nontermioal 
L + En prints the value of the expression generated by E. 
o 

SY NTAX-DIRECTED DEANLTLONS 283 
Inkited Attributes 
An inherited attribute is one whose value at a node in a parse tree is defined 
in terms of attributes at the parent andior siblings of that n d e .  Inherited 
attributes are convenient for expressing the dependence of a programming 
Language construct on the context in which it appears. For example, we can 
use 'an inherited attribute to keep track of whether an identifier appears on the 
left or right side of an assignment in order to decide whether the address or 
the value ot the identifier i s  needed. Although it is atways possible to rewrite 
a syntax-directed definition to use only synthesized attributes, it is often more 
natural lo use synlaxdirected definitions with inherited attributes. 
In the following example, an inherited attribute distributes type information 
to the various identifiers in a declaration. 
Example 5.3, A declaration generated by the nmterminal D in the syntax- 
directed definition in Fig. 5.4 consists of the keyword int or mI, followed by 
a list of identifiers, The noncerminal T has a synthesized attribute type, whose 
value is determined by the keyword in the declaration. The semantic ruIe 
Lin := T.rype, associated with production D -, TL, sels inherited attribute 
L.in to the type in the dedaralion. The rules then pass this type down the 
parse tree using the inherited attribute L i n .  Rules associated with the prduc- 
tions for L call procedure addfypt to add the type of each identifier to its entry 
in the symbol table {pointed to by attribute enfry). 
i 
Fig. 54. Syntax-directcd definition with inherited attribute L-itt. 
Figure 
5.5 
shows 
an 
annotated 
parse 
tree 
for 
the 
sentence 
real id, , id2 ,idj. The value of L.in at the three L-nodes gives the type of 
the identifiers id,, idz, and id3. These values arc determined by computing 
the value of the attribute T.type at the left child of the root and then evaluat- 
ing L+in top-dowrt at the three Lnodes in the right subtree of the root- At 
each L-node we also call the procedure oddtype to insert into the symbd table 
the fact that the identifier at the right child of this node has type real, 
o 

284 
SYNTAX-DIRECTED TRANSLATION 
Fig, 5.5. 
Parse trcc with inherited attribute in at cach node labeled L. 
Dependency Graphs 
If an attribute b at a node in a parse tree depends on an attribute c, then the 
semantic rule for b at ti& 
node must be evaluated after the semantic rule that 
defines c. The interdependencies among the inherited and synthesized attri- 
butes at the nodes in a parse tree can be depicted by a directed graph called a 
bependewy graph, 
Before constructing a dependency graph for a parse tree, we put each 
semantic rule into the form b := f ( c ,  , cz, + . . , q), by introducing a dummy 
synthesized attribute b for each semantic rule that consists of a procedure call. 
The graph has a node for each attribute and an edge to the node for b from 
the node for c if attribute b depends on attribute c, In more detail, the depen- 
dency graph for a given parse tree is constructed as follows. 
lor cach node r, in thc parse lree do 
fw each attribute u of thc grammar symbol at n do 
construct a node in the dependency graph for a; 
for each node tt in the parsc tree do 
far cach semantic rule b :- SIC,, 
c*, . . . . c,) 
associated with the production u
d
 
at n do 
iori := 1 tokdo 
construcr an edge from thc n d c  for r, to the node for k 
For example, suppose A7a := f (X,x, Y,y) is a semantic rule for the product 
tion A + XY. Thk rule defines a synthesized attribute A.u that depnds on the 
attributes X.x and Y.y. If this production is used in the parse tree, then there 
will be three nodes A.a, X.x, and Y.y in the dependency graph with an edge to 
A.a from X.x sine A.a depends on X.x, and an edge to A+a from 7.y since A.o 
also depends on Y.y. 
If the production A 
XY has the semantic rule X.i := g(A.rr, Y.y) associ- 
ated with it, then there will be an edge to X,i from A.u and also an edge to X.i 
from Y.y, since X-i depends on both A.u and Y-y. 

SEC. 5.1 
SY NTA X-DIRECTED EEFlNlTlONS 285 
Exampie 5.4. Whenever the following production is used in a parse tree, we 
add the edges shown in Fig. 5.6 to the dependency graph. 
, 
The three nodes of the dependency graph marked by 
represent the syn- 
thesized attributes E . v d ,  E l  . v d ,  and Ez.vd at the corresponding n d e s  in the 
parse tree. The edge to E . v d  from E I . VH! shows that E . v d  depends on E .vat 
and the edge to E.vd from Ez7va/ shows that E.vd also depends on E 2 . v d .  
The dotted lints represent thc parse tree and are not part of the dependency 
graph. 
12 
Fig. 5.6, E . v ~ l  is synthesized from E , . v d  and Ez.vul. 
Example 5.5, 
Figure 5.7 shows the dependency graph for the parse tree in 
Fig. 5.5. Nodes in the dependency graphs are marked by numbers; these 
numbers will be used below, There is an cdge to node 5 for L i n  from node 4 
for T.fype because the inherited attribute L.irt depends on the attribute T a m e  
according to the semantic rule L.in ;= T . ~ p r  
for the production D -TL. The 
two downward edges into nodes 7 and 9 arise because Ll.in depends on Lirr 
according to the semantic rule Ll.in := L.in for rhc production L - L ,  , M. 
Each of the semantic rules addtypr(id.mrry, L h )  associated with the L- 
pmductiuhs leads iu the ereahn of a dummy allribute. Nodes 6, 8, and 10 
are constructed for [hex dummy attributes. 
0 
Evaluation Order 
A 
i
i
 
o r  of 
a 
directed 
acyclic graph 
is 
any 
ordering 
ml, m2. . . . , ml;. of the nodes of the graph such thal edges go from nodes 
earlier in the ordering tu later nodes: that is, if mi - mi is an cdge from m, to 
mj, then mi appears before m, in the ordering. 
Any topological sort of a dependency graph gives a valid order in which the 
semantic ruies associated with the ndes in a parse tree can be evaluated. 
That is, in the topological sort, the depertdent artributes r.,, cz, . . . , ck in a 
semantic rule b :- ,f ( c  , , c I ,  + . . , C r )  are available at a node before j is 
evaluated. 
The handation specified by a syntax-directed definition car, be made precise 

Fig. 5.7. Dcpcndcncy gupb for p m c  ttcc of Fig. 5.5. 
as folbws. The underlying grammar is used to construct a parse trce for the 
input. TRc dependency graph is constructed as discussed above. From a 
topological sort of the dependency graph, we obtain an evaluation order for 
the semantic rules. Evaluation uf the semantic rules in this order yields the 
translation of the input string. 
Example 5.6. 
Each of the edges in the dependency graph in Fig. 5.7 goes 
from a lower-numbered node to a higher-numbered node. Hence, a lopologi- 
c;ll sort of the dependency graph is obtained by writing down the nodes in the 
order of their numbers, From this tapulogical sort, we obtain the following 
program, We write u ,  for the attribute associated with the node numbered n 
-in the dependency graph. 
Evaluating these semantic rules stores the type red in the symbol-table entry 
for each identifier. 
o 
Several methods have been proposed for cualuating semantic rules: 
I .  Parse-rrw rnerhod,~~ At compile time. these methods obtain an evaluation 
order from a topdogical sort of the dependency graph constructed from 
the parse tree for each input, The* methods will fail to find an evalua- 
tion order only il' the dependency graph fur the particular parse tree 
under consideration has: a cycle. 

SEC. 5.2 
CONmRUCTION OF SYNTAX TREES 287 
2. 
Ruk-hsed rnc-vkod~. At compiler-constrwction lime, the semantic rules 
associated with productions are analyzed, either by hand, or by a special- 
ized tool. Fw each production, the order in which the attributes associ- 
ated with that production are evaluated is predetermined at compiler- 
construction time. 
3. 
Obfivious methrds. An evaluation order is chosen without considering the 
semantic rules. Far example, if translation takes place during parsing, 
then the order of evaluation is forced by the parsing method, independent 
of the semantic rulesA An oblivious evaluation order restricts the class of 
sy ntax-directed be finit ions that can be implemented. 
Rule-bawd and obliuious methods need not explicitly construck the depen- 
dency graph at compile time. so they can be more efficient in their use of 
compile time and space. 
A syntax-directed definition is said to be drcubur il che dependency graph 
for some parsc tree generated by its grammar has a cycle. Section 5.10 
discusses how to test a syntax-directed definition for circularity. 
5.2 CONSTRUCTlON OF SYNTAX TREES 
In this section, we show how syn!ax-directed definitions can be used to specify 
the ccmstruction ut' syntax lrees and oher graphical representations of 
language constructs. 
The use of syntax trees as an intermediate rep~esencation allows translation 
to be dewupled from parsing. Trmdation routines that are invoked during 
parsing must live with two kinds of restrictions, First, a grammar that i s  suit- 
able for parsing may not reflect the natural hierarchical structure of the con- 
structs in the language. Fur example, a grammar for Fortran may view a sub 
routine as consisting simply of a list of statements. However, anaiysis of the 
subroutine may be easier if we use a tree representation that reflects the nest- 
ing of IX, laops, Second, the parsing method constrains the order in which 
nodes in a parse tree are considered. This order may not match the order in 
which information about a construct becomes available. For this reason, corn- 
pilers for C usually construct syntax trees for declaratioas. 
Syntax Trees 
An (abstracl) syntax tree is a condensed form of parse tree useful for 
representing language constructs. The pruduction S - if 8 then $ 1  dwSz 
might appear in a syntax tree as 
if-then-else 
In a syntax tree, operators and keywords do no! appear as leaves, but rathe[ 
are associated with the interior node that would be the parent of those leaves 

288 
SY NTA X-DIR ECTED TRA NSLATlON 
SEC, 5.2 
in the parse tree. Another simplification found in syntax trees is that chains 
of single productions may be collapsed; the parse tree of Fig. 5.3 becomes the 
syntax tree 
Syntax-directed translation can be b a d  on syntax trees as well ax parse 
trees. The approach is the same in each case; we attach attributes to the 
nodes as In a parse tree. 
Constructing Syntax Trees for Expressions 
The construction of a syntax tree for an expression is similar to the translation 
of the e~pression into postfix form. We construct subtrees for the subexpres- 
sions by creating a node for each operator and operand. The children of an 
operator node are the roots of the nodes representing the subexpressi~ns con- 
stituting the oprands of [hat operator. 
Each node in a syntax tree can be implemented as a record with several 
fields, In the node for an operator, one field identifies the operator and the 
remaining fields contain pointers to the nodes for the operands. The operator 
is often called the lukl of the node. When used for translation, the nodes in 
a syntax tree may have additional fields to hold the values {or pointers ta 
values) of attributes attached to the node. In this section, wc use the follow- 
ing functions to create the nodes of syntax trees for expressions with binary 
operators, Each function returns a pointer to a newly created node. 
1. 
mhde(up, 
kfi, right) creates an operator node with label up and two 
fields containing pointers to It$ and right. 
2. mkIeuf (id, w r y )  creates an identifier node with label id and a field con- 
taining entry, a pointer to the xy mbol-table entry for the identifier, 
3. 
m k h f  (num, vull creates a number node with label num and a field con- 
raining v d ,  the value of the number. 
Example 5.7. The following sequence of functions calls creates the syntax 
tree 
for 
the 
expression a - 4 + c 
in Fig. 
5.8. 
In 
this sequence, 
P I ,  
p2. . . . , ps are pointers lo nodes, and mrryrr and e n t r y  are pointers to 
the symbol-table entries for identifiers a and c, respectively. 
The lree is constructed bottom up. The function calls &euf(id, 
enlrya) 
and rnkku~{nurn, 4) construct the leaves for a and 4; the pointers to these 

CONSTRWCTKW OF SYNTAX TREES 289 
to entry fm a 
Fig. 5.8. Syntax trm for rt-a+c, 
nodes are saved using p ,  and pz. The call mkrtde('-'.pl, 
p z )  then con- 
structs the interior node with the leaves for a and 4 as children. After two 
more step, p 5  is left pointing to the root. 
o 
A Syntax-Directed Mnition far Constructing Syntax Trees 
Figure 5.9 contains an S-attributed definition for constructing a syntaK tree for 
an expression containing the operators + and - +  
It uses the underlying pro- 
ductions of the grammar to schedule the catls of the functions m h d e  and 
m k h j  to construct the tree. The synthesized attribute nprr for E and T keeps 
track of the pointers returned by the function calls. 
E - r  E , + T  
E -  E l - T  
E - T  
T -  ( E l  
T - M  
T -. nurn 
T.fiptr : = mkied Inurn, nurn. v d )  
Fig. 5.9. Syntax-dirccted &finition for constructing a syntax trcc for an expression. 
Example 58, An annotated parse tree depicting the cons~rudion of a syntax 
tree for the expression a 4 + c  is shown in Fig. 5.10. The parse tree i s  shown 
dotted, The parsetree nodes labeled by the nonierminals E and T use the 
synthesized attribute npfr to hold a poinler to the syntax-tree node for the 
expression represented by the nonterminal. 
The semantic rules asmiated with the productions T - id and T - num 
define attribute T.fipw to be a pointer to a new leaf for an identifier and a 
number, respectively. Attributes id.sntry and num. vai are the lexical values 
assumed to be returned by the lexical analyzer with the tokens id and mum. 
In Fig. 5.10, when an e~gressicin E is a single term, corresponding to a use 

ac. 
5.2 
. , - 
E n p  
. . 
. 
I 
. - . .  
. - .  
- ,  
E n p t  
. ; I  
L 
T nprr 
, 
, - .  
, . 
I 
: 
1 
E 
I 
T ~ t p r  
I 
id 
I 
: 
I 
I 
1 
1 
I 
T ttprr 
I 
maim 1 
I 
I 
L 
I 
Y 
I 
I 
I 
I 
id l 
I 
L 
I 
I 
1 
1 
I 
1 
I 
v #- 
I 
w 
I 
I 
I 
I 
I 
I 
v 
V Sr 
V 
to cntry for c 
l i d : ,  I 
I 
Fig. 5.10. Construction uf a syntax-trcc for a-4tc. 
of the production E + T, the attribute E. rtpr gets the value of T.nptr. When 
the semantic rule E.nptr := m h d e ( ' - ' ,  E,.wrr, T.~pfr) associated with the 
product ion E - El - T is invoked, prcvious rules have set E .nprr and T-nptr 
to be pointers to the leaves for a and 4, respectively. 
In interpreting Fig. 5.10, it is important to realize that the lower tree, 
formed from records is a "real" syntax tree that constitutes the output, while 
the dotted tree above is the parse tree. which may exist only in a figurative 
sense. In the next section, we show how an S-attributed definition can he sim- 
ply implemented using the stack of a bttom-up parser to keep track of attri- 
bute vdues7 In fact, with this implementation, the node-building functions are 
invoked in the same order as in Example 5.7. 
o 
Directed Aeydic Graphs for Expressions 
A directed acyclic graph I hereafter called a dug) for an expression identifies 
the common subexpressions in the expression. Like a syntax tree, a dag has a 
node for every subexpression of the expression; an interior node represents an 
operator and its children represent its operands. The difference is that a node 
in a dag representing a common subexpression has more than one "parent;" in 
a syntax tree, the' comhm subexpression would be represented as a duplicated 
subtree. 
.I I 
Figure 5.1 1 contains a dag for the expreusion 
The leaf for a has two parents because a is common to the two subexpressions 
a and a * I b - c ) . Likewise, both occurrences of the common subexpression 
b-e are represented by the same node, which also has two parents. 

CONSTRUCTKH OFSYNTAXTREES 291 
Fig. 5,11. Dag for thc cxprcssion a+a* I b-c 1 +I 
b-c 1 *d. 
The syntax-directed definition of Fig. 5.9 will construct a dag instead of a 
syntax tree if we modify the operations fo'r constructing nodes. 
A dag is 
obtained if the function mn~ructing a node first checks to see whether an 
identical nude already exists. Far example, before constructing a new node 
with label op and fields with pointers to /eft- and right, nrkndeIup, left, right) 
can check whether such a node has already been construcled. 
If 
so, 
mknode{op, kft, rixhf) can return a pointer to the previously constructed 
node. The leaf-constructing functions mkfedjcan k h a v e  similarly. 
Example 5.9. The sequence of inst ructions in Fig. 5.12 constructs the dag in 
Fig. 5.1 1 .  provided rnkwd~ and mklerrf create new nodes only when necessary. 
returning pointers to existing n d e s  with the correct label and children when- 
ever possible. In Fig. 5.12, u, b, r., and dpoint to the symbol-table entries for 
identifiers a, b, c, an? d. 
Fig. 5.12, Instructions for constructing th dig of Fig. 5.1 1. 
When the a l l  mAIwf (id, a) i s  repeated on line 2, the node constructed by 
the previous call mkkuf(id, a) is returned, so p1 = P I .  Similarly, the nodes 
returned on lines 8 and 9 are the same as those returned on lines 3 and 4, 
rcspecrively. Hence, the node returned on line 10 must k the same one con- 
structed by the call of m k d e  on line 5. 
n 
In many applications, nodes are implemented as records stored in an array, 
as in Fig. 5.13, In the figure, each record has a label field that determines the 

292 
SYNTAX-DIRECTED TRANSLATION 
SEC. 5.2 
nature of the node. We can refer to a node by its index or pxition in the 
array + The integer index of a node is often called a vuhu number for histori- 
cal reasons. For example, using value numbers, we can say node 3 has Label 
+, its left child is node 1 ,  and its right child is n d e  2, The following algci- 
r ithm can be used to create nodes for a dag representation of an expression. 
Fig* 5.13. Nodcs in a dag for i : = i + 10 allomted frnm an array. 
Algorithm 5.1. Value-number method fur constructing a node in a dag. 
Suppose that nodes are stored in an array, as in Fig. 5.1 3, and that each node 
is referred to by its value number. Let the signutru-r of an operalor node be a 
triple i o p ,  I. r> wnsis~ing uf jis label up, left child I, and righi child r. 
!)MI. 
Label op, node 1, and node r+ 
Output. A node with signature <up, I, r>. 
Methud. Search the array for a node rn with label op, left child I, and right 
child r. If there is such a node, return m; otherwise, create a new node n with 
label up, left child 1, righr child r, and return n. 
An obvious way to determine if node m is already in the array is to keep a11 
previously created nodes on a list and to check each node on the list tu see if 
it has the desired signature. The search for m can be made more efficient by 
using k lists, called buckets, and using a hashing function h to determine 
which bucket to search. ' 
The hash function k computes the number of a bucket from the value of op. 
/, and r. It will always return the same bucket number, given the wrnc argu- 
ments. If m is not in the bucket h(op. !, r ) +  then a new node n is created and 
added to this bucket, so subsequent searches will find it there. Several signs- 
tures may hash into the same bucket number, but in practice we cxpect each 
bucket to contain a small number of nodes. 
Each bucket can be implerncntcd as a linked list as shown in Fig. 5.14. 
Any data slruclllrc that inlplcrncnrs dictionllrics in thc scnsc iof Aho, H{,pcrr,[f, and Cllinun 
I L W l  suffices. Thc implrrlanl property of thc structure is thal given a key. i.c.. a lab1 up and 
two rides I and r, wc can raprdly trbtrin a nrdc m with signururc cop, I, r > .  w detcrmim thal 
nonc c~ists. 

SEC. 5,3 
BOTTOM-UP EVALUATION OF $ATTRIBUTED DEFINITIONS 293 
Each cell in a linked list represents a node. The bucket headers, consisting of 
pointers to the first cell in a list. are stored in an array. The bucket number 
returned by h ( q ,  l, r) is an index into this array of bucket headers. 
Array of buckct heabcrr;, 
9 
inbcxcd by hash ralue 
List clcmcnts 
rcprcscnt ing nodcs 
Fk. L14. Data struclurc for starching buckcts. 
This algorithm can be adapted to apply to nodes that are not allowed 
sequentially from an array. In many cornpikrs, nodes are allocated as they 
are needed, to avoid preallwating an array that may hdd too many ndcs 
most of the time and not enough nodes some of the time. In this case, we 
cannot assume that nodes are in sequentiai storage, so we have ro use pointers 
to refer to nodes, 
I f  the hash function can be made to compute the bucket 
number from a label and pointers to children, then we can use pointers to 
nodes instead of value numbers. Otherwise, we can number the nodes in any 
way and use this number as the value number of the node. 
ci 
Dags can alsu be used to represent sets of expressions, since a dag can have 
rnwc than one root. ln Chapters 9 and 10, the cornputa~ions performed by a 
sequence of assignment statements will be represented as a dag. 
5 -3 BOTTOM-UP EVALUATION OF S-ATTRIBUTED DEFINITIONS 
Now that we have seen how to use syntax-directed definitions to specify trans- 
lations. we can begin to study how to implement translators for them. A 
translator for an arbi~rary syntax-directed definition can be difficult to build+ 
However, there are large classes of useful syn tax-directed definitions for 
which it is easy to construct translators. In this section, we examine one such 
class: the $-attributed definitions, that is, the syntax-directed definitions with 
only synthesized attributes. The following sections consider the implcmenta- 
tion of definitions that have inherited attributes as well. 
Synthesized attributes can be evaluated by a bottom-up parser as the input is 
being parsed. The parser can keep the values of the synthesized attributes 
associated with the grammar symbols on its stack. Whenever a redudion is 
made, the values of rhe new synthesized attributes are computed from the 
attributes appearing on the stack for the grammar symbls on the right side of 
the reducing prduction. This s+xtion shows how the parser' stack can be 
extended to hold the values of these synthesized a~tributes. We shall see in 

Section 5.6 that r his implementation also supports some inherited attributes, 
Only synthesized attributes appear in the syntaxdirected definition in Fig. 
5.9 for construcling the syntax tree for an expression. The approach of this 
sechn can therefore be applied to construct syncax trees during bottom-up 
parsing. As we shall see in Section 5.5, the iranslarion of expressions during 
top-down parsing often uws inherited attributes. We therefore defer trnnsla- 
tion during top-dawn parsing until after "left-to-right" dependencies are 
examined in the next section. 
Synthesized Attributes on the Parser Stack 
A translator for an S-attributed definition can often be implemented with the 
heip of an LR-parser generator, such as the one discussed in Section 4.9. 
From an S-attributed definition, the parser generator can construct a translator 
that evaluates attributes as it parses the input. 
A bottorn-up parser uses a stack to hold information about subtrces that 
have been parsed. We can use extra fields in thc parser stack to hdb the 
values of synthesized attributes, Figure 5 ,  I5 shows an example of a parser 
stack with space for one atiftribute value. Let US suppose, as in the figure, that 
the stack is implemented by a pair of arrays siate and wl. ~
k
h
 
stale entry is 
a pintet (or index) to an LR( I )  parsing table, (Note that the grammar sym- 
bol is implicit in the state and need not be stored in the stack.) It is con- 
venient, however, to refer to the state by the unique grammar symbol that i t  
covers when placed on the parsing stack as described in Section 4.7. If the ith 
stuw symbol is A, then vallil will hold the value of the attribute associated 
with the parse tree node corresponding to this A .  
Fig. 5.15, 
Parser stack with -a fidd for synthcsizcd attributes. 
The current lop of the stack is indicated by the pointer top. We assume 
that synthesized attributes arc evaluated just before each reduction. Suppse 
the semantic rule A.u := J{X.x, Y4y, 2.2) is a ~ m i a t e d  with the production 
A +XYZ. 
Before XYZ is reduced to A ,  the value of the attribute 2.z is in 
vai Imp 1, that of Y+y in vo! [lor, - 11, and that of X.x in vallrrrp -21. 
If a sym- 
bol has no attribu~e, then the correspnding entry in the val array is unde- 
fined. After the reduction, top is decremented by 2, the state covering A is 

SEC. 5.3 
BOTTOM-U P EVALUATION OF S-ATTRIBUTED DEFINITIONS 295 
put in stare Itup ) (i.e., where X was), and the value of the synthesized attri- 
bute A.a is put in val [top 1. 
E m p l e  5.10, Consider again the syntax-directed definition of the desk cal- 
culator in Fig. 5.2. The synthesized attributes in the annotated parse tree of 
Fig. 5.3 can be evaluated by an LR parser during a bottom-up parse of the 
input line 3+5+4n+ As before, we assume that the lexical analyzer supplies 
the value of attribute digit.iemd, which is the numeric value of each token 
reprcscnting a digit, When the parses shifts a digit onto the stack, the token 
digit i s  placed in strzleItop] and its attribute value is placed in vd~rop I. 
We can use the techniques of Section 4.7 to construct an LR parser for the 
underlying grammar. To evaluate attribute, we modify the parser 40 execute 
the code fragments shown in Fig. 5.16 just before making ihe appropriate 
reduction. Note that we can associate attrlblrte evaluation with reducrions, 
because each reduction determines the production to be applied. The code 
fragments have been obtained from the semantic rules in Fig. 5.2 by replacing 
each attribute by a position in the v d  array. 
fig. 5.16. Implcrncntation of a dcsk caalculator with an LR parscr, 
The code fragments do not show how the variables top and nrop are 
managed. When a production with r symbols on the right side i s  reduced, the 
value of m p  is set 10 top-r t I. After each code fragment is executed, top is 
set to ntop. 
Figure 5+17 shows the sequence of moves made by the parser on input 
3*5+48. The contents of the srutr and v d  fields of the parsing stack are 
shown after each move. We again take the liberty of repiacing stack states by 
their corresponding grammar symbols. We take the further iiberty of show- 
ing, instead of token digit, the actua! input digit. 
Consider the sequence of evcnrs on seeing the input symbol 3, In the first 
move, the parser shifts the state corresponding to the token digit (whose attri- 
bute value is 3) onto the stack. (The state is represented by 3 and the value 3 
is in the v d  field,) On the second move, the parser reduces by the production 
F -. digit and implements the semantic rule F.vd := digit./xd. On -the 
third move the parser reduces by T - F. No c d e  fragment is associated with 

296 
SYNTAX-DIRECTED TRANSLATION 
F - digk 
T - F  
E -, E + T  
F"q. 5, If. Moves made by translator on input 3*5+4 n. 
this production, so the v d  array is left unchanged. Note that after each 
reduction the top of the v d  stack contains the attribute value associated with 
the left side of the reducing prduction. 
a 
In the implementation sketched above, d
c
 
fragments are executed just 
before a i-eduction takes place. Reductions provide a "hook" on which actions 
consisting of arbitrary code fragments can be hung. That is, we can allow the 
user to associate an action with a production that is executed when a reduction 
according to that production takes place. Translation schemes considered in 
the next section provide a notation fot interleaving actions with parsing. En 
Section 5.6, we shall see how a larger class of syntax-directed definitions can 
be implemented during bottom-up parsing. 
5.4 L-ATTRIBUTED DEFlNITIONS 
When translation takes place during parsing, the order of evahation of attri- 
butes is linked to the order in which nudes of a parse tree arc "created" by 
the parsing method. A natural order that characterizes many topdown and 
bottom-up translation methods is the one obtained by applying the prwedurc 
@id in Fig. 5. t8 to the root of a parse tree. We call this evaluation order 
the deprh-first order. Even if the parse tree is not actually constructed, it is 
useful to stddy translation during parsing by considering depth-first evaluation 
of attributes at the nodes of a parse tree. 

L-ATTR IBUTED DEFINITIOPIS 297 
pnmdure dhsir ( n  : node); 
WE 
for cach child ma of n, from left to right do kmgin 
evaluate inherited attributes of m; 
d/visil [rn ) 
d; 
evatuate synthesized attributes oi A 
d 
Fig. 5.18. DepB-first evaluation order for attributes in a pa= 
trse. 
We now introduce a class of syntax-directed definitions, called L-attributed 
definitions, whose attributes can always be evaluated in depth-first order. 
(The L i s  for "left," because attribute information appears to flow from left to 
right .) Implementation of progressively larger classes of L-attributed dcfini- 
t i m s  ia c~vered in the next three sections of this chapter. L-attributed defini- 
tions include all syntaxdirected definitions based on LL( I )  grammars; Sect ion 
5.5 gives a methd for implementing such definitions in a single pass using 
predictive parsing methods, A larger class of L-attributed definitions is impk- 
merited in Section 5.6 during bitam-up parsing, by extending the translation 
methds of Section 5.3. A general methd fw implementing all L-attributed 
definitions i s  outlined in Section 5.7. 
A syntalr-birecteb definition is L-aifribuied if each inherited attribute of X,, 
l ~ j ~ n ,  
on the right side of A +XIX2 
. Xn. depends only on 
1. 
the attributes of the symbols XI, X 2 , .  . . , X,-@ to the left ofX, in the 
production and 
2 ,  the inherited attributes of A. 
Note that every Sattributd definition is L-attributed, because the restric- 
tions (1) and (2) apply only to inherited attributes. 
Example 5,lL The syntax-directed definition in Fig. 5.19 is nd L-attributed 
because the inherited attribute Q.i of the grammar symbol Q depnds on the 
attribute R.s of the grammar symbol to its right. Other examptes of befini- 
tiofis that are n d  L-attributed can be found in Sections 5.8 and 5.9. 
0 
A translation scheme is a context-free grammar in which attributes are sssoci- 
ated with the grammar symbls and semantic actions encimd between braces 
{) are inserted within the right sides of ptductions, as in Seaion 2.3. We 

Mg. 5.19. A non-l-attributcb syntaxdircctcd dcfinithn. 
shall use rranslahn schemes in this chapter as a useful notation for specifying 
translation during parsing. 
The translation schemes considered in this chapter can have both syn- 
thesized and inherited attribttles, In the simple translation schemes considered 
in Chapter 2, the attributes were of string type, one fm each symbol, and for 
every production A -. X , . 
X,,, the semantic ruk formed the string for A by 
concatenating the strings for X , . . . , X,,, in order, with some wtional addi- 
tional strings in between, We saw that we could perform the translation by 
simply printing the literal strings in the order they appeared in the semantic 
rules. 
Example 5.12. Here is a simple translation scheme that maps infix expres- 
sions with add ition and subtraction into corresponding postfix exprwsions. It 
is a slight reworking of the translation scheme (2.14) from Chapter 2, 
Figure 5.20 shows the parse tree for the input 9-5+2 with each semantic 
action attached as the appropriate child af the node correspding to the left 
side of their production. In effect, we treat aahnu as though they are termi- 
nal symbols, a viewpoint that is a convenient mnemonic for establishing when 
the actions are to be executed, 
We have taken the liberty of showing the 
actual numbers and additive operator in place of the tokens mum and addop. 
When performed in depth-first order, the actions in Fig. 5+20 print the output 
95-2+. 
o 
When designing a translation scheme. we must observe some restrictions to 
ensure that an attribute value is available when an action refers to it. These 
restrictions, motivated by L-attributed definitions, ensure that an action does 
not refer to an attribute that has not yet k e n  computed. 
The easiest c a ~  
w u r s  when only synthesized attributes are needed. For 
this case, we can construct Ihe translation &ems 
by creating an action 

L-ATTRISUTED DEFINITIONS 299 
Fi. 5.a. Par= trcc for 9-5 t2 showing actions. 
consisting of an assignment for each semantic rule, and placing chis action at 
the end of the right side of the associated prduction. For example, the pro- 
duction and semantic rule 
yield the following production and semantic action 
If we have both inherited and synthesized attributes, we must be more care- 
fu I: 
I. An inherited attribute for a symbol on the right side of a production must 
be computed in an action before that symbol. 
2. 
An actim.musr not refer to a synthesized attribute of a symbol to the 
right of the action. 
3. 
A synthesized attribute for the nonterminal on the left can only be corn- 
puted after all attributes it references have been computed. The action 
computing such attributes can usually be placed at the end of the right 
side of the production. 
In the next two sections, we show how a translation scheme satisfying these 
three requirements can be implemented by generalizations of topdown and 
bot tom-up parsers + 
The following trandation scheme does not satisfy rhe first of these three 
requirements, 
S * A I  A2 { A l . h  := 1;A2.in ;= 2 )  
A + a  
{ prinr ( A .  in) } 
We find that the inherited attribute A,in in the second production is not 
defined when an attempt is made to print its value during a depth-firs1 traver- 
sal of the parse tree for the input string un. That is, a depth-first traversal 

300 
SY NTAX-DIR ECTED TRANSLATION 
SEC. 5.4 
starts at S and visits the subtrees for A ,  and A 2  before the values of A ,.in and 
A in are set. If the action defining the values of A .in and A 2 .  in is embed- 
ded before the A's on the right side of S - A ,  A 2 ,  instead of after, then A.in 
will be defined each time print (A. in) occurs, 
it is always possible to shart with an L-attributed syntax-directed definition 
and wnstruct a translation scheme that satisfies the three requirements above. 
The next example 
illustrates this construction. 
It is based 
0 
the 
mathernatics-formatting language EQN, described briefly in Section 1.2. 
Given the input 
E sub 1 .val 
EQN places E, I ,  and + v d  in the relative positions and sizes shown in Fig. 
5.21. Notice that the subscript I is printed in a smaller size and font, and is 
moved down relative ro E and . v d .  
Fig. 5.21. Syntaxdirmted ptaccrncnt of tsoxcs. 
Example 5.13. From the L-attributed definition in Fig. 5,22 we shall con- 
struct the translation scheme in Fig. 5.23. In lhe figures, the nonterminal B 
(for box) represents a formula. The pcducr ion B -. tr B represents the juxta- 
position of two boxes, and B + B  sub 8 represents the placement of the 
secclnd subscript box in a smaller size thari the first box in the proper relative 
position for a subscript. 
The inherited attribute p$ (for point size) affects the height of a formula. 
The rule for prduction 3 
text causes the normalized height of the teKt to 
be multiplied by the p i n t  size to get the actual height of the text, The attri- 
bute It of text is obtained by table lookup, given the character represented by 
the token text. When production B + B 1 B 2  is appiied, B l  and B2 inherit the 
point size from B by copy rules. The height of B, represented by the syn- 
thesized attribute kr, is the maximum of the heights of B ,  and B2. 
When production 8 + B I sub B 2  is used, the function shrink lowers the 
point size of B2 by 30%. The function disp allows for the downward displace- 
ment of the box B 2  as it computes the height of B. The rules that generate 
the actual typesetter commands as output are not shown, 
The definition in Fig. 5.22 is L-attributed. The only inherited attribute is 
ps of the nonterminal 3. Each semantic rule defines ps only in terms of the 
inherited altribute of the nonterminal on the left of the production. Hence, 
the definition is L-attributed. 
The translation scheme ir, Fig. 5.23 is obtained by inserting assignments 
corresponding to the semantic rules in Fig. 5+22 into the productions, 

PRODUCT~N 
B - text 
I 
B h  := text.h x 8 . p ~  
- 
SEMANTIC RULES 
B + B l s u b B 2  
Fu. 5.22. Sy ntaxdirmcd dcfinitian for size and heighl of boxes. 
Bl.ps:-B.ps 
B +w : = shrink (B+ps) 
8.h := disp(B,.hr, B2.kr) 
8 - 
{ B l .ps := B.l)s } 
B 1 
sub 
{Bl.ps := shrittk(B.ps) ) 
Bz { B.ht := disp[S,.kf, B,.hr) } 
Fig, 5.23. Trarrslat ion schcmc mstructed from Fig. 5.22. 
following the three requirements given above. For readabiiity, each grammar 
symbol in a production is written on a separate line and actions are shown to 
the right. Thus, 
is written as 
Note that actions setting inherited attributes B 1 . p  and B2.ps appear just 
before B ,  and B1 on the right sides of pmduc1ions. 

SEC. 5.5 
5.5 TOP-DOWN TRANSLATION 
in this section. L-attributed defin itiuns will be implemented during predictive 
parsing. We work with  rans slat ion schemes rather than synt ax-directed dtfini- 
tions so we can be explicit about the order in which actions and attribute 
evahations lake place. We also exlend the algorithm for left-recursion elimi- 
nation to translation schemes with synthesized attributes. 
ECiminating Loft Recumion from a Translation Scheme 
Since most arithmetic operators associate to the left, i t  is natural to use lleft- 
recursive grammars for expressions. We now extend the algorithm for elim- 
inating left recursion in Sections 2.4 and 4.3 to allow for attributes when the 
underlying grammar of a translation scheme is transbmeb. The transforma- 
tion applies to translation schemes with synthesized attributes. It allows many 
of the syntax-directed definirims of Sections 5.1 and 5.2 to be implemented 
using predictive parsing, The next example motiv;rres ihe transformation. 
Example 5.14. The translation scheme in Fig+ 5.24 is transformed below into 
the translation scheme in Fig, 5.25. The new scheme produces the annotated 
parse tree of Fig. 5.26 for the expression 9-5+2. The arrows in the figure 
suggest a way of determining the value of the expression. 
E + El + T 
( E+vd := El.vd -t T . v u l )  
E -. E ,  - T  
{ E . v d : = E , . v d  - T+vd] 
E - 7  
{ E. vnl : = T. v d  ) 
T + ( E 1 
{ T+ v d  : = E. vtd } 
T - rmm 
{ T. v d  : = num. v d  ) 
Fig. 5.24. Translatifin schcmc with Icft-recursive grammar- 
In Fig. 5.26. the individual numkrs are generated by T, and T.vd takes its 
value from the lextcal value of the numkr, given by attribute num.vd. The 
9 in the subexpression 9-5 i s  generaled by the kftmost T, but the minus 
operator and 5 are gencratcd by the R at the right child of thc root. The 
inherited atlribute R.i obtains the value 9 from T . w l .  The subtraction 9-5 
and the passing of the result 4 down to the middle node for R are done by 
embedding the following action between Tand R ,  in R + -TRt: 
A similar action adds 2 to the value of 9-5, yielding the result R.i = 6 at the 
bottom node for R. The result is needed ai the root as the value of E. d; 
the 
synthesized attribute s for R ,  not shown in Fig. 5.26, is used to copy thc result 
up to the rout. 
For top-down parsing, we ran assume that an action is executed at the time 
that a symbol in the same position would be expanded. Thus, in the second 

SEC. 5 s  
TOP-DOWN TRANSLATION 
303 
Fig. 5.25. Transfomcd translation scheme with right-rccursivc grammar 
Fig. 5.24. Evaluation of the expression 9-5+2, 
production in Fig. 5.25, the first actSon (assignment to R .i) is done after T 
has been fully expanded to terminals, and the second action is done after R1 
has been fully expanded. As noted in the discussion of L-attributed defini- 
tions in Section 5.4, an inherited attribute of a symbol must Ix computed by 
an action appearing before the symbol and a synthesiixd attribute of the nm- 
terminal on the left must be computed after all the attributes it depends on 
have h e n  computed, 
In order to adapt other left-recursive translation schemes for predictive 
parsing, we shall express the use of attributes R-i and R.s in Fig. 5,25 mare 
abstractly. Suppose we have the following translation scheme 

W SYNTAX-DIRECTED TRANSLATION 
SEC. 5.5 
Each grammar symbol has a synthesized attribute written using the 
corresponding lower case letter, and f and g ere arbitrary functions. The gen- 
eralizat ion to additional A-productions and to productions with strings in place 
of symbuls X and Y can be done as in Example 5.15, below. 
The algorithm for eliminating left recursion in Section 2,4 wnstrlrcts the 
following grammar from (5.21: 
Taking the semantic actions into account, the transformed scheme becomes 
The transformed scheme uses attributes i and s for R, as in Fig. 5.25, To 
see why the results of (5.2) and (5.4) are the same, consider the two anno- 
tated parse trees in Pig. 5.27. The value of A.a is computed according to 
(5.2) in Fig. 5.27(a). Figure 5,27{b) contains the computation of R.i down 
the tree according to (5.4). The value of R.i at the bottom is passed up 
unchanged as R.s, and it becomes the correct value of A,a at the root (R.s is 
not shown in Fig. 5.27(b)). 
Kg. 5.27. Two ways of computing an atttibutc value. 
Example 5.15, 
If the sy~tax-diremd definition in Fig. 5.9 for constructin8 
syntax trees is converted into a translation scheme, then the productions and 

TOP-DOWN TRANSLATION 
305 
semantic actions for E become: 
E - E l  + T { E.nptr := mknode('+', El.npir, T.nptr) ) 
E + El - T { E.npfr ;= mkrtodr('-', El.nprr+ T.npr)) 
E + T  
{ E.qm : = T-npir ] 
When lefr recursion is eliminated from this translation =heme, nonterminal E 
corresponds to A in (5.2) and the strings + T and - T in the first two prduc- 
r i m s  carrespond to F, nonterminal T in the third production corresponds to X, 
The transformed translation scheme is shown in Fig, 5.28, The productions 
and semantic aclions for T are similar to those in the original definition in Fig. 
Fig. 5.28. Transformed translation schcmc for constructing syntax trees . 
Figure 5.29 shows how the actions in Fig. 5.28 construct a syntax tree for 
a-4+c. Synthesized attributes are shown to the right of the node for a gram- 
mar symbol, and inherited attributes are shown to the left. A leaf in the sya- 
tax tree is constructed by actions associated with the prductims T -. id and 
T -. aum, as in Example 5.8. At the leftmost T, attribute T.nptr points to the 
leaf for a. A pointer to the node for a is inherited as attribute R . i  on rhe 
right side of E -. T R. 
When the production R + -TR , is applied at the right child of the root, R.i 
pints to the node for a, and T.nplr to the r i d e  for 4. The node for a-4 is 
constructed by applying mknde to the minus operator and these poi~ters. 
Finally, when production R -. r is applied, R.i pints to the rmt of the 
entire syntax tree. The entire tree is returned through the s attributes of the 
ndes for R [not shown in Fig. 5.29) until it becomes the value OF E.nprr. 
D 

306 SYNTAX-DIRECTED TRANSLATLON 
SEC. 5.5 
to entry for a 
Fi, 
5.29. Use of inherited attributes to construct syntax trecs. 
The next algorithm generalizes the construction of predictive parsers to imple- 
ment a translation scheme based on a grammar suitable for topdown parsing. 
Algorithm 5.2. Construction of a predictive syntax-directed translator. 
Input. A syntax-directed translation scheme with an underlying grammar suit- 
able for predictive parsing. 
Output. Code for a syntax-directed translator. 
Methud, The technique is a modification of the predictive-parser construction 
in Section 2.4. 
1. 
For each nonterminal A, construct a function that has a formal parameter 
for each inherited attribute of A and that returns the values of the syn- 
thesized atiributes of A (possibly as a record, as a pointer to a record with 
a field for each attribute, or using the call-by-reference mechanism for 
passing parameters, discussed in Section 73). For simplicity, we assume 
that each nontcrminal has just m e  synthesinzd attribute. The function 
for A has a Iml variable for each attribute of each grammar symbol that 
appears in a production for A. 
2. 
As in W i o n  2.4, the code for nonterminal A decides what production to 
use based on the current input symhl, 
3. The d
e
 associated with each production does the following. We con- 
sider the tokens, nmterminals, and actions on the right side of the pro- 
duction from left to right. 

i) 
For token X with synthesized attribute x, save the value of x in the 
variable declared for X.X. Then generate a call to match token X and 
advance the input. 
ii) 
For nonterminal B, generate an assignment c := B ( b , ,  b2. . + 
+ . bk) 
with a function call on the right side, where b , ,  b z ,  . . . . bk are the 
variables for the inherited attributes of B and c i s  the variable far the 
synthesized attribute of B+ 
iii) For an action, copy the code into the parser, replacing each reference 
to an attribute by the variable for that attribute. 
o 
Algorithm 5+2 is extended in Section 5.7 to implement any L-attributed 
definition, provided a parse tree has already been constructed. In Section 5+8+ 
we consider ways of improving the translators constructed by Algorithm 5.2. 
For example, it may be possible to eliminate copy statements of the form 
x := y or to use a single variable to hold the values of several attributes. 
Some such improvements can also be done automatically using the methods of 
Chapter 10. 
Example 5-16. The grammar in Fig. 5.28 is LL( I ) ,  and hence suitabk for 
tap-down parsing. From the attributes of the nonterrninals in the grammar, 
we obtain the following types for the arguments and results of the functions 
for E ,  R ,  and T, Since E and T do not have inherited attributes, they have no 
argumen ts7 
function E : t syntax-tree-node; 
function R ( i  : 7 syntax-tree-node): 
t syntax-tree-node; 
hnction T : t syntax-tree-node; 
We combine two of the R-productions in Fig. 5.28 to make the translator 
smaller. The new productions use token addop to represent + and -: 
R -. addop 
T 
{ R 
i : = m k d e  (addop, h e m e ,  R. i, T.nptr) 
R 1 
( R.$ := R @ }  
(5.5) 
R
~
E
 
( R.s ;= R . i  ) 
The code for R is based on the parsing procedure in Fig, 5 3 3 .  If the boka- 
head symbul i s  adclop. then the production R * ddop T R is applied by using 
the procedure march to read the next input token after addop, and then calling 
the procedures for T and R. Otherwise, the procedure does nothing, to mimic 
the production R -. c. 
The procedure for R in Fig, 5 3 1  contains code fur evaluating attributes, 
The lexical value k x v d  of the token ddop is saved in doplexeme, addap is 
matched. T is catied, and its result is saved using nprr. Variable il 
corresponds to the inherited attribute R . i, and si to the synthesized attribute 
Rl.s. The return statement returns the value of s just before control leaves 
the function, The functions for E and T are wnslructed similarly. 
o 

Fig. 5.311. 
Parsing procedure for the productions R 
addop T R I t. 
Fig, 5.31. Recursive-descent construction of syntax trees. 
5 6  BOTTOM-UP EVALUATION OF INHERITED ATTRIBUTES 
In this section, we present a method to implement L-attributed definitions in 
the framework of bottom-up parsing. The method is capable of handling ail 
L-attributed definitions considered 'in the previous section in that it can imple- 
ment any L-attributed definition based on an LL(1) grammar. 11 can also 
implement many (but not all) L-attributed definitions based on LRI I )  gram- 
mars. The method is a generalizalion of the bottom-up translation technique 
intrduced in Section 5.3. 
Rmwirrg Embedding Actions from Translation Schemes 
In the bottom-up translation method of Section 5.3, we relied upon all transla- 
tion actions king at the right end of the prduction, while in the predictive- 

SEC. 5.6 
BCITTOM-UP EVALUATION OF INHERITED ATTRIBUTES 309 
parsing method of Section 5-5 we needed to embed actions at various places 
within the right side. TO begin wr discussion of how inherited attributes can 
be handled bitom up, we introduce a transformation that makes all embed- 
ded actions in a translation scheme occur at the right ends of their prubuc- 
lions. 
The traniibrmation inserts new m u r k  nonrerrninals generating E into the 
base grammar. We replace each embedded action by a distinct marker non- 
terminal M and attach the action to the end of the production A4 + e. For 
example, the translation scheme 
is transformed using marker nonterminals M and N into 
The grammars in the two translation schemes accept exactly the same 
language and, by drawing a parse tree with extra nodes for the actions, we 
can show that the actions are performed in the same order. Actions in the 
transformed translation scheme terminate productions, so they can be per- 
formed just M o r e  the right side is reduced during bottom-up parsing. 
Inheriting Attributes on the Parser Stack 
A bottom-up parser reduces the right side of production A + XY by removing 
X and Y from the top of the parser stack and replacing them by A. Suppoiie X 
has a synthesized attribute X.s, which the implementation of Section 5.3 kept 
along with X on the parser stack. 
Since the vahe of X.s is akeady on the parser stack before any reductions 
take place in the subtree below Y, this value can be inherited by Y. That is, if 
inherited attribute Y.i is defined by the copy rule Y.i := X.s, then the value 
X.s can k used where Y.i is called for. As we shall MX, copy rules phy an 
important role in the evaluation of inherited attributes during bottom-up pars- 
ing. 
Example 5.17, The type of an identifier can be passed by copy rules using 
inherited attributes as shown in Fig. 5.32 (adapted from Fig. 5.7). We shall 
first examine the moves made by a bottom-up parser on the input 
Then we show how the valuc of attribute T.rypc can be accessed when the pro- 
duct ions for L are applied, The translation scheme we wish to implement is 

3 10 
SYNTAX-DIRECTED TRANSLATION 
Fig. 5.32, 
At each ride for L, L. ifi = T.pp. 
If we. ignore the actions in the a b v e  translation scheme, the sequence of 
moves made by the parser on the input of Fig. 5,32 is as in Fig+ 5.33. For 
clarity, we show the corresponding grammar symbol instead of a stack state 
and the actual Identifier instead of the token Jd. 
Fig. 5-33. Whenever a right side for L is r c d u d ,  T is just below thc right sidc. 
Suppose as in Section 5.3 that the parser stack is implemented as a pair of 

arrays stare and v d .  If state li I is for grammar symbol X, then val \i ) holds a 
synthesized attribute X.s. The conrents of the state array are shown in Fig. 
5.33. Note that every time the right side of a prduclion for L is reduced in 
Fig. 5.33, T is in the stack just below the right side, We can use this fact to 
access the amibute value T.ypc, 
The irnpkmentation in Fig. 5.34 uses the fact that attribute T.ryp~ 
is at a 
known place in the val stack, relative to the top. Let tap and mop be the 
indices of the top entry in the stack just before and afier a reduclion takes 
place, respectively. From the copy ruks defining Lin, we know that T.rype 
can be used in place of L i n .  
When the production L - id is applied, M.entry is on top of the v d  stack 
and T+type is just below it, 
Hence, add~pe(vaI~top],va~~rop-1]) 
is 
equivalent to udd~pe(ib.enfry, T.ryp). Similarly, since the right side of the 
ptduction L + L , id has three symbols, T . e p ~  
appears in val[iop-3] when 
this reduction takes place, The copy rules involving L.in are eliminated 
because the value of T.qpP in the slack is used instead. 
o 
Fig. 5.34. Thc value of T.typt+ is used in place of L.in. 
Reaching hto the parser stack for an attribute value works only if the gram- 
mar allows the position of the attribute value to tK predicted. 
Example 5.18. As an instance where we cannot predict the position, consider 
the fohwing translation scheme: 
C inherits the synthesized attribute A.s by a copy rule. Note that there may or 
may not be a B between A and C in the stack. When reduction by C + c is 
performed, the value of C+i is either in val Imp - 1 ] or in v 4  Itup - 21, but il is 
na char which case applies. 
In Fig. 5.35, a fresh marker nonterminal M is inserted just before C on the 

3 12 
SYNTAX-DIRECTED TRANSLATION 
SEC. 5.6 
right side of the second production In (5.6). If we are parsing according to 
prduction S -t bABMC, then C.i inherits the value of A.s indirectly through 
M.i and M.s. When the production M -. E is applied, a copy rule M.s ;= M,i 
ensures that the value M.s = M.i = A.s appears just before the part of the 
stack used for parsing the subtree for C. Thus, the value of C.i can be found 
in vulItp - L I when C + c is applied, independent of whether the first 
saaond pr~ductions in the following modificat~on of (5.61 arc uscd. 
PRODUCTION 
SEMANTIC RULES 
S - aAC 
C.i ;= A.s 
S + bABMC 
M.i := A.s; C.i := M.s 
C + c  
C.s := g(C.i) 
M + E  
M.s := M.i 
(a) original product ion 
. . 
s 
i
:
s
 
t' 
a 
b) modified depcndcncies 
Fig. 5.35. Copying an attribute valuc through a marker M. 
Marker nonterminals can also be used to simulate semantic rules that are 
not copy rules. For example, consider 
This time the rule defining C.i i s  not a copy rule, so the value of C.i is not 
already in the vui stack. This problem can also be solved using a marker. 
The distinct nonterminal N inherits A.s by a copy rule. Its synthesized attri- 
bute N+s is set to f (A,$); then C.i inherits this value using a copy rule. When 
we reduce by # --, r, we find the value of N.i in the place for A.3, that is. in 
val Itop - 1 1. When we reduce by S + dNC, the value of C.i is also found in 
vulltop - I], because it is N,s. Actually, we do not need Chi at this time; we 
needed it during the reduction of a terminal string to C, when its value was 
safely stored on the stack with N. 
Example 5-19. Three marker nonterminaki L, M, and N are used in Fig. 5.36 
to ensure that the value of inher itcd attribute Bps appears at a known posit ion 
in the parser stack while the subtree for B is being reduced. The original 

SEC. 5.6 
BOTTOM-UP EVALUATION OF INHERITED ATTRlBUTES 31 3 
attribute grammar appears in Fig. 5.22 and its relevance to text fwmatting is 
explained in Example 5.13. 
3 - text 
M - c  
N - €  
Fig. 5.36. All inherircd attributes arc hct by copy rules, 
Initialization is done using L. The production for S is S 
L B in Fig. 5.36, 
so L will remain on the stack while the subtree below B is reduced. The value 
10 of the inheriled attribute B,ps - L.s is entered into the parser stack by the 
rule L.s :e 10 associated with L -. c. 
Marker M in 3 - B I M B 2  plays a role similar to that of M in Fig. 5.35; it 
ensures that the value of B,ps appears just below B2 in the parser stack. In 
production B +B,subNB2+ the nonterminal N is used as it is in (5.8). N 
inherits, via the copy rule N.i := B.ps, the attribute value that Bz-ps depends 
on, and synthesizes; the value of B Z + p  
by the rule N.s := shrink(N.i). The 
consequence, which we leave as an exercise. is that (he value of B.ps Is always 
immediately below the right side when we reduce to B. 
Code fragments implementing the syntax-directed definition of Fig. 3+36 are 
shown in Fig+ 5.37. All inherited attributes are set by copy rules in Fig. 5.36, 
so the implementation obtains their values by keeping track of their position in 
the var' stack. AS in previous examples, iop and mop give the indices of the 
top of the stack before and after a reduct ion, respectively. 
Systematic introduction of markers, as in the modification of (5.6) and 
(5.7), can make it possible 10 evaluate L-attributed definitions during LR 
parsing. Since there is only m e  production for each marker, a grammar 

SEC. 5.6 
Fig. 537. lmplcmcntation of thc syntax-directcd definition in Fig. 5.36. 
remains LL(l) when markers are added. Any LL(I) grammar is also an 
LR( 1) grammar so oo parsing conflicts arise when markers arc added to an 
L L I I )  grammar. Unfortunately, the same cannot be said of all LR(1) gram- 
mars; that is, parsing conflicts can arise if markers are introduced in certain 
LR( 1) grammars. 
The ideas of the previous examples can be formalized in the fdlowing nlgo- 
rithm. 
f 
Algorithm 5.3. Bottom-up parsing and translation with inherited attributes. 
h p f .  An L-attributed definition with an underlying Lt( 1 ) grammar. 
Ourpur. A parser that computes the values of all attributes on its parsing 
stack + 
Merhd. Let us assume for simplicity that every nonterminal A has one inher- 
ited attribute A.i, and every grammar symbol X has a synthesized attribute 
X.s. If X is a terminal, then its synthesized attribute is really the lexical value 
returned with X by the lexical analyzer; that lexical value appears on the 
stack, in an array v d ,  as in the previous examples. 
For every production A - X I 
+ . X,, , introduce n new marker nontermi- 
nals, h, 
. . . ,hi,,, and replace the production by A -. M ]XI - . . M,,x,,.' The 
synthesized attribute X,+s will go on the parser stack in the v d  array entry 
asmiated with Xi. The inherited attribute X,.i, if there is one, appears in the 
same array, but associated with M,. 
An important invariant is that as we parse, the inherited attribute A.i, if it 
exists, is found in the psition of the wl array immediately below the position 
for M b .  As we assume the start symbol has no inherited attribute, there is no 
problem with the case when the start symbol is A, but even if there were such 
Alt heugh inscrring M, 
before X , simplifics the discussion- uf rnnrkcr nlmrlsrminals , it has thc u n- 
fortunarc aidc cffcst of introducing parring mnfticts into a Icft-rauruivc grammar. k c  Exrxcisc 
S,LI + As n w l d  k h w ,  MI can bc clitnimrtcd. 

SEC. 5.6 
BOTTOM-UP EVALUATION OF INHERITED ATTRlBUTES 315 
an inherited attribute, it could k placed below the bottom of the stack. We 
may prove the invariant by an easy induction on the number of steps in the 
bottom-up parse, noting the (act that inherited attributes are associated with 
the marker nonterminals M,, and that the attribute X,.i i s  computed at Mi 
before we begin the reduction to X,. 
To x e  thar the attribute5 can be computed as intended during a bottom-up 
parse, consider two cases. First. if we reduce to a marker nontcrminal Mi, 
we 
know which production A + M ,X I - 
+ M , , X ,  that marker belongs to. We 
therefore know rhe positions of any attributes that the inherited attribute X,.i 
needs fw its: computa!ion. 
A.i 
is 
in 
d
m
 
2 
2
,
 X I  . I  
is 
in 
v d I m p  -2j +3j, XI.s is in vul(try, -2jf 41. X1.i is in vdIrop-2j+5j, and so 
on. Therefore, we may compute X,.i and store it in vulltr~p + l 1, which 
becomes the new top of stack after the reduction, Notice how the fact that 
the grammar is LL( I )  is important, or else we might not be sure that we were 
reducing r to one particular marker ncmterminsl, and thus could not locate the 
proper attrjbu tes, ar cven know what formula to apply in general. We ask the 
reader to cake on faith. or derive a proof of rhe fact that every LL(1) gram- 
mar with markers is still LR( I ) +  
The second case occurs when we reduce to a nonmarker symbd, say by pro- 
duction A + M I X - ,  . - - M,,X,,. Then, we have only to compute the syn- 
thesixd attribute A.s; note that A.i was already computed, and lives at the 
position on the stack just below the position into which we insert A itself. The 
attributes needed to compute A,s are clearly available at known positions on 
the stack, the positions of the X,'s. during rhe reduction, 
Thc fdlowing simplifications reduce the number of markers; thc second 
avoids parsing conflicts in left-recursive grammars. 
I .  IC X, has no inherited attribute, we need not use marker hij. Of course, 
the expected positions for attributes on the stack will change if Mi is 
omitted, but this change can be incorporated easily in to the parser. 
2. 
If X I  . i  exists, but is computed by a copy rule X , , i  - A.i, then we can 
omit M 
since we know by our invariant that A,i will already be located 
where we want it. just below X ,  on the stack, and this value can there- 
fore serve for X , . i as well. 
Replacing Inherited by Synthwiwd Attributes 
It is sometimes possible lo avoid the use of inherited attributes by changing 
the underlying grammar. For example, a declaration in Pascal can consist of a 
list of identifiers followed by a type, e.g,, m, n ; integer. A grammar for 
such dtclararions may include productions of the form 
D - L : T  
T - integer 1 char 
L -,!,,idlid 

3 16 
SYNTAX-DIRECTED TRANSLATION 
SEC. 5.6 
Since identifiers are generated by L but the type is not in the subtree for L, we 
cannot associate the type with an identifier using synthesized attr~butes alone. 
In fact, If nonterminal L inherits a type from T to its right in the first produc- 
tion, we get a syntax-directed definition that is not L-attributed. so transla- 
tions based on it cannot be done during parsingA 
A solution to this problem is to restructure the grammar to include the type 
as the last element of the list of identifiers: 
D - i d L  
L + , i d L I : T  
T - integer I char 
Now, the type can be carried along as a synthesized attribute Lrype. As each 
identifier is generated by L, its type can be entered into the symbol table, 
A Diff~cul t Syntax-Directed &finition 
Algorithm 5.3, for implementing inherited attributes during bottom-up pars- 
ing, extends to some, but not dl, LR grammars. The L-attributed definition 
in Fig. 5.38 is based on a simple LR( I )  grammar, but it cannot be imple- 
mented, as is, during LR parsing. Nonterminal L in L -. e inherits the count 
of the number of 1's generated by S+ Since the production L -. e is the first 
that a bottom-up parser would reduce by, the translator at lhat time cannot 
know the number of 1's in the input, 
Fig. 533. Difficult syn tax-dittcted definition. 
5.7 RECURSIVE EVALUATORS 
Recursive t'unaiws that evaluate attributes as [hey Iraverse a parse tree can 
be constructed from a syntax-directed definition using a generalization of the 
techniques for predictive translation in Section 5.5. Such functions allow us to 
implement syntax-dircctcd definitions that cannot be implemented simultane- 
wsiy with parsing. In rhis section, we associate a single translation function 
with each nontwminal. The function visits the children of a node fur the non- 
terminal in some order determined by the production at the node; it is not 
necessary that the children be visited in a left-to-right order. In Section 5.10, 
we shall see how the effect of translation during more than one pass can be 
achievcci by associating multiple procedurzs with nonterminals. 

RECURSIVE EVALUATORS 317 
Left-to Right Traversals 
In Algorithm 5.2. we showed how an L-attributed defini~ion based on an 
LL( I) grammar can be implemented by constructing a recursive function that 
parses and translates each nantermirtal. 
All L-attributed syntax-directed 
definitions can be implemented if a similar recursive function is invoked on a 
node for that nonterrninal in a previously constructed parse tree. By looking 
at the production at the node, the function can determine what its children 
are, The function for a nonterminal A takes a node and the values of the 
inherited attributes for A as arguments, and returns the values of the syn- 
thesized attributes for A as results. 
The details of the construction are exactly as in Algorithm 5+2, except for 
step 2 where ihe function for a nonterminal decides what production to use 
based on the current input symbol. The function here employs a case srste- 
ment to determine the production used at a node. We give an example to 
illustrace the method. 
Example 5,M. Consider the syntax-directed definition for determining the 
size and height of formulas in Fig. 5.22. The nmterminal B has an inherited 
attribute ps and a synthesized attribute kt. Using Algorithm 5.2, modified as 
mentioned above, we construcl the function for B shown in Fig. 5.39. 
Function B takes a node n and a value corresponding to B.ps at the node as 
arguments, and returns a value corresponding lo B.ht at node n. The function 
has a cast for each production with B on the left. The code corresponding to 
each product ion sirnulares the semantic rules associated with [he production. 
The order in which'lhe rules are applied must be such that inherited attributes 
of a nonterminal are computed before the function for the nonterminal is 
cat led. 
In the code corresponding to the production B -. B sub B. variables ps, psi, 
and ps2 hold the values of the inherited attributes B.ps, 8 .ps, and Bz.ps. 
Simila~ly ht, kt), and hr2 hold the values of B.ht. 8 ,  .ht. and B2.ht. We use 
the function c h M ( m ,  i) to refer to the ith child of node m. Since B 2  is the 
label of the third child of n d e  n, the value of B2.ht is determined by the 
function call B (rhild(n, 31, p d ) .  
i~ 
Other Traversals 
Once an explicit parse tree is available, we are free to visit the children of a 
node in any order, Consider the non-L-attributed definition of Example 5.21. 
In a ti-anslation specified by this definition, the children of a node for one pro- 
duction need to be visited from left m right, while the children of a node for 
the other production need to be visited from right to left. 
This abstract example illustrates the power of using mutually recursive func- 
tions fm evaluating the attributes at the nodes of a parse tree. The functions 
need not depend on the order in which the parse tree nodes are created. The 
main consideration for evaluation during a traversal is that the inherited attri- 
butes at a node be computed before the node is firs1 visited and that the 

3 18 
SYNf AX-DIRECTED TRAN!il,+ATION 
SEC. 3.7 
Fig. $3. 
Function for nonccmiinal B in Fig+ 5+22+ 
synthesized attributes be computed before we leave the node for the last time, 
Example 5.21. Each of the nanterrninals in Fig. 5.40 has an inherited attri- 
bute i and a synthesized attribute s. The dependency graphs for the two pro- 
ductions are also shown. The rules associated with A + L M set up left-to- 
right dependencies and the rules associated with A + Q R set up right-to-left 
dependencies. 
The function for rimterminal A is shown in Fig. 5.41; we assume that func- 
tions for L. M. Q, and R can be mnstrudcd. Variables in Fig. 5.41 are 
named after the nonterminal and its attribute; e-g. C i  and 1s are the variables 
corresponding to L.i and L.s. 
The code corresponding to the pr0duCt ion A + L M is constructed as in 
Example 5.20. That is, we determine the inherited attribute of L, call the 
function for L to determine the synthesized attribute of L, and repeat the pro- 
cess for M + The c d e  corresponding to A + Q R visits the subtree for R kfore 
it visits thesubtree fur Q. Otherwise, the code for the two productions is very 
similar. 
Ci 

SPACE FOR ATTRiBUTE VALUES A T  COMPILE TIME 
3 19 
Fig, 5.40, Productions and %mantic rules lor nonterminal A. 
Fig, 5.41. Dcpcndcncics in Fig. 5+40 dcterminc t hc ordcr childrcn are visited in. 

320 
SYNTAX-DIRECTED TRANSLATION 
SEC. 5.8 
5.8 SPACE FOR ATTRlBUTE VALUES AT COMPlLE TIME 
in this sedion we consider the compile-time assignment of space for attribute 
values. We shall use information from the dependency graph for a parse tree, 
so the approach of this section is suitable for parsetree methods that deter- 
mine the evaluation order from the dependency graph. in lhe next section we 
consider the case in which the evaluation order can be predicted in advance, 
so we car, decide on the space for attributes once and for all when the corn- 
piler is constructed. 
Given a not necessarily depth-first order of evaluation for atiributes, the 
hterimc of an attribute begins when the attribute is first computed and ends; 
when all attributes that depend on it have been computed. We can conserve 
space by holding an attribute value only during i t s  lifetime. 
In order 10 
emphasize that the techniques in this section apply to any 
evaluation order, we shall consider the following syntax-directed definition 
that is not L-attributed, for passing type information to identifiers in a 
declaration. 
Example 5,22. The syntax-directed definition in Fig. 5.42 is an extension of 
that in Fig. 5,4 to allow declarations of the form 
A parse tree for (5.10) is shown by the dotted lines in Fig. 5.43(a), The 
numbers at the nodes are discussed in the next example. As in Example 5.3, 
the type obtained from T is inherited by L and passed down towards the iden- 
tifiers in the declaration. An edge from T.typc to L.in shows that L i n  depends 
on T.typc. The syntaxdirected definition in Fig. 5.42 is not L-attributed 
because I ,,in depends on num.vd and num is to rhe right d I ,  in 
f +Il 
[num I .  
o 
Assigning Space for Attributes at Compile Time 
Suppose we are given a sequence of registers to hold attribute values. For 
convenience, we a w m e  that each register can hold any attribute value. 
If 
attributes represent different types, then we can form groups of attribu tcs that 
take the same amount of storage and consider each group separately, We rely 
on information about the jifetimes of attributes to determine the registers into 
which they are evaluated, 
Example 5.23. Suppose that attributes are evaluated in the order given by the 
node numbers in the dependency graph of Fig. 5-43; constructed in the last 
example, The lifetime of each node begins when its attribute i s  evaluated and 
The dcpcnctcncy graph in Fig. 5.43 does not show n d c s  wrrcspding to rhe semantic rule 
ddtyp~Iid.entry. /.in) k c a u x  no wpacc is a1k)cacJ for dummy atlribtcs. Nok, howcvcr. that 
this semantic rulc must nw bc cvalui~cd until aftcr thc value d f . i j r  is available. An vtgrwithm to 
determine this fast mud work wilh a depcndocy griph containing nudes frw h i v  scimintic rulc. 

SPACE FOR ATTRIBUTE VALUES AT COMPILE TIME 
321 
D + T L  
T 
int 
T - mi 
L - L ,  , r  
Fig. 5.42. Passing thc type to idcntificrs in a dcclaration. 
(a) Dcpcndcncy grtrph for a parsc trcc 
(bl Nodcs in wdcr of cvaluntion (a) 
Fig. 5.43. Dctcrmining lifctimcs of rrltributc valucs, 
ends when its attribute is used for the last time. For example. the lifetime of 
node 1 ends when 2 is evaluated because 2 is the only node that depends on 1 .  
The lifetime of 2 ends when 6 is evaluated. 
o 
A method for evaluating attributes that uses as few registers as possible i s  
given in Fig. 544. We consider the n d e s  of dependency graph D fcq a patse 

322 
SYNTAXDtRECTED TRANSLATION 
SEC. 5.8 
tree In the order they are to k evaluated. Initially, we have a pol of regis- 
ters 
r 
r 
. . 
If 
attribute 
b 
is 
defined 
by 
the 
semantic rule 
b := f ( c l , c Z ,  . + 
, q), then the lifetime of one or more of c , ,  c l ,  . , . , ck 
might end with the evaluation of b; registers holding such attributes are 
returned after b is evaluated. Whenever possible, b is evaluated into a regis- 
ter that held one of C, , c 2 ,  . . . , ck+ 
fw each nodc m in n , .  m,, . . . . m, do begin 
for cach nodc rr whose Iifctimc cnds with thc cvaiuahn of m do 
mark n's rcgistcr; 
if somc rcgistcr r is rnarkcd then kgim 
unmark r; 
cvaluatc m into register r; 
rcturn rnarkcd registers to the p l  
end 
ekw f i 
no rcgistcrs wcrc rnarkcd */ 
cvaluatc m into a rcgistcr from the pool; 
/* actions using thc value of m can bc inscrtcd hcrc */ 
if thc lifctirnc of m has cndcd then 
rcturn m's register to thc p
l
 
end 
Fig. 5A4. Assigning attributc vatucs to rcgistcrs. 
Registers used during an evaluation of the dependency graph of Fig. 5.43 
are shown in Fig. 5.45. We start by evaluating node ! lo register r l .  The 
lifetime of node I ends when 2 is evaluated, so 2 is evaluated into r ~ .  
Node 3 
gets a fresh registe~ r2, because node 6 will need the value of 2. 
Fig. 5.45, Rcgiskrs uscd for attributc viilucs in Fig. 5.43. 
We can improve the method of Fig. 5.44 by treating copy ruks as a sjxclal 
case. A copy rule has the form b := c, so if the value of c is in register r, 
then the value of b already appears in register r, The number of atlributes 
defincd by copy rules can t~ significant, so we wish to avoid making explicit 
copies + 

SEC. 5.9 
ASSIGNING SPACE AT COMPII,ER-COHflRUCT[ON TIME 
323 
A set of nodes having the same value forms an equivalence dass. The 
method of Fig. 5.44 can be modified as foliows to hold the value of an 
equivalence class in a register. When node m is considered, we first check if 
it is defined by a copy rule. If it is, then its value must already be in a regis- 
ter, and m joins the equivalence class with values in that register. Further- 
more, a register is returned to the pl only at the end of the lifetimes of all 
nodes with values in the register. 
Exampie 5.24. The dependency graph in Fig. 5.43 is redrawn in Fig. 5.46, 
with an equal sign before each node defined by a copy rule. From the 
syntax-directed definition in Fig. 5.42, we find thal the type de~ermined at 
.node I i s  copied to each element in the list of identifiers, resulting in nodes 2, 
3, 6, and 7 of Fig. 5.43 being copies of I. 
Fig. 5.46. Rcgistws uxd, taking copy rulcs into account. 
Since 2 and 3 are copies of I, their values are taken from register r in Fig. 
5.46, Note that the lifetime of 3 ends when 5 is evaluated, but register r 
holding the value of 3 is not returned to the p o l  because the lifetime of 2 in 
its equivaknce class has not ended. 
The following code shows how the declaration (5.10) of Example 5.22 
might be processed by a compiler: 
In the above, x and y p i n t  to the symbol-table entries for x and y, and pro- 
cedure addtype must be called at the appropriate times to add the types of x 
and y to their symbol-table entries. 
~3 
5.9 ASSIGNING SPACE AT COMPILER-CONSTRUCTION TIME 
Although it is possible to hold ail attribute values on a single stack during a 
traversal, we can sometimes avoid making copies by using multiple slacks. In 
general, if dependencies between attributes make it inconvenient to place wr- 
tain attribute values on a stack, we can save them at nodes in an elrplicitly 

constructed syntax tree. 
We have already seen the use of a stack to hold attribute values during 
bottom-up parsing in Sections 5 3  and 5.6. A stack is also used implicitly by a 
recur~ive-descent parser to keep track of prmdure calls; this issue will be dis- 
cusged in Chapter 7. 
The UIH: of a Mack can be combined with other techniques for saving space. 
The print actions used extensively in thc translation schemes in Chapter 2 emit 
string-valued attributes to an output file whenever possible. While mnstruct- 
ing syntax trees in Section 5.2, wc passed pointers to ndes instead of corn- 
plete subtrees. In general, rather than passing large obhts, we can save 
space by passing winters to them. These techniques will be appkd in Enam- 
ples 5.27 and 5.28. 
When the evaluation wder for attributes is obtained from a particular traver- 
sal of the parse tree, we can predict the lifetimes of attributes at wmpiler- 
construction timc. For example, suppsc: children are visited from left to right 
during a depth-first traversal, as in Section 5.4. starting at a node for produc- 
tion A -. 3 C, the subtree for B is visited, the subtree Tor C is visited, and 
then we return to Ihe node for A .  The parent of A cannot refer to the attri- 
butes of B and C, 
sr, their lifetimes must end whcn we return to A .  Note that 
these observations are based on the produqton A -+ B C and the order in 
which the nodes for these nmterminals are visited. We do nol need to know 
abut the subtrees at B and C, 
With any evaluation order, if the lifetime of attribute c is; contained in that 
of b, then the value of c can be held in a stack above the value of b+ Here h 
and c. do not have to k attributes of the same nonterminal. For the produc- 
tion A +BC, we can use a stack during a dcpth-first traversal in the follow- 
ing way. 
Start at the nude for A with the inherited attributes of A already on the 
stack. Then evaluate and push the values of the inherited attributes of B. 
These attributes remain on the stack as we traverse the subtree of B. returning 
with the synthesized attributes of 3 above them. This process i s  repeated with 
C; that is, we push its inherited attributes, traverse its subtree and return with 
its synthesized attributes on top. Writing I{X) and RX) lor thc inherited and 
synthesized attributes of X. respectively, the stack now contains 
All of the attribute values needed to compute the synthesbed attributes of A 
are now on the stack, so we can return to A with the stack wntaining 
Notice that the number (and presumably the size) of inherited and syn- 
thesized attributes of a grammar symbol is fixed. Thus, at each step of thc 
above prmss we know how far d a m  i:io the stack m b v e  to reach to find 

SEC. 5.9 
ASSIGNING SPACE AT COMPI LERCONSTRUCTION TIME 325 
Example 5.25. Suppose that pttribute values for the typesetting translation of 
Fig. 5.22 are held in a stack as discusxd above. Starting at a node for pro- 
duction B -+ B , B with B.ps on top of the stack, the stack contents before and 
after .visiting a node are shown in Fig. 5+47 to the !eft and right of the node, 
respeclively . As usual stacks grow downwards. 
Fig. 5+47. Stack contents kforc and after visiting a node. 
Note that just before a node far nonterrninal B is visited for the first time, 
its ps attribute is on top of the stack. Just after the last visit, ix., when the 
traversal moves up from that n d e ,  its kr and ps attributes are in the top iwo 
positions of the stack. 
0 
When an attribute h is defined by a copy rule b := c and the value of c is 
on top of the stack of attribute values, it may not be necessary to push a copy 
of c onto the stack. There may be more opportunities for eliminaling copy 
rules if more than one stack is used to hold attribute values. In the next 
example, we use separate stacks for synthesized and inherited attributes. A 
comparison with Example 5.25 shows that more copy rules can be eliminated 
if separate stacks are used. 
Example 5%. With the syntax-directed definition of Fig. 5.22. suppose we 
use separate stacks for the inherited attribute ps and the synthesized attribute 
hi. We maintain the stacks so that B + ~ S  
is on top of the ps stack just before B 
is first visited and just after B 1s last visited. B.hf will be on top of the kt 
stack just after B is visited. 
With separate stacks we can lake advantage of both the copy rules 
B [-ps : = B.ps and B2.ps := B.ps associated with B 
B , B z .  As shown in 
Fig. 5.48, we do not need to push B , .ps because its value is already on cop of 
the stack as l 3 . p .  
A translation scheme based on the syntaxdirected definition of Fig. 5.22 is 
shown in Fig. 5.49. Operation pwh ( v ,  s) pushes the value v onto stack s and 

326 
SYNTAX-DIRECTED TRANSLATION 
Fig. 5.48. Using gpratc stacks for attribulcs ps and h# 
pop ($1 pops the value on top of stack s. We use top (3) to refer to the top ele- 
ment of stack .T+ 
u 
8 - text { pu.d(text+h x ropws), kt) } 
Fig. 5.49, Translation schcmc maintaining slacks ps and hs 
The next example combines the use of a stack for attribute values with 
actions to emit code+ 
Example 5.27. 
Here, we consider techniques for implementing a syntax- 
directed definition specifying the generation of intermediate code. The value 
of a boolean expression EandF is false if E is false. In C, subexpression F 
must not be evaluated if E is false. The evaluation of such bdean expres- 
sions is conuiderect in Seaion 8.4. 
bolean expressions In the syntax-directed definition in Fig. 5.50 are con- 
struc~td from identifiers and the and operator. Each expression E inherits 
two labels E.me and E.fulsc marking the points control must jump to if E is 
true and false, respectively. 

Mg. 5.50. Short-circuit evaluar ion of bootcan expressions. 
E 
E ,  end& 
Suppose E + E l  andE2. If E evaluates to false, then control flows to the 
inherited label E.fak; otherwise, E l  evaluates to true so control flows to the 
code for evahating E l .  A new label generated by function newlabel marks 
the beginning of the d
e
 for E l .  Individual instructions; are formed using 
gen. For further discussion of the relevance of Fig. 5 5 0  to intermediate d e  
generation see %dim 8.4. 
The syntax-directed definition in Fig. 5.50 is L-attributed, so we can can- 
struct a translation scheme for it. The translation scheme in Fig. 5.51 uses a 
procedure emir to generate and emit instructions incrementally. Also shown in 
the figure are actions for setting the values of the inherited attributes, inserted 
before the appropriate grammar symbol as discussed in Section 5.4. 
E,+true ;= rttwhbl 
E , .  fahe := E.jia/se 
E2-frw := E,~ue 
E + j a k  : = E .ju/st 
Emde := E , . c d e  11 g~n('label'E,.rrw) \ E 2 . d e  
Fig. 5.52. Emitting c d c  for bookan expressions. 
The translation scheme in Fig. 5.52 goes further; it u
~
s
 
separate stacks to 
hold the values of the inherited attributes Errus and E+falsc. As in Example 
5.26, copy rules have no effect on the stacks. To implement the rule 
E .true ;= nswldd, a new label i s  pushed onto the true stack before E is 
visited, The lifetime of this label ends with the action emit ( 'label' top Itrue)), 
corresponding to ernit('lak1' E 1 .true), so thc true stack is popped after the 
action. The false stack dues not change in this example, but it is needed when 
the or operator is allowed in addition to the and operator. 
u 

328 
SYNTAX-DLRECTeD TRANSLATION 
emit ( 'if' id,phr 'goto' top (tmt-1): 
emis ( 'goto' fop (Jdtise)) ) 
FCg. 5.52. Emitting a l e  for bootcan expressions. 
A single register is a special case of a stack. If each push operation is fol- 
lowed by a pop, then there can be at most one element in the stack at a time. 
In this case, we can use a register instead of a stack. In terms of lifetimes, if 
the lifetimes of two attributes do not overlap, their values can be held in the 
same register. 
Example 5.28. The syntax4 irected definition in Fig. 5.53 constructs syntax 
trees for list-like expressions with operators at just one precedence levcl. It is 
taken from the translation scheme in Fig. 5-28, 
Fig. 5.53. A syntax-diroctcd definition adaptad from Fig. 5.28. 
We claim that the lifetime of each attribute of R ends when the attribute 
that depends on it is evaluated. We can show that, for any parse tree, the R 
attributes can be evaluated into the same register r. The following reasoning 
is typical of that needed to analyze grammars. The induction is on the size of 
the subtree attached at R in the parse tree fragment in Fig. 554. 
The smallest subtree is obtained if R + E is applied, in which case R.s is a 
copy of R.i, so bcdh their values are in register r. For a larger subtree, the 
pduction at the root of the subtree must be for R -. addapTR,. The 

ANALYSlSOFSYNTAX-DIRECTED DEFINITIONS 329 
fig. 5.54. Dc~ndcncy graph for E -. T R .  
lifetime of R.i ends when R .i is evaluated, so R , . i  can be evaluated into 
register r. From the inductive hypothesis, all the attributes for instances of 
nonterminal R in the subtree for R 1  can be assigned the same register. 
Finally, R.s is a copy of HI.$, 
so its value is already in r. 
The translation scheme in Fig. 5.55 evaluates the attributes in the attribute 
grammar of Fig. 5.53, using register r to hold the values of the attributes R.i 
and R.s for all instances of nonterminal R. 
E - T  
( r := T.nptr f* roow holdsR+i * / )  
R 
{ E.nptr := r /* r has rcturnd with R . s  +/ ) 
R - P d d o p  
T 
{ r := mknd~Isdd+p+Iuxem~, 
r, T-nprr) ) 
R 
R - E  
T - nnrn 
{ T q r r : =  m k ~ ~ ~ n u m ,  
num.vd)) 
FIg. 5.55, Transfwmcd translation rrchcme for constructing syntax trccs. 
For mmpteteness, we show in Fig. $56 the code for implementing the 
translation scheme above; it is constructed according to Algorithm 5.2. Non- 
terminal R no longer has attributes, so R hcomes a procedure rather than a 
function, Variable r was made local to function E, so it is possible for E to be 
d e d  recursively, although we do not need to do so in the scheme of Fig. 
5.55. This code can be improved further by eliminating tail recursion and 
then replacing the remaining call of R by the body of the resulring procedure. 
as in Section 2.5. 
o 
5.10 ANALYSIS OF SY NTAX-DIRECTED DEFINITIONS 
In Section 5.7, attributes were evaluated during a traversal of a tree using a 
set of mutually recursive fuclctions. The function for a nonterminal mapped 
the values of the inherited attributes at a node to the values of the synthesized 
attributes at that node. 

330 
SYNTAX-DIRECTED TRANSLATION 
function E : t synrax-trcc-nodc; 
var r : t syntax-trcc-dc; 
addiydexcm~ : char; 
mh 
r := T; 
R 
return r 
end; 
Fig. 5.56. Comparc prmdurc R with thc oodc in Fig. 5.31. 
The approach of Section 5.7 extends to translations that cannot be per- 
formed during a single depth-first traversal. Here, we shall use a aparale 
function for each synthesized attribute of each nonferminal, although groups 
,of synthesized attributes can be evaluated by a single function. The construc- 
tion in &don 5.7 deals with the special caw in which all synthesized attri- 
butes f o ~ m  one group. The grouping of attributes is determined from the 
dependencies set up by the semantic rules in a syntaxdirected definition. The 
fdlowing abstract example illustrates the construction of a recursive evaluator, 
Example 5e29+ The syntaxdirected definition in Fig. 5.57 is motivated by a 
problem we shall consider in Chapter 6. Briefly, the prdAem is as follows. 
An 'bverloaded'' identifier can have a set of possible types; as a result, an 
expression can have a set of possible types. Context information is used to 
select one of the possible types for each subexpression. The problem can be 
solved by making a bottom-up pass to synthesize the set of possible types, f d -  
lowed by a top-down pass to narrow down the set to a singk type. 
The semantic rules in Fig. 5.57 are afi abstraction of this problem. Syn- 
thesized attribute s represents the set of pssible types and inherited aitribule i 
represents the context information+ An additional synthesized attribute r that 
cannot be evaluated in the same pass as s might represent the generated code 
or the type selected for a subexpression. Dependency graphs for the produc- 
tions in Fig. 5.57 are shown in Fig, 5.58. 
0 

ANALYSIS OF SY NTAX-DIRECTED DEFINITIONS 33 1 
Fig. 5.57. Syntbcsizeb arrributcs .T and t canna be evaluated together 
Fig. 5.58. 
Dependency graphs for productions in Fig. 5,57. 
Recursive Evaluation of Attributes 
The dependency graph for a parse tree is formed by pasting together smaller 
graphs corresp~nding to the semantic rules for a production. The dependency 
graph D, for production p is ba.sed only on the semantic rules for a single pro- 
duction, ix., on the semantic rules for the synthesized attributes of the left 
side and the inherited attributes of the grammar symbds on the right side of 
the production, That is, the graph Dp shows local dependencies only. For 
example, a11 edges in the dependency graph for E -. E Ep in Fig, 5.58 are 
between instances of the same attribute. From this dependency gaph, we 
cannot tell that the s attributes must be computed before the other attributes. 
A close look at the dependency graph for the parse tree in Fig. 5.59 shows 
that the attributes of each instance of nonterrninal E must be evaluated in the 
order E.s, E.i, E.t. Note that all attributes in Fig. 5.59 can be evaluared in 
three passes: a bottom-up pass to evaluate the s attributes, a top-down pass to 
evaluate the i attributes, and a final bottom-up pass to evaluate the i attri- 
bum+ 
In a recursive evaluator, the function for a synthesized attribute takes the 
values of some of the inherited attributes as parameters. In general, if syn- 
thesized attribute A.u can depend on inherited attribute A h ,  then the function 

332 
SY NTAX-DIRECTED TRANSLATION 
Fig. 5.99. Dcpcndency graph for a parsc trcc. 
for A.u takes A.b as a parameter. Before analyzing dependencies, we consider 
an example showing their use. 
Example 5.30. The functions Es and Et in Fig, 5.60 return the values of the 
synthesized attributes s and r at a node n labeled E+ As in Section 5.7, there 
is a case for each production in the function for a % nmtterminal. The code exe- 
cuted in each c s . ~  
simulates the semantic rules associated with the production 
in Fig. 5+57. 
From the above discussion of the dependency graph in Fig. 5.59 we know 
that attribute E.r at a node in a parse tree may depend on E.i. We therefore 
pass inherited attribute i as a parameter to the function El for attribute r. 
Since attribute E.s does not depend on any inherited attributes, function Ex 
has no parameters corresponding to attribute values. 
D 
Strongly N~ncirc~lar 
Syntax-Directed Mnitims 
Recursive evaluators can lx construcled for a d
a
~
~
 
of syntax-directed defini- 
t ions, called ''strongly noncircular " definitions. For a definition in this class, 
the attributes at each node for a nonterminal can be evaluated according to the 
same (partial) order. When we construct the function for a synthesized attri- 
bute of the nonterminal. this order is used to select the inherited attributes 
that become the parameters of the function, 
We give a definition of this class and show that the syntax-directed defini- 
tion in Fig. 5.57 falls in the class. We then give an algorithm for testing cir- 
cularity and strong noncircularity, and show how the implementation of 
Example 5.30 extends to all strongly noncircular definitions. 
Consider nonterminal A at a node n in a parse tree, The dependency graph 
for the parse tree may in general have paths that start at an attribute of node 
n, go through attributes of other nodes in the parse tree, and end at another 
attribute of n. For our purposes, it is enough to look at paths that stay within 
the part of the parse tree below A. A little thought reveals that such paths go 


334 
SY NTAX-DIRECTED TRAHSLATKIN 
SEC. 5+ 10 
write D,,IRA I ,  RAz, . . . , RA,, 1 for the graph obtained by adding edges to D, 
as follows: if RA, orders attribute A,. 6 before A,.c then add an edge from A,.b 
lo A,.c. 
A syntax-directed definition is said to be strongly nomirrular if for each 
nonterminal A we can find a partial order RA on the attributes of A such that 
for each production p with left side A and nonterminals A , ,  A2. . - - , A, 
occurring on the right side 
2. 
if there is an edge from attribute A.b to A.c in Dp IRA ,, RA 
. . . , RA,, 1, 
then RA orders A.b before A x .  
Example 5.31, Let p be the prduction E - E I E 2  from Fig. 5.57, whose 
dependency graph D,? is in the center of Fig. 5.58. Let RE be the partial order 
(total order in this case) s -. i -. r. There are two occurrences of nontermi- 
nals on the right side of p, written E l  and E z ,  as usual. Thus, REI and REI 
are the same as RE, and rhe graph D,[REIt 
REIl is as shown in Fig. 5.61. 
Fig, 5.6t. Augrncntccl dependency graph for a praductiun. 
Among the attributes associated with the root E in Fig. 5.61, the only paths: 
are from i to r. Since RE makes i precede t, there is no violation of condition 
(2) - 
0 
Given a strongly noncircular definition and partial order RA for each non- 
terminal A ,  the function for synthesized attribute .r of A takes arguments as 
follows: if RA orders inherited attribute i before s, then i is an argument of 
the function, otherwise not. 
A Circularity Test 
A syntax-directed definition i s  said 10 be circular if the dependency graph for 
some parse tree has a cycle; circular definitions are ill-formed and meaning- 
less. There is no way we can begin to compute any of the attribute values on 
the cycle. 
Computing the partial orders that ensure that a definition is 
strongly noncircular i s  closely related to testing if a definition is circular. We 
shall therefore first consider a rest for circularity. 
Example 5,32. In the fullawing syntaxdirected definition, paths between the 
attributes of A depend on which production is applied. If A -. 1 is applied, 

SEC. 5.10 
ANA LYSLS OF SY NTAX-DIRECTED DEFINITIONS 335 
then A.s depends on A.i; otherwise, it does not. For complete information 
a b u t  the possible dependencies we Ihexefoce have to keep track of sets of par- 
tial orders on the attributes of a nunterminal. 
The idea behind the algorithm in Fig. 5.62 is as follows. We represent par- 
tial orders by directed acyclic graphs. Giwn dags on the attributes OF sy mbds 
OR the right side of a production, we can determine a dag for the attributes of 
the left side as folbws, 
for grammar symbol X do 
R X )  has a single graph with thc attributes of $ and no dgcs; 
m - t  
c h n p  := f h ;  
fm production p given by A -. X ,XI - 
. Xi do begin 
Paf dags G,EWX,), . . . ,GkfRX4) do beghi 
D := D,; 
for cdge b - rb ia Gj, 
I sjsk do 
add an edge in D between attributes b and r of X,: 
iCD has a cycle tbeo 
fail th& circularity test 
e h  bqgin 
G := a ncw graph with ncldcs for thc attributcs 
of A and no cdgcs; 
for c a d  pair of attributcs b and c of A do 
if thsrrs is a path in D from b to r then 
add b -. c to C; 
is not alrcady in 4[A) then begin 
add G to %A ); 
rhflnp := trw 
Fig. 5.62. 
A circulari~y test. 
Let production p be A - X ,XI - 
+ - Xk with depebdency graph D,. Let D, 
be a dag for Xj. lsjjzk. Each edge 6 - u  in Dj in temporarily added in to 

336 SYNTAX-DIRECTED 
TRANSLATION 
SEC. 5.10 
ihc dependency graph D, for the production. If the resulting graph has a 
cycle, then the syntaxdirected definitiion is circular. Otherwise, paths in the 
resulting graph determine a new dag on the attributes of the left side of the 
production, and the resulting dag is added to $(A). 
The circularity test In Fig. 5.62 takes time exponential in the number of 
graphs in the sets 9(X) for any grammar symbol X+ There are syntax-directed 
definitions that cannot be tested for circularity in polynomial time. 
We can convert h e  algorithm in Fig. 5.62 into a more efficient test if a 
syntax-directed definition is strongly noncircular, as follows. lnstead of main- 
taining a family of graphs 9(X) for each X, we summarize the information in 
the family by keeping a single graph F(X). Note that each graph in 9 ( X )  has 
the same nodes for the attributes of X. but may have different edges. F { X )  is 
the graph on the nodes far the attributes of X that has an edge between X.b 
and X.c if any graph in %X) dws. F ( X )  represents a "worst-case estimate" 
of dependencies between attributw 'of X. In particular, if F(XJ is acyclic, then 
the syntax-directed definition is guaranteed to be noncircular. However, the 
converse need not be true; ix,, if F I X )  has a cycle, it is not necessarily true 
that the syntax-directed definition is circular. 
The modified circularity test constructs acyclic graphs FIX) for each X if it 
succeeds. From these graphs we can construct an evaluator for the syntax- 
directed definition. The method is a straightforward generalization of Exam- 
ple 5.30. The function for synthesized attribute X+s takes as atgunrents all 
and only the inherited attributes that precede s in FCX). The function, caltd 
at node n, calis other functions that compute the needed synthesized attributes 
at the children of n. The routines to compute these attributes are passed 
values for the inherited attributes they neeb. The fact that the strong noncir- 
cularity test succeeded guarantees that these inherited attributes can be corn- 
pu ted , 
5.1 For the input expression (4+7+ 1 ) +2, construct an annotated parse 
tree according to the syntax-directed definition of Fig. 3.2 
5,f Construct the parse tree and syrttax tree for the expression 
(Ia>+lb) 
1 according to 
a) the syntaxdirected definition of Fig. 5,9, and 
h) the translation scheme of Fig. 5.28. 
5.3 Construct the dsg and identify the value numbers for the sukpres- 
sions of the following expression, assuming + associates from the left: 
a+a+ta+a+a+(a+a+a+a))+ 
*5.4 Give a synt ax-directed definition to translate infix expressions into 
infix expressions without redundant parentheses. For example, since 
+ and + associate to the left, I ( a* I b+e ) ) * (d ) ) can be rewritten as 
a* (b+ct)*d. 

CHAPTER 5 
EXERCISES 337 
5,s Give a syntax-directed definition to differentiate expressions formed 
by applying the arithmetic operators + and + io the variable x and 
constants; e.g., x* [ 3*x + x*xl. Assume that no simplification takes 
place, so 3*x translates into 3+? + O*x. 
5.6 The following grammar generates expressions formed by applying an 
arithmetic operator + to integer and real constants. 
When two 
integers are added, the resulting type is integer, otherwise, it is reah 
a) Give a syntax-directed definition to determine the type of each 
subexpression. 
b) E~tend the syntax-directed definition of (a) to translate expres- 
sions into postfix notation as well as determining types. Use the 
unary operator inttmal to convert an integer value into an 
equivalent real value, so that both owrands of + in the postfix 
form have the same type. 
5 7  Extend the syntax-directed definition of Fig. 5.22 to keep track of the 
widths of boxes in addition to keeping track of their heights. Assume 
that terminal text has synthesizcd attribute w giving the normalized 
width of the text. 
5-8 Let synthesized attribute v d  give the value of the binary number gen- 
erated by S in the following grammar, For example, on input 
W1.101, S . v d  = 5.625 
a) Use synthesized attributes to determine S . v d  
b) Determine S , v d  with a syntax-directed definition in which &he only 
synthesized attribute of B is r, giving the contribution of !he bit 
generated by B to the final value. For example, the contribution 
of the first and last bits in 10 1 - 1 0  4 to the value 5,625 is 4 and 
0.125, respectively. 
5.9 Rewrite the underlying grammar in the syntax-directed definition of 
Example 5.3 so that type information can be propagated using syn- 
thesized attributes alone. 
*5,10 When statements generated by the fotlowing grammar are translated 
into abstract machine code, a break statement translates into a jump 
to the instruction following the nearest enclosing while statement. 
For simplicity, expressions are represented by the terminal expr and 
other kinds of statements by the terminal other. These terminals 
have a synthesized attribute code giving their translation. 

338 
SYNTAX-DIRECTED TRANSLATION 
CHAFTER 5 
Give a syntax-d irected definition translating statements into code for 
the stack machine of Section 2.8. Make sure that break statements 
within nested while statements are translated correctly. 
5.11 Eliminate left recursion from the syntax-directed definitions in Exer- 
cise 5.6Ia) and (b). 
5.12 Expressions generated by the following grammar can have assign- 
ments within them. 
S - E  
E + E  : = E  ( E + & I I E ) l i d  
The semantics of expressions are as in C. That is, b; =c is an expres- 
sion that assigns the value of c to b; the r-value of this expression i s  
the same as that of c. Furthermore, a: ={ b: =c) assigns the value 
of c to b and then to a+ 
a) Construct a syntax-directed definition for checking that the kft 
side of an expression i s  an 1-value. Use an inherited attribute side 
of nontcrminal E to indicate whether the expression generated by 
E appears on the left or right side of an assignment. 
b) Extend the syntax-directed definition in (a) to generate intermedi- 
ate code for the stack machine of Section 2.8 as it checks the 
input. 
5.13 Rewrite the underlying grammar of Exercise 512 so that it groups the 
subexpressions of : = to the right and the subexpressions of + to the 
left + 
a) Construct a translation scheme that simulates the sy ntax-directed 
definition of Exercise 5.12Cb). 
b} Modify the translation scheme of (a) to emit code incrementally to 
an output file. 
5.14 Give a translation scheme for checking that the same identifier does 
not appear twice in a list of identifiers. 
5-15 Suppose declarations are generated by the following grammar. 
D + i d L  
L +  , i d L ]  : T  
T -. integer ( real 
a) Construct a translation scheme to enter the type of each identifier 
into the symbol table, as in Example 5.3. 

CHAPTER 5 
EXERClSES 339 
b) Construct a predictive translator from the translation scheme in 
(a). 
516 The following grammar is an unambiguous version of the underlying 
grammar in Fig. 5.22. The braces { 1 are used only for grouping 
boxes, and are eliminated during translation. 
a) Adapt the syntax-directed definition in Fig. 5.22 to use the above 
grammar. 
b) Convert the syntax-directed definition of (a) into a translation 
scheme. 
w5.17 Extend the transformation for eliminating kft recursion in Sectim 5.5 
to allow the following for nmterminal A in (5.2). 
a) lnherited attributes defined by mpy rules+ 
b) lnherited attributes. 
5-18 Eliminate left recursion from the translation scheme of Exercise 
5. l q b )  
V.19 Suppose we have an L-attributed definition whose underlying gram- 
mar is either LL(I), or one for which we can resolve ambiguities and 
construct a predictive parser. Show that we may keep inherited and 
synthesized attributes on the parser stack of a topdown parser driven 
by the predictive parsing table. 
*5.20 Prove that adding unique marker nonterminals anywhere in an LL(1) 
grammar results in sr grammar that is LR(1). 
5.21 Consider 
the following modificatbn 
of 
the LR(I) grammar 
L 4L.b I a: 
a) What order would a bottom-up parser apply the prductions in 'the 
parse tree for the input string ubbb? 
*b) Show that the modified grammar is not LRU). 
'5.22 
Show that in a translation scheme based on Fig. 5.36, the value of 
inherited attribute B.ps is always immedia~ely below the right side, 
whenever we reduce a right side to B. 
5.23 Algorithm 5.3 fa bottom-up parsing and translation with inherited 
attributes uses marker nonterminals to hold the values of inherited 
attributes at predictable pasitions in the parser stack. Fewer markers 

340 SYNTAX-DIRECTED TRANSLATION 
CHAPTER 5 
may be needed if the values are placed on a stack separate from the 
parsing stack. 
a) Convert the syntax4irected definition in Fig. 5.36 into a transla- 
tion scheme. 
b) Mdify the translation scheme mnstructed in (a) so that the value 
of inherited attribute pps appears on a separate stack. Eliminate 
marker nonierminal M in the process. 
*S,M Consider translation during parsing as In Exercise 5.23. $. C. John- 
wn suggests the following method for simulating a separate stack for 
inherited altributes, using markers and a global variable for each 
inherited attribute. In the fdbwing production, the value v i s  pushed 
onto stack i by the f i t  action and is popped by the second action; 
Stack i can be simulated by the foll~wing productions that use a global 
variable g and a marker nonterminal M with synthesized attribute s: 
a) Apply this transformation to the translation scheme of Exercise 
5+23(b). Replace all references to the top of the separate stack by 
references to the global variable. 
b) %ow that the translation scheme constructed in (a) computes the 
same values for the synthesized attribute of the start symbol as 
that in Exercise 5.23Ib). 
5.25 Use the approach of Section 5.8 to implement all the E.side attributes 
in the translation scheme of Exercise 5.12(bj by a single b d e a n  vari- 
able. 
5.26 Modify the use of the stack during the depth-first traversal in Exam- 
ple 5.26 so that the values on the stack correspmd to those kept on 
the parser stack in Example 5.19. 
The use of synthesized attributes to specify the translation of a language 
appears in Irons 119611. The idea of a parser calling for semantic actions i s  
dkussed by Sarnelson and Bauer f19601 and Brouker and Morris 11962). 
Along with inherited attributes, dependency graphs and a test for strong non- 
circularity appear in Knut h 1 i968] - 
a test for circularity a p p r s  in a correc- 
tion t~ the paper. The extended example in the paper uses disciplined side 
effects to global attributes attached to the root of a parse tree. If attributes 
can be functions, inherited attributes can be eliminated; as done in denota- 
tbnal semantics, we can associate a function from inherited to synthesized 
attributes with a nonterminal. Such observations appear in Mayoh [19811. 

CHAPTER 5 
BIBLIOGRAPHLC NOTES 
34 1 
One application in which side effects in semantic ruks are undesirable is 
syntax-directcd editing. Suppose an editor is generated from an attribute 
grammar for the source language, as in Reps 1 19841, and consider an editing 
change to the source program that results in a portion of the parse tree for the 
program being deleted. As long as there are no side effects. attribute values 
for the changed program can be recomputed incmmeotally. 
Ershov ( 19581 uses hrrshing to keep track of common subexpressions. 
The definition of L-attributed grammars in Lewis, Rosenkranrz, and Stea~ns 
119741 is motivated by translation during parsing. Similar restrictions on attri- 
bute dependencies apply to each of the left-to-right depth-first traversals in 
&chmann 
119761. 
Affix grammars, as introduced by Koster 11971). are 
related to L-attribu ted grammars. Restrict ions on L-atrributed grammars are 
proposed in Koskimies and Raihi 1 19831 to control access to global attributes. 
The mechanical construction of a predictive trandator, similar to those con- 
structed by Algorithm 5.2, is described by Bochrnann and Ward 11978j. The 
impression that top-down parsing ailow more flexibility for translation is 
shown to k false by a proof in Brosgd 1 i9741 that a transIation scheme based 
on an LL(1) grammar can be simulated during LR(I) parsing. Independently, 
Watt 11 977 j uwd marker nonterminais to ensure that the values of inherited 
attributes appear on a stack during bottom-up parsing. Positions on the right 
sides of productions where marker nwnlerrninals can safely k inserted without 
losing the LR( 1) property arc considered in Purdom and Brown )1980) (see 
Exercise 5,211. Simply requiring inherited attributes to be defined by copy 
rules is not enough tu ensure that attributes can be evaluated during bottom- 
up pa~sing; sufficient conditions on semantic rules are given in Tarhia 119821, 
A characterization, in terms of parser states, of attributes that can be 
evaluated during LR{ I) parsing, is given by Jones and Madsen {1980J+ As an 
example of a translation that cannot be done during parsing, Giegerich and 
Wilheh 119781 consider code generation for boolean expressions. We shall 
see in Section 8+6 thal backpatching can be used for this problem, so a corn- 
plete second pass is n d  necessary, 
A number of tools for implementing syntax-directed definitions have been 
developed, starting with FOLDS by Fang 119721, but few have seen 
widespread use. DELTA by Lorha [ 19771 constructed a dependency graph at 
compik time. It saved space by keeping track of the lifetimes of attributes 
and eliminating copy rules. Parsetree based attribute evaluation met hods are 
discussed by Kennedy and Ramanathan 119791 and Cohen and Harry 119791, 
Attribute evaluation methods are surveyed by Engelfriet 1 19844. A compan- 
ion paper by Courcdle ( 19841 surveys the theoretical foundations. HLP. 
described by Riiha ci al. [ 1983 1, makes alternating depth-first traversals, as 
suggested by Jazayeri and Walter 118751. LINGUIST by Farrow 119841 also 
makes alternating passes. Canzinger et al. 1 19821 report that MUG aIbws the 
order In which children uf s node are visited to be determined by the produc- 
tion at the n d c .  GAG, due to Kastens, Hutt, and Zirnrnerman 1 19821 allows 
repeated visits to children of a node. GAG implements the ctass of ordered 

342 
SYNTAX-DIRECTED TRANSLATlON 
CHAPTER 5 
attribute grammars defined by Kastens 119801. The idea of repeated visits 
appears in the earlier paper by Kennedy and Warren lt976], where evaluators 
for the larger class of strongly noncircular grammars are constructed. h a r -  
inen 119731 describes a modification of Kennedy and Warren's method that 
saves space by keeping attribute values on a stack if they are not needed dur- 
ing a later visit. An implementation described by Jourdan I19841 constructs 
recursive evaluators for this class. Recursive evaluators are also constructed 
by Katayarna [ L984J. A quite different approach is taken in NEATS by Mad- 
sen 11 9801, where a dag is consiructed for expressions representing attribute 
values. 
Analysis of dependencies at compilerconstrrrcric3n time can save time and 
space at compile time. Testing for circularity is a typical analysis probkm, 
Jazayeri, Ogden, and Rounds 119751 prove that a circularity test requires an 
exponential amount of time as a function of grammar sik. Techniques for 
improving the implementation of a circularity test are considered by Lorho 
and Pair 11975), Raiha and Saarinen 119821, and Deransart, Jourdan, and 
Lor ho 1 19841, 
The space used by naive evaluators has led to the development of tech- 
niques for conserving space. The algorithm for assigning attribute values to 
registers in Section 5.8 was described in a quite different context by Marill 
11962). The problem of finding a toplogical sort of the dependency graph 
that minimizes the number of registers used is shown to be NP-complete in 
Sethi 1 19751. Compile-t h e  analysis of lifetimes in a multi-pass evaluator 
appars in Raiha 11981 1 and Jazayeri and Pozefsky I19811. Branquart et ah 
11976( mention the use of separate stacks for holding synthesized and inher- 
ited at tributes during a traversal. GAG performs lifetime analysis and places 
attribute values in global variables, stacks, and parse tree nodes as needed. A 
comparison of  he space-saving techniques used by GAG and LINGUIST is 
made by Farrow and Yellin 119841. 

CHAPTER 6 
Type 
Checking 
A compiler must check that the source program follows both the syntactic and 
semantic conventions of the source language. This checking, called stutir 
checking (to distinguish it from dynamic checking during execution of the tar- 
get program), ensures that certain kinds of programming errors will be 
detected and reported. Examples of static checks include: 
Type checks. A compiler shuuld report an error if an operator is applied 
to an incompatible operand; for examp1e;if 
an array variable and a func- 
tion variable are added together. 
Flow-of-cvnml checks. Statements that cause flow of control to leave st 
construct must have some place to which to transfer the flow of control. 
For example, a break statement in C causes control to leave the smallest 
endosing while, for, or switch statement; an error occurs if such an 
enclosing statement does not exist. 
Uniqueness chrch. There are situations in which an object musl be 
defined exactly once. For example, in Pasca!, an identifier must be 
deciared uniquely, labels in a case statement must be distinct. and ele- 
ments in a scalar type may not be repeated. 
Nume-reluted chucks. Sometimes, the same name must appear two or 
more times. For example, in Ada, a Imp or block may have a name that 
appears at the beginning and end of the construct. The compiler must 
check that the same name is used at both places. 
In this chapter, we fwus on type checking. As the above examples indicate, 
most of the other static checks are routine and can be implemented using the 
techniques of the last chapter. Some of them can be folded into other activi- 
ties. For example, as we enter information about a name into a symbol table, 
we can check that the name is dedared uniquely. Many Pascal compilers 
combine static checking and intermediate wde generat ion with parsing. With 
more complex constructs, like those of Ada, it may be convenient to have a 
separate type-checking pass between parsing and intermediate code generation, 
as indicated in Fig. 6.1. 
A type checker verifies that the type of a construct matches that expected by 

344 
TYPE CHECKlNG 
its context. For example, the built-in arithmetic operator mad in Pascal 
requires integer operands, so a type checker must verify that the operands of 
mod have type integer. Similarly, the type checker must verify that dere- 
ferencing is applied only to a pinter, that indexing is bone only on an array, 
that a user-defined function is applied to the correct number and type of argu- 
ments, and so forth. A specification of a simple type checker appears in See 
tion 6.2. The representation of types and the question of when two types 
match are discussed in !kction 6.3. 
Type information gathered by a type checker may be needed when c d t  is 
generated. For example. arithmetic operators like + usually apply to either 
integers or reals, perhaps to dher types, and we have to look at the context of 
+ to determine the sense that is intended. A symbol that can represent dif- 
ferent operations in different mntexts is said to be "overloaded." Overload- 
ing may be accompanied by coercion of types, where a compiler supplies an 
operator to convert an operand into the type expected by the context. 
A distinct n d i m  from overloading is that of "polyrnqhism." The body of 
a polymorphic function can be execuled with arguments of several types. A 
unification algorithm for inferring types of polymorphic functions mcludcs 
this chapter, 
lokcn 
O'P 
st rcarn 
c k d e r  
6.1 TYPE SYSTEMS 
The design of a type checker for a language is based on information about the 
syntactic constructs in the language, the notion of types, and the rules for 
assigning types to language constructs, The following excerpts from the Pas- 
cal report and the C reference manual, respectively, are examples of informa- 
tion that a compiler writer might have to start with, 
Fig. 6.1. Position of typc chcckcr. 
syntax 
tree - 
"lf bwth operands of the arithmetic operators of addition, subtraction and 
multiplication are of type integer, then the result is of type integer." 
a 
'The result of the unary 6 operator i s  a pointer to the object referred to 
by the operand. If the type of the operand is '..,', the type of the result 
is 'pointer to . . .'." 
intermdim 
cmdc 
generator 
Implicit in the above excerpts is the idea that ecach expression has a type 
intermdiale 
~ t ~ ~ r c s c n t a t i c i n  

SEC. 6.j 
TYPE SYSTEMS 345 
associated with it. Furthermore, types have structure; the type "pointer lo 
. . ." is c ~ n s t r ~ c t e d  
from the type that ". . ." refers to. 
In both Pascal and C, types are either basic or constructed. Basic types are 
the atomic types with no internal structure as far as the programmer is con- 
cerned. In Pascal, the basic types are boolean, charmer, inicger. and real. 
Subrange types, like 1 . . 10, and enumerated types, like 
can be treated as basic types. Pascal allows a programmer to construct types 
from basic types and other constructed types, with arrays, records, and sets 
beiog examples. In addition, pointers and functions can also ke treated as 
constructed types. 
The type of a language cunstruct will be denoted by a "type expression." 
Informally, a type expression is either a basic type or is formed by applying an 
operator called a type cumscrucror to other type expressions. The sets of basic 
types and constructors depend on the language to be checked. 
This chapter uses the following definition of v p c  exprtssions: 
1. 
A basic type is a type expression. Among the basic types are boolean, 
c h r ,  integer, and real. A special basic type, Qpdrrur, will signal an 
error during type checking. Finally, a basic type void denaing "the 
absence of a value" albws statements to be c h d e d +  
2. 
Since type expressions may be named, a type name is a type expression. 
An example of the use of type names appears in 3(c) below; type cxpres- 
sions containing names are discussed in Section 6.3. 
3. 
A type constructor applied to type e~pressions is a type expression. Con- 
structors include: 
a) 
Arrays. If T is a type expression, then array(!, T) is a t
y
~
 
expres- 
sion denoting the type of an array with elements of type T and index 
set I+ I is often a range of integers. 
For example, the Pascal 
declaration 
var A: array[ 1. . l a  1 of integer ; 
associates the type elrpressim array ( 1 + .10, integ~r) with A. 
b) Products. If TI and T2 are type e~pressions, then their Cartesian 
product TI 
X TI is a type expression. We assume that x associates 
to the left. 
C )  
~ e r o r d ; .  The difference be~ween n record and a product is that the 
fields of a record have ntimes. The record type constructor will k 
applied to a tuple formed from field names and field types. (Techni- 
cally, the fitld names should be part of the type constructor, but it is 

346 
TYPE CHECKING 
SEC. 6.1 
convenient to keep field names together with their associated types. 
In Chapter 8, the type constructor record i s  applied to a pointer to a 
symbol table containing entries for the field names.) For example, 
the Pascal program fragment 
type row a record 
address: integer; 
lexeme: array [1..151 of char 
end ; 
var 
table: array 11..1011 of row; 
declares the type name row representing the type expression 
record ((address x integer) x lexeme x array ( 1.. IS, c h r ) ) )  
and the variable table to lx an array of records of this type. 
Pointers, If T is a type expression, then pointr(T) is a type exprcs- 
sion denoting the type "pointer to an object of type T." For exam- 
pk, in Pascal, the declaration 
var g :  t row 
declares variable p to have type pointer (row). 
Functions, Mathematically, a function maps dements of one set, the 
domain, to another set, the range. We may treat functions in pro- 
gramming languages as mapping a domuin ype D to a range ope R. 
The type of such a function will be denoted by the type expression 
D -+ R. For example, the built-in function mad of Pascal has domain 
type int x in#, ix., a pair of integers, and range type iot. Thus. we 
say mod bas the type' 
As another example, the Pascal declaration 
function fIa, b: char) : f integer; . . .  
says that the domain type of f is denoted by char x char and range 
type by pinr~r(irzr~gw). 
The type of f is thus denoted by the lype 
expl ession 
Often, for implementation reasons discussed in the next chapter, 
there are limitations on the type that a fundion may return; e.g+, no 
arrays or functions may be returned. However, there are languages, 
d which Lisp is the most prominent example, that allow functions to 
' We assume lhat X has higher precedence than -, 
so inr x inr -inr 
is the same as 
Iirrr x int) -. iru. A b ,  -. associates to the right. 

SEC. 6.1 
TYPE SYSTEMS 347 
return objects of arbitrary iypes, so, for example, we can define a 
function g of type 
That is, g takes as argument a function that maps an integer to an 
integer and g produces as a result another function of the same type. 
4. Type expressions may contain variables whose values are type expres- 
sions. Type variables will be introduced in Section 6.6, 
A convenient way to represent a type expressions is to use a graph- Using 
the syntaxdirected approach of Section 5.2, we can construct a tree or a dag 
for a type expression, with interior nodes for type constructors and leaves for 
basic types, type names, and type variables (see Fig, 6.2). Examples of 
representations of type expressions that have been used in compilers are given 
in Section 6.3. 
A ope . y e m  is a collection of rules for assigning type expressions to the vari- 
ous parts of a program. A type checker implements a type system, The type 
systems in this chapter are specified in a synlax-bireded manner, so they can 
k readily implemented using ihe techniques of the previous chapter. 
Different type systems may be wed by different compilers or processors of 
the same language. For example, in Pascal, the type of an array includes the 
index set of the array, so a function with an array argument can only be 
applied to arrays with that index set. Many Pascal compilers, however, allow 
the index set to be left unspxified when an array is passed as an argument. 
Thus these compilers use a different type system than that in the Pascal 
language definition. 
Similarly, in the UNIX system, the lint command 
examines C programs for possible bugs using a more detailed type system than 
the C compiler itself uses, 
Static and Dynamic Checking of Types 
Checking done by a compiler is said to lx static, while checking dme when 
the target program runs i s  termed dynamic. In principle, any check can be 
done dynamically, if the target code carries the type of an element along with 

348 
TYPE CHECK ING 
SEC. 6, I 
the value of that element. 
A sound type system eliminates the need for dynamic checking fur type 
crrrsrs because it allows us to determine statically that lhese errors cannot 
occur when thc target program runs. That is, if u sound t y p  system assigns a 
type other than tyc-error tu a program part, then type errors cannot. occur 
when the target code Tor the program part Is run, A language is srrongl~ ~ywd 
if its compiler can guarantee that the programs it accepts will execute without 
type errors. 
In practicc. wmc checkr can be done only dynamically. For cxamplc, if we 
first declare 
table: array[0..2553 of char; 
i: integer 
and then compute table[ i], a compiler cannot in general guarantee that 
during execution, the value of i will lie in the range 0 to 255.' 
Error Recovery 
Since typc checking has the ptential for catching errors in programs. it is 
important for a type checker to do mmething reasonable when an error is 
discovered. At the very least, thc compiler must report the nature and loca- 
tion of the error. It is desirable fur the type checker to recover from errors, 
so i t  can check the rest of the input + 
Since error handling affects the type- 
chccking rules. it has to bc designed into the type system right from the slart; 
the rules must te prepared to cope with er'rors. 
The inclusion of error handling may result in a type system that goes 
beyond rhe one needed to specify correct programs. For example, oncc an 
error has occurred, we may not know the type of the incorrectly formed pro- 
gram fragment. Coping with missing information requircs techniques similar 
to those needed for languages that do not require identifiers to be declared 
before they are uscd. Type variables, discussed in Section 6 h .  can be used to 
to ensurc consistent usage of undeclared or apparently misdeclarcd identifiers. 
6.2 SPEClFICATION OF A SlMPLE TYPE CHECKER 
In this ~ c t i o n ,  we specify a type checker for a simple languagc In which the 
typc of cach identifier must te declared behre the identifier is uwd. The 
type checker is a translation scheme that synthesizes the type of each exprts- 
sion from the types of its subexpressions. The type checker can hmdk 
arrays, pointers, statements, and functions. 
Lhlil-tlw irnaly\ih icchniyuu-, himilar to [ h o ~  
in Chap~cr I 0  can bc c r ~ d  
to infcr i i  i ia withln 
hvund5 in u w c  pnlprams. Huwcvcr, nu rcchniquc! c;ln n l i ~ k t  rhc Jcciailm arrccrly in all caws. 

SPECIFICATION OF A SIMPLE TYPE CHECKER 
349 
A Simple Language 
The grammar in Fig. 6.3 generates programs, represented by the nonterrninal 
P, consisting of a sequence of declarations D followed by a single expression 
E. 
P - D ; E  
D 4 D ; D  1 
i d ; T  
T - char I integer I array I nurn 1 of T 1 
1 T 
E 
-+ literal 1 num I id I E mud E I E [ E 1 
1 E t 
One program generated by the grammar in Fig. 6.3 is: 
key: integer; 
key mod I999 
Before discussing expressions, consider the types in I he language. The 
language itself has two basic types, chur and i t t r q p r ;  a third basic type 
[vp'tcwor is used to signal errors. For simplicity, we assume  hii it all arrays 
start at I. For example, 
array [ 2 5 6 ]  of char 
leads to the type expression orrrry( 1 ..256. char) consisting of the constructor 
urruy applied to the subrange 1 .  .256 and the type c . l r w .  As in Pascal, the pre- 
fix operator t in declarations builds n pointer typc, so 
1 integer 
leads to thc rypc expression poitrrer(ititqt.r). consisting of the constructor 
p i n i ~ r  
applied to the type inrCpr+ 
In thc translation scheme of Fig. 6.4, 
the action associated with the produc- 
tion L) -c id : T saves a type in a symbol-table entry for an identifier. The 
action nddiype I iden@. T.typc. ) is applied to synthesized attribute unrry pinr- 
ing ro the symbol-table entry for id and 'a type expression reprcscnted by syn- 
thesized attribute rypc of nonterrninal T+ 
If T generates char ur inlqger, then T.rype is defined 10 be c*hr or imgw. 
respectively. The upper bound of an array is obtained from the attribute vul 
of token Rum [hat gives the integer represen led by num. Arrays arc assumed 
to start at 
1, srj the type constructor urrrry is applied to the subrange 
I ..num.vd and the clement type. 
Since D appears M o r e  E un the right side nf P + D ; E, we can k sure 
that the typcs ol' 311 declared identifiers  ill he snved before the cxpresvion 
generated by E is checked. ( k c  Chapter 5 . )  In fact, by suitably modifying 
 he grammar in Fig. 6.3, we can implement the translation schemes in this 
section during either top-down or bortom-up parsing, if desired, . 

350 TYPE CHECKING 
Fig. 1.4. Thc part of a translation schtrnc that sivcs Ihc Iyv of an identifier 
Type Checking of Expressions 
In the following rulcs, the synthesized attribute type for E gives the type 
expression assigned by the type system to the expression generated by E. The 
following semantic rules say that constants represented by the tokens literal 
and num haw type rhw and imcger, respectively: 
We use a function lwkrrpIc) to I c ~ h  
the type saved in the symbol-table 
entry pintcd to by c. When an identifier appears in an cxprsssion, its 
declared type is fetched and assigned to the attribute type; 
E + i d  
{ E.rype := lookup (id.enrry) ) 
The expression formed by applying the mod operatat to two subexpressions 
of type imger has type intrgw; otherwise, its type i s  qpc-error. The rule is 
In an array reference E I E2 1, the index e~pression E2 must have type 
integer, in which casc the result is the dement type s. obtained from the type 
rrrruy(s. I )  of E 1; we make no use of [he index set s of the array. 
Within e~prcssions, the postfix operator t yields the object pointed to by its 
operand. The type of E t is the type t of the object pointed to by the pointer 
E: 
We lcavc it to the rcader to add product ions and scmantic rules lo permit 

SEC. 6+2 
SPEClFlCATlON OF A SIMPLE TYPE CHECKER 
351 
additional types and operations within expressions, For example, to allow 
identifiers to have the type bodean, we can introduce the production 
T - boolean to the grammar of Fig. 6.3. 'The introduction of comparison 
operators like < and bgical connectives like and into the productions for E 
would allow the construction of expressions of t y p  bc)iron. 
Type Checking of Statements 
Stnce language constructs like statements typically do not have values, the spe- 
cial basic type void can lx assigned to them. If an error is detected within a 
statemen!. the type assigned 10 the statement is typeerror. 
The statements we consider are assignment, conditional, and while state- 
ments, Sequences of statements are separated by semimIons + 
The produc- 
tions in Fig. 6.5 can be combined with those of Fig, 6.3 if we change the pro- 
duction for a complete program rrs P -, D ; $. A program now consists of 
declarations followed by statements; the above rules for checking expressions 
are still needed becaux statements can have expressions within them. 
Fig, 6 5  Translation scheme for checking the lype of statcments. 
Rules for checking statements are given in Fig. 6.5. The first rule checks 
that iht left and right sides of an assignment statement have the u m e  type." 
The second and third rules specify that expressions in conditional and while 
statements must have type h k a n .  Errors are propagated by the last rule in 
Fig 6.5 because a sequence of statements has type void only if each sub- 
statement has type void. Tn these rules, a mismatch of types produces the type 
rype~?rror; a friendly type checker would, of course, report the nature and 
location of the type mismatch as well. 
- 
.- 
If an expression is allowd uri thc lcfl sidc of rm assignment, then wc a
h
 haw to distinguish 
between 1-values and r+valucs. For exampk, 1 : = 2 is incorrect bccausc thc cansiant f cannot k 
adgned lo. 

352 
TYPE CHECKING 
SEC. 6.2 
T y p  Checking d Funrtions 
The application of a function to an argument can be captured by the produc- 
tion 
in which an expression 1s the application of one expression to another. The 
rules for associating type expressions with nonterminal Tcan be augmented by 
the following production and action to permit function types in declarations. 
Quotes around the arrow used as a function constructor distinguish it from the 
arrow used as the metasyrnbol in a production. 
The rule for checking the type of a function application is 
This rule says that in an expression formed by applying E to E z ,  the type of 
E must be a function s + r from the lype s of E2 to some range type r; the 
type of E l  { E l  1 is i. 
Many issues related to lype checking in the presence of functions can be dis- 
cussed with respect to the simple syntax above. . The generalization to func- 
tions with more than m e  argument is done by constructing a product type 
consisting of the arguments. Note that n arguments of type TI, . . + , T, can 
be viewed as a single argument of type T I  
X . - - AT,+ For example, we 
might write 
root : (real - real 1 x real 
+ real 
(6.1 
) 
to declare a functian root that takes a function horn reals to reds and a real 
as arguments and returns a real. Pascal-like syntax for this declaration i s  
function root Ifunction f (real): real; x: real): real 
The syntax in (6.1) separates the declaration of the type of a function from 
the names of its parameters. 
6.3 EQUIVALENCE OF TYPE EXPRESSIONS 
The checking rules in the last section have the form, "if two type expressions 
are equal then return a certain type else return lype-errvr." 
It Is therefore 
important to have a precise definition of when two type expressions are 
equivalent + 
Potential ambiguities arise when names are given to type exptes- 
sions and the names are then used in subsequent type expressions, The key 
issue is whether a name in a type expression stands for itself or whether it is 
an abbreviation for another type expression. 
Since there is interaction between the notion of equivalence of types and the 

representation of types, we sfid1 ta!k about both together. For efficiency, 
compilers w e  representations that allow type equivalence to be determined 
quickly, The notion of type equivalence implemented by a specific compiler 
can often be explained using the concepts of structural and name equivalence 
discussed in this section. The discussion is in terms of a graph representation 
of type expressions with leaves for basic types and type names, and interior 
nodes for type mnstructors, as in Fig+ 6.2, As we shall see, recursively 
defined types lead to cycles in the type graph if a name is treated as an abbre- 
viation for a type expression. 
Strudurd Equivalence of Type Expressi~ns 
As long as type expressions are built from basic types and constructors, a 
natural notion of equivalence between two type expressions is strucrurd 
quivaimce; i .e., two expressions are either the same basic type , or are formed 
by applying the same constructor to strucrurally equivalent types. That is, rwo 
type expressions are structurally equivalent if and only if they are identicaL 
For example, the type expression inreger is equivalent only to integer because 
they are the same basic type. Similarly, pointer (integer) is equivalent only to 
poi~ter(integer) because the two are formed by applying the same constructor 
pointer to equivalent types, If we use the value-number method sf Algorithm 
5.1 to construct a dag representation of type expressions, then identical type 
expressions will be represented by the same node. 
Modifications of the notion of structural equivalence are often needed in 
practice to reflect the actual typchecking rules of the source ianguage, For 
example, when arrays are passed as parameters, we may not wish to include 
the array bounds as part of the type. 
The algorithm far testing structural equivalence in Fig. 6.6 can be adapted 
to test modified notions of equivalence. I t  assumes that [he only type con- 
structors are for arrays, product s, pointers, and functions. The algorithm 
recursively compares the structure of type expressions wirho~t checking for 
cycles so it can be appiied to a tree or a dag representation. ldentical type 
expressions do not need to be represented by the same node in the dag. Sruc- 
turai equivalence of rides in type graphs with cycles can be tested using an 
algorithm in Section 6.7. 
The array bounds a l  and i in 
are ignored if the test for array equivalence in lines 4 and 5 of Fig. 6.6 is 
reformulated as 
In certain situations, we can find a representation fw type expressions that 
is significantly more compact than the type graph notation. In the next 

354 
TYPE CHECKING 
Fig. 6.6. 
Testing thc struclural cyuivalence of two typc c~prcsshns .r and t. 
example, some of the information from a type expression is encoded as a 
sequence of bits, which can then te interpreted as a single integer. The 
encoding is such that distinct inlegers represent structuraily inequivalent type 
expressions. The test for structural quivalence can be accelerated by first 
testing for structural inequivalence by comparing the integer representations of 
the types, and then applying the algorithm of Fig. 6.6 only if the integers arc 
the same. 
Example 6.1, The encoding of type expressions in this example is from a C 
compiler writtcn by D. M. Ritchic. 
It i s  also used by the C compiler 
described in Johnson 119791, 
Consider type expressions with the following type constructors for pointers, 
functions, and arrays: p I m r I t )  denotes a pinier to type i, JretursIt) 
denotes a function of some arguments that returns an object of type r, and 
arruyrt) denotes an array (of some indeterminate length) of elements of type 
i. Notice that we have simplified the array and function type constructors. 
We shall keep track of the number of elements in an array, but the number 
will be kept elsewhere, w it is nu1 part of the type consrructor army. Simi- 
larly, the only operand of the constructor frerurns is the type of the result of a 
function; the types of the function arguments will b stored elsewhere. Thus, 
objects with structurally equivalent expressions of this type system might still 
fail to meet the test of Fig, 6.6 applied to the more detailed type system used 
there. 
Since each of these constructors i s  a unary operator, type expressions 
formed by applying these constructors to basic types have a very uniform 
strucrure, Examples of such type expressions are: 

chur 
freturns [char) 
pointer urefurns char)) 
arruy (pointer (frerurns (char))) 
Each of the above expressions can tx represented by a sequence of bits using 
a simple encoding schcme. Since there are only three type constructors, we 
can use two bits to encode a constructor, as follows: 
TYPE CONSTRUCTOR 
ENCODING 
pointer 
0 1 
array 
10 
fr~iums 
I1 
The basic types of C are encodcd using four bits in Johnson 
basic types might be cncoded as: 
119791; our four 
Restricted type expressiuns can now be encoded as sequences of bits. The 
rightmost four bits encode the basic type in a type expression. Moving from 
right to left, the next two bits indicate the constructor applied to the basic 
type, the next t w q  bits describe the consrructor applied to that. and so on. 
For example, 
See Exercise 6.12 for more details. 
Besides saving spcc, such a representation keeps track of the constructors 
that appear in any type expression. Two different bit sequences fannot 
represent the same type because either the basic type or the constructors in the 
type expressions are different. Of course, different types could haw the same 
bit sequence since array size and function arguments are not represented. 
The encoding in this example can be extended to include record types. The 
idea i s  to treat each record as a basic type in the encoding; a separate 
sequence of bits encodes the type of each field of the record. 
Type 
equivalenoe in C is examined further in Example 6.4. 
o 

356 TYPE CHECKING 
SEC. 6.3 
Names for Type Expressions 
In some languages, types can be given names. For example, in the Pascal pro- 
gram fragment 
type link = t cell; 
var next : link; 
last : link; 
P 
: f cell; 
q, r : t cell; 
the idcnrifier link is declared to be a name for the type tcell. The ques- 
tion arises, do the variables next, l a s t ,  p, q, r all have identical t y ~ ~ s ?  
Surprisingly, the answer depends on the implernentatioo, The problem arose 
because the Pascal Report did not define the term "identical type." 
To model this situation. wc allow type expressions to be named and albw 
these names to appcar in type expressions where we previous\y had only basic 
types. 
For example. if cell is the name of a type expression, then 
pointer[celI) is a type expression. For the time being, suppose there are no 
circular type expression definitions such as defining cell to k the name of a 
type expression cmraining c e l l .  
When names are allnwcd in type expressions, two notions of equivalence of 
type expressions arise, depending on the treatment of names. 
Name 
uyuivdmw views each type name as a distinct type, so two type expressions 
are name equivalent if and only if they are identical. 
Under srrucrlrrd 
q u i v d ~ n w ,  names are replaced by the type expressions they define, so two 
type expressions are structurally equivalent if they repremit two structurally 
equivalent type expressions when all names have been substituted out. 
Example 6.2. The type cxpressions that might be associated with the variables 
in the declarations (6.2) are given in the following table. 
VARIABLE 
TYPE€XPRES..ION 
next 
link 
last 
link 
P 
pr)inrpr I cell) 
q 
pointer ( cell) 
r 
pointer { ce 11) 
Under name equivalence, the varirrbles next and last have che same type 
because they have thc same associated kype expressions. The variabks p, q, 
and r also have the same type, but p and next do not, since their associated 
type expressions are different. Undcr structural equivalence, all five variables 
have the same type because link is a name for the type expression 
foin~er(cell 
j. 
a 
The concepts of structural: and name equivalence are useful in explaining 
thc rules used by various languages to associate types with identifiers h-ough 
declarations. 

EC. 6.3 
EQUIVALENCE OF TYPE EXPRESSIONS 357 
Example 6.3. Confusion arises in Pascal from the faa that many implementa- 
tions associate an implicit type name with each declared identifier. tf the 
declaration contains a type expression that is not a name, an implicit name is 
created, A fresh Implicit name is crested every time a type expression appears 
in a variable declaration. 
Thus. implicit names are created for the type expresssions in the two declara- 
tions containing p, q. and r, in (6.2). That is, the declarations are treated as 
if they were 
type link 
nP 
w r  
var next 
last 
P 
Q 
r 
= t cell; 
t cell; 
= f cell; 
: link; 
: link; 
: np; 
: nqr; 
: nqr; 
Mere. new type names np and nqr have been introduced, Under name 
equivalence, since next and l a s t  ire declared with the same type name, 
they are treated as having equivalent types. Similarly, q and r are treated as 
having equivalent types because the same implicit typc name is associated with 
them. However, pl q, and next do not have equivalent types, since they all 
have types w i ~ h  
different names. 
The typical implementation is to construct a type graph to represent the 
types. Every time a type constructor or basic type i s  seen. a new node is 
created. Every time ;i new type name is seen, a leaf i s  created, howcver, we 
keep track of the type expression to which the name refers. 
With this 
representation, two type expressions are equivalent if they are represented by 
the same node in the type graph. Figure 6.7 shows a type graph for the 
declarations (8.2). Dotted lines show the associaiion between variables and 
nodes in the type graph. Note that type name e e l 1  has three parents. all 
labeled poinwr. An equal sign appears between the type name link and the 
node in the type graph to which it refers. 
next 
last 
P 
9 
L 
i ink = p ~ i n ~ t r  
pvin trr 
,&ntr>r 
Fig, 6.7, Associaliun of wriablcs and nrdcs in thc typc graph. 

358 
TYPE CHECKING 
SEC. 6.3 
Basic data structures like linked lists and trees are often defined recurstvely; 
e.g,, a linked list is either empty or consists of a cell with a winter to a linked 
tist. Such data structures are usually implemented using records that contain 
pointers to similar records, and type names play an essential role in defining 
the types of such records. 
Consider a linked list of cells, each containing some integer information and 
a pointer to the next cell in the list. Pascal declarations of type names 
corresponding to links and cells are: 
type link = t cell; 
ek11 = record 
info : integer; 
next : link 
end; 
Note that the type name link is defined in terms of cell and that c e l l  i s  
defined in terms of fink, so their definitions are recursive. 
Recursively defined type names can be suhtituted our if we are wiiIing to 
introduce cycles into the type graph, 
If poinrer(cel1) is substituted for 
link, the type expression shown in Fig. 6.Qa) is obtainkd for cell. Using 
cycles as in Fig. 6.8Ib), we can eliminate mention of cell from the part of 
the type gaph below the node labeled record. 
I 
c e l l  
Fig. 6.8, Rccu rsivcly dcfined typc narnc cell. 
Example 6.4. 
C avoids cycles in type graphs by using structurai equivalence 
for all types except records. In C, the declaration of cell would look like 
struct cell { 
i n t  info; 
atruct cell *next; 
1; 
C uses the keyword struct rather than record, and the name cell 
becomes part of the type of the record. 
In effect, C uses the acyclic 

SEC. 6.4 
representation in Fig, 6+8(a). 
C requires type names to be declared before they arc used, wirh the excep- 
tion of allowing poinlers to undeclared record types. Ail potential cycles, 
therefore, are due to pointers to records. Since thc name of a record is part 
of its type, testing for structural equivalence stops when a record constructor 
is reached - 
either the types being compared are equivalent because they are 
the same named record type or they are inequ ivalent . 
u 
Consider expressions like x +  i where x is of type real and i is of type 
integer. Since the representation of integers and rcah is different within a 
computer, aud different machine instructions arc used for opa'ions on 
integers and reals, the compiler may have to first convert onc of the operands 
of + to ensure that both operands are of the same type when the addition 
takes place. 
The language derinirion specifies what conversions are necessary . When an 
integer is assigned to a real, or vice versa, the conversion is to the type of the 
left side of the assignment, In expressions, the usual transformation is to con- 
vert the integer into a real number and then perform a real operatton on the 
resulting pair of real operands. The type checker in a compiler can k used to 
insert these conversion aperati~ns into the intermediate representation oC Ihe 
source program, For example. postfix notation for x +  i, 
might be 
Merc, the inttoral operator converts i from integer to real and then real+ 
performs real addition on its operands. 
Type conversion often arises in another context. A symbr~I having difrcrent 
meanings depending on its context is said to bc overloaded, Overloading will 
be discussed in the next section, but it is mentioned becausc typc ctmversions 
often accompany overloading. 
Conversion from one type to another is said to be implicit if it is to be done 
automatically by the compiler. Implicit type conversions, also called crrcr- 
rims, are limited in many languages to siluations where no informatian is lost 
in principle; e.g,, an integer may be converted to a real but nut vice-vcrsa. In 
practice, howcvtr, loss ts possible when a real number must fit into the same 
number of bits as an integer. 
Conversion is said to be expiid if the programmer must write something to 
cause the conversion. For all pracrical purposes, all conversions in Ada arc 
explicit. Explicit conversions look just Like function applications to a typc 
checker, so they present no new problems+ 
For example,. in Pascal, a built-in functim ord maps a character to an 
integer, end chr does the inverse mapping from an integer to a character, so 

360 TYPE CHECKING 
SEC. 6.4 
these conversions are explicit, C, on the other hand, coerces (i,e,, implicitiy 
converts) ASCII characters to integers k t w e e n  0 and 127 in arithmetic 
expressions. 
Example 5,5. Consider expressions formed by applying an arithmetic operator 
ap to constants and identifiers, as in the grammar of Fig, 6.9. Suppose there 
are two types - 
real and integer, with integers converted to reals when neces- 
sary. Attribute ope of nonterminal E can be either integer or real, and the 
type-checking rules are shown in Fig, 6.9. As in Scctim 6.2, function 
kmkup (e) returns the type saved in the symbol-table entry pointed to by e. 0 
MANTIC RULE 
I 
I 
then rrul 
Fig. 6.9. Typc-chccking rules for mrciun from integer lo real. 
Implicit conversion of constants can usually be done at compile time, often 
with a great improvement to the running time of Ihe ob@t program. Cn the 
following code fragments, x is an array of rcals that is being initialized to all 
1's. Using one Pascal compiler, Bentlcy 1 1982) found rhal the code fragment 
took 48.4N microseconds, while the fragment 
for I := 1 to l4 do X [ I ]  := 1 - 0  
took 5+4N microseconds. Both fragments assign the value one to elements of 
an array of reals. However, the d
e
 
generated (by this compiler) for the first 
fragment contained a calt to a run-time routine to convert the integer 
representation of 1 into the real-numbcr representation. Since it is known at 
compile time that x is an array of reals, a more thorough compiler would con- 
vert 1 to f .O at compile time, 

6.5 OVERLOADlNG OF FUNCTIONS AND OPERATORS 
An overloaded symbol is one that has different meanings depending on its con- 
text. In mathematics, the addition operator + is overloaded, because + in 
A S B has different meanings when A and B are integers, reah. complex 
numbers, or matrices. In Ada, parentheses I 
1 are overloaded; the expression 
A( I) can k the 1th element of the array A, a call to function A with atgu- 
ment I, or an explicit conversion of expression I to type A. 
Overloading is resolved when a unique meaning for an occurrence of an 
overloaded symbol is determined. For example, if + can denote either integer 
' addition w real addition, then the two occurrences of + in x + i + j ) can 
denote different forms of addition, depending on the types of x, i, and j. 
The resolution of overloading is sometimes referred to as uperator idenfiflca- 
tion. because it determines which operation an operator symbol denotes. 
The arithmetic operators are overloaded in most languages. However, over- 
loading involving arithmetic operators like + can be resolved by looking only 
at the arguments of the operator. The case seaalgsis for determining whether 
to use the integer or real version of + i s  similar to that in-the semantic rule 
for E -. E l  op E z  in Fig, 6.9, where the type of E is determined by kmking at 
the possible types of E I and E l  + 
Set of Possibk Types for a Subexpression 
It is not always possible to resolve overloading by lucking only at the argu- 
ments of a function, as the next example shows. Instead of 3 single type, a 
subexpression standing alone may have a set of pomible types. In Ada, the 
context must provide ijuffjcient information to narrow the choice down 10 a 
single type. 
Example 6.6. In Ada, one of the standard Ii.e., built-in) interpretations of, 
the operator + is that of a function from a pair of integers to an integer. The 
operator can be overbiaded by adding declarations like the following: 
function 'I*" I i, j : integer 1 return complex; 
function "*" [ x, y : complex 1 return complex; 
After the above declarations, the possible types for * include: 
Suppose that the only possible type for 2 ,  3, and 5 is integer. With the 
above declarations, the subexprcssion 3rr 5 either has type integer or complex, 
depending on its antekt + If the complete expression is 2a ( 3* 51, then 3 e 5  
muit have type integer because 
takes either a pair of integers or a pair of 
complex numkrs as arguments. On the other hand, 3+5 must haw type 
complex if the complete errpression is { 3+5 ) +z and z is declared to be corn- 
plex, 

2 
TYPE CHECKING 
SEC+ 6+5 
In Wcian 6+2, we assumed that each expression had a unique type, st, thc 
type-checking rule for function application was: 
The natural generalization of this rule to sets of types apFars in Fig. 6.10- 
The only aperation in Fig. 6.10 is that of function application; the rules far 
checking other operators in expressions are similar. There may be several 
declarations of an overloaded identifier, so we assume that a symbol-table 
entry may contain a set of possible types; this set is returned by the lookup 
function. The starting nmterminal E' generates a complete expression. Its 
role is clarified below. 
Fijg. 6.10. Detcrrnining thc set of possible types of an expression. 
In words, the third rule of Fig. 6.10 says that if s is one of the types of El 
and one of the types of E l  can map s to t, then c is one of the types of 
E l  IE2 ). A type mismatch durhg function application results in the set 
E.types beaming empty, a condition we temporarily use to signal a type error, 
Exampk 6.7. Besides ihstrating the specification in Fig. 6. LO, this exsmplc 
suggests how the approach carries over to other constructs. In particular, we 
consider the expression 3+5. Let the declarations of operator * be as in 
Example 6.6. That is, * can map a pair of integers to either an integer or a 
complex number depending on the context. The set of pssible types for the 
subexpressions of 3*5 is shown in Fig. 6. I I, where i and c. abbreviate in~eger 
and r m p k x ,  respectively. 
E: {LC-} 
/ I ---- 
E: {i) 
E :  {i) 
1 
3: {i) 
Fig, 6*l I. Sct of possiblc l ypcs for the cltpression 3* 5. 
Again, suppse that the only possible type fw 3 and 5 is integer, The 
operator * is therefore applied to a pair of integers. If we treat this pair of 

SEC. 6.5 
OVERLOADING OF FUNCl'lONS AND OPERATORS 363 
integers as a unit, its type is given by integer x i w g ~ r .  There are two func- 
tions in the set of types for * that apply to pairs of integers; one returns an 
integer, while the other returns a complex number, so the root can have either 
type integer or t ypc cump/x. 
D 
Narrowing the Set 01 Prrssibk Types 
Ada requires a complete expression to have a unique type. Given a unique 
type irom the wntext, we can narrow down the typc choices for each sukx- 
pression. If this process does not result in a unique type for each subexpres- 
sion, then a type error is declared for the expression. 
Before working top down from an expression to its subexpressions, we take 
a clme look at the sets E.cypc.c constructed by the rules in Fig. 6,1O, We show 
that every type r in E.iypes is a fcmihk type; i+e., it is possible to choose from 
among the overloaded types of the identifiers in E in such a way that E gets 
type s+ The property holds for identifiers by declaration, since each element 
of id.cype.s is feasible, For the inductive step, consider type r in E.sypes, 
where E is E l  { E l  1. From the rule for function application in Fig. 6.10, for 
some type s, s must be in E2.rypes and a type s-I 
must be in El.rypcs. By 
induction, s and s -.f are feasible types for El and E ,, respectively. I t  follows 
that r is a feasible type for E+ 
There may be several ways of arriving at a feasible type. For example, con- 
sider the expression f ( x )  where f can have the types a--c and b + c ,  and x 
can have the types u and 6. Then, f [ x }  has type rh but x can have either type 
0 or b. 
The syntax-directed definition in Fig. 6.12 is obained from that in Fig. 6.10 
by adding semantic rules to determine inherited attribute unique of E. Syn- 
thesized attribute code of E is discwxd below. 
Since the entire expression is generated by E', we want E'+qpes to be a set 
containing a single type f. 
This single type is inherited as the value of 
E. unique. Again, the basic type type-error signals an error, 
If a function El LEI 1 returns type r, then we can find a type s that is feasi- 
ble for the argument EZ; 
at the same lime, s -t is feasible for the function. 
The set S in the corresponding semantic rule in Fig, 6.12 i s  used to check that 
there is a unique type s with this property. 
The syntax-bitected definition in Fig. 6.12 can be implemented by making 
two depth-first lraversals of a syntax tree for an expression. During the first 
traversal, the attribute 
is synthesized bottom up. During the second 
traversal, the attribute unique is prqmgated top down, and as we return frm 
a node, the code attribute can be synthesized. In practice, the type checker 
may simply attach a unique type to each node of the syntax tree. In Fig. 
6.12, we generate postfix notat ion to suggest how intermediate code might be 
generated. In the p b ~ f i x  notation, each identifier and insqance of the apply 
operator has a type attached to it by the function gcn. 

364 
TYPECHECKING 
SEC. 6.5 
E - M  
Fig. 6.12. Narrowing down the XI cr€ types for an expression. 
6,6 POLYMORPHIC FUNCTIONS 
An ordinary prwdure allows the statements in its M
y
 to be executed with 
arguments of fixed types; each time a polymorphic procedure is calied, the 
statements in its body can be executed with arguments pi different types. The 
term "polymorphic" can also be applied to any piece OF code that can be exe- 
cuted with arguments of different types, so we can talk of polymorphic func- 
tions and operators as well. 
Built-in operators for indexing arrays, applying functions, and manipulating 
pointers ate usually polymorphic because they arc not restricted to a particular 
kind of array, function, or pointer. For example, the C reference manual 
states about the poinrer operator &: "lf the type of the operand is '.+.', the 
type of the result is 'pointer to ...'." 
Since any t y p  can be substituted for 
". . ." the operator & in C is polymorphic, 
In Ada, "generic" functions are polymorphic, but polymorphism in Ada is 
restricted. Since the term "generic" has also been ustd to refer to overloaded 
hnctions and to [he cmrcion of rhe arguments of functions, we shall avoid 
using that term. 
This section addresses the problems that arise in designing a type checker 
for a language with polymorphic functions. To deal with polymorphism, we 
extend our set of type e~pressions to include cxpressions with type variables. 
The inclusion of lype variables raises some algorithmic issues concerning the 
quivalence of type cxpressions. 

POLYMORPHIC FUNCTIONS 
365 
Why Polymorphic Functions? 
Polymorphic functions are attractive because they facilitate the implementation 
of algorithms (hat manipulate data structures, regardless of the rypes of the 
elements in the data structure. For example, it is convenient to have a pro- 
gram that determines the length of a list without having to know the types of 
the elements on the list. 
type link = t cell; 
cell 
record 
info: integer; 
next: link 
end; 
function length ( lptr : link ] ; integer: 
var 
len : integer; 
beg in 
leri : = 0; 
while lptr c r  nil do begin 
len := len + I ;  
l p t r  : =  1ptrt.next 
end: 
length := len 
end : 
Fig. 6.13. Rml program for thc lcngth of a list. 
Languages like Pascal require full specification of the types of function 
parameters, so a function for determining the lcngth of a linked list of 
integers cannot be applied to a list of reds. The Pascal code in Fig. 6.13 is 
for lists of in~egers. The function length follows the next links in the list 
until a nil link is reached, Although the function does not in any way 
depend on the type of the information in a cell, Pasc-dl requires the type of the 
info field to be declared when the length function is writ ten. 
fun lenqth{lptr) = 
i f  null(lptr1 then 0 
else Iengthl tl I l p t r )  1 + 1 ; 
Fig* 6.14, ML prugram for the Icngth of a list. 
in a language with polymorphic furlctions, like ML (Milnertl9841). a func- 
tion length can be written so it applies tr, any kind of list, as shown in Fig. 
6,14. The keyword fun indicates that length is a recursive function. The 
functions null and t1 are predefined: null tests if a list is empty, and tl 

TYPE CHECKING 
SEC. 6.6 
returns the remainder of the list after the first clernen~ is removed, With rhe 
definition shown in Fig. 6.14, both the following applications of the function 
length yield 3: 
In the first, length is applied to a list of strings; in the second, it is applied 
to a list of integers. 
Type Variables 
Variables representing type expressions allow us to talk a b u t  unknown types. 
In the remainder of this section, we shall use Greek letters a, P, . . . for 
type variables in type expressions. 
An important application of type variables is checking consistent usage of 
identiliers in a language that does not require identifiers to be declared before 
they are used. A variable represents the type of an undeclared identifier. We 
can tell by looking at the program whether the undeclared identifier is used, 
say, as an integer in one statement and as an array in another. Such inwn- 
sistent usage can be reported as an crror. On the other hand, if the variable 
is always used as an integer, then we haw not only ensured consistent usage; 
in  he process, we have inferred what its type must be. 
Type infcrmcc is the problem of determining the type of a language con- 
struct from the way it is used. The term is often applied to the problem of 
inferring the type of ;d function from its M y .  
Example 6.8. 
Type-inference techniques can be applied to programs in 
languages like C and Pascal to fill in missing type information at compile 
time. The code fragment in Fig. 6.15 shows a procedure mlist, which has a 
parameter p that is itself a procedure, All we know by looking at the first 
line of the procedure m l i s t  i s  that p is a procedure; in particular, we do not 
know the number or types of the arguments taken by p. Such incomplele 
specifications of the type of p are allowed by C and by the Pascal refcrcnce 
manual. 
The prmdure mliat applies parameter p to every cell in a linked list. Fw 
example. p may be used to initialize or print the integer held in a cell, 
Despite the fact that thc types of the arguments of p are not specifid, we can 
infer from the use of p in the expression p I lptr ) that the type of p must be: 
link + void 
Any call of mlist with a procedure parameter that does not have this type is 
an error. A procedure can be thought uf as a function that does not return a 
value, so its resulz type is void. 
a 
Techniques for type inference and type checking have a lot in common. In 
each case, we have to deal with type expressions containing variables. Rea- 
soning similar to that in the following example is used later in [his seaion by 

type link t c e l l ;  
procedure mlist [ lptr : link; procedure p 1 ; 
mgin 
while lptr qr nil do &gin 
p(1ptr); 
l p t r  :- 1ptrt.next 
end 
end ; 
Fig. 6.15. Prwdurc m l i s t  with proccdurc parnrncter p. 
a type checker to infer the types represented by variables. 
Example 6+9. A type can be inferred for the plymorphic function deref in 
the following pseudo-program. Function deref has the same effect as the 
Pascal operator t for dereferencing pointers. 
function dereflp}; 
begin 
return pt 
end ; 
When the first line 
function deref[p); 
is seen, we know nothing about the type of p, so let us represmt it by a type 
variable p. By definition, the postfix operator t takes a pointer to an object, 
and returns the object. Since the t operator is applied to p in expression pt, 
it follows that p must be a pointer to an object of unknown type ol, so we 
learn that 
where a is another type variable- Furthermore, the expression pt has type a, 
so we can write the t y p  errpression 
for the type of the function deref. 
D 
A Language with hlymwph'ic Funcths 
All we have said so far a b u t  polymorphic functions is that they can be exe- 
cu led with arguments of "different types.'' Precise statements about the set of 
types to which a pcilymurphic function can be applied are made using the sym- 
bol V , meaning "for any type," Thus, 

368 
TYPE CHECKING 
SEC. 6.6 
is how we write the type expression (6.3) for the type of the function deref 
in Exampie 6.9. The pdymorphic function length in Fig. 6.14 takes a list of 
elements of any type and returns an integer, so its type can be written as: 
Here, List is a type constructor. Without the W symbol, we can only give 
examples of possible domain and range types far length: 
Type expressions like (6.5) are the mast general statements that we can make 
about the type of a polymorphic function. 
The V symbol is the rrnlversal q m ~ t @ s r ,  and the type variable to which it is 
applied i s  said to be bound by it. Bound variables can be renamed at will, 
provided all occurrences of the variable are renamed, Thus, the type enpres- 
sion 
is equivalent to (6.4). 
A type expression with a V symbol in it will be 
referred to informally as a '*polymorphic typc." 
The language we shall use for checking polymorphic functions is generaled 
by the grammar in Fig. 6.16. 
Fig. 6.16. Grammar for language with polymorphic functims. 
Programs generated by this grammar consist of a sequence of declarations 
followed by the expression E to be checked, for example, 
We minimize notation by having nonterminal T generate typc expressions 
directly. The constructors -. and x form function and product types. Unary 
constructors, 
represented 
by 
unary-constructor. 
allow 
types 
like 

SEC+ 6.6 
POLYMORPHIC FUNCTIONS 369 
pinter ( ifikger ) and iia ( integer) to be written. Parentheses are used simply 
for grouping types. Expressions whose types are to be checked have a very 
simple syntax: they can be identifiers, sequences of expressions forming a 
tuple, or the application of a function to an argument. 
The typ-checking rules for polymorphic functions differ in three ways from 
those for ordinary functions in Section 6.2. Before presenting the rules, we 
illustrate these differences by considering Ihe expresion Beref (deref Iq) 1 
in the program (6.6). A syntax tree for this expression is shown in Fig. 6.17. 
Attached to each node are two labels. The first tells us the subexpression 
represented by the node and the second i s  a type expression assigned to the 
sukxpression. Subscripts u and i distinguish between the outer and inner 
occurrences of deref. respectively. 
deref, : pointer (a,) 
-c ai 
q : pdntrr Ipinwr (integer)) 
Fig. 6.17. Labclcd syntax tree for deref IdereF I 
q 1 1 .  
The differences from the rules for ordinary functions are: 
4 
Distinct occurrences of a polymorphic function in the same expression 
need not have arguments of the same type. 
In the expression 
dcref, [ deref q ) ) , deref removes one level of pointer indirection, 
st, deref, is applied to an argument of a different type. The implemen- 
tation of this property is based on the interpretation of V a  as *'for any 
type a," Each occurrence of deref has its own view of what the bound 
variable a in (6.4) stands for, We therefore assign to each occurrence of 
&ref a type expression formed by replacing a in (6.4) by a fresh vari- 
able and removing the V quantifier in the process. In Fig. 6.17, the fresh 
variables a, and ai are used in the type expressions assigned to the outer 
and inner occurrences of deref, respectively. 
2. 
Since variables can appear in type expresswns, we have to reexamine the 
notion of equivalence of types. Suppose E l  of (ype s +s' is applied to E 2  
of type 1. Instead of simply determining the quivaknce of s and f. we 
must "unify" them. Unification is defined k b w ;  informally, we deter- 
mine if s and r can & made structurally equivalent by replacing the type 
variables in s and t by type expressions. For example, at the inner node 
lab&d apply in Fig. 6-17, the equality 

370 
TYPE CHECKlNG 
is true if acj is replaced by pointer (irtteger ). 
We need a mechanism for recording the effea of unifying two expres- 
sions, In general, a type variable may appear in several type expressions. 
If unification of s and s' results in variable a repre~nting type l ,  then a 
must continue to represent r as type checking proceeds, For example, in 
Fig. 6.17, a, is the range type of deref;, so we can use it for the type 
of deref, (q)+ Unifying the domain type of &refi with the type of q 
therefore affects the type expression at the inner nude labeled apply. The 
other type variable a ,  
in Fig. 6.17 represents intqer. 
Substitutions, Instances, and Unification 
Information about the types represented by variables is formalized by defining 
a mapping from type variables to type expressions called a sub$titurion. The 
following recursive function subsr(f) makes precise the nation of applying a 
substitution S to replace all type variables in an expression 1. As usual, we 
take the function type constructor to be the "typical" constructor. 
function suht ( I  : type-expression) : ~peexpressiorr; 
begin 
if t is a basic type them return r 
else if r is a variable them return $ I t )  
else if r is t 
+ r2 then return subst It ,) -, subst It2) 
end 
For convenience, we write S ( t l  for the type expression that results when 
suhr is applied to r ;  the result S(t) is called an instutce of r. If substitution $ 
does not specify an expression for variable a, we assume that $(a) is a; that 
is, S is the identity mapping on such variables. 
Example 6,10. 
I n  the fdlowing, we write s < t to indicate that s is an 
instance of I: 
However, in the foilowing, the type expression on the left is not an instance 
of the one on the right (for the reason indicated): 
integer 
reuI 
Subst itutians do not apply to basic types. 
hrqer -. red 
a -. or 
Inconsistent substitution for a. 
Inreger - a 
u -+ 01 
A I1 occurrences of a must be replaced. 
o 
Two type expressions t and r z  m&fy if there exists some substitution S such 
that S ( r  ,) = S t r , ) .  In practice, we are interested in the m s t  general w&r, 

SEC. 6.6 
POLYMORPHIC FUNCTIONS 
371 
which is a substitution that imposes the fewest constraints on the variables in 
the expressions. More precisely, the most general uniher of expressions rl 
and r 2  is a substitution S with the following properties: 
2. for any other substitution 3' such that St(& = Sn(t2), the substitution St 
is an instance of S (that is, for any r ,  $'(I) is an instance of S ( i ) ) .  
In what follows. when we say "unifies" we are referring to the most general 
unifier. 
Checking Polymorphic Functions 
The rules for checking expressions generated by the grammar in Fig. 6.16 will 
be written in terms of the following operations on a graph representation of 
types. 
1. fresh ( r )  replaces the bound variables in type expression r by fresh vari- 
ables and returns a pointer to a node representing the resulting type 
expression. Any V symbols in k are removed in the process. 
2. 
wnifyIm, n) unifies the type expressions represented by the nodes pointed 
lo by m and st. It has the side effect of keeping track of the substitution 
that makes the expresshns equivsr\eat. If the expressions fail to unify, 
the entire type-checking process f a k 4  
Individual leaves and interior nodes in the type graph arc constructed using 
operations mklrclf and mkn& 
similar to those of %ction 5.2, It is necessary 
that there be a unique leaf for each type variable, but other structuraIly 
equivalent expressions need not have unique nodes. 
The unth operation is based on the following graph-theoretic formulstlon of 
unification and substitutions. Suppose nodes m and A of a graph represent 
expressions P and f, respective\y. We say nodes rn and n are equivuknr under 
substitution S if S(e) = SCf). The problem of finding the most general unif- 
ier S can be restated as the probkm of grouping, into sets, nodes that must be 
equivalent under S. For the exprexsions to be equivalent, their roots must be 
equivalent. Also, two nodes rn and n are equivalent if and only IF they 
represent the same operator and their corresponding children ere equivalent, 
An algorithm for unifying a pair of expressions is deferred until the next 
section. The algorithm keeps track of sets of nodes that are equivalent under 
the substitutions that have occurred. 
The type-checking rules for expressions are shown in Fig, 6.18+ We do not 
show how declarations are processed. 
As type expressions generated by 
The ream for aborting  he ~ype-checking ptocess is r h u  the side cffe~ts of wme unifications 
may bc recorded before failure 1s detected. Error recovery a n  be irnptcmcnted if [he side effects 
of h e  kflifj operation are deftncd until rhe cxprcssinns have been unified SUCEPSS~UI~Y. 

372 
TYPE CHECKING 
SEC. 6.6 
1 
ncmterminals T and Q art examined, mkkafand mknode add nodes to the type 
graph, following the dag conslruction in Section 5.2. When an identifier is 
declared, the type in the declaration is saved in the symbol table in the form 
of a pointer to the node representing the type. In Fig, 6.18, this pointer is 
referred to as the synthesized attribute id.rype. As mentioned above, the jir~sh 
operation removes the V symbols as it replaces bound variables by fresh vari- 
ables, The action associated with production E - + E l  , E 2  sets E.ype to the 
product of the types of E 
and E 2  . 
Fig. 6.18. Translation scheme for checking plymc~rphic functions. 
The type-checking rule for the function application E 
+ E I ( E l )  is 
motivated by cclnsidering the case where Et.rypc and Ez.typs are both type 
variables, say, El.rype - a and E2.fype = P+ Here E,.sype must be a func- 
tion such that for some unknown type y, we have a = P - y. In Fig. 6.18, a 
fresh type variable corresponding to y is created and &,.ope is unified with 
E2.sype -y+ A new type variable is returned by each call of newqpevar, a 
leaf for it is constructed by mkleaJ; and a n d e  representing the function to be 
unified with E +ype is constructed by rnhode, After unification succeeds, the 
new leaf represents the result type, 
The rules in Fig. 6.18 will be illustrated by working out a simple example in 
detail. We summarize the workings of the algorithm by writing down the type 
expression assigned to each subexpression, as in Fig. 6.19. At each function 
application, the mi& operation may have the side effect of recording a type 
expression for some of the type variables. Such side effects are suggested by 
the column for a substitution in Fig, 6.19, 
dercf,,(derefi(ql) 
: integw 
u,, ' 
inreger 
- 
Fig. 6.19. Summary of bottom-up type determination. 

SEC+ 6.6 
POLYMORPHIC FUNmIONS 373 
Example 6.11. Type checking [he expression deref,, lderef I q ) ) in prw 
gram (6.6) proceeds bottom up from the leavm. Once again, subscripts o and 
i distinguish between occurrences of deref. When subexpression deref, is 
considered, fresh constructs the following nodes using a new type variable a,, 
. 
The number at a node indicates the equivalence class the n d e  belongs to. 
The part of the type graph for the three identifiers is shown below, The dot- 
ted lines indicate that nodes numbered 3, 6, and 9 are for deref,, derefi, 
and q, respectively. 
deref,, 
deref, 
=! 
The Cunaion application derefi ( q) is checkcd by constructing a node n far 
a function from the type of q to a new type variable j3. This function unifies 
successfully with the type of deref represented by node m below. Before 
nodes m and n are unified, each node had a distinct number. After unifica- 
tion, the equivalent nodes are the ones below with the same number; the 
changed numbers are underlined: 
Note that the node for ai and puinrerIinrcger) are both numbered 8, thal i s  ai 
is unified with this type expression, as shown in Fig. 6.19. Subsequently, u, 
is unified with integer. 
a 
The next example relates type inference of polymorphic functions in ML to 
the type-chccking rules in Fig. 6.18, The syntax of function definitions in ML 
is given by 
fun ido ( id,, + . . , idk 1 = E ; 
where ido represents the function name and Ad,, . . . . idn represent its pararn- 
cters. For simplicity, we assume that the syntax of expression E is as in Fig. 
6,16, and that the only identifiers in E are the funchn name, its parameters, 

374 
TYPE CHECKING 
and built-in functions. 
The approach is a formalization of that 
in Example 6.9, where a 
polymorphic type was inferred for deref. New type variables arc made up 
for the function name and its parameters, The built-in functions have 
polymorphic types, in general; any type variables that apFar in these types 
are bound by V quantifiers. We then chedr that the types of the expressions 
idof id,, . . . , idp) and E match. When the match succeeds, we will have 
inferred a type for the function name. Finally, any variables in the inferred 
type are bound by V quantifiers to give the polymorphic type for the function. 
Example 6+12. Recall  he ML function in Fig. 6+14 for determining the length 
of a hi€: 
fun lengthIlptr1 = 
if nullllptr) then O 
else lengthltl(1ptrl) + 1; 
~~~e variables P and y are introduced for the types of length and iptr. 
respectively. We find that the type of length(lptr1 matches that of the 
expression forming the function M
y
 
and that length must have type 
for any type or, iist(a) dinleger 
so the type of length is 
V a  . list (a) -. integer 
In more detail, we set ~ l p  
the program shown in Fig. 6.20, to which the 
typ-checking rules of Fig. 6.18 can be applied. The declarations in the pro- 
gram associate the new type variables p and y with length and lptr, and 
make explicit the types of the built-in operations- We write condilimals in 
Fig. 6.20. Dcdarations followd by expression to be checkcd. 

POLYMORPHIC FUNCTIONS 375 
EXPRESSION 
: TYPE 
lptr : y 
length : @ 
lengthllptrl : 6 
lptr : y 
null : 1iw (a,) -, him 
nullIlptr 1 : buuhn 
I) : irtreger 
l p t r  : ii~isl(a,) 
tl : iist(a,) 
-c list (a,) 
tl(lptr1 : Iiss(a,) 
length ; list (a,) - 8 
length(tll1ptr)l : 8 
1 : imtger 
+ : integer X integer - infcger 
length I t 1 { lptx 5 ) + 1 : integer 
if : h / r u n  X ai 
X ai -. ai 
if ( . - 
+ I : integer 
match : a, X a ,  
+ a, 
match( - . - 1 : intcgcr 
Fig. 5.21. Inferring the type lisr (a,) dinrqrr for length. 
the style of Fig. 6.16 by applying polymorphic operator if to three operands, 
representing h e  expression to be tested, the then part, and the else part; the 
declaration says that the [hen and else parts can be of any matching type. 
which is also the type of the result. 
Clearly, length{ lptr 1 must have the same type as the function body; this 
check is encoded using an operator match. The use of match i s  a technical 
convenience that albws all checking to be done using a program in the style 
of Fig+ 6.16. 
The effect of applying the type-checking rules in Fig, 6+13 to the program in 
Fig. 6.20 is summarized in Fig. 6.21. The new variables intrduwl by opera- 
tion fresh applied to the plymorphic types of the built-in operations are dis- 
tinguished by subscripts on a. We learn on h e  (3) that length must be a 
function from y to some unknown type 6. Then, when the subexpression 
nullIlptr) is checked, we find on line (6) that y unifies with lisr(a,), 
where a, is an unknuwn type. At this point, we know that the t y p  of 
length musr be 
for any type a,. listfa,) -. 8 
Eventually, when the abdiiion is checked on line (15) - 
we take the liberty of 
writing + between its arguments for clarity - 
6 is unified with integer. 

When checking is complete, the type variable a, remains in the type of 
length. Since no assumptions were made about a,, any type can be substi- 
tuted for it when the function is used. We therefore make it a bound variable 
and write 
for the type of length, 
6,7 AN ALGORITHM FOR LTNIFICATIQN 
Informally, unification i s  the problem of determining whether two expressions 
e and f can k made identical by substituting expressions for the variables in e 
arid f. Testing equality of expressions .is a special case of unification; if e and 
f have constants but no variables, then e and f unify if 
and only if they are 
identid. The unification algorithm in this section can be applied to graphs 
with cycles, so it con be used to test structural cquivaknm of circular types.' 
Unification was defined in the last section in terms of a function $, called a 
substitution, mapping variables to expressions, Wc write S (e) for the expres- 
sion obtained when each variable u in c is replaced by $(a). S is a unifier for 
e and ,f if S re) = SCf). The algorithm in this section determines a substitu- 
tion that is the most general unifier of a pair af e~pressbns. 
Example 6+13, For a perspective on most gene~al unifiers, consider the two 
type expressions 
Two unifiers, $ and S ' ,  for these expressions are: 
These substitutions map e and f as follows: 
Substitution S is the rnwst general unifier of e and J. Note that SP(e) i s  an 
instance of S k )  becaw we can substitute al for b t h  variables in S(e). 
' In mmc applicalinns. it is an error tu unify a variable with an exprersion containirg that vsti- 
abk. Algorithm 6.1 permits such substitutions. 

SEC. 6,7 
AN ALGORlTHM FOR UNIFICATION 377 
However, the reverse is false, because the same expression must be substituted 
for each murrence of rrl in S J ( e ) ,  so we cannot obtain S ( e )  by substituting 
for the variable at in S'le). 
When expressions to be unified are represented by trees, the number of 
nodes in the tree for the substituted expression S ( c )  can be exp~nential in the 
number of ndes in the trees for e and .f, even if S i s  the most general unifier. 
However, such a size blowup need not occur if graphs rather than trees are 
used to represent expressions and subgit utions, 
We shall implement the graph-theoretic formulation of unification, also 
presented in the last section. The problem is that of grouping, into sets, 
nodes that must be equivalent under the most general unifier of two expres- 
sions. The two expressions in Example 6.13 are represented by the two ndes 
labeled +;I 
in Fig. 6+22. The integers at the nodes indicate the equivalence 
classes that the nodes belong to after the nodes numbered 1 are unified. 
These equivalence classes have the property that a11 interior ndes in the class 
are for the same operator. The mrresvding children of interior nodes in an 
equivalence class are also equivalent. 
Flg, 6.22. Equivalcncc chsscs after unilication. 
Algorithm 6.1. Unification of a pair of nodes in a graph. 
Input. A graph and a pair of ndes m and n to be unified. 
Output. Boolean value true if the expressions represented by the nodes m and 
n h i f y ;  false, otherwise. The version of operation unifi needed for the type- 
checking rules of Fig. 6.18 is obtained if the function in this algorithm is 
modified to fail instead of returning false. 
M d d .  A node is represented by a record as in Fig. 6.23 with fields for a 
binary operator and pinters to the left and right children. 
. The sets of equivalent nodes are maintained using the ser field, One node 
in each equivalence class is &own the unique representative of the 
equivalence class by making ils set field cootah a nil pointer. The set fields 
of the remaining nodes in the equivalence claw, wifl point (possibly indirectly 
through other nodes in the set) to the representative. Initially, each node 
i s  
in an equivalenm class by itself, with 
as its own representative node. 

378 
TYPE CHECKING 
Fig. 623. Data structure for a node. 
function mi@ (m, n : nu& ) : h k a n  
twin 
s := jhd(m); 
t := J d ( n ) ;  
Hs = r then 
return true 
dse if s and r arc nodes that represent the samc basic type them 
roturn true 
dme if s is an cp-node with children s and si! awl 
r is an upnode with chihefi r L  and t, then begin 
union( s. t) ; 
return cini$v(sr, I ,  ) and urrifi(sl, 
f2) 
end 
dse iPs or t rcprcscnts a variable then begin 
union(s, f); 
Mum h e  
end 
else return false 
f* interior n d c s  with diffcrcnt qcrators cannot bc unificd */ 
end 
Fig. 6.24. Unification algorithm. 
The unification algorithm, shown in Fig. 6.24, uses the kdlowing two opera- 
tions on nodes: 
I. find(li) returns the representative n d e  of the tquivalence class currently 
contaihing node n. 
2. 
union(m, n) merges the equivalence classes containing d e s  m and n. If 
one of the representatives for the equivalence classes of m and n is a 
non-variable node, union makes !hat non-variable node be the representa- 
tive for the merged equivalence class; ~tfierwise, union makes one or the 
ocher of the original representatives be the new representative. This 
asymmetry in the specification of union is important because a variable 
cannot be used as the representative for an equivalence class for an 

SEC. 6+7 
AN ALGORITHM FOR UNIFICATION 
379 
expression containing a type constructor or basic type. Otherwise, two 
inequivalen t expressions may be unified through that variable. 
The union operation on sets is implemented by simply changing the set field of 
the representalive of one equivalence class so that it points to the representa- 
tive of the other. To find the equivalence class that a node belongs to, we fol- 
Iow the set pointers of nodes until the reprcwntative (the node with a R# 
pinter in the set field) is reached. 
Note that the algorithm in Fig. 6.24 uses s = JZnd(m) and r = find(n) 
rather than rn and a, respectively. The representative nodes s and r are equal 
if m and n are in the same equivalence class. If s and t represent the same 
basic type, the call unijj (m, A )  returns true. If s and t are both interior nodes 
for a binary type constructor, we merge their equivalence classes on specula- 
tion and recursively check that their respective children are equivalent. By 
merging first, we decrease the number of equivalence classes before recur- 
sively checking the children, so the algorithm terminates. 
The substiitution of an expression for a variable is implemented by adding 
the leaf for the variable to the equivalence class containing the node for the 
expression, If either m or n is a leaf for a variable that has been put into an 
equivaknce class that contains a node representing an expression with a type 
constructor or a basic type, then find will return a representative that reflects 
that type constructor or basic type, so that a variable cannot be unified with 
two different expressions. 
o 
Example 6,ld We have shown the initial graph for the two expressions of 
Example 6.13 in Fig. 6.25 with each node numbered and in its own 
equivalence class. To compute untfi( I ,  91, the algorithm notes that nodes I 
and 9 both represent the same operator so it merges 1 and 9 into the same 
equjvalence class and calls unib(2, 10) and unify(8, 14). The result OF corn- 
puting unify( I, 9) i s  the graph previously shown in Fig. &+22. 
o 
Fig. 6.25. lnitial dag with crrch nudc in its own cyuivzllcncc class. 
If Algorithm d I returns true, we can construct a substitution $ that acts as 
the unifier, as folluws. Let each node n of the resulting graph represent the 
expression associated with finb(n). Thus. for each variable a, findtor) gives 
the node rr that is the representative of the equivalence class of a. The 

380 
TYPE CHECKING 
SEC. 6.7 r 
expression represented by n is $(a)+ 
For example, in Fig. 6.22, we see that 
the representative for a? is node 4, which represents a!. The representative 
for US is node 8, which represents Ciss(a2). 
Example 6.15. Algorithm 6.1 can be used to test the structural equivaknce of 
the two type expressions 
The type graphs for these expressions are shown in Fig. 6.26, For conveni- 
ence, each node has been numbered. 
Fig. d2h Grrrph for lwo circular typcs. 
We call u&( 
l , 3 )  to test for the structural equivalence of these two expres- 
sions. The algorithm merges nodes I and 3 into one equivalence class, and 
recursively calls unii(2,4) and unify( 1 ,  S), Since 2 and 4 represent the same 
basic type, the call unry(2,4) returns true. The call rmiftll, 55 adds 5 to the 
equivalence class of I and 3, and recursively calls usr$y(2,61 and unify( l , 3 ) ,  
Fig. 6.27. l'ypc graph sltuwing cquivalcncc classes of nodcs. 
The call uni&(2,6) returns true because 2 and 6 also represent the same 
basic type. The second call of unCfy ( 1 , 3) terminates because we have already 
merged nodes I and 3 into the same equivalence class, The algori[hm then 
terminates, returning true to show that the two type expressions are indeed 
equivalent. Figure '6.27 shows the resulting equivalence dasses of nodes, 
where nodes with the same integer are in the same equivalence class. 

EXERCISES 38 1 
EXERCISES 
6.1 Write type expressions for the folbwinp types. 
a) An array of pointers to reals, where the array index ranges from 1 
to 100. 
b) A two-dimensional array of integers (i.e., an array of arrays) 
whose rows are indexed from 0 ro 9 and whose columns are 
indexed from - LO to 10. 
c) Functions whose domains are functions from integers to pointers 
to integers and whose ranges are records consisting of an integer 
and a character. 
6.2 Suppse we have the following C declarations: 
typtdtf struct { 
int a, b; 
1 CELL, *PCELL; 
CELL foo[lVOJ; 
PCELL barIx, y) int x ;  CELL y { 
, -  . 
1 
Write type expressions for the types of foo and bar. 
6.3 The following grammar defines lists of lists of literals. The interpre- 
talion of symbols is the same as that for the grammar of Fig. 6.3 with 
the addition of the type list, which indicates a list of elements of the 
type T that follows. 
P 
- D ; E  
D 
4 D ; D \ i d : T  
T - list of T I char I integer 
E 
-, I L ) 1 literal [ nurn 1 id 
L 
+ E , L ( E  
Write translation rules similar to those in Section 6,2 to determine the 
types of expressions [ E )  and lists I L 1 + 
6-4 Add to the grammar of Exercise 6.3 the production 
E - nil 
meaning that an expression can be the null (empty) list. Revise the 
rules in your answer to Exercise 6.2 to take a m u n t  of the fact that 
nil can stand for an empty list d elements of any type. 
6,s Using the translation scheme of Section 6.2, compute the types of the 
expressions in the following program fragments, %ow the types at 
each node of the parse tree. 
a) c: char; i: integer; 
c mcd i mod 3 

382 TYPE CHECKING 
b) p: tinteger; a: array [ 7 0 1  of integer; 
a W 1  
CHAPTER 6 
c) f: integer + bcmlean; 
i: integer; j: integer; k; integer; 
while f ti) do 
k := i; 
i := j mod i; 
j ;= k 
6.6 Mudify the translation scheme for checking expressions in Section 6.2 
to print a descriptive message when an error i s  detected and to con- 
tinue checking as if the expected type had been seen. 
6,7 Rewrite the type-checking rules for expressions in Section 6.2 so they 
refer to nodes in a graph representation of type expressions. The 
rewritten rules should use dara structures and operations supparted by 
a language such as Pascal. Use structural equivalence of type expres- 
sions when: 
a) type expressions are represented by trees, as in Fig+ 6.2, and 
b) the type graph i s  a dag with a unique node for each type expres- 
sion. 
b 
6.8 Modify the translation scheme of Fig. 6.5 to handk the following. 
a) Statements that have values. The value of an assignment is the 
value of the expression on the right of the : = sign. The value of a 
conditional or while statement is the value of the statement body; 
the value of a list of statements is rhe value of the last statement 
in the list. 
b) Bookan expressions, Add productions for [he logical operators 
and, or, and not, and for comparison owrators (<, etc.). Then 
add appropriate translation rules giving the type of these expprs- 
sicins. 
6.9 Generalize the iype-checking rules for functions given at the end of 
Section 6.2 to handle m-ary functions, 
6.10 Assume the lype names link and cel i are defined as in Section 6,3. 
Which among the following expressions are structurally equivalent'? 
Which are name equivalent'? 
i) link. 
ii) poinrcr(cs11). 
iii) pointer ( link) 
+ 
iv) polttter(rccurd (( info x inre~er) X (next X p i n w  (cell))) 
6.11 Restate the algorithm for testing structural equivalence in Fig. 6.6 so 
that the arguments of sequiv are pointers to nodes in a dag. 

CHAPTER 6 
EXERCISES 383 
4.12 Consider the encoding of restricted type expressions as sequences of 
bits in Example 6.1. In Johnsun [1979], the two-bit fields for con- 
structors appeared in the oppite order, with the field for the outer- 
most constructor appearing next to the Fwr bits for the basic type; for 
example, 
Using the operators of C write d
e
 
to mnstruct the representatbn of 
urruy(s) from that of s and vice versa, assuming that the e n d i n g  is 
as in: 
a) Johnson [ 19791. 
b) Example 6.1 . 
1.13 Suppose that the type of each Identifier k a  subrange of integers. For 
erspressbns with the operators +, -, a, div, and mod, as in Pascal, 
write type-checking rules that assign to each subexpression the 
subrange its value must lie in. 
k14 Give an algorithm to test the equivalence of C types (see Example 
6.4). 
6.15 Some languages, like PLA, will coerce a &lean 
value inlo an 
integer, with true identified with I and f a k  with 0. kt example, 
3 < 4 4  is grouped ( 3 4 )  
*S, and has the value "true" (or 11, because 
3x4 has the value I, and 1c5 is true. Write translation ruks for 
boolean expressions that prform this coercion. Use conditiond state- 
ments in the intermediate language to assign integer values to tem- 
poraries that represent the value of a boolean expression, when 
nee%kd. 
6.16 Generalize the algorithms OF {a) Fig. 6.9 and (b) Fig. 6.12 to expres- 
sions with the type mnsttudors array, pointer, and Cartesian prduct, 
6.17 Which of the following recursive type expressions are equ iuaknt? 
6.18 Using the rules of Example 6.6, determine which of the following 
expressions have unique types, Assume z is a complex number. 
a) 1 * 2 * 3  
b) 1 + { % + 2 1  

384 
TYPECHECKING 
CHAPTER 6 
C) [ : ? * z )  wz 
6.19 Suppose we allow the type conversions of Example 6.6. Under what 
conditions on the types of a, b, and c (integer or complex) will the 
expres&n (a * b) * c have a unique type? 
6.20 Express, using type variablcs,.the types of the following functions. 
a) The function ref that takes as argument an &ject of any type and 
returns a pointer to that object. 
b) A function that takes as argument an array indexed by integers, 
with elements of any type, and returns an array whose elements 
are the objects pointed to by the elements of the given array. 
2 1  Find the most general unifier of the type expressions 
i) (point~rla)) x (ply) 
iil p x (y-sl 
What if 8 in (ii) were a? 
6.22 Find the most general unifier for each pair of expressions from the 
following list, or determine that none exists. 
8) 011 
b) arroy(p1) + (poinkdp~) 
-c i 3 3 )  
cl Yr 
+ Y2 
4 81 - (61 - 92) 
Extend the type-checking rules in Example 6.6 to mver records. Use 
the following additional syntax for type expressions and expressions: 
What restrictions does the lack of type names i m p  on the types 
that can be defined? 
The resolution of overloading in Section 6.5 proceeds in two phases: 
first the set of possible types for each subexpression is determined and 
then narrowed down in a second phase to a single type after the 
unique type of the entire expression is determined. What data struc- 
lures would you use to resolve overloading in a single bottom-up 
pass? 
The resolution of overloading becomes more difficu It if identifier 
declarations are optional. More precisely, suppuse that declarations 
can be used to overload identifiers representing function symbols, but 
that all occurrences of an undeclared identifier have the same type. 
Show that the problem of determining if an expression in this 
language has a valid type is NP-complete. This problem arises during 

type checking in the exprimcntal language H w  
(Burstali, Mac- 
Queeq, and Sannclla [ 1980j). 
6.26 Following Example 6.12, infer the following polymorphic tm fm 
aap: 
The ML definition of map is: 
The types of built-in identifiers in the function M
y
 
are: 
*%.n Show that the unification aigwirhm of Section 6.7 determines the 
most general unifier. 
%A8 Modify the vniflcation algorithm of Section 6.7 so that: it d m  not 
unify a variable with an expression mntaining that variable, 
+%29 
S u p  expressions are represented by trixs. Find exptcssirms E and 
f such that for any unifier S, the number of nodm in SIC) i s  expnen- 
tial in the numbed af d e s  in .n and J. 
6,M Two d e s  are said to be cungrwrtd if they represent equivalent 
expressions, Even if no two nodes in the original type graph are 
congruent, after unification, it is psible fw distinct nodes to 
congruent. 
a) Give an algorithm to mtF@ a class of mutually mgrwnt nodes 
nodes in& a single ride. 
**b) Extend the algorithm in (a) to merge m p u e n t  nodes until no two 
distind nodes are mgruent . 
*6J1 The expression gig1 on line 9 in the cqmpkte C prugrrm in Fig. 
6.28 i s  the application of a functh to itself. The declaration on line 
3 gives iwpr as the range type g, but the types of the argumnts of 
g is not specified. Try running the program. The wrmpikr may issue 
a warning bacause g is declared to be a function on line 3, instead of 
a pink' to a function. 
a) What can you my about: the t y p  of Q? 

386 TYPE CHECKING 
CHAPTER 6 
(I) int n; 
Fig. 6.28. A C program containing applications of a function to itself. 
b) Use the type-checking rules for polymorphic functions in Fig, 6.18 
to infer a type for g in the following program. 
m : integer; 
times : integer X integer -. integer; 
g : a; 
BIBLIOGRAPHIC NOTES 
The bask trpes and t y p  constructors of early languages like Fortran and 
Algol HI were limited enough rhat type.checking was not a serious problem, 
As a result, descriptions of type checking in their compilars are buried in dis- 
cussions of code generation for expressions. Sheridan [ 1959) describes rhe 
translation of expressions in the original Fortran compiler. The cornpi ler kept 
track of whether the type of an expression was integer or real, but mercicrns 
were not permitted by the language. Backus 11981, p, 541 recalls, *'I think 
just k a u s t  we didn't like the rules as to what happened with mixed mode 
expressions, so we decided, 'Lel's lhrow it out. It's easier.' " Naur [1%5] is 
an early paper on t y p  checking in an Algol mrnpiler; the techniques used by 
the compiler are similar to those discussed in Section 6.2. 
Data-strucluring facilities such as always and ~~ecords 
were an ticipattd in the 
1940's by Zuse in his Plankalkiil that had little direct inftuence {Bauer and 
W6ssner 119721). One of the first programming languages to allow type 
expressions to be constructed systematically is Algd 68. Type expressions can 

CHAPTER 6 
BlI3tlOGRAPHlC NOTES 387 
be recursively defined, and structural equivalence i s  used. A clear distinction 
between name and st;tructura\ equivalence is found in ELI, the choice being 
left to the programmer (Wegbreit [1974]). The critique of Pascal by Welsh, 
Sneeringer, and Hoare [ 19771 drew attention to the distinction. 
The combination of coercion and overloading can lead to ambiguities: c a w -  
ing an argument may lead to overloading being resolved in favor oof a different 
algorithm. Restrictions are therefore placed on one or the other, A free- 
wheeling approach to coercions was taken in PLiI, where the first design cri- 
terion was, "Anyfhinip p w s .  If a particular combination of symbols has a rea- 
sonably sensible meaning, that meaning will be made official (Radln and 
Rogoway [ i965jj." An ordering Is often placed on the set of basic types - 
e.g.+ Hext 11%7j describes a lattice structure imposed on the basic types of 
CPL - 
and lower types may be coerced to higher types. 
Compile-time resolution of overloading in languages such as APL Iverson 
I 1 %2]) and SETL (Schwartz 1 19731) has [he potential for improving the run- 
ning time of programs (Bauer and Saal [ \974D+ Tennenbaum ! 19741 dis- 
tinguished between "forward" resolution that determines the set of possible 
types of an operator from its operands and "backward" resolution based on 
the type expected by the context, Using a lattice of types, Jones and 
Mucbnick 119761 and Kaplau and Ullman 119801 solve constraints on types 
obtained from forward and backward analysis. Overloading in Ada can be 
resolved by making a single forward and then a single backward pass, as in 
Section 6.5. This observation emerges from a number of papers; Ganzinger 
and Ripken 119801; Pennelb, DeRemer and Meyers 119801; Janas 119801; 
Persch et al. 1 19801. Cormack 11 98 1 ] offers a recursive implementat ion and 
Baker 119821 avoids an explicit backward pass by carrying along a dag of pos- 
sible types. 
- 
Type inference was studied by Curry (see Curry and Feys 119581) in con- 
nection with combinatory logic and the lambda calculus of Church 119411. I t  
has long been observed that the Iamtxlrt calculus is at the core of a tunctiunal 
language. We have repeatedly used the application of a flrndion to an argu- 
ment to discuss type-checking concepts in this chapter. Funclions can be 
defined and applied with no regard to types in the IarnMa calculus, and Curry 
was interested in their "functional character," and determining what we would 
now call a most general polymorphic type, consisting of a type expression with 
universal quantifiers as in Section 6.6, Motivated by Curry, Hindley [I9691 
observed that unification could be used to infer the types. Independently, in 
his thesis, Morris 11968al assigned types to !arnMa expressions by se~ting up a 
set of equations and solving them to determine the types associated with vari- 
ables. Unaware of H indley 's work, Milner 119781 also observed that unifica- 
tion could be used to solve the sets of equations, and applied the idea to infer 
types in the ML programming language. 
The pragmatics of type checking in ML are described by Cardelli [1984J. 
This approach has been applied to a language by Mcerterts 119831; Suzuki 
1 1981 1 exphes its application to Smalltalk 1976 (Ingalls 1 19781). Mitchell 

38% TYPE CHECKING 
CHAP~ER 6 
119841 shows how coercions can be included. 
Morris 11968aJ observes that recursive or circular types allow types to k 
inferred for expressions containing the application of a function to itself. The 
C program in Fig. 6.28, containing an application of a function to itself, is 
motivated by an Algol program in Ledgard 11971 j. Exercise 6.3 I is from 
MaQuetn, Plotkin, and Sethi [1984], where a semantic model for recursive 
polymorphic types is given. Different approaches appear in McCracken 119791 
and Cartwright 11985j. R e y n d d ~  119851 surveys the ML type system, themet- 
jcal guidclinefi for avoiding anomalies invdvin g coercions and overloading, 
and higher order polymorphic functions. 
Unification was first studied by Robinson I 19651. The unification algorithm 
in Section 6.7 can readily be adapted from algorithms for testing equivalence 
of ( 1) finite automata and (2) linked lists with cycles (Knuth [1973a]. Section 
2.3.5, Exercise I I), The almost linear algorithm for testing equivalence of 
finite automata due to Hopcroft and Karp 1197 L j can be viewed as an imple- 
mentation of the sketch on p. 594 of Knuth 1 1973aJ. Through clever use of 
data strudures, linear algorithms for the acyclic caw arc presented by Pater- 
son and Wegman 119781 and by Martelli and Montanari 119821. An algorithm 
for finding congruent nodes (see Exerciae 6.30) appears in Downey, Sethi, and 
, 
Tarjan[L9Mj. 
Despeyroux 1 1984) describes a typechecker generator that uses pattern 
matching to create a type checker from an operational-semantic spificatbn 
b a d  on inference rules. 

CHAPTER 7 
Run-Time 
Environments 
Before considering code generation, we need to relate the static source text of 
a program to the actions that must occur at run time to implement the pro- 
gram. As execution proceeds, the same name in the source text can denote 
different data objects in the target machine, This chapter examines the rela- 
tionship between names and data objects. 
The allocation and deallcxation of data objects is managed by the run-rime 
support package, consisting of routines loaded with the generated target code, 
The design of the run-time suppwt package i s  influenced by the semantics of 
procedures. Support packages for languages like Fortran, Pascal, and Lisp 
can be constructed using the techniques in this chapter. 
Each execution of a procedure i s  referred to as an acriwion of the pro- 
cedure. If the procedure is recursive, several of its activations may & alive at 
the same time. Each call of a procedure in Pascal leads to an activation that 
may manipulate data objects alhcated for its use. 
The representation of a data object at run time is dete~mined by its type. 
Often, elementary data types, such as characters, integers, and reals. can be 
represented by equivalent data objects in the target machine. However, 
aggregates, such as arrays, strings, and structures, are usually represented by 
cdlections of primitive objects; their layout is discuswd in Chapter 8. 
7.1 SOURCE LANGUAGE ISSUES 
For specificity, suppm that a program is made up of procedures, as in Pascal. 
This section distinguishes between the source text of a procedure and its 
activations at run time, 
A prmdure definition is a declaration rhat, in its simplest form, associates an 
identifier with a statement. The identifier is the prwedure HMY, 
and the 
statement is the prur-ehre M y .  For example, the P a w l  code in Fig. 7.1 con- 
tains the definition of the procedure named readarray on lines 3-7; the body 
of the procedure is on lines 5-7. Procedures that return values are called funr- 
tiom in many languages; however, it is convenient to refer them as 

390 RUN-TIME ENVIRONMENTS 
( I) program sort [ input, output ) ; 
var a : array [0+.103 of integer; 
procedure readarray; 
var i ; integer; 
begin 
for i := 1 to 9 do read(a[i]l 
end ; 
function part5tionIy, z :  integer) : integer; 
var i, j, x, v: integer; 
begin . . . 
end ; 
procedure quicksortlm, n: inttgtr); 
var i : integer; 
Begin 
if 
n r m 1 then begin 
i := partitionlm,n); 
quickaart[m,i-11; 
q u l c k s m t ~ i + l , n l  
end 
1 
end; 
b t g h  
a[O] :a -9999; all01 := 9999; 
readarray ; 
quicksort l 1 ,9 1 
end, 
Fa. 7.1, 
A Pascal program for reading and sorting integers. 
procedures. A complete program will also be treated as a procedure. 
When a procedure name a p p r s  within an executable statement, we say 
that the procedure is cdkd at that pint. The basic idea is that a procedure 
call executes the procedure b d y  
+ The main program in lines 21 -25 of Fig. 7.1 
calls the p r d u r e  readarray at line 23 and then calls quicksort at line 
24. Note that procedure calls can also occur within expressions, as on line 16. 
Some of the identifiers appearing in a procedure definition are special, and 
are called formal parmerers (or just jormals) of the procedure. (C calls them 
"formal arguments" and Fortran calls them "dummy arguments, ") The iden- 
qifiers m and n on line 12 are formal parameters of quicksort. Arguments, 
known as arml p a r m e i s  (or uctuals) may be passed to a called p r d u r e ;  
they are substituted for the formals in the body. Method$ for setting up the 
correspmdence between actuals and formals are discussed in Section 7+5. 
Line 18 of Fig. 7.1 is a call of quicksort with actlral parameters i+ 7 and n. 

Activation Trm 
We make the following assumptions a b u t  the flow of control among pro- 
cedures during the execution of a program: 
1, 
Contml flows sequentially; that is, the execution of a program consists of 
a sequence of steps, with control king at some spcific poinr in the pro- 
gram at each step. ' 
2. 
Each execution of a procedure starts at the beginning of the procedure 
body and eventually returns control to the point immediately fobwing 
the place where the procedure was called. This means the flow d control 
between procedures can k depicted using trees, as we shall soon see. 
Each execution of a prwedure body Is referred to as an acthation of the 
prmdure, The Iifetime of an activation of a procedure p is the sequence of 
steps between the first and !as! steps in the execution of the procedure M y ,  
including time spent executing procedures called by p, the procedures called 
by them, and so on. in general, the term "lifetime" refers to a consecutive 
sequence of steps during the execution of a program. 
In languages like Pascal, each time control enters a procedure q from pro- 
cedure p, it eventually returns to p (in the absence of a fatal error). More 
precisely, each time control flows from an activation of a procedure p to an 
activation of a procedure q, it returns to the same activation of p. 
If a and b arc p r a d u r e  activations, then their lifetimes are either non- 
overlapping qr are nested. That is, if b i s  entered before a is left, then control 
must leave b before it leaves a. 
This nested property of activation lifetimes can be illustrated by inserting 
two print statements in each procedure, one before the first statement of the 
prmdurc M
y
 
and the other after the last, The first statement prints enter 
followed by the name of the procedure and the values of the actual parame- 
lers; the last statement prints leave fotlowed by the same information. One 
execution of the program in Fig. 7.2 with these print statements produced the 
output shown in Fig. 7,2. The lifetime of the activation quicksort ( 1 ,9 1 is 
the sequence of steps executed between printing enter quicksort ( 1 ,9} 
and printing leave quicksort( l , 9  ). In Fig. 7.2, it i s  assumed that the 
value returned by partition( 9,9 ) i s  4. 
A procedure i s  recursive if a new activation can begin before an earlier 
aclivatim of the same procedure has ended. Figure 7,2 shows that control 
enters the activation of quicksort ( 'I ,9 ) , from Iine 24, early in the execu- 
tion of the program but leaves this activation almost at the end. In the mean- 
time, there are several other activations of quicksort, so quicksoxt is 
recursive. 
A recursive procedure p need not call itself direct!y; 
may call another pro- 
cedure p, which may then call p through some sequence of procedure calls. 
We can u s  a tree, called an activuii~n free, to depicf the way control enters 
and leaves activations. In an activation tree, 

392 
RUNTIME ENVIRONMENTS 
Execution begins.,. 
enter readarray 
leave readarray 
enter quicksort~l,9~ 
enter partition( l , 9  1 
ltave gartition( I, 
9 1 
enter quicksort( l , 3 1  
leave quicksortIl,3~ 
enter quicksort l 5 , 9  ) 
. . .  
leave quicksort l S , 9  1 
leave quick~ort I, 
9 1 
Execution terminated. 
Fig. 7.2. Output suggesting activations of proccdurcs in Fig. 7. I 
each nude represents an activation of a procedure, 
the rmt represents the activation of the main program, 
the node for a is the parent of the node for b if and only if control flows 
from activation a to b, and 
the node for a i s  to the left of the node for b if and only if the lifetime of 
a occurs before the lifetime of b. 
Since each node represents a unique activation, and vice versa, i t  is convenient 
to talk of control king at a node when it is in the activation wprtsentd by 
the node. 
Example 7.1. The activation tree in Fig. 7.3 is constructed from the output in 
Fig. 7.2. ' To save space, only the first letter of each procedure is shown. The 
root of  he activation tree i s  for the entire program sort. During the exwu- 
tion of sort, there is an activation of readarray, represented by the first 
child of the root, with label r. The next activation, represented by the second 
child of the ~oot, is for quicksort, with actuals 1 and 9, During this 
activation, the calls of partition and quicksort on lines 16-18 of Fig. 7,l 
lead to the activations p l 9 , 9  ), q ( 1 , 3  1 , and q E 5,9 1 . Note that the activa- 
t ions q I I ,3 1 and q ( 5,9 1 are recursive, and that they begin and end before 
qI 1,9l ends. 
D 
' The actual d l r  ma& by quickbart dcpcnd on what partitian mum (a 
Aho, Hopcroft. 
and Ullman [ IB31 for dctails of thc algorithm). Figure 7 -3 represents one possible trer of mlh. 
Ir i s  mnnisrcnt with Fig. 7.2, although ccrtain calls low in the trcc arc not xhihown in Fig 7.2. 

SOURCE LANGUAGE ISSUES 393 
Fie. 7.3. An activation trcc corresponding to the output in Fig. 7.2. 
The flow of control in a program corresponds to a depth-first traversal of the 
activation tree that starts at the root, visits a node before its children, artd 
recursively visits children at each node in a left-to-right order. The output in 
Fig. 7.2 can therefore be reconstructed by traversing the activation tree in Fig. 
7.3, printing enter when the node for an activation is reached for the first 
time and printing leave after the entire subtree of the node has k e n  visited 
during the traversal. 
We Can use a stack, called a ronmd siwk to keep track of live procedure 
activations, The idea is to push the node for an activation onto the control 
stack as the activation begins and to pop the node when the activation ends. 
Then the contents of the control stack are related to paths to the root of the 
activation tree. When node n is at the top of the control stack, the stack con- 
tains the nodes along the path from n to the rmt. 
E m p k  7,2. Figure 7.4 shows rides from the activation tree of Fig. 7+3 that 
have been reached when control enters the activation represented by q I 2,3 ) . 
Activations with labels r, g( 1,9 ) , pI 1 , 3  1, and qI 1 , 
O 1 have executed to 
completion, so the figure contains dashed lines to their nodes. The d i d  lines 
mark the path from qi 2 , 3  1 to the root- 
Fig. 7.4. Thc C ~ n t r d  
stack contains nodes don$ a path to thc root. 

394 
RUN-TIME ENVIRONMENTS 
SEC. 7.1 
At this point the control stack contains the folbwing rides along this path 
to the root (the top of the stack is to the right) 
and no other nodes. 
13 
Control stacks extend to the stack storage-allocation technique used to 
impiement languages such as Pascal and C. This technique is discussed in 
detail in Sections 7.3 and 7.4. 
The Scope of a Dedaratim 
A declaration in a language is a syntactic construct that associates information 
with a name. Declarations may be explicit, as in the Pascal fragment 
var i : integer; 
or they may be Implicit. For example, any variable name starting with I is 
assumed to denote an integer in a Fortran program, unless otherwise declared. 
There may be independent declarations of the same name in different parts 
of a program. The scope rules of a language determine which declaration of a 
name applies when the namc appears in the text of a program. In the Pascal 
program In Fig+ 7,1, i 
is declared thrice, on lines 4, 9, and 23, and the uses 
of the name i in procedures readarray, partition, and quickeort are 
independent of each ather. The declaration on line 4 applies €0 the uses of i 
on line 6. That is, the two occurrences of i on line 6 are in the scope of the 
declaration on line 4. The three occurrences of i on lines 16-18 are in the 
scope of the declaration of i on line 13. 
The portion of the program to which a declaration applies is called the scope 
of that declaration. A n  occurrence of a name in a procedure is said to be 
l o r d  to the procedure if it is in the scope of a declaration within the pro- 
cedure; otherwise, the occurrence is said to he mnhI. The dis~inclion 
between local and nonlmal names carries over to any syntactic construct that 
can have declarations within it. 
While scope is a property of the declaration of a name, it is sometimes con- 
venient to use the abbreviation "the mpe of name x" for "the scope of the 
dedarat ion of name x that applies to this occurrence of x." In this sense, the 
scope of i on line 17 in Fig. 7. 
I is the body of 
At compile time, the symbol table can be used to find the declaration that 
applies lo an occurrence of a name. When a declaration is seen. a symbd- 
table enlry is created for it. As Iong as we are in the swpe of the declaration, 
its entry is returned when the name in it is looked up. Symbol tables are dis- 
cussed in Section 7.6, 
? M w  of tho timc, ~ h c  
tcrms namc, idcntificr, variablc. and kkcrnc can bc wxl ibtcrchrngcsbly 
without lhcrc k i n g  any nmfuwn a b m t  thc intcndd ccrnslruci. 

SEC. 7.1 
SOURCELANGUAGE ISSUES 395 
Bindings of Names 
Even if each name is declared once in a program, the same name may denote 
different data objects at run time. 
The informal term "data object" 
corresponds to a storage locat ion that can hold values. 
In programming language semantics, the term ritvirummr refers to a func- 
tion that maps a name to a storage Imation, and the term s i m  refers to a 
funcrion that maps a storage location to the value held [here, as in Fig. 7.5. 
Using the terms !-value and r+.value from Chapter 2, an environment maps a 
name to an r'-value, and a state maps the /-value to an r-value. 
namc 
storage 
valuc 
Ftg, 7.5, Two-stagc mapping from namcs to valucs. 
Environments and states are different; an assignm~nt changes the state, but 
not the environment. For example, suppose that storage address 100, a w c i -  
ated with variable pi, holds 0. After the assignment p i  := 3. 14, the same 
storage address is aswciated with pi, but the value held there is 3.14. 
When an environment associates storage location s with a name x, we say 
that x is h n d  to s; the association itself is referred to as a binding of x. The 
term storage "location" is to be taken figuratively. If x is not of a basic type, 
. the storage s for x may be a collection of memory words. 
A binding is the dynamic counterpart of a declaration, as shown in Fig. 7.6. 
As we have seen, more than one activation of a recur*- 
can be 
alive at the same time. In Pascal, a local variable name in a procedure is 
bund to a different storage location in each activation of a procedure. Tech- 
niques for binding local variable names are considered in Section 7.3. 
Fig. 7.6, Corrcspclnding static and dynamic notions. 
definition nf a primdurc 
dcclarat irm of a namc 
smpc of a declaration 
activations of thc prt~cdurc 
bindings of thc namc 
liktimc of a binding 

3% 
R UN-TIME ENVIRONMENTS 
Questions to Ask 
The nay a compiler for a language must 0rganiz.e its storage and bind names 
is determined largely by answers to questions like the foilowing: 
May procedures be recursive*? 
What happens to the values of Iocal names when control returns from an 
activation of a procedure'? 
May a procedure refer to nonlocal names'? 
How are parameters passed when a procedure is called'? 
May procedures be p a w d  as parameters'' 
May procedures be returiied as results? 
May storage be allocated dynamically under program control? 
Must sbrage be deallocated explicitly? 
The effect of these issues on the run-time support needed for a given pro- 
gramming language is examined in the remainder of this chapter. 
7.2 STORAGE ORGANlZATlON 
I 
The organization of run-time storage in this sect ion can lae u
~
d
 
for languages 
such as Fortran, Pascal, and C. 
Subdivision of Run-Time Memory 
Suppose that the compiler obtains a black of storage From the operating sys- 
tem [or the compiled program to run in. From the discussion in the last sec- 
tion, this run-time storage might be subdivided to hold: 
I. the generated target code, 
2. 
data objects, and 
3. 
a counterpart of the control stack to keep track of procedure activations. 
The size of rhe generated target c d e  is fixed at compile time, so the com- 
ptler can place it in a statically determined area, perhaps in the bw end of 
memory. Similarly, the size of some d the data objects may also be known at 
compile time, and these too can be placed in a statically determined area, as 
in Fig, 7.7, One reason for statically allocating as many data objects as possi- 
ble is that the addresses of these objwts can be compiled into the target code. 
All data objects in Fortran can be allocated staticaliy. 
Implementations of languages like Pascal and C use extensions of the con- 
trol stack to manege activations of procedures, When a call occurs, execution 
of an activation is interrupted and information about the status of the 
machine, such as the value of the program munkr and machine registers, is 
saved on the stack. When control returns from the call, this activation can k 
restarted after restoring the values of reievant registers and setting rhe pro- 
gram counter to the point immediately after the cdl. Data objects whose life- 

STORAGE ORGANlZATlON 397 
Fig. 7,7. Typical subdivision of run-timc mcmnry into d
c
 
and data areas, 
times are contained in that of an activation can be allocated on the slack, 
along with other informat ion associated with the activation. This strategy is 
discussed In the next section. 
A separate area of run-time memory, called a heup, holds ail other informa- 
tion. Pascal allows data to be allocated under program control, as discussed 
in Section 7.7; the storage for such data is taken from the heap. lmpkmenta- 
tions d languages in which the lifetimes of activations cannot be represented 
by an activation tree might use the heap to keep information about activations, 
The controlled way in which data is allocated and deallocated on a stack 
makes it cheaper to plae data on the stack than on the heap. 
The sizes of the stack and the heap can change as the program executes, MI 
we show these at opposite ends of memory in Fig. 7.7. where they can grow 
toward each other as needed. Pascal and C need both a run-time stack and 
heap, but not all languages do. 
By convention, stacks grow down. .That is, the "top" of the stack is drawn 
towards the tottom of the page. Since memory addresses increase as we go 
down a page, "down wards-growing " means toward higher addresses. If top 
marks the top of the stack, offsets from the top of the stack can be computed 
by subtracting the offkt from top. On many machines this computation can 
be done efficiently by keeping the value of top in a register. Stack addresses 
can then k represented as offsets from iq.' 
' Thc r~rganw.ntion in Fig. 7.7 ~ S S I J ~ C S  
that thc run-timc mcmrwy consists of a single crmliguws 
blwk of wmgc obtained at thc start of cxccutiim. This awmption impincs a fixcd limit on the 
wmbincd sizcs o f  thc stack md hcap. If thc limit i s  hrgc cnough thal it is rarcly c~cccdcd. thcn 
11 may h wasrchlly largr for mua programs. Thc idwrmativc crf linking objcas on the stack and 
heap may makc it mtm cxpcrt*ivc €0 kccp track of Ihc lop nf thc htack. Furthcrmurc, thc rargct 
machinc may faror a ditkrcnr placcrncnr r,f arcas. F~IT cxilrnplc. scmc machincs alliw jud pwi- 
tiuc. o f f ~ t s  
from an addrck! in a rcgixtcs. 

398 
RUN-TIME ENVIRONMENTS 
Activation Records 
Information needed by a single execution of a procedure is managed using a 
contiguous block of storage called an urbtivution rerursl or frume, consisting of 
the collection of fields shown in Fig. 7.8. Not all languages, nor all compilers 
use all of these fields; often registers can take the place of one or more of 
them. For languages like Pascal and C, it is customary to push the activation 
record of a procedure on the run-time stack when the procedure is called snd 
to pop the activation recwd off the stack when control returns to the caller. 
The purpose of the fields of an activation tecord is as follbws, starting from 
the field for temporaries. 
Temporary values, such as those arising in i he evaluation of expressions, 
are stored in the field for temporaries. 
The field for lwal data holds data that is local to an execution of a pro- 
cedure. The layout of this field is discussed below. 
The fkld for saved machine status holds information about the state aE 
the machine just before the procedure i s  called. This information 
includes the values of the program counter and machine registers that 
have to be restored when control returns from the procedure. 
The optional access link is used in Section 7.4 to refer to nonlwd data 
held in other activation records. For a language like Fortran access links 
are not needed because nonlocal data is kept in a Fixed place. Access 
links, or the related "display" mechanism. are needed for Pascal. 
The optional cmfrol /ink paints to the activation record of the caller 
local data 
- - - - - -  - - - -  I 
Fig, 7.8. A gcncrd activation rccord. 

SEC. 7.2 
SFOR AGE ORGANIZATION 399 
6. The field for actual parameters is used by the d i n g  procedure to supply 
parameters to the called procedure. We show space for parameters in the 
activation record, but in practice parameters are often passed in machine 
registers for greater efficiency. 
7, 
The fild for the returned value is used by the called procedure to return 
a value to the calling procedure, Again, in practice this value is often 
returned in a register for greater efficiency. 
The sizes of each of these fields can be determined at the time a procedure is 
called. In fact, the sizes of almost all fields can be determined at compile 
time. An exception occurs if a procedure may have a local array whose size i s  
determined by the value of an actual parameter, available only when the pro- 
cedure is called at run time. See Section 7.3 for the allocation of variable- 
kngth data in an activation record. 
Compile-Time Layout of LOEd Data 
Suppose run-time storage comes in blwkxd mntiguws bytes, where a byte is 
the smallest unit of addressable memory. On many machines, a byte is eight 
bits and some number of bytes form a machine word. Multibyte objects are 
stored in consecutive bytes and given the address of the first byte. 
The amount of storage needed for a name i s  determined from its t y p e  An 
elementary data type, such as a character, integer, or real, can usually be 
stored in an inregrd number of bytes. Storage for an aggregate, such as an 
array w record, must be large enough to hold all its components. For easy 
access to the components, storage for aggregates is typically allocated in one 
mntiguous block of bytes. See Sections 8+2 and 8.3 for more details. 
The field for local data is laid out as the declarations in a procedure are 
examined at compile tine, Variable-length data is kept outside this field. We. 
keep a count of the memory locations that have been allocated for previous 
declarations. From the cwnt we determine a rdrrrivu address of the storage 
for a local with respect to some position such as the beginning of the activa- 
tion record. The relative address, or oflsc.r, 
is the difference between the 
addresses of the psition and the data object. 
The storage layout for data objects is strongly influenced by the addressing 
constraints of the target machine, For example, instructions to add integers 
may expect integers to be aligned, that is, placed at certain positions in 
memory such as an address divisible by 4. Although an array of ten charac- 
ters needs only enough bytes to hold ten characters, a compiler may [here fore 
allocate 12 bytes, leaving 2 bytes unused. Space left unused due to alignment 
considerations is referred to as prrrldiqy. When space is at i premium. a torn- 
piler may pack data so thal no padding is left; additional irtstructiuns may then 
need to be executed at run time to position packed data so that ir can be 
operated on as if it were properly aligned. 

400 
RUN-TIME ENVIRONMENTS 
SEC. 7.2 
Example 7+3, Figure 7.9 is a simplification of the data layout used by C corn- 
pikrs for two mad-tincs lhat we call Machine I and Machine 2. C provides 
for three sizes of integers, declared using the keywords short, ink, and 
long. The instruction sets of the two machines are such that the compiler for 
Machine 1 allocates 16, 32, and 32 bits for the three sizes of integers. while 
the compiler for Machine 2 allocates 24, 48, and W bits, respectively. For 
comparison ktween machines, sixes are measured in bits in Fig. 7.9, even 
though neither machine allows bits to be addressed directly. 
The memwy of Machine 1 is organized inlo bytes consisting of 8 bits each. 
Even rhwgh every byte has an address, the instruction set favo1.s short 
inlcgers being positioned at bytes whose addresses are even, and integers 
king positioned at addresses lhat arc divisible by 4. The compiler places 
short integers at even addresses. even if it has to skip a byte as padding in the 
process. Thus, four bytes, consisting of 32 bits, may be abcated for a char- 
acter folbwed by a short integer, 
In Machine 2, each word consists of 64 bits, and 24 bits are allowed for the 
address of a word. There are 64 pssjbilitks for the individual bits inside a 
word, so 6 additional bits are needed to distinguish ktween them. By design, 
a pointer to a character on Machine 2 takes 30 bits - 
24 ro find the word and 
6 for the position of the character inside the word. 
The strong word orientation of the instruction set of Machine 2 has led the 
compiler to allocale a complete word at a time, even when fewer bits would 
suffice to represent all possible values of that type; e.g., only 8 bits art needed 
to represent a character. Hence, under alignment, Fig. 7.9 shows 64 bits for 
each type. Within each word, the bits for each basic type are in specified 
positions.. Two words consisting of I28 bits would be allocated for a character 
followed by a short integer, wilh the character using only 8 of the bits in the 
first word and the short integer using only 24 of the bit8 in the second word. 0 
char ............. 
short ........... 
............... 
int 
1 ong ............. 
float *.......... 
double ......... 
charrultcr pointer 
other pointers . + .  
struclurcs ......... 
Machinc I 
3its) 
Machinc 2 
Machinc I I Machinc 2 
" Charslctcrs in an array arc nligncd cvcry 8 bits- 
Fig. 7-9. Data layouts uxd by two C cr~mpilcrs+ 

SEC. 7,3 
STORAGE-ALLOCATlON STRATEGIES MI 
7,3 WOlUGE*ALLOCATION STRATEGIES 
A different storage-allocation strategy is used in each of the three data areas 
in the organization of Fig, 7.7, 
1. 
Static allocation lays out storage for all data objects at compile time. 
2+ Stack ahcation manages the run-time storage as a stack, 
3, 
Heap albcation allocates and deallocates storage as needed at run time 
from a data area known as a heap, 
These allocation strategies are applied in this section to activation records. 
We also describe how the target code of a proced~re accesses the storage 
bound to a local name, 
In static allocation, names are bunb to storage as the program is compiled, so 
there is no need for a run-time support package. Since the bindings do not 
change at run time, every time a procedure is activated, its names are bound 
to the same storage locations. This property allows the values of Iwal names 
to be retained across activations of a procedure. That is, when control returns 
to a procedure, the values of the locals are the same as they were when con- 
trol left the last time. 
From the type of a name, the compiler determines the amount of storage to 
set aside for that name, as disclrssed in Section 7.2. The address of this 
storage consists of an offset from an end of the activation record for the pro- 
cedure. The compiler must eventually decide where the activation records go, 
relative to the target c d e  and to one another. Once this decision is made, the 
psition of each activation record, and hence of the storage for each name in 
the record is fixed. At compile time we can therefore rill in the addresses at 
which the target code can find the data it operates on. Similarly, the 
addresses at which information is to be saved when a procedure call occurs arc 
also known at cornpile time. 
However, same limitat ions go along with wing static allocation alone. 
1. 
The size of a data object and constraints an its position in memory mud 
k known at compile time. 
2. 
Recursive procedures are restricted, because all activations of a procedure 
use the same bindings for local names. 
3, 
Data structures cannot k created dynamically, since there is no mechan- 
ism for storage allocation at run time. 
Fortran was designed to permit static storage a l b t i o n .  A Fotrran pro- 
gram consists of a main progiam, subroutines, and functions (call them 311 
procedures), as in the Fortran 77 program of Fig. 7.10. Using the memory 
organization of Fig. 7.7, the layout of the code and the activation records for 

402 
RUN-TIME ENVIRONMENTS 
SEC. 7.3 
PROGRAM CNSUME 
CHARACTER * 50 BUF 
INTEGER NEXT 
CHARACTER C, PRDUCE 
DATA NEXT /1#, BUF #' '/ 
C = PRDUCE( 1 
BUF(trEXT:NEXT) = C 
NEXT = NEXT + 1 
IF ( C .NE. ' ' ) GOTO 6 
WRITE I * , ' ( A ) ' )  
BUP 
END 
CHARACTER WNCTION PRDUCE() 
CHARACTER 4 80 BUFFER 
INTEGER NEXT 
SAVE BUFFER, NEXT 
DATA NEXT 181~' 
IF I NEXT .GT- 80 1 THEN 
READ 
[ * , * l A ) ' J  BUFF'ER 
NEXT 1 1 
m D  IF 
PRDUCE = BUFFER I NEXT : NEXT ) 
NEXT = NEXT+1 
END 
Fig. 7.10. A Fortrad 77 program. 
this program is shown in Fig. 7.1 1: Within the activation'record for C N s W  
(read "mnsumc" - 
Fortran does not like long identi'fiers), there is space for 
the locals BUF, NEXT, and C. The storage bound to BUF holds a string of 
fifty characters. I t  is followed by space for holding an integer value for NEXT 
and a character value for C. The fact that WEXT Is also declared io PRDUCE 
presents no problem, because the Icxals of the two p r d u r e s  get space in 
their respective activation records. 
Since the sizes of the executable code and the activation records are known 
at compile time, memory organizations other than the one in Fig. 7.11 are 
s
b
e
 A Fortran compiler might place the aclivation record for a pro- 
cedure together with the code for that procedure. On some computer systems, 
it is feasible to leave the relative position of the activation records unspecified 
and allow the link edimr to link activation records and executable code. 
Elrampee 7.4. The program in Fig. 7.10 relies on the values of locals being 
rerained across procedure activations. A SAVE statement in Fortran 77 speci- 
fies that the value of a local at the kginning of an activalion must be the 
same as that at the end of the last activation. Initial values for these locals 
can be specified using a DATA statement. 

SEC. 7.3 
=OR AGE-ALLOCATION STR ATEGlES 403 
C d c  for CNSUME 
Code for PRDUCE 
I 
INTEGER NEXT 
I 
CHARACTER C c 
- - - - - - - - - - - - - - - -  
CHARACTER+30 BUFFER 
IlWEGm NEXT 
1 
Activation Record 
for CNsuMe 
I 
Activation R w r d  
for PRDUCE 
4 
Fig. 7.11, Static storagc for 1-1 
idcntific~s of a Fortran 77 program. 
The statement on line 18 of procedure PRDUCE reads one line of text at a 
time into a buffer. The procedure delivers successive characters each time it 
is activated. The main program CNSUME also has a buffer in which it accumu- 
lates characters until a blank is seen. On input 
hello world 
the characters reiurned by the activations of PmuCE are depicted in Fig. 7.12; 
the output of the program is 
hello 
The buffer into which PRDUCE reads lines must retain its value between 
activations. The SAVE statement on line 15 ensures that when control returns 
to PRDUCE, locals BUFFER and NEXT have the same values as they did when 
control left , the procedure the last time, The first time control reaches 
PRDUCE, the value of the bcal NEXT is taken from the DATA statement an 
line 16. In this way, NEXT is initialized to 81, 
PRDVCE 
PRWCE 
PRDUCE 
PRWCE 
PRPUCE 
PRDUCE 
h 
e 
1 
1 
o 
I 
* 
Fig, 7-12, Characters rcturncd by aclivatiuns of PRDUCE. 

404 RUN-TIME ENVIRONMENTS 
Stack allocatim is based on the idea of a control slack; storage is organized as 
a stack, and activation records are pushed and popped as activations begin and 
end, respectivtly. Storage for the locals in each call of a procedure i s  con- 
tained in the activation record for that call. Thus loca$ are bound to fresh 
storage in each activation, because a new activation record is pushed onto the 
stack when a call i s  made. Furthermore, the values of locals are dcIetd when 
the activation ends; that IS, the values arc lost because the storage for locals 
disappears when the activation record is popped, 
We first describe a form of stack allocation in which the sizes of a11 act iva- 
tion records are known at compilt time. Situations In which incomplete infor- 
mation about sites is available at compile time ate considered below. 
Suppse that register fop marks the top of the stack. At run time, an 
activation record can be allocated and deallocated by increrntnting and decre- 
meriting top, respectively, by the size of the recard. If procedure q has an 
activation record of size a. then top is incremented by a just before the target 
code of q i s  executed. When control returns from q, top is decremented by a. 
Example 7.5. Figure 7.13 shows the activation records that are pushed onto 
and popped from the run-time stack as contrd flows through the aclivation 
tree of Fig, 7+3. Dashed lines in the tree go to activalions that have ended. 
Execurion begins with an activahn of procedure s. When contrd reaches the 
first call in the b d y  of s, procedure r is activated and its activation record is 
pushed onto the stack+ When control returns from this activation, the record 
is popped leaving just the record for s in the stack. In the activation of s, 
control then reaches a call of q with actuals 1 and 9, and an activation record 
is allocated on top of the stack fix an activation of q, Whenever control is in 
an activation, its activation record is at the top of the stack. 
Several activations occur between the last two snapshots in Fig. 7.13. In the 
lad snapshot in Fig. 7.13, activations p( I , 3 )  and qI 1 , O )  have begun and 
ended during the lifetime of q1 1 , 3 ) ,  w their activation records have comc 
and gone from the stack, leaving the activation record for q( 1 , 3  1 on top. 
0 
In a Pascal procedure, we can determine a relative address for local data in 
an activation record, as discuwied in Section 7+2. At run time, suppose cup 
marks the location of the end of a record. The address of a local name x in 
the target code for the procedure might therefore be written as &(top), to 
indicate that the data bound to x can be found at the location obtained b~ 
adding rlr to the value in register top. Note that addresses can abernativcly be 
taken as offsets from the value in any other register r p i n t  ing to a fixed posi- 
tion in the dctivation record. 
Procedure calls are implemented by generating what are known as d h g  
srlyuunws in the target code. A r d  suqucnc.e allocates an activation record 

SEC. 7.3 
STORAGE-ALLOCATION STRATEGIES 405 
Frame for s 
r is activated 
Frame for r 
has been popped 
and q( 1.91 
pushed 
Control has just returned 
10 qi 1,3 1 
Fig. 7.13, Downward-growing stack allocidrion of activation records. 
and enters information into its fields. - A return srquencc restores the state of 
the machine so the calling procedure can continue execution, 
Calling sequences and activation records differ, even For implementations of 
the same language. The code in a calling sequence is often divided ktween 
the tailing procedure (the caller) and the procedure il calls (the calk). There 
is no exact division of run-time tasks between the caller and cake - the 

a 
RUN-TIME ENVIRONMENTS 
SEC. 7.3 
source language, the target machine, and the operating system i m p =  requirc- 
ments that may favor one solution over another.' 
A principle that aids the design of calling sequences and activation records 
is that Fields whose sizes are fixed early are placed in the middle. In the gen- 
eral activation record of Fig. 7.8, the control link, access link, and machine- 
status fields appear in Ihe middle. The decision a b u t  whether or not to use 
control and access links is part of the design of the compiler, so these fields 
can be fixed at compilertcr)nstruction time. If exactly the same amount of 
machine-status information is saved for each activation, then the same code 
can do the saving and restoring for all activations. Moreover, programs such 
as debuggers will have an easier time deciphering the suck contents when an 
error wcurs. 
Even though the size of the field for temporaries is eventually fixed at com- 
pile time, this size may not be known to the front end. Careful code genera- 
tion or optimization may reduce the number of temporaries needed by the pro- 
cedure, so as far as the front end i s  concerned, the size of this field is 
unknown. In the general activation record, we therefore show this field after 
thar for local data, where changes in i t s  size will not affect the offsets of data 
objects relative to the fields in the middle. 
Since each call has its own actual parameters, the ualler usually evaluaks 
actual parameters and communicates them to the activation record of the cal- 
lee. Methods for passing parameters are discussed in Section 7,5. In the 
run-time stack, the activation record of the caller is just below that for the cal- 
lee, as in Fig. 7.14, There i s  an advantage to placing the fields for parameters 
and a potential returned value next to the activation r m r d  of the caller. The 
caller can then access these fields using offsets from the end of its own activa- 
tion record, without knowing the complete layout of the record for the c a l k .  
In particular, there is no reason for the caller to know a b u t  the local data or 
temporaries of the callee. A benefit of this information hiding is that pro- 
cedures with variable numbers of arguments, such as printf in C. can be 
handled, as discussed below. 
Languages like Pascal ~equire arrays local to a procedure to have a kngth 
that can be determined at compile time. More often, the size of a local array 
may depnd on the value of a parameter passed to the procedure. In that 
case, the size of all the data local to the prwedure cannot k determined until 
the procedure is calkd. Techniques for handling variable-length data arc dis- 
cussed later in this section, 
The following call sequence is motivated by the abve discussion, As in 
Fig. 7.14, the register rup-sp points to the end of the machine-status field in 
an activation record, This position is known to the caller, so it can be made 
responsible for setting top-sp before control flows to the called procedure. 
If a pmedurc is called n times. then h e  portion of the calling sequence in the various callers is 
generared n limes. However, the portion in the catlet is shared by all calls, so it i s  generared just 
once. Hence, it is desirable to put as much of the calling sequencx into the callee as possible. 

SEC. 7.3 
STORAGE-ALLOCATION WRATEGIES 407 
( I parameters and returned value 
links and savcd status 
- - -  - - - - - - - - - - - - -  
temporaries and local data 
\ I parameters and returned value 
links and saved status 
f U P 4 '  
- - - - - - - - - - - - - - - -  
tempora~ics and local data 
a 
Caller's 
rcsponsibilit y 
CalIcc's 
responsibility 
T 
Caller's 
activation 
remrd i 
Callec's 
act hat ion 
record 
I I 
Fig. 7.14. Division of tasks betwecn caller and callec. 
The code for the callee can access its temporaries and local data using offsets 
from top~p. The call seq uenw is: 
I . The caller evaluates actuals, 
2. The d e r  stores a return address and the old value of r o p ~ p  
into the 
cake's activation record. The caller then increments iupsp to the posi- 
tion shown in Fig. 7.14. That is, ropdp is moved past the caller's local 
data and temporaries and the calk's parameter and status fields. 
3. 
The c a l k  saves register values and other status information, 
4. 
The c a l k  initializes its lwal data and begins execution. 
A possible return sequence is; 
1. 
The c a k e  places a return value next to the activation record of the caller. 
2, 
Using the information in the status field, the callee restores r o p p  and 
other registers and branches to a return address in the caller's code. 
J 
3. 
Although iupsp has been decremented, the caller can copy the returned 
value into its own activation record and use it to evaluate an expression. 
The above calling sequences allow the number of arguments of the called 
procedure to depend on the call, Note that, at compile time, the target code 

408 
RUN-TIME ENVIRONMENTS 
SEC. 7.3 
of the caller knows the number of arguments it is supplying ro the cake. 
Hence the caller knows the size of the parameter field. However, the target 
code of the callee must be prepared to handle other calls as well, st, it waits 
until it is called, and then examines the parameter field. Using the organiza- 
tion of Fig. 7,14, information describing the parameters must be placed next 
to the status field so the cake can find it. For example, msider the stan- 
dard library function printf in C. The first argument of printf specifies 
the nature of the remaining arguments, so once printf can h a t e  the first 
argument, it can find the remaining ones. 
A common arategy for handling variable-length data is suggested in Fig. 7.1 5, 
where procedure p has three 1
~
1
 
arrays. The storage for these arrays is not 
part of the activation record for p; only a pointer to the beginning of each 
array appears in the activation record. The relative addresses of these 
pointers are known at compile time, so the target code can access array ele- 
ments through the pointers. 
= 
- - - - - - - - - - - - - - - - - - -  
- writrd link 
i 
Act ivat iun record 
for p 
Arrays of p 
- - - - - - - - - - - - - - - - - - -  
array C 
- - - - - - - - - - - - - - - - - - -  
t 
Activation rword for 
tqup 
pmccdurc q callcd by p 
k 
I 
I 
Arrays of q 
Fig. 7.15. Acccw tto dynamically allocalcd arrays. 

SEC. 7.3 
mORAGE-ALLOCATION STRATEGIES 409 
Also shown in Fig. 7.15 is a procedure q m k d  by p. The activation record 
for q begins after the arrays of p, and the variable-length arrays of q begin 
beyond that. 
Access to data on the stack is through two pointers, top and top-rp, 
The 
first of these marks the actual top of the stack; it points to the position at 
which the next activation record will begin. The second is used to find local 
data, For consistency with the organization of Fig. 7.14, suppose rap+ 
points to the end of the machinestatus field. In Fig. 7.15, rop-~p points to 
the end of this field in the activation record for q. Within the field is a con- 
trol link to the previous value of top-rp when control was in the calling activa- 
tion of p. 
The d
e
 
to reposition top and SOP- 
can be generated at compile time, 
using the sizes of the fields in the activation records. When q returns, the 
new value of top is top+ 
minus the length of the machine-status and parame- 
ter fields in q's activation record, This length is known at compile time. at 
least to the caller. After adjusting top, the new value of top+ 
can be copied 
from the control link of q. 
Whenever storage can be dallocated, the problem oi dangling references 
arises. A dangling rt.Serence occurs when there is a reference to storage that 
has k e n  dcstlloca~cd. It i s  a logical error to use dangling references, since the 
the value of deallocated storage is undefined according to the semantics of 
most languages. Worse, since that storage may later be allocated to another 
datum, mysterious bugs can appear in programs with dangling references. 
Example 7.6, 
Procedure dangle in the C program of Fig. 7.16 returns a 
pointer to the storage bound to the local name i. The pointer is created by 
the operator & applied to i. When control returns to main from dangle. the 
storage fa locals is freed and can be used for other purposes. Since p in 
main rekrs to this storage, the use of p is a dangling reference. 
o 
An example in Section 7.7 involves deallocation under program control. 
main l ) 
{ 
int *p; 
p = dangle(): 
1 
i n t  *dangle I 1 
I 
i n t  i = 23; 
return 61; 
1 
Fig. 7.16. A C program that I c a w  p pointing to dcallocatd storagc. 

410 
RUN-TIMEENVIRONMENTS 
Heap Allocation 
SEC. 7.3 
The stack allocation strategy discussed above cannot be used if either of the 
following is possible. 
I .  
The values of bcal names must be retained when an activation ends. 
2. 
A called activation outlives the caller. This possibility cannot occur for 
those languages where activation trees correctly depic~ the flow of control 
between prmdures. 
In each of the above cases, the dealkation of activation records need not 
occur in a last-in firstaut fashion, so storage cannot be organized as a stack. 
Heap allucation parcels out pieces of contiguous storage, as needed for 
activation records or other objects. Pieces may be deallocated in any order. so 
aver time the heap will consist of alternate areas that are free and in use. 
The difference between heap and stack allocation of activation records can 
be seen from Fig. 7.1 7 and 7.13. In Fig. 7.17, the record for an activation of 
procedure r is retained when the activation ends. The record for the new 
activation q( I ,9) therefore cannot follow that for s physially, as it did in 
Fig. 7.13. Now if the retained activation record for r is deallwated, there 
will be free space in the heap ktween the activation records for s and 
q( 1,9 ) .  It is left to the heap manager to make use of this space. 
~ ! ~ T L O N  
IN THE 
ACTIVATION TREE 
ACTIVATION RECORDS 
IN THE HEAP 
Rctaincd 
adhat ion 
record for r 
'Fig. 7.17. Rmrds for tivc activations need not k adjaccnt in a hcap+ 

SEC. 7.4 
ACCESS TO NUNLOCAL NAMES 4 1 I 
The question of efficient heap management is a somewhat specialized issue 
in data-structure theory; some techniques are reviewed in Section 7.8, There 
is generally some time and space overhead associated with using a heap 
manager. For efficiency reasons, it may be helpfuj to handle small activation 
records or records of a predictable size as a special case, as follows: 
I . 
For each size of interest, keep a linked list of free blocks of that size. 
2. If possible, fill a request for size s with a block of size s', where s' is the 
smallest size greater than or equal to s. When the block is eventua1ly 
dealbcated, it is returned to the hked list it came from. 
3. 
For large blocks of storage use the heap manager, 
This approach results In fast alhcation and deallocation of small amounts of 
storage, since taking and returning a block from a linked list are efficient 
operations. For large amounts of storage we expect the computation to take 
some time to use up the storage, so the time taken by the allocator is often 
negligible compared with the time taken lo do the cornputat ion. 
7.4 ACCESS TO NONLOCAL NAMES 
The storage-allocation strategies of the last section are adapted in this section 
to prmit access to nonlocal names. Although the discussion is based on stack 
allocation of activation records, the same ideas apply to heap allocation. 
The scope rules of a language determine the treatment of references to non- 
local names. A common rule, called the [exit-ad- or static-scope ruk, deter- 
mines the declaration that applies to a name by examining the program text 
alone. Pascal, C, and Ada are among the many languages that use lexical 
scope, with an added "most closely nested" stipulation that Is discussed below. 
An alternative rule, called the dynamic-scope ruk, determines the decla~ation 
applicable to a name at run time, by considering the current activations. Lisp, 
APL, and Snrshl are among the languages that use dynamic scope. 
We begin with blocks and the "most closely nested" rule. Then, we con- 
sider nonlwal names in languages like C, where scopes are leaical, where all 
nonlocal names may be bound to statically all0Cated storage, and where no 
nesled procedure declarations are alkwed. 
In languages like Pascal that have nested procedures and lexical scope, 
names belonging to different procedures may be part of the environment at a 
given time. We discuss two ways of finding the activation records containing 
the storage bound to nonlocal names: access links and displays. 
A final subsecrlon discusses the implementation of dynamic scope. 
A block is a statement containing its own local data declarations. The concept 
of a block originated with Algol. In C, a block has the syntax 

4 12 
RUN-TIME ENVIRONMENTS 
SEC. 7.4 
A characteristic of blaks is their nesting structure. Delimiters mark rhe 
beginning and end of a b k k .  C u
~
s
 
the braces ( and ) as delimiters, while 
the Algol tradition is to use kegin and end. Delimiters ensure that one 
block is either independent of another, or is nested inside the other, That is, 
i t  is not possibk for two blocks B I and B to overlap in such a way that first 
B , begins, then B,, but 8 ends before B 
* +  
This nesting property is some- 
times referred to as bicbk srrwruw. 
The scope of a declaration in a Mock-structured language is given by the 
mrm c l r d y  nwsd rule; 
1. 
The scope of a declaration in a block B includes B 
2. 
If a name x is not declared in a block B, then an occurrence of x in B is 
in the scope of a declaration of x in an enclosing block B' such that 
i) 
8' has a declaration of x, and 
li) B' is more closely nested around B than any other block with a 
dectaratirm of x. 
By design, each declaration in Fig. 7.18 initializes the dedared name to the 
number of the block that it appars in. l
h
~
 
scope of the declaration of b in 
Bo does not include B ,, k c a u x  b i s  redeclared in B I ,  indicated by Bo - B I 
in the figure. Such a gap is called a hok in the scope of the declaration. 
The most cloaely nested scope rule k reflected in the output of the program 
in Fig, 7,I8, Control flows to a block from the point just bfore it, and flows 
from the block to the point just after it in the source text. The print stale- 
ments are therefore executed in the order B z ,  B 3 ,  B I ,  and Bo, the order in 
which control leaves the blocks. The values of a and b in these blocks are: 
Block structure can be implemented using stack allocation. Since the scope 
of a declaration does not extend outside the block in which il appears, the 
space for the declared name can be allocated when the block is entered, and 
deallocated when control leaves the bbck. This view treats a block as a 
"parameterless prmxdurc," called only from the point just before the Mock 
and returning only to the point just after the block. The nonlucal environment 
for a block can be maintained using the techniques for procedures later in this 
section. Note, however, that blocks are simpler than procedures because no 
parameters are passed and because the flow of control from and to a block 
closely fallows the static program lext .' 
' A jump rmt of ;I blr~k inrtr an unclr,sing M c k  can bc irnpkmcnrcd by popping activation tcciwds 
for thc intcrvrminy blwks. A jump into a block is pcrmittcd by wmc kmguagcs. k b r c  cnnlrol is 
transkrred in this way. activation rccords h a w  r i ~  
bc WI up for thc inltrvcning btclckn. Thc 
language ~cmantics Jclcrmincs how I w a l  data in thcx activatim rcmdci i s  b i t  kli7mJ. 

ACCESS TO NONLDCAL NAMES 4 13 
Fig. 7.18. 
Blocks in ii C program. 
i n t  a T O ;  
int a = 0; 
i n t b s ' ~ ;  
int b = 0; 
int b = 1; 
I 
int a = 2; 
ink b = 1; 
int b = 3; 
An alternative implementation is to allocate storage for a complete pro- 
cedure body at one time. 
If there are blocks within the procedure, then 
allowance is made for the storage needed for declarations within the blocks, 
For block BI, in Fig. 7.18, we can allocate storage as in Fig. 7.19. Subscripts 
on locais a and b identify the blocks that the locals are declared in. Note that 
a;! and by may be assigned the same storage because they are in blocks that 
are nbt alive at the same time. 
' B o - B 2  
Bu-Bl 
81-8, 
B 2 
5 
- 
Fig. 7.19. Storage for namcs declard in Fig. 7,18+ 
int a = 2; 
printf(msd w(d\nm. 
a, b): 
In :he absence of variable-length data, the maximum amount of aptorage 
needed during any execution of a block can be determined at compile time. 
(Variable-length data can be handled using pinters as m %tion 7.3.) In 
making this determination, we conservatively assume that all control paths in 

4 14 
RUN-TIME ENVIRONMENTS 
SEC. 7.4 
rhe program can indeed be taken. That is, we assume that b t h  the then- and 
else-parts of a conditional statement can be executed, and that all statements 
within a while Imp can be reached. 
Lexical Smp Without Nested Proceduw 
The lexical-swpe rules for C are simpler than those of Pascal, discussed next, 
because procedure definitions cannot be nested in C, That is. a procedure 
definition cannoi appear within another. As in Fig. 7.20, a C program con- 
sists of a sequence of deciaracims of variables and prcedures (C calls them 
functions), If there is a nonlocal reference to a name a in some function, 
then a must be declared outside any function. The scope of a declaration out- 
side a function consists of the function M i e s  that follow the declaration, with 
holes if the name is redeclared within a function. In Fig. 7.20, noniocal 
occurrences of a in readarray, partition, and main refer to the array 
declared on line I :  
( 1 )  int &Ill]; 
( 2 )  readarray( 1 I , .. a . . . 1 
(3) int partitionly,~) int y, z; I . . . a . . . 1 
(4) quic.ksortIm,n) int m, TI; { ,.. ) 
15) m i n t )  { ... a ... ) 
Fig. 7.20. C program with nonlml occurrcnccs of a. 
In the absence of nested procedures, the slack-allocation strategy for local 
names in Section 7.3 can k u
d
 
directly for a lexically Lscoped language like 
C. Storage for af! names declared outside any procedures can be allocated 
statically. The position of this dorage i s  known at compile time. so if a name 
is nonbcal in some procedure body, we simply use the statically determined 
address. Any other name must be a local of the activation at the top of the 
stack, accessible through the t p  pointer. Nested procedures cause this 
scheme to fail kcause a nonlocal may then refer to data deep in the stack, as 
discussed below. 
An important benefit of static allocation for nonlocals is that declared pro- 
cedures can freely be passed as parameters and returned as results (a function 
is passed in C by passing a pointer to it). With lexical scope and without 
nested procedures, any name nonlocal to one prwedure is nonlwal to ail pro- 
cedures. Its static address can be used by all procedures, regardless of how 
they are activated. Similarly, if procedures are returned as results, nonlocals 
in the returned procedure refer to the storage statically allocated for them. 
For example, consider the P a ~ a l  
program in Fig. 7,21. All occurrences of 
name rn, shown circled in Fig. 7.21, are in the scope of the declaration on line 
2, Since m is nonlocal to all procedures in the program, its storage can be 
allmated statically. Whenever procedures f and g are executed, they can use 

SEC. 7.4 
ACCES TO NONLOCAL NAMES 415 
(I) program gaao[input, outgutl; 
( 2 )  
var @: integer; 
(3) 
function f (n : integer 1 : integer ; 
(4) 
begin f :+@+ nand ( f 1; 
(5) 
function gIn : iatrqer 1 : integer; 
(6) 
begin g :=@* n end { g 1; 
(7) 
procedure b[function hIn : integer) : integer): 
(8) 
begin writelhI2)) end 
b 1; 
Fig. 7.21. Pawl program with nonlmal murrcnfcs of m. 
the static address to access the value of m. The fact that f and g are passed 
as parameters only affects when they are activated; it does not affect how they 
access the value of n, 
In more detail, the call b ( f ) on line I I associates the function f with the 
formal parameter h of the procedure b. So when the formal h is called on 
line 8, in write { h( 2 )  ) + the function f is activated. The activation of f 
returns 2 because nonhcal m has value O and formal n has value 2. Next in 
the execution, the call big) associates g with h; this time, a call of h 
activates g. The output of the program h 
Lexical Scope wYh Nested Procedures 
A nonlocal occurrence of a name a in a Pascal procedure is in the scqe of the 
mot cksely nested dedaration of a in the static program text. 
The nesting of procedure definitions in the Pascal progam .of Fig. 7.22 is 
indicated by the following indentation: 
sort 
r eadar ray 
exchange 
quicksort 
partition 
The wurrence of a on line 15 in Fig+ 7,22 is within function partition, 
which is nested within procedure quicksort. The most cimly nested 
dedaration of a is on line 2 in the procedure consisting of the entire program. 
The mosl cbsely neaed rule applies to procedure names as well. The 

416 
RUN-TIME ENVlRONMENTS 
(I) program sortIinput, output); 
SEC. 7.4 
var P : array [ 0 . . 1 0 ]  of integer; 
x : integer; 
procedure readarray; 
var i : integer; 
begin 
. + .  a . , ,  end { readarray 1; 
procedure exchange( i, 
j: inkegex); 
begin 
x : =  a [ i J ;  a[i] := a [ j ] ;  a [ j ]  :* x 
end ( exchange 1 ; 
procedure quicksort(m, n: integer); 
var k, v : integer; 
function partitionly, z :  integer): integer; 
var 
i ,  j : integer; 
begin ... a ... 
+ . .  
v . . *  
... exchangeIi,jl; , . .  
end { partition 1 ; 
begin ... end { quicksort !; 
begin ... end { sort 1 , 
Fig, 7,22. A Pawal program with ncstcd proccdurcs. 
procedure exchange, called by partition on line 17, is nonlocal to par- 
tition. Applying thc rule, we first check if exchange is defined within 
quicksort; since it is not, we look for it in the main prqgam sort. 
The notion of wsting depth of a procedure is used below to implement lexical 
scope. Let the name of the main program be at nesting depth I ; we add 1 to 
the nesting depth as we go from an enclosing to an enclosed procedure. In 
Fig. 7 2 2 .  procedure quicksort on line 1 I is at nesting depth 2, while par- 
tition on line 13 is at nesting depth 3. With each occurrence of a name, we 
associate the nesting depth of the procedure in which it is declared. The 
occurrences of a. v, and i on lines 15- 17 in partition therefore have nest- 
ing depths 1 , 2, and 3, rtspectiveiy. 
A direct implementation of lc~ical scope for nested procedures is obtained by 
adding a pointer calkd an a c ~ ~ ~ s . ~  
/ink ro each activation record. If procedure 

SEC. 7.4 
ACCESS TO NONLOCAL NAMES 417 
g is nested immediately within q in the source text, then the access link in an 
activation record for p points to the access link in the record for the most 
recent a d  ivat ion of q. 
Snapshots of the run-time stack during an execution of the program in Fig+ 
7.22 are xhown in Fig. 7+23. Again, ro save space in the figure, only the first 
letter of each procedure name is shown. The access link for the activation of 
sort is cmpty, because there is no enclosing procedure. The access link for 
each activatim of quicksort pints to the record for sort. Note in Fig. 
7.23(c) that the access link in the activation record for partition{ 7 , 3  ) 
pints to the access link in the reccsrd of the most recent activation of quick- 
sort, namely quicksort I I, 
3 ). 
Mg. 7.23. Amss links fo ~r finding sto r a g  for nonlocats. 
Suppse pmedure p at nesting depth n,, refers to a nonlwal a with nesting 
depth rr,, 5 n,,. The storage for a can he found as follows. 
1, 
When control is in p, an activation record for p is at the top of the stack. 
Follow n,,-n,, 
access links from the record at the top of the stack. The 
value of n,- 
n, can be precomputed at wmpik time. !f the access link in 
one record points to the access link in another, then a link can be fob 
lowed by performing a single indirection operation. 
2. After following ?,-rid, 
links, we reach an activation record for the pro- 
cedure that a is local to. As disulcsed in the last section. its starage is at 

4 18 
RUN-TIME ENVIRONMENTS 
SEC. 7.4 
a fixed offset relative to a position in the record, En particular, the offset 
can be relative to the access link, 
Hence, the address of bonhxal a in procedure p Is given by the foliowing pair 
cwmputd at cumpile time and stored in the symbol table: 
(n/, -n,, offset within activation record containing a) 
The first component gives the number of access links to be traversed. 
For example, on lines 15-16 in Fig. 7.22, the praxdure partition at 
nesting depth 3 references nonlocak a and v at nesting depths I and 2, 
respectively. The activation records containing the storage for these nonlocals 
are found by following 3- 1 = 2 and 3-2= 1 access links, respectively, from 
the r m r d  for partition. 
The code to set up access !inks is part of the calling sequence. Suppose pro- 
cedure p at nesting depth n, calls procedure x at nesting depth rt, + The code 
for setting up the access link in the called p r d u r e  depends on whether or 
not the called procedure is nested within the caller. 
I. Case n, C n, . Since the called procedure x is nested more deeply than p, 
it must be declared within p, or it would not be accessibIe to p. This 
case occurs when sort calls quicksort in Fig, 7.23(a) and when 
quicksort ca!ls partition in Fig. 7.23(c). In this case, the access 
iink in the called procedure must p i n t  to the access Ihk in the activation 
record of the caller just below in the stack. 
2. 
Case rzp 2 n,. From the m p e  rules, the enclosing procedures at nesting 
depths 1, 2, . . . , n,- 
1 of the called and calling procedures must be the 
same, as when quicksort calls itself in Fig. 7.23Ib) and when p a r t i -  
t i o n  calls exchange in Fig. 7,23(6). Following n, - n, + 1 access links 
from the calier we reach the most recent activation record of the pro- 
cedure that statically encloses both the called and calling procedures most 
closely. The access link reached is the one (a which the access link in the 
called p r d u r e  must point, Again, n,-nlI,+ I can be computed at mm- 
pile time. 
Lexical mpe rules apply even when a nested pr~cedure is passed as a parame- 
ter. The function f on lines 6-7 of the Pascal program in Fig. 7.24 has a non- 
focal m; all occurrences of m are shown circled. On line 8, procedure c assigns 
0 to m and then passes f as a parameter to b. Note that the scope of the 
declaration of m on line 5 does not include the body of b on lines 2-3. 
Within the My of b, the statement writeh [h [ 2 ) } activates f because 
the formal h refers to f. That is, writeln prints the result of the call f ( 2 ) .  
How do we set up the access link for the activation of f? The answer is 
that a nested procedure that is passed as a parameter must take its access link 
along with it, as shown in Fig. 7.25. 
When prwedure c passes f ,  it 

SEC. 7.4 
ACCESS TO HONLOCAL NAMES 4 19 
( I ) program param { input, output ) ; 
(4) 
procedurec; 
(3 
var @ : integer; 
(6) 
function f[n : integer) : integer; 
(7) 
begin f :=@+ n end { f 1; 
IS> 
bagin@:= 0; bIf1 end { c 1; 
Fig. 7.24. An access link must tic passed with actual pramclcr f . 
determines an access link for f, just as it would if it were calling f + This link 
is passed along with f to b. Subsequently, when f is activated from within b, 
the link is used to set up the access link in the activation record for f. 
Fig. 7.25, Actual proccdurc paramctcr f carrics its amss link abng. 

420 
RUN-TIME ENVIRONMENTS 
Faster access to nonlwals than with access links can be obtained using an 
army d of pointers lo activation records, calbd a display, We maintain the 
display so that storage for a nonlml a at nesting depth i is in the activation 
r w r d  pointed to by display element d li 1. 
Suppse control is in an activation of a procedure p at nesting depth j. 
Then, the first j - 1 elements of the display p i n t  to the most recent activa- . 
tims of the procedures that Lexically endose procedure p, and d 1 j j pints to 
the activation of p+ Using a display is generally fasta than foHwing access 
links because the activation record holding a n o n l ~ a l  is found by accessing an 
ekment of d and then following just one pointer. 
A simple arrangement for maintaining the display uses access links In addi- 
tion to the display. As part of the calZ and return sequences, the display i s  
updated by following the chain of access links. When the link to an activation 
record at nesting depth R is followed, display element 3(n] is =I to pint to 
that activation record. in effect, the display duplicates the information in the 
chain of access links. 
The above simple arrangement can be improved upon. The method illus- 
trated in Fig. 7.26 requires less work at procedure entry and e ~ i t  in the usual 
case when procedures are not passed as parameters. In Fig. 7.26 the display 
consists of a global array, separate from the stack. The snapshots in the fig- 
ure refer to an execution of the source text in Fig. 7.22, Again, only the first 
kuer of each procedure name is shown. 
Figure 7+26(a) shows the situation just kfore activation q ( 1 , 3  1 begins. 
Since quieksor t is at nesting depth 2, display element d[2] is affected when 
a new activation of quicksort begins. The effect of the activatim q( 1,3 1 
on 6121 is shown in Fig 7,26(b)+ where d [ 2 ]  now pints to the new activation 
record; the old value of dl21 is saved within the new activation record.' The 
saved value will be needed later to restore the display to its state in Fig. 
7.2qa) when control returns to the activation q{ 1,9 1. 
The display changes when a new activation occurs, and it must be reset 
when contrd returns from the new activation. The smpe rules of Pascal and 
other lexically scopd languages allow the display to be maintained by the fol- 
lowing steps. We dkcuss only the easier case in which procedures are not 
passed as parameters (see Exercise 7.8). When a new activation record for a 
procedure at nesting depth i is set up, we 
I , save the value of d [ i  1 in the new activation record and 
2. 
set dIiI topoint tothe new activationrecord. 
Just before an activation ends, dli 1 is reset to the saved value. 
that q( 1 , 9 )  a
h
 savcll d12), ~rl!biwgh iil hawns that thc sewnd display clcrncnl had ncv- 
cr hcforc bccn uscd and bws nut nccd to bc restored, It is easkr for all u l t s  of q ro store dl21 
thdn to dccidc at run iimc whcthcr such a store is ncccsirary. 

m, 7.26. Maintaining chc display whcn proccdurcs arc not p a w d  as paramctcrs. 
These steps are justified as follows. Suppose a procedure at nesting depth j 
calls a procedure at depth i. There are two cases, depending on whether or 
not the called pmXd~re 
i s  nested within the caller in the source text, as in the 
discussion of access links. 
1 .  
Case j < i .  Then i = j +  l and the called procedure is nested within  he 
caller, The first j elements of the display therefore do not need to k 
changed, and we set bIi 1 to the new activation recurd. This case is illus- 
trated in Fig. 7.2qa) when sort calls quicksort and also when 
quicksort calls partition io Fig. 7,Zqc). 

422 
RUN-TIME EWViRONMENTS 
SEC. 7+4 
2, 
Case j 2 i. 
Again, 
the enclosing procedures at 
nesting depths 
1, 2, . . . , i -  l of the called and calling procedure musr be the same. 
Here, we save the old value of dlij in the new activation record. and 
make d[i] point to the new activatwn record. The display is maintained 
correctly because the first i - 1 elernen ts are left as is. 
An example of Case 2, with i = j = 2, occurs when quicksort is cakd- 
recursively in Fig+ 7.2qb). A more inreresting example occurs when activa- 
tion p I 1 , 3  ) at nesting depth 3 calls e ( I ,3 ) at depth 2, and their enclosing 
procedure is s at depth I, as in Fig. 7.26(6). (The program is in Fig. 7.22.) 
Note that when e ( 1,3 1 is caUed, the value of dl31 belonging to p( 1,3 1 is 
still in the display, although i t  cannot be accessed while control is in e. 
Should e call another procedure at depth 3, that procedure will store 6131 and 
restore i t  on returning to e. We can thus show that each procedure sees the 
correct display for all depths up to its own depth. 
There are several places where a display can be maintained. If there are 
enough registers. then the display, pictured as an array, can be a collection of 
registers. Note that the compiler can determine the maximum length of this 
array; it is the maximum nesting depth of procedures in the program. Other- 
wise, the display can be kept in statically allocated memory and all references 
to activation records begin by using indirect addressing through the appropri- 
ate display pointer. This approach is reasonable on a machine with indirect 
addressing, although each indirection costs a memory cycle. Another possibil- 
ity i s  ED store the display on the run-time stack itdf, and to create a new copy 
at each procedure entry, 
Dynamic Scope 
Under dynamic scope, a new activation inherits the existing bindings of nonla- 
cal names to storage. A nonlocal name a in Ihe called activation refers to the 
same storage that it did in the calling activation. New bindings are set up for 
local names of the called procedure: [he names refer to storage in the new 
activation record. 
The program in Fig. 7.27 ihstrates dynamic scope. Praxdure show on 
lines 3-4 writes the value of nonlucal r. Under lexical scow in Pascal, the 
nonlocal r is in the scope of the declaralion on line 2, so the output of the 
program is 
However, under dynamic scope, the output is 
When show is called on lines !@I 
1 in the main program, 0 +25o is written 

SEC. 7.4 
ACCESS TO NONLOCAL NAMES 423 
(I) program dynamicIinput,output); 
(2) 
var r : real: 
(3) 
procedure show; 
(4) 
begin write( r : 5:3 
1 end; 
(51 
procedure small; 
(6) 
var r : real; 
(7) 
begin r :* 0 . 1 2 5 ;  show end; 
(8) 
&gin 
(9) 
r := 0 . 2 5 ;  
(10) 
show; small; writcln; 
Ill) 
show; small; writeln 
(12) 
end. 
Fig. 7.27. Thc output dcpcnds on whcthcr lcxical or dynamic mpe i s u d .  
because the variable r local to the main program is used. However, when 
show is calkd on line 7 from within small, 0.125 is written because the 
variable r Iwal to small is used. 
The following two approaches to implementing dynarntc scope k a r  some 
resemblance to the use of access links and displays, respectively, in the imple- 
mentation of lexical scope, 
1 .  
Deep rrc'cess. Conceptually. dynamic scope results if access links pint to 
the same activation records that control links do. A simple implemnta- 
tim is to dispense with access links and use the control \ink to search into 
the stack, looking for the first activation record con~aining storage for the 
nonlocal name. The term deep access comes rrom the fact that the search 
may go "deep" into the stack. The depth to which the search may go 
depends on the input to the program and cannot tx determined at compile 
time. 
2. 
Skailuw a m m .  Mere the idea is to hold the current value of each name 
in st~ica!ly allocated storage. When a new activation of a procedure p 
occurs, a local name n in p takes over the storage statically allocated for 
n+ The previous value of n can be sayed in the activation record for p 
and must be restored when the activation of p ends. 
The tradedf between the two approaches is that deep access takes longer to 
a m s s  a nonlocal, but there is no overhead asswiated with kginning and end- 
ing an activation. Shallow access on the other hand allows nonlmals to k 
looked up directly, but time i s  taken to maintain these values when activations 
begin and end. When functions are passed as parameters and returrted as 
results, a more straightforward implementation is obtained with deep access. 

424 
RUN-TIME ENVIRONMENTS 
SEC. 7.5 
7,s PARAMETER PASSING 
When one procedure calls another, the usual method of communication 
between them is through nontwal names and through parameters of the called 
procedure. h e h  nonlwals and parameters are used by the p r d u r e  in Fig. 
7.28 to exchange the values of a [ i ]  and a[ j]. Here the array a is nonlocal 
to the procedure exchange, and 5 and j are parameters. 
( 1 )  
procedure txchangeIi, j: integer): 
12) 
var x : integer; 
(3) 
begin 
(4) 
x := aCi]; a t i ]  := a[j]; a[]] 
:= x 
(3 
end 
Fi..7.28+ The Pascal proocdure swap with nontwals and parameters. 
Several common methods for associating actual and formal parameters are 
discussed in this section, They are: calbby-value, call-by-reference, copy- 
restore, call-by-name, and macro expansion. It is important to know the 
parameter passing method a language lor mmpiler) ues, because the resull of 
a program can depend on the method used. 
Why so many methods? The different methods arise from differing 
interpretations of what an expression represents. In an assignment like 
expression a [ j J represents a value, while a[ i ] represenis a storage location 
into which the value of a[ j 1 is placed. The decision of whether to use the 
location or the value represented by an expression is determined by whether 
the exprewion appears on the lef1 or right, respectively, of the assignment 
symbl. As in Chapter 2, the term 1-value refers to the storage represented by 
an expression, and r-value to the value contained in the storage, The prefixes 
I- and r- come from "left" and "right" side of an assignment. 
Differences k t  ween parameter-passing met hods are based primarily on 
whether an actual parameter represents an f-value, an /-value, or the text of 
the actual parameter itself. 
This is, in a sense, the simplest possible method of passing parameters. The 
actual parameters are evaluated and their r-values arc passed to the called pro- 
cedure. Call-by-value is used in C. and Pascal parameters are usually passed 
this way. All the programs so far in this chapter have relied on this method 
for passing parameters. Call-by-value can be implemented as fdlows. 
1 ,  
A formal parameter is treated just like st bcai name, so the storage for 
the formals is in the activation record of the catled procedure. 

SEC. 7.5 
PARAMETER PASSING 425 
2. The caller evaluates the actual parameters and places their r-values in the 
storage for the formah. 
A distinguishing feature of call-by-value i s  that operations on the formal 
parameters do not affect values in the activation r m r d  of the caller. [f the 
keyword var an line 3 in Fig. 7.29 is left out, Pascal will pass x and y by 
value to the p r d u r e  ewag. The call swap (a, 
b) on line 12 then leaves the 
values of a and b undiaurbed. Under call-by-value, the effect of the call 
swap( a ,b ] is equivalent to the .wquenm of steps 
x := a 
y := b 
temp := x 
X := y 
y := temp 
where x, y, and temp are Iwal to swap. Although thew assignments change 
the values of the locals x, y, and temp, the changes are lost when control 
returns from the call and the activation record for swap is deallwated. The 
call therefore has no effect on the activation record of the caller. 
(I) program rcftreaccIinput,output); 
(2) var a, b: integer; 
(3) procedure swapIvar x, y: integer); 
14) 
var temp : integer; 
15) 
begin 
(6) 
temp : =  x ;  
r 7) 
% 
:EL YI 
m 
y := temp 
19) 
end; 
Fig, 7.29. Pascal pmgram with prtlcedurc swap. 
A prwdure calkd by value can affect its caller either through nonhcal 
names (see exchange in Fig. 7.28) or through pointers that are explicitly 
passed as values. In the C program in Fig. 7.30, x and y are declared an line 
2 to be pointers to integers; the 6 operator in the call swap( La, db) on line 8 
results in pointers to a and b being p a s d  to swap. The ourput ot this pro- 
gram is 
a is now 2 ,  b is now 1 

426 
RUN-TIME ENVIRONMENTS 
SEC. 7.5 
Fig. 7,M. C program using poinrcrs in a procedure callcd by valuc. 
The use of pointers in this example suggests how a compiler using call-by- 
reference would exchange values. 
When parameters are passed by reJmnw (also known as rub/-byaddress or 
r-uII-by-locurion), the caller passes to the called procedure a pointer (o the 
storage address of each actual parameter- 
I .  
If an actual parameter is a name or an expression having an /-value. then 
that Cvalue itself is passed. 
2, 
However, if the actual parameter is an expression, like a + b  or 2, that 
has no I-value, then the expression .is evaluated in a new location, and the 
address of that location i s  passed. 
A reference to a formal parameter in the called procedure becomes, in the tar- 
get code, an indirect reference through the pointer passed to the called pro- 
cedure. 
b 
Example 7.7. 
Consider the procedure swap in Fig. 7.29. A call to swap 
with actual parameters i and a[ i], that is, swap[ i , a[ i] 1 ,  would have the 
same effect as the following sequence of steps, 
I 
I .  
Copy the address (I-values) of 1 and a[il into the activation record of 
the called procedure, say into locations argl and arg2 corresponding to 
x and y. respect iuely. 
2, 
Set temp to the contents of the location pointed to by 9 x 9 1  (i.e., set 
temp equal to l o ,  where I,) is the initial value of i). This step 
corresponds to temp : = x on line 6 in the definition of swag. 
3. 
Set the contents of the location painted to by argl to the value of the 
locat ion pointed to by arg2; that is, i : = a [ 
I .  This step c o r r e ~ p d s  
to x : y on line 7 in swap. 

SEC. 7 s  
PARAMETER PASSING 427 
4. 
Set the contents OI the location pointed by arg2 equal to the value of 
temp; that is, set a[Io3 := i. This step corresponds to y:=temp+ 
3 
Call-by-reference is used by a number of languages; var parameters in Pas- 
cal are passed in this way. Arrays are usually passed by reference. 
A hybrid between call-by-value and call-by-reference is copy-rtww linku~u. 
( a h  known as copy-in copy-oat. or va1u~-rewltj. 
I .  
]Before control flows to the called procedure, the actual parameters are 
evaluated. The r-values of the actuals are passed to the called procedure 
as in call-by-value. In addition, however, [he /-values of those actual 
parameters having I-values are determined before the call. 
2. 
When control returns, the current r-values of the formal parameters are 
copied back into the l-values of the actuals, using the I-values computed 
before the cail. Only actuals having /-values are copied, of course. 
The first step ''copies in" the values of the actuals into the activation rccord of 
the called procedure (into the storage for the formals). The second srep 
"copies out" the final values of the formals into the activation record of the 
caller (into €-values computed from the actuals before the call). 
Note that swag ( i , a [ i 
J ) works correctly using copy-restore, stnce the 
loation of a[ i 1 i s  computed and preserved by the calling program Wore the 
call. Thus, the final value of formal parameter y, which will be the initial 
value of i, 
is copied into the correct location, even though the location of 
a [ i ] is changed by the call (because the value of i changes), 
Copy -restore is used by some Fortran implementations. However, others 
use call-by-reference. Differences between the two can show up if the called 
procedure has more than one way of accessing a location in the activation 
recard of the caller. The activation set up by the call unsafe l a 1 an line 6 of 
Fig. 7.31 can access a as a nonlwal and through formal x. Under call-by- 
reference, the assignments to both x and a immediately affea a, so the final 
value of a is 0, Under copy-restore, however, the value 1 of actual a is 
copied into formal x. The final value 2 of x is copied out into the 1-value of 
a just before control returns, so the find value of a is 2. 
(1) program eopyoutlinput,~utput~; 
(2) 
var a : integer; 
3 
procedure unsaf eI var x : integer) ; 
(41 
begin x := 2; a : =  O end; 
(51 
begin 
(6) 
a :r 7 ;  unsafeIa1; writelnIa1 
(71 
end. 
Fig. 7.32, Thc output changes if call-by-rcfercncc is changcd to copy-rc~turc 

428 
RUN-TIME ENVIRONMENT'S 
SEC. 7.5 
Call-by-Name 
Call-by-name is traditionally defined by the c.ryqw.de of Algol, which is: 
The procedure i s  treated as if it were a macro; that is, its M
y
 is substi- 
tuted for the call in the caller, with the actual parameters literally substi- 
tu ted for the formals. Such a literal subsritution i s  called mnrro-r>xpnsion 
or in-ihu eqwnsion + 
The Iwi names of the called proocdure are kept distinct from the names 
of the calling procedure. We can think of each ImA of the c a k d  pro- 
cedure k i n g  systematically renamed into a distinct new name before the 
macro-e~pansion ifi bone. 
The actual parameters are surrounded by parentheses if necessary to 
preserve their integrily. 
Example 7.8. 
The call swap( i , a[ i] 
1 from Example 7.7 would be irnple- 
mented as though it were 
temp := i 
i := ali} 
a [ i ]  := temp 
Thus, under call-by-name, swap sets i to a[ i I ,  as expected, but has the 
unexpected result of setting a[aIlo]] - 
rat he^ than a[irr] - 
to I,,, where 
It, is the initial value of i. This phenomenon occurs because the Imtian of x 
in the assignment x :=temp of swap is not evaluated until needed, by which 
time the vaBe of i has already changed. A correctly working version of 
swap apparently cannot be written if call-by-name is used (see Fleck 119761).0 
A It bough call-by-name is primarily of theoretical interest , the conceptually 
related technique of in-line expansion has been suggested for reducing the 
running time of a program. There is a certain cost associated with setling up 
an activation of a procedure - 
space is aliocated for the activation record. the 
machine status is saved, links are set up, and then control i s  transferred. 
When a procedure body is small, the code devoted to the calling sequences 
may outweigh the code in the procedure M y .  It may therefore be more effi- 
cient to use in-line expansion of the body into the code for the caller, even if 
the size of the program grows a little. In the next example, in-line expansion 
is applied €0 a procedure called by value, 
Example 7.9. Suppose that the function f in the assignment 
i s  called by value. Here the adual parameters A and 8 are expresskns. Sub- 
stituting expressions A and 8 For each occurrence of the formal parameter in 
the body of f leads to call-by-name; recall a[ i J in the last example. 

SEC. 7.6 
SYMBOLTABLES 429 
Fresh temporary variables can be used to force the evaluation of the actual 
parameters before execution of the procedure W y :  
Now in-tine expansion will replace all occurrences of the formal by t1 and tl 
when the first and second calls, respectively, are expanded .' 
u 
The usual implementation of call-by-name is to pass to the called procedure 
parameterless su brautines, commonly called thunks, that can evaluate the I- 
value or r-value of the actual parameter. Like any procedure passed as a 
parameter in a language using lexical scope, a thunk carries an access link 
with it, pointing to the current activation record for the calling prucedurc. 
7,6 SYMBOL TABLES 
A compiler uses a symbol table to keep track of scope and binding information 
about names. The symbol table is searched every time a name is encountered 
in the source text. Changes to the table wcur if a new name or new Informa- 
tion a h u t  an existing name i s  biacouered. 
A symbul-table mechanism must afhw us to add new entries and find existst- 
ing entries efficiently . The two symbol-table mechanims presented in this 
section are linear lists and hash tables. We evaluate each =heme on the basis 
of the time required to add st entries and make t~ inquiries. A linear list is the 
simplest to implement. but its performan& is pcmr when u and r~ get large. 
Hashing schemes provide better performance for sumewhat gealer program- 
ming effort and space overhead. Both mechanisms can be adapted readily to 
handle the most closely nested scope ruk. 
It is useful for a compiler to k able ro grow the symbol table dynamically, 
if necessary, at compile time. If the size of the symbol tabk is fixed when the 
compiler is written. then [he size must be chosen large enough to handle any 
source program that might kw presented. Such a fixed size is likely to be too 
large for most, and inadequate for some, programs. 
Symbol-Table Entries 
Each entry in the symbol table is for the declaration of a name, The format 
of entries does not have to be uniform, because the information saved about a 
name depends on the usage of the name. Each entry can be implemented as a 

430 
RUN-TIME ENVIRONMENTS 
SEC, 7.6 
record consisting of a Mquence of consecutive words of memory. To keep 
symbol-table records uniform, it may be conwnient for some of the informa- 
tion about a name to be kept outside the table entry, with only a pointer to 
this information stored in the record. 
Information is entered into the symbol table at various times. Keywords 
arc entered into the table initially, if at all. The lexical analyzer in Section 3.4 
looks up sequences of letters and digits in the symbol table to determine if a 
reserved keyword or a name has been coilected. With this approach, key- 
words must be in the symbol table before lexical analysis begins. Alterna- 
tively, if the lexical analyzer intercepts reserved keywords, then they need not 
appear in the symbol table. tf the language bwa not reserve keywords, then it 
is essential that keywords be entered into the symbol table with a warning of 
their pssibk use as a keyword. 
The symbol-table entry itself can be set up when the role of a name 
becomes clear. with the attribute values being filled in as the information 
kcomes available. In some cases, the entry can be initiated from the lexical 
analyzer as soon as a name is seen in the input. More often, one name may 
denote several different objects, perhaps even in the same bluck or procedure. 
For example, the C declarations 
i n t  
X; 
struct x I float y ,  2; 1; 
use x as b t h  an integer and as the tag of a structure with two fields, in such 
cases, the lexical analyzer can only return to the parser the name itself (or a 
pointer to the lexerne forming that name), rather than a pointer to the 
symbol-table entry. The record in the symbol table is created when the syn- 
tactic role played by this name is dis+vered+ For the declarations in (7. I), 
two symbol-table entries for x would be created; one with x as an integer and 
one as a structure. 
Attributes of a name are entered in response to declarations, which may be 
implicit. Labels are often identifiers followed by a d o n ,  so one action aswi- 
ated with recognizing such an identifier may lx to enter this fact into the sym- 
bol table. Similarly, the syntax of procedure declarations specifies that certain 
identifiers are formal parameters. 
Cbracttrs In a Name 
As in Chapter 3, there is a distinction ktween the token id for an identifier 
or name, the lexeme consisting of the character string forming the name, and 
the attributes of the name. Strings of characters may ke unwietdy to work 
with, so compilers often use wme fixed-length representation of the name 
rather than the h e m e .  The lexcme is needed when a symbol-table entry is 
set up for the first time, and when we Imk up a h e m e  found in the input to 
determine whether it is a name that has already appeared. A common 
representat ion of a name is a pointer to a symbol-table entry for it. 

SEC. 7.6 
SYMBOL TABLES 43 1 
If there is a modest upper bound on the length of a name, then the charac- 
ters in the name can be stored in the symbol-table entry, as in Fig. 7.32(a). if 
there is no limit OII the length of a name, or if the limit is rarely reached, the 
indirect scberne of Fig. 7.32(b) can be used. Rather than allocating in each 
symbol-table entry the maximum possible amount of space to hdd a lexeme, 
we can utilize space more efficiently if there is only space for a pointer in a 
symbol-table entry. In the record for a name, we place a pointer to a separate 
array of characters {the string ruble) giving the position of the first character 
of the lexeme. The indirect scheme of Fig. 7.32(b) permits the size of the 
name field of the symbol-table entry itself to remain a constant. 
The complete lexeme constituting a name must be stored to ensure that ail 
uses of the same name can be associated with the same symbol-table record. 
We must, however, distinguish among occurrences of the same h e m e  that 
are in the scopes of different declarations, 
(a) In fixed-size space 
within a record 
(b) In a scparatr: array 
Fig. 7.32. Storing thc chatactcrs of a name. 

432 
RUN-TIME ENVIRONMENTS 
Stwage Allocation Information 
Information about the storage locations that will be b u n d  to names at run 
rime is kept in the symbol table. Consider names with static storage first. If 
the target d
e
 
is assembly language, we can let the assembler take care of 
storage locations for the various names, All we have to do is to scan the sym- 
bol table, after generating assembly code for the program, and generate 
assembly language data definitions to be appended to the assembly language 
program for each name. 
If machine c& 
is to be generated by the compiler, however, then the psi- 
t i m  of each data object relative to a fixed origin, such as the beginning of an 
aclivation record must be ascertained, The same remark applies rc, a blcck gf 
data loaded as a module separate from the program. For example, COMMON 
blocks in Fortran are loaded separately, and the positions of names relative to 
the beginning of the COMMON block in which they lie must be determined, For 
reasons discussed in Section 7.9, the approach of Section 7.3 has to be modi- 
fied for Fortran. in that we must assign offsets for names after all declarations 
for a procedure have been seen and EQUIVALENCE statements have been pro- 
cessed. 
In the case of narncs whose storage is allucatcd on a stack or heap. the corn- 
piler does not allocate atorage at all - the compiler plaos out the activation 
record for cauh prweburc. as in Section 7.3. 
The List Data Structure for Symbol Tabks 
The simplest and easiest to itnplemcnt data structure for a symbol table iis a 
linear list nf records. shown in Fig. 7.33, 
We use a single array, or 
equivalently xveral arrays. ro stare names and their associated informatiun. 
New narncs arc added to the list in the d e r  in which they are encourrtered. 
The position r ~ f  
the end of the array ih marked by the pointer avuiktbk, point- 
ing to where the next zymbol-table entry will go. The search for rl namc 
prrweeds backwards From the end 01: the array to the beginning. When the 
name is located, the associated information can be tbund In the words fdbw- 
iog next. If we reach the beginning of the array without finding the name, a 
fault CICL'U~S - 
an expected namc is not in the table. 
Note that making an entry Tor a name snd looking up the name in the sym- 
hot table are independent operations - we may wish to do one without the 
other. In a block-structured language. an occurrence of a name is In the scope 
of the must closely nested declaration of the name. We can implement this 
scope rule using the list data ~tructure by making a fresh entry for a name 
cvcry lime it is declared. A new entry is made in the words irnmcdiately fob 
lowing the pointer uvaihhti~; that pointer is increased by the sirc of the 
symbl-table record. Since entries are inserted in order, stmirig from the 
beginning of the array, they appear in the order they are created In7 By 
searching rrom clvrrildde towards the beginning of the array, we are sure to 
find the most reccntly crciltcrl cntry. 

SYMBOL TABLES 433 
Fig. 7.33. A linear list of rcmrds. 
If the symbol table contains n names. the work necessary to insert a new 
name is constant if we do :,he insertion without checking to see: if the name is 
already in the table. if multiple entries for names are not allowed. then we 
need to look through the entire table before discovering that a name is not in 
the table, doing work proportional to n in the process. To 6nd the data a b u t  
a name, on the average, we search n12 names, so the cost of an inquiry is also 
proportional to n. Thus, since insertions and inquiries take tirne proportional 
to n, the total work for inserting n names and making u inquiries is at must 
cn (n +el, where L' is a constant representing the time necessary far a few 
machine operations. In a medium-sized program, we might have n - 100 and 
e = 100q, so w e r d  hundred thousand machine operations are utilized in the 
boukkeeping, That may not be painful, since we are talking aboui less than a 
second of tirne. However, if n and r are multiplied by 10, the cost is multi- 
plied by 100, and the bookkeeping time becomes prohibitive. Profiling yields 
valuable data about where a mmpi!er spends its time and can te used to 
decide if too much time is k i n g  spent searching through linear lists. 
Hash Tables 
Variations of the ,searching technique known as hashing have been imple- 
mented in many compilers. Here we consider a rather simpk variant known 
as open hashing, where "upen" refers to the property that there need be nu 
limit on the number of entries that wn be made in the table. Even this 
scheme gives us the capability of performing 
inquiries MI n names in time 
proportional to ~ ( n  
+ e)/m, for any constant m of our choosing. Since rn can 
be made as large as we like. up to n, this method is generally more efficient 
than linear lists and i s  the method of chow for symbol tables in most 

434 
RUN-TIME ENWRONMENTS 
EC. 7+6 
situations. As might be expected, the space taken by the data structure grows 
wirh m, so a time-spaa tradeoff is involved. 
The basic hashing scheme is illustrated in Fig+ 7.34. There arc two parts to 
data structure: 
A h s h  i d l e  consisting of a fixed array of m pointers to table entries. 
Table entries organized into m separate linked lists, called buckers {some 
buckets may be empty). Each record in the symbol tabk appears on 
exactly one of these lists. Storage for the records may k drawn from an 
array of records, as discussed in the next section. Alternatively, the 
dy narnic stwage allocation facilities of the implementation language can 
be used to obtain space for the records, often at some loss of efficiency. 
Array of list hcadcrs, 
indcxcd by hash value 
List elcments crcatcd 
for names shown 
Fig, 7.34. A hash table of size 21 1. 
20 
To determine whether there is an entry for string s in the symbl tabb, we 
apply a hush function h to s, such that h (s) returns an integer between 0 and 
m - I. If s is in the symbol table, then i t  is on the list numbered Ir(s). If s is 
not yet in the symbol table, il is entered by creating a record for s that is 
linked at the front of the list numbered h ($1. 
As a rule of thumb, the average list is n / m  records long if there are n 
names in a table of size m. By choosing m so that d m  i s  bunded by a small 
constant, say 2, the time to a m s s  a table entry is cssent ially constant, 
The space taken by the symbol table mnsists of ns words for the hash table 
* .  
match 
- 
- 
- 
1 
. . .  
l a s t  
action 
ws 
I. 1.H I + I  
I I I 
2 10 
. . .  - 
- 

SEC, 7.6 
SYMBOL TABLES 435 
and cn words for table entries, where r is the numkr of words per table 
entry. Thus the space for the hash table depends only on m, and the space for 
table entries depends only on the number of entries, 
The choice of m depends on the intended application for a symbol table. 
Choosing m to be a few hundred should make table Imkup a negligible frac- 
tion of the total time spent by a compiler, even for moderate-sized programs, 
' 
When the input to a compiler might be generated by another program. how- 
ever, the number of names a n  -greatly exceed that of most human-generated 
programs of the same size, and larger table sizes might be preferable. 
A great deal of attention has been given ro the question of how to design a 
hash function that is easy to compute for strings of characters and distributes 
strings uniformly among the rn lists. 
One suitable approach for computing hash functions is to proceed as fol- 
lows: 
I. 
Determine a positive integer h from the characters c ,, c . ~ ,  . . . , 
in 
string s+ The conversion of single characters to integers is usually sup- 
ported by the implementation language. Pascal provides a function orb 
for this purpose; C automaticaliy converts a character to an integer if an 
arithmetic operation i s  performed on it. 
2. 
Convert the integer h determined above into the number of a list, i.e.. an 
integer between O and rn - 1 .  
Simply dividing by m and taking the 
remainder is a reasonable policy, Taking the remainder seems to work 
better if rn is a prime, hence the choice 21 1 rather than 200 in Fig. 7.34. 
Hash functions that look at all characters in a string are less easily fooled 
than, say, functions that h
k
 only at a few characters at the ends or in the 
middle of a string. Remember, the input to a compiier may have been created 
by a program and may therefore have a stylized form chosen to avoid conflicts 
with names a person or some other program might use+ People tend to "clus- 
ter" names as well, with Choices like baz, ncwbaz, baz 1, and so on. 
A simple technique for computing h is ro add up the integer values of the 
characters in a string. A better idea is to multiply the old value of h by a con- 
stant a before adding in ihe next character. 
That is, take ko = 0, 
hi = ahi-, t q, for I l i ~ k ,  
and kt h = Ifd, where k is the lenglh of the 
string, (Recall, the hash value giving the number of the list is h mod m.) 
Simply adding up the characters i s  the caw a = I. A similar strategy is to 
exdusive-or r, with ah, - , , instead of add<ng. 
For 32-bit integers, if we take u = 65599, a prime near 2Ib, then overflow 
soon occurs during the computation of ahl-,. Since CY is a prime, ignoring 
overflows and keeping only the lower-order 32 bits seems to do well, 
In me set of experiments, the hash function hmhpjw In Fig. 7.35 from P. J. 
Weinkrger's C mmpiler did consistently well at all table sizes rested (see Fig. 
7,361, The sizes included the first primes larger than 100, 200, . . . . 1500. 
A close second was the function that computed h by multiplying the old value 
by 65599, ignoring overflows, and adding in the next character. Function 

436 
RUN-TIME ENVIRONMENTS 
11) #define PRIME 211 
(2) rdef ine EOS "4' 
(3) int hashgjwl s 1 
(4) char 
*s; 
Fig. 7.35. 
Hash Funclion krrshp,jw, written in C. 
hwrskpjw is computed by starring with h = O. For each character r., shift the 
bits of h left 4 p s i t i o n s  and add in c. If any of the four high-order bits of h 
i s  I ,  shift the four bits right 24 positions, exclusive-or them into h, and reset 
to 0 any of the four high-order bits that was 1. 
Example 7.110. 
For best results. the size of the hash table and the expected 
input must be taken into account when a hash function is designed. For 
example. it is desirable that  he hash values for the most frequently occurring 
names in a language he distinct. If kcywords are entered into the symbol 
table. then the keywords are likely to he among the most frequently occurring 
names, although in one sample of C programs. name i occurred over three 
times as often ati while. 
One way of testing a hash function is to look at the number of strings that 
fall onto the same list, Given a file F consisling of n strings, suppose b, 
strings fall onto list j. for W j S m  -1. 
A measure of how uniformly the 
strings are distributed across lists ir, rjbtaincd by computing 
The intuitive justificahn for this term is that we need to look at I list element 
to Find the firs! entry on list j. at 2 tu find the second. and so un up to B, to 
find the lasr entry. The sum of 1. 2 ,  . . . . b, is B,(b, + 1 )  ! 2. 
From Exercise 7+14, the value nf (7.2) for a hash function that distribum 
strings random1 y acruss huckecs is 
d ~1 : 
3 r t  J( !I ,!- 
h
t
 - I 1 
(7.3) 

S C .  7.6 
SYMBOL TABLES 437 
The ratio of the lerrns (7.2) and (7.3) is platled in Fig. 7.36 for severa! 
hash functions applied to nine files. The files are: 
The 50 most frequently occurring names and keywords in a sarnpie of C 
programs. 
Like 
l), but with the 100 most frequently occurring names and key- 
words. 
Like ( I ) ,  but with the 500 most Frequently occurring names and key- 
words, 
952 external names in the UNIX operating system kernel. 
627 namts in a C program generated by C+ + (Strwustrup 11 9861). 
9 15 randomly generated character strings. 
6 14 words from Section 3+1 of this book. 
1201 words in English with xxx added as a prefix and suffix. 
The 300 names v N 0 ,  ~ 1 0 1 , .  . . , ~ 3 9 9 .  
FI. 7.36. Rclatiw pcrfmmancc of hash functions b r  a tablc of sizc 21 1. 
The Function hxhpjw is as in Fig. 7.35. The functions named X u ,  where cr is 
an inieger constant, compute h mid m, where h is obtained iteratively by 
starting with 0, multiplying the old value by a, and adding in tht next 

438 
RUN-TIME ENVIRONMENTS 
SEC. 7.6 
character. The function mid& 
f ~ r m s  h from the middle four characters of a 
string, while ends adds up the first three and last three characters with the 
length to form h. Finally, p u b  groups every four consecutive characters into 
an integer and then adds up the integers+ 
Representing Sope Information 
The entries in the symbol table are for declarations of names. 
When an 
occurrence of a name in the source text is looked up in the symbol table, the 
entry for the appropriate declaration of that name must be returned. The 
scope rules of the source language determine which declaration is appropriate. 
A simple approach is to maintain a separate symbol table for each scope. In 
effect, the symbol table for a procedure or scope is the campiletime 
equivalent of an activation record. Information for the nonlocals of a pro- 
cedure is found by scanning the symhl tables for the enclosing p r d u r e s  
following the scope rules of the language. Equivalently, information about 
the locals of a procedure can be atlached to the node for the procedure in a 
syntax tree for the program. 
With this approach the symbol table is 
integrated into the intermediate representalion of the input, 
Most closely nested scope rules can be implemented by adapting the data 
structures presented earlier in this wction. We keep track of the local names 
of a procedure by giving each procedure a unique number. Blocks must also 
be numbered if the language is block-structured. The numkr of each pro- 
cedure can be computed in a syntax-directed manner from semantic rules that 
recognize the beginning and end of each procedure, The procedure number is 
made a part of all hcals declared in that procedure; the representation of the 
local name in the symbol tablc is a pair consisting of the name and the pro- 
cedure number. (In some arrangements, such as those described below, the 
procedure number need not actually appear, as it can be deduced from the 
position of the record In the symbol table,) 
When we look up a newly scanned name. a match occurs only if the charac- 
ters of the name match an entry character for character, and the associated 
number in the symbol-table entry is the nurnkr of the procedure being pro- 
cessed. Most closeiy nesccd scope rules can be implemented in terms of the 
following operations on a name: 
k
:
 
find the most recently created entry 
i n :  
make a new entry 
d~lere: 
remove the most recently created entry 
"Deleted" entries must be preserved; they are just removed from the acrive 
symbol table. In a one-pass compiler, information in the symbol tablc a b u t  a 
scope consisting of, say, B procedure M y ,  is nor needed at cumpile time after 
the procedure body is processed. However, i t  may be needed at run time, 
particularly if a run-time diagnostic system i s  implemented. In this case, the 
information in the symbol tablc must be added to the generated code for use 

SEC. 7.6 
SYMWL TABLES 439 
by the linker or by the run-time diagnostic system. See also the treatment of 
field names in records in Sections 8+2 and 8.3. 
Each of the data structures discussed in chis section - lists and hash tables 
- 
can be maintained sa as to support the above operations. 
When a linear list mnsisting of an array of records was described earlier in 
this section, we mentioned how lookup can be imp1emente-d by inserting 
entries at one end so that the order of the entries in the array is the same as 
the order of insertion of the entries. 
A scan starting from the end and 
proceeding towards the beginning of the array finds the most ~ecentlg created 
entry for a name, The siluation is similar in a linked list, as shown in Fig. 
7.37. A pointer front points to the most recently created entry in the list. 
The implementation of inseri takes constant time because a new entry is 
placed at the front of the list. The implementation of ioukup is done by scan- 
ning the list starting at the entry pointed to by h n t  and following links until 
the desired name is found, or the end of the list is reached. In Fig. 7.37, the 
entry for a declared in a block B 2 ,  nested within block A,, appears nearer the 
front of the list than the entry for a declared in Bn. 
Fig. 7-37. The most rcccnt entry for a is near rhc front. 
For the delere operation, note that the entries for the declarations in the 
most deeply nested procedure appear nearest the front of the list, Thus, we 
do not need to keep the procedure number with every entry - if we keep 
track of the first entry for each procedure, then all entries up to the first can 
be deleted from the adive symbol table when we finish processing the scope 
of this procedure. 
A hash table consists of m lists accessed through an array, Since a name 
always hashes to the same list, individual lists are maintained as in Fig. 7.37. 
However, fur implementing the delere operation we would rather not have to 
scan the entire hash table imking for lists containing entries to be dekted. 
The fdbwing approach can be used. Suppose each entry has two links: 
I. a hash link that chains the entry to other entries whose names hash to the 
same value and 
2. 
a scope link that chains all entries in the same scope. 
If the scope link is left undisturbed when an entry is deleted from the hash 
table, then the chain formed by the scope links will constitute a separate (inac- 
tive) symbl table for the scope in question. 

440 
RUN-TIME ENVIRONMENTS 
SEC. 7-6 
Deletion of entries from the hash table must be done with care, because 
deletion of an entry affects the previous one on its list. Recall that we delete 
the ith entry by making the i - 1st entry point to the i + 1st. Simply using the 
scope links to find the ith entry is therefore not enough. The i - I st entry can 
be found if the hash links form a circular linked list, in which the lasr enrry 
p i n t s  back to the first. A kernat ively , we can use a stack to keep track of the 
lists containing entries to be deleted. A marker is placed in the stack when a 
new procedure is scanned. Above the marker are the numbers of the lists 
containing entries for names declared in this procedure. When we finish pro-. 
cessing the procedure, the list numkrs can be popped from the stack until the 
marker for the procedure is reached. Another scheme 1s discussed in Exercise 
7+11. 
7.7 LANGUAGE FACILITCES FOR DYNAMIC STORAGE ALLOCATION 
In this section, we briefly describe facilities provided by some languages for 
the dynamic allocation of storage for data, under program control. Storage 
for such data is usuajly taken from a heap. Ahcated data is often retained 
until it i s  explicit1 y deallocsted. The allocation itself p n  be either explirit or 
i
f
.
 In Pascal, for example, explicit allocation i s  performed using the 
standard procedure new. Ekecution of new(p 1 allocates storage for the type 
of object pointed to by p and p is left pointing to the newly allocated object. 
Deallocation is done by calling dispose in most implementations of Pascal. 
lmplicit allocation occurs when evaluation of an e~pression results in storage 
being obtained to hold the value of the expression. Lisp, for example, allo- 
cates a 041 In a list when cons is used; cells [hat can nb longer lx reached are 
automatically reclaimed. Snob01 allows the length of a string to vary at run 
time, and manages the space needed to hold the string in a heap. 
E ~ m p l e  
7.11. The Pascal program in Fig. 7+38 builds rhe linked list shown 
in Fig. 7.39 and prints the integers held in the cells; its output is 
When c~ecutim of the program begins at line 15, storage for the pointer 
head is in the activation record for thc complete program. Each time control 
reaches 
the call new(pt results in a cell being allocated somewhere within the heap; 
pt refers to this cell in the assignments on line I 1 .  
Note from the output of the program that the allocated cells are accessible 
when control returns to the main program from insert. In other words, 
cells allocated using new during an activation of insert are retained when 
control returns to rhe main program from the activation. 

SEC. 7.7 
L4NGUAGE FACILITIES FOR DYNAMIC STORAGE ALLOCATLON 441 
( 1) proqtam table { input, output 1 ; 
( 2 )  type link = t cell; 
(31 
cell = record 
(4) 
key, i n f o  : integer; 
( 5 )  
next ; link 
( 6) 
end ; 
(7) var head : link; 
(8) procedure ins+rt(k, i : integer): 
19) 
var 
p : lid; 
10) 
begin 
( 1 1 )  
new(p); pt.key : =  k; pt.info := i; 
r 12) 
pt.next := head; head := p 
13) 
end ; 
(14) begin 
(15) 
head := n i l ;  
16) 
insart(7,i); i n s e r t I 4 , 2 ) ;  ansert(76,3); 
171 
writelnIheadt.key, headt.inf01; 
(18) 
writeln(headt.ntxtt.kty, headt.nextT.info1; 
( 19) 
writelnlheadt.ntxt t ,next t .key, 
head?. next t . next 7 .  info) 
(20) end- 
Fig. 7.38. Dynamic alloci+tion of cclls using new in Pascal. 
Fig+ 7,39. Linkcd list kilt by program in Fig. 7.38. 
Garbage 
Dynamically allocated storage can become unreachable. Storage that a pro- 
gram allocates but cannor refer to is called gurbugc. In Fig. 7+38, suppose 
n i l  is assigned to head 7. next between lines 16 and 17: 
16) 
insertt7,l); insert14,2); insertI76,3); 
headt.next := nil; 
I 17) 
writelnIheadt.key, headt.inf01; 
The l e h o s t  cell in Fig, 7.39 will now contain a n i l  pointer rather than a 
pointer to the middle cell. When the pointer to the middle cell is lout, the 
middle and rightmost cells k o m e  garbage. 

442 
RUN-TIME ENVIRONMENTS 
SEC. 7.7 
Lisp performs gurhge mlieciivn, a proass discussed in the next section that 
reclaims inaccessible storage. Pascal and C do not have garbage collection, 
leaving it to the program explicitly to dealhate storage that Is no longer 
desired. Tn these languages, deallocated storage can be reused, but garbage 
remains until the program finishes. 
DstngMng References 
An additional complication can arise with explicit dcallwation; dangling refer- 
ences can occur. As mentioned in Section 7.3, a dangling reference occurs 
when storage that has k e n  deallocated is referred to, For example, consider 
the effect of executing dispose [head t. next ) between lines 16 and I 7  in 
Fig. 7.38: 
The call to dispose deallocates the cell fdlowing the one pointed to by head 
as shown in Fig. 7.40. However, head t.next has not been changed, m it is 
a dangling pointer to d d l ~ a e d  
storage. 
Dangling references and garbage are reiated concepts; dangling references 
occur if deallocation occurs before the lasl reference, whereas garbage exists if 
the last reference occurs before dealloat ion. 
head 
7.8 DYNAMIC STORAGE ALLOCATION TECHNIQUES 
The techniques needed to implement dynamic storage allocation depend on 
how storage is deallwated. If deak-cation is implicit, then the run-time sup 
port package is responsible for determining when a storage block i s  no longer 
needed. There i s  less a compiler has lo do if deallocation is done explicitly by 
the programmer. We consider explicil deallocation first. 

SEC. 7+8 
DYNAMIC STORAGE ALLOCATLON TECHNIQUES 443 
Explicit Allocation of FixedSized Biocks 
The simplest form of dynamic allocation involves blocks OF a fixed size. By 
ltnking the bbcks in a list, as in Fig, 7.41, allocation and deallocation can be 
done quickly with little or no storage overhead. 
Fig. 7.4 1. A cicallocatsd block is added to thc licr of available: blocks. 
Suppose that blocks are to be drawn from a cmtiguous arca of storage. Ini- 
tialization of the area is done by using a portion of each block for a link to the 
next block. A pointer r r v d d u  points to the first Muck. Ailoation consists 
of taking a block off the list and deallwtion consis~s of purting the block 
back on the list. 
The compiler routines that manage blocks do not need to know the type of 
object that will be held in the block by the user program. We can treat each 
block as a variant record, with the compiler routines viewing the block as con- 
sisting of a link to the next block and rhe user program viewing the block as 
being of some other type. Thus, there is no space overhead hcause the user 
program can use the entire block for its own purposes, When the blmk is 
returned, then the mrnpiler routines use sonre of the space from the block 
itself to link it into the list of available blocks, as shown in Fig. 7,41. 
When blocks are allocated and deallocated, storage can become frugmen~ed; 
that is, the heap may consist of alternate blocks that are free and in use, as in 
Fig. 7.42. 
Fig. 7.42, Frw and uscd blocks in a bcap. 

444 
RUN-TIME ENVIRONMENTS 
SEC. 7.8 
The situation shown in Fig. 7.42 can occur if a program allocates five 
blmks and then deabcateu the second and fourth, For exampie. Fragrnenta- 
tion is of no consequence if blocks are of fixed size, but if they are of variable 
size, a situation like Fig. 7.42 is a problem, because we could not allocate a 
bick larger than any one of the free blocks, even though the space is avail- 
able in principle. 
One met hod for allwating variable-sized blocks is caljed the first-fir mrrhud. 
When a block of size .r. is allocated, we search for the first free block that is of 
size f r s. This block is then subdivided into a used block of size s, and a 
free block of size f -3. Note that aliacation Incurs a time overhead because 
we must search for a free block that i s  large enough, 
When a b l ~ k  
is deallocated, we check to see if it is next to a free block. If 
possible, the deallocated block is combined with a free block next to it to 
create a larger free block. Combining adjacent free blwks into a larger free 
block prevents further fragmentation from occurring. There are a number of 
subtle details concerning how free blocks are allocated, deallowed, and main- 
tained in an available list or lists. There are also several tradeoffs between 
time, space, and availability of large blocks. The reader is referred to Knuth 
1 1973aj or Aho, Hopcroft, and U llrnan 1 1983 1 for a discussion of these issues. 
Implicit Deallocation 
Implicit deallocat ion requires cooperation between the user program and the 
run-time package, because the latter needs to know when a storage block is no 
longer in use. This cooperation is implemented by fixing the format of 
storage blocks, For the present discussion, suppose that the format of a 
stwage block is as In Fig. 7+43. 
Fig* 7.43. Thc format of a block. 
The first problem is that of recognizing block boundaries. If the size of 
blocks is fixed, then position information can be used, For example, if each 
block occupies 20 words. then a new block begins every 20 words. Otherwise. 

SEC, 7.8 
DYNAMIC STORAGE ALLOCATION TECHNIQUES 445 
in the inaccessible srorage attached to a block we keep h e  size of a block, so 
we can determine where the next b h k  begins. 
The second problem is that of recognizing if a block is in we. We assume 
that a block is in u g  if it is pwibk for the user program to refer to the infor- 
mation in the block. The reference may occur through a winter or after fol- 
lowing a sequence of pointers, so the compiler needs to know the position in 
storage of all pointers. Using the format of Fig, 7.43, pointers are kepr in a 
fixed position in the block. Perhaps more to the point, the assumption is 
made that the user-information area of a blwk dms not contain any pointers, 
Two approaches can be used for implicit deallocation. W e  sketch them 
here; for more details see Aho, Hoproh, and Ullman 1 19331. 
I. 
Refcrenct counts+ We keep track of the number of blocks that point 
directty to the present block. I f  this count ever drops to 0, then the block 
can be deallocated kcauw it cannot be referred to. In orher words, the 
block has become garbage that can be collected. Maintaining reference 
counts can be costly in time; the pointer assignment p :=q leads to 
changes in the reference counts of the blocks pointed to by bath p and q. 
The count for the blmk pinted to by p goes down by one. while that for 
the block pointed to by q goes up by one. Reference counts are bea used 
when pointers between blocks never appear in cycles. For example, in 
Fig. 7.44, neither block is accessible from any other, so they are both gar- 
bage, but each has a reference count of one. 
Murking tc~c.kniyu~s. An alternative approach i s  tu suspend temporarily 
execution of the user program and use the h z e n  pointers to determine 
which blwks are in use. This approach require> all the pointers into the 
heap to be known. Conceprualiy, we pour paint into the heap through 
these pointers. Any block that is reached by the paint is in use and the 
rest can k dcallocated. [n more de~ail, we go through the heap and 
mark all blocks unused. Then, we follow pointers marking as u s d  any 
block that is reached in the process. A Final sequential scan of the heap 
a b w s  all blocks still marked unused to be collecttd. 
Mg. 7.44. 
Garbagc cclls with ntmzcro rcfcrcncc counts. 
With variable-sind blocks, we have the additional possibility of moving 
used storage blmks from their current pusitions.x This process. catied 
-- 
W
c
 
~ u t d  
do >r\ with f i ~ c r l - ~ i ~ . ~  
blrwks. hut no advantrgc rcwlts. 

446 RUN-TIME ENVlRONMENTS 
SEC. 7.8 
~'ompucricm moves ail used blocks to one end of the heap. r i ~  
that all the free 
storage can be collected into one large free block. Compaction also requires 
information about the pointers in blocks because when a used block is moved, 
all pinters to it have to be adjusted to reflect the move. Its advantage is that 
afterwards fragmentat ion of available storage is eliminated. 
7.9 STORAGE ALLOCATION IN FORTRAN 
Fortran was designed to permit static storage allocation, as in Section 7.3. 
However. there are some issues, such as the treatment of COMMON and 
EQUIVALENCE declarations. that are fairly special to Fortran. A Fortran 
compiler can create a numhr of duru areas, i-e., blocks of storage in which 
the vaiues of objects can be stored. In Fortran, there is one data area for 
each procedure and one data area for each named COMMON block and for 
blank COMMON, if used. The symbol table must record for each name the data 
area in which it belongs and its offset in that data area, that is, its position 
relative to the beginning of the area. The compiler must eventually decide 
where the data areas go relative to the executable code and to one another, 
but this choice is arbit~ary, since the data areas arc independent. 
The compiler must compute the size of each data area. For the data areas 
of the procedures, a single counter suffices, since their sizes are known after 
each procedure is processed. For COMMON blocks, a record for each Mock 
must be kept during the processing of all procedures, since each procedure 
using a block may have its own idea of how big the block Is, and the actual 
size is the maximum of the sizes implied by the various procedures. If pro- 
cedures are separately compiled, a link editor must k used lo select the size 
of the COMMON block to be the maximum of all such blocks with the same 
name among the pieces of code being linked. 
For each data area the compiler creates a memory mop, which is a descrip- 
tion of the contents of the area. This "memory map" might simply consist of 
an indication, in the symbol-table entry for each name in the area, of its offset 
in the area. We need not necessarily have an easy way of answering the ques- 
tion, "What are all the names in this data area?" However, in Fortran we 
know the answer for the procedures' data areas, since all names declared in a 
procedure that are not C O ~ O N  
or equivalenced to a COMMON name are in the 
procedure's data area. COMMON names can have their symbol-table entries 
linked, with one chain for each COMMON block, in the order of their appear- 
ance in the block. In fact, as the offsets of names in the data area cannot 
always Ix determined until the entire procedure is processed (Fortran arrays 
can be declared before their dimensions are declared), it is necessary that 
these chains of COMMON names be created, 
A Fortran program consists of a main program, subroutines, and functions 
(we call them all prowdltr~s). Each occurrence of a name ha.; a scope consist- 
ing of one procedure only. We can generate object code for each procedure 
upon reaching the end of that procedure. If we do so, it is possible that most 

SEC. 7.9 
STORAGE ALLOCATION IN FORTRAN 447 
of the information in the symbl tabk can be expunged. We need only 
preserve those names that are external to the routine just processed. These 
are names of other procedures and of common Mocks. These names may not 
truly be external to the entire program k i n g  compiled, but must be preserved 
until the entire collection of procedures is processed. 
We create for each block a record giving the first and last names. belonging (o 
the current procedure, [hat are declared to be in that COMMON block. When 
processing a declaration like 
COMMON /BLOCKI/ 
NAMEl, NAME2 
the compiler must do the following. 
I. In the table for COMMON block names, create a record for BLOCK1, if one 
does not already exist. 
2, 
In the symbol-table entries for NAME1 and NAME2, set a pointer to the 
symbol-table entry for BLOCK 'I, indicating that these are in COMMON and 
members of BLoCK1. 
3. a) 
If the record has just now been created for BLOCK1. set a pointer in 
that record to the symbol-table entry Tor NAMEj, indicating the first 
name in this COMMON block. Then, link the symbol-table entry for 
NAME? to that for NAME2, using a field of the symbol table reserved 
for linking members of the same COMMON blwk. Finally, set a 
pinter in the record for BLOCKI to the symbol-table entry for 
N M E 2 ,  indicating the last found member of that block. 
b 
If, however. this is not the first declaration of BLOCK I, 
simply link 
NAME1 and NAME2 to the end of the iist of names for BLOCKI. The 
pointer to the end of the lisl for BLOCK1, appearing in the record for 
BLOCK I. is updated of course. 
After a procedure has been processed, we apply the equivalencing algo- 
rithm, to be discussed shortly. We may discover that some addilional names 
belong in COWON because they are equivalenced to names thal are themselves 
in COWON. We shall find that it is not actually necessary to link such a name 
XYZ to the chain for its COMMON block. A bit in the symbol-table entry for 
XYZ is set, indicating thar XYZ has been equivalenced to something else, A 
data structure to be discusgd will then give the position of xYz relative to 
some name actually declared to be in COMMON. 
After performing the equivalence operations. we can create a memory map 
for each COMMON block by scanning the list of names for that block. Initialize 
a counter to zero, and for each name on the list, make its offset equal to the 
current value of the counter. Then, add to the counter the number of 
memory units taken by the data objea denoted by the name. The COMMON 

448 
RUN-TIME ENVIRONMENTS 
SEC. 7.9 
block records can then be deleted and the space reused by the next procedure, 
If a name XYZ in COMMON is equivalenced to a name not in COMMON. we 
must determine the maximum offset from the beginning of xYZ for any word 
of storage needed for any name equivalenced to XYZ. For example, if XYz is 
a real, equivalenced to AI 5 , 5  1, where A is a IOX 10 array of reals, Al: 1 ,1) 
appears 44 words before XYZ and A I  10 ,10 1 appears 55 words after XYZ, as 
shown in Fig. 7.45. The existence of A does not affect the counter for the 
COWON block; it is only incremented by one word when XYZ i s  considered, 
independent of what XYz is equivalenced to. However, the end of the data 
area for the COMMON block must be far enough away from the beginning to 
accommodate the array A. We therefore record the largest offset, from the 
beginning of the COWON block, of any word u
d
 
by a name equivalenced to 
a member of that block. In Fig. 7.45, that quantity must be at least the offset 
of XYZ plus 55. We also check that the array A does not extend in front of 
the beginning of the data area; that is, the offset of XYZ must k at kast 44. 
Otherwise, we have an error and must produce a diagnostic message. 
Fig. 7.4. Rclation between COHMON and EQUIVALENCE statements, 
A Simple Equivalence Algorithm 
The first algorithms for processing equivalence statements appeared in assem- 
blers rather than compilers. Since these algorithms can be a bit complex, 
especially when interactions between COMMON and EQUIVALENCE statements 
are considered, let us treat firs1 a situation typical of an assembly language, 
where the only EQUIVALENCE statements are of the 'form 
EQUIVALENCE A, B+wfl~e~ 
where A and B are the names of locations. This stalement makes A denote the 
location that is u@et memory units beyond the location for 8. 
A sequence of EQUIVALENCE statements groups names into equiwknce sets 
whose positions relative to one another are all defined by the EQUIVALENCE 
statements, For example, the sequence of statements 
EQUI VALE~CE 
A, B+ i 
0 0 
EQUIVALENCE C, D-40 
EQUIVALENCE A, C+30 
EQUIVALENCE E, P 

SEC. 7.9 
STORAGE ALLOCATION IN FORTRAN 449 
groups names into the sets {A, B, C, D) and {E, F), where E and F denote the 
same location. C is 70 locations after B, A is 30 after C, and D L 10 after A. 
To mmpute the equivalence sets we create a tree for each set+ Each node 
of a tree represents a name and cmtains the offset of that name relative to the 
name at the parent of this node, The name at the root of a tree we call the 
leu&. 
The position of any name relative to the leader can be computed by 
following the path from the node for that name and adding the offsets along 
the way. 
Example 7.12, The equivalence set {A, B, C, D) mentioned above could be 
represented by the tree shown in Fig. 7.46. D is the leader, and we can dis- 
cover that A is Iwaced 10 positions before D, since the sum of the offsets on 
the path from A to  is I00 + (-110) = -10. 
o 
Let us now give an algorithm for constructing trees for equivalence sets. 
The relevant fields in the symbol-table entries are: 
I. purent, pintingto thesymbol-tableentry for the parent, null if the name 
is a root (or not equivilenced 10 anything), and 
2, 
a@er, giving the offset of a name relative to the parent name. 
The algorithm we give assumes that any name muld be the leader of an 
equivalence set. In practice, in an assembly language, one and only one name 
in the set would have an actual location defined by a pseudo-operation, and 
this name would be made the leader. We trust the reader can see how to 
modify the algorithm 10 make one particular name be the leader. 
Algorithm 7.1. Construct ion of equivalence trees. 
fnput, A list of equivalence-defining statements of the form 
EQUIVALENCE 
A, B+drsl 
Ourpur. A collection of trees such that, for any name mentioned in the input 
list of equivaiences, we may, by following the path from that name to the root 
and summing the oflser's found along the path, determine the position of the 
name relative to the leader. 

Mdmd. We repeat the steps of Fig, 7.47 for each equivalence statement 
EQUIVALENCE A, B+bisl, in turn. The justification for the formula in line 
(12) for the offset of the leader of A relative to the leader of B is as fdlaws. 
The bation of A, say lA, is equal to c. plus the location of the leader of A, say 
mA, The location of B, say la, equals d plus the location of the leader of B, 
say m ~ .  
But rl, = Ig -t dist, so c C m~ = d + ms + d i s ~  Hence r n ~  - ms 
equals d - c + dist. 
n 
begin 
( 1 )  
k t  p and q point to the nodes for A and 8, respectively; 
0) 
c := 0; 6 : = 0; 
#x c and d compute the offscts of A and B 
from the lcadan of their respective sets +/ 
(31 
while parenr ( p )  # null do kgh 
(4) 
r : 
c -t t@wr {p); 
(51 
p : = p a r d  ( p )  
cad; 
l* move p to the leader of a, accumulating 
0ffSCts as WC gQ */ 
(61 
while purent q )  # null do begin 
(7) 
d : = d + uJset(q); 
I81 
q := p r u w  ( q )  
ead; 
/* do the s a w  for B *f 
(91 
iPp = q them /* A and B arc already cquivalend *f 
t 10) 
tC c -d # dist Ihm error; 
/ + A and B have b x n  given two different relative positions *i 
d w  kgin 
/* merge the sets of A and B * i  
( 1  1) 
r
e
 
p 
= q 
!+ 
make thc leader of A 
a chiid of B's leader */ 
(12) 
r ~ f l w r  (p ) : = d - c + di3r 
e d  
erul 
Fig. 7.47. Equivalence algorithm, 
EQUIVALENCE 
A, B+IOO 
EQUIVALENCE 
C, D-40 
we get the cunfiguration shown in Fig. 7.46, but without the offset - i10 in 
the node for B and with no link from B to D+ When we process 
EQUIVALENCE 
A, C+3O 

SEC. 7.9 
SKIRAGE ALLOCATION LN FORTRAN 451 
we find that p points to B after the while-lmp of line (3) and q points to d 
after the while-bop of line (61, We also have c = 100 and d = -40. Then 
at line (1 1) we make D the parent of B and set the oflset field for B to 100, 
which is (-40)- ( 100)+30. 
o 
Algorithm 7.1 c w l d  take time proportional to n2 to profess n equivalences, 
since in the worst case the paths followed in Ihe loops of lines (3) and (6) 
could include every node of their respective trees. Equivalencing requires 
only a tiny fraction of the time spent in compilation, so n2 steps is not prohi- 
bitive, and an algorithm more complex than that of Fig. 7.47 is probably nut 
justified. However, it happens that there are two easy things we can do to 
make Algorithm 7.L take time thiit is just about linear in the number of 
equivalences i t  processes. While it is not likely that equivalence sets will be 
large enough, on the average, that these impr~vements need to be imple- 
mented, it is worth noting that quivalencing serves as a paradigm for a 
number of imprtant processes involving "set merging," For example, a 
number of efficient algorithms for data-flow analysis depend on fast 
equivalence algorithms; the inlerested reader i s  teferred to the bibtiographic 
notes of Chapter 10, 
The first improvement we can make is to keep a count, for each leader, of 
the number of ndes in its wee. Then, at lines ( 1 I) and ( 12), instead of arbi- 
trarily linking the leader of A to the leader of 8, link whichever has the 
smaller count to the other, This makes sure that the trees grow squat so paths 
will be short. It is left as an exercise that n equivalences performed in this 
manner cannot produce paths longer than 10g2n nudes. 
The second idea is known as path compression. When following a pa~h to 
the root in the Imps of lines (3) and (61, make all ndes encountered children 
of the leader if they are not already so. That is, while following the path, 
recwd ail Ihe nodes r r , ,  nz, . . , nk encountered, where n is the node for A 
or B and nk is the leader. Then adjust offsets and make n 1 ,  n 2 ,  . . . * nk - 
children of nk by the steps in Fig. 7+48. 
Fig. 7.48. Adjustrncnt of offscts. 

452 
RUN-TIME ENVIRONMENTS 
An Equivalence Algarithm for Fortran 
There are several additional features that must be added to Algorithm 7+1 to 
make it work for Fortran. First, we must determine whether an equivaknm 
set is in COMMON, which we can do by recording for each leader whether any 
of the names in its set are in COHMON, and if so, in which block. 
Second, in an assembly language, one member of an equivalence set will pin 
down the entire set to reality by being a label of a statement , thus allowing 
the addresses denoted by all names in the set to be computed relative to that 
one location. 
In Fo~tran, however, it is the compikr's job to determine 
storage locations, so an equivalence set not in COMMON may be viewed as 
"floating" until the compiler determines the position of the whole set jn its 
appropriate data area. To do so correctly, the compiler needs to know the 
extent of the equivalence set, that is, the number of locations which the names 
in the set callectively occupy. To handle this problem we attach to the leader 
two fields, b w  and high, giving the offsets rehtivt to the leader of the lawest 
and highest locations used by any memkr of the equivalence set. Third, 
there are minor problems inirduced by the fact that names can be arrays and 
locations in the middle of an array can be equivalenced to locations in other 
arrays. 
Since there are three fields (low, high, and a pointer to a C~MMON 
block) 
that must be asmiated with each leader, we do not want to allocate space for 
these fields in all symbol-table entries, One course of action is to usc the 
parmt field from Algorithm 7.1 to point, in the case of the leader, to a record 
in a new table with three fields, im, high, and rumblk, As this table and the 
symbol table occupy disjoint areas, we can tell to which table a pointer points. 
Alternatively, Ihe symbol table can contain a bit indicating whether a name is 
currently a leader. If space is really ht a premium. an alternative algorithm 
avoiding this extra table at the cost of a bit more programming effort i s  dis- 
cussed in the elrercises. 
Let us consider the calculation that must replace lines ( I  I) and (12) of Fig. 
7.47. The situation in which two equivalence sets, whose leaders are pointed 
to by p and q, must be merged is depicted in Fig 7.49(a1. The data structure 
representing the two sets appears in Fig, 7.49(b). First we, must check that 
there are not two members among the two equivalence sets that are in COM- 
KIN. Even If both are in the same bid, the Fortran standard forbids their 
being equivalenced. If any one COMMON block contains a member of either 
equivalence ser. then the merged set has a pointer to the record for that block 
in comMk. The code doing this check, assuming the leader pointed to by q 
becames the leader of the merged set, is shown in Fig. 7.50. In place of lines 
( I  I )  and (12) of Fig, 7,47 we must also compute the extent of the merged 
equivalence set. Figure 7.49Ia) indicates the formulas for the new values of 
/ow and high relative to the leader pointed to by y. 

SEC. 7.9 
STORAGE ALLOCATION IN FORTRAN 453 
b w  
location of r? 
I 
I 
b- 
dbi -d 
I 
hcrttioa u i  b 
A 
I 1  
I 
J 
- 
low 2 
tocation of Icabcr 
h i ~ h  
2 
pointcd to by y 
(a) rclativc positions of cquivalcncc scts 
(b) data structure 
Fig. 7.49. 
Mcrging cquivalcncc scts. 
Fig. 7.50, Computing COMMON blocks. 

454 
RUN-TIME ENVIRONMENTS 
SEC. 7.9 
Thus we must do 
w
n
 
These statements are followed by lines 1 1 1 } and I 12) of Fig. 7.47 to effect the 
merger of the two equivalence sets. 
Two last details must be covered to make Algorithm 7-1 work for Fortran, 
In Fortran, we may equivalence positions in the middle of arrays to other 
positions in other arrays or to simple names. The offset of an array A from 
its leader means the offset of the first location of A from the first bcation of 
the leader, If a location like A I  5 , 7 )  is equivaienced to, say, BI 20 1, we 
must compute the position of A( 5, 7 1 relative to A ( 1, 1 1 and initialize c to 
the negative of this distance in line (2) of Fig. 7.47. Similarly, d must be ini- 
tialized to the negative of (he position of BIZ01 relative to BI 11, The for- 
mulas in Section 8.3 rogethe~ with a knowledge of the size of elements of the 
arrays A and 9, are sufficient to calculate the initial values of c and ti. 
The last detail to be covered is the fact that Fortran allows an 
EQUIVALENCE which involves many locations, such as 
These may be treated as 
Note that if we do the equiwlences in this order, only A kcorncs.the leader 
of a set of more than one element, A record with low. high, and cumblk can 
be used many times for "equivalence sets" of a single name. 
Mapping Data Areas 
We may now describe the rule3 whereby space in the various data areas is 
assigned for each routine's names. 
1. 
For each COmON block, visit all names declared to be in that block in the 
order of their declarations (use the chains of COMMON names created in 
the symbol table for this purpose). Allmate the number of words needed 
for each name in turn, keeping a count of the number of words allocated, 
so offsets can be computed for each name. If a name A is equivalenced, 
the extent of its cguwalence set does not matter, but we must check that 
the Iuw value for the leader of A does not extend past the beginning of 
the COWSON block. Consult the high value for the leader to put a lower 
limit on the last word of the block. We leave the exact formulas for 
these calculati~ns to the reader. 

2. 
Visit all names for the routine in any order. 
a) 
If a name is in COMMON, do nothing. Space has been allocated in 
(U* 
b) 
If a name is not in CQMHON and not equivalenced. allocate the neces- 
sary number of words in the data area for the routine. 
C) 
If a name A is equivalenced, find its leader, say t. If L has 
already been given a position in lhe data area for the routine, com- 
pute the position of A by adding to that position all the & d s  
found 
in the path from A to L in the tree representing the equivalence slet 
of A and L. If L has not been given a position, allocate the next 
high-low words in the data area for the equivalence set. The psi- 
tion of L in these words is - b w  words from the beginning, and the 
position of A can be calculated by summing ~ # s d s  as before. 
EXERCISES 
7.1 Using the scope rules of Pascal, determine the declarations that apply 
to each occurrence of the names a and b in Pig. 7.5 1. The output of 
the program consists of the integers I 
thrwgh 4, 
program a(input, output): 
procedure b(u, v, x, y: ifiteger); 
var 
a : record a, b : integer end; 
b : record b. a : integer end; 
begin 
with a do begin a := u; b := v end; 
w i t h  b do begin a := x; b := y end; 
writeln(a.a, a.b, b.a, b.b) 
end ; 
begin 
bl1,2, 3 , 4 1  
end* 
Fig. 7.51. Paswl program with iicvcral dcclaraticms of a and b. 
7.2 Consider a Muck-structured language in which a name can k declared 
to be an integer or a real+ Suppose that expressions are represented 
by a terminal expr and that the only statements are assignments, con- 
ditionals. whiles, and sequences of statements. 
Assuming that 
integers are allocated one word and that rcals are allocated two 
words, give a synlax-directed algorithm (based on a reasonable gram- 
mar for declarations and blocks) to determine bindings from names to 
words that can be used by an activation of a block. Does your 

456 
RUN-TIME ENVIRONMENTS 
CHAPTER 7 
allocation use Ihe minimum number of words adequate for any execu- 
tion of the block? 
*7,3 In Section 7.4 we claimed that the display could be maintained 
correctly if each procedure at depth i stored b [ i ]  at the beginning of 
an activation and restored d li I at the etid. Prove by induction on the 
number of calls that each procedure sees a wrred display. 
7.4 A macro is a form of procedure, implemented by literally substituting 
the body for each procedure call. Figure 7.52 shows a Pic program 
and its output. The first two lines define the macros show and 
small. The bodies of the macros are contained between the two X 
signs on the lines, Each of the four circles in the figure is drawn 
using show; the radius of the circle is given by the nonlocal name r. 
Blocks in Pic are delimited by [ and 1. Each variable assigned to 
within a block is implicitly declared within the block. From the out- 
put, what can you say abut the m p e  of each occurrenw of r? 
define show 
% { circle radius r a t  Here 
X 
define small X [ r + 1/32; show 1 % 
[ 
r = 116; 
show; small; 
move ; 
show; small; 
Pig. 7 .S2. Circles drawn by a Pic program. 
7,s Write a procedure to insert an item into a linked list by passing a 
pointer to the head of the list. Under whar parameter passing 
mechanisms does this procedure work? 
7.6 What is printed by the program in Fig. 7.53, assuming la) call-by- 
value, (b) call-by-reference, (c) copy-restore linkage, Id) call-by- 
name? 
7.7 When a procedure is passed as a parameter in a lexically scoped 
language, its nonlocal environment can be passed using an access link. 
Give an algorithm to determine this link. 

CHAPTER 7 
EXERCISES 457 
Fig. 733. P~udo-program illustrating paramctcr passing. 
7.8 The three kinds of environments that could k associated with a pro- 
cedure passed as a parameter are illustrated by the Pascal program in 
Fig. 7.54. The lxicai, passing, and activation environments of such a 
procedure consist of the bindings of identifiers at the point at which 
the procedure is defined, passed as a parameter, and activated, 
respectively. Consider function f. pas& 
as a parameter on line I I. 
( I) program paraml input, a t p u t  1 ; 
(2) 
procedure b(functi0n bin: integer 1 : integer 1; 
13) 
var m : integer; 
(4) 
b g i n  m :s 3; wrlteln(h(2)) end { b 1; 
r 5) 
procedure c ;  
(6) 
var m : integer; 
(91 
prrsatciurt r ; 
10) 
var m : integer; 
(I 1) 
begin m := 7 ;  btf) end I r I ;  
Fig. 7.54. Example of lexicat , passing, and activation cnvironrncnts. 

458 
RUN-TIME ENY lRONMENTS 
CHAPTER 7 
Using the lexical, passing, and activation environments for f, nonh- 
cal m on line 8 is in the scope of the declarations of m on lines 6, 10, 
and 3, respectively. 
a) Draw the activation tree for this program. 
b) What is the output of the program, using the lexical, passing, and 
activation environments for f? 
*c) Modify the display implementation of a lexically smped language 
to set up the lexical environment correctly when a procedure 
passed as a parameter is activated. 
*7.9 The statement $: - a on line 9 1 of the pseudo-program in Fig. 7.55 
calls function a, which passes function addm back as a result, 
a) Draw the activation tree for an execution of this program. 
b) Suppose that lexical scope is used for nonlocal names. Why will 
the program fail if stack ahcation is used? 
C) What is the output of the program with heap allocation? 
Flg. 7.55. Pscudo-program in which function u d h  i s  rcturncd as a result. 
*7,10 Certain languages, like Lisp, have the ability to return newly created 
procedures at run time. I n  Fig. 7.56. all functions, whether defined 
in the source text or created at run time, take at most one argument 
and return one value, either a function or a real. The operator 
stands for composition of functions; that is m){x) 
= j ( g ( x ) ) .  
a) What value is printed by muin'? 
*b) Suppose that whenever a procedure p is created and returned, its 
activation record becomes a child of the activation record of the 
function returning p. The passing environment of p can then k 

CHAPTER 7 
EXERClsES 459 
maintained by keep~ng a tree of activation records rather than a 
stack. What is the tree of :ictivation records when a is computed 
by main in Fig. 7-56? 
*c) AIternatively, suppose an activation record for p is created when p 
is activated, and made a child of the activation record for the pro- 
cedure calling p. This approach can be used to maintain (he 
activation environment for p. Draw snapshots of the activation 
records and their parent-child relationships as the statements in 
min are executed. 1s a stack sufficient to hold activation records 
when this approach is used? 
funrtkr f (x : function j: 
var 
y : function; 
y : = x oh ; j+ crcatcs y when executed x i  
return y 
end { S ); 
function k ( ); 
. 
return sin 
end { k 1; 
function g Is : function); 
var 
w : function; 
w : = n m m  0 z ; 1 + crcatcs w when cxecuted + / 
return w 
end { 
1; 
Fig, 7.56. Pseudo-program creating functions at run timc. 
7.11 Another way to handle deletion from hash tables for names whose 
scope has been passed (as in Section 7+6) 
is to leave expired names on 
a list until that list is again searched. Assuming entries include the 
name of the procedure in which the declaration is made, we can in 
principle tell whether a name i s  old, and delete it if so. Give an 
indexing scheme for procedures that enables us to tell in O(1) time 
whether a procedure is "old." i-e., its scope has been passed. 

RUN-TIME ENVIRONMENTS 
CHAPTER 7 
7,12 Many hash functions can be characterized by a sequence of integer 
constants cue, a,, 
+ . . . 
If ci, I S i S n ,  is the integer value of the ith 
character in string s, then the string is hashed to 
where rn is the size of the hash table. For each of the following cases, 
determine the sequence of constants ao, 
al, A . .  or show that no such 
sequence exists. Each case determines an integer; a hash value is 
obtained by taking that integer mod m. 
a) Take the sum of the characters. 
b) Take the sum of the first and last characters. 
c) Take h,, where hU - 0 and hi = 2hi-, + c,. 
d) Treat the bits in the middle 4 characters as a 32-bit integer. 
e) A 32-bit integer can be viewed as consisting of 4 bytes, where each 
byte rs a digit that takes on 256 possible values. Starting with 
0000, for I l i ~ n ,  
add cf into byte i mod 4, with carries permitted. 
That is? c ,  and c~ are added into byte I ,  c2 and c,j into byte 2, 
and rn on+ Return the final value. 
V.13 Why do hash functions characterized by a sequence of integers 
a@, a , ,  . . . as in Exercise 7.12 sometimes perfmm poorly if the input 
consists of consecutive sqrings, e.g., v000, ~ 0 0 1 , .  . . ?  The symp- 
tom is that somewhere along the way, their behavior deviates from 
random and can be predicted. 
**7,14 When n strings are hashed into m lists, the mean number of strings 
per list is n / m, no matter how unevenly the strings are distributed. 
Suppose that d is a 'Uistribution," i.c., a random string is 
on 
the Ith list with probability d ( i ) .  Suppose that a hash function with 
distribution d happens to place b, randomly selected strings in list j, 
O r j a m  1. Show that the expected vduc W = Ly=<'(b,)(b,+ 
1) / 2  
is linearly related ro the variance 'of the distribution d. For a uniform 
distribution show that the expected value of W is (n / 2m)(n +2m- I). 
7.15 Suppose we have the following sequence of declarations in a Fwtran 
program. 
??how the contents of the data areas for SUB and CBLK (at least the 
portion of CBLK's area accessible from SUB). Why is there no space 
for X and Y? 

CHAPTER 7 
B[BLIOGRAPHIC NOTES 461 
*7.16 A useful data structure for equivalence computations is the ring srruc- 
r e .  We use one pointer and an offset field in each symbol-table 
entry to link members of an equivalence set. This structure is sug- 
gested in Fig, 7.57, where A, B, C ,  and D are equivalent. and E and 
are equivalent, with the location of B being 20 words after that of A, 
and w on. 
Give an algorithm to compute the offset of x relative to Y. assurn- 
ing that X and Y are in the same equivalence set. 
Give an algorithm to cornpule Iow and high, as defined in M i o n  
7.9. relative to the location of some name 2 .  
Give an algorithm to process 
EQUIVALENCE U, V 
Do not assume that u and v are occessarily in different 
equivalence sets. 
Fig. 7S7, Ring structures. 
*7.17 The algorithm to map data areas given in Section 7.9 requires that we 
verify that low for the leader of A'S equivalence set does not caux the 
space for the equivalence set of A to extend before the keginning of 
the COMMON block and that we calculate high for the leader of A to 
increase the upper limit a( the COMMON bbck, if necessary. Give for-. 
muhs in terms of nrxi, the offset of A in the COMMON block, and lust. 
the current last word of the block, ro make the test and to update lust, 
if necessary + 
BIBLIOGRAPHEC NOTES 
Stacks have played an essential role in the implementation of recursive func- 
r i m s .  McCarthy 1 198 1 .  p, 1781 recalls that during a Lisp-implementation pro- 
ject begun in 1958 it was decided to "use a single contiguous public stack 
array to save the values of variables and subrouline return addresses in the 
implementat ion of recursive subroutines. " The inclusion of blccks and recur- 
sive procedures in Algol 60 - 
see Naur 1198 I, Section 2.101 for a derailed 
account of their design - 
also stimulated development of stack aUocation + 
The idea of a display for accessing nonlocals in a lexically scoped language is 
due to Dijkstra ) 1960, 19631. Although Lisp uses dynamic scope, ir is possible 
to achieve the effect of lexical scope using "funargs" consisting of a function 
and an access link: McCarlhy 119811 describes the development of this 

462 
RUN-TIME ENVtRONMENTS 
CHAPTEK 7 
mechanism. Successors of Lisp such as Common Lisp (Steele [I98411 have 
moved away from dynamic scope. 
Explanations of bindings for names can be found in te~tbooks on program- 
ming languages, see for example Aklwn and Sussman [1985], Pratt [1984], 
or Ttnnent [1981]. An alternative approach, suggested in Chapter 2, is to 
read the description of a compiler. The step-by-step development in Ker- 
nighan and Pike [ 19841 starts with a calculator for arithmetic expressions and 
builds an interpreter for a simple language with recursive procedures. Or see 
the code for Pascal-S in Wirth [1981]. A detailed description of stack allma- 
tion, the use of a display, and dynamic allocation of arrays appears In Randell 
and Russell [1%4]. 
Johnson and Ritchie [I9811 dixuss the design of a calling sequence that 
allows the number of arguments of a procedure to vary from call to call. A 
general method for setting a gbbal display is to follow the chain of access 
links, setting the display elements in the process, The approach of Section 7.4 
that touches just one element, seems to have been "well known" for some 
time; a published reference is Rohl [1975]. Moses 119701 discusses the distinc- 
tions between the environments that apply when a funclba i s  passed as a 
parameter and considers the problems that arise when such environments are 
implemented using shallow and deep access. Stack ahcation cannot be used 
for languages with cordutines or multiple processes. Lampson [I9821 mnsid- 
ers fast implementations using heap allocation, 
In mathematical logic, quantified variables of limited scope and substitution 
appear with the Begriffsschrift of Frege [1879]. Substitution and parameter 
passing have been the subject of much debate in both the mathematical logic 
and. programming language communities. Church [ I%, p. 2881 observes, 
"Especially difficult is the matter of correct statement of the rule of substitu- 
tion for functional variables," and relates the development of such a rule for 
the propositional calculus. The lambda calculus of Church [I9411 has h e n  
applied to environments in programming languages, for example by Landin 
[19H], A pair consisting of a function and an access link is often referred to 
as s closure, following tandin 119641. 
Data structures for symbol tables and algorithms for searching them are dis- 
cussed in detail in Knuth [1973b] and Aho, Hopcroft, and Ullrnan 11974, 
19831. The lore of hashing is treated in Knuth [ lW3bl and Morris €196Xb]. 
The original paper discussing bashing is Peterslon [1957]. More on symbol- 
table organization techniques can be found in McKeeman 119761, Example 
7.10 is from Bentky, Cleveland, and Sethi [1985]. Reks [1983] describes a 
symbol-table generator. 
Equivalence algorithms have ken described by Arden, Galler, and Graham 
[l%j J and Galler and Fischer [1%4]; we have adopted the latter approach. 
The efficiency of equivalence algorithms is discussed in Fischer 119721, Hop 
croft and -~llman 
[1973], and Tarjan [1975]. 

CHAPTER 8 
Intermediate Code 
Generation 
In the analysis-synthesis model of a compiler, the front end translates a source 
program into an intermediate representation from which the back cnd gcn- 
prates target code. Derails of the target language are eonfined to the back 
end, as far as pssibk. Although a source program can be translated dircctly 
into the target language, some benefits of using a machine-independent Inter- 
mediate form are: 
I. Retargeting is facilitated; a complier for a different machine can be 
created by attaching a back end for the new machine to an existing front 
end. 
2. 
A machine-independent code optimizer can be appIied to the intermediate 
representation. Such optimixrs are discussed in detail in Chapter 10. 
This chaprer shows how the syntaxdirected methods of Chnptcrs 2 and 5 
can be usd to translate into an intermediate Corm programming language can- 
structs such as dechration s, assignments, and flow-of-control statements. For 
simplicity. we assume that the scsurce program has already ken parsed and 
statically checked, as in the organization of Fig. # , I .  Most of the syniax- 
directed definitions in this chapter can bc implemented during either bottom- 
up or topdown parsing using the techniques of Chapter 5. so intermediate 
code generation can be Folded into prsing, if desired. 
Fig. 8.1. Position of intermediate c d e  generator. 

464 
INTERMEDIATE CODE GENERATION 
8,l INTERMEDIATE LANGUAGES 
Syntax trees and post fit notation, introduced in Sections 5,2 and 2.3, respec- 
tively, are t~ 
kinds of intermediate representations. A rhird, called three- 
address code, will be used in this chapter. The semantic rules f w  generating 
three-address code from common programming language constructs are similar 
to those for constructing syntax trees or for generating p s i f i x  notation. 
Graphical Repmentatbns 
A syntax tree depicts the natural hierarchical structure of a source program. 
A dag gives the same information but in a more compact way because com- 
mon subexpressions are identified. A syntax tree and dag for the assignment 
statement a := b *  - c  + b* - c  appear in Fig+ 8.2. 
assign 
a I \ + 
assign 
a I \ + 
Fig, 8,2, Graphical rcprcscnlations of a : = b * - c + b* - c. 
Postfix notation is a linearized representation of s syntax tree; it is a list of 
the nodes of the tree in which a node appears immediately after its children. 
The postfix nr>tation for the syntax tree in Fig. 8.2Ia) is 
'a b e urninus * b c uminus * + assign 
I 8 . l )  
'I'he edges in a syntax tree do not appear explicitly in postfix notation, They 
can be recovered from thc order in which the nodes appear and the number of 
operands that the operator at a node expects. The recovery of edges is similar 
to the evatuation. using a stack, of an expression in postfix notation. See k c -  
tim 2.8 for mwe details and the relationship between postfix notation and 
code for a stack machine. 
Syntax trees for assignment statements are produced by the syntax-directed 
definition in Fig. 8.3; it is an extension of one in Section 5.2. Nonterminal S 
generates an assignment statement. The two binary operators + and * are 
examples of the full operator set in a typical language. Operator aswrriativi- 
ties and precedences are the usual ones. even though they have not been put 
into the grammar. This definition constructs the tree of Fig+ 8.2(a) from the 
input a ;= b *  - c + b* -c. 

SEC. 8.1 
1NTERMEOlATE LANGUAGES 465 
S t - i d  := E 
E + E l  + E2 
E -c E ,  + E* 
€ + - E l  
E - I  E ,  1 
E 4 id 
Fig. 8.3. Syntax+direckd definition to pr~dwcc sy nlax trcc's for assignment statcmcnts. 
This same syntax-directed definition will produce rhe dag in Fig. 8.2rb) if 
the functions mkunode (up. child) and mkndr(op, kJ, righr) return a pointer 
to an exisring node whenever possible, instead of constructing new nodes. 
The token id has an attribute piuw that points to the symbol-table entry for 
the identifier. In Section 8.3, we show how a symbol-table entry can be found 
from an attribu~e id.numc, representing the lexeme associated with that 
occurrence of id+ If the lexical analyzer holds all lexemes in a single array of 
characters, then attribute nume might be the index of the first character of the 
heme , 
Two representations of the syntax tree in Fig. 8.2(a) appear in Fig. 8.4. 
Each node is represented as a record with a field for its oprator and addi- 
tional fields for pointers to i(s children. In Fig 8.4(b), nudes are allocated 
from an array of records and the index or position of the node serves as the 
pinter to the node. All the nodes in the syntax tree can be visited by follow- 
ing winters, starting from the root at position 10. 
assign' 9 
H ' 
I^-.-_-r .--- 4 
Fig, 8.4. 'Two rcprcscntstjons of thc syntax trcc in Fig. $.?(a) 

466 LNTERMEDIATE CODE GENERATION 
Three-Address Cde 
Threeaddress code is a sequence of statements of the general form 
where x, y, and z are names, constants, or compiler-generated temporaries; 
op stands for any aperator, such ax a fixed- w floating-point arithmetic opera- 
tor, w a logical operalor on boolean-valued data. Note that no buill-up arith- 
metic expressions are permitted, as there is only one operator on the right side 
of a statement, Thus a source language expression like x +  y * z might be 
translated into a sequence 
where tl and t2 are compiler-generated temporary names. Tbis unraveling 
of cornplicared arithmetic expressions and of nested flow-of-control statements 
makes three-address code desirable for target code generation and opt imiza- 
tion. (See Chapters 10 and 12.) The use of names for the inrermediate values 
computed by a program allows three-address d
e
 
to be easily rearranged - 
unlike postfix notation. 
Three-address code is a linearized representation'of a synrax tree or a dag 
in which explicit names correspond to the interior nodes of the graph. The 
syntax tree and dag in Fig. 8.2 are represented by the three-address code 
sequences In Fig. 8.5. Variable names can appear directly in three-address 
statements, so Fig, 8.5(a) has no statements corresponding to the leaves in 
Fig. 8.4. 
(b) Cudc for thc dag, 
FLg, 8.5, Thrcc-address codc a,rrcspmding to thc trcc and dag in Fig. 8.2. 
The reason for the term "t hree-address code" is that each statement usually 
contains three addresses, two for the operands and one for the result. In the 
implementations of three-address code given later in this section, 
a 
programmer-defined name is replaced by a pointer to a symboi-table entry for 
that name. 

INTERMEDIATE LANGUAGES 467 
Types of ThreeAdderss S t a m t s  
Three-address statements are akin to axsembly d e .  Statements can have 
symbolic Iabels and there are statements for flaw of mtrd. A symblic label 
repremts the index of a three-address statement in the array holding inrer- 
mediate code. Actual indices can- be substituted for the labels either by mak- 
ing a separate pass, or by using "backpatching," discussed in Section 8.6. 
Here are the common three-address statements used in the remainder of this 
book : 
Assignment statements of the form x := y op Z, where op is a binary 
arithmetic or logical operation. 
Assignment instructions of the form x : = op y, where up is a unary 
opracion . Essential unary operat ions include unary minus. logical nega- 
h n ,  shift operators, and conversion operators that, for example, convert 
a fixed-point number to a floating-point number, 
Copy srurermwu of the form x : = y where the value of y is assigned to x. 
The unconditional jump goto L+ The three-address statement with label 
L is the next to be executed, 
Conditional jumps such as 
if x rdop y goto L, This instruction 
applies a relational operator ( <, =, 
=, etc,) to x and y, and executes the 
statement with label L next if x stands in relsiion relop to y. If not, the 
three-address statement following if x reiop y goto L is executed 
next, as in the usual sequence. 
garam x and call p, n for procedure calls and return y, where y 
representing a returned value is optional. Their typical use is as the 
sequence of three-address statements 
pararn xl 
pararn xz 
+
4
.
 
pa==' x,t 
call p, n 
generated as part of s call of the procedure p{ xl , x2, . . . , x,, ) . The 
integer n indicating the number of actual-parameters in ''call p , n" is 
not redundant because calls can be nested, The implementation of pro* 
cedure calls is outlined irt Seciion 8.7. 
Indexed assignments of the form x : = y [ i ]  and x[i] :=y. The f h t  of 
these sets x lo the value in the location d memory units beyond location 
y. The statement xli] :=y sets the contents of the location i units 
beyond x to the value of y. In both these instructions, x, y, and i 
refer 
to data objects. 
Address and minter assienments of the form x : = bv. x := ry. and 

468 
MTERMEDIATE CODE GENERATION 
XC+ 8.1 
*x := y. The first of these sets the value of x to be the location of y. 
Presumably y is a name, perhaps a temporary, that denotes an expression 
with an I-value such as A[i, j ] ,  and x is a pointer name or temporary. 
That is, the r-value of x is the I-value (location) of some object. In the 
statement x : = *y, presumably y is a pointer or a temporary whose r- 
value is a location. The r-value of x is made equal to the contents of ihat 
location. Finally, ux := y sets the r-value of the object pointed to by x 
to the r-value of y. 
r 
The choice of allowable operators i s  an important issue in the design of an 
intermediate form. The operator set must clearly be rich enough to imple- 
ment the operations in the source language. A small operator set is easier to 
implement on a new target machine. However, a restricted instruction set 
may force the front end to generate long sequences of statements for some 
source language operations. The optimizer and code generator may then have 
to work harder if g o d  code is to be generated. 
Syntax-Directed Tramlalion i n t ~  
Three-Address Code 
When three-address code is generated, temporary names are made up for the 
interior nodes of a syntax tree. The value of nonterminal E on the left side of 
E -c E l  +E?, will be computed into a new temporary :t, In general, the three- 
address wde for id : = E consists of code to evaluate E into some temporary 
t, followed by the assignment id.place : = t. If an expression i s  a single iden- 
tifier, say y, then y itself holds the value of the expression + For the moment, 
we creak a new name every time a temporary is needed; techniques for reus- 
ing temporaries are given in Section 8,3. 
The S-attributed definition in Fig. 8.6 generates three-address d
e
 for 
assignment statements. Given input a : b * - c + b + - C ,  it produces the 
code in Fig. X.S(a). 
The synthesized attribute $.u& represents the three- 
address code for the assignment S. The nonterminal E has two attributes: 
I .  
E.p/uc~, the name that will hold the value of E, and 
2, 
E.cnde. the sequence of three-address statements evaluaiing E. 
The function newtemp returns a sequence of distinct names t , t:, 
+ . . in 
response to successive calls. 
For convenience. we use the notation p n ( x  ':=' y '+' z) in Fig. 8.6 to 
represent the three-address statement x : = y + 2. 
Expressions appearing 
instead of variables like .r, y, and z are evaluated when passed to Ken, and 
quoted operakcus or operands, Like '+' , are taken literally. ln practice, three- 
addreus statements might be sent to an output file, rather than built up into 
the rock attributes, 
Flow-of-control statements can be added to the language of assignments in 
Fig. 8.6 by productions and semantic rules like the ones for while statements 
in Fig. 8.7. In the figure, the code for S - whik E do SI is generated using 
ncw attributes $.begin and S,ufwr to mark the first statement In the code for E 

SEC. 8.1 
INTERMEDtATE LANGUAGES 469 
- -  
. 
PRODUCTION I 
SEMANTIC RULES 
Fig. 8.6. Syn tax-directed dcfinitbn to producc thrcc-address c d c  for amignments. 
Fig. 8.7. Scmslnric rules generating d
c
 for a whilc starcmcnt , 

470 INTERMEDIATE CODE GENERATION 
XC. 8.1 
and the statement following the d e  for S, respectively. Tbese attributes 
represent labels created by a fundim newE&I 
that returns a new labtl every 
time it is called. Note that S+afrer becomes the labtl of the statement that 
comes after the code for the while stalernent. W e  assume that a non-zero 
expression represents true; thar is, when the value of E becomes zero, control 
leaves the while statement + 
Expressions that govern the flow of control may in general be boolean 
expressions containing relational and logical operators. The semantic rules for 
while statements in Section 8.6 differ from those in Fig. 8.7 to albw for flow 
of cantrol within bolean expressions. 
Postfix notation can be obtained by adapting the semantic rules in Fig. 8.6 
(or see Fig. 2.5). The postfix notation for an identifier is the identifier itself, 
The rules for the other productions concatenate only the operator after the 
d
e
 for the operands. For example, associated with the production E -. - E 1 
is the semantic rule 
In general, the intermediate form produced by the syntax-directed translations 
in this chapter can be changed by making similar modifications to the semantic 
rules. 
Implementatim of Three-Address Stattmnts 
A threaddress statement is an abstract form of intermediate a l e .  In a corn- 
piler, these statements can be implemented as records with fields for the 
operator and the operands. Three such ,representations are quadruples. tri- 
ples, and indirect triples, 
A quadruple is a record structure with four fields, which we call q ,  
arg 1 ,  
arg 2, and resrcir. The op field contains an internal code for the operator. The 
three-address statement x : = y op z is represented by placing y in org I, z 
in wg2, and x in result. Statements with unary operators like x : = -y or 
x : = y do not use arg2. Operators like param use neither arg2 nor result. 
Conditional and unconditional jumps put the target lake1 in rrsuk. The quad- 
rupks in Fig. 8.8(a) are for the assignment a : = b * - c + b * - c . They 
arc obtained from the three-address c d e  in Fig, 8.5(a). 
The contents of fields arg 1 ,  arg2, and result are narrnaily pointers to the 
symbol-table entries for [he names represented by these fields. If so, tern- 
purary names must be entered into the symbol tabk as they are created. 
To avoid entering temporary names into the symbol table. we might refer to a 
temporary value by the psitlon of the statement that computes it. If we do 

SEC. 8.1 
INTERMEDIATE LANGUAGES 47 1 
(a) Quadruples 
(b) Triplcs 
Fig. 8.8, Quadruple and triple reprcscn~a~ions 
of thrcc-addrcss statcmcnts. 
- 
(0) 
( 1 )  
(2) 
( 3) 
(4) 
( 5 )  
so. thrte-address statements can be represented by remrbs with only three 
fields: up, arg I and urg2, as in Fig, 8.8(b)+ The fields arg I and arlg 2, for 
the'aquments of up, are either pointers to the symbol tabk (for programmer- 
defined names or constants) or pointers into the triple structure (for tem- 
porary values). Since three fields are used, this intermediate code format is 
known as triples+' Except for the treatmen I of programmerdefined names. ~ r i -  
pks correspond to the representatim of a syntax tree or dag by an array of 
nodes, as in Fig. 8.4. 
Parenthesized numkrs represent pointers into the triple structure, while 
syrnbol-table pointers are represented by the names themselves. In prxt ice, 
the information needed to interpret the different kinds of entries in the urg 1 
and arg 2 fields can be encoded into the op field or some additional fields. 
The triples in Fig. 8,81b) mrrespcind to the quadruples in Fig. 8.8(a), Note 
that the copy statement a : = ts is encoded in the triple representation by 
placing a in the m g  l field and using the operator aaaign. 
A ternary operation like x [  i] : = y requires two entries in the triple fitrue 
ture, as shown in Fig. 8.9(a)+ while x := y[ i] i s  naturally represented us two 
operations in Fig. 8.%b). 
OP 
minus 
minus 
* 
+ 
: = 
nrg 1 
c 
b 
c 
b 
ti 
, t.5 
(a) x[i] := y 
' Ib) x := y[i] 
Fig, 8.9. More triple rcprcsenlat ions. 
urg2 
t I 
t 3 
t 
4 
' Some refer lo triples as "two-addrc~ d c . "  prcfcrring to idcnlify "quadruplcs" with thc tcrm 
"V~CC-address 
code, " Wc sha!l, howcvcr, trcat "t hwc-i+ddrc.;s adc" as an abaracl notion with 
various implcmcntations, iripks and quadruplcs king t hc principal ones. 

472 
INTER MEDIATE CODE GENERATION 
SEC. 8.1 
Another impiernentat ion of three-address code that has been considered is that 
of listing pointers to triples, rather than listing the triples themselves. This 
implementat ion is naturally called indirect triples, 
For example, let us use an array sraremrrt: 10 list pointers to triples in the 
desired order. Then the triples in Fig. &.8(b) might be represented as in Fig. 
8.10. 
Fig. 8.10. lndircct triplcs rcprcscnrarion of thrcc-addrcss statcmcnts. 
The differen- 
between triples and quadruples may be regarded as a matter of 
how much indirection is present in the representation. When we ultimately 
produce target code, each name. temporary or programmer-defined, will be 
assigned some run-time memory location. This location will be placed in the 
symbol-table entry for the datum. Using the quadruple notation, a three- 
address statement defining or using a temporary can immediately access the 
Iwation for that temporary via the symbol table. 
A more important benefit of quadruples appears in an optimizing compiler. 
where statements are often moved around. Using the quadruple notation, the 
symbol table interposes an extra degree of indirection between the computa- 
tion of a value and irs use. I f  we move a statement computing x, the state- 
ments using x require no change. However, in the triples notation, moving a 
statement that defines a temporary value requires us to change ail references 
to that statement in the arg 1 and urx2 arrays. This problem makes triples 
difficult to use in an optimizing compiler. 
Indirect triples present no such problem. A statement can k moved by 
reordering the s m m r n r  list. Since pointers to temporary values refer to the 
r~p-urg I 
2 arraycs), which are not changed, none of those pointers need be 
changed. Thus, indirect triples look very much like quadruples as far as their 
ulility ix concerned. The two notations require about the same amount of 
space and they are equally efficient for reordering of code. As with ordinary 
triples, allocation of storage to those temporaries needing it must be deferred 
to the code generation phase. However, indirect triples can save some space 

compared with quadruples if the same temporary value is used more than 
once. The reawn is that two or more cntries in the stufPmeni array can point 
to the samc h e  of the np-arg I-wg 2 structure. For example, lines ( 14) and 
16) of Fig, $+I0 could k combined and we cwld then combine T IS) and 
(171. 
8.2 DECLARATIONS 
As the scquena of declarations in a Q ~ X ~ U ~ C  
w block is examined, wc can 
lay out storage for names local to the procedure. For each local name, we 
create a symhd-table entry with infwmation like the type and t k  rchtive 
address of thc storage TOT the name. The rclativc address consists or an offset 
from the base of the static data area or the field for local data in an activation 
rccord. 
When the front cnd generates addrexxs, it may have a target rnachinc in 
mind, Suppose that addresses of conwcutive integers differ by 4 on a byte- 
addressable machine. Thc address calculations generated by the front cnd 
may therefore include multiplications by 4. The instruction set of the targct 
machine may alm favor ccrtain layouts of data objects, and hcnce thcir 
addresses. We ignore alignment of data o b ~ t s  
hew: Exampk 7.3 shows how 
data objects are aligncd by two compilers. 
The syntax of languages such as C, Pawal, and Fortran, allows all the declara- 
tions in a single procedure to k prmssed as a group. In this case, a global 
variabk, say oflset. can keep track of the next available relative address. 
In the translation >wheme of Fig. 8.1 1 nonterminal P generates a sequence 
of declarations of the form id ; T+ Before the first declaration is considered, 
ofi~ei is set to 0. As each new name is seen, that name i s  entered in the xym- 
hol table with offset equal to the current value of oflsrr, and a%)'sct is: incrc- 
mented by the width of the data abject dcnotcd by that name. 
The prwxdure mrtv (name, rypt. @sut) 
creates a symbol-tahlc entry for 
name, gives it type type and relative address osfsa in its data area. We use 
synthesized attributes ryp and width for nonterminal T to indicate the type 
and width. or number of memory units taken by objccts or that type. Attri- 
bute t'yp represents a type expression wnstruded from the basic types integer 
and real by applying the type constructors poinrtr and wry, as in Section 6.1. 
1I type expressions are represented by graphs, then attribute type might be a 
pointer to the n d e  representing a type expression. 
In Fig. 8-11, integers have width 4 and reals have width 8. The width of an 
array is obtained by multiplying the width or each element by the number OF 
elements in the array.' Thc width of each pointer is assurncd to h 4. In 
. - - . . . - 
: For arrays whoo hwer bound is not 0. the calculaiion uf addrew for array ckrncnts is simpli- 
lied if rht o f k r  er~tertd inw the symkl table is aJ$sleJ as discusd in %don 8.3. 

474 
INTERMEDIATE CODE GENERATION 
T -. integer 
T.ryp := inr~ger; 
T+ width : = 4 } 
Fk. 8.11. Computing thc types and rclativc ddrcsscs of declarcd names. 
Pascal and C. a pinier may be seen before we learn the type of the object 
pointed to (see the discussion of recursive types in Section 6.3). Storage allo- 
cation for such types is simpler if all pointers have the same width. 
The initialization of oflret in the translation scheme of Fig. 8.i 1 is more evi- 
dent if the first prduction appars on one line as: 
Nmterminah generating r, called marker nonterminals in Section 5.6. can be 
uwd to rewrite productions so that all actions appear at the ends of right 
sides. Using a marker nanterminal M, (8.2) can be restated as: 
Keeping Track of %ope Imformatbn 
In a language with nested procedures, names local to each procedure can be 
assigned relative addresses using the approach of Fig. 8.1 1. When a nested 
procedure is seen, processing of declarations in the enclosing prcmdure i s  
temporarily suspended. This approach will be illustrated by adding semantic 
rules to the following language, 

SEC. 8.2 
DECLARATIONS 475 
The productions for itonterminals S for statements and T for types are not 
shown because we focus on declarations. The nonterminal T has synthesized 
attributes rype and width, as in the translation scheme ,of Fig, 8,lJ. 
For simplicity, suppose that there is a separate symbol table for each pro- 
cedure in the language (8.3). One possible Implementation of a symbol table 
is a linked list of entries fur names. Clever implementations a n  be substi- 
tuted if desired. 
A 
new 
symbol 
table 
is created 
when 
a 
procedure 
declaration 
D - pcrw: Id D I ; S is seen, and entries for the declarations in D l  are created 
in the new tabk. The new table points back lo the symbol table of the enclos- 
ing procedure; the name represented by id itself is local to the enclosing pro- 
cedure. The only change from the treatment of variable declarations in Fig. 
8.11 is that the procedure eater is told which symbol table to make an entry 
in. 
For example, symbal tables for five procedures are shown in Fig. 8. t 2. The 
nesting structure of the procedures can be deduced from the links between the 
symbol tables; the program is in Fig. 7.22. The symbol tables for procedures 
readarray, exchange, and quicksort pint back to that for the contain- 
ing procedure sort, consisting of the entire program. Since partition is 
declared within quicksort, its tabk points to that of quicksort + 
sort 
I readarray I 
to rradar 
partition 
heuder 
Fig. 8-12. Symbol tables for nested prmxdures. 

476 
INTEUM!:.CIIATECODEGENERATII)N 
SEC. 8.2 
The semantic rules arc defined in terms of the following operations; 
I .  
rnktuM~fi(prc.vio~s) 
creates a new symbd table and returns a pointer to the 
new table. The arguinent pr&~u.s. p i n t s  to a previously created symbol 
table. presumably t h a ~  for the enclosing procedure, The pointer previous 
is placed in a header fur the new syrnbul table, along with additional 
information such as rhe nesting depth of a procedure. We can also 
number the procedures in the order they are declared and keep this 
number in the header. 
2. 
cnftr{tuhk, w m e ,  typtj, q[r.i-~jr) 
creates a new entry for name name in the 
symbol table pointed ED by fuhk. Again, entpr places type type and rela- 
tive address t?fi~w 
in fields within the entry. 
3, 
addwidfh(tublr~, width) recwds the curnuhtiue width of all the entries in 
ruBIc in the header associated with this symbol table. 
4. 
cwcrprrsr.(tdd~~, 
numu, nr~wrublc) creares a new entry for procedure name 
in the symhl tabk pointed tu by rrtblr. The argument ncwtubFc points to 
the sy mbd table fur this procedure n u m +  
The translation scheme in Fig. 8.1 3 shows how data can be laid wt in one 
pass, using a stack rb@r to hold pointers ta symbol tables of the endosing 
procedures. With rhe symbol tables of Fig. 8.12. tb&r 
will contain poinrtrs 
to the tables for sort, quicksort. and partition when the declarations 
in p a r t i t i o n  are considered. The pointer to the current symbol table is on 
top. The other stack c!flsc*f 
is the natural generalization to nested procedures 
of attribute @SCI 
in Fig. 13.1 1 .  The top element of r~fivt~r is the next available 
relative address for a local of the current procedure. 
All scmantic act ions in the subtrces far B and C in 
are done before rrr.tiorrA at the end of the produciion occurs, 
Hence, the 
action associated with the marker M in Fig. 8.13 is the first to be done. 
The action for nontern~inal M initializes stack rblpir with a symbol table for 
the outermost scope, created by operation mkwbk (nil), The action also 
pushes relative address 0 onto stack { f i e f .  The nmterminal N plays a similar 
role when a procedure declaration appears, Its action uses the operation 
* mkrab/t>(tr~p{tbip~r}) 
to create a new symbol table. Here, the argument 
: r ~ ( d d p r r )  gives the enclosing scope of the new table. A pointer to the new 
table is pushed above that for the enclosing scope, Again. 0 is pushed onto 
(met. 
For each variable declaration id : T, an entry Is created for id in the current 
symbd table. This declaration leaves the stack rhlprr unchanged: rhe kip of 
stack t ~ f l w i  is incremented by T. width. When the action on the right side of 
D + prac id ; N D I ; S occurs, the width of all declarations generared by D I 
is on top of stack rffsc~; it is recorded using uddwidfh. Stacks rblpfr and q$wr 

SEC. 8+2 
DECLARATIONS 477 
{ i : = t n k f d d ~ ~  
[mp tb@r 1) : 
push (l. tbdpir); psA (0, vjjkt) ) 
Fig. 8.13, Prrsccssing dcclrrrihns in ncsrcd prcacdurcs. 
are then popped. and we revert to examining the declarations in the enclosing 
p r o ~ d u r e .  At this point, rhe name of the enclosed procedure is entered inio 
the symbd table of its enclosing procedure. 
F i d  Names in R&s 
The following production allows rimterminal T to generate records in addition 
10 basic types, pointers, and arrays: 
The actions in the translation scheme of Fig. 8.14 emphasize the similarity 
betwmn the layout of records as a language constr'uct and activation records. 
Since procedure definitions do not affect the width computations in Fig. 8.13, 
we overlimk the fact that the above production also allows procedure defini- 
tions to appear within records. 
Fig. 8-14. Sctting up a symbol tablc fur ficid narncs in a record. 
After the keyword record is seen. the action associated with the marker L 

478 
INTERMEDIATE CODE GENERATlON 
SEC. 8.2 
creates a new symbol table for the field names. A pointer to this symbol table 
is pushed onto stack tblp~r and relative address 0 is pushed onto stack oflstr. 
The action for D - id : T in Fig. 8+13 therefore enters information about the 
field name id into the symbol table for the record. Furthermore, the top of 
stack oflsrr will hold the width of a l l  the data objects within the record after 
the fields have been examined, The action following end in Fig. 8,14 returns 
this width as synthesized attribute T.willrh. The Iype T+type is obtained by 
appiying the constructor record to the pointer to the symbol table for this 
record. This pointer will be used in the next section to recover the names, 
types, and widths of the fields in the record from T+typf. 
8.3 ASSIGNMENT STATEMENTS 
Expressions can be of type Integer, real, array, and record in this section, As 
part of the translation of assignments into three-address code, we show how 
names a n  be looked up in the symbol table and how elements of arrays and 
records can be accessed. 
Names in the Symbol Tsbk 
In Section 8.1 , we formed ~hree-address statements using names themselves, 
with the understanding h a t  the names stood for pointers to their symbol-table 
entries. The translation scheme in Fig. 8+ 15 shows how such symbol-table 
entries can be found. The h e m e  for the name represented by id is given by 
attribute id+numu. Operation /wkup(id.nurn~) checks if here is an entry for 
this occurrence of the name in the symbol table. If so, a pointer to the entry 
is returned; otherwise, lookup returns nil 10 indicate that no entry was found. 
The semantic actions in Fig. 8.15 use procedure emir to emit three-address 
statements lo an output file, rather than building up code attributes for nonter- 
minals, as in Fig. 8.6. From Section 2.3, translation can be done by emitting 
to an output file if the code attributes of the nonterminals on the left sides of 
productions are formed by concatenating the code attributes of the nmterrni- 
nals on the right, in the same order that the nonterminals appear on the right 
side, perhaps with some additional strings in between. 
By reinterpreting the Imkup operation in Fig. 8.15, the translation scheme 
can be used even if the most closely nested scope rule applies to nonlwal 
names, as in Pascal. For concreteness. suppose that the context in which an 
assignment appears is given by the following grammar. 
Nanterrninal P becomes the new start -symbol when these productions are 

ASSIGNMENT STATEMENTS 479 
Fig. 8,lS. Tranda tion schcmc to prrducc t hrcc-uddrw wdc for assignments. 
added to those in Fig. 8.15. 
For each procedure generated by this grammar, the translation scheme in 
Fig, S .  13 sets up a separate symbol table. Each such symbl rabk has a 
header containing a pointer to the table for the enclosing procedure. ( k e  Fig. 
8.12 for an example.) 
When the statement forming a procedure body is 
examined, a poin'ter to the symbol table for the procedure appears on top of 
the stack rblprr. This pointer is pushed onto the stack by a c t h s  associated 
with the marker nonkrminal N on the right side of D + p m  id ; N D  I ; S. 
Let the productions for nonierminal S be those in Fig. 8.15. Names in an 
assignment generated by S must have ken declared in either the procedure 
that S appears in, or in some enclosing procedure. When applied to name, the 
modified I w h p  operation first checks if mm appears in the current symbol 
table, accessible through rup(tb?'rr). If no?, lookup uses the pointer in the 
header of a table to find the symbol table for the enclosing procedure and 
ids for the name there. lf the name cannot be found in any of these s o p , ,  
then fmkup returns nil. 
For example. suppose that .the symbol tables arc as in Fig. 8+12 and that an 
assignment in the M
y
 of procedure partition is king examined. Opera- 
tion io&p(i) will find an entry in the symbol table for partition. Since v 
is not in this symbol table, fuokrrp(v) will use the pointer in (he header in this 
symbol table to continue the search in the symbol table for the enclosing pro- 
cedure quicksort. 

480 
LNTERMEDIATE CODE GENERATION 
Reusing Temporary N a m  
We have been going along assuming that newtemp generates a new temporary 
name each time a temporary is needed. It is useful, eapecia'lly in optimizing 
compikrs, to actually create a distinct name each time newtemp is called; 
Chapter 10 gives justification for doing so. However, the temporaries used to 
hold intermediate values in expression calculations tend to clutter up the sym- 
bol table, and space has to be allocated to hold their values. 
Temporaries can be reused by changing newtemp. An akrnative approach 
of packing distinct ternpxaries into the same h a t  ion during code generation 
i s  explored in the next chapter. 
The bulk of temporaries denoting data are generaied during the synlax- 
directed translation of expressions, by rules such as those in Fig. 8.15. The 
cude generated by the ruks for E - E + E l  has the general form: 
evaluate E into t, 
evaluate Ez into t2 
t := t, + tZ 
From the rules for the synthesized artributt E.pbw it follows that t, and t2 
are not used elsewhere in the program. The lifetimes of these temporaries arc 
nested like matching pairs of balanced parentheses. In fact, the lifetimes of 
all temporaries used In the evaluation of E2 are contained in the lifetime of 
tl. I t  is therefore possible to modify ncwtemp so that it u m ,  as if it wwe a 
stack, a small array in a prtxedure's data area to hold ternparits. 
Let us assume for simplicity that we are dealing only with integers. Keep a 
count c, initialized to zero. 
Whenever a temporary name is used as an 
operand, decrement c by I .  Whenever a new temporary name is generated, 
use Sc and increase c by 1. 
Note that the "stack" of temporaries is n a  
pushed or popped at run time, although it happens that stores and Joads of 
temporary values are made by the compiler to occur at the "top." 
Example 8,l. Consider the assignment 
Figure 8.16 shows the sequence of thret-address statements that would be 
generated by semantic rules in Fig. 8-15, if newtemp were modified. The fig- 
ure also contains an indication of the "current" value of c after the generation 
of each statement. Note that when we compute $0 - $1, c is decremented to 
zero, so $0 is again available to hold the result. 
0 
Ternporarits that may be assigned and/br used more than once, for exam- 
ple, in a conditional assignment, cannot be assigned names in the last-in first- 
out manner described above. Since they tend to be rare, all such temporary 
values can k a~signed names of their own. The same problem of temporaries 
defined or used more than once occurs when we perform code optimization 
such as combining common subexpressions or m o v i ~ g  a computation out of a 
Imp (xee Chapter 101. A reasonable strategy i s  to create a ncw name 

ASSlGWMEHT STATEMENTS 48 1 
nATEMENT 
I VALUE OF r' 
Fig, 8b16. Thrcc-address code with stacked trsrnporarics. 
whenever we create an additional definition or use for a temporary or move 
its cornputat ion. 
Addressing A m y  Elements 
Elements of an array can be accessed quickly if the elements are stored in a 
block of consecutive locations. If the width of each array element is w, then 
the ith element of array A begins in location 
where low is the lower bound on the subscript and h s c  is the relative adQrexs 
of the storage allocated for the array. That is, bust is the relative address of 
A[iow]. 
The expression (8.4) can be partiaily evaluated at' compile rime if it is 
rewritten as 
The sutxxpression c = h e  - /OW X w can be evaluated when the declaration 
of the array is seen. We assume that th is saved in the symbol table entry for 
A, so the relative address of A[i] is obtained by simply adding i X w to r+. 
Compile-time prwalculat ion can a h  lx applied to address calculations of 
elements of multi-dimensional arrays. A two-dimensional array is normally 
stored in one of two forms, either row-mjor (row-by-row) or colum-mujor 
(mtumfi-by-column). Figure 8.17 shows the layout of a 2x3 array A in (a) 
row-major f
~
~
m
 
and (b) column-major form. 
Fortran uses ~0hmn-mSt~0r 
form; Pascal uses row-major form, because ~ [ i ,  
j] is equivalent to 
A[ i l I 
j I ,  and the elements of each array ~ [ i  
'] are stored consecutively- 
In the case of a twodimensional array stored in row-major form, the reh- 
tiw address of A[iI , iz 1 can be calculated by the formula 
where low and b w ,  are the lower bounds on the values uf i 1 and i2 and nz 
is the number of values rhat i2 
can take. That is, if high2 is the upper b n d  

482 
INTERMEDIATE CODE GENERATKIN 
T 
First row t 
Second row 
1 
T 
First column 
Second column 
Third coturnn 
1 
(a) ROWMAJOR 
(a) COLUMN-MAJOR 
Fig. 8.17. Layouts for a two-dimensional array. 
on the value of i 2 ,  then rrl = high2 - lowz + I .  Assuming that i l  and i2 are 
the only values that are not known at compile time, we can rewrite the above 
expression as 
The last term in this expression can he determined at compile time. 
We can generalize row- or column-major form to many dimensims, The 
generalization of row-major form is to store the elements in such a way that, 
as we scan dawn a block of storage, the riihtmost subscripts appear to vary 
fasresr, like the numbers on an odometer. The expression (8.5) generalizes to 
the following expression for the relative address of A[ i , , i2, 
+ . . , i, 1 
Since for all j, ni = high, - lowj + 1 is assumed fixed, the term an the second 
line of (8.61 can be computed by the compiler and saved with the syrnkt-table 
entry for A,' Column-major form generalizes to the opposite arrangement, 
with the leftmost subscripts varying fastest + 
Some languages permit Qe sizes of arrays to be specified dynamimlly, when 
s prucedure is called at run-time. The rilmation of such arrays on a run-time 
stack was considered in Section 7.3. The formulas for accessing the elements 
of such arrays are the same as for fixed-size arrays, but the upptr and lower 
limits are not known at compile time. 
I In C, a multi-dimensional array is simulated by defining arrays whos elements are arrays. For 
example. sup- 
x is an array of arrays of integers. Then, the language alhws both xt i l and 
XI i l  I j 1 to be writlen, and the widths of thee expressions are difftrent. However, rhc lower 
h n d  of all arrays is 0. rrr the term on tht second lint of (8.6) simplifies to base in each case. 

The chief problem in generating code for array references is to relate the 
mmputation of (8.6) 
to a grammar for array references. Array references can 
be permitted in assignments if nonterrninal L with the following productions is 
albwed where id appears in Fig. 8.15: 
In order that the various dimensional limits fi, of the array be available as we 
group index e x p r e ~ ~ k m  
into an E k ,  it is useful to rewrite the productions as 
That is, the array name is attached to the leftmost index expression rather 
than being pined to Elisr when an L is formed. These prductions allow a 
pointer to the symbl-table entry for the array name to be passed as a syn- 
thesized attribute urruy of ~lisr,' 
We also use ECisr.nrlim to record the number of dimensions (index expres- 
sions) in the Ebt. The Function htir(crrruy, j) returns tt,, the number of ele- 
ments along the jth dimension of the array whose symbol-table entry is 
pointed to by arruy. Finally, Elis;.pluce ddendcs the temporary holding a 
value computed from index expressions in E ~ I .  
An Elisr that produces the first m indices of a k-dimensional array reference 
A { i  I , i 2  , - . . , iA 
1 will generate three-address ccode to compute 
using the recurrence 
Thus, when m = k, a multiplic.ation by the width w is all that will be needed 
to compute the term on the first line of (8.6). Note that the i,'s 
here may 
really be values of expressions, and code to evaluate those expresulons will be 
interspersed with code to &rnpute (8.7). 
An /-value L will have two attributes, L,p!ucu and L . ~ j p r .  
in the case that 
L is a simple name, L.pr!ar:u will be a pointer to the symbi-table entry for that 
name, and L.oflse? will be null, indicating that the h a h e  is a simple name 
rather than an array reference. The rimterminal E has the same translation 
E.pl'ucbu, with the same meaning as in Fig 8.15. 
' T ~ C  
rransfiwmation is ~irnilpr to r>nc wnrioficd at thr: c ~ l l  
of Section 5.6 fur diminiithg inhrrrir- 
cd uttribulcs. Hcrc ttm, wc cwkl have ?;rrlv~J 
thc prrjblcm with inhcritd olttriburcs. 

484 
INTERMEDIATE CODE GENERAWON 
The Translation Scheme for Addressing Army Elements 
Semantic actions will be added to the grammar: 
As in the case of expressions without array references, the three-address code 
itself is produwd by the emir p r d u r e  invoked in the semantic actions. 
We generate a normal assignment if L is a simple name, and an indexed 
assignment into the location denoted by L otherwise: 
The d e  for arithmetic expressions is exactly the same' as in Fig. 8. IS: 
When an array reference L is reduced to E, we want the r-value of L. There- 
fore we use indexing to obtain the contents of the location L.place [L.ufser]: 
(4) 
E -. L 
{ if L.o$wt = nui then /* L is a simple M */ 
E .place : = L .place 
dw begin 
E.place := newrmp; 
emit(E.phm ' : 
=' Lplace'f 'L.oflser' ] ') 
end ) 
Below, L-ofset is a new temporary representing the first term of (8.6); func- 
tion width (Elist.urroy) returns w in 18,6). Lplace represents the second term 
of (8.6), returned by the function c (Eiisr.array]. 

A null offset indicates a simple name. 
(6) 
L - i d  
{ L+pluce :- id.pluce; 
L.o@et : = null } 
When the next index expression is seen, we apply the recurrence (8.8). ln the 
following action, Elis! 1 .phce d~rresp0IId~ 
to em - in (8.8) and EZisr,pl~ce to 
em. Note that if Ells:, has m - 1 components. then Elist on the left side of the 
production has m components. 
E.pbce holds both the value of the expression E and the value of (8.7) for 
m = l .  
Example 8.2, Let A be a 10x20 array with !owl = low2 = I. Therefore, 
n 
= 10 and nl = 20. Take w to be 4, An annotated parse tree Tor the 
assignment x : = A[ y , z ] is shown in Fig, 8,18, The assignment is translated 
into the follow& sy uence of three-address statements: 
t, := y * 20 
. 
t, := tl + Z 
t2 :=I' 
/* constant r = base, - 84 */ 
tj := 4 * tl 
t 4  := t2[t31 
X := t4 
For each variable, we have used irs name in placc of idplace. 
n 
T y p  Conversions within 
-In practice, thcre would be many different types of Variables and constants, so 
the compiler must either reject certain mixed-type operations or generate 
appropriate coercion (type conversion) instructions. 
Consider the grammar for assignment statements as above, ' bat S
U
~
~
X
 
there are two types - 
real a d  integer, wiih integers converted to reals when 

486 
INTERMED1 ATE CODE GENERATION 
Fig. 8.1%. Annotated parse tree for x : = A [ y  , z]. 
necessary. We introduce another attribute Ewe, whose value is either red 
or integer, The semantic ruk for E.rype associated with the P
~
~
U
U
~
Q
~
E + E  + E is: 
This rule is in the spirit of Section 6.4; however, here and elsewhere in this 
chapter, we omit the checks for type errors; a discussion of type checking 
appears in Chapter 6. 
The entire semantic rule for E -. E + E and most of the other productions 
must be modified to generate, when necessary, three-address statements of the 
form x : * inttorealy, whose effect is to convert integer y to a real of 
equal value, called x. We must also include with  he operator d
e
 
an indica- 
tion of whether fixed- or floating-point arithmetic is intended. The complete 
semantic action for a production of the form E + E 
+ E 2  is listed in Fig. 
8.19- 

Fig. 8.19, Scmantic adon for E - El + E l .  
For example, for the input 
assuming x and y have type r~al, and i and j have type itrteger. the output 
would look like 
tl := .i intw j 
t$ := inttoreal t, 
t2 := y xeal+ t3 
X := t2 
The semantic action of Fig. 8.19 u
~
s
 
two attributes E.pIarc and €.type for 
the nonterrninal E. As the number of types subject to anversion increases, 
the number of cases that arise increases quadratically (or worse, if [here are 
operators with more than iwo arguments). T'herehre with large numbers of 
types, careful organizalion of the semantic actions becomes more imprrant. 

488 
INTERMEDIATE CODE GENERATION 
Accessing Fidds in Records 
The compiler musl keep track of both the types and relative addresses of the 
fields of a record+ An advantage of keeping this information in symbol-table 
entries for the field names is that the routine for iooking up names in the sym- 
bol table can also be used for field names. With this in mind. a separate sym- 
bol table was created for each record rype by the semantic actions in Fig. 8.14 
in the last section. If r is a pointer ta the symbol table for a record type, then 
the type recurb(r) formed by applying the constructor record to the pointer 
was returned as T.ryp. 
We use the expression 
to illustrate how a pointer 10 the symbol table can be extracted from an attri- 
bu te E.rypc. From the operat ions in this expression it follows that p must be a 
winter to a record with a field name info whose typc is arithmetic* If types 
are constructed as in Fig. 8.13 and 8.14, the type of p must be given by a type 
expression 
The type of pt is then rewr'rl(r), from which I can be extracted, The field 
name info is looked up in the symbol table pointed to by t. 
8.4 BOOLEAN EXPRESSIONS 
In prugramming languages, boolean expressions have two primary purposes. 
They arc used to compute logical values, but more often they are used as con- 
ditional expressions in statements that alter the flow of control, such as if- 
then, if-then-else, or while-do statements. 
Boolean expressions are composed of the bolean operators (and, or. and 
not) applied to dements [hat are boolean variables or relational expressions. 
In turn, relational expressions are of the form E relop E l ,  where E and E l  
arc arithmetic expressions. b m e  languages, such as PLII, allow more general 
expressions, where b ~ k a n ,  arithmetic, and re jar iunal operators can be 
applied to cxpressio~s of any type whatever, with no distinction between 
boolean and arithmetic values; a coercion is performed when necessary, In 
this section, we considcr boolean expressions generated by [he following gram- 
mar: 
E 1 E w E I E and E 1 not E I E I 
( id relop id true' Mse 
We usc the attribute rsp to detcrmine which of the comparison operators 
<, 5 ,  =, +, >, or 1 is represented by relop. As is customary, we assume 
that or and and are left-associative, and that or has lowest precedence, then 
and, then net, 

Methods of Translatifig B W n  Expressions 
There are two principal methods of representing the value of a blean 
expression. The first method is to encode true and false numerically and to 
evaluate a boolean expression anabgously to an arithmetic expression. Often 
1 is used to denote true and O to denote false, although many other endings 
are also possible. For example, we could let any nonxrp quantity denote true 
and zero denote f a k ,  or we muld let any nonnegative quantity denote true 
and any negative number denote false. 
The second principal method of implementing boplean expressions is by 
flow of control, that is, representing the value of a boolean expression by a 
position reached in a program. This method is 
convenient in 
implementing the bookan expressions in flow-of-control statements, such as 
the if-then and whiledo statements. 
For example, given the expression 
E l  wE2, if we determine that E l  i s  true, then we can conclude that the enlire 
expression is true without having to evaluate E 2 .  
The semantics of the programming language determines whether all parts of 
a boolean expression must be evaluated. If the language definition permits (or 
requires) portions af a boolean expression to go unevaluated, then the com- 
piler can optimize the evaluation of boolean exprassions by computing only 
enough of an expression to determine its value, Thus, in an expression such 
as E o r E 2 ,  neither E , nor E 2  is necessarily evaluated fully. If either E or 
E 2  is an expression with side effects (e.g., contains a function that changes a 
gbbal variable), then an unexpected answer may be obtained. 
Neither of the above methods is uniformly superior to the other. For exam- 
ple, the BLISS1 1 optimizing compiler (Wulf et al, 1975)+ among others, 
chooses the appropriate method for each expression individually. This section 
considers b t h  methods for the translation of bmlean expressions to three- 
address d e .  
k t  us first consider the implementation of boolean expressions using 1 to 
denote true and 0 to denote false. Expressions will be evaluated mmpletdy, 
from Left to right, in a mannet similar to arithmetic expressions. For exam- 
ple, the translation for 
a or b and not c 
i s  the three-address sequence 
t1 := not c 
t2 := b and tl 
t3 := a or t2 
A relational expression such as a < b is equivalent to [he conditional state- 
ment if a < b then 7 t h e  0, which can k translated into the thrce- 
address d
c
 
sequence (again, we arbitrarily start statement numbs at 100): 

490 
1NTERMEUlATE CODE GENERATION 
100: 
if a 
b goto 103 
101: 
t := 0 
102: 
goto 104 
103: 
t := ? 
104: 
A translation scheme for producing three-address code for boolean expres- 
sbns i s  shown in Fig. 8.20. In this scheme, we assume that mi! 
places three- 
oddrcss statement.s into an output file in the right format, lhat n e x ~ ~ u t  
gives 
the index of the next three-address statement in the output sequence, and that 
emir increments ncxtsku after producing each t hree-address statement. 
E - true 
E -- false 
Fb. 8,20. Translation sc+cmc using a nurncrical rcprcscnt;rt ion fur bwlcans. 
Example 8.3. The scheme in Fig. 8.20 would generate the lhree-address code 
in Fig. 8.21 for the expression a < b or c < d and e ,  
< f .  
0 
Short-Circuit Code 
We can also translate i~ 
boolean expression into three-address c d e  without 
generaling code for any of the boolean operators and without having the code 
necessarily cvalunte thc entire expression. This stylc of evaluation is wme- 
times called "short-circuit" or "'jumping" code. It is possible to evaluate 
&lean 
expressions without generating code For thc boolean operators and, 
or, and not if we represent the value of an expression by a position in the 
code scqucnce. FOT cxample, in Fig. 8.21. wlic can tell what value tI will 

SEC. 8,4 
BOOLEAN EXPRESSIONS 491 
100: if a < b gota 703 
107: t2 := 1 
\Ol: t, : = O  
108: if e < f goto 11 1 
102: goto 104 
105): t, :z 0 
103: 
t, := 1 
I 
g ~ t o  
1?2 
104: if c < d goto 107 
I 
tJ := 
7 
105: 
tz : =  0 
112: 
:= tz and tl 
106. goto 108 
113: t5 ; =  t, or t4 
Fig. 8-21. Translation of a < b or c < d  and t f. 
have by whether we reach statement 101 or statement 103, so the value of t, 
is redundant. For many boolean expressions, it is possible to determine the 
value of the expression without having to evaluate it completeiy. 
We now cunsiber the trandation of boolean expressions into three-address 
code in the mntext of if-then, if-then-else, and while-do statements such as 
those gtneratd by the following grammar: 
In each of t h m  prductions, E is the bolean expression to be translated. In 
the translation. we assume that st three-address statement can be symblicaBy 
labeled, and that the function newlabd returns a new symbolic label each time 
it is calkd . 
With a boolean expression E. we associate two labels: E.rrw, the lab1 to 
which control flows if E is true, and E.fadse, the label to which control flows 
if E is false, The seman~ic rules for translating a flow-of-control statemtnt S 
allow control to flow from the translation S.rude to the three-address instruc- 
tion immediate1 y folbwing S.cde. In some cases, t he instruction immediate1 y 
following S,cobe i s  a jump to some label L. A jump to a jump to L from 
within $.code is avoided using an inherired attribute S,nex!. 
Tht value of 
S . ~ x t  
is a label that is attached to the first threeaddress instruction to be exe- 
cuted after the code for s . ~  
The initialization of S m x r  is not shown. 
In translating Ihe if-then statement S -. if E tbn S 1 ,  a new labd E . r w  is 
created and attached to the first three-address instruct bn generated for the 
statement 
as in Fig, 8.22(a). A syntax-directed definition appears in Fig. 
8.23. The d
e
 
for E generates a jump to E.true if E is true and a jump to 
$+next if E is false. We therefore set E.fah to S.nexr+ 
' Lf implemented lliterally, rhc approach d inheriting a label $.next can lead tu a proliferation of 
hhls. The backpa~chiag apptollch of $miom 8.6 creates la&b only when they are need&. 

(c) whilc-do 
Fig. 8.22. C d c  f o r  if-thcn. if-thcn-ctsc. and while-do statcmcnts. 
In translating the if-then-etse statement S + if E then S 1  else S2. the d
e
 
for the boolean expression E has jumps out of it to the first instruction of the 
code for S I  if E is true, and to the first instruction of the code for S 2  if E is 
false, as ilhmated in Fig. 8.22(b)+ As with the if-then statement. an inher- 
ited attribute Smxr gives the label of the three-address instruction to be exe- 
cuted next after executing the code for S, An explicit goto S . t ~ i  
appears 
after the code for S , ,  but no! after S t ,  We leave it to the reader to show 
that, with these semantic rules, if S.nm is not the label of the instruction 
immediately following Sz.rbr&, 
then an enclosing statement will supply the 
jump to label S+nm a f w  the mde for S2. 
The code for S - while E do S is formed as shown in Fig. 8.22Ic). A new 
l a k l  S.br~gin i s  created and attached to the first insiruction generated for E. 
Another new label E.frw Is a~tached to the first instruction for SI 
+ The cde 
for E generates n jump to this label if E is true. a jump to S.ncxr if E: is false; 
again, we set E,.jidst to be S.ncxi. After the code for S 1  we place the instruc- 
tion goto S.bqin, which causes a jump back to the beginning of the code for 
the boolean expression, Note that S l  .nexf is set to this label X b c ~ i n ,  so jumps 
from within S .c.tdt can go directly to S+ 
begin. 

SEC. 8.4 
BOOLEAN EXPRESSIONS 493 
.C - it E then S ,  
S -, if E then S ,  elm SI 
S - whlk E d o S ,  
Fig, 8.23. Synta x-directcd de~inition for flow-of-control statements. 
We discuss the translation of flownf-control statements in more detail in 
Sect ion 8.6 where an alternative mct h d ,  callcd "badrpatching," emits cnde 
for such statements in m c  pw. 
Control-Flow Tmnslath of Boo1mn Expressions 
We now discuss E.corle. the cude produced for the h k a n  expressions E in 
Fig, 8.23, As we have indicated, E is translated into a sequence o l  three- 
address statements that cvaluatcs E as a sequence of conditional and uncondi- 
tional jumps to one of two locations: E,iru~, 
the placc control is to reach if E 
is true, and E.fdse, thc place control is to reach if E is false. 
The bask idca bchind the translation is the fdlowing. Suppuse E is of the 
form a < b+ Then the gencratcd code is of the Tmm 
Suppose E is of the form El or E2. If El is true, then wc immediately 
know that E itsdf i s  truc, so El + m e  is the same as E,true. IT E l  is falw, then 

494 
IHTERMEDlATLl CODE GENERATION 
SEC. 8.4 
E2 must be evaluated, so we make El . j d s t  be the label of the first siatement 
in the code for E 2 .  The true and false exits of E2 can tK made the same as 
the true and false exits of E, respectively. 
Analogous considerations apply to the translation of El aad E2. No c d e  is 
needed for an expression E of rhe form mot E l :  We just interchange the true 
and false exits of E ,  to get the true and false exits of E. A syntax-directed 
definition that generates three-address code for boolean expressions in this 
manner is shown in Fig. 3.24. Note that the rrw and Jolse attributes are 
inherited. 
E 
15, and Ez 
E.c-de := p n  ('if' Ma .phct dop.rjp id,.plrw 'goto' E.rrtre) 
#m('goto' E.jidM) 
Fig. 8.24, SyMax-directed definition to produrn thrcc-addrcss c b c  for bwkans. 
Example 8.4. Let us again consider the expression 
Sup- 
the true and false exits for the entire enpression have k e n  st to 
Ltrue and Lfalse. Then using the definition in Fig. 8.24 we would obtain 
the following d e :  

BOOLEAN EXPRESSIONS 495 
if a < b goto Ltrue 
goto Lt 
L1: if c < d goto L2 
goto Lfalse 
L2: if e < f goto Ltrue 
goto Lfalse 
Note that the code generated is not optimal, in [hat the sec.ond statement 
can be eliminated without changing the value of the code. Redundant instruc- 
tions OF this form can be subsequently removed by a simple peephole optim- 
izer (see Chapter 9). Another approach that avoids generating these redun- 
dant jumps is to translate a relalional expression of the form idl < idZ into 
the statement if idl 2 id2 goto E f u h  with the presumption that when the 
relation is true we fall through the code, 
o 
Example 8 5  Consider the statement 
while a 
b do 
if 
c 4 d then 
x : = y + e  
else 
x : = y - Z  
The syntax-dircctcd definition above, mupled with schemes for assignment 
statements and h k m  expressions, would produce the following code: 
L1: if a < b goto L2 
got0 Lnext 
L2: if c < d goto L3 
goto Ld 
L3: t, := y + z 
X := tl 
got0 L1 
L4: t* : =  y - z 
X := t2 
goto L1 
Lnext : 
We nole that the first two gotus can be eliminated by changing the directions 
of the rests+ This type of local transformation can be done by peephole optim- 
ization discussed in Chapter 9. 
El 
It is irnprtant to realirx that we have simplified the grammar For brwlean 
expressions. ln practice, bolein exprc&ms 
dtcn contain arithmetic suhex- 
pressions as in { a+b 1 c, In languages where false has the numerical value 0 
and true the value 1, l a  e b) 
4 I b < a )  can even be considered an arithmetic 
expression, with value 0 if a and b have the same value, and 1 otherwise. 

4% 
INTERMEDIATE CODE GENERATlQN 
SEC. 8.4 
The method of representing boolean expressions by jumping d
e
 can still 
be used, even if arithmetic e~pressions are reprewsted by code to compute 
their value, For example, consider the repksentaiive grammar 
E - E  + E  I E a n d E  I  rel lop^ { id 
We may suppi* that E + E produces an integer a~ithmetic result (the inclusion 
of real or other arithmetic types makes matters more complicated but adds 
nothing to the instructive value of this example), while expressions E and E 
and E relw E produce boolean values repremted by flow of contrd. 
Expression E and E requires both arguments to be boolean, bur the operations 
+ and rebp take either type of argument, including mixed ones. E -c id is 
also deemed arithmetic, although we could extend this example by allo&ng 
bodean identifiers. 
To generale c d e  in this siiuation, we can use a synthesized attribute E.type, 
which will be either urith or h
I
 depending on the type of E. E will have 
inherited attributes E. rruc and E+fahe for boolean expressions and synthesized 
attribute E.pbce for arithmetic expreisions. Part of the semantic rule for 
E 
El + E 2  is shown in Fig. 8.25. 
Fig. 3.25, Srnantk rulc for production E -c E, + E l .  
fn the mixed-mode case, we generate the code for E l ,  then E l ,  followed by 
the three statements: 

8.5 
C4SE STATEMENTS 497 
SEC, 
The first statement computes the value E l  + I for E when E l  is true, the 
third the value El for E when E 2  is false. The second statement is a jump 
over the third. The sernaotic rules for the remaining cases and the other pro- 
ductions are quite similar, and we leave them as exercises. 
The "switch" or "case" statement is available in a variety of languages, even 
the Fortran computed and assigned goto's can h regarded as varieties of the 
switch statement. Our switch-statement syntax is shown in Fig. 8.26. 
Fig. 8.24. Switch-statcmcnl syntax + 
There is a selector expression, which is to be evaluated, followed by n con- 
stant values that the expression might take, perhaps including a befault 
"value," which always matches the expression if no other value does, The 
intended translation of a switch is code to: 
1. 
Evaluate'tk expression. 
2. 
Find which value in the list of cases is the same as the value of the 
expression. Recall that the default value matches the expression if none 
of the values explicitly mentioned in cases does. 
3. Execute the statement associated with the value found, 
Step (2) is an n-way branch, which can bc implemented in m e  of several 
ways. If the number of cases is not too great, say LO at most, then it is rea- 
sonable to use a sequence of conditional goto's, each of which tests for an 
individual value and transfers to the d
e
 for the corresponding datement, 
A more compact way to implement this wqrrence of conditional goto's is to 
create a table of pairs, each pair consisting of a value and a label for the code 
of the corresponding statement. Code is generated to place at the end of this 
table the value of the expression itself, paired with the label for the default 
statement+ A simple loop can be generated by the compiler to compare the 
value of the expression with each value in the table, being assured that if no 
other match Is found, the last (default) entry is sure to match. 
If the 'number of values exceeds 10 or so, it is more efficient to construct a 

498 
INTERMEDIATE CODE GENERATION 
SEC. 8+5 
hash table (see Section 7.6) for the values, with the labels of the various state- 
ments as entries. If no entry for the value p o s ~ s s e d  by the switch expression 
is found, a jump to the default statement can be generated. 
There is a common special case in which an even more efficient implemen- 
tation of the R-way branch exists. If the values ail lie in some small range, 
say I,,,, to i,,,,,, 
and the number of different values is a reansonablie fraction of 
ilnvx 
-ilnin, 
then we can cunstruct atr array of labels, with the label of the 
statement for value j in the entry of the table with offset j-i,,,i, and the label 
for the default in entries not filled otherwise. To perform the switch, evaluate 
the expression to obtain the value j ,  check that it is in the range i,,in to irlral 
and transfer indirectly to the table entry at offset j-i,,i,. 
For example, if the 
expression is uf type character, a table of, say, I28 entries (depending on the 
character set) may be created and tra~sferred through with no range testing. 
Sy ntax-Directed Translation of Case Statemenis 
Consider the following switch statement. 
switch E 
begin 
Case vI: 
k q  t 
case v,: 
. . -  
s2 
With a syntax-directed translatiun scheme, il is convenient to translate this; 
case statement into intermediate code that has the form of Fig. 8.27. 
The tests all appear at the end so [hat a simple code generator can recognize 
the multiway branch and generate efficient code for it, using the most 
appropriate implementation suggested at the beginning of this section. If we 
generate thc more straightforward sequence shown in Fig 8.28, the compiler 
would have lo do extensive analysis to find the most efficient implementation. 
Note that it is inconvenient to place the branching statements at the beginning. 
because the compiler could not then emit c d e  for each of the Si*s as it saw 
them. 
To translate into the form of Fig. 8.27, when we see the keyword switch, 
we generate two new labels t e s t  and next. and a new temporary t. Then 
as we parse [he expression E, we generate a l e  to evaluate E into t. After 
procasing E ,  we generate the jump goto t e s t .  
Then as we see each case keyword, we create a new label Li and enter i t  
into the symbol table. We place on a stack, used only to store cases, a pointer 
to this symbul-table entry and the value Vi of the case constant. 
(If this 
switch is embedded in one of the statements internal to another switch, we 
place a marker on the stack to wparate caws for the interior switch from 
thc~sc for the outer switch.) 

CASE SFATEMENTS 499 
L] : 
L2: 
Lm-) : 
L, : 
test: 
next: 
Fig. 8.27. 
L1 : 
L* : 
Lnm2 : 
L n m r  
: 
next: 
code to evaluate E into t 
goto t e s t  
code for 
goto next 
d
e
 for S2 
goto next 
4 
+ 
+ 
code for S, - I 
goto next 
code for S, 
goto next 
if t = V ,  goto Ll 
if t = V 2  gcato L2 
4 
. 
lf t a 
CJOtQ La-] 
g o t 0  L, 
Translation of a case statemar. 
code ro evaluate E into t 
if t 3; V, got0 LI 
code for S I 
goto next 
i f  t + V 2  goto LL 
a l e  for S1 
goto next 
i f  t # V,-I 
goto L,-, 
code for S, - 
goto next 
code for S, 
Fig. 8,28+ Another translation of a case statement 
We prwss each statement case Vi : Si by emitting the newly created label 
Li, fdlowed by the code for S;, followed by the jump goto next. Then 
when the keyword end terminating the body of the switch is found, we are 
ready to generate !he code for the n-way branch. Reading the pointer-value 
pairs on the case stack from the bttorn to the top, we can gtnerate a 
sequence of three-address statements of the form 

500 
1NTERMEDlATE CODE GENERATION 
SEC. 8.5 
ease V 1  tl 
ease v2 L* 
where t is the name holding the value of the selector expression E, and L, is 
the l a k l  for Iht default statement. The case Vi L~ three-address statement is 
a synonym for i f  t =Vi goto Li in Fig. 8.27, but the case is easier for the 
final code generator to detect as a candidate for special treatment, At the 
code-generation phase, these sequences of case statements can be translated 
into an n-way branch of the most efficienl type, depending on how many there 
are and whether the values fall into a small range. 
8.6 BACKPATCHING 
The easiest way to implement the syntaxdirected definitions in Section 8.4 is 
tn use two passes. First, construct a syntax tree for the input, and then walk 
the tree in depth-first order, computing the translations given in the definition. 
The main problem with generating code for boolean expressions and flow-of- 
contrd statements in a single pass is that during one single pass we may not 
know the labels that control must go to at the time the jump statements are 
generated. We can get around this problem by generating d series of branch- 
ing statements with the targets of the ju'mps temporarily left unspecified. 
Each such statement will be put on a list of goto statements whose labels will 
be filled in when the proper label can k determined. We call this subsequent 
filling in of labels Backpatching, 
t 
In this section, we show how backpatching can be used to generate d
e
 for 
boolean expressions and flow-of-control statements in one pass. The transla- 
tions we generate will be of the same form as those in Section 8.4, except for 
the manner in which we generate labels. Far specificity, we generate quadru- 
ples into a quadruple array. Labels will be indices into this array. To mani- 
pulate lists of labels, we use three functions: 
I .  maMisr(i) creates a new list containing only i, an index into the array of 
quadruples: w k c h  returns a pointer to the list it has made. 
2. 
merge ( p  I , p Z )  concatenates the lists pointed to by p 1 and pi, and returns 
a pointer to the concatenated list. 
3. 
hckparch(p. i )  inserts i as the target label for each of the statements on 
the list pointed to by p. 

BACKPATCHING 501 
We now construct a translation scheme suitable for producing quadruples for 
bolean expressions during bottom-up parsing. We insert a marker nontermi- 
nal M into the grammar to cause ;e semantic action to pick up, at appropriate 
times, the index of the next quadruple to be generated. The grammar we use 
is the following: 
Synrhesizeb attributes truelist and juiseii.rt of nonterminal E are used to gen- 
crate jumping code for boolean expressioms. As code i s  generated for E, 
jumps to the true and false exits are left incomplete, with the label field 
unfilled. These iincfimplete jumps are placed on lists pointed to by E.rnrebisr 
and E.fdseii,~l, as appropriate. 
Thc sc mantic actions reflect the considerat ions mentioned above. Consider 
the production E -,El and M El. If El i s  false, then E is alw false, so the 
statements on E ,.fd,w!i,~i become part of E. fuisck~t. If E l  is true, however, 
we must next test E 2 ,  SO the target for the statements E .rrua/isr must be the 
beginning of the code generated for E l .  This target is obtained using the 
marker nmterminal M. Attribute M.q& 
records the number d the first 
statement of E 2 . d e .  With the production M -. r we associate the semantic 
act ion 
The variable ncxiquud holds the index of the next quadruple to follow. This 
value will he backpatched onto the El.trwrldst when we have seen the 
remainder of rhe production E -, El and M E2. The translation scheme is as 
io~lows+ 

502 
INTERMEDIATE CODE GENERATION 
\ 
(3) E -c not E l  
( 5 )  E - id, relop id2 
( 6 )  E + true 
(7) E - Btse 
(8) M 
E 
For simplicity, semantic act ion T 5) generates two statements, a conditional 
goto and an unconditional one. Neither has its target filled in. The index of 
the first generated statement is made into a list, and E.truelisr is given a 
pointer to that list, The second generated statement goto - is also made into 
a list and given to E.fuiseiisr. 
Example 8.6. Consider again the expression a < b  or c < d  and r e f .  An 
annotated parse trec is shown in Fig. 8.29. The actions are performed during 
a depth-first traversal of the tree, Since all actions appear at the ends of right 
sides, they can be performed in conjunction with reductions during a bttom- 
up parse. In response to the reduction of a c b to E by production ($1, the 
two quadruples 
are generated. (Wc again arbitrarily start statement numbers at 100.) The 
marker nonterrninal M in the production E - E l  or M El records the value of 
nexrquub, which at this time Is 102, The reduction of c < d to E by produc- 
tion 151 generates the quadruples 
Wc have now scen E l  in the production E -+El and M E l .  The marker non- 
terminal in this production records the current value of mxtquad, which is 
now 104, Rcducing e < f into E by production 15) generates 

BACKPATCHING 503 
Fig. 8.29, Annotated par= tree for a 4 b or c * d and e < f .  
We now reduce by E - E a d  M E? . The corresponding semantic action 
calls BacRpatch({102), 104) wbere (102) as argument denotes a pointer to the 
lis~ containing only 402, that list being the one pointed to by E 1 +truetist. This 
call to hckpu~ch fills in !@I 
in statement 102. The six statements generated 
so far are thus: 
The semantic action asmiiated with the final reduction by E - E 1 or M El 
calls hckpasch({ 101), 102) which leaves the statements hoking like: 
100: 
if a * b gato , 
1 
got0 102 
102: 
if c * d goto 104 
103: goto - 
104: 
i f  t 
f got0 - 
105: 
goto - 
The entire expression i s  true if and only if the goto's of statements 100 or 
104 are reached, and is false if and only if the goto's of s~atemenls 103 or 105 
are reached. These instrltctions will have their targets filled in later in the 
compilation, when it is seen what must be done depending on the truth or 
falsehood of the expression. 
o 

504 INTERMEWATE CODE GENERATION 
We now show how backpatching can be used to translate flow-of-control state- 
ments in one pass. As above, we fix our attention on the generation of quad- 
ruples, and the notation regarding traidarim field names and list-bandling 
procedures from that section carries over to this section as well. As a larger 
example, we develop a translakion scheme for statements generated by the fol- 
!owing grammar: 
Here S denotes a statement, L a statement list, A an assignment statement, 
and E a boolean expression, Note that there must be other productions, such 
as those for assignment statements. The pductions given, however, will be 
sufficient to illustrate the techniques used to translate flow-of-control state- 
ments. 
We use the same slructure of code for if-then, if-then-else, and while-do 
statements as in Section 8.4. We make the tacit assurnptiort that the code that 
follows a given statement in execution also follows it physically in the quad- 
ruple array. If that is not true, an explicit jump must be provided. 
Our general approach will be to fill in the jumps out of statements when 
their targets are found, Nut only do boolean expressions need two lists of 
jumps that m u r  when the expression i s  true and when it is false, but state- 
ments also need lists of jumps (given by attribute nexrlisr) to the d
e
 
that fol- 
lows them in the execution sequence, 
We now describe a syntax-directed translation scheme to generate translations 
for the flow-ofcontral constructs given above. The nonterminal E has two 
attributes E.trdisi and E.jiulselisr, as above, L and S each also need a list of 
unfilled quadruples that must eventually be completed by backpatching. 
These lists are pointed to by the attributes L.nexrlisr and S.nexflist. S . n d i s f  is 
a pointer to a list of all conditional and unconditional jumps to the quadruple 
following the statement 5 in execution order. and Lmxtlisi is defined simi- 
larly . 
In the code layout for S -. whik E do S ,  in Fig. 8,22(c), there are labels 
S.Be~in and E,truc. marking the beginning of the c d e  for the complete state- 
ment S and the body SI. The two occurrences of the marker nonterminal M 
in the following production record the quadruple numbers of these psitiom: 

SEC. 8.6 
BACKPATCHING 505 
Again, the only production far M is M -c E with an adion setting attribute 
M . q d  to the number of the next quadruple, After the M
y
 
S of the while 
statement is executed, control flows to the kginning. Therefore, when we 
reduce while M I  E do M 2  
5 to S, we backpatch S .nextli.~f to make all targets 
on that list be M ,  . q d .  An explicit jump to the beginning of the code for E 
is appended after the code for $, because mntrd may also "fall out the b01- 
tom," E.fruebis~ is backpatched to go to the bginning of Si by making jumps 
on E.rrueIist go to MI .qsrad. 
A more compelling argument for using S. n d i s f  and L. nextllsr comes when 
code is generated for the conditional statement if E then S else 
If control 
"falls out the bot~orn" of S 
as when S is an assignment, we must include at 
the end of the d
e
 for S1 a jump over the code for S2.. We use another 
marker nonterminal to intrdrce this jump after Sl + 
Let nonterminal H be 
this marker with production N + E. N has attribute NmxtIisr, which will be a 
list consisting of the quadruple number of the statement qwto, that is gen- 
erated by the semantic rule for N. We now give the semantic rules for the 
r e v i d  grammar. 
We backpatch the jumps when E is true to the quadruple MI .quad, which is 
the beginning of the d
e
 for S , 
+ Simiiarly, we backpatch jumps when E is 
false €0 go to the begirtning of the code for S2. The list S.srexrfiss indudes all 
jumps out of S1 and S2, as well as the jump generated by N. 
The assignment S.nextht := nil initializes S.nexlList to an empty list. 

506 
iNTERMEDIATE CODE GENERATiON 
SEC. 8.6 
The statement following L I in order of execution is the beginning of S. 
Thus the L I .wxdisr list is backpatched to the hginning of the code for S ,  
which is given by M.yuab. 
Note that no new quadruples are generated anywhere in these semantic 
rules except for rules (2) and ( 5 ) .  All other code is generated by the semantic 
actions associated with assignment statements and expressions, What the flow 
of control dms is cause theh proper backpatching so that the assignments and 
boolean expression evaharions will connect properly. 
Labels and Gotos 
The most clcmcntary programming Iariguage construct Tor changing the flow 
of contrul in a program is the l a k l  and goto. When a compiler encounters a 
stahment like goto L, it must check that there is exactly one statement with 
label L in the scope of this goto statement. If the Label has already appeared, 
cirher in a label-declaratim statement or as the label of some source state- 
ment, then the symbol table will have an entry giving the compiler-generated 
labcl for the first three-address instruction associated with the sourcc state- 
ment labeid 1;. For thc translation we gcnerm a goto three-address stste- 
rnent with that compiler-generated label as target. 
WRm a label L is encountered for the first time in the source program, 
eithcr in a declaration or as the target of a forward goto, we enter L into the 
symhd table and generate a symbolic Labcl fur L. 
8.7 PROCEDURE CALLS 
The prorrcdurei' is such an important and r r p p m l y  used programming con- 
hlrud that it is imperalive for a compiler to generate g o d  code fur procedure 
c n h  and returns. The run-time routines that handle p r a b u r e  argument 
passing, calls, and returns arc part of (he run-time support package, We 4 s -  
cussed the different kinds of mechanisms needed to implement the run-time 
support package in Chuptcr 7, In this section, we discass the c d e  that is typi- 
cally generated for proccdurc calls and returns. 
Let us consider a grammar for a simple proccbure call statement. 
---__.___- 
' 
W t  uw Ihc ICCII~ 
p c ~ t i ~ d u r ~  
to include hlrnr.[im A hnctkm ib a priwdurc that rclurns a yluc. 
i 

PROCEDURE 
CALLS SOf 
As discussed in Chapter 7, the translation for a call indudes a calling 
sequence, a sequence of actions taken on entry to and exit from each pro- 
cedure, While calling sequences differ, even for implementations of the same 
language, the following actions typically take place: 
When a proxdure call uccurs, space must Ix allocated for the activation 
record of the called procedure. The arguments of the called procedure musi 
be evahated and made availabie to the called procedure in a known place. 
Environment pointers must be established to enable the called procedure to 
access data in enclosing blocks, The state of the calling prmedure must be 
saved w it can resume execution after the call. Also saved in a known place 
i s  the return address, the location to which the called routine must transfer 
after it i s  finished, The return address is usually the location of the ins~ruc- 
tion that follows the call in the calling procedure. Finally, a jump to the 
beginning of the code for the called prwxdure must be generated. 
When a procedure returns, several actions also must take place. If the 
called prwdure i s  a function, the resuh must be stored in a known place+ 
The activalion record of the calling prmdure must be restored. A jump to 
the calling prccedure's return address must be generated. 
There is no exact division of the run-time casks between the calling and 
called procedure. Often the source language, the target machine, and the 
operating system impose requirements that favor one mlut ion over another. 
A Simple Example 
Let us consider a simple example in which parameters are paxwd by reference 
and storage is statically allocated. In this situation, we can use the param 
statements themselves as placeholders for the arguments. The callcd pro- 
cedure is passed a pointer in a register to the first d the param statements, 
and can obtain pointers to any of its arguments by using the proper offset 
from this base pointer. When generating threeaddress d
e
 for this r y p  of 
call, it is sufficient to generate the three-address statements necded to evalualc 
those arguments that are expressions other than simple names. then follow 
them by a list of param three-address statemtats, one for each argument. If 
we do not want to mix the argument-evaluating statements with the param 
statements, we shall have to save the value of E.p\aw, for each expression E 
in id(&, E ,  . . . , E).' 
A convenient data sfructure in which to save these values is a queue, a 
firsl-in first-out list. Our semantic routine for E!ist 
+ Eiisi . E will include a 
step to store E.plut.~ on a queue yurur, Then, the semantic routine for 

508 
INTERMEDIATECODEGENERATION 
SEC, 8.7 
S -. rail id I Elist ) will generate a pararn slarement for each item on queue, 
causing these statements to follow the statements evaluating the argument 
expressions. Those statements were generated when the arguments them- 
selves were reduced 10 &. The following syntax-directed translation incor- 
porates these ideas, 
( 1 
S + call M ( Elisr ) 
{ 
foreachitemponqueuedo 
rmi~('param* 
p); 
emit I 'call ' id.pIac~) } 
The code for S is the code fur Elisr, which evaluates the arguments, fol- 
lowed by a paran p statement for each argument, followed by a d 
state- 
ment, A count of the number of parameters is not generated with the call 
statement but could be calculated in the same way we computed E/isr.nbirn In 
the previous section. 
(2) 
E M  - EIisr , E 
( 
append E.plac~ 
to ,the end of queue ) 
(3) 
H i s t  - E 
{ 
initialize queue to contain only E.place 
Here queue is emptied and then gets a single pointer to the symbol table 
Iocatioit for the name that denores the value of E, 
EXERCISES 
8.1 Translate the arit hrnetic expression a r - I b + c 1 into 
a) a syntax tree 
b) postfix notation 
C) three-address code 
8.2 Translate the expression - { a + b) * e + d.1 + ( a + b + 
inla 
a) quadruples 
b) triples 
C) indirect triples. 
8-3 Translate the executable statements of the following C program 
main( 1 
{ 
int i; 
i n t  a[10]; 
i = 1; 
while l i  <= 701 I 
a[i] = 0; i = i + 1; 
1 
1 
into 

CHAFTER 8 
EXERCISES 509 
a) a syntax tree 
b) postfix notation 
C) three-address code. 
"8.4 Prove that If all operaiorx are binary, then a string of operators and 
operands is a postfix expression If and only if ( I )  there 1s exactly one 
fewer operator than operands, and (2) every nonempty prefix of the 
expression has fewer operators than operands. 
8.5 Modify the translation scheme in Fig. $,I 1 for compu[ing the types 
and relalive addresses of declared names lo iilbw lists of names 
instead of single names in declarations of the Corm D - id : T. 
8.6 The gr#x 
form of an expressinn in which operator 8 is appkd to 
expressions c. , t2, . . . + q is Op 
. . . pk, where p, is the prefix 
form of 
P, . 
a) Generate thc prefix form of a * - ( b + c ) .  
**b) Show that infix expressions cannot be translated into prefix form 
by translation ~hernes in which all actions arc printing actions and 
all actions appear at the ends of right sides of productims. 
C) Give a syntax-directed definition to translate infix expressions into 
prefix form. Which of the methods of Chapter 5 can you use? 
8.7 Write a program to implement the syntax-directed definition for 
translating bwleans to three-address code given in Fig. 8.24. 
8.8 Modify the syntax-directcd definition in Fig. 8.24 to generate code for 
the stack machine of Section 2.8. 
8.9 The syntax-directed definition in Fig. 8.24 translates E -. id, < id2 
inla  he pair of statements 
We could translate instead into the single statement 
and fall through the code when E is true. Modify the definition in 
Fig. 8.24 to generate code of this nature+ 
8.10 Wrik a program to implement the syntax-directed definition [or 
flow-of-control statements given in Fig, k23. 
8.11 SWrile a program to implement the backpatching algorithm given in 
kction 8.6. 
8,12 Translate  he followrng assignment slalement Into three-add~ess code 
using the translation schenis in Section 8.3. 

510 
INTERMEDIATE CODE GENERATION 
CHAPTER 8 
*8.13 Some languages, such as PDI, permit a list of names to be given a list 
of attributes and also permit declarations to be nested within one 
another. The following grammar abstracts the problem: 
The meaning of D + ( D ) attrlisr is that all names mentioned in the 
declaration inside parenthem are given the attributes on attrh, no 
matter how many levels of nesting !here are. Note that a declaration 
of n names and m attributes may cause nm pieces of information to be 
entered into the symbol table. Give a syntax-directed definition for 
dedarations defined by this grammar. 
8.14 In C, the fur statement has the following form: 
Taking its meaning ro be 
e l ;  
while I e2 1 { 
construct a syntax-directed definition ra translate C-stylc fur state- 
ments into three-address code. 
8.15 The Pascal standard defines the statement 
to have thc same meaning as the t'dlowlng code sequence; 

CHAPTER 8 
BlBLl€IGR,4PH IC NOTES 5 1 1 
a) Consider the following Pascal program: 
program forlooplinput, output); 
var i, initial, final: integer; 
begin 
read( i n i t i a l ,  f inall ; 
for i:= 
i n i t i a l  to f i n a l  do 
wxiteln(i1 
end, 
What behavior does this program have for initid = MAXINT - 5 
and find = W I N T ,  where WINT 
is Ihe largest integer on the 
target machine, 
* b) Construct a syntax-directed definition that generates correct three- 
address code for Pascal for-statements, 
BIBLIOGRAPHIC NOTES 
UNCOL (for Universal Compikr Oriented Language) is a mythical universal 
intermediate language, sough1 since the mid 1950's. Given an UNCOL. the 
committee report by Strong et al, 119581 showed how cumpilers could be con- 
structed by hooking a front end for a given source language with a back end 
for a given target language. The bootstrapping techniques given in the report 
are routinely used to retarget compilers (see Section 1 1.2). Steel 1 1 % 11 con- 
tains an original proposal for UNCOL. 
A retargetable compiler consists of one front end that can be put tugetkr 
with several back ends to implement a given language on several machincs. 
Neliac is an early example of a language with a retargetable compiler (Hus- 
key, Halstead, and McAtthur 1196U]) written in its own language. See also 
Richards 1197 4 1 for a description of a retargetable compiler for BCPL, Nori et 
al. 119811 for Pascal, and Johnson 11979) for C .  Newey, P d e ,  and Waite 
I 19721 apply the idea of changing the back end to a macro processor, a text 
editor, and a Basic compiler. 
The UNCOL ideal of implementing n languages on m machines by writing rt 
front ends and m back ends, as opposed to nxm distinct wmpiiers, has been 
approachd in a number of ways. Onc approach is tu retro-fit a front end for 
a new language onto an existing compiler. Feldmen 1 1979b1 describes the 
a d d i t h  of a Fortran 77 front end ro the C compilers by Johnson 119791 and 
Ritchie 1 N79] 
+ Compiler organizations dcsigned to accorndate multiple front 
ends and back ends are described by Davidwn and Frascr 119841, Levcrett et 
a]+ 119801, and Tanenbaum el al+ 119831, 
The terms "union*' and "intersection" abstract machines used by Davidmn 
and Fraser 11984b1 highlight the role uf the set of albwlrable operators in an 
intermediate representation. Thc instruction set and addressing modes of an 
intersection machine are limited, so the front end does not have 10 make many 
b k e s  when it gerrerales intermediate code, Union machines provide alterna- 
tive ways of implernen~ing sour~xAeveI constructs. Since all of the alternatives 

5 12 
INTER MEDLATE CODE GENERAWON 
CHAPTER 8 
may not be implemented directly by all target machines, the richer instructwn 
set of the union machine may albw dependence on the target machine to 
creep in, Similar remarks apply to other kinds of intermediate code, such as 
syntax trees and three-address code, Fraser and Hanson 1 19821 consider ways 
of expressing access to the run-time stack using machine-independent opera- 
tions. 
The implementation of Algol 60 i s  discussed In detail by Randeli and 
Russell 119641 and Grau, Hill, and Lartgrnaack [1%7j+ Freiburghouse [I9691 
discusses PLA, W irth 11 97 1 ) Pascal, and Branquart et al. 119761 Algol 63. 
Minker and Minker [I9801 and Giegerich and Wilhelm 119781 dtscuss the 
generation of optimal code for boolean expressions. Exercise 8.15 is from 
Newey and Waite (19851- 

CHAPTER 9 
Code 
Generation 
The final phase in our compiler model is the code generator. It takes as input 
an intermediate representation of the source program and produces as output 
an equivalent target program, as indicated in Fig. 9.1. The code generation 
techniques presented in this chapter can be used whether or not an optimiza- 
tion phase m r s  before wde generation. as in some so-called "optimizing" 
compilers, Such a phase tries to transform the intermediate d
e
 into a form 
from which mort efficient target code can be produd. We shall talk abut 
d
e
 
optimization in detail in the next chrpier. 
. . . . . . . . . -
.
-
7
 
A . . . . . - 
. 
. . 
. 
source 
front 
intermediate i 
d
e
 : intcrrndiatc 
r& 
target 
Program 
end 
#ertr.ruior 
program 
. . - - , - , , - . . . .  
, 
Fig. 9.1. Position d codc generator. 
The requirements traditionally imposed on a code generator are severe. 
The output W e  must be correct and of high quality, meaning that it shoutd 
make effective use of the resources of the target machine. Moreover, the 
code generator itxlf should run efficiently. 
Mathematically, the problem of gene~wating optimal code is undecidable, In 
practice, wc must k content with heuristic techniques that generate gad, but 
not necessarily optimal, d e .  The choice of heuristics iu Important, in that a 
carefully designed d
e
 generation algorithm can easily produce code that is 
sevtral times faster than that produced by a hatjrily conceived algmithm. 

514 
CODE GENERATION 
SEC. 9.1 
9.1 ISSUES IN THE DESlGN OF A CODE GENERATOR 
While the details are dependent on the target language and the operating sys- 
tem, issues such as memory management, instruction selection, register alloca- 
t ion, and evaluation order are inkrent in airnost all &-generation 
prob- 
lems. In this section, we shall examine the generic issues in the design of 
code generators. 
Input to the Code Generator 
The input to the code generator consists of the intermediate representation of 
the source program produced by the front end, together with information in 
the symbl table that is used to determine the run-time addresses of the data 
objects denoted by the names in the intermediate representat ion. 
As we noted in the previous chapter, there are scvtral choices f8r the inter- 
mediate language, including: h e a r  representations such as postfix notation, 
three-address representat ions such as quadruples, virtual machine representa- 
tions such as stack machine code, and graphical representations such as syntax 
trees and dags. Although the a1gotithms;in this chapter are couched in terms 
of three-address code, trees, and dags, many of the techniques also apply to 
the other intermediate representations. 
We assume that prior to code generat ion the front end has scanned, parsed, 
and translated the source program into a reasonably detailed intermediate 
representation, so the values of names appearing in the intermediate language 
can be represented by quantities that the target machine can directly manipu- 
late (bits, integers, reds, pointers, etc.), We also assum that the necessary 
type checking has taken place, so typc-convcrsim operators have been inserted 
wherever necessary and obvious semantic errors (e.g., attempting to index an 
array by a tloating-point numhr) have already been detected. The code gen- 
eration phase can therefore prowed on the assumption that its input is free of 
errors. In some compilers, this kind of semantic checking is done together 
with code generation. 
Target Programs 
The output of the code generator is the target program, Like the intermediate 
code, this output may take on a variety of forms: absolute machine language, 
rclocatable machine language, or assembly language. 
Producing an absolutc rnachine-language program as output has the advan- 
tage that iit can be placed in a fixed \mation in memory and immediately exe- 
cuted. A small program can be compiled and executed quickly. A number of 
"student-job" compilers, such as WATFIV and PWC, prduce absulute code. 
Producing a rebcatable machine-language program (object module) as our- 
put allows subprograms to be compiled separately. A set of relocatable object 
moduies can be linked together and loaded for execution by a linking loader, 
Although we musk pay the added expense of linking and loading if we produce 

SEC. 9.1 
ISSUES IN THE DESIGN OF A CODE GENERATOR 
5 15 
relocatable object modules, we gain a great deal of flexibility in being able to 
compile subroutines separately and to call other previously compiled programs 
from an object module, If the target machine does not handle relocation 
automakally, the compiler must provide explicit relwa~ion information to the 
loader to link the separately cmpikd program segments, 
Producing an assembly-language program as output makes the prmss of 
oode generation somewhat easier. We can generare symbolic instructions and 
use the macro facilities of the assembler to help generate code. The price paid 
is the assembly step after code generation. Because producing assembly code 
d
~
s
 
not duplicate the entire task of the assembler, this choke is another rca- 
sonabk alternative, especially for s machine with a small memory, where a 
compiler must use several paws. In this chapter, we use assembly cwde as 
the target language for readability. However, we should emphasize that as 
long as addresses can be calculated from the offsets and other information 
stored in the symbol table, the d
e
 generator can prduce rehatable or 
absolute addresses for names just as easily as symbolic addresses. 
Mapping names in the source program to addresses of data objects in run-time 
memory is done oooperatively by the front end and the code generator. In the 
last chapter, we ass;urned that a name in a three-address statement refers to a 
symbol-table entry fur the name. In Section 8.2, symbol-table entries were 
created as the dechrations in a procedure were examined. The type in a 
declaration determines the width, i.e., the amount of storage, needed fur the 
declared name. From the symbol-table information, a relative address can be 
determined for the name in a data area for the prtxxdure. In Section 9.3, we 
outline implementations of static and stack allocation of data areas, and show 
how names in the intermediate representation can be converted into addresses 
in the target code. 
If machine code is being generated, labels in three-address statements have 
to be converted to addresses of instructions. This process is analogous to the 
"backpatching" technique in Section 8.6. Suppose that labels refer to quadru- 
ple numbers in a quadruple array, As we scan each quadruple in turn we can 
deduce the location of the first machine instruction generated for that quadru- 
ple, simply by maintaining a count of the aurnber of words used for the 
instructions generated so far+ Tbis count can be kept in the quadruple array 
(in an extra field), so if a reference such as j :  goto i is encountered, and i is 
less than j ,  the current quadruple numkr. we may simply generate a j ~ m p  
instruction with the target address equal to the machine location of the first 
instruction in the code for quadruple i. If, however, the jump is forward, so r' 
exceeds j, we must store on a list fur quadruple i the location of the first 
machine instruction generated for quadruple j. Then, when we proces quad- 
ruple i, we fill in the proper machine location for all instructions [hat are. for- 
ward jumps m i. 

5 16 
CODE GENERATION 
The nature of the instruction set of the target machine determines the diffi- 
cult y of instruction selection. The uniformity and completeness of the instruc- 
tion set are importan1 factors. lf the target machine does not support each 
data type in a uniform manner, then each exception to the general rule 
requires special handling, 
Enstruction speeds and machine idioms are othe~ important factors. If we 
do not care about the efficiency of the target program, instruction d e c t  ion is 
straightforward. For each t y p  of three-address statement, we can design a 
code skekton that outlines the target code to be generated for that construct. 
For cxample, every three-address statement of the form x: =y+z+ where x, 
y, and z are staticatly allocated, can be translated into the d
e
 sequence 
MDV y,RO I+ Load y into register RD */ 
ADD z,RO I* add z to RO */ 
MOV RO,x ,-'* store RO into x +/ 
Unfortunately, this kind of statement-by-statement d
e
 generation often pro- 
duces poor code. For example, the sequence of statements 
would be translated into 
MOV b,RO 
A m  c,RO 
MOV RO,a 
MOV a,RO 
ADD s,RO 
MCW RO,d 
Here the fourth statement is redundant, and so is the third if a is not subse- 
quently used. 
The quality of the generated code is determined by its speed and size. A 
target machine with a rich inslrubion set may provide several ways of imple- 
menting a given operation. Since the cost differences btween different imple- 
mentations may be significant, a naive translation of the intermediate code 
may lead to correct. but unacceptably inefficient target d e .  For example, if 
the target machine has an "increment" instruction (INC), then the three- 
address statement a : = a+ 1 may be implemented more eficientIy by the single 
instruction IHC a, rather than by a more obvious sequence that loads a into a 
register, adds one to the register, and then stores the result back in a: 
MOV 
a, RO 
ADD 
# I ,  RO 
MOV 
RO, a 
lnslruction speeds are heeded to design good code 
sequences but, 

XC. 9.1 
ISSUES IN THE DESIGN OF A CODE GENERATOR 
517 
unfortunately, accurate timing information is often difficult to obtain. Decid- 
ing which machinecode sequence i s  best for a given three-address construct 
may also require knowledge about the context in which that construct appears. 
Tools fur wnstructing instruction selectors are discuswd in Sect ion 9.12. 
Instructians involving register operands are usuaily shorter and faster than 
those involving operands in memory. Therefore, efficient utilization of regis- 
ters is particularly important in generating g d  
c d e .  The use of registers is 
often subdivided into two subproblems: 
I . During register docution, we select the set of variables that will reside in 
registers at a p i n t  in the program. 
2. 
During sl subsequent rcgisrrr assigrrmmr phasc, we pick the specific regis- 
ter that a variable will reside in. 
Finding an optimal assignment of registers to variables is difficult, even 
with single-register values, Mathematically, the problem is NP-complete, The 
problem is further compiicated because the hardware and/or the operating sys- 
tem of thc targct machinc may require that certain register-usage conventions 
be observed. 
Certain machines require rugisrer-pdrs {an cvcn and next odd-numbered 
register) for some operands and results. For example, in the IBM Systeml370 
machines integcr mdtiplication and integer division involve register pairs. 
The multiplication instruction i s  of the form 
where x, the muhiplicand, is the even register of an even/odd register pair. 
The multip!icand value is taken from the odd register of the pair. The multi- 
plier y i s  a single register. The product wcupies the entire eucnhdd register 
pair. 
The division instruction is uf the form 
where the Wbit dividend occupies an evenhdd regiskr pair whose even repis- 
ter is x; y represents the divisor. After division, the even register holds the 
remainder and the odd register the quotient. 
Now, consider the two three-address code *qucnces in Fig. 9.2(a) and (b), 
in which the only differcncc is the operator in the second statement. The 
shortest assembly-codc sequences for (a) and (b) are given in Fig. 9.3. 
Ri stands for register i+ {SRDA' R0,32 shifts the dividend into R1 and 
dews RO so all bits equal its sign bit .) 
L, ST, and A stand for load. store 
and add, respectively. Note that the optimal choice for the register into which 

5 18 CODE GENERATION 
Fig, 9.2. Two three-address d
c
 
equenocs. 
L 
R 7 , a  
L 
RO, a 
A 
R 1 , b  
A 
RO, b 
M 
R O , c  
A 
RO, c 
D 
R O , d  
SRDA RO, 32 
ST 
R1, t 
D 
RO, 6 
ST 
R1, t 
Fig. 9.3, Optimal machinc-code sequenocs, 
a is to be baded depends on what will ultimately happen to t. Qrategies for 
register allocation are discussed in Section 9.7, 
I 
Chdce td Evaluation Order 
The order in which computations are performed can affect the efficiency of 
the target code. Some computation orders require fewer registers to hold 
intermediate r~qults than others, as we shall see. Picking a k s t  order is 
another difficult, NPamplete problem. Initially, we shall avoid the problem 
by generating code for the three-address statements in the order in which they 
have been producect-by the intermediate code generator, 
Approaches to Code Generation 
Undoubtedly the most important criterion for a d
e
 
generator is that it pro- 
duce correct d e .  Correctness takes on special signifiance because of the 
number of special cases that a d
e
 generator might face. Given the premium 
on correctness, designing a code generator so it can k easily implemented, 
tested, and maintained is an imptant design goal. 
Section 9.6 contains a straightforward de-generation algorithm that uses 
information about subsequent uses of an operand to generate d
e
 for a regis- 
ter machine. It considers each statement in turn, keeping operands in regis- 
ters as long as possible. The output of such a d e  generator can be improved 
by peephole optimization techniques such as those discussed in Section 9+9. 
Section 9.7 presents techniques 10 make better use of registers by consider- 
ing the flow of control in the intermediate code. The emphasis is on 

SEC. 9.2 
THETARGETMACHLNE 
519 
allocating registers for heavily used aperands in inner loops. 
%ctions 9. LO and 9.1 1 present some tree-di~ected code-selection techniques 
that facilitate the construclion of retargetable d
c
 
generators. Versions of 
PCC, the portable C compiler, with such code generators have k e n  moved to 
numerous machines. The availability of the UNIX operating system on a 
variety of machines owes much to the purtability of K C .  Section 9.12 shows 
how code generation can be treated as a tree-rewriting process. 
9-2 THE TARGET MACHINE 
Familiarity with the target machine and its instruction set is a prerequisite for 
designing a good d e  generator. Unfortunately, in a general discussion of 
d
e
 generation ir is not possible to describe the nuances of any target 
machine in sufficient detail to be able to generate p o d  code for a complete 
language on that machine. In this chapter, we shall use as the target corn- 
puter a register machine that is representative of several minicomputers. 
However, the code-generation techniques presented in this chapter have also 
been used on many other classes of machines. 
Our target computer is a byte-addressable machine with four bytes to a 
word and n general-purpose registers, RO, R1, . . . , Rn - 1 .  
It has two- 
address instructions of the form 
in which op is an ap-code. and sourre and desrirurrim are data fields. It has 
the following op-codes (among others): 
Other instructions will k introduced as needed. 
The source and destination fields are not long enough to hold memory 
addresses, so certain bit patterns in these fields specify that words following 
an instruction contain aperands and/or addresses. The source and destination 
of an instruction are specified by combining registers and memory locations 
with address m d e s .  Ln the following description, chunccnr,c (o) dcnotes the 
contenls of the register or memory address represented by a. 
The address modes together with their assembly-languagc forms and associ- 
ated costs are as folbws: 

520 CODE GENERATION 
SEC. 9.2 
A memory locat ion M or a register R represents itself when used as a source or 
destination. For example, the instruction 
stores the contents of register RO into memory location H. 
An address offset r from the value in register R is written as c ~ R ) .  Thus, 
stores the value 
into memory location H. 
Indirect versions of the last two modes are indicated by prefix *. Thus, 
stores the value 
into memory Imatbn M. 
. 
A final address mode allows the source to be a constant: 
Thus, the instruction 
MOV R7 ,RO 
loads the constant I into register RO. 
instruction Costs 
We rake the cost of an instruction to be one plus the costs associated with the 
souroe and destination address modes (indicated as "added cost" in the table 
for address modes above). This cost corresponds to the length (in words) of 
the instruction. Address modes involving registers have cost zero. while those 
with a memory location or literal in them haw cost one, because such 
operands have to be stored with the instruction. 
If space is imprtant, then we should clearly minimize instruaion kngth. 
However, doing so has an imprtant additional benefit. For most machines 
and for most instructions,  he time taken to fetch an instruction from memory 
exceeds the time spent executing the instruct ion, Therefore, by minimizing 
the instruction length we also tend ro minimize the time taken to perform the 
instruciion as well.* Some examples follow. 
Thc c w  critcrkn is rncanl co bc inslruclivc rather rhan rcdiaic. Allowing a full word for an in- 
smuttion simplifies thc rutc for dcttrrnining thc w s t .  A mrm accsutalc estimate of the limc takm 
hy an inslrucritrn w w M  crmtdcr whakr an Instrucl iun rcqu ircs the valuc of an operand, as WPM 

SEC. 9.2 
THE TARGET MACHINE 
521 
1. 
The ins~ructbn Mov RO , 1 copies the amtents of register RO into regis- 
ter R 7. This instruction has cast one, since it occupies only one word of 
memory. 
2. The (store) instruction MOV R5 , M copies the contents of register R5 into 
memory location M. This instruction has cost two, sincc the address of 
memory location M is in the word following the instructinn. 
3. 
The instruct ion ADD # 1 , R3 adds the constant I to the contents of regis- 
ter 3, and has cost two, since the constant 1 must appear in the next word 
following the instruction. 
4. 
The instruction SUB 4 (R0 1 , * 12 I 
R 1 1 stores the value 
into the destination + 1 2 I ~ 1 ) .  The cost of this instruction is three, since 
the constants 4 and 12 are stored in the hcxt two words following the 
instruaion .. 
Some of the difficulties in generating code for this machine can seen by con- 
sidering what code to generate for a three-address statement of the form 
a : = b + c where b and c are sirnpk variables in distinct memory twt ions 
denoted by these namesh This statement a n  be implemented by many dif- 
ferent instruction sequences, Here are a few examples: 
2. 
MOV 
b, a 
ADD 
C ,  a 
cost = 6 
cost = 6 
Assuming RO, R I ,  and Rz contain the addresses of a, b, and c. respec- 
tively+ we can use: 
Assuming R I  and R2 contain the values of b and e, respectively, and that 
the value of b is not needed after the assignmenr, we can use: 
4. ADD 
R2, R1 
MOV 
R1, a 
cost = 3 
We see that in order to generate good d
e
 for this machine, we must util- 
ize its addressing capabilities efficiently. There is a premium on keeping the 
1- or r-value of a name in a register, if possible, if it i s  going to be used in the 
near future. 
a h  its ilddrm (found with the instructiunl, to tw feichcd from mcmory. 

9,3 RUN-TIME STORAGE MANAGEMENT 
As we saw in Chapter 7, the semantics of procedures in a language determines 
haw names are bound tu stwage during execution. Information needed during 
an execution of a procedure is kept in a block of storage called an activation 
remd; storage for names local to the procedure also appears in the activation 
record. 
In this section, we discuss what code to generate to manage activation 
records at run time. In Section 7.3, two s~andard storage-allocation strategies 
were presented, namely. static ahcalion and stack allmation. In static alloca- 
tion, the psilion of an activation record in memory is fixed at compile time. 
In stack allocation. a new activation record is pushed onto the stack for each 
cxccuticm of a procedure. The record is popped when the activation ends. 
Later in this section, we consider hnw the target c d e  of a prmedure can refer 
to data objects in ac~ivation rewrds. 
As we saw in Section 7.2, an ac~ivation record for a procedure has fields to 
hold parameters, rcsults, machine-status information, local data, temporaries 
and the like. In this section, we illustrate allocation strategies using the 
machine-status field to hold rhe return addrqs and the field for I ~ a l  
data. 
We assume the vther ficlds are handled as discussed in Chapter 7. 
Since run-time allocation and deallocation of activation records occurs as 
pard nf the prwedure call and return sequences, we focus OR the following 
three-address statements: 
I +  call, 
2. 
return, 
3. 
halt, and 
4. 
action. a phcehotder for uthcr staternenis. 
For exan~ple, thc the-address ctde for pruceriures c and p in Fig. 9.4 con- 
tains just thcx kinds uT statements, The size and layout of activation records 
are cwmrnunicatcd to the r d e  generator via the information abut names that 
is in the symbol table. Fur clarify, we show the byout in Fig. 9.4. rather than 
the form of !he y-nbol-table entries. 
Wc assume that run-time memory is divided into areas for code, static data. 
and a stack, as in Section 7.2 (the additional area for a heap in that section i s  
not used here). 
Static Allocatiun 
Consider the code nccded to impkment static aliocation. A call statement 
in the intcrrnediattt codc is implemented by a sequence of two target-machine 
instructhms. A MOV instruction saves the return address. and a GOTO 
transfers control to the target code for the called procedure: 

RUN-TiME STORAGE MANAGEMENT 523 
I 
action, 
call p 
I 
action:, 
I 
A~TIVAT~ON 
RECORD 
for c 164 bytcs) 
56: 
ACTIVATION R ECORD 
for g (88 bytes) 
0: K t u r n  address I 
I 
return 
Fig. 9.4. input to a code gcncraror. 
The attributes caldee.staric.~rcra and cubkr.code~reu are constants referring to 
the address of the activation record and the first instruction for the called pro- 
cedure, respectively. The source #herr+20 in the HQV instruction is the 
literal return address; it B the address of the instruction follrswing the GOTO 
instruction, (From the discussion in Section 9+2, the three constants plus the 
two instructions in the calling sequence cost 5 words or 20 bytes.) 
The d
e
 for a p r d u r e  ends with a return to the calling procedure, except 
that the first procedure has no caller, so its final instruction is HALT, which 
presumably returns control to the operating system. A return from procedure 
callee is implemented by 
which transfers control to the address saved at the beginning of the activation 
record. 
Eaample 9.1. The code in Fig. 9.5 is constructed from the procedures c and 
p in Fig.'9,4, We use the ~ I ~ U ~ O - ~ ~ S I T U C ~ ~ O ~  
ACTION lo implement the state- 
ment action, which represents three-address code that is not relevant for 
this discussion. We arbitrarily shrt the c d c  For these procedures at addresses 
100 and 200, respectively, and assume that each ACTION instruction takes 20 
bytes. The activatton records for the procedures arc statically allocated start- 
ing at lrwlation 300 and 364, respectively. 
The instructions starting at address 100 implement the statements 
of the first procedure c. 
Execulion therefore slarts with the instruction 
ACTIONl at address 100. The MOV instruction at address 120 saves the return 
address 140 in the machine-status field, which is the first word in the activa- 
tion record of p. The GOTO instruction at address 132 transfers control to the 
first instruction in the target code of the called procedure- 

/r codc for r: */ 
100; 
ACTION, 
120: Mw a140, 364 
savcrclurnadd~css140 
132; GOFO 200 
f *  call p r /  
140: ACTION;! 
1 
HALT 
. . .  
I* code for p + /  
200: ACTION, 
220: GOT0 h364 
I' 
rcturn In addrcss savcd in Imtian 364 */ 
. . .  
/* 364-45i hold activation ncord for p +/ 
/* return address +/ 
/* lord data for p */ 
Fig. 9.5. Targct d
c
 for thc input in Fig+ 9.4. 
S i n e  ,140 was saved at address 3M by the call sequence above, ~ 3 6 4  
represents 14 when the COTO statement st nhdress 220 is executed. Control 
therefore rct nrns to address 140 and execution of procedure c resumes. 
o 
Static allwa~ion can become stack allocation by using relative addresses for 
storage in activation records. The position of the record for an act ivatim of a 
procedure i s  not known until run time. In stack allocation, this position is 
usually stored in a register, so words in the activation mord can be acmssed 
as offsets from thc value in this register. The indexed address mode of our 
target machine is convcnicnt for this purpose. 
Relative addresses in an activation record can be taken as offsets from any 
known position in the activation record, as we saw in Section 7.3. For con- 
venience, we shall use positive offsets by maintaining ln a register SP a 
pintcr to the beginning of the activation record on top of the stack. When a 
proccdu rcr call occurs, the calling procedure increments SP and transfers con- 
trol to the called pr~cedure. After control returns to the caller, it decrements 
SF, thereby deallocating the activation record of the called 
' With ncgatiw o t l ' ~ ~ .  
wc wuld have SP poinr ta thc cnd of [he stack and haw lhc c ~ I c J  
ptu- 
rrrduru incremrnr SP . 

SEC. 9.3 
RUN-TIME STORAGE MANAGEMENT 
525 
The code for the first procedure initializes the stack by setling sP to the 
start of the slack area in memory: 
MOV #siur.ksinrr, SP 
/* initialiix the stack x/ 
code for the  fir^ procedure 
HALT 
/+ terminate execution */ 
A procedure call sequence increments SP. saves the return address, and 
transfers conlrol to the called procedure: 
The attribute ~.a/~er.ruc.rw~i.sizo 
represents the size of an activa~ion record, so 
the ADD instruction leaves SP ptnting to the beginning of the next activation 
record. The source #here + I6 in (he MOV instruction is the address of the 
instruction Fu!lowing the GOTO; ii is saved in the address pointed to by SP. 
The return sequence consists of two parts. The cailed procedure lransfers 
control to the rerurn address using 
GOTO + O I  SP 
I+ return to caller */ 
The reason for using * O  i sP 1 in the GOTO instruction is that we need two lev- 
els of indirection; O( sP) is the address of the first word in the activation 
record,and *O I SP 1 is the return address saved there. 
The second part of the return sequence is in the caller, which decrements 
SP, thereby restoring SP to its previous value. That is, after the subtraction 
SP points to the beginning of rhe activation record of the caller: 
A broader discussion of calling sequences and the tradeoCfs in  he division 
of labor between the calling and called procedures appzars in Section 7.3. 
E~arnple 9.2. The program in Fig. 9.6 is a condensation of Ihc three-address 
code far the Pascal program discussed in Section 7.1. Procedure q is recur- 
sive, .w morc than one activation oC q can bc alive at the same time. 
Suppose that the sizes of' the activation records for prwedures s, p, and q 
have been determined at compile rime to be ssirc, pix and q,rizr. respectively. 
The first word in each activation record will hold a return address. We arbi- 
trarily assume that the code for these prwcedures starts at addrcsscs 100. 200, 
and 300, respectively, and that the stack starts at W. The target code for the 
program in Fig. 9.6 is as follows: 

526 CODE GENERATION 
THREE- 
ADDRESS 
CODE 
/* code for s *# 
action] 
call q 
action2 
halt 
/* code for p */ 
a c t  ion, 
return 
f* code for q */ 
actlcq 
call p 
call q 
a c t i q  
call q 
Fig. 9.6. Three-Adress oode to illustrate stack allocation 
MOV X600, SP 
ACTION I 
ADD Xssize, SP 
MOV f l 5 2 ,  *SP 
GOTO 300 
SUB #$size, 8P 
ACTION2 
HALT 
. . .  
/+ call sequence begins *I 
/* push rHurn address */ 
l'+ a l l  g */ 
/* restore SP *I 
i* c d e  for p *I 
i* 
return */ 
I* code for q *I 
/+ conditional jump to 456 */ 
/* push return address */ 
I* call p *,' 

RUN-TIME STOR AGE MANAGEMENT 527 
MOV #396, *SP 
GOT0 300 
SUB #ysize, SP 
ACTIONh 
ADD #qsizc, SP" 
MOV %448, *SP 
GOTO 300 
SUB #ysizu, SP 
GOT0 *OISP) 
. , +  
/+ push return address */ 
/* call q */ 
/* push return address +/ 
/* call q */ 
/* stack starts here */ 
We aswmc that ACTION4 contains a wndilional jump to the address 456 of 
the return sequence from q; otherwise, the recursive procedure q is con- 
demned lo call itself forever. In an example below, we consider an execution 
of the program in which the first call or q does not return immediately, but all 
subwquen t calls do. 
I f  s s h ,  pize and qsirrr are 20, 40, and 60, respectively, then SF is initial- 
ized to 600, the starting address of the stack, by the first instruction at address 
100. SP holds 620 just before control transfers from s to q, because ssize is 
20. Subsequently. when q calls p, the instruction at address 320 increments 
SP to 680, where the activation record for p begins; s p  reverts to 620 after 
control returns to q. If the next two recursive calls of q return immediately, 
the maximum value of SP during this execution is 680. Note, however, that 
the last stack location used is 739, since the activation rewrd for q starting at 
location 680 extends for 64 bytes. 
D 
Run-Time Addresses for Names 
The storage-allocation strategy and the layout of local data in an activation 
record for a procedure determine how the storage for names is accessed. In 
Chapter 8, we assumed that a name in a three-address slatemen[ is really a 
pointer to a symbol-table enmy for the name. This approach has a significant 
advantage; it makes the wmpiler mare portable, since [ha front end need not 
be changed even if the compiler is moved to a different machine where a dif- 
ferent run-time organization is needed (e,g., the display can be kept in regis- 
ters instead of memory). On the other hand, generating thc specific sequence 
of access steps while generating intermediate code can be of significant advan- 
tage in an optimizing compiler, since it lets the optimizer take advantage of 
details it would not even see in the simple three-address statement. 
In either case. names must eventually be replaced by code to access storage 
locations. We thus consider some elaborations of the simple three-address 
copy statement x : = a. After the declarations in a procedure are processed, 
suppo* the symbol-table entry for x contains a relative address 12 for x. 
First consider the case in which x is in a statically allocated area beginning at 
address static. Then the actual run-time addtess of x is sturic + 12. Akhough 

528 
CODE GENERATION 
SEC. 9.3 
the compiler can eventually determine the value of siuric + 12 at compile time, 
the position of the static area may not be known when intermediate code ta 
access the name is generated. In that caw, it makes sense to generate three- 
address code to "ctlrnpu te" sfuric + 12. with the understanding that this com- 
putation will be carried out during the code-generation phase, or possibly by 
the loader, before the program runs. The assignment x : = 0 then ttanslates 
in to 
If the sratic area slam at address 100, the target code for this statement is 
Otl the other hand, suppose our language is one like Pascal and chat a 
display is used to access nonlwal names, as discussed in Seclion 7.4, Suppose 
also that the display is kep in registers, and that x is local to an active pro- 
cedure whose display pointer is in register R3. Then we may translate the 
copy x : = 0 into ihe three-address statements 
in which t, ccsntains the address of x. This sequence can be implemented by 
the single machine instruction 
MOV XO, 12(83) 
Notice that the value in register R 3  cannot be determined at compile time. 
9.4 BASIC BLOCKS AND FLOW GRAPHS 
A graph representation of three-address statements, called a flow graph, is 
useful for understanding code-generation algorithms, even if the graph is not 
explicitly constructed by a code-generatiun algorithm. N d e s  in the flow 
graph represent computations, and the edges represent the flow of conlrd. In 
Chapter 10, we use the flow graph af a program extensively as a vehicle to 
collect information a
h
 
t the intermediate program. Some register-assignment 
algorithms use flow graphs to find the inner loops where a program is 
expected FO spend most of its time. 
A busic bI1~'k is a sequence of consecutive stalernents in which flow of control 
enters at the beginning and leaves at the end without halt or possibility of 
branching except at the end, The following sequence of three-address state- 
ments forms a basic block: 

BASIC BLOCKS AND FLOW GRAPHS 529 
(9.1) 
A three-address statement x : = y + z is said to define x and to use {w 
rt$ererrrn) y and z. A name in a basic block is said to be liw at a given point 
if its value is used after that point in the program, perhaps in another basic 
block, 
The following algorithm can be used to partition a sequence of three- 
address statements into basic blocks. 
Algorithm 9.1. Partition into basic bbcks. 
Iwut. A sequtnce of t hree-address statements. 
O ~ p u t .  A list of basic blocks with each three-address statement in exactly one 
block. 
1. We first determine the set of leabers, the first statements of basic blocks. 
The rules we use arc the following. 
i) 
The first statement is a bader. 
il) 
Any statement that is the target of a conditional or unconditional 
g t o  is a leader. 
iii) Any statement that immediately f~llaws s goto or aditional goto 
slatement is a leader. 
2, 
For each Icader, its basic block consists of the leader and all statements 
up to but not including the next leader or the end of the program, 
ri 
E q b  9,3. 
Consider the fragment of source code shown in Fig. 9.7; it 
computes the dot product of two vectors r and b of length 20. A list of 
threeaddress statements performing this computation on our target machine is 
shown in Fig. 9.8, 
beg in 
prod :a 0 ;  
i := 1; 
do begin 
prod := prod + a[il w b[il; 
i : = i + l  
end 
while i c= 20 
end 
Fig. 9.7. Program to compute dot product. 

530 
CODE GENERATION 
SEC. 9.4 
Let us apply Algorithm 9.1 to the three-address code in Fig. 9.8 to deter- 
mine its basic blocks. Statement (I) is a leader by rule (i) and statement (3) 
is a leader by rule (ii), since the last statement can jump to it. By rule (iii) 
the statement following ( 12) (recall that Fig. 9+13 is just a fragment of a pro- 
gram) is a leader. Therefore, statements ( I) and (2) form a basic block, The 
remainder of the program keginning with statement (3) forms a second basic 
block. 
o 
pxou : = 0 
i := 1 
t, := 4 * i 
t2 := a I t, I 
t3 := 4 * i 
t, := b I t, I 
tS := tl 
t4 
th :a prod + t5 
prod : = th 
t7 := i t 1 
1 :* t, 
if i a= 20 goto ( 3 1  
Fig. 9.8. fhrm-addrcss codc computing dot produd. 
Transformatioms on Basic Blocks 
A basic block computes a set of exprc~sions, These expressions are the values 
of the names live on exit from the block. Two basic blocks are said to be 
cyuivul~nt if they compute the same set of expressions. 
A number of transformations can be applied to a basic blmk without chang- 
ing 1he set of expressions computed by the block. Many of these transforma- 
tions are useful for improving the quality of code that wit1 be ultimately gen- 
erated from a basic block. In the next chapter, we show how a global code 
"optimizer'+ tries to use such transformations to rearrange the computatioas in 
a program in an effort to reduce the overall running time or space require- 
ment of the final target program. There arc two important classes of local 
transformations that car be applied to basic blocks; these we the structure- 
preserving transformations and the algebraic transformations. 
Structure-Preserving TracrsCwmations 
The primary structure-preserving transformations on basic blocks are: 
I. common subexpression elimination 
2. 
dead-code elimination 
3+ renaming of ternprary variables 
4. 
interchange of two independent adjacent statements 

SEC. 9.4 
BASK BLOCKS AND FLOW GRAPHS 531 
Let us now examine thee transformations in a little more detail, For the time 
being, we assume basic blocks have no arrays, pointers, or procedure calls. 
1. Common sukxpression elimination. Consider the basic block 
The second and fourth statements compute the same expression, namely, 
b+c-d, and'hence this basic Mmk may be transformed into the equivalent 
block 
Note that although the first and third statements in (9.2) and (9.3) appear 
to have the same expression on the right, the second statement redefines b. 
Therefore, the value of b in the third statement is different from the value of 
b in the first, and the first and third statements do not compute the same 
expression. 
2, Dmd-code -eiiminarion. Suppe x is dead, that is, never subsequently 
used, at the point where the statement K : = yf z appears in a basic block. 
Then this statement may be safely removed without changing the value of the 
basic block. 
3. 
R e m i f i g  iemprary 
variabh. 
Suppse we have a 
statement 
t : = b f c ,  where 
t is a temporary. 
If we change this statement to 
u : = b t  e, where a is a new temprary variable, and change all uses of this 
inhanm of t to u, then the value of the basic blmk is nor changed. In fact, 
we a n  always tra~sforrn a basic block into an equivalent blwk in which each 
statement that defines a temprary defines a new temporary. We call such a 
basic block a rwrmal-jorm block. 
4. Inrerchnge of sruremmrs. Suppose we have a block with the two adja- 
cent statements 
Then we can interchange the two statements without affeaing the value of the 
block if and only if neither x nor y is tl and neither b nor e is t2. 
Notice that a normabform basic block prmits all statement interchanges that: 
are possible. 

532 
CODE GENERATION 
Countless algebraic transformalions can t>e used to change the set of expres- 
sions computed by a basic block into an algebraically equivalent set. The use- 
ful ones are those that simplify expressions or replace expensive operations by 
cheaper ones. For example, statements such as 
can be eliminated from a basic block without changing the set of expressions it 
computes. The exponentiation operator in the statement 
usually requires a function call to implement. Using an algebraic transforma- 
tion, this statement can be replaced by the cheaper, but equivalent statement 
Algebraic transformations are discussed in more detail in Section 9.9 m 
peephole optimization and in Section 10.3 on optimization of basic blocks. 
Flow Graphs 
We can add the flow-of-control informat.ion to the set of basic blocks making 
up a program by constructing a directed graph called a fkuw graph. The nodes 
of the flow graph arc the basic blwks. One node is distinguished as iniiid; it 
is the block whose leader i s  the first statement. Thcre is a directed edge from 
blmk B 
to block B2 if B2 can immediately follow B 
in some execution 
xquence; that is, if 
I. there is a conditional or unconditional jump from the last statement of 3 I 
to the first statement of B 2 ,  or 
2. Bz immediately follows B I in the wder of the program, and 8 does not 
end in an unconditional jump. 
Example 9.4. The flow graph of the program of Fig. 9.7 is shown in Fig. 9.9. 
81 is the initial node. Note that in the last statement, the jump lo statement 
( 3 )  has k e n  replaced by an equivalent jump to the beginning of block B z .  
Representation of Bask Blocks 
Basic blocks can be represented by a variety of data structures. For example, 
after partitioning the three-address stalemcnts by Algorithm 9.1, each basic 
block can be reprewmd by a record consisting of a count of the number of 
quadruples in the block. followed by a pointer to the leader (first quadruple) 

SEC. 9.4 
: t5 := t2 * t4 
th := prod + t5 
prod := tb 
tl := i + 1 
i := tt 
prod :s 0 
i 1 -  - 
1 
Flg. 9.9. Flow graph for program. 
B I  
of the block. and by the lists of predecessors and successors of the block. An 
alternative i s  to make a Enked list of the quadruples in each block. Explicit 
references to quadruple numbers in jump statements at the end of basic blocks 
can cause problems if quadruples are moved during code optimization, For 
example, i f  the block B2 running from statements (3) through (12) in the 
intermediate code of Fig. 9.9 were moved elsewhere in the quadruple array or 
were shrunk, the (3) in i f  i 
<= 20 gotci(3) would have to k changed. 
Thus, we prefer to make jumps point to bbcks rather than quadruples, as we 
have done in Fig. 9.9. 
It is important to note rhat an edge of the flow graph from block B to block 
B' does not specify the conditions under which control flows from 8 to B'. 
That is. the edge does nor lell wheiher the conditional jump at the end of B (if 
there is a conditional jump there) gms to the leader of 8' when the condition 
is satisfied or when the wnditim i s  not satisfied. That information can be 
recovered when needed from the jump statement in 8. 
In a flow graph, what is a loop, and how does one find all loops? Most of the 
time, it is easy to answer these questions. For example, in Fig. 9.9 there is 
one loop, consisting of block B 2 .  The general answers to these questions, 
however, are a bit subtle, and we shall examine them in detail in the next 
chapter. For the present, it is sufficient to say that a loop is a mlkction of 
rides in a flow graph such that 
1 .  
All nodes in the collection are strongly connectd; that is, from any node 
in the Imp to any other, there i s  a path of length one or more, wholly 
within the Imp, and 

534 CODE GENERATION 
E C +  9.4 
2. 
The collection of nodes has a unique entry, that is, a node in the Imp 
such that the only wag to reach a node of the loop from a node outside 
the loop is to first go through the entry. 
A loop that contains no other loops i s  called an inner loop. 
9.5 NEXT-USE INFORMATION 
In this section, we collect next-use information about names in basic blocks. 
If the name in a register is no longer needed, then the register can be assigned 
to some other name, This idea of keeping a name in storage only if it will be 
used subsequently can be applied in a number of contexts, We used it in Sec- 
t ion 5.8 to assign space for attribute values. The simple code generator in the 
next section applies it to register assignment. As a final application, we con- 
sider the assignment of storage for temporary names. 
Computing Next Uses 
The use of a name in a three-address statement Is defined as fo1bws. Suppose 
three-address statement i assigns a value to x. if statement j has x as an 
operand, and control can flow from statement i to j along a path that has no 
intervening assignments to x, then we say statement j uses the valuc of x 
computed st i A  
I 
We wish to determine for each three-address statement x : = y up z what 
the next uses of x. y, and z are. For the present, we do not concern our- 
selves with uses outside the basic block containing this three-address statement 
but we may, if we wish, atternpi to de~errnine whether or not there is such a 
use by the Live-variable analysis technique of Chapter 10. 
Our algorithm to determine nexl uses makes a backward pass over each 
basic block. We can easily scan a stream of three-address statements to find 
the ends of basic blocks as in Algorithm 9.1. Since procedures can have arbi- 
trary side effects, we assume for convenience that each procedure call starts a 
new basic block. 
Having found the end of a basic block, we scan backwards to the beginning, 
recording (in the symbol table) for each name x whether x has a next use in 
the block and if not, whether it Is live on exit from that block. If the data- 
flow analysis discussed in Chapter 10 has been done, we know which names 
are live on exit from each block. If no live-variabk analysis has been done, 
we can assume all omtemporary variables are live on exit, to be conservative. 
If the algorithms generating intermediate code or optimizing the d
e
 
permit 
certain temporaries to be used across blwks, these too must be considered 
live. It would be a good idea to mark arty such temporaries, so we do not 
have to consider all temporaries live. 
Suppose we reach three-address statement i: x := y op z in our backward 
scan, We then do the following. 
I. Attach to statement i the information currently found in the symbol table 

SEC. 9.6 
A SIMPLE CODE GENERATOR 
535 
regarding the next use and liveness of x. y, and z . ~  
2. In the symbol table, set x to "not live" and "no next use." 
3. 
In the symbol table, set y and z to "live" and the next uses of y and z 
10 i. Note that the order of steps (2) and (3) may not be interchanged 
kause x may be y or z. 
If three-address statement i is of the form x := y or x : = up y, the steps 
are the same as above, ignoring 2. 
Storage for Tempomy Names 
Although it may be useful in an optimizing compiler to create a distinct name 
each time a temporary is needed (see Chapter 10 for justification), space has 
to be allocated to hold the values of these ternparits. The size of the field 
for temporaries in the general activation record of Section 7.2 grows with rhe 
number of tempraries, 
We can, in general, pack two temporaries into the same location if they are 
not live simultaneously. Since almost all temporaries are defined and used 
within bask blwks, next-use information can be applied to pack temporaries. 
For temporaries that are used across blocks, Chapter 10 discusses the data- 
flow analysis needed to compute livcness. 
We can albcate storage locations for temporaries by examining each in turn 
and assigning a temporary to the first location in the field for temporaries that 
docs not contain a Iive temporary. If a tempwary cannot be assigned to any 
previously created location, add a new location to the data area for the current 
procedure. In many cases, temporaries can be packed into regiskrs' rather 
than memory locations, as in the next section. 
For example, the six tempraries in the basic block (9.1) can be packed into 
two locations. These locations correspond to t, and t2 in: 
9.6 A SIMPLE CODE GENERATOR 
The code-generation strategy in this section generates target code for a 
sequence of thrw-address statement, It considers each statement in turn. 
remembering if any of the operands of the statement are currently in registers, 
and taking advantage of that fad if possible. Far simplicity, we assume that 
- .- 
' If x is not live, then this statemcnt can k dclctcd; riuch ttansforrnaibms arc tcrmadcrcil in Scc- 
lion 9.8. 

536 CODE GENERATION 
SEC. 9.6 
for each operator in a statement there is a corresponding target-language 
operator. We also assume that computed results can tw left in registers as 
long as possible, storing them only (a) if their register is needed for another 
computation or (b) jusi before a procedure call, jump, or labeled statement.' 
Condition (b) implies that everything must k stored just before the end of 
a basic block,"The 
reason we must do so is that, after leaving a basic block, 
we may be able to go to several different blacks, or we may go lo me particu- 
lar block that can be reached from several others. In either case. we cannot, 
without extra effort, assume that a datum used by a block appears in the same 
register no matter haw control reached that block. Thus, to avoid a possible 
error, our simple cde-generation algorithm stores everything when moving 
across basic-block boundaries as well as when p r d u r e  calls are made. 
Latw we consider ways to hold some data in registers across block bun- 
darks. 
We can produce reasonable code for a three-address slatement a : 
= b + c if 
we generate ihe single inhruction ADD R j ,  Ri with cost one, leaving the 
result a in register Ri, This sequence is possible only if register Ri contains 
b, ~j contains c ,  and b i s  not live after the statement; that is, b is not used 
after the statement. 
If Ri crsntains b but c is in a memory location (called c for convenience), 
we can generate Ihe sequence 
ADD 
c ,  Ri 
cost = 2 
HOV 
c , R j  
ADD 
R j, Ri 
cost = 3 
provided b is not subsequently live, The second sequence becomes attractive 
if this value of c is subsequentiy used, as we a n  then take its value from 
register R j+ There are many more cases 10 consider, depending on where b 
and c are currently located and depending on whether the current value of b 
is subsequently used. We must also consider the cases where one or both of 
b and c is a constant. The number of cases that' need to be considered 
further increases if we assume that [he operator + is commutative, Thus, we 
see that code generation involves examining a large number of cases, and 
which case should prevail depends on the context in which a three-address 
statement is seen. 
Ht~wcvar. to pr~dllcc a s.whdi<* 
Any,. which makcx avrilabk thc valucs d mcmory Iwarubns 
and rcgiMcrs in tmms vf thc source program's nrmcs fur thcsc valucsii, il may bc motc mvcnicnt 
ra b v c  prt%rnmmcr-dcfincd variabh Ibul nol noocsmrilg mmpikr-gcncralcd tcmporarics) storcd 
irnrnodiatcly u p n  cirkulntion. uhould a program error suddcnly cause a precipitous intcrrupr and 
cxir . 
* Narc WC arc not assuming thwr ~ h c  
quadruples wcrc aclually partitioned inlo basic blocks by rhc 
wmpilcr: thc n a t ~ m  
of a basic bluck i s  usful conceptually in any cvcnt. 

A BMPLE CODE GENERATOR 
537 
i 
Register a d  Address Descriptors 
The code-generation algorithm uses descriptors to keep track of register con- 
tents and addresses for names, 
1 .  
A register descriptor keeps track of what is currently in each register. It 
is consulted whenever a new register is needed. We assume that initially 
the register descriptor shows; that all registers are empty. (Zf registers are 
assigned across blocks, this would not be the case.) As the code genera- 
tion for the block progresses, each register will hold the value of zero or 
more names at any given time. 
2. 
An address descriptor keeps track of the location Tar locations) where the 
current value of the name can be found at run time. The locarion might 
be a register, a stack location, a memory address, ur some set of these. 
since when copied, a value also stays where it was, This information can 
be stored in the symbol table and is used ro determine the accessing 
method for a name. 
A Cde-Generation Algorithm 
The code-generation algorithm takes as hput a sequence of t hree-address 
statements constitu~ing a basic b h k A  For each threeaddress statement of the 
form x : = y op z we perform ihe folbwing actions: 
t . 
Invoke a function getreg to determine the location L where the result of 
the computation y up E should be stored, L will usually be a register, but 
it auld also be a memory location. We shall describe prrq sshortiy. 
2. 
Consult the address descriptor for y to determine y ' . {one of) the current 
locatim(s) of y+ Prefer the register fur y' if the value of y is currently 
h t h  in memory and a register. If the value of y is not already in L. gcn- 
mite the instruerim MOV y' ,t to place a copy of y in L. 
3. 
Generate the instruction QP z' ,L where z' is a currcnt location of z. 
Again, prefer a register t o a  memory Iwatim if z is in both. Update the 
address descriptor of x to indicate chat x is in Iication L. If L is a regis- 
ter, update its descriptor to indicate that it contains the value ul x, and 
remove x from all uther register descriptors. 
4. If the current values of y and/or z have no next uses, are not live on exit 
from the block, and are in registers, alter the register descriplor to indi- 
cate that, after execution of x : = y up z, those registers no longer will 
contain y and/or z, respectively. 
If the current three-address statement has a unary operator. the steps are 
analogous to those above. and we omit the details. An important special case 
is a three-address shtement x : = y. If y is in a register, simply change the 
register and address descriptors to record that the value of x is now found 
only in the register holding the value of y. If y has no next use and is not 

538 
CODE GENERATION 
SEC. 9.6 
live on exit from the block, the register no longer holds the value of y. 
If y is only in memory, we could in principle record that the value of x is 
in the location of y, but this option would complicate our algorithm, since we 
could not then change the value of y without preserving the value of x. 
Thus, if y is in memory we use grtreg to find a register in which to load y 
and make that register the location of x. 
Alternatively, we can generate a MOV y , x instruction, which would be 
preferable if the value of x has no next use in the block. It is worth noting 
that most, if not all, copy instructions will be eliminated if we use the block- 
improving and copy-propagation algorithm of Chapter 10. 
Once we have prmstied all three-address statements in the basic block, we 
store, by MOV instructions, those names that are live on exit and not in their 
memory locations. To do this we use the register descriptor to determine 
what names are left in registers, the address descriptor to determine that the 
same name i s  not already in its memory Location, and the live variable infor- 
mation to determine whether the name is to be stored. If no live-variable 
infarmation has been computed by data-flow analysis among Mocks, we must 
assume all user-defined names are live at the end of the black, 
The function Reire# returns the location L to hold the value oE x for the 
assignment x : = y op z. A great deal of effort can k expended in irnpk- 
menting this function to produce a perspicacious choice for L. In this section, 
we discuss a simple, easy -to-implement scheme based on the next-use in forma- 
tion mtlccted in the last section, 
1. 
If the name y  is in a register that holds the value of no other names 
(recall that copy instructions such as x : = y could cause a register to 
hold the value of two or more variables simultaneously), and y is not 
live and has no next use after execution af x : = y up z ,  then return 
the register of y For L. Update the address descriptor of y to indicate 
that y i s  no I~nger in L. 
2. 
Failing I I ), return an empty register for L if there is one. 
Failing (2), if x has a next use in the block, or op is an operator, such as 
indexing, that requires a register, find an mupied register R. Sore the 
value of R into a memory location (by MCW R, M) if it is not already in 
the proper memory Imtion M, update the address descriptor for M, and 
return R. If R holds the value of several variables, a MOV instruction 
must he generated for each variable that needs 10 k stored. A suitable 
occupied register might be one whose datum is referenced furthest in the 
future, or me whose value is also in memory. We leave the exact choice 
unspecified. since there i s  no one proven k s t  way to make the selection. 
4. 
If x i s  not used in the block, or no suitable occupied register can be 
found, select the memory location of JC as L. 

SEC. 9.6 
A SIMPLE COVE GENERATOR 
539 
A more sophisticated p t r q  function would also consider the subsequent 
uses of x and the commutativity of the operator up in determining the register 
to hold the value of x. We leave such extensions of gerrq 2s exercises+ 
Errample 9.5. 
The assignment d := {a-b) + l a - c ]  + ( a - c )  might be 
translated into the following three-address code sequence 
t : = a - b  
u : = a - c  
v : = t + u  
d : = v + u  
with d live at the end, The cods-generation algorithm given above would pro- 
duce the d
e
 sequence shown in Fig. 9.10 for this three-address statement 
.sequence, Shown alongside are the values of the register and addres &scrip- 
tws as code generation progresses. Not shown in the address descriptor is the 
fact that a. b, and c are always in memory, We also assume that t, u and 
v, being temporaries, are not in memory unless we explicitly store their values 
with a MQV instruction. 
MOV a, R1 
SUB C ,  R1 
ADD R1, RO 
ADD R1. RO 
rcgistcrs crnpty 
RO contains t 
RO contains t 
R1 contains u 
RO contains v 
R l  contains u 
RO contains d 
Fig, 9.10. Cudc scqucncc. 
The first call of grtrcy returns RO as the location in which to compute t, 
Since a is not in RO, we generate instructions MOV a, RO and SUB b, RO. 
We now update the register descriptor to indicate that RD contains t. 
Code generation proceeds in this manner until the last three-address state- 
ment d : = v + u has been processed. Note that R 1 becomes empty becaux u 
has no next use. We then generate M0v RO, d to store the live variable d at 
the end of the block, 
The cost of the code generated in Fig. 9.10 is 12. We could reduce this to 

I I by generating MOV RO, R ?  immediately after the first instruction and 
removing the instruction MW a, R1, but to do so requires a more wphisti- 
rated code-generation algorithm. The reason for the savings i s  that it is 
cheaper to load ~1 from RO than from memory. 
a 
Generating Code for Other Types of Statements 
lndexing and pointer operat ions in three-address statements are handled in the 
same manner as binary operarions. The table in Fig. 9+11 shows the c-Me 
sequences generated Tor the indexed assignment slatements a : = bIi 1 and 
a[i 3 
:= b, assuming b is statically allocated. 
The current karion of i determines thc code sequence. Three cases are 
covered depending on whether i is in register Ri, whether i is in memwy 
location Mi. or whether i 
is on the stack at offset S i  and the pointer to the 
activation record for i is in register A. The register R is the regis.ter returned 
when the function g r t r q  is called. For the first assignment, we would prefer 
ro kaue a in register R if a has a next use in the block and register R is avail- 
able. In the second assignment. we assume a is statically allocated. 
The table in Fig. 9.12 shows the code .wqaems generated for !be pointer 
assignments a : = +p and +p : = a- Here, the current location of p deter- 
mines the code xquence, 
a[i]:=b 
MOV b,alRil: 
STATEMENT 
Fig. 9. lh Cwk xqucnccs fur pintcr essignrncn ts. 
Three cases are covered depending on whether p i s  initially in register Rp, 
whether p is in rncmmy location Mp, m whether p is on the stack at offset Sp 
3 
 INR RE GIST ERR^ 
, '  
CODE 
Cosr 
! 
1 
a: =+p 
2 
MOV *Rp,a 
MOV Mi,R 
m>V b,aIR) 
~ I N M E M O R Y M ~  
CODE 
C r m  
1 
p IN 9 n c ~  
CODE 
C O ~ T  
I 
MOV Up ,R 
HOV *R,R 
MOV Mp, R 
*p: =a 
' 
MOV a,*R 
MOV a,+Rp 
2 
3 
MOV SiIA1,R 
M V  b.a(R) 
4 
5 
MOV S p I A l  ,R 
MOV +R+R 
3 
NOV a,R 
MOV R,*Sp(AJ 
; 
4 
- 

SEC+ 9.7 
REGISTER ALLOCATION AND ASSIGNMENT 
541 
and the pointer to the activation record for p is in register A. The register R 
is the register returned when the function gcwq is called. In the second 
assjgnment, we assume a is statically allocated. 
Condi tbnd Statements 
Machines implement conditionat jumps in one of two ways. One way is to 
branch if the value of a designated register meets one of the six conditions: 
negative, zero, positive, nonnegative, nonzero, and nonpositive, On such a 
machine a three-address svatement such as if x < y goto z can be imple- 
mented by subtracting y from x in register R. and then jumping to z if the 
value in register R is negative. 
A second approach, common to many machines, uses a set of condition 
d e s  to indicate whether the last quantity computed w loaded into a register 
is negative, ~e.ero, or positive. Often a compare instruction (CMP ln our 
machine) has the desirable property that ir sets the condition code without 
actuaily computing a value. That is, CMP x, y sets the condition code to 
positive if x t y, and so on. A conditional-jump machine instruction makes 
the jump if a designated condition <. = ?  >, 5 ,  # ,  or 2 is met. We use the 
instruction CJ<= z to mean "jump to z if the condition ccide is negative w 
zero." For example, if x . 
y goto z could be implemented by 
If we are generating code for a machine with condition cdes it is useful to 
maintain a condition-cube descriptor as we generate code. This descriptor 
tells the name that last set the condition code, or the pair of names compared. 
if the condition code was last set that way. Thus we could implement 
MOV 
y, RO 
ADD 
z, RD 
MOV 
RO, x 
CJ< 
2 
if we were awarc that the condition code was determined by 
x after 
ADD Z, RO. 
9,7 REGISTER ALLOCATION AND ASSIGNMENT 
Instructions involving only register operands arc shorter and faster than those 
involving memory operands. Therefore, efficient utilization of registers is 
important in generating good code. This section presents various strategies 
for deciding what values in a program should reside in registers (register 

542 
CODE GENERATION 
SEE. 9.7 
allocation) and in which register each value should reside (register assign- 
ment). 
One approach to register allocation and assignment is to assign specific 
values in an object program to certain registers. For example, a decision can 
be made to assign base addresses to one group of registers, arithmetic compu- 
tation to another, the top of the run-time stack tr, a fixed register, and so on+ 
This approach bas the advantage that it simplifies the design of a compiler, 
Its disadvantage i s  that, applied tot, strictly, it uses registers inefficiently; cer- 
tain registers may go unused over substantial p t i o n s  of code, while unneces- 
sary loads and stores arc generated. Nevertheless, it is reasonable in most 
computing environments to reserve a few registers for base registers, stack 
pointers and the like, and to allow the remaining registers to be used by the 
compiler as it sees fit. 
Global Register Allocatiim 
The code-generation algorithm in Section 9.6 wed registers to hold values for 
the duration of a single basic biock. However, all live variables were stored at 
the end of each block. To save some of these stores and corresponding loads, 
we might arrange to assign registers to frequently used variables and keep 
these registers mnsistcrtt across block b n d a r i e s  (ly&ohlly), Since programs 
spend m s t  of their time in inner !clops. a natural approach to gbbal register 
assignment is to try to keep a Frequently used value in a fixed register 
throughout a loop. For the time king, assume that we know the loop struc- 
ture of a flow graph, and that we know what values computed in a basic block 
are used outside that block. The next chapter covers techniques for comput- 
ing this information. 
One strategy for gbbal register allocation is to assign some fixed number of 
registers to hold the most aclive values in each inner loop. The selected 
values may be different in different loops. Registers not already allocated 
may be u.wd to hold values local to one block as in Section 9.6. This 
approach has the drawback that the fixed number of registers is not always the 
right number to make available for global register allmation. Yet the method 
is simple to implement and it has been used in ForIran H ,  the optimizing For- 
tran compiler for the IBM-360 series machines (Lowry and Medlock 1 19691). 
In languages like C and Bliss, a programmer can do some register allocation 
directly by using register declarations to keep certain values in registers for 
the duration of a procedure. Judicious use of register declarations can speed 
up many programs, but a programmer should not do register allocation 
without first profiling the prbgram. 
Usage Counts 
A simple method for determining the savings to k realized by keeping vari- 
able x in a register for the duration of Imp L is to recognize that in our 
machine model we save one unit of cost fur each reference to x if x is in a 

SEC. 9.7 
REGWER ALLOCATION AND ASSIGNMENT 
543 
register. However, if we use the approach of the previous section to generate 
code for a block, there is a g
d
 
chance that after x has h e n  computed in a 
block il will remain in a register if there are subsequent uses of x in that 
block. Thus we count a savings of one for each use of x in Imp L that is not 
preceded by an assignmen I to x in the same block. We also save two units if 
we can avoid a s t h e  o l x  at the end of a block. Thus if x is allmated a regis- 
I&, count a savings of two for each block of L for which x is live on exit and 
in which x is assigned a value, 
On the debit side, if x is live on entry to the Imp header, we must load x 
into its register just before entering Imp L. This bad costs two units. Simi- 
larly, for each exit block 3 of Imp L at which x is live on entry to some suc- 
cessor of B outside of L, we must stwe x at a cost of two. However, on the 
assumption that the loop is iterated many times. we may neglect these debits 
since they occur only once each time we enter the loop. Thus an approximate 
formula for the benefit to be realized from allocating a register to x within 
loop L is: 
where U S P ( X ,  8 )  is the number of times x is used in B prior to any definition 
of x; iive(x, B) is 1 if x is live on exit from B and is assigned a value in R ,  and 
LiveIx, 3) is 0 otherwise. N ~ t e  
that (9+4) is approximate, because not all 
blocks in a loop are executed with equal frequency and also becam (9.4) was 
based on the assumption that: a loop is iterated "many" times. On other 
machines a formula analogous to (9.41, but possibly quite different from it, 
would have to be developed. 
Example 9.6, 
Consider the basic Mocks in the inner Imp depicted in Fig. 
9.13, where jump and conditional jump statements have been omitted. 
Assume regisrtrs RO, R1, and R 2  art allocated to hdd values throughout the 
Imp. Variables live on entry into and on exit from each block are shown in 
Fig. 9.13 for convenience, immediately above and below each block. respec- 
tively. There arc some subtle points a b u t  live variables that we address in 
Chapter 10. For example, notice that both e and f are live at the end of B , ,  
but of these, only e is live on entry to B2 and only f on entry to B J .  In gen- 
eral. the variables live at the end of a block ace the union of those live at the 
beginning of each of i t s  successor blocks, 
To evaluate (9.4) for x = a we observe that a is live on exit from B I and is 
assigned a value there, but is not live on exit from B 2 ,  B 3 ,  or B,. Thus, 
2*live(a, 8 )  = 2. Alsa, u s ~ ( a ,  B I )  = 0, since a is defined in B I  
B inL 
before any u s .  A h ,  use(a, B , )  = use{a. B 3 )  - 1 and usr(a, B4) = 0. 
Thus, 
us~(a, B) = 2. Hence the value of (9.4) for x = a is 4. That is, 
B in L 
four units of cost can be saved by selecting a for one of the global registers. 
The values of (9.4) for b, e, dl e, and f are 6, 3, 6, 4, and 4, respectively. 

SEC. 9.7 
a : = b +  c 
d : = d - b  B, 
e : = a + f  
acdf 
b : = d + f  
e : = a - c  B3 
bcdef 
b , d , e , f  live 
b , c , d , e + f  live 
mg. 9.13. Flow graph of an tnncr loop. 
Thus we may select a, b, and d, for registers RO, R1, and R2, respec- 
tively. Using RO for e or f instead of a would be another choice with the 
same apparent benefit. Fig. 9.14 shows the assembly code generated from 
Fig+ 9,13, assuming that the strategy of Section 9.6 i s  used to generate d
e
 
for each block. We do not show the generated d
e
 for the omitted condi- 
tional or unconditional jumps that end each block in Fig. 9.13, and we there- 
fore do not show the generated code as a single stream, as it would appear in 
praaice. It is worth noting that, if we did not adhere strictly to our strategy 
of reserving RO, R 1, and R2, we could use 
for B 2 ,  saving a unit since a is not live on exit from B * .  A similar saving 
c ~ u l d  be realized at B 3 .  
u 
Register Assignment for Outer h p s  
Hav~ng assigned registers and generated cude for inner loops, we may apply 
the same idea to progressively larger loops. If. an outer Imp L I contains an 
inner loop L 2 ,  the names atlocated registers in L2 need not be allocated regis- 
ters in L I -Lz. However, if name x is allocated a register in loop L I but not 
L2, we must store x on entrance to L2 and load x if we leave L2 and enter a 
block of L I - L2. Similarly, if we choose to allocate x a register in L2 but not 
L I we must load x on entrance to L and store x on exit from L2. We leave 
as an exercise the derivation of a criterion for selecting names to be allocated 
registers in an outer loop L, given that choices have already been made for all 
Imps nested within L. 

REGISTER ALLOCATION AND ANGNMENT 545 
I 
MOV b,R1 
MOV d,R2 I 
Fig. 9.14. C d e  sequence using global register assignment 
Register AlhsSTon by Graph Coloring 
When a register is needed for a computation but all available regisrers are in 
use, the contents of one of the used registers must te stored (~ptpillrd) into a 
memory location in order to free up a register. Graph coloring is a simple, 
systematic technique for allmating registers and managing register spills. 
In this method, two passes are used. In the first, target-machine instruc- 
tions are selected as though there were an infinite number of symbolic regis- 
ters; in effect, names used in the intermediate c d e  become names of registers 
and the three-address statements become machine-language statements. If 
access to variables requires instructions that use stack pointers, display 
pointers, base registen, or other quantities that assist access, then we assume 
that these quantities are held in registers reserved for each purpose. Nor- 
mally, their use is directly translatable into an access mode for an address 
mentioned in a machine instruction. If access is more complex, the access 
must be broken into several machine instructions, and a temporary symblic 
register (or several) may need to be created. 

546 
CODEGENERATION 
sec. 9.7 
Once the instructions have been selected, a second pass assigns physical 
registers to symbolic ones. The goal is to find an assignment that minimizes 
the coa of the spills. 
In the second pass, for each procedure a regis~er-lnterfermce graph is con- 
structed in which the nodes are symbolic registers and an edge connects two 
nodes if one is live at a point where the other is defined. For example, a 
rcgister-interference graph for Figure 9.13 would have nodes fw names a and 
d. In block B ,, a is live at the second statement, which defines d; therefore, 
in the graph there would be an edge between the nodes for a and d. 
An attempt is made to color the register-inrerference graph using k cobrs, 
where k is the nurnbcr of assignable registers. (A graph i s  said to be colored 
if each node has been assigned a mlor in such a way that no two adjacent 
nodes have the same color.) A color represents a register, and the cobring 
makes sure that no two symbolic regisrers that can interfere with each other 
are assigned the same physical register. 
Although the problem of determining whether a graph is k-crslorable is NP- 
complete in general, the following heuristic technique can usually be used to 
do the coloring quickly in practice. Suppse a node n in a graph G has fewer 
than k neighbors (nodes connected to u by an edge). Remove n and its edges 
from G to obtain a graph G'. A kaloring of G' can be extended to a k- 
coloring of G by assigning n a color not assigned to any of its neighbors. 
By repeatedly eliminaling nodes having fewer than k edges from the 
register-interference graph. either we obtain the empty graph, in which case 
we can produce a k-coloring for the original graph by coloring the nodes in the 
reverse order io which they were removed, or we obtain a graph in which 
each node has k or more adjacent nodes, In the latter case a &-coloring is no 
longer possibie. At this point a node is spilled by introducing code to store 
and reload the register. Then the interferenw graph is appropriately modified 
and the coloring process resumed. Chaitin 119821 and Chaitin et at. 11981 J 
describe several heuristics for choosing the node to spill. A general rule is to 
avoid introducing spill c d e  into inner Imps. 
9.8 THE DAG REPRESENTATION OF BASIC BLOCKS 
Directed acyclic graphs (dags) are useful data structures for implementing 
transformations on basic blocks. A dag gives a picture of how the value corn- 
puted by each statement in a basic block is used in subsequent statements of 
the block. Constructing a dag from threeaddress statements is a good way of 
determining common subexpre&ons (expressions computed more than once) 
within a block. determining which names are used inside the block but 
evaluated outside the block, and determining which statements of the block 
could have [heir computed value used outside the block. 
A dugfur a hsic block (or just dug) is a directed acyclic graph with the fol- 
lowing lakls on nodes: 
I .  Leaves are labeled by unique identifiers, either variable names or 

SEC. 8+8 
THE DAG REPRESENTATION OF BASK BLOCKS 547 
constants, From the operator applied to a name we determine whether 
the I-value or r-value of a name is needed; most leaves represent r-values, 
The leaves reprewit initial values of names, and we subwript them with 
0 to avoid confusion with labels denoting "current" values of names as in 
(3) tKI0w. 
2. 
lnterior nodes are labeled by an operator symbol. 
3. 
Nodes are also optionally given a sequence of identifiers for labels. The 
intention is that interior nodes represent computed values, and the iden* 
tifien labeling a n d e  are deemed to have that value. 
lc is important not to confuse dags with flow graphs. Each node of a flow 
graph can be represented by a dag, since each node of the flow graph stands 
for a basic block. 
Fig. 9. IS. 
t l ;= 4 w i 
t2 := a [ t] 1 
t3 := 4 + i 
t,, := b [ t3 ] 
t5 := tz * td 
th := prod + ts 
prod := tl, 
t, := i + 1 
i := t7 
i f  i <= 20 goto ( 1 I 
Thrcc-addrcss code for block B ,. 
shows the three-address code corresponding to 
blwk B2 of Fig. 9+9. Statement numbers starting from ( 1) have been used for 
convenience. The corresponding bag is shown in Fig, 9+16. We discuss the 
significance of the dag after giving an algorithm to construct i t .  For the time 
be~ng let us observe that each node of the dag represents a formula in terms 
of the leaves, that is, the values pssessed by variables and constants upon 
entering the block. For example, the node labeled t4 in Fig. 9.16 represents 
the formula 
that is, the value of the word whose address is 4*i bytes offset from address 
b, which is the intended value of &. 
o 
Dsg Construction 
To construct a dag for a basic block. we process each statement of the block in 
turn. When we see a statement of the form x : = y + z, we I m k  for thc 

548 CODE GENERATION 
Flg. 9.16. Dag for block of Fig. 9.15. 
nodes lbac represent the "current" values: of y and z. These could be leaves, 
or they could be interior nodes of the dag if y and/or z had been evaluated 
by previous statements of the block. We then create a node labeled + and 
give it two children; the left child is the node for y, Ihe right the node for z. 
Then we label this node x. However, if there is already a node denoting the 
same value as y + z, we do not add the new node to the dag, but rather give 
the existir~g node the additional label x. 
Two details should k mentioned, First, if x (not q) 
had previously 
labeled some other node, we remove that label, since the "current" value of 
x is the node just created. Second, for an assignment such as x : = y wc do 
not create a new node. Rather, we append label x to the list of names on the 
node for the "current " value of y . 
We now give the aigorichm to compute a dag from a block. The algorithm 
is almost the same as Algorithm 5.1, except for the additional list of identi- 
fiers we attach to each node here The reader should be warned that this 
algorithm may not operate correctly if there ate assignments to arrays, if there 
are indirect assignments through ptnters, or if one memory location can be 
referred to by two or more names, due to EQUIVALENCE 
statements or the 
mrreiiindences between actual and formal parameters of a procedure caH. 
We discuss the modifications necessary to handle these situations at the end of 
this section. 
Algorithm 9.2. Constructing a dag, 
I n p i .  A basic block. 
Output. A dag for the basic block containing the following information: 
1. 
A lube/ for each node. For leaves the label is an identifier (constants per- 
mitted), and for Interior nodes, an operator symbol. 
2. For each node a {possibly empty) list of attached identifiers (constants not 
perm it ted here}. 

SEC. 9.8 
THE I)AG REPRESENTATION OF BASIC BLOCKS 549 
M u d .  We assume the appropriate data structures are available to create 
nodes with one or two children, with a distinction between "left" and "right" 
children in the latter case. Also available in the structure is a place for a l a k l  
for each node and the facility to create a linked list of attached identifiers for 
each node. 
!n addition to the= components, we need to maintain the set of all identi- 
fiers (including constants) for which there is a node associated. The node 
could be either a leaf lakled by thal identifier or an interior node with that 
identifier on ils attached identifier list. We assume the existence of a function 
nude (Idcwriflcr), which, as we build the dag, returns the mod recently created 
node associated with identifier. Intui~ively, nak(idcnt$ier) 
is the node of the 
dag that represents the value that itlcnr$er has at the current point in the dag 
cunstruction process, In practice, an entry In the symbol-table record for iden- 
tifier would indicate the value of m&(ideniflt)r). 
The dag construction process is to do the following steps ( I )  through ( 3 )  for 
each statement of the block, In turn, Initially, we assume there are no nodes, 
and node is undefined for all arguments. Suppose the "current " three-address 
7 
statement is either (i) x : = y op z, (ii) x : = op y, or (iii) x : = y. We 
refer to these as cases (i), 
(ii), and {iii). We treat a relational operator like 
i e= 20 goto as case (i), with x undefined. 
[f node I y) is undefined, create a leaf labeled y, and let d e ( y )  be this 
node, In case (i), 
~f n d e t  Z) is undefined, create a leaf labeled z and let 
that leaf be rtdr (z)+ 
In case (i), determine if there is a node labeled np, whose left chi!d is 
nude (y) and whose right child is node(z). (This check i s  to catch com- 
mon subexpressions.) If not, create such a node, In either event, let n 
be the nude found or created. ln case (ii). determine whether there is a 
node labeled np, whose lone child is no& (y) . If nd. create such a n d e .  
and let n be the n d e  found or created. I n  case (iii), let n be node Iy), 
Delete x from the list of attached identifiers for m&(x). Append x to 
the list of artached identifiers for the node n found in (2) and set ~~~ (x) 
to n. 
0 
Example 9.8. 
Let us return to the block of Fig, 9.15 and see how the dag of 
Fig. 9.16 is constructed for it. The first statement is tl : = 4 i. 
In step 
( I ) ,  we must create leaves labeled 4 and i0. {We use the subscript 0, as 
before, to help distinguish labels from attached identifiers in pictures, but the 
subscript is not really part of the label.) In step (2), we create a node labeled 
*, and in step (3) we attach'identifier t, to it. Figure 9.17(a) shows the dag 
at this stage. 
' O p m t ~ r s  
arc assurncd to haw ul rnrW tw argumcntsA The gcnmrlk~lhn lo thrcc or mrwc w- 
gumcnts is maiphtforrard. 

550 CODE GENERATION 
Fig. 9.17. Stcps in thc dag construct ion proccss. 
For the semd statement, ti := a'[t, I ,  wc create a new leaf labeled a 
and find the previously created nutie (t ). We also create a new node lakled 
[ 3 to which we atlach the nudes for a and t I as children. , 
For statement (31, t3 : = 4 * 1, we determine that d e ( 4 )  and nde(i) 
already exist, Since the operator is +, we do not create a new node for state- 
ment (31, bur rather append t3 on the identifier list for node tl. The result- 
ing dag is shown in Fig. 9.17(b). The value-number metttd of Section 5.2 
can be used to discover quickly that the node fox 4 * i already exists. 
We invite the reader to complete the construction of the dag. We menlion 
only the steps taken for statement (9), i : = t ~ .  
Before statement (9), 
nde(i) is the leaf labeled io. 
Statement (9) is an instance of case (iii); 
therefore, we find d ( t 7 ) ,  
append i to its identifier list, and set nde(i) to 
nude I t 7 ) .  This is one of only two statements - 
the other is statement (7) - 
where the value of node changes for an identifier. 
It is this change that 
ensures that the new node for i is the left child of the < = operator node con- 
structed for statement { 10). 
o 
Applications of Dqp 
There are several pieces of useful information that we can obtain as we are 
executing Algorithm 9.2. First, note that we automatically detect common 
sukxpressicms, Second, we can determine which identifiers have their values 
used in the block; they are exactly those for which a leaf is created in step I) 
at some time. Third, we can determine which statements compute values that 
muld t~ used outside the block. They are exactly those statements S whose 
node n Con~truCkd or found in step (2) still has node(x) = n at the end of the 
dag mnst~uction, where x is the identifier assigned by statement $. 
(Equivalently, x is still an attached identifier for n.) 

SEC+ 9,8 
THE DAG REPRESENTATIOf4 OF BASK BLOCKS 551 
Example 9.9. In Example 9.8, all statements meet the above constraint 
because the only times d
e
 
is redefined - 
for prod and i - 
the previous 
value of node was a leaf. Thus all interior ndes can have their value uied 
outside the block, Now, suppse we inserted before statement 19) a new 
statement s that assigned a value to i. At statement s we would create a n d e  
m and set nodeIi) = nr. 
However, at statement (9) we would redefine 
nudeli). Thus the value computed at statement s could not be used outside 
the blwk . 
0 
Another important use to which the dag may be put is to reconstruct a sim- 
plified list of quadruples taking advantage of common subexpressions and not 
performing assignments of the form x : = y unless absolutely necessary. That 
is, whenever a ride has more than one identifier on its attached list, we check 
which, if any, of those identifiers are needed outside  he block, As we men- 
tioned, finding live variables for the end of a blwk requires a data-flow 
analysis called "live-variable analysis" discussed in Chapter 10. However, in 
many cases we may assume no temporary name such as tI, t2, . - . , t7 in 
Fig, 9.15 is needed outside the block. (But beware of how logical expressions 
are translated; one expression may wind up spread over several basic blwks.) 
We may in general evaluate the interior ndes of the dag in any order that 
is a topological sort of the dag* In a topological sort, a node is not evaluated 
until all of its children that are interior n d c s  have been evaluated. As we 
evaluate a node, we assign its value to one of its attached identifiers x, prefer- 
ring one whose value is needed outside the block. We may not, however, 
choose x if there is another node m whose value was also held by x such that 
rn has been evaluated and is still "Live." Here, we define rn to be live if its 
value is needed outside the block or if m has a parent which has n a  yet been 
evaluated. 
If there are additional attached identifiers yl , y2+ 
+ 
+ . , yk for a node n 
whose values are also needed outside the block, we assign to them with state- 
ments y, : = x, y2 : = x, . . . , yk : = x. If rr has no attached identifiers at ail 
(this could happen if, say, n was created by an assignment to x. but x was 
subsequently reassigned), we create a new temporary name to hold the value 
of fi. The reader should beware that irt the presence of pointer or array 
assignments, not evwy topological sort of a dag is permissible; we shaH attend 
to this matter shortly. 
Example 9.10, Let us reconstruct a basic blwk from the dag of Fig. 9.16, 
ordering nodes in the same order as they were created: tl, t2, t4, t5, 
t6, t7 ( 4 ) .  Note that statements (3) and (7) of the original block did not 
create new n d e s ,  but added labels t3 and prod to the identifier lists of nodes 
t b  and tb. respectively. We assume none of the temporaries ti are needed 
outside the block. 
We begin with the node representing 4*i. This node has two identifiers 
altached, t, and tj. Let us pick tl to hold the value 4+i, w the first 
rewnstructed statement is 

552 
CODE GENERATION 
just as in the original basic blwk. The second ncde considered is labeled tz . 
The statement constructed from this n d e  is 
also as before. The node considered next is labeled t4 from which we gen- 
erate the statement 
The latter statement uxs tI as an argument rather than t3 as in the original 
basic blwk, because tl is the name chosen to carry the value 4+1. 
Next we consider the n d e  Labeled t5 
and generate the statement 
For the node labeled G, 
prod, we select prod to carry the value, since that 
identifier and not t6 will (presumably) be needed outside the block. Like 
tJ, 
temporary 
disappears. The next statement generated is 
prod := prod + t
5
 
Similarly, we choose i rather than t7 to a r t y  the value i+ 
1 + The last two 
statements generated are 
Note that the ten statements of Fig. 9.15 have been reduced to seven by tak- 
ing advantage of the common subexpressions exposed during the dag crmstruc- 
tion prmss, and by eliminating unnecessary assignments. 
Amys, Paintns, ud Roeedvc Cdls 
Consid w the basic blwk: 
lf we use Algorithm 9.2 to construct the dag for (93, 
at i] would become a 
common subexpression, and the "optimized" block' would turn out to be 
However, (9.5) and (9.6) compute different vabes for z in the case i = j 
and y # aii]. The problem is that when we assign to an array a, we may 
be changing the r-value of expression ali], even though a and i do not 
change. It is therefore necessary that when pressing an assignment to array 

SEC. 9.8 
THE DAG REPRESENTATION OF BASK BLOCKS 553 
a, we kill all nodes labeled 1 1, whose kft argument is a plus or minus a 
constant (possibly zero).' That is, we make these nodes ineligible to be given 
an additional identifier label, preventing them from being falsely reagnized as 
common subexpressims. It is thus required that we keep a bit for each node 
telling whether or not it has been killed. Further, for each array a mentioned 
in the bbck, it is convenient to have a list of all nudes currently not killed but 
that must be killed should we assign to an element of a. 
A similar problem occurs if we have an assignment such as *p : =w. where 
p is a pointer. lf we do not know what p might point to, every node 
currently in the dag being built must be killed in the sense above, if node n 
labeled a i s  killed and there is a subsequent assignment to a, we must create 
a new leaf for a and u s  that leaf rather than n. We later consider the con- 
straints on the order of evaluation caused by the killing of nodes. 
In Chapter 10, we discuss methods whereby we could discover that p could 
only point to some subset of the identifiers. If p muld only p i n t  to r or s, 
then only node (r) and nude (s) rnust be killed. It is alw conceivable that we 
could discover that i = j is impossible in block (93, in which case the node 
for a[ i] need not be killed by a[ j 1 : = y. However. the latter type of 
discovery is not usually worth the trouble. 
A procedure call in a basic block kills all nodes, since in h e  absence of 
knowledge about the called procedure, we must assume that any variable may 
te changed as a side effect. Chapter 10 discusses how we may establish that 
certain identifiers arc not changed by a procedure call, and then nodes tor 
these identifiers need not be killed. 
If we inlend to reassemble the bag into a basic block and may not want to 
use the order in which the nudes of the dag were created, then we must indi- 
cate in the dag that certain apparently independent nodes must te evaluated in 
a certain order. For example, in (9.3, the statement z : = a [ i 1 must follow 
a[ j I : = y, which must follow x := a[ f 1. Let us introduce certain edges 
n -. m in the dag that do not indicate that m is an argument of n, but rather 
that evaluation of H must folbw evaiuat.tio~ of rn in any computation of the 
dag . The rules to be enforced arc the following. 
I. 
Any evaluation of or assignment to an element of array a must follow 
the previous assignment to an element of that array if there is one. 
2. 
Any assignment to an element of array a must fdow any previous 
evaluation of a. 
3. 
Any use of any identifier rnust follow the previous procedure call or 
indirect assignment through a pointer If there i s  one. 
4. 
Any procedure call or indirect assignmen1 through a pointer rnust follow 
all previous evaluations of any identifier. 
Noto that thc argumcnt of [ J indicating thc army name could bc. a itwlf, or an-cxprcssbn like 
a-4. In the latter case, tho nodc a would k a grandchild. rathcr than a child'bf thc n d c  C I. 

554 
CODE GENERATlQN 
SEC. 9.8 
That is, when reordering c d e ,  uses of an array a may not cross each other, 
and no statement may cross a procedure call or an assignment through a 
pointer + 
9.9 PEEPHOLE OPTIMIZATION 
A statement-by-statement code-generation strategy often produces target code 
that contains redundant Instructions and subptima~ constructs. The quality of 
such target code can be improved by applying "optimizing" transformations to 
the target program. The term "optimizing" is somewhat misleading because 
there is no guarantee that the resulting c d e  is optimal under any mathernati- 
cal measure. Nevertheless, many simple transformations can significantly 
improve the running time or space requirement of the target program, so it is 
impo~tant to know what kinds of transformations are useful in practice. 
A simple but effective technique for locally improving the target code i s  
pwpholr optirniruiiun. a method for trying to improve the performance of the 
target program by examining a short sequence of target instructions (called the 
pecphde) and replacing these instructions by a shorter or faliter sequence, 
whenever possible. A It hough we discuss peephole optimization as a technique 
for improving the quality of the target code, the technique can also be applied 
directly after intermediate code generation to improve the intermediate 
representat ion. 
The peephole i s  a small, moving window on the target program. The code 
in , the peephole need not be contiguous? although some implementat ions do 
require this. It is characteristic of peephole optimization that each improve- 
men t may spawn opportunities for additional improvements. In general, 
repeated passes over the target code are necessary to get the maximum bene- 
fit. In this section, we shall give the following examples of program transfor- 
mations that are characterislic of peephole optimizations: 
+ 
redundant-instruction elimination 
+ 
flow-of-con trol optimizations 
algebraic simplifications 
+ 
use of machine idioms 
Redundant Loads and Stores 
If we see the instruction sequence 
( I )  MOV 
RD, a 
(2) MOV 
a, RO 
we can delete instruction (2) because whenever (2) is executed, ( I )  will ensure 
that the value of a is already in register RO. Note that if (2) had a labeP we 
* One advadlagc oE' 
gcncrirting ~ssrrrnbly mdc is that Irbcls will h prcwnt . fnciliiating pccphde 
uptimiratiunx such ah his, If machinc mdc i s  gcncratcd, and pxphulc optiaixatluri is dcsircd. we 
can u.w a bit to mark thc instructiuns that w w l d  haw Labclh. 

E C .  9.9 
PEEPHOLE OPTIMIZATION 
555 
mid not k sure that ( 1 )  was always executed immediately before (2) and so 
we could not remove (2). Put another way, ( I )  and (2) have to be in the 
same basic block for this transformation to be safe. 
While target. code such as (9.7) would not be generated if the aigorithm 
suggested in Section 9.6 were used, it might lx if a more naive algorithm like 
the one mentioned at the kginning of Section 9-1 were used. 
Another opportunity for peephole optimization is the removal of unreachable 
instructions, An unlabeled instruction immediately following an uncondirional 
jump may be removed. This operation can be repeated to eliminate a 
sequence of instructions. For example, for debugging purposes, a iarge pro- 
gram may have within it certain segments thar are executed only if a variable 
debug is I .  In C, the source code might look like: 
#define debug 0 
. . .  
if I debug I 
{ 
print debugging information 
1 
In the intermediate representat ion tb if-statement may be translated as: 
if debug = 1 goto L? 
got0 L2 
L 1 : print debugging information 
L2: 
One obvious peephole optimization is to eliminate jumps over jumps. Thus, 
no matter what the value of debug, (9+8) can be repiaced by: 
if debug # ? goto L2 
print debugging information 
L2: 
Now, since debug is set to 0 at the beginning of the program," constant pro- 
pagation should replace (9.9) by 
i f  (I # 1 goto L2 
print debugging information 
L2: 
As the argument of the first statement ~f (9+10) evaluates to a constant true, 
it can be replaced by goto L2. Then all the statements that print debugging 
aids arc manifestly unreachable and can be eliminated one at a time. 
'' To tell that debug has the valuc O we nced to do a global "rcachisg dchitions" data-flow 
analysis, as d i s c u d  in Chaptcr 10. 

SEC. 9.9 
The intermediate code generation algorithms in Chapter 8 frequently produce 
jumps to jumps, jumps to conditional jumps, or conditional jurnps to jumps. 
These unnecessary jumps can be eliminated in either the intermediate code or 
the target code by the following types of peephole optimizations. We can 
replace the jump sequence 
by the sequence 
If there are now no jumps to L 1 ,I' then it may be possible to eliminate the 
statement LI : goto ~2 provided it is prweded by an unconditional jump. 
Similarly. the sequence 
can bc replaced by 
Finally, suppose there is only one jump to L? and L1 is preceded by an 
u n m d  itional goto. Then the sequence 
may be replaced by 
While the number of instructions in (9.1 !) and (9.12) is the same, we 
" If this pc~phnlc a>ptirni;raticul is attcmpicd. we can counl thc nlrmbcr of jumps R) cqch Irk1 in 
thc tiymbd-tablc cntry fur tha~ Irrbd; a search of thc code is not nmxmry. 

SEC. 9.10 
GENERATING CODE FROM DAGS 
557 
sometimes skip the unconditional jump in (9.12), but never in (9.111, Thus, 
(9.12) is superior to 19.1 I )  in execution time. 
Algebraic Simplification 
There is no end to the amount of algebraic simplification thal can be 
attempted through peephole optimization. 
However, only a few algebraic 
identities occur frequently enough that it is warch considering implementing 
them. For example, statements such as 
are often produced by straightforward intermediate mde-generatton algo- 
rithms. and they can be eliminated easily through peephole optimization. 
Reduction in Strength 
Reduction in strength replaces expensive operations by equivalent cheaper 
ones on the target machine. Certain machine instruct ions are considerably 
cheaper than others and can often be used as special cases of more expensive 
operators. For cxarnple, x2 is invariably cheaper to implement as x+x than 
as a call to an exponentiation routine. Fixed-pint multiplication or division 
by a power of two €s cheaper to implement as a shift. Floating-point division 
by a constant can be implemented (approximated) as multiplication by a con- 
stant, which may be cheaper. 
Use d Machine Idioms 
The target machine may have hardware instructions to implement certain 
specific operations efficiently, Detecting situations that permit the use of 
these instructions can reduce execution time significantly . For example, some 
machines have auto-increment and aulo-decremeni addressing modes, These 
add or subtract one from an operand before or after using its value. The use 
of these modes greatly improves the quality of code when pushing or popping 
a stack, as in parameter passing. These modes can also be used in code for 
statements like i : =i+ 1 . 
9.10 GENERATLNG CODE FROM DAGS 
In this section, we show how to generate code for a basic block from its dag 
representation. The advantage of doing so is that from a dag we can more 
easily see how to rearrange the order of h e  final computation sequence than 
we can starting from a linear sequence of three-address statements or quadru- 
ples. Central to our discussion is  he case where the dag is a tree. For this 
case we can generate code that we can prove is optimal under such criteria as 

558 
CODE GENERATION 
S ~ C .  9.10 
program length or the fewest number of temparks used. This algorithm for 
optimal code generation from a tree is dm useful when the intermediate d
e
 
is a parse tree, 
Let us briefly consider how the order in which cornputa~ions ace done can 
affect the cost OC resulting object a l e .  Consider the following basic block 
whose dag represenlation is shown in Fig, 9,i8 (the dag happens to be a tree). 
Rg, 9.10, Dag for basic b h k .  
Note that the order is the one we would naturally obtain frm a syntrx- 
directed translation of the expression (a+bl -Ie-(c+d) ) by the algorithm of 
Section 8.3. 
If we generate code for the three-address slatements using the alwrithm in 
Section 9.6 we get the d e  sequence of Fig. 9.19 (assuming two registers RO 
and R 1 are available, and ody t4 is live on exit). 
mv a, RO 
ADD b, RO 
HOV 
C ,  R l  
ADD d, R? 
MQV RD, tl 
HOV e, RO 
SUB R1, RO 
MOV t,, R l  
SUB RO, R1 
MOV R1, t4 

SEC. 9. 
!O 
GENERATING CODE FROM DAGS 559 
On the other hand suppse we rearranged the order of the statements so 
that the computation of tl occurs immediately before that of t4 as: 
Then, using the code-generation algorithm of Section 9+6, we get h e  code 
sequence of Fig. 9.20. (Again, only RO and R1 art available.) By prfmm- 
ing the computation in chis order, we have been able to save two instructions, 
MOV RD, t, (which stores the value of RO in memory Iocatjon tl) 
and MOV 
t , , R 1 {which reloads the value of t in register R I). 
MOV 
e, RO 
ADD d, R O  
MOV e, 
Ell 
SUB RO, R1 
MOV a, RO 
ADD b +  RO 
SUB R1, RO 
HOV RO, t4 
Mg. 9.20. Rcviwd c d c  scqucncc. 
A Heuristic Ordtring for Dags 
The reason the above reordering improved the code was that the computation 
of t4 was made to follow immediately after the computation of t , its left 
operand in the tree. That this arrangement is beneficial should be clear, The 
left argument for the computation of t4 must be in a register for efficient 
computation of t4, and computing t, immediately before t4 ensures that 
will be the case. 
In selecting an ordering for the nodes of a dag, we are only constrained to 
be sure that the order preserves the edge relationships af the dag. Recall 
from Section 9.8 that those edges can represent either the operator-operand 
relationship or implied constraints due to possible interactions among p ~ o -  
cedure calls, array assignments, or pointer assignments. We propose the fol- 
lowing heuristic ordering algorithm, which attempts as far as possible to make 
the evaluation of a node immediately folbw the evaluation of its leftmost 
argument. The algorithm of Fig. 9.21 produces the ordering in reverse. 
Example 9.11. The algorithm of Fig. 9.21 applied to the tree of Fig. 9.18 
yields the wder from which the code of Fig. 9.20 was produced. For a more 
complete example, consider the dag of Fig. 9,?2. 

560 
CODE GENERATION 
I) 
while unlisted interior nodes rcmain do begin 
(2) 
sclcct an unlistcd node n, all of whose parcnts have 
bccn listcd; 
(31 
list n; 
(4) 
while the leftmost child m of n has no untisted parents 
and is not a kaf do 
/ r  sincc n was just listcd. rrt is not yct listed */ 
w
n
 
I 3  
list m; 
< 61 
n := m 
end 
end 
Fig. 9.21. Nodc tisting algorithm. 
Fig. 9+22. A dag. 
Initially the only node with no unlisted parents is I, so we set n= 1 at line 
12) and list 1 at .line (31. Now the left argument of I ,  which is 2, has its 
parents listed, so we list 2 and set n=2 at line (6). Now at h e  (4) we find 
the Ieftmost child of 2, which is 6, has an unlisted parent, 5. Thus we select a 
new n at line (2), and node 3 is the only candidate. We list 3 and then 
proceed down its left chain, listing 4, 5, and 6. This leaves only 8 among the 
interior nodes, so we list that. The resulting list i s  1234568 so the suggested 
order of evaluation is 8654321, This ordering corresponds to the sequence of 
three-address siatemen ts: 

GENERATING CODE FROM DACS 561 
which will yield optimal code for the dag on our machine whatever the 
number of registers, if the code-generation algorithm of Ssction 9.6 is used. 
It should be noted that in this example our ordering heuristic never had any 
choices to make at step (2), but in general it may have many choices, 
17 
Optimal Ordering for T r w  
It turns out that for the machine model of Section 9.2 we can give a simple 
algorithm to determine the optimal order in which to evaluate statements in a 
basic block when the dag representation of the block is a tree. Optimal order 
here means the order thal yields the shortest instruction sequence, over all 
instruction sequences that evaluate the tree. This algorilhm, modified co take 
into account register pairs and other target-machine vagaries. has been used in 
compilers for Algol, Bliss, and C. 
The algorithm has two parts. The first part labels each node of the tree, 
bottom-up, with an integer that denotes the fewest number of registers 
required to evaluate the tree with no stores of intermediate results. The 
second part of the algorithm i s  a tree traversal whose order is governed by the 
computed node labels. The output code is generated during the tree traversal. 
Intuitively, the algorithm works, given the two operands of a binary opera- 
tor, by first evaluating the operand requiring more registers (the harder 
operand). If the register requirements of both operands art the same, either 
operand can be evaluated first. 
The L,aMjng Algorithm 
We use the term "left leaf" to mean a node that is a leaf and the leftmost 
descendant of its parent. All other leaves are referred to as "right leaves." 
The labeling can be done by visiting nodes in a bottom-up order so that a 
node is not visited until all its children are labeled. The order in which parse 
tree nodes are created is suitable if the parse tree is used as intermediate code, 
so in this case, the labels can be computed as a syntax-directed translation. 
Figure 9.23 gives the algorithm for computing the label at node n. In the 
importan1 special case that rt is a binary p d e  and its children have labels II 
and i 2 ,  the formula of line (6) reduces to 
Example 9.12. Consider the tree in Fig. 9.18. A postorder traversal" of the 
' I  A postorder rravtrsal recursively visits the subtrees rooked at childcen nl ,n!, . . . , rk of a node 
n, then n. I t  is the wder in which d e s  of aa.'.parse trm are created im a bottom-up par*. 

562 
CODE GENERATION 
SEC. 9.10 
Fig. 9-23. Lahl computation 
nodes visits the nodes in the. order a b t I e e d t2 t3 t,. 
Postorder is 
always an appropriate order in which to do the label computations. Node a i s  
labeled 1 since it is a left leaf. Node b is labeled O since it is a right leaf. 
Node t, is labeled 1 because the labels of its children are unequal and the 
maximum label of a child is I. Figure 9.24 shows the labeled tree that results, 
I t  implies that two registers are needed to evaluate t, and, in fact, two regis- 
ters are needed just to evaluate t3. 
Code Generation Prom s Labeled Tree 
We now present [he algorithm that takes as input a labeled tree T and pro- 
duces as output a machine code sequence that evaluates T into RO. (RO can 
then be stored into the appropriate memory location+) We assume T has only 
binary operators. The generalization to operators with an arbitrary n u m b  of 
operands is not had, and it is left as an exercise. 
The algorithm uses the recursive procedure gtnc.rxk{n) to produrn machine 
code evaluating the subtree of T with root rr into a register. The procedure 
genrde uses a stack rwc4 to allocate registers. initially, r m d  contains all 
available registers, which we assume are RO. R 1, . . + , R(r - I ) .  in this 

SEC. 9.10 
GENERATING CODE FROM DAGS 563 
order, A call of gmcde may find a subset of the regiskrs, perhaps in a dif- 
ferent order, on rstack. When gencode returns, it leaves the registers on 
rstock in the same order it found them. The resulting d
e
 computes the 
value of the tree T in the top register on rmck. 
The function swap(rmwk1 interchanges the top two registers on rsruck. The 
use of swap is to make sure that a left child and its parent are evaluated into 
the same register. 
The procedure gmcLude uses a stack ismck to allcate temporary memory 
locations, We assume tsdack initially contains TO, TI, T2, . . . . In prac- 
tice, tsrark need not be implemented as a list, if we just keep track of that i 
such that Ti is currently on top. The contents of tskack is always a suffix of 
TO, TI, . . . . 
The statement X := pup(smck) means "pop stack and assign the value 
popped to X." Conversely, we use pusR(srmk, X) to mean "push X onto 
stack;" cop (stack) refers to the value on top of stack. 
The code-generation algorithm is to call genc& 
on the root of T, whcre 
g e n c d  is the procedure shown in Fig. 9.25. It can be explained by examin- 
ing each of the five cases, For case 0, we have a subwee of the form 
That is, n is a leaf and the leftmost child of its parent. Therefore we generate 
just a load instruction. In case 1 ,  we have a subtree of the form 
for which we generate code to evaluate n l  into register R=rop(rslack) fd- 
lowed by the instruction op rtam, R. In case 2, we have a subtree of the form 
where n 
can be evaluated without stores but n2 is harder to evaluate Ii.e., 
requires more registers) than n ,. For this case, we swap the top two registers 
on rstuck, then evaluate nz into R = rop(rstock). We remove R from rstack 
and evaluate tr I into S = top(rstuck). Note that S was the rcgiskr initially on 
top of rmck at the beginning of case 2, We then generate the instruction op 
R ,  S, which produces the value of n (the node labeled op) in register S. 
Another call to swap leaves rmnk as it was when this call of genwde began. 
. 

564 mDE GENERATION 
Fig. 9.25, Thc funccion #rrtr;r&. 

E C .  9.10 
GENERATING CODE FROM DAGS 565 
Case 3 is similar to case 2 except that hwe the left subtree i s  harder and is 
evaluated first, There is no need to swap registers here. 
Case 4 occurs when both subtrecs require r or more registers to evaluate 
without stores. Since we must use a temporary memory location, we first 
evaluate the right subtree into the temporary T, then the icft subtree, and 
finally the rmt . 
Example 9.13. Let us generate code for the labeled tree in Fig. 9.24 with 
r s m k  = RO, R l  initially. The sequence of calls to gemode and mde print- 
ing steps is shown in Fig. 9.26, Shown alongside in brackets is the contents of 
rstack st the time of each call, with the top at the right end. The code 
sequence here is a permutation of that in Fig. 9.20. 
a 
Fig. 9.216. Tracc of gmc& 
routine. 
We can prove that gencode produces optimal code on expressions for our 
machine model, assuming that no algebraic properties of operators are taken 
into account and assuming there are no common subexpressians. The proof, 
left as an exercise, is based on showing that any d
t
 
sequence must perform 
I .  an operation for each interior node, 
2. 
a load for each leaf which is the Idamost child oi its parent, and 
3. 
a store for every node both of whose children have h k l s  equal to or 
greater than r, 
Since genr.06~ produces exactly these steps, it b optimal. 
Multiregister Operations 
. 
Wecan mdifyourlabelingalgorithmta handleoperationslikemu!tipli~tion, 
division, or a function call, which normally require more than m e  register to 
perform. Simply modify step (6) of Fig. 9.23, the labeling algorithm, so 

566 CODE GENERATION 
SEC. 9.10 
b M ( n )  is always at least the number of registers required by the opration, 
For example, if a function call is assumed to require all r registers, replace 
line (6) by h l [ n )  = r. If multiplication requires two registers, in the binary 
case use 
where l and iz are the labels of the children of n+ 
Unfortunalely, this modification will not guarantee that a register-pair is 
available for a mu It iplication or division, w for multiple-precision operations. 
A useful trick on some machines is to pretend that multiplication and division 
require three registers. If swap is never used in g e n c d ,  then mtuck wili 
always contain wnsecutive high-numbered registers, i, i + I ,  . . . , r - l for 
some i. Thus the first three of these are sure to include a register pair. By 
taking advantap of the fact that many operations are commutative we can 
often avoid using case 2 of g e n c d ,  the case that calls map. Also, even if 
muck does not contain three consecutive registers at the top, we have a very 
good chance of Finding a register-pair somewhere in rstack. 
If we may assume algebraic laws for various operators, we have, the opprt un- 
ity to replace a given tree T by one with smaller labels (to avoid stores in case 
4 of gemode) and/or fewer left leaves (to avoid loads in cas 0). For exam- 
ple, since + is normally regarded as being commutative, we may replace the 
tree of Pig+ 9,27(a) by that of Fig. 9.27(b], reducing the numkr of left leaves 
by one and possibly lowering some labels as well, 
Since + is usually treated as being associative as well as being commutative, 
we may take a cluster of nodes labeled + as in Fig. 9+27(c) and replace it by a 
left chain as in Fig. 9.27Id). To minimize the label of the root we need only 
arrange that Ti, is one of T I ,  
T1, 
Tj. and Tq with a largest label and that Ti, 
is not a leaf unless all of T I ,  . . . , T4 are. 
When there are common subexpressions in a basic block, the corresponding 
dag will no longer be a tree, The common sulxxpressions will correspond ta 
o d e s  with more than me parent, calkd shred nodes. We can no longer 
apply the labeling algorithm or gencade directly. In fact, common sukxpres- 
sions make code generation markedly more difficult from a mathematical 
p i n t  of view. Bruno and Sethi 119761 showed that optimal code generation 
for dags on a one-register machine is NP-complete. Aho, Johnson, and U11- 
man 11977~41 showed that even with an unlimited number of registers the prob- 
lem remains NP-mmplele. The difficulty arises in trying to determine an 
optimal order in which to evaluate a dag in the cheapest way. 

SEC. 9.11 
DYNAMIC PROGRAMMING CODEGENERATION ALGORITHM 
567 
Fit. 9.27. Commulalivc and assrbciativc transformations, 
In practice, we can obtain a reasonable solution if we partition the dag into 
a set of trees by finding far each root and/or shared node n, the maximal sub- 
tree with n as root that includes no other shared nodes, except as leaves. For 
example, the dag of Fig. 9.22 can be partitioned into the trees of Fig. 9.28. 
Each shared node with p parents a p p r s  as a leaf in at most p trees. Nodes 
with more than one parent in the same tree can k turned into as many leaves 
as necessary, so no leaf has multiple parents. 
Once we have partitioned the dag into trees in this Cashion, we can order 
the evaluation of the trees and use any of t be preceding algorithms to generate 
code for each tree. The order of the trees must be such that shared values 
thal are leaves of a tree rnuq be available when the tree is evaluated. The 
shared quantities can be computed and stwed into memory (or kept in regis- 
ters if enough registers are available). While this process does not necessarily 
generate optimal code, it will often be satisfactory. 
9-11 DYNAMIC PROGRAMMlNG CODE-GENERATION ALGORITHM 
In rhe previous section, the procedure gcwudc produces optimal code from an 
expression tree using an amount of time that is a linear functiwn of the size of 
the tree. This procedure works f ~ r  
machines in which all computation is done 
in registers and in which instructions consist of an operator applied to two 
registers or to a register and a memory location. 

568 
CODE GENERATIOPI 
SEC, 9.1 1 
Mg. 9.23. Partition ifit0 trees. 
An algorithm based on the principle of dynamic programming can be used 
to extend the class of machines for which optimal code can be generated from 
expression trees in linear time. The dynamic programming algorithm applies 
to a broad class of register machines with m m p h  instruction sets, 
A Class of Register Machimes 
The dynamic programming algorithm can be used to generate code for any 
machine with r inte~changeable registers RO, R1, . . . , Rr - I and instruc- 
tions of the form Rr' := E where E is any expression containing operators, 
registers, and memory locations. If E involves m e  or more registers, then A i  
must be one of those registers. This machine m d d  includes the machine 
introduced in Section 9.2, 
For example, the instruction 
ADD RO ,R1 would k represented as 
R1 := R 1 + RO. The instruction 
ADD +RO , R? would be represented as 
R l  : = R 3 + ind RD where ind stands for the indirection operator. 
We assume a machine has a lqad instruction - Ri := H, a store instruction 
PI := Ri, and e register-to-register copy instruction Ri := Rj. For simplicity, 
we also assume every instruction costs one unit, although the dynamic pro- 
gramming aipritbm can easily be modified to work even if each instruction 
has its own cost. 

EC. 9.1 I 
DYNAMIC PROGRAMMLNG CODE-GENERATION ALGORlTHM 
569 
The Principle of Dynamic Programming 
The dynamic programming algorithm partitions the problem of generating 
optimal code for an expression into subproblems of generating optimal code 
for the subexpressions of the given expression. As a simple example, consider 
m expression E of the form E + E 
An optimal program for E is formed 
by combining optimal programs for E l  and El, in one or the other order, foI- 
lowed by code to evduate the operator + . The subproblems of generating 
optimal code for E l  and El are solved similarly. 
An optimal program produced by the dynamic programming algorithm has 
an Important property. It evaluates an expression E = E up E 2  "contigu- 
ously.*' We can appreciate what this means by hoking at the syntax tree T 
For E: 
tiere T 1  and T I  are trees for E 1 and El, respectively. 
Contiguous Evaluation 
We say a program P evaluates a tree T runrigualcrly if it first evaluates those 
subtrees of T that need to be computed into memory, Then, it evaluates the 
remainder of T either in the order TI, T2, and then the root, or in the order 
T I ,  TI, and then the root, in either case using the previously computed values 
from memory whenever necessary. As an example of ncinmntiguous evalua- 
tion, P might first evaluate part of TI leaving the value in a register (instead 
of memory), next evaluate T 2 ,  and then return to evaluate the rest of T1 . 
For the regi~er machine defined above, we can prove that given any 
machine-language program P to evaluate an expression tree T, we can find an 
equivalent program P' such that 
1 .  
P' is of no higher cost than P, 
2, 
P' uses no more registers than P, and 
3, 
P' evaluates the tree in a contiguous fashion. 
This result implies that every expression tree can be evaluated optimally by a 
contiguous program. 
By way of contrast, machines with even-odd register pairs such as the IBM 
Systeml370 machines do not always have optimal contiguous evaluations. For 
these machines, we can give examples of expression trees in which an optimal 
machine language program must first evaluate into a register a portion of the 
left subtree of the root, then a prtiori of the right subtree, then another part 
of the left subtree. then another part of the right, and so on, This type of 
I 

570 CODE GENERATION 
SEC. 9.1 1 
oscillation is unnecessary for an optimal evaluation of any expression tree 
using the general register machine, 
The contiguous evaluation property defined above says that for any expres- 
sion tree T there always exists an optimal program that consists of optimal 
programs for subtrees of the root, followed by an instruction to evaluate the 
root. This property allows us to use the dynamic programming algorithm to 
generate an optimal program for T. 
The Dynamic Prqgrammirg Atgdthm 
The dynamic programming algorithm proceeds in three phases. Suppose the 
target machine haL{ r registers. In the first phase, compute bottom-up for each 
, 
, 
node n of the expression tree T an array C of msrs, in which the ith corn- 
ponent C[i 1 is the optimal cost of computing the subtree S rwted at n into a 
' 
register, assuming i registers are available for the computation, for 1 5 i 5 r. 
The cost includes whatever loads and stores arc necessary to evaluate $ in the 
given number of registers, I t  a h  includes the cost of computing the operator 
at the root of S. The zeroth component of the cost vector is the optimal wst 
of computing the subtree S into memory. The contiguous evaluation property 
ensures that an optimal program fix S can te generated by considering combi- 
nations of optimal programs only for the subtrccs of the rmt of S. This res- 
t rict ion reduces the number of cases that need to k considered. 
To compute C li ] at node n, consider each machine instruction R := E 
whose expression E matches the subexpressim rmted at node n. By examin- 
ing the cost vectors at the corresponding descendants of n, determine the costs 
of evaluating the operands of E. For those operands of E that are registers, 
consider a!l possible orders in which the corresponding subtrees of T can Ix 
evaluated into registers. In each ordering, the first subtree wrrespnding to a 
register operand can be evaluated using i available registers, the second using 
i- I registers, and so on. To account for node n, add in the cost of the 
instruction R : = E that was used to match node n. The value C [i 1 is then the 
minimum cost over all possible ordws. 
The cost vectors for the entire tree T can be computed batom up in time 
linearly proportional to the number of nodes in T. It i s  convenient to store at 
each node the instruction used to achieve the best cost for Cti 1 for each value 
of i .  The smallest cost in the vector for the rout of T gives the minimum cost 
of evaluating T. 
In the second phase of the algorithm, traverse T, using the cast vectors to 
determine which subtrees of T must be computed into memory. In the third 
phase, traverse each tree using the cast vectors and associated instructions to 
generate the final target code. The cude for the subtrees computed into 
memory locations is generated first. These two phases can also be imple- 
mented to run in time linearly proportional to the size of the expression tree, 
Example 9,14. Consider a machine having two registers RO and R I ,  and the 
following instructions. each of unit cast: 

SEC. 9.1 \ 
DYNAMIC PROGRAMMING CODE-GENERATION ALGORITHM 
57 1 
In these instructions, Ri is either RO or R i  , and Mj i s  a memory locat ion. 
Let us apply the dynamic programming algorithm to generate optimal code 
for the syntax tree In Fig. 9.29. [n the first phase, we mrnpute the cost vec- 
tors shown at each node. To illustrate this cost computation + consider the wst 
vector at the leaf a. CiOI, the wst of computing a into memory, is 0 since it 
is already there. CII], the cast of computing a into a register, is 1 since we 
can load it into a register with the instruction RO : = a. C121, the cost of 
loading a into a regislet with two registers available, is the same as that with 
one register available. The cost vector at leaf a is therefore (O,I , I ) .  
Fig. 9-29, Syntax tsec for l a- b) + c* (at 
) with cost vcctor at each nodc. 
Consider the cast vector at the root. We first determine the minimum cost 
of computing the root with one and two registers available. The machine 
instruction RO := RO + M matches the root, because the root i s  labeled with 
the operator +. Using this instruction, the minimum cost of evaluating the 
root with one register available is the minimum cost of computing its right 
subtree into memory, plus the minimum cost of computing its left subtree into 
the register, plus I for the instruction. No other way exists. The cost vectors 
at the right and left children of the root show that  he minimum cost of corn- 
puting the root with one register available is 5+2+ 1 = 8. 
Now consider the minimum cost of evaluating the roo1 with two registers 
available. Three cases arise dbepnding on which instruction is used to corn- 
putc the root and in what order the left and righl subtrees of the root are 
evaluated, 
1 .  
Compute the left subtree with two registers available into register RO, 
compute the right subtree with one register available into register R1, 
and use the instruction RO : = RO + R7 to compute the root. This 
sequence has wst 2+5+ 1 = 8. 

572 
CODE GENERATION 
SEC. 9.1 I 
2, 
Compute the right subtree with two registers available into R 1, compute 
the left subtree with one register available into RO, and use the instruc- 
tion RO := RO + R1. This sequence has cast 4+2+ 1 = 7. 
3. 
Compute the right subtree into memory location M, compute the left sub- 
tree with two registe~s available into register RO, and use the instruction 
RO : = RO + M. This sequence has cost 5+2+ 1 = 8. 
The second choice gives the minimum cost 7. 
The minimum cost of computing the root into memory is determined by 
adding one to the minimum cost of computing the rmt with all registers avail- 
able; that is, we compute the root into a register and then blare the result, 
The cost vector at the root is therefore (8+8,7}. 
From the cost vectors we can easily construct the code sequence by making 
a traversal of the tree, From the tree in Fig. 9.29, assuming two registers are 
available, an optimal code sequence is 
RO := c 
R l  := d 
R l  := Rl r
'
 
e 
RO := RO * R1 
Ell 
:q a 
R l  := R3 - b 
This technique, originally developed in Aho and Johnson [1976j, has been 
used in a number of compilers, including the second version of S. C. 
Johnson's portable C compiler, PCC2. The technique facilitates retargeting 
because of the apptcabiiily of the dynamic programming technique to a broad 
claw of machines. 
9,12 CODEGENERATOR GENERATORS 
Code generation involves picking an evaluation order for operations, assigning 
registers to hold values, and selecting the appropriate target-language instruc- 
tions to implement the operators in the intermediate representation. Even if 
we assume the order of evaluation is given and registers are ailmated by a 
separate mechanism, the problem of deciding what Instructions to u* can be a 
large combinatorid task, especially on a machine rich in addressing modes. 
In this section, we present tree-rewriting techniques that can be used to con- 
struct the instruction select ion phase of a code generator automatically from a 
high-level specification of the taxg& machine. 
Code Generatiam by Tree Rewriting 
Throughout this section, the input to the cde-generation process wilt be a 
sequence of trees at the semantic kvel of the target machine. The trees are 
what we might get after inserting the run-time addresses into the intermediate 

SEC. 9.12 
CODE-GENERATOR GENERATORS $73 
representation, as described in Section 9.3. 
Example 9.15. 
Figure 9.30 contains a tree for the assignment statement 
a [ i ] : =b+l in which a and i are Iwals whose run-time addresses are given 
as offsets const, and const, from SP, the register containing the pointer 
to the beginning of the cursent activation record. The array a is stored on the 
run-time stack, The assignment to aCi 1 is an indirect assignment in which. 
the r-value of the location for a[ i ] is set to the r-value of the expression 
1
.
 The address of array a is given by adding the value of the constant 
eonst, to the contents of register SP; the value of i is in  he location 
obtained by adding the value of the constant consti to the contents of regis- 
ter SP. The variabie b is a global in memory location m e b .  For simplicity, 
we assume all variables are of type character. 
Fig. 9,M. Intermediate-code tree for a [ i ] : =b+ 1 . 
In the tree, the ind operator treats its argument as a memory address. As 
the left child of an assignment operator, the ind node gives the location into 
which the r-va1u.e on the right side of the assignment operator is to be stored. 
If an argument of a + or ind operator is a memory location or a register, 
then the contents of that memory location or register are iaken as the value. 
The leaves in the tree are type attributes with subscripts; the subscript indi- 
cates the value of the attribute. 
o 
The target code is generated during a process irt which the input tree i s  
reduced into a single node by applying a sequence of tree-rewriting rules to 
the tree. Each tree-rewriting ruk is a statement of the form 
where 
I .  
repkcemenr is a single nude, 
2, 
tertrplulrre is a tree, and 
3. 
ucrion is a code fragment, as in a syntax-directed translation scheme. 

574 
CODE GENERATION 
SEC. 9.12 
A set of tree-rewriting rules is called a tree-tmumlasim scheme. 
Each tree ternpiate represents a computation performed by the sequence of 
machine instructions emitted by the associated action. Usually, a template 
corresponds to a s~ngle machine instruction. 'The leaves of the template are 
attributes with subscripts, as in the input tree. Often, certain restrictions 
apply to the values of the subscripts in the templates; these restrictions are 
specified as semantic predicates that must be satisfied kfore the template is 
said to match. For example, a predicate might specify that the value of a con- 
stant fall in a certain range. 
A tree-translation scheme is a convenient way to represent the instruction- 
selection phase of a code generator. As an example of a treerewriting rule, 
consider the rule for the registcr-to-register add instruaion: 
This rule is used as follows. If the input tree contains a subtree that matches 
this tree template, that is, a subtree who= root i s  labeled by the operator + 
and whose left and right children are quantities in registers i and j, then we 
can replace that subtree by a single node labeled reg, and emit the instruc- 
tion ADD Rj , 
Rr as output. Mort than one template may match a subtree at a 
given time; we shall describe shortly wrne mechanisms for deciding which rule 
to apply in cases of conflict. We assume register allocation is done before 
code selection. 
Example 9 3 .  Figure 9.31 contains tree-rewriting rules for a few instructions 
of our target machine. These rules will be used in a running example 
throughout this section. The first tw rules correspond to bad instructions, 
the next two to stare instructions, and the remainder to indexed loads and 
additions. Note that rule (8) requires the value of the constant to tK one. 
This condition would be specified by a semantic predicate. 
o 
A tree-translation scheme works in the following way. Given an input tree, 
the templates in the tree-rewriting rules are applied to its subtrees. If a tem- 
plate matches, the matching subtree in the input tree is replawd with the 
replacement node of the rule and the action associated with the ruk is done. 
If the action contains a sequence of machine instructions, the instructions are 
emitted, This prmss is repeated until the tree is reduced ro a single node, or 
until no more templates match. The sequence of machine instructions gen- 
erated as the input tree is reduced to a single node constitutes the ourput of 
the tree-translation scheme on [he given input tree. 
The process of specifying a code generator becomes similar to that of using 
a syntax-direckd translation scheme to s ~ c i f y  
a translator, We write a tree- 
translation scheme to describe the insrruction set of a larget machine. In prac- 
tice, we would like to find a scheme that causes a minimal-mst instruction 

CODE-GENERATOR GENERATORS 
575 
( MOV Rj, *Ri ) 
reg, 
ind 
Q. %$I, 
Trec-rewriting rules for somc targct-machins instructions. 

sequence to be generated for each input tree. Several tools are available to 
he!p build a code generator automatically from a tree-translation scheme. 
Example 9+17, Let us use the tree-translation scheme in Fig. 9.3 1 to generate 
code for the input tree in Fig. 9.30. Suppose the first rule 
i s  applied to load the constant a into register RO. The label of the leftmost 
leaf then changes from const, to rego and the instruction MOv # ~ , R O  is 
generated- The seventh rule 
f 
(7) reg(> 
t 
/ \ 
{ADD SP,RQ) 
==go 
==gsp 
now matches the leftmost subtree with root labeled +. Using this rule, we 
rewrite this subtree as a single node Inkled rego and generate the instruction 
ADD SP , 
RO . Now the tree looks like 
ind / \ 
+ 
At this point, we could apply rule ( 5 )  to reduce the 
ind 
I 
t 
, ' \  
eonst, 
 SF 
to a single node labeled reg,. However, we can 
the larger subtree 
also use rule (6) to reduce 
consti 
regsp 
into a single node labeled reg, and generate the instruction ADD i I SP) , RD. 

SEC. 9.12 
CODE-GENERATOR GENERATORS 577 
Assuming it is more efficient to use a single instruction to compute the larger 
subtree rather than the smaller one, we choose the latter reduction to get 
In the right subtree, ruk (2) applies to rhe leaf memb. This ruie generates an 
instruction to load b into register 1, say. Now, using rule (8) we can match 
the subtree 
and generate the increment Instruction INC R1. At this pint, the input tree 
has k e n  reduced to 
This remaining 1ree is matched by rule (4). which reduces the tree to a single 
node and generates the instruction MOV ~ 1 ,  
+RO. 
In the protxss of reducing the tree to a single node, we generate the follow- 
ing cade sequence: 
Several aspects of this tree-reduction process need further explication. We 
have not specified haw the tree-patrcrn matching is to be done. Nor have we 
spcified an order In which the templates are matched or what to do if more 
than one template matches at a given time. A h ,  notice that if na template 
matches, then the code-generation process blocks. On the other extreme, it 
may be possible for a single node to be rewritten indefinitely, generating an 
Infinite sequence of register move instructions or an infinite equence of loads 
and stores. 
One way to do the tree-pattern matching efficiently is to extend the 
multiple-keyword pattern-matching algorithm in Exercise 3.32 into a top-down 

578 
CODE GENERATION 
SEC. 9.12 
tree-pattern matching algorithm. Each template can be represented by a set of 
strings. namely, by the set of paths from the root to the leaves. From these 
collections of strings, we can construct a tree-pattern matcher as in Exercise 
3.32. 
The ordcring and multiple-match problems can be resolved by using tree- 
pactern matching in conjunction with the dynamic programming algorithm of 
the previous section. A tree-translation scheme can be augmented with cost 
information by ass~iating with each tree-rewriting rule the cost of the 
sequence of machine instructions generated if that rule is applied. 
In practice, the tree-rewriting process can be implemented by running the 
tree-pattern marcher during a depth-first traversal of the input tree and per- 
forming the reductions as the nodes are visited for the last time. If we run 
the dynamic programming algorithm concurrently, we can select an optimal 
sequence of matches using the cost information associated with each rule, We 
may need to defer deciding upon a match until the cost of all alternatives is 
known. Using this approach, s small, efficient c d e  generator can be con- 
structed quickly from a tree-rewriting scheme. Moreover, the dynamic pro.. 
gramrning algorithm frees the code-generator designer from having to resolve 
conflicting matches or decide upon an order for the evaluation. 
Pattern Matching by Parsing 
Another approach is to use an LR parser to do the pattern matching. The 
input trce can be treared as a string by using its prefix representation, For 
example, the prefix represenlation for the tree in Fig. 9.30 is 
: = ind + + conat, reg,, ind + consti reg,, + mem, const, 
The tree-translalion scheme can be converted into a syntax-directed transla- 
tion scheme by replacing the tree-rewriting rules with the productions of a 
context-free grammar in which the right sides arc prefix representations of the 
instruction templates. 
Example 9.18. 
The syntax-directed translation scheme in Fig. 9.32 i s  based 
on the tree translation scheme in Fig. 9.31. 
c 
From the productions of the translation scheme we build an LR parser using 
one of the LR-parwr construction techniques of Chapter 4. The target code is 
generated by emitting the machine instruction corresponding to each reduc- 
tion. 
A ccde-generat~on grammar is usually highly ambiguous, and some care 
needs to be given to how the parsing-action conflicts are resolved when the 
parser is constructed. In the absence of cost information! a general rule is to 
favor larger reductions over smaller ones. This means that in a reduce-reduce 
conflict, the longer reduction is favored; in a shift-reduce conflict, the shift 
move is chosen. This "maximal munch" approach causes a larger number of 
operations to be performed with a single machine instruction. 
There are several aspects to using LR parsing in d
e
 
generation. First. the 

CODE-GENERATOR GENERATORS 579 
-, C O I I X ~ ~  
{ M O V C ~ , R ~ }  
mem, 
{ M O V ~ , R ~ )  
:= mema regi 
{nW R i , a  ) 
+ : = ind reg, reg 
{MOV ~ j , + ~ i  
} 
-C ind + const= regj 
{MOV c(Rj) , R i }  
+ + regi ind + const, reg, ( ADD c ( Rj 1 , Rr' ) 
+ + regi reg, 
{ A D D  ~ j , ~ i  
) 
- + reg, constl 
{ I N C  Rr' ) 
Fig, 9.32. 
Sy ntax-directed translation scheme constructed from Fig. 9.3 1 .  
parsing method is efficient and well understood, so reliable and efficient code 
generators can be produced using the algorithms described in Chapter 4. 
Second, it is relatively easy to retarget the resulting code generator; a code 
selector for a new machine can be constructed by writing a grammar to 
describe the instructions of the new machine. Third, the quality of the code 
generated can be made efficient by adding special-case productions to take 
advantage of machine idioms. 
However, there are some difficulties as well. 
A left-to-right order of 
evaluation is fixed by the parsing method. Also, for some machines with 
large numbers of addressing modes, the machine-description grammar and 
resulting parser can become inordinately large. As a consequence, specialized 
techniques are necessary to encode and process the machine-description gram- 
mars, We must also be careful that the resulting parser does not block {has 
no next move) while parsing on an expression tree, because the grammar does 
not handle some operator patterns or the parser has made the wrong rescilu- 
tion of some parsingaction conflict. We must also make sure the parser does 
no1 get into an infinite Imp of reductions of productions with single symbols 
on the right side. The looping problem can be solved using a state-splitting 
technique at Ihe time the parser tables are generated (see Glanville [ 19771). 
Routines for Semantk Checking 
The leaves in the input tree are type attributes with subscripts, where a sub- 
script associates a value with an attribute. In a mde-generation translation 
scheme. the same attributes appear. but often with restrictions on what values 
the subscripts can have. For example, a machine instruction may require that 
an attribute value fall in a certain range or that the values of two attributes be 
related. 
These restrictions on attribute values can be specified as predicates that are 
invoked before a reduction is made, In fact, the general use of semantic 
actions and predicates can provide greater flexibtlity and ease of description 
than a purely grammatical specification of a d e  generator. Generic tem- 
plates can be used to represent classes of instructions and the stmant ic actions 
can then be used to pick instructions for specific cases. For example, two 

580 
CODE GENERATION 
SEC. 9.12 
forms of the addition instruction can be represented with one template: 
= =q, 
t 
+ 
{ if c * 1 then 
/ \ 
INC Ri 
const, 
else 
Xegi 
ADD #c,Ri ) 
Parsing-action conflicts can be resolved by disambiguating predicates that 
can allow different sekction strategies to be used in different contexts. A 
smaller description of a target machine is possible because certain aspects of 
the machine architecture, such as addressing rnudes, can be factored into the 
attributes, The ccmplication in this approach is that it may become difficult to 
verify the accuracy of the attribute grammar as a faithful description of the 
target machine, although this problem is shared to some degree by all code 
generators, 
EXERCISES 
9.1 Generate code for the following C statements for the target machine 
of Seaion 9.2 assuming all variables are static. Assume three regis- 
ters are availahkA 
a ) x =  I 
I 
b ) x = y  
c ) x = x +  1 
d ) x = a + b * c  
e) x = a!(b+c) - d+(e+f) 
9.2 Repeat Exercise 9.1 assuming all variables are automatic idlocated on 
the stack). 
9.3 Generale c d e  for the following C statements for the target machine 
of Section 9.2 assuming all variables are static. Assume three rcgis- 
ters are available. 
a) x = a [ i ]  + 1 
b) a [ i l  = b[c[ill 
C) a[ilCjl = blil[kl * cIkl[jl 
d) a [ i ]  = a[i] + b [ j ]  
e) a [ i ]  += b[jJ 
9.4 Do Exercise 9.1 using 
a) the algorithm in Section 9.6 
b) the procedure gencde 
C) the dynamic programming algorithm of Section 9.1 1 
9.5 Generate code for the following C statements 

CHAPTER 9 
9d Generate code for the following C program 
main{) 
I 
int i; 
i n t  a[ 101; 
while (i <= 
a [ i ]  = 0; 
1 
9.7 Suppose [hat for the Imp of Fig. 9.13 we choose to allocate three 
registers to hold a, b, and c .  Generate code for the Mocks of this 
loop. Compare the cost of your code with that in Fig. 9.14. 
9.8 Construct the register-interference graph for the program in Fig. 9.13, 
9+9 Suppose that for simplicity we automatically store all registers on the 
stack (or in memory if a stack is not used) before each procedure call 
and restore them after the return. How does this affect the formula 
(9+4) used to evaluate the utilily of allocating a register to a given 
variable in a loop? 
9,10 Modify the function g d r q  of Section 9.6 to return register-pairs when 
needed, 
9.11 Give an example of a dag for which the heuristic for ordering the 
nodes of a dag given in Fig, 9.21 does not give the best ordering. 
*9.12 Generate optimal code for the following assignment statements: 
a ) x : = a + b + c  
b) x := I a  * -bl + ( c  - Id + el) 
c) x := {alb - c)/d 
d) x := a + Ib + c/b*s)/(f*g - h + i l  
e) aIi,jl := b [ i , j l  - c[a(k,l]] * d [ i + j J  
9,13 Generate code for the following Pascal program: 
program forloopIinput, output); 
var i, in5tia1, f i n a l :  integer; 
begin 
readIinitia1, final); 
for i:= i n i t i a l  to final do 
write1nI i 1 
end. 

582 CODE GENERATION 
CHAPTER 9 
9.14 Construct the dag for the following basic block. 
9+1S What are the legal evaluation orders and names for the values at the 
n d c s  for the dag of Exercise 9-14 
a) assuming a, b and e are live at the end of the basic block? 
b) assuming only a is live at the end? 
9.16 In Exercise 9.15(b), if we are going to generate oode for a machine 
with only one register, which evaluation order is best? Why? 
9.17 We can modify the dag mnstrudion algorithm to take into account 
assignments to arrays and through pinters. When any element of an 
array is assigned, we assume a new value for that array is created. 
This new value is represented by a node whose children are the old 
value of the array, the value of the index into the array, and the value 
assigned. When an assignment through a pointer occurs, we assume 
we have created a new value for every variable that the pointer might 
have pointed to; the children of the node for each new value are the 
value of the pointer and the old value of the variable that might have 
been assigned. Using these assumptions, construct the dag for the 
f01kWing basic block: 
Assume that (a) p can p i n t  anywhere, (b) p points to only b or d. 
Do not forget to show the implied order constraints. 
9.18 If a pointer or array expression such as aEi] or *p i s  assigned and 
then used without the possibility of its value having changed in the 
interim, we can recognize and take advantage of the situation to sim- 
plify the dag For example, in the code of Exercise 9.17, since p is 
not assigned between the second and fourth statements, the statement 
e : = +p can be replaced by e : = c, since we are sure that what- 
ever p points to has the same value as c, even though we don't know 
what p points to. 
Revise the dag construction algorithm to take 
advantage of such inferences. Apply your algorithm 10 the d
e
 of 
Exercise 9.17. 

CHAPTER 9 
Bl BLlOGR A PH IC NOTES 
583 
**9,19 
Devise an algorithm to generate optimal code for a sequence of 
three-address statements of the form a : = b + c on the n-register 
machine of Example 9.14. The statements must be executed in the 
order given. What is the time complexity of your algorithm? 
BIBLIOGRAPHIC NOTES 
The reader interested in overviews of de-generation research should consult 
Waite {2974a,bj, Aho and Sthi 11977j, Graham 11980 and 19845, Gartapathi, 
Fischer, and Heclnessy 119821, Lunelt 1!983j, a ~ d  
Henry 119841, C d e  gen- 
eration for Bliss is discussed by Wulf et al. 119751, for Pascal by Arnmann 
1 19773, and for PL.8 by Auslander and Hopkins 119821. 
Program-usage statistics are useful: for compiler design. Knuth 1 197 I bj did 
an empirical study of Fortran programs. ElshofF 11976) provides some statis- 
tics on PL/! usage, and Shimasaki et a!. 11980j and Carter 119821 analyze Pas- 
cal programs. The performance of several compilers on various computer 
instruction sets is discussed by Lunde 119771. Shustek I19781, and Ditzei and 
Mctellan [ 1982 1, 
Many of the heuristics for code generation propied in this chapter have 
been used in various compilers, Freiburghouse 1 19741 discusses usage counts 
as an aid to generating good code for basic blocks. The strategy used in 
jymg of creating a free register by throwing out of a register the variable 
whose value wiv be unused for the longest time was shown optimal in a page- 
swapping context by k l a d y  [ I M J .  Our strategy of allocating a fixed mumkr 
of registers to hold variables for the duration of a loop was mentioned by 
Marill 11%2j and used in the implementation of Fortran H by Lowry and 
Medlock 119691. 
Horwitz a a]. 119661 give an algorithm for optimizing the use of index 
registers in Fortran. Graph coloring as a register-allocation technique was 
proposed by J .  Cwke, Ershov 11971 j, and Schwartz 1 19731. The treatment of 
graph coloring in Section 9.7 follows Chaitin et al. ) 1981 j and Chaitin 1 1982 1. 
Chow and Henncssy { 19841 describe a priority-based graph coloring algorithm 
for register allocation. Other approaches to register allocation are discussed 
by Kennedy 119721, Johnsson 119751, Harrison 119751, Beatty [1974j, and 
Leverett 1 19821. 
The labeling algorithm for trees in Section 9,10 is reminiscent of an algo- 
rithm for naming rivers: the confluence of a major river and a minor tributary 
continues to use the name of the major river; the confluence of two equal- 
sixd rivers is given a new name. The labeling algorithm originally appeared 
in Ershov 119583. Code-generation algorithms using this method have k e n  
proposed by Anderson 1 1%4 1, Nievergelt 1 1965 1, Nakata 1 1967 1, Redziejowski 
[1%9], and lkatty 119721, Sethi and Uliman 119701 used the labeling method 
in an algorithm that they were able lo prove generated optimal d e  for 
expression trees in a wide variety of situations. The procedure ~ m c d e  
in 
Section 9+10 is a modificalion of Sethi and Ullman's algorithm, due to 

584 CODE GENERATION 
CHAPTER 9 
Stwkhausen 119731. Bruno and Lasagne 119751 and Coffman and Sethi 
119831 give optimal degeneration algorithms for ekpression trees if the tar- 
get machine has registers that must be used as a stack, 
Aho and Johnson f19761 devised the dynamic programming algorithm 
described in Section 9,11. This algorithm was used as the basis of the cude 
generator in S.C. Johnson's portable (3 compiler, PCC2, and was also used by 
Ripken [I9771 in a compiler for the il3M 370 machine. Knuth [1977J general- 
ized the dynamic programming algorithm to machines with asymmetric regis- 
ter classes, such as the IBM 7090 and the CDC 6600. In developing the gen- 
eralization, Knuth viewed d
e
 generation as a parsing problem for context- 
free grammars. 
Floyd El9611 gives an algorithm to handle common subexprcssiwns in arith- 
metic expressions, The partition of dags into trees and the use of a prmxdure 
like $encode on the trees separately is from Waite [1976aj, Whi I19751 and 
Bruno and Sethi [I9761 show that the optimal code-generation problem for 
dags is NP-complete4 Aho, Johnson, and Ultman I1977al show that the prob- 
lem remains NP-complete even with single-register and infinite-register 
machines, Aho, Hopcroft, and Ullman 119741 and Garey and Johnson [C9791 
discuss t hc significance of what it means for n problem to be NP-complete. 
Transformations on basic b l d s  haw k e n  studied by Aho and Ullman 
[ 1972aI and by Downey and Sethi 119781. Peephole optimization is discussed 
by McKeeman l1%5], Fraser [1979], Davibsm and Fraser [I980 and 
1984a,b], Lamb 1 198 1 1, and Giegerich 119831, Tanenbaum, van Staveren, and 
Stevenson [ 19821 advocate using peephole optimization on intermediate d e  
as well, 
Code generation has been treated as a treerewriting process by Wasilew 
(19751, Weingart [1973], dohnwn I19781, and Cattcll 119801. The tree- 
rewriting example in Section 9.12 is derived from Henry [1984]+ Aha and 
Ganapathi [I9851 proposed the combination of efficient tree-pattern matching 
with dynamic programming mentioned in the same section. Tjiang El9861 has 
implemented a dc-generation language called Twig b a d  on the tree- 
translation schemes in Section 9.12, Kron [W75j, Hutt and Levy [1979], and 
Hoffman and O'hnnell [1982j descrik general algorithms for tree-pattern 
matching . 
The GraharncGlanville approach to d
e
 
generation by using an LR parser 
for i;nstruction selection is described and evaluated in Clan vilb [ 19771, Glan- 
vilk and Graham [ 19781, Graham [ 1980 and 19841, Henry 119841, and Aigrain 
et al. 119841. Ganapathi 119801 and Ganapathi and F i d w  119821 have used 
attribute grammars to s p i f y  and implement code generators. 
Other techniques for automating the mnstruclion of d
e
 
generators have 
been proposed by Fraser 119771, Cattelt [1980), and Leverett et al, 119801. 
Compiler portability B also discussed by Richards 11971 and 19771, Szy- 
manksi 119781 and Leverett and Szymanski [I9801 describe techniques for 
chaining span-dependent jump instructions, 
Yannakakis 119851 has a 
p l y  nomid-time algorithm for E~ercise 9.19. 

CHAPTER 
Code 
Optimization 
Ideally, cbmpilers should prduce targer code that is as good as can be written 
by hand. The reality is that this goal i s  achieved pnly in limited cass, and 
with difficulty. However, the code produced by straightforward compiling 
algorithms can often be made to run faster or take less space, or both. This 
improvement is achieved by program transformat ions that are traditionally 
called op~imizuriulrs, although the term "optimization" is a misnomer because 
there is rarely a guarantee that the resulting code is the best possible. Com- 
pilers that apply code-improving transformations are called optimizing wm- 
pikrs. 
The emphasis i? this chapter is on machine-independent optimizations, pro- 
gram transformations that improve the target code without taking into con- 
sideration any properties of the target machine. Machinedependent optimiza- 
tions, such as register allocation and utilization of special machine-instruction 
sequences (machine idioms) were discussed in Chapter 9. 
' 
The most payoff for the least effort is obtained if we can identify the fre- 
quently execuled parts of a program and then make these parts as efficient as ' 
possible+ There is a popular saying that most programs spend ninety per cent 
of their execution time in ten per cent of the code. While the actual percen- 
tages may vary, it is often the case that a small fraction of a program accounts 
for most of the running time. Profihg the run-time execution of a program 
on representative input data accurately identifies the heavily traveled regions 
of a program. Unfortunately, a compiler does not have the benefit of rsarnpk 
input data, so it must make its best guess as to where the program hot spots 
are. 
In practice. the program's inoer loops are g o d  candidates for improvement. 
In a language that emphasizes control constructs like while and for stalements, 
the loops may be evident from the syntax of the program; in general, a pro- 
cess called control-flow analyYis identifies loops in the flow graph OF a pro- 
gram. 
This chapter is a cornucopia of useful optimizing transformations and tech- 
niques for implementing them. The best technique for deciding what transfor- 
mations: are worthwhile to put into a compiler is to collect statistics a b u t  the 
source programs and evaluate the benefit of a given set of optimizations on a 

representative sample of real sour= programs. Chapter 12 describes transfor- 
mations that have proven useful in optimizing compilers for several different 
languages. 
One of the themes of this chapter is data-flow analysis, a process of collect- 
ing information about the way varhbles are used in a program. The informa- 
tion collected at various points in a program can k related using simple set 
equations. We present several algorithms for collecting information using 
data-flow analysis and far effectively using this information in optimization. 
We also consider the impacl of language constructs such as procedures and 
pointers on optimization. 
The lust four sections of this chapter deal with more advanced material. 
They cover some graph-theoretic ideas relevant to contrd-flow analysis and 
apply these ideas to data-flow analysis. The chapter concludes with a discus- 
sion of general-purpose tools for data-flow analysis and techniques for debug- 
ging optimized code. The emphasis throughout this chapter is on optimizing 
techniques that apply to languages in general. Some compilers that use these 
ideas are reviewed in Chapter 12. 
10.1 INTRODUCTION 
To create an efficient target language program, a programmer needs more 
than an optimizing compiler. in this wction, we review the options available 
to a programmer and a compiler for creating efficient target programs. We 
mention the types of code-improving transformations that a programmer and a 
compiler writer can be expected to use to improve the performance of a pro- 
gram, We a tso consider the representat ion of programs on which transforma- 
tions will be applied, 
Criteria for Code-Improving Transformations 
Simply stated, the best program transformations are those that yield the most 
benefit for the least effort. The transformations provided by an optimizing 
compiler shou Id have several properties. 
First, a transformation must preserve the meaning of programs. That is, an 
"optimimtion" must not change the output produced by a program for a given 
input. or cause an error, such as a division by zero, [hat was not present in 
the original version of the source program. The influence of this criterion 
pervades this chapter; at all times we take the "safe" approach of missing an 
opportunity to apply a transformation rather than risk changing what the pro- 
gram does. 
Second, a transformation must, on the average, speed up programs by a 
measurable amount. Sometimes we are interested in reducing the space taken 
by the mmpiled code, although the size of code has less importance than it 
once had. Of course, not every transformation succeeds in improving every 
program, and occasionally an "optimization" may slow down a program 
slightly, as tong as on the average it improves things, 

Third, a transformation must be worth the effort. I t  dws not make sense 
for a compiler writer to e~pend the intellectual effort to implement a code- 
improving transformat ion and to have the compiler expend the additional time 
cornp~ling source programs if this effort is not repaid when the target pro- 
grams are executed. Certain bcal or "peephole" transformations of the kind 
discussed in &dim 9.9 are simple enough and knekial enough to be 
included in any compiler. 
&me 
transformatbns can only be applied after detailed, often tirne- 
consuming, analysis of the source program, so there is little pint in applying 
them to programs that will be run only a few times. For example, a fast, 
nonoptimizing, compiler is likely to be more helpful during debugging or for 
"student jobs" that will be run sucoessfully a few times and thrown away. 
Only when the program in question takes up a significant fraction of the 
machine's cycles d0es improved d
e
 
quality justify the time spent running an 
optimizing compiler on the program. 
Dramatic improvements in the running time of a program - 
such as cutting 
the running time from a few hours to a few seconds - 
are usually obtained 
by improving the program at all levels, from the source level to the target 
Lcvd, as suggested by Fig. i0.1. At each levd, the available options fall 
between the two extremes of finding a better algorithm and of implementing a 
given algorithm so that fewer operations are performed. 
code 
generator 
code 
I 
user can 
profile program 
improve Imps 
usc registers 
change algorithm 
procedure calls 
select instructions 
t~ansfocm loops 
addrcss alcu tat ions 
do p p h d b  transformations 
Fig. 10.1, Places for potcntial improvcmcnts by thc uscr and th,: compiler. 
Algorithmic transformations occasionally produce spectacular improvements 
in running time. For example, k n l l e y  [I9821 relates that the running time of 
a program for sorting N elements dropped from 2 . 0 ~ ~  
rniaos~conds to 
12NlogzN microseconds when a carefully coded "insertion sort" was replaced 
by "quicksort. "' Far N = 100 the replacement speeds up the program by a 
' %e Aho. Hopoff, and Ullman 119831 for a discussion of t h c ~  
mtting algorirhrns and thcir 
speeds. 

588 
CODE OPTIMIZATION 
SEC. 10.1 
factor of 2.5. For N = 100,000 the improvement is far more dramatic: the 
replacement speeds up the program by a factor of more than a thousand. 
Unfortunately, no compiler can find the best algorithm for a given program. 
Sometimes, however, a compiler can replace a xequence of operations by an 
dgebraically equivalent wquence, and thereby reduce the running time of a 
program significantly, 
Such savings are more common when algebraic 
transformations are applied to programs in very-high level languages, cg., 
query languageh for databases (see Ullman 1 1982)), 
In this section and the next, a sorting program called quicksort will t~ used 
to illustrate the effect of various code-improving transformations. The C pra- 
gram in Fig. 10.2 is derived from Sedgewick 11978). where hand-optimization 
of such a program is discussed. We shall not discuss the algorithmic aspects 
of this program here - 
in fact. a [ 0 1 must contain the smallest and a[ max] 
the largest clement to be sorted for the program to work. 
void quicksort(m,n) 
int m,n; 
{ 
i n t  i,j; 
ht v , ~ ;  
i f  I n <= m 1 return; 
r'* 
Jrttpwrtr bqirrs hew 
/ 
i = m-7; j + n; v = a[n]; 
whiitll) { 
do i = i + l ;  while I a[i] 4 v ); 
do j = j-I; while I a l j ]  > v 1; 
if I i  >= j 1 break; 
x = a [ i ] ;  a[i] = a[j1; a [ j ]  = x; 
1 
x = a l i ] ;  a [ i ]  = a[n]; a[n] = x; 
f * fru~rnent tnds k ~ r c  * 1 
quicksort(m,j); quicksort(it1,n); 
1 
Fig. 10.2. C mdc fur quicksort. 
I t  may not be possible to perform certain code-improving transformations at 
the level of the source language. For example, in a language such as Pascal 
or Fortran, a programmer cart only refer to array elements in the usual way, 
e.g., as b [ i ,  j I .  At the level of the intermediate language; howeuer, new 
opportunities for code improvement may be exposed. Three-address code, for 
example, provides many opportunilics for improving address calculalions, 
especially in Loops. Consider the three-address code for determining the value 
of a[ il, assuming that each array element takes four bytes: 

SEC. 10.1 
INTRODUCTION 589 
Naive intermediate code will recalculate 4 * i  every time a[ i ] appears in the 
source program. and the programmer has no control over the redundant 
address calculations. because they are implicit in the implementation of the 
language, rather than being explicit in the code written by the user. In these 
situations, it behooves the compiler to clean them up. In a language like C, 
however, this transformalion can be done at the source level by the prograrn- 
mer, since references ro array elernen~s can be systematically rewritten using 
pointers ro make them more efficient, This rewriting is simiiar to transforma- 
tions that optimizing compilers for Fortran traditionally apply- 
At the level of the target machine, it is the compiler's responsibility to make 
good use of the machine's resources. For example, keeping the most heavily 
used variables in registers can cut running time significantly, often by as much 
as a half. Again, C allows a programmer to advise the compiler that certain 
variables be held in registers, but most languages do not. Similarly. the corn- 
piler can speed up programs significantly by choosing instructions that take 
advantage of [he addressing modes uf the machine to do in one instrucfion 
what naively we might e x p c t  to require two or three, as we discussed in 
Chapter 9. 
Even if it is possible for the prclgramnler to improve the code, it may be 
more convenient to have the compiler make some of the improvements, If a 
compiler can be relied upon to generare efficient code, then the user can con- 
centrarc on writing clear code. 
Am Organization for an Optimizing Compiler 
As we have mentioned, there are often several levels at which a program can 
be improved, Since (he techniques needed to analyze and transform a pro- 
gram do not change significantly with the level, this chapter concentrates on 
the tranr;rwrrnstiun of intermediate c d e  using the organization shown in Fig+ 
10.3. The code-improvement phase consists of contrd-flow and data-flow 
analysis followed by rhe application of transformatioos. The code generator, 
discussed in Chapter 9, produces the target program from the transformed 
intermediate code. 
For convenience of presentation, we assume that the intermediate code con- 
sists of three-address statements. Intermediate code, of the sort produced by 
the techniques in Chapter 8, for a portion of the program in Fig. 10.2 I s  
shown in Fig. 10.4. With urher intermediak representations. the temporary 
variables t,, 
t2. . . . , t15 
in Fig. 10.4 need not appear explicttly. as dis- 
cussed in Chapter 8- 
The organization in Fig. 10.3 has the following advantages: 
I. The operations needed to implement high-level construcls are made expli- 
cit in the intermediate code, so il is possible to optimize I hem. Far exam- 
ple, the address calculalion~ for a[ i] are explicit in Fig, 10.4, so the 
recornpu~ation of expressions like 43i can be eliminated as discussed in 
the next section. 

. . . . . . . . . . . . . . . .  
. . . . . . . . . . . . . . . . .  
: 
front 
, . . . . . . . . . .  
d
c
 
' 
end 
optimizer 
I 
. . . . . . . . . . . . . . . .  
0 
.- 
I 
.-- 
-. 
# 
I 
.- 
r 
% 
, 
transfor- 
mations 
analysis 
analysis 
Mg. 10.3. Organization of the c d c  optimizer. 
Fig. 1k4. Three-address code for fragment in Fig. 10.2. 
2. 
The inlerrnediate d
e
 can be (relatively) independent of the target 
machine, so the optimizer does not have to change much if the d
e
 
gen- 
erator is replaced by one for a different machine. The intermediate code 
in Fig. iO.4 assumes that each element of the array a takes four bytes. 
Some intermediate codes, e.g., P-cde for Pascal, leave it to the d
e
 
gen- 
erator to fill in the size of array elements, so the intermediate code is 
independent of the size of a machine word. We could have done the 
same in our intermediate d e  if we replaced 4 by a symblic constant, 

SEC. 10.1 
INTRODUCTION 
591 
In the code optimizer, programs are represented by flow graphs, in which 
edges indicate the flow of control and nodes represent basic blocks, as dis- 
cussed in Section 9.4, Unless otherwise specified, a program means a single 
procedure. In Section 10.8, we discuss interprocedural optimization. 
Example 10. I. Figure 10.5 contains the flow graph for the program in Fig. 
10.4. B 
is the initial node. All conditional and unwnditional jumps to state- 
ments in Fig. 10.4 have been replaced in Fig. 10.5 by jumps to the block of 
which the statements are kaders, 
In Fig. 10.5, there are three loops. B 2  and B3 are loops by themselves. 
Blaks B z ,  By, B,, and Bs together form a loop, with entry B2. 
IJ 
Fig. 10.5, Flow graph. 

10-2 THE PRINCIPAL SOURCES OF OPTlMIZATION 
In this section, we introduce mme of the most useful code-improving transfor- 
mations. Techniques for implemenling these transformations are presented in 
subsequent sections, A transformation of a program is called Imal if it can be 
performed by looking only at the statements in a basic blwk; otherwise, it is 
called giuhb. Many transformations can be performed at both the local and 
global leveki. Local transformations are usually performed first. 
There are a number of ways in which a compiler can improve a program 
wifho~i changlng the function it computes. Common subexpression elimina- 
tion, copy propagation, dead-code climhation, and constant folding are com- 
mon examples of such function-preserving transformations. Section 9.8 on the 
dag representation of basic blocks showed how Iwal common subexpressions 
could be removed as the dag for the basic block was constructed. The other 
mnsformations come up primarily when giobal optimizations are performed, 
and we shall discuss each in turn. 
Frequently, a program will include several calculations of the same value. 
such as an offset in an array. As mentioned in Section 10.1. some of these 
duplicate calculations cannot k avoided by the programmer because [hey lie 
below the lwei of detail accessible within the source language. For example, 
block B? shown in Fig. I0.qa) recalculates 4+i and 4* j. 
Fig, 10.6. 
Local ctlmrnon subcxprcssiun dimination. 
Common Subexpressions 
An occurrence of an expression E is called a common subexprrssion if E was 
previously computed, and the values of variables in E have not changed since 
the previous computation, We can avoid recomputing the expression if we can 
use the previously computed value. Fur example, the assignments to t7 and 

SEC. 10.2 
THE PRINCIPAL SOURCES OF OPTIMIZATION 593 
t lo have the common subexpressions 4+ i and 4* j, respectively + on the right 
side in Fig. 10.6(a). They have been eliminated in Fig. 10.6(b), by using th 
instead of t7 a d  tg instead of tlD. 
This change is what would result if we 
reconstructed the intermediate code from the dag for the basic block. 
Example 10.2. Figure 10.7 shows the result of eliminating both global and 
local common subexpressions from blocks B s  and Bh in the flow graph of Fig. 
10.5. We first discuss the transformation of BS and then mention some 
subtleties involving arrays. 
Fig. 10.7. Bs 
and B6 aftcr common subcxprcssion clirnination. 
After local common subexpressions are eliminated, Bs stiil evaluates 4 * i  
and 4+ j, as shown in Fig, 10+6(b)+ Both are common subexpressions; in par- 
ticular, the three statements 
in B can be replaced by 

594 
CODE OPTIMIZATION 
SEC. 10.2 
using t4 computed in block B 3 .  In Fig. 10.7, observe that as control passes 
from the evaluation of 4* j in B ,  to B 5 ,  there is no change in j, so t4 can be 
used if 4+ j is needed. 
Another common subexpression comes to light in B5 after t4 replaces tg. 
The new expression a[ t, 1 correspnds to the value of a[ j l  at the source 
level. Not only does j retain its value as control leaves B 3  and then enters 
B5, but a [ j 1, a value computed into a temporary t5, does too because there 
are no assignments lo elements of the array a in the interim. The statements 
in B 5  can therefore be feplaad by 
Analogously, the value assigned to x in block B 5  of Fig. IO.qb) is seen to 
be the same as the value assigned to t, in block B2. Blwk B5 in Fig. 10.7 is 
the resu It of eliminating common subexpressions corresponding to the values 
of the source level expressions a{ i] and a [ j ] from 3 in Fig. lO+6(b) 
+ 
A 
similar series of transformations has been done to Bg in Fig. 10.7. 
The expression a[ t ] in blocks B I and B of Fig. 10.7 is not considered a 
common sukxpression, although t can Ix used in both places. After control 
leaves B 
and before it reaches B6, i t  can go through B 5 ,  where there are 
assignments to a. Hence, a E tr 
3 may not have the same value on reaching B6 
as it did on leaving B 4 ,  and it is not safe to treat a[ tl 
] as a common subex- 
pression, 
o 
Copy Propagation 
Block B5 io Fig. 10.7 can be further improved by eliminating x using two new 
transformations. One concerns assignments of the form f : =g called copy 
sratemmrs, or copies for short. Had we gone into more detail in Example 
10.2, copies would have arisen much sooner, kcausc the algorithm for elim- 
inating common subexpressions introduces them, as do several other algo- 
rithms. For example. when the common subexpression in c :  =d+e is dim- 
inated in Fig. 10.8, the algorithm uses a new variable t to hold the value of 
d+e. Since control may reach c; =d+e either after the assignment to a or 
after the assignment to b, it would be incorrect to replace c : =d+e by either 
c:=aor by c:=b. 
The idea behind the copy-propagation transformation is ro use g for f ,  
wherever possible after the copy statement f : 
=g. For e~ampie. the assign- 
ment x : =t3 in bbck B5 of Fig. 10.7 is a copy + Copy propagation applied to 
B yields: 

THE PRLNClPAL SOURCES OF OPTlMlZATlON 595 
Rg. 10.8. Copktes inttduced during mmcm sukxpression elimination. 
This may not appar to be an improvement, but as we shall we, it gives us the 
opportunity to eliminate the assignment to x. 
A variable is live at a pint in a program if its value can be used subsequently; 
otherwise, it is dead at that point. A related idea is dead or useless mde, 
statements that compute values that never get used. While the programmer is 
unlikely to introduce any dead c d e  intentionally, it may appear as the result 
of previous'transformations. Far cxainple, in Section 9.9 we discussed the use 
I 
of debug that is set lo true or false at various pdnts in the program, and 
used in statements like 
if {debug) print ... 
( 10.2) 
By a data-flow analysis, 11 may be possible to deduce that each time the pro- 
gram teaches this statement, the value of debug is false. Usually, it is 
because there is one particular statement 
debug := false 
that we can deduce to be the last assignment to debug prior to the test ( 104, 
no matter what sequence of branches the program actually takes. If copy pro- 
pagation replaces debug by false, then the print statement is dead because it 
cannot be reached. We can eliminate both the test and printing from the 
abject code. More generally, deducing at compile time that the value of an 
expression is a constant and using the constant instead i s  known as constunt 
f0ldi~g. 
One advantage of copy propagation is that it often turns the copy statement 
into dead cde. For example, copy propagation followed by dead-code elimi- 
nation removes the assignment to x and transforms (10.1) into: 
bhis code is a furher impmvemenr of block B5 in Fig. IO.7. 

SEC. 10.2 
We now give a brief intrduction to a very important place for optimizations, 
namely loops, especially the inner Imps where programs tend ro spend the 
bulk of their time. The running time of a program may be improved if we 
decrease the number of instructions in an inner loop, even if we increase the 
amount of code outside that Imp. Three techniques are important for h
p
 
optimization; C'O& motion, which moves d
e
 
outside a Imp; inrltccrion-vuriabh 
diminrrtirm, which we apply to eliminate i and j from the inner Imps B2 and 
B ,  of Fig. 10.7: and, rclIrrctbn in srrpngrk, which replaces an expensive opera- 
tion by a cheaper one, such as a rnuliiplication by an addition. 
Code Motion 
An important modification that decreases the amount of code in a loop is code 
motion. This transformation takes an expression that yields the same result 
independent of the number of times a Imp is executed la Imp-invuriunr cum- 
putation) and places the expression before the loop. Note that the notion 
"before the loop" assumes the existence of an entry for the bop. For exam- 
ple, evaluation of l i m i t - 2  is a Imp-invariant computation in the following 
while-statement: 
while { i <s limit-2 ) / x  statement does nut change limit */ 
Code mot ion will result in the equiv;alenl of 
t = limit-2; 
while ( i <= t ) /r: statement does not change limit or t */ 
Induction Variables and Reduction in Strength 
While code motion is nut applicable to the quicksort example we have been 
considermg, the other two transformations are. Loops are usually processed 
inside out. For e~amplc, consider ttte Imp around 3 ) .  Only the portion of 
the flow graph relevant to the transformations on B 3  is shown in Fig. 10,9. 
Note [hat the values of j and t4 remain in kk-step; every time the value 
of j decreases by 1, that of t4 decreases by 4 because 4* j is assigned to t4. 
Such identifiers are called iducrim vwkblc.s. 
When there are two or more induction variables in a loop, it may be possi- 
ble to get rid of ail but one, by the process of induction-variable elimination. 
Fw the inner loop around B3 in Fig. 10.9Ia). we cannot get rid of either j or 
t4 completely; t4 is u e d  in B3 and j in B 4 .  However, we can illustrate 
reduction in strength and Illustrate a part of the process of induction-variable 
elimination. Eventually. j will be eliminated when the outer Imp of B 2 - B S  
is considered, 
Example 10.3. 
As the relationship t, = 4+ j surely holds after such an 
assignment to t4 in Fig. 10.9(a), and t4 is nor changed elsewhere in ttte 
inner loop around B ) ,  it follows that just af~er the statement j : = j- 1 the 

THE PRlNClPAL SOURCES OF OPTlMlZATlON 597 
Fig. 10.9. Strength rcductim applicd lo 4+ j in block B 3 .  
relationship t4 - 4+ j-4 must hold. We may therefore replace the assign- 
ment t4:=4*j by t4:=t4-4. 
The only problem is that ta does n d  have a 
value when we enter block B3 for the first time. Since we must maintain the 
relationship t4 - 4+ j on entry lo the Mock 8 I ,  we place an initialization of 
t4 at the end of the block where j itself is initialized. shown by the dashed 
addition to block 8 in Fig. IO.B(b). 
The replacement of a multiplicaticsn by a subtraction will speed up the object 
code if multiplication takes more time than addition or subtraction. as is the 
case on many machines. 
u 
Section 10.7 discusses how inducrioo variables can be detected and what 
transformations can be applied. We conclude this section with one more 
example of induction-variable elimination that treats i and j in the context of 
the outer loop containing B 2 ,  B 3 ,  84, and B , .  
Example 10.4. 
After reduction in strength Is applied to the inner Imps 
around B 2  and B ] ,  the only use of i and j is to determine the outcome of the 
test in block B4. We know that the values of i and t2 satisfy the relation- 
ship t2 - 4 t i ,  while t h w  of j and t4 satisfy the relationship t4 = 4 * j ,  
so [he test t22=td is equivalent to i>= j, Once this replacemen1 is made, i 
in b!ock 8 
2 and j in block B3 become dead variables and the assignments to 
them in these blocks become dead c d e  that can be eliminated, resulting in the 
flow graph shown in Fig+ 10.10. 
IJ 

598 
CODE OPTIMIZATION 
Fig. 10.10, Flow graph after induction-variable elimination, 
The axle-improving transformations have been effective. In Fig. 10. LO, the 
number of instructions in blacks B 2  and Bj has been reduced from 4 to 3 
from the original flow graph in Fig, 10.5, in B5 it has been reduced from 9 to 
3, and in Bg from 8 to 3. True, B ,  has grown from four instructions to six, 
but B I is executed only once in the fragment, w the total running time is 
barely affected by the size of B I . 
10,3 OPTIMIZATION OF BASIC BLOCKS 
In Chapter 9, we saw a number of code-improving transformations for basic 
blocks. These included structure-preserving transformations, such as common 
subexpression elimination and dead-code elirninat ion, and algebraic transfor- 
mations such as reductioo in strength. 
Many of the structure-preserving transformattons can be implemented by 
constructing a dag for a basic block. Recall that there is a n d e  in the dag for 
each of the initial values of the variables appearing in the basic block, and 
there is a n d e  R associated with each statement s within the block. The chil- 
dren of n are those nodes corresponding to statements rhat are the last defini- 
tions prior to s of the operands used by $. Node rr is labeled by the operator 

SEC. 10.3 
WTlMlZATlON OF BASIC BLOCKS 599 
applied at s, and also attached to n is the list of variables for which it is the 
last definition within the block. We also note those nodes, if any, whose 
values are live on exit from the b h k ;  these are the output nodes. 
Common subexpressions can be detected by noticing, as a new node no is 
about to be added + whether there is an existing node n with the same children, 
in the same order, and with the same operator. If so, II computes the same 
value as rn and may k used in its place. 
Example 1O.S. 
A dag for the block ( 10.3) 
is shown in Fig. la+ 11. When we construct the node for the third statement 
e : =b+c, we know that the use of b in b+c refers to the node of Fig. 10.1 1 
labeled -, because that is the most recent definition of b. Thus, we do not 
confuse the values computed at statements one and three. 
Fig. 10.11. Dag for basic block I lO+3). 
However, the node corresponding to the fourth statement d:=a-d has the 
operator - and the nodes labeled a and + as children. Since the operator 
and the children are the same as those for the node corresponding to state- 
ment two, we do not create this node, bur add d to the list of definitions for 
the node labeled -. 
o 
I t  might appear that, since there are only three nodes in the dag of Fig. 
10.11, block (10.3) can k replaced by a block with only three statements. In 
fact, if either b or d i s  not live on exit from the block, then we do not need 
to compute that variable, and can use the other to receive the value 
represented by the node Iabeled - in Fig. 10.1 1 . For example, if b is not live 
on exit, we could use: 

CODE OPTIMlZATION 
SEC. 10.3 
However, if both b and d are live on exit, then a fourth statement must be 
used to copy the value from me to the other.2 
Note that when we l w k  for common subexpressions, we really are looking 
for expressions that are guaranteed to compute the same value, no matter how 
that value is computed. Thus, the dag method will miss the fact that the 
expression computed by the first and fourth statements in the q u e n c e  
is the same, namely, b+c. However, algebraic identities applied to the dag, 
as discussed next, may expose the equivalence. The dag for this sequence is 
shown in Fig. 10.12. 
Fig. lk12. Dag for basic block { 10.4). 
The operat ion on dags that mrcespctnds to dead-code elimination is quite 
straightforward to implement. We delete from a dag any root (node with no 
ancestors) chat has no live variables. Repeated application of this transforma- 
tion will remove all nodes from the dag that correspond to dead code. 
The Use of Algebraic Identitis 
Algebraic ident i r k s  represent another important class of optimizations on basic 
blocks. In Seaion 9.9, we intrduced some simple algebraic transformations 
that one might try during optimization. For example, we may apply arith- 
metic identities, such as 
In general, we have to be careful when reconstructing code From dags to & m e  rhe names of 
variables corresponding to nodes carefully. If a variabk x is defined twice, or if it is assigned 
once and the initial value q is also used, then we must make sure rhnt we do not change the value 
of x until we have made all uses of ihe mode whose value x previously held. 

OPTIMIZATION OF BASIC BLOCKS 60 1 
Another class of algebraic optimimt ions includes reduction in strength, that 
is, replacing a more expensive operator by a cheaper one as ih 
A third class of related optimizations is constant folding. Here we evaluate 
constant expressions at compile time and replace the constant expressions by 
their values.' Thus the expression 213.14 would be replaced by 6.28. 
Many constant expressions arise through the use of symbolic constants. 
The dag-construcfion process can help us apply these and other more gen- 
eral algebraic transformations such as corn mutativity and assaiatjvity . For 
example, suppose * is commutative; that is, x*y = y*x. kfore we create a 
new node labeled 
with left child m and right child n, we check whether such 
a node already exists, We then check for a node having operator *, left child 
n and right child rn. 
The relational operators <=, r = .  <, >, =. and # sometimes generate unex- 
pected common subexpressions. Fw example, the condition x ~ y  
can also be 
tested by subtracting the arguments and performing a test on the condition 
code set by the subtraction. (The subtraction can, however, introduce over- 
flows and underflows while a compare instruction would nor.) Thus, only one 
node of the dag need be generated for x-y and x,y+ 
Associative laws may also be applied ro expose common subexpressions+ 
For example, if the source code has the assignments 
the followtng intermediate code might be generated: 
I f  t is not needed outside this block, we can change this sequence to 
' Arithmeric expressions should be evaluared thc same way at compile rime as they are ah run 
rimc. K. Thompwn has suggested an elegant sulutiun to constant Fuldinp: cornpi!e the constan4 
expression. excculc h e  large1 mdc oc, the spol. and replace thc crpression with the resuit. Thus, 
the compiler dws nut netd it] contain an interprerer. 

602 
CODE OPTlMlZATlON 
using both the associativity and commutativity of + . 
The compiler writer should examine the language specification carefully to 
determine what rearrangements of computations are permitted, since computer 
arithmetic does not always obey the algebraic identities of mathematics, For 
example, the standard for Fortran 77 states that a compiler may evaluate any 
mathematically equivalent 
expression, provided 
that 
the integrity of 
parentheses is not vidated. Thus, a compiler may evaluate x*y-x*z 
as 
x* I y-8 1 but it may not evaluate a+ (b-c 1 as I a+b) -c. A Fortran compiler 
must therefore keep track of where parentheses were present in the source 
language expressions, if it is to optimize programs in amdance with the 
language definition. 
10+4 LOOPS IN FLOW GRAPHS 
Before considering Imp optimizations, we need to define what constitutes a 
loop in a flow graph. We shall use the notion of a itode "dominating" 
another to define "natural loops" and the imprtant special class of "reduci- 
ble" flow graphs, An algorithm for finding dominators and checking reduci- 
bility of flow graphs will be given in Section 10.9. 
We say node d of a flow graph duminam node n, written d durn n, if every 
path from the initial node of the flow graph to n goes through d .  Under this 
definition, every node dominates itself, and the entry of a Imp (as defined in 
Section 9.4) d0minata all ndes in the loop. 
Example 10.6. Consider the flow graph of Fig. 10.13, with initial node I. 
The initial node dominates every node. Node 2 dominates only itself, since 
control can reach any other node along a path that begins 1 -. 3. Node 3 
dominates all but 1 and 2. N d e  4 dominates all but 1, 2 and 3, since all 
paths from 1 must begin 1 -. 2 
3 -. 4 or 1 -. 3 + 4, Nodes 5 and 6 dom- 
inate only themselves, since flow of control can skip around either by going 
through the other. Finally, 7 dominates 7, 8, 9, 10; 8 dominates 8, 9, 10; 9 
and 10 dominate only themxlves. 
o 
A useful way of presenting dominator information is tn a tree, called the 
dumi~rur free. in which the initial node is the root, and each nodc d dom- 
inates only its descendants in the tree+ For example, Fig. 10.14 shows the 
dominator tree for the flow graph of Fig. LO. 13. 
The existence of dominator trees folk~ws from a properly of dominators; 
each nodc n has a unique immed& 
bumimmr m that is the Last dominator of 
n on any path from the initial node to n. In terms of the bum relation, the 
immediate dominator m has that property that if 6 3 n  and d durn n, then 
d d m  m. 

LOOPS IN FLOW GRAPHS 603 
FPg. 1L 13. Elow graph. 
Fig. IQ.14. Dominator trcc for flow graph of Fig. 10.13. 
Natural Loops 
One important application of dominator informat ion is in determining the 
Imps of a flow graph suitable for improvement, There are two essential pro- 
perties of such loops. 
1. 
A loop must have a single entry point, called the "header," This entry 
pint dominates all nodes in the loop, or it would not be the sole entry to 
the Imp. 
2. There must be at least one way to iterate the loop, ix., at least one path 
back to the header. 
A good way to find all the loops in a flow graph is to search for edges in 

the flow graph whose heads dominate their tails. (If a + b is an edge, b is the 
kd and a is the rail.) We call such edges h c k  edges. 
Example 10.7. In Fig. 10+13, there is an edge 7 + 4 ,  and 4 d m  7 .  Simila~ly, 
10 -. 7 is an edge, and 7 h m  10. The other edges with this property are 
4 + 3, 8 -c 3, and 9 - I. Note that these arc exactly the edges that one would 
think of as forming loops in the flow graph. 
o 
Given a back edge n -. ti, we define the norwul Imp of the edge to be d 
plus the set of nodes that can reach a without going through d. Node d is the 
header of the loop. 
Example 10.8. The natural loop of the edge 10 + 7 consists of nodes 7, 8, 
and 10, since 8 and 10 are all those nodes that can reach !O without going 
through 7. The natural Imp of 9 -. I is the entire flow graph, (Don't forget 
the path 10 + 7 + 8 - 9?) 
o 
Algorithm 10-1. Constructing the natural Imp of a back edge, 
input. A flow graph G and a back edge n - J. 
O~tp-pur, The set loop consisting of all nodes in the natural loop of n - d. 
Method. Beginning with node n, we consider each node m #d that we know i s  
in Imp, to make sure that m's predecessors are also plactd in Imp. The a1go- 
rithm is given in Fig, 10,J5+ Each node in Imp, except for b, is placed once 
on suck, so its predecessors will be examined. Note that because d is put in 
the loop initially, we never examine its predecessors. and thus find only those 
nodes that reach n without going through 6. 
o 
pmcedrre inserr(rn); 
iP m is not in Imp then bgin 
hop : = loop U {m ) ; 
push m onto .mck 
end; 
/* main program folhws * / 
? i m k  := cmpty; 
imp := { d}; 
imm( 1 ; 
while srack is not ernpcy do begin 
pop m. the first elemen1 of sfauk. off stuck; 
for tach predecessor p of m do inseri(p ) 
end 
Fig. 10.15. Algorikhm for constructing the natural loop. 

LOOPS IN FLOW GRAPHS 605 
If we me the natural loops as "the bops," then we have the useful property 
that unless two loops have the same header, they are either disjoint or me is 
entirely contained ( n a r d  wtihin) the other. Thus, negkding Imps with the 
same header for  he moment, we have a natural notion of inner bop: one that 
contains no other loops. 
When two loops have the same header, as in Fig. 10.16, it is hard to tell 
which is the inner Imp. For example. if the test at the end of B ,  were 
if a = 1 0 g b t o B 2  
probably the loop {B,, B I , B 3 }  would be the inner loop. However, we could 
not be sure without a detailed examination of the code, Perhaps a is almost 
always 10, and it is typical to go around the {B,,, B , ,  8 2 )  bop many times 
before branching ro B 3 .  Thus, we shall assume that when two natural Imp 
have the same header, bur neither is nested within the other, they are corn- 
bined and treated as a single Imp. 
Fig, 10.11. T w t ~  Icwps with thc samc header 
Pre-Headers 
.Several transformations require us to move statements "before the header." 
We therefore begin treatment of a loop L by creating a new block. called the 
pr~heudtr. The preheader has only the header as successor, and all edges 
which formerly entered the header of L from outside L instead enter the pre- 
header. Edges from inside Imp L to the header are not changed. The 
arrangement is shown in Fig. 10.17. Initially, the preheader is empty. but 
transformations on L may place statements in it. 

CODE OPTIMIZATION 
header 
f I 3 - I  
(a) Beforc 
Fig+ 10,17. Introduction of the prehsadcr. 
Reduaile Flow Graphs 
Flow graphs that m
r
 
in practice frequently fall into the class of reducible 
flow graphs defined below. Exclusive use of structured flow-of-control state- 
ments such as if-then-eise, while-do, a
t
 
hue, and break statements produces 
programs whose flow graphs are always reducible. Even programs written 
using goto statements by programmers with no prior knowledge of structured 
program design are almost always reducible. 
A variety of definitions of "reducible flow graph" have been proposed. The 
one we adopt here brings out one of the most important properties of reduci- 
ble flow graphs; namely, that there are no jumps into the middle of Imps 
from outside; the oniy entry to a Imp is through its header. The exercises 
and bibliographic notes contain a brief history of the concept. 
A flow graph G is reducible if and only if we cam partition the edges into 
two dlsj~int p u p s ,  often called the forward edges and Back edges, with the 
following two properties: 
1. 
The forwa~d edges forrn an acyclic graph in which every node can be 
reached from theinitiai node of G. 
2, 
The back edges consist only of edges whose heads dominate their tails. 
Example 10,% The flow graph of Fig. 10.l3 is reducible. In general, If we 
know the relation born for a flow graph, we can find and remove all the back 
edges. The remaining edges must be the forward edges if the graph is reduci- 
ble, and to check whether a flow graph is reducible it suffices to check that 
the forward edges forrn an acyclic graph. In the case of Fig. 10.13, it i s  easy 
to check that if we remove the five back edges 4 + 3, 7 + 4, 8 -. 3 ,  9 -. 1, 
and 10 + 7, whose heads dominate their tails, the remaining graph is acydic. 
Emrnpde 10.10. Consider the flow graph of Fig. 10.18, whose initial node is 
1. This flow graph has no back edges, since no head of an edge dominates 
the tail of that edge. Thus it could only be reducible if the entire graph were 
aqdic. But since it is not, the flow graph is not reducible. Intuitively, the 
reason this flow graph iS not reducible is that the cycle 2-3 can Ix entered at 

SEC. 10.4 
LOOPS IN FLOW GRAPHS 
607 
Fig. 10.18. A nonrcduciblc flow graph 
two different places, nodes 2 and 3. 
o 
The key property of reducible flow graphs for loop analysis is that in such 
flaw graphs every set of n d e s  that we would informally regard as a hop must 
mntain a back edge, In fact, we need examine only the natural loops of back 
edges in order ro find all Imps in a program whose flow graph is reducible. 
In contrast, the flow graph of Fig. 10.18 appears to have a "Imp" consisting 
of nodes 2 and 3, but there is no back edge of which this is the natural loop. 
In fact, that "loop" has two headers. 2 and 3, making application of many 
de-optimization techniques, such as those introduced in kction 10.2 for 
code motion and induction variable removal, not directly applicable. 
Fortunately, nonreducible control-flow structures, such as that in Fig. i0.18. 
appear so rarely in most programs as to make the study of bops with more 
than one header of secondary importance. There are even languages, such as 
Bliss and Modula 2, that allow only programs with reducible flow graphs., and 
many other languages will yield only reducible flow graphs as long as we use 
no goto's. 
Example 10.11. Returning again to Fig. 10.13, we note that the only '+inner 
lay,'' that is, a bop with no sub\oops, is (7, 8, lo), the natural Imp of back 
edge 10 -. 7. The st (4, 5,'6, 7, 8, 10) is the natural lwp of 7 -.4. 
(Note 
that 8 and I0 can reach 7 via edge 10 -. 7.) Our intuition (hat {4, 5, 6. 7) 
forms a loop is wrong, since 4 and 7 would bcith be entries from outside. 
violating our single-entry requirement. Put another way, there is no reason to 
assume that wntrol spends much time going around the set of ndes (4, 5, 6, 
7); it is just as plausible that control passes to 8 from 7 more often than it 
does to 4, By including 8 and 10 in the Imp. we are more certain of having 
isolated a heavily traveled region of the program. 
It is wise to recognize, however, the danger in making assumptions about 
the frequency of branches, For example, if we moved an invariant statement 
out of 8 or 10 in loop {7, 8, 101, and in fact, control followed edge 7 -. 4 
more frequently than 7 - 8. we would actually increase the number of times 
the moved statement was executed, We shalI discuss methods to avoid this 
problem in Section 10.7. 
The next larger Imp is (3, 4, 5 ,  6, 7, 8, $01, which i s  the nabm! Imp of 
both edges 4 - 3 and 8 - 3. As before, our intuition that {3, 4) shouid be 
regarded as a Imp violates the single header requirement. The last ioop, the 
one for back edge 9 -. 1, is the entire flow graph. 
o 

There are several additional useful properties of reducible flow graphs, 
which we shall introduce when we discuss the topics of depth-first search and 
interval analysis in Section 10.9. 
10.5 INTRODUCTION TO GWBAL DATA-FLOW ANALYSIS 
In order to do code optimization and a good job of c d e  generation, a corn- 
piler needs: to collect information about the program as a whole and to distri- 
bute this information to each blwk in the flow graph. For example, we saw 
in Section 9.7 how knowing what variables are live on exit from each block 
muid Improve register usage. Section 10.2 suggested how we could use 
knowledge of global common subexprcssims to eliminate redundant computa- 
tions. Likewise, Sections 9.9 and 10+3 discussed how a compiler could take 
advanlage of "reaching definitions," such as knowing where a variable like 
debug was last defined before reaching a given block, in order to perform 
transformations like constant folding and d e a d - d e  elimination, These Facts 
are just a few examples of data-flow informaiiun that an optimizing compiler 
colkas by a process known as baca$ow anubysis. 
Data-flow information can k mllecred by setting up and wiving systems of 
equations that relate informatim at various points in a proBram. A typical 
equation has the form 
and can be read as, "the information at the end of a statement is either gen- 
erated within the statement, or enters at the beginning and is not killed as 
control flows through the statement." Such equations are called dutu-flnw 
equrrtions* 
The details of how data-flow equations are set up and solved depend on 
three factors. 
The notions of generating and killing depend on the desired information, 
i.e., on the data-flow analysis problem to tx solved. Moreover, for some 
problems. instead of proceeding along with the flow of control and defin- 
ing out[Sj in terms of in($], we netd to proceed backwards and define 
in [$ j in terms of our IS 1, 
Since data flows along control paths, data-flow analysis is affected by the 
control constructs in a program. In fact, when we write outlS 1 we impli- 
citly assume that there is unique end point where wntrd leaves the state- 
ment; in general. equations are set up at the level of basic blocks rather 
than statements, because blocks do have unique end points. 
There are subtleties that go along with such statements as procedure calls, 
assignments through poinler variables, and even assign mats to array 
variables, 
In this section, we consider the problem of determining the set of 

SEC. 10.5 
INTRODUCTION TO GLOBAL DATA-FLOW AN ALYHS HB 
definitions reaching a point in a program and its use in finding opportunities 
for constant folding. Later in this chapter, algorithms for code motion and 
induction variable elimination will also use this information. 
We initially consider programs constructed using If and do-while statemen@. 
The predictable control flow in these statements allows us to concentrate on 
!he ideas needed to set up and solve data-flow quatims. Assignments in this 
section are either copy statements or are of the form a:=b+c. In this 
chapter, we frequently use "+" as a typical owrator. All that we say applies 
straightforwardly to other operators, including those with one operand, or 
with more than two operands. 
Within a basic block, we talk of the pitit between two adjacent statements, as 
well as the pint before the first statement and after the laa. Thus, blwk B , 
in Fig. 10.19 has four points: one before any of the assignments and one after 
each of the three assignments. 
d,: i := i + l  
1 
Fig. 10.19. A flow graph. 
Now, let US take a gbbal view and consider all the points in all the blocks. 
A path from p to p, is a sequence of pints p , p2, . . . , p,# such that for 
each i between 1 and n - I ,  either 
1. 
pi is the point immediately preceding a statement and pi+ is the p i n '  
immediately following that statement in the same block, or 
2. 
pi is the end of sume block and pi + I is the beginning of a successr>r block. 
Example 10,12. In Fig. 10,19 there is a path from the beginning of block Bs 
to the beginning of block Bb. It travels through the end pint of B and thCn 

6 10 
CODE OPTIMIZATION 
SEC. 10.5 
through ail the pints in B 2 ,  B 3 ,  and B4, in order, bcforc it reaches the begin- 
ning of Bb. 
o 
A dejniliun of a variable x i s  a statement that assigns, or may assign, a value 
to x. The most common forms of definition are assignments to x and state- 
ments that ~ e a d  a value from an PO device and slore it in x. These state- 
ments certainly define a value for x, and they are referred to as unambiguous 
definitions of x. There are certain other kinds of shtements that may define 
a value for x; they are called ambiguous definitions. The most usual forms of 
ambiguous definitions of x are: 
1 .  
A call of a procedure wtth x as a parameter (other than a by-value 
parameter) or a procedure that can access x because x is in the scope of 
the procedure. We also have to consider the possibility of "aliasing," 
where x i s  not in the scope of the prwdure, but x has ken identified 
with another variable that is passed as a parameter or is in the scope. 
These issues are laken up in Section 10.8, 
2. 
An assignment through a pointer that could refer to x. For example, the 
assignment *q: =y is a definition of x if it i s  possible that q pints to x. 
Methods for determining what a pointer could point to are also discussed 
in Section 10.8, but in the absence of any knowledge to the contrary, we 
must assume that an assignment through a pointer is a definition of every 
variable. 
We say a definition d reaches a point p If there is a path from the point 
immediately following d to p, such that d is not "killed" along that path. 
[ntuitively, if a definition d of some variable a reaches p i n 4  p, then d might 
be the place a1 which the value of a used at p might last have been defined. 
We kiII a definition of a variable a if ktween two points along the path there 
is a definition of a. Notice that only unambiguous definitions of a kill other 
definitions of a. Thus, a point can k rcached by an unambiguaus definition 
and an ambiguous definition of the same variable appearing later along one 
path. 
For example, both the definitions i : =m- I and j : =n in block 3 I in FigA 
LO. 19 reach the beginning of blmk B 
as does the definition j : = j - I ,  pro- 
vided there are no assignments to or reads of j in B4, B 5 ,  or the portion of 
B3 €ohwing thar definition. However, the assignment to j in B 3  kills the 
definition j; =n, so the Latter does not reach B 4 ,  B s ,  or Bb. 
By defining reaching definitions as we have, we sometimes allow inaccura- 
cies. However, they are all in the "safe," or "conservative" direction. For 
example, notice our assumption that all edges of a flow graph can be 
traversed. This may not be true in practice. For example, for no values of a 
and b can control actually reach the stssignment a : =4 in the folbwing pro- 
gram fragment: 

SEC. j0.5 
INTRODUCTION TO GLOBAL DATA-FLOW ANALYSIS 61 1 
if a = b then a := 2 
else if a = b then a := 4 
To dedde in general whether each path in a flow graph can be taken is an 
undecidable problem, and we shall not attempt to solve it. 
. 
A recurring theme in the design of code improving transformations is that 
we must make only conservative decisions in the face of any doubt, although 
mnxrvative strategies may cause us to miss some transformations that we 
actually wuld make safely. A decision is conswvarive if it never kads to a 
change in what the program computes. In applications of reaching definitions, 
it is normally conservative to assume that a definition can reach a p i n t  even 
if it might not. Thus, we allow paths that may never be traversed in any exe- 
cution of the program, and we allow definitions to pass through ambiguous 
definitions of the same variable. 
Flow graphs for mntral-flow mnstructs such as do-whik statements have a 
useful property; there is a single beginning pint at which control enters and a 
single end point that control leaves from when execution of the statement is 
over. We expbit this property when we talk of the definitions reaching the 
hginning and end of statements with the following syntax 
E~pressions in this language are similar to those in intermediate code, but the 
flow graphs for statements have restricted forms that are suggested by the 
diagrams in Fig. 10.20. A primary purpose of this section is to study the 
data-flow equations summarized in Fig. f 0.21. 
if E them S, e k  SI 
do S ,  while E 
Fi. 10.20. Somc structured control constructs, 
We define a portion of a flow graph called a region to Ix a set of n d e s  N 
that includes a hadm, which dominates all other nodes in the region. All 

612 CODE OPlrMlZATlON 
Fig. 1Vd21, ~ata-flow equations for reaching dcfinit ions. 
edges between nodes in N are in the region, except (psibly) for some that 
enter the header.4 The portion of a flow graph corresponding to a statement S 
is a region that obeys the further restriction that control can flow to just one 
outside block when it leaves the region. 
As a technical convenience we assume that there are dummy blwks with no 
statements (indicated by open circles in Fig. 10.20) through which control 
flows just before it enters and just before It leaves the region. We say that 
the kginning points of the dummy blocks at the entry and exit of a 
statement's region are the beginning and e ~ b  
points, respectively, of the state- 
ment. 
The equations in Fig, 10.21 are an inductive, or syntax-directed, definition 
of the sets In IS j, our [S J. gen IS 1, and kill IS 1 for all statements S. Sets Ken IS I 
and kil€[Sj are synchesized attributes; they are computed bttom-up, from the 
smallest statements to the largest. Our desire is that definition d is in p
a
 [S 
I 
' A limp is a special caw of a region that is sirongly conmectcd and includes all iis back c d p  mto 
the header. 

SEC. 10.5 
INTRODUCTlON TO GLOBAL DATA-FLOW ANALYSIS 61 3 
if d reaches the end of $, independently of whether it reaches the beginning of 
S. Put another way, d must appear in Sand reach the end of S via a path that 
does not go outside $. That is the justification for saying that Ken IS 1 is the 
set of definitions "generated by 5." 
Similarly, we intend that kkil!iS I be the set of definitions that never reach 
the end of S, even if they reach the beginning. Thus, it makes sense to view 
these definitions as "killed by S+" In wdcr for definition d to be in kill IS j, 
every path from the beginning to the end of S must have an, unambiguous 
definition of the same variable defined by d, and if d appears in S, then fol- 
lowing every occurrence of d along any path musi be another definition d the 
same variable." 
The rules for gcn and kill, king synthesized translations, are relatively easy 
to unders~and. To begin, observe the rules in Fig. 10.21(a) for a single 
assignment of variable a. Surely that assignment is a definition of a, say 
definition d. Then d i s  the only definition sure to reach the end of the state- 
ment regardless of whether it reaches the beginning, Thus 
#Prl IS 1 = {d} 
On the other hand, d "kills" all other definitions of a, so we write 
kii!iS 1 = D h  -{dl 
where D, is the set of all definitions in the program for variable a. 
The rule for a cascade of stalements, illustrated in Fig. 10.21(b). is a bit 
more subtle. 
Under what circumstances is definition d generated by 
S = S I  ; SZ'? First of all, if it i s  generated by S 2 ,  then it is surely generated 
by S. If d is generated by S i ,  it will reach the end of $ provided It i s  not 
killed by S2. Thus, we write 
gcn IS 1 = XCR 1S2j U (#en ISr I - killlSzj) 
Similar reasoning applies to the killing of a definition, so we have 
For the if-statement, illustrated in Fig. 10+21(c), we note that if either 
branch of the ++if" generates a definition, then that definition reaches the end 
of the statement S. Thus, 
However, in order to "kill" definition d, the variable defined by d must be 
killed along any path from the beginning to the end of S .  In particular, it 
must be killed along either branch, so 
kiil IS I = kiiIlS, I fl 
kil/lS,l 

Lastly, consider the rules for Imps, in Fig. 10.21(6). Simply stated, the 
Imp does not affect gen or kill. If definition d is generated within S1, then it 
reaches both the end of S., and the end of S. Conversely, if d is generated 
within S, it can only be generated within S1. Ef d is killed by $, , then going 
around the Imp will not help; the variable of d gets redefined within S each 
time around. Conversely, if d is killed by S, then it must surely be killed by 
S - We wndude that 
Conwvative Estimrtim of Data-FIm Information 
There is a subtle miscalculation in the rules for gsn and kili given in Fig. 
1021. We have made the assumption that the conditional expression E in the 
if and do statements are "uninterpreted;" that is, there exist inputs to the pro- 
gram that make their branches go either way. Put another way, we assume 
that any graph-theoretic path in the flow diagram is also an execation parh, 
Le., a path that is executed when the program is tun with,at least one possible 
input. 
That is not always the case, and in fact we cannot decide in general whether 
a branch can be taken. Suppose, fur example, that the expression E in an if- 
statement- were always true. Then the path through S2 in Fig. 10.22{c) could 
never be taken. This has two consequences. First, a definition generated by 
S2 is not really generated by S ,  because there is no way to get from the begin- 
ning of S into the statement S2. Second, no definition in MI[$ 
I ]  can reach 
the end of S. Therefore, each such definition should bgically be in kill [S 1, 
even if 
i t  is not in kill[SJ. 
When we compare the computed gen with the ''true'' gm we discover that 
the true g m  is always a subset of the computed gem. On the other hand. the 
true kill is always a superset of the cmputed kN. Thew containments hold 
even after we consider the other rules in Fig. 10.21. For example, if the 
expression E in a do$-while-E statement can never be false, then we can 
never get out of the loop. Thus, the true gm is @, and every definition is 
"killed" by the I*. 
The case of a cascade of statements, in Fig. 10.21(b), 
where the inability to get out of S I  or Sz because of an infinite Imp must be 
taken into account, is left as an exercise. 
It is natural to wonder whether these differences between the true and com- 
puted gen and Ail1 sets present a serious obstacle to data-flow analysis, The 
answer lies in the use intended for these data. In the particular case of reacb- 
Ing definitions, we normally use the information to infer that the value of a 
variable x at a point is limited to some small number of possibilities. For 
example, if we find that the only definitions of x reaching that point are of 
the form x := 1, we may infer that x has the value I at the point. Thus, we 
might decide to replace references to x by references to t . 
As a consequence, overestimating the set of definitions reaching a point 

SEC. 10.5 
INTRODUCTlON TO GLOBAL DATA-FLOW ANALYUS 6 15 
does not seem serious; it merely stops us from doing an optimization that we 
could legitimately do. On the other hand, underestimating the set of defini- 
tions is a fatal error; it could Itad us Into making a change in the program 
that changes what the program computes, For example, we may think all 
reaching definitions of x give x the value i ,  and so replace x by 1; but there 
is another undetected reaching definition that gives x the value 2. For the 
case of reaching definitions, then, we call a set of definitions safv or conservu- 
rive if the estimate is a supersel (not necessarily a proper superset) of the true 
set of reaching definitions. We call the estimate unsafe if it is not necessarily 
a superset of the truth. 
For each data-flow problem we must examine the effect of inamrate esti- 
mates on the kinds of program changes they may cause. We generally accept 
dtscrepancies that are sa/e in the sense that they may forbid optimizations that 
could legally be made, but not accept discrepancies that are unsafe in the sense 
that they may cause "*optimizations" that do not preserve the externally 
observed behavior of the program. In each data-flow problem, either a subset 
or a superset of the true answer (but not both) is usually safe. 
Returning now to the implications of safety on the estimation of gen and kill 
for reaching definitions, note that our discrepancies, supersets for pa 
and 
subsets for kill are both in rhe safe direction. Intuitively, increasing gen adds 
to the set of definitions that can reach a p i n t ,  and cannot prevent a definition 
from reaching a place that it truly reached. Likewise, decreasing kill can oniy 
increase the set of definitions reaching any given point. 
Many data-flow problems can be solved by synthesized translations similar to 
those uused to compute gen and kibl. For example, we may wish to determine, 
for each statement $, the set of variables that are defined within S. This 
information can be computed by equations analogous to those for gen, without 
even rquiring sets analogous to kill. It can be used, for example, to deter- 
mine Imp-invariant computations. 
However, there are other kinds of data-flow information, such as the 
reaching-definitions prohiern we have used as an example, where we also need 
to compute certain inherited attributes. i t  turns our that in is an inherited 
attribute, and out is a synthesized attribute depending on in. We intend that 
in15 J be the set of definitions reaching the beginning of S, taking into account 
the flow of control throughout the entire program, including statements out- 
side of S or within which S i s  nested. The set outlS] is defined similarly for 
the end of S. 
It is important to note the distinction between o ~ r [ S  
I and 
genISj. The latter is the set of definitions that reach the end of S without fol- 
lowing paths outside S. 
hs a simple example of the difference, consider the cascade of statements in 
Fig. 10.2 l(b), A statement d may be generated in S I and therefore reach the 
beginning of S2. If d is not in killlSlj+ d will reach the end of S2, and 

616 CODE OPTIM[ZATION 
therefore be in our [,TI 1. However, d is not in g m  [S2 1, 
After computing gen IS 1 and &dl[$] bottom-up, for all statements S, we may 
cornpule in and out starting at the statement repreknting the complete pro- 
gram, understanding that in [Sol = 0 if So is the cumpkte program. That is, 
no definitions reach the beginnihg of the program. For each of the four types 
of statements in Fig. 10.21, we may assume that in[$] i s  known. We must 
use it to compute h for each of the substatements of. S [which is trivial in 
cases (b)+d) and irrelevant in case (a)]. 
Then, we recursively (top-down) 
wmputc our for each of the substatements S1 or S2, and use these sets to 
compute ouf IS J . 
The simplest case is Fig. 10.21(a), where the statement is an assignment. 
Assuming we know inis], we mmprtte OM by equation (10.5), that is 
In words, a definition reaches the end of S if either it is generated by S (i.e., 
it is the definition d that is the statement), or it reaches the beginning af the 
statement and is not killed by the statement. 
Suppose we have computed inIS] and $ is the cascade of two statements 
5 , ;  S2, as in the semnd case of Fig* 10,21. 
We start by observing 
in IS, 1 = in IS 1. 
Then 
we recursively compute usst IS I 1, which gives us 
irn1S2], since a definition reaches the beginning of S2 if and only if it reaches 
the end of S , .  Now we can recursively compute orrrlSL), and this set is equal 
to out [S 1. 
Next, consider the if-statement of Fig. 10.21(c). As we have conservatively 
assumed that control can follow either branch, a definition reaches the begin- 
ning of S ,  or S2 exactly when it reaches the beginning of 5. That is, 
It also follows from the diagram in Fig. 10.21(c) that a definition reaches the 
end of S if and only if it reaches the end of one or both substsrtements; i.e.. 
We thus may use these equations to compute in[SI] and in[Sz] from inlS 1, 
recursiveIy compute out IS 1 I and out ISl], then use these to compute our[$ ]+ 
The last case, Pig+ 10.2 I(d), presents special problems, Let us again assume 
we are given gerr)Sl J and kill[$, J, having computed them bottom-up, and let 
us assume we are given inlS], as we are in the prmss of performing a 
depth-first traversal of ihe parse tree. Unlike cam (b) and (c), we cannot 
simply use in [S ] as in IS I j, b e c a w  definitions inside S ,  that reach the end of 
S I are able to f o l h  the arc back to the beginning of S , ,  and therefore these 
definitions also are in inlS I I. Rather, we have 

SEC. 10.5 
INTRODUCTION TO GLOBAL DATA-FLOW ANALYSIS 61 7 
W t also have the obvious equation for OM IS 1: 
which we can use once we have mmputed owiSI]. However, it seems we 
cannot compute in [$,I 
by (10.6) until we have comp~ted 
I, and our 
general plan has been to compute out for a statement by first computing in for 
that statement. 
Fortunately, lhere is a direct way to express out in terms of ifi; it is given by 
( 10.5), or in this particular case: 
It is important to understand what is going on here. We do not really know 
that (10.7) is true abut an arbitrary statement S , ;  we only suspect that it 
should be true k a u s e  it "makes sense" that a definition should reach the end 
of a statement if and only if it is either generated inside the statement or it 
reaches the beginning and is not killed. However, the only way we know to 
cmpute out for a statement is by the equations given in Fig. 10.2l(a)+c). 
We are going to assume (10,7) and derive the equations for in and out In Fig. 
[0.21(6). Then, we can use the equations of Fig, IO+2 l ( a ) 4 d )  to prove that 
( 10,7) holds for an arbitrary S 
We muld then put these proofs to get he^ to 
make a valid proof by induction on the size of a statement S that the equa- 
tions ( 10.7) and all the equations of Fig. 10.21 hold for S and all its substate- 
ments. We shall not do so; we leave the proofs as an exercise, but the reason- 
irtg we do fallow here should prove insqructive. 
Even assuming (10.6) and (10+7) we are still not out of the woods. These 
two equations define a recurrence for in IS I 1 and our IS, 1 simultaneously, Let 
us write the equations as 
where 1, 0, 
J ,  G, and K correspond to in 131 1, uuiISi I, i r t l S ] ,  genISl j, and 
killlSI 1, respectively. The first two are variables; the other three art con- 
stants. 
To solve 1 lO.8), let us assume that O = 0. We could then use the first 
equation in (10.8) to compute an estimate of I, that is, 
Next, we can use the second equation to get a better estimate of 0: 
Applying the first equation to this new estimate of 0 gives us: 
If we then reapply the second equation, the next estimate of 0 is: 

61 8 CODE OPTIMIZATION 
SEC. 10.5 
Note that 0' = 0'. 
Thus, if we compute the ne* estimate of I, it will be 
qua1 to I ' , which will give us another estimate of 0 equal to 0 ', and so on. 
Thus, the limiting values for I and 0 are those given for i '  and 0' above. 
We have thus derived the equations in Fig. 10.21(6), which are 
The first of these equations is from the calculation above; the m n d  follows 
from examination of the graph in Fig. 10.21Id). 
A detail that remains is wby we were entitled to start with the estimate 
0 = 0. Recall in our discussion of conservative estimates we suggested that 
sets like uuslS j, which 0 stands for, should be overestimated rather than 
underestimated. In fact, if we were to start with O = (4, where d is a defini- 
tion appearing neither in J, G, or K, then d would wind up in the limiting 
values of b t h  I and 0. 
Here, we must invoke the intended meanings of in and ow+ If such a d 
really belunged in in[$\ I, there would have to be a path from wherever d i s  
located to the beginning of S1 that showed how d reaches that point. If d is 
outside S, then d would have to k in in [S 1, while if d were insidt S (and 
therefore insjde S ) it would have to be in g m  [S 1. In the first case, d is in J 
and therefore placed in I by (10.8). in the second case, d is in G and again 
transmitkd to 1 via 0 in {LO,#). The conclusion i s  that starting with a tm 
small estimate and building upward by adjoining more definitions to 1 and O 
is a safe way to estimate inlS,I. 
Sets of definitions, such as $err IS 1 and kill IS 1, can be represented compactly 
using bit vectors. We assigo a number to each definition of interest in the 
flow graph. Then lhg bit vector representing a set of definitions will have I in 
position i if and only if the definition numbered i is in the set, 
The numkr of a definition statement can be taken as the index of the state- 
ment in an array holding pintcrs to statements. Howevkr, not all definitions 
may lx of interest during global data-flow analysis. For example, definitions 
of temporaries that are used only within a single block, as most temporaries 
generared for expression evaluation will be, need not be assigned numbers. 
Therefore. the numbers of definitions of interest wiD typically be recorded in 
a separate table. 
A bit-vector representation for sets also allows set operations to be imple- 
mented efficiently. The union and intersection of two sets can be imple- 
mented by logical or and logical a d ,  respectively, basic operations in most 
systerns-oriented programming languages. The difference A -8 of sets A and 
3 can be implemented by taking the complement of B and then using logical 
awl to compute A A - 8, 
Example 10,13. Figure 10.22 shows a program with seven definitions. 

INTRODUCTION TO GLOBAL DATA-FLOW ANALYSLS 619 
1 :- In-I; 
j := n; 
a := ul; 
do 
i := i+l; 
j := j - 4 ;  
if r4 then 
a := u2 
else 
i := u3 
while e2 
Fig. tO.22. 
Program for iilustrating reaching definitions. 
indicated by d l  through 6, in the comments to the left of the definitions. Btt 
vectors representing the gee and kilI sets for the statements in Fig. 10.22 are 
shown to the left of syntax-tree nodes in Fig. 10.23. The sets lhemselves were 
computed by applying the data-flow equations in Fig. 10+21 to the statements 
represented by the syntax-tree nodes. 
Fig. 10.23. The gm and kill sets at n&s 
in a synaax trtx 
Consider the node for d7 in the bottom-right corner of Fig. 10.23. The gen 
set {d7) is represented by OOOOOO 1 and the kit! set (dl, d4) by 400 1000. That 
is, d7 kills all the other definitions of i, its variable. 

The second and third children of the P node represent the then and else 
parts, respectively, of the eonditionai. Note that the gen set 000001 1 at the if 
nude is the union of the sets 0000010 and OOOOOOI at the second and third 
children. The kill set i s  empty because the definitions killed by the then and 
else parts are disjoint. 
Data-flow equations for a cascade of statements are applied at the parent of 
the if node. The kibj set at this node is obtained by 
In words, nothing Is killed by the conditionai, and d 7 ,  killed by the statement 
d 4 ,  is generated by the conditional, so only d l  and d2 are in the kill set of the 
parent of the IF node. 
Now we can compute in and 01.41, starting from the top of the parse tree. 
We assume the in set at the mot of the syntax tree is empty. Thus wi for the 
left child of the root is ~ e r t  of that node, or 11 10000. This is also the value 
of the in set at the do node, From the data-flow equations associated with the 
do production in Fig* L0.2 1, the in set for the statement inside the do bop is 
obtained by taking the union of the in set I I I 0000 at the do node and the gen 
set 000 11 11 at the statement. The union yields 1 1  I 1 1  1 I ,  so all definitions 
can reach the beginning of the bop W y ,  However, at the point just kfore 
definition d5, the in set is 011 1110, since definition d4 kills d ,  and d,. The 
balance of the in and out calculations arc left as an exercise. 
o 
Space for data-flow information can be traded for time, by saving information 
only at certain points and, as needed, recomputing Information at intervening 
points. Basic blocks are usually treated as a unit during global flow analysis, 
with attention restricted to only those pints that are the beginnings of blocks. 
Since there are usual1y many more points than blocks, restricting our effort to 
blocks is a significant savings, When needed, the reaching definitions for all 
points in a block can Ix calculated from the reaching definitions for the begin- 
ning of a blmk. 
In more detail, consider a sequenw of assignments S, ; S2; + 
+ 
+ ; 
S, in a 
basic block 8. We refer to the kginning of B as p i n t  p u ,  to the p i n t  
between statements Si and 
as pi, and to the end of the bbck as point p,,. 
The definitions reaching point pj can k obtained from in18 j by considering 
the statements SI ; 
S2; 
- ; S,, 
starting with $, and applying the data-flow 
equations In Fig. 10.21 for cascades of statements. Initially, let D = in[S 1. 
When Si is considered, we delete from D the definitions killed by S, and add 
the definitions generated by S,. At rhe end, D consists of the definitions 
reaching fi. 

INTRODUCTION TO GLOBAL DATA-FLOW ANALYaS 621 
Use-Definition Chains 
I t  is often convenient to stare the reaching definition informath as "use- 
definition chains" or "ud-chains," which are lists, for each use of a variable, 
of all the definitions that reach that use. If a use of variable a in block 3 is 
preceded by no unambiguous definition of a, t k n  the ud-chain for that use of 
a is the set of definitions in ifi IB 1 that are definitions of a. [f there are 
unambiguous definitions of a within 8 preceding this use of a, then only the 
iast such definition of a will be on the ud-chain, and in18 ) is not placed on 
the ud-chain. In addition, if there are ambiguous definitions of a, then all of 
these for which no unambiguous definition of a lies between it and the use of 
a are on the ud-chain for this use of a. 
Evaluation Order 
The techniques for cunxrving space during attribute evaluation, discussed in 
Chapter 5, also apply to the computation of data-flow information using 
specifications like the one in Fig. 10+21. Specifically, the only constraint on 
the evaluation order for the gen, kill, in, and out sets for statements is that 
imposed by dependencies &ween 
these sets. Having chosen an evaluation 
order, we are free to release the space for a set after all uses of i t  have 
occurred. 
The data-flow equations of this sectim differ in one respect from semantic 
rules for altributes in Chapter 5: circular dependencies between attributes were 
not allowed in Chapter 5 ,  but we have seen that data-flow equations may have 
circular dependencies, e-g., in IS, j and out [S , I  depend on each other in 10.8. 
For the reachingdefinitions problem, the data-flow equal ions can be rewritten 
to eliminate the circularity - 
compare the nondrcular equations in Fig. 10.21 
with 10.8. Once a noncircular specificatim is obtained, the techniques of 
Chapter 5 can be applied to obtain efficient solutions of data-flow equations. 
General Control Flow 
Data-flow analysis must take all control paths into amunt. If the control 
paths are evident from the syntax, then data-flow equations can be set up and 
solved in a syntaxdirected manner, as in this section. When programs can 
contain goto statements, or even the more disciplined break and continue 
statements, the approach we have taken must be modified to take the actual 
control paths into amunt. 
Several approaches may be taken. The iterative method in the next section 
works for arbitrary flow graphs. Since the flow graphs obtained in the pres- 
ence of break and continue statements are reducible, such constructs can be 
handled systematically using the interval-ba.sed met hods to be discussed in 
Section 10.10. 

However, the syntax-directed approach need not be abandoned when beak 
and continue statements are allowed, Before leaving this section we consider 
an example suggesting Low break-statements can be accommodated, kaving 
the development of the ideas to Sectim 10.10. 
Example N.14. The break-statement within the dwwMe Imp in Fig. 10.24 is 
equivalent to a jump to the end of the loop. How then are we to define the 
get set for the following statement? 
if e3 then a := u2 
else begin i := u3; break end 
We define the gen set to be {d6}. where b, is the definition a: =u2, bemuse 
d6 is the only definition generated along the m t r d  paths from the beginning 
to the end points of ihe statement. Definition d7, ix., i : =u3, will get taken 
into account when the entire &-while Imp is t r e a d ,  
i := m-1; 
j := n; 
a := u3; 
do 
i := i+?; 
j := j-1; 
if +3 thtn 
a := u2 
else begin 
i := u3; 
break 
end 
while e2 
Fig, 10+24. Program containing a break-statement , 
There is a programming trick that allows us to ignore the jump caused by 
the break-statement whik we are processing thf statements within the body d 
the bop: we take the gefi and kill sets for a break-statement to be the empty 
set and U, the universal set of all definitions, respectively, as shown in Fig. 
10.25. The remaining gett and kill sets shown in Fig, 1.0.25 are determind 
using the data-flow equations in Fig. 10.21, with the g m  set shown above the 
kill set. Statements S, and S2 represent sequences of assignments. The gen 
and kill sets at the denode remain to k 
determined. 
The end point of any sequence of statements ending in a break cannot be 
reached, so there is no harm in taking the gen set for the sequence to be 0 
and the kill set to be U; the result will still be a conservative estimate of in 
and our. SimiIarly, the end point of the if-statement can only be reached 
through the thtn part, and the gun and kill sets at the if node in Pig. 10.25 are 
indeed the same as those at its s m n d  child. 

INTRODUCTION TO GLOBAL DATA-FLOW ANALY SlS 623 
Fig. 10.25. Effcct of a brcak-artcmcnr on Ken and kill scls. 
The $en and kiii sets at the do-node must take into account all paths from 
the beginning to the end of the do-statement, so they are affected by the 
break-statement. Let us now compute two sets G and K, initially empty, as 
we traverse the dashed path from the do-node to the break-node. The intui- 
tion is that C and K represent the defin~tions generated and killed as control 
flows to the beak-statement from the beginning of the Imp body. The gcr~ 
set for the do-white sraIement may then be determined by taking the union of 
G and the ,yen set for the Imp body, because control can reach the end of the 
dm either from the break-statement or by falling through the Imp body. For 
the same reason, the kid/ set for the do is derermined by taking the intersection 
of K and the kill set of the loop M y .  
Just before we reach the if n d e  we have G = #en ISa] = '(J,,Js) and K - 
kid1 ISz I = {d ,b2,d7). At the if n d e  we are interested in the case when con- 
trol flows to the break-statement, so the then-part of the conditional has no 
effect m G and K. The next node along the dotted path i s  for a sequence of 
statements, so we compute new values of G and K. Writing S )  for the state- 
ment represented by the left child of the sequence node (the one labeled d 7 ) .  
we use 
The values of G and K on reaching the break-state'ment are therefore (ds. d7j 
and {d 
dl d4), respectively. 
o 

624 
CODE OPTIMIZATION 
lO+6 ITERATIVE SOLUTION OF DATA-FLOW EQUATIONS 
The method of the last section is simple and efficient when it applies, but for 
languages like Fortran or Pascal that permit arbitrary flow graphs, it is not 
sufficiently general. Se~~ion 
LO. I0 diwusm "interval analysis," a way to 
obtain the advantages of the syntax-directed approach to data-flow analysis for 
general flow graphs, at the expense of considerably more conceptual complex- 
ity. 
Here, we shall discuss another important approach t ~ ,  solving data-flow 
problems. Instead of trying to use the parse tree ta drive the calculation of in 
and our sets, we first build the flow graph. and then compute in and out for 
each node simultaneously. While discussing this new methd, we shall also 
take the opportunity to introduce the reader to many different data-flow 
analysis problems, show some of their applications, and pint out the differ- 
ences among the problems. 
The equations for many data-flow problems are similar in form in thal 
information i s  being "generated" and "kilkd." However. there arc two prin- 
cipal ways in which equations differ in ddail. 
I. The equations in the last section for reaching definitions are forward 
equations in the sense that the our sets are computed in terms of the in 
sets. We shall also see data-flow problems that alee hckwwd in that the 
in sets art computed from the out sets. 
2, 
When there is more than one edge entering Mock B, the definitions 
reaching the kginning of B are the union of the definitions arriving along 
each of the edges. We therefore say that union is the cunflu~nce opemlw. 
In contrast, we shall considcr problems like global available expressions, 
where intersection is the confluence operator, because an expression is 
only available at the beginning of B if it is available at the end of every 
prdeasso~ of B+ in Section 10,11 we shal see other examples of conflu- 
ence aperatms. 
In this ~ e ~ q i o n  
we shall see examples of bath Forward and backward equations 
with union and intersection taking turns as the cmfl uence operator. 
For each basic Mock 3, we can define our [B 1, gerr If3 1, kill [S 1, and in [B 1, as 
. 
in the last section. by noting tbat cach block B may be viewed as a statement 
that is lhe cascade of one w more assignment statements. Assuming that gen 
and kiil have been computed for each block, we can create two grDUpS of 
equations, shown in (10.9) below, that relate in and out. The first group of 
equations follows from the observation thal in[B] is the union of the defini- 
tions arriving from all predecessors of B. The sewnd group are special cases 
of the general law (10.5) that we dairn hdds for all statements. These two 
groups are: 

SEC. 10.6 
ITERATIVE SOLUTION OF DATA-FLOW EQUATlIiNS 625 
If a k w  graph has n basic biocks, we get 2n equations from [ 10.9). Thc 2n 
equations can be solved by treating them as recurrences for computing the in 
and out sets, just as the data-flaw equations (10.6) and (10.5) FOT b w h k  
statements were solved in the k
t
 section. In the Last section we began with 
the empty set of definitions to be the starting estimate For all the orrr sets. 
Here we shall start with empty in sets, since we note from (10.9) 
that the in 
sets, being the union of our sets, will be empty if the our sets are. White we 
were able to agree that the equations ( 10.6) and (10.7) needed only one itera- 
tion, in the tax of these more complex equations, we cannot u priuri bound 
the number of iterations. 
Gljprithrn 10-2. Reaching definitions. 
lnptri. A flow graph for which ki//[l? ] and #en IB j have been computed for 
each blwk B. 
M~rhod. We use an iterative approach, starting with the "estimate" 
irl[B] = 0 for all S and converging to the deskred values of ifi and our. As 
we must iterate until the in's (and hence the our's) wnverge, we use a boolean 
variable change to record on each pass through the blocks whether any in has 
changed. The algorithm is sketched in Fig. 10.26. 
0 
(7) 
(81 
19) 
end 
end 
Fi. 10.26. Algorithm lo compute in and nui. 
Intuitively, Algorithm 10,2 propagates definitions as Ear as they will go 
withwt being killed, in a sense simulating all possible executions of the 

626 
CODE OPTIMIZATION 
SEC. 10.6 
program. The bibliographic notes contain references where formal prmfs of 
the correctness of this and other data-flow analysis problems can be found. 
We can see that the algorithm will eventually halt because our[B] never 
decreases in size for any B; once a definition is added, it stays there forever. 
(The proof of this fact is Iefr as an inductive exercise,) Sin= the set OF all 
definitions is finite, there must eventually be a pass of the while-loup in which 
oldout = our [B for each B at line 19). Then chrrge will remain fahe and the 
algorithm terminates. We are safe terminating then because if the ow's have 
not changed, the in's will not change on the next pass. And, if the i d s  do not 
change, the our's cannot, so on all subsequent passes there can be no changes. 
It may be shown that an upper bound on the number of times around the 
while-loup is the number of nodes in the flow graph. Intuitively, the reason is 
that if a definition reaches a point, it can do so along a cycle-free path, and 
the number of nodes in a flow graph is an upper bound on the number of 
nodes in a cycle-free path. Each time around the while-Imp, the definition 
progresses by at least one node along the path in question. 
In fact, if we properly order the Mocks in the for-hop of line {5), there is 
empirical evidence that the average number of iterations on real programs is 
under 5 (set Section 10.10). Since the sets can be represented By bit vectors, 
and the operations on these sets can be implemented by logical operations on 
the bit vectors, Algorithm 10.2 is surprisingly efficient in practice. 
Exarnpde 10.15. The flow graph of Fig. 10.27 has k e n  derived from the pro- 
gram in Fig. 10.22 of the last section. We shall apply Algorithm 10.2 to this 
flow graph so the approaches of the two sections can be compared. 
grntBd = {41 
kiII1l3,j 
= ( d , ,  b,} 
Fig. 10,27. Flow graph for illustrating reaching definitions, 
Only the definitions d l ,  dl, + . . . d7 defining i, j, and a in Fig. 10.27 are 
of interest. As in the last section, we shall represent sets of definitions by bit 
vectors, where bit i from the left represents definition d,. 
The Imp of line ( I )  in Fig. 10.26 Initialixs out [R ] = gen[B] for each B, 

SEE. 10.6 
ITER ATIY E SOLUTLON OF DATA-FLOW EQU ATlONS 627 
and these ifiitial values of our [B 1 are shown in the table in Fig. 10.28. The 
initial values (0) of each in [ B ]  are not computed or used, but are shown for 
completeness. 
Suppose the 
for-loop of 
line 
( 5 )  is 
executed 
with 
B = B I ,  B L ,  B3, B 4 ,  in that order. With B = B , ,  there are no predecessors 
for the initial node so h [ B , ]  remains the empty set, represented by 000 0000; 
as a result, out[Bl] remains equal to gen[Bl]. This value docs not differ 
from oldout computed at h e  (71, so we do not yet set ckange to true. 
Then we consider B = B and compute 
This computation is summarized in Fig. 10.28. At the end of the first pass, 
m i  [B4] = 001 01 1 1, reflecting the fact that b7 i s  generated and that d l ,  d 5 ,  
and d6 reach 8, and are not killed in B4. From the second pass on there are 
no changes in any of the out sets, so the algorithm terminates. 
Fig* 10.28. Computation of in and our. 
Available Expressions 
An expression x+y is awihbie at a p i n t  p if every path (ria necessarily 
cycle-free) from the initial n d e  to p evaluates x+y, and after Ihe last such 
evaluation prior to reaching p, there are no subsequent assignments to x or y. 
For available expressions we say that a block kills expression x+y if it assigns 
(or may assign) x or y and does not subsequently recompute x+y. A block 
gmerutes expression x+y if it definitely evaluates x+y and dms not subse- 
quently redefine x or y. 
Note that the notion of "killing" or "generating" an available expression is 
not exactly the same as that for reaching definitions. Nevertheless, these 
notions of "kill" and "generate" obey the same laws as they do for reaching 
definitions. We could compute them exactly as we did in Section 10.5, pro- 
vided we modified the rules in 10.2 1 (a) for n simple assignment slatement. 

628 
CODE OPTIMIZATION 
SEC. 10.6 
The primary u x  of available expressions information is for detecting com- 
mon subexpressions. For example, in Fig. 10.29, the expression 4*i in blwk 
B7 will k a common subexpression if 4*i is available at the entry point of 
blkk 3 3 .  Ir will be available if i is not assigned r new value in bbck B 
or 
if, as in Fig. 10.29(b), 4 * i  is recomputed after i isassigned in Bz. 
Fig. 10,29. Pntcnt id common subcxprcssiom across blocks. 
We can easily compute [he set of generated expressions {or each point in a 
block, working from beginning to end of the block. At the p i n t  prior to the 
b k k .  assume n o  expressions arc avaiiabk. If at pin1 p set A of expressions 
is available, and q is the point after p, with statement x: =y+z between them, 
then we form the set of expressions avaiiable at y by the following two steps, 
I. Add to A the expression y+z+ 
2. 
Delete from A any expression involving x. 
Note the steps must be done in the correct order, as x could be [he same as y 
or z+ After we reach the end of the block, A is the giet of generated txpres- 
sions for the block. The set of killed expressions is all expressions, say, y+z 
such that eirher y or z is defined in the block, and y+z is not generated by 
the block. 
Example 10.16. Consider the four statements of Fig. 10.30, After the first, 
b+c is available. After the second, a-d becomes available, but b+e is no 
longer available, because b has been redefined. The third does not make b+c 
available again, because the value of c is immediately changed. After the last 
statement, a-d is no longer availabk, because d has changed. Thus no state- 
ments are generated, and all statements involving a, b, e, or d are killed. 
0 
We can find available expressions in a manner reminiscent of the way 
reaching definitions are computed. Suppose U is the "universal" S ~ I  
of all 
expressions appearing on the right of one or more statements of the program. 
For each block 3, let inlB 1 be h e  set of expressions in U that art availabk at 
the point just before the kginning of 19. Let out[B] be the same for the p i n t  
following the end of B. Define cgertll3 1 to be the expressions generated by B 

SEC. 10,6 
ITERATIVE SOLUTlON OF DATA-FLOW EQUATIONS 629 
.......... nobe 
a := b+c 
.......... only b+c 
b : = a-d 
.......... only a-d 
c : = b+c 
.......... only a-d 
d : = a-d 
.......... none 
Flg. 10.30. Computation of avai\aMe expressio~s. 
and e k i € ! [ B ]  to be the set of expressions in U killed in B. Note that in, out, 
e-gen, 
and e-kill can at1 be represented by bit vectors. The following equa- 
tions relate the unknowns in and out to each other and the .known quantities 
sjert and d i l l .  
in[B I ]  = 0 where B I is the initial block. 
Equations (10.10) look almost identical to equations (10.9) for reaching 
definitions. The first difference is that in for the initial node is handled as a 
special case. This is justified on the grounds that nothing is available If the 
program has just begun at the initial node. even though some expression might 
be available along all paths to the initial node from elsewhere in the program. 
If we d'id not force in(B I ]  to be empty, we might erroneously deduce that cer- 
tain expressions were available before the program started. 
The second, and more important, difference is that the confluence operator 
i s  intersection rather than union. This operator is the proper one because an 
expression is available at the kginning of a block only if it is available at the 
end of all its predecessors. In contrast, a definition reaches the beginning of a 
block whenever it reaches the end of one or more of its predecessors, 
The use of fl rather than U makes equations (10.10) behave differently 
from (10.9). While neither set has a unique solution, for (10.9) it is the smaL 
lest solution that corresponds to the definition of "reaching," and we obtained 
that solution by starting with the assumpiion that nothing reached anywhere, 
and building up to the solution, In that way, we never assumed that a defini- 
tion d could reach a potnt p unless an actual path propagating d to p could be 
found. In contrast, for equations (10.10) we want the largest possible soh- 
tion, so Ne start with an appr~xirnati~n 
that is too large and work down, 

630 CODE OPTIMIZATION 
SEC. 10.6 
11 may not be obvious that by starting with the assumption "'everything, ix., 
the set U, Is available everywhere" and eliminating only those expressions for 
which we can discover a path along which it is not available, we do reach a set 
of truly available expressions. In the case of available expressions, it is con- 
servative to produce a subset of the exact set of available expressions, and this 
is what we do. The argument for subsets being conservative is that our 
intended use of the information is to replace the computation of an available 
expression by a previously computed value (see Algorithm 10S in the next 
section), and not knowing an expression is availabk only inhibits us from 
changing the code. 
Example 10J7, We shall concentrate on a single block, B2 in Fig. 10.3 1, to 
illustrate the effect of the initial approximation of in [B1l on out IB21. Let G 
and K abbreviate g m  [B j and kill [B2 1, respectively. The data-flow equations 
for blwk Bl are: 
These equations have been rewritten as rccurrcnccs in Fig. 10.31, with Ij and 
0 j  being the jth approximations of irr IS I and m r  [B2j, respectively. The fig- 
ure also shows that starting with lo=@ 
we get 0' = o2 = C, while starting 
with I'=U we get a larger s t  lor 0'. 11 sci happens that U U ~ ~ B ~ J  
equals o2 
in each case, because the iterations each converge at the points shown. 
Ag. 10.31. 
Initializing the in sets to 0 is too rcstrictiw. 
Intuitively, the sdution obtained starting with I' = U using 
outlSzl = G U IoutIBI] - K) 
is mwe desirable, because it correctly reflects the fact that expressions in 
uwt 1% ,] that are not killed by B2 are available at the end of B2, just as the 
expressions generated by Bl are. 
17 

SEC. 10.6 
ITERATIVE WLUTIIIN OF DATA-FLOW EQU ATKINS 63 ! 
Algorithm 10,3. Available expressions. 
Input. A flow graph G with eJriIlIS I and r-genIB j computed Cclr each block 
8, The initial blwk is B , .  
Outpur. The set h [ B  I for cach block 8. 
Meihud. Execute the algorithm of Fig. 10.32, The explanation of thc steps is 
similar to that fm Fig, 10.26, 
o 
oui[S, ) := t--pnlB I 1: 
1 4  in and out n e w  ch~ngc for the initial notlc, 8 ,  +/ 
hr B # B ,  do nrrflB I := U - p l i M [ B  1; 
initial cstirnate is too large 4,' 
end 
Fig. 1032. Avai lablc c~prcssionx romputatirm+ 
Live-Variable Analysis 
A number of code improving transiormaticsns dcpend on information cum- 
putcd in the direction opposite to the flow aE control in a program: we shall 
confiider some of these now. In liw-vuriubk analysis wc wish to know for 
variable x and p i n t  p whether the valuc of x at p could be uscd a h g  somc 
path in the flow graph starting at p. If MI+ we say x is I i v ~  
at p; otherwise x is 
d~ud 
at p. 
As wd saw in Section 9.7, an important ulic for livc-variablc information 
comes when we generate object d e .  After a value is cnmputcd in a rcglstcr, 
and presumably used within a bbck, i t  is not necessary to storc that vduc if i t  
is dead at the cnd or the block, AI.w, if all rcgistcrs arc Cull and we nccd 
another register, we should favor using a register with n dcad valuc. since that 
valuc docs not haw to k stored, 
Let us define in(B I to bc the sct of variablcs livc at thc prbint immediately 
More blwk 3 and define mr[S 1 to be thc same at the p i n r  immcdiatcly after 
the block. Let deal3 I bc the set of variablcs definitely ussigncd valucs in B 
prior to any use of that variable in B, and let arsvlS 1 be thc sct or variablcs 
whosc values may bc used in B prior to any dcfinitim of thc variahk. Thcn 

632 
CODE OPTlMlZATION 
Ihe equations relating &!and 
w e  to the unknowns in and OM are: 
The first group of equations says that a variable is live coming into a block 
if either it is used before redefinition in the block w it i s  live coming out of 
the block and is not redefined in the block. The second group of equations 
says that a variable is live coming out of a block if and only if it is live corn- 
ing into one of its successor$. 
The relation between ( 10.1 1) and the reachingdefinitions equations ( 10.9) 
should k noticed, Here, in and OW have their roles interchanged, and we 
and def substitute for $en and kiu, respectively. As for (10,9), the solution to 
( 10.1 1) is not necessarily unique, and we want the smalksi solution. The 
algorithm 'used for the minimum solution is essentially a backwards version of 
Algorithm 10.2. Since the mechanism for detecting changes to any of the in's 
is so similar to the way we detected changes to out's in Algorithms 10.2 and 
10.3, we elide the details of checking for termination. 
Algorithm 10.4. Live variable analysis. 
h p r .  A flow graph with def and use computed for each block. 
Ourprrr. our IS 1, the set of variables live on Wit from each block B of the flow 
graph. 
Method. Execute the program in Fig. 10.33. 
u 
fbr crrch block 3 do injB j : = 0: 
whik changes to any of the in's occur do 
for cach block B do begin 
nvrlBJ= 
U h I S j  
$ I sue- 
iemr d B  
in[B j := usel8j U (oul[B I - &fill]) 
end 
Fig. 1 0 3 .  Live variable cahlation. 
Definition-Use Chains 
A calculation done in virtually the same manner as live-variable analysis is 
definition-use chaining (du-chaining). We say a variable is used at statement s 
if its r-value may be required. Fur example, b and c (but not a) are used in 
each of the statements a: =b+e and a[ b] : 
=e. The du-chaining problem is to 

SEC. 10.7 
CODE-IMPROVING TRANSFORMATIONS 633 
compute for a p i n t  p Ihc set of uses s of a variable, say x, such that there is 
a path from p to s that does not redefine x. 
As with live variables, if we can compute ouclB 1, the set of uses reachable 
from the end of block B, then we can compute the definitions reached from 
any point p within block 3 by scanning the portion of Mock 8 that fdlows p. 
In particular, if there is a definition of variable x in the block, we can deter- 
mine the du-chin for that definition, the list of all possible uses of that defini- 
tion. The methd is analogous to that discussed in Section 10.5 for computing 
ud-chains, and we leave it to the reader. 
The equations for computing du-chaining information look exactly like 
( Kh11) with substitution for bef and USLC. [II piace of use lB 1, take the set of 
upwards uxpusd uses in B ,  that is, the set of pairs (s, x) such that s is a state- 
ment in B which uses variable x and such that no prior definition of x occurs 
in 3. Instead of 
j take the sa of pairs Is,x) such that s is a statement 
whkh uses x, s is not in 8, and 8 has a definition of x. Theve equations are 
solved by the obvious analog of Algorithm 10+4, and we shall nor discuss the 
matter further. 
10.7 CODE-IMPROVING TRANSFORMATIONS 
Algorithms for performing the code-improving transformalions introduced in 
Section 10.2 rely on data-flow information. In the last two sections, we have 
seen how this informat ion can be c o k t e d  , Here we consider common subex- 
pression dim ination, copy propagat ion, and transformations for moving loop 
invariant computations out of loops and for eliminating induction variables. 
For many la&ages. 
significant improvements in running time can be 
achieved by improving code in Iwps. When such transformations are imple- 
mented in a compiler, it is possible to perform some of the transformations 
together, However, we shall presenl the ideas underlying the transformations 
individually. 
The emphasis in this section Is on global transformations that uw informa- 
tion about the program as a whole. As we saw in the last two sections, global 
data-flow analysis docs not usually look at points within basic blocks. Global 
transfor mations are therefore n d  a substitute for local transformat ions; both 
must be vrformed. For example. when we perform global common subex- 
pression elimination we shall only be concerned with whether an expression is 
generakd by a block and not with whether it is recomputed several times 
withh a block. 
Elimination of G l d ~ l  
C~mman Subexpressions 
The available expressions data-flow problem discussed in the last section 
allows us to determine if an expression at pin[ p in a flow graph i s  a common 
subexpression. 
The following algorithm formalizes the intuitive ideas 
presented in Section 10.2 for eliminating common subexpressions. 

634 
CODE OPTIMIZATION 
Algorithm 10S, GIoba l common subexpression elimination. 
hpur. A flow graph with available expression information. 
Output. A revised flow graph. 
Methud. For every statement s of the form x : =y+z6 such that y+z is avail- 
able at the beginning of s's block, and neither y nor z is defined prior to 
statement s in that blwk, do the following. 
I 
A 
To discover the evaluations of y+z that reach s's block, we fdlow flow 
graph edges, searching backward from s's block. However, we do not go 
through any block that evaluates y+z, The last evaluation of y+z in each 
block encountered is an evaluation of y+z that reaches s. 
2. 
Create a new variable u. 
3. 
Replace each htatemcnt w: =y+z found in ( 1) by 
4. Replace statement s by x: =u. 
o 
Some remarks abut this algorithm are in order. 
I .  The search in step ( 1 )  of the algorithm for the evaluations of y+z that 
reach slatement s can also be formulated as a data-flow analysis problem. 
However, it does not make sense to solve it for all expressions y+z and 
all statements or blocks, because tm much irrelevant information is gath- 
ered. Rather we should perform a graph search on the flow graph for 
each relevant statement and expression + 
2. 
Not all changes made by Algorithm 10.5 arc improvements. We might 
wish to limit the number of different evaluations ~eaching s found in step 
I I ), probably to one. However, copy propagation, to be discussed next. 
often allows benefit to be obtain4 even when several evaluations of y+z 
reach s. 
3. 
Algorithm 10.5 will miss the fact that a*z and crz must have the same 
value in 
a := x+y 
c := xty 
vs . 
b := a+z 
d := c*z 
because this simple approach to common subexpressions considers only 
the literal expressions themselves, rat her than the values wmputed by 
expressions, 
Kildall 119731 presents a method f 
catching such 
equivalences on one pass; we shall discuss the ideas in Section 10.11. 
However, they can be caught with multiple passes of Algorithm 10.5, and 
* RccaJl wc cumtimuc to usc + as a gcncric ttpctator, 

SEC. 10.7 
CODE-IMPROVING TRANSFORMATIONS 
635 
one might consider repeating it until no further changes occur. I f  a and 
c arc temporary variables that are no1 used outside the block in which 
they appear, then the common subexpression {x+y 1 *z can be caught by 
treating the temporary variables specially, as in the next example. 
Example 10.18, Suppose that there are no assignments to the array a in the 
flow graph of Fig, 10+34(a), so we can safely say that aItz J and aCtbl are 
common subexpressions. The problem is to eliminate this common subexpres- 
sion. 
Fig, 10.34. Eliminating the common suhxprtssion 4+i. 
The common sukxprcssion 4 * i  in Fig. 10.34(a) has been eliminated in 
Fig. 10.34(b). One way to de~errnine that a[ t2 ] and a[t6 3 are also common 
subexpressions is to replace tz and t6 by u using copy propagation (to be 
discussed next); both expressions then become a [ u] , which can be eliminated 
by reapplying Algorithm 10.5. Note that the same new variable u is inserted 
in both blocks in Fig. 1 O.M(b), so local copy propagation is enough to convert 
bothattz] anda[t6: t o a [ u ] .  
There is another way, which takes into account the fact that temporary vari- 
ables are inserted by the compiler and are used only within blocks they appear 
in. We shall look more closely at the way expressions arc represented during 
the computation of available expressions to get around the fact that different 
temporary variables may represent the same expression. The recommended 
te'chnique for representing sets of eapressions is to assign a number to each 
expression and use bit vectors with bit i representing the expression numbered 
i .  The, value-numbering techniques of Section 5.2 can be applied during the 
numbering of expressions to treat temporary variables in a special way. 
[n more detail, suppose 4 + i  has value number 15, The expressions a[tz] 
and a f t n ]  will get the same value numkr if we use the value number 15 
rather than the names of the temporary variables t2 and t6. Suppose the 
resulting value number is Is. Then bit 18 will represent both a[ t; ] and 
a[tb] during data-flow analysis, and we can determine that a[ t6 1 is avail- 
able and can be eliminated. The resulting code is indicated in Fig. 1U434(c). 
We use (15) and ( 13) to represent temporaries correspondtrig to expressions 

with those value numbers. Actually, tb is useless and would be eliminated 
during local live variable analysis. Also t7, being a temporary, would nor be 
computed itself; rather, uses of t7 world be replaced by uses of ( 181. 
G 
Copy Propagation 
Algorithm 10.5 just presented, and various other algorithms such as 
induction-variable elimination discussed later in this section, introduce copy 
statements of the form x: =y+ Copies may also be generated directly by the 
intermediate d
e
 
generator, although most of these inwive tempmar ies local 
to one block and can !x removed by the dag musrruction discussed in Section 
9.B. I t  is sometimes possible to eliminate copy statement s : x : =y if we deter- 
mine all places where this definition of x is used. We may then substitute y 
for x in all these places, provided the following conditions are met by every 
such use u of x. 
1 .  
Statement s must be the only definition of x reaching u (that is, the ud- 
. 
chain for use rr consists only of s). 
2. On every path from s to u, including paths that go through u several 
times (but do not go through s a second time), there are no assignments 
to y. 
Cmdttim 
I )  can be checked using ud-chaining information, but what of 
condition (2)? We shall set up a new data-flpw analysis problem ln which 
in[B I is the set of copies s: K: =y such that every path from the initial node to 
the beginning of B contains the statement s, and subsequent to the last 
occurrence of s there are no assignments to y. The set usrilS J can be defined 
correspondingly, but with respect to the end of B. We say copy statement 
s: x:=y i s  generated in block B if s occurs in B and there is no subsequent 
assignment to y within B. We say s: x: =y is k i k d  in B if x or y is assigned 
there and s is na in 3, The notion that assignments to x "kill" x : =y is 
familiar from reaching definitions, but the idea that assignments to y do so is 
special to this problem. Note the important consequence of the fact that dif- 
ferent assignments x :  =y kill each other; in lB I can contain only one copy 
statement with x on the Left. 
Let U be the "universal" set of ail cppy statements in the program. It i s  
important to note that different statements x : =y are different in U .  Define 
c.qen[Bj to be the set of all copies generated in block B and cAiULB I to k 
the set of copies in U that are killed in B .  Then the following equations relate 
the quantities defined: 

SEC. 10.7 
CODE-IMPROVING TRANSFORMATIONS 637 
Equations 10+!2 are identical to Equations 10.10, if c-kili is replaced by 
ekiId and cden by ejen. Thus, 10.12 can be solved by Algorithm 10.3, and 
we shall not discuss the matter further. We shall, however, give an example 
that exposes some of the nuances of copy optimization. 
Exampk 10.19. Consider the flow graph of Fig. 10.35, Here, cderr[B , ]  = 
{x:=y) and c.-gen[B3) = {x:=z)+ Also, c_kiIl[B,] =. (x:=y}, since y is 
assigned in B , .  Finally, c--kill[B 1 1  = {x: = z )  since x is assigned in B , ,  and 
c*-kill[B3] = ( x :  5y) for the same reason. 
Fig, 103% Example flow graph 
The other c-gen's and c-kill's are 0. Also, inIB 1 1  - 0 by Equations 
10.12. Algorithm 10.3 in one pass determines that 
Likewise. mr[B, j = 0 and 
Finally, rir[B53 = osrt[B2j 1'3 0ur[B4] = 0. 
We observe that neither copy x: =y nor x: =z "reaches" the use of x in B5, 
in the sense of Algorithm 10.5. It is true but irrekwnt that h t h  these defini- 
tions of x "reach" B s  in the sense of reaching definitions, Thus, neither copy 
may be propagated, as it is not possible to substitute y (respectively z) for x 
in all uses of x that definition x: =y (respectively x: =z) reaches. We could 
substitute z for x in B4, but that would not improve the code. 
o 
We now specify  he details of the algorilhm to remove copy statements. 
Algorithm 10.6+ Copy propagation. 
Input, A flow graph C, with ud-chains giving the definitions reaching block 
8, 
and with c-in[S] representing the wlutjon to Equations 10.12. that is, the 
set of copies x: *y that reach block B along every path, with no assignment to 
x or y folkowing the last occurrence of x: =y on the path. We also need du- 

638 
CODE OPTIMlZATlON 
chains giving the uses of each definition. 
Output. A revised flow graph. 
Methid. For each copy s : x: =y do the following. 
1. 
Determine those uses of x that are reached by this definition of x, 
namely, s: x:=y. 
2. 
Defermine whether for every use of x found in ( I ) ,  s is in c-inlB I, where 
B is the bkck of this particular use, and moreover, no definitions of x or 
y occur prior to this use of x within 8. Reail that if s i s  in c-in [ B ] ,  
then s is the only definition of'x that reaches B, 
3, 
If s meets the conditions of (21, then remove s and replace all uses of x 
found in (1) by y, 
0 
We shall make use of ud-chains to detect those computations in a I m p  that are 
Iuupinvariurt:, that is, whose value dues not change as long as control stays 
within the loop. As discussed in Section 10.4, a loop is a region consisting of 
a set of blocks with a header that dominates all thelother blocks, so the only 
way to enter the Imp is through the header. We also require that a h
p
 have 
at least one way to get back to the header from any block in the Imp. 
1T an assignment x: =y+z is at a position in the loop where all possible 
definitions of y and z are outside the loop (including the special case where y 
andlor z is a constant), then y+z is Imp-invariant because its value will be the 
same each time x: 3ytz is encountered, as long as control stays within the 
loop. All such assignments can k detected from the ud-chains, that is, a list 
of all definition pints of y and z reaching assignment x : =y+ z. 
Having recognized that the vahe of x computed at x: =y+z does not change 
within the loop, suppose there is another statement v: =x+w, where w could 
oniy have been defined outside the Ioop. Then x+w is also Imp-invariant . 
We cao use the a b v e  ideas to make repeated passes over the twp, d i m v -  
ering more and more computations whose value is bopinvariant. If we have 
both ud- and du-chains, we do not even have to make repeated passes over the 
code, The du-chain for definition x : =y+z will tell us where this value of x 
could be used, and we need only check among these uses of x, within the 
loop, that use no other definition of x4 These Impinvariant assignments may 
be moved to the preheader, provided their operands besides x are also loop- 
invariant, as discussed in the next algorithm, 
Algorithm 10.7. Detection of Imp-invariant computations. 
inprrr. A loop L consisting of a set of basic blocks, each block containing a 
sequence of threeaddress statements. We assume ud-chains, as computed in 
Section IO.5. are available for the individuai statements. 

SEC. 10.7 
CODE-IMPROVING TRANSFORMATIONS 639 
Outpur. The set of three-address statements that compute the same value each 
time executed, from the time control enters the loop L until control next 
leaves L. 
Method. We shall give a rather informal specificatkm of the algorithm, trust- 
ing that the principles will be ckar. 
1. 
Mark "invariant" those statements whose operands arc all either constant 
or have all their reaching definitions outside L. 
2. 
Repeat step (3) until at some repetition no new statements are marked 
"invariant." 
3 
Mark "invariant" all those statements not previously so marked all of 
whose operands either are constant. have all their reaching definitions 
outside L, or have exactly one reaching definition, and that definition is a 
statement in L marked invariant; 
LI 
Performing Code Motion 
Having found the invariant statements within a loop, we can apply to some of 
them an optimization known as codt moiiutr, in whkh the statements are 
moved to the preheader of the loop, The following three conditions ensure 
that code motion does not change what the program computes. None of the 
conditions is absolu~ely essential; we have selected these conditions because 
they are easy to check and apply to situations that occur in real programs. 
We shall later discuss the possibility of relaxing the conditions. 
The conditions for statement s: x: =y+z are: 
I .  
The block containing s dominates all exti nodes of the loop, where an exit 
of a loop is a nude with a successor not in the hop. 
2. 
There is no other statement in the loop that assigns 10 x. Again, if x is a 
temporary assigned only once, this condition is surely satisfied and need 
not be checked. 
3. 
No use of x in the I m p  is reached by any definition of x other than a. 
This condition too will be satisfied, normally, if x is a temporary. 
The next three examples motivate ~ h c  
above conditions. 
Example 10,20. Moving a statement that need not be executed within a Imp 
to a psition outside the Loop can change what the program computes, as we 
show using Fig. 10.36. This observation motivates condition ( I ) ,  since a 
statement that dominates all the exits cannot fail to be executed, assuming the 
imp does not run forever. 
Consider the flow graph shown in Fig. LU.361a). B 2 .  B 3 ,  and 3, form a 
Imp with header B2. Statement 1 : =2 in B 3  is clearly loop-invariant. How- 
ever, B3 does not dominate B 4 ,  the only loop exit. If we move i : =2 to a 
newly created preheader B6, as shown in Fig. 10.3@b), we may change the 

640 CODE OPTIMIZATION 
i:=l 
( 
B ,  
1, 
i * - 2  
Bb 
uiv gota 
I 
(a) Beforr: 
(b) After 
Fig. 10.36. Example of illcgal d
c
 
motion, 
value assigned to j in Bs, in those caws where 8 never gets executed. For 
example. if u=30 and v=25 when B 2  is first entered, Fig. 10.36Ia) sets j to 7 
at B S ,  since B 3  is never entered, while Fig. 10.36(b) sets j to 2 .  
Exampte 10.21. 
Condition (2) is required when there is more than one 
assignment to x In the Imp. For example, the flow graph structure in Fig. 
10.37 is the same as that in Fig. 10.36(a), and we have the option of creating 
a preheader B 8s in Fig, 10.3qb) 
+ 
Since 8 in Fig. 10.37 dominates the exit B,, condition ( 1) does not prevent 
i:=3 from being moved to the preheadet Bb- However, if we do so, we shall 
set i to 2 whenever Bj is executed, and i will have value 2 when we reach 
B5. even if we f d o w  a sequence such as B 2  d B j  + B g  + B 2  +B4 +B5. 
For example, consider what happens if v is 22 and u is 21 when B 2  is first 
reached, If a ; = S  is in B2, we set j to 3 at B 5 ,  but if i:=3 is removed to the 
preheader, we set j to 2. 
Example 10,22. Now let us consider rule (31, The use k:=i in block B4 of 
Fig. 10.38 is reached by i : = I in block B , , as well as by i : = 2 in 8 3 .  Thus, 
we could not mow i: 
= 2  to the prebeader, k a u s e  the value of k reaching BS 
would change in the case w = v .  For example, if u*v=O then k is set to 1 in 
the flow graph of Fig. 10.38, but if 1: = 2  is moved to the preheader, we set k 
to 2 once and for all. 
!J 

SEC. 10.7 
CODE-IMPROVING TRANSFORMATIONS 641 
Fig. 10.37, Illustrating condition (21, 
Flg, 10.38. Illustrating mndition (3). 
W r i t h m  10.8. Code mution. 
Input. A loop L with ud-chaining information and dominator laformation. 
Ourput. A revised version of the Imp with a preheader and (possibly) some 
statements moved to the preheader + 
1, 
Use Algorithm 10.7 to find hapinvariant statements. 
2. 
For 
/ 
i l 
i i) 
iii) 
each statement s defining x found in step ( I), check: 
that it is in a block that dominates all exits of L, 
that x is not defined elsewhere in L, and 
that all uses in L of x can oniy be reached by the definition of x in 
statement s. 
3. 
Move, in the order found by Algorithm 10.7, each statement s found in 
(1) and meeting conditions (2i)+ (2ii), and (2iii), to a newly created pre- 
header, provided any operands of s that are defined in loop L (in the case 
that s was found in step (3) of Algorithm 10.7) have previously had their 
definition statements moved to the preheader. 
o 
To understand why no change to what the program computes can wcur, 
conditions (2i) and (2ii) of Algorithm 10.8 assure that the value of x m- 
puted at s must be the value of x after any extt blwk of L. When we move s 
to the prel-teadw, s will still be the definition of x that reaches the end of any 
exit block of L. Condition (2iii) assures that any uses of x within L did, and 
will continue to, use the value of x cornputad by s. 

642 CODE OPTIMIZATION 
%c. 10.7 
To see why the transformation cannot increase the running time of the pro- 
gram, we have only to nde that condition (2i) ensures that s i s  executed.at 
least once each time control enters L. After code motion, it will k executed 
exactly once in the preheader and not at all in L whenever control entirs L. 
Aitemative Code Motim Strategies 
We can relax condition ( 1) somewhat if we are willing to take the risk that we 
may actually increase the running time of the program a bit; of course, we 
never change what the program computes. The relaxed version of d e -  
motion cundihn (I} l i t . ,  item ai) in Algorithm 10.81 is that we may move a 
statement s assigning x only if: 
I '. The block containing s either dominates all exits of the Imp, or x is not 
used outside the loop. For example, if x is a temporary variable, we can 
be sure (in many compilers) that the value will bc used only in its own 
block, In general, live variable analysis is needed to tell if x is live at 
any exit from the \oop. 
\ 
If Algorithm 10.8 is modified to use condition ( 1'1, occasiooally the running 
time will increase slightly, but we can expect to do reasonably well on the 
average. The modified algorithm may move to the preheader certasn compu- 
tations that might not be executed in the Imp. Not only dms this risk slowing 
down the program significantly, it may also cause an error in certain cir- 
cumstances. For example, the evaluation of a division x/y in a Imp may be 
preceded by a test to see whether yeti. If we move x / y  to the preheader, a 
division by zero may occur. For this reawn, it is unwise to use condition ( 1 ' )  
unless optimization m y  be inhibited by the programmer, or we apply the 
stricter condition ( I )  to division statements. 
Even if none of conditions (2i), (2ii), and (2iii) of Algorithm 10.8 are met 
by an assignment x : =y+z, we a n  still take the computation y+z outside a 
Imp. Create a new temporary t, and set t : =y+z in the preheader, Then 
replace x : =y+z by x: =t in the loop. in many cases we can then propagate 
out the copy staternenr x : =t+ as discussed earlier in this section. Note that if 
condition (2iii) of Algorithm 10.8 is satisfied, that is, if all uses of x in Imp L 
are defined at x: =y+z (now x : =t), then we can surely remove statement 
x: =t by replacing uses of x in L by uses QC t and placing x: =t after each 
exit of the loop. 
Maintaining b - F l o w  Information After Cde Motion 
The transformations of Algorithm 10.8 do not change ud-chaining informa- 
tion, since by conditions (2i), (X), and (2iii), all uses of the variable assigned 
by a moved statement s that were reached by s are still reached by s from its 
new psitian. Definitions of variables used by s are either outside L, in which 
case they reach the preheader, or they are inside L, in which case by step (3) 
they were moved to the preheader ahead of s. 

SEC. lo+? 
CODE-IMPROVING TRANSFORMATIONS 643 
If the ud-chains are represented by lists of pointers to pointers to statements 
{rather than by lists of pointers to statements), we can maintain ud-chains 
when we rnove'statement s by simply changing the pointer to s when we move 
It. That is, we create for each statement s a pointer p,, which always points 
to s. We put p, on each ud-chain containing s. Then, no matter where we 
move s, we have only to change p,, regardless of how many ud-chains s is 
on. Of course the extra level of indirection costs some compiler time and 
space. 
If we represent ud-chains by a list of staternem addresses (pointers to state- 
ments) we can still maintain ud-chains as we move statements. But then we 
need dudlains, as well, for efficiency. When we move s, we may go down its 
du-chain, changing the ud-chain of all uses that refer to s. 
The dominator information is changed slightly by code motion. The pre- 
header is now the immediate dominator o f  the header, and the immediate 
dominator of the preheader is the node that formerly was the immediate d m -  
inator of the header. That is, the preheader is inserted into the dominator 
tree as the parent of the header. 
A variable x is called an indwrion variable of a Imp L if every time the vari- 
able x changes values, it is incremented or decremented by some constant. 
Often, an induction variable is incremented by the same constant each time 
around the Imp, as i in a Imp headed by for i := I Lo 10. However, our 
methods deal with variables that are incremented w decremented zero, m e ,  
two, or mare times as we go around a Imp. The number of changes to an 
induction variable may even differ at different iterations, 
A common situation is one in which an induction variable, say i, indexes 
an array, and some other induction variable, say t, whose value is a lineqr 
function of i, is the actual offsee used to access the array. Often, the only 
use made of i is in the test for Imp termination, We can then get rid of i by 
replacing its test by one on t. 
The algorithms that follow deal with a restricted class of induction variables 
to simplify the presentation. Some extensions of the algorithms can be done 
by adding more cases, but others require theorems to be proved about elrpres- 
S~DIIS 
involving the usual arithmetic operators. 
We shall look for buic induction variabbes, which are those variables i 
w h w  only assignments within loop L are of the form i : = i k c., where c+ is a 
constant.' We then Imk for additional induction variables j that are defined 
only once within L, and whose value is a linear function of some basic induc- 
tion variable i where j is defined. 
In our discussion of indudion vnriablcs, "+" stands m l y  fur ihc addition opralnr, nol a gcficric 
operator, and likewiw lor the olher standard ariihmetic opcrrtms. 

644 CODE OFTIMLZATION 
Algorithm 10.9, Detection of induction variables. 
Input. A kmp L with reaching definition information and loopinvariant corn- 
putsttion information (from Algorithm 10+7). 
Ourput. A set of induction variables. Associated with each induction variable 
j is a triple (i, c, d ) ,  where i 
is a basic induction variable, and c and d are 
constants such that the vabe of j is given by c*i+d at the point where j is 
defined, We say that j belongs to the jumily of i. The basic induction vari- 
able i belongs to its own family. 
1. 
Find all basic induction variables by scanning the statements of L. We 
use the loop-in variant computation information here, Associated with 
' each basic induction variable i is the triple (i, 1 ,  O), 
2. 
Search for variables k with a single assignment to k within L having one 
of the following forms: 
where b i s  a constant, and j is an induction variable, basic or otherwise. 
If j is basic, then k is in the family of j. The triple for k depends on 
the instruction defining it. For example, if k is defined by k: = j+b, then 
the triple for k is { j, b, 0). The triples for the remaining cases can be 
determined similarly + 
If j is not basic, let j be in the family of i. Then our additional 
requirements are that 
(a) there is no assignment to i between the lone point of assignment to 
j in 4 and the assignment to k, and 
(b) no definition of j outside L reaches k. 
The usual case will be where the definitions of k and j are in temporaries 
in the same block, in which case It i s  easy to check. In general, reaching 
definitions information will provide the check we need if we analyze the 
flow graph of the lwp L to determine those blocks (and therefore those 
definitions) on paths between the assignment to j and the assignment to 
k. 
We compute the triple for k from the triple (i, c,b) for j and the 
instruction defining k. For example, the definition k:=b+j leads to 
(i, b *c, b *d) for k. Note that the multip~ications in b*c and b*d can be 
done as the analysis proceeds because b, c, and d are constants, 
IJ 
Once families of induction variables have been found, we modify the 
instructions computing an induct ion variable to use additions or subtractions 
rather than multiplications, The replacement of a more expensive instrucrion 
by a cheaper one is called strength reduction. 

Example 1023. The lmp consisting of block B 2  in Fig. 10.39(a) has basic 
induction variable i, 
because the \one assignment to i in the loop increments 
its value by 1. The family of i contains t2+ 
because there is a single assign- 
men1 to t2, with right hand side 4*1. Thus the triple for tz is (i,4,O). 
Similarly, j is the only basic induction variable in the loop consisting of B 3 .  
and t4, with ttiple ( j,4,0), is in the family of j. 
(a) Bcforc 
Fig, 10.39. Strcngth rcduc~ion. 
We can also look for induction variables in the outer Imp with header B 2  
and blocks B 2 ,  B 3 ,  B4+ 
B s .  Both i and j are basic induction variables in this 
larger Imp. Again t2 and t4 are induction variables with triples [i,Q,O] and 
(j,4,O), respectively, 
The flow graph in Fig. 10.39(b) is obtained from that in Fig. 10.39(al by 
applying the next algorithm. We shall discuss this transformation below. 

646 CODE OPTIMIZATION 
SEC. 10.7 
Algorithm 10-10. Strength reduction applied to induction variabks. 
h p r .  A loop L with reaching definition information and families of induction 
variables mputed using Algorithm 10.9. 
Uutput. A revised Imp. 
Method. Consider each basic induction variable i in turn. For every induc- 
tion variable j in the family of i with triple (i, r ,  d): 
Create a new variable a (but if two variables jl and jz have the same 
triples, just create one new variable for both). 
Replace the assignment to j by j:=s. 
Immediately after each assignment i : 
=i+n in L, where n is a constant, 
append: 
s := s + r * n  
where the expression c*n is evaluated lo a constant kcause c cad n are 
constants, Place s in the family .of i, 
with triple (i,c,d)+ 
It remains to ensure that s is initialized to c*itd on entry to the loop. 
The initialization may be placed at the end of the prehcadcr. The initiali- 
zation consists of 
s := c*i 
/+just s:=i ifcis1 */ 
I 
s := s+b 
/+ ornitifdis0 *I 
Note that s i s  an induction variable ln the family of i. 
o 
Example 10.24. Suppose we consider the loops in Fig. 10.391a) inside-out. 
Since the treatment of inner loops containing B 2  and B3 i s  very simtlar, we 
shall talk of the Imp around B only. In Example 10.23 we observed thar the 
basic induction variable in the bop around B3 is j, and the other induction 
varia%le is t4 with triple ( j+4,0). In step (!) of Algorithm 10.10, a new vari- 
able a4 is constructed. In step (21, the assignment t4 : =4* j is replaced by 
: =s4 Seep (4) inserts the assignment 5 4 :  =s4-4 after the assignment 
j: = j-1, where the -4 is obtained by multiplying the - 1 in the assignment to 
j and the 4 in the triple (j+4,0) for t4. 
Since B I serves as a preheader for the Imp, we may place the initialization 
of s4 't 
the end of the block B I containing the definition of j. The added 
instructions are shown in the dashed extension to block B I .  
When the outer Imp is considered, the flow graph looks as shown in Fig. 
10+39(b). There are four variables, i, a*, j, and ad, that could be con- 
sidered induction variables. However, step (3) of Algorithm LO. 10 places the 
newiy created variables in the farnilies of i and j, respectively, to faditate 
the elimination of i and j, using the next algorithm. 
D 
After strength reduction we find that the only use of some induction vari- 
ables is in tests, we can replace a test of such an induction variable by that of 

SEC. 10.7 
CODE-IMPROVING TRANSFORMATIONS 647 
another. For example, if i and t are induction variables such that the value 
of t is always four times the value of i, then the test is= j is equivalent to 
t>=4* j. After this replacement, it may be possible to eliminate i. Note 
however that if t=-4*i, then we need to change the relational operator as 
well, kcause irs j is equivalent ta t<=-4* j. In the following algorithm we 
consider the case where the multiplicative constant is positive, leaving the gen- 
eralization to negative constants as an exercise. 
Algdthm 10.1 1. Elimination of induction variables. 
hpw+ A Imp L with reaching definition information, lwp-invariant wmputa- 
tim information (from Algorithm 10.7) and live variabk information, 
Ou~ptrt. A revised Imp. 
t .  
Consider each basic induction variable i whose only uses are to compute 
other induction variables in its family and in conditional branches, Take 
same j in i's family, preferabiy one such that 
C- and d in its triple 
(i, c, d) are as simple as possible {ix., we prefer c = 1 and d = O), and 
modify each test that i appears in to use j instead. We assume in the 
following that c is positive. A test OF the form if i relop x goto B, 
where x is not an induction variable. is replaced by 
r. := r*x 
/* r := x i f r i v  1 +I 
r := r+b 
/r: omit if d is 0 *# 
if j relop r goto B 
. 
where r i s  a new temporary. The case if x mhp i goto B is handled 
analogously. If there are two induction variables il and i2 in the test 
i f  i, relop i2 gotoll, then we check if both i, 
and 
i2 can be 
replaced. The easy case i s  wl~en we have jl with triple I i c , d ) and 
jz with triple (i2, 
cz , dl), and c = cz and dl = d2. Then, il 
relop i2 is 
equivalent to j , relop j2. In more complex cases, replacement of the test 
may not be worthwhile, because we may need to introduce two multipli- 
cative steps and one addition, while as few as two steps may be saved by 
eliminating il and i2. 
Finally, delete all assignments to the eliminated induction variabies 
from the loop L, as they will now be useless. 
2. 
Now, consider each induction variable j for which a statement j : 5s was 
introduced by Algorirhm 10.!0, First check that there can be no assign- 
ment to a between the introduced statement j:=s and any use of j, In 
the usual situation, y is used in the block in which it is defined, simplify- 
ing thts check; 0th erwise, reaching definitions informar ion, plus some 
graph analysis is needcd to implement the check, Then replace all uses of 
j by uses of s and delete statement j : =s 
+ 
0 

Example 10,25. Consider the flow graph in Fig. 10.39Ib). The inner Imp 
around B2 contains two indudion variables i and sz, But neither can be elim- 
inated because ;el is use'd to index into the array a and i is used in a test out- 
side the loop. Similarly, the loop around B3 contains ind~ctian variables j 
and s4, but neither can be eliminated+ 
Let us apply Algorithm 10.11 to the outer Imp. When the new variables 
s2 and s4 were created by Algorithm 10.10, as discussed in Example i0.24, 
s2 was placed in the family of i and s, was p l a d  in the family of j. Con- 
sider the family of i, The only use of i is in the test for loop termination in 
blmk B4, so 1 is a candidate for elimination in step ( 1 )  of Algorithm 10.1 1 .  
The test in block B 4  involves the two induction variables i and j. For* 
tunately . the families of i and j contain s2 and sq with the same constants 
in their triples, because the triples are {i,4,0) and (j,4,0), respectively. The 
te9 i > = j  can therefore be replaced by s 2 > = s p ,  allowing both i and j to k~ 
eliminated. 
Step (2) of the Algorithm 10.11 applies copy propagation to [he newly 
created variables, replacing tl and t4 by s2 and s4, respectively. 
Induction Variables with Lmp-Cnvariant Expressions 
In Algorithms 10.9 and 10.10 we can allow loopinvariant expressions instead 
of constants, However, the triple (i, 
c, d) for an induction variable j may 
then contain loop-invariant expressions rather than constants. The evaluation 
of these expressions should be done outside the Imp L, in the prehcader. 
Moreover, since the intermediate code requires there to be at most one opera. 
tor per instruction, we must be prepared to generate intermediate code state- 
ments for the evaluation of the expressions. The replacement of tests in Algo- 
rithm 10.1 1 requires the sign of the multiplicative mnswnt cb to be known. 
For this reawn i t  may be reasonable to restrict attention to c a m  in which c is 
a known constant. 
16,8 DEALlNG WITH ALIASES 
If two w more expressions denote the same memory address, we say that the 
expressions are aliuses of one another. In this section we shall consider data- 
flow analysis in the presence of pointers and procedures. both of which intro- 
duce aliases. 
The presence of pointers makes data-flow analysis more complex, since they 
cause uncertainty regarding what is defined and used. The only safe assump- 
tion if we know nothing about where pointer p can point to is to assume that 
an indirect assignment through a pointer can potentially change (i.e., define) 
any variable. We must also assume that any use of the data pointed to by a 
pointer, e,g., x: =*p. can potentially use any variable. These assumptions 
result in more live variables and reaching definitions than is realistic and 
fewer available expressions than is realistic. Fortunately, we can use data- 

SEC. 10.8 
DEALING WITH ALIASES 649 
flow analysis to pin down what a pointer might pint to, thus allowing us to 
get more accurate informat ion from our other data-flow analyses, 
As for assignments with pointer variables, when we come to a p r d u r e  
call, we may not have to make our worst-case assumption - 
that everything 
can be changed - 
provided, we can compute the set of variables that the pro- 
cedure might change. As with all code optimizations we may still make errors 
on the conservative side. That is, the sets of variables whose values "may be*' 
changed or used could properly include the variables that are actually changed 
or used in some execution of Ihe program. We shall, as usual, simply try to 
come fairly dose to the true sets of changed and used variables wi~hout work- 
ing unduly hard or making an error that alters what the program does. 
A Simple Pointer Language 
For specificity. let us consider a languap'in which there arc elementary data 
types (e.g+, integers and reds) requiring one word each, and arrays of these 
types. Let there also be pointers to these elements and to arrays, bur not to 
other pointers. We shall be contejt to know that a pointer p is pointing some- 
where in array a, without eonc&ning ourselves with what particular element 
of a is being pointed to. This grouping together of all elements of an array. 
as far as pointer targets are concerned, is reasonable. Typically, pointers will 
be used as cursors to run through an entire array, so a more detailed data- 
flow analysis, if we could accomplish i t  at all, would often tell us that at a 
particular point in the program, p might be pointing to any one of the ele- 
ments of a anyway. 
We must also make certain assumptions about which arithmetic operations 
on pointers are semantically meaningful, First, if pincer p pints ro a primi- 
tive (one word) data element, then any arithmetic operahion on p produces a 
value that may be an integer, but not a pointer. 1f p pints to an array, then 
addition or subtraction of an integer leaves p pinting somewhere in the same 
array, while other arithmetic operations on pointers produce a value that is 
not a pointer. While not all languages prohibit, say, moving 2 pointer from 
one array a to another array b by adding to the pointer, such action would 
depend on the particular implementation to make sure that array b followed a 
in storage. It is our p i n t  of view that an optimizing wmpiler should adhere 
only to the language definition in deciding whal optimizations to perform. 
Each compiler implementer. however, must make a judgment on what specific 
optimizations the compiler should be allowed to do. 
The Effects of Pointer Assignments 
Under these assumptions, the only variables that could possibly be used as 
pincers are those deciarcd to be pointers and temporaries thar receive a value 
that is a pointer plus or rnh-ius a constant, We shall refer to all rhese variables 
as pointers. Our rules for derermining what a pointer p can point to are as 
follows. 

1 .  
If there is an assignment statement s: p: =&a, then immediately after s, p 
p i n t s  only to a. If a is an array, then p can p i n t  only to a after any 
assignment t o i  of the form p: =&akc, where c is a constanr.%$ 
usual, 
La is deemed to refer to the location of the first dement of array a. 
2. 
If there is an assignment statement s: p: =q+c, where c is an integer 
~ t h e r  than zero, and p and q are pointers, then immediately after s, p 
can point to any array that q wuM point to before s, but to nothing else+ 
3. 
If there is an assignment s: p: =q, then immediately after s, p can wint 
to whatever q cwld p i n t  to before s. 
4. 
After any other assignment to p, there is no abject that p could point to; 
such an assignment is probably (depending on the semantics of the 
language) meaningless. 
5, 
After any assignment to a variable other than p, p pints to whatever it 
did before the assignment. Note that this rule assumes that no winter 
can point to a pointer. Relaxing this assumption does not make matters 
particularly more difficult, and we leave the generalization tb the reader. 
We shall define in[B], for block B, to be the fundion that gives for each 
pointer p the set of variables to which p could point at the beginning of B. 
Formally, inIB ] is a set of pairs of the form (p, a), where p is a pinter and 
a a variable, meaning that p might p i n t  to a. In practice, iroIB] might be 
represented as a list for each pinter, the list for p giving the sec of a's such 
that (p, a) is in inIB 1. We define our[B) similarly for the end of B. 
We specify a lransfet function rrmss that defines the effect of block B. 
That is, trans& is a function taking as argument a set of pairs S ,  each pair of 
the form (p, a) for p a pointer and a a nonpointer variable, and prducing 
another set T. Presumably, the set to which warn* is applied will be ;RIB 1 
and the result of the application will be outJB 1. 
We need only tell how to 
compute iram for single statements; transB wit1 then be the composition of 
rraq for each statement s of blwk B. The rules for computing tram are as 
~ O ~ ~ Q W S .  
?rms,(S) = ( S  - {(p, b) I any variable b]) U ((p, a)] 
2. 
If s is p: =q+ c for pointer q and ncmzero integer c ,  then 
~ransJS) = (S - ((p, b) I any variable b}) 
U {(p, b) I (q, b) is in S and b is an array variable j 
Nae that this rule makes sense even if p = q. 
3. 
lfsisp:=q, then 
"n 
h i s  section. + aands for itself rather than a gtnerr operator 

SEC. 10.8 
DEALING WITH ALIASES 651 
4, 
If a assigns to pointer p any other expression, then 
tramS(S) = S - {(p, b) I any variable b) 
5 ,  If s is not aan assignment to a pointer, then ~rurw,(S) = $ 
We may now write the equations relating in, W I ,  and m r r s  as follows. 
where if B consists of statements s , $1, . . . , skr then 
Equations 10.13 can be solved essentialiy like reaching definitions in Algo- 
rithm 10.2. W e  shall  heref fore not go into the details of the algorithm, but 
content ourselves with an example. 
Exam* 
10.26, Consider the flow graph of Fig. 10.40. We suppose a is an 
array and c is m integer; p md q arc pointers. Initially, we set atia[Bl] to @. 
Then, rrum, has the effect of removing any pairs with first compontnt q, 
then adding the pair (q, c). That is, q is asserted to point to e. Thus 
Then, inEB2] = uuc[B ,I. The effect of p: =&c is to replace all pairs with first 
component p by the pair (g, c). The effect of q : 
=&I a [ 2  1 1 is to replace 
pairs with first component q by (q, a). Note that q : =& ( a [ 2 1 1 is actually 
assignment of tht form q: = h + c  for a constant e. We may now compute 
The effect of p: =g+ 1 in B4 is to discard the possibility that p does not point 
to an array, That is, 
Note that whenever BZ is executed, making p p i n t  to c, a semantically mean- 
ingless action takes place if g is used indirectly after p:=p+ 1 in B,, Thus, 
this flow graph is not "realistic," but it does illustrate the inferences about 
winters that we can make. 

652 
CODE OPTIMIZATION 
SEC. 10.8 
Rg. 10.40, Flow graph with pointcr operations shown. 
Continuing, h [ B 5 ]  = uurlB,J, and rrunsa, copies the targets of q and gives 
them to p a s  well. Sinccqcan p i n t  to aor c in inlBsJ, 
On the next pass we find i n i ~ , ]  
= our(B,]. so out(8,J = {(p, a),(q, c)}. 
This value is also the new inlBZ1 and inlS3J, but these new values do not 
change vutlS2 1 or uarlB 31, nor i s  in IS4 I changed. We have thus converged to 
the desired answer. 
17 
Making Use of Pointer Information 
Suppose jin 13 I is the set of vat.iables pointed to by each pointer at the begin- 
ning of block 8 and that we have a reference to pointer p inside block B. 
Starting with in[B 1, apply truns, for each statement s of blwk B that precedes 
the reference to p, This computation tells us what p could point to at the par- 
ticular statement where that information is important. 
Suppose now that we have determined what each pointer muld point to 
when that pointer is used in an indirect reference, either on the left or right of 
the assignment symbol. How can we use this information to get more accu- 
rate solutions to the usual data-flow problems? ln each case we must consider 
in which direction errors are constrvative, and we must utilize pointer infor- 
mation in such a way that only conservative errors are made. T o  see how this 
choice is made, let us consider two examples: reaching definitions and live 
variable analysis. 
To calculate reaching definitions we can use Algorithm 10.2, but we need to 
know the values of kill and Ken for a block. The latter quantities are com- 
puted as usual for statements that are not indirect assignments through 
pointers, An indirect assignment *p:=a is deemed to generate a definition of 

XC+ 10.8 
DEALlNG WITH ALIASES 653 
every variable b such that p could point to b. This assumption is conserva- 
tive, because aas discussed in Section 10.5, it is generally conservative to 
assume definitions reach a point while in reality they do not, 
When we compute kill, we shall assume that +p: =a 'kills definitions of b 
only if b i s  not an array and is the only variable p could possibly p i n t  to+ If 
p could p i n t  to two or more variables, then we do not assume definitions of 
either are killed. Again, we are being conservative because we permit defini- 
tions of b to pass through xp:=a, and thus reach wherever they can, unless 
we can prove that *p: =a redefined b. In other words, when there is doubt, 
we assume that a definition reaches. 
For live variables we may use Algorithm 10.4, but we must reconsider how 
def and IN 
are to be defined for statements of the form *p: =a and a:=+p. 
The staterneni *p: =a uses only a and p, We say i t  defines b only if b is the 
unique variable that p might point to. This assumption allows uses of b to 
pass through unless they are surely blocked by the assignment +p: =a. Thus 
we can never claim thal b is dead at a pint when it is in fact live. The state- 
ment a: =*p surely represents a definition of a. It also represents a use of p 
and a use of any variable lhat p could point to+ By maximizing possible uses, 
we again maximize our estimate of live variables. By maximizing live vari- 
ables, we are normally being conservative. For example, we might generate 
code to store a dead variable, but we shall never fail to sto~e one that was 
live. 
Until now, we have spoken of "programs" that are single prwedures and 
therefore single flow graphs. We shall now see how to gather information 
from many interacting procedures. The basic idea is to determine how each 
procedure influences the g m ,  ki!/. MW, or def informat ion of the others, then 
to compute our data40w information for each procedure independently as 
before. 
During data-flow analysis we shall have to deal with  alias^ set up by 
parameters in procedure calls. As it is not possible for +wo global variables to 
denote the same memory address, at least one of a pair of alilia.~ must be a 
formal parameter. Since formals may be passed to procedures, it is possible 
for two formals to be aliases, 
Example 10.27, Suppose we have a procedure p, with two formal parameters 
x and y passed by reference, In Fig. 10.41. we see a situation in which b+x 
is computed in B I  and B3. Suppose that the only paths from blocks B ,  to B3 
go through B 
and there are no assignments to b or x along any such path. 
Then, is b+x available at B,? The answer depends on whether x and y could 
denote the same memory address. For example, there could be a call 
pl z , z 1, or perhaps a call of pl u,v) , where u and v are formal parameters 
of anather procedure qIu,v), and a call dqEz,z) is possible. 
Similarly, it is possible for x andLy to be aliases if x is a formal parameter, 

654 
CODE OPTIMIZATION 
SEC. 10.8 
say of p I x , w ) ,  and y is a variable with a a p e  accessible to some procedure 
q that calls p, say with call pIy, t 1. Even more complicated situations could 
make x and y aliases of one another, and we shall shortly develop some gen- 
eral rules for determining all such pairs of aliases. 
13 
Fig. 10.41, ll lustration of aliasing problems. 
It will turn out in some situations that il is conservative not to regard vari- 
ables as aliases of one another. For example, in reaching definitions, if we 
wish to assert that a definition of a is killed by a definition of b, we had 
better be sure that a and b are definitely aliases whenever the definition of b 
is executed. Other times it is conservative to regard variables as aliases of 
one another whenever there is doubt. ~~xample 
10.27 i s  one such case. If the 
available expression b+x is not to ?x killed by a definition of y, we had better 
be sure neither b nor x could be an alias of y. 
A M d d  of Code with Procedure Calls 
To illustrate how aliasing might be dealt with, let us consider a language that 
permits recursive procedures, any of which may refer to both local and global 
variables. The data available to a procedure consists of the globals and its 
own locals only; that is, there Is no block structure to the language. Parame- 
ters are passed by reference. We require that all p m X h ~ e s  have a flow 
graph with single entry (the initial node) and a single r e w v  node which causes 
control to pass .-back to the calling routine, We suppose for convenience that 
every n d e  lies on some path from the entry to the return. 
Now suppose we are in a procedure p and we come u p n  a call to pro- 
cedure gI u ,v). If we are interested in computing reaching definitions, avail- 
able expressions or any of a; number of other data-flow analyses, we must 
know whether ql u,v) might change the value of some variable. Note that 
we say "might'' change, rather than ''will" change. As with all data-flow 
problems, it is impssible to know for certain whether or not the value of a 
variable is changed. We can only find a set that includes all varhbles whose 
values do change and perhaps some that don't. By being careful, we can cut 

SEC. 10.8 
DEALING WITH ALIASES 655 
down an the latter class of variables, obtaining a g
d
 
approximation to the 
truth and erring only on the conservative side. 
The only variables whose values the call of q (u,v) could define are the 
globals and the variables u and v, which mighl be local to p. Definitions of 
lwals of q are of no consequence after the call returns. Even if p - q, it 
will be other copies of q's locals that change, and these copies disappear after 
the return. It is easy to determine which globals are explicitly defined by q; 
jua see which have definitions in q, or are defined in a procedure call made 
by q. In addition, u andlor v, which may be global, change if q has a defini- 
tion of its first and/or second formal parameter, respectively, or if these for- 
mal parameters are passed as actual parameters by q to another procedure 
that defines them. However, not every variable changed by a call to q need 
be defined explicitly by q or one of the procedures it calls, because variables 
may have aliases. 
Computing A l h  
Before we can answer the question of what variables might change in a given 
prmdure, we must develop an algorithm for finding aliases, The approach 
we shall use here is a simple one. We compute a relation = on variables that 
formalizes the notion "can be an alias of." In so doing we do not distinguish 
among occurrences of a variable in different calls to ihe same procedure, 
although we distinguish variables I m l  to different procedures but having the 
same identifier. 
To makc things simpler, we do not try to differentiate the sets of aliases at 
differenr points in the program. Rather, if two variables could ever be aliases 
of one another we assume [hey always could be. Finally, we shall make the 
conservative assumption that = is transitive, so variables are grouped into 
equivalence classes, and two variables could be aliases of one another if and 
only if they are in the same class. 
Algorithm 10.12. Simple alias computation. 
hpur. A cdlection of procedures and global variables. 
Ourput. An equivalence relation = with the property that whenever there is a 
position in the program where x and y are aliases of one another. x = y ;  the 
converse need not be true. 
1 
Rename variables, if necessary, so that no two procedures use the same 
formal parameter or local variable identifier, nor do a local, formal, or 
global share an identifier. 
2. 
If 
there 
is a 
procedure p 
, x 
. . . , x, 1 
and 
an 
invocation 
p(yI, yz, . . . , yrt ) of that procedure, set xi = y, for all i. That is, 
each formal parameter can be an alias of any of its corresponding actual 
parameters. 

656 CODE OPTIMIZATION 
SEC. 10.8 
3. 
Take the reflexive and transitive closure of the actual-formal correspon- 
dences by adding 
a) 
x = y whenever y = x+ 
b 
x = z whenever x = y and y = z for some y+ 
Example 10.28, Consider the sketch of the three procedures shown in Fig. 
10.42, where parameters are assumed passed by reference. There are two glo- 
bals, g and h, and two local variables, i for the pr~cadure main and k for 
the procedure two. Procedure one has formals w and x, procedure two has 
formals y and 2, and main has no formal parameters. Thus, no renaming of 
variables i s  necessary. We first compute the aliasing due to actual-fo~rnai 
correspondences. 
The call of one by main makes h = w and i = x. The first call of two 
by one makes 
w = y and w = z. The second call makes g = y and 
X ' 
2. 
The call of one by two makes k = w and y = x. When we take the Iran- 
sitive closure of the alias relationshiprr represented by =, then we find in this 
example that all variables are possible aliases of one another. 
u 
global g, h; 
procedure main( 1; 
local i; 
g := 
* * .  ; 
oneIh, i l  
end 
procedure onelw, x ) ;  
X :' 
r
m
r
 
p 
twoIw, w l ;  
t w i g ,  X )  
end ; 
procedure two( y, z ; 
local k ; 
h := 
r
-
r
 
+ 
cme(k, yl 
end 
The aliasing computation of Algorithm 10. I2 does not often result in such 
extensive aliasing as we found in E~arnpk 10.28. Intuitively, we would not 
often expect two different variables with dissimilar types to be aliases. More- 
over, the programmer undoubtedly has conceptual types for his variables. For 
example, if the first formal parameter of a procedure p represents a velocity, 
it can be expected that the first argument in any call'.to p will also be thought 

SEC. 10.8 
DEALING WITH ALIASES 657 
of by the programmer as a velocity. Thus we intuitively expect most pro- 
grams to produce small groups of possible aliases. 
Data-Flow Analysis in the Presence of Proredurn Calls 
Let us consider, as an example, how available expressions can be calculated in 
the presence of procedure calls, where parameters are passed by reference. 
As in Section 10.6, we must determine when a variable could be defined. thus 
killing an expression, and we must determine when expressions are generated 
(evaluated) + 
We may define, for each prccedure p, a set c-kangelp], whose value is to lx 
the set of global variables and formal parameters of p that might be changed 
during an execution of p. At this point, we do not count a variable as 
changed if a member of its equ~valence class of aliases is changed. 
Let &j [pi be the set sf formal parameters and globals having explicit defin- 
itions within p (not including those defined within procedures called by p}. 
To write the equations for c,kan#e lp], we have only to relate the globals and 
formals of p that are used as actual parameters in calls made by p to the 
corresponding formal parameters of the called procedures. We may write: 
where 
I .  
A = {a I a is a global variable or formal parameter of p such that, for 
some procedure q and integer i, pcalls q with a as the ith actual param- 
eter and the ith formal parameter of q is in change [qj) 
2. G = {g I g is a global in chnge1q] and p calls q). 
, 
It should come as no surprise that Equation (10.14) may be solved for a set 
of procedures by an iterative technique. Although the solution is not unique, 
we only need the smallest one. We may converge to that solution by starting 
with a too-small approximation and iterating. The obvious too-small approxi- 
mation with which to start i s  change lp) = dri'lpj. The de~aik of the iteration 
are left as an exercise for the reader. 
It is worth considering the order in which procedures should be visited in 
the above iieration. For example, if the procedures are not rnutuaily recur- 
sive, then we can first visit procedures that do not call any other (there must 
be at least .me). For these procedures, chunge = d 4  Next, we can compute 
change for those procedures that call only procedures which call nothing. We 
can apply ( !0+14) for this next group of procedures directly, since c.hange[qj 
will be known for any q in (10.14). 
This idea can be made more precise as folIows. We draw a c u h g  ~ r @ ,  
whose nodes are procedures, with an edge From p to q if p calls q.' A 
' Here wc assumc no prwcdurc-vnlud variables. Thcsc complicate thc mnslructiw ilf the caltifig 
graph, as wc must dctcrrninc tht possible  actual^ mrrespmding to profcdurc-valucd formals at Ihc 
same iimt: we dnnstruct thc cdgcs of thc calling graph. 

658 
CODE OPTlMIZATlON 
SEC, 10.8 
coI1ection of procedures that are not mutually recursive will have an acyclic 
calling graph. In this case we can visit each node once. 
We now give an algorithm for computing change. 
Algorithm 10.13. Interprocedural analysis of changed variables. 
p 
A collection of procedures pl , p ~ ,  
. . . , pn. If the calling graph is 
acyclic, we assume pi calls p, only if j<i. Otherwise, we make no assump- 
tion a b u t  which procedures call which. 
Output. For each procedure p, we produce changcIpl, the set of global vari- 
ables and formal parameters of p that may be changed explicitly by p with no 
aliasing, 
Method. 
1. 
Compute def [pt for each procedure p by inspection. 
2. 
Execute tbe program of Fig, 10.43 to compute change. 
(1) fm cach p m d u r c  p do rhmgr lpl : =dej lpl; /* initialim * f  
(2) while changcs to any chmgel pl occur dr 
(3) 
for i := 1 to n do 
(45 
for cach procedure q called by pi do begin 
( 5 )  
add any global variables in r*hangsIq] to rhnngelp, I; 
16) 
for each formal paramctcr x (thc jth) of q do 
I f )  
it x is in c h q e  1 q] then 
(8) 
for each call of q by p, do 
t 91 
if a. the jth actual paramctcr of thc call, 
is a global or formal parametrsr of pi 
I 10) 
then add a to c h n ~ e l p ~ ]  
end 
Fig. 10.43. Itcratht: algorithm to compute change. 
Exampie 10.29. 
Let us consider Fig. 10.42 again. 
By inspection, 
def [main/ = Is}, def lone] = {x) and dqfltwol.= {h)+ These are the initial 
values of change. The calling graph of the procedures i s  shown in Fig. 10.44. 
We shall take t w o ,  one, main as the order in which we visit the procedures. 
Consider pi = two in the program or Fig. 10.42. Then q can only be pro- 
cedure one in line (4). Since chngelonel = {x) initially, nothing is added to 
chrtgc[two] at line ( 5 ) .  At lines (6) and (7) we need wnsider only the 
second formal parameter of procedure one, since the first actual parameter is 
local to two, In the only cali of one by two, the second actual parameter is 
y, and its corresponding formal, x, is changed, so we set chungeltwol to 
{h, Y} at line ( 10). 
We now consider pi = one, At line {4), q can only be procedure two. 

DEALING WITH AL!ASES 
659 
m a i n  E 
Fig. 1 0 A .  Calling graph. 
At lin le (3, h i s a  global in change 1 two], so we set chmge[one J = (h, x). 
At lines (6) and (71, only the first formal parameter of two is in 
change [twol, so we must add g and w to change lone] at h e  (lo), these 
being the two first actual parameters in the calls of procedure two. Thus, 
chnge[onel = {g, h, w, x)+ 
Now consider main. Prwdure one changes both its formals, so both h 
and i will be changed in the call of one by main. However, i is a local 
and n
d
 not be considered. Thus we set c h g e  [main] = (g, h). Finally, 
we repeat the while-loop of line (2). On reconsidering two, we find that one 
modifies gtabal g. Thus the call to oneIk,y) causes g to be modified, so 
thltastge[two1 = {g, h, y}+ No further changes occur in the iteration, 
u 
Use of Change hiformtion 
As an example of the way change can be used, msider the computation of 
global common subertpressions. Suppose we are computing available expres- 
sions for a prwedure p, and we want to compute d i l l [ B  j fox a block B. A 
definition of variable a must be deemed to kit1 any expression involving a or 
invoking some x that could k an alias of a. However, a call in B to pro- 
cedure q cannot kill an expression involving a unless a is an alias (remember 
a is an alias of itself) of some variable in c h ~ s [ q ] .  Thus, the information 
computed by Algorithms 10.12 and 10.i3 can be used to construct a safe 
approximation to the set of expressions killed. 
To compute available e~pressions in programs with procedure calls, we must 
also have a conservative way of estimating the set of expressions generated by 
a procedure call, To be conservative, we may assume a+b is generated by a 
call to q if and only If, on every path from q's entry to its return, we find 
a+b with no subsequent redefinition of a or b. When we Imk for 
occurrences of a+b, we must not accept x+y as such an occurrence unless we 
are sure that, in every call of q, x is an alias of a and y an alias of b. 
We make this requirement because it is conservative to err by assuming that 
an expression is not available when it is. By the same token, we must assume 
a+b is killed by a definition of any z that could possibly be an alias of a or b* 
Thus the simplest way to compute available expressions for all nodes of all 

660 
CODE OPTIMIZATION 
SEC. 10.8 
procedures is to assume that a call generates nothing, and that aA#!lB j for all 
blocks 8 is as computed above. As one does n d  expect many expressions to 
be generated by the typical procedure, this approach is g o d  enough for most 
purposes. 
A more complicated, and mare accurate, alternative approach to the compu- 
tation of available expressions is to compute gt'nlp] for each procedure p 
iteratively. We may initialize genlp] to be the set of expressions available at 
the end of p's return node according to the method above. Thai is, no alias- 
ing is permitted for generated expressions; a+b represents only itself. even if 
other variables could be aliases of a or b. 
Now compute available expressions for all nodes of all procedures again. 
However, a call to qI a, b) generates those expressions in gen Iql with a and 
b substituted for the corresponding formals of q. d i d 1  remains as before, A 
new value of g m  lpj, for each procedure p, can be found by seeing what 
expressions are available at the end of p's return. This iteration may be 
repeated until we get no more changes in available expressions at any node, 
10.9 DATA-FLOW ANALYSIS OF STRUCTURED FLOW GRAPHS 
Gomless programs have reducible flow graphs; so do programs encouraged by 
several programming methodologies. Several studies of large classes of pro- 
grams have revealed that virtually all prograins written by people have flow 
graphs that are reducible. '' 
This observation is relevant for optimization pur- 
poses because we can find optimization algori~hms that run significantly faster 
on reducible flow graphs. In this action we discuss a variety of flow-graph 
concepts, such as "interval analysis," that are primarily relevant to muctured 
flow graphs. 
In essence, we shall apply the syntax-directed techniques 
developed in Section 10+5 to the more general setting where the syntax doesn't 
necessarily provide the structure, but the flow graph does. 
Depth-First Search 
There is a uLwful ordering of the nodes of a flow graph, known as deph-flrst 
orderirrg, which is a generalization of the depth-first traversal of a tree intro- 
duced in Section 2.3. A depth-first ordering can be used to detect Imps in 
any flow graph; it also helps speed up iterative data-flow aelgorithms such as 
hose discussed in Section 10.6. The depth-first ordering is created by starting 
at the initial node and searching the entire graph, trying to visit nodes as far 
away from the initial node as quickly as possible (deph first). 
The route of 
the search forms a tree. Before we give the algorithm, let us consider an 
example. 
'" "Writtcn by pcnplc" ir; nln redundant bccausc wc know of scvcral pnhgrams that gcneruc ordc 
with "rars' news" nf goto's. Thcrc is nothing wrtmg with this; thc xtructurc is in thc input I0 thcse 
programh. 

SEC. 10.9 
DATA-FLOW ANALYSIS OF STRUCTURED FLOW GRAPHS 66 1 
Example 10.30. One possible depth-fwst search of the flaw graph in Fig. 
10.45 is illustrated in Fig. 10,46. Wid edges form the tree; dashed edges-are 
the other edges of the flow graph. The depth-first search of the flow graph 
corresponds to a prmrder traversal of the tree, 1 -. 3 -t 4 - 6 + 7 -. 8 -. 10, 
then back to 8, then to 9. We go back to 8 once more, retreating to 7, 6, and 
4, and then forward to 5. We retreat from 5 back to 4, then back to 3 and 1 .  
From I we go to 2, then retreat from 2, back to I ,  and we have traversed the 
entire tree in preorder, Note that w e  have not yet explained how the tree is 
selected from the flow graph. 
D 
w 
Fig, 19.45. Flow graph 
Fig, 10.46. Depth-first presentation 
The depth$rsr ordering of the nodes is the reverse of the order in which we 
last visit the nodes in the preorder traversal. 
Example 10.31 + in Example 10.30. the complete sequence of nodes visited as 
we traverse the rree is 
The depth-first ordering i s  the sequence of marked numbers in reverse order. 
Here. this sequence happens to be 1 ,  2, . . . , 10. That is, initially the ndes 
were numbered in depth-first order. 
13 
We now give an algorithm that computes a dep(h-first ordering of a flow 
graph by constructing and traversing a tree rooted at the initial node, trying to 
make paths in lhe tree as long as possible. Such a tree i s  called a depsh-fjrsr 
spuming free (@st). It is this algorithm that was used to construct Fig. 10.46 
from Fig. 10.45. 

662 
CODE OPTIMIZATION 
SEC. 10.9 
Algorithm 10.14. Depth-firs1 spanning tree and depth-first ordering, 
Input. A flow graph G. 
Output. A dJst T of G and an ordering of the nodes of G. 
Methud. We use the recursive procedure searchIn) of Fig. 10.47; the algo- 
rithm is to initialize all nodes of G t o  "unvisited," then 4
1
 search{no), where 
no is the initial node. When we call seurch(n), we first mark n '*visited," to 
avdd adding n to the tree twice. We use i to count from the number 6f nodes 
of G down to 1 ,  assigning depth-first numbers dfn[n] to nodes n as we go. 
The sat of edges T forms the depth-first spanning tree for G, and they are 
called tree edges. 
u 
prwdutv searrh(n); 
W n  
r 1) 
mark n "visited ": 
r 2) 
f~ each SUIXXSSO~ s OF st do 
I 3  
if s is "unvisited" then begin 
(41 
add cdgc t~ Ir .s to T; 
( 5 )  
.wrrrch(s j 
end; 
{ 61 
djn[n] := i ;  
(7) 
i := i - 1  
end; 
(8) T := m p o ;  /*I set of edgcs *i 
19) h r  each d
c
 
n of G do mark rt "unvisited"; 
(10) i : = number of m d c s  of G : 
( I I ) searcY n,,) 
Fig, 10-47. Dcpth-Cirst ticarch algorithm. 
Example 10.32. Consider Fig. 10.47, We set i to 10 and call search( l ) ,  At 
line (2) of search we must consider each successor af node 1. Suppose we 
consider s = 3 first. Then we add edge I + 3 to the trw and call seorch(3). 
In se~rch(3) 
we add edge 3 - 4 to T and call searcN4). 
Suppose in swrch(41 we choose s - 6 firstA Then we add edge 4 
6 to T 
and call search(6)+ This in turn causes us to add 6 -. 7 to T and call 
search(7). Node 7 has two successors, 4 and 8. But 4 was already marked 
"visited" by seurd1(4), so we do nothing when s = 4. When s - 8 we add 
edge 7 - 8 to T and call sedi(8). Suppose we then chmse s = 10. We add 
edge 8 
SO and call search(l0). 
Now 10 has a succeLwr, 7, but 7 is already marked "visited," 
sr, in 
search( lo), we fall through to step (6) in Fig. 10.47, setting dfn [ 101 = 10 and 

EC. 10.9 
DATA-FLOW ANALYSH OF STRUCTURED FLOW GRAPHS 663 
i = 9. This completes the call to search( lo}, so we return to seurrh(8). We 
now set s = 9 in seurch(8), add edge 8 -. 9 to T and call search(9). The only 
successor of 9, node I, is already "visited," so we set bfn [9] = 9 and i = 8. 
Then we return to search(8). The last successor of 8, node 3, is "visited," so 
we do nothing for s = 3. At this p i n t ,  we have considered all successors of 
8, so we set dfn[8l = 8 and i = 7, returning to smrch(7). 
A11 of 7's successors have been considered, so we set dfn(7 ) = 7 and I = 6, 
~eturning to smrch(6). Similarly. 6's successors have been considered, so we 
set &[6] = 6 and i = 5 ,  and we return to seurch(4). Successor 3 of 4 has 
been "visited ,+' but 5 has not, so we add 4 -. 5 to the tree and call search(5), 
which results in no further calls, as successor 7 of 5 has been "visited." 
Thus, dfnC51 = 5 ,  i is set to 4, and we return to searck(4). We have corn- 
pleted consideration of the successors of 4, so we set dfil41 = 4 and i = 3, 
returning to search(3). Then we set dfnl31 = 3 and I - 2 and return to 
seurch( 1 ) . 
The final steps are to call sea~h(2) from search( I), wt dfnl21 = 2. i = I, 
return to smrc.h( I), set dfni 
= 1 and i = 0, Note that we chose a number- 
ing of the nodes such that dfn [i / = i. but that relation need not hold for an 
arbitrary graph, or even for another depth-first ordering of the graph of Fig. 
10.45. 
0 
Edges in a Depth-First Presentation of a Fhw Graph 
Whcc we construct a dfs; for a flow graph, the edges of the flaw graph fall 
in to three categories. 
I. 
There are edges that go from a node m to an ancestor of m in the tree 
(possibly to m itself). These edges we shall term wfreutirtg edges. For 
example, 7 -+ 4 and 9 + L are retreating edges in Fig. 10.46. It is an 
incerestlng and useful fact that if the flaw graph is reducible, then the 
retreating edges are exactly the back edges of the flow graph," indcpen- 
dent of the order in which succes-wrs are visited in step (2) of Fig. 10.47. 
For any flow graph, every back edge is retreating, although if the graph 
is nonreducibk there will be some retreating edges that are not back 
edges. 
2. There are edges, called abvutzcing rdgcs, that go from a n d e  m to a 
proper descendant of m in the tree. A\\ edges in the clfsr itself are 
advancing edges. There are no other advancing edges in Fig. HI+&, but, 
for example, if 4 -c 8 were an edge, it would be in this category. 
3. 
There are edges m -. n such that neither m nw n is an ancestor of the 
other in the dfsr. Edges 2 - 3 and 5 -. 7 are the only such examples in 
Fig. 10.46. We call these edges cross  edge,^. An important property of 

664 
CODE OPTIMIZATION 
SEC. 20.9 
cross edges is that if we draw the 
so children of a node are drawn 
from left to right in the order in which they were added 10 the tree, then 
all cross edges travel from right to left. 
It should be noted that m + n 
is a retreating edge if and only if 
dfn[rn I r dInln 1 .  To see why. note that if rn is a descendant of n in the #st, 
then s~ari.h(m) terminates before .reurch(n), so #o(m I 2 dfr~ In j. Conversely, 
if dfnlm I r dffiln 1, then searcklrn) terminates b e f m  seurchInl, or m = n. 
But seurc.k(n) must have begun before .wurrh(rn) if there is an edge m -. n, or 
else the fact that n is a successor of m would have made n a descendant of m 
in the dht. ' Thus the time sem.k(m) is active is a subinterval of the time 
.seurch(n) is active, from which it follows that n is an ancestor of m in the @r. 
Depth of a Maw Graph 
There is an important parameter 6f flow graphs called the dqlh. Given a 
depth-first spanning tree for the graph, the depth is the largest nurnkr of 
retreating edges on any cycle-free path. 
Example 10.33. In Fig. 10.46, the depth is 3, since there is a path 
with three retreating edges, but no cycle-free path with four or more retreat- 
ing edges. It is a coincidence that the "deepest" path here has only retreating 
edges; in general we may have a mixture of retreating, advancing. and cross 
edges in a "deepest" path. 
0 
We can prove the depth is never greater than what one would intuitively 
call the depth of hop nesting in the flow graph. lf a flow graph is reducible, 
we may replace "retreating" by "back" in the definition of "depth," since the 
retreating edges in any 45'f are exactly the back edges. The notion of depth 
then becomes Independent of the r&rt actually chosen, 
Intervals 
The division of a flow graph into intervals serves to put a hierarchical struc- 
ture on the flow graph. That structure in turn allows us to apply the rules for 
syntax-directed data-flow analysis whose development began in Section 10.5. 
Intuitively, an "interval" in a flow graph is a natural loop plus an acyclic 
structure that dangles from the nodes of that loop. An important property of 
intervals is that they have header nodes that dominate all the nodes in the 
interval; that is, every interval is a region. Formally, given a flow graph G 
with initial node no, and a node n of G, the inrerud wirh kcukr R ,  denoted 
Irn), is defined as follows. 
I. nisini(n). 
2. 
If all the predecessors of wrne node nt # no are in Iln), then m in is I(n1. 
3. 
Nothing else is in ]In). 

SEC. 10.9 
DATA-FLOW ANALYSIS OF STRUCTURED FLOW GRAPHS 665 
We therefore may build, I(n) by starting with tt, and adding nodes m by rule 
(2). It does nor matter in which order we add two candidates m because once 
a node's predecessors arc all in l(n), they remain in ltn), and each candidate 
will eventually k added by rule (2). Eventually, no more nodes can be added 
to IIn), and the resulting set of nodes is ihc interval with header n. 
Given any flow graph G, we can partition G into disjoint intervals as follows. 
Algorithm lQ.15. interval analysis of a flow graph, 
Irep~. A flow graph G with initial ncde no. 
Output, A partit ion of G into a set of disjoint intervals. 
Method. For any node n, we compute [(R) by the method sketched above: 
l(n1 := {n}; 
while there exists a node m f rro, 
all of whose predecessors are in I(n) do 
I(n) := l(n) U {m} 
The particular nodes  hat are headers of intervals in the partition are chosen 
as f~llows. Initially, no nodes are "selected." 
construct l(no) and "select" all nodes in that interval; 
while there is a node m, not yet "selected," 
but with a selected predecessor do 
construct I(m) and "select" all nodes in that interval 
Once a candidate m has a predecessor p selected, m can never be added to 
same interval not containing p. Thus, candidate m's remain candidates until 
they are selected to head their own Interva!. Thus, the order in which interval 
headers m are picked in Algorithm 10.15 does not affect the final partition 
into intervals. Also, as long as all nodes are reachable from no, it can be 
shown by induction on the length of a path from 
to n that node n will 
eventually either be put in some other node's interval, or will become a 
header of its own interval. but not both. Thus, ,the set of intervals mnstructed 
in Algorithm 10.15 truly partition G. 
Example 10,3k Let us find the interval partition for Fig, 10.45, We start by 
constructing I( I), because node 1 is the initial node. We can add 2 to !(I) 
because 2's 0 d y  predecessor is I. However, we cannot add 3 because it has 
predecessws, 4 and 8, that are n a  yet in !(I), and similarly, every other node 
except I and ? has predecessors not yet in I(1). Thus. [(I) = { I  .2}. 
We may now compute [(3) because 3 has some "selected" predecessors, 1 
and 2, but 3 'is not itself in an interval. However, no node can be added to 
1(3), so I(3) = (3). Now 4 is a header because it has a predecessor, 3, in an 
interval. We can add 5 and 6 to l(4) kcause these have only 4 as a 

predecessor but no other nodes can be added; e.g., 7 has predecessor 10. 
Next, 7 becomes a header, and he can add 8 to l(7). Then, wt can add 9 
and 10, because these have only 8 as predecessor. Thus, the intervah in the 
partition of Fig. 10.45 are: 
Interval Graphs 
From the intervals of one flow graph G, we can mfistruct a new flow graph 
I(G) by the following ruks. 
I .  The rides of 1(G) correspond to the intervals in the interval partition of 
G. 
2. 
The initial node of I(G) is the interval of G that contains the initial node 
of G. 
3 
There is an edge from interval 1 to a different interval J if and only if in 
G there is an edge from some node in I to the header of J .  Note that 
there could not be an edge entering some node n of J other than the 
header, from outside J ,  because then there would be no way n could have 
been added to J in Algorithm 10,15. 
We may apply Algorithm 10.15 and the interval graph mnstruction alter- 
nately, producing the sequence of graphs G, l(G), l(I(G)) , 
+ . . . Eventually, 
we shall came to a graph each of whose nodes is an interval all by itself. This 
graph is called the limit flow grqh of G. It i s  an interesting fact that a flow 
graph is reducible if and only if its limit flow graph is a single node. l2 
Example 10,35. Fig, 10.48 shows the result of applying the interval mnstruc- 
tion repatedly to Fig. lI)+45. The intervals of that graph were given in 
Example 10.34, and the interval graph mnstructed frwm these is in Fig. 
10.48ta). Note that the edge 10 -. 7 in Fig. 10.45 docs not cause an edge 
from the node representing {7,8,9,10} to itself in Fig, 10.48(a), because the 
interval graph construction explicitly exclu&d such hops, Also note that the 
flow graph of Fig. 10.45 is reducible because its limit flow graph is a single 
node. 
Node Splitting 
If we reach a limit flow graph that is other than a single node, we can pr-ed 
further only if we split one or more nodes. If a node n has k predecessors, we 
may replace n by k nodes, n , n l  , . . . , nk. The ith predecessor of PI 
becomes the predecesscrr of ni only, while all successors of JI 
become 
'l In fact, this definition is hi5twically the original definition, 

DATA-FLOW ANALYSIS OF STRUCTURED FLOW GRAPHS 6 7  
Fig. 10.48. Interval graph scqucncc, 
successors of all of the &,*s. 
If we apply Algorithm 10.15 to the resulting graph, each nj has a unique 
predecessor, and so it will surely become part of that prtdeceswr's interval. 
Thus, one node splitting plus one round of interval partitioning results in a 
graph with fewer nodes. As a consequence, the construction of interval 
graphs, interspersed when necessary with-, node splitting, must eventually 
attain a graph of a single node. The significance of this observation will 
b w m e  clear in the next sect ion, when we design data-flow analysis algo- 
rithms that are driven by thew two opations on graphs. 
Example 10.36. Consider the flow graph of Fig. 10.4qa), which is its own 
limit flow graph- We may split node 2 into 2cr and 2b, with predecewrs 1 
and 3, respectively+ This graph is shown in Fig. 10+49(b). If we apply inter- 
val partitioning twice, we get the sequence of graphs .shown in Fig. 10.49(c) 
and (d), leading to a single node. 
a 
A convenient way to achieve the same effect as interval analysis is to apply 
two simple transformations to flow graphs. 
TI : lf n is a node with a loop, i+e,, an edge n - n, delete that edge, 
TI: If there is a node n, not the initial n d e ,  that has a unique predecessor, 
m, then m may cornme n by dekting rt and making all successors of n 
(including m, possibly) be successors of m. 
&me interesting facts a b u t  the T I  and T2 transformations arc: 
1. 
If we apply T I  and T2 10 a flow graph G in any order, until a flow graph 
results for which no applications of T I  or T2 are possible, then a uuiquc 
flow graph results, The reason is that a candidate for Imp removal by T I  

Fig. 10.49. Nodc splitting followcd by intcrval partitioning. 
or consumption by T I  remains a candidate, even if some other application 
of T I  or T2 is made first. 
2, 
The flow graph resulting from exhausrive application of TI and TI to G is 
the limit flow graph of G+ The pr00f is somewhat subtle and left as an 
exercise. As a consequence, another definition of "reducible flow graph" 
is one that can be converted to a single node by TI and TF 
Exam* 
10.37. 
I n  Fig. 10.50 we see a sequence of TI and T I  reductions 
saarting from a flow graph that is a renaming of Fig. 10.49(b). In Fig. 
lO.Kl(b), c has consumed b. Note that the lwp on cd in Fig. l0.50(b) results 
from the edge d-.c in Fig. 10.50(a)+ That Imp is removed by TI in Fig. 
10.5qc). Also note that when a consumes b in Fig. 10.50(4), the edges from 
u and b to the node cd become a single edge. 
o 
Flg. 10.$0& Reduction by T ,  and TI. 

SEC. 10.9 
DATA-FLOW ANALYSIS OF n R  UCTURED FLOW GRAPHS 669 
Recall from Sction 10.5 that a region in a flow graph is a set of nodes N that 
includes a header, which dominates all the other nodes in a region. All edges 
between rides in N are in the region, except (possibly) for some of those that 
enter the header. For example, every interval is a region, but there are 
regions that are not intervals because, for example, they may omit some nodes 
an interval would include, or they may omit wine edges back to the header. 
There are also regions much larger than any interval, as we shall see. 
As we reduce a flow graph G by T I  and T I ,  at ali times rhe folkwing con- 
ditions are true: 
I. A node represents a region of G. 
2. 
An edge from a to b represents a set of edges. Each such edge is from 
some node in the region represented by n to the header of the region 
represented by b. 
3. 
Each node and edge of G is represented by exactly one node or edge of 
the current graph. 
To see why these observations hold, notice first that they hold trivially for 
G itself. Every node is a region by itself, and every edge represents only 
itself. Suppose we apply T i  to some node n representing a region R, while the 
Imp n - n represents some set of edges E, all of which must enter the header 
of R. If we add the edges E ro region R ,  it is still a region, so after removing 
the edge n - n, the node n represents R and the edges of E, which preserves 
conditions ( 1 )-(3) a h u e  . 
If we instead use TI to consume node b by n d e  u, let a and 6 represent 
regions i. and $ respectively. Also, let E be the set of edges represented by 
the edge a - b. We claim R ,  S, and E together form a region whose header is 
the header d R. To prove this, we must verify that the header of U dom- 
inates every node in S+ If not, then there must k some path to the header of 
S that does not end with an edge of E. Then the last edge of this path would 
have to be represented in the current flow graph by some other d g e  entering 
b+ But there can be no such edge, or TI cannot be used to consume b. 
Example 10,38, The node labeled cd in Fig, 10.50(b) represents the region 
shown in Fig. 10.51(a), which was formed by having r. consume b, Yote that 
the edge d -* o i s  not part of the region; in Fig. 10.50(b) that edge is 
represented by the loop on cd. However, in Fig. 10.50(c), the edge rd + cd 
has been removed, and the node r;b now represents the region shown in Fig. 
10.5 I (b). 
In Fig. 10.58(d), node r-d still represents the region of Fig. 10.5 l(b), while 
node a6 represents the region of Fig. 10.51(c). The edge a& - r.d in Fig. 
10.50Id) represents the edges a + E. and b -, c of the original flow graph In 
Fig. 10.5qa). When we apply T2 to reach Fig. 10,50(e), the remaining node 
represents the entire flow graph, Fig. lO.SO(a). 
a 

670 CODE OPTIMIZATION 
Fig. 10.51. Somc regions. 
We should observe that the property of T i  and T2 reduction menrimed 
above hdds also for interval analysis, We leave as an exercise the fact that as 
we construct [(GI, [UIG)), and so on, each node in each of these graphs 
represents a region, and each edge a set of edges satisfying property (2) 
above. 
I 
Finding Dominators 
We close this section with an efficient algorithm for a concept that we have 
used Frequently, and will continue to use in developing the theory of flow 
graphs and data-flow analysis. We shall give a simple algorithm for comput- 
ing the dominators of every node n in a flow graph, based on the principle 
that if p i ,  p 2 ,  . . . , p~ are all the predecessors of n, and d # tt, then 
d d m  n if and only if J durn pi for each i+ The method is akin to forward 
dara-flow analysis with intersect ion as the confluence operator (e.g., available 
expressions), In that we take an approximation 10 the set of dominators of n 
and refine i t  by repeatedly visiting all the nodes in turn. 
In this case, the initial approximation we choose has the initial node dorn- 
inated only by the initial node, and everything dominating everything besides 
the initial node. Intuitively, the reason this approach works is that dominator 
candidates are ruled cut on!y when we find a path that proves, say, m,dom n 
is false. 4f we cannot find such a path, from the initial node to n avoiding m, 
then m really i s  a dominator of R. 
Algorithm 10.16. Finding dominators. 
Input. A flow graph G with set of nudes N. set of edges E and initial node no. 
Outpur. The relation dom. 
Methud+ We compute D In), the set of dominators of n. iteratively by the pro- 
cedure in Fig. 10.52. At the end, d is in D ( n )  if and only if d dom n. The 
reader may supply the details regarding how changes to Dtn) are detected; 
Algorithm 10.2 will serve as a model. 
One can show that D ( n )  computed at line ( 5 )  of Fig. 10.52 is always a s u b  
set of the current DItr). Since Din) cannot get smaller indefinitely, we must 
eventually terminate the while-Imp. A proof that, after convergence, D ( n )  is 

EFFICIENT DATA-FLOW ALGORITHMS 67 1 
Fig, 10.52. 
Dominator computing algorithm. 
the set of dominators of n is left for the interested reader. The algurithm of 
Fig, 10+52 is quite efficient, as D{n) can be represented by a bit vector and 
the set operations of line ( 5 )  can k done with logical and and or, 
o 
Example 1039. Let us return to the flow graph of Fig. j0.45, and suppose in 
the for-loop of line (4) nodes are visited in numerical order. Node 2 has only 
i for a predecessor, so D(2) : = {2) U D 1). Since 1 is the initial node, D ( 1) 
was assigned (I) at line I I). Thus, D (21 is set to {I, 2) at line ( 5 ) .  
Then node 3, with predecesms 1 ,  2, 4, and 8, is considered. Line 15) 
gives us D(3) = {3) V ({l) f7 {1,2} n {1*2, . . . . 10)) - {1,3}. The remain- 
ing calculations are: 
The second pass through the while-Imp i s  seen to produce no changes, so 
the above values yield the relation d3m. 
a 
10.10 EFFICIENT DATA-FLOW ALGORITHMS 
ln this section we shall consider two ways to use flow-graph lheory to speed 
data-flow analysis. The first is an application of depth-first ordering to reduce 
the number af 'passes that the iterative algorithms of Scctions 10.6 take, and 
the second uses intervals or the TI and T1 transformations to generalize the 
syntax-directed approach of Section 10+5. 

672 CODE OPTIMIZATION 
Depth-First Ordering in iterative Aigorithms 
In all the problems studied so far, such as reaching definitions. available 
expressions, or live variables, any cvent of significance at a n d e  will be pro- 
pagated to that node along an acyclic path. For example, if a definition d is in 
inl81, then there is some acyclic path from the block containing d to B such 
that d is in the in's and out's all along that path, Similarly, if an expression 
x+y is not available at the entrance to block B, then there ir; some acyclic path 
that demonstrates that fact; either the path is from the initial node and 
indudes no statement that kills or generates x+y, or the path is from a block 
that kills x+y and along the path there is no subsequent generation of xty. 
Finally, for live variables. if x is live on exit from block 8, then there is an 
acyclic path from B to a use of x, along whkh there are no definitions of x. 
The reader should check that in each of these cases, paths with cycles add 
nothing. Fw example, if a use of x is reached from the end of block B along 
a path with a cycle, we can eliminate that cycle to find a shorter path along 
which the use of x is still reached from 3, 
if all useful information propagates along acyclic paths, we have an oppor- 
tunity to tailor the order in which we visit nodes in iterative data-flow idgo- 
rithms so that after relatjvdy few passes through the nodes, we can be sure 
inhrmation has passed along all the acyclic paths. In particular, statistics 
gathered in Knuth 11971bj show that typical flow graphs have a very low 
inrcwd depth, which is the number of times one must apply the interval parti- 
tion to reach the firnil flow graph; an average of 2.75 was found. Further- 
more, it can be shown that the interval depth of a fiow graph is never less 
than what we have called the "depth," the maximum number of retreating 
edges on any acyclic path. (If the flow graph is: not reducible, the depth may 
depend on the depth-first spanning tree chosen .) 
Recalling our discussion of the depth-first spanning tree in the previous sec- 
tion, we note that If u -. b is an edge, then the depth-first nurnkr of b is less 
than that of a only when the edge is a retreating edge. Thus, replace line (5) 
of Fig. 10.26, which tells us to visit each block B of the flow graph for which 
we are computing reaching defini~ions, by: 
br each block B in depth-first order do 
Suppose we have a path along which a definition d propagates, such as 
where integers represen1 the depth-first numbers of the blocks along the path. 
Then the first time through the loop of lines (5)-(9) in Fig. 10+26, d will pro- 
pagate from r1ull3j to iu15] to ~ ~ 1 5 1 ,  
and so m, up to nufl35). It will not 
reach in116] on that round. because as 16 preccdes 35, we had already com- 
puted 
161 by the time d was put in ~ ~ 1 3 5 1 .  
However, the next time we run 
through the Imp of Lines (52-(91, when we compute in[ 161, d will be included 
because it is in out1 35 l4 Definition d will also propagate to auri 161, im1231, and 
so on, up to alrrl45I, where it must wait because in141 was already computed. 

SEC 10.10 
EFFICIENT DATA-FLOW ALGORITHMS 673 
On the third pass, d travels to inl41, out14], in1 101, out[ 101, and in1 I7 1, so after 
three passes we establish that d reaches block 17.13 
I t  should not be hard to extract the general principle from this example. If 
we use depth-first order in Fig. 10.26, then the number of passes needed to 
propagate any reaching definition along any acyclic path is no more than one 
greater than the number of edges along that path that go from a higher num- 
bered block to a lower numbered block. Those edges are exactly the retreat- 
ing edges, so the number of passes needed is one plus the depth. Of course 
Algorithm 10.2 does not detect the fact that all definitions have reached wher- 
ever they can reach for one more pass, so the upper bound on the number of 
passes taken by that algorithm with depth-firfl block ordering is actually two 
plus the depth, or 5 if we believe the results of Knuth 11971bJ to be typical. 
The depth-first order is also advantageous For available expressions (Algo- 
rithm 10.31, or any data flaw problem that we solved' by propagation in the 
forward direction. For problems like live variables, where we propagate back- 
wards, the same average of five passes can be achieved if we chose the reverse 
of the depth-first order. Thus, we may propagate a use of a variable In block 
17 back wards along the path 
in one pass to in14], where we must wait for the next pass to in order reach 
out145j. On the second pass it reaches in[ 161, and on the third pass it goes 
from our[35l to out(3J. In general, one 
[he depth passes suffices to carry 
the use of a variable backward, along any acyclic path, if we choose the 
reverse of depth-first order to visit the nudes in a pass, because then, uses 
propagate along any decreasing sequence in a single pass, 
Structure43ased Data-Flow Analysis 
With a bit more effort, we can implement data-flow algorithms that visit 
nodes (and apply data-flow equations) no more times than the interval depth 
of the flow graph, and frequently the average n d e  will be visited even fewer 
times than that. Whether the cxrra effort results in a true time savings has 
nut hen firmly established, but a technique like this one. based on interval 
analysis, has been used in several compilers. Further. the ideas exposed here 
apply to syntax-directed data-flow algorithms For all sorts of structured cunml 
statements, not just the if + 
. then and do . . while discussed in Section 
10.5, and these have also appeared in several compilers, 
We shall base our algorithm on the structure induced on flow graphs by the 
T ,  and T Z  transformations. As in Section 10.5, we are concerned with the 
definitions that are generated and killed as control flows through a region. 
Unlike the regions defioed by if or while statements, a general region can 
have multiple exits, so for each block B in region R we shall compute sets 
'' Dcfinicion d also rcachcs imfI 171, but that is irrclcvanl to thc path in qucsticm. 

674 
CODE OPTIMLZATION 
SEC 10.10 
@WR.B and killRTB of dtfinit ions generated and killed, respectively, along paths 
within the region from the header to the end of block B, These sets will be 
used to define a sransfer function transRPB(S) that tells for any set S of defini- 
tions, what set of definitions reach the end of block B by traveling along paths 
wholly within R, given that all and only the definitions in S reach the header 
of R. 
As we have seen in Sections 10.5 and 10.6, the definitions reaching the end 
of block B fall into two classes. 
I .  Those that are generated within R and propagate to the end of B indepen- 
dent of S. 
2. 
Those that are no1 generated in R, but that also are not kiilcd along some 
path from the header of R to the end of B, and therefore are in 
t r ~ m ~ + ~ ( S )  
if and only if they are in $. 
Thus, we may write trans in the form: 
The heart of the algorithm is a way to m p u t e  t r u n s ~ ~  
for progressively 
larger regions defined by some { T I ,  TI)-decomposition of a flow graph. For 
the moment, we assume that the flaw graph is reducible, although a simple 
modification allows the algorithm to work for nonreducible graphs as well. 
The basis is a region consisting of a single block, B. Here the transfer func- 
tion of the region is the transfer function of the block itself, since a definition 
reaches the end of the block if and only if i t  is either generated by the block 
or is in the set S and not killed. That is, 
NOW, let us consider the construction of a region R by Tz; that is, R is 
formed when R ,  wnsurnes R 2 ,  as suggested in Fig. 10.53. First, note that 
within region R there are no edges from R2 back to R ,  since any edge from 
Rz to the header of R ,  i s  not a part of R. Thus any path totally within R goes 
(optionally? through R 
first, then (ap~ionally) through R 2 ,  but cannat then 
return to R I .  A
h
 note that the header of R is the header of R , .  We may 
conclude that within R, R2 does not affect the transfer function of nodes in 
R1; that is, 
killR.Jj = killR p,B 
for ail B in R 
For 8 in R,, a definition can reach the end of 8 if any of the following con- 
ditions hold. 

EFFICIENT DATA-FLOW ALGORITHMS 675 
Fb. 10.53b Region building by TI+ 
The definition is generated within R 2 .  
The definition is generated within R I, reaches the end of some prcdeces- 
m of the header of R 2 ,  and is not killed going from the header of R to 
B. 
The definition is in the set $ available at the header of R I, not killed 
going to some predecessor of the header of R1, and not killed going from 
the header of R2 to B. 
Hence, the definitions reaching the ends of those blocks in R 1 that are prede- 
cessors of the header of R2 play a special role, In essence, we see what hap- 
pens to a set S entering the header of R , as its definitions try to reach the 
header of R 2 ,  via one of its predecessors. The set of definitions that reach 
one of the predecessors of the header of R I  becomes the input set for R 2 ,  and 
we apply the transfer functions for R2 to that set, 
Thus, kt G bc the union of gen,l,, 
for all predecessors P of the header of 
R Z, and let K be the intersection of killR 
for all those predscesmrs P. Then 
if $ b the sel of dtfiniti~ns that reach the header of R I, the set of definitions 
that reach the header of R2 along paths staying wholly within R is 
C U (S - K ) .  Therefore, the transfer function in R for those blocks B in R 
may te computed by 
Next, consider what happens when a region R is built from a region R l  
ustng transformation TI 
+ The general situation is shown in Fig. 10.54; note 
that R consists of R i  plus some back edges to the header of R I  (which is also 
the header of R, of course). A path going through the header twice would be 
cyclic and, as we argued earlier in the section, need not be considered. Thus, 
all definitions generated at the end of block B are generated in one of two 
ways. 

676 CODE OPTIMIZATION 
Fig. 10.54. Region building by T I .  
I .  
The definition is generated within R 1  and does not need the back edges 
incorporated into R in order to reach the end of B. 
2. 
The definition is generated somewhere within R ,, reaches a predecessor 
of the header, follows one back edge, and i s  not killed going from the 
header ro 3, 
If we let C be the union of R ~ Q , . P  for all predecessors of the header in R ,  
then 
A definition is killed p i n g  from the headcr to B if and only if it is killed 
along all aqclic paths, so the back edges Incorporated into R do not cause 
more definitions to be killed. That is, 
Example 10.40. 
Let us reconsider the flow graph of Fig. 10.50. whose 
{ T I ,  T2)-decomposition is shown in Fig. 10.55, with the regions of the decom- 
position named. We also show in Fig. 10.56 some hypothetical bit vectors 
representing three definitions and whether they are generated or killed by 
each of the blocks in Fig. 10.55. 
Starting from the inside out, we note that for single-node regions, which we 
call A, B, C, and D, gen and kill are given by the table in Fig. 10.56. We may 
then proceed to region R ,  which is formed when C consumes D by T I .  Fol- 
lowing the rules for T2 above, we note that gm and kir'F do not change for C. 
that is, 
For node D ,  we have to find in region C the union of #en for all the predeces- 
sors of the header of the region D. Of course the header of region D is node 

SEC 10.10 
EFFICIENT DATA-FLOW ALGORITHMS 677 
Fig. 10.5, Dccompsirion of a flow .graph. 
Fig. NX5C .ip and kiU information for blocks in Fig. 10.55. 
D, and there ts only one predecessor of [hat node in region C, namely, the 
node C. Thus, 
Now, we build region S from region R by TI. The ki/!'s don't change, so 
we have 
To cornpule the gm's for S we note that the only back edge to the header of S 
that is incorporated going from R to S is the edge D - C. Thus. 
The computation for region T is analogous to that for region R and we 
obtain 

678 CODE OPTlMlZATlOH 
Finally. we compute gen and kill for region U, the entire flow graph. Since 
U is constructed when T mnsumes S by transformation T2, the values of gen 
and kill for nodes A and B do not change from what was just given abve. 
For C and D, we note that the header of S ,  node C, has two predecessors in 
region T, namely A and B. Therefore, we wmpute 
Then we may compute 
Having computed gerr,,, and kiilU,B for each block B, where U is the region 
consisting of the entire flow graph, we essentially have wmputed 0ur[8] for 
each block B+ That is, if we I
d
 at the definition of t r ~ n s ~ . ~ ( S )  
= 
genu-8 U (S -kjllu.~), we note that t r a n ~ ~ , ~ ( 0 )  
is exactly uut[B]. 
But 
t%tt~,y,~(SZI) g e n ~ , ~ .  
Thus, the completion of the structure-based reaching 
definition algorithm is to use the gen's as the out's, and compute Ihe in's by 
taking the union of the out's of the predecessors. These steps are summarized 
in the following algorithm. 
Algdehm 10+17, Structure-based reaching definitions. 
Inpub. A reducible flow graph G and sets of definitions gen [ B ]  and kil1lBj for 
each block 5 of G. 
Output. in[B] for each block B. 
1. 
Find the (TI, 
T21decompositionf~rG. 
2. 
For each region R in the decompmitiorr, from the inside out, compute 
g e n ~ , ~  
and kiURnB for each block B in R, 
3. If U is the name of the region consisting of the entire graph, then for 
each Muck B, stt in[B] to the union, over all prdwssors P of block 8, 
of $ ~ W + P .  
0 

SEC 10.m 
EFFICIENT DATA-FLOW ALGORITHMS 679 
Some Speedups to the Structnre-Based Algorithm 
First, notice that if we have a transfer function C U (S - K I ,  the function is 
not changed if we delete from K some members of G .  Thus, when we apply 
T I ,  instead of using the formulas 
we can replace the second by 
thus saving an operation for each block in region R 2 .  
Anaher useful idea is to notice that the only time we apply T, is after we 
have first consumed some region Rz by R,, and there are some back edges 
from R 2  10 the header of I?]. 
Instead of first making changes in R1 because 
of the TZ operation, and then making changes in R I  and R 2  due to h e  TI 
operation, we can combine the two sets of changes if we do the fdlowi~g. 
1 .  
Using the T? rule, compute the new transfer function for those nodes in 
R that are predecessors of the header of R I . 
2. 
Using the T I  rule. compute the new transfer function for all the nodes of 
h. 
3. 
Using the T z  rule, compute the new transfer function Fur all the nodes of 
Rp. Note thai feedback due to the application uf TI has reached the 
predecessors of R 2  and is passed to all of R2 by the T I  rule; there is no 
need to apply the TI rule for R;. 
Handling Nonrebucible Flow Graphs 
If the ITI, T2)-reduction of a flaw graph stops at a limit flow graph that is 
not a single node, then we must perform a node splitting. Splitting a node of 
the limit flow graph corresponds to duplicating the entire region represented 
by that node. For example, in Fig+ I037 we suggest the effect that nude 
splitting might have on an original nine-node flow graph that was partitioned 
by T and T I  into three regions connected by some edges. 
As mentioned in the previous section, by alternating splits with sequences of 
reduc~ions, we are guaranked to reduce the tlow graph to a single node, The 
result of the splits is that some of the nodes of the original graph will have 
more ~han one copy in the rcgton represented by the one-node graph. We 
may apply Algorithm 10.17 to this region with little change. The only differ- 
ence iis that when we split a node, the p i s  and kiWs for the nodes of the ori- 
ginal graph in the region represented by the split node must be duplicated. 
For example, whatever the values of gen and kill are for the nodes in the 
t wo-node region of Fig. 10.57 on the left become #en and kill for each of the 
corresponding nodes in both two-node regions on the right. At the final step, 

680 
CODE OPTIMIZATION 
Fig. 10.57. Splitting a nonrcduciblc flow graph 
when we compute the in's for all the nodes, those nodes of the original graph 
that have several representatives in the final region have their in's computed 
by taking the union of the in's of all their representatives. 
In the worst case, splitting of nodes couM exponeotiate the total number of 
nodes represented by all the regions. Thus, if we expect many flow graphs to 
be nonteducible, we probably should not use strucr we-based methods. For- 
tunately, nonreducible flow graphs art sufficiently rare that we can generally 
ignore the cost of node splitting. 
10.11 A TOOL FOR DATA-FLOW ANALYSIS 
As we have pointed out before, there are strong similarities among the various 
data-flow problems studied, The data-flow equations of & c t i ~ n  10+6 were 
seen to be distinguished by: 
I, The transfer function used, which in each case studied was of the form 
f (X) = A U (X - B I .  
For example, A = kill and B = gen for reaching 
definitions. 
2, The confluence operator. which in all cases so far has k e n  either union 
or in tmection. 
3. 
The direction of propagation of information: forward or backward. 
Since these distinctions are not great, it should not be surprising that all 
these problems can be treated in a unified way. Such an approach was 
described in Kildall 119731, and a tool to simplify the implementation of data- 
flow problems was implemented by Kildall and used by him in several com- 
piler projects. It has not seen widespread use, probably h a u s e  the amount 
of labor saved by the system is not as great as that saved by tools like parser 
generators. However, we should be aware of what can be done not only 
because it does suggest a simplification for implementers of optimizing 

compilers, but also kcause it helps to unify the various ideas we have seen so 
far in this chapter. Further, this xction suggests how more powerful data- 
flow analysis strategies, ones that provide more precise information than the 
algorithms mentioned so far, can be developed. 
Dab-Flow Analysis Framewwks 
We shall describe frameworks that model forward propagation problems. If 
we consider only the iterative type of solution to data-flow probkrns, then the 
direction of flow makes no difference; we can reverse the direction of edges 
and make some minor adjustments to account for the initial node, and then 
treat a backward problem as if i t  were Forward. Structure-based algorithms 
are somewhat different; the forward and backward problems are not solved in 
quite the same way because the reverse of a reducible flow graph need not be 
reducible. However, the treatment of backward problems will be Lft as an 
exercise, and we shall restrict ourselves to forward problems only. 
A clrriu-flow arroZysis frurnr work consists of: 
I + 
A set V of vduc~s to be propagated. The values of i~ and w
r
 are 
members of V. 
3 ,  
A binary meel oprurion A, on V ,  to represent the confluence operator. 
Example 10.41. For reaching definitions. V consists of all subsets of the set 
of definitions in rhe program. The set F is the set of all functions of the form 
f (X) = A U (X - B ) ,  where A and B are sets of definitions, i.e., members of 
V; A and B arc what we called gc'n and kill, respectively. Finally, the opera- 
tion A is union. 
For available expressions, V consists of a11 subsets of the set of expressions 
computed by the program, and F is the set of expressions of the same form as 
above, but where A and B are now sets of expressions. The meet operation is 
intersection, of course. 
o 
Example 10,42+ Kildall's approach is not limited to the simple examples with 
which we have been dealing, although the complexity, both in terms of corn- 
putation time and intellectual difficulty, rises, The exercises suggest s very 
powerful example, where the data-flow information computed tells us in 
essence, all pairs of expressions that have the same value at a poin~. How- 
ever, we shall get some of the flavor of this example by giving a method for 
telling which variables have constant values sn a Nay that captures more infor- 
mation than reaching definitions. Our new framework understands? for exam- 
ple, that when x is defined by r l :  x: =xt I ,  and x had a constant value before 
ass€gnrnen t . it does so afterward + 
I n  contrast. using reaching definitions for constant propgdf ion we would 
see that statement d was a possible definition of x, and assume therefore that 
x did not have a constant value. Of course, in one pass the right side of 

6 
CODE OPTIMIZATION 
SEC 10.1 1 
d :  x:=x+ 3 might be replaced by a constant, and then another round of con- 
stant propagation could detect rhal uses of the x defined at d were actually 
uses of a constant. 
In the new framework, the set V i s  the set of all mappings from the vari- 
ables of the program to a particular set of values. That set of values consists 
of 
1 .  
All constants. 
2. 
The valuc nonconki, which means that the variable in question has k e n  
determined not to have a constant value. The value nunwnst would be 
assigned to variable x if, say, during data-flow analysis we discovered 
two paths along which the values 2 and 3, respectiveIy, were assigned to 
x, or a path along which the previous definition of x was a read state- 
ment. 
3. 
The value unbej, which means that nothing may yet be asserted abmt the 
variable in question, presumably bemuse early in the execution of the 
data-flow analysis, no definition of the variable has been discovered to 
reach the point in question. 
Note that twntunsr and unw are not the same; they are essentially opposites. 
The first says we have seen m many ways a v a ~ ~ a b k  
could be defined that we 
know it is not constant. The second says we have seen so tittle about the vari- 
able. that we annot say anything at all. 
The meet operation is defined by the following table. We let p and v be 
two members of V; that is, p and v both map each variable to a constant. to 
urtdef, or to nonmmt. Then the function p = pflv is defined in Fig, 10.58, 
where we give the value of p(x) in terms of the values of p(x)and v(x) for 
each variable x. In that table, c is an arbitrary constant, and d is another con- 
sranl definitely not equal to r. Fur example, if FIX) - (. and v ( x )  = 6. a dif- 
ferent constant, then apparently x takes the values c and d along two different 
paths, and at the confluence of those paths, x has no constant value: hence the 
choice p(x) - nnncmst. A s  another example, if dong one path. nothing is 
known about x, reflected by p(x) = rrrrdtf, and along another path, x is 
believed to have the valuc c., then after the confluence of these paths, we can 
only assert that x has the balue c.. Of course, later dimvery of another path 
to the point of confluence, abng which x has a value besides r.. will change 
the assigned valuc for x after the confluence to nunconst, 
Finally, we must design the set of functions F that reflect the transfer of 
information from the beginning to the end of any block. The description of 
this set- of functions is complicated, although the ideas arc straightforward, 
We shall therefore give a "basis" for the set of functions by describing the 
functions that represent single definition statements, and the entire set of 
functions can then be constructed by composing functions from this basis set 
to reflect blocks with more than one definition statement. '- 

A TOOL FOR DATA-FLOW ANALYSIS 683 
The identity function is in F; this function reflects any block that has no 
definition statements. If I is the identity function, and p is any mapping 
from variables 10 values, then I ( F )  = *. Note that + itself need not be 
the identity; it is arbitrary. 
For each variable x and constant ih there is a function f i n  F such that for 
every mapping 
in V, we have f (p) = v, where vfw) = p(w) for all w 
other than x, and v ( x )  = C. These functions reflect the action of an 
assignment statement x : =c. 
For each three (not necessarily distinct) variables x, y, and z, there is a 
function f in F such that for every mapping 
in V ,  we have f (p) = v. 
The mapping v is defined by: for every u hsibes x we have 
v(w) = ~ ( w ) ,  and v(x) = p(y)+ ~ ( z l .  If either p(y) or ~ { z )  
is non- 
L'OHSI, then the sum is mnconst. If either p(y) or p(z) is undef, but nei- 
ther is mn~'om~, 
then the result is iindej. This function expresses the 
effect of the assignment x:=y+z. As usual in this chapter, * may be 
thought of as a generic operator; here an obvious modification is neces- 
sary if the operator is unary, ternary, or higher, and another obvious 
modification is needed to take into account effect of a copy statement, 
x:=y. 
For each variable x there is a function f in F such that for each p. 
f Cp) = v ,  where . v(w) = p(w) 
for 
ail 
w 
other 
than 
x, 
and 
v(xl = nancortst. This function reflects a definition by reading x, since 
after a read statement, x surely must k assumed not to have any particu- 
lar constant value. 
0 
The Axioms d Data-Flw Analysis Frameworks 
In order to make the kinds of data-flow a!gorithms that we have seen so far 
work for an arbitrary framework, we need to assume some things abour the 
set V, the set of functions F, and the meet operator A. Our basic assumphns 
are listed below, although some data-flow algorithms need additional assump- 
tions. 

I ,  
F h a s a n  identity function I, such that f(p) = p for all p in V. 
2, 
F is closed under composition; that is, for any two functionsJand g in F, 
the function h defined by h ( @ )  = g(J(p)) is in F. 
3, 
A i s a n  asmiative,cornmutative,anb idempotentoptration. These three 
properties are expressed algebraically as 
pAIvflp) = w w f l p  
4. There is a top dement T in V, satisfying the law 
Example 10,43, Let us consider reaching definitions. F surely has the iden- 
tity. the function where Ken and kill are both the empty set. To show closure 
under composition, suppose we have two functions 
We may verify that the right side of the above is algebraically equal to 
I f  we let K = K ,  U K 2  and G = (G2 
U (GI -Kl)), 
then we have shown that 
the composition of S, and f l ,  which is f ( X )  - G U (X - K ) ,  is of the form 
that makes it a member of C. 
As for the meet operatar. which is union, it is easy to check that union is 
associative, commutative, and idempotent. The "mp" element turns out to be 
the empty set in this case, since + U X = X for any set X. 
When we consider available expressions, we find that the same arguments 
used for reaching definitions also show that F has an identity and i s  closed 
under mrnpsition, The meet operator is now intersection, but this operator 
tor, is associative, commutative, and idempotent. The top elemen1 makes 
more intuitive sense this time; it is the set E of all expressions in the program, 
since for any set X of expressions, E 17 X = X. 
Example 10.44. Lel us consider the canstant computali~n framework intro- 
duced in Example 10.42. The set of functions F was designed to include the 
identity and to be closed under composition, To check the algebraic laws for 

SEC 10.11 
A TOOL FOR DATA-FLOW ANALYSIS 685 
fi, it suffices to show that they apply for each variable x. As an instance, we 
shall 
check 
idemputence. 
Let 
v = p f l ~ ,  that 
is, 
for 
all 
x, 
wlx) = p(x)flb(x). 
It is simple to check by cases that v I x )  - p(x). For 
example, if p(x) = nonuurtst. then V ( X )  = W~L'OIISI, 
since the result of pairing 
nonwnur with itself in Fig. 10.58 is nonuurur, 
Finally, the top element is the mapping T defined by ~ ( x )  
= undef for all 
variables x+ We can check from Fig. 10.58 that for any mapping p and any 
variable x, if v is the function T A ~ ,  
then V ( X )  -- @(XI, 
since the result of 
pairing un&f 
with any value in Fig, 10.58 is that other value. 
a 
Monotonicity and Ditributivity 
We need another condition to make the iterative algorithm for data-flow 
analysis work, This condition, called monotonicity. says informally that if you 
take any funclion f from the set F. and if you apply $ to two members of V, 
one "bigger" than the csdw, then the result of applying f to the bigger is not 
less than what you get by applying f to the smaller. 
To make the notion of "bigger" precise, we define a relation 5 on Y by 
Example 10.45. 
In the reaching definition framework, where meet is union 
and members of V are =is of definitions, X 5 Y means X U Y = X, that is. X 
is a superset of Y. Thus, 5 looks "backward;" smaller elements of V are 
supersets of larger ones. 
Fw available expressions, where meet is intersection, things work "right," 
and X 5 Y means that X 17 Y 
X; i.e., X is a subset of Y. 
u 
Note from Example 10.45 that 5 in our sense need not have all the proper- 
ties of 5 on integers, It is true that 
is transitive; the reader may prove, as 
an exercise in using the axioms for A, that p 5 v and v 5 p imply p .I p. 
However, 5 in our sense is not a total order. For example, in the available 
expressions framework, we can have two sets X and Y, neither of which is a 
subset of the other, in which case neither X 5 Y nor Y 5 X would be true. 
It ofren helps to draw the set V in a hcrice diugrctm, which is a graph whose 
nodes are the elements of V .  and whose edges are directed downward, from X 
to i' if Y r X. For example, Fig. 10.59 shows the set V for a reaching defini- 
lions data-flow problem where there are three definitions. d l ,  d:, and d3. 
Since I 
is "superset of," an edge is directed downward from any subset of 
rhexe three definitions to each of its super%;ets. Since 5 is transitive, we con- 
ventionally omit the edge from X to Y if there is another path from X to Y left 
in [he diagram. Thus, although (dl, dZ, d 3 }  5 { d l ) ,  we da not draw this 
edge since it is represented by rhe path through { d l ,  d l ) ,  for example. 
It is also useful to note that we can read the meet off such diagrams, since 
XAY is always the highest Z for which there are paths downward to Z from 
b t h  X and Y. For example, if X is { d l }  and Y is Id2), 
then Z in Fig. 10.59 is 
{dl, d?), which makes sense, because the meet operator is union, It i s  also 

686 
CODE OPTIMIZATION 
Fig. 10.59. L a t h  of subsets of definitions. 
true that the top clement wiil apFar at the top of the lattice diagram; that is, 
there is a path hwnward from T to each dement. 
Now, we can define a framework CF, V, A) to be mwtone if 
for all p and v in V, and f i n  F, 
There is an equivalent way to define rnonotonicity: 
for all p and v in V and f in F. It is useful to jump back and forth between 
these two equivalent definitions, so we shall sketch a ,  proof of their 
equivalence, leaving for the reader some simple observations to verify, using 
the definition of 5 and the associative, commutative, and idempotent laws for 
A. 
Let us assume (10.15) and show why (10.16) holds, First, note that for any 
~r, 
and v, &v 
5 
and $flu 5 v both hold; it is a simple proof, left for the 
reader, to prove these facts by proving, 
for any 
and y, 
that 
(xAy)Ay = xAy. Thus, by 
10. is)-, f ( p A v )  5 f (p) and J(pAv) 5 f (v). 
We leave it to the reader to verify the general law that 
x 5 y and x 5 z imply x 5 yflz 
Letting x be$(p"w), y = j(p), and z = SIP), we have (10.16). 
Conversely, let us assume ( LO. 16) and prove ( 10.15). We suppose + 5 v 
and use ( 10.16) to conclude f ( p )  5 f ( v ) ,  thus proving (10.15)+ Equation 
( 10.16) tells us f(pflu) 5 j(fi)flf(v). 
But 
since )L 5 u is assumed, 
~ f l v  - p, by definition. Thus (10,W) saysf(&) 5 f (y)flf(v). As a gen- 
eral rule the reader may show that 
if x 5 yflr then x 5 z 
Thus, ( LO. 16) implies f (p) 5 f ( v ) +  and we have proved (IO.15). 
Often, a framework obeys a condition stronger than (10.16), which we call 
the dhtributivity cusrdirion: 

SEE 10+1 1 
A TOOL FOR DATA-FLOW ANALYSIS 687 
for all p and v in V and f in F .  ~ e r t a i n l ~ ,  
if x = y, then xr\y = x by idem- 
potence, so x 5 y. Thus, distributivity implies monotonicity. 
Enampk 10.46. Consider the reaching definltions framework. Let X and Y 
be sets of definitions, and let 
be a function defined by f (2) = G U ( 2 - K )  
for some sets of defitlitions G and K, Then we can verify that the reaching 
definitions framework satisfies the distribut ivity condition, by checking that 
Drawing a Venn diagram makes the proof of the above relationship trans- 
parent, although it looks complicated. 
a 
Example 10,47, 
Let us show that the constant computation framework is 
monotone, but not distributive. First, it helps to apply the fl operation and 
the 5 relationship to the elements appearing in the table of Fig. 10,58. That 
is, let us define 
nonr.mstAr = noacoasl 
for any constant c 
aAd - nanrortsi 
for constants c # d 
c.Audef = C. 
for any constant c 
nanconsr Audef = rtomonsi 
x
h
 = x 
for any value x 
Then Fig. 10.58 can be interpreted as saying that p(a) = p(a) fl Ha). 
We can determine what the 5 relationship on values is from the fl opera- 
tion, We find 
This relationship is showo in the lattice diagram of Fig. 10.60, where the q's 
are intended lo suggest a11 pwssibk constants. Note that that figure is not 5 
on elements of Y; rather it is a relation on the set of values for ~ ( a )  
for indi- 
vidual variables a. The elements of V may be thought of as vectors of such 
values, one component for each variable, and the lattice diagram for V can be 
extrapdated from Fig. 10.60 if we remember that p 5 v holds if and only if 
p(a) 5 vial for all a; i-e.. if the vectors representing p and v have each corn- 
ponent related by 5 ,  
and the relationship is in the same direction in each 
mmpnen!. 
Thus, to say p 5 v is $to say that whenever p(a) is a constant r, v(a) i s  
either that constant or w d f ,  and whenever p(a) is rmndef, so is v(a). A care- 
ful examination of the various functions f that are associated with the dif- 
ferent kinds of definition statements enables us to verify that if p 5 Y, then 
f(p) 5 f (v). thus proving ( 10.15) and showing rnonotonicity. For example, 
if f is associated with assignment a : =b+c, only pla) and vCa) change, so we 
have to check that if t~ 5 v - i,e+, FIX) 
5 v ( x ) ,  for all x - then 

638 
CODE OPTIMIZATION 
Fig. 10.60. Latticc diagram for valucs 'uf variables. 
I/(P) 
l(a) 5 y (v)j(a) .I4 We must consider all possible values of p(b), p(c), 
u(b)+ and v(c), subject to the constraints that ~ ( b )  
5 v(bl and PIC) 5 vIc). 
For example, if 
then If ( p) a) = wnronst and If (v) l(al = rrnrlef. 
Since nuwonst 5 w&J, 
we have made the check in one caw. The other cases are left as an exercise 
for the reader. 
Now, we must check our claim that the constant computation framework is 
not distributive. For this part, kt J be the function associated with the assign- 
ment a:=b+c. and kt ~ ( b )  
= 2, p(c) = 3, v(b) = 3, and v(c) = 2, Let 
p = pAv. Then p(b) Avfb) = 2fl3 = numc.onsr. Similarly, p(c) A v(c) = 
nonconst. Equivalently, p(b) = PIC) 
= nvnconst. it follows that If(p)Ka) = 
nmrmst, since the sum of two nonconstant values is presumed to be noncon- 
stant, 
On the other hand, vIk)l(a) = 5, since given b = 2 and c = 3, the 
assignment a: =b+c sets a to 
5. 
Similarty, u(v)l(a) = 5. 
Thus, 
u(p)Af(v)#a) = 5. 
We now see that 
p(a) = If(pAv)l(a) 
# 
u(p)flS 
(v) ](a), so the dist~ibutivity condition i s  vidated. 
Intuitively, the reason we get a didtibutivity uioiation is that the constant 
computation framework is not powerful enough to remember all invariants. in 
particular, the fact that along paths who= effects on variables are described 
by either g or v ,  the equation b+c = 5 holds, even though neither b nor c 
are themselves constant. We could devise more complicated frameworks to 
avoid this particular problem, although the payoff from doing so is ~ o t  
clear, 
Fortunalely, monotonicit y h adequate for the iterative data-flow algorithm to 
"work," as we shalt next see. 
" We have lu bc carcful reading an c ~ p r c ~ i o n  
likc YI+L)~(~). 
11 says, YYC apply f to p to gct 
wme mapping f (p), which wc call p '  Thcn. wc apply p' Io a, a d  thc rcsult is onc of thc 
valucs in the diagram of Fig. 10.M. 

SEC 10.11 
A TOOL FOR DATA-FLOW ANALYSIS 689 
Let us imagine lhat a flow graph has associated with each of its nodes a 
transfer function, one of the functions in the set F. For each block B, kt jR 
be the transfer function for B. . 
Consider any path P = BO +B I + . , 
+ +Bk from the initial node Bo to 
some block Bk. We can define the rrumfer fundm for P to be the composi- 
tion of f~,,, 
f ~ , ,  
+ 
+ . , fs, , +  
Note that fs, 
is not part of the composition, 
reflecting the point of view that this path is taken to reach the beginning of 
blo& Elk+ not 'its end. 
We have assumed that the values in V represent information about data 
used by the program, and that the confluence operator A tells how that infor- 
mation is combined when paths converge, It abo makes sense to xee the t ~ p  
element as representing "no information," since a path carrying the top ele- 
ment yields to any other path, as far as what information i s  carried after con- 
fluence of the paths. Thus. if B is a block in the flow graph, the information 
entering 3 should be computable by considering every possible path from the 
initial node to B and seeing what happens along that path, starting out with no 
information. That is, for each path P from BU to 3, we compute fp(T) and 
take the meet of all the resulting elements. 
In principle. this meet wuld be aver an infinite number of different values, 
since there are an infinite numkr af different paths. In practice, it is often 
adequate 10 consider only acyclic paths, and even when it is not, as for the 
constant computation framework discussed above, there are usually other rea- 
sons we can find to make this infinite meet be finite for any particular flow 
graph. 
Formally, we define the meet-over-puchs sdution to a flow graph to be 
The mop mlution to a flow graph makes sense when we realize that as hr as 
information reaching block B is concerned, the flow graph may as well be the 
one suggested in Fig. 10.61, where the transfer function associated with each 
of the (possibly infinite number of) paths P 
P z ,  . . . , in the ortginal flow 
graph has been given an entirely separate path to B. In Fig. 10.61, the infor- 
mation reaching B is given by the meet over all paths, 
Conservative Srrlutim to Flow Problems 
When we try to solve the data-flow equations that come from an arbitrary 
framework, we may w may not be able to obtain the m p  solution easily. 
Fortunately, as with the concrete e~amples of data-flow frameworks in Sec- 
tions 10+5 and 10.6, there is a safe direction in which to err, and the iterative 
data-flow algorithm we bixussed in t h w  sections turns out always to provide 
us with a safe solution. We say a solution inlB I is a safe 
sohtim if 
in[B I 5 mup{B) for all blocks 8. 

Fig. 10.61. Graph showing the sei of ail possible paths to B. 
Despite what the reader might imagine, we did not pull this definition out 
of thin air. Recall that In any flow graph, the set of upprenr paths to a node 
(those that are paths in the flow graph) can k a proper subset of the red 
paths, those that ate taken on some execution of the program corresponding to 
this flow graph. In order that the result of the data-flow analysis be usable 
for whatever purpose we intend it, the data must still be safe if we mwlify the 
flow graph by deleting some paths, since we cannot in general distinguish real 
palhs from apparent paths that are not real. 
Suppose among [he infinite set of paths suggested in Fig. 10.61, x is the 
meet of fp( T) taken over all real paths P that are followed on some execution. 
+ 
Also, let y k the meet of f,(T) over all other paths P. Thus. mp(B) is xAy. 
Then the true answer to our data-flow problem at node B is x, but the mop 
solution is xAy. Recall that xAy 5 y, since (xAy)Ay = xAy. Thus, the mop 
solution is 5 the true solution. 
While we may prefer the "true" solution to the data-flow problem, we shall 
almost surely have no efficient way to tell exactly which paths are real and 
which are not, so we are forced to accept the mc~p solution as the closest feasi- 
ble solution, Thus, whatever use we make of the data-flow information must 
be consistent with the possibility that the mlution we dtain is 
the true 
solution. Once we accept that, we shwld also be able to accept a d u t h  that 
is 5 the mop (and therefore 5 the true solution). Such solutions are easier to 
obtain than the mop for those frameworks that are monotone but not ddistribu- 
tive. FOT distributive frameworks, like hose of Section W.6, the simple itera- 
tive algorithm computes the mop solution. 
The Itmalive Algwithm k r  General Frameworks 
There is an obvious generalization of Algorithm 10.2 that works for a large 
variety of frameworks. The iterative algorithm requires that the framework 
ke monotone, and it requires finiteness, in the sense that the meet over the 
infinite set of paths suggested in Fig. 10.61 is equivalent to r meet over a fin- 
ite subset. We shall give the algorithm and then dixuss the ways in which we 
could assure finitenes~. However, one common guarantee of finiteness i s  the 
one we have had all along: propagation along acyclic paths is sufficient. 

SEC 10.1 1 
A TOOL FOR DATA-FLOW ANALYSIS 691 
Algorithm 10.18. Iterative solution to general data-flow frameworks. 
p u t .  A data-flow graph, a set of "values" V, a set of functions F, a, meet 
operation A, and an assignment of a member of I: to each node of the flow 
graph. 
Ouiput. A vake inlS j in V for each node of the flow graph, 
Method. The algorithm Is given in Fig. 10.62. As with the familiar iterative 
data-flow algorithms, we compute in and out for each nude by successive 
approximations. We assume that J', IS the function in F associated with block 
8; that firnction plays the role of gen and kill in Section I0.6. 
~3 
A&* 10.62, Iterative algorithm for general frameworks. 
A Data-Mow Analysis T d  
We can now see the way the ideas of this section can be applied to a tool for 
data-flow analysis. ~l~orithrn 
10.18 depends for its working on the following 
subroutines. 
1. 
A routine to apply a given Js in F to a given value in V, This routine Is 
used in lines (2) and (6) of Fig. 10.62. 
2. 
A routine to apply the meet operator to two values in V; this routine is 
needed zero or more times in line ( 5 ) .  
3. 
A routine to tell whether two values are equal. This test is not made 
explicitly in Fig. 10.62, but it is Implicit in the test for change in any of 
the values of oat. 
We also need to have spccific data typc declarations for F and V, so w e  can 
pass arguments lo the routines mentioned above. The values of in and out in 
Fig. 10.62 are also of the type declared for V. Finally, we need a routine that 
will take the ordinary representation of the contents of a basic block, that is, a 
k t  of statements, and produce an element of F, the transfer function for that 
block. 

Example 10.48. For the reaching definitions framework, we might first build 
a table that Identified each statement of the given flow graph with a unique 
integer from I up to some maximum m+ Then, the type of V could be bit YCP 
tors of length m. F could lx represented by pairs of bit veaors of that size, 
i ,em, by the #en and kill sets. The routine 10 construct the 
and kill bit vec- 
tors, given the statements of a block and the table associating definition state- 
ments with bit-vector positions, is straightforward. as are the routines to com- 
pute meets (logical or of bit vectors). compare bit vectors for equality, and 
apply functions defined by a gm-kiFi pair to bit vectors. 
D 
The data-flow analysis tod is thus little more than an implementation of 
Fig. 10.62 with calls to the given subroutines whenever a meet, fundion appli- 
cation, or comparison is needed. The tool would supprt a fixed reprcsenta- 
tion of flow graphs, and thus be able to perform tasks likc finding all the 
predecessors of a node, finding the depth-first ordering of the flow graph, or 
applying to each block the routine that computes the function in F associated 
with that block. The advantage of using s u ~ h  a tool is that Ihc graph- 
manipulation and convergence-checking aspccts of Algorithm 10. I8 do not 
have to be rewritten for each data-flow analysis that we do. 
Propedes of Algorithm 10.18 
We should make clear the assumptions under which Algorithm 10.18 actually 
works and exactly what the algorithm converges to when it dues converge. 
First, if th: framework is monotone and converges, then we claim the result 
of thc algorithm is that inlBI 5 mop(3) for all blocks B. The intuitive reawn 
is that along any path P = Bo+ B1, . . . ,Bk from the initial node to B = B,, 
we can show by induction on i that the effect of the path from Bu to 3, is felt 
after at most i iterations of the while-Imp in Fig. 10.62. That is, if Pi is the 
pathBo, + . + ,Bi, then after i rounds, inlSil 5 ,tPi(T). Thus, when and if the 
algorithm converges, inlS] will be 5 fp{T) for every path P from Bo to B. 
Using the rule that if x 5 y and x 5 z,  hen x 5 yAz+ls we can show that 
h I B ]  5 mop(B). 
When the framework i s  distributive, we can show Algorithm 10.18 does in 
fact converge to the mop solution. The essential idea is to prove that at all 
times during the running of the algorithm, in131 and U U ~ ~ B ]  
are each equal to 
the meet of fp(T) for some set of paths P to the beginning and end of B ,  
respectively. However, we show. in the ncxt example that need not be the 
case when the framework is monotone but not distributive. 
Example 10.49. 
Let us exploit the example of nondistributivity of the 
" Thcrc is thc tc~hnicnlily that wc muhl in principtc show this rrlc not juM fix IW values. .V and ; 
(from which fdlaws the rulc hat if .r 5 v, for any finitc ict of ,v,','s thcn s 5 
but that ~ h c  
samc rulc holds for an infinile nun~bcr of .pi's. 
Howwcr, in pracricc, whcnevcr we p t  mnvcr- 
gncc of Algorithm !O. 18, wc shall find a firiite numkr of parha such that the mect ovcr all paths 
is equal lo thc moct uvcr this finitc set. 

!EC 10.11 
A TOOL FOR DATA-FLOW ANALY SlS 693 
constant cmmpuralion framework discussed in Example 10.47; the relevant 
flaw graph is shown in F i g  10.63. The mappings p and v corning out of Bz 
and B4 are the ones from Example 10.47. Mapping p, entering B 5 ,  is FAY, 
and o is the mapping coming out of B 5 ,  setting a to nonronst, even though 
every real path (and every apparent path), computes a = 5 after B5+ 
Fig. 10.63. Example of solution Im than mop solution. 
The problem, intuitively, is that Algorithm 10. IS, dealing with a nondistri- 
butive framework, behaves as if mme sequences of nodes that are not even 
apparent paths (paths in the flow graph), were real paths. Thus, in Fig. 
10.63, the algorithm behaves as if paths like Bn-B,+B4-,B5 
or 
Bo -Bs +B2 +BS were real paths, setting b and c to a combination of values 
that do not sum to five. 
a 
There are various ways that we could prove Algorithm 10.18 converges for a 
particular framework. Probably the most common ca,w 
where only acyclic 
paths are needed, i.e., we can show that the meet over acyclic paths is the 
same as the mop solution, over all paths. If that is the case, not only does [he 
algorithm converge, it will normally do sr, very rapidly, in two mare passes 
than the depth of the flow graph, as we discussed in Wion 10.10. 
On the other hand, frameworks like our constant computation example 
require more than acyclic paths be considered. For example, Fig. 10.64 shows 
a simple flow graph where we have to wnsider the path B I-+B2-B2-B3 
Cr, 
realize that x does not have a canstant value on entering B3. 
However, for constant computations, we can reason that Algorithm 10+18 
converges as follows. First of all, it is easy to show for an arbitrary monotone 
framework that inIB] and aurltt], for any black B, form a nonincreasing 
sequence, in the sense that the new value for one of these variables is always 
5 the old value. If we recall Fig* LO.60, the bnice diagram for the values of 
mappings applied to variables, we realize that for any variable, the value of 

Fig. 10.64. Flow graph requiring cyclic path be inchded in mop. 
h [ B  I or OM[B I can only drop twice, once from uniiuf to a constant, and once 
from that constant to noncunsr. 
Suppose there are n nodes and v variables. Then on every iteration of the 
while-Imp in Fig. 10.62, at least one variable must have its value drop in 
some out18 1, or the algorithm converges, and even infinite iteration of the 
while-loop will not change the values of the i d s  or our's. Thus, the number of 
iterations is limited to 2nv; if that number of changes occurs, then every vari- 
able must have reached nunconst at every block of the flow graph. 
Fixing Lhe Initialization 
I 
In some data-flow problems, there is a discrepancy between what Algorithm 
l0,l8 gives us as a wlution and what we intuitively want. Recall that for 
available expressions, 
is intersection, so T must be the set of all expres- 
sions. Since Algorithm I0+18 initially assumes h [ B j  is T for each block B, 
including the initial node, the mop solution produced by Algorithm 10.18 is 
actually the set of expressions hat, assuming they are available at the initial 
nude (which they are not), would be available on entrance to blmk B. 
The difference, of course, i s  thal there might be paths from the initial node 
to B along which an expression x*y is neither generated nor killed. Algo- 
rithm 10.18 would say x+y is available, when in fact il i s  not, because no 
variable along that path can be found to hold its value. The fixup is simple. 
We can either modify Algorithm 10.18 so that far the available expression 
framewurk, inlBol is set and held equal to the empty set, or we can modify 
the flow graph by introducing a dummy initial node, a predecessor of ihe real 
initial node, that kills every expression. 
10,12 ESTIMATION OF TYPES 
We now cume to a data-fl0.w problem that is more challenging than the frarne- 
works of the previous section. Various languages, ranging from APL to 
SETL to the many dialects of Lisp, do not require that the lypes of variables 
be declared, and even permit the same variable to hold values of different 
types at different times. Serious attempts to compile such Ianguages into effi- 
cient code have used data-flow analysis to infer the types of variables, since 

code to, say, add two integers, is far more efficient than a call to a general 
routine that can add two objects of a variety of types (e.g., integer, real, uec- 
for). 
Our first guess might be that computing types of variables is something like 
computing reaching definirions. We can associate a set of possible types with 
each variable at each point. The confluence operator is union on sets of types, 
since if variabie x has set S I of possible types on me path and set Sl on 
another, then x has any of the types in $ 1  U S2 after the confluence of the 
paths. As control passes through a statement, we may be able to make m e  
inferences abut the types of variables based on the operators in the state- 
ment, the possible types of their oprands, and the types they pruduce as 
results. Example 6.6, which dealt with an operator that could multiply both 
integers and complex numhrs, was an example of this sort of inference. 
Unfortunately, there are at ica~t two problems with this approach. 
I+ The set of possible typzs for a variable may be infinite. 
2. Type determination usually requires both forward and backward ,r)ropaga- 
tion of information to obtain precise estimates of possible types. Thus, 
even the framework of Section 10.i 1 is not general enough to do justice 
to the problem. 
* 
Befort: considering pint ( I), kt us examine some of the kinds of inferences 
about types that can be made in familiar languages. 
Example 10,50, Consider the statements 
SUpposo at first we do not know anythi?'g a b u t  the types of the variables a, 
i, j, and k+ However, k t  us suppw that the array accessing operator I 
] 
requires an integer argument. By examining the first statement we may infer 
that j is an integer at that point, and a is an array of elements of some type. 
Then, the second statement tells us that i is an integer. 
Now, we may propagate inferences backward. If i was computed to k an 
integer in the first statement, then the lype of expression a t i ]  must be 
integer, which means that a must be an array of integers. We can then rea- 
son forward again to discover that the value assigned to k by the second 
statement must also be an integer, Notice it is impxsible to discover that the 
elements of a are integers by reasoning only forward or only backward. 
o 
Desliug with Infinite Type Sds 
There arc numerous examples of pathological cases where thc set of possible 
types for a variable really is infinite. For example, SETL allows a statement 
like 

6 9  CODE OPTIMIZATION 
!m 
m12 
to be executed inside a Imp. If we start out knowing only that x could k an 
integer, then after we consider one iteration of the loop, we realize x could 
be either an integer or a set of integers. After considering a second iteration 
we find that x could also be a set of sets of Integers, and so on, 
A similar problem could occut in a typeless version of a mnventional 
language like C, where the statement 
with the initial possibility that x ir; an integer leads as to discover that x can 
have any iype of the form 
pointer to pointer to . . . pointer to integer 
Thc traditional way to deal with such problems is to reduce the set of p s s i -  
ble types to a finite number. The general idea is to group the infinite number 
of possible types into a finite number of classes, generally keeping the simpler 
types by themwlvcs, and grouping the most complicated, and hopefully the 
rarest, types into iarge classes. When we do so+ we have co exercise judge- 
ment as to how we make inferences about the interaction between types and 
operalors. The following example suggesls what can be done. 
Example 10.51. Lct us continue the example of Chapter 6, where we used the 
oprator + as a type constructor for funct;tms+ Here, our set of types will 
Include the basic type int, and all types of the form T -. u, rcprewnting the 
type of a function with domain type T and range type u, where 7 and rr arc 
t y p  in our set. Thus, our set of types is infinite, including such types as 
To reduce this set ro a finire number of classes, we shall restrict a type expres- 
sion to have only csne function type constructor -, by replacing subapressions 
in a type expression containing at least woe occurrence of -. by the name ~ U R C .  
Thus, there are five different types: 
inr 
h r  + inf 
inr -. ,fum 
f ~ n t  
+ inr 
fuoc - furrc 
1 
< 
We shall represent sets of types as bit vectors of iength five, with the positions 
corresponding to the five types in the order listed above. Thus, OIL1 1 
represents the type for any function applicatiun, i.e., for anything but i ~ t ,  
Note that this is in-a sense the type of func., since a func may no1 be am 
integer. 
The basic assignment statement for our model is 
Knowing the pssibk types of f and y. we can determine the possible types 

SEC 10.12 
E.STIMATIC)N OF TYPES 697 
of x by looking up the type in the table of Fig. 10.45. If f can be of any 
type in the set S ,  and y of any type in set S z .  we take tach pair T in S, and u 
in S z  and Imk up the entry in the row for T and the cdutnn for tr, which 
call ~(u). 
We then take the union of the results of all these lookups to get 
set of possible types for x. 
we 
the 
Fig. 10.&+ Thc valuc of ~ l t r ) +  
For example, if T - Int +Jww and u = int. then T ( I T )  = 01 I l l .  That is, 
the result of applying a mapping of type inr - fuw ta an inr is a junr, which 
means a mapping of any of the four types besides in:. We cannot tell which 
becau.~ our smearing of an infinite number of types into five classes precludes 
our knowing. 
For a second example, let T be as More and u = int - hrr. Then 
~ t w )  = 00000 because the domain type of r ia definitely unequal tv the type 
u, and therefore the mapping is inapplicable. 
n 
A Simpk Type System 
+ 
To illustrate the ideas behind our type-in ferencc a\gorithrns, we int raduce a 
simple type system and language based on Example 10.51. The types are the 
five illustrated in that example. The statements of our language are of three 
kinds. 
1 . read x. A value of x is read from the input, and presumably, nothing 
is known aholrt its type. 
2, 
x ; = f (y ). The value of x is x;ct to be that obtained by applying func- 
tian f to value y, What is known abut thc type of x after the assign- 
ment is summarized in Fig. 10.65. 
3+ use x as 7 .  When we go through such a statement, we may assume 
that the program is correct, and ihereforz. the type of x can only be .r 
both beforeand after the statement, The value and type of x are not 
affected by the statement. 
We infer types by performing a data-flow analysis on a flow graph of 
pro- 
gram consisting of statements of rhese three types, For simplicity, wc assume 

698 
CODE OPTIMIZATION 
SEC 10,12 
that all blocks consist of a single statement. The values of in and ou for 
blocks are mappings from variables to sets of the five types from Example 
10.51. 
Initially, each in and out maps every variable to the set of all five types. As 
we propagate information, we reduce the set of types associated with certain 
variables at certain points, until at some time we cannot reduce any of these 
sets any further. The resulting sets will be assumed to indicate the possible 
types of each variable at each pint. That assumption is conservative, since a 
type is eliminated only if we can prove (given that the program i s  wtrect) that 
the t ypc is impossible. Normally, we expect to take advantage of the fact that 
some types are impssible, not that they are possible. so "too large" is the 
safe diredon for errors. 
We use two schemes to modify the in's and our's: r "forwsrd" scheme and a 
"backward" scheme, The forward scheme uses the statement in a Mock B and 
the value of in[S ] to restrict m t [ B  1," 
and the backward scheme does the 
opposite. In each scheme, the confluence operator is "variable-wise union," 
in the sense that the confluence of two mappings a and (3 is that mapping y 
such that for all variables x, 
The Forward Scheme 
Let us suppose we have a block B with in [Bl the mapping p and 0~rlS1 
the 
mapping v .  The forward scheme lets us restrict v .  The rules for restricting u 
depend upon what instruction is found in block B, naturally. 
I + 
If the statement is read x, then any type could be read. If we already 
know something about the typc of x after the read, we must not forget il 
during this forward pass, so we simply do not change vjx) on the forward 
pass. For all other variables y. we set 
2. 
Now supposc thc statement is use x as T. After this statement, 7 i s  
the only p~ssible type for x. If we already know that type 7 is; impssi- 
ble for x, then there is no possible type for x after the statement. These 
observatiuns can be summarized by: 
'* It is worth noting tha~ in ~raditional forward data-flow whcmcs, we did nor restrict out, but 
rathcr computcd i~ anew from irr each time. We muid do this kcause in's and mds always 
chadgd ia m c  dirccti4m, cirher always growing or always shrinking. Howwcr. in a problem like 
typc inicrcncc, whcrc wc altcrnatcly pcrform forward wd backward passcs. we may haw a situa- 
tion where the hackward pass has left nut mu& smdlcr than we can juslify by applying the for- 
ward rukr to irt, Thus, wc must nn~t accidcntdly rake oiti on the forward m, only lo haw ir 
!uwcrd apain (but prhaps nut aau far) 011 the backward pass, A similar remark applies to the 
backward pas.: we nrus4 restrict in. not recompute it. 

3. 
Now, consider the case that the statement is x := f ( y 1. The only ps- 
sible types for x after the statement are those that 
i) 
arc possible according to the present value of u, and 
ii) are the resuit of applying a mapping of some type T to type w, and T 
and o are types that f and y, respectively, could have before the 
statement is executed. 
Formally, 
t4x) := vIx) n {p I p = ~ ( u ) ,  
T is in p(f), and u is in p(y)) 
We may also make some inferences about the types of f and y, since on 
the assumption of program mrrectness, f cannot have a rype that doesn't 
apply to some type of y, and y cannot have a type that cannot serve as 
the argument type for some possible type o f f .  That is, i f f  # x then 
v(f) := ~ ( f )  
i7  in p(f) Ifor =mew in +(y),~lo) 
+ 0) 
if y # x then 
v(y) : = u(y) fl {(r in blyl 1 for some T in plf), ~ ( m )  + 0) 
Now let us msidcr how, in a backward pass. we can restria p bps& 
on what 
v tells us and what the statement tells us. 
I. If the statement is read x, it ix easy to see that no new inferences about 
impossible types before the staternen1 can be made, so p(x) does not 
change. However, for all y # x, we may propagate information back- 
ward by setting p(y) := p(y) fl v(y). 
2. 
If we have the staternent use x as T, then we mn make the same sort 
of inference we made in the forward dimtian; x can only have type T 
before the statement, and any other variable's types are those that are 
believed possible both before and after the statement. That is: 
PW := CL~XI n 
@(Y) := ~(ul 
fl ~IY) 
for Y f x 
3. 
As before, the mmt complex case is a statement of the form x : = f I y I .  
To begin, nothing new can k inferred about x before the statement, 
unless x happens to be one of f or y. Thus, pfx) is not changed except 
by the following rules concerning f and y, Next, note that as in the for- 
ward ruks, we may make inferences from the fact that the types of f and 
y must be compatible before the statement. However, if f # x, we may 

7m 
CODE OPTIMIZATION 
SEC 10.12 
a h  restrict ~ ( f )  
to the types in v(f), and an analogous Matcrnent holds 
abut y. On the other hand, if f = x, then the types of f after the 
statement are unrelated to the types of f before, so no such restriction is 
permitted. Again, an analogous statement holds if y = x. It is useful 
to define a special mapping, just For f and y, to reflecl this decision. 
Thus, we define; 
Now, we can restrict f and y to those types that are compatible with the 
other's eiet of types. A t  rhe same time, we can restrict the types of f and 
y bas& on the fact that [hey must' not only be compatible, but must yield 
a type that v says x may have. Thus, we define: 
Before proceeding to the type determination algorithm, let us recall from 
ow discussion of reaching definitions in Section 10.5 that if we start with the 
false assumption that some definition d is available at some point in a Imp, 
we may ermneously propagate that fact amund the Imp, leaving us with a set 
of reaching definitions that is larger than necesmry. A similar problem can 
occur in type determination, where the assumption that a variabie can have a 
certain type "proves" itself as we go around a loop. Therefore, we shall 
introduce a 33rd value, in addition to the 32 sets of types from Example 
10.51, that a mapping p may assign to a variable, thc value undeS, This use 
of utrdef is similar to'its use in the constant propagaiion framework of the pre- 
vious section. 
During confluence, the value un&f yields to any other value, i.e., i t  acts 
like the type W. On the other hand, when intersecting sets of types, e.g., 
computing p(x) fl 
u(x), the value &def also yields to any other set of types; 
that is, it funchwi like the type 1 I 1 1 I. Thus, for example, when we read a 
value of a variable x, the fact that the "type" of K was thought to k undrf 
after the read is overruled, and the type of x becomes I I ! I I. 
hpur. A flow graph whose blocks are single stalemznts of the three types 
(read, assignment, and use-as) mentioned above, 
Ouipur. A set of types for each variable at each point. The s%t is conserva- 
t ivc, in the sense that any real computation must lead to a type in the set 
M p i h d .  We compute a mapping irtlB) and a mapping ulrflB] for each block 
B. Each mapping sends the program's variables to sets of types in the type 
system introduced in Example 10.5 1. Initially, all mappings send each vari- 
able to undef+ 

SEC 10.12 
ESI1MATION OF TYPES 701 
We then make alternate forward and backward passes rhrough the flow 
graph, until consecutive forward and backward passes both fail to make any 
changes. The forward pass is performed by: 
for each block B in depth-first order do begin 
in[B]:= 
U 
u ~ r I P j ;  
pcrd. P dB 
OM I3 I ;= function of in IS 1 and uwt [B I as defined a h v e  
end 
The backward pass is: 
for each block B in reverse depth-first order do btgh 
outlBI:= 
U 
in[$]; 
srorr. S 4 B  
in [B I := function of in [B 1 and nrrt!B ] as defined above 
eRd 
t~ 
E w p k  14.52, Consider the simple straight-line program shown in Fig. 
10.66. We are interested in four mappings, which we designate pl through 
pq. Each F, is both outlBi] and in[Bi+]]. Technically, B 1  is not s u p p d  to 
have two statements, because we haw assumed blwks are single statements in 
this section. However, we are not concerned with what happens before the 
end of B , , because all variables can have any type there. 
read a 
"1 
readb 
I 
B ,  use a as int 
I 
I 
Fig. 10.66. Exampk program. 
It turns out that we need five passes before convergene occurs and another 
two to detect that convergence has occurred. These passes are summarized in 
Fig. 10.67{a)-(cf. 
The first pass is forward. When considering 8 2 ,  we dis- 
cover that b cannot be an integer, because it is used as a mapping. We also 
discover that a is used as an integer in B 3 .  and therefore can only be mapped 
to inl in (L) and 
These obwvatiuns are summarized in Fig. 10.67(a). 

702 
CODE OFTIMLZATION 
(a) Fo~wnkt, 
(b) BACKWARD 
Fig, 10.67. Simulation of Algorithm 10.19 on flow graph of Fig. 10.66. 
The second pass, shown in Fig. 10.67(b). is backward, On this pass, when 
considering B Z ,  we know that a must be an integer when b i s  applied to it. 
Thus, the type of b could only be int - int or im -Jmc. On the third pass, 
which is forward, this restriction on the type of b propagates all the way down 
the flow graph, as shown in Fig 10.67(c). 
The fourth pass, backward, is shown in Fig. 10.67(6). Here, the fact that c 
is an argument of b in B j  tells us that c can only k an integer. Alw, when 
we consider BZ, wc find that the result of bIa 1 can only be of the type of c, 
which is im. This fact rules out the possibility that b is of t y p  int -.fit. 
Finally, in Fig. 10.67(e), we see how on the fifth pass, forward, these facts 
abur b and c propagate. On further paws, no new inferences can be made. 
In this case, we have reduced the sets of possible types to single types for each 
variable at each point; a and c arc integers, and b is a mapping from integers 
to integers. In general, we might be left with several possible types for a vari- 
able at a point. 
o 

SEC. 10.13 
SYMBOLIC DEBUGGING OF OPTIMIZED CODE 703 
lk13 SYMBOLIC DEBUGGING OF OPTIMIZED CODE 
A s y h i i c  debugger is a system that allows us to lmk at a program's data 
while rhat program is running. The debugger is usually called when a pro- 
gram error, such as an overflow, occurs or when certain statements, indicated 
by the programmer in the source code, are reached. Once invoked, the sym- 
blic debugger allows the programmer to examine, and possibly change, any 
of the variables that are currently accessible to the running program. 
In order for a user command like "show me the current value of a" to be 
intelligible to the debugger, it must have available certain informat ion. 
There must be a way to associate an identifier like a with the location it 
represents. Thus, the portion of the symbol table that assigns to each 
variable a location, e.g., a place in a global data area or in an activation 
record for some procedure, must be recorded by the compiler and 
preserved for the debugger to use. For example, this information might 
be encoded into the load modu Ie for the program. 
There must be scope information, so we can disambiguate references to 
an identifier that is declared more than once, and SO that we can tell, 
given we are in some procedure p, what other procedures' data is amessi- 
ble and how do we find that data on the stack or other run-time struc- 
ture, Again, this information must k taken from the symbol table of the 
compiler and preserved for future use by the debugger. 
We must know where we are in the program when the debugger is 
invoked. This information is embedded by the compiler in the call to the 
debugger when the compiler handles a user -declared invocahn of the 
debugger. I t  is also obtained from the exception handler when a run-time 
error causes the debugger to be called. 
In order that program-location information, mentioned in I3), make sense 
to the user, there must k a table associaring each machine language 
statement with the source statement from which it came. This table can 
be prepared by the compiler as it generates code. 
While the design of a symbulic debugger i s  interesting in its own right. we 
shall consider only the difficulties that occur when trying to write a symbulic 
debugger for an optimizing compiler, At first glance, it might seem that there 
is no need to debug an optimized program, In the normal development cycle, 
as the user debugs a program, a fast. nonoptimizing compiler is used unt tl the 
user is satisfied that the source program is correct. Only then is an optimizing 
compiler used . 
Unfortunately, a program may run correctly with a nunoptimizing compiler, 
and then fail, with the same input data, when compiled by thc optimizing 
compiler, For example, a bug may extst in the rsprimizing cornpilcr,-or, by 
reordering operat ions, the optimizing compiler may intrrsducc an ~verftow or 
underflow. 
Also, even "nonoptimizing" compilers may do some simple 

transformations, like elimination of I m l  common subexpressions or reorder- 
ing of code within a basic blwk, that make a big difference in how hard it is 
to design a symbdic debugger. Thus, we need to consider what algorithms 
and data structures to use in a symbolic debugger for an optimizing compiler 
that transforms basic biocks in arbitrary ways. 
Deducing Values of Variables in Basic Blocks 
For simplicity, assume that both the source and object c d e  are sequences of 
intermediate statements. Treating the source as intermediate code presents no 
problems, since the latter is more general than the former. For example. the 
user may only be allowed to put breaks (calls to the debugger) between source 
statements, but here we allow breaks after any intermediate statement. Treat- 
ing the object code as intermediate code is queslionable only if the optimizer 
breaks a single intermediate statement into several machine statements that 
get separated. For example, for some reason, we may compile the two inter- 
mediare statements 
into c d e  where the two additions are performed in different registers and 
interleaved. If that is the case, we can treat the Ioads and stores of registers 
as if the registers were temporaries in intermediate code, for example: 
Several problems occur when interacting with the user about rt block, where 
the user thinks the source blwk is being executed, but in fact, an optimized 
version of that block i s  running: 
Suppose we are executiqg the program that results from "optimizing" 
some basic block of the source program, and while executing statement 
a: =b+c, an overflow occurs. We must tell the user that an error has 
occurred in one of the source statements. Since b+c may be a common 
subexpression appearing in two or more of the source statements, to 
which statement do we charge the errorm? 
A harder problem occurs if the user of the debugger wants 10 see the 
"current" value of some variable d. In the optimized prugram, d may 
last have been assigned at some statement s, But in the source program, 
s may come after the statement at which the debugger was invoked, so 
the value of d that is available to the debugger is not the one that the 
user thinks is the "current" value of d according to the listing of the 

SEC. 10.13 
SYMBOLIC DEBUGGING OF OPTIMIZED CODE 
705 
source code. 
Similarly, s may precede the statemen1 invoking the 
debugger, but in the source there is another assignmenr ta d between 
them, so the value of d available to the debugger is out of date. Is it pos- 
sible to make the correct value of d available t~ the user? For exampk, 
could it be the value of some other variable in the optimized version, or 
could it be computed from the values of other variables'? 
3. 
Finally, if the user places a break after some statement of the source 
cude, when should control be given to the debugger during the running of 
the optimized code? 
One solution might be to run the unoptirnized version of the block along 
with the optimized version, to make the correct value of each variable avail- 
able at ail times. We reject this "solution," because the subtlest of bugs, 
especially compiler-in troduced bugs, may disappear when the inst ructtuns that 
mused the problem are separated from one another in time or space. 
The solution we adopt is to provide enough Information about each block to 
the debugger that i t  can at least answer the question: is it possible to provide 
the correct value of variable a, and if so, how? The structure we use to 
embody this information is the dag of the basic block, annotated with informa- 
tion about which variables hold the value corresponding to a node in the dag, 
at what times in both the source end oplimized programs. The notation 
attached to a node means that the value represented by that node is stored jn 
variable a from the beginning of statement i through the portion of statement 
j just before its assignment occurs. I f  j = m ,  then a holds this value until the 
end of the block. 
- 
Example 10.53. In Fig. 10.68Ia) we see a basic block of source cude, and in 
Fig. 10.48( b) is a pssible "optimized" version of that code. Figure 10.69 
shows the dag for either block, with indications of the ranges in which the 
variables hold those values in both the source and optimized code. Primes are 
used to indicate that a statement range is in the optimized code. For example, 
the node labeled + ii; the value of c in the source code from the beginning of 
statement (2) until just before the assignment in statement (3). It is also the 
value of a in the source from the beginning of statemen1 (3) until the end. In 
addition, the same node is the value of d in the optimized code from state- 
ment (2') until the end. 
0 
Now we can answer the first question raised above. Suppose that an error, 
such as overflow, occurs while executing statement j' of the optimized code. 
Since the same value would k computed by any source statement that mm- 
putes the same dag node as statement j J ,  it makes sense to report to the user 
that the error occurred at the first mure statement computing this nude. 
Thus, in Example 10,53, i f  the error occurred at statement I J ,  2', 3', or 4'+ 
we would report that it wurrcd at statement 1 ,  5, 3, and 6, respectively. N6 

706 CODE OPTlMIZATION 
SEC. 10.13 
I 
c := a+b 
(1') 
d := atb 
( 2 )  
d := c 
(2') 
t := bte 
(3) 
c : = c-e 
3 
a : = d-e 
(4) a := d-e 
(4') 
b := d l t  
( 5 )  
b := b*e 
5
'
 c : = a 
(6) b := d/b 
Ia) 
(b) 
Fig. 10.68. 
Source and optimized code. 
Fig. 10.69. Annotated dag. 
error can Occur at statement 5 ' ,  because no value is computed. We defer 
details of how corresponding statements are computed until Example 10.54, 
below, 
We can also answer the second question. Suppose we are at statement j' of 
the optimized code, and the user is told that control is at statement i of the 
source, where an error occurred. If the user asks to see the value of a vari- 
able x, we must find a variable y (frequently, but not always, y is x) such 
that the value of x at statement i of the source is the same dag node as y at 
statement j' in the optimized code+ We inspect the dag to see what n d e  
represents the value of x at i, and we may read off from that node ail the 
variables of the object program that ever have that value, to see if one of 
them holds this value at j'. 
If so, we are done; if not, we may still wmpute the value of x at i from 
other variables at j'. Let n be the node for x at time i .  Then we may con- 
sider the children of n ,  say m and p, to see if both these nodes represent the 
value of some variable at time j. If, say, there is a variable for m, but none 
for p, we may consider the children of p, recursively. Ultimately, we either 
find a way to compute the value of x at time i or conclude that there is no 
such way. If we find a way to compute Ihe values of m and p, then we 

SEC. 10.13 
SYMWLICDEBUGGINGOFOPTlM~ZEDCODE 707 
mmpute them and apply the operator at ra to compute the value of x at time 
- 17 
c. 
Example 10.54. Suppose that when executing the code of Pig. 10.68(b), an 
error occurs at statement 2'. The statement was computing the node labeled * 
in Fig. 10.69, and the first soutce statement computing that value i s  statement 
5, Thus we report an error in statemenl 5. 
In Fig. 10.70. 
we have tabulated the dag node corresponding to each vari- 
able in bo~h the source and optimized code, at the beginning of statements 5 
and 2', respectively; nodes are indicated by their label, either an operator 
symbol or an initial value symbol like A(,. We also indicate how to compute 
the value at time 5 from the values of variables at time 2'. For example. if 
the user asks for the value of a, the value of lhe node labeled - is given. No 
variable has that value at time 2', but fortunately, there are variables, d and 
c, that hdd the value of each of the children of n d e  - at time 2', so we may 
print the value of a by computing the value of d-e. 
o 
Ail 
Bu 
undefined 
+ 
Eo 
undefined 
d-e 
b 
d-e 
d 
e 
Fig. 10.70, Valucs of variables at times 2' and 5 .  
Now let us answer the third question: how to handle debugger calls inserted 
by the user. [n a sense the answer is trivial; if the user asks fw 
a break after 
statement i in the source program, we can halt execution at the beginning of 
the blmk. If the user wants to see the value of some variable x after stale- 
ment i, we consult the annotated bag to see what node represents the desired 
valuc of x, and we cornpu t t  thal value from the initial values of variables for 
. that block. 
On the other hand. we can leave less work for the debugger, and also avoid 
11 A subtiely occurs if ihc computation of thc value of nvdc n t i w c s  anuthcr crmr. WC must then 
rewrt lo the user rhat lhc crror actually ~ c u r r c d  
earlicr. at thc firs1 wurcc Satcmcnt cornpuling 
thc value of H. 

708 
CODE OPTlMlZATlON 
SEC. 10.13 
some situations where attempts to compute a value lead to errors that must be 
announced to the user, if we postpone the call to the debugger to as late a 
time as possible. It Is easy to compute the last statement j' in the optimized 
program, such that we call the debugger after j' and pretend to the user that 
the call was made after statement i of the source program, To find j', let S be 
the set of nodes of the dag that currespnd to the value of some variable of 
the source program imrncdiaiely after statement i. We may be asked by the 
user to compute any value in S. Thus, we may break after statement j' of the 
optimized code only if for each node n in $ there is some k' > j' such that 
some variable is associated with node n at time k' in the optimized code. For 
then we know that the value of n is either available immediately after state- 
ment j', or ir will be computed sometime after statement j', In the former 
case, it i s  trivial to compute the value of n if we break after j', whik in the 
latter case, we know that the values available after j' are sufficient to compute 
n somehow. 
Example 10.55, Consider the source and optimized code of Fig: t0+68 again, 
and suppose chat the user inserts a break after statement (3) of the murcc 
code. To find the set S, inspect the dag of Fig. 10.69, and see which nodes 
have source-program variables attached to them at time 4. These nodes are 
the ones labeled A u ,  B o ,  Eo, +, and - in that figure. 
Now, we look at the dag again to find the largest j' such that each of the 
nodes in S has some variable of the optimized c d e  attached to it at a time 
strictly greater than jJ. The nodes labeled +, - , and Eo present no problem, 
since their values are carried by variables d, a, and e, respectively. at time 
='. Nodes A. and o0 do limit the value of j'. and the earliest of rbese to lose 
its value is A,, whose value is destroyed by statement 3'. Thus, j' = 2' is the 
largest possible value of j'; that is, if the user asks for a break after source 
statement 3, we can give him thc break after statement 2'. 
a 
The reader should be aware of a subtlety in Example 10.55, for which no 
really good sdution exists, If we run the optimized code through statement 2' 
before we call the debugger, an error in the computation of b e  at statement 
2' (for example. an underflow) may cause the debugger to be invoked before 
the intended call. However, since the compulation mrrespnding to statement 
2' doesn't occur ~ n t i l  
statement 5 in the source program, we are going to tell 
the user that the error occurred at statement 5. It will be somewhat mysteri- 
ous to the user how we gor to statement 5 without calhg the debugger at 
statement 3. Probably the best solution to this problem is not to allow j' to be 
$0 large that there is a statement kt of the optimized code, with k' 5 j ' ,  such 
that the source code does not compute the value computed by k' until after the 
statement i at which the break was placed. 
Effcts of G b b l  Optimization 
When our compiler per forms global optimizations. there are harder problems 

SEC, 10.13 
SY NBOLIC DEBUGGIFQG OF OPTIMIZED CODE f 
for the symbolic debugger to solve, and frequently, no way to find the cwrect 
value of a variable at a point exists, Two important transformations that do 
not cause significant trouble art induction-variable elimination and global 
common sukxpression elimination; in each ca.w the prchlern can be localized 
to a few blocks and treated in the manner discussed above. 
Induction-Variable Elimination 
If we eliminate a source program induction variable i in favor of some 
member of i ' s  family, say t, then there is some linear function relating i 
and 
t. Moreover, if we follow the methods of Section 10.7, the optimized code 
will change t in exactly those blmks in which i is changed, so the linear rela- 
tionship between i and t always holds. Thus, after taking into account reord- 
ering of statements within a block that assigns to t (and in the source, assigns 
to i), we can provide the user with rhe "current" value of i by a linear 
transformat ion on t. 
We haw ro be careful if i is not defined'prior to the loop, since t surely 
will Be assigned before loop entry, and we might thus provide a value for i at 
a p i n t  in the program where the user expects i to tie undefined. For* 
tunately, it is usual for a wurce*program variable that is an induction variable 
to be initialized p~ior to the loop, and only compiler-generated variables 
(whose values the user cannot ask for) will be undefined on entry to the loop. 
If that is not the case h r  some induction variable i, then we have a problem 
similar to that for code motion, to be discussed Mow. 
Global Cornmom Subexpression Elimination 
If we perform a global common subexpression elimination for expression a+b, 
we also affect a limited number of blocks in simple ways. If t is the variable 
used to hold the value of a+b, then at certain blocks computing a+b we might 
replace 
This sort of change can be handled by the methods for basic blocks already 
discussed. 
In other bbcks, a use such as d : =a+b may be replaced by a use d : =t. To 
handle this situation by previous methods, we have only to note in the dag for 
this block that the value of t remains for all time at the value of the node for 
a+b {which will appear in the dag for the source code, but would not other- 
wise appear in the dag for the optimized d e ) .  

C d e  Motion 
Other transformations are not so easy to handle. For example, suppose we 
move a statement 
out of a Imp because it is loop-invariant. If we call the debugger within the 
Imp, we do not know whether statement s would have been executed yet in 
(he source program, and thus we cannot know whether the current value of a 
is the one the user would see in the source program. 
One possibility i s  to insert into the optimized code a new variable that, 
within the Imp, indicates whether a has been assigned in the loop, (which can 
only occur at the old position of statement s). However. this strategy may not 
always be sukable, becaux for absolute reliability, only the reid code, not a 
version of the code constructed specially for debugging purposes, should be 
used. 
There is a common special case where we can do better, however. Suppose 
that the block B containing statement s in the source program divides the Imp 
into two sets of nodes: those that dominate B and those dominated by it. 
Moreover, assume that all predecessors of the header are dominated by 3, as 
suggested in Fig. 10.71. Then the First time through the blocks that dominate 
B ,  we may assume a has not been assigned in the loop, while the first time 
through the blocks that 3 dominates, a has been assigned at statement s. Of 
course, the second and subsequent times through the loop, a has surely k c n  
assigned at s. 
Fig. lO,?l+ A block that divides a loop into two pans. 

CHAPTER 10 
EXERCISES 71 I 
Cf the call to the debugggr is caused by a run-time errof, there is a g
d
 
chance that the error is exposed the first time through the loop. [f that is the 
case, then we have only to know whether we are above B or below it in Fig. 
10.71, Then we know whether the value of a is that defined at 5, in which 
case we can just print the value of a produced by the optimized code, or 
whelher the value of a is that which a had on entry to the Loop in the murce 
version of the program. In the latter case, there is little we can do, excepr If 
1 ,  
the debugger has reaching definition information available to i t ,  for both 
the source and optimized program, 
2. 
there is a unique definition of a that reaches the header in the suurce 
program, and 
3. 
that definition is also the unique definition of some variable x that 
reaches the p i n t  where the debugger was calledA 
If a11 these conditions hold. then we can print the value of x and say it ik- the 
value of a. 
The reader should be aware that this line of reasoning dms nut hold if the 
debugger was called by a user-inscrted break p i n t ,  for then we cannot have 
any reason to suspect that we are going around the lwp for the first time. 
However, if user-inserted breaks are being used, then it might also be reason- 
able to insert code into the optimized program to help the debugger tell 
whether this is the first time ahrough the bop, a sulution that we mentioned 
earlier but suggested might not be suitable because it tampered with the 
optimized code. 
EXERCISES 
10.1 Consider the matrix multiplication program in Fig. 10.72. 
begin 
for i := fl to n do 
for j : =  7 to n d o  
c[i,j] : =  0; 
for i : =  1 t o n  do 
for j := 1 t o n d o  
for k := 1 to n do 
c [ i + j ]  := c[i, j l  + a [ i , k ]  + b[k, j] 
end 
Fi. 10.72. Matrix multiplicntion program. 
a) Assuming a, b. and c are allocated static storage and there are 
four bytes per word in a byte-addressed memory, produce three- 
address statements for the program in Fig. 10.72. 

71 2 CODE OPTIMIZATION 
CHAPTER 10 
b) Generate target-machine d
t
 
from the three-address statements. 
C) Construct a flow graph from the three-address slaternents, 
d) Eliminate the common subexpressions from each basic blmk . 
e) Find the loops in rhe flow graph. 
f) Move the Imp-invariant camputations out of the loops. 
g) Find the induction variables of each Imp and eliminate them 
where possible, 
h) Generate target-machine code from the flow graph in {g). Com- 
pare the c d e  with that produced in (b), 
10.2 Cornpule reaching definitions and ud-chains for the original f k ~ w  
graph from Exercise LO. l (c) and the final flow graph from 10.1Ig). 
10.3 The program of Fig. 10+73 counts the primes from 2 to n using the 
sieve method on a suitabjy large array. 
begin 
read n; 
for i := 2 to n do 
a [ i l  := true; 
f* initialize */ 
count : = 0; 
for i := 2 to n *+ . 5  do 
if a! ij then 
/* 1 is a prima */ 
begin 
count : = count + -I ; 
for j := 2 * i to n by i do 
a [ j ]  := false 
l* j i s  divisible by i */ 
end ; 
print count 
end 
Fig. 10.73. Program to calcdatc prirncs. 
a) Translate the program of Fig. 10.73 into ~hree-address statements 
assuming a i s  allocated static storage, 
b) Generate target-machine d
e
 
from the three-address statements, 
C )  Construct a flow graph from the three-address statements. 
d) Show the dominator tree for h e  flow graph in (a}. 
e) For the flow graph in (c), indicate the back edges and their natural 
loops + 
fS Move the invariant computations out of loops using Algorithm 
10.7, 
gl Eliminate induction variables wherever possible. 

CHAPTER 10 
EXERCISES 
713 
h) Propagate out copy statements wherever possible. 
i) Is Imp jamming poasible? If so, do it. 
j) On the assumption that n will always be even, unroll inner loops 
once each. What new optimizations are now possible'? 
10.4 Repeat Exercise 10.3 on the assumption that a i s  allocated dynamic 
storage, with ptr a pointer to the first wwd of a. 
10.5 For the flow graph of Fig, 10.74 compute: 
a) ud- and du-chains, 
b) live variables at the end of each block, 
c) available expressions. 
I 
4 
(3) 
c := a+b 
Bl 
(4) 
c~ := c-a 
(8) 
b := a+b 
B 5  
(9) 
e := c-a 
A .  . 
(10) 
a := b+d 
(7) 
e := e+l 
( 1 1 )  
b := a-d 
& 
Fig. 10.74. Fbw graph. 
10.6 Is any constant folding possible in Fig+ 10.74:' If so. do it. 
10+7 Are there any common subelrpressiuns in Fig. 10.74? If so, eliminate 
them. 
10-8 An expression e is said to be very busy at pint p if no matter what 
path is taken from p ,  the expression c wili be evaluated before any of 
its o p a n d s  are defined. Give a data-flow algorithm in the style of 
Section 10.6 to find all very busy expressions. What cun.fluenm 
operator do you use, and does propagation run forward or backward? 
Apply your algorithm to the flow graph in Fig. 10.74. 

f 14 CODE OPTIMIZATION 
CHAPTER 10 
*10.9 If expression s is very busy at point p we can hoist e by computing it 
at p and preserving ils value for subsequent use. (Note: This optimi- 
zation does not usually save time, but may save space.) Give an algo- 
rithm to hoist very busy expressions. 
I0.10 Are there any expressions that may k hoisted in Fig. 10,74? If so. 
hoist them. 
10.11 Where possible. propagate out any copy steps introduced in the 
modifications of Exercises 10.6, IO. 7, and 10. [a. 
10.12 An cxrended husk block is a sequence of blocks B I , . . . ,Bk such that 
for I 5 i < k, B, is the only predecessor of Bi, k ,  and 8 ,  does not 
have a unique predecessor. Find the extended basic block ending at 
each node in 
a) Fig. 10.39, 
b) the flow graph constructed in Exercise 10. I (cl, 
c) Fig. 10.74. 
*10.13 Give an algorithm that runs in time O(n) on an n-node flow graph to 
find the extended basic block ending at each node. 
10.14 We can do some interblock code optimization without any data-flow 
analysis by treating each extended basic blmk as If it were a basic. 
block. Give algorithms to do the following optimizations within an 
extended basic block, In each case, indicate what effect on other 
extended basic blocks a change wi(hin one exrended basic block can 
have. 
a) Common su bexpressim dimination. 
b) Constanl folding. 
c) Copy propagat ion. 
10.15 For the flowgraph of Exercise IO. l(c): 
a) Find a sequence of T I  and T 2  reductions for the graph. 
b) Find the sequence of interval graphs. 
C) What is the limit flow graph? Is the flow graph reducible? 
10+16 Repeat Exercise lo.! 5 for the flow graph d Fig. 10.74. 
**10+17 Show that the following conditions are equivalent (they are alternative 
definitions of "reducible flow graph "1. 
a) The limit of TI-TI reduction is a single node, 
b) The limit of interval analysis is a single node. 
C) Thc flow graph can have its edges divided into two classes; one 
forms an acyclic graph and the other, the "back" edges, consists of 
edges whose heads dominate their tails. 
d) The flow graph has no subgraph of the form sh~w'n in Fig. 10.75. 
Here, no is the initial node. and no, a, b, and c are ajl disrincr, 
with the exception that a = n o  is possible. The arrows represent 

CHAPTER 10 
Fig, 1175. Forbidden subgraph for reducible flow graphs. 
node-disjoin t paths (except for the endpoints, of course). 
10,18 Give algorithms to compute (a) available expressions and (b) live 
variables for the language with pointers discussed in Section 10.8. Be 
sure to make conservative assumptions about g m ,  kiii, me, and def in 
(b)+ 
10.19 Give an algorithm to compute reaching dcfin itions interprocedurally 
using the model of Section 10.8. Again, be sure you make conserva- 
tive approximations to the truth, 
10.20 Sup- 
parameters are passed by value instead of by reference. Can 
two namcs be aliases of one anaher? What if copy-restore linkage is 
used? 
10.21 What i s  the depth of the flow graph in Exercise 10. llc)? 
**10.22 Prove that the dcpth of a reducible flow graph is never less than the 
number of times interval analysis must be performed to produce a sin- 
gle node. 
*10.23 Generalize the structure-based data-flow analysis algorithm of Section 
10,8 to a general data-flow framework in the sense of Section 10. I I. 
What assumptions abuut F and A do you have to make to ensure that 
your algorithm works? 
*I024 An interesting and powerful data-flow analysis framework is obtained 
by imagining that the "values" to be propagated are a11 the possible 
partit ions of expressions so that two expressions are in the same class 
only if they have the same value. To avoid having to list all of the 
infinity of pssiWe expressions, we can represent such values by list- 
ing only the minimal expressions that are equivalent to,, some other 
expression. For example, if we execute the statements 
then we have the following minimal equivalences: A = B, and 

c = A+D. From these follow other equivalences, such as C = B+D 
and A + E  = B+E, but there is no need to list these explicitly. 
a) What is the appropriate meet, or confluence, operator for this 
framework? 
b) Give a data structure to represent values and an algorithm to 
implement the meet operator. 
C) What are the appropriate functions to associate with statements? 
Explain the effect that the function associated with assignments 
such as A: =B+C should have on a partition. 
d) I s  this framework distributive? Monotone? 
10.25 How would you use the data gathered by the framework of Exercise 
10.24 to perform 
a) common subexpression elimination? 
b) copy propagation? 
c) constant folding? 
r10.26 Give formal proofs of the following about the relation 5 on lattices. 
a) u 5 band a 5 c implies a 5 ( b  fl c). 
b) a 5 ( b  A C) implies a 5 b. 
C) a 2 b and b 5 c implies u 5 c. 
d) a 5 B and & 5 u implies a = b. 
-10.27 
Show that the following condition is necessary and sufficient for the 
iterative data-flow algorithm, with depth-first ordering, to converge 
within 2 plus the depth passes. For all functions Jand g and value a: 
'10.28 Show that the reaching definitions and available expressions frame- 
works satisfy the condition of Exercise 10.27, Note: In fact, these 
frameworks converge in I plus the depth passes. 
**10.29 Is the condition of Exercise 10.27 implied by monotonicity? By distri- 
but ivity? Vice versa'? 
10,M We see in Fig. 10.76 two basic blocks, the first "initiai" code and the 
second an optimized version. 
a) Construcl the bags for the blwks of Fig. 10.76Ia) and (b). Verify 
that, on the assumption only J is live on e ~ i t ,  
these two blocks are 
equivalent . 
b) Annotate the dag with the times s t  which each variable's value is 
known at each node. 
C )  Indicate, for an error occurring at each of statements (1') through 
(4'1, at which statement of Fig. 10,761a) we should say an error 
occurs. 

CHAPTER 10 
EXERCISES 717 
I) 
E ;= A+B 
2) 
F ; = E-C 
3) 
G := F*D 
4) 
H := A+B 
5 )  
I : =  I-c 
6) 
J :=I& 
(a) h!TlAL 
1') 
E :a A+B 
2') 
E := E-C 
3 
F := EID 
4') 
J := E+F 
Fig, 10.76. Initial and optimized code. 
d) For each of the errors in part (c). indicate for which variables of 
Fig, 1 O.%(a) it is possible to compute a vaiue, and how we do so. 
e) Suppose we allowed ourselves to use algebraically valid laws like 
"if 
n + b = c then a = c - b," 
Would your answer to (d) 
change? 
10.31 Generalize Example 10.14 to ~ C C O U ~  
for an arbitrary set of break 
statements. Also. generalize it to allow continue statements. which do 
not break the inner loop but proceed directly to the next iteration of 
the loop. Hint: Use the techniques developed in Seaion LO. I0 for 
reducible flow graphs. 
10.32 Show that in Algorithm 10.3 the in and our sets of definitions never 
decrease. Similarly, show that in Algorithm 10.4 these sets of expres- 
sions never increase, 
10.33 Generalize Algorithm 10.9 for the elimination of induction variables 
to the case where multiplicative constants can be negative. 
10.34 Generalize the algorithm for determining what pointers can p i n t  to, 
from Section 10.8, to the case where pointers are permitted to point to 
other pointers. 
W.35 When estimating each of the idlowing sets, tell whether too-large or 
too-small estimates are conservative, Explain your answr in terms of 
the intended use of the information. 
a) Available expressions. 
b) Variables changed by a procedure. 
C) Variables not changed by a procedure, 
d) induction variables belonging to a given family. 
e) Copy statements reaching a given point. 
*10.36 Refine Algorithm 10.12 to compute the aliases of a given variable at a 
gtven p i n t .  
*10.37 Modify Algorithm 10.12 for the cases that parameters are passed 

a) by value, 
12) by copy-restore. 
*10.38 Prove that Algorithm 10.13 converges to a superset (not necessarily 
proper) of the truly changed variables. 
*10.39 Generalize Algortthm 10+13 to determine changed variables in the 
case that procedure-valued variables are permitted. 
*10,40 Prove that in each interval graph each nude represents a region of the 
original flow graph. 
10+41 Prove that Algorithm 10. I6 correctly computes the set of dominators 
of each node. 
*10,42 
Modify Algorithm 10.17 (struclure-based reaching definitions) to 
compute reaching definitions for designated small regions only, 
without requiting that the whole flow g~aph be present in memory at 
once, Be sure your result is conservative. Adap your algorithm to 
available expressions. Which is more likely to provide useful infor- 
mation? 
*10,43 
In Section 10.10 we proposed a speedup to Algorithm 10.17 based on 
combining a T i  with a Tz reduction. Prove the correctness of the 
mod i f m t  ion. 
10.44 Generalize the iterarive method of Section 10+11 to backward-flowing 
problems. 
@*10.45 Prove that when Algorithm 10.18 converges, the rwulting solution is 
5 the mop solution by showing that for every path P of length i, then 
after i iterations, in [B,] 5 fp[T]. 
10.46 [n Fig. 10.77 is a flow graph of a program in the hypothetical 
language intrduced in Section 10.12. Find the k s t  estimate of the 
types of each variable using Algorithm 10,19. 
BiBLIOGRAPHIC NOTES 
Additional information about code optimization may k found in Cmke and 
Schwartz [1970], A h l  and Bell [1972], Schaefer [1973], l-fecht [1977j, and 
Muchnick and Jones [ 1981 1. Allen 119751 provides a bibliography on program 
optimization. 
Many optimizing compilers are described ln the literature, Ershov 119661 
discusses an early compiler that used sophisticated optimization techniques. 
Lowry and Medlock 119691 and Scarborough and Kolsky [1980] de~ail the con- 
struction of optimizing compilers for Fortran. Busarn and Englund 11%9] and 
Mcicalf 19821 describe additional techniques far Fortran optimization. Wulf 
et al. 19751 discuss the design of an influential optimizing compiler for Bliss. 
Allen et a[. (19801 describe a system that was built to experiment with 

CHAPTER 10 
BlBLlOGRAPHlC NOTES 719 
use a as i n t  
B4 
4 
Fig. 10.77. Example program for r g p  inferencc, 
program optimizations. Cmke and Markstein ) 19801 report on the effective- 
ness of various optimizations for a PLII-like language. A nklam. Cutler, 
Heinen and MacLaren 119821 describe the implementation of optimizing 
transformations that were used in cornpikrs for PL/\ and C. Auslandcr and 
Hopkins 119821 report on a compiler for a variant of PL/l that uses a simple 
algorithm to produce low-level intermediate code that is then improved by glo- 
bal optimizing transformations. Freudenberger, Schwartz, and Sharrr Il983j 
describe experiences with an optimizer for SETL. Chow 11983j reports on 
experiments with a portable, machine-independent, optimizer for Pascal. 
Powell [ 19841 describes a portable, machine-independent, optirn izing compiler 
for Mdula-2. 
The systematic study of data-flow analysis techniques begins with Allen 
11 9701 and Cocke [ 19701, since published together as Allen and Cocke 119761, 
although various data-flow analysis methods were in use before then. 
Syntaxdirected data-flow analysis, as introduced in Section 103, has been 
used inBliss (Wuff et al. [i975], Geschke 119721), SIMPL (Zelkowitz and Bail 
1197411, and Modula-2 (Powell [ 19841). Additional discussions of this family 
of algorithms appears in Htcht and Schaffer (19751, Hecht (19771, and Rosa 
119771. 
The iterative approach to data-flow analysis discussed in Section 10.6 has 
been traced to Vyssotsky (see Vyssotsky and Wegner [1963jl, who used the 
method in a 1962 Fortran compiler. The use of depth-first ordering to 

720 CODE OPTIMlZATION 
CHAPTER 10 
improve the efficiency is from Hecht and Ullman [ I  9751. 
The interval-analysis approach to data-flow analysis was pioneered by Cocke 
[L970]. Kennedy [I9711 originates the use of interval analysis for backward 
flow problems, like live variables. There is reason to believe, following Ken- 
nedy 11 9761, that interval-based methods are somewhat more efficient than 
iterative ones, if the language k i n g  optimized tends to prduce few, if any, 
nonreducible flow graphs. The variant used here, based on T ,  and T 2 ,  is 
frpm Ullman [1973]. A somewhat faster version that takes advantage of the 
fact that most regions have a single exit was given in Graham and Wegman 
[ 1 9761- 
The original definition of a reducible flow graph, one that bemrnes a single 
node under iterated interval analysis, is from Allen [2970]. Equivalent char- 
acterizations are found in Hecht and Ullrnan 11972, 19741, Kasyanov [1973], 
and Tarjan [1974b]+ Node splitting for nonreducible flow graphs is from 
Cocke and Miller [ 19691. 
The idea that structured flow of control is mdeied by reducible flow graphs 
is expressed in Kosaraju 119741, Kasami, Peterson. and Tokura [1973], and 
Cherniavsky, Henderson, and Keohane [l976]. Baker [ I  9771 describes their 
use in a program-structuring algorithm. 
I 
The lattice;-theoretic approach to iterative data-flow analysis began with Kil- 
dall [1973]. Tennenbaum [I9741 and Wegbreit I19751 are similar formula- 
tions, The efficient version of KiMall's algorithm, in which depth-first order 
is used, is from Kam and Ullrnan [1976]. 
While Kildall assumed the distributhi~~ 
condition (which his frameworks 
such as the constant computation framework of Example 10.42 do not actually 
satisfy), the adequacy of monotonicity was perceived in a number of papers 
giving data4ow algorithms, such as Tennenbaurn I 19741, Schwartz 11 975a, b], 
Graham and Wegman 119761, Jones and Muchnick [ 19761, Kam and Ullman 
119771, and Causot and Couwt [1977], 
Since different algorithms require differing assumptions about the data, the 
theory of what properties are needed for certain algorithms was developed by 
Kam and Ullman 119771, Rosen 119801, and Tarjan [198i]. 
Another direction that followed from Kildall's paper was improving the 
algorithms for dealing with the particular data-flow problems (e+g., Example 
10,421 he iatrduced. One key idea i s  that the lattice elements need no1 be 
treated as atomic, but we can exploit the fact thai they are really mappings 
from variables to values. See Reif and Lewis [.I9771 and Wegman and Zadeck 
I1 9851. Also, Kou 119771 exploits the idea for more conventional problems. 
Kennedy [I9811 i s  a survey of data-flow analysis techniques, and Cousot 
[I98 11 surveys lattice-theoretic ideas. 
Gear [1%5] introduced the basic loop optimizations of cadc mdWn and a 
limited form of induction-variable elimination. Allen [ I9691 is a fundamental 
paper on loop optimization; Allen and Cocke [ 19721 and Waite [1976b] are 
more extensive surveys of techniques in the area. Morel and Renvoise [ I  9793 
describe an algorithm that simultaneously eliminates redundant and i nva~iant 

CHAPTER 10 
computations from loops. 
The discussion of induction-variable elimination in Section 10.7 is based on 
Lowry and Medlwk 11969). See Allen, Cocke, and Kennedy [I9811 for more 
powerful algorithms. 
An algorithm for some of the Imp problems not discussed in detail here, 
such as finding whether there is a path from a to b that does not go through c, 
can be solved by an efficient algorithm of Wegrnan [ 19831. 
The use of dominators, both for loop discovery and for performing c d e  
motion was pioneered by Lowry and ~ e d i o f k  11 %9], although they attribute 
the general idea to Prosser 1 1959). Algorithm 10.16 *for finding dominators 
was discovered independently by Purdom and Moore 119721 and Aha and Ull- 
man [1973a1. The use of depth-first ordering to speed the algorithm is from 
Hecht and Ullman 119751, while the asymptotically most efficient way to do 
the job is from Tarjan [1974a]. Lengauer and Tarjan [I9791 describe an effi- 
cient algorithm for finding dominators that is suitable for pact ical use . 
The study of aliasing and interprocedural analysis begins with S p i h a n  
[ 1871) and Allen [1974]. Same more powerful methods than those of Section 
10.8 have been developed. [n general. they deal with the alias relation at 
each p i n t  of the program to avoid some of the impossible alias pairs that our 
simple algorithm "discovers. " These works include Barth 119781, Banning 
[ 19791, and Wcihl [ 19801. See also Ryder [ 19793 on the construction of calling 
graphs. 
An issue similar to interprmdural analysis, the effect of exceptions on the 
data-flow analysis of programs, is discussed by Hennessy 1 I981 1, 
The fundamental paper of determination of types by data-flow analysis is 
Tennenbaum 119741, on which our discussion of Section 10.12 is based. 
Kaplan and Ullman 119801 give a more powerful algorithm for type detection. 
The discussion of symbolic debugging of optimized code in %aim 10.13 is 
from Hennessy 119821- 
There have k e n  a number of papers that attempt to evaluate the improve- 
ment due to various optimizations. The value of an optimization appears to 
be highly dependent on the language being compiled. The reader may wish to 
consult the dassic study of Fortran optimization in Knuth [I971 b1, or the 
papers by Gajewska [1975], Palm 119751, Cocke and Kennedy 1 IW6j, Cwke 
and Markstein 119801, Chow [1983], Chow and Hennessy 11984], and Powell 
1 19841. 
Another topic in optimization that we have not covered here is the optimiza- 
tian of "very high-level" languages, such as the set-theoretic language SETL, 
where we are really changing the underlying algorithms and data structures. 
One central optimization in this area i s  generalized induction-variable elimina- 
tion, as in Earley [1975b], Fong and Ullman j 19761, Paige and Schwertz 
119771, and F m g  [L979j. 
Another key step in optimizalion for very high-level languages is the selec- 
tion of data structures; this topic i s  covered in Schwartz 1!975a, b], Low and 
Rovner [1976], and Manberg, Schwartt, and Sharir (1% 11.. 

We also have not touched on issues iin incremental code optimization, where 
small modificatjons to the program do not require a complete reoptimization. 
Ryder [I9831 discusses incremental data-flow analysis. while Pollock and S f f a  
1 19RSi attempt to do insremcntal optimization of basic blocks, 
Finally, we should mention some of the many oiher ways that data-flow 
analysis techniques have been used. Backhouse 119841 uses it on the transi- 
tion graphs associated with parsers to perform error recovery, 
Harrison 
119771 and Suzuki and lshihaea 119771 discuss ils use in compile-time array- 
bounds checking. 
One of the most significant uses of data-flow analysis outside code optimiza- 
tion is in the area of static (mrnptle-time) checking for program errors. Fos- 
dick and Osterweil 119761 is a fundamental paper, while Osterweil [IYSl], 
Adrion. Bronstad, and Cherniavsky 19821, and Freudenberger [ t 9843 give 
some more recent developments, 

CHAPTER 
Want to Write 
a Compiler? 
Having seen I he principles, techniques, and c u d s  of compiler design, suppse 
we want to write a compiler. With some advance planning the irnplementa- 
tion can prmeed more quickly and smmthly than otherwise. This brief 
chapter raises some implementation issues that .arise in compiler construction. 
Much of the discussion is focussed around writing wmpilers using the UNIX 
operating system and i t s  tools. 
I I .  1 PLANNING A COMPILER 
A new compiler may be for a new source language, or produce new target 
code, w both. Using the framework espoused in this book, we ultimately 
obtain a design for a compiler that consists of a set of modules. Several 
diverse factors impact the design and implementation of these modules. 
Source Language Issues 
The "size" uf a language affects the size and number of the rndules. 
Although there is no precise definition for the size of a language, it is 
apparent that a compiler for a language like Ada or PL/I is bigger and harder 
to implement than a compiler for a little language like Ratfor {a "rational" 
Fortran preprocessor, Kern ighan 1 1975 1) or EQN (a language for typeset( ing 
mathematics). 
Another important factor i s  the extent. to which the source language wi!l 
change during the course of compiler construction. Although the sour- 
language spcification may Imk immutable, few languages remain the same 
for the life of a compiler. Even a mature language evolves, albeit slowly, 
Fortran, for example, has changed considerably from the language it was in 
1957; the looping, Hollerith, and conditional statements in Fortran 77 are 
quice different from those in the original language. Rosler 119841 repoits on 
the evolution of C. 
A new, experimental language, on the other hand, may undergo dramatic 
change as tt is implemented. One way to create a new language is to evolve a 
compikr for a working prototype of the language into one that satisfies the 
needs of a certain group of users. Many of the "little" languages initiatly 

7 24 
WANT TO WRITE A COMPILER? 
SEC. 11.1 
developed on the UNIX system like AWK and EQN were crzated this way. 
Consequently, a compiler writer might anticipate at least some amount of 
change in the definition of the source language over the lifetime of s compiler. 
Modular design and use of tools can help the compiler writer cope with this 
change. For example, using generators to implement the lexical analyzer and 
parser of a compiler allows the compiler writer to accomrnadate syntactic 
changes in the language definition more readily than when the lexical analyzer 
and parser are written directly in code. 
Target Language Issues 
The nature and limitations of the target language and run-time environment 
have to be carefully considered, for they too have a strong influence on the 
compiler design and on what code generation strategies it should use. If the 
target language is new, the compiler writer is well advised to make sure that it 
is correct and that its timing sequences are well understood. A new machine 
or a new assembler may have bugs that a compiler is likely to uncover. Bugs 
in the target language can greatly aggravate the task of debugging the com- 
piler itself. 
A successful source language is likely to be implemented on several target 
machines, If a- language persists, compilers for the language will need to gen- 
erate c d e  for several generalions of target machines. Fdrther evolution in 
machine hardware seems certain, so retargetable compilers are likely to have 
an edge. Consequently, the design of the intermediate language is imprtant, 
as is confining machine-specific details to a small number of modules. 
Performance Criteria 
There art several aspects to compiler performance: compiler speed. d
e
 
qual- 
ity, error diagnostics, portability, and maintainability. The tradeoffs between 
thme criteria are not so clear cut, and the compiler specification may leave 
many of these parameters unspecified. For example, is compilation speed of 
greater importance than the speed of the target code'.' How important are 
good error messages and error recovery'? 
Compiler speed can be achieved by reducing the number of modules and 
passes as much as possible, perhaps to the point of generating machine 
language directly in one pass. However, this approach may not produce a 
compiler that generates high quality target code, nor one that is particularly 
easy to maintain. 
There are two aspects to portability: retargetability and rehostability. A 
retargetable compiler is one that can be modified easily to generate code for a 
new target language. A rehostable compiler is one that can be moved easily 
to run on a new machine. A portable compiler may not be as efficient as a 
compiler designed for a specific machine, because the single-machine compi kr 
can safely make specific assumptions about the target machine that a portable 
compiler cannot. 

SEC. 11.2 
APPROACHES TO COMPILER DEVELOPMENT 725 
11.2 APPROACHES TO COMPILER DEVELOPMENT 
There are several general approaches that a compiler writer can adopt to 
implement a compiler. The simplest is to retarget or rehost an existing com- 
L 
pikr. If there i s  no suitabte existing compiler, the compiler writer might 
adopt the organization of a known compiler for s similar language and imple- 
ment the corresponding components, using component-generation tools or 
implementing them by hand. It is relatively rare that a completely new com- 
piler organization is required. 
No matter what approach is adopted, compiler writing is an exercise in 
software engineering. Lessons from other saftware efforts ( see for example 
Brooks [ 19751) can be applied to enhance the reliability and maintainability of 
the finai product.' A design that readily acwrnmdates change will allow the 
compiler to evolve with the language. The use of compiler-building tools can 
be a significant help in this regard. 
Bootstrapping 
A compiler is a complex enough program that we would like to write it in a 
friendlier language than assembty language. 
In the UNIX programming 
environment, compilers are usually written in C+ Even C compilers; are writ- 
ten in C. Using the facilities offered by a language to compile itself i s  the 
essence of boorstrapping, Here we shall Imk at the use of bootstrapping to 
create compilers and to move them from one machine to another by modifying 
the back end. The basic ideas of bootstrapping have been known since the 
mid 1950*s (Strong et al. 11958]), 
Bootstrapping may raise the question, "How was the first compiler m m -  
piled?" which sounds like, "What came first, the chicken or the egg?" but is 
easier to answer. For an answer we consider how Lisp became a program- 
ming language. McCarthy [ 1% 1 \ notes that in late I958 Lisp was used as a 
notation for writing functions; they were then hand-translated into assembly 
language and run. The implementation of an interpreter for Lisp occurred 
untxpktedly. McCarthy wanted to show that Lisp was a notation for describ- 
ing functions "much neater than Turing machines or the general recursive 
definitions uved in recursive function theory," so he wrote s function 
rvalIe, a I in Lisp that tuok a Lisp expression e as an argument, 3. R .  Russell 
noticed that e v d  could serve as an interpreter for Lisp, hand-cuded it, and 
thus created a programming language with an interpreter. As mentioned in 
Section I. 1, rather than generating target c d e ,  an interpreter actually per- 
forms the operations of the source program. 
For bootstrapping purposes, a mmpiicr i s  characterized by three languages: 
the source language S that it compiles, the target language T that i t  generates 
code for, and the implementation language T thal it is written in. We 
represent the three languages using the following diagram, called a T-diugrum, 
because of its shape (Bratman [1%1]). 

726 
WANT TO WRITE A COMPILER'! 
Within text, we abbreviate the above T-diagram as SIT. The three languages 
S, 1, and T may all be quite different. Fur example, a compiler may run on 
one machine and produce target code for another machine. Such a compiler is 
often called a r-wss-rwmpikr. 
Suppose we write a cross-compiler for a new language L in implementation 
language S to generate code for machine N; that is, we crcate L s N .  If an 
existing compiler for S runs: on machine M and generates code for M. it is 
characterized by $M M. If L SN is run through SM M, we gel a compiler 
L M  Fi, that is, a compiler from L to N that runs on M, This process is illus- 
trated in Fig. I I .  1 by putting together the T-diagrams for these compilers, 
Fig* 1 1.1. Compiling a compilcr 
When T-diagrams are put together as in Fig, I I. I ,  note that the implemcn- 
tation language S of the compiler L s N  must be the same as the source 
language of the existing compiler SM M and that the target languuge M of the 
existing compiler must be that same as the irnplcmentation language of the 
translated form L M N. A trio of T-diagrams such as Fig. 11.1 can be thought 
of as an equation 
Example 11.1. The first version of the EQN compiler (see Section 12. I )  had 
C as the implementation language and generated commands for the text for- 
matter TROFF. As shown in the fullowsiing diagram, a cross-compiler for 
EQN, running on a PDP-I I. was obtained by running EQN cTROEF through 
the C compiler C 1 I 1 I on the PDP- I I .  
EQN 
TROFF 
EQN 
TROFF 
C 
C 
I I 
I I 
7 

SEC. 11.2 
APPROACHES TO COMPILER DEVELOPMENT 
727 
One form of bootstrapping builds up a compiler for larger and larger sub- 
sets of a language, 
Suppose a new language L is to be irnplemcntcd on 
machine M, As a first step we might write ii smdl compiler that translates a 
subset S of L into the target code for M: that is, a compiler SM M. We then 
use the subset S to write a compiler L s M  for L+ When L s M  is run through 
S M M. we obtain an implementation of L, namely, L M M .  Neliac was one of 
the first languages ro be implemented in its own language i Huskey , Hahead, 
and McArthur [19601). 
Wirth [I971 
1 notes that Pascal was first implemented by wr~ting a compiler 
in Pascal itself. The mmpi1er was then translated "by hand" into an available 
low-level language without any attempt at optimization. The compiler was for 
a subset 'Y >M per cent)" of Pascal; several bootstrapping stages later ii com- 
piler for all of Pascal was obtained. Lecarme and Peyrolle-Thomas 119781 
summarize methods that have been used to bootstrap Pascal compilers. 
For the advantages of bootstrapping to be realized fully. a compiler has to 
be written in the language it compiles. Suppose we write a compiler L L N  for 
language L in L to gcnerate code for machine N. Development takes place on 
a machine M, where an existing compilcr L M M for L runs and generates 
c d e  for M. By first compiling L L N with L M M, we obtain n cmss-compiler 
L M N that runs on M, but produces k d c  for N: 
The compiler L L N can be compiled a second time. this time using the gcn- 
erated cross-corn piler : 
The result of the sccond compilation is a compiler L N N  that runs on N and 
generates code for N+ The are a number of useful applications of this two- 
step prrxessb so we shall write it as in Fig. 1 1.2. 
Example 11.2. This example is motivatcd by the dcvelop~ncnt d the Fortran 
H cornpikr (see Section 12.4). "The compilcr was itself written in Fortran 
and bootstrapped three times. The first time was to convert from running on 
the IBM 7094 to System/360 - 
an arduous proccbure. The second rime was 
to optimize itself, which reduced the size of the cumpi1er from about 550K to 
about 4lOK bytes" (Lowry and Medlock [L%9J). 

728 
WANT TO WRITE A COMPILER'! 
Big. 11.2. k m s r  rapphg u compikr. 
Using bootstrappin y techniques, an optimizing compiler can optimize itself. 
Suppose all development is done on machine M. We have SS M, a good 
optimizing compiler for a language S written in S, and we want SM MM, 
a good 
optimizing mrnpikr for S written in M. 
We can create S M$M$, a quick-and-dirty compiler for $ on M chat not 
only generates poor code, but also takes a long time to do so. (M$ 
indicates a 
poor implementation in M. S M$ M$ is a poor implementation of a mrnpiler 
that generates poor code.) However, we can use the indifferent compiler 
S M$ M$ to obtain a good compiler for S in two steps: 
First, the optimizing cumpiler SSM is translated by the quick-and-dirty com- 
piler to produce S M$ M, a poor implementation of the optimizing compiler, 
but one {hat does produce good code. The good optimizing compiler S M M is 
obtained by recompiling SS M through S ~4 M .  
Example 11.3. Arnmenn 1 198 I ] describes how a alean imphneniation d Pas- 
cal was obtained by a process similar ts that of Example 11 -2. Revisions to 
Pascal led to a fresh compiler being written in I972 for the CDC 6000 series 
machines, 
Cn the following diagram. 0 represents "old" Pascal and P 
represents the revised language. 

SEC. 11.3 
THE COMPILER-DEVELOPMENT ENVLRONMENT 729 
A compiler for revised Pascal was written in old Pascal and translated into 
Pm$6000. As in Example 11.2, the symbol $ marks a source of ineffi- 
ciency. The old compiler did not generate sufficiently efficient code. "There- 
fore, the compiler speed of lP 
was rather moderate and its storage 
requirements quite high (Ammaon 11981 I)." Revisions to Pascal were small 
enough that the compiler P04OW could be hand-rranslated w i ~ h  lirtk effm 
into P p
W
 
and run through the inefficient compiler P m $ 6 0 0 0  to obtain a 
clean implementat ion.. 
o 
11.3 THE COMPELER-DEVELOPMENT ENVIRONMENT 
tn a real sense, a compiler is just a program. The environment in which this 
program is developed can affect how quickly and reliably the compiler is 
implemented. The language in which the compiler is implemented is equally 
as important. Although curnpilers have been written in languages like For- 
tran, a clear choice for most compiler writers is a systern~rienteb language 
like C. 
If the source language itself is a new systems-oriented language, 'then it 
makes good sense to write the compiler in its own language. Using the 
bootstrapping techniques discussed in the previous section, compiling the corn- 
piler helps $bug the compiler. 
The software-constr uction tools in the programming environment can 
greatly facilitate the creation of an efficient compiler. In writing a compiler, 
it is customary to partition the overall program into modules, where each 
module may be processed in quite differenl ways. A program that manages 
the prwessing of these modules is an indispensable aid to the compiler writer. 
The UNlX system contains a command called make (Feldman 1 IW9aJ) that 
manages and maintains the modules making up a computer program; mk 
keeps track of the relationships between the modules of the program, and 
issues only those commands needed lo make the modules consistent after 
changes are made. 
Exampk 11.4. The command mrrke reads the specification of what tasks need 
to Ix done from a file called makef ile. In Section 2+9, we constructed a 
translator by compiling sewn files with a C compiler, each depending on a 
global header file g1obal.h. To show how the task of putting the compiler 
together can be done by make, suppose we call the resulting compiler trans. 
The makef i l e  specification might look like: 
OBJS = 1cxer.o passer.0 eaitttr.o symbol.o\ 
i n i t  .o error. Q main. o 

730 
W A N T  TO WRlTE A COMPILER'? 
SEC. 11.3 
The equal sign on the first line makes OBJS on the left stand for the w e n  
object files on rhc righr. (Long lines can tw split by placing 3 backslash a t  the 
end of the continued porttonJ The colon on the second line says t r a n s  on 
its left depends on all the files in OBJS, Such a dependency line can tie €01- 
lowed by rt command to "make" the file to [he left of the colon. The rhird 
line therefore says that the target program trans is created by linking the 
object files lexer. o, parser .o, . . . , main.0. However, rnuka knows 
that it must first create thc object filcs; it automatically does this hy looking 
for the corresponding source files lexer . c, parser. c, . . . , main+c, 
and compiling each with thc C compiler to create the corresponding object 
files. The last line of makef ile says that all seven object files depend on 
the global header file global. h. 
The translator is created by just typing the command mukc, which causes 
the following commands to he issued; 
cc -c 1exer.c 
cc -c parser. c 
cc -c cmittcr.~ 
cc -c symb01.c 
cc -c i n i t .  r: 
cc -C err0r.c 
cc -c main-c 
cc 1exer.o parser.0 emitter.0 symkal.o\ 
init.0 trror+o main.0 -o trans 
Subsequently, a compilation will be redone only if a dependent source file i s  
changcd after thc last compilation. Kcrnighan and Pike 119841 wntains exam- 
ples of the use of m u k  to facilitate the construction of a compiler. 
n 
A profiler i s  another useful compiler-writing t d .  Once the compiler is 
written, a profiler can be uwd to determine where the compiler spending its 
time as it mmpiles a source program. Identification and modification of the 
hot spois in rhe compiler a n  speed up the compiler by a factor of two or 
three. 
[n addition to software-devcloprnent tools. a number of tools have been 
developed specifically for the compiler-dzwlopment process. In Section 3.5, 
we described rhc generator Lex that can be used to automatically produce a 
lexical analyzer from a regular-expression specification of a lexical analyzer; in 
Section 4-9, we described a generator Yaw that can be used to automaticalty 
producc an LR parscr from a gramml;rtical description of the syntax of the 
language. The make command described above will nu tomat ically invoke Lex 
and Yacc wherever needed. In addition to lexical-analyzer generators and 
parser generators, attribute-grammar generators and code-generator generator> 
have k e n  crea~ed to help build compiler components. Many of these 
compiler-constructir~n t d s  have the desirable property that they will catch 
bugs in the specification of thc compiler, 
There has been some debate on the efficiency and convenience of program 

SEC. 11.4 
TEsTlNG AND MAINTENANCE 73! 
generators in compiler construction (Waite and Carter I 1935j). The observed 
fact is that well-implemented program generators arc a significant aid in pro- 
ducing reliable compiler components. I t  is much easier to produce a correct 
parser using a grammatical description of the language and a parser generator, 
than implementing a parser directly by hand. An important issue, however, is 
how well these generators interface with one another and with other pro- 
grams. A common mistake in the design of a generator is to assume that it is 
the center of thc dcsign. A better design has the generator produce subrou- 
tines with clean interfaces that can be callcd by other programs (Johnson and 
Lesk 1 19781). 
11 A TESTING AND MAWTENANCE 
A wmpiler must generate correct code. Ideally, we would like to have a com- 
puter mechanically verify that a compiler faithfully implements its spcifica- 
tion. Several papers do discuss thc correctness of various compiling algo- 
rithms but unfortunately, compilers are rarely specified in such a manner that 
m arbitrary implementation can be mechanically checked against a formal 
specificatian. Since compilers are usually rather complex functions, there is 
also the issue of verifying that the specification itself is correct. 
In practice, we must rewrt to wrne systematic method of testing the com- 
piler in order to increase our confidence that it will work satisfactwily in the 
field. 
One approach used successfully by many compiler writers is the 
"regression" test. Here we maintain a suite of test programs, and whenever 
the compiler is modified, the test programs are compiled using both the new 
and the old versions of the compiler, Any differences in the target programs 
produced by the two compilers are reported to the compiler writer. The 
UNIX system command m k e  can also be used to automate the testing. 
Choosing the programs to include in the test suite is a difficult problem. As 
a goal, we would like the test programs to cxercise every statement in the 
compiler at least once+ It usually requires great ingenuity to find such a test 
suite. Exhaustive test suites have been constructed for several languages (For- 
Iran, w, C ,  
etc.), Many compiler writers add to the regression tests pro- 
grams that have exposed bugs in previous versions of their compiler; it is frus- 
trating to have an old bug reappear kcause of a new correction, 
Performance testing is also important. Some compiler writers check that 
new versions of the compiler generate code that is approximately as good as 
the previous version by dotng timing studies as part of the regression test. 
Maintenance of a compiler is another important problem, particutarly if the 
compiler i s  going to be run in different environments, or if people involved in 
the compiler project come and go. A crucial element tn k i n g  ahk to maintain 
a wmpiler i s  g
d
 programming style and good documentation. The authors 
know of one compiler that was written using only seven comments. one of 
which read 'This code is cursed." Needless to say, suds a program is mme- 
what difficult to maintain by myone except perhaps the original writer. 

732 
WANT TO WRITE P. COMPILERd? 
SEC. 11.4 
Knuth [19#4bl has developed a system called WEB that addresses the prob- 
lem of documenting large programs written in Pascal. WEB facilitates literate 
programming; the documentation is developed at the same time as the code, 
not as an afterthought. Many of the ideas in WEB can be applied equally 
well to other languages. 

CHAPTER 
A Look at 
Some Compilers 
This chapter discusses the structure of some existing compilers for .a text- 
formatting language, Pascal, C, Fortran, Bliss, and Modulsl 2. Our intent is 
not to advocate the designs presented here to the exclusion of others, but 
rather ti, illustraM the variety that is possible in the implementation of a com- 
piler. 
The compilers for Pascal were chosen because they influenced the design of 
the Ianguage itself. The compilers for C were chosen because C is the pri- 
mary programming language on the UNlX operating system. The Fortran H 
compiler was chosen because it has significantly influenced the development of 
optimization techniques. BLIWII was chosen to illustrate the design of a 
compiler whose goal is to optimize space. The DEC Modula 2 compiler was 
chosen because it uses relatively simple techniques to produce excellent d c ,  
and was written by one person ht a few months. 
12.1J EQN, A PREPROCESSOR FOR TYPESBTTlNG MATHEMATICS 
The set of possible inputs to a number of computer programs can be viewed as 
a littie language. The structure of the set can be described by a grammar, and 
syntax-directed translation can be used lo precisely specify what the program 
does. Compiler technology can then be applied to implement the program. 
One of the first compilers for little languages in the UNlX programming 
environment was EQN by Kernighan and Cherry 119751. As described briefly 
in Section 1.2, EQN takes input like '+E rub ? "  and generates kommands for 
the text formatter TROFF to produce output of the form "El ". 
- The implementation of EQN is sketched in Fig. 12. I .  Macro preprmssing 
(sa 
Section 1.4) and lexical analysis are done together. The token stream 
after lexical analysis is translated during parsing into text-formatting com- 
mands. The translator is constructed using the parser generator Yacc, 
described in Section 4.9. 
The approach of treating the input to EQN as a language and applying com- 
piler techndogy to construct a translator has several benefits noted by the 
authors. 

734 
A LOOK AT SOME COMPILERS 
swrcc code 
SEC. 12.1 
f 
I 
macro prcprwcssor. 
token stream 
syntax-direclcd translator 
gencratcd by Yacc i
TROFF text-formatting commands 
Fig, 12.1. EQN implcmentarion. 
1 . 
Ease UJ imp!mwttutiun. "Construcf ion of a working system sufficient to 
try out significant examples required perhqps a person-month." 
2. hn#crage svulaliun . A syntax-directed definir ion facilitates changes in the 
input language. Over the years EQN has evolved in response to user 
needs. 
b 
The authors conclude by ob~erving that "defining a language, and building a 
compiler for it using a compilerkompiler seems like the only sensible way to 
do business." 
12.2 COMPILERS FOR PASCAL 
The design of Pascal and the development of the first cumpilei- for it "were 
interdependent," as Wirth 11971 1 notes. €1 is therefore instructive to examine 
the struciure of the compilers for the language written by Wirth and his col- 
leagues. The first (Wirch 119711) and the second compilers (Amrnann 1 1981, 
19771) generated absolute machine c d e  for the CDC 6000 series machines. 
Portability e~periments with the second compiler led to the Pascal-P compiler 
that generates code, called P-code, for an abstract stack machine (Nori et al. 
[!MI]). 
Each uf the above compilers i s  a one-pass compiler organized around a 
recursive-descent parser, like the "baby" front end in Chapter 2, Wirth 
11971 1 observes that "it turned out to k relatively easy to mould the language 
according to [the restrictions of the parsing method I+" The organization of 
the Pascal-P compiler is shown in FigA 12.2. 
The basic operations of the abstract aack machine used by the Pascal-P 
compiler reflect the needs of Pascal. Storage for the machine is organized into 
four areas: 
I .  
code for the procedures, 
2. 
constanls, 

THE C COMPlLERS 735 
source code 
4 
I 
lexical analymr, 
I 
I marks errors on wpy of murce eode I 
I 
token stream 
t 
- 
I 
predictive [randator. 
1 
Fig. 12.2. Pascal-P compiler, 
3. 
a stack for activation records, and 
4. 
a heap for data allocated by applying the mew operator.' 
Since procedures may be nested in Pascal, the activation record for a pro- 
cedure contains both access and control links. A procedure call is translated 
into a "mark stack" instruction for the abstract machine, with the access and 
control links as parameters. The c d c  for a procedure refers to the storage 
for a 1-1 
name using a displacement from an end d the activation record. 
Storage for nonhcals is referred to by a pair, consisting of the number of 
access links to be iraversed and a displacement, as in Section 7.4. The first 
compiler used a display for efkietlt access to nonlocals. 
Ammann It98L 1 draws the following conclusions from the exprience with 
writing the second mmpikr. On the one hand, the one-pass cumpikr was 
easy to implement and generated modest input/utput activity (the code for a 
procedure M
y
 
is compiled in memory and written out as a unit to secondary 
storage), On the othe~ hand. the onc-pass organization "imposes severe res- 
ttidions on the quality of the generated code and suffers from relatively high 
storage requirements. " 
12.3 THE C COMPILERS 
C is a general-purpose programming language designed by D. M. Ritchie and 
is used as the primary programming language on the UNlX operating system 
(Ritchie and Thompson [1974]). UNlX itself is written in C and has been 
moved to a number of machines, ranging from rnia+prwessors to large main- 
frames, by first moving a C compiler. This section briefly describes the 
overall structure of the cumpiler for the PDP-I I by Ritchie [ 19791 and the 
K C  family of portable C compilers by Johnson (1 979 1. Threequarters of the 
' lhtstrapping is facilit;ited by thc fact that ~ h c  
compiler, writtern in thc subset id wrnpiks, uses 
the heap W e  a slack, so a impk heap manager can be used initially. 

736 A LOOK AT SOME COMPlLERS 
E C .  12.3 
code in PCC is independent of the target machine. All these compilers are 
essentially two-pass; the PDP-I 1 compiler has an optimal third pass that does 
optimization on the assembly-language output, as indicated in Fig, 12+3+ This 
peephole optimization phase eiirninates redundant or inacessible statements. 
murcx mdc 
i 
1 
lexical and syntax analysis 
pastfix or prcfix form for cxprcssions 
a~wmbly code otherwise 
post opt imizarion 
asscmbly Imguagc 
Fig, 12.3. Pass structure: of C mmpilcrs. 
Pass I of each compiler docs lexical analysis, syntax analysis, and intermedi- 
ate code generation. The PDP-I I compiler u x s  recursive descent to parse 
everything cxcept expressions, for which operator preoedenm is used. The 
intermediate c d e  consists of postfix notation For expressions and assembly 
a l e  for control-flow statements. PCC uses an LALR( 1) parser generated by 
Yacc. Its intermediate code consists of prefix notation for expressions and 
assembly code for other constructs. In each case, storage allocation for local 
names is done during the ikst pass, su these names can be referred to using 
offsets into an activation record. 
Within the back end, expressluns are represented By syntax trees. In the 
PDP-I I compiler, code generation is implemented by a tree walk, using a stra- 
tegy similar to the labeling algorithm in Section 9.10. Modifications to that 
algorithm have been made to assure [hat register pairs are available for o p r a -  
tims that need them and to take advantage of operands that are consrants, 
Johnson 11978) reviews the influence of theory an PCC. In both PCC and 
PCC2, a subsequent version d the compiler, code for ertpressions is generated 
by tree rewriting. The code generator in PCC examines a source language 
statement at a time, repeatedly finding maximal subtrees that can be computed 
without stores, using the available registers. Labels computed as in Seclion 

SEC, 12.4 
THE FORTRAN H COMPILERS 737 
9,10 identify subexpressions to be ~0mptJted and stored in temporaries. Code 
to evaluate and store the values rtprewnttd by these subtrees is generared by 
the compiler as the subtrees are selected. The rewriting is more evident in 
PCC2, whose code generalor is based on the dynamic programming algorithm 
of Section 9.1 1 .  
Johnson and Ritchie 139811 describe the influence of the target machine an 
the design of activation records and the procedure callheturn sequence. The 
standard library function printf can have a variable number of arguments, 
so the design of the calling sequence on some machines is dominated by the 
need to allow variable-length argument Lists. 
12.4 THE FORTRAN H COMPILERS 
The original Fortran H compiler written by Lowry and Mtdlock [IN91 was an 
extensive and fairly powerful optimizing compiler built using methods that 
largely predate those described in this h o k .  Several attempts at increased 
performance have k e n  made; an "extended" version of the compiler was 
developed for the lBMt370, and an "mhanwd" version was developed by 
Scarborough and Kolsky [1980j. Fortran H offers the user the choice of no 
optimization, register optimization only, or full optimization. A sketch of the 
compiler in the case that full optimization is performed appears in Fig. 12.4. 
The source text is treated in four passes. The firs1 two perform lexical and 
syntactic analysis, producing quadruples. The next pass incorporates code 
optimization and register optimization and !he final pass generates ob~e~t 
d c  
f r m  the quadruples and register assignments. 
The lexica1 analysis phase is somewhat unusual, since its output is not a 
stream of tokens h t  a stream of "operator-operand pairs," which are roughly 
equivalent to an operand token together with the preceding nonaperand token. 
It should be noted that in Fortran, like most languages, we never have two 
consecutive operand tokens such as identifiers or constants; rather, two such 
tokens are always separated by at least itone punctuation token. 
For example, the assignment statement 
would be translated into the sequence of pairs: 
*'assignment statement', 
A 
- - 
B 
Is 
I 
1 
- 
+ 
C 
The lexical analysis phaw distinguishes between a left parenthesis whose job is 
to introduce a list of parameters or subscripts from one whose job is to group 
operands. Thus, the symbol "1s" is intended to represent a left parenthesis 
used as a subscripting oprator, Right parentheses never have an operand 

COMMON and EQUIVUENCE handling 
I 
operator~prand pairs 
data flow analysis, 
assignment of addresses lo namcs, 
quadruples 
optimize register assignments, 
quadruples with register assignments 
f 
relocatable machine W e  
Fig. 12.4. OutIine of Fortran H compiler, 
following, and Fortran H does not distinguish the two roles for right 
parent hees . 
Associated with lexical analysis is the prrxsessing of COMMCN 
and 
EQUIVALENCE statements. lt is possible at this stage to map out each CW- 
MON block of storage, as well as the &orage blocks associated with the subrou- 
tines, and to determine the location of each variable mentioned by the pro- 
gram in one of these static storage areas, 
Since Fortran has no structured control statements like while-statements, 
parsing, except for expressions, is quite straighaforward, and Fortran H simply 
uses an operator-precedence parser for expressions. Some very simple local 
optimizations are performed during the generation of quadruples; for exampte, 
multipliations by powers of 2 are replaced by left-shift operations. 
Code Optimization in Fortran H 
Each subroutine Is partitioned into basic blacks, and the Imp structure i s  
deduced by finding flow-graph edges whose heads dominate their tails, as 
described in Section 1 O+4. The wrnpiler performs the following optimizations. 

SEC. 12.4 
THE FORTRAN H COMPLLERS 739 
Common subexpression c~imimsion. The compiler looks for local common 
subexpressions and for expressions that are common to a block B and one 
or more bbcks that B dominates. Other instances of common subexpres- 
sions are not detected. Further, the detection of common subexpressions 
is done one expression at a time, rather than using the bit-vector method 
described in kction 10.6. 
Interestingly, in developing the "enhanced" 
version ciC the compiler, the authors found that a major spedup was pos- 
sible by using bit-vector methods. 
Cad& motion. Loop-invariant statements are removed from hops essen- 
tially as described in Sction 10.7. 
Copy propugacion. Again, this is done one mpy statement at a time. 
Ituiuckw variable c&intinrrtion. This optimization i s  performed only for 
variables that are assigned once in the loop, Instead of using the "fam- 
ily" approach descriW in Section 10.7? multiple passes through the d
e
 
art made to detect induction variables that belong to the family of some 
other induction variable . 
Although data flow analysis is done in a one-at-a-time style, the values 
corresponding to what we have called in and, out are stored as bit vectors. 
However, in the original compiler, a limit of length 127 was pbced on these 
vectors, sr, large programs have only the mast freq'uintly used of their varb 
ables involved in optirniza~ions~ 
The en h a n d  version increases the limit but 
does not remove it + 
As Fortran is frequently used for numerical calculations, algebraic optim izsr- 
tion is dangerous, since transformations of expressions can, in computer arith- 
metic, introduce overflows or Imw\ of precision that are not visible if we take 
an idealized view of algebraic simp4 it'icatim . However, algebraic transforma- 
tions involving ifittigers are generally safe, and the enhanced krsion of the 
compiler does some of this optimization in the case of array references only. 
In gwWral, an array reference like A( I, J, K I involves an offset c d d a -  
tion in which an expression of the form a 1  -+ b J + CK + d is computed; the 
exact values of the constants depcnd on the location of A and the dimensions 
of the array. If, say, I and K were canstants, either numerical constants or 
Imp-invariant variables, then the compiler applies the commutative and a s w i -  
ativc law to get an expression b J + e, where e = a I + rK + 6. 
Fortran H divides registers into three classes. These sets of registers are used 
for local register optimization. global register optirnizatioo, and "branch 
optimization." The exact number of registers in each class can be adjusted by 
the compiler, within h i t s .  

740 
A LOOK A T  SOME COMPILERS 
SEC. 12+4 
Global registers are allocated on a loop-by-loop basis to the most frequently 
referend variables in that Imp. A variable that qualifies for a register in 
one Imp L. but not in the Imp immediately containing L. is loaded in the pre- 
header of L and stored on exit from L. 
Local registers are used within a basic block to hold the results of one state- 
ment until it is u x d  in a subsequent statement or statements. Only if not 
enough local registers exist is a temporary value stored. The compiler tries to 
compute new values in the register holding one of its operands, if that 
operand is subsequently dead. In the enhanced version, an attempt is made to 
recognize the situation where global registers may be exchanged with other 
registers to increase the number of times that an operation can take place in 
the register holding one of its operands. 
Branch optimization is an artifact of the IBM/370 instruction set, which puts 
a significant premium on jumping only to locations that can be expressed as 
the contents of some register, plus a constant in the range 0 to 4095. Thus. 
Fortran H allocates some registers to hold addresses in the code space, at 
intervals of 4096 bytes, to allow efficient jumps in all but extremely large pro- 
grams. 
12.5 THE BLISS/11 COMPILER 
This compiler implements the systems programming language Bliss on a PDP- 
11 (WuSf et a]. 11975]1. In a sense, it is an optimizing compiler from a world 
that has ceased to exist, a world where memory space was at enough of a 
premium that it made sense to perform optimkations whose sole purpose was 
to reduce space rather than time. However, most of the optimizations per- 
formed by the compiler save time as well, and descendants of this compiler 
are in use today. 
The compiler i s  worth our attention for several reasons. Its optimization 
performance is strong, and it performs it number of trmsformations found 
almost nowhere else. Futt her, it pioneered the "syntaxdirected" approach to 
optimi7ation, as discussed in Section 10.5. That is, the language Bliss was 
designed to produce only reducible flow graphs (it has no goto's). Thus, it 
was possible for data flow analysis to be performed on the parw tree directly, 
rat he^ than on a flow graph. 
The compiler operates in a single pass, wirh one procedure completely pro- 
cessed before the next is read in. The designers view the compiler as com- 
posed of five modules. as shown in Fig. 12.5. 
LEXSY NFLO performs lexical analysis and parsing. A recursive-descent 
parser ib used, As BLlSS permits no goto-statements, all flow graphs oC 
BLISS procedures are reducible. In fact, the syntax of the language enables 
us to buiid the flow graph, and determine Imps and Imp entries, as we parse. 
LEXSYNFLO does so, and also determines common subexpressions and a 
variant of ud- and du-chains, taking advantage of the structure of reducible 
flow graphs. Another important job of LEXSYNFLO is to detect groups of 

THE BLISS1 I COMPILER 
741 
t 
lexical and syntax anaiy sis, 
I 
flow analysis. 
gather information for 
/ LEXSYNFLOW 
syntax trcc 
selcct optimizations 
DELAY 
to bc pcrforrncd 
syntax trcc plus ordcr ing 
cvaluatcd in thc same register. 
TNBTND 
pack temporaries into registers 
I 
syntax tree, ordering, and rcgihkr assignments 
rclocarablc machine code 
i 
pccphole opt imizalion 
Fl N A L 
crdc generut ion 
f 
rclmatable rnachinc code 
CODE 
Fig. 12.5. Thc BLlW11 compikr+ 
similar expressions. These are candidates for replacement by a sin* 
subrou- 
tine. Note that this replacement makes the program run mare slowly but can 
save space. 
The rncdule DELAY examines the syntax tree ta determine which particu- 
lar instances of the usual optimizations, such as invariant code motion and 
elimination of common subexpressions, are actually likely to produce a profit. 
The order of expression evaluation i s  determined at this tim, based on the 
labeling strategy of Section 9.10. modified to take into amount registers that 
are unavailable because they are used to preserve the values of commun 
subexpressions . Algebraic laws are used to determine whether reordering of 
computations should be done. Conditional expressiofis are evaluated either 
numerically or by control flow, as discussed in fedion 8.4, and DELAY 
decides which mode is cheaper in each instance. 

742 
A LOOK AT SOME COMPILERS 
SEC. 12.5 
TNBIND considers which temporary names should be bound to registers, 
Both registers and memory locations are allocated. The straregy used is to 
first group nodes of the syntax tree that should be assigned the same register. 
As discussed in Section 9.6, there is an advantage to evaluating a node in the 
same rcgider as one of its parents. Next, the advantage to be gained by keep- 
ing a temporary in a regislw is estimated by a calculation favor,ring those that 
are used severai times over a short span. Registers are then assigned until 
used up, packing the most advantageous nodes into registers first. CODE 
converts the tree, with its ordering and register assignment informat ion, to 
relocatable machine d e ,  
This code is then repeatedly examined by FINAL which performs peephole 
optimization until no further improvement results. The improvements made 
indude elimination of (conditional or unconditional) jumps to jumps and corn- 
pkmtntation of conditionals, as discussed in Section 9+9. 
Redundant or unreachable instructions are -eliminated I 
the= could have 
resulted from other FINAL optimizations). 
Merging of simitar code 
sequenms on the two paths of a branch is attempted, as is local propagation of 
constants. 
A number of other local optimizations, some quite machine- 
dependent are attempted. An important one is replacement, where pssible. 
of jump insttuctions by PDP-I I "branches," which require one word but are 
limited in their range to I28 words. 
12.6 MODULA-2 OWIMIZENG COMPILER 
This compiler, described in Powell [1984], was developed with the intent of 
producing good code, using optimizations that provide a high payoff for little 
effort; the author describes his strategy as looking for the "best simple" 
up~imizations. Such a philowphy can be difficult to carry out; without experi- 
mentation and measurement, it Is hard to decide what the "best simple" 
op~imizations are in advance, and some of the decisions made in the Mdula-2 
compiler are probably inapprapr iate for a compiler providing maximum optim- 
ization. Nevertheless, the strategy did achieve the author's goal of producing 
excellent d
e
 with a compiler that was written in a few months by one per- 
son. The five passes of the front end of the compiler are sketched in Fig. 
12.6. 
The parser was generated using Y acc, and it produces syntax trees in two 
passes, since Modula variables do not have to be declared before use. An 
attempt to make this compiler compatible with existing facilities was made. 
The intermediate code is P-code for compatibility with many Pascal compilers. 
The procedure call format for this compiler agrees with that of the Pascal and 
C compilers running under Berkeley UNIX, so procedures written in the three 
languages can be integrated easily. 
The compiler does not do data-flow analysis. Rather, Modula-2, like Bliss, 
is a language that can produce only reducible flow graphs, so the rnethdology 

to identifiers 
optimizations 
compute reference counts 
+ 
and assign registers + 
of Section 10.5 can be used here as well. In k t ,  the Modula compiler goes 
beyond the Bliss-I 1 compiler in the way it takes advantage of the syntax. 
Loops are identified by their syntax; i.e., the compiler looks for while- and 
foranstructs. invariant expressions am detected by the fact that none of 
their variables are defined in the Iwp, and these are moved to a Imp header. 
The only induction variables that are detected arc those in the family of a 
for-lmp index. Global common subexpressions are detected when me is in a 
blwck that dominates the block of the other, but this analysis is done an 
expression at a time, rather than with bit vectors. 
The register allocation strategy is similarly designed ta da reasonable things 
without being exhaustive. In particular, it considers as candidates for alloca- 
tion to a register only: 
1. 
temporaries used during the evaluation of an expression (these rwive 
first priority), 
2. 
values of common su bexpressions, 
3, indices and limit values in for-loops, 
4. the address of E in an expression of the form with E do, and 
5 .  
simple variables (characters, integers, and so on) local to the current pro- 
cedure + 
An attempt is made to estimate the value of keeping each variable in classes 
(2)-(5) in a register. I t  is assumed that a statement is executed id times if it 

744 
A LOOK AT SOME COMPILERS 
SEC. i2.6 
is nested within d loops. Howllcvcr, variables referenced no more than t w b  
are not considered eligible; others are ranked in order of estimated use and 
assigned tb a register if one is adlabk after assigning expression temporaries 
and higher-r an ked variables, 

APPENDIX 
Programming 
Project 
A+1 INTRODUCTION 
This appendix suggests programming exercises that can he used in a program- 
ming laboratory accompanying a mtnpiler-design course based on this b k .  
The exercises consist of implementing the basic components of a cornpikr for 
a subset of Pascal. The subset is minimal, but albws programs such as the 
recursive sorting prwdure in W i o n  7.1 tu be expressed. Being a subset of 
an existing language has certain utility. The meaning of programs in the sub- 
set i s  determined by the semantics of Pascal (Jensen and Wirth [ i9751). If a 
Pascal compiler is availabie. it can be used as a check on the behavior of the 
compiler written as an exercise. The constructs in the subset appear in most 
programming languages, so mrresponding exercises can be formulated using a 
different language if a Pascal compiler is not available. 
A.2 PROGRAM STRUCTURE 
A program consists of a sequence of global data declarations, a seqwnce of 
procedure and function declarations, and a single compound statement that is 
the "main program," Global data is to be allocated static storage. Data hl 
to procedures and functions is allocated storage on a stack. Recursion is per- 
mitted, and parameters are passed by reference. The procedures read and 
write are assu rned supplied by the compiler + 
Fig. A.1 gives an example program. The name of the program i s  exam- 
ple, and input and output are the names of the files used by read and 
write, respectively. 
A,3 SYNTAX OF A PASCAL SUBSET 
Listed below is an LALR(I j grammar for a subset of Pascal. The grammar 
can k mdified for recursive-descent parsing by eliminahg left recursion as 
described in Sctions 2.4 and 4.3. An operator-precedence parser can be 

746 
A PROGRAMMLNG PROJECT 
SEC. A.3 
program examplt[input, output); 
var x, y: integer; 
fmction gcdCa, b; integer): integer; 
begin 
if b = O then ged := a 
else ged := gcdIb, a mod b) 
end; 
begin 
readlx, y )  ; 
writeIgcbix, y ) )  
end. 
Fig. A,1, Example program 
constructed for expressions by substituting out for relop, mrddop, 
and eliminating r-productions. 
The addition of the production 
intrduces the "dangling-else" ambiguity, which can tx ehminated as dis- 
cussed in Secttan 4.3 (see also Example 4.19 if predictive parsing is u.sed). 
There is no syntactic distinction between a simple variable and the call of a 
function without parameters. Both are generated by the production 
Jactor -. id 
Thus, the assignment a : + b sets a to the value returned by the function b, if 

SYPiTAX OF A PASCAL SUBSET 747 
variable + 
id 
1 id r expressiim 1 

748 
A PROGKAMMING PROJECT 
A.4 LEXICAL CONVEN'I'IONS 
Thc notation fur the specifying tokens Is from Sectiun 3.3. 
Comments arc surrounded by I and I .  They may no1 contain a { .  Com- 
ments may appear after any token. 
Blanks between tokens are optional, with the exception that keywords 
must be surrounded by blanks. ncwlinix, the, beginning of the program, 
or the final dot. 
Token id for iden~ificrs matches a letter fdlowed by letters or digirs; 
letter - 1 a - ZA - Z] 
digit + 10 -9 1 
id + letter ( letter I digil I* 
The in~plementer may wish to put a limit on identifier length. 
Token num matches unsigned integers (see Example 3.5): 
digits - digit digit* 
optionaLfrartion - . digits I E 
optional-exponent 
( E i t ] - 1 E ) digits ) ( E 
nnm - digits optionaLfradion optional-exponent 

SEC+ A.5 
SWGGESTED EXERCISES 749 
5. 
Keywords are reserved and appear in boldface in the grammar. 
7. 
The addop's are +, -, and or. 
8, The mulop's arc *, /, div, mod, and and, . 
9. The lexeme for token mignop is : =. 
A.5 SUGGESTED EXERCISES 
A programming exercise suitable fur a une-term course is to write an intcr- 
preter for the language defined above, or for a similar subsel of another high- 
level language. The project involves translating the source program into an 
intermediate representation such as quadruples or stack machine code and 
then interpreting the intermediate representation. We shall propose an order 
for the construction of the modules. The order is different from the order in 
which the modules are executed in the wrnpikr because it is convenient to 
have a working interpreter to debug the other compiler components. 
1. Design a syrnbd-tuMc. mechanism, Decide on the symbol-table organiza- 
tion. 
Altow for information to be mllectcb about names, but leave the 
sy rnbol-table record structure flexible at this time. Write routines 10: 
i) 
Search the symbol table for a given name, create a new entry for that 
name if none is presenl, and in either case return a pointer to the record 
for that name. 
ii) 
Dekk from the symbol table all names local to rt given procedure. 
2. Wriie an interpreter .for qiiudrupk~)~. 
The exact set of quadrupks may be 
left open at this time but they should include the arithmetic and cunditional 
jump statements corresponding lo the set of operators in the language. Alw 
include logical operations; if conditions are evaluated arithmetically rather than 
by position in the program. In addition, expect to need "quadruples" for 
integer-to-real conversion, for marking the beginning and end of procedures, 
and for parameter passing and procedure calls. 
It is also necessary at this time to design the calling sequence and runtime 
organization for the programs being interpreted. The simple stack organiza- 
tion discussed in Section 7.3 is suitaMc for the example language, h a u s e  no 
nested declarations of procedures are permitted in the language; that is, vari- 
ables are either global {declared at the level of the entire program) or !ma! to 
a simple procedure. 
For simplicity, another high-level language may be used in place of the 
interpreter. Each quadruple can be a statement uf a high-level language such 
as C, or even Pascal. The output of the compiler is then a sequence of C 
statements that can be compiled on an existing C compiler, This approach 
enables the implernenter to concentrate on the run-time organization, 

750 
A PROGRAMMING PROJECT 
SEC. A.5 
3. Write she bexicuf arullyzer. Select internal A e s  for the tokens. Decide 
how constants will be represented in the compiler. Count lints for later use 
by an errormessage handler. Produce a listing of the source program if 
desired. Write a program to enter the reserved words into the symbol table. 
Design your lexical analyzer to be a subroutine called by the parser, returning 
a pair (token, attribute value). At present, errors detected by your lexical 
analyzer may be handled by calling an error-printing routine and halting, 
4. Wriir I
~
P
 
sernunlic acrions. Write semantic routines to generate the quad- 
ruples. The grammar will need to be modified in places to make the transla- 
tion easier, Consult Sections 5.5 and 5.6 for examples of how to modify the 
grammar usefuily. Do semantic analysis at this time, converting integers to 
reals when necessary. 
5. Wrire the parser. lf an LALR parser generator is available, this will sim- 
plify the task considerably. if a parser generator handling ambiguous gram- 
mars, like Y acc, is available, then nonterminals denoting expressions can be 
combined. Moreover, the "dangl ing-else" ambiguity can be resolved by shift- 
ing whenever a shifthduce conflict occurs. 
6, Write the error--Aad&ing 
routines, 1 Be prepared to recover from lexical and 
syntacttc errors. Print error ddiagnoslics for lexical, syntactic, and semantic 
errors. 
7. Evaluativn. The program In Fig. J1B.l- can serve as a simple test mu- 
tine. Another test program can be b a d  on the Pascal program in Ftg, 7-1, 
The code for function partition in the figure mrrespnds to the marked 
fragment in the C program of Fig. 10.2. Run your compiler through a pro- 
filer, if one is available. Determine .the routines in which most of che time is 
being spent. What modules would have to be mobifid in order 10 increase 
the speed of your compiler? 
A.6 EVOLUTION OF THE 1NTERPRETER 
An alternative approach to constructing an interpreter for the language is to 
start by implementing a desk calculator, that is, an interpreter for expressions. 
Gradually add mstructs to the language until an interpreter for the entire 
language is obtained. A similar approach is taken in Kernighan and P i e  
[ 19#4]. A proposed order for adding constructs is: 
1. Transhe expressions into p s f u  wtution. Using either rwursivadescent 
parsing, as in Chapter 2, or a parser generator, familiarize yourself with the 
programming environment by writing a translator from simple arithmetic 
expressions inlo postfix notation. 
2. Add a kxicul amlyzer. Allow for keywords, identifiers, and numbers to 
appear in the translator constructed above. Retarget the translator to produce 
either code for a stack machine or quadruples. 

E C .  A.7 
EXTENSIONS 75 1 
Section A.5, a high-level language may be used in 
of the interpreter. 
For the moment, the interpreter need only support arithmetic operations, 
assignments, and inputsutput. Extend the language by allowing global vari- 
able declarations, assignments, and calls of procedures read and write, 
Thew constructs allow the interpreter to k 
tested. 
4. Add statements. A program in the language now consists of a main pro- 
gram without subprogram declarations. Test both the translator and the inter- 
preter, 
5. Add procedures andfsmcr~ns. The symbol table must now allow the scop~s 
of identifiers to be limited to prwedure bodies. Design a calling sequence. 
Again, the simple stack organization of Section 7.3 is adequate, Extend the 
interpreter to support the calling sequence. 
A+7 EXTENSIONS 
There are a number of features that can be added to the language without 
greatly increasing the complexity of compilation. Among these are: 
I . multidimensional arrays 
2 
for- and ease-statements 
3. 
block structure 
4. 
record structures 
If time pe~mits, add one or more of these extensions to your compiler 

Bibliography 
ABEL, N. E. AND J. R. BELL 119721. "Global optimization in compilers," Prm 
Firsd USA-Japan Computer Coif., AFIPS Press, Mmtvak, N . J . 
A BELSON, H . AND G. J. SC~SSMAN [ 19351. Strumre and Interpreturiot of Corn- 
purer Progrrms, MlT Press, Cambridge, Mass. 
ADRION, W. R.+ M. A. BRANSFAD, 
AND I. C. CIIERNIAYSKY 
[19821. "Valida- 
tion, verification, and testing of computer software ," Computing S u m p  
14:2, 1 59- 192. 
AHO, A. V. [1980]. -Pattarn matching in strings," in Book [1980], pp. 325- 
347. 
AHO, A. V. AND M. 
J .  CORAWK 119753. 'Efficient string matching: an aid to 
bibliographic search," Cmm. ACM 186, 333-340. 
AHO, A. V. AND M. GANAPATHI 
119851. "Efficient tree pattern matching: an 
aid to code generation," Twiffh Anrtuul ACM Symwsisw bn Priucipk$ of 
Programming Languages, 334-340, 
AHO, A. V., 5. E. HOWRDFT, 
AND J. D. ULLMAN 
119741. The De,rip n d  
A d y s i s  of Colprputer Algorithm, Addison-Wesley, Reading, Mass, 
AHO, A. V., J. E. HOKROFT, 
AND J. D. UUMAN 119831. Data Slrucrures and 
Algorithms, Addison-Wesley, Reading, Mass. 
A m ,  A. V. AND S. C. JOHNSON 
[1974]. "LR parsing," Computing Surveys 6:2, 
99- 124. 
AHO, A. V .  AND S. C. JOHNSON 119761. "Optimal d
e
 
generation for expres- 
sion trees," J .  ACM W:3, 488-501. 
Am, A, V., S. C, JOHNSON, AND J. I). ULLMAN 
119751- "Deterministic parsing 
of ambiguous grammars," Comm. ACM 18:8,441-452. 
Am, A. V., S. C, JOHHSON~AND~. 
I). ULLMAN 11977a1. '%ode generation for 
expressions with common subexpressims," J .  ACM 24:l, 146- 160. 
Am, A. V,, S, C. JOHNSON, 
ANDJ. D+ 
ULLMAN [1977b]+ '%de generation for 
machines with rnultiregister operations," Fourth ACM Syqosilurr on Prin- 
ciples oj Progruming Languuges, 21-28. 
Am, A. V., B. W, KERNIGHAN, 
AND P+ J. WEINBERGER 
119791. "Awk - a 

pattern scanning and processing language," Software-Practice 
mi Experi- 
ence 9:4, 267-280. 
AHO, A. V. AND T. G. PETERSON 
[1972]. "A minimum distance error- 
correcting parser for wntext-free languages," SIAM J. Computing 1:4, 
3U5-312. 
AHO, A. V. AND R. STHI 
119771. "How hard is compiler code generation?" 
Lecture Notes in Computer Science 52, Springer-Verlag, Berlin, 1- 15. 
' 
AHO, A. V. AND J. C)+ ULLMAN [1972a]. "Optimization of srraight line code," 
S l M J .  Compuring 1:1, 1-19. 
AHO, A .  V .  AND 5, D. ULLMAN 
[1972b]. The Theory of Parsing, Transhrion 
and Compiling, V d .  I: Parsing, Prentice-Hail, E n g l e w d  Cliffs, N . J. 
AHO, A. V. AND J. D. ULLMAN 
{ l973aI. The Theory of Parsing, Transhtion 
arui' Compilhg, Vol. 11: Compiling, Prentice-Hall, E n g l e w d  Cliffs, N. J. 
AHO, A. V. AND J. D. ULLMAN 
C1973bl. "A technique for speeding up LR(k) 
parsers," $/AM J. Computing 2:2, 
1 M- 
127. 
AHO, A, V. AND 1. I). ULLMAN 
[1977]. 'Principles of Compiler Design, 
Addison-Wesley , Reading, Mass. 
AIGRA~N, 
P., S. L. GRAHAM, R. R. HENRY, M. K. MCKU~ICK, 
AND E. PELEGRI- 
LLOPART 
[ 19841. "Experience with a Graham-Glanville style code gencra- 
tor," ACM S I G P M  Notices 196, 1 3-24. 
ALLEN, F. E. I1%9]. 
"Program optimization," A m d  Review in Amrrmfic 
Progmmming 5, 239-307. 
ALLEN, F, E . I 19701. "Control flow analysis," ACM S I G P W  Norices 5:7, 1- 
19. 
ALLEN, F. E , 119741. "Interprocedural data flow analysis," Ir$ormrrfiun Pro- 
cessing 74, North-Hdland, Amsterdam, 398402. 
ALLEN, F. E, [I 9751. 
"Bibliography on program optimization ." RC-5767, 
IBM T, 
J. Watson Research Center, Yorktown Heights, N. Y. 
ALLEN, F. E., I. L. CRRTER, 
J, FARM, J, FERRANTE, 
W. H. HARRISIN., 
P. G .  
LOEWNER, 
AND L+ H . TREVLLYAH 
[1980]. 'The experimental compiling 
system," IBM. d. Research amIDewbpment 246, 695-715. 
ALLEN, F. E. AND J. COCKE [1972]. "A catalogue of optimizing transf~rrna* 
tions," in Rustin [1972], pp. f -30. 
ALLEN, F. E. AND J .  CWKE [19761, "A program data flaw analysis pro- 
cedure," Cumm, ACM 1k3, 137-147. 
ALLEN, F. E., J. CWKE, AND K. KENNEDY 119811. "Redudion of operator 
strength," in Mwhnick and Jones [t981], pp. 79-101. 

754 
BIBLIOGRAPHY 
AMMANN, U +  [1977]+ "On code generation in a Pascal compiler," Sofrware- 
P rac~im a d  Experience 7 9 ,  39 1-423, 
AHMANN, 
V. [1981]. 'The Zurich implementation," in Barron [1981], pp. 
63-82. 
ANDERWK, J. P. 119641. "A note on some compiling algorithms." Cumm+ 
ACM 73, 149-150, 
ANDERSON, 
T., J .  EVE, AND J .  J .  HORNINC 
119731. "Efficient LRi1) parsers," 
A , m  in~urmarica 2:1, 12-39. 
AKKLAM, P+, D+ CUTLER! R. HEINEN, JR., 4 N D  M. D. MACLAREN 
[!982]. 
Engineering a Compiler, Digital Press, Ekdford, Mass. 
ARDEN, B. W.. 0. A. GALLER. 
AND R. M. 
GRAHAM [I961 3 .  "An algorithm for 
equivalence declarations," Comm. ACM 4:7, 3 10-3 14. 
AUSLANDER, M. A. AND M. E. HOPKINS 
[19821. "An overview of the PL.8 
compiler," ACM SICPLAN Notices 126, 22-31. 
BACKHOUSE, 
R . C. (1976). "An alternative approach to the improvement of 
LR parsers, " A m  Irtfurmtica 6:3, 277-2%. 
BACKHOUSE. 
R .  C. [1984]. "Global data flow analysis problems arising in 
l~caliy least-cat error recovery ," TOPUS 6:2, 192-2 14. 
BACKUS, I .  W. 1\98 
11. "Transcript of presentat ion on the history of Fortran 1, 
11, and 111," in Wexelblat [1981], pp. 45-66. 
BACKUS, 
1. W+, R. J .  BEEBER, S+ BBH, R .  GOLDBERG, 
L. M. HALBT, H. 1. HER- 
RICK, R. A .  NELSON, D. SAYRE, 
P. B. SHERIDAH. H+ %ERN, !. ZILLER, R. 
A. HUGHES, 
AND R .  NUIT (19571. "The Fortran automatic d i n g  sys- 
tem," Wesrm Juinr C o m p t ~ r  Cunterence, 188-198. Reprinted in Rown 
[I%?], 
pp. 29-47. 
BAKER, 
B. $. 119771. "An algorithm for structuring programs," 1. ACM U:l, 
98- 120, 
BAKER, 
T. P. [l982]. "A one-pass algorithm for overhad resolution in Ada." 
TOPLAS 4:4, 601-614. 
BANNING, 
J .  P, 119791. "An efficient way to find the side effeets of procedure 
calls and aliases of variables. " Sixth A n n d  ACM Sympo~itlm un Principles 
of Programming Languages, 29-41 
+ 
BARTH, J. M. [ 19781. "A practical interprocedural data flow analysis algo- 
rithm," Comm. ACM 219,724-736. 

BATSUN, A .  [ 19651. 'The organization of symbol tables," Comm. ACM 8:2, 
1 1  1-1 12, 
BAUER, A. M. AND H 
. J .  SAAL 119741. "DOCS APL really need run-lime chcck- 
kg?" Sofware-Primice and Experience 4:2, 1 29- 1 38. 
BAUER, F. L. 119761. "Historical remarks on compiler construction," in Bauer 
and Eickel 119761, pp. 603-621. Addendum by A. P. Ershov, pp+ 622- 
626. 
BAUER, F, L, AND J, EICKEL [1976]. Compiler Cnnstrwtion: An Advurti.cd 
Course, 2nd Ed., Lecture Notes in Computer Science 21, Springer-Vcrlag, 
Berlin. 
BAUER, F. L. A
m
 H. WOSSNER [1972]. 'The 'Plankankiil* of Kcmad Zuse: A 
forerunner of today's programming languages," Comm. ACM 157, 678- 
685. 
BEATTY, J+ C+ 119721. "An axiomatic approach to d
e
 optimization for 
expressions." J. ACM 194, 7 14-724. Errata 20 ( 19731, p. 180 and 538. 
BEATTY, J .  C. [1974]. "Register assignment algorithm for generation of 
highly optimized object d e , "  IBM b. Re,mrch and D~vehqmwtl 5 2 ,  20- 
39. 
BELADY, L. A. [1%]. 
"A study of replacement algorithms for a virtual 
storage computer ,*' IBM Sy~rems J .  5;2, 78- 111 1. 
BENTLEY, J + L. I 19821 + 
Writing EjjTcient Progrums, Prentice-Hall, Engkwood 
Cliffs. N. J. 
BENTLEY, J. L., W. S. CLEVELAND, 
AND R. SETHI [1985J. "Empirical analysis 
of hash functions," manuscript, AT&T Bell Laboratories, Murray Hill, 
N. 1. 
BIRMAN, A .  AND I. D. ULLMAN 
119731. "Parsing algorithms with backtrack," 
fnjormtrrion and Cmrrol ZkI, 1-34. 
~ C H M A N N ,  G+ V. 119761. "Semantic evaluation from left to right .'* 
Cumm. 
ACM 19:2, 55-62. 
BOCHMANIS, G .  V. AND P. WARD 119781. "Compiler writing system for attri- 
bute grammars," Computer J. 21~2, 144-148. 
BOOK, R . V + [ 19801. Formal Lnnguuge Theory, Academic Press, New York. 
~ Y E R ,  R .  S. AND J $. MOORE 
119771. "A fast string searching algorithm," 
Comm. ACM 2k10, 262-272. 
BRANQUART, 
P., J.-P. CARDINAEL, 
.I. LEWI, 
J.-P. DELEXAILLE, 
AND M. VANBEGIN 
{1976]. An Opimizeb Transhtivn Prows$ and its Appiicution to Algol 68, 
Lecture Notes in Computer Science, 38, Springer-Verhg, Berlin. 

756 
HI RUOC R APH Y 
BRATMAN, H. 1196!1. "An alternate form of the 'Uncd diagram'," Comm. 
ACM 4:3. 142. 
BROOKER, 
R. A- AND D. MORRIS 
[1%2]. "A general translation program for 
phrase structure languages," J+ ACM 9:1, I - 10. 
BROOKS, 
F4 P., JR. j 19751. The Mythltir+d Man-Monch, Addison-Wesley, Read- 
ing.   ass. 
BROSG~I.,, 
B. M . 1 1 9741. 
Dcturminisiic Trunshrion Grummurs, Ph . D. Thesis, 
TR 3-74, Harvard Univ., Cambridge, Mass. 
~ U N O ,  
J .  AND T. LASAGNE 
(19751. "The generation of optimal code for stack 
machines." J. ACM 229, 382-396. 
BRUNO, 
I. 
 AN^ R. SETHI 119761. 
"Code generation for a one-rcgisler 
machine," 1. ACM W:3, 502-510. 
BURST~LL, 
R. M., I), B, MACQUEEN, 
nuz, L), T. SAN.~NEI,LA [1980j. "Hope: an 
experimental applicative language," Lisp Conjkrenw, Y.O. 5ox 487, Red- 
wood Estates, Calif. 95044, 136- 143. 
BUSAM, V. A. AND L), E. ENGLUNII 
I IY6YI. "Optimization of expressions in 
Fortran," Comm. ACM 12~12, 646-674. 
CARDELLI, 
L. (19841. "Basic plymwphic typechecking," Computing Science 
Technical Report 112, AT&T Bell Laboratories, Murray Hill, N, J7 
CARTER. 
L . R , 1 19821, An Anulysis uf PUJW! Prugrms, U MI Rescarch Press, 
Ann A rbor. M ichigan . 
CAWTWRIGHT, 
R .  1 1885 1. 'Types as intervals," Twevjh Annud ACM Symposium 
on Prinriples oi Progrurnrning Langrragus, 22-36, 
CATTELL, 
R .  G. G . lg8Uj. "Automatic derivation of code generators from 
machine descriptiuns," TOPLAS 2 2 ,  173- 190. 
CHA~TIN. 
G. 
J .  119821. "Rcgister allocation and spilling via graph culoring," 
ACM SIGPLAhr hir~tices 17:6, 20 1 -207. 
CHAITEN, 
G. 
J., M. A .  AUSLANDER, 
A. K .  CCIANDRA, 
J .  COCKE, M. E. HOPKINS. 
AND P+ W +  MARKSI-EIN 
Il98ll, "Register allocation via cduring," Com 
putu Lan~uapes 6, 47-57. 
CHOMSKY, 
N. I 19561. "Thrcc models for the description of language," IRE 

BIBLIOGRAPHY 757 
CHOW, 
F. I 19831. A Pnrtdde Murhine-lndepcn&nr Giubd Opimizer. Ph. D. 
Thesis, Computer System Lab,, Stanford Univ., Stanford, Calif. 
CHOW, 
F. AND J. L. HENNESSY 
[ 19841. "Register aIlocatim by priority-based 
coloring , " ACM SIGPUN N~r~1ir.t.s. 
19~6, 222-232. 
CHURCH, 
A .  [ 1941 1. The Ca!cdi of h m M u  Cunvrrrsibrt, Annals of Math. SIU- 
dies, No. 6, Princeton University Press, Princeton, N. J .  
CHURCH, 
A. 1 19561. /ntmduc.tinn 10 
Muthrhemutkur' L q k .  Vol . 1, Princeton 
University Press, Princeton, N . J .  
CIESINGER, 
J. 119791. ''A bibliography of error handling," ACM SIGPLAN 
nor ice.^ 14:l, 16-26. 
COCKE, J. [19701. "Global common sube~pression elirninat ion," ACM S G -  
PIAN Notices 57. 20-24. 
COC'KE, 
J. AND K. KENNEDY 11 9761. "Profitability computations on program 
flow graphs," Cusmpurers and Markemarics with App/icla&iow: 2:2, 145-159. 
CUCKE, 
I. AKD K .  KENNEDY Ik977I. "An algorithm for reduction of operator 
strcngt h, " Comm. ACM 20.1 1, 850-856. 
CIKKE. J. AND J. MARKSTEIN 
119801, "Measurement of code improvement 
algorithms," Informtion Priwessing 80, 22 1-228. 
CIXKE, J. AND I .  MILLER [19691. "Some analysis techniques for optimizing 
computer programs," Prcw. 2 d  Huwslii Inti. Cmj. on System Scirnmr, 
143-146. 
COCKE, J. AND 1. T. SCHWARTZ 1 19701, Programrniq Lunguu#es and T k i r  
Cmpi!ers: Prdlminury Nom, Srrmd Revised Vemiort, Courant Institute of 
Mathematical Sciences, New York. 
COWMAN, E. G., JR. AND R. SETHI 119831. "lnstrudion sets for evaluating 
arithmetic expressions," J. ACM 349,457478. 
COHEN, R. AND E. HARRY 
119791, "Automatic generation of near-optirna! 
linear-time translators for non-circular attribute grammars," Sixth ACM 
Symposium on Prinriplcs qf Prugramrnirt# L u q i q e s ,  12 1 - 134. 
CON 
W A Y ,  M . E , 1 19631. "Design of a separable transition diagram compiler ," 
Comm. ACM 67,396-408. 
CONWAY, 
R. W. 
AND W, L. MAXWELL 
[1%3]. "CORC - the Cornell cornput- 
ing language," Cnrnm. ACM 66, 3 17-32 1. 
CWWAY, R. W ,  ANU T. R .  WIL-COX 119'731. "Dcxign and implementation of a 
diagnostic compiler for PLiI," Comssn. ACM 16:3, 169- 179. 
CORMACK, 
G. V .  11981 1. 
"An algorithm for the selec~ion of overloaded 

758 
BIBLIOGRAPHY 
functions in Ada," AC.M SlGPLAhr Notices 162 (February) 48-52. 
CORMACK, 
G +  V., R, N. S. H O R ~ O L ,  
ANII M. KA~SERSWERTH 
[1985]. "Practi- 
cal perfect hashing," Compter J. 28:1, 54-58. 
COURCELLE. 
3. [1984]. "Attribute grammars: definitions, analysis of depen- 
dencies, proof methods," in Lorho 119841, pp, 81-102. 
Couso~, P. 11981 I+ "Semantic foundations of program analysis," in Muchnick 
and Jones 1 198 1 1, pp. 303-342. 
Cousor, P. AND R . COWSOT 
1 19771. "Abstract interpretation: a unified lattice 
model for static analysis of programs by construction or approximation of 
f ixpoin t s, " Fourth ACM 
Symposium nn Princip1e.r of 
Programming 
Languag~s, 238-252. 
CURRY, 
H. B. AND R. Few ( I W8). Combim~ory h g i c ,  Vol. 1 ,  North-Holland, 
Amsterdam. 
DATE, C. J. 119861. An hrrductim IU D a t u h c  Syshms, 4th Ed., Addison- 
Wesley, Reading, Mass. 
D n v l ~ m ~ ,  
J. W. AND C. W. FRAEK [ 19801. "The design and application of a 
retargetable peephole optimizer," TOPUS 2:2, 
191-202. 
Errata 3:l 
(1981) 110. 
DAVIP~N, 
J. W. AND C. W. FRASER 
[I984a]. 
"Automatic generation of 
peephole optlmizat ions," ACM SICPLAN Nutices 19:6, 1 1 1-1 16. 
DAVIPSON, 
J .  W. 
AND C. W. FRASER 
1 l984b1. "Code dection through object 
d
e
 
optirnizat ion ," TOPUS 6 4 ,  505-526, 
DEREMEH, 
F. 11 %9]+ PrudiruI Trunslaioa for 
U ( k )  hngungw, Ph. D. 
Thesis, M.I.T., Cambridge, Mass. 
DERLMER, 
F, 1 197 11. "Simple LR{ k) grammars," Cmm. ACM i4:7. 453-460. 
DEREMER, 
F. AND T. PENHELLO [1982]. "Efficient camputation of LALRI 1) 
look-ahead sets,'' TOPUS 4:4, 61 5-649. 
DEMERS, 
A. J, [1975]. "Elimination of single productions and merging of 
nanterminal symbvls in LRC I) grammars." J. Comp~~ter 
Langwgvs 1 2 ,  
105-1 19. 
DENCKER, 
P,, K. DURRE, 
AND J .  HEUF~ 
[ 19841. 'cOptimi~ation of parser tables 
for poriablc compilers," TOPUS 6:4, 546-572. 
DER 
ANSART, P., M. JOURDAN, AND B. LORHO 1 19841- "Speding up circularity 
tests for attribute grammars, " Act# Informarica 21, 375-39i. 
DESPEY 
ROUX, T. 1 19841, "Executable specifications of static semantics," in 
Kahn, Maqueen, and Plotkin 119841, pp+. 215-233. 
~ 
DUKSTRR, 
E. W. 1 l960j. "Recursive programming," Num~risdie M~rh. 2, 

312-318. Reprinted in Rosen 11%7J, pp. 221-228. 
DIJK~RA, 
E. W. 119631. ''An Algol 60 translator for the X I  ," Annua! Rcvkw 
in A ~ r u m k  
Programming 3, Pergamm Press, New York, 329-345, 
DITZEL, D. AND H. R. MCLELLAN 
I1982j. "Register ailocation for free: the C 
machine stack cache ," Proc. ACM Symp. on Arr+kitet+tsrrd Support for Pro- 
gramming L a n p q p  and Opcruting Syskms, 48-56. 
DOWNEY, P. J. AND R .  SETHI [1978]. "Assignment commands with array refer- 
ences," .I+ 
ACM 254, 652666. 
DOWNEY, 
P. J., R .  SETHI, AND R. E. TARJAN 
119801. "Variations on the corn- 
man subexpression problem ." .I, 
ACU 27~4. 758-77 1 . 
EARLEY, J. [i970]. "An efficient context-free parsing algorithm ," Comm. 
ACM 13~2, 94-102. 
EARLEY, 
J. ( 1975a 1. "A rnbiguity and precedence in syntax description ." A m  
Informabcu 4:2, 183- 192. 
EARLEY, J. [1975b]. "High level iterators and a method of data structure 
choice, " J. Cumpwr Languages 1:4, 32 1-342. 
EMOFF, J. L. 119761. 14An analysis of same cornrne~cial PWI programs," 
JEEE Trans. Sojhvare Engitwering SE2:2, 1 1 3- 1 20. 
ENGELFRIET, 
5. .[ 19841. "Attribute evaluation methds," in Lorho 1 19841, pp. 
103- 138, 
ERSHOV, 
A .  P. i19581. "On programming of arithmetic operations," Comm. 
ACM 1:s (August) 3-6. Figures 1-3 appear in l:9 (September 19581. p. 
16. 
ERSHOV, 
A .  P. 119661. "Alpha - an automatic programming systcm of high 
efficiency," 1. ACM f 3:l, 17-24. 
ERSHOV, A. P, 119711. The A/& 
Auromrir Programming Sywrn, Academic 
Press, New York. 
FANG, 1. 1 197 21. "FOLDS, a declarative formal Language definition sysrcm," 
STAN-€372-329, Stanford Univ. 
FARROW, 
R . I 19841. "Generating a production compiler from an attribute 
grammar," IEEE Software 1 (October) 77-93. 
FARROW, 
R. AND D. YELLIN 
119851. "A ctsmparisun of storage optimizations in 
automatically-generated mmpilers," manuscript, Culumbia Un iv. 
FELDMAN, 
S. 1. 1 L979aI. "Make - a progrdm Ctw maintaining computer 

7m 
BIBLIOGRAPHY 
programs," Software--Practice and Experitnce 9:4, 255-265. 
FELDMAN, 
S. 1. [ 1979bf. "implementation of a portable Fortran 77 compiler 
using modern tools," ACM S I G P M  Notices M:8, 98-106. 
FTSCHER, 
M 
+ J. 119721. "Efficiency of equivalence algorithms," in Miller and 
Thatcher [ 19721. pp. 153-168. 
FLECK. 
A, C. [1976]. "The impossibility of content exchange through the by- 
name parameter ~ransmtssion technique," ACM S I G P W  No~ices 11:11 
(November) 38-4 1 .  
FLOYD, R 
+ W. 1 1961 1, "An algorithm for coding efficienl arithmetic expres- 
sions," Cumm. ACM 4:1, 42-51. 
FLOYD, 
R. W .  [1%3]. "Synlactic analysis and +rator 
precedence," J. ACM 
10:3. 3 !6-333, 
FLOYD, R. W. [ 19641. "Bounded context syntactic analysis," Comm. ACM 
7:2, 62-67. 
FONG, 
A .  C. [ 19791. "Automatic improvement of programs in very high level 
languages ," Sixrk Annual ACM Symposium on Principles of P o g r m h g  
Languagt-s, 2 1-28, 
FONG, 
A .  C. AND I. D. ULLMAN 
[1976]. "Indudion variables in very high-level 
languages," Third A~nuul ACM Synrposim mi Principles of Programming 
Languages, 104- 1 1 2. 
POSDICK, 
t. D. AND L. J. OSFERWHL 
119761. "Dam flow analysis in scrftwarc 
reliability ," Computing Surveys $5, 305-330, 
FOSTER, 
J. M. 119681. "A syntax improving program," Computer J. 11~1, 
3 1-34. 
FRA~ER, 
C. W. I19771. Aurumaric Ge~erution of Code Generams, Ph. D. 
Thesis, Yale Univ., New Haven, Conn. 
FR ASER, C+ W+ [ 19791. "A compact, machineindependent peephole q i m -  
izer," Sixth Annud ACM Sytnpusium on Principles of 
Prugmwwning 
Languages, 1 -6. 
FRASER, 
C. W. AND I). R. HANSOH 
[1982]+ "A machine-independent linker," 
&$ware--Praciice 
and Experience 12, 35! -366. 
FREDMAN, 
M. L., J .  KOMLOS, AND E. SZEMEREDI [1984]. "Storing a sparse 
table with 0 I I )  wo~st case access time," J. ACM 31~3, 538-549. 
FREGE, 
G. [ 18791. ++Begriffsscbrift, a formula language, modeled upon that of 
arithmetic, for pure thought," in Heijenwrt 119671, 1-82, 
FREIBURGHOUSE, 
R. A .  [1%9]. T h e  Multics PLIl compiler," AFIPS Full J u i ~  
Cmprrrer Cwference 35, 187-208. 

BIBLIOGRAPHY 
761 
FREIBURGHOUSE, 
R, A, 1 19741. "Register allocation via usage counts," Corn. 
ACM 1% 11, 638-642. 
FREUDENBERGER, 
S. M. 1 19841. "On the use of global optimization algorithms 
for the deteaion of semantic programming errors," NSO-24. New York 
Univ. 
FREWDENBERGER, 
5. M., J ,  T. SCHWARTZ, AND M. SHARIR lt9831. "Experience 
with the SETL optimizer," TOPUS $:I, 26-45. 
GAJEWSKA, 
H, 11975). "Some statistics on the usage of the C language, 
9 I 
AT&T Be!! Laboratories, Murray Hill, N. 
J. 
GALLER, 
0. A. AND M. J .  FIWHER 1!964\. "An improved equivalence alp- 
rithm," Comm+ ACM 7 5 ,  301-303. 
GANAPATHI, 
M. 11980]. Returgewbk Code Gtnerarion and Optimization using 
Artribute Grammars, Ph+ D. Thesis, Univ. of Wisconsin. Madison, Wis. 
GANAPATHI, 
M. 
AND C. N. FISCHER 
II98ZJ. "Description-driucn code gencra- 
tion using attribute grammars," Ninth ACM Sympasium on Principks of 
Programming h n g ~ a g e s ,  1 08- 1 19. 
GAHRPATHI, 
M., C. N. FIXHER, 
AND 1, L. HENNESSY 
119821, "Retargetable 
compiler code generation ." Cumpuring Surveys 1&4, 573-592- 
GANNON, 
J. D. AND 3. J. MORNING [ 19751. "Language design for programming 
reliability ," IEEE Tram. Software Engineehg SE4:2, 179- 191. 
GANZINGER, 
H., R. GIEGER~CH, 
U, MONCKE. 
AND R. WILHELM 119821. "A truly 
generative semanticsdirected compiler generator, " ACM S G P U N  Noticex 
17:6 (June) 172-184. 
G 
ANZINGER, H .  AND K 
+ R IPKEN 1 19801. "Operator identification in Ada, " ACM 
SKPLAN Nviices 152 (February) M-42. 
GEAR, C. W, [1%5), 
"High sped compilation of efficient objd code." 
Comm. ACM 8:8, 483-488+ 
GEKHKE, C. M. [19721. GIuba Progrum Oprirnizutims, Ph. D. Thesis, &pt. 
of Computer Science, Carnegie-Mellon Univ. 
GIEGERICH, 
R+ 119831. "A formal framework for the derivatim of machine- 
specific optirnizen," TOPUS 5 3 ,  422-448. 
GIEGERICH, 
R 
+ AND R. WILHELM [ I 9781. Mcounter-one-paus Features in one-, 
pass compilation: a formalintion using attribute grammars," fnjomwfim 
Prwessifig Lemrs 7:6, 279-284. 
GLAN~LLE, 
R. S. {IW?]. 
A Machine independent Alprirhm for Code 

Gencruriun mJ ils Use in Rew#etubk Compil~rs, Ph. D. Thesis, Univ. of 
California, Bcrkcley. 
GLANVILLE, 
R, S. AND S. L. GRAHAM 
11978J. "A new method for compiler 
code generation, " F$h ACM Sympusilrm on Principles 4 Pru~rurnming 
h n ~ u u g e s ,  23 1 -240. 
GRAHAM, 
R . M. 1 1964 1. 
''3ounded context translation ," AFIPS Spring Join1 
Cumputcr Cor(ercncc 40, 205-2!7. Reprinted in Rosen 119671, pp. 184- 
205. 
GRAHAM, 
$. L. 11980j. Tabledriven code generation," Cumpur~r I3:8, 25- 
34. 
GR~HAM, 
S. L. [1934]. *'Code generation and optimization," in Lorho 119841. 
pp. 251 -288, 
GRAHAM, 
$. L., C. B. HALEY, 
AND W. N+. 
JOY 119791. "Practical LR error 
recovery," ACM SlGPUN Nufires 148. 168- 175. 
GRAHAM, 
S. t.. M. A+ HARR~SON, 
ANU W. L, RUZZO Il98OJ. "An improved 
context-free recngniir~r ," TOPLAS k3,4 
15-462. 
GRAHAM, 
S. L. AND S. P. R ~ o n ~ s  
119751. "Practical syntactic error recovery." 
Comm+ ACM 18:11, 639-650. 
GRAHAM, 
S. L. 
AND M. 
WEC;MAN 119761. "A fast and usually linear algorithm 
for global data flow analysis,'' J+ ACM 231, 172-202. 
Ciunu, A. A., u+ HILL, AND H. LANGMAACK 
1 
Translation #j Abgd 60, 
Springer-Verlag, New York, 
HAN.WN, 
D . R . [ 198 1 1, "IS block structure necessary*? " Softwure-Pruc-iirc und 
Exprimrc 1 1, 853-866. 
HARRIWN, 
M. C. 119711. "Implementation of the substring test by hahing," 
C s s ~ .  
ACM 14:12, 777-779. 
HARRIWY. 
W. 11975). "A class of register allmation algorithms," KC-$342, 
l8M T. J .  Watsun Research Center, Yorktown Heights, N. Y. 
HARRIWN, 
W. 119771. 'Tompiler analysis of the valuc ranges fur variables," 
IEEE Trms. Srsfswr~re Engineuring 39. 
HECK. M .  S. 1 19771. Fkw Analjsds of Computer Progrms, North-Holland, 
Ncw York. 
HECHT, M. S. AND J .  €3. SHAFFER 119751. "Ideas on the design of a 'quad 
improver' for SIMPL-T, part I: oucrview and intersegment analysis," 
k p t +  of Computer Scicncc, Univ. of Maryland, College Park, Md. 
HECHT, M. S. ~ N I I  3. D. UUMAN [1972], "Flow graph reducibility," SIAM 3. 
Compurinx l, 188-202. 

HECHT, M. S. AND J. D. ULLMAN 
119741. "Characterizations of reducible now 
graphs," 1. ACM 21, 367-375, 
HECHT, M. 
S. AND I. D. ULLMAN 
119751. "A simple algorithm for global data 
flow analysis programs," SIAM J. Computing 4, 5 19-532, 
HEIJENOORT, 
J+ V A N  [1%7j. From Frqp ro GZidel, Harvard univ. Press, Cam- 
bridge, Mass. 
H~n~essu, 
J, [ I98 11, "Rogram optimization and exception handling," Eighth 
Annual ACM Symposium on Principks of Programmirig h p ~ ~ g e . ~ ,  
200- 
206. 
HENNEW, J + [ 19821. "Symbolic debugging of optimized d e , "  TOPLAS 43, 
323-W . 
HENRY, R. R. [ 19841. Grab-Ghnvitk C d c  Generators, Ph. D. Thesis, 
Univ, OF California, Berkeley* 
HEXT, 1. B. [ I%?]. 
"Compile time type-matching," Cumpurer J. 9, 365-369. 
HINDLEY, 
R. [1%9], 'The principal typescheme of an object in combinatmy 
logic," Trans. AMS 146, 29-60. 
HOARE, 
C .  A. R. [1%2a]. 
"Quicksort," Computer J. 5 1 ,  10-15. 
HOARE, 
C ,  A. R. 1!%2b[. "Report on the Elliott Algol translator," Computer 
J .  5:2, 127-129. 
HOFFMAN, 
C+ M. 
AND M. 
J. O'DONNELL [19821. "Pattern matching in trees," 
J. ACM Ek1, 68-95. 
HOPCROFT. 
J .  E. AND R. M. KARP 
11971 1. 
"An algorithm for testing the 
equivalence of finite automata," TR-71-! 14, k p t .  of Computer Science, 
Corneil Univ, See Aho, Hopcroft, and Ullrnan \1974J, pp. 143-145. 
HOPCROFT, 
J .  E. AND J .  D. ULLMAN 
( 1  9691. F o m l  LAmguages and Their Rela- 
rim €0 Autumtu, Addison-Wesley, Reading, Mass. 
HOPCROFT, 
j. E. AND J .  D. ULLMAN 
119731, "!kt merging algorithms," SIAM 
d. Cumparring 2:3, 294-303. 
HOPCROFT, J. E. AND J. D, ULLMAN 
1 19791. htrducriun m Auiomam Tkury. 
Lunguagcs, and Comprrrasion, Addison-Wedey , Reading, Mass. 
HORNING, 
J. J. [l9?6]. "What the compiler should tell the user." in Bauer 
and Eickel 119761, 
HORWITZ, 
L+ P., R. M. KARP, 
R .  E+ M~LLER, 
AND S. W[NDC~RAO 
1 l 9 W .  ''Indc~ 
register allocation," J ,  ACM l3:l, 43-6 
1. 
HUET, G. 
AND G .  KAHN (EM.) 
1197~51. Proving und hproving Programs, Cd- 
loque IRIA, Arc-et-Snans, France, 

HUET, G. 
AND J.-J. LEVY 
[ 1979). 'Call-by-need computations in nonambigu- 
ous linear term rewriting systems," Rapprt de Recherche 359, INRIA 
Laboria! Rocquencourt . 
HUFFMAN, 
D. A. 119541. "The synthesis of sequential machines," J. Frunklln 
h s r .  257, 3-4, 161, 190, 275-303. 
HUNT, 
3, W. 
AND M. D+ MCILRIN 119761. "An algorithm for differential file 
comparison, " Computing Science Technical Report 41, AT&T Bell 
Laboratories, Murray Hill, N. J. 
HUNT, J. W. 
AND T. G .  SZYMAIWI [19771. "A fast algorithm for computing 
longest common subsequences," Cumm. ACM 20:5, 350-353. 
HUSKEY, H. D+, 
M. H. HAWEAD, 
AND R. MCARTHUK 
119601. "Nehac - a 
dialect of Algol ," Comm. ACM 3:8, 4 6 3 4 8 .  
ICHBIAH, 
J ,  I). AND S. P. MORSE 1 l!VOj. 
L'A technique for generating almost 
optimal Floyd-Evans product ions for precedence grammars," Covrt. ACM 
13:8, 501-508. 
INGALLS, 
D. H. H. (19781. "The Smalltalk-76 pgrrrnhing aystsrn design and 
implementation," Fifrk Annual ACM Symposium on Principles qffrogrom- 
m i q  h j ~ u a g ~ s ,  
9- 16. 
INGERMAN, 
P. Z. [ 1%7). 
"Panini-Backus form suggested," Corm. ACM 10:3, 
137, 
IRONS, 
E. T. [ 1%1]. 
"A syntax directed compiler for Algol 60," C m .  ACM 
41, 51-55. 
IRONS, E. T. [t963]. "An error correcting parse algorithm," C u m .  ACM 
6:11, M9-673, 
IVER~N, 
K .  1 19621. A Programming Languuge, Wiley, New York. 
JANAS, J. hi+ [1980]. "A comment on "Oprator identification in Ada" by 
Ganzinger and Ripken ," ACM SICPLAN Norices 159 (September) 39-43. 
JARVIS, J. F. 1l976]. "Feature recognition in line drawings using regular 
expressions," Prw. 3rd M. Joint Corrf. on Pat&rn Recognition, 189- 192. 
JAZAYERI, M ., W. F+ OGLIEN, 
AND W. C .  ROUNDS [1975J. ''The intrinsic 
exponential complexity of the circularity problem for attribute gram- 
mars," Comm. ACM 18:12,697-706. 
JAZAYERI, M, AND D. POZEFSKY 119811. "Sjme-efficient storage management 
in an attribute grammar evaluator," TOPLAS 3:4, 388-404. 
JAZAYERI, M. A W  K .  G, WALTER 119751. "Alternating semantic evaluator," 
Prw. ACM AnnuaC Cunferenre, 230-234. 
JENSEN, K, AND N, WIRTH [1975), Pmcd User MatturrI and Repun, Springer- 

Verlag, New York. 
JOHNWN, 
S. C. 119751. "Yacc - yet another compiler compiler," Computing 
Science Technical Report 32, AT&T Bell Laboratories, Murray Hill. N. J, 
JOHNWN, S. C. 119781. "A portable compiler: theory and practice." Flfrh 
A n n d  ACM Symposium on Principles u j  Prsjgrurrtming Lunguugr.s, 97- 104, 
JOHNWN, $, C. 1 19791. "A lour through the porhble C cumpilcr." AT&T Bell 
Laboratories, Murray Hill, N. J. 
JOHNSON, 5. C. [ 19831. "Code generation for silicon." Ttmh AnnuaF ACM 
Symposium on Principh tf Programming kinguugrs, 14-19. 
J~HNSISN. S .  C. AND M. E. LESK 119781. "Language development tools," BeH 
Sys~ern Technird J+ 57:6, 2 1 55-2 175, 
J O H N ~ N ,  S. C. AND D. M. RITCHIE I I V X I ] .  
"The C languagc calling 
sequence," Computing Science Technical Report 102, AT&T Bet l Labora- 
tories, Murray Hill, N. J .  
JOHNSON, 
W. L.. J .  H. PORTER, S. I. A ~ K L E Y ,  
AND D. T. ROSS 119681. 
"Automatic generation of efficient lexical processors using finite state 
iechniques," Comm. ACM 11:12, 805-813. 
J~IL~AT, M. L. 1 19761. "A simple technique for partial ehminot iun of unit pro- 
ductions from LRIR) parser tables." IEEE Trutrs. OH Cnmprrrer.~ C-2.97, 
763-744. 
JONES, N. D. AND C+ M+ M A ~ N  
1 19801. "Attribute-influenced LR parsing," 
in Jones 119801, pp. 393-407. 
JUNES, N+ D. hHn S. S. MUCHNICK 
119761. "Binding tinic optimization in pro- 
gram ming languages," Third ACM Svmposium tm f hcipks oj' Prqrum- 
m istg h n g u u ~ ~ s ,  
7 7 -9 4. 
JOURDAN, M. ( 19841, 
"Strongly noncircular attribufe grammars and their 
recursive evaluation," ACM SIGPUN Noricys 19:6. 8! -93. 
KAHN, G . ,  D. 3. MACQUEEN, 
 AN^) G. PLOTKIN ( 19841. Semclrrrics of Dafu 
Types, Lecture Notes in Computer Science 173, Springer-Verlag Berlin. 
KAM, J. 6. AND +f. D. ULLMAN 
119761. "Global Jara flow analysis and iterative 
algorithms," J .  ACM U:1, 158-1 7 1 .  
KAM, J+ 0. AND J .  D. ULLMAN 
119771. "Monotone data flow analysis frame- 
works," A m  i@rm~rlru 7:3, 305-3 b8. 

766 BIBLIOGRAPHY 
KAPLAN, 
M, AND J. I). ULLMAN 
[ 19801. "A genera! scheme for the automatic 
inference of variable types," J. ACM 27~1, 128- 145. 
K A ~ A M ~ ,  
T. 11%5]. 
"An efficient recognition and syntax analysis algorithm 
for context-free languages," AFCR L-65-758, Air Force Cambridge 
Research Laboratory, Bedford, Mass. 
KASAUI, T., W. W. PETERSON, 
AND N. TOKURA 
119731. Y h  the capabilities of 
while, repeat, and exit statements," C u m .  ACM 16:& 503-51 2. 
K A ~ H S ,  
U 
+ I 19801, "Ordered attribute grammars," Actu Inforrnatim 13:3, 
229-256. 
K n m ~ s ,  U ., B. Hum, AND E. ZIMMERMANN 
ll932j. GAG: A Practical 'Corn- 
piler Generator, Lecture Notes in Computer Science 141, Springer-Verlag, 
Berlin. 
K m n ~ a v ,  
V . N . ( 19731. ''Some properties of fully reducible graphs," InJbr- 
mation Prwemhg Letters 24, 1 1 3- 1 17, 
KATAYAMA 
, T. [ 1984 1. 
"Translation of attribute grammars into procedures,*' 
TOPUS 6:3, 345-369. 
KENNEDY, 
K. I19711. "A gbbal flow analysis algorithm," Infern. 3. Computer 
M& 
Section A 3, 5+lS. 
KENNEDY, 
K, i19721. "Index register allocation in straight Line d
e
 and sim- 
ple loops," in Rustin [1972], pp, 51-64. 
KENNEDY, K. 1 19761. 
"A comparison d two algorithms for global flow 
analysis," SIAM J .  Computing 51, J 58-180. 
KENHEDY, K. 119811. 
"A survey of data flow analysis techniques," in 
Muchnick and Jones 1 198 1 1, pp. 5-54. 
KENNEDY, 
K. AND J. RAMANATHAN 
1 19791. "A deterministic attribltte grammar 
evaluator based on dynamic sequencing," TOPUS l:l, 
142-160. 
KENNEDY, K. 
AND S. K. WARREN 
1 19761. ''Automatic generation of efficient 
evaluaiors for attribute grammars," Third ACM Sympmiurn on Prhcipks 
of Prqrarnming hnguuges, 32-49. 
KERN~GHAN, B. W. 119751. "Ratfor - a preprmssor for a rational Fortran," 
Suftw~w-Praciitte and Experience 5;4, 395 -406. 
KERNIGHRN, B. W. [19821. "PIC - a language for typesetting graphics," 
Software-P roctire uruf Experience 12: 1, 1 -21 + 
KEBNIGHAN, 6. W. AND L. L. CHEKRY 11975j. "A system for typesetting 
mathematics," Comm. ACM 183, 15 1 - 157, 
KERHIGHAN, B. W . AND R. P ~ K E  119841. The UNIX Prugrmminx Environment, 
Prent ice-Hall, Englewwd Cliffs, N. I. 

BIBLIOGRAPHY 
767 
KERHIGHAN, B. W. 
AND D, M, R ~ H I E  
j 19781. The C Pro#rummin,y Lung~uge, 
Prent ice-Hall, Englewood Cliffs, N. J. 
KILDALL, G. 119731. "A unified approach to global program optimization," 
A CM Sympsiurn rn Princlpks of Progrurnrnhtg hnguuges, 1 94 -206. 
- 
KLEENE, S. C+ 119561. "Representation of events in nerve nets," in Shannon 
and McCarthy 119561, pp. 3-40. 
KNUTH, D. E+ [1%2j. "A history of writing compilers," Cumputurs' und A&w- 
mation (December) 8- 1 8. Reprinted in Pollack 1 19721, pp. 38-56. 
KNUTH, D. E. 11964.1. "Backus Normal Form vs, Backus Naur Form," Cmm. 
ACM 7:12. 735-736. 
KNUTH, D. E. 11%5]. "On the translation of languages from Id1 to right," 
Informatim and Conrrol 8:6, 607-639. 
KNUTH, D. E. [ l%X]. 
"Semantics of context-fret languages," Ms~drrrnatlcui 
Systems Theory 2:2, 127- 145. Errata S:1 ( 1971) 95-96. 
KNUTH, D. E, 1 197 1 a 1. 
"Top-down syntax analysis," Artu imforma~irrr 1:2, 
79-1 10. 
KNUTH, D. E. 11971bI. "An empirical study of FORTRAN programs." 
S u ~ t w a r e P m ~ ~ i c e  
and Experience 1 :2, 1 05- 1 33. 
KNUTH, D+ 
E. 11973aj. T h  Art uf Comp~rcr Prugrumrning: VoI. 1, 2nd. Ed., 
Fundamentui Algorithms, Addison- Wesley, Reading, Mass. 
KNUTH, D, 
E. 1 l973b1. The Arr L$ Computer Pru~rmming: Vol. 3, Sorting and 
search in^. Addison-Wesley , Reading, Mass. 
KNUTH! D. E. 1 19771. "A generalization of Dijkstra's algorithm," InJurmarion 
Processing h i i e r s  6, 1-5. 
KNUTH, D. E. 11984a). The @Yhuk, Addison-Wesley, Reading, Mass. 
KNUTH, D. E. 1 l984bl. "Literate programming," Cosnpurrr J .  28:2, 97- 1 1 1. 
K ~ u m ,  D+ E. 1 1985,19861. Complrrcrs and Typ~scrti~g, 
V d .  I 
: 7&Y, Addison- 
Wesley, Reading, Mass, A preliminary version has been published under 
the title, "m: The. frogrum." 
KNUTH, D. E., J .  H .  MORRIS, AND V +  R. PRATT 119771. "Fast pattern marching 
in strings," SIAM b. Cornpiring 6:2, 323-350. 
KNUTH. D. E. AND L. T R A ~  
PARDO [1977]. "Early deve;elopment of program- 
ming languages, " En~~yclopebiu of Conylucer Science and Tcr+nolr~gy 7, 
Marcel Dekker, New York, 4 1 9-493A 
KORENIAK, 
A. J. 11%9). "A practical methd for constructing LR(k) proces- 
sors." Cmm. ACM W11, 613-623. 

768 BIBLIOGR APHY 
KOSARAJU, 
S, R, 1 19741. "Analysis of structured programs,'' J. Computer m d  
System Sci~rrces 9:3, 232-255. 
KOSKIMIES, 
K + AND K, 
-J . RAIHA 119831. "Modelling of space-efficient one-pass 
translation using attribute grammars," ~f~ure-Prunice and Exp~rierrcc. 
13, 119-129. 
K o ~ R ,  
C. H. A +  119711. "Affix grammars," in Peck 119711, pp. 95-109. 
Lou, L. [1977]. "On livedead analysis for global data flow problems," 6. 
ACM 24:s. 473-483, 
KRISI-ENSEN, 
3. 8. AND 0. 
L. MADSEN 119811. "Methods for wmputing 
LALR(C) kmkahead ," TOPUS 3 1 ,  60-82. 
K~ors, H. [ 19751. Tree Ternpbts and Subtree Trun~ommrional Grammars + Ph + 
D, Thesis, Univ. of California, Santa Cruz. 
LALONDE, 
W + R. [I971 1, "An efficient LALR parser generator," Tech. Rep. 
2, Computer Systems Research Group, Univ. of Toronto. 
LALONDE, 
W ,  R ,  119761. "On directly constructing LR(k) parsers without 
chain reductions," Third ACM Sjrnposi~m on Principles of Programming 
Languag~s, 127- 133. 
LALONDE. 
W. R., E. S. LEE, 
AND J. J. MORNING [f971]+ "An LALR(k) parser 
generator ," Pruc. IFIP Congrw 71 TA-3, 
North-Hol land, Amsterdam, 
153- 157. 
LAMB, D. A. [ 198 1 j. 
"Construction of a peephole oplimizer," Sofrwarr- 
Practice und Experience I 1, 638-647, 
L M A ~ N ,  
€3, U' , (I982j. "Fast procedure calls," ACM SIGPLAN Hvtim 1?:4 
(April) 66-76. 
LANDIN, 
P, J . 11 9641. 'The mechanical evaluation of expressions," Computer 
J. 6:4. 308-320. 
LECARME, 
0. 
AND M .-C. 
PEYROLLE-THOMAS 
[ 19781. "Self-wmpiling compilers: 
an appraisal of heir implementation and prtability," Soffwure-Prucrice 
unb Experience 8. 146- 170. 
LEDGARI), 
H. F. !1971]. "Ten mini-languages: a study of topical issues in pro- 
gramming languages ," Cornpihg Surveys 3:3, 1 1 5- 146. 
LEINIUS, 
R. P. [W70]. Error Defection and Recovery fur Syrrtux Directed Cum- 
piler Sys~cms, Ph. D. Thesis, University of Wisconsin, Madison. 
LENGAUER, 
T. AND R .  E. TARJAN 
119791. "A fast algorithm for finding domi- 
nators in a flowgraph," TOPLAS 1, 121-141. 
LESK, M. E, \1975]+ "Lex - a lexical analyzer generator," Computing Science 
Technical Report 39, AT&T Bell Laboratories, Murray Hill, N. I .  

LEVERETI-, 
0. W. [1982], ''Topics in code generation and register allocation," 
CMU CS82- 130, Computer Science Dept . , Carnegis-Mellon U niv., Pitts- 
burgh, Pennsylvania, 
LEVERETT, 
B. W., R. G .  G. CATTELL, 
S. 0. 
HOBBS, 
J A  M. NEWCOMER, 
A +  H, 
RELNER, 5. R. SCHATZ, AND W. 
A. WULF [1980]. "An overview of the 
pruducticin-quality compiler-cornpiIer project," Computer 13:8, 38-40. 
LEVEREIT, 
B. W. 
AND T. G. SZYMANW I i980]+ ''Chaining span-dependent 
jump instructions," TOPLAS 2:3, 274-289. 
LEVY, 
J. P. I1975j. "Automatic correction of syntax errors in programming 
languages," Acra Iitforrnarica 4, 27 1-292. 
LEWIS, P. M ., 11, D, J .  ROSENKRANTZ, 
AND R. E. STEARNS 
{l974]. "Attributed 
translations ," J. Computer and System Sciences 9:3, 279-307. 
.LEWIS, 
P. M., Il, D. J. ROSENKRANTZ, 
AND R. E. STEARNS [19761. Compiler 
Design T h e y ,  Addim-Wesley, Reading, Mass. 
Lew~s, P. M,, I1 AND R. E. STEARNS [1968]+ "Syntax-directed transduiiian ," 6, 
i 
ACM 153, 465-488. 
LORHO, B. [1977]. "Semantic attribute processing in the system Delta," in 
Ershov and Koster [ 19771, pp. 2 140. 
LORHO, 
B. [ 19841. Methods and Tools for Compiler Cunstrtrc~ion, Cambridge 
Univ. Press, 
LORHO, 8. AND C. PAIR [1975]. "Algorithms for checking consistency of attri- 
bute grammars," in Huet and Kahn 119751, pp. 29-54. 
Low, J, AND P. ROYNER [1976j. 'Techniques for the automatic sdection of 
data structures," Third ACM Symposium on Principks of Pr~grumming 
Langarages, 58-67. 
LOWRI', E. S, AND C. W. MEDLOCK 
119691. 
"Object code optimization," 
Corn. ACM 12, 13-22. 
LUCAS, P. [l %I]. 
"The structure of formula translators," Elekrrurtisclre 
Rechemlagen 3, 159- 1 66. 
LUNDE, A .  [1977]. "Empirical evaluation of some features of instruction set 
processor architectures," Cumm. ACM 209, 143-1 53. 
LUNELL, 
H. [1983}. Cde Generator Writing Sysmns, Ph. D. Thesis. Linkiiping 
University, LinkBping, Sweden. 
MACQUEEN, 
D+ B., G .  P, PLOTKIN, 
AND R. SETHI [19841. "An ideal model of 
recursive polymorphic types," Ekuenth Annual ACM Sy~?~&um on Prim+ 
pdes of Prugmmm irg Languages, 1 65 - 1 74. 
MADSEN, 0, 
L. [19W]. "On defining semantics by means of extended 

attribute grammars," in Jones [1980], pp. 259-299. 
MARILL, 
T. [ 19621. "Computational chains and the simplification of mmpu ter 
programs," IRE Trans. Eimrunic Coquters EC-1 l:2, 173- 1 80. 
MARTELU, 
A. AND U. MONTANARL 
[ 19821. "An efficient unification algo- 
rithm," TOPLAS42, 258-282. 
MAUNEY, 
J. AND C. N4 FITHER [1982]+ "A forward move algorithm for LL 
and LR parsers, " ACM SNXL4iV Norices 17~4, 79-87. 
MAYOH, 
B. H . [ 198 11. 
"Atlribute grammars and mathematical semantics," 
SIAM J .  Cornpuling 1Q3, 503-5 18. 
MCCARTHY, 
J .  [ 19631. "Towards a rnathemat ical science of computation," 
Informrim Prueessiq 1962, North-Holland, Amsterdam, 21-28. 
M~CARTHY. 
J. [1981]. "History of Lisp," in Wexelblat [198tJ, pp. 173-185. 
MCCLURE, R +  M. [t965]. "TMG - a syntax-directed compiler," Prm. 20th 
ACM National Conf.. 262-274. 
MCCR 
ACK EN, N . J . [l979]. An Invtssig~tion 0.f a Prqrammhg Language with u 
Pdymorphic Type Ssruci~re, Ph. D. Thesis, Syracuse University, Syracuse, 
N. Y .  
M~CULLOUGH, 
W .  $. AND WA PITTS [1943]. "A logical calculus of the ideas 
immanent in nervous activity," BrrlIeiin ofMafh. Biophysics 5, 115-133. 
MCKEEMAN, 
W. M. I 19651. "Peephole optimization ," Comm. ACM 8:7, 443- 
444. 
MCKEEMAN, 
W. M. [1976]. "SymM table access," in Bauer and Eickel 
[ 19761, pp. 253-30 1 . 
MCKEEMAN, 
W. M+, 
J +  J. HORNING, 
AND D+ B+ WORTMAN [1970]+ A Compiler 
Gmeraior, Prentice-Hall, Englewmd Cliffs, N. J. 
MCNAUCHTON, 
R .  AND H7 YAMADA [1960]. "Reguhr expressions and state 
graphs for automata," !RE Truns. on Elec~ronic Compiirm EC-9;1. 38-47. 
MEERTENS. 
L. !1983/. "Incremental plymorphic type checking in B," Tenth 
ACM Symposium on Principles of Progrurttming Languages, 265-275. 
METCALF, 
M . [ I 9SZ17 Fortran Opsimizatiun, Academic Press, New Y ork . 
MILLER, 
R. E. AND J. W. THATCHER 
(EDS.) 119721. Complexity of Computer 
Compurati~ns, Academic Press, NEW York, 
MILNER, R .  [19781. ''A theory of type polymorphism in programming," J ,  
. 
Computer rrnd S p e m  Sciences 17~3, 348-375, 
MILNER, 
R . [ 19841. "A proposal €or standard ML," ACM Synpsim on Lisp 
and Ft.ini:tioml Programmifig, 184- 197. 

M~NKER, 
J. AND R. G. M~NKER 
119801. "Optimization of M e a n  expressions- 
historical developments," A. of thc History OJ Compuiing 2:3, 227-238. 
MITCHELL, 
J. C. I1984]. "Coercion and type inference," Eleventh ACM Symp- 
s i m  ofi Primip~es of Programming hrrguages, 175- 1 85. 
MOORE* 
E. F. i19561. Wedanken experiments in sequential machines," in 
Shannon and McCarthy 119561, gp. 129-153. 
M o a ~ t ,  E. 
AND C. RENVOISE f19791. "Global optimization by suppression of 
partial redundancies," C u m ,  ACM 22, %- t 03. 
MORRIS, J , H . I 196881. L a ~ ~ - C a ~ c u l s  
Models of Programmittg hngcragm, 
Ph+ D. Thesis, MIT, Cambridge, Mass. 
MORRIS, R. [lWb]. "Scatter storage techniques," Cmm. ACM t l :l, 38-43 
Moses, J. [1970]. 'The function of FUNCTION in Lisp," SIGSAM Buiktist 15 
(July) 13-27. 
MOULTOH, P. G. AND M. E. MULLER 11%7]. 
"DITRAN - a compiler 
emphasizing diagnostics," C m m .  ACM 1k1, 52-54, 
MUCHNWK, 
S. S. AND N. D. JONES 11981 1. Program Ffow Analysis: Theory unb 
Applications, Prentice-H all, Englewood Cliffs, N. J . 
NAKATA, I. [1%71. 
"On compiling algorithms for arithmetic expressions," 
Corn. ACM 10:8, 492-494. 
NAUR, P. (ED.) 119631. "Revised report on the algori~hmic language Algol 
60," Comm. ACM 61, 1-17. 
NAUR, P. [1%5]. 
"Checking of operand types in Algol compilers," BIT 5, 
151-163. 
NAUR, 
P. [ I  9811, "The European side of the last phase of the devebpment of 
Algol 60," in Wexelblat [1981], pp. 92-139, 147-161. 
WEWEY, M. C., P, C, POOLE, AND W, M. WAWE [1972]+ "Abstract machine 
modelling to produce portable software - a review and evahtatim," 
S ~ r e 4 r a c t i c c  
und Exp~ritwce 2:2, 107- 1 36. 
NEWEY, M. C. AND W. M. WA~TE 11985). '"The robust implementation of 
sequen~e-conrralkd iteration, " Solfrware-P roctire and Experience 15:7, 
655-668. 
N ICHOLLS, I. E. 1 I 975 ] + The Structure iuid Design of Programming Languages, 
Addison-Wesley, Reading, Mass. 
NWERGELT, 
J. 1 I %5j. '*on the automatic simplification of computer code,*' 
Cmm. ACM 8:6, 366-370. 
Now, K .  V + ,  U. AMMANN, K. JEPISEN, H .  H, NAGELI, 
AND CH, JACDBI I1981j. 
"Pascal P implernentarion notes," in Barron [19811, pp. 125- 170. 

772 
BIBLIOGRAPHY 
OSFERWEIL, 
L. J. (l98lI. "Using data flow twis in wftware engineering," in 
Muchnick and Jones 1 1  98 I ] ,  pp. 237-263. 
PAGER, D. [1977a]. "A practical general m e t h d  for constructing LR(k) 
parsers ," Acm inJurmrica 7, 249-268. 
PAGER, D. 1 l977bI. "Eliminating unit productions from LR(k) parsers," A m  
Jnfiwrnatim 9, 3 1-59. 
PAI, A. B. AND R .  B, KIEBURTZ 
119301. "Global context recovery: a new stca- 
tegy for syntactic error recovery by rablc-driven parsers," TOPLAS 2:1, 
18-41 
+ 
PAWE, R. AND J. T. SCHWARTZ 119771. "Expression continuity and the formal 
differentia tion of algorithms, " Fourth ACM Symposium on Prinuipks of 
Prugrumming Languages, 58-7 1 . 
PALM, 
R .  C., JR. (19751. "A portable optimizer for the language C," M. Sc+ 
Thesis, M IT, Cambridge, Mass. 
PARK, J. C. H., 
K ,  M. CHOE, AND C. H. CHANG 
11985]. "A new analysis of 
LALR formalisms," TOPLAS 7:1, 159-175. 
PATBRSDN, 
M+ 5. AND M. WEGMAN 
[1978]. "Linear unification," 3. Cumpurer 
and System Sciencrs 16:2, 158-167. 
PENNELLO, T. AND F. DEREMER 
[1978]. ''A forward move algorithm for tR 
error recovery." F$h Annual ,4CM Symposium un Principie.~ oJ Prupwn- 
ming Languuges, 24 1-254. 
PENNELLO, 
T., F. DEREMER, 
AND R, MEYERS 
[ t 9801. "A simplified operator 
identification scheme for Ada ." ACM SIGPUN Notices 157 {July- 
August) 82-87. 
PERSCH, 
G,, 
G +  WINTERSI-EIN. M. DAUSSMANN, 
AND S+ DRO~POULOU 
119801. 
"Overloading in preliminary Ada?" ACM SIGPLAN Notiws 1511 
(November) 47-56. 
PETERSON, W, W. 
11957). "Addressing for random access storage," IBM J .  
Research und Development 1 :2, 1 30- 1 46. 
POLLACK, B. W. 119721. Compiler Techniqws, Aucrbach Publishers, Prince- 
Ion, N. J. 
POLLOCK, L, L. AND M+ L. WFFA 119851. "Incremental compilation of lcicslly 
optimized code," TweptSrA Annual ACM Symposium OH Principles of Pru- 
gramming Langwges, 152- 164, 
POWELL, 
M. 
L. 119841. "A portable optimizing compiler for Modula-2," ACM 
SIGPLAN Notires 196, 3 10-3 18. 

PRATT, T. W . 1 19841. Prugrumrning Lattguuges: Design and l~/cmenruriun, 
2nd Ed., Prentice-Hall, E n p l e w d  Cliffs* N+ J .  
PRATT, V. R. 119731. "Top down operator precedence," ACM Sympnsium on 
Princ*ip€es of P rogrummirrg La~guuge.~, 
4 1 -5 1 . 
PRICE, C. E. 1 197 1 j. "Table lookup techniques," Computing Surveys 3:2, 49- 
65, 
PROSER, 
R. T. 119591. "Applications OF boolean matrices to the analysis of 
flow diagrams," AFIPS E m ~ e r n  Jmhd C m t p u ~ ~ r  
Cunf., Spartan Books, Bal- 
timore, Md., 133- 138. 
PURMM, P. AND C. A. BROWN 119801. 
"hlantic routines and LR(k) 
parsers," A&I 
lnformuria 14:4, 299-3 15. 
PURDCIM, 
P. W .  AND E. F. MOORE 
119721- "Immediate predorninators in a 
directed graph," Comrn. ACM 15:8, 777-778. 
RABIN, M. 0 .  AND D. &om+ 119591, "Finite automata and their decision 
problems ," IBM J. Rrsmrch und Dwe!oprne~t 3:2, 1 ! 4- 125. 
RAMPI, G +  AND H. P+ ROGOWAY 119651. "NPL: Highlights of a new program- 
ming language." Comrtr. ACM 8: 1,9- 17. 
R h i ~ h ,  K . -J . \ !9$ l ] . A Spwe Munagt~unr Tuchniqw $for Addti-Puss A w i h t c  
E P U ~ L I ~ O ~ S ,  
Ph. D. Thesis. Report A-1981-4, Dept. of Computer Science, 
University of Helsinki, 
R n i ~ n ,  K.-J. AND M. SAARINEN 119821. "Ttsttng attribute grammars for circu- 
lari ty, " Aciu !nformatku 17, 185- 192. 
R m n ,  K.-J., M. SAAR~NEN, M. 
%KJAKOSKI, S. SIPPU, E+ % ~ I S A L O N - ~ I N ~ N E N ,  
AND M. TENARI 
119831. "Revised report on the mmpiler writing system 
HLW8," Report A-1983-1. Dept, of Computer Science, University of 
Helsinki. 
RANDELL. B, AND L. I ,  RUSSELL 119641, A l g d  60 Impkmcn~rion, Academic 
Press, New Y ork. 
REDZIEJOWSKI, 
R +  R .  1i9691. "On arithmetic expressions and trees." Cnmrn. 
ACM 12:2,8 1-84. 
REIF, J ,  H. 
ANT, H .  R .  LEWIS t19771. "Symbolic evaluation and the global 
value graph," 
Fourrh ACM Sympr~siwn on Principks uf Prugrurnming 
L a n g ~ a g w ~  
104- 1 1 8, 
 RE^ S. P. 1 19831. "Generation of compiler symbol processing mechanisms 
from specifications," TOPLAS 5:2. 127- 163. 

REYNOLDS, 
J + C, [1985]* 'Three approaches to type structure," Machemaricul 
F ~ ~ i o m  
of Software Developmenr, Lecture Notes in Computer Science 
185, Springer-Verlag, Berlin, 97- 138. 
RICHARDS, M. [1971]. "The portability of the BCPL compiler," Software- 
Pmcrke and Experience 1:2, 135-146. 
RICHARDS, M. [19771. "The implementation of the BCPL compiler," in P. J +  
Brown (ed.) , Sofnuare Portability: An Advanced Course, Cambridge 
University Press. 
RIPKEN, K. [1977]. "Formale kschreibun vim maschinen, implementierungen 
und optimierender rnasch inen-cdeerzeugung aus 
attributierten pro- 
grammgriphe," TUM-INFO-773 1, lnstitut fur infmmatik, U niversitiit 
Miinchen, Munich. 
RILEY, G. D+ 
AND F+ C, DRUSEIKIS 
[197#]. "A statistical analysis of syntax 
errors," Comp~er 
Languages 3,227-240. 
, 
RITCHIE, 11. M. 119791. ''A tour through the UNlX C compiler,'' AT&T Bell 
Laboratories, Murray Hill, N. J. 
RITCH~E, D. M. 
AND K .  THOMPSON 
[1974]. "The UNlX time-sharing system," 
Cumm. ACM 17:7, 365-375. 
ROBERTSON, 
E. L. [1979]. "Code generation and storage allocation for 
machines with span-dependent instructions," TOPLAS 1: 1, 7 1-83. 
 
- 
. 
. 
' ,  
Rosrprm~, J. A. I1%5]. 
"A machine-oriented logic bawd on the resolution 
principle," J .  ACM 121, 23-41. 
ROHL, J, S. (19751. An lntrodttction to Campifer Writing, American Elsevier, 
New York. 
ROHRICH, 
J. [ I  9XOI. "Methods for the automatic construction of error correct- 
ing parsers," A
m
 fnfurmaiica 13:2, 1 15-139. 
RUSEN, B, K. 119771. "High-level data flow analysis," Cumm, ACM 20, 7 12- 
724. 
ROSEN, 0. K. [l980], "Monoids for rapid data flow analysis," $]AM J .  Cm- 
puling 9 I, 159- ! %. 
ROSEN, S. [ 19671. Programming Sysrems and Languages, McGraw-H ill, New 
York. 
ROSENKRANTZ, 
D. I .  AND R. E. STEARHS [1970]+ "Properties of deterministic 
top-down grammars," Inforrna~ion and Cultfro/ 179, 226-256. 
ROSLER , L. [ I  9W3. "The evolution of C - 
past and future, " AT&T Bell Labs 
Technical Journal 63~8, 1685-1699. 
R US~IN, R . [ 19721, Design and Ophization of 
Cumpiiers, Pren t ice-H all, 

Englewood Cliffs, N .J . 
RYDER, B. 0. 11979). "Constructing the call graph of a program," lEEE 
Trans. S#.ure Enginwring SE-53, 2 1 6-226. 
RYDER, B. G . 119831. "Incremental data flow analysis," Tenth ACM Sympn- 
sium on P rincipks o j  Programming Languages, 1 67- 1 76. 
SAARINEH, M. 11978). "On constructing efficient evaluators for attribute 
grammars, " Aukwnata, Languages an J Programming, Fifh Cuiloquicrm, 
Lecture Notes in Computer Science 62, Springer-Verlag, Berlin, 382-397. 
SAMELSON, K. AND F. L. BAUER IMO1+ "?quential 
formula translation," 
Cmm+ ACM 3:2, 76-83. 
SANKOFF, D. AND J. B. KRUSKAL 
(EDS.) [3983]. Time Warps, Siring Edirs, and 
Mucrmotede$: The Theory urtd Pruciiw of 
Scqwfice Comparison, 
Addimn-Wesley , Reading, Mass. 
SCARBOROUGH, R. G .  AND H. G. KOLSKY 119801. "Improved optimization of 
Fwtran object programs," MA4 J. Research and Dewhpm~nr 246, 660- 
676. 
~ H M F E R ,  M . 1 19731. A Marhrma~icul Thsary uf Global Prugrarn Opimira~imt, 
Prentice Hall, Englewcad Cliffs, N. J. 
SCHONEERG, 
E., J. T. SCHWARTZ, ANT) M. SHARIR 11981 j. "An automatic tech- 
nique for selection of data representa~ims in SETL Programs," TOPLAS 
3~2, 
126-143. 
PHORRE, 
D. V. 119641. 
"Meta-11: 
a syntax-oricnted compiler writing 
language," Proc. 19th ACM Nuriotad Cclmf., Dl.3-1 - D l  3 - 1  1 .  
~ H W A R T Z  
, J . T+ 
1 19731. On Prtqromming: An Inicrirn Report on the SETL Pu3- 
j m ,  Couranr Cnst., New York. 
~ H U N A R T Z ,  
J. T. 11975aj. "Automatic data structure choice in a language of 
very ,high level," Cumm. ACM 18: 12, 722-728, 
SCHWARTZ, 
1. T. I191Sbj. "Optimization of very high level ianguages," Cum- 
purer Lonptt~ges. Part I: "Value transmission and its corollaries." 1:2. 
161- 184; part 11: "Deducing relationships of inclusion and membership," 
I:3, 197-218. 
SEDGEWICK, R . [ 19781. "lmpkmenting Quicksort programs," Cornm. ACM 21, 
847-85'1 
+ 
SETHI, R. 119751. "Complete register allocation problems," SIAM J. Compul- 
i q  49, 226-248. 
&THI. 
R .  AND I. D. ULLMAN 
119701. T h e  generation of optima! code for 
arithmetic expressions," J. ACM 17:4, 715-728. 

776 BIBLIOGRAPHY 
SHANNON, 
C .  AND J .  MCCARTHY 
1 1956). Autumtu Studies, Princeton University 
Press. 
SHER~DAN, P. El. 1 19591. "The arithrnet ic translator-mrnpiler of the ISM For- 
tran automatic coding system ," Corn. ACM 2:2, 9-2 1. 
SHIMASAKI, .M., 3. FUKAYA, 
K .  IKEDA, 
AND T. KJYONO 
I1980J. "An analysis of 
Pascal programs in compiler writing," Software-Practice 
a d  Experimce 
lk2, 149-157. 
SHUS~EK, L. I. 1 l978l. "Analysis and performance of computer instruction 
sets," $LAC Report 205, Stanford Linear Accelerator Center, Stanford 
University, Stan ford, Cahforn ia. 
SIWU, S. [1981]. "Syntax error handling in compilers,*' Rep. A-1931-1, k p t .  
of Compukr Science, Univ. of Helsinki, Helsinki, Finland. 
SPW, 
S. AND E . ~ M L O N - ~ N J N E N  
11983). "A syn taxcrror-handling tech- 
nique and its experimental analysis," TOPUS 54, 656679. 
SOISALON-SOININEN, 
E. 1 19801. "On the space optimizing effect of eliminating 
single productions from LR parsers," Actu itzformu~icu 14, 157-174, 
SDISALCIY-SOININEN, 
E. AND E. UKKONEN 
[1979]. "A method for transforming 
grammars into LL(k) form," Acra Infurmaticcr 12, 339-369. 
SPILLMAN, T. C. 11971 1. 
"Exposing side effects irt a P t i l  optimizing corn- 
piler ," Informaibri Proces,ring 71, North-Holland + Amsterdam, 376-38 1 + 
STEARNS, R . E. 1197 t 1, "Deterministic top-down parsing," Pmc. 5sh A~rrud 
Princeton Cmf. on Infurma~im Sciences and Systems, 182- 1 88, 
STEEL, T. B., 
JR. 1 1%11. 
"A first version of U nwl," Western Joint Computer 
Confer~we, 37 1-378. 
STEELE, G . L., JR. I1 9841. Commoa WSP, Digital Press, Burlington, Mass. 
~TC)CKHAUSEN, P. F. (19731. "Adapting optimai code generation for arithmetic 
expressions to the instruction sets available on present-day computers," 
Cumm. ACM li:6,353-354. Errata: 17:lO ( 1974) 541. 
STONEDRAKER, 
M., E. WOW, P. KREPS, 
AND G .  HELD \1976]. 'The design and 
Implementat ion of INGRES," ACM Trans. Datuhse Systems k3, 189-222. 
STRONG, 
J,, J +  WEGSTEIN, A .  TAITTER, 
J. OLSLTYN, 
0. 
MOCK. AND T, STEEL 
(19581. "The problem of programming communication with changing 
machines: a proposed solution," Cumm. ACM k8 (August) 12-18, Part 2: 
1:9 (September) 9-15. Report of the Share Ad-Hw committee on Univer- 
sal Languages. 
STROUSTRUP, 
I3 . [ 19861. The CS + Programmifig Language, Addison-Wesley , 
Reading, Mass. 

BIBLIOGRAPHY n7 
SUZUKI, N. [I981 1. "Inferring types in Smalltalk ," Eighth ACM Symposium on 
Principles of Prgramming Languages, 1 87- 1 99. 
SUZUKL, N . AND K+ ESH~HATA 
[ 19771. 
"lmplementatim of array bound 
checker,'* Fourth ACM 
Symposium on Principks if Programming 
Languages, 132- 143. 
SZYMANSKI, 
T. G .  IJ9781. "Assembling d
e
 for machines with span- 
dependent instruclions," Comm, ACM 21:4, 300-308, 
TAI, K. C. [l978]. "Syntactic error correction in programming languages,'' 
IEEE Tmns. Sofhvare Engineerifig SE4:S, 4 14-425. 
TANENBAUM, 
A. $., H. V A N  STAVEREN, E. G. KEIZER, AND 1. W. 
STEVENSON 
[1983]. "A practical tool kit for making portable compilers," C u m .  
ACM 2k9, 654-660. 
TANENBAUM, 
A. S., H. 
VAN STAVEREN, AND J .  W. STEVENSON 
[ 19821. "Using 
pephole optimization on intermediate code ," TOPUS 4:1, 21-36. 
TANTZEN, 
R . G. [1%3]. 
"Algorithm 199: Conversions between calendar date 
and Julian day number," Cmm. ACM 6;8, 443. 
T A A H ~ ,  
J. 119821. "Attribute evaluation during LR parsing," Report A- 
1982-4, Dept . of Computer Science, University of Helsinki. 
* 
TARIAH, 
R. E. \1974a]. "Finding dominators in directed graphs,'* SIAM J. 
Cumputig %l,62-89. 
TARJAN, 
R. E. [1974b]. 'Testing flow graph reducibility ," J. Computer aid 
System Sciences 99, 355-365. 
TARSAN, 
R. E. [19751. "Efficiency of a good but not linear set union algo- 
rithm," JACM 22~2, 215-225. 
TARIAN, 
R. E. I19811. "A unified approach to path problems," J .  ACM 28~3, 
577-593. And "Fast algorithms f o ~  
solving path problems," J. ACM -3 
594-6 14, 
TARIAN, 
R +  E. 
AND A. C. YAO [1979], "Storing a sparse table," Comrn. ACM 
22~11, 606-61 1 .  
TENNENBAUM, 
A. M. [1974]. 
"Type determination in very high l e ~ l  
languages,'' NSO-3, Courant Institute of Malh. Sciences, New York 
Univ. 
TENNEW, 
R . D . 1198 11. Principles of Prugrmming Languages, Prentice-Hall 
International, Englewood Cliffs, N. J .  
THOMPSON, 
K. [1%8]. 
"Regular expression search algorithm," C u m .  ACM 
11:6, 419-422. 
TJIANG, 
S. W. K. 119861. 
"Twig language manuat," Computing Science 

T~chnical Report 120, AT&T Bell Laboratories, Murray Hill, N. 1. 
TOKUDA, 
T. 11981 1. "Eliminating unit reductions from LR(k) parscrs using 
minimum contexts," A m  lnformatiru 15, 447-470. 
TRKKEY, 
H .  W ,  [1985]. Cmpihx P a s d  Programs into Silicom, Ph. U. 
Thesis. Slanford U niv . 
ULLMAN, 
J. D. [ 19731. "Fast algorithms for the elimination of common subex- 
pressions," A m  infimnatica 2, 19 1-2 13. 
ULIMAN, 
J. D. [ 19821. PrirrrLipks of Darubus~ $y.rtem, 2nd Ed., Computer 
Siencs Press, Rockville, Md. 
ULLMAN, 
J. D. 119841. Cornputa~iuml Asp~cts of VLSI, Computcr Science 
Press, Rwkville, Md . 
V Y ~ T S K Y ,  
V .  AND P. WEGNER f 19631. "A graph theoretical Fortran source 
language analyzer," manuscript, AT&T Bell' Laboratories, Murray Hill, 
N. J, 
WAGNER, R . A. 1 19741. "Order-n correction for regular languages," Cnmrn, 
ACM 3 8:5, 265-268. 
WAGNER, R. A .  AND M. J. FIXHER 119741. "The string-to-&ring correction 
problem," J. ACM 21:1, 168-174. 
W n m ,  W +  
M .  11976al. "Code generation," in Bauer and Eickcl 119761, 302- 
332. 
I 
WAIT& W, M. IlY76b]+ "Optimimtion," in Bawr and Eickcl 119761, 549602. 
WAITE, W, M. AND L. R. CARTER 
119851. "The cost of a generated parser," 
Snfr wore-Prurbrice a d  Experience 1 53, 22 1 -237. 
WA.S~LEW, 
S. G .  1 lY7i1. A Compiler Writing Sysrm with Opiimization Capabili- 
lies for C u d e x  Order Sircrctures, Ph. D. Thesis, Northwestern Univ., 
Evanston, 111. 
WATT, I), A .  119771. "The parsing problem for affix grammars,'* Acfa Jnjor- 
mutiru 8, 1-20. 
WEGBREIT, B. [19741+ 'The treatment of data types in ELI," Comm. ACM 
175, 25 i-264. 
WEOBRE~, 
B+ 1 1975 1. 
"Property extraction in well-foundcd property sets," 
1EEE Trans. on Snfiwure Engineering 1 :3. 270-285. 
W EGMAN, M. N . 1 lY83j. "Summarizing graphs by regular expressions," Tenrh 
Annuui ACM Sympxium on Prinripks of Programming Lultguages, 203- 
216. 
WEGMAN, M. N. AND F, K .  ZADECK 
119851. ''Consfafit propagation with con- 
ditional branches," Tw@k 
Annual ACM Symposium on Primcipltv uj 

W E G ~ I N ,  
J. H. 119811. "Notes on Algol 60," in Wenelblot [l98l], pp. 126- 
127. 
WEIHL, W. E. [1980]. "ln~erprucedural data flow analysis. in the presence of 
pointers, procedure variables, and label variables," Sew#k AnnrurI ACM 
Sympsisrm on Princ@Ces uj Progr~mming hnguages, 83-94. 
W ELNG ART, S. W . 119733. An Eflcieni atbd Systmutic Methud of Cde Cerwra- 
tion, Ph. D, Thesis, Yak University, New Haven, Connecticut. 
WELSH, J., W. J. SNEERMER, AND C. A. R. HIIARE [19771. "Ambiguities and 
insecurities in Pascal," Sqfhvur+Praclice 
mid Experiemc 7:6, 6856%. 
WEXELBLAT, R. L. [1981]. History of P r o g m i n g  Lungwges, Academic 
Press, New York. 
WIRTH, N. [I%$]. 
"PL 340 - a programming language for the 360 comput- 
ers," 1. ACM 151, 37-74. 
WIRTH, N. [ 19'711. 'The design of a Pascal compiler ," S o ~ a r d r m i c e  
a d  
Experience 1:4, 309-333. 
WJRTH, N. [L981]. "Paxal-S: A subset and its impkmntation," in Barron 
I19811, pp. 199259, 
W w n ~ ,  N. AND H. WEBER 119661. "Euler: a generalization of Algol and its 
formal definition: Part 1 ," C m .  ACM 9:1, 13-23. 
WOOD, D. [1969]. "The theory of left factored languages," Compu&r 3. 
12:4, 349-3%. 
Y ANNAKAKIS, M. [ 19851. private communication. 
YOUNGER, 
D. H. [1%7j. "Recognition and parsing of context-free languages 
in time n >, " Inf0~7~0rion and Corurd 10:2, 189-208. 
ZELKOWITZ, 
M. V. AND W+ G. BAIL [1974], "Oprirnieatbn of structured pro- 
grams," Suffware-Prwtice d 
Experience 4: 1, 5 1-57. 

Index 
A 
Abel, N. E+ 718 
Aklwn, H . 462 
Absolute rnachinc d
c
 
5 ,  19, 514-5 15 
A bstr~ct machine 62-43 
Scc also Stack mwhinc 
Abstract syntax trcc 49 
Sx also Syntax trec 
Awptancc 1 15- 1 16. 199 
Accepting state 114 
A m u  link 398, 416-420, 423 
Adclcy, S. 1. 157 
Au ion tablc 216 
Activation 389 
Activation environment 457-458 
Activation r m r d  398-4 10, 522-527 
Adivation tree 391-393 
Actual parameter 390, 399 
Acyclic graph 
Directed acyclic graph 
Ada 343, 361, 363-364, 366-367, 
41 1 
A ddrcss &scriptor 537 
Addrcss mode 18- 19, 5 19-52 1 , 579-5g0 
Adjaccncy list 1 IQ- 1 I S  
Adrim. W. R. 722 
Advancing cdgc 663 
Affix grammar 341 
Aha, A. V7 158, 181. 204, 277-278, 
292. 392, 444-445, 462, 566, 572, 
583-584, $87, 721 
Aigrain, P. 584 
Algebraic transformation 532, 557, 566, 
600-m, 739 
Algol 24, 80, 82, 157, 277, 428, 461, 
512, 561 
Algol 63 86 
Algol-68 2 I + 386, 5 12 
Alias ~
-
~
.
 
72 I 
Alignmcnt, of data 399-400. 475 
Allen, F. E. 718-721 
Alphakt 92 
Altcrnativc 167 
Ambiguity 30. 171, 174-175, ISQ. 191- 
192, 201-202, 229-230, 247-254, 
261-264, 578 
Ambiguous definition 610 
Ammrnn. U. 82, 51 1, 583, 728-729. 
734-735 
Analysis 2-10 
k c  also Lcxicnl analysis, Parsing 
Andcrsnn, J. P. 583 
Anderson, T. 278 
Anktam. P. 719 
Annotated parse trec 34, 280 
APL 3,387, 411, 694 
Ardcn, B. W. 462 
Arithmetic operator 361 
Array 345.349, 427 
Array rcfcrenm 202, 467, 481-485, 552- 
553, 582, 588,649 
ASCII 59 
Assembly oode 4-5, 15, 17-19, 89, 515, 
519 
Assignment statement 65. 467, 478-488 
Associsltivity 30, 32, 95%. 207. 247- 
249, 253-2649 484 
Attribute 11, 33, 87, 260, 280 
See also Inherited attribute, Lexical 
value, Syntax-directcd definition . 
Synthesized attributc 
Attribute grammar 280, 580 
Augmented depmdcncy graph 334 
Augmented grammar 222 
Auslander. M. A .  546, 583, 719 
Automatic d
e
 
generator 23 
AviiloMc cxpressbn 627-631. 659-660, 
684, 694 
AWK 83. 158,724 
Back cdgc 604, 606, W 

INDEX 781 
Back end 20, 62 
Backhowz, R .  C. 278, 722 
Back patching 21 , 500-506, 5 15 
Backtracking 181-182 
Backus. J. W. 2. 82, 157, 386 
Backus-Naur form 
See BNF 
Backward 
data-Row 
equations 
624. 
699-702 
Bail. W. 
G .  719 
Baker. B. 3. 720 
Baker, T. P. 387 
Banning. J. 721 
Barren, D. W .  82 
Barrh, J. M. 721 
Basic block 528-533, 59 1, 598-602, W. 
7Q4 
See also Exteadcd basic blo& 
Basic induction variable 643 
Basic symbol 95, 122 
ksic type 345 
Bauer, A .  M. 387 
Bauer. F. L. 82, 340, 386 
WPL 51 I .  583 
Bcalty, J+ C. 583 
Bccbcr. R. J. 2 
Begriffsschtift 462 
Beiady, L. A .  583 
Bcll. I .  R .  718 
kntiey, J .  L. 360, 462, 587 
&st, 
S. 2 
Binary alphabet 92 
Binding, of names 395-3% 
Birman. A. 277 
Bliss 489, 542, 561, 583, 607, 718-719, 
740-7 
42 
Block 
See Basic block. Bbck struckurc, 
Common block 
Block structure 412, 438-440 
BNF 25, 82, 159, 277 
Bwhmann, G. 11. 34i 
Body 
See Prmdurc body 
Biwlcan expression 326-328, 488-497, 
50 L -503 
Bootstrapping 725-7 
29 
Bottom-up parsing 41, 195. 290. 293- 
296, 308-316, 463 
See a h  LR parsing, Operator prc- 
cederice parsing, aifl-reduce pars- 
in g 
Bounded contexl parsing 277 
b y e r ,  R. S+ 158 
Branch optimization 740, 742 
Branquart, P. 342. 512 
Branstad, M. 
A. 722 
hatman. H. 725 
Brcal; statement 62 1-623 
Brooker. R .  A. 340 
Brwks. F+ P. 725 
Brosgol, B. M . 341 
Brown. C. A. 341 
Bruno, J. L. 565, 584 
Bucket 434 
k c  also Hashing 
Buff~t 57, 62, 88-92, 129 
Burst all. R . M. 385 
Busam, V. A. 718 
Bytc 399 
C 52. 104-105, 162-163. 326, 358-359, 
364, 366, 396-398, 41 1 ,  414, 424, 
442, 473, 482, 510, 542, 561, 5R9, 
6%, 725, 735-737 
Call 
Scc Proccdurc cal I 
Call-by-addrcss 
Sce Call-by-reference 
Call-by-location 
k c  CalLby -rcfcrcncc 
Call-by-nam~ 428-429 
Call-by-refcrcncc 426 
Call-by-valuc 424-426, 428-429 
Calling quencc 404-408, 507-503 
Canmica! cdcction nf scls of items 222, 
224.230-232 
Canonical derivation 169 
Canonical LR parsing 230-236, 254 
Canonical LR parsing hble 230-236 
Cardclli. L. 387 
Card inael. J.-P. 342, 5 12 
Carter, J. L. 71H, 731 
Carter. L. R .  583 
Cartesian product 345-346 
Cartwright. R. 388 

782 INDEX 
Case statement 497-500 
Cattell, R. G, Ci. 511, 584 
CDC txm 584 
CFG 
See Context-free grammar 
Chaitin, G .  J. 546. 583 
Chandra, A .  K. 546,583 
Chang, C. H. 278 
Changed variable 657-660 
Charader class 92, 97. 148 
Sse also Alphabet 
Cherniavsky, J .  C. 720,722 
Cherry, L. L. 9, 158, 252, 733 
Child 29 
Choe, K .  M. 278 
Chomsky . N, 8 1 
Chomsky normal form 276 
Chow, F. 583, 719,721 
Church, A. 387,462 
Ciesinger, J. 278 
Circular syntax-directed definlrbn 287. 
334-336, 340, 342 
Cleveland, W .  S. 462 
Closure 93-96. 123 
Closure. of set of items 222-223, 225, 
23 1-232 
CNF 
See Chamsky rtorml form 
Cobol 731 
Cocke, J .  160, 277, 546. 583, 718-721 
Cwke-Y ounger-Kasami algorithm 1 m, 
277 
Code generation 15. 513-584, 736-737 
Code hoisting 7 14 
Code motion 596, 638-643. 710-71 1, 
72 1, 742-7 43 
' 
Code optimitat ion 14- 15, 463, 472, M, 
5 1 3, 
530, 
554-557, 
585-722, 
738-740 
Coercion 344, 359-360, 387 
Coffman, E. G .  584 
Cohen, R. 341 
Coloring 
See Graph coloring 
Column-majw form 48 1-482 
Comment 84-85 
Common block 432. -448. 
454-455 
Common Lisp 462 
Comrnm subexpressim 290, 531, 546, 
s51, 564, 592-595, 59P600, 633- 
636,709, 7 39-74 L ,743 
See also Available expression 
Commutativity %, 684 
Compaction, of storage 446 
Compiler-compiler 22 
Composition 684 
Compression 
See Encoding of types 
Concatenation 34-35, 92-%. 123 
Concrete syntax t r e  49 
ke also Syntax tree 
Conditiom, d e  54 1 
Cond itimal expression 
See Boolean expression 
Configuratim 217 
Conflict 
Sae 
Disambiguation 
rule, 
Reduce/redue conflicr , Shifthedurn 
wnflict 
Confluence operator 624, 680.695 
Congruence closure 
%e Congruent nodes 
Congruent nodes 385, 388 
Conservative approximation 61 1, 624- 
I
,
 63Q. 652-654, 
659-440, 
639-6W 
Constant folding 592, 599, 601 , 63 1-685, 
687 -688 
Context-free grammar 25-30, 
4-41, 8 I -  
82, 165-181,280 
See also LL grammar, LR gram- 
mar. Opqrator grammar 
Context-free language 
168, 
172-173, 
179- 181 
Contiguous evaluation 569-570 
Control flow 66-67, 468-470, 488-506. 
556-557, a, 
61 1, 621, 639-690, 
720 
Control link 348,406-409, 423 
Control stack 393-394. 3% 
Conway, M. E. 277 
Conway, R .  W. !&I, 
278 
Copy propagat ion 592, 594-595,636-638 
Copy rule 322-323, 325, 428-429 
Copy statement 467, 55 I, 594 
Copy-restore-linkage 427 
Corasick, M. J .  158 
Core, of set of items 231 

Corntack. G. V. 158. 387 
Courcelle, B, 341 
Cousot, P. 720 
Coumt, R . 720 
CPL 387 
Cross compiler 726 
Cron edge 663 
Curry. H, 0. 387 
Cutler, D. 719 
Cyclc 177 
Cycle, in type graphs 358-359 
Cycle-frec grammar 270 
CYK algorithm 
Ctxkc-Younger-Kammi 
rithm 
DAG 
k t  Dirccfcd acyclic graph 
Dangling dse 174- 175. 191, 201-202, 
249-251. 263, 268 
Dangling rcfcrcucc 409, 442 
Data arca 446, 454-455 
Dala layout 399. 473-474 
Data object 
Set Objcct 
Data statement 402-405 
Data-flaw analysis 586. 6C)B-722 
Data-flow analysis framework 43 1-694 
Data-flw cnginc 23, 690-694 
Data-flow quation 608, 624, 1330 
Date, C. 1. 4 
Daussmann, M, 387 
Davidson, J, W 5 1  1 ,  534 
Dcad cclde 53 1,555, 592, 595 
Debuggcr 406 
Dcbugginy 555 
Sce also Symblic debugging 
Dcclarstion 269, 394-395, 473-478. 5 10 
Decor at ion 
Sce Annotated parse tree 
Deep acccm 423 
Default value 497-498 
Definitiofi 529, 610, 632 
Definition-use chain 
Sce Du-chain 
hkscaillc, J.-P. 342, 5 I 2  
Dclction , of Iwds 404 
DELTA 341 
Dcmers, A .  J. 278 
Dcnckcr, P+ 158 
Dcnotational semantics 5 4 0  
DcFndcncy graph 279-280. 284-287. 
331 -334 
Depth, of rr flow graph W, 672-673. 
716 
Depth-first ordering 296, 6b!. 67 1-673 
Depth-first wrch 660-6154 
Dep t h-f irst spanning trec 662 
Depth-first traversal 36-37, 324-325. 393 
Dcransart, P+ 342 
DeRemcr, F. 278, 387 
Derivation 29, 167-171 
Ekspcyroux, T. 388 
Dcterrninistic 
finite 
automaton 
I 13, 
I
-
2
 127-128, 
132-136. 
141- 
146, 
150, 
180-181, 217, 
222, 
224-226 
Deterministic transit ion diagram 100 
DFA 
k c  Detcrrninislic finite automaton 
Diagnostic 
ScF Error mcssagc 
Directcd acyclic graph 2W293, 347. 
4M-466, 471, 546.554, 558-561, 
582, 584. 598-WZ. @M, 705-708 
Disambiguating rule 171. 175, 247-254. 
26 1 -264 
Disambiguation 387 
See also Overloading 
Display 420-422 
Distance. bctwcen strings 155 
Distributive framework 686-688, 
692 
Distributivit y 720 
Ditzel. D, 583 
Do statomen1 86, 111-112 
Dominator 6U2, 639, 670-671, 721 
Dominator trcc 602 
Downcy, P. J .  388, 584 
Dror>ss~pou 
IOU, S. 387 
Druseikis. F. C+ 162 
Du-chain 632-633. 643 
Dummy argumcot 
Sce Formal parameter 
Durre. K. 158 
Dynamic allocation 40 1. 40-446 
Dynamic chwking 343. 347 

Dynamic programming 277, 567-572, 
5 84 
Dynamic scope 4 1 1. 422-423 
Earley, J .  181. 277-278. 721 
Earley's algorithm 160, 277 
EBCDIC 59 
Edit distance 156 
Efficiency 85, 89, 
126-128, 144.146. 
152, M-244, 279, 360, 388, 433, 
435-438,451, 516, 618620, 72.4 
See also Code optimization 
Egrep 158 
ELI 307 
Elshoff, J .  L. 583 
Emitter 67-68, 72 
Empty set 92 
Empty string 27. 46, 92. 94. 96 
E n d h g  of types 354-355 
Engelfriet, 1. 341 
Englund, D. E. 7 I8 
Entry. to a laop 534 
Environment 395. 457-458 
 closure 1 18-1 1.9, 225 
efrec grammar 270 
t-production 177, 189, 270 
€-transition 1 IQ- I IS, f M 
EQN 9- LO, 252-254, 300, 723-724. 726. 
733-7 34 
Equel 16 
Equivalence, of basic blocks 530 
Equivalence, of finite automata 388 
Equivalence, of grammars 163 
Equivalence, of regular expressions 95, 
150 
Equivalence, of syntax-directed defini- 
tions 302-305 
Equivalence, of type expressions 352-359 
See also Unification 
Equ ivalenw statement 432,448-455 
Equivalence, under a substitution 37 I, 
377-379 
Error handling 11, 72-73, 88, 161-162 
See also Lexical error, Logical 
error. Semantic error, Syntax error 
Error message 194, 2 1 1-215,' 256-257 
Error productions 164-163, 264-266 
Ershov, A. P. MI, 583, 718 
Escape character l 10 
Evaluation order, for bask blocks 518, 
558-56 1 
Evaluation order, for synlax-directed . - .  ' 
definitions 
285-287, 
298-299, 
3 16-336 
Evaluation order, for trees 561-580 
Eve, l. 278 
Explicit allocation 440, 443-444 
Explicit type cmversion 359 
Expression 6-7, 31-32. 166, 290-293, 
350-351 
See also Postfix e~pression 
Extended basic block 7 14 
External referen& 19 
Fabri, J. 718 
Failure function 15 1. 154 
Family, of an induction variable 644 
Fang. 1. 341 
Farrow, R. 341-342 
Feasible type 363 
Feldrnan, S. 1. 157, 511, 729 
Ferrante, I+ 718 
Feys, R. 387 
Fgrep 158 
Fibonacci string 153 
Field, of a record 477-478, 488 
Final slate 
See Accepting state 
' Find 378-379 
Finite automaton ! 13-144 
See also Transition diagram 
FIRST 45-46, 188-190, 193 
Firsipos 135, 137-140 
Fischer. C. Ni 278. 583-584 
Fischer, M. S. 158, 442 
Flsrrk, A. C. 428 
Flow graph 528, 532-534, $47, 591, 602 
% also Reducible flow graph 
Flow of control 529 
See also Control flow 
Flow-of-control check 343 
Floyd, R. W. 277, 584 
FOLDS 341 
FOLLOW 188-190, 193. 23@231 

Fdlowps 135, 137-140 
Fmg, A .  C. 721 
Fwmal parameter 390 
Fortran 2. 86, 111-113. 157, 208, 386. 
396, 401-403, 427, 432, 446-455, 
481, 602, 718, 723 
Fortran H 542, 583. 727-728. 737-740 
Forward 
data-flow 
equations 
624, 
698-702 
Forward edge 606 
Fosdick. L. D+ 7 22 
Foster, J . M. 82,277 
Fragmentation 4-43-44 
Framc 
Scc Activation record 
Frawr, C. W. 51 1-522. 584 
Fredman, M. 158 
Frege, G. 462 
Frciburghousc, R. A +  512, 583 
Frcudenbgcr, S. M. 7 19, 722 
Front end 20,62 
Fukuya. S. 583 
Function 
!ke Procedure 
Function type 346-347, 352, 361-364 
Scc also Polymorphic function 
GAG 341-342 
Gajewska, H . 72 1 
Galler, 8. A. 462 
Ganapathi, M , 583-584 
Cannon, 1. D. 163 
Ganzinger, H. 341, 387 
Garbage collection 441-42 
Garcy, M. R. 584 
Gear. C+ W ,  720 
Gen 603,612-614, 627, 636 
Generation, of a string 29 
Gcfieric function 364 
See also Polymorphic function 
Gcschkc, C. M. 489, 543, 583, 71 
7 19, 740 
Giegerich, R +  341, 512. 584 
Glanvilk. R. S. 579, 584 
Gbbal error correction 164- 165 
Global name 653 
Scc a h  Nonlml name 
G h h l  optimization 592, 633 
Global registcr allocation 542-546 
GNF 
See Greibach normal form 
Goldberg. R. 2 
Goto, of set of items 222, 224-225, 231- 
232. 239 
Goto statcmcnt 5Q6 
Goro table 216 
Graham: R ,  M. 277. 462 
Graham. S. L. 278, 583-584. 720 
Graph 
See Dcpcndcncy graph. Dirccted 
acyclic graph. Flow graph, Interval 
graph, 
Reducible 
flow 
graph, 
Register-interference graph. %arch, 
Transition graph, Trcc. Type graph 
Graph coloring 545-546 
Grau, A. A. 512 
Grcibach normal form 272 
Grep 158 
Haibt, L. M. 2 
Haley. C. B. 278 
Hatstcad, M+ 
H. 512, 727 
Handle 1%-1%. 
200. 205-206, 210, 
225-226 
Handlc pruning 197-198 
Hanwn, D. R. 512 
Harrison, M , A. 278 
Harrison, M. C. 158 
Harrison, W. H. 583,718, 722 
Harry, E. 341 
Hash function 434-438 
Hash tablc 498 
Hashing 292-293, 433-440. 459-460 
Hashpjw 435-437 
Head 604 
Header 603,61 I, 664 
Heap 397, 735 
Heap 
allocation 401-402. 
410-4 
440-446 
Hechl. M, S. 718*721 
Heinen, R. 719 
Held. G. l b  
Hclsinki Language Proccssw 
Sec HLP 

Hcndcrsrm, P. B. 720 
Hcnncssy, J. L. 583, 721 
Hcnry, R. R . 583-584 
Hcrrick. H . L. 2 
Hcuft. J. 158 
Hcxt. J. €3. 387 
H icrarchical analysis 5 
Scc also Parsing 
Hill. U. 512 
H indlcy. R .  387 
H LP 278, 341 
Hwarc, C. A .  K .  $2. 387 
H~bbs, S. 0. 
4HY. 511, 540. 583-584. 
7 1 8-7 19, 740 
Hoffman, C .  M. 584 
Holc in w*copc 4 12 
Hollerith siring 98 
Hopcroft, J. E. 
142, 157, 277. ?92, 
388, 392, 444-445, 462, 584, 587 
Hope 385 
Hopkins. M .  E. 546, 583, 719 
Homing, J 1. 82, 163, 277-278 
Horspooi, R .  N. S. 158 
Horwitz. L. P. 583 
Huct. G. 584 
Huffman, D. A .  157 
Hughes, R. A .  2 
Hunt, J .  W ,  158 
Huskcy. H. D. 51 I. 727 
Hutt. B. 341 
IBM-7090 584 
IBM-370 517, S W ,  SH4, 737,740 
Ichbiah. 3 .  D. 277 
idcmpotcncc 94. 684 
Idcntificr 56, 36-87. 179 
ldcntity function 683-684 
If sratcmsnt 112-1 13. 491-493. 504-505 
Ikcda, K I 583 
lmmcdiatc duminatr,r 602 
Immcdiatc kt7 recursion I76 
Implicit allucdnn 440, 444-446 
Implicit typc conrcrsim 359 
Important s t a k  I34 
lndcxcd addrcwing 51 9. 540 
lndircct addressing 5 14-520 
I ndircct triplcs 472-473 
I nd ircct ion 472 
Induc[ion variablc 
596-598. 
643-648. 
7W, 72!, 739 
infix cxprcssion 33 
lngalls, D. H. H. 387 
lngcrman, P. Z. 82 
lnhcritcd nttributc 34, 28r). 283, 299, 
308-3 16. 324-325. 340 
k c  niso Attributc 
hitiid node 532 
Initial statc 
Sce Stalc 
In- t inc expansion 428-429 
Scc also Macru 
lnncr loop 534, 605 
Input symbol 114 
instance, of a polymorphic typc 370 
Instruction sctcction 516-517 
Intcrmcdiatc crde 12-14. 563-5 12, 5 14, 
5x9, 71)4 
SCC atso Abstract rnachinc, Postfix 
cxprcssion , 
Quadruplc. 
Thrce- 
address c d c ,  Trcc 
Intcrprder 3-4 
Intcrproccdural 
data 
flow 
armlgsis 
653-MO 
lnrcrval 664-667 
Intcrval nnalysiz 623, MI, 667, 72I) 
See also T, -T, analysis 
Interval dcpth 672 
Interval graph 666 
Intcrvd partition 665-666 
Irons, E. T. 82, 278, 340 
lshihara, K. 722 
Item 
Scc 
Kcrml itcm, LRI I )  item, 
LRII)) item 
lrcratirc 
data-flow 
analysis 624-633, 
672-473, 690-694 
Iverson. K 
+ 387 
Jawbi. Ch. 82. 51 1, 734 
Janas, J ,  M, 387 
Jarvis, J. F. 83, 158 
lamycri, M. 341-542 
Jcnscn. K .  82. 5 I 1, 734. 745 
Johnson, D. S. 584 

Johnson, S. C. 4, 157- 158, 257, 278, 
340, 354-355, 383, 462, 511, 566, 
572, 584, 73 1. 735-737 
Johnson, W. L. 157 
hhnsson, R. K .  489, 543, 583. 718- 
719, 740 
Jdiat, M. 278 
Jones, N. D. 341, 387, 718, 720 
Jourdan, M. 342 
Joy, W, 
N. 278 
Kaiscrwerth, M. 158 
Kam, J. B. 720 
Kaplan. MA A. 387, 721 
Karp, R. M. 388. 583 
Kasami, T. 160, 277, 720 
Kastcns, Cf . 341-342 
Kasyanov, V. N. 720 
Katayarna, T. 342 
Kcizcr, E. G .  511 
Kcnncdy, K. 341-342. 583. 720-321 
Kcohanc. J . 720 
Kerncl item 223. 242 
Kcrnighan, B. W .  9, 23. 82, 158, 252, 
462, 723, 730, 733. 750 
Kcyword 56. 86-87, 430 
Kicburtz, R .  B. 278 
Kildall. G. 
A .  634, 680-681, 720 
Kili 608, 612-614, 627. 636 
Kiyono. T. 583 
K lcene closure 
See Closure 
Klecne. S. C. 157 
KMP algorithm 152 
Knuth, D. E. R, 23-24, 82, 157-158, 
277, 340, 3RR, 444. 4h2, 583-584, 
672-673, 721, 732 
Knu~h-Morris-Pratt algorithm 158 
See also KMP algorithm 
Kolsky, H+ 
G +  718, 737 
Komlor. J .  
1% 
Korcnjak. A .  IA 277-278 
Kosaraju, S. R .  720 
Koskimics, K. 341 
Kostcr, C. H. A .  341 
Kou, L. 720 
Kreps. P. 16 
Label 66-67,467, 506, 515 
LaLonde, W .  R .  278 
LALR collection of sets nf kerns 238 
LALR grammar 239 
LALR parsing 
See Lackahcad LR parsing 
LALR parsing table 236-244 
Lamb. D. A. 584 
Lambda calculus 387 
Lampson, B. W. 462 
Landin. P. S .  462 
Langmaack, H . 5 12 
Language 28,92, 1 15, 168,203 
Lassagne. T. 584 
Lastpos 135, 137-140 
Lattice 387 
L-att~ibutcd definition 280, 296-31 8, 341 
Lazy state mnstructim 128, 158 
Leader 529 
Leal 29 
Lecarme, 0. 
727 
Ledgard, H. F. 388 
Left associativity 30-31, 207, 263 
Left factoring t 78- I79 
Left leaf 561 
Left recursion 47-50, 71-72. 176- 
178, 
182, 191 - 192, 302-305 
Left most derivation I69 
Lcft-sententid form 169 
Leinius. R. P. 278 
Lcngaucr, T. 721 
Lcsk, M. E. 157. 731 
Levcrctt , €3. W + 5 t 1. 583-584 
Lcvy. J. P. 278 
LCVY, J.-J. 584 
Lcwi. J .  342, 512 
Lcwis, H . R. 720 
Lcwis. P. M. 277.341 
Lcx 83, 105-113, 128-129, 148-149. 158, 
7 30 
Lcxcmc 12,54,56,61, 85,430-431 
Lcxical analysis 5, 12, 26, 5440, 7i, 
83-158, 160. 172. 261. 264. 738 

788 INDEX 
Lexical environment 457-458 
Lexical error 88, 161 
Lexica! scope 4 1 1 -422 
Lcxical value 12, I 1  1 ,  281 
Library 4-5, 54 
Lifetirnc, of a temporary 480 
Lifetime, of an activation 391. 410 
Lifetime, 
of 
an 
attribute 
320-322. 
324-329 
Limit flow graph 666, 668 
+ear 
analysis 4 
See also Lexical analysis 
LINGUIST 341 -342 
Link editor 19. 402 
Linked list 432-435. 439 
Lint 347 
Lisp 411. 440,442, 461, 694, 725 
Literal swing 86 
Live variable 534-535, 543, 551, 595, 
599-W, 631-632, 642, 653 
LL grammar LW, 162, 191-!92, 221, 
271). 273, 277, 307-308 
Loadcr 19 
Local name 394-395, 398400, 41 1 
Local optimization 592, 633 
Lmwner, P. G .  718 
Logical crror 161 
Longest common subsequence I155 
Lookahcad 90. 111-1 13, 134, 215, 230 
Lookahcad LR parsing 216, 236-244. 
254. 278 
See also Yacc 
Lookahcad symbol 41 
Loop 533-534. 544. 602dM, 616-618, 
660 
Loop header 
See Header 
Lmp optimization 5% 
k c  also Code motion, Induction 
variable, Loop unrolling. Reduction 
in strength 
Loop-invariant computation 
Scc Code motion 
Lorho. 0. 341-342 
Low, J .  721 
Lowry, E. S. 542, 583, 718, 721, 727, 
737 
LR grammar 160, !62, 201-202, 220- 
22 1. 247. 273, 278.308 
LR parsing 21 5-266, 34 1 ,  518-579 
LR{ I )  grammar 235 
LRT ! I  hem 230-23 1 
LRIOI item 221 
Lucas. P. 82. 277 
Lundc. A .  583 
Lunnd, H .  5g3 
L-valuc M-65. 229. 395. 424-429. 547 
Machinc code 4-5. 18. 557, 569 
Machinc starus 398, 4W-408 
MacLarcn. M7 D. 7 19 
MacQuccn, !I. 
B. 385,388 
Macro 16.17, 84, 429. 456 
Madsen. C. M .  341 
Madsen. 0. 
L. 278, 341-342 
Mikt 729-73 1 
Manifest constant 107 
Marill. T. 342. 583 
Marker nonrerrninal 3119. 3 1 1-3 15. 341 
Markstein- J. 719. 721 
Marhstein. P. W .  546, 583 
Martelli, A .  388 
Mauney. S .  278 
Maximal munch 578 
Maxweli. W. 
L. 278 
Maynh, 0. H. 340 
McArthur. R . 5 1 1, 727 
McCarthy, J+ 82, 461, 725 
M d l u r e ,  R. M .  277 
McCraclien, N. 1. 388 
McCullcrch, W. 3. 157 
Mcllroy, M. 
D. 158 
McKeeman, W .  M. 82, 277, 4h2, 584 
McKusick. M , K + 584 
M c k h ,  H . R. 583 
Muh'uughton, R .  157 
Medlrrck. C .  W .  542. 583, 718, 721. 
727, 717 
Mcertens, L .  387 
Meet operator 68 t 
Meet-over-paths sotutiun bRP-6%). 692, 
694 
Memory map 446 
Memory organization 
See Sroragc organiutiim 
META 277 

Mctcalf, M. 718 
Meyers, R . 387 
Milkr, R .  E+ 583, 720 
Milner, R .  365, 387 
Minimum-distance erwr correction 88 
Minktr, J. 512 
Minker, R:G. 512 
Mitchell, J. C. 387 
Mixed strategy precedence 277 
M ixed-mode expression 495-497 
ML 365, 373-374, 387 
Mwk. 0. 82, 511,725, 
Modula 607. 7 19, 742-743 
Moncke, U. 341 
Monotone framework 686-688, 
692 
Monotonicily 720 
Mmtanari. U. 388 
Moore. E. F. \57,721 
Moore, J 3. I58 
Morel, E. 720 
Morris, D. 340 
Morris, J. H . 158. 387-388 
Mwris. R . 462 
Morse, S. P. 277 
Mom, l. 462 
Most dosely nested rule 4 12, 415 
Most general unifier 370-371, 376-377 
Moultun. P. G. 278 
Muchnick. S+ S. 387, 71 8, 720 
MUG 341 
Muller. M. E. 278 
N 
Nageli. H. H. 82,511, 734 
Nakata, I. 583 
Name 389 
Name eq uiralence. of type expressions 
35b357 
Name-related check 343-344 
Natural loop 603-604 
Naur. P. 82, 277, 386, 461 
NEATS 342 
Neliac 51 1, 727 
Nelson. R. A .  2 
Nested procedures 41 5.422, 474-477 
Nesting depth 416 
Nesting. of activations 391 
See also Block structure 
Newcomer, .I. M. 511, 584 
Newey. M. C. 51 1-512 
NF A 
See Nondcierministic finitc aulorna- 
ton 
Nievergeh, J. 583 
Node splitting 666-668, 679480 
Nondeterministic finite automaron 1 13- 
114, 117-128, 130-132, 136 
Nondeterministic transithn diagram I84 
Nonlocal name 395. 41 1-424, 528 
Nonrsdlrcible flow graph 607, 679-680 
Nonreguhr set 180- 181 
See also Regular sel 
Nonterminal 26. 166-167. 204-205 
%e also Ma~ker nonterminal 
Nori, K. V. 82, 51 1, 734 
Nullabk expression 135, 137-140 
Nutt, R. 2 
Object 389, 395 
Objecr code 704 
Objecr language 
See Target language 
O'Donnell, M. J 584 
Offset 397, 400,450,473, 524 
Ogden. W. F. 342 
Olsztyn, J. 82, 51 1,725 
One-pass compiler 
See Single-pass translat~on 
Operalor grammar 203-204 
Operator identifitation MI 
See also Overloading 
Operator precedence 31 
Owrator precedence gransmdr 271 -272 
Operator precedence, parsing 203-2 15, 
277,736 
Opcratm prccedende relations 203-204 
Optimizing campikr 
SeeCdeoptirnization 
Osterweil. L. 5 .  722 
Overloading 330,344, 36 1-364, 384-385, 
387 
P 
Packed data 399 

790 INDEX 
Padding 399 
Pager, D. 27K 
Pai. A .  B. 278 
Paige. R .  721 
Pair, C. 342 
Palm, R. C. 721 
Panic mndc 88, 164, 192-193. 254 
Panini 82 
Pararnetcr passing 4 14-41 5, 424-429. 
653-654 
Parenihcses 95. 98, If 3- 1'14 
Park. J. C. H . 278 
Parse tree 6-7, 28-30. 40-43, 49. 160, 
1 h9- 17 i, !96, 279. 296 
%e also Syntax tree 
Parser generator 22-23, 730-73 1 
See also Yucc 
Parsing 6-7, 12, 30, 40-43, 
57, 7 
1 -72, 
84-85. I 59-278 
Sce 
also 
~ ~ l t ~ m - u p  
parsing, 
Bounded canrcxt 
parsing, 
Top- 
down parsing 
Pdrtial order 833-335 
Partition 142 
Pascal 52. 85, 94, 96-97. 162-163, 347. 
349, 356-357, 365-366. 396-398, 
4 L I , 424-425, 427, 440-442, 471, 
481, 510-512. 583. 719. 727-729. 
734-735 
Pass 20-22 
Passing en vironmenr 457-458 
Paterson. M. S. 388 
Pattern matching 85, 129- I3 1. 577-578, 
584 
PCC 519, 572. 584, 735-737 
?-code 734, 742 
Peephole oprimizar ion 554-558. 584. 587 
Petegr i-Llopar~, E. 584 
Pennetlo. T. 278, 387 
Period, of a string 192 
Persch, G . 387 
Petermn, T. G .  278 
Peterson, W. W. 462. 720 
Peyrolle-Thomas, M.-C. 
777 
Phaw !Q 
See also Code generation, Code 
uptimizsh. Error handling. Intcr- 
mediate 
c d c .  Lexicel 
analysis. 
Parsing, Semantic analysis. Symbol 
[able 
Phrase-lcvel error 
recovery 
164- 165, 
194- 195. 255 
Pic 456 
Pig Liltin 79-80 
Pikc, R. 82, 462, 730, 750 
Pitts, W. 157 
Plankalk~l 386 
PLK 164. 514 
PL/I 21. 80, 87. 162-163, 383, 387. 488, 
510, 512, 719 
Plotkin, G .  388 
Poinl 6W 
Pointcr 349, 409, 467-468, 540-54 1 ,  553, 
582.648-653 
Poinlcr type 346 
Poll;ick, B. W .  24 
~olldck, 1. L. 722 
Polymorphic fundion 344. 364-376 
Polymorphic ~ y p e  
368 
Poole, P. C. 511 
Pop 65 
Port~bility 85, 724 
Pumblc C compiler 
See PCC 
Portcr, d, H. !57 
Positive clctsur6 97 
See also Ciosure 
Post. E .  $2 
Postfix expression 25, 33-34, 464, M, 
470, 509 
Postorder travcral 561-562 
Powll, M. L. 719. 721. 742 
Pozefsky. D. 342 
Pralt. T. W. 462 
Pratt, V, R .  158, 277 
Precdmce 3 1-32, 95, 207, 237-249, 
263-264 
See also Operator precedence gram- 
mar 
Precedence function 208-2!0 
Precedence relations 
Set Operator precedence relations 
Prcdeceswr 532 
Prcdiclive parsing 44-48. 182-1 88, 192- 
195, 2 IS, 302-308 
Predictwe ~ranslatw 306-308 
Prefix 93 
Prefix expression 508 

Prebcader 605 
Preprocessor 4-5, 16 
Pretty printer 3 
Procedure 389 
Procedure body 390 
Prmdure call 202, 3%, 398, 404-4 1 1 ,  
'467, 506-508,522-527, 553.649 
See also Intmprmdural data flow 
analysis 
P t d u r e  definition 390 
Procedure parameter 414-515, 418-4 19 
Product 
See Cartesian product 
Production 26, 166 
Proliler 587 
Programming languagc 
See 
Ada, 
Algol. APL. BCPL, 
Bliss, C, Cobol, CPL, E L I ,  For- 
tran. Lisp. ML, Mdula, Neliac, 
Pascal, 
PU[, 
SETL, 
SIMPL, 
Smalltalk, Snob1 
Programming project 745-75 1 
Project, programming 745-75 I 
Propagation, of Imkahcad 242 
Prusscr, R .  T. 72 1 
Purdorn, P. W .  341, 721 
Push 65 
Quadruples 470, 472-473 
Qucry intcrprctcr 4 
Qucue 507-508 
Quicksort 390, 588 
Rabin, M+ 0. i57 
Radin, O. 387 
Raiha, K.-J. 278, 341-342 
Ramanathan, J. 341 
Randell, 8. 24, 82, 462, 512 
Rat for 723 
Rcaching definition 6 10-62 1. 624-627, 
652-653, 674-680, 684 
Recognizer 1 13 
Record typc 346, 359. 477-478 
Recursion 6-7, 165, 3 16-3 18. 329-332, 
391. 401 
See 
also Lcft recursion, Right 
recursion, Tail rccursim 
Rccursivt-dc.wenr parsing 44, 82, I8 I - 
182, 736. 740 
Rcduceircduce con fl ict 20 1 , 237, 262, 
57 8 
RcducibL now graph W-608.664, W, 
668, 7 14-715, 740 
Reductim 195, 199, 21 1-213, 216, 255 
Reduction in strength 557, 59b598. 601. 
644, 
Redundant cvde 554 
Rcdziejowski, R. R. 583 
Rcfcrcncc 
See Vsc 
Rcfcrcncc count 445 
Rcgion 61 1-612, 669-670. 673-679 
Rcgistcr 519-521 
Register allocation 5 17, 542-546, 562- 
565, 739-743 
Register assignment 15. 517, 537-540. 
544-545 
Rcgistcr dcwripmr 537 
Rcgist cr pair 5 17, 5b6 
Rcgistcr-intwfcrcnw graph 546 
Rcgresion test 73 1 
Rcgular ddin ition 96. 107 
Rcgular cxprcssion 83, 94-98, 107, 113, 
121-125. 129, 135-141, la, 172- 
173, 26R-269 
Rcgular set 98 
Rchostability 724 
Rcif.j. H+ 720 
Rcincr, A .  H. 51 I, 584 
Rciss, S. P, 462 
Rclative ddrcss 
Scc Offset 
Rclmatable machine mdc 5 .  1 8, 5 15 
Relocation bit 19 
Rcnaming 531 
Rcnvoise, C. 720 
Rcps.T. W .  341 
Rcsrvcd ward 56. 87 
Rcturgctability 724 
Rctargcling 463 
Rctcntion. of locals 401-403. 410 
Rctrcating cdgc 663 
Return addrcss 407. 522-527 
Return nodc 654 

792 INDEX 
Rcturn sequencc 4 0 5 4  
Rcturn value 399, W407 
Reynolds, J. C+ 388 
Rhdes, S, P. 278 
Richards, M. 511. 584 
Right associativity 30-3 1, 207, 263 
Right leaf 562 
Right recursion 48 
Rightmost derivation 169, 195- 197 
Right-sentential form 169, 1% 
Ripken, K. 387, 584 
Ripley, G .  D. 162 
Ritchie. D. M. 354,462, 5 1 1, 735-737 
Robinson, I, A. 388 
Rogoway, H. P. 387 
Rohl, J .  S. 462 
Rohrich, J. 278 
Root 29 
Roscn, B. K. 719-720 
Roscn, S, 24 
Rwcnkrantz, D. J. 277, 341 
Roslec, L. 723 
Ross, I). T. 157 
Rounds, W. 
C. 342 
Rovner, P+ 72 1 
Row-major form 48 1-482 
Run-time support 389 
See also Hcap allocation, Stack 
allocation 
Russell, L. 1. 24. 82. 462, 512 
Russell, S. R. 725 
R u m ,  W +  
L. 278 
R-value 64-65. 229. 395, 424-429, 547 
Ryder, B. 0, 
721-722 
s 
Saal, H. J +  387 
4arinen. M. 278, 341-342 
Safe approximation 
See Coowvative approximation 
Samelwn, K. 340 
Sankoif, D. 158 
Sannella, D. 
T. 385 
SPrjakoski, M. 278, 341 
Sattribuccd definition 28 1, 293-296 
Save aatcment 402-403 
Sayre, D+ 2 
Scanner 84 
Scanncr generalor 23 
See also Lex 
Scanning 
% Lexical analysis 
%arbrough, R. G. 718, 737 
Schaefcr. M. 718 
Schaffer, J. B+ 719 
M a t z ,  B. R. 51 1, 584 
Schonhrg, E+ 721 
Schorre, D. V. 277 
Schwartz, J. T. 387. 583, 718-721 
b p e  394.41 1, 438-440,459,474479 
Scott, D. 157 
Search. d a graph 1 19 
See a h  Depth-first search 
Sedgewick, R. 588 
Semantic action 37-38, 260 
Semantic analysis 5, 8 
Semantic error 16 1, 343 
Semantic rule 33, 279-287 
Semantics 25 
Sentencc 92* 168 
Sententid form 168 
Sentinel 91 
Sethi, R . 342, 388, 462, 566, 583-584 
SETL 387,694-695, 
719 
Shallow access 423 
Shared node 5 6 5 6 8  
Sharir, M. 719. 721 
Shcll 149 
Sheridan, P, B. 2, 277, 386 
Shift 199, 216 
Shiftireduce conflict 201, 213-2 15, 237, 
263-264,578 
Shift-reduce parsing 198-203,206 
See also LR parsing, Operator prc 
cedence parsing 
Shimasaki, M. 583 
Short-circuit code 490-49 1 
Shustek . L. J . 583 
Si& effect 280 
Signature, of a DAG node 292 
Silicon compiler 4 
SlMPL 7 19 
Simple LR parsing 216, 22 t -230, 254, 
270 
Simple precedence 2n 
Simple syntax-directed translation 3940, 
298 

INDEX 793 
Single production 248, 270 
Singk-pass translation 279, 7 35 
Sippu. S. 278, 341 
Skeletal parsc tree 206 
SLR grammar 228 
SLR parsing 
See Simple LR parsing 
SLR parsing table 227-230 
%-ringer, 
W. 
1. 387 
Snob1 41 1 
Soffa, M. L. 722 
Sbisalon-Soininen, E. 277-278, 341 
Sund type system 348 
Source language I 
Spillman, T. 
(3. 721 
Spontanwus generatiun, of lookahcad 
24 1 
Stack 126, 186, 198, 217, 255, 215-276, 
290. 294-296, 310.3L5, 324-328. 
393-394. 397.476-479. 562, 735 
See also Control stack 
Stack allocation 401% 404-412, 
522, 
524-528 
Stack machine 62-69, 4M. 584 
Start statc 100 
Qart symbol 27, 29. 166, 281 
Statc 100, 114, 153, 216, 294 
Statc minimization 141 - 144 
State (of stwage) 395 
St~temcnt 26, 28, 32. 67. 352 
See 
also Assignment 
statement, 
C a x  statement, Copy statement, 
Do statement. Equivalence state- 
mcnt. Gnto statement. if statement, 
Whilc shtemcnt 
Static 
allmation 
401-403. 522-524, 
527-528 
Static chccking 3. 343.347. 722 
Static mpe 
See Lcxkat scope 
Staveren. H. van 5 1 1 , 584 
3dio.h 58 
Srcarns, R. E. 277, 341 
Steel, T. B. 82, 511, 725 
Steele, G +  
L. 462 
Stern, H. 2 
Stevenwn, J .  W .  5 1 1, 584 
Stuckhausen, P. F+ 584 
Sonebraker. M. 16 
3urage 395 
Storage alhiat ion 4Ui -41 1 . 432, 440-446 
Storage organization 396-400 
String 92, 167 
String table 431 
Strong, J. 82, 5 1 1, 725 
Strongly 
noncircular 
syn t ax-directed 
definition 332-336, 340 
Strongly typed language 348 
' 
Stroustrup, B. 437 
Structural equivalcn~c, of type expres- 
sions 353-355. 376,380 
Structure editor 3 
Subsequence 93 
Sce also Longest common wbse- 
quenct 
Subscr wnstrudion 117-121. 134 
Substitution 370-37 1, 376-379 
Substring 93 
Successor 532 
Suffix 93 
Sussman,G. 1. 462 
Suzuki, N. 387.722 
Switch statement 
See Case statement 
Symbol table 7, 11, Wb2, 34. 160. 
429-440,470,473,475-480, 703 
Symbolic debugging 703-7 1 1 
Symbolic dump $36 
Symbolic register 545 
Synchronizing tokcn 192- 194 
Syntax 25 
See also ConteM-free grammar 
Syntax analysis 
k e  Parsing 
Symtax error 161-165, 192-195, 199, 206, 
2 10-2 15, 2 18, 254-257, 264-266. 
275, 278 
Syntax tree 2, 7, 49, 287-290, 464-466, 
47 1 
See also Abstract syntax tree, Con- 
crete syntax tree, Parse tree 
Syntax-directed definition 33, 279-287 
Sx also 
Annotaled parse troc, 
Syntax-directcd translation 
Syntax-directed translation 8, 25, 33-40. 
46-54, 279-342, 464-465, 468-470 
Syntaxdirected translation engine 23 
&e also GAG. HLP, LINGUIST. 

794 INDEX 
MUG. NEATS 
Synthesized attribute 34. 280-282, 298- 
299, 316, 325 
See also Attribute 
Szcmercdi, E. 158 
Szymanski. T. G. 158. 584 
Table 
comprcmion 
144- 146, 
I 5 1 .  
244-247 
Ttlblc-driven 
parsing 
186, 
190-192. 
2 16-220 
Scc also Canonical LR parsing. 
LALR 
parsing, 
Opcrator 
prc- 
cedencc parsing, SLR parsing 
Tai. K. C. 2751 
Tail 604 
Tail rwursion 52-53 
Tancnbaum; A .  S. 511,584 
Tantzen, R . G. 82 
Targel Imguage 1 
Target machine 724 
Tarhio. J. 341 
Tarjan, R. E. 158, 388,462, 720-72 1 
Tdiagram 725-728 
Tcmprary 398, 470, 486481. 535, 635. 
639 
Tcrtncnbaum. A .  M. 387. 720-721 
Tcnncnt. R. D. 462 
Terminal 26. 165-167. 281 
Testing 73 1-732 
8-9, 16-17, 82, 731 
Text cdkor !58 
Tcxt formatter 4. 8- 10 
Thompwn. K .  122, 158, 601, 735 
Three-address d e  13- 14, 46W72 
Thunk 429 
Ticnari.M. 278.341 
Tjiang, S. 584 
TMC 277 
Token 4-5, 12, 26-27', 56, 84-86, 98, 
165, 179 
Takuda. T. 278 
Tokura, N, 720 
T , - T analysis 667-668. 67 3-680 
Tmls 724 
Sec o h  Automatic c d c  generator, 
Compilcrampilcr, 
Dab- fluw 
engine. Parscr gcncratw, Scanner 
generator, Syntax-directed transla - 
tion eriginc 
Top element 684 
Topdown paning 41-48, 176, 181-195, 
302, 34 1, 463 
Scc 
also 
Predictive 
parsing, 
Recursive-descent paning 
Topological sort 285. 551 
Trnbb Pardo. L. 24 
Transfer function 674, 681, 689 
Transition diagram 99- 105, 1 14, 183- 
1 85. 226 
Sce also Finitc automaton 
Transition funclion 1 14, 153- 154 
Transition graph 1 14 
Transition tablc 114-1 15 
Translation rule 108 
Translotion s~hcmc 37-40, 297-301 
~ranslhor-writing systcrn 
%e Compiler-compilcr 
Traversal 36, 3163 19 
See also Dcpth-first traversal 
Trcc 2,347,449 
!kt also Activation twc, Depth-first 
spanning 
tree, 
Dt~minator tree, 
Syntax tree 
Trec rewriting 572-580, $84 
Trcc-banslat ion scheme 574-576 
Trcvillyan, L. H. 7 18 
Trickcy. H. W. 4 
Trie 151, 153-154 
Triples 470-472 
Trittcr, A. 82. 511, 725 
TROFF 726. 733-734 
Two-pass assembler 18 
Type 343-388 
Type checking 8, 343-344. 347. 514 
Type constructor 345 
Type mnvcrsion 359-360, 435-487 
See also Coercion 
Type cstirnation 694-702 
T y pc expression 345-347 
Type graph 347, 353, 357-359 
T y v  infcrcnce 364-367, 373-376, 694 
Type name 345-346, 356 
Typ system 347-348, 697-698 
Type variabie 3h6 

U 
Ward, P+ 341 
Ud-chain 621, 642-&I3 
Ukkonen, E, 277 
Ullrnan; J. D. 4, 142, 157, 181, 204, 
277-278, 292, 387, 392, 444-445, 
462, 
566, 
583-584. 
587-588, 
7 Z@'12! 
Unambiguous definition 610 
Unary operator 208 
UNCOL 82, 511 
Unification 376372, 376380,388 
U nim 93-96, 122- 123,378-379 
U niqucness check 343 
Unit production 
See Single production 
Universal quantifier 367-3153 
UNIX 149. 158, 257, 584.725, 735 
Unreachable code 
Sec Dead code 
Upwards exposed usc 633 
Usage munt 542-544, 583 
U w 529. 534535,632 
U =definition chain 
Sce Ud-chain 
Usekss symbal270 
Valid item 225226, 231 
Value number 292-293, 635 
Va be-result linkage 
Sec Copy-restore-lin kage 
Van Staveren 
&e Staveren, H. van 
Vanbegin, M +  342, 512 
Variable 
SBe Identifier, Type variable 
Variable-length data 406, 408-409, 413 
Very busy expression 7 13-7 14 
Viable prefix 201, 217, 224-225. 230-231 
Void type 345, 352 
Vyswtsky, V. 719 
Wagner, R .  A. 158 
Waite, W. M. $1 1-512, 583-584, 720, 
73 r 
' Walter, K. G+ 341 
Warren, S. K. 342 
Wasikw, S, G. 534 
WATFIV 514 
Watt, D. A. 341 
Weak prwcdence 277 
WEB 732 
Wekt, H. 277 
Wegbreit, B. 387, 720 
Wegman, M. N. 388, 72@721 
Wegner, P. 719 
Wegsiein, J. H. 82, 157, 511, 125 
Weihl, W. E, 721 
Weinberger, P. J+ 158, 435 
Weingart, S. 584 
Weinstock, C. B. 489, 543, 583, 
7 19,740 
Welsh, J. 387 
Wexelblat, R. L. 24, 82 
While statement 491493, 504-505 
White space 54,8445, 99 
Wilcox, T. R. 164, 278 
Wilhelm, R+ 341, 512 
Winograd, S. 583 
W interstein. G . 387 
Wirth, N. 82, 277-278, 462, 512, 
W d , D .  277 
. 
Word 92 
Wortman, D. 0. 82, 277 
Wossner, H. 386 
Wulf, W. A. 489, 511, 543, 583-584, 
718-719, 740 
Yacc 257-266, 730.736. 742 
Y arnada, H . 157 
Yannakakis, M. 534 
Kao, A, C. 158 
Yellin, D. 342 
Yield 29 
Younger, D. H. '160, 277 

7% 
INDEX 
Z
I
 
I 2 
Zimrnermann, E. 341 
Zuw. K. 386 

