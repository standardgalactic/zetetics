LUNA: Language Understanding with Number Augmentations on
Transformers via Number Plugins and Pre-training
Hongwei Han1∗†, Jialiang Xu2*†, Mengyu Zhou3‡, Yijia Shao4*, Shi Han3, Dongmei Zhang3
1 Tsinghua University, 2 University of Illinois at Urbana-Champaign
3 Microsoft Research, 4 Peking University
hhw20@mails.tsinghua.edu.cn,
jx17@illinois.edu, shaoyj@pku.edu.cn,
{mezho, shihan, dongmeiz}@microsoft.com
Abstract
Transformers are widely used in NLP tasks.
However, current approaches to leveraging
transformers to understand language expose
one weak spot: Number understanding.
In
some scenarios, numbers frequently occur, es-
pecially in semi-structured data like tables.
But current approaches to rich-number tasks
with transformer-based language models aban-
don or lose some of the numeracy information
– e.g., breaking numbers into sub-word tokens
– which leads to many number-related errors.
In this paper, we propose the LUNA frame-
work which improves the numerical reason-
ing and calculation capabilities of transformer-
based language models. With the number plu-
gin of NumTok and NumBed, LUNA repre-
sents each number as a whole to model in-
put. With number pre-training, including re-
gression loss and model distillation, LUNA
bridges the gap between number and vocabu-
lary embeddings. To the best of our knowl-
edge, this is the ﬁrst work that explicitly in-
jects numeracy capability into language mod-
els using Number Plugins.
Besides evalu-
ating toy models on toy tasks, we evaluate
LUNA on three large-scale transformer mod-
els (RoBERTa, BERT, TabBERT) over three
different downstream tasks (TAT-QA, TabFact,
CrediTrans), and observe the performances
of language models are constantly improved
by LUNA. The augmented models also im-
prove the ofﬁcial baseline of TAT-QA (EM :
50.15 →59.58) and achieve SOTA perfor-
mance on CrediTrans (F1 = 86.17).
1
Introduction
Numbers are common in everyday NLP scenarios.
Transformer-based language models (Qiu et al.,
2020) are popular for number-rich tasks such as
∗The contributions by Hongwei Han, Jialiang Xu and
Yijia Shao have been conducted and completed during their
internships at Microsoft Research Asia, Beijing, China.
† Equal Contribution.
‡ Corresponding author.
Year Ended May 31
-
-
-
Percent Change
-
(Dollars in millions)
2019
Actual
Constant
2018
Interest expense
$2,082
3%
3%
$2,025
Text:
Interest expense increased in fiscal 2019 compared to fiscal 2018 
primarily due to higher average borrowings resulting from our issuance 
of $10.0 billion of senior notes in November 2017, which was partially 
offset by a reduction in interest expense resulting primarily from the 
maturities and repayments of $2.0 billion of senior notes during fiscal 
2019 and $6.0 billion of senior notes during fiscal 2018.
Query:
How much was the actual and constant percentage change in interest 
expense?
Answer:
['3%', '3%']
Operation:
multi-span
Table 1: TAT-QA Example: A Financial Report Table.
TAT-QA (Zhu et al., 2021), TabFact (Chen et al.,
2020) and CrediTrans (Padhi et al., 2021) (see §2.1
for details). For example, Table 1 comes from a ﬁ-
nancial report in TAT-QA. To correctly understand
the question and give the answer, a model must
understand the semantics of each number in this
example. Current approaches to TAT-QA (see §2.1)
often serialize the (table, text, query) triplet into a
sequence, and then feed it into a transformer-based
language model such as RoBERTa.
The weak spot of number understanding of trans-
formers is magniﬁed when facing rich-number
tasks (e.g., the above QA, fact check, time series
modeling tasks) that heavily depends on numeri-
cal reasoning and calculation. Errors could easily
occur by adopting existing number processing and
representation approaches in NLP (e.g., a bad case
for Table 1 in §E.1). There are two great challenges:
tokenization and representation of numbers.
Recent studies have already shown that current
tokenization methods for numbers in language are
suboptimal and sporadic (Thawani et al., 2021):
Numbers are either ﬁltered out, collapsed into
<unk> token, quantized into ﬁnite bins, or split into
arbitrary subword tokens. In other words, numer-
acy information is abandoned or scattered around.
E.g., “3.1415” is split into “3”, “.”, “14”, “15” by
arXiv:2212.02691v2  [cs.CL]  16 Mar 2023

Momentum Transformer 
(no gradient descent)
NumBed
Transformer
𝑯𝑬𝑨𝑫𝑹𝑬𝑮
𝑯𝑬𝑨𝑫𝑪𝑳𝑨
𝑯𝑬𝑨𝑫𝒎𝑪𝑳𝑨
𝑝𝑚
1
𝑝𝑚
2
𝑝𝑚
3
𝑝1
𝑝2
𝑝3
‘Interest’& 𝑝𝑚
1
’08’& 𝑝𝑚
2
𝑝𝑚
3
As Soft Labels
Momentum 
Update
Input Sample
NumTok
CLA Predictions
REG Predictions
𝑣3
[ln(eps+|2082|), sgn(2082)]
CLA labels: hard(𝑳𝑪)&soft(𝑳𝑫)
REG labels(𝑳𝑹)
Distillation
Part II : Number Pre-training
Part I  : Number Plugins
…
…
Evaluating Part I and Part II on 
rich-number downstream tasks 
via fine-tuning
TAT-QA, TabFact, 
CrediTrans
Interest
Ġexpense
$
2
,
08
2
<num 2,082> 
3
%
<num 3%>
…
<mask>
Ġexpense
$
2
,
<mask>
2
<mask> 
3
%
<num 3%>
…
Randomly 
Mask
Q:----------
-------------
------------？
Figure 1: Overview of LUNA Framework. Part I number plugins (introduced in §3.1) include NumTok (number
tokenizer) and NumBed (number embedder). Part II number pre-training (introduced in §3.2). LD, LR, and LC
respectively denote distillation loss (on soft labels), regression loss (on real values), and the original classiﬁcation
loss (classifying the masked tokens, on hard labels). HEADREG and HEADCLA are learnable modules that
respectively project the last hidden states into 2 and vocab-size dimensions. The transformer and momentum
transformer are initialized with the same checkpoint.
BPE (Sennrich et al., 2016) (used in RoBERTa)
and into “3”, “.”, “141”, “##5” by WordPiece (Wu
et al., 2016).
On the other hand, to augment the numerical
reasoning and calculation capability of language
models, previous work such as Time2Vec (Kazemi
et al., 2019) and DICE (Sundararaman et al., 2020)
design value-based feature vectors for number rep-
resentation. However, those methods are not eval-
uated on transformers or real-world tasks (as dis-
cussed in §4, those methods indeed hurt).
To better solve the number issues, we propose
LUNA (Language Understanding with Number
Augmentations on Transformers via Number Plu-
gins and Pre-training) framework. As shown in
Figure 1, LUNA works as a patch for existing lan-
guage models when handling rich-number tasks.
Through tough and massive attempts, we also ex-
clude many failed designs like number prompts (as
discussed in Limitations section), obtaining the
simple and effective LUNA.
Our ﬁrst key idea behind LUNA is that: Each
number should be represented as a whole for model
input (rather than broken into subword tokens or
quantized into binned tokens). Learning embed-
ding representation for raw number strings prop-
erly could help exploit numeracy information for
downstream tasks. In LUNA, this idea corresponds
to Part I number plugin in Figure 1. Each number
in the input (table and text) string will be ﬁrst rec-
ognized by our NumTok (number tokenizer, see
§3.1) and inserted as a special <num ?> (? de-
notes the recognized number string) token into the
original input token sequence. Then, the embed-
ding of <num ?> is computed by our NumBed
(number embedder, see §3.1) which encodes the
recognized number string as an embedding vector
of the same dimension as the token embedding of
the transformer. In detail, for Numtok, we discuss
two designs: AddBack and Replace, which keep
or delete the original representation of numbers,
as will be introduced in §3.1. And for Numbed,
we compare CharLSTM and CharFormer which
encode the char sequence of the number string with
an LSTM or a transformer, and in §3.1, we further
compare different NumBed model sizes.
The newly introduced number embedding may
be incompatible with the embedding space of the
model’s vocabulary tokens. To bridge the gap be-
tween number and vocabulary embeddings, Part II
of LUNA – number pre-training on data from
downstream tasks (see §3.2) – is designed for ei-
ther starting the language model from scratch or
from a pre-trained checkpoint. First, to incorporate
the <num ?> token, we modiﬁed the classical MLM
(masked language model) objective with specially
designed regression loss for it. Then, when starting
from a pre-trained checkpoint, it is easy for the

model to forget existing knowledge and overﬁt. To
prevent this, a teacher-student model distillation
process is designed as a regularization constraint,
and this paper is the ﬁrst work to leverage model
distillation in number pre-training.
By applying LUNA to existing transformer-
based language models, we observe constant perfor-
mance improvements on QA, fact-check, and time-
series tasks (see §4.3) over three transformer-based
language models. We evaluate RoBERTa (Liu et al.,
2019) on TAT-QA (Zhu et al., 2021), BERT (De-
vlin et al., 2018) on TabFact (Chen et al., 2020),
and TabBERT (Padhi et al., 2021) on CrediTrans.
With the help of LUNA framework, RoBERTa
beats the baseline (EM : 50.15 →59.58, and
EMnum : 38.37 →63.33) on TAT-QA. Tab-
BERT reaches new SOTA numbers on CrediTrans
(F1 = 86.17, the previous SOTA is 84.79). By fur-
ther analyzing the dataset and conducting empirical
studies, we further demonstrate how LUNA helps
preserve numeracy information for downstream
tasks. According to the controlled experiments, we
ﬁnd that: 1) For NumTok and NumBed choices,
AddBack is much better than Replace, CharLSTM
is an overall good choice, and a larger numbed
model size is better. 2) For number pre-training,
distillation loss usually helps and regression loss
only helps on tagging tasks. These conclusions
beneﬁt future model design and selection.
In summary, our major contributions are:
• To the best of our knowledge, our work ﬁrst
attempts to explicitly deal with the number
issues on existing transformer-based language
models. Besides evaluating on toy tasks, we
evaluate the numeracy ability of our designs
on real tasks as well.
• We propose LUNA framework to enhance
the number understanding capabilities of lan-
guage models. The code and data of LUNA
are open-sourced at https://github.com/
zmy/LUNA
• We do controlled experiments to discuss
NumTok&NumBed choices and number pre-
training objectives.
2
Related Work
2.1
Rich-number Tasks
In this paper, we take three typical rich-number
datasets and corresponding tasks as examples.
TAT-QA1 (Zhu et al., 2021) is a recent QA
dataset requiring numerical reasoning over realistic
tabular and textual data. In total, TAT-QA contains
16,552 questions associated with 2,757 hybrid (ta-
ble + paragraphs) contexts from real-world ﬁnan-
cial reports. 20% of the hybrid contexts are used
as development and test sets. A table in TAT-QA
has 3∼30 rows and 3∼6 columns with at least 2
human-veriﬁed relevant nearby paragraphs.
TabFact2 (Chen et al., 2020) is a fact veriﬁcation
dataset with 16k Wikipedia tables as evidence for
118k human-annotated statements. In TabFact, for
each Wikipedia table (with caption), the goal is to
distinguish which given statements are entailed by
the table and which are refuted by it. A table in
TabFact has less than 50 rows and 10 columns.
CrediTrans3(Padhi et al., 2021) is a time series
corpus for credit card transactions with 24 million
transactions from 20,000 users. Each transaction
(row) has 12 ﬁelds (columns) consisting of both
continuous and discrete attributes, such as mer-
chant name, merchant address, transaction amount,
etc. We can see an example of TAT-QA over Ta-
ble 1, the one of TabFact and CrediTrans over Ta-
ble 7 and Table 8 in appendix. The numeracy statis-
tics are collected in §B in appendix as well.
Math Word Problems (Sundaram et al., 2022)
are another set of rich-number tasks besides tabular
tasks. We leave it as future work because neural-
based math-word-problem solvers are often genera-
tion models, which need the capability of decoding
numbers. We focus this paper on understanding
instead of generation.
2.2
Numeracy in NLP
In recent years, some effort has been made in boost-
ing numeracy capability in NLP. Position Embed-
ding (Vaswani et al., 2017) embeds integers into
vectors, and as an extension, Time2Vec (Kazemi
et al., 2019) and DICE (Sundararaman et al., 2020)
embed ﬂoat numbers into vectors. TabBERT (Padhi
et al., 2021) quantiﬁes numbers into buckets by
their value. This might be a good attempt, but loses
the information of similarity between nearby num-
bers in different buckets. These approaches embed
numbers based on their values. DICE is famous for
beating many methods on toy tasks like predicting
1Tabular And Textual dataset for Question Answering,
https://nextplusplus.github.io/TAT-QA/
2TabFact, https://tabfact.github.io/
3Credit Card Transaction Dataset, https://ibm.ent.
box.com/v/tabformer-data

+/- of two numbers, and we treat it as a baseline
of NumBed choices. We ﬁnd that DICE is not a
good choice for real tasks4, and even not good on
toy tasks when measured with different metrics, as
discussed in §4.
In most cases, numbers are tokenized in lan-
guage ways in transformer-based language mod-
els. The common practice of language models
usually adopts a ﬁxed vocabulary and the sub-word
tokenization approaches like BPE tokenizer (Sen-
nrich et al., 2016) in RoBERTa and WordPiece
tokenizer (Wu et al., 2016) in BERT. Instead of
considering the number as an ensemble, the tok-
enizer splits the number into arbitrary tokens. Num-
BERT (Zhang et al., 2020) is pre-trained from
scratch over a modiﬁed dataset where all numbers
have been replaced with scientiﬁc notation (e.g.,
replacing 314.1 with 3141[EXP]2), in order to help
the model understand the signiﬁcance and expo-
nent of numbers. However, NumBERT still breaks
a number into sub-word pieces. As we have dis-
cussed in §1, breaking a number into sub-word
pieces or quantifying it is not a grounded practice.
A bad case analysis is provided in §E.1.
Some previous work also focuses on number
decoding (Spithourakis and Riedel, 2018).
3
Methods
As shown in Figure 1, there are two major parts in
LUNA: Part I, number plugin modules – NumTok
(see §3.1) and NumBed (see §3.1) – which change
the input to a language model such as RoBERTa
or BERT. Part II, number pre-training (part II, see
§3.2) from scratch or from an existing checkpoint
of the language model.
In LUNA, we take the following steps to enhance
a language model: First, NumTok changes the re-
sult token sequence of the model’s tokenizer by in-
serting a special <num ?> (? denotes the recognized
number string) token for each number. Second, the
embedding of <num ?> is computed by NumBed
which encodes the recognized number string as an
embedding vector of the same dimension as the
word-embedding. Third, number pre-training is
taken place to align the space of number embedding
and that of vocabulary embedding. This will also
allow the model to ﬁt unlabeled downstream data.
Finally, the common ﬁne-tuning process on the
language model (with its additional task-speciﬁc
4We don’t try Time2Vec, because numbers are not peri-
odic.
“1.76%-2.50%”
1
.
76
%
2
.
50
%
-
1
.
76
%
2
.
50
%
-
<num 
1.76%>
<num 
2.50%>
-
BPE/WordPiece
NumTok (Replace)
NumTok (AddBack)
1
.
76
%
2
.
50
-
<num 
1.76%>
<num 
2.50%>
<num 
1.76%>
<num 
2.50%>
Figure 2: The Two Options of NumTok, Compared
with BPE/WordPiece.
modules) is taken for a downstream task.
3.1
Number Plugins
The ﬁrst step (part I) in LUNA is Number Plugins.
NumTok (as a wrapper) slightly changes the be-
havior of a model’s default tokenizer by inserting
“<num>” into an input string at locations where a
number appears. Then, “<num>” will be recog-
nized as a new special token <num ?> by NumTok
in its output. As shown in Figure 2, there are two
ways <num ?> to be inserted by NumTok: Ad-
dBack and Replace. The former adds <num ?> to
the end of the original vocabulary token(s) of the
number. The latter fully replaces these token(s).
The key algorithm in NumTok is recognizing
numbers in the input string. We adopt a simple
but effective ﬁlter-split-check approach to ensure
characters in the same number are recognized as
a whole, while irrelevant characters are excluded.
E.g., “1.76%-2.50%” should be recognized as two
positive percentages “1.76%” and “2.50%”. Regu-
lar expressions used by NumTok are listed in §C.2.
Then, we should assign each <num ?> token an
embedding vector that could be used for operations
such as attention with other vocabulary tokens. For
this, we propose NumBed, which is a trainable
neural network aiming at generating embedding
reﬂecting numeracy properties of the original num-
ber. As shown in Figure 1, the number (<num ?>
token) embedding is the generated NumBed em-
bedding, which will be part of the input sequence
to the language model.
For the model design of NumBed, there is a wide
range of choices, some provided by previous work
like DICE (Sundararaman et al., 2020) and some
designed by us: CharLSTM embeds the number
on character level, i.e. the characters in the number
string are ﬁrst embedded with one-hot lookup em-
bedding and then passed into a Bidirectional LSTM
model. The underlying intuition is that 1) seman-
tics in numbers could be queried either left-to-right
or right-to-left, and 2) LSTM naturally encodes
positional order of the digits, which mimics the

deﬁnition of number value (the sum of numbers
on all digits, multiplied by its digit signiﬁcance).
The characteristic embedding is obtained by av-
eraging the ﬁnal hidden state of all LSTM layers.
CharFormer utilizes a transformer encoder layer
and rule- based positional encoding. Characters
in the number string are ﬁrst embedded with one-
hot lookup embedding and then added by a posi-
tional embedding indicating its digit signiﬁcance
(i.e. the relative position w.r.t. the decimal point).
The result embedding sequence is then sequentially
passed through the transformer encoder layer and
an LSTM model. The characteristic embedding is
obtained by averaging the ﬁnal hidden state of all
LSTM layers.
3.2
Number Pre-training
The next step (part II) in LUNA is number pre-
training on downstream data. There are two rea-
sons to do so: First, randomly initialized <num ?>
embedding could damage the distribution of orig-
inal input embeddings. Pre-training could help
bridge the gap between number and vocabulary
embeddings. Second, by pre-training on data of
downstream tasks, the model can learn better the
domain distributions of the tasks compared to di-
rectly ﬁne-tuning (Gururangan et al., 2020).
3.2.1
Pre-training from Checkpoints
Reproducing the original pre-training process of a
language model can be very costly. Thus, the ﬁrst
option in LUNA part II is to conduct intermediate
pre-training from a pre-trained checkpoint (like
RoBERTa and BERT) of the model.
The classical mask language model (MLM) ob-
jective (Devlin et al., 2018) is adapted for LUNA.
We keep the common practice that overall 15% of
the input tokens are masked for recovery at the cor-
responding positions of the model output. For 80%
of them, the input tokens are replaced by a [MASK]
token. For 50% of the rest, they are replaced by a
randomly selected token from the vocabulary; For
the other 50%, they keep what they are.
For a masked <num ?> token, a newly designed
regression supervision is applied to ﬁt the log ab-
solute and the sign of its corresponding numeric
value. And for a masked text token, classiﬁca-
tion supervision is used. To be exact, let k de-
note the number of masked tokens in a mini-batch
(about 15% of total), N = {(o, v)} denote the set
of output vector and real value of masked <num
?> tokens, T = {(o, t)} denote the set of output
vector and original token of masked vocabulary
tokens. Obviously, |N| + |T | = k. Let a learnable
MLP HEADREG denote the regression head and
HEADCLA denote the classiﬁcation head. Then
we can deﬁne MLM loss (LMLM) as:
LMLM = 1
k(LREG + LCLA)
LREG =
X
N
MSE(HEADREG(o),
[ln(eps + |v|), sgn(v)])
LCLA =
X
T
CE(HEADCLA(o), onehot(t))
Here the mean-squared-error regression loss
(LREG or LR) takes in ln(eps + |v|) to reﬂect
actual range rather than exact value. The classiﬁca-
tion loss (LCLA or LC) is the usual cross entropy.
Since downstream datasets are usually far
smaller than the original pre-train datasets of the
models, the overﬁtting problem may happen during
the LUNA number pre-training phase. To solve the
problem, we adopt a regularization method called
model distillation (Li et al., 2021).
During model distillation, there is a student net-
work Net and a teacher network (also called mo-
mentum network) Netm. They both start with the
same parameter initialization (from a checkpoint),
take the same masked embedding sequence as in-
put, and generate probability distribution vectors
(p by Net and pm by Netm) over the vocabulary.
Please note that there is not a so-called distillation
head, and p is projected by HEADCLA. When
treating pm as soft label (different from the hard
label t in LC), the distillation loss (Ldistill or LD)
is deﬁned as:
Ldistill = 1
k
X
N+T
CE(p, pm)
When applying LD, Net is updated via gradient,
but Netm is updated only via a fraction of Net.
In other words, Netm is the momentum accu-
mulation of Net. Let τ denote the momentum
coefﬁcient (default set to 0.995). At each step,
Netm ←τNetm + (1 −τ)Net.
The total pre-train loss (Lpre−train) is deﬁned
as (where α is a warm-up coefﬁcient which grows
with the training steps):
Lpre−train = (1 −α)LMLM + αLdistill
Overall, HEADCLA is under the supervision
of LC and LD, HEADREG is under the supervi-

sion of LR, and HEADm CLA is the momentum
accumulation of HEADm CLA.
3.2.2
Pre-training from Scratch
When there are enough resources, one can still re-
run the original pre-training process of a language
model (like TabBERT) with LUNA number plu-
gins and MLM loss (LMLM, which includes the
regression loss) without model distillation. How-
ever, pre-training from scratch is not the focus of
this paper and we just want to verify the effective-
ness of LUNA in different situations.
4
Experiments
4.1
Datasets and Evaluation Metrics
In §2, we have already discussed several language
models (RoBERTa, BERT, TabBERT) and their cor-
responding tasks (TAT-QA, TabFact, CrediTrans).
Both part I and part II of the LUNA framework will
be evaluated on these models and tasks. For sim-
plicity, in this paper, we only implemented limited
model&task combinations: RoBERTa on TAT-QA,
BERT on TabFact, and TabBERT on CrediTrans.
4.1.1
Number Pre-training Datasets
As discussed in §3.2, before ﬁne-tuning on the
aforementioned tasks, in LUNA a language model
will be pre-trained with rich-number data.
When pre-training from the checkpoint with
tuned parameters, MLM loss will be added together
with distillation loss (see §3.2.1). For RoBERTa
and BERT running on text and table inputs, we
collect unlabeled table-text pairs (which are serial-
ized into sequences) from TAT-QA and WikiTables
(where TabFact comes from). When pre-training
from scratch with randomly initialized parameters,
MLM with regression loss is adopted in LUNA (see
§3.2.2). In our experiments, we pre-train TabBERT
(with number plugin) from scratch on CrediTrans
samples. Please see §D.1 for more details about
pre-training datasets.
4.1.2
Evaluation Tasks and Metrics
First, we evaluate NumBed on toy tasks purely
about numbers.
We construct a pure number
dataset from real-world numbers in the number pre-
training dataset mentioned above. Utilizing prob-
ing models and training processes of DICE (Sun-
dararaman et al., 2020), we evaluate models’ nu-
meracy abilities on a series of number-related tasks:
1) decoding the number, 2) predicting the result of
adding/subtracting two numbers, 3) locating the
Decoding
Addition
Subtraction
List Max
Sig↓
Exp
Sig↓
Exp
Sig↓
Exp
Acc
CharLSTM
0.0946
99.97%
0.5572
99.46%
1.367
97.17%
98.55%
CharFormer
0.1799
99.85%
0.6718
99.44%
1.546
96.86%
97.71%
RoBERTa
2.400
34.75%
2.332
37.80%
4.166
52.16%
32.84%
DICE
2.309
67.90%
2.453
69.29%
3.819
39.53%
92.45%
Table 2: Probing numeracy on a number dataset con-
structed from numbers in real-world tables.
We re-
port the RMSE value for all signiﬁcand (sig) predicting
tasks, and classiﬁcation accuracy for all exponent (exp)
predicting tasks and the list maximum task. ↓denotes
lower is better. We use the same probing models as in
Sundararaman et al. (2020).
maximum value in a list of numbers. For 1) and
2), unlike DICE, we separately report RMSE for
the signiﬁcand and ACC for the exponent5 instead
of RMSE for the original value, because we think
numbers with different scales are equally important
(giving 10 when gt=1 should be worse than giving
1200 when gt=1000).
Then, we evaluate LUNA on real-world rich-
number tasks: TAT-QA, TabFact, and CrediTrans.
We split the dataset into train, valid, and test sets
according to their previously reported ratios: 8:1:1,
8:1:1, and 6:2:2, respectively. In this paper, by
default, the test sets will be used for reporting eval-
uation metrics.
On TAT-QA, the original evaluation metric EM
(Exact Match) and F1 scores will be adopted.
EMnum is the exact match score over arithmetic
questions that are ofﬁcially split from TAT-QA by
their authors. The arithmetic questions are those
whose answer is given by operating “+/ −/ × /÷”
on numbers from the table or the text. On Tab-
Fact, beyond the original accuracy ACC, we also
calculate the ACCcx on its complex subset. On
CrediTrans, its original F1 score is calculated.
4.2
Experimental Setups
4.2.1
Baseline and Ablation Studies
To understand how each component of LUNA con-
tributes to the overall improvements, we design a
series of ablation studies. First, with part I number
plugin and part II number pre-training vs. without
them ( 0 , 8 and X in Table 3). Here, row X or
3 correspond to the baseline models where LUNA
is not applied or DICE is used as NumBed. Then,
during pre-training, with the new losses LREG and
Ldistill vs. without them ( 0 , 6 and 7 ).
For fair comparisons, all evaluations are done on
1 node with the same environment conﬁguration.
5E.g., the signiﬁcand and exponent of the number 3142 are
3.142 and 3 respectively, since 3142=3.142E3

LUNA Choices
RoBERTa
BERT
Part I
Part II
TAT-QA
TabFact
NumTok
NumBed
+Size
LD
LR
LC
EM
F1
EMnum
ACC
ACCcx
AVG
0
AddBack
CharLSTM
9M
✓
✓
✓
59.582.7
67.152.7
63.336.4
66.070.5
62.240.4
63.67
1
Replace
CharLSTM
9M
✓
✓
✓
51.871.7
59.941.6
50.214.0
65.410.5
61.430.6
57, 77
2
AddBack
CharFormer
9M
✓
✓
✓
57.034.0
64.833.8
57.489.8
66.810.1
62.580.2
61.75
3
AddBack
DICE
×
✓
✓
✓
47.151.2
54.931.0
35.393.1
62.611.2
59.510.6
51.92
4
AddBack
CharLSTM
1M
✓
✓
✓
57.862.7
65.562.6
58.416.4
65.730.1
61.790.3
61.87
5
AddBack
CharLSTM
0.1M
✓
✓
✓
56.992.0
64.961.7
57.814.5
65.400.7
61.670.6
61.37
6
AddBack
CharLSTM
9M
✓
×
✓
55.512.9
63.622.8
53.755.3
67.230.7
62.940.5
60.61
7
AddBack
CharLSTM
9M
×
✓
✓
54.332.3
62.132.0
51.735.5
65.400.3
62.190.1
59.16
8
AddBack
CharLSTM
9M
×
×
×
52.052.8
59.922.8
44.756.7
62.411.8
59.871.1
55.80
X
×
×
×
×
×
×
50.150.8
57.840.7
38.371.9
62.130.6
59.710.4
53.64
Table 3: LUNA Evaluations on Language Models (RoBERTa, BERT) and Downstream Tasks (TAT-QA, TabFact).
The last column AVG is the average of the ﬁve metrics, we add this column to demonstrate the overall capability
of each controlled experiment.
LUNA Choices
CrediTrans
NumTok
NumBed
+Size
LR
LC
F1
A
AddBack
CharLSTM
9M
✓
✓
86.170.1
B
Replace
CharLSTM
9M
✓
✓
83.040.0
C
×
×
×
✓
✓
85.651.6
D
AddBack
CharLSTM
1M
✓
✓
84.890.6
E
AddBack
CharLSTM
0.1M
✓
✓
84.010.8
X
×
×
×
×
✓
84.790.3
Table 4: LUNA Evaluations on TabBERT and Cred-
iTrans.
Since TabBERT is pre-trained from scratch,
nothing can be learnt from the teacher model and LD
is not used.
By default, all evaluation metrics reported in the
following are averaged over 3 runs for experiments
with randomness, and the standard error is reported
along with the mean as a subscript (in the form
of meanstd). Please ﬁnd more training details in
Appendix §D.
4.2.2
Comparing Number Plugin Choices
There could be many possible ways to design Num-
Tok (see §3.1) and NumBed (see §3.1). Among
many, we choose several promising ones to com-
pare through controlled experiments. For Num-
Tok, we compare the AddBack and Replace al-
gorithms. They correspond to row 0 and 1 re-
spectively. For NumBed, CharLSTM 0 and Char-
Former 2 are compared with each other. For part I
of LUNA, the major hyper-parameter is the model
size of NumBed. In column “+Size” of Table 3
and Table 4, we will examine how model size im-
pact the improvement margin on downstream tasks.
Specially, NumBed choices are also discussed on
toy tasks in Table 2, where row “RoBERTa”6 and
“DICE” are baselines.
6encoding the number string with RoBERTa and using the
pooled output as the number embedding
Subset
Count
baseline
ours
Span
701
62.62
63.48
Multi-span
217
68.66
71.43
Arithmetic
654
36.39
69.57
Count
32
43.75
43.75
Table 5: LUNA Evaluations on TAT-QA Subsets. For
TAT-QA, the metric is EM, and “baseline” and “ours” respec-
tively denote row X and 0 in Table 3.
4.2.3
Exploring Pre-training from Scratch
As we have discussed in §3.2, pre-training from
scratch is an option when enough computing re-
sources and time are available. Based on our exper-
iments of pre-training from checkpoints in Table 3,
in Table 4 we design several controlled experiments
on NumTok strategies, NumBed model size and
loss designs. Here, row X corresponds to the base-
line TabBERT where LUNA is not applied. Row
C denotes adding LR to the baseline TabBERT
but without using Number Plugins.
Note that TabBERT is mainly designed for mul-
tivariate time series data, it models both inter- and
intra-row dependencies by tokenizing rows at the
cell level (i.e., each cell corresponds to a single
token) and using a hierarchical structure to convert
cell embeddings in the same row into row embed-
ding. Since each cell corresponds to a single token,
the original-deﬁned AddBack option of NumTok
is hard to apply on TabBERT while the Replace
option is still applicable. As an alternative, we in-
troduce a new way to “insert” <num ?> in the case
of TabBERT by adding the NumBed embedding
back to the original vocabulary token embeddings
as a variant of AddBack. This variant of AddBack
still tries to retain original token information while
infusing additional number information.

4.3
Results and Insights
By examining Table 3 and Table 4, one can ﬁnd
following insights.
On all rich-number tasks, models with LUNA
outperform their original ones. For each column
in Table 3 and Table 4, the top results are always
achieved by applying one variation of LUNA (row
0 - 8 ) and higher than the original models (row
X ) or related work (row 3 ). This means by prop-
erly selecting part I and part II options for a model
and a task, LUNA could help boost performance.
On
TAT-QA
task,
the
previous
baseline
RoBERTa is improved by more than 9% after ap-
plying LUNA (row 0 , column “RoBERTa” in Ta-
ble 3). On CrediTrans task, the SOTA model Tab-
BERT is also improved by applying LUNA (row
A in Table 4).
AddBack option (row 0 ) for NumTok is gener-
ally a good choice comparing to the Replace option
(row 1 ). The gap is signiﬁcant in RoBERTa and
BERT results. AddBack also yields the best result
(row A ) on CrediTrans task.
The best choice of NumBed varies for models
and tasks, but CharLSTM is an overall good choice.
By comparing row 0 , 2 , 3 with each other, we
can ﬁnd that NumBed choices matter. CharLSTM
and CharFormer are signiﬁcantly better than DICE,
and this phenomenon occurs on the toy task Table 2
as well. According to those experiment results,
we can say that DICE is not a strong baseline for
NumBed. Specially, we also run another baseline
for RoBERTa on TAT-QA that replaces number
strings with their language representations (e.g.,
replacing “1,100” with “a thousand and one hun-
dred”), and the result (EM = 33.35, F1 = 41.29,
and EMnum = 55.13) is too bad to be comparable.
For part II number pre-training of LUNA, we
ﬁnd that model distillation is also an overall good
choice. Comparing row 0 and 7 , we ﬁnd the
gain of adding the distillation loss as regularization
is large while no signiﬁcant drawback is shown.
However, the effect of regression loss on number
tokens is dependent on tasks. E.g., for TAT-QA,
row 6 is worse than row 0 ; for CrediTrans, row
X is worse than row A ; but for TabFact, row 6
is better than row 0 . This might because, TAT-
QA and CrediTrans are tagging tasks that tag on
each token of the last hidden state, and TabFact
is a classiﬁcation task that only uses the <cls>
token of the last hidden state. When we add LD to
each masked token, the information of <cls> gets
diluted and perturbed.
The size of the NumBed model also matters.
From 5 to 4 to 0 , and from E to D to A , with
the NumBed size grows, the performances always
become better.
In detail, we also analyze the improvement we
bring to different subsets of TAT-QA. We select
the best random seed among all three repeats to do
subset result analysis. As shown in Table 5, our
method mostly improves the results on arithmetic
questions in TAT-QA (from 36.39 to 69.57). This
might because arithmetic questions need the most
numeracy capability which LUNA provides.
Let’s summarize,
• LUNA indeed helps for transformer-based lan-
guage models.
• For NumTok choices, AddBack is much better
than Replace.
• For NumBed choices, CharLSTM is an over-
all good choice (much better than the previ-
ously popular DICE), and enlarging numbed
model size also helps.
• For number pre-training, distillation loss helps
when pre-training from a checkpoint, and re-
gression loss only helps on tagging tasks.
To ﬁgure out how LUNA works, we also do
empirical studies in §E, including visualization of
attention maps (in §E.1) and embeddings from dif-
ferent layers (in §E.2).
5
Conclusion
In this paper, we propose a patch framework for
language models – LUNA (Language Understand-
ing Number Augmentations on Transformers via
Number Plugins and Pre-training) – to enhance
their numerical reasoning and calculation capabil-
ities. Through thorough experiments and empiri-
cal studies, we show that by adding number em-
beddings from the whole raw number string, and
continuously pre-training language models with
downstream data, the performance of the language
models could improve considerably. We believe
that the techniques proposed in LUNA could be
applied to more scenarios including Math Word
Problems and other rich-number tasks.
Limitations
LUNA is still preliminary work that requires in-
depth explorations in the future. Many possible

choices and evaluations are not included in the
paper due to time and space limitations.
First, our current NumTok design only handles
decimal numbers, ignoring edge cases such as num-
bers in scientiﬁc notation (e.g., “1.5e-9”), non-
decimal bases (e.g., “0x12BF”), and formats out of
arabic-hindu notations (e.g., Roman number “XII”,
natural language numbers “twenty-one” and “vein-
tiuno”). Also, units and magnitudes are not taken
into the tokenizer design. Some words could even
only be understood by a mixture of numbers and
words (e.g., “H2O”). All these aspects could be
improved in future work.
Second, our current NumBed approach only sup-
ports encoder-only language models. In this paper,
we only explore number encoders and leave out
number decoders. This prevents us from apply-
ing the LUNA ideas to generative language mod-
els such as TaPeX, TableGPT, etc. Also, the best
choice of NumBed and how to initialize its parame-
ters is still an open question. How to bring the best
number representing methods together still requires
lots of research efforts.
Third, MLM with regression loss and model dis-
tillation is a relatively simple approach for interme-
diate pre-training. There are many other possible
pre-training objectives to be tried for better number
understanding. For example, can we design pre-
training objectives for a series/column of numbers?
Finally, before proposing LUNA, we have made
many failed attempts. E.g., using the Numbed
trained on toy tasks as initialization for number
pre-training, or using number prompts that gives
additional key and value for each transformer layer
instead of directly inputting NumBed. We cannot
derive rigorous proof of why they don’t work.
Ethics Statement
Datasets
This work collects the public dataset for
research perposes. We believe there is no privacy
issue, because TAT-QA, TabFact, and CrediTrans
are accessible to the public.
Models
This work uses three large-scale lan-
guage models – RoBERTa, BERT, and TabBERT
– among which RoBERTa and BERT are PLMs
pre-trained on clean and non-evil text, and all the
three models are number-pretrained/ﬁnetuned on
clean downstream data. Therefore, we can make
sure that the models we trained will not produce
discriminatory answers.
Computational Resources
Our methods require
low computational resources because the designed
number pre-training is an intermediate pre-training
that takes a few hours to be completed. Besides,
the carbon footprints of all GPUs are monitored in
real-time.
References
Wenhu Chen, Hongmin Wang, Jianshu Chen, Yunkai
Zhang, Hong Wang, Shiyang Li, Xiyou Zhou,
and William Yang Wang. 2020.
TabFact:
A
large-scale dataset for table-based fact veriﬁca-
tion.
In International Conference on Learning
Representations (ICLR), Addis Ababa, Ethiopia.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2018.
BERT: pre-training of
deep bidirectional transformers for language under-
standing. CoRR, abs/1810.04805.
Suchin
Gururangan,
Ana
Marasovi´c,
Swabha
Swayamdipta, Kyle Lo, Iz Beltagy, Doug Downey,
and Noah A. Smith. 2020. Don’t stop pretraining:
Adapt language models to domains and tasks.
In
Proceedings of the 58th Annual Meeting of the
Association for Computational Linguistics, pages
8342–8360, Online. Association for Computational
Linguistics.
Seyed Mehran Kazemi, Rishab Goel, Sepehr Eghbali,
Janahan Ramanan, Jaspreet Sahota, Sanjay Thakur,
Stella Wu, Cathal Smyth, Pascal Poupart, and Mar-
cus A. Brubaker. 2019. Time2vec: Learning a vector
representation of time. CoRR, abs/1907.05321.
Junnan
Li,
Ramprasaath
R.
Selvaraju,
Akhilesh Deepak Gotmare,
Shaﬁq Joty,
Caim-
ing Xiong, and Steven Hoi. 2021. Align before fuse:
Vision and language representation learning with
momentum distillation. In NeurIPS.
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-
dar Joshi, Danqi Chen, Omer Levy, Mike Lewis,
Luke Zettlemoyer, and Veselin Stoyanov. 2019.
Roberta: A robustly optimized bert pretraining ap-
proach. CoRR, abs/1907.11692.
Inkit Padhi, Yair Schiff, Igor Melnyk, Mattia Rig-
otti, Youssef Mroueh, Pierre Dognin, Jerret Ross,
Ravi Nair, and Erik Altman. 2021.
Tabular
transformers for modeling multivariate time se-
ries.
In ICASSP 2021-2021 IEEE International
Conference on Acoustics,
Speech and Signal
Processing (ICASSP), pages 3565–3569. IEEE.
Xipeng Qiu, Tianxiang Sun, Yige Xu, Yunfan Shao,
Ning Dai, and Xuanjing Huang. 2020. Pre-trained
models for natural language processing: A survey.
CoRR, abs/2003.08271.
Rico Sennrich, Barry Haddow, and Alexandra Birch.
2016. Neural machine translation of rare words with
subword units. In Proceedings of the 54th Annual

Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), pages 1715–
1725, Berlin, Germany. Association for Computa-
tional Linguistics.
Georgios Spithourakis and Sebastian Riedel. 2018.
Numeracy for language models:
Evaluating and
improving their ability to predict numbers.
In
Proceedings of the 56th Annual Meeting of the
Association for Computational Linguistics (Volume
1:
Long Papers), pages 2104–2115, Melbourne,
Australia. Association for Computational Linguis-
tics.
Sowmya S. Sundaram,
Sairam Gurajada,
Marco
Fisichella, Deepak P, and Savitha Sam Abraham.
2022.
Why are NLP models fumbling at elemen-
tary math? A survey of deep learning based word
problem solvers. CoRR, abs/2205.15683.
Dhanasekar Sundararaman, Shijing Si, Vivek Subra-
manian, Guoyin Wang, Devamanyu Hazarika, and
Lawrence Carin. 2020.
Methods for numeracy-
preserving word embeddings. In Proceedings of the
2020 Conference on Empirical Methods in Natural
Language Processing (EMNLP), pages 4742–4753,
Online. Association for Computational Linguistics.
Avijit Thawani, Jay Pujara, Filip Ilievski, and Pedro
Szekely. 2021.
Representing numbers in NLP: a
survey and a vision.
In Proceedings of the 2021
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies, pages 644–656, Online. As-
sociation for Computational Linguistics.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz
Kaiser, and Illia Polosukhin. 2017.
Attention is
all you need.
In Advances in Neural Information
Processing Systems 30:
Annual Conference on
Neural
Information
Processing
Systems
2017,
December 4-9, 2017, Long Beach, CA, USA, pages
5998–6008.
Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V.
Le,
Mohammad Norouzi,
Wolfgang Macherey,
Maxim Krikun,
Yuan Cao,
Qin Gao,
Klaus
Macherey, Jeff Klingner, Apurva Shah, Melvin John-
son, Xiaobing Liu, Łukasz Kaiser, Stephan Gouws,
Yoshikiyo Kato, Taku Kudo, Hideto Kazawa, Keith
Stevens, George Kurian, Nishant Patil, Wei Wang,
Cliff Young, Jason Smith, Jason Riesa, Alex Rud-
nick, Oriol Vinyals, Greg Corrado, Macduff Hughes,
and Jeffrey Dean. 2016. Google’s neural machine
translation system: Bridging the gap between human
and machine translation. CoRR, abs/1609.08144.
Xikun Zhang, Deepak Ramachandran, Ian Tenney,
Yanai Elazar, and Dan Roth. 2020.
Do language
embeddings capture scales?
In Findings of the
Association for Computational Linguistics: EMNLP
2020, Online Event, 16-20 November 2020, vol-
ume EMNLP 2020 of Findings of ACL, pages 4889–
4896. Association for Computational Linguistics.
Fengbin
Zhu,
Wenqiang
Lei,
Youcheng
Huang,
Chao Wang, Shuo Zhang, Jiancheng Lv, Fuli
Feng,
and Tat-Seng Chua. 2021.
TAT-QA:
A question answering benchmark on a hybrid
of tabular and textual content in ﬁnance.
In
Proceedings of the 59th Annual Meeting of the
Association for Computational Linguistics and the
11th International Joint Conference on Natural
Language Processing (Volume 1:
Long Papers),
pages 3277–3287, Online. Association for Compu-
tational Linguistics.
A
Code and Data
Please ﬁnd our current code and data at https:
//github.com/zmy/LUNA
B
Number Statistics
In TAT-QA and TabFact datasets, 100% and
94.79% of the tables have numbers in them. In our
number pre-train dataset, 97% of the tables have
numbers in them, 54.94% of all the 2,471,520 table
cells have numbers in them. (While only 17.72%
table cells contain at least 2 numbers.) When ana-
lyzing tables at row-level or column levels, num-
bers are also hard to ignore for 71.36% of the table
columns and 85.25% of the table rows.
Numbers are shown as strings in tables and texts.
After ignoring edge cases, such as scientiﬁc nota-
tion, the remaining decimal number strings have
clear format patterns in them: Most of them are pos-
itive numbers (0.12% of all numbers are negative);
91.00% and 9.00% of all numbers are integers and
ﬂoat numbers, respectively; 1.51% and 5.02% of all
numbers have percent “%” or comma “,” character
in them, respectively.
Compared to tables, in plain texts numbers are
relatively sparse. Out of 59,645 pieces of texts
(including queries, paragraphs, and captions) in the
three datasets, there are 934,121 English words but
there are only 76,532 numbers in them.
C
NumTok Details
C.1
NumTok Character Set
In this work, we chose a set of characters that suits
our downstream tasks the best, which consists of
numeric digits 0-9, the percentage symbol (“%”),
the plus and minus signs (“+”, “-”), the decimal
point (“.”), and the thousands separator (“,”). This
set can also be updated to suit different downstream
tasks.

C.2
Number Regular Expressions
In this work, we designed regular expressions for
integral, ﬂoat, percentage and thousands-separated
numbers.
Number shape
Regular Expression
Conventional
[+-]?\d+(?:\.\d*)?%?
Thousands-separated
[+-]?\d{1,3}(?:,\d{3})*(?:\.\d+)?%?
Dot-started Decimal
[+-]?\.\d+%?
Table 6: NumTok Regular Expressions
D
Training Details
D.1
Number Pre-training Datasets
We ﬁrst construct a number pre-training dataset
(TAT-QA+WikiTables) for RoBERTa and BERT
(pre-training from checkpoints).
For TAT-QA,
questions and paragraphs will be equally treated as
text, and for TabFact, only positive statements of a
table from the training set (to avoid data leakage)
will be considered as text. Totally, 103K table-text
pairs are collected. The collected dataset contains
18K tables. Over these tables, the average count
of rows and columns is 14.5 and 6.8 respectively,
and the average number of non-empty cells is 30.2.
The average number of words in the text is 19.3.
(see §B for more statistics. )
For TabBERT (pre-training from scratch), fol-
lowing (Padhi et al., 2021), we create samples by
combining 10 contiguous rows in a time-dependent
manner and quantize continuous ﬁelds to build a
local ﬁnite vocabulary. All 2.4M samples are used
in pre-training. To avoid data leaking for the down-
stream fraud detection task, we exclude the label
column "isFraud?" in the pre-training corpus.
D.2
RoBERTa
For number pre-training, we set lr=4e-5, epoch=4,
batch-size=96, and totally train 4.2K steps on 32
GPUs. For TAT-QA ﬁnetune, we set lr=5e-6 for
RoBERTa and lr=1.5e-4 for the rest of parameters,
epoch=25, batch-size=32, and totally train 10K
steps on 8 GPUs.
D.3
BERT
For number pre-training, we set lr=3e-5, epoch=18,
batch-size=24, and totally train 77K steps on 8
GPUs.
For TabFact ﬁnetune, we set lr=1e-5,
epoch=10, batch-size=48, and totally train 19K
steps on 8 GPUs.
year
title
us
 us r&b
        us dance
1984
pretty mess
75
15
13
1985
mechanical emotion
107
23
2018
1986
under the influence
56
9
6
1986
animals
1988
undress
Positive Statement: 
during 1986 , 6 was the value for us dance when the value of us r&b was 9
Negative Statement:
under the influence charted at 56 in the us in 1985 and 1986
Table 7: TabFact Example: A Wikipedia Table.
User
Card
Year
Month
Day
Time
Amount
Use Chip
Merchant Name
Merchant City
...
Errors?
Is Fraud?
0
0
2002
9
9
06:54
$37.50
Swipe
Alice
La Verne
No
0
0
2002
9
9
09:40
$65.50
Swipe
Bob
La Verne
Technical Glitch
No
0
0
2002
9
9
13:19
$56.42
Swipe
Carol
La Verne
No
0
0
2002
9
9
13:31
$2.71
Swipe
Alice
La Verne
No
0
0
2002
9
9
20:02
$144.90
Online
Dave
ONLINE
No
0
0
2002
9
10
06:20
$160.00
Swipe
Eva
Mira Loma
No
0
0
2002
9
10
06:22
$102.18
Swipe
Francis
La Verne
No
0
0
2002
9
10
06:31
$36.73
Swipe
Francis
La Verne
No
0
0
2002
9
10
21:39
$29.33
Swipe
Hans
Mira Loma
No
0
0
2002
9
11
06:28
$162.39
Swipe
Alice
La Verne
No
Table 8: CrediTrans Example: A Transaction Table.
D.4
TabBERT
For pre-training, following the original implemen-
tation of TabBERT, we set lr=5e-5, epoch=3, batch-
size=24, and totally train 38K steps on 8 GPUs.
For CrediTrans ﬁnetune, we set lr=1e-3, epoch=10,
batch-size=256. During the ﬁne-tuning stage, The
TabBERT model is ﬁxed as a feature extractor and
we only update the parameters of the LSTM pre-
diction head.
E
Empirical Studies
In this section, we select several cases in down-
stream tasks to demonstrate how LUNA improves
number understanding. Then we visualize the num-
ber embedding space learned by NumBed, and
compare it with the subword token embedding
space.
E.1
Case Studies
LUNA improves language models by helping them
ﬁxing number-related errors. In Table 5 we have
already shown the statistics. Now let’s take a look
at how the errors discussed in §2.2 are ﬁxed with
the help LUNA framework.
For the TAT-QA input case mentioned in Ta-
ble 1, the original Roberta tags at “$2,082” and
“$2,025”, operates “CHANGE_RATIO” to them
and returns “0.0281”. This is because, the model
is confused by the segment, “percentage change
in interest expense”, in the question. The error

...
...
...
...
baseline
ours
Ans
0.0281
Op
CHANGE
_RATIO
Ans
[0.0300,
0.0300]
Op
MULTI_S
PAN
baseline’s wrong prediction
ours correct prediction
Figure 3: A part of attention map between question and
table+paragraph of Table 1. “baseline” and “ours” re-
spectively denote row X and 0 in Table 3.
Figure 4: The TSNE visualization of the output of
different layers when 0 handles Table 1. Note that
the same number strings may have different embed-
dings due to their different positions in the serialized
sequence.
shows that the model cannot understand numbers
and correlate numbers to text. When seeing the
word “percentage change”, the model arbitrarily
ﬁlters the same-number pairs.
Figure 3 shows the attention map of the last but
one transformer layer of RoBERTa with Table 1 as
input. As you can see, the original attention map is
a mess, which leads to the wrong prediction. Mean-
while, our method focuses on the core information,
“3%”, which results in the correct answer.
E.2
Number Embedding
We apply TSNE visualization to different trans-
former layers when taking Table 1 as input. As
you can see in Figure 4, the deeper the layer is,
the better fusion between numbed and word em-
baseline
without number pre-training
Figure 5: The TSNE visualization of other rows in Ta-
ble 3
bedding occurs. Especially in the last layer, these
embeddings cluster according to their concept. For
example, “2019”, “2018”, “Year”, “May” and “31”
belong to the concept of date. “2,082”, “2,025”,
“08”, “025”, “$” and “Expense” mean the money.
On the other hand, in shallow layers like layer 8,
low-level and ﬁne-granted knowledge is learned.
For instance, the NumBed “2.0”, “6.0”, and “10.0”
are close to each other but the word embedding “2”,
“6”, and “10” are kind of far away.
Figure 5 shows the TSNE visualization of an-
other two rows in Table 3. As shown in the left sub-
ﬁgure, the cluster is not obvious without LUNA.
In the right sub-ﬁgure, without intermediate pre-
training, the distance between “2,082” and “2,025”
is longer than row 0 , which means the model can-
not understand the numbers as well as row 0 .

