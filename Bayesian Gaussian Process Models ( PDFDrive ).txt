Bayesian Gaussian Process Models:
PAC-Bayesian Generalisation Error Bounds
and Sparse Approximations
Matthias Seeger
Doctor of Philosophy
Institute for Adaptive and Neural Computation
Division of Informatics
University of Edinburgh
2003

ii

Abstract
Non-parametric models and techniques enjoy a growing popularity in the ﬁeld of
machine learning, and among these Bayesian inference for Gaussian process (GP)
models has recently received signiﬁcant attention. We feel that GP priors should
be part of the standard toolbox for constructing models relevant to machine
learning in the same way as parametric linear models are, and the results in this
thesis help to remove some obstacles on the way towards this goal.
In the ﬁrst main chapter, we provide a distribution-free ﬁnite sample bound
on the diﬀerence between generalisation and empirical (training) error for GP
classiﬁcation methods. While the general theorem (the PAC-Bayesian bound)
is not new, we give a much simpliﬁed and somewhat generalised derivation and
point out the underlying core technique (convex duality) explicitly. Furthermore,
the application to GP models is novel (to our knowledge). A central feature of
this bound is that its quality depends crucially on task knowledge being encoded
faithfully in the model and prior distributions, so there is a mutual beneﬁt be-
tween a sharp theoretical guarantee and empirically well-established statistical
practices. Extensive simulations on real-world classiﬁcation tasks indicate an im-
pressive tightness of the bound, in spite of the fact that many previous bounds
for related kernel machines fail to give non-trivial guarantees in this practically
relevant regime.
In the second main chapter, sparse approximations are developed to address
the problem of the unfavourable scaling of most GP techniques with large training
sets. Due to its high importance in practice, this problem has received a lot of at-
tention recently. We demonstrate the tractability and usefulness of simple greedy
forward selection with information-theoretic criteria previously used in active
learning (or sequential design) and develop generic schemes for automatic model
selection with many (hyper)parameters. We suggest two new generic schemes and
evaluate some of their variants on large real-world classiﬁcation and regression
tasks. These schemes and their underlying principles (which are clearly stated
and analysed) can be applied to obtain sparse approximations for a wide regime
of GP models far beyond the special cases we studied here.
iii

Acknowledgements
During the course of my PhD studies I have been fortunate enough to beneﬁt
from inspiring interactions with people in the machine learning community, and
I am delighted to be able to acknowledge these here.
My thanks go ﬁrst of all to my supervisor Christopher Williams who guided
me through my research, and whose comments often sparked my deeper interest
in directions which I would have ignored otherwise, or forced me to re-consider
intuitive argumentations more carefully.
I have also beneﬁtted from his wide
overview of relevant literature in many areas of machine learning I have been
interested in.
I would like to thank the members of the ANC group in Edinburgh for many
inspiring discussions and interesting talks covering a wide range of topics. Espe-
cially, I would like to thank Amos Storkey and David Barber for having shared
their knowledge and pointing me to interesting work in areas beyond the neces-
sarily narrow scope of this thesis. I gratefully acknowledge support through a
research studentship from Microsoft Research Ltd.
The “random walk” of my postgraduate studies led me into terrain which
was quite unfamiliar to me, and it would have been a much harder and certainly
much more boring journey without some people I met on the way. Ralf Herbrich
shared some of his deep and pragmatic insights into data-dependent distribution-
free bounds and other areas of learning theory and gave many helpful comments,
and I very much enjoyed the time I spent working with him and Hugo Zaragoza
during an internship at MS Research, Cambridge in the autumn of 2000, not least
the late-night pub sessions in the Eagle. I am grateful for many discussions with
Neil Lawrence, Michael Tipping, Bernhard Sch¨olkopf and Antonio Criminisi who
were there at that time.
I was fortunate enough to work with John Langford, one of whose diverse
interests in learning theory are applications and reﬁnements of the PAC-Bayesian
theorem. I have learned a lot from his pragmatic approach to learning-theoretical
problems, and I would enjoy to do further joint work with him in the future. I am
very grateful to Manfred Opper for sharing some of his enormous knowledge about
iv

learning-theoretical analyses of Bayesian procedures. Manfred got interested in
my work, parts of which reminded him of studies of learning curves for Bayesian
methods he had done some time ago with David Haussler, and invited me to Aston
University, Birmingham for a few days. Our discussions there were invaluable in
that they helped me to abstract from the details and see the big picture behind
the PAC-Bayesian technique, recognising for the ﬁrst time the huge versatility of
convex inequalities. I would also like to thank David McAllester for proposing
and proving the remarkable PAC-Bayesian theorems in the ﬁrst place, and for
some very interesting discussions when we met at Windsor, UK and at NIPS.
My interest in sparse Gaussian process approximations was sparked by work
with Chris Williams (see Section 4.7.1) but also by my frustration with running
time and memory consumption for my experiments with the PAC-Bayesian theo-
rem. My goal was to provide a PAC result which is practically useful, but of course
this calls for a method which practitioners can really use on large datasets. I got
interested in the informative vector machine (IVM) presented by Neil Lawrence
and Ralf Herbrich in a NIPS workshop contribution and worked out some gener-
alisations and details such as the expectation propagation (EP) foundation and
the randomisation strategy, so that my implementation would be able to handle
datasets of the size required for the PAC-Bayesian experiments. I enjoyed this
collaboration (which led to more joint work with Neil Lawrence).
I would like to thank John Platt and Christopher Burges for giving me the
opportunity to spend the summer of 2002 doing research at Microsoft, Redmond.
I enjoyed the work with Chris and would have liked to spend more time with
John who, unfortunately for me but very fortunately for him, became father
at that time.
I would also like to thank Patrice Simard for some interesting
discussions and for many great football (“soccer”) matches (in the end “youth
and stamina” prevailed once more over “experience and wisdom”). Thanks also
to Alex Salcianu for joining me on some great hiking trips around Mt. Rainier,
ﬁshing me out of Lake Washington after some unexpected canoeing manoeuvre,
and for his persistence of getting the photos of our mind-boggling white water
rafting day. I hope we meet again for some hiking, although then I would prefer
v

to do the driving!
Finally, I would like to thank the friends I have made during my time in
this beautiful town of Edinburgh. During my ﬁrst year, Esben and I explored
many of the historical sites in this fascinating country, and he gave me a very
lively account of truths and myths in Scottish history. I had a great time with
Thomas and Bj¨orn who are now out to build artiﬁcial worms and to save the rain
forest. The amazing scenery and remoteness of the Scottish highlands sparked
my interest in hiking and mountain walking, and I would like to thank Karsten,
Bj¨orn, David, Steve and many others for great experiences and views in this vast
wilderness.
I am indebted to Emanuela for sharing these years with me, and
although we are going separate ways now I wish her all the best in the future.
My special thanks go to my mother and sisters who supported me through
these years, especially during the ﬁnal hardest one.
This thesis is written in
memory of my father.
vi

Declaration
I declare that this thesis was composed by myself, that the work contained herein
is my own except where explicitly stated otherwise in the text, and that this work
has not been submitted for any other degree or professional qualiﬁcation except
as speciﬁed.
(Matthias Seeger)
vii

viii

Table of Contents
1
Introduction
1
1.1
Declaration of Previous Work, Collaborations
. . . . . . . . . . .
6
1.1.1
Publications during Postgraduate Studies . . . . . . . . . .
7
2
Background
11
2.1
Bayesian Gaussian Processes . . . . . . . . . . . . . . . . . . . . .
11
2.1.1
Gaussian Processes: The Process and the Weight Space View 12
2.1.2
Some Gaussian Process Models
. . . . . . . . . . . . . . .
20
2.1.3
Approximate Inference and Learning . . . . . . . . . . . .
26
2.1.4
Reproducing Kernel Hilbert Spaces . . . . . . . . . . . . .
31
2.1.5
Penalised Likelihood. Spline Smoothing . . . . . . . . . . .
36
2.1.6
Maximum Entropy Discrimination. Large Margin Classiﬁers 40
2.1.7
Kriging
. . . . . . . . . . . . . . . . . . . . . . . . . . . .
45
2.1.8
Choice of Kernel. Kernel Design . . . . . . . . . . . . . . .
48
2.2
Learning Theory
. . . . . . . . . . . . . . . . . . . . . . . . . . .
66
2.2.1
Probably Approximately Correct
. . . . . . . . . . . . . .
67
2.2.2
Concentration Inequalities . . . . . . . . . . . . . . . . . .
71
2.2.3
Vapnik-Chervonenkis Theory
. . . . . . . . . . . . . . . .
73
2.2.4
Using PAC Bounds for Model Selection . . . . . . . . . . .
76
3
PAC-Bayesian Bounds for Gaussian Process Methods
79
3.1
Data-dependent PAC Bounds and PAC-Bayesian Theorems . . . .
80
3.1.1
The Need for Data-dependent Bounds
. . . . . . . . . . .
80
3.1.2
Bayesian Classiﬁers. PAC-Bayesian Theorems . . . . . . .
84
ix

3.2
The PAC-Bayesian Theorem for Gibbs Classiﬁers
. . . . . . . . .
85
3.2.1
The Binary Classiﬁcation Case
. . . . . . . . . . . . . . .
86
3.2.2
Confusion Distributions and Multiple Classes
. . . . . . .
88
3.2.3
Comments . . . . . . . . . . . . . . . . . . . . . . . . . . .
93
3.2.4
The Case of General Bounded Loss . . . . . . . . . . . . .
96
3.2.5
An Extension to the Bayes Classiﬁer
. . . . . . . . . . . .
97
3.2.6
Some Speculative Extensions . . . . . . . . . . . . . . . . .
99
3.3
Application to Gaussian Process Classiﬁcation . . . . . . . . . . . 102
3.3.1
PAC-Bayesian Theorem for GP Classiﬁcation
. . . . . . . 102
3.3.2
Laplace Gaussian Process Classiﬁcation . . . . . . . . . . . 104
3.3.3
Sparse Greedy Gaussian Process Classiﬁcation . . . . . . . 106
3.3.4
Minimum Relative Entropy Discrimination . . . . . . . . . 107
3.4
Related Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 108
3.4.1
The Theorem of Meir and Zhang
. . . . . . . . . . . . . . 109
3.5
Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 114
3.5.1
The Setup MNIST2/3
. . . . . . . . . . . . . . . . . . . . 114
3.5.2
Experiments with Laplace GPC . . . . . . . . . . . . . . . 115
3.5.3
Experiments with Sparse Greedy GPC . . . . . . . . . . . 117
3.5.4
Comparison with PAC Compression Bound . . . . . . . . . 119
3.5.5
Using the Bounds for Model Selection . . . . . . . . . . . . 123
3.5.6
Comparing Gibbs and Bayes Bounds . . . . . . . . . . . . 127
3.6
Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 129
4
Sparse Gaussian Process Methods
131
4.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 132
4.2
Likelihood Approximations and Greedy Selection Criteria . . . . . 133
4.2.1
Likelihood Approximations . . . . . . . . . . . . . . . . . . 134
4.2.2
Greedy Selection Criteria . . . . . . . . . . . . . . . . . . . 136
4.3
Expectation Propagation for Gaussian Process Models
. . . . . . 140
4.4
Sparse Gaussian Process Methods: Conditional Inference . . . . . 143
4.4.1
The Informative Vector Machine . . . . . . . . . . . . . . . 143
4.4.2
Projected Latent Variables . . . . . . . . . . . . . . . . . . 150
x

4.5
Model Selection . . . . . . . . . . . . . . . . . . . . . . . . . . . . 159
4.5.1
Model Selection for PLV . . . . . . . . . . . . . . . . . . . 159
4.5.2
Model Selection for IVM . . . . . . . . . . . . . . . . . . . 161
4.5.3
Details about Optimisation
. . . . . . . . . . . . . . . . . 161
4.6
Related Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 164
4.7
Addenda . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 171
4.7.1
Nystr¨om Approximations . . . . . . . . . . . . . . . . . . . 171
4.7.2
The Importance of Estimating Predictive Variances . . . . 174
4.8
Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 179
4.8.1
IVM Classiﬁcation: Digit Recognition . . . . . . . . . . . . 179
4.8.2
PLV Regression: Robot Arm Dynamics . . . . . . . . . . . 182
4.8.3
IVM Regression: Robot Arm Dynamics . . . . . . . . . . . 190
4.8.4
IVM Classiﬁcation: Digit Recognition II . . . . . . . . . . 193
4.9
Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 195
5
Conclusions and Future Work
197
5.1
PAC-Bayesian Bounds for Gaussian Process
Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 197
5.1.1
Suggestions for Future Work . . . . . . . . . . . . . . . . . 199
5.2
Sparse Gaussian Process Methods . . . . . . . . . . . . . . . . . . 200
5.2.1
Suggestions for Future Work . . . . . . . . . . . . . . . . . 202
A General Appendix
205
A.1 Notation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 205
A.1.1
Linear Algebra
. . . . . . . . . . . . . . . . . . . . . . . . 205
A.1.2
Probability. Miscellaneous . . . . . . . . . . . . . . . . . . 206
A.2 Linear Algebra. Useful Formulae
. . . . . . . . . . . . . . . . . . 208
A.2.1
Partitioned Matrix Inverses.
Woodbury Formula.
Schur
Complements . . . . . . . . . . . . . . . . . . . . . . . . . 208
A.2.2
Update of Cholesky Decomposition . . . . . . . . . . . . . 209
A.2.3
Some Useful Formulae
. . . . . . . . . . . . . . . . . . . . 211
A.3 Convex Functions . . . . . . . . . . . . . . . . . . . . . . . . . . . 211
xi

A.4 Exponential Families. Gaussians . . . . . . . . . . . . . . . . . . . 216
A.4.1
Exponential Families . . . . . . . . . . . . . . . . . . . . . 217
A.4.2
I-Projections
. . . . . . . . . . . . . . . . . . . . . . . . . 220
A.4.3
Gaussian Variables . . . . . . . . . . . . . . . . . . . . . . 221
A.5 Pattern Recognition
. . . . . . . . . . . . . . . . . . . . . . . . . 225
A.6 Bayesian Inference and Approximations . . . . . . . . . . . . . . . 226
A.6.1
Probabilistic Modelling . . . . . . . . . . . . . . . . . . . . 227
A.6.2
Bayesian Analysis . . . . . . . . . . . . . . . . . . . . . . . 228
A.6.3
Approximations to Bayesian Inference
. . . . . . . . . . . 229
A.6.4
Lower Bound Maximisation. Expectation Maximisation . . 232
A.7 Large Deviation Inequalities . . . . . . . . . . . . . . . . . . . . . 236
B Appendix for Chapter 3
239
B.1 Extended PAC-Bayesian Theorem: an Example
. . . . . . . . . . 239
B.2 Details of Proof of Theorem 3.2 . . . . . . . . . . . . . . . . . . . 241
B.3 Proof of Theorem 3.3 . . . . . . . . . . . . . . . . . . . . . . . . . 242
B.4 Eﬃcient Evaluation of the Laplace GP Gibbs Classiﬁer . . . . . . 244
B.5 Proof of Theorem 3.4 . . . . . . . . . . . . . . . . . . . . . . . . . 245
B.5.1
The Case of Regression . . . . . . . . . . . . . . . . . . . . 247
B.6 Proof of a PAC Compression Bound . . . . . . . . . . . . . . . . . 248
B.6.1
Examples of compression schemes . . . . . . . . . . . . . . 252
C Appendix for Chapter 4
255
C.1 Expectation Propagation . . . . . . . . . . . . . . . . . . . . . . . 255
C.1.1
Expectation Propagation for Exponential Families . . . . . 255
C.1.2
ADF Update for some Noise Models
. . . . . . . . . . . . 260
C.2 Likelihood Approximations. Selection Criteria . . . . . . . . . . . 262
C.2.1
Optimal Sparse Likelihood Approximations . . . . . . . . . 262
C.2.2
Relaxed Likelihood Approximations . . . . . . . . . . . . . 264
C.2.3
Cheap versus Expensive Selection Criteria . . . . . . . . . 264
C.3 Derivations for the Informative Vector Machine
. . . . . . . . . . 265
C.3.1
Update of the Representation . . . . . . . . . . . . . . . . 265
xii

C.3.2
Exchange Moves
. . . . . . . . . . . . . . . . . . . . . . . 268
C.3.3
Model Selection Criterion and Gradient . . . . . . . . . . . 270
C.4 Derivations for Projected Latent Variables . . . . . . . . . . . . . 273
C.4.1
Site Approximation Updates. Point Inclusions . . . . . . . 273
C.4.2
Information Gain Criterion . . . . . . . . . . . . . . . . . . 275
C.4.3
Extended Information Gain Criterion . . . . . . . . . . . . 277
C.4.4
Gradient of Model Selection Criterion . . . . . . . . . . . . 285
Bibliography
291
xiii

xiv

List of Figures
2.1
Sample paths Gaussian covariance function . . . . . . . . . . . . .
53
2.2
Sample paths Mat´ern covariance function . . . . . . . . . . . . . .
55
2.3
Sample paths exponential covariance function
. . . . . . . . . . .
56
2.4
Sample paths polynomial covariance function . . . . . . . . . . . .
57
3.1
Relation between margin and gap bound part
. . . . . . . . . . . 105
3.2
Contribution of pattern to Gibbs error and Bayes margin loss
. . 111
3.3
Upper bound values against expected test errors . . . . . . . . . . 123
3.4
Upper bound values against expected test errors (regression) . . . 124
3.5
Upper bound values, expected test errors and gap bound values
. 125
3.6
Expected training errors against expected test errors (regression) . 126
3.7
Expected test errors against upper bound values on MNIST8/9
. 128
4.1
IVM: Test error against rejection rate on MNIST, 9 vs. rest . . . . 177
4.2
IVM: Comparing diﬀerent criteria to rank predictions (MNIST, 9
vs. rest) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 178
4.3
IVM: Predictive std.dev. against predictive mean (MNIST, 9 vs.
rest) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 178
4.4
Test error against rejection rate for SVM / IVM (MNIST, 9 vs. rest)181
4.5
Learning curves for sparse regression methods (kin-40k) . . . . . . 185
4.6
Learning curves for sparse regression methods (pumadyn-32nm) . 187
4.7
Model selection criterion for full and sparse GP regression methods 188
4.8
Model selection runs for full and sparse GP regression methods . . 190
4.9
IVM: Test error against rejection rate (USPS, all classes) . . . . . 195
xv

A.1 Illustrations of convex duality . . . . . . . . . . . . . . . . . . . . 213
xvi

List of Tables
3.1
Experimental results for Laplace GPC
. . . . . . . . . . . . . . . 116
3.2
Experimental results for sparse GPC (IVM)
. . . . . . . . . . . . 118
3.3
Experimental results for compression bound (IVM)
. . . . . . . . 120
3.4
Experimental results for compression bound (SVM) . . . . . . . . 121
4.1
Test errors and training times IVM vs. SVM (MNIST, c vs. rest) . 180
4.2
IVM vs. SVM on MNIST tasks (full-size images) . . . . . . . . . . 183
4.3
Training times for sparse regression methods . . . . . . . . . . . . 186
4.4
Model selection for full and sparse GP regression methods
. . . . 189
4.5
IVM: Test error rates and model selection times (USPS, c vs. rest) 194
xvii

xviii

Chapter 1
Introduction
In order to solve real-world machine learning problems, we need a principled way
of representing uncertainty quantitatively and manipulating and updating this
representation in the light of observed data. Observed data is corrupted by noise
and subject to measurement error, and many aspects of the phenomenon gener-
ating the data are usually imperfectly known if at all. The concept of uncertainty
is also useful to justify the necessary abstraction of mapping a complex problem
to a tractable domain of variables, accounting for our ignorance of the details.
Uncertainty is most conveniently represented by a probability distribution
over the domain of variables of interest. A model induces a family of such global
distributions via structural (conditional independence) constraints and speciﬁca-
tions of local distribution families of simple well-understood form.1 If a model is
speciﬁed completely (i.e. could be used to generate data just like the true source),
Bayes’ formula can be used to compute marginal query probabilities conditioned
on the observed data. We can distinguish between parametric and non-parametric
models. In the former, the data generation mechanism is completely speciﬁed by
a ﬁnite set of parameters and learning from data amounts to reducing our initial
uncertainty about these, i.e. “ﬁtting” the model to the observations by adjusting
the parameters.
In this thesis, we are mainly concerned with non-parametric
methods whose setup is quite diﬀerent. An important special case which serves
1Note that the domain often includes variables which cannot be observed (latent variables),
because these lead to decompositions and therefore simpliﬁcation of the model description.
1

2
Chapter 1. Introduction
well to illustrate the general idea is that of “smoothing” data by proposing an
underlying curve which optimally solves a trade-oﬀbetween ﬁtting the data and
minimising the “complexity” (quantiﬁed for example in terms of average bending
energy). Non-parametric models can be constructed using random ﬁelds which (in
contrast to the parametric situation) are not typically determined by a ﬁnite num-
ber of parameters. Nevertheless, if we are only interested in a ﬁnite-dimensional
projection of such a ﬁeld, inference becomes a ﬁnite problem and is either analyti-
cally tractable or amenable to common approximation techniques. Typically, this
dimensionality is of the order of the training set size, in marked contrast to the
parametric situation where the parameter dimensionality is usually independent
of the training data.
This thesis contributes in two quite diﬀerent aspects to the ﬁeld of non-
parametric modelling (more accurately to Gaussian process models introduced
in Section 2.1). These are brieﬂy motivated in the following subsections.
PAC-Bayesian Bounds for Gaussian Process Methods
(Chapter 3)
In Chapter 3, we present a generalised and slightly improved version of McAllester’s
PAC Bayesian distribution-free generalisation error bound and provide a much
simpliﬁed proof pointing out the basic concept clearly. We apply this result to
Gaussian process classiﬁcation methods.
Consider the binary classiﬁcation (or pattern recognition) problem where data
S = {(xi, yi) | i = 1, . . . , n} is sampled independent and identically distributed
(i.i.d.) from an unknown joint data distribution over input points x ∈X and
targets y ∈{−1, +1}. A good classiﬁcation rule rS : X →{−1, +1} aims to
minimise the generalisation error gen(S) = Pr{rS(x∗) ̸= y∗} if (x∗, y∗) is sampled
from the data distribution independently of S. In statistical learning theory, the
aim is to ﬁnd distribution-free upper bounds bound(S, δ) on gen(S) which are
computable (i.e. independent of the data distribution which is unknown), such
that the probability (over random draws of S) of a bound violation is ≤δ. The
important point about such a statement is that it holds no matter what the data

3
distribution is. Typically, bound(S, δ) converges to the empirical error
emp(S) = 1
n
n
X
i=1
I{rS(
 i)̸=yi}
as n →∞, so that the bound statement implies convergence in probability of
emp(S) to gen(S) and a bound on the rate of convergence. If this is fast enough,
one obtains almost sure convergence, thus the consistency of the method. Com-
paring rate bounds for diﬀerent methods or for the same method under diﬀerent
conﬁgurations can help to make design choices or even drive model selection.
In critical high-security applications, the existence of a useful distribution-free
bound can be reassuring. In this thesis, we present generalisations and simpliﬁ-
cations of a powerful general data-dependent technique to prove distribution-free
upper bounds for Bayesian-type classiﬁcation rules, and we apply this technique
to non-parametric Bayesian Gaussian process classiﬁcation methods. The main
concept we require is an inequality coming from a notion of duality in convex
analysis. Compared with techniques which have been used in the past to prove
similar distribution-free bounds, our proof methods are surprisingly light-weight
and eﬃcient. As opposed to many “uniform” results, our bounds strongly depend
on prior knowledge encoded into the procedure: predictors learned from samples
S which are very unlikely under the prior assumptions are judged more complex
and suﬀer a larger penalty in the bound than predictors from samples conforming
to our assumptions.
Sparse Gaussian Process Methods (Chapter 4)
In Chapter 4, we present sparse approximation schemes for Gaussian process
methods which often show performance similar to standard GP inference approx-
imations, but have vastly reduced time and memory requirements.
Recall that although practical non-parametric methods have ﬁnite-dimensional
uncertainty representations, these usually scale with the training set size n =
|S|, leading to memory and training time scaling superlinear (typically at least
quadratic) in n and prediction time scaling of at least O(n). Yet if there is to be
any hope of useful generalisation from a source, data must exhibit redundancies as

4
Chapter 1. Introduction
n grows large, and in the second part of this thesis we describe a general way of de-
tecting such redundancies and using them in “sparse” variants of non-parametric
methods to achieve a much more favourable scaling of time and memory require-
ments. The concepts we use are a generic framework for approximate inference
in Gaussian process (GP) models combined with greedy forward selection of “in-
formative” cases from S via information-theoretic criteria from “active learning”
(sequential design). Our contributions here might appeal more to the practitioner
than the theoretician. We present a clear and simple discussion of the concepts
underlying sparse approximations. The concrete algorithms are modiﬁcations of
a non-sparse generic approximate inference technique which are motivated by the
analysis. They span the most interesting cases arising in practice, and the prim-
itives that can be specialised to obtain new variants are pointed out clearly. We
place special emphasis on numerically stable algorithms with a favourable trade-
oﬀbetween memory and running time requirements whose dominating operations
can be vectorised to make use of highly eﬃcient numerical linear algebra software.
Most importantly, our framework is complete in that it includes automatic em-
pirical Bayesian model selection. Its tractability and eﬃciency is demonstrated
on a signiﬁcant number of large tasks.
Background (Chapter 2)
Apart from scientiﬁcally novel contributions, we have included a range of tuto-
rial material which we hope the reader will ﬁnd a further useful contribution.
The thesis is entirely self-contained along its main branches. Chapter 2 contains
background material, notably an extensive discussion (Section 2.1) of notions of
Gaussian processes relevant for many machine learning techniques. Section 2.2
gives a brief description of some key concepts of learning theory where we have
tried to emphasise intuitive ideas rather than technical details.

5
Software
A large amount of C++ software has been developed in order to run the exper-
iments presented in this thesis (among others). Strictly speaking, this is not a
complete contribution at this time since the package has not been released into
the public domain yet.2 The initial reason to implement this system was my frus-
tration with the widely used Matlab prototyping system. The main drawbacks
of Matlab, apart from its high pricing3, are its poor memory management and its
incredible slowness for iterative code. The latter can be circumvented by imple-
menting C++ plug-ins, but for the former it seems impossible to do any sort of
non-trivial operations on matrices without producing two or three copies. Also,
in order to circumvent slow iterative code, artiﬁcial matrices of simple (low-rank)
structure have to be created temporarily to be able to use vectorised operations.
Finally, although Matlab contains a host of powerful functions, none of them are
easily extensible. The system does not allow for proper object-oriented develop-
ment which makes life so much easier w.r.t. extensibility.
Our system contains an extensive matrix and vector library which allows sim-
ple manipulations of parts or permutations of matrices without the need to make
copies. A module for generating pseudo-random variates from all commonly used
distributions is available. Several powerful gradient-based nonlinear optimisers
are included which allow direct control of the optimisation process (non-standard
scenarios such as the one required in Section 4.5.3 are easily accommodated). A
module for dataset import and export from diﬀerent formats is designed to allow
for virtual “views” on large datasets which are accessed via intermediate mem-
ory buﬀers,4 and virtual datasets can be represented as indices into these views.
Generic implementations of frequently used experimental setups (for example,
2This will hopefully happen in the near future, if I can ﬁgure out a good way of marrying the
code with Matlab. Even as stand-alone, many small things have to be tidied up. Some of the
code unfortunately uses the Numerical Recipes distribution [145] which is proprietary. Another
important point is the re-design of the matrix library to conform to the Fortran storage model,
so that optimised numerical software can be linked in.
3For example, essential basic features are packaged in “toolboxes” which have to be bought
separately. The basic installation cannot even be used to sample Gamma variables or do basic
data analysis.
4These features are not fully implemented.

6
Chapter 1. Introduction
model selection by cross-validation) can work with arbitrary “learning methods”,
produce “prediction value vectors” of arbitrary format which can then be evalu-
ated against a test dataset using arbitrary “loss functions”. We use our own data
ﬁle formats for which Matlab functions for read and write access are available,
so results can be visualised in Matlab. We have implemented a rudimentary in-
terface which allows our system to be controlled interactively from Matlab. This
interface is object-oriented and works by ﬁle-based transfer and ﬁle semaphores,
running our system as a server process.
1.1
Declaration of Previous Work, Collaborations
Parts of the material presented in this thesis has been done in collaboration with
other researchers and has been previously published (a list of publications related
to this thesis is given below).
Most material in Chapter 3 appeared previously in [170] and the accompany-
ing report [171]. I became interested in the PAC-Bayesian theorem through work
with John Langford and Nimrod Megiddo [172]. I am grateful to Manfred Opper
for inviting me to a seminar at Aston University, Birmingham. The discussions
I had with Manfred led to great simpliﬁcations in the argumentation, and the
accessibility of this chapter owes a lot to these. I would also like to thank John
Shawe-Taylor for organising the NeuroCOLT workshop on “bounds below 1/2”
in Windsor and inviting me, and to David McAllester, John Langford, Manfred
Opper and others for inspiring discussions at the “Royal” venue. Also thanks to
Ralf Herbrich for many discussions about learning theory and other issues during
my time at MS Research, Cambridge.
Material in Chapter 4 appeared previously in [98] and [165], although a sig-
niﬁcant part (especially about model selection and PLV) is novel. The published
work has been done in collaboration with Neil Lawrence and Ralf Herbrich for the
IVM5 (based on the presentation [97]) and with Christopher Williams and Neil
Lawrence for the PLV regression scheme. Chris Williams sparked my interest for
5In particular, I do not bear any responsibility for the name “informative vector machine”
(or would have chosen a longer and more boring one).

1.1. Declaration of Previous Work, Collaborations
7
sparse approximations with our work [211, 208] on Nystr¨om approximations. As
discussed in detail in the chapter, both our schemes are closely related to a body
of work done by Lehel Csat´o and Manfred Opper.
1.1.1
Publications during Postgraduate Studies
In this section, I list work published during my postgraduate studies (in chrono-
logical order). Some of the work is not described in this thesis, and in these cases
I have added short abstracts here.
• Matthias Seeger, Christopher Williams, Neil Lawrence
Fast Forward Selection to Speed Up Sparse Gaussian Process Regression
Workshop on AI and Statistics 9 (2003). [165]
• Neil Lawrence, Matthias Seeger, Ralf Herbrich
Fast Sparse Gaussian Process Methods: The Informative Vector Machine
Neural Information Processing Systems 15 (2003). [98]
• Matthias Seeger
PAC-Bayesian Generalization Error Bounds for Gaussian Process
Classiﬁcation
Journal of Machine Learning Research 3 (2002), 233–269. [170]
• Matthias Seeger
Covariance Kernels from Bayesian Generative Models
Neural Information Processing Systems 14 (2002), 905–912. [164]
We propose mutual information kernels as general framework of learning
covariance functions for GP models from labelled and unlabelled task data
(related to the Fisher kernel and Haussler’s HMRF kernels discussed in
Section 2.1.8). We show how such kernels can be evaluated approximately
by variational techniques and present experiments with variational Bayesian
mixtures of factor analysers.
• Matthias Seeger, John Langford, Nimrod Megiddo
An Improved Predictive Accuracy Bound for Averaging Classiﬁers

8
Chapter 1. Introduction
International Conference on Machine Learning 18 (2001), 290–297. [172]
Combines McAllester’s PAC Bayesian theorem with the approximation tech-
nique for mixture discriminants suggested in [158] to obtain a PAC Bayesian
bound for Bayes-type classiﬁers.
• Christopher Williams, Matthias Seeger
Using the Nystr¨om Method to Speed Up Kernel Machines
Neural Information Processing Systems 13 (2001), 682–688. [211]
• Christopher Williams, Matthias Seeger
The Eﬀect of the Input Density Distribution on Kernel-based Classiﬁers
International Conference on Machine Learning 17 (2000), 1159–1166. [208]
• Matthias Seeger
Learning with Labelled and Unlabelled Data
Technical report, University of Edinburgh (2001).
See www.dai.ed.ac.uk/∼seeger/.
Provides a rigorous formal deﬁnition of the problem of learning from a
mixture of labelled and unlabelled data, a literature review fairly complete
at that time and the development of the framework of input-dependent
regularisation.
• Matthias Seeger
Input-dependent Regularization of Conditional Density Models
Technical report, University of Edinburgh (2001).
See www.dai.ed.ac.uk/∼seeger/.
Proposes a general statistical framework for learning from labelled and un-
labelled data together with some ideas for algorithms based on standard
latent variable techniques similar to EM.
• Matthias Seeger
Annealed Expectation-Maximization by Entropy Projection
Technical report, University of Edinburgh (2000).
See www.dai.ed.ac.uk/∼seeger/.

1.1. Declaration of Previous Work, Collaborations
9
Complements the frequently used technique of deterministic annealing by
M-step annealing which can be used to learn assignment structure in gen-
eral “resources-components” models. An application to Gaussian mixture
modelling with tied covariance matrix parameters is given.

10
Chapter 1. Introduction

Chapter 2
Background
The statistical models we are concerned with in this thesis are non-parametric.
By employing Gaussian processes (GPs) as prior distributions over latent ﬁelds,
these models and inference techniques for them can be understood from a con-
ceptually simple Bayesian viewpoint. In Section 2.1 we give an introduction to
GPs and their role in non-parametric regression models. Chapter 3 of this the-
sis is concerned with learning-theoretical ﬁnite sample size analyses of Bayesian
GP methods, and concepts and methodology required there is introduced in Sec-
tion 2.2. We hope that both sections will be useful for readers not familiar with
these subjects to grasp the basic concepts. Details are largely omitted, but ref-
erences are provided.
2.1
Bayesian Gaussian Processes
Gaussian processes (GPs) are natural generalisations of multivariate Gaussian
random variables to inﬁnite (countably or continuous) index sets. GPs have been
applied in a large number of ﬁelds to a diverse range of ends, and very many deep
theoretical analyses of various properties are available. On the other hand, in this
thesis we only require quite elementary properties and none of the deep theory
behind GPs. Although our exposition in this section remains selective and on a
fairly elementary and non-rigorous level, it contains much material beyond what is
required to follow the remainder of this thesis. Readers from the machine learning
11

12
Chapter 2. Background
community might ﬁnd our introduction valuable since it tries to connect diﬀerent
branches in which similar ideas have been investigated and phrases concepts in
the familiar vocabulary of statistical modelling. On the other hand, the section
can safely be skipped on the ﬁrst reading and visited later for references.
Other introductions to GP models in the machine learning context are given
in [111, 132, 209, 146].
2.1.1
Gaussian Processes: The Process and the Weight Space
View
Gaussian process (GP) models are constructed from classical statistical models by
replacing latent functions of parametric form by random processes with Gaussian
prior. In this section, we will introduce GPs and highlight some aspects which
are relevant to this thesis. We develop two simple views on GPs, pointing out
similarities and key diﬀerences to distributions induced by parametric models. We
follow [2], Chap. 1,2. A good introduction into the concepts required to study GP
prediction is given in [189], Chap. 2. For concepts and vocabulary from general
probability theory, we refer to [67, 16, 29]. The section can be skipped by readers
not interested in details or familiar with GPs, maybe with a quick glance of what
we mean by process and weight space view.
Let {Xn} be a sequence of complex-valued random variables, and recall that
Xn →X (n →∞) in quadratic mean (or in mean square (m.s.)) if E[|Xn −
X|2] →0. M.s. convergence is weaker than almost sure (a.s.) convergence, but
turns out to be the most useful mode for discussing GP aspects we require here.
In general, X and Y are m.s. equivalent if E[|X −Y |2] = 0. Let X be an non-
empty index set. For the main parts of this thesis, X can be arbitrary, but here
we assume that X is at least a group1 (and sometimes we assume it to be Rd). In
a nutshell, a random process X →C is a collection of random variables (one for
each x ∈X ) over a common probability space. The measure-theoretic deﬁnition
is awkward, but basically the same as for a single variable. A process can be
deﬁned via its ﬁnite-dimensional distributions (f.d.d.’s) which it induces on each
1Has an addition +, an origin 0 and a negation −.

2.1. Bayesian Gaussian Processes
13
ﬁnite subset of X . Kolmogorov [90] proved that for a speciﬁcation of all f.d.d.’s
there exists a random process (in the measure-theoretic sense) iﬀthe f.d.d.’s are
symmetric and consistent. Namely, for every n ∈N>0, x1, . . . , xn ∈X , Borel
sets B1, . . . , Bn and every permutation π of {1, . . . , n} we must have
µ
 π(1),...,
 π(n)(Bπ(1) × · · · × Bπ(n)) = µ
 1,...,
 n(B1 × · · · × Bn)
and
µ
 1,...,
 n(B1 × · · · × Bn−1 × C) = µ
 1,...,
 n−1(B1 × · · · × Bn−1).
The question about uniqueness of random processes is tricky, because two pro-
cesses can be equivalent (u(x) = v(x) almost surely for every x; equivalent
processes are called versions of each other), yet diﬀer signiﬁcantly w.r.t. almost
sure properties of their sample paths. We do not require the study of sample
path properties in this thesis, but see [2] for more information.
The ﬁrst and second-order statistics of u(x) are its mean function m(x) =
E[u(x)] and covariance function2
K(x, x′) = E
h
(u(x) −m(x))(u(x) −m(x))
i
.
The latter is central to study characteristics of the process in the mean-square
sense.
It is a positive semideﬁnite3 form in the sense that for every n ∈N,
x1, . . . , xn ∈X , z1, . . . , zn:
n
X
i,j=1
zizjK(xi, xj) ≥0.
(2.1)
This is clear because for X = P
i zi(u(xi) −m(xi)) we have E[|X|2] ≥0. Note
that this implies that K(x, x) ≥0 and K(x′, x) = K(x, x′). K is called positive
deﬁnite if (2.1) holds with > whenever z ̸= 0. The positive semideﬁniteness of K
leads to an important spectral decomposition which is discussed in Section 2.1.4.
A positive semideﬁnite K will also be referred to as kernel, pointing out its role
as kernel for a linear operator (see Section 2.1.4).
2Here, a + ib = a −ib, a, b ∈R denotes the complex conjugate.
3This term is not uniquely used in the literature, it is sometimes replaced by non-negative
deﬁnite or even positive deﬁnite (which has a diﬀerent meaning here).
A diﬀerent weaker
property is conditional positive semideﬁniteness (see [200], Chap. 2) which is important but not
discussed here.

14
Chapter 2. Background
2.1.1.1
Stationary Processes
In many situations, the behaviour of the process does not depend on the location
of the observer, and under this restriction a rich theory can be developed, linking
local m.s. properties of the process to the behaviour of K close to the origin.
A process is called strictly homogeneous (or strictly stationary) if its f.d.d.’s are
invariant under simultaneous translation of their variables.
This implies that
m(x) is constant and K(x, x′) is a function of x −x′ (we write K(x, x′) =
K(x −x′) in this case; K(x) is sometimes called autocovariance function). A
process fulﬁlling the latter two conditions is called (weakly) homogeneous (or
(weakly) stationary). For a stationary process, the choice of the origin is not
reﬂected in the statistics up to second order. If K(0) > 0,
ρ(x) = K(x)
K(0)
is called correlation function. A stationary process has a spectral representation
as a stochastic Fourier integral (e.g., [2], Chap. 2; [67], Chap. 9; [214]), based
on Bochner’s theorem which (for X = Rd) asserts that ρ(x) is positive semidef-
inite, furthermore uniformly continuous with ρ(0) = 1, |ρ(x)| ≤1 iﬀit is the
characteristic function of a variable ω, i.e.
ρ(x) =
Z
ei
 T
  dF(ω)
(2.2)
for a probability distribution function F(ω). If F(ω) has a density f(ω) (w.r.t.
Lebesgue measure), f is called spectral density.4 This theorem allows to prove
positive semideﬁniteness of K by computing its Fourier transform and checking
that it is non-negative. If so, it must be the spectral density. Note that if ρ(x) is
real (thus an odd function), then the spectral distribution is symmetric around
0, and if f(ω) exists it is odd as well.
The f.d.d.’s of a process determine its mean-square properties, while this is
not true in general for almost sure properties. Even stronger, for a zero-mean pro-
cess, m.s. properties are usually determined entirely by the covariance function
4If ρ(0) > 0, the spectral distribution is a measure of total mass ρ(0).

2.1. Bayesian Gaussian Processes
15
K(x, x′). For stationary processes, it is merely the behaviour of K(x) at the ori-
gin which counts: the m.s. derivative5 D
  u(x) exists everywhere iﬀD
  D
  K(x)
exists at x = 0. For example, a process with the RBF (Gaussian) covariance
function K (2.29) is m.s. analytic, because K is analytic (diﬀerentiable up to any
order) at 0.
2.1.1.2
Isotropic Processes
A stationary process is called isotropic if its covariance function K(x) depends
on ∥x∥only. In this case, the spectral distribution F is invariant under isotropic
isomorphisms (e.g., rotations).
Loosely speaking, second-order characteristics
of an isotropic process are the same from whatever position and direction they
are observed. It is much simpler to characterise isotropic correlation functions
than stationary ones in general. Let ρ(τ) = ρ(x) for τ = ∥x∥. The spectral
decomposition (2.2) simpliﬁes to
ρ(τ) =
Z
Λd/2−1(τ ω)dF(ω)
(2.3)
where F(ω) =
R
I{∥
 ∥≤ω} dF(ω) is a distribution function and Λν is an expression
involving a Bessel function of the ﬁrst kind (see [2], Sect. 2.5). Alternatively, if
the spectral density f(ω) exists and f(ω) = f(ω) for ω = ∥ω∥, then dF(ω) =
Ad−1ωd−1f(ω) dω,6 so we can easily convert to the spectral representation in terms
of f(ω). Denote the set of ρ(τ) corresponding to isotropic correlation functions
in Rd by Dd. Note that (2.3) characterises Dd (by Bochner’s theorem). Also, it
is clear that Dd+1 ⊂Dd.7 Let D∞= T
d≥1 Dd. Since
Λd/2−1(x) →e−x2, d →∞,
one can show that ρ(τ) ∈D∞iﬀρ(τ) =
R
exp(−τ 2ω2) dF(ω). Note that the
assumption of isotropy puts strong constraints on the correlation function, espe-
cially for large d. For example, ρ(τ) ≥infx Λd/2−1(x) ≥−1/n so large negative
5Here, Dx denotes a diﬀerential functional, such as ∂2/(∂x1∂x2).
6Ad−1 is the surface area of the unit sphere in Rd.
7Beware that both F(ω) and f(ω) depend on the dimension d for which ρ(τ) is used to
induce a correlation function.

16
Chapter 2. Background
correlations are ruled out. If ρ(τ) ∈D∞, it must be non-negative. Furthermore,
for large d ρ(τ) is smooth on (0, ∞) while it may have a jump at 0 (additive white
noise). Also, ρ(τ) converges for τ →∞so we can restrict our attention to ρ(τ)
which vanish for large τ. If ρ(τ) ∈Dd and B ∈Rd,d is nonsingular, then
ρ
  (x) = ρ(∥Bx∥)
is a correlation function as well, called anisotropic. Examples of (an)isotropic
covariance functions are given in Section 2.1.8.
2.1.1.3
Two Views on Gaussian Processes
A Gaussian process (GP) is a process whose f.d.d.’s are Gaussian. Since a Gaus-
sian is determined by its ﬁrst and second-order cumulants and these involve
pairwise interactions only, its f.d.d.’s are completely determined by mean and
covariance function.
GPs are by far the most accessible and well-understood
processes (on uncountable index sets). It is clear that for every positive semidef-
inite function K there exists a zero-mean GP with K as covariance function, so
GPs as modelling tool are very ﬂexible. In conjunction with latent variable mod-
elling techniques, a wide variety of non-parametric models can be constructed
(see Section 2.1.2). The fact that all f.d.d.’s are Gaussian with covariance ma-
trices induced by K(x, x′) can be used to obtain approximations to Bayesian
inference fairly straightforwardly (see Section 2.1.3). It is interesting to note that
derivatives D
  u(x) of a GP are GPs again (if they exist), and
E
h
D(1)
 u(x)D(2)
 ′ u(x′)
i
= D(1)
 D(2)
 ′ K(x, x′),
thus derivative observations can be incorporated into a model in the same way
as function value observations (for applications, see [136, 184]). Characteristics
such as m.s. diﬀerentiability up to a given order can be controlled via the co-
variance function, an example is given in Section 2.1.8. For example, one of the
most thoroughly studied GPs is the Wiener process (or Brownian motion, or con-
tinuous random walk) with covariance function K(x, x′) = σ2 min{x, x′} (here,
X = R≥0; for multivariate generalisations to Brownian sheets, see [2], Chap. 8).

2.1. Bayesian Gaussian Processes
17
u(x) is m.s. continuous everywhere, but not m.s. diﬀerentiable anywhere. In fact,
a version of the Wiener process can be constructed which has continuous sample
paths, but for every version sample paths are nowhere diﬀerentiable with prob-
ability 1. The Wiener process can be used to construct other GPs by means of
stochastic integrals (e.g., [67], Chap. 13).
We now develop two elementary views on Gaussian processes, the process and
the weight space view. While the former is usually much simpler to work with
(and will be used almost exclusively in this thesis), the latter allows us to relate
GP models with parametric linear models rather directly. We follow [209].8 The
process view on a zero-mean GP u(x) with covariance function K is in the spirit
of the GP deﬁnition given above. u(x) is deﬁned implicitly, in that for any ﬁnite
subset X ⊂X it induces a f.d.d. N(0, K(X)) over the vector u = u(X) of process
values at the points X. Here, K(X) = K(X, X) = (K(xi, xj))i,j where X =
{x1, . . . , xn}. Kolmogorov’s theorem guarantees the existence of a GP with this
family of f.d.d.’s.9 In practice, many modelling problems involving an unknown
functional relationship u(x) can be formulated such that only ever a ﬁnite number
of linear characteristics of u(x) (e.g., evaluations or derivatives of u(x)) are linked
to observations or predictive queries, and in such cases the process view boils
down to dealing with the “projection” of the GP onto a multivariate Gaussian
distribution, thus to simple linear algebra of quadratic forms.
GPs can also be seen from a weight space viewpoint, relating them to the linear
model. In the Bayesian context this view was ﬁrst suggested by O’Hagan [135] as
a “localized regression model” (the weight space is ﬁnite-dimensional there) while
the generalisation to arbitrary GP priors developed there uses the process view.
This paper is among the ﬁrst to address GP regression in a rigorous Bayesian
context, while the equivalence between spline smoothing and Bayesian estimation
of processes was noticed earlier by Kimeldorf and Wahba [89] (see Section 2.1.5).
8We use the term “process view” instead of “function space view” employed in [209]. The re-
lationship between GPs and associated spaces of smooth functions is a bit subtle and introduced
only below in Section 2.1.4.
9If K is continuous everywhere, a version exists with continuous sample paths, but we do
not require this here.

18
Chapter 2. Background
Recall the linear model
y = Φ(x)Tβ + ε,
(2.4)
where Φ(x) is a feature map from the covariate x and ε is independent Gaus-
sian noise. Every GP whose covariance function satisﬁes weak constraints can
be written as (2.4), albeit with possibly inﬁnite-dimensional weight space. To
develop this view, we use some details which are discussed in detail below in Sec-
tion 2.1.4. Under mild conditions on the covariance function K(x, x′) of u(x),
we can construct a sequence
k
X
ν=1
βνλ1/2
ν φν(x),
which converges to u(x) in quadratic mean (k →∞).10 Here, βν are i.i.d. N(0, 1)
variables. φν are orthonormal eigenfunctions of an operator induced by K with
corresponding eigenvalues λ1 ≥λ2 ≥· · · ≥0, P
ν≥1 λ2
ν < ∞, in a sense made
precise in Section 2.1.4.
Thus, if β = (βν)ν and Φ(x) = (λ1/2
ν φν(x))ν, then
u(x) = Φ(x)Tβ in quadratic mean, and Φ(x)TΦ(x′) = K(x, x′). This is the
weight space view on GPs and allows to view a non-parametric regression model
y = u(x) + ε
as direct inﬁnite-dimensional generalisation of the linear model (2.4) with spher-
ical Gaussian prior on β. We say that Φ(x) maps into a feature space which is
typically (countably) inﬁnite-dimensional. It is important to note that in this
construction of the feature map Φ(x) the individual components λ1/2
ν φν(x) do
not have the same scaling, in the sense that their norm in L2(µ) (the Hilbert
space they are drawn from and that K operates on) is λ1/2
ν
→0 (ν →∞). They
are comparable in a diﬀerent (RKHS) norm which scales with the “roughness”
of a function.
Intuitively, as ν →∞, the graph of φν becomes rougher and
increasingly complicated, see Section 2.1.4 for details.
For all inference purposes which are concerned with f.d.d.’s of u(x) and its
derivatives (or other linear functionals) only, the process and the weight space
10We only need pointwise m.s. convergence, although much stronger statements are possible
under mild assumptions, e.g. [2], Sect. 3.3.

2.1. Bayesian Gaussian Processes
19
view are equivalent: they lead to identical results. However, we feel that often
the process view is much simpler to work with, avoiding spurious inﬁnities11 and
relying on familiar Gaussian manipulations only. On the other hand, the weight
space view is more frequently used at least in the machine learning literature,
and its peculiarities may be a reason behind the perception that GP models are
diﬃcult to interpret. There is also the danger that false intuitions or conclusions
are developed from interpolating geometrical arguments from low-dimensional
Euclidean space to the feature space. We should also note that a weight space
representation of a GP in terms of a feature map Φ is not unique. The route
via eigenfunctions of the covariance operator is only one way to establish such.12
About the only invariant is that we always have Φ(x)TΦ(x′) = K(x, x′).
2.1.1.4
Gaussian Processes as Limit Priors of Parametric Models
We conclude this section by mentioning that one of the prime reasons for focusing
current machine learning interest on GP models was a highly original diﬀerent
way of establishing a weight space view proposed in [131]. Consider a model
f(x) =
H
X
j=1
vjh(x; u(j))
which could be a multi-layer perceptron (MLP) with hidden layer functions h,
weights u(j) and output layer weights v. Suppose that u(j) have independent
identical priors s.t. the resulting h(x; u(j)) are bounded almost surely over a
compact region of interest.
Also, vj ∼N(0, ω2/H) independently.
Then, for
H →∞, f(x) converges in quadratic mean to a zero-mean GP with covariance
function ω2E
  [h(x; u)h(x′; u)].
Stronger conditions would assure almost sure
convergence uniformly over a compact region. The bottom line is that if we take
a conventional parametric model which linearly combines the outputs of a large
number of feature detectors, and if we scale the outputs s.t. each of them in
11Which seem to drop out almost “magically” in the end from the weight space viewpoint,
while inﬁnities do not occur in the process view.
12For example, in Section 2.1.4 we discuss K’s role as reproducing kernel, in the sense that
K(x, x′) = (K(·, x), K(·, x′))K in some Hilbert space with inner product (·, ·)K. We could
deﬁne Φ to map x 7→K(·, x) and use the Hilbert space as weight space.

20
Chapter 2. Background
isolation has only a negligible contribution to the response, we might just as well
use the corresponding Gaussian process model. Neal [131] also shows that if a
non-zero number of the non-Gaussian feature outputs have a signiﬁcant impact
on the response with non-zero probability, then the limit process is typically not
Gaussian.
To conclude, the weight space view seems to relate non-parametric GP mod-
els with parametric linear models fairly directly. However, there are important
diﬀerences in general. Neal showed that GPs are obtained as limit distributions
of large linear combinations of features if each feature’s contribution becomes
negligible, while the output distributions of architectures which ﬁt at least a few
strong feature detectors are typically not Gaussian. Predictions from a GP model
are smoothed versions of the data (in a sense made concrete in Section 2.1.5),
i.e. interpolate by minimising general smoothness constraints encoded in the GP
prior, as opposed to parametric models which predict by focusing on these func-
tions (within the family) which are most consistent with the data. O’Hagan [135]
discusses diﬀerences w.r.t. optimal design.
2.1.2
Some Gaussian Process Models
The simplest Gaussian process model is
y = u + ε,
where u = u(x) is a priori a zero-mean Gaussian process with covariance func-
tion K and ε is independent N(0, σ2) noise.
Inference for this model is sim-
ple and analytically tractable, because the observation process y(x) is zero-
mean Gaussian with covariance K(x, x′) + σ2δ
  ,
 ′.13
Given some i.i.d. data
S = {(xi, yi) | i = 1, . . . , n}, let K = (K(xi, xj))i,j. Then, P(u) = N(0, K) and
P(u|S) = N
 K(σ2I + K)−1y, σ2(σ2I + K)−1K

,
(2.5)
13In the context of this model, it is interesting to note that if K′ is stationary and continuous
everywhere except at 0, it is the sum of a continuous (stationary) covariance K and a white
noise covariance ∝δx,x′. Furthermore, Sch¨onberg conjectured that if K′ is an isotropic bounded
covariance function, it must be continuous except possibly at 0.

2.1. Bayesian Gaussian Processes
21
where u = (u(xi))i. For some test point x∗distinct from the training points,
u∗= u(x∗) ⊥y | u, so that
P(u∗|x∗, S) =
Z
P(u∗|u)P(u|S) du
= N
 u∗|k(x∗)T(σ2I + K)−1y, K(x∗, x∗) −k(x∗)T(σ2I + K)−1k(x∗)

.
Here, k(x∗) = (K(xi, x∗))i, and we used the tower properties (Lemma A.3) and
Gaussian formulae (Section A.4.3). We see that for this model, the posterior pre-
dictive process u(x) given S is Gaussian with mean function yT(σ2I +K)−1k(x)
and covariance function
K(x, x′) −k(x)T(σ2I + K)−1k(x′).
In practice, if only posterior mean predictions are required, the prediction vec-
tor ξ = (σ2I + K)−1y can be computed using a linear conjugate gradients
solver which runs in O(n2) if the eigenvalue spectrum of K (which has to be
stored explicitly) shows a fast decay. If predictive variances for many test points
are required, the Cholesky decomposition (Deﬁnition A.2) σ2I + K = LLT
should be computed, after which each variance computation requires a single
back-substitution.
The pointwise predictive variance is never larger than the corresponding prior
variance, but the shrinkage decreases with increasing noise level σ2. The same
result can be derived in the weight space view with u(x) = Φ(x)Tβ, applying the
standard derivation of Bayesian linear regression (e.g., [209]). Note that just as in
parametric linear regression, the smoothed prediction E[y|S] is a linear function
of the observations y, as is the mean function of the predictive process E[u(x)|S]
(see also Section 2.1.7). Note also that if K(x, x′) →0 as ∥x −x′∥gets big,
predictive mean and variance for points x far from all data tend to prior mean
0 and prior variance K(x, x). Second-level inference problems such as selecting
values for hyperparameters (parameters of K and σ2) or integrating them out are
not analytically tractable and approximations have to be applied. Approximate
model selection is discussed in Section 2.1.3.
We can generalise this model by allowing for an arbitrary “noise distribution”
P(y|u), retaining the GP prior on u(x). The generative view is to sample the

22
Chapter 2. Background
process u(·) from the prior, then yi ∼P(yi|u(xi)) independent from each other
given u(·).14 The likelihood function factors as a product of univariate terms:
P(y|X, u(·)) = P(y|u) =
n
Y
i=1
P(yi|ui).
(2.6)
Since the likelihood depends on u(·) only via the ﬁnite set u, the predictive
posterior process can be written as
dP(u(·)|S) = P(u|S)
P(u) dP(u(·)),
(2.7)
thus the prior measure is “shifted” by multiplication with P(u|S)/P(u) depend-
ing on the process values u only (recall our notation from Section A.1). The
predictive process is not Gaussian in general, but its mean and covariance func-
tion can be obtained from knowledge of the posterior mean and covariance matrix
of P(u|S) as discussed in Section 2.1.3. For a test point x∗,
P(y∗|x∗, S) = E [P(y∗|u∗)]
where the expectation is over the predictive distribution of u∗.
In this gen-
eral model, ﬁrst-level inference is not analytically tractable.
In Section 2.1.3
a general approximate inference framework is discussed. MCMC methods (see
Section A.6.3) can be applied fairly straightforwardly, for example by Gibbs sam-
pling from the latent variables u [132]. Such methods are attractive because the
marginalisation over hyperparameters can be dealt with in the same framework.
However, naive realisations may have a prohibitive running time due to the large
number of correlated latent variables, and more advanced techniques can be diﬃ-
cult to handle in practice. While MCMC is maybe the most advanced and widely
used class of approximate inference techniques, it is not discussed in any further
detail in this thesis.
2.1.2.1
Generalised Linear Models. Binary Classiﬁcation
A large class of models of this kind is obtained by starting from generalised linear
models (GLMs) [133, 118] and replacing the parametric linear function xTβ by
14This is generalised easily to allow for bounded linear functionals of the latent process u(·)
instead of the evaluation functional δxi, as discussed in Section 2.1.4.

2.1. Bayesian Gaussian Processes
23
a process u(x) with a GP prior. This can be seen as direct inﬁnite-dimensional
generalisation of GLMs by employing the weight space view (see Section 2.1.1).
In the spline smoothing context, this framework is presented in detail in [66]. It
employs noise distributions
P(y|u) = exp
 φ−1(y u −b(u)) + c(y, φ)

,
i.e. P(y|u) is in an exponential family with natural parameter u, suﬃcient statis-
tics y/φ and log partition function φ−1b(u) (see Section A.4.1). Here, φ > 0 is a
scale hyperparameter. The linear model is a special case with φ = σ2, u = µ =
Eu[y] and b(u) = (1/2)u2. A technically attractive feature of this framework is
that log P(y|u) is strictly concave in u, leading to a strictly concave, unimodal
posterior P(u|S). For binary classiﬁcation and y ∈{−1, +1}, the GLM for the
binomial noise distribution is logistic regression with the logit noise
P(y|u) = σ(y (u + b)), σ(t) =
1
1 + e−t.
(2.8)
Another frequently used binary classiﬁcation noise model is probit noise
P(y|u) = Φ(y (u + b)) = Eτ∼N(0,1)

I{y(u+b)+τ>0}

(2.9)
which can be seen as noisy Heaviside step. Both noise models are strictly log-
concave.
2.1.2.2
Models with C Latent Processes
We can also allow for a ﬁxed number C ≥1 of latent variables for each case
(x, y), i.e. C processes uc(x). The likelihood factors as
n
Y
i=1
P(yi|u(i)),
u(i) = (uc(xi))c.
uc(x) is zero-mean Gaussian a priori with covariance function K(c). While it
is theoretically possible to use cross-covariance functions for prior covariances
between uc for diﬀerent c, it may be hard to come up with a suitable class of such
functions. Furthermore, the assumption that the processes uc are independent

24
Chapter 2. Background
a priori leads to large computational savings, since the joint covariance matrix
over the data assumes block-diagonal structure. Note that in this structure, we
separate w.r.t. diﬀerent c, while in block-diagonal structures coming from the
factorised likelihood we separate w.r.t. cases i.
An important example using C latent processes is C-class classiﬁcation. The
likelihood comes from a multinomial GLM (or multiple logistic regression). It
is convenient to use a binary encoding for the class labels, i.e. y = δc for class
c ∈{1, . . . , C}.15 The noise is multinomial with
µ = E[y | u] = softmax(u) =
 1T exp(u)
−1 exp(u).
u 7→µ is sometimes called softmax mapping. Note that this mapping is not
invertible, since we can add α1 to u for any α without changing µ. In terms of
Section A.4.1, the parameterisation of the multinomial by u is overcomplete, due
to the linear constraint yT1 = 1 on y, and the corresponding GLM log partition
function
b(u) = log 1T exp(u)
is not strictly convex. The usual remedy is to constrain u by for example ﬁxing
uC = 0. This is ﬁne in the context of ﬁtting parameters by ML, but may be
problematic for Bayesian inference. As mentioned above, we typically use priors
which are i.i.d. over the uc, so if we ﬁx uC = 0, the induced prior on µ is not an
exchangeable distribution (i.e. component permutations of u can have diﬀerent
distributions) and µC is singled out for no other than technical reasons.
We
think it is preferable in the Bayesian context to retain symmetry and accept
that u 7→µ is not 1-to-1. Dealing with this non-identiﬁability during inference
approximations is not too hard since softmax is invertible on any plane orthogonal
to 1 and b(u) is strictly convex on such. Anyway, this detail together with the
two diﬀerent blocking structures mentioned above renders implementations of
approximate inference for the C-class model somewhat more involved than the
binary case.
Furthermore, it turns out that the straightforward extension of
inference approximations used in this thesis (e.g., Sections 4.3 and 4.4) grows by
15We use vector notation for u, y ∈RC associated with a single case. This should not be
confused with the vector notation u, y ∈Rn used above to group variables for all cases.

2.1. Bayesian Gaussian Processes
25
a factor of C2 in running time requirements compared to the binary case. In
contrast, the GP Laplace approximation (see Section 2.1.3) scales linearly in C
only. It is possible that by using further approximations, such a desirable scaling
can be attained for the former methods as well, but this is not pursued in this
thesis. Other examples for C-process models are ordinal regression (“ranking”)
models (see [118] for likelihood suggestions) or multivariate regression.
Note that it is straightforward to ﬁt a C-class model to uncertain class ob-
servations. For example, if the target y is a distribution over 1, . . . , C, the corre-
sponding log likelihood factor can be chosen as
log P(y|u) = −D[y ∥µ],
µ = softmax(u),
which reduces to normal case for y = δc. Note that this is concave in u, and
strictly concave on each plane orthogonal to 1.
This model is also useful for
problems in which points x can belong to multiple classes (multi-label problems).
2.1.2.3
Robust Regression
GP regression with Gaussian noise can lead to poor results if the data is prone to
outliers, due to the light tails of the noise distribution. A robust GP regression
model can be obtained by using a heavy-tailed noise distribution P(y|u) such as
a Laplace or even Student-t distribution. An interesting idea is to use the fact
that the latter is obtained by starting with N(0, τ −1) and to integrate out the
precision τ over a Gamma distribution (e.g., [131]). Thus, a robust model can be
written as
y = u + ε,
ε ∼N(0, τ −1),
where τ is drawn i.i.d. from a Gamma distribution (whose parameters are hyper-
parameters). The posterior P(u|S, τ ) conditioned on the precision values τi is
Gaussian and is computed in the same way as for the case τi = σ−2 above. τ can
be sampled by MCMC, or may be chosen to maximise the posterior P(τ |S). The
marginal likelihood P(y|τ ) is Gaussian and can be computed easily. However,
note that in the latter case the number of hyperparameters grows as n which

26
Chapter 2. Background
might invalidate the usual justiﬁcation of marginal likelihood maximisation (see
Section A.6.2).
2.1.3
Approximate Inference and Learning
We have seen in the previous section that the posterior process for a likelihood of
the general form (2.6) can be written as “shifted” version (2.7) of the prior. About
the only processes (in this context) which can be dealt with feasibly are Gaussian
ones, and a general way of obtaining a GP approximation to the posterior process
is to approximate P(u|S) by a Gaussian Q(u|S), leading to the process
dQ(u(·)) = Q(u|S)
P(u) dP(u(·))
(2.10)
which is Gaussian. An optimal way of choosing Q would be to minimise
D[P(u(·)|S) ∥Q(u(·))] = D[P(u|S) ∥Q(u|S)].
(2.11)
The equality follows from the fact that if dP(u(·)|S) ≪dQ(u(·)|S), then
dP(u(·)|S) = P(u|S)
Q(u|S) dQ(u(·)|S),
and otherwise D[P(u|S) ∥Q(u|S)] = ∞(recall our notation from Section A.1).
At the minimum point (unique w.r.t. f.d.d.’s of Q) Q and P have the same
mean and covariance function. This is equivalent to moment matching (see Sec-
tion A.4.2) and requires us to ﬁnd mean and covariance matrix of P(u|S). Unfor-
tunately, this is intractable in general for large datasets and non-Gaussian noise.
Any other Gaussian approximation Q(u|S) leads to a GP posterior approxima-
tion Q(u(·)), and the intractable (2.11) can nevertheless be valuable as guideline,
as for example in the EP algorithm (see Section C.1.1). In this thesis, we are pri-
marily interested in approximate inference methods for GP models which employ
GP approximations (2.10) to posterior processes via
Q(u|S) = N(u | Kξ, A).
(2.12)
Here, ξ, A can depend on the data S, the covariance function K (often via the
kernel matrix K) and on other hyperparameters. This class contains a variety of

2.1. Bayesian Gaussian Processes
27
methods proposed in the literature, some of them are brieﬂy discussed below (see
also Section 4.3). Virtually all of these have a reduced O(n) parameterisation,
since A has the restricted form
A =
 K−1 + I·,IDII,·
−1
(2.13)
with D ∈Rd,d diagonal with positive entries and I ⊂{1, . . . , n}, |I| = d (recall
our notation I·,I from Section A.1). For the methods mentioned below in this
section, d = n and I·,I = I, but for the sparse approximations in Chapter 4
we have d ≪n. In the latter case, ξ\I = 0 and we use ξ ∈Rd for simplicity,
replacing ξ in (2.12) by I·,Iξ.
From (2.10), the (approximate) predictive posterior distribution of u∗= u(x∗)
at a test point x∗is determined easily as Q(u∗|x∗, S) = N(u∗|µ(x∗), σ2(x∗)),
where
µ(x∗) = kI(x∗)Tξ,
σ2(x∗) = K(x∗, x∗) −kI(x∗)TD1/2B−1D1/2kI(x∗),
B = I + D1/2KID1/2.
(2.14)
Here, kI(x∗) = (K(xi, x∗))i∈I. More general, the GP posterior approximation
has mean function µ(x) and covariance function
K(x, x′) −kI(x)TD1/2B−1D1/2kI(x′).
The predictive distribution P(y∗|x∗, S) is obtained by averaging P(y∗|u∗) over
N(u∗|µ(x∗), σ2(x∗)).
If this expectation is not analytically tractable, it can
be done by Gaussian quadrature (see Section C.1.2) if P(y∗|u∗) is smooth and
does not grow faster than polynomial.
A simple and numerically stable way
to determine the predictive variances is to compute the Cholesky decomposi-
tion B = LLT (Deﬁnition A.2) after which each variance requires one back-
substitution with L. We will refer to ξ as prediction vector. More generally, as
mentioned in Section 2.1.1, we can use derivative information or other bounded
linear functionals of the latent process u(x) in the likelihood and/or for the vari-
ables to be predicted, using the fact that the corresponding ﬁnite set of scalar

28
Chapter 2. Background
variables is multivariate Gaussian with prior covariance matrix derived from the
covariance function K (as discussed in more detail in Section 2.1.4).
A generalisation to the multi-process models of Section 2.1.2 is also straight-
forward in principle. Here, u has dimension C n. Again A is restricted to the
form (2.13), although D is merely block-diagonal with n (d × d) blocks on the
diagonal. Moreover, if the processes are a priori independent, both K and K−1
consist of C (n × n) blocks on the diagonal. The general formulae for prediction
(2.14) have to be modiﬁed for eﬃciency. The details are slightly involved and
may depend on the concrete approximation method, C-process models are not
discussed in further detail in this thesis.
2.1.3.1
Some Examples
A simple and eﬃcient way of obtaining a Gaussian approximation Q(u|S) is via
Laplace’s method (see Section A.6.3), as proposed in [210] for binary classiﬁcation
with logit noise (2.8). To this end, we have to ﬁnd the posterior mode ˆu which
can be done by a variant of Newton-Raphson (or Fisher scoring, see [118]). Each
iteration consists of a weighted regression problem, i.e. requires the solution of an
n × n positive deﬁnite linear system. This can be done approximately in O(n2)
using a conjugate gradients solver. At the mode, we have
ξ = Y σ(−Y ˆu),
D = (diag σ(−Y ˆu))(diag σ(Y ˆu)),
(2.15)
where σ is the logistic function (2.8) and Y = diag y. All n diagonal elements of
D are positive. Recall that the Laplace approximation replaces the log posterior
by a quadratic ﬁtted to the local curvature at the mode ˆu. For the logit noise
the log posterior is strictly concave and dominated by the Gaussian prior far out,
so in general a Gaussian approximation should be fairly accurate. On the other
hand, the true posterior can be signiﬁcantly skewed, meaning that the mode can
be quite distant from the mean (which would be optimal) and the covariance
approximation via local curvature around the mode can be poor.
The expectation propagation (EP) algorithm for GP models is described in
Section 4.3 and employed for sparse approximations in Chapter 4. It can signif-

2.1. Bayesian Gaussian Processes
29
icantly outperform the Laplace GP approximation in terms of prediction accu-
racy (e.g., [139]), but is also more costly to train.16 It is also somewhat harder
to ensure numerical stability. On the other hand, EP is more general and can
for example deal with discontinuous or non-diﬀerentiable log likelihoods. Our
description of EP applies to C-process models just as well, with the unfortu-
nate C2 scaling and the slight technical diﬃculty of having to approximate C-
dimensional Gaussian expectations (see Section C.1.2 for some references). A
range of diﬀerent variational approximations (see Section A.6.3) have been sug-
gested in [63, 168, 82]. Note that for the variational method where Q(u|S) is
chosen to minimise D[· ∥P(u|S)], it is easy to see that the best Gaussian varia-
tional distribution has a covariance matrix of the form (2.13).
All approximations mentioned so far have training time scaling of O(n3) which
is prohibitive for large datasets.
Sparse inference approximations reduce this
scaling to O(n d2) with controllable d ≪n and are the topic of Chapter 4. In
contrast to this, at least for fairly low-dimensional input spaces X (e.g., R, R2
or the 2-dimensional surface of a sphere) spline smoothing methods are very
attractive for computational reasons. These methods are discussed in some detail
in Section 2.1.5. In a nutshell, they enforce smoothness by penalising curvature or
other derivatives of low order. For the corresponding spline kernel as covariance
function of a GP prior, predictive posterior GPs are spline functions (i.e. piecewise
polynomial), and the linear systems during training can be solved in O(n) due to
band-structured system matrices.
2.1.3.2
Model Selection
So far we have only been concerned with ﬁrst-level inference conditioned on ﬁxed
hyperparameters. A useful general method has to provide some means to select
good values for these parameters or to marginalise over them (see Section 2.1.2).
In this thesis, we will focus on marginal likelihood maximisation as general model
selection technique (see Section A.6.2). If we denote the hyperparameters by α,
16Partly due to its more complex iterative structure, but also because its elementary steps
are smaller than for the Laplace technique and cannot be vectorised as eﬃciently.

30
Chapter 2. Background
the log marginal likelihood log P(y|α) is as diﬃcult to compute as the posterior
P(u|S, α) and has to be approximated in general.17 We use the variational lower
bound described in Section A.6.4, thus employ lower bound maximisation for
model selection. Namely, in our case the lower bound (A.19) becomes
log P(y|α) ≥EQ [log P(y|u, α) + log P(u|α)] + H[Q(u|S)]
= EQ [log P(y|u, α)] −D[Q(u|S) ∥P(u|α)].
(2.16)
(the diﬀerential entropy H[·] and relative entropy D[·] are deﬁned in Section A.3).
Note that the posterior approximation Q(u|S) depends on α as well, but it is
not feasible in general to obtain its exact gradient w.r.t. α. Variational EM, an
important special case of lower bound maximisation is iterative, in turn freezing
one of Q, α and maximising the lower bound w.r.t. the other (here, Q can be
chosen from a family of variational distributions). Alternatively, Q can be chosen
in a diﬀerent way as approximation of the posterior P(u|S). The deviation from
the variational choice of Q can be criticised on the ground that other choices
of Q can lead to decreases in the lower bound, so the overall algorithm does
not increase its criterion strictly monotonically. On the other hand, Q chosen
in a diﬀerent way may lie outside families over which the lower bound can be
maximised eﬃciently, thus may even result in a larger value than the family
maximiser.18 Furthermore, the lower bound criterion can be motivated by the
fact that its gradient
EQ(
  |S) [∇
  log P(y, u|α)]
(ignoring the dependence of Q on α) approximates the true gradient
∇
  log P(y|α) = EP (
  |S) [∇
  log P(y, u|α)]
at every point α (see Section A.6.4). In the context of approximate GP inference
methods, the dependence of Q(u|S) on the GP prior (thus on α) is quite explicit
(for example, the covariance of Q is (K−1 + D)−1 which depends strongly on the
17It is analytically tractable for a Gaussian likelihood, for example in the case of GP regression
with Gaussian noise discussed above it is log N(y|0, K + σ2I).
18For example, even though the bound maximiser over all Gaussians has a covariance matrix
of the form (2.13), ﬁnding it is prohibitively costly in practice and proposed variational schemes
[63, 168, 82] use restricted subfamilies.

2.1. Bayesian Gaussian Processes
31
kernel matrix K). We argue that instead of keeping all of Q ﬁxed for the gradient
computation, we should merely ignore the dependence of the essential parameters
ξ, D on α. This typically leads to a more involved gradient computation which
is potentially closer to the true gradient. Alternatively, if this computation is
beyond resource limits, further indirect dependencies on α may be ignored. We
refer to Section C.3.3 for a concrete example and to Section 4.5.3 for details about
the optimisation problem which is slightly non-standard due to the lack of strict
monotonicity.
2.1.4
Reproducing Kernel Hilbert Spaces
The theory of reproducing kernel Hilbert spaces (RKHS) can be used to charac-
terise the space of random variables obtained as bounded linear functionals of a
GP on which any method of prediction from ﬁnite information must be based.
Apart from that, RKHS provide a uniﬁcation of ideas from a wide area of math-
ematics, most of which will not be mentioned here. The interested reader may
consult [6]. Our exposition is taken from [200]. None of the theory discussed here
is used for the essential arguments in this thesis, and the section can be skipped
by readers not interested in the details.
A reproducing kernel Hilbert space (RKHS) H is a Hilbert space of functions
X →R for which all evaluation functionals δ
 are bounded. This implies that
there exists a kernel K(x, x′) s.t. K(·, x) ∈H for all x ∈X and
f(x) = δ
  f = (K(·, x), f)
(2.17)
for all f ∈H, where (·, ·) is the inner product in H. To be speciﬁc, a Hilbert
space is a vector space with an inner product which is complete, in the sense
that each Cauchy sequence converges to an element of the space. For example,
a Hilbert space H can be generated from an inner product space of functions
X →R by adjoining the limits of all Cauchy sequences to H. Note that this is a
rather abstract operation and the adjoined objects need not be functions in the
usual sense: for example L2(µ), the Hilbert space of functions for which
Z
f(x)2 dµ(x) < ∞
(2.18)

32
Chapter 2. Background
contains the Dirac delta “functions” δ
 (representers of the evaluation function-
als) which are not pointwise deﬁned. For an RKHS H such anomalies cannot
occur, since the functionals δ
 are bounded:19
|f(x)| = |δ
  f| ≤C
  ∥f∥.
By the Riesz representation theorem, there exists a unique representer K
 ∈H
such that (2.17) holds with K(·, x) = K
  . It is easy to see that the kernel K is
positive semideﬁnite. K is called reproducing kernel (RK) of H, note that
 K
  , K
 ′
= (K(·, x), K(·, x′)) = K(x, x′).
It is important to note that in a RKHS, (norm) convergence implies pointwise
convergence to a pointwise deﬁned function, since
|fm(x) −f(x)| = |(K
  , fm −f)| ≤C
  ∥fm −f∥.
On the other hand, for any positive semideﬁnite K there exists a unique RKHS
H with RK K. Namely, the set of ﬁnite linear combinations of K(·, xi), xi ∈X
with
 X
i
aiK(·, xi),
X
j
bjK(·, x′
j)
!
=
X
i,j
aibjK(xi, x′
j)
is an inner product space which is extended to a Hilbert space H by adjoining
all limits of Cauchy sequences. Since norm convergence implies pointwise con-
vergence in the inner product space, all adjoined limits are pointwise deﬁned
functions and H is an RKHS with RK K. To conclude, a RKHS has properties
which make it much “nicer” to work with than a general Hilbert space. All func-
tions are pointwise deﬁned, and the representer of the evaluation functional δ
 is
explicitly given by K(·, x).
2.1.4.1
RKHS by Mercer Eigendecomposition. Karhunen-Loeve Expansion
We have already seen that L2(µ) is not a RKHS in general, but for many kernels
K it contains a (unique) RKHS as subspace.
Recall that L2(µ) contains all
19Bounded functionals are sometimes called continuous.

2.1. Bayesian Gaussian Processes
33
functions f : X →R for which (2.18) holds. The standard inner product is
(f, g) =
Z
f(x)g(x) dµ(x).
Often, µ is taken as indicator function of a compact set such as the unit cube.
A positive semideﬁnite K(x, x′) can be regarded as kernel (or representer) of a
positive semideﬁnite linear operator K in the sense
(Kf)(x) = (K(·, x), f).
φ is an eigenfunction of K with eigenvalue λ ̸= 0 if
(Kφ)(x) = (K(·, x), φ) = λ φ(x)
for all x. For K, all eigenvalues are real and non-negative. Furthermore, suppose
K is continuous and
Z
K(x, x′)2 dµ(x)dµ(x′) < ∞.
Then, by the Mercer-Hilbert-Schmidt theorems there exists a countable20 or-
thonormal sequence of continuous eigenfunctions φν ∈L2(µ) with eigenvalues
λ1 ≥λ2 ≥· · · ≥0, and K can be expanded in terms of these:
K(x, x′) =
X
ν≥1
λνφν(x)φν(x′),
(2.19)
and P
ν≥1 λ2
ν < ∞, thus λν →0(ν →∞). This can be seen as generalisation of
the eigendecomposition of a positive semideﬁnite Hermitian matrix. Indeed, the
reproducing property of positive semideﬁnite kernels was recognised and used by
E. H. Moore [126] to develop the notion of general “positive Hermitian matrices”.
In this case, we can characterise the RKHS embedded in L2(µ) explicitly. For
f ∈L2(µ), deﬁne the Fourier coeﬃcients
fν = (f, φν).
20Such operators (kernels) are called trace-class, and in general only such will have a discrete
spectrum (countable number of eigenvalues).

34
Chapter 2. Background
Consider the subspace HK of all f ∈L2(µ) with P
ν≥1 λ−1
ν f 2
ν < ∞. Then, HK is
a Hilbert space with inner product
(f, g)K =
X
ν≥1
fνgν
λν
,
moreover the Fourier series P
ν≥1 fνφν converges pointwise to f. Since {λνφν(x)}
are the Fourier coeﬃcients of K(·, x) (using Equation 2.19), we have
(f, K(·, x))K =
X
ν≥1
fνφν(x) = f(x),
thus K is the RK of HK. It is important to distinguish clearly between the inner
products (·, ·) in L2(µ) and (·, ·)K in HK (see [216] for more details about the
relationship of these inner products). While ∥· ∥measures “expected squared
distance” from 0 (w.r.t. dµ), ∥· ∥K is a measure of the “roughness” of a function.
For example, the eigenfunctions have ∥φν∥= 1, but ∥φν∥K = λ−1/2
ν
thus becoming
increasingly rough.21
The spectral decomposition of K leads to an important representation of a
zero-mean GP u(x) with covariance function K: the Karhunen-Loeve expansion.
Namely, the sequence
uk(x) =
k
X
ν=1
uνφν(x),
(2.20)
where uν are independent N(0, λν) variables, converges to u(x) in quadratic mean
(a stronger statement under additional conditions can be found in [2]). Moreover,
uν =
Z
u(x)φν(x) dµ(x)
which is well deﬁned in quadratic mean. We have already used this expansion in
Section 2.1.1 to introduce the “weight space view”. Note that since the variances
λν decay to 0, the GP can be approximated by ﬁnite partial sums of the expansion
(see [216]).
21In the same sense as high-frequency components in the usual Fourier transform.

2.1. Bayesian Gaussian Processes
35
2.1.4.2
Duality between RKHS and Gaussian Process
If u(x) is a zero-mean GP with covariance function K, what is the exact rela-
tionship between u(x) and the RKHS with RK K? One might think that u(x)
can be seen as distribution over HK, but this is wrong (as pointed out in [200],
Sect. 1.1). In fact, for any version of u(x) sample functions from the process are
not in HK with probability 1! This can be seen by noting that for the partial
sums (2.20) we have
E

∥uk∥2
K

= E
" k
X
ν=1
u2
ν
λν
#
= k →∞(k →∞).
Roughly speaking, HK contains “smooth”, non-erratic functions from L2(µ),
characteristics we cannot expect from sample paths of a random process.
A
better intuition about HK is that it will turn out to contain expected values
of u(x) conditioned on a ﬁnite amount of information. An important duality
between HK and a Hilbert space based on u(x) was noticed in [89]. Namely,
construct a Hilbert space HGP in the same way as above starting from positive
semideﬁnite K, but replace K(·, xi) by u(xi) and use the inner product
(A, B)GP = E[AB],
thus
 X
i
aiu(xi),
X
j
bju(x′
j)
!
GP
=
X
i,j
aibjK(xi, x′
j).
HGP is a space of random variables, not functions, but it is isometrically isomor-
phic to HK under the mapping u(xi) 7→K(·, xi), with
(u(x), u(x′))GP = E[u(x)u(x′)] = K(x, x′) = (K(·, x), K(·, x′))K.
For most purposes, we can regard HGP as RKHS with RK K. If L is a bounded
linear functional on HK, it has a representer ν ∈HK with ν(x) = LK
  . The
isometry maps ν to a random variable Z ∈HGP which we formally denote by
Lu(·). Note that
E [(Lu(·))u(x)] = (ν, K
  )K = ν(x) = LK
  .

36
Chapter 2. Background
More general, if L(1), L(2) are functionals with representers ν(1), ν(2) s.t. x 7→
L(j)K
 are in HK, then
E

(L(1)u(·))(L(2)u(·))

= (ν(1), ν(2))K = L(1)
 (K(·, x), ν(2))K = L(1)
 L(2)
 K(x, y).
Again, it is clear that Lu(·) is (in general) very diﬀerent from the process obtained
by applying L to sample paths of u(x). In fact, since the latter are almost surely
not in HK, L does not even apply to them in general. The correct interpretation
is in quadratic mean, using the isometry between HGP and HK. As an example,
suppose that X = Rd and L = D
 is a diﬀerential functional evaluated at x.
Then, we retrieve the observations in Section 2.1.1 about derivatives of a GP.
The space HGP is of central importance for the sort of inference on GP models
we are interested in, because it contains exactly the random variables we condition
on or would like to predict in situations where only a ﬁnite amount of information
is available (from observations which are linear functionals of the process).
2.1.5
Penalised Likelihood. Spline Smoothing
Historically, GP models originated from spline smoothing techniques via penalised
likelihood estimation, and spline kernels are widely used due to the favourable
approximation properties of splines and algorithmic advantages. A comprehensive
account of spline smoothing and relations to Bayesian estimation in GP models is
[200] which our exposition is mainly based on. Spline smoothing is a special case
of penalised likelihood methods, giving another view on the reproducing kernel
via the Green’s function of a penalisation (or regularisation) operator which will
be introduced below. This section is included mainly for illustrative purposes.
In Section 2.1.4 we have discussed the duality between a Gaussian process and
the RKHS of its covariance function. Apart from the Bayesian viewpoint using
GP models, a diﬀerent and more direct approach to estimation in non-parametric
models is the penalised likelihood approach, the oldest and most widely used in-
carnations of are spline smoothing methods. We will introduce the basic ideas
for the one-dimensional model which leads to the general notion of regularisation
operators, penalty functionals and their connections to RKHS. We omit all de-

2.1. Bayesian Gaussian Processes
37
tails, (important) computational issues and multidimensional generalisations, see
[200] for details. A more elementary account is [66].
We will only sketch the ideas, for rigorous details see [200, 89]. Interpolation
and smoothing by splines originates from the work of Sch¨onberg [163]. A natural
spline s(x) of order m on [0, 1] is deﬁned based on knots 0 < x1 < · · · < xn < 1.
If πk denotes the set of polynomials of order ≤k, then s(x) ∈π2m−1 on [xi, xi+1],
s(x) ∈πm−1 on [0, x1] and on [xn, 1], and s ∈C2m−2 overall. Natural cubic splines
are obtained for m = 2. Deﬁne the roughness penalty
Jm(f) =
Z 1
0
 f (m)(x)
2 dx.
Jm(f) penalises large derivatives of order m by a large value, for example J2
is large for functions of large curvature. Then, for some ﬁxed function values
the interpolant minimising Jm(f) over all f for which the latter is deﬁned22 is a
spline of order m. If we consider the related smoothing problem of minimising
the penalised empirical risk
n
X
i=1
(yi −f(xi))2 + αJm(f),
f ∈Wm[0, 1],
(2.21)
it is clear that the minimiser is again a natural spline s(x) of order m (any other
f ∈Wm[0, 1] can be replaced by the spline with the same values at the knots,
this does not change the risk term and cannot increase Jm). Now, from Taylor’s
theorem:
f(x) =
m−1
X
ν=0
xν
ν! f (ν)(0) +
Z 1
0
Gm(t, u)f (m)(t) dt
with Gm(t, u) = (t −u)m−1
+
/(m −1)! (here, x+ = xI{x≥0}). If f (ν)(0) = 0, ν =
0, . . . , m−1 then (Gm(t, ·), Dmf) = f(t), thus Gm(t, u) is the Green’s function for
the boundary value problem Dmf = g. These functions f form a Hilbert space
with inner product
(f, g)K =
Z 1
0
f (m)(x)g(m)(x) dx
22More precisely, over all f ∈Wm[0, 1], a so-called Sobolev space of all f ∈Cm−1[0, 1] s.t.
f (m−1) is absolutely continuous on [0, 1].

38
Chapter 2. Background
which is a RKHS with RK
K(s, t) =
Z 1
0
Gm(s, u)Gm(t, u) du.
(2.22)
The boundary values can be satisﬁed by taking the direct sum of the space with
πm−1: the norm ∥· ∥K is only a seminorm on this space because ∥p∥K = 0 for
p ∈πm−1. In general (see [200, 144]), we make use of the duality between a
RKHS and a regularisation (pseudodiﬀerential) operator P on L2(µ). Let H be
the Hilbert space of f s.t. Pf ∈L2(µ). For P, consider the pseudodiﬀerential
operator23 P∗P. If this has a null space (such as πm−1 in the example above), we
restrict H to the orthogonal complement. Now, the operator is positive deﬁnite
and has an inverse (its Green’s function) whose kernel K is the RK of H. The
inner product is
(f, g)K = (Pf, Pg)
and the penalty functional is simply the squared RKHS norm. If G(t, u) is s.t.
(G(t, ·), Pf) = f(t) for all f ∈H, the RK is given by
K(s, t) = (G(s, ·), G(t, ·)).
On the other hand, we can start from an RKHS with RK K without null space
and derive the corresponding regularisation operator P. This can give additional
insight into the meaning of a covariance function (see [144, 182]). In fact, if K
is stationary and continuous, we can use Bochner’s theorem (2.2). Namely, if
f(ω) is the spectral density of K, we can take f(ω)−1/2 as spectrum of P.24
The one-dimensional example above is readily generalised to splines on the unit
sphere or to thin plate splines in X = Rd, but the details get quite involved (see
[200], Chap. 2).
Kimeldorf and Wahba [89] generalised this setup to a general variational prob-
lem in an RKHS, allowing for general bounded linear functionals Lif instead of
f(xi) in (2.21). The minimiser is determined by n + M coeﬃcients, where M
is the dimension of the null space of the RK K (M = m + 1 in the spline case
23P∗is the adjoint of P, i.e. (f, Pg) = (P∗f, g).
24P is not uniquely deﬁned, but only P∗P (which has spectrum f(ω)−1).

2.1. Bayesian Gaussian Processes
39
above). These can be computed by direct formulae given in [200], Sect. 1.3. In
the more general penalised likelihood approach [200, 66], n function values or lin-
ear functionals of f are used as latent variables of a likelihood (see Section 2.1.2),
to obtain for example non-parametric extensions of GLMs [66]. The penalised
likelihood is obtained by adding the penalty functional to the likelihood, and just
as above the minimiser is determined by n + M coeﬃcients only (this representer
theorem can be proved using the same argument as in the spline case above). In
general, iterative methods are required to ﬁnd values for these coeﬃcients.
2.1.5.1
Bayesian View on Spline Smoothing
We close this section by reviewing the equivalence between spline smoothing
and Bayesian estimation for a GP model pointed out by Kimeldorf and Wahba
[89]. Given a positive semideﬁnite kernel K with M-dimensional null space, let
the corresponding RKHS H be orthogonally decomposed into an M-dimensional
space H0 with orthonormal basis pν and H1 s.t. K is positive deﬁnite on H1.
Consider the model
F(x) =
M
X
ν=1
θνpν(x) + b1/2u(x),
yi = F(xi) + εi,
where u(x) is a zero-mean GP with covariance function K and εi are independent
N(0, σ2). Furthermore, θ ∼N(0, aI) a priori. On the other hand, let fλ be the
minimiser in H of the regularised risk functional
1
n
n
X
i=1
(yi −f(xi))2 + λ∥P1f∥2,
where P1 is the orthogonal projection onto H1. Kimeldorf and Wahba [89] show
that fλ lies in the span of {pν | ν = 1, . . . , M} ∪{K(·, xi) | i = 1, . . . , n} and
give a numerical procedure for computing the coeﬃcients. If we deﬁne ˆFa(x) =
E[F(x) | y1, . . . , yn], then they show that
lim
a→∞
ˆFa(x) = fλ(x),
λ = σ2
n b
for every ﬁxed x. The proof (see [200], Chap. 1) is a straightforward application
of the duality between the RKHS H1 and the Hilbert space based on u(x), as

40
Chapter 2. Background
described in Section 2.1.4. The procedure of dealing with H0 and the improper
prior on θ is awkward but is not necessary if the covariance function is positive
deﬁnite (has null space {0}).
Finally, we note that a parametric extension of a non-parametric GP model
can be sensible even if K is positive deﬁnite, leading to semiparametric models
(or partial splines). For details about such models, we refer to [66], Chap. 4 and
[200], Chap. 6.
2.1.6
Maximum Entropy Discrimination. Large Margin Clas-
siﬁers
We regard GPs as building blocks for statistical models in much the same way
as a parametric family of distributions (see Section 2.1.2 for examples). Statis-
tical methods to estimate unknown parameters in such models follow diﬀerent
paradigms, and in machine learning the following have been most popular.
1. Probabilistic Bayesian paradigm:
Has been introduced in Section 2.1.2. As noted in Section 2.1.3, the (in-
tractable) posterior process is typically approximated by a GP itself.
2. Large margin (discriminative) paradigm:
Here, a “posterior” process is obtained by associating margin constraints
with observed data, then searching for a process which fulﬁls these (soft)
constraints and at the same time is close to the prior GP, in a sense made
concrete in this section. Since the constraints are linear in the latent out-
puts, the “posterior” process is always a GP with the same covariance as
the prior.
The relationship between Bayesian methods and penalised likelihood or gener-
alised spline smoothing methods has been discussed in Section 2.1.5.
Large
margin methods are special cases of spline smoothing models with a particu-
lar loss function which does not correspond to a probabilistic noise model (e.g.,
[201, 168, 187]). Several attempts have been made to phrase large margin discrim-

2.1. Bayesian Gaussian Processes
41
ination methods as approximations to Bayesian inference (e.g., [187, 168, 166]),
but the paradigm separation suggested in [83] seems somewhat more convincing.
The connection between these two paradigms has been formulated in [83],
this section is based on their exposition. The large margin paradigm has been
made popular by the empirical success of the support vector machine (SVM)
introduced below. In the Bayesian GP setting (see Section 2.1.2), the likelihood
P(y|u) of the observed data y can be seen to impose “soft constraints” on the
predictive distribution, in the sense that functions of signiﬁcant volume under the
posterior must not violate many of them strongly. In the large margin paradigm
whose probabilistic view has been called minimum relative entropy discrimination
(MRED) [83], such constraints are enforced more explicitly.25 We introduce a set
of latent margin variables γ = (γi)i ∈Rn, one for each datapoint. Along with
GP prior P(u(·)) on the latent function, we choose a prior P(γ) over γ. The
margin prior encourages large margins γi ≫0, as is discussed in detail below.
The minimum relative entropy distribution dQ(u(·), γ) is deﬁned as minimiser of
D[Q ∥P], subject to the soft margin constraints
E(u(·),   )∼Q [yiu(xi) −γi] ≥0, i = 1, . . . , n.
(2.23)
Just as in the case of a likelihood function, these constraints depend on the values
u = (u(xi))i of the random process u(·) only. It is well known in information
theory (e.g., [79], Sect. 3.1) that the solution to this constrained problem is given
by
dQ(u(·), γ)
dP(u(·), γ) = Z(λ)−1 exp
 n
X
i=1
λi (yiui −γi)
!
,
(2.24)
where
Z(λ) = E(u(·),   )∼P
"
exp
 n
X
i=1
λi (yiui −γi)
!#
.
The value for the Lagrange multipliers λ is obtained by minimising the convex
function log Z(λ) (sometimes called the dual criterion) under the constraints
λ ≥0. Since the right hand side of (2.24) factorises between u(·) and γ and
25For notational simplicity, we do not use a bias term b here. The modiﬁcations to do so are
straightforward. In the original SVM formulation, b can be seen to have a uniform (improper)
prior.

42
Chapter 2. Background
the same holds for the prior P, we see that Q must factorise in the same way.
Furthermore, it is immediate from (2.24) that Q(u(·)) is again a Gaussian process
with the same covariance kernel K as P(u(·)) and with mean function µ(x∗) =
k(x∗)TY λ, where Y
= diag(yi)i.
Due to the factorised form, we also have
Z(λ) = Zu(·)(λ)Z
  (λ) and
Zu(·)(λ) = E
  ∼P
h
e
 T

  i
= e
1
2
 T

 .
The form of Z
 depends on the choice of the prior P(γ) on the margin variables.
Jaakkola et. al. [83] give some examples of such priors which encourage large
margins. For example, if P(γ) = Q
i P(γi), then P(γi) should drop quickly for
γi < 1 in order to penalise small and especially negative margins (empirical
errors). In order for (2.23) to be a “soft constraint” only w.r.t. margin violations
and also to mimic the SVM situation, we have to use P(γi) = 0 for γi > 1.26 If
P(γi) ∝e−c(1−γi)I{γi≤1}, then
Z
  (λ) ∝
n
Y
i=1
e−λi
1 −λi/c,
and the complete dual criterion is
log Z(λ) = −
n
X
i=1
(λi + log(1 −λi/c)) + 1
2λTY KY λ,
λ ≥0.
(2.25)
Except for the potential term log(1 −λi/c), this is identical to the SVM dual ob-
jective (see below). The so-called hard margin SVM for which margin constraints
are enforced without allowing for violations, is obtained for c →∞. It converges
only if the training data is indeed separable and is prone to over-complicated
solutions. The eﬀect of the potential term on the solution is limited (see [83]).
It keeps λi from saturating to c exactly (which happens in SVM for misclassiﬁed
patterns). The dual criterion can be optimised using eﬃcient algorithms such
as SMO [142], although the nonlinear potential term introduces minor complica-
tions.27 Just like in SVM, sparsity in λ is encouraged and can be observed in
practice.
26As in the SVM setup, the choice of 1 as margin width is arbitrary, because the distance
can be re-scaled in terms of the prior variance.
27SMO makes use of the fact that the SVM criterion is quadratic with linear constraints.

2.1. Bayesian Gaussian Processes
43
To conclude, MRED gives a complete probabilistic interpretation of the SVM,
or at least of a close approximation thereof. Note that SVM classiﬁcation cannot
be seen as MAP approximation to Bayesian inference for a probabilistic model,
because its loss function does not correspond to a proper negative log likelihood
[168, 137, 187].
Interestingly, the MRED view points out limitations of this
framework as opposed to a Bayesian treatment of a Gaussian process model with a
proper likelihood. Recall from above that the margin constraints are linear in the
latent outputs u, leading to the fact that the MRED “posterior” process Q(u(·))
has the same covariance kernel K than the prior. While the constraints enforce
the predictive mean to move from 0 a priori to µ(x), the predictive variances are
simply the prior ones, independent of the data. At least to us this implies that if
predictive variances (or error bars) are to be estimated besides simply performing
a discrimination, then SVMs or other large margin discriminative methods are
not appropriate (this point is discussed in more detail in Section 4.7.2).
More important is the lack of well-founded methods for model selection with
SVM. For Bayesian GP methods, a general model selection strategy is detailed in
Section 2.1.3. Alternatively, hyperparameters can be marginalised over approx-
imately using MCMC techniques [132].
Model selection techniques for sparse
Bayesian GP methods are discussed in Section 4.5. In contrast, model selection
for SVM is typically done using variants of cross validation, which severely limits
the number of free parameters that can be adapted.
2.1.6.1
The Support Vector Machine
For the sake of completeness we brieﬂy review the SVM for binary classiﬁcation.
For details, see [161]. A detailed account on the history of SVM methods can be
found there as well. The algorithm for binary classiﬁcation has been proposed
originally in [33]. SVM can be seen as special case of penalised risk minimisation
or generalised smoothing splines (see Section 2.1.5), with the novelty that the risk
functional is not continuous. This often leads to sparse predictors (in the sense
that many of the n coeﬃcients are 0), but also requires constrained optimisation
for training.

44
Chapter 2. Background
Recall the weight space view from Section 2.1.1. For simplicity, we suppress
the feature mapping here and write x instead of Φ(x), but the reader should keep
in mind that linear functions discussed here may live in spaces of much higher
dimension that the input space. If u(x) = wTx + b is a hyperplane, it separates
all training data correctly if yiu(xi) > 0 for all i ∈{1, . . . , n}. The distance of xi
from the hyperplane is |ui|/∥w∥in the Euclidean space, so the hyperplane which
maximises the minimal margin over the training set is the solution of
min
 ,b ∥w∥2,
subj. to Y u ≥1.
This is the hard-margin support vector machine problem. Even if the data is
separable, neglecting some points may lead to a larger margin. This is important
especially in high-dimensional feature spaces where separations are typically pos-
sible, but may have very small margins due to outliers. The soft-margin support
vector machine problem relaxes the constraints by introducing slack variables
γ = (γi)i:
min
 ,b,
  ∥w∥2 + 1Tγ,
subj. to Y u ≥1 −γ, γ ≥0.
The term 1Tγ penalises each slack value γi linearly.28 By introducing Lagrange
multipliers λ ≥0 for the margin constraints and β ≥0 for the nonnegativity of
γ and minimising the Lagrangian w.r.t. the primal parameters w, b, γ, we obtain
w = XTY λ, where the data matrix X contains the xi as rows, furthermore the
equality constraint 1Tλ = 0. The dual variables β can be eliminated by adding
upper bound constraints on λ: λi ∈[0, 1]. Recall from Lagrange multiplier theory
that we can choose the dual parameters by maximising the Lagrangian, leading
to the dual problem
min
 1
2λTY KY λ −1Tλ,
subj. to λi ∈[0, 1], i = {1, . . . , n}.
The ﬁnal discriminant function is u(x∗) = λTY Xx∗= λTY k(x∗) with k(x∗) =
(K(xi, x∗))i. The Lagrange multiplier theorem also renders the so-called Karush-
28Readers may miss the C parameter in front of the penalty, allowing to adjust the trade-oﬀ
between it and ∥w∥2. We prefer to re-scale the feature space inner product which is equivalent
to multiplying the kernel K by C, the so-called variance parameter (see Section 2.1.8). In some
applications it makes sense to use diﬀerent penalty coeﬃcients Ci for each of the γi, the required
modiﬁcations are straightforward.

2.1. Bayesian Gaussian Processes
45
Kuhn-Tucker (KKT) conditions which must hold at the solution, namely when-
ever λi ∈(0, 1), then the corresponding constraint must hold with equality:
yi(ui +b) = 1. The corresponding patterns are sometimes called essential support
vectors. Also, it yi(ui + b) < 1, i.e. γi > 0, then we must have λi = 1, these
patterns are bound support vectors. Their union are the support vectors, i.e. the
patterns whose dual variables in λ are non-zero. b can be determined from any
set of essential support vectors. As mentioned above, the probabilistic MRED
view on the SVM is not perfect due to the non-quadratic (yet convex) dual cri-
terion, but certainly captures the essentials better than attempts to relate SVM
to Bayesian MAP approximations.
We conclude this section by mentioning a well-known fact about SVM: it is
a compression scheme in the sense deﬁned in Section 3.5.4. In other words, once
we have identiﬁed the set of support vectors, then retraining the machine on this
reduced set only gives back the same solution. This follows from the fact that
the KKT conditions for the reduced problem are the same as for the full problem
with the non-SV dual coeﬃcient set to 0. It also follows easily for MRED above,
because if λi = 0 it can simply be dropped from the criterion (2.25) (which has
a unique global minimum in both the full and the reduced problem).
2.1.7
Kriging
An important and early application of Gaussian random ﬁeld models has been
termed kriging [112] after a South-African mining engineer D. Krige who de-
veloped methods for predicting spatial ore-grade distributions from sampled ore
grades [93]. Optimal spatial linear prediction has its roots in earlier work by
Wiener and Kolmogorov (“closeness in space” may have to be replaced by “close-
ness in time”, since they were mainly concerned with time series). These fun-
damental ideas have been further developed in the ﬁelds of geostatistics [112] as
kriging and in meteorology under the name objective analysis (see [36], Chap. 3
for references).
We will not go into any details, but refer to [36], Chap. 3 and [189] (we follow

46
Chapter 2. Background
the latter here). The basic model is the same as for semiparametric smoothing:
z(x) = m(x)Tβ + ε(x)
where m(x) is a known feature map and ε(x) is a zero-mean random ﬁeld with co-
variance function K. If ε(x) is (weakly) stationary, K(x) is also called autocovari-
ance function. In a nutshell, kriging is a minimum mean-squared-error prediction
method for linear functionals of z(x) given observations z = (z(x1), . . . , z(xn))T
at spatial locations xi ∈Rd. For example, if z(x) measures ore grade at x one
might be interested in predicting
Z
B
z(x) dx
over some area B ⊂Rd. Since they focus on m.s. error and m.s. properties of
z(x) in general, kriging methods typically depend on second-order properties of
the process only, and ε(x) is often assumed to be a Gaussian ﬁeld. Furthermore,
we restrict ourselves to linear predictors λ0+λTz. The optimal predictor of z(x∗)
in the m.s. error sense is the conditional expectation which is linear in z if ε(x)
is Gaussian and β is known:
Kλ = k,
λ0 =
 m(x∗) −M Tλ
T β
where K = (K(xi, xj))i,j, k = (K(xi, x∗))i and M = (m(x1), . . . , m(xn))T.
If β is unknown, a simple procedure is to plug in the generalised least squares
estimate
ˆβ =
 M TK−1M
−1 M TK−1z
for ˆβ. This procedure can be motivated from several angles. If we restrict our
attention to linear predictors of z(x∗) which are unbiased in the sense
E

λ0 + λTz

= λ0 + λTM β = E[z(x∗)] = m(x∗)Tβ
for any β, the suggested approach minimises the m.s. error over these unbi-
ased predictors. It is therefore called best linear unbiased predictor (BLUP). A
Bayesian motivation can be constructed in the same way as mentioned in Sec-
tion 2.1.5. Namely, β is given a Gaussian prior whose covariance matrix scales

2.1. Bayesian Gaussian Processes
47
with a > 0 and ε(x) is a priori Gaussian. Then, the posterior mean for z(x∗)
converges to the BLUP as a →∞(i.e. as the β prior becomes uninformative).
The equations behind the BLUP have been known long before and have been
rediscovered in many areas of statistics. In practice, kriging methods are more
concerned about inducing an appropriate covariance function (or autocovariance
function under the stationarity assumption) from observed data as well.
The
empirical semivariogram is a frequently used method for estimating the autoco-
variance function close to the origin. On the theoretical side, Stein [189] advo-
cates the usefulness of ﬁxed-domain asymptotics to understand the relationship
between covariance model and behaviour of kriging predictors.29 By Bochner’s
theorem (2.2) an autocovariance function is characterised by its spectral distribu-
tion F(ω). Stein points out that ﬁxed-domain asymptotics depend most strongly
on the spectral masses for large ∥ω∥, i.e. the high frequency components, much
less so on the low frequency ones or the mean function m(x)Tβ (if m(x) is
smooth itself, e.g. polynomials). Let f(ω) be the spectral density, i.e. the Fourier
transform of K(x). In general, the lighter the tails of f(ω) the smoother ε(x)
is in the m.s. sense. Stein advocates this expected smoothness as a central pa-
rameter of the GP prior and condemns the uncritical use of smooth (analytic)
covariance functions such as the RBF (Gaussian) kernel (see Section 2.1.8). An-
other important concept highlighted by Stein (see also [200], Chap. 3) is the one
of equivalence and orthogonality of GPs.30 Essentially, GPs with autocovariance
functions of diﬀerent form can be equivalent in which case it is not possible to
unambiguously decide for one of them even if an inﬁnite amount of observations
in a ﬁxed region are given. On this basis, one can argue that for a parametric
family of autocovariance functions inducing equivalent GPs the parameters can
just as well be ﬁxed a priori since their consistent estimation is not possible. On
29Stein restricts his analysis to “interpolation”, i.e. to situations where predictions are re-
quired only at locations which are in principle supported by observations (in contrast to “ex-
trapolation” often studied in the time series context). This should not be confused with the
distinction between interpolation and smoothing used in Section 2.1.5. All non-trivial kriging
techniques are smoothing methods.
30Two probability measure are equivalent if they have the same null sets, i.e. are mutually
absolutely continuous (see Section A.1). They are orthogonal if there is a null set of one of them
which has mass 1 under the other. Gaussian measures are either orthogonal or equivalent.

48
Chapter 2. Background
the other hand, parameters s.t. diﬀerent values lead to orthogonal GPs should be
learned from data and not be ﬁxed a priori.
Note that kriging models are more generally concerned with intrinsic random
functions (IRF) [113], generalisations of stationary processes which are also fre-
quently used in the spline smoothing context. In a nutshell, a k-IRF u(x) is a
non-stationary random ﬁeld based on a “spectral density” whose integral diverges
on any environment of the origin (e.g., has inﬁnite pointwise variance). However,
if c ∈Rn is a generalised divided diﬀerence (g.d.d.) for x1, . . . , xn in the sense
that P
i cip(xi) = 0 for all polynomials p of total degree ≤k, then the variance
of P
i ciu(xi) is ﬁnite and serves to deﬁne an “autocovariance function” K(x)
which is conditionally positive semideﬁnite, namely
n
X
i,j=1
cicjK(xi −xj) ≥0
for all g.d.d.’s c. In practice, one uses semi-parametric models where the latent
process of interest is the sum of a k-IRF and a polynomial of total degree ≤k
whose coeﬃcients are parametric latent variables. In fact, IRFs do not add more
generality w.r.t. high-frequency behaviour of the process since f(ω) must be
integrable on the complement of any 0-environment, so the IRF can be written
as (uncorrelated) sum of a stationary and a non-stationary part, the latter with
f(ω) = 0 outside a 0-environment (thus very smooth). IRFs are not discussed
in any further detail in this thesis (see [113, 189]).
2.1.8
Choice of Kernel. Kernel Design
There is a tendency in the machine learning community to treat kernel methods
as “black box” techniques, in the sense that covariance functions are chosen from
a small set of candidates31 over and over again. If a family of kernels is used,
it typically comes with a very small number of free parameters so that model
selection techniques such as cross-validation can be applied. Even though such
approaches work surprisingly well for many problems of interest in machine learn-
ing, experience almost invariably has shown that much can be gained by choosing
31Some of which are not positive semideﬁnite, see below.

2.1. Bayesian Gaussian Processes
49
or designing covariance functions carefully depending on known characteristics of
a problem (for an example, see [161], Sect. 11.4).
Establishing a clear link between kernel functions and consequences for pre-
dictions is very non-trivial and theoretical results are typically asymptotic argu-
ments. As opposed to ﬁnite-dimensional parametric models, the process prior
aﬀects predictions from a non-parametric model even in ﬁxed-domain asymptotic
situations (see Section 2.1.7). The sole aim of this section is to introduce a range
of frequently used kernel functions and some of their characteristics, to give some
methods for constructing covariance functions from simpler elements, and to show
some techniques which can be used to obtain insight into the behaviour of the
corresponding GP. Yaglom [214] contains extensive material, an accessible review
is [1]. In the ﬁnal part, we discuss some kernel methods over discrete spaces X .
It should be noted that positive deﬁniteness of an arbitrary symmetric form
or function is hard to establish in general. For example, the sensible approach of
constructing a distance d(x, x′) between patterns depending on prior knowledge,
then proposing
K(x, x′) = e−w d(
  ,
 ′)2
(2.26)
as covariance function does not work in general because K need not be positive
semideﬁnite, moreover there is no simple criterion to prove or disprove K as
covariance function.32 If d(x, x′) can be represented in an Euclidean space, K
is a kernel as we will see below. Note that if K(x, x′) is a kernel, so must be
K(x, x′)t for any t > 0.33 Kernels with this property are called inﬁnitely divisible.
Sch¨onberg [162] managed to characterise inﬁnitely divisible kernels (2.26) by a
property on d(x, x′) which unfortunately is just as hard to handle as positive
semideﬁniteness.34
32If d(x, x′) is stationary, one can try to compute the spectral density, but this will not be
analytically tractable in general.
33This is true in general only for t ∈N>0, see below.
34−d(x, x′)2 must be conditionally positive semideﬁnite of degree 0 (see Section 2.1.7).

50
Chapter 2. Background
2.1.8.1
Constructing Kernels from Elementary Parts
We can construct complicated covariance functions from simple restricted ones
which are easier to characterise (e.g. stationary or (an)isotropic covariance func-
tions, see Section 2.1.1). A large number of families of elementary covariance
functions are known (e.g., [214]), some of which are reviewed below.
A generalisation of stationary kernels to conditionally positive semideﬁnite
ones (stationary ﬁelds to IRFs) is frequently used in geostatistical models (see
Section 2.1.7) but will not be discussed here. The class of positive semideﬁnite
forms has formidable closure properties. It is closed under positive linear com-
binations, pointwise product and pointwise limit.
If K(x, x′) is a covariance
function, so is
˜K(y, y′) =
Z
h(y; x)h(y′; x′)K(x, x′) dxdx′
(2.27)
(if ˜K is ﬁnite everywhere). An important special case is ˜K(y, y′) = a(x)K(x, x′)
a(x′). For example, a given kernel (with positive variance everywhere) can always
be modiﬁed to be constant on the diagonal by choosing a(x) = K(x, x)−1/2. Note
that O’Hagan’s “localized regression model” (Section 2.1.1) is also a special case
of (2.27). A general way of creating a non-stationary covariance function ˜K(y, y′)
from a parametric model h(y; θ) linear in θ is to assume a GP prior on θ, then
to integrate out the parameters (see [164] for details).
Furthermore, suppose
we do so with a sequence of models and priors to obtain a sequence of kernels.
If the priors are appropriately scaled, the pointwise limit exists and is a kernel
again. Many standard kernels can be obtained in this way (e.g., [207]). Neal
showed that if the model size goes to inﬁnity and the prior variances tend to 0
accordingly, layered models with non-Gaussian priors will also tend to a GP (due
to the central limit theorem; see Section 2.1.1).
Another important modiﬁcation is embedding.
If K(x, x′) is a covariance
function and h(y) is an arbitrary map, then
˜K(y, y′) = K(h(y), h(y′))
(2.28)
is a covariance function as well (this is a special case of (2.27)).
For exam-
ple, if we have d(x, x′) = ∥h(x) −h(x′)∥in some Euclidean space, then (2.26)

2.1. Bayesian Gaussian Processes
51
is a valid kernel induced from the Gaussian (RBF) kernel (2.29). The Fisher
kernel [81] and mutual information kernels [169] are examples. Embedding can
be used to put rigid constraints on the GP. For example, if K is stationary
in (2.28) and h(y) = h(y′), then u(y) = u(y′) almost surely.35
For h(y) =
(cos((2π/ν)y), sin((2π/ν)y))T, sample paths of u(y) are ν-periodic functions.
2.1.8.2
Some Standard Kernels
In the following, we provide a list of frequently used “standard kernels”. Most
of these will have a variance (scaling) parameter C > 0 in practice, sometimes
an oﬀset parameter vb > 0, thus instead of K one uses C K or C K + vb. C
scales the variance of the process, while a vb > 0 comes from the uncertainty
of a bias parameter added to the process.36
In applications where the kernel
matrix K is used directly in linear systems, it is advised to add a jitter term37
ρδ
  ,
 ′ to the kernel to improve the condition number of K. This amounts to a
small amount of additive white noise on u(x) (ρ can be chosen quite small), but
should not be confused with measurement noise which is modelled separately (see
Section 2.1.2).38 These modiﬁcations are omitted in the sequel for simplicity.
The Gaussian (RBF) covariance function
K(x, x′) = exp

−w
2d∥x −x′∥2
(2.29)
is isotropic for each X = Rd (i.e. D∞). w > 0 is an inverse squared length scale
parameter, in the sense that w−1/2 determines a scale on which u(x) is expected
to change signiﬁcantly. K(x) is analytic at 0, so u(x) is m.s. analytic. Stein
[189] points out that
k
X
j=0
u(j)(0)xj
j! →u(x)
35This is because the correlation ρ(u(y), u(y′)) is 1, thus u(y) = a u(y′) + b for ﬁxed a, b,
then a = 1, b = 0 because both variables have the same mean and variance.
36For reasons of numerical stability, vb must not become too large.
37In the context of kriging (see Section 2.1.7), adding ρδx,x′ has been proposed by Math´eron
to model the so-called “nugget eﬀect” (see [36], Sect. 2.3.1), but other authors have criticised
this practice.
38The probit noise model (2.9) is obtained by adding N(0, 1) noise and passing the result
through a Heaviside step.

52
Chapter 2. Background
in quadratic mean for every x (a similar formula holds for X = Rd), so that u
can be predicted perfectly by knowing all its derivatives at 0 (which depend on u
on an environment of 0 only). He criticises the wide-spread use of the Gaussian
covariance function because its strong smoothness assumptions are unrealistic for
many physical processes, in particular predictive variances are often unreasonably
small given data. The spectral density (in R) is f(ω) = (2πw)−1/2 exp(−ω2/(2w))
with very light tails.
On the other hand, Smola et. al. [182] recommend the
use of the Gaussian covariance function for high-dimensional kernel classiﬁcation
methods because of the high degree of smoothness.39
It is interesting to note
that in the context of using GPs for time series prediction, Girard et. al. [65]
report problems with unreasonably small predictive variances using the Gaussian
covariance function (although they do not consider other kernels in comparison).
Figure 2.1 shows smoothed plots of some sample paths. Note the eﬀect of the
length scale w−1/2 and the high degree of smoothness.
We can consider the anisotropic version, called squared-exponential covariance
function:
K(x, x′) = exp

−1
2d(x −x′)TW (x −x′)

.
(2.30)
Here, W is positive deﬁnite. Typically, W is a diagonal matrix with an inverse
squared length scale parameter wj for each dimension. Full matrices W have
been considered in [144, 198], and factor analysis-type matrices W are a useful
intermediate (e.g., [11, 168]). An important application of the additional d.o.f.’s
in (2.30) as compared to the Gaussian kernel is automatic relevance determina-
tion (ARD), as discussed below. Note that the squared-exponential covariance
function can be seen as product of d one-dimensional Gaussian kernels with dif-
ferent length scales, so the corresponding RKHS is a tensor product space built
from RKHS’s for one-dimensional functions (see Section 2.1.4).
The Mat´ern class of covariance functions (also called modiﬁed Bessel covari-
ance functions) is given by
K(τ) =
π1/2
2ν−1Γ(ν + 1/2)α2ν (ατ)νKν(ατ),
τ = ∥x −x′∥,
(2.31)
39Most experiments in this thesis use the Gaussian covariance function.

2.1. Bayesian Gaussian Processes
53
0
0.2
0.4
0.6
0.8
1
−4
−3
−2
−1
0
1
2
3
4
Gaussian (RBF)
Figure 2.1: Smoothed sample paths from GP with Gaussian covariance function. All
have variance C = 1. Dash-dotted: w = 1. Solid: w = 102. Dashed: w = 502.
where ν > 0, α > 0 and Kν(x) is a modiﬁed Bessel function (e.g., [189], Sect. 2.7).
One can show that zνKν(z) →2ν−1Γ(ν) for z →0, so
K(0) =
π1/2Γ(ν)
Γ(ν + 1/2)α2ν .
K is isotropic for each X = Rd. An important feature of this class is that the
m.s. smoothness of u(x) can be regulated directly via ν.
For example, u(x)
is m times m.s. diﬀerentiable iﬀν > m. The spectral density in R is f(ω) =
(α2 + ω2)−ν−1/2. For ν = 1/2 + m we obtain a process with rational spectral
density, a continuous time analogue of an ARMA time series model. For ν =
1/2, K(τ) ∝e−ατ deﬁnes an Ornstein-Uhlenbeck process, a stationary analogue
to the Wiener process which also has independent increments. In general, for
ν = 1/2 + m we have K(τ) ∝e−ατp(ατ), where p(x) is a polynomial of order m
with constant coeﬃcient 1 (e.g., [189], Sect. 2.7). Note that if α = (w(2ν +1))1/2,

54
Chapter 2. Background
then
α2ν+1f(ω) →e−ω2/(2w) (ν →∞),
thus K(τ) converges to the Gaussian covariance function after appropriate re-
scaling.
The Mat´ern class can be generalised to an anisotropic family in the same way
as the Gaussian kernel. Figure 2.2 show some sample function plots for values
ν = 1/2, 3/2, 5/2, 10. Note the eﬀect of ν on the roughness of the sample paths.
For ν = 1/2 the paths are erratic even though the length scale is 1, i.e. the same
as the horizontal region shown. For ν = 3/2, the process is m.s. diﬀerentiable,
for ν = 5/2 twice so.
The exponential class of covariance functions is given by
K(τ) = e−ατ δ, δ ∈(0, 2].
The positive deﬁniteness can be proved using the Mat´ern class (see [189], Sect. 2.7).
For δ = 1, we have the Ornstein-Uhlenbeck covariance function, for δ = 2 the
Gaussian one. Although it seems that the kernel varies smoothly in δ, the pro-
cesses have quite diﬀerent properties in the regimes δ ∈(0, 1), δ = 1, δ ∈(1, 2)
and δ = 2.
Continuous sample paths can be ensured for any δ ∈(0, 2], but
diﬀerentiable sample paths can only be obtained for δ = 2 (in which case they
are analytic).40 K(τ) is not positive deﬁnite for δ > 2. Figure 2.3 shows some
sample path plots.
We have derived the spline covariance function on [0, 1] (2.22) from ﬁrst prin-
ciples above. This kernel is of interest because posterior mean functions in GP
models (or minimisers of the variational problem over the RKHS) are splines of
order m, i.e. piecewise polynomials in C2m−2 (see Section 2.1.5) and associated
computations are O(n) (where n is the number of training points, or “knots”)
only. On the other hand, technical complications arise because spline kernels are
RKs for subspaces of Wm[0, 1] only, namely of the functions which satisfy the
boundary conditions (see Section 2.1.5). The operator induced by a spline kernel
has a null space spanned by polynomials, and in practice it is necessary to adjoin
40All these statements hold with probability 1, as usual.

2.1. Bayesian Gaussian Processes
55
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
−1.5
−1
−0.5
0
0.5
1
1.5
2
Matern (nu=1/2): Ornstein−Uhlenbeck
0
0.2
0.4
0.6
0.8
1
−3
−2
−1
0
1
2
3
Matern (nu=3/2)
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
−3
−2
−1
0
1
2
3
Matern (nu=5/2)
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
−3
−2
−1
0
1
2
3
Matern (nu=10)
Figure 2.2: Smoothed sample paths from GP with Mat´ern covariance function. All
have variance C = 1. Upper left: Ornstein-Uhlenbeck (Mat´ern, ν = 1/2), w = 1.
Upper right: Mat´ern, ν = 3/2, w = 1 (dash-dotted), w = 102 (solid). Lower left:
Mat´ern, ν = 3/2, w = 1 (dash-dotted), w = 102 (solid). Lower right: Mat´ern,
ν = 10, w = 1 (dash-dotted), w = 102 (solid).
the corresponding (ﬁnite-dimensional) space. The spline kernels are not station-
ary (they are supported on [0, 1]), but we can obtain spline kernels on the circle
by imposing periodic boundary conditions on Wm[0, 1], leading to the stationary
kernel
K(x, x′) =
X
ν≥1
2
(2πν)2m cos(2πν(x −x′)).

56
Chapter 2. Background
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
−3
−2
−1
0
1
2
3
Exponential
Figure 2.3: Smoothed sample paths from GP with exponential covariance function.
All have variance C = 1 and α = 102. Solid: δ = 1.5. Dashed: δ = 1.9. Dash-
dotted: δ = 2 (Gaussian).
From this representation, it follows that the spectral density is
f(ω) =
X
ν≥1
1
(2πν)2m δ2πν(|ω|)
which is discrete. Note that sample functions from u(x) are periodic with proba-
bility 1. In Wahba [200], Chap. 2 it is shown how to construct splines on the sphere
by using the iterated Laplacian, but this becomes quite involved. An equivalent
to splines (in a sense) can be deﬁned in Rd using thin-plate spline conditionally
positive deﬁnite functions (see Section 2.1.7), see [200, 66] for details.
For kernel discrimination methods, polynomial covariance functions
K(x, x′) =
(xTx′ + α)m
((∥x∥2 + α)(∥x′∥2 + α))m/2 ,
α ≥0, m ∈N
are popular although they seem unsuitable for regression problems. Polynomial
kernels can be seen to induce a ﬁnite-dimensional feature space of polynomials

2.1. Bayesian Gaussian Processes
57
of total degree ≤m (if α > 0). It is interesting to note that this is exactly the
RKHS we have to adjoin to one for a conditionally positive deﬁnite kernel of
order m such as the thin-plate spline covariance function. On the other hand, in
the spline case these polynomial parts are usually not regularised at all. By the
Karhunen-Loeve expansion (see Section 2.1.4), we can write u(x) as expansion
in all monomials of total degree ≤m with Gaussian random coeﬃcients. The
regularisation operator (see Section 2.1.5) for polynomial kernels is worked out in
[182]. Note that K(x, x′) is not a covariance function for m ̸∈N, thus the kernel
is not inﬁnitely divisible. Figure 2.4 shows some sample path plots. These are
polynomials and therefore analytic.
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
−2.5
−2
−1.5
−1
−0.5
0
0.5
1
1.5
2
Polynomial
Figure 2.4: Sample paths from GP with polynomial covariance function. All have
variance C = 1 and α = 0.05. Solid: m = 10. Dashed: m = 5.
The Euclidean inner product xTΣx′ is sometimes referred to as “linear kernel”
in the machine learning literature. GP models based on this kernel are nothing
else than straightforward linear models (linear regression, logistic regression, etc.).
It is clear from the weight space view (see Section 2.1.1) that a linear model can

58
Chapter 2. Background
always be regarded as a GP model (or kernel technique), but this makes sense
only if n < d, where n is the number of training points.41 Furthermore, the SVM
with linear kernel is a variant of the perceptron method [154] with “maximal
stability” [156] studied in statistical physics.
Finally, let us give an example of a function which is not a covariance function,
the so-called “sigmoid kernel”
K(x, x′) = tanh
 axTx′ + b

.
K is not positive semideﬁnite for any a, b (see [180]), it is nevertheless shipped
in most SVM packages we know of. It springs from the desire to make kernel ex-
pansions look like restricted one-layer neural networks. The correct link between
MLPs and GP models has been given by Neal (see Section 2.1.1), which involves
taking the limit of inﬁnitely large networks. A covariance function corresponding
to a one-layer MLP in the limit has been given by Williams [207]. In practice, it
is of course possible to ﬁt expansions of kernels to data which are not covariance
functions. However, the whole underlying theory of minimisation in a RKHS (see
Sections 2.1.4 and 2.1.5) breaks down, as does the view as inference in a GP
model. On the practical side, ﬂawed results such as negative predictive variances
can pop up when least expected. Even worse, most optimisation techniques (in-
cluding SVM algorithms) rely on the positive semideﬁniteness of matrices and
may break down otherwise.
2.1.8.3
Guidelines for Kernel Choice
Choosing a good kernel for a task depends on intuition and experience. On high-
dimensional tasks where no suitable prior knowledge is available, the best option
may be to explore simple combinations of the standard kernels listed above. If
invariances are known, they may be encoded using the methods described in
[161], Sect. 11.4. With approximate Bayesian GP inference methods as discussed
in this thesis, one can in principle use combinations of diﬀerent kernels with a lot
of free (hyper)parameters which can be adapted automatically.
41Otherwise, running a kernel algorithm is wasteful and awkward due to a singular kernel
matrix.

2.1. Bayesian Gaussian Processes
59
For low-dimensional X , one can obtain further insight. Stein [189] points out
the usefulness of studying ﬁxed-domain asymptotics (see Section 2.1.7). In this
respect, the tail behaviour of the spectral density (see Section 2.1.1) is important.
The m.s. degree of diﬀerentiability (degree of smoothness) of the process depends
on the rate of decay of f(ω).
Stein recommends kernel families such as the
Mat´ern class (2.31) which come with a degree of smoothness parameter ν. He
also stresses the importance of the concept of equivalence and orthogonality of
GPs (see Section 2.1.7), arguing that if a set of kernels have equivalent processes,
one might as well ﬁx any of them a priori because even with an inﬁnite amount
of data we cannot separate between them with absolute certainty. Both of these
arguments are asymptotic. For example, it is not clear whether ν in the Mat´ern
class can be learned accurately enough from a limited amount of data. Also,
predictions from equivalent processes with diﬀerent kernels can be diﬀerent.42
There are ways of “getting a feeling” for the behaviour of a process by vi-
sualisation, which is an option if X = Rd is low-dimensional, d = 1, 2.
We
can draw “samples” from the process and plot them as follows (the plots in this
section have been produced in this way). Let X ⊂X be a ﬁne grid43 over a
domain of interest, n = |X| and u = u(X) ∼N(0, K(X)). We can sample u
as u = Lv, v ∼N(0, I), where K(X) = LLT is the Cholesky decomposition
(Deﬁnition A.2). If X is too large, u can be approximated using an incomplete
Cholesky factorisation of K(X) (see [212]). If d = 1, the process is isotropic
and the grid is regularly spaced, K(X) has Toeplitz structure44 and its Cholesky
decomposition can be computed in O(n2) (see [44]). Repeatedly sampling u and
plotting (X, u) can give an idea about degree of smoothness, average length scales
(Euclidean distance in X over which u(x) is expected to vary signiﬁcantly) or
other special features of K.
42Stein argues (citing Jeﬀreys) that such diﬀerences cannot be important since they do not
lead to consistency in the large data limit (in a ﬁxed domain).
43For ﬁne grids and smooth kernels such as the Gaussian one, the Cholesky technique de-
scribed here fails due to round-oﬀerrors. The singular value decomposition (SVD) should be
used in this case, concentrating on the leading eigendirections which can be determined reliably.
44A matrix is Toeplitz if all its diagonals (main and oﬀ-diagonals) are constant.

60
Chapter 2. Background
2.1.8.4
Learning the Kernel
One promising approach for choosing a covariance function is to learn it from data
and/or prior knowledge. For example, given a parametric family of covariance
functions, how can we choose45 the parameters in order for the corresponding
process to model the observed data well?
Model selection from a ﬁxed family can be done by the empirical Bayesian
method of marginal likelihood maximisation (see Section A.6.2), a generic approx-
imation of which in the case of GP models is given in Section 2.1.3. Since this
procedure typically scales linearly in the number of hyperparameters, elaborate
and heavily parameterised families can be employed. An important special case
has been termed automatic relevance determination (ARD) by MacKay [109] and
Neal [131]. The idea is to introduce a hyperparameter which determines the scale
of variability of a related variable of interesting (with prior mean 0). For example,
we might set up a linear model (2.4) by throwing in a host of diﬀerent features
(components in Φ(x)), then place a N(β|0, D) on the weights β where D is a
diagonal matrix of positive hyperparameters. If we place a hyperprior on diag D
which encourages small values, there is an a priori incentive for di = Di,i to
become very small, inducing a variance of βi close to 0 which eﬀectively switches
oﬀthe eﬀect of βiφi(x) on predictions.
This is balanced against the need to
use at least some of the components of the model to ﬁt the data well, leading
to an automatic discrimination between relevant and irrelevant components. In
the context of covariance functions, we can implement ARD with any anisotropic
kernel (see Section 2.1.1) of the form
K(x, x′) = ˜K((x −x′)TW (x −x′)),
where ˜K is isotropic and W is diagonal and positive deﬁnite. An example is the
squared-exponential covariance function (2.30). Here, wi determines the scale
of variability of the (prior) ﬁeld as x moves along the i-th coordinate axis. If
we imagine the ﬁeld being restricted to a line parallel to this axis, w−1/2
i
is the
45The proper Bayesian solution would be to integrate out the parameters (see Section A.6.2),
but even if this can be approximated with MCMC techniques, the outcome is a mixture of
covariance functions leading to expensive predictors.

2.1. Bayesian Gaussian Processes
61
length scale of this restriction, i.e. a distance for which the expected change of
the process is signiﬁcant.
If wi ≈0, this length scale is very large, thus the
ﬁeld will be almost constant along this direction (in regions of interest). Thus,
via ARD we can discriminate relevant from irrelevant dimensions in the input
variable x automatically, and predictions will not be inﬂuenced signiﬁcantly by
the latter. Section 4.8.2 provides an example for how important ARD can be in
the context of diﬃcult real-world GP regression problems with high-dimensional
input spaces.
In spatial statistics, semivariogram techniques (see [36], Sect. 2.3.1) are fre-
quently used.
For a stationary process, the (semi)variogram is γ(x −x′) =
(1/2)Var[u(x)−u(x′)]. It is estimated by averaged squared distances over groups
of datapoints which are roughly the same distance apart and ﬁtted to paramet-
ric families by maximum likelihood. Stein [189] criticises the use of the empirical
semivariogram as single input for choosing a covariance function, which is actually
impossible if the ﬁeld is m.s. diﬀerentiable and gives a range of other techniques,
including the empirical Bayesian approach mentioned above.
For classiﬁcation models, the idea of local invariance w.r.t. certain groups
of transformations is important. For example, the recognition of handwritten
digits should not be inﬂuenced by translations or small-angle rotations of the
bitmap.46
If a process is used as latent function in a classiﬁcation problem,
e.g. representing the log probability ratio between classes (see Section 2.1.2),
then starting from some x and applying small transformations from a group
w.r.t. which discrimination should remain invariant should not lead to signiﬁcant
changes in the process output (e.g. in the m.s. sense). To relate this notion to ARD
above, varying x along such invariant directions should induce a coordinate of x
(non-linear in general) which is irrelevant for prediction. Chapter 11 in [161] gives
a number of methods for modifying a covariance function in order to incorporate
invariance knowledge to some degree, also reviewing work in that direction which
we omit here.
Finally, Minka [122] pointed out that instances of the “learning how to learn”
46Although a 180-degree rotation of a 6 results in a 9.

62
Chapter 2. Background
or “prior learning” paradigm can be seen as learning a GP prior from multi-task
data (see his paper for references). In fact, the setup is the one of a standard
hierarchical model frequently used in Bayesian statistics to implement realistic
prior distributions. We have access to several noisy samples and make the as-
sumption that these have been sampled from diﬀerent realisations of the latent
process which in turn have been sampled i.i.d. from the process prior. Data of this
sort is very valuable for inferring aspects of the underlying covariance function.
In a simple multi-task scenario a multi-layer perceptron is ﬁt to several samples
by penalised maximum likelihood, sharing the same input-to-hidden weights but
using diﬀerent sets of hidden-to-output weights for each sample. The idea is that
the hidden units might discover features which are important in general, while
the combination in the uppermost layer is speciﬁc. If we place Gaussian priors
on the hidden-to-output weights, this becomes a GP model with a covariance
function determined by the hidden units. More generally, we can start from any
parametric family of covariance functions and learn hyperparameters or even the
hyperposterior from multi-task data using marginal likelihood maximisation to-
gether with the hierarchical sampling model. An approximate implementation of
this idea has been reported in [141].
2.1.8.5
Kernels for Discrete Objects
As mentioned in Section 2.1.1, in principle the input space X is not restricted
to be Rd or even a group.
For example, Gaussian processes over lattices are
important in vision applications (in the form of a Gaussian Markov random ﬁeld
with sparse structured inverse covariance matrix). For Gaussian likelihoods, the
posterior mean can be determined most eﬃciently using a conjugate gradients
solver47 and the embedded trees algorithm of Wainwright, Sudderth and Willsky
[202] can be used to compute the marginal variances as well. Kernel methods,
i.e. methods which use covariance matrices over variables determined from the
“spatial” relationship of these (or associated covariates) have been proposed for
47Loopy belief propagation renders the correct mean as well if it converges [206], but is much
slower and often numerically unstable.

2.1. Bayesian Gaussian Processes
63
a number of problems involving discrete spaces X (ﬁnite or countably inﬁnite).
Our aim in this section is no more than to give a few selected examples.
Kernels can be deﬁned on the set of ﬁnite-length strings from a ﬁnite alphabet.
Many string kernels have been proposed recently, but we will not try to review
any of this work. Important applications of string kernels (or distance measures
between sequences) arise from problems in DNA or RNA biology where statistical
models have to be build for nucleotide sequences. Many proposed string kernels
are special cases of convolution kernels introduced by Haussler [69]. Maybe the
most interesting case discussed there is the extension of a hidden Markov random
ﬁeld (HMRF). The latter is a Markov random ﬁeld (MRF) with observed variables
x, latent variables u and clique potentials Cd(xd, ud) where xd, ud are subsets
of components of x, u, and u is marginalised over.
If we replace the clique
potential by positive deﬁnite kernels Kd((xd, ud), (yd, vd)) and marginalise over
u, v, the result is a covariance kernel which can also be seen as unnormalised
joint generative distribution for (x, y). If the original MRF has a structure which
allows for tractable computation, the same algorithm can be used to evaluate the
covariance function eﬃciently. For example, a hidden Markov model (HMM) for
sequences can be extended to a pair-HMM in this way, emitting two observed
sequences sharing the same latent sequence, and many string kernels arise as
special cases of this construction.
In practice, string kernels (and more generally kernels obtained from joint
probabilities under pair-HMRFs) often suﬀer from the “ridge problem”: K(x, x)
is much larger than K(x, x′) for many x′ for which a priori we would like to
attain a signiﬁcant correlation, especially if rather long sequences are compared.
For example, in models involving DNA sequences we would like sequences to
correlate strongly if they are homologuous, i.e. encode for proteins of very similar
function. In a standard string kernel, two sequences are strongly correlated if both
can be obtained from a common “ancestor” latent sequence by few operations such
as insertions and substitutions (this ancestor model is motivated by the evolution
of genes and gives a good example for the pair-HMM setup). However, often
homologuous sequences diﬀer quite substantially in regions on which the structure

64
Chapter 2. Background
of the functional part of the protein does not depend strongly. Such “remote”
homologies are the really interesting ones, since very close homologies can often
be detected using simpler statistical techniques than process models based on
string kernels. On the other hand, it may be possible to spot such homologies by
going beyond string kernels and pair-HMRF constructions, for example building
on the general framework given in [32] where kernels are obtained from ﬁnite
transducers.
A conceptually simple way to obtain a kernel on X is to embed X in some
Euclidean space Rd, then to concatenate the embedding with any of the known Rd
kernels, for example the Gaussian one (2.29). An example is the Fisher kernel [81]
which maps datapoints to their “Fisher scores” under a parametric model. There
has been a surge of interest recently in automatic methods for parameterising low-
dimensional non-linear manifolds (e.g., [191, 155]) by local Euclidean coordinates.
Although these methods are non-parametric, they can be used to ﬁt conventional
parametric mixture models in order to obtain a parametric embedding which
could then be used to obtain a kernel.
Recently, Kondor and Laﬀerty [92] proposed kernels on discrete objects using
concepts from spectral graph theory (diﬀusion on graphs). If X is ﬁnite, a covari-
ance function on X is simply a positive semideﬁnite matrix. If H is a symmetric
generator matrix, the corresponding exponential kernel is deﬁned as
K(β) = exp(βH) =
X
j≥1
βj
j! Hj,
β ≥0.
(2.32)
We deﬁne Kβ(x, x′) = K(β)
  ,
 ′, where we use elements of X as indices into the
matrix K(β). K(β) is positive deﬁnite. In fact, it has the same eigenvectors as
H, but the eigenspectrum is transformed via λ →exp(βλ). In practice, general
exponential kernels cannot be computed feasibly if X is large, especially there is
no general eﬃcient way of computing kernel matrices of Kβ over points of interest.
It might be possible to approximate marginalisations of K(β) by sampling. The
kernel and generator matrices are linked by the heat equation
∂
∂β K(β) = HK(β).

2.1. Bayesian Gaussian Processes
65
It is interesting to note that every inﬁnitely divisible covariance function on X has
the form (2.32), namely if Kβ = Kβ
1 with covariance matrix K then H = ∂K/∂β
at β = 0. Kondor and Laﬀerty are interested in diﬀusion kernels on graphs as
special cases of exponential kernels. Here, the generator is the negative of the
so-called graph Laplacian. The construction can be seen as stationary Markov
chain (random walk) in continuous time on the vertices of the graph. The ker-
nel Kβ(x, x′) is the probability of being at x′ at time β, given that the state
at time 0 was x. This interpretation requires that H1 = 0 which is true for
the negative graph Laplacian and which implies that K(β) is (doubly) stochastic.
The same equation describes heat ﬂow or diﬀusion from an initial distribution.
The idea is to describe the structure of X (in the sense of “closeness”, i.e. close
points should be highly correlated under the covariance function) in terms of local
neighbourhood association which induce an (weighted or unweighted) undirected
graph. Then, the correlation at some x with all other points is proportional to
the distribution of a random walk started at x after time β. Similar ideas have
been used very eﬀectively for non-parametric clustering. Kondor and Laﬀerty
give examples for graph of special regular structures for which the diﬀusion ker-
nel can be determined eﬃciently. These include certain special cases of string
kernels (here, X is inﬁnite and the analogue to Markov chains has to be treated
more carefully). In situations where Kβ cannot be determined by known simple
recursive formulae, one could represent X by a representative sample including
the training set (but also unlabelled data). If the generator matrix of the un-
derlying graph (projected onto the representative sample in a sensible way) is
sparse, its leading eigenvectors and eigenvalues could be approximated by sparse
eigensolvers which would lead to an approximation of K(β) which is low-rank
optimal w.r.t. the Frobenius norm. Kondor and Laﬀerty also note that on the
graph given by a regular grid in Rd, the generator matrix converges towards the
usual Laplacian operator and Kβ towards the Gaussian kernel (2.29) as the mesh
size approaches 0.

66
Chapter 2. Background
2.2
Learning Theory
The ﬁeld of computational learning theory (COLT) is concerned with two basic
questions. Given a particular problem setup, described typically as a set of re-
lated random variables, the goal is to predict future outcomes of a particular
variable of interest. The accuracy of a prediction can be quantiﬁed in a frequen-
tist way by imagining that the variable and its prediction are sampled repeatedly
and independently and counting the number of times the prediction is correct.48
Suppose we observe an i.i.d. sample49 of the problem at hand. How many sample
points do we need to learn a predictor which attains a desired accuracy? How
much computation do we have to spend to learn such a predictor?
The ﬁrst
question asks for the sample complexity of the problem. The second question,
although of major importance in practice, is not discussed here in any further
detail. The subﬁeld of COLT which ignores running time complexity issues is
sometimes called statistical learning theory (SLT).
This thesis describes the application of a learning-theoretical result to non-
parametric classiﬁcation (see Chapter 3), but it turns out that in order to un-
derstand the general result and its specialisation to the case of our interest,
most of the mathematical concepts used in learning theory today are not re-
quired at all. Therefore, this introduction to some aspects of learning theory
aims for no more than giving a ﬂavour of the inner workings of PAC and VC
results. Readers familiar with these topics need not spend any time here (they
may want to glance at Section 2.2.1 where our notation is deﬁned). There are
many sources the interested reader can tap for thorough introductions to learning
theory, e.g. [88, 197, 4, 3]. An excellent book about distribution-free analyses of
learning algorithms is Devroye et. al. [49] which we make use of here, see also [68].
The annual COLT conference is a way to get familiar with recent work. It will
become clear that many aspects of SLT are implicit in the wider mathematical
ﬁeld of empirical process theory. For the reader interested in a speciﬁc treatment
48If the target variable is from an inﬁnite set, we can specify a loss function and a threshold
in advance and deﬁne the prediction to be correct iﬀthe loss falls below the threshold.
49The framework can be extended to non-i.i.d. sampling with a known structure, e.g. Marko-
vian, but this is not discussed here.

2.2. Learning Theory
67
for kernel algorithms such as SVM, a number of books is available [161, 72, 37].
The structure of this section is as follows. In Section 2.2.1, we introduce the
PAC framework in the case we are interested in. Section 2.2.2 contains a short
account of concentration inequalities. In Section 2.2.3, we give a brief account
of the notions of VC theory for binary classiﬁcation. Finally, in Section 2.2.4 we
comment on the applicability of PAC results for statistical model selection.
Note that our exposition in parts follows closely the excellent tutorial on
learning theory for classiﬁcation of G´abor Lugosi [105].
2.2.1
Probably Approximately Correct
In this section, we set out to formally deﬁne the framework many COLT prob-
lems are studied in. We restrict ourselves to the binary classiﬁcation problem
introduced in Section A.5, where the training sample is S = {(xi, yi) | i =
1, . . . , n}, xi ∈X , yi ∈{−1, +1}. We also concentrate on the problem of ﬁnding
PAC results for a speciﬁc learning method of interest. Trivially, such a result
gives an upper bound on the sample complexity for a problem. Corresponding
lower bounds can be found typically as well, but this is not in the scope of this
work.
PAC analyses are distribution-free: we assume that S is generated i.i.d. from
some unknown data distribution, but a PAC analysis is not conditioned on any
assumptions about this distribution, its result must hold for any such source.
The variable we are most interested in is the generalisation error gen(S) (see
Section A.5), i.e. the error of the predictor learned from S on a future test case
drawn independently from the data distribution.
The probably approximately
correct (PAC) framework has been introduced by Valiant [195] in the context of
COLT. Suppose that the predictor computed by the learning method is speciﬁed
by some parameter π. A PAC bound is a statement of the following form:
PrS {∃π : gen(π) ≥bound(π, S, δ)} ≤δ.
(2.33)
Here, δ ∈(0, 1) is called conﬁdence parameter.
The upper bound statistic
bound(π, S, δ) is independent of the data distribution and can be computed from

68
Chapter 2. Background
the sample S. Of course, the particular choice of π by the learning method de-
pends on the sample S, and we sometimes write gen(S) and bound(S, δ). The
fact that the bound can only be guaranteed to hold with probability ≥1 −δ over
random draws of S, accounts for the “probably” in PAC. If our learning method
chose a predictor independent of S, the empirical error emp(S) = emp(π, S)
would converge to gen(S) almost surely by the law of large numbers and strong
large deviation inequalities could be used to obtain the rate of convergence (in
fact, large deviation statements are at the heart of PAC theory, and we urge the
reader to glance over our exposition in Section A.7). Therefore, it is reasonable
to write
bound(π, S, δ) = emp(π, S) + gap(π, S, δ),
(2.34)
where the gap bound term gap(π, S, δ) is positive and typically converges to 0 as
n →∞uniformly over π. Note that it is unusual but possible that the range of
values for π (w.r.t. which the ∃is deﬁned in (2.33)) depends on the sample size
n or other statistics. In this thesis, we refer to the variable gen(π) −emp(π, S)
as gap. The “approximately” in PAC accounts for the fact that gap(π, S, δ) is
positive for every sample size n.
Asymptotically, a PAC bound usually implies almost sure convergence50 of a
certain estimator of the generalisation error (e.g., the empirical error) uniformly
over internal choices π of the algorithm, together with an upper bound on the
(uniform) convergence rate, however a PAC statement is somewhat stronger in
that it holds (in the PAC sense) for ﬁnite sample sizes n.
We have already
seen that for a “dumb algorithm” which chooses a predictor independent of the
sample, large deviation inequalities imply a strong PAC bound. For example, if
p = gen, ˆp = emp(S), the Chernoﬀinequality (Theorem A.3) implies that
PrS

DBer[ˆp ∥p] ≥1
n log δ−1, ˆp < p

≤δ.
If the algorithm’s choice is restrict to a ﬁnite set of values for π (sometimes called
50Typically, one employs the Borel-Cantelli lemma to show this (e.g., [67], Sect. 7.3).

2.2. Learning Theory
69
hypothesis space), of size L say, we can use the union bound
Pr
([
i
Ei
)
≤
X
i
Pr {Ei}
(2.35)
in order to obtain a PAC statement:
PrS

DBer[ˆp ∥p] ≥1
n
 log δ−1 + log L

, ˆp < p

≤δ.
If L is very large or the hypothesis space inﬁnite, the union bound cannot be
applied directly, which is where the art of proving PAC bounds begins.
For
example, the classical VC theory (to be discussed in Section 2.2.3) essentially
gives an argument for how the union bound can be applied even in the context of
inﬁnite hypothesis spaces and classiﬁcation problems if these spaces are restricted
in a diﬀerent way.
We now give a rough categorisation of PAC results. Recall the general form
from (2.33) and (2.34). If the gap bound term gap(π, S, δ) is independent of
the concrete choice π (of course, it will usually depend on characteristics of the
hypothesis space), the bound is called uniform. In other words, a uniform PAC
bound deals with variables
sup
 gen(π) −emp(π, S),
where the supremum is over the hypothesis space. The ﬁxation on the empirical
error as only statistic in the bound which depends on the concrete choice of π has
been motivated above with the law of large numbers. In fact, any statistic which
converges almost surely to gen(π) for any ﬁxed π would do, and a class of PAC
bounds indeed focusses on cross-validation error estimates instead of emp(π, S).
Historically, the empirical risk minimisation (ERM) paradigm which requires to
choose a predictor to minimise empirical risk (on the training sample), has been
justiﬁed by uniform gap bounds for the diﬀerence between generalisation and
empirical error. While a uniform bound gives the strongest possible statement
for a ﬁxed hypothesis class, it is argued in Section 3.1.1 that this restriction on
the gap bound term can lead to unnecessarily loose bounds in practice where such
a strong statement is often not required. Non-uniform bounds can be tighter in

70
Chapter 2. Background
practice than uniform ones if they incorporate prior knowledge about the problem
at hand, and if nature is indiﬀerent in the sense that its chosen data distribution
does not “maliciously” contradict these prior assumptions.
If gap(π, S, δ) does not depend on the concrete sample S (apart from simple
statistics such as n = |S|), the gap bound is called data-independent or a priori, as
opposed to data-dependent or a posteriori bounds. This distinction is somewhat
artiﬁcial, since the empirical error in the bound term depends on S. The potential
merits of a posteriori bounds are discussed in Section 3.1.1. The notion of non-
uniform a posteriori bounds has been formalised in the luckiness framework of
Shawe-Taylor et. al. [177].
The statement of a PAC has to be interpreted with some care.51
A PAC
bound is a joint statement over both the generalisation error and the bound term
bound(π, S, δ). Imagine a large number of statisticians being handed a sample S
each drawn independently from the data distribution. Upon training and testing
the method on S and evaluating the bound term, for about 1 −δ of them the
generalisation error of their method will not be smaller than their bound term
value.
This does not rule out the possibility that for example the gap could
be heavily correlated with the empirical error, being large with high probability
whenever emp(π, S) is small. The joint PAC statement could still hold if the
probability for a small empirical error is very small, but we happen to observe
one with our sample. This possibility cannot be ruled out without truly having
access to repeated samples52 or making assumptions about the data distribution.
Phrased diﬀerently, a stronger conditional statement such as
PrS {gen(S) ≥bound(S, δ) | emp(S) ∈I} ≤δ
for a (non-trivial) interval I chosen a priori cannot be directly inferred from a
PAC statement.
51Thanks to Radford Neal for pointing this out.
52In which case it would of course be wasteful not to use them for training.

2.2. Learning Theory
71
2.2.2
Concentration Inequalities
The term concentration inequality touches a large and diverse ﬁeld in empirical
process theory, from fairly elementary large deviation results over Martingale
convergence theorems to deep mathematics. Needless to say, applications of such
inequalities reach far beyond learning-theoretical questions, to which such have
only recently been applied. We are not in the position to provide more than
a brief and incomplete account, borrowing heavily from [106]. The interested
reader may consult [199, 100]. We also neglect to cite results properly, see [106]
for the original references.
Concentration inequalities can be seen as generalisations of laws of large num-
bers. The latter state that under mild conditions, the sum of independent vari-
ables is close to its expectation with high probability. Here, closeness is measured
on the scale of the expectation. This concentration phenomenon is true for wide
classes of general functions of independent variables. The usefulness of a con-
centration result is that the study of a complicated random variable, being a
function of independent ones, can be replaced by studying the expected value
instead which can be much simpler.
One of the simplest concentration results beyond sums is the Efron-Stein
inequality for the variance of Z = g(X1, . . . , Xn), where the Xi are independent
variables. Write
EiZ = E [Z | X1, . . . , Xi−1, Xi+1, . . . , Xn] .
Furthermore, let {X′
i} form an independent copy of the sample {Xi} (a so-called
ghost sample), and let Z′
i = g(X1, . . . , Xi−1, X′
i, Xi+1, . . . , Xn). Then the inequal-
ity asserts
Var[Z] ≤
n
X
i=1
E

(Z −EiZ)2
= 1
2
n
X
i=1
E

(Z −Z′
i)2
.
A proof can be found in [106]. The proof idea borrows from Martingale theory
by considering the Martingale diﬀerence sequence
Vi = E[Z | X1, . . . , Xi] −E[Z | X1, . . . , Xi−1].

72
Chapter 2. Background
Si = E[Z | X1, . . . , Xi] is a Martingale (called Doob’s Martingale, see [67], Chap. 12),
i.e. E[Si+1 | X1, . . . , Xi] = Si, thus E[Vi+1 | X1, . . . , Xi] = 0. Crucially, Z −E[Z] =
P
i Vi and E[Vi Vj] = 0 for i < j, so that
Var[Z] ≤E
" n
X
i=1
V 2
i
#
.
Loosely speaking, there is no “cross-talk” between elements of a Martingale dif-
ference sequence. The rest of the proof makes use of Jensen’s inequality (Theo-
rem A.4) and the convexity of (·)2. Note that the Efron-Stein inequality becomes
an equality for Z being the sum of the Xi, so in this sense sums are the least
concentrated. The inequality can be weakened further by replacing the EiZ by
arbitrary (measurable) functions of X1, . . . , Xi−1, Xi+1, . . . , Xn.
It is useful in
practice if we can guarantee that Z −Z′
i is bounded by ci (say) almost surely,
which is certainly true if the value of g cannot change by more than ci if only the i-
th argument is modiﬁed arbitrarily. For example, if all ci = 1, then Var[Z] ≤n/2,
thus Z is concentrated around E[Z] if the latter is of order n.
A much stronger exponential concentration inequality can be derived starting
from the same bounded diﬀerences assumption on g, this is known as Hoeﬀding’s
inequality (e.g., [67], Sect. 12.3). Namely,
Pr {|Z −EZ| ≥ε} ≤2 exp
 
−2ε2. n
X
i=1
c2
i
!!
.
Let Fi = {X1, . . . , Xi}. Central to the proof is that E[Vi | Fi−1] = 0 and that
conditioned on Fi−1 Vi lies within an interval of size ci. By the convexity of esx
it is not hard to see that
E [exp(s Vi) | Fi−1] ≤e(sci)2/8.
We can now use Chernoﬀ’s bounding technique (see Section A.7), conditioning
on Fi−1, i = n, n −1, . . . in turn and the observation above to ﬁnish the proof.
A large number of non-trivial combinatorial problems lead to variables Z with
bounded diﬀerences. For our purposes, consider
Z = sup
 |emp(π, S) −gen(π)| .
(2.36)

2.2. Learning Theory
73
Then, Z has bounded diﬀerences with ci = 1/n, so is concentrated around its
expected value which can be bounded further using for example VC theory (see
Section 2.2.3).
Hoeﬀding’s inequality can be improved upon along the same lines as its sim-
ple large deviation version by Bernstein’s inequality (a relaxation of Chernoﬀ’s
bound which depends on the average of the variances of Xi and improves upon
Hoeﬀding’s bound if the variances are small), although this is technically quite
complicated. The rough idea is to impose a self-bounding property on g, leading
to Var[Z] ≤EZ. Details of how this can be used to obtain sharper inequali-
ties can be found in [106] and references given there. A whole diﬀerent avenue
is to use information-theoretic concepts and inequalities (such as “conditioning
reduces entropy” and the convexity of relative entropy) instead of the decoupling
by Martingale arguments. For these and other ideas, see [106].
2.2.3
Vapnik-Chervonenkis Theory
According to Section 2.2.2, the task of proving a PAC result can often be de-
composed into showing that the variable quantifying the maximum deviation of
generalisation error from bound term (2.36) is concentrated and bounding its
expectation. The former can be done using Hoeﬀding’s inequality or variance-
dependent reﬁnements. For the latter, the method of Vapnik and Chervonenkis
[196] is classical. In this section, we will motivate the ideas behind their work.
The binary classiﬁcation problem can be generalised as follows. Let X1, . . . , Xn
be i.i.d. variables and A a set of events A. Denote µ(A) = Pr{X1 ∈A} and
ˆµ(A) = n−1 P
i I{Xi∈A}. The variable of interest is
Z = sup
A∈A
|ˆµ(A) −µ(A)| .
It has bounded diﬀerences with ci = 1/n, so by Hoeﬀding:
Pr {|Z −EZ| ≥ε} ≤2e−2nε2.

74
Chapter 2. Background
VC theory allows to bound EZ in terms of combinatorial characteristics of A
even if the latter is inﬁnite. Deﬁne the shatter coeﬃcient by
SA(n) = max
x1,...,xn |{{x1, . . . , xn} ∩A | A ∈A}| ,
i.e. the largest number of subsets we can obtain by intersecting an n-set of points
with all events in A. In the case we are interested in, A is determined by a
classiﬁcation function, A is a hypothesis space and xi contains input and class
label, furthermore xi ∈A iﬀthe classiﬁer determined by A makes a mistake on
xi. Then, SA(n) is the maximum number of diﬀerent labellings that classiﬁers
from A can realise on a n-set of input points. A is the more expressive w.r.t.
n-sets, the larger the shatter coeﬃcient. Theorem 1.9 in [105] shows that
EZ ≤2
p
n−1 log 2SA(n).
We do not give the proof here, but only mention the central ideas. We introduce
a ghost sample {X′
i} as independent copy of {Xi}. First, by Jensen’s inequality
(Lemma A.4),
EZ ≤E

sup
A∈A
|ˆµ(A) −ˆµ′(A)|

,
where ˆµ′ is the empirical measure for {X′
i}. The crucial step is symmetrisation:
introduce Rademacher variables σ1, . . . , σn, i.i.d. and independent of all others,
Pr{σ1 = c} = 1/2, c ∈{−1, +1}. Since every pair Xi, X′
i is i.i.d., c(I{Xi∈A} −
I{X′
i∈A}) has the same distribution for c = −1 and c = +1, thus
E

sup
A∈A
|ˆµ(A) −ˆµ′(A)|

= 1
nE
"
sup
A∈A

n
X
i=1
σi
 I{Xi∈A} −I{X′
i∈A}


#
.
The purpose of introducing the σi is that we can now condition on {Xi}∪{X′
i}
and concentrate on bounding a variable based on the σi only. This is a function
of the variables conditioned on, but it can be “worst-case” bounded independent
of their values by using the shatter coeﬃcient. The key observation is that the
supremum over A can be replaced by a supremum over a ﬁnite representer set of
all possible intersections of A ∈A with values {xi} ∪{x′
i} conditioned on, and
the size of this representer set is bounded above by SA(2n). It is not hard to see

2.2. Learning Theory
75
that this together with a large deviation inequality for the Rademacher variables
leads to the statement above (see [105] for details).
Thus, the central ideas
are: introduction of a ghost sample, symmetrisation with Rademacher variables,
conditioning, replacing A by a ﬁnite representer set, then essentially using the
union bound w.r.t. this set whose size is bounded by the combinatorial shatter
coeﬃcient independent of the random variables. In this sense, the VC technique
allows the application of the union bound under a combinatorial restriction on
the class A. Although A can be inﬁnite, its expressiveness in terms of how many
diﬀerent classiﬁcations it can represent on data is limited. The shatter coeﬃcient
is relevant for deﬁning the “dimension” of A, while its size, dimensionality of
parameter space, etc. are not.
The Shatter coeﬃcient is hard to compute for complicated classes A, but
can be bounded in terms of the VC dimension V . SA(n) is bounded by 2n. If
SA(n) = 2n, we say that A shatters an n-subset of X . As noted above, the shatter
coeﬃcient SA(n) is the maximum number of diﬀerent labellings y = (yi)i we can
impose on any n-subset of X by classiﬁers in A. Note that if SA(n) < 2n that
SA(m) < 2m for all m ≥n, simply because trivially SA(n + 1) ≤2 SA(n). Thus,
if there is some n s.t. SA(n) < 2n, we can deﬁne V ∈N as largest value s.t.
SA(V ) = 2V , thus the size of the largest subset of X which can be shattered.
Otherwise, set V = ∞. As an example, consider a vector space V of functions
g : x →R of dimension m and deﬁne
A = {A(g) | g ∈V},
A(g) = {x | g(x) ≥0}.
Then, V = m which can be seen as follows. For any m + 1 points x1, . . . , xd+1,
the embedding of V into Rm+1 via g 7→(g(xi))i has dimension ≤m, so there
exists γ ∈Rm+1 \ {0} with γT(g(xi))i = 0 for all g ∈V. W.l.o.g. at least one of
the γi is negative. Then, the labelling yi = 2I{γi≥0}−1 cannot be obtained for any
g ∈V. But we can always ﬁnd points x1, . . . , xm s.t. {(g(xi))i | g ∈V} = Rm.
Thus, there exist gj ∈V s.t. (gj(xi))i = δj. Then, a labelling y can be achieved
by g = P
i yigi.

76
Chapter 2. Background
If A has VC dimension V < ∞, Sauer’s lemma states that
SA(n) ≤
V
X
i=0
n
i

≤
ne
V
V
,
the second inequality holds only for n ≥V . It is now clear that if V < ∞, then Z
converges to 0 in probability for any data distribution, and the rate of convergence
can be bounded in terms of the VC dimension. In this case, A is sometimes called
PAC learnable.53 On the other hand, Vapnik and Chervonenkis showed that if
V = ∞, there are data distributions for which Z does not converge to 0 in
probability. Their results imply a method for deciding the uniform consistency of
the ERM paradigm for arbitrary hypothesis spaces A: prove or disprove ﬁniteness
of the VC dimension. A detailed account of VC theory and its application to linear
classiﬁers and SVMs is given in [197].
2.2.4
Using PAC Bounds for Model Selection
Suppose we are given a set {Fα} of hypothesis spaces (or models) and we wish
to select α with the aim that our method (for example, ERM) restricted to Fα
attains small generalisation error. One reason for pursuing this is that the union
∪αFα may be too large a space to avoid the overﬁtting problem (see Section A.5)
if our method is directly applied to it. In principle, a PAC result which holds
for each of the spaces Fα can be used to select a good α, as has been argued for
example in [12]. For a countable set of possible α, we can invoke the multiple
testing lemma (essentially the union bound) to show that minimising the PAC
bound term plus a log factor accounting for the multiple bound application will
select an α close to optimal for large n, see [12] for details. However, it is rather
less clear whether model selection via PAC bounds is justiﬁed for the rather
small sample sizes often encountered in practice, for which common PAC results
frequently cannot give tight guarantees for most values of α.
Deﬁne the slack in a bound as the diﬀerence between bound value and gener-
alisation error. The arguments in [12] are certainly plausible. However, in some
53If we ignore issues of computational complexity.

2.2. Learning Theory
77
COLT publications the virtue of PAC bounds for model selection is suggested for
sample sizes n for which the slack is obviously an order of magnitude above the
generalisation error itself. This is often done by plotting bound values and gener-
alisation error estimates from independent data as curves of α,54 then noting that
the respective minimum points are somewhat close (although the corresponding
minima are not). This can be regarded as an empirical observation, in the same
way as other techniques like cross-validation or marginal likelihood maximisation
are often observed to work ﬁne in practice, but not as much more. An example
is the popular SVM which is often claimed to be “theoretically justiﬁed” by VC
theorems. However, most of the known bounds for SVM are trivial for the region
of sample sizes for which the SVM algorithm can feasibly be run, and model
selection in practice is typically not done by minimising these bounds.
Model selection by bound minimisation in such a case makes sense only if the
slack can be shown to be of much smaller variability (w.r.t. diﬀerent values of α)
than the generalisation error itself. If this were true, then we could infer that the
corresponding minimum points are close. However, it is likely that proving such
robustness of the slack against changing α will be very much harder to show than
a PAC result itself in all but toy situations, or simply does not hold. We are not
aware of any results in this direction. In order to prove a modern PAC result,
typically a host of bounding steps based on very diﬀerent mathematical ideas is
applied and contribute to the slack. While the bound term usually represents a
trade-oﬀbetween good ﬁt to S and complexity of the predictor, the complexity
penalty and the weighting between these terms strongly depends on the bounding
techniques used. It seems very unlikely to us that the resulting slack should not
depend signiﬁcantly on α itself.
54This practice also neglects the multiple testing issue, which does not allow to apply the
bound simultaneously to diﬀerent values of α without accounting for this somehow, but that is
not the point here.

78
Chapter 2. Background

Chapter 3
PAC-Bayesian Bounds for Gaussian
Process Methods
The PAC-Bayesian theorem of McAllester is an unusual result in the ﬁeld of statis-
tical learning theory. While other theorems employ heavy concepts from empirical
process theory and beyond, the PAC-Bayesian theorem can be proved without
much eﬀort, using simple properties of convex functions (one of the contributions
of this chapter is to give such a simple proof). Nevertheless, McAllester’s result
is very powerful, applicable to a large number of Bayes-like learning schemes and
highly conﬁgurable to available task prior knowledge, characteristics which most
other PAC bounds lack to that extent. Maybe because of its simple and direct
proof, the PAC-Bayesian theorem can also be very tight in practically relevant
situations when many other “heavy-weight” theorems struggle to give non-trivial
guarantees. In this chapter, we present various extensions of this remarkable re-
sult, along with a simple proof which points out convex (Legendre) duality as
core principle, in contrast to most other recent PAC results which are based on
the union bound and combinatorics. We then apply the theorem to approximate
Bayesian Gaussian process classiﬁcation, obtaining very tight bounds as judged
by experimental studies. For fairly small sample sizes, the results are highly non-
trivial and outperform other recent kernel classiﬁer bounds we compared against
by a wide margin.
79

80
Chapter 3. PAC-Bayesian Bounds for Gaussian Process Methods
The structure of this chapter is as follows. Section 3.1 is introductory and
stresses the need in practice for bounds which are strongly dependent on learning
method, training sample and task prior knowledge. In Section 3.2 we introduce
several variants of the PAC-Bayesian theorem along with a proof and consider
some extensions. These theorems are applied to Gaussian process classiﬁcation
in Section 3.3.
In Section 3.4, we mention related work, and in Section 3.5
we present experimental results on a handwritten digits recognition task. The
chapter is closed by the discussion in Section 3.6.
Parts of the results presented here have been published previously, as is de-
tailed in Section 1.1.
3.1
Data-dependent PAC Bounds and PAC-Bayesian
Theorems
In this section, we discuss a number of shortcomings of classical Vapnik-Chervo-
nenkis generalisation error bounds w.r.t. tightness in practical applications and
show directions for improvement. We introduce a number of Bayesian types of
classiﬁers. Finally, PAC-Bayesian theorems are motivated as a way to address
the shortcomings of the classical results.
3.1.1
The Need for Data-dependent Bounds
Recall the binary classiﬁcation problem introduced in Section A.5, where the
training sample is S = {(xi, yi) | i = 1, . . . , n}, xi ∈X , yi ∈{−1, +1}. Also
recall the notion of a PAC bound on the generalisation error introduced in Sec-
tion 2.2.1, which provides an estimated upper bound as a function of the training
sample S and a conﬁdence parameter δ, such that the event of the bound being
violated has probability less than δ over random draws of S. Useful PAC bounds
can only be proved in practically relevant situations if the complexity of the range
of classiﬁcation functions in response to ﬁnite training data is limited, e.g. via
regularisation (see Section A.5). Without such limitation, the problem of learning

3.1. Data-dependent PAC Bounds and PAC-Bayesian Theorems
81
from ﬁnite data is ill-posed.
The ideas behind classical VC bounds have been introduced in Section 2.2.3.
It is important to note that the classical theory has not necessarily been devel-
oped with practical applications in mind, but rather to answer the question under
which conditions the principle of empirical risk minimisation (ERM) over a hy-
pothesis space H is uniformly consistent. This question is solved completely by
VC theory: H is “learnable” iﬀits VC dimension is ﬁnite. Only much later have
these results been linked with practically successful schemes such as the SVM
(see Section 2.1.6). However, once applied to real-world situations, reasonable
samples size and non-trivial models, classical VC bounds as well as many more
recent developments have low or zero impact, because the bound values typically
lie orders of magnitude above the truth.
Theoreticians tend to brush away such objections by citing minimax lower
bounds which match the VC upper bounds fairly closely asymptotically. This
argument holds up if we think of a PAC bound as a statement which is written
down once and is then blindly and uniformly applied to any statistical problem
at hand. But it is misleading in the context of real-world statistics, because the
setup of these lower bounds is very diﬀerent from the PAC scenario. Although
a PAC bound holds for all data distributions, its value can depend strongly on
the data distribution. Modern bounds can be orders of magnitude smaller if the
data distribution corresponds to the prior assumptions they encode than if it is
constructed in a malicious way. Practitioners may not be too concerned with
this issue, after all most of statistics and natural science would not work with
nature as a malicious data distribution constructor! But exactly this is the setup
of minimax lower bounds: given a particular statistical method, there are always
some distributions which lead to very slow learning. However, these worst-case
distributions are very unusual1 and constructed in order to exploit weaknesses of
the learning method.
Note that many researchers are more interested in bounding the rate of con-
vergence of the gap as n →∞than in precise gap bounds. The problem with this
1They exhibit characteristics which typically make our model of them (on which a learning
method is based) completely useless.

82
Chapter 3. PAC-Bayesian Bounds for Gaussian Process Methods
is that some insist the rate bounds have to be independent of the data distribu-
tion, thus to be the same for pathetic worst-case distributions than for any other
more sensible distribution (in light of the model used). In order to obtain useful
rate bounds in general (e.g., to show consistency) the model has to be restricted
or regularised appropriately, and ideally theoretical results should guide these
limiting choices in practice. But if a result does not depend on the true source,
it will in general not support the single most important principle in statistics:
obtain as much information as possible about the task and adjust the model to
be compatible with this information. Rather, it will suggest to hedge against
worst-case scenarios by choosing unrealistically simple models. In fact, it will
typically be indiﬀerent to whether we try to encode prior information faithfully
or not, or even worse to vote against such eﬀorts because they might increase our
method’s vulnerability to worst-case scenarios.
Why is it possible to improve on classical VC results in practice?
Recall
that the gap is the diﬀerence between generalisation error and empirical error.
A classical VC theorem will typically be based on a hypothesis space H of ﬁnite
VC dimension and will bound the gap for the worst choice of h ∈H. This bound
will then of course hold for the particular algorithm we really use, but at the
same time it holds just as well for any other method of choosing from H, even for
the “maximally malicious” algorithm which knows the data distribution perfectly
and selects a hypothesis with maximal gap. This may be overly ambitious: in
the words of Vapnik, when solving a given problem, one should avoid solving a
more general problem as an intermediate step [197].
Sometimes it is possible
to shift particularities of the algorithm into the deﬁnition of H, but a better
solution is to consider complexity measures other than the VC dimension (or
scale-sensitive versions thereof) which are speciﬁc to the algorithm used. This
can also alleviate another problem with classical VC bounds, namely that the
gap bound depends on the sample S only via its size n. Recall from Section 2.2.1
that such bounds are called data-independent or a priori, in contrast to data-
dependent or a posteriori bounds. Since for any ﬁxed predictor, the empirical
error converges against the true one almost surely, the empirical error is certainly

3.1. Data-dependent PAC Bounds and PAC-Bayesian Theorems
83
a major “ingredient” for any bound expression, but the limitation to this one and
only statistic is sensible only in the classical setting where ERM alone is to be
analysed. In the bounds we are interested in here, additional statistics are used
to drive data-dependent complexity measures, potentially using more information
in the sample than merely the empirical error and the size. Finally, classical VC
bounds are restricted in how prior knowledge about the task might be encoded
in the bound. This is possible to some degree, by creating a hierarchy2 of nested
hypothesis spaces of growing VC dimension corresponding to a prior based on
Occam’s razor, but the process is very complicated in practice.
To summarise, the VC dimension of a hypothesis space seems unsuitable as a
complexity measure in practice. It is neither ﬂexible nor ﬁne-grained enough and
can hardly be adjusted to algorithms, models and prior knowledge. It also does
not depend on the sample S. These shortcomings have been mentioned before,
and the luckiness framework [177] has been proposed as an alternative.
The
PAC-Bayesian bounds to be discussed in this chapter can be seen as very strong
realisations of this framework, but we will present them within the formally much
simpler and more established framework of Bayesian inference. It is important
to point out that we will not compromise the basic validity of PAC statements in
any way:
• In order to construct a classiﬁcation method and a data-dependent bound
for it, we follow Bayesian modelling assumptions (or other heuristics): avail-
able prior knowledge is encoded, within feasibility constraints, into a prob-
abilistic model and prior distributions, or an algorithm for prediction is
derived in a diﬀerent heuristic way. A distribution-free bound, which will
in general depend on the algorithm, the prior assumptions and the sample
S (beyond just the empirical error) is used to bound the generalisation er-
ror. The extent to which the unknown data distribution is compatible with
these assumptions will in general determine the accuracy of the method
and the observed tightness of the bound, but it does not compromise the
2This structural risk minimisation approach is used to deal with hypothesis spaces of inﬁnite
VC dimension, such as for example SVMs in a feature space.

84
Chapter 3. PAC-Bayesian Bounds for Gaussian Process Methods
validity of the theorem.
• The statement of the bound holds under standard PAC assumptions: We
are given an i.i.d. training sample S from the data distribution which is
otherwise completely unknown. Whether the data distribution is in agree-
ment with the prior assumptions or violates them, does not inﬂuence the
validity of the statement.
3.1.2
Bayesian Classiﬁers. PAC-Bayesian Theorems
Recall the setup of the binary classiﬁcation model deﬁned in Section A.6.1, based
on a family {u(· | w)} parameterised by w and a noise distribution P(y|u).
We assume that the latter has the form P(y|u) = f(y(u + b)), where b is a
bias hyperparameter, and f is symmetric around (0, 1/2): f(−x) = 1 −f(x).
Note that w need not be a ﬁnite-dimensional vector.
For example, in our
application to non-parametric models below we will identify w with the pro-
cess u(· | w) itself. A Bayesian analysis for this model (see Section A.6.2) re-
quires the speciﬁcation of a prior distribution P(w).
If Q(w) is the poste-
rior for the training sample S, the target probability for a new point x∗is
predicted as Q(y∗|x∗, S) = EQ[P(y∗| u(x∗|w))], and the predictive classiﬁer is
sgn(Q(y∗= +1|x∗, S) −1/2).
A number of related classiﬁers have been studied. The Bayes classiﬁer pre-
dicts yBayes(x∗) = sgn(EQ[u(x∗|w)]+b), while the Bayes voting classiﬁer outputs
yVote(x∗) = sgn EQ[sgn(u(x∗|w)+b)]. Note that our terminology is non-standard
here: some authors would refer to our predictive classiﬁer as Bayes classiﬁer,
while others use the term “Bayes classiﬁer” to denote the optimal classiﬁer for
the data distribution. In general, all three classiﬁers (predictive, Bayes, Bayes
voting) are diﬀerent, but if the distribution of u(x∗|w), w ∼Q is symmetric
around its mean for every x∗, then they all agree. Another type of classiﬁer,
called Gibbs classiﬁer, has been studied in learning theory (e.g., [70]). Given a
test point x∗, it predicts the corresponding target by ﬁrst sampling w ∼Q, then
returning y∗= sgn(u(x∗|w)+b), plugging in the sampled parameter vector. Note

3.2. The PAC-Bayesian Theorem for Gibbs Classiﬁers
85
that a Gibbs classiﬁer has a probabilistic element and requires coin tosses for pre-
diction. Note also that if the targets of several test points are to be predicted, the
parameter vectors sampled for this purpose are independent.3 In the situations
we are interested in practice, the Gibbs classiﬁer often performs somewhat worse
than the corresponding Bayes classiﬁer. We will discuss their relationship in more
detail in Section 3.2.5. The Gibbs classiﬁer is the method of choice if for some
reason we are restricted to use a single u(x∗|w) for prediction.
In [116, 115, 117], McAllester proved a number of PAC-Bayesian theorems
applicable to Gibbs classiﬁers. In general, a PAC-Bayesian theorem is simply a
PAC bound which deals with Bayes-like classiﬁers constructed based on expecta-
tions over hypotheses or discriminants with respect to a posterior distribution Q.
It is important to note that Q need not be a Bayesian posterior distribution for
some model, but can be chosen by the learning algorithm at will. McAllester’s
theorem incorporates all directions of improvement mentioned in Section 3.1.1
and suggests a new complexity measure which is data and algorithm-dependent
and can be adjusted based on prior knowledge. More importantly, the measure
is compatible with the aims of Bayesian modelling and prior assessment, so that
the theorem is especially suitable for applications to (approximate) Bayesian al-
gorithms. In the following section, we will present the theorem and a range of
extensions, together with a simple and intuitive proof.
3.2
The PAC-Bayesian Theorem for Gibbs Classi-
ﬁers
In this section, we present and prove a range of PAC-Bayesian theorems for Gibbs
classiﬁers, both for the binary classiﬁcation problem and more general multi-class
scenarios or arbitrary bounded loss functions. A simple extension to binary Bayes
3Readers familiar with Markov chain Monte Carlo methods (see Section A.6.3) will note the
similarity with a MCMC approximation (based on one sample of w only) of the corresponding
Bayes classiﬁer for Q(w). The diﬀerence is that typically in MCMC, the sample representing
the posterior Q(w) is retained and used for many predictions, while in the Gibbs classiﬁer, we
use each posterior sample only once.

86
Chapter 3. PAC-Bayesian Bounds for Gaussian Process Methods
classiﬁers is motivated as well.
3.2.1
The Binary Classiﬁcation Case
In this section, we present McAllester’s PAC-Bayesian theorem for binary clas-
siﬁcation. The theorem deals with a Gibbs classiﬁer (see Section 3.1.2) whose
mixture distribution Q(w) may depend on the training sample S, which is why
Q(w) is referred to as “posterior distribution”. In order to eliminate the prob-
abilistic element in the Gibbs classiﬁer itself, the bound is on the gap between
expected generalisation error and expected empirical error, where the expecta-
tion is over Q(w). The theorem can be conﬁgured by a prior distribution P(w)
over parameter vectors. The gap bound term depends strongly on the relative
entropy (Deﬁnition A.4) D[Q ∥P] between Q(w) and the prior P(w). Here, we
assume that Q(w) and P(w) are absolutely continuous w.r.t. some positive mea-
sure. Recall the Bernoulli relative entropy DBer[q ∥p] from (A.7). It is convex in
(q, p), furthermore DBer[q ∥·] is strictly decreasing for p < q, strictly increasing
for p > q. Thus, the following mapping
D−1
Ber(q, ε) = [pL, pU] s.t. DBer[q ∥pL] = DBer[q ∥pU] = ε, pL ≤q, pU ≥q, (3.1)
is well-deﬁned for q ∈(0, 1) and ε ≥0. We also deﬁne D−1
Ber(0, ε) = [0, 1 −e−ε]
and D−1
Ber(q, ∞) = [0, 1]. D−1
Ber(q, ε) can be seen as “relative entropy ball” of radius
ε around q. Note that due to the convexity of DBer, we can compute the interval
limits of D−1
Ber(q, ε) easily using Newton’s algorithm. It is clear by deﬁnition that
for ε ≥0, p ∈[0, 1]:
p ∈D−1
Ber(q, ε) ⇐⇒DBer[q ∥p] ≤ε.
(3.2)
If δ ∈(0, 1) is a conﬁdence parameter, we have the following result.
Theorem 3.1 (PAC-Bayesian theorem [115]) For any data distribution over
X × {−1, +1}, we have that the following bound holds, where the probability is
over random i.i.d. samples S = {(xi, yi) | i = 1, . . . , n} of size n drawn from the
data distribution:
PrS

gen(Q) ∈D−1
Ber(emp(S, Q), ε(δ, n, P, Q)) for all Q
	
≥1 −δ.
(3.3)

3.2. The PAC-Bayesian Theorem for Gibbs Classiﬁers
87
Here, Q = Q(w) is an arbitrary “posterior” distribution over parameter vectors,
which may depend on the sample S and on the prior P. Furthermore,
emp(S, Q) = E
 ∼Q
"
1
n
n
X
i=1
I{sgn(u(
 i |
 )+b)̸=yi}
#
,
gen(Q) = E
 ∼Q

E(
 ∗,y∗)

I{sgn(u(
 ∗|
 )+b)̸=y∗}

,
ε(δ, n, P, Q) = 1
n

D[Q ∥P] + log n + 1
δ

.
Here, emp(S, Q) is the expected empirical error, gen(Q) the expected generali-
sation error of the Gibbs classiﬁer based on Q(w) (note that the probability in
gen(Q) is over (x∗, y∗) drawn from the data distribution, independently of the
sample S).
Although D−1
Ber is easy to compute, it may be awkward to use in certain sit-
uations. Therefore, DBer is frequently approximated by lower bounds which are
fairly tight if q ≈p. If p ≥q, we have DBer[q ∥p] ≥(p −q)2/(2p) which can
be seen by taking derivatives of both sides w.r.t. q. It follows that if p ≥q and
DBer[q ∥p] ≤ε, then
p ≤q + 2ε +
p
2εq,
leading to
gen(Q) ≤emp(S, Q) + 2ε +
p
2εemp(S, Q),
which shows that the gap bound scales roughly as 2D[Q ∥P]/n if the empirical
error is small.
Needless to say, the use of this additional lower bound is not
recommended in practice.
Note that McAllester’s theorem applies more generally to bounded loss func-
tions and makes use of Hoeﬀding’s inequality for bounded variables. A further
generalisation is shown in Section 3.2.4. However, for the special case of zero-one
loss, we can use techniques tailored for binomial variables which give consider-
ably tighter results than Hoeﬀding’s bound if the expected empirical error of
the Gibbs classiﬁer is small. The theorem can be generalised in various ways.
In Section 3.2.2, we present a multi-class version which encompasses the binary
classiﬁcation case, and the proof given there will serve to prove Theorem 3.1. A

88
Chapter 3. PAC-Bayesian Bounds for Gaussian Process Methods
slightly shorter direct proof can be found in [170]. Complexity measures other
than the relative entropy may be considered, as discussed in Section 3.2.6.1. Fi-
nally, a simple but rather weak extension to the Bayes classiﬁer is given in Section
3.2.5.
We should stress once more, in line with Section 3.1.1, that the PAC-Bayesian
theorem does not require the true data distribution to be constrained in any way
depending on the prior P(w) and the model class. For example, other analyses
(which are not PAC) try to characterise learning curves: given that S has been
generated from the given model and prior, they analyse the generalisation error
of a Gibbs or Bayes classiﬁer, averaged over the data distribution (e.g., [71, 186]).
Or, given that S has been generated i.i.d. from a ﬁxed member w0 of the family,
they derive the convergence rate of some distance between this (product) data
distribution and the Bayesian marginal likelihood EP (
 )[P(S|w)] [30]. It is clear
that under such more restrictive assumptions to start with, stronger results can
be achieved than the PAC-Bayesian theorem.
3.2.2
Confusion Distributions and Multiple Classes
In practice, many classiﬁcation problems come with more than two classes. Al-
though we can always tackle such problems using a suﬃcient number of binary
classiﬁers, it is more principled and data-economic to instead use a multi-class
model (see Section 2.1.2). In this subsection, we assume that the targets to be
predicted are class labels from {1, . . . , C}, C ≥2. Probabilistic rules mapping
input points x to distributions over {1, . . . , C} will also be considered. A rule
π(· |w) is used to predict y∗at a test point x∗by sampling it from the distri-
bution π(x∗|w) over {1, . . . , C}. By restricting ourselves to delta distributions
π(· |w), we can always recover the special case of purely deterministic rules.
The application of a PAC bound is really only a statistical test and as such
has to be embedded into the experimental design.
Even in the binary case
(C = 2), we might be interested in bounding other aspects of the data distri-
bution than the generalisation error, e.g. the probability of false positives, and
as C grows so does the number of possible questions which can be tested based

3.2. The PAC-Bayesian Theorem for Gibbs Classiﬁers
89
on the training sample. Especially in the multi-class case, a useful PAC bound
should support individual designs concerned with more general properties than
the generalisation error, including the latter as a special case.
It is not hard
to extend the PAC-Bayesian theorem in this respect.
We capture the notion
of probabilistic rules by introducing a variable r ∼R(r). r is the source of
random coins required to sample y∗∼π(x∗|w). It is independent of all other
variables, and R is ﬁxed and independent of all other distributions. Eﬀectively,
πy∗(x∗|w) = Pr
  ∼R{y(x∗|w, r) = y∗} for deterministic rules y(x|w, r) mapping
into {1, . . . , C}. Note that the corresponding Gibbs rule based on the distribu-
tion Q(w) is evaluated at x∗by sampling w ∼Q, r ∼R independently, then
outputting y(x∗|w, r). The experimental design now implies a ﬁnite set L of
size L and a distribution p over L which is related to the unknown data dis-
tribution and the posterior distribution Q(w) as follows: if (x∗, y∗) is sampled
from the data distribution, w ∼Q and r ∼R independently, then the variable
l(x∗, y∗, w, r) has distribution p, where l is a known function. For example, if
L = {0, 1} and l(x∗, y∗, w, r) = I{y(
 ∗|
 ,
  )̸=y∗}, then p = (1−eGibbs, eGibbs), where
eGibbs = Pr{y(x∗|w, r) ̸= y∗} is the expected generalisation error of the Gibbs
rule. More generally, we may be interested in the joint confusion distribution
F(y∗, ˜y) = E
 ∗[P(y∗|x∗)E
 ∼Q [π˜y(x∗|w)]] ,
(3.4)
where P(y∗|x∗) denotes the conditional data distribution, for example in order
to bound the probabilities of false positives and true negatives in the case of
binary classiﬁcation. Now, if L = {1, . . . , C} × {1, . . . , C} and l(x∗, y∗, w, r) =
(y∗, y(x∗|w, r)), then p coincides with the joint confusion distribution F.
In
short, a general PAC bound allows us to make inferences about the components
of p based on the observed sample S.
We can now state the following generalisation of the PAC-Bayesian Theorem
3.1 for binary classiﬁcation. Suppose we are given a set L of size L and a map-
ping l(x∗, y∗, w, r) into L, furthermore an arbitrary prior distribution P(w) over
parameter vectors, and we choose a conﬁdence parameter δ ∈(0, 1). Then, the
following result holds.

90
Chapter 3. PAC-Bayesian Bounds for Gaussian Process Methods
Theorem 3.2 (Extended PAC-Bayesian Theorem) For any data distribu-
tion over X × {−1, +1}, we have that the following bound holds, where the prob-
ability is over random i.i.d. samples S = {(xi, yi) | i = 1, . . . , n} of size n drawn
from the data distribution:
PrS {D [ˆp(S, Q) ∥p(Q)] ≤ε(δ, n, P, Q) for all Q} > 1 −δ.
(3.5)
Here, Q = Q(w) is an arbitrary “posterior” distribution over parameter vectors,
which may depend on the sample S and on the prior P. Furthermore, p(Q) is a
distribution over L, induced from the data distribution as follows:
[p(Q)]l∗= Pr(
 ∗,y∗),
 ,
  {l(x∗, y∗, w, r) = l∗} ,
where (x∗, y∗) is drawn from the data distribution, w ∼Q and r ∼R, all inde-
pendently. ˆp(S, Q) is an empirical estimate of p(Q) given by
[ˆp(S, Q)]l∗= 1
n
n
X
i=1
Pr
 ,
  {l(xi, yi, w, r) = l∗} .
Furthermore,
ε(δ, n, P, Q) = D[Q ∥P] + (L −1) log(n + 1) −log δ
n
.
The proof of this theorem is given below in this section. Note that Theo-
rem 3.1 is a special case of Theorem 3.2: it is obtained if we set L = {0, 1} and
l(x∗, y∗, w, r) = I{y(
 ∗|
 ,
  )̸=y∗} with y(x∗|w, r) = sgn(u(x∗|w) + b).
The theorem renders a level ε such that with high conﬁdence 1 −δ we have
D[ˆp ∥p] ≤ε, where ˆp = ˆp(S, Q), p = p(Q) and ε = ε(δ, n, P, Q). In other
words, the unknown vector p ∈RL lies in the closed convex set
P(ˆp, ε) =
n
q
 q ≥0, 1Tq = 1, D[ˆp ∥q] ≤ε
o
.
(3.6)
P(ˆp, ε) can be seen as “relative entropy ball” of radius ε around the centre ˆp,
a multidimensional generalisation of D−1
Ber (see Equation 3.1). By cutting this
ball with planes, we can derive bounds on projections cT p. In Appendix B.1 we
provide an explicit example for how the theorem can be used in practice.

3.2. The PAC-Bayesian Theorem for Gibbs Classiﬁers
91
As one of the major contributions of this chapter, we present a proof of The-
orem 3.2. McAllester’s theorem [117] is a special case of this theorem, yet our
proof is considerably simpler than the original one and leads to several possible
avenues of generalisation. Our bound is also tighter in the special case of binary
classiﬁcation.
Recall the notation introduced further above in this section. For notational
simplicity, we deﬁne ˜w = (w, r), thus pairing the two possible random sources of
the randomised Gibbs rule.4 We also extend prior P and posterior Q by setting
dP( ˜w) = dP(w) dR(r), dQ( ˜w) = dQ(w) dR(r) (product measures). Deﬁne
[p( ˜w)]l∗= E(
 ∗,y∗)

I{l(
 ∗,y∗, ˜
 )=l∗}

,
[ˆp( ˜w)]l∗= 1
n
n
X
i=1
I{l(
 i,yi, ˜
 )=l∗},
l∗∈L,
where the expectation is over the unknown data distribution, and the sample
is S = {(xi, yi) | i = 1, . . . , n}.
Let ∆( ˜w) = D[ˆp( ˜w) ∥p( ˜w)].
∆( ˜w) simply
measures, for a ﬁxed instance ˜w of a ﬁxed rule, the divergence between p( ˜w)
and its empirical estimate ˆp( ˜w) in a convenient way.
Fix ˜w, and write ˆp = ˆp( ˜w), p = p( ˜w). Since ˜w is ﬁxed independent of
the sample S, the strong law of large numbers asserts that ˆp converges against
p almost surely and we can derive a strong large deviation inequality for ∆( ˜w).
We may expect that this guarantee remains valid if instead of ﬁxing ˜w a priori,
we sample it from the prior distribution P( ˜w), because P does not depend on
the sample S either. The ﬁrst part of the proof makes this argument sound. The
reason for our particular choice of ∆( ˜w) is that the corresponding large deviation
inequality is tight and can be proved easily. For ﬁxed ˜w, n ˆp is multinomial (n, p)
distributed. Csisz´ar and K¨orner [42] refer to ˆp as the type of the underlying i.i.d.
sequence {l(xi, yi, ˜w) | i = 1, . . . , n}, and we will use their elegant method of
types (see also [34], Sect. 12.1) for the ﬁrst part of the proof. As is shown in
Appendix B.2, we have
ES

enD[ˆ
 ∥
  ]
≤(n + 1)L−1.
(3.7)
4If the rules (parameterised by w) are deterministic, we set ˜w = w and forget about r
altogether.

92
Chapter 3. PAC-Bayesian Bounds for Gaussian Process Methods
Now, taking the average over ˜w ∼P and using Markov’s inequality (Theo-
rem A.1), we obtain
PrS

E ˜
 ∼P

en ∆( ˜
 )
> (n + 1)L−1
δ

≤δ.
(3.8)
It is of course essential here that P does not depend on the sample S. In other
words, it is easy to prove strong large deviation bounds for “dumb” Gibbs rules
which do not depend on the training sample!
For example, we can use the
concavity of log and the convexity of ∆( ˜w) together with Jensen’s inequality
(Lemma A.4) to see that
D
h
E ˜
 ∼P[ˆp( ˜w)]
 E ˜
 ∼P[p( ˜w)]
i
≤(L −1) log(n + 1) −log δ
n
(3.9)
with probability at least 1−δ over random draws of S. Unfortunately, this is also
quite uninteresting in practice. The cornerstone of the PAC-Bayesian theorem
is a generic way of converting such bounds on “dumb” a priori Gibbs rules into
useful bounds on a posteriori rules (the same method can be applied to Bayes
rules as well, see Section 3.4.1).
Let us take the statement (3.8) for the “dumb” Gibbs rule based on the prior
P and ask what happens if we replace P against the posterior Q. Fix an arbitrary
sample S for which indeed
E ˜
 ∼P

en ∆( ˜
 )
≤K,
K = (n + 1)L−1
δ
.
(3.10)
If we can show that
E ˜
 ∼Q[n ∆( ˜w)] ≤D[Q ∥P] + log E ˜
 ∼P

en ∆( ˜
 )
,
(3.11)
then we have that
E ˜
 ∼Q[∆( ˜w)] ≤D[Q ∥P] + log K
n
.
(3.12)
Note that the relative entropy D[Q ∥P] between Q( ˜w) and P( ˜w) is identical to
the relative entropy between Q(w) and P(w), because the factor R(r) cancels
out in the Radon-Nikodym derivative (recall Deﬁnition A.4).
We use the notion of convex (Legendre) duality (see Section A.3) to prove
(3.11). D[Q ∥P] is convex in Q, and its convex dual is given by the log partition

3.2. The PAC-Bayesian Theorem for Gibbs Classiﬁers
93
function log EP[exp(λ( ˜w))] (see Equation A.8; w has to be replaced by ˜w), where
the dual parameter λ( ˜w) is measurable w.r.t. dP( ˜w). To see this, we only have
to consider λ( ˜w) such that EP[exp(λ( ˜w))] < ∞. For such a candidate, deﬁne
the Gibbs measure
dPG( ˜w) =
eλ( ˜
 )
EP [eλ( ˜
 )] dP( ˜w),
(3.13)
which is a probability measure relative to P( ˜w). The relative entropy is non-
negative (see Section A.3), therefore
0 ≤D[Q ∥PG] =
Z
log
 
EP

eλ( ˜
 )
eλ( ˜
 )
dQ( ˜w)
dP( ˜w)
!
dQ( ˜w)
= D[Q ∥P] + log EP

eλ( ˜
 )
−EQ[λ( ˜w)].
(3.14)
Furthermore, if D[Q ∥P] is ﬁnite, then the density dQ/dP exists, and the in-
equality becomes an equality for λ( ˜w) = log(dQ( ˜w)/dP( ˜w)) + c for any c. Now,
equation (3.11) follows from (A.8) if we use λ( ˜w) = n ∆( ˜w).
We can conclude the proof by noting the convexity of the relative entropy (see
Section A.3) and using Jensen’s inequality. Namely, if (3.12) holds for S, then
D [E ˜
 ∼Q[ˆp( ˜w)] ∥E ˜
 ∼Q[p( ˜w)]] ≤E ˜
 ∼Q [D[ˆp( ˜w) ∥p( ˜w)]]
≤D[Q ∥P] + (L −1) log(n + 1) −log δ
n
.
(3.15)
If we compare this inequality with the inequality (3.9) for the “dumb” classiﬁer
based on P, we see that we have to pay a penalty n−1D[Q ∥P] for replacing the
prior P by the posterior Q. Altogether, since ˆp(S, Q) = E ˜
 ∼Q[ˆp( ˜w)], p(Q) =
E ˜
 ∼Q[p( ˜w)], we can combine (3.8) and the fact that for ﬁxed S, (3.10) implies
(3.15) in order to conclude that (3.5) must hold.
Theorem 3.1 is a special case of Theorem 3.2, and is obtained by setting L =
{0, 1} and l(x∗, y∗, w, r) = I{y(
 ∗|
 ,
  )̸=y∗}. Then, it is easy to see that p(Q) = (1−
gen(Q), gen(Q)), ˆp(S, Q) = (1 −emp(S, Q), emp(S, Q)) and D[ˆp(S, Q) ∥p(Q)] =
DBer[emp(S, Q) ∥gen(Q)]. Theorem 3.1 follows from using (3.2).
3.2.3
Comments
We have seen above in Section 3.2.2 that the proof of the PAC-Bayesian theorem
naturally decomposes into two parts. The ﬁrst part is speciﬁc to the setting and

94
Chapter 3. PAC-Bayesian Bounds for Gaussian Process Methods
consists of proving a large deviation inequality of the style (3.8) for a “dumb”
Gibbs classiﬁer which selects its rule based on the prior P, independent of the
sample S. The second part is generic and quantiﬁes the slack in this inequality
that we have to pay if we want to replace the “dumb” classiﬁer based on P against
the Gibbs rule of interested, based on the posterior Q. This slack is quantiﬁed
directly by the relative entropy D[Q ∥P] between posterior and prior.
Thus,
whenever our learning method holds it necessary to select a posterior distribution
Q which is concentrated on a region deemed very unlikely under P, a high cost
has to be paid in the bound.
The prior distribution P seems to enter the PAC-Bayesian theorem out of
“thin air”, yet its role is central as a parameter which can be tuned a priori to
potentially tighten the bound.5 The choice of P is of course constrained by the
requirement of independence from the sample S. Given that, it should be chosen
to give rise to small D[Q ∥P] for samples S which we believe are likely to be ob-
served for our task. If the PAC-Bayesian theorem is applied to an (approximate)
Bayesian technique, this coincides with the typical Bayesian objective of coding
available task prior knowledge into the prior distribution and the model class.
It is interesting to compare the terms in the PAC-Bayesian theorem with a
frequently used Bayesian model selection criterion: the log marginal likelihood
log P(y) = log
Z
P(y|w) dP(w)
(see Sections A.6.2 and 2.1.3).
If we employ the true posterior in the PAC-
Bayesian theorem, i.e. Q(w) = P(w|y), we have
log P(y) = E
 ∼Q

log P(y|w) dP(w)
dP(w|y)

= E
 ∼Q [log P(y|w)]−D[Q ∥P], (3.16)
therefore
−1
n log P(y) = D[Q ∥P]
n
+ 1
n
n
X
i=1
E
 ∼Q [−log P(yi|xi, w)] .
Thus, the normalised negative log marginal likelihood which is minimised in
Bayesian model selection, is the sum of the complexity term employed in the
5This can be seen as strong instance of the so-called stratiﬁcation technique, although we
average over a continuum of possible ˜w and do not use the union bound (see Section 2.2.1).

3.2. The PAC-Bayesian Theorem for Gibbs Classiﬁers
95
PAC-Bayesian theorem and the sample average of E
 ∼Q[−log P(y|x, w)]. The
latter term, which we refer to as average likelihood, penalises training errors.
Therefore, the log marginal likelihood incorporates a similar trade-oﬀthan the
PAC-Bayesian theorem, using the same complexity term up to log factors. If Q
is just an approximation to the posterior, we see from (2.16) that the r.h.s. of
(3.16) is a lower bound on log P(y). For a very broad model class, the average
likelihood will be small, while for a small model class, the posterior Gibbs rule
will mis-classify more points in the training sample, leading to a larger average
likelihood. However, for a large model class the prior P(w) is necessarily rather
broad and the derivative dQ(w)/dP(w) will be large in the region where the
posterior Q(w) is concentrated, leading to a large complexity term n−1D[Q ∥P].
Put simply, the density ratio between discriminants we consider to be sensible
after and before having seen the data, should increase with growth of the model
class.
We would like to stress that the number of parameters of a model class is
not a sensible measure of complexity. This is fairly obvious, since we can take
any model class and create a new one by adding a large number of parame-
ters which do not or only slightly inﬂuence the rules. Introducing the notion
of “eﬀective number of parameters” helps only if this number is deﬁned un-
ambiguously and can be computed feasibly. For example, the non-parametric
methods we consider in Section 3.3 can be seen as having an inﬁnite number
of parameters, however these parameters are regularised by the prior, and the
conditioning on a ﬁnite amount of data will only render a ﬁnite number of these
parameters any signiﬁcant inﬂuence on predictions.
The complexity measure
D[Q ∥P] behaves correctly in such situations, as the following argument sug-
gests. Let w = (w1, w2), and suppose that w2 has no inﬂuence on the rules, i.e.
P(y|x, w) = P(y|x, w1). Then we have dP(w|y) = dP(w1|y)dP(w2|w1) and
D[P(w|y) ∥P(w)] = D[P(w1|y) ∥P(w1)], thus the complexity measure ignores
w2 and its distribution.
We ﬁnally note that nowhere in the proof of Theorem 3.2 did we require a
union bound (see Section 2.2.1), a distinctive advantage of the PAC-Bayesian

96
Chapter 3. PAC-Bayesian Bounds for Gaussian Process Methods
method.
In a nutshell, while traditional techniques often perform a sort of
“sphere-covering” of a given hypothesis space, then employ some (often loose)
covering number arguments and the union bound, the PAC-Bayesian theorem
employs expectations instead, ﬁrst over the prior P, then changing P for the
posterior Q at the “cost” of n−1D[Q ∥P]. The slack comes from the fact that we
use a linear lower bound to a convex function (see Section A.3).
3.2.4
The Case of General Bounded Loss
We stated and proved the PAC-Bayesian theorem above for the case of zero-one
loss w.r.t. sets of size L. In this subsection, we provide a generalisation to to
general bounded loss functions in Theorem 3.3 below. The proof, which is given
in Appendix B.3, uses a “water-ﬁlling” argument due to [117].
We adopt the notation of Section 3.2.2, but now assume that there is a
bounded loss function l( ˜w, (x∗, y∗)) ∈[0, 1] which quantiﬁes the loss6 one occurs
when using the rule ˜w in order to predict the target corresponding to x∗and the
true target is y∗. For example, the zero-one loss for binary classiﬁcation is given
by l( ˜w, (x∗, y∗)) = I{y(
 ∗| ˜
 )̸=y∗}. We use the notation l( ˜w) = E[l( ˜w, (x∗, y∗))],
where the expectation is over (x∗, y∗) drawn from the data distribution, and
ˆl( ˜w) = n−1 P
i l( ˜w, (xi, yi)). For ﬁxed ˜w, ˆl( ˜w) →l( ˜w) almost surely, and since
l is bounded, the convergence rate is exponential in n. A typical large deviation
inequality (see Section A.7) looks as follows. We have a nonnegative function φ
on [0, 1]2 such that for every ﬁxed ˜w:
Pr
n
ˆl ≥q
o
≤e−nφ(q,l),
q ≥l,
Pr
n
ˆl ≤q
o
≤e−nφ(q,l),
q ≤l,
(3.17)
where l = l( ˜w), ˆl = ˆl( ˜w). We require that φ(q, l) is nondecreasing in |q −l| and
furthermore convex in (q, l), and φ(l, l) = 0 for all l ∈[0, 1]. For simplicity, we
also require that φ(·, l) is diﬀerentiable on (0, l) and (l, 1), for all l ∈(0, 1). We
will give examples for possible inequalities in a moment. Choose some δ ∈(0, 1).
6The notation l(·) used here should not be confused with the notation l ∈L used in subsec-
tion 3.2.2.

3.2. The PAC-Bayesian Theorem for Gibbs Classiﬁers
97
Theorem 3.3 (PAC-Bayesian Theorem, Bounded Loss Functions) For any
data distribution over X ×{−1, +1}, we have that the following bound holds, where
the probability is over random i.i.d. samples S = {(xi, yi) | i = 1, . . . , n} of size
n drawn from the data distribution:
PrS



φ

E ˜
 ∼Q[ˆl( ˜w)], E ˜
 ∼Q[l( ˜w)]

>
1
n−1
 D[Q ∥P] + log 2n+1
δ

for some Q


≤δ. (3.18)
The proof is given in Appendix B.3. Note that the function φ(E ˜
 ∼Q[ˆl( ˜w)], ·)
can be inverted just as easily as DBer above (see equation (3.1)) in order to obtain
upper and lower bounds on E ˜
 ∼Q[l( ˜w)].
If we instantiate this theorem for zero-one loss using Chernoﬀ’s bound, we
essentially obtain Theorem 3.1.
In fact, Chernoﬀ’s bound holds just as well
for general bounded loss and has the form of (3.17) with φ(q, l) = DBer[q ∥l]
(Theorem A.3). The original formulation in [117] uses the Hoeﬀding bound (see
Appendix A.7) which is signiﬁcantly less tight than the Chernoﬀbound if the
expected empirical error is far from 1/2. Other more specialised large-deviation
inequalities can be used instead, and even if they do not come in the form of
(3.17), the proof can probably be adapted. However, note that the constraint on
φ(q, l) to be convex in (q, l) is rather crucial for the last step of the proof. More
speciﬁcally, we have to upper bound φ(EQ[ˆl( ˜w)], EQ[l( ˜w)]) in terms of the bound
on EQ[φ(ˆl( ˜w), l( ˜w))], which is trivial if φ is convex.
3.2.5
An Extension to the Bayes Classiﬁer
In practice, when comparing Gibbs and Bayes classiﬁer for the same posterior
distribution Q(w) directly, it turns out that the Bayes variant often performs
better than the Gibbs variant (the latter is hardly used in practice for this reason).
In this section, we will have a closer look on their relationship and suggest a simple
extension of the PAC-Bayesian theorem for binary Bayes classiﬁers.
We have introduced Gibbs, Bayes and Bayes voting classiﬁers in Section 3.1.2.
Recall that throughout this thesis we assume that the noise model is symmetric
in the sense that it is a function of yu: P(−y|u) = P(y| −u). For simplicity, we

98
Chapter 3. PAC-Bayesian Bounds for Gaussian Process Methods
assume in this section that if a bias parameter b is used, it has already been added
to u. In general, the Bayes, Bayes voting and predictive classiﬁer are all diﬀerent,
but we will be interested exclusively in the case that the distribution of u(x | w)
induced by Q is symmetric around its mean for every x. In this case, all Bayes
classiﬁer variants agree. In fact, it is easy to see that for every nondecreasing f(u)
with f(−u) + f(u) = τ, the classiﬁers sgn(EQ[f(u(x∗| w))] −τ/2) are identical.
Note that if this assumption is violated, the concepts of Gibbs, Bayes and Bayes
voting classiﬁers become questionable anyway and the predictive classiﬁer should
be used. Fortunately, all (approximate) Bayesian methods discussed in this thesis
fulﬁl this assumption. Deﬁne the errors
eGibbs(x∗, y∗) = EQ[I{sgn u(
 ∗|
 )̸=y∗}],
eBayes(x∗, y∗) = I{sgn EQ[u(
 ∗|
 )]̸=y∗}.
Furthermore, for A ∈{Gibbs, Bayes}, deﬁne eA = E(
 ∗,y∗)[eA(x∗, y∗)] where the
expectation is over the data distribution. Intuitively, the Bayes classiﬁer should
outperform the Gibbs variant if the trained model represents the data distribu-
tion well. Assume for now that the data distribution is identical to the model-
generative one: for given x∗, y∗∼P(y∗| u(x∗| w)), w ∼Q. Condition on x∗
and write u = u(x∗| w) = ¯u + v, where ¯u = EQ[u] and v has an even density
function. For simplicity, we write u ∼Q, v ∼Q meaning the corresponding
distributions induced by Q over w (for ﬁxed x∗). Consider sampling u1, u2 ∼Q
independently, furthermore y2 ∼P(y2|u2). Both Gibbs and Bayes classiﬁer err if
sgn u1 = sgn ¯u, y2 ̸= sgn u1, but if sgn u1 ̸= sgn ¯u, they diﬀer depending on the
relationship between y2 and sgn ¯u. If E = {sgn u1 ̸= sgn ¯u}, then
eGibbs −eBayes = E
 ∗E
h
Pr{E} (Pr {y2 = sgn ¯u} −Pr {y2 ̸= sgn ¯u})
 x∗
i
.
Pr{E} is the probability of the tail {v ≥|¯u|} under Q. Note that we have used
that the event {y2 = sgn ¯u} is independent of E, given x∗.
The conditional
diﬀerence of the errors is positive if sgn ¯u ̸= 0, showing that the Bayes classiﬁer
outperforms the Gibbs variant in this situation. On the other hand, it is easy to
construct a “malicious” setting in which the Gibbs classiﬁer does better than the

3.2. The PAC-Bayesian Theorem for Gibbs Classiﬁers
99
Bayes variant (but see below), but recall from Section 3.1.1 that the relevance of
such arguments may be limited in practice.
If nothing is known about the data distribution, we can relate eBayes and eGibbs
in a coarser sense, by noting that if eBayes(x∗, y∗) = 1, then eGibbs(x∗, y∗) ≥1/2,
thus eBayes ≤2 eGibbs (as remarked in [72], Lemma 5.3).7 Therefore, Theorem 3.1
applies to the Bayes classiﬁers as well.
Although we obtained this extension
without eﬀorts, it is not really what we are ideally looking for. First, the bound on
the Bayes classiﬁer error is certainly over-pessimistic.8 Second, the generalisation
error bound on the Bayes classiﬁer is in terms of the expected empirical error of
the Gibbs classiﬁer: even if we prefer the Bayes classiﬁer in practice, we have to
evaluate its Gibbs variant in order to obtain performance guarantees. Third, the
argument does not carry through to more than two classes. Very recently, Meir
and Zhang [121] obtained a PAC-Bayesian margin bound for the Bayes voting
classiﬁer, combining a new inequality based on Rademacher complexities with
the convex duality step. We discuss this result in Section 3.4.1. However, the
Meir/Zhang result can be criticised on other grounds (see Section 3.4.1.1) and did
not render non-trivial guarantees in our experiments (see Section 3.5.6), while the
Gibbs theorem certainly did. In light of practical evidence, the aim is to provide
a PAC-Bayesian theorem for Bayes-type classiﬁers which is at least as tight as the
Gibbs theorem in practically relevant situations, and at least to our knowledge
this remains an open problem.
3.2.6
Some Speculative Extensions
Here, at the end of Section 3.2 we take the liberty of collecting some more or
less speculative thoughts which are not driven to a deﬁnite conclusion. In Sec-
tion 3.2.6.1, we note that in principle the complexity measure n−1D[Q ∥P] could
be replaced by other such measures, as long as they are convex in Q for every P.
In Section 3.2.6.2 we show that the essential slack in the PAC-Bayesian bound
7Even without any assumption on the distributions of u(x∗| w), this is true for Bayes voting
and Gibbs classiﬁer.
8One can construct situations to show that it is tight in principle, but recall from Section 3.1.1
that such “tightness” arguments may be misleading in practice.

100
Chapter 3. PAC-Bayesian Bounds for Gaussian Process Methods
can be written down explicitly. Gaining knowledge about the behaviour of slack,
especially about its degree of dependence on hyperparameters may be a key issue
if PAC bounds are to be used for model selection in practice.
3.2.6.1
Generalisation to Other Complexity Measures
The nature of the proof given in Section 3.2.2 allows us to think about further
generalisations.
Recall that the key for transforming the uninteresting bound
(3.9) into the statement of the PAC-Bayesian theorem is the characterisation
(A.8) of the relative entropy D[Q ∥P] in terms of its convex dual (see Section A.3).
Suppose that d(P, Q) is any divergence measure between P and Q which is convex
in Q for all P. Then, there exists a convex dual g(P, λ) such that
d(P, Q) = max
λ( ˜
 ) (E ˜
 ∼Q [λ( ˜w)] −g(P, λ)) ,
and g(P, λ) is itself convex in λ for every P. For a general divergence measure
d(P, Q), the following program may be feasible to entertain. First, make sure that
d(P, Q) is convex in Q for every P. Second, determine its convex dual g(P, λ).
Third, prove a large deviation bound of the form
PrS

eg(P,n ∆( ˜
 )) > K
δ

≤δ,
(3.19)
where K is polynomial in n. This would serve as equivalent for (3.9), and the
generic part of the proof could be followed in order to obtain a PAC-Bayesian
theorem which asserts that
D [E ˜
 ∼Q[ˆp( ˜w)] ∥E ˜
 ∼Q[p( ˜w)]] ≤d(P, Q) + log K −log δ
n
with probability at least 1 −δ over draws of S. The feasibility of this approach
depends on several factors. Of course, d(P, Q) must be convex in Q and itself
feasible to compute.
Then, we have to determine the convex dual g(P, λ) in
closed form, or at least g(P, n ∆). Second (and probably most diﬃcult) we have
to prove the bound (3.19) for our prior at hand. Note that we are not necessarily
constrained to use the particular divergence ∆( ˜w) as deﬁned in subsection 3.2.2:
the latter was chosen for convenience w.r.t. proving (3.9). However, ∆( ˜w) has to

3.2. The PAC-Bayesian Theorem for Gibbs Classiﬁers
101
be convex in ˜w for every sample S. We have not yet pursued this program for
any divergence d(P, Q) other than the relative entropy.
3.2.6.2
The Slack Term in the PAC-Bayesian Theorem
Several authors have suggested to minimise PAC upper bounds in order to do
model selection.
In Section 2.2.4, we argue that this is theoretically justiﬁed
only if one can prove that the slack in the bound, i.e. the diﬀerence between
bound value and true generalisation error, is signiﬁcantly less variable w.r.t. hy-
perparameters to be selected (over a range of interest) than the generalisation
error itself. It is not even straightforward to formalise this requirement in the
strict PAC sense, and proving it will most probably be much harder than the
bound itself. Nevertheless, the phenomenon occurs in practice on non-trivial real
world examples (see Section 3.5.5), and it would be very valuable to obtain some
analytical results in order to understand it at least on toy models.
An interesting consequence of the simple proof of the PAC-Bayesian Theo-
rem 3.1 given in Section 3.2.2 is that we can essentially write down the slack
analytically. For simplicity, we deal with deterministic rules only, i.e. ˜w = w.
Inspecting the proof, we see that there are three sources of slack: the initial ar-
guments leading to (3.8), the plugging in of n∆(w) for λ(w) in (A.8) and the
ﬁnal application of Jensen’s inequality in (3.15). The bound in (3.8) leads to a
typically minor contribution to the PAC-Bayesian gap bound, and the ﬁnal ap-
plication of Jensen’s inequality should be fairly tight if the posterior Q is rather
concentrated,9 leaving us with the typically dominating slack coming from the
“wrong” choice for λ(w) in (A.8).
From (3.14) we see that this slack in the
right hand side ε(δ, S, P, Q) in (3.5) is D[Q ∥PG], where PG is the Gibbs measure
deﬁned in (3.13). Suppose that Q(w) has the form
dQ(w) = Z−1e
 n
i=1 φi(
 )dP(w),
Z = E
 ∼P
h
e
 n
i=1 φi(
 )i
,
where φi(w) = φ(xi, yi; w). For example, the true Bayesian posterior is obtained
9This could be tested using random sampling for a particular architecture.

102
Chapter 3. PAC-Bayesian Bounds for Gaussian Process Methods
if φ(xi, yi; w) = log P(yi|xi, w). Then,
D[Q ∥PG] = E
 ∼Q

log dQ(w)
dPG(w)

=
n
X
i=1
E
 ∼Q [φi(w) −∆(w)] −log E
 ∼P

e
 i φi(
 )
E
 ∼P [en∆(
 )] .
We can also write
D[Q ∥PG] = n E
 ∼Q [∆(w)] −log E
 ∼P

en∆(
 )
−H[Q(w)].
Admittedly, these expressions are not very useful per se, expect for showing that
the slack will be small if ∆(w) is close to most of the φi(w).
3.3
Application to Gaussian Process Classiﬁcation
In this section, we apply the PAC-Bayesian Theorem 3.1 to a wide class of Gaus-
sian process models for binary classiﬁcation. The class is deﬁned in Section 2.1.3,
and in Section 3.3.1 we show how the bound terms can be computed for any
member. In Sections 3.3.2, 3.3.3 and 3.3.4, we give speciﬁc examples for some
well-known GP approximations (see Section 2.1.3).
3.3.1
PAC-Bayesian Theorem for GP Classiﬁcation
In this section, we specialise the PAC-Bayesian Theorem 3.1 to Gaussian pro-
cess models for binary classiﬁcation, incorporating a wide class of approximate
inference methods. The GP binary classiﬁcation model has been introduced in
Section 2.1.2 (both the logit and probit noise model discussed there satisfy the
symmetry condition required here). Recall that K ∈Rn,n denotes the covariance
matrix evaluated over the training input points {xi}, and u = (ui)i ∈Rn are the
latent outputs at these points. This non-parametric model can be seen as special
case of the scenario of Section 3.1.2 with w ≡u(·), i.e. the “weights” are the
complete latent process, and u(x|w) ≡u(x). Alternatively, we could develop
the process u(x) in an eigensystem of the covariance kernel K and parameterise

3.3. Application to Gaussian Process Classiﬁcation
103
it in terms of a countable number of weights (the “weight space view” of Sec-
tion 2.1.1), however the presentation in the “process view” turns out to be much
simpler.
The general class of approximation methods we are interested in here is deﬁned
in Section 2.1.3. The posterior P(u|S) is approximated by a Gaussian Q(u|S) of
the general form (2.12). For most schemes, the covariance matrix A of Q(u|S) is
further restricted to the form (2.13), thus the approximation is deﬁned in terms
of K and further O(d) parameters, d ≤n. Then, the approximate predictive
distribution is Gaussian with mean and variance given in (2.14). In order to
apply the PAC-Bayesian theorem to this case, we only have to show how to
compute the terms deﬁning the gap bound value: the expected empirical error
and the relative entropy D[Q ∥P]. The former is just the empirical average over
eGibbs(x∗, y∗) = Pru∗∼Q(u∗|
 ∗,S){sgn(u∗+ b) ̸= y∗} = Φ
−y∗(µ(x∗) + b)
σ(x∗)

,
(3.20)
where Φ(·) denotes the c.d.f. of N(0, 1). The relative entropy D[Q ∥P] has been
determined in (2.11). Using (A.17), we obtain
D[Q ∥P] = 1
2

log
A−1K
 + tr
 A−1K
−1 + ξTKIξ −n

.
(3.21)
If A is of the special form (2.13), this simpliﬁes to
D[Q ∥P] = 1
2
 log |B| + tr B−1 + ξTKIξ −d

,
(3.22)
where B is deﬁned in (2.14). These formulae depend on the generic parameters
ξ and A (or I and D) of the posterior approximation Q(u|S). In Sections 3.3.2
and 3.3.3, we show how to compute the relative entropy for a range of concrete
GPC approximations. A simple way to compute (3.22) is to use the Cholesky
decomposition B = LLT (see Appendix A.2.2). Then, log |B| = 2 log | diag2 L|,
and tr B−1 can be computed from L using the same number of operations as
required for B →L.
We can now simply plug in (3.21) or (3.22) and (3.20) into the terms of
Theorem 3.1 in order to obtain a PAC-Bayesian theorem for GP binary Gibbs
classiﬁers. It is important to note that for this theorem to be valid, the prior

104
Chapter 3. PAC-Bayesian Bounds for Gaussian Process Methods
P and the model have to be ﬁxed a priori, i.e. free hyperparameters of the co-
variance function K or the noise model P(y|u) have to be chosen independently
of the training sample S.
For example, the widely used practice of choosing
such parameters by maximising an approximation to the log marginal likelihood
log P(S) (see Sections 2.1.3 and A.6.2) is not compatible with using the theorem
for a statistical test. It is easy to modify Theorem 3.1 to allow for model selec-
tion amongst a ﬁnite set (size polynomial in n) of hyperparameter candidates,
by a straightforward application of the union bound (see Section 2.2.1). This
introduces a O(log n) term in the numerator of ε(δ, n, P, Q) in (3.3). However,
choosing the hyperparameters by continuous optimisation is not admissible.
Note also that Theorem 3.2 in principle applies to multi-class GP methods
(see Section 2.1.2). As mentioned there, the implementation of such methods
is, however, quite involved, and for most methods, additional approximations
are required to avoid a scaling quadratic in the number of classes. In terms of
Theorem 3.2, the computation of ˆp(S, Q) may not be analytically tractable, and
special approximations may have to be considered in order not to compromise
the bound statement. This program is subject to future work.
3.3.2
Laplace Gaussian Process Classiﬁcation
Let us specialise the generic framework of the previous section to Laplace GP
binary classiﬁcation [210], discussed brieﬂy in Section 2.1.3. We end up with a
Gaussian posterior approximation with mean ξ and covariance A = (K−1+D)−1,
where ξ, D depend on the posterior mode ˆu via (2.15). Note that in order to
evaluate the Gibbs classiﬁer, the predictive variance σ2(x∗) has to be computed,
while the (approximate) Bayes classiﬁer is sgn(µ(x∗) + b), independent of σ2(x∗)
(see Equation 2.14).10 The downside of this is that the Gibbs classiﬁer requires
O(n2) for each evaluation, while the Bayes classiﬁer is O(n) if an uncertainty esti-
mate is not required. We show in Appendix B.4 how to evaluate the Gibbs classi-
10Recall from Section 3.1.2 that since Q(u∗|x∗, S) is symmetric around µ(x∗), Bayes, Bayes
voting and predictive classiﬁers predict the same target. The independence of these classiﬁers of
σ2(x∗) could be interpreted as weakness of the Gaussian approximation, since the true posterior
for u∗can be skew. Error bars for the classiﬁers do of course depend on the predictive variance.

3.3. Application to Gaussian Process Classiﬁcation
105
ﬁer more eﬃciently using sparse approximations together with rejection sampling
techniques, resulting in O(n) (average case) per prediction.
Experimental results are given in Section 3.5.
We can gain some insight
into the gap bound value by analysing the relative entropy term D[Q ∥P] (see
Equation 3.22) in Theorem 3.1, as applied to Laplace GPC. In normal situations
(i.e. δ not extremely small), the expression ε(δ, n, P, Q) in (3.3) is dominated by
this term.
−2
−1
0
1
2
3
4
−2
−1.5
−1
−0.5
0
0.5
margin x
f(x) = x sigma(−x)
x*
Figure 3.1: Relation between margin and gap bound part
We assume that P(y|u) is logit noise. First, use (2.15) to see that ξTKξ =
ˆuTξ = P
i yiˆuiσ(−yiˆui) = P
i f(yiˆui), where f(x) = x σ(−x) and yiˆui is the
margin11 at example (xi, yi). f(x) is plotted in Figure 3.1. It is maximal at x∗≈
1.28, converges to 0 exponentially quickly for x →∞and behaves like x 7→x for
11The margin is a learning-theoretical concept frequently used in the context of PAC bounds
for averaging (Bayes) classiﬁers (see for example Section 3.4.1).

106
Chapter 3. PAC-Bayesian Bounds for Gaussian Process Methods
x →−∞. Thus, at least w.r.t. the third term in (3.22), classiﬁcation mistakes (i.e.
yiˆui < 0) render a negative contribution to the gap bound value. This is what we
expect for a Bayesian architecture. Namely, the method choses a simple solution,
at the expense of making this mistake, yet with the goal to prevent disastrous
over-ﬁtting. In our bound, we are penalised by a higher expected empirical error,
but we should be rewarded by a smaller gap bound term. Now to the remaining
terms in (3.22). The matrix B = I + D1/2KD1/2 is positive deﬁnite, and all
its eigenvalues are ≥1. Furthermore, by taking any unit vector z and using the
Fisher-Courant min-max characterisation of the eigenvalue spectrum of Hermitian
matrices (e.g., [78], Sect. 4.2), we have zT Bz = 1 + (D1/2z)TK(D1/2z) ≤
1 + λmaxzTDz < 1 + λmax/4, where λmax is the largest eigenvalue of K. Here,
(2.15) implies that the coeﬃcients of diag D are all < 1/4. Thus, all eigenvalues
of B lie in (1, 1 + λmax/4). By analysing (1/2) log |B| + (1/2) trB−1 for general12
B ⪰I, using a spectral decomposition of B, we see that this term must lie
between n/2 and (n/2)(log(1+λmax/4)+(1+λmax/4)−1), although these bounds
are not very tight.
3.3.3
Sparse Greedy Gaussian Process Classiﬁcation
In practice, the applicability of Gaussian process or other kernel methods is often
severely restricted by their unfortunate scaling: O(n3) time for ﬁrst-level infer-
ence, O(n2) space. Sparse approximations to GP models can improve vastly on
these ﬁgures and are therefore of considerable practical importance. In Chap-
ter 4, we discuss sparse approximations in detail and present a range of diﬀerent
schemes for approximate inference. All of these lie within the class described in
Section 2.1.3, so that Theorem 3.1 applies to their Gibbs classiﬁer versions. In
this section, we focus on the simple IVM scheme introduced in Section 4.4.1, the
most eﬃcient of the algorithms discussed in this thesis.
The parametric representation for IVM is described in Section 4.4.1 and Ap-
pendix C.3.1. In terms of the placeholders of Section 2.1.3 we have D = ΠI,
12B ⪰I means that B −I is positive semideﬁnite. See [24] for details about such generalised
inequalities.

3.3. Application to Gaussian Process Classiﬁcation
107
where I is the active set, furthermore ξ = I·,IΠ1/2
I L−Tβ. The relative entropy
term is given by
D[Q ∥P] = 1
2

log |B| + tr B−1 + ∥β∥2 −
L−Tβ
2 −d

.
The predictive distribution Q(u∗|x∗, S) is derived in Section 4.4.1. An evaluation
of the Gibbs classiﬁer costs O(d2) (one back-substitution with L), while the com-
putation of D[Q ∥P] costs O(d3). The expected empirical error can be computed
in O(n d2).
As discussed in Section 4.3, the non-sparse equivalent of the IVM (i.e. I =
{1, . . . , n}) is the cavity TAP method of [139], and Theorem 3.1 can be applied
to this approximation using the formulae given here. Also note that although
the IVM is a compression scheme (in the sense deﬁned in Appendix B.6), the
PAC-Bayesian theorem is not a compression bound: it holds for sparse non-
compressing methods (such as PLV, see Section 4.4.2) just as well. We will see
in Section 3.5.4 that the PAC-Bayesian theorem can be signiﬁcantly tighter in
practice than a standard PAC compression bound when applied to the same
compression scheme.
3.3.4
Minimum Relative Entropy Discrimination
The MRED framework as probabilistic interpretation of large margin (discrim-
ination) methods such as SVM has been introduced in [83] and is discussed in
Section 2.1.6. The formulation allows a direct application of the PAC-Bayesian
theorem 3.1 to Gibbs version of the corresponding classiﬁers.
Recall that the prior process P and the MRED “posterior” process Q have
the same covariance, thus
D[Q ∥P] = 1
2ξTKξ = 1
2λTY KY λ.
Therefore, the application of Theorem 3.1 to MRED is straightforward. However,
note that the Gibbs variant of the MRED classiﬁer is typically not a very useful
method in practice, due to the failure of MRED (and large margin methods in
general) to provide non-trivial estimates of predictive variance. The predictive

108
Chapter 3. PAC-Bayesian Bounds for Gaussian Process Methods
variance of the latent variable u∗is the same as the corresponding prior variance
which can be large, thus the Gibbs variant is likely to perform much worse than
the Bayes (averaging) one for points close to a decision boundary even if they lie
close to training points.
3.4
Related Work
In this section, we mention some closely related work. Our focus is on a result
very recently proved in [121], giving a PAC-Bayesian theorem for Bayes voting
classiﬁers, as opposed to Theorem 3.1 which holds for Gibbs classiﬁers. In Sec-
tion 3.4.1.2, we show a possible route towards extending this result to the case of
GP regression.
The literature on PAC bounds for kernel methods is large and will not be
reviewed here (the reader may consult [72, 161, 197]). We provide a brief in-
troduction to PAC bounds in Section 2.2. Shawe-Taylor and Williamson [176]
present a PAC analysis of a Bayesian estimator. The notion of PAC-Bayesian
theorems has been developed by McAllester [116, 115, 117] who also gave a proof
of a slightly less tight version of Theorem 3.1. The theorems in [116] can be
seen as special case for a restricted class of “cut-oﬀposteriors” (where the log
likelihood is an indicator function). McAllester’s theorems have been used in a
number of later papers. Herbrich and Graepel [73] use the theorems in [116] to
obtain a margin bound on SVM. Seeger, Langford, and Megiddo [172] combine
the “sampling” approach from [159] with the PAC-Bayesian theorem to obtain a
Bayes classiﬁer version. Langford and Caruana [95] apply McAllester’s theorem
to multi-layer perceptrons. Langford and Shawe-Taylor [94] provide another ex-
tension of Theorem 3.1 to Bayes classiﬁers, however with the same drawbacks as
the one in Section 3.2.5. The interesting feature of this work is that it shows how
to apply the PAC-Bayesian theorem to averaging (Bayes) classiﬁers which do not
provide estimates of predictive variances. A similar idea has been proposed in
[73], however with an unfortunate dependence on the dimensionality of the weight
space. There is a considerable literature on PAC bounds for combined (mixture)

3.4. Related Work
109
classiﬁers and multi-layered classiﬁers. Recent results are given by Koltchinskii
and Panchenko [91] where other references can be found, see also [159].
3.4.1
The Theorem of Meir and Zhang
The PAC-Bayesian Theorem 3.1 holds for Gibbs classiﬁers only, while in practice
Bayes or Bayes voting classiﬁers are used much more frequently, due to their
typically better performance. Recently, Meir and Zhang [121] presented a PAC-
Bayesian margin bound which applies to Bayes voting classiﬁers (recall that for
the approximations we are interested in, Bayes and Bayes voting classiﬁers are
identical, see Section 3.1.2). They obtain their result by combining a recent bound
proved in [91] with a convex duality step identical to the second part of our proof.
In this section, we present their result and compare it with the PAC-Bayesian
Theorem 3.1. In fact, we derive a slightly diﬀerent theorem which is closer to the
relevant Theorem 2 in [91].
Meir and Zhang [121] consider functions f(x|Q) = E
 ∼Q[y(x|w)], y(x|w) =
sgn(u(x|w) + b). Allowing for the choice of a prior P, they deﬁne
QA =

Q
 D[Q ∥P] ≤A
	
, FA =

f(x|Q)
 Q ∈QA
	
.
Note that Meir and Zhang claim that their theorem holds for arbitrary classes FA,
while based on the theorems in [91], we were only able to reproduce their result
under the additional condition13 that all FA are closed under multiplication with
−1. In other words, we require that for every A > 0 and every f ∈FA : −f ∈FA.
Similar to structural risk minimisation bounds [197], they use a nested hierarchy
{FAj} deﬁned a priori via an unbounded sequence A1 < A2 < . . . together with
a distribution (pj)j over N. Finally, let φ(x) be the function which is equal to
1 −x in [0, 1], constant 1 for x ≤0 and constant 0 for x ≥1. Now, choose a
δ ∈(0, 1) and a c > 1.
Theorem 3.4 ([121]) For any data distribution over X × {−1, +1} we have
that the following bound holds, where the probability is over random i.i.d. samples
13The problem will be removed in a longer version of their paper (Tong Zhang, personal
communication).

110
Chapter 3. PAC-Bayesian Bounds for Gaussian Process Methods
S = {(xi, yi) | i = 1, . . . , n} of size n drawn from the data distribution:
PrS
(
Pr(
 ∗,y∗) {y∗f(x∗|Q) ≤0} >
inf
κ∈(0,1] B(κ, S, Q)
+
r
log(2/(pj(Q)δ))
2n
−log log c
n
for some Q
)
≤δ.
Here,
B(κ, S, Q) = 1
n
n
X
i=1
φ (yif(xi|Q)/κ) + 4 c
κ
r
2 Aj(Q)
n
+ log log(2/κ)
n
and j(Q) = min{j | D[Q ∥P] ≤Aj}.
The proof of the theorem is given in Appendix B.5. Note that the search for a
minimiser κ is equivalent to an optimisation w.r.t. margin loss functions φ(·/κ).
Both c > 1 and the hierarchy (Aj, pj) have to be chosen a priori. As long as they
remain within sensible limits, the eﬀect of their precise choice is insigniﬁcant: the
bound value is typically dominated by the ﬁrst two terms in B(κ, S, Q). We may
chose c = 1.01, Aj = j n (∆A), pj = 1/(j(j + 1)), where ∆A > 0 is a grid size.
3.4.1.1
Comparing the PAC-Bayesian Theorems
Theorem 3.4 is quite diﬀerent in form from Theorem 3.1 and the corresponding
statement for the Bayes classiﬁer mentioned in Section 3.2.5. A direct analytical
comparison is diﬃcult, because we would expect both theorems to show their mer-
its only in “lucky” cases (which frequently occur in practice). In this section, we
give some comparative arguments, pointing out a serious lack in tightness in the
result of Meir and Zhang. An empirical comparison is presented in Section 3.5.6.
We are interested in the GP binary classiﬁcation situation. Let ri = (µ(xi) +
b)/σ(xi). From (3.20) we know that
ˆeGibbs = 1
n
n
X
i=1
Φ(−yiri).
It is also easy to see that f(xi|Q) = 2Φ(ri) −1, so that
1
n
n
X
i=1
φ
 κ−1yif(xi|Q)

= 1
n
n
X
i=1
φ
 κ−1(2Φ(yiri) −1)

.

3.4. Related Work
111
The Bayes classiﬁer makes a mistake whenever yiri ≤0, in which case we have
φ(κ−1(2Φ(yiri) −1)) = 1 independent of κ, while the contribution to the Gibbs
empirical error can still be close to 1/2 if |ri| is small.
The reason why the
Bayes variant often outperforms the Gibbs variant can only lie with patterns
for which |ri| is fairly small, because Gibbs will with high probability make the
same prediction as Bayes if |ri| ≫0. In these situations, among the patterns
with small |ri| there are signiﬁcantly more “on the right side”, i.e. yiri > 0. The
contribution to the margin loss is smaller than the one to the Gibbs error once
Φ(yiri) > 1/(2−κ), and in this region the margin loss drops to zero quickly (slope
κ−1) while the Gibbs contribution is ≈0 only for |ri| > 2 (see Figure 3.2).
−1
−0.5
0
0.5
1
1.5
2
2.5
3
3.5
4
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Normalised Margin
Loss Contribution
Gibbs Error 
Margin Loss
(κ=3/4) 
Margin Loss
(κ=1/2) 
Figure 3.2: Contribution of single pattern to expected Gibbs error and Bayes margin
loss respectively. The horizontal unit is the “normalised margin” yiri = yi(µ(xi) +
b)/σ(xi).
The advantage of the margin loss over the expected Gibbs error grows with

112
Chapter 3. PAC-Bayesian Bounds for Gaussian Process Methods
smaller κ, but the other dominant term in the bound of Theorem 3.4 scales as
1/κ. This second term poses a serious problem in the Bayes bound, since it scales
as (D[Q ∥P]/n)1/2 instead of D[Q ∥P]/n in the Gibbs bound. This becomes an
issue w.r.t. tightness if the empirical Bayes error and the empirical margin loss
is much smaller than 1/2, as will usually be the case in practice. It is interesting
that it is exactly this case that the authors put forward in [121] to contrast their
result with McAllester’s. They point out rightly that one should rather focus
PAC studies on the Bayes than on the Gibbs classiﬁer, given that the former
usually has much lower error rates in practice. However, their bound fails to
exploit properly this case of low error rates far from 1/2. In classical VC bounds
which deal with zero-one loss, this problem is elegantly circumvented by bounding
relative deviations such as (p −ˆp)/√p (see [196, 5]). A bound (p −ˆp)/√p ≤ε
implies
p ≤ˆp + 1
2ε2 
1 +
p
1 + 4ˆp/ε2

,
so that the bound on the gap p −ˆp is O(ε) in the worst case ˆp = 1/2, but
can be much smaller if ˆp is small. It would be interesting to ﬁnd out whether
this technique carries over to the PAC-Bayesian theorem.14
In fact, recently
exponential concentration inequalities have been provided which exploit a small
variance of the unknown process [21, 20], these could possibly serve as a substitute
of the bounded diﬀerence inequality used in the present proof. Furthermore, the
critical term drops below 1 only if 32 D[Q ∥P] < n, which may not happen at all
in practice (see Section 3.5.6).
3.4.1.2
Towards a PAC-Bayesian Theorem for Regression
How could we obtain a PAC-Bayesian theorem for a regression model? We are
not aware of the existence of a theorem comparable in tightness to Theorem 3.1 or
Theorem 3.4 for the regression setting. Note that while Gibbs versions of Bayes-
like classiﬁers often show acceptable performance in practice, a Gibbs variant of
14The proof of the zero-one loss situation does not carry over straightforwardly. Symmetri-
sation by a ghost sample and randomisation can be done, but it is not clear (to us) how to get
rid of the margin loss function (in the proof of Theorem 3.4 this is done using a contraction
principle, see Section B.5).

3.4. Related Work
113
a regression estimator would not be sensible, since the estimate is required to
be smooth. In this section, we present some thoughts towards a PAC-Bayesian
theorem for regression, however without coming to a deﬁnite conclusion. Readers
not interested in speculative material should jump to the next section. The details
are given in Appendix B.5.1.
Recall the GP regression model from Section 2.1.2. The latent process u(x|w),
a priori Gaussian with kernel K, is obscured by independent noise P(y|u) before
observation. We will use a non-negative loss function φ(·) which is Lipschitz with
maximum slope κ, and φ(0) = 0. Loss is quantiﬁed as φ(y −f(x)), for example
the empirical risk is
1
n
n
X
i=1
φ (yi −E
 ∼Q[u(xi|w)]) .
(3.23)
Note that if φ is also convex (Huber or Laplace loss are examples of φ which are
Lipschitz and convex, see Section A.6.1), we could obtain a bound on the risk
from a bound on the risk of the not very useful, but theoretically possible Gibbs
regression estimate:
E(
 ∗,y∗) [φ (y∗−E
 ∼Q[u(x∗|w)])] ≤E(
 ∗,y∗),Q [φ (y∗−u(x∗|w))] .
However, such a bound would most probably be in terms of the empirical risk of
the Gibbs estimate, which can be signiﬁcantly larger than the empirical risk of
the Bayes estimate.
We can try to move along the lines of Theorem 3.4. The ﬁrst steps in the
proof of Theorem 1 in [91] make use of Hoeﬀding’s bounded diﬀerences inequality
(see Section 2.2.2) which is not directly applicable to the regression setting with
unbounded loss. Therefore, in order to prove a theorem in the regression setting,
a diﬀerent concentration argument would have to be used.
To this end, it is
probably necessary to make some sort of assumptions (such as bounded variance)
on the distribution of the targets.
However, the remaining part of the proof, namely the bounding of the term
E∥Pn −P∥Gφ can be done, as is shown in Appendix B.5.1. Here, Gφ consists of
the functions (x, y) 7→φ(y −E
 ∼Qu(x|w)), Q ∈QA:
E∥Pn −P∥Gφ ≤
4
κ √n

ES
hp
2 A (tr K/n)
i
+
p
E [y2]

.
(3.24)

114
Chapter 3. PAC-Bayesian Bounds for Gaussian Process Methods
Here, E [y2] is an expectation over the data distribution. Furthermore, n−1 tr K
should concentrate under mild conditions on the data distribution and/or the
kernel function. For example, if K is a stationary kernel (see Section 2.1.1), then
K(x, x) = C for every x, and tr K/n = C, the prior process variance. If we
could show concentration of ∥Pn −P∥Gφ under a boundedness condition on E [y2],
we would end up with a PAC-Bayesian theorem for the GP regression model.
3.5
Experiments
In this section, we present experiments testing instantiations of the PAC-Bayesian
Theorem 3.1 for the Laplace GP Gibbs classiﬁer in Section 3.5.2 and the sparse
greedy GP (IVM) Gibbs classiﬁer in Section 3.5.3 (these special cases of the bound
are described in Sections 3.3.2 and 3.3.3). The results indicate that the bounds
are very tight even for training samples of moderate sizes. In Section 3.5.4, we
compare our bound to a state-of-the-art PAC compression bound for the IVM
Bayes classiﬁer, and to the same compression bound for the soft-margin support
vector classiﬁer in Section 3.5.4.1. In Section 3.5.5 we try to evaluate the model
selection qualities of the PAC-Bayesian bound, again applied to the IVM Gibbs
classiﬁer. Finally, in Section 3.5.6 we strengthen some of the results by repeating
experiments on a diﬀerent setup and present comparisons with the bound of Meir
and Zhang (see Section 3.4.1).
3.5.1
The Setup MNIST2/3
Let us describe the experimental design MNIST2/3 for most of the sections to
come. This design applies in all sections but the last one (3.5.6). A real-world
binary classiﬁcation task was created on the basis of the well-known MNIST hand-
written digits database15 as follows. MNIST comes with a training set of 60000
and a test set of 10000 handwritten digits, represented as 28-by-28-pixel bitmaps,
the pixel intensities are quantised to 8 bit values. First, the input dimensionality
was reduced by cutting away a 2-pixel margin, then averaging intensities over
15Available online at http://www.research.att.com/∼yann/exdb/mnist/index.html.

3.5. Experiments
115
3-by-3-pixel blocks, resulting in 8-by-8-pixel bitmaps. We concentrated on the
binary subtask of discriminating twos against threes, for which a training pool of
12089 cases and a test set of l = 1000 cases were created.
The Gaussian process prior in our model was parameterised by a Gaussian
(RBF) kernel with variance parameter C > 0 and an inverse squared length scale
w > 0 (2.29). Since all the tasks we considered are balanced, we did not employ
a bias parameter in the noise model.
The experimental setup was as follows. An experiment consisted of ten inde-
pendent iterations. During each iteration, three datasets were sampled indepen-
dently and without replacement from the training pool: a model selection (MS)
training set of size nMS, a MS validation set of size lMS and a task training sample
S of size n. Note that the latter set is sampled independently from the model
selection sets, ensuring that the prior P in Theorem 3.1 is independent of the task
training sample. Then, model selection was performed over a list of candidates
for (w, C), where a classiﬁer was trained on the MS training set and evaluated
on the MS validation set (the MS score was the expected empirical error of the
Gibbs classiﬁer on the MS validation set). The winner was then trained on the
task training set S and evaluated on the test set. Alongside, the upper bound
value given by Theorem 3.1 was evaluated. The conﬁdence parameter δ was ﬁxed
to 0.01.16 We also quote total running times for some of the experiments. Where
timing ﬁgures are given, these have been obtained on the same machine using the
same implementation.
3.5.2
Experiments with Laplace GPC
The Laplace approximation framework for GP binary classiﬁcation is described in
Section 2.1.3, and the application of the PAC-Bayesian theorem to this method
in Section 3.3.2. We used a logit noise model without a bias term, P(y|u) =
σ(y u), σ the logistic function. Note that both the computation of the relative
entropy term and the expected empirical error require O(n3), and predictions
16Note that much smaller values of δ could have been used without altering the upper bound
values signiﬁcantly.

116
Chapter 3. PAC-Bayesian Bounds for Gaussian Process Methods
are O(n2) per pattern (we did not employ the sampling techniques from Ap-
pendix B.4). Note also that the complete kernel matrix K has to be evaluated
and stored. Our implementation requires only one buﬀer of size n2.
The speciﬁcations and results for the experiments of this section are listed in
Table 3.1. For all these experiments, we chose model selection validation set size
lMS = 1000 (recall that the test set is ﬁxed with size l = 1000). Experiments #1 to
#5 have growing sample sizes n = 500, 1000, 2000, 5000, 9000, the corresponding
MS training set sizes are nMS = 1000 for experiments #2 to #5, and nMS = 500
for experiment #1. Note that nMS < n in experiments #3 to #5 is chosen for
computational feasibility, due to the considerable size of the candidate list for
(C, w).
#
n
nMS
emp
gen
upper
gen-bayes
time
1
500
500
0.036
0.0469
0.182
0.0339
14
(±0.0039)
(±0.0015)
(±0.0057)
(±0.0023)
2
1000
1000
0.0273
0.036
0.131
0.0274
67
(±0.0023)
(±0.001)
(±0.0041)
(±0.0022)
3
2000
1000
0.0243
0.028
0.1091
0.0236
91
(±0.0026)
(±0.0013)
(±0.0079)
(±0.0029)
4
5000
1000
0.0187
0.0195
0.076
0.0171
762
(±0.0016)
(±0.0011)
(±0.002)
(±0.0016)
5
9000
1000
0.0178
0.0172
0.0706
0.0158
3618
(±0.0012)
(±0.0013)
(±0.0037)
(±0.0017)
Table 3.1: Experimental results for Laplace GPC. n: task training set size; nMS:
model selection training set size.
emp: expected empirical error; gen: expected
generalisation error (estimated as average over test set). upper: upper bound on
expected generalisation error after Theorem 3.1. gen-bayes: test error of corr. Bayes
classiﬁer. time: total running time per run (secs). Figures are mean and width of
95% t-test conﬁdence interval.
Note that the resource requirements for our experiments are well within to-

3.5. Experiments
117
day’s desktop machines computational capabilities. For example, experiment #4
was completed in total time of about 12 to 13 hours, the memory requirements
are around 250M. Now, for this setting both the expected empirical error and
the estimate (on the test set) of the expected generalisation error lie around 2%,
while the PAC bound on the expected generalisation error given by Theorem 3.1
is 7.6% — an impressive, highly non-trivial result on samples of size n = 5000.
Our largest experiment #5 was done mainly for comparison with experiment #2
for IVM (see Section 3.5.3). The total computation time was 6 hours for each
iteration, and the memory requirements are around 690M. We note a slight im-
provement in test errors as well as in the upper bound values (which now lie
around 7%).
The “gen-bayes” column in Table 3.1 contains the test error that a Bayes
classiﬁer with the same approximate posterior as the Gibbs classiﬁer attains (see
Section 3.1.2). Note that it is not necessarily the best we could obtain for a Bayes
classiﬁer, because the model selection is done speciﬁcally for the Gibbs variant. In
the Laplace GPC case we note that Bayes and Gibbs variants perform comparably
well, although the Bayes classiﬁer attains slightly better results and, as mentioned
in Section 3.3.2, can be evaluated more eﬃciently. We include these results for
comparison only: although the Gibbs result implies a bound on the generalisation
error of the Bayes classiﬁer (see Section 3.2.5), the link is too weak to render a
suﬃciently tight result.
3.5.3
Experiments with Sparse Greedy GPC
The sparse IVM approximation to GP binary classiﬁcation is discussed in Sec-
tion 4.4.1 and the corresponding PAC-Bayesian theorem in Section 3.3.3. In com-
parison with Laplace GPC, both training and evaluation are much faster now:
O(n d2) and O(d2) respectively. The bound value can be computed in O(n d2). In
our experiments here, the ﬁnal active set size d was ﬁxed a priori. Training was
done in the same way as discussed in Section 4.8.1. For all experiments reported
here, we chose MS training size nMS = 1000, MS validation size lMS = 1000 and
dMS = 150. Note that in experiments which have the same (n, nMS, lMS) constel-

118
Chapter 3. PAC-Bayesian Bounds for Gaussian Process Methods
lation as Laplace GPC experiments, we use the same data subsets, in order to
facilitate direct comparisons. The results are listed in Table 3.2.
#
n
d
emp
gen
upper
gen-bayes
time
1
5000
500
0.0154
0.0207
0.067
0.0084
16
(±0.0021)
(±0.0015)
(±0.0026)
(±0.0014)
2
9000
900
0.0101
0.0116
0.0502
0.0042
82
(± 6.88e-4)
(± 5.49e-4)
(± 6.13e-4)
(± 8.79e-4)
Table 3.2: Experimental results for sparse GPC. n: task training set size; d: ﬁnal
active set size. emp: expected empirical error; gen: expected generalisation error
(estimated as average over test set). upper: upper bound on expected generalisation
error after Theorem 3.1. gen-bayes: test error of corr. Bayes classiﬁer. time: total
running time per run (secs). Figures are mean and width of 95% t-test conﬁdence
interval.
Let us compare these results to the ones obtained for Laplace GPC. The sparse
GPC Gibbs classiﬁer trained with 5000 examples attains an expected test error
of 2.1%, and the upper bound evaluates to 6.7%. While the former is the same as
for the Laplace GPC variant, the latter is signiﬁcantly lower. The ratio between
upper bound and expected test error is 3.19, the ratio between gap bound and
expected test error is 2.46. Note that experiment #1 for the sparse GPC was
completed in total time of about 16 minutes — almost ﬁfty times faster than the
Laplace GPC experiment #4. It is interesting to observe that for this sample
size, the results here are signiﬁcantly better than for the full Laplace GPC on the
same task17 (experiment #5 in Section 3.5.2). Finally note that we did not try
to optimise the ﬁnal active set size d, but simply ﬁxed d = n/10 a priori. An
automatic choice of d could be based on heuristics which evaluate the error on
17Note that we are comparing two quite diﬀerent ways of approximating the true posterior
by a Gaussian: a Laplace approximation around the mode (which is diﬀerent from the posterior
mean — the “holy grail” of Bayesian logistic regression, see Section 2.1.3) and an approximation
based on repeated moment matching (see Section 4.3). A more meaningful direct comparison
would involve the cavity TAP method of [139] (see Section 4.3) which is, however, more costly
to compute than the Laplace approximation.

3.5. Experiments
119
the datapoints outside the active set (see Section 4.4.1).
The “gen-bayes” column in Table 3.2 serves the same purpose as the “gen-
bayes” column in Table 3.1.
In case of sparse greedy GPC, the results show
that the Bayes classiﬁer performs signiﬁcantly better than the Gibbs variant,
although the latter still attains very competitive results. A possible explanation
for this diﬀerence, given that it cannot be observed for Laplace GPC, is obtained
by inspecting the (C, w) kernel parameters values that are preferred by sparse
greedy GPC. The parameter C is much larger for sparse GPC, i.e. the latent
process has a larger a priori variance. This typically leads to an increase in the
predictive variances, which in turn might introduce more sampling errors in the
Gibbs predictions.
3.5.4
Comparison with PAC Compression Bound
In this section, we present experiments in order to compare our result for sparse
GPC (Section 3.5.3) with a state-of-the-art PAC compression bound. Note that
here, we employ Bayes GP classiﬁers instead of Gibbs GP classiﬁers: it would
not be fair to compare our Gibbs-speciﬁc bound to an “artiﬁcially Gibbs-iﬁed”
version of a result which is typically used with Bayes-like “averaging” classiﬁers.
A compression bound applies to learning algorithms which have some means of
selecting a subsample SI of size d from a sample of size n, such that the hypothesis
they output is independent of S\I = S \ SI, given SI. More details are given in
Appendix B.6, where we also state and prove the compression bound we are
using here (Theorem B.2). Examples of compression schemes are the perceptron
learning algorithm of [154], the support vector machine (SVM) and the sparse
greedy IVM. The PAC compression bound of Theorem B.2 depends only on the
training error for the remaining n −d patterns S\I outside the active set (called
emp\d(S)) and on d. We repeated the experimental setup used in Section 3.5.3
and employed the same dataset splits. The results can be found in Table 3.3.
For both experiments, emp\d(S) = 0 was achieved in all runs, the compression
bound is tightest in this case. Nevertheless, in experiment #1, the upper bound
on the generalisation error is 30.5%, a factor of 50 above our estimate on the

120
Chapter 3. PAC-Bayesian Bounds for Gaussian Process Methods
#
n
d
emp
gen
upper
1
5000
500
0.0025
0.0058
0.3048
(± 6.79e-4)
(±0.0015)
(± 0)
2
9000
900
0.0024
0.003
0.3041
(± 4.25e-4)
(± 7.54e-4)
(± 0)
Table 3.3: Experimental results for PAC compression bound with sparse GP Bayes
classiﬁer. n: task training set size; d: ﬁnal active set size. emp: empirical error
(on full training set); gen: error on test set. upper: upper bound on generalisation
error given by PAC compression bound. Figures are mean and width of 95% t-test
conﬁdence interval.
test set. The ratio is even worse for experiment #2. At least on this task, the
PAC-Bayesian theorem produces much tighter upper bound values than the PAC
compression bound. Note also that the compression bound is more restrictive,
in that it applies to compression schemes only. On the other hand, the PAC-
Bayesian theorem depends much more strongly on the model, prior assumptions
and inference approximation algorithm, while the compression bound cannot dif-
ferentiate between algorithms which attain the same level of compression and
empirical error emp\d(S).
The reader may wonder why the generalisation errors here are slightly lower
than the ones reported in Table 3.2.
This should be due to the fact that in
Section 3.5.3, we evaluated the Bayes classiﬁer based on the hyperparameter
values which have been optimised for the Gibbs variant, while here we performed
model selection for the Bayes classiﬁer explicitly.
3.5.4.1
Comparison with Compression Bound for Support Vector Classiﬁers
We can also compare our results for sparse GP Gibbs classiﬁers with state-of-the-
art bounds for the popular soft-margin support vector machine (SVM). The latter
is introduced in Section 2.1.6, where we also discuss similarities and diﬀerences
to proper GP classiﬁcation models. In the context here, it is important to note

3.5. Experiments
121
that the SVM algorithm often produces rather sparse predictors. However, the
degree of sparseness is not a directly controllable parameter, furthermore it is not
an explicit algorithmic goal of the SVM algorithm to end up with a maximally
sparse expansion. The aim is rather to maximise the “soft” minimal empirical
margin (see Section 2.1.6). SVM is a compression scheme (see Section B.6), where
d is the number of support vectors (vectors which are misclassiﬁed or lie within the
margin tube around the decision hyperplane in feature space; see Section 2.1.6).
Note that for SVM, we always have emp\d(S) = 0, since misclassiﬁed points are
support vectors. Experimental results for SV classiﬁers and the PAC compression
Theorem B.2 can be found in Table 3.4. Again, we employed the same dataset
splits as used in Section 3.5.3.
#
n
emp
gen
upper
num-sv
1
5000
0.0016
0.0048
0.2511
370.8
(± 9.49e-4)
(±0.0012)
(±10.71)
2
9000
0.0021
0.0036
0.213
529
(± 7.67e-4)
(±0.0012)
(±29.12)
Table 3.4: Experimental results for PAC compression bound with SV classiﬁers. n:
task training set size. emp: empirical error (on full training set); gen: error on test
set. upper: upper bound on generalisation error given by PAC compression bound.
Figures are mean and width of 95% t-test conﬁdence interval.
In both experiments, a higher degree of sparsity is attained than the one
chosen in the experiments above for sparse GPC (as mentioned above, we did not
try to optimise this degree in the sparse GPC case), leading to somewhat better
values for the PAC compression bound. However, the values of 25% (experiment
#1) and 21% (experiment #2) are still by factors > 50 above the estimates
computed on the test set, which is not useful in practice.
The compression bound applies to SVM, but is certainly not speciﬁcally tai-
lored for this algorithm, since it does not even depend on the empirical mar-
gin distribution.
Can we get better results with other SVM-speciﬁc bounds?

122
Chapter 3. PAC-Bayesian Bounds for Gaussian Process Methods
The margin bound of [177], commonly used to justify data-dependent structural
risk minimisation for SVM, becomes non-trivial (i.e. smaller than 1) only for
n > 34816 (see [73], Remark 4.33). The algorithmic stability bound of [22] does
not work well for support vector classiﬁcation either. In fact, the gap bound value
converges to zero at some rate r(n) (for n →∞) only if the variance parameter18
C goes to zero at the same rate r(n). If r(n) = O(1/n) or r(n) = O(log n/n), this
would correspond to severe over-smoothing. Herbrich and Graepel [73] use some
older PAC-Bayesian theorems from [116] to prove a bound which depends on
the minimal normalised empirical margin. This theorem applies to hard-margin
SVC only and becomes non-trivial once the minimal normalised (hard) margin19
is > 0.91, given that the feature space has dimension > n. Hard-margin SVMs
tend to overﬁt on noisy real-world data with very small normalised margins at
least on some points, and in practice the soft-margin variant is typically preferred.
In a separate experiment using the same setup and dataset splits as in #1 of this
section (i.e. training sample size n = 5000), but training hard margin SVMs with-
out bias parameter, we obtained generalisation error estimates on the test set of
gen = 0.0056 (± 3.904e-5), minimum normalised margins of minmarg = 0.0242 (±
2.813e-5) and generalisation upper bound values of upper = 16.28 (± 4.7e-3) using
the theorem of [73]. These results back the simple observation that the minimum
normalised (hard) margin is not suitable as a PAC gap bound statistic and should
probably be replaced by one which is more robust against noise, such as soft mar-
gin, sparsity degree or combinations thereof. All in all, and much to our surprise,
we were not able to ﬁnd any proposed “SVC-speciﬁc” bound which would be
tighter on this task than the simple PAC compression bound of Theorem B.2
used above.
18In the SVM literature, it is common practice to separate C from the covariance kernel and
write it in front of the sum over the slack variables. The parameter λ in [22] is λ = 2/(Cn),
and their gap bound behaves as 1/(nλ) as n →∞.
19If we view SVC as a linear method in a feature space induced by the covariance kernel, the
minimal normalised margin is the arc cosine of the maximal angle between the normal vector
of the separating plane and any of the input points mapped into feature space. A minimal
normalised margin close to 1 means that all mapped input points lie within a double cone of
narrow angle around the line given by the normal vector. For noisy data, such a situation is
arguably quite unlikely to happen.

3.5. Experiments
123
3.5.5
Using the Bounds for Model Selection
Can our results be used for model selection? In our opinion, this issue has to be
approached with care. It seems rather obvious that a generalisation error bound
should not be used for model selection on a real-world task if it is very far above
reasonable estimates of the generalisation error on this task.
This argument
is discussed in more detail in Section 2.2.4.
Even though the PAC-Bayesian
theorem applied to GP Gibbs classiﬁers oﬀers highly non-trivial generalisation
error guarantees for the real-world task described in this section, they still lie by
a factor > 3 above the estimates on the test set. In spite of this fact, we follow
the usual conventions and present results of an experiment trying to assess the
model selection qualities of our bound.
2
4
6
8
10
12
0.019
0.021
0.023
0.025
0.027
w
0.065
0.067
0.069
0.071
0.073
Exp. gen. error
Upper bound    
2
4
6
8
10
12
0.019
0.021
0.023
0.025
0.027
w
0.065
0.067
0.069
0.071
0.073
Exp. gen. error
Upper bound    
2
4
6
8
10
12
0.018
0.02
0.022
0.024
0.026
w
0.064
0.066
0.068
0.07
0.072
Exp. gen. error
Upper bound    
2
4
6
8
10
12
0.018
0.02
0.022
0.024
0.026
0.028
w
0.068
0.07
0.072
0.074
0.076
0.078
Exp. gen. error
Upper bound    
2
4
6
8
10
12
0.018
0.02
0.022
0.024
w
0.058
0.06
0.062
0.064
Exp. gen. error
Upper bound    
2
4
6
8
10
12
14
0.018
0.02
0.022
0.024
0.026
w
0.065
0.067
0.069
0.071
0.073
Exp. gen. error
Upper bound    
Figure 3.3: Comparing upper bound values with expected test errors.
Solid line:
expected test error (scale on left). Dashed line: upper bound value (translated, scale
on the right). Respective minimum points marked by an asterisk.
Once more, we used sparse greedy GPC. The experiment consisted of six
independent runs. We ﬁxed C = 120 and used a grid of values for w. In each
run, a sample S of size n = 5000 was drawn from the training pool, the method
was trained for each conﬁguration (C, w) (we ﬁxed d = 500) and evaluated on
the test set. The results are shown in Figure 3.3. We translated the upper bound

124
Chapter 3. PAC-Bayesian Bounds for Gaussian Process Methods
values towards the expected test errors by subtracting a constant (determined as
95% of the average distance between points on the two curves). In each graph,
the scale on the left hand side is for the expected test error, the scale on the
right hand side for the upper bound value. In Figure 3.4, we plot expected test
errors (horizontal) vs. upper bound values (vertical). In this type of plot a mostly
monotonically increasing relationship is what we would ideally expect. The dotted
curves are lines x + b with slope 1, where b is ﬁtted to the corresponding solid
curves by minimum least squares. The ordering of the six subplots is the same
as in Figure 3.3.
0.0195
0.0205
0.0215
0.0225
0.0235
0.067
0.068
0.069
0.07
0.071
0.072
Exp. gen. error
Upper bound
0.019
0.02
0.021
0.022
0.023
0.0665
0.0675
0.0685
0.0695
0.0705
0.0715
0.0725
Exp. gen. error
Upper bound
0.0185
0.0195
0.0205
0.0215
0.0225
0.066
0.067
0.068
0.069
0.07
0.071
0.072
Exp. gen. error
Upper bound
0.0185
0.0195
0.0205
0.0215
0.0225
0.0235
0.07
0.071
0.072
0.073
0.074
0.075
0.076
0.077
Exp. gen. error
Upper bound
0.0185
0.0195
0.0205
0.0215
0.0595
0.0605
0.0615
0.0625
0.0635
Exp. gen. error
Upper bound
0.0182
0.0192
0.0202
0.0212
0.0222
0.0667
0.0677
0.0687
0.0697
0.0707
0.0717
0.0727
Exp. gen. error
Upper bound
Figure 3.4: Comparing upper bound values (vertical axis) with expected test errors
(horizontal axis). Dotted line: ﬁtted regression line with slope 1.
In this particular experiment, there is a surprisingly good monotonically in-
creasing linear correlation between upper bound values and expected test errors,
and model selection based on minimising the upper bound value might have
worked in this case. However, note that the constants we had to subtract from
the upper bound curves in order to bring them close to the expected generalisa-
tion error estimates for visual inspection, were an order of magnitude larger than
the range of variation of the individual curves shown in the plots. An important
future direction of research would be to gain more understanding, both empir-

3.5. Experiments
125
ically and analytically, of the phenomenon observed here: a slack term which
is signiﬁcantly larger than the expected test error, yet also much less variable
than the latter over a range of hyperparameter values of interest (see also Sec-
tion 3.2.6.2). Further experiments in Section 3.5.6 are however less conclusive
and show that for other hyperparameters, the bound may not be predictive.
2
4
6
8
10
12
0.0075
0.0125
0.0175
0.0225
0.0275
w
2
4
6
8
10
12
0.0075
0.0125
0.0175
0.0225
0.0275
w
2
4
6
8
10
12
0.0075
0.0125
0.0175
0.0225
0.0275
w
2
4
6
8
10
12
0.0075
0.0125
0.0175
0.0225
0.0275
w
2
4
6
8
10
12
0.0075
0.0125
0.0175
0.0225
w
2
4
6
8
10
12
0.0075
0.0125
0.0175
0.0225
0.0275
w
Figure 3.5: Comparing upper bound values with expected test errors (upper parts)
and gap bound values with expected training errors (lower parts). Solid: expected
test error (scale on left side). Dashed: upper bound value (translated). Dash-dotted:
expected training error (scale on left side). Dotted: gap bound value (translated).
Respective minimum points marked by an asterisk.
One might suspect that it is really only the expected empirical error which fol-
lows the expected test error closely, and that the diﬀerence between them remains
fairly constant. After all, most of the points the empirical error is evaluated on
are not in the active set. This argument of course ignores the fact that there is
a dependence of the posterior on patterns outside the active set even for a com-
pression scheme such as the IVM, namely they have not been chosen in favour of
the active set. In theory, a PAC bound tells us that such dependencies have to
be accounted for by an additional complexity term which is of rather crude union
bound type in the PAC compression bound (Theorem B.2) and of a more reﬁned

126
Chapter 3. PAC-Bayesian Bounds for Gaussian Process Methods
type in the PAC-Bayesian theorem. By splitting the upper bound curves above
into expected empirical errors and gap bound values, we see that this extension is
indeed necessary even if d ≪n. In Figure 3.5 we plotted all these curves together
in common graphs, using the same ordering of runs as in the other graphs. In
each subplot, the two curves at the top are the upper bound (dashed) and the
expected test error (solid), while the two curves at the bottom are the expected
empirical error (dash-dotted) and the gap bound (dotted). The scale is correct
for both the expected empirical and test error curves, while the gap and upper
bound curves are translated downwards by diﬀerent constants. The scale for the
latter two curves is omitted. Furthermore, the graphs in Figure 3.6 show that
the linear correlation between expected training and test errors is poor, and in-
deed most of the graphs in Figure 3.5 exhibit over-ﬁtting: model selection based
on minimising the expected training error would choose too large values of w,
corresponding to a too narrow kernel width.
0.0195
0.0205
0.0215
0.0225
0.0235
0.011
0.012
0.013
0.014
0.015
0.016
0.017
0.018
Exp. gen. error
Exp. empir. error
0.0192
0.0202
0.0212
0.0222
0.0232
0.0117
0.0127
0.0137
0.0147
0.0157
0.0167
0.0177
Exp. gen. error
Exp. empir. error
0.0187
0.0197
0.0207
0.0217
0.0227
0.011
0.012
0.013
0.014
0.015
0.016
0.017
Exp. gen. error
Exp. empir. error
0.0183
0.0193
0.0203
0.0213
0.0223
0.0233
0.013
0.014
0.015
0.016
0.017
0.018
0.019
0.02
Exp. gen. error
Exp. empir. error
0.0185
0.0195
0.0205
0.0215
0.0085
0.0095
0.0105
0.0115
0.0125
0.0135
Exp. gen. error
Exp. empir. error
0.0182
0.0192
0.0202
0.0212
0.0222
0.012
0.013
0.014
0.015
0.016
0.017
0.018
Exp. gen. error
Exp. empir. error
Figure 3.6: Comparing expected training errors (vertical axis) with expected test
errors (horizontal axis). Dotted line: ﬁtted regression line with slope 1.

3.5. Experiments
127
3.5.6
Comparing Gibbs and Bayes Bounds
The aim of this section is twofold.
First, in order to strengthen the results
obtained in Sections 3.5.3 and 3.5.5 we repeat these experiments here using a
diﬀerent binary classiﬁcation task. Second, we planned to compare the PAC-
Bayesian theorem for Gibbs classiﬁers and its implications for the GP Bayes
classiﬁer against Theorem 3.4 obtained by Meir and Zhang. However, it turned
out that the latter theorem does not render non-trivial bounds on this task,
highlighting the problem pointed out in Section 3.4.1.1.
The setup MNIST8/9 we used here is similar to MNIST2/3 introduced in
Section 3.5.1, but diﬀers in the following points. We now consider the MNIST
subtask of discriminating eights from nines. We used all available images from
the oﬃcial training set in our training pool (11800 cases) and tested on all images
from the oﬃcial test set (l = 1983), and the full 28-by-28-pixel bitmaps served as
input points here.
We repeated the experiments of Section 3.5.3 for the single training sample
size n = 8000, ﬁxing the active size to d = 800, furthermore nMS = lMS = 1000
and dMS = 150. The outcome was emp = 0.0105(± 6.93e-4), gen = 0.0220(±
2.68e-4), upper = 0.0568(± 0.0012), quite in line with the earlier ﬁgures.
In order to compare the two PAC-Bayesian theorems for the GP Bayes clas-
siﬁer, we re-ran these experiments, but now using the Bayes instead of the Gibbs
variant for model selection.20 As noted in Section 3.2.5, we can use twice the
value of the Gibbs bound as an upper bound for the Bayes classiﬁer, however
this is unsatisfactory given the apparent superior performance of the Bayes clas-
siﬁer on this task. When we tried to compare these results against the bound
of Theorem 3.4 of Meir and Zhang, we ran into the problem mentioned in Sec-
tion 3.4.1.1: the term 4cκ−1(2D[Q ∥P]/n)1/2 was larger than one even for κ = 1,
in all ten runs. In this situation, the bound cannot give non-trivial results for
any prior conﬁguration. We suspect that even in cases where the bound becomes
non-trivial, the scaling with the square root of D[Q ∥P]/n will render it very sub-
20The selection results were quite diﬀerent. The Gibbs variant preferred larger C values,
while the Bayes variant required larger w values.

128
Chapter 3. PAC-Bayesian Bounds for Gaussian Process Methods
optimal in practice. Note that this task, with n = 8000, is at the upper range of
sample sizes we considered in our experiments. From these experimental ﬁndings,
we conclude that the problem of a practically satisfying PAC-Bayesian bound for
GP Bayes classiﬁers is not resolved yet.
Given the results in Section 3.5.5, the PAC-Bayesian bound seems quite suit-
able for model selection due to an excellent monotonically increasing linear cor-
relation between expected test errors and upper bound values for diﬀerent values
of w. We repeated these experiments on MNIST8/9 with n = 8000, resulting in
Figure 3.7.
0
2
4
6
8
10
12
14
0.02
0.024
0.028
0.032
w
Exp. gen. error
Upper bound
0.0563 
0.0603 
0.0643
0.0683
C=200 
0
2
4
6
8
10
12
14
0.02
0.025
0.03
0.035
0.04
0.045
w
Exp. gen. error
Upper bound
0.0558
0.0608
0.0658
0.0708
0.0758
c=100 
100
200
300
400
500
0.022
0.023
0.024
0.025
C
Exp. gen. error
Upper bound
0.0565
0.0575
0.0585
0.0595
w=1.5
0.02
0.025
0.03
0.035
0.04
0.045
0.055
0.06
0.065
0.07
0.075
0.08
Exp. gen. error
Upper bound
C=200, varying w 
0.02
0.025
0.03
0.035
0.04
0.055
0.06
0.065
0.07
0.075
0.08
Exp. gen. error
Upper bound
C=100, varying w 
0.022
0.0225
0.023
0.0575
0.058
0.0585
0.059
0.0595
Exp. gen. error
Upper bound
w=1.5, varying C 
Figure 3.7: Comparing expected test errors with upper bound values on MNIST8/9,
n = 8000. Upper row: hyperparameter values vs. test errors (scale left) and trans-
lated bound values (scale right). Lower row: test errors vs. bound values (dotted:
ﬁtted regression line).
Left: varying w with C = 200.
Middle: varying w with
C = 100. Right: varying C with w = 1.5.
These results show that the upper bound value can have shortcomings as “pre-
dictor” for the expected test error. The plots in the left and middle columns are
for ﬁxed C = 200, C = 100 and varying w. Some values bail out of the otherwise
linear relationship, notably the upper bound underestimates the sharp increase in
test error for too small w. Recall that small w translate into large length scale and

3.6. Discussion
129
therefore very smooth discriminant functions (decision boundaries). A preference
for over-smooth solutions hints towards the bound placing too much weight on
the complexity penalty. The fact that for large w values the bound grows faster
than the expected test error also points in this direction. The right column plots
show that the upper bound fails to predict the test error for varying C and ﬁxed
w = 1.5, again favouring over-smooth solutions (C is the prior process variance).
These ﬁndings somewhat back the objections we raise in Section 2.2.4, showing
that model selection based on current PAC bounds is risky at best.
3.6
Discussion
In this chapter, we have shown how to apply McAllester’s PAC-Bayesian the-
orem to approximate Bayesian Gaussian process classiﬁcation methods.
The
generic bound applies to a wide class of GP inference approximations, namely
all that use a GP to approximate the predictive process (see Sections 2.1.3 and
4.6 for references). We have simpliﬁed the original proof of the PAC-Bayesian
theorem considerably, pointing out convex duality (see Section A.3) as the prin-
cipal underlying technique. We have also shown how to generalise the theorem to
multi-class settings and the problem of bounding linear functions of the confusion
matrix. Although in this work, we have focused on Gaussian process models, the
PAC-Bayesian theorem can in principle be applied to any approximate Bayesian
classiﬁcation technique. The terms the bound depends upon typically fall out as
simple byproducts of the inference approximation.
Additional conclusions and suggestions for future work can be found in Sec-
tion 5.1.

130
Chapter 3. PAC-Bayesian Bounds for Gaussian Process Methods

Chapter 4
Sparse Gaussian Process Methods
When approaching a statistical learning problem with a non-parametric method,
we truly let the data speak. Prior knowledge about the problem is used to smooth
the data for visualisation and prediction, but there is no need to constrain the
data generation process to be compatible with a pre-built model. In general, the
principal drawback of non-parametric methods in practice is the heavy scaling
with the size of the training sample, since what we have learned is represented by
the virtually uncompressed sample itself. Predictions require associations of the
test point with all training points, as does incremental learning. This is generally
infeasible for very large samples. In sparse approximations, we try to ﬁlter out
and discard redundant points from the sample, where the notion of redundancy
depends on our current belief about the source. In this chapter, we clarify the
nature of such approximations to Gaussian process inference and present some
new schemes which employ greedy forward selection to ﬁlter the sample.
We
show that these schemes improve on the scaling of their non-sparse counterparts
by orders of magnitude for large samples, thereby widening the range of problems
which can now be attacked successfully using Gaussian process models.
The structure of this chapter is as follows. The introduction in Section 4.1
contrasts parametric and non-parametric methods and sets out our goals.
In
Section 4.2, we develop some general theory behind sparse GP approximations,
namely the central notion of likelihood approximations and a discussion of greedy
selection criteria borrowed from active learning to meet our sample subset selec-
131

132
Chapter 4. Sparse Gaussian Process Methods
tion requirements.
The schemes we focus on here are sparse variants of the
expectation propagation (EP) algorithm for GP models, which is introduced in
Section 4.3. Two schemes for conditional sparse inference are developed in Sec-
tion 4.4: the simple and fast IVM and the more accurate PLV. Approximate
inference techniques are incomplete without dealing with the model selection is-
sue, which we do in Section 4.5.
The ﬁeld of sparse GP approximations has
received a lot of attention recently, relations to other work are clariﬁed in Sec-
tion 4.6. Finally, we present empirical studies in Section 4.8: binary classiﬁcation
and regression estimation with IVM and regression estimation with PLV. We
close the chapter with a discussion in Section 4.9.
Parts of the results presented here have been obtained in collaborations with
other researchers and have previously been published in conference proceedings,
as is detailed in Section 1.1.
4.1
Introduction
Bayesian inference is certainly a very appealing principle, based on simple con-
sistent rules at the core (see Section A.6). However, the computational burden is
more often than not intractable, and the challenge is to ﬁnd suitable approxima-
tions tailored to the particular situation at hand. As we have seen in Section 2.1.3,
schemes for exact or approximate Bayesian inference in GP models typically suf-
fer from heavy scaling with the number n of training points: O(n3), or at least
O(n2) time for training, O(n2) memory for training, ﬁnally O(n) or even O(n2)
memory to represent the predictor and at least O(n) time for each prediction
on a test point. At least the latter ﬁgures are characteristic for non-parametric
methods which use the training data in uncompressed form to represent the belief.
In contrast, parametric methods such as logistic regression (see Section 2.1.2) or
multi-layer perceptrons typically have O(n) training time and memory as well as
test time requirements independent of n. They represent the information in the
training sample which actually aﬀects the belief by an empirical statistic of size
independent of n, which is why the belief can be parameterised in a way which

4.2. Likelihood Approximations and Greedy Selection Criteria
133
does not scale with n. A further advantage of parametric methods is that the
belief can typically be updated fairly easily (in time independent of n) when new
data comes in. A drawback of this approach is that it is often hard to come up
with a sensible model based on some hard-wired statistics when confronted with
a not very well-understood source, and inferences may be biased by artifacts and
shortcomings of the parametric model which have little to do with the actual
observations. Furthermore, while most non-parametric methods are universally
consistent, i.e. will represent the true source in the limit of inﬁnite data, this is
typically not the case for parametric methods (if the size of the suﬃcient statistic
does not grow with n). Finally, the relationship between parameters and repre-
sented distributions can be very complicated (e.g., for a multi-layer perceptron),
in which case current approximation techniques to Bayesian inference can give
very poor results or may be very diﬃcult to handle and time-consuming.
Sparse approximations to GP models in a sense try to combine the ﬂexibility
and simplicity of non-parametric methods with the favourable scaling of paramet-
ric ones. Instead of a priori ﬁxing the statistics which are applied to the training
sample, they try to directly approximate the inference for a full GP model using
a sparse representation for the posterior belief whose size can be controlled inde-
pendently of n. By doing so, they nevertheless try to retain desirable properties
of the scheme based on the full GP model. For example, the sparse approxima-
tions discussed here all retain the property that the predictive posterior process
for the latent outputs is again Gaussian.
4.2
Likelihood Approximations and Greedy Selec-
tion Criteria
In this section, we show how sparse GP approximations can be understood as like-
lihood approximations. We also introduce a number of greedy selection criteria
and discuss their potential signiﬁcance for sparse GP techniques.

134
Chapter 4. Sparse Gaussian Process Methods
4.2.1
Likelihood Approximations
As we have motivated in Section 4.1, a sparse approximation to GP inference
should lead to training time and memory requirements which scale essentially lin-
early in n, the number of training points, yet ideally retain the desirable property
of full inference schemes, namely that the predictive posterior process P(u(·)|S)
is again Gaussian (see Section 2.1.3). In this Section, we discuss a general frame-
work for obtaining such approximations.
In order to retain the predictive posterior GP property, it seems necessary to
follow the same procedure as in the general case (see Section 2.1.3): approximate
P(u|S) by a Gaussian Q(u|S), then deﬁne the approximate predictive process
via (2.10). Of course, it would be possible in principle to replace the conditional
prior process P(u(·)|u) by a diﬀerent GP depending on S and still obtain a
Gaussian Q(u(·)|S). However, such an approximation would violate the property
u(·) ⊥S | u of the true process, which does not seem sensible.
If the true posterior P(u|S) ∝P(S|u)P(u) is approximated by a Gaussian,
we have Q(u|S) ∝Q(S|u)P(u), where Q(S|u) as a function of u is an unnor-
malised1 Gaussian (Deﬁnition A.10). Now, suppose that Q(S|u) really only de-
pends on a subset uI of the components of u, where I ⊂{1, . . . , n}, |I| = d ≪n.
In this case, for the prediction at a new datapoint x∗, we have
Q(u∗|x∗, S) = EQ(
  |S) [P(u∗|u)] = EQ(
 I|S) [P(u∗|uI)] ,
since
P(u∗|u)Q(u|S) = P(u∗|u)P(u\I|uI)Q(uI|S) = P(u∗, u\I|uI)Q(uI|S).
Thus, if I and Q(S|uI) are given, it is straightforward to compute the marginal
Q(uI|S), after which predictions can be done in O(d2), requiring perhaps a pre-
computation of O(d3) (see Section 2.1.3).
On the other hand, the Gaussian posterior approximation can always be writ-
ten as
Q(u|S) = N(u|Ar, A),
A−1 = K−1 + Π,
1We allow for
R
Q(S|u) du not to exist, as long as the resulting posterior can be properly
normalised.

4.2. Likelihood Approximations and Greedy Selection Criteria
135
where Π is symmetric. We can write Π = U DU T where D ∈Rd,d is diagonal
and full rank. In general, neither K nor A has a special structure which could be
used to perform inference in O(n).2 Furthermore, write r = U v + n, U Tn = 0.
Plugging this into (2.14), we end up with
µ(x∗) = kT
∗n + kT
∗U β,
σ2(x∗) = K(x∗, x∗) −
 U Tk∗
T  D + U TKU
−1  U Tk∗

,
(4.1)
where k∗= (K(x∗, xi))i and β ∈Rd.
We see that in order to obtain an expression for the predictive variance which
requires the evaluation of d components of k∗only, we need U = I·,IE for some
index set I of size d and some E ∈Rd,d.
The exact conditions for a sparse
µ(x∗) are less clear, yet it seems that less restrictive constraints than n = 0 and
r = I·,Iv are not feasible to work with. Now, note that Π is exactly the precision
matrix in the Gaussian likelihood approximation Q(S|u), and the constraints
U = I·,IE, r = I·,Iv are equivalent to Q(S|u) depending on uI only. Thus,
given Q(u(·)|u) = P(u(·)|u), a restricted likelihood approximation is necessary
for achieving the desired scaling in general.
It is interesting to note that if a non-sparse predictor (requiring all of k∗) is
acceptable but training has to scale as O(n d2), the restriction r = I·,Iv is not
strictly required, as is shown in Section C.2.2. However, it follows from (4.1) that
the additional d.o.f.’s cannot inﬂuence the predictive variance approximations.
How could we choose a suitable likelihood approximation for a given model?
As an example, note that the EP approximation to a general GP model with
completely factorised likelihood (as discussed in Section 4.3) implies a likelihood
approximation with a diagonal precision matrix Π = diag(πi)i (so do a range of
other GP approximations, see Section 2.1.3 where Π is called D). A straight-
forward idea for a sparse likelihood approximation is simply to clamp most of
the πi to zero. We develop the corresponding sparse method in Section 4.4.1. In
general, the use of non-diagonal Π may be preferable to correct for errors made
by the sparse approximation. The sparse on-line method of [40] (see Section 4.6)
2In spline smoothing with very low-dimensional input spaces (see Section 2.1.5), the relevant
matrices have a banded sparsity pattern which can be used to perform inference very eﬃciently.

136
Chapter 4. Sparse Gaussian Process Methods
is one of the most eﬃcient examples. But what is an optimal form for a sparse
likelihood approximation? Optimality is only deﬁned relatively, yet the relative
entropy (Deﬁnition A.4) between the corresponding posteriors is certainly a sen-
sible criterion.
Lemma 4.1 ([40]) Let P(S|u) be some likelihood and P(u) some prior, such
that the posterior P(u|S) exists. None of them need to be Gaussian. Consider
the set F of positive functions Q(S|uI) of uI with the property that Q(u|S) ∝
Q(S|uI)P(u) exists. We have
argmin
Q(S|
 I)∈F
D [P(u|S) ∥Q(u|S)] ∝EP [P(S|u) | uI]
(4.2)
and
argmin
Q(S|
 I)∈F
D [Q(u|S) ∥P(u|S)] ∝exp (EP [log P(S|u) | uI]) .
(4.3)
If P(u) is Gaussian and P(S|u) is an unnormalised Gaussian w.r.t. u, i.e.
P(S|u) = N U(u | b, Π), then Q(S|uI) ∝N U(E[u|uI] | b, Π) is a maximiser for
(4.3), where E[u|uI] is the conditional mean under P(u). In this case, the max-
imiser of (4.2) also is an unnormalised Gaussian, but is parameterised in terms
of a d-by-d block of the inverse covariance matrix of P(u).
The proof is given in Appendix C.2.1. While the optimal likelihood approximation
in the sense of minimal D[P(u|S) ∥Q(u|S)] is intractable in the context of sparse
methods, the optimal approximation for the criterion D[Q(u|S) ∥P(u|S)] has a
tractable form: the conditional mean is EP[u|uI] = K·,IK−1
I uI, which can be
computed by inverting KI only. If we deﬁne P = K−1
I KI,·, then EP[u|uI] =
P TuI, which can be seen as a reconstruction from an orthogonal projection and
has been termed KL-optimal projection in [38]. In Section 4.4.2, we show how
KL-optimal projections can be combined with expectation propagation to obtain
a powerful sparse approximation method for GP inference.
4.2.2
Greedy Selection Criteria
A likelihood approximation most fundamentally depends on the active set I ⊂
{1, . . . , n}, and a central part of any sparse approximation algorithm is the proce-

4.2. Likelihood Approximations and Greedy Selection Criteria
137
dure for selecting an appropriate I. Finding a globally optimal I (where “optimal”
is deﬁned in any non-trivial way) is a combinatorial problem, so heuristics will
have to be used. We could pick I at random, but this will in general perform
poorly on diﬃcult classiﬁcation tasks if d ≪n. Due to resource limitations, we
will also have to restrict ourselves to methods which do not use active sets of size
> dmax (say) at any time, for example starting with I = {1, . . . , n} and shrink-
ing to a desired size is not an option. Finally, we only consider methods which
change the size of I by at most 1 in each iteration. Advantages include that up-
dates of the posterior approximation can be done easily, and that scoring-based
methods would have to evaluate many more candidates if updates were done in
larger blocks. Amongst such methods, greedy forward selection is the simplest:
elements are included once and for all, without the option to remove them later.
Methods which allow for removals may result in better ﬁnal selections, but are
typically slower than forward selection and in our context are also more likely to
cause numerical problems.
An optimal greedy selection criterion would lead to an active set I such that
the decision boundary of the ﬁnal predictor is close to the optimal one, or at
least close to the one of a corresponding non-sparse predictor. The shape of the
boundary depends most on the patterns close to it, as well as on mis-classiﬁed
ones. Of course, we do not have the option to train a non-sparse predictor and
then identify those points, but we can greedily approximate this goal by including
points which are judged most uncertain (or even mis-classiﬁed) by the current
sparse predictor, in other words prefer patterns which most contradict what we
have learned so far. This idea has been widely used before and is related to active
learning (or sequential design). In this scenario, we assume that a massive amount
of i.i.d. unlabelled data (covariates) is available, from which we are required to
select a training sample as small as possible to learn a classiﬁcation or regression
function. One can distinguish two diﬀerent problems, depending on whether the
labels are available at the time of selection or they are supplied only after each
inclusion decision has been made. The former, the one we are facing, is a practical
one: training on all data is prohibitively expensive. It is also somewhat simpler

138
Chapter 4. Sparse Gaussian Process Methods
than the latter query-based one which has been studied in [53, 108, 174, 58, 31].
An example for the former subset selection scenario is [143]. In [58] it is shown
that a representative unlabelled sample is necessary for successful fast learning,
because otherwise we would typically end up requesting labels at points which
are extremely unlikely to occur (the “Achilles’ heel” mentioned in [108] is related
to this problem).
Methods from these two regimes are often related, since a
query-based scheme can be derived from a subset selection scheme by taking an
additional expectation over the unknown label (for example by sampling based
on the current predictor).
Indeed, the criteria we consider here are the ones
suggested in [108] with the expectation over the chosen target left away.
Let Q(u) be our current belief, based on having included patterns in I, and
let Qnew
i
(u) be the updated belief after including pattern i additionally.
We
can measure the amount of information gained by including i by the decrease in
entropy
∆Ent
i
= H[Qnew
i
] −H[Q]
(4.4)
or by the negative relative entropy
∆Info
i
= −D[Qnew
i
∥Q].
(4.5)
The latter has the interpretation of reduction in coding loss when switching from
a wrong model Q of a source Qnew
i
to the correct one (see Section A.3). The
entropy reduction quantiﬁes by how much the posterior spread shrinks due to the
new inclusion, while the relative entropy more generally quantiﬁes changes in the
posterior. The latter is especially large if the new Qnew
i
puts signiﬁcant weight
into low-density regions of Q. Note that in other work on active learning [174, 58]
a fast shrinkage of the “version space” is the main aim.3 In a certain sense, the
posterior entropy can be seen as proxy for version space volume in the Bayesian
context. MacKay [108] notes that if Q and Qnew
i
are Bayesian posteriors for SI and
SI′ respectively, I′ = I ∪{i}, then E[−∆Ent
i
] = E[−∆Info
i
], where the expectation
is over P(yi|xi, SI). Nevertheless, they can be quite diﬀerent as functions of yi,
especially in the crucial case of yi being diﬀerent than what we would expect
3The version space collects all hypotheses (from a given space) which are consistent with all
data so far. Its volume is measured relative to the full hypothesis space (see [70]).

4.2. Likelihood Approximations and Greedy Selection Criteria
139
from P(yi|xi, SI). We will refer to ∆Ent
i
as diﬀerential entropy score and to ∆Info
i
as (instantaneous) information gain score. Note that both scores are typically
negative and lower values are more desirable.
While the criteria (4.4) and (4.5) have been widely used, it is interesting to
note that while it can be very hard to obtain a useful approximation for them
in the context of nonlinear parametric methods such as multi-layer perceptrons
(MLPs), they are straightforward to compute in the case of Gaussian process
models with the usual Gaussian approximations (see Sections 4.4.1 and 4.4.2.1).
For example, in the case of MLPs Gaussian posterior approximations can be poor,
and selection based on the information criteria which take these approximations
as the truth can suﬀer badly.
In the GP case, the Gaussian approximations
are often quite good, and the criteria (4.4) and (4.5) can be evaluated without
having to make further approximations.4 Using the criteria for query-based active
learning in GP models should be almost equally simple and is subject to future
work. O’Hagan [135] is an early paper discussing optimal design for GP models,
however the assumption that a large pool of unlabelled points is available is not
made, rather the input distribution is considered known. Tong and Koller [193]
give a method for active learning in SVMs (see Section 4.6).
We conclude this section by stressing an important point for subset selection
methods: whatever criterion we use to score inclusion candidates i, we have to be
able to evaluate it fast, indeed signiﬁcantly faster than computing Qnew
i
itself. As
mentioned above, if we had the time to train on all available data or a signiﬁcant
fraction thereof, subset selection would not be required. In the presence of an
expensive selection criterion, all we can do in each iteration is to draw a very
small subsample of the pool and pick the top-scorer from there. Even so, the
training time will then be strongly dominated by the score evaluations, in that
the dominant cost scales with the size of the subsamples. Looking at (4.4) or (4.5),
it seems that Qnew
i
is required to compute them, but this need not be the case (see
Section 4.4.1). If it is indeed, we recommend to look for a cheaper approximation
to these criteria (a general approximation is suggested in Section 4.4.2.1). One
4Cohn et. al. [31] discuss active learning with mixtures of Gaussians and locally weighted
linear regression, for which the criteria can be computed equally easily.

140
Chapter 4. Sparse Gaussian Process Methods
might conjecture that given two sensible criteria, a cheap and an expensive one,
the latter should always be preferred if time is not an issue. This intuition is
wrong in general, as the simple example in Appendix C.2.3 shows.
4.3
Expectation Propagation for Gaussian Process
Models
In this section, we introduce the expectation propagation (EP) scheme [125, 124]
for approximate inference in Gaussian process models. The parametric represen-
tations and update equations of EP are at the heart of the sparse GP algorithms
to be discussed further below. The general EP scheme for exponential families is
described in Appendix C.1.1, and we urge the reader to glance over the material
there. In short, EP builds on the following idea. An intractable (posterior) belief
which can be written as normalised product of positive sites, is to be approxi-
mated by a member of an exponential family F. This is done by replacing each
site which is not in F by a site approximation from the corresponding unnor-
malised family F U (Deﬁnition A.7). The site parameters of these approximations
are updated sequentially and iteratively.
An EP update (Deﬁnition C.2) ex-
changes a site approximation against the true site to obtain a tilted exponential
distribution (Deﬁnition A.6) outside of F. This distribution is projected back
into F preserving the moments w.r.t. which F is deﬁned. Whether or not this
is feasible5 depends crucially on the structure of the sites. The so-called cavity
distribution Q\i is proportional to the current belief Q divided by the i-th site
approximation, and the tilted distribution is proportional to Q\i times the i-th
site.
Suppose now we are given a Gaussian process model with completely fac-
torised likelihood, in the sense that each observed output yi depends on one
corresponding latent output ui = u(xi) only. In this case, we have seen in Sec-
tion 2.1.3 that inference (conditioned on hyperparameters) requires merely the
5Note that the direct projection of the posterior belief onto F is considered intractable,
otherwise an iterative approximation such as EP is not required.

4.3. Expectation Propagation for Gaussian Process Models
141
computation of the posterior P(u|S), where u = (u1, . . . , un)T collects the latent
outputs at the training points S = {(xi, yi)|i = 1, . . . , n}. In this case, we would
like to approximate the posterior
P(u|S) ∝P(u)
n
Y
i=1
P(yi|ui),
where6 the prior is P(u) = N(0, K), K the kernel matrix over the training set,
which matches the situation of Section C.1.1 if F is the family of Gaussians over
u and ti(ui) = P(yi|ui). It is clear that if all sites are Gaussian (like in GP
regression with Gaussian noise, see Section 2.1.2), the posterior can be worked
out directly without the need for an EP approximation.
Note that each site
depends on one component of u only, which allows us to use the locality property
of EP (Lemma C.1). It is in principle straightforward to develop EP for Gaussian
process models in which each site depends on a small number of components of u,
which is the case for example in multi-class GP classiﬁcation (see Section 2.1.2).
For another example, see [38], Sect. 5.4.
Recall the notion of unnormalised Gaussians (Deﬁnition A.10). Due to the
locality property, we can write
˜ti(ui) = N U(ui|bi, πi),
b = (bi)i, Π = diag(πi)i,
where b and Π are the site parameters, thus each site approximation is an unnor-
malised one-dimensional Gaussian. If πi = 0, we always set bi = 0. For reasons of
numerical stability, it is sensible to require that if πi ̸= 0, then it is bounded away
from zero by some threshold. If πi ̸= 0, we also deﬁne mi = bi/πi, the mean of
˜ti(ui) taken as a Gaussian. If πi = 0, we set mi = 0. Note that the EP posterior
approximation Q(u) depends only on 2 n parameters in addition to the kernel
matrix. In fact, we have
Q(u) = N(u|h, A),
A =
 K−1 + Π
−1 , h = Ab
(4.6)
6As an aside, note that this setting does not only include GP models. As has been observed
in [41], it also represents certain independent component analysis (ICA) models in the presence
of Gaussian noise on the observed (mixed) signals. In this case, the ui would correspond to the
latent sources, and the covariance of P(u) is a function of the inverse mixing matrix and the
noise variances.

142
Chapter 4. Sparse Gaussian Process Methods
(using Lemma A.11). The algorithm starts with b = 0, Π = 0, i.e. h = 0, A =
K, Q(u) = P(u), then iterates EP updates (Deﬁnition C.2) until convergence.
In order to do an EP update, we require the marginal Q(ui) = N(hi, ai,i). Details
of how to do the update in the Gaussian case, based on the log partition function
log Zi = log EQ\i[P(yi|ui)], are given in Appendix C.1.2. By the locality property,
the update will only change the site parameters at i.
From the equations in Appendix C.1.2, we see that if log P(yi|ui) is concave in
ui for all possible yi, then the tilted marginal ˆP(ui) always has positive variance.
The new value of πi has the same sign as the second derivative of −log Zi w.r.t.
h\i
i (the mean of the cavity marginal Q\i(ui)), thus is guaranteed to be positive if
the log partition function is concave. Note that if the update results in πi < 0, the
marginal posterior approximation Q(ui) exhibits an increase in variance. Finally,
after bi and πi have been updated, we have to adjust h and A (or at least
diag A) as well. These updates dominate the cost for running EP, and since in
the applications we are interested in K−1 does not follow any sparsity pattern the
burden cannot be eased without further approximations. Since only one element
in Π has changed, the updates can be done in O(n2) using the Woodbury formula
(Lemma A.2). In practice, we would recommend to improve the condition of the
kernel matrix K by adding a small multiple of the identity7 (see [132]). In light
of the deﬁnition of A in (4.6), it is also recommended for reasons of stability to
restrict the components of Π to be nonnegative. In the context of a diﬀerent
approximation problem, Minka [125] reports that such restrictions can lead to
worse results, but for GP binary classiﬁcation we have not even encountered the
case of a negative πi so far.
EP for Gaussian process models has been proposed in [125, 124]. In fact, it is
not hard to see that ﬁxed points of EP in this case coincide with such for the cav-
ity TAP approximation suggested earlier by Opper and Winther [139]. While the
latter authors derived a set of self-consistent equations from a statistical physics
ansatz which was then solved by iteration, EP provides a simpler, typically some-
what faster and more stable algorithm for this purpose. However, the algorithm
7Note that the problem of ill-conditioned K does not plague the sparse GP approximations
to be developed below.

4.4. Sparse Gaussian Process Methods: Conditional Inference
143
has time complexity O(n3) and requires O(n2) of memory, which is prohibitive
for large training set sizes n. As we will see, it nevertheless provides the basis for
the much more eﬃcient sparse algorithms to be developed below.
4.4
Sparse Gaussian Process Methods: Conditional
Inference
In this section, we discuss two schemes for sparse approximate Bayesian infer-
ence on GP models, conditional on a ﬁxed set of hyperparameters. Both can be
applied in principle to arbitrary likelihoods, although we concentrate here on the
case of completely factorised likelihoods, in which the sites are arbitrary positive
functions of one component in the latent vector u each. The informative vector
machine (IVM) is the simpler of the both, and the basic scheme can be seen as
a simpliﬁed version of the sparse on-line method of [40]. However, the IVM is
a greedy algorithm, thus algorithmically quite diﬀerent from an on-line scheme,
and its key merits as compared to other sparse methods are high eﬃciency and
simplicity of implementation. An early version has been proposed in [97], while
the fully developed scheme (as presented here) appears in [98]. The projected
latent variables (PLV) algorithm is a more expensive and potentially more accu-
rate scheme. It can be seen as greedy version of the batch algorithm presented in
[38, 41], where instead of randomly running over the training data and including
new points in an on-line fashion, we try to obtain a more eﬃcient scheme by
ordering candidates in a heuristic manner. This is quite a bit more challeng-
ing than for the simpler IVM, but the more accurate likelihood approximation
employed in PLV may render the method more suitable for model selection (see
Section 4.5.1).
4.4.1
The Informative Vector Machine
In a nutshell, the informative vector machine (IVM) employs a likelihood ap-
proximation (see Section 4.2.1) which follows from using EP (see Section 4.3) but

144
Chapter 4. Sparse Gaussian Process Methods
leaving most of the site parameters clamped at zero. The patterns for which ADF
updates are done, are chosen using greedy forward selection based on diﬀerential
criteria (see Section 4.2.2). The chief merits of the method are its simplicity and
high eﬃciency.
As mentioned in Section 4.2.1, a simple way of obtaining a likelihood approxi-
mation in the context of the EP approximation for GP models is to clamp most of
the site parameters to zero, i.e. πi = 0, bi = 0 for all i ̸∈I. Here, I ⊂{1, . . . , n}
will be called the active set8, |I| =: d < n. As discussed in Section 4.2.2, we
can use greedy forward selection to select new patterns to be included into I:
compute a score function for all points in a selection index J ⊂{1, . . . , n} \ I and
pick the winner.
What representation do we have to maintain and update in order to run the
IVM scheme? It is easy to see that if I was chosen purely at random, a repre-
sentation of Q(u) suﬃcient for these training updates and for future predictions
would require O(d2) parameters only, and the training algorithm would run in
time depending on d only. However, such random selection strategies can perform
very poorly especially for the case of diﬃcult binary classiﬁcation or regression
estimation tasks and d ≪n. For a greedy selection strategy, it seems obvious
that in order to make any statement about the potential value of an inclusion
of point i, we at least need to know the current marginal Q(ui). Although a
minimal O(d2) representation can provide this marginal on demand, the cost for
the evaluation is O(d2), which is prohibitive if selections are done from a large
set J of candidates. A representation which allows the eﬃcient update of the
marginals Q(ui) for a large number of points i ̸∈I, and which can be updated
in a numerically stable way after inclusions, is described in Appendix C.3.1. The
storage requirements are dominated by one d×n matrix. For a ﬁxed active set I,
the posterior approximation Q(uI) is N(KIΠ1/2
I L−Tβ, KIΠ1/2
I B−1Π−1/2
I
), with
β = L−1Π−1/2
I
bI
(4.7)
and B = LLT as deﬁned in (C.4). Recall that bI, ΠI are the parameters of the
8I should not be confused with active sets as deﬁned in the context of constrained optimisa-
tion problems (see [56]). While the latter can be found in polynomial time for convex problems,
the task of choosing I in some “optimal” way is typically of combinatorial complexity.

4.4. Sparse Gaussian Process Methods: Conditional Inference
145
active site approximations. The predictive distribution Q(u∗|x∗, S) is obtained
by averaging P(u∗|uI) over Q(uI), resulting in a Gaussian process with mean
and variance
µ(x∗) = βTm∗,
σ2(x∗) = K(x∗, x∗) −∥m∗∥2,
m∗= L−1Π1/2
I kI(x∗),
(4.8)
where kI(x∗) = (K(x∗, xi))i∈I (see Equation 2.14). If the predictive variance is
not required, it is sensible to pre-compute the prediction vector ξ = Π1/2
I L−Tβ,
then µ(x∗) = ξTkI(x∗). Finally, the predictive target distribution Q(y∗|x∗, S) is
obtained by averaging P(y∗|u∗) over Q(u∗|x∗, S). For binary classiﬁcation and
probit noise P(y∗|u∗) (2.9), we have
Q(y∗|x∗, S) = Φ
 
y∗(µ(x∗) + b)
p
1 + σ2(x∗)
!
,
(4.9)
where b is a bias hyperparameter.
For other noise models P(y∗|u∗), the one-
dimensional integral can be approximated by Gaussian quadrature (see Sec-
tion C.1.2).
In fact, for commonly used noise models, the Bayes classiﬁer is
sgn(µ(x∗)+b), independent of σ2(x∗). This follows from the fact that Q(u∗|x∗, S)
is symmetric around µ(x∗). If we have the option of rejecting a fraction of test
points, the decision should be based on the size of |Q(y∗|x∗, S) −1/2|. More gen-
erally, if an uneven loss function (in the decision-theoretic sense) is appropriate
for our task, our decision should be the one minimising the posterior risk (i.e.
expected loss).
An important fact which distinguishes the IVM likelihood approximation from
other more expensive ones, is that knowledge of the marginal Q(ui) is also suf-
ﬁcient for evaluating a range of sensible selection scores at i in O(1): a direct
consequence of the locality property of EP on models with factorised likelihood
(Lemma C.1). Recall the diﬀerential entropy score and the information gain score
from Section 4.2.2. Due to the locality property, the former is simply the diﬀer-
ence between the marginal entropies before and after inclusion (see Lemma C.1),
i.e.
∆Ent
i
= H[Qnew(ui)]−H[Q(ui)] = 1
2 log λnew
i
ai,i
= 1
2 log(1−λiνi)+ 1
2 log λi
ai,i
, (4.10)

146
Chapter 4. Sparse Gaussian Process Methods
where Q(ui) = N(hi, ai,i) (recall the notation and deﬁnitions from Section 4.3 and
Appendix C.1.2). Given that the marginal is known, this term can be computed
in O(1). Note that ∆Ent
i
is proportional to the reduction in log variance from
Q(ui) to Qnew(ui). The information gain score can be computed equally easily,
by again exploiting Lemma C.1:
∆Info
i
= −D[Qnew(u) ∥Q(u)] = −D[Qnew(ui) ∥Q(ui)].
Using Equation A.17 we have
∆Info
i
= −D[Qnew(ui) ∥Q(ui)] = 1
2

log λnew
i
ai,i
−λnew
i
ai,i
−α2
i ai,i −1

,
(4.11)
where we have used that hnew
i
−hi = αiai,i (see Appendix C.1.2). Again, this is
O(1), given the marginal moments hi, ai,i. From (4.8), we see that the compu-
tation of Q(ui) hinges on knowledge of column i of matrix M = L−1Π1/2
I KI,·
(Equation C.5), the so-called “stub” for i. This completes the description of the
IVM scheme (Algorithm 1).
Algorithm 1 Basic informative vector machine algorithm
Require: Desired sparsity d ≪n, threshold ε > 0 (numerical stability).
I = ∅, b = 0, Π = diag(0), diag A = diag K, h = 0, J = {1, . . . , n}.
repeat
for j ∈J do
Compute ∆j = ∆Ent
j
(Equation 4.10) (or ∆j = ∆Info
j
(Equation 4.11)).
end for
i = argmaxj∈J{∆j | πnew
j
> ε}
Here, πnew
j
, bnew
j
are computed by ADF update (see Appendix C.1.2).
Site updates: πi ←πnew
i
, bi ←bnew
i
.
Update representation as shown in Appendix C.3.1, notably L, M , diag A,
h.
I ←I ∪{i}, J ←J \ {i}.
until |I| = d
The representation described in Appendix C.3.1 allows us to keep all n marginals
Q(ui) up-to-date at any time during the algorithm, at a cost of O(n d) per inclu-

4.4. Sparse Gaussian Process Methods: Conditional Inference
147
sion, leading to a total cost of O(n d2) with a small constant. In this setup, in order
to decide which point to include next, we pick the top-scorer among all points
outside of I. This strategy seems wasteful, especially during later iterations when
the impact of new inclusions on Q diminishes. In a randomised greedy version
of the algorithm, the selection index J is a strict subset of {1, . . . , n} \ I during
later iterations, and only the marginals corresponding to these points have to be
available for scoring. We refer to this procedure as randomised greedy selection
(RGS). In Appendix C.3.1, we describe an updating scheme for the representa-
tion of Q which exploits this selection constraint by delaying the computation of
stubs as long as their patterns are not in the selection index. The more inertia
the selection index J exhibits between iterations, the cheaper the computational
cost becomes. This can be supported by retaining a fraction τ ∈(0, 1) of the
top-scorers from the previous iteration in J, ﬁlling it up with new points drawn
at random. Our options range from “conservative” (large J, small τ) to “risky”
(rather small J, large τ). During early iterations, it seems sensible to use full
greedy selections, not only because early inclusions typically have a larger impact
on the ﬁnal Q than later ones, but also because a complete update of all marginals
is much cheaper while the active set is still small. Our present implementation
allows memory usage to be limited to O(n dlim) with dlim smaller than the ﬁnal
active set size. For very large n, it would become necessary to add reﬁnements
like a cache hierarchy, see Section C.3.1 for some ideas.
Finally, we may want to address the problem of how many points d to include
into the ﬁnal active set. First of all, one of the important advantages of the IVM
over other “sparse” methods such as SVM (see Section 2.1.6) is that d can be con-
trolled. The algorithm will always remain within given resource constraints, and
its running time is predictable a priori. Apart from this, a number of heuristics
may be used to select an appropriate d from a given resource-constrained range.
For example, the error on an independent validation set can be monitored using
predictions via (4.8), requiring O(m d) if m is the number of validation points
(one would not have to compute the error after each inclusion). Another idea is
to monitor the error or predictive log likelihood on the remaining training sam-

148
Chapter 4. Sparse Gaussian Process Methods
ple S\I outside the current active set. This typically underestimates true error,
because the selection criteria prefer patterns which are wrongly classiﬁed by the
current predictor, but it may nevertheless be useful for deciding when to stop.
Evaluating such stopping heuristics empirically is subject to future work.
In the IVM scheme, the decision to include i into I is ﬁnal. It is easy to
modify the scheme to allow for “exchange moves”: for i ∈I, j ̸∈I, such a move
exchanges j against i in I, using an EP update step to modify the site parame-
ters. In Appendix C.3.2, we give details about exchange moves, showing how the
representation is updated and how the diﬀerential scores can be modiﬁed to score
“exchange pairs” (i, j). Exploring the usefulness of exchange moves in practice
is not in the scope of this thesis and subject to future work. Potential diﬃculties
include numerical instabilities when updating the representation and the much
larger number of exchange candidate pairs to be scored in every iteration.
Finally, the reader may wonder why we do not run EP updates until conver-
gence once the ﬁnal active set I has been chosen. Indeed, we did so in preliminary
experiments, but frequently noted a decline in performance after the additional
EP update iterations. We do not have a conclusive explanation for this eﬀect
in the moment, but can merely oﬀer the plausible argument that subsequent EP
iterations will indeed improve the approximation to the posterior P(u|SI) for SI
only, thereby worsening the approximation to the full posterior P(u|S).
4.4.1.1
Running Time Complexity
One attractive feature of the IVM scheme, in contrast to other well-known kernel
methods such as SVM is that the running time requirements can be estimated
and controlled a priori. The IVM scheme without RGS requires O(n d′) for each
inclusion with current active set size d′, thus O(n d2) in total. The dominating
operation during an inclusion is a matrix-vector multiplication with a matrix of
size n × d′. For RGS, the dominating operations are “stub updates”: it needs
O(d′) in order to extend a stub from size d′ to d′+1, so the cost per inclusion is at
least O(|J| d′) where |J| is the current selection index. The exact cost depends on
the details of the setup, notably the retain fraction τ and the cache mechanism

4.4. Sparse Gaussian Process Methods: Conditional Inference
149
for the stubs, but is well below O(n d′) if |J| ≪n.
In practical applications involving numerical mathematics, much can be gained
by organising computations such that the dominant inner loops consists of simple
vector operations such as (x, y) 7→xTy or (x, y) 7→x + αy, with as large vec-
tors as possible. Here lies another advantage of IVM over schemes such as SVM
trained with the popular SMO algorithm [142]. The dominating computation
for IVM without RGS consists of a single O(n d′) matrix-vector multiplication9
per inclusion, while SMO is typically dominated by a very large number of el-
ementary operations, or even covariance function evaluations, which cannot be
vectorised eﬃciently. If IVM is run with RGS and |J| ≪n, not much larger than
d, the size of vectorisation is decreased. Individual stub updates require inner
products of vectors of size d′. If |J| ≫d, it makes sense to bundle these to attain
vectorisation of size |J|, but this is not supported by our implementation in the
moment.
High eﬃciency becomes crucial when the conditional inference described here
is used as subroutine for model selection (see Section 4.5.2). Measurements show
that often the time requirements for conditional inference dominate the total time
for computing the model selection criterion and its gradient, especially if RGS
is involved. One reason for this is that the remaining dominating computations
for the gradient can be vectorised eﬃciently. Eﬀorts to speed up conditional in-
ference should concentrate on reducing the organisatorial overhead for the stub
cache, or to bundle stub updates in some way to increase the vectorisation size.
Our implementation includes one feature worth mentioning. Recall that for small
initial active set sizes d′, the selection index is J = {1, . . . , n}. To obtain eﬃcient
vectorisation, we require the rows of M to be accessible as linear vectors. Later
on during RGS, columns of M need to be accessed as linear vectors to do eﬃ-
cient stub updates. Therefore, once RGS is activated the principal buﬀer matrix
(storing the full M initially, and being fed by the stub cache later) has to be
transposed. Since this matrix can be large, it is sensible (or even essential) to do
this transposition in place, i.e. without requiring a second large buﬀer. In place
9This is a BLAS-2 operation [51], for which the BLAS suite provides highly eﬃcient imple-
mentations for many architectures. Vector-vector operations are supported via BLAS-1.

150
Chapter 4. Sparse Gaussian Process Methods
transposition of non-square matrices is non-trivial, but eﬃcient algorithms exist
[26]. Certainly, other speed-up techniques could be applied to our code10, but we
leave this for future work.
4.4.2
Projected Latent Variables
In contrast to the IVM which employs a factorised approximation to the like-
lihood P(S|u), the PLV scheme uses an approximation in which each latent
variable ui, i ∈I is coupled with all training datapoints, based on the so-called
KL-optimal projection motivated by Lemma 4.1. If P(S|u) itself is an unnor-
malised Gaussian w.r.t. u, say ∝N U(u|b, Π), the corresponding likelihood ap-
proximation is N U(E[u|uI] | b, Π), where E[u|uI] = P TuI the prior conditional
expectation (recall that P = K−1
I KI,·).
If P(S|u) is not Gaussian, we can
embed the mapping via P T into a scheme which renders Gaussian posterior ap-
proximations (such as EP), since P does not depend on the training targets.
Apart from its “KL-optimality”, the approximation can intuitively be under-
stood as a modiﬁcation of the sampling model for the targets y. Namely, we draw
uI ∼P(uI), u\I ∼P(u\I|uI), but instead of sampling y ∼P(y|u), we replace
u\I in this conditional distribution by the mean of the distribution P(u\I|uI) it
has been sampled from. We have seen in Section 4.2.1 that in order to obtain a
sparse approximation, we have to constrain the likelihood to depend on uI only,
and the method of replacing the remaining variables u\I in the likelihood by their
conditional mean11 seems in a way least intrusive.
When compared with IVM, the PLV scheme is somewhat more complicated to
develop and implement and its running time and memory requirements are larger.
Furthermore, in the non-Gaussian likelihood case tensions might arise from the
fact that the two principle underlying approximations (KL-optimal projections
and EP) are “incompatible” in the sense discussed below, and ﬁnally suitable
forward selection criteria are harder to compute and additional approximations
will be required. Where is the potential gain of preferring PLV over IVM? From
10The present version does not incorporate highly eﬃcient numerical packages such as BLAS
[51] or LINPACK, but this will be done in the future.
11The best linear predictor (in the least-squares sense).

4.4. Sparse Gaussian Process Methods: Conditional Inference
151
the experiments we have done to compare the methods (Sections 4.8.2 and 4.8.3)
we can see that PLV can work successfully (especially in conjunction with auto-
matic model selection) with smaller active set sizes d. An intuitive explanation is
that due to the coupling in the PLV likelihood approximation points in the active
set can summarise the information of neighbouring points more accurately. In
IVM, points outside the active set inﬂuence predictions only in the weak sense of
shaping the selection of I, while in PLV every point takes inﬂuence on predictions
directly (the ones in I do so most strongly).
Due to the more elaborate likelihood approximation, representations and up-
dates are somewhat more complicated for PLV than for the IVM, and the ﬁnal
algorithm is more expensive to run. However, very signiﬁcant simpliﬁcation can
be obtained for the special case of GP regression with Gaussian noise (see Sec-
tion 2.1.2). In the latter special case, we can ﬁx b = σ−2y and Π = σ−2I, while
in general these site parameters have to be adjusted iteratively using EP updates.
We have to address the following issues:
1. What representation allows stable and eﬃcient updates, both to include
new points into I and to update site parameters? How to do predictions
based on this representation?
2. How to update the site parameters bi, πi of a point i which may or may not
be in I? How to include a point i into the active set I?
3. How to score a point i w.r.t. the potential value of including it into I?
Before delving into the details, it is helpful to state the key diﬀerences to the
simpler IVM scheme which will serve to motivate the diﬀerences in representation
and updates. First, in PLV every ui, i ∈I is coupled with all n sites.
This
means that each time a new index i is included into I, there is an incentive to
update all O(n) site parameters. Second, the coupling is via the matrix P =
K−1
I KI,·, namely γ = E[u|uI] = P T uI. The coeﬃcients γi are called pseudo-
variables (or projected latent variables) and fulﬁl roughly the same role as the ui
themselves in the IVM scheme. Once u is replaced by γ, the mechanics of PLV
become quite similar to that of IVM. For example, in order to update the i-th

152
Chapter 4. Sparse Gaussian Process Methods
site approximation, we will require the marginal Q(γi) (instead of Q(ui) in IVM).
Some form of P will have to be maintained explicitly in order to map between γ
and uI. Note that γI = uI. ADF updates in PLV are done in the same way as
in IVM, but using pseudo-variables γi instead of ui and ”blurred sites”
˜ti(γi) =
Z
ti(ui)P(ui|uI) dui =
Z
ti(ui)N(ui|γi, l2
i ) dui,
where l2
i = Ki,i −pi = Ki,i −Ki,IK−1
I KI,i (pi is introduced below).
Again, a suitable representation which allows for eﬃcient and stable updates,
is at the core of the algorithm. We have already noted that a form of P has
to be stored. Please note that overlaps with the notation used for the IVM are
unavoidable for reasons of simplicity. Thus, whatever variables are deﬁned for
the inner representations of IVM or PLV, these are meant to be local deﬁnitions
and are not to be transferred to other contexts. For ﬁxed I, let KI = LLT be
decomposed into Cholesky factors, and let
V = L−1KI,·,
M = I + V ΠV T.
Note that P = K−1
I KI,· = L−TV , thus V is nothing but a convenient represen-
tation of the projection matrix. In practice, we would recommend using a jitter
factor in the covariance function (see [132]), which leads to K being replaced by
K + τI for a small τ > 0. This makes sure that the Cholesky factor L can be
computed without problems. Also γ = V TL−1uI, so that
Q(u) ∝P(u)N U(b|γ, Π).
(4.12)
It is easy to see that Q(uI) = N(LM −1V b, LM −1LT). Let M = QQT be
decomposed into Cholesky factors. Also, deﬁne β = Q−1V b. Then, the posterior
can be written as
Q(uI) = N
 uI | LQ−Tβ, LM −1LT
.
The predictive posterior is now obtained from (2.14). Namely, for a new datapoint
x∗the predictive mean µ(x∗) and variance σ2(x∗) are
µ(x∗) = (lM
∗)Tβ,
σ2(x∗) = K(x∗, x∗) −∥l∗∥2 + ∥lM
∗∥2,
l∗= L−1k∗,
lM
∗= Q−1l∗,
(4.13)

4.4. Sparse Gaussian Process Methods: Conditional Inference
153
where k∗= (K(x∗, xi))i∈I. Thus, prediction is O(d2), yet requires two back-
substitutions12 instead of only one as for the IVM. Note that if the predictive mean
is required only, one should pre-compute13 the prediction vector ξ = L−TQ−Tβ,
after which µ(x∗) = k(x∗)Tξ is O(d).
If the likelihood P(yi|ui) is not Gaussian, we have to adjust the site parameters
iteratively so as to obtain a good Gaussian approximation to the true posterior.
In PLV, this is done by EP updates. Deﬁne
p = diag V TV ,
µ = V TQ−Tβ,
q = diag V TM −1V .
(4.14)
The update formulae are derived in Appendix C.4.1.
We have already noted
above that they are virtually identical to the updates for IVM once u is re-
placed against the pseudo-variable vector γ and the true site ti(ui) against the
“blurred site” ˜ti(γi). Note the meaning of the components in (4.14): P(ui|uI) =
Q(ui|uI) = N(ui|γi, l2
i ) with l2
i = Ki,i −pi, and Q(γi) = N(γi|µi, qi). Note that
while p is independent of the site parameters, µ and q are not, and in PLV
the components of the latter are computed on demand. A change in the site
parameters will modify M , so that Q and related variables have to be updated.
Since M new = M + (πnew
i
−πi)vivT
i , where vi = V ·,i, this can be done using the
technique described in Section A.2.2. Finally, in order to include a new point i
into I, the representation has to be updated, Appendix C.4.1 contains the details.
Immediately after the inclusion, the site parameters of i should be updated. In
general, when updating site parameters after the inclusion of i, the ones of i itself
should probably be visited several times.
Note that Var[γ] = V TM −1V , measuring the covariance between pseudo-
variables γj.
Deﬁne C = Q−1V , then Var[γ] = CTC.
If ci = C·,i, then
µi = cT
i β, qi = ∥ci∥2, so columns of C play an important role in updating and
scoring (they are the equivalents of “stubs” in IVM). However, as opposed to V ,
12For maximum eﬃciency, this can be reduced to a single Cholesky factor after training. The
most numerically stable way of doing this uses the singular value decomposition (SVD) of M ,
possibly treating eigenvalues very close to 1 in a special way.
13In the case of full GP inference, Gibbs [63] reports stability problems with this pre-
computation approach.
Again, we would expect that these problems are alleviated in the
case of sparse approximations, especially if a jitter factor is used for KI.

154
Chapter 4. Sparse Gaussian Process Methods
C cannot be maintained and updated incrementally, its columns are computed
on demand.
Note that if we were to update all site parameters after each inclusion of a new
point into I, this would cost O(n d2) per inclusion, thus leading to a prohibitive
overall cost of O(n d3). Csat´o [38] circumvents this problem by choosing an on-line
like scheme, cycling at random over the data and deciding individually based on
some threshold whether to include a point or merely update its site parameters.
In our scheme, site parameter updates are separated from inclusion decisions,
the latter are based on greedy selection. After each inclusion, a set of points
L, I ⊂L, is picked at random and only their site parameters are updated. The
details can be found in Appendix C.4.1, the cost is O(d2) for each update, so
O(|L| d2) for a sweep over all sites in L.
Finally note the subtle diﬀerence in how the scheme is run if the site param-
eters are ﬁxed from the beginning (as in GP regression with Gaussian noise). If
we were to update the site parameters for some i ̸∈I in the same way as in the
general case, they would become diﬀerent from their ﬁxed values even though the
true likelihood is Gaussian. This is because the KL-optimal projection to obtain
the likelihood approximation is not perfectly compatible with the EP scheme.14
Another consequence is that if we adapted site parameters using EP in this set-
ting, the ﬁnal Q would be a worse approximation to the true (Gaussian) posterior
in the sense of KL-optimality (due to Lemma 4.1) than if we ﬁxed them (which
is what we do).
4.4.2.1
Scoring Points for Inclusion
Can the selection heuristics discussed in Section 4.2.2 be applied for PLV as
well? For the IVM, these diﬀerential criteria turned out to depend on the old
and new marginal distributions at the point in question only, a consequence of
the locality property of EP with a factorised likelihood. In PLV, the likelihood
(approximation) is not factorised anymore, all sites depend on all of uI and are
14Recall from Section 4.2.1 that the KL-optimal approximation uses an e-projection (see
Section A.4.2), while EP uses m-projections into the Gaussian family.

4.4. Sparse Gaussian Process Methods: Conditional Inference
155
therefore coupled. In order to score a point with one of the criteria, we have to
spend the same time as would be required to include the point (which is dismissed
as infeasible in Section 4.2.2).
We can make progress nevertheless by considering approximations to the cri-
teria we used for the IVM. The problem is that once i is included, ui is coupled
with all sites immediately. A simple approximation would be to ignore all these
couplings except with site i. To be speciﬁc, let S(Q, Qnew) be the full criterion,
where Q is the current approximate posterior and Qnew is obtained from Q by
inclusion of i. S(Q, Qnew) cannot be computed faster than updating the rep-
resentation for Q →Qnew. The cavity distribution Q\i is obtained from Q by
removing site i together with the pseudo-variable γi = (P TuI)i. Deﬁne ˜Q(u), an
approximation to Qnew(u) as
˜Q(u) ∝Q\i(u)N U(ui|˜bi, ˜πi),
(4.15)
where ˜bi, ˜πi are obtained by an ADF update with i added to I. As is detailed in
Section C.4.2, these are in general diﬀerent from bi, πi even if the latter have just
been updated. Furthermore, ˜S = S(Q, ˜Q).15 In contrast to S, ˜S can be computed
in O(1) given our representation, as is shown in Appendix C.4.2 for the special
case of the information gain criterion (the derivations for other criteria should be
similar). In fact, ˜S may be a rather coarse approximation to S in many cases,
however the vastly reduced cost is a strong argument to explore its usefulness
more thoroughly.
There are intermediates between ˜S and S. For example, we can choose a small
index set H with H ∩I′ = ∅, where again I′ = I ∪{i}, set H′ = H ∪{i} and use
˜Q(u) ∝Q\H′(u)N U(˜bH′
 (P ′
·,H′)TuI′, ˜ΠH′),
Q\H′(u) ∝P(u)N U  b\H′
 (P ·,\H′)TuI, Π\H′
.
(4.16)
Here, P ′ ∈Rd+1,n denotes the projection matrix after inclusion of i, and ˜bH′, ˜ΠH′
may be diﬀerent from the old values bH′, ΠH′ deﬁning Q.
For small H, this
15It seems that we have ignored couplings here between site i and the uI. However, note that
once i is included into I and the likelihood approximation depends on ui, there is no (direct)
coupling between these variables, because P ·,I = I.

156
Chapter 4. Sparse Gaussian Process Methods
criterion can still be computed feasibly, as is shown in Appendix C.4.3. Note
that the full information gain criterion is obtained for H = {1, . . . , n} \ I ′, and
the coupling-free one for H = ∅. Of course, this raises new questions, for example
how to choose H or whether and how to update the parameters bH′, ΠH′ under
the new coupling before evaluating the score. Some ideas towards the former are
discussed below. It seems sensible to update the site parameters for H ′ using EP
steps before evaluating the criterion, otherwise they may be poorly adapted to the
new situation and the increased coupling may not pay oﬀat all. This is not hard
to do, the details are developed in Appendix C.4.3. If |H′| ≤d, the dominating
computation for these updates and the score computation is the evaluation of
C·,H′ (we need the covariance of γH′ under Q), which is O(|H′| d2). This holds
even if |H′| > d. We also require the elements Ki,H′ of the kernel matrix and
|H′| elements of a new row of V . If |H′| > d, the cost of O(|H′| d2) soon becomes
prohibitive.
However, note that for the special case ˜bH′ = bH′,
˜ΠH′ = ΠH′
without updating, the cost of evaluating the criterion drops to O(|H ′| d).
When using the simple coupling-free approximation, it is more eﬃcient to
interleave score computations and site parameter updates. After the inclusion
of a new point, we ﬁrst update the site parameters for the new active set I
together with some randomly chosen points from J. However, in the subsequent
systematic update run over J we compute the score for each pattern immediately
after the update of its site parameters. This is slightly problematic since we are
now comparing score values based on diﬀerent site parameter settings, but it
leads to a signiﬁcant gain in eﬃciency.
4.4.2.2
Overview of the Algorithm
In this section, we give an overview of the algorithm if the simple coupling-free
score approximation is used (see Section 4.4.2.1). The algorithm consists of two
stages. In the ﬁrst stage, the active set I is chosen by greedy forward selection
until a desired size d is attained. In the second stage, the site parameters b, Π
are thoroughly updated (within resource constraints). The ﬁrst stage consists
of iterations each of which leads to the inclusion of a new pattern into I. It is

4.4. Sparse Gaussian Process Methods: Conditional Inference
157
initialised by picking a few points at random for an initial I and setting b =
0, Π = diag 0. Algorithm 2 shows the schema for one iteration.
Algorithm 2 PLV: First-stage iteration, leading to the inclusion of one pattern.
Require: Representation for current active set I up-to-date.
if Site parameters not ﬁxed then
Pick L ⊂{1, . . . , n}, I ⊂L.
for j ∈L do
Update site parameters for j (see Appendix C.4.1).
end for
Pick J ⊂{1, . . . , n}, J ∩I = ∅.
for j ∈J do
Update site parameters for j. Compute coupling-free score approximation
∆j = S(Q, ˜Q).
end for
else
Pick J ⊂{1, . . . , n}, J ∩I = ∅.
for j ∈J do
Compute coupling-free score approximation ∆j = S(Q, ˜Q).
end for
end if
i = argmaxj∈J ∆j
Update representation as shown in Appendix C.4.1.
I ←I ∪{i}
The initial update run over L is required because the site parameters have
not been updated since the last recent inclusion into I. During the run over L,
patterns from I (and especially the one included last recently) should probably
be visited several times. If the resource limits permit it, we would recommend
L = I ∪J. Note that score computations and site updates are interleaved. For
reasons of stability, we update the site parameters for i immediately after its
inclusion into I, and if πnew
i
< ε for a threshold ε > 0, the inclusion is rejected.
It is sensible to give J some “inertia”: J for a new iteration should contain a

158
Chapter 4. Sparse Gaussian Process Methods
fraction of the top-scorers of the previous iteration. As long as d as small, we can
use L = {1, . . . , n} and J = {1, . . . , n} \ I. The cost for scoring all patterns in J
is O(|J|d2).
4.4.2.3
The Extended Score Approximation
Extensions of the simple coupling-free selection score approximation have been
motivated in Section 4.4.2.1 and are derived in Appendix C.4.3. While their use
might lead to better selections of I, they require additional algorithmic details
to be worked out and an exploration of this option in practice is left for future
work. Nevertheless, we collect some remarks here.
The extended approximations require that a “neighbourhood set” H is given
for each pattern i. Ideally, H is chosen such that uH is correlated most strongly
with ui under the posterior, but computing this optimal neighbourhood struc-
ture is infeasible. A simple idea is to determine the structure a-priori, based on
correlations under the GP prior. Given that the choice of the neighbourhood
structure plays a subdominant role within PLV, any eﬃcient clustering technique
based on prior correlation as aﬃnity measure will probably be suﬃcient. In order
not to compromise our running time requirements, we are limited to hierarchical
clustering schemes which represent subclusters by appropriate suﬃcient statis-
tics. If the conditional inference algorithm is embedded into model selection it is
sensible to re-select the structure now and then, since it depends on the kernel
hyperparameters.
In the extended case, it is also more eﬃcient to interleave parameter updates
and score computations. For j ∈K to be scored let H be its neighbour set and
H′ = H∪{j}. We compute C·,H′, update the parameters of sites H′ (j comes last)
while keeping C·,H′ up-to-date. Finally, the extended score is computed, passing
C·,H′. Again, the interleaving implies that we compare inclusion candidates based
on diﬀerent site parameter conﬁgurations, but keeping parameter updates and
scoring separate would be much less eﬃcient. K is chosen such that the overall
number of required C column computations is acceptable. It should contain a
fraction of top-scorers from the previous iteration.

4.5. Model Selection
159
The special case where site parameters are ﬁxed from the beginning (Gaussian
likelihood) is much simpler. C·,H′ is not required for the score computation (see
Appendix C.4.3.1), all we need is one back-substitution with Q. Even this can
be saved if C is maintained explicitly (see Appendix C.4.1).
4.5
Model Selection
So far, we have discussed inference conditioned on free hyperparameters. A dis-
tinctive advantage of GP methods over many other kernel algorithms is that
hyperparameters can be adapted by Bayesian marginal likelihood maximisation
(see Sections 2.1.3 and A.6.2), allowing the use of kernels with a large number
of free parameters. In this section, we show how model selection can be done for
sparse GP approximations.
4.5.1
Model Selection for PLV
We follow the generic model selection scheme outlined in Section 2.1.3 (the hy-
perparameters are denoted by α). Note that the PLV scheme combines posterior
approximations based on diﬀerent optimality criteria. The EP scheme is based
on moment matching, and an approximation to the marginal likelihood should
be consistent with EP, i.e. ﬁxed points of the EP scheme should correspond to
saddle points of the approximation. For EP applied to full GP models, such a
criterion can be constructed by generalising the Bethe free energy [215] as an
analogous criterion for (loopy) belief propagation (see [123]). For the case of a
Gaussian ﬁeld with dense prior P(u) and completely factorised likelihood P(S|u)
(as in Section 4.3), this criterion has been given in [41] and called ADATAP free
energy. However, since PLV also uses KL-optimal projections which are based on
e-projections, the ADATAP free energy does not apply in the same strong way
to the sparse case. The usage of KL-optimal projections suggests the standard
variational lower bound (A.18) as approximation, which is simpler to derive and
handle than the ADATAP free energy.
From the derivation in Section 2.1.3 and the fact that Q(u\I|uI) = P(u\I|uI)

160
Chapter 4. Sparse Gaussian Process Methods
it is easy to see16 that
G = EQ [−log P(y|u)] + D[Q(uI) ∥P(uI)] ≥−log
Z
P(y|u)P(u) du.
Here,
EQ [−log P(y|u)] = −
n
X
i=1
EQ [log ti(ui)] ,
Q(ui) = N(ui|µi, qi + l2
i )
by integrating out uI. In practice, we minimise the sum of G and negative log
priors for the hyperparameters, the latter are suppressed here. Since Q(uI) =
N(LQ−Tβ, LM −1LT), P(uI) = N(0, LLT), we can use Lemma A.13 to obtain
D[Q(uI) ∥P(uI)] = 1
2

log |M | + tr M −1 −d +
Q−Tβ
2
.
(4.17)
The gradient of G w.r.t. parameters of the covariance function and the noise
model is derived in Appendix C.4.4.
The complete PLV scheme including model selection works as follows. In an
outer loop, we minimise G w.r.t. the hyperparameters, using a gradient-based op-
timiser such as Quasi-Newton (see [55]). The optimiser requires evaluations of the
criterion and its gradient for diﬀerent purposes: to select new search directions,
and to drive a line search along a ﬁxed search direction. These computations are
done by calling PLV conditional inference as a subroutine (see Section 4.4.2.2)
whenever a new search direction is to be determined. During line searches, the
conﬁguration I, b, Π is kept ﬁxed. This optimisation scenario is discussed in
more detail in Section 4.5.3.
If the true likelihood is Gaussian (say N(y|u, Π−1)) we can use a simpler
approximation:
−log P(y) ≈−log EP

N(y|γ, Π−1)

,
P(γ) = N(γ|0, V TV ).
(4.18)
This criterion which we use in our experiments in Section 4.8.2 is not equivalent
to the variational G (details about the gradient computation for this criterion can
be found in [165]). If the true likelihood is not Gaussian, this approximation does
not apply because the likelihood approximations from EP need not correspond
to a proper conditional distribution over the observables y.
16Note that we minimise an upper bound here instead of maximising a lower bound, in order
to remain compatible with standard formulations of optimisation algorithms.

4.5. Model Selection
161
4.5.2
Model Selection for IVM
In comparison to PLV, the IVM approximation to the true posterior is rather
crude and model selection may suﬀer from this fact. On the other hand, IVM can
be applied to signiﬁcantly larger tasks for which principled approximate Bayesian
model selection is rarely used.
We can in principle run the same scheme as for PLV above (Section 4.5.1):
maximisation of a variational lower bound on the log marginal likelihood. How-
ever, doing so straightforwardly requires the complete M matrix (Equation C.5),
so that randomised greedy selection (RGS) is not applicable.
We can retain
the randomisation advantage if we approximate the criterion even further. Let
L ⊂{1, . . . , n} with I ⊂L. We use G in principle, but replace the true posterior
by ∝P(yL|uL)P(u), thus ignoring the data outside of L. If L = {1, . . . , n},
this becomes the full criterion. Ideally, L is chosen such that the corresponding
datapoints determine the shape of the true posterior most. Luckily, the greedy
selection strategies we use for IVM are following exactly the same goal, so we can
use information gathered during the selection of I in order to choose L. Recall
from Section C.3.1.1 that RGS works by storing initial parts (stubs) of columns
of M . Whenever a pattern is to be scored, its stub has to be complete. Thus,
if the stub cache is organised to (ideally) contain long stubs of patterns which
render the best score values (or are likely to do so), it is sensible to include the
points with largest available stubs into L. Of course, this also minimises the time
required to complete M ·,L which we need for the criterion computation. Again,
an important advantage of the IVM scheme is that the running time of an iter-
ation can be controlled by limiting the size of L. The details are developed in
Appendix C.3.3.
4.5.3
Details about Optimisation
In this subsection, we comment on how criteria of the approximate log marginal
likelihood type suggested above can be optimised using standard techniques.
The model selection problem as setup in this section boils down to minimise
a function f(α) w.r.t. the vector of hyperparameters α which is assumed to

162
Chapter 4. Sparse Gaussian Process Methods
be a real unconstrained vector.17
However, if ω collects all site parameters,
f(α) = F{g(·; α)}, where F operates on g(ω; α) by choosing the site parameters
conditioned on a ﬁxed α using either PLV or IVM. Strictly speaking, F even has
some random element, and because its computations involves the choice of I we
cannot assume smooth behaviour w.r.t. α.
Even if the active set I is ﬁxed, it seems very hard to compute the gradient
∇f(α) because the mapping α 7→ω is complicated. Thus, a straightforward
gradient-based optimisation of f is not attractive. Our approach is to construct
a sequence (ω(t), α(t)) as
α(t+1) = argmin
 g(ω(t), α),
ω(t+1) = F{g(·; α(t+1))},
where the minimisation is a line search along a search direction constructed from
the gradient sequence g(t) = ∇
 (t)g(ω(t), α(t)), using a standard method such as
conjugate gradients or Quasi-Newton (see [55]). This is motivated by g(t) being
an approximation to ∇f(α(t)) (see Section 2.1.3).
This approach is closely related to variational EM, another variant of lower
bound maximisation (see Section A.6.4). However, the latter case has nicer prop-
erties, due to the fact that F{g(·; α)} = argmin
  g(ω; α). First, the gradients
g(t) are exact in this case, furthermore every step is guaranteed to decrease the
objective f, given that the search directions have positive inner product with the
gradients there. Neither of these properties is true in general for our approach
here. If
f (t) = f(α(t)) = g(ω(t); α(t)),
we might encounter f (t+1) > f (t). In practice, oscillatory behaviour is observed
during later stages and convergence to high relative accuracy cannot be expected.
We note that both variational EM and our approach here do not minimise f
during line searches, but g(ω(t), ·).18 Thus, methods for constructing the sequence
of search directions which rely on fairly accurate line minimisations, such as
17Positivity constraints can be enforced by using log transformations.
18Can we make line searches more eﬃcient by updating ω in between? This might lead to
problems in practice, because the dependence of ω on α is ignored during gradient computa-
tion, so during the line search ﬁnite diﬀerences for small steps will not match corresponding
derivatives (projections of the gradient).

4.5. Model Selection
163
conjugate gradients, can only be used in a modiﬁed form. For example, more
frequent restarts (with the gradient as search direction) are recommended. Quasi-
Newton methods are known to rely less on accurate line searches.
However, even if our approach does not share the monotone convergence prop-
erties (to a local minimum) with variational EM, the optimisation problem is nei-
ther a very hard one. Search directions based on the gradient approximations g(t)
seem to work eﬃciently in practice. Moreover, our optimisation code incorporates
a recover logic which restarts whenever a line search fails or a proposed direction
is not a descent one. A critical problem in practice is the choice of a suitable
stopping criterion. The relative improvement in criterion value for successive it-
erations is not a suitable measure in this context due to oscillatory behaviour. If
we choose two lags T1 ≤T2 > 0 and deﬁne
M (T,t) = min{f (t−T +1), . . . , f (t)},
we can monitor
∆(t) = M (T1,t−T2) −M (T1,t)
|M (T1,t−T2)|
.
For a non-increasing sequence f (t), ∆(t) is non-negative for any T1, T2. In our
context, “convergence” can be assessed based on the size and sign of ∆(t) for not
too small lags, possibly in combination with checking for small ∥g(t)∥. Any such
stopping criterion in practice means trading oﬀunnecessarily long running time19
against the danger of early spurious convergence (see Section 4.8.4 for examples).
An empirical study to ﬁnd an eﬃcient stopping criterion for the model selection
problems of this section is subject to future work.
To conclude, the fact that standard optimisation packages may not be directly
applicable to the model selection problem, or may result in unnecessary ineﬃ-
ciencies is certainly a negative aspect of our strategy. On the other hand, the
modiﬁcations required are relatively minor: essentially, it has to be possible to
use diﬀerent criteria during line searches than for computing new search direc-
tions and assessing convergence, and robust behaviour in the presence of minor
19Given that the optimisation behaves stable close to an optimum. This was the case in all
our experiments.

164
Chapter 4. Sparse Gaussian Process Methods
oscillations is needed. Furthermore, nowadays oﬀ-the-shelf programming pack-
ages are rarely used to solve the problems of very particular structure posed by
kernel methods such as SVM, because specialised code can run much faster.
4.6
Related Work
Sparse approximations to kernel methods have received a lot of recent attention
in the machine learning community. In this section, we review schemes which are
related to the work presented in this chapter.
As already mentioned above, our methods are most closely related to the work
of Csat´o and Opper. The PLV scheme diﬀers from the algorithm in [41] in that
we use greedy forward selection to decide about inclusions, while the latter iter-
ates over the data in an on-line fashion (although the algorithm is not an on-line
scheme), deciding inclusions or removals based on approximation error criteria.
Model selection based on minimising the ADATAP free energy is not reported
in [38], but has been done since then. Greedy forward selection can be more
eﬃcient than a random traversal approach if an eﬃcient selection criterion is able
to pick out informative points early. Also, schemes which repeatedly remove and
include points may be prone to numerical instability, since matrix update for-
mulae like Woodbury (Lemma A.2) are notoriously ill-behaved especially if the
rank-one addition is negative deﬁnite. The IVM is related to the sparse on-line
scheme of [40, 39], but is simpler and slightly more eﬃcient. Csat´o and Opper
[40] use a seemingly more direct representation than ours (see Section C.3.1).
However, both representations are equivalent in terms of running time, storage
requirements and numerical stability, while ours has the advantage of allowing
for randomised greedy selections which can speed up the algorithm signiﬁcantly
(see Section C.3.1.1). Because a factorised likelihood approximation is used, the
greedy selection criteria of Section 4.2.2 can be computed in O(1) (given the rep-
resentation) for the IVM. In contrast, the on-line scheme uses a likelihood which
couples the sites and therefore may lead to more accurate approximations. The
IVM is not an on-line scheme. Csat´o [38] applies the sparse GP approximation

4.6. Related Work
165
technique to a wide range of applications, including binary classiﬁcation, density
estimation and wind-ﬁeld prediction.
The special case of PLV for regression with Gaussian noise [165] is related to
sparse greedy GP regression [183] and Hybrid Adaptive Splines [107]. All these
methods use greedy forward selection to choose patterns (or their corresponding
“basis functions” K(·, xi)) for I. Given the model and the active set I, the opti-
mal predictor can be derived straightforwardly, thus the important diﬀerence for
sparse greedy GP regression methods lies in the selection criterion. Both Smola
and Bartlett [183] and Luo and Wahba [107] use what we called expensive criteria
in Section 4.2.2: the scoring of a point scales in the same way as its inclusion,
namely O(n d). In our opinion, this is prohibitively expensive, since in each round
only a small number of candidates can be scored, and even then the total training
time is strongly dominated by the score computations. In contrast, our approxi-
mate information gain score (see Sections 4.4.2.1, C.4.2, C.4.3) can be evaluated
in O(h d),20 where h can be chosen anywhere between 1 and n, or even in O(1)
if all couplings for the new point are ignored. Furthermore, neither Smola and
Bartlett nor Luo and Wahba consider generalisations to arbitrary GP models, and
both recommend cross-validation schemes to select free hyperparameters which
can be prohibitive if there are many. The Smola/Bartlett scheme can easily be
described in our notation by transforming to dual variables21 α = K−1u (recall
our notation from Section 4.4.2). Their scheme is a sparse MAP approximation,
in that they minimise
π(α) = 1
2αT(σ2K + KTK)α −yTKα
= −(V y)TL−1uI + σ2
2 uT
I (LM −1LT)−1uI,
where α is sparse in the sense that α\I = 0.
π(α) is proportional to the
negative log posterior −log P(uI|y) and is minimised by the posterior mean
σ−2LM −1V y, the minimum value is
πI = min
 π(α) = −σ2
2 βTβ.
20Note that in the regression case, we do not have to update site parameters.
21The α here has nothing to do with the hyperparameter vector α of Section 4.5.

166
Chapter 4. Sparse Gaussian Process Methods
Their selection heuristic for a point i ̸∈I is
∆SB
i
= πI −πI′ = σ2
2 β2
d+1,
I′ = I ∪{i},
i.e. the decrease that can be obtained in π(α) by allowing component i in α to
become non-zero. As shown in Section C.4.1, the computation of βd+1 requires
two n · d computations. Smola and Bartlett consider a randomised scheme in
which ∆SB
i
is evaluated only over a small random subset of size k (the authors
recommend k = 59), leading to a training complexity of O(n k d2) which is k
times more expensive than PLV (for regression). Also, since the random subsets
ﬂuctuate freely over {1, . . . , n}, one typically ends up evaluating a large fraction of
the full kernel matrix K which is problematic if kernel evaluations are expensive.
In contrast, PLV requires K·,I only (I the ﬁnal active set).
A number of authors suggest using low-rank matrix approximations to end
up with sparse versions of conventional kernel algorithms [181, 208, 211, 54, 129,
134, 200]. As mentioned in Section 2.1.3, many GP approximations are in terms
of a matrix (K−1 + D)−1, where D is diagonal and positive, the same holds for
some SVM solvers. Given an active set I ⊂{1, . . . , n}, both Smola and Sch¨olkopf
[181] and Williams and Seeger [208, 211] suggest replacing K by the low-rank
approximation K·,IK−1
I KI,· = V TV , giving diﬀerent theoretical justiﬁcations
for this choice (see also Section 4.7.1). Then,
 K−1 + D
−1 ≈
 V TV + D
−1 = D−1 −D−1V T  I + V D−1V T−1 V D−1,
requiring a d-by-d Cholesky decomposition only. Plugging this approximation
into a given algorithm renders a scaling of O(n d2). While Williams and Seeger
suggested a random choice of I (see also [61, 129]), Smola and Sch¨olkopf use
greedy forward selection with the Frobenius norm as criterion (this is also an
expensive criterion in the sense of Section 4.2.2). A diﬀerent approach is used
in [54], where a Cholesky decomposition of the target matrix (e.g., K) with row
pivoting22 is stopped early to obtain a low-rank approximation. The advantage
of this method is that ﬁnding the largest pivot is very fast, it corresponds to
22Row pivoting is the essential feature here. Note that pivoting is not required for a complete
Cholesky decomposition, since this would not improve numerical stability.

4.6. Related Work
167
greedy forward selection with the criterion tr(K −˜K) if ˜K denotes the low-rank
approximation. Although this trace criterion is always non-negative, it is not very
clear what it means in terms of approximation quality (the authors present some
bounds which are however extremely loose). In contrast, the Frobenius norm
tr(K −˜K)2 used in [181] comes with strong approximation guarantees, but does
not lead to an eﬃcient method. The incomplete Cholesky approximation has
been used in [8], and variants are routinely used in interior-points code for linear
programming [212]. As already noted in [107], all these methods share a common
problem: their choice of I does depend only on the input datapoints, the target
values y are completely ignored (in fact, the low-rank decompositions can be
computed without knowing y).
A powerful technique for sparse Bayesian GP learning is Tipping’s relevance
vector machine (RVM) [192, 52].
The idea is to consider a kernel expansion
u(x) = wTk(x) and sparsify the weight vector w ∈Rn using automatic rel-
evance determination (ARD) ([131]; see also Section 2.1.8). By placing a prior
P(w|α) = N(w|0, diag α−1) on w and an improper uniform prior on the compo-
nents of log α, then learning α by maximising the hyperposterior P(α|S) (this is
a variant of marginal likelihood maximisation, see Section A.6.2) one ﬁnds that
on real-world data many of α’s components tend to ∞, thus ﬁxing the corre-
sponding components of w (in the approximate posterior) to 0. The original
RVM cannot be considered a sparse approximation technique in the sense of Sec-
tion 4.1, because it requires O(n3) training time (it is actually more expensive to
train than most methods mentioned in Section 2.1.3), but later reﬁnements have
shown how to do RVM training incrementally [52]. The authors recommend a
joint maximisation of the marginal likelihood (or the posterior) w.r.t. the spar-
sity hyperparameters α and additional kernel parameters, although they do not
provide experimental results in [52]. The sampling mechanism used to motivate
the RVM does not correspond to a proper GP model (or in fact to any process
model). It implies f.d.d.’s N(u|0, K(diag α)−1K) for u(x) conditioned on α(x)
which are not consistent in the sense of Kolmogorov’s criterion (see Section 2.1.1).
Our view on RVM is that once w is integrated out, the search for α coeﬃcients

168
Chapter 4. Sparse Gaussian Process Methods
is a particular way to construct a sparse Gaussian posterior approximation (i.e.
α parameterises the posterior approximation just as for example D does in Sec-
tion 2.1.3). When viewed like this, RVM falls into the same class as the schemes
described in this chapter, although the Bayesian ARD motivation for selecting α
is weakened then. It is not clear a priori how the selection of I via greedy max-
imisation of the marginal likelihood compares with the information-based criteria
of Section 4.2.2, an experimental comparison may be required to gain clarity here.
A version of RVM for binary classiﬁcation is suggested in [192], using a Laplace
approximation (see Section 2.1.3). An older scheme related to RVM is the subset
of regressors (SR) model [200], where given I we consider the sparse kernel ex-
pansion u(x) = wTkI(x), P(w) = N(0, K−1
I ). From a Bayesian viewpoint, this
model is unusual in that the prior on w depends on the input points and also on
the selection of I, i.e. a particular detail of inference approximation. In fact, the
predictive variance Var[u(x)|S] tends to 0 far from the training inputs for many
isotropic standard kernels, in stark contrast to proper GP methods for which the
variance becomes maximal then. To be fair, the SR model was probably never
intended to be a Bayesian one. The ﬁnite-dimensional Bayesian formulation for
the generalised spline smoothing method of Green and Silverman discussed in
Section 2.1.5 suﬀers from the same problem.
The Bayesian committee machine (BCM) of [194] is a diﬀerent approach, in
that the training sample S is split into a number of blocks of size d (say), then the
n/d predictors based on the subsamples are combined in a somewhat Bayesian
way.
The argument is that conditioned on some query points which may be
the test set or a part of it, the subsamples become approximately conditionally
independent. Under this assumption, the committee renders an approximation
to the joint posterior over the query points. The advantage of the BCM over
other sparse methods discussed so far may be that all data is actually used for
prediction. It does not render an approximation to the predictive distribution
which is a Gaussian process. One could use a single test point as query set, in
which case however the conditional independence assumption seems too strong.
Otherwise, prediction requires bundling of test points into query sets which can

4.6. Related Work
169
be inconvenient. Furthermore, in predictions we are usually interested only in
marginal distributions at test points, while the BCM tries to approximate the
whole joint distribution over the query points. The BCM scales cubically in d
and either in the size of the query set or in n/d. The inﬁnite mixture of Gaussian
process experts [148] is another method which uses a committee of small GP
predictors. It is an MCMC method, based on sampling from a Dirichlet process
(see also [147]), thus in theory leads to correct Bayesian inference in the limit.
However, on high-dimensional real-world data we would not expect the Markov
chain on this model to mix in reasonable time, for example the incentive of an
indicator variable to switch experts should be very low.
The authors present
experimental results on low-dimensional data only.
Tong and Koller [193] consider active learning strategies for support vector
machines (SVMs). It has been noted (see Section 4.2.2 or [38], Sect. 5.2) that
sparse GP approximations like IVM or PLV could be applied to active learning
rather easily, due to the simple forms of the criteria of Section 4.2.2. In contrast to
this, Tong and Koller’s version space23 oriented approach seems fairly elaborate.
The MaxMin approach seems interesting, but requires retraining the SVM twice
(including one more point) in order to evaluate the criterion.
Sparse approximations of the solution to smoothing spline problems (see Sec-
tion 2.1.5) is discussed in [134], see also [200], Chap. 7. The optimisation of the
penalised risk functional is restricted to functions in the span of a ﬁnite number
of basis functions which are chosen in a data-dependent way. Once a given set
of “representative” input points24 si is chosen (of size much smaller than n), the
basis functions are constructed as linear combinations of the K(·, si), where K
is the kernel for the RKHS. This coincides in principle with the methods dis-
cussed in this chapter as long as the si are chosen from the training set (the
BCM includes test input points as well), which is arguably a good strategy if
little is known about the input point distribution. For special spline kernels K
23In this context, the version space is the part of the hypersphere corresponding to the normal
vectors of all hyperplanes which classify all data points correctly.
24The si ∈X need not come from the training set and are therefore denoted by diﬀerent
symbols.

170
Chapter 4. Sparse Gaussian Process Methods
and low-dimensional input spaces, the basis function can be chosen to have com-
pact support (so-called B-splines), which can lead to advantages in the numerical
optimisation if the si are spread out.
The approximation of a GP by a linear model with less than n components
has been studied in [135, 216]. The former method requires O(n3) computations,
while Zhu et. al. [216] propose an a priori optimal approximation based on the
Mercer eigen-expansion of the kernel (see Section 2.1.4) and the input distribution
(which is assumed to be known).
Finally, as mentioned in Section 2.1.6 GP priors have been used to drive
methods from two quite diﬀerent paradigms, the probabilistic Bayesian one and
the large margin discriminative one. While in this chapter we have focussed on
sparse Bayesian GP learning, the same ideas can be applied to sparse variants of
large margin algorithms such as SVM. For example, the framework developed in
this chapter could be applied straightforwardly to the MRED variant of SVM (see
Section 2.1.6). Indeed, since we do not have to worry about estimating predictive
probabilities, the resulting method would even be somewhat simpler to implement
(although we may argue that by estimating these probabilities we can use more
eﬀective scores to select new inclusion candidates). For the standard SVM, such
incremental learning has been proposed in [27]. The idea is to restore the KKT
conditions (see Section 2.1.6 and [161]) after including a new point along with its
dual variable using “adiabatic” increments in order to retain these conditions on
all other dual variables while working towards satisfying them on the new pattern
as well. The method seems to rely crucially on the dual to be a quadratic function
so that its gradient is linear in the dual variables which allows to compute the
exact eﬀects of non-inﬁnitesimal variations.25 Since the authors aim to learn all
support vectors eventually, they do not report using forward selection strategies,
but it would be straightforward to use (normalised) margin-based ones (Tong and
Koller [193] suggest criteria for SVM active learning which could be adapted).
However, the information-theoretic criteria we have used here are not applicable,
25Note that for the same reason, the cross-validation score (or generalised CV score) can
be computed without re-training n times for the generalised spline smoothing procedure of
Section 2.1.5, while this is not possible for general penalised likelihood methods.

4.7. Addenda
171
due to the failure of the SVM to estimate predictive variances.
4.7
Addenda
In this section, we collect some additional material which complements the main
parts of this chapter.
4.7.1
Nystr¨om Approximations
In this section, we brieﬂy review the basic ideas of [211, 208] and some related
work, details can be found in these references.
Recall from Section 2.1.4 that for every positive deﬁnite kernel K, there exists
a uniquely determined Hilbert space (HS) HK admitting K as reproducing kernel
(RK): the reproducing kernel Hilbert space (RKHS) for K. Also recall the eigen-
expansion of K (2.19) in terms of orthonormal eigenfunctions in L2(X ). We are
free in choosing the weighting measure for L2. For a positive density function
p(x) on X consider the modiﬁed inner product
(f, g)p =
Z
p(x)f(x)g(x) dx = E
  [f(x)g(x)].
An eigen-expansion of K exists if tr K2 =
R R
p(x)p(y)K2(x, y) dxdy < ∞. The
Hilbert space H with this inner product is isometrically isomorphic to L2(X ) (with
Lebesgue measure) under the mapping f 7→p1/2f. It is easy to see that if K is
the RK on H, then ˜K(x, y) = p(x)1/2K(x, y)p(y)1/2 is the RK on L2(X ).
In the usual applications of Gaussian processes or spline smoothing methods,
it is not necessary to know the eigen-representation of the kernel K w.r.t. any
inner product, but if the original underlying variational problem is to be approx-
imated by concentrating eﬀorts on a lower-dimensional span of basis functions,
the eigenfunctions of K become important. In order to be able to estimate the
eigenfunctions from data without certain knowledge of the true data distribution
p(x), we use H as above:
λiφi(y) = E
  [K(x, y)φi(x)] ≈1
n
n
X
k=1
K(xk, y)φi(xk)
(4.19)

172
Chapter 4. Sparse Gaussian Process Methods
for large n. A standard approach is to plug in y = xj to obtain the n-dimensional
eigenproblem KU = U Λ, where U TU = I and Λ = diag(ˆλj)j, ˆλ1 ≥ˆλ2 ≥
· · · ≥0. We can use this to approximate φi(xj) ≈√nU i,j,
λi ≈n−1ˆλi, where
we have used n−1 P
k φi(xk)φj(xk) ≈E
  [φi(x)φj(x)] = δi,j. We would expect
this approximation to be most accurate for small i and deteriorate for i close to
n. Plugging this back into (4.19), we obtain the Nystr¨om approximation
φi(y) ≈ˆφi(y) =
√n
ˆλi
n
X
k=1
K(y, xk)U k,i =
√n
ˆλi
U T
·,ik(y),
where k(y) = (K(xj, y))j. It is known that for any ﬁxed k, n−1ˆλk →λk almost
surely as n →∞, for details see [9].
Why would a knowledge of the dominating eigenfunctions be useful for approx-
imation? Recall the Karhunen-Loeve expansion (2.20) for a zero-mean Gaussian
process u(x) with covariance K. We note that most variability in this prior en-
semble is concentrated on the space of eigenfunctions of the leading eigenvalues,
given that the spectrum decays reasonably quickly. In [208] it is argued that at
least in the GP regression setting, the coeﬃcient uν of eigenfunction φν in the
(predictive) posterior process is “damped” to be close to zero with high prob-
ability if nλν is smaller than the noise variance, at least if the data is indeed
sampled from this model. Thus, the eigenfunctions associated with the leading
eigenvalues will typically dominate predictive estimates, therefore are prime can-
didate for building a basis for a reduced approximation.26 A more thorough and
explicit analysis is given in [216]. Both the spectrum and the eigenfunctions de-
pend crucially on the distribution p(x) of the data, and leading entries can often
be estimated reliably from large samples as mentioned above. In [208], the de-
pendence of spectrum and eigenfunctions on p(x) is illustrated in 1-d for the case
of the Gaussian kernel (2.29) and p(x) a mixture of well-separated Gaussians.
The Nystr¨om approximation suggests approximating the GP with covariance
K(x, y) by a linear model with basis functions ˆφi(y), i = 1, . . . , p, p ≪n. This
26This is essentially the same argument as used to motivate PCA.

4.7. Addenda
173
is equivalent to approximating the kernel function by
p
X
i=1
ˆλi ˆφi(x)ˆφ(x′) = k(x)TU ·,1...pΛ−1
1...pU T
·,1...pk(x′),
and it means that the kernel matrix over the training data is approximated by
U ·,1...pΛ1...pU T
·,1...p, which is closest to K in Frobenius norm27 among all rank-p
matrices. This approximation is somewhat related to the method suggested in
[63], where a Lanczos algorithm is used to ﬁnd U ·,1...p approximately. The problem
is that ﬁnding U ·,1...p is hard if p is not extremely small, and in any case requires
at least O(n2 p) and computation and storage of the complete K. In [211], it was
suggested to use the Nystr¨om approximation once more to approximate U ·,1...p
based on a subset {xi | i ∈I}.
After an eigen-decomposition of KI ∈Rd,d,
the remaining values U \I,1...p are ﬁlled in using the approximation ˆφi(xk), i ∈
{1, . . . , n} \ I. If p = d, this is equivalent to replacing the covariance matrix K
by K·,IK−1
I KI,· which can be motivated from many other angles as well (for
example, see Section 4.4.2). The index set I was picked at random in [211], in
contrast to our emphasis on greedy forward selection strategies in this chapter.
Low-rank approximations of kernel matrices are used in [8] to provide sample-
based eﬃcient approximations to the mutual information. Their accuracy hinges
on a suﬃciently fast decay of the kernel matrix eigenspectrum, and the authors
review some work which tries to quantify the decay. For stationary kernels, the
rate of decay of the covariance operator spectrum depends on the tail behaviour of
the spectral density of K (see Section 2.1.1) and of the input density. Both [208]
and [8] contain empirical studies comparing the theoretical decay rates of the
operator eigenvalues with the Nystr¨om approximations computed from actual
kernel matrices and both ﬁnd that the approximations tend to underestimate
the process eigenvalues. The theoretical rates for stationary kernels in higher
dimensions predict a much slower decay than in 1-d. However, since real-world
data in high dimensions often lies close to a low-dimensional manifold, this does
not necessarily imply a slow decay of kernel matrix spectra for such data.
Shawe-Taylor and Williams [175] provide some concentration results which
27The Frobenius norm is ∥A∥2 = (tr AT A)1/2.

174
Chapter 4. Sparse Gaussian Process Methods
lead to gap bounds between matrix and process eigenvalues. The idea is to re-
late sums of leading eigenvalues of covariance matrix and covariance operator,
using their respective Courant-Fisher characterisation. Recall from Section 2.1.4
that if an eigen-decomposition of K in H with inner product (f, g)p is given, the
RKHS admitting K embedded in H can be characterised via the Fourier coeﬃ-
cient vectors of f ∈H, namely fi = (f, φi)p = E[f(x)φi(x)] and f = (fi)i. By the
Courant-Fisher min-max theorems, we can relate n−1 Pk
i=1 ˆλi for leading eigenval-
ues of K with Pk
i=1 λi for leading process eigenvalues. The latter is the maximum
over all k-dimensional subspaces of the feature space of E[∥P (ψ(x))∥2], where P
denotes the projection onto the subspace, and ∥f ∥2 = f Tf in feature space. The
maximising subspace is spanned by the leading k eigenfunctions φi, i = 1, . . . , k.
To relate this to the matrix eigenvalues, we observe that if X = (ψ(x1) . . . ψ(xn))
is the data matrix in feature space, then K = XTX and XXT have the same
leading eigenvalues, and for the latter we can use the Courant-Fisher theorem
in feature space just as in the population case, simply replacing expectations
over p(x) by such over the empirical distribution ˆp(x) = n−1 P
j δ
 j(x). The
maximising subspace is diﬀerent from the one in the population case and can be
constructed from the matrix eigen-decomposition. The min-max characterisation
now allows to infer various inequalities, and for the ﬁxed subspace based on the
process eigenfunctions, we can use concentration results to bound the diﬀerence
between empirical and population expectation.
4.7.2
The Importance of Estimating Predictive Variances
In this section, we focus on the binary classiﬁcation problem. Recall from Sec-
tion 2.1.3 that the class of approximations to Bayesian inference for GP models
we are interested in here aims to approximate the posterior process by a Gaussian
one. This second-order approach naturally leads to predictions of the variance
of this process (2.14) quantifying local uncertainty in the posterior mean predic-
tion28 µ(x∗). On the other hand, valid estimates for predictive uncertainty are
28If there is a bias parameter b, we assume it has already been added to µ(x∗) in the sense
that our point predictions are sgn µ(x∗).

4.7. Addenda
175
not obtained from large margin discriminative methods (Section 2.1.6) such as
SVM. The SVM discriminant output (say s(x∗)) could be related to µ(x∗) since
decisions in SVM and IVM are based on sgn s(x∗) and sgn µ(x∗) respectively, but
since SVM does not constitute a proper probabilistic model (see Section 2.1.6)
there is no reason why s(x∗) should be a useful posterior mean prediction. In
fact, the SVM loss has some peculiar characteristics which are essential for a
large margin discriminative method (see Section 2.1.6): it is not diﬀerentiable at
+1, has zero curvature elsewhere and is ﬂat zero for y s ≥1. Even if it corre-
sponded to a negative log likelihood, we would not consider it a sensible model
for classiﬁcation noise. The ability to produce valid uncertainty estimates comes
with additional cost: both training and prediction time are somewhat longer for
Bayesian GP methods than for comparable SVM techniques (at least for binary
classiﬁcation).
Platt [140] proposed to obtain predictive probabilities by ad-hoc post-processing.
Namely,
Pr{y = +1|x, Q} = σ(a s(x) + b)
where σ(x) is the logistic function (see Equation 2.8). The scalars a, b are ﬁtted
post-hoc by regularised maximum likelihood using some “unbiased version” of the
training set. Clearly, these outputs are in (0, 1) and can be sold as probabilities,
but the overall validity of the method remains unclear.
First, if the SVM is
assumed to perform well it is not sensible to choose b ̸= 0, otherwise the obvious
decision rule based on the predictive probabilities would be diﬀerent from the
SVM decision (and usually perform worse). Next, if a > 0 (we suppose this is
intended) the predictive probability is strictly increasing in s(x). If we deﬁne
m(x) = |Pr{y = +1|x, Q} −1/2|
(4.20)
then our uncertainty in a point prediction decreases with increasing m(x). But a
ranking of a test set w.r.t. this uncertainty is exactly the same for m(x) and for
|s(x)| (given that b = 0). Thus, for ranking as one of the most important appli-
cations of predictive probabilities (see below) Platt’s estimate does not provide
anything new. In fact, all “order-related” problems of |s(x)| are fully transferred

176
Chapter 4. Sparse Gaussian Process Methods
to m(x). Apart from these obvious problems, a number of additional heuris-
tics are required to make it work in practice. Tipping [192] pointed out similar
weaknesses.
Point predictions without uncertainty estimates would be considered an in-
complete analysis by many statisticians, even for classiﬁcation.
On the other
hand, predictive probabilities are important for all but the simplest decision
problems. For example, they are required if decisions result in unequal losses
(or utilities). A very important argument for proper probabilistic models is that
they can be combined and integrated in a simple and consistent way in larger
systems, simply by the consistency of elementary probability laws.29 Here, we
concentrate on a simple decision problem in which we have a reject option for
an α fraction of a set of test points. We use the experimental results described
in Section 4.8.1 for the hardest binary task c = 9 against the others. Figure 4.4
shows an error-reject curve for IVM and SVM (obtained by averaging curves over
10 runs). From about α = 1/20, the error rate after rejection is signiﬁcantly
lower for IVM. Also (not shown here), the variability in the curves over the 10
runs is larger for SVM. Recall that the test points are ranked using |s(x∗)| for
SVM and m(x∗) (4.20) for IVM. For the probit noise model (2.9), if we deﬁne
r(x∗) =
µ(x∗)
p
1 + σ2(x∗)
then
m(x∗) = |Φ(r(x∗)) −Φ(0)| ,
and since Φ(x) −1/2 is odd the IVM ranking can be based on |r(x∗)| as well.
Note the explicit role of the predictive variance σ2(x∗) here: the larger σ2(x∗)
(thus our uncertainty in µ(x∗) as prediction of E[u(x∗)]), the more r(x∗) is shrunk
towards 0. If we believe that SVM provides a good prediction of the posterior
mean, it should lead to a similar ranking than one based on |µ(x∗)| for the IVM.
Figure 4.1 shows error-reject curves for all three ranking criteria.
We see that SVM still performs worst, but is closer to the (also invalid)
ranking via |µ(x∗)| for IVM. On the other hand, even w.r.t. the induced test set
29A simple example is multi-class classiﬁcation by using a C-process model with independent
GP priors (see Section 2.1.2).

4.7. Addenda
177
0
0.05
0.1
0.15
0.2
10
−4
10
−3
10
−2
Rejected Fraction
Test Error Rate
IVM
SVM
IVM(mean)
Figure 4.1: Test error rate against increasing rejection rate for MNIST, 9 vs. rest
(averaged over 10 runs).
orderings they behave quite diﬀerently, giving no evidence for the usefulness of
s(x∗) as estimate of µ(x∗). The role of the predictive variances can be understood
more clearly by plotting |µ(x∗)| and σ(x∗) for misclassiﬁed points x∗only in the
ordering given by r(x∗) (i.e. by the predictive probabilities), as is done (for one
of the runs) in Figure 4.2.
We see that for this selection of points, large wrong values of |µ(x∗)| are
often accompanied by large values of σ(x∗). While the former alone lead to a
suboptimal ordering w.r.t. the rejection scenario, their combination r(x∗) behaves
better. Figure 4.3 shows the relationship between µ(x∗) and σ(x∗) over the whole
test set (we sorted the points w.r.t. their predictive mean value). The right hand
plot shows mean ± one standard deviation computed by local averaging.30
The region µ(x∗) ≈0 is sparsely populated due to the simplicity of the task.
For µ(x∗) ≤−5 there is a noisy roughly linear relationship between predictive
mean and standard deviation, but this does not hold in the area of real interest
µ(x∗) ≈0. Here, σ(x∗) ﬂuctuates more strongly than for larger |µ(x∗)| and we
30Using a box window of size 100. The run is the same as used for Figure 4.2.

178
Chapter 4. Sparse Gaussian Process Methods
0
10
20
30
40
50
60
0
1
2
3
4
5
6
7
Pred. mean
Pred. std.dev.
Ratio r( x)
Figure 4.2: Absolute predictive mean |µ(x∗)|, standard deviation σ(x∗) and ratio
r(x∗) for misclassiﬁed points (IVM, MNIST, 9 vs. rest).
−15
−10
−5
0
5
10
0
1
2
3
4
5
6
Predictive Mean
Predictive Std.dev.
−15
−10
−5
0
5
10
0
1
2
3
4
5
6
Predictive Mean
Predictive Std..dev.
Figure 4.3: Predictive std.dev. against predictive mean over all test points (IVM,
MNIST, 9 vs. rest). Right plot shows mean ± one standard deviation computed by
local averaging.
have seen above that these ﬂuctuations are not simply random, but systematically
“correct” the predictive means w.r.t. ranking.
To conclude, we have shown that predicting posterior variances in an approxi-

4.8. Experiments
179
mate Bayesian way does not only provide a more complete analysis by quantifying
uncertainty, but can be quite essential for decision-theoretic setups beyond dis-
crimination without a reject option. On the other hand, uncertainty estimates
come at small but non-negligible extra cost when compared to optimised large
margin discrimination methods.31
4.8
Experiments
In this section, we give results for a range of experiments testing the IVM and
PLV schemes on real-world tasks.
4.8.1
IVM Classiﬁcation: Digit Recognition
The material in the ﬁrst part of this section has previously appeared in [98].
We used the MNIST handwritten digits database32, comparing the IVM (see
Section 4.4.1) against the SVM algorithm (see Section 2.1.6).
We considered
unbalanced binary tasks of the form ‘c-against-rest’, c ∈{0, . . . , 9}. c is mapped
to +1, all others to −1. We down-sampled the bitmaps to size 13×13 and split the
MNIST training set into a (new) training set of size n = 59000 and a validation
set of size 1000; the test set size is 10000. A run consisted of model selection,
training and testing, and all results are averaged over 10 runs. We employed the
RBF (Gaussian) kernel with variance parameter C > 0 and an inverse squared
length scale w > 0 (2.29). Model selection was done by minimising validation set
error, training on random training set subsets of size 5000. 33
Our goal was to compare the methods not only w.r.t. performance, but also
running time. For the SVM, we chose the SMO algorithm [142] together with
a fast elaborate kernel matrix cache (see [173] for details).
For the IVM, we
31Platt’s post-processing method requires to train SVM several times, leave data out for
validation, etc., so the total cost is also quite a bit higher than simply training an SVM.
32Available online at http://www.research.att.com/∼yann/exdb/mnist/index.html.
33The model selection training set for a run i is the same across tested methods. The list of
kernel parameters considered for selection has the same size across methods.

180
Chapter 4. Sparse Gaussian Process Methods
employed randomised greedy selections with fairly conservative settings.34 Since
each binary digit classiﬁcation task is very unbalanced, the bias parameter b in
the GPC model was chosen to be non-zero. We simply ﬁxed b = Φ−1(r), where
r is fraction of +1 patterns in the training set and added a constant vb = 1/10
to the kernel K to account for the prior variance of the bias hyper-parameter.
Ideally, both b and vb should be chosen by model selection, but initial experiments
with diﬀerent values for (b, vb) exhibited no signiﬁcant ﬂuctuations in validation
errors. To ensure a fair comparison, we did initial SVM runs and initialised the
active set size d with the average number of SVs found, independently for each
c. We then re-ran the SVM experiments, allowing for O(d n) cache space. Table
4.1 shows the results.
SVM
IVM
c
d
gen
time
c
d
gen
time
0
1247
0.22
1281
0
1130
0.18
627
1
798
0.20
864
1
820
0.26
427
2
2240
0.40
2977
2
2150
0.40
1690
3
2610
0.41
3687
3
2500
0.39
2191
4
1826
0.40
2442
4
1740
0.33
1210
5
2306
0.29
2771
5
2200
0.32
1758
6
1331
0.28
1520
6
1270
0.29
765
7
1759
0.54
2251
7
1660
0.51
1110
8
2636
0.50
3909
8
2470
0.53
2024
9
2731
0.58
3469
9
2740
0.55
2444
Table 4.1: Test error rates (gen, %) and training times (time, s) on binary MNIST
tasks.
SVM: Support vector machine (SMO); d: average number of SVs.
IVM:
Sparse GPC, randomised greedy selections; d: ﬁnal active set size. Figures are means
over 10 runs.
Note that IVM shows comparable performance to the SVM, while achieving
34First 2 selections at random, then 198 using full greedy, after that a selection index of size
500 and a retained fraction τ = 1/2.

4.8. Experiments
181
0
0.05
0.1
0.15
0.2
10
−4
10
−3
10
−2
rejected fraction
error rate
SVM
IVM
Figure 4.4: Test error rate against increasing rejection rate for the SVM (dashed) and
IVM (solid), for the task c = 9 vs. the rest. For SVM, we reject based on “distance”
from separating plane, for IVM based on estimates of predictive probabilities. The
IVM line runs below the SVM line exhibiting lower classiﬁcation errors for identical
rejection rates.
lower training times. For less conservative settings of the randomised selection
parameters, further speed-ups are realisable. We also registered (not shown here)
signiﬁcant ﬂuctuations in training time for the SVM runs, while this ﬁgure is
stable and a-priori predictable for the IVM. Within the IVM, we can obtain
estimates of predictive probabilities for test points, quantifying prediction un-
certainties. In Figure 4.4, which was produced for the hardest task c = 9, we
reject fractions of test set examples based on the size of |P(y∗= +1) −1/2|. For
the SVM, the size of the discriminant output is often used to quantify predictive
uncertainty heuristically. For c = 9, the latter is clearly inferior (the diﬀerence is
less pronounced for the simpler binary tasks).
In the SVM community it is common to combine the ‘c-against-rest’ classiﬁers
to obtain a multi-class discriminant as follows: for a test point x∗, decide for
the class whose associated classiﬁer has the highest real-valued output. For the
IVM, the equivalent would be to compare the estimates log P(y∗= +1) from
each c-predictor and pick the maximising c.
This is suboptimal, because the
diﬀerent predictors have not been trained jointly, thus constraints which have to
hold between the latent processes and noise model parameters in order to obtain

182
Chapter 4. Sparse Gaussian Process Methods
a consistent distribution over classes are not enforced (the joint model is not
properly calibrated). However, the IVM estimates of log P(y∗= +1) do depend
on predictive variances, i.e. a measure of uncertainty about the predictive mean,
which cannot be properly obtained within the SVM framework. If we use the
probit noise model (as done here), log P(y∗= 1) lies between Φ(µ∗+ b) and 1/2,
depending on the value of the variance σ2
∗, which shows that the class with the
largest log P(y∗= 1) need not be the one with the largest value for µ∗+ b. These
points are discussed in more detail in Section 4.7.2. The combination scheme
results in test errors of 1.54%(±0.0417%) for IVM, 1.62%(±0.0316%) for SVM
(the ﬁgures are mean and standard deviation over 10 runs). When comparing
these results to others in the literature, recall that our experiments were based
on images sub-sampled to size 13 × 13 rather than the usual 28 × 28.
We have repeated the hardest of the tasks above (c = 5, 8, 9) using the full
MNIST 28×28 input images together with a kernel implementation which exploits
the high ratio of zero-valued pixels in these bitmaps. Apart from these changes,
the setup is the same as above. We compared SVM against four variants of IVM
which diﬀered in the parameters for the randomised greedy selection mechanism
(dfull: size of I until full greedy selection is run; |J|: size of selection set thereafter;
τ: retain fraction): IVM-1: dfull = 100, |J| = 300, τ = 3/4. IVM-2: dfull =
50, |J| = 200, τ = 3/4. IVM-3: dfull = 100, |J| = 500, τ = 3/4. IVM-4:
dfull = 100, |J| = 300, τ = 1/2. The results are shown in Table 4.2.
On the harder tasks SVM slightly outperforms the IVM variants. On the
other hand, the nature of the IVM algorithm allows for a a priori prediction of
required running time depending on the parameters d and the ones controlling
RGS.
4.8.2
PLV Regression: Robot Arm Dynamics
The material in this section has previously appeared in [165]. The aim was to
study the behaviour of PLV for GP regression with Gaussian noise (variance σ2,
a hyperparameter), especially w.r.t. model selection by marginal likelihood max-
imisation as discussed in Section 4.5.1. Note that we employ the approximation

4.8. Experiments
183
SVM
IVM-1
IVM-2
IVM-3
IVM-4
5 versus non-5
gen(%)
0.29
0.33
0.33
0.30
0.31
time(s)
3363
2417
2920
3292
3070
d
3004
3200
3200
3200
3200
8 versus non-8
gen(%)
0.44
0.51
0.51
0.49
0.50
time(s)
4338
3155
3353
3755
4150
d
3776
3600
3600
3600
3700
9 versus non-9
gen(%)
0.55
0.65
0.62
0.62
0.63
time(s)
4282
2805
3444
4494
4510
d
3926
3600
3600
3900
3900
Table 4.2: Comparison of SVM vs. IVM on MNIST tasks (full-size images). Note
that memory usage was limited to a maximum of n × 3600, n the training set size.
(4.18) here, not the more general variational G. Recall from Section 4.4.2 that in
the special case of a Gaussian likelihood, the PLV scheme simpliﬁes considerably,
because the site parameters are ﬁxed from the beginning and do not have to
be adapted (since the true full posterior, conditioned on the hyperparameters is
Gaussian, an EP approximation is not required). For these experiments, we also
used the simple form of the approximate information gain selection score (see Sec-
tion 4.4.2.1) which ignores all couplings (i.e., H = ∅). This version will be called
info-gain here. We compared against a version which selects I at random (ran-
dom) and against the method of Smola and Bartlett (smo-bart, see Section 4.6).
For the latter, we deviated from the author’s suggestions and choose a smaller
random selection set size of k = 30. Note that a priori, info-gain and random
have the same complexity, while smo-bart should be k times slower.
We chose the datasets kin-40k (10000 training, 30000 test cases, 9 attributes)

184
Chapter 4. Sparse Gaussian Process Methods
and pumadyn-32nm (7168 training, 1024 test cases, 33 attributes),35 both artiﬁ-
cially created using a robot arm simulator, highly non-linear and low-noise. We
pre-processed both by subtracting oﬀa linear regression ﬁt and normalising all
attributes to unit variance. Thus, a linear regression on these tasks would result
in averaged squared errors ≈0.5. A better approach would use a semiparametric
model (see Sections 2.1.5 and 2.1.7), but this has not been tried. In both cases,
we used 10 diﬀerent random splits into training and test sets, one for each run.
For a ﬁxed run, we used the same split over all experiments below, in order to
facilitate cross-comparisons.
We used the squared-exponential kernel (2.30) with positive hyperparameters
W = diag(wj)j, C, vb and the hyperpriors employed by [146]. If model selection
with this kernel is successful, it will “switch oﬀ” input attributes which are not
relevant for inference by driving wj →0 (automatic relevance determination
(ARD), see Section 2.1.8). Where not stated otherwise, experiments are repeated
ten times, and we quote medians (as well as quartiles in the plots). Predictive
accuracy is measured in terms of average squared error, i.e. (1/2)(y∗−µ(x∗))2
averaged over the test set, note that this does not depend on the estimates of the
predictive variance.
4.8.2.1
Learning Curves
Here, we compare full GPR against info-gain, random, smo-bart. Hyperparame-
ters were adjusted by maximising their (marginal) log posterior for full GPR over
a random subset of size nMS of the training set. Figure 4.5 shows learning curves
for kin-40k, nMS = 2000 (note that each plot contains upper and lower quartiles
for full GPR as horizontal lines) and Table 4.3 gives the corresponding training
times.
For small sizes d = |I|, smo-bart outperforms the other methods, while for
larger d, random seems less eﬀective. We also see (from curves on the right) that
sparse methods on the full training set (n = 10000) can signiﬁcantly outperform
35kin-40k: thanks to Anton Schwaighofer (www.igi.tugraz.at/aschwaig/data.html). pumadyn-
32nm: thanks to Zoubin Ghahramani (www.cs.toronto/∼delve).

4.8. Experiments
185
0
200
400
600
800
1000
1200
10
−2
10
−1
n = 2000
info−gain 
0
200
400
600
800
1000
1200
10
−2
10
−1
n = 10000
info−gain 
0
200
400
600
800
1000
1200
10
−2
10
−1
n = 2000
random 
0
200
400
600
800
1000
1200
10
−2
10
−1
n = 10000
random 
0
200
400
600
800
1000
1200
10
−2
10
−1
n = 2000
smo−bart 
0
200
400
600
800
1000
1200
10
−2
10
−1
n = 10000
smo−bart 
Figure 4.5: Learning curves for sparse GPR on kin-40k. Left: sparse methods use
model selection training sets of full GPR, n = 2000. Right: sparse methods use full
training set, n = 10000. X-axis: Active set size d. Y-axis: Average squared error.
Horizontal lines: quartiles for full GPR, n = 2000.

186
Chapter 4. Sparse Gaussian Process Methods
Time (secs) for act. set size d
100
200
300
400
500
1000
1250
kin-40k, n = 2000
info-gain 1.0
2.0
5.0
9.0
13.5
53.5
85.0
random
1.0
2.0
4.0
7.0
11.0
48.0
76.0
smo-bart 17.0 54.0
110.0 185.0 281.0
1088.0 1714.5
kin-40k, n = 10000
info-gain 5.0
13.0
25.5
42.0
62.5
230.0
352.0
random
3.0
10.0
21.0
35.5
55.0
215.0
338.5
smo-bart 88.0 265.0 530.5 885.0 1327.5 4977.0 7794.5
Table 4.3: Training time (secs) for methods info-gain, random, smo-bart.
full GPR on a subset (n = 2000), even though the former employ sparser ex-
pansions (d = 1000, 1250). In this case, the freedom of selecting amongst more
points outweighs the advantage of using a larger expansion. While random and
info-gain have similar training times, smo-bart is about thirty times slower, which
probably precludes the latter being used as a subroutine for model selection. It
would be fair to compare info-gain and smo-bart for equal amounts of running
time, in which case the former performed much better.
The set pumadyn-32nm has a lot of irrelevant attributes which, in the context
of GPR have to be identiﬁed in order to achieve good performance. Adapting the
hyperparameters for nMS = 1024 using full GPR leads to four attributes being
singled out, all other wj ≈0. The median of the average squared test errors is
0.0225. Note that traditional techniques like cross-validation are not applicable in
this situation, due to the large number (35) of hyperparameters. To demonstrate
the signiﬁcance of ARD, we ran the same experiment using the Gaussian (RBF)
kernel (2.29), resulting in a squared error of 0.4730. However, if only the four
“relevant” attributes are used, full GPR with the RBF kernel attains a squared
error of 0.024.
We see that covariance functions with many hyperparameters
can be essential for decent performance, which stresses the importance of model
selection techniques which can handle such covariance functions. Figure 4.6 shows

4.8. Experiments
187
learning curves for the sparse methods.
0
20
40
60
80
100
120
140
160
10
−2
10
−1
info−gain 
0
20
40
60
80
100
120
140
160
10
−2
10
−1
random 
0
20
40
60
80
100
120
140
160
10
−2
10
−1
smo−bart 
Figure 4.6: Learning curves for sparse GPR on pumadyn-32nm. X-axis: Active set
size d. Y-axis: Average squared error.
Here, all three methods achieve an accuracy comparable to full GPR after the
inclusion of d = 125 of the n = 7168 training cases, but while the training time
median for full GPR is 1102.5s, info-gain, random and smo-bart require 3s, 2s
and 66s respectively. For unreasonably small values of d, info-gain exhibits some
ﬂuctuations, leading to a worse performance than random. The fact that smo-bart
is 30 times slower than info-gain should be weighted against the slightly faster
decay of its learning curve.

188
Chapter 4. Sparse Gaussian Process Methods
4.8.2.2
Sparse Model Selection
Here, we test selection of σ2 and the kernel parameters by maximising the approx-
imation of the (marginal) hyperposterior described in Section 4.4.2.1. smo-bart
is not considered here due to excessive running time. In a ﬁrst experiment, we
followed the trajectory in hyperparameter space used by full GPR training (on
pumadyn-32nm, nMS = 1024) and computed the corresponding approximate cri-
terion values for info-gain, random for diﬀerent d. Figure 4.7 shows the criterion
curves for info-gain (left) and random (right).
0
20
40
60
80
100
120
−1
−0.5
0
0.5
1
1.5
2
2.5
Full GPR
Info−100
Info−200
Info−300
0
20
40
60
80
100
120
−1
−0.5
0
0.5
1
1.5
2
2.5
Full GPR
Random−100
Random−200
Random−300
Figure 4.7: Criterion for full GPR and sparse approximations (evaluated at the same
hyperparameters). X-axis: Iterations.
Along critical parts of the trajectory, the approximation given by random is
fairly poor (for d = 100, 200), while the info-gain approximation seems acceptable.
Note that for both methods, I is re-selected in every iteration, therefore the poor
approximation by random cannot be attributed to random ﬂuctuations.
In a larger experiment, we ﬁrst trained full GPR on pumadyn-32nm, using
nMS = 2048. We then ran an identical setup,36 however using the sparse approxi-
mate criterion together with info-gain, random and a variant of the latter, named
ﬁxed in which we selected I at random only once and kept it ﬁxed during the
36The initial hyperparameter values were chosen as recommended in [146]. They are not close
to a useful minimum point for this task (note the sharp initial drop of the criterion values).
The optimiser was stopped once the relative improvement and gradient size fell below small
thresholds, or after 200 iterations.

4.8. Experiments
189
optimisation. We used active set sizes d = 50, 100, 200, 500. Only two diﬀerent
proﬁles were observed: either, the relevant attributes mentioned above were sin-
gled out (ARD) and the squared error was below 0.03, or the method failed with
error close to 0.5. In fact, random and ﬁxed for d = 50, 100, 200 failed in all runs,
and info-gain for d = 50 was successful just once. For all other combinations, me-
dians of the average squared error, number of iterations and optimisation running
time over the successful runs are given in Table 4.4.
Method
d
# succ. Squared # Iter. MS time
runs
error
optim.
(secs)
full GPR -
10
0.0236
200
22100.5
info-gain
500 10
0.0296
200
9079
info-gain
200 10
0.0259
177.5
1833.5
info-gain
100 9
0.0252
200
836
random
500 8
0.0244
95
4412
ﬁxed
500 9
0.0239
200
9203
Table 4.4: Comparison of model selection for full and several sparse GPR methods,
dataset pumadyn-32nm, nMS = 2048.
While model selection based on info-gain was reliable even for small sizes
d = 100, 200, the runs for random often converged (to high accuracy) to poor
spurious minima of the criterion approximation. Even in the successful runs, ﬂat
plateaus are traversed before a new downwards direction is found (see Figure 4.8,
left).37 Somewhat surprisingly, info-gain with d = 500 results in larger squared
errors than our scheme for smaller d. In Figure 4.8, we compare criterion curves
for full GPR and sparse methods using a run in which all of them were successful.
On the right, we plot the sizes of the largest inverse length scales w1/2
j
in the
hyperparameter vectors chosen by the diﬀerent methods (recall that the positions
of these within W were the same for all methods).
37Somewhat surprisingly, random does not behave better than ﬁxed w.r.t. spurious converge
(for d = 500, they both fail in the same run). We would have expected random to escape
spurious minima more readily, because I is re-selected at random in every iteration.

190
Chapter 4. Sparse Gaussian Process Methods
0
50
100
150
200
−1
−0.5
0
0.5
1
1.5
2
2.5
3
full GPR
info500
info200
info100
random500
1
2
3
4
5
0
2
4
6
8
10
wj
1/2
full GPR
info500
info200
info100
random500
Figure 4.8: Left: Criterion curves for model selection optimisations (X-axis: Iterations
of optimiser); lower solid line: full GPR; upper solid line: random (d = 500). Right:
Largest values of inverse squared length scales wj in the selected hyperparameter
vector.
Note the dangerously ﬂat plateau in the curve for random (ﬁxed has a very
similar curve, not shown here). As for the suboptimal performance of info-gain
(d = 500), one may suspect overﬁtting38 behaviour. The curve runs below the
one for full GPR, and the concrete w1/2
j
values are signiﬁcantly larger than the
ones chosen by the latter. Indeed, training full GPR using the hyperparameters
found by random, d = 500, results in a squared error of 0.0317, suggesting that
the hyperparameters are suboptimal.
4.8.3
IVM Regression: Robot Arm Dynamics
The aim of the empirical studies of this section is to test the model selection
strategy for the IVM scheme proposed in Section 4.5.2 on the regression estima-
tion task pumadyn-32nm already used in Section 4.8.2 and a direct comparison
with model selection for the more expensive PLV scheme.
The setup is as follows. We use the same 10 dataset splits into n = 7168
training and m = 1024 test cases. As opposed to the PLV experiments where
model selection (MS) was done on randomly chosen subsets of each training set
38In general, empirical Bayesian methods such as ours here can lead to overﬁtting.

4.8. Experiments
191
of size nMS = 2048, here we employ the full training sets for MS, nMS = n =
7168. We employed the squared-exponential covariance function with the same
hyperpriors and initial values as above. Recall from Sections 4.5.2 and 4.4.1 that
a few additional parameters have to be speciﬁed for the IVM scheme. |L| denotes
the size of the MS criterion index L used to approximate the full likelihood in the
MS criterion. Randomised greedy selection (RGS) comes with several parameters:
dfull, |J|, τ (see Section 4.8.1). Note that the cost for a criterion and gradient
computation is O(|L| d2) (see Section C.3.3).
Recall from Section 4.8.2.2 that for a decent performance on pumadyn-32nm
with GPR, the discovery of “relevant” input dimensions is essential. In a ﬁrst
set of experiments, we asked whether the ARD eﬀect can be reproduced reliably
for model selection with the faster IVM scheme. We concentrated on the cases
d = 200 and d = 500. For d = 200, we used |L| = 4096 (4/7 of the full training
set) and did not employ RGS, while for d = 500, we used |L| = 2048, dfull =
200, |J| = 1000, τ = 1/2. The (Quasi-Newton) optimiser was run using the same
parameters as above, except that the maximum number of iterations for d = 500
was limited to 100 here (for d = 200, we adopted the very conservative bound
of 200 from above). Again, no eﬀort was made to choose a stopping criterion
suitable to deal with the ﬁnal oscillatory behaviour of the optimiser (see remarks
in Section 4.5.3): we stopped once the relative improvement between adjacent
steps fell below 10−5 or once the iteration bound was reached.
The “bimodal” behaviour in terms of predictive squared error was observed
here as well. All ten runs for d = 200 failed to detect the relevant dimensions
and had squared errors around 1/2. Each of these runs terminated prematurely,
due to small relative improvements. The “solution” provided by these runs is
invariably to drive C to a small value and σ2 to a value around 1, producing
predictive means and variances quite close to 0 respectively. This is close to the
result linear regression would obtain (which is constant 0). We repeated these
experiments with lower accuracy thresholds, forcing the optimiser to run the full
200 iterations, with no qualitative diﬀerence in the outcomes.
The situation was diﬀerent for d = 500, for which all ten runs were successful

192
Chapter 4. Sparse Gaussian Process Methods
in detecting the four relevant components. The median averaged squared error
was 0.0233, the median MS running time was 6519 secs. These ﬁgures should
be compared against info-gain-(500) in table 4.4, keeping in mind that the latter
ran for twice the number of optimiser iterations.
IVM slightly outperformed
all variants listed in table 4.4, but also runs somewhat longer than PLV for
d = 500. Note that IVM operates on a larger training set (7192 vs. 2048 for
PLV), but uses RGS and a MS selection index of size 2048 to avoid O(n d2)
scaling. We repeated the same experiment with a less conservative RGS setting
(dfull = 50, |J| = 250, τ = 3/4), ending up with median squared error 0.0234
(again all runs were successful) and median MS time 5517 secs, a signiﬁcant speed-
up with no decrease in performance. We also ran IVM with full greedy selection
on the same MS training subsets (nMS = 2048) as used for the PLV experiments
above. Here, three of the ten runs failed, one had squared error 0.0445, and the
median over the remaining 6 runs was 0.0242, the median MS time was 4452 secs.
Of course, just as in the PLV experiments, the ﬁnal training run with the selected
parameters was done on the full training set of 7192 patterns. We see that IVM
with fairly aggressive RGS run on a large training set can be a much more robust
alternative to subsampling the training set for model selection.
To conclude, on this particular task IVM regression with MS on the full train-
ing set slightly outperformed PLV regression with MS on a subset of the training
set, and the running times are comparable. Since IVM with RGS is somewhat
more complicated (especially w.r.t. bookkeeping), a larger gain could probably
be realised by a more careful implementation. It should be kept in mind that for
IVM, running time and complexity of implementation is the same for simple GP
regression (Gaussian noise) and non-Gaussian models like binary classiﬁcation
(see Section 4.8.4), while PLV becomes much more complicated and costly in the
latter case. On the other hand, the more accurate likelihood approximation of
PLV allows us to use smaller active set sizes d before the inference approximation
becomes too coarse to drive model selection: the pumadyn-32nm regression task
is solved successfully for PLV with d = 200 and even d = 100 (see Section 4.8.2)
while we required d = 500 for IVM here. The direct comparison between IVM

4.8. Experiments
193
and PLV presented here is complicated further by the fact that we used diﬀer-
ent approximations for the marginal likelihood used to do model selection. For
IVM, the variational lower bound from Section 4.5.2 was used, while for PLV
we employed the simpler direct approximation (4.18). The latter is probably not
sensible for IVM because it depends on the datapoints in I only.
4.8.4
IVM Classiﬁcation: Digit Recognition II
In this section, we focus on model selection for IVM (see Section 4.5.2) on a
binary classiﬁcation task. The latter is extracted from the USPS handwritten
digits database (see [99] or [161], Chap. A.1), containing grey-scale patterns of size
16 × 16, with attribute values quantised to ﬁve bits (values are (u −15)/16, u =
0, . . . , 30, with u = 0 being attained for 43% of all values). The database comes
split into a training set of n = 7291 and a test set of m = 2007 patterns. The
class prior distributions are somewhat nonuniform, with 0(1194) and 1(1005)
being most, 8(542) and 5(556) least represented. The corresponding recognition
task is quoted to be rather hard, with human error rates estimated at around
2.5%.
Just as in Section 4.8.1 we employed the RBF kernel with variance parameter
C > 0 and inverse squared length scale w > 0 (2.29). The hyperprior on log C was
N(−1, 1), on w−1 Gamma with mean 1, degrees of freedom 1. The probit noise
model (2.9) had an unconstrained bias parameter b with a N(ˆb, 5) hyperprior
where ˆb was computed based on the training data. We focussed on binary tasks
of the form c-against-rest, c ∈{0, . . . , 9}.
For a ﬁrst set of experiments, we employed a MS selection index size |L| = 2000
and RGS with parameters dfull = 200, |J| = 250, τ = 3/4, furthermore an active
set size d = 500. The results for all binary c-against-rest tasks are shown in
Table 4.5.
Csat´o [38] quotes results of his sparse EP algorithm (which is a version of PLV
with an on-line like selection of I) on USPS, 4-against-rest (Figure 5.6a in [38])
with test error ≈1.75, dropping to ≈1.65 for several sweeps (for d = 500). This
is slightly worse than our result of 1.4, probably because the hyperparameters

194
Chapter 4. Sparse Gaussian Process Methods
c gen time c gen time c gen time
0 0.70 4514 1 0.70 5017 2 1.59 4419
3 1.30 4256 4 1.40 4419 5 1.25 4295
6 0.65 5017 7 0.65 4419 8 1.35 4256
9 0.80 4332
Table 4.5: Test error rates (gen, %) and model selection times (time, s) on binary
USPS tasks. Figures are medians over 10 runs.
(Csat´o uses the same covariance function and noise model) are set by hand in
[38]. The curve in [38] suggests that our choice of d = 500 was very conservative.
We also combined the 10 binary predictors in the same heuristic manner as in
Section 4.8.1 (deciding for the class c with maximum log P(y∗= +1) estimate),
resulting in test error rate of 4.98%, comparable to results for other kernel classi-
ﬁers on “black-box” covariance functions (see [161], Table 7.4). If we are allowed
to reject an α-fraction of the test cases, it is sensible to avoid patterns for which
the maximum over c of log P(y∗= +1) is smallest.
Doing so, we obtain the
error-reject curve in Figure 4.9. Csat´o [38] reports errors of 5.4% (single sweep)
and 5.15% (three sweeps) on this task, however using d = 350 and subscribing
to some restrictions we do not (for example, a single set I for all 10 classiﬁers is
selected).
We also repeated the hardest binary task 2-against-rest using the squared-
exponential covariance function (2.30). The median test error slightly dropped
to 1.4% while the MS running time doubled to 8932 secs. All inverse squared
length scales were between 2.4 and 5.8, thus no ARD eﬀect was visible here:
w.r.t. this particular kernel, none of the input components are irrelevant, or at
least the MS method does not single out any. The sharp increase in running time
is due to the O(n d) part in the gradient term which has to be computed for each
of the 259 hyperparameters.

4.9. Discussion
195
0
0.2
0.4
0.6
0.8
1
0
0.02
0.04
0.06
Rejected Fraction
Rem. Error Rate
Figure 4.9: Test error rate against increasing rejection rate for IVM with model
selection on combined USPS task. See text for details.
4.9
Discussion
In this chapter, we have discussed how to develop sparse approximations to
Bayesian nonparametric GP methods in a principled manner. They lead to algo-
rithms with training time scaling of O(n d2) and O(d2) per prediction, where d is
a controllable parameter, and they “converge” towards their full Bayesian coun-
terparts as d →n. We have described two generic schemes to implement sparse
greedy GP approximations based on greedy forward selection with information-
theoretic criteria: the simple, eﬃcient IVM (see Section 4.4.1) and the potentially
more accurate PLV (see Section 4.4.2). For both schemes, we demonstrated how
to do automatic model selection for a large number of hyperparameters by max-
imising sparse approximations to the marginal likelihood of the data, using mod-
iﬁcations of standard optimisation techniques. Both schemes provide estimates
of predictive variances along with predictive means, thus allowing for quanti-
fying uncertainty in error bars or dealing with more general decision-theoretic
setups than balanced discrimination. We established empirically that the meth-
ods presented in this chapter can be used to ﬁt models involving priors with many

196
Chapter 4. Sparse Gaussian Process Methods
hyperparameters to large datasets from diﬃcult real-world tasks.
Further conclusions and suggestions for future work can be found in Sec-
tion 5.2.

Chapter 5
Conclusions and Future Work
In this ﬁnal chapter, we present conclusions from the main contributions in this
work, highlight unresolved issues and give some suggestions for valuable future
work. This is done separately for the two main chapters in Section 5.1 (PAC-
Bayesian theorems) and Section 5.2 (sparse GP approximations).
5.1
PAC-Bayesian Bounds for Gaussian Process
Methods
In Chapter 3, we have given a simple and direct proof of a PAC-Bayesian gen-
eralisation error bound which generalises McAllester’s original result. We have
pointed out convex (Legendre) duality (see Section A.3) as the core technique
in the proof, giving yet another example of the amazing scope and power of
the duality principle in convex analysis (other applications relevant to machine
learning include the EM algorithm, a number of variational approximations to
Bayesian inference and convex (notably linear) programming). For certain classes
of Bayes-type classiﬁers, convex duality provides a more direct (and therefore of-
ten tighter) approach to global (uniform) bounds than mathematically much more
involved techniques based on combinatorial dimensions or covering numbers. We
have shown how to apply PAC-Bayesian theorems to a large generic class of non-
parametric Gaussian process methods spanning most schemes actually used in
197

198
Chapter 5. Conclusions and Future Work
practice, again without having to resort to “heavy-weight” mathematical con-
cepts, ending up with practically tight results for powerful learning methods
which are notoriously diﬃcult to handle with conventional concepts. Although
the PAC-Bayesian theorems certainly do not constitute the ﬁrst application of
convex duality for proving distribution-free bounds, we hope that the examples
given in this thesis will spark new interest in establishing convex duality as core
technique and developing its full potential within learning theory.
Our experimental results indicate that the PAC-Bayesian bounds can be very
tight in practically relevant situations, giving more useful results than other state-
of-the-art PAC bounds for kernel classiﬁers we considered. We have discussed
possible reasons for lack of tightness of classical VC bounds in Section 3.1.1, and
the same apply in principle to many of the current kernel classiﬁer bounds we
know of: the dependence on algorithm and data sample is typically very weak,
given only through restricted statistics such as a large empirical margin or a
certain degree of sparsity, and the bounds are hardly conﬁgurable by given task
prior knowledge. In contrast, the PAC-Bayesian complexity measure, the relative
entropy between posterior and prior, is more ﬂexible, conﬁgurable and depends
more strongly on the particular algorithm than any others employed in kernel
classiﬁer bounds we know of. In the GP case, prior knowledge can be encoded in
a very general way via the choice of the covariance function.
Another “dual” relationship is worth pointing out. Approximate Bayesian
techniques use simpliﬁcations to overcome the intractability of exact Bayesian
analysis on a model, such as factorisation or Gaussian assumptions, and exactly
these simpliﬁcations allow us to feasibly do a PAC analysis of the technique.
The fact that we approximate the intractable posterior process by a Gaussian
one allows us to compute the PAC-Bayesian bound for GP models. For a sparse
GPC approximation, the computational complexity of evaluating the bound drops
accordingly.
Finally, relations between Gibbs and Bayes classiﬁers (see Sec-
tion 3.2.5) can be inferred from symmetry properties of the approximate Gaussian
predictive distribution, while they probably do not hold for the true predictive
distribution. This example suggests that PAC or also average-case analyses could

5.1. PAC-Bayesian Bounds for Gaussian Process
Methods
199
be simpliﬁed (and tightened) if instead of exact (intractable) Bayesian inference
we focussed on tractable approximations which are actually used in practice (an-
other good example is given in [25]). Of course, analyses of the latter type are
also of much higher interest to practitioners.
5.1.1
Suggestions for Future Work
The PAC-Bayesian theorem applies more widely to other approximate Bayesian
inference techniques. For example, its application to parametric models is fairly
straightforward if standard techniques like variational mean ﬁeld or Laplace ap-
proximations are used. As a concrete example, it would be easy to apply it to
Bayesian multi-layer perceptrons [110, 17], using inference approximations based
on Laplace [110] or variational mean ﬁeld [11]. Langford and Caruana [95] con-
sider a diﬀerent application to MLPs. Note that while the Gaussian approxi-
mations employed in approximate GP inference are rather natural (because of
the GP prior) and often excellent, existing approximations to the posterior for
complicated parametric models like MLPs can be very poor.1 The theorem could
also be applied to non-Bayesian methods resulting in classiﬁers which can be
considered as probabilistic mixtures, such as the ones constructed by boosting
methods [57, 59].
Gibbs versions of such classiﬁers would sample component
classiﬁers using the mixture distribution for each prediction. Furthermore, the
use of convex duality to decouple the posterior (with which the mixture classiﬁer
is deﬁned) from the errors made by the individual component classiﬁers should
have much wider applicability as a technique for proving PAC results for mix-
ture classiﬁers (the mixtures could well be hierarchical, such as in the mixture of
experts architecture [86, 205]).
Several open problems remain (to our knowledge). First, although Meir and
Zhang [121] recently obtained a PAC-Bayesian theorem for Bayes-like classiﬁers
(see Theorem 3.4), this result is typically less tight than the theorem for Gibbs
classiﬁers used here, in apparent contradiction to the observation that in practice,
1This is often due to extreme multimodality of the posterior, together with other degeneracies
such as plateaus, ridges, etc. In contrast to that, for many commonly used noise models, the
GP posterior is unimodal and can be approximated very well by a Gaussian.

200
Chapter 5. Conclusions and Future Work
the Bayes version typically outperforms the Gibbs version (in our experiments,
Theorem 3.4 could not give non-trivial guarantees).
Reﬁnements of this very
recent result may lead to a more satisfying theorem for the Bayes version. As
mentioned in Section 3.4.1.1, the problem is the scaling with the square root of
D[Q ∥P]/n together with the large leading constant. In the case of zero-one loss
it is possible to derive variants of VC bounds which scale as O(n−1) for small
empirical error, but it is at least not straightforward to apply the same tech-
nique to the margin loss functions in Theorem 3.4.
Second, a PAC-Bayesian
theorem for restricted, yet unbounded loss functions would be required to deal
with the regression estimation problem. Such a bound would necessarily depend
on statistics of the data distribution, e.g. its variance, and would apply only
if these statistics can somehow be controlled with certainty. Third, the PAC-
Bayesian theorems do not allow for model selection based on the training sample
S from a continuum of models and even the “stratiﬁcation” procedures used to
justify model selection from countable families seem unnatural compared with
the elegant “union-bound free” approach of the PAC-Bayesian technique. The
PAC-Bayesian theorem applies to hierarchical priors, so that integrating out hy-
perparameters (approximately) is admissible. However, empirical Bayesian model
selection translates to integrating them out using spike approximate to the hy-
perposteriors, and this invalidates the bound because the relative entropy term
becomes inﬁnite.
5.2
Sparse Gaussian Process Methods
In Chapter 4, we have discussed how sparse approximations to Bayesian non-
parametric GP methods can be developed in a principled manner. Our strict
requirements on these approximations imply that conditional inference (training)
scales as O(n d2) for running time and O(n d) for memory2, where the sparsity
parameter d is controllable and can often be chosen to be much smaller than
n with negligible loss of prediction performance. The time per prediction is in-
2The memory requirements can be reduced for the IVM scheme at the expense of slightly
increased running time (see discussion of stub caching and trimming in Section C.3.1.1).

5.2. Sparse Gaussian Process Methods
201
dependent of n and scales as O(d2). We placed special emphasis on solutions
which are generic, can be implemented reasonably simply and in a numerically
stable way without having to resort to complicated heuristics and which come
with a favourable running time-memory trade-oﬀ, but at the same time provide
the same range of “services” (e.g. predictive variances, model selection, etc.) as
their non-sparse relatives.
We have shown that many sparse GP approximations can be understood natu-
rally as marrying a conventional (non-sparse) technique with a suitable likelihood
approximation which loosely speaking creates a bottleneck of a reduced number
of “active” latent variables. Our aim was to design generic schemes which ﬁt a
large number of models of interest with minimal speciﬁc primitives to be added to
an otherwise generic implementation. This was achieved by building on the EP
algorithm which is generic up to simple computations involving low-dimensional
Gaussian integrals over likelihood factors. The active set I, i.e. the subset of active
latent variables upon which the likelihood approximation depends, is determined
sequentially by greedy forward selection. More sophisticated and expensive evo-
lutions of I (such as exchange moves) are suggested but not discussed in detail.
The selection criteria we used (see Section 4.2.2) have been proposed for active
learning (sequential design), but their application to sparse GP methods is novel
(to our knowledge). In the context of the Gaussian approximations employed
in Bayesian GP techniques, they can be computed easily and analytically, while
their application to complicated nonlinear parametric architectures is much less
attractive. Their computation is fast if the likelihood approximation factorises in
a way similar to the true likelihood, and we proposed a simple “decoupling” cri-
terion approximation which can be used if the likelihood approximation is fully
coupled. Importantly, the methods we proposed are complete in that they in-
clude a generic empirical Bayesian model selection technique which is shown to
work eﬀectively in experiments with diﬃcult large real-world tasks and heavily
parameterised covariance functions, a setup which is very rarely considered in the
kernel machines literature.
Not surprisingly, none of the basic ingredients used in our methods are en-

202
Chapter 5. Conclusions and Future Work
tirely new.
Sparse approximations to nonparametric techniques, based on fo-
cussing on subsets of the dataset have been proposed many times, owing to the
high signiﬁcance in practice. The selection criteria we have employed here are
well-established in active learning (sequential design). Finally, the sparse approx-
imation of the GP expectation propagation scheme has been developed in [38],
albeit with a diﬀerent strategy for choosing I.
We should stress that our methods do not rely on special features of the co-
variance function or low input dimensionality, while the accuracy (and therefore
usefulness) of the methods discussed here depends on how well the covariance
matrix over the data can be approximated with a matrix of fairly low rank. It is
our opinion that prior distributions (covariance functions in our case) should be
chosen based on task prior knowledge with as few constraints for technical conve-
nience as possible. An inference approximation should ideally be ﬂexible in this
respect, allowing for arbitrary covariance functions, noise models, and providing
automatic model selection which scales linearly in the number of hyperparame-
ters.
5.2.1
Suggestions for Future Work
The methods presented in Chapter 4 are generic and applicable to many nonpara-
metric models beyond binary classiﬁcation and regression with Gaussian noise.
Robust regression, e.g. with t-distributed noise would be straightforward to do.3
Multi-class classiﬁcation or ordinal regression (“ranking”) would require a C-
process model (see Section 2.1.2), running EP with a likelihood which factors
over groups of C latent variables each. This is implicit in our general develop-
ment, with the caveat that the running time complexity increases by a factor
of C2. Further approximations to reduce this to a factor of C may be applica-
ble (for example, the Laplace GP approximation of Section 2.1.3 scales linearly
with C).
Applications of nonparametric kernel methods are not restricted to
standard supervised statistical tasks like classiﬁcation or regression estimation.
3Our implementation allows to plug in arbitrary noise models, as long as certain primitives
are implemented. It can also be used with arbitrary covariance functions.

5.2. Sparse Gaussian Process Methods
203
Csat´o et. al. [41] use sparse GP techniques to approximate density estimation.
Bach and Jordan [8] employ low-rank kernel matrix approximations in the context
of approximations to the mutual information and apply their contrast function to
independent components analysis. Friedman and Nachman [60] propose Gaussian
process networks to model sets of random variables with complicated nonlinear in-
teractions (and only vague prior knowledge about their characteristics) and show
how to learn the network structure. It will be interesting to ﬁnd out whether the
generic sparse methods developed here are applicable to these more ambitious
(and expensive) applications of nonparametric smoothing.
The code used for our experiments has not yet been released into the public
domain, this is a pressing issue for future work. It is written in C++, easily
extensible but unfortunately not as user-friendly as a Matlab implementation
would be.4 We decided against a Matlab implementation fairly early, because
Matlab is known for its poor memory management and its inability of handling
iterative code. A future project would be to wrap our code as MEX functions
which could then be called from Matlab.
4It also contains some proprietary code (Numerical Recipes) in the moment which has to be
replaced before even the present version can be released.

204
Chapter 5. Conclusions and Future Work

Appendix A
General Appendix
This chapter provides the general background which we draw from in the remain-
der of the thesis and is included mainly to render the work self-contained. Many
readers will be familiar with most of the material and are invited consult this
chapter only to match their notational conventions with ours.
Section A.1 deﬁnes general notational conventions. In Section A.2 we state
properties from linear algebra and collect useful formulae.
In Section A.3 we
discuss properties of convex functions central to the ﬁrst part of this thesis. Sec-
tion A.4 introduces exponential families, in particular the Gaussian one and some
of its properties relevant to this work. In Section A.5, we brieﬂy deﬁne some no-
tions of pattern recognition, the statistical problem this thesis is mostly concerned
with. Bayesian inference and some approximations towards such is the topic of
Section A.6. Finally, Section A.7 states basics about large deviation inequalities.
A.1
Notation
A.1.1
Linear Algebra
Vectors a = (ai)i = (a1 . . . an)T (column by default) and matrices A = (ai,j)i,j
are written in bold-face.
If A is a matrix, aj denotes its j-th column, ai,j
its (i, j)-th entry.
If A ∈Rm,n, I ⊂{1, . . . , m}, J ⊂{1, . . . , n} are index
205

206
Appendix A. General Appendix
sets,1 then AI,J denotes the |I| × |J| sub-matrix formed from A by selecting
the corresponding entries (i, j), i ∈I, j ∈J. We use some short notations:
Ak...l,J = A{k...l},J, Ai,J = A{i},J, A·,J = A1...m,J and A\I,J = A{1...m}\I,J, etc.
Here, \I = {1, . . . , m} \ I is the complement of I, sorted in ascending order. For
example, Ai,j = ai,j, A·,j = aj and A·,· = A.
Some special vectors and matrices are deﬁned as follows: 0 = (0)i and 1 = (1)i
the vectors of all zero and all ones, δj = (δi,j)i the j-th standard unit vector. Here,
δi,j = 1 if i = j, and 0 otherwise (Kronecker symbol). Furthermore, I = (δi,j)i,j
is the identity matrix. For these matrices, the size will always be clear from the
context.
The superscript T denotes transposition. diag a is the matrix with diagonal a
and 0 elsewhere. diag A is the vector containing the diagonal of A, and diag2 A is
the diagonal matrix with the same diagonal as A. tr A is the sum of the diagonal
elements of A, tr A = 1T(diag A). diag and tr operators have lower priority than
multiplication. For example, diag ATb is the diagonal matrix from ATb, not the
inner product of diag A and b. |A| denotes the determinant of the square matrix
A. For p > 1, ∥a∥p denotes the p-norm of the vector a, ∥a∥p = (P
i |ai|p)1/p. If
nothing else is said, ∥·∥= ∥·∥2, the Euclidean norm. If A, B have the same size,
the Hadamard product (or direct product, or component-wise product) is deﬁned
as A ◦B = (ai,jbi,j)i,j. The symmetrisation function is sym A = (1/2)(A + AT).
Relations are vectorised in Matlab style, as are scalar functions: a ≥b means
that ai ≥bi for all i, and f(a) = (f(ai))i.
A.1.2
Probability. Miscellaneous
We do not distinguish notationally between a random variable and its possible
values. Vector or matrix random variables are written in the same way as vectors
or matrices. If a distribution has a density, we generally use the same notation
for the distribution and its density function. If x is a random variable, then
E[x] denotes the expectation (or expected value) of x. If A is an event, then
1All index sets and sets of data points are assumed to be ordered, although we use a notation
known from unordered sets.

A.1. Notation
207
Pr{A} denotes its probability. The probability space will usually be clear from
the context, but for clarity we often use an additional subscript, e.g. PrS{A} or
E
  ∼P[x]. The latter is sometimes abbreviated to EP[x]. By IA, we denote the
indicator function of an event A, i.e. IA = 1 if A is true, IA = 0 otherwise. Note
that Pr{A} = E[IA]. The delta distribution δ
  places mass 1 onto the point x and
no mass elsewhere, δ
  (B) = I{
  ∈B}. Let X , Y, Z be sets of random variables,
X , Y non-empty. We write X ⊥Y | Z to denote the conditional independence of
X and Y given Z: the conditional distribution of X given Y, Z does not depend
on Y.
log denotes the logarithm to Euler’s base e. The notation f(x) ∝g(x) means
that f(x) = cg(x) for c ̸= 0 constant w.r.t. x. We often use this notation with the
left hand side being a density. By sgn x, we denote the sign of x, i.e. sgn x = +1
for x > 0, sgn x = −1 for x < 0, and sgn 0 = 0. The Landau O-notation is deﬁned
as g(n) = O(f(n)) iﬀthere exists a constant c ≥0 such that g(n) ≤c f(n) for
almost all n.
We use some probability-theoretic concepts and notation which might be un-
familiar to the reader. A measure is denoted by dµ(x), the Lebesgue measure in
Rd is denoted by dx. If A is a measurable set, µ(A) =
R
I{
  ∈A}dµ(x) denotes its
mass under µ. A measure is ﬁnite if the mass of the whole space is ﬁnite, and a
probability measure if this mass is 1. If dµ is a probability measure, we denote
its distribution by µ. The sets A of mass 0 are called null sets.2 dµ1 is called
absolutely continuous w.r.t. dµ2 if all null sets of dµ1 are null sets of dµ2 (the
notation is dµ1 ≪dµ2). The theorem of Radon and Nikodym states that dµ1 has
a density f(x) w.r.t. dµ2, i.e.
µ1(A) =
Z
I{
  ∈A}f(x) dµ2(x)
for all dµ1-measurable A, iﬀdµ1 ≪dµ2. In this case,
f(x) = dµ1(x)
dµ2(x)
is called Radon-Nikodym derivative or simply density w.r.t. dµ2.
2We always assume that the underlying probability space is complete, i.e. its σ-ﬁeld contains
all subsets of null sets.

208
Appendix A. General Appendix
A.2
Linear Algebra. Useful Formulae
A.2.1
Partitioned Matrix Inverses. Woodbury Formula. Schur
Complements
Deﬁnition A.1 (Schur Complement) Let A ∈Rn,n be some nonsingular ma-
trix, and I ⊂{1, . . . , n} an ordered index set, I ̸= ∅, I ̸= {1, . . . , n}.
Let
R = {1, . . . , n} \ I be the complement of I, and assume that AI is nonsingular as
well3. The Schur complement A/AI is deﬁned as
A/AI = AR −AR,I(AI)−1AI,R.
Note that since both A and AI are nonsingular, so is the Schur complement,
and
|A| = |A/AI| |AI| .
If furthermore AR is nonsingular, the same relation holds for A/AR, and for the
special case AI = I, AR = I (but of diﬀerent size), we have the useful relation
I + U U T =
I + U TU
 ,
whenever the determinants exist.
Lemma A.1 (Partitioned Inverse Equations) Assume that AI is nonsingu-
lar. Then, A is nonsingular iﬀthe Schur complement A/AI is invertible, and
in this case:
 A−1
R
= (A/AI)−1 ,
 A−1
I,R
= −(A/AI)−1 (AR,I(AI)−1) ,
 A−1
R,I
= −((AI)−1AI,R) (A/AI)−1 ,
 A−1
I
= (AI)−1 + ((AI)−1AI,R) (A/AI)−1 (AR,I(AI)−1) .
Of course we can simply interchange I and R in these equations, which renders
the Woodbury formula as a useful corollary.
3Generalisations are possible using pseudo-inverses, but we do not require them here.

A.2. Linear Algebra. Useful Formulae
209
Lemma A.2 (Woodbury Formula) Assume that both AI, AR are nonsingu-
lar. Then, A is nonsingular iﬀany of the Schur complements A/AI, A/AR are
invertible. In this case, both complements are nonsingular, and we have
(A/AR)−1 =
 AI −AI,RA−1
R AR,I
−1
= (AI)−1 + (AI)−1AI,R (A/AI)−1 AR,I(AI)−1,
(A.1)
since both sides are equal to (A−1)I.
The Woodbury formula is most useful if |R| is much smaller than |I|. Its
straightforward application often leads to severe numerical instabilities, and it
should be used as a symbolic rewriting tool rather than for doing actual compu-
tations. The incremental Cholesky decomposition (see Section A.2.2) should be
preferred. For more details on Schur products, see [78].
A.2.2
Update of Cholesky Decomposition
In the context of linear systems with symmetric positive deﬁnite matrices, the
Cholesky decomposition is the method of choice (for an exact treatment), due to
its high numerical stability, eﬃciency and simplicity.
Deﬁnition A.2 (Cholesky Decomposition) Let M ∈Rn,n be a symmetric
matrix. The Cholesky decomposition of M exists iﬀM is positive deﬁnite, i.e.
xTM x > 0 for all x ̸= 0. It is deﬁned as
M = LLT,
where the Cholesky factor L is lower triangular with positive elements on the
diagonal.
L is deﬁned uniquely by this relation. Computing the Cholesky factor is algorith-
mically straightforward, and in contrast to many other decompositions, pivoting
and row/column permutations are not necessary. The decomposition costs O(n3),
but is more than twice as fast as a matrix inversion.4 The Cholesky factor can
4In fact, the proper and fastest way to invert M is to compute L, then M −1 from the latter.

210
Appendix A. General Appendix
be stored together with the system matrix itself in one n-by-n matrix plus an
additional n-vector. Once L is computed, systems M x = b are solved by back-
substitution: Lu = b, LTx = u. Furthermore, aTM −1b = (L−1a)T(L−1b).
Two back-substitutions come at the cost of one general matrix-vector multiplica-
tion.
The Woodbury formula (A.1), although often useful as a symbolic rewriting
tool, is notoriously unstable in the presence of round-oﬀerrors. It is our opinion
that for explicit computations, Cholesky techniques should always be preferred.5
Here is how you update the Cholesky factor for rank-1 changes of the system
matrix. In the sequel, we will use a slightly diﬀerent variant of the Cholesky
decomposition, namely let M = LDLT where D is diagonal with positive entries
and diag L = I. Suppose we want to compute the decomposition for M + vvT.
First, solve Lp = v by back-substitution, thus M + vvT = L(D + ppT)LT. If
we ﬁnd ˜LD′ ˜L
T = D + ppT, the new factor is L ˜L and the new diagonal matrix
is D′.
It turns out that ˜L really only depends on O(n) parameters and can be
computed in O(n) given p. Namely, ˜Li,j = piβj, i > j and ˜Li,i = 1. One can
also show that these parameters come out of the following recurrence: start with
t0 = 1, then iterate i = 1, . . . , n:
a = ti−1di + p2
i ,
ti = a
di
,
d′
i =
a
ti−1
,
βi = pi
a ,
where βn is not needed.
D′ can overwrite D.
If we want to update M to
M −vvT, the same method applies (given that the resulting matrix is positive
deﬁnite), but the following recurrence should be used instead: start with s0 = −1,
then iterate i = 1, . . . , n:
a = pisi−1,
d′
i = di + pia,
βi = a
d′
i
,
si = si −aβi.
Due to the simple form of ˜L, back-substitutions or multiplications with this
factor can be done in O(n). For example, if matrices of the form L−1A are to
5Unfortunately, these techniques are only poorly supported in standard Matlab. The com-
mand cholupdate performs a rank-1 update, but returns the new Cholesky factor explicitly,
instead of allowing for the implicit ˜L form.

A.3. Convex Functions
211
be updated in the sense that L is replaced by the new Cholesky factor, this can
be done eﬃciently as (L ˜L)−1A = ˜L
−1(L−1A). The modiﬁcations for the case
M = LLT are obvious: replace D by I and right-multiply ˜L by (D′)1/2.
In order to physically update the Cholesky factor L we can use the in-place
scheme described in Algorithm 3.
Again, if M = LLT, L′ has to be right-
multiplied by (D′)1/2 afterwards. Although this technique is routinely used in
numerical programming, we have come across it via [54].
Algorithm 3 Update of Cholesky Factor
for i = 1, . . . , n do
ρ = li,ipi, σ = 0. l′
i,i = li,i.
for j = i −1, i −2, . . . , 1 do
σ ←σ + ρ. ρ = li,jpj.
l′
i,j = li,j + σβj
end for
end for
A.2.3
Some Useful Formulae
In this section, we collect some formulae which do not ﬁt in elsewhere.
Lemma A.3 (Tower Properties) Let x, y be random variables such that E[yyT]
exists.
E[y] = E [E[y | x]] ,
Var[y] = Var [E[y | x]] + E [Var[y | x]] .
(A.2)
Here, Var[y] = E[(y −E[y])(y −E[y])T] and Var[y | x] = E[(y −E[y | x])(y −
E[y | x])T | x].
A.3
Convex Functions
Properties of convex functions and sets are of central importance in this thesis
and have an immense number of applications in machine learning and statistics.

212
Appendix A. General Appendix
A convex function can be globally lower-bounded by a hyperplane, and the bound
is locally tight wherever desired. Some of the powerful consequences of this simple
fact are developed here, a comprehensive treatment of the subject can be found
in [152]. The literature on convex optimisation is large, we refer to [24, 76, 77].
Deﬁnition A.3 (Convexity) A subset C ⊂V of a vector space V is convex if
for every x1, x2 ∈C, λ ∈(0, 1), λx1 + (1 −λ)x2 ∈C.
A function f : C →R (C convex) is convex if for every x1, x2 ∈C, λ ∈(0, 1),
f(λx1 + (1 −λ)x2) ≤λf(x1) + (1 −λ)f(x2).
Note that this is equivalent to the set epi(f) = {(x, y) ∈C × R | f(x) ≤y} being
convex. f is called strictly convex if the inequality is strict whenever x1 ̸= x2.
Note that if C is open and f is twice continuously diﬀerentiable on C then f is
strictly convex iﬀits Hessian is positive deﬁnite everywhere, and f is convex iﬀ
its Hessian is positive semideﬁnite everywhere.
Suppose that V is an inner product space with dual space V∗, so that linear
functions V →R can be written as uTx with uT ∈V∗. A half space is deﬁned
as {(x, y) | uTx −µ ≤y}. If f is convex, then the convex set epi(f) can be
represented as intersection of all half spaces which contain it (this is a property
of convex sets). But a half space contains epi(f) iﬀ
µ ≥sup
  ∈C
uTx −f(x),
and in fact to represent epi(f) and therefore f, we only need to consider the
halfspaces (u, µ) with
µ = f ∗(u) = sup
  ∈C
uTx −f(x),
(A.3)
where the domain of f ∗is the set of all u such that the supremum exists. f ∗is
called the Legendre dual of f. Intuitively, if f is nicely behaved, then the dual
represents f in terms of all tangent planes to f, and f ∗maps gradients to oﬀsets.
For any u ∈C∗= dom(f ∗), uTx −f ∗(u) is a tight aﬃne global lower bound to
f(x). One might expect that
f(x) = sup
  ∈C∗uTx −f ∗(u),
(A.4)

A.3. Convex Functions
213
which is indeed the case, furthermore f ∗is convex itself. We have the bound
uTx ≤f(x) + f ∗(u),
x ∈C, u ∈C∗.
(A.5)
The relationship f ↔f ∗is called Legendre or convex duality. If V = Rd, C is
open and f is strictly convex and continuously diﬀerentiable, we can solve (A.3)
by diﬀerentiating: u = ∇
  f(x). Due to the strict convexity, this equation can
have at most one solution. Thus, C∗is the gradient space of f, and u = ∇
  f(x)
deﬁnes a one-to-one mapping x ↔u between Legendre pairs. The mapping is
inverted via x = ∇
  f ∗(u), and (A.5) is an equality iﬀ(x, u) is a Legendre pair.
The duality between f(x) and f ∗(u) is illustrated in Figure A.1.
−3
−2
−1
0 
1 
2 
3 
4 
5 
6 
7 
−8
−6
−4
−2
0 
2 
4 
6 
8 
10
x
y
f(x) 
uTx − f*(u) 
uTx −  µ, µ > f*(u)
f*(u) = supx uTx − f(x)
−3
−2
−1
0
1
2
3
4
5
6
7
−6
−4
−2
0
2
4
6
8
10
x
y
uTx* − f*(u) < f(x*)
x* 
f(x) 
slope u 
f(x*) = supu uTx* − f*(u)
D(u*; u)
[Bregman] 
slope u* 
Figure A.1: Illustrations of convex (Legendre) duality. Left: f ∗parameterises aﬃne
tight lower bounds to f. Right: reconstruction of f from lower bounds given by f ∗;
Bregman divergence.
For every (ˆx, u), we have f(ˆx) ≥uT ˆx−f ∗(u), with equality iﬀu = ˆu, (ˆx, ˆu)
a Legendre pair. The error (w.r.t. local approximation at ˆx) made by using the
lower bound based on u instead, is quantiﬁed by
D(ˆu; u) = f(ˆx) −uT ˆx + f ∗(u) = (ˆu −u)T ˆx + f ∗(u) −f ∗(ˆu),
where (ˆx, ˆu) is a Legendre pair. D(ˆu; u) is called Bregman divergence w.r.t. f ∗,
and if f ∗is strictly convex, then D(ˆu; u) = 0 iﬀu = ˆu. Note that the Bregman
divergence is convex in u. By applying the same procedure to the dual f, we

214
Appendix A. General Appendix
obtain a Bregman divergence between elements of C. Maybe the most important
Bregman divergence, the relative entropy, is introduced below. See Figure A.1
for a graphical illustration.
Examples for convex dualities are given by the p-norms. If p, q > 1 such
that 1/p + 1/q = 1, then f(x) = (1/p)∥x∥p
p and f ∗(u) = (1/q)∥u∥q
q are duals.
Note that f(x) = (1/2)xTx is self-dual. Another important duality, featuring
the relative entropy, is given below.
A useful inequality can be directly deduced from Legendre duality:
Lemma A.4 (Jensen’s Inequality) If f is convex and x is a random variable
with ﬁnite mean, then
E [f(x)] ≥f (E[x]) .
If f is strictly convex, then equality here implies x = E[x] almost surely.
Namely, let ¯x = E[x]. Using (A.4), for every ε > 0 we have a u such that
f(x) −uT(x −¯x) ≥uT ¯x −f ∗(u) > f(¯x) + ε. Now, take expectations and let
ε →0.
In the remainder of this section, we introduce some information-theoretic
functions whose convexity properties we use extensively in this thesis. For an
excellent introduction to information theory, see [34].
Deﬁnition A.4 (Relative Entropy) Let P, Q be two probability measures on
the same space with Q ≪P, such that the density dQ/dP exists almost every-
where. The relative entropy is deﬁned as
D[Q ∥P] = EQ

log dQ
dP

=
Z 
log dQ
dP

dQ.
If Q is not absolutely continuous w.r.t. P, we set D[Q ∥P] = ∞. It is always
non-negative, and equal to 0 iﬀQ = P. The function (Q, P) 7→D[Q ∥P] is
strictly convex.
The non-negativity of D[Q ∥P] follows directly from the strict convexity of
−log and Jensen’s Inequality A.4. It is sometimes called information inequality.

A.3. Convex Functions
215
The convexity of D[Q ∥P] follows from the log sum inequality (see [34], Theo-
rem 2.7.1). Note that D[Q ∥P] is not a metric, since it is not symmetric in general
and also does not satisfy the triangle inequality in its usual form. An intuitive
motivation for D[Q ∥P] is in terms of coding cost. If we were to encode symbols
drawn i.i.d. from Q, but used a code optimal for P, we would lose on average
D[Q ∥P] “nats” per symbol as compared to using an optimal code for Q. Thus,
Q may be viewed as “true distribution” and P as approximation to Q, although
in some applications these roles are ﬂipped.
Let Q, P be distributions of w, absolutely continuous w.r.t. some dominating
positive measure. If the mass of w is concentrated on a ﬁnite set of size L, say
{1, . . . , L}, i.e. the dominating measure is the counting measure putting weight
1 on each of l ∈{1, . . . , L}, then Q(w) and P(w) are ﬁnite distributions and
D[Q ∥P] =
L
X
l=1
PrQ{w = l} log PrQ{w = l}
PrP{w = l}.
(A.6)
If w ∈Rm, the dominating measure is typically the Lebesgue measure dw and
D[Q ∥P] = E
 ∼Q

log Q(w)
P(w)

(recall that we use the same notation for a distribution and its density w.r.t. dw,
i.e. Q(w) = dQ/dw). A special case of (A.6) for Bernoulli distributions (skew
coins) with probabilities of heads q, p is frequently used in this thesis:
DBer[q ∥p] = q log q
p + (1 −q) log 1 −q
1 −p.
(A.7)
DBer is convex in (q, p), furthermore p 7→DBer[q ∥p] is strictly monotonically
increasing for p ≥q, mapping [q, 1) to [0, ∞), and strictly decreasing for p ≤q,
mapping (0, q] to [0, ∞).
Q 7→D[Q ∥P] is convex, and its representation w.r.t. its Legendre dual is
given by
D[Q ∥P] = max
λ
 E
 ∼Q [λ(w)] −log E
 ∼P

eλ(
 )
,
(A.8)
where the maximum is over all λ(w) measurable w.r.t. P(w) (λ is only deﬁned
up to an additive constant). Thus, its Legendre dual is the log partition function

216
Appendix A. General Appendix
f ∗(λ) = log EP[exp(λ(w))]. For Q with D[Q ∥P] < ∞, the dual parameter is
λ(w) = log dQ(w)/dP(w) + c. This is shown in Section 3.2.2, within the proof
of Theorem 3.2. The relative entropy also emerges as Bregman divergence. If
ˆλ = log d ˆQ/dP is the choice for the Legendre pair ( ˆQ, ˆλ), corresponding to c = 0,
then it is easy to see that D( ˆQ; Q) = D[Q ∥ˆQ] (including the case ∞= ∞) as
Bregman divergence w.r.t. D[· ∥P].
The relative entropy can be used to derive other information-theoretic func-
tions. If we ﬁx a base measure P0 (ﬁnite, need not be a probability), the entropy
can be deﬁned as H[Q] = −D[Q ∥P0]. In the case of ﬁnite distributions (Equa-
tion A.6), one normally uses the counting measure for P0, therefore
H[Q] = −
L
X
l=1
PrQ{w = l} log PrQ{w = l}.
(A.9)
In this case, H[Q] ≤log L. For continuous distributions over Rd, the uniform
(Lebesgue) measure is not ﬁnite. The usual remedy is to subtract oﬀan inﬁnite
part of the entropy which does not depend on the argument Q, ending up with
the diﬀerential entropy
H[Q] = −
Z
Q(w) log Q(w) dw.
(A.10)
Both entropy and diﬀerential entropy are concave functions (being the negative
of convex ones). The convex duality representation for the “negentropy” −H[Q]
is given by
−H[Q] = max
λ
EQ[λ(w)] −log
Z
eλ(
 ) dw,
(A.11)
which can be obtained from (A.8) by using the Lebesgue measure for P and
subtracting oﬀthe same inﬁnite part on both sides. The corresponding Bregman
divergence is D( ˆQ; Q) = D[Q ∥ˆQ], where a Legendre pair satisﬁes ˆQ ∝exp(ˆλ).
A.4
Exponential Families. Gaussians
In this section, we collect deﬁnitions and useful properties of general exponential
families of distributions and of the Gaussian family.

A.4. Exponential Families. Gaussians
217
A.4.1
Exponential Families
Deﬁnition A.5 (Exponential Family) A set F of distributions with densities
P(x|θ) = exp
 θTϕ(x) + h(x) −Φ(θ)

,
θ ∈Θ,
Φ(θ) = log
Z
exp
 θTϕ(x) + h(x)

dx
is called an exponential family. Here, θ are called natural parameters, Θ the nat-
ural parameter space, ϕ(x) the suﬃcient statistics, and Φ(θ) is the log partition
function. Furthermore, η = E
  [ϕ(x)] are called moment parameters.
One of the important reasons for considering exponential families is that the
likelihood function for i.i.d. data from F is a function of the sample average of the
suﬃcient statistics ϕ(x) which has the ﬁxed dimensionality of θ, independent of
the sample size. Even if a model does not give rise to posteriors in an exponential
family (see Section A.6), members of F can be used as approximating distri-
butions, since new information can be incorporated without increasing the size
of the parametric representation. Many familiar distributions form exponential
families, such as Gaussians (see Section A.4.3), multinomials, gammas, etc.
The natural parameter space Θ for θ is always convex. If there are linear or
aﬃne dependencies between the components of ϕ(x), then some components in
θ are redundant, and the representation is called overcomplete. Otherwise, it is
called minimal. Note that many useful properties hold only (in general) for min-
imal representations, which are also most useful in practice, however sometimes
notationally clumsy to work with. Our approach here is to state general prop-
erties for minimal representations only, however use these properties for special
overcomplete representations occasionally. This can be justiﬁed by adding linear
constraints on θ, which do not destroy the convexity of Θ. In the remainder of
this section, we assume that the representation of F is minimal.
The log partition function Φ(θ) is closely related to the cumulant generating
function of ϕ(x), x ∼P(x|θ):
log E
 
exp(εTϕ(x))

= Φ(θ + ε) −Φ(θ)

218
Appendix A. General Appendix
which exists iﬀθ + ε ∈Θ. Thus, if θ is in the interior of Θ, the cumulants of
ϕ(x) are obtained as derivatives of Φ(θ), especially ∇
  Φ = E
  [ϕ(x)] = η and
∇∇
  Φ = Var
  [ϕ(x)]. Since the representation is minimal, we see that Φ(θ) is
strictly convex, and using Legendre duality (Section A.3), we obtain the following
Lemma A.5 (Natural and Moment Parameters) If F is an exponential fam-
ily with minimal representation, then there is a bijective mapping between the
natural parameters θ and the moment parameters η. The log partition function
Φ(θ) is strictly convex and has the Legendre dual
Ψ(η) = E
  [log P(x|η) −h(x)] ,
where E
  [·] denotes expectation w.r.t. P(x|η) = P(x|θ(η)). Conversions between
θ and η are done as follows:
η(θ) = ∇
  Φ,
θ(η) = ∇
  Ψ.
Note that θ are also called exponential parameters, and η are also known as mean
parameters.
If P(x|θ) is in F and x = (yT zT)T, then the conditional distribution P(y|z)
is in an exponential family with suﬃcient statistics ϕ′(y) = ϕ(x) and h′(y) =
h(x) and has natural parameters θ within this family. The new family typically
has an overcomplete representation even if F is minimal, so a re-parameterisation
is necessary if we want to use the properties of minimally parameterised families.
Note that the class of all exponential family distributions is not closed w.r.t.
marginalisation. For example, if P(x|θ) is a joint of a continuous Gaussian and
a discrete multinomial variable, marginalising over the latter results in a mixture
of Gaussians which is in general not in an exponential family.
A number of
exponential subfamilies such as the (multivariate) Gaussian or multinomial ones
are however closed under marginalisation.
Lemma A.6 (Product of Exponential Distributions) If h ≡0, then a prod-
uct of densities from F is an unnormalised member of F:
m
Y
j=1
P(x|θj) = P
 
x

m
X
j=1
θj
!
exp
 
Φ
 m
X
j=1
θj
!
−
m
X
j=1
Φ(θj)
!
,

A.4. Exponential Families. Gaussians
219
given that P
j θj lies in Θ.
Given a positive function f(x), we can induce a tilted exponential family
from F by essentially adding log f(x) to h(x) and recomputing the log partition
function.
Deﬁnition A.6 (Tilted Exponential Family) If F is an exponential family
with natural parameter θ and f(x) is a positive function such that
Φf(θ) = log EP (·|
  ) [f(x)] + Φ(θ)
exists for every θ, then the tilted exponential family Ff induced by f(x) from F
contains the distributions
Pf(x|θ) = exp
 θTϕ(x) + h(x) + log f(x) −Φf(θ)

∝f(x)P(x|θ).
Ff has the same natural parameter space Θ than F.
Since Ff is a proper exponential family, the moment parameter of Pf(x|θ) can
be computed as derivatives of Φf(θ), i.e.
EPf(·|
  )[ϕ(x)] = ∇
  log EP (·|
  ) [f(x)] + η.
(A.12)
Note that Ff is the same set of distributions than F iﬀlog f(x) = εTϕ(x) for
some vector ε such that ε + Θ = {ε + θ | θ ∈Θ} = Θ.
If we “update” a distribution from F by multiplying with a positive factor
and renormalising, we will end up in F iﬀthe update factor has the structure of
a ratio of members of F.
Deﬁnition A.7 (Unnormalised Exponential Family) If F is an exponen-
tial family with natural parameter θ ∈Θ, the set of functions
P U(x|θ) = exp
 θTϕ(x)

,
θ = θ1 −θ2, θ1, θ2 ∈Θ,
is referred to as unnormalised exponential family F U associated with F.
Note that members of F U are in general no probability densities, and some of
them may not be normalisable at all.
If P(x|θ) ∈F, P U(x|˜θ) ∈F U, then
P(x|θ)P U(x|˜θ) is proportional to a member of F (namely, to P(x|θ + ˜θ)) iﬀ
θ + ˜θ ∈Θ. Note also that 1 ≡P U(x|0) ∈F U.

220
Appendix A. General Appendix
Lemma A.7 (Relative Entropy) The relative entropy
D[P(x|θ) ∥P(x|θ∗)] = E
  [log P(x|θ) −log P(x|θ∗)]
(see Deﬁnition A.4), also denoted D[θ ∥θ∗] or D[η ∥η∗], is given by
D[θ ∥θ∗] = ηT(θ −θ∗) + Φ(θ∗) −Φ(θ)
= (θ∗)T(η∗−η) + Ψ(η) −Ψ(η∗).
(A.13)
From (A.13), we see that D[θ ∥θ∗] is convex in θ∗, and D[η ∥η∗] is convex in η.
A.4.2
I-Projections
The relative entropy (Deﬁnition A.4) is a natural replacement for Euclidean dis-
tance (or its square) when dealing with manifolds of probability distributions,
allowing for generalisations of important geometrical concepts such as orthogonal
projection, triangle inequalities and Pythagorean theorems. The equivalent of
orthogonal projection has been introduced by Csisz´ar [43] as I-projections.
Deﬁnition A.8 (I-projections) Let P be some closed convex set (see Section A.3)
of distributions P, and let Q be some distribution. The e-projection of Q onto P
is
P [e] = argmin
P ∈P
D[P ∥Q],
and the m-projection of Q onto P is
P [m] = argmin
P ∈P
D[Q ∥P].
They are both examples of I-projections.
Note that both I-projections are well-deﬁned, since P is convex and the rel-
ative entropy is convex in both arguments.
Also, they project Q onto itself
iﬀQ ∈P. Suppose that P is an exponential family (Deﬁnition A.5). Then,
D[Q ∥P] = Φ(θ) −θTEQ[ϕ(x)], and P [m] is given by the moment parameter
µ[m] = EQ[ϕ(x)].
In this important special case, the m-projection is equiva-
lent to moment matching between Q and P [m].
Another example is given in
Section A.6.4.

A.4. Exponential Families. Gaussians
221
A.4.3
Gaussian Variables
Deﬁnition A.9 (Gaussian Distribution) A variable x ∈Rd has a (non-de-
generate) Gaussian distribution with mean µ and covariance matrix Σ iﬀits
p.d.f. is given by
P(x) = N(x|µ, Σ) = |2πΣ|−d/2 exp

−1
2(x −µ)TΣ−1(x −µ)

.
A less stringent deﬁnition is the following: x has Gaussian distribution if for
every vector a ∈Rd, the variable aTx has either a non-degenerate Gaussian dis-
tribution or is (almost surely) constant. This allows for degenerate distributions
with supports conﬁned to aﬃne subspaces of Rd.
The Gaussian family is the most important family of continuous distributions,
both in theory and in practice, for a number of reasons. It has superb closeness
properties, e.g. under linear transformations, conditioning and marginalisation.
The Gaussian arises as the ideal noise or error distribution from the central limit
theorem. Among all distributions with a given mean and covariance matrix, it has
the least structure, i.e. maximises the diﬀerential entropy. All cumulants of order
higher than 2 vanish (the cumulant generating function is θTµ + (1/2)θTΣθ, a
quadratic). Gaussian variables which are not correlated, are independent.
In this thesis, Gaussians play a large role.
Gaussian processes (GPs, see
Section 2.1) induce Gaussian distributions when evaluated on ﬁnite point sets.
In the remainder of this section, we introduce some notation which will make it
easier to manipulate Gaussian expressions. In Section A.4.3, we collect a number
of Gaussian formulae which are used in this thesis.
The Gaussian family is an exponential family, and from Deﬁnition A.7 we
have
Deﬁnition A.10 (Unnormalised Gaussian)
N U(x|r, V , c) = exp

−1
2xTV x + rTx −1
2c

,
where V is symmetric (w.l.o.g.), but need not be positive deﬁnite. Furthermore,
N U(x|r, V ) = N U(x|r, V , 0). A centred version is
N UC(x|µ, V ) = exp

−1
2(x −µ)TV (x −µ)

.

222
Appendix A. General Appendix
Note that for a given N U, there may exist none, one or many N UC which
are proportional.
N U(x|r, V ) is proportional to a Gaussian density iﬀV is
positive deﬁnite. We can use r and V as natural parameters θ, together with
the suﬃcient statistics x and −(1/2)xxT. This parameterisation is not minimal,
due to the symmetry constraint on V , but is easier to work with than a minimal
one.
The corresponding moment parameters are µ = V −1r and Σ = V −1,
i.e. mean and covariance matrix6 of the Gaussian P(x|θ) ∝N U(x|r, V ). The
conversion between natural and moment parameters requires the inversion of a
positive deﬁnite matrix. The log partition function is given by
Φ(θ) = 1
2
 rTV −1r −log
2πV −1
.
The set of all N U is the associated unnormalised exponential family. Conversion
formulae are given in the following
Lemma A.8 (Conversions) Conversions between (unnormalised) Gaussians:
• N(µ, Σ) ↔N UC(µ, V )
N(µ, Σ) = |2πΣ|−1/2 N UC  µ, Σ−1
,
N UC(µ, V ) =
2πV −11/2 N
 µ, V −1
,
V pos. def.
• N UC(µ, V ) ↔N U(r, V , c)
N UC(µ, V ) = N U  V µ, V , µTV µ

,
N U(r, V , c) = N UC(µ, V ) exp

−1
2
 c −µTr

,
V µ = r.
As specialisation of Deﬁnition A.6, for some positive function f(x), let
N U
f (x|r, V , c) = f(x)N U(x|r, V , c).
If N U
f is normalisable for all positive deﬁnite V , then ˆP ∝N U
f form a tilted
exponential family, and we can compute the moments of ˆP using derivatives of
log Z(V , r) = log E(
 ,
  ) [f(x)] ,
6Strictly speaking, the second moment parameter is Eθ[−(1/2)xxT ] = −(1/2)(Σ + µµT ).

A.4. Exponential Families. Gaussians
223
namely
E ˆP[x] = ∇
  log Z + µ,
Var ˆP[x] = ∇∇
  log Z + Σ
(A.14)
(see Section A.4.1).
Manipulating Gaussians in the sense of forming linear combinations, marginal-
isation and conditioning boils down to simple linear algebra, as is summarised in
the following lemmas.
Lemma A.9 (Conditional Gaussian) Let x ∈Rn be distributed as N(µ, Σ).
If I ⊂{1, . . . , n}, R = {1, . . . , n} \ I, then xR|xI is Gaussian with covariance
matrix
Σ(R|I) =
 Σ−1−1
R = Σ/ΣI = ΣR −ΣR,I (ΣI)−1 ΣI,R
and mean
µ(R|I) = µR + (Σ/ΣR)(ΣR,I/Σ)(xI −µI) = µR + ΣR,I(ΣI)−1(xI −µI).
This can be shown by taking N(x|µ, Σ) as function of xR and use Lemma A.1.
Note that the covariance matrix of xR|xI does not depend on xI.
Lemma A.10 (Sums and Aﬃne Transforms) Let xi ∼N(µi, Σi) be inde-
pendent Gaussian variables and y = P
i Aixi + b. Then,
y ∼N
 
y

X
i
Aiµi + b,
X
i
AiΣiAT
i
!
.
Marginalisation is a special case: if x ∼N(µ, Σ) and I ⊂{1, . . . , d} does not
contain duplicates, then xI ∼N(µI, ΣI).
For unnormalised Gaussians, we note
N U  Ax + b
 r, V , c

= N U  x
 AT(V b + r), ATV A, c + bTV b

.
Products of unnormalised Gaussians are unnormalised Gaussians again, and
the natural parameters combine additively (see Lemma A.6).

224
Appendix A. General Appendix
Lemma A.11 (Products) Let there be m unnormalised Gaussians over x ∈
Rn, each of them either of the form N UC(x|µi, V i) or N U(x|ri, V i, ci). Convert
the former via
ri = V iµi,
ci = µT
i V iµi.
Deﬁne V , r and µ as
V =
X
i
V i,
r =
X
i
ri,
µ = V −1r,
where we assume that V is invertible.7 Then,
m
Y
i=1
N U(x|ri, V i, ci) = N U(x|r, V ,
X
i
ci)
= N UC(x|µ, V ) × exp
 
1
2rTµ −1
2
m
X
i=1
ci
!
.
(A.15)
If V is positive deﬁnite, the ﬁrst factor on the right is simply |2πΣ|1/2N(x|µ, Σ),
Σ = V −1. Under additional conditions, we can obtain a recursive formula which
we state for m = 2 (for larger m, use Lemma A.11 to combine m −1 factors).
Lemma A.12 (Products, Recursive Formulation) Under the conditions of
Lemma A.11 (m = 2) we assume that V 1, V 2 and V −1
1
+ V −1
2
are invertible.
Then,
N UC(x|µ1, V 1)N UC(x|µ2, V 2) = N UC(x|µ, V )N UC 
µ1
µ2,
 V −1
1
+ V −1
2
−1
.
(A.16)
If furthermore both V 1 and V 2 are positive deﬁnite and Σi = V −1
i , this
becomes the familiar equation
N(x|µ1, Σ1)N(x|µ2, Σ2) = N(x|µ, Σ)N(µ1|µ2, Σ1 + Σ2).
Lemma A.13 (Diﬀerential and Relative Entropy) The diﬀerential entropy
of a Gaussian is
H[N(µ, Σ)] = 1
2 log |2π e Σ| .
7The single V i need not be invertible.

A.5. Pattern Recognition
225
The relative entropy (see Deﬁnition A.7) between two Gaussians is given by
D[N(µ1, Σ1) ∥N(µ0, Σ0)] = 1
2 log
Σ−1
1 Σ0
 + 1
2 tr
 Σ−1
1 Σ0
−1 −I

+ 1
2 (µ1 −µ0)T Σ−1
0 (µ1 −µ0) .
(A.17)
A.5
Pattern Recognition
In this section, we brieﬂy introduce the general problem this thesis is mainly
concerned with. In the classiﬁcation or pattern recognition problem, we are given
data S = {(xi, yi) | i = 1, . . . , n}, xi ∈X , yi ∈Y, sampled independently and
identically distributed (i.i.d.) from an unknown data distribution over X × Y.
Here, Y is a ﬁnite set. If C = |Y| > 2, we will assume that Y = {1, . . . , C}, while
for C = 2, Y = {−1, +1}. The latter situation is called the binary classiﬁcation
problem, while the case C > 2 is referred to as multi-class problem. The sample
S is also called training set. Our goal is to compute a classiﬁcation function or
classiﬁer c : X →Y from S which has small generalisation error
gen(c) = Pr {c(x∗) ̸= y∗} = E

I{c(
 ∗)̸=y∗}

,
where (x∗, y∗) is sampled from the data distribution, independently of S. c has
to be computed from S only, without any further deﬁnite knowledge about the
data distribution. This process is called training. A more ambitious goal is to
estimate the conditional data distribution P(y∗|x∗) or related moments thereof
directly, leading to so-called predictors. In the context of binary classiﬁcation,
many classiﬁers are deﬁned in terms of discriminants, i.e. real-valued functions f
for which we obtain classiﬁers by thresholding: c(x) = sgn(f(x) −θ), where θ is
a threshold. This notion can be generalised to the multi-class problem by using
one discriminant fy per class and setting c(x) = argmaxy fy(x).
Functions of the training sample S are referred to as (empirical) statistics.
An important statistic is the empirical or training error
emp(c, S) = 1
n
n
X
i=1
I{c(
 i)̸=yi}.

226
Appendix A. General Appendix
Note that both generalisation and empirical error are deﬁned as expectations of
the so-called zero-one loss l0/1(c) = I{c(
  )̸=y}. This scenario can be generalised to
other non-negative loss functions of predictors, in which case the expected loss
over the data distribution is called (true) risk and the expected loss over the
empirical distribution n−1 Pn
i=1 δ(
 i,yi) is called empirical risk. This framework
encompasses other statistical problems such as regression estimation where Y =
R, and our task is to estimate E[y|x] as a function of x.
For any ﬁxed classiﬁer c, the empirical error will converge to the generalisation
error almost surely, by the strong law of large numbers. But if c is chosen depend-
ing on S, e.g. to minimise the empirical error over some broad set of candidates,
we will typically not end up with the best candidate w.r.t. gen(·), in fact not even
with an acceptable performance. This problem is known as the overﬁtting eﬀect
and can be attributed to the fact that trying to ﬁt the training data overly well
leads to our choice being too dependent on the noise in S. One way around this
is to penalise supposedly overcomplex candidates in the choice of c, this is called
regularisation. A more principled, yet often also more costly solution is Bayesian
analysis (see Section A.6) where a classiﬁer is constructed as expectation over a
set of candidates or models. Also here, overcomplex candidates are usually given
low weights in the expectation. One of the key points we would like to make in
this thesis is that the notion of complexity cannot be deﬁned in a both universal8
and practically feasible way. It has to depend on the context of the statistical
problem and on available prior knowledge about characteristics and constraints.
A.6
Bayesian Inference and Approximations
In this section, we introduce some aspects of Bayesian analysis relevant to this
thesis. For more detailed accounts, see [15, 14, 23]. We also brieﬂy sketch the
basics of some general approximation techniques for Bayesian inference which are
relevant to machine learning. An excellent up-to-date introduction to many of
these is given in [84].
8A universal deﬁnition would be Kolmogorov complexity (e.g., [34], Chap. 7) which is however
not practically feasible to use.

A.6. Bayesian Inference and Approximations
227
A.6.1
Probabilistic Modelling
A probabilistic model is a simpliﬁed abstract description of a (partially) ob-
servable data source. Consider the binary classiﬁcation problem deﬁned in Sec-
tion A.5. The observed variables of the domain are the input points x ∈X and
the targets (or labels) y ∈{−1, +1}. A simple model is obtained by introducing a
latent variable u ∈R together with the assumption that y ⊥x | u. The introduc-
tion of u can be motivated by a variety of reasons. First, our prior assumptions
include the notion of smoothness: the targets at two close-by input points should
a priori be the same with high probability. To express this, we need to include
the probability P(y|x) amongst the variables, although it cannot be observed.
Second, it is easier to model P(u|x) and P(y|u) independently than P(y|x) di-
rectly. The model description is completed by choosing prior distributions for
P(u|x) and P(y|u).
The speciﬁcation of P(u|x) is sometimes called the systematic component of
the model, while the speciﬁcation of P(y|u) is the random component. The former
can be speciﬁed in a parametric or a non-parametric way: postulate a parametric
family P(u|x, w) = δu(
  ;
 )(u) and a prior P(w) with the dimensionality of w
independent of dataset sizes, or place a prior distribution directly on the proba-
bilistic relationship x 7→u which penalises u(x) by low probability assignment
for violating prior assumptions, e.g. for behaving non-smooth, being discontinu-
ous, etc.9 Non-parametric modelling and relationships to parametric models are
discussed in some detail in Section 2.1. Semi-parametric models combine both
ways by for example modelling u(x) as sum of a parametric and a non-parametric
“residual” part. Semi-parametric models are useful to test a parametric modelling
assumption by observing the relevance of the non-parametric residual part on the
prediction of u.
Noise distributions P(y|u) for several standard models are given in Section 2.1.2.
Both P(y|u) and P(u|x) can depend on further parameters, called hyperparam-
eters, their prior distributions are called hyperpriors. Note that for a parametric
9Some care is required here, because u(x) in general does not have to be a deterministic
function, but can be a random process for which characteristics like smoothness and continuity
are deﬁned diﬀerently, see Section 2.1.1.

228
Appendix A. General Appendix
model, the primary parameter vector w is not treated as hyperparameter. Note
that we have not said anything about the distribution of the input points x.
These are examples of covariates, variables which can be considered given at any
time when a prediction of the target variables is required. To sum up, our domain
partitions into observed and latent variables, the former into observed target vari-
ables and covariates, the latter into latent target variables and so-called nuisance
variables. We are interested in predicting the target variables from covariates
and a training sample S. In our example, y is the only target variable, while
u is a nuisance variable. The main assumption behind this model is that the
diﬀerent cases (xi, yi) are sampled i.i.d., given a common latent parameter w
(or latent process u(·)) sampled from the prior, i.e. that the data distribution
satisﬁes exchangeability (de Finetti’s theorem, see [15]).
A.6.2
Bayesian Analysis
Bayesian inference and prediction, as central part of Bayesian analysis work as
follows. Given a complete model for all variables except the covariates, we condi-
tion on the observed data S and marginalise over the nuisance (latent) variables
to obtain the posterior distribution
P(θ|S) ∝P(θ)P(S|θ) = P(θ)
Z
P(S, h|θ) dh.
Here, h are the nuisance variables, P(S, h|θ) is called complete data likelihood,
and P(S|θ) is called observed data likelihood. In our example, h = u. By the
exchangeability assumption,
P(S|θ) =
n
Y
i=1
P(yi|xi, θ),
P(yi|xi, θ) =
Z
P(yi|ui)P(ui|xi, θ) dui.
For the same reason, the predictive distribution for a test case (x∗, y∗) is given by
P(y∗|x∗, S) =
Z
P(y∗|x∗, θ)P(θ|S) dθ.
Thus, Bayesian inference amounts to updating the prior belief P(θ) into the
posterior belief P(θ|S). Predictions are then obtained as posterior averages.

A.6. Bayesian Inference and Approximations
229
The computational requirements of Bayesian inference can be prohibitive, and
techniques to approximate Bayesian computations such as posterior expectations
are essential in a Bayesian’s toolbox. We discuss some of these in Section A.6.3.
A frequently used general empirical Bayesian method for marginalising over nui-
sance hyperparameters is marginal likelihood maximisation or maximum likeli-
hood II. Let θ be split into primary parameters w and hyperparameters α, and
P(θ) = P(w|α)P(α). Typically, α is of much smaller dimension than w. The
posterior is P(θ|S) = P(w|S, α)P(α|S). If S is suﬃciently large, P(α|S) will
usually be highly peaked around its mode ˆα due to central limit behaviour, and
we can replace it by δ ˆ
  (α). This is an example of a maximum a posteriori (MAP)
approximation. Now, posterior expectations can be approximated as
EP (
  |S) [f(θ)] = EP (
 |S, ˆ
 ) [f(w, ˆα)] .
ˆα is the maximiser of log P(S, α) = log P(S|α) + log P(α), where
P(S|α) =
Z
P(S|w, α)P(w|α) dw
is called marginal (data) likelihood. Since log P(α) is typically a simple function
of α, the main task is to optimise log P(S|α) w.r.t. α. In many situations, the
log marginal likelihood cannot be computed tractably either, due to the integral
over w, and further approximations have to be considered. Finding and plugging
in ˆα is a special case of (Bayesian) model selection.
A.6.3
Approximations to Bayesian Inference
Let f(θ) be a positive function, where for simplicity we assume that dom(f) = Rd.
Let I =
R
f(θ) dθ < ∞, and P(θ) = f(θ)/I. A general approximation technique
should render ˜I ≈I, furthermore an approximation Q(θ) to P(θ) such that
EP[g(θ)] ≈EQ[g(θ)] for other functions g, and the latter expectation can be
computed feasibly. The focus here is on methods which can work well even if d
is large, given that P(θ) is fairly peaked and does not have heavy tails.
The oldest approximation technique for Bayesian inference was introduced by
Laplace to approximate the posterior for a binomial model, it is called Laplace

230
Appendix A. General Appendix
or saddle-point approximation. Suppose that −log f(θ) has a unique minimum
point ˆθ and is twice continuously diﬀerentiable in a open set containing ˆθ. Then,
the Hessian H is positive deﬁnite at ˆθ and we can approximate
f(θ) ≈exp

log f(ˆθ) −1
2(θ −ˆθ)TH(θ −ˆθ)

.
By normalising the approximation, we have Q(θ) = N(ˆθ, H−1) and log ˜I =
f(ˆθ)−(1/2) log |2πH|. Note that this approximation is entirely local, depending
only on the value and the curvature of f around its mode. In special situations,
the Laplace approximation can be applied to multimodal P(θ), namely if the
modes are clearly separated and all have the same local shape. On the other
hand, it can perform poor even on unimodal distributions due to its local nature,
and it is not applicable if f is not twice continuously diﬀerentiable around its
mode.
Variational approximations work by reformulating the approximation problem
into an optimisation problem, typically the maximisation of a lower bound. One
approach is to bound f(θ) ≥g(θ, λ) such that expectations w.r.t. g(θ, λ) can be
done feasibly for every λ. Then, I ≥
R
g(θ, λ) dθ for every λ. The optimisation
problem is now to choose λ in order to maximise the lower bound on I. For the
resulting λ∗, we have ˜I =
R
g(θ, λ∗) dθ and Q(θ) = g(θ, λ∗)/˜I. A diﬀerent, more
global variational approach can be derived from the convex duality (A.11) between
negentropy and log partition function (substitute λ(θ) = log f(θ)), leading to
log I = log
Z
elog f(
  ) dθ ≥EQ[log f(θ)] + H[Q]
(A.18)
for any distribution Q(θ). The diﬀerence between log I and the lower bound (i.e.
the Bregman divergence) is given by D[Q ∥P] (note that (log f, P) is a Legendre
pair). We can now choose Q amongst a tractable family in order to maximise the
lower bound. In contrast to the other variational technique described above, this
minimises at the same time the relative entropy D[· ∥P], bringing us closer to the
target distribution P. Note that as opposed to the Laplace approximation, the
variational techniques render a guaranteed lower bound on I. This is reassuring if
our goal is to maximise I w.r.t. other parameters, but as long as no corresponding

A.6. Bayesian Inference and Approximations
231
upper bound is available, we have no method of checking the tightness of the lower
bound which would be considerably easier than the original approximation task (a
general method to upper bound the log partition function for sparsely connected
graphical models is given in [204], and a method for two-layer networks with
binary variables can be found in [80]). Moreover, if our goal is to approximate
I or P(θ), insisting on a lower bound seems fairly restrictive. A variant of the
second variational method, called variational Bayes has recently received a lot
of attention [7, 62, 19].
Variational Bayes is essentially a special case of the
variational mean ﬁeld approximation, in which the family to draw Q from is
deﬁned via factorisation constraints. This allows for a simple analytic update of
one of the factors, given all the others are ﬁxed, and this process can be iterated.
Note that historically methods are called “mean ﬁeld” approximations if they
use completely factorised variational distributions, and the obvious extension to
partial factorisations is called “generalised mean ﬁeld” or “structural mean ﬁeld”,
but we do not follow this nomenclature.
A general framework for variational
Bayes has been given in [62], and general user-friendly software is available [19].
Given that variational mean ﬁeld approximations can lead to rather poor
results, a range of improvements have been suggested. A general ansatz is to
look for approximations to a so-called negative free energy I = log
R
exp(g(θ)) dθ.
From Section A.3 we know that the convex dual is the negentropy, so that
log
Z
exp(g(θ)) dθ = max
Q EQ[g(θ)] + H[Q],
and the optimal Q (such that (g, Q) is a Legendre pair) is Q(θ) ∝exp(g(θ)), the
desired target distribution. The mean ﬁeld approximation maximises the lower
bound w.r.t. factorised Q distributions, and if our goal is to approximate some
marginals of the target distribution, we may use the corresponding marginals of
the maximiser Q. A diﬀerent approach is to approximate the r.h.s. (in particu-
lar the intractable entropy term) by a function of certain marginals of Q only,
disregarding the fact that many sets of marginals we deal with during the opti-
misation are not actually consistent with a single joint Q. Nevertheless, because
we can choose overlapping marginals, the approximation can be a signiﬁcant im-
provement on mean ﬁeld, although it is not in general a lower bound anymore.

232
Appendix A. General Appendix
Algorithms will maximise the r.h.s. under the constraint that the Q-marginals are
at least locally consistent, i.e. marginalise correctly w.r.t. overlaps. On structured
graphs, these are known as loopy belief propagation, generalised belief propagation,
etc. [215, 128, 119]. We will be interested in Section 4.3 in a variant of belief
propagation called expectation propagation [124], but applied to dense rather than
sparse graphs.
Maybe the most developed class of approximate inference techniques in Bayes-
ian statistics are Markov chain Monte Carlo (MCMC) methods. The idea is that
if we were able to obtain independent samples from the target distribution P(θ),
we could approximate an expectation w.r.t. P by the corresponding empirical
average over the samples. By the law of large numbers, the convergence of this
average is roughly O(n−1/2), n the number of samples, where the constant depends
on smoothness properties of P and the function to average over, but is largely
independent of the dimensionality of θ.
It is usually not possible to directly
obtain independent samples from complicated posteriors if θ is high-dimensional,
but it is possible to deﬁne an ergodic Markov chain which has the posterior as
equilibrium distribution, so that if we run this chain long enough and discard an
initial part of the trajectory for “burn-in”, the sample path average will converge
just as well, again in principle independent of θ’s dimensionality. There is an
enormous number of sampling techniques to choose from, for an excellent review
consult [130], see also [151, 104, 64]. Powerful software packages for MCMC are
available (e.g., [188]).
A.6.4
Lower Bound Maximisation. Expectation Maximisation
A very important concept of classical statistics is maximum likelihood (ML) es-
timation. Suppose we are given some i.i.d. data S = {x1, . . . , xn}, and we use a
model P(x|θ) based on an exponential family (Deﬁnition A.5). The log likelihood
function is
l(θ) = log P(S|θ) =
n
X
i=1
log P(xi|θ) = n
 θT ¯ϕ −Φ(θ) + C

,

A.6. Bayesian Inference and Approximations
233
where ¯ϕ = n−1 P
i ϕ(xi) and C is some constant. Note that l(θ) depends on
S only via the empirical statistic ¯ϕ, hence the name “suﬃcient statistic” for ϕ.
The unique maximum point of l(θ) is given by the moment parameter µ = ¯ϕ:
the maximum likelihood estimate. It is important to note that µ = ¯ϕ is nothing
but the m-projection (moment matching; see Section A.4.2) of the empirical
distribution ˆP = n−1 P
i δ
 i onto the family {P(x|θ)}. But what if the family is
not an exponential one, for example because P(x|θ) =
R
P(x, h|θ) dh for some
nuisance variables h?
This is an instance of the general problem of maximising a function f(θ) =
log
R
exp(g(h; θ)) dh. In general, this is a hard task and we may have to content
ourselves with ﬁnding a local maximum. From (A.11) we know that for ﬁxed θ,
f is convex as a function of g(·; θ), so we can ﬁnd a global lower bound linear in
g which is tight at θ:
f(θ′) ≥EQ [g(h; θ′)] + H[Q],
(A.19)
where Q(h) ∝exp(g(h; θ)), so that (g(·; θ), Q) is a Legendre pair. We can now
maximise the right hand side w.r.t. θ′. Since for θ′ = θ both sides are equal, the
maximiser will improve on f(θ) if ∇
  f(θ) ̸= 0. Furthermore, the gradients of
f(θ) and the lower bound are identical at θ:
∇
  f(θ) = EQ [∇
  g(h; θ)] .
(A.20)
Thus, we are guaranteed convergence to a local maximum of f. The whole pro-
cedure relies on EQ [g(h; θ′)] being simple to optimise, while
R
exp(g(h; θ)) dh is
not. We refer to this method as lower bound maximisation. A very important
special case is as follows. Let g(h; θ) be the log of an exponential family distri-
bution over h and additional observed variables x (whose values are plugged in)
and θ the natural parameters. The gradient can be computed straightforwardly
as ∇
  f(θ) = EQ[ϕ(h, x)] −µ, where Q(h) = Q(h|x) is the corresponding
conditional distribution (from a diﬀerent exponential family). The lower bound
is maximised by µ′ = EQ[ϕ(h, x)] which has the same form as the maximum
likelihood estimate except that the latent h are marginalised over Q.
The expectation maximisation (EM) algorithm for maximum likelihood esti-
mation in the presence of nuisance variables [48] is a special case of this algorithm:

234
Appendix A. General Appendix
h = (h1, . . . , hn) are nuisance variables, g(h; θ) = P
i gi(hi; θ) is the complete
data log likelihood which decomposes. Note that the posterior Q(h) is a prod-
uct distribution w.r.t. h1, . . . , hn whose factors are often simple to compute. For
many models, the complete data likelihood is actually in an exponential family,
so the maximisation of the lower bound simply corresponds to an m-projection
(Deﬁnition A.8) in the very same way as for ML estimation without nuisance
variables (the distribution to be projected is simply the product of the empirical
distribution of the observed variables and Q(h)). Historically, the computation
of Q(h) is called E-step, and the lower bound maximisation is called M-step.
Another important interpretation of the EM algorithm comes from informa-
tion theory. Suppose that the complete data likelihood comes from an exponential
family and we observe an instance ¯S of the sample variable S. Then, the M-step
corresponds to an m-projection, and if we deﬁne the manifold E of distributions
over (S, h) via the constraint P(S) = δ ¯S(S), P ∈E, then it is easy to see that
the E-step corresponds to an e-projection (Deﬁnition A.8) onto E (the projection
results in δ ¯S(S)Q(h)). EM is a special case of an alternating minimisation proce-
dure, a class introduced by Csisz´ar and Tusnady [43] to show the common roots
of a number of algorithms in statistics and information theory. They showed
that for some distance d(P, Q), convex in both P and Q, and closed convex sets
P, Q, there is a unique global minimum minP ∈P,Q∈Q d(P, Q) and a corresponding
minimum point (P ∗, Q∗) is found by the alternating minimisation procedure. So
what’s wrong with EM, which can get trapped into local minima? The problem
is that the exponential families {P(S, h|θ)} commonly used with EM are not
convex as a set of distributions. Although the m-projections in the M-steps are
always well-deﬁned, local minima do exist and are often a real problem. There
are simple cases in which EM is used with convex model families, such as the
Blahut-Arimoto algorithm10.
There are cases where even the lower bound in (A.19) cannot be computed and
optimised feasibly. In this case, we are of course free to choose a diﬀerent lower
bound given by a suboptimal distribution Q. The optimal Q globally maximises
10Which computes the maximum likelihood mixing proportions in a mixture model with ﬁxed
component distributions, see [34], Sect. 13.8.

A.6. Bayesian Inference and Approximations
235
the lower bound at θ′ = θ, so our search for Q should do the same thing, but
restricted to a family for which the bound can actually be evaluated feasibly.
This can be seen as variational generalisation of EM (see Section A.6.3), as was
pointed out by Hinton and Neal [75]. Note that this generalised algorithm is
not guaranteed to converge to a local minimum of f. It oﬀers a general way of
doing marginal likelihood maximisation (see Section A.6.2). There are numerous
other generalisations of EM, such as deterministic E-step annealing [153], M-step
annealing [167], sequential EM, etc.
A more direct approach to maximising f(θ) is to use gradient-based opti-
misation. As shown in (A.20), the gradient can be computed at least as easily
as maximising the lower bound.
Furthermore, gradient-based optimisers such
as Quasi-Newton, conjugate gradients or restricted memory Quasi-Newton (e.g.,
[55]) can exhibit much faster convergence than lower bound maximisation tech-
niques (which typically has ﬁrst-order convergence). Convergence properties of
EM have been studied and compared to those of gradient-based optimisation in
[213, 149]. The consensus seems that fairly close to a minimum point, modern
gradient-based methods clearly exhibit much faster convergence to high accuracy
in θ. On the other hand, EM often attain a point of reasonably low f(θ) sig-
niﬁcantly faster than gradient-based optimisers, due to its ability to make large
steps in θ. Another advantage of EM is that it can often be implemented very
easily, although packages for gradient-based optimisation are publicly available.
A simple hybrid employed in this thesis uses a modern gradient-based method,
but keeps Q ﬁxed during line searches along descent directions. If changes in Q
can lead to discontinuous behaviour in the lower bound, this variant promises
higher stability.
While EM has its important place amongst modern optimisation techniques if
the M-steps can be done eﬃciently, other lower bound maximisation algorithms
are clearly inferior to gradient-based optimisation in many areas of application.
An example is the generalised iterative scaling algorithm [45] applied to log-linear
models for sequence tagging.11 Here, a sequence of lower bounds is applied to
11Iterative scaling is useful for maximum likelihood ﬁtting of undirected graphical models if
clique sizes are not very small. However, the improved variant of [190] should be used in which

236
Appendix A. General Appendix
achieve a decoupling w.r.t. model parameters. While this is quoted as advantage
by several authors who have used GIS, it is actually a very questionable thing
to do in our opinion. In the optimisation literature, decoupling techniques are
avoided because they lead to a phenomenon called zig-zagging: given a function
which correlates its variables signiﬁcantly, the optimisation of this function by
in turn updating one variable while keeping all others ﬁxed leads to very slow
progress in total along a characteristic zig-zag trajectory.
A.7
Large Deviation Inequalities
In this section, we describe Chernoﬀ’s bounding technique of deriving exponential
inequalities from the basic Markov inequality. We follow the exposition in [114].
Theorem A.1 (Markov Inequality) Let X be an almost surely non-negative
random variable with ﬁnite mean. Then, for every ε > 0,
Pr {X ≥ε} ≤E[X]
ε
.
For the proof, assume w.l.o.g. that ε = 1 and note that Pr{X ≥1} =
E[I{X≥1}] ≤E[X]. Chebishev’s inequality for a variable Y with zero mean and
ﬁnite variance,
Pr {|Y | ≥ε} ≤Var[Y ]
ε2
follows from Markov’s by setting X = Y 2.
However, in order to bound probabilities of events which are exponentially
unlikely, we require inequalities which are exponential in nature.
A standard
technique to obtain tighter large deviation results is to apply Markov’s inequality
to the exponentiated large deviation. The technique is often accredited to Chernoﬀ
[28], but has actually been used by Cram´er before [35]:
Pr {X ≥x} = Pr

eβX ≥eβx	
≤e−βxE

eβX
,
x ≥E[X].
The same inequality holds for Pr{X ≤x}, x ≤E[X]. In the remainder of this
section, we will focus on the upper tail.
parameter updates and inference are interleaved.

A.7. Large Deviation Inequalities
237
Theorem A.2 (ChernoﬀInequality) Let X be a random variable with ﬁnite
mean. Then, for every x ≥E[X],
Pr {X ≥x} ≤e−S(X,x),
S(X, x) = sup
β
xβ −log E

eβX
.
Let K(β) = log E[eβX] be the cumulant generating function of X. By deﬁni-
tion, the cumulants of X (if they exist) can be obtained as derivatives of K(β),
evaluated at β = 0. Note that this implies that K(β) is convex w.r.t. β, and
therefore its convex dual S(X, x) is convex w.r.t. x (see Section A.3). If P is the
distribution of X, we can deﬁne the tilted distribution Pβ by dPβ(X) = exp(βX−
K(β)) dP(X) whenever K(β) < ∞. It is easy to see that S(X, x) = D[Pβ(x) ∥P],
where (x, β(x)) is a Legendre pair (see Section A.3). Since the cumulant gener-
ating function of Pβ is Kβ(γ) = K(γ + β) −K(β), the cumulants of Pβ are given
by the derivatives of K(β) w.r.t. β, evaluated at β.
If K(β) is known, then S(X, x) can be computed by noting that the supremum
is attained at β = β(x) such that K′(β) = Eβ[X] = x. If X = Pn
i=1 Xi, with the
Xi being distributed i.i.d., we have that K(β) = n K1(β) = n log E[eβX1]. In this
case, we look for β such that K′
1(β) = x/n, then S(X, x) = n(β(x/n) −K1(β)).
Cram´er has shown that the bound of Theorem A.2 is asymptotically tight in this
case, i.e. for every ﬁxed q ∈[0, 1]:
lim
n→∞
1
n (log Pr {X ≥n q} + S(X, n q)) = 0.
For example, if the Xi are i.i.d. Bernoulli(p) variables and x ∈[0, n], then K1(β) =
log(1 −p + p eβ) and S(X, x) = n DBer[x/n ∥p] (see Equation A.7).
Theorem A.3 (ChernoﬀInequality, Binomial Variables) Let X1, . . . , Xn be
i.i.d. Bernoulli(p) variables and X = Pn
i=1 Xi. Then, for every q ∈[p, 1],
Pr {X ≥n q} ≤e−n DBer[q ∥p].
The same bound holds for {X ≤n q}, q ≤p. Furthermore, for ε > 0,
Pr {DBer[X/n ∥p] ≥ε, X/n < p} ≤e−n ε,

238
Appendix A. General Appendix
and the same holds for {DBer[X/n ∥p] ≥ε, X/n > p}. These bounds hold also
if X1, . . . , Xn are i.i.d. bounded variables, Xi ∈[0, 1], E[Xi] = p. However, they
are tightest for sums of Bernoulli variables.
Namely, let Xi ∈[0, 1] and E[Xi] = p. Since eβX1 is convex, we have eβX1 ≤
1 −X1 + X1 eβ for X1 ∈[0, 1], thus K1(β) ≤˜K1(β), where ˜K1 is the cumulant
generating function for a Bernoulli(p) variable. Therefore, S(X, x) ≥S( ˜X, x),
where ˜X is the sum of n i.i.d. Bernoulli(p) variables. If the appearance of DBer
is inconvenient, we can use the quadratic lower bound DBer[q ∥p] ≥2(q −p)2 in
order to derive Hoeﬀding’s Inequality:
Pr {X/n ≥p + ε} ≤e−2n ε2.
However, the quadratic bound is fairly poor if X/n is far from 1/2 (which is
usually what we are interested in), and the use of Hoeﬀding’s inequality “just for
convenience” is not recommended.

Appendix B
Appendix for Chapter 3
In this chapter, we provide derivations and additional details in order to complete
the presentation in Chapter 4.
B.1
Extended PAC-Bayesian Theorem: an Example
Theorem 3.2 provides a powerful extension of the PAC-Bayesian Theorem 3.1 to
multiple classes and confusion distribution. In this section, we work through an
example in order to see how it can be used in practice. In the binary classiﬁcation
case, we might be interested in upper bounding the probability that the Gibbs
rule outputs ˜y = +1, given that the true class is y∗= −1 (i.e. the probability of a
false positive). This probability can be written as F(y∗, ˜y)/P(y∗), where F(y∗, ˜y)
is the joint confusion distribution of (3.4).
We ﬁrst use Chernoﬀ’s inequality (Theorem A.2) to obtain a lower bound on
p∗= P(y∗). For ˆp∗= n−1 P
i I{yi=y∗},
PrS {DBer[ˆp∗∥p∗] > ε, ˆp∗> p∗} ≤e−n ε.
Thus, if ν is chosen such that DBer[ˆp∗∥ˆp∗−ν] = ε = (1/n) log δ−1
1 , then we have
p∗≥ˆp∗−ν with probability at least 1 −δ1. Note that ˆp∗−ν is the left limit of
the interval D−1
Ber(ˆp∗, (1/n) log δ−1
1 ), as deﬁned in (3.1). We will show in a moment
how to use Theorem 3.2 in order to obtain an upper bound on F(y∗, ˜y) which
holds with conﬁdence 1 −δ2. Applying the union bound, we conclude that both
239

240
Appendix B. Appendix for Chapter 3
bounds hold at the same time with conﬁdence 1 −δ1 −δ2, resulting in an upper
bound on the ratio F(y∗, ˜y)/P(y∗). The conﬁdence parameters must be chosen
a priori. Since the bound on P(y∗) is much tighter, you may want to choose
δ1 < δ2.
We now apply Theorem 3.2 using the setup for L and l introduced after
(3.4). Note that L = 4 and p = p(Q) = (p1 . . . p4). W.l.o.g. let p1 be the entry
representing F(y∗, ˜y) for the values y∗= +1, ˜y = −1.
Writing ˆp = ˆp(S, Q)
and ε = ε(δ, n, P, Q), we can upper bound F(y∗, ˜y) by the maximum value of
p1, given that p ∈P(ˆp, ε), i.e. lies in the convex bounded feasible set deﬁned in
(3.6). This is an optimisation problem with linear criterion and convex constraints
which can be solved easily, at least under the assumption that all components of
ˆp are positive. It is shown below in this subsection that the solution has the form
p1 =
r
λˆp1 + 1
4(λ −1)2 −1
2(λ −1),
pl =
λ
λ + p1
ˆpl, l = 2, 3, 4.
(B.1)
Here, λ > 0 is a Lagrange multiplier. If we denote the solution for a ﬁxed multi-
plier by pλ, then the function λ 7→D[ˆp ∥pλ] is strictly monotonically decreasing,
thus a simple one-dimensional search will ﬁnd the unique multiplier value λ for
which D[ˆp ∥pλ] = ε. The solution is p = pλ for this value of λ, and its component
p1 is the desired upper bound for F(y∗, ˜y).
It remains to show that (B.1) is the solution for the convex problem stated
above.
This is a straightforward exercise in using Lagrange multipliers.
The
primal problem is to minimise −p1 = −δT
1 p, subject to p ∈P(ˆp, ε), where the
feasible set P(ˆp, ε) is given by (3.6). We assume that all components of ˆp are
positive. Let us introduce Lagrange multipliers λ ≥0 and ρ for the constraints
D[ˆp ∥p] −ε ≤0 and 1Tp −1 = 0. The Lagrangian is
Λ = −δT
1 p + λ(D[ˆp ∥p] −ε) + ρ(1Tp −1).
Setting the gradient of Λ w.r.t. p equal to zero, we obtain
−δl,1 −λ ˆpl
pl
+ ρ = 0,
l = 1, . . . , L.
Since all ˆpl > 0, all pl must be positive as well, otherwise D[ˆp ∥p] = ∞would

B.2. Details of Proof of Theorem 3.2
241
violate the constraint. Thus,
pl(ρ −δl,1) = λˆpl,
l = 1, . . . , L.
Summing over l and using the constraints, we see that ρ = λ + p1 and
pl =
λ
λ + p1 −δl,1
ˆpl.
For l = 1, this is a quadratic in p1 with the unique nonnegative solution
p1 =
r
λˆp1 + 1
4(λ −1)2 −1
2(λ −1).
It is easy to see that dp1/dλ < 0 for all λ > 0, with p1 = 1 for λ = 0 and p1 →ˆp1
as λ →∞. If pλ = (pλ,l)l,
pλ,1 =
r
λˆp1 + 1
4(λ −1)2 −1
2(λ −1),
pλ,l =
λ
λ + pλ,1
ˆpl, l > 1,
then λ 7→D[ˆp ∥pλ] is strictly decreasing, and pλ →δ1 (λ →0), pλ →ˆp (λ →
∞). Thus, a simple one-dimensional search reveals the unique λ > 0 such that
D[ˆp ∥pλ] = ε, and p = pλ is the solution we are looking for.
B.2
Details of Proof of Theorem 3.2
Here, we give the proof of Inequality (3.7) (see [34], Sect. 12.1).
Recall the
notation deﬁned in the proof of Theorem 3.2 (Section 3.2.2). First, the probability
of a sequence (l1, . . . , ln) depends on its type only (i.e. the type is a suﬃcient
statistic for the sequence):
n
Y
i=1
pli = en(−D[ˆ
 ∥
  ]−H[ˆ
  ])
(B.2)
(recall the entropy H[ˆp] from Equation A.9). Second, the number of sequences
with type ˆp can be upper bounded by enH[ˆ
  ]. To see this, consider sequences
being sampled i.i.d. from ˆp (not from p). Under this distribution, the probability
of an arbitrary sequence of type ˆp is e−nH[ˆ
  ] (using (B.2)). Since the sum of these
probabilities must be ≤1, we obtain the bound on the number of sequences. Note

242
Appendix B. Appendix for Chapter 3
that this bound is very tight: a lower bound on this number is enH[ˆ
  ] (n + 1)1−L
(see [34], Theorem 12.1.3). Third, the number of diﬀerent types is upper bounded
by (n + 1)L−1. Thus, we have
ES

enD[ˆ
 ∥
  ]
≤
X
ˆ
 enD[ˆ
 ∥
  ]en(−D[ˆ
 ∥
  ]−H[ˆ
  ])enH[ˆ
  ] =
X
ˆ
 1 ≤(n + 1)L−1.
By using the lower bound on the number of sequences of given type, it is easy to
show that ES[exp(n λ D[ˆp ∥p])] grows exponentially in n for every λ > 1, so that
in this sense the exponent we are using is optimal.
B.3
Proof of Theorem 3.3
Here, we give a proof of the PAC-Bayesian theorem for arbitrary bounded loss
functions, Theorem 3.3. This is done by starting from an inequality bounding the
probability of large deviations between expected and empirical loss for ﬁxed rules
in order to obtain a “exponential” statement like (3.8), which can then be plugged
into the generic part of the proof as given in Section 3.2.2. This implication is
proved using a beautiful “water-ﬁlling” argument due to [117].
Recall the notation deﬁned in Sections 3.2.4 and 3.2.2. Given a loss function
l( ˜w, (x∗, y∗)) ∈[0, 1], the risk is deﬁned as l( ˜w) = E[l( ˜w, (x∗, y∗))] and the
empirical risk as ˆl( ˜w) = n−1 P
i l( ˜w, (xi, yi)). For some nonnegative function φ
on [0, 1]2, we assume that the inequalities (3.17) hold for every ˜w, moreover that
φ(q, l) is nondecreasing in |q −l|, convex in (q, l), φ(l, l) = 0 for all l ∈[0, 1],
ﬁnally that φ(·, l) is diﬀerentiable on (0, l) and (l, 1), for all l ∈(0, 1). Now ﬁx ˜w
and let l = l( ˜w), ˆl = ˆl( ˜w). If we can show that
ES
h
e(n−1)φ(ˆl,l)i
≤2n + 1,
(B.3)
we can take the expectation over ˜w ∼P, exchange the order of the two ex-
pectations and apply Markov’s inequality to end up with the statement we are
after:
PrS

E ˜
 ∼P
h
e(n−1)φ(ˆl( ˜
 ),l( ˜
 ))i
> 2n + 1
δ

≤δ.

B.3. Proof of Theorem 3.3
243
Note that if l ∈{0, 1}, then ˆl = l with probability 1, thus (B.3) is trivially
true. In order to prove (B.3) for l ∈(0, 1), we look for the distribution over ˆl
which maximises the left-hand side of (B.3), subject to the constraints imposed
by (3.17) for every q. Since φ(q, l) is nondecreasing in |q −l|, it is obvious that
a maximiser should fulﬁl these constraints with equality for all1 q. A measure
over ˆl with density f(ˆl) fulﬁlling all constraints with equality can be obtained by
diﬀerentiating the constraints w.r.t. q. We obtain
f(ˆl) = sgn(l −ˆl) d
dˆl
e−nφ(ˆl,l).
It is straightforward to check that f(ˆl) is nonnegative and integrates to 2 −
e−nφ(0,l) −e−nφ(1,l) ≤2. Since this may be a value < 1, we consider the measure
over ˆl which is the sum of the measure with density f(ˆl) and a point measure of
mass 1 at l. The expectation of e(n−1)φ(ˆl,l) over this measure is an upper bound on
this expectation over any probability distribution fulﬁlling the constraints (3.17),
thus:
E
h
e(n−1)φ(ˆl,l)i
≤1 +
Z 1
0
e(n−1)φ(ˆl,l) sgn(l −ˆl) d
dˆl
e−nφ(ˆl,l) dˆl
= 1 + n
Z 1
0
sgn(ˆl −l)dφ
dˆl
(ˆl, l)e−φ(ˆl,l) dˆl
= 1 + n
Z 1
0
sgn(l −ˆl) d
dˆl
e−φ(ˆl,l) dˆl
= 1 + n
 2 −e−φ(0,l) −e−φ(1,l)
≤2n + 1,
proving the statement (B.3).
Applying the generic part of the proof in Sec-
tion 3.2.2 and using the convexity of φ(q, l) completes the proof of Theorem 3.3.
1This is a water-ﬁlling argument. The density function f(ˆl) fulﬁlling all constraints with
equality may integrate to more than 1. In this case, the maximising probability density is
obtained by shrinking the initial part of f(ˆl) to zero. Since this tightens the inequality only
insigniﬁcantly, we however stick with f(ˆl) here.

244
Appendix B. Appendix for Chapter 3
B.4
Eﬃcient Evaluation of the Laplace GP Gibbs
Classiﬁer
Recall from Section 2.1.3 that in order to evaluate the Laplace GP Gibbs clas-
siﬁer on a test point x∗, we require the predictive variance σ2(x∗) which can
be computed in O(n2) using a back-substitution. The same is true for all other
non-sparse members of the family of approximation methods mentioned in Sec-
tion 2.1.3 (i.e., d = n). Note that σ2(x∗) is required in order to compute an
uncertainty estimate for both Bayes and Gibbs classiﬁer, but if this is not re-
quired, the rejection sampling technique developed here can be used to end up
with more eﬃcient predictions. We describe this technique for the case of Laplace
GPC, but the development for any other non-sparse method should be clear using
the notation introduced in Sections 3.3.1 and 2.1.3.
Fix x∗and let b = D1/2k(x∗). Then,
σ2
∗= σ2(x∗) = K(x∗, x∗) −bTB−1b.
It is possible to obtain upper and lower bounds on the critical term bTB−1b
without having to compute a factorisation of B, by using Lanczos methods (e.g.,
[157]). The bounds are due to Skilling [178] and have been used in the context
of GP methods by [63]. The idea is to maximise the quadratic function
q(u) = bTu −1
2uTBu
approximately. If we do a sparse greedy maximisation of q, allowing only a con-
trolled number d of components of u to become nonzero, we can compute upper
and lower bounds on σ2
∗in O(n d2) (see [183]), without having to know more than
d rows of B. Alternatively, faster incomplete Cholesky techniques can be used
[54]. Now suppose we are given σ2
L ≤σ2
∗≤σ2
U. Then, we can create envelope
functions L(u∗) and U(u∗) on the p.d.f. f(u∗) = N(u∗|0, σ2
∗), simply by setting
L(u∗) = (2πσ2
∗)−1/2 exp(−u2
∗/(2σ2
L)) and U(u∗) = (2πσ2
∗)−1/2 exp(−u2
∗/(2σ2
U)).
Note that U(u∗) ∝g(u∗) = N(u∗|0, σ2
U), and that we can compute the ratio
r(u∗) = L(u∗)
U(u∗) = exp

−u2
∗
2
 1
σ2
L
−1
σ2
U

(B.4)

B.5. Proof of Theorem 3.4
245
without knowledge of σ2
∗. We can use these facts to sample u∗∼N(0, σ2
∗), using
a sandwiched rejection method (e.g., [150]), as follows:
• Independently sample u∗∼N(0, σ2
U) and u uniformly from [0, 1].
• If u ≤r(u∗), return u∗. The ratio r(u∗) is given by (B.4).
• Otherwise, compute the variance σ2
∗exactly.
Now, if u ≤f(u∗)/U(u∗),
return u∗. Otherwise, sample u∗∼N(0, σ2
∗) and return it.
Note that the method produces a sample of f(u∗) without having to compute the
variance σ2
∗, whenever the algorithm returns in the second step. Let E denote
this event, and denote V (U) =
R
U(u∗) du∗, V (L) =
R
L(u∗) du∗. Now,
Pr{E} = Pr

u ≤L(u∗)
U(u∗)

= E
 L(u∗)
U(u∗)

=
Z
L(u∗)
V (U)g(u∗)g(u∗) du∗
= V (L)
V (U) = σL
σU
,
where we have used that U(u∗) = V (U)g(u∗).
If the bounds are tight, this
probability is close to 1. For example, if we have σ2
L ≥(1 −ε)σ2
∗, σ2
U ≤(1 + ε)σ2
∗,
then
V (L)/V (U) ≥
p
(1 −ε)/(1 + ε) = 1 −ε/(1 + ε) + O(ε2).
B.5
Proof of Theorem 3.4
Theorem 3.4 is a PAC-Bayesian theorem for Bayes voting classiﬁers.
In this
section, we replicate the proof given in [121] in our notation. Section 2.2 contains
an introduction to some of the concepts used here.
The proof uses a recent bound given in [91]. We need some machinery from
empirical process theory. Let F be a set of classiﬁers2 f : X →{−1, +1}. Let
S = {(xi, yi)} be an n-sample as above, and deﬁne the empirical Rademacher
complexity of F as
ˆRn(F) = E
 "
sup
f∈F

1
n
n
X
i=1
εif(xi)

#
,
(B.5)
2Of course, there are some measurability issues here, we refer to the original work.

246
Appendix B. Appendix for Chapter 3
where ε is a vector of i.i.d. Rademacher variables (unbiased coins with sides
+1, −1), independent of S.
Note that ˆRn(F) is a random variable, but in-
dependent of the targets yi.
It can be shown that it is concentrated around
Rn(F) = ES[ ˆRn(F)], the Rademacher complexity of F, if S is drawn i.i.d. as
usual (in fact, ˆRn(F) has bounded diﬀerences of size ≤1/n, so Hoeﬀding’s in-
equality applies, see Section 2.2.2). Let φ be the margin loss function deﬁned in
Section 3.4.1, and choose δ ∈(0, 1), c > 1.
Theorem B.1 ([91], Theorem 2) For any data distribution over X ×{−1, +1},
we have that
PrS
(
Pr(
 ∗,y∗) {y∗f(x∗) ≤0} > inf
κ∈(0,1] B(κ, S, Q)
+
r
log(2/δ)
2n
for some f ∈F
)
≤δ.
Here,
B(κ, S, Q) = 1
n
n
X
i=1
φ (yif(xi)/κ) + 4 c
κ Rn(F) + log log(2/κ) −log log c
n
.
The proof of this theorem is not very hard. First, the bounded loss allows us
to apply Hoeﬀding’s inequality based on bounded Martingale diﬀerence sequences
(see Section 2.2.2). We then use Ledoux and Talagrands contraction principle to
get rid of the margin loss function, exploiting the fact that (φ(·/κ) −1)/κ is a
contraction.3 The details are in [91]. Recall the deﬁnitions of f(x|Q), QA and FA
from Section 3.4.1. We can use convex duality for D[Q ∥P] in order to bound the
empirical Rademacher complexity ˆRn(FA), getting rid of the supremum over FA.
First note that since every class FA is closed under multiplication with −1, we
can remove the absolute value operator in (B.5). Recall that for any (measurable)
z(w),
E
 ∼Q[z(w)] ≤D[Q ∥P] + log E
 ∼P

ez(
 )
3f is a contraction if |f(x) −f(y)| ≤|x −y| for all x, y.

B.5. Proof of Theorem 3.4
247
(see Equation A.8). Write short y(i) = y(xi|w). Choosing z(w) = (λ/n) P
i εiy(i),
we have
ˆRn(FA) ≤1
λ
 
A + E
 "
log E
 ∼P
"
exp
 
λ
n
X
i
εiy(i)
!##!
.
(B.6)
Now use Jensen’s inequality to pull the expectation over ε inside the log and
evaluate the moment generating function of the εi:
E
 "
log E
 ∼P
"
exp
 
λ
n
X
i
εiy(i)
!##
≤log E
 ∼P
" n
Y
i=1
cosh
λ y(i)
n
#
.
Note that log cosh x is concave as a function of x2 (e.g., [80], Sect. 3.B), thus can
be upper bounded by quadratics. We use log cosh x ≤(1/2)x2:
log E
 ∼P
" n
Y
i=1
cosh(λ y(i)/n)
#
≤log E
 ∼P
"
exp
 
λ2
2 n2
X
i
(y(i))2
!#
= λ2
2 n,
and minimisation w.r.t. λ leads to
ˆRn(FA) ≤
r
2 A
n ,
the same bound holds for Rn(FA). For ﬁxed A, Theorem B.1 holds with F = FA
and Rn(F) replaced by
p
2 A/n. Now, replace A and δ by Aj and δj = pjδ.
Since P
j δj = δ, we can conclude the proof of Theorem 3.4 by applying the
union bound.
B.5.1
The Case of Regression
In Section 3.4.1.2, we mentioned a possible route towards a PAC-Bayesian the-
orem for GP regression. Here, we give the details for this argumentation. Our
starting point is Theorem 1 in [91], where in the second part they prove a bound
on E∥Pn −P∥Gφ in terms of the Rademacher complexity Rn(FA). In the case we
are interested in, Gφ consists of the functions (x, y) 7→φ(y −f(x)), f ∈FA. Re-
call that f(x) = EQ[u(x|w)], Q ∈QA, and that φ is a Lipschitz-κ loss function
with φ(0) = 0. Obviously, this part of the proof remains valid, and we have
E∥Pn −P∥Gφ ≤4
κRn(FA),

248
Appendix B. Appendix for Chapter 3
where the Rademacher complexity is now given by Rn(FA) = ES[ ˆRn(FA)],
ˆRn(FA) = E
 "
sup
f∈FA,s∈{−1,+1}
s
n
n
X
i=1
εi(yi −f(xi))
#
.
Denote ui = u(xi|w), so that f(xi) = EQ[ui]. We can use convex duality as
above, with z(w) = (λ/n) P
i εi(yi −ui), resulting in
ˆRn(FA) ≤1
λ

A + E
 
max
s∈{−1,+1} log EP

exp
λ s
n εT(y −u)

.
Note that u ∼N(0, K). Setting r = −s εTu ∼N(0, εTKε), we have
log EP

exp
λ s
n εT(y −u)

= λ s
n εTy + log EP

eλ r/n
= λ s
n εTy + λ2
2 n2 εTKε.
All in all, if
Ψ(y) = E
 
max
s∈{−1,+1} s εTy

= E
 εTy

,
we have
E
 
max
s∈{−1,+1} log EP

exp
λ s
n εT(y −u)

= λ2
2 n2 tr K + λ
nΨ(y),
where we used E[εεT] = I. A minimisation w.r.t. λ results in
ˆRn(FA) ≤n−1/2p
2 A (tr K/n) + n−1Ψ(y).
Finally, making use of Jensen’s inequality on the concave square root function,
we have
ES[Ψ(y)] = E
  ,S
q
(εTy)2

≤
q
E

(εTy)2
=
p
E [yTy] =
p
n E [y2],
and (3.24) follows easily.
B.6
Proof of a PAC Compression Bound
In Section 3.5.4, we empirically compare the PAC-Bayesian Theorem 3.1 for bi-
nary GP classiﬁcation with a standard PAC compression bound. In this section,

B.6. Proof of a PAC Compression Bound
249
we state this bound (Theorem B.2) and give a proof. The theorem here is a
slight reﬁnement of Theorem 5.18 in [72] which in turn is based on results in
[103]. It is interesting to note that in spite of the extremely simple derivation
of the compression bound, it seems to be among the tightest known bounds for
the popular support vector machine in practically relevant regimes, way ahead of
many results employing much deeper mathematics.
Let us deﬁne what we mean by a compression scheme. Suppose we are given a
hypothesis space H of binary classiﬁers and a learning algorithm A mapping i.i.d.
training samples S from an unknown data distribution to hypotheses A(S) ∈H.
As usual, we deﬁne generalisation and empirical error of a hypothesis h as
gen(h) = Pr(
 ∗,y∗){h(x∗) ̸= y∗},
emp(h, S) = 1
n
n
X
i=1
I{h(
 i)̸=yi},
where S = {(xi, yi)} and (x∗, y∗) is drawn from the data distribution, inde-
pendently of S. A compression scheme of size d < n is a learning algorithm
A for which we can ﬁnd a mapping I from samples S to ordered d-subsets of
{1, . . . , n} and another algorithm R mapping samples ˜S of size d to hypotheses
R( ˜S) ∈H, such that for every n-sample S we have that A(S) = R(SI(S)), where
SI(S) = {(xi, yi) | i ∈I(S)}. This means that we can extract from each sample S
a subsample SI(S) such that the algorithm essentially has to be trained on SI(S)
only in order to produce the same result as on the full sample S. Once I(S) is
chosen, S\I(S) = S \ SI(S) is ignored. Thus, the data can be compressed before
presenting it to the learner, and it seems intuitive that this characteristic can
be exploited to prove generalisation error bounds for such schemes. A further
characteristic that can be somewhat exploited is permutation-invariance of the
scheme w.r.t. the data sample. If A is a d-compression scheme (with associated
mappings I and R) and 0 ≤l ≤d, we call A l-permutation-invariant if for every
sample S, feeding R with a sample obtained from SI(S) by permuting the last l
points leads to the same result, independent of the permutation. Special cases
are l = 0 (we are not allowed to permute any points) and l = d (we are allowed
to permute all points). Herbrich [72], Sect. 5.2.1 provides further motivation and
gives examples of compression schemes.

250
Appendix B. Appendix for Chapter 3
Fix δ ∈(0, 1), and choose d ∈{1, . . . , n}, l ∈{0, . . . , d} a priori (this can be
relaxed, as is shown below).
Theorem B.2 (PAC compression bound) Suppose the algorithm A is, for
samples S of size n, a l-permutation invariant d-compression scheme (with asso-
ciated mappings I and R). Then, for any data distribution we have that the fol-
lowing bound holds, where the probability is over i.i.d. samples S = {(xi, yi) | i =
1, . . . , n} of size n drawn from the data distribution:
PrS



gen(A(S))
≥emp
 A(S), S\I(S)

+ε

(n −d) emp(A(S), S\I(S)), d, ˜δ



≤δ,
(B.7)
where ˜δ = δ/(n −d + 1) and ε(q, d, ˜δ) = max D−1
Ber(q/(n −d), ρ) with
ρ =
1
n −d

log
n
d

(d −l)!

+ log ˜δ−1

(recall the deﬁnition of D−1
Ber from Equation 3.1).
Note that the error on the remaining patterns S\I(S), namely emp(A(S), S\I(S))
is denoted by emp\d(S) in Section 3.5.4 and also below in this section.
As for the proof, ﬁx q ∈{0, . . . , n −d}. Our ﬁrst goal is to upper bound
PrS

emp(A(S), S\I(S)) =
q
n −d, gen(A(S)) ≥
q
n −d + ε

,
(B.8)
where ε > 0.
Recall that A(S) = R(SI(S)).
We ﬁrst apply a simple union
bound argument, summing over all possible values of I(S), in order to obtain the
following upper bound on (B.8):
X
I
PrS

emp(R(SI), S\I) =
q
n −d, gen(R(SI)) ≥
q
n −d + ε

.
(B.9)
We will determine the range and number of admissible I later. For now, ﬁx I.
We can upper bound the addend in (B.9) corresponding to I by
ESI

PrS\I

ˆp ≤
q
n −d, p ≥
q
n −d + ε
 SI

,
(B.10)

B.6. Proof of a PAC Compression Bound
251
where we denote p = gen(R(SI)) and ˆp = emp(R(SI), S\I). Since R(SI) depends
just on SI, but not on S\I, we have that, conditioned on SI, (n−d) ˆp is (n−d, p)-
binomial distributed. Therefore, we can use Chernoﬀ’s bound (Theorem A.2) to
obtain
PrS\I

ˆp ≤
q
n −d, p ≥
q
n −d + ε
 SI

≤e−(n−d) DBer[q/(n−d) ∥q/(n−d)+ε]. (B.11)
Since the r.h.s. in (B.11) does not depend on SI, it is also an upper bound of
(B.10). It is also independent of I, thus we can bound (B.9) by simply counting
the number of admissible I, leading to potentially diﬀerent R(SI). Since A is
l-permutation invariant, we can permute the last l points in SI without changing
R(SI). Thus, there are
 n
d

(d −l)! distinguishable values for I. Altogether we
have from (B.9)
PrS

emp(A(S), S\I(S)) =
q
n −d, gen(A(S)) ≥
q
n −d + ε

≤
n
d

(d −l)! e−(n−d) DBer[q/(n−d) ∥q/(n−d)+ε].
(B.12)
Equating the r.h.s. in (B.12) to ˜δ > 0 and solving for ε leads to the (unique)
solution ε = ε(q, k, ˜δ). Therefore, we have for ﬁxed q
PrS

emp(A(S), S\I(S)) =
q
n −d, gen(A(S)) ≥
q
n −d + ε(q, d, ˜δ)

≤˜δ,
where ˜δ = δ/(n −d + 1). Noting that there are n −d + 1 diﬀerent values for q,
we see that
PrS



gen(A(S))
≥emp(A(S), S\I(S))
+ε

(n −d) emp(A(S), S\I(S)), d, ˜δ




≤
n−d
X
q=0
PrS
(
emp(A(S), S\I(S)) =
q
n−d,
gen(A(S)) ≥
q
n−d + ε(q, d, ˜δ)
)
≤δ.
This concludes the proof of Theorem B.2.
Note that we have proved Theorem B.2 under the assumption that d is ﬁxed
a priori. This can be relaxed, allowing for a dependence of d on the sample S,
by using a further union bound argument. We end up with a theorem of exactly

252
Appendix B. Appendix for Chapter 3
the same form of Theorem B.2, however ˜δ has to replaced by ˜δ = 2δ/((n + 1)n).
Here, we assume that l is a function of d, which is reasonable, however can be
relaxed as well if desired.
Note that the main contribution to the gap bound value comes from the
binomial coeﬃcient (for not too small d) and is introduced by the crude union
bound argument (summing over all I). This is necessary since we do not make
any assumptions about the mapping I(S). For a particular algorithm, it may be
possible to come up with a more informed weighting than the uniform one (over
the possible sets I), which might tighten the bound signiﬁcantly. Also, note that
in the special case that we attain emp\d(S) = 0, we can solve for ε analytically,
since DBer[0 ∥ε] = −log(1 −ε) (see Equation A.7), so that for q = 0, we have
ε(0, d, ˜δ) = 1 −
n
d

(d −l)! ˜δ−1
−1/(n−d)
.
We ﬁnally note that in many compression schemes, the index mapping I is partly
randomised, i.e., is a function of the sample S and of random coin tosses. Since
the latter are independent of S, this poses no problem for Theorem B.2, which
holds for all possible outcomes of the coin tosses (although, of course, the bound
value depends on these outcomes).
B.6.1
Examples of compression schemes
Herbrich [72], Sect. 5.2.1 gives a range of examples of compression schemes. It
is shown there that the perceptron learning algorithm of Rosenblatt [154] is a 0-
permutation invariant d-compression scheme, where d is the number of patterns
the algorithm selects for an update of the weight vector. Note that, due to the
perceptron convergence theorem, it is possible to upper bound d in terms of the
margin of the data. Furthermore, the popular support vector machine can be
seen as d-permutation invariant d-compression scheme, where d is the number
of support vectors (this follows from the discussion in Section 2.1.6). Thus, our
application of the compression bound in Section 3.5.4.1 is justiﬁed. Since the
ﬁnal SVM discriminant can make mistakes only on support vectors, we always

B.6. Proof of a PAC Compression Bound
253
have emp\d(S) = 0. Note that d cannot be ﬁxed a priori, thus we have to use
˜δ = 2δ/((n + 1)n) in Theorem B.2.
Furthermore, the IVM (see Section 4.4.1) is a compression scheme as well.
This can be seen by noting that only patterns which are selected for inclusion into
the active set, are ever used to update model parameters. In our experiments (see
Section 3.5.3) we ﬁxed d a priori, therefore Theorem B.2 applies in its original
form if we set l = d −drand. Note that in variants of the scheme d is chosen
depending on the sample S, and in such cases we have to use the modiﬁed ˜δ.

254
Appendix B. Appendix for Chapter 3

Appendix C
Appendix for Chapter 4
In this chapter, we provide derivations and additional details in order to complete
the presentation in Chapter 4.
C.1
Expectation Propagation
The EP algorithm [125] realises a general-purpose framework for approximating
posterior beliefs by exponential family distributions. A generic introduction is
given in Section C.1.1, serving to develop the Gaussian case in Section 4.3. Details
of doing ADF or EP updates in this case are given in Section C.1.2.
C.1.1
Expectation Propagation for Exponential Families
In this section, we introduce the general expectation propagation (EP) algo-
rithm [125], whose specialisation to GP models we require in Chapter 4 (see
Section 4.3). Suppose we are given some statistical model with observables S
and latent variables u, with a prior distribution P(u) from an exponential fam-
ily F. Some properties of exponential families which we require here, are given in
Section A.4.1. The likelihood function P(S|u) often factors in a particular way,
P(S|u) =
n
Y
i=1
ti(u),
255

256
Appendix C. Appendix for Chapter 4
for example in the case of i.i.d. data S or Bayesian networks. We refer to the
ti(u) as sites. If the true posterior
P(u|S) ∝P(u)
n
Y
i=1
ti(u)
(C.1)
is analytically intractable, we may approximate it by a distribution Q(u) from
F:
Q(u) = Q(u|θ) = exp
 θTϕ(u) + h(u) −Φ(θ)

, θ ∈Θ.
An often tractable way for choosing Q is to start from Q(u) = P(u) and in-
corporate the sites ti(u) one after the other, following some sequential ordering.
Namely, in order to incorporate ti(u), ﬁrst compute the true Bayesian update
ˆP(u) = Z−1
i Q(u)ti(u),
Zi = E
  ∼Q[ti(u)],
then m-project it onto F (see Section A.4.2) to obtain the updated belief Qnew:
Qnew(u) = argmin
˜Q∈F
D
h
ˆP(u) ∥˜Q(u)
i
.
Recall that m-projection is equivalent to match moments between ˆP and Qnew.
Namely, if Qnew(u) = Qnew(u|θnew), and η, ηnew are the (dual) moment param-
eters for θ, θnew (see Section A.4.1), then ηnew = E ˆP[ϕ(u)]. We refer to this
process as inclusion of site ti(u) into the belief Q. An inclusion is diﬀerent from
a true Bayesian update, since the full updated belief ˆP is “collapsed” to Qnew, a
member of F, which allows inclusions to be chained. ˆP lives in the tilted exponen-
tial family Fti induced by ti (Deﬁnition A.6), and its moments can be computed
via (A.12) which is often feasible (or amenable to numerical approximations) even
though the moments of the full posterior P(u|S) remain intractable. This simple
idea has been used extensively, for example in the context of Bayesian on-line
learning [138] or switching linear dynamical systems [10, 25, 127], see [125] for
more exhaustive references. It is known as assumed density ﬁltering (ADF). Nev-
ertheless, each site may be included only once, and in the context of dynamical
systems we are restricted to updates in one direction along the backbone chain
(ﬁltering), while bidirectional smoothing would maybe improve the approxima-
tion.

C.1. Expectation Propagation
257
In [125], a new view on ADF is established which allows these shortcomings
to be removed. The process of including ti(u) results in Q(u) being replaced
by Qnew(u), which can also be seen as multiplying Q(u) by the ratio ˜ti(u) ∝
Qnew(u)/Q(u) and renormalising. This operation becomes particularly simple in
the natural parameters: θnew = θ + (θnew −θ). The ratio ˜ti(u) is a member of
the unnormalised exponential family F U associated with F (Deﬁnition A.7): it
has a form very similar to Q and Qnew, but θnew −θ will not in general lie in the
natural parameter space Θ of F.1 This view motivates representing Q as
Q(u) ∝P(u)
n
Y
i=1
˜ti(u),
(C.2)
where the ˜ti(u) = ˜ti(u|θ(i)) ∈F U are referred to as site approximations, and
their natural parameters θ(i) as site parameters. If θ(0) denotes the parameters
of P(u), then
θ = θ(0) +
n
X
i=1
θ(i).
Note that we allow θ(i) = 0, ˜ti(u) ≡1, in fact in the beginning all site approx-
imations are constant, leading to θ = θ(0), i.e. Q(u) = P(u). An ADF update
(inclusion) w.r.t. site ti can now be seen as follows (note that ˜ti ≡1):
Deﬁnition C.1 (ADF Update, Inclusion)
1. Compute moments of ˆP(u) ∝Q(u)ti(u) and pick Qnew(u) ∈F with these
moments.
2. In order to replace Q(u) by Qnew(u), we replace ˜ti(u) ≡1 by ˜tnew
i
(u) ∝
Qnew(u)/Q(u).
From this viewpoint, it becomes clear how ADF can be generalised to a full-
ﬂedged iterative approximation scheme, allowing for multiple iterations over the
sites. An EP update (inclusion-deletion) w.r.t. site ti works as follows:
1For example, if F is the family of Gaussians, then ˜ti may correspond to a “Gaussian with
negative variance”.

258
Appendix C. Appendix for Chapter 4
Deﬁnition C.2 (EP Update, Inclusion-Deletion)
1. Delete the site approximation ˜ti(u) from Q(u) by renormalising Q(u)/˜ti(u),
obtaining
Q\i(u) ∝P(u)
Y
j̸=i
˜tj(u).
In natural parameters: θ\i = θ −θ(i).
2. Let ˆP(u) = Z−1
i ti(u)Q\i(u) and compute
ηnew = E ˆP[ϕ(x)] = ∇
  \i log Zi + η\i,
Zi = E
 \i[ti(u)],
and pick Qnew ∈F with these moments.
3. Replace ˜ti by ˜tnew
i
(u) ∝Qnew(u)/Q\i(u).
In natural parameters: θ(i) = θnew −θ\i.
In line with [139], we will refer to Q\i as cavity distribution.
On networks with discrete nodes or Gaussian Markov random ﬁelds, EP can
be seen as generalisation of loopy belief propagation, allowing for a more ﬂexible
choice of approximating structure and distribution family (see [125] and [203],
Chap. 6). The algorithm does not always converge, but if it does, the ﬁxed point
must be a saddle points of an approximation to the free energy which is a gen-
eralisation of the Bethe free energy [123, 74, 203]. Double-loop concave-convex
algorithms can be applied in order to ensure convergence [74]. Problems of con-
vergence can sometimes be overcome by using “damped” updates: instead of
θ →θnew, we update θ to a convex combination of θ and θnew. Also, updates
which lead to θnew outside of Θ (or very close to its boundary) should be rejected.
In practice, it is important to address the issue of numerical stability: the conver-
sion between natural and moment parameters is typically not a stable operation.2
If possible, an implementation should remain in the moment parameters entirely
and fold conversions with update operations into a single mapping which can then
2For example, for the Gaussian family we have to invert a matrix.

C.1. Expectation Propagation
259
be stabilised. This is of course less generic than the presentation above, and it is
also not clear how to do damped updates (which are convex combinations in the
natural parameters) in this way.
It is important to note that EP is not simply a local approximation of the
sites ti(u) by corresponding ˜ti(u), but a global ﬁt by Q ∈F to the distribution
obtained by replacing ˜ti(u) by ti(u) in the current belief Q(u). In fact, the sites
may not even be continuous functions of u. EP has been applied for approximate
inference in two very diﬀerent regimes: sparsely connected Bayesian or Markov
networks and models with fully connected Gaussian prior P(u). In the former
regime, every single ti(u) depends on a small number of components of u only, e.g.
in the Markov network case on small cliques of the underlying graph. By choosing
a special structure of the approximating distribution Q, based on a tractable sub-
graph of the decomposable extension of the model graph, and requiring that the
prior P(u) follows this structure, one can run EP as a message-passing scheme,
updating the parameters of Q and certain small extensions thereof. This notion
is developed and generalised in [203], Chap. 6 (for discrete variables), and in [123]
(does not address the issue of how to represent Q). In the second regime, which
we are interested here, F is the family of Gaussians, and P(u) is typically densely
connected, while the likelihood factors ti(u) are local again. The special case for
a completely factorised likelihood P(S|u) has been given in [41]. Manipulations
on Gaussians will in general be O(n3), which is the cost for converting between
natural and moment parameters. Nevertheless, a locality property of EP can be
used here as well in order to simplify EP updates and the representation of the
belief Q. Let u = (uT
J , uT
\J)T, and suppose that site ti(u) depends on uJ only,
i.e. ti(u) = ti(uJ). Then, ˆP(u) = ˆP(uJ)Q\i(u\J|uJ) and
D[ ˆP(u) ∥Qnew(u)] = D[ ˆP(uJ) ∥Qnew(uJ)]
+ E
 J∼ˆP

D[Q\i(u\J|uJ) ∥Qnew(u\J|uJ)]

.
In order to minimise this expression, we set Qnew(u\J|uJ) = Q\i(u\J|uJ) and
match moments between the marginals ˆP(uJ) and Qnew(uJ).
It follows that
˜ti(u) = ˜ti(uJ), i.e. the site approximations inherit the locality of the corre-
sponding sites and can be parameterised economically, in the sense that many

260
Appendix C. Appendix for Chapter 4
of the components in θ(i) can be clamped to zero. Furthermore, since Q(u) ∝
Q\i(u)˜ti(uJ), we see that Q(uJ) ∝Q\i(uJ)˜ti(uJ), so that in order to update
the site approximation ˜ti, we only need to access the marginal Q(uJ): although
EP is a global approximation, its updates will be local. Note however that a
change of ˜ti(uJ) in general aﬀects all marginals of Q(u), due to the densely con-
nected prior. We will make use of the locality property below, furthermore of the
following lemma.
Lemma C.1 If Q(u), ˜Q(u) are Gaussians and Q(uJ|u\J) = ˜Q(uJ|u\J), where
u = (uT
J , uT
\J)T, then
H[Q(u)] −H[ ˜Q(u)] = H[Q(uJ)] −H[ ˜Q(uJ)].
and
D[Q(u) ∥˜Q(u)] = D[Q(uJ) ∥˜Q(uJ)].
As for the proof, note that
H[Q(u)] −H[Q(uJ)] = E
 J∼Q

H[Q(u\J|uJ)]

= E
 J∼Q
h
H[ ˜Q(u\J|uJ)]
i
.
The entropy of a Gaussian distribution depends on its covariance matrix only
(Lemma A.13), and the covariance matrix of ˜Q(u\J|uJ) does not depend on uJ,
again a special property of Gaussian distributions (Lemma A.9). Therefore, we
can take the expectation just as well over uJ ∼˜Q, and this concludes the proof
of the ﬁrst part of the Lemma. The second part is straightforward.
C.1.2
ADF Update for some Noise Models
Here, we show how to do ADF updates (Deﬁnition C.1) or EP updates (Def-
inition C.2) for some noise models used in this thesis, in the case of Gaussian
beliefs.
In general, let ti(u) = ti(uJ) for J ⊂{1, . . . , n}.
In order to up-
date the site approximation ˜ti(uJ) = N U(uJ|bi, Πi), we require the marginal
Q(uJ) = N(hJ, AJ). First, we compute Q\i(uJ) = N(h\i
J , Λ) by converting to
natural parameters, subtracting oﬀbi, Πi and converting back. This results in
Λ =
 A−1
J −Πi
−1 = AJ (I −ΠiAJ)−1 ,
h\i
J = hJ + Λ (ΠihJ −bi) .

C.1. Expectation Propagation
261
For an ADF update, Λ = AJ and h\i
J = hJ, because the site parameters bi, Πi
have been zero. Next, we need the moments of the tilted Gaussian ˆP(uJ) ∝
ti(uJ)Q\i(uJ), which can be computed using (A.14). Let Zi = EQ\i[ti(uJ)]. In
the Gaussian context, it is more direct to compute derivatives of log Zi w.r.t. the
mean h\i
J , but since h\i
J = Λr, r the vector natural parameter of Q\i, these are
readily converted. Let
αi = ∇
 \i
J log Zi,
νi = −∇∇
 \i
J log Zi.
Then,
hnew
J
= h\i
J + ∇
  log Zi = h\i
J + Λαi,
Anew
J
= (I −Λνi) Λ.
To compute the new site parameters, we convert these into natural parameters
and subtract oﬀthe natural parameters of Q\i:
Πnew
i
= νi (I −Λνi)−1 ,
bnew
i
= Πnew
i

h\i
J + ν−1
i αi

.
Note that if the likelihood P(S|u) of the model factorises completely over u, then
|J| = 1 and the matrices and vectors simplify to scalars.
In case of binary classiﬁcation and a probit noise model P(yi|ui) = Φ(yi(ui +
b)), Φ the c.d.f. of N(0, 1), i.e. J = {i}, we have
Zi =
Z
Φ(yi(ui + b))Q\i(ui) dui = Φ
 
yi(h\i
i + b)
√1 + λi
!
(see Equation 4.9). Then,
zi = yi(h\i
i + b)
√1 + λi
,
αi = yiN(zi|0, 1)
Φ(zi)√1 + λi
,
νi = αi
 
αi + h\i
i + b
1 + λi
!
.
In practice, the direct evaluation of N(zi|0, 1)/Φ(zi) is instable for zi ≪0, and
our implementation evaluates a linear lower bound instead.
For general sites ti(uJ), the Gaussian integrals for log Zi and its derivatives
are not analytically tractable. If |J| = 1, the method of choice for approximat-
ing Gaussian integrals is Gaussian quadrature (of the Gauss-Hermite type, e.g.
[145], Sect. 4.5). This method delivers approximations to integrals
Z
f(x)e−x2 dx,

262
Appendix C. Appendix for Chapter 4
requiring the evaluation of f(x) at a few chosen points only. In a nutshell, Gaus-
sian quadrature constructs the unique set of polynomials orthonormal w.r.t. the
inner product
(f, g) =
Z
f(x)g(x)e−x2 dx,
uses the zeros of the degree N member as abscissas and adjusts the weights s.t.
the rule is exact for f(x) being a polynomial of order ≤N −1. It turns out that
the corresponding rule is exact for all polynomials of order ≤2N −1, owing to
the d.o.f.’s for the free choice of abscissas. Thus, its general accuracy hinges on
f(x) being smooth s.t. it can be well-approximated by a polynomial in the region
where |f(x)e−x2| is signiﬁcantly diﬀerent from 0. If this does not hold true for
f(x) (in our case a suitable linear transformation of ti(ui)) it may be necessary
to split the integral into parts or deviate from the standard quadrature rules.
For |J| > 1, Gaussian quadrature is less attractive because the number of grid
points required grows exponentially in |J|. Relevant literature for this problem
is reviewed in [102]. In general, if |J| is large one may have to resort to Monte
Carlo (MC) techniques but their accuracy for rather small |J| is poor compared
to quadrature techniques. The method of exact monomials [120, 47] is suitable
in our context exploiting the symmetry under orthogonal transformation of the
weight function N(0, I).
C.2
Likelihood Approximations. Selection Criteria
In this section, we provide additional details concerning likelihood approximations
and greedy selection criteria for sparse GP approximations.
C.2.1
Optimal Sparse Likelihood Approximations
Recall the notations used in Section 4.2.1. In this section, we give a proof for
Lemma 4.1. For the ﬁrst part (4.2), note that Q(S|uI) is only determined up to a
constant factor. Let us remove this indeterminacy by requiring E
 I∼P[Q(S|uI)] =
1, i.e. Q(u|S) = Q(S|uI)P(u). The problem of minimising (4.2) subject to this

C.2. Likelihood Approximations. Selection Criteria
263
constraint has the Lagrangian
L = −EP (
 I|S) [log Q(S|uI)] + λEP (
 I) [Q(S|uI)] ,
leading to Q(S|uI) = λ−1P(uI)−1P(uI|S). Finally,
P(uI|S) ∝
Z
P(S|u)P(u) du\I = P(uI)EP[P(S|u) | uI],
so that Q(S|uI) ∝EP[P(S|u) | uI]. For the second part (4.3), we note that the
relative entropy term can be rewritten (up to a constant) as
EQ(
 I|S)

log
Q(uI|S)
exp (EP[log P(S|u) | uI]) P(uI)

,
where we used the fact that the prior terms are the same and cancel out. This
is minimised by making sure that numerator and denominator are proportional.3
Note that this is essentially the same argument as the one used to derive free-
form optimal smoothing distributions in variational approximations (see Sec-
tion A.6.3).
Now, suppose that P(S|u) = N U(u|b, Π) and denote by µ =
EP[u|uI] the conditional mean of u given uI under the prior P. Then,
EP[log P(S|u) | uI] ∝−1
2µTΠµ + µTb,
simply log P(S|u) is quadratic in u and for a conditional from a joint Gaussian
distribution, the conditional covariance matrix does not depend on the variable
we condition on (Lemma A.9). Therefore, the optimal likelihood approximation
in the sense of statement (4.3) in the Gaussian case is
Q(S|uI) ∝N U  EP[u|uI]
 b, Π

.
(C.3)
The important fact in the context of Chapter 4 is that the conditional mean µ can
be computed by inverting the marginal covariance of P(uI) only. Unfortunately,
in the case of the ﬁrst statement (4.2), the form of the optimal likelihood approx-
imation requires the computation of a d-by-d block from the inverse covariance
matrix of P(u), which is not feasible for sparse approximations (see Lemma A.1).
3Recall that the relative entropy (Deﬁnition A.4) is non-negative, and 0 iﬀits arguments are
identical distributions.

264
Appendix C. Appendix for Chapter 4
C.2.2
Relaxed Likelihood Approximations
Recall the discussion in Section 4.2.1. From (4.1) it is clear that in general the
O(n2) computation of K can be avoided only if Π = I·,IEII,· for symmetric
nonsingular E ∈Rd,d. However, if µ(x∗) is allowed to depend on all of k∗the
restriction on r can be dropped. Let E = F F ′. Then, the prediction vector
ξ = r −I·,IF (I −F ′KIF )−1 F ′KI,·r
can be computed within our resource constraints (which allow for the evaluation
of KI,·).
Note that our parameterisation translates directly into the form for the like-
lihood approximation which is
Q(S|u) = N U(u|r, Π).
This can be brought into a centred form N UC(µ, Π) iﬀthere exists a µ s.t.
Πµ = r, and in this case r\I = 0. However, the existence of such a µ is not
a strict requirement for a likelihood approximation. For example, consider the
MRED view of large margin classiﬁers (Section 2.1.6) whose “log likelihood” is
linear in u (although there are additional constrained margin variables, adding
nonlinear dependencies). With r\I ̸= 0 the likelihood approximation depends on
u\I in a log-linear way:
Q(S|u) = N U(uI|rI, E) exp
 rT
\Iu\I

.
Whether these additional d.o.f.’s can be used to obtain more accurate approxi-
mations to the true posterior remains open here.
C.2.3
Cheap versus Expensive Selection Criteria
In Section 4.2.2 we noted that for the task of subset selection, cheap criteria
should generally be preferred over expensive ones. In this context, “expensive”
means that a score evaluation is as costly as the actual inclusion, i.e. computation
of Qnew
i
. But if time was not an issue, would a sensible expensive criterion always

C.3. Derivations for the Informative Vector Machine
265
be preferable over a cheap one? Here, we give a simple example that this need
not be the case. Within given resource constraints, criteria should be chosen
according to characteristics and requirements of the task.
Given the symmetric positive deﬁnite linear system Ax = b, we would like
to ﬁnd a sparse approximate solution ˜x = I·,Iβ, where I has a given size d.
Here, A is not available in memory and has to be computed on demand. Let
x∗= A−1b. Minimising ∥˜x −x∗∥directly is not feasible incrementally, but we
can try to minimise φCG(x) = (1/2)xTAx −bTx or φLS(x) = ∥b −Ax∥2 by
greedy forward selection: in each iteration, every i ̸∈I is scored by the maximum
reduction in the criterion achievable if component i of ˜x is allowed to be non-zero.
If |I| = d, this score is O(d) for φCG, but requires the evaluation of the column
A·,i for φLS: φCG is cheap, φLS expensive. But if ∥u∥2
 = uTBu, then φCG(x)
is equal up to constants to ∥x −x∗∥2
  , while φLS(x) corresponds to ∥x −x∗∥2
 2.
If our goal is a good approximation w.r.t. ∥x −x∗∥, then φCG should actually
be preferred as a criterion in this case, because even if φCG over-penalises errors
along eigendirections with high A eigenvalues, φLS does so much stronger. In
this particular case, the task requirements actually lead us to prefer the cheaper
criterion, even if we could aﬀord to use the expensive one.
C.3
Derivations for the Informative Vector Machine
In this section, we collect details concerning the IVM approximation, as intro-
duced in Section 4.4.1.
C.3.1
Update of the Representation
Recall the notation and deﬁnitions used in Section 4.4.1, and also recall that
in order to decide which point to include next into the active set I, we score
all remaining points (or a signiﬁcant fraction thereof). In order to compute a
score for point j ̸∈I, we need to know the corresponding marginal Q(uj), i.e. the
corresponding elements of h and diag A. Here, we describe an eﬃcient and stable
scheme for maintaining this information. We will ﬁrst assume that all marginals

266
Appendix C. Appendix for Chapter 4
have to be kept up-to-date between inclusions.
Further below, we relax this
requirement to end up with a randomised greedy scheme.
Recall that A = (K−1 + Π)−1. Using the Woodbury formula (Lemma A.2)
we can write
A = K −KT
I,·Π1/2
I B−1Π1/2
I KI,·,
B = I + Π1/2
I KIΠ1/2
I .
(C.4)
We maintain a Cholesky decomposition B = LLT (see Section A.2.2) with which
A = K −M TM, where
M = L−1Π1/2
I KI,·
(C.5)
is a matrix we have to maintain in memory. Note that h = M Tβ, using (4.7).
In short, knowledge of the marginal Q(ui) hinges on column M ·,i of M . Suppose
a new point i is to be included into I, and its site parameters bi and πi have
already been computed.
We have to update L →L′ ∈Rd+1,d+1 and M →
M ′ ∈Rd+1,n.
First, L′
1...d,1...d = L and li = L′
1...d,d+1 = L−1π1/2
i
Π1/2
I KI,i =
π1/2
i
M ·,i, furthermore li = L′
d+1,d+1 = (1 + πiKi,i −lT
i li)1/2 = (1 + πiσ2(xi))1/2.
Next, (M ′)1...d,· = M and µ = (M ′)T
d+1,· = l−1
i (π1/2
i
K·,i −M Tli), furthermore
diag Anew = diag A −(µ2
j)j. For the update of h, note that hnew = h + βnewµ,
where βnew is the new component of βnew. We can compute this directly, by
observing that hnew = h + ηai, since Anew −A ∝aiaT
i and bnew −b ∝δi. The
factor η can be determined by solving hnew
i
= hi + ηai,i, resulting in η = αi (see
Section C.1.2 and recall that we are doing ADF updates only). It is also easy to
see that ai = liπ−1/2
i
µ, thus hnew = h + αiliπ−1/2
i
µ and βnew = αiliπ−1/2
i
. The
storage requirements are dominated by M , the running time requirements by
the computation of the new row µ of M , which is O(n d) and the evaluation of
column i of the kernel matrix.
C.3.1.1
Randomised Greedy Selection
Recall from Section 4.4.1 that for randomised greedy selection (RGS), we have
to keep only a subset of the marginals up-to-date, corresponding to a selection
index J. The speed-up comes from the fact that not all columns of M have
to be kept up-to-date at any time, we can use the following delayed updating

C.3. Derivations for the Informative Vector Machine
267
scheme. To this end, we maintain an integer vector g ∈Nn. For any i ̸∈I, gi
is the iteration number of the last recent inclusion for which column i of M was
up-to-date. Denote by L(d) ∈Rd,d, M (d) ∈Rd,n the matrices L, M after the
inclusion of d points. For each i ̸∈I, we know that M 1...gi,i = M (gi)
1...gi,i, where
M is the current buﬀer matrix. We call this vector ∈Rgi the stub for i. Now
suppose we have included d points already and are about to include a further
point, after which the selection index will be modiﬁed to J. In order to update
the components of diag A and h corresponding to J, we have to go through all
j ∈J and make sure that all stubs corresponding to the new J have full size
d + 1. Now, it is central to note that the back-substitution with L(d+1) required
to do this is naturally broken up into steps g = 1, . . . , d + 1, s.t. step g uses the
g-th row of L(d+1) only and produces element g of the stub. Thus, we can save
the ﬁrst gj steps of this procedure and directly start with the stub of j, then do
steps g = gj + 1, . . . , d + 1 as described above, ﬁnally set gj = d + 1. Note that in
order for this randomised scheme to be eﬃcient, J not only has to be signiﬁcantly
smaller than {1, . . . , n} \ I, but also has to be exhibit some “inertia” over time.
If n and d are not too large, a simple implementation would just use O(n d)
memory. As long as this is possible without exhausting the system’s RAM, this
is the most eﬃcient option in terms of running time. If not, a cache structure
should be used. Our present implementation uses a simple structure consisting
of two buﬀers.
The smaller one keeps M·,J (size d × |J|), the larger one has
n −|J| columns of size at most dlim, where dlim should be chosen as large as
possible within resource constraints. Whenever j1 ∈J is to be replaced by j2, we
exchange their stubs in the corresponding buﬀers. This may lead to the stub of
j1 being trimmed to size dlim. In this case, we chop the ﬁrst gj1 −dlim elements,
because these can be re-computed faster later, should j1 return into J at some
later time. This scheme is fairly simple to implement, but may need to be modiﬁed
if very large n are to be used. In this case, the process of selecting new patterns
for inclusion into J should not be uniform, but prefer patterns which have been
in J before. Also, a retain fraction τ which grows with d should be considered.
One can then implement a cache hierarchy which leads to less trimming and re-

268
Appendix C. Appendix for Chapter 4
computation. A general problem in practice with RGS is that the dominating
computational steps which scale with n for full selection, have to be done one-by-
one. For full greedy selection, the large back-substitutions can be done en-block,
having the inner-most loops scale with n or even making use of optimised software
like BLAS,4 while for randomised selection this is not possible. As much as one
would like to ignore such issues, they become painfully important when dealing
with very large n, but this is not within the scope of this thesis.
C.3.2
Exchange Moves
In this section, we derive update equations for exchange moves which at the same
time remove j ∈I from the active set and include i ̸∈I, thus leaving the total
size constant. Exchange candidates (j, i) can be scored in much the same way as
inclusion candidates.
Both the move itself and the score computation require knowledge of the
marginal Q(ui, uj). By the locality property of EP (Lemma C.1), Qnew(u\i|ui) =
Q\j(u\i|ui) and ˜tnew
i
(ui) ∝Qnew(ui)/Q\j(ui). Although here Q\j(u\i|ui) ̸= Q(u\i|ui),
we still have that Q\j(u\j|uj) = Q(u\j|uj) and
Qnew(u\i,j|ui, uj) = Q(u\i,j|ui, uj),
where u\i,j denotes all components of u except ui, uj. Let Q\j(ui) = N(h\j
i , λ\j
i ).
Then,
λ\j
i = ai,i +
πja2
i,j
1 −πjaj,j
,
h\j
i = hi + ai,jπj(hj −mj)
1 −πjaj,j
.
It is clear that the ADF update remains the same as for the simple inclusion of
i, however replacing h\i
i = hi by h\j
i
and λi = ai,i by λ\j
i . Note that the variance
λnew
i
of Qnew(ui) is given by λnew
i
= λ\j
i (1 −νiλ\j
i ).
The representation discussed in Section C.3.1 can be updated as follows. Sup-
pose, the position of j in I is l. Then, we have to replace Π1/2
I
by Π1/2
I
+ (π1/2
i
−
π1/2
j
)δlδT
l . Furthermore, KI = II,·KI·,I, but due to the change of I, I·,I has
to be replaced by I·,I + (δi −δj)δT
l . Note that here, δi, δj ∈Rn, but δl ∈Rd.
4Our present implementation does not include such packets.

C.3. Derivations for the Informative Vector Machine
269
Therefore, Bnew = I + QTKQ with
Q =
 I·,I + (δi −δj)δT
l
 
Π1/2
I
+ (π1/2
i
−π1/2
j
)δlδT
l

.
Noting that I·,Iδl = δj and Π1/2
I δl = π1/2
j
δl, we arrive at
Q = I·,IΠ1/2
I
+

π1/2
i
δi −π1/2
j
δj

δT
l .
It is now easy to see that if we set v = Π1/2
I II,·K(π1/2
i
δi−π1/2
j
δj) = Π1/2
I (π1/2
i
KI,i−
π1/2
j
KI,j), then
Bnew −B = δlvT + vδT
l + ηδlδT
l ,
where η = πiKi,i+πjKj,j−2π1/2
i
π1/2
j
Ki,j > 0. Now, with ˜δl = η1/2δl, ˜v = η−1/2v
we arrive at
Bnew −B =

˜δl + ˜v
 
˜δl + ˜v
T
−˜v ˜vT.
We can now use the technique described in Section A.2.2 in order to update L.
Denote ∆= diag(πi, −πj), the changes in Π, and γ = (πimi, −πjmj)T, the
changes in Πm. Note that
Anew = A −AI·,{ij}∆1/2P −1∆1/2I{ij},·A,
P = I + ∆1/2A{ij}∆1/2,
(C.6)
therefore Anew
{ij} = A{ij} −A{ij}∆1/2P −1∆1/2A{ij} = A{ij}∆1/2P −1∆−1/2. Thus,
by using the deﬁnition of h (see (4.6)), we arrive at
hnew = h + A·,{ij}η,
η = ∆1/2P −1 
∆−1/2γ −∆1/2h{ij}

.
(C.7)
Finally, M is updated in two steps. First, compute
M ′ = L−1 
Π1/2
I
+ (π1/2
i
−π1/2
j
)δlδT
l

KI,· = M + (π1/2
i
−π1/2
j
)
 L−1δl

Kl,·.
Second, note that the new Cholesky factor can be written as LL(1)L(2), but
the factors L(k) have a special structure, allowing for back-substitutions in O(d).
Therefore, the new M matrix can be computed from M ′ in O(n d).
Note that in order to an exchange move, we require the element ai,j of A
which can be computed from M if Ki,j is known. If enough memory is available,
KI,· could be stored alongside M . If the algorithm is at a stage for which further

270
Appendix C. Appendix for Chapter 4
new inclusions are not required, it is sensible to convert from M into an explicit
representation of A which can then be updated using the Woodbury formula5.
Finally, if kernel evaluations are inexpensive, the required elements of K can be
re-evaluated whenever needed.
The diﬀerential entropy score can be computed using Lemma C.1 twice,
namely since Qnew(u\i|ui) = Q\j(u\i|ui) and Q\j(u\j|uj) = Q(u\j|uj), we have
∆Ent
(j,i) = H[Qnew(ui)] −H[Q\j(ui)] + H[Q\j(uj)] −H[Q(uj)]
= 1
2 log
 
λnew
i
(A\j)j,j
λ\j
i aj,j
!
= 1
2 log

1 −λ\j
i νi

−1
2 log(1 −aj,jπj).
For the information gain score, we apply Lemma C.1 once more to obtain
−D[Qnew(u) ∥Q(u)] = −D[Qnew(ui, uj) ∥Q(ui, uj)].
Here, Q(ui, uj) = N(h{ij}, A{ij}) and Qnew(ui, uj) = N(hnew
{ij}, Anew
{ij}), the latter
are given by (C.6) and (C.7). Now, using (A.17), we arrive at
∆Info
(j,i) = −1
2
 log |P | + tr P −1 + ηTA{ij}η −2

.
C.3.3
Model Selection Criterion and Gradient
In Section 4.5.2, we suggested a model selection criterion for IVM which works
together eﬃciently with randomised greedy selection (RGS). Let L ⊂{1, . . . , n}
with I ⊂L such that a buﬀer of size d-by-|L| can be stored. In order to compute
the criterion and its gradient, we have to evaluate M ·,L which is O(|L| d2) from
scratch, but if we include patterns into L which have the largest stubs in the RGS
cache, the evaluation can be signiﬁcantly cheaper. A strategy for choosing L is
discussed below.
The criterion is deﬁned in the same way as QMF for PLV (see Section 4.5.1),
but the true likelihood P(y|u) is replaced by P(yL|uL). This leads to
GL = EQ [−log P(yL|uL)] + D[Q(uI) ∥P(uI)],
5This may be less numerically stable than the Cholesky updates here.

C.3. Derivations for the Informative Vector Machine
271
with
D[Q(uI) ∥P(uI)] = 1
2

log |B| + tr B−1 + ∥β∥2 −
L−Tβ
2 −d

.
With q = diag A = diag(K −M TM ) we have
EQ[−log P(yL|uL)] = −
X
i∈L
EQ[log P(yi|ui)],
Q(ui) = N(hi, qi).
Deﬁne c via ci = EQ[log P(yi|ui)] for later use. We have β = L−1Π−1/2
I
bI and
B = I + Π1/2
I KIΠ1/2
I , thus dB = Π1/2
I (dKI)Π1/2
I
for a variation dK. If we
deﬁne
v = L−Tβ,
e(1) = Π1/2
I B−1v,
e(2) = Π1/2
I v,
then some algebra gives
d log |B| + d tr B−1 = tr B−1Π1/2
I (dKI)

Π1/2
I
−B−1Π1/2
I
T
and
d∥β∥2 −d
L−Tβ
2 = −e(2)T (dKI)e(2) + e(1)T (dKI)e(2) + e(2)T (dKI)e(1).
Furthermore, hL = (M ·,L)Tβ = KL,IΠ1/2
I L−Tβ, so that
dhL = (dKI,L)Te(2) −ET(dKI)e(2)
with
E = Π1/2
I L−TM ·,L.
Also, qL = diag(KL −M T
·,LM ·,L), so
dqL = d(diag K)L −2 diag ET(dKI,L) + diag ET(dKI)E.
(C.8)
Next,
dEQ[−log P(yL|uL)] = −
X
i∈L
EQ [(log P(yi|ui))d log Q(ui)] .
With ai = EQ[(log P(yi|ui))(ui −hi)/qi] and bi = EQ[(log P(yi|ui))((ui −hi)/qi)2],
furthermore di = (1/2)(ci/qi −bi) we have
dEQ[−log P(yL|uL)] = dT
L(dqL) −aT
LdhL.
(C.9)

272
Appendix C. Appendix for Chapter 4
The computation of a, b, c can be done using numerical quadrature.
Since we only need an inner product with dqL, we can use the relation
uT diag ATDB = tr DT  A(diag u)BT
(C.10)
for matrices A, D, B and vector u. Namely, with
˜E = E (diag dL) ,
˜F = E (diag dL) ET
we have
dEQ[−log P(yL|uL)] = dT
Ld(diag K)L −tr

2 ˜E + e(2)aT
L
T
(dKI,L)
+ tr ˜F (dKI) + (EaL)T (dKI)e(2).
If the noise model depends on hyperparameters, a variation of these results in
dEQ[−log P(yL|uL)] = −
X
i∈L
EQ[d log P(yi|ui)].
However, if the hyperparameter (α, say) is a location parameter in the sense that
(d/dα) log P(yi|ui) = (d/dui) log P(yi|ui), we can use partial integration to see
that
EQ
d log P(yi|ui)
dα

= EQ
ui −hi
qi
log P(yi|ui)

= ai,
i.e. (d/dα)EQ[−log P(yL|uL)] = −aT
L1.
The dominating precomputations are M ·,L, E and ˜F , the latter two require
O(|L| d2) (the matrix ˜E overwrites M ·,L). The former can be computed eﬃ-
ciently by completing stubs stored in the RGS cache (see Section C.3.1.1). To
this end, we form L from the patterns with the largest stubs in this cache, given
that L also contains I. As discussed in Section 4.5.2, the objectives for keeping
a pattern’s stub in the cache and having it in L are virtually identical. Given
the precomputations, the computational cost is O(n d) per gradient component.
If the cost for the precomputation violates resource constraints, the dependence
of q on K may be ignored to obtain an approximate gradient, although it is
not clear whether criterion and approximate gradient can still be fed into a cus-
tom optimiser which typically requires that ﬁnite diﬀerences converge against
gradients.

C.4. Derivations for Projected Latent Variables
273
Our implementation requires one d-by-|L| matrix buﬀer by allowing for the
derivative matrices of KI,L to be computed on the ﬂy. It also manages to complete
M ·,L from the stubs available from RGS without having to allocate a second large
buﬀer at any moment.
C.4
Derivations for Projected Latent Variables
In this section, we collect derivations required to develop the projected latent
variables (PLV) scheme introduced in Section 4.4.2.
C.4.1
Site Approximation Updates. Point Inclusions
Recall that γ = E[u|uI] = V TL−1uI.
We have EQ[γ] = µ and VarQ[γ] =
P TVarQ[uI]P = V TM −1V , therefore VarQ[γi] = qi. Q(u) is given by (4.12).
We mark variables after an update (or inclusion) by a prime (e.g., Q →Q′).
Due to the nondiagonal likelihood approximation, the update of the site pa-
rameters for a point i ̸∈I is slightly more involved than in the IVM case. If
Q\i(u) ∝Q(u)N U(γi| −bi, −πi) and ˆP(u) ∝ti(ui)Q\i(u), the site parameters
have to be adjusted such that Q′(γi) has the same mean and variance as ˆP(γi).
If
˜ti(γi) =
Z
ti(ui)N(ui|γi, l2
i ),
l2
i = Ki,i −pi,
then ˆP(γi) ∝˜ti(γi)Q\i(γi) since we have Q\i(ui|uI) = P(ui|ui) = N(ui|γi, l2
i ).
ˆP(γi) is a tilted Gaussian, we can use the derivation in Section C.1.2 if we sub-
stitute ui for γi and ti(ui) for ˜ti(γi). To this end, we require Q\i(γi) which is
Q\i(γi) = N

γi
 µi + (q−1
i
−πi)−1(πiµi −bi), (q−1
i
−πi)−1
by an application of Lemma A.11. Note that we always have q−1
i
−πi > 0. To
see this, let M \i = I + V ·,\iΠ\iV T
·,\i, which is positive deﬁnite. Then,
π−1
i
−qi = π−1
i
−vT
i

M \i + πivivT
i
−1
vi =

πi + (πivi)TM \i(πivi)
−1
> 0
if πi > 0.

274
Appendix C. Appendix for Chapter 4
Note that the update now requires applying one-dimensional Gaussian expec-
tations twice instead of only once in the diagonal case, which may raise problems
if these expectations are done using numerical quadrature. For the probit clas-
siﬁcation noise model ti(ui) = P(yi|ui) = Φ(yi(ui + b)), we arrive at the same
formulae as in Section C.1.2, except that yi has to be replaced by yi/
p
1 + l2
i (and
h\i
i , λi by mean and variance of Q\i(γi)). It is easier to update the site parameters
for i ∈I, because γi = ui and we can use a normal EP update as described in
Section 4.3: just replace ai,i by l2
i + qi and hi by µi.
Since Q(ui) = N(µi, qi), we require µi, qi. Recall that C = Q−1V and note
that µi = cT
i β, qi = ∥ci∥2 where ci = C·,i.
We need to compute ci which
is O(d2). After the update, Q and related representation variables have to be
updated. Suppose π′
i = πi + ∆π, b′
i = bi + ∆b. Then, M ′ = M + ∆πvivT
i with
vi = V ·,i. Therefore, Q′ = Q ˜Q and back-substitutions with ˜Q are O(d) only (see
Section A.2.2). ˜Q can be computed in O(d) given ci which is available from the
update. Q′ itself is obtained from Q in O(d2). We can also update β = Q−1V b
as
β′ = ˜Q
−1 (β + ∆bci)
which is O(d).
For reasons of numerical stability, M and Q should be “refreshed” now and
then using the following procedure. After a ﬁxed number k < n of site parameter
updates we have M = M old + V (∆Π)V T, where ∆Π = Π −Πold. Here, at
most k entries in ∆Π are non-zero and ∆M = V (∆Π)V T can be computed in
O(k d2). We then recompute Q in O(d3). In our implementation, k = d.
Let us ﬁnally see how to include a new point i into I. To this end, we have to
update L →L′ ∈Rd+1,d+1, V →V ′ ∈Rd+1,n, p →p′ and Q →Q′ ∈Rd+1,d+1.
First, (L′)T
d+1,1...d = L−1KI,i = vi, and li = (L′)d+1,d+1 = (Ki,i −vT
i vi)1/2 =
(Ki,i −pi)1/2.
Furthermore, (V ′)1...d,· = V , and if v = (V ′)T
d+1,·, then v =
l−1
i (K·,i −V Tvi), and p′ = p + v ◦v. Then, z = (Q′)d+1,1...d = Q−1V Πv and
z = (Q′)d+1,d+1 = (1+vTΠv−zT z)1/2. Finally, we update β →β′ by adding the
component β′
d+1 = z−1(vTb−βTz). The running time is dominated by the O(n d)
computations of V Tvi and V Πv, furthermore we have to compute the kernel

C.4. Derivations for Projected Latent Variables
275
matrix column K·,i. The site parameters of i should be updated immediately
after the inclusion using the procedure described above.
The update of Q is
unstable for very small ∆π, and we recommend simply to skip such updates of
Π.
In the special case where the site parameters are ﬁxed from the beginning
(Gaussian likelihood, see end of Section 4.4.2), the vectors µ and q (means and
variances of Q marginals) can be maintained and updated explicitly like p. The
new row of C′ is c = z−1(v −CTz), then µ′ = µ + β′
d+1c, q′ = q + c ◦c. Since
CTz = V TQ−Tz, the matrix C need not be stored explicitly.
C.4.2
Information Gain Criterion
Suppose we want to score pattern i w.r.t. its value for inclusion into I. Recall from
Section 4.4.2.1 that in order to obtain a cheap approximation for an expensive
score S(Q, Qnew), where Qnew(u) is the posterior approximation after inclusion
of i, we can try to replace Qnew by ˜Q as deﬁned in (4.15). Here, we show how
the corresponding approximation to the negative information gain score can be
computed:
∆i = −D[ ˜Q(u) ∥Q(u)].
As a ﬁrst step, we have to update the site parameters to ˜bi, ˜πi given that i is
included into I. This is done by an ADF update, but it is a subtle point to note
that the outcome is general diﬀerent from the update described in Section C.4.1,
because ˜Q has a likelihood approximation which depends on ui.
The cavity
marginal Q\i(ui) is given by
Q\i(ui) = N

ui
 µi + (q−1
i
−πi)−1(πiµi −bi), (q−1
i
−πi)−1 + l2
i

,
derived in the same way as in Section C.4.1 and noting that Q\i(ui|uI) = P(ui|uI) =
N(ui|γi, l2
i ). The ADF update is then done as in Section C.1.2, rendering new
values ˜bi, ˜πi. We denote the old values by bi, πi.
Now, ˜Q is obtained from Q by multiplying with N U(−bi|γi, −πi) and N U(˜bi|ui, ˜πi),
then renormalising. Denote the normalisation constant by Zi. In the sequel, we

276
Appendix C. Appendix for Chapter 4
will use ⟨·⟩to denote expectations over ˜Q and ⟨· | ·⟩for conditional expectations
based on ˜Q. We can ﬁrst marginalise over u\I′:
∆i = log Zi + 1
2
 ˜πi⟨( ˜mi −ui)2⟩−πi⟨(mi −γi)2⟩

(C.11)
(recall that mi = bi/πi). In order to compute Zi, we ﬁrst marginalise over ui. Let
τi = (1 + l2
i ˜πi)−1 and ∆b = τi˜bi −bi, ∆π = τi˜πi −πi, ∆m = ∆b/∆π Then,
Zi = τ 1/2
i
exp
1
2∆b∆m −1
2 ˜mi˜bi(τi −1)

Eγi∼Q
h
N UC 
γi
 ∆m, ∆π
i
.
Recall that Q(γi) = N(µi, qi), and deﬁne ρi = (1 + ∆πqi)−1. We can now use
(A.16) and some algebra to obtain
log Zi = 1
2

log τi + ∆m∆b −˜mi˜bi(τi −1) + log ρi −∆πρi(µi −∆m)2
.
(C.12)
Now to the second part of (C.11). We have ⟨( ˜mi −ui)2⟩= ⟨⟨( ˜mi −ui)2|uI⟩⟩=
⟨( ˜mi −⟨ui|uI⟩)2⟩+ ⟨Var(ui|uI)⟩. Now, ˜Q(ui|uI) ∝N U(ui|˜bi, ˜πi)P(ui|uI), i.e.
⟨ui|uI⟩= (˜πi + l−2
i )−1(˜bi + l−2
i γi),
Var(ui|uI) = (˜πi + l−2
i )−1.
Therefore, ⟨( ˜mi −ui)2⟩= τ 2
i ⟨( ˜mi −γi)2⟩+ l2
i τi, and ⟨(mi −γi)2⟩= (mi −
⟨γi⟩)2 + Var(γi). From the application of (A.16) we know that Var(γi) = qiρi and
⟨γi⟩= ∆m + ρi(µi −∆m), therefore ⟨(x −γi)2⟩= qiρi + (x −∆m −ρi(µi −∆m))2.
This is suﬃcient for computing (C.11).
Simpliﬁcations are possible if ˜bi = bi, ˜πi = πi.
In this case, ∆π = (τi −
1)πi, ∆b = (τi −1)bi and ∆m = mi, thus
⟨(mi −ui)2⟩−⟨(mi −γi)2⟩= l2
i τi + (τ 2
i −1)
 qiρi + ρ2
i (µi −mi)2
.
Deﬁne
ξi = (1 −τi)ρi =
1
1 + l−2
i π−1
i
−πiqi
.
Then, some very tedious algebra reveals
∆i = −log(liπ1/2
i
) + 1
2
 log ξi + ξiπi(1 −κi)(mi −µi)2 −κi + 2

,
κi = ξi
 1 + 2π−1
i l−2
i

.
(C.13)

C.4. Derivations for Projected Latent Variables
277
C.4.3
Extended Information Gain Criterion
Suppose we want to score i ̸∈I for inclusion. Recall from Section 4.4.2.1 that we
can obtain successively more accurate approximations to the full information gain
criterion by allowing for couplings between i and points from H with H ∩I ′ = ∅.
Let H′ = H ∪{i} and h = |H|. Recall that γ = E[u|uI] = P TuI. Denote
by P ′ the projection matrix after inclusion of i, and γ′ = E[u|uI′] = (P ′)TuI′.
From the deﬁnition of P , we see that P ′
d+1,H = l−1
i vT
H, where v = (V ′
d+1,·)T is
the new row of V ′ when including i. Furthermore, if ν = (P ′
d+1,H, 1)T ∈Rh+1,
then γ′
H′ = γH′ + (ui −γi)ν. We also use mH′ = Π−1
H′bH′ and ˜mH′ = ˜Π
−1
H′ ˜bH′,
with the usual convention that mj = 0 if πj = bi = 0. In Section 4.4.2.1, we
deﬁned ˜Q as being obtained from Q by multiplying with N UC(γH′|mH′, −ΠH′)
and N UC(γ′
H′| ˜mH′, ˜ΠH′). Note that here, we allow for bH′, ΠH′ to be diﬀer-
ent from ˜bH′, ˜ΠH′.
Finally, recall that Q(γH′) = N(µH′, ΣH′) with ΣH′ =
(V TM −1V )H′ = (C·,H′)TC·,H′. The computation of the covariance matrix is
O(|H′|d2) if C·,H′ is not available, O(|H′|2d) otherwise. The criterion derived in
Section C.4.2 is obtained as special case H = ∅.
We use the same approach as in Section C.4.2. First, we integrate out u\I′,
then determine the log partition function Zi for ˜Q and the parameters for ˜Q(ui|uI)
and ˜Q(γH′). To compute Zi, we ﬁrst marginalise over ui. Again, Q(ui|uI) =
N UC(ui|γi, l−2
i )(2πl2
i )−1/2. Second,
N UC( ˜mH′|γ′
H′, ˜ΠH′) = N UC(uiν | ˜mH′ −γH′ + γiν, ˜ΠH′),
With
a = ˜mH′ −γH′ + γiν,
b = νT ˜ΠH′ν,
a = b−1νT ˜ΠH′a,
c = aT ˜ΠH′a −ba2,
the factor becomes N UC(ui|a, b) exp(−c/2). Using (A.16), we obtain
Zi = (1+l2
i b)−1/2E
 I∼Q

e−c/2N UC  a
 γi, (l2
i + b−1)−1
N UC  mH′
 γH′, −ΠH′
.
(C.14)
The integrand is now a function of γH′. To simplify c, deﬁne
e = b−1 ˜ΠH′ν,
ε = eT ˜mH′ = b−1νT ˜bH′.

278
Appendix C. Appendix for Chapter 4
We have c = aT( ˜ΠH′ −beeT)a and a = ˜mH′ −(I −νδT
h+1)γH′. Note that
a = eTa and eTν = 1. Furthermore, a −γi = ε −eTγH′. If c = γT
H′AγH′ −
2f TγH′+g, i.e. e−c/2 = N U(γH′|f, A, g), then by noting that ( ˜ΠH′−beeT)ν = 0,
i.e. ˜ΠH′ −beeT is not changed by multiplication with I −νδT
h+1, we obtain
A = ˜ΠH′ −beeT, f = ˜ΠH′ ˜mH′ −bεe and g = ˜mT
H′ ˜ΠH′ ˜mH′ −bε2. For later
use, we deﬁne
A(x) = ˜ΠH′ −ΠH′ −xeeT ,
f (x) = ˜bH′ −bH′ −xεe,
g(x) = ˜mT
H′ ˜ΠH′ ˜mH′ −mT
H′ΠH′mH′ −xε2.
Note that
−2 log N U(w|f (x), A(x), g(x)) = −2 log N U(w|f(0), A(0), g(0)) −x(ε −eT w)2.
(C.15)
The second factor in (C.14) is N UC(a|γi, τ) with τ = (l2
i + b−1)−1, this is equal to
N UC(eTγH′|ε, τ). Now, (C.15) implies that multiplying the ﬁrst factor in (C.14)
by the two remaining ones results in A, f, g being replaced by A(b −τ), f (b −
τ), g(b −τ), i.e.
Zi = (1 + l2
i b)−1/2E
 I∼Q

N U(γH′ | f(b −τ), A(b −τ), g(b −τ))

.
(C.16)
We can now use (A.15) to obtain both Zi and ˜Q(γH′). Let
R = Σ−1
H′ + A(b −τ),
r = Σ−1
H′µH′ + f (b −τ),
˜µ = R−1r.
Then, ˜Q(γH′) = N(˜µ, R−1), and
log Zi = −1
2
 log(1 + l2
i b) + log |I + ΣH′A(b −τ)| −rT ˜µ
+ µT
H′Σ−1
H′µH′ + g(b −τ)

.
Again, denote expectation over ˜Q by ⟨·⟩. We have to compute (as second part of
the criterion)
D
−log N UC( ˜mH′|γ′
H′, ˜ΠH′) + log N UC(mH′|γH′, ΠH′)
E
.
(C.17)
As seen above, ˜Q(ui|uI) has variance (l−2
i
+b)−1 and mean a+(1+l2
i b)−1(γi −a).
We ﬁrst condition on uI and integrate out ui. This aﬀects −log N UC(ui|a, b) only

C.4. Derivations for Projected Latent Variables
279
and results in
b
2(1 + l2
i b)−2(γi −a)2 + 1
2(1 + l−2
i b−1)−1

=
1
2b−1τ 2(ε −eTγH′)2 + 1
2l2
i τ

,
the ﬁrst addend is just −log N UC(eT γH′|ε, b−1τ 2).
Again, we can use (C.15)
and merge this into the representation of c. Deﬁne σ = 1 −(1 + l2
i b)−2, so that
b −b−1τ 2 = bσ. Then, (C.17) evaluates to

−log N U(γH′|f (bσ), A(bσ), g(bσ)) + 1
2l2
i τ

= 1
2
 tr A(bσ)
 R−1 + ˜µ ˜µT
−2f (bσ)T ˜µ + g(bσ) + l2
i τ

.
Finally, deﬁne ρ = (1 + b−1l−2
i )−1 = l2
i τ, and note that b −τ = bρ. Combining
the two parts, we ﬁnally have
∆i = 1
2

−log(1 + l2
i b) −log |I + ΣH′A(bρ)| + rT ˜µ −µT
H′Σ−1
H′µH′
+ ε2b(ρ −σ) + tr A(bσ)
 R−1 + ˜µ ˜µT
−2f(bσ)T ˜µ + ρ

.
Using (C.15) we can bring this into a diﬀerent form:
∆i = 1
2

−log(1 + l2
i b) −log |ΣH′| −log |R| + rT ˜µ
−µT
H′Σ−1
H′µH′ −g(bρ) + tr A(bσ)R−1 + ρ
−2 log N U  ˜µ
 f(0), A(0), g(0)

−bσ
 ε −eT ˜µ
2

.
(C.18)
C.4.3.1
Site Parameters Do Not Change
Note that if ˜bH′ = bH′, ˜ΠH′ = ΠH′, the criterion expression can be simpliﬁed
considerably. Let α1 = eTΣH′e, α2 = eTµH′, furthermore
ξi =
1
ρ−1 −bα1
, κi = ξi
 1 + 2(bl2
i )−1
.
Then, in the case h ≤d, some tedious algebra gives
∆i = −log(lib1/2) + 1
2
 log ξi + bξi(1 −κi)(ε −α2)2 −κi + 2

.
(C.19)

280
Appendix C. Appendix for Chapter 4
From this expression, it is easy to recognise (C.13) as the special case H = ∅.
Note that (C.19) is all we need in the case of GP regression with Gaussian noise,
since the site parameters are ﬁxed throughout the algorithm. It also serves as
starting point for the general case, in which we start with ˜bH′ = bH′, ˜ΠH′ = ΠH′,
then update ˜bH′, ˜ΠH′ sequentially, as described below. Note that in order to
compute (C.19), we do not need to compute ΣH′ or µH′ explicitly. It is suﬃcient
to compute u = Q−1V ·,H′e = C·,H′e, since α1 = uTu, α2 = uTβ. If C·,H′
is not available, this costs O(d2) for the back-substitution which dominates the
evaluation of (C.19) if h < d. We can identify (C.13) as special case in which
e = 1 and u = C·,i, thus α1 = qi, α2 = µi.
C.4.3.2
Updating Site Parameters
Before the criterion is evaluated, it is sensible to update the site parameters for
H′, using EP steps. Here, we describe a sequential scheme which updates ∆i
and associated variables (such as e) alongside, which is necessary in order to
compute the marginals required for the EP updates. Namely, for j ∈H ′ located
at position l in H′ we require the cavity marginal ˜Q\j(uj), obtained by setting
the corresponding elements in ˜b, ˜Π to zero. For simplicity, we refer to this cavity
marginal as ˜Q. Recall that γ′
H′ = γH′ + (ui −γi)ν, a Gaussian variable under ˜Q.
Recall that the mean of ˜Q(ui|uI) is
b−1ρ(l−2
i γi + ba) = γi + ρ(a −γi),
the variance is b−1ρ. Therefore, ⟨γ′
H′⟩= ˜µ + ⟨a −γi⟩ρν = ˜µ + ρ(ε −eT ˜µ)ν. For
Var[γ′
H′], we use (A.2) and ⟨γ′
H′ | uI⟩= (I −ρνeT)γH′ + ερν to obtain
Var[γ′
H′] =
 I −ρνeT
R−1  I −ρeνT
+ b−1ρννT.
Recall from Section C.4.1 that for an ADF update we need the cavity marginal
˜Q(γ′
j). Since γ′
j = δT
l γ′
H′, we can read oﬀthe marginal
˜Q(γ′
j) = N
 ρενl + (δl −ρνle)T ˜µ, b−1ρν2
l + (δl −ρνle)TR−1(δl −ρe)

. (C.20)
Recall that if j ̸= i, we also require l′
j
2 = Kj,j −p′
j, where p′
j = pj + (liνl)2 (recall
that ν1...d = l−1
i vH).

C.4. Derivations for Projected Latent Variables
281
A sequential updating scheme could be implemented using the Woodbury
formula (Lemma A.2), but for reasons of numerical stability, we use incremental
Cholesky techniques instead (see Section A.2.2). Let
R0 = Σ−1
H′ + A(0) = L0LT
0 .
Since R = R0 −bρeeT, the Cholesky factor of R can be written as L0L1, where
L1 is of a type discussed in Section A.2.2: back-substitutions with L1 are O(h).
Given L−1
0 e, we can compute L1 in O(h). Factors of this type will be called
cheap, they can be stored in O(h). To ease the notation, we deﬁne
x[A] = A−1x
for a vector x and a Cholesky factor A. Note that x[AB] = x[A][B], and
if A is cheap, then x[A] can be computed in linear time.
Furthermore, let
r0 = r + bρεe = Σ−1
H′µH′ + ˜bH′ −bH′. As a representation, we maintain the fol-
lowing variables up-to-date: δm[L0], m = 1, . . . , h + 1, e[L0], r0[L0], f(0)[L0],
furthermore log |R0|, g(0).
The update procedure has two stages, both modify
˜bH′ →˜bH′ + ∆bδl,
˜ΠH′ →˜ΠH′ + ∆πδlδT
l .
Furthermore, deﬁne ∆g such that ˜b
T
H′ ˜mH′ →˜b
T
H′ ˜mH′ +∆g, i.e. g(0) →g(0)+∆g.
This will change all associated variables, and we mark the corresponding new
versions with a prime (for example, e →e′). The Cholesky factor for R′ is L′
0L′
1.
Since
R′
0 = R0 + ∆πδlδT
l = L0
 I + ∆πδl[L0]δl[L0]T 
LT
0 ,
we can compute L′
0 = L0 ˜L0, where ˜L0 is cheap. This allows us to update the
x[L0] variables. Since R′ = R′
0 −b′ρ′e′e′T, we can compute the new (cheap) L′
1
in O(h), given e′[L′
0]. We also deﬁne
g(α, β) = −αe′ + βδl,
h(α, β) = g(α, β)[L′
0L′
1] = −αe′[L′
0L′
1] + βδl[L′
0L′
1].

282
Appendix C. Appendix for Chapter 4
Note that h(α1, β1)Th(α2, β2) = g(α1, β1)T(R′)−1g(α2, β2). Also note that h(α, β)
can be computed in O(h). Since h(α, β) is required fairly often, it makes sense
to store −e′[L′
0L′
1] = h(1, 0) and δl[L′
0L′
1] = h(0, 1) explicitly.
It is easy to see that
b′ = b + ∆πν2
l ,
ε′ = (b′)−1(bε + ∆bνl),
furthermore
e′ = αee + βeδl,
αe = b/b′, βe = ∆πνl/b′.
Thus, e′[L′
0] = αee[L0][ ˜L0] + βeδl[L′
0], from which L′
1 can be computed as men-
tioned above.
In the ﬁrst stage, we compute the cavity marginal (C.20) in order to perform
an EP update, i.e. ∆b = −˜bj, ∆π = −˜πj. If w = g(ρ′νl, 1), then we need wT ˜µ′
and wT(R′)−1w. We have w[L′
0L′
1] = h(ρ′νl, 1). The inner product with ˜µ′ is
computed as described in the next paragraph. Given the cavity marginal, we do
an EP step to obtain new values ˜π′
j, ˜b′
j. Apart from these, we drop all values
computed in the ﬁrst stage. Only if ˜π′
j is signiﬁcantly diﬀerent from ˜πj, do we
enter the second stage, where the representation is updated explicitly.
Inner products with ˜µ′ are done as follows. Note that ˜µ′ = (R′)−1r′ and
recall that r0 = r + bρεe. Then,
r′[L′
0L′
1] = r′
0[L′
0][L′
1] + h(b′ρ′ε′, 0),
which is used to do inner products with ˜µ′: xT ˜µ′ = x[L′
0L′
1]Tr′[L′
0L′
1]. r0[L0] is
updated as
r′
0[L′
0] = (r0 + ∆bδl)[L′
0] = r0[L0][ ˜L0] + ∆bδl[L′
0].
In the second stage, ∆π = ˜π′
j −˜πj, ∆b = ˜b′
j −˜bj and ∆g accordingly. We
ﬁrst compute ˜L0 and e′[L′
0], δm[L′
0], m = 1, . . . , h + 1 by back-substitutions,
furthermore L′
1 (see above).
The criterion can be computed given the repre-
sentation variables, so it suﬃces to update the latter. Convergence w.r.t. the
criterion could be used as stopping criterion, but the update of the representa-
tion alone is somewhat cheaper than the full criterion computation. We have

C.4. Derivations for Projected Latent Variables
283
r′
0′ = r0 + ∆bδl, f ′(0) = f(0) + ∆bδl, so that in both cases the update works as
x′[L′
0] = x[L0][ ˜L0] + ∆bδl[L′
0].
Next, log |R′
0| = log |R0| + 2 log | ˜L0| and g′(0) = g(0) + ∆g. We complete the
second stage by updating ˜bj, ˜πj.
Finally, here is how to compute ∆′
i given the representation (using primed
notation). First,
log |R′| = log |R′
0| + log

1 −b′ρ′ ∥e′[L′
0]∥2
.
Next, r′T ˜µ′ = ∥r′[L′
0L′
1]∥2, as has been shown above. g(b′ρ′) = g′(0) −b′ρ′(ε′)2,
and
tr A′(b′σ′)(R′)−1 = tr A′(0)
 diag(R′)−1
−b′σ′ ∥h(1, 0)∥2 ,
since A′(0) is diagonal. The elements of diag(R′)−1 are the squared norms of
δm[L′
0L′
1], the latter vectors have to be computed for m = 1, . . . , h + 1. Next,
˜µ′TA′(0)˜µ′ =
h+1
X
m=1
(A′(0))m,m
 ˜µ′Tδm
2 ,
for which we require the δm[L′
0L′
1] once more. Finally, f ′(0)T ˜µ′ and e′T ˜µ′ are
computed as usual.
If h < d, the cost of updating the site parameters and evaluating ∆i is dom-
inated by the computation of ΣH′, which requires h + 1 back-substitutions with
Q (to compute C·,H′). Depending on the update strategy of the concrete imple-
mentation, a larger part or the complete C·,H′ may be available.
C.4.3.3
The Case |H′| > d
So far, we have silently assumed that |H′| ≤d.
If this is not the case, the
above derivation has to be modiﬁed as follows. We have γH′ = P T
·,H′uI, with
Q(uI) = N(µI, ΣI), ΣI = LM −1LT. Go as far as (C.16), but take the integrand
as a function of uI:
Zi = (1 + l2
i b)−1/2E
 I∼Q
h
N U(uI | ˜f(b −τ), ˜A(b −τ), g(b −τ))
i
,

284
Appendix C. Appendix for Chapter 4
with
˜A(x) = P ·,H′

˜ΠH′ −ΠH′

P T
·,H′ −x˜e˜eT,
˜f (x) = P ·,H′

˜bH′ −bH′

−xε˜e,
˜e = b−1P ·,H′ ˜ΠH′ν.
We now use ˜Q(uI) = N(˜µ, R−1) instead of ˜Q(γI′) (the latter is degenerate),
leading to
R = Σ−1
I
+ ˜A(bρ),
r = Σ−1
I µI + ˜f (bρ),
˜µ = R−1r.
With these re-deﬁnitions, we have
∆i = 1
2

−log(1 + l2
i b) −log
I + ΣI ˜A(bρ)
 + rT R−1r −µT
I Σ−1
I µI
+ ε2b(ρ −σ) + tr ˜A(bσ)
 R−1 + ˜µ ˜µT
−2˜f (bσ)T ˜µ + ρ

,
or equivalently
∆i = 1
2

−log(1 + l2
i b) −log |ΣI| −log |R| + rT ˜µ
−µT
I Σ−1
I µI −g(bρ) + tr ˜A(bσ)R−1 + ρ
−2 log N U 
˜µ
 ˜f(0), ˜A(0), g(0)

−bσ
 ε −˜eT ˜µ
2

.
The simpler form (C.19) is valid in this case if we re-deﬁne α1 = ˜eTΣI˜e, α2 =
˜eTµI. In fact, it is easy to see that the computation of the form (C.19) is exactly
the same in both cases h < d and h ≥d. Since ˜e = b−1L−TV ·,H′ ˜ΠH′ν, we can
compute α1 = uTu, α2 = uTβ with
u = Q−1LT ˜e = C·,H′e
at a cost of O(h d) if C·,H′ is given. If we denote the columns of P ·,H′ by wm ∈
Rd, we can use the sequential updating scheme described above as it stands if
we do the substitutions implied by the re-deﬁnitions and also replace δm by
wm everywhere.
The computation of P ·,H′ is O(h d2). Instead of keeping all
wm[L0] up-to-date, we maintain L0 explicitly which allows us to compute wl[L0]

C.4. Derivations for Projected Latent Variables
285
directly in each iteration. Although the ﬁnal criterion evaluation can be done
more eﬃciently, we simply copy the scheme above, evaluating for example
tr ˜A
′(0)(R′)−1 =
h+1
X
m=1
( ˜ΠH′ −ΠH′)m,m ∥w′
m[L′
0L′
1]∥2
at a cost of O(h d2). A more eﬃcient evaluation in O(d2) is possible if the repre-
sentation is extended, but this is not considered here.
C.4.4
Gradient of Model Selection Criterion
Recall the deﬁnition of the PLV model selection criterion G from Section 4.5.1.
In practice, we minimise the sum of G and negative log hyperpriors, the latter
are omitted here for simplicity. In this section, we derive the gradient of G w.r.t.
hyperparameters.
Since Q(uI) depends on the kernel hyperparameters (along with the site pa-
rameters b, Π), the correct gradient has to take this dependence into account.
In fact, the only assumption we use here is that the site parameters b, Π are
independent of the hyperparameters. Recall that
G = EQ [−log P(y|u)] + D[Q(uI) ∥P(uI)]
with the relative entropy term given by (4.17). For a variation dK we have
dD[Q(uI) ∥P(uI)] = 1
2

d log |M | + d tr M −1 + d
Q−Tβ
2
.
The gradient computations are quite involved, and our strategy is as follows. We
aim to write
dG = tr A(1)(dKI) + tr A(2)T (dKI,·) + a(3)
where A(1) is symmetric. In the following, we show how to accumulate A(1), A(2).
Deﬁne
˜L = LQ,
E = ˜L ˜L
T = KI + KI,·ΠK·,I,
b1 = ˜L
−Tβ.
Note that
dE = dKI + 2 sym ((dKI,·)ΠK·,I) ,

286
Appendix C. Appendix for Chapter 4
thus
tr A(dE) = tr A(dKI) + 2 tr (AKI,·Π)T (dKI,·)
(C.21)
(A symmetric) which will frequently be used. We also deﬁne B1 = ˜L
−TC =
E−1KI,·. Note that
µ = K·,Ib1,
C = ˜L
−1KI,·,
b1 = B1b,
Eb1 = KI,·b.
(C.22)
We have
Q−Tβ
2 = bTKT
I,·E−1KIE−1KI,·b.
If we deﬁne
b2 = E−1KIE−1KI,·b = E−1KIb1 = ˜L
−TQ−1LTb1
(using (C.22)), we have
d
Q−Tβ
2 = 2bT
2 (dKI,·)b −2bT
1 (dE)b2 + bT
1 (dKI)b1.
Now, ΠK·,Ib1 = Πµ and ΠK·,Ib2 = (B1Π)TµI, and some algebra gives
d
Q−Tβ
2 = bT
1 (dKI) (b1 −2b2) + 2bT
2 (dKI,·)(b −Πµ)
−2bT
1 (dKI,·)(B1Π)TµI,
thus the accumulation is
A(1)+ = 1
2 sym(b1 −2b2)bT
1 ,
A(2)+ =

b2(b −Πµ)T −b1
 ΠBT
1 µI
T
.
Next, M = L−1EL−T, so that
d log |M | = d log |E| −d log |KI|
= tr
 E−1 −K−1
I

(dKI) + 2 tr(B1Π)T(dKI,·)
and
A(1)+ = 1
2
 E−1 −K−1
I

,
A(2)+ = B1Π.
Next, tr M −1 = tr E−1KI, thus
d tr M −1 = tr E−1(dKI) −tr E−1KIE−1(dE).

C.4. Derivations for Projected Latent Variables
287
Deﬁne
F = EK−1
I E = ˜LQT(˜LQT)T,
then
d tr M −1 = tr
 E−1 −F −1
(dKI) −2 tr(E−1KI)T(dKI,·)(B1Π)T
Here, E−1KI = ˜L
−TQ−1LT. This leads to the accumulations
A(1)+ = 1
2
 E−1 −F −1
,
A(2)+ = −E−1KIB1Π.
Recall that
EQ [−log P(y|u)] = −
n
X
i=1
EQ [log ti(ui)] ,
Q(ui) = N(ui|µi, qi + l2
i ),
where l2
i = Ki,i −pi which is 0 for i ∈I. From (C.9) we know that
EQ [−log P(y|u)] = dT  dq + d(l2
i )i

−aTdµ
with d, a ∈Rn as deﬁned there (see Section C.3.3; replace hi there by µi here, qi
there by qi + l2
i here), and we can make use of relation (C.10) once more. Since
p = diag V TV = KT
I,·K−1
I KI,·, we have
dTd(l2
i )i = dT(d diag K)+dT diag
 V TL−1(dKI)L−TV

−2dT diag
 V TL−1(dKI,·)

.
Using (C.10), this gives
A(1)+ = L−T  V (diag d)V T
L−1,
A(2)+ = −2L−TV (diag d),
a(3)+ = dT(d diag K).
Next, q = diag CTC = diag KT
I,·E−1KI,·, thus
dq = 2 diag BT
1 (dKI,·) −diag BT
1 (dE)B1.
Using (C.10) and (C.21) and deﬁning
˜F = B1(diag d)BT
1 ,

288
Appendix C. Appendix for Chapter 4
we obtain
A(1)+ = −˜F ,
A(2)+ =

2B1(diag d) −˜F KI,·Π

.
Finally, µ = KT
I,·E−1KI,·b, thus
−aTdµ = −bT
1 (dKI,·)a + (B1a)T(dE)b1 −(B1a)T(dKI,·)b
and
A(1)+ = sym b1(B1a)T,
A(2)+ =
 −b1aT −(B1a)bT +
 sym b1(B1a)T
KI,·Π

.
How to compute the gradient eﬃciently, given that in general we expect a
fairly large number of hyperparameters? A straightforward accumulation of A(2)
is wasteful due to the many diﬀerent d × n matrices which are related to each
other by O(n d2) back-substitutions. Here, we settle for a method which might
be problematic w.r.t. numerical stability. In fact, it is easy to see that
A(2) = H(0) + H(1)V Π + H(2)V (diag d),
where H(0) can be computed cheaply and H(1), H(2) ∈Rd,d. Collecting from
above and noting that B1 = ˜L
−TQ−1V = E−1LV , we have
H(1) = −E−1KI ˜L
−TQ−1 + ˜L
−TQ−1 −˜F L +
 sym b1(B1a)T
L,
H(2) = −2L−T + 2 ˜L
−TQ−1,
H(0) =

b2(b −Πµ)T −b1
 ΠBT
1 µI
T
−b1aT −(B1a)bT.
Once the H(i) have been computed, A(2) can be computed overwriting V without
the need for a second large buﬀer.
Note that neither C nor B1 have to be
computed explicitly although we use them for notational convenience. Thus a
sophisticated implementation requires only one n×d matrix (holding V , followed
by A(2)) since the derivative matrices for dKI,· can be computed on the ﬂy. The
gradient computation itself is O(|α| n d) while the precomputations are O(n d2).
If the likelihood factors ti have an associated hyperparameter, a variation
leads to
dG = −
n
X
i=1
EQ[d log ti(ui)]

C.4. Derivations for Projected Latent Variables
289
which again may require numerical quadrature. In the special case where the
hyperparameter α is a location parameter in the sense that
d log ti(ui)
dα
= d log ti(ui)
dui
,
we can use partial integration to see that
EQ
d log ti(ui)
dα

= EQ
ui −µi
qi + l2
i
log ti(ui)

= ai,
so that dG = −aT1dα. Note that bias parameters in classiﬁcation noise models
such as the probit or logit one are location parameters.

290
Appendix C. Appendix for Chapter 4

Bibliography
[1] P. Abrahamsen. A review of Gaussian random ﬁelds and correlation func-
tions. Technical report, Norwegian Computing Centre, 1997.
[2] R. J. Adler. Geometry of Random Fields. John Wiley & Sons, 1981.
[3] M. Anthony and P. Bartlett. Neural Network Learning: Theoretical Foun-
dations. Cambridge University Press, 1999.
[4] M. Anthony and N. Biggs. Computational Learning Theory. Cambridge
University Press, 1997.
[5] M. Anthony and J. Shawe-Taylor. A result of Vapnik with applications.
Discrete Applied Mathematics, 47(2):207–217, 1993.
[6] N. Aronszajn. Theory of reproducing kernels. Trans. Amer. Math. Soc.,
68(3):337–404, 1950.
[7] H. Attias. A variational Bayesian framework for graphical models. In Solla
et al. [185], pages 209–215.
[8] F. Bach and M. Jordan. Kernel independent component analysis. Journal
of Machine Learning Research, 3:1–48, 2002.
[9] C. T. H. Baker. The Numerical Treatment of Integral Equations. Clarendon
Press, Oxford, 1977.
[10] Y. Bar-Shalom and X. Li. Estimation and Tracking: Principles, Techniques,
and Software. Artech House, 1993.
291

292
Bibliography
[11] D. Barber and C. Bishop. Ensemble learning for multi-layer networks. In
M. Jordan, M. Kearns, and S. Solla, editors, Advances in Neural Informa-
tion Processing Systems 10, pages 395–401. MIT Press, 1998.
[12] P. Bartlett, S. Bocheron, and G. Lugosi. Model selection and error estima-
tion. In N. Cesa-Bianchi and S. Goldman, editors, Conference on Compu-
tational Learning Theory 13, pages 286–297. Morgan Kaufmann, 2000.
[13] S. Becker, S. Thrun, and K. Obermayer, editors. Advances in Neural In-
formation Processing Systems 15. MIT Press, 2003. To appear.
[14] J. O. Berger. Statistical Decision Theory and Bayesian Analysis. Springer,
2nd edition, 1985.
[15] Jos´e M. Bernardo and Adrian F. M. Smith. Bayesian Theory. John Wiley
& Sons, 1st edition, 1994.
[16] P. Billingsley. Probability and Measure. John Wiley & Sons, 3rd edition,
1995.
[17] C. Bishop. Neural Networks for Pattern Recognition. Oxford University
Press, 1st edition, 1995.
[18] C. Bishop and B. Frey, editors. Workshop on Artiﬁcial Intelligence and
Statistics 9, 2003. Electronic Proceedings (ISBN 0-9727358-0-1).
[19] C. Bishop and J. Winn.
Structured variational distributions in VIBES.
In Bishop and Frey [18], pages 244–251. Electronic Proceedings (ISBN 0-
9727358-0-1).
[20] S. Boucheron, G. Lugosi, and P. Massart. A sharp concentration inequality
with applications. Random Structures and Algorithms, 16:277–292, 2000.
[21] O. Bousquet. A Bennett concentration inequality and its application to
suprema of empirical processes. C. R. Acad. Sci. Paris, 334:495–500, 2002.

Bibliography
293
[22] Olivier Bousquet and Andr´e Elisseeﬀ. Stability and generalization. Journal
of Machine Learning Research, 2:499–526, 2002.
[23] G. Box and G. Tiao. Bayesian Inference in Statistical Analysis. John Wiley
& Sons, 1992.
[24] S. Boyd and L. Vandenberghe. Convex Optimization. 2002. Available online
at www.stanford.edu/~boyd/cvxbook.html.
[25] X. Boyen and D. Koller. Tractable inference for complex stochastic pro-
cesses. In G. Cooper and S. Moral, editors, Uncertainty in Artiﬁcial Intel-
ligence 14. Morgan Kaufmann, 1998.
[26] N. Brenner. Matrix transposition in place. Communications of the ACM,
16(11):692–694, 1973. CACM Algorithm 467.
[27] G. Cauwenberghs and T. Poggio. Incremental and decremental support
vector machine learning. In Leen et al. [101], pages 409–415.
[28] H. Chernoﬀ.
A measure of asymptotic eﬃciency of tests of a hypothe-
sis based on the sum of observations. Annals of Mathematical Statistics,
23:493–507, 1952.
[29] K. L. Chung. A Course in Probability Theory. Academic Press, 2nd edition,
1974.
[30] B. S. Clarke and A. R. Barron. Information-theoretic asymptotics of Bayes
methods. IEEE Transactions on Information Theory, 36(3):453–471, 1990.
[31] D. Cohn, Z. Ghahramani, and M. Jordan. Active learning with statistical
models. Journal of Artiﬁcial Intelligence Research, 4:129–145, 1996.
[32] C. Cortes, P. Haﬀner, and M. Mohri. Rational kernels. In Becker et al. [13].
To appear.
[33] C. Cortes and V. Vapnik. Support vector networks. Machine Learning,
20:273–297, 1995.

294
Bibliography
[34] Thomas Cover and Joy Thomas. Elements of Information Theory. Series
in Telecommunications. John Wiley & Sons, 1st edition, 1991.
[35] H. Cram´er. Sur un nouveau th´eor`eme limite de la th´eorie des probabilit´es.
Actualit´es Sci. Indust., 736, 1938.
[36] N. Cressie. Statistics for Spatial Data. John Wiley & Sons, 2nd edition,
1993.
[37] N. Cristianini and J. Shawe-Taylor.
An Introduction to Support Vector
Machines and Other Kernel Based Methods. Cambridge University Press,
2000.
[38] Lehel Csat´o. Gaussian Processes — Iterative Sparse Approximations. PhD
thesis, Aston University, Birmingham, UK, March 2002.
[39] Lehel Csat´o and Manfred Opper. Sparse representations of Gaussian pro-
cesses. In Leen et al. [101], pages 444–450.
[40] Lehel Csat´o and Manfred Opper. Sparse online Gaussian processes. Neural
Computation, 14:641–668, 2002.
[41] Lehel Csat´o, Manfred Opper, and Ole Winther. TAP Gibbs free energy,
belief propagation and sparsity. In Dietterich et al. [50], pages 657–663.
[42] I. Csisz´ar and J. K¨orner. Information Theory: Coding Theorems for Dis-
crete Memoryless Systems. Academic Press, 1981.
[43] I. Csisz´ar and G. Tusn´ady. Information geometry and alternating minimiza-
tion procedures. In et. al. E. F. Dedewicz, editor, Statistics and Decisions,
pages 205–237. Oldenburg Verlag, Munich, 1984.
[44] G. Cybenko and M. Berry. Hyperbolic Householder algorithms for factoring
structured matrices. SIAM J. Matrix Anal. Appl., 11:499–520, 1990.
[45] J. Darroch and D. Ratcliﬀ. Generalized iterative scaling for log-linear mod-
els. Annals of Mathematical Statistics, 43(5):1470–1480, 1972.

Bibliography
295
[46] A. Darwiche and N. Friedman, editors. Uncertainty in Artiﬁcial Intelligence
18. Morgan Kaufmann, 2002.
[47] P. Davis and P. Rabinovitz. Methods of Numerical Integration. Academic
Press, 1984.
[48] A. Dempster, N. Laird, and D. Rubin. Maximum likelihood from incomplete
data via the EM algorithm. Journal of Roy. Stat. Soc. B, 39:1–38, 1977.
[49] L. Devroye, L. Gy¨orﬁ, and G. Lugosi. A Probabilistic Theory of Pattern
Recognition. Applications of Mathematics: Stochastic Modelling and Ap-
plied Probability. Springer, 1st edition, 1996.
[50] T. Dietterich, S. Becker, and Z. Ghahramani, editors. Advances in Neural
Information Processing Systems 14. MIT Press, 2002.
[51] J. Dongarra, J. Du Croz, S. Hammarling, and R. Hanson. An extended set
of FORTRAN basic linear algebra subprograms. ACM Trans. Math. Soft.,
14:1–17, 1988.
[52] A. Faul and M. Tipping. Analysis of sparse Bayesian learning. In Dietterich
et al. [50], pages 383–389.
[53] V. Fedorov. Theory of Optimal Experiments. Academic Press, 1972.
[54] S. Fine and K. Scheinberg. Eﬃcient SVM training using low-rank kernel
representations. Journal of Machine Learning Research, 2:243–264, 2001.
[55] Roger Fletcher. Practical Methods of Optimization: Unconstrained Opti-
mization, volume 1. John Wiley & Sons, 1980.
[56] Roger Fletcher. Practical Methods of Optimization: Constrained Optimiza-
tion, volume 2. John Wiley & Sons, 1989.
[57] Y. Freund and R. Schapire. Experiments with a new boosting algorithm. In
L. Saitta, editor, International Conference on Machine Learning 13, pages
148–156. Morgan Kaufmann, 1996.

296
Bibliography
[58] Y. Freund, H. Seung, E. Shamir, and N. Tishby. Selective sampling using
the query by committee algorithm. Machine Learning, 28:133–168, 1997.
[59] J. Friedman, T. Hastie, and R. Tibshirani.
Additive logistic regression:
a statistical view of boosting. Technical report, Department of Statistics,
Stanford University, 1998.
[60] N. Friedman and I. Nachman. Gaussian process networks. In C. Boutilier
and M. Goldszmidt, editors, Uncertainty in Artiﬁcial Intelligence 16, pages
211–219. Morgan Kaufmann, 2000.
[61] A. Frieze, R. Kannan, and S. Vempala. Fast Monte-Carlo algorithms for
ﬁnding low-rank approximations. In Foundations of Computer Science 39,
pages 370–378, 1998.
[62] Z. Ghahramani and M. Beal.
Propagation algorithms for variational
Bayesian learning. In Leen et al. [101], pages 507–513.
[63] Mark N. Gibbs. Bayesian Gaussian Processes for Regression and Classiﬁ-
cation. PhD thesis, University of Cambridge, 1997.
[64] W. Gilks, S. Richardson, and D. Spiegelhalter, editors.
Markov Chain
Monte Carlo in Practice. Chapman & Hall, 1st edition, 1996.
[65] A. Girard, C. Rasmussen, and R. Murray-Smith.
Gaussian process pri-
ors with uncertain inputs — application to multiple-step ahead time series
forecasting. In Becker et al. [13]. To appear.
[66] P.J. Green and Bernhard Silverman. Nonparametric Regression and Gener-
alized Linear Models. Monographs on Statistics and Probability. Chapman
& Hall, 1994.
[67] Geoﬀrey Grimmett and David Stirzaker. Probability and Random Processes.
Oxford University Press, 3rd edition, 2001.
[68] L. Gy¨orﬁ, editor. Principles of Nonparametric Learning. Springer, 2002.

Bibliography
297
[69] David Haussler. Convolution kernels on discrete structures. Technical Re-
port UCSC-CRL-99-10, University of California, Santa Cruz, July 1999.
See http://www.cse.ucsc.edu/~haussler/pubs.html.
[70] David Haussler, Michael Kearns, and Robert Schapire.
Bounds on the
sample complexity of Bayesian learning using information theory and the
VC dimension. Machine Learning, 14:83–113, 1994.
[71] David Haussler and Manfred Opper. Mutual information, metric entropy
and cumulative relative entropy risk. Annals of Statistics, 25(6):2451–2492,
1997.
[72] Ralf Herbrich. Learning Kernel Classiﬁers. MIT Press, 1st edition, 2001.
[73] Ralf Herbrich and Thore Graepel. A PAC-Bayesian margin bound for linear
classiﬁers: Why SVMs work. In Leen et al. [101], pages 224–230.
[74] Tom Heskes and Onno Zoeter. Expectation propagation for approximate
inference in dynamic Bayesian networks. In Darwiche and Friedman [46].
[75] G. E. Hinton and R. M. Neal.
A new view on the EM algorithm that
justiﬁes incremental and other variants. In Jordan [85].
[76] J.-B. Hiriart-Urruty and C. Lemar´echal. Convex Analysis and Minimiza-
tion Algorithms I. Number 305 in Grundlehren der mathematischen Wis-
senschaften. Springer, 1993.
[77] J.-B. Hiriart-Urruty and C. Lemar´echal. Convex Analysis and Minimiza-
tion Algorithms II. Number 306 in Grundlehren der mathematischen Wis-
senschaften. Springer, 1993.
[78] Roger Horn and Charles Johnson. Matrix Analysis. Cambridge University
Press, 1st edition, 1985.
[79] Shunsuke Ihara. Information Theory for Continuous Systems. World Sci-
entiﬁc, 1st edition, 1993.

298
Bibliography
[80] T. Jaakkola. Variational Methods for Inference and Estimation in Graphical
Models. PhD thesis, Massachusetts Institute of Technology, 1997.
[81] T. S. Jaakkola and D. Haussler. Exploiting generative models in discrimi-
native classiﬁers. In Kearns et al. [87], pages 487–493.
[82] Tommi Jaakkola and David Haussler. Probabilistic kernel regression mod-
els. In D. Heckerman and J. Whittaker, editors, Workshop on Artiﬁcial
Intelligence and Statistics 7. Morgan Kaufmann, 1999.
[83] Tommi Jaakkola, Marina Meila, and Tony Jebara. Maximum entropy dis-
crimination. In Solla et al. [185], pages 470–476.
[84] M. Jordan. An introduction to probabilistic graphical models. In prepara-
tion, 2003.
[85] M. I. Jordan, editor. Learning in Graphical Models. Kluwer, 1997.
[86] M. I. Jordan and R. A. Jacobs. Hierarchical mixtures of experts and the
EM algorithm. Neural Computation, 6:181–214, 1994.
[87] M. Kearns, S. Solla, and D. Cohn, editors. Advances in Neural Information
Processing Systems 11. MIT Press, 1999.
[88] M. Kearns and U. Vazirani. An Introduction to Computational Learning
Theory. MIT Press, 1994.
[89] G. Kimeldorf and G. Wahba. A correspondence between Bayesian estima-
tion of stochastic processes and smoothing by splines. Annals of Mathe-
matical Statistics, 41:495–502, 1970.
[90] A. N. Kolmogorov. Foundations of the Theory of Probability. Chelsea, New
York, 2nd edition, 1933. Trans. N. Morrison (1956).
[91] V. Koltchinskii and D. Panchenko.
Empirical margin distributions and
bounding the generalization error of combined classiﬁers. Annals of Statis-
tics, 30(1):1–50, 2002.

Bibliography
299
[92] R. I. Kondor and J. Laﬀerty. Diﬀusion kernels on graphs and other dis-
crete input spaces. In C. Sammut and A. Hofmann, editors, International
Conference on Machine Learning 19. Morgan Kaufmann, 2002.
[93] D. Krige. A statistical approach to some basic mine valuation problems
on the witwatersrand. Journal of the Chemical, Metallurgical and Mining
Society of South Africa, 52:119–139, 1951.
[94] J. Langford and J. Shawe-Taylor. PAC-Bayes and margin. In Becker et al.
[13]. To appear.
[95] John Langford and Rich Caruana. (Not) bounding the true error. In Diet-
terich et al. [50], pages 809–816.
[96] P. Langley, editor. International Conference on Machine Learning 17. Mor-
gan Kaufmann, 2000.
[97] Neil D. Lawrence and Ralf Herbrich. A sparse Bayesian compression scheme
- the informative vector machine. Presented at NIPS 2001 Workshop on
Kernel Methods, 2001.
[98] Neil D. Lawrence, Matthias Seeger, and Ralf Herbrich. Fast sparse Gaussian
process methods: The informative vector machine. In Becker et al. [13]. See
www.dai.ed.ac.uk/~seeger/papers.html.
[99] Y. LeCun, B. Boser, J. Denker, D. Henderson, R. Howard, W. Hubbard, and
L. Jackel. Backpropagation applied to handwritten zip code recognition.
Neural Computation, 1:541–551, 1989.
[100] M. Ledoux and M. Talagrand. Probability in Banach Spaces. Springer,
1991.
[101] T. Leen, T. Dietterich, and V. Tresp, editors. Advances in Neural Informa-
tion Processing Systems 13. MIT Press, 2001.
[102] U. Lerner. Hybrid Bayesian Networks. PhD thesis, Stanford University,
2002.

300
Bibliography
[103] Nick Littlestone and Manfred Warmuth. Relating data compression and
learnability. Technical report, University of California, Santa Cruz, 1986.
[104] Jun Liu. Markov chain Monte Carlo and related topics. Technical report,
Department of Statistics, Stanford University, 1999.
[105] G. Lugosi. Pattern classiﬁcation and learning theory. In Gy¨orﬁ[68].
[106] G. Lugosi. Concentration-of-measure inequalities. Technical report, 2003.
Presented at Summer School Machine Learning, ANU Canberra.
[107] Z. Luo and G. Wahba. Hybrid adaptive splines. Journal of the American
Statistical Association, 92:107–116, 1997.
[108] D. MacKay. Information-based objective functions for active data selection.
Neural Computation, 4(4):589–603, 1991.
[109] D. MacKay. Bayesian non-linear modeling for the energy prediction com-
petition. In ASHRAE Transactions, volume 100, pages 1053–1062, 1994.
[110] D. MacKay.
Probable networks and plausible predictions – a review of
practical Bayesian methods for supervised neural networks.
Network —
Computation in Neural Systems, 6(3):469–505, 1995.
[111] D.
MacKay.
Introduction
to
Gaussian
processes.
Tech-
nical
report,
Cambridge
University,
UK,
1997.
See
http://wol.ra.phy.cam.ac.uk/mackay/README.html.
[112] G. Matheron. Principles of geostatistics. Economic Geology, 58:1246–1266,
1963.
[113] G. Matheron. The intrinsic random functions and their applications. Jour-
nal for Applied Probability, 5:439–468, 1973.
[114] D. McAllester and L. Ortiz. Concentration inequalities for the missing mass
and for histogram rule error. In Becker et al. [13]. To appear.

Bibliography
301
[115] David McAllester. PAC-Bayesian model averaging. In Conference on Com-
putational Learning Theory 12, pages 164–170, 1999.
[116] David McAllester.
Some PAC-Bayesian theorems.
Machine Learning,
37(3):355–363, 1999.
[117] David McAllester. PAC-Bayesian stochastic model selection. To appear in
Machine Learning. See www.autoreason.com, 2002.
[118] P. McCullach and J.A. Nelder. Generalized Linear Models. Number 37 in
Monographs on Statistics and Applied Probability. Chapman & Hall, 1st
edition, 1983.
[119] R. McEliece, D. MacKay, and J.-F. Cheng. Turbo decoding as an instance
of Pearl’s belief propagation algorithm. IEEE Journal on Selected Areas in
Communications, 16(2):140–152, 1998.
[120] J. McNamee and F. Stenger. Construction of fully symmetric numerical
integration formulas. Numerische Mathematik, 10:327–344, 1967.
[121] Ron Meir and Tong Zhang. Data-dependent bounds for Bayesian mixture
methods. In Becker et al. [13]. See citeseer.nj.nec.com/536920.html.
[122] T.
Minka
and
R.
Picard.
Learning
how
to
learn
is
learn-
ing
with
point
sets.
Unpublished
manuscript.
Available
at
http://wwwwhite.media.mit.edu/~tpminka/papers/learning.html.,
1997.
[123] Thomas Minka. The EP energy function and minimization schemes. See
www.stat.cmu.edu/~minka/papers/learning.html, August 2001.
[124] Thomas Minka. Expectation propagation for approximate Bayesian infer-
ence. In J. Breese and D. Koller, editors, Uncertainty in Artiﬁcial Intelli-
gence 17. Morgan Kaufmann, 2001.
[125] Thomas Minka. A Family of Algorithms for Approximate Bayesian Infer-
ence. PhD thesis, Massachusetts Institute of Technology, January 2001.

302
Bibliography
[126] E.
H.
Moore.
On
properly
positive
hermitian
matrices.
Bull. Amer. Math. Soc., 23:59, 1916.
[127] K. Murphy. Dynamic Bayesian Networks: Representation, Inference and
Learning. PhD thesis, University of California, Berkeley, 2003.
[128] K. Murphy, Y. Weiss, and M. Jordan. Loopy belief propagation for approx-
imate inference: An empirical study. In K. Laskey and H. Prade, editors,
Uncertainty in Artiﬁcial Intelligence 15, pages 467–475. Morgan Kaufmann,
1999.
[129] B. Natajaran. Sparse approximate solutions to linear systems. SIAM Jour-
nal of Computing, 24(2):227–234, 1995.
[130] R. M. Neal. Probabilistic inference using Markov chain Monte Carlo meth-
ods. Technical report, University of Toronto, 1993.
[131] R. M. Neal. Bayesian Learning for Neural Networks. Number 118 in Lecture
Notes in Statistics. Springer, 1996.
[132] Radford M. Neal. Monte Carlo implementation of Gaussian process models
for Bayesian classiﬁcation and regression. Technical Report 9702, Depart-
ment of Statistics, University of Toronto, January 1997.
[133] J. Nelder and R. Wedderburn.
Generalized linear models.
Journal of
Roy. Stat. Soc. B, 135:370–384, 1972.
[134] D. Nychka, G. Wahba, S. Goldfarb, and T. Pugh. Cross-validated spline
methods for the estimation of three dimensional tumor size distributions
from observations on two dimensional cross sections. Journal of the Amer-
ican Statistical Association, 79:832–846, 1984.
[135] A. O’Hagan. Curve ﬁtting and optimal design. Journal of Roy. Stat. Soc. B,
40(1):1–42, 1978.

Bibliography
303
[136] A. O’Hagan. Some Bayesian numerical analysis. In J. Bernardo, J. Berger,
A. Dawid, and A. Smith, editors, Bayesian Statistics 4, pages 345–363.
Oxford University Press, 1992.
[137] M. Opper and O. Winther. Gaussian process classiﬁcation and SVM: Mean
ﬁeld results and leave-one-out estimator. In Smola et al. [179].
[138] Manfred Opper.
A Bayesian approach to on-line learning.
In D. Saad,
editor, On-Line Learning in Neural Networks. Cambridge University Press,
1998.
[139] Manfred Opper and Ole Winther.
Gaussian processes for classiﬁcation:
Mean ﬁeld algorithms. Neural Computation, 12(11):2655–2684, 2000.
[140] J. Platt. Probabilistic outputs for support vector machines and comparisons
to regularized likelihood methods. In Smola et al. [179].
[141] J. Platt, C. Burges, S. Swenson, C. Weare, and A. Zheng.
Learning a
Gaussian process prior for automatically generating music playlists.
In
Dietterich et al. [50], pages 1425–1432.
[142] John C. Platt. Fast training of support vector machines using sequential
minimal optimization. In Sch¨olkopf et al. [160], pages 185–208.
[143] M. Plutowski and H. White. Selecting concise training sets from clean data.
IEEE Transactions on Neural Networks, 4(2):305–318, 1993.
[144] T. Poggio and F. Girosi. Networks for approximation and learning. Pro-
ceedings of IEEE, 78(9):1481–1497, 1990.
[145] William H. Press, Saul A. Teukolsky, William T. Vetterling, and Brian P.
Flannery. Numerical Recipes in C. Cambridge University Press, 2nd edition,
1992.
[146] C. E. Rasmussen. Evaluation of Gaussian Processes and Other Methods for
Nonlinear Regression. PhD thesis, University of Toronto, 1996.

304
Bibliography
[147] C. E. Rasmussen. The inﬁnite Gaussian mixture model. In Solla et al.
[185], pages 554–560.
[148] C. E. Rasmussen and Z. Ghahramani. Inﬁnite mixtures of Gaussian process
experts. In Dietterich et al. [50], pages 881–888.
[149] R. Redner and H. Walker. Mixture densities, maximum likelihood and the
EM algorithm. SIAM Review, 26:195–239, 1984.
[150] B. D. Ripley. Stochastic Simulation. John Wiley & Sons, 1987.
[151] Christian P. Robert and George Casella. Monte Carlo Statistical Methods.
Texts in Statistics. Springer, 1st edition, 1999.
[152] R. Rockafellar. Convex Analysis. Princeton University Press, 1970.
[153] K. Rose. Deterministic annealing for clustering, compression, classiﬁcation,
regression, and related optimization problems. Proceedings of the IEEE,
86(11):2210–2239, 1998.
[154] F. Rosenblatt. The perceptron: A probabilistic model for information stor-
age and organization in the brain. Psychological Review, 65(6):386–408,
1958.
[155] S. Roweis and L. Saul. Nonlinear dimensionality reduction by locally linear
embedding. Science, 290:2323–2326, 2000.
[156] P. Rujan.
A fast method for calculating the perceptron with maximal
stability. Journal de Physique I, 3:277–290, 1993.
[157] Y. Saad. Iterative Methods for Sparse Linear Systems. International Thom-
son Publishing, 1st edition, 1996.
[158] R. Schapire, Y. Freund, P. Bartlett, and W. S. Lee. Boosting the mar-
gin: A new explanation for the eﬀectiveness of voting methods. Annals of
Statistics, 26(5):1651–1686, 1998.

Bibliography
305
[159] R. Schapire, Y. Freund, P. Bartlett, and W. S. Lee. Boosting the mar-
gin: A new explanation for the eﬀectiveness of voting methods. Annals of
Statistics, 26(5):1651–1686, 1998.
[160] B. Sch¨olkopf, C. Burges, and A. Smola, editors. Advances in Kernel Meth-
ods: Support Vector Learning. MIT Press, 1998.
[161] Bernhard Sch¨olkopf and Alexander Smola. Learning with Kernels. MIT
Press, 1st edition, 2002.
[162] I. J. Sch¨onberg.
Metric spaces and completely monotone functions.
In
Proc. Nat. Acad. Sci., volume 39, pages 811–841, 1938.
[163] I. J. Sch¨onberg. Spline functions and the problem of graduation. Annals of
Mathematicals, 52:947–950, 1964.
[164] M. Seeger. Covariance kernels from Bayesian generative models. In Diet-
terich et al. [50], pages 905–912.
[165] M. Seeger, C. Williams, and N. Lawrence. Fast forward selection to speed
up sparse Gaussian process regression. In Bishop and Frey [18], pages 205–
212. Electronic Proceedings (ISBN 0-9727358-0-1).
[166] Matthias Seeger. Bayesian methods for support vector machines and Gaus-
sian processes. Master’s thesis, University of Karlsruhe, Germany, 1999.
See http://www.dai.ed.ac.uk/~seeger/papers.html.
[167] Matthias Seeger. Annealed expectation-maximization by entropy projec-
tion.
Technical report, Institute for ANC, Edinburgh, UK, 2000.
See
http://www.dai.ed.ac.uk/~seeger/papers.html.
[168] Matthias Seeger. Bayesian model selection for support vector machines,
Gaussian processes and other kernel classiﬁers. In Solla et al. [185], pages
603–609.

306
Bibliography
[169] Matthias Seeger.
Covariance kernels from Bayesian generative mod-
els.
Technical report, Institute for ANC, Edinburgh, UK, 2000.
See
http://www.dai.ed.ac.uk/~seeger/papers.html.
[170] Matthias Seeger. PAC-Bayesian generalization error bounds for Gaussian
process classiﬁcation. Journal of Machine Learning Research, 3:233–269,
October 2002.
[171] Matthias
Seeger.
PAC-Bayesian
generalization
error
bounds
for
Gaussian
process
classiﬁcation.
Technical
Report
EDI-INF-RR-
0094, Division of Informatics, University of Edinburgh,
2002.
See
www.dai.ed.ac.uk/~seeger/papers.html.
[172] Matthias Seeger, John Langford, and Nimrod Megiddo. An improved pre-
dictive accuracy bound for averaging classiﬁers. In C. E. Brodley and A. P.
Danyluk, editors, International Conference on Machine Learning 18, pages
290–297. Morgan Kaufmann, 2001.
[173] Matthias
Seeger,
Neil
D.
Lawrence,
and
Ralf
Herbrich.
Sparse
Bayesian learning:
The informative vector machine.
Technical re-
port,
Department of Computer Science,
Sheﬃeld,
UK, 2002.
See
www.dcs.shef.ac.uk/~neil/papers/.
[174] H. S. Seung, M. Opper, and H. Sompolinsky. Query by committee. In
Conference on Computational Learning Theory 5, pages 287–294. Morgan
Kaufmann, 1992.
[175] J. Shawe-Taylor and C. Williams. The stability of kernel principal com-
ponents analysis and its relation to the process eigenspectrum. In Becker
et al. [13]. To appear.
[176] J. Shawe-Taylor and R. Williamson. A PAC analysis of a Bayesian esti-
mator. In Conference on Computational Learning Theory 10, pages 2–9,
1997.

Bibliography
307
[177] John Shawe-Taylor, Peter L. Bartlett, Robert C. Williamson, and Martin
Anthony.
Structural risk minimization over data-dependent hierarchies.
IEEE Transactions on Information Theory, 44(5):1926–1940, 1998.
[178] J. Skilling. Bayesian numerical analysis. In John Skilling, editor, Maximum
Entropy and Bayesian Methods. Cambridge University Press, 1989.
[179] A. Smola, P. Bartlett, B. Sch¨olkopf, and D. Schuurmans, editors. Advances
in Large Margin Classiﬁers. MIT Press, 1999.
[180] A. Smola, Z. ´Ov´ari, and R. Williamson. Regularization with dot-product
kernels. In Leen et al. [101], pages 308–314.
[181] A. Smola and B. Sch¨olkopf. Sparse greedy matrix approximation for ma-
chine learning. In Langley [96], pages 911–918.
[182] A. Smola, B. Sch¨olkopf, and K.-R. M¨uller. The connection between regular-
ization operators and support vector kernels. Neural Networks, 11:637–649,
1998.
[183] Alex Smola and Peter Bartlett. Sparse greedy Gaussian process regression.
In Leen et al. [101], pages 619–625.
[184] E. Solak, R. Murray-Smith, W. Leithead, and C. Rasmussen. Derivative
observations in Gaussian process models of dynamic systems. In Becker
et al. [13]. To appear.
[185] S. Solla, T. Leen, and K.-R. M¨uller, editors. Advances in Neural Informa-
tion Processing Systems 12. MIT Press, 2000.
[186] Peter Sollich. Learning curves for Gaussian processes. In Kearns et al. [87],
pages 344–350.
[187] Peter Sollich. Probabilistic methods for support vector machines. In Solla
et al. [185], pages 349–355.

308
Bibliography
[188] D. Spiegelhalter, A. Thomas, N. Best, and W. Gilks.
BUGS: Bayesian
inference using Gibbs sampling. Technical report, MRC Biostatistics Unit,
Cambridge University, 1995.
[189] M. Stein. Interpolation of Spatial Data: Some Theory for Kriging. Springer,
1999.
[190] Y. Teh and M. Welling. On improving the eﬃciency of the iterative propor-
tional ﬁtting procedure. In Bishop and Frey [18], pages 213–220. Electronic
Proceedings (ISBN 0-9727358-0-1).
[191] J. Tenenbaum, A. de Silva, and J. Langford. A global geometric framework
for nonlinear dimensionality reduction. Science, 290:2319–2323, 2000.
[192] Michael Tipping. Sparse Bayesian learning and the relevance vector ma-
chine. Journal of Machine Learning Research, 1:211–244, 2001.
[193] Simon Tong and Daphne Koller. Support vector machine active learning
with applications to text classiﬁcation. Journal of Machine Learning Re-
search, 2:45–66, 2001.
[194] Volker Tresp.
A Bayesian committee machine.
Neural Computation,
12(11):2719–2741, 2000.
[195] L. G. Valiant. A theory of the learnable. Communications of the ACM,
27(11):1134–1142, 1984.
[196] V. Vapnik and A. Chervonenkis. On the uniform convergence of relative
frequencies of events to their probabilities. Theory of Probability and Ap-
plications, 16(2):264–280, 1971.
[197] Vladimir N. Vapnik. Statistical Learning Theory. Wiley, 1st edition, 1998.
[198] F. Vivarelli and C. K. I. Williams. Discovering hidden features with Gaus-
sian process regression. In Kearns et al. [87].

Bibliography
309
[199] A. van der Waart and J. Wellner. Weak Convergence and Empirical Pro-
cesses. Springer, 1996.
[200] Grace Wahba. Spline Models for Observational Data. CBMS-NSF Regional
Conference Series. SIAM Society for Industrial and Applied Mathematics,
1990.
[201] Grace Wahba. Support vector machines, reproducing kernel Hilbert spaces
and the randomized GACV. In Sch¨olkopf et al. [160], pages 69–88.
[202] M. Wainwright, E. Sudderth, and A. Willsky. Tree-based modeling and
estimation of Gaussian processes on graphs with cycles.
In Leen et al.
[101], pages 661–667.
[203] M. J. Wainwright. Stochastic Processes on Graphs with Cycles. PhD thesis,
Massachusetts Institute of Technology, January 2002.
[204] M. J. Wainwright, T. Jaakkola, and A. S. Willsky. A new class of upper
bounds on the log partition function. In Darwiche and Friedman [46].
[205] S. R. Waterhouse and A. J. Robinson.
Classiﬁcation using hierarchical
mixtures of experts.
In IEEE Workshop on Neural Networks for Signal
Processing 4, pages 177–186, 1994.
[206] Y. Weiss and W. Freeman. Correctness of belief propagation in Gaussian
graphical models of arbitrary topology. In Solla et al. [185], pages 673–679.
[207] C. Williams. Computation with inﬁnite neural networks. Neural Compu-
tation, 10(5):1203–1216, 1998.
[208] Christopher Williams and Matthias Seeger. The eﬀect of the input density
distribution on kernel-based classiﬁers. In Langley [96], pages 1159–1166.
[209] Christopher K. I. Williams.
Prediction with Gaussian processes: From
linear regression to linear prediction and beyond. In Jordan [85].

310
Bibliography
[210] Christopher K. I. Williams and David Barber. Bayesian classiﬁcation with
Gaussian processes. IEEE Transactions on Pattern Analysis and Machine
Intelligence, 20(12):1342–1351, 1998.
[211] Christopher K. I. Williams and Matthias Seeger. Using the Nystr¨om method
to speed up kernel machines. In Leen et al. [101], pages 682–688.
[212] S. Wright. Modiﬁed Cholesky factorizations in interior-point algorithmsfor
linear programming. SIAM Journal on Optimization, 9(4):1159–1191, 1999.
[213] L. Xu and M. Jordan. On convergence properties of the EM algorithm for
Gaussian mixtures. Neural Computation, 8(1):129–151, 1996.
[214] A. Yaglom. Correlation Theory of Stationary and Related Random Func-
tions, volume I. Springer, 1987.
[215] J. S. Yedidia, W. T. Freeman, and Y. Weiss. Generalized belief propagation.
In Leen et al. [101], pages 689–695.
[216] H. Zhu, C. K. I. Williams, R. Rohwer, and M. Morciniec. Gaussian regres-
sion and optimal ﬁnite dimensional linear models. In C. Bishop, editor,
Neural Networks and Machine Learning, volume 168 of NATO ASI series.
Springer, 1998.

