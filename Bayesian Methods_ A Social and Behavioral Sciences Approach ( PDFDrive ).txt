Bayesian Methods
A Social and Behavioral 
Sciences Approach
Third Edition
Jeff Gill
Statistics in the Social and Behavioral Sciences Series
Chapman & Hall/CRC 


Bayesian 
Methods
A Social and  
Behavioral Sciences  
Approach
Third Edition

Statistics in the Social and Behavioral Sciences Series
Aims and scope
Large and complex datasets are becoming prevalent in the social and behavioral  
sciences and statistical methods are crucial for the analysis and interpretation of such 
data. This series aims to capture new developments in statistical methodology with 
particular relevance to applications in the social and behavioral sciences. It seeks to 
promote appropriate use of statistical, econometric and psychometric methods in 
these applied sciences by publishing a broad range of reference works, textbooks and 
handbooks. 
The scope of the series is wide, including applications of statistical methodology in  
sociology, psychology, economics, education, marketing research, political science, 
criminology, public policy, demography, survey methodology and official statistics. The 
titles included in the series are designed to appeal to applied statisticians, as well as 
students, researchers and practitioners from the above disciplines. The inclusion of real 
examples and case studies is therefore essential.
Jeff Gill
Washington University, USA
Wim van der Linden
CTB/McGraw-Hill, USA
Steven Heeringa
University of Michigan, USA
J. Scott Long
Indiana University, USA
Series Editors
Chapman & Hall/CRC 
Tom Snijders
Oxford University, UK  
University of Groningen, NL

Published Titles
Analyzing Spatial Models of Choice and Judgment with R 
David A. Armstrong II, Ryan Bakker, Royce Carroll, Christopher Hare, Keith T. Poole, and 
Howard Rosenthal
Analysis of Multivariate Social Science Data, Second Edition 
David J. Bartholomew, Fiona Steele, Irini Moustaki, and Jane I. Galbraith
Latent Markov Models for Longitudinal Data  
Francesco Bartolucci, Alessio Farcomeni, and Fulvia Pennoni
Statistical Test Theory for the Behavioral Sciences 
Dato N. M. de Gruijter and Leo J. Th. van der Kamp
Multivariable Modeling and Multivariate Analysis for the Behavioral Sciences  
Brian S. Everitt 
Multilevel Modeling Using R 
W. Holmes Finch, Jocelyn E. Bolin, and Ken Kelley 
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition 
Jeff Gill
Multiple Correspondence Analysis and Related Methods 
Michael Greenacre and Jorg Blasius
Applied Survey Data Analysis 
Steven G. Heeringa, Brady T. West, and Patricia A. Berglund
Informative Hypotheses: Theory and Practice for Behavioral and Social Scientists  
Herbert Hoijtink
Generalized Structured Component Analysis: A Component-Based Approach to Structural 
Equation Modeling 
Heungsun Hwang and Yoshio Takane
Statistical Studies of Income, Poverty and Inequality in Europe: Computing and Graphics in 
R Using EU-SILC 
Nicholas T. Longford
Foundations of Factor Analysis, Second Edition  
Stanley A. Mulaik
Linear Causal Modeling with Structural Equations 
Stanley A. Mulaik
Age–Period–Cohort Models: Approaches and Analyses with Aggregate Data 
Robert M. O’Brien
Handbook of International Large-Scale Assessment: Background, Technical Issues, and 
Methods of Data Analysis  
Leslie Rutkowski, Matthias von Davier, and David Rutkowski
Generalized Linear Models for Categorical and Continuous Limited Dependent Variables 
Michael Smithson and Edgar C. Merkle
Incomplete Categorical Data Design: Non-Randomized Response Techniques for Sensitive 
Questions in Surveys  
Guo-Liang Tian and Man-Lai Tang
Computerized Multistage Testing: Theory and Applications  
Duanli Yan, Alina A. von Davier, and Charles Lewis


Statistics in the Social and Behavioral Sciences Series
Chapman & Hall/CRC 
Jeff Gill
Washington University 
St. Louis, Missouri, USA
Bayesian 
Methods
A Social and  
Behavioral Sciences  
Approach
Third Edition

CRC Press
Taylor & Francis Group
6000 Broken Sound Parkway NW, Suite 300
Boca Raton, FL 33487-2742
© 2015 by Taylor & Francis Group, LLC
CRC Press is an imprint of Taylor & Francis Group, an Informa business
No claim to original U.S. Government works
Version Date: 20141105
International Standard Book Number-13: 978-1-4398-6249-0 (eBook - PDF)
This book contains information obtained from authentic and highly regarded sources. Reasonable efforts have been 
made to publish reliable data and information, but the author and publisher cannot assume responsibility for the valid-
ity of all materials or the consequences of their use. The authors and publishers have attempted to trace the copyright 
holders of all material reproduced in this publication and apologize to copyright holders if permission to publish in this 
form has not been obtained. If any copyright material has not been acknowledged please write and let us know so we may 
rectify in any future reprint.
Except as permitted under U.S. Copyright Law, no part of this book may be reprinted, reproduced, transmitted, or uti-
lized in any form by any electronic, mechanical, or other means, now known or hereafter invented, including photocopy-
ing, microfilming, and recording, or in any information storage or retrieval system, without written permission from the 
publishers.
For permission to photocopy or use material electronically from this work, please access www.copyright.com (http://
www.copyright.com/) or contact the Copyright Clearance Center, Inc. (CCC), 222 Rosewood Drive, Danvers, MA 01923, 
978-750-8400. CCC is a not-for-profit organization that provides licenses and registration for a variety of users. For 
organizations that have been granted a photocopy license by the CCC, a separate system of payment has been arranged.
Trademark Notice: Product or corporate names may be trademarks or registered trademarks, and are used only for 
identification and explanation without intent to infringe.
Visit the Taylor & Francis Web site at
http://www.taylorandfrancis.com
and the CRC Press Web site at
http://www.crcpress.com

This book is again dedicated to Jack Gill, who is still there when I need him.


Contents
List of Figures
xix
List of Tables
xxi
Preface to the Third Edition
xxv
Preface to the Second Edition
xxix
Preface to the First Edition
xxxvii
1
Background and Introduction
1
1.1
Introduction
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1
1.2
General Motivation and Justiﬁcation
. . . . . . . . . . . . . . . . . . . . .
4
1.3
Why Are We Uncertain about Uncertainty?
. . . . . . . . . . . . . . . . .
7
1.3.1
Required Probability Principles
. . . . . . . . . . . . . . . . . . . .
8
1.4
Bayes’ Law . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
9
1.4.1
Bayes’ Law for Multiple Events . . . . . . . . . . . . . . . . . . . . .
12
1.5
Conditional Inference with Bayes’ Law
. . . . . . . . . . . . . . . . . . . .
15
1.5.1
Statistical Models with Bayes’ Law
. . . . . . . . . . . . . . . . . .
17
1.6
Science and Inference
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
20
1.6.1
The Scientiﬁc Process in Our Social Sciences . . . . . . . . . . . . .
20
1.6.2
Bayesian Statistics as a Scientiﬁc Approach to Social and Behavioral
Data Analysis
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
22
1.7
Introducing Markov Chain Monte Carlo Techniques
. . . . . . . . . . . . .
24
1.7.1
Simple Gibbs Sampling . . . . . . . . . . . . . . . . . . . . . . . . .
25
1.7.2
Simple Metropolis Sampling
. . . . . . . . . . . . . . . . . . . . . .
27
1.8
Historical Comments
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
30
1.9
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
33
2
Specifying Bayesian Models
37
2.1
Purpose
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
37
2.2
Likelihood Theory and Estimation
. . . . . . . . . . . . . . . . . . . . . .
37
2.3
The Basic Bayesian Framework
. . . . . . . . . . . . . . . . . . . . . . . .
40
2.3.1
Developing the Bayesian Inference Engine . . . . . . . . . . . . . . .
40
2.3.2
Summarizing Posterior Distributions with Intervals
. . . . . . . . .
42
ix

x
Contents
2.3.2.1
Bayesian Credible Intervals . . . . . . . . . . . . . . . . . .
43
2.3.2.2
Bayesian Highest Posterior Density Intervals . . . . . . . .
46
2.3.3
Quantile Posterior Summaries
. . . . . . . . . . . . . . . . . . . . .
48
2.3.4
Beta-Binomial Model . . . . . . . . . . . . . . . . . . . . . . . . . .
49
2.4
Bayesian “Learning”
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
53
2.5
Comments on Prior Distributions
. . . . . . . . . . . . . . . . . . . . . . .
56
2.6
Bayesian versus Non-Bayesian Approaches
. . . . . . . . . . . . . . . . . .
57
2.7
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
61
2.8
Computational Addendum: R for Basic Analysis . . . . . . . . . . . . . . .
66
3
The Normal and Student’s-t Models
69
3.1
Why Be Normal?
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
69
3.2
The Normal Model with Variance Known . . . . . . . . . . . . . . . . . . .
70
3.3
The Normal Model with Mean Known
. . . . . . . . . . . . . . . . . . . .
72
3.4
The Normal Model with Both Mean and Variance Unknown
. . . . . . . .
74
3.5
Multivariate Normal Model, μ and Σ Both Unknown
. . . . . . . . . . . .
76
3.6
Simulated Eﬀects of Diﬀering Priors . . . . . . . . . . . . . . . . . . . . . .
80
3.7
Some Normal Comments
. . . . . . . . . . . . . . . . . . . . . . . . . . . .
82
3.8
The Student’s-t Model
. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
83
3.9
Normal Mixture Models
. . . . . . . . . . . . . . . . . . . . . . . . . . . .
87
3.10 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
89
3.11 Computational Addendum: Normal Examples
. . . . . . . . . . . . . . . .
93
3.11.1 Normal Example with Variance Known . . . . . . . . . . . . . . . .
93
3.11.2 Bivariate Normal Simulation Example . . . . . . . . . . . . . . . . .
94
3.11.3 Multivariate Normal Example, Health Data . . . . . . . . . . . . . .
95
4
The Bayesian Prior
97
4.1
A Prior Discussion of Priors
. . . . . . . . . . . . . . . . . . . . . . . . . .
97
4.2
A Plethora of Priors
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
98
4.3
Conjugate Prior Forms
. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
100
4.3.1
Example: Conjugacy in Exponential Speciﬁcations . . . . . . . . . .
100
4.3.2
The Exponential Family Form . . . . . . . . . . . . . . . . . . . . .
101
4.3.3
Limitations of Conjugacy . . . . . . . . . . . . . . . . . . . . . . . .
104
4.4
Uninformative Prior Distributions . . . . . . . . . . . . . . . . . . . . . . .
104
4.4.1
Uniform Priors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
105
4.4.2
Jeﬀreys Prior . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
107
4.4.2.1
Bernoulli Trials and Jeﬀreys Prior . . . . . . . . . . . . . .
108
4.4.2.2
Other Forms of Jeﬀreys Priors . . . . . . . . . . . . . . . .
109
4.4.2.3
Jeﬀreys Prior in the Multiparameter Case
. . . . . . . . .
110
4.4.3
Reference Priors . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
111
4.4.4
Improper Priors . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
113

Contents
xi
4.5
Informative Prior Distributions
. . . . . . . . . . . . . . . . . . . . . . . .
114
4.5.1
Power Priors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
115
4.5.2
Elicited Priors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
116
4.5.2.1
The Community of Elicited Priors . . . . . . . . . . . . . .
118
4.5.2.2
Simple Elicitation Using Linear Regression . . . . . . . . .
119
4.5.2.3
Variance Components Elicitation
. . . . . . . . . . . . . .
121
4.5.2.4
Predictive Modal Elicitation . . . . . . . . . . . . . . . . .
123
4.5.2.5
Prior Elicitation for the Normal-Linear Model . . . . . . .
126
4.5.2.6
Elicitation Using a Beta Distribution . . . . . . . . . . . .
127
4.5.2.7
Eliciting Some Final Thoughts on Elicited Priors
. . . . .
128
4.6
Hybrid Prior Forms
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
129
4.6.1
Spike and Slab Priors for Linear Models . . . . . . . . . . . . . . . .
130
4.6.2
Maximum Entropy Priors . . . . . . . . . . . . . . . . . . . . . . . .
131
4.6.3
Histogram Priors . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
133
4.7
Nonparametric Priors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
133
4.8
Bayesian Shrinkage
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
136
4.9
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
138
5
The Bayesian Linear Model
145
5.1
The Basic Regression Model
. . . . . . . . . . . . . . . . . . . . . . . . . .
145
5.1.1
Uninformative Priors for the Linear Model
. . . . . . . . . . . . . .
147
5.1.2
Conjugate Priors for the Linear Model . . . . . . . . . . . . . . . . .
151
5.1.3
Conjugate Caveats for the Cautious and Careful . . . . . . . . . . .
154
5.2
Posterior Predictive Distribution for the Data
. . . . . . . . . . . . . . . .
155
5.3
Linear Regression with Heteroscedasticity
. . . . . . . . . . . . . . . . . .
161
5.4
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
165
5.5
Computational Addendum
. . . . . . . . . . . . . . . . . . . . . . . . . . .
169
5.5.1
Palm Beach County Normal Model
. . . . . . . . . . . . . . . . . .
169
5.5.2
Educational Outcomes Model . . . . . . . . . . . . . . . . . . . . . .
171
5.5.3
Ancient China Conﬂict Model
. . . . . . . . . . . . . . . . . . . . .
172
6
Assessing Model Quality
175
6.1
Motivation
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
175
6.1.1
Posterior Data Replication . . . . . . . . . . . . . . . . . . . . . . .
177
6.1.2
Likelihood Function Robustness
. . . . . . . . . . . . . . . . . . . .
180
6.2
Basic Sensitivity Analysis
. . . . . . . . . . . . . . . . . . . . . . . . . . .
181
6.2.1
Global Sensitivity Analysis . . . . . . . . . . . . . . . . . . . . . . .
181
6.2.1.1
Speciﬁc Cases of Global Prior Sensitivity Analysis . . . . .
182
6.2.1.2
Global Sensitivity in the Normal Model Case . . . . . . . .
182
6.2.1.3
Example: Prior Sensitivity in the Analysis of the 2000 U.S.
Election in Palm Beach County . . . . . . . . . . . . . . .
183

xii
Contents
6.2.1.4
Problems with Global Sensitivity Analysis . . . . . . . . .
183
6.2.2
Local Sensitivity Analysis . . . . . . . . . . . . . . . . . . . . . . . .
184
6.2.2.1
Normal-Normal Model
. . . . . . . . . . . . . . . . . . . .
185
6.2.2.2
Local Sensitivity Analysis Using Hyperparameter Changes
186
6.2.3
Global and Local Sensitivity Analysis with Recidivism Data
. . . .
187
6.3
Robustness Evaluation
. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
189
6.3.1
Global Robustness . . . . . . . . . . . . . . . . . . . . . . . . . . . .
190
6.3.2
Local Robustness
. . . . . . . . . . . . . . . . . . . . . . . . . . . .
192
6.3.2.1
Bayesian Linear Outlier Detection . . . . . . . . . . . . . .
193
6.3.3
Bayesian Speciﬁcation Robustness . . . . . . . . . . . . . . . . . . .
195
6.4
Comparing Data to the Posterior Predictive Distribution
. . . . . . . . . .
196
6.5
Simple Bayesian Model Averaging . . . . . . . . . . . . . . . . . . . . . . .
198
6.6
Concluding Comments on Model Quality
. . . . . . . . . . . . . . . . . . .
200
6.7
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
202
7
Bayesian Hypothesis Testing and the Bayes Factor
207
7.1
Motivation
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
207
7.2
Bayesian Inference and Hypothesis Testing
. . . . . . . . . . . . . . . . . .
209
7.2.1
Problems with Conventional Hypothesis Testing . . . . . . . . . . .
209
7.2.1.1
One-Sided Testing . . . . . . . . . . . . . . . . . . . . . . .
211
7.2.1.2
Two-Sided Testing
. . . . . . . . . . . . . . . . . . . . . .
214
7.2.2
Attempting a Bayesian Approximation to Frequentist Hypothesis
Testing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
215
7.3
The Bayes Factor as Evidence
. . . . . . . . . . . . . . . . . . . . . . . . .
216
7.3.1
Bayes Factors for a Mean . . . . . . . . . . . . . . . . . . . . . . . .
218
7.3.2
Bayes Factors for Diﬀerence of Means Test . . . . . . . . . . . . . .
219
7.3.3
Bayes Factor for the Linear Regression Model
. . . . . . . . . . . .
219
7.3.4
Bayes Factors and Improper Priors . . . . . . . . . . . . . . . . . . .
223
7.3.4.1
Local Bayes Factor
. . . . . . . . . . . . . . . . . . . . . .
224
7.3.4.2
Intrinsic Bayes Factor . . . . . . . . . . . . . . . . . . . . .
225
7.3.4.3
Partial Bayes Factor
. . . . . . . . . . . . . . . . . . . . .
226
7.3.4.4
Fractional Bayes Factor . . . . . . . . . . . . . . . . . . . .
226
7.3.4.5
Redux
. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
227
7.3.5
Two-Sided Hypothesis Tests and Bayes Factors . . . . . . . . . . . .
228
7.3.6
Challenging Aspects of Bayes Factors . . . . . . . . . . . . . . . . .
229
7.4
The Bayesian Information Criterion (BIC)
. . . . . . . . . . . . . . . . . .
231
7.5
The Deviance Information Criterion (DIC)
. . . . . . . . . . . . . . . . . .
233
7.5.1
Some Qualiﬁcations . . . . . . . . . . . . . . . . . . . . . . . . . . .
236
7.6
Comparing Posterior Distributions with the Kullback-Leibler Distance
. .
237
7.7
Laplace Approximation of Bayesian Posterior Densities
. . . . . . . . . . .
239
7.8
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
243

Contents
xiii
8
Bayesian Decision Theory
247
8.1
Introducing Decision Theory . . . . . . . . . . . . . . . . . . . . . . . . . .
247
8.2
Basic Deﬁnitions
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
249
8.2.1
Personal Preference . . . . . . . . . . . . . . . . . . . . . . . . . . .
250
8.2.2
Rules, Rules, Rules
. . . . . . . . . . . . . . . . . . . . . . . . . . .
250
8.2.3
Lots of Loss
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
251
8.2.4
Risky Business . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
253
8.2.4.1
Notes on Bayes Rules . . . . . . . . . . . . . . . . . . . . .
256
8.2.5
Minimax Decision Rules . . . . . . . . . . . . . . . . . . . . . . . . .
258
8.3
Regression-Style Models with Decision Theory . . . . . . . . . . . . . . . .
259
8.3.1
Prediction from the Linear Model
. . . . . . . . . . . . . . . . . . .
261
8.4
James-Stein Estimation . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
262
8.5
Empirical Bayes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
267
8.6
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
271
9
Monte Carlo and Related Iterative Methods
275
9.1
Background
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
275
9.2
Basic Monte Carlo Integration . . . . . . . . . . . . . . . . . . . . . . . . .
277
9.3
Rejection Sampling
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
280
9.3.1
Continuous Form with Bounded Support . . . . . . . . . . . . . . .
281
9.3.2
Continuous Form with Unbounded Support . . . . . . . . . . . . . .
284
9.4
Classical Numerical Integration
. . . . . . . . . . . . . . . . . . . . . . . .
288
9.4.1
Newton-Cotes
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
289
9.4.1.1
Riemann Integrals . . . . . . . . . . . . . . . . . . . . . . .
289
9.4.1.2
Trapezoid Rule
. . . . . . . . . . . . . . . . . . . . . . . .
289
9.4.1.3
Simpson’s Rule
. . . . . . . . . . . . . . . . . . . . . . . .
290
9.5
Gaussian Quadrature
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
292
9.5.1
Redux . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
295
9.6
Importance Sampling and Sampling Importance Resampling
. . . . . . . .
296
9.6.1
Importance Sampling for Producing HPD Regions . . . . . . . . . .
301
9.7
Mode Finding and the EM Algorithm . . . . . . . . . . . . . . . . . . . . .
302
9.7.1
Deriving the EM Algorithm . . . . . . . . . . . . . . . . . . . . . . .
304
9.7.2
Convergence of the EM Algorithm . . . . . . . . . . . . . . . . . . .
307
9.7.3
Extensions to the EM Algorithm . . . . . . . . . . . . . . . . . . . .
313
9.7.4
Additional Comments on EM . . . . . . . . . . . . . . . . . . . . . .
315
9.7.5
EM for Exponential Families . . . . . . . . . . . . . . . . . . . . . .
316
9.8
Survey of Random Number Generation
. . . . . . . . . . . . . . . . . . . .
320
9.9
Concluding Remarks
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
322
9.10 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
323
9.11 Computational Addendum: R Code for Importance Sampling
. . . . . . .
328

xiv
Contents
10 Basics of Markov Chain Monte Carlo
333
10.1 Who Is Markov and What Is He Doing with Chains?
. . . . . . . . . . . .
333
10.1.1 What Is a Markov Chain? . . . . . . . . . . . . . . . . . . . . . . . .
334
10.1.2 A Markov Chain Illustration . . . . . . . . . . . . . . . . . . . . . .
335
10.1.3 The Chapman-Kolmogorov Equations . . . . . . . . . . . . . . . . .
338
10.1.4 Marginal Distributions
. . . . . . . . . . . . . . . . . . . . . . . . .
339
10.2 General Properties of Markov Chains
. . . . . . . . . . . . . . . . . . . . .
339
10.2.1 Homogeneity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
340
10.2.2 Irreducibility . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
340
10.2.3 Recurrence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
340
10.2.4 Stationarity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
341
10.2.5 Ergodicity
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
342
10.3 The Gibbs Sampler
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
343
10.3.1 Description of the Algorithm . . . . . . . . . . . . . . . . . . . . . .
343
10.3.2 Handling Missing Dichotomous Data with the Gibbs Sampler . . . .
345
10.3.3 Summary of Properties of the Gibbs Sampler . . . . . . . . . . . . .
353
10.4 The Metropolis-Hastings Algorithm
. . . . . . . . . . . . . . . . . . . . . .
353
10.4.1 Background . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
354
10.4.2 Description of the Algorithm . . . . . . . . . . . . . . . . . . . . . .
354
10.4.3 Metropolis-Hastings Properties . . . . . . . . . . . . . . . . . . . . .
356
10.4.4 Metropolis-Hastings Derivation . . . . . . . . . . . . . . . . . . . . .
356
10.4.5 The Transition Kernel . . . . . . . . . . . . . . . . . . . . . . . . . .
358
10.4.6 Example: Estimating a Bivariate Normal Density
. . . . . . . . . .
359
10.5 The Hit-and-Run Algorithm
. . . . . . . . . . . . . . . . . . . . . . . . . .
360
10.6 The Data Augmentation Algorithm
. . . . . . . . . . . . . . . . . . . . . .
362
10.7 Historical Comments
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
367
10.7.1 Full Circle? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
368
10.8 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
368
10.9 Computational Addendum: Simple R Graphing Routines for MCMC
. . .
375
11 Implementing Bayesian Models with Markov Chain Monte Carlo
377
11.1 Introduction to Bayesian Software Solutions
. . . . . . . . . . . . . . . . .
377
11.2 It’s Only a Name: BUGS . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
378
11.3 Model Speciﬁcation with BUGS
. . . . . . . . . . . . . . . . . . . . . . . . .
380
11.3.1 Model Speciﬁcation
. . . . . . . . . . . . . . . . . . . . . . . . . . .
383
11.3.2 Running the Model in WinBUGS . . . . . . . . . . . . . . . . . . . . .
385
11.3.3 Running the Model in JAGS . . . . . . . . . . . . . . . . . . . . . . .
388
11.4 Diﬀerences between WinBUGS and JAGS Code . . . . . . . . . . . . . . . . .
392
11.5 Technical Background about the Algorithm
. . . . . . . . . . . . . . . . .
401
11.6 Epilogue
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
408
11.7 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
408

Contents
xv
12 Bayesian Hierarchical Models
413
12.1 Introduction to Multilevel Speciﬁcations
. . . . . . . . . . . . . . . . . . .
413
12.2 Basic Multilevel Linear Models
. . . . . . . . . . . . . . . . . . . . . . . .
414
12.3 Comparing Variances
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
416
12.4 Exchangeability
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
420
12.5 Essential Structure of the Bayesian Hierarchical Model
. . . . . . . . . . .
425
12.5.1 A Poisson-Gamma Hierarchical Speciﬁcation . . . . . . . . . . . . .
427
12.6 The General Role of Priors and Hyperpriors
. . . . . . . . . . . . . . . . .
434
12.7 Bayesian Multilevel Linear Regression Models
. . . . . . . . . . . . . . . .
436
12.7.1 The Bayesian Hierarchical Linear Model of Lindley and Smith . . .
436
12.8 Bayesian Multilevel Generalized Linear Regression Models
. . . . . . . . .
441
12.9 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
446
12.10 Computational Addendum
. . . . . . . . . . . . . . . . . . . . . . . . . . .
451
12.10.1R Function for importing BUGS output . . . . . . . . . . . . . . . . .
451
13 Some Markov Chain Monte Carlo Theory
453
13.1 Motivation
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
453
13.2 Measure and Probability Preliminaries
. . . . . . . . . . . . . . . . . . . .
453
13.3 Speciﬁc Markov Chain Properties
. . . . . . . . . . . . . . . . . . . . . . .
455
13.3.1 ψ-Irreducibility . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
455
13.3.2 Closed and Absorbing Sets . . . . . . . . . . . . . . . . . . . . . . .
455
13.3.3 Homogeneity and Periodicity . . . . . . . . . . . . . . . . . . . . . .
455
13.3.4 Null and Positive Recurrence . . . . . . . . . . . . . . . . . . . . . .
456
13.3.5 Transience
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
456
13.3.6 Markov Chain Stability . . . . . . . . . . . . . . . . . . . . . . . . .
457
13.3.7 Ergodicity
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
458
13.4 Deﬁning and Reaching Convergence . . . . . . . . . . . . . . . . . . . . . .
458
13.5 Rates of Convergence
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
460
13.6 Implementation Concerns
. . . . . . . . . . . . . . . . . . . . . . . . . . .
464
13.6.1 Mixing
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
466
13.6.2 Partial Convergence for Metropolis-Hastings
. . . . . . . . . . . . .
467
13.6.3 Partial Convergence for the Gibbs Sampler . . . . . . . . . . . . . .
469
13.7 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
471
14 Utilitarian Markov Chain Monte Carlo
475
14.1 Objectives
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
475
14.2 Practical Considerations and Admonitions
. . . . . . . . . . . . . . . . . .
476
14.2.1 Starting Points . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
476
14.2.2 Thinning the Chain . . . . . . . . . . . . . . . . . . . . . . . . . . .
477
14.2.3 The Burn-In Period . . . . . . . . . . . . . . . . . . . . . . . . . . .
478
14.3 Assessing Convergence of Markov Chains . . . . . . . . . . . . . . . . . . .
479

xvi
Contents
14.3.1 Autocorrelation
. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
485
14.3.2 Graphical Diagnostics . . . . . . . . . . . . . . . . . . . . . . . . . .
487
14.3.3 Standard Empirical Diagnostics
. . . . . . . . . . . . . . . . . . . .
493
14.3.3.1 The Geweke Time-Series Diagnostic . . . . . . . . . . . . .
494
14.3.3.2 Gelman and Rubin’s Multiple Sequence Diagnostic
. . . .
496
14.3.3.3 The Heidelberger and Welch Diagnostic . . . . . . . . . . .
499
14.3.3.4 The Raftery and Lewis Integrated Diagnostic
. . . . . . .
503
14.3.4 Summary of Diagnostic Similarities and Diﬀerences
. . . . . . . . .
505
14.3.5 Other Empirical Diagnostics
. . . . . . . . . . . . . . . . . . . . . .
507
14.3.6 Why Not to Worry Too Much about Stationarity . . . . . . . . . . .
509
14.4 Mixing and Acceleration
. . . . . . . . . . . . . . . . . . . . . . . . . . . .
510
14.4.1 Reparameterization . . . . . . . . . . . . . . . . . . . . . . . . . . .
510
14.4.2 Grouping and Collapsing the Gibbs Sampler
. . . . . . . . . . . . .
512
14.4.3 Adding Auxiliary Variables . . . . . . . . . . . . . . . . . . . . . . .
513
14.4.4 The Slice Sampler . . . . . . . . . . . . . . . . . . . . . . . . . . . .
513
14.5 Chib’s Method for Calculating the Marginal Likelihood Integral
. . . . . .
515
14.6 Rao-Blackwellizing for Improved Variance Estimation . . . . . . . . . . . .
517
14.7 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
520
14.8 Computational Addendum: Code for Chapter Examples
. . . . . . . . . .
523
14.8.1 R Code for the Death Penalty Support Model . . . . . . . . . . . . .
523
14.8.2 JAGS Code for the Military Personnel Model
. . . . . . . . . . . . .
524
15 Markov Chain Monte Carlo Extensions
525
15.1 Simulated Annealing
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
525
15.1.1 General Points on Simulated Annealing . . . . . . . . . . . . . . . .
529
15.1.2 Metropolis-Coupling . . . . . . . . . . . . . . . . . . . . . . . . . . .
530
15.1.3 Simulated Tempering
. . . . . . . . . . . . . . . . . . . . . . . . . .
531
15.1.4 Tempored Transitions . . . . . . . . . . . . . . . . . . . . . . . . . .
532
15.1.5 Comparison of Algorithms
. . . . . . . . . . . . . . . . . . . . . . .
533
15.1.6 Dynamic Tempered Transitions . . . . . . . . . . . . . . . . . . . . .
535
15.2 Reversible Jump Algorithms
. . . . . . . . . . . . . . . . . . . . . . . . . .
536
15.3 Perfect Sampling
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
538
15.4 Hamiltonian Monte Carlo
. . . . . . . . . . . . . . . . . . . . . . . . . . .
542
15.5 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
547
Appendix A
Generalized Linear Model Review
553
A.1
Terms
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
553
A.1.1
The Linear Regression Model . . . . . . . . . . . . . . . . . . . . . .
555
A.2
The Generalized Linear Model . . . . . . . . . . . . . . . . . . . . . . . . .
557
A.2.1
Deﬁning the Link Function . . . . . . . . . . . . . . . . . . . . . . .
558
A.2.2
Deviance Residuals
. . . . . . . . . . . . . . . . . . . . . . . . . . .
560

Contents
xvii
A.3
Numerical Maximum Likelihood . . . . . . . . . . . . . . . . . . . . . . . .
564
A.3.1
Newton-Raphson and Root Finding . . . . . . . . . . . . . . . . . .
564
A.3.1.1
Newton-Raphson for Statistical Problems . . . . . . . . . .
566
A.3.1.2
Weighted Least Squares . . . . . . . . . . . . . . . . . . . .
566
A.3.1.3
Iterative Weighted Least Squares
. . . . . . . . . . . . . .
567
A.4
Quasi-Likelihood
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
568
A.5
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
572
Appendix B
Common Probability Distributions
579
References
583
Author Index . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
643
Subject Index
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
663


List of Figures
1.1
Gibbs Sampling, Marginal Exponentials . . . . . . . . . . . . . . . .
27
1.2
Metropolis Sampling, Bivariate Exponential . . . . . . . . . . . . .
31
2.1
Posterior ∝Prior × Likelihood . . . . . . . . . . . . . . . . . . . . .
41
2.2
CI for State Duration Time to Adaptation, 1998-2005 . . . . . . .
44
2.3
Bimodal Distribution Highest Posterior Density Interval . . . .
46
2.4
HPD Region, State Duration Time to Adaptation, 1998-2005
. .
48
2.5
Prior and Posterior Distributions in the Beta-Binomial Model
52
3.1
A Menagerie of Inverse Gamma Forms . . . . . . . . . . . . . . . . .
73
3.2
Posterior Distribution versus Likelihood Function . . . . . . . .
80
4.1
Prior from Transformation . . . . . . . . . . . . . . . . . . . . . . . .
106
4.2
Binomial: Jeffreys Prior . . . . . . . . . . . . . . . . . . . . . . . . . .
109
4.3
Finding the Beta Parameters . . . . . . . . . . . . . . . . . . . . . . .
125
4.4
t Ratios . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
126
4.5
Spike and Slab Prior
. . . . . . . . . . . . . . . . . . . . . . . . . . . .
130
5.1
MCMC Running Means, Chinese Conflicts
. . . . . . . . . . . . . .
164
6.1
Outcome Comparison: Observed versus Simulated
. . . . . . . . .
180
6.2
Marginal Posteriors, Palm Beach County Model
. . . . . . . . .
184
6.3
Exponential Model for θ with Gamma Prior . . . . . . . . . . . . .
186
6.4
Exponential Model Sensitivity . . . . . . . . . . . . . . . . . . . . . .
187
6.5
Comparison of Posteriors, Recidivism Model . . . . . . . . . . . . .
190
6.6
Posterior Predictive Distribution Draws, Africa Growth . . . .
199
7.1
One-Sided Testing for the French Strikes Data
. . . . . . . . . .
214
8.1
Empirical Bayes Estimates versus Data Values
. . . . . . . . . . .
270
9.1
Rejection Sampling for Bounded Forms . . . . . . . . . . . . . . . .
283
9.2
Rejection Sampling for Unbounded Forms
. . . . . . . . . . . . . .
285
9.3
Rejection Sampling, Optimal Coverage . . . . . . . . . . . . . . . .
286
9.4
Folded Normal Rejection Sampling Example . . . . . . . . . . . . .
288
xix

xx
List of Figures
9.5
Illustration of Numerical Integration
. . . . . . . . . . . . . . . .
292
9.6
Contourplot Matrix, Rural Poverty in Texas
. . . . . . . . . . .
300
9.7
EM Demonstration: U.S. Energy Consumption . . . . . . . . . . . .
313
10.1
Gibbs Sampling Demonstration . . . . . . . . . . . . . . . . . . . . . .
346
10.2
Gibbs Sampling Path for Terrorism Events . . . . . . . . . . . . . .
351
10.3
Afghan Fatalities Posterior Histogram, HPD Region . . . . . . .
352
10.4
Metropolis-Hastings, Bivariate Normal Simulation
. . . . . . . .
360
10.5
Hit-and-Run Algorithm Demonstration . . . . . . . . . . . . . . . .
362
11.1
OECD Employment and Productivity Data . . . . . . . . . . . . . .
382
11.2
Adaptive Rejection Sampling Step
. . . . . . . . . . . . . . . . . . .
403
11.3
Adaptive Rejection Sampling Squeezing . . . . . . . . . . . . . . . .
404
12.1
Poisson-Gamma Hierarchical Model
. . . . . . . . . . . . . . . . . .
428
12.2
Traceplot of Output for Gamma . . . . . . . . . . . . . . . . . . . .
430
12.3
Smoothed Posteriors for α and β . . . . . . . . . . . . . . . . . . . .
433
12.4
Lambda Posterior Summary, Italy Model . . . . . . . . . . . . . . .
434
12.5
Running Means, Italy Model
. . . . . . . . . . . . . . . . . . . . . . .
435
12.6
Correlation Distributions from W(Σ|α, β)
. . . . . . . . . . . . . .
439
14.1
Traceplot and Histogram Graphics, Tobit Model . . . . . . . . .
489
14.2
Traceplot and Histogram Graphics, Military Personnel Model
490
14.3
Traceplots of the Same Chain with Different Spans
. . . . . . .
491
14.4
Running Mean Diagnostic, Tobit Model . . . . . . . . . . . . . . . .
492
14.5
Running Mean Diagnostic, Military Personnel Model . . . . . . .
493
14.6
Traceplot and Histogram, Last 2,000 Iterations . . . . . . . . . .
502
14.7
A Comparison of Gibbs Sampling for Normals . . . . . . . . . . . .
511
14.8
Slice Sampler Output . . . . . . . . . . . . . . . . . . . . . . . . . . . .
515
15.1
Simulated Annealing Applied to the Witch’s Hat Distribution .
531
15.2
A Highly Multimodal Bounded Surface . . . . . . . . . . . . . . . .
534
15.3
A Comparison of Travels
. . . . . . . . . . . . . . . . . . . . . . . . .
535
15.4
Samples from Hamiltonian Monte Carlo
. . . . . . . . . . . . . . .
547

List of Tables
2.1
State Duration Time to Adaptation, 1998-2005
. . . . . . . . . . .
45
2.2
HPD Regions: Predicted 1982 Vote Percentages for Five Major
Parties
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
60
3.1
Posterior Summaries, Normal Models . . . . . . . . . . . . . . . . .
79
3.2
A Bivariate Normal, 1000 Simulations . . . . . . . . . . . . . . . . .
81
3.3
Another Bivariate Normal, 1000 Simulations
. . . . . . . . . . . .
82
3.4
Posterior Summary: National IQ Scores
. . . . . . . . . . . . . . .
86
4.1
Some Exponential Family Forms and Their Conjugate Priors . .
103
4.2
Some Jeffreys Priors for Common Forms . . . . . . . . . . . . . . .
110
5.1
PBC Data Variable Definitions
. . . . . . . . . . . . . . . . . . . . .
149
5.2
Posterior: Linear Model, Uninformative Priors
. . . . . . . . . .
150
5.3
Linear Regression Model, Prior Comparison . . . . . . . . . . . . .
153
5.4
Posterior: Linear Model, Conjugate Priors . . . . . . . . . . . . .
155
5.5
Posterior: Meier Replication Model . . . . . . . . . . . . . . . . . .
159
5.6
Posterior: Interaction Model . . . . . . . . . . . . . . . . . . . . . .
160
5.7
Heteroscedastic Model, Ancient Chinese Conflicts . . . . . . . .
165
6.1
Model Summary, Abortion Attitudes in Britain . . . . . . . . . . .
179
6.2
Recidivism by Crime, Oklahoma, 1/1/85 to 6/30/99 . . . . . . . . .
188
6.3
λ Posterior Summary, Recidivism Models . . . . . . . . . . . . . . .
189
6.4
Oklahoma Recidivism 95% HPD Regions, 1/1/85 to 6/30/99
. . .
192
6.5
Sub-Saharan Africa Average Economic Growth Rate, 1990-1994
198
7.1
French Coal Strikes, by Year . . . . . . . . . . . . . . . . . . . . . .
213
7.2
1964 Electoral Data
. . . . . . . . . . . . . . . . . . . . . . . . . . . .
221
7.3
1964 Election Survey, Grouped Data . . . . . . . . . . . . . . . . . .
222
7.4
Available Model Specifications, Thai Migration
. . . . . . . . . .
235
7.5
Model Comparison, Thai Migration . . . . . . . . . . . . . . . . . . .
236
7.6
Martikainen et al. (2005) Mortality Parameters . . . . . . . . . . .
238
7.7
Martikainen et al. (2005) Mortality Differences . . . . . . . . . . .
239
8.1
Prediction Risk: Education Outcomes Model
. . . . . . . . . . . .
262
xxi

xxii
List of Tables
8.2
MLE and James-Stein Estimates of Social Spending . . . . . . . .
266
9.1
1990-1993 W.Europe Ethnic/Minority Populations . . . . . . . . .
280
9.2
Monte Carlo Quantiles for Pareto Parameter α
. . . . . . . . .
281
9.3
Logit Regression Model: Poverty in Texas . . . . . . . . . . . . . .
299
9.4
EM Output: U.S. Energy Consumption . . . . . . . . . . . . . . . . .
311
9.5
Model Results, More Developed Areas . . . . . . . . . . . . . . . .
320
10.1
Count of Terrorism Incidents in Britain, 1970-2004 by Rows
. .
347
10.2
Gibbs Sampler Draws for k Parameter . . . . . . . . . . . . . . . . .
349
10.3
Terrorism in Great Britain Model, Posterior Summary
. . . . .
350
10.4
NATO Fatalities in Afghanistan, 10/01 to 1/07 . . . . . . . . . . .
350
10.5
Afghan Fatalities Posterior
. . . . . . . . . . . . . . . . . . . . . . .
352
11.1
OECD Protection versus Productivity . . . . . . . . . . . . . . . .
381
11.2
OECD Model Results . . . . . . . . . . . . . . . . . . . . . . . . . . . .
389
11.3
Lobbying Influence Results . . . . . . . . . . . . . . . . . . . . . . . .
399
11.4
Presidential Election Model Results
. . . . . . . . . . . . . . . . .
407
12.1
Depression Treatment by Clinic, Control Group . . . . . . . . . .
424
12.2
Italian Marriage Rates per 1,000, 1936-1951 . . . . . . . . . . . . .
430
12.3
Posterior Summary Statistics, Italy Model . . . . . . . . . . . . . .
432
12.4
Posterior Summary Statistics, Economic Indicators Model
. . .
441
12.5
Posterior Summary Statistics, Drug Use by Arrestees Model
.
444
12.6
Contraception Data, Developing Countries (by Size) . . . . . . .
446
12.7
Racial Composition of U.S. Federal Agencies (1998) . . . . . . . .
450
14.1
Posterior, Tobit Model
. . . . . . . . . . . . . . . . . . . . . . . . . .
482
14.2
Proportional Changes in Eastern European Militaries, 1948-
1983
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
483
14.3
Posterior, Military Personnel Model . . . . . . . . . . . . . . . . .
485
14.4
Correlations and Autocorrelations, Tobit Model . . . . . . . . .
487
14.5
Correlations and Autocorrelations, Military Personnel Model
488
14.6
Geweke Diagnostic, Tobit Model . . . . . . . . . . . . . . . . . . . .
495
14.7
Geweke Diagnostic, Military Personnel Model . . . . . . . . . . .
495
14.8
Gelman and Rubin Diagnostic, Tobit Model . . . . . . . . . . . . .
498
14.9
Gelman and Rubin Diagnostic, Military Personnel Model . . . .
499
14.10 Diagnostic Scorecard, Tobit Model
. . . . . . . . . . . . . . . . . .
506
14.11 Diagnostic Scorecard, Military Personnel Model . . . . . . . . .
507
A.1
Natural Link Functions for Common Specifications . . . . . . . .
559
A.2
Summed Deviances for Common Specifications . . . . . . . . . . . .
561
A.3
Poisson Model of Military Coups . . . . . . . . . . . . . . . . . . . .
563

List of Tables
xxiii
A.4
Normalizing Constants and Variance Functions . . . . . . . . . . .
570


Preface to the Third Edition
General Comments
Welcome to the third edition of BMSBSA. When the ﬁrst edition appeared in 2002
Bayesian methods were still considered a bit exotic in the social sciences. Many distrusted
the use of prior distributions and some mysterious simulation process that involved non-iid
sampling and uncertainty about convergence. The world is completely diﬀerent now, and
Bayesian modeling has become standard and MCMC is well-understood and trusted. Of
course it helps that Moore’s Law (doubling of computing power every two years, presumably
until we reach the 7 nanometer threshold) continues without pause making our computers
notably faster and allowing longer sampling procedures and parallel process without an
agonizingly long wait. In this context the third edition spends less time justifying proce-
dures and more time providing implementation details on these procedures. This is also an
opportunity to expand the set of examples.
Changes from the Second Edition
As expected there are a number of additions in the new edition. First, and most labo-
riously, the number of exercises has been doubled such that there are now twenty in each
chapter. The former exercises are now the odd-numbered exercises with the answer key be-
ing fully publicly distributed (not just to instructors). The new exercises emphasize recent
developments in Bayesian inference and Bayesian computing as a way to include more mod-
ern material. All of the chapters have been refreshed, although some more than others. The
basic material has not changed in over a century, so there is no need to dramatically alter ba-
sic material. Conversely, Bayesian stochastic simulation (MCMC) has undergone dramatic
developments in the last decade, including having become routine in applied settings. New
MCMC material includes Hamiltonian Monte Carlo and expanded model implementation.
Second, there are two new chapters. A chapter on Bayesian decision theory is long overdue,
and this is now Chapter 8. It includes discussion of both Bayesian and frequentist decision
theory since this is where the two paradigms are most intertwined. Included topics are: loss
xxv

xxvi
Preface to the Third Edition
functions, risk, decision rules, and regression-model applications. The chapter ﬁnishes with
two important topics not featured in previous editions: James-Stein estimation and empir-
ical Bayes. While empirical Bayes was brieﬂy covered in the second edition, its connection
with James-Stein estimation was ignored. This section now covers the important topics in
this area and provides Bayesian context. Also new is a chapter on practical implementation
of MCMC methods (Chapter 11). This covers mechanical issues with the BUGS language,
including calling the software from R. The goal is to provide a detailed introduction to the
essential software for running Bayesian hierarchical regression models. Relatedly the chap-
ter on hierarchical models is greatly expanded. This is an area of great applied interest
right now and provides a strong motivation for the Bayesian paradigm. Finally, on a more
practical side, there is a wealth of new examples and applications. These are chosen from
a variety of social science disciplines and are intended to illustrate the key principles of the
relevant chapter. In addition, the BaM package in R that accompanies this manuscript has
been greatly expanded with new datasets and new code. This includes new procedures for
calling BUGS packages from R.
Course Plans
The recommended course plans remain essentially the same as outlined in the preface to
the second edition. The one critical diﬀerence is adding Chapter 11 (Implementing Bayesian
Models with Markov Chain Monte Carlo) to a basic course or comprehensive course. The
longer length of the text means that not all chapters are practical in a one-semester course.
For a standard introductory Bayesian social science graduate course, the most succinct set
of chapters are:
▷Chapter 1: Background and Introduction
▷Chapter 2: Specifying Bayesian Models
▷Chapter 3: The Normal and Students’-t Models
▷Chapter 4: The Bayesian Prior
▷Chapter 5: The Bayesian Linear Model
▷Chapter 10: Basics of Markov Chain Monte Carlo
▷Chapter 11: Implementing Bayesian Models with Markov Chain Monte Carlo
▷Chapter 12: Bayesian Hierarchical Models
▷Chapter 14: Utilitarian Markov Chain Monte Carlo.
This assumes some knowledge of basic Monte Carlo methods of the students. Chapter 11
and Chapter 14 could also be assigned reading rather than part of lectures since the focus
is on very practical concerns.

Preface to the Third Edition
xxvii
Support
As done in the last two editions of this text, there is a dedicated website provided
to support readers: http://stats.wustl.edu/BMSBSA3. This site has software, errata,
comments, and the answer key for odd-numbered exercises. All of the code is also provided
in the associated R package, BaM, which has been substantially updated to include new
code and data. Where possible BUGS code is included in this package. Note that in many
cases the code relies on multiple R packages, as well as stand-alone software such as JAGS
and WinBUGS, so changes over time may introduce incompatibilities that need to be worked
out. In many cases this amounts to downloading the most recent version of some software.
Relevant updates will be posted at the dedicated webpage when they come to my attention.
Acknowledgments
Since this edition has been seven years in the making, there are many people to thank.
First, I appreciate the support from the Department of Political Science, the Division of
Biostatistics, and the Department of Surgery (Public Health Sciences) at Washington Uni-
versity for being supportive home environments. This includes support from the Transdis-
ciplinary Research on Energetics and Cancer (TREC) grant and Graham Colditz and Sarah
Gehlert. I also thank the Summer School in Social Science Data Analysis at The University
of Essex leadership, Thomas Pl¨umper and Vera Troeger, since some of the datasets and
software were developed in the process of teaching there. In particular, new ideas for the
chapter on hierarchical models were created in the welcoming community that is Essex.
The manuscript was ﬁnished while on sabbatical Spring 2014 at the University of Min-
nesota Division of Biostatistics (with additional ﬁnancial support from the Departments
of Political Science, Psychology, Sociology, and Statistics). I thank Brad Carlin, Sudipto
Banerjee, John Freeman, and Niels Waller for making this happen and providing a pleas-
ant (but really cold!) environment to wrap up this project. Finally, I acknowledge the
support of NSF grants: DMS-0631632 and SES-0631588, which are my last in cooperation
with George Casella. Numerous people have sent useful comments, notably Gary McDon-
ald, Kentaro Fukumoto, Ben Begozzi, Patrick Brandt, Yuta Kamahara, Bruce Desmarais,
Skyler Cranmer, Ryan Bakker, and Gary in particular was relentless in his comments (in
a good way!). In the Spring of 2011 twelve graduate students at Washington University
participated in a weekly reading group around the manuscript giving useful and insightful
comments on methods, computing, and even the writing. I thank Peter Casey, Adriana
Crespo-Tenorio, Constanza Figueroa Schibber, Morgan Hazelton, Rachael Hinkley, Chia-yi
Lee, Michael Nelson, Christopher Pope, Elizabeth Rose, and Alicia Uribe. All of these stu-

xxviii
Preface to the Third Edition
dents have ﬁnished their studies and gone on to successful careers. This is the ﬁrst edition
of this book after George Casella left us. He made a big impact on the ﬁrst two issues as my
neighbor, close friend, colleague, and coauthor. As Christian Robert states, George simply
made everyone’s work better for those fortunate to be around him. This work is certainly
better because of George’s inﬂuence.

Preface to the Second Edition
Starters
Wow, over ﬁve years have elapsed since the ﬁrst edition appeared. Bayesian methods in
the social sciences have grown and changed dramatically. This is a positive and encouraging
development. When I was writing the ﬁrst version I would often get questions from social
science colleagues about why I would write on a seemingly obscure branch of statistics. This
is clearly no longer an issue and Bayesian approaches appear to have a prominent role in
social science methodology. I hope that the ﬁrst edition contributed to this development.
Bayesian methods continue to become more important and central to statistical analysis,
broadly speaking. Seemingly, no issue of the Journal of the American Statistical Association
arrives without at least one Bayesian application or theoretical development. While this
upward trend started in the 1990s after we discovered Markov chain Monte Carlo hiding
in statistical physics, the trend accelerates in the 21st century. A nice foretelling is found
in the 1999 Science article by David Malakoﬀ, “Bayesian Boom,” complete with anecdotes
about popular uses in biology and computing as well as quotes from John Geweke. Back
in 1995, the Bayesian luminary Bruno de Finetti predicted that by the year 2020 we would
see a paradigm shift to Bayesian thinking (quoted in Smith [1995]). I believe we are fully
on track to meet this schedule.
Bayesian computing is broader and more varied than it was at the writing of the ﬁrst
edition. In addition to BUGS and WinBUGS, we now routinely use MCMCpack, JAGS, openbugs,
bayesm, and even the new SAS MCMC procedure. The diagnostic routines in R, BOA, and
CODA continue to be useful and are more stable than they were. Of the course the lingua
franca of R is critical, and many researchers use C or C++ for eﬃciency. Issues of statistical
computing remain an important component of the book. It is also necessary to download
and use the R packages CODA and BOA for MCMC diagnostics.
Bayesian approaches are also increasingly popular in related ﬁelds not directly addressed
in this text. There is now an interesting literature in archaeology that is enjoyable to read
(Reese 1994, Freeman 1976, Laxton et al. 1994), and the best starting point is the seminal
paper by Litton and Buck (1995) that sets the agenda for Bayesian archaeometrics. Re-
searchers in this area have also become frustrated with the pathetic state of the null hypoth-
esis signiﬁcance test in the social and behavioral sciences (Cowgill 1977). One area where
xxix

xxx
Preface to the Second Edition
Bayesian modeling is particularly useful is in archaeological forensics, where researchers
make adult-age estimates of early humans (Lucy et al. 1996, Aykroyd et al. 1999).
Changes from the First Edition
A reader of the ﬁrst edition will notice many changes in this revision. Hopefully these
constitute improvements (they certainly constituted a lot of work). First, the coverage of
Markov chain Monte Carlo is greatly expanded. The reason for this is obvious, but bears
mentioning. Modern applied Bayesian work is integrally tied to stochastic simulation and
there are now several high-quality software alternatives for implementation. Unfortunately
these solutions can be complex and the theoretical issues are often demanding. Coupling
this with easy-to-use software, such as WinBUGS and MCMCpack, means that there are users
who are unaware of the dangers inherent in MCMC work. I get a fair number of journal and
book press manuscripts to review supporting this point. There is now a dedicated chapter
on MCMC theory covering issues like ergodicity, convergence, and mixing. The last chapter
is an extension of sections from the ﬁrst edition that now covers in greater detail tools like:
simulated annealing (including its many variants), reversible jump MCMC, and coupling
from the past. Markov chain Monte Carlo research is an incredibly dynamic and fast growing
literature and the need to get some of these ideas before a social science audience was strong.
The reader will also note a substantial increase on MCMC examples and practical guidance.
The objective is to provide detailed advice on day-to-day issues of implementation. Markov
chain Monte Carlo is now discussed in detail in the ﬁrst chapter, giving it the prominent
position that it deserves. It is my belief that Gibbs sampling is as fundamental to estimation
as maximum likelihood, but we (collectively) just do not realize it yet. Recall that there
was about 40 years between Fisher’s important papers and the publication of Birnbaum’s
Likelihood Principle.
This second edition now provides a separate chapter on Bayesian
linear models. Regression remains the favorite tool of quantitative social scientists, and
it makes sense to focus on the associated Bayesian issues in a full chapter. Most of the
questions I get by email and at conferences are about priors, reﬂecting sensitivity about
how priors may aﬀect ﬁnal inferences. Hence, the chapter on forms of prior distributions is
longer and more detailed. I have found that some forms are particularly well-suited to the
type of work that social and behavioral researchers do. One of the strengths of Bayesian
methods is the ease with which hierarchical models can be speciﬁed to recognize diﬀerent
levels and sources in the data. So there is now an expanded chapter on this topic alone,
and while Chapter 12 focuses exclusively on hierarchical model speciﬁcations, these models
appear throughout the text reﬂecting their importance in Bayesian statistics.
Additional topics have crept into this edition, and these are covered at varied levels
from a basic introduction to detailed discussions. Some of these topics are older and well-

Preface to the Second Edition
xxxi
known, such as Bayesian time-series, empirical Bayes, Bayesian decision theory, additional
prior speciﬁcations, model checking with posterior data prediction, the deviance information
criterion (DIC), methods for computing highest posterior density (HPD) intervals, conver-
gence theory, metropolis-coupling, tempering, reversible jump MCMC, perfect sampling,
software packages related to BUGS, and additional models based on normal and Student’s-t
assumptions.
Some new features are more structural. There is now a dedicated R package to accom-
pany this book, BaM (for “Bayesian Methods”). This package includes data and code for
the examples as well as a set of functions for practical purposes like calculated HPD in-
tervals. These materials and more associated with the book are available at the dedicated
Washington University website: http://stats.wustl.edu/BMSBSA. The second edition in-
cludes three appendices covering basic maximum likelihood theory, distributions, and BUGS
software. These were moved to separate sections to make referencing easier and to preserve
the ﬂow of theoretical discussions. References are now contained in a single bibliography
at the end for similar reasons. Some changes are more subtle. I’ve changed all instances
of “noninformative” to “uninformative” since the ﬁrst term does not really describe prior
distributions. Markov chain Monte Carlo techniques are infused throughout, beﬁtting their
central role in Bayesian work. Experience has been that social science graduate students
remain fairly tepid about empirical examples that focus on rats, lizards, beetles, and nuclear
pumps. Furthermore, as of this writing there is no other comprehensive Bayesian text in
the social sciences, outside of economics (except the out-of-print text by Phillips [1973]).
Road Map
To begin, the prerequisites remain the same. Readers will need to have a basic working
knowledge of linear algebra and calculus to follow many of the sections. My math text,
Essential Mathematics for Political and Social Research (2006), provides an overview of
such material. Chapter 1 gives a brief review of the probability basics required here, but
it is certainly helpful to have studied this material before. Finally, one cannot understand
Bayesian modeling without knowledge of maximum likelihood theory. I recognize graduate
programs diﬀer in their emphasis on this core material, so Appendix A covers these essential
ideas.
The second edition is constructed in a somewhat diﬀerent fashion than the ﬁrst. The
most obvious diﬀerence is that the chapter on generalized linear models has been recast as an
appendix, as mentioned. Now the introductory material ﬂows directly into the construction
of basic Bayesian statistical models and the procession of core ideas is not interrupted by
a non-Bayesian discussion of standard models. Nonetheless, this material is important to
have close at hand and hopefully the appendix approach is convenient. Another notable

xxxii
Preface to the Second Edition
change is the “promotion” of linear models to their own chapter. This material is important
enough to stand on its own despite the overlap with Bayesian normal and Student’s-t models.
Other organization changes are found in the computational section where considerable extra
material has been added, both in terms of theory and practice. Markov chain Monte Carlo
set the Bayesians free, and remains an extremely active research ﬁeld. Keeping up with this
literature is a time-consuming, but enjoyable, avocation.
There are a number of ways that a graduate course could be structured around this
text. For a basic-level introductory course that emphasizes theoretical ideas, the ﬁrst seven
chapters provide a detailed overview without considering many computational challenges.
Some of the latter chapters are directed squarely at sophisticated social scientists who have
not yet explored some of the subtle theory of Markov chains. Among the possible structures,
consider the following curricula.
Basic Introductory Course
▷Chapter 1: Background and Introduction
▷Chapter 2: Specifying Bayesian Models
▷Chapter 3: The Normal and Student’s-t Models
▷Chapter 5: The Bayesian Linear Model
▷Chapter 12: Bayesian Hierarchical Models
Thorough Course without an Emphasis on Computing
▷Chapter 1: Background and Introduction
▷Chapter 2: Specifying Bayesian Models
▷Chapter 3: The Normal and Student’s-t Models
▷Chapter 5: The Bayesian Linear Model
▷Chapter 4: The Bayesian Prior
▷Chapter 6: Assessing Model Quality
▷Chapter 7: Bayesian Hypothesis Testing and the Bayes Factor
▷Chapter 12: Bayesian Hierarchical Models
A Component of a Statistical Computing Course
▷Chapter 2: Specifying Bayesian Models
▷Chapter 9: Monte Carlo and Related Iterative Methods
▷Chapter 10: Basics of Markov Chain Monte Carlo
▷Chapter 13: Some Markov Chain Monte Carlo Theory

Preface to the Second Edition
xxxiii
▷Chapter 14: Utilitarian Markov Chain Monte Carlo
▷Chapter 15: Markov Chain Monte Carlo Extensions
A Component of an Estimation Course
▷A: Generalized Linear Model Review
▷Chapter 1: Background and Introduction
▷Chapter 2: Specifying Bayesian Models
▷Chapter 5: The Bayesian Linear Model
▷Chapter 7: Bayesian Hypothesis Testing and the Bayes Factor
Of course I am eager to learn about how instructors use these chapters independent of
any advice here.
Acknowledgments
So many people have commented on this edition, the previous edition, related papers,
associated conference presentations, and classes taught from the book that I am unlikely to
remember them all. Apologies to anyone left out from this list. This edition received three
detailed, formal reviews from Patrick Brandt, Andrew Gelman, and Andrew Martin. Their
comments were invaluable and dramatically improved the quality of this work.
A substantial part of the writing of this book was done during the 2006-2007 academic
year while I was Visiting Professor at Harvard University’s Department of Government and
Fellow at the Institute for Quantitative Social Science. I thank Gary King for inviting me
into that dynamic and rewarding intellectual environment. The Fall semester of that year
I taught a graduate seminar titled Bayesian Hierarchical Modeling, which enabled me to
produce and distribute chapter material on a weekly basis. Participants in the seminar
provided excellent critiques of the principles and exposition. These students and guests
included: Justin Grimmer, Jonathan Harris, Jennifer Katkin, Elena Llaudet, Clayton Nall,
Emre Ozaltin, Lindsay Page, Omar Wasow, Lefteris Anastasopoulos, Shelden Bond, Janet
Lewis, Serban Tanasa, and Lynda Zhang.
The Teaching Fellows for this seminar were
Skyler Cranmer and Andrew Thomas, who were instrumental in improving various technical
sections. I also thank Jens Hainmueller, Dominik Hangartner, and Holger Lutz Kern for
productive discussions of statistical computing during the Spring semester of that year.
Since 2000 I have taught a course based on this book at the Inter-University Consortium
for Political and Social Research (ICPSR) Summer Program at the University of Michigan.
Many of the highly motivated students in this program had constructive comments on the

xxxiv
Preface to the Second Edition
material. I also beneﬁted immensely from interactions with colleagues and administrators
in the program, including Bob Andersen, David Armstrong, Ken Bollen, Dieter Burrell,
John Fox, Charles Franklin, Hank Heitowit, Bill Jacoby, Jim Johnson, Dean Lacy, Scott
Long, Jim Lynch, Tim McDaniel, Sandy Schneider, Bob Stine, and Lee Walker. The three
teaching assistants over this period were incredibly helpful in developing homework and
computer lab assignments: Ryan Bakker, Leslie Johns, and Yu-Sung Su. Overall, ICPSR
is unmatched as a high-intensity teaching and learning Summer experience.
I have tried to record those making comments on the ﬁrst edition or the manuscript
version of the second edition.
In addition to those already mentioned, useful critiques
came from: Attic Access, Larry Bartels, Neal Beck, Jack Buckley, Sid Chib, Boyd Collier,
Skyler J. Cranmer, Chris Dawes, Daniel J. Denis, Persi Diaconis, Alexis Dinno, Hanni
Doss, George Duncan, James Fowler, Justin Gross, Josue Guzman, Michael Herrmann, Jim
Hobert, Kosuke Imai, Brad Jones, Lucas Leemann, Jane Li, Rod Little, John Londregan,
Enrico Luparini, Jonathan Nagler, Shunsuke Narita, Keith T. Poole, Kevin Quinn, Rafal
Raciborski, Michael Smithson, John Sprague, Rick Waterman, Bruce Western, and Chris
Zorn. I also want to give a special thank you to my friend and coauthor George Casella
who has provided irreplaceable brotherly guidance over the last eight years.
The research on Dirichlet process priors (Chapter 15) was supported by National Science
Foundation grants DMS-0631632 and SES-0631588. My work on elicited priors (Chapter 4)
was helped by research with Lee Walker that appeared in the Journal of Politics.
The
discussion of dynamic tempered transitions (Chapter 15) draws from an article written
with George Casella that appeared in Political Analysis. Comments from the editors, Bill
Jacoby and Bob Erikson, made these works better and more focused. The education policy
example (Chapter 5) using a Bayesian linear model is drawn from an article I wrote with
my former graduate student Kevin Wagner. The discussion of convergence in Chapter 13
beneﬁted from my recent article on this topic in Political Analysis. Finally, while there is
no direct overlap, my understanding of detailed statistical computing principles beneﬁted
from the book project with Micah Altman and Michael McDonald.
References
Aykroyd, Robert G., Lucy, David, Pollard, Mark A., and Roberts, Charlotte A. (1999).
Nasty, Brutish,
But Not Necessarily Short: A Reconsideration of the Statistical Methods Used to Calculate Age At
Death From Adult Human Skeletal and Dental Age Indicators. American Antiquity 64, 55-70.
Cowgill, G. L. (1977). The Trouble With Signiﬁcance Tests and What We Can Do About It. Philosophical
Transactions of the Royal Society 327, 331-338.
Freeman, P. R. (1976).
A Bayesian Approach to the Megalithic Yard. Journal of the Royal Statistical
Association, Series A 139, 279-295.

Preface to the Second Edition
xxxv
Gill, Jeﬀ. (2006). Essential Mathematics for Political and Social Research. Cambridge, England: Cam-
bridge University Press.
Laxton, R. R., Cavanaugh, W. G., Litton, C. D., Buck, C. E., and Blair, R. (1994).
The Bayesian
Approach to Archaeological Data Analysis: An Application of Change-Point Analysis to Prehistoric
Domes. Archeologia e Calcolatori 5, 53-68.
Litton, C. D. and Buck, C. E. (1995).
The Bayesian Approach to the Interpretation of Archaeological
Data. Archaeometry 37, 1-24.
Lucy, D., Aykroyd, R. G., Pollard, A. M., and Solheim, T. (1996).
A Bayesian Approach to Adult Human
Age Estimation from Dental Observations by Johanson’s Age Changes. Journal of Forensic Sciences
41, 189-194.
Malakoﬀ, David. (1999). Bayes Oﬀers a ‘New’ Way to Make Sense of Numbers. Science 286, 1460-1464.
Phillips, L. D. (1973). Bayesian Statistics for Social Scientists. Thomas Nelson and Sons, London.
Reese, R. (1994). Are Bayesian Statistics Useful to Archaeological Reasoning? Antiquity 68, 848-850.
Smith, A. F. M. (1995). A Conversation with Dennis Lindley. Statistical Science 10, 305-319.


Preface to the First Edition
Contextual Comments
This book is intended to ﬁll a void. There is a reasonably wide gap between the back-
ground of the median empirically trained social or behavioral scientist and the full weight of
Bayesian statistical inference. This is unfortunate because, as we will see in the forthcoming
chapters, there is much about the Bayesian paradigm that suits the type of data and data
analysis performed in the social and behavioral sciences. Consequently, the goal herein is to
bridge this gap by connecting standard maximum likelihood inference to Bayesian methods
by emphasizing linkages between the standard or classical approaches and full probability
modeling via Bayesian methods.
This is far from being an exclusively theoretical book. I strongly agree that “theoretical
satisfaction and practical implementation are the twin ideals of coherent statistics” (Lindley
1980), and substantial attention is paid to the mechanics of putting the ideas into practice.
Hopefully the extensive attention to calculation and computation basics will enable the
interested readers to immediately try these procedures on their own data. Coverage of var-
ious numerical techniques from detailed posterior calculations to computational-numerical
integration is extensive because these are often the topics that separate theory and realistic
practice.
The treatment of theoretical topics in this work is best described as “gentle but rigorous”:
more mathematical derivation details than related books, but with more explanation as well.
This is not an attempt to create some sort of “Bayes-Lite” or “Bayes for Dummies” (to
paraphrase the popular self-help works). Instead, the objective is to provide a Bayesian
methods book tailored to the interests of the social and behavioral sciences. It therefore
features data that these scholars care about, focuses more on the tools that they are likely
to require, and speaks in a language that is more compatible with typical prerequisites in
associated departments.
There is also a substantial eﬀort to put the development of Bayesian methods in a
historical context. To many, the principles of Bayesian inference appear to be something
that “came out of left ﬁeld,” and it is important to show that not only are the fundamentals
of Bayesian statistics older than the current dominant paradigms, but that their history and
development are actually closely intertwined.
xxxvii

xxxviii
Preface to the First Edition
Outline of the Book
This book is laid out as follows. Chapter 1 gives a high-level, brief introduction to the
basic philosophy of Bayesian inference. I provide some motivations to justify the time and
eﬀort required to learn a new way of thinking about statistics through Bayesian inference.
Chapter 2 (now Appendix A) provides the necessary technical background for going on:
basic likelihood theory, the generalized linear model, and numerical estimation algorithms.
Chapter 1 describes the core idea behind Bayesian thinking: updating prior knowledge
with new data to give the posterior distribution. Examples are used to illustrate this process
and some historical notes are included. The normal model and its relatives are no less
important in Bayesian statistics than in non-Bayesian statistics, and Chapter 3 outlines the
key basic normal models along with extensions.
Specifying prior distributions is a key component of the Bayesian inference process and
Chapter 4 goes through the typology of priors. The Bayesian paradigm has a cleaner and
more introspective approach to assessing the quality of ﬁt and robustness of researcher-
speciﬁed models, and Chapter 6 outlines procedures so that one can test the performance
of various models. Chapter 7 is a bit more formal about this process; it outlines a number
of ways to explicitly test models against each other and to make decisions about unknown
parameters.
The most modern component of this book begins with Chapter 9, which is an intro-
duction to Monte Carlo and related methods. These topics include the many varieties of
numerical integration and importance sampling, and culminating with the EM algorithm.
While none of these tools are exclusively Bayesian in nature, Bayesians generally make
more use of them than others. Chapter 10 formally introduces Markov chain Monte Carlo
(MCMC). These are the tools that revolutionized Bayesian statistics and led to the cur-
rent renaissance. This chapter includes both theoretical background on Markov chains as
well as practical algorithmic details. Chapter 12 discusses hierarchical models that give
the Bayesian researcher great ﬂexibility in specifying models through a general framework.
These models often lead to the requirement of MCMC techniques and the examples in this
chapter are illustrated with practical computing advice. Finally, Chapter 11 (in the ﬁrst
edition) discusses necessary details about the mechanics of running and testing MCMC
inferences.
The structure of each chapter is reasonably uniform. The basic ideas are enumerated
early in the chapter and several of the chapters include an advanced topics section to further
explore ideas that are unlikely to be of interest to every reader or to the ﬁrst-time reader.
All chapters have exercises that are intended to give practice developing the central ideas
of each topic, including computer-based assignments.
There are unfortunately several topics that I have not had the space to cover here.
Foremost is Bayesian decision theory. Many social and behavioral scientists do not operate

Preface to the First Edition
xxxix
in a data-analytic environment where the explicit cost of making a wrong decision can be
quantiﬁed and incorporated into the model. This may be changing and there are a number
of areas that are currently oriented toward identifying loss and risk, such as applied public
policy. In the meantime, readers who are focused accordingly are directed to the books by
Berger (1985), Winkler (1972), Robert (2001), and the foundational work of Wald (1950).
The second major topic that is mentioned only in passing is the growing area of empirical
Bayes. The best introduction is the previously noted text of Carlin and Louis (2001). See
also the extensive empirical Bayes reference list in Section 12.4. I would very much have
liked to cover the early, but exciting developments in perfect sampling (coupling from the
past). See the original work by Propp and Wilson (1996).
Bayesian game theory is an important topic that has been omitted. Some of the better
known citations are Raiﬀa (1982), Blackwell and Girshick (1954), Savage (1954), and Bayarri
and DeGroot (1991). The Bayesian analysis of survival data as a distinct subspecialty is
somewhat understudied. The recent book by Ibrahim, Chen, and Sinha (2001) goes a long
way toward changing that. Chapter 10 provides the essentials for understanding Markov
chains in general.
The study of Markov chains extends well beyond basic MCMC and
the mathematical references that I often ﬁnd myself reaching for are Meyn and Tweedie
(1993), Norris (1997), and Nummelin (1984). The Bayesian hierarchical models covered in
Chapter 12 naturally and easily extend into meta-analysis, a subject well-covered in the
social sciences by Cooper and Hedges (1994), Hunter and Schmidt (1990), and Lipsey and
Wilson (2001).
Background and Prerequisites
This is not a book for a ﬁrst-semester course in social and behavioral statistics. Instead,
it is intended to extend the training of graduate students and researchers who have already
experienced a one-year (roughly) sequence in social statistics. Therefore good prerequisites
include intermediate-level, regression-oriented texts such as Fox (1997), Gujarati (1995),
Hanushek and Jackson (1977), Harrell (2001), Neter et al. (1996), and Montgomery et
al. (2001). Essentially it is assumed that the reader is familiar with the basics of the linear
model, simple inference, multivariate speciﬁcations, and some nonlinear speciﬁcations.
A rudimentary understanding of matrix algebra is required, but this does not need to
go beyond the level of Chapter 1 in Greene (2000), or any basic undergraduate text. The
essential manipulations that we will use are matrix multiplication, inversion, transposition,
and segmentation. The calculus operations done here are more conceptual than mechanical;
that is, it is more important to understand the meaning of diﬀerentiation and integration
operations rather than to be an expert on the technicalities. A knowledge at the level of
Kleppner and Ramsey’s (1985) self-teaching primer is suﬃcient to follow the calculations.

xl
Preface to the First Edition
The core philosophical approach taken with regard to model speciﬁcation comes from
the generalized linear model construct of Nelder and Wedderburn (1972), elaborated in
McCullagh and Nelder (1989).
This is an integrated theoretical framework that uniﬁes
disparate model speciﬁcations by re-expressing models based on making the appropriate
choice of model conﬁguration based on the structure of the outcome variable and the nature
of the dispersion. This fundamental way of thinking is independent of whether the model
is Bayesian (see Dey, Ghosh, and Mallick 2000) or classical (see Fahrmeir and Tutz 2001).
Software
The concepts and procedures in this book would be of little practical value without a
means of directly applying them. Consequently, there is an emphasis here on demonstrating
ideas with statistical software.
All code in R and BUGS and all data are posted at the
dedicated webpage:
http://web.clas.ufl.edu/~jgill/BMSBSA.
A great deal of the material in this book focuses on developing examples using the R and
BUGS statistical packages. Not only are these extremely high-quality analytical tools, they
are also widely distributed free of charge.
It is hard to overstate the value of the R statistical environment. R is the Open Source
implementation of the S statistical language (from AT&T-Bell Labs), which has become the
de facto standard computing language in academic statistics because of its power, ﬂexibility,
and sense of community. R was initially written by Robert Gentleman and Ross Ihak at
the University of Auckland, but is now supported by a growing group of dedicated scholars.
An important aspect of R is the user community itself, and the user-written packages have
been shown to be an eﬀective way for scholars to share and improve new methods.
The homesite for R (see the details in Chapter 2, now Appendix A), contains documen-
tation on installation and learning the language. In addition, because R is “non-unlike” S,
any published book on S-Plus will be useful. The standard text for statistical modeling
in S is the work of Venables and Ripley (1999). The forthcoming book by Fox (2002) is
a particularly helpful and well-written introduction to doing applied work in S. In addi-
tion, an increasing number of applied methodology books that feature the S language have
appeared, and I try to keep up with these on a webpage:
http://web.clas.ufl.edu/~jgill/s-language.help.html.
Any applied Bayesian today that wants to feel good about the state of the world with
regard to software need only look at Press’ 1980 summary of available Bayesian analysis pro-
grams. This is a disparate, even tortured, list of mainframe-based programs that generally
only implement one or two procedures each and require such pleasantries as “Raw data on

Preface to the First Edition
xli
Paper tape.” In contrast, the BUGS package makes Bayesian analysis using MCMC pleasant
and engaging by taking the odious mechanical aspects away from the user, allowing one to
focus on the more interesting aspects of model speciﬁcation and testing. This unbelievable
gift to the Bayesian statistical community was developed at the MRC Biostatistics Unit in
Cambridge:
http://www.mrc-bsu.cam.ac.uk/bugs/.
Acknowledgments
I am indebted to many people for criticisms, suggestions, formal reviews, and inﬂuential
conversations. These include Alan Agresti, Micah Altman, Attic Access, Sammy Barkin,
Neal Beck, Jim Booth, Brad Carlin, George Casella, Lauren Cowles, John Fox, Wayne
Francis, Charles Franklin, Malay Ghosh, Hank Heitowit, Jim Hobert, Bill Jacoby, Renee
Johnson, Gary King, Andrew Martin, Michael Martinez, Mike McDonald, Ken Meier, Elias
Moreno, Brad Palmquist, Kevin Quinn, Christian Robert, Stephen Sen, Jason Wittenberg,
Sam Wu, Chris Zorn, and anonymous reviewers. I am especially grateful to George Casella
for his continued advice and wisdom; I’ve learned as much sitting around George’s kitchen
table or trying to keep up with him running on the beach as I have in a number of past
seminars. Andrew Martin also stands out for having written detailed and indispensable
reviews (formally and informally) of various drafts and for passing along insightful critiques
from his students.
A small part of the material in Chapter 7 was presented as a conference paper at the Fifth
International Conference on Social Science Methodology in Cologne, Germany, October
2000. Also, some pieces of Chapter 10 were taken from another conference paper given at
the Midwestern Political Science Association Annual Meeting, Chicago, April 2001.
This book is a whole lot better due to input from students at the University of Florida
and the University of Michigan Summer Program, including: Javier Aprricio-Castello, Jorge
Aragon, Jessica Archer, Sam Austin, Ryan Bakker, David Conklin, Jason Gainous, Dukhong
Kim, Eduardo Leoni, Carmela Lutmar, Abdel-hameed Hamdy Nawar, Per Simonnson, Jay
Tate, Tim McGraw, Nathaniel Seavy, Lee Walker, and Natasha Zharinova. For research
assistance, I would also like to thank Ryan Bakker, Kelly Billingsley, Simon Robertshaw,
and Nick Theobald.
I would like to also thank my editor, Bob Stern, at Chapman & Hall for making this
process much more pleasant than it would have been without his continual help.
This
volume was appreciatively produced and delivered “camera-ready” with LATEX using the
AMS packages, pstricks, and other cool typesetting macros and tools from the TEX world.
I cannot imagine the bleakness of a life restricted to the low-technology world of word
processors.

xlii
Preface to the First Edition
References
Bayarri, M. J. and DeGroot, M. H. (1991).
What Bayesians Expect of Each Other.
Journal of the
American Statistical Association 86, 924-932.
Berger, J. O. (1985).
Statistical Decision Theory and Bayesian Analysis.
Second Edition.
New York:
Springer-Verlag.
Blackwell, D. and Girshick, M. A. (1954).
Theory of Games and Statistical Decisions. New York: Wiley.
Carlin, B. P. and Louis, T. A. (2001).
Bayes and Empirical Bayes Methods for Data Analysis. Second
Edition. New York: Chapman & Hall.
Cooper, H. and Hedges, L. (eds.) (1994).
The Handbook of Research Synthesis. New York: Russell Sage
Foundation.
Dey, D. K., Ghosh, S. K., and Mallick, B. K. (2000).
Generalized Linear Models: A Bayesian Perspective.
New York: Marcel Dekker.
Fahrmeir, L. and Tutz, G. (2001).
Multivariate Statistical Modelling Based on Generalized Linear Models.
Second Edition. New York: Springer.
Fox, J. (1997). Applied Regression Analysis, Linear Models, and Related Methods. Thousand Oaks, CA:
Sage.
Fox, J. (2002). An R and S-Plus Companion to Applied Regression. Thousand Oaks, CA: Sage.
Greene, W. (2000). Econometric Analysis. Fourth Edition. Upper Saddle River, NJ: Prentice Hall.
Gujarati, D. N. (1995). Basic Econometrics. New York: McGraw-Hill.
Hanushek, E. A. and Jackson, J. E. (1977).
Statistical Methods for Social Scientists. San Diego: Academic
Press.
Harrell, F. E. (2001). Regression Modeling Strategies: With Applications to Linear Models, Logistic Re-
gression, and Survival Analysis. New York: Springer-Verlag.
Hunter, J. E. and Schmidt, F. L. (1990).
Methods of Meta-Analysis: Correcting Error and Bias in
Research Findings. Thousand Oaks, CA: Sage.
Ibrahim, J. G., Chen, M-H., and Sinha, D. (2001).
Bayesian Survival Analysis. New York: Springer-
Verlag.
Kleppner, D. and Ramsey, N. (1985).
Quick Calculus: A Self-Teaching Guide. New York: Wiley Self
Teaching Guides.
Lindley, D. V. (1980).
Jeﬀreys’s Contribution to Modern Statistical Thought. In Bayesian Analysis in
Econometrics and Statistics: Essays in Honor of Harold Jeﬀreys. Arnold Zellner (ed.). Amsterdam:
North Holland.
Lipsey, M. W. and Wilson, D. B. (2001).
Practical Meta-Analysis. Thousand Oaks, CA: Sage.
McCullagh, P. and Nelder, J. A. (1989).
Generalized Linear Models. Second Edition. New York: Chapman
& Hall.
Meyn, S. P. and Tweedie, R. L. (1993).
Markov Chains and Stochastic Stability. New York: Springer-
Verlag.
Montgomery, D. C. C., Peck, E. A., and Vining, G. G. (2001).
Introduction to Linear Regression Analysis.
Third Edition. New York: John Wiley & Sons.
Nelder, J. A. and Wedderburn, R. W. M. (1972).
“Generalized Linear Models.” Journal of the Royal
Statistical Society, Series A 135, 370-85.
Neter, J., Kutner, M. H., Nachtsheim, C., and Wasserman, W. (1996).
Applied Linear Regression Models.
Chicago: Irwin.
Norris, J. R. (1997). Markov Chains. Cambridge: Cambridge University Press.

Preface to the First Edition
xliii
Nummelin, E. (1984). General Irreducible Markov Chains and Non-negative Operators. Cambridge: Cam-
bridge University Press.
Press, S. J. (1980). Bayesian Computer Programs. In Studies in Bayesian Econometrics and Statistics, S.
E. Fienberg and A. Zellner, eds., Amsterdam: North Holland, pp. 429-442..
Propp, J. G. and Wilson, D. B. (1996).
Exact Sampling with Coupled Markov Chains and Applications
to Statistical Mechanics. Random Structures and Algorithms 9, 223-252.
Raiﬀa, H. (1982). The Art and Science of Negotiation. Cambridge: Cambridge University Press.
Robert, C. P. (2001). The Bayesian Choice: A Decision Theoretic Motivation. Second Edition. New York:
Springer-Verlag.
Savage, L. J. (1954). The Foundations of Statistics. New York: Wiley.
Venables, W. N. and Ripley, B. D. (1999).
Modern Applied Statistics with S-Plus, Third Edition. New
York: Springer-Verlag.
Wald, A. (1950). Statistical Decision Functions. New York: Wiley.
Winkler, R. L. (1972). Introduction to Bayesian Inference and Decision. New York: Holt, Rinehart, and
Winston.


Chapter 1
Background and Introduction
1.1
Introduction
Vitriolic arguments about the merits of Bayesian versus classical approaches seem to have
faded into a quaint past of which current researchers in the social sciences are, for the most
part, blissfully unaware. In fact, it almost seems odd well into the 21st century that deep
philosophical conﬂicts dominated the last century on this issue. What happened? Bayesian
methods always had a natural underlying advantage because all unknown quantities are
treated probabilistically, and this is the way that statisticians and applied statisticians really
prefer to think. However, without the computational mechanisms that entered into the ﬁeld
we were stuck with models that couldn’t be estimated, prior distributions (distributions
that describe what we know before the data analysis) that incorporated uncomfortable
assumptions, and an adherence to some bankrupt testing notions. Not surprisingly, what
changed all this was a dramatic increase in computational power and major advances in the
algorithms used on these machines. We now live in a world where there are very few model
limitations, other than perhaps our imaginations. We therefore live in world now where
researchers are for the most part comfortable specifying Bayesian and classical models as it
suits their purposes.
It is no secret that Bayesian methods require a knowledge of classical methods as well
as some additional material. Most of this additional material is either applied calculus or
statistical computing. That is where this book comes in. The material here is intended
to provide an introduction to Bayesian methods all the way from basic concepts through
advanced computational material. Some readers will therefore be primarily interested in
diﬀerent sections. Also it means that this book is not strictly a textbook, a polemic, nor a
research monograph. It is intended to be all three.
Bayesian applications in medicine, the natural sciences, engineering, and the social sci-
ences have been increasing at a dramatic rate since the middle of the early 1990s. In-
terestingly, the Mars Rovers are programmed to think Bayesianly while they traverse that
planet. Currently seismologists perform Bayesian updates of aftershocks based on the main-
shock and previous patterns of aftershocks in the region. Bayesian networks are built in
computational biology, and the forefront of quantitative research in genomics is now ﬁrmly
Bayesian.
1

2
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
So why has there been a noticeable increase in interest in Bayesian statistics? There are
actually several visible reasons. First, and perhaps most critically, society has radically in-
creased its demand for statistical analysis of all kinds. A combined increase in clinical trials,
statistical genetics, survey research, general political studies, economic analysis, government
policy work, Internet data distribution, and marketing research have led to golden times for
applied statisticians. Second, many introspective scholars who seriously evaluate available
paradigms ﬁnd that alternatives to Bayesian approaches are fraught with logical inconsis-
tencies and shortcomings. Third, until recent breakthroughs in statistical computing, it
was easy to specify realistic Bayesian statistical models that could not provide analytically
tractable summary results.
There is therefore ample motivation to understand the basics of Bayesian statistical
methodology, and not just because it is increasingly important in mainstream analysis
of data. The Bayesian paradigm rests on a superior set of underlying assumptions and
includes procedures that allow researchers to include reliable information in addition to the
sample, to talk about ﬁndings in intuitive ways, and to set up future research in a coherent
manner. At the core of the data-analytic enterprise, these are key criteria to producing
useful statistical results.
Statistical analysis is the process of “data reduction” with the goal of separating out
underlying systematic eﬀects from the noise inherent in all sets of observations. Obviously
there is a lot more to it than that, but the essence of what we do is using models to distill
ﬁndings out of murky data. There are actually three general steps in this process: collection,
analysis, and assessment. For most people, data collection is not diﬃcult in that we live in
an age where data are omnipresent. More commonly, researchers possess an abundance of
data and seek meaningful patterns lurking among the various dead-ends and distractions.
Armed with a substantive theory, many are asking: what should I do now? Furthermore,
these same people are often frustrated when receiving multiple, possibly conﬂicting, answers
to that question.
Suppose that there exists a model-building and data analysis process with the following
desirable characteristics:
▷overt and clear model assumptions,
▷a principled way to make probability statements about the real quantities of theoretical
interest,
▷an ability to update these statements (i.e., learn) as new information is received,
▷systematic incorporation of previous knowledge on the subject,
▷missing information handled seamlessly as part of the estimation process,
▷recognition that population quantities can be changing over time rather than forever
ﬁxed,
▷the means to model all data types including hierarchical forms,
▷straightforward assessment of both model quality and sensitivity to assumptions.

Background and Introduction
3
As the title of this book suggests, the argument presented here is that the practice of
Bayesian statistics possesses all of these qualities. Press (1989) adds the following practical
advantages to this list:
▷it often results in shorter conﬁdence/credible intervals,
▷it often gives smaller model variance,
▷predictions are usually better,
▷“proper” prior distributions (Chapter 4) give models with good frequentist properties,
▷reasonably “objective” assumptions are available,
▷hypotheses can be tested without pre-determination of testing quality measures.
This text will argue much beyond these points and assert that the type of data social and
behavioral scientists routinely encounter makes the Bayesian approach ideal in ways that
traditional statistical analysis cannot match. These natural advantages include avoiding the
assumption of inﬁnite amounts of forthcoming data, recognition that ﬁxed-point assump-
tions about human behavior are dubious, and a direct way to include existing expertise in
a scientiﬁc ﬁeld.
What reasons are there for not worrying about Bayesian approaches and sticking with
the, perhaps more comfortable, traditional mindset? There are several reasons why a reader
may not want to worry about the principles in this text for use in their research, including:
▷their population parameters of interest are truly ﬁxed and unchanging under all real-
istic circumstances,
▷they do not have any prior information to add to the model speciﬁcation process,
▷it is necessary for them to provide statistical results as if data were from a controlled
experiment,
▷they care more about “signiﬁcance” than eﬀect size,
▷computers are slow or relatively unavailable for them,
▷they prefer very automated, “cookbook” type procedures.
So why do so-called classical approaches dominate Bayesian usage in the social and be-
havioral sciences? There are several reasons for this phenomenon. First, key ﬁgures in
the development of modern statistics had strong prejudices against aspects of Bayesian in-
ference for narrow and subjective reasons. Second, the cost of admission is higher in the
form of additional mathematical formalism. Third, until recently realistic model speciﬁca-
tions sometimes led to unobtainable Bayesian solutions. Finally, there has been a lack of
methodological introspection in a number of disciplines. The primary mission of this text
is to make the second and third reasons less of a barrier through accessible explanation,
detailed examples, and speciﬁc guidance on calculation and computing.
It is important to understand that the Bayesian way does not mean throwing away one’s
comfortable tools, and it is not itself just another tool. Instead it is a way of organizing one’s

4
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
toolbox and is also a way of doing statistical work that has sharply diﬀerent philosophical
underpinnings. So adopting Bayesian methods means keeping the usual set of methods,
such as linear regression, ANOVA, generalized linear models, tabular analysis, and so on. In
fact, many researchers applying statistics in the social sciences are not actually frequentists
since they cannot assume an inﬁnite stream of iid (independent and identically distributed)
data coming from a controlled experimental setup. Instead, most of these analysts can be
described as “likelihoodists,” since they obtain one sample of observational data that is
contextual and will not be repeated, then perform standard likelihood-based (Fisherian)
inference to get coeﬃcient estimates.
Aside from underlying philosophical diﬀerences, many readers will be comforted in ﬁnd-
ing that Bayesian and non-Bayesian analyses often agree. There are two important instances
where this is always true. First, when the included prior information is very uninformative
(there are several ways of providing this), summary statements from Bayesian inference will
match likelihood point estimates. Therefore a great many researchers are Bayesians who
do not know it yet. Second, when the data size is very large, the form of the prior informa-
tion used does not matter and there is agreement again. Other circumstances also exist in
which Bayesian and non-Bayesian statistical inferences lead to the same results, but these
are less general than the two mentioned. In addition to these two important observations,
all hierarchical models are overtly Bayesian since they deﬁne distributional assumptions at
levels. These are popular models due to their ﬂexibility with regard to the prevalence of
diﬀerent levels of observed aggregation in the same dataset. We will investigate Bayesian
hierarchical models in Chapter 12.
We will now proceed to a detailed justiﬁcation for the use of modern Bayesian methods.
1.2
General Motivation and Justiﬁcation
With Bayesian analysis, assertions about unknown model parameters are not expressed
in the conventional way as single point estimates along with associated reliability assessed
through the standard null hypothesis signiﬁcance test. Instead the emphasis is on making
probabilistic statements using distributions. Since Bayesians make no fundamental distinc-
tion between unobserved data and unknown parameters, the world is divided into: imme-
diately available quantities, and those that need to be described probabilistically. Before
observing some data, these descriptions are called prior distributions, and after observing
the data these descriptions are called posterior distributions. The quality of the modeling
process is the degree to which a posterior distribution is more informed than a prior distribu-
tion for some unknown quantity of interest. Common descriptions of posterior distributions
include standard quantile levels, the probability of occupying some aﬀected region of the

Background and Introduction
5
sample space, the predictive quantities from the posterior, and Bayesian forms of conﬁdence
intervals called credible intervals.
It is important to note here that the pseudo-frequentist null hypothesis signiﬁcance
test (NHST) is not just sub-optimal, it is wrong. This is the dominant hypothesis testing
paradigm as practiced in the social sciences. Serious problems include: a logical inconsis-
tency coming from probabilistic modus tollens, confusion over the order of the conditional
probability, chasing signiﬁcance but ignoring eﬀect size, adherence to completely arbitrary
signiﬁcance thresholds, and confusion about the probability of rejection. There is a general
consensus amongst those that have paid attention to this issue that the social sciences have
been seriously harmed by the NHST since it has led to ﬁxations with counting stars on
tables rather than looking for eﬀect sizes and general statistical reliability. See the recent
discussions in Gill (1999) and Denis (2005) in particular for detailed descriptions of these
problems and how they damage statistical inferences in the social sciences. Serious criti-
cism of the NHST began shortly after its creation in the early 20th century by textbook
writers who blended Fisherian likelihoodism with Neyman and Pearson frequentism in an
eﬀort to oﬀend neither warring and evangelical camp. An early critic of this unholy union
was Rozeboom (1960) who noticed its “strangle-hold” on social science inference. In 1962
Arthur Melton wrote a parting editorial in the Journal of Experimental Psychology reveal-
ing that he had held authors to a 0.01 p-value standard: “In editing the Journal there has
been a strong reluctance to accept and publish results related to the principal concern of
the research when those results were signiﬁcant at the .05 level, whether by one- or two-
tailed test!” This had the eﬀect of accelerating the criticism and led to many analytical
and soul-searching articles discussing the negative consequences of this procedure in the so-
cial sciences, including: Barnett (1973), Berger (2003), Berger, Boukai, and Wang (1997),
Berger and Sellke (1987), Berkhardt and Schoenfeld (2003), Bernardo (1984), Brandst¨atter
(1999), Carver (1978, 1993), Cohen (1962, 1977, 1988, 1992, 1994), Dar, Serlin and Omer
(1994), Falk and Greenbaum (1995), Gigerenzer (1987, 1998a, 1998b, 2004), Gigerenzer
and Murray (1987), Gliner, Leech and Morgan (2002), Greenwald (1975), Greenwald, et
al. (1996),
Goodman (1993, 1999), Haller and Krauss (2002), Howson and Urbach (1993),
Hunter (1997), Hunter and Schmidt (1990), Kirk (1996), Krueger (2001), Lindsay (1995),
Loftus (1991, 1993), Macdonald (1997), McCloskey and Ziliak (1996), Meehl (1978, 1990,
1997), Moran and Soloman (2004), Morrison and Henkel (1969, 1970), Nickerson (2000),
Oakes (1986), Pollard (1993), Pollard and Richardson (1987), Robinson and Levin (1997),
Rosnow and Rosenthal (1989), Schmidt (1996), Schmidt and Hunter (1977), Schervish
(1996), Sedlmeier and Gigerenzer (1989),
Thompson (1996, 1997, 2002a, 2002b, 2004),
Wilkinson (1977), Ziliak and McCloskey (2007).
And this is only a small sample of the
vast literature describing the NHST as bankrupt. Conveniently some of the more inﬂuential
articles listed above are reprinted in Harlow et al. (1997). We will return to this point in
Chapter 7 (starting on page 209) in the discussion of Bayesian hypothesis testing and model
comparison.
This focus on distributional inference leads to two key assumptions for Bayesian work.

6
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
First, a speciﬁc parametric form is assumed to describe the distribution of the data given
parameter values. Practically, this is used to construct a likelihood function (A) to incorpo-
rate the contribution of the full sample of data. Note that this is an inherently parametric
setup, and although nonparametric Bayesian modeling is a large and growing ﬁeld, it exists
beyond the scope of the basic setup.
Second, since unknown parameters are treated as
having distributional qualities rather than being ﬁxed, an assumed prior distribution on the
parameters of interest unconditional on the data is given. This reﬂects either uncertainty
about a truly ﬁxed parameter or recognition that the quantity of interest actually behaves
in some stochastic fashion.
With those assumptions in hand, the essentials of Bayesian thinking can be stated in
three general steps:
1. Specify a probability model that includes some prior knowledge about the parameters
for unknown parameter values.
2. Update knowledge about the unknown parameters by conditioning this probability
model on observed data.
3. Evaluate the ﬁt of the model to the data and the sensitivity of the conclusions to the
assumptions.
Notice that this process does not include an unrealistic and artiﬁcial step of making a
contrived decision based on some arbitrary quality threshold. The value of a given Bayesian
model is instead found in the description of the distribution of some parameter of interest
in probabilistic terms. Also, there is nothing about the process contained in the three steps
above that cannot be repeated as new data are observed. It is often convenient to use the
conventional signiﬁcance thresholds that come from Fisher, but Bayesians typically do not
ascribe any major importance to being barely on one side or the other. That is, Bayesian
inference often prescribes something like a 0.05 threshold, but it is rare to see work where
a 0.06 ﬁnding is not taken seriously as a likely eﬀect.
Another core principle of the Bayesian paradigm is the idea that the data are ﬁxed once
observed. Typically (but not always) these data values are assumed to be exchangeable; the
model results are not changed by reordering the data values. This property is more general
than, and implied by, the standard assumption that the data are independent and identically
distributed (iid): independent draws from the same distribution, and also implies a common
mean and variance for the data values (Leonard and Hsu 1999, p.1). Exchangeability allows
us to say that the data generation process is conditional on the unknown model parameters
in the same way for every data value (de Finetti 1974, Draper et al. 1993, Lindley and
Novick 1981). Essentially this is a less restrictive version of the standard iid assumption.
Details about the exchangeability assumption are given in Chapter 12. We now turn to a
discussion of probability basics as a precursor to Bayesian mechanics.

Background and Introduction
7
1.3
Why Are We Uncertain about Uncertainty?
The fundamental principles of probability are well known, but worth repeating here.
Actually, it is relatively easy to intuitively deﬁne the properties of a probability function:
(1) its range is bounded by zero and one for all the values in the applicable domain, (2) it
sums or integrates to one over this domain, and (3) the sum or integral of the functional
outcome (probability) of disjoint events is equal to the functional outcome of the union of
these events. These are the Kolmogorov axioms (1933), and are given in greater detail in Gill
(2006), Chapter 7. The real problem lies in describing the actual meaning of probability
statements. This diﬃculty is, in fact, at the heart of traditional disagreements between
Bayesians and non-Bayesians.
The frequentist statistical interpretation of probability is that it is a limiting relative fre-
quency: the long-run behavior of a nondeterministic outcome or just an observed proportion
in a population. This idea can be traced back to Laplace (1814), who deﬁned probability as
the number of successful events out of trials observed. Thus if we could simply repeat the
experiment or observe the phenomenon enough times, it would become apparent what the
future probability of reoccurrence will be. This is an enormously useful way to think about
probability but the drawback is that frequently it is not possible to obtain a large number
of outcomes from exactly the same event-generating system (Kendall 1949, Placket 1966).
A competing view of probability is called “subjective” and is often associated with the
phrase “degree of belief.” Early proponents included Keynes (1921) and Jeﬀreys (1961), who
observed that two people could look at the same situation and assign diﬀerent probability
statements about future occurrences.
This perspective is that probability is personally
deﬁned by the conditions under which a person would make a bet or assume a risk in
pursuit of some reward. Subjective probability is closely linked with the idea of decision-
making as a ﬁeld of study (see, for instance, Bernardo and Smith [1994, Chapter 2]) and
the principle of selecting choices that maximize personal utility (Berger 1985).
These two characterizations are necessarily simpliﬁcations of the perspectives and de
Finetti (1974, 1975) provides a much deeper and more detailed categorization, which we
will return to in Chapter 12. To de Finetti, the ultimate arbiter of subjective probabil-
ity assignment is the conditions under which individuals will wager their own money. In
other words, a person will not violate a personal probability assessment if it has ﬁnancial
consequences. Good (1950) makes this idea more axiomatic by observing that people have
personal probability assessments about many things around them rather than just one, and
in order for these disparate comparative statements to form a body of beliefs they need to be
free of contradictions. For example, if a person thinks that A is more likely to occur than B,
and B is more likely to occur than C, then this person cannot coherently believe that C is
more likely than A (transitivity). Furthermore, Good adds the explicitly Bayesian idea that
people are constantly updating these personal probabilities as new information is observed,

8
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
although there is evidence that people have subadditive notions of probability when making
calculations (the probability of some event plus the probability of its complement do not
add to certainty).
The position underlying nearly all Bayesian work is the subjective probability charac-
terization, although there have been many attempts to “objectify” Bayesian analysis (see
Chapter 4). Prior information is formalized in the Bayesian framework and this prior in-
formation can be subjective in the sense that the researcher’s experience, intuition, and
theoretical ideas are included. It is also common to base the prior information on previous
studies, experiments, or just personal observations and this process is necessarily subject to
a limited (although possibly large) number of observations rather than the inﬁnite number
assumed under the frequentist view. We will return to the theme of subjectivity contained
in prior information in Chapter 4 and elsewhere, but the principal point is that all statisti-
cal models include subjective decisions, and therefore we should ceteris paribus prefer one
that is the most explicit about assumptions. This is exactly the sense that the Bayesian
prior provides readers with a speciﬁc, formalized statement of currently assumed knowledge
in probabilistic terms.
1.3.1
Required Probability Principles
There are some simple but important principles and notational conventions that must
be understood before proceeding. We will not worry too much about measure theory until
Chapter 13, and the concerned reader is directed to the ﬁrst chapter of any mathematical
statistics text or the standard reference works of Billingsley (1995), Chung (1974), and Feller
(1990, Volumes 1 and 2). Abstract events are indicated by capital Latin letters: A, B, C,
etc. A probability function corresponding to some event A is always indicated by p(A). The
complement of the event A is denoted Ac, and it is a consequence of Kolmogorov’s axioms
listed above that p(Ac) = 1 −p(A). The union of two events is indicated by A ∪B and the
intersection by A ∩B. For any two events: p(A ∪B) = p(A) + p(B) −p(A ∩B). If two
events are independent, then p(A ∩B) = p(A)p(B), but not necessarily the converse (the
product relationship does not imply independence).
Central to Bayesian thinking is the idea of conditionality. If an event B is material
to another event A in the sense that the occurrence or non-occurrence of B aﬀects the
probability of A occurring, then we say that A is conditional on B. It is a basic tenet of
Bayesian statistics that we update our probabilities as new relevant information is observed.
This is done with the deﬁnition of conditional probability given by: p(A|B) = p(A∩B)/p(B),
which is read as “the probability of A given B is equal to the probability of A and B divided
by the probability of B.”
In general the quantities of interest here are random variables rather than the simple
discrete events above.
A random variable X is deﬁned as a measurable function from
a probability space to a state space.
This can be deﬁned very technically (Shao 2005,
p.7), but for our purposes it is enough to understand that the random variable connects

Background and Introduction
9
possible occurrences of some data value with a probability structure that reﬂects the relative
“frequency” of these occurrences. The function is thus deﬁned over a speciﬁc state space of
all possible realizations of the random variable, called support. Random variables can be
discrete or continuous. For background details see Casella and Berger (2002), Shao (2005),
or the essay by Doob (1996). The expression p(X ∩Y ) is usually denoted as p(X, Y ), and
is referred to as the joint distribution of random variables X and Y . Marginal distributions
are then simply p(X) and p(Y ).
Restating the principle above in this context, for two
independent random variables the joint distribution is just the product of the marginals,
p(X, Y ) = p(X)p(Y ). Typically we will need to integrate expressions like p(X, Y ) to get
marginal distributions of interest. Sometimes this is done analytically, but more commonly
we will rely on computational techniques.
We will make extensive use of expected value calculations here. Recall that if a random
variable X is distributed p(X), the expected value of some function of the random variable,
h(X), is
E[h(X)] =
⎧
⎪
⎪
⎨
⎪
⎪
⎩
k
i=1
h(xi)p(xi)
k-category discrete case

X h(X)p(X)dx
continuous case.
(1.1)
Commonly h(X) = X, and we are simply concerned with the expectation of the random
variable itself. In the discrete case this is a very intuitive idea as the expected value can be
thought of as a probability-weighted average over possible events. For the continuous case
there are generally limits on the integral that are dictated by the support of the random
variable, and sometimes these are just given by [−∞, ∞] with the idea that the PDF
(probability density function) indicates zero and non-zero regions of density. Also p(X)
is typically a conditional statement: p(X|θ). For the k × 1 vector X of discrete random
variables, the expected value is: E[X] =  Xp(X). With the expected value of a function
of the continuous random vector, it is common to use the Riemann-Stieltjes integral form
(found in any basic calculus text): E[f(X)] = 
X f(X)dF(X), where F(X) denotes the
joint distribution of the random variable vector X. The principles now let us look at Bayes’
Law in detail.
1.4
Bayes’ Law
The Bayesian statistical approach is based on updating information using what is called
Bayes’ Law (and synonymously Bayes’ Theorem) from his famous 1763 essay. The Rev-
erend Thomas Bayes was an amateur mathematician whose major contribution (the others
remain rather obscure and do not address the same topic) was an essay found and pub-
lished two years after his death by his friend Richard Price. The enduring association of
an important branch of statistics with his name actually is somewhat of an exaggeration of

10
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
the generalizeability of this work (Stigler 1982). Bayes was the ﬁrst to explicitly develop
this famous law, but it was Laplace (1774, 1781) who (apparently independently) provided
a more detailed analysis that is perhaps more relevant to the practice of Bayesian statistics
today. See Stigler (1986) for an interesting historical discussion and Sheynin (1977) for
a detailed technical analysis. Like Bayes, Laplace assumed a uniform distribution for the
unknown parameter, but he worried much less than Bayes about the consequences of this
assumption. Uniform prior distributions are simply “ﬂat” distributions that assign equal
probability for every possible outcome.
Suppose there are two events of interest A and B, which are not independent. We know
from basic axioms of probability that the conditional probability of A given that B has
occurred is given by:
p(A|B) = p(A, B)
p(B) ,
(1.2)
where p(A|B) is read as “the probability of A given that B has occurred, p(A, B) is the
“the probability that both A and B occur” (i.e., the joint probability) and p(B) is just the
unconditional probability that B occurs. Expression (1.2) gives the probability of A after
some event B occurs. If A and B are independent here then p(A, B) = p(A)p(B) and (1.2)
becomes uninteresting.
We can also deﬁne a diﬀerent conditional probability in which A occurs ﬁrst:
p(B|A) = p(B, A)
p(A) .
(1.3)
Since the probability that A and B occur is the same as the probability that B and A occur
(p(A, B) = p(B, A)), then we can rearrange (1.2) and (1.3) together in the following way:
p(A, B) = p(A|B)p(B)
p(B, A) = p(B|A)p(A)
p(A|B)p(B) = p(B|A)p(A)
p(A|B) = p(A)
p(B)p(B|A).
(1.4)
The last line is the famous Bayes’ Law. This is really a device for “inverting” conditional
probabilities. Notice that we could just as easily produce p(B|A) in the last line above by
moving the unconditional probabilities to the left-hand side in the last equality.
We can also use Bayes’ Law with the use of odds, which is a common way to talk about
uncertainty related to probability. The odds of an event is the ratio of the probability of
an event happening to the probability of the event not happening. So for the event A, the
odds of this event is simply:
Odds =
p(A)
1 −p(A) = p(A)
p(¬A),
(1.5)

Background and Introduction
11
which is this ratio expressed in two diﬀerent ways. Note the use of “¬A” for “not A,” which
is better notation when the complement of A isn’t speciﬁcally deﬁned and we care only that
event A did not happen. Since this statement is not conditional on any other quantity, we
can call it a “prior odds.” If we make it conditional on B, then it is called a “posterior
odds,” which is produced by multiplying the prior odds by the reverse conditional with
regard to B:
p(A|B)
p(¬A|B) = p(A)
p(¬A)
p(B|A)
p(B|¬A).
(1.6)
The last ratio, p(B|A)/p(B|¬A) is the “likelihood ratio” for B under the two conditions for
A. This is actually the ratio of two expressions of Bayes’ Law in the sense of (1.4), which
we can see with the introduction of the ratio p(B)/p(B):
p(A|B)
p(¬A|B) = p(A)/p(B)
p(¬A)/p(B)
p(B|A)
p(B|¬A).
(1.7)
This ratio turns out to be very useful in Bayesian model consideration since it implies a
test between the two states of nature, A and ¬A, given the observation of some pertinent
information B.
■Example 1.1:
Testing with Bayes’ Law. How is this useful? As an example, hy-
pothetically assume that 2% of the population of the United States are members of
some extremist Militia group (p(M) = 0.02), a fact that some members might at-
tempt to hide and therefore not readily admit to an interviewer. A survey is 95%
accurate on positive Classiﬁcation, p(C|M) = 0.95, (“sensitivity”) and the uncondi-
tional probability of classiﬁcation (i.e., regardless of actual militia status) is given by
p(C) = 0.05.
To illustrate how p(C) is really the normalizing constant obtained
by accumulating over all possible events, we will stipulate the additional knowl-
edge that the survey is 97% accurate on negative classiﬁcation, p(Cc|M c) = 0.97
(“speciﬁcity”). The unconditional probability of classifying a respondent as a mili-
tia member results from accumulation of the probability across the sample space of
survey events using the Total Probability Law: p(C) = p(C ∩M) + p(C ∩M c) =
p(C|M)p(M) + [1 −p(Cc|M c)]p(M c) = (0.95)(0.02) + (0.03)(0.98) ∼= 0.05.
Using Bayes’ Law, we can now derive the probability that someone positively classiﬁed
by the survey as being a militia member really is a militia member:
p(M|C) = p(M)
p(C) p(C|M) = 0.02
0.05(0.95) = 0.38.
(1.8)
The startling result is that although the probability of correctly classifying an individ-
ual as a militia member given they really are a militia member is 0.95, the probability
that an individual really is a militia member given that they are positively classiﬁed
is only 0.38.
The highlighted diﬀerence here between the order of conditional probability is often
substantively important in a policy or business context.
Consider the problem of

12
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
designing a home pregnancy test. Given that there exists a fundamental business
trade-oﬀbetween the reliability of the test and the cost to consumers, no commercially
viable product will have perfect or near-perfect test results. In designing the chemistry
and packaging of the test, designers will necessarily have to compromise between the
probability of PRegnancy given positive Test results, p(PR|T), and the probability
of positive test results given pregnancy, p(T|PR). Which one is more important?
Clearly, it is better to maximize p(T|PR) at the expense of p(PR|T), as long as the
reduction in the latter is reasonable: it is preferable to give a higher number of false
positives, sending women to consult their physician to take a more sensitive test, than
to fail to notify many pregnant women. This reduces the possibility that a woman
who does not realize that she is pregnant might continue unhealthy practices such
as smoking, drinking, or maintaining a poor diet.
Similarly, from the perspective
of general public health, it is better to have preliminary tests for deadly contagious
diseases designed to be similarly conservative with respect to false positives.
1.4.1
Bayes’ Law for Multiple Events
It would be extremely limiting if Bayes’ Law only applied to two alternative events.
Fortunately the extension to multiple events is quite easy. Suppose we observe some data
D and are interested in the relative probabilities of three events A, B, and C conditional
on these data. These might be rival hypotheses about some social phenomenon for which
the data are possibly revealing. Thinking just about event A, although any of the three
could be selected, we know from Bayes’ Law that:
p(A|D) = p(D|A)p(A)
p(D)
.
(1.9)
We also know from the Total Probability Law and the deﬁnition of conditional probability
that:
p(D) = p(A ∩D) + p(B ∩D) + p(C ∩D)
= p(D|A)p(A) + p(D|B)p(B) + p(D|C)p(C).
(1.10)
This means that if we substitute the last line into the expression for Bayes’ Law, we get:
p(A|D) =
p(D|A)p(A)
p(D|A)p(A) + p(D|B)p(B) + p(D|C)p(C),
(1.11)
which demonstrates that the conditional distribution for any of the rival hypotheses can be
produced as long as there exist unconditional distributions for the three rival hypotheses,
p(A), p(B), and p(C), and three statements about the probability of the data given these
three hypotheses, p(D|A), p(D|B), p(D|C). The ﬁrst three probability statements are called
prior distributions because they are unconditional from the data and therefore presumably
determined before observing the data. The second three probability statements are merely

Background and Introduction
13
PDF (probability density function) or PMF (probability mass function) statements in the
conventional sense. All this means that a posterior distribution, p(A|D), can be determined
through Bayes’ Law to look at the weight of evidence for any one of several rival hypotheses
or claims.
There is a more eﬃcient method for making statements like (1.11) when the number
of outcomes increases. Rather than label the three hypotheses as we have done above, let
us instead use θ as an unknown parameter whereby diﬀerent regions of its support deﬁne
alternative hypotheses. So statements may take the form of “Hypothesis A: θ < 0,” or any
other desired statement. To keep track of the extra outcome, denote the three hypotheses
as θi, i = 1, 2, 3. Now (1.11) is given more generally for i = 1, 2, 3 as:
p(θi|D) =
p(D|θi)p(θi)
3
j=1 p(D|θj)p(θj)
(1.12)
for the posterior distribution of θi. This is much more useful and much more in line with
standard Bayesian models in the social and behavioral sciences because it allows us to
compactly state Bayes’ Law for any number of discrete outcomes/hypotheses, say k for
instance:
p(θi|D) =
p(θi)p(D|θi)
k
j=1 p(θj)p(D|θj)
.
(1.13)
Consider also that the denominator of this expression averages over the θ variables and
therefore just produces the marginal distribution of the sample data, which we could overtly
label as p(D). Doing this provides a form that very clearly looks like the most basic form
of Bayes’ Law: p(θi|D) = p(θi)p(D|θi)/p(D). We can contrast this with the standard like-
lihood approach in the social sciences (King 1989, p.22), which overtly ignores information
available through a prior and has no use for the denominator above: L(ˆθ|y) ∝p(y|ˆθ), in
King’s notation using proportionality since the objective is simply to ﬁnd the mode and
curvature around this mode, thus making constants unimportant. Furthermore, in the con-
tinuous case, where the support of θ is over some portion of the real line, and possibly all
of it, the summation in (1.13) is replaced with an integral. The continuous case is covered
in the next chapter.
■Example 1.2:
Monty Hall.
The well-known Monty Hall problem (Selvin 1975) can
be analyzed using Bayes’ Law. Suppose that you are on the classic game show Let’s
Make a Deal with its personable host Monty Hall, and you are to choose one of three
doors, A, B, or C. Behind two of the doors are goats and behind the third door is a
new car, and each door is equally likely to award the car. Thus, the probabilities of
selecting the car for each door at the beginning of the game are simply:
p(A) = 1
3,
p(B) = 1
3,
p(C) = 1
3.
After you have picked a door, say A, before showing you what is behind that door
Monty opens another door, say B, revealing a goat. At this point, Monty gives you

14
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
the opportunity to switch doors from A to C if you want to.
What should you
do? The psychology of this approach is to suggest the idea to contestants that they
must have picked the correct door and Monty is now trying to induce a change. A
na¨ıve interpretation is that you should be indiﬀerent to switching due to a perceived
probability of 0.5 of getting the car with either door since there are two doors left.
To see that this is false, recall that Monty is not a benign player in this game. He is
deliberately trying to deny you the car. Therefore consider his probability of opening
door B. Once you have picked door A, success is clearly conditional on what door
of the three possibilities actually provides the car since Monty has this knowledge
and the contestant does not. After the ﬁrst door selection, we can deﬁne the three
conditional probabilities as follows:
The probability that Monty opens door B,
given the car is behind A:
p(BMonty|A) = 1
2
The probability that Monty opens door B,
given the car is behind B:
p(BMonty|B) = 0
The probability that Monty opens door B,
given the car is behind C:
p(BMonty|C) = 1.
Using the deﬁnition of conditional probability, we can derive the following three joint
probabilities:
p(BMonty, A) = p(BMonty|A)p(A) = 1
2 × 1
3 = 1
6
p(BMonty, B) = p(BMonty|B)p(B) = 0 × 1
3 = 0
p(BMonty, C) = p(BMonty|C)p(C) = 1 × 1
3 = 1
3.
Because there are only three possible events that cover the complete sample space, and
these events are non-overlapping (mutually exclusive), they form a partition of the
sample space. Therefore the sum of these three events is the unconditional probability
of Monty opening door B, which we obtain with the Total Probability Law:
p(BMonty) = p(BMonty, A) + p(BMonty, B) + p(BMonty, C)
= 1
6 + 0 + 1
3 = 1
2.
Now we can apply Bayes’ Law to obtain the two probabilities of interest:
p(A|BMonty) =
p(A)
p(BMonty)p(BMonty|A) =
1
3
1
2
× 1
2 = 1
3
p(C|BMonty) =
p(C)
p(BMonty)p(BMonty|C) =
1
3
1
2
× 1 = 2
3.

Background and Introduction
15
Therefore you are twice as likely to win the car if you switch to door C! This example
demonstrates that Bayes’ Law is a fundamental component of probability calculations,
and the principle will be shown to be the basis for an inferential system of statistical
analysis. For a nice generalization to N doors, see McDonald (1999).
1.5
Conditional Inference with Bayes’ Law
To make the discussion more concrete and pertinent, consider a simple problem in
sociology and crime studies. One quantity of interest to policy-makers is the recidivism rate
of prisoners released after serving their sentence. The quantity of interest is the probability
of committing an additional crime and returning to prison. Notice that this is a very elusive
phenomenon.
Not only are there regional, demographic, and individualistic diﬀerences,
but the aggregate probability is also constantly in ﬂux, given entries and exits from the
population as well as exogenous factors (such as the changing condition of the economy).
Typically, we would observe a change in law or policy at the state or federal level, and
calculate a point estimate from observed recidivism that follows.
Perhaps we should not assume that there is some ﬁxed value of the recidivism proba-
bility, A, and that it should be estimated with a single point, say ¯
A. Instead, consider this
unknown quantity in probabilistic terms as the random variable A, which means concep-
tualizing a distribution for the probability of recidivism. Looking at data from previous
periods, we might have some reasonable guess about the distribution of this probability
parameter, p(A), which is of course the prior distribution since it is not conditional on the
information at hand, B.
In all parametric statistical inference, a model is proposed and tested in which an event
has some probability of occurring given a speciﬁc value of the parameter. This is the case for
both Bayesian and traditional approaches, and is just a recognition that the researcher must
specify a data generation model. Let us call this quantity p(B|A), indicating that for posited
values of recidivism, we would expect to see a particular pattern of events. For instance, if
recidivism suddenly became much higher in a particular state, then there might be pressure
on the legislature to toughen sentencing and parole laws. This is a probability model and we
do not need to have a speciﬁc value of A to specify a parametric form (i.e., PMF or PDF).
Of course what we are really interested in is p(A|B), the (posterior) distribution of A after
having observed an event, which we obtain using Bayes’ Law: p(A|B) = p(A)
p(B)p(B|A). From
a public policy perspective, this is equivalent to asking how do recidivism rates change for
given statutes.
We are still missing one component of the right-hand-side of Bayes’ Law here, the
unconditional probability of generating the legal or policy event, p(B). This is interpretable
as the denominator of (1.13), but to a Bayesian this is an unimportant probability statement

16
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
since B has already been observed and therefore has probability one of occurring. Recall
that for Bayesians, observed quantities are ﬁxed and unobserved quantities are assigned
probability statements. So there is no point in treating B probabilistically if the actual
facts are sitting on our desk right now. This does not mean that everything is known about
all possible events, missing events, or events occurring in the future. It just means that
everything is known about this event. So the only purpose for p(B) in this context is to
make sure that p(A|B) sums or integrates to one.
This last discussion suggests simply treating p(B) as a normalizing constant since it
does not change the relative probabilities for A. Maybe this is a big conceptual leap, but
if we could recover unconditional p(B) later, it is convenient to just use it then to make
the conditional statement, p(A|B), a properly scaled probability statement. So if p(A|B)
summed or integrated to ﬁve instead of one, we would simply divide everywhere by ﬁve and
lose nothing but the agony of carrying p(B) through the calculations. If we temporarily
ignore p(B), then:
p(A|B) ∝p(A)p(B|A),
(1.14)
where “∝” means “proportional to” (i.e., the relative probabilities are preserved). So the
ﬁnal estimated probability of recidivism (in our example problem) given some observed
behavior, is proportional to prior notions about the distribution of the probability times
the parametric model assumed to be generating the new observed event. The conditional
probability of interest on the left-hand side of (1.14) is a balance between things we have
already seen or believe about recidivism, p(A), and the contribution from the new observa-
tion, p(B|A). It is important to remember that there are occasions where the data are more
inﬂuential than the prior and vice-versa. This is comforting since if the data are poor in size
or information we want to rely more on prior knowledge, prior research, researcher or prac-
titioner information and so on. Conversely, if the data are plentiful and highly informed,
then we should not care much about the form of the prior information. Remarkably, the
Bayesian updating process in (1.14) has this trade-oﬀautomatically built-in to the process.
As described, this is an ideal paradigm for inference in the social and behavioral sciences,
since it is consentaneously desirable to build models that test theories with newly observed
events or data, but also based on previous research and knowledge. We never start a data
analysis project with absolutely no a priori notions whatsoever about the state of nature (or
at least we should not!). This story actually gets better. As the number of events increases,
p(B|A) becomes progressively more inﬂuential in determining p(A|B). That is, the greater
the number of our new observations, the less important are our previous convictions: p(A).
Also, if either of the two distributions, p(A) and p(B|A), are widely dispersed relative to the
other, then this distribution will have less of an impact on the ﬁnal probability statement.
We will see this principle detailed-out in Chapter 2. The natural weighting of these two
distributions suitably reﬂects relative levels of uncertainty in the two quantities.

Background and Introduction
17
1.5.1
Statistical Models with Bayes’ Law
The statistical role of the quantities in (1.14) has not yet been identiﬁed since we have
been talking abstractly about “events” rather than conventional data. The goal of inference
is to make claims about unknown quantities using data currently in hand. Suppose that we
designate a generic Greek character to denote an unobserved parameter that is the objective
of our analysis. As is typical in these endeavors, we will use θ for this purpose. What we
usually have available to us is generically (and perhaps a little vaguely) labeled D for data.
Therefore, the objective is to obtain a probabilistic statement about θ given D: p(θ|D).
Inferences in this book, and in the majority of Bayesian and non-Bayesian statistics, are
made by ﬁrst specifying a parametric model for the data generating process. This deﬁnes
what the data should be expected to look like given a speciﬁc probabilistic function con-
ditional on unknown variable values. These are the common probability density functions
(continuous data) and probability mass functions (discrete data) that we already know,
such as normal, binomial, chi-square, etc., denoted by p(D|θ).
Now we can relate these two conditional probabilities using (1.14):
π(θ|D) ∝p(θ)p(D|θ),
(1.15)
where p(θ) is a formalized statement of the prior knowledge about θ before observing the
data. If we know little, then this prior distribution should be a vague probabilistic statement
and if we know a lot then this should be a very narrow and speciﬁc claim. The right-
hand side of (1.15) implies that the post-data inference for θ is a compromise between
prior information and the information provided by the new data, and the left-hand side of
(1.15) is the posterior distribution of θ since it provides the updated distribution for θ after
conditioning on the data.
Bayesians describe π(θ|D) to readers via distributional summaries such as means, modes,
quantiles, probabilities over regions, traditional-level probability intervals, and graphical
displays. Once the posterior distribution has been calculated via (1.15), everything about
it is known and it is entirely up to the researcher to highlight features of interest. Often it
is convenient to report the posterior mean and variance in papers and reports since this is
what non-Bayesians do by default. We can calculate the posterior mean using an expected
value calculation, conﬁning ourselves here to the continuous case:
E[θ|D] =
∞

−∞
θπ(θ|D)dθ
(1.16)

18
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
and the posterior variance via a similar process:
Var[θ|D] = E
	
(θ −E[θ|D])2|D

=
∞

−∞
(θ −E[θ|D])2π(θ|D)dθ
=
∞

−∞

θ2 −2θE[θ|D]) + E[θ|D]2
π(θ|D)dθ
=
∞

−∞
θ2π(θ|D)dθ −2E[θ|D]
∞

−∞
θπ(θ|D)dθ +
⎛
⎝
∞

−∞
θπ(θ|D)dθ
⎞
⎠
2
= E[θ2|D] −E[θ|D]2
(1.17)
given some minor regularity conditions about switching the order of integration (see Casella
and Berger 2002, Chapter 1). An obvious summary of this posterior would then be the
vector (E[θ|D],

Var[θ|D]), although practicing Bayesians tend to prefer reporting more
information.
Researchers sometimes summarize the Bayesian posterior distribution in a deliberately
traditional, non-Bayesian way in an eﬀort to communicate with some readers. The posterior
mode corresponds to the maximum likelihood point estimate and is calculated by:
M(θ) = argmax
θ
π(θ|D),
(1.18)
where argmax function speciﬁes the value of θ that maximizes π(θ|D).
Note that the
denominator of Bayes’ Law is unnecessary here since the function has the same mode with or
with including it. The accompanying measure of curvature (e.g., Fisher Information, deﬁned
in Appendix A) can be calculated with standard analytical tools or more conveniently from
MCMC output with methods introduced in Chapter 9. The posterior median is a slightly
less popular choice for a Bayesian point estimate, even though its calculation from MCMC
output is trivial from just sorting empirical draws and determining the mid-point.
■Example 1.3:
Summarizing a Posterior Distribution from Exponential Data.
Suppose we had generic data, D, distributed p(D|θ) = θe−θD, which can be either a
single scalar or a vector for our purposes. Thus D is exponentially distributed with the
support [0:∞); see Appendix A for details on this probability density function. We
also need to specify a prior distribution for θ: p(θ) = 1, where θ ∈[0:∞). Obviously
this prior distribution does not constitute a “proper” distribution in the Kolmogorov
sense since it does not integrate to one (inﬁnity, in fact).
We should not let this
bother us since this eﬀect is canceled out due to its presence in both the numerator
and denominator of Bayes’ Law (a principle revisited in Chapter 4 in greater detail).

Background and Introduction
19
This type of prior is often used to represent high levels of prior uncertainty, although
it is not completely uninformative. Using (1.15) now, we get:
π(θ|D) ∝p(θ)p(D|θ) = (1)θe−θD = θe−θD.
(1.19)
This posterior distribution has mean:
E[θ|D] =
∞

0
(θ)

θe−θD
dθ =
2
D3 ,
(1.20)
which is found easily using two iterations of integration-by-parts. Also, the expecta-
tion of θ2|D is:
E[θ2|D] =
∞

0
(θ2)

θe−θD
dθ =
6
D4 ,
(1.21)
which is found using three iterations of integration-by-parts now. So the posterior
variance is:
Var[θ|D] = E[θ2|D] −E[θ|D]2 = 6D−4 −4D−6.
(1.22)
The notation would be slightly diﬀerent if D were a vector.
Using these quantities we can perform an intuitive Bayesian hypothesis test, such
as asking what is the posterior probability that θ is positive p(θ|D) > 0.
In the
context of a regression coeﬃcient, this would be the probability that increases in
the corresponding X explanatory variable have a positive aﬀect on the Y outcome
variable. Testing will be discussed in detail in Chapter 7. We can also use these
derived quantities to create a Bayesian version of a conﬁdence interval, the credible
interval for some chosen α level:

E[θ|D] −

Var[θ|D]fα/2:E[θ|D] +

Var[θ|D]f1−α/2

,
(1.23)
where fα/2 and f1−α/2 are lower and upper tail values for some assumed or empirically
observed distribution for θ (Chapter 2).
The purpose of this brief discussion is to highlight the fact that conditional probability
underlies the ability to update previous knowledge about the distribution of some unknown
quantity. This is precisely in line with the iterative scientiﬁc method, which postulates
theory improvement through repeated speciﬁcation and testing with data. The Bayesian
approach combines a formal structure of rules with the mathematical convenience of prob-
ability theory to develop a process that “learns” from the data. The result is a powerful
and elegant tool for scientiﬁc progress in many disciplines.

20
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
1.6
Science and Inference
This is a book about the scientiﬁc process of discovery in the social and behavioral
sciences. Data analysis is best practiced as a theory-driven exploration of collected obser-
vations with the goal of uncovering important and unknown eﬀects. This is true regardless
of academic discipline. Yet some ﬁelds of study are considered more rigorously analytical
in this pursuit than others.
The process described herein is that of inference: making probabilistic assertions about
unknown quantities. It is important to remember that “in the case of uncertain inference,
however, the very uncertainty of uncertain predictions renders question of their proof or
disproof almost meaningless” (Wilkinson 1977). Thus, confusion sometimes arises in the
interpretation of the inferential process as a scientiﬁc, investigative endeavor.
1.6.1
The Scientiﬁc Process in Our Social Sciences
Are the social and behavioral sciences truly “scientiﬁc”? This is a question asked about
ﬁelds such as sociology, political science, economics, anthropology, and others. It is not
a question about whether serious, rigorous, and important work has been done in these
endeavors; it is a question about the research process and whether it conforms to the
empirico-deductive model that is historically associated with the natural sciences. From a
simplistic view, this is an issue of the conformance of research in the social and behavioral
sciences to the so-called scientiﬁc method.
Brieﬂy summarized, the scientiﬁc method is
characterized by the following steps:
▷Observe or consider some phenomenon.
▷Develop a theory about the cause(s) of this phenomenon and articulate it in a speciﬁc
hypothesis.
▷Test this hypothesis by developing a model to ﬁt experimentally generated or collected
observational data.
▷Assess the quality of the ﬁt to the model and modify the theory if necessary, repeating
the process.
This is sometimes phrased in terms of “prediction” instead of theory development, but we
will use the more general term.
If the scientiﬁc method as a process were the deﬁning
criterion for determining what is scientiﬁc and what is not, then it would be easy to classify
a large proportion of the research activities in the social and behavioral sciences as scientiﬁc.
However useful this typology is in teaching children about empirical investigation, it is a
poor standard for judging academic work.
Many authors have posited more serviceable deﬁnitions. Braithwaite (1953, p.1) notes:

Background and Introduction
21
The function of a science, in this sense of the word, is to establish general laws covering the
behavior of the empirical events or objects with which the science in question is concerned,
and thereby to enable us to connect together our knowledge of the separately known events,
and to make reliable predictions of events as yet unknown.
The core of this description is the centrality of empirical observation and subsequent ac-
cumulation of knowledge. Actually, “science” is the Latin word for knowledge. Legendary
psychologist B. F. Skinner (1953, p.11) once observed that “science is unique in showing
a cumulative process.” It is clear from the volume and preservation of published research
that social and behavioral scientists are actively engaged in empirical research and knowl-
edge accumulation (although the quality and permanence of this foundational knowledge
might be judged to diﬀer widely by ﬁeld). So what is it about these academic pursuits that
makes them only suspiciously scientiﬁc to some? The three deﬁning characteristics about
the process of scientiﬁc investigation are empiricism, objectivity, and control (Singleton and
Straight 2004). This is where there is lingering and sometimes legitimate criticism of the
social and behavioral sciences as being “unscientiﬁc.”
The social and behavioral sciences are partially empirical (data-oriented) and partially
normative (value-oriented), the latter because societies develop norms about human behav-
ior, and these norms permeate academic thought prior to the research process. For instance,
researchers investigating the onset and development of AIDS initially missed the eﬀects of
interrelated social factors such as changes in behavioral risk factors, personal denial, and
reluctance to seek early medical care on the progress of the disease as a sociological phe-
nomenon (Kaplan et al. 1987). This is partially because academic investigators as well as
health professionals made normative assumptions about individual responses to sociological
eﬀects. Speciﬁcally, researchers investigating human behavior, whether political, economic,
sociological, psychological, or otherwise, cannot completely divorce their prior attitudes
about some phenomenon of interest the way a physicist or chemist can approach the study
of the properties of thorium: atomic number 90, atomic symbol Th, atomic weight 232.0381,
electron conﬁguration [Rn]7s26d2. This criticism is distinct from the question of objectivity;
it is a statement that students of human behavior are themselves human.
We are also to some extent driven by the quality and applicability of our tools. Many
ﬁelds have radically progressed after the introduction of new analytical devices. Therefore,
some researchers may have a temporary advantage over others, and may be able to answer
more complex questions: “It comes as no particular surprise to discover that a scientist
formulates problems in a way which requires for their solution just those techniques in
which he himself is especially skilled” (Kaplan 1964). The objective of this book is to “level
the pitch” by making an especially useful tool more accessible to those who have thus far
been accordingly disadvantaged.

22
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
1.6.2
Bayesian Statistics as a Scientiﬁc Approach to Social and
Behavioral Data Analysis
The standard frequentist interpretation of probability and inference assumes an inﬁnite
series of trials, replications, or experiments using the same research design. The “objec-
tivist” paradigm is typically explained and justiﬁed through examples like multiple tosses
of a coin, repeated measurements of some physical quantity, or samples from some ongoing
process like a factory output. This perspective, which comes directly from Neyman and
Pearson (1928a, 1928b, 1933a, 1933b, 1936a, 1936b), and was formalized by Von Mises
(1957) among others, is combined with an added Fisherian ﬁxation with p-values in typical
inference in the social and behavioral sciences (Gill 1999). Efron (1986), perhaps overly
kindly, calls this a “rather uneasy alliance.”
Very few, if any, social scientists would be willing to seriously argue that human behav-
ior ﬁts this objectivist long-run probability model. Ideas like “personal utility,” “legislative
ideal points,” “cultural inﬂuence,” “mental states,” “personality types,” and “principal-
agent goal discrepancy” do not exist as parametrically uniform phenomena in some phys-
ically tangible manner. In direct contrast, the Bayesian or “subjective” conceptualization
of probability is the degree of belief that the individual researcher is willing to personally
assign and defend.
This is the idea that an individual personally assigns a probability
measure to some event as an expression of uncertainty about some event that may only be
relevant to one observational situation or experiment.
The central idea behind subjective probability is the assignment of a prior probability
based on what information one currently possesses and under what circumstances one would
be willing to place an even wager. Naturally, this probability is updated as new events
occur, therefore incorporating serial events in a systematic manner. The core disagreement
between the frequentist notion of objective probability and the Bayesian idea of subjective
probability is that frequentists see probability measure as a property of the outside world
and Bayesians view probability as a personal internalization of observed uncertainty. The
key defense of the latter view is the inarguable point that all statistical models are subjective:
decisions about variable speciﬁcations, signiﬁcance thresholds, functional forms, and error
distributions are completely nonobjective.1
In fact, there are instances when Bayesian
subjectivism is more “objective” than frequentist objectivism with regard to the impact
of irrelevant information and arbitrary decision rules (e.g., Edwards, Lindman, and Savage
1963, p.239).
1As a brief example, consider common discussions of reported analyses in social science journals and
books that talk about reported model parameters being “of the wrong sign.” What does this statement
mean? The author is asserting that the statistical model has produced a regression coeﬃcient that is positive
when it was a priori expected to be negative or vice versa. What is this statement in eﬀect? It is a prior
statement about knowledge that existed before the model was constructed. Obviously this is a form of the
Bayesian prior without being speciﬁcally articulated as such.

Background and Introduction
23
Given the existence of subjectivity in all scientiﬁc data analysis endeavors,2 one should
prefer the inferential paradigm that gives the most overt presentation of model assump-
tions. This is clearly the Bayesian subjective approach since both prior information and
posterior uncertainty are given with speciﬁc, clearly stated model assumptions. Conversely,
frequentist models are rarely presented with caveats such as “Caution: the scientiﬁc conclu-
sions presented here depend on repeated trials that were never performed,” or “Warning:
prior assumptions made in this model are not discussed or clariﬁed.” If there is a single
fundamental scientiﬁc tenet that underlies the practice and reporting of empirical evidence,
it is the idea that all important model characteristics should be provided to the reader. It
is clear then which of the two approaches is more “scientiﬁc” by this criterion. While this
discussion speciﬁcally contrasts Bayesian and frequentist approaches, likelihood inference is
equally subjective in every way, and as already explained, ignores available information.
These ideas of what sort of inferences social scientists make are certainly not new or
novel. There is a rich literature to support the notion that the Bayesian approach is more
in conformance with widely accepted scientiﬁc norms and practices. Poirer (1988, p.130)
stridently makes this point in the case of prior speciﬁcations:
I believe that subjective prior beliefs should play a formal role so that it is easier
to investigate their impact on the results of the analysis. Bayesians must live
with such honesty whereas those who introduce such beliefs informally need not.
The core of this argument is the idea that if the prior contains information that pertains
to the estimation problem, then we are foolish to ignore it simply because it does not
neatly ﬁt into some familiar statistical process. For instance, Theil and Goldberger (1961)
suggested “mixed” estimation some time ago, which is a way to incorporate prior knowledge
about coeﬃcients in a standard linear regression model by mixing earlier estimates into the
estimation process and under very general assumptions is found to be simultaneously best
linear unbiased with respect to both sample and prior information (see also Theil [1963]).
This notion of combining information from multiple sources is not particularly controversial
among statisticians, as observed by Samaniego and Reneau (1994, p.957):
If a prior distribution contains “useful” information about an unknown param-
eter, then the Bayes estimator with respect to that prior will outperform the
best frequentist rule. Otherwise, it will not.
A more fundamental advantage to Bayesian statistics is that both prior and posterior pa-
rameter estimates are assumed to have a distribution and therefore give a more realistic
picture of uncertainty that is also more useful in applied work:
With conventional statistics, the only uncertainty admitted to the analysis is
2See Press and Tanur (2001) for a fascinating account of the role of researcher-introduced subjectivity
in a number of speciﬁc famous scientiﬁc breakthroughs, including discoveries by Galileo, Newton, Darwin,
Freud, and Einstein.

24
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
sampling uncertainty. The Bayesian approach oﬀers guidance for dealing with
the myriad sources of uncertainty faced by applied researchers in real analyses.
Western (1999, p.20). Lindley (1986, p.7) expresses a more biting statement of preference:
Every statistician would be a Bayesian if he took the trouble to read the litera-
ture thoroughly and was honest enough to admit he might have been wrong.
This book rests on the perspective, sampled above, that the Bayesian approach is not only
useful for social and behavioral scientists, but it also provides a more compatible methodol-
ogy for analyzing data in the manner and form in which it arrives in these disciplines. As we
describe in subsequent chapters, Bayesian statistics establishes a rigorous analytical plat-
form with clear assumptions, straightforward interpretations, and sophisticated extensions.
For more extended discussions of the advantages of Bayesian analysis over alternatives,
see Berger (1986b), Dawid (1982), Efron (1986), Good (1976), Jaynes (1976), and Zellner
(1985). We now look at how the Bayesian paradigm emerged over the last 250 years.
1.7
Introducing Markov Chain Monte Carlo Techniques
In this section we brieﬂy discuss Bayesian computation and give a preview of later
chapters. The core message is that these algorithms are relatively simple to understand in
the abstract.
Markov chain Monte Carlo (MCMC) set the Bayesians free. Prior to 1990, it was rela-
tively easy to specify an interesting and realistic model with actual data whereby standard
results were unobtainable. Speciﬁcally, faced with a high dimension posterior resulting from
a regression-style model, it was often very diﬃcult or even impossible to perform multiple
integration across the parameter space to produce a regression table of marginal summaries.
The purpose of MCMC techniques is to replace this diﬃcult analytical integration process
with iterative work by the computer. When calculations similar to (1.16) are multidimen-
sional, there is a need to summarize each marginal distribution to provide useful results
to readers in a table or other format for journal submission. The basic principle behind
MCMC techniques is that if an iterative chain of computer-generated values can be set up
carefully enough, and run long enough, then empirical estimates of integral quantities of
interest can be obtained from summarizing the observed output. If each visited multidimen-
sional location is recorded as a row vector in a matrix, then the marginalization for some
parameter of interest is obtained simply by summarizing the individual dimension down
the corresponding column. So we replace an analytical problem with a sampling problem,
where the sampling process has the computer perform the diﬃcult and repetitive processes.
This is an enormously important idea to Bayesians and to others since it frees researchers

Background and Introduction
25
from having to make artiﬁcial simpliﬁcations to their model speciﬁcations just to obtain
describable results.
These Markov chains are successive quantities that depend probabilistically only on the
value of their immediate predecessor: the Markovian property. In general, it is possible to
set up a chain to estimate multidimensional probability structures (i.e., desired probability
distributions), by starting a Markov chain in the appropriate sample space and letting it
run until it settles into the target distribution. Then when it runs for some time conﬁned
to this particular distribution, we can collect summary statistics such as means, variances,
and quantiles from the simulated values. This idea has revolutionized Bayesian statistics by
allowing the empirical estimation of probability distributions that could not be analytically
calculated.
1.7.1
Simple Gibbs Sampling
As a means of continuing the discussion about conditional probability and covering some
basic principles of the R language, this section introduces an important, and frequently used
Markov chain Monte Carlo tool, the Gibbs sampler. The idea behind a Gibbs sampler is to
get a marginal distribution for each variable by iteratively conditioning on interim values
of the others in a continuing cycle until samples from this process empirically approximate
the desired marginal distribution. Standard regression tables that appear in journals are
simply marginal descriptions. There will be much more on this topic in Chapter 10 and
elsewhere, but here we will implement a simple but instructive example.
As outlined by Example 2 in Casella and George (1992), suppose that we have two
conditional distributions, where they are conditional on each other such that the parameter
of one is the variable of interest in the other:
f(x|y) ∝y exp[−yx],
f(y|x) ∝x exp[−xy],
0 < x, y < B < ∞.
(1.24)
These conditional distributions are both exponential probability density functions (see Ap-
pendix B for details). The upper bound, B, is important since without it there is no ﬁnite
joint density and the Gibbs sampler will not work. It is possible, but not particularly pleas-
ant, to perform the correct integration steps to obtain the desired marginal distributions:
f(x) and f(y). Instead we will let the Gibbs sampler do the work computationally rather
than us do it analytically.
The Gibbs sampler is deﬁned by ﬁrst identifying conditional distributions for each pa-
rameter in the model. These are conditional in the sense that they have dependencies on
other parameters, and of course the data, which emerge from the model speciﬁcation. The
“transition kernel” for the Markov chain is created by iteratively cycling through these dis-
tributions, drawing values that are conditional on the latest draws of the dependencies. It
is proven that this allows us to run a Markov chain that eventually settles into the desired
limiting distribution that characterizes the marginals. In other language, it is an iterative
process that cycles through conditional distributions until it reaches a stable status whereby

26
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
future samples characterize the desired distributions. The important theorem here assures
us that when we reach this stable distribution, the autocorrelated sequence of values can
be treated as an iid sample from the marginal distributions of interest. The amazing part
is that this is accomplished simply by ignoring the time index, i.e., putting the values in a
“bag” and just “shaking it up” to lose track of the order of occurrence. Gibbs sampling is
actually even more general than this. Chib (1995) showed how Gibbs sampling can be used
to compute the marginal distribution of the sample data, i.e., the denominator of (1.13),
by using the individual parameter draws. This quantity is especially useful in Bayesian
hypothesis testing and model comparison, as we shall see in Chapter 7. The second half of
this text applies this tool and similar methods of estimation.
For two parameters, x and y, this process involves a starting point, [x0, y0], and the
cycles are deﬁned by drawing random values from the conditionals according to:
x1 ∼f(x|y0),
y1 ∼f(y|x1)
x2 ∼f(x|y1),
y2 ∼f(y|x2)
x3 ∼f(x|y2),
y3 ∼f(y|x3)
:
:
:
:
xm ∼f(x|ym−1),
ym ∼f(y|xm).
If we are successful, then after some reasonable period the values xj, yj are safely assumed
to be empirical samples from the correct marginal distribution. There are many theoretical
and practical concerns that we are ignoring here, and the immediate objective here is to
give a rough overview.
The following steps indicate how the Gibbs sampler is set up and run:
▷Set the initial values: B = 10, and m = 50, 000. B is the parameter that ensures that
the joint distribution is ﬁnite, and m is the desired number of generated values for x
and y.
▷Create x and y vectors of length m where the ﬁrst value of each is a starting point
uniformly distributed over the support of x and y, and all other vector values are ﬁlled
in with unacceptable entries greater than B.
▷Run the chain for m = 50, 000 −1 iterations beginning at the starting points. At
each iteration, ﬁll-in and save only sampled exponential values that are less than B,
repeating this sampling procedure until an acceptable value is drawn to replace the
unacceptable B + 1 in that position.
▷Throw away some early part of the chain where it has not yet converged.
▷Describe the marginal distributions of x and y with the remaining empirical values.
This leads to the following R code, which can be retyped verbatim, obtained from the
book’s webpage, or the book’s R package BaM to replicate this example:

Background and Introduction
27
B <- 10; m <- 50000
gibbs.expo <- function(B,m) {
x <- c(runif(1,0,B),rep((B+1),length=(m-1)))
y <- c(runif(1,0,B),rep((B+1),length=(m-1)))
for (i in 2:m)
{
while(x[i] > B) x[i] <- rexp(1,y[i-1])
while(y[i] > B) y[i] <- rexp(1,x[i])
}
return(cbind(x,y))
}
gibbs.expo(B=5, m=500)
0
2
4
6
8
10
0.0
0.2
0.4
0.6
0.8
Marginal Distribution of x
0
2
4
6
8
10
0.0
0.2
0.4
0.6
0.8
Marginal Distribution of y
FIGURE 1.1: Gibbs Sampling, Marginal Exponentials
These samples are summarized by histograms of the empirical results for x and y in
Figure 1.1, where m = 50, 000 samples are drawn and the ﬁrst 40, 000 are discarded (these
are called “burn-in” values).
It is clear from the ﬁgure that the marginal distributions
are exponentially distributed. We can recover parameters by using the empirical draws to
calculate sample statistics. This part of the MCMC process is actually quite trivial once
we are convinced that there has been convergence of the Markov chain. In later chapters
we will see this process in a more realistic, and therefore detailed, setting. This example
is intended to give an indication of activities to come and to reinforce the linkage between
Bayesian inference and modern statistical computing.
1.7.2
Simple Metropolis Sampling
Another Markov chain Monte Carlo tool with wide use is the Metropolis algorithm from
statistical physics (Metropolis et al. 1953). The Metropolis algorithm is more ﬂexible than

28
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
Gibbs sampling because it works with the joint distribution rather than a full listing of
conditional distributions for the parameters in the model. As a result, many variations
have been developed to satisfy particular sampling challenges posed by complicated models
and ill-behaved target functions. Later chapters will cover these extensions in detail.
The essential idea behind the Metropolis algorithm is that, while we cannot easily gen-
erate values from the joint (posterior) distribution of interest, we can often ﬁnd a “similar”
distribution that is easy to sample from. Obviously we need to make sure that this alterna-
tive distribution is deﬁned over the same support as the target distribution and that it does
not radically favor areas of low density of this target. Once a candidate point in multivariate
space has been produced by this candidate-generating distribution we will accept or reject
it based upon characteristics of the target distribution. The algorithm is characterized by
the following steps.
1. The candidate-generating distribution proposes that we move to some other point by
drawing a point from its generating mechanism.
2. If this point produces a step on the target distribution that is of higher density, then
we will always go there.
3. If this point produces a step on the target distribution that is of lower density, then
we will go there probabilistically proportional to how much lower the step is in density.
Thus it is easy to see that the Markov chain “wanders around” the target density describing
it as it goes and favoring higher density regions. The nice part is that the Markov chain will
also explore other lower density regions as well, but with lower probability as we would want.
Analogously, consider locking a house cat in large room with features that are attractive to
cats (the high density regions of the posterior), and features that are unattractive to cats
(the low density regions of the posterior). Anyone who has spent time with house cats can
see at least some Markovian feature to their nature, as well as an innate curiosity. As our
feline Markov chain wanders the room in a memory-less state, we record the coordinates of
their travel. Over time we will ﬁnd that the cat spends more time in the attractive areas, but
still occasionally investigates the unattractive areas. If this attractiveness is proportional to
density we want to describe, then the cat eventually produces description of the posterior
distribution.
We can more precisely describe this algorithm. Suppose we have a two-dimensional
target distribution, p(x, y), which can be a posterior distribution from a Bayesian model, or
any other form that is hard to marginalize, i.e., produce individual distributions p(x) and
p(y). A single Metropolis step is produced by:
1. Sample (x′, y′) from the candidate-generating distribution, q(x′, y′).
2. Sample a value u from u[0 : 1].
3. If
a(x′, y′|x, y) = p(x′, y′)
p(x, y) > u
then accept (x′, y′) as the new destination.
4. Otherwise keep (x, y) as the new destination.

Background and Introduction
29
The result is a chain of values, [(x0, y0), (x1, y1), (x2, y2), . . .]. Note that unlike the Gibbs
sampler, the Metropolis algorithm does not necessarily have to move to a new position at
each iteration, and the decision to stay put is considered a Markovian step to the current
position (time is consumed by the step).
There are a few technical details that we will worry about in much more detail later
beginning in Chapter 10. Often the candidate-generating distribution produces values con-
ditional on the current position, q(x′, y′|x, y), but this is not strictly necessary. The basic
version described here requires that the candidate-generating distribution be symmetrical
in its arguments, q(x′, y′|x, y) = q(x, y|x′, y′). Also, the choice of candidate-generating dis-
tribution can be complicated by the need to match irregularities in the target distribution.
Finally, it is important in real applications to run the Markov chain for some initial period
to let it settle into the distribution of interest before recording values.
Consider a problem similar to that above, but where we have a joint distribution for the
parameters and not the desired marginals (or conditionals as used in the Gibbs sampler).
The bivariate exponential distribution for x, y ∈[0:∞] is given by the function:
p(x, y) = exp[−(λ1 + λ)x −(λ2 + λ)y −λ max(x, y)],
(1.25)
with non-negative parameters λ1, λ2, and λ. This model is common in reliability analysis
(Marshall and Olkin 1967), where the interpretation is that the ﬁrst two parameters are
the event intensities for systems 1 and 2, and the non-subscripted parameter is the shared
intensity between systems. In this literature events are usually machine failures, but for
our purposes they can be death, graduation, cabinet dissolutions, divorce, cessation of war,
and so on. In this example we have the parameters:
λ1 = 0.5,
λ2 = 0.5,
λ = 0.01,
B = max(x) = max(y) = 8,
which produces the bivariate distribution shown in the ﬁrst panel of Figure 1.2. The maxi-
mum in the function makes it a little harder to analytically calculate marginal distributions
with integration, so we might want to apply MCMC to save trouble. This is exactly anal-
ogous to the process where complicated Bayesian model speciﬁcations sometimes make it
diﬃcult to describe marginal posteriors for parameters of interest.
To implement the Metropolis algorithm we need a candidate-generating distribution
from which to draw potential destinations for the Markov chain. Typically researchers look
for some convenient distribution from the commonly used form since software such as R
makes drawing values trivial. Here we will exploit the stipulated bounds on the problem
and note that the bivariate exponential is enclosed in a big box with length and width equal
to B = 8 and maximum height equal to one from the form of (1.25). The process is further
covered in Chapter 9, but note here that it is easy to draw points inside this box from scaled
uniforms. Nicely, we do not have to rescale the distribution of q(x′, y′) because the values
are drawn from this distribution but inserted into p(). It is important to note, without
getting too far ahead of ourselves, that a better ﬁtting candidate-generating distribution
could be found and that drawing from uniform boxes is not particularly eﬃcient.

30
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
To begin we deﬁne our function in R according to:
biv.exp <- function(x,y,L1,L2,L)
exp( -(L1+L)*x - (L2+L)*y -L*max(x,y) )
So it will return density values for given (x, y) pairs and speciﬁc parameters. The candidate-
generating function is:
cand.gen <- function(max.x,max.y)
c(runif(1,0,max.x),runif(1,0,max.y))
where we could have stipulated the B value but left the function slightly more general.
Markov chains require starting positions and we arbitrarily select (x = 0.5, y = 0.5 here.
The algorithm is now given to be the following R code, which (again) can be retyped
verbatim to replicate the example:
m <-5000; x<-0.5; y<-0.5; L1<-0.5; L2<-0.5; L<-0.01; B<-8
for (i in 1:m)
{
cand.val <- cand.gen(B,B)
a <- biv.exp(cand.val[1],cand.val[2],L1,L2,L)
/ biv.exp(x[i],y[i],L1,L2,L)
if (a > runif(1)) {
x <- c(x,cand.val[1])
y <- c(y,cand.val[2])
}
else
{
x <- c(x,x[i])
y <- c(y,y[i])
}
}
The resulting values are shown by the histograms in the second and third panels of Fig-
ure 1.2, where the algorithm has been run for m = 5, 000 iterations but the ﬁrst 3, 000 are
discarded. We could also simply summarize the resulting marginals for x and y empirically
with means, quantiles, or other simple statistics. The Metropolis algorithm shown here
will be expanded and generalized in Chapter 10 by loosening restrictions on the candidate-
generating distribution and allowing for hybrid processes that accommodate diﬃcult fea-
tures in the target distribution. The two MCMC algorithms described here form the basis
for all practical work needed to estimate complex Bayesian models in the social sciences.
1.8
Historical Comments
Statistics is a relatively new ﬁeld of scientiﬁc endeavor. In fact, for much of its history
it was subsumed to various natural sciences as a combination of foster-child and household

Background and Introduction
31
x
y
density
0.00
0.02
0.04
0.06
0.08
0.10
0
1
2
3
Marginal Distribution of x
0.00
0.02
0.04
0.06
0.08
0
1
2
3
Marginal Distribution of y
FIGURE 1.2: Metropolis Sampling, Bivariate Exponential
maid: unwanted by its natural parents (mathematics and philosophy), yet necessary to
clean things up. Beginning with the work of Laplace (1774, 1781, 1811), Gauss (1809, 1823,
1855), Legendre (1805), and de Morgan (1837, 1838, 1847), statistics began to emerge as
a discipline worthy of study on its own merits. The ﬁrst renaissance occurred around the
turn of the last century due to the monumental eﬀorts of Galton (1869, 1875, 1886, 1892),
Fisher (1922, 1925a, 1925b, 1934), Neyman and (Egon) Pearson (1928a, 1928b, 1933a,
1933b, 1936a, 1936b), Gossett (as Student, 1908a, 1908b), Edgeworth (1892a, 1892b, 1893a,
1893b), (Karl) Pearson (1892, 1900, 1907, 1920), and Venn (1866). Left out of the twin
intellectual developments of frequentist inference from Neyman and Pearson and likelihood
inference from Fisher (see Chapter 2, Section 2.6 for details), was the Bayesian paradigm.
Sir Thomas Bayes’ famous (and only) essay was published in 1763, two years after his death
(Bayes chose to perish before publishing), suggesting to some that he was ambivalent about
the approach of applying a uniform prior to a binomial probability parameter. This inge-
nious work unintendedly precipitated a philosophy about how researcher-speciﬁed theories
are ﬁt to empirical observations. Interestingly, it was not until the early 1950s that Bayesian
statistics became a self-aware branch (Fienberg 2006).
Fisher in particular was hostile to the Bayesian approach and was often highly critical,
though not always with substantiated claims: Bayesianism “which like an impenetrable
jungle arrests progress towards precision of statistical concepts” (1922, p.311). Fisher also
worked to discredit Bayesianism and inverse probability (Bayesianism with an assumed
uniform prior) by pressuring peers and even misquoting other scholars (Zabell 1989). Yet
Fisher (1935) develops ﬁducial inference, which is an attempt to apply inverse probability

32
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
without uniform priors, but this approach fails; Efron (1998, p.105) calls this “Fisher’s
biggest blunder.” In fact, Lindley (1958) later proved that ﬁducial inference is consistent
only when it is made equivalent to Bayesian inference with a uniform prior. The Neyman-
Pearson paradigm was equally unkind to the development of Bayesian statistics, albeit on
a less vindictive level. If one is willing to subscribe to the idea of an inﬁnite sequence of
samples, then the Bayesian prior is unimportant since the data will overwhelm this prior.
Although there are scenarios where this is a very reasonable supposition, generally these
are far more diﬃcult to come by in the social and behavioral sciences.
Although Bayesianism had suﬀered “a nearly lethal blow” from Fisher and Neyman
by the 1930s (Zabell 1989), it was far from dead. Scholars such as Jeﬀreys (1961), Good
(1950), Savage (1954, 1962), de Finetti (1972, 1974, 1975), and Lindley (1961, 1965) re-
activated interest in Bayesian methods in the middle of the last century in response to
observed deﬁciencies in classical techniques. Lindley and Novick (1978, 1981) published
important applied work in education psychology that carefully studied exchangeability and
utility from a Bayesian perspective, and Novick et al. (1976) developed an early Bayesian
software program for estimating simple models: CADA, Computer Assisted Data Analy-
sis. Unfortunately many of the speciﬁcations developed by these modern Bayesians, while
superior in theoretical foundation, led to mathematical forms that were intractable.3 For-
tunately, this problem has been largely resolved in recent years by a revolution in statistical
computing techniques, and this has led to a second renaissance for the Bayesian paradigm
(Berger 2001).
Markov chain Monte Carlo (MCMC) techniques solve a lingering problem in Bayesian
analysis, and thus earn a special place in this work. Often Bayesian model speciﬁcations
considered either interesting or realistic produced inference problems that were analytically
intractable because they led to high-dimension integral calculations that were impossible
to solve analytically. Previous numerical techniques for performing these integrations were
often diﬃcult and highly specialized tasks (e.g., Shaw 1988, Stewart and Davis 1986, van
Dijk and Kloek 1982, Tierney and Kadane 1986).
Beginning with the foundational work
of Metropolis et al. (1953), Hastings (1970), Peskun (1973), Geman and Geman (1984),
and the critical synthesizing essay of Gelfand and Smith (1990), there is now a voluminous
literature on Markov chain Monte Carlo. In fact, modern Bayesian statistical practice is
intimately and intrinsically tied to stochastic simulation techniques and as a result, these
tools are an integral part of this book. We introduce these tools in this chapter in Section 1.7
and in much greater detail in Chapter 10.
Currently the most popular method for generating samples from posterior distributions
using Markov chains is the WinBUGS program and its Unix-based precursor BUGS and the
more recent functional equivalent JAGS. The name BUGS is a pseudo-acronym for Bayesian
inference Using Gibbs Sampling, referring to the most frequently used method for producing
Markov chains.
In what constitutes a notable and noble contribution to the Bayesian
3This led one observer (Evans 1994) to compare Bayesians to “unmarried marriage guidance counselors.”

Background and Introduction
33
statistical world, the Community Statistical Research Project at the MRC Biostatistics
Unit and the Imperial College School of Medicine at St.
Mary’s, London provide this
high-quality software to users free of charge, and it can be downloaded from their web
page: http://www.mrc-bsu.cam.ac.uk/software/bugs/. These authors have even made
available extensive documentation at the same site by Spiegelhalter et al. (1996a, 1996b,
2000, 2012).
Alternative ways to use WinBUGS with R as the interface are: BRugs, rbugs,
and R2WinBUGS. There are also facilities for calling WinBUGS from SAS, stata, and excel.
The JAGS program (Just Another Gibbs Sampler) is an engine for the BUGS language that has
nearly the same structure as WinBUGS, with only a few syntactical diﬀerences. Authored
by Martyn Plummer, it is extremely well-developed software that runs on non-windows
platforms and is command-line driven rather than point-and-click. It can be downloaded at
http://www-ice.iarc.fr/∼martyn/software/jags/. There are also facilities for calling
JAGS from R: R2jags, Rjags, and runjags. Most of the BUGS code in this text are run with
JAGS from the command window. Other high-quality R packages using or providing MCMC
computing include: BMS, dclone, eco, glmdm, HI, lmm, MasterBayes, mcmc, MCMCglmm,
MCMCpack, MNP, pscl, spBayes, tgp, and zic.
Of these MCMCpack is the most general,
whereas most of the others are MCMC implementations to solve a speciﬁc problem. Given
the rapid pace of R package development, this list is growing rapidly.
1.9
Exercises
1.1
Restate the three general steps of Bayesian inference from page 6 in your own words.
1.2
Given k possible disjoint (non-overlapping) events labeled: E1, . . . , Ek where k
could even be inﬁnity, denote p(Ei) as the mapping from events Ei to [0:1] space.
Write the Kolmogorov axioms of probability in technical detail.
1.3
Rewrite Bayes’ Law when the two events are independent. How do you interpret
this?
1.4
Equation (1.11) on page 12 showed that p(A|D) = p(D|A)p(A)/(p(D|A)p(A) +
p(D|B)p(B) + p(D|C)p(C)). Rewrite this expression for p(A|D) when there are
arbitrary k ∈I+ events including A.
1.5
Suppose f(θ|X) is the posterior distribution of θ given the data X. Describe the
shape of this distribution when the mode, argmax
θ
f(θ|X), is equal to the mean,

θ θf(θ|X)dθ.
1.6
The R´enyi countable additivity axiom is deﬁned by: (1) for any events E1 and
E2, p(E1|E2) ≥0 (and reversed), p(Ei|Ei) = 1, (2) for disjoint sets E1, . . . and

34
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
another arbitrary event D, p(∪∞
i=1Ei|D) = ∞
i=1 p(Ei|D), and (3) for every sub-
set of events, Ei, Ej, Ek, with Ej ⊆Ek and p(Ej|Ek) > 0, we get p(Ei|Ej) =
p(Ei, Ej|Ek)/p(Ej|Ek). Show that the Kolmogorov axioms are a special case.
1.7
Using R run the Gibbs sampling function given on page 26. What eﬀect do you see
in varying the B parameter? What is the eﬀect of producing 200 sampled values
instead of 50,000?
1.8
Some authors have objected to the uniform prior, p(θ) = 1, θ ∈[0:1] to describe
unknown probabilities in a binomial model and suggested instead the Haldane prior:
p(θ) ∝[θ−1(1 −θ)]−1 (Haldane [1938], Novick and Hall [1965], Villegas [1977], ).
Plot this prior and the uniform prior over [0:1] in the same graph.
1.9
Rerun the Metropolis algorithm on page 30 in R but replacing the uniform gen-
eration of candidate values in cand.gen with a normal truncated to ﬁt in the
appropriate range. What diﬀerences do you observe?
1.10
The Gibbs sampler described in Section 1.7.1 from Casella and George (1992) was
originally done as follows: (1) set initial values for B = 5, k = 15, m = 5, 000, and
the set of accepted values (x, y) as an empty object, (2) run m chains of length
k + 1 where the ﬁrst value is the uniformly distributed starting point [0 : B]) and
the rest are sampled conditional exponential values that are less than B, (3) save
only the last value from the x and y series, x16 and y16 to the stored Markov chain
until 5,000 of each are obtained. Implement this alternative algorithm in R and
compare it to the output shown in Figure 1.1 on page 27.
1.11
If p(D|θ) = 0.5, and p(D) = 1, calculate the value of p(θ|D) for priors p(θ),
[0.001, 0.01, 0.1, 0.9].
1.12
Buck, Cavanaugh, and Litton (1996) demonstrate the use of Bayesian statistics for
radiocarbon dating of Early Bronze Age archaeological samples (seeds and bones)
from St. Veit-Klinglberg, Austria. These ten age data points are produced by the
Oxford accelerator dating facilities:
Context #
μi
σi
758
3275
75
814
3270
80
1235
3400
75
493
3190
75
925
3420
65
923
3435
60
1168
3160
70
358
3340
80
813
3270
75
1210
3200
70

Background and Introduction
35
Given the model Xi|μi, σi ∼N(μi, σ2
i ), (1) calculate the probability that sample
358 originates between 3300 to 3400 years ago, (2) generate 10,000 samples from
the distribution for sample 493 and sample 923 and plot a histogram of these in
the same ﬁgure (side-by-side), (3) give the proportion of values that overlap, and
(4) how do you interpret this overlap probabilistically with regard to the age of the
samples?
1.13
Sometimes Bayesian results are given as posterior odds ratios, which for two possible
alternative hypotheses is expressed as:
odds(θ1, θ2) = p(θ1|D)
p(θ2|D).
If the prior probabilities for θ1 and θ2 are identical, how can this be re-expressed
using Bayes’ Law?
1.14
Using the posterior distribution in (1.19) on page 19, produce the posterior mean
for θ in (1.20) and the posterior variance for θ in (1.21).
1.15
Suppose we had data, D, distributed p(D|θ) = θe−θD as in Section 1.5.1 starting
on page 18, but now p(θ) = 1/θ, for θ ∈(0:∞). Calculate the posterior mean.
1.16
Modify the Gibbs sampler in Section 1.7.1 starting on page 25 to sample from
two mutually conditional gamma distributions instead of exponential distributions.
The exponential distribution is a simpliﬁed form of the rate parameter gamma
distribution where the ﬁrst (shape) parameter is 1 (Appendix B, page 580). Set
the two relevant shape parameters to values of your choosing α > 1. Produce a
graphs of the marginal draws.
1.17
Since the posterior distribution is a compromise between prior information and the
information provided by the new data, then it is interesting to compare relative
strengths. Perform an experiment where you ﬂip a coin 10 times, recording the
data as zeros and ones. Produce the posterior expected value (mean) for two priors
on p (the probability of a heads): a uniform distribution between zero and one, and
a beta distribution (Appendix B) with parameters [10, 1]. Which prior appears to
inﬂuence the posterior mean more than the other?
1.18
Fisher (1930) deﬁned ﬁducial inference for a parameter θ with maximum likelihood
estimate ˆθ by making the CDF (cumulative distribution function) of ˆθ|θ uniform
over [0 : 1].
Taking a derivative of this CDF with respect to ˆθ then gives the
associated PDF:
f(ˆθ|θ) =

d
dˆθ
F(ˆθ|θ)
 ,
and taking a derivative of the CDF with respect to θ produced what Fisher called
the ﬁducial distribution of the parameter θ given the statistic ˆθ. Fisher (1956) later
claimed that this approach “uses the observations only to change the logical status

36
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
of the parameter from one in which nothing is known of it, and no probability
statement about it can be made, to the status of a random variable having a well
deﬁned distribution.” Show that this cannot be true and that this is not Bayesian
inference.
1.19
The Stopping Rule Principle states that if a sequence of experiments (trials) is
governed by a stopping rule, η, that dictates cessation, then inference about the
unknown parameters of interest must depend on η only through the collected sam-
ple. Obvious stopping rules include setting the number of trials in advance (i.e.,
reaching n is the stopping rule), and stopping once a certain number of successes
are achieved. Consider the following experiment and stopping rule. Standard nor-
mal distributed data are collected until the absolute value of the mean of the data
exceeds 1.96/√n. Explain why this fails as a non-Bayesian stopping rule for test-
ing the hypothesis that μ = 0 (the underlying population is zero), but is perfectly
acceptable for Bayesian inference.
1.20
The PDF of a C-ﬁnite mixture of normal distributions is given by:
f(yi|μ, Σ) =
C

c=1
ωcN(yi|μc, σ2
c),
where ωc is the weight placed on distribution c with  ωc = 1, and N() de-
notes a normal distribution. For a mixture with means μ = [−3, 1, 4], variances
Σ = [0.3, 1, 0.2], and weights Ω = [0.2, 0.5, 0.3] develop a simple Metropolis (sym-
metric) algorithm to get draws from this mixture using a Student’s-t candidate
distribution where you determine a good choice for the degrees of freedom param-
eter, ν. Describe the mixture distribution empirically with summary statistics and
a graph from producing 10,000 draws and disposing of the ﬁrst 5,000.

Chapter 2
Specifying Bayesian Models
2.1
Purpose
This chapter changes the discussion from the basic workings of Bayes’ Law in a probabil-
ity context to a focus on the use of Bayes’ Law for realistic statistical models. Consequently,
the ﬁrst order of business is to go from our previous vague deﬁnition of data, D, to a rect-
angular n×k matrix of data, X. In this chapter we also make the move from the unspeciﬁc
p() for posterior distributions to the more clear π() notation in order to distinguish them
from priors, likelihoods, and other functions. Also, from now on we use the vector form of
theta, θ, since nearly all interesting social science models are multidimensional.
In the immediately forthcoming material, we cover the core idea of Bayesian statistics:
updating prior distributions by conditioning on data through the likelihood function. We
will also look at repeating this updating process as new information becomes available.
There is an additional historical discussion placing this modeling approach into context.
2.2
Likelihood Theory and Estimation
In order to make inferences about unknown model parameters in generalized linear
models, Bayesian probability models, or any other parametric speciﬁcation, we would like
to have a description of parameter values that are more or less probable given the observed
data and the parametric form of the model. In other words, some values of an unknown
parameter are certainly more likely to have generated the data than others, and if there is
one value that is more likely than all others, we would typically prefer to report that one.
For instance, suppose we wanted to know the probability of getting a heads with a
possibly unfair coin. Flipping it ten times, we observe 5 heads. It seems logical to infer
that p = 0.5 is more likely than p = 0.4, or p = 0.6, or any other value for that matter.
In this case, p = 0.5 is the value that maximizes the likelihood function given the observed
series of ﬂips. Maximizing a likelihood function with regard to coeﬃcient values is without
question the most frequently used estimation technique in applied statistics.
37

38
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
Stipulate now that we are interested in analyzing a model for a k-dimensional unknown
θ vector, k −1 explanatory variables, a constant, and n data points.
Asymptotic the-
ory assures us that for suﬃciently large samples the likelihood surface is unimodal in k
dimensions for the commonly used forms (Lehmann 1999). Denote this likelihood func-
tion as L(θ|X) even though it is constructed as the joint distribution of the iid outcomes:
p(X|θ) = f(x1|θ)f(x2|θ) · · · f(xn|θ).
The likelihood function diﬀers from the inverse probability, p(θ|X), in that it is neces-
sarily a relative function since it is not a normalized probability measure bounded by zero
and one. From a frequentist standpoint, the probabilistic uncertainty is a characteristic of
the random variable X, not the unknown but ﬁxed θ. Barnett (1973, p.131) clariﬁes this
distinction: “Probability remains attached to X, not θ; it simply reﬂects inferentially on
θ.” Thus maximum likelihood estimation substitutes the unbounded notion of likelihood for
the bounded deﬁnition of probability (Casella and Berger 2002, p.316; Fisher 1922, p.327;
King 1989, p.23). This is an important theoretical distinction, but of little signiﬁcance in
applied practice. If we regard p(X|θ) as a function of θ for some given observed data X,
then L(θ|X) = n
i=1 p(X|θ) (DeGroot 1986, p.339).
Typically it is mathematically more convenient to work with the natural log of the like-
lihood function. This does not change any of the resulting parameter estimates because
the likelihood function and the log likelihood function have identical modal points for com-
monly used forms. Using a PDF for a single parameter of interest, the basic log likelihood
function is very simple:
ℓ(θ|X) = log(L(θ|X)),
(2.1)
where we use ℓ(θ|X) as shorthand to distinguish the log likelihood function from the like-
lihood function, L(θ|X).
The score function is the ﬁrst derivative of the log likelihood function with respect to
the parameters of interest:
˙ℓ(θ|X) = ∂
∂θ ℓ(θ|X).
(2.2)
Setting ˙ℓ(θ|X) equal to zero and solving gives the maximum likelihood estimate, ˆθ. This is
now the “most likely” value of θ from the parameter space Θ treating the observed data as
given: ˆθ maximizes the likelihood function at the observed values. The Likelihood Principle
(Birnbaum 1962) states that once the data are observed, and therefore treated as given, all of
the available evidence for estimating θ is contained in the (log) likelihood function, ℓ(θ|X).
This is a handy data reduction tool because it tells us exactly what treatment of the data is
important to us and allows us to ignore an inﬁnite number of alternates (Poirer 1988, p.127).
The key diﬀerence between the classic likelihood approach and the Bayesian inference is
that more information is used in the analysis and more information is provided through
descriptions of the posterior beyond modal summaries. Thus the likelihood principle only
has relevance here for part of the Bayesian model.
The maximum likelihood doctrine states that an admissible θ that maximizes the likeli-
hood function probability (discrete case) or density (continuous case), relative to alternative

Specifying Bayesian Models
39
values of θ, provides the θ that is “most likely” to have generated the observed data, X,
given the assumed parametric form. Restated, if ˆθ is the maximum likelihood estimator for
the unknown parameter vector, then it is necessarily true that L(ˆθ|X) ≥L(θ|X) ∀θ ∈Θ,
where Θ is the admissible set of θ. Admissible here means values of θ are taken from the
valid parameter space (Θ): values of θ that are unreasonable according to the form of the
sampling distribution of θ are not considered (integrated over).
Setting the score function from the joint PDF or PMF equal to zero and rearranging
gives the likelihood equation:

t(Xi) = n ∂
∂θ E[X]
(2.3)
where  t(Xi) is the remaining function of the data, depending on the form of the probabil-
ity density function (PDF) or probability mass function (PMF), and E[X] is the expectation
over the kernel of the density function for X. The kernel of a PDF or PMF is the component
of the parametric expression that directly depends on the form of the random variable, i.e.,
what is left when normalizing constants are omitted. We can often work with kernels of
distributions for convenience and recover all probabilistic information at the last stage of
analysis by renormalizing (ensuring summation or integration to one). The kernel is the
component of the distribution that assigns relative probabilities to levels of the random
variable (see Gill 2000, Chapter 2). For example the kernel of a gamma distribution is just
the part xα−1 exp[−xβ], without the normalizing constant βα/Γ(α).
The underlying theory here is remarkably strong. Solving (2.3) for the unknown co-
eﬃcient produces an estimator that is unique (due to a unimodal posterior distribution),
consistent (converges in probability to the population value), and asymptotically unbiased,
but not necessarily unbiased in ﬁnite sample situations. On the latter point, the maximum
likelihood estimate for the variance of a normal model, ˆσ2 =
1
n
(xi −¯x)2 is biased by
n/(n −1). This diﬀerence is rarely of signiﬁcance and clearly the bias disappears in the
limit, but it does illustrate that unbiasedness of the maximum likelihood estimate is guar-
anteed only in asymptotic circumstances.
It is also asymptotically eﬃcient (the variance
of the estimator achieves the lowest possible value as the sample size becomes adequately
large: the Cram´er-Rao lower bound, see Shao 2005). This result combined with the central
limit theorem gives the asymptotic normal form for the estimator: √n(ˆθ −θ)
P→N(0, Σθ).
This means that as the sample size gets large, the diﬀerence between the estimated value
of θ and the true value of θ gets progressively close to zero, with a variance governed by
1
√nΣθ, where Σθ is the k × k variance-covariance matrix for θ. Furthermore,  t(xi) is a
suﬃcient statistic for θ, meaning that all of the relevant information about θ in the data is
contained in  t(xi). For example, the normal log likelihood expressed as a joint exponen-
tial family form is ℓ(θ|X) =

μ  xi −nμ2
2

/σ2−
1
2σ2
 x2
i −n
2 log(2πσ2). So t(X) =  Xi,
d
dμ
nμ2
2
= nμ, and equating gives the maximum likelihood estimate of μ to be the sample
average that we know from basic texts:
1
n
 xi. Bayesian inference builds upon this strong
foundation by combining likelihood information, as just described, with prior information
in a way describes all unknown quantities distributionally.

40
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
2.3
The Basic Bayesian Framework
Our real interest lies in obtaining the distribution of the unknown k-dimensional θ
coeﬃcient vector, given an observed X matrix of data values: p(θ|X). If we choose to
here, we can still determine the “most likely” values of the θ vector using the k-dimensional
posterior mode or mean, but it is better to more fully describe the shape of the posterior
distribution, given by Bayes’ Law:
p(θ|X) = p(X|θ) p(θ)
p(X)
(2.4)
where p(X|θ) is the joint probability function for data (the probability of the sample for a
ﬁxed θ) under the assumption that the data are independent and identically distributed
according to p(Xi|θ) ∀i = 1, . . . , n, and p(θ), p(X) are the corresponding unconditional
probabilities. This is mechanically correct but it does not fully represent Bayesian thinking
or notation about the inference process.
2.3.1
Developing the Bayesian Inference Engine
From the Bayesian perspective, there are only two fundamental types of quantities:
known and unknown. The goal is to use the known quantities along with a speciﬁed para-
metric expression to make inferential statements about the unknown quantities. The deﬁni-
tion of such unknown quantities is very general; they can be any missing data or unknown
parameters. When quantities are observed, they are considered ﬁxed and conditioned upon.
Suppose we fully observe the data X. This is now a ﬁxed and given quantity in the in-
ferential process. The ﬁrst implication is that p(X|θ) in (2.4) does not make notational
sense since the known quantity is conditional on the unknown quantity. Instead label this
quantity as L(θ|X) and treat it as a likelihood function. It is a likelihood function of course,
but note that the justiﬁcation is inherently Bayesian (i.e., probabilistic). Also, since the X
are treated as ﬁxed, p(X) is not especially useful here. However, this quantity performs an
important role in model comparison as we shall see in Chapter 7.
The prior distribution, p(θ), must be speciﬁed, but need not be highly inﬂuential. This is
simply a distributional statement about the unknown parameter vector θ, before observing
or conditioning on the data. Much controversy has developed about the nature of prior
distributions and we will look at alternative forms in detail in Chapter 4. It is essential to
supply a prior distribution in Bayesian models and well over 100 years of futile searching for a
way to avoid doing so have clearly demonstrated this. Currently an approach called objective
Bayes (O-Bayes) seeks to mathematically minimize the eﬀect of prior speciﬁcations.
Start with the form of Bayes’ Law deﬁned with conditional probability, giving the pos-
terior of interest:
π(θ|X) = p(θ)L(θ|X)
p(X)
,
(2.5)

Specifying Bayesian Models
41
which is an update of (2.4) that gives the desired probability statement on the left-hand side
now using the π() notation as a reminder. This states that the distribution of the unknown
parameter conditioned on the observed data is equal to the product of the prior distribu-
tion assigned to the parameter and the likelihood function, divided by the unconditional
probability of the data. The form of (2.5) can also be expressed as:
π(θ|X) =
p(θ)L(θ|X)

Θ p(θ)L(θ|X)dθ ,
(2.6)
where

Θ p(θ)L(θ|X)dθ is an expression for p(X) explicitly integrating the numerator over
the support of θ.
This term has several names in the literature: the normalizing con-
stant, the normalizing factor, the marginal likelihood, and the prior predictive distribution,
although it is actually the marginal distribution of the data, and it ensures that π(θ|X) in-
tegrates to one as required by the deﬁnition of a probability function. A more compact and
succinct form of (2.6) is developed by dropping the denominator and using proportional
notation since p(X) does not depend on θ and therefore provides no relative inferential
information about more or less likely values of θ:
π(θ|X) ∝p(θ)L(θ|X),
(2.7)
meaning that the unnormalized posterior (sampling) distribution of the parameter of inter-
est is proportional to the prior distribution times the likelihood function:
Posterior Probability ∝Prior Probability × Likelihood Function.
It is typically (but not always, see later chapters) easy to renormalize the posterior distri-
bution as the last stage of the analysis to return to (2.6).
0.00
0.05
0.10
0.15
Support
Density
Prior
Likelihood
Posterior
FIGURE 2.1: Posterior ∝Prior × Likelihood

42
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
As an illustration, suppose we have data that are iid exponentially distributed f(X|θ) =
θe−θX, X, θ ∈(0, ∞), and an exponential prior distribution for the unknown parameter
p(θ) = βe−θβ, β ∈(0, ∞), where β = 30 here as an arbitrary modeling choice. These data
are actually taken from Example 2.3.2.1 on page 44 below but the speciﬁc data context is
not yet important here. The exponential assumption for the data means that the likelihood
function is a gamma distribution with parameters n + 1 and  Xi: L(θ) ∝θne−θ  Xi.
Multiplying the likelihood and the prior produces a another gamma distribution with new
parameters n+1 and  Xi +β (Exercise 2 in this chapter). This is illustrated in Figure 2.1
where we see that the prior distribution on the left pulls the likelihood function towards
it in the creation of the posterior distribution. This is called shrinkage in the Bayesian
literature and it means that the posterior mean “shrinks” towards the prior mean. Fig-
ure 2.1 is purposefully over-dramatic in showing this eﬀect, but movement such as this is
a characteristic of all Bayesian models: the posterior distribution is always a compromise
between the prior distribution and the likelihood function. The question is how inﬂuential
is the prior distribution in this calculation.
We can also state Bayes’ Law in odds form as done in (1.6) on page 11. Suppose we
have two competing models expressed by θ1 and θ2, which are considered to exhaust the
possible states of nature. This latter assumption may be unrealistic, but it is often the case
that a researcher will consider only two alternatives at a time. If we now observe the data,
X, then Bayes’ Law in odds form is:
π(θ1|X)
π(θ2|X) =
p(θ1)
p(X) L(θ1|X)
p(θ2)
p(X) L(θ2|X)
= p(θ1)
p(θ2)
L(θ1|X)
L(θ2|X)
Posterior Odds = Prior Odds × Likelihood Ratio.
This likelihood ratio will later be generalized in Chapter 7 as the Bayes Factor. Furthermore,
if we assume equal prior probabilities, the posterior odds is simply equal to the likelihood
ratio. Since likelihood ratio testing is a very popular tool in non-Bayesian model comparison,
this is a nice linkage: under basic circumstances Bayesian posterior odds comparison is
equivalent to simple likelihood ratio testing.
2.3.2
Summarizing Posterior Distributions with Intervals
In Chapter 1, we noted the value of describing posterior distributions with simple quan-
tiles, and calculated analytical posterior moments: E[θ|X], and Var[θ|X]. However, such
summaries may miss distributional features and should be complemented with additional
interval-based measures.
The ﬁrst descriptive improvement here is found by moving from conﬁdence intervals
to Bayesian credible intervals.
Recall that conﬁdence intervals are intimately tied with

Specifying Bayesian Models
43
frequentist (not likelihoodist!) theory since a 100(1−α)% conﬁdence interval covers the true
underlying parameter value across 1 −α proportion of the replications in the experiment,
on average. So conﬁdence is a property of frequentist replication from a large number of
repeated iid samples and underlying parameters that are ﬁxed immemorial. In fact, the
conﬁdence interval may be considered the most frequentist summary possible since it does
not have an interpretation without multiple replications of the exact same experiment. One
major problem with the conﬁdence interval lies in its interpretation. Most consumers of
statistics want conﬁdence intervals to be probabilistic statements about some region of the
parameter space, but careful writers discourage this by explaining the actual nature of
conﬁdence intervals: “a 95% conﬁdence interval covers the true value of the parameter in
nineteen out of twenty trials on average.” In most social science settings with observational
data it is not practical to repeat some experiment nineteen more times with an assumed iid
data-generating source.
2.3.2.1
Bayesian Credible Intervals
The Bayesian analogue to the conﬁdence interval is the credible interval and more gen-
erally the credible set, which does not have to be contiguous. Most of the time in practice
it is calculated in exactly the same way as the conﬁdence interval for unimodal symmetric
forms. For instance calculating a 95% credible interval under the Gaussian normal assump-
tion means marching out 1.96 standard errors from the mean in either direction, just like
the analogous conﬁdence interval is created. However, for asymmetric distributions this
algorithm would produce a credible interval with unequal tails and incorrect coverage.
The diﬀerence between conﬁdence intervals and Bayesian credible intervals is in the
interpretation of what the interval means. A 100(1 −α)% credible interval gives the region
of the parameter space where the probability of covering θ is equal to 1−α (it may actually
be a little more than 1 −α for discrete parameter spaces in order to guarantee at least this
level of coverage). In contrast, applying this new deﬁnition to the conﬁdence interval means
that the probability of coverage is either zero or one, since it either covers the true θ or it
doesn’t.
Formally, an equal tail credible set for the posterior distribution is deﬁned as follows.
Deﬁne C as a subset of the parameter space, Θ, such that a 100(1 −α)% credible interval
meets the condition:
1 −α =

C
π(θ|X)dθ
(2.8)
(this is summation instead of an integration for discrete parameter spaces, but we will
discuss mostly continuous parameter spaces here). It is important to note that credible
intervals are not unique. That is, we can easily deﬁne C in diﬀerent ways to cover varying
parts of Θ and still meet the probabilistic condition in (2.8). It is not necessary that we
center these intervals at a mean or mode. Important diﬀerences arise in asymmetric and
multimodal distributions, and the convention is to create equal tail intervals: no matter
what the shape of the posterior distribution. This means that the 100(1 −α)% credible

44
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
interval is created such that α/2 of the density is put in both the left and right tails outside
of the designated credible interval.
0.12
0.14
0.16
0.18
Posterior Support
Posterior Density
0
10
20
30
40
50
90% Credible Interval, (0.14034, 0.16528)
FIGURE 2.2: CI for State Duration Time to Adaptation, 1998-2005
■Example 2.1:
Credible Interval, Fifty U.S. States Time to Adoption for
Health Bills.
Boehmke (2009) counts bills passed in the ﬁfty states between 1998
and 2005 that contain policy implications for the increasing obesity rates in the U.S.
These include limits on sugary drinks at schools, requiring insurers to cover particular
medical procedures, as well as limitations on lawsuits from consumer groups on the fast
food industry. We deﬁne duration data, X, to be the time in years through this period
for a bill to be passed. Assume that X is exponentially distributed p(X|θ) = θe−θX
deﬁned over [0, ∞), where interest is in the posterior distribution of the unknown pa-
rameter θ. Specify the prior distribution as p(θ) = 1/θ, for θ ∈[0:∞). The resulting
posterior is given by:
π(θ|X) ∝p(θ)L(θ|X) =
1
θ

θn exp

−θ
n

i=1
xi

= θn−1 exp

−θ
n

i=1
xi

(2.9)
(note that we are using the proportionality shortcut from (2.7)). If we stare at this
for a few moments we can see that θ|X ∼G(θ|n,  xi) with the “rate” speciﬁcation
for the second parameter. Putting the constants back in front to recover the full form

Specifying Bayesian Models
45
of this gamma posterior distribution produces:
π(θ|X) = ( xi)n
Γ(n)
θn−1 exp

−θ

xi

(details in Appendix B). As stated, once we produce the posterior distribution, we
know everything about the distribution of θ and can convey to our readers any sum-
mary we would like.
TABLE 2.1:
State Duration Time to Adaptation, 1998-2005
Mean
Mean
Mean
State
N
Duration
State
N
Duration
State
N
Duration
AL
2
7.500
LA
14
5.571
OK
12
6.583
AK
12
6.667
ME
2
5.500
OH
0
NaN
AZ
12
6.250
MD
11
6.455
OR
1
8.000
AR
6
6.167
MA
7
7.143
PA
12
7.083
CA
46
6.000
MI
4
7.000
RI
7
7.000
CO
11
6.636
MN
2
7.000
SC
6
6.333
CT
2
7.000
MS
7
7.143
SD
1
7.000
DE
4
7.000
MO
18
5.556
TN
17
7.235
FL
11
6.364
MT
2
7.000
TX
16
6.250
GA
7
5.857
NE
5
7.400
UT
3
7.667
HI
8
6.375
NV
4
8.000
VT
8
6.625
ID
6
6.000
NH
1
5.000
VA
15
6.533
IL
4
6.750
NJ
6
7.333
WA
12
6.083
IN
31
7.065
NM
6
6.500
WV
2
7.500
IA
3
5.000
NY
9
6.556
WI
4
7.750
KS
4
8.000
NC
8
7.250
WY
1
8.000
KY
4
7.500
ND
9
6.111
The complete data are given in Table 2.1 for annualized periods, as well as in the
R package BaM. Note the “NaN” value for the Ohio mean duration given by R since
there is nothing to average. We will leave this case out of the subsequent analysis
since the time to adoption is inﬁnity, or more realistically, censored from us. The
state averages from the third column of the table are weighted by N in the second
column to reﬂect the number of such events: XiNi. Since the suﬃcient statistic in
the posterior distribution is a sum, there is no loss of information from not having
the full original data from the authors (sums of means times n equal the total sum).
The end-points of the equal tail credible interval are created by solving for the limits
(L and H) in the two integrals:
α
2 =
 L
0
π(θ|X)dθ
α
2 =
 ∞
H
π(θ|X)dθ
(2.10)
or, more simply, we could use basic R functions to manipulate the state.df dataframe
containing the data in the table above:
state.df <- state.df[-35,] # REMOVES OHIO
qgamma(0.05,shape=sum(state.df$N),rate=sum(state.df$N*state.df$dur))

46
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
[1] 0.14034
qgamma(0.95,shape=sum(state.df$N),rate=sum(state.df$N*state.df$dur))
[1] 0.16528
for a 90% credible interval. These points and a plot of the posterior distribution are
given in Figure 2.2. The slight asymmetry of this gamma distribution means that the
left tail region needs to reach higher (moving the boundary to the right) in order to
equal the right tail in total posterior density. Therefore the density values (y-axis) at
endpoints diﬀer: 14.384 versus 12.898. To contrast with these results, the maximum
likelihood value is ˆθ = 0.018, (the inverse of the weighted mean of the data, Casella
and Berger [2002]), whereas the posterior mean Eπ[θ|X] = 0.153, showing that the
prior p(θ) = 1/θ has some inﬂuence.
2.3.2.2
Bayesian Highest Posterior Density Intervals
Credible intervals are common and useful, but a theoretically more defensible inter-
val can be produced by incorporating some additional ﬂexibility. When looking at pos-
terior distributions, we really care where the highest density exists on the support of the
C
k
FIGURE 2.3: Bimodal Distribution Highest
Posterior Density Interval
posterior
density,
regardless
of
whether it is contiguous or not. So
the big idea behind highest poste-
rior density (HPD) regions is that
no region outside of the interval will
have higher posterior density than
any region inside the HPD region.
Hence for multimodal distributions
the HPD region may actually be a
set of individually non-contiguous
intervals. See Hyndman (1996) for
interesting forms as well as a general
introduction to HPD regions. The
use of the word “intervals” is com-
mon instead of “regions,” but HPD
regions possess an automatic ability
to be non-contiguous so the latter is
more correct. For symmetric unimodal forms the HPD interval will be contiguous and iden-
tical to an equal tail credible interval.
More speciﬁcally, a 100(1 −α)% highest posterior density region is the subset of the
support of the posterior distribution for some parameter, θ, that meets the criteria:
C = {θ : π(θ|x) ≥k},

Specifying Bayesian Models
47
where k is the largest number such that:
1 −α =

θ:π(θ|x)>k
π(θ|x)dθ
(2.11)
(Casella and Berger 2002, p.448). This is the 1−α proportion subset of the sample space of
θ where the posterior density of θ is maximized. So k is a horizontal line that slices across
the density producing inside HPD areas and outside HPD areas. This will be a regular
interval if the posterior distribution is unimodal, and it may be a discontiguous region
if the posterior distribution is multimodal. This is shown in Figure 2.3 where it is clear
that a bimodal distribution having a deep trough in the middle produces a non-contiguous
HPD region emphasizing the undesirability of incorporating the middle region. Multimodal
forms appear in Bayesian mixture models, and an example appears in Chapter 3 starting
on page 87.
Chen and Shao (1999) provide another way to conceptualize the HPD region. For a
unimodal posterior form, given by π(θ|X), our objective is to ﬁnd the values [θL, θU] that
deﬁne a (1 −α) HPD region. It turns out that the answer to this question is given by also
using cumulative (Π) diﬀerences:
min
θL<θU
⎡
⎢⎣|π(θU) −π(θL)|
 
!"
#
diﬀerence in “height”
+ |Π(θU) −Π(θL) −(1 −α)|
 
!"
#
diﬀerence in “width”
⎤
⎥⎦.
(2.12)
So the ﬁrst diﬀerence lines up k across the two HPD region endpoints and the second
diﬀerence gives the coverage probability. In many circumstances this minimization gives
zero, but for posteriors with ﬂat regions it would need to be modiﬁed with some additional
criteria to provide a unique interval such as picking the one with smallest θL value.
■Example 2.2:
HPD Region, Fifty U.S. States Time to Adoption for Health
Bills. Returning to the example from time to adopt health laws, Figure 2.4, shows the
HPD region for this posterior along with the determining line at k = 16.873. Notice
in comparing the HPD region to the credible interval for this model, that the HPD
region has equal height at the end-points at k = 13.602, and that the endpoints of the
interval diﬀer slightly from the corresponding equal-tail credible interval. The HPD
region is constructed in a very simple way by starting at the posterior mode, then
incrementing a horizontal line down vertically until the separation between the higher
density and lower density regions reﬂects the desired coverage. So for each value of
k, the level on the y-axis, we separately sum the area inside and outside the coverage
area, regardless of contiguity. The Computational Addendum at the end of this
chapter provides the R code used here for a posterior gamma distributed form. This
process was quite easy to implement in R since we know the exact form of the gamma
distribution from the model. Later when estimating marginal posterior forms with
Bayesian stochastic simulation (MCMC), we will see that there are similarly easy ways
to make this calculation even when we do not have an exact parametric description of

48
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
0.12
0.14
0.16
0.18
Posterior Support
Posterior Density
0
10
k
20
30
40
50
90% HPD, (0.14011, 0.16504)
FIGURE 2.4: HPD Region, State Duration Time to Adaptation, 1998-2005
the posterior distribution. There is also the HPDinterval function in the CODA library
for this, but it does not illustrate the underlying theory as directly as the exposition
here.
2.3.3
Quantile Posterior Summaries
Often interval results are given basic quantile summaries without explicitly labeling
these as credible intervals or HPD regions. Of course quantile summaries for unimodal
forms are credible interval deﬁnitions so this is an analogous procedure.
However, for
highly multimodal forms basic quantile summaries can be misleading in the same way that
boxplots hide such characteristics. Frequently summary tables for regression models contain
quantile summaries since they are eﬃcient with printed space and reveal characteristics of
the distribution of interest.
■Example 2.3:
Quantiles, Fifty U.S. States Time to Adoption for Health
Bills.
Using the data in Example 2.3.2.1 (starting on page 44), we can also cal-
culate quantiles in addition to the credible intervals done before. We know that the

Specifying Bayesian Models
49
time to adopt health laws example has a unimodal posterior, so simple quantiles are
applicable. Consider the simple R commands:
q.vals <- c(0.025,0.05,0.25,0.5,0.75,0.95,0.975)
rbind( quant.vals, "quantiles"=
qgamma(quant.vals,shape=sum(state.df$N),
rate=sum(state.df$N*state.df$dur)) )
[,1]
[,2]
[,3]
[,4]
[,5]
[,6]
[,7]
quant.vals 0.0250 0.05000 0.25000 0.50000 0.75000 0.95000 0.97500
quantiles
0.1381 0.14034 0.14742 0.15247 0.15764 0.16528 0.16781
using the state.df dataframe described above.
This gives quantiles for common
interval deﬁnitions (α = 0.05 and α = 0.10), the interquartile range (IQR), and the
median).
Note that the values (0.14034, 0.16528) were given before with the 90%
credible interval.
2.3.4
Beta-Binomial Model
This model illustrates the development of a posterior distribution for an interesting
implication and shows how the properties of this posterior distribution can be described in
conventional terms. Let X1, X2, . . . , Xn be independent random variables, all produced by
the same probability mass function (iid): BR(p), and place a BE(A, B) prior distribution
on the unknown population probability, p (see Appendix B for details on these forms). The
goal is to get a posterior distribution for p, and this process is greatly simpliﬁed by noting
that the sum of n Bernoulli(p) random variables is distributed binomial(n, p). Deﬁne a new
variable: Y = n
i=1 Xi. The joint distribution of Y and p is the product of the conditional
distribution of Y and the marginal (prior) distribution of p:
f(y, p) = f(y|p)f(p)
=
'n
y

py(1 −p)n−y
(
×
' Γ(A + B)
Γ(A)Γ(B)pA−1(1 −p)B−1
(
=
Γ(n + 1)Γ(A + B)
Γ(y + 1)Γ(n −y + 1)Γ(A)Γ(B)py+A−1(1 −p)n−y+B−1.
(2.13)
The marginal distribution of Y is easy to calculate by integrating (2.13) with respect to p
using a standard trick:
f(y) =
 1
0
Γ(n + 1)Γ(A + B)
Γ(y + 1)Γ(n −y + 1)Γ(A)Γ(B)py+A−1(1 −p)n−y+B−1dp
=
 1
0
Γ(n + 1)Γ(A + B)
Γ(y + 1)Γ(n −y + 1)Γ(A)Γ(B)
Γ(y + A)Γ(n −y + B)
Γ(n + A + B)
×
Γ(n + A + B)
Γ(y + A)Γ(n −y + B)py+A−1(1 −p)n−y+B−1dp

50
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
f(y) =
Γ(n + 1)Γ(A + B)
Γ(y + 1)Γ(n −y + 1)Γ(A)Γ(B)
Γ(y + A)Γ(n −y + B)
Γ(n + A + B)
×
 1
0
Γ(n + A + B)
Γ(y + A)Γ(n −y + B)py+A−1(1 −p)n−y+B−1dp
 
!"
#
equal to one
=
Γ(n + 1)Γ(A + B)
Γ(y + 1)Γ(n −y + 1)Γ(A)Γ(B)
Γ(y + A)Γ(n −y + B)
Γ(n + A + B)
.
(2.14)
The trick here is rearranging the terms such that a complete beta distribution with dif-
fering parameters is integrated across the support of the unknown random variable. The
probability density function given in the last line of (2.14) is called (not surprisingly) the
beta-binomial. Obtaining the posterior distribution of p is now a simple application of the
deﬁnition of conditional probability:
f(p|y) = f(y, p)
f(y)
=

Γ(n + 1)Γ(A + B)
Γ(y + 1)Γ(n −y + 1)Γ(A)Γ(B)py+A−1(1 −p)n−y+B−1
 )

Γ(n + 1)Γ(A + B)
Γ(y + 1)Γ(n −y + 1)Γ(A)Γ(B)
Γ(y + A)Γ(n −y + B)
Γ(n + A + B)

=
Γ(n + A + B)
Γ(y + A)Γ(n −y + B)p(y+A)−1(1 −p)(n−y+B)−1.
(2.15)
This can easily be seen as a new beta distribution with parameters A′ = y + A and
B′ = n −y + B. While it would be more typical of a Bayesian to describe this poste-
rior distribution with quantiles, credible sets or highest posterior density intervals, we can
get a point estimate of p here by taking the mean of this beta distribution:
ˆp =
y + A
A + B + n.
(2.16)
Rearrange (2.16) algebraically to produce:
ˆp =
'
n
A + B + n
(  y
n

+
'
A + B
A + B + n
( 
A
A + B

,
(2.17)
which is the weighted combination of the sample mean from the binomial and the mean
of the prior beta distribution where the weights are determined by the beta parameters, A
and B, along with the sample size n. Holding A and B constant at reasonable values and
increasing the sample size places more weight on the y/n term since the weight for the beta
mean,
A
A+B , has n only in the denominator and the weights necessarily add to one. This
turns out to be theoretically more interesting than it would ﬁrst appear and it highlights an
important and desirable property of Bayesian data analysis: as the sample size increases,
the likelihood function, f(y|p), is iteratively updated to incorporate this new information

Specifying Bayesian Models
51
and eventually subsumes the choice of prior, f(p), because of sample size. Conversely, when
the sample size is very small it makes sense to rely upon reliable prior information if it
exists.
This hierarchical parameterization of the binomial with a random eﬀects component
(the name commonly used in non-Bayesian settings) is often done when there is evidence of
overdispersion in the data: the variance exceeds that of the binomial: np(1−p) (see Agresti
2002, p.151, Lehmann and Casella 1998, p.230, Carlin and Louis 2001, p.44, McCullagh and
Nelder 1989, p.140). When the posterior has the same distributional family as the prior,
as in this case, we say that the prior and the likelihood distributions are conjugate. This is
an attractive property since it not only assures that there is a closed form for the prior, it
means that it is also easy to calculate. Conjugate priors are discussed in detail in Chapter 4,
Section 4.3, and Appendix B lists conjugate relationships, if they exist for commonly used
distributions.
■Example 2.4:
A Cultural Consensus Model in Anthropology. Romney (1999)
looks at the level of consensus among 24 Guatemalan women on whether or not 27
diseases known to all respondents are contagious. The premise is that a high level of
consensus about something as important as the spread of diseases indicates to what
extent knowledge is a component of culture in this setting. The survey data for polio
are given by a vector, x, containing:
1
1
1
1
0
1
1
0
1
0
1
1
1
0
1
1
1
1
1
1
0
0
0
1
where 1 indicates that the respondent believes polio to be noncontagious and 0 indi-
cates that the respondent believes polio to be contagious (Romney ranks it 13 out of 27
on a contagious scale). Here we apply the beta-binomial model with two diﬀerent beta
priors exactly in the manner discussed above. The ﬁrst prior is a BE(p|15, 2) prior that
imposes a great deal of prior knowledge about the unknown p parameter. The second
prior is a BE(p|1, 1) prior that is actually a uniform prior over [0:1] indicating a great
deal of uncertainty about the location of p. Because the beta distribution is conju-
gate to the binomial the resulting posterior distributions are also beta and we obtain
BE( xi + 15, n−xi + 2) = BE(32, 9), and BE( xi + 1, n−xi + 1) = BE(18, 8),
respectively. Notice that there is almost no work to be done here (e.g. just plugging-
in the speciﬁed values) since we have already worked out the analytical form of the
posterior distribution.
These posteriors along with the speciﬁed priors are shown
in Figure 2.5 where the posterior is illustrated with the darker line and 95% HPD
intervals are shown.

52
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
95% HPD Interval
Beta(15,2) prior in grey,  95% HPD Interval at: [0.644:0.892]
95% HPD Interval
Beta(1,1) prior in grey,  95% HPD Interval at: [0.506:0.851]
FIGURE 2.5: Prior and Posterior Distributions in the Beta-Binomial Model
It is clear from this ﬁgure that even though the prior speciﬁcations are very distinct,
the resulting posteriors diﬀer only modestly as evidenced by the highest posterior
density regions indicated below the distributions. The uniform prior clearly pulls the
posterior density to the left in contrast to the BE(15, 2), but it is apparent that the
likelihood has a substantial eﬀect even though there are only 24 data points. Sub-
stantively, we would have to say that this analysis only provides modest support for
Romney’s theory that believing speciﬁc diseases are contagious is a learned cultural
response (although he looks at 26 additional diseases as well).
This relatively simple inference engine, as described, belies an immensely powerful pro-
cedure for developing and testing parametric models of inference. The central philosophical
point is that the posterior distribution, which summarizes everything we know about the
unknown parameter, is a weighted average of knowledge we possess before observing the
data and the most likely value given the data. Furthermore, as the size of the data increases
the likelihood becomes increasingly inﬂuential relative to the prior where in the limit the

Specifying Bayesian Models
53
prior is immaterial. This is powerful because it explicitly incorporates knowledge about the
parameter that researchers possess before developing the empirical model and collecting the
data.
2.4
Bayesian “Learning”
There is actually no restriction on what constitutes “prior information,” provided it
can be expressed in a distributional form, and as new data are observed a new posterior
can be created by treating the old posterior as a prior and updating with the new data
through the new likelihood function. This is a rigorous formulation for the way people
think: we have opinions about the manner in which something works and this opinion is
updated or altered as new behavior is observed. Suppose there exist three serial events,
C, B, A, that are nonindependent and we wanted to update the joint probability distribution
as each event occurs.
The ﬁrst is just p(C), and no updating is needed.
The second
event occurs conditionally on the ﬁrst and we get the joint probability distribution by
serially updating: p(B, C) = p(B|C)p(C).
Now the third event occurs conditional on
the ﬁrst two, and the resulting joint distribution is a new update of the previous update:
p(C, B, A) = p(A|B, C)p(B|C)p(C).
It is easy to see that this can continue as long as
we want, and that as long as the last conditional is multiplied by the string of previous
conditionals, we always obtain the complete joint. In this simple example, we could think
of p(C) as the ﬁrst prior since it is not conditional on any other event.
To be a bit more statistically concrete about this point, start with a univariate prior
distribution, p(θ), on an unknown variable θ. We observe the ﬁrst set of iid data, x1, and
calculate the posterior from the likelihood function along with the speciﬁed prior:
π1(θ|x1) ∝p(θ)L(θ|x1).
(2.18)
Subsequent to calculating this posterior, we observe a second set of iid data, x2, independent
of the ﬁrst set but from the same data-generating process. To update our posterior and
therefore improve our state of knowledge, we simply treat the previous posterior as a prior
and proceed to calculate using a likelihood function from the new data, and this is exactly
the same result we would have obtained if all the data had arrived at once:
π2(θ|x1, x2) ∝π1(θ|x1)L(θ|x2)
= p(θ)L(θ|x1)L(θ|x2)
= p(θ)L(θ|x1, x2).
(2.19)
Needless to say this process can be repeated ad inﬁnitum and the model will continue to
update the posterior conclusions as new information arrives. This cycle of prior to posterior
is actually a very principled way of conceptualizing the scientiﬁc process: we take what

54
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
knowledge we have in hand and update it with new information when such results become
available.
■Example 2.5:
Timing of Campaign Polls.
The updating characteristic of the
Bayesian framework is ideal for analyzing time-series, either statically (after all the
data have been collected) or dynamically (as it comes in). In many commercial and
economic settings, data analysis is often performed continually as the daily, monthly,
or yearly ﬁgures arrive. In modern elections for political oﬃce, campaigns will do
multiple polls of the relevant electorate in order to update their strategies. Suppose
a campaign observes the candidate support ﬁgures for period t, supportt, after the
conclusion of the period. The question is: given the known information, including
previous polling data prior to period t (Dt), what does this tell us about support
for the next period? Formalized, this means that we have a posterior at the end of
time period t that reﬂects the current understanding of the underlying nature of the
support pattern for the candidate:
π(supportt|Dt).
(2.20)
Now before viewing the next period’s support ﬁgures, we treat this exact same distri-
bution as a new prior:
p(supportt+1) = π(supportt|Dt).
(2.21)
After period t + 1, we have new support data and create a new posterior:
π(supportt+1|Dt+1) = p(supportt+1)p(Dt+1).
(2.22)
Suppose that after period t, we were informed that a competing candidate suﬀered a
public scandal. This is also new information (It) and would be incorporated in our
prior for the next period’s support:
p(supportt+1) = π(supportt|Dt, It).
(2.23)
What this means is that we should adjust our electoral expectations based on past
support and this new information. The determination is still probabilistic because
we cannot know for certain whether support will increase due to the competitors’
misfortune: it could be that many of their supporters will move their intended vote to
a third candidate, or they could be suﬃciently loyal that the scandal is disregarded.
In a campaign environment, we might also want to predict support for future peri-
ods beyond a single period into the future. This is done by applying the sequential
property of Bayes’ Law:
p(supportt+2) = p(supportt+1|Dt)π(supportt|Dt).
(2.24)
This process can also be extended further into the future: p(supportt+k), albeit at
the cost of progressively increasing uncertainty.

Specifying Bayesian Models
55
■Example 2.6:
Example:
A Bayesian Meta-Estimate of Deaths in Stalin’s
Gulags. It is very diﬃcult to obtain a reliable estimate of the number of people that
perished in the Soviet Gulags (forced labor camps) during Stalin’s era as dictator
(1924-1953). While there will apparently never be a deﬁnitive answer (Solzhenitsyn
1997), Blyth (1995) uses Bayesian conditional inference to provide a meta-estimate
based on the best guesses of multiple historical researchers. The basic idea is to take
summary notions of the number of deaths by these experts and translate them into
workable probability functions using Lindley’s (1983) location-scale translation (and
adjusting them by subjective assessments of possible prejudices). Building a likelihood
function based on these estimates would generally be straightforward multiplicatively
with independent guesses, except that the experts have seen and are inﬂuenced by
each others’ work. Blyth’s solution to this nonindependence is to explicitly recognize
the chronology, and to build the likelihood function by conditional updating.
Denote all widely available knowledge on the scale of Gulag deaths from demographics,
journalistic descriptions, and published personal accounts as X.
The opinions of
four experts and their associated estimates are considered by translating each best
guess and level of uncertainty into a normal speciﬁcation, or in the case where no
uncertainty is given a uniform speciﬁcation. For instance, in the case of one expert
who estimates 10 to 20 million deaths, this is treated as a 95% credible interval. The
normal distribution then is obtained by backing out the resulting coverage probability.
The result is the following list of chronologically conditional statements:
Wiles, 1965:
p(θ1|X) ∼U(θ)
Kurganov, 1973:
p(θ2|θ1, X) ∼U(θ)
Conquest, 1978:
p(θ3|θ2, θ1, X) ∼N(18.2, 8.5)
Medvedev, 1989:
p(θ4|θ3, θ2, θ1, X) ∼N(12, 9).
Therefore the likelihood function from these “data” is:
L(θ|θ4, θ3, θ2, θ1, X) = p(θ4|θ3, θ2, θ1, X)p(θ3|θ2, θ1, X)p(θ2|θ1, X)p(θ1|X).
(2.25)
The “supra-Bayesian” posterior developed here is modeled as a normal weighted
by the precisions with the assumption that the intermediate conditionals are nor-
mal, N(μi, σ2
i ), and this posterior form is therefore given by N(μπ, σ2
π) where σ2
π =
4
i=1 σ−2
i
−1
and μπ = σ2
π
4
i=1(μi/σ2
i ). Blyth assigns the relatively “diﬀuse” nor-
mal prior (i.e., widely spread out by specifying a large variance parameter) N(8, 12)
and produces the posterior:
π(θ|θ4, θ3, θ2, θ1, X) ∝p(θ)L(θ|θ4, θ3, θ2, θ1, X) ∼N(13.2, 3.2).

56
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
This translates to a 95% credible interval of [9.7 : 16.7] million deaths, which is a
compromise between the four experts and the author of the meta-analysis.
2.5
Comments on Prior Distributions
The most controversial aspect of Bayesian statistics is the necessary assignment of a prior
distribution. The primary criticism here is that this is a subjective process or worse yet,
that it is a tool that allows researchers to manipulate the probability calculations to obtain a
desired result. In truth, there exist subjective aspects of every statistical model, including:
the experimental design or observational setting that produces the data, the parametric
form of the model, the speciﬁcation of explanatory variables, the choice of hypotheses to
be tested, the selected signiﬁcance level, and the determination of an adequate sample size
(Barnett 1973, p.160; Howson and Urbach 1993, p.12). Obviously we should add the choice
of prior distribution to this list, but note that Bayesians spend considerably more time
and energy defending a prior distribution than non-Bayesians do justifying other subjective
decisions.
Prior distributions can be categorized as either proper or improper. Proper priors meet
the Kolmogorov axioms, most speciﬁcally that they integrate or sum to a ﬁnite value. A non-
normalized prior that integrates or sums to some positive value other than one can always be
renormalized, and this distinction is immaterial with Bayes’ Law expressed proportionally
anyway. Improper priors are those that sum or integrate to inﬁnity, and yet they are useful
and play an important role in Bayesian inference.
Importantly, a standard maximum likelihood inferential model is identical to a Bayesian
model in which the prior probability distribution is an appropriate (correctly bounded for
the parameter at hand) uniform distribution function, and the two models are asymptoti-
cally identical for any proper prior distribution. Speciﬁcally, if ˆθ is the MLE and ˜θ is the
posterior mean from a Bayesian model using the same likelihood, but any proper prior (and
most improper priors), then:
√n(˜θ −ˆθ) −→
n→∞0
(2.26)
almost assuredly for reasonable starting values of θ (Chao 1970). This is not to say that
prior distributions are actually unimportant, but rather that in the presence of overwhelming
data size we should not care about whether the inferential model puts non-zero mass on
the prior or not. More practically, there are plenty of instances where we cannot rely on
data size alone to drive the quality of statistical inference. Is it unreasonable to study 25
European Union countries, 7 Central American countries, a set of small group experiments,
presidential nominees, 15 CIS countries, classroom level education, or other comparable
problems?

Specifying Bayesian Models
57
The strongest substantive argument for inclusion of priors is that there often exists sci-
entiﬁc evidence at hand before the statistical model is developed and it would be foolish to
ignore such previous knowledge (Tiao and Zellner 1964a; Press 1989, Section 2.7.1). Fur-
thermore, a formal statement of the prior distribution is an overt, nonambiguous assertion
within the model speciﬁcation that the reader can accept or dismiss (Box and Tiao 1973,
p.9; Gelman et al. 2003, p.14). Also, imprecise or vague knowledge often justiﬁes a diﬀuse
(very large variance) or even uniform (ﬂat) prior if bounded (Jeﬀreys 1961, Chapter III;
Zellner 1971, p.41ﬀ), and certain probability models logically lead to particular forms of
the prior for mathematical reasons (Good 1950; Press 1989).
An immediate payoﬀfor applying this Bayesian framework is that it facilitates the
explicit comparison of rival models about the system under study: H1 and H2 (even if
these are not nested models). In a preview of Chapter 6, suppose Γ1 and Γ2 represent two
competing hypotheses about the location of some unknown parameter, γ, which together
form a partition of the sample space: Γ = Γ1 ∪Γ2. Initially prior probabilities are assigned
to each of the two outcomes:
p1 = p(γ ∈Γ1)
and
p2 = p(γ ∈Γ2).
(2.27)
This allows us to calculate the posterior probabilities from the two alternative priors and
the likelihood function:
π1 = p(γ ∈Γ1|D, H1)
and
π2 = p(γ ∈Γ2|D, H2).
(2.28)
The Bayes Factor combines the prior odds, p1/p2, and the posterior odds, π1/π2, as evidence
for H1 versus H2 by calculating the ratio:
B = (π1/π2)
(p1/p2)
(2.29)
(Berger 1985; Kass and Raftery 1995; Lee 2004). Thus the Bayes Factor is the odds favoring
H1 versus H2, given the observed data incorporating both prior and posterior information.
As we will see in Chapter 7, this Bayes Factor model testing framework is even more ﬂexible
than this discussion implies.
2.6
Bayesian versus Non-Bayesian Approaches
There is a long history of antagonism between Bayesians and those adhering to strongly
classical approaches: frequentist methods from Neyman and Pearson, and likelihood based
methods from Fisher.
However, this disagreement has greatly diminished over the last
three decades.
The core frequentist paradigm bases a sampling model on an imagined
inﬁnite series of replications of the same analysis where the reliability of the calculated

58
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
statistics is derived from their asymptotic properties. The likelihood approach is diﬀerent
in that only the currently observed sample is considered and statistics are produced from
these data to estimate unknown population parameters by determining the value that is
most likely, given that observed sample.
The likelihood theorem (Birnbaum 1962) states that all information that can be used for
estimating some unknown parameter is contained in the likelihood function: a statement
about the most likely value of the parameter given the observed data, and the Bayesian
approach uses this same likelihood function (albeit with the addition of a prior distribution
on the unknown parameters). Thus for likelihood inference, all information needed from the
sample comes from the likelihood function. This does not mean that there is no additional
information in the sample for other forms of inference.
We know that every likelihood
model is actually a Bayesian model with the appropriately bounded uniform prior and
every Bayesian model is asymptotically equivalent to a corresponding likelihood model for
any given prior. Therefore likelihoodists are simply Bayesians that do not know it or do
not care to worry about the convenience of describing unknown quantities probabilistically.
This means that the real diﬀerences are with classical frequentists.
There is a second distinction that causes more disagreement than it should. In clas-
sical inference, one assumes that the population parameters are ﬁxed and unknown and
therefore estimated with sample quantities. Conversely in Bayesian inference unknown in-
ference parameters are treated as random quantities as a consequence of the application
of Bayes’ Law to invert the conditional probability statement. Actually this distinction
is not very important in practice as the frequentist “sampling distribution” is exactly the
same principle as the Bayesian posterior distribution, except that the imagined asymptote
is unavailable. This explains why one hears social science researchers applying traditional
inference procedures still using the word “posterior.” Perhaps they have become frustrated
with the diﬃculty in teaching the distinction between a sample distribution and a sampling
distribution. Furthermore, Lewis (1994) points out the easily observed, but often forgot-
ten fact that “Most applied statisticians have little interest in confrontation between rival
philosophies but have a keen interest in pragmatic solutions to real problems . . . .” This is
true of quantitative social scientists in particular.
A substantial amount of frequentist theory is built on the asymptotic normality of the
sampling distribution of calculated statistics, and the associated calculation of such prop-
erties (Barndorﬀ-Nielson and Cox 1989).
Associated with this is the assumption of an
unending stream of iid data. While Bayesian inference does not assume inﬁnite replications
to deﬁne sampling distributions, the posterior, being a compromise between the prior and
the likelihood, will be aﬀected by the same asymptotic properties as the amount of the data
increases. Laplace was the ﬁrst to note the near-normality of posterior distributions, as long
ago as 1811! This property was later fully explored around the 1960s (Chao 1963, 1965),
and Diaconis and Freedman (1986) subsequently gave mathematically rigorous conditions
for the consistency of these Bayesian estimates, thus subjecting frequentist and Bayesian
procedures to the same quality standard. 1970; Fabius 1964; Freedman

Specifying Bayesian Models
59
So it is important to understand where Bayesian inference stands relative to the Neyman-
Pearson frequentist paradigm. We can now tabulate core diﬀerences between Bayesian and
frequentist approaches. Most of these contrasts have been noted already in this chapter,
and simply summarized here. This is done in the context of the following categories:
Interpretation of Probability
Frequentist:
Observed result from inﬁnite series of trials performed
or imagined under identical conditions.
Probabilistic quantity of interest is p(data|H0).
Bayesian:
Probability is the researcher/observer “degree of belief”
before or after the data are observed.
Probabilistic quantity of interest is p(θ|data).
What Is Fixed and Variable
Frequentist:
Data are an iid random sample from continuous stream.
Parameters are ﬁxed by nature.
Bayesian:
Data observed and so ﬁxed by the sample generated.
Parameters are unknown and described distributionally.
How Results Are Summarized
Frequentist:
Point estimates and standard errors.
95% conﬁdence intervals indicating that 19/20 times the
interval covers the true parameter value, on average.
Bayesian:
Descriptions of posteriors such as means and quantiles.
Highest posterior density intervals indicating region of
highest posterior probability.
Inference Engine
Frequentist:
Deduction from p(data|H0), by setting α in advance.
Accept H1 if p(data|H0) < α.
Accept H0 if p(data|H0) ≥α.
Bayesian:
Induction from p(θ|data), starting with p(θ).
100(1 −α)% of highest probability levels in 1 −α HPD region.
Quality Checks
Frequentist:
Calculation of Type I and Type II errors.
Sometimes: eﬀect size and/or power.
Usually: attention to small diﬀerences in p-values.
Bayesian:
Posterior predictive checks.
Sensitivity of the posterior to forms of the prior.
Bayes Factors, BIC, DIC (see Chapter 7).
In some ways, the seemingly wide gap between frequentist/likelihoodist and Bayesian
thinking outlined above is an artiﬁcial and superﬁcial divide.
The maximum likelihood
estimate is equal to the Bayesian posterior mode with the appropriate (correctly bounded)
uniform prior, and they are asymptotically equal and normal given any proper prior (i.e.,
meeting the Kolmogorov axioms). Both approaches make extensive use of the central limit

60
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
theorem and normal theory in general. However, diﬀerences are seen particularly in small
sample problems where the asymptotic equivalence is obviously not applicable. A common
frequentist criticism of the Bayesian approach is that “subjective” priors have great impact
on the posterior distribution for problems with small sample sizes. There is a developing
literature on robust Bayesian analysis that seeks to mitigate this problem by developing
estimators that are relatively insensitive to a wide range of prior distributions (Berger
1984).
■Example 2.7:
The Timing of Polls. Bernardo (1984) developed a precinct-level
Bayesian hierarchical model of vote choice for the Spanish election of 1982 in which
the Socialist party obtained control of the government for the ﬁrst time since the Civil
War. Bayesian hierarchical models recognize and organize diﬀering levels of data and
prior information (see Chapter 12 for more details). The author deﬁnes nij as the
number of voters in the ith precinct voting for the jth party.
The data from the
m precincts surveyed ({n1,j, n2,j, . . . , nm,j}, j = 1 to 5 major political parties) are
assumed to be from an underlying multinomial distribution with unknown parameters
θij representing the probability of a vote for the jth party in the ith precinct with the
constraints that these values are nonnegative and sum to one. Bernardo speciﬁes a
uniform prior distribution on the θij values and this leads naturally to a Dirichlet
form (a multivariate generalization of the beta) of the posterior.
TABLE 2.2:
HPD Regions: Predicted 1982 Vote Percentages
for Five Major Parties
Valencia
Party
Socialist
Conservative
Center
Center-Left
Communist
Four weeks
before election
[39.0:48.9]
[12.6:19.4]
[7.9:13.2]
[4.0: 7.8]
[5.1:9.2]
One week
before election
[47.3:54.2]
[13.3:24.9]
[5.0:11.8]
[7.0:11.9]
[4.0:6.7]
First 100 votes
from 20 polls
[49.0:57.7]
[23.5:31.2]
[2.3: 4.6]
[1.1: 2.9]
[4.0:6.2]
Total vote
from 20 polls
[50.1:56.8]
[26.6:32.6]
[3.3: 4.6]
[1.8: 2.7]
[3.7:5.6]
Actual results
53.5
29.4
4.4
2.3
5.3
A substantively interesting aspect of the methodology is the scheduling of data col-
lection and analysis. Data are collected in the province of Valencia at four points in
time: by a survey four weeks before the election (n = 1, 000), by a survey one week
before the election (n = 1, 000), using the ﬁrst 100 valid votes from 20 representative
polling stations, and all valid votes from these same polling stations after the polls
are closed. Data collection was performed with the full cooperation of the Spanish
government and the results were immediately provided to the national media.

Specifying Bayesian Models
61
Bernardo presents the Bayesian estimates of predicted vote proportion by party as
0.90 highest posterior density regions. These results are summarized in Table 2.2.
One interesting result from this analysis is that the 0.90 HPD regions shrink as the ﬁnal
tally nears and better polling data are received. This reﬂects the growing certainty
about the estimates as data quality improves. In addition, the estimates from actual
polling data are remarkably accurate for the two parties receiving the largest vote
share. Note that the uniform prior does not constrain the ﬁnal, highly nonuniform,
posterior distribution.
The last example demonstrates that Bayesian data analysis is essentially free from the
well-known problems with the null hypothesis signiﬁcance test. Inferences are communi-
cated to the reader without artiﬁcial decisions, p-values, and confused conditional proba-
bility statements. The Bayesian approach also interprets sample size increases in a more
desirable manner: larger sample sizes reduce the importance of prior information rather
than guarantee a low but meaningless p-value.
2.7
Exercises
2.1
Suppose that 25 out of 30 ﬁrms develop new marketing plans during the next year.
Using the beta-binomial model from Section 2.3.4 starting on page 49, apply a
BE(0.5, 0.5) (Jeﬀreys prior) and then specify a normal prior centered at zero and
truncated to ﬁt on [0 : 1] as prior distributions and plot the respective posterior
densities. What diﬀerences do you observe?
2.2
Derive the posterior distribution for a sample of size n of iid data distributed
f(X|θ) = θe−θX, X, θ ∈(0, ∞)), with a prior for θ, p(θ) = βe−θβ, β ∈(0, ∞).
What is common to all three distributions?
2.3
Prove that the gamma distribution,
f(μ|α, β) =
1
Γ(α)βαμα−1e−βμ,
μ, α, β > 0,
is the conjugate prior distribution for μ in a Poisson likelihood function,
f(y|μ) =
* n
+
i=1
yi!
,−1
exp

log(μ)
n

i=1
yi

exp[−nμ],
that is, calculate a form for the posterior distribution of μ and show that it is also
gamma distributed.
2.4
One requirement for specifying prior distributions is that the support of the assigned

62
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
prior must match the allowable range of the parameter being modeled. What com-
mon distributions, without modiﬁcation, can be used as priors on model variance
components?
2.5
Use the gamma-Poisson conjugate speciﬁcation developed in Exercise 2.3 to analyze
the following count data on worker strikes in Argentina over the period 1984 to 1993,
from McGuire (1996). Assign your best guess as to reasonable values for the two
parameters of the gamma distribution: α and β. Produce the posterior distribution
for μ and describe it with quantiles and graphs using empirically simulated values
according to the following procedure:
▷The posterior distribution for μ is gamma(δ1, δ2) according to some parameters
δ1 and δ2 that you derived above, which of course depends on your choice of
the gamma parameters.
▷Generate a large number of values from this distribution in R, say 10,000 or
so, using the command:
posterior.sample <- rgamma(10000,d1,d2)
▷Produce posterior quantiles, such as the interquartile range, according to:
iqr.posterior <- c(sort(posterior.sample)[2500],
sort(posterior.sample)[7500])
Note: the IQR function in R gives a single value for the diﬀerence, which is not
as useful.
▷Graph the posterior in diﬀerent ways, such as with a smoother like lowess (a
local-neighborhood smoother, see Cleveland [1979, 1981]):
post.hist <- hist(posterior.sample,plot=F,breaks=100)
plot(lowess(post.hist$mids,post.hist$intensities),
type="l")
Economic Sector
Number of Strikes
Public Administrators
496
Meat Packers
56
Teachers
421
Paper Industry Workers
55
Metalworkers
199
Sugar Industry Workers
50
Municipal Workers
186
Public Services
47
Private Hospital Workers
181
University StaﬀEmployees
43
Bank Employees
133
Telephone Workers
39
Court Clerks
128
Textile Workers
37
Bus Drivers
113
State Petroleum Workers
32
Construction Workers
92
Food Industry Workers
28
Doctors
83
Post Oﬃce Workers
26
Nationalized Industries
77
Locomotive Drivers
25
Railway Workers
76
Light and Power Workers
21
Maritime Workers
57
TOTAL
2701

Specifying Bayesian Models
63
2.6
For θ ∼binomial(10, 0.5) construct an even tail credible interval that has at least
0.90 coverage. Is it possible to get exact coverage?
2.7
In his original essay (1763, p.376) Bayes oﬀers the following question:
Given the number of times in which an unknown event has happened
and failed: Required the chance that the probability of its happening in
a single trial lies somewhere between any two degrees of probability that
can be named.
Provide an analytical expression for this quantity using an appropriate uniform
prior (Bayes argued reluctantly for the use of the uniform as a “no information”
prior: Bayes postulate).
2.8
Given a proper prior distribution, p(θ), and a likelihood function, L(θ|X), demon-
strate that the only way that the prior distribution and the resulting posterior
distribution, π(θ|X) can be identical is when the likelihood function does not con-
tain θ.
2.9
Suppose we have two urns containing marbles; the ﬁrst contains 6 red marbles and
4 green marbles, and the second contains 9 red marbles and 1 green marble. Now
we take one marble from the ﬁrst urn (without looking at it) and put it in the
second urn. Subsequently, we take one marble from the second urn (again without
looking at it) and put it in the ﬁrst urn. Give the full probabilistic statement of
the probability of now drawing a red marble from the ﬁrst urn, and calculate its
value.
2.10
In an experimental context Gill and Freeman (2013) ask participants to answer
a wide range of background questions prior to eliciting prior distributions from
watching video clips. One of these,
“What proportion (percent) of undergraduate students at the University
of Minnesota are women?”
generates the following response times in seconds to this question:
7
7
11
7
7
10
7
5
8
7
5
7
12
6
8
7
8
7
28
13
6
4
10
6
13
11
6
14
4
7
12
16
8
9
8
9
4
5
8
4
5
15
9
7
7
8
4
9
7
9
19
19
9
7
5
6
6
17
7
6
10
7
15
Assume that the distribution of these times is G(4.5, 2) (shape and scale). Find
and graph a 95% HPD region for an additional sample point drawn from the same
population. Now suppose we do not know the distribution with certainly but impose
a prior distribution that is G(3, 3). Find and graph the resulting 95% HPD regions
for the posterior distribution.

64
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
2.11
This is the famous envelope problem. You and another contestant are each given
one sealed envelope containing some quantity of money with equal probability of
receiving either envelope. All you know at the moment is that one envelope contains
twice the cash as the other. So if you open your envelope and observe $10, then the
other envelope contains either $5 or $20 with equal probability. You are now given
the opportunity to trade with the other contestant. Should you? The expected
value of the unseen envelope is E[other] = 0.5(5) + 0.5(20) = 12.50, meaning that
you have a higher expected value by trading. Interestingly, so does the other player
for analogous reasons. Now suppose you are oﬀered the opportunity to trade again
before you open the newly traded envelope. Should you? What is the expected
value of doing so? Explain how this game leads to inﬁnite cycling. There is a
Bayesian solution.
Deﬁne M as the known maximum value in either envelope,
stipulate a probability distribution, and identify a suitable prior.
2.12
Radiocarbon dating of the famous Shroud of Turin cloth that some believe was used
to wrap Jesus Christ’s body (since it has front and rear impressions of a bearded
male with whipping and cruciﬁxion injuries) was done by the “Arizona Group”
(Linick et al. 1986) using accelerator mass spectrometry. Their serial process in
1988 produced the following estimated ages in years and associated standard errors:
Iteration
Mean
SE
1
606
51
2
574
52
3
753
51
4
632
49
5
676
59
6
540
57
7
701
47
8
701
47
As done in Example 2.4 starting on page 55, treat these as consecutive updates
on the posterior distribution and produce the set of posterior distributions under
normally distributed assumptions for each. Stipulate a reasonable prior to begin
the process.
2.13
If the posterior distribution of θ is N(1, 3), then calculate a 99% HPD region for θ.
2.14
Given a posterior distribution for θ that is BE(0.5, 0.5), calculate the 95% HPD
region for θ.
2.15
Assume that the data [1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1] are produced
from iid Bernoulli trials.
Produce a 1 −α credible set for the unknown value
of p using a uniform prior distribution.

Specifying Bayesian Models
65
2.16
Browne, Frendreis, and Gleiber (1986) tabulate complete cabinet duration (consti-
tutional inter-election period) for eleven Western European countries from 1945 to
1980 for annualized periods:
Country
N
Average Duration
Italy
38
0.833
Finland
28
1.070
Belgium
27
1.234
Denmark
20
1.671
Norway
17
2.065
Iceland
15
2.080
Austria
15
2.114
West Germany
15
2.168
Sweden
15
2.274
Ireland
14
2.629
Netherlands
12
2.637
The country averages from the third column of the table are weighted by N in
the second column to reﬂect the number of such events: XiNi. Assume that the
durations, X, are exponentially distributed p(X|θ) = θe−θX deﬁned over (0, ∞),
and like Example 2.3.2.1 on page 44 specify the prior distribution of p(θ) = 1/θ,
for θ ∈(0:∞). Calculate an equal tail credible interval and an HPD region for the
resulting posterior distribution of θ. Plot the posterior density and indicate the
location of these intervals.
2.17
The beta distribution, f(x|α, β) =
Γ(α+β)
Γ(α)Γ(β)xα−1(1−x)β−1, 0 < x < 1, α > 0, β > 0,
is often used to model the probability parameter in a binomial setup. If you were
very unsure about the prior distribution of p, what values would you assign to α
and β to make it relatively “ﬂat”?
2.18
An improper prior distribution is a function that does not sum or integrate to a
ﬁnite constant. Show that it is possible to still get a proper posterior distribution
through (2.6). A possible prior for μ in a Poisson likelihood function is p(μ) = 1/μ.
Show that this is improper.
2.19
Laplace (1774, p.28) derives Bayes’ Law for uniform priors. His claim is
. . . je me propose de d´eterminer la probabilit´e des causes par les ´ev´enements
mati`ere neuve `a bien des ´egards et qui m´erite d’autant plus d’ˆetre cultiv´ee
que c’est principalement sous ce point de vue que la science des hasards
peut ˆetre utile dans la vie civile.
He starts with two events: E1 and E2 and n causes: A1, A2, . . . , An. The assump-
tions are: (1) Ei are conditionally independent given Ai, and (2) Ai are equally

66
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
probable. Derive Laplace’s inverse probability relation:
p(Ai|E) =
p(E|Ai)

j p(E|Aj).
2.20
Martins (2009) is concerned with Bayesian updating by interacting actors who
pay attention to each others’ choices. Actor i has a prior distribution fi(θ), and
E[θ] = xi. This actor’s posterior for θ is aﬀected by the average estimates of others
xj, giving fi(θ|xj). Show that the mixture likelihood:
f(xj|θ) = ωN(θ, σj) + (1 −ω)U(0, 1),
(for mixture parameter ω), leads to the posterior:
f(θ|xj) ∝ω exp
'
−1
2σ2
i
((θ −xi)2 + (xj −θ)2)
(
(1 −ω) exp
'
−1
2σ2
i
(xi −xj)2
(
.
2.8
Computational Addendum: R for Basic Analysis
This code gives the analysis and graphing of the cultural anthropology example in Sec-
tion 2.3.4 starting on page 51.
par(oma=c(1,1,1,1),mar=c(0,0,0,0),mfrow=c(2,1))
x <- c(1,1,1,1,0,1,1,0,1,0,1,1,1,0,1,1,1,1,1,1,0,0,0,1)
ruler <- seq(0,1,length=300)
A <- 15; B <- 2
beta.prior <- dbeta(ruler,A,B)
beta.posterior <- dbeta(ruler,sum(x)+A,length(x)-sum(x)+B)
plot(ruler,beta.prior, ylim=c(-0.7,9.5),
xaxt="n", yaxt="n", xlab="", ylab="", pch=".")
lines(ruler,beta.posterior)
hpd.95 <- qbeta(c(0.025,0.975),sum(x)+A,length(x)-sum(x)+B)
segments(hpd.95[1],0,hpd.95[2],0,lwd=4)
text(mean(hpd.95),-0.4,"95% HPD Region",cex=0.6)
text(0.25,5,paste("Beta(",A,",",B,
") prior,
95% HPD Regionat: [",round(hpd.95[1],3),
":",round(hpd.95[2],3),"]",sep=""),cex=1.1)
A <- 1; B <- 1
beta.prior <- dbeta(ruler,A,B)
beta.posterior <- dbeta(ruler,sum(x)+A,length(x)-sum(x)+B)
plot(ruler,beta.prior, ylim=c(-0.7,9.5),
xaxt="n", yaxt="n", xlab="", ylab="", pch=".")
lines(ruler,beta.posterior)

Specifying Bayesian Models
67
hpd.95 <- qbeta(c(0.025,0.975),sum(x)+A,length(x)-sum(x)+B)
segments(hpd.95[1],0,hpd.95[2],0,lwd=4)
text(mean(hpd.95),-0.4,"95% HPD Region",cex=0.6)
text(0.25,5,paste("Beta(",A,",",B,
") prior,
95% HPD Region at: [",round(hpd.95[1],3),
":",round(hpd.95[2],3),"]",sep=""),cex=1.1)
The following is the simple HPD region calculation used in Example 2.3.2.2.
hpd.gamma <- function(g.shape,g.rate,target=0.90,steps=300,tol=0.01)
{
if (steps %% 2 == 1) steps <- steps + 1
g.mode
<- sum(state.df$N)/sum(state.df$N*state.df$dur)
g.range <- seq(qgamma(0.001,g.shape,g.rate), qgamma(0.999,g.shape,
g.rate),length=steps)
g.range <- c(g.range[1:(steps/2)],g.mode,g.range[(steps/2+1):steps])
g.dens
<- dgamma(g.range,g.shape,g.rate)
g.probs <- pgamma(g.range,g.shape,g.rate)
for (i in 1:(steps/2)) {
k.dir <- which(c(g.dens[(steps/2-i)],g.dens[(steps/2+i)]) ==
max(g.dens[(steps/2-i)],g.dens[(steps/2+i)]))
k <- c(g.dens[(steps/2-i)],g.dens[(steps/2+i)])[k.dir]
k.loc <- c((steps/2-i),(steps/2+i))[k.dir]
if (k.dir == 2) k2.range <- c(1:(steps/2))
else k2.range <- c((steps/2 + 1):steps)
k2.min <- which(abs(k-g.dens[k2.range])==min(abs(k-g.dens[k2.range])))
if (k.dir == 1) k2.min <- k2.min + steps/2
if (g.probs[k.loc] + (1-g.probs[k2.min]) < 1-target)
break
bounds <- c(g.range[k.loc],g.range[k2.min])
}
return(list("cdf.vals"=c(g.probs[k.loc],g.probs[k2.min]),
"bounds"=bounds,"k"=k))
}
state.hpd <- hpd.gamma(g.shape=sum(state.df$N),
g.rate=sum(state.df$N*state.df$dur))


Chapter 3
The Normal and Student’s-t Models
Statistical models built on the normal distribution are as common in Bayesian statistics
as they are in non-Bayesian approaches. There are several reasons for this. First, nature
seems to have an aﬃnity for this form as evidenced through empirical observation as well as
from the central limit theorem. The weakest form of the central limit theorem essentially
says that an interval measured statistic with bounded variance will eventually be normally
distributed provided suﬃcient sample size. Therefore it is quite common to see situations
where quantities behave approximately normally. Second, a huge class of posterior distri-
butions can be modeled by combining an assumed normal likelihood function with diﬀering
priors. Finally, when Bayesian models were more diﬃcult to estimate numerically, the nor-
mal distribution sometimes provided an analytically tractable posterior when other forms
were less compliant.
3.1
Why Be Normal?
Often when the posterior is known to be unimodal and symmetric, we can eﬀectively
model it with a normal distribution even if we know that the form is only nearly normal. In
cases where the researcher has a rough idea of where an unknown parameter is centered, the
normal provides a useful way of modeling this guess that allows the level of uncertainty to be
described by the normal variance term. This convenience can provide good approximations
to the desired posterior density with the knowledge that as more data are observed, such
assumptions decrease in importance.
As demonstrated below, the Bayesian normal model has desirable frequentist properties.
While the emphasis in Bayesian analysis is not on point estimates, it can be shown that
with increasingly large samples the mean of the Bayesian posterior approaches the maximum
likelihood estimate. This property exists because the posterior is a weighted compromise
between the user speciﬁed prior distribution, normal in this chapter, and the data-driven
likelihood function, also normal in this chapter. As the data size increases, the likelihood
becomes increasingly dominant in this weighting.
In the case of a normal mean, illustrated here, the variance of the frequentist sampling
distribution decreases with increases in sample size. In the Bayesian context, the shrinking
69

70
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
mean variance from the likelihood function eventually overwhelms even a deliberately large
prior variance. Therefore if the expected size of the dataset is large, researchers can aﬀord
to be liberal in specifying the prior variance.
3.2
The Normal Model with Variance Known
We begin with a very simple case. Suppose the data are assumed to follow a normal
distribution with known variance (σ2
0), and the unknown parameter of interest is the mean
of this distribution, which is itself given a normal prior with assigned parameters m and s2.
These assumptions are summarized as follows:
X|μ, σ2
0 ∼N(μ, σ2
0) = (2πσ2
0)−1
2 exp
'
−1
2σ2
0
(X −μ)2
(
−∞< μ < ∞, σ2
0
known
μ|m, s2 ∼N(m, s2) = (2πs2)−1
2 exp
'
−1
2s2 (μ −m)2
(
m, s2 known.
(3.1)
In this setup the parameters on the prior distribution (m,s2) are either known or assigned
for substantive reasons. After the data, X, are observed, the posterior distribution of μ is
produced by the product of the prior and the likelihood function:
π(μ|x) ∝p(x|μ)p(μ)
∝
n
+
i=1
exp
'
−1
2σ2
0
(xi −μ)2
(
exp
'
−1
2s2 (μ −m)2
(
= exp

−1
2
*
1
σ2
0
n

i=1
(xi −μ)2 + 1
s2 (μ −m)2
,
.
Now expand the squares.
= exp

−1
2
*
1
σ2
0
n

i=1
(x2
i −2xiμ + μ2) + 1
s2 (μ2 −2μm + m2)
,
= exp

−1
2
1
σ2
0s2
*
s2
n

i=1
x2
i −2s2μn¯x + nμ2s2 + σ2
0μ2 −2σ2
0μm + σ2
0m2
,
= exp

−1
2
1
σ2
0s2
*
μ2(σ2
0 + ns2) −2μ(mσ2
0 + s2n¯x) + (m2σ2
0 + s2
n

i=1
x2
i )
,
.

The Normal and Student’s-t Models
71
The last term in the expansion can be treated as part of the normalizing constant, k.
∝exp
'
−1
2

μ2
 1
s2 + n
σ2
0

−2μ
m
s2 + n¯x
σ2
0

+ k
(
= exp
⎡
⎣−1
2
 1
s2 + n
σ2
0
 ⎛
⎝μ2

1
s2 + n
σ2
0


1
s2 + n
σ2
0
 −2μ

m
s2 + n¯x
σ2
0


1
s2 + n
σ2
0
 + k
⎞
⎠
⎤
⎦
∝exp
⎡
⎢⎣−1
2
 1
s2 + n
σ2
0
 ⎛
⎝μ −

m
s2 + n¯x
σ2
0


1
s2 + n
σ2
0

⎞
⎠
2⎤
⎥⎦.
(3.2)
Therefore the posterior for μ is a normal distribution with mean:
ˆμ =
m
s2 + n¯x
σ2
0
 -  1
s2 + n
σ2
0

,
(3.3)
and variance:
ˆσ2
μ =
 1
s2 + n
σ2
0
−1
=
s2σ2
0
σ2
0 + ns2 .
(3.4)
Several points bear mentioning here. First, notice that the posterior for μ depends on the
data only through ¯x. Thus ¯x is a suﬃcient statistic in this context, meaning that this data
summary is all that is required from the data to estimate the unknown mean.1 Second, in
several instances we dropped constant terms out of the derivation above, because as long
as these are not changing relative values of the unknown parameter it is only necessary to
normalize at the end.
The term
1
s2 is called the prior precision, and the term
n
σ2
0 is called the data precision.
These terms play an important role in Bayesian statistics because they link uncertainty
from the prior assumptions and uncertainty from the observed data. The posterior precision,
easily obtained in this example, is just the sum of the prior precision and the data precision:
1
ˆσ2μ =
1
s2 +
n
σ2
0 . This is true in all normal-normal cases, not just the single example given
here.
As the data size increases, the value of the posterior mean, ˆμ, is increasingly determined
by the data mean, ¯x, making prior assumptions less important. Consider a ﬁxed value for
the data variance as we have done above, σ2
0, and let n go to inﬁnity.
The asymptotic posterior mean is given by:
lim
n→∞ˆμ = lim
n→∞
m
s2 + n¯x
σ2
0
1
s2 + n
σ2
0
= lim
n→∞
mσ2
0
ns2 + ¯x
σ2
0
ns2 + 1
= ¯x,
(3.5)
1More technically, given the sample X whose distribution is conditional on a (possibly vector) parameter
τ, a (possibly vector) t(X) is jointly suﬃcient for τ if the likelihood, ℓ(τ|X) is proportional to some function
g(τ|t(X)) (Box and Tiao 1973). The suﬃciency principle states that the distribution of X depends on τ
only through t(X) (Casella and Berger 2002, p.272; Hogg and Craig 1978, p.343). Suﬃciency is one of the
central contributions of Fisher (1922, 1925a, 1925b).

72
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
and the asymptotic posterior variance is given by:
lim
n→∞ˆσ2
μ = lim
n→∞
1
1
s2 + n
σ2
0
= lim
n→∞
σ2
0
σ2
0
s2 + n
= σ2
0
n .
(3.6)
The asymptotic distribution of this Bayesian posterior mean is N(¯x, σ2
0
n ), which is exactly
the result from frequentist theory (Berger 1985, p.224). In fact this result is sometimes
called the “Bayesian Central Limit Theorem” (Carlin et al. 1993), and the importance of
this is that Bayesian and frequentist results are identical in the limit. Furthermore, if we
choose a huge value for the prior variance of μ, including the assignment s2 = ∞, then
we also get the frequentist result. This indicates that total or near total prior ignorance
about the variance of μ translates to the same results as the standard likelihood model.
Conversely, the frequentist approach unrealistically assumes the presence of asymptotic
properties even in small sample analyses.
3.3
The Normal Model with Mean Known
Suppose, instead of knowing the variance parameter for normal data and obtaining a
posterior distribution for the mean, that we know the mean parameter and estimate the
variance. Speciﬁcally:
p(X|μ0, σ2) = (2πσ2)−1
2 exp
'
−1
2σ2 (X −μ0)2
(
.
Here the notation μ0 indicates a known value for μ and the absence of a subscript on σ2
reminds us that this is the target of our investigation. Given an iid sample of size n, we
have the likelihood function:
L(σ2|x) ∝(σ2)−n
2 exp
'
−
n
2σ2
*
1
n
n

i=1
(xi −μ0)2
,
 
!"
#
suﬃcient statistic
(
.
(3.7)
Here the expression 1
n
n
i=1(xi −μ0)2 is a suﬃcient statistic for σ2, and we need no other
information from the data. In fact, we can label this suﬃcient statistic as ˜x to make the
notation easier from this point on.
The conjugate prior for the variance parameter in the normal likelihood model is the
inverse gamma distribution, meaning that an inverse gamma form for the prior on σ2 gives
an inverse gamma posterior for σ2 where parameters values change. The inverse gamma,
for this reason, is a common choice for Bayesian speciﬁcations, and is less well-known
in non-Bayesian settings. If a random variable X is distributed gamma (shape and rate
parameters), then the random variable 1/X is distributed inverse gamma (Berger 1985,

The Normal and Student’s-t Models
73
p.561, Shao 2005).
The probability density function of the inverse gamma is given in
Appendix B by:
IG(σ2|α, β) =
βα
Γ(α)(σ2)−(α+1) exp[−β/σ2]
where:
σ2 > 0, α > 0, β > 0.
(3.8)
The inverse gamma PDF is not as well behaved as one would expect for a distribution linked
theoretically to the normal. For instance the ﬁrst two moments need to be deﬁned with the
parameter restrictions:
E[σ2] =
β
α −1,
α > 1
Var[σ2] =
β2
(α −1)2(α −2),
α > 2.
(3.9)
0
5
10
15
0
0.2
0.4
0.6
0.8
1
1.2
IG(2,1)
IG(0.1,0.1)
IG(2,5)
IG(15,25)
IG(5,30)
FIGURE 3.1: A Menagerie of Inverse Gamma Forms
The inverse gamma distribution is also slightly less intuitive to specify a desired shape than
the gamma distribution. Figure 3.1 shows ﬁve diﬀerently parameterized forms from using
the dinvgamma(x,shape=,scale=) command in R (package MCMCpack). Notice that the
IG(0.1, 0.1) form rapidly ﬂattens out as it asymptotes to zero on the y-axis. Inverse gamma
priors with small and equal parameters are popular with BUGS users and appear frequently
in the example models provided with the software. Yet this form would hardly be called

74
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
a low information prior based on its form. An even more pronounced eﬀect comes from
using IG(1, ϵ), where ϵ is a very small number. This is also a popular variance component
prior speciﬁcation in the BUGS world, and it very rapidly declines and then near ﬂattens out
moving to the right.
We now have enough tools to develop the posterior distribution of σ2:
π(σ2|x) ∝L(σ2|x)p(σ2|α, β)
= (σ2)−n
2 exp

−n
2σ2 ˜x
 βα
Γ(α)(σ2)−(α+1) exp
	
−β/σ2
∝(σ2)−((α+ n
2 )+1) exp

−

β + n
2 ˜x
 .
σ2
.
(3.10)
It is easy to see the last line as the kernel (core component of the PDF or PMF stripped
of normalizing factors) of an IG(α + n
2 , β + n
2 ˜x) density function. This is a nice result since
we now have the exact form of the prior for σ2 without a lot of involved calculations.
While the derivation of π(σ2|x) illustrates conjugacy, since an inverse gamma prior led
to an inverse gamma posterior, it does not provide any guidance on the choice of values for
the parameters of the prior. Carlin and Louis (2001, p.326) suggest solving the moment
equations for α and β using the empirical mean and standard deviation, and substituting
the recommended value of 3 for both to get a relatively vague prior speciﬁcation. This is
a rudimentary empirical Bayes approach, and more details will be discussed in Chapter 8.
Although we know the value of the data mean here, we follow their advice for illustrative
purposes. Begin with:
μσ2 =
β
α −1
s2
σ2 =
β2
(α −1)2(α −2)
α = μ2
σ2
s2
σ2
+ 2
β = μσ2
μ2
σ2
s2
σ2
+ 1

.
Setting both μσ2 and sσ2 to 3 gives α = 3, and β = 6. Therefore the posterior distribution
of σ2 is:
π(σ2|X) ∝(σ2)−(( n
2 )+4) exp
	
−(6 + n˜x)/σ2
.
(3.11)
3.4
The Normal Model with Both Mean and Variance Unknown
We can now develop the model with both μ and σ2 unknown, with conjugate priors
for both. The new wrinkle is that the conjugate prior for the mean is conditional on the
variance. This is a mathematical necessity to preserve the “pass-through” nature of the
normal distributional form for conjugacy. Start again with normally distributed data with

The Normal and Student’s-t Models
75
mean μ and variance σ2. We will use the inverse gamma and normal conjugate priors:
p(σ2|α, β) ∝(σ2)−(α+1) exp
	
−β/σ2
(3.12)
p(μ|m, σ2/s0) ∝(σ2)−1
2 exp
'
−
1
2σ2/s0
(μ −m)2
(
,
(3.13)
where the prior for μ is explicitly conditional on σ2.
The parameter s0 is a so-called
“conﬁdence parameter” that measures the researcher’s strength of belief on σ2 on μ in the
prior speciﬁcation. This is intended to be a convenient feature of this model, but if one does
not want to be bothered by an additional parameter speciﬁcation, it can be set to 1 and
dismissed (although this too is a parameter speciﬁcation). Some authors choose to specify
more intricate parameterizations for the prior on σ2 in order to produce simpliﬁed marginal
posterior expressions, but we will stay with the same parameterizations as given above for
continuity and comparison. The resulting posterior is then:
π(μ, σ2|x) = p(x|μ, σ2)p(σ2|α, β)p(μ|σ2/s0, m)
∝(σ2)−α−n
2 −3
2 exp

−1
σ2 β −
1
2σ2
n

i=1
(xi −μ)2 −
1
2σ2/s0
(μ −m)2

= (σ2)−α−n
2 −3
2 exp

−1
σ2 β −
1
2σ2
* n

i=1
x2
i −2n¯xμ + nμ2
,
−
1
2σ2/s0
(μ2 −2μm + m2)

= (σ2)−α−n
2 −1 exp

−1
σ2 β −
1
2σ2
* n

i=1
x2
i −n¯x2
,
× (σ2)−1
2 exp
'
−1
2σ2

(n + s0)μ2 −2(n¯x + ms0)μ + (n¯x2 + s0m2)
(
(3.14)
The last form of the posterior obviously sets up easy integration across the two parameters
to get the marginal posterior distributions since the second term (line 2) is a normal kernel
for μ and the ﬁrst term (line 1) does not contain μ. First we perform this operation over μ
to get the posterior for σ2:
π(σ2|x) =
 ∞
−∞
π(μ, σ2|x)dμ
∝(σ2)−α−n
2 −1 exp

−1
σ2
*
β + 1
2
* n

i=1
x2
i −n¯x2
,,
.
(3.15)

76
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
Therefore, observing the trivial fact that −α −n
2 −1 = −(α + n/2) −1, the posterior
distribution of σ2 is another inverse gamma according to:
σ2|x ∼IG
*
α + n
2 , β + 1
2
n

i=1
(xi −¯x)2
,
.
(3.16)
Since the prior (and posterior) distribution of μ is conditional on σ2, we cannot integrate
as done above. Fortunately, there is a simple trick based on an obvious cancellation:
π(μ|σ2x) = π(μ, σ2|x)
π(σ2|x)
∝σ−1 exp
'
−1
2σ2

(n + s0)μ2 −2(n¯x + ms0)μ + (n¯x2 + s0m2)
(
= σ−1 exp
'
−
1
2σ2/(n + s0)

μ2 −2n¯x + ms0
n + s0
μ + n¯x2 + s0m2
n + s0
(
.
(3.17)
We can see now that the posterior distribution of μ is the normal:
μ|σ2, x ∼N
'n¯x + ms0
n + s0
,
σ2
n + s0
(
.
(3.18)
Note that the prior dependence on σ2 ﬂows through to the posterior for μ.
3.5
Multivariate Normal Model, μ and Σ Both Unknown
We will conclude our discussion of normal-conjugate models with the most intricate
case: both parameters unknown, and multivariate data. While this is the most complex
speciﬁcation in this family, it is also the most realistic and therefore worthy of considerable
attention here. The conjugate prior speciﬁcation has an added complexity because it is only
possible to specify it with a dependency of the mean on the variance: p(μ|σ2), and p(σ2).
If this is unrealistic, then a nonconjugate prior should be speciﬁed. The posterior is then
determined by:
π(μ, σ2, x) ∝L(μ, σ2|x)p(μ|σ2)p(σ2) = L(μ, σ2|x)p(μ, σ2).
(3.19)
This is given here for a univariate normal case only. For the multivariate normal, we assume
that each of the n rows of X is a k-dimensional vector representing a single case, so now
μ is a k-length vector and Σ is a k × k matrix, both to be estimated. From the PDF
of the multivariate normal (Appendix B), the likelihood function can be expressed and

The Normal and Student’s-t Models
77
manipulated as follows:
L(μ, Σ|X) =
n
+
i=1

(2π)−k/2|Σ|−1/2 exp
'
−1
2(xi −μ)′Σ−1(xi −μ)
(
∝|Σ|−n/2 exp

−1
2
n

i=1
(xi −μ)′Σ−1(xi −μ)

= |Σ|−n/2 exp

−1
2
n

i=1

x′
iΣ−1xi −2x′
iΣ−1μ + μ′Σ−1μ


= |Σ|−n/2 exp

−1
2
* n

i=1
x′
iΣ−1xi −nΣ−1¯x′¯x + nΣ−1¯x′¯x
−2n¯x′Σ−1μ + nμ′Σ−1μ
,
= |Σ|−n/2 exp

−1
2
*
tr(Σ−1)
* n

i=1
x′
ixi −n¯x′¯x
,
+ n(¯x −μ)′Σ−1(¯x −μ)
,
.
(3.20)
Since n
i=1(x′
ixi) −n¯x′¯x = n
i=1(xi −¯x)′(xi −¯x) ≡S2, then L(μ, Σ|X) is a function of
the data only through the two-component suﬃcient statistic: [¯x, S2].
Robert (2001, p.189) suggests the following form of the conjugate priors:
μ|Σ ∼Nk

m, 1
n0
Σ

,
Σ−1 ∼W(α, β),
(3.21)
where W() denotes the Wishart distribution, which is a multivariate generalization of the
gamma PDF (an obvious choice for modeling multivariate variances):2
W(Σ−1|α, β) = |Σ−1|(α−(k+1))/2
Γk(α)|β|α/2
exp[−tr(β−1Σ−1)/2]
where: Γk(α) = 2αk/2πk(k−1)/4
k
+
i=1
Γ
α + 1 −i
2

,
2α > k −1,
and β nonsingular.
(3.22)
The β matrix in the conjugate prior for Σ−1 need only be non-singular and symmetric (the
latter is an assumption of the application not the PDF), but in practice it is generally given
a diagonal form unless there are strong a priori reasons for assuming covariance terms. The
term Γk(α) is the k-dimensional generalized gamma function, and is ignorable except for
normalizing considerations. The parameter n0 in Robert’s prior parameterization is not a
2For mathematical and statistical properties of the Wishart distribution see Appendix B and the refer-
ences: Krzanowski (1988, pp.209-210), Tong (1990, pp.51-55), and Stuart and Ord (1994), p.573-579). For
generating procedures see Gentle (1998, p.107), Kleibergen and van Dijk (1993), and Smith and Hocking
(1972).

78
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
prior sample size; it is intended to be a reﬂection of prior precision relative to the sample
size that is tunable by the researcher to reﬂect prior conﬁdence as speciﬁed before in (3.13).
The smaller the ratio n0/n, the less weight on the prior, and therefore the closer the results
will be to classical results.
This setup leads to an articulation of the joint posterior:
π(μ, Σ|x) ∝|Σ|−n/2 exp

−1
2
*
tr(Σ−1S2) + n(¯x −μ)′Σ−1(¯x −μ)
,
×

Σ
n0

−1/2
exp

−1
2(μ −m)′
 1
n0
Σ
−1
(μ −m)

× |β|−α/2|Σ−1|(α−(k+1))/2 exp[−tr(β−1Σ−1)/2].
(3.23)
We can then proceed (with steps analogous to the production of marginals (3.16) and (3.18),
only more involved) to the posterior distributions of interest:
μ|Σ ∼Nk
n0m + n¯x
n0 + n
,
1
n0 + nΣ

Σ−1 ∼Wk
*
α + n,
'
β−1 + S2 +
n0n
n0 + n(¯x −m)(¯x −m)′
(−1,
.
(3.24)
Note that the dependency exists here in the multivariate case as well. We will see a more
elegant application of a Bayesian normal model in the context of linear regression in Chap-
ter 6. The distributional assignment for the variance described here is sometimes called the
inverse-Wishart since it is inversely assigned to the matrix. Gelman and Hill (2007, p.286)
note that the choice of degrees of freedom parameter (α) can add undesirable constraints
to estimation of Σ components, eﬀectively imposing a computational trade-oﬀbetween di-
agonal and oﬀ-diagonal elements. Their solution, the scaled inverse-Wishart distribution,
pre- and post-multiplies the inverse-Wishart with a vector of scale parameters.
■Example 3.1:
Variance Estimation with Public Health Data.
Consider data
from the 2000 U.S. census and North Carolina public records (North Carolina Division
of Public Health, Women’s and Children’s Health Section in Conjunction with State
Center for Health Statistics), which is available in the BaM package. Each case is
one of 100 North Carolina counties, and we will use only the following subset of the
variables.
▷Substantiated.Abuse: within family documented abuse for the county.
▷Percent.Poverty: percent within the county living in poverty, U.S. deﬁnition
(see http://www.census.gov/hhes/www/poverty
/threshld/thresh98.html).
▷Total.Population: county population/1000.

The Normal and Student’s-t Models
79
So each X row is a 3-dimensional vector representing a single case, distributed N(μ, Σ)
as in the model given above where both the mean and variance are unknown.
First we specify the relatively uninformed prior parameters α = 3, m = (250, 16, 88),
n0 = 0.01, along with a β, an identity matrix divided by 100 in this case (a selection
not related to n here), to deﬁne the priors from (3.21).
This produces marginal
posteriors according to (3.24), which we describe with posterior quantiles in the case
of the mean μ in the left-side of Table 3.1.
TABLE 3.1:
Posterior Summaries, Normal Models
μ
Uninformed Prior
Semi-Informed Prior
Quantile
Abuse
%Poverty
Population
Abuse
%Poverty
Population
0.01
195.8976
14.2399
77.9827
138.4181
9.19079
82.3306
0.25
199.6618
14.3123
79.7873
147.1816
9.90282
83.6443
0.50
201.2110
14.3409
80.5230
150.7495
10.18752
84.1989
0.75
202.7294
14.3698
81.2590
154.3365
10.47835
84.7918
0.99
206.4080
14.4400
83.0124
163.0994
11.15920
86.2538
Since Σ is a 3×3 matrix, each cell possessing a distinct posterior, we need to describe
nine posteriors. This description can be in tabular form like the posterior for μ, or
we can abbreviate it and just give nine posterior means arranged in the same tabular
positions:
¯Σ =
⎡
⎢⎣
531.5540
−3.2724
200.2079
−3.2724
0.1871
−1.6727
200.2079
−1.6727
117.9017
⎤
⎥⎦
To provide a contrast, specify strong priors, α = 3, m = (100, 6, 88), n0 = 99, and β
an identity matrix divided by 10. This now gives posterior values summarized in the
right-side of Table 3.1 and with the following matrix means for the posterior of Σ:
¯Σ =
⎡
⎢⎣
5678.6595
421.2349
−181.0511
421.2349
35.2098
−33.1597
−181.0511
−33.1597
146.3097
⎤
⎥⎦
As a way of seeing the eﬀect of the priors and the diﬀerences provided by the model,
we now compare the marginal posterior distribution of the ﬁrst cell value of the vari-
ance matrix from the diﬀuse model above having mean Σ[1, 1] = 531.5540 with the
likelihood function from a conventional non-Bayesian model. Figure 3.2 shows the two
forms along with the associated 95% conﬁdence and 95% credible interval endpoints.
Note that there are diﬀerences in the form of the distributions, as well as the fact that
interval inference would produce notably diﬀerent conclusions. Such diﬀerences are
observable even though the priors were set up to be relatively uninformed.

80
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
200
400
600
800
1000
0.000
0.001
0.002
0.003
0.004
0.005
Likelihood
Posterior
FIGURE 3.2: Posterior Distribution versus Likelihood Function
3.6
Simulated Eﬀects of Diﬀering Priors
To illustrate the diminishing eﬀect of the prior as the size of the data increases, we
ﬁrst generate progressively larger contrived data sets distributed bivariate normal with
mean vector [ 0
0 ], and variance matrix [ 1 0
0 1 ]. By giving the data this simple distribution,
the relative eﬀects of diﬀering and intentionally wrong priors will be more apparent. We
calculate and summarize the posterior distributions for three diﬀerent sample sizes, given
in Table 3.2.
In each case, 1,000 simulated values from the posterior distribution are generated and
the reported posterior statistics are the mean (ˆμ), the standard error (ˆσμ), and the 95%
HPD region. Note that this generation of simulated values is a very diﬀerent aspect of this
demonstration than the generation of the original data. Here the size of the original data
set is the quantity being varied. Once we have a parametric form of the prior, we can be
as arbitrarily accurate as our patience permits (some computers being faster than others),
since this is just a function of how long we want to run the simulation. The posterior
simulation size of 1,000 was chosen for convenience.
The prior given here is a reasonably misguided speciﬁcation: the prior normal mean
vector for μ is m = [ 5
5 ], α = 3 > (k −1)/2, and the prior Wishart parameters for Σ are
β = [ 5 0
0 5 ]. Table 3.2 shows the posterior eﬀect for all estimated parameters as sample size
goes from 10 to 100 to 1000.

The Normal and Student’s-t Models
81
TABLE 3.2:
A Bivariate Normal, 1000 Simulations
Speciﬁcation
Mean
Standard Error
95% HPD Interval
n = 10,
μ1
1.736
0.726
[ 0.276: 3.113]
p ∼N

5
5

5 0
0 5

μ2
1.399
0.893
[-0.559: 3.090]
σ1
8.175
3.618
[ 3.598:17.429]
σ2
12.076
5.762
[ 5.251:27.849]
ρ
0.836
0.088
[ 0.617: 0.952]
n = 100,
μ1
0.174
0.137
[-0.100: 0.435]
p ∼N

5
5

5 0
0 5

μ2
0.279
0.146
[ 0.001: 0.565]
σ1
2.106
0.286
[ 1.614: 2.718]
σ2
2.158
0.310
[ 1.639: 2.848]
ρ
0.563
0.066
[ 0.425: 0.679]
n = 1000,
μ1
0.023
0.034
[-0.051: 0.087]
p ∼N

5
5

5 0
0 5

μ2
-0.017
0.033
[-0.081: 0.043]
σ1
1.083
0.049
[ 0.991: 1.189]
σ2
1.164
0.051
[ 1.074: 1.272]
ρ
0.059
0.032
[-0.007: 0.124]
Original data generated from the N

0
0

1 0
0 1

distribution.
As the sample size increases by orders of magnitude, the posterior variance for each
estimated coeﬃcient decreases as indicated in (3.2). This example shows that the likelihood
function eventually overwhelms the prior speciﬁcation.
Were we to carry this example
further and produce larger initial sample sizes for the bivariate normal data, we could come
arbitrarily close to the standard maximum likelihood estimates.
Now we will replicate this simulation analysis with a slightly more interesting data
speciﬁcation. The three sample sizes will be generated as before, but now according to
mean [ 1
3 ], and variance matrix [ 1.0 0.7
0.7 3.0 ], the main point being the introduction of fairly high
data correlation. This time the prior speciﬁcation will again be deliberately incorrect, but
far more dispersed. This means that the increase in sample size according to the previous
scheme should take longer to overwhelm the prior. This is exactly what we observe in
Table 3.3.
Although the examples developed in these simulations are contrived, in that we assumed
knowledge of the true form of the population data, the principles observed are exactly those
that apply in more complex speciﬁcations. The point is to illustrate that the estimation
technique is reasonably easy to implement (the R code is contained in this chapter’s Com-
putational Addendum).

82
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
TABLE 3.3:
Another Bivariate Normal, 1000 Simulations
Speciﬁcation
Mean
Standard Error
95% HPD Interval
n = 10,
μ1
1.220
0.303
[ 0.567: 1.820]
p ∼N

2
2

10 5
5 10

μ2
2.676
0.409
[ 1.863: 3.498]
σ1
1.304
0.644
[ 0.546: 3.029]
σ2
2.490
1.286
[ 1.025: 5.836]
ρ
0.313
0.264
[-0.295: 0.749]
n = 100,
μ1
0.923
0.109
[ 0.707: 1.126]
p ∼N

2
2

10 5
5 10

μ2
2.909
0.186
[ 2.542: 3.289]
σ1
1.208
0.176
[ 0.925: 1.616]
σ2
3.507
0.476
[ 2.634: 4.532]
ρ
0.427
0.079
[ 0.260: 0.567]
n = 1000,
μ1
1.059
0.031
[ 0.996: 1.119]
p ∼N

2
2

10 5
5 10

μ2
2.994
0.055
[ 2.885: 3.105]
σ1
0.996
0.044
[ 0.919: 1.091]
σ2
2.914
0.135
[ 2.649: 3.176]
ρ
0.376
0.028
[ 0.320: 0.430]
Original data generated from the N

1
3

1.0 0.7
0.7 3.0

distribution.
3.7
Some Normal Comments
The primary purpose of this chapter is to illustrate the Bayesian normal model in detail.
While it is often not reasonable to make normal assumptions about the distribution of data
or parameters, such approaches are eﬀective surprisingly often. Given the knowledge that
normality is an asymptotic property of many of the estimators studied, the normal model
is well worth careful inspection. Excellent general expositions on further consequences of
the Bayesian linear model speciﬁcation can be found in Box and Tiao (1973, Chapter 2)
and Zellner (1971, Chapters 3 and 8). Hartigan (1983, Chapter 9) gives a detailed mathe-
matical overview of normality in the Bayesian linear model, and Tanner (1996, Chapter 1)
discusses normal approximations in a Bayesian context. The role of normal assumptions in
various Bayesian time-series models is covered by Bauwens, Lubrano, and Richard (1999),
Broemeling (1985), Pole, West, and Harrison (1994), Sims (1988), and West and Harrison
(1997). In addition, the normal speciﬁcation is at the heart of the Bayesian linear model
speciﬁcation and leads nicely to the full Bayesian generalized linear model (Dey, Ghosh,
and Mallick 2000).
In Chapter 5, we will use normal and t-distribution assumptions in the development of

The Normal and Student’s-t Models
83
the Bayesian linear regression model by treating the standard linear model, y = Xβ + ϵ,
with priors on β and ϵ. Chapter 12 develops the linear hierarchical Bayesian model in
which structured levels of terms and data are speciﬁed. A substantial amount of work in
this area is based on normal and t-distribution assumptions for coeﬃcient or error terms.
Also, Lee (2004) gives the Bayesian treatment of classical two-sample problems by Student
(Gossett actually, 1908a, 1908b) in the original derivation of the t-distribution theory.
3.8
The Student’s-t Model
In this section we will see how the t-distribution naturally arises in the case where we
specify vague prior information for the normal parameters μ and σ2 in the normal model.
The t-distribution resembles the normal except that it has heavier tails, thus expressing
greater posterior uncertainty. Furthermore, the Student’s-t is a natural choice for small
sample applications because as the sample size increases, the distribution converges to a
normal.
Assume that we have normally distributed data with both parameters unknown and the
goal is to estimate these. We start with the most basic likelihood function for assumed
normally distributed data, and rearrange it slightly:
L(μ,σ2|x) = (2πσ2)−n
2 exp

−1
2σ2
n

i=1
(xi −μ)2

= (2πσ2)−n
2 exp

−1
2σ2
n

i=1
[(xi −¯x) −(μ −¯x)]2

= (2πσ2)−n
2 exp

−1
2σ2
* n

i=1
(xi −¯x)2 −2
n

i=1
(xiμ −xi¯x −¯xμ + ¯x2) + n(¯x −μ)2
,
∝σ−n exp
'
−1
2σ2

(n −1)s2 + n(¯x −μ)2(
.
(3.25)
The purpose here is to re-express the likelihood function strictly in terms of two suﬃcient
statistics: s2 =
1
n−1
(xi −¯x)2 and ¯x = 1
n
(xi), which are distributed (σ2/(n −1))χ2
n−1
and N(μ, σ2/n), respectively, under classical results.
Sometimes it is desirable to insert relatively little subjective information into the anal-
ysis. One way to do this is to stipulate vague or diﬀuse prior distributions for the unknown
parameters. A common assignment of vague priors for the normal model is given by the
pair:
p(μ) ∝c,
−∞< μ < ∞
p(σ2) ∝σ−2,
0 < σ < ∞,
(3.26)

84
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
where c is an arbitrary constant. These are examples of “improper” prior distributions
in that they do not integrate to a ﬁnite quantity and therefore do not meet the technical
requirements for density functions. This turns out to be a minor issue and we will discuss
the characteristics of improper priors extensively in Chapter 4. Observe also that we are
assuming independent priors for the normal parameters, unlike the conjugate model in
Section 3.4.
The resulting posterior is created by π(μ, σ|x) ∝L(μ, σ|x)p(μ)p(σ) according to:
π(μ, σ2|x) ∝σ−(n+2) exp
'
−1
2σ2

(n −1)s2 + n(μ −¯x)2(
.
(3.27)
The real objective here is to get the marginal posterior distribution for both parameters.
We could obtain the marginal posterior for μ by integrating out σ:

π(μ, σ|x)dσ, except
that this is a diﬃcult integration to do by brute force. Instead consider the handy integral
formula based on the inverse gamma PDF:
 ∞
0
x−b−1 exp[−a/x2]dx = 1
2a−b
2 Γ
 b
2

,
(3.28)
which is used by Box and Tiao (1973, p.145) as well as by many calculus texts. Setting
σ2 = x2, n + 1 = b, and 1
2

(n −1)s2 + n(μ −¯x)2
= a means that integrating (3.27) with
respect to σ can be done by:
π(μ|x) =
 ∞
0
π(μ, σ2|x)dσ2
= 1
2
'1
2

(n −1)s2 + n(μ −¯x)2(−(n+1)
2
Γ
n + 1
2

=
1
2

s−(n+1)
1
2(n −1)
−(n+1)
2
Γ
n + 1
2
 
1 +
1
n −1
μ −¯x
s/√n
2−(n+1)
2
.
The structure here is much more revealing if we make the transformation t =
μ−¯x
s/√n with
the Jacobian J = d
dtμ =
s
√n:
π(t|x) =
1
2

s−(n+1)
1
2(n −1)
−(n+1)
2
Γ
n + 1
2
 '
1 +
1
n −1t2
(−(n+1)
2
' s
√n
(
∝Γ
 n+1
2

Γ
 n
2

1
(nπ)
1
2

1 +
1
n −1t2
−(n+1)
2
.
(3.29)
This is exactly the form of the Student’s-t given in Appendix B. Therefore the marginal
posterior of
μ−¯x
s/√n is Student’s-t with θ = n degrees of freedom, and the marginal posterior
of μ itself is:
π(μ|x) = Γ
 θ+1
2

Γ
 θ
2

s/√n
(θπ)
1
2 (1 + μ2/θ)(θ+1)/2 + ¯x,
(3.30)

The Normal and Student’s-t Models
85
where the role of the two suﬃcient statistics, s2 =
1
n−1
(xi −¯x)2 and ¯x = 1
n
(xi), is
clear.
The calculation of the marginal posterior for σ2 is considerably less involved because we
can once again use the conditional probability property:
π(σ|x) = π(μ, σ|x)
π(μ|x) .
(3.31)
This means that we can obtain the marginal posterior of σ by dividing the joint posterior
by the marginal distribution of μ assuming that σ is independent. This works out very
nicely, provided suﬃcient sample size:
π(σ|x) =
 n
2π
 1
2

(n−1)s2
2
 n−1
2
1
2 Γ( n−1
2 )
σ−(n+2) exp
	
−1
2σ2

(n −1)s2 + n(μ −¯x)2
Γ( θ+1
2 )
Γ( θ
2)
s/√n
(θπ)
1
2 (1+μ2/θ)(θ+1)/2 + ¯x
∝σ−((n+1)+1) exp
'
−1
2(n −1)s2/σ2
(
.
(3.32)
The last line shows that the marginal posterior of σ2 (note the change) is distributed inverse
gamma since this line is the kernel from IG( n+1
2 , n−1
2 s2) (see Appendix B).
From this speciﬁcation, we see that the resulting marginal posteriors for both μ and
σ are more diﬀuse than those we produced by using conjugate priors. This makes a lot
of sense since we used vague priors and it is logical that this decision should have some
inﬂuence on the resulting posterior. However, consistent with all Bayesian speciﬁcations, if
the data size becomes very large, then it does not matter whether a conjugate prior, giving
a high level of a priori information, or a diﬀuse improper prior, giving a very low level of a
priori information, is used.
■Example 3.2:
National IQ Scores. Standard IQ tests are designed to measure in-
telligence and reasoning with a mean of 100 and a standard deviation of 15. However,
these tests are also said to have economic and cultural biases that favor some groups
over others. An additional complication is added when IQ scores are aggregated at
the national level because within-country features are masked. This example analyzes
internationally collected IQ data (Lynn and Vanhanen 2001) for 80 countries from
published national sources. The key idea in describing the posterior distribution is
whether national eﬀects alter the intended parameterization.

86
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
Average IQ Score By Country
Argentina
96
Australia
98
Austria
102
Barbados
78
Belgium
100
Brazil
87
Bulgaria
93
Canada
97
China
100
Congo (Br.)
73
Congo (Zr.)
65
Croatia
90
Cuba
85
Czech Repub.
97
Denmark
98
Ecuador
80
Egypt
83
Eq. Guinea
59
Ethiopia
63
Fiji
84
Finland
97
France
98
Germany
102
Ghana
71
Greece
92
Guatemala
79
Guinea
66
Hong Kong
107
Hungary
99
India
81
Indonesia
89
Iran
84
Iraq
87
Ireland
93
Israel
94
Italy
102
Jamaica
72
Japan
105
Kenya
72
Korea (S.)
106
Lebanon
86
Malaysia
92
Marshall I.
84
Mexico
87
Morocco
85
Nepal
78
Netherlands
102
New Zealand
100
Nigeria
67
Norway
98
Peru
90
Philippines
86
Poland
99
Portugal
95
Puerto Rico
84
Qatar
78
Romania
94
Russia
96
Samoa
87
Sierra Leone
64
Singapore
103
Slovakia
96
Slovenia
95
South Africa
72
Spain
97
Sudan
72
Suriname
89
Sweden
101
Switzerland
101
Taiwan
104
Tanzania
72
Thailand
91
Tonga
87
Turkey
90
Uganda
73
U.K.
100
U.S.
98
Uruguay
96
Zambia
77
Zimbabwe
66
The IQ testing instrument is designed to have a mean response of 100 with a standard
deviation of 15 (the Stanford-Binet version has a standard deviation of 16). The data
provide the mean IQ score for 80 countries. Industrialized Asian countries seem to do
relatively quite well, and poorer developing countries generally fair less well.
TABLE 3.4:
Posterior Summary: National IQ Scores
Quantile:
0.01
0.10
0.25
0.50
0.75
0.90
0.99
μ
85.081
86.489
87.311
88.213
89.106
89.920
91.379
σ
10.006
10.784
11.284
11.882
12.539
13.191
14.427
From (3.29), (3.32), and using the improper priors p(μ) ∝c, p(σ2) ∝σ−2, we get
the posterior quantiles in Table 3.4. This result demonstrates that the national level
summary through the posterior no longer resembles the original distributional goal of
the test. Noticeably, the distribution of μ is centered at about 88 rather than 100, and
the median of the posterior variance implies a standard error of roughly 12 (recall that
we are analyzing unweighted means by country, which will lower variance). Part of
the results can be attributed to the aggregation eﬀects that show up in the posterior
mean. This may be due to some cultural impact, as the instrument was designed
originally for use in an English-speaking industrialized country and then subsequently
used around the world. The R code is provided below:

The Normal and Student’s-t Models
87
data(iq)
n <- length(iq)
t.iq <- (iq-mean(iq))/(sd(iq)/sqrt(n))
r.t <- (rt(100000, n-1)*(sd(iq)/sqrt(n))) + mean(iq)
quantile(r.t,c(0.01,0.10,0.25,0.5,0.75,0.90,0.99))
r.sigma.sq <- 1/rgamma(100000,shape=(n+1)/2,
rate=var(iq)*(n-1)/2)
quantile(sqrt(r.sigma.sq),
c(0.01,0.10,0.25,0.5,0.75,0.90,0.99))
Some brief caveats are warranted. There is nothing implied here about innate natural
intelligence of nationalities since test results can be a function of health, sociological,
and political factors. Furthermore, there are diﬀerences by country on who takes (or is
allowed to take) the test. Finally, since these data are unweighted means, population
size is not taken into account.
3.9
Normal Mixture Models
Mixture models allow the parametric description of distributions that cannot be de-
scribed with conventional PDFs and PMFs. This approach is a compromise between fully
nonparametric models that completely avoid imposing an underlying probability generating
form, and the standard assumption that all the observed data result from a single identi-
ﬁable process. The basic idea is to incorporate more than one simple distributional form
into a single model in a way that recognizes heterogeneity.
For example, suppose that a sample is observed in which every data point, x1, . . . , xn is
generated by the same density function but with a diﬀerent indexing parameter: θ1, . . . , θn.
Then the “true” density of the sample can be approximated by:
g(xi|θ) = 1
nf(xi|θ1) + 1
nf(xi|θ2) + . . . + 1
nf(xi|θn),
(3.33)
which for data value xi substitutes a mean value, g(xi|θ), for the exact value, f(xi|θi).
The speciﬁcation in (3.33) is a somewhat pessimistic view of the world in that it is
unlikely that every single data point is produced by a unique generating process, given that
we have collected these cases together for some reason. Instead it is far more likely that
there is some number, m ≪n, of these functions that are required. So now we can condense
(3.33) by collecting terms using a simple weighting scheme that indicates the proportion of
the xi with identical θ parameters:
g(xi|ω, θ) = ω1f(xi|θ1) + ω2f(xi|θ2) + . . . + ωmf(xi|θm)
=
m

j=1
ωjf(xi|θj),
(3.34)

88
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
where  ωj ≤1. Thus the vector ω describes variation in the θi and therefore variation
in the f(x|θ). This is very straightforward but requires the unrealistic assumption that
we have perfect knowledge of the weights. It is far more likely that we have to make this
assignment through an unobserved component indicator vector:
zij =
⎧
⎨
⎩
1,
iﬀxi ∼f(xi|θj)
0,
otherwise,
(3.35)
where the n×m matrix z, containing a single 1 in each row, is the stacking of these vectors.
Although unobserved, each row of z can be modeled with the multinomial, a multicategory
generalization of the binomial (MN(zi|m, ω), see Appendix B). Now the joint distribution
of the observed data, x, and the unobserved latent data, z is just:
f(x, z) =
n
+
i=1
m
+
j=1
[(ωjf(xi|θj)]zij .
(3.36)
This model is much more explicitly Bayesian when we consider ω to be the parameters of
the multinomial and then assign them a prior distribution.
To fully elaborate this model, consider now the xi to be normally distributed so that
f(xi|θj) in (3.36) is N(μj, σj) provided that zij = 1, where θj in (3.34) is now a vector:
θj =
	 μj
σj

. Consider the following priors to complete the model:
pj(σ2
j ) ∼IG
*
τj
2 , ρ2
j
2
,
pj(μj|σ2
j ) ∼N
*
νj, σ2
j
nj
,
p(ω) ∼D(ω|α1, . . . , αm),
(3.37)
and the τj, ρj, νj, and α1, . . . , αm are all assigned speciﬁc parameter values.
Here nj
is the number of cases within the jth category. The last assignment from (3.37) is a k-
category Dirichlet prior, the multicategory generalization of the beta PDF (B). Accordingly,
this model is now a generalization of the beta-binomial model from Chapter 2. This is a
completely conjugate speciﬁcation and the product of all of these priors and the likelihood
function leads to the following posterior distributions:
μj|σj ∼N
*
njνj + ¯zj ¯xj(z)
nj + ¯zj
,
σ2
j
σ2
j + n
,
σ2
j ∼IG
*
τj + nj
2
, 1
2

ρ2
j +
n

i=1
zij(xj −¯xj(z))2 + nj ¯zj(¯xj(z) −νj)2
nj + ¯zj
,
ω ∼D(α1 + ¯z1, α2 + ¯z2, . . . , αk + ¯zm),
(3.38)
where:
¯zj =
n

i=1
zij
¯xj(z) = 1
¯zj
n

i=1
zijxi.

The Normal and Student’s-t Models
89
If we knew the z values, this would be a very easy estimation problem. Unfortunately
this is often not the case. However, from (3.38) and the multinomial speciﬁcation for z,
we have a full set of full conditional distributions in order to use Gibbs sampling. Another
method that can be used here is to treat z as missing information and apply the EM
algorithm described in Chapter 9. Both Gelman et al. (2003) and Robert (1996) provide
extensive implementation details on both of these estimation approaches.
The original
EM algorithm article of Dempster, Laird, and Rubin (1977, Section 4.3) discusses a similar
model. Carlin and Louis (2009, p.184) show how the scale mixture of normals developed here
can be extended to models with non-normal (heavier tail) errors. Also, the classic reference
on mixture models belongs to Titterington, Smith, and Makov (1985), and contains both
Bayesian and non-Bayesian methods.
3.10
Exercises
3.1
The most important case of a two-parameter exponential family is when the second
parameter is a scale parameter. Designate ψ as such a scale parameter, then the
exponential family form expression of a PDF or PMF is rewritten:
f(y|θ) = exp
'yθ −b(θ)
a(ψ)
+ c(y, ψ)
(
.
Rewrite the normal PDF in exponential family form.
3.2
Show that the Student’s-t PDF cannot be put into the exponential family form.
3.3
Suppose the random variable X is distributed N(μ, σ2). Prove that the random
variable Y = (X −μ)/σ is N(0, 1): standard normal.
3.4
Suppose that X1, . . . , Xn and Y1, . . . , Yn are independent normally distributed sam-
ples with means μX, μY , and ﬁnite variances σ2
X, σ2
Y . Let r be the standard (Pear-
son Product Moment) correlation coeﬃcient between these two samples. Show that
√n −2r/
√
1 −r2 has a Student’s-t distribution with ν = n −2 degrees of freedom,
but (n −2)r2/(1 −r2) has an F distribution with 1 and n −2 degrees of freedom.
3.5
Missing Data. Suppose we have an iid sample of collected data: X1, X2, . . . , Xk,
Yk+1, . . . , Yn ∼N(μ, 1), where the Yi values represent data that has gone missing.
Specify a joint posterior for μ and the missing data with a N(0, 1/n) prior.
3.6
Obesity is an increasing public health and policy problem in the United States,
but rates diﬀer by the individual states. Consider the following average body mass
index (BMI = [kilograms]/[meters]2 = [4.88 × pounds]/[feet]2) data by state
from 2009:

90
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
Alabama
31.0
Alaska
24.8
Arizona
25.5
Arkansas
30.5
California
24.8
Colorado
18.6
Connecticut
20.6
Delaware
27.0
Florida
25.2
Georgia
27.2
Hawaii
22.3
Idaho
24.5
Illinois
26.5
Indiana
29.5
Iowa
27.9
Kansas
28.1
Kentucky
31.5
Louisiana
33.0
Maine
25.8
Maryland
26.2
Massachusetts
21.4
Michigan
29.6
Minnesota
24.6
Mississippi
34.4
Missouri
30.0
Montana
23.2
Nebraska
27.2
Nevada
25.8
New Hampshire
25.7
New Jersey
23.3
New Mexico
25.1
New York
24.2
North Carolina
29.3
North Dakota
27.9
Ohio
28.8
Oklahoma
31.4
Oregon
23.0
Pennsylvania
27.4
Rhode Island
24.6
South Carolina
29.4
South Dakota
29.6
Tennessee
32.3
Texas
28.7
Utah
23.5
Vermont
22.8
Virginia
25.0
Washington
26.4
Washington DC
19.7
West Virginia
31.1
Wisconsin
28.7
Wyoming
24.6
(also available as bmi.2009 in the BaM package in R). Plot these data with a his-
togram and overlay a normal PDF with the mean and variance from the data. Can
you assert that these data are normally distributed? If you need to use these data
as a prior distribution for the analysis of future BMI data, how would you construct
this prior?
3.7
Suppose X ∼N(μ, μ), μ > 0. Give an expression for the conjugate prior for μ.
3.8
Suppose that X1, . . . , Xn and Y1, . . . , Yn are independent normally distributed sam-
ples with means μX, μY , variances σ2
X = σ2
Y = 1. Calculate a 95% credible interval
for μX −μY , using normal priors on μX, μY . Compare this to the frequentist 95%
conﬁdence interval for μX, μY .
3.9
Returning to the example where the normal mean is known and the posterior dis-
tribution for the variance parameter is developed (equation (3.10), on page 74),
plot a ﬁgure that illustrates how the likelihood increases in relative importance to
the prior by performing the following steps:
(a) Plot the IG(5, 5) density over its support.
(b) Specify a posterior distribution for σ2 using the IG(5, 5) prior.
(c) Generate four random vectors of size: 10, 100, 200, and 500 distributed stan-
dard normal. Using these create four diﬀerent posterior distributions of σ2,
and add the new density curves to the existing plot.
(d) Label all distributions, axes, and legends.

The Normal and Student’s-t Models
91
Hint: modify the R code in the Computational Addendum to provide a posterior
for the variance parameter instead of the mean parameter.
3.10
An 1854 study on mental health in the fourteen counties of Massachusetts yields
data on 14 cases. This study was performed by Edward Jarvis (then president of
the American Statistical Association), and has variables for: the number of “lu-
natics” per county (NBR), distance to the nearest mental healthcare center (DIST),
population in the county by thousands (POP), population per square county mile
(PDEN), and the percent of “lunatics” cared for in the home (PHOME). Use graphical
tools to make a claim about whether any of these variables can be treated as coming
from an underlying normal data generation process. Do you suspect that a larger
dataset would alter your conclusions? Use data(lunatics) from the BaM package
in R to start this exercise.
3.11
Rejection Method. Like the normal, the Cauchy distribution is a unimodal, sym-
metric density of the location-scale family: C(X|θ, σ) =
1
πσ
1
1+( x−θ
σ )
2 , where −∞<
X, θ < ∞, 0 < σ < ∞. Unlike the normal, the Cauchy distribution has very heavy
tails, heavy enough so that the Cauchy distribution has no moments and is there-
fore less easy to work with. Given a C(0, 3) distribution, ﬁnd the probability of
observing a point between 3 and 7. To do this, simulate 100,000 values of C(0, 3)
(in R this is done by rcauchy(100000,0,3)), and count the number of points in
the desired range. Graph your results.
3.12
The bivariate Cauchy distribution (Student’s-t with degrees of freedom ν = 1) is
given by f(X1, X2) = k(1+X2
1 +X2
2) for X1, X2 ∈ℜ, with k a normalizing constant.
Is this form log-concave to the x-axis? Can this bivariate form be used as a joint
prior for the mean vector?
3.13
The expressions for the mean and variance of the inverse gamma were supplied in
(3.9). Derive these from the inverse gamma PDF. Show all steps.
3.14
Replicate the simulated results in Table 3.2 and Table 3.3 (Section 3.6, page 80).
Why are your results very slightly diﬀerent (besides rounding)?
3.15
Modify the function biv.norm.post given in the Computational Addendum so
that it provides posterior samples for multivariate normals, given speciﬁed priors
and data. Also modify the function normal.posterior.summary so that the user
can specify any level of density coverage.
3.16
Consider the replication below of Figure 6.2 from Meier and Gill (2000). The left-
hand side is a histogram of equity data and the right-hand side is a qqnorm plot of
the same data (quantiles of the data plotted against quantiles of a standard normal
distribution). Explain the deviance from normality indicated by the bracket in the
second panel. Why is this not as apparent in the histogram?

92
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
0.2
0.4
0.6
0.8
1.0
0
50
100
150
+
++
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
++
+
++
+
+
+
+
+
++
+
+
+
+
+
+
+
+
++
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
++
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
++
+
+
++
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+++
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
++
+
+
+
+
+
+
+
++
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
++
+
+
+
+
++
+
++
+
+
+
++
+
+
+
+
++
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
++
+
+
+
+
+
+
+
+
++
+
++
+
+
+
+
+
++
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
++
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
++
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
++
+
+
+
++
++
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
++
+
+
+
+
+
+
+
+
+
+
++
+
+
++
+
+
+
+
+
+
+
+
+
+
+
+
+
−3
−2
−1
0
1
2
3
0.4
0.6
0.8
1.0
3.17
Specify a normal mixture model for the following data from Brooks, Dellaportas,
and Roberts (1997), and summarize the posterior distributions of the model param-
eters with tabulated quantiles: X = {2.3, 3.7, 4.1, 10.9, 11.6, 13.8, 20.1, 21.4, 22.3}.
3.18
Using Bettina Gruen’s bayesmix package in R, run the following Bayesian mixture
model (which calls JAGS) using the Swiss suicide data in BaM.
lapply(c("bayesmix","BaM"),library, character.only=TRUE)
data(suicide)
model1 <- BMMmodel(suicide$age, k = 3, initialValues = list(S0 = 2),
priors = list(kind = "independence", parameter = "priorsFish",
hierarchical = "tau"))
j.control <- JAGScontrol(variables = c("mu", "tau", "eta", "S"),
burn.in = 1000, n.iter = 5000, seed = 10)
jags.out <- JAGSrun(suicide$age, model = model1, control = j.control)
( jags.sort <- Sort(jags.out, by = "mu") )
Interpret the model results including the ﬁt of the selected number of mixtures.
3.19
Use the following data on the race of convicted murderers in the United States in
1999 to specify a bivariate model. Specify reasonable priors and produce poste-
rior descriptions of the unknown parameters according to Section 3.5 starting on
page 76. (Source: FBI, Crime in the United States 1999,
http://www.fbi.gov/about-us/cjis/ucr/crime-in-the-u.s/1999.)

The Normal and Student’s-t Models
93
Age Group
White
Black
9 to 12
7
11
13 to 16
218
247
17 to 19
672
976
20 to 24
987
1,285
25 to 29
619
660
30 to 34
493
429
35 to 39
444
303
40 to 44
334
228
45 to 49
236
134
50 to 54
153
73
55 to 59
89
60
60 to 64
55
24
65 to 69
47
17
70 to 74
23
10
75 and up
57
14
3.20
For a bivariate normal distribution for x1 and x2, show that the conditional distri-
butions are N(μ1+ρ(σ1/σ2)(x2−μ2), (1−ρ2)σ2
1) and N(μ2+ρ(σ2/σ1)(x1−μ1), (1−
ρ2)σ2
2), where ρ is the correlation coeﬃcient. Design and code a Gibbs sampler us-
ing these conditional distributions and run it with the White and Black variables as
x1 and x2 from the data on the race of convicted murderers in exercise 19. Interpret
your results.
3.11
Computational Addendum: Normal Examples
3.11.1
Normal Example with Variance Known
This R language function produces the posterior mean, variance, and 95% credible in-
terval for user-speciﬁed prior, in the normal example with known population variance.
n.post1 <- function(data.vec,pop.var,prior.mean,prior.var) {
if(length(data.vec) <= 1)
stop("n.post1: input data must be a vector")
mu.hat <-
(prior.mean/prior.var +
length(data.vec)*mean(data.vec)/pop.var)/
(1/prior.var + length(data.vec)/pop.var)
sigma.hat <- 1/(1/prior.var + length(data.vec)/pop.var)
credible.int<-c(mu.hat-1.96*sqrt(sigma.hat),mu.hat+1.96*sqrt(sigma.hat))
list(mu.hat=mu.hat,sigma.hat=sigma.hat,
credible.interval=credible.int)
}

94
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
3.11.2
Bivariate Normal Simulation Example
This code runs the bivariate normal simulation example by ﬁrst producing a set of
bivariate normal data according to the user speciﬁcation, and then running the estimation
procedure described in Section 3.5 on page 76.
The rwishart function for generating
random Wishart matrices (Appendix B) is found in multiple R packages.
# FUNCTION FOR GENERATING POSTERIOR QUANTITIES, BVT. NORMAL
biv.norm.post <- function(data.mat,alpha,beta,m,n0=5) {
n <- nrow(data.mat)
xbar <- apply(data.mat,2,mean)
S2 <- (n-1)*var(data.mat)
Wp.inv <-
solve(beta)+S2+((n0*n)/(n0+n))*(xbar-m)%*%t(xbar-m)
Sigma<-solve(rwishart(alpha+n,solve(Wp.inv))$W)
mu <- rmultinorm(1, (n0*m + n*xbar)/(n0+n), Sigma/(n0+n))
return(c(mu1=mu[1],mu2=mu[2],sig1=Sigma[1,1],
sig2=Sigma[2,2],rho=Sigma[2,1]))
}
# A SIMPLE POSTERIOR SUMMARIZING FUNCTION
normal.posterior.summary <- function(reps)
{
reps[,5] <- reps[,5]/sqrt(reps[,3]*reps[,4])
reps <- apply(reps,2,sort)
out.mat <- cbind("mean"=apply(reps,2,mean),
"std.err"=apply(reps,2,sd),
"95% HPD Lower"=reps[25,],
"95% HPD Upper"=reps[975,])
return(out.mat)
}
# GENERATE THE CONTRIVED DATA, OBTAIN POSTERIOR SAMPLES
data.n10 <- rmultinorm(10, c(1,3),
matrix(c(1.0,0.7,0.7,3.0),2,2))
rep.mat <- NULL; reps <- 1000
for (i in 1:reps)
rep.mat <- rbind(rep.mat,
biv.norm.post(data.n10,3,
matrix(c(10,5,5,10),2,2),c(2,2)))
round(normal.posterior.summary(rep.mat),3)

The Normal and Student’s-t Models
95
3.11.3
Multivariate Normal Example, Health Data
This last set of example code runs the multivariate normal model where the mean and
variance are both unknown. The data used are the North Carolina county health data. The
rwishart function here is from the bayesm package in R by Peter Rossi and Rob McCulloch.
data(nc.sub.dat)
library(bayesm)
# FIRST SPECIFICATION
# SECOND SPECIFICATION
Alpha <- 3 + nrow(nc.sub.dat)
# Alpha <- 3 + nrow(nc.sub.dat)
Beta.inv <- solve(diag(3)*100)
# Beta.inv <- solve(diag(3)*10)
m <- c(250,16,88)
# m <- c(100,6,88)
n0 <- 0.01
# n0 <- 99
x.bar <- apply(nc.sub.dat,2,mean)
S.sq <- var(nc.sub.dat)
k <- (n0 * nrow(nc.sub.dat))/(n0 + nrow(nc.sub.dat))
p.Beta <- solve( Beta.inv + S.sq
+ k * round((x.bar-m) %*% t(x.bar-m),2) )
Sigma <- array(NA,dim=c(3,3,1))
for (i in 1:10000)
Sigma <- array(c(Sigma,
rwishart(Alpha,p.Beta)$IW),dim=c(3,3,(i+1)))
Sigma <- Sigma[,,-1]
Sigma.Mean <- apply(Sigma,c(1,2),mean)
Sigma.SD <- apply(Sigma,c(1,2),sd)
# ANALYTICAL MEAN OF THE INVERSE WISHART:
( (Alpha-ncol(nc.sub.dat)-1)^(-1) )*solve(p.Beta)
# MEAN BY SIMULATION
Mu <- rmultinorm(5000,(n0*m + nrow(nc.sub.dat)*x.bar)/
(n0 + nrow(nc.sub.dat)),Sigma.Mean/(n0+nrow(nc.sub.dat)))
apply(Mu,2,quantile, probs = c(0.01,0.25,0.50,0.75,0.99))


Chapter 4
The Bayesian Prior
4.1
A Prior Discussion of Priors
Specifying Bayesian models necessarily means providing prior distributions for unknown
parameters.
The prior plays a critical role in Bayesian inference through the updating
statement: π(θ) ∝p(θ)L(θ|X). Central to the Bayesian philosophy is that all unknown
quantities are described probabilistically, even before the data has been observed. Generally
these unknown quantities are the model parameters, θ in the general notational sense, but
missing data are handled in the same fashion. This is very much at odds with the frequentist
notion that unknown parameters are ﬁxed, unyielding quantities that can be estimated with
procedures that are either repeated many times or imagined to be repeated many times.
The immobile parameter perspective, although widespread, is contradictory to the way
that most social and behavioral scientists conduct research. It simply is not possible to
rerun elections, repeat surveys under exactly the same conditions, replay the stock market
with exactly matching economic forces, ﬁght the same war, or re-expose clinical subjects to
identical stimuli.
The core of the disagreement between frequentists and Bayesians is the diﬀering funda-
mental interpretations of probability. Frequentists believe that probabilities are long-run
tendencies of events that eventually converge on some true population proportion that can
be interpreted as a probability of such events even in the short term.
Bayesians generally
interpret probability as “degree of belief,” meaning that prior distributions are descriptions
of relative likelihoods of events based on the researcher’s past experience, personal intuition,
or expert opinion, and posterior distributions are these prior distributions updated by con-
ditioning on new observed data. Some authors also focus on the distinction between priors
that are explicitly based on previous empirical work versus priors that represent general
knowledge possessed by the researcher (Zellner 1971, p.18). Mathematically this does not
change the mechanics of prior inclusion, however.
It is important to understand that priors are not merely annoyances that must be dealt
with before moving on to more interesting parts of the speciﬁcation process. They are actu-
ally an opportunity to systematically include qualitative, narrative, and intuitive knowledge
into statistical models. Because there is a lot of historical controversy concerning the impact
of prior distributions, many applied Bayesians in the social and behavioral sciences seek to
use only highly diﬀuse forms such as the uniform distribution. Some of these researchers
97

98
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
merely want the probabilistic inferential process and the powerful machinery of MCMC
without being “fully Bayesian.” These “Bayesians of convenience” miss the point that in-
formed prior distributions are incredibly useful for integrating non-quantitative information
into the statistical model.
Speciﬁcally, see Gill and Walker (2005) in political science,
Vanpaemel (2010) in psychology, Raftery (1999) in sociology, Gill and Meier (2000) in pub-
lic administration, Wolfson, Kadane and Small (1996) in policy studies, and Martin and
Quinn (2007) in legal studies.
4.2
A Plethora of Priors
We have already seen several types of prior distributions in previous chapters. This
chapter lays out the three general categories of prior distributions used in Bayesian models:
conjugate, uninformed, and informative. The categorization is artiﬁcial (but convenient)
since the boundaries are blurred: conjugate forms are sometimes highly informed, the dis-
tinction between uninformed and informed is across a relative spectrum, and there does not
exist a prior distribution with absolutely no information (hence the use of “uninformative”
rather than “noninformative” here). This distinction was made clear in a historical context
by Diaconis and Ylvisaker (1985). In their words, the classical Bayesian views the prior
distribution as a necessary inconvenience and typically attempts to specify a “ﬂat” prior so
as to interject the least amount of prior knowledge as possible. These priors are correctly
called uninformative and are sometimes diﬃcult to ﬁnd. Note that these are frequently
called noninformative, but this is actually an inaccurate term for reasons that we will see
shortly. Modern parametric Bayesians specify priors possessing deliberate characteristics,
such as conjugacy, and assign prior parameter values according to speciﬁc criteria unrelated
to substantive knowledge. Subjective Bayesians elicit prior distributions according to pre-
existing scientiﬁc knowledge in a substantive ﬁeld. This can come from previous empirical
work in the ﬁeld or from expert opinions by non-statisticians. In practice these categories
are far from mutually exclusive, and it is more common to see a mixed approach that com-
bines aspects of previous knowledge, mathematical convenience, and a desire not to overly
aﬀect the ﬁnal conclusions with a strongly inﬂuential prior.
Prior distributions remain the most controversial aspect of Bayesian inference. Critics
have focused on the supposedly personal-subjective nature of priors, generally neglecting to
notice that all statistical models involve subjective choices. One ecumenical position taken
by this book is that it is rare to approach a problem with absolutely no prior information
about parameters of interest, and that even in these situations there will exist a class of
priors designed to supply relatively little prior information. In fact, most readers would
be reluctant to accept model speciﬁcations by authors who knew nothing before analyzing
the data.
While it is coy to say “everyone is a Bayesian, some of us know it,” most

The Bayesian Prior
99
researchers tell us about their prior knowledge even if it is not put directly in the form of a
prior distribution. Consider the following quotation from Canes-Wrone, Brady, and Cogan
(2002) in political science.
The eﬀects of campaign spending and district ideology are consistently in the
expected direction and statistically signiﬁcant. Those on challenger quality also
have the correct sign in each regime and sample and are signiﬁcant with the
exception of the marginal regime of the 1980-1996 test. In addition, the coeﬃ-
cients for the remaining variables that are not included as a main eﬀect typically
have the predicted sign and they are signiﬁcant only with the expected sign.
(Italics added.) Obviously there is a lot of prior information revealed in that passage. Not
only are they indicating the direction of eﬀects, they tell us which ones were expected to
be statistically reliable. The Bayesian prior provides a way for researchers to be more overt
about their knowledge, attitudes, and opinions on studied social phenomena.
Leamer (1983) speciﬁes a hierarchy of priors based on the level of conﬁdence in some
eﬀect of interest: truths (axioms) > facts (proven from axioms/observations) > opinions >
conventions. So modeling decisions should be based on the most supportable level of this
hierarchy. For instance, if we design an experiment in which one of two possible events occurs
and the number of trials is established beforehand such that each trial is produced identically
from the same distribution, then the truth is that a binomial distribution should be used to
model the likelihood for the event of primary interest. The weakest form of prior evidence
comes from conventions. These include not only defensible model choices, but also things
like linearity assumptions, normal speciﬁcations, and pre-set α levels.1 Leamer’s point is
that “. . . the choice of a particular sampling distribution, or a particular prior distribution,
is inherently whimsical,” so we should not pretend to ﬁnd “objective” priors, and should
not bury assumptions, but instead should seek to overtly obtain and describe the highest
current level in the hierarchy. Of course we don’t always have truths or facts and therefore
need to rely on opinions: “As I see it, the fundamental problem facing econometrics is how
adequately to control the whimsical character of inference, how sensibly to base inferences
on opinions when facts are unavailable” (Leamer 1983, p.38).
What may have seemed like a weakness to 20th century critics of Bayesian inference
is actually a core strength.
Priors are a means of systematically incorporating existing
human knowledge, quantitative or qualitative, into the statistical speciﬁcation. For excellent
discussions of the nuances see Berger (1985, p.74-82), Savage (1972, Chapters 3 and 4),
Barnett (1973, p.80-88), and the thoughtful essays contained in Wright and Ayton (1994).
This chapter covers the previously noted trilogy of prior distributions commonly applied
in Bayesian work: conjugate, uninformed, and informative. All priors are subjective in the
1The common α levels of 0.1, 0.05, and 0.01 come from Fisher’s tables and the reluctance of mid-20th
century scientists to challenge or recalculate normal tail values.
Fisher’s justiﬁcation actually rests on
no scientiﬁc principle other than the assertion that these levels represented some standard convention in
human thought: “It is usual and convenient for experimenters to take 5 per cent as a standard level of
signiﬁcance. . . ” (1934, p.15).

100
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
sense that the decision to use any prior is completely that of the researcher, subject to
mathematical constraints that may apply. It is important to remember that the choice of
priors is no more subjective than the choice of likelihood, the selection or collection of a
given sample, the algorithm for estimation, or the statistic used for data reduction. The
set of priors described herein are by no means the complete set of available forms. For a
review of additional prior speciﬁcations see Kass and Wasserman (1996).
4.3
Conjugate Prior Forms
One diﬃcult aspect of Bayesian inference is that the posterior distribution of the θ
vector might not have an analytically tractable form (the primary motivation for the simu-
lation techniques discussed in later chapters), particularly in higher dimensions. Speciﬁcally,
producing marginals from high-dimension π(θ) by repeated analytical integration may be
diﬃcult or even impossible mathematically. One way to guarantee that the posterior has an
easily calculable form is to specify a conjugate prior. As brieﬂy described in Chapter 2 in
Section 2.3 and developed in some detail for the normal model in Chapter 3 in Section 3.3,
conjugacy is a joint property of the prior and the likelihood function that provides a poste-
rior from the same distributional family as the prior. In other words, the mathematical form
of the prior distribution “passes through” the data-conditioning phase and endures in the
posterior: closure under sampling. Thus the general form for the distribution of the eﬀect
of interest is invariant to Bayesian inference. We have already seen two important cases
in Chapter 3 where a normal prior along with a normal joint likelihood function produced
a normal posterior for the mean parameter, and an inverse gamma prior with the same
normal likelihood produced an inverse gamma posterior for the variance parameter. It is
important to keep in mind that inverse gamma distributions with small parameter values
are not necessarily low information forms since they place most of the density near one,
implying very small precision (see Figure 3.1). In variance terms these forms then specify
a great amount of the density at large values that may be unrealistic in modeling terms.
(Gelman 2006, Hodges and Sargent 2001, Natarajan and Kass 2000).
4.3.1
Example: Conjugacy in Exponential Speciﬁcations
A very simple way to model the time that something endures (wars, lifetimes, regimes,
bull markets, marriages, etc.) is to use the exponential PDF, which is a special case of the
gamma distribution where the shape parameter is ﬁxed at one (Appendix B). This has the
form:
E(X|θ) = θ exp[−θX],
0 ≤X, 0 < θ
(4.1)
(note the use of the “rate” form of the gamma distribution).
An attractive candidate
prior for θ in the exponential PDF is the gamma distribution because not only does a

The Bayesian Prior
101
random variable distributed gamma have the same support as that of an exponential,2 but
also because the gamma is an extremely ﬂexible parametric form. The gamma PDF from
Appendix B is:
f(θ|α, β) =
1
Γ(α)βαθα−1 exp[−βθ],
θ, α, β > 0.
Suppose we now observe x1, x2, . . . , xn ∼iid E(X|θ) and produce the likelihood function:
L(θ|x) =
n
+
i=1
θe−θxi = θn exp

−θ
n

i=1
xi

.
Note that n
i=1 xi is a suﬃcient statistic for θ. The posterior distribution is produced as
follows:
π(θ|x) ∝L(θ|x)p(θ)
= θn exp

−θ
n

i=1
xi

1
Γ(α)βαθα−1 exp[−βθ]
∝θ(α+n)−1 exp

−θ
* n

i=1
xi + β
,
.
(4.2)
It is easy to see that this is the kernel of a G(α + n,  xi + β) PDF, and therefore the
gamma distribution is shown to be conjugate to the exponential likelihood function.
4.3.2
The Exponential Family Form
Recall that a family of PDFs or PMFs qualify as an exponential family form if they can be
rearranged in a speciﬁc manner that recharacterizes familiar functions into a formula that is
useful theoretically and demonstrates similarity between seemingly disparate mathematical
forms. Suppose we temporarily consider only a one-parameter PDF or PMF, f(x|θ), with
one datum. This function is classiﬁed as an exponential family only if it can be rearranged
to be of the form: f(x|θ) = exp
	
t(x)u(θ)

r(x)s(θ), where r and t are real-valued functions
of x that do not depend on θ, and s and u are real-valued functions of θ that do not depend
on x, and r(x) > 0, s(θ) > 0 ∀x, θ. Also, many forms, such as the normal, include a scale
parameter making the form of the exponential family: f(x|θ) = exp
	
t(x)u(θ)/φ

r(x)s(θ),
where φ (sometimes denoted a(φ)) is a scale parameter (σ2 for the normal), and sometimes
explicitly weighted as in φ/ωi. Because of the properties of logs and exponentiation, this
form can always be rewritten as:
f(x|θ) = exp
	
t(x)u(θ)
 
!"
#
interaction
component
+ log(r(x)) + log(s(θ))
 
!"
#
additive component

,
(4.3)
2Actually the exponential PDF is a special case of the gamma PDF where the ﬁrst (shape) parameter
is ﬁxed at one. See Exercise 4.1.

102
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
(Gill 2000). The labeled interaction component in (4.3), t(x)u(θ), reﬂects the product-
indistinguishable relationship between x and θ. It should be noted that this component
must specify t(x)u(θ) in a strictly multiplicative manner.3 The structure of (4.3) is preserved
under sampling so the joint density function of iid random variables X = {X1, X2, . . . , Xn}
is simply:
f(x|θ) = exp

u(θ)
n

i=1
t(xi) +
n

i=1
log(r(xi)) + n log(s(θ))

.
(4.4)
Fisher (1934) originated this idea to show that many common PDFs and PMFs are
actually all special cases of the more general classiﬁcation labeled exponential family because
subfunctions are contained within the exponent component. This contrasts sharply with
the econometric approach of seeing these distributions as leading to separate modeling
constructs. Fisher also showed that these isolated subfunctions quite naturally produce a
small number of suﬃcient statistics that compactly summarize even large data sets without
any loss of information. Barndorﬀ-Nielsen (1978, p.114) demonstrated that exponential
family probability functions possess all of their moments (deﬁned in Appendix A), and are
therefore easier to characterize in a Bayesian framework. Morris (1982, 1983a), building
upon the results of Diaconis and Ylvisaker (1979) showed that the most commonly used
exponential families of probability functions when speciﬁed as likelihood functions possess
conjugate priors.
Consonni and Veronese (1992) proved that this is true for any mean
parameter in an exponential family form with quadratic variance (the mean parameter is
contained in an algebraic term with an exponent equal to 2), and Guti´errez-Pe˜na and Smith
(1995) extend this result with transformations of the mean prior.
We can develop a general form of the posterior that corresponds to the exponential family
expression of the joint likelihood function in (4.4). Start with a corresponding conjugate
prior form in the same generalized notation:
p(θ|k, γ) = c(k, γ) exp[ku(θ)γ + k log(s(θ))].
(4.5)
Now calculate the posterior from (4.5) and (4.4):
π(θ|x, k, γ) ∝f(x|θ)p(θ|k, γ)
∝exp

u(θ)
* n

i=1
t(xi) + kγ
,
+ (n + k) log(s(θ))

= exp
'
u(θ)(n + k)
n
i=1 t(xi) + kγ
n + k

+ (n + k) log(s(θ))
(
.
(4.6)
So the posterior distribution is an exponential family form like the prior with parameters:
k′ = (n + k), γ′ =
n
i=1 t(xi)+kγ
n+k
. The connection between the exponential family form and
3For instance, consider the Weibull PDF (useful for modeling general failure times):
f(x|γ, β) =
γ
β xγ−1 exp(−xγ/β) for x ≥0, γ, β > 0.
The term −1
β xγ in the exponent disqualiﬁes this PDF from
the exponential family classiﬁcation since it cannot be expressed in the additive or multiplicative form of
(4.3). However, if γ is known (or we are willing to assign an estimate), then the Weibull PDF reduces to
an exponential family form.

The Bayesian Prior
103
conjugacy is a very useful result because if we know that the form of our likelihood function
is an exponential family form, then it is almost certain to have a conjugate prior. Table 4.1
provides a list of common PDFs and PMFs with their associated conjugate prior.
TABLE 4.1:
Some Exponential Family Forms and Their
Conjugate Priors
Likelihood Form
Conjugate Prior Distribution
Hyperparameters
Bernoulli
Beta
α > 0, β > 0
Binomial
Beta
α > 0, β > 0
Multinomial
Dirichlet
θj > 0, Σθj = θ0
Negative Binomial
Beta
α > 0, β > 0
Poisson
Gamma
α > 0, β > 0
Exponential
Gamma
α > 0, β > 0
Gamma (incl. χ2)
Gamma
α > 0, β > 0
Normal for μ
Normal
μ ∈R, σ2 > 0
Normal for σ2
Inverse Gamma
α > 0, β > 0
Pareto for α
Gamma
α > 0, β > 0
Pareto for β
Pareto
α > 0, β > 0
Uniform
Pareto
α > 0, β > 0
Two important classes of probability density functions are not members of the exponen-
tial family. The Student’s-t and the uniform distribution cannot be put into the form of
(4.3) above. In general, a probability function in which the parameterization is dependent
on the bounds, such as the uniform distribution, are not members of the exponential family.
Even if a probability function is not an exponential family member, it can sometimes qualify
under particular circumstances.
In fact, the exponential family form provides more practical Bayesian help than it ap-
pears at ﬁrst. Since the exponential family form guarantees a ﬁnite-dimension suﬃcient
statistic, there are no diﬃculties in ﬁnding maximum likelihood estimates for unknown
parameters. This is formalized in the Darmois-Pitman-Koopman Theorem (independently
and almost concurrently proven by Darmois [1935], Pitman [1936], and Koopman [1936]),
which states that a distribution function possesses an associated ﬁxed dimensional suﬃcient
statistic for its parameter (Pitman) or parameter vector (Koopman) if and only if the dis-
tribution can be expressed in exponential family form (Anderson 1970, p.1248; Barankin
and Maitra 1963, p.217; Hipp 1974, p.1283; Jeﬀreys 1961, p.168). This well-known prop-
erty just assures us that with the exponential family form the right thing happens and
that with other forms we may or may not encounter additional diﬃculties; see Bickel and
Doksum (1977) or DeGroot (1986) for basic discussions. Furthermore, Crain and Morgan

104
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
(1975) show that Bayesian models based on exponential family form produce asymptotically
normal posteriors.
The theoretical linkage between suﬃcient statistics and the likelihood function is impor-
tant for our purposes. The minimum dimension suﬃcient statistic is actually the smallest
collection of statistics that completely summarizes the likelihood function (Brown 1964,
p.1458; Fraser 1963, p.117).
Therefore under nonrestrictive regularity conditions, the
likelihood function from an exponential family distribution can be characterized by a ﬁ-
nite, usually small dimension, suﬃcient statistic regardless of the sample size underlying
it. Therefore, we gain an immense amount of inferential leverage by restricting ourselves
to exponential family forms and summarizing the sample information through likelihood
functions. In subsequent sections, we will take advantage of small-dimensional suﬃcient
statistics to simplify the analyses.
4.3.3
Limitations of Conjugacy
The primary advantage in specifying conjugate priors is their mathematical convenience
in producing posterior inferences. This is not, however, identical to uncovering some abso-
lute truth in the data-generation process. A conjugate prior, no matter how mathematically
convenient or easily interpretable, should not be construed as the right answer in a data-
analytic exercise and in fact may seriously misrepresent the actual truth (Barnett 1973,
p.188). It is also not the case that conjugate priors are no-information default alternatives.
One should be particularly cautious about using conjugate priors as ignorance priors, such
as normal distributions centered at zero with small precision, in that they still indicate
speciﬁc parametric prior knowledge. If the form of the conjugate prior through its alterna-
tive parametric values (some are more ﬂexible than others) ﬁts prior knowledge about the
distribution of the parameter, then this is a fortunate and desirable outcome rather than
an inevitability.
There was a time when conjugate speciﬁcations were very important in Bayesian statis-
tics. Before the advent of MCMC techniques (Chapter 10), many proposed models were
simply too hard to estimate without some trick like conjugacy. This is no longer the case,
but conjugate priors can still be useful in practice and they are an excellent expository tool.
4.4
Uninformative Prior Distributions
An uninformative prior is one in which little new explanatory power about the unknown
parameter is provided by intention. Despite the unfortunate name, uninformative priors
are very useful from the perspective of traditional Bayesianism that sought to mitigate
frequentist criticisms of intentional subjectivity. Consider a situation in which absolutely

The Bayesian Prior
105
no previous subjective information is known about the phenomenon of interest. Does this
preclude a Bayesian analysis?
If we could make a probabilistic statement that did not
favor any outcome over another, then it would be a simple matter to use this as a prior
distribution allowing us to continue the analysis. Unfortunately this is often more diﬃcult
than one would expect.
4.4.1
Uniform Priors
An obvious candidate for the uninformative prior is the uniform distribution. Uniform
priors are particularly easy to specify in the case of a parameter with bounded support.
For instance, a uniform prior for the probability parameter in a Bernoulli, binomial, or
negative binomial model can be speciﬁed by: p(θ) = 1, 0 ≤θ ≤1, or if there is some
reason to specify a non-normalized uniform: p(θ) = 1, 0 ≤θ ≤k. The second is non-
normalized because it does not integrate to one, but as we demonstrated in Chapter 2, this
provides no problem whatsoever in Bayesian analysis. Both of these forms are referred to
as proper since they integrate to a ﬁnite quantity. Proper uniform priors can be speciﬁed
for parameters deﬁned over unbounded space if we are willing to impose prior restrictions.
Thus if it is reasonable to restrict the range of values for a variance parameter in a normal
model, instead of specifying it over [0:∞], we restrict it to [0:ν] and can now articulate it
as p(σ) = 1/ν, 0 ≤θ ≤ν.
It is also possible to specify improper uniform priors that do not possess bounded in-
tegrals and surprisingly, these result in fully proper posteriors under most circumstances
(although this is far from guaranteed, see Section 4.4.4). Consider the common case of an
uninformative uniform prior for the mean of a normal distribution. It would necessarily
have uniform mass over the interval: p(θ) = c, [−∞≤θ ≤∞]. Therefore to give any
nonzero probability to values on this support, p(θ) = ϵ > 0, would lead to a prior with
inﬁnite density:
 ∞
−∞p(θ)dθ = ∞.
The uniform prior is not invariant under transformation: simple transformations of
the uniform prior produce a re-expression that is not uniform and loses whatever sense of
uninformedness that the equiprobability characteristic of the uniform gives. For example,
suppose again that we are interested in developing an uninformative prior for a normal
model variance term and specify the improper uniform prior: p(σ) = c, 0 ≤σ < ∞. A
simple transformation that provides a parameter space over the entire real line is given
by: τ = log(σ), and the new PDF is given by applying the transformation with Jacobian
(J = | d
dτ g−1(τ)|), to account for the rate of change diﬀerence, to the original PDF:
τ = g(σ) = log(σ) −→g−1(τ) = σ = eτ
p(τ) = p(g−1(τ))

d
dτ g−1(τ)
 = (c)

d
dτ eτ
 ∝eτ.
This resulting prior clearly violates even the vaguest sense of uninformedness or “ﬂatness,”
and makes a strong statement about values that are a priori more likely than others. Lest

106
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
one think that this problem is restricted to the class of improper uniform priors, consider
a proper uniform prior on a Bernoulli probability parameter: f(p) = 1, 0 ≤p ≤1. If
we change from the probability metric to the odds ratio metric (fairly common), then we
impose the transformation: q =
p
1−p and the new distribution is given by:
q = g(p) =
p
1 −p −→g−1(q) =
q
1 + q
f(q) = f(g−1(q))

d
dq g−1(q)
 = (1)

d
dq
q
1 + q
 = (1 + q)−2.
Once again, a straightforward change imposes a serious departure from the uniform charac-
teristic: no prior information about p does not imply no prior information about a simple
transformation of p.
0
2
4
6
8
0.0
0.2
0.4
0.6
0.8
1.0
Support
Density
p(q) = (1 + q)−2
FIGURE 4.1: Prior from Transformation
This is shown in the adjacent ﬁg-
ure (with the right-hand-tail trun-
cated for convenience). Notice that
it is asymmetrical and places more
prior density on the region [0 : 1]
than on the region [1 : 2], which
may violate some people’s notion of
symmetry around the odds ratio of
one (equally probable success and
failure). Nevertheless, it is interest-
ing to note that it has a symmetric
property around one by area:
 1
0
(1+q)−2dq =
 ∞
1
(1+q)−2dq = 1
2.
(4.7)
In the early 20th century, there was a great amount of controversy focused around
the use of uniform priors and their role in calculating “inverse probability” (i.e., the early
application of Bayes’ Law).
Fisher (1930, p.531) was characteristically negative on the
subject: “. . . how are we to avoid the staggering falsity of saying that however extensive our
knowledge of the values of x may be, yet we know nothing and can know nothing about the
values of θ?” Other predecessors and contemporaries had notable criticisms of the default
uniform prior speciﬁed in Bayes (1763) and Laplace (1774). Boole (1854, p.370) objected to
its “arbitrary nature,” and Venn (1866, p.182) called it “completely arbitrary.” de Morgan
(1847, p.188) was uncomfortable with the implication that events that were observed to
occur have the same probability as events that were observed not to occur. Edgeworth
(1921, pp.82-83 footnote) in comments directed at Pearson pointed out that there are many
continuous prior speciﬁcations that can cancel out in the calculation of inverse probabilities
so ﬁxating on the uniform is unnecessary, and in 1884 he may have been the ﬁrst scholar to
point out the invariance property of the uniform distribution discussed above.

The Bayesian Prior
107
What is really clear from reading these early authors is that they rarely decoupled the
use of uniform priors from Bayesian inference in general. Therefore, for a long period of
time, producing posterior probabilities with Bayes’ Law was synonymous with ﬂat prior
speciﬁcations and the associated philosophical and mathematical problems that ensued.
Because of this inﬂexibility the uniform prior has long been a primary means of critiquing
the Bayes-Laplace construct, and too little thought was given to broadening the scope of
priors. Certainly no thought was given to ﬁnding other “no information” priors.
Others have pointed out that the uniform priors have an inherent bias against the
endpoints of the speciﬁed interval (Novick and Hall 1965, Villegas 1977), and therefore
do not necessarily provide the coverage that the researcher desires (particularly if these
endpoints are of theoretical importance as might be the case for the U(0, 1) speciﬁcation).
Nonetheless, uniform priors remain very popular in applied work and there are certainly
situations where the uniformness is a desired property for subjective reasons rather than as
an uninformative choice.
Bauwens, Lubrano, and Richard (1999) give some justiﬁcations for using ignorance priors
like the uniform: (1) as the sample size increases any eﬀect from the uniform diminishes,
(2) they are a suitable and convenient choice for so-called nuisance parameters that are
going to be integrated out of the posterior anyway, and (3) the uniform distribution is a
limit of some conjugate prior distributions (for example, a bounded normal with increasing
scale parameter).
4.4.2
Jeﬀreys Prior
Jeﬀreys (1961, p.181) addresses the problems associated with uniform priors by suggest-
ing a prior that is invariant under transformation:
...if we took the prior probability density for the parameters to be proportional
to ||gik||, it could be stated for any law that is diﬀerentiable with respect to all
parameters in it, and would have the property that the total probability in any
region of the αi would be equal to the total probability in the corresponding
region of α′
i; in other words, it satisﬁes the rule that equivalent propositions
have the same probability.
He means that the transformation from α to α′ is invariant with respect to probabilities:4
pJ(α) = pJ(α′)

dα′
dα
 .
(4.8)
The Jeﬀreys prior for a single parameter, θ, is produced by the square root of negative ex-
pected value of the second derivative of f(x|θ) (and more generally the likelihood function):
p(θ) ∝
'
−EX|θ
 d2
dθ2 log f(x|θ)
( 1
2
.
(4.9)
4See Dawid (1983) and Hartigan (1964) for broader deﬁnitions of invariance.

108
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
Note that the expectation here is taken over f(X|θ). This is also the square root of the
determinant of the familiar negative expected Fisher Information matrix. Expression 4.9 is
given in the single-dimension case, for a parameter vector, θ, the Jeﬀreys prior is:
p(θ) ∝

−EX|θ
*' ∂
∂θ log f(x|θ)
(′ ' ∂
∂θ log f(x|θ)
(,1/2
.
(4.10)
The Jeﬀreys prior is straightforward to calculate and use for many parametric forms.
Ibrahim and Laud (1991) give a general theoretical justiﬁcation for using the Jeﬀreys prior
with exponential family forms and therefore generalized linear models by showing that
proper posteriors are produced. Hartigan (1983, p.74) tabulates a number of exponential
family distributions with their associated Jeﬀreys prior. Guti´errez-Pe˜na and Smith (1995)
relate the Jeﬀreys prior to standard conjugate priors, Poirer (1994) gives the Jeﬀreys prior
for the logit model, and Kass (1989) carefully describes the full properties of the Jeﬀreys
prior, including a geometric interpretation.
4.4.2.1
Bernoulli Trials and Jeﬀreys Prior
Consider a repeated Bernoulli trial with x successes out of n attempts in which we are
interested in obtaining a posterior distribution for the unknown probability of success p.
The binomial PMF, for the sum of the trials, is given by:
BN(x|n, p) =
n
x

px(1 −p)n−x,
x = 0, 1, . . ., n,
0 ≤p ≤1,
and the log likelihood is given by:
ℓ(p|n, x) = log
n
x

+ x log(p) + (n −x) log(1 −p).
The ﬁrst and second derivatives are given by:
d
dpℓ(p|n, x) = x
p + n −x
1 −p (−1)
= xp−1 −(n −x)(1 −p)−1,
d2
dp2 ℓ(p|n, x) = −xp−2 −(n −x)(1 −p)−2(−1)(−1)
= −x
p2 −
n −x
(1 −p)2 .
Since E[x] = np, the last stage is trivial:
J =

E[−d2
dp2 ℓ(p|n, x)]
 1
2
=
np
p2 + n −np
(1 −p)2
 1
2
=

n
p(1 −p)
 1
2
,
which suggests the prior p(p) = p−1
2 (1 −p)−1
2 , a BE
 1
2, 1
2

distribution. This is shown
in Figure 4.2. Notice that this form is far from uniform in shape. The Jeﬀreys prior is
frequently used, meaning that researchers either highly value the invariance property or they
want a form, in this case, that is bimodal with most of the density at the extremes. Clearly
this is not an “uninformed” choice and should not be treated as a default prior distribution

The Bayesian Prior
109
0.0
0.2
0.4
0.6
0.8
1.0
1
2
3
4
5
6
7
8
Support
Density
BE(1/2,1/2)
FIGURE 4.2: Binomial: Jeffreys Prior
for general purposes.
This high-
lights an important point.
So-
called “default” priors are common
forms, perhaps even conjugate, that
some researchers gravitate to out
of convenience.
This is a danger-
ous practice and all prior speciﬁca-
tions should be carefully considered
in the context of the data and the
model.
Since the advent of mod-
ern Bayesian computing (MCMC),
analytical limitations on prior spec-
iﬁcations are rare, meaning that a
wealth of alternatives exists for any
given problem.
4.4.2.2
Other Forms of Jeﬀreys Priors
In the binomial case the Jeﬀreys prior is quite easy to calculate, but this is not always
true in other cases. Another simple form is the Jeﬀreys prior for a Poisson log-likelihood
function. Starting with:
ℓ(λ|x) = −nλ + (

xi) log λ −log

(xi!),
taking derivatives produces:
d
dλℓ(λ|x) = −n + λ−1 
xi
d2
dλ2 ℓ(λ|x) = −λ−2 
xi,
followed by the negative expectation over x:
Ex|λ

−d2
dλ2 ℓ(λ|x)

= Ex|λ

λ−2 
xi)

= (nλ)λ−2 ∝λ−1.
Finally, the square root produces pJ(λ) = λ−1
2 .
Invariance to transformation is not the only useful characteristic of the Jeﬀreys prior.
Jeﬀreys’ Rule (Box and Tiao 1973, p.42) states that any prior that is proportional to the
Jeﬀreys prior is uninformative in the sense that it interjects as little subjective informa-
tion into the posterior as possible in terms of distributional invariance. Furthermore, the
Jeﬀreys prior in the single parameter case, such as the binomial above, can be thought of
as specifying a uniform distribution to the parameterization of the random variable where
the Fisher Information is constant (Perks 1947). Hartigan (1964) extends Jeﬀreys’ idea
to relative parameter invariance priors and asymptotically locally invariant priors based

110
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
on inverses or ratios of log-forms, but these can get quite complicated without substantial
added beneﬁt.
These forms and a more general deﬁnition of invariance are covered by
Dawid (1983). In addition, there is a strong debate on priors for time-series models with
unit roots, with Phillips (1991) advocating a Jeﬀreys prior and others (Sims 1988, Sims and
Uhlig 1991) believing that uniform distributions provide better comparisons to classical unit
root distribution theory.
The most useful aspect of the Jeﬀreys prior is that it comes from a very mechanical pro-
cess, yet almost always produces an uninformative form, and with the invariance property.
Thus it can serve as a starting point for a more thoughtful determination of an uninformed
prior. It is therefore handy to have basic forms tabulated for reference, see Table 4.2.
TABLE 4.2:
Some Jeffreys Priors for Common Forms
Likelihood Form
Parameter
Jeﬀreys Proportional
Bernoulli
p ∈[0:1]
(p(1 −p))−1
2
Binomial
p ∈[0:1]
(p(1 −p))−1
2
Multinomial(k)
pi ∈[0:1],  pi = 1
(p1p2 · · · pk)−1
2
Negative Binomial
p ∈[0:1]
(1 −p)−1/2p−1
Poisson
λ ≥0
λ−1/2
Normal for σ2 = 1
−∞< μ < ∞
1
Normal for μ = 0
σ2 > 0
σ−1
Normal μ, σ2 independent
σ2 > 0
σ−1
Normal μ, σ2 nonindependent
−∞< μ < ∞, σ2 > 0
σ−2
4.4.2.3
Jeﬀreys Prior in the Multiparameter Case
While the Jeﬀreys prior is straightforward in one dimension, unfortunately it can be
quite diﬃcult in multiparameter models, except the normal, and the researcher may be
required to seek approximate solutions. In fact it is possible to construct multidimensional
priors that violate the likelihood principle (Birnbaum 1962, Evans, Fraser, and Monette
1986), and can lead to poor performing estimators even with the normal model (Dawid,
Stone, and Zidek 1973). In some cases, multivariate Jeﬀreys priors can be constructed from
independent univariate forms if we are willing to live up to a strict prior independence
assumption.
There are situations where Jeﬀreys priors can be constructed quite easily even in high
dimension. A common application is the multivariate generalized linear model (A). In this
case common regression models can be easily speciﬁed with Jeﬀreys priors, producing a
closed-form posterior expression that can often be integrated to obtain marginals. Follow-
ing the setup of Ibrahim and Laud (1991), establish y as n × 1 outcome vector, X as an

The Bayesian Prior
111
n×k matrix of explanatory variables including a constant, and β as the k coeﬃcient whose
posterior we wish to describe. The likelihood of interest is L(β|X, y, φ) = n
i=1 f(yi|θi, φ).
Using standard exponential family language, b(θ) from the canonical form, scale param-
eter φ, and θi = g−1(X′
iβ), the Fisher Information matrix for this GLM is given by the
determinant operation:
Iβ ∝|X′Ωv(β)δ(β)X|/φ,
(4.11)
with:
term
structure
elements (i = 1 : n)
Ω
n × n (optional) diagonal weight matrix
ωi
v(β)
n × n diagonal matrix
d2b(θi)/dθ2
i
δ(β)
n × n diagonal matrix
dθi/dX′
iβ
Notice that δ(β) expresses the link function here and would be just an identity matrix for
the standard linear model. The general form of the Jeﬀrey’s prior for this GLM is:
pJ(β) ∝E[Iβ]
1
2 = |X′Ωv(β)δ(β)X|
1
2 ,
(4.12)
producing the joint posterior distribution of the β:
π(β|X, y, φ) = exp
 n

i=1
ωi(yiθi −b(θi)/φ

|X′Ωv(β)δ(β)X|
1
2 ,
(4.13)
which varies in complexity depending on the particular link function. In the standard linear
model with no weighting, the Jeﬀreys prior above simpliﬁes down to pJ(β) ∝|X′X|
1
2 = c,
for some constant c, over the support [−∞:∞].
4.4.3
Reference Priors
A reference prior is a, not necessarily ﬂat, prior distribution such that for the given
problem (only), the likelihood is data translated (imposed on the posterior). The distinction
between a reference prior and an uninformative prior is murky and author-dependent. Also,
a number of authors refer to reference priors as “automatic priors” (Jeﬀreys 1961, Zellner
and Siow 1980).
Box and Tiao (1973, p.23) deﬁne a reference prior as “a prior which
it is convenient to use as a standard” and is “dominated by the likelihood.”
Thus it
need not be uninformative although some authors see it that way (e.g., Bernardo 1979).
Informally Bernardo states that “Reference analysis may be described as a method to
derive model-based, nonsubjective posteriors, based on information-theoretical ideas, and
intended to describe the inferential content of the data for scientiﬁc communication” (quoted
in Irony and Singpurwalla [1996]).
Reference priors can be enormously helpful in dealing
with nuisance parameters (Robert 2001, p.136), and have been shown to possess desirable
asymptotic qualities under certain circumstances (Ghosh and Mukerjee 1992). Kass and

112
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
Wasserman (1995) identify two distinct interpretations of reference priors: as an expression
of ignorance or as a socially agreed upon standard (model speciﬁc) alternative to subjective
priors, and they proceed to identify a litany of associated problems with the use of reference
priors.
The original idea for the modern deﬁnition of a reference prior is from Bernardo (1979),
who introduces the useful notion that a diﬃcult parameter vector (perhaps diﬃcult in the
sense that the Jeﬀreys prior does not work well), can be segmented into two components,
parameters of high interest and nuisance parameters. This often leads to more complex
derivations of the prior even before the data are considered (Berger and Bernardo 1989),
but does have the advantage that higher order problems can be dealt with systematically
(Berger and Bernardo 1992).
So a reference prior for the parameters of high interest is found by minimizing the
distance between the chosen likelihood and the resulting posterior according to some criteria
like the Kullback-Leibler distance (see Section 7.6 for details). This is also called a dominant
likelihood prior, since it is a prior that is dominated by the likelihood function over the region
of interest. One example is a very diﬀuse normal distribution centered somewhere near the
expected mode of the posterior. The Zellner-Siow (1980) prior is similar but more dispersed:
a Cauchy distribution restricted to the range of interest. These priors can be nearly uniform
through the region of interest, but have the advantage of being mathematically easy to deal
with.
Another reference prior of note is the previously mentioned locally uniform prior (see
Section 4.4.1). This is just a normalized or non-normalized uniform prior over a bounded
region of the support of the unknown parameter. While these can be tremendously helpful
as a reference (hence the name), they suﬀer from the invariance problem discussed previ-
ously, the frequent diﬃculty in determining the bounds required to make the prior proper,
the unrealistic notion that nothing is known in advance, and the classical arguments against
the imposition of equiprobability through uniform priors. Where such a prior can be useful
as a reference benchmark is in comparison with some subjective prior as a means of demon-
strating the posterior eﬀect of the prior deviance from uniformness. However, not a lot of
theoretical importance should be attached to this diﬀerence.
There is a philosophical problem with such reference priors in the social sciences. To
what degree is the model actually Bayesian if the goal is to reduce the inﬂuence of the prior
to zero or near zero? Such a model still retains the Bayesian interpretation of a posterior,
but it removes the ability to include a wealth of information that is usually available in
the social sciences. In fact, suppose that one simply ran a maximum likelihood estimation
on some non-Bayesian speciﬁcation and declared it equivalent to a Bayesian model with
the “best possible” reference prior. This would allow our hypothetical researcher to discuss
results probabilistically (HPD regions, quantiles, etc.) without the trouble of being speciﬁc
about prior speciﬁcation. For most researchers in this area, the idea runs counter to the
core tenets of Bayesian inferences and would create an artiﬁcial class of “lazy Bayesians,”
which is surely not in the interest of scientiﬁc progress in any ﬁeld.

The Bayesian Prior
113
4.4.4
Improper Priors
As discussed previously, it is possible to specify so-called improper priors. These are
prior distributions that do not add (PMF case) or integrate (PDF case) to a ﬁnite value.
These improper forms typically arise in the search for an uninformative prior speciﬁca-
tion. An interesting and common case is the normal distribution with the improper prior:
p(μ) = c, −∞< μ < ∞. Obviously the equality in this speciﬁcation can be replaced with
proportionality as the integral is inﬁnity in any case. We can now produce the posterior
distribution for μ under the assumption that σ2 is known (assumed simply for the clarity of
exposition). We will also not use the standard Bayesian convenience of reducing notational
complexity by using proportionality until the last step. This is done in order to make the
point about the form of the posterior more emphatic. The posterior for μ is produced by:
p(μ|x, σ0) =
p(μ)L(μ|x, σ0)

p(μ)L(μ|x, σ0)dμ
=
c(2πσ2
0)−n/2 exp

−
1
2σ2
0
n
i=1(xi −μ)2

c(2πσ2
0)−n/2 exp

−
1
2σ2
0
n
i=1(xi −μ)2

dμ
=
c(2πσ2
0)−n/2 exp

−1
2σ2
0 (n
i=1 x2
i −n¯x2) −
1
2σ2
0 (nμ2 −2n¯xμ + n¯x2)


c(2πσ2
0)−n/2 exp

−1
2σ2
0 (n
i=1 x2
i −n¯x2) −
1
2σ2
0 (nμ2 −2n¯xμ + n¯x2)

dμ
=
c(2πσ2
0)−n/2 exp

−
1
2σ2
0 (n
i=1 x2
i −n¯x2)

exp

−n
2σ2
0 (μ −¯x)2
c(2πσ2
0)−n/2 exp

−
1
2σ2
0 (n
i=1 x2
i −n¯x2)
 
exp

−n
2σ2
0 (μ −¯x)2

dμ
=
exp

−n
2σ2
0 (μ −¯x)2

exp

−n
2σ2
0 (μ −¯x)2

dμ
∝exp
'
−n
2σ2
0
(μ −¯x)2
(
.
(4.14)
So it is clear that the posterior for μ is a non-normalized, but proper N(¯x, σ2
0/n) distribution.
It may be surprising that an improper prior distribution can still lead to a proper posterior
distribution, and conversely that data-dependent proper priors can become improper priors
in the limit (Akaike 1980). Also, improper priors do not necessarily imply a lack of attention
to prior speciﬁcations and some authors prefer these forms as diﬀuse alternatives without
heavy parametric decisions (Taralsden and Lindqvist 2010).
This example suggests a compromise between the artiﬁciality of conjugate priors and
the frequent diﬃculties of uninformed priors. Recall that the posterior distribution of the

114
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
mean parameter in the conjugate normal model with the variance known is:
μ ∼N
m
s2 + n¯x
σ2
0
 )  1
s2 + n
σ2
0

,
 1
s2 + n
σ2
0
−1
.
(4.15)
If an improper prior for μ is speciﬁed as N(m = 0, s2 = ∞), then the resulting posterior
is clearly proper N(¯x, σ2
0/n).
The compromise suggested is to make the prior variance
(s2) very large so that the prior is normal and proper but very spread out. This is often
considered a “conservative” choice of prior since the relative probability structure is quite
ﬂat. These diﬀuse priors or vague priors are quite popular in hierarchical models where
there are many regression-style parameters of only moderate interest.
The best rationale for improper priors, provided that there is appreciable substantive
motivation for assigning them, is that if the model is set up so that the likelihood dominates
the prior to such an extent that the posterior is still proper, then their use is not harmful.
However, the only way to get an improper posterior is to specify an improper prior, and
this decision rests entirely with the researcher.
4.5
Informative Prior Distributions
This section introduces several forms of informed (or informative) priors. In this chapter
we looked closely at conjugate prior speciﬁcations, which are generally informed (although
specifying ∞for various parameters can dilute such qualities). Informative priors are those
that deliberately insert information that researchers have at hand. On one level this seems
like a reasonable and reasoned approach since previous scientiﬁc knowledge should play
a role in statistical inference. The key concern that some readers, reviewers, and editors
harbor is that the author is deliberately manipulating prior information to obtain a desired
posterior result. Therefore there are two important requirements to any written research
using informative priors: overt declaration of prior speciﬁcations, and detailed sensitivity
analysis to show the eﬀect of these priors relative to uninformed types. The latter require-
ment is the subject of Chapter 6, and the former requirement is discussed periodically in
this section.
So where do informative priors come from? Generally there is an abundance of previous
work in the social and behavioral sciences that can guide the researcher, including her own.
So generally, these priors are derived from:
▷previous studies, published work,
▷researcher intuition,
▷interviewing substantive experts,
▷convenience through conjugacy,
▷nonparametrics and other data derived sources,

The Bayesian Prior
115
which can obviously be overlapping deﬁnitions. Prior information from previous studies
need not be in agreement. One fruitful strategy is to construct prior speciﬁcations from
competing intellectual strains in order to contrast the resulting posteriors and say something
informed about the relative strength of each. The last item on this list can be productive if
the data used are distinct from that at hand to be used to construct the likelihood functions.
There is considerable controversy, otherwise, about “double-use” of the data.
4.5.1
Power Priors
Ibrahim and Chen (2000a) introduce an informed prior that explicitly uses data from
previous studies (also discussed by Ibrahim, Chen, and Sinha [2003] as well as Chen and
Ibrahim [2006]). Their idea is to weight data from earlier work as input for the prior used in
the current model. Deﬁne x0 as these older data and x as the current data. Their primary
application is to clinical trials for AIDS drugs where a considerable amount of previous data
exist. In a social science context, there are many settings where previous research informs
extant model speciﬁcations. Our interest centers on the unknown parameter θ, which is
studied in both periods. Specify a regular prior for θ, p(θ) that would have been used
un-modiﬁed if the previous data were not included. This can be a diﬀuse prior if desired,
although it will become informed through this process.
An elementary power prior is created by updating the regular prior with a likelihood
function from the previous data in a very simple manner, which is scaled by a value a0 ∈
[0:1]:
p(θ|x0, a0) ∝p(θ)[L(θ|x0)]a0.
(4.16)
It is important to remember that this is still a prior form and the regular process follows
wherein the posterior is obtained by conditioning this distribution on the data through the
likelihood function based on the current data:
π(θ|x, x0, a0) ∝p(θ|x0, a0)L(θ|x).
(4.17)
The parameter a0 scales our conﬁdence in the similarity or applicability of the previous
data for current inferences. If it is close to zero then we do not particularly value the older
observations or studies, and if it is close to one then we believe strongly in the ties to the
current data. So lower values favor the regular prior speciﬁcation, p(θ), and the choice of
this parameter can be very inﬂuential. Note that we would not want this value to exceed
one, since that would be equivalent to valuing older over newer data.
To reduce the inﬂuence of a single choice for a0, we specify a mixture of these priors
using a speciﬁed distribution for this parameter, p(a0|.). Thus (4.16) becomes
p(θ|x0) =
 1
0
p(θ|x0, a0)p(a0|·)da0
=
 1
0
p(θ)[L(θ|x0)]a0p(a0|·)da0.
(4.18)

116
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
Here the parameterization for p(a0|·) is left vague since its parametric form remains unde-
ﬁned. Chen and Ibrahim recommend a beta distribution as the “natural” choice, but point
out that truncated forms of the normal or gamma work as well. The mixture speciﬁcation
has the eﬀect of inducing heavier tails in the marginal distribution of θ and thus represents
a more conservative choice of prior.
4.5.2
Elicited Priors
A completely diﬀerent class of priors is derived not from real or desired mathematical
properties, but from previous human knowledge on the subject of investigation.
These
elicited priors are discussed in detail in Gill and Walker (2005), with a detailed application
to attitudes towards the judicial system in Nicaragua. Typically the source for elicited
priors is from subject area experts with little or no concern for the statistical aspects of the
project. These include physicians, policy-makers, theoretical economists, and qualitative
researchers in various ﬁelds. However, there is no reason that politicians, study participants,
outside experts, or opinion leaders in general could not be used as a source for informative
priors as well.
The bulk of the published work on elicited priors is on the Bayesian analysis of clinical
trials.
In these settings, it is typical to elicit qualitative priors from the clinicians as a
means of incorporating local expertise into the calculations of posteriors and trial stopping
points (Freedman and Spiegelhalter 1983, Kadane 1986, Spiegelhalter, Abrams, and Myles
2004, Chapter 5). There is also a small literature on elicitation of priors for variable selection
(Garthwaite and Dickey 1988, 1992; Ibrahim and Chen 2000b). Here we will concentrate on
the more basic task of using elicitation to specify a particular parametric form for the prior.
It is relatively common to use conjugate priors or mixtures of conjugate priors for this task
so as to remove additional complications. However, this is certainly not a mathematical or
theoretical restriction.
Although an overwhelming proportion of the studies employing elicited priors are in the
medical and biological sciences, the methodology is ideal for a wide range of social science
applications. In virtually every ﬁeld and subﬁeld of the various disciplines there are prac-
ticing “experts” whose opinions can be directly or indirectly elicited. Furthermore, the fact
that the social and behavioral sciences are focused on varying aspects of human behav-
ior means that describing current knowledge and thinking about some speciﬁc behavioral
phenomenon probabilistically is a more realistic way to incorporate disparate judgments.
Restated, uncertain and divided opinion is better summarized in probabilistic language than
with deterministic alternatives.
The central challenge here is how to translate expert knowledge into a speciﬁc probability
statement. This process ranges from informal assignments to detailed elicitation plans and
even regression analysis across multiple experts (Johnson and Albert 1999, Chapter 5).
Spetzler and Sta¨el von Holstein (1975) outline three general steps in the process:

The Bayesian Prior
117
1. Deterministic Phase.
The problem is codiﬁed and operationalized into speciﬁc
variables and deﬁnitions.
2. Probabilistic Phase. Experts are interviewed and tested in order to assign proba-
bilistic values to speciﬁc outcomes.
3. Informational Phase. The assigned probabilities are tested for inconsistencies and
completeness is veriﬁed.
The deterministic phase includes specifying the explanatory variables and possibly their
assumed parametric role in the model (Steﬀey 1992), determining data sources and data
collection processes ﬁtted to this methodology (Garthwaite and Dickey 1992), determining
how many experts to query and where to ﬁnd these experts (Carlin, et al. 1993), and
ﬁnally judging their contributions (Hogarth 1975). Some of this work is far from trivial:
experts might need to be trained prior to elicitation (Winkler 1967), variable selection
can be inﬂuenced by the diﬃculty of elicitation (Garthwaite and Dickey 1992), and cost
projections can be diﬃcult.
The informational phase is somewhat mechanical and it includes testing elicitation re-
sponses for consistency, calibrating responses with known data, and perhaps weighting
expert opinions. Determining consistency is an important requirement and experts diﬀer in
their familiarity with the details of the project at hand. Less experienced respondents tend
to show more inconsistencies (especially with continuous rather than discrete choices), and
more experienced respondents as well as normative experts show high levels of consistency
(Hogarth 1975, Winkler 1967). By consistency it is meant that answers do not contradict
each other, for instance, the subset of an event having a higher probability than the event
itself. Calibration generally involves comparison of results after the rest of the analysis and
can be a safety check for future work as well as a conﬁrmation of the reliability of the ex-
perts (Seidenfeld 1985). Sometimes these checks are further complicated when the subject
is a rapidly changing area and the experts’ earlier statements can quickly become outdated
(Carlin, et al. 1995). Leamer (1992) also gives a diagnostic approach that helps categorize
elicitations into blunt responses removing the necessity of further inquiry.
By far the most challenging is the probabilistic phase and this has consumed the bulk
of the literature. For instance, the experts can be asked ﬁxed value (“P-methods”) and/or
ﬁxed probability (“V-methods”) questions where speciﬁc estimates of the probability or
relative likelihood of events are queried (Spetzler and Sta¨el von Holstein 1975, p.347).
In
addition, the experts can be asked these questions directly with regard to a cumulative den-
sity function (CDF), or indirectly by way of physical devices or hypothetical constructions.
A more challenging, but perhaps informative, approach is to ask open-ended questions and
code the response. In all of these cases, it is important to clarify to experts that they are giv-
ing probability estimates rather than utility assessments (Kadane and Winkler 1988). The
concern is that these experts will otherwise express their normative ideals about outcomes,
and preferred outcomes will be given unrealistic probabilities.

118
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
In general it is not feasible to ask subject-matter experts to make determinations about
coeﬃcient estimates or about moments for speciﬁed PDFs and PMFs convenient to the
statistician. So the most common strategy, dating back to the seminal paper of Kadane
et al. (1980), is to query these experts about outcome variable quantiles for given (hypo-
thetical) levels of explanatory variables. For example, in one study an emergency room
physician is asked about survival probabilities of patients with speciﬁed injury type, injury
severity score, trauma score, age, and type of injury (Bedrick, Christensen, and Johnson
1997). The idea is then to take these quantiles and solve for the parameters of an assumed
distributional form for the (often conjugate) prior.
One particularly simple application is in the case of a binomial outcome. For psycho-
logical reasons, it appears to be easier to elicit hypothetical binary outcomes. Using the
beta conjugate prior several authors have suggested algorithms for elicitation (Chaloner and
Duncan 1983, 1987; Gavasakar 1988).
The basic process is to hypothesize a ﬁxed set of
Bernoulli trials, ask the expert to give a most likely number of successes given the particular
scenario and reasonable bounds on the uncertainty, work these values backward into the
beta-binomial PMF to get the beta parameters, and ﬁnally show the expert the posterior
implications of these values. If they are found to be unreasonable, then adjustments are
made and the process repeats itself.
4.5.2.1
The Community of Elicited Priors
The priors that are elicited from experts can have a variety of characterizations. Kass
and Greenhouse (1989) coined the phrase “community of priors” to describe the range of
attitudes that equally qualiﬁed experts may have about the same phenomenon. These can
be categorized as well:
▷Clinical Priors. These are priors elicited from substantive experts who are taking
part in the research project. This is often done because these individuals are easily
captured for interviews and are motivated by a direct stake in the outcome.
▷Skeptical Priors. These are priors built with the assumption that the hypothesized
eﬀect does not actually exist and are usually operationalized with a zero mean. Skep-
tical priors can be created because of actual skepticism or because overcoming such
a prior provides stronger evidence: “. . . set up as representing an adversary who will
need to be disillusioned by the data . . . ” (Spiegelhalter et al. 1994, Spiegelhalter,
Abrams, and Myles 2004).
▷Enthusiastic Priors. These are obviously the opposite of the skeptical prior. The
priors are built around the positions of partisan experts or advocates and generally as-
sume the existence of the hypothesized eﬀect. For comparative purposes, enthusiastic
priors can be speciﬁed with the same variance, but diﬀerent mean, as corresponding
skeptical priors.

The Bayesian Prior
119
▷Reference Priors. Such priors are occasionally produced from expert sources, but
they are somewhat misguided because the purpose of elicitation is to glean information
that can be described formally.
The priors are restated from Spiegelhalter et al. (1994) in order to be less focused on
the application to clinical trials. The key point is to understand the diﬀering perspectives
of experts. One approach is to contrast the posterior results obtained from divergent prior
perspectives, including a formalized process of overcoming adversarial prior speciﬁcations in
favor of priors more sympathetic to research questions through additional sampling (Lindley
and Singpurwalla 1991), randomization strategies (Kass and Greenhouse 1989), scoring
rules (Savage 1971), or other means.
4.5.2.2
Simple Elicitation Using Linear Regression
An analyst asks an expert for predictions on an expected outcome for some interval-
measured event of interest. The V-method question asked is: what would be an expected
low value as a 0.25 quantile (labeled x0.25) and an expected high value as a 0.75 quantile
(labeled x0.75)? These two supplied quantile values, x0.25 and x0.75, correspond to normal
z-scores z0.25 = −0.6745 and z0.75 = 0.6745, which specify the shape of a normal PDF since
there are two equations and two unknowns:
z0.25 = x0.25 −α
β
z0.75 = x0.75 −α
β
.
(4.19)
Here α and β are the mean and standard deviation parameters of the normal PDF:
f(x|α, β) = (2πβ2)−1
2 exp
'
−1
2β2 (x −α)2
(
.
(4.20)
This notation for the parameters of a normal is diﬀerent, but the reason shall soon become
apparent. When we solve for α and β in (4.19), we have a fully deﬁned prior distribution
in (4.20) and the elicitation is complete.
Of course, one expert is typically not enough to produce robust prior forms, so now
query experts 1, 2, . . . , J. This produces an over-speciﬁed series of equations since there
are J × 2 equations and only two unknowns (Spiegelhalter et al. [1994], for instance, use
J = 10). It is necessary to assume that these experts are exchangeable meaning that they
all provide equal quality elicitations.
Secondly, given the cost of interviewing, we are likely to ask each expert for more
than just these two quantiles. Each assessor is asked to give ﬁve quantile values at m =
[0.01, 0.25, 0.5, 0.75, 0.99] corresponding to standard normal points zm. At this point, (4.19)
can be re-expressed for the quantile level m given by assessor j: xjm = α + βzjm, and
the total amount of expert-elicited information constitutes the following over-speciﬁcation

120
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
(J × 5 equations and 2 unknowns) of a normal distribution:
x11 = α + βz11
x21 = α + βz21 . . .
x(J−1)1 = α + βz(J−1)1
xJ1 = α + βzJ1
x12 = α + βz12
x22 = α + βz22 . . .
x(J−1)2 = α + βz(J−1)2
xJ2 = α + βzJ2
x13 = α + βz13
x23 = α + βz23 . . .
x(J−1)3 = α + βz(J−1)3
xJ3 = α + βzJ3
x14 = α + βz14
x24 = α + βz24 . . .
x(J−1)4 = α + βz(J−1)4
xJ4 = α + βzJ4
x15 = α + βz15
x25 = α + βz25 . . .
x(J−1)5 = α + βz(J−1)5
xJ5 = α + βzJ5
The approach suggested by this setup is to run a simple linear regression to estimate α as
the intercept and β as the slope. There are two issues to worry about. One must check for
logical inconsistencies in consistent quantile values for each assessor (see Lindley, Tversky,
and Brown
[1979] for a discussion of problems). Also, it is critical to apply necessary
mathematical constraints such as ensuring that the estimated coeﬃcient for β remains
positive (if substantively required) since the basic linear model imposes no such restriction
(Raiﬀa and Schlaifer 1961).
■Example 4.1:
Eliciting Expected Campaign Spending.
We are interested in
eliciting a prior distribution for expected campaign contributions received by major-
party candidates in an impending U.S. Senate election in order to specify an en-
compassing Bayesian model.
Elicitation replaces data here since the election has
not yet taken place.
Eight campaign experts are queried for quantiles at levels
m = [0.1, 0.5, 0.9], and they provide the following values reﬂecting the national range
of expected total intake by Senate candidates (in thousands):
x11 = 400
x12 = 2500
x13 = 4000
x21 = 150
x22 = 1000
x23 = 2500
x31 = 300
x32 = 900
x33 = 1800
x41 = 250
x42 = 1200
x43 = 2000
x51 = 450
x52 = 1800
x53 = 3000
x61 = 100
x62 = 1000
x63 = 2500
x71 = 500
x72 = 2100
x73 = 4200
x81 = 300
x82 = 1200
x83 = 2000
where x83 is expert 8’s third quantile. None of the experts have supplied quantile
values out of logical order, so these results are consistent. Using these “data” we
regress x on z to obtain the intercept and slope values:

The Bayesian Prior
121
x <- c( 400, 2500, 4000,
150, 1000, 2500,
300,
900,
1800,
250, 1200, 2000,
450, 1800, 3000,
100,
1000, 2500,
500, 2100, 4200,
300, 1200 ,2000)
z <- qnorm(rep(c(0.1,0.5,0.9),8))
summary(lm(x~z))
which returns:
Estimate Std. Error t value Pr(>|t|)
(Intercept)
1506.3
127.0
11.858 4.99e-11
qnorm(y)
953.4
121.4
7.854 8.00e-08
Thus the normal prior mean is ˆα = 1506.3 and the normal prior standard deviation
is ˆβ = 953.4.
4.5.2.3
Variance Components Elicitation
A problem with direct quantile elicitation is that assessors often misjudge the probability
of unusual values because it is more diﬃcult to visualize and estimate tail behavior than to
estimate means or medians. Hora et al. (1992) found that when non-technical assessors are
asked to estimate spread by providing high probability coverage intervals such as at 99%,
then they tend to perceive this as near-certainty coverage and overstate the bounds. But
this ﬁnding is not universal: in other settings people tend to think of rare events in the
tails of distributions as more likely than they really are (an eﬀect exploited by casinos and
lotteries). Accordingly, O’Hagan (1998) improves elicited estimates of spread by separately
requiring assessors to consider two types of uncertainty:
▷uncertainty about an estimate relative to an assumed known summary statistic
▷secondary uncertainty of this summary only.
The elicitee ﬁrst gives a (modal) point estimate for the explanatory variable coeﬃcient,
τ, and is then asked “given your recent estimate of τ, what is the middle 50% probability
interval around τ?” The elicitees must understand that this is the interval that contains the
middle half of the expected values. So this (V-method) speciﬁes a density estimate centered
at the assessors modal point, and if the form of the distribution is assumed or known,
then the exact value for the variance can be calculated under a distributional assumption
(normal, Student’s-t, or a log-normal if a right-skewed interval is required).
O’Hagan (1998) prefers asking for the middle 66% of the density, which he calls the
“two-to-one interval” since the middle coverage is twice that of the combined tails. Now if a
normal prior is used then this interval quickly yields a value for the standard deviation since
it covers approximately two of them. It should actually be multiplied by 68
66 to be exactly
correct but analysts often do not worry about the diﬀerence since measurement error is
almost certainly greater than the diﬀerence.
Now the researcher calculates the implied
variance from this and shows the assessor credible intervals at familiar (1 −α)-levels. If
these are deemed by the elicitee to be too large or too small, then the process is repeated.

122
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
We want to elicit prior distributions for τi across n cases, with unknown total T =
n
i=1 τi. The assessor ﬁrst provides point estimates for each case: x1, x1, . . . , xn, so that
the estimated total is given by xT = n
i=1 xi.
These are useful values, but it is still
necessary to get a measure of uncertainty in order to produce a variance for the full elicited
prior distribution.
The individual deviance of the ith estimate from its true value can be rewritten alge-
braically:
τi −xi =

τi −xi
xT
T

+ xi
xT
(T −xT ) .
(4.21)
The ﬁrst quantity on the right-hand side of (4.21) is the deviance of τi from an estimate
that would be provided if we knew T for certain:
E[τi|T ] = xi
xT
T
(4.22)
(which can be considered as between-case deviance). Now the second quantity on the right-
hand side of (4.21) is the weighted deviation of T , i.e., uncertainty about the true total.
The expected value form (4.22) helps us obtain the variance of τi:
Var(τi) = E [Var(τi|T )] + Var (E[τi|T ])
= E
'
τi −xi
xT
T
(2
+

E
	
E[τi|T ]2
−(E [E[τi|T ]])2
= E
'
τi −xi
xT
T
(2
+
 xi
xT
2
Var(T ),
(4.23)
which shows the general form of the two variance components. A better form for elicitation
is achieved by dividing both sides of this equation by x2
i :
Var
 τi
xi

= Var
 τi
xi
−T
xT

+ Var
 T
xT

.
(4.24)
At this point elicitees are queried about the middle spread around these two quantities
individually. First, they give an estimate of middle spread around
T
xT , assuming accuracy
of the sum xT as an estimate of T . Then, they give the middle spread around each
τi
xi
assuming that
T
xT = 1. This means that there is no second component in the variance to
consider at this moment. Once the individual means and variances are elicited, these
xi
xT
values are put into the assumed distribution deﬁned over [0:1] (they are proportions) to
form the complete prior distribution speciﬁcation. Two common distributional forms are the
normal CDF and the beta distribution. In the case where τi
T is from a beta distribution, we
can solve for the parameters with the beta distribution mean and variance: E
	 τi
T

=
α
α+β ,
Var
 τi
T

=
αβ
(α+β)2(α+β+1).
■Example 4.2:
Minority Political Participation. This example is from Gill and
Walker (2005).
An expert on minority electoral participation is asked to estimate
upcoming Hispanic turnout for n precincts in a given district: τ1, τ2, . . . , τn, with total

The Bayesian Prior
123
Hispanic turnout in the district equal to T . She ﬁrst gives estimates x1, x2, . . . , xn
for each precinct, which give a district turnout estimate of T by summing, xT . This
result does not yet give the variance information necessary to build a prior using an
assumed normal distribution.
The expert is now asked to provide the two-to-one interval for
T
xT , giving [0.7:1.3]:
they believe that the summed estimate of Hispanic turnout is correct to plus or minus
30% with probability 0.66 (from the two-to-one interval). To conﬁrm the expert’s
certainty about this, the value σT = 0.3 is plugged into the normal CDF at levels to
give credible interval summaries:
50% CI = [Φμ=1,σ=0.3(0.25):Φμ=1,σ=0.3(0.75)] = [0.798:1.202]
99% CI = [Φμ=1,σ=0.3(0.005):Φμ=1,σ=0.3(0.995)] = [0.227:1.773].
These are then displayed to the elicitee, and if she agrees that these are reasonable
summaries then the variance is σ2
T = (0.3)2 = 0.09 and there is no need to iterate here.
The expert is now asked to repeat this process for each of the xi estimates under the
assumption that xT = T (the estimate of the total above is correct). This temporary
ﬁxing of xT means that the right-hand-side of (4.24) reduces to the variance of
τi
xi
and the expert can do the same interval process as was done with
T
xT for each of the
n precincts.
Suppose that two-to-one interval for the estimate of Hispanic turnout at the ﬁrst
precinct (x1 = 0.2) is given as [0.5:1.5]: she believes the estimate to be correct to
plus or minus 50% with probability 0.66. This gives a variance of σ1 = (0.5)2 = 0.25,
and we will note that the subsequent 50% and 99% credible interval summaries are
approved by the expert. So the total elicited variance for the ﬁrst precinct is given
by (4.24) where x2
1 is moved back to the right-hand-side: Var(τ1) = x2
1(0.25 + 0.09) =
0.0136.
4.5.2.4
Predictive Modal Elicitation
If the outcome variable of interest is distributed Bernoulli or binomial, then it is usually
straightforward to query experts directly for prior probabilities. For psychological reasons
probabilities of binary or summed binary outcomes are more intuitively easy to visualize
(Cosmides and Tooby 1996). Using the natural (conjugate) choice of a beta conjugate prior
Chaloner and Duncan (1983, 1987) produce the predictive modal (PM) elicitation algorithm
(see also Gavasakar [1988] for a second application). First ﬁx a hypothetical total number
of Bernoulli trials, and then ask the elicitee to specify the most likely number of successes
as well as reasonable bounds on the uncertainty. These values are then worked backward
into the beta-binomial parametric setup (Chapter 1) to get the beta prior distribution
parameters. The elicitee is now shown the implications of their stipulated values on the
shape of the beta prior on a computer terminal or with pre-prepared ﬂip-charts. If the

124
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
elicitee ﬁnds the distribution to be unlike their prior expectations, then adjustments are
made in the deterministic phase and the process is repeated.
The data, X1, X2, . . . , Xn, are distributed iid Bernoulli, so use Y = n
i=1 Xi distributed
binomial(n, p). Interest lies in the posterior distribution of the probability of occurrence
of some event of interest (bill passes/fails, treaty/no-treaty, etc.), which is p in this bino-
mial PMF. The PM method ﬁrst assumes a beta distribution prior for p with unknown
parameters: p ∼beta(A, B) with A, B > 1. The individual steps are:
▷select a ﬁxed number of trials for a hypothetical experiment (n = 20 is recommended)
▷ask the elicitees to give the prior predictive modal value for this n: the most likely
number of successes out of n trials, m ∈(1:n).
The numerator of Bayes’ Law is:
f(y|p)f(p) =
Γ(n + 1)Γ(A + B)
Γ(y + 1)Γ(n −y + 1)Γ(A)Γ(B)py+A−1(1 −p)n−y+B−1.
(4.25)
Now the marginal distribution for y is obtained by integrating over p:
f(y|A, B) =
 1
0
Γ(n + 1)Γ(A + B)
Γ(y + 1)Γ(n −y + 1)Γ(A)Γ(B)py+A−1(1 −p)n−y+B−1dp
=
Γ(n + 1)Γ(A + B)
Γ(y + 1)Γ(n −y + 1)Γ(A)Γ(B)
Γ(y + A)Γ(n −y + B)
Γ(n + A + B)
(4.26)
(as shown before on page 49). Since m is the mode of this distribution, then f(y = m|A, B)
is the maximum value obtainable for the function f(y|A, B). The random variable y can
only take on discrete values so f(y = m−1|A, B) < f(y = m|A, B) and f(y = m+1|A, B) <
f(y = m|A, B). Chaloner and Duncan calculate the following two ratios using properties of
the binomial distribution:
dℓ= f(y = m −1|A, B)
f(y = m|A, B)
=
(n −m)(m + A)
(m + 1)(n −m + B −1)
dr = f(y = m + 1|A, B)
f(y = m|A, B)
=
m(n −m + B)
(n −m + 1)(m + A −1).
(4.27)
Both terms are bounded by (0:1). Once the elicitee has identiﬁed m for the researcher,
then the prior parameters (A, B) must be constrained to lie in a cone originating at [1, 1] in
the A, B plane as shown by the solid lines in Figure 4.3. This cone is determined because
the equations in (4.27) deﬁne linear limits in two-space from the same starting point. Any
particular point within the cone represents a two-dimensional Cartesian distance from the
uniform prior since the origin of the cone speciﬁes a beta(1, 1).
We do not yet have a complete answer since there are an inﬁnite number of (A, B) pairs
that could be selected and still remain inside the cone. At this point the elicitee is told to
think about spread around the mode and is shown a histogram for binomial(n, m/n), and
is asked: “If we were to go one unit up (and down), how much do you think the probability
of occurrence would decrease?” With these two values (up and down) we can now calculate

The Bayesian Prior
125
dℓand dr directly. Thus the equations in (4.27) deﬁne line segments for values of A and B
that are necessarily bounded by the cone. The point of intersection is the (A, B) pair that
satisﬁes both the one unit up restriction and the one unit down restriction, assuming that
the following restriction is met:
dℓdr >
m(n −m)
(m + 1)(n −m + 1).
(4.28)
If it this is not true, then the assessor is asked to provide new values of f(y = m−1|A, B) and
f(y = m + 1|A, B). The line segments and their intersection are also shown in Figure 4.3.
Label this point of intersection (A1, B1), and calculate a new beta mode with these values:
m1 =
A1 −1
A1 + B1 −2.
(4.29)
We then display it to the assessor with the middle 50% of the density. The assessor is then
asked if this interval is too small, too large, or just right. The interval is adjusted according
to the following:
“too small” =⇒h = −1,
“too large” =⇒h = +1,
“just right” =⇒h = 0,
where h is inserted into:
A2 = 1 + 2h(A1 −1),
B2 = 1 + 2h(B1 −1),
(4.30)
A
B
1
1
A
B
1
1
FIGURE 4.3: Finding the Beta Parameters

126
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
for adjusted parameter values. This process is repeated until the assessor is satisﬁed with
the interval (h = 0).
■Example 4.3:
Labor Disputes in OECD Countries. Suppose we are interested
in eliciting a prior distribution for the probability of a strike given a major labor
dispute in 20 OECD countries over a certain year. Given n = 20 an elicitee indicates
m = 5, and is then shown a histogram of the binomial(20, 5/20) distribution. The
assessor now asserts that the one unit up and down probability change is 1/50, so
dℓ= 0.98 = (15)(5+A)
(6)(14+B) and dr = 0.98 =
5(15+B)
(16)(4+A) (these values are acceptable to the
condition above since dℓdr = 0.9604 >
m(n−m)
(m+1)(n−m+1) = 0.7813). Solving the equations
produces A1 = 1.4, and B1 = 2.327, which gives a modal value for the probability
of a strike given a labor dispute of ˆp = 0.232 with the middle of 50% of the density
[0.193 : 0.537]. The elicitee states that this interval is too large, so we set h = +1
and produce A2 = 1 + 2h(1.4 −1) = 1.8, and B2 = 1 + 2h(2.327 −1) = 3.654, which
gives the middle of 50% of the density as [0.184:0.456]. The elicitee still believes that
this is too large so the process is repeated with h = +1 now providing [0.182:0.386].
This interval is smaller than the ﬁrst and is acceptable to the elicitee. Therefore the
elicited prior distribution for the probability of a strike given a major labor dispute
has the distribution beta(2.6, 6.308).
4.5.2.5
Prior Elicitation for the Normal-Linear Model
Using the standard regression setup described on page 145, we want to elicit priors on β
ν = df
a( ˜X)
ν = df
a( ˜X)
3
2.76
12
2.37
4
2.62
14
2.36
5
2.53
16
2.35
6
2.48
18
2.34
7
2.45
20
2.33
8
2.42
30
2.31
9
2.40
40
2.31
10
2.39
60
2.30
∞
2.27
FIGURE 4.4: t Ratios
from experts (we can retain the uninformed approach
for σ2 or we can elicit for it as well).
Kadane et
al. (1980) as well as Kadane and Wolfson (1998) ﬁrst
establish m design points of the explanatory variable
vector: ˜X1, ˜X2, . . . , ˜Xm, where again these represent
interesting cases or values spanning the range of the
k variables.
These values must be chosen such that stacking
the vectors into an m × k matrix, ˜X, gives a posi-
tive deﬁnite matrix ˜X′ ˜X. The elicitees are asked to
study each of the ˜Xi row vectors and produce y50, a
vector of outcome variable medians whose elements
correspond to the design cases.
Such values then
represent typical responses to the hypothesized design points speciﬁed in the ˜Xi. Now an
elicited prior point estimate for β is given by: ˆb0.50 = ( ˜X′ ˜X)−1 ˜X′y0.50.
To get a full prior description for β assume that this distribution is Student’s-t around
ˆb0.50 with greater than ν = 2 degrees of freedom (we do not want to worry about the
existence of moments). This is a somewhat conservative prior since large data size under

The Bayesian Prior
127
weak regularity conditions leads to Bayesian posterior normality of linear model coeﬃcients,
and t-distributed forms with smaller data size (Berger 1985, p.224; Lindley and Smith 1972).
There is no direct way to set the degrees of freedom for this t-distribution since the m value
was established arbitrarily by the researchers.
Kadane et al. (1980) suggest that after eliciting y0.50 for each ˜Xi, researchers should also
elicit y0.75 by asking for the median point above the median point just provided. Repeat
this process two more times in the same direction to obtain y0.875, and y0.9375. For each of
the m assessments calculate the ratio:
a( ˜X) = (y0.9375 −y0.50)/(y0.75 −y0.50),
(4.31)
where diﬀerencing makes the numerator and denominator independent of the center, and
the ratio produced is now independent of the spread described. This ratio uniquely describes
tail behavior for a t-distribution because it is the relative “drop-oﬀ” in quantiles. Kadane
et al. give tabulated degrees of freedom against values of this ratio, a subset of which is
given in Figure 4.4.
Values greater than 2.76 indicate that the researcher should instruct the elicitee to
reconsider their responses, and values less than 2.27 imply that a standard normal prior
centered at ˆb0.50 is appropriate. Other distributions are feasible, but this tabulation makes
the Student’s-t particularly easy. Once the unique degrees of freedom are identiﬁed, the
elicited prior is fully identiﬁed.
4.5.2.6
Elicitation Using a Beta Distribution
Gill and Freeman (2013) developed an elicitation process for updating social networks
using beta distribution forms. The process starts with the more general form of the beta
probability density function,
f(y) = Γ(α + β)
Γ(α)Γ(β)
(y −a)α−1(b −y)β−1
(b −a)α+β−1
,
(4.32)
where: 0 ≤a < y < b, and α, β > 0. Now the support is over [a : b] rather than just
[0:1], but it reduces to the standard form with the change of variable: X = Y −a
b−a , where
0 < x < 1, but α and β are unchanged. This also means that Y = (b −a)X + a. This
change means that qualitative experts can be queried on the more natural [0 :100] scale
and it is converted back to a regular beta form. Now recall the following beta distribution
properties:
mean:
μx =
α
α + β = μy −a
b −a
variance:
σ2
x =
αβ
(α + β)2(α + β + 1) =
σ2
y
(b −a)2 .

128
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
Rearranging the equations above gives us the following two important relations from Law
and Kelton (1982, p.205):
α =
'μx(1 −μx)
σ2x
−1
(
μx
β =
'μx(1 −μx)
σ2x
−1
(
(1 −μx).
(4.33)
So querying for a mean and variance would produce all the necessary information to obtain
the implied α and β and thus fully describe the beta distribution. Research to date demon-
strates that humans are not very good subjective assessors of variance (O’Hagan 1998),
necessitating other means of getting to the parameters.
Gill and Freeman propose querying the experts for their lowest but reasonable value and
highest but reasonable value with the idea that these can be explained as endpoints of a
95% credible interval. Some training is therefore necessary, but not a substantial amount.
Restricting α > 1 and β > 1 guarantees that the beta distribution will be unimodal, so the
credible interval is not broken up into disjoint components. Combining these, we know that
4σy ≈b −a. We also know that
σ2
y = (b −a)2σ2
x = (b −a)2
αβ
(α + β)2(α + β + 1).
Substituting produces the approximation: σx ≈
1
4, which leads directly to preliminary
estimates of the two parameters. This draws on the PERT (Program Evaluation and Review
Technique) analysis for industrial and engineering project planning that uses σx = 1
6 when
the beta mode is between 0.13 and 0.87, and is justiﬁed by a normal approximation (Farnum
and Stanton 1987, Lu 2002).
More importantly, the value of 1/4 is just a starting point for
a software “slide” that the elicitee is allowed to adjust on a computer screen. The elicitee
is shown the resulting beta distribution graphically and given two slides: one that changes
μx =
α
α+β and one that changes σ2
x =
αβ
(α+β)2(α+β+1). Subsequently, there are new values
of μx and σx that the elicitee prefers. This can be repeated for any reasonable number of
priors.
4.5.2.7
Eliciting Some Final Thoughts on Elicited Priors
Elicited priors in Bayesian studies have been studied for quite some time, going back to
the foundational papers of Kadane (1980), Kadane et al. (1980), Hogarth (1975), Lindley,
Tversky, and Brown (1979), Savage (1971), Spetzler and Sta¨el von Holstein (1975), Tversky
(1974), Tversky and Kahneman (1974), and Winkler (1967). There have been two main
impediments to widespread acceptance and use of elicited priors. First, there has long been
a distrust by some Bayesians and others of overtly subjective priors. The failed attempt to
produce a universally accepted no-information prior was fueled in part by these sentiments.
Second, there remains worry about the cognitive process that generates elicited priors.
These concerns are summed up by Hogarth (1975, p.273):

The Bayesian Prior
129
In summary, man is a selective, stepwise information processing system with
limited capacity, and, as I shall argue, he is ill-equipped for assessing subjective
probability distributions. Furthermore, man frequently just ignores uncertainty.
This does not mean that we should never use human experts for generating prior informa-
tion, but rather that we should carefully elicit probabilistic statements, knowing that the
sources have cognitive limitations. A very promising approach is the use of interactive com-
puter queries to elicit priors (Garthwaite and Dickey 1992). The software can be written so
that inconsistent, illogical, and contradictory answers are rejected at the data-entry stage
rather than corrected or ignored later. Web-based implementations, though still untried,
promise even greater returns given their convenience and ubiquity.
There is a close relationship between elicited priors and meta-analysis. It is possible
to assemble the results from previous studies and treat these as elicitations.
This is a
fundamentally Bayesian idea and essentially represents a Bayesian treatment of conventional
meta-analysis where the predictive distribution from previous works forms the basis of a
prior on the current project (Carlin 1992). One caveat is warranted, however. Kass and
Greenhouse (1989) warn that this approach implies that the previous studies pooled to form
a prior are now assumed to be exchangeable and this might not be appropriate.
4.6
Hybrid Prior Forms
Some prior speciﬁcations are attempts to mix informative and uninformed types in such
a way that gradations of information can be reﬂected. Technically this makes them infor-
mative priors, but we will treat them as compromises since this is how they are typically
viewed. The key point is that the speciﬁcation of prior distributions is very ﬂexible, and
can be tailored by researchers to reﬂect varying levels of qualitative or quantitative infor-
mation. In this section we describe some recent designs for prior distributions that focus
on addressing speciﬁc model problems rather than general prior speciﬁcations.
A number of other prior forms are common in applied settings. We will describe mixture
priors in Chapter 9, how empirical Bayes uses the observed data to establish hyperpriors
in a hierarchical model (prior parameters on the highest level of priors), and the idea of
specifying both hierarchical structure and subjective information into the prior leads to
hierarchical models in general. Much work has been done toward ﬁnding new criteria for
uninformative priors. These approaches include maximally vague entropy priors (Spall and
Hill 1990; Berger, Bernardo, and Mendoza 1989), indiﬀerence conjugate priors (Novick and
Hall 1965, Novick 1969), and the general idea of proper but very diﬀuse priors (Box and
Tiao 1973).

130
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
4.6.1
Spike and Slab Priors for Linear Models
Mitchell and Beauchamp (1988) introduce a species of priors designed to facilitate ex-
planatory variable selection in linear regression. The basic idea is to specify a reasonably
skeptical prior distribution by stipulating density spike at zero surrounded symmetrically by
−fk
fk
0
h0k
h1k
FIGURE 4.5: Spike and Slab Prior
a ﬂat slab with speciﬁed boundaries.
If desired, other non-zero values can
be evaluated by simply oﬀsetting
the coeﬃcient value. Also, this type
of prior has only been applied to lin-
ear regression, but it would only be
slightly more involved to apply to
generalized linear models.
Thus the prior makes a reason-
ably strong claim about zero eﬀect
for some regression parameter, but
admits the possibility of non-zero
eﬀects.
Suppose we are seeking a
prior for the jth regression coeﬃ-
cient (from j ∈1, . . . , k) in a standard linear model: y = xβ + ϵ, with all of the usual
Gauss-Markov assumptions, k < n, and ϵ ∼N(0, σ2). The prior is then:
spike:
p(βj = 0) = h0j
slab:
p(βj ∈[−fj:fj], βj ̸= 0) = 2h1jfj,
−fj < βj < fj,
where h0j + 2h1jfj = 1. This is illustrated in Figure 4.5. So obviously h0j is the height
of the spike and h1j is the height of the slab, and varying degrees of skepticism about the
veracity of the eﬀect of xj can be modeled by altering the ratio:
γj = h0j
h1j
= 2fj
h0j
1 −h0j
.
For terms that should be included in the model with probability one, just set h0j = 0. The
nice part of this priors setup is that we can easily reﬂect our prior belief in complete model
speciﬁcations. Miller (2002, 203) notes that taken by itself, h0j is a statement that the jth
variable should not be in the model, and taken by itself, 1 −h0j is a statement that the jth
variable matters in some way. Since all prior density not accounted for by the spike falls to
the slab, the prior for the mth model, Am, is just the product given by:
p(Am) =
+
j∈Am
(1 −h0j)
+
j̸∈Am
(h0j),
which is the product of the slab density for those coeﬃcients in model Am, times the

The Bayesian Prior
131
product of the spike density for those coeﬃcients not in Am (note that the jth coeﬃcient
only contributes a single term).
For the linear regression model Mitchell and Beauchamp put a prior on σ such that
log(σ) is uniform between −log(σ0) and log(σ0), for some large value of σ0. They then
derive the resulting posterior for model Am with km number of coeﬃcients (including the
constant) in the matrix Xm:
π(Am|X, y) ∝πkm/2Γ
n −km
2

|X′
mXm|−1
2 (S2
m)−(n−km)/2 +
j̸∈Am
γj,
where S2
m = (y −Xmˆβm)′(y −Xm ˆβm), ˆβ = (X′
mXm)−1X′
my, and c is a normalizing
constant. We can see from this how the data informs across all of the proposed model
speciﬁcations. This setup also facilitates coeﬃcient quality assessment as well since we can
average coeﬃcient probabilities across model space. So the joint posterior distribution of
all k of the coeﬃcients is given by:
p(β|X, y) =

m
π(Am|X, y)p(β|αm, X, y)
where p(β|αm, X, y) is multivariate t-distribution centered at ˆβ (see Chapter 5 for more
details on posterior distributions from linear models).
This general approach is called
Bayesian model averaging and is discussed in Chapter 6.
Such priors are easy to construct for linear models and can be applied in more general
settings (see Pang and Gill 2012). Several words of caution are warranted, however. First,
each coeﬃcient operates on a diﬀerent scale so direct comparison cannot be done without
some adjustment like centering and scaling (Mitchell and Beauchamp’s suggestion). Second,
the researcher’s choice of fj and h0j can have a more dramatic eﬀect than intended.
4.6.2
Maximum Entropy Priors
Jaynes (1968, 1980, 1983) introduces the idea of an entropy prior to describe relative
levels of uncertainty about the distribution of prior parameters. One advantage to the en-
tropy approach is that within the same framework, uncertainty ranging from that provided
by the uniform prior to that provided by absolute certainty given by a degenerate (single
point) distribution can be modeled in the same way. Thus the primary advantage to en-
tropy priors is their ﬂexibility. Unfortunately, however, entropy priors are not invariant
to reparameterization and therefore have somewhat limited applicability. Nonetheless they
are worthy of study in their own right because of the link to the idea of information in the
general study of information theory, which is closely tied to the idea of prior knowledge.
The core idea of entropy is the quantiﬁcation of uncertainty of a transmission or obser-
vation (Shannon 1948, Ayres 1994), and this can be interpreted as uncertainty in a PDF or
PMF (Rosenkranz 1977). Initially assume that we are interested in a discrete parameter θ.

132
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
Deﬁne the entropy of θ for a given parametric form, p(θ), as:
H(θ) = −

Θ
p(θi) log[p(θi)],
(4.34)
where the sum is taken over the categories of θ. In the case of discrete distributions, we
have a wide selection of parametric forms for p(θ), but consider two very diﬀerent varieties
for k possible values in the sample space: Θ = [θ1, θ2, . . . , θk]. Following the discussion of
uniform priors, if we assign each outcome a uniform prior probability of occurrence: 1/k,
then the entropy of this prior is at its maximum:
H(θ) = −

Θ
1
k log
'1
k
(
= log[k].
(4.35)
It is clear from this result that prior uncertainty increases logarithmically with increases in
the number of discrete alternatives. At the other end of the uncertainty spectrum, we can
stipulate that p(θi) = 1, and p(θ¬i) = 0; that is, θ equals some value with probability one
and every other value with probability zero. The entropy of this prior speciﬁcation is given
by:
H(θ) = −

Θ[−i]
0 log[0] + 1 log[1] = 0,
(4.36)
where this calculation requires the assumption that 0 log[0] = 0. Here we are completely
certain about distribution of θ and the entropy is zero reﬂecting minimum prior entropy.
The utility of the entropy prior is that we can take some limited knowledge in the form
of a restriction, and for a given parametric speciﬁcation of the prior, produce the prior with
maximum possible entropy. The goal is to include what we may know but to interject as
little else as possible in the creation of the prior: minimally informative about θ for the
speciﬁed p(θ) (Eaves 1985). So a distribution is produced that is as diﬀuse as possible given
the identiﬁed “side conditions.”
Suppose that we could stipulate the ﬁrst two moments as constraints:
E[θ] =

Θ
p(θ)θ = μ1
E[θ2] =

Θ
p(θ)θ2 = μ2.
Adding the further constraint that p(θ) is proper gives the prior density:
˜p(θi) =
exp
	
λ1θi + λ2(θi −μ1)2

j exp [λ1θj + λ2(θj −μ1)2],
(4.37)
where the constants, λ1 and λ2, are determined from the constraints. This is a simpliﬁed
version, and in a more general setting there are additional moments than the two estimated,
say M in E[gm(θ)]. So producing the constants is more involved than the example here
(Robert 2001, p.110; Zellner and Highﬁeld 1988).

The Bayesian Prior
133
The continuous case is both more diﬃcult and more straightforward. It is more diﬃcult
because the analog to (4.34),
H(θ) = −

Θ
p(θ) log[p(θ)]dθ,
(4.38)
leads to diﬀerent answers depending on alternative deﬁnitions of the underlying reference
measures (Jaynes 1968; Robert 2001, p.110). Essentially this just means that mathematical
requirements for getting a usable prior are more diﬃcult to obtain and in some circumstances
are actually impossible (Berger 1985, p.93-94). For the M moment estimation case, the
constraining statement is now:
E[gm(θ)] =

Θ
p(θ)gm(θ)dθ = μm,
m = 1, . . . , M.
(4.39)
Consider the following simple example (O’Hagan 1994). The support of θ is [0:∞] and
we specify the constraint that the expected value is equal to some constant: E[θ] = c. If
we further specify that the prior distribution has the least informative possible exponential
PDF form, then (4.39) speciﬁes the prior: f(θ|c) = c exp(−cθ).
4.6.3
Histogram Priors
Johnson and Albert (1999) suggest the “histogram prior” based on the idea that it is
easy to summarize relatively vague, but informative, prior information through this graph-
ical device. Their prior exploits the idea that it is much easier to elicit or generate a small
number of binned probability statements. This is essentially a nonparametric approach in
that there is no a priori structure on the form of the resulting histogram and is therefore
attractive when the researcher is uncomfortable with specifying a standard form. The es-
timation process is done in segments across the range of the histogram corresponding to a
bin. Within each bin a separate posterior is calculated with a bounded uniform prior pro-
ducing a noncontinuous posterior weighted by the heights of the histogram bins. Although
the discontinuities can be smoothed out, the resulting form may still be unreﬂective of a
compromise between prior and likelihood information.
4.7
Nonparametric Priors
Nonparametric priors were originally introduced by Ferguson (1973) and Antoniak (1974),
but not fully developed until the advent of better computing resources for estimation.
Dirichlet process priors stipulate that the data are produced by a mixture distribution
wherein the Bayesian prior speciﬁcations are produced by a Dirichlet process, which con-
stitutes a distribution of distributions since each produced parameter deﬁnes a particular
distribution. These distributions can be made conditional on additional parameterizations

134
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
(as done in Escobar and West [1995]) and thus the models are hierarchical. This means that
realizations of the Dirichlet process are discrete (with probability one), even given support
over the full real line, and are thus treated like countably inﬁnite mixtures.
This approach is substantially diﬀerent from the conventional use of the Dirichlet distri-
bution in Bayesian models as a conjugate prior distribution for the multinomial likelihood.
First, let us look at the Dirichlet PDF, which is the multivariate generalization of the beta
distribution.
Suppose X = [x1, x2, . . . , xk], where each individual x is deﬁned over the
support [0:1], and sum to one: n
i=1 xi = 1. The Dirichlet PDF uses a parameter vec-
tor, α = [α1, α2, . . . , αk], with the condition that all αi > 0. These structures are then
generalizations from the beta for X = [x, 1 −x], and α = [α, β], where k = 2. The PDF is:
f(X|α) = Γ (n
i=1 αi)
n
i=1 Γ(αi)
n
+
i=1
xαi−1
i
.
(4.40)
The resemblance to the beta is quite strong here. This distribution has expected value and
variance:
E[Xi] =
αi
n
k=1 αk
(4.41)
Var[Xi] =
αi (n
k=1 αk −αi)
(n
k=1 αk)2 (n
k=1 αk + 1)
(4.42)
Cov[Xi, Xj] =
−αiαj
(n
k=1 αk)2 (n
k=1 αk + 1)
.
(4.43)
Consider the question of modeling dichotomous individual choices, Yi, like turning out
to vote, voting for a speciﬁc candidate, joining a social group, discontinuing education, and
so on. The most common “regression-style” modeling speciﬁcation is to assume that an
underlying smooth utility curve dictates such preferences, providing the unobserved, but
estimated threshold θ ∈[0, 1]. The individual’s threshold along this curve then determines
the zero or one outcome conditional on an additive right-hand speciﬁcation, Xβ. Real-
istically, we should treat this threshold diﬀerently for each individual, but we can apply
the reasonable Bayesian approach of assuming that these are diﬀerent thresholds yet still
generated from a single distribution G which is itself conditional on a parameter α, thus
E[nG(θi|Xβ, α)] is the expected number of aﬃrmative outcomes. Suppose there were struc-
tures in the data such as unexplained clustering eﬀects, unit heterogeneity, autocorrelation,
or missingness that cast doubt on the notion of G as a single model. Note that this can
happen in a Bayesian or non-Bayesian setting, the diﬀerence being the distributional or de-
terministic interpretation of θ. The choice of G is unknown by the researcher but determined
by custom or intuition. We suggest, instead, a nonparametric Bayesian approach that draws
θ from a mixture of appropriate prior distributions conditional on data and parameters (in
this simple case a mixture of beta distributions according to BE(α(Xβ), Z −α(Xβ)) for
some prior parameter Z).
Gill and Casella (2007) apply this type of a prior to ordinal regression models. Such

The Bayesian Prior
135
outcomes occur frequently in the social sciences, especially in survey research where in-
struments such as Likert and Guttman scale responses are used. Gill and Casella look at
self-reported levels of stress for political executives in the United States where a ﬁve-point
ordinal scale is used. Start with data Y1, Y2, . . . , Yn assumed to be drawn from a mixture of
distributions denoted p(ψ) where the mixing over ψ is independent from distribution G, and
the prior on G, D is a mixture of Dirichlet processes. The ordered probit model assumes
ﬁrst that there is a multinomial selection process:
Yi ∼Multinomial(1, (p1, p2, . . . , pC)),
i = 1, . . . n
(4.44)
where 
j pj = 1, and Yi = (yi1, . . . , yiC) is a C × 1 vector with a 1 in one position and
0 elsewhere. The placement of the 1 denotes the class that the observation falls into. In
addition, the pj are ordered by the probit model
pj = p(θj−1 ≤Ui ≤θj)
(4.45)
where these cutpoints between categories have the property that −∞= θ0 < θ1 < · · · <
θC = ∞. Deﬁne now the random quantity:
Ui ∼N(Xiβ + ψi, σ2)
(4.46)
where Xi are covariates associated with the ith observation, β is the coeﬃcient vector, and
ψ denotes a random eﬀect to account for subject-speciﬁc deviation from the underlying
model. Here the Ui are unobservable random variables, and we could specify the model
without them, that is, from (4.45) and (4.46),
pj = Φ
θj −Xiβ −ψi
σ

−Φ
θj−1 −Xiβ −ψi
σ

.
(4.47)
If we do not want to require a particular structure or distribution on this random eﬀect we
can make our model semiparametric by assuming that ψ is an observation from a Dirichlet
Process,
ψi
∼
G
G
∼
DmGμ,τ2
(4.48)
where Gμ,τ 2 is a base measure and m is a concentration parameter. Thus, ψ is modeled
to come from a distribution that sits in a neighborhood of Gμ,τ 2, with the size of the
neighborhood being controlled by m. For now, we take m to be ﬁxed, but later we will let
it vary. In particular, with the mixture setup we take the prior on m to be in a discrete set
and G to have root density N(μ, τ 2).
It is necessary to stipulate a full set of priors for the other terms, and the following make
intuitive sense.
β
∼
N(β0, σ2
β)
μ
∼
N(0, dτ 2)
(4.49)
1
τ 2
∼
Gamma(a, b)

136
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
It is common to set σ2
β = ∞, resulting in a ﬂat prior on β. The parameters in the priors
on μ and τ 2 can be chosen to make the priors suﬃciently diﬀuse to allow the random eﬀect
to have an eﬀect. The choice of prior mean zero for ψ does not lose generality, as the Xiβ
term in (4.48) locates the distribution.
Estimation of this model requires MCMC tools, such as the Gibbs sampler. So ﬁrst
write:
ci
∼
Discrete(q1, . . . , qK)
ψci
∼
g(ψ) = N(μ, τ 2)
(4.50)
q
∼
Dirichlet(m/K, . . . , m/K),
where the ci serve only to group the ψi, resulting in a common value of ψi = ψj if ci = cj.
In the Gibbs sampler, the ci are generated conditionally:
c
=
(c1, c2, . . . , cn)
c−i
=
(c1, c2, . . . , ci−1, ci+1, . . . , cn)
n−i,ℓ
=
#(ci = ℓ)
f(yi|ψi)
=
pj see (4.45),
then, for i = 1, . . . , n
p(ci = ℓ|c−i) ∝
⎧
⎨
⎩
n−i,ℓ
n−1+mf(yi|ψi)
if n−i,ℓ> 0
m
n−1+mHi
if n−i,ℓ= 0
,
where
Hi =

f(yi|ψ)g(ψ)dψ.
This then sets up the full conditional distributions for Gibbs sampling (see page 25).
4.8
Bayesian Shrinkage
The prior distribution works to move the posterior away from the likelihood and toward
its own position. In cases where sharply deﬁned priors are speciﬁed in the form of distribu-
tions with small variance, Bayesian estimates will have lower variance than corresponding
classical likelihood-based estimates. Furthermore, the greater the correlation between coef-
ﬁcients in a given model, the greater the extent of the “shrinkage” toward the prior mean.
As we shall see in Chapter 12, it is often the case that models with hierarchical speciﬁcations
(multiple levels of priors) display more shrinkage due to correlations between parameters.

The Bayesian Prior
137
As a speciﬁc example of shrinkage from Hartigan (1983, p.88), consider the normal
model in Chapter 3 with prior mean m and variance s2 for μ and σ0 known. Rewrite the
posterior mean according to:
ˆμ =
m
s2 + n¯x
σ2
0
 -  1
s2 + n
σ2
0

,
=
σ2
0
σ2
0 + ns2 m +
ns2
σ2
0 + ns2 ¯x
= (Sf)m + (1 −Sf)¯x,
(4.51)
where Sf = σ2
0/(σ2
0 + ns2) is the shrinkage factor that is necessarily bounded by [0:1]. This
shows that the shrinkage factor gives the proportional distance that the posterior mean has
shrunk back to the prior mean away from the classical maximum likelihood estimate ¯x. The
form of the shrinkage factor in this case highlights the fact that large data variance means
that the denominator will dominate and there will be little shrinkage.
The posterior variance in the normal-normal model can also be rewritten in similar
fashion:
ˆσ2 =
 1
s2 + n
σ2
0
−1
=
σ2
0s2
σ2
0 + ns2 =
'
σ2
0
σ2
0 + ns2 σ2
0
( 1
2 '
ns2
σ2
0 + ns2
s2
n
( 1
2
= (Sfσ2
0)1/2

(1 −Sf)s2
n
1/2
.
(4.52)
This is interesting because it shows that the posterior variance is a product of the square
root of the prior variance weighted by the shrinkage factor and the square root of the
data variance weighted by the complement of the shrinkage factor. So we see speciﬁcally
how the shrinkage factor determines the compromise between prior uncertainty and data
uncertainty.
It is noteworthy to look at both (4.51) and (4.52) when the prior distribution is very
diﬀuse. In the extreme, if we pick an improper prior by setting s2 = ∞(or perhaps just some
huge value), then it is clear that Sf →0, and the likelihood model dominates. Conversely,
if s2 is close to zero, reﬂecting strong prior knowledge about the distribution of μ, then Sf
is close to one and the prior dominates.
Of course shrinkage is not just a feature of normal models or even models based on
location-scale distributions. Returning to the Beta-Binomial conjugate setup given in Ex-
ample 2.3.4:
Y ∼BN(n, p)
p ∼BE(A, B)
(4.53)
where A and B are ﬁxed prior parameters. The posterior distribution for p was shown to

138
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
be:
p|y ∼BE(y + A, n −y + B),
(4.54)
with:
ˆp =
(y + A)
(y + A) + (n −y + B) =
'
n
A + B + n
(  y
n

+
'
A + B
A + B + n
( 
A
A + B

.
(4.55)
Here
A+B
A+B+n is the shrinkage estimate where the degree of shrinkage is determined by the
magnitude of A + B relative to n. So for large n the shrinkage gets small since n is in the
denominator of the weight on the prior mean. While the shrinkage is overt in the example
here, we can also consider the eﬀect for the more complicated priors forms described in this
chapter.
4.9
Exercises
4.1
Show that both the exponential PDF and chi-square PDF are special cases of the
gamma PDF.
4.2
Suppose you have an iid sample of size n from the Rayleigh distribution:
f(x|σ) = x
σ2 exp
'
−1
2σ2 x2
(
,
x ≥0, σ > 0.
Produce the likelihood function for this distribution and assign an appropriate prior
distribution for σ2.
4.3
(Pearson 1920). An event has occurred p times in n trials. What is the probability
that it occurs r times in the next m trials? How would your answer be interpreted
diﬀerently in Bayesian terms than in standard frequentist terms?
4.4
Demonstrate that an improper prior distribution is a measure but not a probability
measure.
4.5
Suppose that you had a prior parameter with the restriction: [0 < η < 1]. If you
believed that η had prior mean 0.4 and variance 0.1, and wanted to specify a beta
distribution, what prior parameters would you assign?
4.6
Suppose X1, . . . , Xn are iid G(α, β), Y1, . . . , Yn are iid G(α, γ) and independent of
the Xi. Produce the distribution of ¯X/ ¯Y .
4.7
Derive the Jeﬀrey’s prior for a normal likelihood model under three circumstances:
(1) σ2 = 1, (2) μ = 1, and (3) both μ and σ2 unknown (nonindependent).

The Bayesian Prior
139
4.8
For X ∼N(μ, 1), show that the prior distribution μ ∼N(m, s2) gives the posterior
distribution:
μ|X ∼N

1
1 + s2 m +
s2
1 + s2 X,
s2
1 + s2

.
.
4.9
(Robert 2001) Calculate the marginal posterior distributions for the following se-
tups:
▷x|σ2 ∼N(0, σ2), 1/σ2 ∼G(1, 2).
▷x|λ ∼P(λ), λ ∼G(2, 1).
▷x|p ∼NB(10, p), p ∼BE(0.5, 0.5).
4.10
The Haldane prior for parameter ζ ∈(0 : 1) is given by p(ζ) ∝ζ−1(1 −ζ)−1.
Suppose we had data X1, . . . , Xn Bernoulli distributed according to BR(x|ζ) =
ζx(1−ζ)1−x. Show that the resulting posterior distribution is π(ζ|X) ∝ζ
 xi−1(1−
ζ)n− xi−1.
What is the resulting posterior mean?
Can the Haldane prior be
expressed as a special case of a beta distribution? What happens if there are no
zeros or no ones in the X data?
4.11
Calculate the Jeﬀreys prior for the distributional forms in Exercise 4.9.
4.12
The triangular distribution for θ is given with limits (a, b) and mode m by the
expression:
p(θ|a, b, m)
⎧
⎪
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎪
⎩
0
θ < a
2(θ−a)
(b−a)(m−a)
a ≤θ ≤m
2(b−θ)
(b−a)(b−m)
m ≤θ ≤b
0
θ > b.
Find the mean and variance of this distribution. Produce a posterior distribution
for θ assuming data that are distributed N(θ, 1).
4.13
The Bayesian framework is easily adapted to problems in time-series. One of the
most simple time-series speciﬁcations is the AR(1), which assumes that the previous
period’s outcome is important in the current period estimation.
Given an observed outcome variable vector, Yt measured at time t, AR(1) model
for T periods is:
Yt = Xtβ + ϵt
ϵt = ρϵt−1 + ut,
|ρ| < 1
ut ∼iid N(0, σ2
u).
Here Xt is a matrix of explanatory variables at time t, ϵt and ϵt−1 are residuals

140
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
from period t and t −1, respectively, ut is an additional zero-mean error term
for the autoregressive relationship, and β, ρ, σ2
u are unknown parameters to be
estimated by the model. Backward substitution through time to arbitrary period s
gives ϵt = ρsϵt−s + T −1
j=0 ρjut−j, and since E[ϵt] = 0, then var[ϵt] = E[ϵ2
t] =
σ2
u
1−ρ2 ,
and the covariance between any two errors is: Cov[ϵt, ϵt−j] =
ρjσ2
u
1−ρ2 .
Assuming
asymptotic normality, this setup leads to a general linear model with the following
tridiagonal T × T weight matrix (Amemiya 1985, p.164):
Ω = 1
σ2u
⎡
⎢⎢⎢⎢⎢⎢⎢⎣
1
−ρ
−ρ
(1 + ρ2)
−ρ
0
...
0
−ρ
(1 + ρ2)
−ρ
−ρ
1
⎤
⎥⎥⎥⎥⎥⎥⎥⎦
.
Using the following data on worldwide fatalities from terrorism compared to some
other causes per 100,000 (source: Falkenrath 2001), develop posteriors for β, ρ, and
σu for each of the causes (Y ) as a separate model stipulating the date (minus 1983)
as the explanatory variable (X). Specify conjugate priors (Berger and Yang [1994]
show some diﬃculties with nonconjugate priors here), or use a truncated normal for
the prior on ρ (see Chib and Greenberg [1998]).
Can you reach some conclusion
about the diﬀerences in these four processes?
Year
Terrorism
Car Accidents
Suicide
Murder
1983
0.116
14.900
12.100
8.300
1984
0.005
15.390
12.420
7.900
1985
0.016
15.150
12.380
8.000
1986
0.005
15.920
12.870
8.600
1987
0.003
19.106
12.710
8.300
1988
0.078
19.218
12.440
8.400
1989
0.006
18.429
12.250
8.700
1990
0.004
18.800
11.500
9.400
1991
0.003
17.300
11.400
9.800
1992
0.001
16.100
11.100
9.300
1993
0.002
16.300
11.300
9.500
1994
0.002
16.300
11.200
9.000
1995
0.005
16.500
11.200
8.200
1996
0.009
16.500
10.800
7.400
4.14
The number of cases needed to give a 1 −α conﬁdence interval of width ω for
a proportion p is n = (zα)2p(1 −p)/ω2. Plot n versus ω at α = 0.05 for p =
(0.01, 0.3, 0.5, 0.8).
4.15
Laplace (1774) wonders what the best choice is for a posterior point estimate. He

The Bayesian Prior
141
sets three conditions for the shape of the posterior: symmetry, asymptotes, and
properness (integrating to one). In addition, Laplace tacitly uses uniform priors.
He proposes two possible criteria for selecting the estimate:
▷La primi´ere est l’instant tel qu’en ´egalement probable que le v´eritable instant
du ph´enom`ene tombe avant ou apr`es; on pourrait appeler cet instant milieu
de probabilit´e.
Meaning: use the median.
▷Le seconde est l’instant tel qu’en le prenant pour milieu, la somme des erreurs
`a craindre, multipli´ees par leur probabilit´e, soit un minimum; on pourrait
l’appeler milieu d’erreur ou milieu astronomique, comm ´etant celui auquel les
astronomes doivent s’arreter de pr´ef´erence.
Meaning: use the quantity at the “astronomical center of mass” that mini-
mizes: f(x) =

|x −V |f(x)dx. In modern terms, this is equivalent to min-
imizing the posterior expected loss: E[L(θ, d)|x] =

Θ L(θ, d)π(θ|x)dθ, which
is the average loss deﬁned by the posterior distribution and d is the “decision”
to use the posterior estimate of θ (see Berger [1985]).
Prove that these two criteria lead to the same point estimate.
4.16
In their popular text Gelman and Hill (2007) often specify variance components
priors according to the following two steps:
σ ∼U(0, 100)
τ = σ−2,
where σ is a standard error and τ is a precision. Explain why this might not be a
good idea for probit regression models.
4.17
Review one body of literature in your area of interest and develop a detailed plan
for creating elicited priors.
4.18
Kadane and Wolfson (1996) specify a normal linear model for elicitation according
to:
Y|X, β, σ2 ∼N(Xβ, σ2I)
β ∼N(b, σ2R−1)
1
σ2 ∼1
wδ G(δ/2, 2).
Develop a program to elicit scalars b, w, δ, and matrix R using a series of quantile
questions.
4.19
Test a Bayesian count model for the number of times that capital punishment is

142
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
implemented on a state level in the United States for the year 1997. Included in
the data below (source: United States Census Bureau, United States Department
of Justice) are explanatory variables for: median per capita income in dollars, the
percent of the population classiﬁed as living in poverty, the percent of Black citizens
in the population, the rate of violent crimes per 100,000 residents for the year before
(i.e., 1996), a dummy variable to indicate whether the state is in the South, and
the proportion of the population with a college degree of some kind. The data are
given below and available in the BaM dataset executions.
In 1997, executions were carried out in 17 states with a national total of 74. The
model should be developed from the Poisson link function, θ = log(μ), with the
objective of ﬁnding the best β vector in:
g−1(θ)
 !" #
17×1
= exp
'
1β0 + INCβ1 + POVβ2 + BLKβ3 + CRIβ4 + SOUβ5 + DEGβ6
(
.
Specify a suitable prior with assigned prior parameters, then summarize the result-
ing posterior distribution.
Exe-
Median
Percent
Percent
Violent
Prop.
State
cutions
Income
Poverty
Black
Crime
South Degrees
Texas
37
34453
16.7
12.2
644
1
0.16
Virginia
9
41534
12.5
20.0
351
1
0.27
Missouri
6
35802
10.6
11.2
591
0
0.21
Arkansas
4
26954
18.4
16.1
524
1
0.16
Alabama
3
31468
14.8
25.9
565
1
0.19
Arizona
2
32552
18.8
3.5
632
0
0.25
Illinois
2
40873
11.6
15.3
886
0
0.25
S. Carolina
2
34861
13.1
30.1
997
1
0.21
Colorado
1
42562
9.4
4.3
405
0
0.31
Florida
1
31900
14.3
15.4
1051
1
0.24
Indiana
1
37421
8.2
8.2
537
0
0.19
Kentucky
1
33305
16.4
7.2
321
0
0.16
Louisiana
1
32108
18.4
32.1
929
1
0.18
Maryland
1
45844
9.3
27.4
931
0
0.29
Nebraska
1
34743
10.0
4.0
435
0
0.24
Oklahoma
1
29709
15.2
7.7
597
0
0.21
Oregon
1
36777
11.7
1.8
463
0
0.25
4.20
Zellner’s g-prior can be expressed most simply in a linear regression context as:
β|τ ∼N

B, g
τ (X′X)−1

p(τ) ∝1
φ,
where B is the prior mean, and τ is the usual homoscedastic precision parameter,
meaning that the covariance matrix of the β is given by I/τ. Common choices for
g include g = n (the data size), g = k2 (the square of the number of explanatory

The Bayesian Prior
143
variables in the model), and g = max(n, k2). Rerun the model in Exercise 5.7 with
this prior using your preferred value of g. How do the results diﬀer?


Chapter 5
The Bayesian Linear Model
5.1
The Basic Regression Model
This chapter develops the Bayesian linear regression model with diﬀering priors and
assumptions. We will consider both informed and uninformed prior speciﬁcations as well
as look at the common problem of heteroscedasticity. Detailed technical expositions of the
Bayesian linear regression model are found in the classic article by Lindley and Smith (1972)
with discussion, the follow-up article with generalizations by Smith (1973), Geweke’s (1993)
exposition on t-distributed errors, and the early work by Tiao and Zellner (1964b). Else-
where Zellner and Tiao (1964) provide estimation techniques for general error models, Davis
(1978) considers the Bayesian general linear model with inequality constraints, and Pollard
(1986) gives a helpful chapter on Bayesian linear forms. A modern and very comprehensive
volume on linear model theory is given by Ravishanker and Dey (2002).
The ﬁrst treatment presented here assumes homoscedasticity. A discussion of unequal
variances for the Bayesian linear model follows, and additional discussions can be found in
Leonard (1975) and Boscardin and Gelman (1996). Le Cam (1986, Chapter 13) gives a de-
tailed asymptotic analysis, and more elementary treatments of heteroscedastic linear models
can be found in Goldberger (1964, p.235), Huang (1970, p.147), and Rao and Toutenburg
(1995, p.101). A detailed theoretical discussion of linear model heteroscedasticity is given in
Amemiya (1985, Section 6.5), and the corresponding application of the jackknife and boot-
strap are outlined in Shao and Tu (1995, Chapter 7). See also Fomby, Hill, and Johnson
(1980).
Start with the well-known basic multiple linear regression model, described in Ap-
pendix A, conforming to the Gauss-Markov assumptions. Deﬁne the terms conventionally:
y = Xβ + ϵ,
(5.1)
where X is an n × k, rank k matrix of explanatory variables with a leading vector of ones
for the constant, β is a k × 1 vector of coeﬃcients to be estimated, y is an n × 1 vector
of outcome variable values, and ϵ is a n × 1 vector of errors distributed N(0, σ2I) for a
constant σ2. The likelihood function for a sample of size n is:
L(β, σ2|X, y) = (2πσ2)−n
2 exp
'
−1
2σ2 (y −Xβ)′(y −Xβ)
(
.
(5.2)
145

146
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
So far we have the standard non-Bayesian approach to linear modeling, and once the data,
[X, y], are observed, this likelihood function (or rather its log) is maximized relative to the
unknown parameter vector β and the unknown scalar σ. We know the values for which
(5.2) is at its maximum from standard likelihood theory (bias corrected for σ2) and ordinary
least squares principles:
ˆb = (X′X)−1X′y,
ˆσ2 = (y −Xˆb)′(y −Xˆb)
(n −k)
,
(5.3)
and we can therefore plug these values into (5.2) and process according to:
L(β,σ2|X, y) ∝σ−n exp
'
−1
2σ2 (y′y −2β′X′y + β′X′Xβ)
(
= σ−n exp
'
−1
2σ2 (y′y −2β′X′y + β′X′Xβ
−2((X′X)−1X′y)′X′y + 2((X′X)−1X′y)′X′X((X′X)−1X′y))
 
!"
#
sums to zero
(
= σ−n exp

−1
2σ2 ((y −Xˆb)′(y −Xˆb)
+ ˆb′X′Xˆb + β′X′Xβ −2β′X′Xˆb)

= σ−n exp
'
−1
2σ2 (ˆσ2(n −k) + (β −ˆb)′X′X(β −ˆb))
(
.
(5.4)
The “trick” used here is completing the square after inserting a quantity that is simultane-
ously subtracted and added (therefore adding zero), and using the property X′X(X′X)−1 =
I to rearrange terms. The other mildly subtle point is the replacement of −2β′X′y with
−2β′X′Xˆb from the normal equation (X′Xˆb = X′y).
Thus the result is a likelihood function expressed in terms of unknown coeﬃcients to
be estimated and observable data (Zellner 1976, p.401).
This is the starting point for
the Bayesian analysis of the standard linear model and we now begin to quantify existing
knowledge about the mean vector and the variance parameter (a scalar due to the ho-
moscedasticity assumption). The Bayesian complement to maximum likelihood estimation
not only provides for prior information, but also easily incorporates: linear additive spec-
iﬁcations of nonlinear functions (Bernardo and Smith 1994, pp.221-222; Leonard and Hsu
1999, Section 5.2), linear inequality constraints (Davis 1978), analysis of variance of means
(Hartigan 1983, Chapter 9), polynomial regression (Halpern 1973; Lempers 1971), small
area estimation (Datta and Ghosh 1991), and time-series models (Ferreira and Gamerman
2000; Kitagawa and Gersch 1996; Mar´ın 2000; Pole, West, and Harrison 1994), among other
extensions.

The Bayesian Linear Model
147
5.1.1
Uninformative Priors for the Linear Model
Deﬁne the improper uninformed priors p(β) ∝c and p(σ2) = 1
σ over the support [−∞:
∞] and [0:∞], respectively (Tiao and Zellner 1964a, p.220). Note that we are assuming
independence between β and σ2. Therefore the joint posterior from the likelihood function
(5.4) is provided by:
π(β, σ2|X, y) ∝L(β, σ2|X, y)p(β)p(σ2)
∝σ−n−1 exp
'
−1
2σ2 (ˆσ2(n −k) + (β −ˆb)′X′X(β −ˆb))
(
.
(5.5)
Note that the constant c drops out with proportionality.
To obtain the desired marginal distribution of β, ﬁrst make the transformation: s = σ−2,
with a required Jacobian: J = | d
dsσ| = | d
dss−1
2 | = 1
2s−3
2 . Reexpressing (5.5) in terms of s
gives:
π(β, s|X, y)
∝(s−1
2 )−n−1 exp
'
−1
2s(ˆσ2(n −k) + (β −ˆb)′X′X(β −ˆb))
( 1
2s−3
2

∝s
n
2 −1 exp
'
−1
2s(ˆσ2(n −k) + (β −ˆb)′X′X(β −ˆb))
(
.
(5.6)
Now integrate with respect to s to get the marginal for β:
π(β|X, y) =
 ∞
0
s
n
2 −1 exp
'
−1
2s(ˆσ2(n −k) + (β −ˆb)′X′X(β −ˆb))
(
ds.
This integral is quite easy to calculate when one notices that inside the integral is a gamma
PDF kernel, with the integration performed over the appropriate support.
So use the
following substitution from a gamma PDF:
1 =
 ∞
0
qp+1
Γ(p + 1)spe−qsds,
Γ(p + 1)
qp+1
=
 ∞
0
spe−qsds.
Setting p = n
2 −1 and q = 1
2(ˆσ2(n −k) + (β −ˆb)′X′X(β −ˆb)), and deﬁning the degrees of
freedom as ν = n −k means that:
π(β|p, q) ∝q−(p+1) = q−n
2 ,
(5.7)
which can then be resubstituted back to get:
π(β|X, y) ∝

(n −k) + (β −ˆb)′ˆσ−2X′X(β −ˆb)
−n
2 .
(5.8)
It is easy to recognize π(β|X, y) as the kernel of a multivariate-t distribution for β −ˆb,
provided that the covariance matrix, R = (n−k)ˆσ2(X′X)−1
n−k−2
, is positive deﬁnite (Box and Tiao
1973, p.440; Press 1989, p.135; Tong 1990, Chapter 9). Thus E[ˆb] = β, and the covariance
between any two coeﬃcients is given by the elements of the R matrix: Cov[ti, tj] = Rij,
where ˆσ2 is deﬁned by (5.3).

148
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
Obtaining the marginal distribution for σ2 is considerably less involved due to an obvious
shortcut. Start with the deﬁning integral and separate terms in the exponent:
π(σ|X, y) ∝
 ∞
−∞
σ−n−1 exp
'
−1
2σ2 (ˆσ2(n −k) + (β −ˆb)′X′X(β −ˆb))
(
dβ.
= σ−n−1 exp
'
−1
2σ2 ˆσ2(n −k)
(
×
 ∞
−∞
exp
'
−1
2σ2 (β −ˆb)′X′X(β −ˆb)
(
dβ.
The second exponential term is a k-dimensional kernel of a multivariate normal distribution
providing the following substitution and simpliﬁcation:
π(σ|X, y) ∝σ−n−1 exp
'
−1
2σ2 ˆσ2(n −k)
(
(2πσ2)
k
2
∝(σ2)−1
2 (n−k)−1
2 exp
'
−1
2σ2 ˆσ2(n −k)
(
.
(5.9)
It is not immediately obvious, but the posterior distribution of σ is the kernel of an inverse-
gamma distribution (Appendix B), and can also be parameterized as an inverse Wishart
distribution (Tiao and Zellner 1964b).
To see how this is really an inverse gamma form,
apply the simple transformations: α = 1
2(n −k −1), β = 1
2 ˆσ2(n −k), so that 5.9 is now:
π(σ2|α, β) ∝(σ2)−(α+1) exp[−β/σ2].
(5.10)
So far (5.10) is exactly the same inferential result that we would expect from a standard
likelihood analysis of the linear model The maximum likelihood solution, which is also
equivalent to minimizing the summed squared errors, is equivalent to a Bayesian solution
in which improper uniform priors over the entire support of the unknown parameters are
speciﬁed. Thus, the omnipresent non-Bayesian approach to linear modeling is a special case
of a Bayesian model.
■Example 5.1:
The 2000 U.S. Election in Palm Beach County. The 2000 U.S.
election for president was marked by considerable controversy concerning the casting
of ballots in the state of Florida. Because the election was so tightly contested in
Florida and the state’s 25 electoral college delegates were the ﬁnal determining fac-
tor for electing the president in the contest between Al Gore and George W. Bush,
various problems and aberrations in the voting process became magniﬁed in impor-
tance.
There was considerable evidence that the ﬁnal certiﬁed outcome declaring
Bush the winner by 537 votes (out of approximately 6 million) was shaped by tech-
nical problems with the voting apparatus, ballot confusion by voters, and outright
discrimination against minority voters.
At the nexus of this controversy is Palm Beach County, a liberal-leaning, upper-middle
class area with a considerable number of northeastern retirees where the far-right con-
servative candidate Pat Buchanan did suspiciously well. The data here (available in

The Bayesian Linear Model
149
the BaM package associated with this text) consist of all 516 reporting precincts in
Palm Beach County collected by the Palm Beach Post from state and federal report-
ing sources. The variables include party aﬃliation percentages, racial demograph-
ics, registration status, voting technology, and the outcome variable of interest: the
number of spoiled ballots. Ballots are spoiled if the voter designates more than one
presidential selection or marks the ballot in some other inappropriate way. Therefore
this variable does not count so-called under-votes wherein the voter does not select
any presidential candidate. For the purpose of this example, the variables described
in Table 5.1 are included in the linear speciﬁcation.
TABLE 5.1:
PBC Data Variable Definitions
Bad Ballots
Total Number of Spoiled Ballots
Technology
0 for a datapunch machine (butterﬂy ballot),
1 for votomatic
New
Number of “new” voters
(have not voted in the precinct for previous 6 years)
Size
Total number of precinct voters
Republican
Number of voters registered as Republican
White
Number of white (nonminority) voters
First we specify an uninformed joint prior, p(β, σ2) = 1/σ, and calculate the resulting
posterior density according to Section 5.1.1. This joint prior comes from the simulta-
neous determination of the improper forms: p(β, σ2) = p(β|σ2)p(σ2). The resulting
posterior is summarized in Table 5.2 where the posterior summaries, posterior mo-
ments, and quantiles for the coeﬃcient estimates were produced directly from the
analytical t-distribution result. Note that the mean and median are identical for the
coeﬃcient posteriors indicating symmetry.
Section 5.1.1 also demonstrated that the posterior distribution of σ2 is an inverse-
gamma speciﬁcation as given by (5.9), and we can analytically determine the quantiles
of interest with α = (n −k −1)/2, β = 1
2 ˆσ2(n −k) inserted into the inverse-gamma
PDF (see Appendix B). However, it turns out to be much easier, and nearly equally
accurate, to generate a large number of values of σ2 and use the empirical quan-
tiles. The R code for all estimates in Table 5.2 is included in the Computational
Addendum for this chapter.
Some comments are necessary regarding the presentation of Table 5.2.
A strictly
canonical Bayesian may object to the traditional non-Bayesian reporting of the mean
and standard error in the left-hand side and want more than three quantile summaries
on the right-hand side. In a conventional linear regression table we would expect to see
t-statistics, p-values, and “stars” instead of these quantiles. Most Bayesians publishing
in the social sciences see the usefulness of giving means and standard errors to comfort
reviewers and readers who may not have had substantial exposure to Bayesian results.

150
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
But, these same researchers would certainly object to t-statistics since this implies a
test that is not undertaken, to p-values since there is no null distribution with which to
deﬁne them, and to “stars” since this is an idiotic practice. Thus Table 5.2 represents a
compromise between accessibility and the Bayesian preference for purely distributional
summaries.
TABLE 5.2:
Posterior: Linear Model,
Uninformative Priors
β Covariate
Mean
Std. Error
0.025 Q.
Median
0.975 Q.
Intercept
107.351
7.759
92.108
107.351
122.593
Technology
-50.529
3.492
-57.389
-50.529
-43.670
New
-0.353
0.038
-0.427
-0.353
-0.278
Size
0.149
0.006
0.137
0.149
0.161
Republican
-0.084
0.007
-0.098
-0.084
-0.069
White
-0.048
0.006
-0.059
-0.048
-0.037
σ
22.003
0.698
20.695
21.983
23.434
What we see from the analysis in Table 5.2 is that the linear model with an uninformed
prior for β and σ2 provides an apparently very good ﬁt to the data. Each of the
marginal posterior distributions is narrow and easily statistically distinguishable from
zero. Since this is still a linear model, the interpretation of these coeﬃcients is very
direct:
▷Precincts with votomatic technology have an expected 50 less spoiled ballots.
▷For every 2.5 new voters, there is an expected 1 less spoiled ballot.
▷For each additional voter that turns out, there is an additional 0.15 expected increase in spoiled
ballots: in other words, a turnout addition of 100 provides an expected increase in spoiled
ballots of 15. This includes the new voters above; so in fact, this eﬀect is really greater than
this coeﬃcient indicates since the new voters are actually suppressing this eﬀect somewhat.
▷For each additional Republican voter there is about a 9% decrease in expected spoiled ballots.
That is, we expect about 1 less spoiled ballot for every 11 increased Republican voters.
▷For each additional white voter, there is a 5% decrease in expected spoiled ballots. That means
about 1 less spoiled ballot for every 20 increased white voters.
It is also important to remember that while the reported mean and standard error
can be identical between those produced with Bayesian methods and ordinary least
squares (equivalently maximum likelihood here), they come from fundamentally dif-
ferent processes and assumptions (the OECD example on page 381 gives identical
coeﬃcient estimates even though one was produced with MCMC tools and the other
with minimizing squared residuals). A posterior mean is a point estimate that comes
from an underlying distributional assumption about posterior quantities and the OLS
point estimate comes from an assumption of a ﬁxed underlying quantity set by na-
ture. Furthermore, a posterior standard error also arises from the belief that unknowns

The Bayesian Linear Model
151
should be described with distributions, whereas a maximum likelihood estimate-based
standard error comes from the curvature around the mode of the likelihood function
under the assumption that the mode is the optimal estimator. Obviously these values
basically agree in the presence of uninﬂuential prior distributions, or large samples,
but they do not have to.
5.1.2
Conjugate Priors for the Linear Model
We can also stipulate conjugate priors according to the principles outlined in Chapter 3.
The linear regression model developed in this section is actually just a multivariate gen-
eralization of the normal model in that chapter, where the maximum likelihood estimates
from the linear parametric likelihood speciﬁcation are used. The well-known conjugate prior
distributions are: multivariate normal for the mean vector β and inverse gamma for σ2.
We can therefore proceed in essentially the same manner as in the basic normal model, but
incorporating the likelihood function from the linear regression setup.
In this case of conjugacy we also have a dependency of β on σ2 as in the simple normal-
normal model in Chapter 3:
p(β|σ2) = (2π)−k
2 |Σ|−1
2 exp
'
−1
2(β −B)′Σ−1(β −B)
(
,
and:
p(σ2) ∝σ−(a−k) exp
'
−b
σ2
(
.
(5.11)
This aﬀects the joint posterior as a simple joint prior since:
p(β, σ2) = p(β|σ2)p(σ2).
(5.12)
However, the dependency in (5.11) will ﬂow through to the posterior of β. Here Σ = σ2I
by assumption (this form is adaptable but not only makes the operations conformable,
it allows us to easily transition to the general linear model later). Thus the prior for β
conditional on σ2 is a multivariate normal with mean B and variance σ2. The prior for σ2
is an inverse gamma kernel with parameters a and b (employing the form from page 73, see
also Appendix B). Using such notation, assign α −1 = a −k, and β = b since these are free
prior parameters. The inverse gamma prior for σ2 is not only the conjugate prior, but also
the marginal posterior from the uninformed prior model. So to summarize the notation, we

152
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
now have:
n × k
size of the X matrix
β
the unknown linear model coeﬃcient vector
B
prior mean vector for β
σ2
prior variance for β, collected in the diagonal matrix Σ
ˆb
(X′X)−1X′y
ˆσ2
(y −Xˆb)′(y −Xˆb)
(n −k)
a −k
assigned ﬁrst parameter for σ2 prior
b
assigned second parameter for σ2 prior.
Combining the data likelihood from (5.4) with the prior speciﬁcations from (5.11) and
applying Bayes’ Law gives the joint posterior:
π(β,σ2|X, y)
∝σ−n exp
'
−1
2σ2 (ˆσ2(n −k) + (β −ˆb)′X′X(β −ˆb))
(
× (2π)−k
2 |Σ|−1
2 exp
'
−1
2(β −B)′Σ−1(β −B)
(
σ−(a−k) exp
'
−b
σ2
(
∝σ−n−a exp
'
−1
2σ2

ˆσ2(n −k) + (β −ˆb)′X′X(β −ˆb) + 2b + (β −B)′Σ−1(β −B)
(
,
(5.13)
where σ2 moves out of |Σ| from the determinant operation. This form can be simpliﬁed
by coweighting the precisions and re-expressing as a Gaussian kernel. First deﬁne (Zellner
1971, Chapter 3):
˜β = (Σ−1 + X′X)−1(Σ−1B + X′Xˆb)
(5.14)
˜s = 2b + ˆσ2(n −k) + (B −˜β)′Σ−1B + (ˆb −˜β)′X′Xˆb.
(5.15)
The initial form of the joint posterior can now be re-expressed (Exercise 5.9) as:
π(β, σ2|X, y) ∝(σ2)−n+a
2
exp
'
−1
2σ2

˜s + (β −˜β)′(Σ−1 + X′X)(β −˜β)
(
.
(5.16)
The advantage of this new form is that it immediately allows us to use the same marginal-
ization trick as we did with the posterior from the uninformed priors to get the distribution
of β|X, y. This produces:
π(β|X, y) ∝

˜s + (β −˜β)′(Σ−1 + X′X)(β −˜β)
−n+a
2
+1
,
(5.17)
which is the kernel of a multivariate-t distribution with ν = n+a−k −2 degrees of freedom
(Bauwens, Lubrano, and Richard 1999, Section 2.7, Zellner 1971, Section 3.2, Box and Tiao

The Bayesian Linear Model
153
1973, Section 8.4). Therefore the mean and variance of the parameter estimates are given
by:
E(β|X, y) = ˜β,
Cov(β|X, y) = ˜s(Σ−1 + X′X)−1
n + a −k −3
.
(5.18)
The marginal distribution of σ2 is also produced in similar fashion to the uninformed prior
derivation from before: π(σ2|X, y) ∝σ−n−a+k−1 exp
	
−1
2σ2 ˆσ2(n + a −k)

.
This is the
kernel of an IG(n + a −k, 1
2 ˆσ2(n + a −k)) distribution. For details, see Note B of Zellner
(1971).
We can now compare the informed conjugate model with the uninformed model devel-
oped previously. There are some interesting similarities as well as diﬀerences in Table 5.3.
Note ﬁrst that the parametric form for the marginal posteriors comes from the same family
but with important diﬀerences in the parameters. Speciﬁcally, conjugacy provides a in each
of the possible places. Since the researcher controls a ∈[0:∞), then there is temptation
to say that this is manipulable to provide customary levels of statistical reliability. This
is most easily seen in the posterior distribution of β|σ2 where larger values of a shrink the
tails of the Student’s-t toward that of the normal producing smaller posterior coeﬃcient
variance. What prevents abuse of this term is the convention that authors overtly state
their parameter values and the substantive reasons behind them.
TABLE 5.3:
Linear Regression Model, Prior Comparison
Setup
Prior
Posterior
Conjugate
β|σ2 ∼N(B, σ2)
β|X ∼t(n + a −k −2)
σ2 ∼IG(a, b)
σ2|X ∼IG(n + a −k, 1
2 ˆσ2(n + a −k))
Uninformative
β ∝c over [−∞:∞]
β|X ∼t(n −k)
σ2 = 1
σ over [0:∞]
σ2|X ∼IG( 1
2(n −k −1), 1
2 ˆσ2(n −k))
The role of sample size is also highlighted in Table 5.3. For the t-distribution posteriors,
increases in n will obviously push this distribution towards the normal. Since most models
in the social sciences have a relatively modest number of covariates (compared to statistical
genetics or some ﬁelds in engineering), k will have little eﬀect for large sample sizes in
this transition. The role of n in the inverse gamma forms is also clear from the form of
p(σ2) in (5.11) since both are negative exponents. So asymptotically the conjugate and the
uninformative linear regression models both converge to conventional large sample results
with normally distributed coeﬃcients and ﬁxed variance.

154
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
5.1.3
Conjugate Caveats for the Cautious and Careful
Some caveats are warranted about the use of conjugate priors in the Bayesian linear
regression model as described here. Conjugate priors with exponential family distributions
typically provide linear posterior expectations (Berger 1984, Diaconis and Ylvisaker 1979,
Ghosh 1969, Goel and DeGroot 1980) which are highly nonrobust to inﬂuential outliers
(West 1984), although these outliers can be explicitly accounted for in a Bayesian context
(Box and Tiao 1968). In the absence of credible normal assumptions about the prior, due to
small samples or less well-behaved error structures, linear expectations and linear variance
estimates (estimating the mean squared error) can be substituted (Hartigan 1969), but only
by averaging over both the data and the parameters. Because the posterior distribution
inherits its tail structure from the prior in conjugate speciﬁcations, robustness is often
diﬃcult to incorporate (Anscombe 1963; Dickey 1974; Hill 1974; Lindley 1968; Rubin 1977).
Geweke (1993) shows that when the model residuals are shown to be noticeably non-normal,
more complex models are required (mixtures, distributional assignment for the degrees of
freedom parameter) and these sometimes require Bayesian stochastic simulation techniques
(see later chapters).
The normal-normal conjugacy setup in Chapter 3 required that the prior and posterior
for the mean be conditional on the variance. This was a mathematical requirement for
the parameter form to ﬂow through the likelihood function, but it seems to water down
the spirit of conjugacy.
For this reason it is sometimes called “pseudo-conjugacy” (or
sometimes “semi-conjugacy”), which is a term that some authors dislike. This requirement
on the mean also applied to the linear regression models in this chapter, and was no weaker
in this case. Here, we have actually relaxed pure conjugacy even further since the normal
prior for the mean produces a Student’s-t posterior. However, unless the sample size, n,
and the set parameter, a, are set relatively low compared to the number of covariates, k,
the distribution is likely to approach normality anyway.
■Example 5.2:
The 2000 U.S. Election in Palm Beach County, Continued.
Now we can impose a conjugate prior as explained in Section 5.1.2 on these data
and re-perform this analysis. Suppose we now specify a “pessimistic” prior on the
β coeﬃcients with the following multivariate normal distribution and diﬀuse inverse-
gamma distribution:
β ∼N(B, Σ)
where:
B = [0, 0, 0, 0, 0, 0], Σ = diagonal6×6(2)
σ2 ∼IG(A, B)
where:
A = 3, B = 9.
and the notation “diagonal6×6(2)” indicates a six-by-six square matrix with 2’s on the
diagonal and zeros elsewhere. The hyperparameters A = 3, B = 9 on σ2 were chosen
to provide a diﬀuse and therefore relatively uninﬂuential form of the prior distribution.

The Bayesian Linear Model
155
Later we will use better prior distributions for σ2 based on folded distributions (i.e.,
normals, or Student’s-t, excluding negative values), but these forms are not conjugate
and require more elaborate estimation procedures. The speciﬁcation above produces
the model results summarized in Table 5.4 with posterior moments and quantiles.
TABLE 5.4:
Posterior: Linear Model, Conjugate
Priors
β Covariate
Mean
Std. Error
0.025 Q.
Median
0.975 Q.
Intercept
96.337
7.347
81.904
96.337
110.770
Technology
-46.635
3.327
-53.171
-46.635
-40.099
New
-0.378
0.040
-0.456
-0.378
-0.300
Size
0.155
0.006
0.143
0.155
0.167
Republican
-0.085
0.007
-0.099
-0.085
-0.070
White
-0.049
0.006
-0.060
-0.049
-0.038
σ
15.577
0.343
14.925
15.569
16.273
From this reanalysis, we can see that it does not make a dramatic diﬀerence whether an
uninformed or a conjugate prior is speciﬁed (at least with these assigned parameters).
While there are slight diﬀerences, it seems to be of little substantive concern. This
is partly due to the sample size of course (n = 516 precincts), meaning that the
likelihood dominates our choice of prior here.
5.2
Posterior Predictive Distribution for the Data
We will explore the topic in greater detail in Chapter 7, but it is interesting at this
point to derive the marginal distribution of future draws of the data from the Bayesian
linear model. This is the predictive distribution of the data assumed to be generated by
the model implied by the posterior distribution of the parameters. Thus we can compare
the distribution of the data from this prediction with the actual data where large observed
diﬀerences may indicate poor model ﬁt.
Consider predictive data, a vector ˜y of length k < n−2, generated from inserting actual
or hypothetical covariate values into the q × k matrix ˜X. Instead of a dataset of size n, we
create our own set of explanatory values of size q. The q ×1 vector of actual predictive data
is assumed to be produced from the linear model,
˜y = ˜Xβ + ϵ,
(5.19)
but interest lies instead in the posterior distribution of the ˜y unconditional on parameters:

156
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
π(˜y|y, ˜X, X).
This means that we are interested in linear predictions that result from
constructing a design matrix of covariate values and multiplying this by the estimated
coeﬃcients from the original model with this linear form, but we want a distribution for
this prediction that incorporates all sources of variance, including that of the coeﬃcient
posteriors. To get this unconditional distribution, start with the relation:
π(˜y, β, σ2| ˜X, X, y) = π(˜y|β, σ2, ˜X)π(β, σ2|X, y),
(5.20)
where the ﬁrst term on the right-hand side is just the normal PDF of new data, given
parameterization,
π(˜y|β, σ2, ˜X) ∝
1
(σ2)q/2 exp
'
−1
2σ2 (˜y −˜Xβ)′(˜y −˜Xβ)
(
(5.21)
and the second term on the right-hand side is joint posterior of the parameters from the
linear model,
π(β, σ2|X, y) ∝(σ2)−n+1
2
exp
'
−1
2σ2 (ˆσ2(n −k) + (β −ˆb)′X′X(β −ˆb))
(
(5.22)
(ˆb = (X′X)−1X′y), equation (5.5) from the uninformed priors p(β) ∝c and p(σ2) = 1
σ .
We want to unwind the square in the exponent according to: (β −ˆb)X′X(β −ˆb) = (β −
(X′X)−1X′y)′X′X(β −(X′X)−1X′y) = β′X′Xβ −2β′X′y + y′y = (y −Xβ)′(y −Xβ).
Putting these last two together gives:
π(˜y, β, σ2| ˜X, X, y) ∝(σ2)−n+q+1
2
× exp
'
−1
2σ2

(y −Xβ)′(y −Xβ) + (˜y −˜Xβ)′(˜y −˜Xβ)
(
.
(5.23)
The quantity of interest is obtained by integrating out β and σ2 individually, starting
with σ2:
π(˜y, β| ˜X, X, y) =
 ∞
0
π(˜y, β, σ2| ˜X, X, y)dσ2
∝
 ∞
0
(σ2)−n+q+1
2
exp
'
−1
2σ2 ((y −Xβ)′(y −Xβ)
+(˜y −˜Xβ)′(˜y −˜Xβ)

dσ2.
∝

(y −Xβ)′(y −Xβ) + (˜y −˜Xβ)′(˜y −˜Xβ)
−n+q
2
,
(5.24)
where we used the same gamma PDF trick as before in (5.7). To integrate out β we need
to collect terms in a more useful way, starting with breaking out the squares:
π(˜y, β| ˜X, X, y) =

y′y −2βX′y + β′X′Xβ + ˜y′˜y −2β ˜X′˜y + β′ ˜X′ ˜Xβ
−n+q
2
.
(5.25)

The Bayesian Linear Model
157
Deﬁning L = X′y + ˜X′˜y and M = X′X + ˜X′ ˜X, this last expression can be expressed as:
π(˜y, β| ˜X, X, y) ∝
	
y′y + ˜y′˜y + β′Mβ −2β′L

−n+q
2
=
	
y′y + ˜y′˜y −L′M−1L
+(β′Mβ −β′MM−1L −L′M−1Mβ + L′M−1MM−1L)

−n+q
2
=
	
y′y + ˜y′˜y −L′M−1L + (β −M−1L)′M(β −M−1L)

−n+q
2
.
(5.26)
Zellner (1971, p.73) integrates this form over the k length β vector to produce the posterior
predictive distribution of interest:
π(˜y| ˜X, X, y) ∝
	
y′y + ˜y′˜y + β′Mβ −2β′L

−n+q−k
2
.
(5.27)
He then deﬁnes the substitution H = (I −˜XM−1 ˜X′)/ˆσ2 and reintroduces ˆb to produce the
form:
π(˜y| ˜X, X, y) ∝

(n −k) + (˜y −˜Xˆb)′H(˜y −˜Xˆb)
−n+q−k
2
,
(5.28)
which makes it easier to see that this is a multivariate t-distribution with ν = n−k degrees
of freedom. Recalling the properties of the multivariate-t (Appendix B), this result means
that:
E[˜y] = ˜Xˆb,
Cov[˜y] =
n −k
n −k −2H−1,
(5.29)
with the obvious restriction on the size of n −k from the denominator.
This result is actually quite intuitive. While the data themselves are assumed to be nor-
mally distributed, future claims about the data conditional on the model are t-distributed
reﬂecting added uncertainty.
■Example 5.3:
A Model of Educational Eﬀects. The Bayesian linear model is
further illustrated through a partial replication of a Meier, Polinard, and Wrinkle
(2000) study of bureaucratic eﬀects on education outcomes in public schools (see also
the reanalysis in Wagner and Gill [2005], and extensions in Meier and Gill [2000]).
These authors are concerned with whether the education bureaucracy is the product
or cause of poor student performance. The issue is controversial, and Chubb and Moe
(1988, 1990) argue otherwise that the institutional structure of the schools, especially
the overhead democratic control, resulted in the schools being ineﬀective. The institu-
tional structure and the bureaucracy created a process that leads to poor performance
by the public schools.
This conclusion is challenged by Meier and Smith (1994), as
well as in Smith and Meier (1995), who contend that bureaucracy is an adaptation
to poor performance and not the cause.
Meier, Polinard, and Wrinkle develop a linear model based on panel dataset from
more than 1,000 school districts for a seven-year period to test organizational theory
and educational policy, producing an impressively large dataset here with n = 7301

158
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
cases. The question asked is whether there is a causal relationship between bureau-
cracy and poor performance by public schools. The central issue in this literature
is one of causality through a “production function” that maps inputs to outputs in
essentially an economic construct. Therefore their outcome variable is the percent of
students in district/year that pass the Texas Assessment of Academic Skills (TAAS),
which measures mastery of basic skills. In addition to bureaucratic causes, student
and school performance can be inﬂuenced by a number of variables, some of which are
causally related, including class size, state funding, teacher salary, and teacher expe-
rience. The data measure bureaucracy as the total number of full-time administrators
per 100 students and lag the variable so as to create a more likely causal relation-
ship. Other variables include three measures of ﬁnancial capital, which consist of the
average teacher salary, per pupil expenditures for instruction and the percentage of
money each district receives from state funds. A measure of human capital was in-
cluded based on teacher experience, and two policy indicators were used by measuring
the average class size in the district and the percent of students in gifted classes.
The linear regression model proposed by Meier et al. is aﬀected by both serial cor-
relation and heteroscedasticity.
Meier et al. address these concerns through a set
of six dummy variables for each year as well as through the use of weighted least
squares. The Meier et al. results are obtained by specifying diﬀuse normal priors on
the unknown parameters. These are Gaussian normal speciﬁcations centered at zero
with small precision. The model is summarized in “stacked” notation that shows the
distributional assumptions (priors and likelihood):
Y [i] ∼N(λ[i], σ2),
λ[i] = β0 + β1x1[i] + . . . + βkxk[i] + ϵ[i]
β[i] ∼N(0.0, 10)
ϵ[i] ∼N(0.0, τ)
τ ∼IG(16, 6)
(5.30)
Note the hierarchical expression of these distributional and modeling assumptions
here, which is the conventional way to notate models with dependent distributional
features (Chapter 12). This speciﬁcation above allows a close Bayesian replication
of the original model since all of the coeﬃcient prior distribution forms are highly
diﬀuse, and the precision prior distribution is tuned to resemble 1/σ2 from Meier et
al. This model is estimated using BUGS software, although with more agony it could
be directly computed. The results are provided in Table 5.5.
For this model we can calculate the posterior predictive distribution of the data as
given by (5.28) in the previous section. While there are certainly many interesting

The Bayesian Linear Model
159
TABLE 5.5:
Posterior: Meier Replication Model
Explanatory Variables
Mean
Std. Error
95% HPD Region
Intercept
9.172
1.358
[ 6.510:11.840]
Low Income Students
-0.108
0.006
[-0.119:-0.097]
Teacher Salaries
0.073
0.053
[-0.035: 0.181]
Teacher Experience
-0.009
0.046
[-0.099: 0.082]
Gifted Classes
0.097
0.023
[ 0.054: 0.139]
Class Size
-0.220
0.052
[-0.322:-0.118]
State Aid Percentage
-0.002
0.004
[-0.010: 0.006]
Funding Per Student (×1000)
0.065
0.174
[-0.276: 0.406]
Lag of Student Pass Rate
0.677
0.008
[ 0.661: 0.693]
Lag of Bureaucrats
-0.081
0.262
[-0.595: 0.431]
Posterior standard error of τ = 0.00072
cases that a scholar of education policy might insert into the rows of the design matrix,
˜X, we will limit the analysis here to the predicted outcome for all of the explanatory
variables set at their mean.
Producing other cases to reveal important eﬀects is
much like a ﬁrst diﬀerence calculation for evaluating the eﬀect of coeﬃcients in GLM
models. The posterior predictive value for ˜y where ˜X = ¯X is 48.18 (with a standard
error of 0.0007202 due to the huge size of the dataset), yet the mean of the y vector
is 63.84. This is interesting because it shows that the mean model outcome predicts a
much poorer outcome than that observed suggesting that the model is missing some
important features of the data-generating process. The other obvious suggestion that
the data are right-skewed is not true.
To expand on the Meier et al. model, it is possible to include non-sample information
for the creation of the Bayesian prior drawn from Meier’s previous work on school
bureaucracy and school performance with Kevin Smith (1994).
Clearly, Meier et
al. were not uninformed when specifying the model above and Bayesian inference al-
lows for the incorporation of that knowledge. The Smith and Meier work includes data
and inference on the impact of funding and other institutional variables on student
achievement in Florida. These include district level data for all of the public schools
in Florida. Smith and Meier note also that the Florida data provides a diverse group
of students with constant measures over time. The Florida data represents both rural
and urban districts as well as diﬀerent ethnic and socioeconomic compositions. The
prior distributions remain normal, but are now centered around values drawn from

160
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
the 1995 Smith and Meier ﬁndings:
β[0] ∼N(0.0, 10)
β[1] ∼N(−0.025, 10)
β[2] ∼N(0.0, 10)
β[3] ∼N(0.23, 10)
β[4] ∼N(0.615, 10)
β[5] ∼N(−0.068, 10)
β[6] ∼N(0.0, 10)
β[7] ∼N(−0.033, 10)
β[8] ∼N(0.299, 10)
β[9] ∼N(0.0, 10)
β[10] ∼N(0.0, 10)
β[11] ∼N(0.0, 10)
β[12] ∼N(0.0, 10)
β[13] ∼N(0.0, 10)
β[14] ∼N(0.0, 10)
β[15] ∼N(0.0, 10)
The numbered β terms represent the prior information assigned to each explanatory
variable (such BUGS statements are more elegant vectorized but the prior assignments
are then less obvious to a reader). The terms are ordered as in Table 5.5, with the
additional β[10 : 15] values representing years 1993-97. Some of these are left relatively
uninformed since the data from the Smith and Meier research were insuﬃcient to
address all of the current terms.
Interestingly, Meier et al. expected to ﬁnd a positive relationship between teacher
salaries and student performance, but did not ﬁnd one. The model that includes stip-
ulated priors generated results that were closer to the expectation of the researchers
since it incorporated knowledge to which the researchers already had access. Meier et
al. noted that economic theory expects higher salaries attract better teachers.
TABLE 5.6:
Posterior: Interaction Model
Explanatory Variables
Mean
Std. Error
95% HPD Region
Intercept
4.799
2.373
[ 0.165: 9.516]
Low Income Students
-0.105
0.006
[-0.117:-0.094]
Teacher Salaries
0.382
0.099
[ 0.189: 0.575]
Teacher Experience
-0.066
0.046
[-0.156: 0.025]
Gifted Classes
0.096
0.021
[ 0.054: 0.138]
Class Size
0.196
0.191
[-0.180: 0.569]
State Aid Percentage
0.002
0.004
[-0.006: 0.010]
Funding Per Student (×1000)
0.049
0.175
[-0.294: 0.392]
Lag of Student Pass Rate
0.684
0.008
[ 0.667: 0.699]
Lag of Bureaucrats
-0.042
0.261
[-0.557: 0.469]
Class Size × Teacher Salaries
-0.015
0.007
[-0.029:-0.002]
Posterior standard error of τ = 0.00071
In addition, the new model adds a multiplicative interaction between class size and
teacher salary as the variables are claimed to be related in this way. The interaction

The Bayesian Linear Model
161
coeﬃcient was found to have a negative sign with 95% credible interval bounded away
from zero, as provided in Table 5.6. So larger class sizes have an apparent dampening
eﬀect on the positive impact of increasing teacher salaries. Also, the posterior distri-
bution for class size now shows a much less reliable eﬀect in the interaction model (the
95% credible interval is almost centered at zero). Interaction eﬀects can sometimes
“steal” explanatory power and reliability from main eﬀects. This ﬁnding says that
the eﬀects of class size are now only reliable in this model in the context of speciﬁed
teachers’ salary levels.
If this model is qualitatively diﬀerent than the Meier Replication Model, then we would
expect the posterior predictive distribution to show some change reﬂecting better ﬁt
to the data. From the mean model, applying the data mean as input ˜X, the posterior
predictive value of y is 51.77 (with a standard error of 0.00071022), compared to the
y data mean of 63.84. So the enhanced model predicts slightly better but not as much
as we would have hoped. The point from this comparison is that one way to compare
model ﬁt in model development is to compare the predicted data to the actual. We
will be much more systematic about this process in Chapter 7, but it easy to see here
that posterior quantities form the basis of any such comparison.
5.3
Linear Regression with Heteroscedasticity
It is not uncommon to encounter the linear regression problem of non-constant error
variance. The typical means of dealing with this problem in the non-Bayesian setting is
through a more general form of least squares inserting a weighting matrix. The Bayesian
approach is similar but leads to some diﬃcult analytical issues, which we will overcome
here. The deﬁnitive citation is Geweke (1993) and useful discussions can also be found in
Mouchart and Simar (1984), who show that the Bayesian model can be worked out with
least squares theory, and Leonard (1975), who shows the importance of exchangeability in
this context.
Instead of the usual assumption about the distribution of y given X, we now assert
that yi|Xi ∼N(Xiβ, σ2ωi), where ω = (ω1, ω2, . . . , ωn) is a vector of unknown regression
weights (parameters), which we can also organize along the diagonal of a n × n matrix Ω
for convenience. The linear model is now deﬁned as:
y = Xβ + ϵ,
Var[ϵ] = σ2Ω,
(5.31)
which implies from the conditional distribution of the yi that ϵi ∼N(0, σ2ωi). Thus the
likelihood function from (5.4) becomes:
L(β, σ2|X, y) ∝σ−n|Ω|−1
2 exp
'
−1
2σ2 (ˆσ2(n −k) + (β −ˆb)′X′Ω−1X(β −ˆb))
(
.
(5.32)

162
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
We will again use the uninformed priors p(β) ∝c and p(σ2) =
1
σ over the support
[−∞:∞] and [0:∞]. Geweke (1993) suggests using independent chi-square distributions,
ω|ν ∼χ2(df = ν), which is also expressible as the gamma distribution G(ν/2, 1/2). This ν
parameter is ﬂexible and can be ﬁxed or estimated The resulting joint posterior distribution
is:
π(β, σ2, Ω|X, y) ∝L(β, σ2|X, y)p(β)p(σ2)p(Ω)
∝σ−n−1|Ω|−ν+3
2 exp
'
−1
2σ2

(ˆσ2(n −k)
+ (β −ˆb)′X′Ω−1X(β −ˆb) + νtr(Ω)−1
(
.
(5.33)
This posterior turns out to be quite diﬃcult to integrate for marginals, although π(β, Ω|X, y)
can be obtained with methods given previously in this chapter. Geweke instead ﬁnds three
conditional distributions according to the following (suppressing the data in the conditional
for emphasis).
▷The conditional posterior distribution of β:
π(β|σ2, Ω) ∝exp
'
−1
2σ2 (β −ˆb∗)′X′Ω−1X(β −ˆb∗)
(
,
(5.34)
where ˆb∗= (X′ΩX)−1X′Ωy. Thus the conditional distribution of the β vector is
multivariate normal according to N(ˆb∗, σ2(X′Ω−1X)−1).
▷The conditional posterior distribution of σ2:
π(σ2|Ω) ∝(σ2)−n+1
2
exp
'
−1
2σ2 ˆσ2∗
(
,
(5.35)
where ˆσ2∗= (y −Xˆb)′Ω−1(y −Xˆb). This is clearly an inverse gamma distribution
according to IG(σ2|(n −1)/2, ˆσ2∗/2), and Geweke also expresses it in χ2 terms.
▷The conditional posterior distribution of Ω:
π(Ω|β, σ2) ∝|Ω|−ν+3
2 exp
'
−1
2

ˆσ2∗/σ2 + νtr(Ω)−1(
(5.36)
where ˆσ2∗is deﬁned as before. So an individual diagonal element of Ω is conditionally
distributed ωi|β, σ2 ∝ω
−ν+3
2
i
exp
	
−(u2
i /2σ2 + ν/2)/ωi

, where ui = yi −xiβ. This
gives an inverse gamma PDF with parameters ν+1
2
and (u2
i /σ2 + ν)/2.
Another interesting feature of this model is that the posterior distribution of the residuals
are no longer normal according to ϵi ∼N(0, σ2ωi). In fact, they end up being Student’s-t
distributed with ν = n −k degrees of freedom. Geweke also points out that the model de-
veloped in this fashion produces residuals that are independent Student’s-t (an independent
χ2 denominator of a normal), whereas previous versions produced residuals with a joint
Student’s-t (a common χ2 denominator of a normal).

The Bayesian Linear Model
163
While the results above may not seem as useful as the unconditional marginal posteriors
we derived earlier in this chapter, it does set us up perfectly for the Gibbs sampler as a way
to get desired unconditionals. Recall from Chapter 1 that the Gibbs sampler uses iterated
samples from full conditional distributions for the parameters of interest to obtain empirical
estimates of marginals (page 25). In the present case we sample iteratively at the j + 1 step
according to:
β[j+1] ∼π(β|σ2[j], Ω[j])
σ2[j+1] ∼π(σ2|Ω[j])
Ω[j+1] ∼π(Ω|β[j+1], σ2[j+1])
where the β vector and the Ω matrix (which is really a vector’s worth of information) can be
sampled individually (β[j]
1 , . . . , β[j]
k , ω[j]
1 , . . . , ω[j]
n ) or as a block. Miraculously this iterative
process eventually produces sample values that behave as if they were generated from the
marginal distributions rather than the conditional distributions. Details on this MCMC
procedure are given starting in Chapter 10 and essentially constitute the second half of this
text.
■Example 5.4:
War in Ancient China. To demonstrate the described heteroscedas-
tic linear model with Bayesian priors we use conﬂict data from West Asia for events
taking place between 2700 BCE to 722 BCE. Cioﬃ-Revilla and Lai (1995, 2001) coded
documents from multiple epigraphic and archaeological sources on war and politics
in ancient China covering the Xia (Hsia), Shang, and Western Zhou (Chou) periods.
These data (n = 104 conﬂicts) are available via the Murray Archive. This is the only
quantitative dataset covering Chinese war during this period, but Cioﬃ-Revilla and
Lai use the modern Long-Range Analysis of War deﬁnitions.
The outcome variable of interest here is an additive combination of two of the coded
variables: Political Level (1 for internal war, 2 for interstate war) and Political Com-
plexity (governmental level of the warring parties), where the ﬁrst variable is multiplied
by ten for scale purposes. Thus we are looking to explain the political scope of conﬂicts
in terms of governmental units aﬀected. Explanatory variables of interest are EXTENT
(number of belligerents), DIVERSE (number of ethnic groups participating as belliger-
ents), ALLIANCE (total number of alliances among belligerents), DYADS (number of
alliance pairs), TEMPOR (type of war: protracted rivalry, integrative conquest, disinte-
grative/fracturing conﬂict, sporadic event), DURATION of conﬂict, measured in years,
BALANCE (the diﬀerence in military capabilities: minor-minor, minor-major, major-
major), ETHNIC (intra-group or inter-group), and POLAR (number of relatively major
or great powers at the time of onset). See Cioﬃ-Revilla and Lai (2001) or the asso-
ciated codebook for further details. Graphical investigation indicates the presence of
heteroscedastic eﬀects of concern with a homoscedastic linear model. See Figure 5.1.

164
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
Iterations
Issue Space
0e+00
2e+04
4e+04
6e+04
8e+04
1e+05
−0.5
0.0
0.5
1.0
FIGURE 5.1: MCMC Running Means, Chinese Conflicts
The Bayesian heteroscedastic linear model is ﬁt without an intercept (zero levels do
not make sense here), using uninformed priors p(β) ∝c and p(σ2) = 1
σ , as discussed
above. Furthermore, ﬁx ν = 50 to elongate the distribution of weights for this exam-
ple. The Gibbs sampler is run for 100, 000 iterations, throwing away the ﬁrst 50, 000
(this early “burn-in” period will discussed at length starting on page 478). Table 5.7
summarizes the resulting marginal posteriors from this estimation. The R code for
running this model is provided in the Computational Addendum to this chapter.
Details about running Gibbs samplers are provided starting in Chapter 10 and di-
agnostics for MCMC convergence are discussed particularly in Chapter 14. For now
note that the Markov chain iterations become stable in the cumulative mean plots
given in Figure 5.1.
Observe from the resulting HPD regions for the coeﬃcients that the model ﬁts very
well with no coeﬃcient 95% HPD regions crossing zero. We see here that the number
of belligerents, the number of ethnic groups, imbalance in capabilities, ethnic com-
position, and number of major groups all have accelerating eﬀects on the political
scope of the conﬂict. The type of war does as well but its interpretation is not as
clear. Conversely, the number of alliances, the number of dyads, and the duration give

The Bayesian Linear Model
165
TABLE 5.7:
Heteroscedastic Model, Ancient Chinese
Conflicts
Explanatory Variables
Mean
Std. Error
95% HPD Region
EXTENT
1.0145
0.1077
[ 0.8034: 1.2256]
ALLIANCE
-0.2840
0.0756
[-0.4321:-0.1359]
DYADS
-0.6540
0.0739
[-0.7988:-0.5092]
TEMPOR
0.1391
0.0302
[ 0.0799: 0.1984]
DURATION
-0.0779
0.0353
[-0.1471:-0.0087]
BALANCE
0.2810
0.0692
[ 0.1454: 0.4165]
ETHNIC
0.3210
0.0574
[ 0.2086: 0.4335]
POLAR
0.0189
0.0078
[ 0.0035: 0.0343]
Mean of σ2 = 0.0454
the opposite eﬀect. These are all as expected, despite using the most diﬀuse priors
possible.
5.4
Exercises
5.1
Derive the posterior marginal for β in (5.17) from the joint distribution given by
(5.16).
5.2
Write an R function that calculates R2 and the F-statistic for the two models given
in Table 5.3. Defend your choice of point estimate from the posterior distributions
used for these calculations. Does the Bayesian interpretation of these values diﬀer?
Is it reasonable to use these in applied Bayesian work?
5.3
For uninformed priors, the joint posterior distribution of the regression coeﬃ-
cients was shown to be multivariate-t (page 147), with covariance matrix: R =
(n−k)ˆσ2(X′X)−1
n−k−2
. Under what conditions is this matrix positive deﬁnite (a require-
ment for valid inferences here).
5.4
Clogg, Petkova, and Haritou (1995) give detailed guidance for deciding between
diﬀerent linear regression models using the same data. In this work they deﬁne the
matrices X, which is n × (p + 1) rank p + 1, and Z, which is n × (q + 1) rank q + 1,
with p < q. They calculate the matrix A =
	
X′X −X′Z(Z′Z)−1Z′X

−1. Find the
dimension and rank of A.
5.5
Under standard analysis of linear models, the hat matrix is given by ˆy = Hy where
H is X(X′X)−1X′ where the diagonal values of this matrix indicate leverage, which

166
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
is obviously a function of X only. Can the stipulation of strongly informed priors
change data-point leverage? Inﬂuence in linear model theory depends on both hat
matrix diagonal values and yi. Calculate the inﬂuence on the Technology variable
of each datapoint in the Palm Beach County model with uninformed priors by
jackkniﬁng out these values one at a time and re-running the analysis.
Which
precinct has the greatest inﬂuence?
5.6
Prove that the M matrix (I −H) from linear regression is symmetric and idempo-
tent.
5.7
Meier and Keiser (1996) used a linear pooled model to examine the impact of several
federal laws on state-level child-support collection policies. Calculate a Bayesian
linear model and plot the posterior distribution of the parameter vector, β, as well
as σ2, specifying an inverse gamma prior with your selection of prior parameters,
and the uninformative uniform prior: f(σ2) = 1
σ. Use a diﬀuse normal prior for the
β, and identify outliers using the hat matrix method (try the R command hat).
The data are collected for the 50 states over the period 1982 to 1991, where the out-
come variable, SCCOLL, is the change in child-support collections. The explanatory
variables are: chapters per population (ACES), policy instability (INSTABIL), policy
ambiguity (AAMBIG), the change in agency staﬃng (CSTAFF), state divorce rate
(ARD), organizational slack (ASLACK), and state-level expenditures (AEXPEND).
These data can be downloaded on the webpage for this book or from the BaM pack-
age. Additional description can be found in their original article or Meier and Gill
(2000, Chapter 2).
5.8
Replicate the Meier Interaction Model in Table 5.6 on page 160, using the dataset
student.score in BaM. Modify the prior variances for the coeﬃcient from 10 to
other numbers. Does this change the prior predictive distribution of the data (at
means for ˜X)? Why or why not?
5.9
Returning to the discussion of conjugate priors for the Bayesian linear regression
model starting on page 151, show that substituting (5.14) and (5.15) into (5.13)
produces (5.16).
5.10
Consider a linear regression setting where the matrix X′X is singular (X is n × k).
Clearly non-Bayesian solutions are limited, but careful stipulation of priors can lead
to workable results. Using the setup of Zellner (1971, p.75) we start with the usual
joint conjugate prior p(β, σ) = p(β|σ)p(σ), which is essentially (5.1.2), and
p(σ) ∝
1
σν0+1 exp
'
−ν0c2
0
2σ2
(
p(β|σ) ∝|A|−1
2
σk
exp
'
−1
2σ2 (β −B)′A(β −B)
(

The Bayesian Linear Model
167
where B is the prior mean for β, the prior covariance matrix, σ2A−1, is nonsingular.
Using the likelihood function in (5.2), we get the joint posterior distribution:
p(β, σ|X, y) ∝σ−n′−k−1 exp
'
−1
2σ2 (n′c2 + (β −˜β)(A + X′X)(β −˜β)
(
,
where n′ = n + ν0, and n′c2 = ν0c2
0 + y′y + B′AB −˜β
′(A + X′X)˜β (notice that
there is no need to invert X′X). Show that the marginal posterior distribution for
β is
π(β|X, y) ∝

n′c2 + (β −˜β
′(A + X′X)(β −˜β)
−n′+k
2
,
and give the distributional form with parameters identiﬁed.
5.11
For the Bayesian linear regression model, prove that the posterior that results from
conjugate priors is asymptotically equivalent to the posterior that results from
uninformative priors. See Table 5.3 on page 153.
5.12
The following data come from the 1998 European Household Community Panel
Survey (given in the R package BaM as ehcps).
The two variables are: (1) the
median (EU standardized) income of individuals age 65 and older as a percentage
of the population age 0–64, and (2) the percentage of all age groups with income
below 60% of the median (EU standardized) income of the national population.
Regress the second variable on the ﬁrst with a linear model, constructing conjugate
and uninformative prior distributions for Over 65 Relative Income. Compare these
results. Is it possible to specify highly inﬂuential priors in the conjugate case that
diﬀer markedly from the uninformative case?
Nation
Over 65 Relative Income
Total Poverty Rate
Netherlands
93.00
7.00
Luxembourg
99.00
8.00
Sweden
83.00
8.00
Germany
97.00
11.00
Italy
96.00
14.00
Spain
91.00
16.00
Finland
78.00
17.00
France
90.00
19.00
United.Kingdom
78.00
21.00
Belgium
76.00
22.00
Austria
84.00
24.00
Denmark
68.00
31.00
Portugal
76.00
33.00
Greece
74.00
33.00
Ireland
69.00
34.00
5.13
Develop a Bayesian linear model for the following data that describe the average
weekly household spending on tobacco and alcohol (in pounds) for the eleven re-
gions of the United Kingdom (Moore and McCabe 1989, originally from Family
Expenditure Survey, Department of Employment, 1981, British Oﬃcial Statistics).

168
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
Specify both an informed conjugate and uninformed prior using the level for al-
coholic beverages as the outcome variable and the level for tobacco products as
the explanatory variable. Do you notice a substantial diﬀerence in the resulting
posteriors? Describe.
Region
Alcohol
Tobacco
Northern Ireland
4.02
4.56
East Anglia
4.52
2.92
Southwest
4.79
2.71
East Midlands
4.89
3.34
Wales
5.27
3.53
West Midlands
5.63
3.47
Southeast
5.89
3.20
Scotland
6.08
4.51
Yorkshire
6.13
3.76
Northeast
6.19
3.77
North
6.47
4.03
5.14
Using your two model results from the Family Expenditure Survey model above,
create two graphs of predicted outcome values using the following steps for each:
(a) Create a matrix of X values over the range of the data, ˜X, of size greater than
the data.
(b) Draw m (large) values from the posterior distributions of β: ˜β.
(c) Create a vector of predicted outcomes according to: ˜y = ˜X˜β.
(d) Plot these values with a histogram and a random sample of them against the
actual data with qqplot.
Observe any diﬀerences occurring from the use of diﬀerent priors.
5.15
The standard econometric approach to testing parameter restrictions in the linear
model is to compare H0 :Rβ −q = 0 with H1 :Rβ −q ̸= 0 (e.g., Greene 2011),
where R is a matrix of linear restrictions and q is a vector of values (typically zeros).
Thus for example R =
 1 0 0 0
0 1 1 0
0 0 1 1

and q = [0, 1, 2]′ indicate β1 = 0, β2 + β3 = 1, and
β4 = 2. Derive expressions for the posteriors π(β|X, y) and π(σ|X, y) using both
conjugate and uninformative priors.
5.16
The inverse of a matrix Υ−1 of Υ is deﬁned as meeting ﬁve conditions:
(1)
HΥ−1Υ = H, (2) Υ−1ΥΥ−1 = Υ−1, (3) (ΥΥ−1)′ = Υ−1Υ, (4) (Υ−1Υ) =
ΥΥ−1, and (5) Υ−1Υ = I (1–4 implied by 5, H a symmetric, full rank matrix). The
Moore-Penrose generalized inverse matrix Υ−of Υ meets only the ﬁrst four condi-
tions. A pseudo-variance matrix is calculated as V′V, where V = GCHOL(H−),
GCHOL(·) is the generalized Cholesky, and H−is the generalized inverse of the

The Bayesian Linear Model
169
Hessian matrix. The result is a pseudo-variance matrix that is in most cases well
conditioned (not nearly singular). Show that if the Hessian is invertible, the pseudo-
variance matrix is the usual inverse of the negative Hessian.
5.17
Using the Palm Beach County electoral data (page 148, also available in the R
package BaM using data(pbc)), calculate the posterior predictive distribution of
the data using a skeptical conjugate prior (i.e., centered at zero for hypothesized
eﬀects and having large variance).
5.18
Returning to the 1998 European Household Community Panel Survey data, run
a heteroscedastic speciﬁcation using the Gibbs steps given on page 163 written in
R. Do you see a model improvement over a regular non-Bayesian model assuming
homoscedasticity? Provide evidence.
5.19
Rerun the Ancient China Conﬂict Model using the code in this chapter’s Com-
putational Addendum using informed prior speciﬁcations of your choice (you
should be willing to defend these decisions though). Calculate the posterior mean
for each marginal distribution of the parameters and create a set of 104 predicted
outcome data values as customarily done in linear models analysis. Graph the yi
against ˆyi and make a statement about the quality of ﬁt for your model. Can you
improve this ﬁt by dramatically changing the prior speciﬁcation?
5.20
The “Grenander Conditions” for establishing asymptotic properties of the linear
model are given by: [G1:] for each column of X: X′
kXk −→+∞(sums of squares
grow as n grows, no columns of all zeros), [G2:] no single observation dominates
each explanatory variable k:
lim
n−→∞
X2
ik
X′
kXk = 0, i = 1, . . . , n, [G3:] X′X has rank
k by Gauss-Markov assumption, deﬁne X−0 as the explanatory variable matrix
minus the leading column of 1s, then
lim
n−→∞X−0′X−0 = C, C a positive deﬁnite
matrix. Prove that for a Bayesian linear model with conjugate priors these provide:
ˆβ
asym.
∼
N

β, ˆσ2
n Q−1
, where Q = lim
n→∞
1
nX′X, ˆβ is the posterior mean vector for
the coeﬃcients, and ˆσ is the posterior mean of σ.
5.5
Computational Addendum
5.5.1
Palm Beach County Normal Model
The R code here was used to develop the linear model example with the Palm Beach
County voting data.
# RETURNS A REGRESSION TABLE WITH CREDIBLE INTERVALS
t.ci.table <- function(coefs,cov.mat,level=0.95,degrees=Inf,
quantiles=c(0.025,0.500,0.975))
{

170
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
quantile.mat <- cbind( coefs, sqrt(diag(cov.mat)),
t(qt(quantiles,degrees) %o% sqrt(diag(cov.mat)))
+ matrix(rep(coefs,length(quantiles)),
ncol=length(quantiles)) )
quantile.names <- c("Mean","Std. Error")
for (i in 1:length(quantiles))
quantile.names <- c(quantile.names,paste(quantiles[i],
"Quantile"))
dimnames(quantile.mat)[2] <- list(quantile.names)
return(list(title="Posterior Quantities",round(quantile.mat,4)))
}
# READ IN THE DATA AND USE MULTIPLE IMPUTATION ON MISSING
lapply(c("BaM","mice","nnet"),library,character.only=TRUE)
data(pbc.vote)
attach(pbc.vote)
X <- cbind(tech, new, turnout, rep, whi)
Y <- badballots
detach(pbc.vote)
imp.X <- mice(X)
X <- as.matrix(cbind(rep(1,nrow(X)), complete(imp.X)))
dimnames(X)[[2]] <- c("tech", "new", "turnout", "rep", "whi")
# UNINFORMED PRIOR ANALYSIS
bhat <- solve(t(X)%*%X)%*%t(X)%*%Y
s2 <- t(Y- X%*%bhat)%*%(Y- X%*%bhat)/(nrow(X)-ncol(X))
R <- solve(t(X)%*%X)*((nrow(X)-ncol(X))*
s2/(nrow(X)-ncol(X)-2))[1,1]
uninformed.table <- t.ci.table(bhat,R,
degrees=nrow(X)-ncol(X))[[2]]
alpha <- (nrow(X)-ncol(X)-1)/2
beta <- 0.5*s2*(nrow(X)-ncol(X))
sort.inv.gamma.sample <- sort(1/rgamma(10000,alpha,beta))
sqrt.sort.inv.gamma.sample <- sqrt(sort.inv.gamma.sample)
uninformed.table <- rbind(uninformed.table,
c( mean(sqrt.sort.inv.gamma.sample),
sqrt(var(sqrt.sort.inv.gamma.sample)),
sqrt.sort.inv.gamma.sample[250],
sqrt.sort.inv.gamma.sample[5000],
sqrt.sort.inv.gamma.sample[9750] ))
# CONJUGATE PRIOR ANALYSIS
A <- 3; B <- 9
BBeta <- rep(0,6); Sigma <- diag(c(2,2,2,2,2,2))
tB <- solve(solve(Sigma)
+ t(X)%*%X)%*%(solve(Sigma)%*%BBeta+t(X)%*%X%*%bhat)

The Bayesian Linear Model
171
ts <- 2*B + s2*(nrow(X)-ncol(X)) + (t(BBeta)-t(tB))%*%
solve(Sigma)%*%BBeta + t(bhat-tB)%*%t(X)%*%X%*%bhat
R <- diag(ts/(nrow(X)+A-ncol(X)-3))*
solve(solve(Sigma)+t(X)%*%X)
alpha <- nrow(x)+A-ncol(X); beta <- 0.5*S2*(nrow(X)+A-ncol(X))
conjugate.table<-t.ci.table(tB,R,
degrees=nrow(X)+A-ncol(X)-2)[[2]]
sort.inv.gamma.sample <- sort(1/rgamma(10000,alpha,beta))
sqrt.sort.inv.gamma.sample <- sqrt(sort.inv.gamma.sample)
conjugate.table<- rbind(conjugate.table,
c( mean(sqrt.sort.inv.gamma.sample),
sqrt(var(sqrt.sort.inv.gamma.sample)),
sqrt.sort.inv.gamma.sample[250],
sqrt.sort.inv.gamma.sample[5000],
sqrt.sort.inv.gamma.sample[9750] ))
5.5.2
Educational Outcomes Model
While programming in BUGS is not explained until later chapters, the code for the Meier
et al. model and the informed extension is provided here for reference.
Notice, for the
time being, that BUGS code shares many features with R in terms of specifying models and
distributions. The ﬁrst model could be coded in a more eﬃcient manner by making theta
a vector, but leaving it as a set of scalars makes modifying the prior with substantive prior
information easier to specify and evaluate. The X[] variables are out of order in the linear
speciﬁcation because the data matrix orders them diﬀerently than the model listing in the
original article. Also, we would normally specify a theta vector below instead of a series of
scalars, but the individual notation is retained to make it easier to contrast the two model
speciﬁcations.
# MEIER, ET AL. REPLICATION MODEL
Model {
theta0~dnorm(0.0,0.001);
theta1~dnorm(0,0.001)
theta2~dnorm(0.0,0.001);
theta3~dnorm(0.0,0.001)
theta4~dnorm(0.0,0.001);
theta5~dnorm(0.0,0.001)
theta6~dnorm(0.0,0.001);
theta7~dnorm(0.0,0.001)
theta8~dnorm(0.0,0.001);
theta9~dnorm(0.0,0.001)
theta10~dnorm(0.0,0.001); theta11~dnorm(0.0,0.001)
theta12~dnorm(0.0,0.001); theta13~dnorm(0.0,0.001)
theta14~dnorm(0.0,0.001); theta15~dnorm(0.0,0.001)
tau~dgamma(16,6); sigma.sq <- 1
for (i in 1 : N)
{
epsilon[i]~dnorm(0.0, sigma.sq)
lambda[i] <- theta0 + theta1*X9[i]
+ theta2*X10[i] +
theta3*X2[i]
+ theta4*X3[i]
+ theta5*X4[i] +

172
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
theta6*X5[i]
+ theta7*X6[i]
+ theta8*X7[i] +
theta9*X8[i]
+ theta10*X11[i] + theta11*X12[i] +
theta12*X13[i]
+ theta13*X14[i] + theta14*X15[i] +
theta15*X16[i]
+ epsilon[i]
Y[i] ~ dnorm(lambda[i], tau)
}
}
# MODEL WITH INFORMED PRIORS AND INTERACTION
Model
{
theta0~dnorm(0.0,0.1);
theta1~dnorm(-.025,0.1)
theta2~dnorm(0.0,0.1);
theta3~dnorm(0.23,0.1)
theta4~dnorm(0.615,0.1); theta5~dnorm(-0.068,0.1)
theta6~dnorm(0.0,0.1);
theta7~dnorm(-.033,0.1)
theta8~dnorm(0.299,0.1); theta9~dnorm(0.0,0.1)
theta10~dnorm(0.0,0.1);
theta11~dnorm(0.0,0.1)
theta12~dnorm(0.0,0.1);
theta13~dnorm(0.0,0.1)
theta14~dnorm(0.0,0.1);
theta15~dnorm(0.0,0.1)
tau~dgamma(16,6); sigma.sq <- 1
for (i in 1 : N)
{
epsilon[i]~dnorm(0.0, sigma.sq)
lambda[i] <- theta0 + theta1*X9[i]
+ theta2*X10[i] +
theta3*X2[i]
+ theta4*X3[i]
+ theta5*X4[i] +
theta6*X5[i]
+ theta7*X6[i]
+ theta8*X7[i] +
theta9*X8[i]
+ theta10*X11[i] + theta11*X12[i] +
theta12*X13[i]
+ theta13*X14[i] + theta14*X15[i] +
theta15*X16[i]
+ theta16*X6*X3
+ epsilon[i]
Y[i] ~ dnorm(lambda[i], tau)
}
}
5.5.3
Ancient China Conﬂict Model
This section provides the R code for running the Gibbs sampler in the Chinese wars
example.
data(wars)
attach(wars)
X <- cbind(EXTENT,DIVERSE,ALLIANCE,DYADS,TEMPOR,DURATION)
y <- SCOPE
detach(wars)
n <- nrow(X); k <- ncol(X)
nu <- 5
num.sims <- 10000
war.samples <- matrix(NA,nrow=num.sims,(ncol=k+n+1))

The Bayesian Linear Model
173
beta <- rep(1,ncol(X));sigma.sq <- 3;Omega <- 3*diag(n)
b <- solve(t(X) %*% X) %*% t(X) %*% y
yXb <- y-X%*%b
for (i in 1:num.sims)
{
Omega.inv <- solve(Omega)
X2.Om
<- solve(t(X) %*% Omega %*% X)
b.star <- X2.Om %*% t(X) %*% Omega %*% y
s.sq.star <- t(yXb) %*% Omega.inv %*% (yXb)
u <- y - X %*% beta
beta <- as.vector( rmultinorm(1, b.star, sigma.sq *solve(t(X)
%*% Omega.inv %*% X) ) )
sigma.sq <- 1/rgamma(1,shape=(n-1)/2,rate=s.sq.star/2)
for (j in 1:n) Omega[j,j] <- 1/rgamma(1, shape=(nu+1)/2,
rate=((sigma.sq^(-1))*u^2 + nu)/2 )
war.samples[i,] <- c(beta,sigma.sq,diag(Omega))
}


Chapter 6
Assessing Model Quality
6.1
Motivation
The third step in Bayesian analysis is to evaluate the model ﬁt to the data and determine
the sensitivity of the posterior distribution to the assumptions. This is typically an interac-
tive and iterative process in which diﬀerent approaches are taken until there is reasonable
evidence that the conclusions are both statistically reliable and stable under modest changes
in the assumptions. So the emphasis in this chapter is on model adequacy: the suitability of
a single model under consideration. The subsequent chapter is really about model testing:
determining why we should prefer one model speciﬁcation over another. Both of these are
critical considerations, and it is important to keep them distinct.
Model checking is a critical part of the estimation process since there is nothing in the
procedures outlined in previous chapters that would per se prevent one from producing in-
correct and overly sensitive posterior distributions for the unknown parameters of interest.
In George Box’s (1995) words: “Statistics has no reason for existence except as a catalyst for
scientiﬁc inquiry in which only the last stage, when all the creative work has already been
done, is concerned with a ﬁnal ﬁxed model and rigorous test of conclusions.” Furthermore,
Box (1980) also notes that “No statistical model can safely be assumed adequate.” Accord-
ingly, this chapter provides various methods for assessing the quality of “ﬁnal” models in
the sense that other considerations, such as data collection, parametric speciﬁcation, and
variable selection, have already been determined and our goal is to understand the quality
of those choices.
The quality of a Bayesian posterior inference is attributable to three model assignments:
the prior, the likelihood function, and the loss function (if the latter is explicitly speciﬁed).
Prior speciﬁcations include the assigned parametric form as well as the prior distribution’s
parameter values, and in the case of hierarchical models the hyperprior speciﬁcations: higher
level prior distributions assigned to the ﬁrst level of priors. For a given prior, there is a range
of possible outcomes, including: the prior is subsumed by a large data set and therefore not
particularly important, the prior is moderately inﬂuential even with reasonable sample size,
and the prior strongly aﬀects the form of the posterior. The second model assignment is
the speciﬁed parametric form of the likelihood function, which itself ranges from relatively
175

176
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
unimportant to mostly determining the shape of the posterior. Finally, if a loss function is
stipulated, the characteristics of this function can also aﬀect posterior conclusions.1
After looking at some simple comparative methods, this chapter focuses on three re-
lated approaches to model checking in applied Bayesian work: sensitivity analysis, global
robustness, and local robustness. Sensitivity analysis is the informal process of altering
assumptions according to researcher intuition with the objective of determining the extent
to which these changes modify the posterior distribution. In particular, does varying the
prior parameters or modestly changing the form of the prior itself lead to vastly diﬀerent
conclusions from the posterior? Robustness is posterior insensitivity to broad classes of
user-speciﬁed assumptions and is therefore a desirable quality of Bayesian models.2 Ro-
bustness evaluation is the systematic process of determining the degree to which posterior
inferences are aﬀected by both potential misspeciﬁcation of the prior and inﬂuential data
points. It is actually important to remember the distinction between sensitivity analysis
and robustness (Skene, Shaw, and Lee 1986). Global robustness evaluation performs a for-
mal analysis of a large class of priors to determine the subsequent range of the inferences.
Local robustness generally uses diﬀerential calculus to determine the volatility of speciﬁc
reported results.
The distinction between global and local robustness does not need to be conﬁned to the
prior distributions under consideration for a Bayesian model. Any aspect of the speciﬁcation
is equally suspect and therefore deserving scrutiny. Smith (1986), for example, expresses
model criticism distinctness in very general terms (quoting, p.97):
▷global criticism, which basically asks the question “should we abandon the current
framework completely, despite the fact we have nothing at all to propose in its place?”
▷local criticism, which asks “should the model be modiﬁed or extended in this or that
particular direction?”
The deﬁnition of global criticism may be a little bit stark here since it is almost always
true that the researcher has potential alternatives. Smith actually argues that his given
characterization of global robustness checking is too limited and not particularly Bayesian
in spirit. Note that this is essentially the Fisherian setup where models are tested against
a vague null concept that is not speciﬁed but represents a lack of systematic eﬀect. The
underlying philosophy that undergirds either approach is that a single model applied to some
data produces a single conclusion, but a range of models applied to some data produces
1The implications of loss function robustness are not considered here in detail and the reader is referred
to the general discussion on page 247, and to: Dey and Micheas (2000), Dey, Lou, and Bose (1998), Mart´ın,
R´ıos Insua, and Ruggeri (1998), and Ramsey and Novick (1980). Kadane and Chuang (1978), extended
in Kadane and Srinivasan (1996), provide an integrated way of looking at the prior and the loss function
together by introducing the broader idea of posterior stability.
2Since robustness increases automatically with increases in sample size, then robustness evaluation is
less important in models with large samples. Conversely, models with modest sample size (say less than
100, but this is data-dependent) should be thoroughly analyzed in this regard.

Assessing Model Quality
177
a range of conclusions. The degree to which this range of conclusions diﬀers for modest
changes in basic assumptions warns the researcher about the delicateness of their ﬁndings.
This chapter also covers the posterior predictive distribution as a way to investigate
the quality of models through unseen data that they imply. Since all unknown quantities
are treated probabilistically, Bayesian posterior results can be averaged across models to
give more robust coeﬃcient estimates as they cover multiple-model space. Bayesian model
averaging can therefore be used to reduce the uncertainty inherent in presenting a single
speciﬁcation.
Chapter 7 also shows how the Kullback-Leibler distance can be used to
compare distributional distances. This allows us to measure diﬀerences between priors and
posteriors.
6.1.1
Posterior Data Replication
One very simple way to evaluate model quality is to produce a replicate dataset under the
estimated model and compare it to the observed data using summary statistics or graphical
analysis. This can be done analytically, but is much easier with simulation. While the core
ideas of Monte Carlo simulation and MCMC estimation are covered in later chapters, we
have been systematically introducing some of these ideas throughout the early part of this
text. Here we will see a preview of two important tools in this section: the BUGS language
for estimating Bayesian models with Markov chain Monte Carlo, and the use of simulation
across posterior distributions as a means of describing the implications of the model.
The process works as follows. First, produce a posterior distribution for all unknown
quantities in the customary fashion, where the outcome variable vector of interest, y, is
modeled by observed predictors, X, and parameters β, according to the general speciﬁcation
f(y|X, β). Second, ﬁx X at these observed values and draw m replicated values of β from
the corresponding posterior distribution: β(1), β(2), . . . , β(m). This last step can be done
analytically, but is almost trivially easy if the model has been estimated with MCMC
procedures described already on pages 25 and 164, since these procedures give empirical
draws from the coeﬃcient posterior distributions and we can simply sample from these.
Third, take such a sample of posterior draws and treat them as ﬁxed to fully deﬁne a
single version of the posterior model and calculate the implied outcome variable vector,
f(yrep(i)|X, β(i)), for posterior draws i = 1, 2, . . . , m. Finally, plot and compare the yrep(i)
with the observed y where large deviations imply poor model ﬁt.
There are obviously
many variations to this procedure that correspond to diﬀerent model characteristics (see
the discussion in Gelman and Hill [2007, pp.517-524], with R and BUGS code).
■Example 6.1:
Posterior Comparison for a Model of Abortion Attitudes in
Britain. To illustrate this model-checking procedure, we develop a model of support
for abortion under diﬀerent scenarios using survey data from Britain in consecutive
years from 1983 to 1986. The panel data for 264 respondents is collected annually by
McGrath and Waterton (1986) where seven scenarios are provided and these respon-
dents have the option of expressing support or disagreement for abortion (see also the

178
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
reanalysis by Knott et al. [1990]). The full collection of these seven queries do not
fall into an obvious ordinal scale, so we will treat them here as nominal and judge
total support for abortion as a binomial test for each respondent at each wave of the
panel. The scenarios are: (1) the woman decides on her own that she does not wish
to have the child, (2) the couple agree that they do not wish to have the child, (3)
the woman is not married and does not wish to marry the man, (4) the couple cannot
aﬀord any more children, (5) there a strong chance that the baby has a biological
defect, (6) the woman’s health is seriously endangered by the pregnancy, and (7) the
woman became pregnant as a result of rape. So each respondent has the ability to
produce an outcome from 0 to 7. These data are available in the R package BaM.
Naturally we do not want to treat the repeated annual trials as independent since
this would ignore correlation within subjects. So the model is given by the following
speciﬁcation for i = 1, . . . , 264 respondents across j = 1, . . . , 4 panel waves:
yij ∼BN(n, pij)
logit(pij) = β0,j + β1,iX1,i
β0,j ∼N(μ0, τ0)
β1,i ∼N(μ1, τ1)
μ0 ∼N(0, 100)
μ1 ∼N(0, 100)
τ0 ∝Chalf(25)
τ1 ∝Chalf(25)
(6.1)
where the second term in the normals is a variance and half-Cauchy priors are positive-
support, zero-centralized forms (location) with a scale term equal to A = 25, f(τ) =
(1 + τ/A)−1, τ > 0. This is a form recommended by Gelman (2006) as an alternative
to gamma distributions with small parameters for providing low-information priors
for variances terms. Here X1,i is the ith person’s self-identiﬁed religion: Catholic (1),
Protestant (2), Other (3), and No Religion (4), and n = 7 at each wave for each
person. The JAGS code is:
model
{
for (i in 1:PEOPLE)
{
for (j in 1:WAVES)
{
logit(p[i,j]) <-
b0[j] + b1[i]*x1[i];
r[i,j] ~ dbin(p[i,j], n[i]);
}
b0[i] ~ dnorm(mu0, nu0);
b1[i] ~ dnorm(mu1, nu1);
}
mu0
~ dnorm(0.0,1.0E-2);
mu1
~ dnorm(0.0,1.0E-2);
tau0
~ dnorm(0,1)T(0,);
tau1
~ dnorm(0,1)T(0,);
sigma ~ dgamma(2,2);

Assessing Model Quality
179
nu0
<- 1/(25*tau0/sqrt(sigma));
nu1
<- 1/(25*tau1/sqrt(sigma));
}
While this is getting a little ahead of ourselves, it is useful to look at the JAGS code
before Chapter 11. Notice that the code inside the “for” loop is essentially how we
would describe the model statistically on paper. That is, p comes from a linear additive
component with a link function, then the outcome variable is given a distributional
assumption. The term n[i] is indexed for generality but does not need to be since all
respondents are given seven statements at each wave. The Gibbs sampler is run for
20,000 iterations dispensing with the ﬁrst 10,000 values. All convergence diagnostics
point towards convergence. This is a hierarchical model in the sense of those described
in Chapter 12, so in addition to obtaining posterior distributions for the two μ and τ
parameters, there will be n = 264 posterior distributions for each β. Therefore we will
not report the full posterior results beyond Table 6.1, and will concentrate instead on
assessing ﬁt.
TABLE 6.1:
Model Summary, Abortion
Attitudes in Britain
Posterior Quantiles
0.025
0.25
0.50
0.75
0.975
μ0
-0.6708
-0.2563
-0.0294
0.1960
0.6245
τ0
0.0283
0.0500
0.0641
0.0789
0.1113
μ1
0.3029
0.4129
0.4728
0.5291
0.6336
τ1
0.0303
0.3136
0.6726
1.1507
2.2453
More importantly, consider Figure 6.1 where the jittered observed data values from the
last wave of the panel are plotted against nine (uniformly) randomly selected jittered
iterations from the recorded Markov chain. This is not a test for ﬁt but indicates
both positive and negative aspects of the results. The closer the results are to the
upward sloping diagonal, the better the ﬁt. Thus for high support, the model ﬁts well
and for low support the model ﬁts less well. This makes sense since the questions
are diverse and mixed support is therefore much harder to model than uniformly or
nearly-uniformly strong support. Notice also the few values in the radically wrong
corners of the graphs. More covariate information in the data would likely tighten the
ﬁt around the diagonal. Note that in the case where the outcome variable of interest
is continuously measured, the graphical display is more straightforward with lines or
points instead of these discrete categories.

180
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+++
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
++
++ +
+
+
+
+
+
+
+
+
+
+
++
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
++
+
+
+
+
+
+
+
+
+
++
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
++
+
+
+
+
+
+
+
+
+
+
++
++
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
++
+
+
+
+
+
+
+
+
+
+
+
+ +
+
+
++
+
++
+
+
+
7490
1
2
3
4
5
6
7
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
++
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
++
+
+
+
+
+
++
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
++
+ +
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
++
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
++
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
8664
+
+
+
+ +
+
+
+
+
+
+
+
+
+
+
+
++
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+ ++
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+++
+
+
+
+
+
+
+
+
+
+
+
+
+
+ +
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
++
+ +
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
++
+
+
+
+
+
+
+
+
+
+
+
+
+
++
+
+
+
+
+
++
+
+
+
+
+
+
+
+
+
+
+ +
++
+
+
++
+
+
+
+
+
+
+
+
+
+
+
+++
+
+
+
+
+
+
+
+
+
+
++
+
+
+
+
+
+
++
+
++
+
+
+
7621
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+ +
++
+
+
+
++
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
++
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
++
+
+
+
+
+
+
+
+
+
++
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+ +
+
+
+
+
+
+
+
+
+
+
+
+
++
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+ +
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
++
+
+
+
+
+
+
+
+
+
+
++
+
+
+
7678
1
2
3
4
5
6
7
+
+
+
++
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
++
+
+
+
+
+
+
+
+
+
++
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
++
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
++
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
++
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
++
+
++
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
++
+
+
++
+
+
+
+
+
++
+
+
+
6972
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
++
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+ +
+
+
+
+
+
+
+
+
+
+
+
+
+
++++
+
+
+
+
+
+
+
+
+
+
+
+
+
++
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
++
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
++
+
+
+
+
++
+
+
+
+
+
+
+
+
+
++
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
++
+
+
+ +
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
++
+
+
+
7771
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+ +
+
+
+
+
+
+
+
+
+
+
+
+
+
+
++
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
++
+
+
+
+
+
+
++
+
+
+
+
++
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+++
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+ +
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
++
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+ +
+
+
+
+
+
+
+
+
+
+
+
+
+
++
+
+
+
+
+
++
+
+
+
+
+
+
+
+
+
+
++
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+ +
+
+
+
+
+
+
3050
1
2
3
4
5
6
7
1
2
3
4
5
6
7
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
++
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
++
+
+
+
+ +
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+ +
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
++
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
++
+
+
+
+
+
+
+
+
+
++
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+ +
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+++
+
++
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
++
+
+
+
8673
1
2
3
4
5
6
7
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
++
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+ +
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+++
+
+
+
+
+
+
+
++
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
++
+
+
+
+
+
+
+
+
+
+
+
++
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
++
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
++
+
+ +
+
+
+
+
+
+
+
++
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+ +
+
+
+
+
+
++
+
+
+
+
+
+
+
+
+
+
+
++
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
++
+
++
+
+
+
2614
1
2
3
4
5
6
7
Iterative Simulation Values
4th Panel Wave
FIGURE 6.1: Outcome Comparison: Observed versus Simulated
6.1.2
Likelihood Function Robustness
The speciﬁcation of the likelihood function is generally not very controversial in specify-
ing Bayesian or non-Bayesian models because it is often narrowly deﬁned by the form of the
outcome variable and is well-studied as a contributor to model quality in the generalized
linear models literature (Fahrmeir and Tutz 2001; Gill 2000; McCullagh and Nelder 1989).
Conversely, frequentist criticism (and therefore Bayesian defensiveness) is often centered on
the posterior implications of prior assumptions. It should be noted, however, that the like-
lihood function component of the model is no less suspect than the prior speciﬁcation and
worth attention as well since nearly all researchers “directly announce the likelihood itself
without deriving it” (Poirer 1988, p.131). Also, the likelihood selection process is no less
subjective than the prior selection process (de Finetti 1974, 1975).
For example, the selec-
tion of a Poisson link function versus a negative binomial link function accords signiﬁcantly
less attention and defensive eﬀorts than the choice of a prior for the same model.
Shyamalkumar (2000) suggests a means of checking likelihood robustness across related
likelihood classes, and he shows that it is relatively simple to deﬁne a ﬁnite class of re-
lated likelihood models with greater robustness properties. For instance, it is well known
that prior speciﬁcations with wider tails have better robustness properties than conjugate
choices (Berger 1984, 1985). So Shyamalkumar recommends a Cauchy comparison class
by matching up normal and Cauchy quantiles (for instance the interquartile range of a
N(θ, 1) distribution matches those of a C(θ, 0.675)). It is also possible to formulate this
neighborhood speciﬁcation nonparametrically (Lavine 1991a).

Assessing Model Quality
181
6.2
Basic Sensitivity Analysis
A very simple and helpful method for assessing posterior model quality is to vary the
prior or other assumptions in some ad hoc but intuitive way and observe the implications
with respect to posterior quantities of interest. If reasonably large changes in model as-
sumptions are seen to have a negligible eﬀect on the calculated posterior density, then we
can comfortably establish that the data are suﬃciently inﬂuential or that the varied assump-
tion is suﬃciently benign (or perhaps both) to eliminate further worry about the subjective
inﬂuence of assumptions. Conversely, a far more alarming scenario occurs when one makes
mild changes to the established model assumptions and dramatic changes are observed in
the summary measures of the posterior.
6.2.1
Global Sensitivity Analysis
There are essentially two types of Bayesian sensitivity analysis: global and local (Leamer
1978).
Global sensitivity analysis is a broad approach that evaluates a wide range of:
alternative prior speciﬁcations (Berger 1984, 1990), forms of the link function (Draper 1995),
missing data implications (Kong, Liu, and Wong 1994), error sensitivity (Polasek 1987),
and perturbations of the likelihood and prior speciﬁcations (Kass and Raftery 1995). The
purpose of global sensitivity is to vary the widest possible range of assumptions, although
some authors have used the term to describe analysis of the sensitivity of the posterior to
diﬀerences provided within a given family of priors (Polasek 1987).
Leamer’s original objective in global sensitivity analysis is to determine the maximum
amount that the model assumptions can be varied without dramatically changing the pos-
terior inferences (1985, p.308). Thus substantive inferences are deemed to be reliable only
if the range of assumptions is wide enough to be realistic and the corresponding poste-
rior inferences are narrow enough that they provide consistent results. This is a naturally
appealing idea because it would indicate which speciﬁcations are relatively “fragile” with
respect to researcher-speciﬁed model assumptions, although Pagan (1987) believes that this
approach does not go far enough since it is restricted to altering the parameters of the prior
distribution only. Unfortunately the idea of absolute global sensitivity analysis opens a vast
array of alternative speciﬁcations and tests to consider, even restricting oneself to the prior
only. Often there are too many of these alternatives to consider in a reasonable amount of
time, and there is also the additional challenge of clearly reporting these results to readers.
From many published works, we can see that the obvious, but not solitary, target of
sensitivity analysis is the form of the prior distribution. Routinely, a ﬂat prior of some sort
is substituted for an informative prior. The observed change in the posterior can range
from trivial to substantial. It should be noted, however, that if the informed prior has
a substantial theoretical basis, large changes to the posterior are not a priori a suﬃcient

182
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
reason to disqualify it from the analysis. Conversely, strongly informative priors that are
speciﬁed purely for mathematical convenience should be evaluated carefully if they produce
a radically diﬀerent posterior form than a reference prior of some kind.
6.2.1.1
Speciﬁc Cases of Global Prior Sensitivity Analysis
The classical prior juxtaposition is to specify an alternative uninformative prior (typ-
ically uniform) over the support of the parameter of interest and compare it with the
stipulated informative prior. If there is a substantial diﬀerence in the form of the posterior,
then this is something that often needs to be explained to readers. It is not necessary to
use the uniform as a comparison prior, but many researchers ﬁnd it to be a convenient com-
parison form. An obvious question arises regarding how much of a change in the posterior
results indicates strong prior inﬂuence from the informative prior. First, if the diﬀerence
changes the substantive conclusions from the model, then the informed prior should be
earnestly defended. Consider the diﬀerence in the posteriors for Teacher Salaries in the two
models of educational eﬀects in Section 5.2: a posterior mean and standard deviation of
(0.073, 0.053) with uninformed priors (Table 5.5), versus (0.382, 0.099) with informed pri-
ors. With a sample size of 7301 cases we can safely assume that the posteriors are normally
distributed for this linear model speciﬁcation. Accordingly, the ﬁrst posterior has 0.0842
of the density below zero while the second posterior has 0.0006 of the density below zero.
Often the diﬀerences are not this stark, and sometimes the diﬀerences are merely trivial. In
such settings we would be less concerned with the inﬂuence of the prior distribution since
the shrinkage of the posterior to the prior is small.
If the researcher-speciﬁed prior is of the normal form, then t-distributions, varying by
the degrees of freedom, can be used as wider tail alternatives (Lange, Little, and Taylor
1989). One advantage of the t-distribution approach is that it avoids problems associated
with specifying improper uniform priors over the real numbers. Jeﬀreys (1961) recommends
using the Cauchy distribution as a prior in this context since its very heavy tail structure
implies a high level of conservatism, although there is often not a substantial comparative
diﬀerence between an informative prior relative to a uniform prior and an informative prior
relative to a Cauchy prior.
6.2.1.2
Global Sensitivity in the Normal Model Case
A form of sensitivity analysis with the normal model was performed previously with
simulated data in Table 3.2. That example demonstrated that an intentionally misspeciﬁed
prior aﬀected posterior quantities of interest: HPD regions and point estimates. The obvious
point was that as the sample size increased from 10 to 1,000, the misspeciﬁcation of the
prior parameters on the normal prior mattered considerably less.
Suppose we assign an inﬁnitely diﬀuse prior for an unknown mean in the normal model
discussed in Chapter 3 as a way to assess the impact of some other imposed prior. The
chosen prior is p(μ) = 1, μ ∈[−∞:∞], meaning that a density of one is assigned for values

Assessing Model Quality
183
of μ along the entire real line. An important question is whether it is even possible to
achieve a comparative posterior distribution with such an improper prior. The posterior
here is proportional to the likelihood in the following simple manner:
π(μ|x) ∝p(x|μ)p(μ)
=
n
+
i=1
exp
'
−1
2σ2
0
(xi −μ)2
(
× 1
= L(μ|x),
(6.2)
which can be shown to be ﬁnite over the real line:
 ∞
−∞
L(μ|x)dμ =
 ∞
−∞
n
+
i=1
(2πσ2
0)−n
2 exp
'
1
−2σ2
0
(xi −μ)2
(
dμ,
(6.3)
with σ2
0 known. Therefore this prior leads to a posterior that is just the same conclusion
that a non-Bayesian likelihood analysis would produce. This is interesting because it is one
of a few cases where Bayesian analysis and non-Bayesian analysis agree perfectly, despite
starting from diﬀerent assumptions.
6.2.1.3
Example: Prior Sensitivity in the Analysis of the 2000 U.S. Election
in Palm Beach County
Returning again to the linear regression example of the voting results from the 2000
presidential election (see page 169 forward in Chapter 5 for the setup R code), we can
graphically compare the posterior implications of the uniform prior speciﬁcation versus the
conjugate prior developed in Section 5.1 as means of global sensitivity analysis.
Figure 6.2 shows the marginal posterior density for each posterior dimension (omitting
the constant) where the dotted line results from the conjugate prior and the solid line results
from the uniform prior. It is clear from these ﬁgures that there is very little diﬀerence in
the marginal posteriors for the two prior speciﬁcations with the possible exception of the
technology variable. Despite these similarities, it is important to note that there may be
substantive reasons to prefer one over the other in terms of what is being assumed about
voter behavior.
6.2.1.4
Problems with Global Sensitivity Analysis
To Leamer (1984, 1985) such analysis is “global” when the test of sensitivity to the form
of the prior includes a wide variety of prior forms. The motivation is that no single prior
can be assumed to fully describe prior knowledge or expert opinion about the distribution
of parameters, and therefore specifying a neighborhood of priors around the original gives
an important picture of the ramiﬁcations of uncertain prior information. Furthermore, if
in order to get a suitably narrow HPD region, the set of prior neighborhood speciﬁcations
had to be severely limited, then this would also be evidence of fragility.
Global sensitivity is often very diﬃcult to achieve in practice because it is not always

184
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
−65
−60
−55
−50
−45
−40
Technology
−0.50
−0.45
−0.40
−0.35
−0.30
−0.25
−0.20
New
0.13
0.14
0.15
0.16
0.17
Size
−0.11
−0.10
−0.09
−0.08
−0.07
−0.06
Republican
−0.07
−0.06
−0.05
−0.04
−0.03
White
14.5
15.0
15.5
16.0
16.5
17.0
Sigma squared
Dotted line from conjugate prior, solid line from uninformed prior
FIGURE 6.2: Marginal Posteriors, Palm Beach County Model
clear what qualiﬁes as a “neighborhood prior,” and what range of these alternative priors
constitutes suﬃcient testing criteria (O’Hagan and Berger 1988, Walley, Gurrin, and Burton
1996). This reduces to a diﬃcult decision as to what should be considered a reasonably wide
range of included prior speciﬁcations without including those that are highly unreasonable
for mathematical or substantive reasons. Sometimes global sensitivity analysis is just not
possible due to “an inability to suﬃciently reﬁne the usually subjective inputs of model,
prior, and loss” (Berger 1986a).
Furthermore, the primary diﬃculty is that global sensitivity applies to simultaneous
neighborhood generalization across all dimensions of the parameter vector.
There can
therefore be an enormous number and variety of speciﬁcations to consider and evaluate
with diﬃcult inclusion criteria. As Poirer notes, Leamer’s advice is “easier to preach than
to practice” (Poirer 1988, p.130). However, systematic Monte Carlo methods can consid-
erably ease this task (Canova 1994). One very creative approach to handling this problem
is the idea of “backward” sensitivity analysis or prior partitioning as advocated by Carlin
and Louis (2009, p.188-194). The idea is to ﬁx the posterior at some interesting or realistic
conﬁguration and see what types of priors are consistent with reaching this posterior given
the data.
6.2.2
Local Sensitivity Analysis
Local sensitivity analysis is the more modest and realizable process of making minor
changes in the prior parameterization while looking at the subsequent posterior eﬀects.
This has the advantage of realizing many of the beneﬁts of global sensitivity analysis such as

Assessing Model Quality
185
indicating posterior fragility, but at a much lower cost in terms of eﬀort and reporting. The
distinction between global and local sensitivity analysis is not as formal as the distinction
between global and local robustness evaluation studied below, but the central point is that
the basic form of the prior is not altered in local sensitivity analysis.
Generally modifying the parameters of the prior is done to produce a more diﬀuse form
of the prior than that used in the initial analysis. The associated argument is that if the
posterior does not appreciably change, then this is support for the initially speciﬁed form
of the prior as one that does not interject substantial subjective prior information into the
subsequent posterior. Naturally this is data-dependent and therefore application-speciﬁc.
6.2.2.1
Normal-Normal Model
Returning to the normal model example from Chapter 3 with variance known and a
normal prior on the unknown mean, we specify a prior with parameters N(m, s2). This
produces a normal posterior according to:
π(μ|X, σ2) ∼N
m
s2 + n¯x
σ2
0
 )  1
s2 + n
σ2
0

,
 1
s2 + n
σ2
0
−1
.
(6.4)
The sensitivity to the prior mean, m, can be analyzed by adding and subtracting one prior
standard deviation, m∗
1 = m −s, m∗
2 = m + s, and recalculating the posterior distribution
two more times using m∗
1 and m∗
2 separately as substitutes for m. It is clear from the form
of (6.4) that this provides a location shift of the posterior whose scaled distance is averaged
into the sample mean scaled by the known variance. In other words, if this distance is large
relative to information provided by the sample, the location shift of the posterior will be
large. Conversely, if this distance is small relative to sample information (n¯x), then the
mode of the posterior will be relatively stable.
From the form of the posterior distribution for μ in (6.4), we can also see that adding or
subtracting some ﬁxed quantity from the prior mean m does not aﬀect the posterior variance
in any way because the normal distribution is a location-scale family distribution, meaning
that one parameter is strictly a description of the measure of centrality, and the other strictly
a measure of dispersion (see Casella and Berger 2002, Chapter 3; Lehmann 1986, Chapter
1). Here conjugacy provides invariancy of this property, meaning that a conjugate location-
scale prior gives a location-scale form of the posterior. There are actually situations where
Bayesian invariance of location-scale family characteristics does not depend on conjugacy
(Dawid 1979).
If we want to alter the variance parameter of the prior in order to assess posterior
sensitivity, we need only to change the s2 prior parameter. A typical procedure for variance
parameters in this setting is to double or halve its value, s2∗
1 = s2/2, s2∗
2 = 2s2 and observe
the eﬀect on the resulting posterior. It is easy to see from the form of (6.4) that this change
alters the dispersion of the posterior, subject to the relative weighting of the variance
from the likelihood function; that is, the greater n is the less important the prior variance
speciﬁcation becomes (Lindley 1972).

186
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
6.2.2.2
Local Sensitivity Analysis Using Hyperparameter Changes
The normal-normal model described previously is actually quite a simple case in that as
a location-scale family distribution, the eﬀect of altering one parameter is independent of
the other. In more general speciﬁcations the distinction is not always as clear. For example,
in the beta-binomial model of Chapter 2 it is not obvious how one would alter the A and B
parameters in (2.13), except to note from posterior quantities on page 50 that it can make
a large diﬀerence. In such cases it is essential to employ substantive theoretical motivations
for specifying ranges of the parameter space.
Figure 6.3 shows the eﬀect of changes of the prior parameters of a model with an expo-
nential likelihood function and a conjugate gamma prior, as described in Section 4.3.1. The
posterior distribution for the unknown exponential PDF parameter is given by: π(θ|x) ∝
θ(α+n)−1 exp [−θ ( xi + β)] . In the ﬁrst panel, α is set at 10 and β, the “scale parameter,”
is varied to show the eﬀect on the resulting posterior distribution for θ for ﬁxed  xi and
n. In the second panel, β is ﬁxed at 4 and α, the “shape parameter,” is varied. Since the
posterior is ﬂattened and elongated for higher values of α and lower values of β, then it is
clear that arbitrary simultaneous changes in both α and β can be diﬃcult to interpret.
FIGURE 6.3: Exponential Model for θ with Gamma Prior
A better means of analyzing posterior sensitivity to changes in prior parameters that do
not have an immediate interpretation (unlike the normal model) is to relate these parameters
to the variance of the PDF or PMF of the prior. The variance of the prior distribution of θ
in the exponential-gamma model is α/β2, suggesting that we can vary this amount by some
proportional quantity and view the subsequent sensitivity of the posterior. Starting with

Assessing Model Quality
187
the prior parameter speciﬁcation α = 10, β = 2 we now vary the total prior variance by
50% in both directions by modifying either parameter singly. This exercise is demonstrated
in Figure 6.4 where the posterior for α = 10, β = 2 is located centrally in both panels with
the 95% HPD region shaded. The ﬁrst panel varies α and the second panel varies β, in
both cases enough to alter the prior variance by 50% in either direction.
••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••
••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••
FIGURE 6.4: Exponential Model Sensitivity
What we can see from this exercise is that the exponential-gamma posterior is relatively
insensitive to modest modiﬁcations of the prior speciﬁcation. If this were not true, Figure 6.4
would show dramatic changes in the location or dispersion of the posterior. This approach
to local sensitivity analysis is generally very easy to perform and often provides convincing
diagnostic evidence to support some researcher-generated prior speciﬁcation.
6.2.3
Global and Local Sensitivity Analysis with Recidivism Data
In criminology and the public policy study of the penal system a key concern is the
rate of recidivism: convicted criminals committing additional crimes after release from
prison (see the running discussion in Chapter 1). This example looks at state-level recidi-
vism data from Oklahoma over the period from January 1, 1985 through June 30, 1999.
Table 6.2 shows these data (by descending percentage) as collected by the Oklahoma De-
partment of Corrections (http://www.doc.state.ok.us), and provided in the R package
BaM as recidivism.
It is clear from the table that recidivism rates diﬀer according to the crime of original
conviction, but it is not unreasonable to assert that there also exist some underlying social

188
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
TABLE 6.2:
Recidivism by Crime, Oklahoma, 1/1/85
to 6/30/99
Crime Type
Released
Returned
Percentage
Unauthorized use of motor vehicle
1522
621
40.8
Burglary I
821
314
38.3
Burglary II
7397
2890
39.1
Larceny
9557
3525
36.9
DUI-2nd oﬀense
8891
3212
36.1
Murder I
182
65
35.7
Forgery
2529
882
34.9
Escape
2747
951
34.6
Robbery
3001
1035
34.5
Bogus check
1341
414
30.9
Weapons
1950
558
28.6
Bribery
7
2
28.6
Possession/Obtaining drugs
7473
2125
28.4
Assault
3133
848
27.1
Other nonviolent crimes
1637
429
26.2
Fraud
1239
307
24.8
Arson
439
106
24.2
Distribution of drugs
8775
2066
23.5
Embezzlement
804
188
23.4
Other violent crimes
1081
234
21.7
Rape
1269
262
20.7
Gambling
20
4
20.0
Manslaughter
846
167
19.7
Kidnapping
162
31
19.1
Murder II
271
51
18.8
Sex oﬀenses
2143
362
16.9
Drug traﬃcking
426
33
7.8
factors that aﬀect all rates. Although there are several obvious ways to look at these data,
we develop a simple Poisson model of the log of the recidivism rate (log of the Number
Returned variable). Actually four models are produced, three with diﬀerently parameterized
gamma priors producing the posterior: π(λ|X) ∝λ(α+ xi)−1 exp[−λ(β +n)], and one with
a bounded uniform prior producing the posterior: π(λ|X) ∝λ
 xi exp[−λn]. The posterior
results are summarized in Table 6.3 with quantiles (2.5, 50, 97.5) and the mean for the four
prior forms.
Table 6.3 shows that there is not much of a diﬀerence across the prior speciﬁcations,
including the uninformative uniform prior. This is evidence that the model is reasonably
insensitive to diﬀering conjugate prior parameterizations and to diﬀerent prior forms. Also,
the uniform prior can be considered a reference prior in the sense discussed in Chapter 4 and
therefore it appears that the conjugate gamma prior is not overly inﬂuential in determining
the posterior relative to the uniform reference. Further evidence is seen in Figure 6.5, which
shows 1,000 empirical replications from each of the posterior distributions in Table 6.3 using
a histogram and a smoothed density estimate.

Assessing Model Quality
189
TABLE 6.3:
λ Posterior Summary, Recidivism Models
Prior
2.5% Quantile
Mean
Median
97.5% Quantile
G(2, 2)
3.7765
4.4743
4.4630
5.2004
G(1, 4)
4.0534
4.7677
4.7644
5.4945
G(7, 1)
3.8328
4.4557
4.4369
5.1765
U(0, 100)
3.5177
4.1255
4.1019
4.8808
6.3
Robustness Evaluation
This section looks at two deﬁnitions of robustness: insensitivity to misspeciﬁcation of the
prior and insensitivity to inﬂuential data-points through the likelihood function. There is
some confusion about the language of robustness, resistance, and sensitivity analysis. Clas-
sical robustness focuses on the linear model’s sensitivity to inﬂuential outliers and seeks
to mitigate this eﬀect. There is now a vast literature on robust methods for identifying
and handling inﬂuential outliers in the linear modeling context (Andrews 1974; Andrews et
al. 1972; Barnett and Lewis 1978; Belsley, Kuh, and Welsch 1980; Cook and Weisberg 1982;
Emerson and Hoaglin 1983; Hamilton 1992; Hampel 1974; Hampel et al. 1986; Huber 1972,
1973, 1981; Rousseeuw and Leroy 1987), including some from an explicitly Bayesian context
(Bradlow, Weiss, and Cho 1998; Chaloner and Brant 1988; Guttman 1973; Guttman, Dut-
ter, and Freeman 1978; Pettit and Smith 1985). The more appropriate term for this work
is actually “resistance” reﬂecting the goal of making inferences more resistant to particular
data cases, and robustness is correctly deﬁned as low sensitivity to violations of underlying
model assumptions.
Bayesian robustness is somewhat broader than the classical approach.
The primary
objective of Bayesian robustness is to determine the posterior sensitivity to substantively
reasonable changes in model assumptions and thereby understand the context of uncertainty
introduced by the prior, the likelihood function, and if appropriate, the loss function. The
modern formal approach to Bayesian robustness, due primarily to Berger (1984), develops
a mechanical framework for altering assumptions and taking these assumptions through
the Bayesian inferential machinery to determine their posterior implications. Speciﬁcally, a
nonrobust model speciﬁcation is one in which “reasonable” changes in the prior, likelihood
function, or loss function, produce large changes in the range of the posterior quantity of
interest.
An associated goal is the identiﬁcation of data values that strongly inﬂuence posterior
inferences, and the complexity of this endeavor is greatly magniﬁed as the dimension of the
problem is increased. Generally computationally intensive tools are required for this analysis
(Bradlow, Weiss, and Cho 1998), and it is common that time-saving summary measures are
substituted for complete data expositions such as the AIC implications of subsetting (Kita-

190
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
Posterior Distribution with Gamma(2,2) Prior
2.5
3.0
3.5
4.0
4.5
5.0
5.5
6.0
0
50
100
150
200
250
Posterior Distribution with Gamma(1,4) Prior
2.5
3.0
3.5
4.0
4.5
5.0
5.5
6.0
0
50
100
150
200
250
Posterior Distribution with Gamma(7,1) Prior
2.5
3.0
3.5
4.0
4.5
5.0
5.5
6.0
0
50
100
150
200
250
Posterior Distribution with Uniform Prior
2.5
3.0
3.5
4.0
4.5
5.0
5.5
6.0
0
50
100
150
200
250
FIGURE 6.5: Comparison of Posteriors, Recidivism Model
gawa and Akaike 1982).
A radically diﬀerent, and substantially more complex, approach
performs a priori analysis to determine robustness properties from the initial assumptions
rather than as post hoc model adjustments. One such method by Seidenfeld, Schervish,
and Kadane (1995) develops a complete Bayesian decision-theoretic axiomatic construct
to derive preferences from utility and probability statements.
Therefore the robustness
properties are a direct result of the initial setup.
6.3.1
Global Robustness
The most common method for determining global robustness is the range of the posterior
quantity of interest (Wasserman 1992). For instance, we can see how much the bounds of
the 95% HPD region change under diﬀerent prior assumptions. Other statistics of obvious
interest include the posterior mean, various quantiles, and measures of dispersion. If modest
changes to a speciﬁed prior distribution, say varying its parameters somewhat, produce
dramatic changes in the range of these posterior quantities, then it is an indication that we
should be cautious about the use of this particular prior, or at least that we should report
this instability to our readers.

Assessing Model Quality
191
Berger (1984) introduced the idea of global robustness as an explicit recognition that
no single prior distribution can be shown to be absolutely correct and a “class” of priors
therefore provides more information about model uncertainty. The most frequent approach
uses the so-called ϵ-contamination neighborhood priors. Starting with a speciﬁed prior,
p0(θ), identify a wider class of “contamination” forms that includes this prior but broadens
the range of speciﬁc forms considerably: Q containing alternatives q(θ). It is also necessary
to give a subjective probability indicating how uncertain we are about the selection of p0(θ);
label this probability ϵ. The class priors evaluation is the subset of Q given by all forms of
p(θ) that satisfy:
Γ(Q, ϵ) = {p(θ): p(θ) = (1 −ϵ)p0(θ) + (ϵ)q(θ), q(θ) ∈Q}.
(6.5)
Thus Γ(Q, ϵ) is an ϵ-weighted compromise between the initial prior and the selected overar-
ching class of alternatives. The primary question, of course, is how do we deﬁne Q? Choices
include all possible contaminants (Berger and Berliner 1986), all symmetric and unimodal
forms (Basu 1994, Sivaganesan and Berger 1989), unimodality-preserving forms (Moreno
and Gonz´alez 1990), normals (Berger and Berliner 1986; see however, the warning in Lavine
1991a), more than one Q (Bose 1994a), mixtures (Bose 1994b), hierarchical classes (Moreno
and Pericchi 1993; Sivaganesan 2000), and quantile restraints on Q (Moreno and Cano 1991;
Moreno and Pericchi 1991, Ruggeri 1990).
Notice that ϵ is constant across the parameter space for θ, and it may be the case that
particular regions such as the tails have higher or lower researcher conﬁdence in p0(θ). To
accommodate this goal, a function, ϵ(θ) can be deﬁned so that prior uncertainty diﬀers over
Θ (Berger 1994; Liseo, Petrella and Salinetti 1996).
To show the eﬀects of diﬀerent contamination classes, Moreno (2000) gives the following
hypothetical analysis. Suppose the data consist of a single data point distributed N(θ, 1),
the prior is speciﬁed as N(0, 2), and we set ϵ = 0.2 (an atheoretic but common value in
such analyses). Consider three contaminants of the form: all distributions (q(θ) ∈QAll),
all distributions with the same median as the prior (q(θ) ∈QMedian), and all distributions
with the same quartiles as the prior (q(θ) ∈QQuartiles). The posterior quantity of interest is
the 95% HPD region for θ, and Table 6.4 shows the most extreme upper and lower bounds
reached over each contaminating class for ﬁve diﬀerent observed data values (calculations
are found in Moreno [2000, p.50]).
This example was selected because it illustrates several aspects of robust Bayesian anal-
ysis with ϵ-contaminations. First, notice that the more we restrict the contamination class,
the smaller the posterior range: the ranges decrease going right across the table.
This
means that there is a trade-oﬀin this analysis between generalizeability of the robustness
and practical utility: a very broad Q may lead to a large range and a very narrowly deﬁned
Q may lead to a small range, but neither one of these results tells as much as a broad class
leading to a narrow range or a narrow class leading to a broad range. Second, since the
data value is always greater than the prior mean here, the inﬁmum of the posterior HPD
regions is the more variable component, and this is true even as the data value gets larger

192
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
TABLE 6.4:
Oklahoma Recidivism 95% HPD
Regions, 1/1/85 to 6/30/99
x
QAll
QMedian
QQuartiles
0.5
[0.82:0.96]
[0.84:0.96]
[0.91:0.96]
1.0
[0.77:0.97]
[0.82:0.96]
[0.87:0.96]
2.5
[0.71:0.97]
[0.79:0.96]
[0.86:0.96]
3.0
[0.36:0.98]
[0.52:0.97]
[0.66:0.97]
4.0
[0.13:0.99]
[0.20:0.99]
[0.50:0.97]
reﬂecting variance scale uncertainty as well as location uncertainty. Finally, it is a clear
(and welcome) result that the data matter in all of this: the range increases as the data
point moves away from the prior mean.
Several problems often occur in performing this analysis in real data-analytic situations:
the speciﬁcation of the contamination class may be diﬃcult in cases where the obvious
choices are not reasonable, the calculations of posterior bounds can be foreboding in high-
dimension problems, and the results may unfortunately be inconclusive.
This is an active research area and there are well-developed alternatives to ϵ-contamination
forms, including those based on banding with standard CDF forms (Basu and DasGupta
1995), general quantile-restrained classes (Berger and O’Hagan 1988; O’Hagan and Berger
1988), quantile-restrained classes with robust prior properties (Moreno, Martinez, and Cano
1996), density-ratio classes (Lavine 1991b), Empirical Bayes (Maritz 1970; Morris 1983b),
asymptotic Bayes risk (Berger 1984), entropy (Dey and Birmiwal 1994), and concentration
functions (Regazzini 1992, Fortini and Ruggeri 2000).
6.3.2
Local Robustness
Because the scope of performing global sensitivity in a completely rigorous and thorough
manner is beyond the resources of many research projects, there is a motivation for a more
globally limited, but still informative, paradigm for assessing robustness in Bayesian models
(Basu, Jammalamadaka, and Liu 1996; Cuevas and Sanz 1988; Delampady and Dey 1994;
Gelfand and Dey 1991, Sivaganesan 1993). Local robustness evaluates the rate of change
in posterior inferences that occur due to very small (inﬁnitesimal) perturbations in the
prior (Ruggeri and Wasserman 1993; Gustafson 1996). Because this process uses derivative
quantities rather than integrals, it is often much easier to calculate than alternatives.
Deﬁne the locally perturbed prior to be a weighting of the original prior and the per-
turbance:
p∗(θ) = (1 −ϵ)p(θ) + (ϵ)q(θ),
(6.6)
where q(θ) is a single identiﬁed disturbance prior here (rather than one of a class as in global
robustness), and ϵ is not an expression of doubt about the original prior but a weighting

Assessing Model Quality
193
of how much to perturb with q(θ). Given the likelihood function, L(θ|x), the subsequent
ϵ-weighted (mixture) posterior is:
πϵ(θ|x) = (1 −ϵ)p(θ)L(θ|x) + (ϵ)q(θ)L(θ|x)
(1 −ϵ)mp(x) + (ϵ)mq(x)
(6.7)
(O’Hagan 1994). This is the standard nonproportional derivation of the Bayesian posterior,
but weighted by the marginal posterior densities of the data using the original prior (mp(x))
and the disturbance prior (mq(x)):
mp(x) =

Θ
p(θ)L(θ|x)dθ,
mq(x) =

Θ
q(θ)L(θ|x)dθ.
(6.8)
Further deﬁne, according to Gustafson and Wasserman (1995), the diﬀerence between the
ϵ-weighted posterior and the nondisturbed posterior as ϵ goes to zero:
D(q) = lim
ϵ→0(πϵ(θ|x) −p(θ)L(θ|x)/mp(x))
=
q(θ)L(θ|x)
mq(x)
−p(θ)L(θ|x)
mp(x)
 mq(x)
mp(x),
(6.9)
which gives ||D(q)|| as the local (inﬁnitesimal) posterior sensitivity from disturbing p(θ) by
q(θ), and under very general conditions can be thought of as the derivative of the posterior
with prior p(θ) in the direction of q(θ) (see Diaconis and Freedman 1986, especially p.13).
There are many possible choices for the disturbance prior, and Gustafson (2000, 75) gives
a tabularized list for the best-known examples. One advantage to this approach is that it
can readily be programmed in a language like R to cycle over many diﬀerent alternative
disturbance speciﬁcations, as well as to ﬁnd worst-case behavior or average behavior across
ranges of priors.
6.3.2.1
Bayesian Linear Outlier Detection
Outlier detection and robustizing is very straightforward in the Bayesian linear model.
The basic idea, from Chaloner and Brant (1988), Zellner and Moulton (1985), and Zellner
(1975), is to look at the modeled distribution of the error term after producing the poste-
rior. Returning to the linear speciﬁcation in Bayesian terms given in Section 5.1 with the
improper prior p(θ, σ2) = 1
σ , deﬁne the vector of residuals resulting from the insertion of
maximum likelihood into (5.5):
ϵ = y −Xˆθ.
(6.10)
The “hat” matrix (Amemiya 1985, Chapter 1) is:
H = X(X′X)−1X′,
(6.11)

194
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
where we are principally interested in the diagonal values, hii, of the H matrix.3
Outliers
are deﬁned to be those having residuals with high posterior probabilities of exceeding some
arbitrary cutoﬀon a normalized scale, k. For a candidate point i, identify cutoﬀpoints,
which identify the distance from the k-deﬁned tail and the posterior residual value in both
directions:
z1i = k −√σϵi
√hii
z2i = k + √σϵi
√hii
,
(6.12)
and the associated standard normal CDF values:
Φ(z1i) = Φ
k −√σϵi
√hii

Φ(z2i) = Φ
k + √σϵi
√hii

.
(6.13)
The probability of interest is thus the weighted posterior area of the support of the variance
term bounded by these cutoﬀpoints:
p(|ϵi| > kσ|X, y) =

(1 −Φ(z1i) −Φ(z2i))π(σ|X, y)dσ,
(6.14)
where π(σ|X, y) ∝σ−(n−k)−1 exp
	
−
1
2σ2 ˆσ2(n −k)

.
This quantity can be compared for
each data point to 2Φ(−k) where larger values are inferred to be high probability outliers
(Chaloner and Brant 1988, p.652).
The probability that a given point is an outlier is
therefore determined by either a large absolute model residual, |ϵi|, or a small hat value.
Polasek (1984) provides a diagnostic for the general least squares estimator with weights
given for the variance on the prior p(ϵ) ∼N(0, Σ), and a normal prior independent of Σ:
p(β) ∼N(B, S). Weighting in linear model analysis is very useful not only to compensate
for heteroscedasticity, but also as a diagnostic tool (Birkes and Dodge 1993; Carroll and
Ruppert 1988; the essays in Hoaglin, Mosteller, and Tukey 1983). The resulting setup from
Polasek gives the conditional posterior for β:
π(β|Σ, X, y)
= N

(S−1 + X′Σ−1X)−1(S−1B + X′Σ−1X), (S−1 + X′Σ−1X)−1
(6.15)
which is not only very easy to work with, but also gives a simple way to test local sensitivity
by changing the weights in the Σ matrix. Polasek gives the local sensitivity of the posterior
mean and precision by taking the derivative of these with respect to the prior precision on
3The hii values are termed “leverage points” by Cook and Weisberg (1982, p.15), and are particularly
useful in determining outlying values in a high-dimensional model where visualization is diﬃcult. These
values have several important characteristics: (1) the greater hii the more that the ith case determines the
relationship between y and ˆy, (2) the greater hii the smaller the variance of the ith residual, (3) deﬁning
the model standard error by s2 = ϵ′ϵ/(n −k) and therefore the ith contribution as (si)2 = (1 −hii)s2, then
the ith jackknifed residual (also called an externally studentized residual) is calculated from s(i), which is
an estimate of σ when the regression is run omitting the ith case: t(i) = ϵi/(s(i)
√1 −hii). This statistic
can also be thought of as the residual weighted inversely proportional to the jackknifed standard error.

Assessing Model Quality
195
ϵ:
∂
∂Σ−1
	
S−1 + X′Σ−1X

= −vec(X)vec(X)′(S−1 + X′Σ−1X)
∂
∂Σ−1
	
S−1 + X′Σ−1X)−1(S−1B + X′Σ−1X

= vec(X)(S−1 + X′Σ−1X)
× vec(y −X(S−1 + X′Σ−1X)−1(S−1B + X′Σ−1X))′,
(6.16)
which is long but not complicated. The advantage of this setup is that once programmed,
it is easy to run hypothetical priors through the matrix algebra and see posterior eﬀects
immediately. The unweighted local sensitivity of the ith data point is found by ﬁrst replacing
the Σ−1 matrix by an identity matrix substituting the ith one on the diagonal with the
corresponding weight from the general linear model, wi, to produce Wi. The resulting
unweighted estimate in the ith case is b(wi) = (X′WiX)−1X′Wiyi with the local sensitivity
given by ∂b(wi)/∂wi = (X′X)−1X′ei where ei is the standard OLS residual for the data
point i (Polasek 1984). This is a very simple setup and these priors can easily be developed
in R (see the exercises).
Outlier and inﬂuence analysis in Bayesian models is certainly not restricted to the linear
case. Kass, Tierney, and Kadane (1989) give outlier diagnostics for Bayesian models in gen-
eral based on asymptotic principles by jackkniﬁng out cases and looking at the subsequent
change in the posterior expectation of some statistic of interest. Weiss (1996) takes a simi-
lar approach but uses the Bayes Factor (Chapter 7) to understand the resulting inﬂuence.
Bradlow and Zaslavsky (1997) introduce a way of speeding up this process by importance
weighting (Chapter 9) approximations of full jackkniﬁng when the data size is large.
6.3.3
Bayesian Speciﬁcation Robustness
An important question is whether it is possible to construct a prior, or class of pri-
ors, that provide desired robustness qualities before observing the data and conducting the
inferential analysis. Lavine (1991a, 1991b) suggests a method for producing prior distribu-
tions that have similar support to some initially proposed prior, but possessing enhanced
robustness characteristics. Suppose that the multidimensional prior vector of coeﬃcients
can be segmented according to θ = [θ1, θ2] on Θ (the multidimensional sample space),
and we specify a joint prior consisting of the product of a marginal and conditional prior:
p(θ) = pc(θ2|θ1)pm(θ1). Lavine (1991b) ﬁnds four general classes of priors for the condi-
tional prior, pc, and the marginal or unconditional prior, pm. These classes are designated
Γc and Γm, and possess robust qualities for the forms: (1) quantile partitioning of prior
space, (2) the ϵ-contaminated class (Berger and Berliner 1986), (3) a density ratio form of
upper and lower limits, and (4) a density-bounded class. The primary advantage of this
approach is that the problem of ﬁnding tightly bounded posterior inferences for a large class
of priors reduces to a manageable sequence of linear optimizations (Lavine 1991b, p.401).

196
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
6.4
Comparing Data to the Posterior Predictive Distribution
Rubin (1984) recommends posterior predictive distributions because regardless of the
complexity of the model speciﬁcation, “model monitoring” is easily accomplished with sim-
ple summary statistics.
This idea integrates the two previous approaches of sensitivity
analysis and robust evaluation because it simultaneously tests for the two by looking at
posterior reasonableness through simulated data. The basic idea is that if the data generat-
ing mechanism implied by the calculated posterior is wildly divergent from the original data
or from data that we would expect given substantive knowledge, then the Bayesian analysis
is suspect. Furthermore, this idea can also be extended to predictive model selection (Laud
and Ibrahim 1995).
First consider a prior predictive distribution of a new data value, xnew before observing
the full dataset:
p(xnew) =

Θ
p(xnew, θ)dθ =

Θ
p(xnew|θ)p(θ)dθ.
(6.17)
In other words the marginal distribution of an unobserved data value is the product of the
prior for θ and the single variable PDF or PMF, integrating out this parameter. This makes
intuitive sense as uncertainty in θ is averaged out to reveal a distribution for the data point.
More usefully, from a diagnostic perspective, is the distribution of a new data point,
xnew after the full iid data set, x, has been observed: the posterior predictive distribution.
This is produced by:
p(xnew|x) =

Θ
p(xnew, θ|x)dθ =

Θ
p(xnew, θ|x)
p(θ|x)
p(θ|x)dθ
=

Θ
p(xnew|θ, x)p(θ|x)dθ. =

Θ
p(xnew|θ)p(θ|x)dθ.
(6.18)
The last simpliﬁcation comes from the assumption that xnew and x are independent. The
integral means that the posterior predictive distribution is the product of the single variable
PDF or PMF times the full data likelihood in which we integrate over uncertainty in θ to
result in a probability statement that is dependent on the observed data only.
For example, suppose we consider the now familiar example from Chapter 3 in which
X1, X2, . . . , Xn are distributed iid N(μ, σ2
0), and σ2
0 is known but μ is unknown. Placing a
normal prior on μ according to μ ∼N(m, s2) gives:
π(μ|x) ∝exp
⎡
⎢⎣−1
2
 1
s2 + n
σ2
0
 ⎛
⎝μ −

m
s2 + n¯x
σ2
0


1
s2 + n
σ2
0

⎞
⎠
2⎤
⎥⎦.
(6.19)

Assessing Model Quality
197
As a simplifying procedure, re-express this posterior in terms of its mean and variance:
π(μ|x) ∝exp
'
−1
2σ2
1
(μ −μ1)2
(
where: σ2
1 =
 1
s2 + n
σ2
0
−1
μ1 =

m
s2 + n¯x
σ2
0


1
s2 + n
σ2
0
 .
Therefore (6.18) for this model is:
p(xnew|x) =

μ
p(xnew|μ)p(μ|x)dμ
∝

μ
exp
'
−1
2
(xnew −μ)2
σ2
0
+ (μ −μ1)2
σ2
1
(
dμ.
(6.20)
This allows us to calculate summary statistics for the posterior predictive distribution:
E[xnew|x] = E[E(xnew|μ, x)|x]
= E[E(μ|x)]
= E[μ] = μ1.
Var[xnew|x] = E[Var(xnew|μ, x)|x] + Var[E(xnew|μ, x)|x]
= E[σ2
0|x] + Var[μ|x]
= σ2
0 + σ2
1/n.
(6.21)
These results demonstrate that the posterior predictive distribution in this example has the
same expected value as the posterior but greater variance, reﬂecting additional uncertainty
about an unobserved quantity rather than the posterior description of the observed quanti-
ties, x. Furthermore, since we know that the distribution for xnew is normal, then we have
the unconditional PDF:
xnew|x ∼N
⎛
⎝

m
s2 + n¯x
σ2
0


1
s2 + n
σ2
0
 , σ2
0 +
 1
s2 + n
σ2
0
−1 1
n
⎞
⎠,
(6.22)
which we can easily draw from.
Since the parametric form for additional observations is fully deﬁned by (6.18), multiple
draws can be made to form a simulated data set. If the replicated data have distributional
qualities noticeably diﬀerent from the observed data, then this is an indication of poor
model ﬁt (Gelman et al. 2003, Meng, 1994a). The posterior predictive distribution of the
data can also be used to make explicit model comparisons between considered alternatives
(Chen, Dey, and Ibrahim 2000).
■Example 6.2:
Economic Growth in Sub-Saharan Africa. This example makes
use of data provided by Bratton and Van De Walle (1997) from various authorita-
tive and reliable sources, made available through the Inter-University Consortium for

198
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
Political and Social Research (ICPSR 1996). The authors collect data on regime tran-
sition for 47 sub-Saharan countries over the period from each colonial independence
to 1989, with some additional variables collected for the period 1990 to 1994. These
data include 99 variables describing governmental, economic, and social conditions
for the 47 cases. Also included are data from 106 presidential and 185 parliamentary
elections, including information about political parties, turnout, and openness. We
use only the average annual rate of growth in GNP per capita, in percent. One case
is missing this value, so n = 46 here.
TABLE 6.5:
Sub-Saharan Africa Average Economic Growth
Rate, 1990-1994
Average Economic Growth by African Country, 1965-1989
Angola
-2.1
Benin
-0.1
Botswana
8.5
Burkina Faso
1.4
Burundi
3.6
Cameroon
3.2
Cape Verde
4.0
Ctrl. Afr. Repub.
-0.5
Chad
-1.2
Comoros
0.5
Congo
3.3
Cote d’Ivoire
0.8
Djibouti
NA
Eq.Guinea
0.0
Ethiopia
-0.1
Gabon
0.9
Gambia
0.7
Ghana
-1.5
Guinea
0.0
Guinea Bissau
-3.0
Kenya
2.0
Lesotho
5.0
Liberia
-2.0
Madagascar
-1.9
Malawi
1.0
Mali
1.7
Mauritani
-0.5
Mauritius
3.0
Mozambique
-8.2
Namibia
-1.2
Niger
-2.4
Nigeria
0.2
Rwanda
1.2
Saotome
1.5
Senegal
-0.7
Seychelles
3.2
Sierra Leone
0.2
Somalia
0.3
S.Africa
3.5
Sudan
-2.0
Swaziland
2.1
Tanzania
-0.1
Togo
0.0
Uganda
-2.8
Zaire
-2.0
Zambia
-2.0
Zimbabwe
1.2
We will start with the prior N(0, 4) for μ, which might be somewhat cynical, and set
σ2
0 = 7 as if it were known. In order to describe the posterior predictive distribution
we plot 5000 draws from the distribution given by (6.22), as shown in Figure 6.6.
In this ﬁgure the solid line is a density estimate from the draws and the dashed line
shows the prior distribution.
6.5
Simple Bayesian Model Averaging
Anything not known for certain in the Bayesian framework is treated probabilistically.
There is no reason that model speciﬁcation, which is rarely known for certain, cannot also be

Assessing Model Quality
199
Samples
Density
−10
−5
0
5
10
0.00
0.05
0.10
0.15
FIGURE 6.6: Posterior Predictive Distribution Draws, Africa Growth
treated as such. There are two key components to this approach: assigned priors to models,
and producing posterior distributions that are averaged across model space. This idea of
putting priors on models makes sense since we typically have information that supports
some speciﬁcations over others. For recent discussions see Montgomery and Nyhan (2010)
and Young (2009).
A starting point is the idea of a mixture distribution described in detail in Section 3.9
starting on page 87. We could create a likelihood based on mixtures by stipulating K distinct
parametric forms that might be used to explain the data generation process, weighting these
ω1, ω2, . . . , ωK, where  ωi = 1. We will stipulate that each of the parametric forms is from
the same parametric family, but diﬀers in parametric assignment. Thus fk(x|θk) is the kth
speciﬁcation for x with values given in the parameter vector θk. Now the likelihood for this
mixture is given by:
Lm(θ|x) =
n
+
i=1
K

k=1
ωkfk(xi|θk).
(6.23)
The likelihood function is therefore the product of the mixture sums for each data value.
Mixture models of this type are very ﬂexible (see Titterington, Smith, and Makov [1985]),
but suﬀer from the problem that lots of quantities need to be speciﬁed directly by the
researcher: the mixture proportions, and the full θ vector for each mixture component. So
for a mixture of ten normals, thirty constants need to be provided. For discussion of this

200
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
issue and implementation concerns in more complicated scenarios, see Diebolt and Robert
(1994), Kalyanam (1996), Stephens (2000), and West (1992).
Suppose now that we are choosing between: M1, M2, . . . , MK and for each k, determine
a model prior: p(Mk). Recall that the integrated likelihood is the denominator of Bayes’
Law:
p(x|Mk) =

p(θk|Mk, x)p(θk|Mk)
 
!"
#
likelihood×prior
dθk.
(6.24)
This lets us calculate the posterior model probability for each model, κ = 1 . . . K:
π(Mκ|x) =
p(x|Mκ)p(Mκ)
K
j=1 p(x|Mj)p(Mj)
.
(6.25)
This discussion uses a simple averaging scheme from Raftery (1995), but see also Hoeting
et al. (1999). Consider the posterior mean and variance for the jth coeﬃcient of the model
indexed by κth, θj(κ) and V arθj(κ). Now:
p(θj ̸= 0|x) =

θj∈Mκ
π(Mκ|x),
(6.26)
which is just the “posterior probability that θj is in the model,” as well as:
E[θj|x] ≈
K

κ=1
θj(κ)π(Mκ|x)
(6.27)
and:
Var[θj|x] ≈
K

κ=1
	
(Varθj(κ) + θj(κ)2)π(Mκ|x)) −E[θj|x]2
,
(6.28)
where we apply standard deﬁnitions to these new quantities. Note that the summation
in both the expected value and the variance calculations are both averages when (as by
convention) the prior model weights sum to one. Criticism of this approach, in particular
generation of values such as (6.26) and the subsequent summaries, is that the researcher
has made decisions about what variables to include in the various speciﬁcations and can
therefore increase or decrease expected values or variances by strategical selection. However,
since inclusions or exclusions are overt, readers can judge the quality of resulting averages.
6.6
Concluding Comments on Model Quality
The quality of a statistical model is a function of several criteria:
▷Parsimony
▷Completeness and generalizeability
▷Clarity

Assessing Model Quality
201
▷Robustness
▷Resistance
▷Predictive ability
▷Adherence to trends in the data.
General assessment of model quality is a vast literature in both Bayesian and classical statis-
tics; landmarks include Barnett (1973), Blalock (1961), Box (1980), Howson and Urbach
(1993), Leamer (1978), Miller (2002), and Russell (1929). Regretfully, recommended criteria
can be at odds with each other. For example, parsimony comes at the expense of closeness
to the data in that we can never reduce the overall ﬁt of the model by adding more explana-
tory variables, but this often creates overparameterized and theoretically confusing results
(Kreuzenkamp and McAleer 1995). In a familiar example, the linear model is well-known to
be fairly robust to mild deviations from the underlying Gauss-Markov assumptions (Shao
2005), but it is not very resistant to outliers in the data (Leamer 1984, Rao and Toutenburg
1995, Chapter 9).
Model checking is the general procedure for comparing ﬁnal conclusions with our expec-
tations or knowledge of reality. Obviously we will want to dismiss any model speciﬁcation
that produces inferences (Bayesian or otherwise) that do not comport with some sense of
substantive reasonableness. Where this arises most often is when certain model features
are speciﬁed for mathematical or data collection convenience. Suppose that a conjugate
prior is speciﬁed for a given model without a great amount of substantive justiﬁcation. A
reader is less inclined to be critical if it can be shown that the consequences of the posterior
distribution are in accordance with sound thinking in terms of the theoretical or situational
setting. Conversely, one should be wary of assumptions like conjugate priors applied merely
for analytical ease if they provide dubious substantive conclusions.
Model checking, through sensitivity analysis or robustness evaluation, is not a panacea.
Bartels points out that ad hoc nonsystematic sensitivity analysis can produce wildly wrong
statements about model quality (1997, pp.668-669). We should therefore see these tools in
a more general context as components of the broader task of checking that also includes
looking at the data, carefully considering the likelihood function used, and reasonableness
checks with posterior predictive distributions.
The Bayesian approach to model checking also circumvents a common problem in social
science data analysis. Typically when a social science model with a null hypothesis signif-
icance test is reported, it is presented as if only two models were ever considered or ever
deserved to be considered: the null hypothesis and the provided research hypothesis. The
quality of a research ﬁnding is then solely judged on the ability to reject the complementary
null hypothesis with a suﬃciently low p-value.
This process proceeds in an artiﬁcially exclusive manner regarding the selecting of a
model M: “. . . examining the data in x to identify the single ‘best’ choice M* for M, and
then proceeding as if M* were known to be correct in making inferences and predictions”
(Draper 1995, p.46). However, during the development of the reported model, many dif-

202
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
fering alternate mixes of independent variables are tested. This is an “illusion of theory
conﬁrmation” (Greenwald 1975; Lindsay 1995) because the null hypothesis signiﬁcance test
is presented as evidence of the exclusivity of explanation of this single research hypothesis.
Summary statistics are reported from the nth equation as if the other n −1 never existed
and this last model is produced from a fully controlled experiment (Leamer 1978, p.4). The
null hypothesis signiﬁcance test thus provides an inﬁnitely strong bias in favor of a single
research hypothesis against a huge number of other reasonable hypotheses: all other distri-
butional alternatives are assumed to have probability zero (Rozeboom 1960, Lehmann 1986,
68, Popper 1968, 113). Worse yet are solutions that rely upon atheoretic stepwise or other
mechanical search processes; for a litany and review, see Adams (1991), or Sala-I-Martin
(1997).
Two entirely reasonable and statistically signiﬁcant, competing models can lead to sub-
stantively diﬀerent conclusions using exactly the same data (Raftery 1995). This leads to
the question of determining which of the two equally plausible speciﬁcations is in some way
“better.” The answer is generally one in which the posterior is less sensitive to changes in the
underlying assumptions (robustness) and less sensitive to inﬂuential outliers (resistance).
The described process of sensitivity analysis makes small changes in the assumptions of the
model, such as the parameterization of the prior distribution, and observes whether or not
unreasonably large changes are observed in the subsequent posterior. We can therefore de-
termine which assumptions are worth further investigation, and possibly which conclusions
are more “fragile.” This illustrates how the Bayesian approach is more naturally inclusive
of alternatives and more overtly skeptical about assumptions.
6.7
Exercises
6.1
Derive the marginal posterior for β in
π(β|X, y) ∝

˜s + (β −˜β)′(Σ−1 + X′X)(β −˜β)
−n+a
2
,
where:
˜β = (Σ−1 + X′X)−1(Σ−1B + X′Xˆb)
˜s = 2b + ˆσ2(n −k) + (B −˜β)′Σ−1B + (ˆb −˜β)′X′Xˆb
from the joint distribution given by
π(β,σ2|X, y)
∝σ−n−a exp
'
−1
2σ2

˜s + (β −˜β)′(Σ−1 + X′X)(β −˜β)
(
.
See Chapter 5 (page 152) for terminology details.

Assessing Model Quality
203
6.2
Using the JAGS code in Example 6.1.1 starting on page 177, replicate Figure 6.1 on
page 180 with diﬀerent variance components priors. Is it possible to improve the
visual ﬁt? See Chapter 11 for details on running the JAGS software.
6.3
Derive the form of the posterior predictive distribution for the beta-binomial model.
See Section 6.4 starting on page 196.
6.4
On page 180 it was noted that the IQR of N(θ, 1) distribution matches that
of C(θ, 0.675). Find the Cauchy PDF with matching (0.05, 0.95) quantiles for a
N(θ, 2.5).
6.5
Calculate the Kullback-Leibler distance between two gamma distributions, f(X|α, β),
g(X|γ, δ), based on the same data but diﬀerent parameters.
6.6
Prove that the integral in (6.3) on page 183 is ﬁnite.
6.7
Calculate the Polasek (6.15) linear model diagnostic for each of the data points in
your model from Exercise 5.7. Graphically summarize your result identifying points
of concern.
6.8
Given an iid random sample X ∼N(θ, σ2), deﬁne a normal conjugate neighborhood
prior for θ as Gamma = {N(μ, τ −1) : 0 < τL < τ < τ U < ∞}. The posterior
expectation of θ is E[θ|X, μ, τ] = (nσ2 ¯X + τμ)/(nσ2 + τ). Derive the range of this
expectation.
6.9
In Section 6.2.2 it was shown how to vary the prior variance 50% by manipulating
the beta parameters individually. Perform this same analysis with the two parame-
ters in the Poisson-gamma conjugate speciﬁcation. Produce a graph like Figure 6.4
on page 187.
6.10
Show that the prior in (6.6) produces the posterior in (6.7) (page 192), and plot a
comparison for diﬀerence values of ϵ with simulated data.
6.11
The following data (firearm.deaths in BaM) are annual ﬁrearm-related deaths in
the United States per 100,000 from 1980 to 1997, by row (source: National Center
for Health Statistics, Health and Aging Chartbook, August 24, 2001):
14.9
14.8
14.3
13.3
13.3
13.3
13.9
13.6
13.9
14.1
14.9
15.2
14.8
15.4
14.8
13.7
12.8
12.1
Specify a normal model for these data with a diﬀuse normal prior for μ and a
diﬀuse inverse gamma prior for σ2 (see Section 3.5 starting on page 76 for the more
complex multivariate speciﬁcation that can be simpliﬁed). In order to obtain local
robustness of this speciﬁcation, calculate ||D(q)|| (see page 193) for the alternative
priors: q(μ) = N(14, 1), q(μ) = N(12, 3), q(σ−2) = G(1, 1), q(σ−2) = G(2, 4).

204
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
6.12
Berger (1990) gives four important features for a class of priors from expert elici-
tation: (1) simple elicitation and interpretation, (2) including as many reasonable
forms as possible, (3) no unreasonable forms that are precluded by measurement or
functional form, and (4) easy calculations. Find a published example that violates
at least one of these principles.
6.13
Calculate the posterior predictive distribution of X in Exercise 6.11 assuming that
σ2
0 = 0.75 and using the prior: p(μ) = N(15, 1). Perform the same analysis using
the prior: C(15, 0.675). Describe the diﬀerence graphically and with quantiles.
6.14
Returning to the Africa economic growth data from Example 6.4, notice that
Botswana appears to be an outlier at 8.5% growth. Calculate the probability of
getting this value or higher from the prior distribution given in the example and
the posterior predictive distribution calculated. Make a histogram of the posterior
predictive distribution and draw a vertical bar at the value for Botswana.
6.15
Calculate the posterior predictive distribution for the Palm Beach County model
(Section 5.1.1 starting on page 148) using both the conjugate prior and the unin-
formative prior.
6.16
The cross-validation (also called the leave-one-out) posterior predictive distribution
is given by:
p(xnew|x−i) =

Θ
p(xnew|θ)p(θ|x−i)dθ.
where x−i denotes the dataset removing the ith case (Stern and Cressie 2000).
A
small value of p(xnew|x−i) means that this new value will occur infrequently in the
absence of xi. Further deﬁne:
A = 1
n
n

i=1
log (p(xnew|x−i)) ,
which is smaller for better ﬁtting models and asymptotically equivalent to the AIC
for iid samples (Stone 1977a, 1977b). Replicate the plot from Example 6.4 using
the cross-validation posterior predictive distribution. Observe any diﬀerences from
the original analysis. Calculate the A statistic.
6.17
Using the model speciﬁcation in (6.1), produce a posterior distribution for appropri-
ate survey data of your choosing. Manipulate the forms of the priors and indicate
the robustness of this speciﬁcation for your setting. Summarize the posterior results
with tabulated quantiles.
6.18
Calculation of the posterior predictive distribution in cases where the integral can-
not be calculated or standard parametric forms are not used can be challenging.
Fortunately, the common use of MCMC estimation in Bayesian modeling makes
this task easier. Suppose X and Y are bivariate normal with mean vector (0, 0)

Assessing Model Quality
205
and covariance matrix
	 1 ρ
ρ 1

, with ρ = 0.75. Write a Gibbs sampler in R that draws
from the full conditional distribution:
X|Y ∼N(ρy, 1 −ρ2) = ρy +

1 −ρ2N(0, 1)
Y |X ∼N(ρx, 1 −ρ2) = ρx +

1 −ρ2N(0, 1).
and show graphically that the marginal distributions are as expected.
Take a
bivariate sample of n = 100 and treat this as observed data. Now estimate ρ as if
unknown and produce a sample of posterior predictive values from this result. Do
they diﬀer from the Gibbs sampling output?
6.19
Leamer (1983) proposes reporting extreme bounds for coeﬃcient values during the
speciﬁcation search as a way to demonstrate the reliability of eﬀects across model-
space. Thus the researcher would record the maximum and minimum values that
every coeﬃcient took on as diﬀerent model choices were attempted. Explain how
this relates to Bayesian model averaging.
6.20
Observed data of size n are known to be classiﬁed into k = 1, . . . , K groups and
distributed xk ∼N(θk, s2
k) where the θk parameters are distributed N(μ, σ2). In
this hierarchical setting (continued in Chapter 12) posterior conclusions about θ and
μ are known to be sensitive to the prior treatment of the random eﬀects standard
deviation σ and many approaches have been suggested. DuMouchel and Normand
(2000) suggest a uniform prior for s0/(s0 + σ) where s2
0 is the harmonic mean of
the group variances: ( 1
K
 s−2
k )−1. Show that this means that p(σ) = s0/(s0 + σ)2
and p(σ2) = s0/(2σ(s0 + σ)2).


Chapter 7
Bayesian Hypothesis Testing and the Bayes
Factor
7.1
Motivation
This chapter systematically explores Bayesian hypothesis testing, and describes the
Bayes Factor as the evidence of the quality of one model speciﬁcation over another. We
start with a general discussion of the state of hypothesis testing in the social and behavioral
sciences and outline where Bayesian posterior descriptions can replace a number of prob-
lematic practices. The emphasis will be mainly on using Bayesian tools for determining
the strength of evidence, rather than the details of the many diﬀerences between Bayesian
and non-Bayesian approaches. For a very clear, and succinct, discussion of the historical
developments in this regard, see Marden (2000).
The material in this chapter assumes knowledge of generalized linear models, although
basic likelihood theory is brieﬂy reviewed. Appendix A contains a review of McCullagh and
Nelder (1989) style GLM theory. In particular we will see their treatment of the exponential
family form when discussing the structure of prior distributions. Excellent and accessible
works on likelihood theory abound. Among the most commonly assigned are Berger and
Wolpert (1988), Cramer (1994), Edwards (1992), Eliason (1983), and King (1989). There
are also an increasing number of texts on generalized linear models.
Dobson (1990) is
particularly accessible. The classic and enduring work is McCullagh and Nelder (1989),
and more recent additions include Lindsey (1997) and Fahrmeir and Tutz (2001).
More topically, this is a chapter about model testing: determining why we should pre-
fer one model speciﬁcation over another. Obviously practically inclined social science re-
searchers have many potential speciﬁcations to choose from, and it turns out that the
Bayesian paradigm has powerful and sophisticated tools for comparing alternatives. The
core advantage over non-Bayesian approaches is that everything unknown is given a proba-
bility assessment, including model choices. This facilitates clear comparisons and stands in
direct contrast to the methods in the last chapter, which focused on model adequacy for a
single speciﬁcation.
Bayesian hypothesis testing is often less formal than the non-Bayesian varieties. By far,
the most common procedure for summarizing results in social science research is to simply
describe the posterior distribution rather than to apply a rigid decision process. However,
207

208
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
Bayesian decision theory (Chapter 8) is a well-developed area, particularly in those ﬁelds
where costs, risks, and consequences in general are measurable (Cyert and DeGroot 1987).
More basically, the Bayes Factor described in this chapter allows a very general and directly
implementable means of model comparison that is already popular in the Bayesian social
sciences (Meeus, et al. 2010, Pang 2010, Quinn, Martin, and Whitford 1999, Raftery 1995,
Sened and Schoﬁeld 2005, Zhang and Luck 2011).
The null hypothesis signiﬁcance testing paradigm that currently dominates tests of sta-
tistical reliability in the social sciences is seriously defective and widely misunderstood.
A ﬁxation with arbitrary thresholds and unjustiﬁed frequentist assumptions has damaged
empirical research in many ﬁelds for quite some time (Bakan 1960, Cohen 1994, Hunter
1997, Meehl 1978, Pollard 1993, Rozeboom 1960, Serlin and Lapsley 1993, Schmidt 1996).
The use of p-values and “stars” (asterisks on tables) as evidence for or against the null
hypothesis is ﬂawed in many social science studies since the long-run probabilistic assump-
tion of Neyman-Pearson, upon which these measures are built, generally does not apply to
single-point cross-sectional studies with uniquely occurring data. Social scientists employ
standard Neyman-Pearson frequentist standards and practices without meeting the key un-
derlying assumption of long-run replicability, and therefore average over unobserved and
unlikely events.
There is a lengthy but frustrated literature attempting to reconcile frequentist and
Bayesian approaches with the idea that the resulting inferences should be the same if one
could just carefully restrict the a priori model speciﬁcations (Bartholomew 1965; Casella
and Berger 1987a; Berger, Brown, and Wolpert 1994; Berger, Boukai, and Wang 1997; De-
Groot 1973; Good 1983a, 1992; Good and Crook 1974; Jeﬀreys 1961; Pratt 1965). Detailed
and sophisticated works have still produced only limited success in a series of special cases
that are not of general interest.
Also, there is much less controversy about Bayesian inference and Bayesian hypothesis
testing than there was a few decades ago. This is because of an increasing recognition
that the Bayesian approach, which treats all unknown quantities with distributional state-
ments, is closer to common scientiﬁc intuition. Social and behavioral scientists typically
do not have the unending stream of iid data that canonical Neyman-Pearson frequentist
inference requires. Consider that the standard conﬁdence interval means that on average
19 times out 20, for α = 0.05, the interval covers the true parameter value. Yet we gen-
erally have only one sample of observational data, often collected by others. Also recall
that maximum likelihood analysis is equivalent to a Bayesian setup with the appropriately
bounded uniform distribution prior. It is also true that the two approaches lead to identi-
cal inferences asymptotically for any proper prior distribution speciﬁcation (i.e., the data
will eventually overwhelm the prior knowledge). Rather than treat the previous facts as a
comforting reason to continue advocating traditional likelihoodist practices, along with the
accompanying null hypothesis signiﬁcance test, we observe that Bayesians have shown that
uniform prior speciﬁcations can lead to a number of problematic posterior results. The fact
that uniform priors are not invariant under nonlinear transformation was critical to Fisher’s

Bayesian Hypothesis Testing and the Bayes Factor
209
wholesale (and vitriolic) rejection of Bayesian inference (Fisher 1930; 1956, Chapter 2). In
addition, there are many situations where uniform priors are unreasonable starting points
like elections with two major party candidates and a third fringe candidate. Why would
a researcher pretend that these three have equal probability of winning the election before
collecting data?
7.2
Bayesian Inference and Hypothesis Testing
Bayesian modes of inference can be divided into two basic approaches: one in which
basic descriptions of the posterior are provided as evidence of some eﬀect (as we have been
doing up until this point), and one in which explicit testing mechanisms are performed.
The second approach is due mainly to Jeﬀreys (1961). Often posterior description and
articulated testing are not provided as exclusive demonstrations of evidence of some eﬀect,
but are given in conjunction.
When there are multiple competing model speciﬁcations
arising either from theoretical propositions or from alternative speciﬁcations of the same
theory, a set of posterior distributions is produced, requiring some method for comparison.
The most straightforward is the ratio of the posterior probability of some speciﬁcation
relative to another.
This posterior odds ratio gives the odds of one model relative to
another and is called the Bayes Factor (discussed in detail in this chapter).
7.2.1
Problems with Conventional Hypothesis Testing
The standard process for hypothesis testing in the social sciences is an odd mix that can
be called quasi-freqentist. Suppose we observe X1, X2, . . . , Xn iid f(x|θ), where θ is some
unknown value on the parameter space Θ. A one-sided (non-nested) test is deﬁned by:
H0: θ ≤0
vs.
H1: θ > 0,
(7.1)
and a two-sided (nested) test is similarly deﬁned by:
H0: θ = 0
vs.
H1: θ ̸= 0.
(7.2)
In the standard setup used in empirical social and behavioral science analysis, a test statistic
(T ), some function of θ and the data, is calculated and compared with its known distribution
under the assumption that H0 is true. Commonly used test statistics are sample means
( ¯X), chi-square statistics (χ2), and t-statistics in linear (OLS) regression analysis. The test
procedure assigns one of two decisions (D0, D1) to all possible values in the sample space of
T , which correspond to supporting either H0 or H1, respectively. The p-value (“associated
probability”) is equal to the area in the tail (or tails; we will illustrate with a one-tailed
discussion for now) of the assumed distribution under H0 (θ = 0 ﬁxed), which starts at the

210
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
point designated by the placement of T on the horizontal axis and continues to inﬁnity:
p(x) = p(T (x) ≥T |θ = 0) =
 ∞
T
f(t|θ = 0)dt.
(7.3)
The sample space of T is segmented into two complementary regions (S0, S1), whereby the
probability that T falls in S1, causing decision D1, is either a predetermined null hypothesis
cumulative distribution function (CDF) level: the probability of getting this or some lower
value given a speciﬁed parametric form such as normal, F, t, etc. (α = size of the test,
Neyman and Pearson 1928a, 1928b, 1933a, 1933b), or the cumulative distribution function
level corresponding to the value of the observed test statistic under H0 is reported (i.e., the
p-value =

S1
PH0(T = t)dt, Fisher [1925a]).
There are many criticisms of the use of p-values in empirical work. Hwang et al. (1992)
point out that since the p-value is the density under the null hypothesis starting at the test
statistic and continuing to inﬁnity on the support for θ, that is an average over unlikely
sample values that have not actually occurred. In addition, Berger and Wolpert (1984) note
that this deﬁnition violates the likelihood principle (Birnbaum 1962) that inferences must
come from observed, not hypothetical, data. Casella and Berger (1987a) observe that there
is nothing “frequentist” about a p-value since it is not the probability of a Type I error.
Koop (1992) shows that classic frequentist analysis with p-value evidence fails to provide
evidence of unit root problems in time-series analysis. While Bayesian time-series models
are not within the main scope of this text (see Pole, West, and Harrison [1994] as a good
starting point), it should be mentioned that asymptotic analysis for the unit root problem
is a serious problem in the non-Bayesian setting and is well-behaved for Bayesian models, a
qualitative diﬀerence that is even greater when the estimated parameters occupy restricted
space.
A core problem (there are many) with the null hypothesis signiﬁcance test as practiced
is that researchers pretend to select α levels a priori as in experiments based on Neyman-
Pearson, but actually report p-values (or worse yet, ranges of p-values indicated by asterisks)
as the strength of evidence: quasi-frequentism.
This is because the social sciences are
encumbered with Fisher’s arbitrary thresholds (even he later recanted), despite the fact
that there has never been a theoretical justiﬁcation to support 0.01, 0.05, and 0.10 levels.
Aitkin (1991) observes that atheoretic use of ﬁxed test sizes leads to an “unreasonable test
in completely speciﬁed models,” and Barnard (1991, discussion of Aitkin) points out that
these conventions originated from the lack of ready computing (i.e., the propagation of
Fisher’s tables). Because the test is typically performed once on a set of social data in time
and will not reoccur in the same fashion, the reported p-value is not a long run frequentist
probability. Furthermore, since only one model speciﬁcation is tested (at least as far as the
reader ever gets to know!), an inﬁnite number of alternate speciﬁcations are not ruled out.
A second problem with the null hypothesis signiﬁcance test that pertains directly to
research in applied settings such as public policy analysis is that there is no explicitly
modeled consequence of making the wrong decision (Pollard and Richardson 1987). Unlike

Bayesian Hypothesis Testing and the Bayes Factor
211
purely academic research, decisions taking place in policy analysis and implementation
have direct consequences for citizens, employees, managers, and agencies in general. Yet
hypothesis testing confuses inference and decision making since it “does not allow for the
costs of possible wrong actions to be taken into account in any precise way” (Barnett 1973).
Decision theory (Raiﬀa and Schlaifer 1961) is the logical adjunct to hypothesis testing that
formalizes the cost of alternatives by explicitly deﬁning the cost of making the wrong decision
by specifying a loss function and associated risk for each alternative (Berger 1985, Pollard
1986). These principles are discussed on Chapter 8. Despite the utility of this extension to
settings where decision making is required, it is rare to see applications in policy studies.
What makes this surprising is that loss functions have a natural role in applied settings
since obvious asymmetries occur in political and social decision-making: peace versus war,
election victory versus loss, social group acceptance or rejection, and so on.
7.2.1.1
One-Sided Testing
One-sided Bayesian hypothesis testing for a speciﬁed parameter is fairly basic and a
Bayesian version of the standard p-value can be produced once the posterior distribution
is obtained. For the simple one-sided case in (7.1), the speciﬁed prior distribution of θ
provides an a priori probability over the two regions of the sample space of θ: H0: p(−∞<
θ ≤0) = π0, H1: p(0 < θ < ∞) = π1 = 1 −π0. While this can take on an obviously large
number of forms, the uninformative uniform distribution is particularly useful, and many
authors have suggested that lacking speciﬁc information π0 = p(H0 is true) = 1
2 is a useful
value (Berger and Sellke 1987, Jeﬀreys 1961).
Once prior probabilities are assigned, the Bayesian posterior probability is derived
from the non-normalized region deﬁned by the null hypothesis divided by the total non-
normalized region, which can be derived as follows:
p(H0|x) =
 ∞
−∞
p(H0, θ|x)dθ
=
 ∞
−∞
p(x|H0, θ)p(H0, θ)
p(x)
dθ
=
 ∞
−∞
p(x|H0, θ)p(H0, θ)dθ/p(x)
=
 ∞
−∞p(x|H0, θ)p(H0, θ)dθ
 ∞
−∞[p(x|H0, θ)p(H0, θ) + p(x|H1, θ)p(H1, θ)]dθ
=
 0
−∞p(x|θ)π0dθ
 0
−∞p(x|θ)π0dθ +
 ∞
0
p(x|θ)π1dθ
=
 0
−∞p(x|θ)π0dθ
 ∞
−∞p(x|θ)π0dθ ,
(7.4)

212
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
where the part of the integral in the numerator from 0 to ∞contributes zero to this cal-
culation since H0 is on the right-hand-side of the conditionals (and the same logic holds
for the H1 part). The terms inside the integrals are modiﬁed using the deﬁnition of con-
ditional probability: p(x|H0, θ)p(H0, θ) = p(x|H0, θ)p(θ|H0)p(H0) = p(x|H0, θ)p(θ|H0)π0.
In the last step, the simpliﬁcation occurs only if π0 = π1 (otherwise the denominator is a
weighting of the two integrals seen in the penultimate state).
More generally, this is just the slice of the density that corresponds to the one-sided
restriction deﬁning the null hypothesis calculated over the posterior distribution.
This
posterior probability, while slightly more diﬃcult to construct than the standard p-value,
is far more useful because it is the value that many people mistake a p-value for: the
probability that the null hypothesis is true, given the data and the model. Conversely, the
standard p-value is the far less revealing probability of seeing these or more extreme data,
given the model and an assumed true null hypothesis.
Casella and Berger (1987a) showed that when X is generated by a symmetric location
density with monotone likelihood ratio1 (a condition greatly aided by the central limit theo-
rem as the data set size increases), and if the prior distribution of θ is symmetric about zero,
then inf π(H0|X) ≤p(x), where the inﬁmum (parameter or structural minimization, see
Gill [2006]) is taken over the class of suitable priors. Yet there is no theoretical justiﬁcation
for picking the prior distribution that leads to the inﬁmum over any other justiﬁable prior.
Also, in the less common cases (at least with relatively large-n social science research) where
the sampling density of X does not have a monotone likelihood ratio, then p(H0|X) < p(x).
In fact, Casella and Berger’s proof shows that frequentist p-values are radically biased
against the null. For example (Berger and Sellke 1987, p.113, Casella and Berger 1987a,
p.110), if a random variable X distributed N(θ, 1) is observed to be 1.645, then the one-
sided p-value is 0.05. However, for all prior distributions assigning mass of 1
2 at zero and
1
2 elsewhere, inf π(H0|x = 1.645) = 0.21. This example demonstrates that concentrating
non-zero mass on point null position (zero here) leads to unreasonable (and downwardly
biased) posterior inferences, and are thus not “impartial” expressions of prior ignorance.
And it gets worse for the p-value. Casella and Berger also show that equality of the p-value
and the Bayesian posterior quantile is achieved under the same circumstances but with the
extreme constant pseudo-density improper prior that gives constant density for all values
in ℜ; yet a persistent frequentist criticism of Bayesian inference is the use of these improper
priors in estimation as unreasonable probability constructs.
■Example 7.1:
Example: One-Sided Testing with French Labor Strike Data
One characteristic of labor strikes in France is imitative behavior by unions: news
of other strikes can stimulate additional strikes by signaling that the conditions are
amenable. Conell and Cohn (1995) look at French Third Republic coal mining strikes
1Suppose we have a family of probability density functions h(t|θ) in which the random variable t is
conditional on some unknown θ value to be tested. This family has a monotone likelihood ratio if for every
θ1 > θ2, the corresponding h(t|θ1)
h(t|θ2) is a nondecreasing function of the random variable t.

Bayesian Hypothesis Testing and the Bayes Factor
213
with particular attention to follow-on strike behavior by unions. Their data are given
in Table 7.1
TABLE 7.1:
French Coal Strikes, by Year
1902
1906
1912
1914
1919
1921
1923A
1923B
1926
1930
1933
9
8
13
23
15
23
13
6
13
15
10
There are two periods assigned to 1923 because there were two distinct “salary of-
fensives” during this year. Since these are counts, it is natural to consider a Poisson
model for the data. However, the Poisson model assumes that the mean and variance
are equal and this is not the case here. Consequently we specify a negative binomial
model with a Jeﬀreys prior.
Here we use an equivalent variant of the negative binomial PMF as that given in
Appendix B:
NB(y|r, p) =
r + y −1
y

pr(1 −p)y,
(7.5)
where the interpretation is that y represents the number of failures before reaching
the rth success. Recall that the Jeﬀreys prior is calculated from the negative ex-
pected value of the second derivative of the log-likelihood: (−EX|θ
d2
dθ2 log f(x|θ))
1
2 =
r
1
2 p−1(1 −p)−1
2 . This turns out here to be the kernel of a beta distribution with
parameters (0, 1/2), which is not strictly an allowable parameterization of the beta
(both parameters are constrained to be positive). Interestingly, this does not harm
the inference process in this case, although alternatively we could specify a Pois-
son/gamma model. In fact, from Table 4.1 we know that the beta distribution is the
conjugate prior for the negative binomial. The resulting posterior is therefore also
beta distributed for known r:
π(p|y, r) ∝r
1
2 p−1(1 −p)−1
2 pnr(1 −p)
 yi
n
+
i=1
r + yi −1
yi

∝pnr−1(1 −p)
 yi−1
2
∼BE

nr,

yi + 1
2

.
(7.6)
An initial or follow-on strike is considered a “failure” in the model and the corre-
sponding “success” is an end to the series of strikes for that period. Therefore for the
purpose of this analysis, we set r = 1. The hypothesis of interest is:
H0: p ≤0.05
H1: p > 0.05,
meaning that the posterior probability of a cessation to the series of strikes is one in

214
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
0.00
0.05
0.10
0.15
0.20
Support
0.1715
Density
FIGURE 7.1: One-Sided Testing for the French Strikes Data
twenty or less under the null. By (7.4):
p(H0|y) =
 0.05
0
π(p|y)p(p)dp
 1
0 π(p|y)p(p)dp
= 0.171.
(7.7)
This fraction is depicted in Figure 7.1 where the shaded region represents the numer-
ator and the entire PDF is the denominator. We could integrate to get this quantity
but it is actually easier, and just as accurate, to simulate the result. In fact it is
nearly trivial in R: we just randomly generate 1,000,000 values distributed according
to beta(nr,  yi + 1
2) and count the proportion that are less than 0.05 (actually not
this many simulated values are needed for an accurate estimate but the calculation is
extremely fast).
7.2.1.2
Two-Sided Testing
While the standard, semi-frequentist approach naturally accommodates both one-sided
and two-sided hypothesis tests, the Bayesian framework does not. While one-sided hypoth-
esis testing is very straightforward in Bayesian hypothesis testing, two-sided hypothesis
testing is quite diﬃcult and remains fairly controversial in practice. This is not surprising
since a Bayesian is likely to be uncomfortable placing prior mass on a point null hypothesis
(H0: θ = 0, for example). While this appears to be a ﬂaw in the Bayesian construct, it
is actually an indication of how much more reasonable the approach is: nobody actually
believes that some parameter of interest is exactly zero. Instead, most researchers are ei-
ther truly interested in a directional conclusion (direct and regular communication reduces
hostility, countries ruled by dictators are more likely to go to war, smaller classes lead to
better student performance, etc.), or whether some eﬀect size is approximately equal to zero.

Bayesian Hypothesis Testing and the Bayes Factor
215
Despite the evidence that point null hypothesis testing is antithetical to the Bayesian phi-
losophy, there has been considerable eﬀort expended trying to ﬁnd a reconcilable Bayesian
approach (Berger and Sellke 1987, Berger, Brown, and Wolpert 1994; Berger, Boukai, and
Wang 1997, Lehmann 1993, Meng 1994a, Rubin 1984). Furthermore, unlike the non-nested
case, posterior probability quantiles in the nested case are often substantially diﬀerent than
frequentist p-values (Lee 2004).
In testing H0: θ = 0 versus H1: θ ̸= 0, we cannot assign a continuous prior distribution
for θ since this would assign zero mass at the null point, thus providing an inﬁnite bias
against the nesting. One alternative is to specify a small interval around the null point,
creating a focused null region: H0: −ϵ ≤θ ≤ϵ.
7.2.2
Attempting a Bayesian Approximation to Frequentist Hypothesis
Testing
There is not a general manner in which evidence from Bayesian posterior quantiles can
be calibrated with p-values since the two measures are fundamentally diﬀerent in theory:
 ∞
T
p(θ)L(θ|x)dθ ̸= p = p(T (x) ≥T |θ = 0) =
 ∞
T
f(x|θ = 0)dx
(7.8)
for some α-driven critical value T and some test statistics T (x) (Casella and Berger 1987b,
p.133; Hinkley 1987, p.128). This does not mean that for a given frequentist model some
Bayesian parameterization that is forced to coincide cannot be found.
Particular cases
include Severini (1991, 1993) for HPD regions, Stein (1965) in a repeat-sample context,
DiCiccio and Stern (1994) in a multivariate setting, Thatcher (1964) for the binomial,
Chang and Villegas (1986) for the multivariate normal, and Nicolaou (1993) for dealing
with nuisance parameters.
Since there does not exist a default prior that is subsumed to any subsequently ob-
served posterior except in the limit, then any Bayesian setup designed to agree with quasi-
frequentist results is by deﬁnition a subjective assessment of the structure of the data. In
addition, the notion of a Bayesian p-value analog has been described as a “paradox” (Meng,
X-L. 1994a) since the quasi-frequentist averages over data that do not exist compared to
the Bayesian approach of averaging over the allowable parameter space.
The ﬁrst attempt to develop a Bayesian procedure that agrees with a two-sided classical
test is that of Lindley (1961). If the prior information is suﬃciently vague so that one has no
particular belief that θ = θ0 versus θ = θ0 ± ϵ, where ϵ is some small value, then a reference
(ignorance-expressing, Chapter 4) prior can be used to obtain a posterior, and H0 is rejected
for values that fall out of the (1 −α)100 HPD region. Highest posterior density regions are
preferred over credible intervals for asymmetric distributions since credible intervals simply
space out a speciﬁed distance from the mean regardless of overlying density.

216
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
7.3
The Bayes Factor as Evidence
Bayes Factors have dominated the literature on Bayesian model testing because they are
often easy to calculate and have a naturally intuitive interpretation (likelihood ratio tests
are a special case). The central notion is that prior and posterior information should be
combined in a ratio that provides evidence of one model speciﬁcation over another. Bayes
Factors are also very ﬂexible in that multiple hypotheses can be simultaneously compared.
Moreover, model nesting is not required in order to make comparisons, addressing a major
deﬁciency with classical approaches (Cox 1961).
The most general form of the Bayes Factor can be described as follows. Suppose we
observe data x and wish to test two competing models, M1 and M2, relating these data
to two diﬀerent sets of parameters, θ1 and θ2. This is a problem of deciding between two
families of density speciﬁcations:
M1: f1(x|θ1)
M2: f2(x|θ2)
(7.9)
where θ1 and θ2 are either nested within a larger set of alternative parameters, Θ, or drawn
from distinct parameter spaces, Θ1 and Θ2. The standard Bayesian setup speciﬁes a prior
unconditional distribution for the parameter vectors: p1(θ1) and p2(θ2), and therefore a
prior probability of the two models: p(M1) and p(M2). The posterior odds ratio in favor of
Model 1 versus Model 2 are therefore produced by Bayes’ Law:
π(M1|x)
π(M2|x)
 
!"
#
posterior odds
= p(M1)/p(x)
p(M2)/p(x)
 
!"
#
prior odds/data
×

θ1 f1(x|θ1)p1(θ1)dθ1

θ2 f2(x|θ2)p2(θ2)dθ2
 
!"
#
Bayes Factor
.
(7.10)
So the quantity of interest turns out to be the ratio of marginal likelihoods (page 41) from
the two models. This expression equates the posterior odds ratio on the left-hand side to
the product of the prior odds ratio and the ratio of integrated likelihoods. Note that with
fairly complicated models, the integrals in (7.10) can be quite challenging to compute, even
with the Markov chain Monte Carlo procedures introduced starting in Chapter 10. By
rearranging we get the standard form of the Bayes Factor, which can be thought of as the
magnitude of the evidence for Model 1 over Model 2, contained in the data:
BF(1,2) = π(M1|x)/p(M1)
π(M2|x)/p(M2),
(7.11)
which is also called the posterior to prior odds ratio for the obvious reason revealed in this
form. In the case where we are willing to put equal prior probability on the two models
(p(M1) = p(M2) = 1
2) and the models share the same parameter space but hypothesize
diﬀering levels, then the Bayes Factor reduces to the common likelihood ratio.
This is
equivalent to assigning simple point mass through the priors. It is also possible to rearrange

Bayesian Hypothesis Testing and the Bayes Factor
217
(7.11) since the p(x) is the same for both models:
π(M1|x)/p(M1)
π(M2|x)/p(M2) = π(M1, x)/(p(x)p(M1))
π(M2, x)/(p(x)p(M2))
= π(M1, x)/p(M1)
π(M2, x)/p(M2) = π(x|M1)
π(x|M2),
(7.12)
which gives another general form provided by some authors. Commonly, the natural log of
the Bayes Factor is calculated for reasons of numerical stability.
Bayes Factors are also transitive in that multi-way comparisons are relative. So if we
have BF(1,2) and BF(2,3), then:
BF(1,2)BF(2,3) = π(M1|x)/p(M1)
π(M2|x)/p(M2)
π(M2|x)/p(M2)
π(M3|x)/p(M3) = BF(1,3),
(7.13)
which is useful for multiple model comparisons using the same data. This property also
means that if there exists a null model, then a series of alternatives can be tested against
it and the resulting values are comparable on the same relative scale.
Bayes Factors do not have an inherent scale, exactly in the manner that likelihood
ratios do not either. A fundamental criticism of Bayes Factors is that because they lack
an underlying metric, all results are therefore arbitrary and subjective. This is not quite
right since they are an overt relative comparison of model ﬁt. Clearly we would rather see
extremely large or extremely small values of the Bayes Factor since that indicates obvious
superiority of one speciﬁcation over another.
While the Bayesian approach typically eschews arbitrary decision thresholds, Jeﬀreys
(1961, p.432) gives the following typology for comparing Model 1 versus Model 2:
BF(1,2) > 1
model 1 supported
1 > BF(1,2) ≥10−1
2
minimal evidence against model 1
10−1
2 > BF(1,2) ≥10−1
substantial evidence against model 1
10−1 > BF(1,2) ≥10−2
strong evidence against model 1
10−2 > BF(1,2)
decisive evidence against model 1,
where Model 1 is assumed to be a null model.
Kass and Raftery (1995) modify these
categories slightly and provide a more intuitive logarithmic scale for decision criteria (also
discussed in Raftery 1996). Note that there is no explicit “acceptance” or “rejection” of
hypotheses as in the Neyman-Pearson context. Instead the Bayes Factor (or the log of
the Bayes Factor) is considered simply the weight of evidence for Model 1 over Model 2
provided by the data, given the prior and the model speciﬁcation (Good 1985).
Good
points out elsewhere (1980b) that this is not a very new idea since Pierce ﬁrst used “weight
of evidence” in comparing hypotheses as early as 1878, and Turing (a contemporary of
Jeﬀreys) in 1940 used the expression “factor in favor of a hypothesis” (reported in Good
1972, p.15) to mean nearly the same thing. Karl Pearson also uses this phraseology, but in
a less formal comparative manner (1914 and elsewhere).

218
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
It is important to remember that Jeﬀreys’ typology is still an arbitrary designation of
levels. However, under speciﬁc circumstances the Bayes Factor relates directly to standard
posterior quantities. For instance, if we are willing to take the classical stance that there are
only two plausible alternative hypotheses, then it follows that p(H0|data) + p(H1|data) = 1
(we could also denote this p(M1|x) + p(M2|x) = 1 if we wanted to be speciﬁc that each
hypothesis is represented by a model). Starting with this we can use Bayes’ Law and the
deﬁnition of the Bayes Factor to produce:
p(H0|data) = 1 −p(H1|data)
= 1 −p(data|H1) p(H1)
p(data)
= 1 −p(data|H0)
BF(1,2)
p(H1)
p(data)
= 1 −
1
BF(1,2)
'p(data)
p(H0) p(H0|data)
( p(H1)
p(data)
=
'
1 +
1
BF(1,2)
p(H1)
p(H0)
(−1
.
(7.14)
Naturally other posterior quantities can be related in similar fashion to the Bayes Factor
as well. This result shows that the posterior probability of the null hypothesis is a function
of the Bayes Factor scaled by the ratio of priors, and highlights quite clearly the strong
inﬂuence that prior speciﬁcations can have on Bayesian hypothesis testing.
Lavine and
Schervish (1999) provide cautionary advice when extending the Bayes Factor beyond such
basic comparisons.
7.3.1
Bayes Factors for a Mean
Consider a simple setup where X ∼N(μ, σ2), where the population mean μ is unknown
and the population variance σ2 is known. We are interested in a two-sided test of H1: μ = μ0
versus H0: μ ̸= μ0 (where μ0 is often 0). We specify a normal prior under the research
hypothesis (H1) with mean m and variance s2. A sample of size n is collected with mean
¯x. This leads to the Bayes Factor:
BF(H1,H0) =

1 + ns2
σ2

exp
'
−n
2
 1
σ2 −
1
σ2 + ns2

(¯x −μ0)2
(
(7.15)
(Exercise 6). Despite the simplicity of this calculation, there is plenty to be uncomfort-
able about. First, testing a point null hypothesis is not really a Bayesian operation since
μ0 is unknown and therefore should be assigned a prior distribution as well (as noted in
Section 7.3.5). As soon as we want to assign this prior the idea of a single point ceases to
make obvious sense. There are ways to use the Dirac Delta function as a surrogate for such
a distribution, but these are not very intuitive from an inferential sense. The next obvious

Bayesian Hypothesis Testing and the Bayes Factor
219
alternative is to substitute a small region around the point of interest for the single point,
but this leads to some additional noted challenges.
7.3.2
Bayes Factors for Diﬀerence of Means Test
This section develops the Bayesian version of the standard Student’s t-test for nor-
mal data that uses a Bayes Factor. For additional details and an example, see G¨onen et
al. (2005). Surprisingly little has been done to adapt this standard tool to Bayesian use.
Suppose we are interested in the two-sided test:
H0 : μ1 = μ2
H1 : μ1 ̸= μ2,
with common variance σ2 in the two groups. First we need to specify the prior distribution
of the eﬀect size (diﬀerence) to be tested. We will say that under the hypothesis of a non-
zero diﬀerence, the standardized diﬀerence, |μ1 −μ2|/σ has prior mean δ and prior variance
σ2
δ. Note that this allows great ﬂexibility for the test to be performed. Next calculate the
standard diﬀerence of means test statistic:
t =
¯x1 −¯x2

(n1−1)s2
1+(n2−1)s2
2
n1+n2−2
 1
2 /√nδ
(7.16)
where nδ = (n−1
1
+ n−1
2 )−1, and the degrees of freedom are ν = n1 + n2 −2. The Bayes
Factor for H0 over H1 (large values favoring the null) is:
BF(0,1) =
Tν(t|0, 1)
Tν(t|δ√nδ, 1 + nδσ2
δ),
(7.17)
where Tν(t|A, B) denotes the value that results from plugging t into a non-central t-
distribution PDF with ν degrees of freedom and parameters A for location and B
1
2 for
scale (see Johnson, Kotz, and Balakrishnan 1994).
G¨onen et al. (2005) point out that
this is easily implemented in R by ﬁrst determining whether the following terms can be
identiﬁed:
pv =
/
1 + nδσ2
δ
ncp = δ√nδ
pv
,
which is easy to implement. The Bayesian version of the diﬀerence of means test diﬀers
noticeably from the non-Bayesian variant mainly in that we get to specify the tested eﬀect
size prior, which is an important advantage.
7.3.3
Bayes Factor for the Linear Regression Model
An obvious and useful application for the Bayes Factor is the standard linear regression
model where we want to compare two, not necessarily nested, diﬀerent right-hand-side
speciﬁcations in y = Xβ + ϵ, where X is an n × k, rank k matrix of explanatory variables
with a leading vector of ones, β is a k × 1 unknown vector of coeﬃcients, y is an n × 1

220
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
vector of outcomes, and ϵ is a n × 1 vector of residuals with N(0, σ2I) for a constant σ2
(homoscedasticity). On page 145 the likelihood function for model j is:
Lj(βj, σ2
j |Xj, y) = (2πσ2
j )−n
2 exp

−1
2σ2
j
(y −Xjβj)′(y −Xjβj)

(7.18)
where j = 0, 1 providing models M0 and M1.
Notice that y is not indexed here since
both models intend to explain the structure the same outcome variable. Again, make the
deﬁnitions ˆb = (X′X)−1X′y, and ˆσ2 = (y −Xˆb)′(y −Xˆb)/(n −k).
Now specify possibly diﬀerent conjugate priors for each of these models with kj columns
of X according to:
p(βj|σ2) = (2π)−
kj
2 |Σj|−1
2 exp
'
−1
2(βj −Bj)′Σ−1
j (βj −Bj)
(
,
and:
p(σ2
j ) ∝σ−(aj−kj)
j
exp

−bj
σ2
j

(7.19)
as done on page 151 except for a multiplier hj on the variance term in the normal prior for
βj: Σj = hjσ2
j I. If we make the common choice of prior mean for β to be B = 0 in both
models, then the marginal likelihood for model j from this setup is:
pj(y|Xj, Mj) =
|X′
jXj + h|−1
2 |hj|
1
2 baj
j Γ(aj + aj
2 )
π
n
2 Γ(aj)

2bj + (n −kj) ˆσj
2
.
(7.20)
This means that the Bayes Factor for Model 1 over Model 0 is given by:
BF(1,0) = p1(y|X1, M1)
p0(y|X0, M0) =
|X′
1X1+h|−1
2 |h1|
1
2 ba1
1 Γ(a1+ a1
2 )
π
n
2 Γ(a1)

2b1 + (n −k1) ˆσ1
2
|X′
0X0+h|−1
2 |h0|
1
2 ba0
0 Γ(a0+ a0
2 )
π
n
2 Γ(a0)

2b0 + (n −k0) ˆσ0
2.
(7.21)
This is a long expression but a relatively simple form due to the elegance of the linear
model.
■Example 7.2:
Bayes Factors for a Model of Election Surveys. For generalized
linear models with dichotomous outcome variables, it is common to specify a logit or
probit link function as described in Appendix A. That is, g−1(Xβ) is either the logit
function, Λ(Xβ), or the standard normal CDF, Φ(Xβ). So for outcome variable Yi,
and prior distribution p(β) on the coeﬃcients, we obtain the following posterior:
π(β|X, Y) =
p(β) n
i=1 g−1(Xβ)yi(1 −g−1(Xβ))1−yi

β p(β) n
i=1 g−1(Xβ)yi(1 −g−1(Xβ))1−yidβ .
(7.22)
When the prior is some numerical constant (i.e., not a function of the β) then it passes
out of the integral in the denominator and cancels. This then becomes equivalent to
the classical model described in every econometric book ever printed (only a slight

Bayesian Hypothesis Testing and the Bayes Factor
221
exaggeration). However, in general (7.22) is not available in closed form for most
prior speciﬁcations and MCMC techniques are generally required (see Chapter 12 for
speciﬁc applications of this model). Assume for the moment that we speciﬁed a simple
multidimensional uniform prior for p(β) and that we wish to calculate a Bayes Factor
for comparing one coeﬃcient vector against another: H0: β = β0 vs. H1: β = β1.
Determining the weight of evidence for Model 2 versus Model 1 here is performed
simply by inserting β0 and β1 separately into (7.22) and calculating the Bayes Factor
according to BF(1,0) = π(β1|X, Y)/π(β0|X, Y). This is actually just the likelihood
ratio for Model 2 over Model 1. This process is relatively general in that we can
test diﬀering speciﬁcations as well as restricted coeﬃcient vectors versus unrestricted
estimates.
TABLE 7.2:
1964 Electoral Data
N
F
L
W
IND
DEM
WR
WD
SD
109
0.102
-2.175
9.984
0
0
0
0
0
35
0.115
-2.041
3.562
1
0
0
0
0
33
0.214
-1.301
5.551
0
1
0
0
0
75
0.258
-1.056
14.358
0
0
1
0
0
50
0.544
0.176
12.403
1
0
1
0
0
52
0.677
0.740
11.731
0
1
1
0
0
70
0.606
0.431
16.713
0
0
0
1
0
56
0.890
2.091
5.482
1
0
0
1
0
189
0.975
3.664
4.607
0
1
0
1
0
31
0.727
0.979
6.153
0
0
0
0
1
56
0.893
2.122
5.351
1
0
0
0
1
344
0.990
4.595
3.406
0
1
0
0
1
Hanushek and Jackson (1977) develop a grouped logit model for Factor data based
on a clever estimation approach suggested by Theil (1970) in which the link function
is applied to the outcome variable, then a grouped data form of least squares is used
to obtain coeﬃcient estimates.
All generalized linear models automatically imply
interaction eﬀects because of the link function.2 But when Theil’s method is used,
these automatic interaction eﬀects are not provided, so if we believe that such eﬀects
2To see that this is true, calculate the marginal eﬀect of a single coeﬃcient by taking the derivative of a
GLM speciﬁcation, which does not explicitly contain an interaction term, E(Yi) = g−1(β0 +β1Xi1 +β2Xi2),
with regard to a variable of interest. If the form of the model implied no interactions, then we would obtain
a marginal eﬀect free of other variables, but this is clearly not so:
∂Yi
∂Xi2 =
∂
∂Xi2 g−1(β0 + β1Xi1 + β2Xi2) =
(g−1)′(β0 + β1Xi1 + β2Xi2) β2. The calculation demonstrates that the presence of a link function requires
the use of the chain rule and therefore retains other terms on the right-hand side in addition to β2, and
therefore we always get for a given variable partial eﬀects that are dependent on the levels of the other
explanatory variables.

222
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
exist, they must now be explicitly provided in the model speciﬁcation. The data come
from a 1964 election survey in the United States regarding the presidential election
(available in BaM). The outcome variable is the log-ratio of the group proportion
voting for Lyndon Johnson and the binary explanatory variables are: self-indicated
indiﬀerence to the election (IND), a stated preference for Democratic party issues
(DEM), and indications of party status as weak Republican (WR), weak Democrat (WD),
or strong Democrat (SD). The full data set is given in Table 7.2, which is a replication
from Hanushek and Jackson.
The vector N is the number of cases in the group, the vector F is the observed
cell proportion voting for Johnson, L is log-ratio of this proportion given by L =
log[F/(1 −F)] (see Theil [1970, p.107]) for a justiﬁcation), and W collects the inverse
of the diagonal of the matrix for the group-weighting from [NiFi(1−Fi)]. The uniform
prior coeﬃcient estimate is produced by
b = (X′WX)−1XW
L,
(7.23)
along with the asymptotic variance-covariance matrix (X′WX)−1, and log-likelihood
ℓ(b) = −(n/2) log(2π) −(n/2) log(s2) −(1/(2s2))ϵ′ϵ.
(7.24)
Table 7.3 gives the results for the simple model in Hanushek and Jackson and a second
model that hypothesizes an interaction between indiﬀerence and weak Republican.
TABLE 7.3:
1964 Election Survey, Grouped Data
Hanushek and Jackson
Intercept Model
Coeﬃcent
Std. Error
Coeﬃcent
Std. Error
Intercept
-2.739
0.062
-2.709
0.063
IND
1.145
0.061
0.952
0.099
DEM
2.167
0.063
2.187
0.063
WR
1.598
0.080
1.477
0.095
WD
3.459
0.090
3.465
0.091
SD
4.049
0.121
4.083
0.122
(IND)(WR)
0.456
0.214
What these results show is that the interaction term is statistically reliable according
to traditional levels and that the rest of the model is not substantively changed. So
which speciﬁcation is better? The Bayes Factor for the interaction model relative to
the Hanushek and Jackson model is 0.944, indicating little support for adding the in-
teraction according to Jeﬀreys’ typology (given of course the unsupported assumption
of uniform priors).

Bayesian Hypothesis Testing and the Bayes Factor
223
7.3.4
Bayes Factors and Improper Priors
From (7.10) it is easy to see that the form of the prior has a noticeable eﬀect on the
resulting Bayes Factor. This sensitivity to the prior is a main criticism of Bayes Factors in
general (see Kim 1991). Interestingly, the form of the prior has a much greater eﬀect on
the Bayes Factor than other forms of Bayesian inference such as quantile descriptions of the
posterior or posterior predictive distribution (Aitkin 1991; Gelman, Meng, and Stern 1996).
In standard Bayesian analysis, a substantial similarity between the prior and the posterior
is evidence that the data had much less of an impact than the prior. This easily detected
situation would be cause for alarm. For instance, in the case where a conjugate prior is
speciﬁed, if the shape of the posterior distribution is very close to the prior, then we know
that the beginning assumptions are relatively unmodiﬁed by conditioning on the data.
In the case where improper priors are used, the Bayes Factor cannot be speciﬁed except
under very uninteresting scenarios3 (Berger and Mortera 1999, p.542; Kass and Wasserman
1995 [discussion], p.777). The most common improper prior setup is to specify one or both
of the densities proportional to a multiplicative constant:
p1(θ1) = c1g1(θ1)
p2(θ2) = c2g2(θ2),
(7.25)
where g1 and/or g2 are functions whose integrals over the respective sample spaces do
not diverge. A very common improper prior speciﬁcation is a constant, p(θ) = k, over the
Lebesgue measure on (−∞, ∞) (see Chapter 4, Section 4.4.4). One way to visualize this is as
a rectangle that is k = g(θ) high and c = ∞wide. Obviously c is an inﬁnite proportionality
constant, but it typically does not prevent the calculation of a proper posterior distribution
since:
π(θ|x) =
p(θ)p(x|θ)

θ p(θ)p(x|θ)dθ
=
cg(θ)p(x|θ)
c 
θ g(θ)p(x|θ)dθ
=
g(θ)p(x|θ)

θ g(θ)p(x|θ)dθ .
(7.26)
To see that this is not true for Bayes Factors, take the form from (7.10) and substitute
priors with speciﬁed proportionality constants according to (7.25) to obtain:
BF(1,2) = c1
c2

θ1 g1(θ1)p(x|θ1)dθ1

θ2 g2(θ2)p(x|θ2)dθ2
.
(7.27)
So for any two proper priors (or improper priors that are ﬁnite and proportional to proper
priors) the Bayes Factor can still be calculated. However, if both of c1, c2 are unbounded,
3Improper priors can be assigned to unknown nuisance parameters that are common to both models.
Therefore they are not applicable to the parameters that motivate the model comparison in the ﬁrst place.

224
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
then the Bayes Factor is incalculable because the ratio of two diﬀerent unknown unbounded
quantities does not cancel out.
Returning to the problematic case of the two-sided (nested) Bayesian hypothesis test,
this section treats the problem with Bayes Factors instead of posterior distribution quantiles.
This case represents the widest gulf between the frequentist and the Bayesian approaches,
where tests to coincide can result in diﬀerences greater than an order of magnitude (Berger
and Sellke 1987, Casella and Berger 1987a, Lindley 1957). While the easiest solution to
the problems of two-sided testing is to dismiss this setup, as atheoretical and impractical,
avoiding such nested problems is unreasonable in practice since linear regression, easily the
most commonly used statistical procedure in the social and behavioral sciences, nests a null
hypothesis of a zero coeﬃcient value within the full sample space.
7.3.4.1
Local Bayes Factor
Smith and Spiegelhalter (1980) and Spiegelhalter and Smith (1982) developed an in-
genious way to solve the problem posed in (7.27) for nested linear models and known
variances using an idea based loosely on Atkinson (1978), Geisser and Eddy (1979), and
Lempers (1971, Chapter 6). Suppose we “imagine” a set of data as a training sample and
solve for c1
c2 in (7.27):
BF0

θ2 g2(θ2)p(x0|θ2)dθ2

θ1 g1(θ1)p(x0|θ1)dθ1
= Est.
'c1
c2
(
(7.28)
(the order of the fraction with integrals changes as it moves to the left-hand-side of the
equality). The choice of the training data, x0, is done with the objective of ﬁnding the
smallest data set that gives proper posterior distributions in (7.28) and so that the simplest
model is most favored: BF0(x) is maximized for this speciﬁcation over the other. So that the
second criterion is not perpetuated into the next stage of the test, Smith and Spiegelhalter
set BF0(x) = 1 with the theoretical justiﬁcation that if the training model is truly minimal,
then the value of one is a good estimate. Since the imaginary data set is conjured to support
the model in the numerator to the greatest extent possible, this value of one is actually a
conservative value as it is now the lower bound on BF0(x).
The ﬁnal Bayes Factor is thus a product of the training “prior” for c1
c2 and the Bayes
Factor for the rest of the data:
BF(1,2) = Est.
'c1
c2
( 
θ1 g1(θ1)p(x|θ1)dθ1

θ2 g2(θ2)p(x|θ2)dθ2
(7.29)
(note that the order of the fraction is returned because it is back on the right-hand-side
of the equality).
Therefore by this method, we remove the non-identiﬁability problem
associated with the undeﬁned ratio of normalizing constants and obtain a solution for the
Bayes Factor. Pettit (1992) applies this method to nested linear models with outliers to
judge the sensitivity of Bayes Factors to outliers when specifying improper priors (which
necessarily put more weight on the model with more outliers). Adman and Raftery (1986)
apply local Bayes Factors (i.e., one that compares a speciﬁc model, M1 with closely related

Bayesian Hypothesis Testing and the Bayes Factor
225
models contained in a super-set model M2 where closeness is deﬁned as those alternatives
that give a high weight for producing similar coeﬃcient estimates)4 to a nonhomogeneous
Poisson process using bounded improper priors (Adman and Raftery 1986, Raftery and
Adman 1986), noting that this case is more diﬃcult with linear models since there are data
conditions that can provide maximal support for the null resulting in an undeﬁned ratio of
constants.
While this idea of a minimal training sample providing a prior estimate of the incal-
culable quantity is very creative, two obvious problems exist.
The ﬁrst is the ﬁxing of
B0(x) = 1, a decision that lacks any theoretical justiﬁcation other than the relatively vague
idea that it is likely to be close in an optimal situation (for which there is no guarantee and
no test). The second is that determination of the training sample is diﬃcult both in terms
of selecting the sample size (Lempers [1971] arbitrarily picked half of the full sample as the
size of the training sample) and the sample components (there are
 n
m

ways of picking a
training sample of m out of a full sample of n). For example, consider a state of nature
that is maximally malevolent in that the training sample produces a premultiplier in (7.29)
that is as diﬀerent from the rest of the Bayes Factor calculation as possible. Since all of the
integration in (7.29) is done over the sample space of θ, then the sample size of the training
sample is equally weighted with the rest of the presumably much larger sample, and the
ability to change the ﬁnal Bayes Factor is thus substantial.
7.3.4.2
Intrinsic Bayes Factor
Berger and Pericchi (1996a, 1996b) propose a method of picking the training sample
that depends on the number of possible training samples being relatively small.
Their
“intrinsic” Bayes Factor is an average of the Bayes Factors from every combinatorically
possible training sample. In cases where the number of possible training sets is prohibitively
large (i.e., almost every realistic scenario in the social sciences), then a random sample of
the possible training samples can be used.
To create the intrinsic Bayes Factor, Berger and Pericchi ﬁrst start with intrinsic prior
densities ν(θ1) and ν(θ2).
An intrinsic prior is uninformative prior in the sense that it
provides the asymptotic equivalence of the Bayes Factor to the maximum likelihood ratio.
For details see Berger and Pericchi (1996a) or Berger and Mortera (1999). Next deﬁne a
minimal training sample, xm, the smallest possible subset of the full sample, xn, so that at
least one of the resulting posterior densities using the intrinsic priors are proper. Calculate
these minimum training set posterior distributions: πI
i (θi|x) ∝ν(θi)fi(xm|θi). The intrinsic
Bayes Factor is then deﬁned to be:
BI(x) = pI
1(θ1|x)
pI
2(θ1|x)AV E
'πI
2(θ2|x)
πI
1(θ1|x)
(
,
(7.30)
where AV E[ ] is some average: arithmetic mean, geometric mean, harmonic mean, median,
4Smith and Spiegelhalter (1980) demonstrate a linkage between local Bayes Factor comparisons and
Akaike information criterion (AIC) comparisons, see p.232.

226
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
etc., over all possible minimal training samples.
This avoids the previously mentioned
problem of inadvertently selecting a maximally skewed training sample. Furthermore, since
the average selected does not have to be a mean, some other robustizing measure of central
location can be used so that the procedure is robust to outliers. Unfortunately, selection
of the averaging procedure can also make a substantive diﬀerence in the resulting Bayes
Factor, and Berger and Pericchi do not provide any theoretical justiﬁcation for one form
over another.
In general, intrinsic Bayes Factors are more appropriate for two-sided tests than one-
sided tests, since there is no explicit incorporation of direction (see Moreno 1997). Berger
and Pericchi identify intrinsic priors for large classes of nested models and speciﬁcation, but
found that they are not generally appropriate for developing Bayes Factors in non-nested
models.
The challenge in developing the intrinsic Bayes Factor is obtaining the appropriate in-
trinsic prior distributions for each of the tested models. While Berger and Pericchi tabulate
some common forms, most situations will require separate derivation.
However, Berger
et al. (1998) ﬁnd a number of invariant speciﬁcations where the marginal density of the
minimal training set is actually available in closed form analytically.
7.3.4.3
Partial Bayes Factor
O’Hagan (1995) attempts to repair the diﬃculties in specifying the local Bayes Factor
in two ways, beginning with “partial Bayes Factors.” The procedure is quite simple. First
divide the sample into two components, x = (y, z), where y is the training sample of size
m, and z is the sample proportion of size n −m used for model comparison. Use y to
obtain training posteriors for the alternative models: π1(θ1|y) and π2(θ2|y) by assigning
any desired prior for θ (including an improper prior) and using π(θ|x) ∝p(θ)p(x|θ). These
training posteriors will necessarily be proper, and are used as priors for calculating the
Bayes Factor for the rest of the sample:
BF(z|y) =

θ1 π1(θ1|y)f1(z|θ1, y)dθ1

θ2 π2(θ2|y)f2(z|θ2, y)dθ2
,
(7.31)
where:

θi
πi(θi|y)fi(z|θi, y)dθi =

θi
πi(θi)fi(x|θi)dθi
) 
θi
πi(θi)fi(y|θi)dθi.
Since BF(x) = BF(y, z), then BF(z|y) = BF(x)/BF(y), and it is clear that the partial
Bayes Factor simply divides out the undeﬁned ratio of normalizing constants. Furthermore,
O’Hagan (1995, p.105) shows that the partial Bayes Factor is asymptotically consistent in
that it will choose the correct model with probability one as n/m →∞.
7.3.4.4
Fractional Bayes Factor
The partial Bayes Factor eliminates the local Bayes Factor assumptions about B0 and
the problems associated with averaging over many possible training samples as with the

Bayesian Hypothesis Testing and the Bayes Factor
227
intrinsic Bayes Factor, but it still requires determination of the proportion of the data
selected as the training sample.
O’Hagan (1995) suggests a modiﬁcation of the partial
Bayes Factor, which makes the selection of the training sample, y, less important. Deﬁne
η = m/n. If m and n are reasonably large, then by the properties of likelihood estimators
ℓ(y|θ) ≈ℓ(x|θ)η, meaning that the likelihood based on the training sample approximates
the full likelihood adjusted for sample size. For proper or improper priors on θ, this leads
to a Bayes Factor of the following form:
BFη(x) =

θ1 π1(θ1)f1(x|θ1)dθ1/

θ1 π1(θ1)f1(x|θ1)ηdθ1

θ2 π2(θ2)f2(x|θ2)dθ2/

θ2 π2(θ2)f2(x|θ2)ηdθ2
.
(7.32)
Therefore if π1 or π2 are speciﬁed as improper priors, then the indeterminate constant will
cancel out in either the numerator or the denominator. Also, the initial step of calculating
training posteriors has been removed by the η-ratio: absolute values of the prior density
are normalized out.
As with the partial Bayes Factor, determination of the proportion of the sample to
commit to solving the prior problem is inﬂuential on the resulting ratio.
It is obvious
that η should progress toward zero as n goes to inﬁnity: in the limit the likelihood func-
tion subsumes any prior, including improper priors. O’Hagan recommends the value m0,
which is the minimum value that provides a consistent model choice, since this provides
the greatest possible proportion of the data for comparing the models. In the presence of
outliers, or potential outliers, he recommends √n or log(n) as robustizing values. In any
event, the subjectivity of priors is replaced to some extent by the subjectivity of sample
dichotomization.
The fractional Bayes Factor loses quite a bit of the character of Bayesian inference by
mechanically removing the impact of the improper prior. The resulting quantity no longer
has a Bayes Factor interpretation since it is not the ratio of alternate posteriors over priors,
although it remains useful (Conigliani, Castro, and O’Hagan 2000). Rubin (1984, p.1152)
sets out the following deﬁnition of Bayesianly justiﬁable calculation:
. . . it treats known values as observed values of random variables, treats un-
known values as unobserved random variables, and calculates the conditional
distribution of unknowns given knowns and model speciﬁcations using Bayes’
theorem.
As new data are observed, the researcher will have to adjust the value of η or recognize that
the test is changing criteria. Therefore new fractional Bayes Factors will not be constants
of previous fractional Bayes Factors and the update from the new data. While this is only
mildly inconvenient, it does violate the conditional updating of estimates of unknowns based
on knowns in the Rubin statement of Bayesianly justiﬁable.
7.3.4.5
Redux
It is appropriate to end this section with a brief discussion of what these methods are
doing relative to alternatives. Given the general problem that Bayes Factors can be sensitive

228
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
to the selection of priors and the speciﬁc problem that Bayes Factors are incalculable for
improper priors, these four techniques segment the sample in diﬀering ways to cancel out the
eﬀects of indeterminate constants as seen in (7.27). While these approaches all use sample
quantities to determine prior density speciﬁcations, they are not empirical Bayesian methods
(Carlin and Louis 2009, Chapter 5) in the classic sense: use of the data to empirically
estimate the highest level of hyperpriors in a hierarchical model.
Furthermore, none of
these models produces the goal of “objective Bayesianism” since subjective decisions are
made in every case. Nonetheless, despite the discussed ﬂaws, each of these methods provides
a means of presenting Bayes Factor evidence for one model over another using improper
priors and either a nested or non-nested test.
7.3.5
Two-Sided Hypothesis Tests and Bayes Factors
Berger and Delampady (1987) propose integrating over two sections of the prior space,
p(θ), to create separate prior densities for the unknown coeﬃcient under the alternate
assumptions that H0 and H1 are true.
This is an attempt to show that the common
classical practice of testing a point null hypothesis can be approximated by a precise null
interval hypothesis. The test begins with segmenting an unconditional prior probability
across the sample space of θ, and deﬁning two conditional priors:
g0(θ|H0) = 1
π0
p(θ)IΩ(θ)
g1(θ|H1) =
1
1 −π0
p(θ)IΩ′(θ)
where:
Ω = {θ:|θ −θ0| < ϵ},
and
π0 =

Ω
p(θ)dθ.
(7.33)
In this intuitive setup π0 is the prior probability corresponding to H0 and g0 is the con-
ditional distribution of θ assuming that this H0 is true. Conversely, 1 −π0 is the prior
density corresponding to H1 and g1 is the conditional distribution of θ assuming that H1
is true. The primary diﬃculty is determining a reasonable interval around the point null:
{θ0 −ϵ : θ0 + ϵ} so that the test: H0: |θ −θ0| ≤ϵ, H1: |θ −θ0| > ϵ substitutes for the true
point null test: H0: θ = θ0, H1: θ ̸= θ0.
The core problem is that larger values of ϵ move further away from the desired nature
of the two-sided problem and that smaller values of ϵ move toward “Lindley’s Paradox,”
alternately called “Jeﬀreys’ Paradox” (Jeﬀreys 1961, Lindley 1957, Shafer 1982); this is the
fact that in testing a point null hypothesis for a ﬁxed prior, and posterior cutoﬀpoints
calibrated to match some constant frequentist α value, as the sample size goes to inﬁnity,
p(H0) →1. This happens no matter how small the value for α happens to be because pa-
rameter values of negligible likelihood are given nonzero prior weights (Aitkin 1991, p.115).
Furthermore, there is a problem when the range imposed by ϵ is not theoretically driven:
“Such limits would be a sheer guess and merely introduce an arbitrariness” (Jeﬀreys 1961,
367). Casella and Berger (1987b) object to Berger and Delampady’s practice of assigning

Bayesian Hypothesis Testing and the Bayes Factor
229
π0 = 1
2 as a no-information prior since no reasonable researcher would begin some enterprise
with an expectation of failure at 50%.
Berger, Brown, and Wolpert (1994) followed by Berger, Boukai, and Wang (1997) de-
signed a conditioning statistic so that frequentist probabilities coincide with analogous
Bayesian posterior density regions, where the cost of this coincidence is the introduction of
a middle “no-decision” region into the hypothesis test.
Starting with a purely Bayesian perspective, if a prior probability, π0, is assigned to H0
with complementary prior, 1 −π0, for H1, then the posterior probability in favor of H0 is:
p(H0|x) =
'
1 + 1 −π0
π0
1
BF(0,1)
(−1
,
(7.34)
where BF(0,1) is the Bayes Factor for H0 over H1. If equal probability is assigned to the
two hypotheses, then
p(H0|x) = BF(0,1)/(1 + BF(0,1)),
and
p(H1|x) = 1/(1 + BF(0,1)).
(7.35)
This leads to the selection of H0 if BF(0,1) ≤1, and H1 if BF(0,1) > 1.
The setup just described is deliberately rigged to resemble a frequentist decision-making
process. These authors specify F0 and F1 as the assumed smooth and invertible cumulative
distribution function of the Bayes Factor, BF(0,1), under the assumption of Model 0 and
Model 1 respectively. Now deﬁne:
r = 1,
α = F −1
0
(1 −F1(1)),
if
F −1
0
(1 −F1(1)) ≥1
r = F −1
1
(1 −F0(1)),
α = 1,
if
F −1
0
(1 −F1(1)) < 1.
(7.36)
This leads to the following test with three decision regions:
Reject H0 if
BF(0,1) ≤r
No decision if
r < BF(0,1) ≤α
Accept H0 if
BF(0,1) ≥α
(7.37)
for values of the Bayes Factor. The frequentist conditioning statistic S = min[B, F −1
0
(1 −
F1(B))] leads to identical conditional error probabilities as the Bayes Factor test outlined
in (7.37). Despite the authors’ claims that the no-decision region is observed to be small
in their empirical trials, most Bayesians are likely to be uncomfortable with the idea that
some fraction of the sample space of the test statistic remains ambiguous (should a loss
function be assigned to the no-decision region as well?). Non-Bayesians, however, are more
accustomed to the idea of “weak evidence” for a given hypothesis. Finally, short of showing
a clever intersection of Bayesian and frequentist testing, this approach may be too restrictive
to both sides to be widely useful.
7.3.6
Challenging Aspects of Bayes Factors
It should also be noted that there is nothing in the setup of the Bayes Factor or the
subsequent judgment about the strength of evidence for one hypothesis over another that

230
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
accommodates a directional (one-sided) test. Therefore Bayes Factors are restricted to the
two-sided test considered here (Berger and Mortera 1999), although some authors have
found exceptions for very particular situations such as strict parameter restrictions (Dudley
and Haughton 1997), or orthogonalized parameters (Kass and Vaidyanathan 1992). For
discussions of one-sided Bayes Factors see Casella and Berger (1987a), Moreno (2005),
Marden (2000), and Wetzels et al. (2009).
Occasionally authors make simplifying assumptions that are hard to support in actual
data-analytic settings. For instance, some researchers use Laplace’s method, described in
Section 7.7, which makes the assumption that the posterior is approximately normally
distributed to get Bayes Factors in otherwise diﬃcult cases. See, for instance, Kass (1993),
Kass, Tierney, and Kadane (1989), Kass and Raftery (1995), Tierney and Kadane (1986),
and Wong and Li (1992). This assumption is far less worrisome with a large sample and in
small dimensions, yet this is not always the case in social science research.
Some modiﬁcations of the Bayes Factor appear not to work in practice. The posterior
Bayes Factor (Aitkin 1991) uses the ratio of the posterior means as a substitute for the
standard form in order to be less sensitive to the form of the prior. Unfortunately the
posterior Bayes Factor introduces some minor undesirable properties such as noninvari-
ance to hypothesis aggregation (Lindley 1991, Comments), and that it can prefer models
independently of the strength of the data (Goldstein 1991).
Sometimes the Bayes Factor is just too unstable to compute numerically due to large
ratio diﬀerences and computer rounding/truncating at the register level. Machine accuracy
matters, of course, when comparing very large to very small numbers. Using natural loga-
rithms is often an eﬀective way to deal with numerical problems, and the log of the Bayes
Factor has a nice interpretation:
log(BF(1,2)) = log
π(M1|x)/p(M1)
π(M2|x)/p(M2)

= log
π(M1|x)
π(M2|x)

−log
p(M1)
p(M2)

.
(7.38)
This means that the log of the Bayes Factor is the log ratio of the posteriors minus the log
ratio of the priors, which contrasts posterior and prior information between the two models.
Gelman and Rubin (1995) criticize the use of Bayes Factors for the same reason that
others denigrate the null hypothesis signiﬁcance test: it is assumed in the comparison that
one of the proposed models is the correct speciﬁcation. This is really more of a problem
in the way that social science research is presented in published work where often only a
very limited number of speciﬁcations are tested, whereas many more were posited in earlier
stages of the research. Also, Han and Carlin (2001) outline computational problems that
arise from using Bayes Factors for some hierarchical models (see Chapter 12) and some of
the Markov chain Monte Carlo challenges (see Chapter 10).
Occasionally the integrals in (7.10) are forbidding, but Chib (1995) provides a way of
ﬁnding marginal likelihoods from Gibbs sampling output, and Chib and Jeliazkov (2001)
show how this can be done with Metropolis-Hastings output.
We will describe these tech-
niques later in Chapter 14 in Section 14.5 (starting on page 515).
Also Morey et al. (2010)
use the Savage-Dickey density ratio with MCMC output to conveniently calculate the Bayes

Bayesian Hypothesis Testing and the Bayes Factor
231
Factor. These tools are extremely useful, but restricted to MCMC output. Additionally,
the R package MCMCpack by Martin, Quinn and Park provides a useful function, bayesF,
for calculating the Bayes Factor for commonly used regression models, and the R package
BayesFactor by Morey and Rouder provides the Bayes Factor individually for a list of
common models. Other package authors have included functions as well to address speciﬁc
problems.
7.4
The Bayesian Information Criterion (BIC)
Kass (1993) and Kass and Raftery (1995) suggest using the Bayesian information crite-
rion (BIC) as a substitute for the full calculation of the Bayes Factor when such calculations
are diﬃcult since the BIC can be calculated without specifying priors. It is therefore more
appealing to non-Bayesians and seemingly less subjective. Alternately called the Schwarz
criterion, after the author of the initial article (1978), it is given by:
BIC = −2ℓ(ˆθ|x) + p log(n)
(7.39)
where ℓ(ˆθ|x) is the maximized log likelihood value, p is the number of explanatory variables
in the model (including the constant), and n is the sample size. This is very similar to
the earlier “consistent” AIC of Bozdogan (1987): CAIC = −2ℓ(ˆθ|x) + p(1 + log(n)). To
compare two models with the BIC, create a Schwarz criterion diﬀerence statistic according
to:
S = BICmodel 1 −BICmodel 2 = ℓ(ˆθ1|x) −ℓ(ˆθ2|x) −1
2(p1 −p2) log(n)
(7.40)
where the subscripts indicate the model source. This is extremely easy to calculate and has
the following asymptotic property:
S −log(BF(1,2))
log(BF(1,2))
−−−−→
n→∞0.
(7.41)
Unfortunately S is only a rough approximation to log(BF(1,2)) since the relative error of this
approximation is ◦(1) (although this can be improved for very speciﬁc models, e.g., Kass
and Wasserman (1995) for an example using normal priors), and typically large samples are
required. In fact, the log form in (7.41) is essential since
exp[S]
BF(1,2) ↛1 as n goes to inﬁnity
(Kass and Wasserman 1995, p.928). Nonetheless, Kass and Raftery (1995) argue that it
is often possible to use the approximation: S ≈−2 log(BF(1,2)). Robert (2001, p.353) is
critical of the use of the BIC to estimate Bayes Factors because it removes the eﬀect of any
speciﬁed priors, it is diﬃcult or impossible to calculate in complex models with non-iid data,
and maximum likelihood estimations are required for every model compared and therefore
might not be as much of a shortcut as intended.
The BIC should be used with caution under certain circumstances. First, the BIC can be

232
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
inconsistent as the number of parameters gets very large (Stone 1977a 1977b). Models that
are this large (thousands of parameters) are rare in the social sciences, but they do occur
in other areas such as statistical genetics. Berger et al. (2003) follow-up on Stone’s brief
comment with a more detailed exposition of how the BIC can be a poor approximation to the
logarithm of Bayes Factor using the Stone’s ANOVA example and suggest the alternative
forms: Generalized Bayes Information Criterion (GBIC) and a Laplace approximation to the
logarithm of the Bayes Factor. Chakrabarti and Ghosh (2006) prove some of the properties
of these alternative forms.
A commonly used measure of goodness-of-ﬁt outside of Bayesian work is the Akaike in-
formation criterion (Akaike, 1973, 1974, 1976). The idea is to select a model that minimizes
the negative likelihood penalized by the number of parameters:
AIC = −2ℓ(ˆθ|x) + 2p,
(7.42)
where ℓ(ˆθ|x) is the maximized model log likelihood value and p is the number of explanatory
variables in the model (including the constant). The AIC is very useful in comparing and
selecting non-nested model speciﬁcations, but many authors have noted that the AIC has a
strong bias toward models that overﬁt with extra parameters since the penalty component is
obviously linear with increases in the number of explanatory variables, and the log likelihood
often increases more rapidly (Carlin and Louis 2009, p.53; Neft¸ci 1982, p.539; Sawa 1978,
p.1280).
Using the value of the log likelihood alone without some overﬁtting penalization as
a measure of model ﬁt is a poor strategy since the likelihood never decreases by adding
more explanatory variables regardless of their inferential quality. See also the lesser known
alternatives: MIC (Murata, Yoshizawa, and Amari 1994) and TIC (Takeuchi 1976), NIC
(Stone 1974), and the explicitly Bayesian EAIC (Brooks 2002). Burnham and Anderson
(2002) argue in their book that when the data size is small relative to the number of speciﬁed
parameters, one should use a modiﬁed AIC according to:
AICc = AIC + 2p(p + 1)/(n −p −1),
(7.43)
which add a further penalty.
Both the AIC and the BIC have a problem with a widened deﬁnition of parameters. Sup-
pose we have panel data with n individuals each measured over t time-points. If individual-
speciﬁc eﬀects are drawn from a single random eﬀects term (i.e., given a distributional
assignment), then group-level variables from this speciﬁcation play diﬀerent roles. Such
models are very common Bayesian speciﬁcations and are the subject of Chapter 12. In this
simple case should this latent variable be counted as one parameter or n parameters? If it
counts as n parameters, then the eﬀect of shrinkage is ignored as well, substantially inﬂating
the value of the AIC and BIC. On the other hand, if it counts as one parameter it appears
that we are intentionally reducing the penalty component to advantage the model relative
to one without such a random eﬀect. We now turn to a tool speciﬁcally designed to handle
this problem.

Bayesian Hypothesis Testing and the Bayes Factor
233
7.5
The Deviance Information Criterion (DIC)
A very useful tool for model assessment and model comparison is the deviance informa-
tion criterion created by Spiegelhalter, Best, Carlin, and van der Linde (2002), although
earlier, more narrow versions exist. The DIC has already become a popular model com-
parison choice since it is integrated into the WinBUGS package for MCMC estimation (see
Chapter 10). There are two objectives here: describing “model complexity” and model ﬁt as
in the AIC or BIC. For instance, in the BIC the term −2ℓ(ˆθ|x) is thought of as a describer
of ﬁt, whereas p log(n) shows complexity in the form of parameter vector size in the model.
Suppose we have a model under consideration, deﬁned by p(y|θ) for data y and parameter
vector θ. The ﬁrst quantity to deﬁne is the “Bayesian deviance” speciﬁed by Spiegelhalter
et al. as:
D(θ) = −2 log[p(y|θ)] + 2 log[f(y)],
(7.44)
where f(y) is some function of just the data, the “standardizing factor.” Spiegelhalter et
al. (2002) de-emphasize f(y) for model comparison and even suggest using f(y) = 1 (giving
zero contribution above) since this term must be identical for both model calculations and
therefore cancels out. Note the similarity to the AIC and BIC, except that there is no
p term in the second part. We can use (7.44) in explicitly posterior terms by inserting a
condition on the data and taking an expectation over θ:
D(θ) = Eθ[−2 log[p(y|θ)|y]] + 2 log[f(y)],
(7.45)
(the authors switch notation between D(θ) and ¯D). This posterior mean diﬀerence is now
a measure of Bayesian model ﬁt. Now deﬁne ˜θ as a posterior estimate of θ, which can be
the posterior mean or some other easily produced value (although Spiegelhalter et al. note
that obvious alternatives such as the median may produce problems). Observe that we can
also insert ˜θ into (7.44). The eﬀective dimension of the model is now deﬁned by:
pD = D(θ) −D(˜θ).
(7.46)
This is the “mean deviance minus the deviance of the means.” Another way to think about
eﬀective dimensions is by counting the roles that parameters take on in Bayesian models. In
this way, pD is the sum of the parameters each of which is weighted according to: ωp = 1 for
parameters unconstrained by prior information, ω = 0 for parameters completely speciﬁed
(ﬁxed) by prior information, and ω ∈[0:1] for parameters with speciﬁc dependencies on
the data or priors.
Also, due to the subtraction, pD is independent of our choice of f(y). This is illustrated

234
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
by expanding (7.46) and applying Bayes’ Law:
pD = D(θ) −D(˜θ)
=
0
Eθ|y[−2 log[p(y|θ)|y] + 2 log[f(y)]
1
−
2
−2 log[p(y|˜θ)] + 2 log[f(y)]
3
= Eθ|y[−2 log(p(y|θ))] + 2 log(p(y|˜θ))
= Eθ|y
'
−2 log
p(θ|y)p(y)
p(θ)
(
+ 2 log
*
p(˜θ|y)p(y)
p(˜θ)
,
= Eθ|y[−2 log(p(θ|y)/p(θ))] + 2 log(p(˜θ|y)/p(˜θ)).
(7.47)
We can think of the ratio p(θ|y)/p(θ) here as the gain in information provided by condition-
ing on the data in the model, and correspondingly p(˜θ|y)/p(˜θ) as the gain in information
after plugging in the chosen estimate.
In a simulation context (subsequent chapters), pD can be computed as the mean of
simulated values of D(θ) minus D(θ) plugging the mean of the simulated values of θ, hence
the interpretation as the diﬀerence between the posterior mean of the deviance and the
deviance at the poster mean (or some other chosen statistic). So while D(θ) is a Bayesian
measure of model ﬁt, pD is designed to be a “complexity measure” for the eﬀective number
of parameters in the model. As such it is the Bayesian analogy to the second term in the
AIC.
The Deviance Information Criterion is created by adding an additional model ﬁt term
to the eﬀective dimension:
DIC = D(θ) + pD = 2D(θ) −D(˜θ)
(7.48)
and is thus the Bayesian measure of model ﬁt above with an extra complexity penalization.
The logic here goes back to a common criticism of the AIC that using a plug-in value rather
than integrating out unknown parameters leads to insuﬃcient incentive for parsimonious
models. The DIC also uses a plug-in value (˜θ), but incorporates an additional penalty
to compensate. Some authors still prefer an adapted version of the AIC, such as Brooks
(2002) who suggests the expected Akaike information criterion: EAIC = D(θ) + 2p, and
D(¯θ) + 2p.
■Example 7.3:
Hierarchical Models of Rural Migration in Thailand. Garip and
Western (2011, 2005 results shown here) use the example of village-level migration
to contrast hierarchical model speciﬁcations for a dichotomous outcome.5 They look
at survey data on young adults (18-25) in 22 Northeastern Thai villages where the
outcome variable is 1 if the respondent spent at least two months away from the village
5Note: this example is based on an earlier version of the paper which had more starkly contrasting pD
and DIC values.

Bayesian Hypothesis Testing and the Bayes Factor
235
in 1990 and 0 otherwise. The individual-level explanatory variables are sex, age, years
of education, number of prior trips, and the village-level explanatory variables are prior
trips from the villages and the Gini index of prior trips.
TABLE 7.4:
Available Model Specifications, Thai
Migration
Model
Speciﬁcation
Prior Parameters
Pooled
logit(pij) = α + x′
ijβ + z′
jγ
α ∼N(0, 106)
β[1 : 4] ∼N(0, 106)
γ[1 : 2] ∼N(0, 106)
Fixed Eﬀect
logit(pij) = αj + x′
ijβ
α[1 : 22] ∼N(0, 106)
β[1 : 4] ∼N(0, 106)
Random Eﬀect
logit(pij) = αj + x′
ijβ + z′
jγ
μ = 0
α[1 : 22] ∼N(μ, τ 2)
τ ∼IG(10−3, 10−3)
β[1 : 4] ∼N(0, 106)
γ[1 : 2] ∼N(0, 106)
Random
logit(pij) = αj + x′
ijβj + z′
jγ
μα ∼N(0, 106)
Intercept and
α[1 : 22] ∼N(μα, τ2
α)
τ 2
α ∼IG(10−3, 10−3)
Random Slope
β[1 : 22][1 : 4] ∼N(μβ, τ2
β)
μβ ∼N(0, 106)
τ2
β ∼IG(10−3, 10−3)
γ[1 : 2] ∼N(0, 106)
Methodological interest here centers on the utility of specifying hierarchies in the
model. These are useful tools for recognizing diﬀerent levels that the data aﬀect some
outcome. For classical non-Bayesian references, see Bryk and Raudenbush (2001) or
Goldstein (1985). We will study Bayesian hierarchical models in considerable detail in
Chapter 12, and for the moment an excellent source for background reading is Good
(1980a). There are two data matrices here with rows: xij for individual i in village j,
and zj for village j. The four contrasting models for the probability of migration, pij,
given by Western and Garip are given in Table 7.4 where α is a common intercept,
αj is a village-speciﬁc intercept, and [β, γ] is the unknown parameter vector to be
described with a posterior distribution. There are many contrasting features in these
models having to do with the relative eﬀects in the data and the role we would like
these eﬀects to play in the model. Suppose we simply wanted to test the relative
quality of the models with the DIC, ignoring for now any substantive issues with
diﬀerent hierarchies.
Despite the notable structural diﬀerences in these models, both the individual-level

236
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
and village-level coeﬃcients are remarkably similar and do not present any substan-
tively diﬀerent stories whatsoever (except of course that the ﬁxed eﬀects model does
not have village-level coeﬃcients). It is a modest diﬀerence, but the posterior distri-
butions for the random intercept and random slope/random intercept are more diﬀuse
than for the pooled model, reﬂecting the random eﬀects modeling heterogeneity.
TABLE 7.5:
Model Comparison, Thai Migration
Fixed
Random
Rdm. Slope
Model:
Pooled
Eﬀect
Eﬀect
& Intercept
Intercept
-0.80 (0.12)
-0.66 (0.10)
-0.79 (0.13)
-0.76 (0.14)
Male
0.32 (0.14)
0.38 (0.14)
0.33 (0.14)
0.30 (0.15)
Age
-0.13 (0.08)
-0.15 (0.08)
-0.13 (0.08)
-0.13 (0.09)
Education
0.39 (0.07)
0.38 (0.08)
0.39 (0.07)
0.39 (0.08)
Prior trips(i)
1.08 (0.09)
1.13 (0.10)
1.08 (0.09)
1.22 (0.16)
Prior trips(j)
-0.61 (0.37)
-0.59 (0.43)
-0.69 (0.46)
Gini trips(j)
-0.62 (0.21)
-0.60 (0.24)
-0.61 (0.25)
Posterior means (posterior standard deviations in parentheses)
pD
7.02
26.24
10.93
30.58
DIC
1259.41
1273.29
1259.62
1247.55
We see from the pD = 7.02 value (Table 7.5) that the pooled model is the most par-
simonious, with the extra 0.02 above the 7 speciﬁed parameters coming from prior
information. Interestingly, even though it contains 7 + 22 parameters, the random
intercept model is only slightly less parsimonious with pD = 10.93. Not surprisingly,
the random slope/random intercept model is the least parsimonious with 110 ran-
dom eﬀects and 2 village-level parameters speciﬁed, which is only moderately less
parsimonious than the random eﬀect model with 26 parameters, justifying the hierar-
chical structure imposed. Furthermore, with the lowest DIC value, we ﬁnd additional
support for the more complex structure.
7.5.1
Some Qualiﬁcations
Some concerns emerge with the use of the DIC (the Spiegelhalter et al. (2002) paper is
accompanied by 23 pages of discussion). First, its lack of invariance to reparameterizations
of the parameters where the subsequent diﬀerences can be large, Therefore, we should
be cautious in interpreting large DIC changes that come from reparameterizations only.
Secondly, there is evidence that DIC comparisons are most straightforward when the form

Bayesian Hypothesis Testing and the Bayes Factor
237
of the likelihoods (focus) are the same and only the selected explanatory variables diﬀer.
Otherwise it is essential to make the standardizing factor the same. Likelihood functions
from hierarchical models with diﬀerent level structures provide diﬃcult interpretational
problems (see the example above). In fact, the DIC is strongly justiﬁed only for likelihood
functions based on exponential family forms (Appendix (A)). Helpfully, Celeux et al. (2006)
survey strengths and weakness for the DIC beyond applications to exponential families.
When the posterior distributions are non-symmetric or multimodal, the use of the posterior
mean may be inappropriate. It is possible for pD to be negative, so DIC can also be negative.
Clearly this provided interpretational problems. The DIC is not a function of the marginal
likelihood, therefore it is not related to Bayes Factor comparisons. The DIC does not work
when the likelihood depends on discrete parameters, so it does not work with mixture
likelihoods (WinBUGS will “gray-out” the DIC button automatically). Finally, speciﬁcation
of the DIC implies use of the data twice: once to produce the posterior and once again with
p(y|θ) in the expectation. The two objections are: philosophical problems with violating
canonical tenets of Bayesian inference, and practical problems with the increased tendency
to overﬁt the data.
7.6
Comparing Posterior Distributions with the
Kullback-Leibler Distance
There are many situations where it is convenient to compare distributions, posteriors
in particular. The Bayes Factor (Chapter 7), in particular, provides a mechanical way to
inferentially compare two model results. A diﬀerent and more general method for evaluating
the diﬀerence between two distributions is the Kullback-Leibler distance (sometimes called
the entropy distance), which is given by:
I(f, g) =

log
'f(x)
g(x)
(
f(x)dx,
(7.49)
for two candidate distributions f(x) and g(x) (Robert and Casella 1999, p.222, White 1996,
p.9). This is the expected log-ratio of the two densities with the expectation taken relative to
one selected distribution (f(x) here). Despite the simple form of (7.49), it can occasionally
be diﬃcult to calculate analytically. If f(x) and g(x) are exponential family distributions,
the resulting cancellations can considerably simplify the integral (McCulloch and Rossi
1992). For instance, suppose we are interested in comparing the Kullback-Leibler distance
between two proposed univariate Poisson distributions indexed by intensity parameters λ1

238
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
and λ2. The form of I(f, g) is given by:
I(f, g) =

X
log
'e−λ1λx
1/x!
e−λ2λx
2/x!
( e−λ1λx
1
x!
=

X
'
(λ2 −λ1) + x log
λ1
λ2
( e−λ1λx
1
x!
= λ2 −λ1 + λ1 log
λ1
λ2

.
(7.50)
However, the Kullback-Leibler calculations are rarely this direct. Furthermore, the Kullback-
Leibler distance is best used only comparatively among a set of alternative speciﬁcations
since it is not a true Cartesian distance, the scale is still arbitrary, and it is not symmetric:
I(f, g) ̸= I(g, f). Nonetheless, this is an excellent way to measure the closeness of a set of
posteriors.
General details about using the Kullback-Leibler distance can be found in Kullback
(1968) and Brown (1986), and hypothesis testing applications are given by Goel and
DeGroot (1979), Janssen (1986), Ebrahimi, Habibullah, and Sooﬁ(1992), and Robinson
(1991).
The relationship to the AIC model ﬁtting criteria (Chapter 7) is explored by
Hurvich and Tsai (1991) and Hurvich, Shumway, and Tsai (1990). Efron (1978) uses the
“curve of constant Kullback-Leibler information” in relating the natural parameter space
of coeﬃcient with its model-induced expected parameter space, and several authors, such
as Hernandez and Johnson (1980), Johnson (1984), and Sakia (1992), use Kullback-Leibler
information to estimate the parameter of the popular Box-Cox transformation. There is also
a wealth of studies using Kullback-Leibler information to make direct model comparisons,
such as Leamer (1979), Sawa (1978), Vuong (1989), and White (1996, Chapter 9). We will
return to this technique in Section 9.3.1 with a numerical example.
TABLE 7.6:
Martikainen et al. (2005) Mortality
Parameters
Married
Never Married
Divorced
Widowed
Men
1.00
1.84
2.08
1.51
Women
1.00
1.59
1.62
1.28
■Example 7.4:
Example: Models of Relative Mortality Martikainen et al. (2005)
seek to explain why the non-married population has a higher mortality rate compared
to the married population, an eﬀect seen in many countries. Their supposition is that
the health-related behavior of non-married individuals contributes to their higher
rates. Part of this study uses data from 1996 to 2000 in Finland for individuals aged
30-64 years. Their models produce mortality rates relative to the married populations

Bayesian Hypothesis Testing and the Bayes Factor
239
where the values are interpreted as Poisson intensity parameters. Using their “full
model” we see the relative intensity parameter estimates in Table 7.6.
These values can be evaluated as single point estimates, as the original authors do,
but actually they are describing distributional diﬀerences. Given the asymmetry of
the Poisson distribution, simple numerical comparison of the estimated intensity pa-
rameters may not give a complete summary. So using (7.50) we can summarize dis-
tributional diﬀerences, as given in Table 7.6.
TABLE 7.7:
Martikainen et al. (2005) Mortality
Differences
Men
Never Married
Divorced
Widowed
Married
0.23023
0.34763
0.09789
Never Married
0.01441
0.03369
Divorced
0.09614
Women
Never Married
Divorced
Widowed
Married
0.12627
0.13757
0.03314
Never Married
0.00028
0.03483
Divorced
0.04162
The results in Table 7.7 reinforce the main ﬁndings that men fare worse outside of mar-
riage than women, which supports the authors’ hypothesis about behavioral factors:
accidents from risky ventures, violence, and alcohol. The Kullback-Leibler distribu-
tional diﬀerences highlight more directly comparisons between the non-marriage cate-
gories. For example the distribution diﬀerence between Never Married and Divorced
is much greater for Men than for Women (roughly two orders of magnitude). Since these
are all relative measures, we can assert that there is virtually no distributional dis-
tance between Divorced and Never Married for Women (notice that 0.00028 is much
smaller than 0.13757 −0.12627 −0.0113).
7.7
Laplace Approximation of Bayesian Posterior
Densities
Laplace’s method (1774) is a well-known method for approximating the shape of marginal
posterior densities that is very useful in the Bayesian context when direct calculations are
diﬃcult. The now standard reference to approximating Bayesian posterior densities with
Laplace’s method is Tierney and Kadane (1986), and theoretical details on the accuracy
of the approximation can be found in Wong and Li (1992) and Kass and Vaidyanathan
(1992). Kass (1993) shows how the Laplace approximation can be handy for calculating
Bayes Factors. In general, the Laplace approximation is a very useful tool to have at one’s

240
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
disposal when a normal approximation posterior is reasonable, and can be especially useful
in higher dimensions when other procedures fail.
The basic idea is to carry out a Taylor series expansion around the maximum likelihood
estimate value, ignore the negligible terms, and normalize. For simplicity we will derive
the approximation in only one dimension, but the generalization is obvious. Start with a
posterior density of interest calculated by the likelihood times the speciﬁed prior: π(θ|x) ∝
L(x|θ)p(θ), and assume that this distributional form is nonnegative, integrable, and single-
peaked about a mode, ˆθ, which is a reasonable set of assumptions provided that sample
sizes are not small.6
The posterior expectation of some smooth function of θ, g(θ|x) (such as a mean, variance,
or other desired summary quantity) is given by:
E[g(θ|x)] =

θ
g(θ|x)L(x|θ)p(θ)dθ =

θ
g(θ|x)π(θ|x)dθ.
(7.51)
Using Bayes’ Law we know that:
π(θ|x) =
L(x|θ)p(θ)

θ L(x|θ)p(θ)dθ .
(7.52)
So (7.51) becomes:
E[g(θ|x)] =

θ
g(θ|x)
L(x|θ)p(θ)

θ L(x|θ)p(θ)dθ dθ =

θ g(θ|x)L(x|θ)p(θ)dθ

θ L(x|θ)p(θ)dθ
.
(7.53)
Now deﬁne the quantities:
−nh1(θ) = log g(θ|x) + log L(x|θ) + log p(θ)
−nh2(θ) = log L(x|θ) + log p(θ).
(7.54)
Substituting these values into (7.53) produces:
E[g(θ|x)] =

exp[−nh1(θ)]dθ

exp[−nh2(θ)]dθ .
(7.55)
If we have a mathematically convenient approximation for either h1(θ) or h2(θ), then
E[g(θ|x)] becomes very easy to calculate. Consider a Taylor series expansion around some
arbitrary point θ0 for h(θ) where this generic identiﬁcation applies to either h1(θ) or h2(θ):
h(θ) = h(θ0) + (θ −θ0)h′(θ0) + 1
2!(θ −θ0)2h′′(θ0)
+ 1
3!(θ −θ0)3h′′′(θ0) + Rn(θ),
(7.56)
6Kass and Raftery (1995) recommend that for k explanatory variables one should ideally have a sample
size of at least 20k, and that less than 5k is “worrisome” (naturally such guidelines depend on the behavior
of the likelihood function in general and substantial deviations from normality will obviously require even
greater sample sizes).
Carlin and Louis (2009, p.110) note that the Laplace approximation is therefore
rarely helpful for higher dimensional problems such as those with more than ten explanatory variables.

Bayesian Hypothesis Testing and the Bayes Factor
241
where Rn(θ) is the remainder consisting of progressively smaller terms in the expansion.
In fact, since limθ→θ0 Rn(θ)/(θ −θ0)3 = 0 (Robert and Casella 2004), then we can safely
ignore higher order terms and just use: h1(θ) or h2(θ):
h(θ) ≈h(θ0) + (θ −θ0)h′(θ0) + 1
2!(θ −θ0)2h′′(θ0) + 1
3!(θ −θ0)3h′′′(θ0).
(7.57)
Recall that we are most interested in the modal point of the posterior density, so pick θ0 to
be the point where the ﬁrst derivative vanishes: ˆθ. Substituting ˆθ into (7.57) eliminates the
linear term: (θ−ˆθ)h′(ˆθ). If we ignore the rapidly vanishing cubic term, 1
3!(θ−ˆθ)3f ′′′(ˆθ), then
this is said to be a ﬁrst-order approximation. In the cases where more accuracy is required,
this term can be expanded in a second Taylor series around ˆθ to produce second- and third-
order accuracy. Often the sample size is suﬃciently large so that ﬁrst-order accuracy is
suﬃcient (see Robert and Casella [2004, p.109] for details on this second-order expansion).
Take the now reduced form in (7.57) and make the substitution ˆσ2 = (h′′(θ0))−1 to
derive:

θ
exp[ −nh(θ)]dθ
=

θ
exp

−nh(ˆθ) −
n
2ˆσ2 (θ −ˆθ)2
dθ,
= exp[−nh(ˆθ)]

θ
exp

−n
2ˆσ2 (θ −ˆθ)2
dθ,
= exp[−nh(ˆθ)](
√
2πˆσ2n−1
2 )

θ
n
1
2
√
2πˆσ2 exp

−n
2ˆσ2 (θ −ˆθ)2
dθ,
= exp[−nh(ˆθ)](
√
2πˆσ2n−1
2 ),
(7.58)
using the standard trick of pushing terms into the integral so that it equals one. This is
precisely the justiﬁcation for a normal approximation in (7.53). The resulting approximation
for E[g(θ|x)] is:
ˆE[g(θ|x)] =

θ exp[−nh1(θ)]dθ

θ exp[−nh2(θ)]dθ
= exp[−nh1(ˆθ1)](

2πσ2
1n−1
2 )
exp[−nh2(ˆθ2)](

2πσ2
2n−1
2 )
= σ1
σ2
exp[−n(h1(ˆθ1) −h2(ˆθ2)].
(7.59)
The last line of (7.59) is exactly the form found by Tierney and Kadane to be accurate to
within:
E[g(θ|x)] = ˆE[g(θ|x)](1 + ◦(n−2)),
(7.60)
where ◦(n−2) indicates “on the order of” or “at the rate of” 1/n2 as n increases. Therefore
Laplace’s method replaces integration as a method of obtaining an estimate of E[g(θ|x)] with
diﬀerentiation. Typically diﬀerentiation is not only easier to perform, but the algorithms
are typically more numerically stable (Gill, Murray, and Wright 1981).

242
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
The variance of g(θ|x) is produced from standard theory: Var[g(θ|x)] = ˆE[g(θ|x)2] −
( ˆE[g(θ|x)])2, which also has relative error on the order of ◦(n−2) (Tierney and Kadane 1986,
p.83). The asymptotic properties are given in detail by several authors: de Bruijn (1981,
Chapter 7), Kass, Tierney, and Kadane (1989), Tierney, Kass, and Kadane (1989b), and
Schervish (1995). For an applicable, but more generalized discussion, of the asymptotic
behavior of Gaussian kernel estimators, see Le Cam (1986, Section 12.4) or Barndorﬀ-
Nielsen and Cox (1989, Chapter 1).
The multivariate formulation of (7.59) follows directly by substituting θ for θ, |Σ|
1
2 for
σ, where Σ is the inverse negative of the second derivative matrix (Hessian) of the likelihood
function evaluated at the modal point. Occasionally the multivariate approximation imposes
some additional computational diﬃculties, such as the conditioning of the Hessian (Albert
1988; Hsu 1995).
In the case where the derivatives required to produce (7.59) are not
readily calculable, MCMC simulation techniques (introduced in Chapter 10) have been
developed to produce desired interim quantities (Raftery 1996). In some cases a slightly
more complicated “saddle point” approximation can be substituted (Goutis and Casella
1999; Tierney, Kass, and Kadane 1989b). Tierney, Kass, and Kadane (1989a)
present a
more ﬂexible form of the basic Laplace procedure variant that reduces the need for typically
more diﬃcult higher-order derivatives.
Several implementation details are worth observing. First, note that the two modal
points, ˆθ1 and ˆθ2 diﬀer.
The second point is simply the standard maximum likelihood
value. Tierney and Kadane (1986) as well as Press (1989, Section 3.3.1) point out that
the ﬁrst value typically lives in the same neighborhood and therefore can easily be found
with a simple numerical search algorithm such as Newton-Raphson, using ˆθ2 as a starting
point. Also, under fairly general circumstances (7.59) is the empirical Bayes estimator for
g(θ|x) (Lehmann and Casella 1998, p.271). It is also possible to use simulation techniques
as a substitute for diﬃcult derivatives in complex models. This approach is then called
Laplace-Metropolis estimation (Lewis and Raftery 1997, Raftery 1996).
One convenient feature of Laplace’s approximation is its ﬂexibility with regard to new
prior forms and newly observed data. Deﬁne pnew(θ) as an alternative prior distribution that
the researcher is interested in substituting into the calculation of the statistic of interest,
E[g(θ|x)]. Rather than recalculate the Laplace approximation, we can use the following
shortcut (Carlin and Louis 2009, pp.111-112). First calculate a prior odds ratio of the new
prior to the one already used in the calculation:
b(θ) = pnew(θ)
p(θ)
,
(7.61)
so the new expected value of g is:
ˆEnew[g(θ|x)] =

g(θ|x)L(x|θ)p(θ)b(θ)dθ

L(x|θ)p(θ)b(θ)dθ
.
= b(θ1)
b(θ2)
ˆE[g(θ|x)].
(7.62)

Bayesian Hypothesis Testing and the Bayes Factor
243
This means that we don’t have to fully recalculate the expected value; instead we use
the already maximized θ values, ˆθ1 and ˆθ2, from the previous calculations.
So we can
very quickly try a large number of alternate prior speciﬁcations and immediately see the
implications.
7.8
Exercises
7.1
Perform a frequentist hypothesis test of H1: θ = 0 versus H0: θ = 500, where
X ∼N(θ, 1) (one-tail at the α = 0.01 level). A single datapoint is observed, x = 3.
What decision do you make? Why is this not a defensible procedure here? Does a
Bayesian alternative make more sense?
7.2
A random variable X is distributed N(μ, 1) with unknown μ. A standard (non-
Bayesian) hypothesis test is set up as: H1: μ = 0 versus H0: μ = 50 with α = 0.05
(one-sided such that the critical value is 1.645 for rejection). A sample of size 10 is
observed, (2.77, 0.91, 1.88, 2.28, 1.86, 1.33, 2.99, 2.07, 1.58, 2.99), with mean 2.07 and
standard deviation 0.70. Do you decide to reject the null hypothesis? Why might
this not be reasonable?
7.3
Akaike (1973) states that models with negative AIC are better than the saturated
model and by extension the model with the largest negative value is the best choice.
Show that if this is true, then the BIC is a better asymptotic choice for comparison
with the saturated model.
7.4
For a given binomial experiment we observe x successes out of n trials. Using a
conjugate beta prior distribution with parameters α and β, show that the marginal
likelihood is:
n
x
Γ(α + β)Γ(x + α)Γ(n + β −x)
Γ(α)Γ(β)Γ(n + α + β)
.
7.5
Derive the last line of (7.14) from:
p(H0|data) = 1 −p(H1|data) = 1 −
1
BF(1,0)
'p(data)
p(H0) p(H0|data)
( p(H1)
p(data).
7.6
Derive the Bayes Factor for a two-sided test of a mean in (7.15) on page 218.
7.7
(Berger 1985). A child takes an IQ test with the result that a score over 100 will be
designated as above average, and a score of under 100 will be designated as below
average. The population distribution is distributed N(100, 225) and the child’s
posterior distribution is N(110.39, 69.23). Test competing designations on a single
test, p(θ ≤100|x) vs. p(θ > 100|x), with a Bayes Factor using equally plausible

244
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
prior notions (the population distribution), and normal assumptions about the
posterior.
7.8
Recalculate the model and the Bayes Factor for including an interaction term from
the Hanushek and Jackson election surveys example (page 220) using informed
normal priors.
7.9
Returning to the beta-binomial model from Chapter 2, set up a Bayes Factor for:
p < 0.5 versus p ≥0.5, using a uniform prior, and the data:
[0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1]. Perform the integration step using rejection sam-
pling (Chapter 9).
7.10
Consider Example 15 from Appendix A (page 575). Specify normal priors for the
three-dimensional vector β with mean and variance equal to the observed sample
quantities in Table 15 on page 576.
Bedrick, Christensen, and Johnson (1997)
suggest using Bayes Factors to test competing link functions, as opposed to the
more conventional idea of testing competing variable speciﬁcations. Calculate the
Bayes Factor for each of the three link functions against each other. Which would
you use? Why?
7.11
Demonstrate using rejection sampling the following equality from Bayes’ original
(1763) paper, using diﬀerent values of n and p:
 1
0
n
p

xp(1 −x)n−pdx = 1
n.
7.12
Using the Palm Beach County voting model in Section 5.1.1 compare the full model
with a null model (intercept only) writing your own DIC function for linear speci-
ﬁcation in R.
7.13
Calculate the posterior distribution for β in a logit regression model
r(X′b) = p(y = 1|X′b) = 1/[1 + exp(−X′b)]
with a BE(A, B) prior. Perform a formal test of a BE(4, 4) prior versus a BE(4, 1)
prior. Calculate the Kullback-Leibler distance between the two resulting posterior
distributions.
7.14
Taking the state-level obesity data from Exercise 6 on page 89, segment the cases
in Southern and non-Southern states to form two groups. Do a Bayesian version of
the standard Student’s t-test for normal data that uses a Bayes Factor as described
in Section 7.3.2, starting on page 219.
7.15
(Aitkin 1991). Model 1 speciﬁes y ∼N(μ1, σ2) with μ1 speciﬁed and σ2 known,
Model 2 speciﬁes y ∼N(μ2, σ2) with μ2 unknown and the same σ2. Assign the
improper uniform prior to μ2: p(μ2) = C/2 over the support [−C :C], where C is

Bayesian Hypothesis Testing and the Bayes Factor
245
large enough value to make this a reasonably uninformative prior. For a predeﬁned
standard signiﬁcance test level critical value z, the Bayes Factor for Model 1 versus
Model 2 is given by:
B =
2Cn1/2φ(z)
σ

Φ

n1/2 ¯y+C
σ

−Φ

n1/2 ¯y−C
σ
.
Show that B can be made as large as one likes as C →∞or n →∞for any ﬁxed
value of z. This is an example of Lindley’s paradox for a point null hypothesis test
(Section 7.3.5 starting on page 228).
7.16
Chib (1995) introduces MCMC based tools for obtaining marginal likelihoods and
therefore Bayes Factors. Using the notation of that paper, f(y|θk, Mk) is the den-
sity function of the observed data under model Mk with model-speciﬁc parameter
vector θk, which has the prior distribution π(θk|Mk) (notice that this diﬀerent
terminology for a prior than that used in this text). If a model is estimated with
a Gibbs sampler then G iterated values, θ(1), . . . , θ(G), are produced. Chib notes
that the marginal likelihood under model Mk:
m(y|Mk) =

f(y|θk, Mk)π(θk|Mk)dθk
can be estimated with the harmonic mean of the generated likelihood values:
ˆmNR =
4
1
G
G

g=1
*
1
(y|θ(g)
k , Mk)
,5−1
.
For more details see Section 14.5 starting on page 515.
Write an algorithm in
pseudo-code to implement this process. Why is this estimate “simulation-consistent”
but not numerically stable?
7.17
Given a Poisson likelihood function, instead of specifying the conjugate gamma dis-
tribution, stipulate p(μ) = 1/μ2. Derive an expression for the posterior distribution
of μ by ﬁrst ﬁnding the value of μ, which maximizes the log density of the posterior,
and then expanding a Taylor series around this point (i.e., Laplace approximation).
Compare the resulting distribution with the conjugate result.
7.18
Returning to the example data from wars in ancient China (Section 5.3, starting on
page 163), calculate the Bayes Factors from possible mixes of the covariates using
the BayesFactor package in R. Start with the following code:
lapply(c("BaM","BayesFactor"),library,character.only=TRUE)
data(wars)
regressionBF(SCOPE ~ ., data=wars)

246
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
From this long list pick a baseline model where the covariate selection is substan-
tively reasonable and determine a small set of alternative models that ﬁt the better
according to the Bayes Factor criteria.
7.19
Using the Palm Beach County voting model in Section 5.1.1 (starting on page 148)
with uninformative priors, compare the full model with the nested model, leaving
out the technology variable using a local Bayes Factor. Experiment with diﬀerent
training samples and compare the subsequent results. Do you ﬁnd that the Bayes
Factor is sensitive to your selection of training sample? The dataset is available as
pbc.vote in BaM.
7.20
Using the capital punishments data from Exercise 19 on page 141 (in the BaM
package as dataset executions), write a Gibbs sampler in R for a probit model
(Appendix A) with a latent variable representation (Chib and Greenberg 1998)
and uninformative priors.
Use the following steps with conditional distributions
at the jth iteration:
(a) Sample the latent variable according to truncated normal speciﬁcation:
z[j]
i
∼
⎧
⎪
⎨
⎪
⎩
T N (−∞,0)

x′
iβ[m−1], 1

if yi = 0
T N [0,∞)

x′
iβ[m−1], 1

if yi = 1
for i = 1, . . . , n. This creates the vector z[j].
(b) Sample the coeﬃcient vector according to the normal speciﬁcation:
β[j] ∼N

(X′X)−1X′z[j], (X′X)−1
Run this Gibbs sampler for two diﬀerent speciﬁcations (mixes of covariates). Modify
the code to include a calculation of the DIC within the sampler and use these values
to compare the two model ﬁts.

Chapter 8
Bayesian Decision Theory
8.1
Introducing Decision Theory
This chapter introduces the basics of decision theory in both a Bayesian and frequentist
context. The discussion is actually a continuation of the last chapter in that we will add a
“cost” dimension to model comparison and testing. The emphasis here is not on abstract
theory, but rather some practical applications to problems in the social and behavioral sci-
ences. It is important to note that decision theory is a topic that is very deep mathematically
and one that touches many academic ﬁelds, including economics, management, statistics,
international relations, pure mathematics, psychology, philosophy, and more. Therefore it
would be hard to do justice to this vast enterprise in a single chapter. Interested readers
may want to explore the cited works here, beginning with the foundational work of: Raiﬀa
and Schlaifer (1961), Savage (1972), and Lindley (1972).
Decision theory is the logical adjunct to hypothesis testing that formalizes the cost
of alternatives through explicitly deﬁning the penalty for making the wrong decision by
specifying a function that describes the loss, and therefore the associated risk, for each
alternative choice. With the quasi-frequentist/quasi-Fisherian null hypothesis signiﬁcance
test (NHST) described in Section 7.2.1, we used a test statistic to make one of two decisions:
D0 or D1, and only these two decisions, or actions, were allowable. So it is technically
incorrect to make statements from the test such as “it provides modest evidence” or “it is
barely signiﬁcant” since that adds decisions that are not fully deﬁned. Despite this, many
authors confuse the decision process with the strength of evidence. That is, the NHST
version of hypothesis testing (described in Section 7.2.1) confuses inference and decision
making since it “does not allow for the costs of possible wrong actions to be taken into
account in any precise way” (Barnett 1973). The cost of being wrong in the commonly
applied test is completely abstract at the time of the decision.
A more reasoned approach is to assign a loss function to each of the two decisions in
a hypothesis testing setup (Shao 1989, Leamer 1979). This is a real-valued function that
explicitly provides a penalty for decision i selecting hypothesis Hi given that β is the true
parameter value: d(β, Hi). So from this we can build a decision rule that codiﬁes some
loss criteria that the researcher might have: minimize the maximum loss, minimize squared
errors, and many others.
This decision rule deﬁnes a loss function that explicitly links
247

248
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
decisions that the research makes to costs of that decision. This extra criteria makes a lot
of intuitive sense. Basic statistics courses introduce Type I and Type II errors that result
from simple hypothesis testing, but do not provide the actual cost of making either of these
errors. Naturally there are applications where the cost of making an incorrect statistical
decision are important.
Chapter 2 introduced the key building blocks of Bayesian model speciﬁcations in order
to combine prior information with data information. Here we extend that list of model
components so that the researcher now must specify:
▷A likelihood function that stipulates the distribution of the data and the conditionality
on parameters: L(θ|X).
▷A prior distribution that describes the distribution of the parameters of interest before
seeing the data: p(θ).
▷A loss function that gives the cost mis-specifying the posterior distribution away from
the true form: L() (denoted with L here to distinguish it from a likelihood function).
This means that there are now three spaces over which distributions must be speciﬁed:
the data space X, the parameter space θ, and the decision space T .
This means that
an additional parametric form for the loss function is required above what was required
before, which can considerably add to the speciﬁcation burden. Unfortunately there are
many alternatives, and the complexity of these forms increases with the number of possible
decisions that can be made. In the simple hypothesis testing setup described above there are
only two alternatives but there can be many more (even an inﬁnite number). Fortunately
standard forms for the loss function have existed for over 200 years (Laplace 1774, Legendre
1805, Gauss 1855).
Naturally this choice of loss function is a subjective decision like all
model choices in statistics, regardless of paradigm. Some have argued that the choice of
loss function is tied to the choice of prior and that they should be speciﬁed in conjunction
(Lindley 1985). Bernardo and Smith (1994, Chapter 2) give a very detailed discussion of
linking beliefs (prior distributions) to actions and utilities.
The quality of the decision rule is generally judged by a corresponding risk function.
This is just the average loss across decisions by using this decision rule for a given value of
β, where the average can be over diﬀerent stochastic quantities (see below) depending on
the priorities of the researcher. Risk in this context therefore combines uncertainty with
cost. Decision rules will vary in quality since they stipulate diﬀering actions for the same
input. Importantly, though, good decision rules from this process have lower risk functions
than known alternatives and therefore manage uncertainty more eﬀectively. Much work has
been done to assess the quality of decision rules for such criteria (DeGroot 1970).
However, some social science research applications appear to be diﬃcult applications for
assigning speciﬁc deﬁnitions of risk in inferential decision-making. It is unclear what the
exact cost of being “wrong” is in analyzing second-party observational data for theoretical
topics.
Nonetheless this is a growth area and there are many well-developed resources
(Berger 1985; Brown, Cohen, and Strawderman 1980, 1989; Carlin and Louis 2009; Ferguson

Bayesian Decision Theory
249
1967; Gupta and Berger 1982; Lehmann and Casella 1998; Weiss 1961). Furthermore, this
is a very active research area for time-series data (Ni and Sun 2003, Clements 2004; Granger
1999; Chu and White 1982), notably in the last few years (Hong and Lee 2013, Turkov,
Krasotkina, and Mottl 2012, Demir, Bovolo, and Bruzzone, 2012).
Probably the biggest concern with applied decision theory in the social sciences is that
it can often be diﬃcult (or perhaps impossible) to specify an accurate utility function.
This may be either because not enough concrete information exists about costs/beneﬁts
or because the criteria are highly multidimensional with complex trade-oﬀs.
The same
issues apply to the more mechanical process of deﬁning loss functions. Another criticism is
that “optimal” point estimates based on this mechanical process are not sympathetic with
Bayesian reasoning that seeks to describe an entire posterior distribution. Finally, Gelman
et al. (2003, p.567) note that it is easy to manipulate the process, through the speciﬁcation
of decision criteria, to obtain some desired end.
Even given these warnings, there are important circumstances when risk and loss func-
tions should be included in a Bayesian model speciﬁcation. Primarily this is true in eco-
nomic, policy, and public management circumstances where gains and losses can be easily
quantiﬁed in units of dollars, lives, or time. In addition, some of the concerns listed above
can be mitigated by sensitivity analysis (Martin, Insua, and Ruggeri 1996) and robustness
(Abraham and Cadre 2004) around the choice of loss function analogous to the investigation
of alternative priors. Shao (1989) gives pre-MCMC simulations methods for dealing with
diﬃcult Bayesian decision theory calculations. Savage (1972) lays down a foundation for
the Bayesian treatment of decision problems that gives principled reasons why the produc-
tion of posterior quantities should be coupled with the costs of making decisions based on
them, and (Herman) Rubin (1987) demonstrated that the choice of loss function and prior
distribution cannot be done separately under a weak system of axioms with the assumption
of rational behavior.
8.2
Basic Deﬁnitions
In this section we clarify the key terms for both Bayesian and frequentist versions of
decision theory. Suppose we are interested in some unknown quantity, designated θ with
distribution f(θ). Since we are going to contrast Bayesian and frequentist decision theory,
we will also consider situations where θ is an unknown constant ﬁxed by nature. In either
case we are interested in making some explicit decision based on the data, which causes us to
perform some action.
Decisions and actions are often used synonymously in the literature,
as will be done here, but sometimes it is useful to separate the deliberative process of a
decision from the resulting physical action that is performed thereafter. For instance, if our
model produces a decision that some ﬁnancial security is expected to gain in value in the

250
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
near-term with high probability, then a rational resulting action would be to purchase some
amount of this security (other issues being equal).
8.2.1
Personal Preference
The ﬁrst concept that needs to be deﬁned is preference, which is just the notion that
the decider prefers certain outcomes over others. More formally, starting with an arbitrary
set of outcomes A, B, and C, the operator “≤” indicates that the ﬁrst outcome is “not
preferred” to the second outcome. A simple ordering results if for every possible A, B, and
C:
▷either A ≤B or B ≤A,
▷if A ≤B and B ≤C then A ≤C.
Sometimes this is called a weak ordering due to the less-than-or-equal relation. A strong
ordering results from the use of absolutely-less-than relations, “<”. It is also common to
use the notation A .= B to say that we are indiﬀerent between A and B. The full set of
outcomes that can occur is either ﬁnite or inﬁnite, although it is more common to work
with ﬁnite sets. For the ﬁnite set of outcomes above:
▷there always exists A and C in this set such for all B also in the set, A ≤B ≤C.
This just means that for a ﬁnite set of outcomes there is always a least preferred and
most preferred outcome in the existence of simple ordering. So under this circumstance
if outcomes are selected deterministically, then the rational actor simply picks their most
preferred state. What makes decision theory interesting is that these outcomes are asso-
ciated with a probability structure that does not guarantee such a deterministic choice.
Now preference is more nuanced. Would a decision-maker employ a strategy that gives
reasonably preferred outcomes with high probability over a most preferred outcome with
low probability? Obviously this would depend on the probabilities as well as characteristics
of the decision-maker.
8.2.2
Rules, Rules, Rules
To formalize how decisions are made we would like to have a codiﬁed rule that expresses
mathematically how the decision is made in potentially repeated applications and based on
some evidence in the form of data. Given a sample X assumed to be generated conditional
on θ by f(X|θ), we apply a decision rule, d in the set D, that dictates a speciﬁc decision
(action):
d(X) = A,
A ∈A
(8.1)
where A is the allowable class of actions. The decision rule is a function of X in the sense
that each draw of X creates an estimate of θ that is mapped to a speciﬁc action A.
Consider a very simple example that is well-known.
If a z-score test statistic for a

Bayesian Decision Theory
251
diﬀerence of means test between two continuously measured data vectors is greater than 1.96
in absolute value, then we will “decide” that these two samples are not drawn from the same
underlying population at the α = 0.05 level. This is a decision rule based on characteristics
of the data at hand where we will make one of two possible decisions according to:
d(X) =
⎧
⎨
⎩
same underlying population if
|z| < 1.96
diﬀerent underlying populations if
|z| ≥1.96
(8.2)
where z is the standard z-statistic produced in a diﬀerence of means analysis. The “rule”
here is determined by the magnitude of z. The necessity in this example of making a choice
distinguishes decision theory from data description.
8.2.3
Lots of Loss
The decision rule is also a result of a loss function, such that if X is observed and our
decision rule is d(X), then the loss is dictated by another function:
L(A, d(X)),
(8.3)
where smaller losses are preferred. Now L(A, d(X)) maps the decision/action made, A from
the decision rule d(X), to a quantiﬁable penalty. This idea of loss can be considered in the
negative sense as a penalty for decisions that are far from the true or optimal alternative
where the exactly correct decision results in a zero loss.
It can also be considered in
the positive sense as choosing from a set of alternative decisions, all of which provide
rewards, but some rewards are higher than others, motivating a search for the highest
possible return (so “losses” are unrealized gains). Bounding the loss function by (−∞:0)
is convenient and avoids a host of mathematical complexities that are not necessary here.
In more elaborate settings, such structures can be combined for a set of possible decisions
that return absolute losses or gains. DeGroot (1970) generalizes the directional implication
of loss and reward by using the term utility. Utility is a personalistic consideration that
simply means that outcomes can be ordered in terms of preference by the relevant actor
as described in Section 8.2.1. Note that these outcomes can be both positive and negative.
However, bounding the loss function to (−∞: 0), as commonly done, restricts utility to
positive outcomes deﬁned by L(A, d(X)) = −U(A, d(X)).
Since a loss is associated with a particular action, we may want to consider an overall
sense of loss from the set of possible decisions. Denote π(θ|X, A) as the posterior distribution
produced from observing X and taking action A as inputs to the model. Then:
Eπ [L(A, d(X))] =

Θ
L(A, d(X))dFπ(θ)
(8.4)
is the Bayesian expected loss (sometimes called the posterior expected loss) over the set of
possible decisions according to the associated posterior distribution π(θ|X, A). Since this
averages over the loss in θ conditional on an observed X, it is the average loss resulting

252
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
from having to make some decision for this problem. This is an overtly Bayesian statement
of loss since we are averaging over uncertainty in the posterior distribution of θ, not over
the distribution of data since it has already been observed. Thus the Bayesian version is
more principled in a world where social scientists are not confronted with an endless stream
of iid data. Note also that while the integration is performed over the space of the posterior
distribution, the Bayesian expected loss that results is a scalar value.
Loss functions can take on many mathematical forms, mostly with stipulated penalty
for moving away from some ideal point. Common examples of loss functions for the true θ
and the estimated θA include:
▷squared error loss: L(A, d(X)) = (θ −θA)2
▷absolute error loss: L(A, d(X)) = |θ −θA|
▷norm loss for vectors: L(A, d(X), k) =∥θ −θA ∥k, for k ∈I+k (k > 1)
▷0 −1 loss for discrete state spaces: L(A, d(X)) = 0 if θ = θA, and 1 otherwise
▷interval loss: L(A, d(X)) = 0 if CI1−α[θA] covers θ, and 1 otherwise
▷entropy loss:

X log f(X|θ)
f(X|ˆθ)f(X|θ)dx
▷LINEX loss: L(A, d(X), k) = exp[k(θ −θA)] −k(θ −θA) −1, where k is some assigned
constant (Varian 1975).
Note that these are frequentist-oriented deﬁnitions since there is an assumed true θ. The
Bayesian analog uses these same parametric forms but only as input into an expected loss
calculation as in (8.4).
So θ is then treated distributionally and summarized with the
Bayesian expected loss value over this distribution.
A more directly Bayesian version of the loss functions above substitutes interval decisions
for point estimate decisions. Instead of θ = θA, consider two intervals that constitute a
partition of the parameter space such that θ ∈ΘA or θ ∈ΘB. Now set cA as the cost of
incorrectly deciding that θ ∈ΘB (d(X) = B), and cB as the cost of incorrectly deciding
that θ ∈ΘA (d(X) = A). The complete loss function is now:
L(A, d(X)) =
⎧
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎩
cA
if θ ∈ΘA
and d(X) = B
cB
if θ ∈ΘB
and d(X) = A
0
otherwise.
(8.5)
Since cA does not have to be set equal to cB, this setup can have asymmetric costs for
wrong decisions.
An associated decision rule is picking the interval that produces the
highest posterior probability from a speciﬁed model:
d(X) =
⎧
⎨
⎩
A
if π(θ ∈ΘA|X) > π(θ ∈ΘB)
B
if π(θ ∈ΘA|X) < π(θ ∈ΘB)
(8.6)
■Example 8.1:
Example:
Interval Decisions with the French Labor Strike
Data.
Returning to the data and analysis in Section 7.2.1.1, on page 212 ,we deﬁne

Bayesian Decision Theory
253
ΘA = (0:0.05) and ΘB = [0.05:∞). From the calculation of the posterior in that
section, we have:
π(θ ∈ΘA|X) = 0.1715
and
π(θ ∈ΘB|X) = 0.8285.
(8.7)
Therefore according to (8.6) above d(X) = B. Suppose it was much worse to incor-
rectly choose B, thus being over-optimistic about the cessation to the series of strikes.
In this case we might set cA = 1 and cB = 10 to make the cost of B ten times higher,
giving:
L(A, d(X)) =
⎧
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎩
1
if θ ∈ΘA
and d(X) = B
10
if θ ∈ΘB
and d(X) = A
0
otherwise.
(8.8)
This might or might not change our decision, depending on how we determine risk.
8.2.4
Risky Business
The notion of frequentist Risk is just the average loss across the data for a given decision
rule:
RF (θ, d(X)) = EX [L(A, d(X))] =

X
L(A, d(X))dFX (x|θ).
(8.9)
That is, frequentist risk integrates over the distribution of the data conditional on the
deﬁned decision rule for this problem, giving the expected loss of applying the same decision
rule over and over again as new datasets arrive. Thus (8.9) preserves the essential frequentist
notion of a ﬁxed underlying parameter and an endless stream of iid data. This is a diﬀerent
assumption about uncertainty than (8.4) since it is concerned with uncertainty from a
distribution of data given a ﬁxed parameter, rather than a uncertainty from a distribution
of the parameter given a ﬁxed set of data. Furthermore, given a single dataset at hand, this
approach may not be appropriate since the expectation is taken over the parametric form
describing the data generation process not the single sample of data itself. This distinction
is only unimportant with large samples or repeated sampling from the same data-generating
mechanism, both conditions that are not pervasive in the social sciences.
In specifying a risk function, the frequentist researcher may want to compare alter-
natives.
Suppose we have two candidate decision rules, d1(X) and d2(X), then d1(X)
is called R-better (for “risk better”) than d2(X) if for every possible value of θ we have
RF (θ, d1(X)) ≤RF (θ, d2(X)), and for at least one value of θ we have RF (θ, d1(X)) <
RF (θ, d2(X)). For the frequentist this becomes an absolutist criteria such that some Rd(X)
is called admissible if there is no R-better alternative (not worse than every other decision
rule for all possible values of the parameter of interest), and inadmissible otherwise. Such
criteria may be overly rigid since it is unlikely that a single alternative be pairwise R-better
in the same way than all alternatives with regard to the less-than or equal-to nature of
the deﬁnition. Relatedly, the frequentist deﬁnition of an unbiased decision rule also uses
expected loss compared to alternatives, but focuses on diﬀerences for true and false values

254
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
of θ. A decision rule in this context is unbiased if it has no greater expected loss (frequentist
risk) for the true value of θ than for all false values, θ′, according to:

X
L(A, d(X))dFX (x|θ) ≤

X
L(A, d(X))dFX (x|θ′).
(8.10)
Notice again the hard-coded frequentist language here about true and false ﬁxed parameter
values. Unfortunately, requiring unbiased rules can lead to many pathological and illogical
results (Ferguson 1967), including estimates outside of the allowed parameter space.
To further illustrate frequentist risk, suppose X1, . . . , Xn are iid with true mean μ and
true variance σ2. Our decision rule is d(X) = ¯X, for estimating μ. The chosen loss function
here is squared error loss: L(A, d(X)) = (μ −¯X)2, meaning that the penalty for decisions
increases quadratically as this estimator moves in either direction away from the true mean.
Since we have an endless stream of samples (a rarity in this book!), we will perform this
estimation many times and then become concerned with the frequentist risk:
RF (θ, d(X)) = EX[(μ −¯X)2]
= EX[μ2 −2μ ¯X + ¯X2]
= μ2 −2μEX[ ¯X] + EX[ ¯X2]
= μ2 −2μEX[ ¯X] + Var[ ¯X] + (EX[ ¯X])2
= Var[ ¯X]
= σ2
n .
Frequentist risk is shown to be easy to determine given a known loss function and decision
rule. The downside is, of course, the assumption of ﬁxed parameters and inﬁnite data,
which does not comport well with most social and behavioral science settings.
Bayesian risk analysis starts with the observation that the loss as given here is a function,
and it is therefore diﬃcult to directly compare two losses. So we use the average of the risk
function integrating (ﬁrst) over the prior distribution, p(θ), instead of the distribution of
the data as done before. The Bayesian prior risk of a decision rule (sometimes called the
preposterior risk) is then:
Rb(θ, d(X)) = Ep [RF (A, d(X))]
=

Θ
RF (A, d(X))dFp(θ)
=

Θ

X
L(A, d(X))dFX (x|θ)dFp(θ),
(8.11)
where we use the form of the frequentist risk function but not the frequentist interpretation.
In this expression dFX denotes integration over the joint distribution of the data, and dFp(θ)
denotes integration over the prior distribution of the parameter θ. Here the model is ﬁxed,
the data is ﬁxed but yet to be observed, and only the decision rule is to be determined.
Since the data are yet to be observed, this is a hypothetical Bayesian statement. Bayesian

Bayesian Decision Theory
255
posterior risk (also called integrated risk) is more useful, substituting the posterior for the
prior by conditioning on an observed dataset:
RB(θ, d(X)) = Eπ [R(A, d(X))]
=

Θ
RF (A, d(X))dFπ(θ)
=

Θ

X
L(A, d(X))dFX (x|θ)dFπ(θ),
(8.12)
where dFπ(θ) denotes integration over the posterior distribution of θ. This now reﬂects an
update of the prior risk resulting from the obtaining data X. Note that this is also still
conditional on the form of the prior, and we have just added the eﬀect of the data through
Bayes’ Law. Like the Bayesian expected loss, the Bayesian posterior risk is a single scalar
so that we can directly compare risk under diﬀerent decision rules where the decision rule
with the smallest Bayesian risk should be preferred.
An estimator that minimizes the Bayesian posterior risk is one that for every X ∈X
speciﬁes a decision rule that minimizes the posterior expected loss (Eπ [L(A, d(X))]). The
decision rule that minimizes RB(θ, d(X)) for every X ∈X is called optimal, and limiting
the set to this choice is called a Bayes rule, denoted 6
RB(θ, d(X)) here, and deﬁned by the
value of θA that satisﬁes:
6
RB(θ, d(X)) = inf
θA

Θ
RF (A, d(X))dFπ(θ).
(8.13)
Analogous to the analytical calculation of maximum likelihood estimates, we can set the
derivative of this expression equal to zero and solve for the resulting minimum value of θA (it
is a convex rather than concave function, in contrast to many other statistical calculations).
For example, if we apply squared error loss then the process for mean estimation is:
0 = d
dθ
'
Θ
(θ −θA)2dFp(θ)
(
= d
dθ
'
Θ

θ2 −2θθA + θ2
A

dFp(θ)
(
= d
dθ
'
Θ
θ2dFp(θ) −2θA

Θ
θdFp(θ) + θ2
A

Θ
dFp(θ)
(
= d
dθ
	
Varf(θ|x)[θ] −2θAEf(θ|x)[θ] + θ2
A

= 0 −2Ef(θ|x)[θ] + 2θA.
(8.14)
Therefore the Bayes rule estimate, θA, is the posterior mean, Ef(θ|x)[θ]. Similarly, if we
specify absolute error loss, L(A, d(X)) = |θ −θA|, instead, then the Bayes rule estimate is
the posterior median (Exercise 6). Also, for the 0-1 error loss, L(A, d(X)) = 0 if θ = θA,
and 1 otherwise, the Bayes rule estimate is the posterior mode. Obviously with unimodal
(non-uniformly so) symmetric posterior forms, these are concurrent.
■Example 8.2:
Normal Distribution Application. As an example of Bayesian risk

256
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
analysis, consider the frequentist mean estimation process above but adding a normal
prior distribution for μ: N(m, s2), for hyperprior values m and s2. We will use squared
error loss again, except that in this case the posterior mean for μ, ˆμ = n¯x+ms0
n+s0
from
the derivations that produced (3.18) on page 76, is an improved substitute for the
data mean. This produces:
RB(θ, d(X)) =

Θ
RF (A, d(X))dFπ(θ)
=

μ

μ −n¯x + ms0
n + s0
2 
2π
σ2
n + s0
−1
2
exp

−
1
2σ2/(n + s0)

μ −n¯x + ms0
n + s0
2
dμ.
If we make the change of variable calculation δμ = μ −ˆμ = μ −(n¯x + ms0)/(n + s0),
it is easy to see that this is expression is just RB(θ, d(X)) = E[δ2
μ]. Therefore:
RB(θ, d(X)) = Var[δμ] + (E[δμ])2 = Var[μ] −0 =
σ2
n + s0
.
(8.15)
Where the variance of δμ equals the variance μ since ˆμ consists of all constants (ob-
served or set), E[δμ] = 0 by design, and the ﬁnal form of the variance comes from
(3.18). We now see a practical implication for the s0 parameter introduced on page 74.
This parameter can be used by the researcher to scale the Bayesian risk relative to fre-
quentist risk for this normal problem. Increasing values in (0:∞) decrease Bayesian
risk (zero is excluded due to the functional form of p(μ|m, σ2/s0), implying useful
prior information. Also, the relationship demonstrated here comes from the mean
being conditional on the variance in the conjugate normal setup.
8.2.4.1
Notes on Bayes Rules
From the deﬁnition of Bayes rule above we get some very important results. First, any
Bayes rule corresponding to a proper prior is admissible: there is no R-better alternative
(Bernardo and Smith 1994). This is a very powerful argument for the use of proper forms
since it means that we do not have to worry about a large set of alternative rules that result.
It also works in reverse: all admissible rules must be Bayes rules (Exercise 14).
Unfortunately, some Bayes rules that correspond to improper priors are inadmissible
(James and Stein 1961), see the discussion in Section 8.4. Restricting the set of alterna-
tives to Bayes rules also has a computational advantage: deriving Bayes rules circumvents
specifying the risk function and needs only to minimize the posterior expected loss in (8.4).
Together all of these ﬁndings mean that Bayesian decision theory dominates frequentist
application in both theoretical and practical ways.
■Example 8.3:
Nuclear Deterrence Between the United States and the So-
viet Union Patterson and Richardson (1963) consider the problem of risk in treaty
observance between the two great Cold War powers (a very topical issue at the time
of their writing). The key problem is whether or not to agree to a speciﬁc treaty

Bayesian Decision Theory
257
restricting the number of thermonuclear tests, given the subjective probability that
policy makers have about the probability that the other side will cheat. Speciﬁcally,
should the United States pursue a negotiation strategy of insisting on a small number
of tests (which is good for humanity), knowing that it increases the probability that
the Soviet Union will cheat. This is critical at the time since there did not yet exist
perfect detection of such events.
So consider RT to be the risk under some negotiated treaty and RN is the risk of no
treaty and unrestricted testing. The deﬁnition of risk here is not only increasing the
probability of global thermonuclear war under some treaty scenario, but also the cost
of “losing face” in the presence of cheating by the other side and therefore a weakened
negotiating position in the next round. Label n the actual number of tests by the
other side, which is observed imperfectly with an observed value d for detections.
These detections, however, are split between r real events and u unreal events (false
alarms): d = r +u. The number of detections is assumed to be modeled as a binomial
with number of possible events k and probability p, r ∼BE(n, p). Also, u is stochastic
so we model it with a Poisson process (our protagonist side does not know the upper
bound of the cheating on the other side) with intensity parameter λ: u ∼P(λ). Since
n > d = r + u, we also know that u = d −r. This leads to a convolution (summing)
of g(r) and g(u) (Casella and Berger 2002, p.215) to get the probability of detections
given events (e.g., the reliability of the veriﬁcation process):
p(d|n) =
d

r=0
'n
r

pr(1 −p)n−r
( 'e−λλd−r
(d −r)!
(
.
(8.16)
Therefore E[d|n] = pn + λ and Var[d|n] = p(1 −p)n + λ (Exercise 9.6).
Of course from a gamesmanship perspective we are really more interested in estimating
the probability of actual events given detections, which is obtained by ﬁrst specifying
a prior distribution, p(n), and then applying Bayes’ Law,
π(n|d) =
p(n)p(d|n)

k p(n)p(d|n),
(8.17)
to get this posterior distribution.
Deﬁne Lr(d),n as the loss from specifying response rule r(d) for n actual events. This
response can be from diplomatic, conciliatory, or belligerent actions. The expected
loss is therefore the sum over detected events of this loss times our distribution of
detections:
ρ(n) =

d
Lr(d),np(d|n).
(8.18)
That is, our actor chooses a loss function related to the probability of detections
because this calculation occurs before negotiations are ﬁnished and any events can be
observed. Determining ρ(n) in advance means that we want to minimize this expected

258
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
loss with respect to the prior distribution on n calculated by:
ρ = Ep(n) [ρ(n)]
= Ep(n)

d
Lr(d),np(d|n)

=

n

d
Lr(d),np(d|n)p(n)
=

d
 
n
p(d|n)p(n)
 
!"
#
normalizing constant

n
Lr(d),n
 !" #
loss
p(d|n)p(n)

n p(d|n)p(n)
 
!"
#
π(n|d)

.
(8.19)
So the response rule that minimizes ρ is really obtained with respect to a calculation
on the posterior distribution of n.
What this states in substantive terms is that
the model update provides the best incorporation of risk into the calculation. This
decision will determine the strategy for negotiating RT .
8.2.5
Minimax Decision Rules
One way to narrow down the set of alternative decision rules is to apply a very con-
servative criteria that minimizes the worst possible risk over the sample space. If D is the
set of alternative decision rules, we want d∗(X) that gives the minimum maximum-possible
value:
sup
θ∈Θ
RF (A, d∗(X)) =
inf
d(X)∈D sup
θ∈Θ
RF (A, d(X)).
(8.20)
On the right-hand side, inf
d∈D speciﬁes the d(X) that gives the smallest (“inﬁmum”) value of
RF (A, d(X)) where sup
θ∈Θ
requires the value of θ that gives the largest (“supremum”) value
of RF (A, d(X)), in the frequentist sense. There are several challenges with this approach:
▷Sometimes such decision rules may be very diﬃcult to calculate.
▷The d∗(X) may give very high risk over areas of the parameter space that fall just
short of being the maximum, and therefore provide overall unacceptable risk. Second,
minimax estimators can be barely unique, and so the choice-set can remain unhappily
large.
▷Protection against the worst possible result may be reasonable in the competitive
context of game theory; it does not have an analogous motivation in standard data
analysis.
▷Minimax decision rules are generally not unique (Shao 2005), although unique mini-
max estimators are admissible!
▷Some choices violate the transitivity of preference ordering given in Section 8.2.1
(Lindley 1972).
▷Unique Bayes estimates from a constant risk function (RB(θ, d(X)) = k) are minimax
decision rules and this may not be substantively defensible.

Bayesian Decision Theory
259
▷Minimax decision rules are Bayes rules deﬁned with respect to the prior distribution
that maximizes Bayes risk (Lehmann 1986).
The last point above is worth dwelling on. It turns out that this prior distribution is
also called the least favorable prior in the sense that any other prior chosen has the same
or lower Bayesian posterior risk: RB(θ, dminimax(X)) ≥RB(θ, dother(X)). What makes this
interesting is that it ties together the form of the prior with the choice of decision rule.
Robert (2001, pp.251-252) also notes that in conventional hypothesis testing, this approach
is always biased against the null hypothesis (see also Berger and Sellke [1987] and Berger
and Delampady [1987]).
The penultimate point above is also more nuanced than it appears at ﬁrst.
Under
very general assumptions, a unique Bayes estimator with constant risk is minimax, and
admissible. However, an estimator that is not a unique Bayes form may be either admissible
or non-admissible, and a unique Bayes estimator with constant risk can be minimax but
non-admissible (Ferguson 1967). Also, Bayes estimators do not cover the set of admissible
estimators for inﬁnite parameter space (they do for ﬁnite parameter spaces). For example
(Lehman 1986), suppose X1, . . . , Xn is a sample from a normal distribution N(θ, 1), and
we stipulate squared error loss. Then the sample mean, ¯X is both unbiased and admissible
for θ with positive variance, but is not a Bayes estimate corresponding to any possible prior
distribution.
8.3
Regression-Style Models with Decision Theory
Applying decision theory to linear and generalized linear models usually means focusing
on making reliability decisions about estimated regression parameters, often in the process
of model choice (Brooks 1972, Brown, et al. 1999, Lewis and Thayer 2009). The estimation
of σ2 in a linear context, and related quantities in hierarchical or nonlinear forms, is usually
treated as a secondary concern and not applied as a loss function. For such purposes modify
(8.3) to be:
L(βk, A, d(X)),
(8.21)
for the true kth regression coeﬃcient, taking the Ath action, with data-based decision rule
d(X). Here we temporarily suppress the k notation such that ˆβ is the estimate of β for this
general k variable. For most social scientists d(X) is a dichotomous or trichotomous decision:
d(X) =
⎧
⎨
⎩
ˆβ is a reliable estimator if

ˆβ
se ˆ
β
 ≥1.96
ˆβ is a not reliable estimator if

ˆβ
se ˆ
β <
 1.96.
(8.22)

260
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
and,
d(X) =
⎧
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎩
ˆβ is a highly reliable estimator if

ˆβ
se ˆ
β
 ≥2.576
ˆβ is a reliable estimator if
1.96 ≤

ˆβ
se ˆ
β
 < 2.576
ˆβ is a not reliable estimator if

ˆβ
se ˆ
β
 < 1.96.
(8.23)
While these are generally not principled thresholds, as discussed in Chapter 7, they are
pervasive in most social and behavioral science literatures. Furthermore, this discussion is
relevant here since many Bayesian results are reported with 95% HPD regions, although
Bayesians tend to see this as a convenient convention rather than prescribing any theoretical
importance to the choice.
More formally for our purposes, let β ∈B be the true population parameter and ˆβ
be its estimate from some regression result. The researcher deﬁnes two non-overlapping,
exhaustive regions of the support for β, BI, and BII and wants to make a decision about the
corresponding location of the true parameter. The ﬁrst decision rule above can formalized
by two actions: AI for deciding that β is in region BI, and AII, for deciding that β is in
region BII. These are associated with negative utilities (costs) for being wrong, UI and
UII, deﬁning the well-known loss function:
L(βk, A, d(X)) =
⎧
⎪
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎪
⎩
UI
if β ∈BII and A = AI
0
if β ∈BI and A = AI
UII
if β ∈BI and A = AII
0
if β ∈BII and A = AII.
(8.24)
This means that the Bayes (optimal) action is determined probabilistically by regions of
the posterior distribution:
ˆA =
⎧
⎨
⎩
AI
if π(BI) >
UII
UI+UII
AII
if π(BI) <
UII
UI+UII .
(8.25)
In other words, if the posterior probability of a given action is higher than the loss proportion
of the alternative, then it is rational to take this action. Kempthorne (1986) contrasts this
(traditional) decision-theoretic approach to coeﬃcient hypothesis testing with the loss from
simply choosing the posterior mean as the decision with squared error loss. For a selected
value of the posterior mean for the regression coeﬃcient, β∗, the posterior risk reduces to:
RB(β, d(X)) = se ˆβ + ∥β∗−ˆβ∥,
(8.26)
meaning that Bayesian posterior risk under these circumstances is always minimized by
selecting the Bayesian posterior mean. The decision-theoretic setup above based on tradi-
tional hypothesis testing steps gives the posterior risk:
RB(β, d(X)) = UIπ(BI)IAII + UIIπ(BII)IAI ,
(8.27)
which is just the expected loss of a choice times the probability that it was the wrong choice,

Bayesian Decision Theory
261
where IAi is an indicator function for making the ith choice, i ∈{1, 2}. This contrast in
risks rests on size of the posterior standard error for the coeﬃcient and the costs of making
wrong choices.
This is also the contrast, in a Bayesian sense, from reporting posterior
summaries versus making a sharp decision based on the posterior distribution.
8.3.1
Prediction from the Linear Model
In Chapter 5 (Section 5.2, page 155) the posterior predictive distribution of future
y values from a Bayesian linear model was shown to have the multivariate Student’s-t
distribution:
π(˜y| ˜X, X, y) ∝

(n −k) + (˜y −˜Xˆb)′H(˜y −˜Xˆb)
−n+q−k
2
,
(8.28)
with mean vector and variance-covariance matrix:
E[˜y] = ˜Xˆb,
Cov[˜y] =
n −k
n −k −2H−1
(8.29)
where H = (I −˜XM−1 ˜X′)/ˆσ2, and M = I = X(X′X)−1X′. The quadratic loss function
for this setup is:
L(ˆb, A, d(X)) = (˜y −¯y)′Υ(˜y −¯y),
(8.30)
where Υ is any positive deﬁnitive k × k matrix of ﬂexible form. Then ˜y is the single point
prediction that minimizes Bayesian expected loss:
Eπ

L(ˆb, A, d(X))

=

β
L(ˆb, A, d(X))dFπ(β)
(8.31)
(proof given in Zellner and Chetty 1965, p.610).
This shows that squared error loss is
directly tied to normal-linear assumptions. In fact, if y is distributed N(μ, σ2) and we want
a prediction under the assumption of squared error loss with chosen estimator μA, then the
frequentist risk is:
Ey [L(μ, μA)] = Ey[(y −μA)2]
= Ey[(y −μ + μ −μA)2]
= Ey[(y −μ)2] + Ey[(μ −μA)2]
= σ2 + (μ −μA)2.
(8.32)
Since σ2 and μ are ﬁxed quantities, then we actually only care about squared expected loss
that results from the choice of estimator of μA to minimize expected loss, according to some
stated criteria.
■Example 8.4:
Prediction Risk for a Model of Educational Eﬀects. The second
Bayesian linear model based on work by Meier, Polinard, and Wrinkle (2000) from
Example 5.2 starting on page 157 is further analyzed here. This model used results
from earlier work with educational data from Florida by Meier and Smith (1994)

262
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
to inform priors for an additional analysis of the outcome: percent of students in
district/year that pass the Texas Assessment of Academic Skills (TAAS).
The quantities of interest are obviously regression coeﬃcients. So consider three sce-
narios: [β = −0.5, σ2
β = 0.01], [β = 0.0, σ2
β = 0.03], and [β = 0.5, σ2
β = 0.01]. We
clearly would not normally apply the same criteria to each of the regression coeﬃ-
cients from the model, but doing so will show a range of eﬀects for illustration here.
Using the model estimates in Table 5.6 on page 160, we can calculate the frequentist
risk identiﬁed in (8.32) above. This is give in Table 8.1.
TABLE 8.1:
Prediction Risk: Education Outcomes Model
Explanatory Variables
Mean
SE
Ey [L(−0.5, 0.01)]
Ey [L(0.0, 0.03)]
Ey [L(0.5, 0.01)]
Intercept
4.799
2.373
28.080
23.031
18.482
Low Income Students
-0.105
0.006
0.156
0.012
0.366
Teacher Salaries
0.382
0.099
0.778
0.147
0.014
Teacher Experience
-0.066
0.046
0.188
0.005
0.320
Gifted Classes
0.096
0.021
0.355
0.010
0.163
Class Size
0.196
0.191
0.485
0.039
0.093
State Aid Percentage
0.002
0.004
0.252
0.001
0.248
Funding Per Student (×1000)
0.049
0.175
0.302
0.003
0.204
Lag of Student Pass Rate
0.684
0.008
1.402
0.469
0.034
Lag of Bureaucrats
-0.042
0.261
0.210
0.003
0.294
Class Size × Teacher Salaries
-0.015
0.007
0.235
0.001
0.265
Clearly the constant term fares the worst since it is the most disproportional to the
criteria. Of the others, the estimate for Lag of Student Pass Rate has high frequentist
risk based on its relative large coeﬃcient estimate compared to {−0.5, 0.0, 0.5}. The
range of values across rows is also interesting since even though the criteria appeared
to be similar, they had a large range of eﬀects with the middle column showing lower
risk due to μ = 0.
8.4
James-Stein Estimation
Stein (1955), writing in the context of a two-person zero-sum game, ﬁnds that the maxi-
mum likelihood estimator for the mean of a multivariate normal distribution is inadmissible
under mean squared error risk for three or more dimensions. James and Stein (1961) then
derived an alternative estimate that has lower squared error loss. This paradoxical result

Bayesian Decision Theory
263
was very surprising at the time and still seems like an oddity. In short, it was demonstrated
that if one was willing to increase bias, the variance of the estimator could be reduced
dramatically. We will brieﬂy discuss these estimators and their properties in this section,
but for more detailed reviews see Lehmann and Casella (1998, Chapter 4), Carlin and Louis
(2009, Chapter 5), and for nice overviews see Stein (1981), Lehmann and Casella (1998), or
Heumann (2011).
First we will deﬁne a simpliﬁed multivariate normal model where an n-length data vector
is distributed N(μ, I). This is a very basic model since all of the variances are assumed to
be equal to one. Start with the mean prior deﬁned in (3.21) on page 77:
μ|Σ ∼Nn

0, τ2I

,
(8.33)
except we specify prior mean zero for all dimensions and τ2 is temporarily assumed to be
known. Consider this to be μi mean values for xi data values. Deﬁne b =
τ 2
τ 2+1. From
(3.24), this leads to the following succinct posterior distribution for vector μ:
μ|x ∼Nn (bx, bI)
(8.34)
(Exercise 17). Now suppose that we use some arbitrary point estimate ˆμ from this distribu-
tion such as the posterior mean. Then the squared error loss function for the total sample
is:
L(A, d(X)) =
N

i−1
(μi −ˆμi)2,
(8.35)
with associated frequentist risk
RF (μ, d(X)) = EX
 N

i−1
(μi −ˆμi)2

.
(8.36)
For ˆμ = ¯x, the maximum likelihood point estimate, this gives RF (μ, d(X)) = n (Exer-
cise 15). The corresponding Bayesian risk is RB(μ, d(X)) = bn from (8.15). So depending
on the speciﬁed value of τ 2, the Bayesian alternative could have much lower risk. For large
values of b there will not be much of a diﬀerence since b will be close to one, but for small
values of b the Bayesian alternative will be superior (lower risk).
Now suppose that we want to estimate τ 2 instead of assuming that it is known a priori.
One way to do this is to use the data in an additional way (Efron and Morris 1972a, 1972b).
This is called empirical Bayes and we will cover the topic in more detail in Section 8.5. The
marginal distribution of the data (integrating out μ), is x ∼N(0, (τ 2 + 1)I). Subsequently
the scaled sum of squares for the data is chi-square distributed according to: x′x/(τ 2 +1) ∼
χ2
df=n (Keating and Mason 1988). This means that Ex[n −2/x′x] = 1/(τ 2 + 1). Carlin and
Louis (2009), and others, label B = τ2/(τ 2 + σ2) where σ2 is the variance of the data and
τ 2 is the prior variance of μ. This now replaces the more basic b = τ2/(τ 2 + 1) used above.
We make use of the most straightforward possible estimation process where each μi is

264
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
estimated with the corresponding xi collected in the vector x. By the logic in (8.13) on
page 255, and in the notation above, the Bayes rule for this estimator gives the posterior
distribution μ|x ∼N(Bx, BI) where again B = τ2/(τ 2 +σ2). For a prior with mean vector
0, the Bayes estimator is:
μ(x) =

1 −
1
1 + τ 2 σ2

x.
(8.37)
The maximum likelihood estimate is given simply by the vector μMLE(x) = x, giving
squared error loss L(A, d(X)) = (x −μ)2. This means that the frequentist risk is:
RF (μMLE, d(X)) = EX [L(A, d(X))] = EX

1
n
n

i=1
(μ −x)2

= σ2.
(8.38)
Stein’s 1955 proof showed that this is inadmissible for three or more dimensions, and the
1961 paper by James and Stein gave an estimator with frequentist risk no larger than σ2.
The James-Stein estimator is created by modifying the usual Bayesian estimator in this
context:
μJS(x) =

1 −(n −2)
x′x
σ2

x,
(8.39)
requiring n > 2 to avoid additional complexities (see Lehmann and Casella [1998, p.275-277]
for details). Notice that this is the Bayes estimator in (8.37) with (n−2)/x′x substituting for
1/(1+τ 2). The James-Stein estimator is also called a “shrinkage estimator” since (n−2)/x′x
is shrinking the posterior estimate towards the prior mean of zero. The frequentist risk of
this estimator is given by:
RF (μJS, d(X)) = EX

1
n
n

i=1
(μ −μJS)2

=
'
1 −n −2
n
EX
(n −2)
x′x
σ2
(
.
(8.40)
Notice that RF (μJS, d(X)) is always less than RF (μMLE, d(X)) since the second term
inside the parentheses is necessarily always positive. While discussed only in the context of
a simple normal model with mean zero, the important theorem given by James and Stein
(1961) shows that the James-Stein estimator dominates the maximum likelihood estimator
in terms of risk in all μ settings where n > 2.
Naturally, it is not required that we have prior means of zero, and specifying a diﬀerent
prior mean vector only slightly complicates the equations. The data are now assumed to
be distributed N(μ, σ2I), i = 1 . . . , n where σ2I is a diagonal matrix of identical variances.
We now restate the prior distribution for μ as:
μ|Σ ∼Nn

m, τ2I

.
(8.41)
This means that the marginal distribution of the data (integrating out μ), is
x ∼N(m, (τ2 + σ2)I),
(8.42)
and the posterior distribution for μ is:
μ|x ∼Nn

m(1 −B) + Bx, Bσ2I

.
(8.43)

Bayesian Decision Theory
265
The Bayes rule m(1 −B) + Bx gives the James-Stein estimator for each of i = 1, . . . , n:
μJS(xi) = ¯x +

1 −
(n −3)
n
i=1(xi −¯x)2 σ2

(xi −¯x),
(8.44)
now requiring n > 3. This new estimator obviously generalizes the form of (8.39).
The James-Stein result means that there is unseen latent information between seemingly
separate and independent decisions that is not used when modeled in isolation but appears
when analyses are performed simultaneously and connected by the loss function. Essentially
risk is reduced because each case borrows strength from the rest of the others, thus reducing
uncertainty. A 1977 article in Scientiﬁc American by Efron and Morris provides a nice
intuitive discussion of these ideas. Furthermore, notice that μJS(x) is estimated empirically
and simultaneously from the data using n identical speciﬁcations. Thus the James-Stein
estimator is considered an empirical Bayes process.
■Example 8.5:
Government Social Spending in OECD Countries. The follow-
ing data (also provided in BaM) are the total public social expenditure as a percentage
of GDP for 28 OECD countries (Organization for Economic Cooperation and Devel-
opment) over the years 2006-2013 (OECD Social Expenditure Statistics Database).
Chile, Japan, Korea, Mexico, and Turkey were members of the OECD during this pe-
riod but are dropped from the analysis due to severe data problems. Social spending
is considered an important indicator of national policy.
2006
2007
2008
2009
2010
2011
2012
2013
Australia
0.165
0.164
0.178
0.178
0.179
0.182
0.188
0.195
Austria
0.268
0.263
0.268
0.291
0.289
0.279
0.279
0.283
Belgium
0.260
0.260
0.273
0.297
0.295
0.297
0.305
0.307
Canada
0.169
0.168
0.176
0.192
0.187
0.181
0.181
0.182
Czech Republic
0.183
0.181
0.181
0.207
0.208
0.208
0.210
0.218
Denmark
0.271
0.265
0.268
0.302
0.306
0.306
0.308
0.308
Estonia
0.127
0.127
0.158
0.200
0.201
0.182
0.176
0.177
Finland
0.258
0.247
0.253
0.294
0.296
0.292
0.300
0.305
France
0.298
0.297
0.298
0.321
0.324
0.320
0.325
0.330
Germany
0.261
0.251
0.252
0.278
0.271
0.259
0.259
0.262
Greece
0.213
0.216
0.222
0.239
0.233
0.244
0.241
0.220
Hungary
0.228
0.230
0.231
0.239
0.229
0.219
0.216
0.216
Iceland
0.159
0.153
0.158
0.185
0.180
0.181
0.176
0.172
Ireland
0.161
0.167
0.197
0.236
0.237
0.233
0.224
0.216
Israel
0.158
0.155
0.155
0.160
0.160
0.158
0.158
0.158
Italy
0.250
0.247
0.258
0.278
0.277
0.275
0.280
0.284
Luxembourg
0.218
0.203
0.208
0.236
0.230
0.226
0.232
0.234
Netherlands
0.217
0.211
0.209
0.232
0.234
0.234
0.240
0.243
New Zealand
0.189
0.186
0.198
0.212
0.213
0.214
0.220
0.224
Norway
0.203
0.205
0.198
0.233
0.230
0.224
0.223
0.229
Poland
0.208
0.197
0.203
0.215
0.218
0.205
0.206
0.209
Portugal
0.230
0.227
0.231
0.256
0.254
0.250
0.250
0.264
Slovak Republic
0.160
0.157
0.157
0.187
0.191
0.181
0.183
0.179
Slovenia
0.208
0.195
0.197
0.226
0.236
0.237
0.237
0.238
Spain
0.211
0.213
0.229
0.260
0.267
0.264
0.268
0.274
Sweden
0.284
0.273
0.275
0.298
0.283
0.276
0.281
0.286
United Kingdom
0.203
0.204
0.218
0.241
0.238
0.236
0.239
0.238
United States
0.161
0.163
0.170
0.192
0.198
0.196
0.197
0.200

266
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
TABLE 8.2:
MLE and James-Stein
Estimates of Social Spending
μMLE(x)
2007-2013 mean
μJS(x)
Australia
0.165
0.181
0.181
Austria
0.268
0.279
0.249
Belgium
0.260
0.291
0.243
Canada
0.169
0.181
0.183
Czech Republic
0.183
0.202
0.193
Denmark
0.271
0.295
0.251
Estonia
0.127
0.174
0.156
Finland
0.258
0.284
0.242
France
0.298
0.316
0.268
Germany
0.261
0.262
0.244
Greece
0.213
0.231
0.212
Hungary
0.228
0.226
0.222
Iceland
0.159
0.172
0.177
Ireland
0.161
0.216
0.178
Israel
0.158
0.158
0.176
Italy
0.250
0.271
0.237
Luxembourg
0.218
0.224
0.216
Netherlands
0.217
0.229
0.215
New Zealand
0.189
0.210
0.197
Norway
0.203
0.220
0.206
Poland
0.208
0.208
0.209
Portugal
0.230
0.247
0.224
Slovak Republic
0.160
0.176
0.178
Slovenia
0.208
0.224
0.209
Spain
0.211
0.254
0.211
Sweden
0.284
0.282
0.259
United Kingdom
0.203
0.231
0.206
United States
0.161
0.188
0.178
Mean
0.21146
0.22971
0.21143
Variance
0.00202
0.00186
0.00087
The year 2006 proportion is used as the maximum likelihood estimate for the average
over following seven years for each country. The James-Stein estimator is produced
using the following R code:
mean.2006 <- mean(oecd[,1])
mean.vec.2006 <- oecd[,1]
var.2006 <- mean.2006*(1-mean.2006)/prod(dim(oecd))
oecd.shrink <- 1 - ((nrow(oecd)-3)*var.2006)/
((nrow(oecd)-1)*var(mean.vec.2006))
oecd.js <- rep(NA,length(oecd.mean))
for (i in 1:nrow(oecd))
oecd.js[i] <- mean.2006 +
oecd.shrink*sum(mean.vec.2006[i]-mean.2006)
mean.vec.2007.2013 <- apply(oecd[,2:ncol(oecd)],1,mean)
round(cbind(mean.vec.2006,mean.vec.2007.2013,oecd.js),3)
The Efron and Morris 1975 and 1977 papers do not make clear the best strategy
to get the variance (they added a transformation in their running baseball batting
averages example). Here a binomial-derived variance is scaled by the size of the data.

Bayesian Decision Theory
267
Table 8.2 gives the two sets of country estimates compared to the mean of the next
seven years. There are noticeable diﬀerences since the James-Stein estimates borrow
strength from other cases. Notice also the much lower variance for the James-Stein
estimates than the maximum likelihood estimates. However, observe that the means
are almost identical in these results.
8.5
Empirical Bayes
As noted, the James-Stein estimator is an empirical Bayes estimator since it makes use
of the marginal distribution of the data. In this section we will more fully explore empirical
Bayes methods. Empirical Bayes is an approach that employs the data not only at the
lowest form of the hierarchy but also to estimate these highest-level hyperpriors, which
can be done with or without a speciﬁc distributional form at this level (parametric versus
nonparametric empirical Bayes). This is an idea that actually oﬀends some Bayesians since
the idea of “using the data twice” (hence the name “empirical”) appears to controvert
standard Bayesian philosophy (Lindley famously said that “. . . there is no one less Bayesian
than an empirical Bayesian” [1969, p.421]).
It also bothers some Bayesians because it
implies that “regular” Bayesian inference is not empirical (Gelman et al. 2003, p.121),
which clearly is not true. Furthermore, estimation of parameters at the top of the hierarchy
is done with non-Bayesian techniques (maximum likelihood, method of moments, etc.), and
thus strikes some as both arbitrary and counter to the core tenets of Bayesian philosophy
(Robert 2001). While empirical Bayes is not an emphasis of this text, we will nonetheless
explain the basic principles in this section.
For parametric empirical Bayes start with the standard setup, with a prior that is
directly conditional on some parameter vector, Υ, which leads to:
π(θ|X, Υ) =
p(θ|Υ)L(θ|X)

Θ p(θ|Υ)L(θ|X)dθ ,
(8.45)
and we can label the denominator as q(X|Υ) for convenience.
In regular cases, the Υ
parameter values are set by researchers to reﬂect prior information, and in the case of
conjugacy q(X|Υ) can be easily estimated.
With empirical Bayes we instead estimate
Υ from the data, often simply using the maximum likelihood estimate of Υ from q(X|Υ).
Armed with ˆΥ, we can plug these values into the Bayesian calculation of the posterior above.
The ﬁrst extension, with the odd name Bayes empirical Bayes (Deely and Lindley 1981),
adds a hyperprior distribution, p(Υ, ψ) so that the posterior distribution is produced by
averaging over Υ: π(θ|X, ψ). Therefore there is a clear connection between empirical Bayes
and the hierarchical models of Chapter 12. A good starting point for the parametric version
is Morris (1983b), and important considerations (such as interval estimation) are covered

268
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
in Carlin and Gelfand (1990) as well as Laird and Louis (1987). There are strong links
between empirical Bayes and frequentist objectives (something that further oﬀends some
Bayesians), and these ideas are well-explored in works by Efron and Morris (1971, 1972a,
1972b, 1973, 1976), as well as Deely and Zimmer (1976).
Commonly cited applications
include: Kass and Steﬀey (1989), and Rubin (1980). See also the collection of papers in
Ahmed and Reid (2001).
As an illustration, consider the (balanced one-way) ANOVA setup from Casella (1985)
with p columns of interest having diﬀerent means, ¯Xi but the same known variance, σ2
0. So
by standard assumptions:
¯Xi ∼N(μi, σ2
0)
(8.46)
μi ∼N(m, s2),
i = 1, . . . , p.
(8.47)
The resulting posterior distribution for μi is normal with mean:
ˆμi = σ2
0m + s2 ¯Xi
σ2
0 + s2
,
(8.48)
and variance:
ˆσ2
μ =
σ2
0s2
σ2
0 + s2 ,
(8.49)
which is the ANOVA analog to the posterior mean on page 71. Empirical Bayes proceeds
by identifying the distribution of the data value of interest unconditional on μi,
q( ¯Xi|m, s2) =

μ
f( ¯Xi|μ)p(μ|m, s2)dμ,
(8.50)
which easily leads to the distributional property that ¯Xi ∼N(m, σ2
0 + s2). Now, rather
than assigning parameters values, we take convenient expectations over this marginal dis-
tribution:
E[ ¯Xi] = m,
E

(p −3)σ2
0
( ¯Xi −¯¯X)2

=
σ2
0
σ2
0 + s2 ,
(8.51)
where ¯¯X is the grand mean. These are convenient values in the sense that they can be
plugged directly into (8.48) to produce the empirical Bayes estimate of the posterior mean:
ˆμEB
i
=

(p −3)σ2
0
( ¯Xi −¯¯X)2

¯¯X +

1 −
(p −3)σ2
0
( ¯Xi −¯¯X)2

¯Xi.
(8.52)
The interesting part about this estimator is that it balances information from the column
of interest and all of the data.
Nonparametric empirical Bayes, as introduced by Robbins (1955, 1964, 1983), treats
p(θ|Υ) as only p(θ), which is estimated directly from the data. Important developments
are contained in: van Houwelingen (1977), van Houwelingen and Stijnen (1993), as well as
Maritz and Lwin (1989).
Robbins’ (1955) canonical example is illustrative. Start with a
Poisson-distributed random variable such that P(xi|θi) = θxi
i e−θi
xi!
, xi = 0, 1, . . ., 0 ≤θi < ∞.

Bayesian Decision Theory
269
Under squared error loss, ℓ(θ, T ) = (θ −T )2 (i.e. the cost of mis-estimation is the square of
the distance from the true value), the posterior mean for θi is:
ˆθi =

t
txi+1 exp[−t]
xi!
dp(θ)

t
txi exp[−t]
xi!
dp(θ)
= (xi + 1)un(xi + 1)
un(xi)
,
(8.53)
where un() is Robbins’ notation for the marginal distribution. The task is then reduced
to ﬁnding a nonparametric replacement function for these marginals based on observed
frequencies: un(z) = (number of terms z1, . . . , zn equal to z). Therefore:
ˆθEB
i
= (xi + 1)(# terms = xi + 1)
(# terms = xi)
.
(8.54)
Thus like the parametric-normal case above, the estimate for each θi borrows strength from
the other components j ∈1, . . . , n, i ̸= j.
■Example 8.6:
Empirical Bayes Estimates for Cabinet Duration. Returning to
the cabinet duration data in Exercise 16 on page 65, we will estimate the individual
Poisson intensity parameters λi, i = 1, . . . , 11. The data are cabinet duration (con-
stitutional inter-election period) for eleven Western European countries from 1945 to
1980 for annualized periods given by the number of cabinets (xi) and the average
duration (δi). The model speciﬁcation is given by:
xi|δi, λi ∼P(δiλi)
λi ∼G(α, βrate)
for i = 1, . . . , n, where the δi are weights on the intensity parameters and the gamma
distribution is speciﬁed in the rate version. This produces the posterior distribution:
λi ∼G(α + xi, β + δi),
with posterior expected value:
E[λi|xi, δi, α, β) = α + xi
β + δi
(see Exercise 18). For a multilevel model we would specify distributions for α and β,
and then estimate with the principles in this chapter. Instead we will use standard
maximum likelihood estimation with the data to get “plug-in” values of these unknown
parameters for an empirical Bayes solution. First we need the marginal distribution

270
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
of the data given these parameters:
p(xi|α, β) =
 ∞
0
(δiλi)xi exp[−δiλi]
xi!
×
βα
Γ(α) exp[βλi]λα−1
i
dλi
= βαδxi
i
Γ(α)xi!
 ∞
0
λ(α+xi)−1
i
exp[−(δi + β)λi]dλi
= βαδxi
i
Γ(α)xi!
Γ(α + xi)
(δi + β)α+xi
= Γ(α + xi)
Γ(α)xi! βα(δi + β)−αδxi
i (δi + β)−xi
= Γ(α + xi)
Γ(α)xi!

β
δi + β
α 
δi
δi + β
xi
.
The integral is calculated from the second to the third line by noticing that the
+
+
+
+
+
++++
+
+
10
20
30
40
10
20
30
40
x
λ^
+
+
+
+
+
++++
+
+
FIGURE 8.1: Empirical Bayes Estimates versus Data Values
form within is the kernel of a gamma distribution and simply multiplying this by the
appropriate constants gives the integral value of one, leaving the inverse of these terms.
The ﬁnal expression of p(xi|α, β) would be a negative binomial form for yi = δi/(δi+β)
if α = 1 −xi, but it does not make sense to impose this restriction here. The joint
likelihood function for α and β is simply the product of these terms:
L(α, β|x) =
n
+
i=1
Γ(α + xi)
Γ(α)xi!

β
δi + β
α 
δi
δi + β
xi
,
which we can describe with MCMC tools, but estimation with the optim function in

Bayesian Decision Theory
271
R is just as straightforward. Once this is done we can produce:
ˆλEB
i
= ˆα + xi
ˆβ + δi
,
where ˆα and ˆβ are the MLEs from this process. The following R function performs
these tasks:
EB.Poisson.Log.Like <- function(x.vec, d.vec) {
poisson.ll <- function(params) {
alpha <- exp(params[1]); beta <- exp(params[2])
gamma.term <- sum(lgamma(alpha+x.vec) - lgamma(alpha))
ratio.term1 <- sum(x.vec*log(d.vec/(d.vec+beta)))
ratio.term2 <- sum(alpha*log(beta/(d.vec+beta)))
return(gamma.term + ratio.term1 + ratio.term2)
}
optim.out <- optim(par=c(1,1),fn=poisson.ll,method="BFGS",
control=list(fnscale=-1))
alpha.hat <- exp(optim.out$par[1])
beta.hat <- exp(optim.out$par[2])
return((x.vec+alpha.hat)/(d.vec+beta.hat))
}
We can specify the data and call this function with:
d.vec <- c(0.833,1.070,1.234,1.671,2.065,2.080,2.114,2.168,2.274,
2.629,2.637)
x.vec <- c(38,28,27,20,17,15,15,15,15,14,12)
( EB.out <- EB.Poisson.Log.Like(x.vec, d.vec) )
[1] 40.538450 24.541938 20.926250 12.088241
8.592669
7.640462
[7]
7.526011
7.351118
7.030418
5.775079
5.042394
Figure 8.1 plots the original xi values against the produced ˆλi
EB estimates with an
overlayed loess smoother (f = 0.5). While the relationship is very near linear, the
empirical Bayes estimates are mostly lower than the corresponding xi.
8.6
Exercises
8.1
Savage (1972). For arbitrary outcomes under simple ordering prove:
▷if A .= B then B .= A

272
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
▷A .= A for any A
▷if A ≤B and B .= C then A ≤C
▷if A < B then B ≰A
8.2
Generalize the principles in (8.1) (page 250) to an inﬁnite number of events using
eﬃcient notation.
8.3
The decision rule is a function of X where each draw of X creates an estimate of
θ which is mapped to a speciﬁc action A. Create a graphic that illustrates this
function.
8.4
Equation (8.2) gave a decision rule for a normal (z-statistic) setup (diﬀerence of
means). Restate this for a χ2 test at the same α-level.
8.5
Show that the standard mean square error corresponds to a quadratic loss function.
Illustrate your argument with a data vector distributed N(μ, σ2
n).
8.6
Prove that the Bayes rule estimator using absolute error loss (L(A, d(X)) = |θ−θA|)
is the posterior median.
8.7
Show that μA = ¯y is the estimator of μ that minimizes expected loss in (8.32).
8.8
Derive the mean and variance of p(d|n) in (8.16).
8.9
Consider the following table of US and Soviet nuclear tests:
Year
U.S. Tests
Soviet Tests
Year
U.S. Tests
Soviet Tests
1960
0
0
1976
21
21
1961
10
1
1977
20
24
1962
98
1
1978
21
31
1963
47
0
1979
16
31
1964
47
9
1980
17
24
1965
39
14
1981
17
21
1966
48
18
1982
19
19
1967
42
17
1983
19
25
1968
56
17
1984
20
27
1969
46
19
1985
18
10
1970
39
16
1986
15
0
1971
24
23
1987
15
23
1972
27
24
1988
15
16
1973
24
17
1989
12
7
1974
23
21
1990
9
1
1975
22
19
1991
8
0
(also provided in the R package BaM). Returning to (8.19), construct a prior for
Soviet tests from 1960 to 1969 (p(n)), and specify a reasonable loss function based
on U.S. tests from 1970-1990 that allows the U.S. to test at the observed rates

Bayesian Decision Theory
273
assuming that the two powers want to test at the same rate. Use these to construct
the expected loss with respect to the prior distribution. Would it be in the U.S.
interest to renegotiate at some intermediate time between 1970 and 1990?
8.10
Deﬁne X1, . . . , Xn to be a set of iid dichotomous random variables with p = p(Xi =
1) and 1 −p = p(Xi = 0). Show that the data mean are an admissible estimator of
unknown p using squared error loss. Find another loss function that also has this
property.
8.11
If  t(xi) is a suﬃcient statistic for θ (Appendix A), then provide the Bayes (op-
timal) action that is a function of  t(xi).
8.12
For ﬁnite parameter spaces all admissible estimators are Bayesian estimators (Fer-
guson 1967) . Consider estimating ζ on ℜfrom a sample distributed N(ζ, 1) using
¯X.
While this is admissible for ζ, show that it cannot be a Bayes action with
respect to any prior distribution since it is unbiased but has non-zero variance for
ﬁnite samples.
8.13
Given an iid sample, X1, . . . , Xn from a normal distribution N(μ, σ2), where σ2 is
known, show that the maximum likelihood estimate for μ the frequentist risk can
be expressed as:
L(A, d(X), k) = |θ −θA|k = Γ(
k + 1
2

σkn−k/2(2k/π)
1
2 .
8.14
Prove that all admissible rules must be Bayes rules and give an example of a Bayes
rule that is inadmissible.
8.15
For the normal model described on page 78, show that for squared error loss the
maximum likelihood point estimate ˆμ = ¯x has the risk function RF (μ, d(X)) = n.
8.16
Using the election survey data in Example 7.3.3 (page 220), calculate the maxi-
mum likelihood estimate and the James-Stein estimate for each value in IND (self-
indicated indiﬀerence to the election).
Compare the variances and contrast the
estimated vectors graphically.
8.17
Show that the prior distribution (8.33) leads to the posterior distribution (8.34).
8.18
Derive the posterior distribution and expected value of λi from Example 8.5.
8.19
(Formerly Exercise 10.10) Plug (8.51) into (8.48) to obtain (8.52), the empirical
Bayes estimate of the posterior mean. Using the same logic and (8.49), produce
the empirical Bayes estimate of the posterior variance, assuming that it is now not
known.

274
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
8.20
Given the model:
xj ∼BN(nj, pj),
j = 1, . . . , J
p ∼BE(α, β)
with α > 0 and β > 0 both unknown, show that the marginal distribution of the
full set of X is:
p(X|α, β) =
J
+
j=1
nk
xk
Γ(α + β)Γ(α + xj)Γ(nj −xj + β)
Γ(α)Γ(β)Γ(α + β + nj)
.
If ˆα and ˆβ are the MLEs of these parameters, show that the empirical Bayes esti-
mator of pj is given by:
E[pj|xj, ˆα, ˆβ] =
ˆα + xj
ˆα + ˆβ + nj
.

Chapter 9
Monte Carlo and Related Iterative
Methods
9.1
Background
This is the ﬁrst chapter speciﬁcally about simulation techniques, even though some prin-
ciples have already been discussed. Simulation work in applied statistics replaces analytical
work on behalf of the researcher with repetitious, low-level eﬀort by the computer. The key
advantage is that when a model speciﬁcation leads to a posterior form that is diﬃcult or
impossible to manipulate analytically, then it is often possible to create a set of simulated
values that share the same distributional properties. So we describe the posterior by using
empirical summaries of these simulated values rather than perform some uncomfortable
integration or other operation. This is actually an old idea (see the really interesting 1951
chronicle: “Report on a Monte Carlo Calculation Performed with the Eniac,” by Mayer),
but one that is particularly easy to implement now that computers are ubiquitous and fast
(and getting more so every day). Excellent references in addition to those speciﬁcally cited
in this chapter include: Fang, Hickernell, and Niederreiter (2002), Fishman (2003), Lange
(2000), Rubinstein (1981), and Sobol (1994).
To begin with, suppose we were interested in the expected value of some quantity ex-
pressed as an integral:
E[θ] =
 b
a
θf(θ)dθ,
(9.1)
where f() is a suitably complex form and θ is of suﬃciently high (k) dimension such that
the integral is prohibitively challenging from a straight analytical standpoint. Of course,
as we have seen already, we are often interested in other posterior quantities like intervals,
and this chapter will address Monte Carlo simulation of the quantities of primary Bayesian
interest.
If we are lucky enough to have an easy method for producing random vectors θ1, θ2, . . . , θn
with θi = [θi1, θi2, . . . , θik], which are known to be from the correct distribution on [a:b],
then it is possible to use these empirical draws to summarize the unknown integral quantity
by counting those that fall in this range. The idea is quite powerful: if we can generate
samples from the desired sampling distribution, then we can summarize the theoretical
275

276
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
distribution by using these simulated values rather than being required to use diﬃcult
analytical calculations.
Monte Carlo methods are digital computation methods, originally to randomly generate
a large number of numerical values to perform diﬃcult physical calculations of interest.
Naturally, the deﬁnition of “large” in this context has changed considerably over time. While
the varied collection of Monte Carlo techniques thus far accumulated by researchers are very
important in Bayesian analysis, none of the tools provided herein are restricted to Bayesian
use. In fact, most of these were developed for other purposes and eventually co-opted by
Bayesians. Interestingly, the history of Monte Carlo simulation is intimately tied with the
development of thermonuclear weapons, notably in the papers of Metropolis, and Ulam
(1949), Kahn (1949), von Neumann (1951), Metropolis et al. (1953), and Pasta and Ulam
(1953). The original idea came to Ulam when he was playing solitaire convalescing from
an illness, and was formally developed by Ulam and von Neumann. Since the method was
created in a classiﬁed environment investigating neutron diﬀusion in ﬁssionable material, it
was given the codename Monte Carlo by Metropolis who said that the iterative process of
probability sampling reminded him of casino gambling. The internal report by Richtmyer
(the Theoretical Division Leader at Los Alamos at the time), Ulam, and von Neumann was
not declassiﬁed until 1959, partly explaining the slow pace of dissemination into general
scientiﬁc use.
Common statistical computing tasks to worry about in traditional settings are optimiza-
tion techniques like numerical diﬀerentiation of likelihood functions and solution ﬁnding in
systems of constrained linear equations (Gill, Murray, and Wright 1981). The production
of ﬁxed-point estimates are produced by algorithmic mode-ﬁnding, where the major numer-
ical diﬃculties are ﬁnding the dominating mode and the curvature around that mode. In
contrast, the primary numerical challenge in Bayesian analysis is the summary of posterior
distributions through integration. This diﬀerence is both philosophical (ﬁxed population pa-
rameters versus population parameters with distributions) and practical (summary through
point estimates and standard errors versus summary through descriptions of distributions).
Since the most frequent goal in numerical Bayesian analysis is to estimate an integral
quantity of some sort based on limited information using random elements, we will concen-
trate in this chapter on methods for calculating integrals numerically. First we will cover
basic integral quantities that are relatively well deﬁned but require simulations to describe,
then we will discuss more elaborate stochastic simulation techniques in which the simulation
process contains serial correlation.
One ﬁnal word of caution is necessary here. Performing statistical analysis (description,
estimation, prediction) with Monte Carlo tools is still statistical analysis. Thus giving a
Monte Carlo point estimate without a corresponding standard error is incomplete and mis-
leading. In general, Monte Carlo techniques computationally produce samples and typically
an appropriate mean, ˆθ, is calculated from these n samples, θ1, . . . , θn. So it is trivial to
calculate the Monte Carlo standard error:
/
sθ = 1
n
(θi −ˆθ)2, yet this is often neglected.

Monte Carlo and Related Iterative Methods
277
Fortunately the size of this error is “controllable” since the size of n is determined by the
patience of the researcher (and computers continue to increasingly cater to the impatient).
9.2
Basic Monte Carlo Integration
Suppose that we had a (normalized) probability function, g(θ), that was diﬃcult to
express or manipulate but for which we could easily generate samples on an arbitrary
support of interest: [a:b]. A common quantity of interest is:
I[a, b] =
 b
a
g(θ)h(θ)dθ,
(9.2)
that is, the expected value of some function, h(θ), of θ distributed g(θ). If h(θ) = θ, then
I[a, b] simply calculates the mean of θ over [a:b]. A substitute for analytically calculating
(9.2) is to randomly generate n values of θ from g(θ) and calculate:
ˆI[a, b] = 1
n
n

i=1
h(θi).
(9.3)
The idea is to replace analytical integration with summation from a large number of sim-
ulated values, rejecting values outside the range of interest, [a : b].
The beauty of this
approach is that by the strong law of large numbers, ˆI[a, b] converges with probability one
to the desired value, I[a, b].
A second positive feature is that although ˆI[a, b] now has
“simulation error,” this error is easily measured by the empirical variance of the simulation
estimate:
Var(ˆI[a, b]) =
1
n(n −1)
n

i=1
(h(θi) −ˆI[a, b])2.
(9.4)
Note that the researcher fully controls the simulation size, n, and therefore the simulation
accuracy of the estimate. Furthermore, the central limit theorem applies here as long as
Var(I[a, b]) is ﬁnite, so, for instance, 95% credible intervals can be directly calculated by:
[95%lower, 95%upper]
=
'
ˆI[a, b] −1.96
/
Var(ˆI[a, b]), ˆI[a, b] + 1.96
/
Var(ˆI[a, b])
(
,
(9.5)
or by reporting the 0.025 and 0.975 quantiles of the set of θi. Obviously other credible
intervals of interest can be similarly calculated.
Monte Carlo integration can be placed in more explicitly Bayesian context for our pur-
poses by replacing g(θ) with a posterior statement π(θ|x) and noting that h() is typically an
estimate of some function of the unknown parameter: h(θ). So I[a, b] is really the (posterior)
expectation of h(θ|x):
E[h(θ|x)] =

π(θ|x)h(θ)dθ ≈1
n
n

i=1
h(θi).
(9.6)

278
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
Often h(θ) = θ and this is then just the ﬁrst moment of the posterior distribution.
To show how easy this process can be with modern computing consider two small prob-
lems. First, suppose we replace the following integration operation with its simulated value
(φ indicates the standard normal PDF):
I[−2, 1] =
 1
−2
θ2φ(θ)dθ ≈ˆI[−2, 1] = 1
n
n

i=1
θ2
i
(9.7)
by using the R code which ﬁrst samples 100, 000 random standard normal values and then
applies the criteria of interest.
norm.sample <- rnorm(100000)
mean(norm.sample[norm.sample>-2 & norm.sample <1]^2)
This produces 0.5747261 (try it), saving us some analytical agony since the normal PDF
is typically integrated by using polar coordinates. Now consider a much harder analytical
problem:
I[e, π] =
 π
e
arctan(θ
1
3 )C(θ|μ = 3, σ = 2)dθ
(9.8)
where C() denotes the Cauchy PDF (C(θ|μ, σ) =
1
πσ
1
1+( θ−μ
σ )
2 , −∞< θ, μ < ∞, 0 < σ, B).
Obviously this would not be a lot of fun to work through with pencil and paper. However
the Monte Carlo solution in R is trivial:
c.sample <- rcauchy(100000,3,2)
mean(atan(c.sample[c.sample > exp(1) & c.sample < pi]^(1/3)))
which produces 1.058232.
■Example 9.1:
Monte Carlo Integration with the Pareto Distribution,
European Migration Data.
The Pareto distribution (B) is often used to model
phenomena that are bounded by zero and have long positive tails. Examples include
city and country populations, income, and wealth. Suppose we have data at hand
that are assumed to be distributed PA(x|α, β), with α and β unknown. Arnold and
Press (1983, 1989) develop a two-stage prior for the Pareto where there is an assumed
dependence between the two parameters:
α ∼G(C, D) = Γ(C)DCαC−1e−Dα
(9.9)
β|α ∼PA(αA, B) = αAB(Bβ)−(αA+1).
(9.10)
This idea, ﬁrst suggested by Lwin (1972) and also addressed by Nigm and Handy
(1987), is that independence of the two parameters is somewhat unrealistic in practice
and it is better to speciﬁcally model the conditionality. We saw a normal model version
of this type of prior structure in Chapter 3. The likelihood function given the data is:
L(α, β|x) = αnβ−nαexp

−(α + 1)
n

i=1
log(xi)

,

Monte Carlo and Related Iterative Methods
279
which leads to the following form of the joint posterior:
π(α, β|x) ∝L(α, β|x)p(β|α)p(α)
∝αn+C(Bβ)−(nα+Aα+1)
× exp

−Dα −
* n

i=1
log(xi) + Alog[B] −(n + A)log[min(B, x)]
,
α

,
provided that βmin(B, x) > 0, and α > 0. The conjugacy of this setup gives the
marginal posteriors:
α|x ∼G
*
n + C, D +
n

i=1
log(xi) + Alog[B] −(n + A)log[min(B, x)]
,
β|α, x ∼PA(nα + Aα, min(B, x)).
Peach (1997) looks at postwar migration into western Europe due to: the eﬀects of a
retreat from colonization, worker mobility from poorer areas of the world, and inﬂux
due to political ﬂight. There were substantial diﬀerences in assimilation across western
European countries due to local politics and culture as well as the ethnicity of the
immigrants. Table 9.1 gives the 1990-1993 total ethnic minority population in western
European countries, including cross-migration (excluding the “other” category).
We ﬁrst set the prior parameters according to the vector: [A, B, C, D] = [20, 100, 600, 3]
and then evaluate the posterior according to the set forms above. See Arnold and
Press (1989, p.1083) for speciﬁc guidance on empirically setting these parameters to
meet moment expectations. The marginal posterior mean and variance for α are easy
enough to calculate analytically since the form is a speciﬁed gamma distribution not
conditioned on any unknown quantity.
Employing this method with 1,000 simulations (in actual practice this number should
probably be an order of magnitude or more higher) produces a posterior mean from the
α simulations of 4.754, as opposed to the theoretical value of 4.751 (not that accuracy
is implied to the level of this distinction since there exists measurement error in the
data as well). The presence of simulation error explains this modest diﬀerence: the
standard error of the Monte Carlo statistic is 0.003.
It is often easier to use Monte Carlo integration to calculate tail probabilities and
thresholds than it is to derive them analytically.
It is also more ﬂexible in that
calculating diﬀerent values is a trivial counting procedure using empirical draws rather
than a new integral calculation. All we would have to do is to generate a set of random
gamma variates and summarize numerically using simple Monte Carlo integration.
Speciﬁcally, we generate N draws from the distribution of interest, sort them, and
pick out the values of probabilistic interest.
If we generate N = 1, 000, then, for
example, the 0.025 tails on either end begin approximately at the sorted 25th and
975th empirical values. Table 9.2 gives a set of quantiles for α from the Pareto model
using exactly this procedure with the N = 1, 000 simulated values:

280
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
TABLE 9.1:
1990-1993 W.Europe Ethnic/Minority
Populations
Country of Origin
Estimated Total(K)
Percent of Total
Turkey
2483.9
17.79
Italy
1518.9
10.88
Former Yugoslavia
1241.1
8.89
Morocco
1178.1
8.44
Caribbean
1000.0
7.16
Portugal
925.7
6.63
India
876.6
6.28
Algeria
698.8
5.00
Ireland
641.0
4.59
Spain
572.5
4.10
Pakistan
492.8
3.53
Greece
402.9
2.89
France
331.8
2.38
Poland
328.7
2.35
Tunisia
292.2
2.09
Germany
280.7
2.01
Iran
183.8
1.32
Bangladesh
183.3
1.31
Croatia
153.1
1.10
Bosnia-Herzegovina
139.1
1.00
Afghanistan
30.0
0.21
Indonesia
9.5
0.07
9.3
Rejection Sampling
Suppose now that we could not readily produce random values from the posterior dis-
tribution of interest. This would temporarily preclude us from performing Monte Carlo
integration as described above because we could not obtain the simulated sample required
to calculate empirical summaries. We are not necessarily completely thwarted because if
there exists an expressed form of the posterior so that for candidate values, θ, the density,
f(θ), is calculable, then we can often employ rejection sampling to produce an empirical
sample.
Rejection sampling, or the rejection method, is a means of obtaining an integral quan-
tity using the generation of candidate random variables (the sample here) and accepting
those that are determined to belong to the distribution of interest. This idea, which is

Monte Carlo and Related Iterative Methods
281
only slightly more involved than Monte Carlo integration, was brieﬂy introduced in Exer-
cise 3.11. By comparing the quantity generated to the quantity accepted, we get a ratio
value corresponding to the area we would like to measure. This principle, also called the
acceptance-rejection method, is very general and applies to any situation where uniformly
random variates can be generated over an enclosing region.
TABLE 9.2:
Monte Carlo Quantiles for Pareto
Parameter α
Minimum
0.01
0.25
0.50
0.75
0.99
Maximum
4.132
4.633
4.664
4.750
4.759
4.872
5.336
We could assume that α = 2 (see Johnson, Kotz, and Balakrishnan [1997, p.575]) for a
discussion of the economic history of this parameter), and get a posterior for β, but rather
than “take the easy way out” we will integrate out α and simulate the subsequent marginal
posterior values for β, thus incorporating uncertainty from α rather than ignoring it. This
calculation starts with:
π(β|x) =

α
π(β|α, x)π(α|x)dα
∝

α
αn+C(Bβ)−(nα+Aα+1)exp [−α(D + E)] dα,
(9.11)
where for simpliﬁcation we introduce the shorthand for constants: E = n
i=1 log(xi) +
Alog[B] −(n + A)log[min(B, x)]. Unfortunately there appears to be no convenient analyt-
ical marginalization here because our usual trick of segmenting oﬀthe kernel of a PDF is
not possible. While it is possible to solve this integral with brute force mathematics, the
investment of time will be nowhere near worthwhile. Instead we will employ a Monte Carlo
marginalization based on importance sampling, which is provided in Section 9.6.
The terminology is slightly confusing in that rejection sampling is a technique that can
measure diﬃcult integral quantities as well as generate random variates from candidate
distributions from which analytical sampling might otherwise be diﬃcult (Kronmal and
Peterson 1981).
The original idea behind rejection sampling comes from mid-twentieth
century pioneers, von Neumann (1951) and Metropolis and Ulam (1949), who developed
the idea of replacing repeated computational work for analytical calculations.
9.3.1
Continuous Form with Bounded Support
Imagine that we would like to estimate some integral quantity based on a continuous
random variable where we know that the support of this variable is bounded. This means
only that we know the limits for simulation purposes. This does not have to be a complete

282
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
probability density function, it may be a non-normalized form, a “slice” of the density, or
some other composite form that does not necessarily integrate to one.
For the most basic case, posit a function f(θ) so that we can evaluate candidate values of
θ through the function but we cannot easily integrate the function. If f(θ) can be evaluated
for any value over the support of θ: Sθ = [A, B], then it is usually a simple process to
obtain the function’s maximum value (if it exists and is unique) over this support: ˜θ. There
are two general methods for obtaining this maximum: analytical root ﬁnding and standard
numerical techniques. Root ﬁnding is the basic calculus technique whereby we take the
derivative of f(θ) with respect to θ, set it equal to zero and solve for θ. This produces the
θ value where the derivative is zero (˜θ), i.e., where the tangent line is ﬂat at the mode,
see Appendix A for details.
Of course there is much more eﬀort involved for multiple
modes, minimum points, or saddle points. Tests for these conditions are very nearly trivial
and involve checking the second derivative at the candidate point (the second derivative is
negative at a true maximum point indicating a sloping downward in both directions away
from the modal point). Numerical techniques are eﬀective and vary in complexity from
simple linear candidate testing across the range of possible θ values (“gridding”) to the
more sophisticated numerical root-ﬁnding algorithms discussed in this chapter.
If we have a known bounded support and maximum value for θ, then we can deﬁne a
rectangle in 2-space that is guaranteed to bound all possible values for the pair: (θ, f(θ)).
This is useful because it is trivial to generate a two-dimensional random variable in this
rectangle by independently sampling uniformly over [A, B] and [0, ˜θ] and then pairing these
points. Therefore once we have “boxed in” the area of interest, we can randomly generate
points over the box and count the number of values that fall under the curve.
The value of the integral, the area under the curve, is just the ratio of points under the
curve to the total number of points scaled by the size of the box:
number of points under curve
total number of points
× size of box −→
n→∞
 B
A
f(θ)dθ.
(9.12)
The fact that this is a converging quantity, rather than an analytically deterministic value,
should not alarm us since the degree of accuracy is entirely controlled by the researcher
through the number of points generated. We can therefore be as accurate as we want just
by increasing the number of simulated values to test. Discrete problems turn out to be
much more straightforward as it is usually a matter of counting bin heights and taking a
weighted sum.
Figure 9.1 demonstrates how rejection sampling works. In this case 100 points are sam-
pled uniformly from the two-dimensional rectangle deﬁned over: [(A, B), (0, max(f(θ)))] =
[(0, 10), (0, 0.4)], and observed values that are contained in the region of interest (some PDF
deﬁned on the interval (A, B)). In total 26 values fell into the area we wish to integrate,
so by (9.12) we obtain the size of the interval from: (26/100)(10 × 0.4) = 1.04. This makes
sense since this example was constructed by using a transformed gamma PDF (lopping oﬀ
a very small tail area) and the actual value is therefore very close to one. We could get
much closer to one if we simulated more values than only 100.

Monte Carlo and Related Iterative Methods
283
Support
Density
maxf(θ)
0
A
B
FIGURE 9.1: Rejection Sampling for Bounded Forms
This expression of rejection sampling can be particularly useful in determining the nor-
malizing factor for non-normalized posterior distributions, provided that each dimension has
bounded support that can be “boxed.” Obvious examples include the beta distribution and
normal forms with reasonably small tails. In the cases where this is not possible, another
approach is required. Also, it is not essential that the bounding region be determined by
a rectangle, and other shapes are regularly used such as the triangle, trapezoid, or various
densities (see Morgan [1984, Section 5.4]).
■Example 9.2:
Calculating the Kullback-Leibler Distance. In Section 7.6 we in-
troduced the Kullback-Leibler distance as a means of calculating the distance between
two distributions. It was pointed out that the measure is diﬃcult to analytically calcu-
late for a number of distributional forms. This is true for the beta distribution, and we
will use rejection sampling to produce the Kullback-Leibler distance for the two beta
posteriors from the beta-binomial model of cultural consensus given in Example 2.3.4.
The two posteriors given were f(p) = BE(32, 9), and g(p) = BE(18, 8). These were
plotted in Figure 2.5, and it did not appear that there was a dramatic diﬀerence
despite the diﬀerent approach to the priors. Using the form for the Kullback-Leibler

284
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
distance given in (7.49) with the two beta posteriors, we obtain:
I(f, g) =
 1
0
log
	
EpA−C(1 −p)B−D
 Γ(A + B)
Γ(A)Γ(B)pA−1(1 −p)B−1dp
= log(E) + (A −C) Γ(A + B)
Γ(A)Γ(B)
 1
0
log(p)pA−1(1 −p)B−1dp
+ (B −D) Γ(A + B)
Γ(A)Γ(B)
 1
0
log(1 −p)pA−1(1 −p)B−1dp,
(9.13)
where A = 32, B = 9, C = 18, D = 8, and the collected-constant term here is
E = (Γ(A + B)Γ(C)Γ(D))/(Γ(C + D)Γ(A)Γ(B)) = 284.4174. The two integrals are
quite annoying to calculate analytically, but contain simple functions and are deﬁned
over the bounded support [0 : 1], and are therefore easily calculated with rejection
sampling. The process proceeds simply as: deﬁning the box bound by the support
for p and the interval given by zero and the minimum of the function in the integral,
generating 1,000 points uniformly over this box, comparing these points on the y-axis
with the function values, and multiplying the size of the box by the number of points
that fall beneath the function values. These steps are performed for the two integrals
so that (9.13) gives 0.4816 (see Exercise 9.9).
9.3.2
Continuous Form with Unbounded Support
Suppose we wish to calculate the integral of some target function f(θ) in which the
analytical solution is diﬃcult or impossible, and the form of f(θ) has unbounded tails. The
trick is to specify a “majorizing function,” g(θ), which for every value of θ in the support of
f(θ) has the property that g(θ) ≥f(θ). This is done with the idea that we can pick some
g(θ) function, such as a convenient PDF, from which it is easy to sample and therefore
produce candidate values for acceptance or rejection. Thus the uniform candidate values
over a two-dimensional rectangle, a nontheoretical convenience, are replaced with some
distributional form with desired coverage probabilities.
If the target distribution has unbounded tails, then obviously the majorizing function
must also have this property (Geweke 1989, p.1319), and simply picking a PDF for g(θ)
which has heavier tails than f(θ) will not work since it will then have other regions where
it is not uniformly greater than the target distribution by the fact that it must integrate to
one. The solution is to use a multiplication factor so that:
f(θ) ≤kg(θ),
∀θ, k > 1.
(9.14)
Thus we sample θi from g(θ) and then make an accept/reject decision based on f(θi).
Mechanically this means that for each θi point, we randomly draw a uniform(0,1) variate
and accept θi if this uniform is less than f(θi)/kg(θi). This process is illustrated in Figure 9.2
where the accepted points are distinguished from the rejected points (although the ratio
no longer has the intuitive interpretation of that given by Figure 9.1 since the bounding

Monte Carlo and Related Iterative Methods
285
f(θ)
kg(θ)
Support
Density
FIGURE 9.2: Rejection Sampling for Unbounded Forms
structure is no longer rectangular). The graphic underneath the distribution is called (rather
cutely) a “rug,” and indicates the marginal distribution of θ generated by g(θ). Note that
if the majorizing distribution is very dissimilar from the target distribution, then the high-
density area of the rug would be away from the mode of the target distribution, and the
sampling procedure would be less eﬃcient (more values rejected).
The key is to specify the majorizing function to have the same asymptotic properties as
the target distribution in addition to its enveloping requirement. In the synthetic example
in Figure 9.2, both distributions have a right-hand side asymptote. The second important
characteristic is that the majorizing function resembles the target function as much as pos-
sible while still enveloping it for every value of θ in the sample space. This amounts to
a performance issue rather than a mathematical requirement since an enveloping distribu-
tion that diﬀers greatly from the target distribution will tend to generate many rejected
candidate values and therefore be considerably less computationally eﬃcient.
■Example 9.3:
Enveloping with an Exponential Density.
Suppose that we have
obtained a non-normalized posterior distribution resulting from the proportionality
of Bayesian inference, of the form:
f(θ) = exp[−θ2/2],
θ ≥0.
(9.15)
This is the kernel of a folded normal PDF, which is a non-normalized reﬂection of the
standard normal deﬁned only for positive values, leaving out constant terms. In order
to normalize this posterior, we require the integral quantity in the denominator of the
right-hand side of Bayes’ Law (as in (2.7)): I(θ) =
 ∞
0
exp[−θ2/2]dθ, which is a time-
consuming integration process (although we could obtain it via normal properties).

286
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
0.0
0.5
1.0
1.5
2.0
2.5
3.0
(1.0,0.6065307)
FIGURE 9.3: Rejection Sampling, Optimal Coverage
Instead of calculating an analytical solution to this integration problem, we decide to
estimate the integral using the rejection method by specifying an exponential envelop-
ing distribution: g(θ) = k exp[−θ]. The exponential form is particularly useful here
because it has the asymptotic property in the positive limit and because the constant
term allows us to ensure coverage. The question remains, however, what value of k
is best to ensure coverage but maximize algorithmic eﬃciency by minimizing rejected
values.1
Panel 1 of Figure 9.3 shows prospective exponential functions along with the folded
normal kernel (in bold). The exponential functions in the ﬁgure displayed are for
k = 1.0, 1.5, 2.0, 2.5, 3.0 as indicated by the θ = 0 point (i.e., when k exp[0] = k).
The ﬁrst two (k = 1.0, 1.5) are unacceptable since they do not provide the required
coverage. The others are acceptable but not optimal.
To ﬁnd an optimal coverage exponential function, we would like one that has a single
intersecting point with the folded normal (at its inﬂection point). This process turns
out to be very straightforward, where we merely equate the two functions and solve
1It may seem like an inordinate amount of trouble to worry about the calculations required to reduce
computing time since the computing time diﬀerence is not substantial in such a contrived example. Where
it makes a signiﬁcant diﬀerence is in cases we will deal with later, where such routines are called many
times by higher-level Monte Carlo simulation routines and graphing functions. In these cases even small
improvements in algorithmic eﬃciency at the lowest level of function calls can have an enormous impact.

Monte Carlo and Related Iterative Methods
287
for the k that gives the roots to the resulting quadratic equation:
g(θ) = k exp[−θ] = exp[−θ2/2] ≡f(θ)
k = exp[θ −θ2/2]
0 = θ2 −2θ + 2 log k.
Equate the two possible quadratic solutions, one of which must be 1:
1 = 2 log k
∴k = exp(1/2).
Using this value, k = 1.649, in g(θ) = k exp[−θ], we get the enveloping function
displayed in Panel 2 of Figure 9.3, where the single point of intersection occurs at
[1, 0.607].
Once we have the desired enveloping function, the next task is to sample candidate
values from this function and accept or reject them based on their position relative
to the folded normal. The procedure is very simple:
1. Draw n EX (1) random variables: θ1, θ2, . . . , θn.
2. For each θi, calculate the corresponding value of the majorizing function: g(θi) =
(1.649) exp[−θi].
3. Draw a random uniform value for each θi over interval from 0 to kg(θi): uθi.
4. Accept this draw as being from f(θ) if uθi is less than f(θi).
5. The number of accepted values relative to the total number of draws is propor-
tional to the ratio of the area under the target function to the area under the
majorizing function. So:
I(θ) =
 ∞
0
f(θ)dθ = k × number of accepted points
total number of draws
.
For only n = 100 draws, we get the surprisingly accurate result of 1.25308, which
is very close to the true value of 1.25314. This process is illustrated in Figure 9.4,
which shows how determining the single point of intersection (more than one point of
intersection would imply a violation of the covering principle) substantially increases
the eﬃciency of the simulation process: there are relatively few rejected points.
This section provides both the theory and the details of rejection sampling. This discus-
sion is important in the overall context of where we are heading in that it is the ﬁrst example
where analytical work is replaced by computational eﬀort. Virtually every technique dis-
cussed in subsequent chapters builds on this idea. For a further discussion and comparison
of various numerical estimation techniques in Bayesian estimation, see Monahan and Genz
(1996), and Stern (1997) for modern reviews.

288
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
Support
Density
FIGURE 9.4: Folded Normal Rejection Sampling Example
9.4
Classical Numerical Integration
This section brieﬂy reviews the traditional “quadrature” methods for numerically solving
integral quantities.
Long before widespread interest in Bayesian methods produced the
need for a computational method of producing posterior quantities, applied mathematicians
developed a wide range of tools for integrating functions that are diﬃcult or impossible to
solve analytically. It is well beyond the scope of this book to describe all of these in detail,
and interested readers are directed to the excellent references of Davis and Rabinowitz
(1984), Flournoy and Tsutakawa (1991), Haber (1970), Krommer and Ueberhuber (1998),
and Stroud (1971), as well as the detailed practical guides on implementation of Espelid
and Genz (1992), Golub and Van Loan (1996), and Press et al. (1986).
Diﬃcult integrals tend to be high-dimensional where the analytic production of an as-
sociated antiderivative is unavailable, but for expositional simplicity, we will consider the

Monte Carlo and Related Iterative Methods
289
general deﬁnite integral form of:
I(g) =
 b
a
g(θ)dθ =
n

i=1
ωig(θi).
(9.16)
Here the term ωi is an integration weight and θi is an integration node in [a : b]. The general
idea is to replace the integration process with a summation so that the weights provide the
appropriate density contribution at the chosen node. Accuracy is controlled by the limit of
the index i in the summation.
9.4.1
Newton-Cotes
Newton-Cotes methods involve ﬁtting polynomial forms over equally spaced regions of
the integral. The idea is to ﬁt the shape of the integral piece-wise where the accuracy of the
estimate is increased with progressively ﬁner granularization of the support of the integral
and increasing degree of the polynomial. Obviously there is a cost to increasing both of these
criteria and the quality of the resulting approximation is partially a function of the skill of
the researcher. While all of these tools share desirable asymptotic properties, results can
diﬀer substantially in ﬁnite simulations. There are three basic variants of Newton-Cotes:
Riemann integrals, the trapezoid rule, and Simpson’s rule.
9.4.1.1
Riemann Integrals
This method, sometimes called the rectangle rule, is the simplest, least accurate method
for numerical integration, and is derived directly from basic calculus theory. The idea is
to deﬁne n disjoint intervals of length h = (b −a)/n so that: θ0 = a, θn = b, and for
i = 2, . . . , n −1, θi = a + ih, to produce a histogram-like approximation. In the notation
of (9.16), ωi = h and g(θi) = g(a + ih). One of the reasons that this rule is so basic is that
the weighting scheme is identical and equal to the spacing. The only wrinkle here is that
one must select whether to employ a “left” or “right” Riemann integration:
 b
a
g(θ)dθ =
n

i=1
ωig(θi) =
⎧
⎨
⎩
h n−1
i=0 g(a + ih),
left Riemann integral
h n
i=1 g(a + ih),
right Riemann integral.
(9.17)
Despite the obvious roughness of approximating a smooth curve with a series of rectangular
bins, Riemann integrals can be extremely useful as a crude starting point since they are
easily implemented. Furthermore, by calculating both left and right Riemann integrals, one
can obtain an interval quantity that is guaranteed to bound the true value for monotonically
increasing or decreasing functions.
9.4.1.2
Trapezoid Rule
The ﬁrst and most obvious improvement to the rectangular approach of Riemann in-
tegrals is to substitute a trapezoid that uses the same binning strategy but connects the
starting point of each bin to its ending point, thus creating a trapezoid. So unlike Riemann

290
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
integrals, both points are on the curve and the top of the trapezoid gives a linear approxi-
mation to the curve over the bin width. The process takes a sum of trapezoids of the form:
(h/2)(g(θi) + g(θi−1)), and simpliﬁes through collection of terms to:
 b
a
g(θ)dθ =
n

i=1
ωig(θi) = h
n−1

i=1
g(θi) + h
2 (g(a) + g(b)),
(9.18)
so it remains that ωi = h, but we have an improved method for producing g(θi). Obviously
the closer the curve is to linear within the bins, the better the trapezoid idea works. It is
also clear that as the bin size gets progressively smaller, the approximation improves, albeit
with a computational cost.
9.4.1.3
Simpson’s Rule
The trapezoid rule does not work particularly well when the curve is very nonlinear
within the bins. This occurs when the function has many local maxima and minima and
when these points are sharply deﬁned.
An improved procedure, called Simpson’s rule,
replaces the linear approximation within each bin with a quadratic interpolating polynomial
across the same space.
Suppose we wanted to ﬁt a quadratic for the integral using only three points: a′, b′, and
the midpoint c′ = (a′ + b′)/2 over some bin within the desired range of integration: [a:b].
A quadratic form that is guaranteed to intersect with the integral at these three points is
given by:
 b′
a′ g(θ)dθ =
 b′
a′
' (θ −c′)(θ −b′)
(a′ −c′)(a′ −b′)g(a′)+
(θ −a′)(θ −b′)
(c′ −a′)(c′ −b′)g(c′) + (θ −a′)(θ −c′)
(b′ −a′)(b′ −c′)g(b′)
(
dθ.
(9.19)
It is really easy to see that the quadratic form in (9.19) intersects with the integral at
θ = a′, θ = b′, and θ = c′ since in each case the other two terms will cancel out and the
preceding fraction on the remaining simpliﬁes to one (try it!). The value of this expression
is that because c is chosen as the midpoint, (9.19) reduces to:
 b′
a′ g(θ)dθ = b′ −a′
6
[g(a′) + 4g(c′) + g(b′)] .
(9.20)
If g(θ) is a quadratic form between a′ and b′, then Simpson’s rule will ﬁt exactly.
In
general though, it doesn’t ﬁt exactly but is much more accurate than the trapezoid rule
and equally simple to code. Once again accuracy is a function of the tolerance for increased
computational cost as the number of bins increases.
■Example 9.4:
Estimating Models with Mixture Priors. Sometimes it is diﬃcult
to accurately describe prior information with a single PDF or PMF. In these cases
it is sometimes possible to apply a mixture of distributions combined in such a way

Monte Carlo and Related Iterative Methods
291
that they give a shape that is not possible otherwise. Fortunately, mixture priors
possess desirable distributional and mathematical properties (O’Hagan 1994, Section
6.44-6.46). Furthermore, Dalal and Hall (1983) show that any distributional form can
be approximated as accurately as we like with mixtures of conjugate distributions,
although the number of components is not always easy to determine (see Carlin and
Chib (1995) for a way to use Bayes Factors to test speciﬁcations with diﬀering numbers
of components). Gelman et al. (2003) also caution about some problems that can arise
with improper and non-normalized mixture priors.
Generally though it is easy enough to normalize a mixture, say of several normals, or
some other common distributional form. However, it is possible that one might want
to create a more complicated mixture prior, and therefore have a need to numerically
normalize it. Panel 1 of Figure 9.5 shows a prior over the support [0:12] created by:
p(θ) =
⎧
⎨
⎩
(6 −θ)2/200 + 0.011
for θ ∈[0 : 6)
C(11, 2)/2
for θ ∈[6 : 12]
(9.21)
where C(11, 2) denotes a Cauchy distribution with location parameter 11 and scale
parameter 2. This prior distribution is created in R by combining the functions:
f1 <- function(x)
((6-x)^2)/200+0.011
f2 <- function(x)
dcauchy(x,11,2)/2
While it is possible to analytically calculate the normalizing constant for this prior, it is
actually much easier to numerically estimate it according to the described algorithms.
Figure 9.5 displays the integration process of this unnormalized distribution by the
three methods described.
Only ten bins are displayed here for purely expository
purposes and the integral quantities estimated, provided above the three panels, are
calculated using 100 bins over this interval. While this is much more accurate, it does
not provide a very revealing graph. In actual practice we might even be motivated to
exceed 100 bins since the speed of calculation is very quick.
Two standard reference works on mixture distributions in general are: Titterington,
Smith, and Makov (1985) and McLachlan and Basford (1988).
A more Bayesian
treatment of mixture models can be found in Hill and Tsai (1988), West and Turner
(1994), Mengersen and Robert (1996), Ferguson (1983), and West (1993). Mixture
distributions play an important role in Bayesian estimation because of their ﬂexibility.
At one extreme is the idea of nonparametric mixing functions such as the Dirichlet
process prior (Dalal and Hall 1980; Ferguson 1973; Ferguson and Phadia 1979; Hjort
1996; Petrone and Raftery 1997), although these tools can be extremely complex
and computationally diﬃcult (Carlin and Louis, 2000 (51ﬀ), Escobar 1994, Escobar
and West 1995; Korwar and Hollander 1976). At the other extreme, normal mixture
priors are a relatively straightforward extension (Andrews and Mallows 1974; Chib

292
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
and Tiwari 1991; Geweke and Petrella 1998; Vidakovic 1999), and mixtures of t-
distributions are only slightly more involved (Garthwaite and Dickey 1992).
0
2
4
6
8
10
12
0.05
0.10
0.15
Mixture Prior
0
2
4
6
8
10
12
0.05
0.10
0.15
Riemann, Integral = 0.6816
0
2
4
6
8
10
12
0.05
0.10
0.15
Trapezoid Rule, Integral = 0.6893
0
2
4
6
8
10
12
0.05
0.10
0.15
Simpsons Rule, Integral = 0.7047
FIGURE 9.5: Illustration of Numerical Integration
9.5
Gaussian Quadrature
Gaussian quadrature replaces the analytical integration process with a very accurate
form of (9.16) where the nodes of integration, the “abscissae,” are no longer equally spaced.
We ﬁx an integration formula of the form:
 b
a
ω(θ)g(θ)dθ ≈
n

i=1
mig(θi) + ϵ,
(9.22)
where: n is the order of the quadrature, mi are nonnegative weights, θi are the abscissae
constrained to be in the interval of integration, and ϵ is an error term. Now we are free
to deﬁne a sequence of polynomials of degree k, so that they are orthogonal to the weight

Monte Carlo and Related Iterative Methods
293
function on [a:b]:
 b
a
mipi(θ)pj(θ)dθ = 0,
(9.23)
for any two polynomials pi(θ) and pj(θ), where i ̸= j. Normally it would seem diﬃcult to
produce a useful set of orthogonal polynomials and the appropriate weights. Fortunately
though these are already tabulated for a wide range of integral types, and the bulk of
the work is reduced to ﬁtting the quadrature formula that corresponds to the range of
integration.
Before summarizing some of the standard Gaussian quadrature formulas, an artiﬁcial
and commonly provided example is reviewed to illustrate the mechanics of the orthogonal
polynomial manipulation. Suppose we want to integrate the standard normal PDF over the
interval [0:1]. Obviously this quantity is easily obtained from Fisherian tables or from R:
(pnorm(1) - 1/2), but using this objective allows us to check the accuracy of our procedure.
The quadrature formula will be calculated with four simple orthogonal polynomials, and at
only two nodes (for simplicity these will be the endpoints of the interval [a = 0 : b = 1]).
The objective is then to ﬁnd an explicit form for:
 b
a
ω(θ)g(θ)dθ ≈ω1g(θ1) + ω2g(θ2).
(9.24)
The four polynomials are deﬁned over the interval of interest and evaluated with a single
m = (a + b)/2:
▷Constant Function:
g(θ) = (θ −m)0 =
 b
a
(1)dθ = b −a
ω1(θ1 −m)0 + ω2(θ2 −m)0 ≡b −a.
▷Linear Function:
g(θ) = (θ −m)1 =
 b
a
(θ −m)dθ = (θ −m)2
2

θ=b
θ=a
= 0
ω1(θ1 −m)1 + ω2(θ2 −m)1 ≡0.
▷Cubic Function:
g(θ) = (θ −m)3 =
 b
a
(θ −m)3dθ = (θ −m)4
4

θ=b
θ=a
= 0
ω1(θ1 −m)3 + ω2(θ2 −m)3 ≡0.
▷Quartic Function:
g(θ) = (θ −m)4 =
 b
a
(θ −m)4dθ = (θ −m)5
5

θ=b
θ=a
= (b −a)3
12
ω1(θ1 −m)4 + ω2(θ2 −m)4 ≡(b −a)3
12
.

294
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
We now have 4 equations and four unknowns allowing us to solve for these unknowns:
θ1 = a + b
2
−b −a
2
√
3
θ2 = a + b
2
+ b −a
2
√
3
ω1 = b −a
2
ω2 = b −a
2
.
Therefore:
 b
a
ω(θ)g(θ)dθ ≈b −a
2
g
a + b
2
−b −a
2
√
3

+ b −a
2
g
a + b
2
+ b −a
2
√
3

.
The integral of interest is then solved as:
 1
0
1
√
2πe−1
2 θ2dθ ≈
1
√
2π
'1
2g
1
2 −
1
2
√
3

+ 1
2g
1
2 +
1
2
√
3
(
= 0.3412211,
whereas the actual answer produced by R is 0.3413447. This demonstrates that even the
most simple of Gaussian quadrature schemes can produce a reasonably accurate answer.
Obviously we would not want to perform these steps for many points and more com-
plicated integral forms. Fortunately, there are some standard forms that work very well
in a wide variety of circumstances. The classic forms are expressed as follows, where the
polynomials are given in the notation of Abramowitz and Stegun (1977, Chapter 22):
▷Legendre
integral range: [−1 : 1]
weight function: ω(θ) = 1
mj = −
2
(n + 1)pn+1(aj)p′n(aj)
polynomial: Pn(θ) = a−1
n
n

m=0
cmθm.
▷Jacobi
integral range: [−1 : 1]
weight function: ω(θ) = (1 −θ)α(1 + θ)β
mj =
−(2 + 2n + α + β)Γ(1 + n + α)Γ(1 + n + β)2(α+β)
(1 + n + α + β)Γ(1 + n + α + β)(n + 1)!pn+1(aj)p′n(aj)
polynomial: P (α,β)
n
(θ) = a−1
n
n

m=0
cm(θ −1)m.

Monte Carlo and Related Iterative Methods
295
▷Hermite
integral range: [−∞: ∞]
weight function: ω(θ) = exp(−θ2)
mj =
2n+1n!π
1
2
pn+1(aj)p′n(aj)
polynomial: Hn(θ) =
n

m=0
cmθm.
▷Laguerre
integral range: [0 : ∞]
weight function: ω(θ) = exp(−θ)
mj =
n!2
pn+1(aj)p′n(aj)
polynomial: Ln(θ) = a−1
n
n

m=0
cmθm.
Thisted (1988, Chapter 5) gives an extensive list of variations on these quadrature rules
as well as a practical description of the theory behind orthogonal polynomials.
Classic
and useful references on Gaussian quadrature include Davis and Rabinowitz (1984), Galant
(1969), Golub and Welsch (1969), Steen, Byrne, and Gelbard (1969), as well as Stroud and
Secrest (1966). Acton (1996, Chapter 3) gives an excellent and informative analysis of what
can go wrong in implementing these algorithms. There are also a number of well-known
and commonly used extensions to the basic procedure. These computational tools include:
Kronrod’s (1965) idea of inserting an additional n+1 points interleaved around the original
n with Legendre polynomials in order to improve the accuracy of the estimation, Patterson’s
(1968) similar notion of adding the maximally precision increasing p additional nodes to
the original n, and the principle of concentrating more attention on problematic regions of
the integral either automatically or manually (Naylor and Smith 1982, 1983; Skene 1983).
9.5.1
Redux
This subsection brieﬂy introduced some of the very basic concepts in numerical integra-
tion. The purpose is to provide a frame of reference for the posterior estimation tools that
we develop later in this chapter and in the subsequent discussion of MCMC. The presen-
tation of the classical numerical integration procedures is far from complete as there are
many variations on the ideas presented and most of these are well described in both the
applied mathematics literature as well as in the cited statistics publications.
An area of importance that we have ignored here is the determination of error rates for

296
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
the various techniques. In general these are extensively worked out, and detailed explana-
tions can be found in Kennedy and Gentle (1980, Chapter 5), Pennington (1970, Chapter
7), Thisted (1988, Chapter 5), and many other references. A lot of work has been done to
improve the error rates of various techniques such as the Romberg integration improvement
to the trapezoid rule (Gray 1988; Pizer 1975), and composite rules (Conte and de Boor
1980, Section 7.4).
Finally, the importance of transitioning from these traditional approaches of calculating
integral quantities to the stochastic simulation techniques described in later chapters cannot
be overstated. While the tools just described are often suﬃcient, there are circumstances
where they fail to produce a solution such as with high-dimensional problems, complex pa-
rameterizations, and tricky bounds (Stewart 1983). Concerning advances in the Bayesian
computation of integrals, Adrian Smith observed rather presciently in 1984 that “. . . we are
about to witness signiﬁcant breakthroughs in this area, both in purpose-built Monte Carlo
methodology for statistical problems and in adaptive quadrature rules exploiting statisti-
cally motivated kernels.” He was certainly right, although the former has far outstripped
the latter.
9.6
Importance Sampling and Sampling Importance
Resampling
Importance sampling is a modiﬁcation of rejection sampling that adjusts the standard
procedure by placing greater emphasis on the “important” regions: those that have a higher
density than others (Marshall 1956, Rubin 1988). Points that do not closely reﬂect the tar-
get distribution are no longer rejected (discarded), they are instead downweighted. Thus,
importance sampling provides a much more eﬃcient method by making use of every gen-
erated value, albeit with diﬀering weights. It can also be made more ﬂexible and adaptive
(Oh and Berger 1992).
Importance sampling is motivated by the key deﬁnition underlying Monte Carlo simu-
lation:
ˆE[h(θ)] = 1
m
m

i=1
h(θi) −→

Θ
h(θ)f(θ)dθ = E[h(θ)],
(9.25)
where the arrow denotes almost sure convergence provided by the (strong) Law of Large
Numbers (Geweke 1989). We introduce now an approximation function, g(θ), on the same
support as f(θ) and hopefully resembling it over this support. Introducing the additional
function allows us to express the integral in (9.25) as:
E[h(θ)] =

Θ
h(θ)f(θ)g(θ)
g(θ)dθ =

Θ
h(θ)g(θ)
f(θ)
g(θ)

dθ.
(9.26)

Monte Carlo and Related Iterative Methods
297
This suggests expressing the sum in (9.25) as
ˆE[h(θ)] = 1
m
m

i=1
h(θi)
f(θi)
g(θi)

,
(9.27)
the fundamental identity for importance sampling. As long as the ratio f(θi)/g(θi) is easily
calculated, this is a direct calculation. Now return to the familiar scenario where f() is
diﬃcult to generate from, but g() is not. There is no requirement in the calculation of
ˆE[h(θ)] that the θi values be produced from the target distribution, so why not use the
approximation distribution for this purpose? The nicest part of this approach is that we
only require that the approximation function be deﬁned on the same support and be close
to the target distribution, so within such constraints, we are free to use any form that
is convenient to draw from. Once the value is drawn, the calculation proceeds by simply
plugging this value into three deﬁned functions.
The ﬁrst explicit application of importance sampling was due to Kloek and van Dijk
(1978), and Zellner and Rossi (1984) give an early, inﬂuential application to qualitative
responses. Geweke (1989) gives an extensive treatment of the theory along with proofs. For
detailed summaries, see Gelman et al. (2003), Robert and Casella (2004, Chapter 1), and
Tanner (1996, pp.54-59).
It is critical to start with an approximation distribution that is near the target distribu-
tion. Failing to do so usually requires excess computation since many points will not reﬂect
high density areas of the target distribution and be given very low weights. One indication
of a poor choice for the approximation distribution is a low average acceptance rate, and
this suggests monitoring it if possible. There are a number of possible criteria for choosing
the approximation distribution form, including: matching of moments or modes, Laplace
approximations (normals), mixtures, and reparameterizations of convenient forms.
Since the generation of candidate values is determined by the approximation distribution
and not the target distribution, these values are all characteristic of the target distribution
but are on the same support. Summary statistics from the draws are adjusted by importance
weights to correct for the generation process:
ωi = f(θi)
g(θi) ,
(9.28)
where again f() is the target distribution, g() is the approximation distribution producing
value θi.
This ratio is small in cases where f(θi) is a low density point for the target
distribution and a g(θi) is a high density point for the generating distribution, and vice-
versa. For example, an estimate of the mean of the f() function from m generated values
is given by a restatement from above:
E[θ] =
m
i=1 θiωi
m
i=1 ωi
,
(9.29)
and more generally for some function of the θs, h(θ), as deﬁned above:
E[h(θ)] =
m
i=1 h(θi)ωi
m
i=1 ωi
.
(9.30)

298
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
Actually there is an important caveat here; this empirical quantity converges to the expected
value above, and the rate of convergence depends on the quality of the approximation
distribution (Rubin 1987a, 1988; Tanner 1996). Liu (2001, p.35) provides a handy way to
measure how much the approximation distribution diﬀers from target distribution. Deﬁne
Varg[ω(θ)] as the variance of a sample of m importance weights over the distribution deﬁned
by g(), then the eﬀective sample size (ESS) is deﬁned by:
ESSm =
m
1 + Varg[ω(θ)],
(9.31)
where a large variance leads to low eﬃciency relative to sample size.
For calculating credible intervals and various quantiles such as tail probabilities, it is
generally easier to use the sorted accepted values. So, for instance, the median statistic is
produced by the sorted vector value such that the sum of the normalized sampling weights
of lower values is equal to 0.5. As with rejection sampling and other similar procedures, we
can easily calculate the variance of the statistics produced from importance sampling. For
a statistic, h(θ), the variance from simulating is given by:
Var[h(θ)] =
m
i=1((h(θi) −E[h(θi)])ωi)2
m
i=1 ωi
.
(9.32)
In fact, without this adjustment, the na¨ıve importance sampling variance taken directly
from the empirical draws can sometimes be “too good”: lower than with draws taken from
the actual distribution (so-called super-eﬃciency; see Liu [1996a] for a detailed discussion).
■Example 9.5:
Sampling-Importance Resampling for Improving Variance Es-
timation. This example, derived from a more complex model in Gill and King (2004),
presents a data analysis where importance sampling is used to provide a more accurate
view of the posterior variance of coeﬃcient estimates from a simple logit model (see
also the overview in Li [2004]). The application is to public policy data on poverty and
its associated potential explanations, measured by state at the county-level (census
“FIPS”). The data consist of 1989 county level economic and demographic variables
for the 196 nonmetropolitan counties in Texas out of all 2276 nonmetropolitan U.S.
counties (“ERS Typology,” http://www.census.gov/). The dichotomous outcome
variable indicates whether 20% or more of the county’s residents live in poverty.
We place diﬀuse (uniform) priors on each of these explanatory variables. Since the
outcome variable (Y ) is dichotomous, we analyze these data using a generalized linear
model with a logit link function: p(Yi = 1|Xi) = [1 + exp(Xiβ)]−1, where Xi is a set
of explanatory variables for case i.
Table 9.3 gives the logit model results before and after importance sampling with a
multivariate-t approximating distribution. In general there is no substantial diﬀerence,
but Transfer becomes noticeably more reliable statistically. What is happening here
is that the explanatory variable Federal is a poor contribution to the model and
actually makes other explanatory variables less reliable than when it is excluded.

Monte Carlo and Related Iterative Methods
299
TABLE 9.3:
Logit Regression Model: Poverty in
Texas
Standard Results
Importance Resampling
Parameter
Coeﬃcient
Std. Error
Coeﬃcient
Std. Error
Black
15.91
3.70
15.99
3.83
Hispanic
8.66
1.48
8.46
1.64
Govt
1.16
0.78
1.18
0.74
Service
0.17
0.62
0.19
0.56
Federal
-5.78
16.20
-3.41
17.19
Transfer
1.29
0.71
1.25
0.63
Population
-0.39
0.22
-0.38
0.21
Intercept
-0.47
1.83
-0.51
1.68
The dichotomous outcome variable ﬂags whether 20+% of the county’s residents live
below the poverty line. The explanatory variables selected are:
Govt
a dichotomous variable indicating whether government
activities contributed a weighted annual average of 25% or
more labor and proprietor income over the previous 3 years
Service
a dichotomous variable indicating whether service activities
contributed a weighted annual average of 50% or more
labor and proprietor income over the 3 previous years
Federal
a dichotomous variable indicating whether federally owned
lands make up 30% or more of a county’s land area
Transfer
a dichotomous variable indicating whether income from
transfer payments (federal, state, and local) contributed a
weighted annual average of 25% or more of total personal
income over the past 3 years
Population
the log of the county population total for 1989
Black
the proportion of Black residents in the county
Hispanic
the proportion of Hispanic residents in the county
To demonstrate this near-singularity, Figure 9.6 provides a matrix of the bivariate pro-
ﬁle contour plots for each pair of coeﬃcients given at contours of 0.05, 0.15, . . ., 0.95,
where the 0.05 contour line bounds 0.95 of the data, holding constant all other pa-
rameters at their mode. Each posterior surface is concave at the global maxima, but
the curvature for Federal is very slight. This almost-ﬂatness produces a near-ridge

300
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
in the contours for each other explanatory variable paired with Federal where the
ridge is gently sloping around maximum value in each proﬁle plot.
7.0
8.0
9.0
10.0
Latino
Black
1
2
3
4
5
Government
Latino
–1.5
–0.5
0.5
1.5
Service
Government
–150
–100
–50
0
Federal
Service
1.0
2.0
3.0
4.0
Transfer
Federal
–2
0
2
4
6
8
10
–0.40
–0.30
–0.20
2
3
4
5
6
7
1
2
3
4
5
–1.5
–1.0
–0.5
0.0
0.5
1.0
–150
–100
–50
0
50 0.0
0.5
1.0
1.5
2.0
2.5
3.0
Population
Transfer
FIGURE 9.6: Contourplot Matrix, Rural Poverty in Texas
Figure 9.6 indicates that the distribution of the coeﬃcient on Federal is quite asym-
metric, and shows that the probability density drops as we come away from the
near-ridge. This case illustrates how importance sampling can be used as a means of
improving variance estimates even when less satisfactory results already exist.
■Example 9.6:
Monte Carlo Marginalization, European Migration Data.
We
return here to the Pareto model of European migration counts from Section 9.2. Recall
that it was prohibitively time-consuming to produce an analytical posterior marginal
for β free from the other model parameter α: π(β|x). The joint posterior distribution
was given by:
π(α, β|x) ∝αn+C(Bβ)−(nα+Aα+1)exp [−α(D + E)]
with E =
n

i=1
log(xi) + Alog[B] −(n + A)log[min(B, x)].
(9.33)
We also know that the posterior distribution of α is G(n + C, D + E). These two
posteriors are enough to marginalize using a variant of importance sampling that will
allow us to obtain the unconditional marginal posterior distribution for β. This process

Monte Carlo and Related Iterative Methods
301
(Robert and Casella [2004], Gelman et al. [2003]) uses π(α|x) as the approximation
distribution since it is fully known and therefore easy to sample from. Considering
the ratio of the known joint posterior to the known marginal as the importance ratio
provides:
π(β|x) =

α
π(α, β|x)dα
=

α
π(α, β|x)
π(α|x) π(α|x)dα
≈Eα|x
'π(α, β|x)
π(α|x)
(
.
(9.34)
This is implemented by the following steps:
▷Grid the support of β into I discrete values: [β1, β2, . . . , βI].
▷For each value of βi:
1. Draw J values of αj from π(α|x);
2. Plug each of these αj into π(αj,βi|x)
π(αj|x) , and take the mean;
3. So the result is π(βi|x), the posterior probability value for β at grid-point i.
The “art” involved in this process is really the determination of the grid-points for
the range of β and the number of α values to sample at each iteration. Obviously
there is a trade-oﬀbetween computer time and accuracy, but with each iteration of
new hardware, the preference for more samples should increase. The choice here is
to grid the range of β into 1,000 subunits and draw 100 random values of α on each
iteration. This produces an empirical marginal posterior for β. We can summarize
this distribution by posterior quantiles or just calculate the weighted empirical mean
and standard error: ¯βposterior = 141.96, and SE(β)posterior = 62.61.
9.6.1
Importance Sampling for Producing HPD Regions
Returning to a more appropriately Bayesian purpose, this section shows how importance
sampling can be useful in producing highest posterior density intervals from simulated
posterior values. Recall from Section 2.3.2.2 that HPD regions provide the region of the
posterior support that meets two criteria for some chosen value α:
▷the posterior density value for every point in the HPD region exceeds that for every
point outside of the HPD region,
▷the interval is the shortest length along the support that provides (1 −α) proportion
of the total posterior density.
Notably this deﬁnition means that the HPD region does not need to be contiguous like
the credible interval. Unfortunately, the HPD region is sometimes much more diﬃcult to

302
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
calculate since it evaluates the shape of the posterior rather than simply marching out some
designated length from the mean.
Suppose that we have generated an importance sample for the posterior of interest,
θ1, θ2, . . . , θm from π(θ|X), using the Monte Carlo tools in this chapter or the MCMC tools
in the next chapters. Denote the generating distribution as g(θ). The trick is to think in
CDF terms using the empirically collected values, so that the value of interest for a speciﬁed
α-level is:
α =
 θ[α]
−∞
π(θ|X)dθ ≈
m
i=1 I(θi≤θ[α])π(θi|X)/g(θi)
m
i=1 π(θi|X)/g(θi)
(9.35)
where I(θi≤θ[α]) is an indicator function equal to one if the condition is met and zero oth-
erwise. Therefore everything in this fraction except m
i=1 I(θi≤θ[α]) is really an importance
weight, wi. Intuitively, this means that we can sort the simulated values and approximate
the CDF value at some arbitrarily chosen θ[α] with the sum i
j=1 w[j] for θ[i] ≤θ[α] < θ[i+1].
The precision of this approximation is simply a function of how close the consecutive sorted
values are to each other (meaning really how many values were drawn). Now stated in
steps, the process is:
1. generate θi ∼g(θ), i = 1, . . . , m,
2. sort these values, giving θ[i], ∋θ[i−1] < θ[i], i = 2, . . . , m,
3. calculate the associated importance weights for the sorted values:
w[i] =
π(θ[i]|X)/g(θ[i])
m
i=1 π(θ[i]|X)/g(θ[i])
4. for i > 1 and i < m (since the empirical CDF is assigned to be zero and one at the
endpoints),
α[i] =
i

j=1
w[j].
Once we have this last vector, the corresponding value, θ[α], for any chosen CDF level, α[i]
is simply selected. If we want a 0.95 HPD region for a unimodal posterior, then select
the sorted θ[i] values corresponding to the (0.05/2)m and (1 −0.05/2)m vector locations
(one reason that it is often convenient to generate samples in multiples of 100s or 1,000s).
Chen and Shao (1999) explore in detail the properties of importance sampling estimates of
cumulative posterior functions.
9.7
Mode Finding and the EM Algorithm
Expectation-Maximization (EM) is a very ﬂexible and popular technique for so-called
incomplete data problems. This description arises because the EM algorithm is designed to

Monte Carlo and Related Iterative Methods
303
“ﬁll in” unknown information for a speciﬁed model where the notion of what is “missing”
is very general. This missing information can be unknown parameters, or missing data, or
both. In fact, the missing data can even be hypothetical results, but one must be careful
in interpreting these results.
The idea of the EM algorithm is marvelously and beguilingly simple. In the ﬁrst step,
temporary data that represent a reasonable guess are assigned to replace the missing data.
Then, the parameter estimation proceeds as if we now have a complete data problem (com-
plete in the sense that observed and missing data are now both “available” in the analysis).
Once this produces a solution for the parameter estimates, then we use these to update
the assignment of the temporary data values with better guesses, and again perform a full
model estimation process. This two-step process is repeated as often as required until the
diﬀerence in the parameter updates becomes arbitrarily close to zero and we therefore have
convergence.
The EM algorithm does not just represent a practical convenience.
There is strong
theoretical justiﬁcation for this method in that the iterated EM process gives a series of
parameter estimates that are monotonically increasing on the likelihood metric and are
guaranteed to converge to a maximum point under very general and nonrestrictive regu-
larity conditions (Boyles 1983; Dempster, Laird, and Rubin 1977; Wu 1983). In the case
of likelihood functions from exponential family forms (i.e., generalized linear modeling ap-
plications) there are no substantial complexities (Nelder 1977, p.23), but there do exist
other cases where the ﬁrst step of determining candidate values can be complicated in large
dimensions or with complicated likelihood functions. Worse yet, all non-exponential family
applications of the EM algorithm require the use of an asymptotic approximation to an
exponential family form (Rubin 1991).
The EM algorithm was formalized by Dempster, Laird, and Rubin in their seminal 1977
article (henceforth referred to as DLR-77, see also the accompanying discussion such as
Beale [1977]). However, this was not a completely new idea and actually the ad hoc process
of iteratively ﬁlling in tentative data and estimating parameters had been developed by
earlier authors. Several authors refer to a piece by McKendrick (1926) as the ﬁrst known
iterative EM-like technique, and Newcomb used a similar approach as early as 1886. Healy
and Westmacott (1956) propose an iterative technique for least squares implemented on
newly developed “automatic computers.” Hartley’s (1958) “simple iterative method” ﬁlls
in missing tabular values due to censoring and truncation with an iterative process that
is essentially a less general form of the EM algorithm restricted to discrete distributions
only because his ﬁll-in step is: “Using 1θ compute ‘improved’ estimates, 1ni of the ‘miss-
ing frequencies’ by proportional allocation based on the Poisson distribution. . . ” (p.176).
Zangwill’s textbook (1969) provides some of the key proofs, including a critical one con-
cerning conditions for monotonic convergence (Section 4.5) as well as a description of the
cyclic coordinate ascent (p.111) that is very close to the general EM procedure. Haberman
(1974a, Chapter 3) discusses another nearly concurrent technique, iterative proportional
ﬁtting, in the context of count data. Other notable works that develop precursors to the

304
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
fully articulated EM algorithm of DLR-77 include Baum and Petrie (1966), Baum and
Eagon (1967) and Baum et al. (1970) for Markov models, Beale and Little (1975) applied
to multivariate normals, Hartley and Hocking (1971) for a special missing data case, Or-
chard and Woodbury (1972) for the “missing information principle,” and Sundberg (1974)
for the exponential family. Specialized works that predate DLR-77 are Blight (1970), Buck
(1960), Carter and Myres (1973), Chen and Fienberg (1974), and Turnbull (1976).
9.7.1
Deriving the EM Algorithm
Suppose that we are interested in obtaining the posterior distribution of an unknown
k-dimensional θ coeﬃcient vector, given an outcome variable vector y, and an observed
X matrix of explanatory data values assumed to be distributed iid according to f(X|θ).
In a complete data situation, where all of X is observed, we would normally proceed with
standard maximum likelihood estimation by ﬁnding the maximum of the function ℓ(θ|X),
and solving for ˆθ as described in Appendix A. Analogously in a Bayesian process, we would
produce a posterior distribution from π(θ|X) ∝p(θ)p(X|θ). Neither of these approaches
are directly possible if there happen to be missing data in X unless the data are considered
“missing completely at random,” meaning that they are safely ignorable from a parameter
estimation perspective and can be removed from the analysis (Little and Rubin 2002).
However, neither of these approaches are directly possible if there happen to be missing
data in the X matrix, since deleting cases produces biased results (Schafer 1997).
The algorithm starts with ﬁrst segmenting the X-matrix into two constituent parts:
X = [Xobs, Xmis], and restating the distribution function as:
f(X|θ) = f(Xobs, Xmis|θ) = f(Xobs|θ)f(Xmis|Xobs, θ).
(9.36)
Now similarly segment the log likelihood function into two distinct components:
ℓ(θ|X) = ℓ(θ|Xobs, Xmis)
= ℓ(θ|Xobs) + logf(Xmis|Xobs, θ).
(9.37)
Rearrange this form to create a statement with both unknowns collected on the right-hand
side:
ℓ(θ|Xobs) = ℓ(θ|Xobs, Xmis) −logf(Xmis|Xobs, θ).
(9.38)
Since we have an expression for the distribution of these missing values, f(Xmis|Xobs, θ),
conditional on the quantities Xobs and θ, we can average over this uncertainty by taking
expectations with respect to Xmis|Xobs, θ on both sides:

ℓ(θ|Xobs)f(Xmis|Xobs, θ)dXmis
=

ℓ(θ|Xobs, Xmis)f(Xmis|Xobs, θ)dXmis
−

logf(Xmis|Xobs, θ)f(Xmis|Xobs, θ)dXmis.
(9.39)

Monte Carlo and Related Iterative Methods
305
The left-hand side here simpliﬁes back to ℓ(θ|Xobs) because the integral ends up operating
over just the isolated complete PDF for Xmis:

ℓ(θ|Xobs)f(Xmis|Xobs, θ)dXmis
= ℓ(θ|Xobs)

f(Xmis|Xobs, θ)dXmis
= ℓ(θ|Xobs).
(9.40)
Now we have an expression based only on the observed data that relates the obtainable
likelihood to two quantities that can be manipulated.
ℓ(θ|Xobs) =

ℓ(θ|Xobs, Xmis)f(Xmis|Xobs, θ)dXmis
−

logf(Xmis|Xobs, θ)f(Xmis|Xobs, θ)dXmis.
(9.41)
But it is still not enough because we do not have values for θ on the right-hand side of
the conditionals. So start with an obviously wrong, but hopefully reasonable, value for
the vector. Plug this starting value into the conditional statement for the distribution of
the missing data, but not into the likelihood component since that is the object of our
procedure. This produces the following expression with subfunctions labeled according to
the original notation in Dempster, Laird, and Rubin (1977, p.6):
ℓ(θ|Xobs) =

ℓ(θ|Xobs, Xmis)f(Xmis|Xobs, θ(0))dXmis
 
!"
#
Q(θ|θ(0))
−

logf(Xmis|Xobs, θ)f(Xmis|Xobs, θ(0))dXmis.
 
!"
#
H(θ|θ(0))
(9.42)
The key is to evaluate the Q and H functions in (9.42) separately.
We focus exclusively on Q(θ|θ(0)) for the moment. Treat θ(0) as known and perform the
integration to obtain the conditional expectation of this missing data, given the observed
data and the temporarily acceptable value for the θ vector to produce:
Q(θ|θ(0)) = ℓ∗(θ|Xobs, θ(0)).
(9.43)
This is just the expected log likelihood as if θ were θ(0) produced by substituting expecta-
tions for the missing data. Call this part of the process the “E-Step” since we are taking
an expectation.
Given that we now have a full-information log likelihood function for the unknown θ
vector, we can apply maximum likelihood estimation to produce a new estimate for θ. This
is the “M-Step,” and it constitutes nothing more than a straightforward application of MLE
where the result, Q(θ(1)|θ(0)), is the most likely value of θ, given the observed data and the
E-Step expectation over the unobserved data (which was itself conditioned on the initial

306
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
values for θ). By deﬁnition, the MLE process means that:
Q(θ(1)|θ(0)) ≥Q(θ|θ(0))
(9.44)
for all other values of θ not equal to θ(1), where equality occurs only at the maximum.
Note that this is where the unimodal assumption plays a critical role. So on each iteration
we are necessarily increasing the value of the Q function, conditional on the current value,
θ(0).
Now let us worry about the H(θ|θ(0)) component. In full form it is:
H(θ|θ(0)) = EXmis|θ(0)[logf(Xmis|Xobs, θ(0))].
(9.45)
The E-Step of going from θ(0) to estimated value θ is characterized by expectation of the
function:
g(θ(0)) = logf(Xmis|Xobs, θ(0)),
(9.46)
where the expectation is concave to the x-axis and therefore has a single maximum value.
By the deﬁnition of the function g(θ(0)):
H(θ|θ(0)) = E[g(θ(0))]
and
H(θ(0)|θ(0)) = g(E[θ(0)]).
(9.47)
Thus, we will subtract a smaller value of H(θ|θ(0)) in the E-Step by moving from θ(0) to
θ(1), by Jensen’s inequality for concave functions: E[g(θ)] ≤g(E[θ]) (Casella and Berger
2002, p.190; Shao 2005; Stuart and Ord 1994, p.67). Therefore, we have a proof that we
can ignore the H(θ|θ(0)) part since it gets smaller automatically on each iteration of the
algorithm.
Actually this is more general since we now have proof that ℓ(θ(1)|Xobs) ≥ℓ(θ(0)|Xobs)
for any chosen starting value of θ. This property, the “monotonicity of EM” property, holds
true because:
Q(θ(1)|θ(0)) ≥Q(θ|θ(0)),
(9.48)
and simultaneously
H(θ|θ(0)) ≥H(θ(1)|θ(0)).
(9.49)
Now comes the best part. Since we chose θ(0) completely arbitrarily, then we can repeat
the process assured of getting an equally desirable or better result.
So to summarize the steps (rather than the justiﬁcation):
1. Start with. . .
the data: X = [Xmis, Xobs],
a likelihood function: f(X|θ),
and arbitrary values for the vector: θ(k).
2. [E-Step:].. . compute:
Q(θ(k+1)|θ(k)) =

ℓ(θ|Xobs, Xmis)f(Xmis|Xobs, θ(k))dXmis

Monte Carlo and Related Iterative Methods
307
3. [M-Step:].. . choose the value for θ that maximizes
Q(θ(k+1)|θ(k))
4. repeat until the diﬀerence between θ(k+1) and θ(k) is arbitrarily small.
If in the M-Step we merely increase the Q function rather than maximize it because
maximization is too diﬃcult, then this variant is called the generalized EM (GEM) proce-
dure (DLR-77, 6-7). However, this process is quite diﬀerent in that now an iterative scheme
such as Newton-Raphson needs to be applied so that the ﬁnal result of the M-Step is a
conditional maxima that can be used in the next stage of algorithm. Typically some con-
straints are required on this process and these can make the GEM somewhat more complex
than the standard EM algorithm (McLachlan and Krishnan 1997).
9.7.2
Convergence of the EM Algorithm
Unlike most numerical maximization techniques such as Newton-Raphson, steepest de-
scent, or Fletcher-Powell (see Thisted 1988, Chapter 4), the EM algorithm does not require
the calculation of ﬁrst or second derivatives. This can be a distinct advantage for compli-
cated parametric forms. The EM algorithm is also fast relative to many competitors. The
convergence rate is geometric (Laird 1978, Sundberg 1976, as well as DLR-77),2 and there
are a number of tricks for speeding up convergence further (e.g., Jamshidian and Jennrich
1993; Louis 1982, Section 5; Meilijson 1989). This rate of convergence is, however, propor-
tional to 1−m, where m is the proportion of missing information (Rubin 1991, p.244). One
common criticism is that convergence can be particularly slow in the absence of well-deﬁned
modes (Jamshidian and Jennrich 1993; Laird, Lange, and Stram 1987; Rubin 1991).
Wu (1983, p.98) proved that the limit points of an EM algorithm are stationary points
given a continuity condition that is “very weak and should be satisﬁed in most practical
situations,” a result supported by Baum et al. (1970), Haberman (1977), and Boyles (1983).
Unfortunately stationary points (deﬁned as a point ˜x so that
d
dxf(˜x) = 0 [Gill, Murray, and
Wright 1981, p.62]) are not just maxima, they can be minima or inﬂection points as well.
Luckily exponential family forms and a wide variety of the other parametric speciﬁcations
used in the social sciences meet a stronger condition that assures convergence to a local or
global maxima (see Wu 1983, Theorem 3). If the likelihood function is unimodal with only
one stationary point, then the EM algorithm is guaranteed to ﬁnd this stationary point
(DLR-77, Theorem 1; Wu 1983, Property vii), and with reasonable sample size, this is
rarely a diﬃcult condition for typical parametric forms.
It should be pointed out that the fact that the EM algorithm ﬁnds local as well as
2Geometric convergence means that a series converges to its limiting value on the order of rn as a
geometric sequence deﬁned by: sn = α + ◦(rn) as n →∞for |r| < 1, and ﬁnite constant α. A process that
possesses a geometric convergence rate reaches its limit approximately as fast as the series ri, i = 0 →∞
(|r| < 1), reaches 1/(1 −r).

308
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
global maxima is not necessarily a liability. In fact, there are instances when we would like
to know where all maxima are located and the EM algorithm started from diﬀerent points
in the sample space is an ideal mode-ﬁnder.
This characteristic will prove very useful
later when we have procedures that describe posterior distributions by wandering around
in them (i.e., MCMC), but we require some sense of the right “neighborhood” to sample.
Obviously knowledge of the mode or modes of a posterior distribution will greatly facilitate
such a process. However, the use of the EM algorithm as mode ﬁnding precursor to other
techniques does highlight a deﬁciency of the procedure: it does not naturally produce a
measure of uncertainty describing the curvature around the mode. However, this weakness
is easily solved by adjunct procedures.
It is not necessary to furnish further proof of the convergence of the EM algorithm
as the basics have just been laid out here and complete details are furnished in DLR-77,
Chapter 3 of McLachlan and Krishnan (1997), and particularly carefully by Wu (1983).
Boyles (1983) gives convergence results speciﬁcally for the generalized EM algorithm. The
wealth of published applications of the EM algorithm in statistics and in social science ﬁelds
is a testament to the robustness and durability of the algorithm across many speciﬁcations
and data types.
■Example 9.7:
EM Applied to a Multinomial Model of U.S. Energy Con-
sumption This example presents a non-Bayesian model in which the consumption
of energy in the United States is treated as a multinomial count by quadrillion British
Thermal Units (BTUs) across diﬀerent sources. The exposition here is distinct but in
the spirit of the ﬁrst example of DLR-77 (resummarized by other authors many, many
times) in which they model genetic frequencies. A multinomial PMF with relatively
few categories provides a particularly illuminating demonstration of the EM algorithm
because the theoretical steps can be presented without extraneous technical material.
Consumption of energy inputs is counted annually by rounded quadrillion BTUs ac-
cording to source type: fossil fuel, coal, hydroelectric, and nuclear, denoted in this
order by the vector: Y = [y1, y2, y3, y4]. These counts are assumed to arise from a
multinomial(n, p1(θ)) form with cell probabilities:
p1(θ) =
'2
9 + 2
9θ, 3
9 −2
9θ, 2
9, 2
9
(
,
for:
0 ≤θ ≤1,
where these cell probabilities are driven by substantive theoretical work on energy
policy and politics in the United States (Bohi, Toman, and Wells 1996; Davis 1992;
Sharp, Register, and Grimes 1999), and are currently a very topical issue in public
policy and politics.
Therefore the speciﬁed multinomial PMF is:
f(y|n, θ) =
n!
y1!y2!y3!y4!
2
9 + 2
9θ
y1 3
9 −2
9θ
y2 2
9
y3 2
9
y4
(9.50)
(Johnson, Kotz, and Balakrishnan [1997]). This is a straightforward application of the

Monte Carlo and Related Iterative Methods
309
multinomial distribution and it presents no particular estimation problem as speciﬁed.
Normally we would observe some data for a given year, [y1, y2, y3, y4], and simply
estimate θ via maximum likelihood. Now suppose that the category for fossil fuels
was known to be composed of two subcategories for natural gas and petroleum with
the theoretically driven cell probabilities given by:
zpetroleum = 2
9θ
znatural gas = 2
9
y1 = zpetroleum + znatural gas = 2
9 + 2
9θ.
The cell probabilities are given by the 5-category vector:
p2(θ) =
'2
9θ, 2
9, 3
9 −2
9θ, 2
9, 2
9
(
,
for:
0 ≤θ ≤1.
Although we know that the fossil fuel category is split in terms of its theoretical
contribution to the model, we do not have the data for natural gas and petroleum
separately. Therefore this is a missing data problem because while we have composite
data for this category, we lack the individual contributions in the data. The mapping
from the 4-category incomplete data speciﬁcation, Y, to the new 5-category complete
data speciﬁcation, Z, is given by:
y1 = z1 + z2
y2 = z3
y3 = z4
y4 = z5.
So the multinomial PMF given in (9.50) is now given with the extra category:
g(z|n, θ) =
n!
z1!z2!z3!z4!z5!
2
9θ
z1 2
9
z2 3
9 −2
9θ
z3 2
9
z4 2
9
z5
.
(9.51)
This is a missing information problem because we know that the data are generated by
a 5-category PMF according to (9.51) but we are only able to observe the 4 categories
deﬁned by Y. From a public policy point of view it is important to distinguish natural
gas usage from petroleum usage although both constitute critical fossil fuels.
The log likelihood corresponding to (9.51) is:
ℓ(θ|z, n) = k + z1log
2
9θ

+ z3log
3
9 −2
9θ

(9.52)
where the terms that do not depend on the value of θ have been swept into a constant
term k. The derivative of the log likelihood function (being careful about the sign
from the chain rule) is:
∂
∂θℓ(θ|z, n) = z1
2
9θ −
z3
3
9 −2
9θ.
(9.53)

310
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
Setting this quantity equal to zero easily produces a complete data maximum likeli-
hood value of θ, given a little algebra:
ˆθ =
3z1
2z1 + 2z3
.
(9.54)
Of course we do not have a direct means of obtaining z1 since we can only observe
y1 = z1 + z2. This is where the EM-algorithm helps out. First we will iteratively
average over the missing data (the E-Step) to obtain a complete data speciﬁcation for
θ, then we will use maximum likelihood to obtain an interim estimate (the M-Step).
Averaging over Z and expressing in terms of the observed data Y is done by categories:
E[z1|y] = y1
2
9θ
2
9 + 2
9θ = y1
θ
1 + θ
E[z2|y] = y1
2
9
2
9 + 2
9θ = y1
1
1 + θ
E[z3|y] = y2
E[z4|y] = y3
E[z5|y] = y4.
Now back these values into the full data expected log likelihood for θ, (9.52), and
conditional on observed y:
h(θ) = E[logg(z|n, θ)|y]
= k + y1
θ
1 + θlog
2
9θ

+ y1
1
1 + θlog
2
9

+ y2log
3
9 −2
9θ

,
(9.55)
where once again the terms not dependent on θ are swept into a constant (i.e., con-
stants do not aﬀect the calculation of the MLE). We now have a full information log
likelihood, but based on only the observables since we averaged over the nonobserv-
ables. This is all we need to begin the EM algorithm. The EM steps are summarized
by:
The EM algorithm is thus deﬁned by cycling between the E-Step and the M-Step above
until we are satisﬁed with the resulting parameter estimate (subsequent changes are
minute). The data vector used is for the year 1989, which is just prior to the Gulf
War (an interesting case since it is a conﬂict associated with petroleum availability).
These values are given by:
Y1989 = [yfossilfuels = 44, ycoal = 19, yhydroelectric = 3, ynuclear = 6],
given in quadrillion BTUs (source: U.S. Department of Energy, http://
www.energy.gov/). The magnitude of usage here along with the form of (9.50) show
that on a macro-scale, energy usage is essentially a trade-oﬀbetween coal and fossil
fuels. Hydroelectric and nuclear power generation is ﬁxed for a given time period due

Monte Carlo and Related Iterative Methods
311
TABLE 9.4:
EM Output: U.S. Energy Consumption
Iteration j
θ(j)
log likelihood
1
0.9900000
NA
2
0.8029986
-119.9546
3
0.7615855
-119.3924
4
0.7504438
-119.2320
5
0.7472966
-119.1860
6
0.7463954
-119.1728
7
0.7461364
-119.1690
8
0.7460619
-119.1679
9
0.7460404
-119.1676
10
0.7460342
-119.1675
11
0.7460325
-119.1674
12
0.7460320
-119.1674
13
0.7460318
-119.1674
14
0.7460318
-119.1674
15
0.7460318
-119.1674
16
0.7460317
-119.1674
17
0.7460317
-119.1674
18
0.7460317
-119.1674
19
0.7460317
-119.1674
20
0.7460317
-119.1674
to the inherent cost of building generation facilities and the inﬂexible nature of the
capacity of these facilities. Since fossil fuel sources are predominantly imported and
coal fuel sources are predominantly domestically sourced, there are national security
issues involved as well.
This simple R program implements the steps described above for the Y vector and
the starting point, θ(0) = 0.99:
▷[Initialize:]
Specify the observed data, Y = [y1, y2, y3, y4], and some reasonable
starting point for the unknown parameter, θ(0).
▷[E-Step:]
Calculate: z(1)
1
= y1
θ(0)
1+θ(0) and z(1)
3
= y2.
▷[M-Step:]
Estimate: ˆθ(1) =
3z(1)
1
2z(1)
1
+2z(1)
3 .

312
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
▷[Repeat:]
Obtain [z(2)
1 , z(2)
2 , z(2)
3 , ˆθ(2)], . . . , [z(M)
1
, z(M)
2
, z(M)
3
, ˆθ(M)], until ˆθ(M) −
ˆθ(M−1) is suﬃciently small.
log.like <- function(theta,z)
{
k <- z[2]*log(2/9) + z[4]*log(2/9) + z[5]*log(2/9)
return( k + z[1]*log((2/9)*theta)
+ z[3]*log((3/9)-(2/9)*theta))
}
y <- c(44,19,3,6); z <- rep(NA,5)
num.iterations <- 20
out.mat <- matrix(rep(NA,num.iterations*2),ncol=2)
out.mat[1,1] <- 0.99
dimnames(out.mat)[[2]] <- c("theta","loglike")
for (j in 2:num.iterations)
{
z[1] <- y[1]*(out.mat[(j-1),1]/(1+out.mat[(j-1),1]))
z[2] <- y[1]*(1/(1+out.mat[(j-1),1]))
z[3] <- y[2]
z[4] <- y[3]
z[5] <- y[4]
out.mat[j,1] <- 3*z[1]/(2*z[1]+2*z[3])
out.mat[j,2] <- log.like(out.mat[j,1],z)
}
A couple of things are worth noting here. The log-factorial notation has not been
incorporated in the k constant, but it easily could be (clearly it does not change the
inference here). Secondly, several eﬃciencies can be obtained by setting constants,
but the approach here makes the algorithm more transparent. Note also that the NA
placeholder is irrelevant since there is no conditioning on this value at the starting
point.
The algorithm is run for only 20 steps to produce the output given in Table 9.4. It is
clear that the algorithm converges to a stable value after ten iterations, to the point
[0.7460317, −119.1674].
Figure 9.7 shows how the EM algorithm works in this example.
We begin right
after the starting point where θ values are indicated on the x-axis and the associated
likelihood value designated by the ﬁrst point on the lower curve. So the y-axis indicates
increasing (conditional) values of the likelihood as the algorithm iterates (the scale
is left oﬀsince it only matters in the relative sense). From there we condition on
the observed data in the E-Step and move up the ﬁrst vertical line on the right-hand
side to the likelihood value that corresponds to the starting point for θ and the ﬁrst
conditional. Thus the E-Step moves up the vertical line to an improved estimate of
the value of the likelihood function. Now we calculate the maximum likelihood value

Monte Carlo and Related Iterative Methods
313
0.747
0.75
0.762
0.803
[0.746032:−119.167426]
M−Step
E−Step
FIGURE 9.7: EM Demonstration: U.S. Energy Consumption
for θ to produce θ(1), which moves us across the ﬁrst horizontal line on the right-hand
side to the ﬁrst complete point. This process continues until the steps converge in the
upper-left corner. This ﬁgure is quite intuitively informative because it shows how the
upper line, connecting the E-Step points, converges with the lower line, connecting
the M-Step points, as the number of iterations converges. Note from the “rug” along
the x-axis on that the steps get progressively smaller as the algorithm converges.
9.7.3
Extensions to the EM Algorithm
In cases where the GEM E-Step is particularly diﬃcult or time-consuming, it can be
simulated without too much trouble by sampling M realizations from the distribution Xm ∼
f(Xmis|Xobs, θ(k)) and calculating:
Q(θ(k+1)|θ(k)) = 1
M
M

m=1
ℓ(θ|Xobs, Xm),
(9.56)
a substitute step that uses complete data conditional maximum likelihood estimation (CM),
where the conditionality is over some convenient function of the parameter estimates (Mc-
Culloch 1994). This is the Monte Carlo EM algorithm (MCEM), which is intended to further

314
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
replace analytical work with simulation (Booth and Hobert 1999; Chan and Ledolter 1995;
Guo and Thompson 1991, 1994; Wei and Tanner 1990). Since the Monte Carlo step is
performed for each iteration of the EM algorithm, obviously it is important to determine a
value of M that represents a good compromise between eﬃciency and accuracy. As the EM
algorithm nears the mode (or if it is started near the mode), it is less important to have a
large value for M.
Celeux and Diebolt 1985 introduce Stochastic EM (SEM), which incorporates a stochas-
tic step to simulate a realization of the missing data from its calculated predictive density.
This predictive density then is updated by maximizing the log likelihood function of the
complete data in the standard EM fashion. Further implementation details are given in
Diebolt and Ip (1996), and Celeux, Chauveau, and Diebolt (1996). Also, Dias and Wedel
(2004) compare SEM to related approaches.
Another variant of the EM algorithm is the expectation conditional maximization (ECM)
form (Meng and Rubin 1993; Meng 1994b), which deals with a diﬃcult M-Step by replacing
the standard approach with a sequence of conditional maximizations of Q(θ(k+1)|θ(k)). The
basic idea is to hold some convenient function of all the parameters constant except one and
then perform the M-Step for this parameter only thus conditioning on the others. Then
this is repeated for every other parameter value. The result is a GEM process very much
like the idea of Gibbs sampling MCMC (discussed in Chapter 10).
The ECM steps are summarized as follows:
1. Segment the vector θ into S subvectors: {θ1, θ2, . . . , θS}.
2.
(a) Strategy A: select one subvector, θs, and maximize, holding all others constant
at previous step levels. Cycle through all subvectors.
(b) Strategy B: select one subvector, θs, to hold constant at previous step levels and
maximize all others. Cycle through subvectors.
There are many possible extensions, and some restrictions apply for particular problems
(Meng and Rubin 1993).
The primary motivation is that the resulting algorithm can
be much faster than EM or ECM (Kowalski et al. 1997; Liu and Rubin 1998; Mkhadri
1998). One interesting variant is Liu and Rubin’s (1994, 1995, 1998) ECME (Expecta-
tion/Conditional Maximization Either), which promises faster convergence than regular
ECM. In this procedure, a subset of the CM steps are replaced with maximizing rather in-
creasing processes for the constrained likelihood function implied by holding other θ values
constant.
Rubin (1991) suggests that data augmentation can be thought of as a combination of
EM and the multiple imputation procedure for handling missing data. Missing data values
can be eﬀectively addressed with the multiple imputation procedure (Little and Rubin 1983,
2002, Rubin 1987b), rather than being ignored in the modeling process. Essentially, this
procedure creates a posterior distribution for the missing data conditional on the observed
data, and draws randomly from this distribution to create multiple replications (normally

Monte Carlo and Related Iterative Methods
315
5-10) of the original dataset. The speciﬁed model is then run on each of these replicated full
datasets and the results are averaged (with a slight ANOVA adjustment for the standard
errors of the point estimates). This is a very simple and very common process at this point,
although one with minor warnings (see Zhou and Reiter 2010). Data augmentation (details
in Chapter 10) uses an added variable or variables in such a way that the estimation or opti-
mization process is simpliﬁed rather than complicated. Consider the following modiﬁcation
of the EM steps:
▷E-Step: take m draws of Xmis from the current form of f(Xmis|Xobs, θ(k)) to produce
X(k)
mis (k = 1, 2, . . ., m), or the current posterior predictive distribution in the fully
Bayesian sense.
▷M-Step: for each X(k)
mis draw a θ value from ℓ(θ|Xobs, Xmis), and average as in
multiple imputation.
Notice that this breaks up the two components of the function, Q(θ(k+1)|θ(k)). This is a
data augmentation in the sense that Xmis need not be missing data in the strict sense,
but can be any data that makes the M-Step easier. Rubin also notes that if m = 1, this
algorithm is a Gibbs sampler (Chapter 10).
9.7.4
Additional Comments on EM
This section presented the theory and examples of the EM algorithm.
Clearly two
detailed cases cannot be fully representative of the applicability of the technique. Very
basic explications include Greene (2011), and Jackman (2000) for latent variable models,
the original DLR-77 piece, and the survey by Rubin (1991). Recently Hill and Kriesi (2001)
have shown how EM can help estimate a complicated survey model speciﬁcation in political
science.
It is important to remember that EM does not simply provide joint maximization of
the parameters and missing data together. Instead the two unknowns are treated diﬀer-
ently since we integrate over the missing data and then calculate the maximum likelihood
estimates of the unknown parameters. Treating all the unknown information, data and
parameters the same is problematic since they do not share all the desirable properties of
proper maximum likelihood estimation and because it is not feasible when the proportion
of missing data is nontrivial (Little and Rubin 1983).
The EM algorithm is a mode ﬁnding algorithm and as developed here, does not provide
the curvature around the functional mode as a means of producing variance-covariance
estimates. A substantial literature has emerged to address this deﬁciency, including: Louis
(1982), Meng and Rubin (1991), and the text by McLachlan and Krishnan (1997).
EM is easily one of the most popular statistical techniques for mode-ﬁnding and dealing
with missing data. For instance, Meng and Pedlow (1992) found over 1,000 applications in
the ﬁrst 15 years after the publication of DLR-77, spread over a wide range of ﬁelds and
subﬁelds. While the bulk of EM citations are in applied work (and thus a testament to

316
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
its status as a useful and widely accepted procedure), there exists ongoing and productive
theoretical work. Heyde and Morton (1996) adapt the EM algorithm to instances where an
explicit form of the likelihood function is not available using quasi-likelihood and projection
methods. Laird and Louis (1982) tie EM estimation more tightly into the Bayesian world by
giving a very helpful setup for calculating posteriors from conjugate priors with incomplete
data sampling where this “incompleteness” comes very generally from missing values or
censored, truncated, grouped, missing, or mixture data.
Often the EM algorithm is lumped together with MCMC (Chapter 10), but it is not
an MCMC technique because while it does have serial nature, it does not sample on each
iteration: each step is a deterministic decision given the present state: maximization not
simulation. However, if the maximization step in EM is replaced by a simulation from the
distribution identiﬁed by the E-Step, then EM is a data augmentation application of the
MCMC Gibbs sampler (Robert and Casella 2004; Tanner 1996, Chapter 5). The primary
linkage between EM and MCMC lies in the fact that EM is an excellent precursor to
the MCMC techniques where it is important to start a chain near the high-density region
(Gelman and Rubin 1992a).
9.7.5
EM for Exponential Families
Expressing the EM algorithm for problems based on exponential family forms is partic-
ularly straightforward due to the isolation of the suﬃcient statistic in parametric form. In
practice the EM algorithm performs exceedingly well on exponential family forms due to
the consistency, and asymptotic normality maximum likelihood estimates (Berk 1972).
Start with a one-parameter complete data conditional exponential family PMF or PDF
for the random variable X in:
f(x|ζ) = exp
	
t(x)u(ζ)

r(x)s(ζ),
(9.57)
where: r and t are real-valued functions of z that do not depend on ζ, and s and u are
real-valued functions of ζ that do not depend on x, and r(x) > 0, s(θ) > 0 ∀x, ζ. The log
likelihood of independent, identically distributed (iid) X
X
X = {X1, X2, . . . , Xn} is therefore
in canonical form (Gill 2000, Section 2):
ℓ(θ|x) = t(x)θ −s′(θ) + r′(x).
(9.58)
Here t(x) is the complete data suﬃcient statistic, which we can enumerate for the jth EM
algorithm iteration: t(x)(j). This notational approach greatly simpliﬁes the EM algorithm
where xobs is the non-missing data and θ(j) is the current coeﬃcient estimate:
▷[E-Step:] calculate t(x)(j+1) = Ex[t(x)(j)|xobs, θ(j)], to update the suﬃcient statistic.
▷[M-Step:] solve for θ(j+1) using the ﬁrst two components in the right-hand-side of
the likelihood equation in (9.58), plugging in t(x)(j+1) for t(x).
Once the suﬃcient statistic is identiﬁed (a trivial exercise with exponential family forms),
there is very little analytical work to be done here.

Monte Carlo and Related Iterative Methods
317
■Example 9.8:
EM Estimation for a Mixture Model in Demography.
The example is
a demography application from Li and Choe (1997) analyzing second births in China.
The 1979 Chinese law allows families that accept the one-child certiﬁcate to get ex-
tra beneﬁts from the government, including: cash, housing preferences, healthcare
services, and school preferences. However, families that break the one-child certiﬁ-
cate face penalties, except under special (usually tragic) circumstances.
How can
researchers determine the eﬀectiveness of the law? Speciﬁcally, what other general
covariates lead to a second child decision? Li and Choe attempt to answer these ques-
tions using Chinese government survey data from the Han province, 1978 to 1982, of
women who had one child. It is important now to note that this law is evolving in
China.
The model speciﬁcation is a mixture of logistic (did a second baby occur) and hazard
(how long between ﬁrst and second) components. Here Y is a dichotomous variable
indicating whether a woman eventually has a second child, which is only partly ob-
served since the second child could occur after the period of the study (1982). Thus
this is a (censored) missing data problem and appropriate for EM. The general idea is
to add a weighting to the likelihood function where the weight is 1 for women observed
to have a second child and a predictive weight (the probability of ever having a second
child given levels of covariates) for those still with one child at the end of the survey.
Model 1 is given by:
p(Y |Xβ) = [1 + exp(−Xβ)]−1
with covariates and coeﬃcients, X, aﬀecting the eventual occurrence of a second birth.
Deﬁne T as the random variable for the interval between births for women having the
second child, with realization t. The general equation for those not having a second
child by time period t:
S(t) = p(later than t) + p(never) = pS1(t) + (1 −p),
where S1(t) is the probability of not having the second child in this period conditional
on eventually having a second child, and p is the probability of ever having a second
child.
Model 2 is speciﬁed by f(T |ZG) with covariates and coeﬃcients aﬀecting the interval
between births, giving. . .
the survivor function:
the hazard rate:
S(t|ZG) = 1 −F(t) = p(T ≥t)
h(t|ZG) = f(t|ZG)/S(t|ZG).
Note that the hazard rate is zero for women who will never have a second child and
thus not included in Model 2. This is therefore unlike similar hazard rate models
where everyone eventually has the event (i.e., death). The “non-parametric piecewise
proportional hazards” model (see Allison 1982) divides up the period after the ﬁrst

318
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
birth into k segments (months):
ξ1 = [9:20] < ξ2 = [21:32] < ξ3 = [33:44] < ξ4 = [45:56] . . .
. . . < ξ5 = [57:68] < ξ6 = [69:∞]
and assumes constant baseline hazard rates within each segment,
{λ1, λ2, λ3, λ4, λ5, λ6},
taken from actuarial tables, and proportional changes in these hazards determined by
covariates. This is done to be “simple” in that:
▷no parametric assumptions about the shape of the survival function are required
since it is now a series of steps.
▷nonparametric ﬂexibility can be achieved by altering the number of periods.
▷there are fewer extra parameters to estimate relative to competitors (Cox PH,
etc.).
All these assumptions provide the hazard rate for the jth period:
h(t|ZG) = λj exp(ZG), for t ∈ξj,
with covariates and coeﬃcients aﬀecting the duration T . Thus negative values of G
lengthen T by lowering the hazard rate, and positive values of G shorten T by raising
the hazard rate. Note: some authors/literatures specify exp(−ZG), just switching
around the above statement.
Modeling only the observed Yi induces obvious bias, so introduce δi as a dichotomous
variable indicating that the ith woman had the second child before the end of the
study (δi = 0), or not (δi = 1). Note that δi is fully observed in the data. Specify
μi as the expectation (probability, as in E-Step) of Yi = 1 in period j, including after
the survey (j = 6):
μi = 1,
if δi = 0
μi = p(second baby)p(no second baby before time t)/
p(second baby)p(no second baby before time t)
+ p(never has second baby)
=
p(Y |Xiβ)S(t|ZiG)
p(Y |Xiβ)S(t|ZiG) + (1 −p(Y |Xiβ))(1),
if δi = 1.
The EM steps for Q proceed as follows:
Q =

sample

periods
'
(1 −δi) × log h(had second baby before period j)
+(weighting) × log p(has second baby at period j)
+(1 −weighting) × log p(never has second baby)
(
.

Monte Carlo and Related Iterative Methods
319
So within each period:
Q =
n

i=1
[(1 −δi) log(h(t|ZiG))
+μi log(p(Y |Xi|β)S(t|ZiG)) + (1 −μi) log(1 −p(Y |Xiβ))]
=
n

i=1
[μi log(p(Y |Xiβ) + (1 −μi) log(1 −p(Y |Xiβ))]
 
!"
#
Qβ
+
n

i=1
[(1 −δi) log(h(t|ZiG)) + μi log(S(t|ZiG))]
 
!"
#
QG
where the last line shows that we can separate out β and G maximum likelihood
procedures.
The likelihood function intuition on the observed second child event
(δi = 0, μi = 1) for a single individual (i) at time t, exp(Qi) reduces to:
exp(Qi) = h(t|ZiG)[p(Y |Xiβ)S(t|ZiG)]
= f(t|ZiG)
S(t|ZiG)[p(Y |Xiβ)S(t|ZiG)]
= p(Y |Xiβ)f(t|ZiG)
The likelihood function intuition on unobserved second child event (δi = 1, μi ̸= 1)
for a single individual at time t, exp(Qi) reduces similarly to:
exp(Qi) = [p(Y |Xiβ)S(t|ZiG)]μi[1 −p(Y |Xiβ)]1−μi
which is an intuitive looking proportional weighting. Thus our mechanical steps are:
1. Pick MLE values using observed Y for β(0) and G(0).
2. kth E-Step:
(a) Compute μi for all cases where δi = 1 using the current values: β(k) and
G(k).
(b) Plug these values into Q(k).
3. kth M-Step:
(a) Obtain MLE of β using Q(k)
β , assign to β(k+1).
(b) Obtain MLE of G using Q(k)
G , assign to G(k+1).
4. Repeat until ||β(k∗+1) −β(k∗)|| < ϵ and ||G(k∗+1) −G(k∗)|| < ϵ.
The results can be stated in typical tabular form for regression models. Here the
reference subject categories are: reside in what environment, education below junior
high, ﬁrst child’s gender, and age when ﬁrst child is born. Table 9.5 shows the values.
Here the reference category on residing is the countryside.

320
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
TABLE 9.5:
Model Results, More Developed Areas
Covariates
Coeﬃcient
Std. Error
Coeﬃcient
Std. Error
Accepted Certificate
-2.322
0.313
-1.703
0.191
Reside City
-3.566
0.286
-0.021
0.220
Reside Town
-0.453
0.241
0.058
0.220
Low Education
0.825
0.373
0.002
0.272
First Child Female
1.205
0.211
0.125
0.095
Age at First Born
-0.236
0.095
-0.051
0.046
Age-Squared at First Born
-0.095
0.042
0.068
0.023
9.8
Survey of Random Number Generation
The most important simulation topic not detailed so far is the generation of random
variables from various parametric forms. Many of the procedures in this chapter and in
Chapter 10 depend on the reliable generation of values from speciﬁc PDFs and PMFs.
In fact, a wide range of Bayesian and non-Bayesian simulation techniques require random
number generation as a means of creating empirical sequences of interest. With the Gibbs
sampler, we need to sample from full conditional distributions and with the Metropolis-
Hastings algorithm, we need to sample from the jumping distribution. Unreliable production
of random variables will therefore certainly aﬀect the quality of the MCMC estimates.
To see the importance of having an adequate random number, consider the simple
numerical integration described in Section 9.2. If the random numbers, as generated, favored
some region of the random variable support, then it is clear that the resulting integral will be
incorrect. Such inaccuracies could also occur in more complicated Monte Carlo estimation
procedures including MCMC.
General descriptions of the issues involved in random number generation are found in
the now-classic text of Kennedy and Gentle (1980, Chapter 6) along with Gentle’s update
(1998), the book by Devroye (1986), and more theoretical discussions such as Evans and
Swartz (1995), Dieter (1975), Dieter and Ahrens (1971, 1973),
Gentle (1990), Jagerman
(1965), Jansson (1966), and Knuth’s seminal work on the topic (1981, Chapter 3). Also
worth reading is the foundational series of papers by Ahrens and Dieter (1972, 1973, 1974,
1980, 1982), sections of Morgan (1984, Chapters 3 to 6), the recent work of Robert and
Casella (2004, Chapter 2), and the introduction by Ripley (1983).
There are many algorithms for generating diﬀerent forms of random variates and these
are usually cataloged by the associated PDF or PMF. Because of its basic nature and role
in more complex operations, the generation of random uniforms holds special signiﬁcance.
There are a great many stochastic simulation techniques that require uniform variates as a
means of equitably comparing multidimensional regions in a parameter space. In addition,

Monte Carlo and Related Iterative Methods
321
nearly all other distributional forms can be derived by transformations or manipulations of
the uniform. Consequently, there is a large and well-developed literature on the generating
algorithms for uniform random numbers, including such key work as Falk (1999), MacLaren
and Marsaglia (1965), Marsaglia (1961a), Mason and Lurie (1973), M¨uller (1959b), Walker
(1974), Westlake (1967), and L’Ecuyer (1998).
A number of papers on the more modern, well-known congruential generator method
for producing uniform random variates have been written by Burford (1973, 1975), Chay,
Fardo, and Mazumdar (1975), Dieter (1975), Downham and Roberts (1967), Eichenauer-
Herrmann (1996), Fuller (1976), Golder (1976), Hull and Dobell (1964), Lehmer (1951),
Marsaglia (1968, 1972), Neave (1973), Perkins and Menzefricke (1975), Prentice and Miller
(1968), and Sibuya (1961), among others. Another important algorithm is the shift register
method described by Golomb (1967), Hurd (1974), Liniger (1961), Lewis and Payne (1973),
and Marsaglia (1961b, 1977).
Second only to the uniform is the attention paid to generating normally random vari-
ates. Due mainly to asymptotic properties, a wide range of data-generated distributions of
interest can be modeled with the normal PDF. There is therefore an extensive literature
describing algorithms for generating normal random variates. Important works include Box
and M¨uller’s (1958) famous technique for generating normals from independent uniforms,
along with Best (1979), Brent (1974), Cheng (1985), Gates (1978), Gebhardt (1964), George
(1963), Golder and Settle (1976), Hurst and Knop (1972), Kinderman and Ramage (1976),
Kinderman, Monahan, and Ramage (1975), Marsaglia (1964), Marsaglia and Bray (1964),
Marsaglia, MacLaren, and Bray (1964), M¨uller (1958, 1959a), Niederreiter (1972, 1974,
1976), Payne (1977), and Scheuer and Stoller (1962).
Generating gamma random variables is also a fundamental task since it is a very ﬂexi-
ble family form and the support (zero to positive inﬁnity) makes the gamma a convenient
distribution for modeling variance terms. Some of the often-cited works include Atkinson
(1977), Atkinson and Pearce (1976), Best (1983), Bowman and Beauchamp (1975), Cheng
(1977), Cheng and Feast (1979, 1980), Greenwood (1974), Kinderman and Monahan (1980),
Phillips and Beightler (1972), Tadikamalla (1978), Tadikamalla and Ramberg (1975), Wal-
lace (1974), Wheeler (1974, 1975), as well as Whittaker (1974).
There is an important and growing literature on testing the large number of currently
available random number generators for good properties of randomness as well as compu-
tational eﬃciency: Altman and McDonald (2001, 2003), Atkinson (1980), Butcher (1961),
Coveyou and MacPherson (1967), Downham (1970), Dudewicz (1976), Good (1957), Goren-
stein (1967), Krawczyk (1992), Kronmal (1964), Learmonth and Lewis (1973), Marsaglia
(1968), McArdle (1976), McCullough (1999), McCullough and Wilson (1999), Toothill,
Robinson, and Adams (1971), and Whittlesey (1969). Some of these are a reaction to ob-
served problems in serial correlation in the output stream of these produced values and the
well-established problems with IBM’s long-lived RANDU algorithm as well as other related
techniques: Coveyou (1960, 1970), Fishman and Moore (1982), Hellekalek (1998).
A number of articles give informative histories and overviews of random number gener-

322
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
ation. These include Dudewicz (1975), Halton (1970), Marsaglia (1985), and Mihram and
Mihram (1997). Some early-era bibliographies provide helpful background as well: Nance
and Overstreet (1972), and Sowey (1972, 1978). New work worthy of attention can be found
in Anderson and Louis (1996), Malov (1998), and Jones and Lunn (1996). This remains
an active area of research for two main reasons. Since all “random” numbers generated
are really “pseudo-random” in that they are produced deterministically by computational
processes, there is always the opportunity to improve their random properties. In addition,
as the use of compute-intensive techniques increases (such as Markov chain Monte Carlo),
increases in eﬃciencies in the underlying algorithms becomes more important.
9.9
Concluding Remarks
It is important to remind readers that numerical statistical methods are not the panacea
that some might expect or hope. The tools presented in this chapter, and of course the more
complex simulation techniques to come, are susceptible to all of the problems associated
with the misapplication of statistical tools by incautious researchers. Usually these issues
occur due to misunderstanding the assumptions underlying a model or procedure as it is
applied in a data-analytic setting.
Furthermore, some have suggested that because the
computer does the bulk of the mechanical work with these tools, there is a tendency for
less human consideration of the details and an over-trusting of the results (Cleveland 1993;
Higham 1996; Mooney 1997), and it is not unusual for researchers to ﬁnd such faults in
others’ work (Siegmund 1976, p.679).
Fox (1971), also summarized in Higham (1996, p.35), indicates where researchers can go
awry in numerical computing in the provocatively titled piece: “How to Get Meaningless
Answers in Scientiﬁc Computation and What to Do about It.” His reasons that computed
answers may be useless, all applicable to statistical computing and statistical simulation,
include:
▷The problem might be ill-conditioned or unstable numerically.
▷There might be an expectation that the computer can do more than it is capable of
doing.
▷The appearance of consistency in the simulations may be deceiving.
▷The application may be too specialized and insuﬃciently generalizeable for the re-
search objective.
As we move forward to Bayesian stochastic simulation, these are certainly important caveats
to keep in the back of our minds.

Monte Carlo and Related Iterative Methods
323
9.10
Exercises
9.1
Use the trapezoidal rule with n = 8 to evaluate the deﬁnite integral:
I(g) =
 π
0
ex cos(x)dx.
The correct answer is I(g) = −12.070346; can you explain why your answer diﬀers
slightly?
9.2
The density function in R can be used to get a normalizing constant for non-
standard distributions. Create a mixture distribution of normals with x.vals <-
c(rnorm(100,1,1),rnorm(100,8,2),rnorm(100,15,1)), and then run the den-
sity function changing the smoothing parameter until it has a satisfactory shape:
x.dens <- density(x.vals,n=length(x.vals),adjust=...). Use the returned
objects in x.dens to estimate a normalizing constant.
9.3
Use Simpson’s rule with n = 8 to evaluate the deﬁnite integral in Exercise 9.1. Is
your answer better?
9.4
Normally a histogram takes x1, . . . , xn produced from some distribution f(x) and
bins them over an equally spaced mesh along the x-axis with selected bin width
h. Selection of a starting point for the bin edges is arbitrary but occasionally has
an eﬀect. The average shifted histogram (ASH) smooths out the bins by specifying
multiple adjacent versions that are averaged (Scott 1985) to form a single estimate.
Now specify m histograms instead of one where each of these has bin width h like the
original speciﬁcation now deﬁning a set of bin edges given by {ih/m, i = 0, . . . m}.
This creates a ﬁner mesh of K bins over the range of the data, {t1, . . . , tK}, with
widths h/m. If Ik = [tk−1 : tk) is the kth bin and nk is the number of xi values in
this bin, then the value (height) of the ℓth shifted histogram is:
Hℓ(x) = 1
nh
m−1

j=0
nj+i+m⌊(k−i)/m⌋(x ∈Ik) for ℓ= 0, . . . , (m −1),
which counts cases in the larger bin of width h. The ASH is produced by the point
wise average:
ˆH(x) = 1
m
m−1

ℓ=0
(x ∈Ik) .
Using the recidivism data on page 188 in Chapter 6, code the ASH algorithm in
R and create average shifted histograms separately for the Released and Returned
vectors.
9.5
It is well known that the normal is the limiting distribution of the binomial (i.e.,

324
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
that as the number of binomial experiments gets very large the histogram of the
binomial results increasing looks like a normal). Using R, generate 25 BN(10, 0.5)
experiments.
Treat this as a posterior for p = 0.5 even though you know the
true value and improve your estimate using importance sampling with a normal
approximation density.
9.6
Antithetic variates are on old idea in Monte Carlo simulation (Hammersley and
Morton 1956)
that reduces variance by drawing negatively correlated pairs and
averaging them or using both. Suppose we draw a uniformly between 0 and 1, ui1,
and then create ui2 = 1 −ui1, for i = 1, . . . , m. Show that the mean and variance
of the ui1 and ui2 are the same. Derive the correlation.
9.7
Casella and Berger (2002, p.638) give the famous temperature and failure data on
space shuttle launches before the Challenger disaster, where failure is dichotomized.
Actually several o-ring failure events were multiple events. Treat the number of
multiple events as missing data and write an EM algorithm to estimate the Poisson
parameter given the missing data. Here are the dichotomized failure data in R form:
temp <- c(53,57,58,63,66,67,67,68,69,70,70,70,70,70,72,73,
75,75,76,76,78,79,81),
fail <- c(1,1,1,1,0,0,0,0,0,0,0,0,1,1,0,0,0,1,0,0,0,0,0).
9.8
Generate a homogeneous Poisson process for intensity parameter λ by writing an
R function that performs the following steps:
▷Start with time t1 = 0 and size n = 1
▷sample u ∼U(0, 1)
▷produce tn+1 = tn −1
λ log(u)
▷increment: n < −n + 1,
which continues until the desired sample size is reached. Now modify your func-
tion to create a non-homogeneous Poisson process where the time between events
declines by 1/t.
9.9
Replicate the calculation of the Kullback-Leibler distance between the two beta
distributions in Section 9.3.1 using rejection sampling. In the cultural anthropology
example given in Section 2.3.4 the beta priors BE(15, 2) and BE(1, 1) produced beta
posteriors BE(32, 9) and BE(18, 8) with the beta-binomial speciﬁcation. Are these
two posterior distributions closer or farther apart in Kullback-Leibler distance than
their respective priors? What does this say about the eﬀect of the data in this
example?
9.10
Monahan (2001) discusses a posterior distribution, which can be produced by the
R command:

Monte Carlo and Related Iterative Methods
325
m.llike <- function(t1,t2)
n1*log(2*t1*t2) + n2*log(t1*(2-t1-2*t2))
+ n3*log(t2*(2-t2-2*t1)) + 2*n4*log(t1*(1-t1-t2))
where n1 = 1, n2 = 10, n3 = 4, n4 = 9. Create a contour plot and a perspective
plot over the range [0.01:0.49] in both dimensions (for both variables). Estimate
the mode numerically. Use this value to calculate the Hessian analytically.
9.11
The Cauchy distribution (B) is a unimodal, symmetric density like the normal but
has much heavier tails. In fact the tails are suﬃciently heavy that the Cauchy dis-
tribution does not have ﬁnite moments. To understand this characteristic, perform
the following experiment at least 10 times:
▷Generate a sample of size 100 from C(x|θ, σ), where you choose θ, σ.
▷Perform Monte Carlo integration to attempt to calculate E[X].
▷Compare your estimate with the mode (θ).
What do you conclude about the moments of the Cauchy?
9.12
The Kolmogorov-Smirnov test uses cumulative distribution statistics test the simi-
larity of the empirical distribution of some observed data and a speciﬁed PDF. The
test statistic is created by:
D = max
i=1:n
7 i
n −F(i), F(i) −i −1
n
8
,
where F(i) is the ith ordered value.
Large values indicate dissimilarity and the
rejection of the hypothesis that the empirical distribution matches the queried
theoretical distribution. The p-value is calculated from the Kolmogorov-Smirnoﬀ
CDF:
p(D ≤x)
√
2π
x
∞

i=1
exp
	
−(2i −1)2π2/8x2
,
which generally requires approximation methods (see Marsaglia, Tsang, and Wang
2003). This so-called nonparametric test (this label comes from the fact that the
distribution of the test statistic does not depend on the distribution of the data
being tested) performs poorly in small samples, but works well in a simulation envi-
ronment. Write an R function that implements this test where the reference distri-
bution is normal. Using R generate 1,000 Cauchy random variables (rcauchy(1000,
location = 0, scale = 1)) and perform the test.
9.13
Generate 100 standard normal random variables in R and use these values to perform
rejection sampling to calculate the integral of the normal PDF from 2 to ∞. Repeat
this experiment 100 times and calculate the empirical mean and variance from your
replications. Now generate 10,000 Cauchy values and use this as an approximation

326
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
distribution in importance sampling to obtain an estimate of the same normal
interval. Which approach is more accurate?
9.14
Write a function in R to evaluate the deﬁnite integral:
I(g) =
 π
0
excos(x)dx.
Use Simpson’s rule with n = 8. Give an answer with 6 digits of accuracy.
9.15
The times to failure of two groups are observed, which are assumed to be dis-
tributed exponential with unknown θ.
The times to failure for the ﬁrst group,
X1, X2, . . . , Xm, are fully observed, but the second group is censored at time t > 0
because the study grant ran out, therefore the Y1, Y2, . . . , Yn values are either t or
some value less than t, which can be given an indicator function: Oi = 1 if Yi = t
and zero otherwise. The expected value of the suﬃcient statistic is:
E
 n

i=1
(Yi|Oi)

=
* n

i=1
Oi
,
(t + θ−1)
+
*
n −
n

i=1
Oi
,
(θ−1 −t[1 + exp(t/θ)]−1),
and the complete data log likelihood is:
ℓ(θ) = −
m

i=1
(log(θ−1) + xiθ) −
n

i=1
(log(θ−1) + yiθ).
Substitute the value of the suﬃcient statistic into the log likelihood to get the
Q(θ(k+1)|θ(k)) expression and run the EM algorithm for the following data:
Xi
1
9
4
3
11
7
2
2
-
-
Yi
2
3
2
4
4
2
4
4
3
1
9.16
Suppose a mixture of exponential distributions is expressed as:
fj(x|θj) = θj exp(−θjy), j = 1, . . . J.
Each θj is given the a conjugate prior GA(aj, bj). Now include zij as an indicator
variable for case i belonging to mixture j and z(k)
ij
as the probability that case
i belongs to mixture j after the kth step of the EM algorithm.
If n(k)
j
is the
number of cases assigned to mixture j at the kth step, the M-Step is given by
υ(k+1)
j
= −(n
i=1 z(k)
ij + bj)/(n(k)
j
+ aj −1), where the θ(k+1)
j
= −

υ(k+1)
j
−1
.
9.17
One of the easier, but very slow, ways to generate beta random variables is J¨ohnk’s
method (J¨ohnk 1964; Kennedy and Gentle 1980), based on the fact that order
statistics from iid random uniforms are distributed beta. The algorithm to deliver
a single BE(A, B) random variable is given by:

Monte Carlo and Related Iterative Methods
327
(a) Generate independent:
u1 ∼U(0, 1),
u2 ∼U(0, 1)
(b) Transform:
v1 = (u1)1/A,
v2 = (u2)1/B
(c) Calculate the sum:
w = v1 + v2
and return v1/w if w ≤1, otherwise return to step 1.
Code this algorithm in R and produce 100 random variables each according to:
BE(15, 2), BE(1, 1), BE(2, 9), and BE(8, 8). For each of these beta distributions,
also produce 100 random variables with the rbeta function in R and compare with
the corresponding forms produced with J¨ohnk’s method using qqplot. Graph these
four comparisons in the same window by setting up the plot with the windowing
command: par(mfrow=c(2,2)).
9.18
Consider the following example from Wu (1983) using bivariate mean-zero data
from Murray (1977) in his discussion of DLR-77:
Variable 1:
1
1
-1
-1
2
2
-2
-2
NA
NA
NA
NA
Variable 2:
1
-1
1
-1
NA
NA
NA
NA
2
2
-2
-2
where NA denotes a missing value. Show with an EM algorithm starting at diﬀerent
points that the global maxima is at ρ = ±0.5, σ2
1 = σ2
2. What happens when the
EM algorithm is started at ρ = 0? What happens when the EM algorithm includes
code that bounds ρ away from zero?
9.19
Hartley (1958, p.182) ﬁts a Poisson model to the following “drastically censored”
grouped data:
Number of Events
0
1
2
3-10
Total
Group Frequency
11
37
64
128
240
Develop an EM algorithm to estimate the Poisson intensity parameter.
9.20
Efron (1979) and Efron and Tibshirani (1993) introduced the bootstrap as a compute-
intensive procedure to get the sampling properties of an estimator when there is not
a known closed-form analytical form. It is similar to Monte Carlo simulation in that
repeated computation provides information in the absence of direct calculation. For
some statistic of interest, θ, and dataset x, the steps are:

328
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
(a) Draw B “bootstrap” samples of size n (old advice: 25-200; new advice: thou-
sands) independently, with replacement from the sample x of size n:
x∗1, x∗2, . . . , x∗B
(note the notation to diﬀerentiate the bootstrap sample from the original sam-
ple).
(b) Calculate the sample statistic of interest, θ∗b for each bootstrap sample, and
the mean of these statistics:
¯θ∗= 1
B
B

b=1
θ∗b
(c) Estimate the bootstrap standard error of the statistic by:
Var(θ) =
1
B −1
B

b=1

θ∗b −¯θ∗2
where SE(θ) =

Var(θ).
The limit of this standard error as B goes to inﬁnity is called the ideal bootstrap
estimate, and this procedure is called the nonparametric bootstrap estimate. Con-
sider two variables on sub-Saharan African countries: size of military (in thousands)
and a dichotomous outcome indicating that there was a military coup during this
period:
Coup
94
197
16
38
99
141
23
No Coup
52
104
146
10
50
31
40
27
46
Note that these data are imbalanced. The question is whether there is a diﬀer-
ence by size of military. Calculate the median of each group and its bootstrapped
standard error.
9.11
Computational Addendum: R Code for Importance Sampling
This is a very simple importance sampling routine that provides a normal or Student’s-t
approximation function. The procedure is to draw a random sample from the multivariate
normal or Student’s-t PDF with mean equal to the MLE estimates from the input log-
likelihood function, and the standard error equal to the square root of the diagonal of the
inverse of the variance-covariance matrix. The function sir performs importance sampling
with a normal approximation as a default, but one can easily change the speciﬁcation by
specifying a degrees of freedom parameter other than zero. It is important to note that this
really implements Rubin’s SIR since that way there is an empirical maximum to divide by.

Monte Carlo and Related Iterative Methods
329
# Call:
# sir(data.mat,
data matrix organized in columns
#
theta.vector, the initial coefficient estimates
#
theta.matrix, the initial vc matrix
#
M,
the number of draws
#
m,
the desired number of accepted values
#
tol,
the rounding/truncating tolerance
#
ll.func,
loglike function for empirical posterior
#
df)
the df for using the t distribution as the
#
approx distribution, default=0 for gaussian.
sir <- function(data.mat,theta.vector,theta.matrix,M,m,
tol=1e-06,ll.func,df=0)
{
importance.ratio <- rep(NA,M)
rand.draw <- mvrnorm(M,theta.vector,theta.matrix,tol=1e-04)
if (df > 0)
rand.draw <- rand.draw/(sqrt(rchisq(M,df)/df))
empirical.draw.vector<-apply(rand.draw,1,ll.func,data.mat)
if (sum(is.na(empirical.draw.vector)) == 0)
{
print("SIR: finished generating from posterior")
empirical.draw.vector <- 1000*empirical.draw.vector
print(summary(empirical.draw.vector))
}
else {
print(paste("SIR: found",
sum(is.na(empirical.draw.vector)),
"NA(s) in generating from posterior density
function, quitting"))
return()
}
if (df == 0)
{
normal.draw.vector
<- apply(rand.draw,1,normal.posterior.ll,data.mat)
}
else {
theta.matrix <- (df)/((df-2)*theta.matrix
normal.draw.vector
<- apply(rand.draw,1,t.posterior.ll,data.mat,df)
}
if (sum(is.na(normal.draw.vector)) == 0)
{
print("SIR: finished generating from approximation
distribution")
print(summary(normal.draw.vector))
}
else {
print(paste("SIR: found",

330
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
sum(is.na(normal.draw.vector)),
"NA(s) in generating from approximation
distribution, quitting"))
return()
}
importance.ratio<-(empirical.draw.vector-normal.draw.vector)
importance.ratio<-importance.ratio/max(importance.ratio)
if (sum(is.na(importance.ratio)) == 0)
{
print("SIR: finished calculating importance weights")
print(summary(importance.ratio))
}
else
{
print(paste("SIR: found",sum(is.na(importance.ratio)),
"NA(s) in calculating importance weights, quitting"))
return()
}
accepted.mat <- rand.draw[1:2,]
while(nrow(accepted.mat) < m+2)
{
rand.unif <- runif(length(importance.ratio))
accepted.loc <-
seq(along=importance.ratio)[(rand.unif-tol)
<=importance.ratio]
rejected.loc <-
seq(along=importance.ratio)[(rand.unif-tol)
>importance.ratio]
accepted.mat<-rbind(accepted.mat,rand.draw[accepted.loc,])
rand.draw <- rand.draw[rejected.loc,]
importance.ratio <- importance.ratio[rejected.loc]
print(paste("SIR: cycle complete,",
(nrow(accepted.mat)-2),"now accepted"))
}
accepted.mat[3:nrow(accepted.mat),]
}
The following are log likelihood functions that can be plugged into the sir function above.
logit.posterior.ll <- function(theta.vector,X)
{
Y <- X[,1]
X[,1] <- rep(1,nrow(X))
sum( -log(1+exp(-X%*%theta.vector))*Y
-log(1+exp(X%*%theta.vector))*(1-Y) )
}
normal.posterior.ll <- function(coef.vector,X)
{
dimnames(coef.vector) <- NULL
Y <- X[,1]
X[,1] <- rep(1,nrow(X))

Monte Carlo and Related Iterative Methods
331
e <- Y - X%*%solve(t(X)%*%X)%*%t(X)%*%Y
sigma <- var(e)
return(-nrow(X)*(1/2)*log(2*pi)
-nrow(X)*(1/2)*log(sigma)
-(1/(2*sigma))*(t(Y-X%*%coef.vector)%*%
(Y-X%*%coef.vector)) )
}
t.posterior.ll <- function(coef.vector,X,df)
{
Y <- X[,1]
X[,1] <- rep(1,nrow(X))
e <- Y - X%*%solve(t(X)%*%X)%*%t(X)%*%Y
sigma <- var(e)*(df-2)/(df)
d <- length(coef.vector)
return(log(gamma((df+d)/2)) - log(gamma(df/2))
- (d/2)*log(df)
-(d/2)*log(pi) - 0.5*(log(sigma))
-((df+d)/2*sigma)*log(1+(1/df)*
(t(Y-X%*%coef.vector)%*%
(Y-X%*%coef.vector)) ))
}
probit.posterior.ll <- function (theta.vector,X,tol = 1e-05)
{
Y <- X[,1]
X[,1] <- rep(1,nrow(X))
Xb <- X%*%theta.vector
h <- pnorm(Xb)
h[h<tol] <- tol
g <- 1-pnorm(Xb)
g[g<tol] <- tol
sum( log(h)*Y + log(g)*(1-Y) )
}


Chapter 10
Basics of Markov Chain Monte Carlo
10.1
Who Is Markov and What Is He Doing with Chains?
The use of Markov chain Monte Carlo (MCMC) methods to evaluate integral quantities
has exploded over the last two decades. Beginning with the seminal review paper by Gelfand
and Smith (1990), the rate of publication of MCMC works has grown exponentially. While
this is relatively a recent development, the genesis dates back to two important works: the
1953 essay by Metropolis, Rosenbluth, Rosenbluth, Teller, and Teller, as well as Geman
and Geman’s (1984) introduction of the Gibbs sampler as a method for obtaining diﬃcult
posterior quantities in the process of image restoration. The lack of early recognition of
the importance of the 1953 contribution is a testament to the barriers that may exist
between statistical physics and other ﬁelds, and the hindsight that suﬃciently powerful
computational resources were not widely available until sometime afterwords. This history
is nicely reviewed by Robert and Casella (2011), as well as by Richey (2010). Another
fundamental tool in this family is simulated annealing (Kirkpatrick, Gelatt, and Vecchi
[1983], ˇCern´y [1985]), which is described at the begining of Chapter 15.
This chapter introduces the basic ideas behind MCMC methods with the goal of pro-
viding accessible introductions, whereas Chapter 13 covers more technical issues and Chap-
ter 14 discusses some practical guidance on running MCMC simulations. In Chapter 15
we look at extensions and enhancements to the standard MCMC algorithms. The primary
distinction made in this chapter is between standard Monte Carlo simulation methods, as
covered in Chapter 9, and the Markov chain type of Monte Carlo methods characterized
by a dependence structure between consecutive simulated values. Standard Monte Carlo
methods produce a set of independent simulated values according to some desired proba-
bility distribution. MCMC methods produce chains in which each of the simulated values
is mildly dependent on the preceding value. The basic principle is that once this chain has
run long enough it will ﬁnd its way to the desired posterior distribution of interest and
we can summarize this distribution by letting the chain wander around, thus producing
summary statistics from recorded values. The “magic” that occurs is that a process based
on mechanically producing serially correlated values from joint or conditional distributions
eventually gives values that can be treated as independent draws from marginals.
333

334
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
10.1.1
What Is a Markov Chain?
Chapter 9 paid extensive attention to the second “MC” in “MCMC” and we have yet
to provide a precise deﬁnition for the ﬁrst “MC.” The initial deﬁnition required is that of a
more primitive concept that underlies Markov chains. A stochastic process is a consecutive
set of random quantities deﬁned on some known state space, Θ, indexed so that the order
is known: {θ[t]:t ∈T }. Here the state space (which we can also refer to as the parameter
space when directly referring to the support of a parameter vector of interest) is just the
allowable range of values for the random vector of interest. This will be more precisely
deﬁned in Chapter 13. Frequently, but not necessarily, T is the set of positive integers
implying consecutive, even-spaced time intervals: {θ[t=0], θ[t=1], θ[t=2], . . .}. With MCMC
we are concerned only with this restricted type of stochastic process.
The state space, Θ, is either discrete or continuous depending on how the variable of
interest is measured, but the implications for our purposes apply more to notation than
to fundamental theory. Standard references on stochastic processes include Doob (1990),
Hoel, Port, and Stone (1987), Karlin and Taylor (1981, 1990), and Ross (1996).
A Markov chain is a stochastic process with the property that at time t in the series,
the probability of making a transition to any new state is dependent only on the current
state of the process, θ[t], and is therefore conditionally independent of the previous values:
θ[0], θ[1], . . . , θ[t−1]. This is stated more formally:
p(θ[t] ∈A|θ[0], θ[1], . . . , θ[t−2], θ[t−1]) = p(θ[t] ∈A|θ[t−1]),
(10.1)
where A is any identiﬁed set (an event or range of events) on the complete state space.
So a Markov chain wanders around the state space “remembering” only where it has been
in the last period. This property turns out to be enormously useful in generating samples
from desired limiting distributions of the chain because when the chain eventually ﬁnds
the region of the state space with the highest density, it will produce a sample from this
distribution that is only mildly nonindependent. These are the sample values that we will
then use to describe the posterior distribution of interest.
A fundamental concern is the transition process that deﬁnes the probabilities of moving
to other points in the state space, given the current location of the chain.
The most
convenient way to think about this structure is to deﬁne the transition kernel, K, as a
general mechanism for describing the probability of moving to some other speciﬁed state
based on the current chain status (Robert and Casella 2004, p.208). The advantage of this
notation is that it subsumes both the continuous state space case as well as the discrete
state space case. It is required that K(θ, A) be a deﬁned probability measure for all θ points
in the state space to the set A ∈Θ. Thus the function K(θ, A) maps potential transition
events to their probability of occurrence.
When the state space is discrete, then K is a matrix mapping, k × k for k discrete
elements in A, where each cell deﬁnes the probability of a state transition from the ﬁrst

Basics of Markov Chain Monte Carlo
335
term in the parentheses to all possible states:
PA =
⎡
⎢⎣
p(θ1, θ1)
. . .
p(θ1, θk)
:
:
p(θk, θ1)
. . .
p(θk, θk)
⎤
⎥⎦,
(10.2)
where the row indicates where the chain is at this period and the column indicates where the
chain is going in the next period. The rows of PA sum to one and deﬁne a conditional PMF
since they are all speciﬁed for the same starting value and cover each possible destination
in the state space: for row i: k
j=1 p(θi, θj) = 1. Each matrix element is a well-behaved
probability, p(θi, θj) ≥0, ∀i, j ∈A.
When the state space is continuous, then K is a
conditional PDF: f(θ|θi), meaning a properly deﬁned probability statement for all θ ∈A,
given some current state θi.
An important feature of the transition kernel is that transition probabilities between
two selected states for arbitrary numbers of steps m can be calculated multiplicatively. For
instance, with a discrete state space the probability of transitioning from the state θi = x
at time 0 to the state θj = y in exactly m steps is given by the multiplicative series:
pm(θ[m]
j
= y|θ[0]
i
= x) =

θ1

θ2
· · ·

θm−1
 
!"
#
all possible paths
p(θi, θ1)p(θ1, θ2) · · · p(θm−1, θj)
 
!"
#
transition products
.
(10.3)
So pm(θ[m]
j
= y|θ[0]
i
= x) is also a stochastic transition matrix, and this property holds for
all discrete chains exactly as given, and for continuous Markov chains with only a slight
modiﬁcation involving integrals rather than summations. The basic idea of (10.3) is that the
complete probability of transitioning from x to y is a product of all the required intermediate
steps where we sum over all possible paths that reach y from x.
10.1.2
A Markov Chain Illustration
Start with a two-dimensional discrete state space, which can be thought of as discrete
vote choice between two political parties, a commercial purchase decision between two
brands, or some other choice. Suppose that voters/consumers who normally select θ1 have
an 80% chance of continuing to do so, and voters/consumers who normally select θ2 have
only a 40% chance of continuing to do so. Since there are only two choices, this leads to
the transition matrix P:
next period
"
# 
!
θ1
θ2
current period
⎧
⎨
⎩
θ1
θ2
⎡
⎣0.8
0.2
0.6
0.4
⎤
⎦.
All Markov chains begin with a starting point assigned by the researcher. This two-
dimensional initial state deﬁnes the proportion of individuals selecting θ1 and θ2 before

336
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
beginning the chain. For the purposes of this example, assign the starting point:
S0 =

0.5
0.5

;
that is, before running the Markov chain, 50% of the population select each alternative. To
get to the ﬁrst state, we simply multiply the initial state by the transition matrix:
S1 =

0.5
0.5
 
0.8
0.2
0.6
0.4

=

0.7
0.3

= S1.
So after the ﬁrst iteration we have the new proportions: 70% select θ1 and 30% select θ2.
This process continues multiplicatively as long as we like:
Second state:
S2 =

0.7
0.3
 
0.8
0.2
0.6
0.4

=

0.74
0.26

Third state:
S3 =

0.74
0.26
 
0.8
0.2
0.6
0.4

=

0.748
0.252

Fourth state:
S4 =

0.748
0.252
 
0.8
0.2
0.6
0.4

=

0.7496
0.2504

.
As one might guess, the choice proportions are converging to [0.75, 0.25]. This is because
the transition matrix is pushing toward a steady state or more appropriately “stationary”
distribution of the proportions. So when we reach this distribution all future states, S, are
constant: SP = S.
Imagine that this stationary distribution was the articulation of some PMF or PDF that
we could not analytically describe but would like to. If we could run this Markov chain
suﬃciently long we would eventually get the stationary distribution for any point in the
state space. In fact, for this simple example we could solve directly for the steady state
S = [s1, s2] by stipulating:

s1
s2
 
0.8
0.2
0.6
0.4

=

s1
s2

,
and solving the resulting two equations for the two unknowns (using necessarily s1+s2 = 1).
While this example is wildly oversimpliﬁed, it serves to show some basic characteristics of
Markov chains. The operation of running a Markov chain until it reaches its stationary
distribution is a critical part of the process employed in MCMC estimation for Bayesian
models.
■Example 10.1:
A Markov Chain for Card-Shuﬄing.
A simplistic algorithm for
shuﬄing a deck of cards is to take the top card and insert it uniformly back into the
deck, and repeat this process many times. Thus the top card has the probability 1/52
of being placed at any position in the stack, including returning to the top position.
This is clearly a stochastic process operating on a discrete state space since there is

Basics of Markov Chain Monte Carlo
337
a sequential set of identiﬁable states from a ﬁnite number (52!) of possible states. Is
this a Markov chain though? The probability of some state being the next observed
state is conditional on two things: (1) the current arrangement of the cards in the
deck, and (2) the placement of the top card at this step. Therefore conditional on the
current state, previous states are irrelevant and the probabilistic process depends only
on the transition process and this current state. So we can claim that this shuﬄing
process is a Markov chain.
More interestingly, what is the limiting (stationary) distribution of this Markov chain?
Such examples from games of chance can be quite interesting on their own, but they
also show the mechanics of basic Markov chains.
See Bayer and Diaconis (1992)
or Diaconis (1988)
for more details. Rather than analytically look at the limiting
distribution, let’s simulate it with R. For simplicity (without loss of generality), we
will use only n = 3 cards. This means that the sample space (3! elements large) is the
set:
{[1, 2, 3], [1, 3, 2], [2, 1, 3], [2, 3, 1], [3, 1, 2], [3, 2, 1]}
and the transition kernel is:
K =
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
1
3
0
1
3
1
3
0
0
0
1
3
0
0
1
3
1
3
1
3
1
3
1
3
0
0
0
0
0
0
1
3
1
3
1
3
1
3
1
3
0
0
1
3
0
0
0
1
3
1
3
0
1
3
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
where the placement of zeros on any given row indicate that element of state space is
an impossible destination given the starting point indicated by the row. So on the top
row, we have the starting state [1, 2, 3], making [1, 3, 2], [3, 1, 2], and [3, 2, 1] impossible
from the operation of moving the top card only. We can set up a simulation with the
following:
P <- matrix(c(1/3,0,1/3,0,1/3,0,0,1/3,1/3,0,1/3,0,1/3,
0,1/3,0,0,1/3,1/3,0,0,1/3,0,1/3,0,1/3,0,
1/3,1/3,0,0,1/3,0,1/3,0,1/3),nrow=6)
MC.multiply <- function(P.in,N)
{
P1 <- c(1,0,0,0,0,0)%*%P.in
for (i in 1:(N-1))
{
P1 <- P1%*%P.in
print(P1)
}
P1
}
MC.multiply(P,15)

338
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
These 15 iterations produce:
0.2222222
0.1111111
0.2222222
0.2222222
0.1111111
0.1111111
0.1851852
0.1481481
0.1851852
0.1851852
0.1481481
0.1481481
0.1728395
0.1604938
0.1728395
0.1728395
0.1604938
0.1604938
0.1687243
0.1646091
0.1687243
0.1687243
0.1646091
0.1646091
0.1673525
0.1659808
0.1673525
0.1673525
0.1659808
0.1659808
0.1668953
0.1664380
0.1668953
0.1668953
0.1664380
0.1664380
0.1667429
0.1665905
0.1667429
0.1667429
0.1665905
0.1665905
0.1666921
0.1666413
0.1666921
0.1666921
0.1666413
0.1666413
0.1666751
0.1666582
0.1666751
0.1666751
0.1666582
0.1666582
0.1666695
0.1666638
0.1666695
0.1666695
0.1666638
0.1666638
0.1666676
0.1666657
0.1666676
0.1666676
0.1666657
0.1666657
0.1666670
0.1666664
0.1666670
0.1666670
0.1666664
0.1666664
0.1666668
0.1666666
0.1666668
0.1666668
0.1666666
0.1666666
0.1666667
0.1666666
0.1666667
0.1666667
0.1666666
0.1666666
0.1666667
0.1666666
0.1666667
0.1666667
0.1666666
0.1666666
This is not a proof, but there is evidence here that the limiting distribution is uniform
across the six states. In fact running MC.multiply for thousands or tens of thousands
of iterations produces no changes in probabilities. We could therefore conclude that
this algorithm produces a shuﬄed deck, albeit not in the most eﬃcient fashion.
10.1.3
The Chapman-Kolmogorov Equations
The form of (10.3) also leads to a more general notion of how chain probabilities are
strung together. The Chapman-Kolmogorov equations specify how successive events are
bound together probabilistically. These are given here for both discrete and continuous
state spaces where we abbreviate the left-hand side expression of (10.3):
pm1+m2(x, y) =

all z
pm1(x, z)pm2(z, y)
discrete case,
pm1+m2(x, y) =

range z
pm1(x, z)pm2(z, y)dz
continuous case.
(10.4)
The Chapman-Kolmogorov equations are particularly elegant for the discrete case because
(10.4) can be represented as a series of transition matrix multiplications:
pm1+m2 = pm1pm2 = pm1pm2−1p = pm1pm2−2p2 = . . . .
(10.5)
Thus iterative probabilities can be decomposed into segmented products in any way that
we like, depending on the interim steps.

Basics of Markov Chain Monte Carlo
339
10.1.4
Marginal Distributions
The ﬁnal basic notational characteristic of Markov chains that we will provide here is
the marginal distribution at some step mth from the transition kernel. For the discrete case
the marginal distribution of the chain at the m step is obtained by inserting the current
value of the chain, θ[m]
i
, into the row of the transition kernel for the mth step, pm:
πm(θ) = [pm(θ1), pm(θ2), . . . , pm(θk)].
(10.6)
So the marginal distribution at the ﬁrst step of the Markov chain is given by:
π1(θ) = π0(θ)p1,
(10.7)
where π0 is the initial starting value assigned to the chain and p1 = p is the simple transition
matrix given in (10.2). A neat consequence of the deﬁning characteristic of the transition
matrix is the relationship between the marginal distribution at some (possibly distant) step
and the starting value:
πn = pπn−1 = p(pπn−2) = p2(pπn−3) = . . . = pnπ0.
(10.8)
Since it is clear here that successive products of probabilities quickly result in lower proba-
bility values, the property above shows how Markov chains eventually “forget” their starting
points. The marginal distribution for the continuous case is only slightly more involved since
we cannot just list as a vector the quantity:
πm(θj) =

θ
p(θ, θj)πm−1(θ)dθ,
(10.9)
which is the marginal distribution of the chain, given that the chain is currently on point
θj at step m.
10.2
General Properties of Markov Chains
There are several properties of Markov chains that are important to us, particularly
when discussing long run Markov chain stability. These properties have intimidating names
that are inherited from mathematical Markov chain theory, but in reality are fairly straight-
forward ideas. Generally, if we can describe the mathematical status of a particular chain,
then we can often determine if it is capable of producing a useful sample from the distribu-
tion of interest. The properties are only brieﬂy summarized here and those interested in a
more technical and detailed treatment should read: Gamerman and Lopes (2006), Iosifescu
(1980), Norris (1997), Nummelin (1984), or Tierney (1996).

340
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
10.2.1
Homogeneity
A homogeneous Markov chain at step n has a transition probability that does not de-
pend on the value of n. So the decision to move at this step is independent of this being
the current point in time.
Interestingly, but not importantly, at the starting point the
chain cannot be homogeneous since the marginal distribution for the ﬁrst step is clearly not
independent of the initial values, which are hand-picked. One reason that the Gibbs sam-
pler and the Metropolis-Hastings algorithm, both given in detail in this chapter, dominate
MCMC implementations is that the chains they deﬁne possess this critical property: there
is no explicit value of n that governs the transition kernel.
10.2.2
Irreducibility
There are also properties directly associated with states. A state is absorbing if once the
chain enters this state it cannot leave: p(A, Ac) = 0. The obverse of absorbing is transient.
A state is transient if, given that a chain currently occupies state A, the probability of not
returning to A is non-zero. A more relevant case of absorbing is the situation where a state,
A, is closed with regard to some other state, B: p(A, B) = 0.
A Markov chain is irreducible if every point or collection of points (a subspace, required
in the continuous case), A, can be reached from every other point or collection of points.
A convenient way to remember the principle behind irreducibility is the notion that you
could reduce the set if you wanted to (this is obviously always possible except for the null
set), but that you do not want to because then there will be points that cannot be reached
from other points. This means that p(θi, θj) ̸= 0, ∀i, j ∈A. Notice that irreducibility is a
characteristic of both the chain and the subspace. So irreducibility implies the existence of
a path between any two points in the subspace. The key relationship of interest is between
irreducibility and recurrence.
10.2.3
Recurrence
If a subspace is closed, ﬁnite, and irreducible, then all states within this subspace are
recurrent. Recurrence is a desirable property of Markov chains. An irreducible Markov
chain is called recurrent with regard to a given state, A, which is a single point or a deﬁned
collection of points (required for the bounded-continuous case), if the probability that the
chain occupies A inﬁnitely often over unbounded time is nonzero. This can also be restated
by saying that the expected number of returns to A in the limit is inﬁnity. The Markov
chain is positive recurrent if the mean time to return to A is bounded, otherwise it is called
null recurrent.
This characteristic of Markov chains was introduced by Doeblin (1940)
(obviously without knowing of its eventual importance in practical Bayesian applications).
If we only had to deal with discrete or ﬁnite-bounded state spaces then this form of
recurrence would be enough, but with unbounded-continuous state spaces it is necessary
to have a stricter deﬁnition of recurrence that guarantees that the probability of visiting

Basics of Markov Chain Monte Carlo
341
A inﬁnitely often in the limit is now one: Harris recurrence (Harris 1956). First we say
that a set A is Harris recurrent if the probability of the chain visiting A inﬁnitely often in
the limit is one. An irreducible Markov chain is Harris recurrent if, for a ﬁnite probability
measure P, the chain at time n has for all such ﬁnite subsets A of the measure space a
non-zero probability of reaching A (Athreya and Ney 1978). Formally: if there exists a
σ-ﬁnite probability measure P on the measure space S so that an irreducible Markov chain,
Xn, at time n has the property: p(Xn ∈A) = 1, ∀A ∈S where P(A) > 0.
The distinction between recurrence and Harris recurrence is important in demonstrating
the convergence of speciﬁc Markov chain algorithms for continuous state spaces. In fact,
an aperiodic, irreducible chain with an invariant distribution on an unbounded continuous
state space that is not Harris recurrent has a positive probability of getting stuck forever
in an area bounded away from convergence, given a starting point there.
Additional details on recurrence are found in the key works in this area: Meyn and
Tweedie (1994), Tierney (1994), and Athreya, Doss, and Sethuraman (1996).
For our
purposes it is suﬃcient to simply consider the point: “Harris recurrence essentially says
that there is no measure-theoretic pathology” (Chan 1994). The greatest concern here is
that ill-chosen starting points can cause eventual problems. and Geyer
Given a set of recurrent states (nonempty, and bounded or countable) or Harris recurrent
states, then the union of these states creates a new state that is closed and irreducible (Meyn
and Tweedie 1993). This means that the linkage between recurrence and irreducibility is
important in deﬁning a subspace that captures a Markov chain and at the same time assures
that this Markov chain will explore all of the subspace. Whenever a chain wanders into a
closed, irreducible set of Harris recurrent states, it then stays there and visits every single
state (eventually) with probability one (replaces almost sure convergence with convergence
at every point).
10.2.4
Stationarity
Deﬁne π(θ) as the stationary distribution of the Markov chain for θ on the state space
A. We denote p(θi, θj) to indicate the probability that the chain will move from θi to θj
at some arbitrary step t from the transition kernel, and πt(θ) as the marginal distribution.
This stationary distribution is then deﬁned as satisfying:

θi
πt(θi)p(θi, θj) = πt+1(θj)
Discrete case

πt(θi)p(θi, θj)dθi = πt+1(θj)
Continuous case.
(10.10)
Therefore multiplication by the transition kernel and evaluating for the current point (the
summation step for discrete sample spaces and the integration step for continuous sample
spaces) produces the same marginal distribution: π = πp in shorthand. This demonstrates
that the marginal distribution remains ﬁxed when the chain reaches the stationary distri-

342
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
bution and we might as well drop the superscript designation for iteration number and just
use π(θ).
Once the chain reaches its stationary distribution (also called its invariant distribution,
equilibrium distribution, or limiting distribution if discussed in the asymptotic sense), it
stays in this distribution and moves about, or “mixes,” throughout the subspace according
to marginal distribution, π(θ), forever.
This is exactly what we want and expect from
MCMC. If we can set up the Markov chain so that it reaches a stationary distribution that
is the desired posterior distribution from our Bayesian model, then all we need to do is let it
wander about this subspace for a while, producing empirical samples to be summarized. The
good news is that the two primary forms of MCMC kernels we will use have the property
that they are guaranteed to eventually reach a stationary distribution that is the desired
posterior distribution.
10.2.5
Ergodicity
It is also possible to deﬁne the period of a Markov chain. This is simply the length
of time to repeat an identical cycle of chain values. It is desirable to have an aperiodic
chain, i.e., where the only length of time for which the chain repeats some cycle of values
is the trivial case with cycle length equal to one. Why? It seems as though we would not
necessarily care if there were some period to the chain values, particularly if the period
were quite long, or perhaps in the discrete state if it included every value in the state space.
The answer is that the recurrence property alone is not enough to assure that the chain
reaches a state where the marginal distribution remains ﬁxed and identical to the posterior
of interest.
If a chain is irreducible, positive Harris recurrent, and aperiodic, then we call it ergodic.
Ergodic Markov chains have the property:
lim
n→∞pn(θi, θj) = π(θj),
(10.11)
for all θi, and θj in the subspace (Nummelin 1984). Therefore, in the limit, the marginal
distribution at one step is identical to the marginal distribution at all other steps. Better
yet, because of the recurrence requirement, this limiting distribution is now closed and
irreducible, meaning that the chain will never leave it and will eventually visit every point
in the subspace. Once a speciﬁed chain is determined to have reached its ergodic state,
sample values behave as if they were produced by the posterior of interest from the model.
The ergodic theorem is analogous to the strong law of large numbers but for Markov
chains. It states that any speciﬁed function of the posterior distribution can be estimated
with samples from a Markov chain in its ergodic state because averages of sample values
give strongly consistent parameter estimates. More formally, suppose θi+1, . . . , θi+n are
n (not necessarily consecutive) values from a Markov chain that has reached its ergodic

Basics of Markov Chain Monte Carlo
343
distribution, a statistic of interest, h(θ), can be calculated empirically:
ˆh(θi) = 1
n
i+n

j=i+1
h(θj) ≈h(θ),
(10.12)
and for ﬁnite quantities this converges almost surely: p[ˆh(θi) →h(θ), as n →∞] =
1 (Roberts and Smith 1994, p.210; Tierney 1994, p.1717).
The remarkable result from
ergodicity is that even though Markov chain values, by their very deﬁnition, have serial
dependence, the mean of the chain values provides a strongly consistent estimate of the
true parameter. For a given empirical estimator ˆh(θi) with bounded limiting variance, we
get the central limit theorem results:
√n
ˆh(θi) −h(θ)
/
Var(ˆh(θi))
−→
n→∞N(0, 1).
(10.13)
This is an important principle for MCMC estimation because it says that we can take
the simulation values from the stationary distribution and safely ignore the serial nature of
their production. For more detailed theoretical justiﬁcations, see Meyn and Tweedie, (1993,
Chapter 15), or Tierney (1994).
10.3
The Gibbs Sampler
The Gibbs sampler, originating with Geman and Geman (1984), is the most widely
used MCMC technique. This is a testament to its simplicity and reliability as a method
for producing useful chain values. The Gibbs sampler requires speciﬁc knowledge about
the conditional nature of the relationship between the variables of interest.
The basic
idea, which is not diﬃcult to conceptualize, is that if it is possible to express each of the
coeﬃcients to be estimated as conditioned on the others, then by cycling through these
conditional statements, we can eventually reach the true joint distribution of interest.
10.3.1
Description of the Algorithm
The Gibbs sampler is a transition kernel created by a series of full conditional distri-
butions that is a Markovian updating scheme based on conditional probability statements.
If the limiting distribution of interest is π(θ) where θ is a k length vector of coeﬃcients
whose posterior distribution we want to describe, then the objective is to produce a Markov
chain that cycles through these conditional statements moving toward and then around this
distribution. The set of full conditional distributions for θ are denoted Θ and deﬁned by
π(Θ) = π(θi|θ−i) for i = 1, . . . , k, where the notation θ−i indicates a speciﬁc parametric
form from Θ without the θi coeﬃcient.

344
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
There must be an analytically deﬁnable full conditional statement for each coeﬃcient
in the θ vector and these probability statements need to be completely articulated so that
it is possible to draw samples from the described distribution. This requirement facilitates
the iterative nature of the Gibbs sampling algorithm, which cycles through these full con-
ditionals drawing parameter values based on the most recent version of all of the previous
parameters in the list. The order does not matter, but it is essential that the most recent
draws from the other samples be used. This looks like the following procedure (note the
use of the most recent iteration values at each step):
1. Choose starting values: θ[0] = [θ[0]
1 , θ[0]
2 , . . . , θ[0]
k ]
2. At the jth starting at j = 1 complete the single cycle by drawing values from the k
distributions given by:
θ[j]
1
∼
π(θ1
|
θ[j−1]
2
,
θ[j−1]
3
,
θ[j−1]
4
,
. . . ,
θ[j−1]
k−1 ,
θ[j−1]
k
)
θ[j]
2
∼
π(θ2
|
θ[j]
1 ,
θ[j−1]
3
,
θ[j−1]
4
,
. . . ,
θ[j−1]
k−1 ,
θ[j−1]
k
)
θ[j]
3
∼
π(θ3
|
θ[j]
1 ,
θ[j]
2 ,
θ[j−1]
4
,
. . . ,
θ[j−1]
k−1 ,
θ[j−1]
k
)
...
θ[j]
k−1
∼
π(θk−1
|
θ[j]
1 ,
θ[j]
2 ,
θ[j]
3 ,
. . . ,
θ[j]
k−2,
θ[j−1]
k
)
θ[j]
k
∼
π(θk
|
θ[j]
1 ,
θ[j]
2 ,
θ[j]
3 ,
. . . ,
θ[j]
k−2,
θ[j]
k−1)
3. Increment j and repeat until convergence.
Once convergence is reached, all simulation values are from the target posterior distribution
and a suﬃcient number should then be drawn so that all areas of the posterior are explored.
Notice the important feature that during each iteration of the cycling through the θ vector,
conditioning occurs on θ values that have already been sampled for that cycle; otherwise
the θ values are taken from the last cycle. So in the last step for a given j cycle, the sampled
value for the kth parameter gets to condition on all j-step values.
The statements above clearly demonstrate that it is required to have the full set of
conditional distributions to run the Gibbs sampling algorithm. As we will see in Chap-
ter 12, these necessary statements often fall naturally out of the hierarchical conditional
relationships. In some cases they come from classic theory. For example, a simple bivariate
normal speciﬁcation for θ1 and θ2 given by: N

[ 0
0 ] ,
	 1 ρ
ρ 1


, gives the set of full conditional
distributions:
θ[j]
1 |θ[j−1]
2
∼N(ρθ[j−1]
2
, 1 −ρ2)
θ[j]
2 |θ[j]
1 ∼N(ρθ[j]
1 , 1 −ρ2).

Basics of Markov Chain Monte Carlo
345
If the Gibbs sampler has run suﬃciently long, forthcoming full cycles of the algorithm
produce a complete sample of the coeﬃcients in the θ vector. All future iterations from
this point on produce samples from the desired limiting distribution and can therefore
be described empirically. The most impressive aspect of the Gibbs sampler is that these
conditional distributions contain suﬃcient information to eventually produce a sample from
the full joint distribution of interest.
10.3.2
Handling Missing Dichotomous Data with the Gibbs Sampler
Suppose that a coin is ﬂipped 20 times and the result is recorded such that a head is
ﬁxed as 1 and a tails is ﬁxed as 0. Of these X1, X2, . . . , X19 are observed and the last, X20,
is lost or unobserved. The standard question for this experiment is whether or not the coin
is fair, or more generally, what is the probability that coin produces a head, θ.
A useful simpliﬁcation is to treat the sum of the observed 0/1 outcomes as a single
random variable as done in Example 2.3.4 (page 49), Y = 19
i=1 Xi, and ignore the missing
value. Thus Y is now distributed according to the binomial probability mass function:
p(Y |θ) =
n
Y

θY (1 −θ)n−Y
where n = 19 here.
Of course this neglects the eﬀect of the missing value (X20) and
may cause bias. However, we can use Gibbs sampling to estimate its value simultaneously
along with the unknown θ.
Clearly, the PMF of the missing value is still a Bernoulli
form conditional on θ. If we were able to treat Y and X20 as known (observed) then the
distribution of θ would be:
p(θ|Y, X20) =

20
Y + X20

θY +X20(1 −θ)20−Y −X20
which gives the kernel of a beta probability density function. This means that we now have
two full conditional distributions, given temporary values for each unknown, θ∗and X∗
20:
θ|X∗
20, Y ∼beta(Y + X∗
20 + 1, 20 −Y −X∗
20 + 1)
X20|Y, θ∗∼Bernoulli(θ∗).
This is an ideal setup for Gibbs sampling since the joint distribution can be treated as a full
conditional distribution for either θ or X20 if the other is given a temporary value. So if our
fundamental interest lies in estimating θ without bias, this process explicitly incorporates
the missing value rather than ignoring it.
We begin with the sample:
{1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1}, and run the
chain for 50,000 iterations. Let us consider the ﬁrst 20,000 iterations as “burn-in” while
the Markov chain ﬁnds its limiting distribution and dispose of them as pre-convergence
values. The posterior mean of the rest of the simulated θ values is 0.714 (with variance
0.009) suggesting that coin is far from “fair.” This is also diﬀerent than the observed data

346
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
0.3
0.4
0.5
0.6
0.7
0.8
0.9
30001
50000
θ
MCMC Traceplot
Histogram with 95% HPD Interval
[ 0.525 : 0.896 ]
FIGURE 10.1: Gibbs Sampling Demonstration
mean of 0.737, which would have been a biased estimator. The posterior mean for the
missing datapoint is 0.723 making it seem more likely to be a one than a zero, although
the posterior variance of approximately 0.2 suggests some caution in the claim. However,
regardless of our conﬁdence in the posterior claim for the missing data, the estimate of θ
remains unbiased.
We can, perhaps more usefully, describe the output of the Gibbs sampler graphically,
as done in the two-panel Figure 10.1. Here we have a traceplot that shows the time-series
of the Markov chain path and a histogram of the last 30,000 draws. One advantage to
describing posterior distributions with MCMC is that summary statistics are trivial to
generate since we can calculate various quantities directly on the empirical draws. The
95% highest posterior density region was produced from taking a histogram with a large
number of bins (300 rather than the 100 shown in the graph) and sorting the density values
observing where the thresholds of interest fall. Naturally this is just an approximation since
a histogram is not a perfectly smooth density, but the parameters are suﬃciently adjustable
for 30,000 draws that it can be a very accurate approximation. Importance sampling can
also be used to produce HPD regions as described on page 301.
■Example 10.2:
Changepoint Analysis for Terrorism in Great Britain: 1970-
2004. This example looks at a series of terrorist events over a 35 year period in the
U.K. in which there were injuries and/or fatalities. This period, 1970 to 2004, is dom-
inated by events related to the “the troubles” in Northern Ireland and the Provisional
IRA’s objective of inﬂuencing British policy by bombing in England. The beginning
date is selected to roughly coincide with three important events: the split of the IRA
into the Oﬃcial IRA and the Provisional IRA (1969), the start of internment for IRA
members (1971), and the “Bloody Sunday” march in which British paratroopers killed

Basics of Markov Chain Monte Carlo
347
14, and injured 13, demonstrators. These data are subsetted from TWEED (“Ter-
rorism in Western Europe: Events Data”) compiled by Jan Oskar Engene and made
available at http://folk.uib.no/sspje/tweed.htm as well as in BaM. The data are
characterized by relatively high counts in the early era and relatively low counts in
the late era. Thus the question is when was there a pronounced change in terrorism
rates? The data are shown in Table 10.1.
TABLE 10.1:
Count of Terrorism
Incidents in Britain, 1970-2004 by Rows
1
21
23
9
15
17
18
5
4
8
4
1
5
2
3
0
1
2
3
3
6
5
3
2
3
0
1
1
1
1
0
0
0
0
0
Statistically, the objective is to use this sequence to estimate the changepoint and also
to obtain posterior estimates of the intensity parameters of the two separate Poisson
processes (Poisson because these are counts). This process is addressed more generally
for an unknown number of changepoints by Phillips and Smith (1996).
Speciﬁcally, x1, x2, . . . , xn are a series of count data where there exists the possibility
of a changepoint at some period, k, along the series. Therefore there are two Poisson
data-generating processes:
xi|λ ∼P(λ)
i = 1, . . . , k
xi|φ ∼P(φ)
i = k + 1, . . . , n,
where the determination of which to apply depends on the location of the changepoint
k. So now there are three parameters to estimate: λ, φ, and k. This problem is
distinguished by the added complexity that one of the parameters, k, operates in a
diﬀerent capacity on the others: determining a change in the serial data generation
process, rather than as a conventional parametric input.
The three independent priors applied to this model are:
λ ∼G(α, β)
φ ∼G(γ, δ)
k ∼discrete uniform on[1, 2, . . . , n],
where for purposes of this example, the prior parameters are assigned according to:
α = 4, β = 1, γ = 1, δ = 2. Since the mean of a gamma distribution is the ratio
of its parameters (in the scale version of the gamma distribution), this assignment of
parameters roughly is close to, but slightly larger than, the mean of the ﬁrst 50% of
the data (αβ), and the second 50% of the data (γδ). This leads to the following joint

348
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
posterior and its proportional simpliﬁcation:
π(λ, φ, k|y) ∝L(λ, φ, k|y)π(λ|α, β)π(φ|γ, δ)π(k)
=
* k
+
i=1
e−λλyi
yi!
, *
n
+
i=k+1
e−φφyi
yi!
,  βα
Γ(α)λα−1e−βλ

×
 δγ
Γ(γ)φγ−1e−δφ
 1
n
∝λα−1+k
i=1 yiφγ−1+n
i=k+1 yi exp[−(k + β)λ −(n −k + δ)φ].
The last line easily provides the full conditional distribution for λ and φ:
λ|φ, k ∼G(α +
k

i=1
yi, β + k)
φ|λ, k ∼G(γ +
n

i=k+1
yi, δ + n −k),
but the full conditional distribution for k requires more work. Start with the likelihood
function under the assumption that λ and φ are ﬁxed and rearrange:
p(y|k, λ, φ) =
* k
+
i=1
e−λλyi
yi!
, *
n
+
i=k+1
e−φφyi
yi!
,
=
* n
+
i=1
1
yi!
,
ek(φ−λ)e−nφλ
k
i=1 yi
*
n
+
i=k+1
φyi
, * k
+
i=1
φyi
φyi
,
=
* n
+
i=1
e−φφyi
yi!
, *
ek(φ−λ)
λ
φ
k
i=1 yi,
= f(y, φ)L(y|k, λ, φ).
(10.14)
What this does is provide two functions, the ﬁrst of which is free of k. Since our
objective is simply a full conditional statement for k, we will use only the modiﬁed
likelihood function that contains k, L(y|k, λ, φ), after a cancellation below. Suppress-
ing the conditioning on λ and φ for notational clarity only, apply Bayes’ Law with a
generic prior on k, p(k):
p(k|y) =
f(y, φ)L(y|k)p(k)
n
ℓ=1 f(y, φ)L(y|kℓ)p(kℓ)
=
L(y|k)p(k)
n
ℓ=1 L(y|kℓ)p(kℓ).
(10.15)
Here we took advantage of the discrete feature of k and summed over all possible

Basics of Markov Chain Monte Carlo
349
values with the index ℓ. For simple priors like our discrete uniform, this simpliﬁes
even more such that with proportionality now p(k|y) = L(y|k)/ n
ℓ=1 L(y|kℓ). So
each iteration of the Gibbs sampler will calculate an n-length probability vector for k
and draw a value accordingly. This is implemented in the following R code:
cp.gibbs <- function(theta.matrix,y,a,b,g,d)
{
n <- length(y); y.bar <- mean(y)
k.prob <- rep(0,length=n)
for (i in 2:nrow(theta.matrix))
{
lambda <- rgamma(1,a+sum(y[1:theta.matrix[(i-1),3]]),
rate=b+theta.matrix[(i-1),3])
phi
<- rgamma(1,g+sum(y[theta.matrix[(i-1),3]:n]),
rate=d+length(y)-theta.matrix[(i-1),3])
for (j in 1:n)
k.prob[j] <- exp( j*(phi-lambda) +
log(lambda/phi)*sum(y[1:j]-y.bar/j) )
k.prob <- (lambda/phi)^y.bar*k.prob
k
<- sample(1:n,size=1,prob=k.prob)
theta.matrix[i,] <- c(lambda,phi,k)
}
theta.matrix
}
There are a few subtleties in the code. In the second for-loop the data y[] are mean-
centered and this is undone in the next line to increase numerical stability (large
count values can send entries of k.prob to inﬁnity). Notice also that the function
does not normalize the k.prob vector. This is because the sample function will do it
automatically in cases where the vector values do not sum to one.
TABLE 10.2:
Gibbs Sampler
Draws for k Parameter
Value
7
8
9
10
11
21
Count
4352
330
10
283
24
1
The function is run for 10,000 iterations retaining only the last 5,000 chain values,
which are summarized in Table 10.3. This is a conservative strategy with this simple
model and there is no evidence of non-convergence for the Markov chain. The quantile
results for k seem confusing at ﬁrst until one remembers that k is a discrete variable.
The Markov chain therefore jumps from integer value to integer value. In fact, the
chain had strong “preferences” as indicated by the values shown in Table 10.2. The
Markov chain was started with k = 21, which explains that isolated value.
Figure 10.2 shows the ﬁrst 100 iterations of the sampler in a pair of two-dimensional
graphs where k is depicted on the y-axis in both cases plotted against λ and φ,
respectively. Notice that the Gibbs sampler converges quickly to the region of the
reported posterior.

350
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
TABLE 10.3:
Terrorism in Great Britain
Model, Posterior Summary
Quantile
λ
φ
k
Minimum
8.61
1.39
7.00
First quartile
12.40
2.60
7.00
Median
13.30
2.85
7.00
Third quartile
14.40
3.09
7.00
Maximum
18.90
4.10
11.00
Mean
13.30
2.83
7.29
Each movement is a straight line here since the intermediate steps of the Gibbs sam-
pler hold all other parameter values constant while sampling for a single parameter
conditioned on these other values. The k dimension is jittered slightly to reveal the
concentration at k = 7. So looking at the ﬁrst step in the left-hand side panel of
Figure 10.2, we begin by conditioning on k = 21 and move from the starting value
λ = 1.0 to a sampled value of 6.447. Then the second half of the ﬁrst iteration holds
this value for λ constant and draws a value for k of 10. After these two steps, the
ﬁrst full iteration of the sampler is done. This process then continues for 4999 more
iterations. Similarly, in the second panel the sampler goes from the same starting
point to φ = 2.084. Note that the algorithm proceeds rapidly to a more preferred
region.
■Example 10.3:
Changepoint Analysis for Military Fatalities in Afghanistan.
A more topical example comes from looking at military fatalities among coalition
forces in Afghanistan for Operation Enduring Freedom, the ongoing eﬀort to depose
and defeat the Taliban government forces that had sheltered Al Queda. The monthly
count data covers the period from October 2001 to January 2007 and are culled from
U.S. military and NATO sources, and are provided in Table 10.4 for 52 monthly
periods, listed by rows.
TABLE 10.4:
NATO Fatalities in Afghanistan, 10/01 to 1/07
3
5
4
10
12
14
9
1
3
0
3
1
6
1
8
4
7
12
2
2
7
2
4
2
6
8
1
11
2
3
3
9
5
2
3
4
8
7
1
3
2
6
19
4
29
2
33
12
10
7
4
0

Basics of Markov Chain Monte Carlo
351
0
5
10
15
8
10
12
14
16
18
20
λ
k
φ
8
10
12
14
16
18
20
1
2
3
4
k
FIGURE 10.2: Gibbs Sampling Path for Terrorism Events
There is strong evidence that the resurgent Taliban made a strategic decision at some
point about the possibility of winning and correspondingly escalated their activities.
Naturally, we cannot know for certain the exact timing of such a decision, but it may
be possible to estimate the timing by looking for a changepoint in coalition fatalities.
Looking at the data for only a short period of time provides some idea that there is a
measurable diﬀerence in the early periods and the later periods. In fact, the mean of
the ﬁrst 40 periods is about 4.95 and the last ten periods 10.66. Using these means we
construct gamma priors for λ and φ with α = 1, β = 2, γ = 4, and δ = 4. The same
algorithm from the Terrorism in Great Britain example is used, producing marginal
posteriors summarized in Table 10.5.
This turns out to be quite revealing. The posterior mean for the later period intensity
parameter is about twice the size of that for the early period.
This supports the
popular conception that the war has become more “hot” in more recent years. Also, it
appears that there is strong evidence that the change occurred around the 42nd month,
which is March of 2005. Figure 10.3 shows that the posterior for the changepoint is

352
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
TABLE 10.5:
Afghan Fatalities Posterior
Quantile
λ
φ
k
Minimum
3.675
6.258
31.00
First quartile
4.480
8.533
31.00
Median
4.703
9.035
42.00
Third quartile
4.913
9.590
42.00
Maximum
5.755
12.532
44.00
Mean
4.703
9.057
41.49
highly concentrated, indicating that the data are strongly asserting a point around 40
(recall that the prior for k was uniform over the full range of months).
0.0
0.2
0.4
0.6
0.8
1.0
1.2
4.06
5.337
λ
0.0
0.1
0.2
0.3
0.4
7.389
10.744
φ
0.0
0.1
0.2
0.3
0.4
0.5
0.6
40
k
FIGURE 10.3: Afghan Fatalities Posterior Histogram, HPD Region
These changepoint models have many varieties and extensions. The classic, early ar-
ticle by Berry and Hartigan
(1993) is an excellent starting point. Other common
parametric forms for the outcomes are normal (Menzefricke 1981, Skates, Pauler, and
Jacobs
2001) and binomial/negative binomial (Zhao and Chu
2006, Smith 1975).
While we have only covered one changepoint here in two examples, it is common to
have multiple changepoints that need to be estimated and sometimes an unknown
number of them (Chib 1998, Yang and Kuo 2001,
Chen and Gupta 1997,
Braun,
Braun, and Muller 2000, Stephens 1994, Suchard et al. 2003, Fearnhead 2006, Green
1995).
Finally, see the paper by Carlin et al. (1992) for a hierarchical model imple-
mentation of changepoint models.

Basics of Markov Chain Monte Carlo
353
10.3.3
Summary of Properties of the Gibbs Sampler
We ﬁnish this section with a summary of the properties of the Gibbs sampler that make
it the most commonly used MCMC kernel speciﬁcation. These properties are covered in
greater detail in Chapter 13, for those interested in the underlying technical issues.
▷Since the Gibbs sampler conditions on values from the last iteration of its chain values,
it clearly constitutes a Markov chain.
▷The Gibbs sampler has the true posterior distribution of the parameter vector as its
limiting distribution.
▷The Gibbs sampler is a homogeneous Markov chain: the consecutive probabilities are
independent of n, the current length of the chain.
▷The Gibbs sampler converges at a geometric rate: the total variation distance between
an arbitrary time and the point of convergence decreases at a geometric rate in time
(t).
▷The Gibbs sampler is an ergodic Markov chain.
See Roberts and Polson (1994) and Tierney (1994) for the (very general) conditions. Of
course the number of iterations required is still a function of the complexity of the model
(hierarchies, hyperprior assignments, data characteristics, etc.), and therefore a motivation
for having convergence diagnostics (Chapter 14).
Casella and George (1992) provide a
very clear basic introduction to the Gibbs sampler and its properties (the original title
of the conference paper was “Gibbs for Kids”), a piece of which we have already seen in
Chapter 1. Also, a very useful discussion of the Gibbs sampler and its relation to other
MCMC techniques is provided by Smith and Roberts (1993).
Credit for introducing the Gibbs sampler on ﬁnite state spaces is usually given to Geman
and Geman (1984), but Ulf Grenander (a student of Harald Cram´er) actually applied it
to Bayesian modeling in a well-known but unpublished paper in 1983. Early restricted
versions, typically labeled as the heatbath algorithm in statistical physics can be found in
Creutz (1979), Creutz, Jacobs and Rebbi (1983), and Ripley (1979).
10.4
The Metropolis-Hastings Algorithm
The full set of conditional distributions for the Gibbs sampler are often quite easy to
specify from the hierarchy of the model since conditional relationships are directly artic-
ulated in such statements.
However, the Gibbs sampler of Geman
and Geman (1984)
obviously does not work when the complete conditionals for the θ parameters do not have
an easily obtainable form. In these cases a chain can be produced for these parameters us-
ing the Metropolis-Hastings algorithm from statistical physics (Chib and Greenberg 1995;

354
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
Metropolis et al. 1953; Hastings 1970; Peskun 1973) which is often applied in ﬁelds com-
pletely unrelated to the original application (e.g., Cohen et al. 1998).
10.4.1
Background
The original work by Metropolis et al. postulated a two-dimensional enclosure with
n = 10 molecular particles, and sought to estimate the state-dependent total energy of
the system at equilibrium. The central problem that they confronted is that there is an
incredibly large number of locations for the molecules in the system that must be accounted
for and this number grows exponentially with time. The key contribution of Metropolis et
al. is to model the system by generating moves that are more likely than others based on
positions that are calculated using uniform probability generated candidate jump points.
The moves are accepted probabilistically and likely ﬁnal states are determined after a set
of periods where many such decisions are made.
Therefore the simulation produces an
estimated force based on a statistical, rather than deterministic, arrangement of particles.
The critical assumptions are already familiar to us: any molecular state can be reached from
another (ergodicity), and state changes are induced probabilistically with an instrumental
distribution. The result, after convergence, is a distribution of particles from which energy
calculations can be made.
10.4.2
Description of the Algorithm
The simplest Metropolis-Hastings algorithm for a single selected parameter vector works
as follows. Suppose we have a J-length parameter vector, θ ∈ΘJ to estimate, with J
determining the dimension of the state space and the posterior of interest, π(θ). At the tth
step of the Markov chain when the chain is at the position θ, draw θ′ from a distribution
over the same support. This distribution is called the instrumental, jumping, or proposal
distribution and is denoted qt(θ′|θ). There is an obvious dependency on the data in both the
posterior and the candidate-generating forms, but we will suppress this for notational clarity.
Note that, unlike the Gibbs sampler, we are producing a multidimensional candidate value
all at once and not serially throughout the j = 1, . . . , J. It must be possible to determine
the reverse function value, qt(θ|θ′), and under the original constraints of Metropolis et
al. the two conditionals need to be equal (symmetry), although we now know that this is
not necessary.
Deﬁne the acceptance ratio (sometimes called the acceptance function) to be the follow-
ing:
a(θ′, θ) = π(θ′)
π(θ)
qt(θ|θ′)
qt(θ′|θ).
(10.16)
At time t, the decision that produces the t + 1st point in the chain is probabilistically

Basics of Markov Chain Monte Carlo
355
determined according to:
θ[t+1] =
⎧
⎪
⎪
⎨
⎪
⎪
⎩
θ′
with probability
min(a(θ′, θ[t]), 1)
θ[t]
with probability
1 −min(a(θ′, θ[t]), 1).
(10.17)
Several features of this algorithm are interesting. Observe that because of the ratio in the
decision above, we only have to know the posterior distribution π(θ), up to a constant of
proportionality. Note also that unlike the Gibbs sampler, the Metropolis-Hastings algorithm
does not necessitate movement on every iteration: the next chosen location may be the
current location. Finally, it is easy to see that in the case of symmetry in the candidate-
generating density, qt(θ|θ′) = qt(θ′|θ), the acceptance ratio simpliﬁes to a ratio of just the
posterior densities at the two points, which is the original 1953 construct.
We can describe a single Metropolis-Hastings iteration from the symmetric form of
(10.17) in the following steps:
1. Sample θ′ from q(θ′|θ), where θ is the current location.
2. Sample u from u[0 : 1].
3. If a(θ′, θ) = π(θ′)/π(θ) > u then accept θ′.
4. Otherwise keep θ as the new location.
Obviously we want to choose the q() distribution so that it is easy to sample from, but it is
also important that π(θ′)/q(θ′|θ) is fully known up to some arbitrary constant independent
of θ. So three things can happen here: we can sample a value of higher density and move
with probability one, we can sample a value of lower density but move anyway by drawing a
small uniform random variable in Step 2 above, or we can draw a uniform random variable
larger than the ratio of posteriors and therefore stay in the same location. One interesting
feature of the Metropolis-Hastings algorithm, in contrast to EM, is that it is “okay” to
move to lower density points, albeit probabilistically. So the algorithm will always move
to higher density points but will move to lower density points probabilistically based on
the ratio diﬀerence between the current point and the proposal. This algorithm describes
the full posterior density, so it is necessary at times to move from a high-density point to
a low-density point to traverse the space. Conversely, the EM algorithm never makes this
kind of decision and therefore is only a mode ﬁnder rather than a method of fully sampling
from the target distribution.
It can be shown that the Gibbs sampler is a special case of Metropolis-Hastings where
the probability of accepting the candidate value is always one (Gelman 1992, 436; Tanner
1996, p.182). However, it is a Metropolis-Hastings algorithm where the full conditionals
are required, and therefore is more restrictive in one sense (Gamerman and Lopes (2006),
Besag et al. 1995, Tierney 1991). Furthermore, Billera and Diaconis
(2001) show that
the Metropolis-Hastings algorithm is the optimal variant from those in its “natural class of
related algorithms,” which would obviously include tools like the Gibbs sampler.

356
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
10.4.3
Metropolis-Hastings Properties
Consider a posterior of interest, π(θ) (suppressing conditionalities for notational conve-
nience) with

π(θ)dθ = 1, for θ on some state space, S, which is deﬁned on a d-dimensional
Lebesgue measure: Ω ⊆ℜd.
The motivation for seeking an iterative solution through
MCMC techniques is that this π(θ) distribution is analytically complicated or unwieldy
(Gelman 1992), so we want a procedure that eventually arrives at this distribution through
simulation. The Metropolis-Hastings algorithm provides this function with a two-part tran-
sition kernel that has the property that it is closed with respect to the limiting distribution
of π(θ):
π(θ)p(θ, θ′) = π(θ′)p(θ′, θ)
∀θ, θ′ ∈Ω,
(10.18)
where p(a, b) deﬁnes a transition kernel from state a to state b. This is called the reversibility
condition for p() and also the detailed balance equation.
The detailed balance equation
guarantees that the Markov chain will eventually reach a single limiting distribution. The
values for π are simply the posterior distribution evaluated at the two points: θ and θ′, and
p(θ, θ′) is the appropriately sized transition kernel from θ to θ′. Robert and Casella (2004,
Chapter 7) provide a proof that under very general conditions, virtually any conditional
distribution over the appropriate support will provide a candidate jumping distribution
that provides a Metropolis-Hastings chain that will eventually converge to the limiting
distribution of π(θ) provided that (10.18) holds. This is the key theoretical importance of
the algorithm because it shows that the right thing will happen if we let the chain run long
enough.
A less general, and therefore less useful, condition is symmetry: p(θ, θ′) = p(θ′, θ). This
was originally the speciﬁed requirement related to the distributional relationship between
the two points, but the contribution of Hastings (1970) was to demonstrate that this is not
strictly necessary and reversibility can be substituted.
10.4.4
Metropolis-Hastings Derivation
There are really two parts to the transition kernel, a jumping density q(θ′|θ), and a
jumping probability d(θ, θ′) (or jumping decision):
p(θ, θ′) = q(θ′|θ)d(θ, θ′),
(10.19)
which determine the distribution of new θ′ values to move to and the probability of making
such a move, respectively. The distribution used for q(θ′|θ) is arbitrary, but it must be
straightforward to draw from. We sample from q(θ′|θ) to get potential values of the chain
to jump to. The form of this jumping distribution plays only a part in determining values
that we might jump to, and therefore it is not the decision to jump. By symmetric logic
we can also deﬁne the reverse jump:
p(θ′, θ) = q(θ|θ′)d(θ′, θ).
(10.20)

Basics of Markov Chain Monte Carlo
357
The decision to jump or not represents a second level of randomization as determined by
the probability d(θ, θ′). The candidate jumping point is favored if its probability is large
relative to the posterior probability associated with remaining at the current point.
A decision rule can be derived by starting with (10.19) solved for d(θ, θ′):
d(θ, θ′) = p(θ, θ′)
q(θ′|θ) ,
and then inserting (10.18) solved for p(θ, θ′):
d(θ, θ′) = π(θ′)p(θ′, θ)
π(θ)q(θ′|θ) .
Using p(θ′, θ) = q(θ|θ′)d(θ′, θ), this can be arranged as:
d(θ, θ′)
d(θ′, θ) = π(θ′)q(θ|θ′)
π(θ)q(θ′|θ) .
(10.21)
An acceptance rule (Hastings 1970) that meets this criteria and accommodates the situation
where we require for a jump d(θ′, θ) > d(θ, θ′) is:
α(θ′, θ) = min
'π(θ′)q(θ|θ′)
π(θ)q(θ′|θ) , 1
(
.
(10.22)
This is exactly (10.17) restated where q(θ′, θ) = qt(θ|θ′) and q(θ, θ′) = qt(θ′|θ). If we were
willing to substitute symmetry for reversibility, we would get the original simpliﬁed rule
from Metropolis et al. which is similar, but much more intuitive:
α(θ′, θ) = min
'π(θ′)
π(θ) , 1
(
,
(10.23)
because of cancellation. This states that the chain will move with probability one in a
direction of higher posterior probability if oﬀered by the jumping distribution, and will
move with probability a = π(θ′)/π(θ) to the new point otherwise. Therefore low values of
a will often result in staying at the same chain value for that dimension. Due to the two
levels of randomization, three things can now happen during each chain iteration: move to
the new point with probability one, generate a uniform random variable (bounded by zero
and one) that is less than α(θ′, θ) thus moving to the new point, or generate a uniform
random variable greater than a and then stay at the current point.
Furthermore, Chib and Greenberg (1995, pp.328-329) give a very nice intuitive way to
justify the detailed balance equation. Suppose (10.18) was “out of balance” in that:
π(θ)p(θ, θ′) > π(θ′)p(θ′, θ).
(10.24)
This means that the algorithm moves from θ to θ′ more often than it should and moves
from θ′ to θ less often than it should. We know from (10.22) that the decision to jump
to θ′ is really governed by p(θ, θ′) = q(θ′|θ)α(θ′, θ) (the probability of drawing θ′ times

358
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
the probability of deciding to accept it), so we really need to make α(θ, θ′) large enough to
rectify:
π(θ)q(θ′|θ)α(θ′, θ) > π(θ′)q(θ|θ′)α(θ, θ′),
(10.25)
meaning
α(θ′, θ) > π(θ′)q(θ|θ′)
π(θ)q(θ′|θ) α(θ, θ′).
(10.26)
Since α(θ, θ′) is a probability function, it is bounded by zero and one.
Therefore the
right-hand-side that makes the inequality into an equality is the minimum value of either:
π(θ′)q(θ|θ′)/π(θ)q(θ′|θ) (when α(θ, θ′) has to equal one), or one (when α(θ, θ′) has to be
equal to the inverse of the fraction on the right-hand-side). Thus the deﬁning equation of
the Metropolis-Hastings decision is speciﬁed. Since θ and θ′ are arbitrarily chosen here,
this argument works for any two values in the sample space.
10.4.5
The Transition Kernel
With a little bit of trouble, we can rigorously deﬁne the properties of the 1953 Metropolis
transition kernel. Start by deﬁning an indicator function for the event that the δ′ point
is accepted: I(δ′) = 1 if δ′ accepted, 0 otherwise (although this is a slightly diﬀerent
notation for the indicator function than was used before, it makes the explication below
more clean). Thus the probability of transitioning from θ = θ[k] to the proposed jumping
value θ′ = θ[k+1] at the kth step is:
p(θ, θ′) = p(θ[k+1] = θ′, I(θ′)|θ[k] = θ)
= p(θ[k+1] = θ′|θ[k] = θ)p(I(θ′))
= q(θ′|θ)min
'
1, π(θ′)
π(θ)
(
θ ̸= θ′.
(10.27)
The probability calculation for transitioning from θ = θ[k] to the current value (that is, not
moving at all) is only slightly more complicated because it can occur two ways: a successful
transition to the current state and a failed transition to a diﬀerent state. The ﬁrst event has
probability zero in continuous state space, but is worth covering for discrete applications.
This probability is:
p(θ, θ) = p(θ[k+1] = θ, I(θ)|θ[k] = θ)
 
!"
#
moving back to same point
+ p(θ[k+1] ̸= θ, ¬I(θ)|θ[k] = θ)
 
!"
#
not moving
= q(θ, θ) +

θ′̸=θ
q(θ′, θ)

1 −min
'
1, π(θ′)
π(θ)
(
.
(10.28)
In both of these calculations, the simpler situation of symmetry is assumed, but moving to
the reversibility assumption is just a matter of substituting (q(θ|θ′)π(θ′))/(q(θ′|θ)π(θ)) for
π(θ′)/π(θ).
A critical component of the choice for the jumping distribution is the speciﬁed variance.

Basics of Markov Chain Monte Carlo
359
If this variance is too large, then the jumping distribution will be too wide relative to the
target distribution and each successive step will move too far in some direction causing
us to move awkwardly through the sample space in exaggerated steps. It is also possible
to stipulate a jumping distribution variance that is too small causing overly cautious small
steps through the sample space. In this case we will converge slowly and mix poorly through
the limiting distribution once we have converged.
10.4.6
Example: Estimating a Bivariate Normal Density
This example is somewhat oversimpliﬁed in order to show the mechanics of the Metropolis-
Hasting algorithm. Suppose, according to the example in Chib and Greenberg (1995, 333),
we wanted to simulate the bivariate normal distribution: N
'
1
2

1.0
−0.9
−0.9
1.0
(
. Obviously we
could do this by more direct means, but the goal here is to demonstrate the workings of
Metropolis-Hastings. The speciﬁed jumping distribution is a bivariate Student’s-t and we
will start from four overdispersed positions as well as at the mode of the distribution. The
point here is to show that even though we deliberately disadvantage the process with regard
to starting points and a heavy-tailed jumping distribution, the end-product is descriptive
of our expectations.
The algorithm is given in R by:
metropolis <- function(theta.matrix,reps,I.mat)
{
for (i in 2:reps)
{
theta.star <- mvrnorm(1,theta.matrix[(i-1),],I.mat)/
(sqrt(rchisq(2,5)/5))
a <-dmultinorm(theta.star[1],theta.star[2],c(0,0),I.mat)/
dmultinorm(theta.matrix[(i-1),1],theta.matrix[(i-1),2],
c(0,0),I.mat)
if (a > runif(1)) theta.matrix[i,] <- theta.star
else theta.matrix[i,] <- theta.matrix[(i-1),]
}
theta.matrix
}
Figure 10.4 shows the last 200 iterations of the algorithm in the ﬁrst panel and the path of
the chain in the second. Notice that as with the Gibbs sampler example, convergence occurs
rather quickly to the posterior region for this simple setup. Unlike the Gibbs sampler, each
movement here is not a straight line parallel to one of the axes since the jumping decision
takes place simultaneously in two dimensions.
It is diﬃcult to see here, but the other
diﬀerence is that the chain is not required to move at each iteration.

360
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
−3
−2
−1
0
1
2
3
−3
−2
−1
0
1
2
3
−3
−2
−1
0
1
2
3
−3
−2
−1
0
1
2
3
   
FIGURE 10.4: Metropolis-Hastings, Bivariate Normal Simulation
10.5
The Hit-and-Run Algorithm
The Hit-and-Run algorithm is a special case of the Metropolis-Hastings algorithm that
separates the move decision into a direction decision and a distance decision. This makes it
especially useful in tightly constrained parameter space because we can tune the jumping
rules to be more eﬃcient (Gelman et al. 1996). It is also helpful when there are several
modes of nearly equal altitude. This discussion is introduced here as an example of the
ﬂexibility of the Metropolis-Hastings algorithm. Summarize the steps as following, starting
from an arbitrary point θk, at time k:
Step 1: Generate a multidimensional direction, Drk, on the surface of a
unit hypersphere from the distribution f(Dr|θ[k]).
Step 2: Generate a signed distance, Dsk, from density g(Ds|Drk, θ).
Step 3: Set the candidate jumping point to: θ′ = θ[k] + DskDrk and
calculate:
a(θ′, θ[k]) = π(θ′|X)
π(θ[k]|X)
.
Step 4: Move to θ[k+1] according to:
θ[t+1]
j
=
⎧
⎪
⎨
⎪
⎩
θ′
with probability
min(a(θ′, θ[k]), 1)
θ[k]
with probability
1 −min(a(θ′, θ[k]), 1).

Basics of Markov Chain Monte Carlo
361
There are several important assumptions to consider. First, for all Dri, f

Dr|θ[k]
> 0.
This just means that the directional distribution must be positive for all outcomes. Second,
the distance distribution, g(Ds|Dr, θ) must also be strictly greater than zero and have the
property:
g(Ds|Dr, θ) = g(−Ds| −Dr, θ).
(10.29)
This gives a new form of the detailed balance equation:
g(||θ[k] −θ′||)a(θ′, θ[k])π(θ[k]|X) = g(||θ′ −θ[k]||)a(θ[k], θ′)π(θ′|X)
(10.30)
Meeting these conditions means that the hit-and-run algorithm deﬁnes an ergodic Markov
chain with stationary distribution π(θ|X).
Typically f(Dr|θ[k]) is chosen to be uniform but others are possible, and the a(θ′, θ[k])
criterion can be made much more general. One advantage to this algorithm over standard
Metropolis-Hastings variant is that g(Ds|Dr, θ) is also ﬂexible and disengaged from the
direction decision, which makes it very tunable (Chen and Schmeiser 1993, Smith 1996). In
fact, it can also be made adaptive as the chain matures: adaptive directional sampling.
■Example 10.4:
We use this algorithm to simulate again from a bivariate normal.
Typically we would use hit-and-run with more problematic forms, but this example
shows the integrity of the process. The problem is made more interesting specifying a
strong correlation (0.95), which motivates some tuning of the algorithm. Accordingly,
random directions are drawn favoring the ﬁrst and third quadrants. Consider the
following R code:
hit.run <- function(theta.matrix,reps,I.mat)
{
for (i in 2:reps)
{
u.vec <- c(runif(1,0,pi/2),
runif(1,pi/2,pi),
runif(1,pi,3*pi/2), runif(1,3*pi/2,2*pi))
u.dr <- sample(u.vec,size=1, prob=c(1/3,1/6,1/3,1/6))
g.ds <- rgamma(1,1,1)
xy.theta <- c(g.ds*cos(u.dr),g.ds*sin(u.dr))
+ theta.matrix[(i-1),]
a <- dmultinorm(xy.theta[1],xy.theta[2],
c(0,0),I.mat)/dmultinorm(theta.matrix[(i-1),1],
theta.matrix[(i-1),2],c(0,0),I.mat)
if (a > runif(1)) theta.matrix[i,] <- xy.theta
else theta.matrix[i,] <- theta.matrix[(i-1),]
}
theta.matrix
}
Now we can run this function for our simple example and graph. Details on this
step are found in the Computational Addendum to this chapter. This produces

362
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
Figure 10.5 where the contour bands are produced from the true target distribution,
which we would ordinarily not know, and the points are the last 5,000 steps of the
algorithm. There are several tuning parameters set here in the implementation of the
algorithm: favoring positive slope direction by two-to-one in the picking of direction,
a gamma distribution for the candidate-generating distribution, and the parameters
of this gamma. Often when writing MCMC algorithms from scratch, these decisions
deserve close inspection.
−3
−2
−1
0
1
2
3
−3
−2
−1
0
1
2
3
FIGURE 10.5: Hit-and-Run Algorithm Demonstration
10.6
The Data Augmentation Algorithm
Tanner and Wong (1987) introduced data augmentation (sometimes called substitution
sampling) as a method for dealing with missing data or unknown parameter values by
augmenting known information with candidate values much in the same way that EM
does and iteratively improving the quality of these augmented quantities. In fact, data
augmentation can be used instead of EM to estimate models with missing data when more

Basics of Markov Chain Monte Carlo
363
than the mode of the likelihood function is required. Data augmentation is an MCMC
technique that successively substitutes improved estimates conditioned on the previous state
and therefore forms a Markov chain.
Much like the situation in which we applied the EM algorithm, suppose that we are
interested in estimating a single-dimension (for now) parameter θ. We observe some relevant
data but lack the complete set: X = [Xobs, Xmis], where all of the data (observed or not)
is conditional on θ. Data augmentation requires that we know the parametric form of the
posterior p(θ|X) corresponding to the complete data speciﬁcation, and the predictive form
for the missing data according to p(Xmis|Xobs). The algorithm proceeds by augmenting
the observed data with simulated values of the missing data, obtained by cycling through
these conditions according to the algorithm now described.
Start by deﬁning the posterior identity, which is the desired quantity stated as if we
could integrate out the missing data:
p(θ|Xobs) =

Xmis
p(θ|Xobs, Xmis)p(Xmis|Xobs)dXmis.
(10.31)
We can also deﬁne the predictive identity by asserting that there is some unknown param-
eter, φ on the sample space of θ, critical to generating the unobserved data but integrated
out:
p(Xmis|Xobs) =

Φ
p(Xmis|φ, Xobs)p(φ|Xobs)dφ.
(10.32)
Now insert (10.32) into (10.31) for the last term and interchange the order of integration:
p(θ|Xobs) =

Xmis
p(θ|Xobs, Xmis)
'
Φ
p(Xmis|φ, Xobs)p(φ|Xobs)dφ
(
dXmis
=

Φ

Xmis
p(θ|Xobs, Xmis)p(Xmis|φ, Xobs)dXmis
 
!"
#
K(θ,φ)
p(φ|Xobs)dφ.
(10.33)
Here Tanner and Wong use the shorthand K(θ, φ) to make the notation cleaner, not to
imply that this is a joint probability (it is really a transition kernel!).
The form of (10.33) implies that an iterative algorithm could be constructed that gen-
erates values of θ given an approximation for Xmis, and then generates new values of Xmis,
given this θ. Speciﬁcally, data augmentation at the ith iteration is (beginning with candidate
values for Xmis):
▷[Imputation-Step:]
▶generate θ[i] from p[i−1](θ|Xobs),
▶generate m values of Xmis from p(Xmis|θ[i], Xobs).
▷[Posterior-Step:]
▶update the parametric approximation using Xmis,1, . . . , Xmis,m:
p[i](θ|Xobs) = 1
m
m

j=1
p(θ|Xobs, Xmis,i).
(10.34)

364
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
In very similar fashion to importance sampling, there are two interrelated researcher-
generated speciﬁcations here: the tolerance value for determining convergence (p[i](θ|Xobs)−
p[i+1](θ|Xobs)), and the number of simulation values at each step (m). The second decision
is perhaps more crucial, and is a (now) familiar balance between speed and accuracy. In
their original article, Tanner and Wong provide m values of 1600 and 6400 for relatively
simple model speciﬁcations. It is therefore recommended that a similar value can be used
as an initial parameter. With computers becoming increasingly faster, it is unlikely that
this will place a heavy computation burden on the average system. Since larger values of
m give better intermediate approximations, there is necessarily a trade-oﬀbetween longer
runs and longer calculations at each run. In fact, if m = 1, then data augmentation is
actually Gibbs sampling, and obviously the emphasis is then purely on the length of runs.
Convergence of the data augmentation algorithm is demonstrated in Tanner and Wong.
Rosenthal (1993) showed that this convergence is on the order of the log of the number
of missing cases in the data for cases like this example. This is particularly encouraging
because it means that even for the very largest data sets that we see in the social and
behavioral sciences, data augmentation will likely be a reasonable computation process.
Also, including latent data from the data augmentation procedure does not preclude model
comparison or the generation of the standard model tests discussed in Chapter 7.
For
instance, Raftery (1996, p.182) gives the details of Bayes Factor tests when one or both of
the models include such latent data.
There are enough similarities between data augmentation and EM that one might wonder
when one is more applicable than the other. Both algorithms rely upon using the likelihood
function under the most simple circumstances possible, and making these circumstances
simple by completing the data with successively improved estimates of the missingness. EM
makes more sense when the objective is simply to get the mode of the posterior because
it is faster (less within-step calculations to perform). However, if the goal is to describe
the complete posterior distribution, then data augmentation is more appropriate. Since
data augmentation is a special case of Gibbs sampling and unlike Gibbs sampling, it is not
directly implemented in BUGS (see Congdon [2001, p.114] for a nifty workaround, however),
then one might have a natural preference for Gibbs sampling in practice unless one wanted
to treat the missingness more explicitly.
Finally, data augmentation (Tanner and Wong 1987) can help with mixing properties
even though it is most often posited as a means of dealing with missing data in Bayesian
models (Liu 2001, pp.135-8). Albert and Chib (1993), Swendsen and Wang (1987), Liu,
Wong, and Kong (1994), Carlin and Polson (1991), Rosenthal (1993), and more recently,
Imai and van Dyk (2004), and Gelman et al. (2008). The basic idea behind data augmen-
tation in the context here is that if an MCMC algorithm to marginalize π(θ|X) is mixing
slowly, then it is sometimes possible to ﬁnd another convenient random variable Υ such that
the algorithm operates more eﬃciently through sampling from π(θ, Υ|X) by alternating be-
tween π(θ|Υ, X) and π(Υ|θ, X). Chib (1992) explains how this can be quite simple and
elegant when Υ is a latent feature in the model speciﬁcation. Meng and van Dyk (1999), as

Basics of Markov Chain Monte Carlo
365
well as van Dyk and Meng (2001), extend this idea with conditional and marginal versions
of data augmentation to take advantage of unidentiﬁable parameters in order to improve
the rate of convergence.
■Example 10.5:
Data Augmentation for Decision-Making in the Mars Rover.
Planetary Rovers operate in environments where human exploration is expensive,
dangerous, or impossible. So NASA needs to implement local analysis and decision-
making in Rovers due to bandwidth limitations (3.5K to 12K per second to Mars)
and delay in telemetry and control signals (about 7 minutes one-way travel time to
Mars, available 3 hours per day).
This motivates the idea that the Rover should
“learn” about its environment as a way to update and improve the quality of the
decision-making process on its own. The Ames Research Center/CMU innovation
is the addition of Bayesian updating within Rover circuitry to enable autonomous
actions: obstacle avoidance, path planning, visual tracking, and stereo processing.
Why might this be relevant to social scientists? Proxies (agents or subordinates) need
to be autonomous under some circumstances, communicating at irregular intervals
and with limited information. Another tie-in is that formal models of principal/agent
relationships often assume asymmetric information and divergent goals. So real-time
analysis can be improved with semi-autonomous decision-making where Bayesian up-
dating maximally leverages incomplete knowledge.
The important criteria for robotic updating are: the current status of internal sys-
tems and resources, current environmental eﬀects, and the ramiﬁcations of actions
on resources and status. Note that Rover states are discrete modes, but measured
parameters are deﬁned on continuous spaces. All diagnosis here is done as a Bayesian
belief-updating system:
▷begin with some prior belief over possible states,
▷collect observations on status and environment,
▷update the distributions to reﬂect new evidence.
Interestingly, the sequence of states is actually Markovian. We now formalize this
in convenient notation in order to show the Bayesian learning process. For details,
see the research papers from the Carnegie Mellon team: Verma, Langford, and Sim-
mons (2001), Verma, Thrun, and Simmons (2003), and Verma, Gordon, Simmons, and
Thrun (2004), as well as the technical reports from JPL/Caltech and Ames/NASA:
Volpe, Nesnas, Estlin, Mutz, Petras, and Das (2001), Volpe and Peters (2003), Dear-
den and Clancy (2002), and Estlin, Volpe, Nesnas, Mutz, Fisher, Engelhardt, and
Chien (2001).
At time t the true multivariate state of the Rover systems is denoted Xt, but the
observed state by instruments is denoted Ot (sensors are noisy and limited). The
state of the Rover is Markovian since we know that:
p(Xt|X0, X1, . . . , Xt−1) = p(Xt|Xt−1),

366
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
and the observations are also conditionally independent of all but the last state:
p(Ot|X0, X1, . . . , Xt−1) = p(Ot|Xt−1).
Bayesian “ﬁltering” (data augmentation with Xt−1) for this particular state-estimating
problem is done by:
π(Xt|O1:t−1, Ot) = p(Ot|Xt, O1:t−1)p(Xt|O1:t−1)
p(Ot|O1:t−1)
∝p(Xt|O1:t−1)
 
!"
#
prior
p(Ot|Xt)
 
!"
#
likelihood
=

p(Xt|O1:t−1, Xt−1)p(Xt−1|O1:t−1)dXt−1 p(Ot|Xt)
=

p(Xt|Xt−1)
 
!"
#
transition model
p(Xt−1|O1:t−1)
 
!"
#
previous status
dXt−1 p(Ot|Xt)
 
!"
#
observation model
(10.35)
This works because:

p(Xt|O1:t−1, Xt−1)p(Xt−1|O1:t−1)dXt−1
=
 p(Xt, O1:t−1, Xt−1)
p(O1:t−1, Xt−1)
p(O1:t−1, Xt−1)
p(O1:t−1)
dXt−1
=

p(Xt, O1:t−1, Xt−1)/p(O1:t−1)dXt−1
=

p(Xt, Xt−1|O1:t−1)dXt−1
= p(Xt|O1:t−1)
(10.36)
Often the dimensionality of this posterior, π(Xt|O1:t), is too great for calculation in
real-time, so the Rover uses a recursive particle ﬁlter (importance sampling):
π(Xt|O1:t) ≈n−1
n

i=1
δ(Xt −x[i]
t )
(10.37)
where δ() is the Dirac delta function, and x[i]
t
are samples drawn from some pro-
posal distribution. The algorithm uses p(Xt|Xt−1) as the proposal distribution and
p(Ot|Xt) for importance weights, and then proceeds:
1. for i = 1 : n, draw x[i]
t ∼p(Xt|x[i]
t−1) and set ω[i]
t = p(Ot|X[i]
t ).
2. from this set, accept values x[i]
t
with probability proportional to ω[i]
t .
These transaction functions map a present mode to a PMF for future modes. So
state change probabilities are estimated then updated with inherent uncertainty, and
autonomous decisions are made based on the most recent posterior. So all of these
calculations occur in the context of executing general instructions: “go there,” “in-
vestigate that,” etc. The Rover then proceeds towards its objective while Bayesianly

Basics of Markov Chain Monte Carlo
367
updating the posterior for Xt, where intermediate decisions (movement, direction,
sensing) are made autonomously with criteria such as: safety, power conservation,
noting interesting phenomenon. In this fashion the Bayesian characteristics are ap-
parent.
10.7
Historical Comments
The background of the development of modern MCMC methods is interesting unto itself.
The ﬁrst important event was the publication of a 1953 paper by Nicholas Metropolis and
his colleagues: Arianna Rosenbluth, Marshall Rosenbluth, Augusta Teller, and Edward
Teller (who we know also made notable contributions in nuclear physics related to rather
large explosions). Because the paper was published in the Journal of Chemical Physics
and because it was applied exclusively to the problem of particles moving around a square,
interest was restricted primarily to physics.
Metropolis et al. were interested in obtaining the positions and therefore the potential
between all molecules in an enclosure and noted that even very modest sized setups lead
to integrals of very high dimensions. Speciﬁcally, if ℏij represents the shortest distance
between particles i and j, and V (ℏij) is the associated potential, then the total potential
energy of the whole system is given by: E = 1
2
n
i=1
n
j=1,j̸=i V (ℏij). Due to some sim-
pliﬁcations this leads to an integral of “only” 200 dimensions for the force of the system.
So rather than try to track and calculate future positions, they arrived at the idea of set-
ting a molecule movement criterion in a formal model sense and then simulating a series
of potential positions. In other words, it was suﬃcient to know where the molecules were
probabilistically at some future point in time as opposed to exactly.
The Metropolis et al. paper was slow to permeate other disciplines including statistics
partly because the authors appear not to have been aware of the widespread applicability of
their technique. This is a presumed explanation for why the authors chose not to generalize
it beyond the application given. The key to the dissemination of the algorithm was the
reﬁnement and generalization done by Hastings (1970) some time later. He showed that
reversibility can be substituted for symmetry in the approximation distribution, applicable
to continuous state spaces, and he makes the ideas accessible to statisticians. In addition,
Peskun (1973) should be credited with further introducing the Metropolis algorithm to the
statistics community and proving a number of important properties, including principles of
reversibility.
The Geman and Geman (1984) paper introduced a new use for the Gibbs distribution
in simulation and applies the tool to restoration of degraded images. This paper is very
diﬃcult to work through and most people do not persevere. Instead, the landmark Gibbs
sampling paper, as far as widespread eﬀects are concerned, is that of Gelfand and Smith
(1990).
They demonstrate how widely useful Gibbs sampling is in terms of setting up
Markov chains to estimate posterior distributions. While nothing is entirely new in this

368
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
paper, the synthesis and integration of Gibbs sampling into Markov chain Monte Carlo
theory for the ﬁrst time is an invaluable contribution.
One key reason for the explosion of academic attention to MCMC that occurred in the
1990s is the substantial improvement in computing power on the average desktop. This point
cannot be overstated. By deﬁnition these techniques are computer-intensive, and it is hard
to imagine earlier researchers being pleased with either the speed of their microcomputers
or the convenience of their campus mainframes. Fortunately Moore’s Law (doubling of
computing power every two years) continues to apply.
The overall impact on statistics and applied statistics cannot be reasonably overstated.
One long-time observer notes that “the Bayesian ‘machine’ together with MCMC is arguably
the most powerful mechanism ever created for processing data and knowledge” (Berger
2001). Essentially the advent of MCMC freed the Bayesian analyst from the Faustian choice
of accepting oversimpliﬁcation of the assumptions in order to merely get a tractable answer.
Because of these relatively simple tools, models of seemingly endless complexity can be
estimated. There are some additional issues to worry about, such as practical considerations
with Markov chain convergence (Chapter 14), but these are adequately handled with modern
software.
10.7.1
Full Circle?
An inquiring mind may have realized that many of the properties used to analyze and
exploit MCMC techniques are frequentist in nature. Principles used here such as the cen-
tral limit theorem, the law of large numbers, general asymptotic analysis, and transition
invariance, are all basic principles from traditional non-Bayesian statistics.
Speciﬁcally,
the tool that revolutionized Bayesian statistics is in fact a frequentist construction. Efron
(1998) notes that Fisher’s work directly implied several modern statistical computing tech-
niques that Fisher could not have employed for purely mechanistic reasons. These include
bootstrapping (the bootstrap plug-in principle is anticipated by the calculation of Fisher
Information and Rubin [1981] shows how the bootstrap can be made explicitly Bayesian),
empirical calculation of conﬁdence/credible intervals, empirical Bayes (developing a prior
using the data), and Bayes Factors.
10.8
Exercises
10.1
Find the values of α1, α2, and α3 that make the following a transition matrix:
⎡
⎢⎣
0.4
α1
0.0
0.3
α2
0.6
0.0
α3
0.4
⎤
⎥⎦.

Basics of Markov Chain Monte Carlo
369
10.2
For the following simple transition matrix:
θ1
θ2

0
1
1
0

.
For an initial state, [0.25, 0.75], describe repeated applications of the transition
matrix: P2, P3, P4, . . .. Is this an ergodic Markov chain?
10.3
Using the transition matrix from Section 10.1.2, run the Markov chain mechanically
(step by step) in R using matrix multiplication. Start with at least two very diﬀerent
initial states. Run the chains for at least ten iterations.
10.4
Consider a simple Ehrenfest urn problem with two urns and only two balls. For
simplicity start with one ball in each urn, then at each step one of the two balls is
selected with probability 0.5 and then placed in the urn that it currently is not in.
So there are only three possible states: [2, 0], [1, 1], [0, 2] Show that this is a Markov
chain, provide the transition matrix, and derive the unique stationary distribution.
10.5
For the following transition matrix, which classes are closed?
⎡
⎢⎢⎢⎢⎢⎢⎣
0.50
0.50
0.00
0.00
0.00
0.00
0.50
0.00
0.50
0.00
0.00
0.00
0.50
0.50
0.00
0.00
0.00
0.75
0.25
0.00
0.50
0.00
0.00
0.00
0.50
⎤
⎥⎥⎥⎥⎥⎥⎦
10.6
Males in three professions have the following probabilities that their primary son
follows them into the same profession: Professor p = 0.4, Plumber p = 0.8, Play-
wright p = 0.2. If the son does not pick the same profession as the patriarch, he
picks with even probability from the other two professions. Each protagonist pos-
sesses at least one male progeny. Produce the transition matrix for this plan and
provide the steady state.
10.7
From the following transition matrix, calculate the stationary distribution for pro-
portions:
⎡
⎢⎣
0.0
0.4
0.6
0.1
0.0
0.9
0.5
0.5
0.0
⎤
⎥⎦.
What is the substantive conclusion from the zeros on the diagonal of this matrix?
10.8
An AR(1) stochastic process (autoregressive) is commonly deﬁned as:
θ[t+1] = ϕθ[t] + Ωt,
where the Ωt values are generated iid from N(0, τ2) for ﬁnite τ2, and the distri-
bution of the θ values is also iid with ﬁnite variance σ2. From the covariance of

370
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
generated values m −1 away from each other, Cov(θ[t+m], θ[t]) = ϕmVar(θ), using
the central limit theorem, show that if this process is stationary that:
θ ∼N(0, σ2),
where:
σ2 =
τ 2(1 + ϕ)
(1 −ϕ2)(1 −ϕ),
and state the necessary restriction necessary on ϕ.
10.9
Develop a Bayesian speciﬁcation using BUGS to model the following counts of the
number of social contacts for children in a daycare center with the objective of
estimating λi, the contact rate:
Person
1
2
3
4
5
6
7
8
9
10
Age (ai)
11
3
2
2
7
5
6
9
7
4
Social Contacts (xi)
22
4
2
2
9
3
4
5
2
6
Use the following speciﬁcation:
λi ∼G(α, β)
α ∼G(1, 1)
β ∼G(0.1, 1)
to produce a posterior summary for λi.
10.10
Show that for a stationary Markov chain that a set of batches (a set of consecutive
draws) of the same batch length have the same joint distribution.
10.11
The table below provides the results of nine postwar elections in Italy by proportion
per political party. The listed parties are:
▷Democrazia Cristiana (DC),
▷Partito Comunista Italiano (PCI),
▷Partito Socialista Italiano (PSI),
▷Partito Socialista Democratico Italiano (PSDI),
▷Partito Repubblicano Italiano (PRI),
▷Partito Liberale Italiano (PLI),
▷Others.
The “Others” category is a collapsing of smaller parties: Partito Radicale (PR),
Democrazia Proletaria (DP), Partito di Unit`a Proletaria per il Comunismo (PdUP),
Movimento Sociale Italiano (MSI), South Tyrol Peoples Party (SVP), Sardinian

Basics of Markov Chain Monte Carlo
371
Action Party (PSA), Valdˆotaine Union (UV), the Monarchists (Mon), and the
Socialist Party of Proletarian Unity (PSIUP). In two cases parties presented joint
election lists and the returns are split across the two parties here. The composi-
tional data suggest a sense of stability for postwar Italian elections even though
Italy has averaged more than one government per year since 1945.
Party
1948
1953
1958
1963
1968
1972
1976
1979
1983
DC
0.485
0.401
0.424
0.383
0.3910
0.388
0.387
0.383
0.329
PCI
0.155
0.226
0.227
0.253
0.2690
0.272
0.344
0.304
0.299
PSI
0.155
0.128
0.142
0.138
0.0725
0.096
0.096
0.098
0.114
PSDI
0.071
0.045
0.045
0.061
0.0725
0.051
0.034
0.038
0.041
PRI
0.025
0.016
0.014
0.014
0.0200
0.029
0.031
0.030
0.051
PLI
0.038
0.030
0.035
0.070
0.0580
0.039
0.013
0.019
0.029
Others
0.071
0.154
0.113
0.081
0.1170
0.125
0.095
0.128
0.137
Source: Instituto Centrale di Statistica, Italia
Develop a multinomial-logistic model for pij as the proportion received by party
i and election j with BUGS, using time as an explanatory variable and specifying
uninformative priors.
10.12
Section 10.3 described carefully how the Gibbs sampler must use the most recent
draws as values on the right-hand-side of the conditional distribution for subsequent
draws. Modify the R code for the Gibbs sampling in Example 10.3.2 such that the
conditioned-upon values of λ and φ are from the previous step not the current step
in the draw for k. Is this now a transient Markov chain?
10.13
Blom, Holst, and Sandell (1994) deﬁne a “homesick” Markov chain as one where
the probability of returning to the starting state after 2m (m > 1) iterations is at
least as large as moving to any other state: p2m(x0, x¬0) ≤p2m(x0, x0). Does the
Markov chain deﬁned by the following transition matrix have homesickness?
P =
⎡
⎢⎢⎢⎢⎣
0.5
0.5
0.0
0.0
0.5
0.0
0.5
0.0
0.0
0.5
0.0
0.5
0.0
0.0
0.5
0.5
⎤
⎥⎥⎥⎥⎦
Do Markov chains become less homesick over time?
10.14
Using the four Metropolis-Hastings samplers from Exercise 18, modify the R plotting
procedures in the Computational Addendum for this chapter to graph these in
four partitions of the same graph.
10.15
Koppel (1999) studies political control in hybrid organizations (semi-governmental)

372
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
through an analysis of government purchase of a speciﬁc type of venture capital
funds: investment funds sponsored by the Overseas Private Investment Corporation
(OPIC). The following data (opic.df in BaMprovide three variables as of January
1999.
Fund
Age
Status
Size ($M)
AIG Brunswick Millennium
3
Investing
300
Aqua International Partners
2
Investing
300
Newbridge Andean Capital Partners
4
Investing
250
PBO Property
1
Investing
240
First NIS Regional
5
Investing
200
South America Private Equity Growth
4
Investing
180
Russia Partners
5
Investing
155
South Asia Capital
3
Investing
150
Modern Africa Growth and Investment
2
Investing
150
India Private Equity
4
Investing
140
New Africa Opportunity
3
Investing
120
Global Environmental Emerging II
2
Investing
120
Bancroft Eastern Europe
3
Investing
100
Agribusiness Partners International
4
Investing
95
Caucus
1
Raising
92
Asia Paciﬁc Growth
7
Divesting
75
Global Environmental Emerging I
5
Invested
70
Poland Partners
5
Invested
64
Emerging Europe
3
Investing
60
West Bank/Gaza and Jordan
2
Raising
60
Draper International India
3
Investing
55
EnterArab Investment
3
Investing
45
Israel Growth
5
Investing
40
Africa Growth
8
Divesting
25
Allied Capital Small Business
4
Divesting
20
Develop a model using BUGS where the size of the fund is modeled by the age of
the fund and its investment status according to the speciﬁcation:
Yi ∼N(mi, τ)
mi = β0 + β1X1i + β2X2i + ϵi
ϵ ∼N(0, k),
where: Y is the size of the fund, X1 is the age of the fund, and X2 is a dichotomous
explanatory variable equal to one if the fund is investing and zero otherwise. Set
a value for the constant k and an appropriate prior distribution for τ. Summarize
the posterior distributions of the unknown parameters of interest.
10.16
Another special case of the Metropolis-Hastings algorithm is the multiple-try Metropo-
lis algorithm (Liu, Liang, and Wong 2000) with local optimization steps.
It is
designed to be more computationally eﬃcient with diﬃcult posterior shapes. As-
suming the current position to be θ, the steps for a single iteration are:

Basics of Markov Chain Monte Carlo
373
▷From a proposal density, q(θ′|θ), draw θ∗
1, . . . , θ∗
k.
▷Deﬁne a non-negative arbitrary function λ(θ∗
i , θ).
▷For each θ∗
i calculate ω(θ∗
i , θ) = π(θ∗
i )q(θ|θ∗
i )λ(θ∗
i , θ).
▷Draw a single θ∗proportional to these weights.
▷Set θ1 = θ (current chain position), and draw θ1, . . . , θ from q(θ|θ∗)
▷Accept θ∗with the Metropolis decision probability:
rq = min
'
1, ω(θ∗
1, θ) + · · · ω(θ∗
k, θ)
ω(θ1, θ∗) + · · · ω(θk, θ∗)
(
or reject with probability 1 −rq.
Implement this algorithm in R to produce samples from a target density π(θ) that
is a correlated central bivariate Student’s-t distribution with ρ = 0.9 and ν = 5:
π(θa, θb) =
1
2π

1 −ρ2
'
1 + θ2
a + θ2
b −2ρθaθb
ν(1 −ρ2)
(−ν
2 −1
(see Appendix B for a more general multivariate non-central deﬁnition).
10.17
(Norris 1997) A Markov chain is reversible if the distribution of θn|θn+1 = t is the
same as the θn|θn−1 = t. This means that direction of time does not alter the
properties of the chain. Show that the following irreducible matrix does not deﬁne
a reversible Markov chain.
⎡
⎢⎣
0
p
1 −p
1 −p
0
p
p
1 −p
0
⎤
⎥⎦.
See Besag et al. (1995) for details on reversibility.
10.18
(Chib and Greenberg 1995).
To produce draws from a bivariate normal target
distribution that is normal with:
μ = c(1, 2),
and
Σ =

1.0
0.9
0.9
1.0

,
write a Metropolis-Hastings algorithm in R with the following alternative candidate-
generating strategies with the goal of a 40% to 50% acceptance rate:
▷a random walk where the oﬀset is a bivariate uniform with ranges [−0.75 : 0.75]
for the ﬁrst dimension and [−1, 1] for the second dimension.
▷a random walk where the oﬀset is a bivariate normal with mean (0, 0) and
variance [ 0.6
0
0
0.4 ].
▷a pseudorejection scheme where independent candidates are ﬁltered through
an acceptance-rejection step using the dominating function given by
ch(x)c(2π)−1|D|−1
2 exp
	
−1
2(x −μ)′D(x −μ)

, where D = [ 2 0
0 2 ] and c = 0.9
(use μ = c(1, 2) from above).

374
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
▷an autoregressive density with θ[t+1] = μ −(θ[t] −μ) + Ω, where Ω is an
independent bivariate uniform drawn in both dimensions from [0 : 1]. Note
that this reﬂects the draw to the other side of μ then applies an oﬀset.
10.19
(Grimmett and Stirzaker 1992) A random walk is recurrent if the mean size of the
jumps is zero. Deﬁne a random walk on the integers by the transition from integer
i to either integer i + 2 or i −1 with probabilities:
p(i, i + 2) = p,
p(i, i −1) = 1 −p.
A random walk is recurrent if the mean recurrence time,  nfii(n), is ﬁnite, oth-
erwise it is transient. What values of p make this random walk recurrent?
10.20
The following data give the number of casualties for 103 suicide attacks in Israel
with explosives over a three-year period from November 6, 2000 to November 3,
2003 when there was a steep drop (the early period of the ﬁrst “Intifada”). These
data are provided by the International Policy Institute for Counter-Terrorism, and
subsetted by Harrison (2006), and modeled in Kyung et al. (2011).
0
3
81
38
29
126
6
10
1
1
67
50
3
27
0
2
0
63
15
58
57
0
0
0
0
123
4
71
71
20
17
65
4
49
5
35
57
71
0
12
67
59
5
52
62
0
75
0
0
106
30
0
3
45
4
31
32
180
0
1
91
49
61
51
3
0
1
9
0
2
151
26
8
8
75
199
12
2
2
1
93
0
13
21
145
0
0
13
0
2
141
2
65
0
105
0
61
6
27
53
20
5
0
There are two modes, with the larger one at zero. Fit a two-component gamma
mixture model using data augmentation for p (the probability of being in the right-
mode) with a beta distribution, and with a binary assignment vector as part of the
Gibbs sampler. At iteration t the steps are:
▷Draw p from BE(α + n1, β + n2).
▷For each case i, draw Ii ∼BR(p).
▷If Ii = 0, draw from G(α0, β0).
▷If Ii = 1, draw from G(α1, β1).
▷Determine n1 and n2 from the Ii assignments.

Basics of Markov Chain Monte Carlo
375
Here α, β, α0,β0, α0, and β0 are hyperprior parameters and n = n1 + n2 at that
point in the sampler (assign starting values).
10.9
Computational Addendum: Simple R Graphing Routines for
MCMC
This addendum gives the background for the graphing R code in this chapter. In the
next chapter we develop Bayesian MCMC solutions for more complex, and realistic, models
The BUGS package is recommended in general for implementing MCMC estimation models.
Figure 10.2 was produced using the following function for graphing the path of a Gibbs
sampler above in two chosen dimensions. You must give it values indicating which two
columns of the input matrix to graph since it is a two-dimensional plot. For larger dimen-
sional joint posteriors, modiﬁcations to this function are easy to perform.
plot.walk.G <- function(walk.mat,sim.rm,X=1,Y=2)
{
plot(walk.mat[1,X],walk.mat[1,Y],type="n", xlim=range(walk.mat[,X]),
ylim=range(walk.mat[,Y]), xlab="",ylab="")
for(i in 1:(nrow(walk.mat)-1))
{
segments(walk.mat[i,X],walk.mat[i,Y],
walk.mat[(i+1),X],walk.mat[i,Y])
segments(walk.mat[(i+1),X],walk.mat[i,Y],
walk.mat[(i+1),X],walk.mat[(i+1),Y])
}
}
par(mfrow=c(1,2),mar=c(2,3,1,1),oma=c(1,1,3,1))
plot.walk.G(theta.matrix[1:100,],X=1,Y=3)
mtext(outer=TRUE,side=2,cex=1.3,expression(k))
mtext(outer=FALSE,side=3,cex=1.3,expression(lambda),line=2)
plot.walk.G(theta.matrix[1:100,],X=2,Y=3)
mtext(outer=FALSE,side=3,cex=1.3,expression(phi),line=2)
The following R code produced Figure 10.4 for this Metropolis-Hastings output in two
dimensions. It takes a matrix where the rows indicate chain iterations.
plot.walk.MH <- function(walk.mat)
{
plot(walk.mat[1,1],walk.mat[1,2],type="n",
xlim=round(range(walk.mat[,1])*1.2),
ylim=round(range(walk.mat[,2])*1.2),
xlab="",ylab="")
for(i in 1:(nrow(walk.mat)-1))
{
segments(walk.mat[i,1],walk.mat[i,2],

376
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
walk.mat[(i+1),1],walk.mat[(i+1),2])
}
}
par(mfrow=c(1,2),mar=c(2,2,2,2),oma=c(1,1,3,1))
plot(theta.matrix[801:1000,],pch=".",xlab="",ylab="",
xlim=c(-3,3), ylim=c(-3,3),cex=3)
plot.walk.MH(theta.matrix[801:1000,])
mtext(outer=TRUE,side=3,cex=1.2,
"Metropolis-Hastings Demonstration, Bivariate Normal")
The previous function needs to produce bivariate normal density values, and a function
for doing this follows. It is currently written only for bivariate calculations, but it can easily
be vectorized to accommodate higher dimensions.
dmultinorm <- function(xval,yval,mu.vector,sigma.matrix)
{
normalizer <- (2*pi*sigma.matrix[1,1]*sigma.matrix[2,2]
*sqrt(1-sigma.matrix[1,2]^2))^(-1)
like <- exp(-(1/(2*(1-sigma.matrix[1,2]^2)))* (
((xval-mu.vector[1])/sigma.matrix[1,1])^2
-2*sigma.matrix[1,2]*(((xval-mu.vector[1])
/sigma.matrix[1,1])*
((yval-mu.vector[2])/sigma.matrix[2,2]))
+((yval-mu.vector[2])/sigma.matrix[2,2])^2 ))
normalizer*like
}
This last code segment implements the Hit-and-Run example along the graph on page 362.
num.sims <- 10000
Sig.mat <- matrix(c(1.0,0.95,0.95,1.0),2,2)
walks<-rbind(c(-3,-3),matrix(NA,nrow=(num.sims-1),ncol=2))
walks <- hit.run(walks,num.sims,Sig.mat)
z.grid <- outer(seq(-3,3,length=100),seq(-3,3,length=100),
FUN=dmultinorm,c(0,0),Sig.mat)
contour(seq(-3,3,length=100),seq(-3,3,length=100),z.grid,
levels=c(0.05,0.1,0.2))
points(walks[5001:num.sims,],pch=".")

Chapter 11
Implementing Bayesian Models with
Markov Chain Monte Carlo
11.1
Introduction to Bayesian Software Solutions
While many models can be run in R and other programming environments, as demon-
strated in previous chapters, it is necessary to use Markov chain Monte Carlo procedures to
ﬁt some realistic and useful social and behavioral science Bayesian models. As noted, the
revolution began with the review paper of Gelfand and Smith (1990), and there were simply
a class of models with hierarchies and other complex features before 1990 that couldn’t be
marginalized to produce a regression table. In this chapter we focus exclusively on the
details of running MCMC with the BUGS language (Lunn et al. 2000). This will enable the
attentive reader to immediately begin running MCMC procedures.
The exposition here is introductory and cannot be comprehensive due to space limita-
tions. However, there are many high-quality texts dedicated to the BUGS language and its
variations. The recent book by Lunn et al. (2012) stands out, perhaps because of the overlap
of authors with the WinBUGS package. The text by Ntzoufras (2009) has detailed coverage
of the language and guidance for standard models. Congdon’s (2001, 2003, 2005, 2010) four
texts provide an extensive library of model implementations in BUGS with accompanying
descriptions. The book by K´ery (2010) is a nice basic introduction to WinBUGS, and also
gives guidance on calling WinBUGS from R (although the applied focus is ecology not social
science). There is also a follow-on work by K´ery and Schaub (2011) with hierarchical mod-
eling as a focus. Kruschke (2010) produced an accessible introduction to Bayesian basics,
with an orientation towards the behavioral sciences, that includes examples and exercises
in BUGS.
Finally, this chapter is not meant to imply that the only way that one can estimate
complex models with MCMC is to use the BUGS language. Many researchers write their
own samplers directly in R (often calling C or FORTRAN for the highly repetitive parts, or
increasingly in MATLAB). There is an excellent and exhaustive inventory of R resources for
Bayesian analysis in the CRAN Task View on Bayesian Inference. As a reminder all R and
BUGS code in this text are contained in the BaM package at CRAN. All updates to code and
data are maintained in that package and therefore the guidance in this chapter will remain
377

378
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
fairly generic and general since software changes outpace publisher changes by a considerable
margin.
11.2
It’s Only a Name: BUGS
In a spectacularly bad example of naming computer software, the Cambridge MRC
Biostatistics Unit distributed BUGS for Unix and DOS operating systems in 1989, and cor-
responding research papers appeared a few years earlier. This version is now referred to
as “Classic BUGS” and is no longer supported by the Cambridge team. In the early era of
BUGS users were required to register annually to obtain a software key for unrestricted use
(now BUGS is “immortal”). The documentation was sparse, but the software came with 41
examples fully worked out (mostly from biostatistics). These were a splendid learning tool,
particularly in the late 1990s when no resources existed other than trial-and-error. There
are currently 50 detailed examples in three volumes, along with valuable documentation.
Although running these well-documented examples is a good way to learn the language, one
caution is required. It is far better to write one’s own code from scratch after learning the
language than trying to modify these examples to ﬁt the problem at hand. Inevitably the
latter strategy leads to making changes in models and assumptions to ﬁt clean running code,
and this means that the software is then dictating the research rather than the researcher.
While “Classic” BUGS lived only until 1996 (R.I.P.), there are now a variety of ways
to obtain the software and related packages.
And while there are separate estimation
“engines,” they all use approximately the same language and they all interface to other
environments. Many users will prefer WinBUGS,1 which is described in this chapter, since
it has a nice graphical user interface and is relatively easy to learn. As the name implies,
WinBUGS is conﬁned to Windows machines. The follow-on to WinBUGS is openbugs, which
is an open-source version of the package that actually runs models from a “compound
document” that encloses: model and data description text, data tables, plots, model graphs,
code, and more. openbugs runs on Windows and Linux operating systems, but needs the
Wine emulator to run on OS X systems. An increasingly popular alternative is the JAGS
(“Just Another Gibbs Sampler”) program written by Martyn Plummer. This is open source
software that was developed independently of the BUGS project, but uses almost exactly
the same language (diﬀerences will be described in this chapter). The main advantage of
JAGS is that it runs on all commonly used operating system platforms, and Plummer has
added extensible features to allow more development of user-developed functions. In this
chapter both WinBUGS and JAGS models are run, where “WinBUGS” also applies to openbugs.
However, by convention in this text “BUGS” refers to the general language/approach and
1Available at http://www.mrc-bsu.cam.ac.uk/software/bugs/the-bugs-project-winbugs/.

Implementing Bayesian Models with Markov Chain Monte Carlo
379
therefore all three software engines. Points about speciﬁc packages will be made by name
as required.
Most users now run one of the packages listed above called from a standard statistical
software environment, most notably R. The R package R2WinBUGS provides an interface
between R and WinBUGS such that commands in R remotely direct the WinBUGS environment
without the graphical interface. Similarly the R packages Rjags, runjags, bayesmix, and
R2jags run JAGS from within R by commands, but with the advantage of working on all
popular operating systems. Also, openbugs can be run in the same way from R with both the
BRugs and the rbugs packages. John Thompson, Tom Palmer, and Santiago Moreno (2006)
at the University of Leicester have developed stata ado ﬁles to run WinBUGS remotely and
recover the samples. The BUGS Project homepage also oﬀers several ways to remotely call
WinBUGS from SAS. There is also an available a MATLAB helper function called mat2bugs.m
that evokes a WinBUGS interface. Finally, Phil Woodward created a Microsoft Excel add-
in called BugsXLA interfacing WinBUGS that accompanies his text.
The discussion in this
chapter focuses exclusively on running BUGS from R or in a standalone manner.
So what is BUGS? Oﬃcially it is an integrated language for specifying Bayesian models,
and hierarchical models in particular (Chapter 12). It provides a library of sampling routines
along with a text-based user interface for running these samplers. The syntax is intentionally
R-like, but there are vastly fewer commands and some important diﬀerences in how terms
are used. In BUGS the user speciﬁes models in commands that reﬂect statistical thinking:
variable relationships, distributions, hierarchies, and prior distributions. The software then
translates these statements into a set of full conditional distributions that the user does not
see. This is done in several steps that include compiling the written code and initializing
the model checking for dimension mis-matches.
Once this process is ﬁnished, the user
then speciﬁes a number of Gibbs sampling iterations to run and evaluates the monitored
iterations.
Usually the sampled values are post-processed in R, either with standard R
functions or with one of two dedicated packages. This last step is quite easy since the
sampled values are treated mechanically just like regular data, taking means, variances,
quantiles, correlations, etc.
Because it is a Markov chain, this last process can be interrupted and recommenced at
will with no implications: the only information that needs to be saved in order to restart
the chain is the last chain value produced. This is important when considering the status of
convergence with the diagnostics described in Chapter 14. There are other considerations
during the running of the chain, including looking at traceplots that show the path of the
chain in each dimension. Occasionally the BUGS variants will fail to ﬁnd a full conditional
distribution for one or more of the model dimensions using a Gibbs sampler. In this case
the software reverts to a Metropolis-Hastings chain or slice sampling for those dimensions
only, and this is done automatically. It is impossible to overstate the amazing convenience
that BUGS provides. It unburdens the researcher from having to produce the full set of full
conditional distributions for Gibbs sampling or the candidate-generating distribution for
Metropolis-Hastings.

380
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
11.3
Model Speciﬁcation with BUGS
Obviously the most important part of the model development process is understanding
the data and developing a principled statistical speciﬁcation that tests theories of interest
and importance. This chapter focuses on what happens after that part and assumes that the
speciﬁed model already exists. Specifying statistical models within the software production
process is always a poor practice.
There are four general steps to the BUGS modeling process:
1. state the distributional features of the model, and the quantities to be estimated,
2. compile the instructions into a run-time program,
3. run this sampler that produces Markov chains,
4. and summarize the empirical distributions and assess convergence using basic diag-
nostics in BUGS or R.
Each of these steps will be described in detail here with developed examples. First we
make a few notes about the use of the language. Unlike with other programming languages,
statements are not processed serially; they constitute a full speciﬁcation. This may seem
strange to users used to writing code in standard programming languages where the order
of statements is critical. In BUGS the order of statements within the blocks of code is not
important since the software reads the blocks as a complete unit.
Sometimes this also
makes debugging more diﬃcult since the point-of-failure is often not as clear as in a more
serial process. Usually coding errors are detected in the compilation step and most of the
frustration for a new user comes from iterating between steps one and two above. It turns
out that the rest of the process is amazingly easy as we shall see.
There is a host language that comes with the use of BUGS. Nodes are values and vari-
ables in the model that are speciﬁed by the researcher. These come in diﬀerent types and
reﬂect the focus on hierarchical modeling. While we do not develop extensive Bayesian
hierarchical modeling until Chapter 12, it is intuitive to know that some model terms in
Bayesian speciﬁcations depend on other terms in the classic sense of conditional inference
(e.g., f(θ|α, β)). A parent node is one that inﬂuences other nodes: it is higher in the
hierarchy, it is on the right-hand-side of some conditional statement. A descendant node
is the opposite of the parent node in that it is downstream from some node, meaning lower
in the hierarchy. Indicative of the ﬂexibility of hierarchical models, it can also be a parent
node. A founder node is a ﬁxed parameter rather than a variable, and it therefore has
no parents. The opposite of a ﬁxed parameter in the model is a stochastic node, which
is a node that is assumed to have some distribution and these are both parameters and
data. Lastly, a deterministic node is one that is a logical consequences of other nodes. A

Implementing Bayesian Models with Markov Chain Monte Carlo
381
prototypical example is θ in the link from the linear structure, Xβ, to the linear predictor,
θ = g(μ) in the speciﬁcation of a GLM.
There are some additional qualiﬁcations to keep in mind here. With BUGS all priors and
all likelihood functions must be either: (1) discrete, (2) conjugate, or (3) log-concave (the
Gibbs sampling implications of this are discussed in Section 11.5). This is not a big deal
as all GLMs with canonical link functions are well-behaved in this respect. It also means
that deterministic nodes must be linear functions. Interestingly, these restrictions can be
ﬁnessed with clever programming. Also BUGS likes simple, clean model structure, meaning
that embedded variable transformations and heavy data processing tend to bog-down the
sampler and may even lead to crashes. So if pre-processing in R can be done, it generally
helps.
As mentioned, the BUGS language deliberately looks a lot like R syntax. Key identical
components are: the assignment operator, <-, looping and indexing syntax, commenting
beginning with #, and a host of simple functions like exp, log, mean, sd, etc. The notation
for distributional statements is also consistent with R, with some exceptions. Generally
ddist is the identiﬁer for distribution “dist,” and the inventory of available distributions
is extensive (plus new ones can be constructed). Keep in mind that deﬁnitions inside of a
ddist speciﬁcation may diﬀer from R, e.g., dnorm(mu,tau) speciﬁes a mean and precision
not a mean and variance in BUGS. Finally, all variants of BUGS (engines) can be called in
batch mode from pre-constructed ﬁles.
Linear Model Example
Consider economic data from the Organization for Economic Cooperation and Develop-
ment (OECD) that highlights the relationship between commitment to employment protec-
tion measured on an interval scale (0 to 4) indicating the quantity and extent of national
legislation to protect jobs, and the total factor productivity diﬀerence in growth rates be-
tween 1980-1990 and 1990-1998 (see The Economist, September 23, 2000 for a discussion).
The original data appear in Table 11.1 and these points are graphed in Figure 11.1.
TABLE 11.1:
OECD Protection versus Productivity
Prot.
Prod.
Prot.
Prod.
Prot.
Prod.
United States
0.2
0.5
Canada
0.6
0.6
Australia
1.1
1.3
New Zealand
1.0
0.4
Ireland
1.0
0.1
Denmark
2.0
0.9
Finland
2.2
0.7
Austria
2.4
-0.1
Belgium
2.5
-0.4
Japan
2.6
-0.4
Sweden
2.9
0.5
Netherlands
2.8
-0.5
France
2.9
-0.9
Germany
3.2
-0.2
Greece
3.6
-0.3
Portugal
3.9
0.3
Italy
3.8
-0.3
Spain
3.5
-1.5

382
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
We know from Gauss-Markov theory that the posterior distribution of both the intercept
and the slope coeﬃcients is Student’s-t with n −k −1 = 16 degrees of freedom. So why are
we running BUGS on a linear model? Consider how diﬀerent the estimation process really is
here:
ˆb = (X′X)−1X′y
versus
α1 ∼f(α|β0),
β1 ∼f(β|α1)
α2 ∼f(α|β1),
β2 ∼f(β|α2)
:
:
αm ∼f(α|βm−1),
βm ∼f(β|αm).
not to mention the potential eﬀect of priors! So this example is an implied test of the
integrity of the MCMC process.
0
1
2
3
4
−1.5
−0.5
0.0
0.5
1.0
1.5
UnitedStates
Canada
Australia
NewZealand
Ireland
Denmark
Finland
Austria
Belgium Japan
Sweden
Netherlands
France
Germany
Greece
Portugal
Italy
Spain
Employment Protection Scale
Total Factor Productivity Difference
FIGURE 11.1: OECD Employment and Productivity Data

Implementing Bayesian Models with Markov Chain Monte Carlo
383
11.3.1
Model Speciﬁcation
Two primary blocks of code need to be combined to create a full model statement,
which wraps model { } around the whole process. The general program structure (not
actual model statements) looks like this:
MODEL NAME
DECLARATIONS
{
LOOPING THROUGH DATA
DISTRIBUTIONAL STATEMENTS
}
where the part between the curly-braces is called the model description, and it has two parts
where the ordering of these parts does not matter. The ﬁrst line, MODEL NAME, consists of
two parts: (1) the word model is necessary to indicate the start of a model speciﬁcation,
and (2) an optional model name. Regarding the latter, it is sometimes convenient to have a
model name to distinguish individual speciﬁcations when many similar models are created.
Many people ignore this feature. The second variable declaration statement identiﬁes the
set of the nodes to be used and their size.
There is no variable declaration process in
WinBUGS, whereas Classic BUGS required this. Both JAGS and WinBUGS use model statements
(stipulated relations and inheritance from parents) and data size to determine the size
of “undeclared” nodes. However, variable declaration is optional in JAGS. If compilation
problems arise, one solution may be declaring variables, and it is also a convenient reminder
when working on the rest of the model. For the OECD model, the optional JAGS declaration
statement is:
var x[N], y[N], mu[N], alpha, beta, tau
Generally these statements are only necessary when stipulating objects of higher dimension,
but giving variable declarations is also not harmful under basic circumstances. The model
description is mainly contained in the LOOPING THROUGH DATA part, and this contains the
statistical structure of the code. For the OECD example it is:
mu[i] <- alpha + beta*x[i]
y[i]
~
dnorm(mu[i],tau)
where the indexing of i indicates processing through the data, i = 1, . . . , N. The ﬁrst line
above is a GLM linear additive component collected into μ. This is a linear model so no link
function is required. The second line above states that y has a normal distribution around
μ with precision τ. It is important to remember that the second parameter in a dnorm()
statement is a precision not a variance. The rest of the model description is contained
in DISTRIBUTIONAL STATEMENTS where all of the unknown variables, except deterministic
nodes, are listed along with their prior distribution assumptions. For the OECD example,
these are:

384
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
alpha ~ dnorm(0.0,0.001)
beta
~ dnorm(0.0,0.001)
tau
~ dgamma(1,0.1)
Observe that these are parent nodes since they are higher in the hierarchy than μ, x, and
y. The priors in this example are deliberately diﬀuse so as to mimic a regular non-Bayesian
linear model as much as possible. We also need to take care of indexing with the ﬁrst block
above by specifying a “for” loop in exactly the same way as R. So the complete speciﬁcation
for the OECD model is:
model {
for (i in 1:N) {
mu[i] <- alpha + beta*x[i]
y[i] ~ dnorm(mu[i],tau)
}
alpha ~ dnorm(0.0,0.001)
beta
~ dnorm(0.0,0.001)
tau
~ dgamma(1,0.1)
}
The looping in the BUGS model here is through the data with i. The production of chain
values is not stipulated by the user in this code since chain values are generated after
compilation of the model. This is an important principle to remember.
Since it is a simple linear model, the code above can be run in WinBUGS, JAGS, or
openbugs exactly as written for each.
In both WinBUGS and JAGS the program code is
stored in a text ﬁle, but WinBUGS users often keep the model in the graphical environment
model window for easy of manipulation. We still have to specify the data ﬁle. Unfortunately
WinBUGS and JAGS have diﬀerent types of data statements, but fortunately, they both use
R language formats. WinBUGS uses R list format, so the data from Table 11.1 is speciﬁed by:
list(x= c(0.20, 0.60,
1.10,
1.00,
1.00,
2.00,
2.20,
2.40,
2.50,
2.82, 2.90,
2.80,
2.90,
3.20,
3.60,
3.90,
3.90,
3.50),
y= c(0.50, 0.60,
1.30,
0.40,
0.10,
0.90,
0.70, -0.10, -0.40,
-0.40, 0.50, -0.60, -0.90, -0.20, -0.30,
0.30, -0.30, -1.50),
N=18
)
There is an alternative matrix-structured rectangular ﬁle format for WinBUGS that some
users may prefer, although constants such as N=18 must be handled separately (see Lunn
et al. 2012, p.303 or Ntzoufras 2009, p.127).
Notice that the sample size, N is in the data
statement rather than inside the for loop. This is good programming practice since code
is often recycled for future work with diﬀerent data, and forgetting to change for (i in
1:18) is remarkably easy to do. Another good practice is writing the looping statement
conditional on the data and therefore removing the possibility of future sizing mistakes:
for (i in 1:length(Y)) {

Implementing Bayesian Models with Markov Chain Monte Carlo
385
Users of WinBUGS often keep the data list in the same text window as the model statement
(below it), particularly if the data size is small, since this is a convenience in the point-and-
click environment of WinBUGS. However, JAGS is not an enclosed environment, so this is not
possible, and the data are given in a ﬁle. A JAGS data ﬁle uses the R vector and matrix
assignment convention, giving:
x <- c(0.20, 0.60,
1.10,
1.00,
1.00,
2.00,
2.20,
2.40,
2.50,
2.82, 2.90,
2.80,
2.90,
3.20,
3.60,
3.90,
3.90,
3.50)
y <- c(0.50, 0.60,
1.30,
0.40,
0.10,
0.90,
0.70, -0.10, -0.40,
-0.40, 0.50, -0.60, -0.90, -0.20, -0.30,
0.30, -0.30, -1.50)
N=18
which is stored in the ﬁle oecd.jags.dat. There are no commas between these data objects
as required in the list format above.
We also need to stipulate starting values for the Markov chain, and there is a diﬀerence
between WinBUGS and JAGS here too. The format in WinBUGS is again the R list format:
list(alpha = 0.0, beta = 0.0, tau = 1.0)
and the format in JAGS is the scalar assignment form stored in the ﬁle oecd.jags.init:
alpha <- 0.0
beta
<- 0.0
tau
<- 1.0
(although vectors can be speciﬁed too). We now have all of the pieces required to run the
model. The next steps are to compile the model in BUGS and run the chain, recording values.
11.3.2
Running the Model in WinBUGS
The WinBUGS software has lots of “bells and whistles” to explore, such as running the
model straight from the doodle (graphical summary, see the examples that are furnished
with the software), and printing summary functions. The data from any plot can be recov-
ered by double-clicking on it. Setting the seed may be important to you: leaving the seed
as is exactly replicates chains. The WinBUGS interface uses what is called the compound
document interface. This is an omnibus ﬁle format that holds: text, tables, code, formulae,
plots, data, and initial values. The goal is to minimize cross-applications work by centraliz-
ing the model development process in one piece of software. Some useful features include: if
one of the CDI elements is focused, its associated tools are made available; a built-in editor;
and online documentation that has details about creating ﬁles.
The ﬁrst window required is the speciﬁcation tool, which requires that you use the
following buttons:
▷check model: checks the syntax of your code.
▷load data: loads data from same or other ﬁle.
▷num of chains: sets number of parallel chains to run.

386
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
▷compile: compiles your code as speciﬁed.
▷load inits: loads the starting values for the chain(s).
▷gen inits: lets WinBUGS specify initial values.
It is important that these steps are performed in this order (the order of the buttons looks
like a question-mark in this window). The speciﬁcation tool window can be dismissed when
done. The update window is the manner in which chain values are generated. The key
buttons are:
▷updates: you specify the number of chain iterations to run this cycle.
▷refresh: the number of updates between screen redraws for traceplots and other dis-
plays.
▷update: hit this button to begin iterations.
▷thin: number of values to thin out of chain between saved values.
▷iteration: current status of iterations, by UPDATE parameter.
▷over relax: click in the box for option to
▷generate multiple samples at each cycle,
▷pick sample with greatest negative correlation to current value.
Trades cycle time for mixing qualities.
▷adapting: box will be automatically clicked while the algorithm for Metropolis or
slice sampling (using intentionally introduced auxiliary variables to improve conver-
gence and mixing) is still tuning optimization parameters (4000 and 500 iterations,
respectively). Other options are “greyed out” during this period.
The sampling window is the primary mechanism for stipulating nodes to be monitored and
summarizing the resulting chains. Its buttons are:
▷node: sets each node of interest for monitoring; type name and click SET for each
variable of interest.
▷Use the “ * ” in the window when you are done to do a full monitor.
▷chains: “1 to 10” sets subsets of chains to monitor if multiple chains are being run.
▷beg, end: the beginning and ending chain values current to be monitored. BEG is 1
unless you know the burn-in period.
▷thin: yet another opportunity to thin the chain.
▷clear: clear a node from being monitored.
▷trace: do dynamic traceplots for monitored nodes.
▷history: display a traceplot for the complete history.
▷density: display a kernel density estimate.
▷quantiles: displays running mean with running 95% CI by iteration number.

Implementing Bayesian Models with Markov Chain Monte Carlo
387
▷auto cor: plots of autocorrelations for each node with lags 1 to 50.
▷coda: display one window with the monitored chain history in “CODA” format (one
long vector starting with the history of the ﬁrst parameter followed by the others), and
ordering information for this vector that record the location parameter boundaries in
this vector.
▷stats: summary statistics on each monitored node using: mean, sd, MC error, current
iteration value, starting point of chain and percentiles from PERCENTILES window.
▷Notes on stats:
WinBUGS regularly provides both: naive SE = sample variance/√n and: MC Error =
√spectral density var/√n = asymptotic SE.
There are quite a few additional windows and pull-down features, but those described
above are the essential group for running MCMC in WinBUGS. Note that WinBUGS gives two
types of measures of uncertainty in the stats information. The “MC error” is Monte Carlo
error (see the discussion starting on page 9.4 in Chapter 9), which can be reduced with
additional iterations. The result labeled “sd” is the conventional posterior variance of the
mean estimate, which is the quantity that should be reported in regression tables.
The following steps are given for the Classic BUGS where each command corresponds to
a speciﬁc button in the WinBUGS GUI as just described. Here we run the Markov chain for
10,000 iterations without recording visited values, then monitor the nodes and run it an
additional 50,000 values while recording:
▷Compile:
Bugs>compile("oecd.bug")
▷Run the chain for a burn-in period:
Bugs>update(10000)
time for
10000
updates was
00:00:01
▷Turn on chain value recording:
Bugs>monitor(alpha)
Bugs>monitor(beta)
▷Run the chain for a much longer series of values:
Bugs>update(50000)
time for 50000 updates was 00:00:05
▷Ask for summary statistics:
Bugs>stats(alpha)
mean
sd
2.5% : 97.5%
CI
median
sample
8.619E-1
3.402E-1
1.734E-1
1.527E+0
8.650E-1
50000
Bugs>stats(beta)
mean
sd
2.5% : 97.5%
CI
median
sample
-3.507E-1
1.299E-1 -6.065E-1 -8.877E-2
-3.525E-1
50000

388
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
So WinBUGS goes all the way to regression-style results. As we shall see, there are good
reasons to use the CODA button and analyze the results in R with the CODA package described
further in Chapter 14. Note that a burn-in period of 10,000 and mixing period of 50,000
is clearly over-kill for this simple model, but since it mixes so quickly these values do not
inconvenience. This issue is discussed in Chapter 14.
11.3.3
Running the Model in JAGS
Calling JAGS from R is described below, and this section shows how to run the model
from a JAGS text window. After downloading and installing the software, JAGS can be called
directly to produce a command window. Since we have saved the ﬁles containing code, data,
and starting values, we now use these in command form in this window. The same process
as done with WinBUGS above is completed by executing the commands:
model in "oecd.jags"
data in "oecd.jags.dat"
compile
inits in "oecd.jags.init"
initialize
update 10000
monitor set alpha
monitor set beta
update 50000
coda *
exit
We could also put these commands in a ﬁle (e.g., my.command.file) and simply type jags
my.command.file at the prompt. Note that there may be path names required before the
ﬁle names above, depending on where in the ﬁle system JAGS is started.
These in-line
statements are exactly analogous to the WinBUGS consecutive buttons above, except that we
use the coda * statement to drop a ﬁle for R to ingest. Both WinBUGS and JAGS are capable
of running multiple parallel chains at the same time. This facilitates convergence diagnostic
discussions in Chapter 14. The JAGS window will display the following, indicating positive
completion:
. Reading data file oecd.jags.dat
. Compiling model graph
Resolving undeclared variables
Allocating nodes
Graph Size: 74
. Reading initial values file oecd.jags.init
. . Updating 10000
---------------------------------------| 10000
**************************************** 100%
. . . Updating 50000

Implementing Bayesian Models with Markov Chain Monte Carlo
389
---------------------------------------| 50000
**************************************** 100%
To run parallel chains in JAGS from diﬀerent starting points, we repeat the initial value
statements in diﬀerent ﬁles and then replace the compile and inits in statements above
with:
compile, nchains(3)
inits in "oecd-init1.R"
inits in "oecd-init2.R"
inits in "oecd-init3.R"
for three parallel chains. Here we would have to produce three initial value ﬁles instead
of one, but it is typically trivial to copy the ﬁrst and make changes to the values in the
resulting copies.
TABLE 11.2:
OECD Model Results
OLS Estimation
MCMC Posterior
Estimate
Std. Error
Mean
Std. Error
Intercept
0.859
0.317
0.859
0.322
Slope
-0.349
0.121
-0.349
0.123
Using the posterior mean as a point estimate from the WinBUGS or JAGS output above,
we can compare with lm in R. This is shown in Table 11.2. Observe that the posterior means
are identical (rounded to three places), but the posterior variances are slightly bigger. This
makes theoretical sense: the Bayesian model has prior distributions on α and β, whereas
the non-Bayesian model assumes a ﬁxed underlying parameter.
Suppose we want to calculate some standard model summary quantities in the context of
BUGS estimation. One set of values that we might be interested in are the linear predictions,
the ˆy values. This is done by simply monitoring the node μ in the sampler since mu[i] <-
alpha + beta*x[i]. Another typically reported quantity is the estimate of the standard
error of y, ˆσ. Recall that dnorm requires speciﬁcation of the precision not the variance, so we
can get the required quantity by adding sigma <- pow(tau,-2) to the code outside of the
i loop and monitoring this new node. While the R2 measure is a fairly blunt measure (it is
quadratic in [0:1] not linear, and is technically not a statistic), it is commonly reported for
linear models. The easiest way to obtain it is to ﬁrst create a deterministic node y.hat[i]
<- mu[i] (we could work with μ, but this makes the code easier to read), then accumulate
regression sum of squares contributions and total sum of square contributions, and outside
the loop perform R2 <- sum(SSR)/sum(SST). We want to remember to monitor any new
nodes of interest. So with all of these enhancements, the new model is given by:

390
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
model {
for (i in 1:n) {
mu[i] <- alpha + beta*x[i];
y[i] ~ dnorm(mu[i],tau);
y.hat[i] <- mu[i]
SSR[i] <- (y.hat[i] - mean(y))^2
SST[i] <- (y[i] - mean(y))^2
}
sigma <- pow(tau,-2)
R2 <- sum(SSR)/sum(SST)
alpha ~ dnorm(0.0,0.001)
beta
~ dnorm(0.0,0.001)
tau
~ dgamma(1,0.1)
}
Obviously this adds some computational burden since we are making more work inside the
loop, which is wholly repeated many times as we iterate the chain values. However, with a
linear model this is rarely an annoying addition to run-time. With more complicated models,
values that do not change across iterations, like mean(y), would be taken out of the loop
and calculated beforehand and used as a constant. Similarly, we could output intermediate
values that allow us to perform the ﬁnal R2 calculation in R. Notice that this approach
implies that R2 is a statistic since it will have variability across chain iterations. This is not
true. It is simply the calculation of a deterministic measure, given the current chain values
for α and β, which give a ˆy vector. Since we do not want to imply a distribution for R2 it is
better to summarize the realizations with a median (m = 0.3763). This is the same reason
that we would not necessarily want to post-process based on the posterior mean only since
that elevates it as the only considerable point summary. Finally, this model extension is
intended to show that additional features can be added as desired.
■Example 11.1:
A Logit Model of HMO Eﬀectiveness Here we look at a less
contrived example regarding public policy for healthcare delivery. Health Maintenance
Organizations (HMOs) are private entities that provide managed care directly to
patients with the goal of cost containment through contractual services. The data
here include 1,180 children in Florida who visited their HMO clinic and did or did
not subsequently require an emergency room visit shortly thereafter. The question is
whether the HMO visit adequately addressed the child’s condition. We will focus on
only three variables:
▷erodd, the dichotomous outcome variable, [0, 1], indicating whether or not there
was an emergency room visit.
▷np, indication of proﬁt, [1], or nonproﬁt, [−1], status of the HMO. This is the
key explanatory variable of interest.
▷metq, a severity score, [1, 2, 3], indicating the degree of illness diagnosed at the
HMO visit. This is a required control variable.

Implementing Bayesian Models with Markov Chain Monte Carlo
391
The speciﬁcation is a simple logit GLM with diﬀuse priors (the second parameter is
a variance in these deﬁnitional statements):
eroddi ∼BE(pi)
logit(pi) = α0 + α1npi + α2metqi
α0 ∼N(0, 10)
α1 ∼N(0, 10)
α2 ∼N(0, 10)
where the results of interest are the posterior distributions of α0, α1, and α2 (10 is
the variance above, so we will use 0.10 as the precision below). So the BUGS code is:
model
{
for( i in 1 : N ) {
logit(p[i]) <- alpha0 + alpha1 * np[i]
+ alpha2 * metq[i]
erodd[i] ~ dbern(p[i])
}
alpha0 ~ dnorm(0.0,0.1)
alpha1 ~ dnorm(0.0,0.1)
alpha2 ~ dnorm(0.0,0.1)
}
Notice that the second parameter in the normal distributional statements is now a
precision. The WinBUGS data and initial value statements are:
list(erodd = c(1,1,1,0,1,0,1,...),
np = c(-1,-1,-1,1,-1,-1,1,...),
metq = c(3,3,3,3,3,2,3,...),
N=1180)
list(alpha0 = 0, alpha1 = 0, alpha2 = 0)
where the dots indicate continuation of the data. The JAGS data ﬁle looks like:
erodd <- c(1,1,1,0,1,0,1,...)
np = c(-1,-1,-1,1,-1,-1,1,...)
metq = c(3,3,3,3,3,2,3,...)
N=1180
and the JAGS initial values ﬁle contains:

392
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
alpha0 = 0
alpha1 = 0
alpha2 = 0
These data can be obtained from the BaM package in R. The chain is run with a burn-in
period of 1,000 cycles and then run for 9,000 more, producing the following WinBUGS
output:
node
mean
sd
MC error
2.5%
median
97.5%
st. sample
alpha0
-1.971
0.22280
0.008254
-2.41300
-1.968
-1.540
1
9000
alpha1
0.1646
0.08042
9.194E-4
0.008893
0.1639
0.3213
1
9000
alpha2
0.2808
0.09423
0.003505
0.098070
0.2803
0.4645
1
9000
So there is evidence that an HMO run as a for-proﬁt corporation (usually owned by a
set of physicians and investors) has more children that need emergency room medical
care shortly after their HMO visit. This is seen by the positive posterior mean for
α1, and the 95% credible interval bounded away from zero. The control variable for
injury/illness severity also has a positive posterior coeﬃcient mean, as expected.
11.4
Diﬀerences between WinBUGS and JAGS Code
We have already seen that there are diﬀerences in ﬁle formats between WinBUGS and JAGS.
These diﬀerences are easy to maintain with two functions. Terry Elrod’s WriteDatafileR
takes an R dataframe and writes it to a ﬁle in a speciﬁed directory. This makes it easy
to manipulate a dataset in R but instantly have a properly conﬁgured ﬁle for WinBUGS.
Unfortunately this function is not available at CRAN, but it is available on the dedicated
webpage for this text and elsewhere online. There is no corresponding function for JAGS,
but the R package CODA contains the function bugs2jags that converts a WinBUGS formatted
data ﬁle to a JAGS formatted data ﬁle. So consecutive use of these two functions gives JAGS
users a properly formatted ﬁle after manipulation in R.
The most important diﬀerence between WinBUGS and JAGS is the handling of censoring
and truncation. In WinBUGS the I(,) construct is used in WinBUGS for censoring as a
posterior restriction, as well as truncation of top-level parameters as a prior restriction. The
format is z ∼ddist(theta)I(lower, upper) for a restriction on an unobserved z which
is distributed according to ddist (dnorm, dgamma, etc.) with a parameter θ, where this
distribution exists only between the numerical values indicated by lower and upper. One-
sided restrictions are achieved by omitting one of the restrictions: I(lower,) or I(,upper).
If the variable z becomes observed, then these restrictions are ignored. This construct should
only be used on model nodes and not to restrict the distribution of data that produces a
likelihood function. Such data restrictions should be stipulated in the algebraic form of the

Implementing Bayesian Models with Markov Chain Monte Carlo
393
likelihood. The WinBUGS documentation also warns that if z, θ, and the limits are unknown,
then the limits cannot be modeled as functions of θ. However, I(,) can have nodes inside
the function, e.g. I(,z), I(y,) to force ordering of the nodes y < z.
In contrast, JAGS separates censoring and truncation into two functions. For simple
truncation the T(lower,upper) function is used, and it works like the I(lower,upper)
function in WinBUGS. The interpretation is that the truncated variable is known a priori
100% to lie between lower and upper, which can be numeric values or other nodes for
ordering. Actually, it is possible to specify I(,) as well in JAGS, but only if the distribution
being truncated is fully known (no parameters to be estimated in ddist). Censoring in JAGS
is represented by the novel distribution dinterval, which is not in WinBUGS. For example,
suppose we had data that had left-censored values:
bounds.data
x
y
[1,] 0.2470532 1.9955092
[2,]
NA 0.5612988
[3,] 1.0240813 1.1813461
:
:
A missing indicator (NA) is used where censored exists. If we wanted to treat the censored
data as coming from a normal likelihood we would include the model statements in the data
loop:
is.censored[i] ~ dinterval(t[i], bounds.data[i,])
t[i] ~ dnorm(mu,tau)
where: So is.censored[i] is a censoring indicator for the ith case, and t[i] is a censoring
time. Here JAGS is more careful to distinguish unobservable due censoring, not visible to
the researcher, and unobservable due to truncation, not visible because of the deﬁnition of
a random quantity.
Finally, there is also an important diﬀerence in how the dedicated R functions process
categorical data for WinBUGS and JAGS. None of the BUGS engines handles factors in the same
natural way that R does, so factors in R must be converted to numerical values in a way
that is consistent with the interpretation of the variable, particularly with regard to ordered
factors. For windows systems BRugs converts to numerical values automatically, but not
necessarily with the desired coding, but R2WinBUGS does not. The general advice is to take
each factor variable and manually convert it to a numerical scheme in a way that makes
sense. This process also includes identifying and removing one category as the reference
group to create a contrast. None of the BUGS engines do the automatic contrast process that
model statements in R perform. So it is up to the researcher to determine the appropriate
handling of categorical data. Consider an example from Kyung et al. (2012) where SYS
is a factor indicating classes of government structure: direct presidential elections, strong
president elected by assembly, and dominant parliamentary government, summarized in R
by

394
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
summary(SYS)
Direct.Pres
Strong.Pres Parliamentary
37
27
86
To create an ordered numeric variable as.numeric(SYS) -1 turns these into 0,1,2 values,
respectively. Note that this is a strong assumption, not just about the order, but also that
the ordered distance between categories is identical. So if parliamentary government was
not only the assumed highest category on this scale, but we also wanted it to be 3 times
the distance as that between direct presidential elections and strong president elected by
assembly, we would stipulate SYS[SYS ==3] <- 4. Perhaps no ordering is appropriate and
this is a purely nominative measurement. Then a treatment contrast (dummy coding) must
be given explicitly with the reference category identiﬁed. Suppose we want parliamentary
government as the reference category, then to create two new variables to pass along to
BUGS, the following R steps are necessary:
Direct <- abs(1-as.numeric(SYS)%%3)
Strong <- 1-abs(2-as.numeric(SYS))
Some people prefer sum contrast coding (Helmert and polynomial contrasts are rare in the
social sciences), which can be produced for this example in R by:
Direct <- (as.numeric(SYS))%%3-1
Strong <- 2-as.numeric(SYS)
The key point is that the user is now fully responsible for coding categorical variables,
ordered or unordered, in BUGS.
■Example 11.2:
Example: A Hierarchical Model of Lobbying Inﬂuence in the
U.S. States
The American State Administrator’s Project (ASAP) survey asks administrators
about the inﬂuence of a variety of external political actors including “clientele groups”
in their agencies. Clientele group is arguably not perfectly synonymous with interest
group, but previous studies have used these terms interchangeably (Kelleher and Yac-
kee 2009).
Gill and Witko (2013) reanalyze these data with a hierarchical Bayesian
speciﬁcation that accounts for the nesting of public administrators within states, and
also substituting the variable elected.board below for their original measure of merit
position, which is dropped.
Consider a 713 × 22 matrix X with a leading column of 1’s for individual level ex-
planatory variables, and a 50×3 matrix Z for state-level explanatory variables. These
variables are:
▷contracting: scale from 0 : 6 where higher indicates more private contracting
within the respondent’s agency.
▷gov.influence: respondents’ assessment of the governor’s inﬂuence on contract-
ing in their agency.

Implementing Bayesian Models with Markov Chain Monte Carlo
395
▷leg.influence: respondents’ assessment of the legislatures’ inﬂuence on con-
tracting in their agency, ranging from 0 : 21.
▷elect.board: dichotomous variable coded 1 if appointed by a board, a commis-
sion or elected, and 0 otherwise.
▷years.tenure: number of years that the respondent has worked at their current
agency.
▷education: ordinal variable for level of education possessed by the respondent.
▷partisan.ID: a 5-point ordinal variable (1-5) for the respondent’s partisanship
(strong Democrat to strong Republican).
▷category []: categories of agency type.
▷med.time: whether the respondent spent more or less than the sample median
with representatives of interest groups.
▷medt.contr: interaction variable betweenmed.time and contracting.
▷gov.ideology: state government ideology from Berry et al. (1998) from 0 to
100.
▷lobbyists: total state lobbying registrants in 2000-01 from Gray and Lowery
(1996, 2001).
▷nonprofits provides the total number of nonproﬁt groups in the respondents’
state in the year 2008, divided by 10,000.
The outcome variable (group.inf) measures the respondents’ perception of interest
groups’ inﬂuence on total budget, special budgets, and general public policies. The
linear hierarchical model is given by:
Yi ∼N(αij + βXi, σ2
y),
for i = 1, . . . , 713
αj ∼N(GZ, σ2
α),
for j = 1, . . . , 50,
where αij indicates that the ith respondent is nested in the jth state to produce
a state-speciﬁc random intercept. This random intercept is then parameterized at
a second level by the three explanatory variables in Z (gov.ideology, lobbyists,
nonprofits) and their corresponding estimated coeﬃcients, G. We will return to this
model and the hierarchical issues in Chapter 12, but for now we will concentrate on
the mechanics of estimation with MCMC.
The model uses semi-informed versions of the prior distributions for the unknown
parameters since a high-quality source exists: prior distributions are diﬀuse normals
centered at the point estimates from Kelleher and Yackee (2009), (their Model 3, 2009,
p.593).
This gives the speciﬁcation:
β ∼N(βky, Σβ)
G ∼N(Gky, ΣG),
where the Σβ and ΣG matrices are diagonal forms with large variances relative to the

396
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
Kelleher and Yackee point estimates.
This leads to the following BUGS code minus
the prior speciﬁcations:
model
{
for (i in 1:SUBJECTS) {
mu[i] <- alpha[state.id[i]]
+ beta[1]*contracting[i]
+ beta[2]*gov.influence[i]
+ beta[3]*leg.influence[i] + beta[4]*clientappt[i]
+ beta[5]*years.tenure[i]
+ beta[6]*gender[i]
+ beta[7]*education[i]
+ beta[8]*partisan.ID[i]
+ beta[9]*category_2[i]
+ beta[10]*category_3[i]
+ beta[11]*category_4[i]
+ beta[12]*category_5[i]
+ beta[13]*category_6[i]
+ beta[14]*category_7[i]
+ beta[15]*category_8[i]
+ beta[16]*category_9[i]
+ beta[17]*category_10[i]
+ beta[18]*category_11[i]
+ beta[19]*category_12[i]
+ beta[20]*timegroupsmed[i]
+ beta[21]*timemedXcont[i]
group.inf[i] ~ dnorm(mu[i],tau)
}
for (j in 1:STATES) {
eta[j] <- gamma[1]*gov.ideology[j] + gamma[2]*lobbyists[j]
+ gamma[3]*nonprofits[j]
alpha[j] ~ dnorm(eta[j],tau.alpha)
}
}
The prior distributions are speciﬁed by:
beta[1]
~ dnorm(0.070,1)
# |
beta[2]
~ dnorm(-0.054,1)
# |
beta[3]
~ dnorm(0.139,1)
# |
beta[4]
~ dnorm(0.468,1)
# | PRIOR MEANS FROM KELLEHER AND
beta[5]
~ dnorm(0.017,1)
# | YACKEE 2009, MODEL 3
beta[6]
~ dnorm(0.207,1)
# |
beta[7]
~ dnorm(0.056,1)
# |
beta[8]
~ dnorm(0.039,1)
# |
beta[9]
~ dnorm(0.0,1)
beta[10]
~ dnorm(0.0,1)
beta[11]
~ dnorm(0.0,1)
beta[12]
~ dnorm(0.0,1)
beta[13]
~ dnorm(0.0,1)
beta[14]
~ dnorm(0.0,1)
beta[15]
~ dnorm(0.0,1)
beta[16]
~ dnorm(0.0,1)
beta[17]
~ dnorm(0.0,1)
beta[18]
~ dnorm(0.0,1)

Implementing Bayesian Models with Markov Chain Monte Carlo
397
beta[19]
~ dnorm(0.0,1)
beta[20]
~ dnorm(0.184,1)
# | PRIOR MEANS FROM KELLEHER AND
beta[21]
~ dnorm(0.146,1)
# | YACKEE 2009, MODEL 3
gamma[1]
~ dnorm(0.0,1)
gamma[2]
~ dnorm(0.0,1)
gamma[3]
~ dnorm(0.0,1)
tau
~ dgamma(1.0,1)
tau.alpha ~ dgamma(1.0,1)
And the JAGS formatted starting values are:
tau <- 10
tau.alpha <- 10
alpha <- c(1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,
1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1)
zeta <- 1
beta <- c(1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1)
gamma <- c(1,1,1)
The data are too large to present here but can be obtained from the R package BaM.
We now turn our attention to processing within R. First load the necessary libraries,
ﬁle, and dataset:
lapply(c("rjags","arm","coda","superdiag","R2WinBUGS","R2jags","lme4"),
library, character.only=TRUE)
source("WriteDatafileR.R"); data(asap.data.list)
Be aware that these libraries change over time as does JAGS and R, meaning that small
errors can occur because some future setup is not exactly like the one used at the time
of this example. In this example we will run JAGS remotely from R using Rjags. The
ﬁrst step is to deﬁne the model in some ﬁle, making it an R function. Once satisﬁed
with the model speciﬁcation it can then be sourced or pasted into the R environment.
The model here is given by:
asap.model.rjags
<- function()
{
for (i in 1:SUBJECTS) {
mu[i] <- alpha[state.id[i]] + beta[1]*contracting[i]
+ beta[2]*gov.influence[i] + beta[3]*leg.influence[i]
:
tau.alpha ~ dgamma(1.0,1);
}

398
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
where this is exactly the same model BUGS code as before (text shortened for space
purposes) except that it is enclosed in asap.model.rjags <- function() { } in the
R environment. It is important to note some features here. Since there are two levels
here, there are two looping structures: for (i in 1:SUBJECTS) { } and for (j in
1:STATES) { }. In this way BUGS loops through both indices. The linkage between the
individual subjects level and the states level is mu[i] <- alpha[state.id[i]]. Here
the intercept serves to express the state-level diﬀerences since state.id[i] asserts
that the ith individual is nested in one of the 50 states. We are then free to specify
the linear model at the state level. This is called nested indexing as speciﬁed here
and it is not the only way specify hierarchical models and data in BUGS, but it is the
cleanest and closest to the theory (Lunn et al. [2012, pp.231-232] discuss alternatives:
“padding-out” and “oﬀsets”).
It is often convenient and safe to save the model into a ﬁle, which is done by:
write.model(asap.model.rjags, "asap.model2.rjags")
This requires the R2WinBUGS package even though we are working with JAGS. Setting
up the initial values and naming the parameters for the JAGS function in R is done by:
asap.inits <- function() list("tau.y" = 10, "tau.alpha" = 10,
"beta" = rep(1,20), "gamma" = c(1,1,1))
asap.params <- c("beta","gamma","tau.y","tau.alpha")
Now that we have these values saved, we can ask JAGS to compile it with three parallel
chains, and asking it to use 5,000 iterations to tune the variance of the Metropolis-
Hastings candidate-generating distribution (if necessary):
asap.out <- jags(data=asap.jags.list, inits=asap.inits, asap.params,
n.iter=5000, model="asap.model.rjags", DIC=TRUE)
Actually, DIC=TRUE is the default, but it is stated here as a reminder. Now run 200,000
iterations of the Markov chain saving all sampled values (we can remove burn-in values
later:
asap.out3 <- update(asap.out, n.iter=200000)
A quick summary table is achieved with:
print(asap.out3)
A more convenient form is obtained by turning the output into an mcmc object in R:
asap.mcmc3 <- as.mcmc(asap.out3)

Implementing Bayesian Models with Markov Chain Monte Carlo
399
This lets us load the results into CODA and BOA as well as use the diagnostic suite in
the superdiag packager:
superdiag(as.mcmc.list(asap.mcmc3), burnin=100000)
The results are easily summarized from the R command to produce a table. Notice
from Table 11.3 that the α values are summarized by their mean. This is common
in hierarchical models like this where there are 50 random intercepts and this would
overwhelm readers in the context of a table.
TABLE 11.3:
Lobbying Influence Results
Parameters
Mean
Std. Error
95% HPD Interval
α mean(1:50)
1.3905
0.7037
[ 0.0112 : 2.7698]
contracting
0.1987
0.0963
[ 0.0099 : 0.3874]
gov.influence
0.0481
0.0367
[-0.0239 : 0.1202]
leg.influence
0.3519
0.0397
[ 0.2741 : 0.4297]
elect.board
1.3436
0.3546
[ 0.6486 : 2.0386]
years.tenure
0.0347
0.0233
[-0.0110 : 0.0804]
education
0.1249
0.1217
[-0.1136 : 0.3634]
partisan.ID
-0.0046
0.0845
[-0.1703 : 0.1611]
category2
-0.4282
0.5423
[-1.4912 : 0.6348]
category3
-0.0596
0.5885
[-1.2131 : 1.0938]
category4
1.5501
0.4571
[ 0.6541 : 2.4461]
category5
-0.5473
0.5010
[-1.5292 : 0.4347]
category6
0.9227
0.5395
[-0.1348 : 1.9801]
category7
1.7014
0.4353
[ 0.8482 : 2.5546]
category8
1.0013
0.4986
[ 0.0240 : 1.9785]
category9
0.9412
0.4860
[-0.0115 : 1.8938]
category10
0.6157
0.4634
[-0.2925 : 1.5239]
category11
-0.1264
0.4265
[-0.9624 : 0.7096]
category12
-0.1592
0.5727
[-1.2816 : 0.9632]
med.time
1.1435
0.3587
[ 0.4405 : 1.8465]
medt.contr
-0.0869
0.1372
[-0.3559 : 0.1821]
gov.ideology
0.0182
0.0062
[ 0.0060 : 0.0303]
lobbyists
0.0007
0.0008
[-0.0007 : 0.0022]
nonprofits
-0.0217
0.1267
[-0.2701 : 0.2266]
τy
0.0763
0.0042
[ 0.0682 : 0.0845]
τα
3.1021
1.3523
[ 0.4517 : 5.7525]
These ﬁndings are consistent with the literature regarding interest group inﬂuence in
state agencies. More contracting is positively related to perceptions of interest group
inﬂuence, and this ﬁnding is statistically reliable. There is evidence that those agency
heads that spent above the median amount of time with organized interests perceived
groups to have more inﬂuence over their agencies. Agency heads that were elected
or appointed by boards or commissions perceive interest group inﬂuence to be much
greater, and this coeﬃcient is reliable. A legislature perceived to be more inﬂuential
is associated with more powerful clientele groups, which become legislatures’ interest
group allies when making decisions. Also, the more time in the current position is

400
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
associated with the perception of greater interest group inﬂuence, even though 7% of
the posterior density is below zero under a normal posterior assumption.
To compare this model to the null model we need to rewrite the code preserving
the hierarchical component but remove all of the individual explanatory variables.
Unfortunately WinBUGS requires that every variable in the data deﬁnition must be
used in the model speciﬁcation, which can make it awkward for the model ﬁt process.
An old trick is to assign unused variables in the data to a “dead end” child node. This
is a node that does nothing but collects the unwelcome nodes and satisﬁes WinBUGS.
Fortunately JAGS and openbugs do not worry about this match.
We could easily
write a “new” null model but this is irritating and can introduce new errors to debug.
Instead we leave the linear additive collection but assign it to ν instead of μ, and μ
gets just the mean eﬀect:
asap.null.rjags
<- function()
{
for (i in 1:SUBJECTS) {
nu[i] <- alpha[state.id[i]]
+ beta[1]*contracting[i]
+ beta[2]*gov.influence[i]
+ beta[3]*leg.influence[i] + beta[4]*elect.board[i]
+ beta[5]*years.tenure[i]
+ beta[6]*education[i]
+ beta[7]*partisan.ID[i]
+ beta[8]*category2[i]
+ beta[9]*category3[i]
+ beta[10]*category4[i]
+ beta[11]*category5[i]
+ beta[12]*category6[i]
+ beta[13]*category7[i]
+ beta[14]*category8[i]
+ beta[15]*category9[i]
+ beta[16]*category10[i]
+ beta[17]*category11[i]
+ beta[18]*category12[i]
+ beta[19]*med.time[i]
+ beta[20]*medt.contr[i]
mu[i] <- alpha[state.id[i]]
group.infl[i] ~ dnorm(mu[i],tau.y)
for (j in 1:STATES) {
eta[j] <- gamma[1]*gov.ideology[j] + gamma[2]*lobbyists[j]
+ gamma[3]*nonprofits[j]
alpha[j] ~ dnorm(0,tau.alpha)
}
:
}
Now WinBUGS is satisﬁed and we can rerun the R steps from above being careful to
relabel the output objects:
write.model(asap.null.rjags, "asap.null.rjags")
asap.null.out <- jags(data=asap.jags.list, inits=asap.inits,
asap.params, n.iter=5000, model="asap.null.rjags", DIC=TRUE)
asap.null.out3 <- update(asap.null.out, n.iter=200000)
print(asap.null.out3)

Implementing Bayesian Models with Markov Chain Monte Carlo
401
asap.null.mcmc3 <- mcmc(asap.null.out3)
superdiag(as.mcmc.list(asap.null.mcmc3), burnin=100000)
The statement print(asap.out3) or:
dic.samples(asap2.model, n.iter=2500, type="pD")
(for more options) returns DIC = ¯D + pD = 3861 + 34.04 ≈3895. For the null
model run above, this command returns ¯D = 3964 and pD = 49.26. The null model
DIC is the rounded up sum of these two: 4014. Since lower DIC values are preferred,
the speciﬁed model is a better ﬁt (3895 versus 4014). Note that these estimates are
built on Monte Carlo quantities, so diﬀerent runs of the same sampler will naturally
produce slightly diﬀerent values.
11.5
Technical Background about the Algorithm
The process of creating full conditional distributions from a model speciﬁcation is the
biggest contribution of the BUGS software. The mechanism for doing this and creating a run-
time MCMC program is sensitive to the level of measurement of the nodes. For categorical
variables CDF inversion is used, and for closed forms from interval-measured conjugate
relationships, direct sampling from known analytical solutions is used. These are simple
solutions that have already been discussed in other contexts. Strictly log-concave but not
conjugate forms use adaptive rejection sampling, which is described here. Other continuous
forms require Metropolis-Hastings or slice sampling (Chapter 14).
The software WinBUGS and openbugs (but not JAGS) uses an underlying engine of adap-
tive rejection sampling (Gilks 1992, Gilks and Wild 1992), which is an MCMC implementa-
tion that implements rejection sampling with an adaptive function. It works as follows. We
want to sample from f(θ), or g(θ) = cf(θ), deﬁned on the support Θ. Deﬁne the enveloping
function: gu(θ) ≥g(θ), ∀θ ∈Θ. Now deﬁne the squeezing function: gq(θ) ≤g(θ), ∀θ ∈Θ.
Here is a general statement of the algorithm steps, to be repeated until the desired number
of samples are obtained:
1. sample θ∗from gu(θ)
2. sample u from U(0, 1)
3. decide
If:
u ≤gq(θ∗)/gu(θ∗)
Then:
accept θ∗
Else:
If:
u ≤g(θ∗)/gu(θ∗)
Then:
accept θ∗
Else:
reject θ∗, use it to update enveloping
and squeezing functions

402
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
The adaptive rejection sampling works as follows:
▷Tθ is a small set of points on the support of θ: {θ0, θ1, . . . , θs+1}, the abscissae.
▷h(θ) = log g(θ) is known up to a constant (dropping conditional terms for notational
convenience), and concave everywhere in Θ.
▷Since h(θ) is concave, any non-tangent line segment that intersects the curve does so
at two points corresponding to a pair of selected θi.
▷Call this line Li,i+1 determined by the points:
(θi, h(θi)) and (θi+1, h(θi+1)).
▷Now consider the triangle created by Li,i+1 and the two tangent lines at (θi, h(θi))
and (θi+1, h(θi+1)).
▷The triangle clearly encloses the h(θ) function, and so we also have bounds on the
f(θ) metric.
▷For each point in Tθ, calculate h(θ) and h′(θ). . .
▷deﬁne the upper hull formed by the tangent lines to h(θ) at the abscissae in Tθ.
▷Note that these tangent lines create the upper hull by their intersections, for the lines
tangent at θj and θj+1, the intersection point is:
zj = h(θj+1) −h(θj) −θj+1h′(θj+1) + θjh′(θj)
h′(θj) −h′(θj+1)
▷So any point on the upper hull between zj−1 and zj can be found by:
u(θ) =
h(θj)
 !" #
value at abscissa
+
(θ −θj)
 !" #
distance from abscissa
×
h′(θj)
 !" #
slope of tangent
.
The triangles in this process are illustrated in Figure 11.2.
The squeezing function works as follows.
▷Also for each point in Tθ, again use h(θ) and h′(θ). . .
▷Deﬁne the lower hull formed by the chords connecting h(θ) at the abscissae in Tθ.
▷For the chord connecting θj and θj+1, the line is given by:
ℓ(θ) = h(θj)(θj+1 −θ) + h(θj+1)(θ −θj)
θj+1 −θj
▷Log-concavity assures ℓ(θ) ≤h(θ) ≤u(θ), where equality holds at the abscissae.
This is shown in Figure 11.3.
Now we can state the full algorithm in greater detail:
1. Initialize the (possibly new) abscissae, Tθ, calculate u(θi) and ℓ(θi) at these points.
2. Sample θ∗from exp[u(θ)]/

D exp[u(θ′)]dθ′, and sample u from U(0, 1).

Implementing Bayesian Models with Markov Chain Monte Carlo
403
t1
t2
L12
h
Theta Support
Density
FIGURE 11.2: Adaptive Rejection Sampling Step
3. Squeezing Test:
If:
u ≤exp[ℓ(θ∗)]
exp[u(θ∗)]
Then:
accept θ∗
Else:
calculate h(θ∗), h′(θ∗). If:
u ≤exp[h(θ∗)]
exp[u(θ∗)]
Then: accept θ∗
Else: reject θ∗
4. Updating Test: If θ∗was rejected, add to the list in Tθ.
So the algorithm focuses on eﬃciency by trying to accept points in the most eﬃcient method
ﬁrst. It also gains accuracy over time (a higher rate of accepted points) because of the
increase in abscissae points. Note that this is a regular Monte Carlo implementation of an

404
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
Theta Support
Density
FIGURE 11.3: Adaptive Rejection Sampling Squeezing
MCMC algorithm since the h(θ) function is a single full conditional distribution from an
inner step of the Gibbs sampler. This algorithm turns out to be quite ﬂexible as well. For
instance, Gilks, Best, and Tan (1995) show how to incorporate a Metropolis-Hastings step
that allows sampling from non-log-concave target distributions.
■Example 11.3:
Example: An Ordered Logit Model for U.S. Election Data
The 1960 U.S. presidential election between John Kennedy and Richard Nixon was
one of the closest contests in national history, with Kennedy’s margin of victory less
than one percent of the popular vote. Campaigns, journalistic accounts, and social
contexts can modify pre-election perceptions of the competitiveness of an uncertain
outcome. This has implications for turnout and therefore may aﬀect the election as
well. This example uses the 1960 American National Election Study (subsetted in the
R library BaM) to explore the link between personal characteristics and perception of
closeness of the impending election. The outcome variable has four ordered categories:
[one candidate will win by a lot], [one candidate will win by quite a bit], [this will be
a close race–fairly even], [this will be a very close race]. The speciﬁed explanatory
variables are:

Implementing Bayesian Models with Markov Chain Monte Carlo
405
▷education, 1=8th grade or lower (233 cases), 2=highschool (428 cases), 3=some
college or more (185 cases).
▷sex, 1=male, (412 cases), 2=female (434 cases).
▷seedebates, 1=no (141 cases), 2=yes (660 cases).
▷importance, 1=care very much (270 cases), 2=care pretty much (308 cases),
3=pro-con/depends (6 cases), 4=don’t care very much (155 cases), 5=don’t care
at all (85 cases).
▷involvement, 8 categories from low to high with the distribution of cases:
(158, 165, 245, 101, 78, 55, 4, 39).
▷catholic, 0=no (623 cases), 1=yes (179 cases).
▷partyid, 1=strong Democrat (198 cases), 2=not very strong Democrat (201
cases), 3=independent closer to Democrats (52 cases), 4=independent (67 cases),
5=independent closer to Republicans (59 cases), 6=not very strong Republican
(117 cases), 7=strong Republican (139 cases).
Ordered choice models are constructed by assuming that there is a continuous latent
metric dictating the categorical choices since researchers construct the scale not the
respondents. So the outcome variable Y has C ordered categories separated by esti-
mated thresholds (sometimes called “cutpoints” or “fences”) sitting over a continuous
utility metric U that cannot be seen:
Ui : θ0 ⇐==⇒
c=1
θ1 ⇐==⇒
c=2
θ2 ⇐==⇒
c=3
θ3 . . . θC−1 ⇐==⇒
c=C
θC,
where the end-categories extend out to −∞and ∞, respectively. The eﬀect of the
explanatory variables is determined by a linear additive speciﬁcation on the latent
scale such that the ith person’s utility is Ui = Xiβ + ϵi, where the β do not depend
on the θ values. Note that some authors prefer a minus sign in front of Xiβ, but the
model deﬁned here does not as is also the case with the R function polr. The vector
of utilities across individuals is determined in the following way:
▷the probability of the ith person choosing the kth category or less is p(Yi ≤k|X),
▷this is equal to the probability that the ith person’s utility is less than or equal
to next threshold to the right of this category, p(Ui ≤θk),
▷now substitute in the linear additive component for the utility to get
p(Xiβ + ϵi ≤θk),
▷rearrange to leave the stochastic component alone on the left p(ϵi ≤θk −Xiβ),
▷notice that this is just the CDF of ϵi, Fϵi(θk −Xiβ), at the point θk −Xiβ,
▷specifying a logistic distribution for this distribution results in the model p(Yi ≤
k|X) = [1 + exp(Xiβ −θk)]−1.
An ordered probit model can also be created by specifying p(Yi ≤k|X) = Φ(θk −Xiβ)
instead.
The ordered logit model is produced in JAGS with the following code.

406
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
model {
for (i in 1:Nsub) {
mu[i] <- beta[1]*education[i] + beta[2]*sex[i]
+ beta[3]*seedebates[i] + beta[4]*importance[i]
+ beta[5]*involvement[i]
+ beta[6]*importance[i]*involvement[i]
+ beta[7]*catholic[i] + beta[8]*partyid[i]
for (j in 1:(Ncat-1)) { logit(Q[i,j]) <- cut[j] - mu[i] }
p[i,1] <- Q[i,1]
for (j in 2:(Ncat-1)) { p[i,j] <- Q[i,j] - Q[i,(j-1)] }
p[i,Ncat] <- 1 - Q[i,(Ncat-1)]
close[i] ~ dcat(p[i,1:Ncat])
E.y[i] <- close[i] - mu[i]
}
sd.y <- sd(E.y[])
for (k in 1:Nvar) { beta[k] ~
dt(0,1,5) }
for (k in 1:(Ncat-1)) {
cut0[k] ~
dt(0,1,5) }
cut[1:(Ncat-1)] <- sort(cut0)
}
There are several features of this model that have not been introduced yet. Notice that
the logistic speciﬁcation is speciﬁed by for (j in 1:(Ncat-1)) { logit(Q[i,j])
<- cut[j] - mu[i] }, which loops through each of the ﬁrst k −1 categories relat-
ing the linear additive component to a cumulative probability. The last category is
not necessary to calculate since it equals one. Next we want to calculate the indi-
vidual category probabilities from the cumulative probabilities. The ﬁrst category is
easy since they are equivalent: p[i,1] <- Q[i,1]. Then we loop through all of the
higher categories except for the last, diﬀerencing the adjacent cumulative probabilities
to get the marginal probabilities: for (j in 2:(Ncat-1)) { p[i,j] <- Q[i,j] -
Q[i,(j-1)] }. Finally the right-most category is obtained by p[i,Ncat] <- 1 -
Q[i,(Ncat-1)]. The outcome variable is then modeled with these probabilities with:
close[i] ∼dcat(p[i,1:Ncat]). The last line in the 1:N loop creates a residual
vector, which is then summarized outside of the loop with a standard error function.
The β coeﬃcients and the cutpoints are both given Cauchy priors with 5 degrees of
freedom. The last line creates the variable cut from the intermediate variable cut0
to create a sorted form from the prior distribution.
As noted before in this chapter, it is good programming practice not to bury constants
into the BUGS code. A common mistake is using the same code for another application
(possibly modiﬁed) but not noticing that such values are hard-coded into the model
statement. With good luck the program crashes leading to time debugging, but with
bad luck the constant “works” but modiﬁes the intended purpose of the new model

Implementing Bayesian Models with Markov Chain Monte Carlo
407
such as looping only through part of the data. Using R the needed constants are
appended to the data ﬁle:
system("echo ’Nsub <- 846’ >> anes.jags.dat")
system("echo ’Ncat <- 4’ >> anes.jags.dat")
system("echo ’Nvar <- 8’ >> anes.jags.dat")
(although this could also be done with and editor). The system commands above
require a Unix-based operating system.
TABLE 11.4:
Presidential Election Model Results
Parameters
Mean
Std. Error
95% HPD Interval
education
0.2773
0.1067
[ 0.0681: 0.4864]
sex
0.3708
0.1410
[ 0.0944: 0.6472]
seedebates
0.4438
0.1813
[ 0.0884: 0.7992]
importance
-0.0432
0.1235
[-0.2852: 0.1988]
involvement
-0.3021
0.1045
[-0.5070:-0.0973]
importance×involvement
0.0534
0.0280
[-0.0015: 0.1084]
catholic
0.2484
0.1741
[-0.0928: 0.5896]
partyid
0.0403
0.0328
[-0.0240: 0.1047]
θ1
-1.7139
0.5141
[-2.7215:-0.7064]
θ2
-0.2879
0.4989
[-1.2658: 0.6900]
θ2
3.0812
0.5142
[ 2.0734: 4.0890]
sy = 0.7517, Model DIC: 1580.64, Null DIC: 1609.95
The modeling process started with using the mice package in R to create 5 complete
datasets with imputed values for missing data. Then 5 chains with 5 diﬀerent starting
points and 5 diﬀerent random seeds are run in JAGS for 100,000 iterations disposing
of the ﬁrst half of these. There was no indication of non-convergence using the suite
of diagnostics in superdiag. The MCMC outputs are combined by ﬁrst taking the
mean and standard deviation of each run of the chain and combined according to stan-
dard practice with multiple imputation (mean for the coeﬃcient mean, and weighted
combination of between and within variances for the coeﬃcient standard error). The
results are summarized in Table 11.4.
We see that those higher levels of education, women, and those that watched the
debates are more likely to see the race as closer. The measurement for involvement
in the race combines whether the respondent cares about who wins with the degree
of interest in the campaign. There is evidence from the model that higher levels of
involvement lead to more likely believing one of the candidates will win convincingly.
Regretfully the interaction with importance of the race does not work here, but this
interaction provides better ﬁt when included. Oddly there is no evidence that being
Catholic inﬂuenced perception of closeness in an election where Kennedy’s Catholicism

408
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
was an important campaign issue. Finally that stalwart of U.S. electoral models, party
identiﬁcation, is not contributing here.
11.6
Epilogue
This is the most “vocational” chapter in this text since it is concerned with the nuts-and-
bolts of estimation with popular MCMC software. The purpose is to make the mechanical
part of the process as comfortable as possible.
However, as we will see in subsequent
chapters, there are more issues to worry about.
Unfortunately, MCMC estimation will
never be as automated as MLE estimation simply because there are more nuances that
humans should worry about.
In fact, WinBUGS documentation includes the admonition
“Beware: MCMC sampling can be dangerous!”
In this chapter we also built up the level of complexity gradually going from a simple
linear model to a logit model to a hierarchical model. In the process, WinBUGS and JAGS
were highlighted, along with an example of the use of Rjags. This does not imply these are
the only reasonable approaches and given the many R packages for calling the three BUGS
engines, users can use the one that they are comfortable with. The choice between WinBUGS
and openbugs is evolving. Now WinBUGS is enormously popular due to the user interface,
but the developers state that new development is being done only on openbugs. The last
update of WinBUGS was version 1.4.3, released August 2007 (the last of a series of patches
from version 1.4 released September 2004). Finally, the software approaches in this chapter
will become dated with time (hopefully not too quickly!), but the theoretical discussion in
this chapter surrounding this one are based on hardened mathematical principles and will
not change (although obviously new theory will be added over time).
11.7
Exercises
11.1
Rerun the model in Example 11.4. Why are your posterior summaries very slightly
diﬀerent?
11.2
Consider the following subset of BUGS code:
for (i in 1:N) {
Y[i] ~ dnorm(eta[i], tau)
:
}
:

Implementing Bayesian Models with Markov Chain Monte Carlo
409
tau ~ dgamma(1.0E-2, 1.0E-2)
sigma <- 1/sqrt(tau)
where “:” denotes additional code not essential to this question. What is σ, and
how is it distributed?
11.3
Rerun the model in Example 11.3.3 using a probit link function and Cauchy priors
for the three parameters.
11.4
Using the data from Example 2.3.4 (the Cultural Consensus Model in Anthropol-
ogy) on page 51, write a simple logit speciﬁcation with no covariates (mean eﬀect
only) in BUGS and run the model.
11.5
Write a BUGS program to calculate the posterior predictive distribution using the
data in Example 6.4 concerning Economic Growth in Sub-Saharan Africa on page 197.
Your model is an MCMC implementation of (6.22). Specify three diﬀerent prior
distributions and compare the results.
11.6
Using the Palm Beach County electoral data described on page 148, and distributed
in the BaM package, specify both a linear model and a Poisson (log-normal) model
in BUGS for the outcome badballots in the dataset pbc.vote. Use diﬀuse prior
distributions in both cases. Compare the results. Is it necessary to use the Poisson
GLM?
11.7
Write a Two-Way ANOVA model for the recidivism data in Example 6.2 on page 188
where the columns are determined by the variables Released and Returned, and the
rows are determined by crime type. Do this with a double loop in BUGS where the
values in the table are modeled according to Y[i,j]
dnorm(eta[i,j], tau)
where the double indexing accounts for both row and column mean eﬀects.
11.8
In 2006 the U.S. Senate voted 56 to 44 to conﬁrm Justice Samuel Alito to the
Supreme Court. The corresponding dataset senate.vote is in the R package BaM.
The variables are: PARTY indicates Democrat, Republican, or Independent, STATE
is the U.S. state represented, ALITO is Yea or Nay on Alito, ADA is the Americans
for Democratic Action ideology score, NUMREPS is the number of U.S. House repre-
sentatives for the state, REGION is the region of the country (East, Midwest, South,
West), SENIORITY is the order of service in the Senate (rank from the ﬁrst senator
on), and HOLMES is the vote on the district court nomination of James Leon Holmes
in 2003. Construct a model in BUGS where the Alito vote is the outcome variable,
picking a subset of the other variables as explanatory variables. Notice that the
factors will have to be coded into numerical variables.
11.9
On page 389 the linear model in BUGS was enhanced to give an R2 measure. Modify
the linear model code to produce an F-statistic and run this code with the OECD

410
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
data to get a value. Summarize the CODA output with a simple regression table
(further details on CODA will be given in Chapter 14).
11.10
Replicate the French Labor Strikes model in Example 7.2.1.1, starting on page 212
by writing a BUGS program for a negative binomial model with a Jeﬀreys prior.
11.11
Extending the state-level obesity data from Exercise 3.6 on page 89, now add the
vector of general revenue by state (divided by 1M) for 2009 to the BMI data. These
are:
Alabama
21.203
Alaska
11.532
Arizona
25.548
Arkansas
15.211
California
186.315
Colorado
19.303
Connecticut
21.079
Delaware
6.700
Florida
66.734
Georgia
34.366
Hawaii
9.164
Idaho
6.452
Illinois
53.810
Indiana
29.946
Iowa
16.246
Kansas
13.576
Kentucky
21.472
Louisiana
28.080
Maine
7.891
Maryland
29.677
Massachusetts
41.573
Michigan
49.635
Minnesota
29.043
Mississippi
16.820
Missouri
24.441
Montana
5.712
Nebraska
8.403
Nevada
9.431
New Hampshire
5.721
New Jersey
49.175
New Mexico
13.1
New York
134.951
North Carolina
41.175
North Dakota
13.478
Ohio
54.383
Oklahoma
18.691
Oregon
17.752
Pennsylvania
60.726
Rhode Island
6.527
South Carolina
20.693
South Dakota
3.677
Tennessee
23.656
Texas
96.156
Utah
12.837
Vermont
4.978
Virginia
34.993
Washington
32.520
Washington DC
9.753
West Virginia
11.113
Wisconsin
29.482
Wyoming
6.041
(also available as bmi.2009 in the BaM package in R). Fit a bivariate model with
BUGS where BMI is the outcome variable using appropriate prior distributions.
11.12
Returning to Exercise 4.13, construct a time-series speciﬁcation in BUGS where the
outcome variable is the rate of fatalities by terrorism and the other columns are
explanatory variables. Use double-indexing to account for time and variables.
11.13
Consider data on depression for teenagers with the following variable deﬁnitions:
anxiety (”Low”,”Medium”,”High”), behavior problems (”Present”,”Absent”), sex
(”Male”,”Female”), and depression: (”Absent”,”Mild”,”Severe”). The R dataframe
can be created with the statements:
freq <- c(9, 32, 4, 1, 8, 4, 3, 1, 40, 6, 2, 0, 1, 0, 8, 1, 9, 14,
9, 6, 9, 41, 2, 5, 5, 7, 23, 3, 1, 7, 31, 24, 99, 2, 6, 33)
psych.df <- data.frame(freq,expand.grid(anxiety=1:3,behavioral=1:2,
depression=1:3,sex=1:2))

Implementing Bayesian Models with Markov Chain Monte Carlo
411
Format these data for BUGS and run the appropriate model for depression as the
outcome.
11.14
None of the BUGS engines has a Haldane prior (Exercise 4.10 on page 139) for
modeling purposes. Provide code for specifying a Haldane prior in BUGS without
using dbeta.
11.15
Replicate Exercise 7.20 on page 246 in BUGS rather than R.
11.16
Exercise 3.18 analyzed Swiss suicides data using Bettina Gruen’s bayesmix package.
Rewrite the model directly in BUGS. Compare the results.
11.17
Merge the Senate data from Exercise 8 with the obesity data from Exercise 3.6 on
page 89. Can you produce a reliable model outcome using BUGS where BMI is the
outcome variable? Does this mean that there is a political component to obesity
at the state level?
11.18
Using the likelihood function in Exercise 9.10 on page 324, set t1 = 0.25, and write
an adaptive rejection sampling algorithm (Section 11.5) in R to eﬃciently sample
from the resulting function.
11.19
Write a module of BUGS code to calculate the conventional null and model deviance
for a GLM speciﬁcation (Appendix A). Apply it to the model in Exercise 8 above.
11.20
Write an R function to implement adaptive rejection sampling where the input is
a function and two bounds. The function should include a check for log-concavity.
Implement the function by drawing values from a log-normal distribution with
μ = 1 and σ = 1
2, between the bounds [1:4].


Chapter 12
Bayesian Hierarchical Models
12.1
Introduction to Multilevel Speciﬁcations
Hierarchical data structures are regularly encountered in the social and behavioral sci-
ences since measurement often takes place at diﬀerent levels of collection. A synonymous
term is “multilevel” data meaning that the data represent diﬀerent levels of aggregation for
the subjects of study. We will use both terms herein since both are widely used across many
social science literatures. A major advantage to the Bayesian paradigm is that these mul-
tilevel/hierarchical models are described and estimated in a very clean and direct manner
with distributional statements.
As an example, we can study employees in a ﬁrm by department, plant, region, or na-
tion. Consider a typical Fortune 500 employee; they have individual attributes such as
demographics, they are aﬀected by the actions of the immediate department, they are also
aﬀected by the decisions of local senior managers and executives at corporate headquarters,
and perhaps even by outside economic forces. However, the level at which these inputs mat-
ter diﬀers widely, and individuals feel them diﬀerently depending on their position in the
hierarchy. In sociological survey analysis, we might augment the collected data from indi-
viduals with historical, governmental, or economic variables measured at various geographic
levels. The question then arises as to how we should treat the diﬀerent levels of variables in
the same statistical model. We would like predictors to enter the model at the correct level
for how they aﬀect cases and groups of cases. Ignoring the aggregate information excludes
potentially important eﬀects and treating the aggregate information as individual level ef-
fects confuses covariance in the model. The solution is to employ a hierarchical model that
recognizes the diﬀerent groupings or time points that information about individual obser-
vations occur, thereby speciﬁcally stipulating correlations that would not have otherwise
been assumed to exist. Another common justiﬁcation for specifying hierarchical models is
that some distributional forms cannot adequately account for overdispersion in the outcome
variable of interest (Cox 1983). Adding an additional level then models this attribute.
Hierarchical models are deliberately set up so that the data are assumed to be condi-
tioned on a set of parameters, which are in turn themselves conditioned on other parameters,
which may depend on data at other levels of aggregation. By conditioned, it is meant that
there is a speciﬁed PDF or PMF that describes the parametric relationship between data or
413

414
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
variables to higher-level speciﬁcations of other variables. Notice that this is a very Bayesian
setup in that distributions are stipulated in the modeling process and estimated parame-
ters are related through these distributions. So multilevel models are not only symbiotic
with the Bayesian paradigm, all forms of multilevel models are essentially Bayesian since
unknown quantities are speciﬁed with distributions.
Multilevel modeling is a very adaptable idea because there is no mathematical restriction
to the number of levels of these parametric relationships or the number of relationships at
each level, and as many as are practical and convenient can be used. In addition, because of
these features the Bayesian hierarchical model can be used as a vehicle for making decisions
about variable inclusion (Carlin and Chib 1995, George and McCulloch 1993, Green 1995,
Mitchell and Beauchamp 1988, Phillips 1995).
12.2
Basic Multilevel Linear Models
A good starting point for our purposes is the most basic form of the multilevel model.
Multilevel linear models take the standard linear model speciﬁcation and remove the stan-
dard restriction that the estimated coeﬃcients are constant across individual cases by spec-
ifying levels of additional eﬀects. Start with a standard linear model speciﬁcation indexed
by subjects and a ﬁrst level of grouping, the context level. Use a single explanatory variable
that has the form:
Yij = βj0 + βj1Xij + ϵij.
(12.1)
Now add a second level to the model that explicitly nests eﬀects within groups and index
these groups j = 1 to J:
βj0 = γ00 + γ10Zj0 + uj0
βj1 = γ01 + γ11Zj1 + uj1,
(12.2)
where all individual level variation is assigned to groups producing department level resid-
uals: uj0 and uj1. These Zji are context level variables in that their eﬀect is assumed to
be measured at the group level rather than at the individual level.
The basic two-level model is now produced by inserting the context level speciﬁcations,
(12.2), into the original linear expression for the outcome variable of interest, (12.1). Per-
forming this substitution gives:
Yij = γ00 + γ01Xij + γ10Zj0 + γ11XijZj1 + uj1Xij + uj0 + ϵij.
(12.3)
This equation shows that the composite error structure, uj1Xij+uj0+ϵij, is now clearly het-
eroscedastic since it is conditioned on levels of the explanatory variable, causing additional
estimation complexity.

Bayesian Hierarchical Models
415
The multilevel models derived from this setup have annoying synonyms, but the most
commonly named forms are discussed below. Gill and Womack (2013, pp.13-14) note the
pervasiveness of this problem. Here we ﬁrst assume an intercept term, α, and a single
explanatory variable β. For the data matrices, Xij for individual i in cluster or group j,
and Zj for cluster j, there are ﬁve canonical models:
“Fully Pooled”
Yij = α + X′
iβ + Z′
jγ + eij
“Random Intercept”
Yij = αj + X′
iβ + eij
“Random Slope”
Yij = α + X′
ijβ + Z′
jγ + eij
“Random Intercept/Random Slope”
Yij = αj + X′
ijβj + Z′
jγ + eij
“Fully Unpooled”
Yi,j=1 = αj=1 + X′
i,j=1βj=1 + Z′
j=1γj=1 + ei,j=1
Yi,j=2 = αj=2 + X′
i,j=2βj=2 + Z′
j=2γj=2 + ei,j=2
...
Yi,j=J = αj=J + X′
i,j=Jβj=J + Z′
j=Jγj=J + ei,j=J
The fully pooled (or just “pooled”) model treats the group-level data as if it were mea-
sured at the individual level. This means that individuals will have some identical covariates
as other individuals, the Zj, since they share ignored group membership. The random in-
tercept model, also sometimes called the “random eﬀect” model, allows the intercept to
vary in the context of a speciﬁed distribution. This means that the αj are the same for
individual i cases in the same group j, drawing from this distribution. If there is no distri-
butional assumption, then there are J −1 separate coeﬃcients estimated using a contrast
speciﬁcation such as “dummy variable” coding (the treatment contrast). This is routinely
called the “ﬁxed eﬀect” model, but the distinction is unimportant in the Bayesian hierar-
chical context since it simply matters whether additional distributions with hyperpriors are
assigned or not. So the j subscript on αj can mean either of these two approaches.
The problem is that “ﬁxed” and “random” can diﬀer in deﬁnition by literature (Kreft
and De Leeuw 1988, Section 1.3.3, Gelman 2005). The random slope model puts a group
distinction on the β coeﬃcient, j = 1, . . . , J to impose group diﬀerences on the eﬀect of
the X variable. So each individual i in group j shares the same βj eﬀect. Like the random
intercept this can come from a distribution or a contrast, and both the intercept and the
slope can contribute group eﬀects. The fully unpooled model runs J completely separate
regressions for each of the J groups, under the assumption that there is commonality to
be modeled between them. Given the confusion in names across ﬁelds, the best way to
conceptualize these speciﬁcations is to consider them as members of a larger multilevel

416
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
family where indices are turned-on turned-oﬀsystematically depending on the hierarchical
purpose.
12.3
Comparing Variances
With multilevel speciﬁcations there are at least two important variance terms to eval-
uate. Consider the simple linear speciﬁcation: Yij = αj + X′
iβ + eij, with j = 1, . . . , J
groups speciﬁed for the random intercept. The resulting regression output (using the glmer
function in R or with the BUGS speciﬁcations in this chapter) will give σ2
y and σ2
α, where their
interpretation is very diﬀerent. The variance of the regression, σ2
y, describes variability from
the residuals left over after the systematic eﬀects. In basic hierarchical models this term is
assumed to be constant across the data where group level diﬀerences result from diﬀering
within-group sizes, nj, according to SDj = σy/nj.
Obviously like any other regression
setting, we prefer to minimize σ2
y for better model ﬁt. Conversely, σ2
α gives the variance
between groups at the higher level, and so we want this term to be as big as possible since
it is an explicitly modeled term and it justiﬁes the group distinction at the higher level.
Furthermore, it contains variability that falls to σ2
y if the second level is eliminated from
the model. In other words, the model ﬁts best when σ2
y is minimized and σ2
α is maximized.
This can also be described in ANOVA terms: σ2
y measures within-group variance and σ2
α
measures between-group variance.
This comparison of variances is at the heart of determining whether the second level
is justiﬁed. That is, if the grouping variable reveals no between-group diﬀerences in the
form of a small σ2
α relative to σ2
y, then it is clear that this grouping is not justiﬁed and
the level should be eliminated from the speciﬁcation. Thus a simple test is built into the
process. Relatedly, it may be more diﬃcult to justify the grouping with smaller J, but this
is also revealed by the variance comparison. Therefore silly “rules of thumb” about the
minimum number of groups or the minimum number of cases in each group are unnecessary
as the model tests this inherently. Such rules should be completely disregarded. In fact,
if σ2
α is comparable or large relative to σ2
y for small J, then the data are very clear that
this group diﬀerence is important. Unfortunately there is no distributional test that makes
sense when comparing these two variances. In a very general sense if σα is not an order of
magnitude smaller than σy, then the group distinctions are likely to make substantive sense.
However, if σα is vastly smaller than σy, then the group distinctions speciﬁed are unlikely
to matter. Notice that this comparison is on the standard deviation metric. Sometimes this
comparison is formally described with the intraclass correlation coeﬃcient using variances:
ICC =
σ2
α
σ2α + σ2y
,
(12.4)
although this still does not supply a distributional test. Sagan (2013, p.583) suggests that

Bayesian Hierarchical Models
417
the ICC can be as low as 0.05 to 0.20 and still justify the hierarchy (see Muth´en and Satorra
[1995] as well).
Consider the following approximation for the mean of the jth group (Gelman and Hill
2007, p.253):
αj ≈
nj
σ2y
¯yj + 1
σ2α
¯yall
 -nj
σ2y
+ 1
σ2α

.
(12.5)
where:
¯yj
unpooled estimate for group j
¯yall
completely pooled estimate
σ2
y
assumed equal within-group variance
σ2
α
variance among the mean estimates
Although this is an approximation (we get fuller results from MCMC output but with less
intuition on these issues), it reveals the important structure of multilevel models. Ignoring
the denominator for the moment, the numerator reveals an important aﬀect from the size
of group j. Obviously the numerator is a weighting of the group mean and the full sample
mean from all groups. If nJ is small then αj will be more inﬂuenced by ¯yall. Therefore
such small groups retain their unique identity and they are also able to “borrow strength”
from other groups through the overall mean in this weighting. In addition, averages from
groups with smaller sample sizes contribute less to ¯yall since they are a smaller weighted
contribution to the sum. Suppose a new group is discovered to exist, but there are no
samples made available within this group. The αj for this group with nj = 0 (no observed
values!) needs to balance its own zero weighting relative to what is known about the group
αj = ¯yall. This makes total sense: an unknown group that belonged in a set of groups
should get the best available estimate, which is the overall mean. Conversely, groups with
larger sample sizes have much greater inﬂuence over the overall model through their larger
contribution to ¯yall. Also, their value of αj is less reliant on the rest of the sample due to
multiplication by nj. Furthermore, as nj increases to some size much larger (all the way to
inﬁnity), group j will increasingly dominate the value of ¯yall.
We can also consider variance eﬀects in (12.5). First split by the numerator for additional
clarity:
αj ≈
nj
σ2
y ¯yj
nj
σ2y +
1
σ2α
. +
1
σ2α ¯yall
nj
σ2y +
1
σ2α
.
(12.6)
As σ2
α →0, the ﬁrst term in the right-hand-side above goes to zero since 1/σ2
α is in the
denominator and nowhere else. However, in the second term 1/σ2
α is in both the denominator
and the numerator and therefore cancels out leaving ¯yall since nj/σ2
y will be negligible as an
addition as 1/σ2
α gets very large. Therefore as group diﬀerences disappear it makes sense
simply to move to the completely pooled estimate. On the other hand, as σ2
α →∞the
second term in the right-hand-side goes to zero due to the numerator, but the ﬁrst term
simply reduces to ¯yj since the nj/σ2
y terms cancel out after 1/σ2
α goes to zero. In this case

418
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
the best estimate of αj is the completely unpooled estimate. So in this way multilevel model
speciﬁcations are self-regulating by σ2
α: the degree to which this value is large compared to
σy is the degree to which the group distinctions are justiﬁed.
It is important to fully understand the role of σ2
α. As odd as it sounds, we want this
variance to be large since it justiﬁes the group distinctions that form the hierarchy. Large
values relative to σy simply mean that the groups are suﬃciently diﬀerent that adding a
second level improves the ﬁt of the model by moving variability from σ2
y to σ2
α. This also
means that if there are other non-nesting (distinct) group diﬀerences that exist, adding this
feature to the model will continue to improve ﬁt (producing another between group term,
say σβ). Conversely, if the groupings matter and are ignored then σ2
y will be larger, and
the model will have a poorer ﬁt. Also, as n increases (even asymptotically), the variance
measured by σ2
α will not go to zero.
This is because this inter-group set of diﬀerences
still persists, if valid, even in the presence of more data in general. Obviously under this
circumstance σy will decrease, and this diﬀerence in eﬀects demonstrates the fundamentally
diﬀerent roles that these variances have in a multilevel model.
■Example 12.1:
Bayesian Multinomial Speciﬁcations for Employment Status.
To illustrate how the diﬀerent models above can be applied in practice, we look at
an application from Pettitt et al. (2006) where they analyze the employment status
of 5192 immigrants to Australia (the population of such persons 15 years or older,
between September 1993 and August 1995), with the outcome variable being an 11-
category work-status outcome measured at three time periods. In the lowest level
of the hierarchical model, individual status is assumed to be distributed multinomial
where the ith individual at time period t has a J = 11-length outcome vector with
10 zeros plus a single i1, deﬁned as: yit ∼
multinomialj(pit, 1), where pitj is the
probability for person i, time t, and category j. The second level of the hierarchy
connects pitj to modeled causal eﬀects through a standardized term:
log(μitj) = X′
itβj + Z′
itγj + αij + ϵitj
pitj =
μitj
J
j=1 μitj
(12.7)
where Xit is a matrix of explanatory variables and Zit is a matrix of lagged out-
come variables, βj and γj are associated vectors of unknown parameters, and αij is
a random eﬀect term. The model is not identiﬁed without a reference category, and
category 1 (“wage or salary earner”) is chosen (β1, γ1, αi1 all set to zero) such that
log(pitj/pit1) = log(μitj/μit1) = log(μitj) for j ̸= 1. It is assumed that the remain-
ing αij are multivariate normal: αi = (αi2, αi3, . . . , αij) ∼MVN(0, Σ). Now with
uninformative prior speciﬁcations (page 147), the joint posterior can be completed
according to:
p(β, γ, α, Σ|X, y) ∝p(X, y|β, γ, α, Σ)p(α|Σ)p(β)p(γ)p(Σ).
(12.8)

Bayesian Hierarchical Models
419
See also the Bayesian multinomial probit speciﬁcation of Imai and van Dyk (2005) for
a competing approach.
The interesting part of this example from our perspective is how ﬂexible the hier-
archical model can be with only these simple components. Pettitt et al. specify six
fundamentally diﬀerent models by varying the use of the model components above
across individuals, time, and response treatment:
▷Model 1: E[log(μitj)] = X′
itβj.
The eﬀect of the (unlagged only) regression parameters diﬀers across outcome
categories j, but remains constant for all individuals and over time given a state
j. So employment probabilities are constant across people and constant over
time.
▷Model 2: E[log(μitj)] = X′
itβj + αij.
The eﬀect of the regression parameters diﬀers across outcome categories j but
not over time or individuals, and a random eﬀect term captures within-individual
clustering (i.e., varying across individuals) but is constant over time given a state
j. So employment prospects are constant over time, but diﬀerent for individuals
where this diﬀerence does not change over time.
▷Model 3: E[log(μitj)] = X′
itβj + Z′γj.
The eﬀect of the β regression parameters diﬀers across outcome categories j for
the current time period, the eﬀect of the γ regression parameters diﬀers across
outcome categories j for the lagged time period, and no between-individual vari-
ation is modeled, given state j. So employment prospects are constant across
people again (Model 1), but now there is a lagged time eﬀect that alters proba-
bilities going forward.
▷Model 4: E[log(μitj)] = X′
itβj + Z′γj + αij.
The eﬀect of the β regression parameters diﬀers across outcome categories j
for the current time period, the eﬀect of the γ regression parameters diﬀers
across outcome categories j for the lagged time period, and between-individual
variation is modeled with the random eﬀect, given state j.
So employment
prospects are constant across people again and there is a lagged time eﬀect that
alters probabilities going forward (Model 3), but now a diﬀerent eﬀect across
individuals where this diﬀerence does not change over time (Model 2).
▷Model 5: E[log(μitj)] = X′
itβj + Z′γij.
This is exactly the same as Model 3, except that the lagged eﬀects are now
assumed to be time-varying in their eﬀect across panels. Since the panels are
not equally spaced, this may be a more robust speciﬁcation. So employment
prospects are constant across people (Model 1), there is a lagged time eﬀect that
alters probabilities going forward (Model 3), and now this lagged time eﬀect is
individualized.
▷Model 6: E[log(μitj)] = X′
itβj + Z′γij + αij.

420
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
This is the same as Model 5, but adding back the random eﬀect to capture
between-individual variation. So employment prospects are constant across peo-
ple (Model 1), there is a lagged time eﬀect that alters probabilities going forward
(Model 3), this lagged time eﬀect is individualized (Model 5), and an added dif-
ferent eﬀect across individuals where this diﬀerence does not change over time
(Model 2).
How many more could we have? Standard extensions include interaction eﬀects be-
tween levels, diﬀerent lagged periods, smoothing of some covariates (generalized ad-
ditive models), spatial terms, and more.
Marginal posterior distributions are produced using BUGS (10,000 iterations, disposing
of the ﬁrst 4,000) and compare ﬁts with the DIC (Section 7.5). The authors ﬁnd poor
convergence of some Σ values for models 4 and 6, leading to the conjecture that the
data do not support inclusion of both γ and α in the model. Model 1 is dismissed as
too basic, and the DIC points towards models 3 and 5 as ﬁtting better than model 2.
12.4
Exchangeability
Exchangeability (sometimes also called symmetry or permutability) is an important
property for hierarchical models. As mentioned in Chapter 1, exchangeability allows us
to say that the data are produced conditional on the unknown model parameters in the
same way for every data value. More technically, given a sample X1, X2, . . . , Xn and any
possible permutation of these data X[1], X[2], . . . , X[n], the data are exchangeable if the
joint distributions are equal: f(X1, X2, . . . , Xn) = f(X[1], X[2], . . . , X[n]), i.e., invariant to
permutation. This is equivalent to saying that the subscripts (the labeling of the data)
are uninformative in the sense that any subset of the data are assumed to have the same
marginal distribution. Actually, as described this is ﬁnite exchangeability, meaning that n
is ﬁxed here. We can obtain the deﬁnition of inﬁnite exchangeability by simply adding the
condition that every ﬁnite subset of an inﬁnite series of Xi is exchangeable in the sense
above.
The provided deﬁnition of exchangeability has thus far conveniently omitted any depen-
dence on parameters. The role of exchangeability becomes more obvious when we include
the typical dependence of the data generation process on a parameter or parameter vector.
Suppose that Xi is a series of inﬁnitely exchangeable series of Bernoulli random variables.
de Finetti’s (1930) famous representation theorem states that there is guaranteed to exist
a unique probability measure Q(θ) so that:
f(x1, x2, . . . , xn) =
 1
0
n
+
i=1
θxi(1 −θ)1−xidQ(θ),
(12.9)

Bayesian Hierarchical Models
421
where θ −→
n→∞
1
n
 xi. What this implies is that conditional on θ, the exchangeable data
are now iid.
Diaconis and Freedman (1980) provide some technical diﬀerences between
the ﬁnite and inﬁnite applications of de Finetti’s theorem. The theorem does not quite
hold in the ﬁnite case and Diaconis and Freedman give the error bound for the ﬁnite case
distance to the nearest iid set.
Also, it is usually the case that the probability measure
is suﬃciently well-behaved as to deﬁne a proper PDF: dQ(θ) = p(θ)dθ. This allows us to
produce a justiﬁcation for Bayesian inference based on making this substitution using the
deﬁnition of conditional probability:
f(x1, x2, . . . , xn) =
 1
0
p(x1, x2, . . . , xn, θ)dθ
=
 1
0
p(x1, x2, . . . , xn|θ)p(θ)dθ
=
 1
0
n
+
i=1
θxi(1 −θ)1−xi
 
!"
#
likelihood function
p(θ)
 !"#
prior
dθ.
(12.10)
This is easily recognized as the denominator in Bayes’ Law. More importantly it shows that
we can suppose a latent random variable θ interpreted as the limiting frequency of p(x = 1).
Now conditional on this θ the xi values are distributed iid Bernoulli with parameter θ:
x|θ ∼BR(θ)
θ ∼
iid p(θ).
(12.11)
This is now (barely) hierarchical in the sense that there is a model ﬁrst for manifest zeros
and ones and a higher level model for actual rate determining generation. So while we
cannot observe θ, we are not precluded from speciﬁcally modeling it as a random quantity,
where the distributional feature expresses our uncertainty about this unseen parameter. de
Finetti’s 1937 proof is somewhat lengthy and need not be replicated here. Bernardo and
Smith (1994, Section 4.3) give a rough sketch of the proof steps, Hartigan (1983) gives a
proof based on Baire functions, and Heath and Sudderth (1976) give a briefer but accessible
description. Also see Haag (1924) for perhaps the ﬁrst discussion of this idea.
Also, while the theorem is given so far only for Bernoulli random variables, the class of
probability model to which it applies is much larger and includes the standard distributions
we commonly work with (de Finetti 1937). In particular, Freedman and Diaconis (1982)
proved that de Finetti’s property holds true for a mixture of location symmetric interval
measured random variables (symmetric about a location parameter, which itself is a random
quantity, obviously an important context to Bayesians). Consider an inﬁnitely exchangeable
set of continuously measured random variables from the distribution F, x1, x2, . . . , xn, with
probability measure P such that:
P(x1, x2, . . . , xn) =

D
n
+
i=1
f(xi)dQ(F),
(12.12)

422
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
where Q is now a probability measure over D, the space of all distribution functions de-
ﬁnable over the real line (Bernardo and Smith 1994). Furthermore Q(F) is the limiting
distribution of the empirical distribution of the xi, meaning that the hierarchical distribu-
tion speciﬁes F for some distribution over all possible distributions. Clearly this is much
more abstract and less practical than the deﬁnition given before, but it motivates the treat-
ment of the prior in hierarchical terms for an extremely broad set of speciﬁcations.
The underlying implications of de Finetti’s theorem are vast.
As previously stated,
frequentists believe that there is an unknown but ﬁxed parameter, θ, in the data generation
process. The theorem gives the asymptotic distribution of θ as a quantity from the data,
not the other way around as given in introductory texts. Here θ is assigned a distribution
and can therefore be thought of subjectively as a belief about the probability of success.
In fact, this notion is the philosophical basis for some Bayesians’ rationale for treating
parameters as random quantities. For instance: “It is my tentative view that the concept of
personal probability, introduced and in the preceding chapter, is except possibly for slight
modiﬁcations, the only probability concept essential to science and other activities that
call upon probability” (Savage 1954, p.56).
Jeﬀreys (1961, p.401) is equally direct: “The
essence of the present theory is that no probability, direct, prior, or posterior is simply a
frequency.” Such opinions and counter-opinions convulsed thought on Bayesian inference
for at least 200 years. The interested reader is referred for details to Dale (1991), Earman
(1992), Lindley, Tversky, and Brown (1979), Smith (1965), and Suppes (1974), just to start.
Actually this is not just a completely theoretical or epistemological debate as the eﬀects
of researcher subjectivism can be central to assessing the validity of empirical research in
the social sciences; see, for instance, the controversy over Margaret Mead’s famous work in
Samoa (Freeman 1983; Mead 1973; Orans 1996).
Actually, de Finetti’s work is seen by some as the key link between subjective probabil-
ity and the formulation of Bayesian models (Lindley and Novick 1981; Smith 1984), and at
least one set of authors make the claim that exchangeability is actually an even more funda-
mental notion than that of probability because the individual psychological interpretation
of similarity is more native than that of relative frequency (Draper et al. 1993).
Critically de Finetti (1930) points out that since exchangeability is a weaker condition
than iid (iid actually implies exchangeability), then frequentist inference is a special case of
Bayesian inference: the iid assumption gives a limiting frequency θ and the exchangeability
assumption gives the same limiting frequency except for a region of the measure space with
probability zero. This also means that the strong law of large numbers results from the
theorem (Press 1989).
What does this mean to hierarchical models? We can re-express de Finetti’s theorem in
the following hierarchical way for X1, X2, . . . , Xn that are exchangeable:
θ ∼p(θ)
f(X|θ) ∼
iid BN(n, θ).
(12.13)
The theorem shows that exchangeability and independence are related in a hierarchical

Bayesian Hierarchical Models
423
model through conditionality (as noted above). Suppose now that we couldn’t be certain
that every single Xi is generated from a conditional distribution with the same value of θ:
f(Xi|θ). In fact, the only information that we can be absolutely sure about is that each
Xi is generated from some θi. This obviously creates an impractical estimation process.
However, if we impose the assumption of exchangeability on the θi, then by de Finetti’s
theorem, we can treat the joint distribution as if it came from a mixture of iid speciﬁcations:
f(X1, X2, . . . , Xn|θ1, θ2, . . . , θn) = n
i=1 f(X|θ).
To generalize to a broader class of hierarchical models, start with a potentially inﬁ-
nite exchangeable series of continuous-measured random variables: x[1], x[2], . . . , x[N], with
parameters θ1, θ2, . . . , θN. At the ﬁrst stage, relate the data to parameters:
f(x[1], x[2], . . . , x[N]|θ1, θ2, . . . , θN) =
N
+
i=1
f(xi|θi).
(12.14)
So the x[i] are independent conditional on the θi. Now deﬁne a second stage modeling the
θi parameters as if they are a random sample from a new parametric family
g(θ1, θ2, . . . , θN) =
N
+
i=1
g(θi|φ).
(12.15)
Exchangeability means that we can treat the θi as iid realizations from this distribution.
Finally, specify a hyperprior that deﬁnes the form of φ:
φ ∼h(φ|A, B),
(12.16)
where A and B are either ﬁxed hyperparameters or random in a higher level of hyperprior
hierarchy. This series of independent hierarchical speciﬁcations must end at some point and
the obvious question that arises is how do we specify the top level of ﬁxed hyperparameter
values. Two approaches dominate. The diﬀuse hyperprior approach stipulates making the
highest level of priors as uninformative as possible by picking hyperparameters that give
a broad and ﬂat form (Pericchi and Nazaret 1988). This is the approach taken in many
of the canned BUGS examples provided by Spiegelhalter et al. (1996a, 1996b, 2012) and
subsequently (unfortunately perhaps) retained by many users of the software.
■Example 12.2:
Exponential-Gamma Model of Clinical Eﬀects. This is an ex-
ample from pharmapsychology, but the clustering problem is a more general one
resolved by hierarchical models. In addition, we will assess whether it is reasonable
to assume exchangeability of units across diﬀerent modeled groups (clinics). Stangl
(1995) is concerned with modeling the time to recurrence of depression in patients
served at ﬁve diﬀerent clinics. The patients received antidepressant medication for
eight weeks and then were randomized to either continue to take the drug (treat-
ment, coded 1) or taken oﬀmedication (control, coded 0). This is a hazard model
(see page 317 for a deeper description of the assumptions, and Dellaportas and Smith
[1993] for details on a Gibbs sampler tailored for such models) because the outcome

424
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
variable is the time to ﬁrst recurrence of depression (censored at two years). The data
values for each patient, labeled tijk for the ith patient at the jth clinic getting the
kth treatment, are modeled with an exponential distribution at the ﬁrst level. The
hierarchy is summarized by:
tijk|θj ∼EX(θj)
θj|α, β ∼G(α, β)
α = 5,
β ∼G(0.172, 5).
(12.17)
The rationale behind prior and hyperprior selection starts with noticing that the larger
the α parameter, the greater the inﬂuence of the prior relative to the data. Several
values were selected, including the rather diﬀuse α = 5, during a sensitivity analysis.
Holding α constant, β is interpreted as the sum of α exponential survival times, and
therefore modeled gamma since the sum of independent exponential random variables
is gamma distributed. The parameters of the gamma distribution at the third level
are set up by setting the ﬁrst equal to the pooled hazard rate before specifying a
model, and the second equal to α. Therefore the hierarchy is a compromise between
the assumed level of prior information and the data in a quasi-empirical Bayes fashion.
See Ibrahim et al. (2001) for a broader discussion of Bayesian survival models.
TABLE 12.1:
Depression Treatment by Clinic,
Control Group
Clinic
A
B
C
D
E
Sample Size
11
10
8
25
17
Number of Events
6
6
7
12
15
Sum of Events Times
499
391
177
1367
240
MLE Value
0.012
0.015
0.040
0.009
0.063
Mean Recurrence Time at MLE
83
67
25
111
16
Posterior Mean
0.012
0.013
0.023
0.010
0.041
Posterior Mean Recurrence Time
83
77
43
100
24
More importantly, the degree to which the hierarchical posteriors diﬀer from sepa-
rately estimated maximum likelihoods by clinic indicates the extent to which there is
“sharing” in the data across clinics: evidence to support the assumption of exchange-
ability. Table 12.1 indicates that for the control group the Bayesian posterior modes
are more compactly distributed than the MLEs, supporting the idea that there are
programmatic commonalities.

Bayesian Hierarchical Models
425
12.5
Essential Structure of the Bayesian Hierarchical Model
As always, our central interest is in the generation of a posterior distribution from the
likelihood times the prior:
π(θ|X) ∝L(θ|X)p(θ).
(12.18)
Following O’Hagan (1994) or Berger (1985), now suppose that the parameter θ is conditional
on another unknown parameter ψ, which will be given its own prior distribution p(ψ). The
calculation of the posterior becomes:
π(θ, ψ|X) ∝L(θ|X)p(θ|ψ)p(ψ),
(12.19)
which can be quite straightforward if the forms of p(θ|ψ) and p(ψ) are cooperative. Many
times this seemingly simple extension complicates the production of (12.19) to the point
where MCMC tools are required to get the marginal distribution of θ. Here θ has the prior
distribution, p(θ|ψ), but it in turn is now conditional on another parameter that has its
own prior, p(ψ), called a “hyperprior,” which can also have “hyperparameters” if desired.
Inference for either parameter of interest can be obtained through looking at the respective
marginal densities:
π(θ|X) =

ψ
π(θ, ψ|X)dψ
π(ψ|X) =

θ
π(θ, ψ|X)dθ,
(12.20)
where the other parameter has been integrated out of the joint posterior. What should be
discernible from (12.19) is that we can continue stringing hyperpriors to the right, beginning
with making ψ conditional on another parameter, p(ψ|ζ), and adding its new hyperprior
distribution, p(ζ), to the calculation of the posterior:
π(θ, ψ, ζ|X) ∝L(θ|X)p(θ|ψ)p(ψ|ζ)p(ζ).
(12.21)
Now ζ is the highest level parameter and therefore the only hyperprior that is unconditional.
In more realistic settings we will want to think of ψ as a parameter vector with its own
hyperparameters and dependencies: ψ = (ψ1, ψ2 . . .). As a simple example, consider data
that are normally distributed with known variance and unknown mean, Xi ∼N(μi, σ2
0),
i = 1, . . . , n, as described in Section 3.2 (page 70), except that instead of an unknown
constant μ we assume random μi, i = 1, . . . , n that are drawn independently from a common,
also normal, distribution: N(mμ, s2
μ). The model can be summarized as:
⎡
⎢⎢⎢⎢⎣
X1
X2
...
Xn
⎤
⎥⎥⎥⎥⎦
∼N
⎛
⎜
⎜
⎜
⎜
⎝
⎡
⎢⎢⎢⎢⎣
μ1
μ2
...
μn
⎤
⎥⎥⎥⎥⎦
, σ2
0I
⎞
⎟
⎟
⎟
⎟
⎠
⎡
⎢⎢⎢⎢⎣
μ1
μ2
...
μn
⎤
⎥⎥⎥⎥⎦
∼N(mμ, s2
μ).
(12.22)

426
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
The key here is that the Xi values are assumed to be generated by distinct underlying
means in the normal speciﬁcation, but these means are themselves drawn from a common
distribution with ﬁxed hyperparameters: mμ, s2
μ. This is essentially the same idea as the
non-Bayesian multilevel models above except that a second stage is explicitly given in the
form of a common prior distribution for the unknown μi. It diﬀers, however, from the model
on page 71 in that more uncertainty is imposed into the data generation process for X. This
model can be further extended in two ways. First, by making mμ and s2
μ additional random
quantities with their own prior distributions, we add to the depth of the hierarchy. Second,
as we add hierarchies to the model, we can specify how data at diﬀerent levels enter the
model speciﬁcation. For instance, we might specify that the second-level model for mμ is
also normal, N(Zγ, Σ), with data Z that speciﬁcally aﬀect the mean.
While there is no theoretical restriction to the number of levels that can be speciﬁed in
the hierarchy, the practical restriction is that in speciﬁcations that have greater than three
or four levels, the interpretation of the estimated coeﬃcients can be challenging. Frequently
there is little reason to go beyond a two-level model in terms of nesting. Goel and DeGroot
(1981) show that posterior certainty about parameters decreases at each progressive level
of the hierarchical model. This is formalized by Goel (1983, Theorem 3.2) using the idea
of the conditional amount of sample information (CASI), which is the “distance” between
the prior and the posterior, indicating the relative contribution of the data. Deﬁne the f
divergence measure of distance between two distributions g1(θ) and g2(θ) deﬁned on the
sample space of θ by:
IF (g1, g2) =

Θ
f
g1(θ)
g2(θ)

g2(θ)dθ,
(12.23)
for an arbitrary convex function labeled f() deﬁned on (0:∞), where standard measure-
theoretic properties are assumed to hold. This is similar to the Kullback-Leibler distance
and slightly more general in that the natural log function certainly qualiﬁes as a candidate
for f() (note, however, that the order of the fraction is reversed relative to the outside term).
Titterington, Smith, and Makov (1985, p.116) give a nice summary of such related distance
measures. Goel’s theorem means that after the second level of hierarchy, IF (g1(θi), g2(θi+k))
is a decreasing function with increasing k. In other words, hyperparameters that are far-
ther up the hierarchy have lower levels of information supplied by the data. This idea is
actually very intuitive, given that each additional level is separated by one more parametric
assumption from the root data-level of the hierarchy.
There is also a distinction between nested hierarchies and non-nested hierarchies. Nest-
ing means that a hierarchy exists at a level above some hierarchy and that this hierarchy
does not aﬀect an other hierarchy at lower levels. We now assert that ψ in (12.19) had
further parameterization instead of just a distribution p(ψ): ψk|δ = f(δk0 + δk1Wk1 + v),
where δk0 and δk1 are given further distributions (typically conditional on further hyper-
parameters), Wk1 is a set of covariates at this higher level of the hierarchy, and v is an
additional error term. A speciﬁc example of this setup is given in the next section. Non-
nested hierarchies do not aﬀect each other but jointly aﬀect a lower node. So if the jointed

Bayesian Hierarchical Models
427
aﬀected quantity is the data in a normal model, then a non-nested speciﬁcation looks like:
yi ∼f(μ + θji + δki, σ2
y),
for i = 1, . . . , n
θj ∼g(. . .),
for j = 1, . . . , J
δk ∼h(. . .),
for k = 1, . . . , K
where f(), g(), and h() are user-speciﬁed distributions, j = 1, . . . , J index the θ hierarchy,
and k = 1, . . . , K index the δ hierarchy. In this way these higher levels aﬀect the data level
without interacting in any way, although additional model features can be added to associate
them. The ﬂexibility in diﬀerent arrangements of nested and non-nested hierarchies provides
a dramatically greater number of possible model speciﬁcations for a given dataset.
12.5.1
A Poisson-Gamma Hierarchical Speciﬁcation
Building on a speciﬁcation already presented, suppose that the observed outcome vari-
able is modeled Poisson. The obvious choice for a prior on the intensity parameter, λ, is
a gamma distribution since not only is it conjugate, but also because it is a very ﬂexi-
ble parametric form (i.e., it can be parameterized to provide many diﬀerent shapes over
positive real support). This gamma distribution is in turn characterized by two further
parameters (often labeled α and β), which can also be assigned their own hyperpriors to
reﬂect diﬀerences at the next level in the model speciﬁcation. Of course these gamma PDF
parameters are restricted to the positive real numbers just like λ, so it is logical to consider
giving them gamma priors as well. In doing so, we also need to determine parameters for
the gamma distributions assigned to α and β. If they are assigned ﬁxed values, they are
called hyperparameters. Otherwise we can add another level to the hierarchy and assign
an additional round of hyperpriors. Figure 12.1 graphically displays the model where the
gamma distributions for α and β are the ﬁnal hierarchical level.
It is important to notice that the intensity parameter of the Poisson distribution is now
indexed by i since it is no longer assumed to be a ﬁxed eﬀect that is constant in its impact
on the data. Instead, we are now asserting that λ varies across cases in the data, and is
therefore a random eﬀect whose distribution is assumed here to be gamma: λi ∼G(α, β).
This is a fundamentally new and diﬀerent idea from the types of models estimated in
previous chapters. The relationship between cases in the data has now changed: instead of
a sampling process that comes from a single distribution function, we have substituted a
data generation process where each case is produced by a distinct but related mechanism
because of the common distribution.
It is conventional to represent hierarchical models by “stacking” the parametric speci-
ﬁcations from the data-level speciﬁcation to the highest-level hyperprior speciﬁcation. For

428
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
yi
λi
Gλ(α, β)
Gα(A, B)
Gβ(C, D)
Observed Values
Poisson Parameter
Gamma Prior
Gamma Hyperpriors
FIGURE 12.1: Poisson-Gamma Hierarchical Model
the example model, this representation is:
yi ∼P(λi)
λi ∼G(α, β)
α ∼G(A, B)
β ∼G(C, D),
(12.24)
where the yi are assumed conditionally independent given λ, and α and β are assumed
independent. Here we can see very clearly that not only does the Poisson intensity parameter
have its own speciﬁed distribution, but also the two gamma parameters that determine this
Poisson distribution have their own speciﬁed distribution as well.
Other non-gamma priors are also feasible here. George, Makov, and Smith (1993) con-
trast four diﬀerent hyperparameter speciﬁcations using this model and demonstrate the
ﬂexibility of the speciﬁcation.
Cohen et al. (1998) apply a reference prior speciﬁcation
p(α, β) ∝[α(1 + β)2]−1 in their study of arrest rates for drug oﬀenders and apply a mixed
Gibbs-Metropolis chain since the full conditional distributions for α and β are then no
longer available.
We can write a complete speciﬁcation for the posterior distribution even though there
is now additional complexity from having higher levels in the model. Since each hyperprior
is a prior for a lower-level prior distribution, it is possible to assemble the joint distribution
multiplicatively using Bayes’ Law and the deﬁnition of conditional probability includes each

Bayesian Hierarchical Models
429
of the hierarchical speciﬁcations (and dropping conditionality on constants on the left-hand
side):
p(y, λ, α, β) =
n
+
i=1
p(yi|λi)p(λi|α, β)p(α|A, B)p(β|C, D)
=
n
+
i=1
'
(yi!)−1λyi
i exp(−λi)βαΓ(α)−1 exp(−λiβ)λα−1
i
× BAΓ(A)−1 exp(−αB)αA−1DCΓ(C)−1 exp(−βD)βC−1
(
,
so:
p(λi, y, α, β) ∝λ
 yi+α−1
i
αA−1βα+C−1Γ(α)−1 exp[−λi(1 + β) −αB −βD].
(12.25)
This looks forbidding, especially since there are three posterior parameters of interest. Note
the role of exchangeability here for the λi since the yi all have the same relationship to the
hyperprior speciﬁcations through the λi. Once the y data are observed, we can obtain the
joint posterior distribution of interest according to:
π(λi, α, β|y) =
p(y, λi, α, β)

p(y, λi, α, β)dλdαdβ
=
λ
 yi+α−1
i
αA−1βα+C−1Γ(α)−1 exp[−λi(1 + β) −αB −βD]

λ
 yi+α−1
i
αA−1βα+C−1Γ(α)−1 exp[−λi(1 + β) −αB −βD]dλdαdβ
,
=

λ
 yi+α−1
i
αA−1βα+C−1Γ(α)−1 exp[−λi(1 + β) −αB −βD]
 )

λ
 yi+α−1
i
αA−1βα+C−1Γ(α)−1
× exp[−λi(1 + β) −αB −βD]dλdαdβ

,
(12.26)
which is unpleasant. Recall from Chapter 10 that if we can get the full conditional dis-
tributions for each of the coeﬃcients in the posterior, we can run the Gibbs sampler and
obtain the marginal posterior distributions with MCMC. The hierarchical model expressed
in stacked notation immediately gives conditional expressions for the speciﬁed variables.
First consider the full conditional distribution for λ, using the properties of conditional
probabilities, and the assumption that α and β are independent:
π(λ|α, β) = p(λ, α, β|y)
p(α, β|y)
= p(λ, α, β)
p(α, β)
= p(λ, α, β)
p(α)p(β)
∝λ
 yi+α−1 exp[−λ(1 + β)].
(12.27)
So the random variable λ|α, β ∼G(yi + α, 1 + β), and given (interim) values for α and
β, we can sample from the distribution for λ. This turns out to be the only real hard work

430
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
required since: p(α|β, λ) is just p(α), and p(β|α, λ) = p(β), by the initial assumptions of the
hierarchical model. Therefore we now have all three full conditional distributions required
and the Gibbs sampler can be implemented quite easily.
There are quite a few studies that use this exact Poisson-gamma hierarchical model or
some close variant of it, including: Carlin and Louis (2009, pp.32-37), Cohen et al. (1998),
Christiansen and Morris (1997), DeSouza (1992), Hadjicostas and Berry (1999), Leonard,
Hsu, and Tsui (1989), Makov, Smith, and Liu (1996), Newton and Raftery (1994), Lawson
(2013, Chapter 2), Haque, Chin, and Huang (2010), De Oliveira (2013), and Neyens, Faes,
and Molenberghs (2012). The subsequent posterior estimation tasks in all of these works
necessitate stochastic simulation rather than analytical methods. This is common as hier-
archical speciﬁcations in general lead to posteriors that are typically estimated by MCMC,
and Gibbs sampling in particular since the speciﬁcation usually gives easily determined full
conditional distributions for the parameters.
Christiansen
Chain Iteration
Chain Values for Alpha
2
4
6
8
10
12
14
100K
102K
104K
106K
108K
110K
Chain Iteration
Chain Values for Beta
0.5
1.0
1.5
100K
102K
104K
106K
108K
110K
FIGURE 12.2: Traceplot of Output for Gamma
■Example 12.3:
Marriage Rates in Italy. This is an example of the Poisson-gamma
hierarchical model just described. The data come from annual marriage counts per
1,000 of the population in Italy from 1936 to 1951 (from Columbo 1952). The substan-
tive question addressed is whether it is practical to model marriage rates that occurred
during World War II to rates just before and after. The data (yi from above) are given
by Table 12.2.
TABLE 12.2:
Italian Marriage Rates per
1,000, 1936-1951
7
9
8
7
7
6
6
5
5
7
9
10
8
8
8
7

Bayesian Hierarchical Models
431
We clearly would not want to model these data with a nonhierarchical speciﬁcation
because that would require the Poisson intensity parameter to be constant over war
and non-war years. The hierarchical model substitutes that assumption with the as-
sumption that the λi are diﬀerent in each year but drawn from a common distribution.
Obviously this is a more ﬂexible speciﬁcation. However, if the hierarchical approach
fails to ﬁt, then there is evidence that the marriage rates are so fundamentally diﬀerent
that a wartime indicator parameter is needed.
The model is speciﬁed with relatively uninformed priors for α and β: both G(1, 1).
The following simple BUGS code runs this model:
model {
for (i in 1:N) {
lambda[i] ~ dgamma(alpha,beta);
y[i] ~ dpois(lambda[i]);
}
alpha ~ dgamma(1,1);
beta ~ dgamma(1,1);
}
These data are available in the BaM package, but the small size makes them easy to
process separately as well. The WinBUGS data and initial value statements are for
comparison simply:
list(y=c(7,9,8,7,7,6,6,5,5,7,9,10,8,8,8,7), N=16)
list(alpha=10, beta=10)
whereas the JAGS data ﬁle contains stored here in italy.jags.data:
N <- 16
y <- c(7,9,8,7,7,6,6,5,5,7,9,10,8,8,8,7)
and the JAGS initial value ﬁle, italy.jags.inits, contains:
alpha <- 10
beta <- 10
This is all that is needed to run this JAGS scriptﬁle:
model in "italy.bug"
data in "italy.jags.data"
compile

432
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
inits in "italy.jags.inits"
initialize
update 50000
monitor set alpha
monitor set beta
monitor set lambda
update 50000
coda *
The model is run for 50,000 iterations of the chain without recording the chain values
This is a very conservative burn-in period, but comes with a relatively low cost with
such a simple model. Recording the chain values is turned on in BUGS and 50,000
iterations are run. Table 12.3 gives the summary statistics from these chain values.
Figure 12.2 provides traceplots of the last 10,000 chain values for α and β. A traceplot
is a common visual diagnostic for MCMC work in which a time-series of the chain
values over the selected period are displayed. Prolonged visible upward or downward
trends are generally evidence of nonconvergence.
TABLE 12.3:
Posterior Summary Statistics, Italy
Model
Parameter
Mean
Std. Error
Median
95% HPD Region
α
5.166
1.777
4.935
[1.985: 8.644]
β
0.717
0.264
0.683
[0.240: 1.221]
1936
7.122
2.108
6.903
[3.362:11.400]
1937
8.301
2.284
8.062
[4.311:12.997]
1938
7.712
2.172
7.516
[3.785:11.982]
1939
7.100
2.123
6.861
[3.253:11.361]
1940
7.126
2.121
6.911
[3.316:11.380]
1941
6.509
1.981
6.312
[2.819:10.309]
1942
6.507
2.006
6.306
[2.887:10.506]
1943
5.930
1.904
5.734
[2.498: 9.698]
1944
5.934
1.902
5.756
[2.426: 9.654]
1945
7.080
2.073
6.840
[3.369:11.241]
1946
8.293
2.316
8.080
[4.032:12.879]
1947
8.899
2.373
8.678
[4.780:13.769]
1948
7.708
2.168
7.488
[3.889:12.164]
1949
7.721
2.147
7.511
[3.771:11.877]
1950
7.668
2.188
7.452
[3.686:11.937]
1951
7.110
2.103
6.916
[3.386:11.402]

Bayesian Hierarchical Models
433
0
2
4
6
8
10
12
95% HPD Interval
alpha
0.0
0.5
1.0
1.5
2.0
95% HPD Interval
beta
FIGURE 12.3: Smoothed Posteriors for α and β
We are primarily interested in α and β since these values completely describe the
distribution of the λi’s. Figure 12.3 shows the smoothed density estimates of the last
10,000 iterates of the chain for α and β.1 It is apparent from these density estimates
that posterior distribution is unimodal and close to symmetric. We will focus ex-
tensively on MCMC convergence diagnostics in Chapter 14, but for now it is worth
mentioning that two popular convergence diagnostics (from the CODA and BOA diagnos-
tic suites in R) do not provide any evidence of nonconvergence. Geweke’s diagnostic
based on normal theory shows an α variable z-score of -1.1973498 and a β variable
z-score of -1.337 (being in the tails of N(0, 1) indicates evidence of nonconvergence).
The Heidelberger and Welch diagnostic gives Est.SE¯α = 0.225 > HW.CI¯α = 0.128,
and Est.SE ¯β = 0.258 > HW.CI ¯β = 0.019. If the halfwidth of the credible interval for
the mean is greater than a rough estimate of the variance of the sample mean given
by the sample mean times some ϵ, usually defaulted to 0.1, then this test provides
evidence of nonconvergence.
We also might be interested in the distribution of the λi parameters. Returning to the
substantive question, it is clear that there is a drop in marriage rates during the war, as
evidenced in Figure 12.4, which gives the posterior mean and error bars corresponding
1See Venables and Ripley (1999, pp.132-141) for speciﬁc guidance on producing smoothed density es-
timates in R. The frequently used density function is part of the base R package, and more elaborate
alternatives such as ash, GenKern, and KernSmooth are available at CRAN.

434
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
2
4
6
8
10
12
Year
lambda
1936
1938
1940
1942
1944
1946
1948
1950
FIGURE 12.4: Lambda Posterior Summary, Italy Model
to one posterior standard error. While the trend is clear, it does not seem to be an
extreme change. It is also advisable at times to give measures of uncertainty through
the path of the chain as well as the standard summaries of central tendency. Returning
to the model of Italian marriage rates from Example 12.5.1, we now construct a slightly
diﬀerent kind of running mean plot of α and β across all the iterations from 1 to 10,000
with an envelope of 1.96 standard deviations from the mean on each cycle. This is
shown in Figure 12.5 and strongly supports claims of convergence.
12.6
The General Role of Priors and Hyperpriors
Consider a data matrix X produced conditionally on an unknown parameter vector
θ1.
In specifying a prior distribution for θ1, it is possible that there is an additional
parameterization to be taken into account: θ1 is actually conditional on some second level
of parameters, θ2.
For example, in the beta-binomial model given by (2.14), the data
are conditional on the parameter θ1 = p, which is itself conditional on θ2 = [A, B] from
the beta PDF. The two coeﬃcients can be ﬁxed at researcher-selected constants ending
the hierarchy at the second level, or an additional level of uncertainty can be speciﬁed
by assigning distributions to A and B.
These parameters are restricted to the support
0 < A, B < ∞by the form of the beta distribution. This makes the gamma or inverse
gamma PDFs natural candidates for A ∼f(A|α, β), and B ∼g(B|γ, δ). Both of these
forms are indexed by two positive parameters so we now have θ3 = [α, β, γ, δ]. Continuing
this process, it is now necessary to assign ﬁxed values for θ3 or assign another level of
distributional assumptions.
At each level the length of the θj vectors can diﬀer, as seen in the previous example:
θ1 = p, θ2 = [A, B], and θ3 = [α, β, γ, δ]. The size of the hyperprior vector for the next

Bayesian Hierarchical Models
435
0
2000
4000
6000
8000
10000
2
3
4
5
6
alpha
0
2000
4000
6000
8000
10000
0.3
0.4
0.5
0.6
0.7
0.8
beta
FIGURE 12.5: Running Means, Italy Model
level is determined by the number of parameters assumed to be random variables and their
assigned parametric form, at the current level. Because not every parameter at a given
level is required to have a speciﬁed distribution, it is common to have mixed speciﬁcation
where some parameters are assigned ﬁxed hyperparameters and some are assumed to have
additional hyperpriors. The overall speciﬁcation can end up being somewhat diﬃcult to
describe, so some authors use tools such as the directed acyclic graph (DAG): a structured
ﬂowchart with parametric relationships connected by line segments in order to map out
the paths of priors and hyperpriors. For use in Bayesian hierarchical models, see Gilks,
Thomas, and Spiegelhalter (1994), Satten and Longini (1996), Spiegelhalter et al. (2000),
and for general background see Lauritzen and Spiegelhalter (1988), and Whittaker (1990).
One way of thinking about hierarchical models is to consider the hierarchical structure
as a complicated ﬂat prior speciﬁcation like (12.3). This works because the hierarchy of the
prior can be re-expressed through a series of chained conditional probability statements:
p(θ1|θ2)p(θ2|θ3) · · · p(θJ−1|θJ) for J levels of the hierarchy, so that the simpliﬁed prior is
obtained by:
p(θ1) =

Θ2···ΘJ
p(θ1|θ2)p(θ2|θ3) · · · p(θJ−1|θJ)dθ2 · · · dθJ.
(12.28)
This shows that the hierarchical model is a correct and proper Bayesian speciﬁcation since
π(θ1|X) ∝L(θ1|X)p(θ1). The form of (12.28) implies greater uncertainty than in those
provided by the standard conjugate prior speciﬁcations studied in Chapter 4 because there
is added uncertainty provided by averaging over distributions given at higher levels. Yet,
except in special cases, the hierarchical form of the prior gives more parametric information

436
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
than a standard uninformative prior speciﬁcation. In this way, the Bayesian hierarchical
model provides a level of subjective information that is between full conjugacy and uninfor-
mative speciﬁcations. Also, since most practitioners specify fairly vague forms at the top
of the hierarchy at the point where ﬁxed hyperpriors are required, hierarchical models are
typically more robust than standard ﬂat forms (Berger 1985).
In general, realistic hierarchical speciﬁcations lead to intractable posterior speciﬁcations
and necessitate estimation with MCMC methods (Seltzer, Wong, and Bryk 1996).
Thus
the development of Bayesian hierarchical models is intimately tied to the theoretical devel-
opment and increased computational ease of MCMC in practice. Interestingly, it is rare to
ﬁnd a discussion of hierarchical models in Bayesian texts published prior to 1990, and those
that do exist are very limited. Conversely, such models are now the mainstay of applied
Bayesian work because of their ﬂexibility as well as the ready adaptation of the Gibbs sam-
pler to such speciﬁcations. The latter point is seen in the form of (12.28), where a set of
conditional distributions is automatically speciﬁed from the hierarchy.
12.7
Bayesian Multilevel Linear Regression Models
We now move on to understanding Bayesian multilevel linear models from both a the-
oretical and a practical standpoint.
Nearly all Bayesian multilevel speciﬁcations are ﬁt
with MCMC software, so after covering some underlying theory we will proceed to BUGS
solutions.
12.7.1
The Bayesian Hierarchical Linear Model of Lindley and Smith
The linear setup of Lindley and Smith (1972) is important enough to warrant a separate
discussion. This model, developed before the era of widespread MCMC computing, speci-
ﬁes hierarchies of linear hyperpriors each of which has an associated matrix of explanatory
variables. Therefore it is a Bayesian generalization of the simpler and more common hier-
archical linear speciﬁcation given above. Since each level has a unique conditional normal
speciﬁcation of diﬀering dimension, subscripting is extensive.
Exchangeability is required here because in the estimation of the coeﬃcients from (12.3)
we need to assume that the joint distribution of each of the groups is invariant to reordering.
Exchangeability is discussed in greater detail in Section 12.4 on page 420 in this chapter. In
addition, we can place a prior distribution on the γ coeﬃcients. Typically these are assigned
convenient forms. The well-known and classical references on the Bayesian approach to
HLMs are: Good (1980a, 1983b), Lindley and Smith (1972) plus the follow-on article by
Smith (1973), the two papers by Tiao and Zellner (1964a, 1964b), and the article by Hartigan
(1969). See also the extensive listing of references in Berger (1985, p.183).

Bayesian Hierarchical Models
437
Begin with a typical multivariate normal-linear model speciﬁcation:
y ∼N(X1β1, Σ1)
(12.29)
for X1 (n × k1), β1 (k1 × 1), and positive deﬁnite Σ1 (n × n). The prior on β1 is also
normal, but according to:
β1 ∼N(X2β2, Σ2)
(12.30)
for X2 (k1 × k2), β2 (k2 × 1), and Σ2 (k1 × k1), positive deﬁnite. The second data matrix
is typically of a diﬀerent dimension than the ﬁrst because it is measured at a diﬀerent level
of aggregation or clustering. The implications of this speciﬁcation are that:
y ∼N(X1X2β2, Σ1 + X1Σ2X′
1)
β1|y ∼N(Σ∗
2β∗
2, Σ∗
2)
where:
β∗
2 = X′
1Σ−1
1 y + Σ−1
2 X2β2
Σ∗
2 =
	
X′
1Σ−1
1 X1 + Σ−1
2

−1 .
(12.31)
If the hierarchy ends here by assigning ﬁxed hyperprior values to β2 and Σ2, then the model
is equivalent to the simple one given above.
Rather than stopping at the second level of the hierarchy, β2 could be treated as a
further random component by specifying:
β2 ∼N(X3β3, Σ3),
(12.32)
where we specify additional data X3 (k2 × k3), and ﬁxed hyperpriors β3 (k3 × 1), and
positive deﬁnite Σ3 (k2 × k2). Winding these terms back into the prior for β1 gives the
relatively simple form: β1 ∼N(X2X3β3, Σ2 + X2Σ3X′
2) (notice that all the second-level
terms have been replaced). Of course our real interest lies in the posterior distribution of
β1, which Lindley and Smith proved to be:
β1|y, Xj, Σj, β3 ∼N(Σ∗
3β∗
3, Σ∗
3),
j = 1, 2, 3
where:
β∗
3 = X′
1Σ−1
1 y + (Σ2 + X2Σ3X′
2)−1X2X3β3
Σ∗
3 =
	
X′
1Σ−1
1 X1 + (Σ2 + X2Σ3X′
2)−1
−1 .
(12.33)
The nifty thing about this result is that exactly like the simple forms of Bayesian-normal
inference given in Chapter 3, the posterior mean is a weighted compromise between the
prior mean speciﬁcation and the data mean (here the OLS estimate). To see this, note that
the Σ1 weighted OLS estimator is (X′
1Σ−1
1 X1)−1X′
1Σ−1
1 y, and the prior mean is equal to

438
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
X2X3β3. Rearranging the mean expression in (12.33) according to
Σ∗
3β∗
3 =
	
X′
1Σ−1
1 X1 + (Σ2 + X2Σ3X′
2)−1
−1 X′
1Σ−1
1 y
+
	
X′
1Σ−1
1 X1 + (Σ2 + X2Σ3X′
2)−1
−1
× (Σ2 + X2Σ3X2)−1X2X3β3
(12.34)
reveals that ﬁrst additive term is the OLS estimate weighted by its corresponding dispersion
weighted and (Σ2 + X′
2Σ3X′
2)−1, plus the prior estimate weighted by its corresponding
dispersion weighted and X′
1Σ−1
1 X1 (see Smith [1973] for discussion).
It should be clear that it is possible to continue to build the normal hierarchy in the
same manner to the point where our tolerance for subscripts equals the utility of additional
levels. It is rarely useful to go beyond the forms given here, however. Recall the result
from Goel (1983) that we get progressively less information from the data moving up the
hierarchy.
The focus in these forms has been on developing a normal hierarchy based on the mean
term. It has been assumed that the variance speciﬁcations at each level of the prior could
be reasonably speciﬁed. A number of alternatives have been suggested such as treating
these matrices like nuisance parameters and integrating them out, perhaps specifying that
each level is an arbitrary multiple of the likelihood dispersion (Lindley and Smith 1972), or
putting a prior speciﬁcation on these terms as well such as a Wishart (Robert 2001).
The Wishart prior (introduced in Chapter 3 on page 77, and described in Appendix B)
is a ﬂexible parametric form for modeling multivariate variance structures. Denote the PDF
for a variance-covariance matrix Σ as W(Σ|α, β), where α is a scalar and β is a symmetric
nonsingular matrix. To show the ﬂexibility of this parametric form consider a simple 2 × 2
case:
Σ =

σ11
σ12
σ21
σ22

.
(12.35)
Figure 12.6 shows σ12/(σ11σ22) for three diﬀerent values of α and where β is simply an
identity matrix. Additional features can be imposed by specifying a more complex version
of β. From the three examples we see that a variety of prior forms can be speciﬁed for
various components in a hierarchical linear model.
One ﬁeld that makes extensive use of such multilevel linear models is education research
(Burstein, Kim, and Delandshere 1989, Bryk and Raudenbush 1989, Kreft and De Leeuw
1998). This is a natural ﬁt because students are nested within classrooms, classrooms within
schools, schools within districts, districts within states, and states within nations. Therefore
data are measured at necessarily diﬀerent levels of bureaucratic hierarchy and it would be
na¨ıve to treat variables as if they were produced at the same level. Standard references
include Bryk and Raudenbush (2001), Goldstein (1987, 1985, 2003), Kreft (1993), Lee and
Bryk (1989), and Raudenbush and Bryk (1986). A particular feature of this literature is the
almost exclusive speciﬁcation of distributions based on normal or t-distribution assumptions
(Cocchi and Mouchart 1996; Goldstein 1986; Mason, Wong, and Entwistle 1983; Goel and

Bayesian Hierarchical Models
439
0.0
0.2
0.4
0.6
0.8
1.0
α = 2, β = I
−1.0
−0.5
0.0
0.5
1.0
0.0
0.2
0.4
0.6
0.8
1.0
α = 4, β = I
−1.0
−0.5
0.0
0.5
1.0
0.0
0.2
0.4
0.6
0.8
1.0
α = 10, β = I
−1.0
−0.5
0.0
0.5
1.0
FIGURE 12.6: Correlation Distributions from W(Σ|α, β)
DeGroot 1981; Smith 1973; Wakeﬁeld et al. 1994; Western 1998; Wong and Mason 1985,
1991), although more complex variations could include the incorporation of mixed eﬀects
(Datta and Ghosh 1991; Kackar and Harville 1984).
■Example 12.4:
A Normal-Normal Hierarchical Model for Economic Data.
This example shows a minimally hierarchical model for economic indicators can be
speciﬁed with BUGS (actually ﬁtted with the JAGS package). The data come from
the U.S. Department of Commerce, Survey of Current Business, and describe activity
from the ﬁrst quarter of 1979 to fourth quarter 1989, and therefore cover j = 44,
also available for easy download at: http://lib.stat.cmu.edu/DASL as “Predicting
Retail Sales.”
The variables provided are: DSB, national income wage and salary
disbursements (in billions of dollars), EMP, employees on non-agricultural payrolls (in
thousands), BDG building material dealer sales (in millions of dollars), CAR, retail
automotive dealer sales (in millions of dollars), FRN, home furnishings dealer sales (in
millions of dollars), and GMR, general merchandise dealer sales (in millions of dollars).
All values are scaled for the model by dividing by one thousand.
Label as xj j = 1 . . . 44 the time variable, and list the cases (indices) by i = 1, . . . , 5
such that yij is the value of the ith economic value at time j. A simple normal-normal
speciﬁcation that accounts for the hierarchy of time within case is given by:

440
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
yij ∼N(β0i + β1ixj, τ)
β0 ∼N(μβ0, τβ0)
β1 ∼N(μβ1, τβ1)
τ ∼G(0.01, 0.01),
(12.36)
and the hyperparameters for β0 and β1 are speciﬁed to give somewhat uninformed
distributions. It should be noted that specifying the G(ϵ, ϵ) distribution with small
parameter values is still an informed choice, and in situations with small estimated
variance inference will be sensitive to arbitrary values of ϵ (Hodges and Sargent 2001,
Natarajan and Kass 2000). Gelman (2006) recommends various folded forms such as
normals or Student’s-t (including the Cauchy) as more stable (see also Gelman and
Hill [2007, p.433]).
This speciﬁcation is a (now) standard starting point in learning
BUGS and the only subtlety is sweeping a mean through the x terms, and the BUGS
creators note that this reduces the dependence of beta0 and beta1. In fact, this
feature notably improves the ﬁt of the model with these data. The ﬁnal BUGS code is
as follows:
model {
for (i in 1:VALUE)
{
for (j in 1:TIME)
{
mu[i,j] <- beta0[i] + beta1[i]*(x1[j]-x1.mean);
y[i,j]
~ dnorm(mu[i,j],tau);
}
beta0[i]
~ dnorm(mu.beta0,tau.beta0);
beta1[i]
~ dnorm(mu.beta1,tau.beta1);
}
tau
~ dgamma(1.0E-2,1.0E-2);
mu.beta0
~ dnorm(0,1.0E-2);
tau.beta0
~ dgamma(1.0E-2,1.0E-2);
mu.beta1
~ dnorm(0,1.0E-2);
tau.beta1
~ dgamma(1.0E-2,1.0E-2);
x1.mean <- mean(x1[]);
}
Note the use of the double index on mu[i,j]. This model is estimated using JAGS
with the supplied code and run for 100,000 iterations including a burn-in of 20,000
iterations, which are discarded. The posterior is summarized in Table 12.4 where we
see that all but one of the posterior distributions have 95% HPD regions distinct from
zero. So under conventional regression quality terms, this model appears to ﬁt the
data well.

Bayesian Hierarchical Models
441
TABLE 12.4:
Posterior Summary
Statistics, Economic Indicators Model
Parameter
Mean
Std. Error
95% HPD
β01
1.852
0.752
[ 0.377: 3.333]
β02
96.323
0.750
[94.854:97.805]
β03
17.085
0.751
[15.616:18.556]
β04
66.062
0.748
[64.597:67.533]
β05
16.173
0.747
[14.709:17.632]
β06
37.882
0.754
[36.388:39.352]
β.11
0.039
0.059
[-0.077: 0.156]
β12
0.496
0.059
[ 0.380: 0.612]
β13
0.315
0.058
[ 0.201: 0.431]
β14
1.522
0.059
[ 1.406: 1.639]
β15
0.368
0.059
[ 0.253: 0.483]
β16
0.548
0.059
[ 0.433: 0.662]
12.8
Bayesian Multilevel Generalized Linear Regression Models
The extension from Bayesian multilevel linear models to Bayesian multilevel generalized
linear models is straightforward both theoretically and in practice (the latter mainly due
to BUGS software solutions).
Begin by replacing the linear link function from (12.29) on page 437 with a more general
form:
y ∼g−1(X1β1, Σ1)
(12.37)
for X1 (n × k1), β1 (k1 × 1), and positive deﬁnite Σ1 (n × n), as described in Ap-
pendix A. In general (but not necessarily) the prior on β1 is again normal according to:
β1 ∼N(X2β2, Σ2) for X2 (k1 × k2), β2 (k2 × 1), and Σ2 (k1 × k1), positive deﬁnite.
As before, the second data matrix is of a diﬀerent dimension than the ﬁrst because it is
measured at a diﬀerent level of aggregation. Because of g−1() this setup leads to a huge
number of models and posterior forms. The resulting Bayesian hierarchical generalized lin-
ear model (BHGLM) is then a very ﬂexible form for describing the nonlinear systematic
eﬀect on the outcome variable, but often with normal assumptions at all higher levels on
the right-hand side. Such normal assumptions are not necessary, but were very convenient
prior to the advent of MCMC estimation. Now there are almost no distributional restric-
tions give the ﬂexibility of BUGS software and the ability to write MCMC algorithms for
customized models in standard programming languages. Racine et al. (1986, pp.113-116)

442
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
give a detailed example of model choice here, and Albert (1988) provides an overview of
posterior estimation problems that can occur.
The following two examples give nonlinear (logistic) regression speciﬁcations that are
typical forms.
The ﬁrst example uses arrest data in diﬀerent cities to highlight group
diﬀerences that matter to social scientists studying heterogeneous crime and drug use by
geographic units. The second example speciﬁes two non-nested hierarchies on the regression
parameters.
■Example 12.5:
Hierarchical Model of Drug Use by Arrestees This example
uses data from Diﬀerences in the Validity of Self-Reported Drug Use Across Five
Factors in Indianapolis, Fort Lauderdale, Phoenix, and Dallas, 1994 (ICPSR Study
Number 2706, Rosay and Herz (2000), from the Arrestee Drug Abuse Monitoring
(ADAM) Program/Drug Use Forecasting, ICPSR Study Number 2826. The original
purpose of the study was to understand the accuracy of self-reported drug use, which
is a diﬃcult problem for obvious reasons. These data provide variables measuring:
gender, race, age, type of drug, and oﬀense seriousness (misdemeanor versus felony)
for 4,752 arrestees in four cities. For our purposes here, the sample is restricted to
felony arrests reducing the sample size to 2,965. Conﬁrmation of drug-use status was
conﬁrmed with past-arrest urine testing. The outcome variable used here is MJTEST,
a dichotomous variable indicating a positive urine test for marijuana (2,085 negative,
880 positive). The explanatory variables selected are: AGEGRP (1 for 1,700 cases 18
through 30 years old, 2 for 1,265 cases 31 years old or over), SEX (1 for 2,213 male
cases, 2 for 752 female cases), RACE (1 for 1,554 black cases, 2 for 1,411 white cases),
and COCSELF indicating self-reported cocaine usage prior to arrest (0 for 2,220 negative
responses, 1 for 745 positive responses). Since there are four separate cities in this
study, we add a second level to account for possible diﬀerences with the variable SITE
coded according to: Indianapolis = 1 (759 cases), Ft. Lauderdale = 2 (974 cases),
Phoenix = 3 (646 cases), and Dallas = 4 (586 cases). The data are available in the
BaM package.
Pre-processing of the R data ﬁle to BUGS and then to JAGS is accomplished with the
following R code:
lapply(c("coda","BaM","superdiag"),library, character.only=TRUE)
source("writeDatafileR.R")
data(adam); attach(adam)
adam.df <- data.frame(SITE,MJTEST,AGEGRP,SEX,RACE,COCSELF)
writeDatafileR(adam.df,"adam.bugs.dat")
bugs2jags("adam.bugs.dat","adam.jags.dat")
system("echo ’CASES <- 2965 \n CATS <- 4 \n COVARS <- 4’
>> adam.jags.dat")
detach(adam.df)

Bayesian Hierarchical Models
443
These steps are necessary only if using JAGS in a separate terminal window, as opposed
to running JAGS or BUGS from R with the tools described in Chapter 11 such as with the
library rjags. The system() command adds the necessary constants to the data ﬁle
in a Unix context. This is better than hard-coding these into the JAGS code since those
reusing the model code with other data (a common occurrence) may forget to change
them resulting in misspeciﬁcation.
Users of WinBUGS do not need the bugs2jags
statement and also need to change the system command appropriately to add these
constants to the adam.bugs.dat ﬁle or add them with an editor (an easy task since
they can be added right after the list() characters before the ﬁrst variable). Note
that path names are not speciﬁed here so the use of setwd() is required beforehand.
Armed with these data the following BUGS logit model is speciﬁed and run with three
parallel chains from three diﬀerent starting points using JAGS:
model {
for (i in 1:CASES) {
logit(p[i]) <- alpha[SITE[i]]
+ beta[1]*AGEGRP[i] + beta[2]*SEX[i]
+ beta[3]*RACE[i] + beta[4]*COCSELF[i]
MJTEST[i] ~ dbern(p[i])
}
for (j in 1:CATS) { alpha[j]
~ dnorm(0,tau.alpha) }
for (k in 1:COVARS) { beta[k] ~ dnorm(0,tau.beta) }
tau.y
<- pow(sigma.y,-2)
sigma.y ~ dt(0,1,1)T(0,)
tau.alpha
<- pow(sigma.alpha,-2)
sigma.alpha ~ dt(0,1,1)T(0,)
tau.beta
<- pow(sigma.beta,-2)
sigma.beta ~ dt(0,1,1)T(0,)
}
Notice in particular the alpha[SITE[i]] statement of the random eﬀect for the four
cities, providing a simple hierarchy with no covariates at that level. The variance com-
ponents are speciﬁed such that the prior standard errors are each a “folded Cauchy”
distribution, which gives the correct support and larger-than-normal tails to be some-
what conservative (Gelman 2006, Polson and Scott 2012). These speciﬁcations are
connected to the necessary precision statement in the normal distribution by the nega-
tive square operation. The following commands (which correspond to menu selections
in WinBUGS) are entered into a JAGS text window to run the model:
load dic
model in adam.jags
data in adam.jags.dat
compile, nchains(3)

444
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
parameters in adam1.inits
parameters in adam2.inits
parameters in adam3.inits
initialize
monitor set alpha
monitor set beta
monitor set sigma.y
monitor set sigma.alpha
monitor set sigma.beta
monitor set deviance
update 500000
coda *
The function mcmc.table provided in the Computational Addendum imports
these samples back into R for analysis, and we have used other convenient ways to
import CODA ﬁles.
The output is a set of tables, depending on how many chains
are speciﬁed. The use of superdiag shows no evidence of non-convergence from the
four popular formal diagnostics introduced in Chapter 14. Being comfortable with
convergence, we summarize the model in Table 12.5.
TABLE 12.5:
Posterior Summary Statistics,
Drug Use by Arrestees Model
Mean
Std. Error
95% HPD Interval
alpha[1]
1.0213
0.2415
0.5481
0.5481
alpha[2]
0.6248
0.2480
0.1386
0.1386
alpha[3]
0.6412
0.2751
0.1020
0.1020
alpha[4]
0.6432
0.2441
0.1648
0.1648
AGEGRP
-0.7604
0.0913
-0.9394
-0.9394
SEX
-0.4480
0.1059
-0.6556
-0.6556
RACE
0.0292
0.0907
-0.1485
-0.1485
COCSELF
-0.2129
0.0994
-0.4076
-0.4076
sigma.y
6.1765
105.3818
-200.3679
-200.3679
sigma.alpha
0.9271
0.4790
-0.0117
-0.0117
sigma.beta
0.6135
0.3024
0.0208
0.0208
deviance: 3454.8251
First, since σα is not much smaller than σy, we know that there are important group
diﬀerences accounted for in the model. In the absence of the random intercept speciﬁ-
cation, σy would increase non-trivially by absorbing variance accounted for in σalpha.
These intercepts are also substantively interesting. It appears that Indianapolis has a
noticeably larger value than the three other cities, which are relatively close in their
eﬀect. All four of the β coeﬃcients in the model are reliable at standard levels (the
95% HPD regions do not cover zero). Younger arrestees are more likely to test positive

Bayesian Hierarchical Models
445
than those over 30. Males are more likely to test positive than females. Black arrestees
are more likely than white arrestees. And those that self-report recent cocaine usage
are less likely to test positive for marijuana than those that did not. The last result
may be due to a substitution eﬀect.
■Example 12.6:
A Random Eﬀects Logistic Model of Contraception Use.
The hierarchical model of Wong and Mason (1985) recognizes two levels of data in
cross-national surveys: the individual response level and the national aggregation level
for 15 countries. The research question addressed here is what individual and national
factors in lesser-developed countries aﬀect the decision by married women, aged 40
to 44, to use modern contraception techniques (measured as 1=ever, 0=never). The
explanatory variables from surveys are: the years of education for the woman (WED),
a dichotomous variable for urban versus rural childhood (URC), exposure to family-
planning eﬀorts (FPE), and an interaction term speciﬁed by Wong and Mason (WED ×
FPE).
The hierarchy is set up to express within-country variation at the ﬁrst level and across-
level variation at the second level, where the outcome variable is Yij indicating the
binary response for the ith observation in the jth country with nj respondents. The
hierarchy starts with a standard logit speciﬁcation with k = 4 regressors for unit ij
and then adds normal assumptions at higher levels according to:
Yij|pij ∼BR(pij)
pij = [1 + exp(−Xij
(1×k)
βj
(k×1)
)]−1
βjk = Gkl
(1×l)
ηk
(l×1)
+ αjk
αjk ∼N(0, γkk)
ηk ∼N(mη, Ση),
(12.38)
where βj is the jth country coeﬃcient vector, and each of these k individual coeﬃcients
βjk is explained by a higher-level parameterization in which Gkl is a vector of second-
level explanatory variables and ηk is the corresponding second-level coeﬃcient, which
is given a mean hyperparameter vector mη and variance hyperparameter matrix Ση.
The second-level error term is given a normal zero mean prior, and the prior on the
ηk is a multivariate normal with the covariance matrix parameterized so that n is
contained in the denominator of the terms whereby this prior becomes deterministic
in the limit.
The data values and ηk posterior summaries, the primary coeﬃcient of interest, are
provided in Table 12.6. Wong and Mason demonstrate that hierarchy matters, by
comparing these results to a straight logit regression model (note the shrinkage).
The central point in such a comparison is that a nonhierarchical model treats all the
second-level units as equivalent and thus misses any cross-national eﬀects of interest.

446
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
TABLE 12.6:
Contraception Data, Developing
Countries (by Size)
Country j
nj
%(Yij = 1)
URC
WED
FPE
WED × FPE
Indonesia
787
25
9
1.2
14
16.8
Sri Lanka
752
22
20
3.8
12
45.6
Malaysia
726
38
29
1.6
18
28.8
Korea
700
61
15
4.5
24
108.0
Peru
581
14
17
2.7
0
0.0
Fiji
517
60
15
3.7
22
81.4
Thailand
465
36
8
2.1
20
42.0
Kenya
401
9
4
0.9
6
5.4
Jordan
348
44
53
1.4
0
0.0
Costa Rica
332
59
18
4.7
21
98.7
Guyana
313
42
20
6.1
0
0.0
Panama
312
59
50
5.3
19
100.7
Columbia
294
37
47
2.7
16
43.2
Lesotho
236
6
4
3.9
0
0.0
Jamaica
210
44
8
6.9
23
158.7
ηk Posterior Mode
0.4909
0.2147
0.0850
-0.0053
ηk Posterior SD
0.1008
0.0397
0.0269
0.0024
It is almost trivial to add national-level economic and social indicator terms restricted
to the second level to create even more ﬂexible forms (Wong and Mason 1991), or to
simplify this model greatly by reducing either the number of levels and complexity of
the normal forms (Gilks 1996). Albert and Chib (1995) also give a direct Bayesian
residuals analysis for these models.
12.9
Exercises
12.1
Show that inserting (12.2) into (12.1) produces (12.3).
12.2
Explain when you would want to run a multilevel model and when you would not
want to run a multilevel model. What part of the model output from the multilevel
model tells you that deﬁning groups is justiﬁed. What output from a non-multilevel
model might suggest that multilevel model should be run?

Bayesian Hierarchical Models
447
12.3
For the following simple hierarchical form,
Xi|θ ∼N(θ, 1)
θ|σ2 ∼N(0, σ2)
σ2 ∼1/k,
0 < σ ≤k,
express the full joint distribution p(θ, σ, X).
12.4
Explain carefully the distinction between: complete pooling, no pooling, and multi-
level approaches.
12.5
Using the data on ﬁrearm-related deaths per 100,000 in the United States from 1980
to 1998 (Source: National Center for Health Statistics. Health, United States, 1999,
With Health and Aging Chartbook, Hyattsville, Maryland: 1999, see the online
government resources at http://www.cdc.gov/nchs/express.htmand Chapter 6),
test the hypothesis that there is a period of higher rates by specifying a normal
mixture model and comparing the posterior distributions of λ1 and λ2 in the model:
xi ∼N(λki|τ)
ki ∼Categorical2(K)
τ ∼G(ϵg1, ϵg2)
λi ∼N(0, ϵli)
K ∼D(1, 1),
where K is the proportion of the observations distributed N(λk2|τ) and 1 −K
is the proportion of the observations distributed N(λk1|τ). Specify a BUGS model
choosing values of the ϵ.. parameters that specify somewhat diﬀuse hyperpriors.
The data are repeated here:
14.9
14.8
14.3
13.3
13.3
13.3
13.9
13.6
13.9
14.1
14.9
15.2
14.8
15.4
14.8
13.7
12.8
12.1
12.6
Using equation 12.5, analytically derive the properties of αj as nj →0 and nj →
∞. Produce a graph of these eﬀects from simulated data.
12.7
Describe a circumstance where data are exchangeable but not iid. How does this
aﬀect your choice of model?
12.8
Show that de Finetti’s property holds true for a mixture of normals. Hint: see
Freedman and Diaconis (1982).
12.9
(Carlin and Louis 2009, pp.125-128). Given the hierarchical model:
Yi|λi ∼P(λiti)
λi ∼G(α, β)
β ∼IG(A, B),
α known

448
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
where the hyperpriors A and B are ﬁxed, express the full conditional distributions
for all of the unknown parameters.
12.10
Using the employment status data description from Example 12.3, add three addi-
tional hierarchical models to the six already listed (interactions, lags, etc.).
12.11
Hobert and Casella (1998) specify the following one-way random eﬀects model:
yij ∼N(β + ui, σ2
ϵ )
ui ∼N(0, σ2)
p(β) ∝1
p(σ2
ϵ ) ∼1/σ2
ϵ), 0 < σ2
ϵ < ∞
p(σ2) ∼1/σ2), 0 < σ2 < ∞,
where i = 1, . . . , K and j = 1, . . . , J. Using the R function rnorm() generate con-
trived data for yij with K = 50, J = 5, and estimate the model in BUGS. When
improper priors are speciﬁed in hierarchical models, it is important to make sure
that the resulting posterior is not itself then improper. Non-detection of posterior
impropriety from looking at chain values was a problem in the early MCMC liter-
ature. This model gives improper posteriors although it is not possible to observe
this problem without running the chain for quite some time. Compare the standard
diagnostics for a short run with a very long run. What do you see? Now specify
proper priors and contrast the results.
12.12
Returning to the Palm Beach County data introduced in Example 5.1.1 on page 148
in Chapter 5 (data(pbc.vote)): specify a non-nested hierarchical model with levels
for technology and your own segmentation of size into K groups (histogram for
investigation), where there is a modeled correlation between these two hierarchies
according to:
yi ∼N(β0 + βj1X1 + βk2X2 + γZ, σ2
y),
i = 1, . . . , n
*
βj
βk
,
∼N
**
μβ1
μβ2
,
,
*
σ2
β1
ρσβ1σβ2
ρσβ1σβ2
σ2
β2
,,
.
The outcome variable is badballots and the matrix Z contains other covariates
of interest. Run this model in BUGS and report the results along with convergence
diagnostics from superdiag.
12.13
To introduce the “near-ignorance prior” Sans´o and Pericchi (1992) specify the hi-
erarchical model starting with a multivariate normal speciﬁcation:
Y|θ ∼MVN(Xθ, σ2I)
θi|λi ∼N(μi, λ2
i )
λ2
i ∼EX (1/τ2
i )

Bayesian Hierarchical Models
449
where i = 1, . . . , p and μi, τi, σ2 are known. Show that by substitution and integra-
tion over λ that this model is equivalent to the hierarchical speciﬁcation:
Y|θ ∼MVN(Xθ, σ2I)
θi ∼DE(μi, τi).
12.14
Using the general model setup in Example 12.3 starting on page 418 specify three
more distinct speciﬁcations in addition to the six given.
12.15
Clyde, M¨uller, and Parmigiani (1995) specify the following hierarchical model for
10 Bernoulli outcomes:
yi|βi, λi ∼BR(p(β, λ, x))
p(yi = 1|β, λ, x) = (1 + exp[−βi(x −λi) −log(0.95/0.05)])−1

log βi
log λi

∼MVN(μ, Σ)
μ ∼MVN
⎛
⎝

0
2

,

25
5
5
25
−1⎞
⎠
Σ ∼W
⎛
⎝10, 10 ×

0.44
−0.12
−0.12
0.14
−1⎞
⎠.
Using BUGS, obtain the posterior distribution for log β and log λ using the data:
y = [1, 0, 1, 1, 1, 1, 0, 1, 1, 0] with X generated in R from MVN((1, 2, 3), Σ), Σ = 2I
along with a leading column of 1s.
12.16
Example 9.6 (page 298) discussed models of state-level poverty from Gill and King
(2004). Using the dataset fips in the package BaM construct a multilevel model for
the poverty outcome with a hierarchy for state: Texas versus Florida. Implement
this speciﬁcation in BUGS and summarize the sampled values.
12.17
Scollnik (2001) considers the following actuarial claims data for three groups of
insurance policyholders, with missing values.
Group 1
Group 2
Group 3
Year
Payroll
Claims
Payroll
Claims
Payroll
Claims
1
280
9
260
6
NA
NA
2
320
7
275
4
145
8
3
265
6
240
2
120
3
4
340
13
265
8
105
4
5
NA
NA
285
NA
115
NA

450
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
Replicate Scollnik’s hierarchical Poisson model using BUGS:
Yij ∼P(λij)
λij = Pijθj
θj ∼G(α, β)
Pij ∼G(γ, δ)
α ∼G(5, 5)
γ ∼U(0, 100)
β ∼G(25, 1)
δ ∼U(0, 100),
where i = 1, 2, 3 and j = 1, . . . , 5.
12.18
Using the data in Table 12.7 on racial composition at federal agencies, construct a
two-way ANOVA model as a non-nested hierarchical model in BUGS. The data are
also provided by the package BaM.
TABLE 12.7: Racial Composition of U.S. Federal Agencies (1998)
Agency
Black
Hispanic
Asian
Native
White
Agriculture
10.6
5.6
2.4
2.5
78.9
Commerce
18.3
3.4
5.2
0.6
72.5
DOD
14.2
6.2
5.4
1.0
73.3
Army
15.3
5.9
3.7
1.1
73.9
Navy
13.4
4.3
9.8
0.8
71.8
Air.Force
10.6
9.5
3.1
1.1
75.7
Education
36.3
4.7
3.3
1.0
54.7
Energy
11.5
5.2
3.8
1.3
78.2
EOP
24.2
2.4
4.2
0.3
69.0
HHS
16.7
2.9
5.1
16.9
58.5
HUD
34.0
6.7
3.2
1.1
55.0
Interior
5.5
4.3
1.6
15.9
73.2
Justice
16.2
12.2
2.8
0.8
68.1
Labor
24.3
6.6
2.9
0.7
65.6
State
14.9
4.2
3.7
0.4
76.7
Transportation
11.2
4.7
2.9
1.5
79.7
Treasury
21.7
8.4
3.3
0.8
65.8
VA
22.0
6.0
6.7
0.8
64.6
GS
28.4
5.0
3.4
0.7
62.5
NASA
10.5
4.6
4.9
0.9
79.2
EEOC
48.2
10.6
2.7
0.5
38.0
Data Source: Oﬃce of Personnel Management
12.19
One way to think of panel data models is as hierarchical models where each per-
son/case is nested in their own group.
Regular time-series model features can
then be included. Rerun the BUGS model for the British abortion attitudes data

Bayesian Hierarchical Models
451
in Chapter 6 (the example beginning on page 177) adding a correlation parameter
over panel waves.
12.20
Obtain the socatt subset data from the British Social Attitudes (BSA) Survey
1983-1986 from either the BaM package or directly from the Centre for Multilevel
Modelling, University of Bristol. The variables included are:
▷District, identifying for geographic district
▷Respondent.Code, respondent identiﬁer
▷Year.Code, 1 = 1983, 2 = 1984, 3 = 1985, 4 = 1986
▷Num.Answers, number of positive answers to seven questions
▷Party, 1 = Conservative, 2 = Labour, 3 = Lib/SDP/Alliance, 4 = others
▷Social.Class, 1 = middle, 2 = upper working, 3 = lower working
▷Gender, 1 = male, 2 = female
▷Age, age in years 18-80
▷Religion 1 = Roman Catholic, 2 = Protestant/Church of England, 3 = others,
4 = none.
This is a diﬀerent subset from that used in the abortion attitudes example on
page 177. For additional details see McGrath and Waterton (1986).
Construct an
ordered probit model with BUGS where Num.Answers is the outcome variable and
respondents are nested in both years and social class.
12.10
Computational Addendum
12.10.1
R Function for importing BUGS output
The following R function imports MCMC output without requiring the use of the CODA
menu structure. This is faster and more convenient to those that do not intend to use the
other features of CODA.
mcmc.table <- function(n.chains=1,burnin=1/2,alpha=0.05) {
lapply(c("coda","xtable"),library, character.only=TRUE)
print(paste("current working directory:",getwd()))
for (i in 1:n.chains) {
full.out <- read.coda(paste("CODAchain",i,".txt",sep=""),
"CODAindex.txt",quiet=TRUE)
start <- nrow(full.out)*burnin; stop <- nrow(full.out)
chain.mean <- apply(full.out[start:stop,],2,mean)
chain.se <- apply(full.out[start:stop,],2,sd)
full.tab <- cbind(chain.mean,chain.se,
chain.mean-qnorm(1-alpha/2)*chain.se,

452
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
chain.mean-qnorm(1-alpha/2)*chain.se)
print(xtable(full.tab,digits=4))
}
}
The output is given for each chain in a LATEXformatted table that can then be modiﬁed.
Users will want to modify the default parameters, particularly burnin=1/2, which stipulates
one-half of the chain as a burn-in default.

Chapter 13
Some Markov Chain Monte Carlo Theory
13.1
Motivation
This chapter revisits the theoretical basis for Markov chain Monte Carlo but with greater
detail and more attention to issues of convergence. Most of the technical content is intended
to give a greater appreciation for the underlying mathematical process of Markov chains and
therefore an understanding of the issues involved in convergence and mixing. Both conver-
gence and mixing behavior of Markov chains aﬀect the ﬁnal inferences made with Bayesian
models using MCMC. Readers who wish to move onto more practical considerations can
proceed to Chapter 14 and return to this information as needed.
13.2
Measure and Probability Preliminaries
First we return to the idea of a stochastic process. A stochastic process is a set of observed
values θ[t] (t ≥0) on the probability space: (Ω, F, P) where the superscript [t] denotes the
serial order of occurrence, Ω is the non-empty outcome space with the associated σ-algebra
F and probability measure P (Billingsley 1995, Doob 1990, Ross 1996). We say that F is
the associated ﬁeld or algebra of subsets of Ω if: Ω ∈F, it is closed under complementation
as well as intersections of its subsets, and a σ-ﬁeld or σ-algebra if it is also closed under
countable unions of subsets. These subsets can be individual elements, denoted θ, or sets of
such elements, denoted A, B, C, . . .. A function P is a probability measure on Ω if it maps
subsets of Ω to [0:1] according to the Kolmogorov axioms: (1) p(A) ∈[0:1] ∀A ∈F, (2)
p(Ω) = 1, p(∅) = 0, and (3) for non-overlapping multiple Ai, P (;n
i=1 Ai) = n
i=1 p(Ai)
(even if n = ∞).
These were stated more colloquially on page 7, but see also Billingsley
(1995, Chapter 2) for an in-depth explanation or Gill (2006, Chapter 7) for an accessible
introduction. Now (Ω, F, P) is the probability measure space for θ often shortened to the
state space.
The sequence of Ω-valued θ[t] random elements, indicated by t = 0, 1, . . ., deﬁnes the
Ω-valued stochastic process. Typically this is just ℜ-valued with restrictions (Karlin and
453

454
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
Taylor 1981, 1990, Hoel, Port, and Stone 1987). By convention, the sequence is labeled
with consecutive even-spaced intervals: T : {θ[t=0], θ[t=1], θ[t=2], . . .}. This conﬁguration can
be generalized to non-consecutive labels or non-equal time periods, although it does not
provide any additional utility for current purposes.
At time t the history of the stochastic process is the increasing series of sub-σ-algebras
deﬁned by: F0 ⊆F1 ⊆. . . Ft where θ is measurable on each one. A T -valued stochastic
process, with transition probability p, and initial (starting) value θ0, is a Markov chain,
M(θt, t ≥0), if at the arbitrary (t + 1)st time point in T , the following is true:
p(θ[t+1]|Ft) = p(θ[t+1]|θ[t]), ∀t ≥0
(13.1)
(Zhenting and Qingfeng [1978, Chapter 6]). As stated previously in Chapter 10, the only
component of the history that matters in determining movement probabilities to an arbi-
trary set A, which we deﬁne as a collection of individual elements, for Markov chains at the
current step, is the present realization of the stochastic process:
p(θ[t+1] ∈A|θ[0], θ[1], . . . , θ[t−1], θ[t]) = p(θ[t+1] ∈A|θ[t]).
(13.2)
This is the deﬁning Markovian property that distinguishes a Markov chain from the general
class of stochastic processes.
Retain Ω as the space giving the support of θ, a random variable where components
are denoted with subscripts: θi, θj, θk, . . .. Here F is again the associated σ-algebra and
P is a probability measure on Ω.
Now use f, g, h, . . . to denote real-valued measurable
functions deﬁned on this state space, and deﬁne M as the full collection of signed measures
on measure space (Ω, F) where λ and μ denote elements by notational convention. The
class of positive measures is given by: M+ = {λ ∈M : λ(T ) > 0), and includes P.
A Markov chain transition kernel, K(θ, A), is the mechanism for describing the proba-
bility structure of the Markov chain, M(θt, t ≥0): a probability measure for all θ points in
the state space to the set A ∈F. Thus, it is a mapping of the potential transition events
to their probability of occurrence (Robert and Casella 2004, p.208). Formally, K is a non-
negative, σ-ﬁnite kernel on the state space (Ω, F) that provides the mapping Ω × F →ℜ+
given the following three conditions:
1. for every ﬁnite subset A ∈F, K(•, A) is measurable,
2. for every point θi ∈Ω: K(θi, •) is a measure on the state space,
3. there is a positive measurable function, f(θi, θj), ∀θi, θj ∈Ω where

K(θi, dθj)f(θi, θj) <
∞.
This preliminary discussion sets up the discussion of Markov chain properties and char-
acteristics. Importantly, a Markov chain is just a regular probability mechanism with two
added features: time-seriality, and a memory-less decision-process.

Some Markov Chain Monte Carlo Theory
455
13.3
Speciﬁc Markov Chain Properties
Having deﬁned a stochastic process of interest, we now return to the core properties
of Markov chains given in Chapter 10 but with greater attention to underlying theoretical
details. These properties are important because they lead us to convergence of the Markov
chain, without which no practical statistical inference can be made.
13.3.1
ψ-Irreducibility
Irreducibility is a description of accessibility. A set, A, is irreducible if every point or
collection of points in A can be reached from every other point or collection of points in A.
The associated Markov chain is irreducible if it operates on an irreducible set as its state
space. Deﬁne now ψ as a positive σ-ﬁnite measure on (Ω, F) with arbitrary A ∈F such
that ψ(A) > 0. Given the transition kernel K(θ, A), if every positive ψ-subset, A′ ⊆A,
can be reached from every part of A, then A is called ψ-communicating. When the full
state space for the Markov chain T is ψ-communicating, then the kernel that deﬁnes the
Markov chain is ψ-irreducible. Typically, we assume that ψ-irreducible here is maximally
ψ-irreducible: for any other positive σ-ﬁnite measure on (Ω, F), ψ′, it is necessarily true
that ψ > ψ′ (Meyn and Tweedie 1993, pp.88-89).
13.3.2
Closed and Absorbing Sets
A non-empty set A ∈F is obtainable from θ for the Markov chain deﬁned at time t by
the kernel Kt if Kt(θ, A) > 0, for some t ≥1, and unobtainable if Kt(θ, A) = 0, for all t ≥1.
Here, A is called closed for K if Ac is not obtainable from A: Kt(θ, Ac) = 0, for all θ ∈
A, and all t ≥1. The condition of absorbing (Revuz 1975) is more restrictive than closed:
K(θ, A) = K(θ, Θ) = 1, for all θ ∈A, since it is possible under the closed condition, but
impossible under the absorbing condition, that K(θ, A) = K(θ, Θ) ̸= 1, for some θ ∈A. So
a closed set can have subsets that are unavailable but an absorbing state fully communicates
with all of its subsets.
13.3.3
Homogeneity and Periodicity
A Markov chain is homogeneous at the tth step if the transition probabilities at this
step do not depend on the value of t. The period of a Markov chain is the length of time
to repeat an identical cycle of values, and it is desirable that the Markov chain not have
such a deﬁned cycle. A chain that does not repeat in this fashion is called aperiodic. We
can easily deduce that a periodic Markov chain is non-homogeneous because the cycle of
repetitions deﬁnes transitions based on speciﬁc times.
Deﬁne now T (A) as the ﬁrst hitting time (shortest return time to an arbitrary sub-

456
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
space) for the Markov chain M(θt, t ≥0) with invariant distribution π to the set A ∈F not
including time 0: T (A) = inf(t ≥1:θt ∈A). Athreya, Doss, and Sethuraman (1996, p.72)
stipulate two conditions that lead to convergence towards the invariant distribution, which
is our eventual goal.
The ﬁrst condition says that for every element in A the probability
that the hitting time is less than inﬁnity is greater than zero for all of these elements, with
probability 1 under the invariant distribution. The second condition forces the transition
probability from one of these elements to any value in the space to be greater than or equal
to the probability under the measure up to a constant. Periodicity violates the second
condition because it means that at any arbitrary step of the chain we can deﬁne a set D as
the next value in the sequence speciﬁed by a period of moves.
13.3.4
Null and Positive Recurrence
Putting these previous deﬁnitions together, a homogeneous, ψ-irreducible Markov chain
on a closed set (discrete or continuous and bounded) is recurrent or persistent with regard
to the set, A, if the probability that the chain occupies each subset of A inﬁnitely often over
unbounded time is one.
When a chain moves into a recurrent state, it stays there forever
and eventually visits every sub-state an inﬁnite number of times. Furthermore, a recurrent
Markov chain is positive recurrent if the average time to return to A is bounded, otherwise
it is a null recurrent Markov chain (Doeblin 1940).
In the case of unbounded continuous state spaces, we have to work with a slightly
more complicated version of recurrence. Deﬁne ﬁrst for a set A and all elements x in A,
the expected number of visits by the Markov chain to x in the limit: ηx = ∞
n=1 I(θn∈x),
which is a function that counts visits to x. Now we say a ψ-irreducible Markov chain is
Harris recurrent if there is σ-ﬁnite probability measure ψ for (Ω, F) such that at time
n it has the property: ψ(A) > 0, ∀A ∈F (Harris 1956, Athreya and Ney 1978). The
new deﬁnition for unbounded continuous state spaces is required because a ψ-irreducible
chain with an invariant distribution on an unbounded continuous state space that is not
Harris recurrent has a positive probability of getting stuck indeﬁnitely in an area bounded
away from convergence, given a starting point there. The Harris deﬁnition allows us to
avoid worrying about the existence of a pathological null set in ℜk. The standard MCMC
algorithms implemented on ﬁnite state computers are Harris recurrent (Tierney 1994).
13.3.5
Transience
Now consider the number of visits to the set itself, rather than speciﬁc elements in the set:
for a set A, the expected number of visits by chain θn to A in the limit: ηA = ∞
n=1 I(θn∈A),
where the I() function counts hits on A. Transience and recurrence can both be deﬁned in
terms of expectation for this ηA: A is uniformly transient if there exists a scalar M < ∞
such that E[ηA] ≤M ∀θ ∈A. A single state, θ, in the discrete state space case is transient
if: E[ηθ] < ∞. Conversely, A is recurrent if: E[ηA] = ∞∀θ ∈A. A single state in the

Some Markov Chain Monte Carlo Theory
457
discrete state space case is recurrent if: E[ηθ] = ∞. For proofs see: Meyn and Tweedie
(1993, pp.182-3) and Nummelin (1984, p.28).
The important theorem here is:
Theorem. If M(θt, t ≥0) is a ψ-irreducible Markov chain with transition kernel
K(θ, A), then it must either be transient or recurrent depending on whether it
is deﬁned on a transient or recurrent set A.
The proof is a direct consequence of Kolmogorov’s zero-one law, and details are given in
Billingsley (1995, 120).
This means that there is a two-state world to worry about: the chain is either recurrent
and we know that it will eventually settle into a stable distribution, or it is transient and it
will never achieve stability.
We can also deﬁne the convergence parameter of a kernel rK as the real number 0 ≤R <
∞on a closed set A such that: ∞
0 rnKn < ∞for every 0 ≤r < R, and ∞
0 rnKn = ∞
for every R ≥r. It turns out that for ψ-irreducible Markov chains there always exists a
ﬁnite R that deﬁnes whether or not the kernel for this Markov chain is R-transient if the
ﬁrst condition holds, and R-recurrent if the second condition holds (Meyn and Tweedie
1993).
13.3.6
Markov Chain Stability
Label π(θ) as the stationary (limiting) distribution of the Markov chain for θ on the
state space Ω, with transition probability p(θi, θj) that gives the probability that the chain
will move from arbitrary point θi to arbitrary point θj, as stipulated by the transition
kernel K (i.e., from the ith row of the transition matrix for a discrete state space). Natu-
rally, this stationary distribution, π(θ), is really the posterior distribution of interest from
some Bayesian model where marginalization is not possible or convenient analytically. This
stationary or invariant distribution satisﬁes the following condition from Chapter 10:

θi
πt(θi)p(θi, θj) = πt+1(θj)
Discrete State Space

πt(θi)p(θi, θj)dθi = πt+1(θj)
Continuous State Space,
(13.3)
(see speciﬁcally page 339). Multiplication by the transition kernel and evaluating for the
current point (summation for discrete sample spaces, integration for continuous sample
spaces) produces the same marginal distribution, π(θ) = π(θ)P. Therefore the marginal
distribution remains ﬁxed when the chain reaches the stationary distribution and we can
ignore superscripts giving iteration number for iteration purposes. Once the chain reaches
the stationary distribution (synonymously the equilibrium distribution, limiting distribu-
tion), its movement is dictated by the marginal distribution, π(θ) from that point on.
A
ψ-irreducible, aperiodic Markov chain is guaranteed to have exactly one such stationary

458
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
distribution (H¨aggstr¨om 2002, p.37). This is the critical theoretical basis for estimation
with MCMC: if the stationary distribution of the Markov chain is the posterior distribution
of interest, then we are certain to eventually get samples from this posterior.
13.3.7
Ergodicity
The ergodic theorem is the key link between the mechanical process of the Monte Carlo
simulation and the inferential result of MCMC. If a chain is positive (Harris if necessary)
recurrent, and aperiodic on some state A, it is an ergodic Markov chain (Tweedie 1975).
Importantly, ergodic Markov chains have the property that:
lim
n→∞
P n(θi, θj) −π(θj)
 = 0,
(13.4)
for all possible θi, and θj in the subspace (Norris 1997, p.53). So the transition probabilities
of the chain have converged to those of the limiting distribution and therefore all future
draws are treated as if from this marginal distribution of interest. This means that once
a speciﬁed chain is determined to have reached this ergodic state, inference comes from
running the chain for some length of time and summarizing the empirical draws. We have
thus replaced analytical work (i.e., integrating over some diﬃcult form) with empirical
analysis.
Ergodic Markov chains provide the following result:
lim
t→∞
1
t

f(θt) =

Θ
f(θ)π(θ)dθ,
(13.5)
proven originally by Doeblin (1940).
This result means that empirical averages for the
function f() converge to a probabilistic average of the function over the limiting distribution.
In fact, it is this principle that underlies and justiﬁes all MCMC for Bayesian stochastic
simulation; it is exactly the link between “Markov chain” and “Monte Carlo.”
Ergodicity gives a means of asserting eventual convergence (although not the only one),
but it does not provide a ﬁrm bound on the time required to reach convergence to the lim-
iting distribution. There are actually multiple “ﬂavors” of ergodicity that provide diﬀering
rates of convergence for the Markov chain, and these are described in Section 13.5.
13.4
Deﬁning and Reaching Convergence
Return to the abstract measure space (Ω, F) with events A, B, C, . . . in Ω, real-valued
measurable functions f, g, h, . . . on F, and signed measure M+ with elements λ and μ.
First, deﬁne an appropriate norm operator. The elementary form for a bounded signed
measure, λ, is:
||λ|| ≡sup
A∈Ω
λ(A) −inf
A∈Ωλ(A)
(13.6)

Some Markov Chain Monte Carlo Theory
459
which is just the total variation of λ.
Second, assume that K is R-recurrent given by
probability measure P, and the stationary distribution is normed such that π(h) = 1 for h
on F. In addition, assume also that
M(θt, t ≥0)
is R-recurrent (discrete or bounded continuous space) or Harris R-recurrent (continuous
unbounded space) Markov chain with transition kernel K. If K has period p ≥2 then by
deﬁnition the associated Markov chain cycles between the states: {A0, A1, A2, . . . , Ap−1}.
The ψ-null set, Ψ = {A0 ∪A1 ∪· · ·∪Ap−1}c, deﬁnes the collection of such states not visited
in the p-length iterations, and we want to drive this to a set of size zero.
Let the (positive) signed measures λ and μ be any two initial distributions of the Markov
chain at time zero, and therefore before convergence to any other distribution. Nummelin
(1984, Chapter 6) shows that if M(θt, t ≥0) is aperiodic, then:
lim
n→∞||λP n −μP n|| = 0.
(13.7)
This is essentially Orey’s (1961) total variation norm theorem applied to an aperiodic,
recurrent Markov chain (Orey’s result was more general but not any more useful for our
endeavors; see also Athreya and Ney [1978, p.498] for a proof). However, we know that
any ψ-irreducible and aperiodic Markov chain has one and only one stationary distribution
and ψ-irreducibility is implied here by recurrence. Therefore we can substitute into (13.7)
the stationary distribution π to get:
lim
n→∞||λP n −π|| = 0,
(13.8)
which gives ergodicity. This shows in greater detail than before the conditions by which
convergence in distribution to the stationary distribution is justiﬁed.
Convergence to stationarity is distinct from convergence of the empirical averages, which
are usually the primary substantive interest. Consider the limiting behavior of a statistic
of interest, h(θ), from an aperiodic Harris recurrent Markov chain. We typically obtain
empirical summaries of this statistic using the partial sums such as:
¯h = 1
n
n

i=1
h(θi).
(13.9)
The expected value of the target h is Efh(θ), so by the established properties of Harris
recurrent Markov chains (Br´emaud 1999, p.104), it is known that ¯h →Efh(θ) as n →∞.
Equivalently, it is true that:
1
n
n

i=1
h(θi) −Efh(θ) −→
n→∞0.
(13.10)
We can also consider the true distribution of h(θ) at time n (even if it is not directly
observed) from a chain with starting point θ0. The interest here is in Eθ0h(θ), where the

460
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
expectation is with respect to the distribution of θn conditional on θ0. In the next step add
and subtract this term on the left-hand side of 13.10 to obtain:
' 1
n
n

i=1
h(θi) −Eθ0h(θ)
(
−
'
Efh(θ) −Eθ0h(θ)
(
−→
n→∞0.
(13.11)
The second bracketed term is obviously the diﬀerence between the expected value of the
target h(θ) in the true distribution at time n and the expected value of h(θ) in the stationary
distribution. For any ergodic Markov chain, this quantity will eventually to converge to
zero. The ﬁrst bracketed term is the diﬀerence between the current empirical average and
its expectation at time n. Except at the uninteresting starting point, these quantities are
never non-asymptotically equivalent, and so even in stationarity, the empirical average has
not converged. This is not bad news, however, since we know for certain by the central
limit theorem that:
1
n
n
i=1 h(θi) −Eθ0h(θ)
σ/√n
d→N(0, 1).
(13.12)
(Meyn and Tweedie 1993, p.418, Jones 2004). Therefore as √nδn →0, convergence to
stationarity proceeds at a much faster rate, but does not bring along convergence of empirical
averages. Note also that (13.12) explains the shape of post-convergence marginal density
plots.
13.5
Rates of Convergence
So far little has been said about the actual rate of convergence, merely that chains
are or are not in a state of convergence.
Ergodicity, resulting from positive (Harris if
necessary) recurrence and aperiodicity, is merely an asymptotic property and thus a rather
indeterminate statement for Markov chains run in ﬁnite time, that is, every Markov chain
ever run in actual practice (Rosenthal 1995c).
We now provide the ﬁrst improvement on basic ergodicity to produce a more rapid
transition to Markov chain stability. If the Markov chain has invariant distribution, π(θ),
and π(A) > 0 ∀A ∈F it is ergodic of degree 2 if:

A
π(dθ)Eθ[T (A)2] < ∞
(13.13)
(Nummelin 1984, p.118). In other words, the condition is that the second moment of the
ﬁrst hitting times must be ﬁnite. What does this buy us? It turns out that if the functions
f(θ) (arbitrary) and π(θ) are regular (ﬁnite total density and ﬁnite expectations over all sub-
regions of the support, see Billingsley [1995, p.174] for details), then the rate of convergence
is proportional to n−2:
lim
n→∞n2∥f(θt) −π(θ)∥−→0.
(13.14)

Some Markov Chain Monte Carlo Theory
461
This is an interesting result but unfortunately it is diﬃcult to assert degree 2 ergodicity
with many practical problems.
A more useful and stronger type of ergodic convergence is geometric ergodicity. Make
the same assumptions as those above about the Markov chain but substitute the hitting
time assumption with the following requirement:
∥f(θt) −π(θ)∥≤m(θ)ρt, ∀θ, 0 < ρ < 1,
(13.15)
where m(θ) is any ﬁnite, non-negative function. Under these conditions the tth step tran-
sition probability converges to the invariant distribution at a geometric rate, which can be
very quick depending on the value of ρ. If instead of specifying the function m(θ) we ﬁnd
a constant m such that:
∥f(θt) −π(θ)∥≤mρt, ∀θ, 0 < ρ < 1,
(13.16)
then the chain is uniformly ergodic, which means it converges even faster. The value of
these properties is two-fold.
First, knowing that a chain is geometrically or uniformly
ergodic is comforting in that it is an assurance of convergence in some reasonably practical
amount of time (depending of course on the complexity of the model and the structure of the
data). Second, it allows the derivation of bounds on the number of iterations to convergence
for some Markov chains. These claims are usually made by analyzing minorization and
drift conditions. The minorization condition means that for any sub-space A, the σ-ﬁnite
measure ϕ in Ω with ϕ(A) > 0 contains a small set C with the property that for any θ ∈C:
Kt(θ′, θ) ≥δv(θ′)
(13.17)
for θ′ in Ω, δ > 0, and time t > 0, where v is a probability measure concentrated on C.
See Meyn and Tweedie (1993, Chapter 5) for details on small sets for Markov chains. A
Metropolis-Hastings chain always meets these conditions if qt(θ|·) and π(θ) are both positive
and continuous. (Roberts and Rosenthal 1998).
We can now catalog some popular variants of MCMC algorithms by their ergodic prop-
erties. These are given with references for the associated proofs. It is assumed in this list
that every Markov chain is at least ergodic as well as meeting the small set (minorization)
condition above. As a reminder, we will indicate θ for a ﬁnding in the single-dimensional
case, and θ for a ﬁnding in the multi-dimensional case, and all statements refer to contin-
uous state spaces unless otherwise stated. This is not a complete listing of the numerous
variants, by any means, but represents most of the more important and relevant results.
See also the discussion in Roberts and Smith (1994).
Gibbs Sampling
▷A chain operating on a ﬁnite state space, with a positive invariance distribution is
geometrically ergodic (Geman and Geman 1984, Besag 1974). The positivity con-
dition means that the support of the invariant distribution must be the Cartesian
product of the marginal supports as a way of guaranteeing irreducibility.

462
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
▷A chain cast as two-parameter data augmentation is geometrically ergodic (Tanner
and Wong 1987, Rosenthal 1993).
▷A chain meeting the Geman and Geman conditions with purely systematic scan (the
order of the parameter-by-parameter updating is unchanged over Gibbs iterations):
1, 2, 3, ..., d is geometrically ergodic (Schervish and Carlin 1992).
▷A chain meeting the Geman and Geman conditions with random scan (at the t’th step
only one of the θ[t]
i
is randomly selected, usually uniformly p = 1/d, and updated) is
geometrically ergodic (Liu, Wong, and Kong 1994).
▷Suppose there exists a non-negative function K∗() on ℜd with K∗(θ′) > 0 on Ω, where
for some t > 0 we have K[t](θ, θ′) ≥K∗(θ′), ∀θ (θ in the continuous domain of the
Markov chain), and K(θ, θ′) is positive over all (θ, θ′) ∈ℜd × ℜd. Then the chain is
uniformly ergodic (Roberts and Polson 1994).
▷A chain where π(θ) is produced from improper priors may lead to improper posteriors
(Hobert and Casella 1996, 1998, Cowles 2002), so there is no longer a justiﬁcation for
geometric ergodicity (Chan 1993), or even degree 2 ergodicity since it may be possible
to come up with sub-spaces where 
A π(dθ)Eθ[T (A)2] = ∞.
Metropolis-Hastings
▷Suppose π(θ) = h(θ) exp(p(θ)) where p(θ) is a (exponential family form) polynomial
of order m ≥2, and pm(θ) →−∞as |θ| →∞, where (pm(θ) is the sub-polynomial
consisting of only the terms in p(θ) of order m). There is actually a subtlety lurking
here in the multivariate case. Take the term with the highest total order across terms
as m(Θ), and determine if there is a case whereby setting all terms to zero but one
does not result in −∞. So the bivariate normal (∝exp[−1
2(x2 −2xy+y2)]) passes but
a function like exp[−1
2(x2 +2x2y2 +y2)] fails. Note that this characterizes the normal
distribution and those with lighter tails. Then for a symmetric candidate distribution
bounded away from zero, the chain is geometrically ergodic (Roberts and Tweedie
1996).
▷A chain with π(θ) log-concave in the tails (meaning there is an α > 0 such that
for y ≥x, log π(x) −log π(y) ≥α(y −x) and for y ≤x, log π(x) −log π(y) ≥
α(x−y)) and symmetric candidate distribution where q(θ′|θ) = q(θ −θ′) = q(θ′ −θ)
is geometrically ergodic (Mengersen and Tweedie 1996).
▷A chain with σ-ﬁnite measure ϕ < ∞on (Ω, F), where qt(θ|·) and π(θ) are bounded
away from zero is uniformly ergodic (Tierney 1994). Practically, this means trun-
cating the posterior support such that π(θ) > 0 (strictly!), which may be challenging
in high dimensions.
▷An independence chain (deﬁned later on page 468) with the bounded weight function,
w(θ) = π(θ)/f(θ) and π(θ) bounded away from zero is uniformly ergodic with
ρ ≤1 −sup(w(θ))−1 (Tierney 1994).

Some Markov Chain Monte Carlo Theory
463
This listing also highlights another important point. It is not necessarily worth changing
the structure of the simulation in regular practice to produce a uniformly ergodic Markov
chain from a geometrically ergodic version, but it is almost always worth the trouble to ob-
tain a geometrically ergodic setup (discarding degree 2 ergodicity as analytically diﬃcult to
assert in almost all cases). Without geometric ergodicity convergence can take dramatically
longer and, for the purposes of practical MCMC work, essentially inﬁnite time for ergodic
chains. Fortunately, for many of the model types encountered in the social sciences these
conditions are met with little trouble using the two standard algorithms.
Where problems may arise is in the use of hybrid chains, such as Metropolis-within-Gibbs
and Variable-at-a-Time Metropolis-Hastings (Roberts and Rosenthal 1998), that combine
features of more basic algorithms and are usually speciﬁed because of posterior irregularities
in the ﬁrst place (note, actually, that “Metropolis-within-Gibbs” is a misnomer since Gibbs
sampling is a special case of Metropolis-Hastings in which a candidate is always accepted).
These can sometimes be checked with the property that a Markov chain satisfying a
minorization condition and a drift condition with V (θ) > 2b/(1−δ) is geometrically ergodic
(Rosenthal 1995a). For instance, Jones and Hobert (2004, 2001: Appendix A) give speciﬁc
minorization and drift conditions for a block Gibbs sampler to be geometrically ergodic
(block or grouped Gibbs sampler update parameters in blocks where the joint updatings
are presumed to be marginalizable post-MCMC, i.e., something like drawing θ[j]
1 , θ[j]
2
∼
π(θ1, θ2|θ[j−1]
3
), θ[j]
3 ∼π(θ3|θ[j]
1 , θ[j]
2 ), see Roberts and Sahu [1997]). Also, Jarner and Roberts
(2002) connect the drift condition to polynomial rate convergence measured by standard
norms.
From a utilitarian standpoint knowing that the Markov chain is geometrically or uni-
formly ergodic is not enough. It is only part of the complete process to worry about when
running the chain to obtain reliable results. While it is obviously important to demonstrate
ergodic properties, it does not actually conﬁrm any set of applied results. Rosenthal (1995b,
p.741) makes this particularly clear:
It is one thing to say that the variation distance to the true posterior distribution
after k steps will be less than Aαk for some α < 1 and A > 0. It is quite another
to give some idea of how much less than 1 this α will be, and how large A is,
or equivalently to give a quantitative estimate of how large k should be to make
the variation distance less than some ϵ.
In other words, it is necessary to assert at least geometric ergodicity but it is not going to
directly help the practitioner make decisions about the length of the runs. In the next two
sections we provide ﬁndings that lead to explicit advice about how to treat convergence for
the Metropolis-Hastings algorithm and the Gibbs sampler.

464
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
13.6
Implementation Concerns
In applied work there are two practical questions that a user of Markov chain Monte
Carlo algorithms must ask: (1) how long should I run the chain before I can claim that it
has converged to its invariant (stationary) distribution, and (2) how long do I need to run
the chain in stationarity before it has suﬃciently mixed throughout the target distribution?
The key factor driving both of these questions is the rate at which the Markov chain is
mixing through the parameter space: slow mixing means that the deﬁnition of “long” gets
considerably worsened.
There is also a diﬀerence between being in the state of convergence and measuring
the state of convergence. A Markov chain for a single dimension has converged at time
t to its invariant distribution (the posterior distribution of interest for correctly set up
Bayesian applications) when the transition kernel produces univariate draws arbitrarily
close to this distribution and the process therefore generates only legitimate values from a
distribution in proportion to the actual target density. For a given measure of “closeness”
(i.e., for some speciﬁed threshold, see below), a Markov chain is either in its invariant
distribution or it is not. For single-dimension chains, there is no such thing as “somewhat
converged” or “approaching convergence” (this gets more complicated for chains operating
in multiple dimensions). In fact, Rosenthal (1995a) gives an example Markov chain that
converges in exactly one step. So diagnostics and mathematical proofs that make claims
about convergence are thus analyzing only a two-state world.
To put more precision on such statements, deﬁne the vector θt ∈S ⊆ℜd as the tth
empirical draw (reached point) from the chain M(θt, t ≥0), operating on d-dimensional
measure space (Ω, F), having the transition operator f deﬁned on the Banach space of
bounded measurable functions, and having π(θ) as its invariant distribution. A normed
vector space is called a Banach space if it is complete under this metric. Completeness
means that for a given probability measure space, (Ω, F, P), if A ⊂B, B ∈Ω, p(B) = 0,
then A ∈Ω and p(A) = 0. This condition allows us to ignore a set of measure problems that
can otherwise occur. It also provides results on a general state space as well as the easier
case of a ﬁnite countable state space. Invariance in this context means that π is a probability
measure on (Ω, F) such that π(s) =

f(θ, s)π(dθ), ∀s ∈Ω. The transition kernel of the
Markov chain, f() (generalizing K above), is the mechanism that maps Ω × F →[0, 1] such
that for every A ∈F, the function f(·, A) is measurable and for every θ ∈Ω the function
f(θ, ·) is a valid probability function, as noted previously.
A chain that is positive recurrent or positive Harris recurrent (whichever appropriately
applies) and aperiodic is also α-mixing, meaning that:
α(t) = sup
A,B
p(θt ∈B, θ0 ∈A) −p(θt ∈B)p(θ0 ∈A)
 −→
t→∞0
(13.18)
(Rosenblatt 1971).
This means that for sub-spaces A and B that produce the largest

Some Markov Chain Monte Carlo Theory
465
diﬀerence, the joint probability of starting at some point in A and ending at some point
in B at time t converges to the product of the individual probabilities. This means that
these events are asymptotically (in t) independent for any deﬁnable sub-spaces. This second
result from ergodicity justiﬁes our treatment of Markov chain iterations as iid samples (Chan
1993).
There are actually some additional measure-theoretic nuances and extensions of these
properties, such as the implied assumption that π(θ) is not concentrated on a single point
as in a Dirac delta function, but the deﬁnitions given here are suﬃcient for the present
purposes.
Also, it is important to remember that ergodicity is just one way to assert
convergence. It turns out, for instance, that a periodic Markov chain can also converge under
a diﬀerent and more complicated set of assumptions (Meyn and Tweedie 1993, Chapter 13),
and we can even deﬁne ergodicity without an invariant distribution (Athreya and Ney 1978).
A Markov chain that has converged has the property that repeated applications of the
transition kernel produce an identical distribution: πf = π. By far the most commonly
used method of claiming such convergence is the total variation norm, which is restated
from (13.7):
∥f(θt) −π(θ)∥= 1
2sup A
θ∈A

Θ
|f(θt) −π(θ)| dθ.
(13.19)
This is half of the well-known L1 distance, although the L2 “chi-square” distance is useful
as well, see Diaconis and Saloﬀ-Coste (1996).
Another suggestion is the inﬁnity norm
∥f∥∞= sup
θ∈ℜd∥f(θ)∥(Roberts and Polson 1994).
The vector θ is a d-dimensional random
variable lying within A, the sub-space that makes the diﬀerence within the integral as great
as possible. When we integrate over θ ∈A it produces a supremum over the measurable
sub-space A for the set of all measurable functions on A (a set that includes f(θt) and π(θ)).
So there are two important operations occurring in the statement of (13.19). First, there
is selection of a sub-space that makes the resulting quantity as large as possible. Second,
there is integration of the distributional diﬀerence over this sub-space. Thus one gets the
most pessimistic view of the diﬀerence between f(θt) and π(θ) as possible.
The 1/2 in (13.19) constant comes from limit theory: as t →∞, the total variation
norm for A converges to twice the empirical diﬀerence for all such sub-spaces (Meyn and
Tweedie 1993, p.311):
lim
t→∞∥f(θt, ·) −π(θ)∥= 2 lim
t→∞sup
A
∥f(θt, A) −π(A)∥.
(13.20)
Another way to write the total variation norm ﬁrst deﬁnes μ(A) as a signed measure on the
state space S for the sub-space A. In the notation above, μ(A) is the integrated diﬀerence of
two distributional statements over all of A. Now the total variation norm can be expressed
as:
∥μ∥= sup
A∈S
μ(A) −inf
A∈Sμ(A),
(13.21)
which shows the same principle as (13.19) due to the explicit statement of the integral.

466
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
Zellner and Min (1995) propose three potentially useful alternatives as well, such as: the
anchored ratio convergence criterion, which selects two arbitrary points in the sample space
to calculate a posterior ratio baseline for comparison to reached chain points, the diﬀerence
convergence criterion, which compares diﬀerent analytical equivalent conditionals from the
joint distribution, and the ratio convergence criterion, which uses the last idea in ratio
form rather than diﬀerencing (see particularly p.922). We will return to the Zellner-Min
diagnostics in Chapter 14.
Thus if ∥f(θt) −π(θ)∥→0 as n →∞, the distribution of θ
converges to that of a random variable from π(θ), and this convergence is actually stronger
than standard convergence in distribution (i.e., convergence of CDFs). When ∥f(θt)−π(θ)∥
reaches values close to zero (say δ for now) we are willing to assert convergence.
The
problem, of course, is that π(θ) is a diﬃcult form to work with analytically, which is
why we are using MCMC in the ﬁrst place. Theoretical work that puts explicit bounds
on convergence includes: Lawler and Sokal (1988), Frieze, Kannan, and Polson (1994),
Ingrassia (1994), Liu (1996b), Mengersen and Tweedie (1996), Robert (1995), Roberts and
Tweedie (1996), Rosenthal (1995a), as well as Sinclair and Jerrum (1988).
For discrete problems it turns out that the converge rate can be established in proportion
to the absolute value of the second eigenvalue of the transition matrix (kernel) (Diaconis
and Stroock 1991, Fill 1991, Fulman and Wilmer 1999, Sinclair and Jerrum 1989) but this
can also be quite diﬃcult to produce for realistic problems (Frigessi, Hwang, Di Stefano, and
Sheu 1993). For examples where these approaches work in practice see: Amit (1991), Amit
and Grenander (1991), Cowles and Rosenthal (1998), Goodman and Sokal (1989), Meyn and
Tweedie (1994), Mira and Tierney (2001b), Polson (1996), Roberts and Rosenthal (1999),
and Rosenthal (1995b, 1996). Usually these solutions are particularistic to the form of the
kernel and can also produce widely varying or impractical bounds.
13.6.1
Mixing
Markov chain mixing is a related but diﬀerent concern than convergence. Mixing is the
rate at which a Markov chain traverses about the parameter space, before or after reaching
the stationary distribution. Thus slow mixing causes two problems: it retards the advance
towards the target distribution, and once there, it makes full exploration of this distribution
take longer. Both of these considerations are critical to providing valid inferences since
the pre-convergence distribution does not describe the desired marginals and failing to
mix through regions of the ﬁnal distribution biases summary statistics. Mixing problems
generally come from high correlations between model parameters or weakly identiﬁed model
speciﬁcations, and are often more pronounced for model precision parameters.
Detailed guidance about assessing mixing properties for particular applications is given
later in Chapter 14, but several general points are worth mentioning here. When a Metropolis-
Hastings chain is mixing poorly it usually has a very low acceptance rate and therefore stays
in single locations for long periods. Usually this is obvious, like acceptance ratios over some
period of time less that 1%. This view is unavailable for a Gibbs sampler chain since it

Some Markov Chain Monte Carlo Theory
467
moves on every iteration. Often, though, if one has an indication of the range of the high
density area for the posterior (for instance, a rough idea of the 90% HPD region), then
poor mixing is observed by reasonable chain periods that traverse a very limited subset of
this interval. Often such problems with the Gibbs sampler are caused by high correlations
between parameters (also discussed in Chapter 14).
13.6.2
Partial Convergence for Metropolis-Hastings
As the number of dimensions increases, the sensitivity (and complexity) of the Metropolis-
Hastings algorithm increases dramatically since the measure space (Ω, F) is deﬁned such
that each abstract point in Ω is d-dimensional and the σ-ﬁeld of subsets F is generated by a
countable collection of sets on ℜd. An ergodic Markov chain M(θt, t ≥0) has the property:
∥f(θt) −π(θ)∥→0
as n →∞, ∀θ ∈Θ, but the size of d is critical in determining the
rate since each step is d-dimensional. The primary complexity introduced by dimensionality
here has to do with the strictness by which we apply f(θt) −π(θ). Suppose now that there
is a subset of these dimensions e < d that are of primary interest and the remaining d−e are
essentially a result of nuisance parameters. Is it then reasonable to require only evidence of
partial convergence? That is, at time t for some small δ:
∥f(θ∗
t ) −π(θ∗)∥≈δ,
∀θ∗∈ℜe
(13.22)
but,
∥f(θ†
t) −π(θ†)∥≫δ
∀θ† ∈ℜd−e,
(13.23)
where decisions are made one at a time for each of these dimensions using standard empir-
ical diagnostics of stationarity. Even though our evidence is derived from these diagnostics
it is important to note that they measure Markov chain stability rather than actual conver-
gence and so the sum of each of these across dimensions is used to just assert convergence
(convergence in total variation norm gives stationarity but the converse is not similarly
guaranteed).
There is one very important distinction to be made here. The Markov chain M(θt, t ≥0)
is assumed ergodic over all of Ω and is thus guaranteed to eventually converge across all
of ℜd. What we see by observing (13.22) and (13.23) at time point t is a lack of evidence
to say that there is full dimensional convergence.
Can a Markov chain operating in d
dimensions be drawing from the true invariant distribution in e sub-dimensions but not
in d −e sub-dimensions? The standard empirical diagnostics in WinBUGS, CODA, and BOA
(Brooks and Gelman, 1998a, 1998b; Geweke 1992; Heidelberger and Welch 1981a, 1981b;
Raftery and Lewis, 1992, 1996, all described in Chapter 14), as well as others used in
practice (Brooks, Dellaportas, and Roberts [1997] develop one based on the total variation
norm discussed above), provide a series of parameter-by-parameter tests of non-convergence.
Hence they indicate when a single dimension chain is suﬃciently trending as to violate
speciﬁc distributional assumptions that reﬂect stability, but they do assert convergence in
the opposite case.

468
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
These diagnostics operate on marginal distributions individually since the output of the
MCMC process is a set of marginal empirical draws. Unfortunately the total variation
norm given above only shows us if the chain has converged simultaneously across every
dimension, providing a strong disconnect between theoreticians who derive convergence
properties for speciﬁc chains under speciﬁc circumstances and the masses who want to run
simple empirical diagnostics in easy-to-use software environments. To see this disconnect
more clearly, consider the right-hand-side of (13.19) written out with more detail:
1
2sup A
θ∈A


θ1
· · ·

θe

θe+1
· · ·

θd
[f(θ1, . . . , θd)t −π(θ1, . . . , θd)]
dθ1 · · · dθedθe+1 · · · dθd
.
(13.24)
If f() and π() were able to be expressed as independent products (i.e., f(x, y) = f(x)f(y))
for the θi, then this would be a straightforward integration process. As stated this cannot
be true for π since we are doing MCMC for this very reason. But what about f()? Consider
the actual transition probability for the Metropolis-Hastings algorithm from θ to θ′ from
(10.28):
A(θ, θ′) = min
7π(θ′)g(θ|θ′)
f(θ)g(θ′|θ) , 1
8
g(θ′|θ) + (1 −r(θ))δθ(θ′),
(13.25)
where g() is the proposal distribution,
r(θ) =

min
7f(θ′)g(θ|θ′)
f(θ)g(θ′|θ) , 1
8
g(θ′|θ)dθ′,
and
δθ(θ′) = 1 if θ = θ′ and zero otherwise
(the Dirac delta function). It is clear from looking at (13.25) that we cannot generally
disentangle dimensions. In particular, note the conditionals that exist across θ and θ′.
What this means is that decisions to jump to a proposal point in d-space (Ω, F) are made
based on the current position in every dimension for the Metropolis-Hastings algorithm. So
if the chain has not converged in the ith dimension, i ∈[e + 1:d], its current placement
eﬀects the single acceptance ratio and therefore the probability of making a complete d-
dimensional jump. And this is all under the assumption of ergodicity.
So now that we know that non-convergence in at least one dimension aﬀects decisions to
move in all dimensions, the natural question is how does this work? A Metropolis-Hastings
chain dimension that has not converged is producing on average lower density contributions
in the acceptance ratio. Therefore in cases where the conditionality on the current state is
explicit (all general forms except the independence chain Metropolis-Hastings where jump-
ing values are selected from a convenient form as in the random walk chain, but ignoring the
current position completely: g(θ′|θ) = f(θ′)) it retards the mixing of the whole chain. Be-
cause the chain is ergodic, it is alpha mixing (sup
A,B
|p(θt ∈B, θ0 ∈A) −p(θt ∈B)p(θ0 ∈A)|
goes to zero in the limit) but ineﬃciently so (slowly) since non-convergence for the d −e

Some Markov Chain Monte Carlo Theory
469
dimensions implies poorer mixing and greater distance between p(θt ∈B, θ0 ∈A) and
p(θt ∈B)p(θ0 ∈A).
A chain that is completely in its stationary distribution mixes better (Robert and Casella
2004, Chapter 12).
So even in the case where θi, the un-converged dimension here, is not
a parent node in the model speciﬁed by π, there is a negative eﬀect: a Markov chain
that has not suﬃciently mixed through the target distribution produces biased empirical
summaries because collected chain values will be incomplete, having had insuﬃcient time
to fully explore the target.
13.6.3
Partial Convergence for the Gibbs Sampler
Robert and Richardson (1998) show that when a Markov chain, M(θt, t ≥0), is derived
from another Markov chain, M(φt, t ≥0), by simulating from a distribution according to
π(θ|φt), the properties of the ﬁrst chain inherit that of the conditional. Critically, this
conditionality deﬁnes new sub-spaces of (Ω, F) with new measure properties.
For our purposes the important point is that if M(φt, t ≥0) is geometrically ergodic,
then M(θt, t ≥0) is as well, which is easy to demonstrate using the data augmenta-
tion principle.
The marginal distribution for the geometrically ergodic chain at time t
is πt(φ) with invariant distribution π(φ). We can now express the invariant distribution of
θ in conditional terms: π(θ) =

φ π(θ|φ)π(φ)dφ, with the marginal distribution at time t:
πt(θ) = 
φ π(θ|φ)πt(φ)dφ. The conditional π(θ|φ) appears in the second expression without
reference to time since θt is simulated at each step from π(θ|φ). These deﬁne the total
variation norm for θt:
∥πt(θ) −π(θ)∥= 1
2sup A
θ∈A


θ

φ
π(θ|φ)πt(φ)dφdθ −

θ

φ
π(θ|φ)π(φ)dφdθ

= 1
2sup A
θ∈A


θ

φ
π(θ|φ) [πt(φ) −π(φ)] dφdθ

≤∥πt(φ) −π(φ)∥,
(13.26)
where the inequality comes from π(θ|φ) ≤1 by the integration of a probability function
over the measure space for φ (the rate ρ carries through as well). Switching the order of
integration comes from stated regularity conditions on probability functions. Note that this
process is related to, but distinct from, so-called Rao-Blackwellization where intentional con-
ditioning is imposed to reduce the variance of computed expectations or marginals (Casella
and Robert 1996). The result in (13.26) is that a non-convergent dimension to the Gibbs
sampler (θ here) “pushes” the others (φ here) away from stationarity as well, even if these
pass an empirical diagnostic for convergence.
One utility of this result is that if we can intentionally augment a target chain with a
simple form that is known to be geometrically ergodic, then we can impose this property
even though we increase the dimension of (Ω, F) (Diaconis and Saloﬀ-Coste 1993, Fill 1991).

470
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
Robert and Richardson (1998) point out that this is particularly useful when a target chain
of unknown convergence characteristics is conditioned on a simple discrete Markov chain
known to be geometrically ergodic with speciﬁc ρ and m(θ) (alternately m). Also, if the
chain that is conditioned on is α-mixing, the target chain will be as well.
The big point comes from the structure of the Gibbs sampler (the default engine of
BUGS). Since the kernel is an iteration of full conditionals, π(θj|θ−j) for j = 1, . . . , d, then
according to the logic just discussed, either the Gibbs sampler is geometrically ergodic in
every dimension or it is not geometrically ergodic in any dimension. Importantly, since the
sub-chains share the same geometric rate of convergence, ρ, then one should be cautious
with empirical diagnostics since they provide evidence of non-convergence not evidence of
convergence (see Asmussen, Glynn, and Thorisson [1992] for a detailed discussion on this
point).
Recall that at any given time t a Markov chain is either converged to its invariant
distribution or it has not: there is a speciﬁc time when ∥f(θt) −π(θ)∥< δ for some chosen
δ, we just do not necessarily know the moment.
Suppose for a two-parameter Gibbs sampler θ1 passes some empirical diagnostic and
θ2 does not. Since they share the same rate of convergence, for θ1 to be in its invariant
distribution while θ2 is not means that you are testing the chain for convergence during the
very small number of intervals where the diﬀering results are due to probabilistic features
of the chain or the test. Given the standard number iterations expected for MCMC work in
the social sciences (generally tens or hundreds of thousands), the probability that you have
stumbled up this exact interval is essentially zero. Conversely, the test fails for θ2 because
the Markov chain for this dimension is either not yet in stationarity, or is in stationarity but
has failed to suﬃciently explore the target distribution to produce a stable summary statistic
for the chosen diagnostic. The latter condition only exists for a relatively short period of
time, even with poor mixing. Moreover, the faster the convergence rate (i.e., geometric
or uniform), the smaller the numerator in the calculation of this probability making it
even less likely that the user caught the interval of intermediate results using the Gibbs
samplers with listed properties above, where the size of this eﬀect is notably a function of
m(θ) (or m) and ρ.
Therefore, for the Gibbs sampler, evidence of non-convergence in
any dimension is evidence of non-convergence in all dimensions. So for users of the usual
diagnostic packages, CODA and BOA, the standard for multidimensional convergence needs to
be high. For a relatively small number of parameters, one would expect broad consensus
across diagnostics. However, for large numbers of model parameters, we need to be aware
that the diagnostics are built on formal hypothesis tests at selected α levels and therefore
1 −α tests for large numbers of parameters will fail for about α proportion of dimensions,
even in full convergence.
Note that this same logic applies to Metropolis-Hastings MCMC for parameters with
conditions formed by hierarchies, which are a natural and common feature of Bayesian
model speciﬁcations. This inheritance of convergence properties does not necessarily occur,
however, for every parameter as in the perfectly symmetric case of the Gibbs sampler. It
also is not reciprocal in that the conditions in a Bayesian hierarchical model ﬂow downward

Some Markov Chain Monte Carlo Theory
471
from founder nodes to dependent nodes. Note that these conditionals result explicitly from
the model rather than through algorithmic conditioning in the Gibbs sampler sense. In
addition, parameters can be highly correlated without these structural relationships. The
diﬃculty posed by all of these characteristics is that they generally slow the mixing of the
chain, making convergence and full exploration more diﬃcult.
13.7
Exercises
13.1
In Section 13.2, three conditions were given for F to be an associated ﬁeld of Ω.
Show that the ﬁrst condition could be replaced with ∅∈F using properties of
one of the other two conditions. Similarly, prove that the Kolmogorov axioms can
be stated with respect to the probability of the null set or the probability of the
complete set.
13.2
Given a Markov chain on Ω and two sub-states A, B ∈Ω, where all elements of B
can be reached from A: A is called essential for B if all elements of A can also be
reached from B, otherwise inessential. Show that if A is essential for B, then B is
essential for A.
13.3
Suppose we have the probability space (Ω, F, P), sometimes called a triple, and
A1, A2, . . . , Ak ∈F. Prove the ﬁnite sub-additive property that:
P
* k<
i=1
Ai
,
≤
k

i=1
p(Ai),
(Boole’s Inequality).
13.4
Using the transition matrix from Example 10.1.2 in Chapter 10 (page 336),
K =
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
1
3
0
1
3
1
3
0
0
0
1
3
0
0
1
3
1
3
1
3
1
3
1
3
0
0
0
0
0
0
1
3
1
3
1
3
1
3
1
3
0
0
1
3
0
0
0
1
3
1
3
0
1
3
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
,
construct a directed graph showing the immediate paths between these six states.
Does this reveal anything about moving from states that are not immediately con-
nected.
13.5
Prove that uniform ergodicity gives a faster rate of convergence than geometric
ergodicity and that geometric ergodicity gives a faster rate of convergence than
ergodicity of degree 2.

472
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
13.6
Write a Metropolis-Hastings algorithm in R that is purposely transient using the
data from Example 10.5 in Chapter 10 (page 361). Plot the ﬁrst 500 iterations.
13.7
Construct a stochastic process without the Markovian property and construct de-
liberately non-homogeneous Markov chain.
13.8
A Markov chain operates on a k-cycle if it moves clockwise and counterclockwise
on a loop of 1, 2, . . . , k ordered values. If θ moves clockwise with probability p and
counterclockwise with probability 1 −p, then give the stationary distribution for
p = 1
2. Is this diﬀerent for p ̸= 1
2?
13.9
Give an example of a maximally ψ-irreducible measure and one that it dominates.
13.10
One urn contains k black marbles and another urn contains k white marbles. At
each iteration of a process one ball is uniformly randomly selected from each urn
and exchanged between the urns. Deﬁne θ[t] as the number of white balls in urn
number 1 after the tth step. Obtain the following:
a. Prove that this is a Markov chain.
b. Derive the transition probabilities for θ[t].
c. Produce the stationary distribution for θ[t].
13.11
A single-dimension Markov chain, M(θt, t ≥0), with transition matrix P and
unique stationary distribution π(θ) is time-reversible iﬀ:
π(θi)p(θi, θj) = π(θj)p(θj, θi).
for all θi and θj in Ω. Prove that the reversed chain, deﬁned by M(θs, s ≤0) =
M(θt, −t ≥0), has the identical transition matrix.
13.12
Consider a Markov chain based on an AR(1) speciﬁcation:
θ[j+1] = εθ[j] + ϵ[j],
where ϵ[j] ∼N(0, 1), for all j = 1, . . . , J iterations, and ε ∈(0 : 1).
See also
Exercise 13 (page 139). Show that the stationary distribution of this Markov chain
is N(0, 1/(1 −ε2)).
13.13
Meyn and Tweedie (1993, p.73) deﬁne an n-step taboo probability as:

AP n(x, B) = Px(Φn ∈B, τA ≥n),
x ∈X, A, B ∈B(X),
meaning the probability that the Markov chain, Φ, transitions to set B in n steps
avoiding (not hitting) set A. Here X is a general state space with countably gen-
erated σ-ﬁeld B(X), and τA is the return time to A (their notation diﬀers slightly
from that in this chapter). Show that:

AP n(x, B) =

Ac P(x, dy)AP n−1(y, B),
where Ac denotes the complementary set to A.

Some Markov Chain Monte Carlo Theory
473
13.14
For Exercise 12, implement the AR(1) Markov chain in R and graph the trajectory
of the chain for three diﬀerent values of ε in the same plot.
13.15
The Langevin algorithm is a Metropolis-Hastings variant where on each step a small
increment is added to the proposal point in the direction of higher density. Show
that making this increment an increment of the log gradient in the positive direction
produces an ergodic Markov chain by preserving the detailed balance equation.
13.16
A transition matrix K is deﬁned on the binary set {0, 1} by:
K =

1 −a
a
b
1 −b

with 0 < a, b ≤1 and at least one of these less than one. Show that the stationary
distribution of the Markov chain deﬁned by this transition kernel is given by p(0) =
b/(a + b), p(1) = a/(a + b).
13.17
Consider a stochastic process, θ[t] (t ≥0), on the probability space (Ω, F, P). If:
Ft ⊂Ft+1, θ[t] is measurable on F with ﬁnite ﬁrst moment, and with probability
one E[θ[t+1]|Ft] = θ[t], then this process is called a Martingale (Billingsley 1995,
p.458). Prove that Martingales do or do not have the Markovian property.
13.18
Replacing E[θ[t+1]|Ft] = θ[t] in the last exercise with E[θ[t+1]|Ft] ≥θ[t] produces
a supermartingale, and replacing with E[θ[t+1]|Ft] ≤θ[t] produces a submartingale.
If t ≥0, show that θ[t] is a supermartingale if and only if −θ[t] is a submartingale,
and vice versa. , and
13.19
Prove that for a Markov chain that is positive Harris recurrent, ∃σ−ﬁnite prob-
ability measure, P, on S such that for an irreducible Markov chain, Xt, at time
t, p(Xn ∈A) = 1, ∀A ∈S where P(A) > 0, and aperiodic is also α-mixing,
α(t) = sup
A,B
p(θt ∈B, θ0 ∈A) −p(θt ∈B)p(θ0 ∈A)
 −→
t→∞0.
13.20
A random walk Markov chain on I has the following transition kernel with ﬁxed
0 < p < 1:
K(i, j) =
⎧
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎩
p
if j −1 = i
1 −p
if j + 1 = i
0
otherwise.
Show that this Markov chain is irreducible with period 2. Suppose now that p
is randomly drawn on each iteration of the Markov chain from the distribution
∼U(0, 1) (exclusive of the endpoints). Is this still an irreducible Markov chain?


Chapter 14
Utilitarian Markov Chain Monte Carlo
14.1
Objectives
This chapter has several rather practical purposes related to applied MCMC work: to
introduce formal convergence diagnostic techniques, to provide tools to improve mixing
and coverage, and to note a number of challenges that are routinely encountered. This
is a stark contrast to the last chapter, which was concerned with theoretical properties
of Markov chains and Markov chain Monte Carlo. Since applied work is generally done
computationally through the convenient programs BUGS (in any of the versions) and JAGS,
or by writing source code in R, C, or even Fortran, practical considerations are important
to getting reliable inferences from chain values. Most of the concern centers on assessing
convergence, but the speed of the sampler, and its ability to thoroughly explore the sample
space are also important issues to be concerned with. This chapter also describes the two
very similar R packages for analyzing MCMC output and evaluating convergence: BOA and
CODA. These are merely convenient functional routines, and users will often want to go
beyond their capabilities, particularly in graphics. However, the purpose here is mainly to
understand the key workings of these tools rather than to function as a detailed description
of the syntax of software. See Albert (2009) or Ntzoufras (2009) for recent book-length
works with very detailed R and BUGS code description.
As estimation with MCMC tools becomes more common, mastery of computational
mechanical challenges grows in importance. It is fundamentally import to remind ourselves
that MCMC estimation is not a cookbook procedure and that it is necessary to pay attention
to convergence issues. There are two critical and practical challenges in assessing MCMC
reliability:
▷For any Markov chain at some given time t, there is no absolute assurance that this
chain is currently in its stationary (target) distribution.
▷There is no way to guarantee that a Markov chain will explore all of the areas of the
target distribution in a ﬁnite run time.
The bulk of the tools in this chapter are a means of adding to our conﬁdence that a speciﬁc
application has addressed these two concerns. Gelman (1996) adds three related standard
problems to also consider: an inappropriately speciﬁed model, errors in programming the
475

476
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
Markov chain (the achieved stationary distribution of the chain may not be the desired
target distribution), and slow convergence (distinct from mixing through the posterior).
The latter can be particularly troublesome even for experienced researchers in this area
as it is often diﬃcult to anticipate problems like the chain getting stuck in low-density,
high-dimension regions for long periods of time (see the litany of worries in Guan, Fleißner,
and Joyce [2006]). Essentially these concerns boil down to three worries: setting up the
parameters of the process correctly, ensuring good mixing through the sample space, and
making a reliable decision about convergence at some point in time. These are the primary
concerns of this chapter.
This preliminary discussion seems to imply that the use of MCMC algorithms is fraught
with danger and despair (the WinBUGS webpage and manual contain a warning message
that would impress tobacco regulators). However, we know from Chapter 13 that all er-
godic Markov chains are guaranteed to eventually converge (see also Chan [1993] and Polson
[1996]), and it is often quite easy to determine if a given chain has not converged. Con-
sequently, convergence diagnostics have moved from fairly ad hoc methods (Gelfand and
Smith 1990) to the reasonably sophisticated statistical tests outlined in this chapter. In
addition, progressively faster processors mean that the time-dependent issues are guaran-
teed to diminish in the future. Furthermore, in nearly all occasions with typical generalized
linear models and non-exotic priors, the Markov chain converges quickly with obvious ev-
idence.
Nonetheless, users of MCMC for Bayesian estimation should use all reasonably
caution with respect to controllable features.
14.2
Practical Considerations and Admonitions
This section covers a few practical considerations in implementing MCMC algorithms.
There are several critical design questions that must be answered before actually setting
up the chain and running it, whether one is using a packaged resource or writing original
code.
These include such decisions as: the determination of where to start the chain,
judging how long to “burn-in” the chain before recording values for inference, determining
whether to “thin” the chain values by discarding at intervals, and setting various software
implementation parameters.
14.2.1
Starting Points
Starting points as an integrated part of the simulation speciﬁcation is an under-studied
aspect, except perhaps in the case of one particular convergence diagnostic (Gelman and
Rubin 1992a). Generally it is best to try several starting points in the state space and
observe whether they lead to noticeably diﬀerent descriptions of the posteriors. This is

Utilitarian Markov Chain Monte Carlo
477
surely a sign of non-convergence of the Markov chain, although certainly not a systematic
test. Unfortunately the reverse does not hold: it is not true that if one starts several Markov
chains in diﬀerent places in the state space and they congregate for a time in the same region
that this is the region that characterizes the stationary distribution. It could be that all
of the chains are seduced by the same local maxima, and will for a time mix around in its
local region.
Overdispersing the starting points relative to the expected modal point is likely to pro-
vide a useful assessment (Gelman and Rubin 1992a, 1992b).
We will look in detail at
Gelman and Rubin’s formal diagnostic in Section 14.3.3.2, but for now it is warranted to
talk about strategies for determining such overdispersed points. If one can determine the
mode with reasonable certainty, either by using the EM algorithm, a grid search, or some
other technique (possibly analytically), then it is relatively simple to spread starting points
around it at some distance. If this is not possible, or perhaps excessively complicated in
high dimensions, then it is often straightforward to carefully spread starting points widely
throughout the sample space.
Sometimes starting points of theoretical interest are available. It might be the case that
a starting point can be assigned to values associated with other studies, subject-matter
expertise, previous work with the same data, or even the modal point of the associated
likelihood function. Often such points are close to the high density region for the posterior
being explored. These strategies, of course, do not involve overdispersion and the use of the
Gelman and Rubin convergence diagnostic.
Some researchers randomly distribute starting points through the state space with the
idea that if little is known before the estimation process, these will at least be reasonably
overdispersed points. Overdispersed starting points relative to the central region of the
distribution mean that if the chains coalesce into the same region anyway, it is some evidence
that they have converged to the stationary distribution.
14.2.2
Thinning the Chain
It is sometimes the case that with very long simulations, storage of the observed chain
values on the computer becomes an issue, although this need has diminished substantially
over time. The need for large storage ﬁles results from running the chain for extended
periods, running multiple parallel chains, or monitoring a large number of parameters.
Social science models, however, rarely call for hundreds or thousands of model parameters
as one might see in statistical genetics, though it may be necessary in future research as
the bounds between disciplines recede. More often long runs are needed in the presence
of: high autocorrelation in the iterations, slow convergence of the chain to its limiting
distribution combined with many parallel simultaneous runs of the chain, possibly combined
with relatively high dimensionality of the model. These are addressed in this chapter. The
problem introduced is that disk storage may be strained by these demands. Furthermore,
many of the convergence diagnostics described in this chapter slow down considerably with

478
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
very large matrix sizes. Some researchers even report software failures due to excessively
large chains (Rathbun and Black 2006). However, as computer storage capabilities continue
to increase rapidly in size (including cloud storage), while decreasing in cost, these problems
will decline in importance.
The idea of thinning the chain is to run the chain normally but record only every kth
value of the chain, k some small positive integer, thus reducing the storage demands while
still preserving the integrity of the Markovian process. Importantly, note that thinning does
not in any way improve the quality of the estimate (suggested presumably but erroneously
as a way to increase the independence of the ﬁnal evaluated values [Geyer 1992]), speed
up the chain, or help in convergence and mixing. Instead, it is purely a device for dealing
with possibly limited computer resources. In fact, the quality of the subsequent estimation
always suﬀers because the resulting variance estimate will be higher for a given run-time
(MacEachern and Berliner 1994), albeit to varying degrees depending on the length of the
chain and the nature of the model. Also since the variance estimate is wrong it confounds
the estimate of the serial correlation of the chain for the purpose of understanding the
mixing properties. However, sometimes this variance issue is a trivial concern and thinning
remains a convenience.
Given the trade-oﬀs between storage and accuracy as well as diagnostic ability, what
value of k is appropriate in a given application?
The greater the amount of thinning,
the more potentially important information is lost, suggesting caution. Conversely, prior
to assumed convergence, thinning is irrelevant inferentially and may therefore be useful,
provided it gives suﬃcient information to the diagnostics used. Many researchers pick every
fourth, ﬁfth, or tenth iteration to save, but for completely arbitrary reasons. Occasionally
applications will thin to every 30th, 50th, or even 100th iteration, but this tends to decrease
chain eﬃciency more substantially.
14.2.3
The Burn-In Period
First, one must decide the length of the burn-in period, the beginning set of runs that
are discarded under the assumption that they represent pre-convergence values and are
therefore not equivalent draws of the desired limiting distribution. The slower the chain is
to converge, the more careful one should be about the burn-in period. Usually this involves
cautiously extending the length of the chain, but this chapter also presents some customized
tools for speeding up this process. Unfortunately, even starting the chain right in the area
of highest density does not guarantee that the burn-in period is unimportant as it will still
take the Markov chain some time to “forget” its starting region, take some time to settle
into the stationary distribution, and then need some further time to fully explore the target
distribution.
There is no systematic, universal, guaranteed way to calculate the length of the burn-in
period, and considerable work on convergence diagnostics has been done to make speciﬁc
recommendations and identify helpful tests. Raftery and Lewis (1992, 1996) suggest a run-

Utilitarian Markov Chain Monte Carlo
479
ning diagnostic for the length of the burn-in period, which starts with the analysis of an
initial run. The idea is to solve for the number of iterations required to estimate some
quantile of interest within an acceptable range of accuracy, at a speciﬁed probability level.
The procedure is based on conventional normal distribution theory and implemented in the
R package mcgibbsit, as well as in the standard diagnostic packages BOA and CODA. Unfor-
tunately, this procedure does not always work, but it often does provide good approximate
guidance (Robert and Cellier 1998, Brooks and Roberts 1997). See Section 14.3.3.4 for an
extended description.
14.3
Assessing Convergence of Markov Chains
As shown in Chapter 10, the empirical results from a given MCMC analysis are not
deemed reliable until the chain has reached its stationary distribution and has time to
suﬃciently mix throughout. Until M(θt, t ≥0) converges at time t∗(i.e. ∥f(θ∗
t ) −π(θ)∥
is negligible), it is not possible to rely upon the eﬀect of any variant of the central limit
theorem. Therefore the single greatest risk in applied MCMC work is that the user will
misjudge the required length of the burn-in period and assert convergence before the Markov
chain has actually reached the stationary distribution.
Unfortunately, some convergence problems come directly from the model speciﬁcation
and it may not be obvious when the MCMC process fails. Natarajan and McCulloch (1995)
found that it is possible to have a proper form for every full conditional distribution in a
Gibbs sampler and still specify a joint posterior that is improper. Hobert and Casella (1998)
in a seminal paper demonstrate that resulting improper posteriors are useless for purposes
of inference. Obviously many of these issues emanate from the (sometimes desired) speciﬁ-
cation of improper priors, which are typically not a problem in simple, stylized models but
can present diﬃcult algorithmic challenges in fully-developed social science speciﬁcations.
One common alternative (Chapter 4) is to use highly diﬀuse but proper priors. This is
usually an eﬀective alternative but can sometimes lead to slow convergence of the Markov
chain, and this should be checked. Furthermore, if the model is truly non-identiﬁed in the
classic sense (Manski 1995), then the MCMC estimation process is going to fail to provide
useful results, even if it appears to converge (which is highly unlikely).
There are basically three approaches to determining convergence for Markov chains: as-
sessing the theoretical and mathematical properties of particular Markov chains, diagnosing
summary statistics from in-progress models, and avoiding the issue altogether with perfect
sampling, which uses the idea of “coupling from the past” to produce a sample from the
exact stationary distribution (Propp and Wilson 1996). The emphasis in this chapter is on
the second approach, whereas perfect sampling is described in Chapter 15, Section 15.3.
The ﬁrst approach is to study the mathematical properties of individual chain transition

480
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
kernels, perhaps placing restrictions on data or analysis, and then determining in advance
the total variation distance to the target distribution with some speciﬁed tolerance. MCMC
algorithms on discrete state space converge at a rate related to how close the second largest
eigenvalue of the transition matrix for discrete state spaces is to one. This is an elegant
theoretical result that is often diﬃcult to apply to actual sampling output.
The purely theoretical approach has several disadvantages: it often becomes inordi-
nately complex mathematically, the bounds can be “weak” meaning suﬃciently wide as to
not be useful in practice (i.e., lacking reasonable guidance about when to stop sampling),
some of the approaches are restricted to unrealistically simple models, and generally the
calculations are model-speciﬁc. Nonetheless, a great deal of important work has been done
here: a Gibbs sampler on variance components models for large data size and number of
parameters (Rosenthal 1995b), transition kernels that satisfy certain minorization condi-
tions on the state space (Rosenthal 1995a), log-concave functions over a discretized sample
space (Frieze, Kannan, and Polson 1994), required properties of the eigenvalues (Frigessi et
al. 1993, Ingrassia 1994), data augmentation on a ﬁnite sample space (Rosenthal 1993), the
special case of Gaussian forms (Amit 1991, 1996), the conditions and rate of convergence
for standard algorithms (Roberts and Polson 1994; Roberts and Smith 1994); and more
generally, the models described theoretically in Meyn and Tweedie (1993, 1994). Madras
and Sezer (2010) use Steinsaltz’s drift functions for obtaining bounds on the rate of con-
vergence on a general state space, Fort, Moulines, and Priouret (2011)
get convergence
bounds for new adaptive and interacting Markov chains, and Roberts and Rosenthal (2011)
extend their work on independence samplers from geometrically ergodic Markov chains to
non-geometrically ergodic Markov chains.
The second convergence assessment method involves monitoring the performance of the
chain as part of the estimation process and making an often subjective determination about
when to stop the chain. Also related to this process are eﬀorts to “accelerate” convergence
through various particularistic properties. There are quite a few of these techniques and
several of the most popular and straightforward will be discussed here.
For additional
discussion of these tools, see the review essays by Brooks (1998a), Brooks and Roberts
(1999), Cowles and Carlin (1996), Mengersen, Robert, and Guihenneuc-Jouyaux (1999), as
well as Gelfand and Sahu (1994).
The general process is to run the chain for some conservatively large number of iterations,
and then dispose of a conservatively large proportion of the early values (say half), and
then to run all of the standard empirical diagnostics. If there is any indication from these
diagnostics that the Markov chain is not in its stationary distribution then run the chain
for an additional conservatively long period and retest. This process is iterated until the
researcher is convinced that there are no long concerns from the diagnostics. There is no
“magic” number of iterations that makes the process conservative, but as computers get
faster something on the order of 104 has become something on the order of 105 or higher.
Of course more complex models require more iterations so such numbers should not be
considered as a “rule-of-thumb” (that phrase usually presages bad advice in statistics).

Utilitarian Markov Chain Monte Carlo
481
Finally, after one is satisﬁed with the length of the chain, and have moved on to writing
up the results, continue the iterating chain in the background or on another machine for a
very long time as an “insurance run” on the order of 106 and later summarize the last 104
as additional veriﬁcation. Almost certainly the numerical values will be the same (within
simulation error), but this provides additional faith in the results.
It is essential to remember that the convergence diagnostics described below, as well as
others in the literature, are actually indicators of nonconvergence. That is, failing to ﬁnd
evidence of nonconvergence with these procedures is just that; it is not direct evidence of
convergence. The careful practitioner should treat encouraging results from one test with
continued skepticism and be willing to use multiple diagnostics on any single Markov chain,
any one of which can provide suﬃcient evidence of failure. It is also important to remember
that this is the process of evaluating the integrity of the MCMC process; it does not evaluate
the overall quality of the speciﬁed statistical model.
The following subsections outline the use of popular MCMC diagnostics including those
provided by BOA and CODA, and some are also integrated directly into the WinBUGS environ-
ment. The R package superdiag (Tsai and Gill 2012) calls all of the conventional conver-
gence diagnostics used in typical MCMC output assessment in one convenient R statement.
Each of these diagnostics have limitations, and therefore it is recommended that cautious
users evaluate Markov chain output with each of these. Here these convergence diagnostics
are illustrated with two real-data examples where one produces clean and obvious conver-
gence assessments while the other remains problematical. In this way readers can see both
positive and negative outcomes for comparison with their own results.
■Example 14.1:
Tobit Model for Death Penalty Support.
Norrander (2000)
uses Tobit models (Tobin 1958) to look at social and political inﬂuences on U.S. state
decisions to impose the death penalty since the Supreme Court ruled the practice
constitutional in Furman v.
Georgia 1972.
The research question is whether the
ideological, racial, and religious makeup, political culture, and urbanization are causal
eﬀects for state-level death sentences from 1993 to 1995. Norrander posits a causal
model whereby public opinion centrally, inﬂuenced by past policies and demographic
factors, determines death penalty rates by legitimating the practice over time. The
Tobit model to account for censoring is appropriate here because 15 states did not
have capital punishment provisions on the books in the studied period causing the
actual eﬀect of public opinion on death penalty rates to be substantively missing. If
these states had the legal ability to impose death penalty sentences, then it would
be possible to observe whether there exists a relationship between the explanatory
variables and the count. For instance, the death penalty in murder cases in Hawaii is
recorded as zero, but is unlikely to actually be zero if observable. In addition, these
data are also truncated at zero since states cannot impose a negative number of death
penalty sentences.
Deﬁne now terms consistent with the discussion in Amemiya (1985, Chapter 10). If

482
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
z is a latent outcome variable in this context with the assumptions z = xβ + η and
zi ∼N(xβ, σ2), then the observed outcome variable is produced according to: yi = zi
if zi > 0, and yi = 0, if zi ≤0. The likelihood function is then:
L(β, σ2|y, X) =
+
yi=0
'
1 −Φ
xiβ
σ
( +
yi>0
(σ−1) exp
'
−1
2σ2 (yi −xiβ)2
(
.
(14.1)
Chib (1992) introduces a blocked Gibbs sampling estimation process for this model
using data augmentation, Albert and Chib (1993) extend this generally to discrete
choice outcomes, and Chib and Greenberg (1998) focus on multivariate probit. This
is a quite natural approach since augmentation (Chapter 10, Section 10.6) can be
done with the computationally convenient latent variable z. Furthermore, a ﬂexible
parameterization for the priors is given by Gawande (1998):
β|σ2 ∼N(β0, Iσ2B−1
0 )
σ2 ∼IG
γ0
2 , γ1
2

(14.2)
with vector hyperparameter β0, scalar hyperparameters B0, γ0 > 2, γ1 > 0, and
appropriately sized identity matrix I. Substantial prior ﬂexibility can be achieved
with varied levels of these parameters, although values far from those implied by the
data will make the algorithm run very slowly. The full conditional distributions for
Gibbs sampling are given for the β block, σ2, and the individual zi|yi = 0 as:
β|σ2, z, y, X ∼N

(B0 + X′X)−1)(β0B0 + X′z),
(σ−2B0 + σ−2X′X)−1)

σ2|β, z, y, X ∼IG
γ0 + n
2
, γ1 + (z −Xβ)′(z −Xβ)
2

zi|yi = 0, β, σ, X ∼T N(Xβ, σ2)I(−∞,0),
(14.3)
where T N () denotes the truncated normal and the indicator function I(−∞,0) provides
the bounds of truncation. The results from this model are sensitive to values of B0,
and this is why it is common to see very diﬀuse priors in this speciﬁcation.
TABLE 14.1:
Posterior, Tobit Model
Mean
Std. Error
Median
95% HPD Interval
Intercept
-14.5451
3.6721
-14.5019
[-21.7661 : -7.3500]
Past Rates
171.1460
8.0482
171.1385
[155.2004 : 186.6079]
Political Culture
0.3461
0.1452
0.3438
[ 0.0596 :
0.6216]
Current Opinion
3.9738
1.0667
3.9632
[ 1.8575 :
6.0223]
Ideology
3.1423
1.1107
3.1436
[ 0.9728 :
5.3146]
Murder Rate
0.0088
0.0802
0.0095
[ -0.1567 :
0.1590]
To extend Norrander’s model, we add two additional explanatory variables: state

Utilitarian Markov Chain Monte Carlo
483
average ideology and the state-level murder rate. A Gibbs sampler code in R is applied
using the full conditional distributions given above for β, σ2, and the zi using estimates
from the MLE as starting points (see the Computational Addendum). The prior
parameters for the inverse gamma speciﬁcation are stipulated to provide a relatively
diﬀuse form: γ0 = 300, γ1 = 100, B0 = 0.02, and β = 0. Making the gamma form less
diﬀuse than the one speciﬁed here leads to a poor mixing chain as generated values in
the truncation range become rare. The chain is run for 50, 000 iterations with the ﬁrst
40, 000 values discarded as burn-in. The marginal results summarized in Table 14.1
do not generally contradict the original multiple analyses in Norrander (2000). We
will see through the course of this chapter whether such results should be trusted.
TABLE 14.2:
Proportional Changes in Eastern
European Militaries, 1948-1983
Proportional Change in Military Personnel
Yugo.
Alb.
Bulg.
Czec.
GDR
Hung.
Poland
Rum.
USSR
1949
0.000
0.083
0.166
0.000
1.000
0.571
0.250
0.006
0.241
1950
0.000
-0.077
0.142
0.000
0.833
0.909
0.286
0.305
0.194
1951
0.498
-0.083
-0.043
0.000
0.864
0.476
0.220
0.234
0.163
1952
0.000
0.109
-0.050
0.000
0.244
0.097
0.125
0.004
0.160
1953
0.000
-0.131
-0.016
0.000
0.255
0.065
0.000
0.004
0.000
1954
0.000
-0.226
0.139
0.000
0.266
0.066
-0.333
0.004
0.000
1955
0.008
-0.049
0.000
0.000
0.296
0.057
0.000
0.000
0.000
1956
0.000
-0.051
-0.103
0.000
0.038
0.054
0.000
-0.143
-0.121
1957
0.000
-0.108
0.005
0.000
0.037
-0.977
0.000
-0.167
-0.118
1958
0.000
-0.061
-0.130
0.000
-0.124
5.000
0.000
0.057
-0.133
1959
0.000
-0.129
-0.144
0.212
-0.131
0.833
-0.333
0.054
-0.077
1960
0.000
0.000
-0.175
0.000
0.291
0.455
0.000
0.051
0.000
1961
0.032
0.037
0.017
-0.182
-0.099
0.438
0.275
-0.022
-0.167
1962
-0.102
0.071
0.125
-0.254
-0.150
0.043
0.008
0.369
0.200
1963
-0.114
0.167
0.096
0.000
0.365
0.125
0.000
-0.056
-0.083
1964
-0.089
0.086
0.115
0.270
0.034
0.030
0.058
-0.017
-0.039
1965
-0.108
0.000
0.012
0.000
0.017
0.036
0.018
-0.085
-0.076
1966
0.146
0.368
0.024
0.085
0.574
0.000
0.173
-0.027
0.159
1967
-0.085
-0.019
0.000
0.039
0.026
-0.049
-0.031
-0.112
0.022
1968
-0.077
0.000
0.012
0.000
-0.005
0.000
0.013
0.000
0.000
1969
-0.008
-0.020
-0.012
0.000
-0.036
-0.058
-0.031
-0.135
0.023
1970
0.084
0.080
-0.029
-0.234
0.069
0.062
-0.071
-0.062
-0.004
1971
-0.105
-0.222
-0.096
-0.064
-0.356
-0.270
-0.059
-0.116
-0.044
1972
0.000
-0.167
0.000
0.000
0.000
0.000
0.000
0.125
0.000
1973
0.043
0.086
0.000
0.000
0.000
0.000
0.037
-0.056
0.015
1974
-0.042
0.000
0.000
0.053
0.154
0.000
0.071
0.000
0.029
1975
0.000
0.000
0.000
0.000
-0.067
0.100
0.000
0.000
0.014
1976
0.087
0.237
0.133
-0.100
0.143
-0.091
0.000
0.059
0.020
1977
0.040
-0.043
-0.118
0.000
0.000
0.000
0.033
0.000
0.008
1978
0.038
-0.089
0.000
0.056
0.000
0.100
0.000
0.000
-0.011
1979
-0.037
0.049
0.000
0.000
0.000
-0.091
0.032
0.000
0.005
1980
0.000
-0.047
0.000
0.053
0.013
-0.070
0.000
0.028
-0.025
1981
-0.027
0.049
-0.007
-0.030
0.031
0.086
0.000
0.000
0.028
1982
-0.008
0.000
-0.007
0.015
-0.006
0.050
0.000
-0.022
0.011
1983
-0.044
-0.070
0.095
0.041
0.006
-0.009
0.063
0.050
-0.043
■Example 14.2:
A Normal-Hierarchical Model of Cold War Military Per-
sonnel
These data describe changes in military personnel for seven Warsaw Pact

484
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
countries plus two (Yugoslavia and Albania) during the period from 1948 to 1983,
an interval that covers the height of the Cold War. These are collected by Faber
(1989, ICPSR-9273) for 78 countries total, from which these 9 are taken, and include
covariates for military, social, and economic conditions.
The model is a normal-gamma hierarchy with several levels and somewhat vague nor-
mal and inverse gamma hyperprior speciﬁcations. While there is a good argument
to treat these data as a time-series, we will not do that here in order to produce
illustrative results from a diagnostic perspective. The inverse gamma speciﬁcation
is articulated from the gamma statements through the BUGS convention of specify-
ing precisions instead of variances in normal speciﬁcations. The motivation for the
hierarchy is that the context level recognizes national diﬀerences from economic and
cultural factors, while the primary level recognizes the political inﬂuence that comes
from Warsaw Pact membership as well as the obvious pressure exerted by the USSR.
The (possibly mis-speciﬁed) model is a hierarchy of normals according to:
Yij ∼N(αi + β1i(Xj) + β2i(X2
j ) + β3i cos(Xj), τc)
αi ∼N(αμ, ατ)
αμ ∼N(1, 0.1)
ατ ∼G(1, 0.1)
β1i ∼N(β1,μ, β1,τ)
β1,μ ∼N(0, 0.1)
β1,τ ∼G(1, 0.1)
β2i ∼N(β2,μ, β2,τ)
β2,μ ∼N(0, 0.1)
β2,τ ∼G(1, 0.1)
β3i ∼N(β3,μ, β3,τ)
β3,μ ∼N(0, 0.1)
β3,τ ∼G(1, 0.1)
τc ∼G(1, 0.1),
(14.4)
where the Yij are proportional changes in military personnel for country i at time
period j and Xj is the index of the year (rows in the table). In addition to the linear
treatment of the years, there are squared and cosine terms to account for systematic
ﬂuctuations over time. While the posterior distribution of the linear coeﬃcient, β1 is
of primary interest, these secondary coeﬃcients may reveal additional structures in
the data. The τ terms and normal variances here conform to the BUGS requirement,
mentioned above, of specifying precisions instead of variances.
It should be clear
from the hyperparameters that the prior structure is given proper distributions in
this setup. Interestingly, the WinBUGS examples routinely assign very small parameter
values for the gamma distribution priors (α = 0.001, β = 0.001) on precisions, which
puts prior density on gigantic values for the variance that exceed the typical posterior
ranges for reasonable models. This practice should be replaced with other forms such
as a gamma with α = 1 and small β (Gill 2010) or a (folded) half-t (Gelman 2006).
The JAGS code for this example is given in the Computational Addendum for this
chapter, and the data are given here (starting at 1949 since the variable has been
changed from absolute numbers to annual proportional change). After compilation in
JAGS, 150,000 iterations of the Markov chain are run and discarded. Monitoring is
turned on for all nodes and then an additional 350,000 iterations are run. This Markov

Utilitarian Markov Chain Monte Carlo
485
chain turns out to be relatively slow mixing and there are several lurking problems.
The posterior results are summarized in Table 14.3. Clearly these results are mixed
evidence of the value of the speciﬁcation given in (14.4) since the standard errors
indicate rather diﬀuse posterior forms. Interestingly, the quadratic term is the most
reliable predictor, although it weakens in the absence of the linear and trigonometric
contributions.
TABLE 14.3:
Posterior, Military Personnel
Model
Mean
Std. Error
Median
95% HPD Interval
αμ
0.7404
3.3585
0.9814
[-5.8422 :
7.3229]
ατ
10.0155
9.9068
7.0303
[-9.4019 : 29.4328]
β1,μ
0.0965
0.4740
0.0983
[-0.8326 :
1.0256]
β1,τ
13.3070
10.8181
10.3000
[-7.8965 : 34.5105]
β2,μ
2.0737
0.1031
2.0643
[ 1.8717 :
2.2758]
β2,τ
24.6509
6.9466
23.9347
[11.0357 : 38.2662]
β3,μ
-0.0011
0.0130
-0.0011
[-0.0266 :
0.0243]
β3,τ
180.0586
42.5261
176.7230
[96.7075 : 263.4097]
τc
0.0308
0.0038
0.0307
[ 0.0233 :
0.0383]
14.3.1
Autocorrelation
High correlation between the parameters of a chain tends to produce slow convergence,
whereas high correlation within a single parameter (autocorrelation) chain leads to slow
mixing and possibly individual nonconvergence to the limiting distribution because the
chain will tend to explore less space in ﬁnite time. This is a problem since, of course, all
chains are run in ﬁnite time. Furthermore, these are obviously interrelated problems for
most speciﬁcations.
In addition to the empirical mean and standard deviation of the chain values (the pri-
mary inferential quantities of interest), the output from CODA gives the so-called “na¨ıve”
standard error of the mean (NaiveSE, which is the square root of √sample variance/√n,
and the standard time-series adjusted standard error of the mean (TimeseriesSE), which
is √spectral density var/√n = asymptotic SE. The na¨ıve standard error gives an overly-
optimistic view (smaller) of the posterior dispersion since it ignores serial correlation that
exists by deﬁnition with Markov chains. The time-series standard error is produced from
an estimate of the spectral density at zero using binned chain values.
One method works around the autocorrelation problem by looking at the means of
batches of the parameter. If the batch size is large enough, the batch means should be
approximately uncorrelated and the normal formula for computing the standard error should
work. More speciﬁcally, a quantity of interest from the simulated values, h(θ), is calculated

486
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
empirically from a series i = 1, 2, . . . , n:
ˆh(θ) = 1
n
n

i=1
h(θi)
(14.5)
with the associated Monte Carlo variance:
Var[h(θ)] = 1
n
*
1
n −1
n

i=1
(h(θi) −ˆh(θ))2
,
,
(14.6)
as described in Chapter 9.
The problem with applying this to MCMC samples is that
it ignores the serial nature of their generation. Instead consider speciﬁcally incorporating
autocorrelation in the variance calculation. Deﬁne ﬁrst for a lag t, the autocovariance:
ρt = Cov [h(θi), h(θi−t)] .
(14.7)
Chen et al. (2000, 87) show that the expected variance from MCMC samples, knowing ρt
and σ2 (the true variance of h(θ)) is:
E[σ2] = σ2

1 −
2
n −1
n−1

t=1

1 −t
n

ρt

,
(14.8)
which shows the variance diﬀerence produced by serially dependent iterates.
In practice the lagged autocovariance is measured by:
ˆρnt = 1
n
n

t+1

h(θi) −ˆh(θ)
 
h(θi−t) −ˆh(θ)

(14.9)
(Priestley 1981, p.323).
In analyzing Markov chain autocorrelation, it is helpful to identify speciﬁc lags here in
order to calculate the long-run trends in correlation, and in particular whether they decrease
with increasing lags. Diagnostically, though, it is rarely necessary to look beyond 30 to 50
lags. Recall that for a series of length n, the lag k autocorrelation is the sum of n −k
correlations according to: ρk = n−k
i=1 (xi −¯x)(xi+k −¯x)/ n
i=1(xi −¯x)2.
Fortunately, both BOA and CODA give straightforward diagnostic summaries and graphical
displays of autocorrelation with chains and cross-correlation matrices. The four default
lag values of the correlations for the Tobit model of death penalty attitudes is given in
Table 14.4. Notice that the within-chain correlations decline sharply with increasing lag,
indicating no problem autocorrelation in any of these dimensions of the Markov chain.
However, there are some large cross-correlations that might cause the chain to mix poorly
due to constrained high-density space between two parameters. The graphical diagnostics
will reveal if this is really a problem in mixing.
The model correlation structure of the path of the chain for the Eastern European
military personnel example is summarized in Table 14.5 along with cross-correlations. Un-
fortunately, the picture is not as optimistic as in the last example. The autocorrelations
for αμ, β1,μ, β2,μ, and τc are unambiguously indicative of poor mixing.
It is possible to

Utilitarian Markov Chain Monte Carlo
487
TABLE 14.4:
Correlations and Autocorrelations, Tobit Model
Past
Political
Current
Murder
Intercept
Rates
Culture
Opinion
Ideology
Rate
Within-Chain Correlations
Lag 1
0.205
0.030
0.218
0.295
0.201
0.212
Lag 5
0.003
-0.006
-0.003
0.007
0.001
0.014
Lag 10
0.011
-0.005
-0.007
0.002
0.023
-0.004
Lag 50
-0.0016
-0.0156
-0.0065
-0.0183
0.0049
0.0001
Cross-Chain Correlations
Intercept
1.000
-0.120
0.527
-0.086
-0.975
-0.241
Past Rates
1.000
-0.167
-0.102
0.146
-0.123
Political Culture
1.000
0.022
-0.587
-0.649
Current Opinion
1.000
-0.089
0.139
Ideology
1.000
0.182
Murder Rate
1.000
look at longer lags for these variables, but the lag of 50 is suﬃcient evidence here from
the marginals to show that the overall chain is not mixing well. The other variables show
Markov chain autocorrelations that are very typical of fair to good mixing. Interestingly,
although some within-chain correlations are quite high, there is little evidence that corre-
lation between chains is slowing down the mixing. Only β2,μ and τc appear to be highly
(negatively) correlated. One of the main weapons for dealing with high autocorrelations is
reparameterization as described in Section 14.4.1.
14.3.2
Graphical Diagnostics
Graphical diagnostics can be very useful in evaluating mixing and convergence of the
chain. While they are not a formal test, as we will do in Section 14.3.3, they often show
stochastic properties of importance.
Figures 14.1 and 14.2 simultaneously provide two
common visual diagnostics: traceplots of the path of the Gibbs sampler runs (with the
burn-in period omitted to make the scale more readable), and histograms for the chains
over this same period on the same vertical scale. Traceplots merely follow the path of the
Markov chain over time on the x-axis, giving the consecutive mixing through the support
of the posterior space on the y-axis. Because “time” is accounted for moving left to right,
we can see the properties of the mixing of the chain.
In every case, the traceplots for the death penalty analysis show ideal properties: free
travel up and down through the space and a ﬂat trend across the window. This “fuzzy
caterpillar” pattern is the desired outcome from looking at traceplots because it implies
(not proves!) that the Markov chain is in its stationary distribution and is exploring it

488
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
TABLE 14.5:
Correlations and Autocorrelations, Military
Personnel Model
αμ
ατ
β1,μ
β1,τ
β2,μ
β2,τ
β3,μ
β3,τ
τc
Within-Chain Correlations
Lag 1
0.998
0.896
0.981
0.871
0.883
0.304
-0.001
0.023
0.587
Lag 5
0.998
0.622
0.911
0.537
0.883
0.284
-0.001
0.003
0.534
Lag 10
0.991
0.441
0.836
0.323
0.882
0.284
-0.001
-0.004
0.533
Lag 50
0.962
0.075
0.444
0.009
0.881
0.283
0.002
-0.002
0.531
Cross-Chain Correlations
αμ
1.000
0.020
-0.034
0.003
0.201
-0.023
-0.003
0.000
-0.184
ατ
1.000
0.003
0.006
0.017
0.004
0.006
-0.002
-0.012
β1,μ
1.000
-0.008
0.024
-0.007
0.004
-0.002
-0.016
β1,τ
1.000
-0.017
0.007
0.003
0.002
0.024
β2,μ
1.000
-0.100
-0.006
0.003
-0.682
β2,τ
1.000
0.003
0.001
0.097
β3,μ
1.000
0.005
0.002
β3,τ
1.000
-0.002
τc
1.000
fully. The histograms show a strong central limit theorem eﬀect from the ergodicity of the
Markov chain. This complete graphical eﬀect from Figure 14.1 is the desired result.
The graphics contained in Figure 14.2 for the Military Personnel model are not as promis-
ing. The ﬁrst traceplot shows a pattern called “snaking” for αmu, indicating poor mixing
since the chain does not move freely through the sample space. However, the traceplots for
β1,μ, β3,μ, and β3,τ show a non-trending (ﬂat trajectory) pattern where the chain is also
making liberally wide moves through the sample space. Two variance components, ατ and
β1,τ are somewhat long-tailed as evidenced by both the traceplots and the histograms. This
suggests potential, but not certain, issues with the model speciﬁcation. It is obvious that
the problematic dimensions are those for β2,μ, and τc, where strong evidence of trending
exists. This indicates that the chains are likely not to be in their stationary distribution
since the time period is quite long (150,000 iterations).
One general problem with traceplots is that if the chain remains attracted to a nonop-
timal mode for a long period of time, there is no visual indication that this area is not the
desired high-density region. The standard solutions are to extend the run for a very long
time since eventually it will leave (e.g., Geweke 1992), and to start the Markov chain from
multiple widely dispersed starting points (e.g., Gelman and Rubin 1992a). Also, due to
a feature in WinBUGS that keeps a dynamic running traceplot going in a separate window,
it is easy to use this as a default convergence assessor. Yet WinBUGS automatically resizes
the Y -axis as the chain runs for this graphic, giving an inattentive viewer the illusion of
stability. So the “History” option is superior for real traceplot diagnostic purposes in the
WinBUGS environment.

Utilitarian Markov Chain Monte Carlo
489
FIGURE 14.1: Traceplot and Histogram Graphics, Tobit Model
One important warning about traceplots is warranted here. It is common practice to
run multiple parallel chains from diﬀerent and dispersed starting points as a means of
assessing convergence (Section 14.3.3.2). However, graphing traceplots for the full span of
these chains can be very misleading because the starting, or early, values are quite far from
the region where the chain will eventually settle. So the scale of the graph hides features
of the chain inside a densely plotted and narrow band of line segments. As an example,
Figure 14.3 shows a single dimension of the same Markov chain where the ﬁrst panel covers
iterations 1 through 50,000 and the second panel shows the last 500 iterations only. The
longer view implies a great deal of stability and good mixing as well because the scale of
the y-axis covers the large distance between the starting point at 20 and the stable region
centered around 0.272. The plot of the last 500 iterations shows that there is considerable
amount of snaking going on within the dense view on the left showing that the mixing is not
very eﬃcient. Unfortunately the longer view with distant starting points and small, dense

490
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
FIGURE 14.2:
Traceplot and Histogram Graphics, Military Personnel
Model
stable regions can be found in published analyses and are sometimes used by researchers as
a conﬁrmation of stability and mixing.
One very useful diagnostic is the running mean graph, which, as its name implies, gives
the mean across previous values running along the ﬁgure from left to right. Thus a stable
chain ﬂuctuates in its early period due to a small denominator and then ends up as a ﬂat
line. This is not iron-clad evidence of convergence because a chain may just be lingering
for a long period of time at some sub-optimal attraction, but it is strong evidence given
suﬃciently lengthy evaluation periods. Figure 14.4 gives the running means for the last

Utilitarian Markov Chain Monte Carlo
491
0
100
200
300
400
500
−
FIGURE 14.3: Traceplots of the Same Chain with Different Spans
10, 000 values for the death penalty support model, and supports the claim of convergence
for each parameter in the boxes on the left. Notice that each chain settles down to a stable
horizontal trend.
The right side gives kernel density estimates of the chain values and
can be considered a smoothed version of the histograms in Figure 14.1. Underneath each
density plot is a 95% HPD interval for the last half of the analyzed chain values for this
plot. The values of 0.95 for the HPD interval and 0.5 for the window width can be changed
in the function mean.kernel.mcmc to other user-selected values. This function graphical
and the function for traceplots with histograms are both included in the BaM package in R
that accompanies this book.
The idea behind an interval summary of a subset of the values used to make the density
plot is that trending in the Markov chain should cause this interval to be not centered in the
plot (skewed distributions will still have equal tails). These intervals in Figure 14.4 show
no sign of problems.
The left-hand side of Figure 14.5 gives the same running mean diagnostic seen before
with 150, 000 simulated values after the burn-in period for the Military Personnel model.
The graph is designed to contrast the two views: long-term stability and distributional
summary. In particular β2,μ, β,2τ, and τc look untrustworthy in the ﬁrst column of the
ﬁgure since they appear not to have settled into a stable running mean. This is interesting
since the Markov chain for β2,τ appeared to be reasonably stable in the latter part of the
traceplot. Robert and Mengersen (1999) use a similar diagnostic to show poor convergence
properties in a normal mixture model.

492
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
−15.5
−14.5
−13.5
−12.5
Constant
164 166 168 170 172
Past Rates
0.30
0.35
0.40
0.45
Political Culture
4.0
4.2
4.4
4.6
4.8
Current Opinion
2.6
2.8
3.0
3.2
Ideology
0
2000
4000
6000
8000
10000
−0.04
−0.02
0.00
0.02
Murder Rate
FIGURE 14.4: Running Mean Diagnostic, Tobit Model
There are a number of other graphical techniques that often give useful diagnostic in-
formation. Gelman, Gilks, and Roberts (1996) give the theoretical result that Metropolis-
Hastings algorithms with a low acceptance rate will be less stable and poor at mixing
through the distribution due to this ineﬃciency. Therefore a serial plot of the acceptance
rate over time can be helpful in showing the performance of the chain.
Other graphical methods exist. Robert (1997) introduces allocation maps as a way of
assessing convergence in mixture models. The idea is to represent each component with a
diﬀerent level of gray. Sudden and unambiguous shifts in color across increasing iterations
is evidence of nonconvergence. Cui et al. (1992) and Tanner (1996, p.154) use a simple
version of this idea on the θ parameter of a “witch’s hat” distribution (see the discussions
in Chapter 15) to show a sudden shift in chain behavior.

Utilitarian Markov Chain Monte Carlo
493
−4
−2
0
2
alpha.mu
6
8 10
14
alpha.tau
−0.8
−0.4
0.0
beta1.mu
4
6
8
12
beta1.tau
2.10
2.20
2.30
beta2.mu
22
26
30
beta2.tau
−0.004
0.002
beta3.mu
180
220
260
beta3.tau
0
50000
100000
150000
0.024
0.027
0.030
tau.c
FIGURE 14.5: Running Mean Diagnostic, Military Personnel Model
14.3.3
Standard Empirical Diagnostics
This section ﬁrst reviews in detail the four empirical diagnostics used most commonly in
practice, giving results for our two running models of interest. These diagnostics, Geweke,
Gelman-Rubin, Raftery-Lewis, and Heidelberger-Welch (all named after their creators),
dominate because they are provided in easy-to-use menu form in the R packages BOA and
CODA. Subsequent discussion, though, will present some interesting and useful alternatives
that require user-coding.

494
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
14.3.3.1
The Geweke Time-Series Diagnostic
Geweke (1992) develops a test based on comparing some proportion of the early era of
the chain after the burn-in period with some nonoverlapping proportion of the late era of
the chain. He proposes a diﬀerence of means test using an asymptotic approximation for the
standard error of the diﬀerence (although other summary statistics could be compared if
desired). Since the test statistic is asymptotically standard normal, values that are atypical
of a standard normal distribution provide evidence that the two selected portions of the
chain diﬀer reasonably (in the ﬁrst moment), and one then concludes that the chain has
not converged.
Preselect two nonoverlapping window proportions, one early in the chain and one later
in the chain: θ1 of length n1 and θ2 of length n2 along with some function of interest g().
Then Geweke’s diagnostic is given by:
G = ¯g(θ1) −¯g(θ2)
/
s1(0)
n1
+ s2(0)
n2
.
(14.10)
Typically, the function choice is the mean of the chain in each window: ¯g(θ1) = n1
i=1 g(θi)/n1,
¯g(θ2) = n2
i=1 g(θi)/n2, and s1(0) and s2(0) are the symmetric spectral density functions
(see: Chatﬁeld 2003, Section 6.2, or Beckett and Diaconis 1994), deﬁned for the time-series
conﬁned within these windows, provided the assumption that there are no discontinuities at
the frequency 0. The spectral density function in a given window can be normalized to give
a Fourier transform of the autocovariance function and essentially provides a way of think-
ing about the uncorrelated contribution from the individual values to the total variance (see
Granger and Hatanaka [1964] or Priestley [1981] for the missing technicalities).
Geweke suggests using the ratios n1/n = 0.1 and n2/n = 0.5, occupying the 0.0 to 0.1
and 0.5 to 1.0 quantiles of the series, respectively, and if these proportions are held ﬁxed
as the chain grows in length, then the central limit theorem applies and G converges in
distribution to standard normal under the null hypothesis of convergence of the chain. The
utility is therefore that we now have a nonarbitrary method for asserting that the means
of the functions of the values diﬀer statistically. It is customary to appeal to basic normal
theory and worry about values of G that are greater in absolute value than 2.
This is an overtly time-series manner of proceeding and reﬂects Geweke’s belief that more
can be learned by one very long chain since it will end up exploring areas where humans
might not think to send it. Unfortunately the window proportions can greatly aﬀect the
value of G, and it is therefore important not to just rely on Geweke’s recommendation
(0.1/0.5), which are the default values in the diagnostic suites BOA and CODA.
■Example 14.3:
Geweke Diagnostic, Tobit Model. We ﬁrst perform the Geweke
diagnostic for the Tobit model described above using BOA. In order to be cautious,
the default settings are used and compared with windows of 20% on either side of the
middle point (a more cynical approach since it includes a larger early window and
smaller later window). These results are given in Table 14.6.

Utilitarian Markov Chain Monte Carlo
495
TABLE 14.6:
Geweke Diagnostic, Tobit Model
0.1 vs. 0.5
0.2 vs. 0.2
Z-score
p-value
Z-score
p-value
Intercept
0.1901
0.4246
-0.6632
0.2536
Past Rates
0.4342
0.3321
0.5781
0.2816
Political Culture
0.0915
0.4636
-0.4210
0.3369
Current Opinion
1.0500
0.1436
0.8934
0.1858
Ideology
-0.7092
0.2391
0.4351
0.3317
Murder Rate
1.1230
0.1307
0.5769
0.2820
The output here provides little to worry about. No values are observed to be in the
tail of the normal distribution at standard levels (although such points are arbitrary
anyway), and so there is no evidence provided here to indicate nonconvergence. The
diagnostic for Current Opinion shows a little bit of extra-variability, but not enough
to cause alarm.
■Example 14.4:
Geweke Diagnostic, Military Personnel Model. Using the de-
fault window widths (other choices were shown to produce similar results), CODA re-
turns the Geweke diagnostics here in Table 14.7. The Geweke diagnostic conﬁrms our
previous skepticism about αμ, ατ, β2,μ, β2,τ, β3,μ, and τc. The corresponding p-values
are well into the tails of the standard normal for both window conﬁgurations. The
similarity comes from the large sample size involved in both analyses.
TABLE 14.7:
Geweke Diagnostic, Military
Personnel Model
0.1 vs. 0.5
0.2 vs. 0.2
Z-score
p-value
Z-score
p-value
αμ
3.9268
0.0001
3.9261
0.0000
ατ
0.4336
0.3323
0.7866
0.2158
β1,μ
0.2288
0.4095
0.2202
0.4129
β1,τ
-2.1575
0.0144
-1.1873
0.1176
β2,μ
44.0073
0.0001
53.2793
0.0001
β2,τ
-6.1943
0.0001
-3.9272
0.0001
β3,μ
-2.6983
0.0035
-2.7704
0.0028
β3,τ
0.5392
0.2949
0.9035
0.1831
τc
-35.9739
0.0001
-47.9269
0.0001

496
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
14.3.3.2
Gelman and Rubin’s Multiple Sequence Diagnostic
Gelman and Rubin’s (1992a) convergence diagnostic is based on comparing a set of
chains with diﬀerent starting points that are overdispersed relative to the target distribution
(greater variability than the presumed limiting distribution).
The method is based on
normal theory approximations to the marginal posteriors using an ANOVA-based test with
a normal or a Student’s-t distribution-based diagnostic. Thus convergence decisions can be
made inferentially.
The test is run according to the following steps for each scalar parameter of interest (θ):
1. Run m ≥2 chains of length 2n from overdispersed starting points with subscripts
denoting distinct chains, (1), (2), . . . , (m):
θ[0]
(1),
θ[1]
(1),
. . .
θ[n]
(1),
. . .
θ[2n−1]
(1)
,
θ[2n]
(1)
θ[0]
(2),
θ[1]
(2),
. . .
θ[n]
(1),
. . .
θ[2n−1]
(2)
,
θ[2n]
(2)
...
θ[0]
(m),
θ[1]
(m),
. . .
θ[n]
(1),
. . .
θ[2n−1]
(m)
,
θ[2n]
(m).
(14.11)
The starting points can be determined by overdispersing around suspected or known
modes. Discard the ﬁrst n chain iterations.
2. For each parameter of interest calculate the following:
▷Within chain variance:
W =
1
m(n −1)
m

j=1
n

i=1
(θ[i]
(j) −¯θ(j))2
(14.12)
where ¯θ(j) is the mean of the n values for the jth chain.
▷Between chain variance:
B =
n
m −1
m

j=1
(¯θ(j) −¯¯θ)2
(14.13)
where ¯¯θ is the grand mean (i.e. the mean of means since each subchain is of
equal length).
▷Estimated variance:
6
Var(θ) = (1 −1/n)W + (1/n)B.
(14.14)
3. The convergence diagnostic is a single value for each dimension, called the potential
scale reduction factor (or shrink factor):
=R =
>
6
Var(θ)
W
.
(14.15)

Utilitarian Markov Chain Monte Carlo
497
Along with this comes an upper conﬁdence interval bound. The CODA output also pro-
vides an omnibus single scalar version called the multivariate potential scale reduction
factor.
4. Values of =R near 1 are evidence that the m chains are all operating on the same
distribution (in practice values less than roughly 1.1 or 1.2 are acceptable according
to Gelman [1996]).
The logic behind this test is quite simple. Before convergence, W underestimates total
posterior variation in θ because the chains have not fully explored the target distribution
and W is therefore based on smaller diﬀerences. Conversely, 6
Var(θ) overestimates total
posterior variance because the starting points are intentionally overdispersed relative to the
target. However, once the chains have converged, the diﬀerence should be incidental since
the chains are exploring the same region and are therefore overlapping.
Commonly, the number of separate chains is about 5 to 10, but more complicated model
speciﬁcations and more posterior complexity may require the speciﬁcation of additional
starting points. Note that the test here is described for only one dimension and in practice
each of the m chains starts from a point in ℜk for a k-length vector of estimation parameters
θ. Thus dimensionality can considerably add to the work involved in selecting overdispersed
starting points.
Practically, it is very easy to monitor convergence by periodically checking the value of
=R as the Markov chain runs (WinBUGS readily supplies a slightly diﬀerent version of this).
Therefore we can run the chain until satisﬁed with the statistic and then treat the most
recent n values as empirical values from the desired target distribution.
The primary diﬃculty with the Gelman and Rubin diagnostic (noted by many observers)
is that it is not always easy to obtain suitably overdispersed starting points, since determin-
ing their position requires some knowledge of the target distribution to begin with. Geyer
(1992) points out that the determination of good starting points is actually essential in the
Gelman and Rubin diagnostic because of the underdispersed/overdispersed distributional
contrast. Cowles and Carlin (1996) worry about the heavy reliance on normal approxi-
mations, and it is obvious that substantial deviations from normality make the choice of
starting values much more diﬃcult (somewhat mitigated by the Brooks-Gelman modiﬁ-
cation [1998a, 1998b] which uses the more defensible Student’s-t distribution assumption).
Also, Brooks and Giudici (2000) give an extended split-variance reﬁnement of this diagnostic
focused on model choice, including reversible jump MCMC (see Section 15.2 in Chapter 15
starting on page 536).
In addressing the starting point issue, Gelman and Rubin (1992a) advise the use of the
EM algorithm to ﬁnd modes and caution that EM should be started from several diﬀerent
points if multimodality is suspected (1992a, 459).
In practice, it is not always easy to
write a customized EM algorithm for a reasonably intricate model. With the case of known
multimodality Gelman and Rubin advise using a more complicated procedure that involves

498
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
constructing a mixture of normals centered at the found modes, and “sharpened” with
importance sampling as a means of determining the scale of overdispersion required.
■Example 14.5:
The Gelman and Rubin Diagnostic for the Tobit Model. Re-
turning to the Tobit model, we perform the Gelman and Rubin diagnostic with ﬁve
separate chain paths with starting points perturbed oﬀof the MLE from a regular
GLM to overdisperse. The draw for the zi values precluded dramatically overdis-
persed starting points in that dimension since the random truncated normal gener-
ator becomes extremely ineﬃcient for highly unlikely parameter values relative to
the associated data. Each chain is again run for 50, 000 iterations at these starting
points with the ﬁrst 40, 000 iterations discarded in all cases. The CODA diagnostic
suite, with a normal distribution assumption, is used to produce the following results,
which show no evidence of nonconvergence (Table 14.8). Table 14.8 also gives the
multivariate potential scale reduction factor from CODA output. The MPSRF is an
additional statistic from Brooks and Gelman (1998b) that seeks to give a fully mul-
tivariate view of chain convergence using the full variance-covariance matrix of the
output and therefore providing a more conservative view. This is particularly use-
ful for high-dimensional models as a convenient omnibus test, although Brooks and
Gelman suggest looking at the individual PSRF statistics for important parameters.
TABLE 14.8:
Gelman and Rubin Diagnostic, Tobit Model
Past
Political
Current
Murder
Intercept
Rates
Culture
Opinion
Ideology
Rate
PSRF
1.01
1.00
1.02
1.01
1.01
1.02
95% Upper CI
1.03
1.02
1.05
1.05
1.02
1.07
Multivariate Potential Scale Reduction Factor: 1.03
■Example 14.6:
The Gelman and Rubin Diagnostic for the Military Person-
nel Model. Again ﬁve separate Markov chains are run using overdispersed starting
points. These were determined by constructing a wide grid along the sample space of
each of the ﬁve parameters. Then the joint posterior is applied systematically to go
through the dimensions to look for modes. This can be a very time-consuming process
if the grid is reasonably granular, and it is often possible to get away with fairly large
bins.
We now run the Gelman and Rubin procedure on the model of Military Personnel using
BOA defaults for the ﬁve chains, including the convention of using only the second half
of the recorded change values. Overdispersed starting points were determined from
the posterior means and HPD intervals in Table 14.3 on a marginal basis. The results
are given in Table 14.9.

Utilitarian Markov Chain Monte Carlo
499
TABLE 14.9:
Gelman and Rubin Diagnostic, Military Personnel
Model
αμ
ατ
β1,μ
β1,τ
β2,μ
β2,τ
β3,μ
β3,τ
τc
PSRF
1.00
1.00
2.28
2.01
25.32
2.49
1.04
4.70
1.36
95% Upper CI
1.01
1.00
3.58
10.11
128.56
4.13
1.04
15.49
1.82
Multivariate Potential Scale Reduction Factor: 21.7
Here all but three of the =R statistics in the ﬁrst row of the table give cause for concern.
Only αμ, ατ, and β3,τ provide PSRF values that we would be comfortable with. Note
also that the PSRF value for β2,μ is enormous. The second line of the table provides
the upper 95% credible interval for the statistics and shows large tails for the three
β parameter variance componentsβ1,τ, β2,τ, and β3,τ.
All of this provides strong
evidence of overall nonconvergence of the Markov chain. Markov chains with Gibbs
or Metropolis kernels do not converge partially by dimension (Gill 2010). In addition,
the Brooks and Gelman (1998a, 1998b) correction also gives a multivariate summary
for the whole model, which is 21.7 in this case and is clearly dominated by the result
for β2,μ.
14.3.3.3
The Heidelberger and Welch Diagnostic
This diagnostic works in its implemented form for individual parameters in single chains.
It is easy to code on one’s own and conveniently supplied in BOA and CODA. Heidelberger
and Welch (1983) originally developed the algorithm in another context for simulations
in operations research and based on Brownian bridge theory, using the Cram´er-von Mises
statistic (Heidelberger and Welch 1981a, 1981b; Schruben 1982; and Schruben, Singh, and
Tierney 1983).1
Like the Geweke diagnostic, it has an inherently time-series orientation and uses the
spectral density estimate. The null hypothesis for this test is that the chain is currently in
the stationary distribution and the test starts with the full set of iterations produced from
running the sampler from time zero. The general form for steps are then as follows.
1. Specify: a number of iterations (N, generally given by the current status of the chain
at the proposed stopping point), an accuracy (ϵ), and an alpha level for the test.
2. Evaluate the ﬁrst 10% of the chain iterations.
3. Calculate the test statistic on these samples and observe whether it indicates a tail
value.
1A Brownian Bridge is a Wiener process (also called Brownian motion, Priestley [1981, pp.167-8]) over
the interval [0, 1], in which the starting and stopping points are tied to the endpoints 0 and 1 and the
“string” in between varies.
The details are beyond the scope of this work and the interested reader is
directed to Chu and White (1992), Lee (1998), as well as Nevzorov and Zhukova (1996).

500
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
4. If the test rejects the null, the ﬁrst 10% of the iterations are discarded and the test is
run again.
5. This process continues until either 50% of the data have been dismissed or the test
fails to reject the null with the remaining iterations.
If some proportion of the data are found to be consistent with stationarity, then the halfwidth
analysis part of the diagnostic is performed.
The process is actually a variant of the Kolmogorov-Smirnov nonparametric test for large
sample sizes referred to as the Cram´er-von Mises test after the form of the test statistic. A
parameter, s, is deﬁned as the proportion of the continuing sum of the chain, ranging from
zero to one. We are interested in the diﬀerence between the sum of the ﬁrst sT of the chain
(where T is the total length of the chain), and the mean of the complete set of chain values
scaled by sT . If the chain were in its stationary distribution, then this diﬀerence would
be negligible. For the disparity between empirical and theoretical CDFs, the Cram´er-von
Mises test statistic is
Cn(F) =

X
[Fn(x) −F(x)]2dF(x),
(14.16)
with a predeﬁned rejection region Cn(F) > c, established by normal tail values under
asymptotic assumptions and the hypothesis that Fn(x) ̸= F(x).
There are some additional aspects that must be deﬁned to get from the Cram´er-von
Mises to the Heidelberger and Welch diagnostic. Deﬁne formally the following quantities:
▷T = the total length of the “accepted” chain after discards.
▷s ∈[0:1] = the test chain proportion.
▷T⌊sT ⌋= ⌊sT ⌋
i=1 θi = the sum of the chain values from one to the integer value just
below sT .
▷⌊sT ⌋¯θ = the chain mean times the integer value just below sT .
▷s(0) = the spectral density of the chain.
The notation ⌊X⌋denotes the integer value of X (the “ﬂoor” function). Using these pro-
duced quantities, for any given s, we can construct the test statistic:
BT (s) = T⌊sT ⌋−⌊sT ⌋¯θ

T s(0)
,
(14.17)
which is the Cram´er-von Mises test statistic for sums as cumulative values scaled by the
spectral density. Now BT (s) can be treated as an approximate Brownian bridge and tested
using normal tail values to decide on the 10% discards (where some adjustments are neces-
sary because s(0) is estimated rather than known).
The halfwidth part of the test compares two quantities:
▷Using the proportion of the data not discarded, the halfwidth of the (1 −α)% cred-
ible interval is calculated around the sample mean, where the estimated asymptotic
standard error is the square root of spectral density divided by the remaining sample
size, s(0)/n∗.

Utilitarian Markov Chain Monte Carlo
501
▷If the mean divided by the halfwidth is lower than ϵ, then the halfwidth test is passed
for this dimension.
▷If this test fails, then longer runs are required to accurately produce an empirical
estimate of the mean and assert convergence.
It should be remembered that the quality of this second part of the diagnostic is completely
dependent on the ability of the ﬁrst to produce a subchain in the true limiting distribution,
and if it has settled for some time on a local maxima, then the halfwidth analysis can
be wrong. Finally, it should be noted that most practitioners pay attention only to the
Cram´er-von Mises test statistic part of the test since it has an easy interpretation like the
Geweke statistic.
■Example 14.7:
Heidelberger and Welch Diagnostic for the Tobit Model.
Running the H-W diagnostic at ϵ = 0.1, α = 0.05 in BOA produces:
Stationarity
Test
Keep Discard C-von-M Halfwidth
Mean Halfwidth
Ratio
Intercept
passed 10000
0
0.2288
passed -14.5451
0.0971 -0.006
Past Rates
passed 10000
0
0.1926
passed 171.1460
0.1661
0.001
Political Culture passed 10000
0
0.0397
passed
0.3461
0.0038
0.011
Current Opinion
passed 10000
0
0.2211
passed
3.9738
0.0274
0.007
Ideology
passed 10000
0
0.1592
passed
3.1423
0.0299
0.009
Murder Rate
passed 10000
0
0.0511
failed
0.0088
0.0020
0.227
Notice that all of the chain values passed the Cram´er-von Mises test component, which
is not surprising given the previously observed health of this chain run and the fact
that we have already discarded 40, 000 values to begin the chain. However, on the
halfwidth test, the dimension for Murder Rate fails since the ratio of the halfwidth to
the mean is 0.227, which is the only one greater than ϵ = 0.1. Recall that this variable
was the only one found to be statistically unreliable in the model given in Table 14.1
(irrespective of the unimportant constant). This is worth further investigation. The
individual autocorrelations revealed no obvious stickiness problems:
Lag 1
Lag 5
Lag 10
Lag 50
Murder Rate
0.212
0.014
-0.004
0.0001
but there was evidence that this parameter is strongly correlated with Political
Culture:
Past.Rates
Political Culture
Current Opinion
Ideology
-0.123
-0.649
0.139
0.182
(which should explain some of the mixing). Recall, however, that slow mixing is not
a serious problem if the full sample space is still fully explored. The wealth of other
diagnostics certainly point us in this direction.
Figure 14.6 provides a traceplot and histogram of the last 2, 000 iterations of the

502
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
marginal chain values for Murder Rate with the mean and 95% credible intervals
indicated in each. Both ﬁgures support stability of the chain over this last period.
The traceplot shows some minor snaking, possibly indicating mixing at a pace slower
than the other dimensions, but actually nothing even slightly alarming is revealed
about convergence. The chain is stable around its current mean, balanced between the
interval ends, and the histogram shows strong evidence of the central limit theorem.
−0.3
−0.2
−0.1
0.0
0.1
0.2
0.3
Iterations
Parameter Space
8000
8500
9000
9500
10000
Support
−0.3
−0.2
−0.1
0.0
0.1
0.2
0.3
FIGURE 14.6: Traceplot and Histogram, Last 2,000 Iterations
■Example 14.8:
The Heidelberger and Welch Diagnostic for the Military Per-
sonnel Model. Using the standard defaults in BOA (again at the default parameter
values of ϵ = 0.1, α = 0.05), the H-W test gives the following output:
Stationarity
Test
Keep
Discard C-von-M
Halfwidth
Mean
Halfwidth
Ratio
alpha.mu
passed
150000
0
0.3162
failed
0.7403
0.8404
1.1352
alpha.tau
passed
150000
0
0.2236
passed
10.0155
0.2672
0.0267
beta1.mu
passed
150000
0
0.1825
failed
0.0965
0.0238
0.2466
beta1.tau
passed
150000
0
0.2458
passed
13.3070
0.2432
0.0183
beta2.mu
failed
60000
90000
6.4482
passed
1.9996
0.0114
0.0057
beta2.tau
failed
60000
90000
2.5148
passed
25.9008
0.7142
0.0276
beta3.mu
passed
120000
30000
0.3397
passed
-0.0011
0.0001
-0.0909
beta3.tau
passed
150000
0
0.1642
passed
180.0586
0.1941
0.0011
tau.c
failed
60000
90000
6.0374
passed
0.0330
0.0004
0.0121
Obviously the results show poor convergence properties according to this diagnostic.
The ﬁrst test is the stationary test from the Brownian bridge approximation, and three
marginal results show a failure to obtain acceptable Cram´er-von Mises test statistics
for discarding less than 50% of the data in 15, 000 increments. The associated large
values of C-von-M fall into the tail region past a stipulated critical value.
In the
second stage of the diagnostic we can ignore the results for β2,μ, β2,τ, and τc, since

Utilitarian Markov Chain Monte Carlo
503
they have already failed. Now notice that αμ and β1,μ fail here with a small value
for the halfwidth, which gives a subsequent ratio of the halfwidth to the mean that
exceeds ϵ = 0.1.
14.3.3.4
The Raftery and Lewis Integrated Diagnostic
The diagnostic proposed and developed in Raftery and Banﬁeld (1991), Raftery and
Lewis (1992), and summarized in Raftery and Lewis (1996) provides a rough indication
of convergence for a running chain. It was originally designed to elicit desired thresholds
from the user and then estimate the chain length that provides a satisfactory result. Con-
sequently, the Raftery and Lewis diagnostic addresses the burn-in period, thinning of the
chain, the posterior quantities of interest, and probabilistic reliability, as well as the number
of iterations.
For a single given parameter of interest, the steps are given as follows:
1. Select the posterior quantile of interest, q, such as a tail value or some substantively
interesting point:
q = p(f(θ) ≤f(θq)|X).
(14.18)
2. Select an acceptable tolerance for this quantile, r, and the desired probability of being
within that tolerance, s. For example (Raftery and Lewis 1996), suppose we want to
report a 95% credible interval around the median with reliability between 92.5% and
97.5%. This is equivalent to the assignments: q = 0.5, r = 0.0125, and s = 0.95.
3. Select a convergence tolerance value ϵ, which is used to determine a stopping point
based on a parallel Markov chain process (defaulted to 0.001).
4. Run a “pilot” sampler whose length is determined as if there is no autocorrelation in
the parallel chain, given by rounding:
npilot =

Φ−1
s + 1
2
 
q(1 −q)
r
2
,
(14.19)
where Φ−1() denotes the inverse of the normal CDF. For the example above at the
defaults, npilot = 2294.
5. The program (BOA, CODA, or mcgibbsit) will then return: the length of the burn-in
period (M), the estimated number of post burn-in iterations required to meet the goals
(N), and the thinning interval (k). The user then runs a Markov chain according to
these parameters.
The method is based on setting up a parallel (but not Markov) chain during the pilot run,
where the iterations are a binary series according to whether the generated value in the
primary chain at that stage is less than the chosen quantile. Raftery and Lewis extrapolate
values that give this secondary chain of zeros and ones some Markovian properties, and are

504
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
then able to estimate ﬁrst-chain probabilities that satisfy the user constraints (see Raftery
and Lewis [1996, pp.116-118] for speciﬁcs). A nice feature of this process is that the ﬁnal
run of N iterations can be treated as a new pilot run, so that if the diagnostic returns a
new recommended N, it would be motivation for additional sampling.
Unfortunately this process must be individually repeated for every quantile of every
parameter of interest. Also, the number of iterations required (N) is diﬀerent for diﬀerent
quantiles picked (in the example above, the number of required iterations would be only 436
here if q = 0.95 were of interest instead of q = 0.5). Since the process is inherently iterative,
picking quantiles and monitoring pilot runs, then the process can be quite time-consuming
(in the human sense) for large numbers of parameters.
Robert and Casella (1999, First Edition) list a number of more serious problems with
the Raftery and Lewis diagnostic: the inherently single-dimension nature does not account
for potential correlations between coeﬃcients (and therefore provides evidence of collective
marginal convergence rather than joint convergence, a point also mentioned by Brooks and
Roberts [1997]), the dependence on the representativeness of the pilot sample, and suspect
nature of the Markov approximation in the secondary chain; see Guihenneuc-Joyaux and
Robert (1998) for a more rigorous but complex approach based on a divergence evaluation
of two chains (Section 4.4). Brooks and Roberts (1998) also warn against widespread use
of the default q = 0.025 setting since it can lead to substantial underestimation of the
burn-in period. Nonetheless, the Raftery and Lewis diagnostic often works quite well with
simple models and can give a general approximation of the scope of the number of iterations
required in more elaborate speciﬁcations. In addition, its inclusion in BOA and CODA mean
that many practitioners will make use of it regardless of any warnings that might appear
in the general statistics literature.
■Example 14.9:
The Raftery and Lewis Diagnostic for the Tobit Model. Run-
ning the Raftery and Lewis diagnostic at BOA defaults, q = 0.025, r = ±0.005, s = 0.95,
produces:
Burn-in
Total
Lower Bound
Dependence
(M)
(N)
(Nmin)
Factor (I)
Intercept
3
4061
3746
1.08
Past Rates
3
4061
3746
1.08
Political Culture
3
4129
3746
1.10
Current Opinion
2
3802
3746
1.01
Ideology
3
4028
3746
1.08
Murder Rate
3
4061
3746
1.08
where we should not be alarmed by anything. The low burn-in numbers result from
starting the diagnostic after burn-in 40, 000 iterations. Note that BOA gives a thinning
value in the summary whereas CODA does not.
The Lower Bound is the npilot estimate of the number of iterations necessary to satisfy
r and s under the assumption of independent chain values, which is less than 100,000

Utilitarian Markov Chain Monte Carlo
505
so no warning was given. Burn-in is the estimated m, and Thin is the k value used
in the calculations. Total is the estimated number of iterations required N. The last
column gives the “dependence factor”:
Isd =
M
npilot
,
(14.20)
which is interpreted by Raftery and Lewis (1992) as the proportional increase in the
number of iterations attributable to serial dependence (the version of Isd implemented
in BOA is slightly diﬀerent than that given by Raftery and Lewis). Their prescriptive
advice is to worry about dependence factors exceeding 5, as it might be due to an
inﬂuential starting value, high correlations between coeﬃcients, or poor mixing.
■Example 14.10:
The Raftery and Lewis Diagnostic for the Military Person-
nel Model. We now run the integrated diagnostic with the values with q = 0.05,
r = ±0.005, and s = 0.95 for the Military Personnel example. The routine in BOA
gives the output:
Burn-in
Total
Lower Bound
Dependence
(M)
(N)
(Nmin)
Factor (I)
alpha.mu
864
889248
3746
237.00
alpha.tau
336
341740
3746
91.20
beta1.mu
168
178500
3746
47.70
beta1.tau
40
44616
3746
11.90
beta2.mu
608
701176
3746
187.00
beta2.tau
24
30752
3746
8.21
beta3.mu
2
3817
3746
1.02
beta3.tau
2
3819
3746
1.02
tau.c
124
146568
3746
39.10
No pilot run was necessary since 150,000 iterations were performed prior to running the
diagnostic in R using BOA (if this was an insuﬃcient period, BOA informs the user). In
this second example, we should be worried about all but two of the dependence factors
(beta3.mu and beta3.tau), given the length of the chain and the correlations given in
Table 14.5. While this diagnostic is gives only a general indication of problems, such
results as these are strong evidence that the chain has not yet reached its stationary
distribution.
14.3.4
Summary of Diagnostic Similarities and Diﬀerences
The empirical examples make it clear that the four diagnostics described in detail show
diﬀerent characteristics of the Markov chain. Given the potential complexities of model
space, the variability of posterior forms, and possibly high dimensions, it is not surprising
that the stochastic behavior of the chain can be diﬃcult to describe with a single summary

506
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
statistic.
It is therefore strongly recommended that researchers run all of the standard
formal diagnostics with superdiag in addition to generating graphical displays. Obviously
not all of these need to be included in the writeup, but they are certain to be useful in
understanding chain behavior. Shockingly, there are deep misunderstandings about this
process, even by producers of commercial software. For example, the Amos User’s Guide in
spss provides an MCMC estimation process for structural equation models. The software
manual states “The default burn-in period of 500 is quite conservative, much longer than
needed for most problems” (Arbuckle 2009). Obviously this is a dangerous recommendation,
even for simple models.
TABLE 14.10:
Diagnostic Scorecard, Tobit Model
Geweke
Gelman-Rubin
Heidelberger-Welch
Raftery-Lewis
Intercept
✓
✓
✓
✓
Past Rates
✓
✓
✓
✓
Political Culture
✓
✓
✓
✓
Current Opinion
✓
✓
✓
✓
Ideology
✓
✓
✓
✓
Murder Rate
✓
✓
✗
✓
To summarize the convergence assessment for the two empirical examples, we can look
at a complete score-card by parameter-dimension. First consider the summary for the Tobit
model in Table 14.10 that summarizes each of the four diagnostics. Only the Heidelberger
and Welch test for Murder Rate, and only for the halfwidth part of the test, fails here.
Such a small deviation is not worth worrying about given the other results. It is important
to remember that these are all distributional hypothesis tests and so running multiple
parameters through multiple tests will lead to some tail values.
To be more concrete,
suppose that we ran a Geweke test on a model with 100 parameters and set α = 0.05 for
the diﬀerence of means test. Then we would expect about 5 parameters to fail under the
null hypothesis that the complete chain had converged.
The scorecard for the Military Personnel model in Table 14.11 does not show such
positive results. Overall the Raftery and Lewis diagnostic gives the most pessimistic view
of convergence, even though it is the bluntest instrument. On the opposite end of the scale
the Heidelberger and Welch diagnostic is the most positive with six of the nine parameter
dimensions passing the test. The two most popular diagnostics disagree on αµ, β1,µ,β3,µ,
and β3,τ. This is actually a useful outcome because it demonstrates that the diagnostics are
looking at diﬀerent characteristics of stability. Such diﬀerences highlight the importance of
using multiple diagnostic tools.

Utilitarian Markov Chain Monte Carlo
507
TABLE 14.11:
Diagnostic Scorecard, Military Personnel
Model
Geweke
Gelman-Rubin
Heidelberger-Welch
Raftery-Lewis
αµ
✗
✓
✓
✗
ατ
✓
✓
✓
✗
β1,µ
✓
✗
✓
✗
β1,τ
✗
✗
✓
✗
β2,µ
✗
✗
✗
✗
β2,τ
✗
✗
✗
✗
β3,µ
✗
✓
✓
✓
β3,τ
✓
✗
✓
✓
τc
✗
✗
✗
✗
14.3.5
Other Empirical Diagnostics
There are a wealth of additional diagnostics that are less convenient since they are not
provided by BOA and CODA. These others generally require some programming or time spent
understanding public domain code. Cowles and Carlin (1996, p.890) provide a very nice
tabular summary of the 13 most popular methods as of 1996 (some time ago in MCMC-
terms). Early debates on diagnostic strategies centered essentially on the “one long chain”
versus the “many short chain” perspectives emblemized by Geweke versus Gelman and
Rubin. Essentially both sides won the debate in that many of the current strategies can be
characterized as “many long run” methods since computing power has become signiﬁcantly
more economical. What is very clear from published discussions is that it is always possible
to ﬁnd some example that “fools” a given convergence diagnostic (see the discussion of
the Gelman and Rubin [1992b] and Geyer [1992] papers).
The following are very brief
descriptions of some additional proposed diagnostics that require individual coding for each
model application.
Because these are customized for each individual application, they
cannot be “canned” in BUGS or other software.
Zellner and Min (1995).
This diagnostic was ﬁrst introduced here in Chap-
ter 13, but without giving the details below.
Assume that a θ vector of unknown pa-
rameter estimates can be segmented into two parts: (θ1, θ2), which is easy in the case
of Gibbs sampling.
From the deﬁnition of conditionals, we know that π(θ1, θ2|D) =
π(θ1|θ2, D)π(θ2|D) = π(θ2|θ1, D)π(θ1|D), for given D indicating both the data and
the prior.
If ˆπn(θ1, θ2, D) (note the “hat” on ˆπ) is the vector position at the candi-
date stopping point of the chain at iteration n, then the “diﬀerence convergence crite-
rion” statistic is formed by the diﬀerential Rao-Blackwellized estimate (discussed below)
of the conditional times the current value, and is close to zero if the chain has converged:
ˆηn = π(θ1|θ2, D)ˆπn(θ2|D) −π(θ2|θ1, D)ˆπn(θ1|D), where the distribution ˆπ(θi|D) comes
from a smoothed empirical estimate. Sometimes there is a natural segmentation of the θ
vector into the two components, but in general this is an artiﬁcial decision made by the

508
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
researcher. A related quantity based on the same identity, called the “ratio convergence
criterion,” is given by the ratio: ˆγn = (π(θ1|θ2, D)ˆπn(θ2|D))/π(θ2|θ1, D)ˆπn(θ1|D)), which
should be approximately one at convergence. Zellner and Min also give the procedure for
obtaining the posterior odds for the hypothesis H0: η = 0 versus the hypothesis H1: η ̸= 0,
with equal probability priors for the two alternatives and a Cauchy prior on η. Using a
reasonably large number of preceding values for ˆηi (i < n), we can calculate an empiri-
cal variance with the slightly unrealistic assumption of serial independence: ˆφn. So the
posterior odds of H0 over H1 are: KOA =

Nπ/2(1 + (ˆη2
n)(nˆφ2
n)) exp[−(ˆη2
n)(2ˆφ2
n)].
Ritter and Tanner (1992). The “Gibbs stopper” of Ritter and Tanner assigns im-
portance weights (see Chapter 9, Section 9.6) to each iteration of a Gibbs sampler so that
a comparison is made between the current draw and a normalized expression for the joint
posterior density. This method supports both a single chain or multiple chains (by seg-
menting the single chain) and assesses the bias of the full joint distribution of the Gibbs
sampler (only). The Gibbs stopper, however, can be slow in complex models.
Roberts (1992, 1994). This “distance evaluation” diagnostic, initially restricted to
the Gibbs sampler, gives a bias assessment of the chain simultaneously across all dimensions
with a graphic summary. The idea is to cycle through multiple Gibbs samplers (generalized
to other forms in 1994) in a stipulated order and then cycle back in the reverse order,
creating a dual chain. Roberts suggests as many as 10 to 20 separate chains started from
overdispersed starting points. The resulting statistic compares within and between behavior
with a summary that has known asymptotics.
Liu, Liu, and Rubin (1992).
These authors posit a “global control variable” to
estimate the bias of the full joint distribution using multiple chains. The test statistic is a
function of this variable whose expected value is the variance of the ratio of the posterior
at the distribution implied by the current iteration to that at the limiting distribution. It
is then necessary to use either graphical methods or the test setup of Gelman and Rubin
to make stopping decisions.
Yu and Mykland (1998).
A barely customized and particularly easy-to-implement
diagnostic is the CUSUM plot (see also Brooks [1998b]).
This plot is created by ﬁrst
calculating a running tally of the following statistic from the post-burn-in chain values
(1, . . . , t): St = t
i=1(θt −¯θ), and then plotting this cumulative sum against the time
index. The argument is that smoothness in the resulting graph and a tendency to spend
large periods away from zero are indications of slow mixing. Note that this is essentially
the information provided in a plot such as the left-hand side of Figures 14.1 and 14.2.
Mykland, Tierney, and Yu (1995).
This diagnostic is based on splitting a Markov
chain into small sets according to renewal criteria. A Markov chain is regenerative (a less
restrictive form of renewal) if there are times when future values are fully independent of
past values and all future values are iid. Notationally, a renewal set A exists if for a given real
0 < ϵ < 1 and marginal probability η(B) on another set B: p(θn+1 ∈B|θn) ≥ϵη(B), ∀B
(see Robert 1995, Section 3 for additional details and examples).
The diagnostic is a
graphical summary of the regeneration probability from iteration to iteration (and thus

Utilitarian Markov Chain Monte Carlo
509
implemented more directly with the Metropolis-Hastings algorithm and on discrete state
spaces), where stability indicates convergence. This procedure usually involves modifying
the algorithm to perform the regeneration calculations as it runs (as opposed to summarizing
existing chain runs), and the resulting hybrid processes can be complex (see: Johnson [1998],
and Gilks, Roberts, and Sahu [1998]). Giakoumatos (2005) produce a convergence diagnostic
based on subsampling of this method and drawbacks for unobserved ARCH models, and
Flegal provides a general convergence test based on subsampling bootstrap methods. Other
than such specialized settings there has not been much new work in this area since the 1990s
and the four main formal diagnostics described in this chapter dominate applied work.
14.3.6
Why Not to Worry Too Much about Stationarity
Considerable attention has been given to convergence diagnostics (here and elsewhere)
for good reason: Markov chains that are not mixing through the target distribution do
not provide useful inferential values. In this chapter we have seen a number of ways for
conveniently asserting convergence or nonconvergence. However, it can be shown that these
eﬀorts are relatively cautious in their practical implications with applied work (Robert
and Casella 2004, 461). While ﬁnite runs of the chain always fail to achieve asymptotic
properties, each step of the chain does bring us closer, in probability, to a realization from
the stationary (invariant) distribution (Robert 2001, p.303). Notably, Geyer (1991, p.158)
points out that the accuracy of calculated quantities depends on the adequacy of the burn-in
period, which can never be validated for certain. This does not mean that non-asymptotic
results are necessarily useless. In fact, the ergodic theorem guarantees that averages from
sample values give strongly consistent estimates (Meyn and Tweedie 1993), and therefore
the resulting inferences are no less reliable than other sample-based procedures.
Recall the discussion in Section 13.4, where it was shown that convergence to stationarity
is diﬀerent from convergence of empirical averages. The arbitrary statistic of interest, h(X),
from a Harris recurrent Markov chain, is summarized with ¯h =
1
n
n
i=1 h(xi), with the
property: n
i=1 h(Xi)/n −→Efh(X) as n →∞. The diﬀerence between the expected
value of h(X) in the true distribution at time n, and the expected value of h(X) in the
stationary distribution, is:
' 1
n
n

i=1
h(Xi) −Ex0h(X)
(
−
'
Efh(X) −Ex0h(X)
(
→0 as n →∞.
(14.21)
For a geometrically ergodic Markov chain, this quantity converges geometrically fast
to zero since ||Efh(x) −Ex0h(x)|| ≤kδn for some positive k, and δ ∈(0 : 1).
The
ﬁrst bracketed term is the diﬀerence between the empirical average and its expectation
at time n. Except at the trivial point where n = 0, these are never non-asymptotically
equivalent values and this demonstrates that even in stationarity, the empirical average has
not converged.
In fact, all we know for certain from the central limit theorem is that:

510
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
( 1
n
n
i=1 h(Xi) −Ex0h(X))/(σ/√n) converges in distribution to a standard normal, but
does not bring along convergence of empirical averages.
Thus, convergence to stationarity has no bearing on the convergence of the empirical
averages, which are usually of primary interest. Moreover, if stationarity is still a concern,
merely start the chain in the stationary distribution (easy for Metropolis-Hastings because
the stationary distribution is explicitly given) by applying an accept-reject method (a simple
trial method based on testing candidate values from some arbitrary instrumental density;
see Robert and Casella [2004, Chapter 1]) until a random variable from the stationary
distribution is produced, then run the chain forward from there. This works because a
chain started in the stationary distribution remains in the stationary distribution, and it is
typically the case that the wait for the accept-reject algorithm to produce a random variable
from the stationary distribution will be shorter than any reasonable burn-in period.
14.4
Mixing and Acceleration
Simple social science model speciﬁcations, even those with hierarchical features, often
mix through the target distribution rather easily with standard MCMC kernels and present
no estimation challenges with MCMC procedures. Conversely, mixing problems most com-
monly occur in high-dimension state spaces.
The example models given with the BUGS
software are very well-behaved in this regard, but as model speciﬁcations get larger (more
parameters to estimate) and more complex, it is often the case that even when the chain
has converged, it will move slowly through the stationary distribution. The problem with
this is that if the chain is stopped too early, then empirical estimates will be biased.
Slow mixing chains also make the problem of convergence diagnosis more diﬃcult because
a chain that has not fully explored the limiting distribution will give biased results based
on a subset of the appropriate state space.
While the problem of mixing diminishes with increasingly powerful desktop computing,
it is still important to have some tools that can increase mixing rates. These typically involve
“tuning” (case-speciﬁc model estimation changes) since in general they are modiﬁcations of
standard algorithms that are done on a case-by-case basis. We will also provide a completely
diﬀerent class of approaches to solving this problem in the next chapter based on modifying
the transition kernel in order to facilitate easier traversal of the chain.
14.4.1
Reparameterization
Often slow mixing through the target distribution can be attributed to high correlation
between model parameters. This is particularly exacerbated with the Gibbs sampler since
each sub-step is restricted to a move that is parallel to an axis by construction, and the

Utilitarian Markov Chain Monte Carlo
511
full conditional distributions will specify small horizontal or vertical steps (Nandram and
Chen 1996).
Therefore correlations, which by deﬁnition deﬁne nonparallel structures in
the posterior, will tend to restrict free movement through the parameter space, resulting in
small steps and poor mixing properties. High intra-parameter correlation is also a problem
with the Metropolis-Hastings algorithm in that it also slows mixing, but is made obvious
by observing many rejected candidate values. So while the Gibbs sampler will chug along
in a limited region without indicating such a problem to the observer, a similarly aﬀected
Metropolis-Hastings algorithm will tend to reject many candidate jumping positions and
therefore be more obvious in its behavior.
 0.1 
 0.2 
 0.3 
 0.4 
ρ = −0.95
Euclidean Distance: 2.29
−2
−1
0
1
2
−2
−1
0
1
2
−2
−1
0
1
2
 0.05 
 0.1 
 0.15 
ρ = 0
Euclidean Distance: 15.6
FIGURE 14.7: A Comparison of Gibbs Sampling for Normals
As an example consider two parameters with standard normal marginal posteriors and
a correlation term, ρ. In Figure 14.7 each panel shows the ﬁrst 10 steps of a Gibbs sampler
starting at (1, −1). The total Euclidean distance traveled is an order of magnitude greater
for the case where the correlation coeﬃcient is 0.00 instead of −0.95, even though the
two marginal posteriors are deﬁned on the same support. Notice that the chain on the
uncorrelated surface travels about seven times further in the same number of steps. This
means that the second Markov chain will mix through the target distribution much more
eﬃciently than the ﬁrst since it is not constrained from taking big orthogonal moves in two
steps.
Usually this problem is easy to detect since slow mixing leads to underestimates of the
variance, an eﬀect that appears clearly in Figure 14.7. In addition, as already shown, BUGS
supplies the parameter correlation matrix. Cowles, Roberts, and Rosenthal (1999) used a

512
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
simulation study of this problem, also with standard normal marginal distributions, and
showed that the variance underestimation can be appreciable. Worse yet, they demonstrate
that discarding iterations by thinning the chain exacerbates the problem.
A number of solutions via reparameterization have been suggested to deal with this
problem. In the case of normal and related models simple algebraic forms include: γ1 =
θ1+θ2, γ2 = 3(θ1−θ2) (Gilks and Roberts 1996), centering the covariates x′
j = xj −¯θj (Hills
and Smith 1992, Zuur, Garthwaite, and Fryer 2002), centering the covariates by replacing
something like Yijk = μ+αi +βij +ϵijk in the normal linear model with Yijk = η+ρij +ϵijk
where ηi = μ+αi, and ρij = μ+αi +βij (Gelfand, Sahu, and Carlin 1995), or just centering
by “sweeping” all of the parameter means into a single term (Vines, Gilks, and Wild 1996).
These reparameterizations are particularistic to the context of the model and the data and
require considerable thought about the forms chosen as it possible to actually make the
mixing worse.
14.4.2
Grouping and Collapsing the Gibbs Sampler
Liu (1994) suggests altering the conditional calculations of the Gibbs sampler in the
following way in order to mitigate the eﬀects of high cross-chain correlations. Using the
standard notation, but for only three variables, consider the following two modiﬁcations:
Grouped Gibbs Sampler
θ[j]
1 , θ[j]
2 ∼π(θ1, θ2|θ[j−1]
3
)
θ[j]
3 ∼π(θ3|θ[j]
1 , θ[j]
2 )
Collapsed Gibbs Sampler
θ[j]
1 , θ[j]
2 ∼π(θ1, θ2)
θ[j]
3 ∼π(θ3|θ[j]
1 , θ[j]
2 )
where the diﬀerence is that the collapsed Gibbs sampler ﬁrst draws from the unconditional
joint distribution of θ1 and θ2, whereas the grouped (also called blocked) Gibbs sampler
ﬁrst conditions on θ3. So it must be possible to express and sample from the joint forms
dictated by grouping or collapsing (the ﬁrst two lines using θ1 and θ2 above).
The appeal of grouping and collapsing is that they shift the high correlations from the
Gibbs sampling process over to a more direct random vector generation process (Seewald
1992). This principle means that if there are diﬃcult components of the Gibbs sampler
(with regard to convergence and mixing), then we can devise a “side process” to draw
values in a more eﬃcient manner, since high correlations can be “internalized” into a single
joint draw.
Obviously these modiﬁcations are not a panacea; Roberts and Sahu (1997) warn that
incautious speciﬁcation of the blocking can even slow down the convergence of the Markov
chain (see the example in Whittaker [1990]). It is also possible to encounter situations
where it is diﬃcult to sample from the unconditional joint form, although Chen, Shao, and

Utilitarian Markov Chain Monte Carlo
513
Ibrahim (2000, p.78) point out that it is possible to run sub-chains within the ﬁrst step to
accomplish this (that is, to draw θ1 and θ2 from a two-component sub-Gibbs sampler).
The choice of grouping versus collapsing is typically determined by the structure of the
problem: the parametric form will frequently dictate which of the two forms are available.
It is generally better in the presence of correlation problems to use grouping (Chen, Shao,
and Ibrahim 2000). The real challenge is to specify the useful conﬁguration of blocking
collapsing in the sampler since this is a customized process outside the province of BUGS.
14.4.3
Adding Auxiliary Variables
Another idea for improving mixing and convergence is the strategy of introducing an
additional variable into the Markov chain process as a means of improving mixing and
convergence (Besag and Green 1993, Edwards and Sokal 1988, Swendsen and Wang 1987,
Mira and Tierney 2001a). The principle is to augment a k-dimensional vector of coeﬃ-
cients, θ ∈Θ, with (at least) one new variable, u ∈U, so that the (k + 1)-dimensional
vector on Θ × U space has good chain properties. The augmenting variables sometimes
have direct interpretational value, but this is not necessary as the real goal is to improve
the Markov chain. Actually the Metropolis-Hastings algorithm is already an auxiliary vari-
able process since the candidate-generating distribution is auxiliary to the distribution of
interest. Higdon (1998) points out that the Metropolis-Hastings algorithm is an auxiliary
variable process because the use of the uniform random variable for acceptance decisions
can be rewritten in this form.
A number of extensions have been developed to deal with circumstances that arise when
one of the conditional probability statements is not directly available or when some less
simple alternative to the transition kernel is required (Fredenhagen and Marcu 1987). There
is a small literature on the theoretical properties of auxiliary variable Markov chains (Brooks
1998b). Hurn (1997) ﬁnds that auxiliary variable algorithms have problems with posterior
distributions that are highly multimodal or have strong interaction components.
More
recent developments include a linkage to the slice sampler (below), (Wakeﬁeld, Gelfand,
and Smith 1991; Besag and Green 1993).
14.4.4
The Slice Sampler
The slice sample is a very important tool for modern MCMC software.
As noted,
introducing an additional variable into the Markov chain process can improve mixing or
convergence. The strategy with slice sampling is to augment a k-dimensional vector of
coeﬃcients, θ ∈Θ, with an additional variable, ψ ∈Ψ, with the result that the (k + 1)-
dimensional vector on Θ × Ψ space has better mixing through this space (Neal 2003).
First consider the posterior distribution of θ is given as π(θ), where conditioning on the
data is assumed. Now deﬁne π(ψ|θ) and the joint posterior distribution given by π(θ, ψ) =

514
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
π(ψ|θ)π(θ). Clearly the distribution π(ψ|θ) is picked to be easy to sample from, otherwise
the implementation problems are worse. In total there are two conditional transition kernels
required: one that updates θ, P[θ →θ′|ψ], and one that updates ψ, P[ψ →ψ′|θ]. In
the case of Gibbs sampling these are exactly the full conditional distributions: π(θ′|ψ)
and π(ψ′|θ), meaning that the algorithm cycles normally through the θ statements (now
conditional on one more parameter) and then adds a step for ψ. This two-step aspect of the
process ensures that the stationary process remains π(θ) since the detailed balance equation
is maintained. The nice part is that we run the chain normally, presumably with better
mixing, and then just discard the ψ values.
Generally the added variable in the slice sampler is a uniform draw, so we update the
marginals and the joint distribution with uniforms in the following way:
ψ[j+1]|θ[j] ∼U(0, π(θ[j])
θ[j+1]|ψ[j+1] ∼U(θ:π(θ[j+1]) > ψ[j+1])
(14.22)
in going from the jth to the (j + 1)st step. In the basic setup the target distribution is
expressed as a product of the marginals, π(θ) =  π(θi). This means that the auxiliary
variable is sampled from a uniform bounded by zero and the θ, then these θ are in turn
sampled from a uniform bounded below by the auxiliary variable(s).
Roberts and Rosenthal (1998) show that the slice sampler has many good theoretical
properties, such as geometric convergence, thus making it one of the more successful aux-
iliary variable applications (see also Mira and Tierney [2001b]). For speciﬁc applications
see Damien, Wakeﬁeld, and Walker (1999), Tierney and Mira (1999), as well as Robert and
Casella (2004, Chapter 8) for a list of diﬃculties.
A simple example given by Robert and Casella (1999), and implemented in R by Altman,
Gill, and McDonald (2003), uses a uniform distribution to generate desired normals. Begin
with at target f(θ) ∝exp(−θ2/2), and stipulate a uniform such that:
ψ|θ ∼U[0, exp(−θ2/2)],
θ|ψ ∼U[−

−2 log(ψ),

−2 log(ψ)],
which makes this ψ an auxiliary variable with direct sampling properties. Notice that the
ﬁrst statement could easily be modiﬁed to generate other forms. The R code to implement
this follows directly:
n <- 1000; t.vals <- 0; p.vals <- 0
for (i in 2:n)
{
p.vals <- c(p.vals,runif(1,0,exp(-0.5*t.vals[(i-1)]^2)))
t.vals <- c(t.vals,runif(1,-sqrt(-2*log(p.vals[i])),
sqrt(-2*log(p.vals[i]))))
}
The resulting t.vals draws are displayed in Figure 14.8, which shows them to closely
resemble normally distributed random variables. We can consider this particular example
as an MCMC version of the Box-M¨uller algorithm (1958).

Utilitarian Markov Chain Monte Carlo
515
+
+
+
+
+
+
+
+
+
+
++
+
+
+
+
+
+
++
+
+++
+
++
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
++
+
+
+
+
++
+
+
+
++
+
+
+
+
+
+
++
+
+
+
++
+
+
+
+
+
+
+
+
+
++
+
+
+
+
+
+
+
+
+
+
+
++
+
+
+
+
+
+
+
+++
+
+
++
+
+
+
+
+
++
+
+
+
+
+
+
+
+
+
++
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
++
+
+
+
+
+
+
++
+
+
+
+++
+
+
++
+
+++
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
++
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
++
+
+
+
+
+
+
+
+
+
+
+
+
+
+
++
+
+
+
+
+
+
++
+
+
+
+
+
++
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
++
+
++
++
+
+
+
+
+
++
+
+
+
++
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+++
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
++
+
+
+
+
+
+
+
++
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
++
+
+
+
+
++
+
+
+
+
+
+
+
+
+
++
+
+
++
+
+
++
+
+
+
+
+
+
++
+
+
+
+
+
+
+
+
+
+
+
++
+
+
+
+
++
++
+
+
+
++
+
+
++
+
+
+
+
+
+
+
+
++
++
+
+
+
+
+
+
+
+
+
++
+
+
+
+
+
+++
+
+
+
+
++
+
+
+
+
+
++
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
++
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+++
++
+
+
+
+
++
+
++
+
+
+
+
++
+
+
++
++
+
+
+
+
+
+
+
+
+
++
+
+
+
+
+
+
+
+
+
+
+
+
++
+
+
+
+
+
+
+
++
+
+
+
+
+
+
+
++
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
++
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
++
++
+
+
+
+
++
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
++
+
+
+
+
+
+
+
+
+
+
+
++
+
+
+
+
+
+
+
+
+
++
++
+
+
+
+
+
+
+
+
+
+
++
+
+
+
+
+
+
+
+
+
+
+
+
+
++
+
+
++
+
+
+
+
++
+
+
+
+
+
+
++
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
++
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
++
+
+
++
+++
+
+
+
+
+
+
+
+
+
++
+
+
+
+
+
+
+
++
++
+
+
+
+
+
+
+
+
+
+
+++
+
+
+
+
+
+
+
+
++
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
++
+
+
+
+
+
++
+
+
+
+
+
+
+
+
+
++++
+
+
+
+
+
+
++
+
+
+
+
+
++
+
−3
−2
−1
0
1
2
3
−3
−2
−1
0
1
2
Normal Quantiles
Sorted Draws
Histogram of Draws
FIGURE 14.8: Slice Sampler Output
14.5
Chib’s Method for Calculating the Marginal Likelihood Inte-
gral
As noted in Chapter 7, the integrals in Bayes Factors can be challenging to estimate with
reasonably realistic models. Usually hierarchical speciﬁcations lead to these complications,
but hierarchical modeling is one of the strengths of the Bayesian paradigm. One general
class of tools for getting these marginal likelihoods is provided by Chib (1995) for Gibbs
sampling and Chib and Jeliazkov (2001) for Metropolis-Hastings. See also Geweke (2005,
pp.257-261).
The particular objective is to calculate p(x|Mi) =

θ fi(x|θi)pi(θi)dθi in
(7.10) (page 216) for the ith model. It is easier to look at this quantity if we rearrange
Bayes’ Law and take logs (dropping the model index since we will consider only one model
here for now):
log p(x) = ℓ(θ′|x) + log p(θ′) −log π(θ′|x)
(14.23)
where θ′ is a completely arbitrary (but acceptable) point in the sample space. Typically
we will choose a point from the high density region such as the posterior mean. So if we
had an estimate of π(θ′|x) from simulation, this would be a straightforward calculation.
We will now describe the method for Metropolis-Hastings. The Gibbs sampling version,
a direct extension of data augmentation (Tanner and Wong 1987) is described clearly in
Chib (1995), and the algorithm is described in Chapter 7, Exercise 20 (page 246). The

516
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
Metropolis-Hastings case (Chib and Jeliazkov 2001) is particularly elegant since it is tied to
the acceptance process. Consider the standard form of the candidate acceptance probability
for a generic Metropolis-Hastings setup:
α(θ′, θ) = min
'π(θ′)
π(θ)
qt(θ|θ′)
qt(θ′|θ), 1
(
.
(14.24)
Recall that the probability of transitioning to arbitrary point θ′ is the probability that the
candidate-generating distribution produces θ′ times the probability that it is accepted from
above:
p(θ, θ′) = q(θ′|θ)α(θ′, θ)
(14.25)
such that for any arbitrary point, the detailed balance equation, (10.18) from page 356, can
be expressed as:
π(θ)q(θ′|θ)α(θ′, θ) = π(θ′)q(θ|θ′)α(θ, θ′).
(14.26)
The innovation is to now take integrals of both sides with respect to θ, realizing that π(θ′)
is a function evaluation at an arbitrary point and can therefore be moved outside of the
integral. Doing this and rearranging slightly gives:
π(θ′) =

Θπ(θ)q(θ′|θ)α(θ′, θ)dθ

Θ q(θ|θ′)α(θ, θ′)dθ
,
(14.27)
which is really just a ratio of two expected value calculations:
π(θ′) = Eπ(θ)[q(θ′|θ)α(θ′, θ)]
Eq(θ|θ′)[α(θ, θ′)]
.
(14.28)
Of course what expected value calculations do is set us up for simulation solutions, and what
Chib and Jeliazkov realized is that in the course of running a standard Metropolis-Hastings
algorithm for marginal posterior distributions, we can get the marginal likelihood along the
way with not much extra trouble. So replace (14.28) with its simulation analog:
πsim(θ′) =
1
M
M
m=1 α(θ′, θm)q(θ′|θm)
1
N
N
n=1 α(θN, θ′)
,
(14.29)
which of course uses known functions and values readily at hand. Typically, but not by
necessity, M = N here.
Recall that θ′ is chosen arbitrarily but within some high density region of the posterior.
So this process substitutes a diﬃcult integration process with simulation of the posterior
density at a single point by completing (14.23) with the simulated result:
log psim(x) = ℓ(θ′|x) + log p(θ′) −log πsim(θ′|x).
(14.30)
The other quantities on the right-hand side are readily available, so the marginal likelihood
is calculated.

Utilitarian Markov Chain Monte Carlo
517
14.6
Rao-Blackwellizing for Improved Variance
Estimation
Gelfand and Smith (1990) introduced the idea of “Rao-Blackwellization” as a means of
exploiting the conditional nature of Gibbs sampling in order to produce improved variance
estimates for statistics of interest through conditioning (the pre-MCMC simulation idea
appears to originate with the text of Hammersley and Handscomb [1964]). This technique
can also be applied to the Metropolis-Hastings algorithm (Casella and Robert 1996), data
augmentation (Liu, Wong, and Kong 1994), EM (Meng and Schilling 1996), nonparametric
Bayes (Liu 1996b), to detect outliers (Haro-L´opez, Mallick, and Smith 2000), and also in
general non-Bayesian procedures.
The Rao-Blackwell Theorem states that estimates conditioned on suﬃcient statistics are
subsequently unaﬀected or improved in terms of the variance of their sampling distribution.
When applied to variance calculations (the interest here), the theorem can be expressed
through the relationship between conditional and unconditional variance and is not always
based on suﬃcient statistics. Start with a statistic of interest such as a mean or quantile
for one parameter of a two-parameter Markov chain: h(θ1). The variance relationship is
deﬁned by:
Var[h(θ1)] = Var[E(h(θ1|θ2))] + E(Var[h(θ1|θ2)]).
(14.31)
So by simply rearranging we can show that the variance of the expected value of the con-
ditional cannot be lower since variances are never negative:
Var[E(h(θ1|θ2)] = Var[h(θ1)] −E(Var[h(θ1|θ2)]),
(14.32)
provided that the method for producing the Markov chain values does not introduce ex-
acerbating correlations between terms (see Liu, Wong, and Kong [1995] and Geyer [1995]
for speciﬁcs). A standard means of comparison is the asymptotic relative eﬃciency (ARE),
deﬁned by the ratio of the variance of the limiting distribution of two estimates. In the case
of a two-variable Gibbs sampler, these are for θ1:
Var[h(θ1)]
n→∞
= Var[h(θ1)] + 2
∞

i=1
Cov(h(θ[0]
1 ), h(θ[i]
1 ))
Var[E(h(θ1|θ2))]
n→∞
= Var[E(h(θ1|θ2))]
+ 2
∞

i=1
Cov(E[h(θ[0]
1 |θ[0]
2 )], E[h(θ[i]
1 |θ[i]
2 )]),
(14.33)
where the brackets in the super-script denote Markov chain iterations. Levine (1996) showed
that the ARE of the ﬁrst unconditional variance over the Rao-Blackwellized variance is
always greater than or equal to one, thus favoring use of the Rao-Blackwellized version.
Fortunately, the generalization of this last result issue is not typically a problem in that

518
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
most conventional applications and a number of authors have provided suitable conditions.
For instance, Liu, Wong, and Kong’s (1994, 1995) identiﬁcation of an interleaving property
is satisﬁed by the conditions:
▷θ[i]
1 and θ[i+1]
1
are conditionally independent given θ[i]
2 ,
▷θ[i−1]
2
and θ[i]
2 are conditionally independent given θ[i]
1 ,
▷and the chain has converged such that (θ[i]
1 , θ[i−1]
2
), (θ[i]
1 , θ[i]
2 ) are iid.
The unconditional and Rao-Blackwellized empirical estimate of h(θ1) from n post-
convergence Markov chain values are given by the ergodic theorem as:
h(θ1)uc = 1
n
n

i=1
h(θ[i]
1 ),
h(θ1)rb = 1
n
n

i=1
E(h(θ[i]
1 |θ[i]
2 ).
(14.34)
Furthermore, both of these estimators are unbiased and are known to converge to the
marginal density h(θ1).
The general strategy of Gelfand and Smith (1990) is therefore to condition each param-
eter estimate on the others: if π(θi) is a marginal density estimate (smoothed or empirical),
then the Rao-Blackwellized estimate can be calculated by ˆπ(θi) =
1
k−1
k−1
j=1 π(θi|θj,−i), for
k parameters as an empirical estimate of

θj π(θi|θj,−i)π(θj,−i)dθj,−i. It turns out that this
is not too hard to do and can lead to vastly improved variance estimates.
Other more elaborate schemes can also be implemented. For instance if there exists a
parallel chain structure (as in the Raftery and Lewis diagnostic) conditioning on this chain
at each iteration: ˆπ(θi) =
1
k−1
k−1
j=1 π(θi|zj,−i). Casella and Robert (1996) use the rejected
values in a Metropolis-Hastings algorithm in the following way. If θa are the accepted values
of the algorithm and θc are the candidate values, only some of which are accepted in the
application of the Metropolis-Hastings algorithm in the process of obtaining n chain values,
then the uniform random variable used to make the accept/reject decision is an ancillary
statistic and can be conditioned on (integrated over). If the estimator is expressed usually
using only the accepted values:
h(θ1)uc = 1
n
n

i=1
h(θ[i]
a ),
(14.35)
then we can use an indicator function to re-express to include the rejected values:
h(θ1)rb = 1
n
n

i=1
⎡
⎣h(θ[i]
c )
n

j=i
I(θ[i]
a =θ[i]
c )
⎤
⎦.
(14.36)
Recall that the acceptance ratio was deﬁned in Chapter 10 by a(θc, θi−1) and the acceptance
probability of the candidate value k versus status quo value i (ρik) is the minimum of
a(θc, θi−1) and 1. Now deﬁne at the ith step going forth by the j:

Utilitarian Markov Chain Monte Carlo
519
ξij =
j+
k=i+1
(1 −ρik)
i < j,
and
ξii = 1
δi = p(θ[i]
a = θ[i]
c |θ[1]
c , . . . , θ[i]
c ) =
i−1

j=1
δjξj(i−1)ρji
φi = δi
n

j=i
ξij.
(14.37)
Then we get the following version of the Rao-Blackwellized estimator:
h(θ1)rb = 1
n
n

i=1
φih(θc).
(14.38)
Casella and Robert subsequently show circumstances where this leads to a superior es-
timator over the unconditional version. This process is not limited in its application to
the Metropolis-Hastings algorithm, and Chen, Shao, and Ibrahim (2000) give extensions
(page 360).
Unfortunately, the application of Rao-Blackwellization is particularistic to each given
model, and can require extensive customized coding of the chain since it is integrally tied
into the functioning of the chain. This may deter a number of users, despite its appealing
characteristics.
Utilitarian Epilogue
Some issues covered here are conceptually and managerially simple, like starting points,
thinning, and the idea of a burn-in period. Most frequent users of MCMC tools fall into
good habits and are aware such details need to be considered. More important is the con-
sideration of the length of the chain to be run, conditional on the complexity of the model.
Discussion ranged herein from the four dominant diagnostic tests to a range of less com-
mon alternatives and graphical approaches. At some point experienced users with relatively
complex model speciﬁcation leave the comfortable world of BUGS and write customized sam-
plers. In such endeavors tools like reparameterization, grouping/collapsing, augmentation,
and alternatives like the slice sampler can be enormously helpful in producing eﬃciently
mixing Markov chains. Rao-Blackwellization from sampler output also has the potential to
improve results with user-developed samplers. One purpose of the short discussion of these
methods in this chapter is to direct interested researchers into the ongoing literature on
sampler development.

520
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
14.7
Exercises
14.1
For the hierarchical model of ﬁrearm-related deaths in Chapter 6, perform the
following diagnostic tests from Section 14.3 in CODA or BOA: Gelman and Rubin,
Geweke, Raftery and Lewis, Heidelberger and Welsh.
14.2
(Robert and Casella 2004, Chapter 8).
In Chapter 7, Exercise 20 (page 246),
samples from the truncated normal distribution f(x) = k exp[−1
2(x + 3)2]I0,1 (k a
normalizing constant) were generated. Rejection sampling here can be ineﬃcient,
so write a slice sampler in R according to the following steps at iteration t:
(a) draw u[t+1] ∼Uu(0:f(x[t])) (the vertical step)
(b) draw x[t+1] ∼Uf(x)(u[t+1]:1) (the horizontal step)
where the second step means that a value of x is drawn uniformly such that
k exp[−1
2(x + 3)2]I0,1 > uf(x[t]).
14.3
Plot in the same graph a time sequence of 0 to 100 versus the annealing cooling
schedules: logarithmic, geometric, semi-quadratic, and linear. What are the trade-
oﬀs associated with each with regard to convergence and processing time? (This
question is left here to be compatible with the answer key to the Second Edition.
Read Section annealing.section before attempting.)
14.4
Explain how dramatic increases in computational power has made the 1990s debate
about “one long run” versus “many short runs” for assessing convergence unimpor-
tant.
14.5
Show how the full conditional distributions for Gibbs sampling in the Bayesian
Tobit model on page 482 can be changed into grouped or collapsed Gibbs sampler
forms (Section 14.4.2).
14.6
Write a sampler in R to implement the grouped or collapsed Gibbs sampler from the
problem above and use it to estimate the marginal posteriors for the death penalty
support data (Example 14.3, page 481) by modifying the code in this chapter’s
Computational Addendum.
14.7
BUGS includes a quick diagnostic command, diag, which implements a version
of Geweke’s (1992) time-series diagnostic for the ﬁrst 25% of the data and the
last 50% of the data. Rather than calculate spectral densities for the denomina-
tor of the G statistic, BUGS divides the two periods into 25 equal-sized bins and
then takes the variance of the means from within these bins. Run the example
in Section 14.3 using the BUGS code in the Computational Addendum with
10,000 iterations, and load the data into R with the command . military.mcmc <-

Utilitarian Markov Chain Monte Carlo
521
boa.importBUGS("my.bugs.dir/bugs"). Write a function to calculate the short-
cut for Geweke’s diagnostic for each of the ﬁve chains (columns here). Compare
your answer with BUGS.
14.8
David C. Baldus, Charles Pulaski, and George Woodworth (1983, 1990), i.e., the
“Baldus Study,” looked at the potential disparity in the imposition of the death
sentence in Georgia based on the race of the murder victim and the race of the
defendant. Using the dataset baldus in BaM, write a model in BUGS or JAGS with
the variable sentence (death sentence imposed: 325 0 cases, 127 1 cases) and
specify a linear model including a multivariate normal prior (dmnorm). Modify this
prior speciﬁcation to make it a multivariate version of Zellner’s g-prior (Chapter 4,
Exercise 20, on page 142).
14.9
For the model of marriage rates in Italy given in Section 12.5.1, calculate the Zellner
and Min diagnostic with the segmentation: θ1 = α, θ2 = β, for the posteriors
deﬁned at the iteration points: ˆπ1,000(α, β, λ, X) and ˆπ10,000(α, β, λ, X). Test H0:
η = 0 versus H1: η ̸= 0 with the statistic KOA assuming serial independence.
14.10
Write an R function that implements the Ritter and Tanner diagnostic (page 508)
and apply it to the Tobit death penalty example (starting on 481).
14.11
(Raftery and Lewis 1996). Write a Gibbs sampler in R to produce a Markov chain to
sample for θ1, θ2, and θ3, which are distributed according to the trivariate normal
distribution
⎡
⎢⎣
θ1
θ2
θ3
⎤
⎥⎦∼N
⎛
⎜
⎝
⎡
⎢⎣
0
0
0
⎤
⎥⎦,
⎡
⎢⎣
99
−7
−7
−7
1
0
−7
0
0
⎤
⎥⎦
⎞
⎟
⎠
by cycling through the normal conditional distributions with variances: V (θ1|θ2, θ3) =
1, V (θ2|θ1, θ3) = 1/50 and V (θ3|θ1, θ2) = 1/50. Contrast the Geweke diagnostic
with the Gelman and Rubin diagnostic for 1,000 iterations.
14.12
Write a dynamic animation that shows the witch’s hat distribution melting with
simulated annealing.
14.13
Prove that data augmentation is a special case of Gibbs sampling.
14.14
The military personnel model in Section 14.3 ﬁts poorly because of lack of inde-
pendence in the countries (columns) and the years (rows). Write the appropriate
model in BUGS/JAGS that accounts for one very inﬂuential country and the time
series eﬀect. Compare your results to those in Table 14.3.
14.15
For the following simple bivariate normal model

θ1
θ2

∼N
*
0
0

,

1
ρ
ρ
1
,
,

522
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
the Gibbs sampler draws iteratively according to:
θ1|θ2 ∼N(ρθ2, 1 −ρ2)
θ2|θ1 ∼N(ρθ1, 1 −ρ2).
Write a simple implementation of the Gibbs sampler in R, then calculate the un-
conditional and conditional (Rao-Blackwellized) posterior standard error, for both
ρ = 0.05 and ρ = 0.95. Compare these four values and comment.
14.16
The Lehmann-Scheﬀ´e theorem states that an unbiased estimate that is conditioned
on a complete and suﬃcient statistic is the unique most eﬃcient estimator: if T
is a complete and suﬃcient statistic and E(f(T )) = θ, then f(T ) is the minimum
variance (unbiased) estimator of θ. This is therefore a stronger statement than
the Rao-Blackwell theorem. Why then is the Rao-Blackwellized posterior standard
error more useful for MCMC purposes than a “Lehmann-Scheﬀ´e” posterior standard
error?
14.17
Produce the ARE result for the Rao-Blackwellization from (14.33) with a two-
variable Metropolis-Hastings algorithm instead of the Gibbs sampler that was used.
14.18
The Behrens-Fisher problem is an old problem that is awkward in non-Bayesian
settings and straightforward with a Bayesian treatment. Suppose there are two
independent samples that are both normally distributed according to:
x11, . . . , x1n1 ∼N(μ1, σ2
1),
x21, . . . , x2n1 ∼N(μ2, σ2
2)
where all four parameters are unknown.
Following the advice of Berger (1985)
specify the following prior distributions:
p(μ1) ∝1p(μ2) ∝1
p(σ2
1) ∝1/σ2
1p(σ2
2) ∝1/σ2
2.
Using two selected countries from military personnel data in Section 14.3, write a
sampler to produce posterior values from μ1 −μ2.
14.19
Modify the Metropolis-Hastings R code on page 359 to produce the marginal like-
lihood with Chib’s method.
14.20
Write an implementation of Chib and Jeliazkov’s (2001) method for obtaining the
marginal likelihood from Gibbs sampling output in R.

Utilitarian Markov Chain Monte Carlo
523
14.8
Computational Addendum: Code for Chapter Examples
14.8.1
R Code for the Death Penalty Support Model
# PACKAGES AND DATA LOAD
lapply(c("BaM","msm","survival","LearnBayes"),library,character.only=TRUE)
# msm FOR THE rtnorm FUNCTION, survival FOR THE survreg FUNCTION
# LearnBayes FOR THE rigamma FUNCTION
data(norr)
# SETUP REDUCED DATA STRUCTURE FOR GIBBS
attach(norr.df)
Y <- c(ds9395p)
X <- cbind(rep(1,nrow(norr.df)),ep4089n,polcul,d8892r2,id8892m2,murder90)
detach(norr.df)
dimnames(X)[[2]] <- c("Intercept","Past Rates","Political Culture",
"Current Opinion","Ideology","Murder Rate")
# RUN BIVARIATE REGRESSIONS TO GET PRIOR MEANS FOR BETAS
beta0 <- rep(NA,ncol(X)); beta0[1] <- 1
for (i in 2:ncol(X))
beta0[i] <- summary(lm(Y~X[,i]))$coef[2,1]
# SET PARAMETERS FOR GIBBS
gamma0 <- 300; gamma1 <- 100
# HYPERPARAMETERS
B0 <- 0.02
# SCALING ON SIGMA^0
mc.start <- 1; mc.stop <- 50000
# MCMC CONTROL
norr.tob <- survreg(Surv(Y,Y>0,type=’left’) ~ X[,-1],
dist=’gaussian’,data=norr.df)
B.mat
<- matrix(summary(norr.tob)$coef,nrow=1) # STARTING POINTS
s.sq
<- matrix(rep(10,6), nrow=1)
# PRIOR on SIGMA^2 OF BETA
y.star <- Y
# LATENT DATA START
# GIBBS SAMPLER
for (i in mc.start:mc.stop)
{
for (j in 1:length(Y)) {
if (Y[j] == 0)
y.star[j] <- rtnorm(1,X[j,]%*%B.mat[i,],
s.sq[i,],lower=-Inf,upper=0)
}
delta <- t(y.star - X %*% B.mat[i,]) %*% (y.star - X %*% B.mat[i,])
s.temp <- rep(Inf,ncol(s.sq)); while(!is.finite(sum(s.temp)))
s.temp <- rigamma(ncol(s.sq), (gamma0+length(Y))/2,
(gamma1+delta)/2)
s.sq <- rbind(s.sq,s.temp)
B.hat <- solve( B0 + t(X)%*%X ) %*% (B0*beta0 + t(X)%*%y.star)
B.var <-
make.symmetric( solve( s.sq[(i+1)]^(-1)*B0
+ s.sq[(i+1)]^(-1)*t(X)%*%X ) )

524
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
B.mat <- rbind( B.mat, rmultinorm(1,B.hat,B.var,tol=1e-06) )
if (i %% 100 == 0)
print(paste("iteration:",i))
}
14.8.2
JAGS Code for the Military Personnel Model
model {
for (i in 1:YEAR) {
for (j in 1:COUNTRY) {
mu[i,j] <- alpha[i] + beta1[i]*(x[j]) + beta2[i]*(x[j]^2)
+ beta3[i]*cos(x[j])
y[i,j]
~ dnorm(mu[i,j],tau.c)
}
alpha[i] ~ dnorm(alpha.mu,alpha.tau)
beta1[i] ~ dnorm(beta1.mu,beta1.tau)
beta2[i] ~ dnorm(beta2.mu,beta2.tau)
beta3[i] ~ dnorm(beta3.mu,beta3.tau)
}
alpha.mu
~ dnorm(1,0.1)
alpha.tau ~ dgamma(1,0.1)
beta1.mu
~ dnorm(0,0.1)
beta1.tau ~ dgamma(1,0.1)
beta2.mu
~ dnorm(0,0.1)
beta2.tau ~ dgamma(1,0.1)
beta3.mu
~ dnorm(0,0.1)
beta3.tau ~ dgamma(1,0.1)
tau.c
~ dgamma(1,0.1)
}

Chapter 15
Markov Chain Monte Carlo Extensions
This chapter reviews some recent extensions and modiﬁcations of the classic MCMC algo-
rithms described in previous chapters. Generally, researchers develop these methods to deal
with problematic estimation conditions such as multimodality, extremely high dimension,
and diﬃcult convergence issues. This chapter also discusses some more recent developments
that speed-up the process of MCMC estimation in ways that extend standard Bayesian
stochastic estimation. These deviate from the normal orthodoxy but perhaps point towards
future development. One very promising direction described here is Hamiltonian Monte
Carlo, which is a variant of the Metropolis-Hastings algorithm. This literature is incredibly
dynamic will remain so for the next decade or more, so some of these sections merely point
at emerging literatures.
15.1
Simulated Annealing
Kirkpatrick, Gelatt, and Vecchi (1983) and ˇCern´y (1985) proposed the MCMC traver-
sal technique simulated annealing, which, like Metropolis-Hastings, has roots in statistical
physics but is also useful in general stochastic simulation for Bayesian modeling. Simulated
annealing is a ﬂexible stochastic procedure for iteratively traversing highly textured sample
spaces where conditions have been changed to make movement easier. Correctly set up and
run, the annealing algorithm is guaranteed to explore the global maxima (Lundy 1985),
unlike the EM algorithm, which seeks the nearest mode and remains there. This makes it
an enormously useful approach in MCMC work with multimodal posterior forms, although
it can also be very slow both in movement and with necessary human involvement (van
Laarhoven and Aarts 1987). There is also a related literature on simulated annealing for
pure optimization (Eglese 1990, Fleischer 1995) rather than posterior description which is
our interest here.
We have seen that it is possible for a Markov chain to get “stuck” for an inordinately
long period of time in a nonoptimal region of the sample space exploring attractive modal
features that are not the primary density areas of interest. This is particularly troublesome
when such a region separates two high-density areas and the chain ﬁnds it diﬃcult to fully
explore both. Usually this problem is more common in higher dimensions, but imagine a
525

526
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
two-dimensional, bimodal structure with a large mode, a small mode, and a wide gulf in the
middle. It may be the case in this example that the trough in the middle seriously impedes
the chain from easily passing from one mode to the other.
The analogous idea behind simulated annealing is the principle that less brittle solid
metals are produced when allowed to cool slowly from a liquid state. In fact the metallurgic
process is called annealing. Imagine that we can increase the “temperature” of the state
space, melting down prominent features: ﬂattening out modes and valleys such that the
chain ﬁnds it easier to move through previously low-density regions. Then when the temper-
ature is reduced back to normal levels the chain has hopefully passed through impediments
to free travel. This process is done by temporarily altering all of the probabilities in the
transition kernel for a Metropolis-Hastings chain, running the Markov chain for a time, and
then returning the probabilities to their previous values.
The primary decision is the determination of the temperature schedule that dictates
the process by which the chain returns to its original “cold” state. Start by deﬁning a
temperature parameter at time t: Tt > 1, and modifying the transition kernel according
to π∗(θ) = π(θ)
1
T , being careful to renormalize. So heating the kernel by making T large
ﬂattens out its probability structure toward a uniform distribution. If there are multiple
modes, they will melt into the surface and therefore no longer be strong attractions. It is
always necessary to deﬁne an initial temperature, T0, that provides suﬃcient melting, then
determine a rate that slowly decreases Tt until reaching one and therefore returning to the
original cold transition kernel probabilities, which are those of inferential value. The cooling
schedule recognizes the competing phenomenon: (1) slow cooling enables greater coverage
of the sample space, and (2) faster cooling gives more reasonable simulation times.
Consider what happens to a Metropolis-Hastings algorithm in this scenario. As the
jumping distribution generates candidate positions, very few of these will be rejected and
the Markov chain will rarely stay in place. This is “good;” it means that the chain can freely
explore the sample space without impediments. It is also “bad” in that there is obviously
much less of a tendency to remain in the (previous) high density areas. This is where the
researcher-speciﬁed cooling process comes in.
The following modiﬁed Metropolis-Hastings setup, which was the original idea behind
simulated annealing as ﬁrst proposed by Metropolis et al. (1953), gives a simple Markov
chain for example purposes.
Suppose that at time (step) k we are at temperature Tk,
having started at step 0, and the heated up temperature T0. Then the next value is chosen
according to the algorithm:
▷At the tth step draw θ′ from a uniform distribution around the current position, θ[t].
▷Deﬁne: a(θ′, θ) = exp[−(π(θ′
j) −π(θ[t]
j ))/Tt], and make the decision:
θ[t+1]
j
=
⎧
⎪
⎪
⎨
⎪
⎪
⎩
θ′
j
with probability
min (a(θ′, θ), 1)
θ[t]
j
with probability
1 −min (a(θ′, θ), 1) .
(15.1)

Markov Chain Monte Carlo Extensions
527
▷After sampling suﬃciently that convergence is concluded at this temperature, move
up the temperature schedule from Tt to Tt+1.
▷Repeat steps 1-3 until the temperature schedule has been completed.
Therefore the transition matrix is ﬁrst heated up such that the chain converges weakly over
a near-uniform distribution. Once the cooling begins, the chain is observed to converge at
progressively cooler temperatures until the transition matrix returns to its original state.
This process can be repeated as many times as necessary to collect a suﬃcient number
of cold chain values. This iterative process, which requires human interaction in this basic
case, can be generalized in several ways described below to reduce the required management
by the researcher.
The hottest jumping distribution for generating candidate values does not need to be
uniform, but often is speciﬁed to be. While the Markov chain described here is not homo-
geneous as is the standard Metropolis-Hastings algorithm, H`ajek (1988) showed that the
discrete state space still has required convergence properties by considering the recorded
“cold” chain values to be the only expression of the Markovian process similar (but diﬀerent)
than the idea of thinning with an appeal to a classic theorem about grouping Markovian
iterations in Meyn and Tweedie (1993).
Similar properties hold for the continuous case
and are given by Duﬂo (1996), Geman and Hwang (1986), Holley, Kusuoka, and Stroock
(1989), and Jeng and Woods (1990) in more complex settings.
In general the temperature (cooling) schedule should gradually cool down by decreasing
Tt very slightly on each iteration after T0. A number of schemes have been proposed such as
a logarithmic scale (Geman and Geman 1984): Tt = kT1/ log(t), a geometric-type process:
Tt = ϵTt−1, 0 < ϵ < 1 (Mitra, Romeo, and Sangiovanni-Vincentelli 1986), as well as more
complex algorithms from statistical physics (Aarts and Kors 1989). H`ajek (1988) showed
that a logarithmic cooling schedule proportional to the height of the global maximum (often
obtainable with EM or some other search tool) converges asymptotically to the full set of
maxima.
The choice of a cooling schedule is necessarily a compromise between slow cooling to
ensure greater coverage of the sample space and faster cooling back to zero in order to give
a reasonable simulation time. This is not necessarily a stark trade-oﬀ. For instance, in the
case of the Metropolis-Hastings algorithm the bounds of the jumping distribution can be
adjusted so that as the temperature cools down the number of rejected jumping points does
not go up dramatically (Gelfand and Mitter 1993).
■Example 15.1:
An Illustrative Simple Simulated Annealing Setup. This is
a contrived and pathological example designed purely to show the characteristics of
the simulated annealing algorithm. Consider a 21 × 21 matrix representing a two-
dimensional discrete posterior state space.
This matrix of unnormalized posterior
density values is design to deter a standard Metropolis-Hastings chain from reaching
and describing the global maxima values in the corners by putting an attractive mode
in the middle, ringing it with compelling but nonoptimal points:

528
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
9
8
3
3
1
1
1
1
1
1
1
1
1
1
1
1
1
3
3
8
9
8
3
3
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
3
3
8
3
3
1
1
1
1
1
1
3
3
3
3
3
1
1
1
1
1
1
3
3
3
1
1
1
1
1
1
3
6
6
6
6
6
3
1
1
1
1
1
1
3
1
1
1
1
1
1
3
6
3
3
3
3
3
6
3
1
1
1
1
1
1
1
1
1
1
1
3
6
3
1
1
1
1
1
3
6
3
1
1
1
1
1
1
1
1
1
3
6
3
1
1
1
1
1
1
1
3
6
3
1
1
1
1
1
1
1
3
6
3
1
1
1
1
1
1
1
1
1
3
6
3
1
1
1
1
1
3
6
3
1
1
1
4
4
4
4
4
1
1
1
3
6
3
1
1
1
1
3
6
3
1
1
1
4
6
6
6
4
1
1
1
3
6
3
1
1
1
1
3
6
3
1
1
1
4
6
6
6
4
1
1
1
3
6
3
1
1
1
1
3
6
3
1
1
1
4
6
6
6
4
1
1
1
3
8
3
1
1
1
1
3
8
3
1
1
1
4
4
4
4
4
1
1
1
3
6
3
1
1
1
1
1
3
6
3
1
1
1
1
1
1
1
1
1
3
6
3
1
1
1
1
1
1
1
3
6
3
1
1
1
1
1
1
1
3
6
3
1
1
1
1
1
1
1
1
1
3
6
3
1
1
1
1
1
3
6
3
1
1
1
1
1
1
1
1
1
1
1
3
6
3
3
3
3
3
6
3
1
1
1
1
1
1
3
1
1
1
1
1
1
3
6
6
6
6
6
3
1
1
1
1
1
1
3
3
3
1
1
1
1
1
1
3
3
3
3
3
1
1
1
1
1
1
3
3
8
3
3
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
3
3
8
9
8
3
3
1
1
1
1
1
1
1
1
1
1
1
1
1
3
3
8
9
As a demonstration, we ﬁrst run the simulated annealing chain without any tem-
perature schedule (in other words a Metropolis-Hastings chain), so that the decision
criteria is based on exp[−(π(θ′
j) −π(θ[t]
j ))]. At each iteration the eight adjoining cells
for any presently occupied location (fewer for locations on boundaries) is equally likely
as a candidate point: a discrete uniform jumping distribution. The starting point is
randomly selected within the state space and the algorithm is run for 10,000 iterations.
The resulting distribution of visits is summarized by the matrix:
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
1
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
11
4
3
4
6
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
13 132 169
166
92
93
1
0
0
0
0
0
0
0
0
0
0
0
0
0
1
87
5
7
6
7
9
66
0
0
0
0
0
0
0
0
0
0
0
0
2
54
3
0
0
2
4
0
4
106
2
0
0
0
0
0
0
0
0
0
0
69
4
0
2
3
5
5
2
0
4
128
3
0
0
0
0
0
0
1
4
77
1
1
1
0
3
5
4
3
0
0
1
123
0
0
0
0
1
2
5
100
4
1
1
0
27
22
39
25
18
1
1
1
6
82
2
0
0
2
0
4
68
9
2
1
1
34 1681 1539 1590 25
1
1
0
6
103
3
0
0
2
0
3
62
5
2
1
1
23 1624 1573 1637 30
1
1
0
8
133
7
0
0
1
1
0
135
4
0
0
0
27 1525 1561 1588 20
0
2
2
4
179
2
1
0
1
1
9
182
3
1
0
1
36
52
28
21
16
2
2
2
9
127
4
0
0
1
1
3
6
226
6
0
1
2
2
2
0
1
1
1
5
130
5
0
0
0
1
1
0
0
4
189
6
1
1
2
3
1
2
0
1
66
6
1
0
0
0
3
1
0
0
0
10 151
6
3
1
1
1
0
2
71
6
1
0
0
0
0
0
2
0
0
1
1
1
162 12
16
5
0
0
118
1
0
0
0
0
0
0
0
0
1
1
2
1
1
14 195 175
106
81
102
0
0
0
0
0
0
0
0
0
0
0
0
1
0
1
0
3
6
6
7
1
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
2
0
1
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
3
1
0
0
0
0
0
0
0
0
It is clear that as intended, the Metropolis-Hastings algorithm missed the high modal
values in the corners but spent quite some time following the ring around the center
and exploring the nonoptimal mode area in the center, despite the fact that all of
these points are lower than the corner values. The rejection rate was 0.6014, meaning
that the algorithm was fairly ineﬃcient as well. This is an indication that the chain
chose to avoid low-density areas rather than having had the opportunity to mix well.
Now we add a temperature schedule according to the popular logarithmic choice:

Markov Chain Monte Carlo Extensions
529
Ti = log(2)/ log(i) for i = 1, . . . , 100, and sample 100 times at each of these cooling
levels. In real applications we would not be so cavalier about the number of chain
values at each temperature, and we would instead take care to observe evidence of
convergence each time (hence the reason that simulated annealing can take a long
time). This setup produces the following matrix of visits for the ﬁrst temperature
schedule point:
9
3
4
6
5
7
8
17
18
19
15
14
17
18
22
25
36
45
33
34
24
2
5
5
4
10
9
15
30
32
26
22
23
27
38
25
55
53
32
52
46
46
2
1
4
7
4
14
16
20
40
28
41
38
45
31
39
41
47
36
44
58
24
5
2
4
6
16
14
13
15
62
77
46
47
49
45
33
34
51
44
38
31
22
6
4
3
4
13
9
14
48
22
22
39
42
28
44
50
33
38
37
35
18
13
5
15
10
11
11
25
64
35
20
18
26
23
30
29
44
54
24
27
23
19
11
9
14
11
6
18
49
23
21
19
21
24
18
18
20
32
35
36
14
19
19
9
9
17
20
20
56
16
16
14
23
19
17
14
20
19
23
33
32
22
22
23
10
11
18
14
30
20
23
16
25
29
25
30
20
26
23
26
30
32
47
24
15
9
8
16
29
49
23
17
22
19
34
44
39
31
39
22
20
23
38
31
26
14
8
12
15
25
27
18
16
18
24
37
50
35
47
39
20
27
23
35
28
48
24
10
5
9
15
29
19
18
24
25
58
54
44
45
40
23
14
35
26
46
21
26
9
8
9
13
25
22
18
24
39
53
55
33
34
27
21
25
26
28
25
21
18
9
8
10
14
24
39
24
12
35
30
26
28
20
18
17
26
35
39
26
20
15
8
11
14
9
20
25
27
22
17
22
30
29
23
17
22
40
44
32
20
16
16
14
7
14
9
16
14
14
32
21
21
23
23
22
29
35
45
32
12
12
19
21
24
8
12
16
13
13
15
25
33
36
39
28
22
33
38
20
23
16
14
23
28
17
8
21
20
12
13
25
18
35
49
40
31
33
39
26
27
20
15
14
23
23
18
15
18
27
19
20
23
20
28
42
39
23
26
25
31
24
28
21
15
12
11
22
18
19
14
30
32
29
37
31
34
30
21
17
21
12
22
20
28
19
23
31
25
37
20
18
23
24
24
19
18
12
15
17
12
14
15
12
22
12
20
20
19
35
Not only does the annealing algorithm do a much better job of exploring the sample
space, it also had a rejection rate of only 0.0676 (okay here due to the small sample
space), an order of magnitude better than the version without a temperature schedule.
In addition, rejections remained rare even as the cooling levels reached back to normal
levels.
15.1.1
General Points on Simulated Annealing
A more general simulated annealing algorithm is given by Kirkpatrick, Gelatt, and Vec-
chi (1983), where the procedure is the same as that given above, but cast in more Bayesian
statistics than physics terms. Speciﬁcally, we generate candidates in general Metropolis-
Hastings fashion from any desired distribution, and we do not need the exponential metric.
The cold distribution is again given by π(θ), and we will assume a general temperature
schedule f(Tt|t). So at iteration t and temperature Tt we have the current position θ[t],
and perform the steps:
▷Generate multivariate θ′ from a candidate-generating distribution q(θ′|θ) that does
or does not account for the current position.
▷Deﬁne:
a(θ′, θ[t]) =

q(θ[t]|θ′)
q(θ′|θ[t])
π(θ′)
1
Tt
π(θ[t])
1
Tt

.

530
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
▷θ[t+1] =
⎧
⎨
⎩
θ′
with probability
min(a(θ′, θ[t]), 1)
θ[t]
with probability
1 −min(a(θ′, θ[t]), 1)
▷Update Tt to Tt+1 from the temperature schedule.
Notice that this is a straightforward extension of the Metropolis-Hastings algorithm.
Good overviews of simulated annealing are given by Bertsimas and Tsitsiklis (1993),
Brooks and Morgan (1995), and H`ajek (1985). Neal (2001) connects importance sampling
(Chapter 9) to simulated annealing as a way to create importance sampling distributions
in high dimensions where it would otherwise be diﬃcult to get reasonable acceptance rates.
Other extensions exist and simulated annealing continues to be an active research area in
terms of both theoretical development and applied work.
A number of authors have worried about the various conditions necessary to assert
convergence (Ferrari, Frigessi, and Schonmann 1993; Gidas 1985; Locatelli 2000; Lundy
and Mees 1986, Szu and Hartley 1987) generally by restricting the form of the domain or
specifying a uniform minorization condition for a speciﬁc chain. As mentioned, the time to
run useful simulated annealing programs can be long. Several works look at estimating this
time and how it might be improved (Chiang and Chow 1988; Ingber 1992; Tsitsiklis 1988;
Mitra, Romeo, and Sangiovanni-Vincentelli 1986).
Applications are usually motivated by a need to optimize ill-behaved functions, often
as a reasonable alternative to exhaustive searches of the sample space (e.g., Bohachevsky,
Johnson, and Stein 1986; Kollman, Miller, and Page 1997).
Explicitly Bayesian works
include Bernardo (1992), van Laarhoven et al. (1989), Geyer and Thompson (1995), and
Press et al. (1986).
15.1.2
Metropolis-Coupling
With high-dimensional and multimodal objective functions (posteriors) of interest, the
candidate distribution and the temperature schedule in simulated annealing must be chosen
with great care to allow adequate exploration of the space. Unfortunately it is possible to
stipulate wildly inappropriate choices of both, thus preventing convergence or mixing with
simulated annealing. One early solution to this problem is Metropolis-Coupled Markov chain
Monte Carlo (MCMCMC) (Geyer 1991). This algorithm is characterized by the steps:
▷Run N parallel chains at diﬀerent heat levels from m1 to m1/βN, where the tempera-
ture values have the characteristic: β1 = 1 < β2 < . . . < βN.
▷Thus N transition kernels are deﬁned, K1, K2, . . . , KN.
▷At time t select two chains, i and j, and attempt to swap states:
θ[t]
i ⇐θ[t]
j ,
θ[t]
j ⇐θ[t]
i ,
▷with a Metropolis decision, probability:
min
4
1,
mi(θ[t]
j )mj(θ[t]
i
mi(θ[t]
i )mj(θ[t]
j )
5
.

Markov Chain Monte Carlo Extensions
531
▷Record only the cold chain, m1, for inferential purposes.
The key advantage to MCMCMC is that chains that get stuck in non-optimal maxima will
eventually get swapped-out to some other, presumably more free, state. A notable disad-
vantage, though, is the need to possibly run many parallel chains for problems with highly
complex targets. Also, it is possible to run parallel chains concurrently using additional
software such as snow (Olivera and Gill 2011).
15.1.3
Simulated Tempering
Marinari and Parisi (1992) and (independently) Geyer and Thompson (1995) propose an
alternative algorithm called simulated tempering, which reduces the MCMCMC algorithm
to a single chain. Essentially the temperature itself becomes a random variable so the system
can heat and cool as time proceeds. Why would one want do this in a simulated annealing
process? Now elderly chains can still avoid being trapped at local maxima by getting more
general Metropolis-Hastings candidate positions. That is, step 1 of the simulated annealing
algorithm above is replaced with:
1a. Generate β from some distribution of temperature, f(β).
1b. Generate θ′
j ∼

θ[t′]
j
+ N(0, σ2)

.
The number of f(β) choices is obviously vast, but this decision can be simpliﬁed by using
a discrete distribution that resembles some desired, but not implemented, cooling schedule.
Often this can be rigged to provide a large number of cold draws. The algorithm above
can also be thought of as an augmented sampler in the context of Tanner and Wong (1987)
where temperature is the augmentation variable.
Temperature T=1
Temperature T=25
Temperature T=300
FIGURE 15.1: Simulated Annealing Applied to the Witch’s Hat Distribution
Figure 15.1 shows the infamous witch’s hat distribution centered at [0.6, 0.4] (see Ex-
ercise 15.5 for details) which has been suggested as an MCMC diagnostic since it causes
serious mixing problems with various algorithms, particularly in high dimensions. Geyer
and Thompson (1995) apply simulated tempering to improve the mixing problem caused
by the diﬃculty of moving from the brim area to the peak, and obtain an acceptance rate

532
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
improvement from 7% to 72%. The panels of Figure 15.1 show the “cold” distribution at
T=1 and heated up versions at T=25 and T=300 plotted on the unit square. Notice how
the troublesome crease at the base of the peak is melted down.
15.1.4
Tempored Transitions
Neal (1996) builds on simulated tempering with tempered transitions to heat up the
posterior distribution in-place so that a random walk can move more freely, but also to pre-
serve the detailed balance equation (reversibility condition) at each step.
See also Celeux,
Hurn, and Robert (2000) for an application to mixture distributions, and Liu and Sabatti
(1999) for the “simulated simpering” variant. The basic idea is to “ladder” up and down in
heat at each time t with random walk steps. Each ladder step speciﬁes a (non-normalized)
stationary distribution deﬁned on the same state space but at progressively hotter temper-
atures going up. Finally the last (bottom) ladder value is accepted or discarded with a
Metropolis decision. This process is summarized by:
▷τ1 is the target joint density,
▷βi is the temperature value at the ith ladder step,
▷tempered transitions: deﬁne a sequence of candidate densities mi, i = 1, . . . , N, where
as i increases the mi get “ﬂatter” going up the ladder, then again more peaked going
down the ladder.
▷parameterize: mi = m1/βi,
▷where: 1 < β1 < β2 < · · · < βN−1 < βN
▷then: βN > βN+1 > · · · > β2N−2 > β2N−1 > 1.
▷Starting from the original candidate m, at each step we cycle through the mi as
follows:
1. If we let K(θ, m) denote an MCMC kernel with position θ and stationary dis-
tribution m,
2. then we use the following transitions starting at iteration t:
step 0:
θ′
1,0 ∼K(θ[t]
1 , τ1)
step 1:
θ′
1,1 ∼K(θ′
1,0, m1)
...
step N:
θ′
1,N ∼K(θ′
1,N−1, mN)
step N+1:
θ′
1,N+1 ∼K(θ′
1,N, mN−1)
...
step 2N-1:
θ′
1,2N−1 ∼K(θ′
1,2N−2, m1).

Markov Chain Monte Carlo Extensions
533
3. The sequence of θ1 values is then input into a ﬁnal Metropolis-Hastings accep-
tance step, accepting θ′
1,2N−1 as θ[t+1]
1
with probability:
min
4
1, m1(θ[t]
1 )
τ1(θ[t]
1 )
. . . mN(θ′
1,N−1)
mN−1(θ′
1,N−1) . . . τ1(θ′
1,2N−1)
m1(θ′
1,2N−1)
5
,
which preserves the detailed balance condition.
To look at this in a slightly diﬀerent way, we can also substitute the βi parameterization
back in. Now accept θ[t]
1,2N−1 as θ[t+1]
1
with probability:
min
4
1,
*
m1/β1(θ[t]
1,0)
m1(θ[t]
1,0)
, *
m1/β2(θ[t]
1,1)
m1/β1(θ[t]
1,1)
, *
m1/β3(θ[t]
1,2)
m1/β2(θ[t]
1,2)
,
. . .
*
m1/βN−1(θ[t]
1,N−2)
m1/βN−2(θ[t]
1,N−2)
, *
m1/βN (θ[t]
1,N−1)
m1/βN−1(θ[t]
1,N−1)
, *
m1/βN+1(θ[t]
1,N)
m1/βN (θ[t]
1,N)
,
. . .
*
m1/β2(θ[t]
1,2N−3)
m1/β3(θ[t]
1,2N−3)
, *
m1/β1(θ[t]
1,2N−2)
m1/β2(θ[t]
1,2N−2)
, *
m1(θ[t]
1,2N−1)
m1/β1(θ[t]
1,2N−1)
, 5
.
Note that θ[t]
1,0 = θ[t]
1 .
We can also add a weighting function within each term above:
w(θ[t]
1,0)
w(θ[t]
1,2N−1) (sometimes called a pseudo-prior in this context). The original stationary dis-
tribution of the Markov chain is maintained as long as the mi satisfy a detailed balance
condition, which is given in Neal (1996) with proof.
This sequence of transitions allows excellent exploration of the parameter space, as the
density mN is typically chosen as very “hot,” for example, uniform on the entire space.
Setting βi −βi+1 as small gives higher acceptance rates but poorer mixing. Conversely a
large diﬀerence between m and m1/β is good for mixing around the space but may lead to
inordinately high rejection rates. Both criteria can be satisﬁed with taller ladders: more
steps and a higher maximum temperature.
15.1.5
Comparison of Algorithms
It is interesting to compare these approaches with a deliberately “ugly” example objec-
tive function on [−1, 1]2:
f(x, y) = abs((x sin(20y −90) −y cos(20x + 45))3a cos(sin(90y + 42)x)
(15.2)
+ (x cos(10y + 10) −y sin(10x + 15))2a cos(cos(10x + 24)y)).
(15.3)
This function is displayed in Figure 15.2. While the vast majority of posterior forms will not
exhibit such challenging characteristics, it still serves as a benchmark for algorithm perfor-
mance. Furthermore, increasingly complex model speciﬁcations are much more prevalent in

534
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
FIGURE 15.2: A Highly Multimodal Bounded Surface
recent Bayesian and non-Bayesian work in the social sciences, leading to potentially similar
forms. This function, while bounded, presents problems for conventional MCMC algorithms
since it has multiple modes concentrated at the corners and a wide, ﬂat plain in the middle
that must be traversed to fully describe the functional form.
We now apply a regular random walk Metropolis-Hastings algorithm, simulated anneal-
ing, and tempered transitions to this function. Each Markov chain is run for 5,000 iterations
(an insuﬃcient but illustrative period), and the chain visits are given in Figure 15.3. Such
a number of iterations is revealing here because, while all of these algorithms are ergodic
and will therefore eventually explore the full target form, our concern is with the rate at
which they do so. Speciﬁcally, do the algorithms diﬀer in the eﬃciency by which they mix
through the space?
It is clear that the standard algorithm fails during this period to break out of the
diagonal, and spends the bulk of its time visiting two of the four corners where large modes
exist. The simulated annealing algorithm appears to be traversing the space much better
but still fails to break out of the diagonal area. Conversely, the algorithm based on tempered
transitions manages to explore the full state space, even in this small number of iterations.
So the leverage that we gain from using the tempered transitions is greater assurance that
we have mixed through the target form in a ﬁxed amount of time.

Markov Chain Monte Carlo Extensions
535
+
+
+
+++++
+
+
+
+
+ +
+
+
+
+
+
+
+ +
++
+
+
+
+
++
+
+
+
+
+
+
+
+
+ +
+ +
++++
++
+
+
+
+
+
+
+
+
+
+
+
+
+
+
++
+ ++
+
+
+
+
+
+++
+
+
+++
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
++
+
+
+
+
++
+
+
+
+
+
+
+
+
+
+
+
+
+ +
+
+
+
+++++ +
+++
+
+
+
+
+ +
+
++
+
+
+
+
+ +
+
+
+
+
+
+
+
+ +
+
+
+
+
+
+
+
+
+
+
+ +
+ +
+
+++
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
++
++
++
+
+
+
+
+
+
+
+
+
+
+
+
++
+
+
+
+
+
+ +
+
+ +
+
++
+
+ ++
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
++
+
+
+ +
+ +
+
+
++
++
++
+
+
+++
+
+
+
+
++ +
+ +
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+++++
+++
+
+ +
+
+
+
+
++
++ +
+
++
+
+
++
+++
+
+
+
+
+
+
+
+
++
+
+
++
+
+
+
+
+
+
+
++ +
+
++++
+++
+
+
+
+
+++
+
+
+
+ +++
+
+
+
+
+
+
+
+
+
+
++
+
+
+
++
+
+
+
+
+
+
+
++
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
++
+
+
++
+
+
+
+++
+
+
+
++
+ +
+ +
+
+
+
+
+
+
+
+
+
+
+
+
+
++ ++
+
+
+
+
+
+
+
+
++
++
+ +
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
++++
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+ +
+
++
+++
+
+ +
+
+
+
+
+
+
+ ++
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+ ++
+
+
+
+
+
++
+
+
+
+
+
+
+ +
+
+++
+
+
++
+
+
+
+
+++
++
+
+ +
+
+
++
+
+
+
++
+
+
+
++
+
+
+
+
+
+
+
++
+
+
++
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
++
+
+
+ +
+
++ +
+
+
++
+
+
+
++++
+
+
+
+
++
+
+
+
+
+
+
+
+
+
+ ++
+
+
++
++
+
+
+
+
++
+
+
+
+
+
+ ++
+
+
+ +
+
+
+ +
+
+
+
++
++ +
+
+
+
+
+
++
+
+
+
+
+
+
+ +
+
+
+ ++
++
+
+
+
+
+
+
+
+
++
+
+ +
+
++
+
+++
+
+
+
+
+
+
+
+
+
+
++
+
++
+
+
++ +
+ +
+
+
+
+
+
+ +
+ +
+
+
+
+
+
+
+
+
+
++
+
+
+
+
+
+
+
+
+
+
+
+
+
+
++
+
+++
+
+
+ +
+
+
+
+
+
+
+
++
++
+
++++
++
+
+
+
+
+
++
+
++
+
++
+ ++
+
+
+
+
+
++ +
+
++
+
+
+
+
+
++ +
++++
++
++
+
++
+
+
+
++
+ +
++
+
+
+
+
+
+
+
+
+
+
+
++
++
+
+
+
+
+
+
+
+
+
+
+
++
+
+
++
+
+
++
+
+
+
+
+
++ +
+
+
+
+
+
++
+
+
+
++ +
+
+
+
+
++
+
+
+
+ +
+
+++
+
++
++ +
+ +
+
+
++
+
+
++
+
++
+
+
+
+
+
+
++
+
+
+
+
+
+
++
+++
++
+
+
+
+
++
++
++
++
+
+
+
+ +
+
+
++
+ +
+
+
++
++
+
++
+
+
+
+
+
++ +
+
+
+
+
+ ++
++++
+
+
+
+
+
+
+
++
++++
+
++ +
+++
+++
++
+
+
+
+
+
++
+
+
++
+
+
+
+
+
++
++
+ +
++
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+++
+
+
+
+
+
+
+
+
+
+
+
+
++
+
+
+
+
+
+
++++++
+
+++
++
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+ ++
+
+
+ +
+ +
+
+
+
+
+
+
+
+
+
+
++
+ +
+
+
+
+
+ +
+
+
+
+
+
+
+
+
+
+
+ +
+
+
+
+
+
+
+
+ +
+
+
+
+
++
+
+
+
+
+
+
+
+
+
+
+
+
+
+ ++ +
+
+
+
+
+
+
++
++
++
+
+
+
+
+
+
+
+
++
+
+
+ +
+
+
+
+
+
+
+
+++++
+
+
+
+ +
+
+
++
+++
++
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
++
+
++
+
++++
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
++
+++
+++
+
+
+
+
+
+
+
+
+ +
++
++
+
++
+
+
+
+
+
+
+
+
+
+
++
+
+++ +
+
+
+
+
+
++
+
+
+
+
+
+ +
+
+
+
+
+
+
+
+
++
+
+
+
+
+
+
+
+
+
++
++
+
+
+
++
++
+
+
+
+
+
+
+
+
+
+
+
+
+
+ ++
+
+
+ +
++
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
++
+
+
+
+
+
+
+++
+
+ +
+
+
+
+
+
++
+
+
+
+
++
+++ +
+ ++
+
+
+
+
+
+
+
+
+
+
+ +
+
+
+
+
+
+
++
+
++
+ +
+
+
+
+
+ ++
+ +
+
+
+
+
+
+
+
+
+
+
+
++ +
+
+
+
+
+
+
+
+
+ +
+++
+
+
+
+
+
+
+
+ +
+
++
+
+
+
+
+
++
+
++
+
+
+++++
+
+
+
+ +
+
+
+
+
+
++
++++
+
+
++
+
+
+
+
+
+
+
+
+
+
+
++
+ +
+
+
+
++
+
+
++
+
++++++
+++
+
++
+ +
+
+
+
+
++
+
++
+
+
+
+
+
+
+
+
+
+
+
+
+
+
++
+
+
+
+
+
+
+
+
+
+
++
+
+
+
++
+
+ +
+
++
+
+
+
+
+
+
+
+
+
+
+
+
+ +
+
+
+
+
+
+
+
++
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+ ++
+
+
+
+ +
+
++
+
++
+
+
+
++++
+
+
+
+
+
++
+
+
++
++
++
+
+
+
+
++
+
+
++
+
+
+
++
++
+
++ +
+
+
++
+
++ +
+
++
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+ +
+
+
+
++
+++
+
+
+
++
+
+
+
+
+
++
+++
++
+
+ +++
+
+
++
+
+
+
++
+++
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+ +
+
+
+
+
++
+
+
+
+ +++
+
+
+
++
+
+
+
++
+++
++
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+ +
+
+++
+
+
+
+
+
+
+
++
+
+
+ ++
+
++
+
+
+
+
+
+
+
++
+
+
+
+
+
++
+ +
+
+
++
+
+
++++
+
+
+
+
+++
+
+
+
+
+
+
+
+
+
+
+
+
++
+
+
+
++
+
+
+
+
+
+
+
+
+
+
+ +
+ +
+
+
++
+
+
++
++
+
+
+
+
+
+
++
+
+
+
++
+
+
+
++
+ +
+
++
+
+
+
++
++
+
+
+
+
+
+
+
+
+
++
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
++
+
++
+
+
+
+
+
+
+
+
+
+
+++
+
+
+
+
+
+
+
+
+
+++
+ ++
+
+
+
++++
+
+
+
+
+
+
+ +
+
+
+
+
+
+
+
+
+
+
+
+
+++
++
+
+
+
+ +
+
++
+
+
+
++++
+
−1.0
−0.5
0.0
0.5
1.0
−1.0
−0.5
0.0
0.5
1.0
With Regular Metropolis
+
+
+
+++ ++
+
+
+
+
+
+
+
+ +
+
+ + +
+++
+
+
+
+
++
+ +
+
++ +
+
+
+
+
+
+
++
+ ++
+
+
+
+
+
++
+
+
+
+ +
+
+
+
+
+ ++
+
++
+
+
++ +
+
+++
++ +
+
+
++
+
+
+
+
+ +
+
+
+
+
+
+
+
+ +
+
+
+
+
+
+++
+
+++
+
+
+
+
+
+
+
+
+
+
+
+
+
+ +
+
+
+ +
+
+
+
+
+
+
+
+ +
+
+
+
+
+
++
+
+
++
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
++
+
+ +
+
+
+
+
++
+
+
+
+
+
+
+
+++ +
+
+
+
+
+
++
+
+
+
+
+
+
+
+
+
+
+
+
+
+ +
+
+
+
+
++
+
+
+++
+
++
+
+
++
+
+
+ +
++ +
+
+
+
+
+
+
+
+
+
+
+
+
+ +
+
+
+
+
+
+
+
+
+
+
+ +
+ + +
+
+
+
+
+
+
+
+ +
+
+
+
+
+
+ ++
+
+
+
+
+
+
+
++
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
++++
+
+
++
++++++++
+
+
+
+
+
++
+
+
+
+
+
+
++
+
+
+
+
+
+
+++ +
++
+
+
+
+
+
+
+
+ +
+
+
+
+
+
+
+
+
++
+
++
+
+
+
+
+ +
+
+
+
+
+
+
+ +
++
+
+
++
+
+
+
+
+
+
+
+
+
+
+
+
+ +
+ +
+
+
+
++
+
++
+
+
+
+
++
++
+
+
+
+
+
+
+
+
+
+
+
+
+
++
+
+ +
+
+
+
+
+
+
+++
+
++
+
+
+
+
+
+
++
+
+
+
++
+
+
+
+
+
+
+
+
++
+
+
+
+
+
+
++
+ ++
+
+ +
+
+
+
++
+
+
++
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
++
+
+
+
+
+
+
+
+
+ +
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
++
+
+
+
+
+
+
+
+
+
++
+
+
+
+
+
+
+ +
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+++
++
+
+
+
+++
+
+
+
++
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
++
+
+
+
++
++
+ +
+
+
+
+
++
+ +
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
++
+
+
+
+
+
+
+ +
+
+
+
+
+
+
++
+ +
+
+
+
+
++
++
+
+
+
++
+
+++
+
+
+
+
+
+
++
+
+
+
+
++
+
+ +
+
+
+
+ ++
+
+
+
+
+
+
++
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+++
++
+
+
+
+
+
+
+
+ +
+
+
+
+
+
+++++
+
+
+
+
+
+
+
+
+
++
+ +
++
+
+ +
+
+
+
+ +
+
+
+
+
+
+
++
+
++
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+ + +
+
+
+
+ +
+
+
+
++
+
+
+
+
+
+ +
+ +
+
+
+
+
+
+
++ +
++
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
++
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
++
+
+
+
+
+
+
+ +
+
+
+
+
+
++
++
+
+
+
++
+
+
+
+
+
+
+
+
+
+
+++
+
+
+
+
+
++
+
+
+ +
+
+
+
+
+
+
++
+
+
+
++
+
+
+ +
+
+
+
+
+
+
+
+ +
+
+
+
+
+
+
+
+
+
+
+
+ +
+
+
+
+
+
+
+
+
++
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+ +
+
+
+
++
+
+
+
+
++
++
+
+
+
+
+
+
+
+
+
++
+
+
+
+
+
+
++ +
+
+
++
++
+
+
+
+
+
+ +
+
+
++
+
+
+
+ +
+
+
+
+
+
+
+
+
++
+
+
+
+
+
+
+
+
+
+
+
+
++
+
+
+
+
+
+
+
+
++
++
++
++ +
+
++++ +
+
+
+
+
+
+
+
+
++
+
+
+
+
++
++
+
+
+
+
+
+
+
+++
+
+++ +
+
+
+
+
+
+
+
+
+
+
+++
+
+
+ +
+
+
+
+
+++
+
+++
+
+
+
+
+
+
+
+
++
+
+
+++
+
+++++
+
+
+ +
+
++
+
+
+
+
+
+
+
++
+
+
++ ++
+
+
+
+
+
+
+
+
+
+
+
+
+
+ +
++
+
+ ++
+
++
+
+
+
+
+
+
++
+
+
+
+
++ +
+
+
+
+
+
+
+
+
+
+
+
+
+
+ +
+
+
+
+
+
++
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
++
+
+
+
+
+ +
+
++
+
+
+
++
++
+
+
+
+
+
+
+
+
+
+
+
+
+++
+
+
++
+
+
+
+
+
+
+
+
+
+
+
+
++
+
++ +
+
++
+
+
+
+
+
+
++
+
+
+
+
+ + +
+
+
+
+
+
+
+
+
+
+
+
++
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
++ +
++
+
+
+
+
+
+
+
++
+
+
+
+
+++
++
+
+
+
+
++
+
+
+++
+
++
+
+
+
+
+
+
+
+ +
+ +
+
+
++
+
+
+
+
+
+
+
+
+
+
+
++
++
+
+
+
+
+
+
+
+
+ +
+
++
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+ +
+
+
++
+
+
+
+
+
+
++
+
+
+
+
+
+
+
+
+ +
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+ +
+
+
+
+
++
+
+
+
+
+
+
+
+
+
++
+
+
+
++
+
+
+
+
+
+
+ +
+
+
+
+
++
+
++
+
++
+
+
+
+
+
+
+
+
+
++
+
+
+
+
+
+
++
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
++
+
+
+
+++
+
+
+
+
+ +
+
+
++
+
+
+
+
+
++
+
+
+
+
+ +
+
+
+
+
+
+
+
+
+
+
+
+
+
+
++
++ +
+
+
++
+
+
+
+
+
+ +
+
+
+
+
++
+
+
+
+
+
+
+
++
+
+
+
+
+
+
+
+
+
++
+
+
+
+
++
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
++
+
+++
+
+
++
+
++
+ +
+
+
+ +
+
+
+
+
+
+
+
+
+
+
+
++ ++
+
+++
+
+
+
+
+
+
++
+
+
+
+
+
+
+
+
+
+
+
+ + +
+
+
+
+
+ +
++
+
+
+
+
+
+
+
++
+
+
+
+
+ +
+
+
+
+
+
+
+
+
+
+ +
+
+
+
+
+
+
+
+
++
+
+
+
+ +
++
++
+
++++
+
+
+
++
+
+
+
+
+
+
+
+
+
+ +
+
+
+
+
+
+ ++
+
++
+
++
+++
++
+
+ +
++
++ +
+
+
+
+
+
+
+
++
++
++
+++
+
+
+
+ ++
+
+
++
+
+
++
++
+ +
+
+
+
+
+ +
+
+
++
+
+
+
++
+
+
+
++
+
+
+ +
+
+
+
+ +
+ +
+
+
++
+
+
+++
+
+
+
+
+
+
+
+
+
+
++
+
++++
+
+
+
+
+
+
+
+ +
+
+
+
+ +
+
+
+
+
+
+
+
+
+
+
+
+
+
+
++
+
+
+
+
+ +
+
+
+
+ +
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+ +
+
+
+
+
+ +
+
+
+
+
+
+
+
+
++
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+++
+ +
+
+
+
+
+
+
+
+
+
+
+ +
+
+
+
+
+
++
+
+
+
+
+
+
++
+
+
+
+
+
+
+
+
+
++
+
+
+
++
+
+
+
+
+
+
+
+ ++
++
+
+
+
+
+
+ +
+
++ + +
+
+
+
+++
+
++
+
+
+
+
+
++
+
+
+ +
+
++
+
+
+
+
+
+
+
+
+
+
++
++
+
++
+++
+
+
+
+
+
+
+
+
+
+
+
+ +
+
+
+
+
+
++
+
+
+
+
+
+
+
+ +
+
+
+
+
+
+
+
+
+
+
+ +
+
+
+
+
+
++
+
+
+
+
+
++
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
++++
+
+
+
+
++
++
+
+
+
+
+
+
+
+ +
+
+
+
++
+
+
+
+
+
+
+
++
+
+
+
+
++
+
+
+
+
+
+
+ +
++
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
++
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+ +
+
+
+
+
+
+
+
+
+
+
+
+
++
+
+
+
+
+
+
+
+
+
+
+
+
+
+
++
+
+
+ +
+ +++
+
+
+
+
+
+
+
+
+
+++
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
++
+
+
+
+
+
+
+ +
+
+
+++
+ +
+
+
+
+
+
+++
+
+
+
+
++
+
+
+
+
++ +
+
+
+
+
+++
+
+
+
+
+
+
++
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+++
+
+
++++
+
+
+
++
+
+
+
+
+
+
+
+
+ +
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
++
+
++
+
+
+
++
+
+
+
+
+
+
+ ++
+
+
+
+
+
+
+++
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+ +
+
+
+
+
+
+
+
+
+
++
+
+
+
+
++
+++++
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
++
+
+
+
+
+
+
+
+
+
+
+
+
+
++
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
++
+
+
+
++
++
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
++
+
+
++
+
+
++
++
+
+
++
+
+
+++
+
+
++++
+ +
+
+
+
++
+
+
+
+
+++
++
++
+
+
+
+ +
+
+
+
+ +
++++
++
++
+
++
++ +
+
+
+
++
+++
+ +
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
++
+
+
+
++
+
+
++
+
+
+
+
+
++
+
+
++
+
+
+++
+++
+
+
+
+
+ +
+
+
+
+
++
+
+
+
+
+
+
+
+ ++
+
+
+
+
+
+
+
+
++ +
+
+
+
+
+ +
+ + +
+
++
+
+
+
+
+
+
+++
++
+
++
+
+
+
++
+
+
+
+
+
+
+
+
+
+
+
+
+
+ + +
+
+
+
+
+
+
+
+
+
+
+ +
+
+
+
+
+
+
+
+
+
++
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
++
+
+
+
+
+
+
++
+ + +
+
+
+
+
+
+
++
+
+ +
+
+
+
+
+
+
+
+
+
+
+
++
+
+
+
++
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
++
++ +++
+ ++
+
+
++
+
+
+
+
+
++ +
++
+ +
++++
++
+
+
+
+
+
+
+
+ + +
+
+
++
+
+
+
+
+
+
+ +
+
+
+
+
+
++
++
+
+
+
+
++
+
+
+
+
+
+
+
+
+
++
+ +
+
+
+
+
+
+
+
+
+
+
+
+++
+
+
+
+
+
+
+
+
+
+
++
+
+
+
+
+
+
+
+
+
+
+
+
+
+ +++
+
+
+
+
+
+
+
+++
+
+
+
+++
+
+
+
+
+
+
+
+
++++
+
++
+
+ +
+
+
+
+
+
+
+
++
+
+
+
+
+
+
+
+
+
+ +
+
+
+
+
+
+
+
++
+
+
+
+
+
+
+
+
+ +
+
+
+
+
+
+
+
+
+
++
++
+
+
++++
+
+
+
++
+
++
++
+
+
+
+++
+
+
+
+
+
+
+
+
+
++
+
+
+
+
+
+
+
+
+ ++
+
+
+
+
+
+
+
+
+
+
+
+
+
+
++
+
+
+
+
+ +
+
+
+
++
+
+
+
+
++
+
+
+
+
+
+
+
+ +
+ +
+
+
+
+
+
+
++
+
++
+
+
+
+ +
+
+
+
+
++ +
+++
+
+
+
+
+
+
++
++
+
+
+
+
+
+
+
+
+
+
+
+
+
++
+
+
+
+
+
+ +
+
+
+
++
+
+
+
+
+
+
+
+++
+
+ ++
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+ +
+
+
+
+
+
+
+
++
+++
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+++
+
+
+ +
+
+
+
+
+
+ +
+
+
+
+
+
++
+
+
+++
+
+
+
+
++
+
+
+
+
+
+
+
+
+ +
+
+
++
+
+
+
++
+
+
+
+++
+
+
+
+
+
+
+
+
+
+
+
+
+
+
++
+
+
++++
+
+
+
+
+ +
+
+
+
+
+++
+
+
+
+
+
+
+
+
+
+
+
+
+
+++
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+ +
+
+
+
+
+
+
++
+
++
+
+
+
+
+
+
++
+
+
+
+
+
+
+
+
+
+
+
+
+
+++
+
+
+
+
++
+
+
+
+
+ +
+
+ +
+
+
+
+
+
+
+
+
+
++
+
+
+
++
+
+
++
+
+
+ +
+
+ +
+ + +
+
+
+
+ +
+ +
+
+
+
+
++
++
++
+
+
+
+
+
+
+
+
+
+
+
+
+
+
++
+
+ +
+
+
++
+
++
+
+
+
+
+
+
+
++
+
+
+
+
+++
+
+
+
+
+
+
+
+
+
+
+
+++
+
+
+
+
+
+
+
++
+
+
+
+
+
+ +
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
++++
+
++
+
+
+
+
+ +
+
+
++
+
++ +++
+ +
+
+
+
+
+
+
+
+
+
+
+
+
+
+ +
+
+
+
+
+ +++
+
+
+
+
+
+
+
+
+
+
+
+
+
+ +
+
+
++
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+ +
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
++
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
++
+
+
+
+
+
+
+
++
+
+
+
+
+
+
+
+
+
++
+
+++
+
+ +
+ +
+
+++
++
+
+
+
+
+
+
+
+
+
+ +
+
+
+
+
+
+
+
+
+
+
+
++
++
+
+
+
+
+
+
+
+
+
+
+
+
+
+
++
+
+
+
+
+
+
+
+
+
+
++
++
+
+
+
+
+ +
+
+
+
+
+
+
+
+
+
+
+
++
+ +
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
++++
+
+
+
+
+
+
+
+
+
+
+++
+ +
+
+ +
+
+
+
++
+
+
+
+
+
+ +
+
+
+
+
+
++
++
+
+
+
+
+
+
+
+
+ +
+
++
+
+
+
+
+
++
+
+
+
++
++
+
+
+
++
+
+
+
+
+
+
+
+
+
+
+
++
+
+
+
+
++
+
+
+
+
+
+
+
+
+
+
+ +
+
+
+
+
+
+
+
++
++
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
++
+ ++
+
+
+
+++
+
+
+
+
++
+++
+
+
+
+
+
+
+
++
++
+++
+
++
+ +
++
++
+
+
+
+
+
+
+
++
+
+
+ +
+
+
+
+
+
++
+
+
+
+
+
++
+
+
+
+
+
+
+
+
+
+
+
+
+
+
++
+
+
+
+
++ +
+
+
+ +
+
+ +
+
+
++
++
++
+
++
++
+
+
+
+
+
+
+
+
+
+
+
+
+
+
++
+
++
+ +
++
+
+
++
+
+
+
+
+
+
+
+
+
++
+
+
++ +
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
++
+
+
+
+
+
+
+
+
+
+
+
+
+ +
+
+
++
+
+
+
+
+
+
+
++
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
++
+
+
++
+
+
+
+
+ +
+++
+
+
+
+
++
+
+
+
++
+
+
++
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+++
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
++
+
+
+
+
+
+
+
+ +
+
+ +
+
+
++
+
+
+
+
+
+
+
+
+
+
+
+
+
++
++
+ +
+
+
++
++
+
+
+
+
+
+
+
+
++
+
+
+
+
++
+
++
+ +
++
+
+
+
+
+
+
+
+
+
+
+
+
+
++
+
+
+
+
+
+
+
+
+
+
++ +
++
+
++
+ +
++
+
+
+
+
+
+
++
+
+
+
+
+
+
+++ ++
+
+ +
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+++
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
++
+
+ +
+
+
++
+
+
+
+
+
+
+
+
+
+
++ +
+
+
++++
+
+ +
+
+
+
+
+
+
+
+
+
+
+
+
++
+
+
+
+
+
+
+
++
+
+
+
+
+
+
+ +
+
++
+ +
+
++ +
+
+
+
+
+ +
+
+
+ +
++
+
+
++
+
+
+ +
+
+++
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
++
+
+
+
+
+
+
+ +
+
+
+
++
+
+
+
+
+
+ ++
+
+
+
+
++
+
+
+
+
+ ++
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+++
++
+
+ +
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+ +
+++
+
+
+++
+
+
+
+
+
+
+++++
−1.0
−0.5
0.0
0.5
1.0
−1.0
−0.5
0.0
0.5
1.0
With Simulated Annealing
+
+
+
+
+
+
+
+
+
+
+
+
+
+
++
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+ + +
+ +
+
+
+
+
++
+
+
+
+
+
+
+
+
+
+
+
+
++
+
+
+
+
+
+
+
+
+
+
+
++
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+ +
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+ +
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
++
+
+ +
+
++
+
+
+
+
+
+
++ +
+
+
+
+
+
+
+
+ +
+
+
+
+
+
+
+
+
+
+
+
+
++
+
+
+
+
++
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
++
+
+
++
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
++
+
+
+
+
+
+
+
+
+
+
+
+
+
++
+
+
+
+
+
+
+
+
++
+
++
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
++
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
++
++
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+ +
+
+
+
+
+ +
+
+
+
+
+
+
+
+
+ +
+
+
+
+
+
++
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
++
+
+
+ +
+
+
++
++
+
+
+
+
+
+
+
+
+
+
+
+ +
+
+
+
+
+
++
+
+
+
+
+
++
+
+
+
+
+
+
++
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
++
+
+
+ ++
+
+
+
+
++
++
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+ +
+
+
+
+
+
+
+
+
+
+
+
+
+ +
+
++
+
+
+
+
+
+
+
+
+ +
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+ +
+
+
++
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
++
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
++
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+++
+
+
+
+
+
+
++
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
++
+
+
+
+
+
++
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
++
++
+
+
+
+
+
+
+
+
+
+
++
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
++
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+ +
+
+
+
+
+
+
+
++
+ +
+
+
+
+
+
++
+
+
+
+
+
+
+ +
+
+
+
+
+
+
+
+
+
+
+
+
+
+
++
+
+
+
+++
+
+
+
+
+
+
+
+
+
+ +
+
+
+
+
+
+
++
+
+
+
+
+
+
+
+
+
+
+
+
+
+
++
+
+
+
+
+
+ +
+
+
+
+
+
+
+
+
++
+
+
+
+
+
+
+
+
+
+
+
+++
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
++
+
++
+
+
+
+
+
+
+
+
+
++
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
++
+
+
+
+
+
+
+
+
+ +
+
+
+
+
+
+
+
+
+
+
+
+
+ +
+
+
+
+
+
+ +
+
+
+
+
+
+
+
+
+
+
+
+
+ +
++
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+ +
+
+
+
+
+
+
+
++
+
+
+
++
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
++
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
++
+
+
+
++
+
+
+
+
+
++
+
+
+
+
+
+
+
+
+
+
+
+
+
++
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+ + +
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
++
+
+
+
+
+
+
+
+
+
+
+
+
+ +
+
+
+
+
+
+
++
+
+
+
+
+
+
+
+
+
+
+
+
+ +
+
+
+ +
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
++
++
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+ +
+
++
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+ ++
+
++
+
+ + +
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+ +
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
++
+
+
+
+
+++
+
+
++
+
++
++
+
+
+
+
+
+
+
++
+
+
+
+
+
++
+
+
+
+
+
+
+
+
+
+
+ +
+
+
+
+
+
+
+ +
+
+
+
+
+
+
+
+
+
+
+
+
+
+ +
+
+
+ +
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
++
++
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+ +
+
+
+
+
+
+ ++
+
+
+
+
+
+
+
+
+
+
+
+
++
+
+
+
+
+
+
+
+ +
+
+
+
+
+
+
+
+
+
+
++
+
+
+
+
+
+
++
+
+
+
+
+
++
+
+
+
+
+
+
+
++
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
++
+
++
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
++
+ +
+
+
+
+
+ +
+
+
+
+ +
+ +
+
+
+
+
+
+
+
+
+
+
+
++
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+ +
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+ +
+
+
++
+
++++
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
++
+
+
+
+
+
+
+
+
+
++
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
++
+
+
+
+
+
++
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
++
+
+
+
+
+
+
++
+
+
+
+ +
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
++
+
++
+
+
++
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
++
+
+
+
+
+
+ +
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+ +
+
+
+
+
+
++
+
+
+
+
+
+
+
+
+
+
+
+
+ +
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
++
+
+ +
+ +
+
+
+
+
+
+
+
+
+
+
+
+
+
+ +
+
+
+
+
+
++
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
++
+
+
+
++
+
+
+
+
+
+
++
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+ +
+
+
+
+
+
+
++
++
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+ ++
+
+
+
+
+
+
++
+
+
+
+
+
+
+
+
+
+
+
+
+
++
+
+
+
+
+
+
+
+
+
+
+ +
+
+
+
+
+ +
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+ +
+
+
+
+ +
+
+
+
+
++
+
+
+
+
+
+
+
+
+
+
+
++
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+ +
+ +
++
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
++
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
++
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+ +
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
++
+
+
+
+
+
+
+
+
+
+
+ ++
+
+
+++
+
+
+
+
+ +
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
++
+
+
+
+
+
+
+
+
+
+
+
++
+
+
+
+
+
++
++
+
+
+
+
+
+
+
+
+
+
+
+
+
++
+
+
+
+
+
+
+
+
+
+
+
+
+
+ ++
+
+
+
+
+
+
+
+
+
+ +
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+ +
+
+
+
+
++
+
++
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+ +
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+ +
+
+
+
+
+
+
+
+
+
+
+
+
+
+
++
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+ +
+
++
+
+
+
+
+
+
+
+
+ + +
+
+
+
+
+
+
++
+
+
+
+
+
+
++
+
+
+
+
+
+
+ +
+
+
++
+
+
+
+
+
+
+ +
+ +
+
+
+
+
+
++
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+ +
+
+
+
+
+
++
+
+
+
++
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
++
+
+
+
+
+ +
+ ++
+
+
+
+
+
++
+
+
+
+
+ +
+ +
+
+
+
+
+
+
++
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+ +
+
+
+
+
+
+
+
+
+
+
+++
+
++
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
++
+
+
+
+
+
+
+
+
+
+
+
+ +
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
++
+
+
+
++
++
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+ +
+
+
+
+
+
+
+
+
+
+
+
+
+
+ +
+
+
+
++
+
+
+
+
+
+
++ + +
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
++
+
+
+
+
+
+
+
+
+
+ +
+
+ +
+
+
++
+
+
+
+
+
+
+
++
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+++
+
+
+
++
+
+
+
+
+
+
+
+
+
+
+
+
+
+ +
+
+ +
+
+
+
+
+
+
+
+
+
+++
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+ +
+
+
+
+
+
++
+
+ +
+
+
+
+
+
+
+
+
+
+
+ +
+
+ +
+
++
+
++
+
+
+
++
+
+
++
+
+
+
+
+
+
+
+
+
+
+
+
+
+
−1.0
−0.5
0.0
0.5
1.0
−1.0
−0.5
0.0
0.5
1.0
With Simulated Temporing
FIGURE 15.3: A Comparison of Travels
15.1.6
Dynamic Tempered Transitions
Gill and Casella (2004) extend the idea of tempered transitions to account for current
placement of the chain in the state space. The objective is to escape the necessary trade-oﬀs
in ladder height (maximum heating level) and spacing between rungs (number of steps).
When the area around the chain is highly irregular, it is better to have a lower (cooler
maximum temperature) ladder in order to get to the top of the mode and fully explore
this area. When the area around the chain is smooth, it is better to have a longer (hotter
maximum temperature) ladder in order to avoid being trapped in the low density region.
Also, setting the number of rungs for a given ladder height as too small can reduce the
acceptance rate because high-quality candidates may not be oﬀered. Conversely, setting
the number of rungs as too large can also reduce the acceptance rate because of the product
in the Metropolis-Hastings decision step.
The general strategy in these regards is to specify a distribution of ladders all having
the same number of rungs, but diﬀering heights (diﬀering maximum temperatures). We
observe the multidimensional curvature at the current Markov chain location and specify
a greater probability of selecting a cooler ladder when this curvature is high, and a greater
probability of selecting a hotter ladder when the curvature is low. The number of rungs
is essentially a nuisance parameter, which can be ﬁxed at the beginning of the chain or
tuned during the early runs by comparing acceptance probabilities. The actual goal is not
to determine the exact optimal number of rungs, but to pick a reasonable number that is
not overly aﬀecting the acceptance probabilities.
The key challenge is that by making the behavior of the Markov chain adjust to its
surroundings (i.e., conditional on changes in the posterior for θ), we run the risk of creating
a non-homogeneous Markov chain, and also losing the detailed balance equation.
This
would then deny the ability to assert ergodicity of the chain. We can solve this problem by
taking advantage of the structure of the Metropolis algorithm.
Let f(θ) be the stationary distribution (objective function), let g(θ′|θ) be a candidate
distribution, and let K(θ, θ′) be the associated transition kernel. By the construction of

536
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
the Metropolis algorithm, K(θ, θ′) is given by:
K(θ, θ′) = min
7f(θ′)g(θ|θ′)
f(θ)g(θ′|θ) , 1
8
g(θ′|θ) + (1 −r(θ))δθ(θ′),
(15.4)
where:
r(θ) =

min
7f(θ′)g(θ|θ′)
f(θ)g(θ′|θ) , 1
8
g(θ′|θ)dθ′
(15.5)
and δθ(θ′) = 1 if θ = θ′ and zero otherwise. The kernel K(θ, θ′) now satisﬁes detailed
balance with f(θ) as the stationary distribution (exactly from Robert and Casella 1999,
Theorem 6.2.3).
Now, for each θ, let ρ(λ|θ) be a probability distribution, that is, ρ(λ|θ) ≥0 and

ρ(λ|θ)dλ = 1. Here we are considering λ to be continuous, to be general, but usually
λ will be discrete. Our candidate distribution is:
g∗
λ(θ′|θ) = ρ(λ|θ),
(15.6)
and forms the Metropolis kernel based on g∗
λ(θ′|θ) and f(θ). By construction, detailed
balance is satisﬁed and we have an ergodic Markov chain.
As an example, suppose that there are i = 1, . . . , k ladders, and as i increases the
ladders get hotter. If |f ′′(θ)| is big (so we are near a mode) we might want to favor the
cooler ladders. To do this we can take ρ(λ|θ) to be a binomial mass function with k trials
and success probability p(θ), where
logitp(θ) = a −b|f ′′(θ)|,
b > 0.
(15.7)
So big values of |f ′′(θ)| would result in small p(θ), which would favor the smaller values
of i and the cooler ladders. This way, on average, we spend suﬃcient time exploring the
modal area. Eventually, since there is always a positive probability of getting a hot ladder,
the chain eventually escapes from every mode. In ﬂat areas, since hot ladders are more
probable, this happens on average sooner.
15.2
Reversible Jump Algorithms
Green (1995) introduces reversible jump Markov chain Monte Carlo (RJMCMC) where
the model speciﬁcation (i.e., variable selection) is part of the estimation process. The key
idea is to modify a Metropolis-Hastings kernel to jump not only between points in the pa-
rameter space but also between diﬀerent model speciﬁcations (although the Gibbs sampler
has been used in at least one RJMCMC algorithm, Keith et al. [2004]). Recently Geyer
(2011) argues that RJMCMC is a fundamental feature of MCMC and regular implementa-
tions are really special cases.
Reversible jump MCMC is a form of Bayesian model averaging (page 198) where the

Markov Chain Monte Carlo Extensions
537
posterior probability of a model is provided by the proportion of the total MCMC (post-
convergence) run-time that the chain spends in the space deﬁned by that model. Interest-
ingly, the number of models considered does not need to be speciﬁed in advance, but model
priors are still required. For example, Richardson and Green (1997) express this idea with
“births” and “deaths” of model alternatives (see also Zhang et al. [2004]). We will very gen-
erally describe the algorithm here, and for details, see the original article, or the reviews by
Waagepetersen and Sorensen (2001), Clyde (1999), Chen et al. (2000, pp.300-303), Andrieu,
Djuri´c, and Doucet (2001), or Richardson and Green (1997).
Suppose we have a countable set of K alternative model speciﬁcations to evaluate,
denoted {Mk, k ∈K}. Naturally these models have diﬀerent numbers of parameters, which
may or may not provide nesting, such model Mk has a nk-length (nk ≥1) parameter vector
θ(k) measured on ℜ(nk). For observed data y, the joint distribution of interest is formed
from the product of the prior on model choice, p(k), the prior for the coeﬃcient vector given
model choice, p(θ(k)|k), and the likelihood for y given model choice, L(y|k, θ(k)), producing:
p(k, θ(k), y) = p(k)p(θ(k)|k)L(y|k, θ(k)).
(15.8)
Since the algorithmic focus is on moving between model speciﬁcations, start with the pair
(k, θ(k)), and denote it with simply x, which for a speciﬁc case of k must lie in the parameter
space Ck = {k} × ℜnk. Therefore in the course of the algorithm x varies over the combined
parameter space deﬁned by C = ∪k∈KCk. Since the pairing is inseparable, the posterior of
interest is the joint expression:
π(x|y) = π(k, θ(k)|y) = p(k|y)p(θ(k)|k, y).
(15.9)
If (k, θ(k)) is the current position of the Markov chain at time t, then denote x′ =
(m, θ(m)) ∈{m} × ℜnm as a proposed destination for time t+ 1 produced by the candidate-
generating distribution with probability qm(x, x′). One useful way to generate such candi-
dates is by the inclusion of a random component, U in ℜnkm, km ≥1, independent of x
with density qkm(θ(km), ·). So deﬁne the deterministic mapping gmk :ℜnk+nkm →ℜnm such
that we can rewrite: x′ = gmk(x, u). Now gmk is called a bijection since it gives a one-to-one
mapping between (θ(k), u) and (θ(m), u′), where u′ is the reverse analog of u from x′ instead
of x.
This is a setup to meet the dimension matching condition, which is a necessary part of
ensuring that the detailed balance equation holds. The potential move under consideration
is:
(k, θ(k)) −→(m, θ(m)) = (m, gmk(x, u)),
(15.10)
with the corresponding reverse move:
(m, θ(m)) −→(k, θ(k)) = (k, gkm(x′, u′)).
(15.11)
This now makes it straightforward to match the dimensions:
nk + nkm = nk + nmk
(15.12)

538
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
so that
π(k, θ(k), y)gmk(x, u) = π(m, θ(m), y)gkm(x′, u′)
(15.13)
holds. This leads directly to the Metropolis-Hastings acceptance criteria:
α(x, x′) = min
7
1, π(m, θ(m), y)
π(k, θ(k), y)
qm(x, x′)
qk(x′, x)
qmk(θ(mk), u′)
qkm(θ(km), u)

∂gmk(x, u)
∂x∂u

8
,
(15.14)
where the Jacobian is necessary because x′ = gmk(x, u) is a deterministic function in the
proposal process for the change in variable from (x, u) to x′. Under a wide range of applied
circumstances α(x, x′) is more simple than the general form given here.
Finally, suppose we have a (post-convergence) MCMC sample indexed i = 1, 2, . . . , I,
and deﬁne the following indicator function:
∞i(ik) =
⎧
⎨
⎩
1,
if Mi = Mk
0,
if Mi ̸= Mk.
(15.15)
This function produces a 1 if the ith model in the series is the kth speciﬁcation in the
countable set: k ∈K. Now the posterior probability of model k is simply:
π(k|y) = 1
I
I

i=1
∞i(ik),
(15.16)
where the posterior variance can be calculated according to the tools in Section 14.3.1.
The mechanics of RJMCMC are sometimes diﬃcult to implement and must be cus-
tomized for each application, although Hastie (2005) has developed the AutoMix package
based on normal mixtures as a reasonably general approach. Like all advanced MCMC tech-
niques there are a number of tuning parameters that researchers need to pay close attention
to. In particular the candidate-generating distribution can be challenging in that proposals
now have two criteria for acceptance: parameter values and model space. Nonetheless, this
is an active research area because the idea of assessing model quality within the context of
the sampler is attractive.
15.3
Perfect Sampling
As discussed, one obvious worry is the length of the pre-convergence era of a particular
Markov chain. While we can be comforted by knowing that MCMC algorithms used in
general practice are ergodic, an eventual decision about cessation is required. An inge-
nious solution to this problem is provided by Propp and Wilson (1996) who give a way
to automatically obtain perfect samples immediately from an MCMC algorithm that are
guaranteed to be from the stationary distribution of interest. Their method, called coupling
from the past, (CFTP) is a particular kind of perfect sampling, although the terms are often
(but incorrectly) used synonymously.

Markov Chain Monte Carlo Extensions
539
The idea behind CFTP is that if a chain had been started at step t = −∞and run
forward in time, then at time t = 0 it must be in its stationary distribution. Their inno-
vation is that they found a way to obtain this same sample without the inconvenience (or
impossibility!) of dealing with the inﬁnite past. Deﬁne terms as in previous chapters where
the Markov chain operates on a ﬁnite state space with the transition matrix K(θ, θ′) and
the stationary (target) distribution π(θ). Since this is a ﬁnite state space we can consider
all the ways to transition forward from θ[−1] to θ[0] using the expression in (10.3):
p(θ[0] = j|θ[−1] = i) = K(i, j)
(15.17)
and summing over these possible ways to produce the cumulative transition probability:
p(θ[0] ≤j|θ[−1] = i) =
j

k=1
K(i, k).
(15.18)
Call this last equation C(i, j) for clarity to distinguish it from K(i, j). Now draw a uniform
random number (u0) between zero and one and use the cumulative function to specify a
move:
θ[0] = j
if
C(i, j −1) < u0 ≤C(i, j).
(15.19)
This is fairly routine MCMC so far but now perform this operation using the same u0 for
every state at time t = −1, not just θ[−1] = i. This means we have a parallel set of Markov
chains equal in number to the set of unique states at time t−1. We can apply the transition
rule deﬁned by:
θ(0) = φ(u0, θ(−1)).
(15.20)
Now comes the innovative part. If
φ(u0, θ(−1) = i) = j,
∀i
(15.21)
then we say that the chain has coupled and the value given by j is an exact (perfect) sample
from π(θ).
Why is this true?
If, hypothetically, the Markov chain had been run from
t = −∞to t = −1, then there is no question that it is in stationarity at time t = −1. If the
Markov chain is in stationarity at time t = −1 then it is clearly in stationarity at time t = 0
from the condition imposed by φ(u0, θ(−1)), and of course for any time thereafter. All this
certainty makes this result sound frequent and easy, but the truth is that the probability
that all of the parallel Markov chains couple at this one step is inconveniently low. This is
not a diﬃcult impediment since the selection of the zero time point was arbitrary, thus:
θ(−t) = φ(u−t, θ(−t−1)).
(15.22)
Applying this general idea recursively through the history of the chain gives:
θ(0) = φ(u0, φ(u−1, (φ−2, . . . , φ(u−T +1(θ(−T )) · · · )))),
(15.23)
where T is some large number of our choosing, possibly very large, and we have a series of

540
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
uniform draws identiﬁed: u−T , u−T +1, . . . , u0. If we start our M parallel Markov chains at
time −T , for each of the M possible discrete locations in the state space, then at step −T +1
we will have a number of unique chain locations equal to or less than M: a coalescence of
chains.
Since such chains remain coalesced permanently, over time the collection of M
chains will eventually coalesce to a single chain, and when this happens we will call it time
t = 0. Note that any collection of M Markov chains started at t = −∞will have come
through the time t = −T with M or less unique states, then the sample at t = 0 must be
that deﬁned by θ(0) above.
Various implementations of CFTP use the coupling strategy in incremental steps. The
objective is to ﬁnd the time −T that accomplishes the full coalescence above, and such that
θ(0) is independent of θ(0). Suppose again that there are M states in the sample space and
φ(uj, θ(j)) is a transition rule at time j. Then the steps proceed as follows:
1. Start M parallel chains in each of the M states at time t = −1, and generate u0 ∼
U(0, 1).
2. Apply the transition rule, φ(u0, θ(−1)), to each of the chains. If all of the chains have
coalesced at time t = 0, then set −T = −1. The value θ(0) is a perfect draw from
π(θ).
3. If all the chains have not coalesced, then go to time t = −2, generate u−1 ∼U(0, 1),
and apply the transition rule, φ(u−1, θ(−2)), to each of the chains. If all of the chains
have coalesced at time t = −1, then set −T = −2. The value θ(0) is a perfect draw
from π(θ).
4. If all the chains have not coalesced, then go to time t = −2, and repeat.
5. Continue as necessary back in time until full coalescence.
It is important to remember that no matter how far we go back, the ﬁrst value that is a
perfect draw from the stationary distribution is still the common value at time t = 0. From
that point on we can then collect perfect simulations without worrying about convergence.
One other reminder is warranted. Moving backward it is critical to record the uniform
draws so that we can use these exact values going forward again: u−T, u−T +1, . . . , u−1, u0.
It should be clear that the time required, T , and θ(0) are random variables dependent on
each other such that the full coalescence process must complete before sampling can move
forward with draws from the stationary distribution. This may be a problem in practical
applications if the initial process is interrupted. Fill (1998) addresses this with a version
of perfect sampling (perfect rejection sampling) that is independent of T . It proceeds in
similar fashion for the same setup:
1. Choose a time T and a state at this time θ(T ) = θ at convenience.
2. Generate a conditional series backward to the zero point: θ(T −1)|θ(T ), θ(T −2)|θ(T −1),
. . . , θ(1)|θ(2), θ(0)|θ(1).

Markov Chain Monte Carlo Extensions
541
3. Generate a series of associated uniform draws:
(u1|θ(0), θ(1)), (u2|θ(1), θ(2)), . . . , (uT −1|θ(T −2), θ(T −1)), (uT |θ(T −1), θ(T )).
4. Run M parallel chains starting at time T = 0 and use the same uniform draws to
update all chains.
5. If all the chains have coalesced by the time the series runs out at T , then accept θ(0)
as a draw from π(θ).
6. If all the chains have not coalesced by this time then run the algorithm again, perhaps
with a larger value of T .
Relatively large values of T in general are recommended by Fill (1998) and others.
Fill et al. (1999) and Casella et al. (2001) provide the necessary proofs that this time re-
versal strategy removing the dependence between the backward path length and the sample
value generated at time T = 0 provides samples from the stationary distribution. Clearly
the second step of conditioning given the initial path is the most challenging part here.
However, the independence between the length of the path generation and the production
of θ(0) values combined with the reuse of the uniform draws makes this process relatively
eﬃcient for users. Notable extensions have been developed, including the general version of
this algorithm provided by Fill et al. (1999). Murdoch and Rosenthal (1998) remove the
required assumption of stochastic monotonicity of the chain.
Møller and Schladitz (1999)
are also able remove this assumption in the context of stochastic repulsive sequences, leading
new applications.
Several mechanical challenges remain with perfect sampling. Unless the number of states
is relatively small, then the size of the process at each step can be awkward and tracking
coalescence can be burdensome. For non-trivial state spaces (i.e., excluding the toy examples
that dominate published introductions), it can be very diﬃcult to show that a large number
of paths have all coalesced. However, if the states are ordered with a monotone transition
rule, then one convenient shortcut is to track coalescence of just the maximum and minimum
values at each step since these will squeeze the other values towards complete coupling. By
monotone here we mean that θ(t)
i
≥θ(t)
j
implies θ(t+1)
i
≥θ(t+1)
j
(Fill 1998, Deﬁnition 4.2),
i.e., paths on lower starting points remain below paths on higher starting points until full
coalescence. Furthermore, setting up and running unbiased algorithms is labor-intensive
and speciﬁc to each application. Therefore we are not likely to see commercial software
support in the foreseeable future. Finally, it is critical to reuse the uniform random draws
correctly for unbiased properties to hold.
Perfect sampling is one of the more exciting new research areas in MCMC. Good reviews
of perfect sampling include Casella et al. (2001) and Craiu and Meng (2011).
The biggest
challenge, of course, is to generalize the state space deﬁnition (see Green and Murdoch
[1998] as well as Murdoch and Green [1998]).
Meng (2000) produced an algorithm based
on a multistage version of the CFTP backward coupling scheme using cluster sampling to

542
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
reduce time to coalescence. Hobert et al. (1999) are able to impose the monotonicity re-
quirement on cases with two and three component mixtures, but apparently with a high
computational cost. Corcoran and Tweedie (1998) show that perfect sampling based on
the independent Metropolis-Hastings kernel has useful monotonicity properties and is there-
fore a good algorithmic choice. Brooks et al. (2006) link perfect sampling with simulated
tempering discussed earlier in this chapter. Casella et al. (2002) develop perfect slice sam-
plers and apply them to mixtures of distributions. Murdoch and Takahara (2006) develop
applications to queueing theory and networks.
However, there are more applications in
statistical physics and point processes (stochastic processes with binary outcomes occurring
over continuous time) than there are in the social sciences.
15.4
Hamiltonian Monte Carlo
Hamiltonian Monte Carlo (also called Hybrid Monte Carlo) is a variant of the Metropolis-
Hastings algorithm that uses physical system dynamics as a means of generating candidates
for a Metropolis decision. When properly implemented it is faster than regular Metropolis-
Hastings because it incorporates more information about posterior topography. This idea
was originally presented by Duane et al. (1987), and further developed by Neal (1993). This
section will very generally outline the method and the most detailed description for MCMC
purposes is currently Neal (2011).
As noted by every description to date, a comprehension of Hamiltonian dynamics is a
necessary qualiﬁcation for understanding hybrid Monte Carlo. This is a core topic in physics,
so there exist many relevant textbooks such as that by Meyer and Hall (1992). Hamiltonian
dynamics is the model whereby physicists describe an object’s trajectory within a deﬁned
system. First deﬁne ϑt as a k-dimensional location vector and pt as a k-dimensional mo-
mentum (mass times velocity) vector, both recorded at time t. The Hamiltonian system at
time t with 2k dimensions is described by the joint Hamiltonian function:
H(ϑt, pt) = U(ϑt) + K(pt)
(15.24)
(sometimes just abbreviated H), where U(ϑt) is the function describing the potential energy
at the point ϑt, and K(pt) is the function describing the kinetic energy for momentum pt.
Neal (2011) gives the simple 1-dimensional example:
U(ϑt) = ϑ2
t
2
K(pt) = p2
t
2 ,
(15.25)
which is equivalent to a standard normal distribution for ϑ. Commonly the kinetic energy
function is deﬁned as:
K(pt) = p′
tΣ−1pt,
(15.26)
where Σ is a symmetric and positive-deﬁnite matrix that can be as simple as an identity

Markov Chain Monte Carlo Extensions
543
matrix times some scalar that can serve the role of a variance: Σ = σ2I.
This simple
form is equivalent to the log PDF of the multivariate normal with mean vector zero and
variance-covariance matrix Σ.
Hamiltonian dynamics describe the gradient-based way that potential energy changes
to kinetic energy and kinetic energy changes to potential energy as the object moves over
time throughout the system (multiple objects require equations for gravity, but that is
fortunately not our concern here). The mechanics of this process are given by Hamilton’s
equations, which are the set of simple diﬀerential equations:
∂ϑit
∂t
= ∂H
∂pit
= K(∂pit)
∂pit)
(15.27)
∂pit
∂t
= −∂H
∂ϑit
= −U(∂ϑit)
∂ϑit)
(15.28)
for dimension i at time t. For continuously measured time these equations give a mapping
from time t to time t + τ, meaning that from some position ϑt and momentum pt at time
t we can predict ϑτ and pτ. Returning to the one-dimensional standard normal case, these
equations are simply dϑt/dt = p and dp/dt = −ϑ.
There are three important properties of Hamiltonian dynamics that are actually required
if we are going to use them to construct an MCMC algorithm (Neal 2011). First, Hamil-
tonian dynamics is reversible, meaning that the mapping from (ϑt, pt) to (ϑt+τ, pt+τ) is
one-to-one and therefore also deﬁnes the reverse mapping from (ϑt+τ, pt+τ) to (ϑt, pt).
Second, total energy is conserved over time t and dimension k, and the Hamiltonian is
invariant, as shown by:
∂H
∂t =
k

i=1
'∂ϑi
∂t
∂H
∂ϑi
+ ∂pi
∂t
∂H
∂pi
(
=
k

i=1
'∂H
∂pi
∂H
∂ϑi
−∂H
∂ϑi
∂H
∂pi
(
= 0.
(15.29)
This provides detailed balance (reversibility) for the MCMC algorithm. Second, Hamil-
tonian dynamics preserve volume in the 2k dimensional space. In other words, elongating
some region in a direction requires withdrawing another region as the process continues over
time. This ensures that there is no change in the scale of Metropolis-Hastings acceptance
probability. Finally, Hamiltonian dynamics provides a symplectic mapping in R2k space.
Deﬁne ﬁrst the smooth mapping ψ : R2k
→
R2k with respect to some constant and
invertible matrix J with J′ = −J and det(J) ̸= 0, along with having Jacobian ψ(z) for
some z ∈R2k. The mapping ψ is symplectic if:
ψ(z)′J−1ψ(z) = J−1.
(15.30)
Leimkuhler and Reich (2005, p.53) give the following mapping in 2-dimensional space z =
(ϑ, p):
ψ(ϑ, p) =

p
1 + bϑ + ap2

,
(15.31)

544
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
with constants a, b ̸= 0. The Jacobian of ψ(ϑ, p) is calculated by:
∂
∂ϑ
∂
∂pψ(ϑ, p) =

0
1
b
2ap

.
(15.32)
We check symplecticness by:
' ∂
∂ϑ
∂
∂pψ(ϑ, p)
(′
J−1
' ∂
∂ϑ
∂
∂pψ(ϑ, p)
(
=

0
1
b
2ap
′ 
0
−1
1
0
 
0
1
b
2ap

= −bJ−1.
(15.33)
Thus we say that ψ(ϑ, p) is symplectic for b = −1 and any a ̸= 0.
Everything discussed so far assumed continuous time, but obviously for a computer
implementation in a Markov chain Monte Carlo context we need to discretize time. So we
will grid t + τ time into intervals of size υ: υ, 2υ, 3υ, . . ., mυ. We need a way to obtain this
discretization while preserving volume, and so we use a tool called the leapfrog methods.
The notation is more clear if we now move t from the subscript to functional notation: ϑ(t)
and p(t), which is also a reminder that time is now discrete rather than continuous. To
complete a single step starting at time t, ﬁrst update each of the momentum dimensions by
υ/2 with the following:
pi

t + υ
2 )

= pi(t) −υ
2
∂U(ϑt)
∂ϑi(t) .
(15.34)
Now take a full υ-length step to update each of the position dimensions to leapfrog over the
momentum:
ϑi(t + υ) = ϑi(t) + υ
∂K(pt)
∂pi(t + υ
2 ),
(15.35)
and ﬁnish with the momentum catching up in time the step:
p(t + υ) = pi

t + υ
2 )

−υ
2
U(ϑt)
∂ϑi(t + υ).
(15.36)
Notice that the leapfrog method is reversible since it is a one-to-one mapping from t to t+υ.
Obviously, running these steps M times completes the Hamiltonian dynamics for M × υ
period of total time. The determination of υ is a key tuning parameter in the algorithm
since smaller values give a closer estimation to continuous time but also add more steps to
the algorithm.
A Metropolis-Hastings algorithm is conﬁgured such that the Hamiltonian function serves
as the candidate-generating distribution.
This requires connecting the regular posterior
density function, π(θ), to a potential energy function, U(ϑt), where a kinetic energy func-
tion, K(pt), serves as a (multidimensional and necessary) auxiliary variable in the man-
ner discussed in Section 14.4.3 starting on 513. This connection is done via the distribu-
tion!canonical commonly used in physics:
p(x) = 1
Z exp
'
−E(x)
T
(
,
(15.37)
where E(x) is the energy function of some system at state x, T is the temperature of the

Markov Chain Monte Carlo Extensions
545
system (which can simply be set at 1), and Z is just a normalizing constant so that p(x) is
a regular density function. In the Hamiltonian context (15.37) is:
p(ϑ, p) = 1
Z exp
'
−H(ϑ, p)
T
(
= 1
Z exp
'
−U(ϑt) + K(pt)
T
(
= 1
Z exp
'
−U(ϑt)
T
(
exp
'
−K(pt)
T
(
,
(15.38)
demonstrating that ϑ and p are independent. Finally we connect the energy function metric
with the regular posterior density metric with the function:
E(ϑ) = −log(π(θ)),
(15.39)
thus completing the connection. Notice that the θ variables must all be continuous in the
model, although Hamiltonian Monte Carlo can be combined with other MCMC strategies
in a hybrid algorithm.
The Hamiltonian Monte Carlo algorithm uses two general steps at time t:
▷generate, independent of the current ϑt, the momentum pt from the multivariate
normal distribution implied by K(pt) = p′
tΣ−1pt with mean vector zero and variance-
covariance matrix σ2I (or some other desired symmetric and positive-deﬁnite form).
▷run the leapfrog method M times with υ steps to produce the candidate (˜ϑ, ˜p).
▷accept this new location or accept the current location as the t+1 step with a standard
Metropolis decision using the H function:
min

1, exp(−H((˜ϑ, ˜p)) + H(ϑ, p)

.
(15.40)
While this process looks simple, there are several complications to consider. We must be
able to take the partial derivatives of the log-posterior distribution, which might be hard.
Also the chosen values of the leapfrog parameters, M and υ are critical. If υ is too small
then exploration of the posterior density will be very gradual with small steps, and if υ
is too big then many candidates will be rejected. Choosing M is important because this
parameter allows the Hamiltonian process to explore strategically with respect to gradients.
Excessively large values of M increase compute time, but excessively small values of M also
lead to many rejected candidates. In both cases where the parameters are too small we lose
the advantages of the gradient calculations and produce an ineﬃcient random walk. Finally,
σ2 aﬀects eﬃciency of the algorithm in the conventional sense of appropriating tuning the
variance of the multivariate normal for the momentum. These can be diﬃcult parameter
decisions and Neal (2011) gives speciﬁc guidance on trial runs and analysis of the results.
■Example 15.2:
An Illustrative Simple Simulated Hamiltonian Setup. This is

546
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
a basic illustration of Hamiltonian Monte Carlo applied to the same target distribution
as the Hit-and-Run sampler in Example 10.5 (page 361). The target distribution is
a bivariate normal PDF with a correlation of 0.95. Similar to the data structures
in the Hit-and-Run code and the basic Metropolis-Hastings code in Example 10.4.6
(starting on page 359), a matrix is created with all NA values to ﬁll in. Here there are
four stored values on each row: x, y, ϑ1, and ϑ2. Consider the following R code:
ham.norm <- function(theta.matrix,reps,I.mat,upsilon,M,Sigma)
{
for (i in 2:reps)
{
vartheta <- theta.matrix[(i-1),3:4]
p <- rnorm(length(vartheta),0,1)
p.half <- p - (upsilon/2)*vartheta
for (j in 1:M)
{
vartheta <- vartheta + upsilon*p.half
p.half <- p.half - upsilon*vartheta
}
p.full <- -(p.half + (upsilon/2)*vartheta)
new.U <- t(vartheta) %*% solve(I.mat) %*% vartheta/2
new.K <- t(p.full) %*% solve(Sigma) %*% p.full
old.U <- t(theta.matrix[(i-1),1:2]) %*% solve(I.mat)
%*% theta.matrix[(i-1),1:2]/2
old.K <- t(-p) %*% solve(Sigma) %*% -p
a <- exp(old.U - new.U - new.K + old.K)
if (a > runif(1)) theta.matrix[i,1:2] <- vartheta
else theta.matrix[i,1:2] <- theta.matrix[(i-1),1:2]
theta.matrix[i,3:4] <- vartheta
}
theta.matrix
}
This code is also provided in the BaM package in R. The following lines code give
the setup and the function call for 10,000 iterations. The last 5,000 iterations are
displayed in Figure 15.4. Notice the values υ = 0.001 and M = 1000. These were
set up by trial-and-error, and even a model as simple as this requires some tuning of
these parameters.
num.sims <- 10000
Sig.mat <- matrix(c(1.0,0.95,0.95,1.0),2,2)
upsilon.in <- 0.001
M.in <- 1000
Sigma.in <- matrix(c(3,0,0,3),2,2)
walks<-rbind(c(-3,-3,1,1),matrix(NA,nrow=(num.sims-1),ncol=4))
walks <- ham.norm(walks,num.sims,Sig.mat,upsilon.in,M.in,Sigma.in)

Markov Chain Monte Carlo Extensions
547
This is intended to be an illustrative example only since a bivariate normal does
not require estimation with MCMC. For a fully developed package that implements
Hamiltonian Monte Carlo see the Stan package developed by Andrew Gelman and his
colleagues at http://mc-stan.org.
 0.05 
 0.1 
 0.2 
−3
−2
−1
0
1
2
3
−3
−2
−1
0
1
2
3
FIGURE 15.4: Samples from Hamiltonian Monte Carlo
15.5
Exercises
15.1
Plot in the same graph a time sequence of 0 to 100 versus the annealing cooling
schedules: logarithmic, geometric, semi-quadratic, and linear. What are the trade-
oﬀs associated with each with regard to convergence and processing time?
15.2
Plot a bivariate normal distribution at 4 diﬀerent temperature levels with enough
variation that the subplots are distinct.
15.3
Replicate the contrived simulated annealing example from Section 15.1 using each
of the cooling schedules from Exercise 1, checking for convergence before moving
on to the next temperature. What diﬀerences do you observe?
15.4
Using the HMO data in Example 11.3.3 on page 390 (available in the BaM package)

548
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
implement a Metropolis-coupling algorithm (MCMCMC) at 10 diﬀerent temper-
ature levels. The lowest temperature deﬁnes the regular joint posterior and the
highest temperature deﬁnes a joint uniform density. Run this in R to produce a
posterior estimate from the cold chain values to compare with the BUGS code in
Chapter 11.
15.5
The so-called witch’s hat distribution is given this name because it looks like a
conical spike in the middle of a wide ﬂat plane. This distribution can be disastrous
for the Gibbs sampler because all but one coordinate must be lined up with the
peak before a subchain step can move to the peak and the probability that this
happens reduces exponentially with increasing dimensions.
Matthews (1993) gives the following multivariate form based on a normal/uniform
mixture, deﬁned over the d-dimensional unit cube:
p(θ|x) = (1 −δ)[2πσ2]−d/2 exp

−
d

i=1
1
2σ2 (xi −θi)2

+ δI(0,1)(xi),
where I(0,1)(xi) is an indicator function equal to one when xi is in the interval (0, 1)
and zero otherwise. Cui et al. (1992) sample this distribution setting: δ = 10−11,
σ2 = 0.0009, and the nine-dimensional peak at (0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9).
Run a Gibbs sampler for this problem starting at 10 diﬀerent points in the unit
cube, and test convergence with the Gelman and Rubin diagnostic. What do you
conclude about convergence?
15.6
Using again the witch’s hat distribution from the last exercise, write a Metropolis-
Hastings variant that fully explores the “brim” of the hat eﬃciently (in reasonable
time).
15.7
(Bohachevsky, Johnson, and Stein 1986). Use simulated annealing to ﬁnd the global
minima of the function
f(x, y) = x2 + 2y2 −0.3 cos(3πx) −0.4 cos(4πy) + 0.7,
on the support −1 ≤x, y ≤1. Using the same function from Bohachevsky, Johnson,
and Stein (1986), write a simple Metropolis-Hastings algorithm in R to explore the
sample space. Show the problems associated with this approach by graphing the
acceptance rate over time.
15.8
Simulated Tunneling.
Wenzel and Hamacher (1999) design a stochastic search
related to simulated annealing that randomly “hops” from point to point to accom-
modate eﬃcient search of complex multimodal functions without being trapped
for long periods of time at local minima. Replace the target function f(x) with
fSTUN = 1 −exp[−ζ(f(x) −fmin)], where fmin is the lowest minimum yet encoun-
tered and ζ > 0 is a tunable “tunneling parameter” that regulates the steepness

Markov Chain Monte Carlo Extensions
549
of descents. Write and run a Metropolis-Hastings implementation using the same
function as the last exercise.
15.9
One of the diﬃculties with slice sampling is that in high dimensions, the Markov
chain may be restricted to a set of unusual subspaces of the parameter space: non-
linear and varying dramatically in size. This can obviously lead to poor convergence
and mixing properties. Develop a simulated annealing algorithm that alleviates this
problem in at least one application.
15.10
Consider the surface (3D function) created by the following R code:
f.xyz <- function(x, y)
{
0.75*exp( -((10*x-1)^2 + (10*y-1)^2)/5 ) +
0.50*exp( -((10*x-7)^2 + (10*y-5)^2)/5 ) -
0.25*exp( -((10*x-4)^2 + (10*y-7)^2)/5 )
}
set.seed(pi);
n <- 15;
x2 <- x1 <- seq(0,1,length=n)
y <- outer(x1, x2, f.xyz)
y <- y + rnorm(n^2,0,0.05*max(abs(y)))
Write a simulated tunneling application for this surface that accommodates maxima
instead of minima.
Graph the function and 200 post-convergence visits on the
graph.
15.11
It is possible that not all full conditional probability statements can be identiﬁed to
set up a Gibbs sampler. One way to solve this problem is to embed a Metropolis-
Hastings algorithm within the Gibbs sampler, called “Metropolis-within-Gibbs,”
for the parameter or parameters that are causing the diﬃculty. Write out the full
statement of this algorithm and derive the detailed balance equation.
15.12
Using the terrorism in Great Britain example (Example 10.3.2, starting on page 346)
implement a Metropolis-with-Gibbs example where λ and φ get a Gibbs draw as
done before but k is sampled using Metropolis-Hastings.
15.13
In his original article Green (1995) analyzes the well-known coal mining data used
in many papers and texts, except that he compares models with one, two, and three
possible changepoints using reversible jump MCMC and uses days instead of years.
Green’s analysis supports the notion of two changepoints (his Figure 2). Replicate
that analysis using R and make a decision about which model should eventually be
preferred with a Bayes Factor calculation. This is the data vector:
coal.mining.disasters <- c(4,5,4,0,1,4,3,4,0,6,3,3,4,0,2,6,
3,3,5,4,5,3,1,4,4,1,5,5,3,4,2,5,
2,2,3,4,2,1,3,2,2,1,1,1,1,3,0,0,

550
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
1,0,1,1,0,0,3,1,0,3,2,2,0,1,1,1,
0,1,0,1,0,0,0,2,1,0,0,0,1,1,0,2,
3,3,1,1,2,1,1,1,1,2,4,2,0,0,1,4,
0,0,0,1,0,0,0,0,0,1,0,0,1,0,1)
(also included in the BaM package).
15.14
Using the death penalty data in Example 14.3 on page 481, write a RJMCMC
procedure in R for the Tobit model.
15.15
Show that the acceptance criteria given in (15.14) satisﬁes the detailed balance
equation.
15.16
Umbrella Sampling. Torrie and Valleau (1977) suggested a generalization of im-
portance sampling, solidiﬁed by Geyer (2011). Here the approximation distribution
is replaced with a mixture distribution: g(θ) ≡J
j=1 g(θj)cj for a J-component
mixture where cj is a set of normalizing weights (using the language of Section 9.6
starting on page 296, rather than that of Geyer). This makes the importance weight
ωi = f(θi)/ J
j=1 g(θj)cj. The choice of mixture distribution is problem-dependent
and must be made carefully. Apply this sampler to the function given in (15.2).
15.17
Using R run a coupling from the past algorithm with the following transition matrix:
K =

0.9
0.1
0.5
0.5

.
Print out the reached values as part of the code, indicate when coalescence occurs,
and determine stationary distribution.
15.18
A state space has four ordered states, {A, B, C, D}, where D wraps around back
to A.
For any given state the probability of moving “forward” is 2/3 and the
probability of moving “backward” is 1/3. Implement a CFTP algorithm in R to
obtain the stationary distribution.
15.19
Rerun the CFTP algorithm from Exercise 15.17 but instead of re-using the uniform
draws, generate new ones at each step. What diﬀerence do you see? Why did this
go wrong?
15.20
The Hoﬀ(2005, 2009) social networking model starts with the n × n symmetric
matrix Y of links between n individuals, where yij = 1 indicates a known link
between node i and node j, and yij = 0 indicates the absence of evidence of a link.
The n × n × K array X deﬁnes for each n × n relationship between individual i
and individual j with a K-length vector of covariate information. Relate X and Y
with the random eﬀects logistic regression speciﬁcation:
p(Y|θij) =
+
i̸=j
exp(θij)
1 + exp(θij)
θij = β′xij + zij
zij = u′
iGvj + ϵij

Markov Chain Monte Carlo Extensions
551
where β is a K-length vector of coeﬃcients to estimate, and zij is a random eﬀects
term to account for dependencies between attribute relationships. The zij term
has two components: a u′
i vector of sender-speciﬁc latent factors, a vj vector of
receiver-speciﬁc latent factors, a G diagonal matrix of unknown coeﬃcients, plus a
ϵij scalar error speciﬁc to the ij edge. Assign priors and write a Metropolis-Hastings
variant using the Roethlisberger and Dickson (1939) Hawthorne Plant wiring room
dataset in the R package BaM.


Appendix A
Generalized Linear Model Review
A.1
Terms
Consider a one-parameter conditional probability density function or probability mass
function for the random variable Z of the form: f(z|ζ). This family form is classiﬁed as
an exponential family form if it can be reparameterized as: f(z|ζ) = exp
	
t(z)u(ζ)

r(z)s(ζ),
where: r and t are real-valued functions of z that do not depend on ζ, and s and u are
real-valued functions of ζ that do not depend on z, and r(z) > 0, s(ζ) > 0 ∀z, ζ. The
canonical form is the result of one-to-one transformation that reduces the complexity of
the symbolism and reveals structure.
If t(z) = z, then we say that this PDF or PMF
is in its canonical form for the random variable Z. Otherwise, we can make the simple
transformation: y = t(z) to force a canonical form. Similarly, if u(ζ) = ζ in this expression,
then this PDF or PMF is in its canonical form for the parameter ζ. If not, we can again
force a canonical form by transforming: θ = u(ζ), and call θ the canonical parameter. This
produces the ﬁnal form: f(y|θ) = exp
	
yθ −b(θ) + c(y)

. Here b(θ) plays a key role in
calculating the moments of the distribution and other important quantities. If the form of
θ, the canonical link between the original form and the θ parameterized form, is the source
of the link function in generalized linear models (Fahrmeir and Kaufmann 1985; Jørgensen
1983; Wedderburn 1976), then this process is equivalent to ﬁnding a k-dimensional global
modal point.
Our real interest lies in obtaining the posterior distribution of the unknown k-dimensional
θ coeﬃcient vector, given an observed X matrix of data values: p(θ|X). This allows us to
determine the “most likely” values of the θ vector using the k-dimensional mode (maximum
likelihood inference, Fisher 1925b), or alternatively to describe the resulting distribution
(as in Bayesian inference). This posterior is produced by Bayes’ Law:
p(θ|X) = p(X|θ) p(θ)
p(X)
(A.1)
where p(X|θ) is the n-dimensional joint probability function for data (the probability of the
sample for a ﬁxed θ) under the assumption that the data are independent and identically
distributed according to p(Xi|θ) ∀i = 1, . . . , n, and p(θ), p(X) are the corresponding
unconditional probabilities.
The Bayesian approach integrates to obtain p(X) (or ignores it using proportionality)
553

554
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
and speciﬁes an assumed (prior) distribution on θ, thus allowing fairly direct computation
of p(θ|X) from (A.1). If we regard p(X|θ) as a function of θ for some given observed data
X, then L(θ|X) = n
i=1 p(X|θ) is called a likelihood function (DeGroot 1986, p.339). The
maximum likelihood principle states that an admissible1 θ that maximizes the likelihood
function probability (discrete case) or density (continuous case), relative to alternative
values of θ, provides the θ that is most “likely” to have generated the observed data, X,
given the assumed parametric form. Restated, if ˆθ is the maximum likelihood estimator for
the unknown parameter vector, then it is necessarily true that L(ˆθ|X) ≥L(θ|X) ∀θ ∈Θ,
where Θ is the admissible set of θ.
The likelihood function diﬀers from the inverse probability, p(θ|X), in that it is neces-
sarily a relative function since it is not a normalized probability measure (P()) bounded
by zero and one.2 Thus maximum likelihood estimation substitutes the unbounded notion
of likelihood for the bounded deﬁnition of probability (Casella and Berger 2002, p. 316;
Fisher 1922, p.327; King 1989, p.23). This is an important theoretical distinction, but of
little signiﬁcance in applied practice.
Typically it is mathematically more convenient to work with the natural log of the
likelihood function. This does not change any of the resulting parameter estimates because
the likelihood function and the log likelihood function have identical modal points. Using
a probability density function for a single parameter of interest the basic log likelihood
function is very simple:
ℓ(θ|X) = log(L(θ|X)),
(A.2)
where we use ℓ(θ|X) as shorthand to distinguish the log likelihood function from the like-
lihood function: L(θ|X).
The score function is the ﬁrst derivative of the log likelihood function with respect to
the parameters of interest:
˙ℓ(θ|X) = ∂
∂θ ℓ(θ|X).
(A.3)
Setting ˙ℓ(θ|X) equal to zero and solving gives the maximum likelihood estimate, ˆθ. This is
now the “most likely” value of θ from the parameter space Θ treating the observed data as
given: ˆθ maximizes the likelihood function at the observed values. The Likelihood Principle
(Birnbaum 1962) states that once the data are observed, and therefore treated as given,
all of the available evidence for estimating ˆθ is contained in the (log) likelihood function,
ℓ(θ|X). This is a very handy data reduction tool because it tells us exactly what treatment
1Admissible here means values of θ are taken from the valid parameter space (Θ): values of θ that are
unreasonable according to the form of the sampling distribution of θ are not considered (integrated over).
A Bayesian decision rule is called admissible if it is no worse (equal or lower risk) than every other decision
rule and has at least one better alternative for all possible values of the parameter as discussed in Chapter 8
See also: Berger (1985, Section 1.3; Ghosh and Meeden (1997, Section 2.2).
2From a frequentist standpoint, the probabilistic uncertainty is a characteristic of the random variable
X, not the unknown but ﬁxed θ.
Barnett (1973, p.131) clariﬁes this distinction: “Probability remains
attached to X, not θ; it simply reﬂects inferentially on θ.”

Generalized Linear Model Review
555
of the data is important to us and allows us to ignore an inﬁnite number of alternates
(Poirer 1988, p.127).
Setting the score function from the joint PDF or PMF equal to zero and rearranging
gives the likelihood equation:

t(xi) = n ∂
∂θE[x]
(A.4)
where  t(xi) is the remaining function of the data, depending on the form of the probability
density function (PDF) or probability mass function (PMF), and E[x] is the expectation
over the kernel of the density function for x.3 The underlying theory is remarkably strong.
Solving (A.4) for the unknown coeﬃcient produces an estimator that is unique (a unimodal
posterior distribution), consistent (converges in probability to the population value),4 and
asymptotically eﬃcient (the variance of the estimator achieves the lowest possible value as
the sample size becomes adequately large: the Cram´er-Rao lower bound, see Shao 2005).
This result combined with the central limit theorem gives the asymptotic normal form for
the estimator: √n(ˆθ −θ)
D
→N(0, Σθ). This means that as the sample size gets large,
the diﬀerence between the estimated value of θ and the true value of θ gets progressively
close to zero, with a variance governed by
1
√nΣθ, where Σθ is the k × k variance-covariance
matrix for θ. Furthermore,  t(xi) is a suﬃcient statistic for θ, meaning that all of the
relevant information about θ in the data is contained in  t(xi). For example, the normal
log likelihood expressed as a joint exponential family form is ℓ(θ|X) =

μ  xi −nμ2
2

/σ2−
1
2σ2
 x2
i −n
2 log(2πσ2). So t(x) =  xi,
d
dμ
nμ2
2
= nμ, and equating gives the maximum
likelihood estimate of μ to be the sample average that we know from basic texts:
1
n
 xi.
A.1.1
The Linear Regression Model
The classic linear model dates back to the early nineteenth century (see Stigler 1999,
Chapter 17) and is the undeniable workhorse of statistical work in the social and behavioral
sciences. The linear model, as elegant as it is, requires a relatively strict set of assumptions.
The Gauss-Markov Theorem states that if one can assume that:
1. the relationship between each explanatory variable and the outcome variable is ap-
proximately linear in structure,
3The kernel of a PDF or PMF is the component of the parametric expression that directly depends on
the form of the random variable, i.e., what is left when normalizing constants are omitted. We can often
work with kernels of distributions for convenience and recover all probabilistic information at the last stage
of analysis by renormalizing (ensuring summation or integration to one). The kernel is the component of the
distribution that assigns relative probabilities to levels of the random variable (see Gill 2000, Chapter 2).
For example the kernel of a gamma distribution is just the part xα−1 exp[−xβ], without βα/Γ(α) (see A
for gamma distribution details).
4In one’s enthusiasm for the maximum likelihood estimator it is easy to forget that it is asymptotically
unbiased, but not necessarily unbiased in ﬁnite sample situations. For instance the maximum likelihood
estimate for the variance of a normal model, ˆσ2 = 1
n
(xi −¯x)2 is biased by n/(n −1). This diﬀerence is
rarely of signiﬁcance and clearly the bias disappears in the limit, but it does illustrate that unbiasedness of
the maximum likelihood estimate is guaranteed only in asymptotic circumstances.

556
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
2. the residuals are independent with mean zero and constant variance,
3. there is no correlation between any regressor and disturbance,
then the solution produced by selecting coeﬃcient values that minimize the sum of the
squared residuals is unbiased and has the lowest total variance amongst unbiased linear
alternatives. This is sometimes called BLUE for Best Linear Unbiased Estimate.
The linear model can be expressed as follows:
Y
(n×1) =
Xβ
(n×p)(p×1)
+
ϵ
(n×1),
E[Y]
(n×1)
=
Xβ.
(n×p)(p×1)
(A.5)
The right-hand sides of the two equations are very familiar: X is the model matrix of
observed data or the design matrix of stipulated values (organized by column), β is the
column vector of unknown coeﬃcients to be estimated, Xβ is called the “linear structure
vector,” and ϵ is the column vector of (assumed) independent, normally distributed error
terms with constant variance: the stochastic component. In the expectation component of
(A.5), E[Y] = θ is the column vector of means: the systematic component. The vector,
Y, is distributed iid normal with mean θ, and constant variance σ2. This is exactly the
multivariate linear model described in basic statistics texts, but provided here in matrix
form. Gauss (1809) showed that a good way to estimate the unknown linear slope term(s)
is to minimize the sum of the squared errors. The squaring is a convenience since otherwise
positive and negative residuals will cancel each other out. Actually there are many ways to
accomplish this such as taking the absolute value of the residuals: L1 regression (Legendre
1805). The principle behind L-regression (also called quantile regression) is that a selected
quantile, κ ∈[0, 1], is selected and the following quantity is minimized:
min
ˆβ∈R
 n

i=1
κ|yi −xi ˆβ| +
n

i=1
(1 −κ)|yi −xi ˆβ|

.
(A.6)
Obviously if κ = 1
2, this is greatly simpliﬁed and in fact becomes a well-known alternative to
least squares estimates called the least absolute errors (LAE) estimator, the L1 estimator,
or just the median regression estimator. In the notation of (A.5), minimizing squared error
is equivalent to:
minb
n

i=1
ε2
i = minb
n

i=1
(yi −Xib)2.
(A.7)
Or in matrix notation, this is the process of minimizing the quantity S(b) over the range
of possible values of b:
S(b) = ϵ′ϵ = (y −Xb)′(y −Xb) = y′y −2bX′y + bX′Xb.
(A.8)
This is a quadratic form with a positive sign on the squared term, so there is a unique value
that minimizes S(b). We can therefore diﬀerentiate with respect to b and solve for zero to

Generalized Linear Model Review
557
get:
∂
∂bS(b) = −2X′y + 2X′Xb ≡0
X′Xb = X′y
(the “normal equation”)
ˆb = (X′X)−1X′y.
In addition, it is also possible to derive an estimator of β using maximum likelihood
estimation. By the Gauss-Markov along with the central limit theorem, the residuals are
normally distributed with mean zero and constant variance. The likelihood equation for
the residuals is therefore:
L(ϵ) = (2πσ2)−n
2 exp
'
−1
2σ2 ϵ′ϵ
(
= (2πσ2)−n
2 exp
'
−1
2σ2 (y −Xb)′(y −Xb)
(
.
The function L(ϵ) is concave for this equation, although concavity is not a guaranteed
feature of the likelihood equation.5
The log of L(ϵ) is maximized at the same point as the function itself, allowing us to use
the easier form, taking the derivative with respect to b, and solving for zero:
log L(ϵ) = −n
2 log(2π) −n
2 log(2σ2) −
1
2σ2 (y −Xb)′(y −Xb)
∂
∂b log L(ϵ) = 1
σ2 X′(y −Xb) ≡0
ˆb = (X′X)−1X′y.
This demonstrates that the least squares estimator is identical to the maximum likelihood
estimator for the parameter vector b in linear regression. In general, most statistical pro-
cedures do not provide such a nice linkage between disparate theories, and it is a reﬂection
of both the elegance and the fundamental nature of the linear model that this is true here.
A.2
The Generalized Linear Model
To look at a broader application of maximum likelihood estimation, we now turn to the
generalized linear model construct for specifying nonlinear regression speciﬁcations. This
will demonstrate the utility of maximizing the likelihood function relative to some observed
data in a more complex setting.
5In the absence of concavity over the allowable range of parameter values multiple modes may exist
or the maximum may not be unique (Shao 2005).
A number of authors have shown the existence and
uniqueness of a maximum value not on the boundaries of the parameter space (and therefore concavity)
for commonly speciﬁed likelihood functions: Haberman (1974b), Kaufmann (1988), Lesaﬀre and Kaufmann
(1992), Makelainen, Schmidt, and Styan (1981), Silvapulle (1981), Wedderburn (1976), and Fahrmeir and
Tutz (2001, p.43) discuss the properties for generalized linear models.

558
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
A.2.1
Deﬁning the Link Function
Consider the standard linear model meeting the Gauss-Markov conditions:
V
(n×1) =
Xβ
(n×p)(p×1)
+
ϵ
(n×1),
E(V)
(n×1)
=
θ
(n×1) =
Xβ
(n×p)(p×1)
,
(A.9)
where E(V) = θ is the vector of means (the systematic component), and V is distributed
iid normal with mean θ, and constant variance σ2. We use V instead of Y (as opposed to
(A.5)), because V is not the real outcome variable of interest; it is merely a description of
the linear component on the right-hand-side. Now generalize this slightly with a function
based on the mean of the outcome variable, which is no longer required to be normally
distributed or even continuous:
g(μ)
(n×1)
=
θ
(n×1) =
Xβ.
(n×p)(p×1)
(A.10)
Here g() is required to be an invertible, smooth function of the mean vector μ of the yi.6
Information from the explanatory variables is now expressed in the model only through
the link from the linear structure, Xβ, to the linear predictor, θ = g(μ), controlled by the
form of the link function, g(). This link function connects the linear predictor to the mean
of the outcome variable, not directly to the expression of the outcome variable itself, so the
outcome variable can now take on a variety of non-normal forms. The link function connects
the stochastic component that describes some response variable from a wide variety of forms
to all the standard normal theory supporting the systematic component through the mean
function:
g(μ) = θ = Xβ
g−1(g(μ)) = g−1(θ) = g−1(Xβ) = μ = E(Y).
So the inverse of the link function ensures that Xβ maintains the Gauss-Markov assumptions
for linear models and all the standard theory applies even though the outcome variable no
longer meets the required assumptions.
The generalization of the linear model now has four components derived from the ex-
pressions above.
I. Stochastic Component: Y is the random or stochastic component that remains
distributed iid according to a speciﬁc parametric family distribution with mean μ.
II. Systematic Component: θ = Xβ is the systematic component with an associated
Gauss-Markov normal basis.
III. Link Function: the stochastic component and the systematic component are linked
by a function of θ, which is exactly the canonical link function, summarized in Table
6More speciﬁcally, g() must be a one-to-one function that is everywhere diﬀerentiable over the support
of μ.

Generalized Linear Model Review
559
A.1 below. We can consider g(μ) as “tricking” the linear model into thinking that it
is still acting upon normally distributed outcome variables.
IV. Residuals: Although the residuals can be expressed in the same manner as in the
standard linear model, observed outcome variable value minus predicted outcome vari-
able value, a more useful quantity is the deviance residual described in detail below.
This setup is much more powerful than it initially appears. The outcome variable described
by the outcome family form is aﬀected by the explanatory variables strictly through the
link function applied to the systematic component, g−1(Xβ), and nothing else.
Table A.1 summarizes the link functions for common distributions.7 Note that g() and
g−1() are both included.
TABLE A.1: Natural Link Functions for Common Specifications
Canonical Link
Inverse Link
Distribution
θ = g(μ)
μ = g−1(θ)
Poisson
log(μ)
exp(θ)
Normal
μ
θ
Gamma
−1
μ
−1
θ
Negative binomial
log(1 −μ)
1 −exp(θ)
Binomial logit
log

μ
1−μ

exp(θ)
1+exp(θ)
Binomial probit
Φ−1 (μ)
Φ (θ)
Binomial cloglog
log (−log(1 −μ))
1 −exp (−exp(θ))
A substantial advantage of the generalized linear model is its freedom from the standard
Gauss-Markov assumption that the residuals have mean zero and constant variance. Yet
this freedom comes with the price of interpreting more complex stochastic structures. Cur-
rently, the dominant philosophy is to assess this stochastic element by looking at (summed)
discrepancies: a function that describes the diﬀerence between observed and expected out-
come data for some speciﬁed model: D = n
i=1 d(θ, yi). This deﬁnition is intentionally left
vague for the moment to stress that the format of D is widely applicable. For instance, if
the discrepancy in D is measured as the squared arithmetic diﬀerence from a single mean,
then this becomes the standard form for the variance. In terms of generalized linear mod-
els, the squared diﬀerence from the mean will prove to be an overly restrictive deﬁnition of
discrepancy, and a likelihood-based measure will be shown to be far more useful.
7In the case of the binomial, only the logit link function qualiﬁes as the canonical link function. The
other two forms are provided because of their substitutabillity and widespread use. See also the robit model
for robust estimation of dichotomous outcomes (Gelman and Hill 2007, pp.124-5).

560
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
A.2.2
Deviance Residuals
Starting with the log likelihood for a proposed model from the (A.2) notation, add the
‘ˆ’ notation as a reminder that it is evaluated at the maximum likelihood values:
ℓ(ˆθ, ψ|y) =
n

i=1
yiˆθ −b(ˆθ)
a(ψ)
+ c(y, ψ),
where ψ is a scale parameter and the exponential family form is given by its “canonical”
form: f(y|θ) = exp

yθ−b(θ)
a(ψ)
+ c(y, ψ)

. When a given PDF or PMF does not have a scale
parameter, then a(ψ) = 1 and this reduces to the previous form. Often we need to explicitly
treat these nuisance parameters instead of ignoring them or assuming they are known, and
the most important case of a two-parameter exponential family is this form here where the
second parameter is a scale parameter.
Also consider the same log likelihood function with the same data and the same link
function, except that it now has n coeﬃcients for the n data points, i.e., the saturated
model log likelihood function with the ‘˜’ function to denote the n-length θ vector:
ℓ(˜θ, ψ|y) =
n

i=1
yi˜θ −b(˜θ)
a(ψ)
+ c(y, ψ).
This is the highest possible value for the log likelihood function achievable with the given
data, y. Yet it is also often analytically unhelpful except as a benchmark. The deviance
function is deﬁned as minus twice the log likelihood ratio (that is, the arithmetic diﬀerence
since both terms are already written on the log metric):
D(θ, y) = −2

ℓ(ˆθ, ψ|y) −ℓ(˜θ, ψ|y)

= −2
n

i=1

yi(ˆθ −˜θ) −

b(ˆθ) −b(˜θ

a(ψ)−1.
(A.11)
A utility of the deviance function is that it also allows a look at the individual deviance
contributions in an analogous way to linear model residuals. The single point deviance
function is just the deviance function for the yth
i
point (i.e., without the summation):
d(θ, yi) = −2

yi(ˆθ −˜θ) −

b(ˆθ) −b(˜θ

a(ψ)−1.
To deﬁne the deviance residual at the yi point, we take the square root:
RDeviance = (yi −μi)
|yi −μi|

|d(θ, yi)|,
where (yi−μi)
|yi−μi| is a sign-preserving function. For example, the natural link function for the
binomial is the logit function: θ = log(μ/(1 −μ)), and the log likelihood contribution from
a single datum, in exponential family form, is given by ℓ(p|yi, ni) = yi log(p/(1 −p)) −
(−ni log(1 −p)) + log
ni
yi

. Substituting in the maximum likelihood result for p, μi = niˆp,
and the saturated model result for p, yi = ni˜p into (A.11) gives:
D(p, y) = −2
n

i=1
'
yi log
 yi
μi

+ (ni −yi) log
 ni −yi
ni −μi
(
.
(A.12)

Generalized Linear Model Review
561
Comparisons are made between: the null model, a common mean μ for all y meaning
y = g−1(μ+ϵ) (data is modeled as all random variation). the saturated or full model, where
the data are explained exactly but no data reduction or underlying trend information is
obtained. This is typically n parameters for n datapoints (data is modeled as all systematic).
the proposed model where we have partitioned the data into systematic structures and a
random component according to some theoretical consideration. The log-likelihood for the
full model versus the research model can be compared in ratio terms:
2(ℓ(y, φ|y) −ℓ(ˆμ, φ|y)).
(A.13)
Assuming iid data and a(φi) = φ/wi, this becomes:

i
2wi(yi(˜θi −ˆθi) −b(˜θi) + b(ˆθi))/φ
(A.14)
Call D(y, ˆμ) the deviance, and the above forms the scaled deviance (D(y, ˆμ)/φ). For common
models, these deviances are summarized in Table A.2.
TABLE A.2: Summed Deviances for Common Specifications
GLM
Unscaled Deviance, D(y, ˆμ)
Gaussian

i(yi −ˆμi)2
Poisson
2 
i [yi log(yi/ˆμi) + (yi −ˆμi)]
Binomial
2 
i [yi log(yi/ˆμi) + (m −ˆμi) log(((m −yi)/(m −ˆμi))]
where m is the sample size so μ is the count not the proportion.
Gamma
2 
i [−log(yi/ˆμi) + (yi −ˆμi)/ˆμi]
Negative Binomial

i(yi −ˆμi)2/(ˆμ2
i yi)
In standard practice, the two main tools: the deviances, as described, and Pearson’s
statistic:
X2 =

i
R2
P earson =

i
(yi −ˆμi)2
var(ˆμi)
(A.15)
which lead to asymptotic χ2 tests with the degrees of freedom equal to the diﬀerence in
the number of parameters. Paradigm: We compare two nested models where the more
parsimonious model is one that puts linear restrictions (usually βj = 0) on the parameters.
Intuition: an unrestricted model versus a restricted model. The goodness of ﬁt test to the
data is just a nested test where the nesting model is the saturated model. Two caveats: if

562
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
φ ̸= 1, more elaborate testing required (estimates of φ), and sample sizes need to be large
for less-granular responses (huge for dichotomous).
For the deviance comparison we are comparing a large (possibly saturated) model Ω to
a restricted research model of interest ω. The diﬀerence in the scaled deviances, Dω −DΩ is
asymptotically χ2 with df the number of restrictions. The restricted model will have larger
deviances because we are making theoretical statements away from just trending through
the data. General test: Model 1 has p parameters and model 2 has q > p parameters. Then
Dp −Dq asym
∼
χ2
df=q−p.
(A.16)
If this diﬀerence is “small” then the restrictions make sense. If this diﬀerence is “large” then
they take us far from what the data want to say. Our claim is that the p −q parameters
all have coeﬃcients equal to zero (this is the restriction). The number of parameters in
each model determine the corresponding degrees of freedom: the saturated model has n
parameters, the research model with p parameters (counting the intercept), and the null
model has 1 parameter to account for the mean. To test the research model versus null
model: the research model gives DΩ, the null model gives Dω, and the degrees of freedom
are #params(Ω)−#params(ω) = p−1. So large values of Dω −DΩ support the saturated
model. In this case we want to be in the tail of the associated chi-square test to distinguish
the research model from the simple null model. To test the saturated model versus research
model: the saturated model gives DΩ, the research model gives Dω, and the degrees of
freedom are #params(Ω) −#params(ω) = n −p So small values of Dω −DΩ support the
research model by indicating that it is statistically close to the saturated model. Note that
failing this test does not discredit any speciﬁcation as it merely means more information
may be available to use.
■Example A.1:
Poisson Model of Military Coups. Sub-Saharan Africa has ex-
perienced a disproportionately high proportion of regime changes due to the military
takeover of government for a variety of reasons, including ethnic fragmentation, ar-
bitrary borders, economic problems, outside intervention, and poorly developed gov-
ernmental institutions. These data, selected from a larger set collected by Bratton
and Van De Walle (1994),
look at potential causal factors for counts of military
coups (ranging from 0 to 6 events) in 33 sub-Saharan countries over the period from
each country’s colonial independence to 1989. Seven explanatory variables are chosen
here to model the count of military coups: Military Oligarchy (the number of years of
this type of rule); Political Liberalization (0 for no observable civil rights for political
expression, 1 for limited, and 2 for extensive); Parties (number of legally registered
political parties); Percent Legislative Voting; Percent Registered Voting; Size (in one
thousand square kilometer units); and Population (given in millions).
A generalized linear model for these data with the Poisson link function is speciﬁed
as:
g−1(θ) = g−1(Xβ) = exp [Xβ] = E[Y] = E[Military Coups].

Generalized Linear Model Review
563
In this speciﬁcation, the systematic component is Xβ, the stochastic component is
Y = Military Coups, and the link function is θ = log(μ). We can re-express this
model by moving the link function to the left-hand side exposing the linear predictor:
g(μ) = log(E[Y]) = Xβ (although this is now a less intuitive form for understanding
the outcome variable). The R language GLM call for this speciﬁcation is:
data(africa)
africa.out <- glm(MILTCOUP ~ MILITARY + POLLIB + PARTY93 + PCTVOTE
+ PCTTURN + SIZE*POP + NUMREGIM*NUMELEC,
family=poisson, data=africa)
summary(africa.out)
This gives the model results in Table A.3.
TABLE A.3:
Poisson Model of Military Coups
Coeﬃcient
Std. Error
95% CI
Intercept
2.9209
1.3368
[ 0.3008: 5.5410]
Military Oligarchy
0.1709
0.0509
[ 0.0711: 0.2706]
Political Liberalization
-0.4654
0.3319
[-1.1160: 0.1851]
Parties
0.0248
0.0109
[ 0.0035: 0.0460]
Percent Legislative Voting
0.0613
0.0218
[ 0.0187: 0.1040]
Percent Registered Voting
-0.0361
0.0137
[-0.0629:-0.0093]
Size
-0.0018
0.0007
[-0.0033:-0.0004]
Population
-0.1188
0.0397
[-0.1965:-0.0411]
Regimes
-0.8662
0.4571
[-1.7621: 0.0298]
Elections
-0.4859
0.2118
[-0.9010:-0.0709]
(Size)(Population)
0.0001
0.0001
[ 0.0001: 0.0002]
(Regimes)(Elections)
0.1810
0.0689
[ 0.0459: 0.3161]
Note that the two interaction terms are speciﬁed by using the multiplication character.
The iteratively weighted least squares algorithm converged in only four iterations using
Fisher scoring, and the results are provided in the table. The model appears to ﬁt the
data quite well: an improvement from the null deviance of 62 on 46 degrees of freedom
to a residual deviance of 7.5 on 35 degrees of freedom (evidence that the model does
not ﬁt would be supplied by a deviance value in the tail of a χ2
n−k distribution).
Nearly all the coeﬃcients have 95% conﬁdence intervals bounded away from zero and
therefore appear reliable in the model. Since this is a Poisson link function, we might
also want to check for overdispersion:
sum(residuals(africa.out,type="pearson")^2)
[1] 6.823595

564
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
1-pchisq(sum(residuals(africa.out,type="pearson")^2),
df=africa.out$df.residual, lower.tail=FALSE)
[1] 0.9984635
This gives no evidence that we have extra-Poisson variance.
A.3
Numerical Maximum Likelihood
Generally the maximum likelihood estimate for the unknown parameter vector in a
generalized linear model does not have a closed-form analytical solution. So this section
explains the method for applying maximum likelihood estimation to generalized linear mod-
els using numerical techniques. The primary iterative root-ﬁnding procedure is iteratively
weighted least squares (IWLS), created by Nelder and Wedderburn (1972).
In order to
fully understand the numerical aspects of this technique, this section ﬁrst discusses ﬁnding
coeﬃcient estimates in nonlinear models (i.e., simple root ﬁnding), followed by weighted
regression, and ﬁnally the iterating algorithm. This section can be skipped on ﬁrst reading
as it addresses the lower level computational aspects of statistical software only.
A.3.1
Newton-Raphson and Root Finding
The problem of ﬁnding the best possible estimate for some coeﬃcient value typically
reduces to the problem of ﬁnding the mode of the likelihood function, given some observed
data and a parametric speciﬁcation. In textbook examples, this is the challenge of ﬁnding
the point where the ﬁrst derivative of the likelihood function is equal to zero (including the
secondary consideration that the second derivative be negative to ensure that the point is
not a minima). However, in realistic work, the problem is that of ﬁnding a high-dimensional
mode with likelihood functions that are considerably less simple and even have suboptimal
modes.
Rather than using analytical solutions we are driven to using numerical techniques, which
are algorithms that manipulate the data and the speciﬁed model to produce a mathematical
solution for the modal point. Because the solutions are produced by iterating/searching
procedures using the data, they are considerably more messy with regard to machine-
generated round-oﬀand truncation in intermediate steps of the applied algorithm.
Consider the problem of numerical maximum likelihood estimation as that of ﬁnding the
top of an “ant hill” in the parameter space. It is easy then to see that this is equivalent to
ﬁnding the parameter vector value where the derivative of the likelihood function is equal
to zero: where the tangent line is horizontal. The most widely used procedure is called
Newton-Raphson, based on Newton’s method for ﬁnding the roots of polynomial equations.
Newton’s method exploits the properties of a Taylor series expansion around some given

Generalized Linear Model Review
565
point. The Taylor series expansion gives the relationship between the value of a mathemat-
ical function at point, x0, and the function value at another point, x1, given as:
f(x1) = f(x0) + (x1 −x0)f ′(x0) + 1
2!(x1 −x0)2f ′′(x0)
+ 1
3!(x1 −x0)3f ′′′(x0) + . . . ,
where f ′ is the ﬁrst derivative with respect to x, f ′′ is the second derivative with respect to
x, and so on. Note that it is required that f() have continuous derivatives over the relevant
support of the function here: the allowable parameter space of x. Otherwise the algorithm
fails.
Inﬁnite precision is achieved with the inﬁnite extending of the series into higher or-
der derivatives and higher order polynomials (of course the factorial component in the
denominator means that these are rapidly decreasing increments). This process is both
unobtainable and unnecessary, and only the ﬁrst two terms are required as a step in an
iterative process.
The point of interest is x1 such that f(x1) = 0. This value is a root of the function, f()
in that it provides a solution to the polynomial expressed by the function. It is also the point
where the function crosses the x-axis in a graph of x versus f(x). This point could be found
in one step with an inﬁnite Taylor series: 0 = f(x0)+(x1 −x0)f ′(x0)+ 1
2!(x1 −x0)2f ′′(x0)+
. . . +
1
∞!(x1 −x0)∞f (∞)(x0) + . . . . While this is impossible, it is true that we could use
just the ﬁrst two terms to get closer to the desired point: 0 ∼= f(x0) + (x1 −x0)f ′(x0). Now
rearrange to produce at the (j + 1)th step: x(j+1) = x(j) −f(x(j))
f ′(x(j)), so that progressively
improved estimates are produced until f(x(j+1)) is suﬃciently close to zero. It has been
shown that this method converges quadratically to a solution provided that the selected
starting point is reasonably close to the solution, although the results can be very bad if
this condition is not met.
■Example A.2:
A Simple Application of Newton-Raphson. Suppose that we
wanted a numerical routine for ﬁnding the square root of a number, μ.
This is
equivalent to ﬁnding the root of the simple equation f(x) = x2 −μ = 0.
The
ﬁrst derivative is just
d
dxf(x) = 2x. If we insert these functions into (j + 1)th step:
x(j+1) = x(j) −f(x(j))
f ′(x(j)), we get:
x(j+1) = x(j) −(x(j))2 −μ
2x(j)
= 1
2(x(j) + μ(x(j))−1).
(A.17)
A very basic R function for implementing (A.17) is:
newton.raphson.ex <- function(mu,x,iterations)
{
for (i in 1:iterations)
x <- 0.5*(x + mu/x)
return(x)
}

566
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
This function is run with the command newton.raphson.ex(99,2,3) to ﬁnd the
square root of 99 starting at 2, but only taking 3 steps. The result is the (incorrect)
value 10.74386. However, running the function according to newton.raphson.ex(99,2,6),
that is six iterations instead of three, gives the correct value of 9.949874.
A.3.1.1
Newton-Raphson for Statistical Problems
The Newton-Raphson algorithm, when applied to mode ﬁnding in a statistical setting,
substitutes θ(j+1) for x(j+1) and θ(j) for x(j) (where the θ values are iterative estimates of
the parameter vector) and f() is the score function (A.3). The real goal is to estimate a
k-dimensional ˆθ estimate, given data and a model. The applicable multivariate likelihood
updating equation is now provided by:
θ(j+1) = θ(j) −∂
∂θ ℓ(θ(j)|x)

∂2
∂θ∂θ′ ℓ(θ(j)|x)
−1
.
(A.18)
When θ(j) is the maximum likelihood coeﬃcient vector, the quantity in the denominator is
a matrix called the Hessian. For exponential family distributions and natural link functions,
the observed and expected Hessian are identical (Fahrmeir and Tutz 2001, p.42; Lehmann
and Casella, 1998, pp.124-8), although Efron and Hinkley (1978) give situations where the
observed information is preferable. So it is common to replace this calculation with forms
that are equivalent for the exponential family, such as the Fisher Information:
AF = −E

∂2
∂θ∂θ′ ℓ(θ(j)|x)

(Fisher 1925), or the square of the score function:
AB = E
' ∂
∂θ ℓ(θ(j)|x)′ ∂
∂θ ℓ(θ(j)|x)
(
.
which is sometimes called the BHHH method (Berndt, Hall, Hall, and Hausman 1974).
At each step of these Newton-Raphson steps, a system of equations determined by the
multivariate normal equations must be solved:
A
 !"#
angle
(θ(j+1) −θ(j))
 
!"
#
direction: δb
=
∂
∂θ ℓ(θ(j)|x)
 
!"
#
size of direction: u
,
(A.19)
which builds a linear structure in the parameter vector, and leads to estimates from the
system of linear equations: δb = A−1u.
It is computationally convenient to solve on
each iteration by least squares, so that the problem of mode ﬁnding reduces to a repeated
weighted least squares application in which the inverse of the diagonal values of the second
derivative matrix in the denominator are the appropriate weights (this is a diagonal matrix
by the iid assumption).
A.3.1.2
Weighted Least Squares
A standard compensating technique for nonconstant error variance (so-called heteroscedas-
ticity) is to insert a diagonal matrix of weights, Ω, into the calculation of ˆβ such that this

Generalized Linear Model Review
567
heteroscedasticity is removed by design. The Ω matrix is created by taking the error vari-
ance of the ith case (estimated or known), νi, and assigning it to the ith diagonal Ωii =
1
νi ,
leaving the oﬀ-diagonal elements as zero. So large error variances are reduced by premulti-
plying the model terms by this reciprocal.
We can premultiply each term in the standard linear model setup, by the square root of
the Ω matrix (that is, by the standard deviation). This “square root” is actually produced
from a Cholesky factorization: if A is a positive deﬁnite symmetric (A′ = A) matrix,
then there must exist a matrix G such that: A = GG′. A matrix, A, is positive deﬁnite
if for any nonzero p × 1 vector x, x′Ax > 0. In our case, this decomposition is greatly
simpliﬁed because the Ω matrix has only diagonal values (all oﬀ-diagonal values equal to
zero). Therefore the Cholesky factorization is produced simply from the square root of these
diagonal values. Premultiplying gives:
Ω
1
2 Y = Ω
1
2 Xβ + Ω
1
2 ϵ.
(A.20)
Instead of minimizing squared errors in the usual manner, we now minimize (Y−Xβ)′Ω(Y−
Xβ), and the subsequent weighted least squares estimator is found by ˆβ = (X′ΩX)−1X′ΩY.
The weighted least squares estimator gives the best linear unbiased estimate (BLUE) of the
coeﬃcient estimator in the presence of heteroscedasticity.
A.3.1.3
Iterative Weighted Least Squares
It is more common that the individual variances used to make the reciprocal diagonal
values for Ω are unknown, and cannot be easily estimated, but are known to be a function
of the mean of the outcome variable: νi = f(E[Yi]). So if the expected value of the outcome
variable, E[Yi] = μ and the form of the relation function, f() are known, then this is a very
straightforward estimation process. Unfortunately, although it is common for the variance
structure to be dependent on the mean function, it is relatively rare to know the exact form
of the dependence.
Nelder and Wedderburn (1972) provide a solution to this problem that iteratively re-
estimates the weights, improving the estimate on each cycle using the mean function. Be-
cause μ = g−1(Xβ), then the coeﬃcient estimate, ˆβ, provides a mean estimate and vice
versa. So the algorithm iteratively estimates these values using progressively better weights.
This proceeds as follows. First assign starting values to the weights, generally equal to one:
1
ν(1)
1
= 1, and specify the diagonal matrix Ω, guarding against division by zero. Then iterate
the following steps:
▷Deﬁne the current (or starting) point of the linear predictor by:
ˆη0
(n×1)
=
Xβ0
(n×p)(p×1)
with ﬁtted value ˆμ0 from g−1(ˆη0).

568
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
▷Form the “adjusted dependent variable” according to:
z0
(n×1)
=
ˆη0
(n×1)
+
*
∂η
∂μ

ˆμ0
,
diag(n×n)
(y −ˆμ0)
(n×1)
which is a linearized form of the link function applied to the data. As an example of
this derivative function, the Poisson form looks like η = log(μ) =⇒
∂η
∂μ = 1
μ.
▷Form the quadratic weight matrix, which is the variance of z:
w−1
0
(n×n)
=
*
∂η
∂μ

ˆμ0
,2
v(μ)|ˆμ0
where v(μ) is the variance function:
∂
∂θb′(θ) = b′′(θ).
▷So the general iteration scheme is:
1. Construct z, w. Regress z on the covariates with weights to get a new interim
estimate:
ˆβ1
(p×1)
= ( X′
(p×n) w0
(n×n)
X
(n×p))−1 X′
(p×n) w0
(n×n)
z0
(n×1)
2. Use the coeﬃcient vector estimate to update the linear predictor:
ˆη1 = X′ ˆβ1
3. Iterate: z1, w1 ⇒ˆβ2, ˆη2, z2, w2 ⇒ˆβ3, ˆη3, z3, w3 ⇒ˆβ4, ˆη4, . . . .
These steps are repeated until convergence (i.e., Xˆβj −Xˆβj+1 is close to zero). Under very
general conditions, satisﬁed by the exponential family of distributions, the IWLS procedure
ﬁnds the mode of the likelihood function, thus producing the maximum likelihood estimate
of the unknown coeﬃcient vector, ˆβ, and the variance matrix, ˆσ2(X′ΩX)−1 (Gill 2000,
Green 1984, Del Pino 1989).
The purpose of this section is to illustrate the means by which generalized linear models
are estimated in practice. This process is the core methodological practice in non-Bayesian
statistical analysis in the social and behavioral sciences, even though many practitioners be-
lieve they are using some particularistic model (logit, Poisson, duration, etc.). It should not
be inferred from this discussion that this process always works without computational diﬃ-
culties. For extensive discussions of maximum likelihood computational issues and what can
go wrong, see Altman and McDonald (2001), McCullough (1998, 1999), and Gill, Murray,
and Wright (1981, Chapter 8).
A.4
Quasi-Likelihood
Wedderburn (1974) introduced the concept of “quasi-likelihood” estimation to address
circumstances when either the parametric form of the likelihood is known to be misspec-

Generalized Linear Model Review
569
iﬁed, or only the ﬁrst two moments8 are deﬁnable. Albert (1988) and Albert and Pepple
(1989) detail the utility of applying quasi-likelihood models to hierarchical Bayesian esti-
mation problems where the posterior calculation is diﬃcult. This is before the revolution
brought on by the widespread use of MCMC techniques, but quasi-likelihood remains useful
as a way of relaxing distributional assumptions in model speciﬁcations. General applica-
tions of quasi-likelihood speciﬁcations in the social sciences include: White’s (1982) look at
econometric model misspeciﬁcation, Goldstein and Rasbash’s (1996) application to public
policy, the use of quasi-likelihood in analyzing multinational corporate decision-making by
Hannan et al. (1995), Western’s (1995) application to studying union decline, Sampson and
Raudenbush’s (1999) study of behavior in public spaces, and Mebane’s (1994) evaluation
of the linkage between taxation policy and subsequent elections.
Suppose that we know something about the parametric form of the distribution gener-
ating the data, but not in complete detail. Obviously this precludes the standard maxi-
mum likelihood estimation of unknown parameters since we cannot specify a full likelihood
equation. Wedderburn’s idea was to develop an estimation procedure that only requires
speciﬁcation for the mean function of the data and a stipulated relationship between this
mean function and the variance function. This is useful in a Bayesian context when we have
prior information readily at hand but only a vague idea of the form of the likelihood.
Re-express the exponential family form as a joint distribution function of observed data:
f(y|θ) = exp
 n

i=1
yiθ −nb(θ) +
n

i=1
c(yi)

,
(A.21)
and with the more realistic assumption of a multiparameter model with k parameters:
f(y|θ) = exp
n
i=1
k
j=1 yθj −nb(θj) + n
i=1 c(y)

. Adopting the canonical form with a
scale parameter gives: f(y|θ) = exp

yθ−b(θ)
a(ψ)
+ c(y, ψ)

.
We can also deﬁne a variance function for a given exponential family expression, which is
used in generalized linear models to indicate the dependence of the variance of Y on location
and scale parameters: v(μ) =
∂2
∂θ2 b(θ), meaning that Var[Y ] = a(ψ)v(μ) indexed by θ. Note
that the dependence on b(θ) explicitly states that the variance function is conditional on the
mean function, whereas there was no such stipulation with the a(ψ) form. It is conventional
to leave the variance function in terms of the canonical parameter, θ, rather than return it
to the parameterization in the original probability function as was done for the variance of
Y . Table A.4 summarizes some common variance functions.
The log likelihood function in this context is now written as:
ℓ(θ) =
n

i=1
k

j=1
yθj −nb(θj)
a(ψ)
+
n

i=1
c(y, ψ),
(A.22)
8Moments are characterizations of distributions based on expectations. The nth moment of a random
variable X is given by: μn = E[Xn]. So the mean of X is the ﬁrst moment: E[X] = μ1, and the variance
of X is the second moment minus the square of the ﬁrst moment: Var[X] = μ2 −(μ1)2. See Stuart and
Ord (1994, Chapter 3) for a lengthy discussion of moments and related quantities.

570
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
TABLE A.4: Normalizing Constants and Variance Functions
Distribution
b(θ)
v(μ) =
∂2
∂θ2 b(θ)
Poisson
exp(θ)
exp(θ)
Binomial
n log (1 + exp(θ))
n exp(θ)(1 + exp(θ))−2
Normal
θ2
2
1
Gamma
−log(−θ)
1
θ2
Negative binomial
r log(1 −exp(θ))
r exp(θ)(1 −exp(θ))−2
where E[y] = ∂b(θj)
∂θ . The expected value of the ﬁrst derivative of the joint distribution is
equal to zero since this is the slope of the tangent line at the mode:
E
'∂ℓ(θ)
∂θ
(
= E
 n

i=1
y
a(ψ) −∂nb(θj)
∂θ
1
a(ψ)

= 0,
and
Var
'∂ℓ(θ)
∂θ
(
= −E
'∂2ℓ(θ)
∂θ2
(
= −E
'
−∂2nb(θj)
∂θ2
1
a(ψ)
(
= ∂2nb(θj)
∂θ2
1
a(ψ).
So again Var[Y ] = a(ψ)v(μ), where v(μ) = ∂2nb(θj)
∂θ2
is the variance function.
Instead of taking the ﬁrst derivative of log likelihood with respect to the parameter
vector, θ, suppose we take this derivative with respect to the mean function in a generalized
linear model, μ, with the analogous property that: E

∂ℓ(θ)
∂μ

= 0. Thus
Var
'∂ℓ(θ)
∂μ
(
= −E
'∂2ℓ(θ)
∂μ2
(
=
1
a(ψ)v(μ).
Therefore what we have here is a linkage between the mean function and the variance
function that does not depend on the form of the likelihood function. If we combine this
with a form of ∂ℓ(θ)
∂μ
that satisﬁes the condition above that the expected value of its ﬁrst
derivative is zero, and the variance property above holds, then we have a replacement for the
unknown speciﬁc form of the likelihood function that still provides the desired properties
of maximum likelihood estimation as described. Thus we imitate these three criteria of the
score function with a function that contains signiﬁcantly less parametric information: only
the mean and variance.
A substitution function that satisﬁes all of these conditions is: q =
y−μ
a(ψ)v(μ) (McCullagh
and Nelder 1989, 325; McCulloch and Searle 2001, 152; Shao 2005). The contribution to
the log likelihood function from the ith point is deﬁned by:
Qi =
 μi
yi
yi −t
a(ψ)v(μ)dt,
so ﬁnding the maximum likelihood estimator for this setup, ˆθ is equivalent to solving:
∂
∂θ
n

i=1
Qi =
n

i=1
yi −μi
a(ψ)v(μ)
∂μi
∂θ =
n

i=1
yi −μi
a(ψ)v(μ)
xi
g(μ) = 0,

Generalized Linear Model Review
571
where g(μ) is the canonical link function for a generalized linear model speciﬁcation. In
other words we can use the usual maximum likelihood engine for inference with complete
asymptotic properties such as consistency and normality (McCullagh 1983), by only speci-
fying the relationship between the mean and variance functions as well as the link function
(which actually comes directly from the form of the outcome variable data).
As an example, suppose we assume that the mean and variance function are related by
stipulating that a(ψ) = σ2 = 1, and b(θ) = θ2
2 , so v(μ) = d2b(θ)
dθ2
= 1. Then it follows that:
Qi =
 μi
yi
yi −t
a(ψ)v(μ)dt = −(yi −μi)2
2
.
The quasi-likelihood solution for ˆθ comes from solving the quasi-likelihood equation:
d
dθ
n

i=1
Qi = d
dθ
n

i=1
yi −θ
2
= −
n

i=1
yi + nθ = 0.
In other words, ˆθ = ¯y, because this example was set up with the same assumptions as a
normal maximum likelihood problem but without specifying a normal likelihood function.
Quasi-likelihood models drop the requirement that the true underlying density of the
outcome variable belongs to a particular exponential family form.
Instead, all that is
required is the identiﬁcation of the ﬁrst and second moments and an expression for their
relationship up to a proportionality constant.
It is assumed that the observations are
independent and that mean function describes the mean eﬀect of interest.
Even given
this generalization of the likelihood assumptions, it can be shown that quasi-likelihood
estimators are consistent asymptotically equal to the true estimand (Fahrmeir and Tutz
2001, pp.55-60, Firth 1987; McCullagh 1983).
However, a quasi-likelihood estimator is
often less eﬃcient than a corresponding maximum likelihood estimator (McCullagh and
Nelder 1989, pp.347-348; Shao 2005).
Despite this drawback with regard to variance, there are often times when it is conve-
nient or necessary to specify a quasi-likelihood model. A number of authors have extended
the quasi-likelihood framework to: extended quasi-likelihood models to compare diﬀerent
variance functions for the same data (Nelder and Pregibon 1987), pseudo-likelihood models,
which build upon extended quasi-likelihood models by substituting a χ2 component instead
of a deviant component in dispersion analysis (Breslow 1990; Carroll and Ruppert 1982;
Davidian and Carroll 1987), and models where the dispersion parameter is dependent on
speciﬁed covariates (Smyth 1989). Nelder and Lee (1992) provide an informative overview
of these variations. It is also the case that quasi-likelihood models are not more diﬃcult
to compute (Nelder 1985), and the R package has preprogrammed functions that make the
process routine (see below).
■Example A.3:
A Quasi-Likelihood Model of Military Coups.
We can develop
a quasi-likelihood model for counts in the Africa Coups data as an alternative to
stipulating a regular Poisson GLM as done in the previous example. First stipulate

572
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
that:
μi = v(μi),
(A.23)
so that
Qi =
 μi
yi
yi −t
a(ψ)v(μ)dt = (yi log μi −μi −yi log +yi)/a(ψ).
(A.24)
Therefore we get the form:
d
dμ
n

i=1
Qi = d
dμ
n

i=1
(yi log μi −μi),
(A.25)
which is the same as the regular GLM model for a Poisson link function except that
the scale parameter is no longer ﬁxed to be equal to one and is instead estimated by:
ˆ
a(ψ) = (n −k)−1 
(yi −μi)2/v(μi).
(A.26)
This means that the coeﬃcient estimates will be exactly as they were before but the
corresponding errors will diﬀer here because the scale parameter is now free to vary.
Constructing this model is very easy in R:
africa.quasi.out <- glm(formula = MILTCOUP ~ MILITARY + POLLIB
+ PARTY93 + PCTVOTE + PCTTURN + SIZE*POP + NUMREGIM*NUMELEC,
data=africa, family=quasipoisson(link="log"))
summary(africa.quasi.out)
Running this we notice that the coeﬃcient estimates are identical to the regular Pois-
son GLM speciﬁcation, as are the deviance summaries. The estimated scale parameter
is estimated to be 0.325, justifying use of the more ﬂexible form. The only diﬀerences
found, as expected, are the resulting standard errors:
Intercept
MILITARY
POLLIB
PARTY93
0.76200
0.02901
0.18920
0.00619
PCTVOTE
PCTTURN
SIZE
POP
0.01240
0.00780
0.00041
0.02260
NUMREGIM
NUMELEC
SIZE:POP
NUMREGIM:NUMELEC
0.26057
0.12070
0.00002
0.03929
A.5
Exercises
A.1
Suppose X1, . . . , Xn are iid exponential: f(x|θ) = θe−θx,
θ > 0. Find the max-
imum likelihood estimate of θ by constructing the joint distribution, express the
log likelihood function, take the ﬁrst derivative with respect to θ, set this function
equal to zero, and solve for ˆθ the maximum likelihood value.

Generalized Linear Model Review
573
A.2
If the variance of the residuals in the linear model is not constant, then the regres-
sion model is heteroscedastic. The general linear model can be used when the form
of the heteroscedasticity is known. Assuming the residuals are uncorrelated, the
new n × n variance matrix is given by:
σΩ =
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎣
σ2
1
0
. . .
. . .
0
0
σ2
2
0
. . .
0
...
. . .
...
. . .
...
...
. . .
. . .
...
...
0
0
0
. . .
σ2
n
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎦
.
Using this matrix, calculate the new maximum likelihood estimate for the unknown
parameter vector b.
A.3
Below are two sets of data each with least square regression lines calculated (ˆy =
6 + 0x). Answer the following questions by looking at the plots.
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
• •
•
•
•
•
•
•
•
•
•
•
•
•
•
••
•
•
•
•
•
•
•
•
•
•
•
•
•
••
•
•
•
•
•
••
•
•
•
••
•
•
•
•
•
•
•
•
•
•
• •
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
••
•
•
•
•
•
•
••• •
•
•
•
•
•
•
•
•
•
•
••
•
••
•
•
•
• •
•
•
•
•
•
•
•••
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
••
•
•
•
••
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
••
•
(a) Does the construction of the least squares line in panel 1 violate any of the
Gauss-Markov assumptions?
(b) Does the construction of the least squares line in panel 2 violate any of the
Gauss-Markov assumptions?
(c) Does the identiﬁed point (identically located in both panels) have a substan-
tively diﬀerent interpretation?
A.4
Calculate the maximum likelihood estimate of the intensity parameter of the Pois-
son distribution, f(y|μ) = e−μμy
y!
,
μ > 0, for the data: [7, 4, 3, 4, 7, 6, 9, 11, 21, 3].
A.5
Consider the bivariate normal PDF:
f(x1, x2) =

2πσ1σ2

1 −ρ2
−1
×
exp
'
−
1
2(1 −ρ2)
(x1 −μ1)2
σ2
1
−2ρ(x1 −μ1)(x2 −μ2)
σ1σ2
+ (x2 −μ2)2
σ2
2
(
.

574
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
for −∞< μ1, μ2 < ∞, σ1, σ2 > 0, and ρ ∈[−1 : 1].
For μ1 = 3, μ2 = 2, σ1 = 0.5, σ2 = 1.5, ρ = 0.75, calculate a grid search using R for
the mode of this bivariate distribution on R2. A grid search bins the parameter
space into equal space intervals on each axis and then systematically evaluates each
resulting subspace. First set up a two dimensional coordinate system stored in a
matrix covering 99% of the support of this bivariate density, then do a systematic
analysis of the density to show the mode without using “for” loops. Hint: see the
R help menu for the outer function. Use the contour function to make a ﬁgure
depicting bivariate contour lines at 0.05, 0.1, 0.2, and 0.3 levels.
A.6
For the simpliﬁed Pareto PDF:
f(x|θ) = θx−θ−1,
0 ≤x, θ < ∞,
ﬁnd the maximum likelihood estimate for an iid sample: X1, . . . , Xn.
A.7
Derive the exponential family form and b(θ) for the Inverse Gaussian distribution:
f(x|μ, λ) =

λ
2πx3
1/2
exp
'
−
λ
2μ2x(x −μ)2
(
,
x > 0, μ > 0.
Assume λ = 1.
A.8
Using exponential family assumptions and notation, show that the score function
has the following three important properties:
▷EY [ ∂
∂μi ℓ(θ(μi)|yi, φ)] = 0,
▷VarY

∂
∂μi ℓ(θ(μi)|yi, φ)

=
1
φv(μi),
▷−EY

∂2
∂μ2
i ℓ(θ(μi)|yi, φ)

=
1
φv(μi).
Now prove that the quasi-likelihood substitution function, qi = yi−μi
φv(μi), also satisﬁes
these three properties for likelihood estimation.
A.9
Show that the Weibull distribution, w(x|γ, β) = γ
β xγ−1 exp

−

x
β
γ
is an expo-
nential family form when γ = 2, labeling each part of the ﬁnal answer.
A.10
A well-known alternative to Newton-Raphson that does not require the calculation
of derivatives is the secant method, where at the (j+1)th step we calculate: x(j+1) =
x(j) −f(x(j))
x(j)−x(j−1)
f(x(j))−f(x(j−1)). Write an algorithm (R or pseudo-code) to apply the
secant method to the problem in the root of the simple equation f(x) = x2 −μ = 0
(i.e. a function that ﬁnds the square root of μ).
A.11
For normally distributed data a robust estimate of the standard deviation can be
calculated by (Devore 1999):
˜σ =
1
0.6745n
n

i=1
Xi −¯X
 .

Generalized Linear Model Review
575
Write an R function to calculate ˜σ for the FARM variable in Exercise A.15 below.
Compare this value to the standard deviation.
Is there evidence of inﬂuential
outliers in this variable?
A.12
Cigarette smoking among elderly males has decreased substantially whereas cigarette
smoking among elderly females has remained fairly constant. Given the following
data, calculate the maximum likelihood estimate of the mean and variance for each
group under a normal assumption. Test for a signiﬁcant diﬀerence.
Proportion of Smokers, 65 and Over
Year
1965 1974 1979 1983 1985 1990 1992 1993 1994 1995 1997 1998
Male
28.5
9.6 24.9 12.0 20.9 13.2 22.0 13.1 19.6 13.5 14.6 11.5
Female
16.6 12.4 13.5 10.5 13.2 11.1 14.9 11.5 12.8 11.5 10.4 11.2
Source: Centers for Disease Control and Prevention, National Center for Health
Statistics. National Health Interview Survey, respective years.
A.13
Derive the generalized hat matrix from ˆβ produced from the ﬁnal (nth) IWLS step.
Speciﬁcally, give the quantities of interest from the nth and (n−1)th step, and show
how the hat matrix is produced. Secondly, give the form of Cook’s D appropriate
to a GLM and show that it contains hat matrix values.
A.14
An exponential family form has quadratic variance if the variance term can be
expressed as v(μ) = φ′′(θ) = η0 + η1μ + η2μ2, where μ is the mean function. Show
that there exists an exponential family form conjugate distribution and that it also
has quadratic variance.
A.15
The likelihood function for dichotomous choice regression is given by:
L(b, y) =
n
+
i=1
[F(xib)]yi [1 −F(xib)]1−yi .
There are several common choices for the F() function:
Logit:
Λ(xib) =
1
1+exp[xib]
Probit:
Φ(xib) =
 xib
−∞
1
√
2πexp[−t2/2]dt
Cloglog:
CLL(xib) = 1 −exp (−exp(xib)).
Using the depression era economic and electoral data, calculate a dichotomous

576
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
regression model for using each of these link functions according to the following
model speciﬁcation:
FDR ∼f[(POST.DEP −PRE.DEP) + FARM]
where: FDR indicates whether or not Roosevelt carried that state in the 1932
presidential elections, PRE.DEP is the mean per-state income before the onset of
the Great Depression (1929) in dollars, POST.DEP is the mean per-state income
after the onset of the great depression (1932) in dollars, and FARM is the total
farm wage and salary disbursements in thousands of dollars per state in 1932.
State
FDR
PRE.DEP
POST.DEP
FARM
Alabama
1
323
162
4067
Arizona
1
600
321
6100
Arkansas
1
310
157
8134
California
1
991
580
83371
Colorado
1
634
354
10167
Connecticut
0
1024
620
10167
Delaware
0
1032
590
3050
District of Columbia
1
1269
1054
0
Florida
1
518
319
14234
Georgia
1
347
200
10167
Idaho
1
507
274
7117
Illinois
1
948
486
27451
Indiana
1
607
310
11184
Iowa
1
581
297
28468
Kansas
1
532
266
22368
Kentucky
1
393
211
8134
Louisiana
1
414
241
10167
Maine
0
601
377
6100
Maryland
1
768
512
14234
Massachusetts
1
906
613
14234
Michigan
1
790
394
14234
Minnesota
1
599
363
25418
Mississippi
1
286
127
4067
Missouri
1
621
365
15251
Montana
1
592
339
10167
Nebraska
1
596
307
17284
Nevada
1
868
550
4067
New Hampshire
0
686
427
3050
New Jersey
1
918
587
18301
New Mexico
1
410
208
5084
New York
1
1152
1676
38635
North Carolina
1
332
187
8134
North Dakota
1
382
176
14234
Ohio
1
771
400
18301
Oklahoma
1
455
216
9150
Oregon
1
668
379
11184
Pennsylvania
0
772
449
25418
Rhode Island
1
874
575
2033
South Carolina
1
271
159
7117
South Dakota
1
426
189
8134
Tennessee
1
378
198
6100
Texas
1
479
266
33552
Utah
1
551
305
4067
Vermont
0
634
365
5084
Virginia
1
434
284
15251
Washington
1
741
402
14234
West Virginia
1
460
257
6100
Wisconsin
1
673
362
21351
Wyoming
1
675
374
5084
(also provided in the R package BaM). Do you ﬁnd substantially diﬀerent results
with the three link functions? Explain. Which one would you use to report results?
Why?
A.16
Suppose you have an iid sample of size n from the exponential distribution:
f(y|θ) = 1
θ exp

−y
θ

,
y ≥0, θ > 0.
Derive the MLE for θ2. For the data y = [5, 3, 2, 1, 4], calculate the MLE.

Generalized Linear Model Review
577
A.17
Tobit regression (censored regression) deals with an interval-measured outcome
variable that is censored such that all values that would have naturally been ob-
served as negative are reported as zero, generalizeable to other values (Tobin 1958,
Amemiya (1985, Chapter 10), Chib (1992). There can be left censoring and right
censoring at any arbitrary value, single and double censoring, mixed truncating
and censoring. If z is a latent outcome variable in this context with the assumed
relation:
z = xβ + η
and
zi ∼N(xβ, σ2),
then for left censoring at zero, the observed outcome variable is produced according
to:
yi =
⎧
⎨
⎩
zi if zi > 0
0 if zi ≤0.
The resulting likelihood function is:
L(β, σ2|y, x) =
+
yi=0
'
1 −Φ
xiβ
σ
( +
yi>0
(σ−1) exp
'
−1
2σ2 (yi −xiβ)2
(
,
where σ2 is called the scale. Explain the structure of the likelihood function: what
parts accommodate observed values and what parts accommodate unobserved val-
ues? Replicate Tobin’s original analysis in R using the survreg function in the
survival package. The data are obtainable by:
tobin <- read.table(
"http://artsci.wustl.edu/~jgill/data/tobin.dat",header=TRUE)
or in the BaM package.
A.18
An eﬀective way of compensating for heteroscedasticity in probit models is to iden-
tify explanatory variables that introduce widely varying response patterns and
speciﬁcally associate them in the model with the dispersion term.
With this
method, based on Harvey (1976), the functional form is changed from: P(Y = 1) =
Φ(Xβ) to P(Y = 1) = Φ

Xβ
eZα

, where the exponent function in the denominator
is just a convenience that prevents division by zero. This approach distinguishes
between the standard treatment of the estimated coeﬃcients β with corresponding
matrix of explanatory observations X, and the set of dispersion determining esti-
mated coeﬃcients α with a corresponding matrix of explanatory observations Z.
By this means the dispersion term is reparameterized to be a function of a set of
coeﬃcients and observed values that are suspected of causing the diﬀerences in er-
ror standard deviations: σ2
i = eZiα. Derive the log likelihood of the heteroscedastic
probit and give a formal likelihood ratio test for the existence of heteroscedasticity.
A.19
Sometimes when estimation is problematic, Restricted Maximum Likelihood (REML)
estimation is helpful (Bartlett 1937).
REML uses a likelihood function calculated

578
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
from a transformed set of data so that some parameters have no eﬀect on the
estimation on the others.
For a One-Way ANOVA, y|G ∼N(Xβ + ZG, σ2I),
G ∼N(0, σ2D), such that Var[y] = Var[ZG] + Var[ϵ] = σ2ZDZ′ + σ2I. The un-
conditional distribution is now: y ∼N(Xβ, σ2(I + ZDZ′) where D needs to be
estimated. If V = I + ZDZ′, then the likelihood for the data is: ℓ(β, σ, D|y) =
−n
2 log(2π) −1
2 log |σ2V | −
1
2σ2 (y −Xβ)′V−1(y −Xβ).
The REML steps for
this model are: (1) ﬁnd a linear transformation k such that k′X = 0, so k′y ∼
N(0, k′σk), (2) run MLE on this model to get ˆD, which no longer has any ﬁxed
eﬀects, (3) then estimate the ﬁxed eﬀects with ML in the normal way. Code this
procedure in R and run the function on the depression era economic and electoral
data in Exercise 15.
A.20
Maltzman and Wahlbeck (1996) look at the reasons that Supreme Court justices
switch their votes. Consider the following distances between a justice and the au-
thor of the opposing opinion for four cases:
case
1
1
1
1
1
1
1
distance
-22
22
-22
-22
22
12
-22
case
1
2
2
2
2
2
2
distance
-22
-42.1
-42.1
14.3
-32.1
-44
-13.9
case
2
2
3
3
3
3
3
distance
-42.1
-42.1
-2.2
2.2
-2.2
2.2
2.2
case
3
3
3
4
4
4
4
distance
-2.2
2.2
2.2
-57.6
16.6
-39.8
-57.6
case
4
4
4
4
4
distance
20.4
-39.4
-57.6
-57.6
-19.6
Run a One-Way ANOVA model in R for these data according to:
library(lme4)
scd.out <- lmer(dist2 ~ 1+(1|case), scd)
summary(scd.out)
and interpret the REML results.

Appendix B
Common Probability Distributions
This appendix serves as a reference for the parametric forms used in the text. Considerably
more detail can be found in the standard references: Johnson et al. (2005) for univariate
forms on the counting measure; Johnson, Kotz, and Balakrishnan (1997) for multivari-
ate forms on the counting measure; Johnson, Kotz, and Balakrishnan (1994, 1995) for
univariate forms on the Lebesgue measure; Johnson, Kotz, and Balakrishnan (2000) for
multivariate forms on the Lesbegue measure; Fang, Kotz, and Ng (1990) concentrating on
symmetric forms; Kotz and Nadarajah (2000) for extreme value distributions; and more
generally, Evans, Hastings, and Peacock (2000), Balakrishnan and Nevzorov (2003), and
Krishnamoorthy (2006).
▷Bernoulli
▶PMF: BR(x|p) = px(1 −p)1−x, x = 0, 1,
0 < p < 1.
▶E[X] = p.
▶Var[X] = p(1 −p).
▷Beta
▶PDF: BE(x|α, β) =
Γ(α+β)
Γ(α)Γ(β)xα−1(1 −x)β−1,
0 < x < 1, 0 < α, β.
▶E[X] =
α
α+β .
▶Var[X] =
αβ
(α+β)2(α+β+1).
▷Binomial
▶PMF: BN(x|n, p) =
n
x

px(1 −p)n−x, x = 0, 1, . . . , n,
0 < p < 1.
▶E[X] = np.
▶Var[X] = np(1 −p).
▷Cauchy
▶PDF: C(x|θ, σ) =
1
πσ
1
1+( x−θ
σ )
2 ,
−∞< x, θ < ∞, 0 < σ.
▶E[X] = Does not exist.
▶Var[X] = Does not exist.
▶Note: sometimes θ and σ are labeled location and scale, respectively.
▷Dirichlet
▶PDF: D(x|α1, . . . , αk) =
Γ(α1+...+αk)
Γ(α1)···Γ(αk) xα1−1
1
· · · xαk−1
k
0 ≤xi ≤1, k
i=1 xi = 1, 0 <
αi, ∀i ∈[1, 2, . . . , k].
▶E[Xi] = αi
α0 , where α0 = k
j=1 αj.
579

580
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
▶Var[Xi] = αi(α0−αi)
α2
0(α0+1) .
▶Cov[Xh, Xi] = −
αhαi
α2
0(α0+1).
▷Double Exponential
▶PDF: DE(x|μ, . . . , τ) =
1
2τ exp[−|x −μ|/τ]
−∞< μ, x < ∞, 0 < τ.
▶E[X] = μ.
▶Var[X] = 2τ 2.
▷F
▶PDF: F(x|ν1, ν2) =
Γ
 ν1+ν2
2

Γ(
ν1
2 )Γ(
ν2
2 )

ν1
ν2
	ν1/2
x(ν1−2)/2

1+ ν1
ν2 x
 ν1+ν2
2
,
0 ≤x < ∞, ν1, ν2 ∈I+.
▶E[X] =
ν2
ν2−2, ν2 > 2.
▶Var[X] = 2

ν2
ν2−2
	2 ν1+ν2−2
ν1(ν2−4),
ν2 > 4.
▷Gamma
▶PDF rate version: G(x|α, β) =
βα
Γ(α)xα−1 exp[−xβ],
0 ≤x < ∞,
0 ≤α, β.
▶PDF scale version: G(x|α, β) = β−α
Γ(α)xα−1 exp[−x/β],
0 ≤x < ∞,
0 < α, β.
▶E[X] = α
β , rate version.
▶Var[X] =
α
β2 , rate version.
▶E[X] = αβ, scale version.
▶Var[X] = αβ2, scale version.
▶Note: the χ2 distribution is G
 ν
2 , 1
2

(ν is the degrees of freedom parameter), and
the exponential distribution comes from setting the shape parameter to one: EX(β) is
G(1, β) (rate version).
▷Geometric
▶PMF: GEO(x|p) = p(1 −p)x−1,
x = 1, 2, . . . ,
0 ≤p ≤1.
▶E[X] = 1
p.
▶Var[X] = 1−p
p2 .
▷Hypergeometric
▶PMF: HG(x|n, m, k) = (m
x)(n−m
k−x)
(n
k)
,
m −n + k ≤x ≤m,
n, m, k ≥0.
▶E[X] = km
n .
▶Var[X] = km(n−m)(n−k)
n2(n−1)
.
▷Inverse Gamma
▶PDF: IG(x|α, β) =
βα
Γ(α)(x)−(α+1) exp[−β/x],
0 < x, α, β.
▶E[X] =
β
α−1, α > 1.
▶Var[X] =
β2
(α−1)2(α−2), α > 2.
▷Lognormal
▶PDF: LN(x|μ, σ) = (2πσ2)−1
2 x−1 exp[−(log(x) −μ)2/2σ2],
−∞< μ, x < ∞, 0 < σ2
▶E[X] = exp[μ + σ2/2]

Common Probability Distributions
581
▶Var[X] = exp[2(μ + σ2)] −exp[2μ + σ2].
▷Multinomial
▶PMF: MN(x|n, p1, . . . , pk) =
n!
x1!···xk!px1
1 · · · pxk
k ,
xi = 0, 1, . . . , n,
0 < pi < 1,
k
i=1 pi = 1.
▶E[Xi] = npi.
▶Var[Xi] = npi(1 −pi).
▶Cov[Xi, Xj] = −npipj.
▷Negative Binomial
▶PMF: NB(x|r, p) =
x−1
r−1

pr(1 −p)x−r,
x = 0, 1, . . . , 0 < p < 1, r ∈I+.
▶E[X] = r(1−p)
p
.
▶Var[X] = r(1−p)
p2
.
▷Normal
▶PDF: N(x|μ, σ2) = (2πσ2)−1
2 exp

−
1
2σ2 (x −μ)2
,
−∞< μ, x < ∞, 0 < σ.
▶E[X] = μ.
▶Var[X] = σ2.
Multivariate case: Nk(x|μ, Σ2) = (2π)−k/2|Σ|−1/2 exp

−1
2(x −μ)′Σ−1(x −μ)

▷Pareto
▶PDF: PA(x|α, β) = αβαx−(α+1),
β < x, 0 < α, β.
▶E[X] =
βα
α−1, exists provided α > 1.
▶Var[X] =
β2α
(α−1)2(α−2), exists provided α > 2.
▷Poisson
▶PMF: P(x|λ) = λxe−λ
x!
,
x = 0, 1, . . . ,
0 ≤λ < ∞.
▶E[X] = λ.
▶Var[X] = λ.
▷Student’s-t
▶PDF: T (x|ν) =
Γ( ν+1
2 )
Γ( ν
2 )
1
(πν)
1
2 (1+x2/ν)(ν+1)/2 ,
−∞< μ, x < ∞, ν ∈I+.
▶E[X] = 0, 1 < ν.
▶Var[X] =
ν
ν−2, 2 < ν.
▷Student’s-t, Multivariate
▶PDF: MVT (x|M, ν) = |M|−1
2 (πν)−k/2 Γ( ν+k
2 )
Γ( ν
2 )

1 + (x−μ)′M(x−μ)
ν
	−ν+k
2 , where x is a
k-length vector, M is a k × k positive deﬁnite matrix, and ν is a positive scalar.
▶E[X] = μ.
▶Var[X] =
ν
ν−2M−1.
▷Uniform
k-Category Discrete Case PMF:

582
Bayesian Methods: A Social and Behavioral Sciences Approach, Third Edition
▶U(x) = p(X = x) =
⎧
⎨
⎩
1
k,
for x = 1, 2, . . . , k
0,
otherwise
▶E[X] = k+1
2 .
▶Var[X] = (k+1)(k−1)
12
.
Continuous Case PDF:
▶U(x) = f(x) =
⎧
⎨
⎩
1
b−a,
for a = 0 ≤x ≤b = 1
0,
otherwise
▶E[X] = b−a
2 .
▶Var[X] = (b−a)2
12
.
▷Weibull
▶PDF: w(x|γ, β) = γ
β xγ−1 exp

−

x
β
	γ	
if x ≥0 and 0 otherwise, where:
γ, β > 0.
▶E[Xij] = βΓ

1 + 1
γ

.
▶Var[Xij] = β2

Γ

1 + 2
γ

−γ

1 + 1
γ
2
▷Wishart
▶PDF: W(X|α, β) = |X|(α−(k+1))/2
Γk(α)|β|α/2
exp[−tr(β−1X)/2]
where: Γk(α) = 2αk/2πk(k−1)/4 k
i=1 Γ
 α+1−i
2

, 2α > k −1,
β symmetric nonsingular, and X symmetric positive deﬁnite.
▶E[Xij] = αβij
▶Var[Xij] = α(β2
ij + βiiβjj)
▶Cov[Xij, Xkl] = α(βikβjl + βilβjk)

References
Aarts, E. H. L. and Kors, T. J. (1989).
Simulated Annealing and Boltzmann Machines: A Stochastic
Approach to Combinatorial Optimization and Neural Computing. New York: John Wiley & Sons.
Abraham, C. and Cadre, B. (2004).
Asymptotic Global Robustness in Bayesian Decision Theory. The
Annals of Statistics 32, 1341-1366.
Abramowitz, M. and Stegun, I. A. (eds.). (1977).
Handbook of Mathematical Functions: With Formulas,
Graphs, and Mathematical Tables. Mineola, NY: Dover Publications.
Acton, F. S. (1996). Real Computing Made Real: Preventing Errors in Scientiﬁc and Engineering Calcu-
lations. Princeton: Princeton University Press.
Adams, J. L. (1991).
A Computer Experiment to Evaluate Regression Strategies.
Proceedings of the
Computational Statistics Section of the American Statistical Association 14, 55-62.
Adman, V. E. and Raftery, A. E. (1986).
Bayes Factors for Non-Homogeneous Poisson Processes with
Vague Prior Information. Journal of the Royal Statistical Society, Series B 48, 322-329.
Agresti, A. (2002). Categorical Data Analysis. Second Edition. New York: John Wiley & Sons.
Ahmed, S. E. and Reid, N. (2001).
Empirical Bayes and Likelihood Inference. New York: Springer-Verlag.
Ahrens, J. H. and Dieter, U. (1972).
Computer Methods for Sampling from the Exponential and Normal
Distributions. Communications of the ACM 15, 873-882.
Ahrens, J. H. and Dieter, U. (1973).
Extensions of Forsythe’s Method for Random Sampling from the
Normal Distribution. Mathematics for Computation 27, 927-937.
Ahrens, J. H. and Dieter, U. (1974).
Computer Methods for Sampling from Gamma, Beta, Poisson, and
Binomial Distributions. Computing 12, 223-246.
Ahrens, J. H. and Dieter, U. (1980).
Sampling from Binomial and Poisson Distributions: A Method with
Bounded Computation Times. Computing 25, 193-208.
Ahrens, J. H. and Dieter, U. (1982).
Generation of Poisson Deviates from Modiﬁed Normal Distributions.
ACM Transactions on Mathematical Software 8, 163-179.
Aitkin, M. (1991). Posterior Bayes Factor. With Discussion. Journal of the Royal Statistical Society, Series
B 53, 111-142.
Akaike, H. (1973). Information Theory and an Extension of the Maximum Likelihood Principle. In Pro-
ceedings of the 2nd International Symposium on Information Theory, N. Petrov and F. Caski (eds.).
Budapest: Akad´emiai Kiad´o, pp.176-723.
Akaike, H. (1974). A New Look at Statistical Model Identiﬁcation. IEEE Transactions Automatic Control
AU-19, 716-722.
Akaike, H. (1976). Canonical Correlation Analysis of Time Series and the Use of an Information Criterion.
In System Identiﬁcation: Advances and Case Studies, R. K. Mehra and D. G. Lainiotis (eds.).
San
Diego: Academic Press, pp.52-107.
Akaike, H. (1980). The Interpretation of Improper Prior Distributions as Limits of Data-Dependent Proper
Prior Distributions. Journal of the Royal Statistical Society, Series B 42, 46-52.
Albert, J. H. (1988). Computational Methods Using a Bayesian Hierarchical Generalized Linear Model.
Journal of the American Statistical Association 83, 1037-1044.
583

584
References
Albert, J. H. (1990). Bayesian Computation with R (Use R!). New York: Springer-Verlag.
Albert, J. H. and Chib, S. (1993).
Bayesian Analysis of Binary and Polychotomous Response Data.
Journal of the American Statistical Association 88, 669-679.
Albert, J. H. and Chib, S. (1995).
Bayesian Residual Analysis for Binary Response Regression Models
Biometrika 82, 747-759.
Albert, J. H. and Pepple, P. A. (1989).
A Bayesian Approach to Some Overdispersion Models. Canadian
Journal of Statistics 17, 333-344.
Allison, P. D. (1982). Discrete-Time Methods for the Analysis of Event Histories. Sociological Methodology
13, 61-98.
Altman, M., Gill, J. and McDonald, M. P. (2003).
Numerical Issues in Statistical Computing for the
Social Scientist. New York: John Wiley & Sons.
Altman, M. and McDonald, M. P. (2001).
Choosing Reliable Statistical Software. PS: Political Science
and Politics 34, 681-687.
Altman, M. and McDonald, M. P. (2003).
Replication with Attention to Numerical Accuracy. Political
Analysis. 11, 302-307.
Amemiya, T. (1985). Advanced Econometrics. Cambridge, MA: Harvard University Press.
Amit, Y. (1991). On Rates of Convergence of Stochastic Relaxation for Gaussian and non-Gaussian Distri-
butions. Journal of Multivariate Analysis 38, 82-99.
Amit, Y. (1996). Convergence Properties of the Gibbs Sampler for Perturbations of Gaussians. Annals of
Statistics 24, 122-140.
Amit, Y. and Grenander, U. (1991).
Comparing sweep strategies for stochastic relaxation. Journal of
Multivariate Analysis 37, 197-222.
Anderson, E. B. (1970). Suﬃciency and Exponential Families for Discrete Sample Spaces. Journal of the
American Statistical Association 65, 1248-1255.
Anderson, J. E. and Louis, T. A. (1996).
Generating Pseudo-Random Variables From Mixture Models by
Exemplary Sampling. Journal of Statistical Computation and Simulation 54, 45-53.
Andrews, D. F. (1974). A Robust Method for Multiple Linear Regression. Technometrics 16, 523-531.
Andrews, D. F., Bickel, P. J., Hampel, F. R., Huber, P. J. Rogers, W. H. and Tukey, J. W. (1972).
Robust Estimates of Location. Princeton: Princeton University Press.
Andrews, D. F. and Mallows, C. L. (1974).
Scale Mixtures of Normality. Journal of the Royal Statistical
Society, Series B 36, 99-102.
Andrieu, C., Djuri´c, P. M. and Doucet, A. (2001).
Model Selection by MCMC Computation.
Signal
Processing 81, 19-37.
Anscombe, F. J. (1963). Bayesian Inference Concerning Many Parameters with Reference to Supersaturated
Designs. Bulletin of the International Statistical Association 40, 733-741.
Antoniak, C. E. (1974).
Mixtures of Dirichlet Processes with Applications to Bayesian Nonparametric
Problems. Annals of Statistics 2, 1152-1174.
Arbuckle, J. L. (2009). Amos 18 User’s Guide. Chicago: SPSS Inc.
Arnold, B. C. and Press, S. J. (1983).
Bayesian Inference for Pareto Populations. Journal of Econometrics
21, 287-306.
Arnold, B. C. and Press, S. J. (1989).
Bayesian Estimation and Prediction for Pareto Data. Journal of
the American Statistical Association 84, 1079-1084.
Asmussen, S. P., Glynn, P. and Thorisson, H. (1992).
Stationarity Detection in the Initial Transient
Problem. ACM Transactions on Modeling and Computer Simulation 2, 130-57.
Athreya, K. B. and Ney, P. (1978).
A New Approach to the Limit Theory of Recurrent Markov Chains.
Transactions of the American Mathematical Society 245, 493-501.

References
585
Athreya, K. B., Doss, H. and Sethuraman, J. (1996).
On the Convergence of the Markov Chain Simulation
Method. Annals of Statistics 24, 69-100.
Atkinson, A. C. (1977).
An Easily Programmed Algorithm for Generating Gamma Random Variates.
Journal of the Royal Statistical Society, Series A 140, 232-234.
Atkinson, A. C. (1978). Posterior Probabilities for Choosing a Regression Model. Biometrika 65, 39-48.
Atkinson, A. C. (1980). Tests of Pseudo-Random Numbers. Applied Statistics 29, 164-171.
Atkinson, A. C. and Pearce, M. C. (1976).
The Computer Generation of Beta, Gamma, and Normal
Random Variables. Journal of the Royal Statistical Society, Series A 139, 431-461.
Ayres, D. (1994). Information, Entropy, and Progress. New York: American Institute of Physics Press.
Bakan, D. (1960). The Test of Signiﬁcance in Psychological Research. Psychological Bulletin 66, 423-437.
Balakrishnan, N. and Nevzorov, V. B. (2003).
A Primer on Statistical Distributions. New York: John
Wiley & Sons.
Baldus, D. C., Pulaski, C. and Woodworth, G. (1983).
Comparative Review of Death Sentences: An
Empirical Study of the Georgia Experience. Journal of Criminal Law and Criminology 74, 661-753.
Baldus, D. C., Pulaski, C. and Woodworth, G. (1990).
Equal Justice and the Death Penalty: A Legal and
Empirical Analysis. Boston: Northeastern University Press.
Barankin, E. W. and Maitra, A. P. (1963).
Generalization of the Fisher-Darmois-Koopman-Pittman
Theorem on Suﬃcient Statistics. Sankhy¯a, Series A 25, 217-244.
Barnard, G. A. (1991). Discussion of Aitkin. Journal of the Royal Statistical Society, Series B 53, 128-130.
Barndorﬀ-Nielsen, O. E. (1978). Information and Exponential Families in Statistical Theory. New York:
John Wiley & Sons.
Barndorﬀ-Nielsen, O. E. and Cox, D. R. (1989).
Asymptotic Techniques for Use in Statistics. London:
Chapman & Hall.
Barnett, V. (1973). Comparative Statistical Inference. New York: John Wiley & Sons.
Barnett, V. and Lewis, T. (1978).
Outliers in Statistical Data. New York: Wiley & Sons.
Bartels, L. M. (1997).
Speciﬁcation Uncertainty and Model Averaging.
American Journal of Political
Science 41, 641-674.
Bartholomew, D. J. (1965). A Comparison of Some Bayesian and Frequentist Inferences. Biometrika 52,
19-35.
Bartlett, M. S. (1937). Maximum Likelihood Approaches to Variance Component Estimation and to Related
Problems. Proceedings of the Royal Society A: Mathematical, Physical, and Engineering Sciences 160,
268-278.
Basu, S. (1994).
Variations of Posterior Expectations for Symmetric Unimodal Priors in a Distribution
Band. Sankhy¯a, Series A 31, 320-334.
Basu, S. and DasGupta, A. (1995).
Robust Bayesian Analysis with Distribution Bands. Statistics and
Decisions 13, 333-349.
Basu, S., Jammalamadaka, S. R. and Liu, W. (1996).
Local Posterior Robustness with Parametric Priors:
Maximum and Average Sensitivity. In Maximum Entropy and Bayesian Methods, G. Heidlbreder (ed.).
Dordrecht: Kluwer, pp.97-106.
Baum, L. E. and Eagon, J. A. (1967).
An Inequality with Applications to Statistical Estimation for
Probabilistic Functions of Markov Processes and to a Model for Ecology.
Bulletin of the American
Mathematical Society 73, 360-363.
Baum, L. E. and Petrie, T. (1966).
Statistical Inference for Probabilistic Functions of Finite Markov
Chains. Annals of Mathematical Statistics 37, 1554-1563.
Baum, L. E., Petrie, T., Soules, G. and Weiss, N. (1970).
A Maximization Technique Occurring in the
Statistical Analysis of Probabilistic Functions of Markov Chains. Annals of Mathematical Statistics
41, 164-171.

586
References
Bauwens, L., Lubrano, M. and Richard, J-F. (1999).
Bayesian Inference in Dynamic Econometric Models.
Oxford, UK: Oxford University Press.
Bayer, D. and Diaconis, P. (1992).
Trailing the Dovetail Shuﬄe to its Lair. Annals of Applied Probability
2, 294-313.
Bayes, T. (1763). An Essay Towards Solving a Problem in the Doctrine of Chances. Philosophical Trans-
actions of the Royal Society of London 53, 370-418.
Beale, E. M. L. (1977). Discussion of the Paper by Professor Dempster, Professor Laird and Dr. Rubin.
Journal of the Royal Statistical Society, Series B 139, 22.
Beale, E. M. L. and Little, R. J. A. (1975).
Missing Values in Multivariate Analysis. Journal of the Royal
Statistical Society, Series B 37, 129-145.
Beckett, L. and Diaconis, P. (1994).
Spectral Analysis for Discrete Longitudinal Data.
Advances in
Mathematics 103, 107-128.
Bedrick, E. J., Christensen, R. and Johnson, W. (1997).
Bayesian Binomial Regression: Predicting Survival
at a Trauma Center. The American Statistician 51, 211-218.
Belsley, D. A., Kuh, E. and Welsch, R. E. (1980).
Regression Diagnostics. New York: John Wiley & Sons.
Berger, J. O. (1984). The Robust Bayesian Viewpoint. In Robustness of Bayesian Analysis, Joseph B.
Kadane (ed.). Amsterdam: North Holland, pp.63-144.
Berger, J. O. (1985).
Statistical Decision Theory and Bayesian Analysis.
Second Edition.
New York:
Springer-Verlag.
Berger, J. O. (1986a). Discussion: On the Consistency of Bayes Estimates. Annals of Statistics 14, 30-37.
Berger, J. O. (1986b). Bayesian Salesmanship. In Bayesian Inference and Decision Techniques with Ap-
plications: Essays in Honor of Bruno de Finetti, Arnold Zellner (ed.). Amsterdam: North Holland,
pp.473-488.
Berger, J. O. (1990). Robust Bayesian Analysis: Sensitivity to the Prior. Journal of Statistical Planning
and Inference 25, 303-28.
Berger, J. O. (1994). An Overview of Robust Bayesian Analysis. Test 3, 5-124.
Berger, J. O. (2001). Bayesian Analysis: A Look at Today and Thoughts of Tomorrow. In Statistics in the
21st Century, Adrian E. Raftery, Martin A. Tanner and Martin T. Wells (eds.).
New York: Chapman
& Hall, pp.275-290.
Berger, J. O. (2003). Could Fisher, Jeﬀreys and Neyman Have Agreed on Testing? Statistical Science 18,
1-32.
Berger, J. and Berliner, L. M. (1986).
Robust Bayes and Empirical Bayes Analysis with
ϵ-Contaminated Priors. The Annals of Statistics 14, 461-486.
Berger, J. O. and Bernardo, J. M. (1989).
Estimating the Product of Means: Bayesian Analysis with
Reference Priors. Journal of the American Statistical Association 84, 200-207.
Berger, J. O. and Bernardo, J. M. (1992).
On the Development of the Reference Prior Method.
In
Bayesian Statistics 4, James O. Berger, Jos´e M. Bernardo, A. P. Dawid and Adrian F. M. Smith (eds.).
Oxford: Oxford University Press, pp.35-49.
Berger, J. O., Brown, L. D. and Wolpert, R. L. (1994).
A Uniﬁed Conditional Frequentist and Bayesian
Test for Fixed and Sequential Simple Hypothesis Testing. Annals of Statistics 22, 1787-1807.
Berger, J. O. and Delampady, M. (1987).
Testing Precise Hypotheses. With Discussion. Statistical Science
2, 317-352.
Berger, J. O., Ghosh, J. K. and Mukhopadhyay, N. (2003).
Approximations to the Bayes Factor In Model
Selection Problems and Consistency Issues. Journal of Statistical Planning and Inference 112, 241-258.
Berger, J. O. and Mortera, J. (1999).
Default Bayes Factors for Nonnested Hypothesis Testing. Journal
of the American Statistical Association 94, 542-554.
Berger, J. O. and O’Hagan, A. (1988).
Ranges of Posterior Probabilities for Unimodal Priors with Speciﬁed

References
587
Quantiles. In Bayesian Statistics 3, J. M. Bernardo, M. H. DeGroot, D. V. Lindley and A. F. M. Smith
(eds.).
Oxford: Oxford University Press, pp.45-65.
Berger, J. O. and Pericchi, L. R. (1996a).
The Intrinsic Bayes Factor for Model Selection and Prediction.
Journal of the American Statistical Association 91, 109-122.
Berger, J. O. and Pericchi, L. R. (1996b).
The Intrinsic Bayes Factor for Linear Models. In Bayesian
Statistics 5, J. O. Berger, J. M. Bernardo, A. P. Dawid, D. V. Lindley and A. F. M. Smith (eds.).
Oxford: Oxford University Press, pp.23-42.
Berger, J. O., Pericchi, L. R. and Varshavsky, J. A. (1998).
Bayes Factors and Marginal Distributions in
Invariant Situations. Sankhy¯a, Series A (Special Issue on Bayesian Analysis) 60, 307-321.
Berger, J. O. and Sellke, T. (1987).
Test of a Point Null Hypothesis: the Irreconcilability of Signiﬁcance
Levels and Evidence. With Discussion. Journal of the American Statistical Association 82, 112-139.
Berger, J. O. and Wolpert, R. L. (1984).
The Likelihood Principle. Hayward, CA: Institute of Mathematical
Statistics Monograph Series.
Berger, J. O. and Wolpert, R. L. (1988).
The Likelihood Principle.
Second Edition.
Hayward, CA:
Institute of Mathematical Statistics Monograph Series.
Berger, J. O. and Yang, R. (1994).
Noninformative Priors and Bayesian Testing for the AR(1) Model.
Econometric Theory 10, 461-482.
Berger, J. O., Bernardo, J. M. and Mendoza, M. (1989).
On Priors That Maximize Expected Information.
In Recent Developments in Statistics and Their Applications, J. Klein and J. Lee (eds.).
Seoul, ROK:
Freedom Academy, pp.1-20.
Berger, J. O., Boukai, B. and Wang, Y. (1997)
Uniﬁed Frequentist and Bayesian Testing of a Precise
Hypothesis. Statistical Science 12, 133-160.
Berk, R. H. (1972). Consistency and Asymptotic Normality of MLE’s for Exponential Models. Annals of
Mathematical Statistics 43, 193-204.
Bernardo, J. M. (1979). Reference Prior Distributions for Bayesian Inference. Journal of the Royal Statis-
tical Society, Series B 41, 113-147.
Bernardo, J. M. (1984). Monitoring the 1982 Spanish Socialist Victory: A Bayesian Analysis. Journal of
the American Statistical Association 79, 510-515.
Bernardo, J. M. (1992).
Simulated Annealing in Bayesian Decision Theory.
Computational Statistics.
Proceedings of 10th Symposium on Computational Statistics, Volume 1. Heidelberg: Physica-Verlag.
547-552.
Bernardo, J. M. and Smith, A. F. M. (1994).
Bayesian Theory. New York: John Wiley & Sons.
Berndt, E., Hall, B., Hall, R. and Hausman, J. (1974).
Estimation and Inference in Nonlinear Structural
Models. Annals of Economic and Social Measurement 3/4, 653-665.
Berry, D. and Hartigan, J. A. (1993).
A Bayesian Analysis for Changepoint Problems. Journal of the
American Statistical Association 88, 309-319.
Berry, W. D., Rinquist, E. J., Fording, R. C. and Hanson, R. L. (1998). Measuring Citizen and Government
Ideology in the American States, 1960-93. American Journal of Political Science 42, 327-348.
Bertsimas, D. and Tsitsiklis, J. N. (1993).
Simulated Annealing. Statistical Science 8, 10-15.
Besag, J. (1974). Spatial Interaction and the Statistical Analysis of Lattice Systems. Journal of the Royal
Statistical Society, Series B 36, 192-236.
Besag, J. and Green, P. J. (1993).
Spatial Statistics and Bayesian Computation. Journal of the Royal
Statistical Society, Series B 55, 1-52.
Besag, J., Green, P. J., Higdon, D. M. and Mengersen, K. L. (1995). Bayesian Computation and Stochastic
Systems (with discussion). Statistical Science 10, 3-66.
Best, D. J. (1979). Some Easily Programmed Pseudo-Random Normal Generators. Australian Computing
Journal 11, 60-62.

588
References
Best, D. J. (1983).
A Note on Gamma Variate Generators With Shape Parameter Less Than Unity.
Computing, Archives for Informatics and Numerical Computation 30, 185-188.
Best, N. G., Spiegelhalter, D. J., Thomas, A. and Brayne, C. E. G. (1996). Bayesian Analysis of Realistically
Complex Models. Journal of the Royal Statistical Society, Series A 159, 323-342.
Bickel, P. and Doksum, K. (1977).
Mathematical Statistics. San Francisco: Holden-Day.
Billera, L. J. and Diaconis, P. (2001).
A Geometric Interpretation of the Metropolis-Hastings Algorithm.
Statistical Science 16, 335-339.
Billingsley, P. (1995). Probability and Measure. Third Edition. New York: John Wiley & Sons.
Birkes, D. and Dodge, Y. (1993).
Alternative Methods of Regression. New York: Wiley & Sons.
Birnbaum, A. (1962).
On the Foundations of Statistical Inference.
Journal of the American Statistical
Association 57, 269-306.
Blalock, H. M. (1961). Causal Inferences in Nonexperimental Research. Chapel Hill, NC: University of
North Carolina Press.
Blight, B. J. N. (1970). Estimation from a Censored Sample for the Exponential Family. Biometrika 57,
389-395.
Blom, G., Holst, L. and Sandell, D. (1994).
Problems and Snapshots from the World of Probability. New
York: Springer-Verlag.
Blyth, S. (1995). The Dead of the Gulag: an Experiment in Statistical Investigation. Applied Statistic,
307-21.
Boehmke, F. J. (2009). Approaches to Modeling the Adoption and Modiﬁcation of Policies with Multiple
Components. State Politics and Policy Quarterly 9, 229-252.
Bohachevsky, I. O., Johnson, M. E. and Stein, M. L. (1986).
Generalized Simulated Annealing for Function
Optimization. Technometrics 28, 209-217.
Bohi, D. R., Toman, M. A. and Wells, M. A. (1996).
The Economics of Energy Security. Boston: Kluwer
Academic Publishers.
Boole, G. (1854).
An Investigation of the Laws of Thought on Which Are Founded the Mathematical
Theories of Logic and Probabilities. London: Macmillan.
Booth, J. G. and Hobert, J. P. (1999).
Maximizing Generalized Linear Mixed Model Likelihoods with an
Automated Monte Carlo EM Algorithm. Journal of the Royal Statistical Society, Series B 61, 265-285.
Boscardin, J. W. and Gelman, A. (1996).
Bayesian Computation for Parametric Models of Heteroscedas-
ticity in the Linear Model.
In Advances in Econometrics, Volume 11 (Part A), R. C. Hill (ed.).
Connecticut: JAI Press Inc., pp.87-109.
Bose, S. (1994a). Bayesian Robustness with Mixture Classes of Priors. Annals of Statistics 22, 652-667.
Bose, S. (1994b). Bayesian Robustness with More Than One Class of Contaminations. Journal of Statistical
and Inference 40, 177-187.
Bowman, K. O. and Beauchamp, J. J. (1975).
Pitfalls with Some Gamma Variate Simulation Routines.
Journal of Statistical Computation and Simulation 4, 141-154.
Box, G. E. P. (1980). Sampling and Bayes’ Inference in Scientiﬁc Modeling and Robustness. Journal of the
Royal Statistical Society, Series A 143, 383-430.
Box, G. E. P. (1995). Discussion. Journal of the Royal Statistical Society, Series B 57, 77.
Box, G. E. P. and M¨uller, M. E. (1958).
A Note on Generation of Normal Deviates. Annals of Mathematical
Statistics 28, 610-611.
Box, G. E. P. and Tiao, G. C. (1968).
A Bayesian Approach to Some Outlier Problems. Biometrika 55,
119-129.
Box, G. E. P. and Tiao, G. C. (1973).
Bayesian Inference in Statistical Analysis. New York: John Wiley
& Sons.

References
589
Boyles, R. A. (1983). On the Convergence of the EM Algorithm. Journal of the Royal Statistical Society,
Series B 45, 47-50.
Bozdogan, H. (1987). Model Selection and Akaike’s Information Criterion (AIC): The General Theory and
Its Analytical Extensions. Psychometrika 52, 345-370.
Bradlow, E. T. and Zaslavsky, A. M. (1997).
Case Inﬂuence Analysis in Bayesian Inference. Journal of
Computational and Graphical Statistics 6, 314-331.
Bradlow, E. T., Weiss, R. E. and Cho, M. (1998).
Bayesian Identiﬁcation of Outliers in Computerized
Adaptive Tests. Journal of the American Statistical Association 93, 910-919.
Braithwaite, R. B. (1953). Scientiﬁc Explanation. New York: Harper & Brothers.
Brandst¨atter, Eduard. (1999). Conﬁdence Intervals As An Alternative To Signiﬁcance Testing. Methods of
Psychological Research Online 4.2, 33-46.
Bratton, M. and Van De Walle, N. (1994).
Neopatrimonial Regimes and Political Transitions in Africa.
World Politics 46, 453-489.
Bratton, M. and Van De Walle, N. (1997).
Political Regimes and Regime Transitions in Africa, 1910-
1994. ICPSR Study Number I06996. Ann Arbor: Inter-University Consortium for Political and Social
Research.
Braun J. V., Braun R. K. and Muller H. G. (2000).
Multiple Changepoint Fitting Via Quasilikelihood,
With Application to DNA Sequence Segmentation. Biometrika 87, 301-314.
Br´emaud, P. (1999).
Markov Chains: Gibbs Fields, Monte Carlo Simulation, and Queues. New York:
Springer-Verlag.
Brent, R. P. (1974). A Gaussian Pseudo-Random Number Generator. Communications of the Association
for Computing Machinery 17, 704-706.
Breslow, N. (1990). Tests of Hypotheses in Overdispersed Poisson Regression and Other Quasi-Likelihood
Models. Journal of the American Statistical Association 85, 565-571.
Broemeling, L. D. (1985). Bayesian Analysis of Linear Models. New York: Marcel Dekker.
Brooks, R. J. (1972). Decision Theory Approach to Optimal Regression Designs. Biometrika 59, 563-571.
Brooks, S. P. (1998a). Markov Chain Monte Carlo and its Applications. The Statistician 47, 69-100.
Brooks, S. P. (1998b). Quantitative Convergence Assessment for Markov Chain Monte Carlo Via Cusums.
Statistics and Computing 8, 267-274.
Brooks, S. P. (2002). Discussion on the Paper by Spiegelhalter, Best, Carlin, and van der Linde.
Journal
of the Royal Statistical Society, Series B 64, 616-618.
Brooks, S. P., Fan, Y. and Rosenthal, J. S. (2006).
Perfect Forward Simulation via Simulated Tempering.
Communications in Statistics, Simulation and Computation 35, 683-713.
Brooks, S. P. and Roberts, G. O. (1999).
On Quantile Estimation and Markov Chain Monte Carlo
Convergence. Biometrika 86,710-717.
Brooks, S. P. and Gelman, A. (1998a).
Convergence Assessment Techniques for Markov Chain Monte
Carlo. Statistics and Computing 8, 319-335.
Brooks, S. P. and Gelman, A. (1998b).
General Methods for Monitoring Convergence of Iterative Simula-
tions. Journal of Computational and Graphical Statistics 7, 434-455.
Brooks, S. P. and Giudici, P. (2000).
MCMC Convergence Assessment via Two-Way ANOVA. Journal of
Computational and Graphical Statistics 9, 266-285.
Brooks, S. P. and Morgan, B. J. T. (1995).
Optimization Using Simulated Annealing. The Statistician
44, 241-257.
Brooks, S. P. and Roberts, G. O. (1998).
Convergence Assessment Techniques for Markov Chain Monte
Carlo. Statistics and Computing 8, 319-335.
Brooks, S. P., Dellaportas, P. and Roberts, G. O. (1997).
An Approach to Diagnosing Total Variation
Convergence of MCMC Algorithms. ‘Journal of Computational and Graphical Statistics 6, 251-265.

590
References
Brooks, S. and Roberts, G. (1997).
On Quantile Estimation and MCMC Convergence. Biometrika 86,
710-717.
Brown, L. D. (1964).
Suﬃciency Statistics in the Case of Independent Random Variables.
Annals of
Mathematical Statistics 35, 1456-1474.
Brown, L. D. (1986).
Foundations of Exponential Families.
Hayward, CA: Institute of Mathematical
Statistics Monograph Series 6.
Brown, L. D., Cohen, A. and Strawderman, W. E. (1980).
Complete Classes for Sequential Tests of
Hypotheses. Annals of Statistics 8, 377-398.
Brown, L. D., Cohen, A. and Strawderman, W. E. (1989).
Correction of: Complete Classes for Sequential
Tests of Hypotheses. (1980, Annals of Statistics 8, 377-398). Annals of Statistics 17, 1414-1416.
Brown, P. J., Fearn, T. and Vannucci, M. (1999).
The Choice of Variables in Multivariate Regression: A
Non-Conjugate Bayesian Decision Theory Approach. Biometrika 86, 635-648.
Browne, E. C., Frendreis, J. P. and Gleiber, D. W. (1986).
The Process of Cabinet Dissolution: An
Exponential Model of Duration and Stability in Western Democracies. American Journal of Political
Science 30, 628-650.
Bryk, A. S. and Raudenbush, S. W. (1989).
Toward a More Appropriate Conceptualization of Research on
School Level Eﬀects: A Three Level Hierarchical Linear Model. In Multilevel Analysis of Educational
Data, R. Darrell Bock (ed.). San Diego: Academic Press, pp.159-204.
Bryk, A. S. and Raudenbush, S. W. (2001).
Hierarchical Linear Models. Second Edition. Beverly Hills:
Sage.
Buck, S. F. (1960). A Method of Estimation of Missing Values in Multivariate Data Suitable for Use with
an Electronic Computer. Journal of the Royal Statistical Society, Series B 22, 302-306.
Buck, C. E., Cavanaugh, W. G. and Litton, C. D. (1996). Bayesian Approach to Interpreting Archaeological
Data. New York: John Wiley & Sons.
Burford, R. L. (1973). A Better Additive Congruential Random Number Generator. Decision Sciences 4,
190-193.
Burford, R. L. (1975). A Better Additive Congruential Random Number Generator? A Reply. Decision
Sciences 6, 199-201.
Burkhardt, H. and Schoenfeld, A. H. (2003).
Improving Educational Research: Toward a More Useful,
More Inﬂuential, and Better-Funded Enterprise. Educational Researcher,32, 3-14.
Burnham, K. P. and Anderson, D. R. (2002).
Model Selection and Multimodal Inference: A Practical
Information-Theoretical Approach. Second Edition. New York: Springer-Verlag.
Burstein, L., Kim, K-S. and Delandshere, G. (1989).
Multilevel Investigations of Systematically Varying
Slopes: Issues, Alternatives, and Consequences. In Multilevel Analysis of Educational Data, R. Darrell
Bock (ed.). San Diego: Academic Press, pp.233-279.
Butcher, J. C. (1961). A Partition Test for Pseudo-Random Numbers. Mathematics of Computation 15,
198-199.
Campbell, A. and Converse, P. (1999).
American National Election Study, 1960. ICPSR07216-v3. Ann
Arbor, MI: Inter-university Consortium for Political and Social Research [distributor].
Carver, Ronald P. (1978). The Case Against Statistical Signiﬁcance Testing. Harvard Education Review
48, 378-99.
Carver, Ronald P. (1993). The Case Against Statistical Signiﬁcance Testing, Revisited. Journal of Experi-
mental Education 61, 287-92.
Canes-Wrone, B., Brady, D. W. and Cogan, J. F. (2002).
Out of Step, Out of Oﬃce: Electoral Account-
ability and House Members’ Voting. American Political Science Review 96, 127-140.
Canova, F. (1994). Statistical Inference in Calibrated Models. Journal of Applied Econometrics 9, S123-
S144.

References
591
Carlin, B. P. and Chib, S. (1995).
Bayesian Model Choice Via Markov Chain Monte Carlo Methods.
Journal of the Royal Statistical Society, Series B 57, 473-484.
Carlin, B. P. and Gelfand, A. E. (1990).
Approaches for Empirical Bayes Conﬁdence Intervals. Journal of
the American Statistical Association 409, 105-114.
Carlin, B. P. and Louis, T. A. (1996).
Identifying Prior Distributions That Produce Speciﬁc Decisions,
With Application to Monitoring Clinical Trials. In Bayesian Analysis in Statistics and Econometrics:
Essays in Honor of Arnold Zellner, D. Berry, K. Chaloner and J. Geweke (eds.).
New York: John
Wiley & Sons, pp.493-503.
Carlin, B. P. and Louis, T. A. (2001).
Bayes and Empirical Bayes Methods for Data Analysis. Second
Edition. New York: Chapman & Hall.
Carlin, B. P. and Louis, T. A. (2009).
Bayes and Empirical Bayes Methods for Data Analysis. Third
Edition. New York: Chapman & Hall.
Carlin, B. P. and Polson, N. G. (1991).
Inference for Nonconjugate Bayesian Models Using the Gibbs
Sampler. Canadian Journal of Statistics 19, 399-405.
Carlin, B. P., Chaloner, K., Church, T., Louis, T. A. and Matts, J. P. (1993).
Bayesian Approaches for
Monitoring Clinical Trials with an Application to Toxoplasmic Encephalitis Prophylaxis. The Statisti-
cian 42, 355-367.
Carlin, B. P., Chaloner, K., Louis, T. A. and Rhame, F. S. (1995). Elicitation, Monitoring, and Analysis
for an AIDS Clinical Trial. In Case Studies in Bayesian Statistics, Volume II, Constantine Gatsonis,
James S. Hodges, Robert E. Kass and Nozer D. Singpurwalla (eds.).
New York: Springer-Verlag,
pp.48-78.
Carlin, B. P., Gelfand, A. E. and Smith, A. F. M. (1992).
Hierarchical Bayesian Analysis of Changepoint
Problems. Applied Statistics 41, 389-405.
Carlin, J. B. (1992). Meta-analysis for 2 × 2 Tables: A Bayesian Approach. Statistics in Medicine 11,
141-158.
Carroll, R. J. and Ruppert, D. (1982).
Robust Estimation in Heteroscedastic Linear Models. Annals of
Statistics 10, 429-441.
Carroll, R. J. and Ruppert, D. (1988).
Transforming and Weighting in Regression. New York: Chapman
& Hall.
Carter, W. J., Jr. and Myres, R. H. (1973).
Maximum Likelihood Estimation from Linear Combinations
of Discrete Probability Functions. Journal of the American Statistical Association 68, 203-206.
Casella, G. (1985). An Introduction to Empirical Bayes Data Analysis. The American Statistician 39,
83-87.
Casella, G. (2005).
James-Stein Estimator.
In Encyclopedia of Biostatistics 4.
Peter Armitage and
Theodore Colton (eds.). New York: Wiley & Sons.
Casella, G. and Berger, R. L. (1987a).
Reconciling Bayesian and Frequents Evidence in the One-Sided
Testing Problem. Journal of the American Statistical Association 82, 106-111.
Casella, G. and Berger, R. L. (1987b).
Rejoinder. Journal of the American Statistical Association 82,
133-135.
Casella, G. and Berger, R. L. (2002).
Statistical Inference.
Second Edition.
Belmont, CA: Duxbury
Advanced Series.
Casella, G. and George, E. I. (1992).
Explaining the Gibbs Sampler. The American Statistician 46,
167-174.
Casella, G., Lavine, M. and Robert, C. (2001).
Explaining the Perfect Sampler. The American Statistician
55, 299-305.
Casella, G., Mengersen, K. L., Robert, C. P. and Titterington, D. M. (2002)
Perfect Slice Samplers for
Mixtures of Distributions. Journal of the Royal Statistical Society, Series B, 64, 777-790.
Casella, G. and Robert, C. P. (1996).
Rao-Blackwellization of Sampling Schemes. Biometrika 93, 81-94.

592
References
Celeux, G. and Diebolt, J. (1985).
The SEM Algorithm: A Probabilistic Teacher Algorithm Derived from
the EM Algorithm for the Mixture Problem. Computational Statistics 2, 73-82.
Celeux, G., Chauveau, D. and Diebolt, J. (1996). Stochastic Versions of the EM Algorithm: an Experimental
Study in the Mixture Case. Journal of Statistical Computation and Simulation 55, 287-314.
Celeux, G., Forbes, F., Robert, C. P. and Titterington, D. M. (2006). Deviance Information Criteria for
Missing Data Models. Bayesian Analysis 1, 1-24.
Celeux, G., Hurn, M. and Robert, C. P. (2000).
Computational and Inferential Diﬃculties with Mixture
Posterior Distributions. Journal of the American Statistical Association 95, 957-70.
ˇCern´y, V. (1985). A Thermodynamic Approach to the Traveling Salesman Problem: An Eﬃcient Simula-
tion. Journal of Optimization Theory and Applications 45, 41-51.
Chakrabarti, A. and Ghosh, J. K. (2006).
A Generalization of BIC for the General Exponential Family.
Journal of Statistical Planning and Inference 136, 2847-2872.
Chaloner, K. and Brant, R. (1988).
A Bayesian Approach to Outlier Detection and Residual Analysis.
Biometrika 75, 651-659.
Chaloner, K. and Duncan, G. T. (1983).
Assessment of a Beta Prior Distribution: PM Elicitation. The
Statistician 27, 174-180.
Chaloner, K. and Duncan, G. T. (1987).
Some Properties of the Dirichlet-Multinomial Distribution and
Its Use in Prior Elicitation. Communications in Statistics, Part A, Theory and Methods 16, 511-523.
Chan, K. S. (1993).
Asymptotic Behavior of the Gibbs Sampler.
Journal of the American Statistical
Association 88, 320-326.
Chan, K. S. and Geyer, C. J. (1994).
Discussion of “Markov Chains for Exploring Posterior Distributions.”
Annals of Statistics 22, 1747-1758.
Chan, K. S. and Ledolter, J. (1995).
Monte Carlo EM Estimation for Time Series Models Involving Counts.
Journal of the American Statistical Association 90, 242-252.
Chang, T. and Villegas, C. (1986).
On a Theorem of Stein Relating Bayesian and Classical Inferences in
Group Models. Canadian Journal of Statistics 14, 289-296.
Chao, M. T. (1970). The Asymptotic Behavior of Bayes’ Estimators. Annals of Mathematical Statistics
41, 601-608.
Chatﬁeld, C. (2003). The Analysis of Time Series. Sixth Edition. New York: Chapman & Hall.
Chay, S. C., Fardo, R. D. and Mazumdar, M. (1975).
On Using the Box-M¨uller Transformation with
Multiplicative Congruential Pseudo-Random Number Generators. Applied Statistics 24, 132-135.
Chen, J. and Gupta A. K. (1997).
Testing and Locating Changepoints With Application To Stock Prices.
Journal of the American Statistical Association 92, 739-747.
Chen, J. and Ibrahim, J. G. (2006).
The Relationship Between the Power Prior and Hierarchical Models.
Bayesian Analysis 1, 551-574.
Chen, M-H. and Shao, Q-M. (1999).
Monte Carlo Estimation of Bayesian Credible and HPD Intervals.
Journal of Computational and Graphical Statistics 8, 69-92.
Chen, M-H. and Schmeiser, B. (1993).
Performance of the Gibbs, Hit-and-Run, and Metropolis Samplers.
Journal of Computational and Graphical Statistics 2, 251-72.
Chen, M-H., Dey, D. and Ibrahim, J. G. (2000).
Bayesian Criterion Based Model Assessment for Categor-
ical Data. Biometrika 91, 45-63.
Chen, M-H., Shao, Q-M. and Ibrahim, J. G. (2000).
Monte Carlo Methods in Bayesian Computation.
New York: Springer-Verlag.
Chen, T. T. and Fienberg, S. E. (1974).
Two-dimensional Contingency Tables with Both Completely and
Partially Cross-Classiﬁed Data. Biometrics 30, 629-642.
Cheng, R. C. H. (1977). The Generation of Gamma Variables with Non-Integral Shape Parameter. Applied
Statistics 26, 71-75.

References
593
Cheng, R. C. H. (1985). Generation of Multivariate Normal Samples with Given Mean and Covariance
Matrix. Journal of Statistical Computation and Simulation 21, 39-49.
Cheng, R. C. H. and Feast, G. M. (1979).
Some Simple Gamma Variate Generators. Applied Statistics
28, 290-295.
Cheng, R. C. H. and Feast, G. M. (1980).
Gamma Variate Generators with Increased Shape Parameter
Range. Communications of the ACM 23, 389-393.
Chiang, T-S. and Chow, Y. (1988).
On Eigenvalues and Optimal Annealing Rate. Mathematical Operations
Research 13, 508-511.
Chib, S. (1992). Bayes Inference in the Tobit Censored Regression Model. Journal of Econometrics 51,
79-99.
Chib, S. (1995). Marginal Likelihood from the Gibbs Output. Journal of the American Statistical Associ-
ation 90, 1313-1321.
Chib S. (1998). Estimation and Comparison of Multiple Change-point Models. Journal of Econometrics
86, 221-241.
Chib, S. and Greenberg, E. (1995).
Understanding the Metropolis-Hastings Algorithm. The American
Statistician. 49, 327-335.
Chib, S. and Greenberg, E. (1998).
Analysis of Multivariate Probit Models. Biometrika 85, 347-361.
Chib, S. and Jeliazkov, I. (2001).
Marginal Likelihood from the Metropolis-Hastings Output. Journal of
the American Statistical Association 96, 270-281.
Chib, S. and Tiwari, R. C. (1991).
Robust Bayes Analysis in Normal Linear Regression With An Improper
Mixture Prior. Communications in Statistics, Part A, Theory and Methods 20, 807-829.
Christiansen, C. L. and Morris, C. N. (1997).
Hierarchical Poisson Regression Modeling. Journal of the
American Statistical Association 92, 618-632.
Chu, C-S. J. and White, H. (1992).
A Direct Test for Changing Trend. Journal of Business and Economic
Statistics 10, 298-299.
Chubb, J. E. and Moe, T. (1988).
Politics, Markets and the Organization of American Schools. American
Political Science Review 82, 1065-1089.
Chubb, J. E. and Moe, T. (1990).
Politics, Markets and America’s Schools. Washington, DC: Brookings
Institution.
Chung, K. L. (1974). A Course in Probability Theory. San Diego: Academic Press.
Cioﬃ-Revilla, C. and Lai, D. (1995).
War and Politics in Ancient China, 2700 B.C. to 722 B.C.: Measure-
ment and Comparative Analysis. Journal of Conﬂict Resolution 39, 467-494.
Cioﬃ-Revilla, C. and Lai, D. (2001).
The Second International System: China and East Asia, 5000 B.C.
to 200 B.C. Presented at the Hong Kong Meeting of the International Studies Association, July 26-28,
2001.
Clements, Michael.
(2004).
Evaluating the Bank of England Density Forecasts of Inﬂation.
Economic
Journal 114, 844-866.
Cleveland, W. S. (1979). Robust Locally Weighted Regression and Smoothing Scatterplots. Journal of the
American Statistical Association 74, 829-36.
Cleveland, W. S. (1981). LOWESS: A Program for Smoothing Scatterplots by Robust Locally Weighted
Regression. The American Statistician 35, 54.
Cleveland, W. S. (1993). Visualizing Data. Murray Hill, NJ: Hobart Press.
Clogg, C. C., Petkova, E. and Haritou, A. (1995).
Statistical Methods for Comparing Regression Coeﬃ-
cients Between Models. American Journal of Sociology 100, 1261-1293.
Clyde, M. (1999). Bayesian Model Averaging and Model Search Strategies. In Bayesian Statistics 6, J. O.
Berger, J. M. Bernardo, A. P. Dawid, D. V. Lindley and A. F. M. Smith (eds.).
Oxford: Oxford
University Press, pp.157-185.

594
References
Clyde, M., M¨uller, P. and Parmigiani, G. (1995).
Optimal Designs for Heart Deﬁbrillators. Case Studies
in Bayesian Statistics II. Lecture Notes in Statistics 105, 278-292. New York: Springer-Verlag.
Cocchi, D. and Mouchart, M. (1996).
Quasi-Linear Bayes Estimation in Stratiﬁed Finite Populations.
Journal of the Royal Statistical Society, Series B 58, 293-300.
Cohen, J. (1962). The Statistical Power of Abnormal-Social Psychological Research: A Review. Journal of
Abnormal and Social Psychology 65, 145-153.
Cohen, J. (1977).
Statistical Power Analysis for the Behavioral Sciences (revised edition).
New York:
Academic Press.
Cohen, J. (1988). A Power Primer. Psychological Bulletin 112, 155-159.
Cohen, J. (1992). Statistical Power Analysis. Current Directions In Psychological Science 1, 98-101.
Cohen, J. (1994). The Earth is Round (p < .05). American Psychologist 12, 997-1003.
Cohen, J., Nagin, D., Wallstrom, G. and Wasserman, L. (1998).
Hierarchical Bayesian Analysis of Arrest
Rates. Journal of the American Statistical Association 93, 1260-1270.
Columbo, B. (1952). Preliminary Analysis of Recent Demographic Trends in Italy. Population Index 18,
265-279.
Conell, C. and Cohn, S. (1995).
Learning from Other People’s Actions: Environmental Variation and
Diﬀusion in French Coal Mining Strikes, 1890-1935. American Journal of Sociology 101, 366-403.
Congdon, P. (2001). Bayesian Statistical Modeling. New York: Wiley & Sons.
Congdon, P. (2003). Applied Bayesian Modelling. New York: Wiley & Sons.
Congdon, P. (2005). Bayesian Models for Categorical Data. New York: Wiley & Sons.
Congdon, P. (2010). Applied Bayesian Hierarchical Methods. New York: Wiley & Sons.
Conigliani, C., Castro, J. I. and O’Hagan, A. (2000).
Bayesian Assessment of Goodness of Fit Against
Nonparametric Alternatives. Canadian Journal of Statistics 28, 327-342.
Conquest, R. (1978). Kolyma: The Arctic Death Camps. New York: Viking Adult.
Consonni, G. and Veronese, P. (1992).
Conjugate Priors for Exponential Families Having Quadratic
Variance Functions. Journal of the American Statistical Association 87, 1123-1127.
Conte, S. D. and de Boor, C. (1980).
Elementary Numerical Analysis: An Algorithmic Approach. Third
Edition. New York: McGraw-Hill.
Cook, D. R. and Weisberg, S. (1982).
Residuals and Inﬂuence in Regression. New York: Chapman &
Hall.
Corcoran, J. N. and Tweedie, R. L. (1998).
Perfect Sampling From Independent Metropolis-Hastings
Chains. Journal of Statistical Planning and Inference 104, 297-314.
Cosmides, L. and Tooby, J. (1996).
Are Humans Good Intuitive Statisticians After All? Rethinking Some
Conclusions from the Literature on Judgment Under Uncertainty. Cognition 58, 1-73.
Coveyou, R. R. (1960). Serial Correlation in the Generation of Pseudo-Random Numbers. Journal of the
Association for Computing Machinery 7, 72-74.
Coveyou, R. R. (1970). Random Numbers Fall Mainly in the Planes (Review). ACM Computing Reviews,
225.
Coveyou, R. R. and MacPherson, R. D. (1967).
Fourier Analysis of Uniform Random Number Generators.
Journal of the ACM 14, 100-119.
Cowles, M. K. (2002).
MCMC Sampler Convergence Rates for Hierarchical Normal Linear Models: A
Simulation Approach. Statistics and Computing 12, 377-389.
Cowles, M. K. and Carlin, B. P. (1996).
Markov Chain Monte Carlo Convergence Diagnostics: A Com-
parative Review. Journal of the American Statistical Association 91, 883-904
Cowles, M. K., Roberts, G. O. and Rosenthal, J. S. (1999).
Possible Biases Induced by MCMC Convergence
Diagnostics. Journal of Statistical Computation and Simulation 64, 87-104.

References
595
Cowles, M. K. and Rosenthal, J. S. (1998).
A Simulation Approach to Convergence Rates for Markov
Chain Monte Carlo Algorithms. Statistics and Computing 8, 115-124.
Cox, D. R. (1961). Tests of Separate Families of Hypotheses. Proceedings of the Fourth Berkeley Symposium
on Mathematical Statistics and Probability. Berkeley: University of California Press, 105-123.
Cox, D. R. (1983). Some Remarks on Oversdispersion. Biometrika 70, 269-274.
Crain, B. R. and Morgan, R. L. (1975).
Asymptotic Normality of the Posterior Distribution for Exponential
Models. Annals of Statistics 3, 223-227.
Craiu, R. V. and Meng, X-L. (2011).
Perfection Within Reach: Exact MCMC Sampling. In Handbook of
Markov Chain Monte Carlo, Steve Brooks, Andrew Gelman, Galin L. Jones and Xiao-Li Meng (eds.).
Boca Raton: Chapman & Hall/CRC, pp.199-223.
Cramer, J. S. (1994). Econometric Applications of Maximum Likelihood Methods. Cambridge: Cambridge
University Press.
Creutz, M. (1979). Conﬁnement and the Critical Dimensionality of Space-Time. Physical Review Letters
43, 553-56.
Creutz, M., Jacobs, L. and Rebbi, C. (1983).
Monte Carlo Computations in Lattice Gauge Theories.
Physical Review 95, 201.
Cuevas, A. and Sanz, P. (1988).
On Diﬀerentiability Properties of Bayes Operators. In Bayesian Statistics
3, J. M. Bernardo, M. H. DeGroot, D. V. Lindley and A. F. M. Smith (eds.).
Oxford: Oxford
University Press, pp.569-577.
Cui, L., Tanner, M. A., Sinha, D. and Hall, W. J. (1992).
Comment: Monitoring Convergence of the
Gibbs Sampler: Further Experience with the Gibbs Stopper. Statistical Science 7, 483-486.
Cyert, R. M. and DeGroot, M. H. (1987).
Bayesian Analysis and Uncertainty in Economic Theory.
Totowa, NJ: Rowman & Littleﬁeld.
Dalal, S. R. and Hall, G. J., Jr. (1980).
On Approximating Parametric Bayes Models by Nonparametric
Bayes Models. Annals of Statistics 8, 664-772.
Dalal, S. R. and Hall, W. J. (1983).
Approximating Priors by Mixtures of Natural Conjugate Priors.
Journal of the Royal Statistical Society, Series B 45, 278-286.
Dale, A. I. (1991). A History of Inverse Probability: From Thomas Bayes to Karl Pearson. New York:
Springer-Verlag.
Damien, P., Wakeﬁeld, J. C. and Walker, S. (1999).
Gibbs Sampling for Bayesian Non-Conjugate and
Hierarchical Models by Using Auxiliary Variables. Journal of the Royal Statistical Society, Series B
61, 331-344.
Dar, R., Serlin, R. C. and Omer, H. (1994).
Misuse of Statistical Tests in Three Decades of Psychotherapy
Research. Journal of Consulting and Clinical Psychology 62, 75.
Darmois, G. (1935). Sur les Lois de Probabilit´es `a Estimation Exhaustive. Acadamie de Sciences Paris
200, 1265-1266.
Datta, G. S. and Ghosh, M. (1991).
Bayesian Prediction in Linear Models: Applications to Small Area
Estimation. Annals of Statistics 19, 1748-1770.
Davidian, M. and Carroll, R. J. (1987).
Variance Function Estimation. Journal of the American Statistical
Association 82, 1079-1091.
Davis, D. H. (1992). Energy Politics. New York: St. Martins Press.
Davis, P. J. and Rabinowitz, P. (1984).
Methods of Numerical Integration. Second Edition. San Diego:
Academic Press.
Davis, W. W. (1978).
Bayesian Analysis of the Linear Model Subject to Linear Inequality Constraints.
Journal of the American Statistical Association 73, 573-579.
Dawid, A. P. (1979).
Conditional Independence in Statistical Theory.
Journal of the Royal Statistical
Society, Series B 41, 1-31.

596
References
Dawid, A. P. (1982). The Well-Calibrated Bayesian. Journal of the American Statistical Association 77,
605-613.
Dawid, A. P. (1983). Invariant Prior Distributions. In Encyclopedia of Statistical Sciences, S. Kotz and N.
L. Johnson (eds.).
New York: John Wiley & Sons, pp.228-236.
Dawid, A. P., Stone, M. and Zidek, J. V. (1973).
Marginalization Paradoxes in Bayesian and Structural
Inference. Journal of the Royal Statistical Society, Series B 35, 189-233.
Dearden, R. and Clancy, D. (2002).
Particle Filters for Real-Time Fault Detection in Planetary Rovers.
Proceedings of the Thirteenth International Workshop on Principles of Diagnosis, 1-6.
De Bruijn, N. G. (1981). Asymptotic Methods in Analysis. Dover Edition. New York: Dover Publications.
Deely, J. J. and Lindley, D. V. (1981).
Bayes Empirical Bayes.
Journal of the American Statistical
Association 76, 833-841.
Deely, J. J. and Zimmer, W. J. (1976).
Asymptotic Optimality of the Empirical Bayes Procedure. Annals
of Statistics 4, 576-580.
de Finetti, B. (1930). Funzione Caratteristica di un Fenomeno Aleatorio. Mem. Accad. Naz. Lincei 4,
86-133.
de Finetti, B. (1937). La Pr´evision: ses Lois Logiques, ses Sources Subjectives. Annales de l’Institut Henri
Poincar´e 7, 1-68. Translated by H. E. Kyburg Jr. in H. E. Kyburg Jr. and H. E. Smokler (ed.). Studies
in Subjective Probability (1964). New York: Wiley, 93-158.
de Finetti, B. (1972). Probability, Induction, and Statistics. New York: John Wiley & Sons.
de Finetti, B. (1974). Theory of Probability, Volume 1. New York: John Wiley & Sons.
de Finetti, B. (1975). Theory of Probability, Volume 2. New York: John Wiley & Sons.
DeGroot, M. H. (1970). Optimal Statistical Decisions. New York: John Wiley & Sons.
DeGroot, M. H. (1973). Doing What Comes Naturally: Interpreting a Tail Area as a Posterior Probability
or as a Likelihood Ratio. Journal of the American Statistical Association 68, 966-969.
DeGroot, M. H. (1986). Probability and Statistics. Second Edition. Reading, MA: Addison-Wesley.
De Oliveira, V. (2013).
Hierarchical Poisson Models For Spatial Count Data.
Journal of Multivariate
Analysis 122, 393-408.
Del Pino, G. (1989). The Unifying Role of Iterative Generalized Least Squares in Statistical Algorithms.
Statistical Science 4, 394-408.
Delampady, M. and Dey, D. K. (1994).
Bayesian Robustness for Multiparameter Problems. Journal of
Statistical Planning and Inference 50, 375-382.
Dellaportas, P. and Smith, A. F. M. (1993).
Bayesian Inference for Generalized Linear and Proportional
Hazards Models via Gibbs Sampling. Applied Statistics 42, 443-459.
Demir, B., Bovolo, F. and Bruzzone, L. (2012).
A Novel System for Classiﬁcation of Image Time Series
with Limited Ground Reference Data. 2012 IEEE International on Geoscience and Remote Sensing
Symposium (IGARSS). Munich, Germany, 22-27 July 2012, 158-161.
de Morgan, A. (1837). Review of Laplace’s Th´eorie Analytique des Probabilit´es. Dublin Review 2, 338-354,
3, 237-248.
de Morgan, A. (1838). An Essay on Probabilities and their Application to Life Contingencies and Insurance
Oﬃces. London: Longman, Orme, Brown, Green, & Longmans.
de Morgan, A. (1847). Formal Logic; or, the Calculus of Inference, Necessary and Probable. Formal Logic.
London: Taylor & Walton.
Dempster, A. P., Laird, N. M. and Rubin, D. B. (1977).
Maximum Likelihood from Incomplete Data via
the EM Algorithm. Journal of the Royal Statistical Society, Series B 39, 1-38.
Denis, D. J. (2005) The Modern Hypothesis Testing Hybrid: R. A. Fisher’s Fading Inﬂuence. Journal de
la Soci´et´e Fran¸caise de Statistique 145, 5-26.

References
597
DeSouza, C. M. (1992).
An Approximate Bivariate Bayesian Method for Analyzing Small Frequencies.
Biometrics 48, 1113-1130.
Devore, J. (1999). Probability and Statistics for Engineering and the Sciences. Boston: PWS Publishing.
Devroye, L. (1986). Non-Uniform Random Variate Generation. New York: Springer-Verlag.
Dey, D. K. and Birmiwal, L. R. (1994).
Robust Bayesian Analysis Using Entropy and Divergence Measures.
Statistical Probability Letters 20, 287-294.
Dey, D. K. and Micheas, A. (2000).
Ranges of Posterior Expected Losses and ϵ-Robust Actions.
In
Robust Bayesian Analysis, David R´ıos Insua and Fabrizio Ruggeri (eds.).
New York: Springer-Verlag,
pp.71-88.
Dey, D. K., Ghosh, S. K. and Mallick, B. K. (2000).
Generalized Linear Models: A Bayesian Perspective.
New York: Marcel Dekker.
Dey, D. K., Lou, K. and Bose, S. (1998).
A Bayesian Approach to Loss Robustness.
Statistics and
Decisions 16, 65-87.
Diaconis, P. (1988).
Group Representations in Probability and Statistics.
Hayward, CA: Institute of
Mathematical Statistics.
Diaconis, P. and Freedman, D. A. (1980).
Finite Exchangeable Sequences.
Annals of Probability 8,
745-764.
Diaconis, P. and Freedman, D. A. (1986).
On the Consistency of Bayes Estimates. Annals of Statistics
14, 1-67.
Diaconis, P. and Ylvisaker, D. (1979).
Conjugate Priors for Exponential Families. Annals of Statistics 7,
269-281.
Diaconis, P. and Ylvisaker, D. (1985).
Quantifying Prior Opinion.
In Bayesian Statistics 2, J. M.
Bernardo, M. H. DeGroot, D. V. Lindley and A. F. M. Smith (eds.).
Amsterdam: North Holland
Press, pp.133-156.
Diaconis, P. and Saloﬀ-Coste, L. (1993).
Comparison Theorems for Reversible Markov Chains. Annals of
Applied Probability 3, 696-730.
Diaconis, P. and Saloﬀ-Coste, L. (1996).
Logarithmic Sobolev Inequalities for Finite Markov Chains.
Annals of Applied Probability 6, 695-750.
Diaconis, P. and Stroock, D. W. (1991).
Geometric Bounds for Eigenvalues of Markov Chains. Annals of
Applied Probability 1, 36-61.
Dias, J. and Wedel, M. (2004).
An Empirical Comparison of EM, SEM and MCMC Performance for
Problematic Gaussian Mixture Likelihoods. Statistics and Computing 14, 323-332.
DiCiccio, T. J. and Stern, S. E. (1994).
Frequentist and Bayesian Bartlett Correction of Test Statistics
Based on Adjusted Proﬁle Likelihood. Journal of the Royal Statistical Society, Series B 56, 397-408.
Dickey, J. M. (1974).
Bayesian Alternatives to the F-test and Least-Squares Estimator in the Normal
Linear Model. In Studies in Bayesian Econometrics and Statistics, S. Fienberg and A. Zellner (eds.).
Amsterdam: North Holland, pp.515-554.
Diebolt J. and Ip, E. H. S. (1996).
Stochastic EM: Method and application. In Markov Chain Monte
Carlo in Practice, W. R. Gilks, S. Richardson and D. J. Spiegelhalter (eds.).
New York: Chapman &
Hall, pp.259-273.
Diebolt, J. and Robert, C. P. (1994).
Estimation of Finite Mixture Distributions through Bayesian
Sampling. Journal of the Royal Statistical Society, Series B 56, 363-375.
Dieter, U. (1975). Statistical Interdependence of Pseudo-Random Numbers Generated by the Linear Con-
gruential Method. Applications of Number Theory to Numerical Analysis, S. K. Zaremba (ed.). San
Diego: Academic Press, 287-318.
Dieter, U. and Ahrens, J. H. (1971).
An Exact Determination of Serial Correlations of Pseudo-Random
Numbers. Numerische Mathematik 17, 101-123.

598
References
Dieter, U. and Ahrens, J. H. (1973).
A Combinatorial Method for Generation of Normally Distributed
Random Variables. Computing 11, 137-146.
Dobson, A. J. (1990). An Introduction to Generalized Linear Models. New York: Chapman & Hall.
Doeblin, W. (1940). ´El´ements d’une Th´eorie G´en´erale des Chaˆınes Simples Constantes de Markoﬀ. Annales
Scientiﬁques de l’Ecole Normale Sup´erieure 57, 61-111.
Doob, J. L. (1990). Stochastic Processes. New York: Wiley & Sons.
Doob, J. L. (1996). The Development of Rigor in Mathematical Probability (1900-1950). American Math-
ematical Monthly 103, 586-595.
Downham, D. Y. (1970). The Runs Up and Test. Applied Statistics 19, 190-192.
Downham, D. Y. and Roberts, F. D. K. (1967).
Multiplicative Congruential Pseudo-Random Number
Generators. Computer Journal 10, 74-77.
Draper, D. (1995). Assessment and Propagation of Model Uncertainty. Journal of the Royal Statistical
Society, Series B 57, 45-97.
Draper, D., Hodges, J. S., Mallows, C. L. and Pregibon, D. (1993).
Exchangeability and Data Analysis.
Journal of the Royal Statistical Society, Series A 156, 9-37.
Duane, S., Kennedy, A. D., Pendleton, B. J. and Roweth, D. (1987).
Hybrid Monte Carlo. Physics
Letters B 195, 216-222.
Dudewicz, E. J. (1975). Random Numbers: The Need, the History, the Generators. In A Modern Course
on Statistical Distributions in Scientiﬁc Work, Volume 2, G. P. Patil, S. Kotz and J. K. Ord (eds.).
Boston: D. Reidel, pp.25-36.
Dudewicz, E. J. (1976).
Speed and Quality of Random Numbers for Simulation.
Journal of Quality
Technology 8, 171-178.
Dudley, R. M. and Haughton, D. (1997).
Information Criteria for Multiple Data Sets and Restricted
Parameters. Statistica Sinica 7, 265-284.
Duﬂo, M. (1996). Random Iterative Models. In Series Applications of Mathematics, I. Karatzas and M.
Yor (eds.).
New York: Springer-Verlag, Volume 34.
DuMouchel, W. and Normand, S. (2000).
Computer-Modeling and Graphical Strategies for Meta-Analysis.
In Meta-Analysis in Medicine and Health Policy, D. K. Stangl and D. A. Berry (eds.).
New York:
Marcel Dekker, pp.127-78.
Earman, J. (1992). Bayes or Bust: A Critical Examination of Bayesian Conﬁrmation Theory. Cambridge,
MA: MIT Press.
Eaves, D. M. (1985).
On Maximizing Missing Information about a Hypothesis.
Journal of the Royal
Statistical Society, Series B 47, 263-266.
Ebrahimi, N., Habibullah, M. and Sooﬁ, E. S. (1992).
Testing Exponentiality Based on Kullback-Leibler
Information. Journal of the Royal Statistical Society, Series B 54, 739-748.
Edgeworth, F. Y. (1892a). Correlated Averages. Philosophical Magazine, 5th Series, 34, 190-204.
Edgeworth, F. Y. (1892b). The Law of Error and Correlated Averages. Philosophical Magazine, 5th Series,
34, 429-438 & 518-526.
Edgeworth, F. Y. (1893a). Exercises in the Calculation of Errors. Philosophical Magazine, 5th Series, 36,
98-111.
Edgeworth, F. Y. (1893b). Note on the Calculation of Correlation Between Organs. Philosophical Magazine,
5th Series, 36, 350-351.
Edgeworth, F. Y. (1921). Molecular Statistics. Journal of the Royal Statistical Society 84, 71-89.
Edwards, A. W. F. (1992). Likelihood. Baltimore: Johns Hopkins University Press.
Edwards, R. G. and Sokal, A. D. (1988).
Generalization of the Fortuin-Kasteleyn-Swendsen-Wang Repre-
sentation and Monte Carlo Algorithm. Physical Review Letters 38, 2009-2012.

References
599
Edwards, W., Lindman, H. and Savage, L. J. (1963).
Bayesian Statistical Inference for Psychological
Research. Psychological Research 70, 193-242.
Efron, B. (1978). The Geometry of Exponential Families. Annals of Statistics 6, 362-376.
Efron, B. (1979). Bootstrap Methods: Another Look at the Jackknife. The Annals of Statistics 7, 1-26.
Efron, B. (1986). Why Isn’t Everyone a Bayesian? The American Statistician 40, 1-11.
Efron, B. (1998). R. A. Fisher in the 21st Century. Statistical Science 13, 95-122.
Efron, B. and Hinkley, D. V. (1978).
Assessing the Accuracy of the Maximum Likelihood Estimator:
Observed Versus Expected Fisher Information. Biometrika 65, 457-482.
Efron, B. and Morris, C. N. (1971).
Limiting the Risk of Bayes and Empirical Bayes Estimators, Part I:
The Bayes Case. Journal of the American Statistical Association 66, 807-815.
Efron, B. and Morris, C. N. (1972a).
Limiting the Risk of Bayes and Empirical Bayes Estimators, Part II:
The Empirical Bayes Case. Journal of the American Statistical Association 67, 130-139.
Efron, B. and Morris, C. N. (1972b).
Empirical Bayes on Vector Observations: An Extension of Stein’s
Method. Biometrika 59, 335-347.
Efron, B. and Morris, C. N. (1973).
Stein’s Estimation Rule and Its Competitors: An Empirical Bayes
Approach. Journal of the American Statistical Association 68, 117-130.
Efron, B. and Morris, C. N. (1975).
Data Analysis Using Stein’s Estimator and Its Generalizations.
Journal of the American Statistical Association 70, 311-319.
Efron, B. and Morris, C. N. (1976).
Multivariate Empirical Bayes and Estimation of Covariance Matrices.
Annals of Statistics 4, 22-32.
Efron, B. and Morris, C. N. (1977). Stein’s Paradox In Statistics. Scientiﬁc American May, 119-127.
Efron, B. and Tibshirani, R. J. (1993).
An Introduction to the Bootstrap. New York: Chapman & Hall.
Eglese, R. W. (1990).
Simulated Annealing:
A Tool for Operational Research.
European Journal of
Operational Research 46, 271-281.
Eichenauer-Herrmann, J. (1996).
Equidistribution Properties of Inversive Congruential Pseudorandom
Numbers With Power of Two Modulus. Metrika 44, 199-205.
Eliason, S. R. (1983). Maximum Likelihood Estimation: Logic and Practice. Thousand Oaks, CA: Sage.
Emerson, J. D. and Hoaglin, D. C. (1983).
Resistant Lines for y Versus x. In Understanding Robust and
Exploratory Data Analysis, D. C. Hoaglin, Frederick Mosteller and John Tukey (eds.).
New York:
Wiley & Sons, pp.129-165.
Escobar, M. D. (1994). Estimating Normal Means with a Dirichlet Process Prior. Journal of the American
Statistical Association 89, 268-277.
Escobar, M. D. and West, M. (1995).
Bayesian Density Estimation and Inference Using Mixtures. Journal
of the American Statistical Association 90, 577-588.
Espelid, T. O. and Genz, A. (1992).
Numerical Integration: Recent Developments, Software, and Appli-
cations. Dordrecht, NL: Kluwer.
Estlin, T., Volpe, R., Nesnas, I. A. D., Mutz, D., Fisher, F., Englehardt, B. and Chien, S. (2001).
Decision-Making in a Robotic Architecture for Autonomy.
The 6th International Symposium on
Artiﬁcial Intelligence, Robotics, and Automation in Space, Montreal Canada, 18-21.
Evans, M. J., Fraser, D. A. S. and Monette, G. (1986).
On Principles and Arguments to Likelihood.
Canadian Journal of Statistics 14, 181-199.
Evans, M. J., Hastings, N. and Peacock, B. (2000).
Statistical Distributions. New York: John Wiley &
Sons.
Evans, M. J. and Swartz, T. (1995).
Methods for Approximating Integrals in Statistics with Special
Emphasis on Bayesian Integration Problems. Statistical Science 10, 254-272.
Evans, S. J. W. (1994). Discussion of the Paper by Spiegelhalter, Freedman, and Parmar. Journal of the
Royal Statistical Society, Series A 157, 395.

600
References
Faber, J. (1989).
Annual Data on Nine Economic and Military Characteristics of 78 Nations (SIRE
NATDAT), 1948-1983. Ann Arbor: Inter-University Consortium for Political and Social Research and
Amsterdam, and Amsterdam, the Netherlands: Europa Institute, Steinmetz Archive.
Fabius, J. (1964). Asymptotic Behavior of Bayes’ Estimates. Annals of Mathematical Statistics 35, 846-856.
Fahrmeir, L. and Kaufmann, H. (1985).
Consistency and Asymptotic Normality of the Maximum Likeli-
hood Estimator in Generalized Linear Models. The Annals of Statistics 13, 342-368.
Fahrmeir, L. and Tutz, G. (2001).
Multivariate Statistical Modelling Based on Generalized Linear Models.
Second Edition. New York: Springer.
Falk, M. (1999). A Simple Approach to the Generation of Uniformly Distributed Random Variables With
Prescribed Correlations. Communications in Statistics, Part B, Simulation and Computation 28, 785-
791.
Falk, R. and C. W. Greenbaum. (1995).
Signiﬁcance Tests Die Hard. Theory and Psychology 5, 396-400.
Falkenrath, R. (2001).
Analytical Models and Policy Prescription: Understanding Recent Innovation in
U.S. Counterterrorism. Studies in Conﬂict and Terrorism 24, 159-181.
Fang, K.-T., Hickernell, F. J. and Niederreiter, H. (2002).
Monte Carlo and Quasi-Monte Carlo Methods.
New York: Springer-Verlag.
Fang, K.-T., Kotz, S. and Ng, K. W. (1990).
Symmetric Multivariate and Related Distributions. New
York: Chapman & Hall.
Farnum, N. R. and Stanton, L. W. (1987).
Some Results Concerning the Estimation of Beta Distribution
Parameters in PERT. Journal of the Operational Research Society 38, 287-90.
Fearnhead, P. (2006). Exact and Eﬃcient Bayesian Inference for Multiple Changepoint Problems. Statistics
and Computing 16, 203-213.
Feller, W. (1990). An Introduction to Probability Theory and its Applications. Volume 1. New York: John
Wiley & Sons.
Feller, W. (1990). An Introduction to Probability Theory and its Applications. Volume 2. New York: John
Wiley & Sons.
Ferguson, T. S. (1967). Mathematical Statistics: A Decision-Theoretic Approach. San Diego: Academic
Press.
Ferguson, T. S. (1973). A Bayesian Analysis of Some Nonparametric Problems. Annals of Statistics 1,
209-230.
Ferguson, T. S. (1983).
Bayesian Density Estimation by Mixtures of Normal Distributions.
In Recent
Advances in Statistics, H. Rizvi and J. Rustagi (eds.).
San Diego: Academic Press, pp.287-302.
Ferguson, T. S. and Phadia, E. G. (1979).
Bayesian Nonparametric Estimation Based on Censored Data.
The Annals of Statistics 7, 163-186.
Ferrari, P. A., Frigessi, A. and Schonmann, R. J. (1993).
Convergence of Some Partially Parallel Gibbs
Samplers with Annealing. Annals of Applied Probability 3, 137-153.
Ferreira, M. A. R. and Gamerman, D. (2000).
Dynamic Generalized Linear Models. In Generalized Linear
Models: A Bayesian Perspective, Dipak K. Dey, Sujit K. Ghosh and Bani K. Mallick (eds.).
New
York: Marcel Dekker, pp.41-53.
Fienberg, S. E. (2006). When Did Bayesian Inference Become Bayesian? Bayesian Analysis 1, 1-40.
Fill, J. A. (1991). Eigenvalue Bounds on Convergence to Stationarity for Nonreversible Markov Chains with
an Application to the Exclusion Process. Annals of Applied Probability 1, 62-67.
Fill, J. A. (1998). An Interruptible Algorithm for Perfect Sampling Via Markov Chains. Annals of Applied
Probability 8, 131-162.
Fill, J. A., Machida, M., Murdoch, D. J., Rosenthal, J. S. (1999).
Extensions of Fill’s Perfect Rejection
Sampling Algorithm to General Chains. Random Structures and Algorithms 17, 290-316.
Firth, D. (1987). On the Eﬃciency of Quasi-Likelihood Estimation. Biometrika 74, 233-245.

References
601
Fisher, R. A. (1922). On the Mathematical Foundations of Theoretical Statistics. Philosophical Transac-
tions of the Royal Statistical Society of London A 222, 309-360.
Fisher, R. A. (1925a). Statistical Methods for Research Workers. Edinburgh: Oliver and Boyd.
Fisher, R. A. (1925b). Theory of Statistical Estimation. Proceedings of the Cambridge Philosophical Society
22. 700-725.
Fisher, R. A. (1930). Inverse Probability. Proceedings of the Cambridge Philosophical Society 26. 528-535.
Fisher, R. A. (1934). The Design of Experiments. First Edition. Edinburgh: Oliver and Boyd.
Fisher, R. A. (1935). The Fiducial Argument in Statistical Inference. Annals of Eugenics 6, 391-398.
Fisher, R. A. (1956). Statistical Methods and Scientiﬁc Inference. Second Edition. New York: Hafner.
Fishman, G. S. and Moore, L. R. (1982).
A Statistical Evaluation of Multiplicative Congruential Random
Number Generators With Modulus 231−1. Journal of the American Statistical Association 77, 129-136.
Fishman, G. (2003). Monte Carlo. New York: Springer-Verlag.
Flegal, J. M. (2012). Applicability of Subsampling Bootstrap Methods In Markov Chain Monte Carlo. In
Monte Carlo and Quasi-Monte Carlo Methods 2010, Leszek Plaskota and Henryk Wozniakowski (eds.).
Springer Berlin Heidelberg, pp.363-372.
Fleischer, M. A. (1995). Simulated Annealing: Past, Present, and Future. Proceedings of the 27th Winter
Simulation Conference 00, 155-161.
Flournoy, N. and Tsutakawa, R. K. (eds.). (1991).
Statistical Multiple Integration. Providence: American
Mathematical Society.
Fomby, T. B., Hill, R. C. and Johnson, S. R. (1980).
Advanced Econometric Methods.
New York:
Springer-Verlag.
Fort, G., Moulines, E. and Priouret, P. (2011).
Convergence of Adaptive and Interacting Markov Chain
Monte Carlo Algorithms. The Annals of Statistics 39, 3262-3289.
Fortini, S. and Ruggeri, F. (2000).
On the Use of the Concentration Function in Bayesian Robustness. In
Robust Bayesian Analysis, David R´ıos Insua and Fabrizio Ruggeri (eds.).
New York: Springer-Verlag,
pp.109-126.
Fox, L. (1971). How to Get Meaningless Answers in Scientiﬁc Computation and What to Do About It.
IMA Bulletin 7, 296-302.
Fraser, D. A. S. (1963). On Suﬃciency and the Exponential Family. Journal of the Royal Statistical Society,
Series B 25, 115-123.
Fredenhagen, K. and Marcu, M. (1987).
A Modiﬁed Heat-Bath Method Suitable for Monte Carlo Simula-
tions on Vector and Parallel Processors. Physical Letters, B 193, 486-488.
Freedman, D. A. (1963). On the Asymptotic Behavior of Bayes’ Estimates in the Discrete Case. Annals of
Mathematical Statistics 34, 1386-1403.
Freedman, D. A. (1965). On the Asymptotic Behavior of Bayes Estimates in the Discrete Case II. Annals
of Mathematical Statistics 36, 454-456.
Freedman, D. A. and Diaconis, P. (1982).
de Finetti’s Theorem for Symmetric Location Families. Annals
of Statistics 10, 184-189.
Freedman, L. S. and Spiegelhalter, D. J. (1983).
The Assessment of Subjective Opinion and Its Use in
Relation to Stopping Rules for Clinical Trials. Statistician 32, 153-160.
Freeman, D. (1983).
Margaret Mead and Samoa: The Making and Unmaking of an Anthropological Myth.
Cambridge, MA: Harvard University Press.
Frieze, A., Kannan, R. and Polson, N. G. (1994).
Sampling from Log-Concave Distributions. Annals of
Applied Probability 4, 812-837.
Frigessi, A., Hwang, C.-R., Di Stefano, P. and Sheu, S.-J. (1993).
Convergence Rates of the Gibbs
Sampler, the Metropolis Algorithm, and Other Single-Site Updating Dynamics. Journal of the Royal
Statistical Society, Series B 55, 205-220.

602
References
Fuller, A. T. (1976). The Period of Pseudo-Random Numbers Generated by Lehmer’s Congruential Method.
Computer Journal 19, 173-177.
Fulman, J. and Wilmer, E. L. (1999).
Comparing Eigenvalue Bounds for Markov Chains: When Does
Poincare Beat Cheeger? Annals of Applied Probability 9, 1-13.
Galant, D. (1969). Gauss Quadrature Rules for the Evaluation of 2π−1/2  ∞
0
exp(−x2)f(x)dx. Mathematics
of Computation 23, 674.
Galton, F. (1869). Heredity Genius: An Inquiry into its Laws and Consequences. Second Edition. London:
Macmillan.
Galton, F. (1875). Statistics by Intercomparison, with Remarks on the Law of Frequency of Error. Philo-
sophical Magazine, 4th Series (49), 33-46.
Galton, F. (1886). Regression Towards Mediocrity in Hereditary Stature. Journal of the Anthropological
Institute 15, 246-263.
Galton, F. (1892). Finger Prints. London: Macmillan.
Gamerman, D. and Lopes, H. F. (2006).
Markov Chain Monte Carlo.
Second Edition.
New York:
Chapman & Hall.
Garip, F. and Western, B. (2011).
Model Comparison and Simulation for Hierarchical Models: Analyzing
Rural-Urban Migration in Thailand. In Handbook of Markov Chain Monte Carlo, Steve Brooks, Andrew
Gelman, Galin L. Jones and Xiao-Li Meng (eds.), 563-574. Boca Raton: Chapman & Hall/CRC. pp.563-
574.
Garthwaite, P. H. and Dickey, J. M. (1988).
Quantifying Expert Opinion in Linear Regression Problems.
Journal of the Royal Statistical Society, Series B 50, 462-474.
Garthwaite, P. H. and Dickey, J. M. (1992).
Elicitation of Prior Distributions for Variable Selection
Problems in Regression. Annals of Statistics 20, 1697-1719.
Gates, C. E. (1978). On Generating Random Normal Deviates Using the Butler Algorithm. Proceedings of
the Statistical Computing Section. Alexandria, VA: American Statistical Association, pp.111-114.
Gauss, C. F. (1809). Theoria Motus Corporum Coelestium. Hamburg: Perthes et Besser.
Gauss, C. F. (1823).
Theoria Combinationis Observationum Erroribus Minimis Obnoxiae.
G¨ottingen:
K¨oniglichen Gesellschaft der Wissenschaften.
Gauss, C. F. (1855).
M´ethode des Moindres Carr´es.
M´emoires sur la Combination des Observations.
Translated by J. Bertrand. Paris: Mallet-Bachelier.
Gavasakar, U. (1988). A Comparison of Two Elicitation Methods for a Prior Distribution for a Binomial
Parameter. Management Science 34, 784-790.
Gawande, K. (1998). Comparing Theories of Endogenous Protection: Bayesian Comparison of Tobit Models
Using Gibbs Sampling Output. Review of Economics and Statistics 80, 128-140.
Gebhardt, F. (1964). On the Risk of Some Strategies for Outlying Observations. The Annals of Mathe-
matical Statistics 35, 1524-1536.
Geisser, S. and Eddy, W. F. (1979).
A Predictive Approach to Model Selection. Journal of the American
Statistical Association 74, 153-160.
Gelfand, A. E. and Dey, D. K. (1991).
On Bayesian Robustness of Contaminated Classes of Priors.
Statistical Decisions 9, 63-80.
Gelfand, A. E. and Mitter, S. K. (1993).
Metropolis-type Annealing Algorithms for Global Optimization
in ℜd. SIAM Journal of Control and Optimization 31, 111-131.
Gelfand, A. E. and Sahu, S. K. (1994).
On Markov Chain Monte Carlo Acceleration. Journal of Compu-
tational and Graphical Statistics 3, 261-276.
Gelfand, A. E. and Smith, A. F. M. (1990).
Sampling Based Approaches to Calculating Marginal Densities.
Journal of the American Statistical Association 85, 398-409.

References
603
Gelfand, A. E., Sahu, S. K. and Carlin, B. P. (1995).
Eﬃcient Parameterizations For Normal Linear Mixed
Models. Biometrika 82, 479-488.
Gelman, A. (1992). Iterative and Non-Iterative Simulation Algorithms. Computing Science and Statistics
24, 433-438.
Gelman, A. (1996). Inference and Monitoring Convergence. In Markov Chain Monte Carlo in Practice, W.
R. Gilks, S. Richardson and D. J. Spiegelhalter (eds.).
New York: Chapman & Hall, pp.131-144.
Gelman, A. (2005). Analysis of Variance: Why It Is More Important Than Ever. Annals of statistics 33,
1-53.
Gelman, A. (2006). Prior Distributions for Variance Parameters in Hierarchical Models. Bayesian Analysis
1, 515-533.
Gelman, A., Gilks, W. R. and Roberts, G. O. (1996).
Eﬃcient Metropolis Jumping Rules. In Bayesian
Statistics 5, J. O. Berger, J. M. Bernardo, A. P. Dawid, D. V. Lindley and A. F. M. Smith (eds.).
Oxford: Oxford University Press, pp.599-608.
Gelman, A. and Hill, J. (2007).
Data Analysis Using Regression and Multilevel/Hierarchical Models.
Cambridge: Cambridge University Press.
Gelman, A., Meng, X-L. and Stern, H. S. (1996).
Posterior Predictive Assessment of Model Fitness Via
Realized Discrepancies. Statistica Sinica 6, 733-807.
Gelman, A. and Rubin, D. B. (1992a).
Inference from Iterative Simulation Using Multiple Sequences.
Statistical Science 7, 457-472.
Gelman, A. and Rubin, D. B. (1992b).
A Single Sequence from the Gibbs Sampler Gives a False Sense of
Security. In Bayesian Statistics 5, J. O. Berger, J. M. Bernardo, A. P. Dawid, D. V. Lindley and A. F.
M. Smith (eds.).
Oxford: Oxford University Press, pp.223-253.
Gelman, A. and Rubin, D. B. (1995).
Avoiding Model Selection in Bayesian Social Research Sociological
Methodology
25, 165-173.
Gelman, A., Carlin, J. B., Stern, H. S. and Rubin, D. B. (2003).
Bayesian Data Analysis.
Second
Edition. New York: Chapman & Hall.
Gelman, A., Huang, Z., van Dyk, D. A. and Boscardin, J. W. (2008).
Using Redundant Parameterizations
to Fit Hierarchical Models. Journal of Computational and Graphical Statistics 17, 95-122.
Geman, S. and Geman, D. (1984).
Stochastic Relaxation, Gibbs Distributions and the Bayesian Restoration
of Images. IEEE Transactions on Pattern Analysis and Machine Intelligence 6, 721-741.
Geman, S. and Hwang, C-R. (1986).
Diﬀusions for Global Optimization. SIAM Journal of Control and
Optimization 24, 1031-1043.
Gentle, J. E. (1990). Computer Implementation of Random Number Generators. Journal of Computational
and Applied Mathematics 31, 119-125.
Gentle, J. E. (1998). Random Number Generation and Monte Carlo Methods. New York: Springer-Verlag.
George, E. I. and McCulloch, R. E. (1993).
Variable Selection via Gibbs Sampling.
Journal of the
American Statistical Association 85, 398-409.
George, E. I., Makov, U. E. and Smith, A. F. M. (1993).
Conjugate Likelihood Distributions. Scandinavian
Journal of Statistics 20, 147-156.
George, R. (1963). Normal Random Variables. Communications of the Association for Computing Ma-
chinery 6, 444.
Geweke, J. (1989). Bayesian Inference in Econometric Models Using Monte Carlo Integration. Econometrica
57, 1317-1339.
Geweke, J. (1992). Evaluating the Accuracy of Sampling-Based Approaches to the Calculation of Posterior
Moments. In Bayesian Statistics 4, J. M. Bernardo, A. F. M. Smith, A. P. Dawid and J. O. Berger
(eds.).
Oxford: Oxford University Press, pp.169-193.

604
References
Geweke, J. (1993). Bayesian Treatment of the Independent Student-t Linear Model. Journal of Applied
Econometrics 8, S19-S40.
Geweke, J. (2005). Contemporary Bayesian Econometrics and Statistics. New York: Wiley & Sons.
Geweke, J. and Petrella, L. (1998).
Prior Density-ratio Class Robustness in Econometrics. Journal of
Business and Economic Statistics 16, 469-478.
Geyer, C. J. (1991). Markov Chain Monte Carlo Maximum Likelihood. Computing Science and Statistics,
Proceedings of the 23rd Symposium on the Interface, 156-63.
Geyer, C. J. (1992). Practical Markov Chain Monte Carlo. Statistical Science 7, 473-511.
Geyer, C. J. (1995). Conditioning in Markov Chain Monte Carlo. Journal of Computational and Graphical
Statistics 4, 148-154.
Geyer, C. J. (2011). Importance Sampling, Simulated Temporing, and Umbrella Sampling. In Handbook of
Markov Chain Monte Carlo, Steve Brooks, Andrew Gelman, Galin L. Jones and Xiao-Li Meng (eds.).
Boca Raton: Chapman & Hall/CRC, pp.3-47.
Geyer, C. J. and Thompson, E. A. (1995).
Annealing Markov Chain Monte Carlo With Applications to
Ancestral Inference. Journal of the American Statistical Association 90, 909-920.
Ghosh, J. K. (1969). Only Linear Transformations Preserve Normality. Sankhy¯a, Series A 31, 309-312.
Ghosh, M. and Meeden, G. (1997).
Bayesian Methods for Finite Population Sampling.
New York:
Chapman & Hall.
Ghosh, M. and Mukerjee, R. (1992).
Hierarchical and Empirical Bayes Multivariate Estimation.
In
Current Issues in Statistical Inference: Essays in Honor of D. Basu, M. Ghosh and P. K. Pathak
(eds.).
Hayward, CA: Institute of Mathematical Statistics Monograph Series 17, pp.1-12.
Giakoumatos, S. G., Delaportas, P. and Politis, D. N. (2005).
Bayesian Analysis of the Unobserved ARCH
model. Statistics and Computing 15, 103-111.
Gidas, B. (1985). Nonstationary Markov Chains and Convergence of the Annealing Algorithm. Journal of
Statistical Physics 39, 73-131.
Gigerenzer, G. (1987). Probabilistic Thinking and the Fight Against Subjectivity. In Kr¨uger, Lorenz, Gerd
Gigerenzer and Mary Morgan, eds.
The Probabilistic Revolution. Volume 2. Cambridge, MA: MIT
Press.
Gigerenzer, G. (1998a).
We Need Statistical Thinking, Not Statistical Rituals.
Behavioral and Brain
Sciences 21, 199-200.
Gigerenzer, G. (1998b). The Superego, the Ego, and the Id in Statistical Reasoning. In G. Keren and C.
Lewis, eds. A Handbook for Data Analysis in the Behavioral Sciences: Methodological Issues. Hillsdale,
NJ: Lawrence Erlbaum Associates.
Gigerenzer, G. (2004). Mindless Statistics. The Journal of Socio-Economics 33, 587-606.
Gigerenzer, G. and D. J. Murray.
(1987).
Cognition as Intuitive Statistics.
Hillsdale, NJ: Lawrence
Erlbaum Associates.
Gilks, W. R. (1992).
Derivative-Free Adaptive Rejection Sampling for Gibbs Sampling.
In Bayesian
Statistics 4, J. O. Berger, J. M. Bernardo, A. P. Dawid and A. F. M. Smith (eds.).
Oxford: Oxford
University Press, pp.641-649.
Gilks, W. R. (1996). Full Conditional Distributions. In Markov Chain Monte Carlo in Practice, W. R.
Gilks, S. Richardson and D. J. Spiegelhalter (ed.).
New York: Chapman & Hall, pp.75-88.
Gilks, W. R., Best, N. G. and Tan, K. K. C. (1995).
Adaptive Rejection Metropolis Sampling Within
Gibbs Sampling. Applied Statistics 44, 455-472.
Gilks, W. R. and Roberts, G. O. (1996).
Strategies for Improving MCMC. In Markov Chain Monte Carlo
in Practice, W. R. Gilks, S. Richardson and D. J. Spiegelhalter (eds.).
New York: Chapman & Hall,
pp.89-114.

References
605
Gilks, W. R., Roberts, G. O. and Sahu, S. K. (1998).
Adaptive Markov Chain Monte Carlo Through
Regeneration. Journal of the American Statistical Association 93, 1045-1054.
Gilks, W. R., Thomas, A. and Spiegelhalter, D. J. (1994).
A Language and Program for Complex Bayesian
Modelling. The Statistician 43, 169-177.
Gilks, W. R. and Wild, P. (1992).
Adaptive Rejection Sampling for Gibbs Sampling. Applied Statistics
41, 337-348.
Gill, J. (1999). The Insigniﬁcance of Null Hypothesis Signiﬁcance Testing. Political Research Quarterly
52, 647-674.
Gill, J. (2000). Generalized Linear Models: A Uniﬁed Approach. Thousand Oaks, CA: Sage.
Gill, J. (2006). Essential Mathematics for Political and Social Research. Cambridge, England: Cambridge
University Press.
Gill, J. (2010). Critical Diﬀerences in Bayesian and Non-Bayesian Inference. In Current Methodological
Developments of Statistics in the Social Sciences.
Stanislav Kolenikov, Lori Thombs and Douglas
Steinley (eds.). 135-158. New York: John Wiley & Sons, pp.383-408.
Gill, J. and Casella, G. (2007).
Nonparametric Priors For Ordinal Bayesian Social Science Models:
Speciﬁcation and Estimation. Journal of the American Statistical Association 104, 453-464
Gill, J. and Casella, G. (2004).
Dynamic Tempered Transitions for Exploring Multimodal Posterior
Distributions. Political Analysis, 12, 425-433.
Gill, J. and Freeman, J. (2013).
Dynamic Elicited Priors for Updating Covert Networks. Network Science,
1, 68-94.
Gill, J. and King, G. (2004).
What to Do When Your Hessian Is Not Invertible: Alternatives to Model
Respeciﬁcation in Nonlinear Estimation. Sociological Methods and Research 33, 54-87.
Gill, J. and Meier, K. J. (2000).
Public Administration Research and Practice: A Methodological Mani-
festo. Journal of Public Administration Research and Theory, 10, 157-200.
Gill, J. and Walker, L. (2005).
Elicited Priors for Bayesian Model Speciﬁcations in Political Science
Research. Journal of Politics 67, 841-872.
Gill, J. and Witko C. (2013).
Bayesian Analytical Methods: A Methodological Prescription for Public
Administration. Journal of Public Administration Research and Theory 23, 457-494.
Gill, J. and Womack, A. (2013).
The Multilevel Model Framework. In The Sage Handbook of Multilevel
Modeling. Marc A. Scott, Jeﬀrey S. Simonoﬀand Brian D. Marx (eds.). Thousand Oaks, CA: Sage
Publications, pp.3-20.
Gill, P., Murray, W. and Wright, M. H. (1981).
Practical Optimization. San Diego: Academic Press.
Gliner, J. A., Leech, N. L. and Morgan, G. A. (2002).
Problems With Null Hypothesis Signiﬁcance Testing
(NHST): What Do the Textbooks Say? The Journal of Experimental Education 71, 83-92.
Goel, P. K. (1983).
Information Measures and Bayesian Hierarchical Models. Journal of the American
Statistical Association 78, 408-410.
Goel, P. K. and DeGroot, M. H. (1979).
Comparison of Experiments and Information Measures. Annals
of Statistics 7, 1066-1077.
Goel, P. K. and DeGroot, M. H. (1980).
Only Normal Distributions Have Linear Posterior Expectations
in Linear Regression. Journal of the American Statistical Association 75, 895-900.
Goel, P. K. and DeGroot, M. H. (1981).
Information About Hyperparameters in Hierarchical Models.
Journal of the American Statistical Association 76, 140-147.
Goldberger, A. S. (1964). Econometric Theory. New York: Wiley & Sons.
Golder, E. R. (1976). The Spectral Test for the Evaluation of Congruential Pseudo-Random Generators.
Applied Statistics 25, 173-180.
Golder, E. R. and Settle, J. G. (1976).
The Box-M¨uller Method for Generating Pseudo-Random Normal
Deviates. Applied Statistics 25, 12-20.

606
References
Goldstein, H. (1985). Multilevel Statistical Models. New York: Halstead Press.
Goldstein, H. (1986). Multilevel Mixed Linear Model Analysis Using Iterative Generalized Least Squares.
Biometrika 73, 43-56.
Goldstein, H. (1987).
Multilevel Models in Education and Social Research.
Oxford: Oxford University
Press.
Golstein, H. (2003). Multilevel Statistical Models. Third Edition. Oxford: Oxford University Press, Kendall
Library of Statistics 3.
Goldstein, H. and Rasbash, J. (1996).
Improved Approximations for Multilevel Models with Binary
Responses. Journal of the Royal Statistical Society, Series A 159, (1996), 505-513.
Goldstein, M. (1991). Belief Transforms and the Comparison of Hypotheses. The Annals of Statistics 19,
2067-2089.
Golomb, S. W. (1967). Shift Register Sequences. San Francisco: Holden-Day.
Golub, G. H. and Van Loan, C. F. (1996).
Matrix Computations.
Third Edition.
Baltimore: Johns
Hopkins University Press.
Golub, G. H. and Welsch, J. H. (1969).
Calculation of Gauss Quadrature Rules. Mathematics of Compu-
tation 23, 221-230.
G¨onen, M., Johnson, W. O., Yonggang, L. and Westfall, P. H. (2005).
The Bayesian Two-Sample t Test.
The American Statistician 59, 252-257.
Good, I. J. (1950). Probability and the Weighting of Evidence. London: Griﬃn.
Good, I. J. (1957).
On the Serial Test for Random Sequences.
Annals of Mathematical Statistics 28,
262-264.
Good, I. J. (1972). Statistics and Today’s Problems. American Statistician 26, 11-19.
Good, I. J. (1976). The Bayesian Inﬂuence, or How to Sweep Subjectivism Under the Carpet. In Foun-
dations of Probability Theory, Statistical Inference, and Statistical Theories of Science II, William L.
Harper and Cliﬀord A. Hooker (eds.). Dordrecht: D. Reidel, pp.125-174.
Good, I. J. (1980a).
Some History of the Hierarchical Bayes Methodology.
In Bayesian Statistics 2, J.
M. Bernardo, M. H. DeGroot, D. V. Lindley and A. F. M. Smith (ed.). Amsterdam: North Holland,
pp.489-515.
Good, I. J. (1980b). The Contributions of Jeﬀreys to Bayesian Statistics. In Bayesian Analysis in Econo-
metrics and Statistics: Essays in Honor of Harold Jeﬀreys. Arnold Zellner (ed.). Amsterdam: North
Holland.
Good, I. J. (1983a). The Bayes/Non-Bayes Compromise or Synthesis and Box’s Current Philosophy. Journal
of Statistical Computation and Simulation 18, 234-236.
Good, I. J. (1983b). Good Thinking: The Foundations of Probability and Its Applications. Minneapolis:
University of Minnesota Press.
Good, I. J. (1985). Weight of Evidence: A Brief Survey. In Bayesian Statistics 2, J. M. Bernardo, M. H.
DeGroot, D. V. Lindley and A. F. M. Smith (eds.).
Amsterdam: North Holland Press, pp.249-270.
Good, I. J. (1992). The Bayes/Non-Bayes Compromise: A Brief Review. Journal of the American Statistical
Association 87, 597-606.
Good, I. J. and Crook, J. F. (1974).
The Bayes/Non-Bayes Compromise and the Multinomial Distribution.
Journal of the American Statistical Association 69, 711-720.
Goodman, J. and Sokal, A. D. (1989).
Multigrid Monte Carlo Method. Conceptual Foundations. Physics
Review D 40, 2035-2071.
Goodman, S. N. (1993). P values, Hypothesis Tests, and Likelihood: Implications for Epidemiology of a
Neglected Historical Debate. American Journal of Epidemiology 137, 485-496.
Goodman, S. N. (1999). Toward Evidence-Based Medical Statistics. 1: The P Value Fallacy. Annals of
Internal Medicine 130, 995-1004.

References
607
Gorenstein, S. (1967).
Testing a Random Number Generator.
Communications of the Association for
Computing Machinery 10, 111-118.
Gossett, W. S. (1908a). The Probable Error of a Mean. Biometrika 6, 1-25.
Gossett, W. S. (1908b). Probable Error of a Correlation Coeﬃcient. Biometrika 6, 302-309.
Goutis, C. and Casella, G. (1999).
Explaining the Saddlepoint Approximation. The American Statistician
53, 216-224.
Granger, C. W. J. (1999). Empirical Modeling in Economics: Speciﬁcation and Evaluation. Cambridge:
Cambridge University Press.
Granger, G. W. and Hatanaka, M. (1964).
Spectral Analysis of Economic Time Series.
Princeton:
Princeton University Press.
Gray, H. L. (1988). On Uniﬁcation of Bias Reduction and Numerical Approximation. In Probability and
Statistics Essays in Honor of Franklin A. Graybill, J. N. Srivastava (ed.).
Amsterdam: North Holland,
pp.105-116.
Gray, V. and Lowery, D. (1996).
The Population Ecology of Interest Representation: Lobbying Commu-
nities in the American States. Ann Arbor, Michigan: University of Michigan Press.
Gray, V. and Lowery, D. (2001).
The institutionalization of state communities of organized interests.
Political Research Quarterly 54, 265-284.
Green, P. J. (1984). Iteratively Reweighted Least Squares for Maximum Likelihood Estimation, and Some
Robust and Resistant Alternatives. Journal of the Royal Statistical Society, Series B 46, 149-192.
Green, P. J. (1995). Reversible Jump Markov Chain Monte Carlo Computation and Bayesian Model De-
termination. Biometrika 82, 711-732.
Green, P. J. and Murdoch, D. J. (1998).
Exact Sampling for Bayesian Inference: Towards General Purpose
Algorithms. Bayesian Statistics 6, 301-321.
Greene, W. (2011). Econometric Analysis. Seventh Edition. Upper Saddle River, NJ: Prentice Hall.
Greenwald, A. G. (1975). Consequences of Prejudice against the Null Hypothesis. Psychological Bulletin
82, 1-20.
Greenwald, A. G., Gonzalez, R., Harris, R. and Guthrie, D. (1996).
Eﬀect Sizes and p-values: What
Should Be Reported and What Should Be Replicated? Psychophysiology 33, 175-183.
Greenwood, J. A. (1974). A Fast Generator for Gamma-Distributed Random Variables. In Compstat 1974:
Proceedings in Computational Statistics, G. Bruckman, F. Ferschl and L. Schmetterer (eds.).
Vienna:
Physica Verlag. pp.19-27.
Grenander, U. (1983). Tutorial in Pattern Theory. Unpublished Manuscript, Division of Applied Mathe-
matics. Providence, RI: Brown University.
Grimmett, G. R. and Stirzaker, D. R. (1992).
Probability and Random Processes.
Oxford: Oxford
University Press.
Guan, Y., Fleißner, R. and Joyce, P. (2006).
Markov Chain Monte Carlo in Small Worlds. Statistics and
Computing 16, 193-202.
Guihenneuc-Jouyaux, C. and Robert, C. P. (1998).
Valid Discretization via Renewal Theory. In Discretiza-
tion and MCMC Convergence Assessment, Lecture Notes in Statistics, 135. Christian P. Robert (ed.).
New York: Springer-Verlag, pp. 67-98.
Guo, S. W. and Thompson, E. A. (1991).
Monte Carlo Estimation of Variance Component Models for
Large Complex Pedigrees. IMA Journal of Mathematics Applied in Medicine and Biology 8, 171-189.
Guo, S. W. and Thompson, E. A. (1994).
Monte Carlo Estimation of Mixed Models for Large Complex
Pedigrees. Biometrics 50, 417-432.
Gupta, S. S. and Berger, J. O. (1982).
Statistical Decision Theory and Related Topics III. San Diego:
Academic Press.

608
References
Gustafson, P. (1996). Local Sensitivity of Inferences to Prior Marginals. Journal of the American Statistical
Association 91, 774-781.
Gustafson, P. (2000). Local Robustness in Bayesian Analysis. In Robust Bayesian Analysis, David R´ıos
Insua and Fabrizio Ruggeri (eds.).
New York: Springer-Verlag, pp.71-88.
Gustafson, P. and Wasserman, L. (1995).
Local Sensitivity Diagnostics for Bayesian Inference. The Annals
of Statistics 23, 2153-2167.
Guti´errez-Pe˜na, E. and Smith, A. F. M. (1995).
Conjugate Parameterizations for Natural Exponential
Families. Journal of the American Statistical Association 90, 1347-1356.
Guttman, I. (1973). Care and Handling of Univariate or Multivariate Outliers in Detecting Spuriosity-A
Bayesian Approach. Technometrics 15, 723-738.
Guttman, I., Dutter, R. and Freeman, P. R. (1978).
Care and Handling of Univariate Outliers in the
General Linear Model to Detect Spuriosity. Technometrics 20, 187-193.
Haag, J. (1924). Sur un Probleme Deneral de Probabilities et Ses Diverses Applications. Proceedings of the
International Congress of Mathematics. Toronto, 659-674.
Haber, S. (1970). Numerical Integration of Multiple Integrals. SIAM Review 12, 481-526.
Haberman, S. J. (1974a). The Analysis of Frequency Data. Chicago: University of Chicago Press.
Haberman, S. J. (1974b). Log-linear Models for Frequency Tables Derived by Indirect Observations: Max-
imum Likelihood Equations. Annals of Statistics 2, 911-924.
Haberman, S. J. (1977). Product Models for Frequency Tables Involving Indirect Observations. Annals of
Statistics 5, 1124-1147.
Hadjicostas, P. and Berry, S. M. (1999).
Improper and Proper Posteriors with Improper Priors in a
Poisson-Gamma Hierarchical Model. Test 8, 147-166.
H¨aggstr¨om, Olle. (2002). Finite Markov Chains and Algorithmic Applications. London: London Mathe-
matical Society (Student Texts 52).
H`ajek, B. (1985). A Tutorial Survey of Theory and Applications of Simulated Annealing. In Proceedings of
the 24th IEEE Conference on Decision and Control, New York, IEEE, pp.55-760.
H`ajek, B. (1988). Cooling Schedules for Optimal Annealing. Mathematical Operations Research 13, 311-
329.
Haldane, J. B. S. (1938). The Estimation of the Frequency of Recessive Conditions in Man. Annals of
Eugenics 7, 255-262.
Haller, H. and Krauss, S. (2002).
Misinterpretations of Signiﬁcance: A Problem Students Share With
Their Teachers. Methods of Psychological Research 7, 1-20.
Halpern, E. F. (1973). Polynomial Regression from a Bayesian Approach. Journal of the American Statis-
tical Association 68, 137-143.
Halton, J. H. (1970). A Retrospective and Prospective Survey of the Monte Carlo Method. SIAM Review
12, 1-63.
Hamilton, L. C. (1992). Regression with Graphics. Paciﬁc Grove, CA: Brooks/Cole Publishing Company.
Hammersley, J. M. and Morton, J. M. (1956).
A New Monte Carlo Techinique: Antithetical Variables.
Proceedings of the Cambridge Philosophical Society 52, 449-475.
Hammersley, J. M. and Handscomb, D. C. (1964).
Monte Carlo Methods. London: Methuen.
Hampel, F. R. (1974). The Inﬂuence Curve and its Role in Robust Estimation. Journal of the American
Statistical Association 69, 383-393.
Hampel, F. R., Rousseeuw, P. J., Ronchetti, E. M. and Stahel, W. A. (1986).
Robust Statistics: The
Approach Based on Inﬂuence Functions. New York: John Wiley & Sons.
Han, C. and Carlin, B. P. (2001).
MCMC Methods for Computing Bayes Factors: A Comparative Review.
Journal of the American Statistical Society 96, 1122-1132.
Hannan, M. T., Carroll, G. R., Dundon, E. A. and Torres, J. C. (1995).
Organizational Evolution in

References
609
a Multinational Context: Entries of Automobile Manufacturers in Belgium, Britain, France, Germany,
and Italy. American Sociological Review 60, 509-528.
Hanushek, E. A. and Jackson, J. E. (1977).
Statistical Methods for Social Scientists. San Diego: Academic
Press.
Haque, M. M., Chin, H. C. and Huang, H. (2010).
Applying Bayesian Hierarchical Models To Examine
Motorcycle Crashes At Signalized Intersections. Accident Analysis & Prevention 42, 203-212.
Harlow, L. L. E., Mulaik, S. A. and Steiger, J. H. (1997).
What If There Were No Signiﬁcance Tests?
Lawrence Erlbaum Associates Publishers.
Haro-L´opez, R., Mallick, B. K. and Smith, A. F. M. (2000).
Binary Regression Using Data Adaptive
Robust Link Functions. In Generalized Linear Models: A Bayesian Perspective, Dipak K. Dey, Sujit
K. Ghosh and Bani K. Mallick (eds.).
New York: Marcel Dekker, pp.243-253.
Harris, T. E. (1956). The Existence of Stationary Measures for Certain Markov Processes. In Proceedings
of the 3rd Berkeley Symposium on Mathematical Statistics and Probability, Volume II. Berkeley and
Los Angeles: University of California Press, pp.113-124.
Hartigan, J. A. (1964). Invariant Prior Distributions. Annals of Mathematical Statistics 35, 836-845.
Hartigan, J. A. (1969). Linear Bayesian Models. Journal of the Royal Statistical Society, Series B 31,
446-454.
Hartigan, J. A. (1983). Bayes Theory. New York: Springer-Verlag.
Harrison, M. (2006) Bombers and Bystanders in Suicide Attacks in Israel, 2000 to 2003. Studies in Conﬂict
& Terrorism, 29, 187-206.
Hartley, H. O. (1958). Maximum Likelihood Estimation From Incomplete Data. Biometrics 14, 174-194.
Hartley, H. O. and Hocking, R. R. (1971).
The Analysis of Incomplete Data. Biometrics 27, 783-808.
Harvey, A. C. 1976. Estimating Regression Models with Multiplicative Heteroscedasticity. Econometrica
44, 461-5.
Hastie, D. (2005). Towards Automatic Reversible Jump Markov Chain Monte Carlo. Un-published doctoral
thesis, University of Bristol, Bristol, UK.
Hastings, W. K. (1970). Monte Carlo Sampling Methods Using Markov Chains and Their Applications.
Biometrika 57, 97-109.
Healy, M. J. R. and Westmacott, M. (1956).
Missing Values in Experiments Analyzed on Automatic
Computers. Applied Statistics 5, 203-206.
Heath, D. and Sudderth, W. (1976).
de Finetti’s Theorem on Exchangeable Variables. The American
Statistician 30, 188-189.
Heidelberger, P. and Welch, P. D. (1981a).
Adaptive Spectral Methods for Simulation Output Analysis.
IBM Journal of Research and Development 25, 860-876.
Heidelberger, P. and Welch, P. D. (1981b).
A Spectral Method for Conﬁdence Interval Generation and
Run Length Control in Simulations. Communications of the Association for Computing Machinery
24, 233-245.
Heidelberger, P. and Welch, P. D. (1983).
Simulation Run Length Control in the Presence of an Initial
Transient. Operations Research 31, 1109-1144.
Hellekalek, P. (1998). Good Random Number Generators Are (not so) Easy to Find. Mathematics and
Computers in Simulation 46, 485-505.
Hernandez, F. and Johnson, R. A. (1980).
The Large-Sample Behavior of Transformations to Normality.
Journal of the American Statistical Association 75, 855-861.
Heumann, C. (2011). James-Stein Estimator. In International Encyclopedia of Statistical Science. Lovric,
Miodrag (ed.). New York: Springer-Verlag, pp.699-701.
Heyde, C. C. and Morton, R. (1996).
Quasi-Likelihood and Generalizing the EM Algorithm. Journal of
the Royal Statistical Society, Series B 58, 317-327.

610
References
Higdon, D. M. (1998).
Auxiliary Variable Methods for Markov Chain Monte Carlo With Applications.
Journal of the American Statistical Association 93, 585-595.
Higham, N. J. (1996). Accuracy and Stability of Numerical Algorithms. Philadelphia: Society for Industrial
and Applied Mathematics.
Hill, B. (1974). On Coherence, Inadmissibility, and Inference About Many Parameters in the Theory of
Least Squares. In Studies in Bayesian Econometrics and Statistics: in Honor of Leonard J. Savage, S.
Fienberg and A. Zellner (eds.).
Amsterdam: North Holland, pp.555-584.
Hill, J. R. and Tsai, C-L. (1988).
Calculating the Eﬃciency of Maximum Quasilikelihood Estimation.
Applied Statistics 37, 219-230.
Hill, J. and Kriesi, H. (2001).
Classiﬁcation by Opinion Changing Behavior: A Mixture Model Approach.
Political Analysis 9, 301-324.
Hills, S. E. and Smith, A. F. M. (1992).
Parameterization Issues in Bayesian Inference.
In Bayesian
Statistics 4, J. O. Berger, J. M. Bernardo, A. P. Dawid and A. F. M. Smith (eds.).
Oxford: Oxford
University Press, pp.227-246.
Hinkley, D. V. (1987). Comment. Journal of the American Statistical Association 82, 128-129.
Hipp, C. (1974). Suﬃcient Statistics and Exponential Families. Annals of Statistics 2, 1283-1292.
Hjort, N. L. (1996). Bayesian Approaches to Non- and Semiparametric Density Estimation. In Bayesian
Statistics 5, J. O. Berger, J. M. Bernardo, A. P. Dawid, D. V. Lindley and A. F. M. Smith (eds.).
Oxford: Oxford University Press, pp.223-253.
Hoaglin, D. C., Mosteller, F. and Tukey, J. W. (1983).
Understanding Robust and Exploratory Data
Analysis. New York: Wiley & Sons.
Hobert, J. P. and Casella, G. (1996).
The Eﬀect of Improper Priors on Gibbs Sampling in Hierarchical
Linear Mixed Models. Journal of the American Statistical Association 91, 1461-1473.
Hobert, J. P. and Casella, G. (1998).
Functional Compatibility, Markov Chains, and Gibbs Sampling with
Improper Posteriors. Journal of Computational and Graphical Statistics 7, 42-60.
Hobert, J. P., Robert, C. P. and Titterington, D. M. (1999).
On Perfect Simulation for Some Mixtures of
Distributions. Journal of Statistics and Computing 9, 287-298.
Hodges, J. S. and Sargent, D. J. (2001).
Counting Degrees of Freedom in Hierarchical and Other Richly
Parameterized Models. Biometrika 88, 367-379.
Hoel, P. G., Port, S. C. and Stone, C. J. (1987).
An Introduction to Stochastic Processes.
Prospect
Heights, IL: Waveland Press.
Hoeting, J. A., Madigan, D., Raftery, A. E. and Volinsky, C. T. (1999).
Bayesian Model Averaging: A
Tutorial. Statistical Science 14, 382-417.
Hoﬀ, P. D. (2009). Multiplicative Latent Factor Models for Description and Prediction of Social Networks.
Computational and Mathematical Organization Theory 15, 261-272.
Hoﬀ, P. D. (2005). Bilinear Mixed-eﬀects Models for Dyadic Data. Journal of the American Statistical
Association 100, 286-295.
Hogarth, R. M. (1975). Cognitive Processes and the Assessment of Subjective Probability Distribution.
Journal of the American Statistical Association 70, 271-289.
Hogg, R. V. and Craig, A. T. (1978).
Introduction to Mathematical Statistics. New York: Macmillan
Publishing Co.
Holley, R. A., Kusuoka, S. and Stroock, D. W. (1989).
Asymptotics of the Spectral Gap with Applications
to the Theory of Simulated Annealing. Journal of Functional Analysis 83, 333-347.
Hong, Y. and Lee, Y-J. (2013).
A Loss Function Approach To Model Speciﬁcation Testing and Its Relative
Eﬃciency. Annals of Statistics 41, 1166-1203.
Hora, S. C., Hora, J. A. and Dodd, N. G. (1992).
Assessment of Probability Distributions for Continuous

References
611
Random Variables: A Comparison of the Bisection and Fixed-value Methods. Organizational Behavior
and Human Decision Processes 51, 133-55.
Howson, C. and Urbach, P. (1993).
Scientiﬁc Reasoning: The Bayesian Approach.
Second Edition.
Chicago: Open Court.
Hsu, J. S. J. (1995). Generalized Laplacian Approximations in Bayesian Inference. Canadian Journal of
Statistics 23, 399-410.
Huang, D. S. (1970). Regression and Econometric Methods. New York: Wiley & Sons.
Huber, P. J. (1972). Robust Statistics: A Review. Annals of Mathematical Statistics 43, 1041-1067.
Huber, P. J. (1973). Robust Regression: Asymptotics, Conjectures, and Monte Carlo. Annals of Statistics
1, 799-821.
Huber, P. J. (1981). Robust Statistics. New York: Wiley & Sons.
Hull, T. E. and Dobell, A. R. (1964).
Mixed Congruential Random Number Generators for Binary
Machines. Journal of the Association for Computing Machinery 11, 31-40.
Hunter, J. E. (1997). Needed: A Ban on the Signiﬁcance Test. Psychological Science 8, 3-7.
Hunter, J. E. and Schmidt, F. L. (1990).
Methods of Meta-Analysis: Correcting Error and Bias in
Research Findings. Beverly Hills: Sage.
Hurd, W. J. (1974). Eﬃcient Generation of Statistically Good Pseudonoise by Linearly Interconnected Shift
Registers. IEEE Transactions on Computers C-23, 146-152.
Hurn, M. (1997). Diﬃculties in the Use of Auxiliary Variables in Markov Chain Monte Carlo Methods.
Statistics and Computing 7, 35-44.
Hurst, R. L. and Knop, R. E. (1972).
Generation of Random Normal Correlated Variables: Algorithm
425. Communications of the Association for Computing Machinery 15, 355-357.
Hurvich, C. M. and Tsai, C-L. (1991).
Bias of the Corrected AIC Criterion for Underﬁtted Regression and
Time Series Models. Biometrika 78, 499-509.
Hurvich, C. M., Shumway, R. and Tsai, C-L. (1990).
Improved Estimators of Kullback-Leibler Information
for Autoregressive Model Selection in Small Samples. Biometrika 77, 709-719.
Hwang, J. T., Casella, G., Robert, C. P., Wells, M. T. and Farrell, R. H. (1992).
Estimation of Accuracy
in Testing. Annals of Statistics 20, 490-509.
Hyndman, R. J. (1996). Computing and Graphing Highest Density Regions. The American Statistician
50, 120-126.
Ibrahim, J. G. and Chen, M-H. (2000a).
Power Prior Distributions for Regression Models. Statistical
Science 15, 46-60.
Ibrahim, J. G. and Chen, M-H. (2000b).
Prior Elicitation and Variable Selection for Generalized Linear
Mixed Models. In Generalized Linear Models: A Bayesian Perspective, Dipak K. Dey, Sujit K. Ghosh
and Bani K. Mallick (eds.).
New York: Marcel Dekker, pp.41-53.
Ibrahim, J. G. and Laud, P. W. (1991).
On Bayesian Analysis of Generalized Linear Models Using Jeﬀreys
Prior. Journal of the American Statistical Association 86, 981-986.
Ibrahim, J. G., Chen, M.-H. and Sinha, D. (2001).
Bayesian Survival Analysis. New York: Springer-
Verlag.
Ibrahim, J. G., Chen, M-H. and Sinha, D. (2003).
On Optimality of the Power Prior. Journal of the
American Statistical Association 98, 204-213.
Imai, K. and van Dyk, D. (2004).
Causal Inference with General Treatment Regimes: Generalizing the
Propensity Score. Journal of the American Statistical Association 99, 854-866.
Imai, K. and van Dyk, D. A. (2005).
A Bayesian Analysis of the Multinomial Probit Model Using Marginal
Data Augmentation. Journal of Econometrics 124, 311-334.
Ingber, L. (1992). Genetic Algorithms and Very Fast Simulated Reannealing: A Comparison. Mathematical
Computation and Modeling 16, 87-100.

612
References
Ingrassia, S. (1994).
On the Rate of Convergence of the Metropolis Algorithm and Gibbs Sampler by
Geometric Bounds. Annals of Applied Probability 4, 347-389.
Iosifescu, M. (1980). Finite Markov Processes and their Applications. New York: John Wiley & Sons.
Irony, T. Z. and Singpurwalla, N. D. (1996).
Noninformative Priors Do Not Exist: A Discussion with Jos´e
M. Bernard. Journal of Statistical Planning and Inference 65, 159-177.
Jackman, S. (2000). Estimation and Inference via Bayesian Simulation: An Introduction to Markov Chain
Monte Carlo. American Journal of Political Science 44, 375-404.
James, W. and Stein, C. (1961).
Estimation With Quadratic Loss. Proceedings of the Fourth Symposium
1 J. Neyman and E. L. Scott (eds.).
Berkeley: University of California Press, 361-380.
Jagerman, D. L. (1965). Some Theorems Concerning Pseudo-Random Numbers. Mathematics of Compu-
tation 19, 418-426.
Jamshidian, M. and Jennrich, R. I. (1993).
Conjugate Gradient Acceleration of the EM Algorithm. Journal
of the American Statistical Association 88, 221-228.
Janssen, A. (1986). Asymptotic Properties of Neyman-Pearson Tests for Inﬁnite Kullback-Leibler Informa-
tion. Annals of Statistics 14, 1068-1079.
Jansson, B. (1966). Random Number Generators. Stockholm: Victor Pettersons.
Jarner, S. F. and Roberts, G. O. (2002).
Polynomial Convergence Rates of Markov Chains. Annals of
Applied Probability 12, 224-247.
Jarvis, Edward. (1858). Distribution of Lunatic Reports. American Journal of Psychiatry 14, 248-253.
Jaynes, E. T. (1968). Prior Probabilities. IEEE Transactions on Systems Science and Cybernetics SSC-4,
227-241.
Jaynes, E. T. (1976). Conﬁdence Intervals vs. Bayesian Intervals. In Foundations of Probability Theory,
Statistical Inference, and Statistical Theories of Science II, William L. Harper and Cliﬀord A. Hooker
(eds.).
Dordrecht: D. Reidel, pp.175-257.
Jaynes, E. T. (1980). Marginalization and Prior Probabilities. In Bayesian Analysis in Econometrics and
Statistics, A. Zellner (ed.). North Holland: Amsterdam, pp.43-78.
Jaynes, E. T. (1983). Papers on Probability, Statistics and Statistical Physics, R. D. Rosencrantz (ed.).
Dordrecht: Reidel.
Jeﬀreys, H. (1961). The Theory of Probability. Third Edition. Oxford, England: Oxford University Press.
Jeng, F-C. and Woods, J. W. (1990).
Simulated Annealing in Compound Gaussian Random Fields. IEEE
Transactions on Information Theory 36, 94-107.
J¨ohnk, M. D. (1964). Erzeugung von Betaverteilter und Gammaverteilter Zufallszahlen. Metrika 8, 5-15.
Johnson, N. L., Kotz, S. and Balakrishnan, N. (1994).
Continuous Univariate Distributions, Volume 1.
New York: John Wiley & Sons.
Johnson, N. L., Kotz, S. and Balakrishnan, N. (1995).
Continuous Univariate Distributions, Volume 2.
New York: John Wiley & Sons.
Johnson, N. L., Kotz, S. and Balakrishnan, N. (1997).
Discrete Multivariate Distributions. New York:
John Wiley & Sons.
Johnson, N. L., Kotz, S. and Balakrishnan, N. (2000).
Continuous Multivariate Distributions, Models and
Applications, Volume 1. New York: John Wiley & Sons.
Johnson, N. L., Kemp, A. W. and Kotz, S. (2005). Univariate Discrete Distributions. New York: John
Wiley & Sons.
Johnson, R. A. (1984). The Analysis of Transformed Data: Comment. Journal of the American Statistical
Association 79, 314-315.
Johnson, V. E. (1998).
A Coupling-Regeneration Scheme for Diagnosing Convergence in Markov Chain
Monte Carlo Algorithms. Journal of the American Statistical Association 93, 238-248.
Johnson, V. E. and Albert, J. H. (1999).
Ordinal Data Modeling. New York: Springer-Verlag.

References
613
Jones, G. L. and Hobert, J. P. (2001).
Honest Exploration of Intractable Probability Distributions via
Markov Chain Monte Carlo. Statistical Science 16, 312-34.
Jones, G. L. and Hobert, J. P. (2004).
Suﬃcient Burn-in for Gibbs Samplers for a Hierarchical Random
Eﬀects Model. Annals of Statistics 32, 784-817.
Jones, M. C. and Lunn, A. D. (1996).
Transformations and Random Variate Generation: Generalized
Ratio-of-Uniforms Methods. Journal of Statistical Computation and Simulation 55, 49-55.
Jørgensen, B. (1983). Maximum Likelihood Estimation and Large-Sample Inference for Generalized Linear
and Nonlinear Regression Models. Biometrics 70, 19-28.
Kackar, R. N. and Harville, D. A. (1984).
Approximations for Standard Errors of Estimators of Fixed and
Random Eﬀects in Mixed Linear Models. Journal of the American Statistical Association 79, 853-862.
Kadane, J. B. (1986). Progress Toward a More Ethical Method Clinical Trials. Journal of Medical Philos-
ophy 11, 385-405.
Kadane, J. B. (1980). Predictive and Structural Methods for Eliciting Prior Distributions. In
Bayesian
Analysis in Econometrics and Statistics: Essays in Honor of Harold Jeﬀreys, Arnold Zellner (ed.).
Amsterdam: North Holland, pp.89-109.
Kadane, J. B. and Chuang, D. T. (1978).
Stable Decision Problems. Annals of Statistics 6, 1095-1110.
Kadane, J. B. and Srinivasan, C. (1996).
Bayesian Robustness and Stability. In Bayesian Robustness, J. O.
Berger, B. Betr´o, E. Moreno, L. R. Pericchi, F. Ruggeri, G. Salinetti and L. Wasserman (eds.).Hayward,
CA: Institute of Mathematical Statistics Monograph Series 29, pp.139-156.
Kadane, J. B. and Winkler, R. L. (1988).
Separating Probability Elicitation From Utilities. Journal of
the American Statistical Association 83, 357-363.
Kadane, J. B. and Wolfson, L. J. (1996).
Priors for Design and Analysis of Clinical Trials. In Bayesian
Biostatistics, D. A. Berry and D. K. Stangl (eds.). New York: Chapman & Hall/CRC, pp.157-186.
Kadane, J. B. and Wolfson, L. J. (1998).
Experiences in Elicitation. Journal of The Royal Statistical
Society, Series D 47, 3-19.
Kadane, J. B., Dickey, J. M., Winkler, R. L., Smith, W. S. and Peters, S. C. (1980).
Interactive Elicitation
of Opinion for a Normal Linear Model. Journal of the American Statistical Association 75, 845-854.
Kahn, H. (1949). Stochastic (Monte Carlo) Attenuation Analysis. No. RAND-P-88 (rev). Defense Technical
Information Center.
Kalyanam, K. (1996). Pricing Decisions under Demand Uncertainty: A Bayesian Mixture Model Approach.
Marketing Science 15, 207-221.
Kaplan, A. (1964). Conduct of Inquiry. San Francisco: Chandler Publishing Company.
Kaplan, H. B., Johnson, R. J., Bailey, C. A. and Simon, W. (1987).
The Sociological Study of AIDS:
A Critical Review of the Literature and Suggested Research Agenda.
Journal of Health and Social
Behavior 28, 140-157.
Karlin, S. and Taylor, H. M. (1981).
A Second Course in Stochastic Processes. San Diego: Academic
Press.
Karlin, S. and Taylor, H. M. (1990).
A First Course in Stochastic Processes. San Diego: Academic Press.
Kass, R. E. (1989). The Geometry of Asymptotic Inference. Statistical Science 4, 188-219.
Kass, R. E. (1993). Bayes Factors in Practice. The Statistician 42, 551-560.
Kass, R. E. and Greenhouse, J. B. (1989).
Comments on the paper by J. H. Ware. Statistical Science 4,
310-317.
Kass, R. E. and Raftery, A. E. (1995).
Bayes Factors. Journal of the American Statistical Association
90, 773-795.
Kass, R. E. and Steﬀey, D. (1989).
Approximate Bayesian Inference in Conditionally Independent Hierar-
chical Models. Journal of the American Statistical Association 84, 717-726.

614
References
Kass, R. E., Tierney, L. and Kadane, J. B. (1989).
Approximate Methods for Assessing Inﬂuence and
Sensitivity in Bayesian Analysis. Biometrika 76, 663-674.
Kass, R. E. and Vaidyanathan, S. K. (1992).
Approximate Bayes Factors and Orthogonal Parameters,
with Application to Testing Equality of Two Binomial Proportions. Journal of the Royal Statistical
Society, Series B 54, 129-144.
Kass, R. E. and Wasserman, L. (1995).
A Reference Bayesian Test for Nested Hypotheses and Its
Relationship to the Schwarz Criterion. Journal of the American Statistical Association 90, 928-934.
Kass, R. E. and Wasserman, L. (1996).
The Selection of Prior Distributions by Formal Rules. Journal of
the American Statistical Association 91, 1343-1370.
Kass, R. E., Tierney, L. and Kadane, J. B. (1989).
Approximate Methods for Assessing Inﬂuence and
Sensitivity in Bayesian Analysis. Biometrika 76, 663-674.
Kaufmann, H. (1988). On the Existence and Uniqueness of Maximum Likelihood Estimates in Quantal and
Ordinal Response Models. Metrika 35, 291-313.
Keating, J. P. and Mason, R. L. (1988).
James-Stein Estimation from an Alternative Perspective. The
American Statistician 42, 160-164.
Keith, J., K., Kroese, D. P. and Bryant, D. (2004).
A Generalized Markov Sampler. Methodology and
Computing in Applied Probability 6, 29-53.
Kelleher, C. A. and Yackee, S. W. (2009).
A Political Consequence of Contracting: Organized Interests
and State Agency Decision-making. The Journal of Public Administration Research and Theory 19,
579-602.
Kempthorne, P. J. (1986). Decision-Theoretic Measures of Inﬂuence in Regression. Journal of the Royal
Statistical Society, Series B 48, 370-378.
Kendall, M. G. (1949). On Reconciliation of the Theories of Probability. Biometrika 36, 101-116.
Kennedy, W. J. and Gentle, J. E. (1980).
Statistical Computing. New York: Marcel Dekker.
K´ery, M. (2010). Introduction to WinBUGS for Ecologists: A Bayesian Approach to Regression, ANOVA
and Related Analyses. Burlington, MA: Academic Press.
K´ery, M. and Schaub, M. (2011).
Bayesian Population Analysis Using WinBUGS: A Hierarchical Per-
spective. Burlington, MA: Academic Press.
Keynes, J. M. (1921). A Treatise on Probability. London: MacMillan.
Kim, D. (1991). A Bayesian Signiﬁcance Test of the Stationarity of Regression Parameters. Biometrika 78,
667-675.
Kinderman, A. J. and Monahan, J. F. (1980).
New Methods for Generating Student’s t and Gamma
Variables. Computing 25, 369-377.
Kinderman, A. J. and Ramage, J. G. (1976).
Computer Generation of Normal Random Variables. Journal
of the American Statistical Association 71, 893-896.
Kinderman, A. J., Monahan, J. F. and Ramage, J. G. (1975).
Computer Generation of Random Variables
with Normal and Student’s Distributions. Proceedings of the Statistical Computing Section. Alexandria,
VA: American Statistical Association. 128-131.
King, G. (1989). Unifying Political Methodology: The Likelihood Theory of Statistical Inference. Cam-
bridge: Cambridge University Press.
Kirk, R. E. (1996). Practical Signiﬁcance: A Concept Whose Time Has Come. Educational and Psycho-
logical Measurement 56, 746-759.
Kirkpatrick, S., Gelatt, C. D. and Vecchi, M. P. (1983).
Kirkpatrick, S.
Optimization by Simulated
Annealing. Science 220, 671-680.
Kitagawa, G. and Akaike, H. (1982).
A Quasi Bayesian Approach to Outlier Detection. Annals of the
Institute of Statistical Mathematics 34B, 389-398.

References
615
Kitagawa, G. and Gersch, W. (1996).
Smoothness Priors Analysis of Time Series. New York: Springer-
Verlag.
Kleibergen, F. and van Dijk, H. K. (1993).
Eﬃcient Computer Generation of Matrix-Variate t Drawings
with an Application to Bayesian Estimation of Simple Market Models. In Computer Intensive Methods
in Statistics, W. Hardle and L. Simar (eds.).
Heidelberg: Physica-Verlag, pp.30-46.
Kloek, T. and van Dijk, H. K. (1978).
Bayesian Estimates of Equation System Parameters; An Application
Integration by Monte Carlo. Econometrica 46, 1-19.
Knott, M., Albanese, T. M. and Galbraith, J. (1990).
Scoring Attitudes to Abortion. The Statistician 40,
217-223.
Knuth, D. E. (1981). The Art of Computer Programming, Volume 2: Seminumerical Algorithms. Second
Edition. Menlo Park, CA: Addison-Wesley.
Kollman, K., Miller, J. H. and Page, S. E. (1997).
Political Institutions and Sorting in a Tiebout Model.
American Economic Review 87, 977-992.
Kolmogorov, A. (1933). Grundbegriﬀe der Wahrscheinlichkeitsrechnung. Berlin: Julius Springer.
Kong, A., Liu, J. S. and Wong, W. H. (1994).
Sequential Imputations and Bayesian Missing Data Problems.
Journal of the American Statistical Association 89, 278-288.
Koop, G. (1992). “Objective” Bayesian Unit Root Tests. Journal of Applied Econometrics 7, 65-82.
Koopman, L. H. (1936). On Distributions Admitting a Suﬃcient Statistic. Transactions of the American
Mathematical Society 39, 399-409.
Koppel, J. G. S. (1999). The Challenge of Administration by Regulation: Preliminary Findings Regarding
the U.S Government’s Venture Capital Funds. Journal of Public Administration Research and Theory
9, 641-666.
Korwar, R. M. and Hollander, M. (1976).
Empirical Bayes Estimation of a Distribution Function. The
Annals of Statistics 4, 581-588.
Kotz, S. and Nadarajah, S. (2000).
Extreme Value Distributions: Theory and Applications. Singapore:
World Scientiﬁc Publications.
Kowalski, J., Tu, X. M., Day, R. S. and Mendoza-Blanco, J. R. (1997).
On the Rate of Convergence of the
ECME Algorithm for Multiple Regression Models With t-distributed Errors. Biometrika 84, 269-281.
Krawczyk, H. (1992). How to Predict Congruential Generators. Journal of Algorithms 13, 527-545.
Kreft, I. G. G. (1993). Using Multilevel Analysis to Assess School Eﬀectiveness: A Study of Dutch Secondary
Schools. Sociology of Education 66, 104-129.
Kreft, I. G. G. and De Leeuw, J. (1988).
The Seesaw Eﬀect: A Multilevel Problem? Quality and Quantity
22, 127-137.
Kreft, I. G. G. and De Leeuw, J. (1998).
Introducing Multilevel Modeling. Thousand Oaks, CA: Sage.
Kreuzenkamp, H. A. and McAleer, M. (1995).
Simplicity, Scientiﬁc Inference, and Econometric Modelling.
The Economic Journal 105, 1-21.
Krishnamoorthy, K. (2006). Handbook of Statistical Distributions with Applications. New York: Chapman
& Hall/CRC.
Krommer, A. R. and Ueberhuber, C. W. (1998).
Computational Integration. Philadelphia: Society for
Industrial and Applied Mathematics.
Kronmal, R. (1964).
The Evaluation of a Pseudorandom Normal Number Generator.
Journal of the
Association for Computing Machinery 11, 357-263.
Kronmal, R. and Peterson, A. V., Jr. (1981).
A Variant of the Acceptance-Rejection Method for Computer
Generation of Random Variables. Journal of the American Statistical Association 76, 446-451.
Kronrod, A. S. (1965). Nodes and Weights of Quadrature Formulas. New York: Consultants Bureau.
Krueger J. (2001). Null Hypothesis Signiﬁcance Testing: On the Survival of a Flawed Method. American
Psychologist 56, 16-26.

616
References
Kruschke, J. (2010). Doing Bayesian Data Analysis: A Tutorial Introduction with R. Burlington, MA:
Academic Press.
Krzanowski, W. J. (1988). Principles of Multivariate Analysis. Oxford: Oxford University Press.
Kullback, S. (1968). Information Theory and Statistics. New York: Wiley & Sons.
Kurganov, I. A. (1973). Speeches and Writings, 1945-1976. Palo Alto, CA: Online Archive of California,
http://www.oac.cdlib.org
Kyung, M., Gill, J. and Casella, G. (2011).
New Findings from Terrorism Data: Dirichlet Process Random
Eﬀects Models for Latent Groups. Journal of the Royal Statistical Society, Series C 60, 701-721.
Kyung, M., Gill, J. and Casella, G.. (2012). Sampling Schemes for Generalized Linear Dirichlet Process
Random Eﬀects Models. With Discussion and Rejoinder.
Statistical Methods and Applications 20,
259-290.
Laird, N. M. (1978). Nonparametric Maximum Likelihood Estimation of a Mixing Distribution. Journal of
the American Statistical Association 73, 805-811.
Laird, N. M. and Louis, T. A. (1982).
Approximate Posterior Distributions for Incomplete Data Problems.
Journal of the Royal Statistical Society, Series B 44, 190-200.
Laird, N. M. and Louis, T. A. (1987).
Empirical Bayes Conﬁdence Intervals Based on Bootstrap Samples.
Journal of the American Statistical Association 82, 739-750.
Laird, N. M., Lange, N. and Stram, D. O. (1987).
Maximum Likelihood Computations with Repeated
Measures: Applications of the EM Algorithm. Journal of the American Statistical Association 82,
97-105.
Lange, K. L. (2000). Numerical Analysis for Statisticians. New York: Springer-Verlag.
Lange, K. L., Little, R. J. A. and Taylor, J. M. G. (1989).
Robust Statistical Modeling Using the t
Distribution. Journal of the American Statistical Association 84, 881-896.
Laplace, P. S. (1774). M´emoire sur la Probabilit´e des Causes par le ´Ev`enemens. M´emoires de l’Acad´emie
Royale des Sciences Present´es par Divers Savans 6, 621-656.
Laplace, P. S. (1781). M´emoire sur la Probabilit´es. M´emoires de l’Acad´emie Royale des Sciences de Paris
1778, 227-332.
Laplace, P. S. (1811). M´emoire sur les Integrales D´eﬁnies et leur Application aux Probabilit´es, et Speciale-
mement `a Recherche du Milieu Qu‘il Faut Chosier Entre les Resultats des Observations. M´emoires de
l’Acad´emie des Sciences de Paris, 279-347.
Laplace, P. S. (1814). Essai Philosophique sur les la Probabilit´es. Paris: Ve Courcier.
Laud, P. W. and Ibrahim, J. G. (1995).
Predictive Model Selection.
Journal of the Royal Statistical
Society, Series B 57, 247-262.
Lauritzen, S. and Spiegelhalter, D. J. (1988).
Local Computations with Probabilities on Graphical Struc-
tures and their Application to Expert Systems. Journal of the Royal Statistical Society, Series B 50,
157-194.
Lavine, M. (1991a).
Sensitivity in Bayesian Statistics:
The Prior and the Likelihood.
Journal of the
American Statistical Association 86, 396-399.
Lavine, M. (1991b). An Approach to Robust Bayesian Analysis for Multidimensional Parameter Spaces.
Journal of the American Statistical Association 86, 400-403.
Lavine, A. M. and Schervish, M. J. (1999).
Bayes Factors: What They Are and What They Are Not. The
American Statistician 53, 119-122.
Law, A. M. and Kelton, W. D. (1982).
Simulation Modeling and Analysis. New York: McGraw-Hill.
Lawler, G. F. and Sokal, A. D. (1988).
Bounds on the L2 Spectrum for Markov Chains and Their
Applications. Transactions of the American Mathematical Society 309, 557-580.
Lawson, A. B. (2013). Bayesian Disease Mapping: Hierarchical Modeling in Spatial Epidemiology. Boca
Raton: Chapman & Hall/CRC.

References
617
Le Cam, L. (1986). Asymptotic Methods in Statistical Decision Theory. New York: Springer-Verlag.
Leamer, E. E. (1978). Speciﬁcation Searches: Ad Hoc Inference with Nonexperimental Data. New York:
John Wiley & Sons.
Leamer, E. E. (1979). Information Criteria for Choice of Regression Models: A Comment. Econometrica
47, 507-510.
Leamer, E. E. (1983). Let’s Take the Con Out of Econometrics. American Economic Review 73, 31-43.
Leamer, E. E. (1984). Global Sensitivity Results for Generalized Least Squares Estimates. Journal of the
American Statistical Association 79, 867-870.
Leamer, E. E. (1985). Sensitivity Analysis Would Help. The American Economic Review 75, 308-313.
Leamer, E. E. (1992). Bayesian Elicitation Diagnostics. Econometrica 80, 919-942.
Learmonth, G. P. and Lewis, P. A. W. (1973).
Some Widely Used and Recently Proposed Uniform Random
Number Generators. Proceedings of Computer Science and Statistics: Seventh Annual Symposium on
the Interface, W. J. Kennedy (ed.). Ames, IA: Iowa State University. 163-171.
L’Ecuyer, P. (1998). Uniform Random Number Generators. 1998 Winter Simulation Conference Proceed-
ings, Society for Computer Simulation. 97-104.
Lee, P. M. (2004). Bayesian Statistics: An Introduction. Second Edition. New York: Hodder Arnold.
Lee, S. (1998).
On the Quantile Process Based on the Autoregressive Residuals. Journal of Statistical
Planning and Inference 67, 17-28.
Lee, V. E. and Bryk, A. S. (1989).
Multilevel Model of the Social Distribution of High School Achievement.
Sociology of Education 62, 172-192.
Legendre, A. M. (1805).
Nouvelles M´ethodes Pour la D´etermination des Orbites des Com`etes.
Paris:
Courcier.
Lehmann, E. L. (1986). Testing Statistical Hypotheses. New York: Springer-Verlag.
Lehmann, E. L. (1993). The Fisher, Neyman-Pearson Theories of Testing Hypotheses: One Theory or Two?
Journal of the American Statistical Association 88, 1242-1249.
Lehmann, E. L. (1999). Elements of Large-Sample Theory. New York: Springer-Verlag.
Lehmann, E. L. and Casella, G. (1998).
Theory of Point Estimation.
Second Edition.
New York:
Springer-Verlag.
Lehmer, D. H. (1951). Mathematical Models in Large-scale Computing Units. Proceedings of the Second
Symposium on Large Scale Digital Computing Machinery. Cambridge: Harvard University Press. 141-
146.
Li, L. and Choe, M. K. (1997). A Mixture Model for Duration Data: Analysis of Second Births in China.
Demography. 34, 189-197.
Leimkuhler, B. and Reich, S. (2005).
Simulating Hamiltonian Dynamics. Cambridge: Cambridge Univer-
sity Press.
Lempers, F. B. (1971).
Posterior Probabilities of Alternative Linear Models.
Rotterdam:
Rotterdam
University Press.
Leonard, T. (1975). A Bayesian Approach to the Linear Model with Unequal Variances. Technometrics
17, 95-102.
Leonard, T. and Hsu, J. S. J. (1999).
Bayesian Methods: An Analysis for Statisticians and Interdisci-
plinary Researchers. Cambridge, England: Cambridge University Press.
Leonard, T., Hsu, J. S. J. and Tsui, K-W. (1989).
Bayesian Marginal Inference. Journal of the American
Statistical Association 84, 1051-1058.
Lesaﬀre, E. and Kaufmann, H. (1992).
Existence and Uniqueness of the Maximum Likelihood Estimator
for a Multivariate Probit Model. Journal of the American Statistical Association 87, 805-811.
Levine, R. (1996). Post-Processing Random Variables. Ph.D. Thesis, Biometrics Unit, Cornell University.

618
References
Lewis, C. and Thayer, D. T. (2009).
Bayesian Decision Theory for Multiple Comparisons. In IMS Lecture
Notes Monograph Series, Optimality: the Therd Erich L. Lehmann Symposium 57, pp.326-332.
Lewis, J. A. (1994). Discussion of the Paper by Spiegelhalter, Freedman, and Parmar. Journal of the Royal
Statistical Society, Series A 157, 392.
Lewis, S. M. and Raftery, A. E. (1997).
Estimating Bayes Factors Via Posterior Simulation With the
Laplace-Metropolis Estimator. Journal of the American Statistical Association 92, 648-655.
Lewis, T. G. and Payne, W. H. (1973).
Generalized Feedback Shift Register Pseudorandom Number
Algorithm. Journal of the Association for Computing Machinery 20, 456-468.
Li, H-L. (2004).
The Sampling/Importance Resampling Algorithm.
In Applied Bayesian Modeling and
Causal Inference from Incomplete-Data Perspectives, Andrew Gelman and Xiao-Li Meng (eds.).
New
York: John Wiley & Sons, pp.265-276.
Li, L. and Choe, M. K. (1997).
A Mixture Model for Duration Data: Analysis of Second Births in China.
Demography 34, 189-197.
Lindley, D. V. (1957). A Statistical Paradox. Biometrika 44, 187-192.
Lindley, D. V. (1958). Fiducial Distributions and Bayes’ Theory. Journal of the Royal Statistical Society,
Series B 20, 102-107.
Lindley, D. V. (1961).
The Use of Prior Probability Distributions in Statistical Inference and Decision.
Proceedings of the Fourth Berkeley Symposium on Mathematical Statistics and Probability. Berkeley:
University of California Press. 453-468.
Lindley, D. V. (1965). Introduction to Probability and Statistics from a Bayesian Viewpoint, Parts 1 and
2. Cambridge, England: Cambridge University Press.
Lindley, D. V. (1968). The Choice of Variables in Multiple Regression. Journal of the Royal Statistical
Society, Series B 30, 31-66.
Lindley, D. V. (1969).
Discussion of Compound Decisions and Empirical Bayes, J. B. Copas. Journal of
the Royal Statistical Society, Series B 31, 397-425.
Lindley, D. V. (1972). Bayesian Statistics: A Review. Philadelphia: Society for Industrial and Applied
Mathematics.
Lindley, D. V. (1983). Reconciliation of Probability Distributions. Operations Research, Journal of Oper-
ations Research Society of America 31, 866-880.
Lindley, D. V. (1985). Making Decisions. Second Edition. New York: John Wiley & Sons.
Lindley, D. V. (1986). Comment. The American Statistician 40, 6-7.
Lindley, D. V. (1991). Subjective Probability, Decision Analysis and Their Legal Consequences. Journal of
the Royal Statistical Society, Series A 154, 83-92.
Lindley, D. V. and Novick, M. R. (1978).
The Use of More Realistic Utility Functions in Educational
Applications. Journal of Educational Measurement 15, 181-191.
Lindley, D. V. and Novick, M. R. (1981).
The Role of Exchangeability in Inference. Annals of Statistics
9, 45-58.
Lindley, D. V. and Singpurwalla, N. D. (1991).
On the Evidence Needed to Reach Agreed Action Between
Adversaries, with Application to Acceptance Sampling. Journal of the American Statistical Association
86, 933-937.
Lindley, D. V. and Smith, A. F. M. (1972).
Bayes Estimates for the Linear Model. Journal of the Royal
Statistical Society, Series B 34, 1-41.
Lindley, D. V., Tversky, A. and Brown, R. V. (1979).
On the Reconciliation of Probability Assessments.
Journal of the Royal Statistical Society, Series A 142, 146-180.
Lindsay, R. M. (1995).
Reconsidering the Status of Tests of Signiﬁcance: An Alternative Criterion of
Adequacy. Accounting, Organizations and Society 20, 35-53.
Lindsey, J. K. (1997). Applying Generalized Linear Models. New York: Springer-Verlag.

References
619
Linick, T. W., Jull, A. J. T., Toolin, L. J. and Donahue, D. J. (1986). Operation of the NSF-Arizona Ac-
celerator Facility For Radioscope Analysis and Results From Selected Collaborative Research Projects.
Radiocarbon 28, 522-533.
Liniger, W. (1961).
On a Method by D. H. Lehmer for the Generation of Pseudo-Random Numbers.
Numerische Mathematik 3, 265-270.
Liseo, B., Petrella, L. and Salinetti, G. (1996).
Bayesian Robustness:
an Interactive Approach.
In
Bayesian Statistics 5, J. O. Berger, J. M. Bernardo, A. P. Dawid, D. V. Lindley, (eds.).
Oxford:
Oxford University Press, pp.223-253.
Little, R. J. A. and Rubin, D. B. (1983).
On Jointly Estimating Parameters and Missing Data by
Maximizing the Complete-Data Likelihood. The American Statistician 37, 218-220.
Little, R. J. A. and Rubin, D. B. (2002).
Statistical Analysis with Missing Data. Second Edition. New
York: John Wiley & Sons.
Liu, C. and Rubin, D. B. (1994).
The ECME Algorithm: A Simple Extension of EM and ECM With
Faster Monotone Convergence. Biometrika 81, 633-648.
Liu, C. and Rubin, D. B. (1995).
ML Estimation of the t Distribution Using EM and Its Extensions, ECM
and ECME. Statistica Sinica 5, 19-39.
Liu, C. and Rubin, D. B. (1998).
Maximum Likelihood Estimation of Factor Analysis Using the ECME
Algorithm With Complete and Incomplete Data. Statistica Sinica 8, 729-748.
Liu, C., Liu, J. S. and Rubin, D. B. (1992).
A Variational Control Variable for Assessing the Convergence of
the Gibbs Sampler. Proceedings of the American Statistical Association, Statistical Computing Section.
74-78.
Liu, J. S. (1994). The Collapsed Gibbs Sampler in Bayesian Computations with Applications to a Gene
Regulation Problem. Journal of the American Statistical Association 89, 958-966.
Liu, J. S. (1996a).
Metropolized Independent Sampling with Comparisons to Rejection Sampling and
Importance Sampling. Statistics and Computing 6, 113-119.
Liu, J. S. (1996b). Nonparametric Hierarchical Bayes via Sequential Imputations. Annals of Statistics 24,
911-930.
Liu, J. S. (2001). Monte Carlo Strategies in Scientiﬁc Computing. New York: Springer-Verlag.
Liu, J. S., Liang, F. and Wong, W. H. (2000).
The Multiple-Try Method and Local Optimization in
Metropolis Sampling Journal of the American Statistical Association 95, 121-134.
Liu, J. S. and Sabatti, C. (1999).
Simulated Sintering: Markov Chain Monte Carlo with Spaces Varying
Dimension. In Bayesian Statistics, J. M. Bernardo, A. F. M. Smith, A. P. Dawid and J. O. Berger
(eds.). Oxford: Oxford University Press, pp.389-414.
Liu, J. S., Wong, W. H. and Kong, A. (1994).
Covariance Structure of the Gibbs Sampler with Applications
to the Comparisons of Estimators and Augmentation Schemes. Biometrika 81, 27-40.
Liu, J. S., Wong, W. H. and Kong, A. (1995).
Correlation Structure and Convergence Rates of the Gibbs
Sampler with Various Scans. Journal of the Royal Statistical Society, Series B 57, 157-169.
Locatelli, M. (2000). Simulated Annealing Algorithms for Continuous Global Optimization: Convergence
Conditions. Journal of Optimization Theory and Applications 104, 121-133.
Loftus, G. R. (1991). On the Tyranny of Hypothesis Testing In the Social Sciences. Contemporary Psy-
chology 36, 102-105.
Loftus, G. R. (1993). A Picture Is Worth a Thousand rho Values: On the Irrelevance of Hypothesis Testing
In the Microcomputer Age. Behavioral Research Methods, Instruments, and Computers 25, 250-256.
Louis, T. A. (1982). Finding the Observed Information Matrix when Using the EM Algorithm. Journal of
the Royal Statistical Society, Series B 44, 226-233.
Lu, M. (2002). Enhancing Project Evaluation and Review Technique Simulation through Artiﬁcial Neu-
ral Network-based Input Modeling. Journal of Construction Engineering and Management Septem-
ber/October, 438-45.

620
References
Lundy, M. (1985).
Applications of the Annealing Algorithm to Combinatorial Problems in Statistics.
Biometrika 72, 191-198.
Lundy, M. and Mees, A. (1986).
Convergence of an Annealing Algorithm. Mathematical Programming
34, 111-124.
Lunn, D. J., Thomas, A., Best, N. G. and Spiegelhalter, D. (2000).
WinBUGS–A Bayesian Modelling
Framework: Concepts, Structure, and Extensibility. Statistics and Computing 10, 325–337.
Lunn, D. J., Jackson, C., Spiegelhalter, D. J., Best, N. G. and Thomas, A. (2012).
The BUGS book: A
Practical Introduction to Bayesian Analysis. Boca Raton: Chapman & Hall/CRC.
Lwin, T. (1972). Estimation of the Tail Paretian Law. Scandinavian Actuarial Journal 55, 170-178.
Lynn, R. and Vanhanen, T. (2001).
National IQ and Economic Development. Mankind Quarterly LXI,
415-437.
Macdonald, R. R. (1997). On Statistical Testing in Psychology. British Journal of Psychology 88, No. 2
(May), 333-49.
MacEachern, S. N. and Berliner, L. M. (1994).
Subsampling the Gibbs Sampler. The American Statistician
48, 188-190.
MacLaren, M. D. and Marsaglia, G. (1965).
Uniform Random Number Generators.
Journal of the
Association for Computing Machinery 12, 83-89.
Madras, N. and Sezer, D. (2010).
Quantitative Bounds for Markov Chain Convergence: Wasserstein and
Total Variation Distances. Bernoulli 16, 882-908.
Makelainen, T., Schmidt, K. and Styan, G. P. H. (1981).
On the Existence and Uniqueness of the Maximum
Likelihood Estimate of a Vector-Valued Parameter in Fixed-Size Samples. Annals of Statistics 9, 758-
767.
Makov, U. E., Smith, A. F. M. and Liu, Y-H. (1996).
Bayesian Methods in Actuarial Science.
The
Statistician 45, 503-515.
Malov, S. V. (1998). Random Variables Generated by Ranks in Dependent Schemes. Metrika 48, 61-67.
Maltzman, F. and Wahlbeck, P. J. (1996).
Strategic Policy Considerations and Voting Fluidity on the
Burger Court. American Political Science Review 90, 581-92.
Manski, C. F. (1995). Identiﬁcation Problems in the Social Sciences. Cambridge: Harvard University Press.
Marden, J. I. (2000).
Hypothesis Testing: From p Values to Bayes Factors.
Journal of the American
Statistical Association 95, 1316-1320.
Mar´ın, J. M. (2000). A Robust Version of the Dynamic Linear Model with an Economic Application. In
Robust Bayesian Analysis, David R´ıos Insua and Fabrizio Ruggeri (eds.).
New York: Springer-Verlag,
pp.373-383.
Marinari, E. and Parisi, G. (1992).
Simulated Tempering: A New Monte Carlo Scheme.
Europhysics
Letters 19, 451-458.
Maritz, J. S. (1970). Empirical Bayes Methods. London: Methuen.
Maritz, J. S. and Lwin, T. (1989).
Empirical Bayes Methods. Second Edition. New York: Chapman &
Hall.
Marsaglia, G. (1961a). Expressing a Random Variable in Terms of Uniform Random Variables. Annals of
Mathematical Statistics 32, 894-898.
Marsaglia, G. (1961b). Generating Exponential Random Variables. Annals of Mathematical Statistics 32,
899-900.
Marsaglia, G. (1964). Generating a Variable from the Tail of the Normal Distribution. Technometrics 6,
101-102.
Marsaglia, G. (1968). Random Numbers Fall Mainly in the Planes. Proceedings of the National Academy
of Sciences 61. 25-28.

References
621
Marsaglia, G. (1972). The Structure of Linear Congruential Sequences. In Applications of Number Theory
to Numerical Analysis, S. K. Zaremba (ed.). San Diego: Academic Press, pp.249-286.
Marsaglia, G. (1977). The Squeeze Method for Generating Gamma Variates. Computers and Mathematics
with Applications 3, 321-326.
Marsaglia, G. (1985). A Current View of Random Number Generators. In Computer Science and Statistics:
16th Symposium on the Interface, L. Billard (ed.). Amsterdam: North Holland, pp.3-10.
Marsaglia, G. and Bray, T. A. (1964).
A Convenient Method for Generating Normal Variables. SIAM
Review 6, 260-264.
Marsaglia, G., MacLaren, M. D. and Bray, T. A. (1964).
A Fast Procedure for Generating Normal Random
Variables. Communications of the Association for Computing Machinery 7, 4-10.
Marsaglia, G., Tsang, W. W. and Wang, J. (2003).
Evaluating Kolmogorov’s Distribution. Journal of
Statistical Software 8, 1-4.
Marshall, A. W. (1956).
The Use of Multi-Stage Sampling Schemes in Monte Carlo Computations.
In
Symposium on Monte Carlo Methods, M. Meyer (ed.). New York: John Wiley & Sons, pp.123-140.
Marshall, A. W. and Olkin, I. (1967).
A Multivariate Exponential Distribution. Journal of the American
Statistical Association 62, 30-44.
Martikainen, P., Martelin, T., Nihtil¨a, E., Majamaa, K. and Seppo, K. (2005).
Diﬀerences in Mortality
by Marital Status in Finland from 1976 to 2000: Analyses of Changes in Marital-Status Distributions,
Socio-Demographic and Household Composition, and Cause of Death. Population Studies 59, 99-115.
Martin, A. D. and Quinn, K. (2007).
Assessing Preference Change on the US Supreme Court. Journal of
Law, Economics, and Organization 23, 365-385.
Mart´ın, J., R´ıos Insua, D. and Ruggeri, F. (1996).
Local Sensitivity Analysis in Bayesian Decision Theory.
Lecture Notes-Monograph Series 29 (Bayesian Robustness), 119-135.
Mart´ın, J., R´ıos Insua, D. and Ruggeri, F. (1998).
Issues in Bayesian Loss Robustness. Sankhy¯a, Series
A 60, 405-417.
Martins, A. C. R. (2009). Bayesian Updating Rules In Continuous Opinion Dynamics Models. Journal of
Statistical Mechanics: Theory and Experiment 2, P02017.
Mason, R. L. and Lurie, D. (1973).
Systematic Simulators of Joint Order Uniform Variates. In Proceedings
of Computer Science and Statistics: Seventh Annual Symposium on the Interface, W. J. Kennedy (ed.).
Ames, IA: Iowa State University, pp.156-162.
Mason, W. M., Wong, G. Y. and Entwistle, B. (1983).
Contextual Analysis Through the Multilevel Linear
Model. In Sociological Methodology 1983-1984, S. Leinhardt (ed.). Oxford: Blackwell, pp.72-103.
Matthews, P. (1993). A Slowly Mixing Markov Chain With Implications for Gibbs Sampling. Statistics &
Probability Letters 17, 231-236.
Mayer, M. (1951).
Report on a Monte Carlo Calculation Performed with the Eniac.
In Monte Carlo
Method, A. S. Householder, G. E. Forsyth and H. H. Germond (eds.).
Applied Mathematics Series 12,
Washington: National Bureau of Standards, pp.19-20.
McArdle, J. J. (1976).
Empirical Test of Multivariate Generators.
In Proceedings of the Ninth Annual
Symposium on the Interface of Computer Science and Statistics, D. C. Hoaglin and R. Welsch (eds.).
Boston: Prindle, Weber, and Schmidt, pp.263-267.
McCloskey, D. N. and Ziliak, S. T. (1996).
The Standard Error of Regressions. Journal of Economic
Literature 34, 97-114.
McCullagh, P. (1983). Quasi-Likelihood Functions. Annals of Statistics 11, 59-67.
McCullagh, P. and Nelder, J. A. (1989).
Generalized Linear Models. Second Edition. New York: Chapman
& Hall.
McCulloch, C. E. (1994). Maximum Likelihood Variance Components Estimation for Binary Data. Journal
of the American Statistical Association 89, 330-335.

622
References
McCulloch, C. E. and Searle, S. R. (2001).
Generalized, Linear, and Mixed Models. New York: John
Wiley & Sons.
McCulloch, R. E. and Rossi, P. E. (1992).
Bayes Factors for Nonlinear Hypotheses and Likelihood
Distributions. Biometrika 79, 663-676.
McCullough, B. D. (1998). Assessing the Reliability of Statistical Software: Part I. The American Statis-
tician 52, 358-366.
McCullough, B. D. (1999). Assessing the Reliability of Statistical Software: Part II. The American Statis-
tician 53, 149-159.
McCullough, B. D. and Wilson, B. (1999).
On the Accuracy of Statistical Procedures in Microsoft Excel
97. Computational Statistics and Data Analysis 31, 27-37.
McDonald, G. C. (1999). Letter to the Editor. The American Statistician 53, 393.
McGrath, K. and Waterton, J. (1986).
British Social Attitudes, 1983-86 Panel Survey. London, Social
and Community Planning Research.
McGuire, J. W. (1996). Strikes in Argentina: Data Sources and Recent Trends. Latin American Research
Review 31, 127-150.
McKendrick, A. G. (1926). Applications of Mathematics to Medical Problems. Proceedings of the Edinburgh
Mathematical Society 44, 98-130.
McLachlan, G. J. and Basford, K. E. (1988).
Mixture Models: Inference and Application to Clustering.
New York: Marcel Dekker.
McLachlan, G. J. and Krishnan, T. (1997).
The EM Algorithm and Extensions. New York: John Wiley
& Sons.
Mead, M. (1973). Coming of Age in Samoa. New York: Morrow.
Mebane, W. R., Jr. (1994). Fiscal Constraints and Electoral Manipulation in American Social Welfare.
American Political Science Review 88, 77-94.
Medvedev, R. (1989). Let History Judge: The Origins and Consequences of Stalinism. New York: Columbia
University Press.
Meehl, P. E. (1978). Theoretical Risks and Tabular Asterisks: Sir Karl, Sir Ronald, and the Slow Progress
of Soft Psychology. Journal of Counseling and Clinical Psychology 46, 806-834.
Meehl, Paul E. (1990). Why Summaries of Research on Psychological Theories Are Often Uninterpretable.
Psychological Reports 66, 195-244.
Meehl, P. E. (1997). The Problem Is Epistemology, Not Statistics: Replace Signiﬁcance Tests By Conﬁ-
dence Intervals and Quantify Accuracy of Risky Numerical Predictions. In What If There Were No
Signiﬁcance Tests, L. L. E. Harlow, S. A. Mulaik and J. H. Steiger (eds.)
Lawrence Erlbaum Associates
Publishers, pp.393-425.
Meeus, W., van de Schoot, R., Keijsers, L., Schwartz, S. J. and Branje, S. (2010).
On the Progression
and Stability of Adolescent Identity Formation: A Five-Wave Longitudinal Study in Early-to-Middle
and Middle-to-Late Adolescence. Child Development 81, 1565-1581.
Meier, K. J. and Gill, J. (2000).
What Works: A New Approach to Program and Policy Analysis. Boulder,
CO: Westview Press.
Meier, K. J. and Keiser, L. R. (1996).
Public Administration as a Science of the Artiﬁcial: A Methodology
for Prescription. Public Administration Review 56, 459-466.
Meier, K. J., Polinard, J. L. and Wrinkle, R. (2000).
Bureaucracy and Organizational Performance:
Causality Arguments about Public Schools. American Journal of Political Science 44, 590-602.
Meier, K. J. and Smith, K. B. (1994).
Representative Democracy and Representative Bureaucracy. Social
Science Quarterly 75, 798-803.
Meilijson, I. (1989). A Fast Improvement to the EM Algorithm on Its Own Terms. Journal of the Royal
Statistical Society, Series B 51, 127-138.

References
623
Meng, X-L. (1994a). Posterior Predictive p-Values. Annals of Statistics 22, 1142-1160.
Meng, X-L. (1994b). On the Rate of Convergence of the ECM Algorithm. Annals of Statistics 22, 326-339.
Meng, X-L. (2000). Towards a More General Propp-Wilson Algorithm: Multistage Backward Coupling.
Monte Carlo Methods-Fields Institute Communications 26, 85-93.
Meng, X-L. and Pedlow, S. (1992).
EM: A Bibliographic Review With Missing Articles. Proceedings of the
Statistical Computing Section, American Statistical Association. Alexandria, VA: American Statistical
Association. 24-27.
Meng, X-L. and Rubin, D. B. (1991).
Using EM to Obtain Asymptotic Variance Covariance Matrices: the
SEM Algorithm. Journal of the American Statistical Association 86, 899-909.
Meng, X-L. and Rubin, D. B. (1993).
Maximum Likelihood Estimation via the ECM Algorithm: A General
Framework. Biometrika 80, 267-278.
Meng, X-L. and Schilling, S. (1996).
Fitting Full-Information Item Factor Models and an Empirical
Investigation of Bridge Sampling. Journal of the American Statistical Association 91, 1254-1267.
Meng, X-L. and van Dyk, D. A. (1999).
Seeking Eﬃcient Data Augmentation Schemes Via Conditional
and Marginal Augmentation. Biometrika 86, 301-320,
Mengersen, K. L. and Robert, C. P. (1996).
Testing for Mixtures: A Bayesian Entropic Approach. In
Bayesian Statistics 5, J. O. Berger, J. M. Bernardo, A. P. Dawid, D. V. Lindley and A. F. M. Smith
(eds.).
Oxford: Oxford University Press, pp.255-276.
Mengersen, K. L. and Tweedie, R. L. (1996).
Rates of Convergence of the Hastings and Metropolis
Algorithms. Annals of Statistics 24, 101-121.
Mengersen, K. L., Robert, C. P. and Guihenneuc-Jouyaux, C. (1999).
MCMC Convergence Diagnostics:
A Reviewww. In Bayesian Statistics 6, J. O. Berger, J. M. Bernardo, A. P. Dawid, D. V. Lindley and
A. F. M. Smith (eds.).
Oxford: Oxford University Press, pp.415-440.
Menzefricke, U. (1981). A Bayesian Analysis of a Change in the Precision of a Sequence of Independent
Normal Random Variables at an Unknown Time Point. Applied Statistics 30, 141-146.
Metropolis, N. and Ulam, S. (1949).
The Monte Carlo Method.
Journal of the American Statistical
Association 44, 335-341.
Metropolis, N., Rosenbluth, A. W., Rosenbluth, M. N., Teller, A. H. and Teller E. (1953).
Equation of
State Calculations by Fast Computing Machines. Journal of Chemical Physics 21, 1087-1091.
Meyer, K. R. and Hall, G. R. Jr. (1992).
Introduction to Hamiltonian Dynamical Systems and the N-Body
Problem. New York: Springer-Verlag.
Meyn, S. P. and Tweedie, R. L. (1993).
Markov Chains and Stochastic Stability. New York: Springer-
Verlag.
Meyn, S. P. and Tweedie, R. L. (1994).
Computable Bounds for Convergence Rates of Markov Chains.
Annals of Applied Probability 4, 981-1011.
Mihram, G. A. and Mihram, D. (1997).
A Review and Update on Pseudo-Random Number Generation, on
Seeding, and on a Source of Seeds. ASA Proceedings of the Statistical Computing Section. Alexandria,
VA: American Statistical Association. 115-119.
Miller, A. (2002). Subset Selection in Regression. Second Edition. Boca Raton: Chapman & Hall/CRC.
Mira, A. and Tierney, L. (2001a).
On the Use of Auxiliary Variables in Markov Chain Monte Carlo
Sampling. Scandanavian Journal of Statistics 29, 1-12.
Mira, A. and Tierney, L. (2001b).
Eﬃciency and Convergence Properties of Slice Samplers. Scandinavian
Journal of Statistics 29, 1035-53.
Mitchell, T. J. and Beauchamp, J. J. (1988).
Bayesian Variable Selection in Linear Regression. Journal of
the American Statistical Association 83, 1023-1032.
Mitra, D., Romeo, F. and Sangiovanni-Vincentelli, A. L. (1986).
Convergence and Finite-Time Behavior
of Simulated Annealing. Advances in Applied Probability 18, 747-771.

624
References
Mkhadri, A. (1998). On the Rate of Convergence of the ECME Algorithm. Statistics & Probability Letters
37, 81-87.
Møller, J. and Schladitz, K. (1999).
Extensions of Fill’s Algorithm for Perfect Simulation. Journal of the
Royal Statistical Society, Series B 61, 955-969.
Monahan, J. F. and Genz, A. (1996).
A Comparison of Omnibus Methods for Bayesian Computation.
Computing Science and Statistics 27, 471-480.
Monahan, J. F. (2001). Numerical Methods of Statistics. Cambridge: Cambridge University Press.
Montgomery, J. M. and Nyhan, B. (2010).
Bayesian Model Averaging: Theoretical Developments and
Practical Applications. Political Analysis 18, 245-270.
Mooney, C. Z. (1997). Monte Carlo Simulation. Thousand Oaks, CA: Sage.
Moore, D. S. and McCabe, G. P. (1989). Introduction to the Practice of Statistics. WH Freeman/Times
Books/Henry Holt & Company.
Moran, J. L. and Solomon, P. J. (2004).
A Farewell To p-values? Critical Care and Resuscitation Journal
6, 130.
Moreno, E. (1997). Bayes Factors for Intrinsic and Fractional Priors in Nested Models. In L1-Statistical
Procedures and Related Topics. Y. Dodge (ed.). Hayward, CA: Institute of Mathematical Statistics
Monograph Series, pp.257-270.
Moreno, E. (2000). Global Bayesian Robustness for Some Classes of Prior Distributions. In Robust Bayesian
Analysis, David R´ıos Insua and Fabrizio Ruggeri (eds.).
New York: Springer-Verlag, pp.45-70.
Moreno, E. (2005). Objective Bayesian Methods for One-Sided Testing. Test 14, 181-198.
Moreno, E. and Cano, J. A. (1991).
Robust Bayesian Analysis with ϵ-Contaminations Partially Known.
Journal of the Royal Statistical Society, Series B 53, 143-155.
Moreno, E. and Gonz´alez, A. (1990).
Empirical Bayes Analysis of ϵ-Contaminated Classes of Prior
Distributions. Brazilian Journal of Probability and Statistics 4, 177-200.
Moreno, E. and Pericchi, L. R. (1991). Robust Bayesian Analysis for ϵ- Contaminations with Shape and
Quantile Restraints. In Proceedings of the Fifth International Symposium on Applied Stochastic Models,
R. Guti´eterrez and M. Valderrama (eds.).
Singapore: World Scientiﬁc, pp.454-470.
Moreno, E. and Pericchi, L. R. (1993). Bayesian Robustness for Hierarchical, ϵ- Contamination Models.
Journal of Statistical Planning and Inference 37, 159-168.
Moreno, E., Mart´ınez, C. and Cano, J. A. (1996). Local Robustness and Inﬂuences for Contamination
Classes of Prior Distributions. In Bayesian Robustness, J. O. Berger, B. Betr´o, E. Moreno, L. R.
Pericchi, F. Ruggeri, G. Salinetti and L. Wasserman (eds.). Hayward, CA: Institute of Mathematical
Statistics Monograph Series 29, pp.139-156.
Morey, R. D., Rouder, J. N., Pratte, M. S. and Speckman, P. L. (2011).
Using MCMC Chain Outputs to
Eﬃciently Estimate Bayes Factors. Journal of Mathematical Psychology 55, 368-378.
Morgan, B. J. T. (1984). Elements of Simulation. London: Chapman & Hall.
Morris, C. N. (1982). Natural Exponential Families with Quadratic Variance Functions. Annals of Statistics
10, 65-80.
Morris, C. N. (1983a). Natural Exponential Families with Quadratic Variance Functions: Statistical Theory.
Annals of Statistics 11, 515-529.
Morris, C. N. (1983b). Parametric Empirical Bayes Inference: Theory and Applications. Journal of the
American Statistical Association 78, 47-65.
Morrison, D. E. and Henkel, R. E. (1969). Statistical Tests Reconsidered. The American Sociologist 4,
131-140.
Morrison, D. E. and Henkel, R. E. (1970). The Signiﬁcance Test Controversy–A Reader. Chicago: Aldine.
Mouchart, M. and Simar, L. (1984). A Note on Least-Squares Approximation in the Bayesian Analysis of
Regression Models. Journal of the Royal Statistical Society, Series B 46, 124-133.

References
625
M¨uller, M. E. (1958). An Inverse Method for the Generation of Random Normal Deviates on Large Scale
Computers. Mathematical Tables and Other Aids to Computation 12, 167-174.
M¨uller, M. E. (1959a). A Comparison of Methods for Generating Normal Deviates on Digital Computers.
Journal of the Association for Computing Machinery 6, 376-383.
M¨uller, M. E. (1959b). A Note on a Method for Generating Points Uniformly on N-dimensional Spheres.
Communications of the Association for Computing Machinery 2, 19-20.
Murata, N., Yoshizawa, S. and Amari, S. (1994). Network Information Criterion-Determining the Number
of Hidden Units for Artiﬁcial Neural Network Models. IEEE Transactions on Neural Networks 5, 865-
872.
Murdoch, D. J. and Green, P. J. (1998). Exact Sampling From a Continuous State Space. Scandinavian
Journal of Statistics 25, 483-502.
Murdoch, D. J. and Rosenthal, J. S. (1998). An Extension of Fill’s Exact Sampling Algorithm to Non-
Monotone Chains. Technical Report, http://www.probability.ca/jeff/ftpdir/fill.pdf.
Murdoch, D. J. and Takahara, G. (2006). Perfect Sampling for Queues and Network Models. ACM Trans-
actions on Modeling and Computer Simulation 16, 76-92.
Murray, G. D. (1977). Contribution to Discussion of Paper by A. P. Dempster, N. M. Laird and D. B.
Rubin. Journal of the Royal Statistical Society, Series B 39, 27-28.
Muth´en, B. and Satorra, A. (1995). Complex Sample Data in Structural Equation Modeling. Sociological
Methodology 25, 87-99.
Mykland, P., Tierney, L. and Yu, B. (1995). Regeneration in Markov Chain Samplers. Journal of the
American Statistical Association 90, 233-241.
Nance, R. E. and Overstreet, C. (1972). A Bibliography on Random Number Generation. ACM Computing
Reviews 13, 495-508.
Nandram, B. and Chen, M-H. (1996). Reparameterizing the Generalized Linear Model to Accelerate Gibbs
Sampler Convergence. Journal of Statistical Computing and Simulation 54, 129-144.
Natarajan, R. and Kass, R. E. (2000). Reference Bayesian Methods for Generalized Linear Mixed Models.
Journal of the American Statistical Association 95, 227-237.
Natarajan, R. and McCulloch, C. E. (1995). A Note on the Existence of the Posterior Distribution for a
Class of Mixed Models for Binomial Responses. Biometrika 82, 639-643.
Naylor, J. C. and Smith, A. F. M. (1982). Applications of a Method for the Eﬃcient Computation of
Posterior Distributions. Applied Statistics 31, 214-225.
Naylor, J. C. and Smith, A. F. M. (1983). A Contamination Model in Clinical Chemistry: An Illustration
of a Method for the Eﬃcient Computation of Posterior Distributions. The Statistician 32, 82-87.
Neal, R. M. (2011). MCMC Using Hamiltonian Dynamics. In Handbook of Markov Chain Monte Carlo,
Steve Brooks, Andrew Gelman, Galin L. Jones and Xiao-Li Meng (eds.). Boca Raton: Chapman &
Hall/CRC, pp.113-162.
Neal, R. M. (1996). Sampling from Multimodal Distributions Using Tempered Transitions. Statistics and
Computing 4, 353-66.
Neal, R. M. (2001). Annealed Importance Sampling. Statistics and Computing 11, 125-139.
Neal, R. M. (2003). Slice Sampling. Annals of Statistics 31, 705-767.
Neal, R. M. (1993). “Bayesian Training of Backpropagation Networks by the Hybrid Monte Carlo Method.”
Technical Report CRG-TR-92-1, Department of Computer Science, University of Toronto.
Neave, H. R. (1973). On Using the Box-M¨uller Transformation with Multiplicative Congruential Pseudo-
Random Number Generators. Applied Statistics 22, 92-97.
Neft¸ci, S. N. (1982). Speciﬁcation of Economic Time Series Models Using Akaike’s Criterion. Journal of the
American Statistical Association 77, 537-540.

626
References
Nelder, J. A. (1977). Comments on “Maximum Likelihood From Incomplete Data Via the EM Algorithm.”
Journal of the Royal Statistical Society, Series B 39, 23-24.
Nelder, J. A. (1985). Quasi-likelihood and GLIM. Lecture Notes in Statistics 32, 120-127.
Nelder, J. A. and Lee, Y. (1992). Likelihood, Quasi-likelihood and Pseudo-likelihood: Some Comparisons.
Journal of the Royal Statistical Society, Series B 54, 273-284.
Nelder, J. A. and Pregibon, D. (1987). An Extended Quasi-Likelihood Function. Biometrika 74, 221-232.
Nelder, J. A. and Wedderburn, R. W. M. (1972). Generalized Linear Models. Journal of the Royal Statis-
tical Society, Series A 135, 370-385.
Nevzorov, V. B. and Zhukova, E. E. (1996). Wiener Process and Order Statistics. Journal of Applied
Statistical Science 3, 317-323.
Newcomb, S. (1886). A Generalized Theory of the Combination of Observations So As to Obtain the Best
Results. American Journal of Mathematics 8, 343-366.
Newton, M. A. and Raftery, A. E. (1994). Approximate Bayesian Inference with the Weighted Likelihood
Bootstrap. Journal of the Royal Statistical Society, Series B 56, 3-48.
Neyens, T., Faes, C. and Molenberghs, G. (2012). A Generalized Poisson-Gamma Model For Spatially
Overdispersed Data. Spatial and Spatio-Temporal Epidemiology, 3) 185-194.
Neyman, J. and Pearson, E. S. (1928a). On the Use and Interpretation of Certain Test Criteria for Purposes
of Statistical Inference. Part I. Biometrika 20A, 175-240.
Neyman, J. and Pearson, E. S. (1928b). On the Use and Interpretation of Certain Test Criteria for Purposes
of Statistical Inference. Part II. Biometrika 20A, 263-294.
Neyman, J. and Pearson, E. S. (1933a). On the Problem of the Most Eﬃcient Test of Statistical Hypotheses.
Philosophical Transactions of the Royal Statistical Society, Series A 231, 289-337.
Neyman, J. and Pearson, E. S. (1933b). The Testing of Statistical Hypotheses in Relation to Probabilities.
Proceedings of the Cambridge Philosophical Society 24. 492-510.
Neyman, J. and Pearson, E. S. (1936a). Contributions to the Theory of Testing Statistical Hypotheses.
Statistical Research Memorandum 1, 1-37.
Neyman, J. and Pearson, E. S. (1936b). Suﬃcient Statistics and Uniformly Most Powerful Tests of Statis-
tical Hypotheses. Statistical Research Memorandum 1, 113-137.
Ni, S. and Sun, D. (2003). Noninformative Priors and Frequentist Risks of Bayesian Estimators in Vector
Autoregressive Models. Journal of Econometrics 115, 159-197.
Nickerson, R.S. (2000). Null Hypothesis Signiﬁcance Testing: A Review of an Old and Continuing Contro-
versy. Psychological Methods 5, 241-301.
Nicolaou, A. (1993). Bayesian Intervals With Good Frequentist Behaviour in the Presence of Nuisance
Parameters. Journal of the Royal Statistical Society, Series B 55, 377-390.
Niederreiter, H. (1972). On the Distribution of Pseudo-Random Numbers Generated by the Linear Congru-
ential Method. Mathematics of Computation 26, 793-795.
Niederreiter, H. (1974). On the Distribution of Pseudo-Random Numbers Generated by the Linear Congru-
ential Method, II. Mathematics of Computation 28, 1117-1132.
Niederreiter, H. (1976). On the Distribution of Pseudo-Random Numbers Generated by the Linear Congru-
ential Method, III. Mathematics of Computation 30, 571-597.
Nigm, A. M. and Handy, H. I. (1987). Bayesian Prediction Bounds for the Pareto Lifetime Model. Com-
munications in Statistics 16, 1761-1722.
Norrander, B. (2000). The Multi-Layered Impact of Public Opinion on Capital Punishment Implementation
in the American States. Political Research Quarterly 53, 771-793.
Norris, J. R. (1997). Markov Chains. Cambridge: Cambridge University Press.
Novick, M. R. (1969). Multiparameter Bayesian Indiﬀerence Procedures. Journal of the Royal Statistical
Society, Series B 31, 29-64.

References
627
Novick, M. R., Isaacs, G. L. and DeKeyrel, D. F. (1976). CADA User’s Manual-1976. Iowa City: Iowa
Testing Programs, The University of Iowa.
Novick, M. R. and Hall, W. J. (1965). A Bayesian Indiﬀerence Procedure. Journal of the American Sta-
tistical Association 60, 1104-1117.
Ntzoufras, I. (2009). Bayesian Modeling Using WinBUGS. New York: John Wiley & Sons.
Nummelin, E. (1984). General Irreducible Markov Chains and Non-negative Operators. Cambridge: Cam-
bridge University Press.
Oakes, M. (1986). Statistical Inference: A Commentary for the Social and Behavioral Sciences. New York:
John Wiley & Sons.
O’Hagan, A. (1994). Kendall’s Advanced Theory of Statistics: Volume 2B, Bayesian Inference. London:
Arnold.
O’Hagan, A. (1995). Fractional Bayes Factors for Model Comparison. Journal of the Royal Statistical So-
ciety, Series B 57, 99-138.
O’Hagan, A. (1998). Eliciting Expert Beliefs in Substantial Practical Applications. The Statistician 47,
21-35.
O’Hagan, A. and Berger, J. O. (1988). Ranges of Posterior Probabilities for Quasiunimodal Priors With
Speciﬁed Quantiles. Journal of the American Statistical Association 83, 503-508.
Oh, M.-S. and Berger, J. O. (1992). Adaptive Importance Sampling in Monte Carlo Integration. Journal
of Statistical Computation and Simulation 41, 143-168.
Olivera, S. and Gill, J. (2011). Parallel Gibbs Sampling with snowfall. The Political Methodologist 19,
4-7.
Orans, M. (1996). Not Even Wrong: Margaret Mead, Derek Freeman, and the Samoans. Novato, CA:
Chandler & Sharp.
Orchard, T. and Woodbury, M. A. (1972). A Missing Information Principle: Theory and Applications.
Proceedings of the 6th Berkeley Symposium on Mathematical Statistics and Probability 1. 697-715.
Orey, S. (1961). Strong Ratio Limit Property. Bulletin of the American Mathematical Society 67, 571-574.
Orton, C. (1997). Testing Signiﬁcance or Testing Credulity? Oxford Journal of Archaeology 16, 219.
Pagan, A. (1987). Three Econometric Methodologies: A Critical Appraisal. Journal of Economic Surveys
1, 3-24.
Pang, X. (2010). Modeling Heterogeneity and Serial Correlation in Binary Time-Series Cross-sectional Data:
A Bayesian Multilevel Model with AR(p) Errors. Political Analysis 18, 470-498.
Pang, X. and Gill, J. (2012). Spike and Slab Prior Distributions for Simultaneous Bayesian Hypothesis
Testing, Model Selection, and Prediction, of Nonlinear Outcomes. Technical Report,
http://jgill.wustl.edu/research/current.html.
Pasta, J. and Ulam, S. (1953). Heuristic Studies in Problems of Mathematical Physics on High Speed
Computing Machines. Technical Report, Los Alamos Scientiﬁc Lab.
Patterson, R. L. and Richardson, W. (1963). A Decision Theoretic Model for Determining Veriﬁcation
Requirements. The Journal of Conﬂict Resolution 7, 603-607.
Patterson, T. N. L. (1968). The Optimum Addition of Points to Quadrature Formulae. Mathematics of
Computation 23, 847-856.
Payne, W. H. (1977). Normal Random Numbers: Using Machine Analysis to Choose the Best Algorithm.
ACM Transactions on Mathematical Software 3, 346-358.
Peach, C. (1997). Postwar Migration to Europe: Reﬂux, Inﬂux, Refuge. Social Science Quarterly 78, 269-
283.
Pearson, K. (1892). The Grammar of Science. London: Walter Scott.
Pearson, K. (1900). On the Criterion that a Given System of Deviations from the Probable in the Case of

628
References
a Correlated System of Variables is Such That It Can Reasonably be Supposed to Have Arisen From
Random Sampling. Philosophical Magazine, 5th Series, 50, 157-175.
Pearson, K. (1907). On the Inﬂuence of Past Experience on Future Expectation. Philosophical Magazine,
6th Series, 13, 365-378.
Pearson, K. (1914). On the Probability that Two Independent Distributions of Frequency are Really Samples
of the Same Population, with Special Reference to Recent Work on the Identity of Trypanosome Strains.
Biometrika 10, 85-143.
Pearson, K. (1920). The Fundamental Problem of Practical Statistics. Biometrika 13, 1-16.
Pennington, R. H. (1970). Introductory Computer Methods and Numerical Analysis. Second Edition. Lon-
don: Collier-MacMillan.
Pericchi, L. R. and Nazaret, W. (1988). On Being Imprecise at the Higher Levels of a Hierarchical Linear
Model. In Bayesian Statistics 3, J. M. Bernardo, M. H. DeGroot, D. V. Lindley and A. F. M. Smith
(ed.).
Oxford: Oxford University Press, pp.569-577.
Perkins, W. C. and Menzefricke, U. (1975). A Better Additive Congruential Random Number Generator?
Decision Sciences 6, 194-198.
Perks, W. (1947). Some Observations on Inverse Probability Including A New Indiﬀerence Rule. Journal of
the Institute of Actuaries 73, 285-312.
Peskun, P. H. (1973). Optimum Monte Carlo Sampling Using Markov Chains. Biometrika 60, 607-612.
Petrone, S. and Raftery, A. E. (1997). A Note on the Dirichlet Process Prior in Bayesian Nonparametric
Inference With Partial Exchangeability. Statistics & Probability Letters 36, 69-83.
Pettit, L. I. (1992). Bayes Factors for Outlier Models Using the Device of Imaginary Observations. Journal
of the American Statistical Association 87, 541-545.
Pettit, L. I. and Smith, A. F. M. (1985) Outliers and Inﬂuential Observations in Linear Models. In Bayesian
Statistics 2, J. M. Bernardo, M. H. DeGroot, D. V. Lindley and A. F. M. Smith (eds.).
Amsterdam:
North Holland Press, pp.473-494.
Pettitt, A. N., Tran, T. T., Haynes, M. A. and Hay, J. L. (2006).
A Bayesian Hierarchical Model for
Categorical Longitudinal Data From a Social Survey of Immigrants. Journal of the Royal Statistical
Society, Series A 169, 97-114.
Phillips, D. B. and Smith, A. F. M. (1996). Bayesian Model Comparison Via Jump Diﬀusions. In Markov
Chain Monte Carlo in Practice, W. R. Gilks, S. Richardson and D. J. Spiegelhalter (eds.). New York:
Chapman & Hall, pp.214-240.
Phillips, D. T. and Beightler, C. S. (1972). Procedure for Generating Gamma Variates with Non-Integer
Parameter Sets. Journal of Statistical Computation and Simulation 1, 197-208.
Phillips, P. C. B. (1991). To Criticize the Critics: An Objective Bayesian Analysis of Stochastic Trends.
Journal of Applied Econometrics 6, 333-364.
Phillips, P. C. B. (1995). Bayesian model selection and prediction with empirical applications. Journal of
Econometrics 69, 289-331.
Pitman, E. J. G. (1936). Suﬃcient Statistics and Intrinsic Accuracy. Proceedings of the Cambridge Philo-
sophical Society 32. 567-579.
Pizer, S. M. (1975). Numerical Computing and Mathematical Analysis. Chicago:
Science Research Associates.
Placket, R. L. (1966). Current Trends in Statistical Inference. Journal of the Royal Statistical Society,
Series A 129, 249-267.
Poirer, D. J. (1988). Frequentist and Subjectivist Perspectives on the Problems of Model Building in Eco-
nomics. Journal of Economic Perspectives 2, 121-144.
Poirer, D. J. (1994). Jeﬀreys Prior for Logit Models. Journal of Econometrics 63, 327-339.

References
629
Polasek, W. (1984). Regression Diagnostics for General Linear Regression Models. Journal of the American
Statistical Association 79, 336-340.
Polasek, W. (1987). Bounds on Rounding Errors in Linear Regression Models. The Statistician 36, 221-227.
Pole, A., West, M. and Harrison, J. (1994). Applied Bayesian Forecasting and Time Series Analysis. New
York: Chapman & Hall.
Pollard, W. E. (1986). Bayesian Statistics for Evaluation Research. Thousand Oaks, CA: Sage.
Pollard, P. (1993). How Signiﬁcant is ‘Signiﬁcance’? In A Handbook for Data Analysis in the Behavioral
Sciences: Methodological Issues, G. Keren and C. Lewis (eds.). Hillsdale, NJ: Lawrence Erlbaum As-
sociates, pp.448-460.
Pollard, P. and Richardson, J. T. E. (1987). On the Probability of Making Type One Errors. Psychological
Bulletin 102, 159-163.
Polson, N. G. (1996). Convergence of Markov Chain Monte Carlo Algorithms. In Bayesian Statistics 5, J.
O. Berger, J. M. Bernardo, A. P. Dawid, D. V. Lindley and A. F. M. Smith (eds.).
Oxford: Oxford
University Press.
Polson, N. G. and Scott, J. G. (2012). On the Half-Cauchy Prior For a Global Scale Parameter. Bayesian
Analysis 7, 887-902.
Popper, K. (1968). The Logic of Scientiﬁc Discovery. New York: Harper and Row.
Pratt, J. W. (1965). Bayesian Interpretation of Standard Inference Statements. With Discussion. Journal
of the Royal Statistical Society, Series B 27, 169-203.
Prentice, M. J. and Miller, J. C. P. (1968). Additive Congruential Pseudo-Random Number Generators.
Computer Journal 11, 341-346.
Press, S. J. (1989). Bayesian Statistics: Principles, Models, and Applications. New York: John Wiley &
Sons.
Press, S. J. and Tanur, J. M. (2001). The Subjectivity of Scientists and the Bayesian Approach. New York:
John Wiley & Sons.
Press, W. H., Flannery, B. P., Teukolsky, S. A. and Vetterling, W. T. (1986).
Numerical Recipes: The Art
of Scientiﬁc Computing. Cambridge: Cambridge University Press.
Priestley, M. B. (1981). Spectral Analysis and Time Series: Volumes I and II. San Diego: Academic Press.
Propp, J. G. and Wilson, D. B. (1996). Exact Sampling with Coupled Markov Chains and Applications to
Statistical Mechanics. Random Structures and Algorithms 9, 223-252.
Quinn, K. M., Martin, A. and Whitford, A. B. (1999). Voter Choice in Multi-Party Democracies: A Test
of Competing Theories and Models. American Journal of Political Science 43, 1231-1247.
Racine, A., Grieve, A. P., Fl¨uhler, H. and Smith, A. F. M. (1986).
Bayesian Methods in Practice: Expe-
riences in the Pharmaceutical Industry. Applied Statistics 45, 275-309.
Raftery, A. E. (1995). Bayesian Model Selection in Social Research. Sociological Methodology 25, 111-164.
Raftery, A. E. (1996). Hypothesis Testing and Model Selection. In Markov Chain Monte Carlo in Practice,
W. R. Gilks, S. Richardson and D. J. Spiegelhalter (eds.). New York: Chapman & Hall, pp.163-188.
Raftery, A. E. (1999). Bayesian Model Selection in Social Research. Sociological Methodology 25, 111-163.
Raftery, A. E. and Adman, V. E. (1986). Bayesian Analysis of a Poisson Process with a Change-Point.
Biometrika 73, 85-89.
Raftery, A. E. and Banﬁeld, J. D. (1991). Stopping the Gibbs Sampler, the Use of Morphology, and Other
Issues in Spatial Statistics. Annals of the Institute of Statistical Mathematics 43, 32-43.
Raftery, A. E. and Lewis, S. M. (1992). How Many Iterations in the Gibbs Sampler? In Bayesian Statistics
4, J. M. Bernardo, A. F. M. Smith, A. P. Dawid and J. O. Berger (eds.).
Oxford: Oxford University
Press, pp.763-773.
Raftery, A. E. and Lewis, S. M. (1996). Implementing MCMC. In Markov Chain Monte Carlo in Practice,
W. R. Gilks, S. Richardson and D. J. Spiegelhalter (eds.). New York: Chapman & Hall, pp.115-130.

630
References
Raiﬀa, H. and Schlaifer, R. (1961). Applied Statistical Decision Theory. Cambridge: Harvard School of
Business Administration.
Ramsey, J. O. and Novick, M. R. (1980). PLU Robust Bayesian Decision Theory: Point Estimation. Jour-
nal of the American Statistical Association 75, 901-907.
Rao, C. R. and Toutenburg, H. (1995). Linear Models: Least Squares and Alternatives. New York: Springer-
Verlag.
Rathbun, S. L. and Black, B. (2006). Modeling and Spatial Prediction of Pre-Settlement Patterns of Forest
Distribution Using Witness Tree Data. Environmental and Ecological Statistics 13, 427-448.
Raudenbush, S. and Bryk, A. S. (1986). A Hierarchical Model for Studying School Eﬀects. Sociology of
Education 59, 1-17.
Ravishanker, N. and Dey, D. K. (2002). A First Course In Linear Model Theory. New York: Chapman &
Hall/CRC.
Regazzini, E. (1992). Concentration Comparisons Between Probability Measures. Sankhy¯a, Series B 54,
129-149.
Revuz, D. (1975). Markov Chains. Amsterdam: North-Holland.
Richardson, S. and Green, P. J. (1997). On Bayesian Analysis of Mixtures with an Unknown Number of
Components. Journal of the Royal Statistical Society, Series B 59, 731-732.
Richey, M. (2010). The Evolution of Markov Chain Monte Carlo Methods. The American Mathematical
Monthly 117, 383-413.
Ripley, B. D. (1979). Algorithm AS 137: Simulating Spatial Patterns: Dependent Samples from a Multi-
variate Density. Applied Statistics 28, 109-112.
Ripley, B. D. (1983). Computer Generation of Random Variables-A Tutorial. International Statistical Re-
view 51, 301-319.
Ritter, C. and Tanner, M. A. (1992). Facilitating the Gibbs Sampler: The Gibbs Stopper and the Griddy-
Gibbs Sampler. Journal of the American Statistical Association 87, 861-868.
Robbins, H. (1955). An Empirical Bayes Approach to Statistics. In Proceedings of the 3rd Berkeley Sympo-
sium on Mathematical Statistics and Probability 1, Berkeley: University of California Press. pp.157-164.
Robbins, H. (1964). The Empirical Bayes Approach to Statistical Decision Problems. Annals of Mathemat-
ical Statistics 35, 1-20.
Robbins, H. (1983). Some Thoughts on Empirical Bayes Estimation. Annals of Statistics 1, 713-723.
Robert, C. P. (1995). Convergence Control Methods for Markov Chain Monte Carlo Algorithms. Statistical
Science 10, 231-253.
Robert, C. P. (1996). Mixtures of Distributions: Inference and Estimation. In Markov Chain Monte Carlo
in Practice, W. R. Gilks, S. Richardson and D. J. Spiegelhalter (eds.). New York: Chapman & Hall,
pp.441-464.
Robert, C. P. (1997). Discussion of Richardson and Green’s Paper. Journal of the Royal Statistical Society,
Series B 59, 758-764.
Robert, C. P. (2001). The Bayesian Choice: A Decision Theoretic Motivation. Second Edition. New York:
Springer-Verlag.
Robert, C. P. and Casella, G. (1999). Monte Carlo Statistical Methods. First Edition. New York: Springer-
Verlag.
Robert, C. P. and Casella, G. (2004). Monte Carlo Statistical Methods. Second Edition. New York: Springer-
Verlag.
Robert, C. P. and Casella, G. (2011). A Short History of Markov Chain Monte Carlo: Subjective Recollec-
tions from Incomplete Data. Statistical Science 26, 102-115.
Robert, C. P. and Mengersen, K. L. (1999). Reparameterization Issues in Mixture Estimation and Their
Bearings on the Gibbs Sampler. Computational Statistics and Data Analysis 29, 325-343.

References
631
Robert, C. P. and Cellier, D. (1998). Convergence Control of MCMC Algorithms. In Discretization and
MCMC Convergence Assessment, Lecture Notes in Statistics, 135. Christian P. Robert (ed.). New
York: Springer-Verlag, pp.27-46.
Robert, C. P. and Richardson, S. (1998).
Markov Chain Monte Carlo Methods. In Discretization and
MCMC Convergence Assessment, Lecture Notes in Statistics, 135. Christian P. Robert (ed.). New
York: Springer-Verlag, pp.1-25.
Roberts, G. O. (1992). Convergence Diagnostics of the Gibbs Sampler. In Bayesian Statistics 4, J. M.
Bernardo, A. F. M. Smith, A. P. Dawid and J. O. Berger (eds.).
Oxford: Oxford University Press,
pp.775-782.
Roberts, G. O. (1994). Methods for Estimating L2 Convergence of Markov Chain Monte Carlo. In Bayesian
Analysis in Statistics and Econometrics: Essays in Honor of Arnold Zellner, D. Berry, K. Chaloner
and J. Geweke (eds.). New York: John Wiley & Sons, pp.373-384.
Roberts, G. O. and Polson, N. G. (1994). On the Geometric Convergence of the Gibbs Sampler. Journal
of the Royal Statistical Society, Series B 56, 377-384.
Roberts, G. O. and Rosenthal, J. S. (1998). Markov Chain Monte Carlo: Some Practical Implications of
Theoretical Results. Canadian Journal of Statistics 26, 5-32.
Roberts, G. O. and Rosenthal, J. S. (1999). Convergence of the Slice Sampler Markov Chains. Journal of
the Royal Statistical Society, Series B 61, 643-60.
Roberts, G. O. and Rosenthal, J. S. (2011). Quantitative Non-Geometric Convergence Bounds for Inde-
pendence Samplers. Methodology and Computing in Applied Probability 13, 391-403.
Roberts, G. O. and Sahu, S. K. (1997). Updating Schemes, Correlation Structure, Blocking and Parame-
terization for the Gibbs Sampler. Journal of the Royal Statistical Society, Series B 59, 291-307.
Roberts, G. O. and Smith, A. F. M. (1994). Simple Conditions for the Convergence of the Gibbs Sampler
and Metropolis-Hastings Algorithms. Stochastic Processes and their Applications 49, 207-216.
Roberts, G. O. and Tweedie, R. L. (1996). Geometric Convergence and Central Limit Theorems for Mul-
tidimensional Hastings and Metropolis Algorithms. Biometrika 83, 95-110.
Roberts, G. O. and Rosenthal, J. S. (1998b). Two Convergence Properties of Hybrid Samplers. Annals of
Applied Probability 8, 397-407.
Robinson, D. H. and Levin, J. R. (1997). Research News and Comment: Reﬂections On Statistical and
Substantive Signiﬁcance, With a Slice of Replication. Educational Researcher 26, 21-26.
Robinson, P. M. (1991). Consistent Nonparametric Entropy-Based Testing. The Review of Economic Stud-
ies 58, 437-453.
Roethlisberger, F. and Dickson, W. (1939). Management and the Worker. Cambridge: Cambridge Univer-
sity Press.
Romney, A. K. (1999). Culture Consensus as a Statistical Model. Current Anthropology 40 (Supplement),
S103-S115.
Rosay, A. B. and Herz, C. D. (2000). Diﬀerences in the Validity of Self-Reported Drug Use Across Five
Factors in Indianapolis, Fort Lauderdale, Phoenix, and Dallas, 1994. ICPSR02706-v1. Ann Arbor, MI:
Inter-university Consortium for Political and Social Research [distributor].
Rosenblatt, M. (1971). Markov Processes. Structure and Asymptotic Behavior. New York: Springer-Verlag.
Rosenkranz, R. D. (1977). Inference, Method, and Decision. Towards a Bayesian Philosophy of Science.
Dordrecht: Reidel.
Rosenthal, J. S. (1993). Rates of Convergence for Data Augmentation on Finite Sample Spaces. Annals of
Applied Probability 3, 819-839.
Rosenthal, J. S. (1995a). Minorization Conditions and Convergence Rates for Markov Chain Monte Carlo.
Journal of the American Statistical Association 90, 558-566.
Rosenthal, J. S. (1995b). Rates of Convergence for Gibbs Sampling for Variance Components Models.
Annals of Statistics 23, 740-61.

632
References
Rosenthal, J. S. (1995c). Convergence Rates for Markov Chains. SIAM Review 37, 387-405.
Rosenthal, J. S. (1996). Analysis of the Gibbs Sampler for a Model Related to James-Stein Estimators.
Statistics and Computing 6, 269-75.
Rosnow, R. L. and Rosenthal, J. S.. (1989). Statistical Procedures and the Justiﬁcation of Knowledge in
Psychological Science. American Psychologist 44, 1276-84.
Ross, S. (1996). Stochastic Processes. New York: Wiley & Sons.
Rousseeuw, P. J. and Leroy, A. M. (1987). Robust Regression and Outlier Detection. New York: John
Wiley & Sons.
Rozeboom, W. W. (1960). The Fallacy of the Null Hypothesis Signiﬁcance Test. Psychological Bulletin. 57,
416-428.
Rubin, D. B. (1980). Using Empirical Bayes Techniques in the Law School Validity Studies. Journal of the
American Statistical Association 75, 801-827.
Rubin, D. B. (1981). The Bayesian Bootstrap. Annals of Statistics 9, 130-134.
Rubin, D. B. (1984). Bayesianly Justiﬁable and Relevant Frequency Calculations for the Applied Statisti-
cian. Annals of Statistics 12, 1151-1172.
Rubin, D. B. (1987a). A Noniterative Sampling/Importance Resampling Alternative to the Data Augmen-
tation Algorithm for Creating a Few Imputations When Fractions of Missing Information Are Modest:
the SIR Algorithm. Discussion of Tanner & Wong (1987). Journal of the American Statistical Society
82, 543-546.
Rubin, D. B. (1987b). Multiple Imputation for Nonresponse in Surveys. New York: John Wiley & Sons.
Rubin, D. B. (1988). Using the SIR Algorithm to Simulate Posterior Distributions. In Bayesian Statistics 3,
J. M. Bernardo, M. H. DeGroot, D. V. Lindley and A. F. M. Smith (eds.).
Oxford: Oxford University
Press, pp.395-402.
Rubin, D. B. (1991). EM and Beyond. Psychometrika 56, 241-254.
Rubin, H. (1977). Robust Bayesian Estimation. In Statistical Decision Theory and Related Topics II, S. S.
Gupta and D. Moore (eds.). San Diego: Academic Press, pp.351-356.
Rubin, H. (1987). A Weak System of Axioms for “Rational” Behavior and the Nonseparability of Utility
From Prior. Statistical Decisions 5, 47-58.
Rubinstein, R. Y. (1981). Simulation and the Monte Carlo Method. New York: John Wiley & Sons.
Ruggeri, F. (1990). Posterior Ranges of Functions of Parameters under Priors with Speciﬁed Quantiles.
Communications in Statistics, Part A, Theory and Methods 19, 127-144.
Ruggeri, F. and Wasserman, L. (1993). Inﬁnitesimal Sensitivity of Posterior Distributions. Canadian Jour-
nal of Statistics 21, 195-203.
Russell, B. (1929). Mysticism and Logic and Other Essays. New York: W.W. Norton & Company.
Sagan, A. (2013). Market Research and Preference Data. In The Sage Handbook of Multilevel Modeling.
Marc A. Scott, Jeﬀrey S. Simonoﬀand Brian D. Marx (eds.). Thousand Oaks, CA: Sage Publications,
pp.581-598.
Sakia, R. M. (1992). The Box-Cox Transformation Technique: A Review. The Statistician 41, 169-178.
Sala-I-Martin, X. (1997). I Just Ran Two Million Regressions. The American Economic Review 87, 178-
183.
Samaniego, F. J. and Reneau, D. M. (1994). Toward a Reconciliation of the Bayesian and Frequentist
Approach to Point Estimation. Journal of the American Statistical Association 89, 947-957.
Sampson, R. J. and Raudenbush, S. W. (1999). Systematic Social Observation of Public Spaces: A New
Look at Disorder in Urban Neighborhoods. American Journal of Sociology 105, 603-651.
Sans´o, B. and Pericchi, L. R. (1992). Near Ignorance Classes of Log-Concave Priors for the Location Model.
TEST, 1, 39-46.

References
633
Satten, G. A. and Longini, I. M. (1996). Markov Chains With Measurement Errors: Estimating the True
Course of a Marker of the Progression of HIV Disease. Applied Statistics 45, 275-309.
Savage, L. J. (1954). The Foundations of Statistics. New York: Wiley.
Savage, L. J. (1962). The Foundations of Statistical Inference. London: Methuen.
Savage, L. J. (1971). Elicitation of Personal Probabilities and Expectations. Journal of the American Sta-
tistical Association 66, 783-801.
Savage, L. J. (1972). The Foundations of Statistics. New York: Dover Publications.
Sawa, T. (1978). Information Criteria for Discriminating among Alternative Regression Models. Economet-
rica 46, 1273-1291.
Schafer, J. L. (1997). Analysis of Incomplete Multivariate Data. London: Chapman & Hall.
Schervish, M. J. (1995). Theory of Statistics. New York: Springer-Verlag.
Schervish, M. J. (1996). P values: What They Are and What They Are Not. The American Statistician
50, 203-206.
Schervish, M. J. and Carlin, B. P. (1992). On the Convergence of Successive Substitution Sampling. Journal
of Computational and Graphical Statistics 1, 111-127.
Scheuer, E. M. and Stoller, D. S. (1962). On the Generation of Normal Random Vectors. Technometrics
4, 278-281.
Schmidt, F. L. (1996). Statistical Signiﬁcance Testing and Cumulative Knowledge in Psychology: Implica-
tions for the Training of Researchers. Psychological Methods 1, 115-129.
Schmidt, F. L. and Hunter, J. E. (1977). Development of a General Solution to the Problem of Validity
Generalization. Journal of Applied Psychology 62, 529-40.
Schruben, L. W. (1982). Detecting Initialization Bias in Simulation Output. Operations Research 30, 569-
590.
Schruben, L. W., Singh, H. and Tierney, L. (1983). Optimal Tests for Initialization Bias in Simulation
Output. Operations Research 31, 1167-1178.
Schwarz, G. (1978). Estimating the Dimension of a Model. Annals of Statistics 6, 461-464.
Scollnik, D. P. M. (2001). Actuarial Modeling with MCMC and BUGS. North American Actuarial Journal
5, 95-124.
Scott, David W. (1985). Average Shifted Histograms: Eﬀect Nonparametric Density Estimators in Several
Dimensions. Annals of Statistics 13, 1024-1040.
Sedlmeier, P. and Gigerenzer, G. (1989). Do Studies of Statistical Power Have an Eﬀect on the Power of
Studies? Psychological Bulletin 105, 309-15.
Seewald, W. (1992). Discussion of Hills and Smith (1992). In Bayesian Statistics 4, J. M. Bernardo, A. F.
M. Smith, A. P. Dawid and J. O. Berger (eds.).
Oxford: Oxford University Press, pp.241-243.
Seidenfeld, T. (1985). Calibration, Coherence, and Scoring Rules. Philosophy of Science 52, 274-294.
Seidenfeld, T., Schervish, M. J. and Kadane, J. B. (1995). A Representation of Partially Ordered Prefer-
ences. The Annals of Statistics 23, 2168-2217.
Seltzer, M. H., Wong, W. H. and Bryk, A. S. (1996). Bayesian Analysis in Applications of Hierarchical
Models: Issues and Methods. Journal of Educational and Behavioral Statistics 21, 131-167.
Selvin, S. (1975). On the Monty Hall problem. (Letter To the Editor.) The American Statistician 29, 134.
Sened, I. and Schoﬁeld, N. (2005). Multiparty Competition in Israel, 1988-96 British Journal of Political
Science 35, 635-663.
Serlin, R. C. and Lapsley, D. K. (1993). Rational Appraisal of Psychological Research and the Good-
Enough Principle. In A Handbook for Data Analysis in the Behavioral Sciences: Methodological Issues,
G. Keren and C. Lewis (eds.). Hillsdale, NJ: Lawrence Erlbaum Associates, pp.199-228.
Severini, T. A. (1991). On the Relationship Between Bayesian and Non-Bayesian Interval Estimates. Journal
of the Royal Statistical Society, Series B 53, 611-618.

634
References
Severini, T. A. (1993). Bayesian Interval Estimates Which Are Also Conﬁdence Intervals. Journal of the
Royal Statistical Society, Series B 55, 533-540.
Shafer, G. (1982). Lindley’s Paradox. Journal of the American Statistical Association 77, 325-351.
Shannon, C. (1948). A Mathematical Theory of Communication. Bell System Technology Journal 27, 379-
423, and 623-56.
Shao, J. (1989). Monte Carlo Approximations in Bayesian Decision Theory. Journal of the American Sta-
tistical Association 84, 727-732.
Shao, J. (2005). Mathematical Statistics. Second Edition. New York: Springer-Verlag.
Shao, J. and Tu, D. (1995). The Jackknife and Bootstrap. New York: Springer-Verlag.
Sharp, A. M., Register, C. A. and Grimes, P. W. (1999). Economics of Social Issues. Fifteenth Edition.
New York: McGraw-Hill.
Shaw, J. E. H. (1988). A Quasirandom Approach To Integration in Bayesian Statistics. Annals of Statistics
16, 895-914.
Sheynin, O. B. (1977). Laplace’s Theory of Errors. Archive for History of Exact Sciences 17, 1-61.
Shyamalkumar, N. D. (2000). Likelihood Robustness. In Robust Bayesian Analysis, David R´ıos Insua and
Fabrizio Ruggeri (eds.). New York: Springer-Verlag, pp.127-144.
Sibuya, M. (1961). Exponential and Other Variable Generators. Annals of the Institute of Statistical Math-
ematics 13, 231-237.
Siegmund, D. (1976). Importance Sampling in the Monte Carlo Study of Sequential Tests. Annals of Statis-
tics 4, 673-684.
Silvapulle, M. J. (1981). On the Existence of Maximum Likelihood Estimates for the Binomial Response
Models. Journal of the Royal Statistical Society, Series B 43, 310-313.
Sims, C. A. (1988). Bayesian Skepticism on Unit Root Econometrics. Journal of Economic Dynamics and
Control 12, 463-474.
Sims, C. A. and Uhlig, H. (1991). Understanding Unit Rooters: A Helicopter Tour. Econometrica 59,
1591-1599.
Sinclair, A. J. and Jerrum, M. R. (1988). Conductance and the Rapid Mixing Property for Markov Chains:
The Approximation of the Permanent Resolved. Proceedings of the 20th Annual ACM Symposium on
the Theory of Computing, 235-44.
Sinclair, A. J. and Jerrum, M. R. (1989). Approximate Counting, Uniform Generation and Rapidly Mixing
Markov Chains. Information and Computation 82, 93-133.
Singleton, R., Jr. and Straight, B. C. (2004). Approaches to Social Research. Fourth Edition. New York:
Oxford University Press.
Sivaganesan, S. (1993). Robust Bayesian Diagnostics. Journal of Statistical Planning and Inference 35,
171-188.
Sivaganesan, S. (2000). Global and Local Robustness Approaches: Uses and Limitations. In Robust Bayesian
Analysis, David R´ıos Insua and Fabrizio Ruggeri (eds.). New York: Springer-Verlag, pp.89-108.
Sivaganesan, S. and Berger, J. O. (1989). Ranges of Posterior Measures for Priors with Unimodal Contam-
inations. Annals of Statistics 17, 868-889.
Skates, S. J., Pauler, D. K. and Jacobs, I. J. (2001). Screening Based on the Risk of Cancer Calculation
from Bayesian Hierarchical Changepoint and Mixture Models of Longitudinal Markers. Journal of the
American Statistical Association 96, 429-439.
Skene, A. M. (1983). Computing Marginal Distributions for the Dispersion Parameters of Analysis of Vari-
ance Models. The Statistician 32, 99-108.
Skene, A. M., Shaw, E. H. and Lee, T. D. (1986). Bayesian Modeling and Sensitivity Analysis. The Statis-
tician 35, 281-288.
Skinner, B. F. (1953). Science and Human Behavior. Toronto: Macmillan.

References
635
Smith, A. F. M. (1973). A General Bayesian Linear Model. Journal of the Royal Statistical Society, Series
B 35, 61-75.
Smith, A. F. M. (1975). A Bayesian Approach to Inference About a Change-point in a Sequence of Random
Variables. Biometrika 62, 407-416.
Smith, A. F. M. (1984). Present Position and Potential Developments: Some Personal Views: Bayesian
Statistics. Journal of the Royal Statistical Society, Series A 147, 245-259.
Smith, A. F. M. (1986). Some Bayesian Thoughts on Modelling and Model Choice. The Statistician 35,
97-101.
Smith, A. F. M. and Roberts, G. O. (1993). Bayesian Computation via the Gibbs Sampler and Related
Markov Chain Monte Carlo Methods. Journal of the Royal Statistical Society, Series B 55, 3-24.
Smith, A. F. M. and Spiegelhalter, D. J. (1980). Bayes Factors and Choice Criteria for Linear Models.
Journal of the Royal Statistical Society, Series B 42, 213-220.
Smith, C. A. B. (1965). Personal Probability and Statistical Analysis. Journal of the Royal Statistical
Society, Series A 128, 469-499.
Smith, Kevin B. and Meier, Kenneth J. (1995). The Case Against School Choice: Politics, Markets, and
Fools. Armonk, NY: M.E. Sharpe.
Smith, R. L. (1996). The Hit-And-Run Sampler: A Globally Reaching Markov Chain Sampler for Generating
Arbitrary Multivariate Distributions. Proceedings of the 1996 Winter Simulation Conference J. M.
Charnes, D. J. Morrice, D. T. Brunner and J. J. Swain (eds.). 260-264. Coronado, CA.
Smith, W. B. and Hocking, R. R. (1972). Wishart Variate Generator. Algorithm AS53. Applied Statistics
21, 241-245.
Smyth, G. K. (1989). Generalized Linear Models with Varying Dispersion. Journal of the Royal Statistical
Society, Series B 51, 47-60.
Sobol, I. M. (1994). A Primer for the Monte Carlo Method. New York: Chapman & Hall.
Solzhenitsyn, A. I. (1997). The Gulag Archipelago. Volume I. Thomas P. Whitney and H. Willetts, trans-
lators. Boulder, CO: Westview Press.
Sowey, E. R. (1972). A Chronological and Classiﬁed Bibliography on Random Number Generation and
Testing. International Statistical Review 40, 355-371.
Sowey, E. R. (1978). A Second Classiﬁed Bibliography on Random Number Generation and Testing. Inter-
national Statistical Review 46, 89-102.
Spall, J. C. and Hill, S. D. (1990). Least-Informative Bayesian Prior Distributions for Finite Samples Based
on Information Theory. IEEE Transactions on Automatic Control 35, 580-583.
Spetzler, C. S. and Sta¨el von Holstein, C. S. (1975). Probability Encoding in Decision Analysis. Manage-
ment Science 22, 340-358.
Spiegelhalter, D. J., Abrams, K. R. and Myles, J. P. (2004). Bayesian Approaches to Clinical Trials and
Health-Care Evaluation. New York: John Wiley & Sons.
Spiegelhalter, D. J., Freedman, L. S. and Parmar, M. K. B. (1994). Bayesian Approaches to Randomized
Trials. Journal of the Royal Statistical Society, Series A 157, 357-416.
Spiegelhalter, D. J. and Smith, A. F. M. (1982). Bayes Factors for Linear and Log-linear Models with
Vague Prior Information. Journal of the Royal Statistical Society, Series B 44, 377-387.
Spiegelhalter, D. J., Thomas, A., Best, N. G. and Gilks, W. R. (1996a).
BUGS 0.5*Examples: Volume 1
(version i). MRC Biostatistics Unit:
http://www.mrc-bsu.cam.ac.uk/wp-content/uploads/WinBUGS Vol1.pdf.
Spiegelhalter, D. J., Thomas, A., Best, N. G. and Gilks, W. R. (1996b).
BUGS 0.5*Examples: Volume 2
(version i). MRC Biostatistics Unit:
http://www.mrc-bsu.cam.ac.uk/wp-content/uploads/WinBUGS Vol2.pdf.
http://www.mrc-bsu.cam.ac.uk/wp-content/uploads/WinBUGS Vol3.pdf

636
References
Spiegelhalter, D. J., Thomas, A., Best, N. G. and Lunn, D. J. (2000).
WinBUGS User Manual Version
1.4 MRC Biostatistics Unit:
http://www.mrc-bsu.cam.ac.uk/wp-content/uploads/manual14.pdf.
Spiegelhalter, D. J., Thomas, A., Best, N. G. and Lunn, D. J. (2012).
BUGS 0.5*Examples: Volume 3
(version i). MRC Biostatistics Unit:
http://www.mrc-bsu.cam.ac.uk/wp-content/uploads/WinBUGS Vol3.pdf.
Spiegelhalter, D., Best, N. G., Carlin, B. P. and van der Linde, A. (2002).
Bayesian Measures of Model
Complexity and Fit. Journal of the Royal Statistical Society, Series B 64, 583-640.
Stangl, D. K. (1995). Prediction and Decision Making Using Bayesian Hierarchical Models. Statistics in
Medicine 14, 2173-2190.
Steen, N. M., Byrne, G. D. and Gelbard, E. M. (1969). Gaussian Quadrature for the Integrals
 ∞
0
f(x)dx
and  b
0 exp(−x2)f(x)dx. Mathematics of Computation 23, 661-671.
Steﬀey, D. (1992). Hierarchical Bayesian Modeling With Elicited Prior Information. Communications in
Statistics 21, 799-821.
Stein, C. (1955). Inadmissability of the Usual Estimator for the Mean of a Multivariate Normal Distribu-
tion. In Proceedings of the Third Berkeley Symposium on Mathematical Statistics and Probability. 1
Berkeley: University of California Press, pp.197-206.
Stein, C. (1965). Approximation of Improper Prior Measures by Prior Probability Measures. In Bernoulli-
Bayes-Laplace Anniversary Volume: Proceedings of an International Research Seminar Statistical Lab-
oratory, J. Neyman and L. M. Le Cam (eds.). New York: Springer-Verlag, pp.217-240.
Stein, C. (1981). Estimation of the Mean of a Multivariate Normal Distribution. Annals of Statistics 9,
1135-1151.
Stephens, D. A. (1994). Bayesian Retrospective Multiple-Changepoint Identiﬁcation. Applied Statistics 43,
159-178.
Stephens, M. (2000). Bayesian Analysis of Mixture Models with an Unknown Number of Components-An
Alternative to Reversible Jump Methods. Annals of Statistics 28, 40-74.
Stern, S. E. (1997). Simulation-Based Estimation. Journal of Economic Literature XXXV, 2006-2039.
Stern, H. S. and Cressie, N. (2000). Posterior predictive model checks for disease mapping models Statistics
in Medicine 19, 2377-2397.
Stewart, L. (1983). Bayesian Analysis Using Monte Carlo Integration-a Powerful Methodology for Handling
Some Diﬃcult Problems. Statistician 32, 195-200.
Stewart, L. and Davis, W. W. (1986). Bayesian Posterior Distributions Over Sets of Possible Models With
Inferences Computed by Monte Carlo Integration. The Statistician 35, 175-182.
Stigler, S. M. (1982). Thomas Bayes’ Bayesian Inference. Journal of the Royal Statistical Society, Series A
145, 250-258.
Stigler, S. M. (1986). The History of Statistics: The Measurement of Uncertainty before 1900. Cambridge,
MA: Harvard University Press.
Stigler, S. M. (1999). Statistics on the Table: The History of Statistical Concepts and Methods. Cambridge,
MA: Harvard University Press.
Stone, M. (1974). Cross-Validatory Choice and Assessment of Statistical Predictions. Journal of the Royal
Statistical Society, Series B 36, 111-147.
Stone, M. (1977a). An Asymptotic Equivalence of Choice Model by Cross-Validation and Akaike’s Criterion.
Journal of the Royal Statistical Association, Series B 39, 44-47.
Stone, M. (1977b). Comments On Model Selection Criteria of Akaike and Schwarz. Journal of the Royal
Statistical Society, Series B, 41, 276-278.
Stroud, A. H. (1971). Approximate Calculation of Multiple Integrals. Englewood Cliﬀs, NJ: Prentice-Hall.

References
637
Stroud, A. H. and Secrest, D. H. (1966). Gaussian Quadrature Formulas. Englewood Cliﬀs, NJ: Prentice-
Hall.
Stuart, A. and Ord, J. K. (1994). Kendall’s Advanced Theory of Statistics: Volume I, Distribution Theory.
Sixth Edition. London: Edward Arnold.
Student, A. (1908a). On the Probable Error of a Mean. Biometrika 6, 1.
Student, A. (1908b). On the Probable Error of a Correlation Coeﬃcient. Biometrika 6, 302.
Suchard, M. A., Weiss, R. E., Dorman, K. S. and Sinsheimer, J. S. (2003).
Inferring Spatial Phyloge-
netic Variation along Nucleotide Sequences: A Multiple Changepoint Model. Journal of the American
Statistical Association 98, 427-437.
Sundberg, R. (1974). Maximum Likelihood Theory for Incomplete Data from an Exponential Family. Scan-
dinavian Journal of Statistics 1, 49-58.
Sundberg, R. (1976). An Iterative Method for Solution of the Likelihood Equations for Incomplete Data
From Exponential Families. Communications in Statistics, Part B, Simulation and Computation 5,
55-64.
Suppes, P. (1974). The Measurement of Belief. Journal of the Royal Statistical Society, Series B 36, 160-
191.
Swendsen, R. H. and Wang, J. S. (1987). Nonuniversal Critical Dynamics in Monte Carlo Simulations.
Physical Review Letters 58, 86-88.
Szu, H. and Hartley, R. (1987). Fast Simulated Annealing. Physics Letters A 122, 157-162.
Tadikamalla, P. R. (1978). Computer Generation of Gamma Random Variables. Communications of the
Association for Computing Machinery 21, 419-422.
Tadikamalla, P. R. and Ramberg, J. S. (1975). An Approximate Method for Generating Gamma and Other
Variates. Journal of Statistical Computation and Simulation 3, 275-282.
Takeuchi, K. (1976). Distribution of Informational Statistics and A Criterion of Model Fitting. Suri-Kagaku
(Mathematical Statistics) 153, 12-18.
Tanner, M. A. (1996). Tools for Statistic Inference: Methods for the Exploration of Posterior Distributions
and Likelihood Functions. New York: Springer.
Tanner, M. A. and Wong, W. H. (1987). The Calculation of Posterior Distributions by Data Augmentation.
Journal of the American Statistical Society 82, 528-550.
Taralsden, G. and Lindqvist, B. H. (2010). Improper Priors Are Not Improper. The American Statistician
64, 154-158.
Thatcher, A. R. (1964). Relationships Between Bayesian and Conﬁdence Limits for Predictions. Journal of
the Royal Statistical Society, Series B 26, 176-210.
Theil, H. (1963). On the Use of Incomplete Prior Information in Regression Analysis. Journal of the Amer-
ican Statistical Association 58, 401-414.
Theil, H. (1970). On the Estimation of Relationships Involving Qualitative Variables. American Journal of
Sociology 76, 103-154.
Theil, H. and Goldberger, A. S. (1961). On Pure and Mixed Statistical Estimation in Economics. Interna-
tional Economic Review 2, 65-78.
Thisted, R. (1988). Elements of Statistical Computing. New York: Chapman & Hall.
Thompson, B. (1996). AERA Editorial Policies Regarding Statistical Signiﬁcance Testing: Three Suggested
Reforms. Educational Researcher 25, 26-30.
Thompson, B. (1997). Editorial Policies Regarding Statistical Signiﬁcance Testing: Further Comments. Ed-
ucational Researcher 26, 29-32.
Thompson, B. (2002a). Statistical, Practical, and Clinical: How Many Kinds of Signiﬁcance Do Councilers
Need to Consider? Journal of Counciling and Development 80, 64-71.

638
References
Thompson, B. (2002b). What Future Quantitative Social Science Research Could Look Like: Conﬁdence
Intervals for Eﬀect Sizes. Educational Researcher 31, 24-31.
Thompson, B. (2004). The “Signiﬁcance” Crisis in Psychology and Education. Journal of Socio-Economics
33, 607-613.
Thompson J. R., Palmer, T. M. and Moreno, S. (2006). Bayesian Analysis In Stata Using WinBUGS. The
Stata Journal 6, pp.530-549.
Tiao, G. C. and Zellner, A. (1964a). Bayes’s Theorem and the Use of Prior Knowledge in Regression
Analysis. Biometrika 51, 219-230.
Tiao, G. C. and Zellner, A. (1964b). On the Bayesian Estimation of Multivariate Regression. Journal of
the Royal Statistical Society, Series B 26, 277-285.
Tierney, L. (1991). Exploring Posterior Distributions Using Markov Chains. In Computing Science and
Statistics: Proceedings of the 23rd Symposium on the Interface. E. M. Keramidas (ed.). Fairfax Station,
VA: Interface Foundation, pp.563-570.
Tierney, L. (1994). Markov Chains for Exploring Posterior Distributions. Annals of Statistics 22, 1701-1728.
Tierney, L. (1996). Introduction to General State-Space Markov Chain Theory. In Markov Chain Monte
Carlo in Practice, W. R. Gilks, S. Richardson and D. J. Spiegelhalter (eds.). New York: Chapman &
Hall, pp.59-74.
Tierney, L. and Kadane, J. B. (1986). Accurate Approximations for Posterior Moments and Marginal
Densities. Journal of the American Statistical Association 81, 82-86.
Tierney, L., Kass, R. E. and Kadane, J. B. (1989a). Fully Exponential Laplace Approximations to Expec-
tations and Variances of Nonsensitive Functions. Journal of the American Statistical Association 84,
710-716.
Tierney, L., Kass, R. E. and Kadane, J. B. (1989b). Approximate Marginal Densities of Nonlinear Func-
tions. Biometrika 76, 425-433.
Tierney, L. and Mira, A. (1999). Some Adaptive Monte Carlo Methods for Bayesian Inference. Statistics
in Medicine 18, 2507-2515.
Titterington, D. M., Smith, A. F. M. and Makov, U. E. (1985). Statistical analysis of ﬁnite mixture
distributions. New York: John Wiley & Sons.
Tobin, James. (1958). Estimation of Relationships for Limited Dependent Variables. Econometrica 26, 24-
36.
Tong, Y. L. (1990). The Multivariate Normal Distribution. New York: Springer-Verlag.
Toothill, J. P. R., Robinson, W. D. and Adams, A. G. (1971). The Runs Up and Down Performance of
Tausworthe Pseudo-Random Number Generators. Journal of the Association for Computing Machinery
18, 381-399.
Torrie, G. M. and Valleau, R. L. (1977). Nonphysical Sampling Distributions in Monte Carlo Free-Energy
Estimation: Umbrella Sampling. Journal of Computational Physics 23, 187-199.
Tsai, T-H. and Gill, J. (2012). superdiag: A Comprehensive Test Suite for Markov Chain Non- Conver-
gence. The Political Methodologist. 19, 12-18.
Tsitsiklis, J. N. (1988). A Survey of Large Time Asymptotics of Simulated Annealing Algorithms. In
Stochastic Diﬀerential Systems, Stochastic Control Theory and Applications, W. Fleming and P. L.
Lions (eds.). New York: Springer, pp.583-599.
Turkov, P., Krasotkina, O. and Mottl, V. (2012). The Bayesian Logistic Regression In Pattern Recognition
Problems Under Concept Drift. 2012 21st International Conference on Pattern Recognition (ICPR).
Tsukuba, Japan, 11-15 Nov. 2012, 2976-2979.
Turnbull, B. W. (1976). The Empirical Distribution with Arbitrary Grouped, Censored and Truncated
Data. Journal of the Royal Statistical Society, Series B 38, 290-295.
Tversky, A. (1974). Assessing Uncertainty. Journal of the Royal Statistical Society, Series B 36, 148-159.

References
639
Tversky, A. and Kahneman, D. (1974). Judgment Under Uncertainty: Heuristics and Biases. Science 185,
1124-1131.
Tweedie, R. L. (1975). Suﬃcient Conditions for Ergodicity and Recurrence of Markov Chains on a General
State Space. Stochastic Processes Applications 3, 385-403.
Ulam, S. M. (1961). On Some Statistical Properties of Dynamical Systems. Proceedings of the 4th Berkeley
Symposium on Mathematical Statistics and Probability 3, 315-320.
van Dijk, H. K. and Kloek, T. (1982). Monte Carlo Analysis of Skew Posterior Distributions: An Illustrative
Econometric Example. The Statistician 32, 216-223.
van Dyk, D. and Meng, X-L. (2001). The Art of Data Augmentation. Journal of Computational and
Graphical Statistics 10, 1-50.
van Houwelingen, H. C. (1977). Monotonizing Empirical Bayes Estimators for a Class of Discrete Distribu-
tions with Monotone Likelihood Ratio. Statistica Neerlandica 31, 95-104.
van Houwelingen, H. C. and Stijnen, T. (1993). Monotone Empirical Bayes Estimators Based on More
Informative Samples. Journal of the American Statistical Association 88, 1438-1443.
van Laarhoven, P. J. M. and Aarts, E. H. L. (1987). Simulated Annealing: Theory and Applications. Reidel:
Dordrecht.
van Laarhoven, P. J. M., Boender, P., Aarts, E. H. L. and Rinnooy, K. A. (1989).
A Bayesian Approach
to Simulated Annealing. Probability in the Engineering and Informational Sciences 3, 453-475.
Vanpaemel, W. (2010). Prior Sensitivity In Theory Testing: An Apologia For the Bayes Factor. Journal of
Mathematical Psychology 54, 491-498.
Varian, H. R. (1975). A Bayesian Approach to Real Estate Assessment. In Studies in Bayesian Econometrics
and Statistics in Honor of Leonard J. Savage, S.E. Fienberg and A. Zellner (eds.). North-Holland,
Amsterdam, pp.195-208.
Venables, W. N. and Ripley, B. D. (1999). Modern Applied Statistics with S-Plus, Third Edition. New
York: Springer-Verlag.
Venn, J. (1866). The Logic of Chance. London: Macmillan.
Verma, V., Gordon, G., Simmons, R. and Thrun, S. (2004). Real-time Fault Diagnosis.
Robotics & Au-
tomation Magazine, IEEE 11, 56-66.
Verma, V., Langford, J. and Simmons, R. (2001). Non-Parametric Fault Identiﬁcation for Space Rovers.
International Symposium on Artiﬁcial Intelligence and Robotics in Space.
Verma, V., Thrun, S. and Simmons, R. (2003). Variable Resolution Particle Filter. Proceedings of the
International Joint Conference on Artiﬁcial Intelligence, AAAI, August.
Vidakovic, B. (1999). Linear Versus Nonlinear Rules for Mixture Normal Priors. Annals of the Institute of
Statistical Mathematics 51, 111-124.
Villegas, C. (1977). On the Representation of Ignorance. Journal of the American Statistical Association
72, 651-654.
Vines, S. K., Gilks, W. R. and Wild, P. (1996). Fitting Bayesian multiple random eﬀects models. Statistics
and Computing 6, 337-346.
Volpe, R., Nesnas, I. A. D., Estlin, T., Mutz, D., Petras, R. and Das, H. (2001). The CLARAty Architecture
for Robotic Autonomy. Proceedings of the 2001 IEEE Aerospace Conference, Big Sky, Montana, March
10-17.
Volpe, R. and Peters, S. (2003). Rover Technology Development and Infusion for the 2009 Mars Science
Laboratory Mission. Proceedings of 7th International Symposium on Artiﬁcial Intelligence, Robotics,
and Automation in Space, Nara, Japan, May 19-23.
Von Mises, R. (1957). Probability, Statistics and Truth. Second Revised English Edition prepared by H.
Geiringer. London: George Allen and Unwin.

640
References
von Neumann, J. (1951). Various Techniques Used in Connection with Random Digits, “Monte Carlo
Method.” U.S. National Bureau of Standards Applied Mathematics Series 12, 36-38.
Vuong, Q. H. (1989). Likelihood Ratio Tests for Model Selection and Non-Nested Hypotheses. Econometrica
57, 307-333.
Waagepetersen, R. and Sorensen, D. (2001). A Tutorial on Reversible Jump MCMC with a View toward
Applications in QTL-Mapping. International Statistical Review/Revue Internationale de Statistique
69, 49-61.
Wagner, K. and Gill, J. (2005). Bayesian Inference in Public Administration Research: Substantive Dif-
ferences from Somewhat Diﬀerent Assumptions. International Journal of Public Administration 28,
5-35.
Wakeﬁeld, J. C., Gelfand, A. E. and Smith, A. F. M. (1991). Eﬃcient Generation of Random Variates via
the Ratio-of-Uniforms Method. Statistics and Computing 1, 129-133.
Wakeﬁeld, J. C., Smith, A. F. M., Racine-Poon, A. and Gelfand, A. E. (1994).
Bayesian Analysis of Linear
and Non-linear Population Models by using the Gibbs Sampler. Applied Statistics 43, 201-221.
Walker, A. J. (1974). Fast Generation of Uniformly Distributed Pseudorandom Numbers with Floating-
Point Representation. Electronics Letters 10, 533-534.
Wallace, N. D. (1974). Computer Generation of Gamma Random Variables with Non-Integral Shape Pa-
rameters. Communications of the Association for Computing Machinery 17, 691-695.
Walley, P., Gurrin, L. and Burton, P. (1996). Analysis of Clinical Data Using Imprecise Prior Probabilities.
Statistician 45, 457-485.
Wasserman, L. (1992). Recent Methodological Advances in Robust Bayesian Inference. In Bayesian Statis-
tics 4, J. O. Berger, J. M. Bernardo, A. P. Dawid and A. F. M. Smith (eds.).
Oxford: Oxford University
Press, pp.763-773.
Wedderburn, R. W. M. (1974). Quasi-Likelihood Functions, Generalized Linear Models, and the Gauss-
Newton Method. Biometrika 61, 439-447.
Wedderburn, R. W. M. (1976). On the Existence and Uniqueness of the Maximum Likelihood Estimates
for Certain Generalized Linear Models. Biometrika 63, 27-32.
Wei, G. C. G. and Tanner, M. A. (1990). A Monte Carlo Implementation of the EM Algorithm and the Poor
Man’s Data Augmentation Algorithm. Journal of the American Statistical Association 85, 699-704.
Weiss, L. (1961). Statistical Decision Theory. New York: McGraw-Hill.
Weiss, R. E. (1996). An Approach to Bayesian Sensitivity Analysis. Journal of the Royal Statistical Society,
Series B 58, 739-750.
Wenzel, W. and Hamacher, K. (1999). A Stochastic Tunneling Approach for Global Minimization. Physical
Review Letters 82, 3003-3007.
West, M. (1984). Outlier Models and Prior Distributions in Bayesian Linear Regression. Journal of the
Royal Statistical Society, Series B 46, 431-439.
West, M. (1992). Modelling With Mixtures. In Bayesian Statistics 4, J. O. Berger, J. M. Bernardo, A. P.
Dawid and A. F. M. Smith (eds.).
Oxford: Oxford University Press, pp.227-246.
West, M. (1993). Approximating Posterior Distributions by Mixtures. Journal of the Royal Statistical So-
ciety, Series B 55, 409-422.
West, M. and Harrison, J. (1997). Bayesian Forecasting and Dynamic Models. New York: Springer-Verlag.
West, M. and Turner, D. A. (1994). Deconvolution of Mixtures in Analysis of Neural Synaptic Transmission.
Statistician 43, 31-43.
Western, B. (1995). A Comparative Study of Working-Class Disorganization: Union Decline in Eighteen
Advanced Capitalist Countries. American Sociological Review 60, 179-201.
Western, B. (1998). Causal Heterogeneity in Comparative Research: A
Bayesian Hierarchical Modelling Approach. American Journal of Political Science 42, 1233-1259.

References
641
Western, B. (1999). Bayesian Methods for Sociologists: An Introduction. Sociological Methods & Research
28, 7-34.
Westlake, W. J. (1967). A Uniform Random Number Generator Based on the Combination of Two Con-
gruential Generators. Journal of the Association for Computing Machinery 14, 337-340.
Wetzels, R., Raaijmakers, J. G. W., Jakab, E. and Wagenmakers, E-J. (2009).
How to Quantify Support
For and Against the Null Hypothesis: A Flexible WinBUGS Implementation of a Default Bayesian
t-Test. Psychonomic Bulletin & Review 16, 752-760.
Wheeler, D. J. (1974). Simulation of Arbitrary Gamma Distributions. IEEE Transactions 6, 167-169.
Wheeler, D. J. (1975). An Approximation for Simulation of Gamma Distributions. Journal of Statistical
Computation and Simulation 3, 225-232.
White, H. (1982). Maximum Likelihood Estimation of Misspeciﬁed Models. Econometrica 50, 1-26.
White, H. (1996). Estimation, Inference and Speciﬁcation Analysis. Cambridge: Cambridge University Press.
Whittaker, J. (1974). Generating Gamma and Beta Random Variables with Non-Integral Shape Parameters.
Applied Statistics 23, 210-214.
Whittaker, J. (1990). Graphical Models in Applied Mathematical Multivariate Analysis. New York: John
Wiley & Sons.
Whittlesey, J. R. B. (1969). On the Multidimensional Uniformity of Pseudo-Random Generators. Commu-
nications of the Association for Computing Machinery 12, 247.
Wiles, P. (1965). Rationalizing the Russians. New York Review of Books (October, 28) 5, 37-38.
Wilkinson, G. N. (1977). On Resolving the Controversy in Statistical Inference. Journal of the Royal Sta-
tistical Society, Series B 39, 119-171.
Winkler, R. L. (1967). The Assessment of Prior Distributions in Bayesian Analysis. Journal of the American
Statistical Association 62, 776-800.
Wolfson, L. J., Kadane, J. B. and Small, M. J. (1996). Bayesian Environmental Policy Decisions: Two Case
Studies. Ecological Applications 6, 1056-1066.
Wong, G. Y. and Mason, W. M. (1985). The Hierarchical Logistic Regression Model for Multilevel Analysis.
Journal of the American Statistical Association 80, 513-524.
Wong, G. Y. and Mason, W. M. (1991). Contextually Speciﬁc Eﬀects and Other Generalizations of the
Hierarchical Linear Model for Comparative Analysis. Journal of the American Statistical Association
86, 487-503.
Wong, W. H. and Li, B. (1992). Laplace Expansion for Posterior Densities of Nonlinear Functions of
Parameters. Biometrika 79, 393-398.
Woodward, Phil. (2011). Bayesian Analysis Made Simple: An Excel GUI for WinBUGS. Boca Raton: Chap-
man & Hall/CRC.
Wright, G. and Ayton, P. (1994). Subjective Probability. New York: John Wiley & Sons.
Wu, C. F. J. (1983). On the Convergence Properties of the EM Algorithm. Annals of Statistics 11, 95-103.
Yang, T. Y. and Kuo, L. (2001). Bayesian Binary Segmentation Procedure For a Poisson Process With
Multiple Changepoints. Journal of Computational and Graphical Statistics 10, 772-785.
Young, C. (2009). Model Uncertainty in Sociological Research: An Application to Religion and Economic
Growth. American Sociological Review 74, 380-397.
Yu, B. and Mykland, P. (1998). Looking at Markov Samplers Through CUSUM Path Plots: A Simple
Diagnostic Idea. Statistics and Computing 8, 275-286.
Zabell, S. (1989). R. A. Fisher on the History of Inverse Probability. Statistical Science 4, 247-263.
Zangwill, W. I. (1969). Nonlinear Programming: A Uniﬁed Approach. Englewood Cliﬀs, NJ: Prentice-Hall.
Zellner, A. (1971). An Introduction to Bayesian Inference in Econometrics. New York: Wiley & Sons.
Zellner, A. (1975). Bayesian Analysis of Regression Error Terms. Journal of the American Statistical As-
sociation 70, 138-144.

642
References
Zellner, A. (1976). Bayesian and Non-Bayesian Analysis of the Regression Model with Multivariate Student-
t Error Terms. Journal of the American Statistical Association 71, 400-405.
Zellner, A. (1985). Bayesian Econometrics. Econometrica 53, 253-269.
Zellner, A. and Chetty, V. K. (1965). Prediction and Decision Problems in Regression Models from the
Bayesian Point of View. Journal of the American Statistical Association 60, 608-616.
Zellner, A. and Highﬁeld, R. A. (1988). Calculation of Maximum Entropy Distributions and Approximation
of Marginal Posterior Distributions. Journal of Econometrics 37, 195-209.
Zellner, A. and Min, C-K. (1995). Gibbs Sampler Convergence Criteria. Journal of the American Statistical
Association 90, 921-927.
Zellner, A. and Moulton, B. R. (1985). Bayesian Regression Diagnostics with Applications to International
Consumption and Income Data. Journal of Econometrics 29, 187-211.
Zellner, A. and Rossi, P. E. (1984). Bayesian Analysis of Dichotomous Quantal Response Models. Journal
of Econometrics 25, 365-393.
Zellner, A. and Siow, A. (1980). Posterior Odds Ratios for Selected Regression Hypotheses. In Bayesian
Statistics, J. M. Bernardo, M. H. DeGroot, D. V. Lindley and A. F. M. Smith (eds.).
Valencia: Valencia
University Press, pp.586-603.
Zellner, A. and Tiao, G. C. (1964). Bayesian Analysis of the Regression Model With Autocorrelated Errors.
Journal of the American Statistical Association 59, 763-778.
Zhang, W. and Luck, S. J. (2011). The Number and Quality of Representations in Working Memory
Psychological Science 22, 1434-1441.
Zhang, Z., Chan, K. L., Wu, Y. and Chen, C. (2004).
Learning a Multivariate Gaussian Mixture Model
With the Reversible Jump MCMC Algorithm. Statistics and Computing 14, 343-355.
Zhao, X. and Chu, P. S. (2006). Bayesian Multiple Changepoint Analysis of Hurricane Activity in the
Eastern North Paciﬁc: A Markov Chain Monte Carlo Approach. Journal of Climate. 19, 4893-4901.
Zhenting, H. and Qingfeng, G. (1978). Homogeneous Denumerable Markov Chains. Berlin: Springer-Verlag
Science Press.
Zhou, X. and Reiter, J. P. (2010). A Note on Bayesian Inference After Multiple Imputation. The American
Statistician 64, 159-163.
Ziliak, S. T. and McCloskey, D. N. (2007). The Cult of Statistical Signiﬁcance: How the Standard Error
Costs Us Jobs, Justice, and Lives. Ann Arbor: University of Michigan Press.
Zuur, G., Garthwaite, P. H. and Fryer, R. J. (2002). Practical Use of MCMC Methods: Lessons from a
Case Study. Biometrical Journal 44, 433-455.

Now that Bayesian modeling has become standard, Markov chain Monte Car-
lo (MCMC) is well understood and trusted, and computing power continues to 
increase, Bayesian Methods: A Social and Behavioral Sciences Approach, 
Third Edition focuses more on implementation details of the procedures and less 
on justifying procedures. The expanded examples reflect this updated approach.
New to the Third Edition
•	 A chapter on Bayesian decision theory, covering Bayesian and frequentist 
decision theory as well as the connection of empirical Bayes with James–
Stein estimation
•	 A chapter on the practical implementation of MCMC methods using the 
BUGS software
•	 Greatly expanded chapter on hierarchical models that shows how this area is 
well suited to the Bayesian paradigm
•	 Many new applications from a variety of social science disciplines 
•	 Double the number of exercises, with 20 now in each chapter 
•	 Updated BaM package in R, including new datasets, code, and procedures 
for calling BUGS packages from R
This bestselling, highly praised text continues to show readers from the social and 
behavioral sciences how to use Bayesian methods in practice, preparing them for 
sophisticated, real-world work in the field. It gets readers up to date on the latest 
in Bayesian inference and computing. 
Jeff Gill is a professor in the Department of Political Science, the Division of Bio-
statistics, and the Department of Surgery (Public Health Sciences) at Washington 
University. His research applies Bayesian modeling and data analysis to ques-
tions in general social science quantitative methodology, political behavior and in-
stitutions, and medical/health data analysis using computationally intensive tools.
Statistics
K12896
w w w . c r c p r e s s . c o m

