Domenico Salvagnin
Michele Lombardi (Eds.)
 123
LNCS 10335
14th International Conference, CPAIOR 2017
Padua, Italy, June 5–8, 2017
Proceedings
Integration
of AI and OR Techniques
in Constraint Programming

Lecture Notes in Computer Science
10335
Commenced Publication in 1973
Founding and Former Series Editors:
Gerhard Goos, Juris Hartmanis, and Jan van Leeuwen
Editorial Board
David Hutchison
Lancaster University, Lancaster, UK
Takeo Kanade
Carnegie Mellon University, Pittsburgh, PA, USA
Josef Kittler
University of Surrey, Guildford, UK
Jon M. Kleinberg
Cornell University, Ithaca, NY, USA
Friedemann Mattern
ETH Zurich, Zurich, Switzerland
John C. Mitchell
Stanford University, Stanford, CA, USA
Moni Naor
Weizmann Institute of Science, Rehovot, Israel
C. Pandu Rangan
Indian Institute of Technology, Madras, India
Bernhard Steffen
TU Dortmund University, Dortmund, Germany
Demetri Terzopoulos
University of California, Los Angeles, CA, USA
Doug Tygar
University of California, Berkeley, CA, USA
Gerhard Weikum
Max Planck Institute for Informatics, Saarbrücken, Germany

More information about this series at http://www.springer.com/series/7407

Domenico Salvagnin
• Michele Lombardi (Eds.)
Integration
of AI and OR Techniques
in Constraint Programming
14th International Conference, CPAIOR 2017
Padua, Italy, June 5–8, 2017
Proceedings
123

Editors
Domenico Salvagnin
University of Padua
Padua
Italy
Michele Lombardi
University of Bologna
Bologna
Italy
ISSN 0302-9743
ISSN 1611-3349
(electronic)
Lecture Notes in Computer Science
ISBN 978-3-319-59775-1
ISBN 978-3-319-59776-8
(eBook)
DOI 10.1007/978-3-319-59776-8
Library of Congress Control Number: 2017943007
LNCS Sublibrary: SL1 – Theoretical Computer Science and General Issues
© Springer International Publishing AG 2017
This work is subject to copyright. All rights are reserved by the Publisher, whether the whole or part of the
material is concerned, speciﬁcally the rights of translation, reprinting, reuse of illustrations, recitation,
broadcasting, reproduction on microﬁlms or in any other physical way, and transmission or information
storage and retrieval, electronic adaptation, computer software, or by similar or dissimilar methodology now
known or hereafter developed.
The use of general descriptive names, registered names, trademarks, service marks, etc. in this publication
does not imply, even in the absence of a speciﬁc statement, that such names are exempt from the relevant
protective laws and regulations and therefore free for general use.
The publisher, the authors and the editors are safe to assume that the advice and information in this book are
believed to be true and accurate at the date of publication. Neither the publisher nor the authors or the editors
give a warranty, express or implied, with respect to the material contained herein or for any errors or
omissions that may have been made. The publisher remains neutral with regard to jurisdictional claims in
published maps and institutional afﬁliations.
Printed on acid-free paper
This Springer imprint is published by Springer Nature
The registered company is Springer International Publishing AG
The registered company address is: Gewerbestrasse 11, 6330 Cham, Switzerland

Preface
This volume is a compilation of the research program of the 14th International
Conference on the Integration of Artiﬁcial Intelligence and Operations Research
Techniques in Constraint Programming (CPAIOR 2017), held in Padova, Italy, during
June 5–8, 2017.
After a successful series of ﬁve CPAIOR international workshops in Ferrara (Italy),
Paderborn (Germany), Ashford (UK), Le Croisic (France), and Montreal (Canada), in
2004 CPAIOR evolved into a conference. More than 100 participants attended the ﬁrst
meeting held in Nice (France). In the subsequent years, CPAIOR was held in Prague
(Czech Republic), Cork (Ireland), Brussels (Belgium), Paris (France), Pittsburgh (USA),
Bologna (Italy), Berlin (Germany), Nantes (France), Yorktown Heights (USA), Cork
(Ireland), Barcelona (Spain), and Banff (Canada), in 2017 CPAIOR returned to Italy.
The aim of the CPAIOR conference series is to bring together researchers from
constraint programming (CP), artiﬁcial intelligence (AI), and operations research
(OR) to present new techniques or applications in the intersection of these ﬁelds, as
well as to provide an opportunity for researchers in one area to learn about techniques
in the others. A key objective of the conference is to demonstrate how the integration of
techniques from different ﬁelds can lead to highly novel and effective new methods for
large and complex problems. Therefore, papers that actively combine, integrate, or
contrast approaches from more than one of the areas were especially welcome.
Application papers showcasing CP/AI/OR techniques on innovative and challenging
applications or experience reports on such applications were also strongly encouraged.
In all, 73 long and short papers were submitted to the conference. The papers
underwent a rigorous peer-reviewing process, with each submission receiving at least
three reviews. Overall, 36 papers were selected by the international Program Com-
mittee. Four of the accepted papers were then selected for a fasttrack publication
process in the Constraints journal: only their abstracts appear in these proceedings.
The technical program of the conference was preceded by a master-class on “Com-
putational Techniques for Combinatorial Optimization,” with lectures from Tobias
Achterberg, Laurent Michel, Pierre Schaus, and Pascal Van Hentenryck. The main pro-
gram also enjoyed two invited talks, one from Andrea Lodi on “On the Role of (Machine)
Learning in (Mathematical) Optimization” and one from Kristian Kersting on “Relational
Quadratic Programming: Exploiting Symmetries for Modelling and Solving Quadratic
Programs.” The conference hosted one of the instantiations the DSO Workshop 2017,
organized by the EURO Working Group on Data Science Meets Optimization.
Putting together a conference requires help from many sources. We want to thank
the Program Committee and all other reviewers, who worked very hard in a busy
period of the year: Their effort to provide detailed reviews and discuss all papers in
depth after the author feedback to come up with a strong technical program is greatly
appreciated. We are also deeply grateful to the staff from the Department of Information
Engineering of the University of Padova for their great support in the event

organization, to the chairs from past CPAIOR editions for their advice, and ﬁnally to
the CPAIOR Steering Committee for giving us the opportunity to contribute to the
conference series.
The cost of holding an event like CPAIOR would be much higher without the help
of generous sponsors. We received outstanding support from Decision Brain, IBM
Research, and Data61. We also thank Gurobi Optimization, GAMS, AIMMS, and
COSLING. A ﬁnal acknowledgment goes to EasyChair and Springer, who allowed us
to put together these proceedings.
April 2017
Michele Lombardi
Domenico Salvagnin
VI
Preface

Organization
Program Chairs
Domenico Salvagnin
University of Padova and IBM Italy, Italy
Michele Lombardi
University of Bologna, Italy
Conference Chair
Domenico Salvagnin
University of Padova and IBM Italy, Italy
Steering Committee
Nicolas Beldiceanu
Ecole des Mines de Nantes, France
Natashia Boland
Georgia Institute of Technology, USA
Bernard Gendron
Université de Montréal, Canada
Willem-Jan van Hoeve
Carnegie Mellon University, USA
John Hooker
Carnegie Mellon University, USA
Jeff Linderoth
University of Wisconsin-Madison, USA
Michela Milano
Università di Bologna, Italy
George Nemhauser
Georgia Institute of Technology, USA
Gilles Pesant
Ecole Polytechnique de Montréal, Canada
Jean-Charles Régin
Université de Nice-Sophia Antipolis, France
Louis-Martin Rousseau
Ecole Polytechnique de Montréal, Canada
Michael Trick
Carnegie Mellon University, USA
Pascal Van Hentenryck
University of Michigan, USA
Mark Wallace
Monash University, Australia
Program Committee
Chris Beck
University of Toronto, Canada
David Bergman
University of Connecticut, USA
Timo Berthold
Fair Isaac Germany, Germany
Pierre Bonami
IBM Spain, Spain
Hadrien Cambazard
Grenoble INP, France
Andre A. Cire
University of Toronto, Canada
Matteo Fischetti
University of Padova, Italy
Bernard Gendron
Université de Montréal, Canada
Ambros Gleixner
Zuse Institute Berlin – ZIB, Germany
Carla Gomes
Cornell University, USA
Tias Guns
Vrije Universiteit Brussel, Belgium
John Hooker
Tepper School of Business and Carnegie Mellon
University, USA

Matti Järvisalo
University of Helsinki, Finland
Serdar Kadioglu
Oracle Corporation, USA
Philip Kilby
NICTA and Australian National University, Australia
Joris Kinable
Carnegie Mellon University, USA
Jeff Linderoth
University of Wisconsin-Madison, USA
Andrea Lodi
Ecole Polytechnique de Montréal, Canada
Ines Lynce
Instituto Superior Tecnico – Lisboa, Portugal
Laurent Michel
University of Connecticut, USA
Michele Monaci
University of Bologna, Italy
Siegfried Nijssen
Leiden University, The Netherlands
Barry O’Sullivan
University College Cork and Insight Center, Ireland
Claude-Guy Quimper
Université Laval, Canada
Jean-Charles Régin
Université de Nice-Sophia Antipolis, France
Louis-Martin Rousseau
Ecole Polytechnique de Montréal, Canada
Ashish Sabharwal
Allen Institute for Artiﬁcial Intelligence, USA
Scott Sanner
University of Toronto, Canada
Pierre Schaus
UC Louvain, Belgium
Christian Schulte
KTH Royal Institute of Technology, Sweden
Helmut Simonis
University College Cork, Ireland
Christine Solnon
LIRIS CNRS UMR 5205, France
Peter-J. Stuckey
University of Melbourne, Australia
Michael Trick
Carnegie Mellon University, USA
Pascal Van-Hentenryck
University of Michigan, USA
Willem-Jan Van-Hoeve
Tepper School of Business and Carnegie Mellon
University, USA
Sicco Verwer
Delft University of Technology, The Netherlands
Toby Walsh
UNSW and Data61, Australia
Alessandro Zanarini
ABB CRC, Switzerland
Yingqian Zhang
TU Eindhoven, The Netherlands
The CPAIOR 2017 conference was sponsored by:
VIII
Organization

Invited Talks (Abstracts)

On The Role of (Machine) Learning
in (Mathematical) Optimization
Andrea Lodi
Polytechnique Montréal, Montreal, Canada
andrea.lodi@polymtl.ca
Abstract. In this talk, I try to explain my point of view as a Mathematical
Optimizer – especially concerned with discrete (integer) decisions – on Big
Data. I advocate a tight integration of Machine Learning and Mathematical
Optimization (among others) to deal with the challenges of decision-making in
Data Science. For such an integration I concentrate on three questions: (1) what
can optimization do for machine learning? (2) what can machine learning do for
optimization? (3) which new applications can be solved by the combination of
machine learning and optimization? Finally, I will discuss in details two areas in
which machine learning techniques have been (successfully) applied in the area
of mixed-integer programming.

Relational Quadratic Programming:
Exploiting Symmetries for Modelling
and Solving Quadratic Programs
Kristian Kersting
TU Dortmund, Dortmund, Germany
kristian.kersting@cs.tu-dortmund.de
Abstract. Symmetry is the essential element of lifted inference that has recently
demonstrated the possibility to perform very efﬁcient inference in highly-
connected, but symmetric probabilistic models models aka. relational proba-
bilistic models. This raises the question, whether this holds for optimization
problems in general. In this talk I shall demonstrate that for a large class of
mathematical programs this is actually the case. More precisely, I shall introduce
the concept of fractional symmetries of linear and convex quadratic programs
(QPs), which lie at the heart of many machine learning approaches, and exploit
it to lift, i.e., to compress them. These lifted QPs can then be tackled with the
usual optimization toolbox (off-the-shelf solvers, cutting plane algorithms,
stochastic gradients etc.): If the original QP exhibits symmetry, then the lifted
one will generally be more compact, and hence their optimization is likely to be
more efﬁcient. This talk is based on joint works with Martin Mladenov, Martin
Grohe, Leonard Kleinhans, Pavel Tokmakov, Babak Ahmadi, Amir Globerson,
and many others.

Fast Track Papers
for the “Constraints” Journal

Auto-tabling for Subproblem Presolving
in MiniZinc (Summary)
Jip J. Dekker1, Gustav Björdal1, Mats Carlsson2, Pierre Flener1,
and Jean-Noël Monette3
1 Uppsala University, Uppsala, Sweden
Pierre.Flener@it.uu.se
2 SICS, Kista, Sweden
Mats.Carlsson@sics.se
3 Tacton Systems AB, Stockholm, Sweden
Jean-Noel.Monette@tacton.com
If poor propagation is achieved within part of a constraint programming (CP) model,
then a common piece of advice is to table that part. Tabling amounts to computing all
solutions to that part and replacing it by an extensional constraint requiring the vari-
ables of that part to form one of these solutions. If there are not too many solutions,
then the hope is that the increased propagation leads to faster solving. While powerful,
the tabling reformulation is however often not tried, because it is tedious and
error-prone to perform; further, it obfuscates the original model if it is actually
performed.
In [1], in order to encourage modellers to try tabling, we extend the MiniZinc
toolchain to perform the automatic tabling of suitably annotated constraint predicates,
without requiring any changes to solvers, thereby eliminating both the tedium and the
obfuscation. We introduce a presolve(autotable) annotation and apply it on
four case studies: the black-hole patience problem, the block party metacube problem,
the JP-encoding problem, and the handball tournament scheduling problem.
We see three advantages to our extension. First, the modeller is more likely to
experiment with the well-known and powerful tabling reformulation if its tedium is
eliminated by automation. Second, the original model is not obfuscated by tables, at the
often negligible cost of computing them on-the-ﬂy. Our experiments show that auto-
mated
tabling
yields
the
same
tables
as
manual
tabling.
Third,
tabling
is
technology-neutral: our experiments show that tabling can be beneﬁcial for CP back-
ends, with or without lazy clause generation, constraint-based local search (CBLS)
backends, Boolean satisﬁability (SAT) backends, SAT modulo theory (SMT) back-
ends, and hybrid backends; we have no evidence yet that a mixed-integer programming
(MIP) backend to MiniZinc can beneﬁt from tabling.
Modelling is an art that is difﬁcult to master: ﬁnding a model that is solved efﬁ-
ciently on at least one solver is often difﬁcult. We argue that our contribution lowers the

skill level required for designing a good model: the way an auto-tabled predicate is
formulated matters very little, as the same table will be generated.
Reference
1. Dekker, J.J., Björdal, G., Carlsson, M., Flener, P., Monette, J.N.: Auto-tabling for subproblem
presolving in MiniZinc. In: Constraints Journal Fast Track of CPAIOR 2017 (2017)
XVI
J. J. Dekker

Cumulative Scheduling with Variable Task
Proﬁles and Concave Piecewise Linear
Processing Rate Functions (Abstract)
Margaux Nattaf, Christian Artigues, and Pierre Lopez
LAAS-CNRS, Université de Toulouse, INSA, CNRS, Toulouse, France
{nattaf,artigues,lopez}@laas.fr
We consider a cumulative scheduling problem where a task duration and resource
consumption are not ﬁxed. The consumption proﬁle of the task, which can vary con-
tinuously over time, is a decision variable of the problem to be determined and a task is
completed as soon as the integration over its time window of a non-decreasing and
continuous processing rate or efﬁciency function of the consumption proﬁle has
reached a predeﬁned amount of energy. This model is well suited to represent energy
consuming tasks, for which the instantaneous power can be modulated at any time
while a global amount energy has to be received the task through a non linear efﬁciency
function. This is for example the case of melting operation in foundries.
The goal is to ﬁnd a feasible schedule, which is an NP-complete problem. Previous
studies considered identity and linear processing rate functions [1]. In this paper we
study the case where processing rate functions are concave and piecewise linear. This is
motivated by the fact that there exist many real-world concave processing rate func-
tions. Besides, approximating a function by a concave piecewise linear one is always at
least as good as an approximation by a linear function. We present two propagation
algorithms. The ﬁrst one is the adaptation to concave functions of the variant of the
energetic reasoning previously established for linear functions. Furthermore, a full
characterization of relevant intervals for time-window adjustments is provided. The
second algorithm combines a ﬂow-based checker with time-bound adjustments derived
from the time-table disjunctive reasoning for the cumulative constraint. Complemen-
tarity of the algorithms is assessed via their integration in a hybrid branch-and-bound
and computational experiments on a set of randomly generated instances. This abstract
refers to the full paper [2].
References
1. Nattaf, M., Artigues, C., Lopez, P., Rivreau, D.: Energetic reasoning and mixed-integer linear
programming for scheduling with a continuous resource and linear efﬁciency functions. OR
Spectrum 1–34 (2015)
2. Nattaf, M., Artigues, C., Lopez, P.: Cumulative scheduling with variable task proﬁles and
concave piecewise linear processing rate functions constraints (2017, to appear)

Efﬁcient Filtering for the Resource-Cost
AllDifferent Constraint
Sascha Van Cauwelaert and Pierre Schaus
UCLouvain, Louvain-la-Neuve, Belgium
{sascha.vancauwelaert,pierre.schaus}@uclouvain.be
Abstract. High energy consuming industries are increasingly concerned about
the energy prices when optimizing their production schedule. This is by far due
to today politics promoting the use of renewable energies. A consequence of
renewable energies is the high ﬂuctuation of the prices. Today one can forecast
accurately the energy prices for the next few hours and up to one day in
advance. In this work, we consider a family of problems where a set of items,
each requiring a possibly different amount of energy, must be produced at
different time slots. The goal is to schedule them such that the overall energy bill
is minimized. In Constraint Programming, one can model this cost in two ways:
(a) with a sum of ELEMENT constraints; (b) with a MINIMUMASSIGNMENT constraint.
In the latter case, the cost of an edge linking a variable (i.e., an item) to a value
(i.e., a time slot) is the product of the item consumption with the energy price
of the time slot. Unfortunately, both approaches have limitations. Modeling with
a sum of ELEMENT constraints does not take into account the fact that the items
must all be assigned to different time slots. The MINIMUMASSIGNMENT incorpo-
rates the ALLDIFFERENT constraint, but the algorithm has a quite high time
complexity of Oðn3Þ, where n is the number of items. This work proposes a
third approach by introducing the RESOURCECOSTALLDIFFERENT constraint [1]
and an associated scalable ﬁltering algorithm, running in Oðn:mÞ, where m is the
maximum domain size. Its purpose is to compute the total cost by dealing with
the fact that all assignments must be different in a scalable manner. The con-
straint RESOURCECOSTALLDIFFERENT(X, C, P, T) ensures that
^
AllDifferentðXÞ
T ¼ PjXj
i¼1 CðXiÞ  PðXiÞ

where X is a sequence of integer variables; C is a sequence of jXj integer
constants; P is a sequence of H integer constants (H  jXj); and T is an
integer variable that is the total resource cost. We ﬁrst evaluate the
efﬁciency of the new ﬁltering on a real industrial problem and then on
the Product Matrix Travelling Salesman Problem, a special case of the
Asymmetric Travelling Salesman Problem. The study shows experimentally
that our approach generally outperforms the decomposition and the
MINIMUMASSIGNMENT ones.

Reference
1. Van Cauwelaert, S., Schaus, P.: Efﬁcient ﬁltering for the resource-cost all different constraint.
Constraints (2017)
Efﬁcient Filtering for the Resource-Cost AllDifferent Constraint
XIX

Mining Time-Constrained Sequential Patterns
with Constraint Programming
John O.R. Aoga1
, Tias Guns2,3 and Pierre Schaus1
1 UCLouvain, ICTEAM, Louvain-la-Neuve, Belgium
{john.aoga,pierre.schaus}@uclouvain.be
2 VUB Brussels, Brussels, Belgium
tias.guns@vub.ac.be
3 KU Leuven, Leuven, Belgium
tias.guns@cs.kuleuven.be
This abstract is related to the original paper in the Constraint Journal [1].
Constraint Programming has proven to be an effective platform for constraint-based
sequential pattern mining. Previous work has focussed on standard sequential pattern
mining, as well as sequential pattern mining with a maximum ‘gap’ between two matching
events in a sequence. The main challenge in the latter is that this constraint can not be
imposed independently of the omnipresent frequency constraint. In this work, we go
beyond that and investigate the integration of timed events, the gap constraint as well as the
span constraint that constrains the time between the ﬁrst and last matching event. We show
how the three areinterrelated, and whatthe requiredchangestothe frequencyconstraintare.
Our contributions can be summarized as follows: (1) we have the backtracking-aware
datastructure to store all possible occurences of the pattern in a sequence, including the
ﬁrst matching symbol to support span constraints; (2) we avoid scanning a sequence for a
symbol beyond the (precomputed) last occurence of that symbol in the sequence; (3) we
introduce the concept of extension window of an embedding and avoid to scan overlap-
ping windows multiple times; (4) we avoid scanning for the start of an extension window,
which is speciﬁc to the gap constraint, by precomputing these in advance; and ﬁnally
(5) we experimentally demonstrate that the proposed approach outperforms both spe-
cialised and CP-based approaches in most cases, and that the difference becomes
increasingly large for low frequency thresholds. Furthermore, this time-aware global
constraint can be combined with other constraints such as regular expression constraints,
item inclusion/exclusion constraints or pattern length constraints.
Our proposal, called Preﬁx Projection Incremental Counting with time restrictions
Propagator (PPICt), is implemented in CP-Solver Oscar1.
Reference
1. Aoga, J.O.R., Guns, T., Schaus, P.: Mining time-constrained sequential patterns with con-
straint programming. In: Salvagnin, D., Lombardi, M. (eds.) CPAIOR 2017, LNCS, vol.
10335, p. XX. Springer, Switzerland (2017)
1 The data and code are available at http://sites.uclouvain.be/cp4dm/spm/ppict/.

Contents
Technical Papers
Sharpening Constraint Programming Approaches for Bit-Vector Theory. . . . .
3
Zakaria Chihani, Bruno Marre, François Bobot, and Sébastien Bardin
Range-Consistent Forbidden Regions of Allen’s Relations . . . . . . . . . . . . . .
21
Nicolas Beldiceanu, Mats Carlsson, Alban Derrien,
Charles Prud’homme, Andreas Schutt, and Peter J. Stuckey
MDDs are Efficient Modeling Tools: An Application to Some
Statistical Constraints. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
30
Guillaume Perez and Jean-Charles Régin
On Finding the Optimal BDD Relaxation. . . . . . . . . . . . . . . . . . . . . . . . . .
41
David Bergman and Andre Augusto Cire
Design and Implementation of Bounded-Length Sequence Variables . . . . . . .
51
Joseph D. Scott, Pierre Flener, Justin Pearson, and Christian Schulte
In Search of Balance: The Challenge of Generating Balanced
Latin Rectangles . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
68
Mateo Díaz, Ronan Le Bras, and Carla Gomes
Debugging Unsatisfiable Constraint Models . . . . . . . . . . . . . . . . . . . . . . . .
77
Kevin Leo and Guido Tack
Learning Decision Trees with Flexible Constraints and Objectives
Using Integer Optimization. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
94
Sicco Verwer and Yingqian Zhang
Relaxation Methods for Constrained Matrix Factorization Problems:
Solving the Phase Mapping Problem in Materials Discovery. . . . . . . . . . . . .
104
Junwen Bai, Johan Bjorck, Yexiang Xue, Santosh K. Suram,
John Gregoire, and Carla Gomes
Minimizing Landscape Resistance for Habitat Conservation . . . . . . . . . . . . .
113
Diego de Uña, Graeme Gange, Peter Schachte, and Peter J. Stuckey
A Hybrid Approach for Stator Winding Design Optimization . . . . . . . . . . . .
131
Alessandro Zanarini and Jan Poland

A Distributed Optimization Method for the Geographically Distributed
Data Centres Problem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
147
Mohamed Wahbi, Diarmuid Grimes, Deepak Mehta, Kenneth N. Brown,
and Barry O’Sullivan
Explanation-Based Weighted Degree . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
167
Emmanuel Hebrard and Mohamed Siala
Counting Weighted Spanning Trees to Solve Constrained Minimum
Spanning Tree Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
176
Antoine Delaite and Gilles Pesant
The Weighted Arborescence Constraint . . . . . . . . . . . . . . . . . . . . . . . . . . .
185
Vinasetan Ratheil Houndji, Pierre Schaus, Mahouton Norbert
Hounkonnou, and Laurence Wolsey
Learning When to Use a Decomposition . . . . . . . . . . . . . . . . . . . . . . . . . .
202
Markus Kruber, Marco E. Lübbecke, and Axel Parmentier
Experiments with Conflict Analysis in Mixed Integer Programming . . . . . . .
211
Jakob Witzig, Timo Berthold, and Stefan Heinz
A First Look at Picking Dual Variables for Maximizing Reduced
Cost Fixing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
221
Omid Sanei Bajgiran, Andre A. Cire, and Louis-Martin Rousseau
Experimental Validation of Volume-Based Comparison
for Double-McCormick Relaxations. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
229
Emily Speakman, Han Yu, and Jon Lee
Minimum Makespan Vehicle Routing Problem
with Compatibility Constraints . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
244
Miao Yu, Viswanath Nagarajan, and Siqian Shen
Solving the Traveling Salesman Problem with Time Windows
Through Dynamically Generated Time-Expanded Networks . . . . . . . . . . . . .
254
Natashia Boland, Mike Hewitt, Duc Minh Vu, and Martin Savelsbergh
A Fast Prize-Collecting Steiner Forest Algorithm for Functional
Analyses in Biological Networks. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
263
Murodzhon Akhmedov, Alexander LeNail, Francesco Bertoni, Ivo Kwee,
Ernest Fraenkel, and Roberto Montemanni
Scenario-Based Learning for Stochastic Combinatorial Optimisation . . . . . . .
277
David Hemmi, Guido Tack, and Mark Wallace
XXII
Contents

Optimal Stock Sizing in a Cutting Stock Problem
with Stochastic Demands . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
293
Alessandro Zanarini
Stochastic Task Networks: Trading Performance for Stability . . . . . . . . . . . .
302
Kiriakos Simon Mountakis, Tomas Klos, and Cees Witteveen
Rescheduling Railway Traffic on Real Time Situations
Using Time-Interval Variables . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
312
Quentin Cappart and Pierre Schaus
Dynamic Temporal Decoupling. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
328
Kiriakos Simon Mountakis, Tomas Klos, and Cees Witteveen
A Multi-stage Simulated Annealing Algorithm for the Torpedo
Scheduling Problem. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
344
Lucas Kletzander and Nysret Musliu
Combining CP and ILP in a Tree Decomposition of Bounded Height
for the Sum Colouring Problem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
359
Maël Minot, Samba Ndojh Ndiaye, and Christine Solnon
htd – A Free, Open-Source Framework for (Customized) Tree
Decompositions and Beyond . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
376
Michael Abseher, Nysret Musliu, and Stefan Woltran
The Nemhauser-Trotter Reduction and Lifted Message Passing
for the Weighted CSP . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
387
Hong Xu, T.K. Satish Kumar, and Sven Koenig
A Local Search Approach for Incomplete Soft Constraint Problems:
Experimental Results on Meeting Scheduling Problems . . . . . . . . . . . . . . . .
403
Mirco Gelain, Maria Silvia Pini, Francesca Rossi,
Kristen Brent Venable, and Toby Walsh
Author Index . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
419
Contents
XXIII

Technical Papers

Sharpening Constraint Programming
Approaches for Bit-Vector Theory
Zakaria Chihani(B), Bruno Marre, Fran¸cois Bobot, and S´ebastien Bardin
CEA, LIST, Software Security Lab, Gif-sur-Yvette, France
{zakaria.chihani,bruno.marre,francois.bobot,sebastien.bardin}@cea.fr
Abstract. We address the challenge of developing eﬃcient Constraint
Programming-based approaches for solving formulas over the quantiﬁer-
free fragment of the theory of bitvectors (BV), which is of paramount
importance in software veriﬁcation. We propose CP(BV), a highly eﬃ-
cient BV resolution technique built on carefully chosen anterior results
sharpened with key original features such as thorough domain combina-
tion or dedicated labeling. Extensive experimental evaluations demon-
strate that CP(BV) is much more eﬃcient than previous similar attempts
from the CP community, that it is indeed able to solve the majority of
the standard veriﬁcation benchmarks for bitvectors, and that it already
complements the standard SMT approaches on several crucial (and
industry-relevant) aspects, notably in terms of scalability w.r.t. bit-
width, theory combination or intricate mix of non-linear arithmetic and
bitwise operators. This work paves the way toward building competitive
CP-based veriﬁcation-oriented solvers.
1
Introduction
Context. Not so long ago, program veriﬁcation was such an ambitious goal
that even brilliant minds decided it was “bound to fail” [37]. At the time, the
authors concluded their controversial paper saying that if, despite all their rea-
sons, “veriﬁcation still seems an avenue worth exploring, so be it”. And so it was.
Today, software veriﬁcation is a well established ﬁeld of research, and industrial
adoption has been achieved is some key areas, such as safety-critical systems.
Since the early 2000’s, there is a signiﬁcant trend in the research community
toward reducing veriﬁcation problems to satisﬁability problems of ﬁrst-order
logical formulas over well-chosen theories (e.g. bitvectors, arrays or ﬂoating-point
arithmetic), leveraging the advances of modern powerful SAT and SMT solvers
[6,30,38,43]. Besides weakest-precondition calculi dating back to the 1970’s [19],
most major recent veriﬁcation approaches follow this idea [15,25,29,35].
The problem. While SMT and SAT are the de facto standard in veriﬁcation, a
few teams explore how Constraint Programming (CP) techniques can be used in
Work partially funded by ANR under grants ANR-14-CE28-0020 and ANR-12-INSE-
0002. The CP solver COLIBRI is generously sponsored by IRSN, the French nuclear
safety agency.
c
⃝Springer International Publishing AG 2017
D. Salvagnin and M. Lombardi (Eds.): CPAIOR 2017, LNCS 10335, pp. 3–20, 2017.
DOI: 10.1007/978-3-319-59776-8 1

4
Z. Chihani et al.
that setting [16,26,27,33,42]. Indeed, CP could in principle improve over some
of the well-known weaknesses of SMT approaches, such as non-native handling
of ﬁnite domains theories (encoded in the Boolean part of the formula, losing
the high-level structure) or very restricted theory combinations [39].
Yet, currently, there is no good CP-based resolution technique for (the
quantiﬁer-free fragment of) the theory of bitvectors [30], i.e. ﬁxed-size arrays
of bits equipped with standard low-level machine instructions, which is of para-
mount importance in veriﬁcation since it allows to encode most of the basic
datatypes found in any programming language.
Goal and challenge. We address the challenge of developing eﬃcient Con-
straint Programming-based approaches for solving formulas over the quantiﬁer-
free fragment of the theory of bitvectors (BV). Our goal is to be able to solve
many practical problems arising from veriﬁcation (with the SMTCOMP chal-
lenge1 as a benchmark) and to be at least complementary to current best SMT
approaches. The very few anterior results were still quite far from these objec-
tives [44], even if preliminary work by some of the present authors was promising
on conjunctive-only formulas [1].
Proposal and contributions. We propose CP(BV), a highly eﬃcient BV res-
olution technique built on carefully chosen anterior results [1,36] sharpened with
key original features such as thorough domain combination or dedicated label-
ing. Extensive experimental evaluations demonstrate that CP(BV) is much more
eﬃcient than previous similar attempts from the CP community, that it is indeed
able to solve the majority of the standard veriﬁcation benchmarks for bitvec-
tors, and that it already complements the standard SMT approaches on several
crucial (and industry-relevant) aspects, notably in terms of scalability w.r.t. bit-
width, theory combination or intricate mix of non-linear arithmetic and bitwise
operators. Our main contributions are the following:
• We present CP(BV), an original framework for CP-based resolution of BV
problems, which built on anterior results and extend them with several key
new features in terms of thorough domain combination or dedicated labeling.
This results in a competitive CP-based solver, excelling in key aspects such
as scalability w.r.t. bitwidth and combination of theories. A comparison of
CP(BV) with previous work is presented in Table 1.
• We perform a systematic and extensive evaluation of the eﬀect of our diﬀerent
CP improvements on the eﬃciency of our implementation. This compares
the options at our disposal and justiﬁes those we retained, establishing a
ﬁrm ground onto which future improvements can be made. It also shows the
advantage of our approach relative to the other CP approaches applied to BV,
and that our approach is able to solve a very substantial part of problems
from the SMTCOMP challenge.
• Finally, we perform an extensive comparison against the best SMT solvers for
BV problems, namely: Z3 [8], Yices [20], MathSAT [14], CVC4 [4] and
1 smtcomp.sourceforge.net.

Sharpening Constraint Programming Approaches for Bit-Vector Theory
5
Table 1. Overview of our method
Bardin et al. [1]
Michel et al. [36]
CP(BV)
Bitvector domain
+
++
++ [36]
Arithmetic domain
++
+
++ [1]
Domain combination
+
+
++
Simpliﬁcations
+
x
++
BV-aware labeling
x
x
++
Implemented
Yes
No
Yes
Benchmark
Conjunctive formulas ≈
200 formulas
Arbitrary formulas ≈
30,000 formula
Boolector [11]. This comparison exhibits the strenghts of CP over SMT
approaches on particular instances. Speciﬁcally, our implementation surpasses
several (and sometimes all) solvers on some examples involving large bit-
vectors and/or combination with ﬂoating-point arithmetic.
Discussion. Considering that our current CP(BV) approach is far from optimal
compared to existing SMT solvers (implemented in Prolog, no learning), we
consider this work as an important landmark toward building competitive CP-
based veriﬁcation-oriented solvers. Moreover, our approach clearly challenges the
well-accepted belief that bitvector solving is better done through bitblasting,
opening the way for a new generation of word-level solvers.
2
Motivation
The standard (SMT) approach for solving bit-vector problem, called bit-
blasting [7], relies on a boolean encoding of the initial bitvector problem, one
boolean variable being associated to each bit of a bitvector. This low-level encod-
ing allows for a direct reuse of the very mature and ever-evolving tools of the
SAT community, especially DPLL-style SAT solvers [38,43]. Yet, crucial high-
level structural information may be lost during bitblasting, leading to potentially
poor reasoning abilities on certain kinds of problems, typically those involv-
ing many arithmetic operations [12] and large-size bitvectors. Following anterior
work [1,36], we propose a high-level encoding of bitvector problems, seen as Con-
straint Satisfaction Problems (CSP) over ﬁnite (but potentially huge) domains.
Each bitvector variable of the original BV problem is now considered as a (CSP)
bounded-arithmetic variable, with dedicated domains and propagators.
Now illustrating with concrete examples, we show the kind of problems where
our approach can surpass existing SMT solvers. Consider the three following
formulas:
x × y = (x & y) × (x | y) + (x & y) × (x & y)
(A)
x1 < x2 < · · · < xn < x1
(B)
(
n−2

i=1
xi < xi+1 & xi+2) ∧(xn−1 < xn & x1) ∧(xn < x1 & x2)
(C)

6
Z. Chihani et al.
where · (resp. · & ·, · | ·) is the bit-wise negation (resp. conjunction, disjunction),
∧is the logical conjunction, < is an unsigned comparison operator, n was chosen
to be 7. As an example, the SMT-LIB language [5] encoding of formula A is:
(assert (= (bvmul X Y) (bvadd (bvmul (bvand X Y) (bvor X Y))
(bvmul (bvand X (bvnot Y)) (bvand (bvnot X) Y)))))
Table 2 shows the time in seconds according to bit-vector size, both for the
satisﬁability proof of the valid formula A and the unsatisﬁablity of B and C.
CP(BV) is the name of our technique, and TO means that the solver was halted
after a 60-second timeout.
Table 2. Comparison of performance (time in sec.) for diﬀerent solvers
Formula size(bits) Z3
Yices MathSAT CVC4 Boolector CP(BV)
A
512
TO
1.60
6.04
17.28
20.55
0.24
1024
TO
7.25
26.72
TO
TO
0.23
2048
TO
31.83 TO
TO
TO
0.23
B
512
0.53
0.82
1.37
0
2.75
0.26
1024
1.75
4.89
4.23
0
7.39
0.22
2048
5.73 16.15 22.76
0
16.81
0.21
C
512
0.15
0.85
1.55
0.76
3.15
0.25
1024
0.33
1.25
4.53
3.49
3.81
0.22
2048
0.70
5.55
19.57
8.82
14.73
0.25
On these examples, CP(BV) clearly surpasses SMT solvers, reporting no TO
and a very low (size-independent) solving time. In light of this, we wish to empha-
size the following advantages of our CP(BV) high-level encoding for bitvector
solving:
• Each variable is attached to several and complementary domain represen-
tations, in our case: intervals plus congruence, bitlist [1] (i.e. constraints on
the possible values of speciﬁc bits of the bitvector) and global diﬀerence con-
straint (delta). Each domain representation comes with its own constraint
propagation algorithms and deals with diﬀerent aspects of the formula to
solve. We will call integer domains or arithmetic domains those domains deal-
ing mainly with high-level arithmetic properties (here: intervals, congruence,
deltas), and BV domains or bitvector domains those domains dealing with
low-level aspects (here: bitlist).
• These domains can freely communicate between each other, each one reﬁn-
ing the other through a reduced product (or channeling) mechanism [41]. For
example, in formula B, adding the diﬀerence constraint to the delta domain
does allow CP(BV) to conclude unsat directly at propagation. Having mul-
tiple domains also allows to search for a solution in the smallest of them (in
terms of the cardinality of the concretization of the domain abstraction).

Sharpening Constraint Programming Approaches for Bit-Vector Theory
7
• In the case of formula C, a reduced product is not enough to conclude at prop-
agation. Here, a BV constraint itself reﬁnes an arithmetic domain: indeed,
with the simple observation that, if a & b = c then c ⩽a and c ⩽b, the
bit-vector part of CP(BV) not only acts on the bit-vector representation of
the variables, but also “informs” the global diﬀerence constraints of a link it
could not have found on its own.
3
Background
This section lays down the ground on which our research was carried out, both
the theoretical foundations, anterior works and the COLIBRI CP solver [33].
3.1
BV Theory
We recall that BV is the quantiﬁer-free theory of bitvectors [30], i.e. a theory
where variables are interpreted over ﬁxed-size arrays (or vectors) of bits along
with their basic operations: logical operators (conjunction “ & ”, disjunction “|”
and xor “⊕”, etc.), modular arithmetic (addition +, multiplication ×, etc.) and
other structural manipulations (concatenation :: , extraction ⌊.⌉i.j, etc.).
3.2
CP for Finite-Domain Arithmetic
A Constraint Satisfaction Problem [18] (CSP) consists in a ﬁnite set of variables
ranging over some domains, together with a set of constraints over these variables
– each constraint deﬁning its own set of solutions (valuations of the variables
that satisfy the constraint). Solving a CSP consists in ﬁnding a solution meeting
all the constraints of the CSP, or proving that no such solution exists. We are
interested here only in the case where domains are ﬁnite. Constraint Program-
ming [18] (CP) consists in solving a CSP through a combination of propagation
and search. Propagation consists mainly in reducing the potential domains of
the CSP variables by deducing that some values cannot be part of any solution.
Once no more propagation is possible, search consists in assigning a value to
a variable (taken from its reduced domain) and continue the exploration, with
backtrack if required.
The CP discipline gets its strenght from global constraints and capabilities
for dense interreductions between domain representations, along with constraint
solving machinery. We present in this section standard domains and constraints
for bounded arithmetic.
Union of intervals. A simple interval [a; d], where a, d ∈N represents the
fact that a given variable can take values only between a and d. A natural
extension of this notion is the union of intervals (Is). As a shortened notation,
if x ∈{a} ⊎[b; c] ⊎{d}, one writes [x] = [a, b··c, d].
Congruence [31]. If the division remainder of a variable x by a divisor d is r (i.e.,
x satisﬁes the equation x%d = r), then ⟨x⟩= r[d] is a congruence and represents
all values that variable x can take, e.g., ⟨x⟩= 5[8] means x ∈{5, 13, 21, . . .}.

8
Z. Chihani et al.
Global diﬀerence constraint (Delta) [23]. A global diﬀerence constraint is
a set of linear constraints of the form x −y ⋄k where ⋄∈{=, ̸=, <, >, ⩽, ⩾}.
Tracking such sets of constraints allows for better domain propagation and early
infeasibility detection, thanks to a global view of the problem compared with
the previous (local) domains.
3.3
The COLIBRI Solver for FD Arithmetic
The COLIBRI CP solver [33] was initially developped to assist CEA veriﬁca-
tion tools [2,9,17,47]. COLIBRI supports bounded integers (both standard and
modular arithmetic [28]), ﬂoating-points [34] and reals. Considering arithmetic,
COLIBRI already provides all the domains described in Sect. 3.2, together with
standard propagation techniques and strong interreductions between domains.
Search relies mostly on a standard fail-ﬁrst heuristics. COLIBRI is implemented
in ECLiPSe Prolog, yielding a signiﬁcant performance penalty (compared with
compiled imperative languages such as C or C++) but allowing to quickly pro-
totype new ideas.
3.4
Former CP(BV) Approaches
Two papers must be credited with supplying the inspiration for this work, written
by Bardin et al. [1] and by Michel and Van Hentenryck [36]. Put together, these
two papers had good ideas, which we adopted, unsatisfactory ideas which were
disgarded, and ﬁnally ideas that were not advanced enough which we extended.
The ﬁrst paper [1] introduces the bitlist domain, i.e. lists of four-valued items
ranging over {0, 1, ?, ⊥} – indicating that the ith bit of a bitvector must be 0, 1,
any of these two values (?), or that a contradiction has been found (⊥) – together
with its propagators for BV operators. Moreover, the authors also explain how
arithmetic domains (union of intervals and congruence) can be used for BV,
and describe ﬁrst interreduction mechanisms between bitlists and arithmetic
domains.
The second paper [36] introduces a very optimized implementation of bitlists,
using two BVs ⟨1x, 0x⟩to represent the bitlist of x, where 1x (resp. 0x) represents
bits known to be set (resp. cleared) in x. The eﬃciency comes from the use of
machine-level operations for performing domain operations, yielding constant
time propagation algorithms for reasonable bitvector sizes.
Basically, we improve the technique described in [1] by: borrowing the opti-
mized bit-vector domain representation from [36] (with a few very slight improve-
ments), signiﬁcantly improving inter-domain reduction, and designing a BV-
dedicated search labeling strategy.
As improving inter-domain reduction is one of our key contributions, we
present hereafter the reductions between BV domains and arithmetic domains
described in [1]:
With congruence: the BV domain interacts according to the longest sequence
of known least signiﬁcant bits. For example, a BV domain [[10?00?101]] of a

Sharpening Constraint Programming Approaches for Bit-Vector Theory
9
variable b indicates that b satisﬁes the equation b[8] = 5, which therefore
constrains the congruence domain using 5[8]. Conversely a known congruence
of some power of 2 ﬁxes the least signiﬁcant bits.
With Is: for a variable x, the union of intervals can reﬁne the most signiﬁcant
bits of the BV domain by clearing bits according to the power of two that
is immediately greater than the maximum extremum of the Is. And the BV
domain inﬂuences Is by (only) reﬁning the extrema through the maximum
and minimum bit-vectors allowed, and by removing singletons that do not
conform to the BV domain.
4
Boosting CP(BV) for Eﬃcient Handling of Bit-Vectors
In this section, we delve into the speciﬁcities of our approach, leaving complex
details to a technical report2. We ﬁrst start by presenting a signiﬁcantly improved
inter-domain reduction, followed by new reductions from BV constraints to other
domains, then we show some simpliﬁcations and factorisations at the constraint
level, and we ﬁnish by presenting our BV-dedicated labeling strategy.
4.1
Better Interreduction Between BV- and Arithmetic- Domains
We present here several signiﬁcant improvements to the inter-domain reduc-
tion techniques proposed in [1] between bitlists and unions of intervals. Our
implementation also borrows the inter-reduction between bitlists and congru-
ence from [1].
Is to BVs. Let m and M be respectively the minimal and the maximal value
of a union of intervals. Then, the longest sequence of most-signiﬁcant bits on
which they “agree” can also be ﬁxed in the bit-vector domain. For example,
m = 48 and M = 52 (00110000 and 00110100 in binary) share their ﬁve most-
signiﬁcant bits, denoted [[00110???]]. Therefore, a bit-vector bl = [[0??1???0]] can
be reﬁned into [[00110??0]]. For comparison, the technique in [1] only reduces bl
to [[00?1???0]].
BV to Is. Consider a variable b with a Is domain [b] = [153, 155, 158··206, 209],
and a bit-vector domain b = [[1??1??01]]={· · · , 153, 157, 177, 181, 185, 189,
209, · · · }, as illustrated in Fig. 1. The inter-domain reduction from [1] can reﬁne
the extremum of the Is (here: nothing to do, since 153 and 209 both conforms
to b) and removes the singletons that are not in b (here: 155), yielding [b] =
[153, 158··206, 209]. We propose to go a step further by reﬁning each bound
inside a Is, such that after reduction each bound of [b] conforms to b. Here, 158
(resp. 206) is not allowed and should be replaced by its closest upper (resp. lower)
value in b, i.e. 177 (resp. 189), yielding [b] = [153, 177··189, 209].
We have designed such a correct and optimal reduction algorithm from bitlist
to Is. Since we work on the ⟨1x, 0x⟩representation of bitlists, the algorithm relies
2 sites.google.com/site/zakchihani/cpaior.

10
Z. Chihani et al.
Fig. 1. Enlarging the gaps in the Is according to the BV domain
on machine-level operations and is linear in the size of the bitvector (cf. technical
report). We describe the procedure for increasing a lower bound in Algorithm 1;
decreasing the upper bound (symmetrically) follows the same principle (cf. tech-
nical report). In order to calculate a new bound r accepted by the bit-vector
domain b, we start by imposing on the lower bound l what we already know,
i.e., set what is set in 1b and clear what is cleared in 0b (line 1 of Algorithm 1).
Then ﬂag the bits that were changed by this operation, going from cleared to
set and from set to cleared.
Algorithm
1.
Increasing
the
lower
bound l according to b
1: r := 1b | l & 0b
2: set2cl:= l & r
3: cl2set:= l & r
4: if cl2set > set2cl then
5:
size:=log2(cl2set)
6:
mask0 := −1 ≪size
7:
can-cl:= mask0 | 1b
8:
r := r & can-cl
9: else
10:
size:=log2(set2cl)
11:
cl-can-set:= r & 0b
12:
next-to-set := left-cl-can-set-of(size,
cl-can-set)
13:
r :=set(r,next-to-set)
14:
mask0 := −1 ≪next-to-set
15:
can-cl:= mask0 | 1b
16:
r := r & can-cl
17: end if
To reﬁne the lower bound, we
must raise it as much as necessary
but not one bit higher, i.e., we should
look for the smallest amount to add
to the lower bound in order to make
it part of the concretisation of b.
This entails two things: a cleared bit
can only become set if all bits of lower
signiﬁcance get cleared. For example,
to increase the binary represented
integer 010 exactly until the left-
most bit gets set, we will pass by 011
and stop at 100 : going to 101 would
increase more than strictly necessary.
Similarly, the smallest increase that
clears a set bit i is one where the
ﬁrst cleared bit on the left of i can be
set (line 12 of Algorithm 1, function
left-cl-can-set-of). For example,
to clear the third most signiﬁcant bit
(in bold) in 011011, one needs to increase to 011100, 011101, 011110, 011111
then reach 100000. Doing so clears not only the target bit i but all the bits of
lower signiﬁcance.
Drilling the Is according to BV. If the Is contains only one interval, then our
technique does not improve over [1], and is only slighly superior to the channeling
method of [36]. For this reason, we force the bit-vector domain to create at least
one gap in the union of intervals. Consider for example a domain bl = [[0?10?1?]].

Sharpening Constraint Programming Approaches for Bit-Vector Theory
11
When observing the concretisation {18, 19, 22, 23, 50, 51, 54, 55}, the largest gap
is between 23 and 50, i.e., 0010111 and 0110010, obtained by ﬁxing the most
signiﬁcant unknown bit (msub). More generally, for a variable x the largest gap
is created by intersecting [x] with [1x··a, b··0x], where a is obtained by clearing
the msub and setting all other unknown bits, and b is obtained by setting the
msub and clearing all other unknown bits. One can of course enforce more gaps,
but there is a tradeoﬀbetween their propagation cost (as Is) and their beneﬁts.
In this work, using one gap was satisfactory.
4.2
BV Constraints Reducing Arithmetic Domains
Our CP(BV) approach strives to keep each of its domains as aware as possible of
the other domains. We now show how non-BV domains can be reduced through
BV constraints. In the following, we recall that [x] denotes the union of intervals
attached to variable x.
4.2.1
BV Constraints on Is
It turns out that most BV constraints can inﬂuence unions of intervals.
Bitwise binary operands: a disjunction x | y = z can reﬁne [z] in more than
one way, but experimentation showed a notable eﬀect only when [x] and [y] con-
tain only singletons, at which case [z] can be reﬁned by the pairwise disjunction
of those singletons. Similar reﬁnements can occure through the bitwise conjunc-
tion and exclusive disjunction. For the latter, one can also reﬁne in the same
manner the Is of the operands, since x ⊕y = z implies the same constraint for
all permutation of x, y, z.
Negation: from a negation constraint x = y, one can reﬁne the Is of one variable
from that of the other. By mapping each singleton {c} ∈[x] and interval a··b ∈
[x] to {c} and b··a, we build a Is to populate [y]. The symmetric construction
populates [x].
Shifts: from a right shift x ≫y = z, which is equivalent to a natural division
(i.e., x/2y = z), one can reﬁne [z] simply by right-shifting all elements of [x] (sin-
gletons and bounds of internal intervals) by y. The left-shift constraint is treated
mostly in the same way but requires extra care as it is a modular multiplication
and it can overﬂow.
Sign-Extension: when extending the sign of x by i positions to obtain z, the
method consists in splitting the [x] by the integers that are interpreted as nega-
tive, most signiﬁcant bit is 1, and the one interpreted as positive, most signiﬁcant
bit is 0 and to apply the sign extention separately, disjunction with 2i −1 ≪i
for the ﬁrsts and identity for the seconds.
Extractions: when extracting from the left-most to any position, it’s the same
as a right logical shift. The more general case is tricky. Take ⌊x⌉i.j = y to mean
the extraction from bit i to j of x to obtain y (with ∥x∥> i ⩾j ⩾0), then a
singleton in for an interval xa··xb,

12
Z. Chihani et al.
• If (xb ≫j) −(xa ≫j) > 2i−j, then the interval necessarily went through all
integer coded on i bits, so the integer domain cannot be reﬁned.
• else, if (xb ⊕xa) & 2i = 0, then no power of 2 was traversed, the bounds can
simply be truncated and stay in that order: (⌊xa⌉i.j)··(⌊xb⌉i.j)
• else, 2i was traversed, then the Is is [0··(⌊xb⌉i.j), (⌊xa⌉i.j)··(2(i−j) −1)]
For example, using the binary representation for the integer bounds of a union
of intervals, an extraction of the 3 rightmost bits of a variable whose union of
intervals contains 01110··10011 would not produce the invalid interval 110··011
because its lower bound is greater than its upper bound. This falls in the third
case above, and would generate the two intervals 000··011 and 110··111.
Concatenation: for a concatenation x :: y = z, the inner structure of [z] can
be reﬁned from [x] and [y]. Let v≪x be v | (x ≪∥y∥) and (a··b)≪x be a≪x··b≪x.
For example, if [x] = [xa··xb] and [y] = [y1, y2··y3, y4··y5], then [z] can be reﬁned
by [(y1)≪xa, (y2··y3)≪xa, (y
≪xa
4
)··(y
≪xb
1
), (y2··y3)≪xb, (y4··y5)≪xb]. The algorithm
is described in the technical report. One can also reﬁne [x] and [y] from [z].
4.2.2
BV Constraints on Deltas
As seen in the motivation section, keeping the deltas informed of the relationship
between diﬀerent variables can be an important factor for eﬃcient reasoning.
Bitwise operations: a constraint x | y = c implies that that (c −y ⩽x ⩽
c) ∧(c −x ⩽y ⩽c). Symmetric information can be derived for conjunction.
Exclusive disjunction, however, does not derive knowledge regarding deltas. The
bitwise negation has limited eﬀect and can only impose that its argument and
its result be diﬀerent.
Extraction: regardless of the indices, the result of an extraction is always less
than or equal to its result. As a matter of fact, an extraction ⌊x⌉i.j = (x%2i)/2j
and can enjoy the same propagations on the non-BV domains.
More generally: many BV constraints can be mapped to an integer counterpart
and propagate on non-BV domains. For example, a concatenation x :: y can have
the same eﬀect as the (overﬂowless) integer constraint z = x × 2∥y∥+ y would.
4.3
Factorizations and Simpliﬁcations
In the course of solving, a constraint can be simpliﬁed or become duplicate or
a subsumption of another constraint. These shortcuts can be separated in two
categories.
Simpliﬁcations. Neutral and absorbing elements provide many rewriting rules
which replace constraints by simpler ones. In addition to these usual simpliﬁca-
tions one can detect more sophisticated ones, such as if z ≫y = z and y > 0
then z = 0 (without restricting the value of y), and when z is x rotated i times,
if the size of x is 1 or if i%∥x∥= 0, then z = x. Furthermore, if i and ∥x∥are
coprimes, and x = z, then x = 0 or x = 2∥x∥−1.

Sharpening Constraint Programming Approaches for Bit-Vector Theory
13
Factorizations. The more constraints are merged or reduced to simpler con-
straints, the closer we get to a proof. Functional factorization allows to detect
instances based on equality of arguments, but some other instances can be fac-
tored as well, for example:
• from x⊕y = z and x⊕t = y, we deduce that t = z, unifying the two variables
and removing one of the constraints, now considered duplicates
• when x ≪y1 = z1 and x ≪y2 = z2 and y1 < y2, and z1 is a constant, then
one can infer the value of z2. A similar operation can be carried out for ≫.
• two constraints x & y = 0 and x | y = 2∥x∥−1 can be replaced by x = y
• a constraint x & y = z (resp. x | y = z) is superﬂuous with the constraint
x = y once z is deducted to be equal to 0 (resp. 2∥x∥−1).
• the constraints x = y and x & z = y (resp. x | z = y) can both be removed
once deducted that x = 2∥x∥−1, y = z = 0 (resp. x = 0, y = z = 2∥x∥−1).
4.4
BV-Dedicated Labeling Strategies
A labeling strategy (a.k.a. search strategy) consists mainly of two heuristics: vari-
able selection and value selection. For variable selection, we rely on the fail-ﬁrst
approach implemented in COLIBRI [33]. Basically, the variable to be selected is
the one with the smallest domain (in terms of concretization). Adding the BV
domain allows here to reﬁne the notion of smallest. For value selection, in the
event that BV is the smallest domain, our strategy is the following:
• First, we consider certain values that can simplify arithmetic constraints. In
particular, we start by trying 0 (for +, −, ×, /, & , |, ⊕), 1 (for ×, /) and 2s−1
where s is the bitvector size (for & , |);
• Second, we ﬁx the value of several most signiﬁcant and least signiﬁcant
unknown bits (msub, lsub) at once, allowing to strongly reﬁne all domains
thanks to inter-reduction, and to ﬁx early the sign of the labeled variable
(useful for signed BV operations). Currently, we ﬁx at each labeling step one
msub and two lsub, yielding 8 possible choices. We choose whether to set ﬁrst
or clear ﬁrst in an arbitrary (but deterministic) way, using a ﬁxed seed.
5
Experimentation
We describe in this section our implementation and experimental evaluation of
CP(BV).
Implementation. We have implemented a BV support inside the COLIBRI
CP solver [33] (cf. Sect. 3.3). Modular arithmetic domains and propagators are
treated as blackbox, and we add the optimized bitlist domain and its associated
propagators from [36], as well as all improvements discussed in Sect. 4. Building
on top of COLBRI did allow us to prototype our approach very quickly, compared
with starting from scratch.
Because it is written in a Prolog dialect, the software must be interpreted at
each run, inducing a systematic 0.2 s starting time. This obstacle is not troubling

14
Z. Chihani et al.
for us because any real-world application would execute our software once and
feed its queries in a chained manner through a server mode. Yet, the SMTCOMP
rules impose that the software be called on command line with exactly one .smt2
ﬁle, which excludes a “server mode”.
Experimental setup and caveats. We experiment CP(BV) on the 32 k BV-
formulas from the SMTCOMP benchmark, the leading competition of the SMT
community. These formulas are mostly taken from veriﬁcation-oriented industrial
case-studies, and can be very large (up to several hundreds of MB). The ﬁrst
set of experiments (Sect. 5.1) has been run on the StarExec server3 provided by
SMTCOMP, they are made public4. The second set of experiments (Sect. 5.2) is
run on a Intel® CoreT M i7-4712HQ CPU @ 2.30 GHz with 16 GB memory. Two
points must be kept in mind.
• We ﬁx a low time-out (60 s) compared with the SMTCOMP rules (40 min),
yet we argue that our results are still representative: ﬁrst, such low time-outs
are indeed very common in applications such as bug ﬁnding [25] or proof
[19]; second, it is a common knowledge in ﬁrst-order decision procedures that
“solvers either terminate instantly, or timeout” – adding more time does not
dramatically increase the number of solved formulas;
• SMT solvers such as Z3 and CVC4 are written in eﬃcient compiled languages
such as C/C++, with near-zero starting time. Hence, we have a constant-
time disadvantage here – even if such a burden may not be so important in
veriﬁcation: since we are anyway attacking NP-hard problems, we are looking
for exponential algorithmic improvements ; constant-time gains can only help
marginally.
5.1
Evaluation Against State-of-the-Art Benchmark
Absolute performance. Our implementation solved 24 k formula out of 32 k
(75%). While it would not have permitted us to win the SMTCOMP, it is still
a signiﬁcantly more thorough comparison with the SMT community than any
previous CP eﬀort on bitvectors, demonstrating that CP can indeed be used for
veriﬁcation and bitvectors.
Comparing diﬀerent choices of implementation. Improvements oﬀered by
our diﬀerent optimizations are very dependent on the type of formulas, and
these details would be diluted if regrouping all of the benchmarks. The reader
is invited to consult our detailed results on the StarExec platform. Yet, as a
rule of thumb, our extensions yield a signiﬁcant improvement on some families
of examples, and do not incur any overhead on the other, proving their overall
utility. For example :
• On the family of formulas named stp samples(≃400 formulas), when deac-
tivating the reductions from BV constraints to other domains (Sect. 4.2), the
3 www.starexec.org.
4 www.starexec.org/starexec/secure/explore/spaces.jsp?id=186070.

Sharpening Constraint Programming Approaches for Bit-Vector Theory
15
solver is unable to solve a quarter less formulas that it did with the full imple-
mentation. Removing the interreduction with the Is (Sect. 4.1), the loss rises
to half ;
• Solving the spear family suﬀers little from deactivating BV/Is inter-
reductions, but half (200) formulas are lost without the BV constraints reduc-
ing other domains (Sect. 4.2);
• On some other families, such as pspace and dwp formulas, there is no tangi-
ble eﬀect (neither positive nor negative) to the deactivation of improvements.
5.2
Comparison to State-of-the-Art SMT Solvers
We demonstrate in the following that CP(BV) is actually complementary to
SMT approaches, especially on problems with large bitvectors or involving multi-
theories. As competitors, we select the ﬁve best SMT solvers for BV theory: Z3,
Yices, MathSAT, CVC4 and Boolector.
SMTCOMP: large formulas. To study the eﬀect of our method on scalability,
we show here the results on three categories of formulas, regrouped according to
the (number of digits of the) size of the their largest bit-vectors: 3-digit (from
100 to 999), 4-digit and 5-digit, respectively having 629, 298 and 132 formulas.
Results in Table 3 show on the one hand the scalability of CP(BV) – the larger
BV sizes, the greater impact the CP approach has – and on the other hand its
complementarity with SMT. In particular, it shows the result of duels between
CP(BV) and each of the SMT solvers : a formula is considered a win for a solver
if it succeeds (TO = 60 s) while the other solver does not. We report results
on a format Win/Lose (Solve), where Win and Lose are from CP(BV) point
of view, and Solve indicates the number of formulas solved by the SMT solver.
For example, MathSAT could solve 17 of the 132 5-digit formulas – all of which
being solved by CP(BV), while CP(BV) could solve 63 formulas – 46 of which
were unsolved by MathSAT. Here, CP(BV) solves the higher number of 5-digit
size formulas (equality with CVC4), and no solver but Boolector solves formulas
that CP(BV) does not. On other sizes, CP(BV) solves less formulas, but it can
still solve formulas that SMT solvers do not.
SMTCOMP: hard formulas. We deﬁne hard formulas by separating 5 classes
of diﬃculty, with class i regrouping the formulas on which i SMT solvers out
of 5 spend more than 5 s. We compare CP(BV) to SMT solvers on these hard
Table 3. Comparing CP(BV) with ﬁve state-of-the-art solvers on large formulas
sz
#f
CP(BV)
#solved
Z3
w/l (s)
Yices
w/l (s)
MathSAT
w/l (s)
CVC4
w/l (s)
Boolector
w/l (s)
5
132
63
63/0 (0)
53/0 (10)
46/0 (17)
0/0 (63)
32/10 (41)
4
298
44
34/153 (163)
40/87 (91)
43/68 (69)
42/150 (152)
43/204 (205)
3
629
35
24/496 (507)
23/262 (274)
23/419 (431)
23/511 (523)
25/507 (517)
sz: size (#digits) - #f: # of formulas
w/l (s): #win/#lose for CP(BV), s: #formulas solved by SMT solver

16
Z. Chihani et al.
Table 4. Overall comparison on hard examples
Category
1-fail 2-fail 3-fail 4-fail All-fail
#benchs
1083
382
338
1075
873
CP(BV) under 5 s
139
108
10
68
61
problems. Results are presented in Table 4, where we report for each class i the
number of formulas from this class that CP(BV) solves quickly. Especially, the
5th column (All-fail) shows that 61 formulas are solved only by CP(BV) in less
than 5 s.
Mixing bitvectors and ﬂoats. Considering multi-theory formulas combining
BV and FP arithmetics, COLIBRI has been tested on 7525 industrially-provided
formulas (not publically available). It was able to solve 73% of them, standing
half-way between Z3 / MathSAT and CVC4 (Table 5). Considering now the last
SMTCOMP QF BVFP category (Table 6), even with the 0.2 s starting time,
CP(BV) would have won the competition – admittedly, there are only few for-
mulas in this category.
Table 5. Industrial formula with bitvectors and ﬂoats
CP(BV) Z3
MathSAT CVC4
#solved 5512
7225 7248
2245
Ratio
73%
96%
96%
29%
Total: 7525 formulas
Table 6. SMTCOMP, QF BVFP category
Z3
MathSAT CP(BV)
int to ﬂoat complex 2.smt2 1.04
0.13
0.25
int to ﬂoat simple 2.smt2
2.17
0.22
0.21
int to ﬂoat complex 1.smt2 0.95
0.08
0.25
int to ﬂoat simple 1.smt2
0.02
0.02
0.25
nan 1.smt2
0
0
0.26
incr by const.smt2
8.20 30.50
0.26
int to ﬂoat complex 3.smt2 1.89
0.44
0.25
quake3 1.smt2
TO
TO
TO
6
Related Work
CP-based methods for BV. This work strongly stands upon the prior results
of Bardin et al. [1] and Michel et al. [36]. Our respective contribution is already

Sharpening Constraint Programming Approaches for Bit-Vector Theory
17
discussed at length in Sects. 2 and 3. Basically, while we reuse the same gen-
eral ideas, we sharpen them through a careful selection of the best aspects of
each of these works and the design of new mechanisms, especially in terms of
domain combination and labeling strategies. As a result, experiments in Sect. 5
demonstrate that our own CP(BV) approach performs much better than previ-
ous attempts. Moreover, we perform an extensive comparison with SMT solvers
on the whole SMTCOMP benchmark, while these previous eﬀorts were either
limited to conjunctive formulas or remain only theoretical. The results by Michel
et al. have been applied to a certain extent [46] as an extension of MiniSat [21],
yet with no reduced product, to a limited set of BV operations and on BV sizes no
larger than 64 bits. Older word-level approaches consider straightforward trans-
lations of bit-vector problems into disjunctive or non-linear arithmetic problems
[10,22,40,45,48,49] (including bitblasting-like transformation for logical bitwise
operators), and then rely on standard methods from linear integer programming
or CP. Experimental results reported in [1,44] demonstrate that such straight-
forward word-level encoding yield only very poor results on formulas coming
from software veriﬁcation problems.
SMT-based methods for BV. While state-of-the-art methods heavily rely
on bitblasting and modern DPLL-style SAT solvers [38,43], the community is
sensing the need for levels of abstraction “where structural information is not
blasted to bits”[12]. Part of that need comes from the knowledge that certain
areas, arithmetic for example, are not eﬃciently handled by bit-level reasoning
tools. As a mitigation, SMT solvers typically complement optimized bitblasting
[12,13,32] with word-level preprocessing [3,24]. Compared to these approaches,
we lack the highly-eﬃcient learning mechanisms from DPLL. Yet, our domains
and propagations yield more advanced simpliﬁcations, deeply nested with the
search mechanism.
7
Conclusion
This
work
addresses
the
challenge
of
developing
eﬃcient
Constraint
Programming-based approaches for solving formulas over (the quantiﬁer-free
fragment of) the theory of bitvectors, which is of paramount importance in soft-
ware veriﬁcation. While the Formal Veriﬁcation community relies essentially on
the paradigm of SMT solving and reduction to Boolean satisﬁability, we explore
an alternative, high-level resolution technique through dedicated CP principles.
We build on a few such anterior results and sharpen them in order to propose a
highly eﬃcient CP(BV) resolution method. We show that CP(BV) is much more
eﬃcient than the previous attempts from the CP community and that it is indeed
able to solve the majority of the standard veriﬁcation benchmarks for bitvectors.
Moreover CP(BV) already complements the standard SMT approach on several
crucial (and industry-relevant) aspects, such as scalability w.r.t. bit-width, for-
mulas combining bitvectors with bounded integers or ﬂoating-point arithmetic,
and formulas deeply combining non-linear arithmetic and bitwise operators.

18
Z. Chihani et al.
Considering that our current CP(BV) approach is far from optimal compared
with existing SMT solvers, we believe this work to be an important landmark
toward building competitive CP-based veriﬁcation-oriented solvers. Moreover,
our approach clearly challenges the well-accepted belief that bitvector solving
is better done through bitblasting, opening the way for a new generation of
word-level solvers.
References
1. Bardin, S., Herrmann, P., Perroud, F.: An alternative to SAT-based approaches for
bit-vectors. In: Esparza, J., Majumdar, R. (eds.) TACAS 2010. LNCS, vol. 6015,
pp. 84–98. Springer, Heidelberg (2010). doi:10.1007/978-3-642-12002-2 7
2. Bardin, S., Herrmann, P.: OSMOSE: automatic structural testing of executables.
Softw. Test. Veriﬁcation Reliab. 21(1), 29–54 (2011)
3. Barret, C., Dill, D., Levitt, J.: A decision procedure for bit-vector arithmetic. In:
DAC (1998)
4. Barrett, C., Conway, C.L., Deters, M., Hadarean, L., Jovanovi´c, D., King, T.,
Reynolds, A., Tinelli, C.: CVC4. In: Gopalakrishnan, G., Qadeer, S. (eds.) CAV
2011. LNCS, vol. 6806, pp. 171–177. Springer, Heidelberg (2011). doi:10.1007/
978-3-642-22110-1 14
5. Barrett, C., et al.: The SMT-LIB Standard: Version 2.0. Technical report (2010)
6. Barrett, C.W., et al.: Satisﬁability modulo theories. In: Handbook of Satisﬁability
(2009)
7. Biere, A., Cimatti, A., Clarke, E., Zhu, Y.: Symbolic model checking without
BDDs. In: Cleaveland, W.R. (ed.) TACAS 1999. LNCS, vol. 1579, pp. 193–207.
Springer, Heidelberg (1999). doi:10.1007/3-540-49059-0 14
8. Bjørner, N.: Taking satisﬁability to the next level with Z3. In: Gramlich, B., Miller,
D., Sattler, U. (eds.) IJCAR 2012. LNCS, vol. 7364, pp. 1–8. Springer, Heidelberg
(2012). doi:10.1007/978-3-642-31365-3 1
9. Blanc, B., et al.: Handling state-machines speciﬁcations with GATeL. Electron.
Notes Theor. Comput. Sci. 264(3), 3–17 (2010)
10. Brinkmann, R., Drechsler, R.: RTL-datapath veriﬁcation using integer linear pro-
gramming. In: 15th International Conference on VLSI Design (2002)
11. Brummayer, R., Biere, A.: Boolector: an eﬃcient SMT solver for bit-vectors and
arrays. In: Kowalewski, S., Philippou, A. (eds.) TACAS 2009. LNCS, vol. 5505,
pp. 174–177. Springer, Heidelberg (2009). doi:10.1007/978-3-642-00768-2 16
12. Bruttomesso, R., Cimatti, A., Franz´en, A., Griggio, A., Hanna, Z., Nadel, A.,
Palti, A., Sebastiani, R.: A lazy and layered SMT(BV) solver for hard industrial
veriﬁcation problems. In: Damm, W., Hermanns, H. (eds.) CAV 2007. LNCS, vol.
4590, pp. 547–560. Springer, Heidelberg (2007). doi:10.1007/978-3-540-73368-3 54
13. Bryant, R.E., Kroening, D., Ouaknine, J., Seshia, S.A., Strichman, O., Brady, B.:
Deciding bit-vector arithmetic with abstraction. In: Grumberg, O., Huth, M. (eds.)
TACAS 2007. LNCS, vol. 4424, pp. 358–372. Springer, Heidelberg (2007). doi:10.
1007/978-3-540-71209-1 28
14. Cimatti, A., Griggio, A., Schaafsma, B.J., Sebastiani, R.: The MathSAT5 SMT
solver. In: Piterman, N., Smolka, S.A. (eds.) TACAS 2013. LNCS, vol. 7795, pp.
93–107. Springer, Heidelberg (2013). doi:10.1007/978-3-642-36742-7 7
15. Clarke, E., Kroening, D., Lerda, F.: A tool for checking ANSI-C programs. In:
Jensen, K., Podelski, A. (eds.) TACAS 2004. LNCS, vol. 2988, pp. 168–176.
Springer, Heidelberg (2004). doi:10.1007/978-3-540-24730-2 15

Sharpening Constraint Programming Approaches for Bit-Vector Theory
19
16. Collavizza, H., Rueher, M., Hentenryck, P.: CPBPV: a constraint-programming
framework
for
bounded
program
veriﬁcation.
In:
Stuckey,
P.J.
(ed.)
CP
2008. LNCS, vol. 5202, pp. 327–341. Springer, Heidelberg (2008). doi:10.1007/
978-3-540-85958-1 22
17. David, R., et al.: BINSEC/SE: A dynamic symbolic execution toolkit for binary-
level analysis. In: SANER 2016 (2016)
18. Dechter, R.: Constraint Processing. Morgan Kaufmann Publishers Inc., Massa-
chusetts (2003)
19. Dijkstra, E.W.: A Discipline of Programming, vol. 1. Prentice-Hall Englewood
Cliﬀs, New Jersey (1976)
20. Dutertre, B.: Yices 2.2. In: Biere, A., Bloem, R. (eds.) CAV 2014. LNCS, vol. 8559,
pp. 737–744. Springer, Cham (2014). doi:10.1007/978-3-319-08867-9 49
21. E´en, N., S¨orensson, N.: An extensible SAT-solver. In: Giunchiglia, E., Tacchella,
A. (eds.) SAT 2003. LNCS, vol. 2919, pp. 502–518. Springer, Heidelberg (2004).
doi:10.1007/978-3-540-24605-3 37
22. Ferrandi, F., Rendine, M., Sciuto, D.: Functional veriﬁcation for SystemC descrip-
tions using constraint solving. In: Design, Automation and Test in Europe (2002)
23. Feydy, T., Schutt, A., Stuckey, P.J.: Global diﬀerence constraint propagation for
ﬁnite domain solvers. In: PPDP (2008)
24. Ganesh, V., Dill, D.L.: A decision procedure for bit-vectors and arrays. In: Damm,
W., Hermanns, H. (eds.) CAV 2007. LNCS, vol. 4590, pp. 519–531. Springer, Hei-
delberg (2007). doi:10.1007/978-3-540-73368-3 52
25. Godefroid, P.: Test generation using symbolic execution. In: D’Souza, D., Kavitha,
T., Radhakrishnan, J. (eds.) FSTTCS, vol. 18, pp. 24–33. Schloss Dagstuhl, Ger-
many (2012)
26. Gotlieb, A.: TCAS software veriﬁcation using constraint programming. Knowl.
Eng. Rev. 27(3), 343–360 (2012)
27. Gotlieb, A., Botella, B., Rueher, M.: Automatic test data generation using con-
straint solving techniques. In: ISSTA (1998)
28. Gotlieb, A., Leconte, M., Marre, B.: Constraint Solving on Modular Integers (2010)
29. Henzinger, T.A., et al.: Lazy abstraction. In: POPL (2002)
30. Kroening, D., Strichman, O.: Decision Procedures: An Algorithmic Point of View,
1st edn. Springer Publishing Company Incorporated, Heidelberg (2008)
31. Leconte, M., Berstel, B.: Extending a CP solver with congruences as domains for
program veriﬁcation. In: Trends in Constraint Programming (2010)
32. Manolios, P., Vroon, D.: Eﬃcient circuit to CNF conversion. In: Marques-Silva, J.,
Sakallah, K.A. (eds.) SAT 2007. LNCS, vol. 4501, pp. 4–9. Springer, Heidelberg
(2007). doi:10.1007/978-3-540-72788-0 3
33. Marre, B., Blanc, B.: Test selection strategies for Lustre descriptions in GaTeL.
Electron. Notes Theor. Comput. Sci. 111, 93–111 (2005)
34. Marre, B., Michel, C.: Improving the ﬂoating point addition and subtraction con-
straints. In: Cohen, D. (ed.) CP 2010. LNCS, vol. 6308, pp. 360–367. Springer,
Heidelberg (2010). doi:10.1007/978-3-642-15396-9 30
35. McMillan, K.L.: Lazy abstraction with interpolants. In: Ball, T., Jones, R.B. (eds.)
CAV 2006. LNCS, vol. 4144, pp. 123–136. Springer, Heidelberg (2006). doi:10.1007/
11817963 14
36. Michel, L.D., Hentenryck, P.: Constraint satisfaction over bit-vectors. In: Milano,
M. (ed.) CP 2012. LNCS, pp. 527–543. Springer, Heidelberg (2012). doi:10.1007/
978-3-642-33558-7 39
37. Millo, R.A.D., Lipton, R.J., Perlis, A.J.: Social processes and proofs of theorems
and programs. Commun. Assoc. Comput. Mach. 22(5), 271–280 (1979)

20
Z. Chihani et al.
38. Moskewicz, M.W., et al.: Chaﬀ: engineering an eﬃcient SAT solver. In: Design
Automation Conference, DAC (2001)
39. Nelson, G., Oppen, D.C.: Simpliﬁcation by cooperating decision procedures. ACM
Trans. Program. Lang. Syst. 1(2), 245–257 (1979)
40. Parthasarathy, G., et al.: An eﬃcient ﬁnite-domain constraint solver for circuits.
In: 41st Design Automation Conference (2004)
41. Pelleau, M., Min´e, A., Truchet, C., Benhamou, F.: A constraint solver based on
abstract domains. In: Giacobazzi, R., Berdine, J., Mastroeni, I. (eds.) VMCAI
2013. LNCS, vol. 7737, pp. 434–454. Springer, Heidelberg (2013). doi:10.1007/
978-3-642-35873-9 26
42. Scott, J.D., Flener, P., Pearson, J.: Bounded strings for constraint programming.
In: ICTAI (2013)
43. Silva, J.P.M., Sakallah, K.A.: GRASP: a search algorithm for propositional satis-
ﬁability. IEEE Trans. Comput. 48(5), 506–521 (1999)
44. S¨ulﬂow, A., et al.: Evaluation of SAT like proof techniques for formal veriﬁcation
of word level circuits. In: 8th IEEE Workshop on RTL and High Level Testing
(2007)
45. Vemuri, R., Kalyanaraman, R.: Generation of design veriﬁcation tests from behav-
ioral VHDL programs using path enumeration and constraint programming. IEEE
Trans. VLSI Syst. 3(2), 201–214 (1995)
46. Wang, W., Søndergaard, H., Stuckey, P.J.: A bit-vector solver with word-level
propagation. In: Quimper, C.-G. (ed.) CPAIOR 2016. LNCS, vol. 9676, pp. 374–
391. Springer, Cham (2016). doi:10.1007/978-3-319-33954-2 27
47. Williams, N., Marre, B., Mouy, P.: On-the-ﬂy generation of K-path tests for C
functions. In: ASE 2004 (2004)
48. Zeng, Z., Ciesielski, M., Rouzeyre, B.: Functional test generation using constraint
logic programming. In: 11th International Conference on Very Large Scale Inte-
gration of Systems-on-Chip (2001)
49. Zeng, Z., Kalla, P., Ciesielski, M.: LPSAT: a uniﬁed approach to RTL satisﬁability.
In: 4th Conference on Design, Automation and Test in Europe (2001)

Range-Consistent Forbidden Regions
of Allen’s Relations
Nicolas Beldiceanu1
, Mats Carlsson2(B)
, Alban Derrien5
,
Charles Prud’homme1
, Andreas Schutt3,4
, and Peter J. Stuckey3,4
1 TASC (LS2N-CNRS), IMT Atlantique, Nantes, France
{Nicolas.Beldiceanu,Charles.Prudhomme}@imt-atlantique.fr
2 RISE SICS, Kista, Sweden
Mats.Carlsson@sics.se
3 Data61, CSIRO, Canberra, Australia
{Andreas.Schutt,Peter.Stuckey}@data61.csiro.au
4 University of Melbourne, Melbourne, Australia
5 Lab-STICC UMR 6285 – CNRS, Universit´e de Bretagne-Sud,
Lorient, France
Alban.Derrien@univ-ubs.fr
Abstract. For all 8192 combinations of Allen’s 13 relations between
one task with origin oi and ﬁxed length ℓi and another task with origin
oj and ﬁxed length ℓj, this paper shows how to systematically derive a
formula F(oj, oj, ℓi, ℓj), where oj and oj respectively denote the earliest
and the latest origin of task j, evaluating to a set of integers which are
infeasible for oi for the given combination. Such forbidden regions allow
maintaining range-consistency for an Allen constraint.
1
Introduction
More than 30 years ago Allen proposed 13 basic mutually exclusive relations [1]
to exhaustively characterise the relative position of two tasks. By considering all
potential disjunctions of these 13 basic relations one obtains 8192 general rela-
tions. While most of the work has been focussed on qualitative reasoning [5,8]
with respect to these general relations, and more speciﬁcally on the identiﬁcation
and use of the table of transitive relations [11], or on logical combinators involv-
ing Allen constraints [4,10], no systematic study was done for explicitly charac-
terising the set of infeasible/feasible values of task origin/length with respect to
known consistencies. In the context of range consistency the contribution of this
paper is to derive from the structure of basic Allen Relations the exact formulae
for the lower and upper bounds of the intervals of infeasible values for the 8192
general relations and to synthesised a corresponding data base [2].
After recalling the deﬁnition of Basic Allen’s relations, Sect. 2.1 gives the for-
bidden regions for these basic Allen’s relation, Sect. 2.2 unveils a regular structure
on the limits of those forbidden regions, and Sect. 2.3 shows how to systemati-
cally compute a compact normal form for the forbidden regions of all the 8192
general relations.
c
⃝Springer International Publishing AG 2017
D. Salvagnin and M. Lombardi (Eds.): CPAIOR 2017, LNCS 10335, pp. 21–29, 2017.
DOI: 10.1007/978-3-319-59776-8 2

22
N. Beldiceanu et al.
Deﬁnition 1 (Basic Allen’s relations). Given two tasks i, j respectively
deﬁned by their origin oi, oj and their length ℓi > 0, ℓj > 0, the following
13 basic Allen’s relations systematically describe relationships between the two
tasks, i.e. for two ﬁxed tasks only one basic Allen’s relation holds.
• b : oi + ℓi < oj
• m : oi + ℓi = oj
• o :
oi < oj∧
oi + ℓi > oj∧
oi + ℓi < oj + ℓj
• s : oi = oj∧
oi + ℓi < oj + ℓj
• d : oj < oi∧
oi + ℓi < oj + ℓj
• f : oj < oi∧
oi + ℓi = oj + ℓj
• e : oi = oj∧
oi + ℓi = oj + ℓj
The basic relations bi, mi, oi, si, di and ﬁare respectively derived from b, m,
o, s, d and f by permuting task i and task j. The expression i r j denotes that
the basic relation r holds between task i and task j.
Deﬁnition 2 (Allen’s constraint). Given two tasks i, j respectively deﬁned
by their origin oi, oj and their length ℓi > 0, ℓj > 0, and a basic relation r, the
Allen(r, oi, ℓi, oj, ℓj) constraint holds if and only if the condition i r j holds.
if r = b then
propagate oi + ℓi < oj
else if r = m then
propagate oi + ℓi = oj
else if . . . then
. . .
end if
Note
that
oi,
oj,
ℓi,
ℓj
are
integer
variables.
Similarly,
the
basic
relation r is an integer variable r, whose initial domain is included in
{b, bi, m, mi, o, oi, s, si, d, di, f, ﬁ, e}. This constraint could be decomposed as shown
above, but such a decomposition would propagate nothing until r has been ﬁxed,
whereas our formulae capture perfect constructive disjunction for all the 8192
general relations, e.g. for use in a range-consistency propagator.
2
Range Consistency
Given an integer variable o, D(o), o, o respectively denote the set of values,
the smallest value, the largest value that can be assigned to o. The range of a
variable o is the interval [o..o] and is denoted by R(o). A constraint ctr is range
consistent (RC) [3] if and only if, when a variable o of ctr is assigned any value
in its domain D(o), there exist values in the ranges of all the other variables of
ctr such that the constraint ctr holds.
2.1
Forbidden Regions Normal Form of Basic Allen’s Relations
For each of the 13 basic Allen’s relations column RC of Table 1 provides the
corresponding normalised forbidden regions.

Range-Consistent Forbidden Regions of Allen’s Relations
23
Table 1.
Inconsistent values for RC for the 13 basic Allen’s relations between two
tasks i and j respectively deﬁned by their origin oi, oj and their length ℓi and ℓj subject
to Allen’s relation i r j with r ∈{b, bi, . . . , e} (for reasons of symmetry we only show
the ﬁltering of task i).
Rel RC
Parameter cases
Inconsistent values
b
oi /∈[oj −ℓi.. + ∞)
bi
oi /∈(−∞..oj + ℓj]
m
oi /∈(−∞..oj −ℓi −1] ∪[oj −ℓi + 1.. + ∞)
mi
oi /∈(−∞..oj + ℓj −1] ∪[oj + ℓj + 1.. + ∞)
o
ℓi > 1 ∧ℓj > 1 ∧ℓi ≤ℓj:
ℓi > 1 ∧ℓj > 1 ∧ℓi > ℓj:
ℓi = 1 ∨ℓj = 1:
oi /∈(−∞..oj −ℓi] ∪[oj.. + ∞)
oi /∈(−∞..oj −ℓi] ∪[oj + ℓj −ℓi.. + ∞)
oi /∈(−∞.. + ∞)
oi
ℓi > 1 ∧ℓj > 1 ∧ℓi ≤ℓj:
ℓi > 1 ∧ℓj > 1 ∧ℓi > ℓj:
ℓi = 1 ∨ℓj = 1:
oi /∈(−∞..oj + ℓj −ℓi] ∪[oj + ℓj.. + ∞)
oi /∈(−∞..oj] ∪[oj + ℓj.. + ∞)
oi /∈(−∞.. + ∞)
s
ℓi < ℓj:
ℓi ≥ℓj:
oi /∈(−∞..oj −1] ∪[oj + 1.. + ∞)
oi /∈(−∞.. + ∞)
si
ℓj < ℓi:
ℓj ≥ℓi:
oi /∈(−∞..oj −1] ∪[oj + 1.. + ∞)
oi /∈(−∞.. + ∞)
d
ℓi + 1 < ℓj:
ℓi + 1 ≥ℓj:
oi /∈(−∞..oj] ∪[oj + ℓj −ℓi.. + ∞)
oi /∈(−∞.. + ∞)
di
ℓj + 1 < ℓi:
ℓj + 1 ≥ℓi:
oi /∈(−∞..oj + ℓj −ℓi] ∪[oj.. + ∞)
oi /∈(−∞.. + ∞)
f
ℓi < ℓj:
ℓi ≥ℓj:
oi /∈(−∞..oj + ℓj −ℓi −1] ∪[oj + ℓj −ℓi +
1.. + ∞)
oi /∈(−∞.. + ∞)
ﬁ
ℓj < ℓi:
ℓj ≥ℓi:
oi /∈(−∞..oj + ℓj −ℓi −1] ∪[oj + ℓj −ℓi +
1.. + ∞)
oi /∈(−∞.. + ∞)
e
ℓj = ℓi:
ℓj ̸= ℓi:
oi /∈(−∞..oj −1] ∪[oj + 1.. + ∞)
oi /∈(−∞.. + ∞)
Lemma 1. (a) The correct and complete forbidden region for oi + ℓ< oj is
oi ̸∈[oj −ℓ.. + ∞). (b) The correct and complete forbidden region for oj + ℓ< oi
is oi ̸∈(−∞..oj + ℓ].
Proof. (a) Given oi+ℓ< oj then clearly oi < oj −ℓand hence oi ̸∈[oj −ℓ..+∞).
Given v < oj −ℓthen oi = v, oj = oj is a solution of the constraint. (b) Given
oj +ℓ< oi then clearly oi > oj +ℓand hence oi ̸∈(−∞..oj +ℓ]. Given v < oj +ℓ
then oi = v, oj = oj is a solution of the constraint.
□

24
N. Beldiceanu et al.
Lemma 2. Given constraint c ≡c1 ∧c2 if oi ̸∈R1 is a correct forbidden region
of c1 and oi ̸∈R2 is a correct forbidden region of c2 then oi ̸∈R1 ∪R2 is a
correct forbidden region of c.
Proof. Since there can be no solution of c1 with oi ∈R1 and no solution of c2
with oi ∈R2 there can be no solution of c with oi ∈R1 ∪R2.
□
Theorem 1. Forbidden intervals of consecutive values shown in column RC of
Table 1 are correct and complete.
Proof. We proceed by cases and omit relations bi, mi, oi, si, di, ﬁfor which the
reasoning is analogous to b, m, o, s, d, f.
b
Follows from Lemma 1(a).
m Correctness: given i meets j then oi + ℓi = oj thus oi + ℓi ≤oj ∧oi + ℓi ≥oj
thus oi + (ℓi −1) < oj ∧oj + (−ℓi −1) < oi. From Lemma 1 we have that
[oj −ℓi + 1.. + ∞) and (−∞..oj −ℓi −1] are correct forbidden regions and by
Lemma 2 correctness holds. Completeness: choose v ∈[oj −ℓi..oj −ℓi] then
oi = v, oj = v + ℓi is a solution.
o
Correctness: given i overlaps with j we have oi < oj ∧oi + ℓi > oj ∧oi + ℓi <
oj + ℓj. Suppose ℓi = 1 then this implies oi < oj ∧oi + 1 > oj contradiction,
or suppose ℓj = 1 then this implies oi +ℓi > oj ∧oi +ℓi < oj +1 contradiction
hence (−∞.. + ∞) is a correct forbidden region. Lemma 1 gives us correct
forbidden regions (−∞..oj −ℓi], [oj + ℓj −ℓi.. + ∞), [oj.. + ∞). If ℓi ≤ℓj
this is equivalent to (−∞..oj −ℓi] ∪[oj.. + ∞). If ℓi > ℓj this is equivalent to
(−∞..oj −ℓi] ∪[oj + ℓi −ℓj.. + ∞). Completeness: when ℓi = 1 or ℓj = 1 then
completeness follows from the contradiction. Choose v ∈[oj −ℓi + 1..oj +
min(0, ℓj −ℓi) −1] then oi = v, oj = v + ℓi −1 is a solution.
s
Correctness: If ℓi ≥ℓj then the constraints are unsatisﬁable and (−∞..+∞)
is a correct forbidden region. Otherwise from s we have that oi < oj +1∧oi >
oj −1 ∧oi + ℓi < oj + ℓj and Lemma 1 gives us correct forbidden regions
[oj + 1.. + ∞), (−∞..oj −1] and [oj + ℓj −ℓi.. + ∞). If ℓi < ℓj then this gives
(−∞..oj−1]∪[oj+1..+∞). Completeness: If ℓi ≥ℓj then completeness follows
from the unsatisﬁability. Otherwise choose v ∈[oj..oj] then oi = v, oj = v is
a solution.
d
Correctness: If ℓi + 1 ≥ℓj then oi + ℓi ≥oi + ℓj −1 ≥oj + ℓj but this
contradicts oi + ℓi < oj + ℓj hence (−∞.. + ∞) is a correct forbidden region.
Otherwise Lemma 1 gives us correct forbidden regions (−∞..oj] and [oj +
ℓj −ℓi.. + ∞) whose union is the correct forbidden region. Completeness:
The contradiction proves completeness when ℓi + 1 ≥ℓj. Otherwise choose
v ∈[oj + 1..oj + ℓj −ℓi −1] then oi = v, oj = v −1 is a solution.
f
Correctness: Suppose ℓi ≥ℓj then oj < oi = oj +ℓj −ℓi ≤oj, a contradiction,
hence (−∞.. + ∞) is a correct forbidden region. Otherwise oj + ℓj = oi + ℓi
is equivalent to oj + ℓj −1 < oi + ℓi ∧oj + ℓj + 1 > oi + ℓi. From these two
inequalities and from oj < oi, Lemma 1 gives us correct forbidden regions
(−∞..oj + ℓj −ℓi −1], [oj + ℓj −ℓi + 1.. + ∞) and (−∞..oj]. Since ℓi < ℓj the

Range-Consistent Forbidden Regions of Allen’s Relations
25
correct union is (−∞..oj +ℓj −ℓi −1]∪[oj +ℓj −ℓi +1..+∞). Completeness:
If ℓi ≥ℓj then the contradiction gives the completeness. Otherwise choose
v ∈[oj + ℓj −ℓi..oj + ℓj −ℓi] then oi = v, oj = v + ℓi −ℓj is a solution.
e
Correctness: Suppose ℓi ̸= ℓj then the constraints oi = oj ∧oi + ℓi = oj + ℓj
contradict and (−∞.. + ∞) is a correct forbidden region. When ℓi = ℓj
Lemma 1 gives us correct forbidden regions (−∞..oj −1], [oj + 1.. + ∞) from
both constraints, and their union is the correct answer. Completeness: If ℓi ̸=
ℓj then the contradiction proves completeness, otherwise choose v ∈[oj..oj]
then oi = v, oj = v is a solution.
□
2.2
Structure of the Normalised Forbidden Regions
All forbidden regions of the basic Allen’s relations given in Sect. 2.1 consist of one
or two intervals of the form (−∞..up], [low.. + ∞) or (−∞.. + ∞). Indeed, only
the forbidden regions for b and bi consist of a single (nonuniversal) forbidden
region. In the following, we call upper limit (resp. lower limit) the terms up
(resp. low). In the case of a single universal forbidden region, up = +∞and
low = −∞.
We show that all upper limits (resp. lower limits) can be totally ordered
provided we know the relative order between the lengths ℓi and ℓj of the corre-
sponding tasks. This is because all upper limits (resp. lower limits) correspond to
linear expressions involving +oj (resp. +oj). Figure 1 illustrates this for the case
ℓi < ℓj, where each limit is a node mentioning the associated formula, the basic
Allen’s relation(s) from which it is generated and the restriction on the parame-
ters. We also show that we always have that the kth upper limit is strictly less
than the k + 1th lower limit. This is because the kth upper limit and the k + 1th
lower limit are issued from the same basic Allen’s relation. Within Fig. 1 a solid
arrow from a start node to an end node indicates that the limit attached to the
start node is necessarily strictly less than (resp. strictly less by one than) the
limit attached to the end node.
Fig. 1. Ordering the upper limits (resp. lower limits) of the forbidden regions of a
general Allen relation R depending on the relative length of the two tasks i and j
when ℓi < ℓj; a solid arrow from a limit x to a limit y represents an inequality of
the form x < y, while a dashed arrow represents an inequality of the form x + 1 < y.
Upper (resp. lower) limits of each of the three cases are identiﬁed by a unique identiﬁer
located on the corresponding lower rightmost corner.

26
N. Beldiceanu et al.
2.3
Normal Form for the Forbidden Regions
Given any general Allen’s relation R we now show how to synthesise a normalised
sequence of forbidden regions for this relation under the diﬀerent cases regarding
the relative sizes of the two tasks to which R applies (i.e., ℓi < ℓj, ℓi = ℓj,
ℓi > ℓj). This will lead to a data base [2] of normalised forbidden regions for the
8192 general relations. A typical entry of that data base, for instance for relation
{b, bi, d, di, e, f, ﬁ, m, mi, si}, looks like:
⎧
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎩
[oj −ℓi + 1..oj] ∪[oj + ℓj −ℓi + 1..oj + ℓj −1]
if ℓi < ℓj ∧ℓi > 1
[oj..oj]
if ℓi = 1 ∧ℓj > 1
∅
if ℓj = 1
[oj −ℓi + 1..oj + ℓj −ℓi −1] ∪[oj + 1..oj + ℓj −1]
if ℓi ≥ℓj ∧ℓi > 1 ∧ℓj > 1
(1)
Each case consists of a normalised sequence of forbidden regions F and of
a condition C involving the lengths of the tasks; such a case will be denoted
as (F if C). Generating such cases is done by using the normalised forbidden
regions of the 13 Allen’s basic relations given in column RC of Table 1, as well
as the strong ordering structure between the limits (see Fig. 1) of these forbidden
regions we identiﬁed in Sect. 2.2 in three steps as follows.
1. Extracting the lower/upper limits of forbidden regions of basic
Allen’s Relations in R.
(a) First, we ﬁlter from the considered general Allen’s relation R those basic
relations which are neither mentioned in the upper nor in the lower limits
of the forbidden regions attached to the relevant case (i.e., ℓi < ℓj, ℓi = ℓj,
ℓi > ℓj). This is because such Allen’s basic relations generate one single
forbidden region of the form (−∞.. + ∞) and can be therefore removed
from the disjunction. For the same reason, we also ﬁlter from R those
basic relations for which the parameter restriction does not hold.
(b) Second, we group together the set of restrictions attached to the remaining
basic Allen’s relations. This leads to a set of restrictions in {⊤, ℓi > 1 ∧
ℓj > 1, ℓi + 1 < ℓj, ℓi > ℓj + 1}. For those restrictions diﬀerent from ⊤
we consider all possible combinations where each relation holds or does
not hold. When the relation does not hold we remove the corresponding
Allen’s basic relation for the same reason as before. This gives us a number
of cases for which we will generate the forbidden regions using the next
steps. Since to each lower limit correspond an upper limit we remain with
n lower limits low αk (with 0 ≤α1 < α2 < · · · < αn ≤n) and n upper
limits upβk (with 1 ≤β1 < β2 < · · · < βn ≤n + 1). A case for which
n = 0 means a full forbidden region (−∞.. + ∞).
2. Combining the limits of forbidden regions of basic Allen’s rela-
tions to get the forbidden regions of R.
Second, the forbidden regions of the considered general Allen’s relation R are
given by 
k∈[1,n]|αk̸=βk[low αk..upβk].

Range-Consistent Forbidden Regions of Allen’s Relations
27
3. Removing empty forbidden regions of R.
Using the following steps, we eliminate from 
k∈[1,n]|αk̸=βk[low αk..upβk] the
intervals that are necessarily empty when the origin of task j is ﬁxed.
(a) if ℓi < ℓj ∧ℓi = 1 then eliminate [low..up] such that low (resp. up)
is attached to a limit associated with m (resp. s). In the following, for
simplicity, we just say eliminate [m,s]. Similarly we eliminate [f,mi].
(b) if ℓi < ℓj ∧ℓi + 1 = ℓj then eliminate [s,f].
(c) if ℓi = ℓj ∧ℓi = 1 then eliminate [m,e] and [e,mi].
(d) if ℓi > ℓj ∧ℓj = 1 then eliminate [m,ﬁ] and [si,mi].
(e) if ℓi > ℓj ∧ℓj + 1 = ℓi then eliminate [ﬁ,si].
We now show that the previous three steps procedure generates a symbolic
normal form for the forbidden regions of a general relation R.
Lemma 3. For a general relation R by systematically combining the three cases
ℓi < ℓj, ℓi = ℓj, ℓi > ℓj with all possible restrictions from {⊤, ℓi > 1 ∧ℓj > 1,
ℓi + 1 < ℓj, ℓi > ℓj + 1} we generate all possible cases for that relation R.
Proof. The Cartesian product of {ℓi < ℓj, ℓi = ℓj, ℓi > ℓj}×{⊤}×{ℓi > 1∧ℓj >
1} × {ℓi + 1 < ℓj} × {ℓi > ℓj + 1} is considered.
□
Lemma 4. For a general relation R consider one of its case generated in step 1
and the corresponding limits low αk and upβk. The forbidden regions of R are
given by 
k∈[1,n]|αk̸=βk[low αk..upβk].
Proof. A forbidden region of R is an interval of consecutive values that are
forbidden for all basic relations of R. Since both the lower limits low αk and the
upper limits upβk are sorted in increasing order, and since [low p..upq] = ∅for
all p ≥q we pick up for each start of a forbidden region low αk the smallest end
upβk of the forbidden region that was starting before low αk.
□
Lemma 5. When oj is ﬁxed the intervals removed by step 3 are the only empty
intervals [low p..upq] where p < q.
Proof. The other cases being similar we only show the proof for the lower limit
oj −ℓi + 1 that was generated from m when ℓi < ℓj.
– Within the case ℓi < ℓj, oj −ℓi + 1 is the lower limit of index 2 in Fig. 1.
Consequently we ﬁrst look at the upper limit of index 3, namely oj −1 that
was generated from s. Since we want to check when oj −ℓi +1 will be strictly
greater than oj −1 when oj is ﬁxed, we get oj −ℓi+1 > oj −1, which simpliﬁes
to −ℓi +1 > −1 and to ℓi ≤1, which means that we can eliminate [m,s] when
ℓi = 1.
– We now need to compare oj −ℓi + 1 with the next upper limit, namely the
upper limit of index 4, i.e. oj. We get oj −ℓi + 1 > oj, which simpliﬁes to
ℓi < 1 which is never true. Consequently the interval [m, d] is not empty when
oj is ﬁxed. This implies that the other intervals [m, f], [m, oi], [m, mi], [m, bi]
are also not empty when oj is ﬁxed since their upper limit are located after
the upper limit of index 4.
□

28
N. Beldiceanu et al.
Example 1. Assuming ℓi < ℓj we successively illustrate how to generate the
normalised forbidden regions for the relation R1 = {b, m, mi, bi} (i.e. nonover-
lapping), for R2 = {b, m}, and for R3 = {b, s, bi}.
1. By keeping the limits related to the basic relations b, m, mi, bi of R1 we get
α1 = 1, α2 = 2, α3 = 8 and β1 = 1, β2 = 7, β3 = 8. Since α1 = β1 and α3 = β3
we only keep α2 and β2 and get the interval [low α2..upβ2] = [low 2..up7] =
[oj −ℓi + 1..oj + ℓj −1], the expected result for a nonoverlapping constraint
between two tasks.
2. By keeping the limits related to the basic relations b, m of R2 we get α1 = 1,
α2 = 2 and β1 = 1, β2 = 9. Since α1 = β1 we only keep α2 and β2 and get
the interval [low α2..upβ2] = [low 2..up9] = [oj −ℓi + 1.. + ∞).
3. By keeping the limits related to the basic relations b, s, bi of R3 we get α1 = 1,
α2 = 4 and β1 = 3, β2 = 8, which leads to [low α1..upβ1] ∪[low α2..upβ2] =
[low 1..up3] ∪[low 4..up8] = [oj −ℓi..oj −1] ∪[oj + 1..oj + ℓj].
Merging Similar Cases. For a given Allen’s general relation R, two cases
(D1 if C1) and (D2 if C2) can be merged to a single case (D12 if C12) if the
following conditions all hold:
– C12 is equivalent to C1∨C2 and can be expressed as a conjunction of primitive
restrictions.
– D1, D2, and D12 consist of the same number of intervals.
– For every interval [b1, u1] ∈D1 there are intervals [b2, u2] ∈D2 and [b12, u12] ∈
D12 at the same position such that:
• b1 = b12 and u1 = u12, for any values taken by ℓi and ℓj such that C1
holds.
• b2 = b12 and u2 = u12, for any values taken by ℓi and ℓj such that C2
holds.
We used a semi-automatic approach to discover such endpoint generalisation
rules. For every Allen’s general relation, using these rules, we identiﬁed and
merged pairs and triples of cases until no more merging was possible. As the
result of this process, the data base [2] consists of 32396 cases covering all the
8192 general relations. In this data base, the maximum number of intervals for
a case is 5, the average number of intervals is 2.14 and the median is 2.
3
Conclusion
This work belongs to the line of work that tries to synthesise in a systematic way
constraint propagators for speciﬁc classes of constraints [6,7,9]. Future work may
generalise this for getting a similar normal form for other families of qualitative
constraints.
Acknowledgment. The Nantes authors were partially supported both by the INRIA
TASCMELB associated team and by the GRACeFUL project, which has received
funding from the European Union’s Horizon 2020 research and innovation programme
under grant agreement No 640954.

Range-Consistent Forbidden Regions of Allen’s Relations
29
References
1. Allen, J.F.: Maintaining knowledge about temporal intervals. Commun. ACM
26(11), 832–843 (1983)
2. Beldiceanu, N., Carlsson, M., Derrien, A., Schutt, A., Stuckey, P.J.: Range-
consistent forbidden regions of Allen’s relations. Technical report T2016-2, Swedish
Institute of Computer Science (2016). http://soda.swedishict.se
3. Bessi`ere, C.: Constraint propagation. In: Rossi, F., van Beek, P., Walsh, T. (eds.)
Handbook of Constraint Programming, chap. I.3, pp. 29–83. Elsevier (2006)
4. Derrien, A., Fages, J.-G., Petit, T., Prud’homme, C.: A global constraint
for a tractable class of temporal optimization problems. In: Pesant, G. (ed.)
CP 2015. LNCS, vol. 9255, pp. 105–120. Springer, Cham (2015). doi:10.1007/
978-3-319-23219-5 8
5. Gennari, R., Mich, O.: E-Learning and deaf children: a logic-based web tool. In:
Leung, H., Li, F., Lau, R., Li, Q. (eds.) ICWL 2007. LNCS, vol. 4823, pp. 312–319.
Springer, Heidelberg (2008). doi:10.1007/978-3-540-78139-4 28
6. Gent, I.P., Jeﬀerson, C., Linton, S., Miguel, I., Nightingale, P.: Generating custom
propagators for arbitrary constraints. Artif. Intell. 211, 1–33 (2014)
7. Lauri`ere, J.-L.: Constraint propagation or automatic programming. Techni-
cal report 19, IBP-Laforia (1996). In French. https://www.lri.fr/∼sebag/Slides/
Lauriere/Rabbit.pdf
8. Ligozat, G.: Towards a general characterization of conceptual neighborhoods in
temporal and spatial reasoning. In: AAAI 1994 Workshop on Spatial and Temporal
Reasoning (1994)
9. Monette, J.-N., Flener, P., Pearson, J.: Towards solver-independent propagators.
In: Milano, M. (ed.) CP 2012. LNCS, pp. 544–560. Springer, Heidelberg (2012).
doi:10.1007/978-3-642-33558-7 40
10. Roy, P., Perez, G., R´egin, J.-C., Papadopoulos, A., Pachet, F., Marchini, M.:
Enforcing structure on temporal sequences: the allen constraint. In: Rueher, M.
(ed.) CP 2016. LNCS, vol. 9892, pp. 786–801. Springer, Cham (2016). doi:10.1007/
978-3-319-44953-1 49
11. van Beek, P., Manchak, D.W.: The design and experimental analysis of algorithms
for temporal reasoning. J. Artif. Intell. Res. (JAIR) 4, 1–18 (1996)

MDDs are Eﬃcient Modeling Tools:
An Application to Some Statistical Constraints
Guillaume Perez and Jean-Charles R´egin(B)
I3S, CNRS, Universit´e Nice-Sophia Antipolis, Sophia Antipolis, France
guillaume.perez06@gmail.com, jcregin@gmail.com
Abstract. We show that from well-known MDDs like the one modeling
a sum, and operations between MDDs we can deﬁne eﬃcient propagators
of some complex constraints, like a weighted sum whose values satisfy a
normal law. In this way, we avoid deﬁning ad-hoc ﬁltering algorithms. We
apply this idea to diﬀerent dispersion constraints and on a new statistical
constraint we introduce: the Probability Mass Function constraint. We
experiment out approach on a real world application. The conjunction
of MDDs clearly outperforms all previous methods.
1
Introduction
Several constraints, like spread [16], deviation [21–24], balance [3,5] and
dispersion [17], have mainly been deﬁned to balance certain features of a solu-
tion. For example, the balanced academic curriculum problem [1] involves courses
that have to be assigned to periods so as to balance the academic load between
periods. Most of the time the mean of the variables is ﬁxed and the goal is to
minimize the standard deviation, the distance or the norm.
The dispersion constraint is a generalization of the deviation and spread
constraints. It ensures that X, a set of variables, has a mean (i.e. μ = 
x∈X x)
belonging to a given interval and Δ a norm (i.e. 
x∈X(x −μ)p) belonging to
another given interval. If p = 1 then it is a deviation constraint and p = 2
deﬁnes a spread constraint. Usually, the goal is to minimize the value of Δ or
ﬁnd a value below a given threshold.
In some problems, variables are independent from a probabilistic point of
view and are associated with a distribution (e.g. a normal law) that speciﬁes
probabilities for their values. Thus, globally the values taken by the variables
have to respect that law and we can deﬁne a constraint ensuring this property,
either by using a spread, a dispersion, a KolmogorovSmirnov or a Student’s
t-test constraint [19]. However, if only a subset of variables is involved in a
constraint, then the values taken by these variables should be compatible with
the distribution (e.g. the normal law), but we cannot impose the distribution
for a subset of values because this is a too strong constraint. Therefore, we
need to consider interval of values for μ and Δ. The deﬁnition of an interval
for μ can be done intuitively. For instance we can consider an error rate of
10%. Unfortunately, this is not the case for Δ. It is hard to control the relation
c
⃝Springer International Publishing AG 2017
D. Salvagnin and M. Lombardi (Eds.): CPAIOR 2017, LNCS 10335, pp. 30–40, 2017.
DOI: 10.1007/978-3-319-59776-8 3

MDDs are Eﬃcient Modeling Tools
31
between two values of Δ, because data are coming from measures and there are
some errors and because it is diﬃcult to apply a continuous law on ﬁnite set
of values. Since we use constraint programming solvers we have to make sure
that we do not forbid tuples that could be acceptable. This is why, in practice,
the problem is not deﬁned in term of μ and Δ but by the probability mass
function (PMF). The probability mass function gives the probability that Xr,
a discrete random variable, is exactly equal to some value. In other words, if
Xr takes its values in V , then the PMF gives the probability of each value
of V . The PMF is usually obtained from the histogram of the values. From
fP , a PMF, we can compute the probability of any tuple by multiplying the
probability of the values it contains, because variables are independent. Then,
we can avoid outliers of the statistical law but imposing that the probability of
a tuple belongs to a given interval [Pmin, Pmax]. With such a constraint we can
select a subset of values from a large set having a mean in a given interval while
avoiding outliers of the statistical law. Roughly, the minimum probability avoids
having tuples with values having only very little chance to be selected and the
maximum probability avoids having tuples whose values have only the strongest
probability to be selected. Thus, we propose to deﬁne the PMF constraint, a new
statistical constraint from μ, fP , Pmin and Pmax.
Since we deﬁne a new constraint we need to deﬁne a propagator for it. Instead
of designing an ad-hoc propagator we propose to represent the constraint by
an MDD and to use MDD propagators, like MDD4R [13], for establishing arc
consistency of the constraint. MDDs of constraints can also be intersected in
order to represent the combination of the constraints and MDD operators applied
on them. Recent studies show that such combinations give excellent results in
practice [20].
Pesant has proposed a speciﬁc algorithm for ﬁltering the dispersion con-
straint. His propagator establishes domain consistency on the X variables. Unfor-
tunately, it is ad-hoc and so it cannot be easily combined with other constraints.
Thus, we propose to also use MDD propagators for the classical version of the
dispersion constraint.
The advantage of using MDDs representing the constraints in these cases, is
that these MDDs are deﬁned on sum constraints which are well-known MDDs.
We tested all propagators on a part of a real world application mainly involv-
ing convolutions which is expressed by a knapsack constraint (i.e.  αixi). The
results show the advantage of our generic approach.
The paper is organized as follows. First, we recall some basics about MDDs,
MDD propagators and the dispersion constraint. Then, we introduce simple
models using MDDs for modelling the dispersion constraint with a ﬁxed or a
variable mean, and we show how we can combine them in order to obtain only
one MDD. Next, we present the PMF constraint and show how it can be repre-
sented by an MDD and ﬁltered by MDD propagators. We give some experiments
supporting our approach. At last, we conclude.

32
G. Perez and J.-C. R´egin
2
Preliminaries
Multi-valued decision diagram (MDD). An MDD is a data-structure rep-
resenting discrete functions. It is a multiple-valued extension of BDDs [6]. An
MDD, as used in CP [2,4,7,9–11,13], is a rooted directed acyclic graph (DAG)
used to represent some multi-valued function f : {0...d −1}r →{true, false},
based on a given integer d. Given the r input variables, the DAG representation
is designed to contain r + 1 layers of nodes, such that each variable is repre-
sented at a speciﬁc layer of the graph. Each node on a given layer has at most d
outgoing arcs to nodes in the next layer of the graph. Each arc is labeled by its
corresponding integer. The arc (u, v, a) is from node u to node v and labeled by
a. All outgoing arcs of the layer r reach the true terminal node (the false terminal
node is typically omitted). There is an equivalence between f(a1, ..., ar) = true
and the existence of a path from the root node to the true terminal node whose
arcs are labeled a1, ..., ar (Fig. 1).
Fig. 1. An MDD of the tuple set {(a,a),(a,b),(c,a),(c,b),(c,c)}. For each tuple, there
is a path from the root node (node 0) to the terminal node (node tt) whose arcs are
labeled by the tuple values.
MDD of a constraint. Let C be a constraint deﬁned on X(C). The MDD
associated with C, denoted by MDD(C), is an MDD which models the set of
tuples satisfying C. More precisely, MDD(C) is deﬁned on X(C), such that layer
i corresponds to the variable xi and the labels of arcs of the layer i correspond to
values of xi, and a path of MDD(C) where ai is the label of layer i corresponds
to a tuple (a1, ..., ar) on X(C).
Consistency with MDD(C). A value a of the variable x is valid iﬀa ∈D(x),
where D(x) is the possible values of the variable x. An arc (u, v, a) at layer i is
valid iﬀa ∈D(xi). A path is valid iﬀall its arcs are valid.
Let paths
tt(MDD(C)) be the set of paths from s, the root node, to tt in
MDD(C). The value a ∈D(xi) is consistent with MDD(C) iﬀthere is a valid
path in paths
tt(MDD(C)) which contains an arc at layer i labeled by a.
MDD propagator. An MDD propagator associated with a constraint C is an
algorithm which removes some inconsistent values of X(C). The MDD propa-
gator establishes arc consistency of C if and only if it removes all inconsistent

MDDs are Eﬃcient Modeling Tools
33
values with MDD(C). This means that it ensures that there is a valid path from
the root to the true terminal node in MDD(C) if and only if the corresponding
tuple is allowed by C and valid.
Cost-MDD. A cost-MDD is an MDD whose arcs have an additional informa-
tion: the cost c of the arc. That is, an arc is a 4-uplet e = (u, v, a, c), where u
is the head, v the tail, a the label and c the cost. Let M be a cost-MDD and p
be a path of M. The cost of p is denoted by γ(p) and is equal to the sum of the
costs of the arcs it contains.
Cost-MDD of a constraint [9]. Let C be a constraint and fC be a function
associating a cost with each value of each variable of X(C). The cost-MDD of
C and fC is denoted by cost-MDD(C, fC) and is MDD(C) whose the cost of an
arc labeled by a at layer i is fC(xi, a).
Cost-MDD propagator [8,15]. A cost-MDD propagator associated with C,
fC, a value H, and a symbol ≺(which can be ≤or ≥) is an MDD propagator
on MDD(C) which ensures that for each path p of cost-MDD(C, fC) we have
γ(p) ≺H. A cost-MDD propagator establishes arc consistency of C iﬀeach arc of
cost-MDD(C) belongs to p a valid path of paths
tt(cost-MDD(C)) with γ(p) ≺H.
MDD of a Generic Sum Constraint [25]. We deﬁne the generic sum con-
straint Σf,[a,b](X) which is equivalent to a ≤
xi∈X f(xi) ≤b, where f is a non
negative function. The MDD of the constraint 
xi∈X f(xi) is deﬁned as follows.
For the layer i, there are as many nodes as there are values of i
k=1 f(xk).
Each node is associated with such a value. A node np at layer i associated with
value vp is linked to a node nq at layer i + 1 associated with value vq if and
only if vq = vp + f(ai) with ai ∈D(xi). Then, only values v of the layer |X|
with a ≤v ≤b are linked to tt. The reduction operation is applied after the
deﬁnition and delete invalid nodes [14]. The construction can be accelerated by
removing states that are greater than b or that will not permit to reach a. For
convenience, Σid,[α,α](X) is denoted by Σα(X).
0
3
3
7
7
10
7
6
3
3
14
7
17
7
13
3
3
7
20
3
7
Fig. 2. MDD of the  xi = nμ constraint

34
G. Perez and J.-C. R´egin
Figure 2 is an example of MDD(Σ20(X)) with {3, 7} as domains. Since fC in
non negative, the number of nodes at each layer of MDD(Σf,[a,b](X)) is bounded
by b.
Dispersion Constraint [17]. Given X = {x1, ..., xn}, a set of ﬁnite-domain
integer variables, μ and Δ, bounded-domain variables and p a natural number.
The constraint dispersion(X, μ, Δ, p) states that the collection of values taken
by the variables of X exhibits an arithmetic mean μ = n
i=1 xi and a deviation
Δ = n
i=1 |xi −μ|p.
The deviation constraint is a dispersion constraint with p = 1 and a
spread constraint is a dispersion constraint with p = 2.
The main complexity of the dispersion constraint is the relation between
μ and Δ variables, because μ is deﬁned from the X variables, and Δ is deﬁned
from X and from μ. So, some information is lost when these two deﬁnitions are
considered separately. However, when μ is assigned, the problem becomes simpler
because we can independently consider the deﬁnitions of μ and Δ. Therefore,
we propose to study some models depending on the fact that μ is ﬁxed or not.
3
Dispersion Constraint with Fixed Mean
Arc consistency for the X variables has been established by Pesant [17], who
proposed an ad-hoc dynamic programming propagator for this constraint. How-
ever, it exists a simpler method avoiding such problems of ad-hoc algorithms:
we deﬁne a cost-MDD from μ and Δ and obtain a propagator having the same
complexity.
3.1
MDD on μ and Δ as Cost
The mean μ is deﬁned as a sum constraint. Since μ is ﬁxed, we propose to use
the cost-MDD of the constraint  xi = nμ and the cost function deﬁned by Δ.
The constraint  xi = nμ can be represented by MDD(Σnμ(X)).
Δ as cost. We represent the dispersion constraint by cost-MDD(Σμ(X), Δ).
There are two possible ways to deal with the boundaries of Δ. Either we
deﬁne two cost-MDD propagators on cost-MDD(Σμ(X), Δ), one with a and
≥, and one with b and ≤; or we deﬁne only one cost-MDD propagator on cost-
MDD(Σμ(X), Δ) which integrates the costs at the same time as proposed by
Hoda et al. [11].
These methods are simpler than Pesant’s algorithm because they do not
require to develop any new algorithm. If we use an eﬃcient algorithm [15] for
maintaining arc consistency for cost-MDDs then we obtain the the same worst
case complexity as Pesant’s algorithm but better result in practice.
3.2
MDD on μ Intersected with MDD on Δ
Since μ is ﬁxed, then the deﬁnition of Δ corresponds to a generic sum as pre-
viously deﬁned. Thus, the dispersion constraint can be model by deﬁning the

MDDs are Eﬃcient Modeling Tools
35
MDD of Σμ(X) and the MDD of ΣΔ,[Δ,Δ](X) and then by intersecting them.
Replacing a cost-MDD by the intersection of two MDDs may strongly improve
the computational results [15]. In addition, we can intersect the resulting MDD
with some other MDDs in order to combine more the constraints. This method
is the ﬁrst method establishing arc consistency for both μ and Δ. The drawback
is the possible size of the intersection.
With similar models we can also give an eﬃcient implementation of the
Student’s t-test constraint and close the open question of Rossi et al. [19].
4
Dispersion Constraint with Variable Mean
In order to deal with a variable mean, we can consider all acceptable values
for nμ, that is the integers in [n⌊μ⌋, n⌈μ⌉], and for each value we separately
apply the previous models for the ﬁxed mean. Unfortunately, this often leads
to a large number of constraints. Therefore it is diﬃcult to use this approach
in practice. In addition, note that there is no advantage in making the union of
these constraints because they are independent.
Thus, we propose another model using the probability mass function.
5
Probability Mass Function (PMF) Constraint
In this section we deﬁne the PMF constraint which aims at respecting a variable
mean and avoiding outliers according to a statistical law given by a probability
mass function.
Given a discrete random variable X taking values in X = {v1, ...vm} its
probability mass function P: X →[0, 1] is deﬁned as P(vi) = Pr[X = vi] and
satisﬁes the following condition: P(vi) ≥0 and m
i=1 P(vi) = 1
The PMF gives for each value v, P(v) the probability that v is taken. Let fP
be a PMF and consider a set of variables independent from a probabilistic point
of view and associated with fP that speciﬁes probabilities for their values. Since
the variables are independent, we can deﬁne the probability of an assignment of
all the variables (i.e. a tuple) as the product of the probabilities of the assigned
values. Then, in order to avoid outliers we can constrain this probability to be
in a given interval.
Deﬁnition 1. Given a set of ﬁnite-domain integer variables X = {x1, x2, ..., xn}
that are independent from a probabilistic point of view, a probability
mass function fP , a bounded variable μ (not necessarily ﬁxed), a mini-
mum probability Pmin and a maximum probability Pmax. The constraint
PMF(X, fP , μ, Pmin, Pmax) states that the probabilities of the values taken by
the variables of X is speciﬁed by fP , the collection of values taken by the vari-
ables of X exhibits an arithmetic mean μ and that Πxi∈Xxi the probability of
any allowed tuple satisﬁes Pmin ≤Πxi∈XfP (xi) ≤Pmax.

36
G. Perez and J.-C. R´egin
This constraint can be represented by cost-MDD(Σid,[μ,μ](X), logP) where
logP is the logarithm of the PMF that is logP(x) = log(fP (x)). We take the
logarithm because in this way we have a sum function instead of a product
function: log(ΠfP (xi)) =  log(fP (x)) =  logP(x). Then, we deﬁne a cost-
MDD propagator on cost-MDD(Σid,[μ,μ](X),logP with log(Pmin) and ≥and with
log(Pmax) and ≤.
6
Experiments
The experiments were run on a macbook pro (2013) Intel core i7 2.3 GHz with
8 Go. The constraint solver used is or-tools. MDD4R [13] is used as MDD prop-
agator and cost-MDD4R as cost-MDD propagator [15].
The data come from a real life application: the geomodeling of a petroleum
reservoir [12]. The problem is quite complex and we consider here only a sub-
part. Given a seismic image we want to ﬁnd the velocities. Velocities values are
represented by a probability mass function (PMF) on the model space. Veloci-
ties are discrete values of variables. For each cell cij of the reservoir, the seismic
image gives a value sij and the from the given seismic wavelet (αk) we deﬁne a
sum constraint 22
k=1 αklog(xi−11+k−1j) = sij. Locally, that is for each sum, we
have to avoid outliers w.r.t. the PMF for the velocities. Globally we can use the
classical dispersion constraint. The problem is huge (millions of variables) so we
consider here only a very small part.
The ﬁrst experiment involves 22 variables and a constraint Cα: n
i=1 αixi =
I, where I is an tight interval (i.e. a value with an error variation). Cα is repre-
sented by mddα = MDD(Σai,I(X)) where ai(xi) = αixi.
First, we impose that the variables have to be distributed with respect to a
normal distribution with μ, a ﬁxed mean.
Mσ<,σ> represents the model of Sect. 3.2: one cost-MDD propagator on
mddμ = cost-MDD(Σnμ(X), σ) with σ and ≤and one with σ and ≥. This
model is similar to Pesant’s model.
MGCC involves a GCC constraint [18] where the cardinalities are extracted
from the probability mass function.
Mμ∩σ represents the mean constraint by mddσ = MDD(Σnμ(X)). It repre-
sents the sigma constraint by the MDD(Σσ(X)). Then the two MDDs are inter-
sected. An MDD propagator is used on this MDD, named mddμσ. See Sect. 3.2.
Mμ∩σ∩α intersects mddα, the MDD of the constraint Cα, with mddμσ the
previous MDD to obtain mddsol. In this case, all constraints are combined.
Then, we consider a PMF constraint and that μ is variable:
Mlog.
We deﬁne a cost-MDD propagator on mddIµ = cost-MDD(Σid,[μ,μ](X), logP)
with log(Pmin) and ≥and with log(Pmax) and ≤. See Sect. 5.
Mlog∩α. We deﬁne mddIlog = MDD(ΣlogP,Ilog(X)) and we intersect it with
mddIµ. Then, we intersect it with mddα, the MDD of Cα, to obtain mddlogα.

MDDs are Eﬃcient Modeling Tools
37
Table 1 shows the result of these experiments. As we can see when the prob-
lem involves many solutions, all the methods perform well (excepted MGCC).
We can see that an advantage of the intersection methods is that they contain
all the solutions of problem. Table 2 shows the diﬀerent sizes of the MDDs.
Table 1. Comparison solving times (in ms) of models. 0 means that this is immediate.
T-O indicates a time-out of 500 s.
Fixed μ
Variable μ
Sat?
#sol
Mσ<,σ> MGCC
Mμ∩σ Mμ∩σ∩α Mlog Mlog∩α
Sat
Build
50
31
138
2,203
34
317,921
10 sol
14
T-O
16
0
14
0
All sol T-O
T-O
T-O
0
T-O
0
UnSat Build
55
28
121
151
37
133,752
10 sol
T-O
T-O
T-O
0
T-O
0
All sol T-O
T-O
T-O
0
T-O
0
Table 2. Comparison of MDD sizes (in thousands) of diﬀerent models. 0 means that
the MDD is empty.
Fixed μ
Variable μ
Sat?
N/A
mddα mddμ mddσ mddμσ mddsol mddIµ
mddIlog
mddlogα
Sat
#nodes
3
3
5
67
521
2
18
24,062
Sat
#arcs
44
27
55
660
4,364
30
268
341,555
UnSat #nodes
3
2
5
67
0
2
18
0
UnSat #arcs
46
27
55
660
0
30
268
0
Random instances. The intersection methods Mμ∩σ and Mμ∩σ∩α have been
tested on random bigger instances. Tables 3 and 4 gives some results showing
how this method scales with the number of variables. In the ﬁrst line, the couple
is #var/#val. Times are in ms. Experiments of Table 3 set 0 < σ < 4n for having
a delta depending on the number of variables like in [17], whereas experiments of
Table 4 impose 100 < σ < 400, these numbers come from our real world problem.
These experiments show that the Mμ∩σ model can often be a good trade-
oﬀbetween space and time. Using the lower bound of the expected size of the
MDD [15], we can estimate and decide if it is possible to process Mμ∩σ∩α. The
last two columns of Table 3 show that it is not always possible to build such an
intersection.

38
G. Perez and J.-C. R´egin
Table 3. Time (in ms) and size (in thousands) of the MDDs of models Mμ∩σ and
Mμ∩σ∩α. 0 means that the MDD is empty. M-O means memory-out.
0 < σ < 4n
Method
n/d
20/20 30/20 40/30
40/40
50/40
50/50
100/40 100/100
Mμ∩σ
T(ms)
26
132
391
401
848
875
12,780
14,582
#nodes 18
63
153
1578
306
311
2,285
2,532
#arcs
198
808
2,308
2,427
5,196
5,354
53,757
62,057
Mμ∩σ∩α T(ms)
561
3,084
11,864 10,789 58,092 60,513 M-O
M-O
#nodes 163
764
0
0
0
0
M-O
M-O
#arcs
1,788
8,416
0
0
0
0
M-O
M-O
Table 4. Time (in ms) and size (in thousands) of the MDDs of models Mμ∩σ and
Mμ∩σ∩α. Mμ∩σ∩α is empty because there is no solution.
100 < σ < 400
Method
n/d
20/20
30/20
40/30
40/40
Mμ∩σ
T(ms)
162
333
586
602
#nodes 81
184
326
338
#arcs
823
1,865
3,329
3,479
Mμ∩σ∩α T(ms)
2,663
10,379 21,063 26,393
#nodes 1,098
2,555
35
0
#arcs
11,166 23,764 151
0
7
Conclusion
We have shown that modeling constraints by MDDs has several advantages in
practice. It avoids to develop ad-hoc algorithms, gives competitive results and
leads to eﬃcient combination of constraints outperforming the other approaches.
We have emphasized our approach on statistical constrains including the new
PMF constraint we proposed.
References
1. Problem 30 of CSPLIB. www.csplib.org
2. Andersen, H.R., Hadzic, T., Hooker, J.N., Tiedemann, P.: A constraint store based
on multivalued decision diagrams. In: Bessi`ere, C. (ed.) CP 2007. LNCS, vol. 4741,
pp. 118–132. Springer, Heidelberg (2007). doi:10.1007/978-3-540-74970-7 11
3. Beldiceanu, N., Carlsson, M., Demassey, S., Petit, T.: Global constraint catalog:
past, present and future. Constraints 12(1), 21–62 (2007)
4. Bergman, D., Hoeve, W.-J., Hooker, J.N.: Manipulating MDD relaxations for com-
binatorial optimization. In: Achterberg, T., Beck, J.C. (eds.) CPAIOR 2011. LNCS,
vol. 6697, pp. 20–35. Springer, Heidelberg (2011). doi:10.1007/978-3-642-21311-3 5

MDDs are Eﬃcient Modeling Tools
39
5. Bessiere, C., Hebrard, E., Katsirelos, G., Kiziltan, Z., Picard-Cantin,
´E.,
Quimper, C.-G., Walsh, T.: The balance constraint family. In: O’Sullivan, B. (ed.)
CP 2014. LNCS, vol. 8656, pp. 174–189. Springer, Cham (2014). doi:10.1007/
978-3-319-10428-7 15
6. Bryant, R.E.: Graph-based algorithms for boolean function manipulation. IEEE
Trans. Comput. C 35(8), 677–691 (1986)
7. Cheng, K., Yap, R.: An mdd-based generalized arc consistency algorithm for pos-
itive and negative table constraints and some global constraints. Constraints 15,
265–304 (2010)
8. Demassey, S., Pesant, G., Rousseau, L.-M.: A cost-regular based hybrid column
generation approach. Constraints 11(4), 315–333 (2006)
9. Gange, G., Stuckey, P., Szymanek, R.: MDD propagators with explanation. Con-
straints 16, 407–429 (2011)
10. Hadzic, T., Hooker, J.N., O’Sullivan, B., Tiedemann, P.: Approximate compila-
tion of constraints into multivalued decision diagrams. In: Stuckey, P.J. (ed.) CP
2008. LNCS, vol. 5202, pp. 448–462. Springer, Heidelberg (2008). doi:10.1007/
978-3-540-85958-1 30
11. Hoda, S., Hoeve, W.-J., Hooker, J.N.: A systematic approach to MDD-based con-
straint programming. In: Cohen, D. (ed.) CP 2010. LNCS, vol. 6308, pp. 266–280.
Springer, Heidelberg (2010). doi:10.1007/978-3-642-15396-9 23
12. Pennington, W.D.: Reservoir Geophys. 66(1), 25–30 (2001)
13. Perez, G., R´egin, J.-C.: Improving GAC-4 for table and MDD constraints. In:
O’Sullivan, B. (ed.) CP 2014. LNCS, vol. 8656, pp. 606–621. Springer, Cham
(2014). doi:10.1007/978-3-319-10428-7 44
14. Perez, G., R´egin, J-C.: Eﬃcient operations on MDDs for building constraint pro-
gramming models. In: International Joint Conference on Artiﬁcial Intelligence,
IJCAI 2015, Argentina, pp. 374–380 (2015)
15. Perez, G., R´egin, J.-C.: Soft and cost MDD propagators. In: Proceedings of the
AAAI 2017 (2017)
16. Pesant, G., R´egin, J.-C.: SPREAD: a balancing constraint based on statistics. In:
Beek, P. (ed.) CP 2005. LNCS, vol. 3709, pp. 460–474. Springer, Heidelberg (2005).
doi:10.1007/11564751 35
17. Pesant, G.: Achieving domain consistency and counting solutions for dispersion
constraints. INFORMS J. Comput. 27(4), 690–703 (2015)
18. R´egin, J.-C.: Generalized arc consistency for global cardinality constraint. In: Pro-
ceedings of the AAAI 1996, Portland, Oregon, pp. 209–215 (1996)
19. Rossi, R., Prestwich, S.D., Armagan Tarim, S.: Statistical constraints. In: ECAI
2014–21st European Conference on Artiﬁcial Intelligence, Prague, Czech Republic
- Including Prestigious Applications of Intelligent Systems (PAIS 2014), 18–22
August 2014, pp. 777–782 (2014)
20. Roy, P., Perez, G., R´egin, J.-C., Papadopoulos, A., Pachet, F., Marchini, M.:
Enforcing structure on temporal sequences: the allen constraint. In: Rueher, M.
(ed.) CP 2016. LNCS, vol. 9892, pp. 786–801. Springer, Cham (2016). doi:10.1007/
978-3-319-44953-1 49
21. Schaus, P., Deville, Y., Dupont, P., R´egin, J.-C.: The deviation constraint. In:
Hentenryck, P., Wolsey, L. (eds.) CPAIOR 2007. LNCS, vol. 4510, pp. 260–274.
Springer, Heidelberg (2007). doi:10.1007/978-3-540-72397-4 19
22. Schaus, P., Deville, Y., Dupont, P., R´egin, J.-C.: Simpliﬁcation and extension of
the SPREAD constraint. In: Future and Trends of Constraint Programming, pp.
95–99. ISTE (2007)

40
G. Perez and J.-C. R´egin
23. Schaus, P., R´egin, J.-C.: Bound-consistent spread constraint 2(3) (2014)
24. Schaus, P., Deville, Y., Dupont, P.: Bound-consistent deviation constraint. In:
Bessi`ere, C. (ed.) CP 2007. LNCS, vol. 4741, pp. 620–634. Springer, Heidelberg
(2007). doi:10.1007/978-3-540-74970-7 44
25. Trick, M.: A dynamic programming approach for consistency and propagation for
knapsack constraints. In CPAIOR 2001 (2001)

On Finding the Optimal BDD Relaxation
David Bergman1 and Andre Augusto Cire2(B)
1 Department of Operations and Information Management,
University of Connecticut, Mansﬁeld, USA
david.bergman@uconn.edu
2 Department of Management, University of Toronto Scarborough,
Toronto, USA
acire@utsc.utoronto.ca
Abstract. This paper presents an optimization model for identifying
limited-width relaxed binary decision diagrams (BDDs) with tightest
possible relaxation bounds. The model developed is a network design
model and is used to identify which nodes and arcs should be in a relaxed
BDD so that the objective function bound is as close to the optimal
value as possible. The model is presented speciﬁcally for the 0–1 knap-
sack problem, but can be extended to other problem classes that have
been investigated in the stream of research on using decision diagrams for
combinatorial optimization problems. Preliminary experimental results
indicate that the bounds provided by the relaxed BDDs are far supe-
rior to the bounds achieved by relaxed BDDs constructed via previously
published compilation algorithms.
1
Introduction
The use of decision diagrams (DDs) for optimization has driven an accelerating
stream of research in both mathematical programming and constraint program-
ming (CP) communities [8]. The topic stemmed from the idea of approximating
the feasible solution space of a problem using limited-size relaxed DDs. Such an
approximation could either play the role of the constraint store in CP [2,18], or
function as an alternative relaxation in discrete optimization problems [5,6,9]. In
all these cases, the use of relaxed DDs provided strong optimization bounds that
were eﬀective in pruning large portions of the search space, thereby signiﬁcantly
speeding up the solution process in a variety of problems.
The success of relaxed DDs therefore hinges on the tightness of the optimiza-
tion bounds they provide. This results in fundamentally diﬀerent challenges than
the ones that are typically addressed in the classical use of DDs in Boolean logic
[1,11,12,21], where the focus is on minimizing the size of a DD that exactly rep-
resents a Boolean formula (see, e.g., [3,13,14,17,22,23]). In the context of opti-
mization, the question of interest changes: Given a maximum size of a relaxed
DD, what is the best possible optimization bound that can be achieved?
To date, this research question has remained largely unanswered. Previous
papers indicate that the order of the decision variables within a DD aﬀects
c
⃝Springer International Publishing AG 2017
D. Salvagnin and M. Lombardi (Eds.): CPAIOR 2017, LNCS 10335, pp. 41–50, 2017.
DOI: 10.1007/978-3-319-59776-8 4

42
D. Bergman and A.A. Cire
the bounds [5], in that variable orderings with smaller exact DDs also lead to
stronger relaxations. Relaxed DD constructions have since then primarily focused
on heuristics that search for good variable orderings, and very few theoretical or
computational guarantees on their quality have been provided thus far.
The contribution of this paper is to present the ﬁrst methodology for obtain-
ing the relaxed DD with the provably tightest optimization bound. Speciﬁcally,
we propose a new linear integer programming (IP) model that constructs the
optimal-bound relaxed DD for a given ﬁxed size. Besides providing a new tech-
nique to build a relaxed DD (e.g., by ﬁnding any feasible solution to this IP),
such a methodology can be used, for instance, to benchmark the quality of
the heuristically-constructed relaxed DDs in current branch-and-bound and CP
techniques [4]. This model may also allow for new theoretical results that link
the quality of the bound with the underlying polyhedral structure of the IP,
similar to what is investigated in approximate dynamic programming [10]. It
also shares connections with existing policy construction mechanisms in Markov
decision processes [24] and general approximate inference schemes [15,16].
In this work we restrict our attention to the 0–1 knapsack problem and there-
fore to binary decision diagrams (BDDs), but later discuss how the overall con-
cepts can be generalized. The proposed IP determines which nodes and arcs to
include in the BDD so that the longest path from the root to the terminal is as
small as possible (assuming a maximization problem). It shares some similarities
with the problem of ﬁnding a variable ordering that minimizes the size of the
exact BDD, also formulated as an IP for the knapsack problem [3]. However,
it diﬀers from previous work in that we only need to enforce that no feasible
solution is lost in our BDD representation, which makes it easier to generalize.
Preliminary experimental results on small-scale problems indicates that, even
when not solved to optimality, the model can identify far superior relaxed BDDs
than those obtained through the standard compilation technique from the liter-
ature. This is a surprising result that may be used to directly improve existing
DD-based technology and open new potential research directions, while showing
that existing BDD-relaxations can be substantially improved.
The remainder of the paper is organized as follows. In Sect. 2 we formally
deﬁne BDDs and later provide an example of a knapsack instance which exhibits
the property that, for a ﬁxed maximum width, current methods result in a
relaxed BDD with weaker bounds than the relaxed BDD with the best possible
relaxation bound. In Sect. 3 an IP formulation for determining the optimal BDD
relaxation is presented. Section 4 describes the results on preliminary experimen-
tal evaluation, and a conclusion is provided in Sect. 5.
2
BDDs for Optimization
We focus the discussion on the 0–1 knapsack problem (KP) deﬁned by n items
I = {1, . . . , n}, a knapsack capacity C, and where each item i ∈I has a proﬁt and
weight pi, wi ∈Z+, respectively. Each subset I ⊆I has weight w(I) = 
i∈I wi
and proﬁt p(I) = 
i∈I pi. The goal is to ﬁnd the subset I∗⊆I with maximum

On Finding the Optimal BDD Relaxation
43
proﬁt whose weight does not exceed the knapsack capacity. Let S be the family
of subsets of I that do not exceed the knapsack capacity.
A standard IP model associates a 0–1 variable xi with each item i ∈I
indicating whether to include or not the i-th item in the knapsack, as follows:
max

i∈I
pixi :

i∈I
wixi ≤C, xi ∈{0, 1} , ∀i ∈I

(KP-IP)
For the purpose of this paper, a BDD is a layered-acyclic arc-weighted digraph
with node set U and arc set A, where the paths encode subsets of I. U is
partitioned into n + 1 layers L1, . . . , Ln+1, and each node u ∈U belongs to
the layer indexed by ℓ(u) ∈{1, 2, . . . , n + 1}; i.e., Li = {u | ℓ(u) = i}. In
particular, L1 = {r} and Ln+1 = {t}, where r and t are referred to as the
root and terminal node, respectively. The width of a layer Li is the number of
nodes in Li, i.e. |Li|, and the width W(B) of B is W(B) = max1≤i≤n+1 |Li|.
Each arc a = (t(a), h(a)) ∈A has tail t(a) ∈U and a head h(a) ∈U, with
ℓ(h(a)) −ℓ(t(a)) = 1; i.e. arcs connect only nodes in adjacent layers. A node
u ∈U\{t} is the tail of at most one 0-arc and at most one 1-arc. The layer ℓ(a)
of an arc a corresponds to the layer of the tail of a: ℓ(a) = ℓ(t(a)). Moreover, a is
associated with an arc value v(a) ∈Z∗and an arc domain d(a) ∈{0, 1}. An arc
a is referred to as a 0-arc and 1-arc when d(a) = 0 and d(a) = 1, respectively.
A 0-arc a in layer ℓ(a) corresponds to not including the item ℓ(a) in the
knapsack and has a value of v(a) = 0. In turn, an 1-arc a in layer ℓ(a) corresponds
to including item ℓ(a) in the knapsack and has a value of pℓ(a). Each path
p = (a1, . . . , ak) therefore encodes a subset I(p) ⊆I through the arc domains
of its arcs: I(p) = {ℓ(aj) : d(aj) = 1, 1 ≤j ≤k} . As such, the sum of the arc
values in p yields the proﬁt of the subset I(p).
Let P be the set of r −t paths in a BDD B. The solution set S(B) is the
collection of subsets encoded by paths in P: I(B) = 
p∈P I(p). B is called exact
if S(B) = S and is called relaxed if S(B) ⊇S. For an exact BDD, any longest
r −t path p∗in terms of the values v(·) therefore encodes an optimal solution
I(p∗) and its total value is the optimal value w(p∗). For a relaxed BDD, I(p∗)
may be infeasible, but w(p∗) ≥w(I∗) and we obtain a bound on w(I∗). Because
B is directed and acyclic, the longest path can be found in O(|U|).
Note that in our deﬁnition the i-th layer of the BDD is associated directly
with the i-th item of the knapsack. However, any one-to-one mapping between
layers and variables (i.e., a variable ordering) yields a valid exact or relaxed
BDD, but each with possibly distinct size or optimization bound [6]. For the
purposes of this paper we restrict our attention to the variable ordering given
by the index of the items, which is assumed without loss of generality.
Motivating Example. We now provide a KP instance for which building a relaxed
BDD via the standard approach from [6] yields a weaker bound than the best
possible relaxed BDD of a ﬁxed width. Consider the following instance:

44
D. Bergman and A.A. Cire
max
94x1 + 98x2 + 97x3 + 75x4
(E)
s.t.
51x1 + 56x2 + 66x3 + 93x4 ≤133, x ∈{0, 1}4
The exact BDD for problem (E) is depicted in Fig. 1(a) and has a width of
3. The longest path is depicted by the shaded arcs in the ﬁgure and corresponds
to the solution x = (0, 1, 1, 0) with a value of 195. Existing works in BDDs
for optimization (e.g., [6,8]) consider a top-down construction where layers are
built one at a time, “merging” nodes if a given maximum width of the layer
is exceeded. The chief merging strategy in practice is to aggregate nodes with
the least longest path values from the root r to those nodes (denoted by minLP
procedure in [6]). This procedure outputs the 2-width BDD in Fig. 1(b), where
the longest path corresponds to the infeasible solution x = (1, 1, 0, 1) with a
value of 267. An optimal 2-width BDD relaxation is depicted in Fig. 1(c) and its
optimal longest path yields the optimal solution x = (0, 1, 1, 0). Notice that it is
still a relaxed DD since it contains the infeasible solution x = (0, 1, 0, 1).
Fig. 1. (a) Exact BDD, (b) relaxed BDD using the procedure from [6], and (c) an
optimal relaxed BDD for the KP example (E). 0-arcs and 1-arcs are depicted by dashed
and solid arcs, respectively. Shaded arcs represent the longest path in each BDD.
3
IP Model for the Optimal Relaxed BDD
The relaxations bounds obtained by relaxed BDDs is perhaps the most crucial
ingredient to the successful application of BDDs for optimization. Since there are
problems for which the exact BDD will be exponential in size, relaxed BDDs are
used to approximate the feasible set and provide relaxation bounds. The variable
ordering chosen for the layers can have a signiﬁcant impact on the bound, but
we focus here on ﬁnding the relaxed BDD with width less than or equal to W
that provides the strongest (i.e., lowest) relaxation bound.
The problem can be cast as the IP model (1), as follows. Let [t] = {1, . . . , t}
for any number t ∈Z+. For each layer ℓ, we deﬁne W variables yℓ,i, i ∈[W],
which indicates whether or not a node with index i is used on layer ℓ. Addition-
ally, binary variables xd
ℓ,i,j are deﬁned for every d ∈{0, 1}, every pair of indices

On Finding the Optimal BDD Relaxation
45
in [W], and for every layer in [n]. This variable indicates if a d-arc between a
node with index i ∈Lℓand a node with index j ∈Lℓ+1 exists.
A node u in the resulting BDD can be equivalently written as u = (ℓ, i),
indicating that u is the i-th node at layer ℓ. Variable Sℓ,i deﬁnes a state for each
node u = (ℓ, i), which is used to enforce that no feasible solution in S is lost.
The variables pℓ,i deﬁne the longest path from that node to the terminal. The
size of the resulting model is approximately O(nW 2) (both in terms of variables
and constraints).
min
p1,1
(1a)
s.t.

d∈{0,1}

j∈[W ]
xd
1,1,j ≥1
(1b)

d∈{0,1}

i∈[W ]
xd
n+1,i,1 ≥1
(1c)
xd
ℓ,i,j ≤yℓ,i , xd
ℓ,i,j ≤yℓ,j,
∀d ∈{0, 1} , ∀ℓ∈[n] , ∀i, j ∈[W]
(1d)

j∈[n]
x0
ℓ,i,j = yℓ,i,
∀ℓ∈[n] ∀i ∈[W]
(1e)

j∈[n]
x1
ℓ,i,j ≤yℓ,i,
∀ℓ∈[n] ∀i ∈[W]
(1f)
S1,1 = 0
(1g)
Sℓ,i ≤C
(1h)
Sℓ+1,i ≤Sℓ,j + d · wℓ+

1 −xd
ℓ,i,j

· M,
∀d ∈{0, 1} , ∀ℓ∈[n] , ∀i, j ∈[W]
(1i)
Sℓ,i + wℓ≥
⎛
⎝1 −

j∈[W ]
x1
ℓ,i,j
⎞
⎠· (C + 1) ,
∀ℓ∈[n] , ∀i ∈[W]
(1j)
pℓ,i ≥pℓ+1,j + d · pℓ−

1 −xd
ℓ,i,j

· M,
∀d ∈{0, 1} , ∀ℓ∈[n] , ∀i, j ∈[W]
(1k)
yℓ,i ∈{0, 1} , Sℓ,i, pℓ,i ∈Z+,
∀ℓ∈[n + 1] ∀i ∈[W]
(1l)
xd
ℓ,i,j ∈{0, 1} ,
∀d ∈{0, 1} , ∀ℓ∈[n] , ∀i, j ∈[W]
(1m)
pn+1,1 = 0
(1n)
Theorem 1. The optimal solution to model (1) yields a relaxed BDD B with
node set U = {(ℓ, i) : yℓ,i∗= 1}, partitioned into n+1 layers lexicographically by
ℓ, and arcs A ⊆U × U, deﬁned by arcs a = ((ℓ, i), (ℓ+ 1, j)) ∈A ↔xd
ℓ,i,j
∗= 1,
with d(a) = d, for d ∈{0, 1}.
Proof. We ﬁrst claim that the digraph is connected. Associate r with node (1, 1)
and t with node (n + 1, 1). Constraints (1b) and (1c) required that at least one
node is directed out of r and into t. Constraints (1d) require that if an arc is
deﬁned, so are both of its endpoints, and the result follows. It is layered and
acyclic by deﬁnition, and so the resulting graph is a BDD.

46
D. Bergman and A.A. Cire
We claim the following: for any path p consisting of arcs a1, . . . , aℓ′−1 tra-
versing nodes u1 = r, u2, . . . , uℓ′, with uℓ= (ℓ, jℓ) for ℓ= 1, . . . , ℓ′, we have
Sℓ′,jℓ′ ≤

ℓ∈[ℓ′−1]:d(aℓ)=1
wℓ.
(2)
We proceed by induction on ℓ′, showing that for any possible path of length
less than or equal to ℓ′ −1, inequality (2) is satisﬁed. For ℓ′ = 1, S1,1 = 0
by constraint (1h). For ℓ′ = 2, consider any index j for which y2,j∗= 1. Any
arc directed to this node has tail r with S1,1 = 0. If a 0-arc is directed at this
node, constraint (1i) ensures that S2,j = 0 so that the inequality is trivially
satisﬁed. Otherwise, there is a single arc directed at this node which is a 1-arc,
and constraint (1i) enforces that S2,j ≤0 + w1, as desired.
By induction, suppose that for all k ≤ℓ′ −1, the inequality is satisﬁed.
Then, Sℓ′−1,jℓ′−1 ≤
ℓ∈[ℓ′−2]:d(aℓ)=1 wℓ. If d(aℓ′−1) = 0, then constraint (1h)
will enforce that
Sℓ,jℓ′ ≤Sℓ′−1,jℓ′−1 ≤

ℓ∈[ℓ′−2]:d(aℓ)=1
wℓ=

ℓ∈[ℓ′−1]:d(aℓ)=1
wℓ,
and if d(aℓ′−1) = 1, then constraint (1h) will enforce that
Sℓ,jℓ′ ≤Sℓ′−1,jℓ′−1 + wℓ′+1 ≤

ℓ∈[ℓ′−2]:d(aℓ)=1
wℓ+ wℓ′+1 =

ℓ∈[ℓ′−1]:d(aℓ)=1
wℓ.
We now show that the BDD is relaxed using inequality (2). This requires
establishing that any set I ∈S is encoded by some path of the BDD. By
way of contradiction, suppose I′ is a set with no corresponding path. Con-
sider the ordered set of nodes (u1, . . . , un+1), with uℓ= (ℓ, jℓ), and arcs
(a1, . . . , an) deﬁned inductively as follows. Initialize the path with u1 = (1, 1)
and a1 = (u1, (2, j2)), with j2 determined by, if 1 ∈I′, the index for which
x1
1,1,j1
∗= 1, and if 1 /∈I′, the index for which x0
1,1,j1
∗= 1. This also establishes
u2 as (2, j2). For ℓ′ = 1, . . . , n, having deﬁned u1, . . . , uℓ′ and a1, . . . , aℓ′−1 (there-
fore also j1, . . . , jℓ′), arc aℓ′ and node uℓ′+1 can be written as follows: If ℓ′ ∈I′,
let aℓ′ = (uℓ′, (ℓ′ + 1, jℓ′+1)) where jℓ′+1 is the index for which x1
ℓ′,jℓ′,jℓ′+1
∗= 1,
and if ℓ′ /∈I′, let aℓ′ = (uℓ′, (ℓ′ + 1, jℓ′+1)) where jℓ′+1 is the index for which
x0
ℓ′,jℓ′,jℓ′+1
∗= 1.
If this procedure successfully identiﬁes a node and arc on each layer, then
the resulting path p′ satisﬁes that I(p′) = I′. Therefore, there must be some
index ℓ′ for which the procedure fails to ﬁnd an arc aℓ′. This cannot be a 0-arc,
because, if uℓ′ exists, constraints (1e) require that a 0-arc is directed out of every
node. It must thus fail on a layer with ℓ′ ∈I′ where there is no index jℓ′+1 for
which x1
ℓ′,jℓ′,jℓ′+1
∗= 1. Because I′ ∈S, 
i∈I′∩[ℓ′] wi = 
i∈I′∩[ℓ′−1] wi+wℓ′ ≤C.
Also, by constraint (1j), Sℓ′,jℓ′ + wℓ′ ≥(1 −(0)) · (C + 1) = C + 1, resulting in
Sℓ′,jℓ′ > 
i∈I′∩[ℓ′−1] wi. However, this contradicts inequality (2) because for p′,
the set I′ ∩[ℓ−1] is identically the set {i ∈[ℓ′ −1] : d(ai) = 1}.
□

On Finding the Optimal BDD Relaxation
47
The following theorem establishes that the results bounds are valid.
Theorem 2. At the optimal solution for (1), the values pℓ,i∗will be an upper
bound on the length of the longest path from node (ℓ, i) to t in the BDD deﬁned
by the solution, as in the statement of Theorem 1.
□
Extensions. We brieﬂy comment on some possible extensions of the model. To
incorporate variable ordering, we can introduce a new binary variable zℓ,j that
indicates whether layer ℓis associated with item j. The constant wℓshould then
be replaced by 
j wℓzℓ,j (similarly for pj), and exactly one variable {zℓ,j}∀j
must have a value of 1 for any layer ℓ. One could also impose a maximum
number of nodes as opposed to a maximum width by introducing a new binary
variable uℓ,i indicating if a node i is added to a layer ℓ, adjusting the constraints
as necessary (e.g., that an arc must link nodes from adjacent layers).
Finally, to adapt for other discrete optimization problems, one must modify
the state variable Sℓ,i and associated constraints to enforce suﬃcient conditions
for the removal of arcs, i.e., any condition that certiﬁes that the paths crossing
the arc are infeasible. This would yield valid relaxed BDDs for the problem,
albeit not the optimal ones. Existing suﬃcient conditions have been proposed
for a variety of global constraints in CP [8]. However, conditions that are also
necessary, i.e., that certiﬁes that at least one path crossing the arc is feasible,
will yield the strongest possible relaxed BDD for the problem, but are typically
NP-Hard to verify [19].
4
Experimental Results
The experiments ran on an Intel(R) Xeon(R) CPU E5-2640 v3 at 2.60 GHz with
128 GB RAM. The BDD method was implemented in C++ and compiled with
GCC 4.8.4. We used ILOG CPLEX 12.6.3 with 2 cores to solve all IPs. Our source
code and all tested instances will be made available at http://www.andrew.
cmu.edu/user/vanhoeve/mdd/. We generated random KP instances following
the procedure adapted from [20]. The values pi and wi were drawn uniformly at
random from the set {1, . . . , 100}, and C = ⌈r × n
i=1 wi⌉, where r is a ratio
such that r ∈{0.1, . . . , 0.9}. We considered instances of size |I| ∈{15, 20} and
generated 10 samples for each pair (n, r).
Fig. 2. Percentage gap × scaled ratio (r × 10) for |I| = 15 (left) and |I| = 20 (right).

48
D. Bergman and A.A. Cire
Table 1 shows the results considering widths of W ∈{4, 8} for the relaxed
BDDs. The column |B| represents the average width of the exact BDD, while
IP represents the optimality gap of the best solution found by our IP model
(1) in 1,800 s (i.e., gap = 100*(best ub – optimal)/optimal). The column MinLP
refers to the optimality gap of the standard relaxed BDD construction from [6,7],
and MinRandom is the best gap obtained by 50 runs of a completely random
relaxed BDD construction, as investigated in [5]. None of the IP models were
solved to optimality within the time limit (the average IP optimality gap was
35% and 52% for instances with |I| = 15 and |I| = 20, respectively).
The bound provided by the IP model was signiﬁcantly superior in almost all
cases but for r = 0.9 and |I| = 20. Figure 2 depicts the bound comparison, where
the IP model was particularly tighter when the ratio was small (as intuitively
this results in a much smaller search space).
Table 1. Results for W-relaxed BDDs
|I| = 15
W = 4
W = 8
r
|B|
IP
MinLP MinRandom IP
MinLP MinRandom
0.1
7.50
0.71 37.50
36.34
0.18
12.34
18.22
0.2
21.50
6.08 47.68
69.34
1.12
31.40
59.89
0.3
43.30
11.82 45.24
61.86
4.02
34.43
58.33
0.4
73.00
14.09 36.03
49.84
4.31
29.86
47.59
0.5
80.70
7.83 23.47
31.28
3.23
18.93
30.66
0.6
73.30
4.25 15.64
19.40
3.11
12.72
19.40
0.7
44.40
1.48
6.74
10.63
0.67
3.98
10.63
0.8
23.20
0.80
3.47
6.13
0.71
1.76
6.13
0.9
8.70
0.12
0.16
1.69
0.00
0.00
1.69
|I| = 20
W = 4
W = 8
r
|B|
IP
MinLP MinRand
IP
MinLP MinRand
0.10
19.20
5.89 66.03
85.13
2.67 34.92
62.94
0.20
67.70 14.52 62.49
85.57
9.55 45.34
81.62
0.30 161.90 19.42 46.44
57.43
13.86 37.95
57.62
0.40 243.80 23.76 38.15
45.83
17.22 32.33
45.68
0.50 307.80 11.59 19.85
24.14
9.17 17.91
24.14
0.60 248.30
8.24 13.67
15.70
7.79 12.15
15.70
0.70 157.80
3.66
6.24
8.05
3.09
4.69
8.05
0.80
73.30
1.55
2.37
3.67
1.32
1.46
3.67
0.90
18.40
0.63
0.58
1.91
0.61
0.13
1.91

On Finding the Optimal BDD Relaxation
49
5
Conclusions and Future Work
This paper opens the door for a new stream of research on BDD minimiza-
tion. In particular, it introduces a mathematical programming model for ﬁnding
the relaxed BDD with strongest relaxation bound for knapsack problems, given
a maximum width. The BDDs identiﬁed by these mathematical programming
models provide much stronger bounds than the BDDs obtained by previously
investigated compilation techniques, showing promise for the approach. The
model developed, however, is simplistic and computationally challenging. Many
enhancements are possible, for example through the use of symmetry breaking
constraints, developing search heuristics, and designing stronger formulations.
Also, this model presented is speciﬁc for knapsack problem, but can be extended
to other problem classes and lead to similar bound improvements.
References
1. Akers, S.B.: Binary decision diagrams. IEEE Trans. Comput. 27, 509–516 (1978)
2. Andersen, H.R., Hadzic, T., Hooker, J.N., Tiedemann, P.: A constraint store based
on multivalued decision diagrams. In: Bessi`ere, C. (ed.) CP 2007. LNCS, vol.
4741, pp. 118–132. Springer, Heidelberg (2007). doi:10.1007/978-3-540-74970-7 11.
http://dx.doi.org/10.1007/978-3-540-74970-7 11
3. Behle,
M.:
On
threshold
BDDs
and
the
optimal
variable
ordering prob-
lem.
J.
Comb.
Optim.
16(2),
107–118
(2007).
http://dx.doi.org/10.1007/
s10878-007-9123-z
4. Bergman, D., Cire, A.A., van Hoeve, W.J.: MDD propagation for sequence con-
straints. J. Artif. Intell. Res. 50, 697–722 (2014). http://dx.doi.org/10.1613/jair.
4199
5. Bergman, D., Cire, A.A., van Hoeve, W.-J., Hooker, J.N.: Variable ordering for the
application of BDDs to the maximum independent set problem. In: Beldiceanu, N.,
Jussien, N., Pinson, ´E. (eds.) CPAIOR 2012. LNCS, vol. 7298, pp. 34–49. Springer,
Heidelberg (2012). doi:10.1007/978-3-642-29828-8 3
6. Bergman, D., Cire, A.A., van Hoeve, W.J., Hooker, J.N.: Optimization bounds
from binary decision diagrams. INFORMS J. Comput. 26(2), 253–268 (2014).
http://dx.doi.org/10.1287/ijoc.2013.0561
7. Bergman, D., Cire, A.A., van Hoeve, W.J., Hooker, J.N.: Discrete optimization
with decision diagrams. INFORMS J. Comput. 28(1), 47–66 (2016)
8. Bergman, D., Cire, A.A., van Hoeve, W.J., Hooker, J.: Decision Diagrams for Opti-
mization. Artiﬁcial Intelligence: Foundations, Theory, and Algorithms, 1st edn.
Springer, Switzerland (2016)
9. Bergman, D., Hoeve, W.-J., Hooker, J.N.: Manipulating MDD relaxations for
combinatorial optimization. In: Achterberg, T., Beck, J.C. (eds.) CPAIOR
2011. LNCS, vol. 6697, pp. 20–35. Springer, Heidelberg (2011). doi:10.1007/
978-3-642-21311-3 5. http://dx.doi.org/10.1007/978-3-642-21311-3 5
10. Bertsekas, D.P.: Dynamic Programming and Optimal Control, 4th edn. Athena
Scientiﬁc, Belmont (2012)
11. Bryant, R.E.: Graph-based algorithms for boolean function manipulation. IEEE
Trans. Comput. 35, 677–691 (1986)

50
D. Bergman and A.A. Cire
12. Bryant, R.E.: Symbolic boolean manipulation with ordered binary decision dia-
grams. ACM Comput. Surv. 24, 293–318 (1992)
13. Drechsler, R., Drechsler, N., G¨unther, W.: Fast exact minimization of BDD’s. IEEE
Trans. CAD Integr. Circ. Syst. 19(3), 384–389 (2000). http://dx.doi.org/10.1109/
43.833206
14. Felt, E., York, G., Brayton, R.K., Sangiovanni-Vincentelli, A.L.: Dynamic
variable reordering for BDD minimization. In: Proceedings of the European
Design Automation Conference 1993, EURO-DAC 1993 with EURO-VHDL 1993,
Hamburg, Germany, 20–24 September 1993. pp. 130–135. IEEE Computer Society
(1993). http://dx.doi.org/10.1109/EURDAC.1993.410627
15. Gogate,
V.,
Domingos,
P.M.:
Approximation
by
quantization.
CoRR
abs/1202.3723 (2012). http://arxiv.org/abs/1202.3723
16. Gogate, V., Domingos, P.M.: Structured message passing. In: Proceedings of
the Twenty-Ninth Conference on Uncertainty in Artiﬁcial Intelligence, UAI
2013, Bellevue, WA, USA, 11–15 August 2013 (2013). https://dslpitt.org/uai/
displayArticleDetails.jsp?mmnu=1&smnu=2&article id=2386&proceeding id=29
17. G¨unther, W., Drechsler, R.: Linear transformations and exact minimization of
BDDs. In: 8th Great Lakes Symposium on VLSI (GLS-VLSI 1998), 19–21 February
1998, Lafayette, LA, USA, pp. 325–330. IEEE Computer Society (1998). http://
dx.doi.org/10.1109/GLSV.1998.665287
18. Hadzic, T., Hooker, J.N., O’Sullivan, B., Tiedemann, P.: Approximate compila-
tion of constraints into multivalued decision diagrams. In: Stuckey, P.J. (ed.) CP
2008. LNCS, vol. 5202, pp. 448–462. Springer, Heidelberg (2008). doi:10.1007/
978-3-540-85958-1 30
19. Hoda, S., Hoeve, W.-J., Hooker, J.N.: A systematic approach to MDD-based
constraint programming. In: Cohen, D. (ed.) CP 2010. LNCS, vol. 6308,
pp. 266–280. Springer, Heidelberg (2010). doi:10.1007/978-3-642-15396-9 23.
http://dl.acm.org/citation.cfm?id=1886008.1886034
20. Kirlik, G., Sayın, S.: A new algorithm for generating all nondominated solutions of
multiobjective discrete optimization problems. Eur. J. Oper. Res. 232(3), 479–488
(2014). http://www.sciencedirect.com/science/article/pii/S0377221713006474
21. Lee, C.Y.: Representation of switching circuits by binary-decision programs. Bell
Syst. Tech. J. 38, 985–999 (1959)
22. Shiple, T.R., Hojati, R., Sangiovanni-Vincentelli, A.L., Brayton, R.K.: Heuristic
minimization of BDDs using don’t cares. In: DAC, pp. 225–231 (1994). http://doi.
acm.org/10.1145/196244.196360
23. Soeken, M., Große, D., Chandrasekharan, A., Drechsler, R.: BDD minimization
for approximate computing. In: 21st Asia and South Paciﬁc Design Automation
Conference, ASP-DAC 2016, Macao, Macao, 25–28 January 2016, pp. 474–479.
IEEE (2016). http://dx.doi.org/10.1109/ASPDAC.2016.7428057
24. St-Aubin, R., Hoey, J., Boutilier, C.: APRICODD: approximate policy construc-
tion using decision diagrams. In: Proceedings of Conference on Neural Information
Processing Systems, pp. 1089–1095 (2000)

Design and Implementation of Bounded-Length
Sequence Variables
Joseph D. Scott1(B), Pierre Flener1, Justin Pearson1, and Christian Schulte2
1 Department of Information Technology, Uppsala University, Uppsala, Sweden
{Pierre.Flener,Justin.Pearson}@it.uu.se, JosephDScott@gmail.com
2 KTH Royal Institute of Technology, Stockholm, Sweden
cschulte@kth.se
Abstract. We present the design and implementation of bounded -
length sequence (BLS) variables for a CP solver. The domain of a
BLS variable is represented as the combination of a set of candidate
lengths and a sequence of sets of candidate characters. We show how
this representation, together with requirements imposed by propagators,
aﬀects the implementation of BLS variables for a copying CP solver,
most importantly the closely related decisions of data structure, domain
restriction operations, and propagation events. The resulting implemen-
tation outperforms traditional bounded-length string representations for
CP solvers, which use a ﬁxed-length array of candidate characters and a
padding symbol.
1
Introduction
String variables are useful for expressing a wide variety of real-world problems,
such as test generation [7], program analysis [4], model checking [10], security [3],
and data mining [16]. Despite this usefulness, string variables have never received
an optimized implementation in a modern constraint programming (CP) solver.
We describe the design and implementation of a string variable type for
a copying CP solver. Properly designed, a string variable can be much more
eﬃcient, in both time and space complexity, than the decompositions commonly
used to model strings in CP. Additionally, string variables greatly simplify the
modeling of problems including strings, with beneﬁts to both readability and
correctness. We choose to implement a bounded-length sequence (BLS) variable
type, as it provides much more ﬂexibility [9,18] than ﬁxed-length approaches,
but avoids the blow-ups that plague unbounded-length representations.
We make contributions in three dimensions. First, we select a data struc-
ture for the BLS variable domain, namely a dynamic list of bitsets, to repre-
sent domains in reasonable space. The data structure is designed for eﬃcient
implementation of domain restriction operations, which BLS variables expose
to propagators; we develop a correct and minimal set of these operations. Vari-
ables in turn notify propagators of domain changes using propagation events; we
design a monotonic set of propagation events that are useful for propagators on
BLS variables. Second, the BLS variable is implemented on top of Gecode [25],
c
⃝Springer International Publishing AG 2017
D. Salvagnin and M. Lombardi (Eds.): CPAIOR 2017, LNCS 10335, pp. 51–67, 2017.
DOI: 10.1007/978-3-319-59776-8 5

52
J.D. Scott et al.
a mature CP toolkit with state-of-the-art performance and many deployments
in industry. Third, we show that BLS variables outperform other string solv-
ing methods in CP, as demonstrated upon both a string problem in the CSPlib
repository and a representative benchmark from the ﬁeld of software veriﬁcation.
In this paper, we summarize [22, Chaps. 10 and 11]: this is an improvement
of our [23], as discussed in the experiments of Sect. 7. We forego discussion in
this paper of several interesting and important theoretical aspects of the BLS
representation, as previous publications cover these theoretical aspects in detail:
[22] provides an extended discussion of the logical properties of the representa-
tion, a justiﬁcation for the utility of BLS variables in string constraint problems,
a comparison of diﬀerent modeling strategies, a survey of related work in CP
such as [11], and a discussion of intensional representations; these topics are also
presented in briefer form in [23]. Hence we choose to deal speciﬁcally with the
implementation of an eﬃcient BLS variable, rather than repeating material that
has been presented elsewhere.
Plan of the Paper. Section 2 discusses the design of variable implementations
in general. Section 3 deﬁnes a domain representation appropriate for BLS vari-
ables and motivates the design choices of subsequent sections. Section 4 consid-
ers several representations for each of the components of a BLS variable and
motivates why particular choices have been made. Section 5 deﬁnes restriction
operations, which allow propagators to modify the domain of such a variable.
Section 6 describes BLS-speciﬁc systems of propagation events, which describe
restrictions of the domain of a variable. Section 7 justiﬁes these design choices
by an experimental evaluation. Section 8 concludes the paper.
2
Variables and Propagators
We summarize key concepts relating to propagators as implementations of con-
straints and three fundamental choices of how to implement a variable type.
Propagators. A propagator implements a constraint and is executed by a prop-
agation loop. The execution of a propagator results in the restriction of some
variable domains pruning values for variables that are in conﬂict with the imple-
mented constraint. The propagation loop executes the propagators until no prop-
agator can prune any values, the propagator is said to be at ﬁxpoint. For a more
detailled discussion of propagators see for example [19].
Variables. The central decision in implementing a variable type is how to repre-
sent the domain of a variable. With this representation decided, the implemen-
tation of the variable type must take into account three primary choices: how a
domain is stored, how a propagator prunes a domain, and how a propagator is
informed of a restriction of a domain. We consider each of these choices in turn.
No data structure exists in a vacuum, and every choice of data structure rep-
resents a tradeoﬀbetween memory requirements and computational complexity.

Design and Implementation of Bounded-Length Sequence Variables
53
Variable
Implementation
Domain
Data Structure
Propagator
propagation event
restriction operation
Fig. 1. Interactions of a variable implementation and a propagator: a propagator
requests a change to the domain of a variable via a restriction operation, and a variable
notiﬁes a propagator via a propagation event that its domain has been restricted.
Evaluating the choices made in designing a data structure, therefore, requires
considering the environment in which the data structure will function. For a
variable implementation, that means considering how it interacts with the CP
solver, and most importantly how it interacts with propagators. A simplistic
view of this interaction is shown in Fig. 1.
In an implementation, propagators work by modifying the domains of vari-
ables. These domains are not exposed directly to propagators; rather the vari-
able implementation encapsulates the domain representation and exposes some
restriction operations by which propagators can request that the domain be mod-
iﬁed. Restriction operations provide an interface to the domain that allows the
removal of candidate values; no other modiﬁcations are allowed, as a propagator
is required to be contracting on domains.
When the domain of a variable is restricted by a propagator, any other prop-
agator that implements a constraint on the corresponding variable must be noti-
ﬁed, as it is possible that the latter propagator is no longer at ﬁxpoint. Most
CP solvers make use of more ﬁne-grained information than simply which vari-
able has a newly restricted domain. This more detailed information is called a
propagation event (for an extended discussion of events, see [20]); for an inte-
ger variable X, for example, a propagation event might indicate that the upper
bound of dom(X ) has shrunk, or that dom(X ) has become a singleton.
3
BLS Variables
The domain of a string variable is a set of strings. Even in the bounded-length
case, sets of strings are diﬃcult to represent in extension, due to high space com-
plexity; on the other hand, intensional representations, such as ﬁnite automata
or regular expressions, generally have a higher time complexity for operations.
However, both the time complexity and space complexity may be reduced if the
representation of the set of strings is not required to be exact. For example, an
integer interval is a compact and eﬃcient representation for a set of integers.
Of course, many sets of integers cannot be exactly represented by an interval;
however, every set of integers may be over-approximated by some interval.
We now deﬁne an over-approximation that is appropriate for representing the
domain of a bounded-length set of strings. We then give an example of how a
propagator for a string constraint would interact with domains thus represented.

54
J.D. Scott et al.

A b, N

=

A[1], . . . , A[5], A[6], . . . , A[10], A[11], . . . , A[15]

,

5, 7, 10

mandatory
optional
forbidden
∅
∅
· · ·
Fig. 2. Example of a b-length sequence ⟨Ab, N⟩with maximum length b = 15. Each
set A[i] of candidate characters is the set of all symbols at index i for some string in
the domain; the set N of candidate lengths is {5, 7, 10}. The lower bound, 5, and upper
bound, 10, of the set of candidate lengths partition the sets of candidate characters into
mandatory, optional, and forbidden regions, as indicated. A set of candidate characters
is empty if and only if it is found in the forbidden region.
Representation of a String Domain. For any ﬁnite set W of strings over an
alphabet Σ there exists an upper bound on the lengths of the strings in W,
i.e., some number b ∈N such that for every string w ∈W the length of w is
not greater than b. Such a set W may be over-approximated by a pair ⟨Ab, N⟩,
consisting of a sequence Ab of sets ⟨A[1], . . . , A[b]⟩over Σ, where every A[i] ⊆
Σ is the set of all candidate characters occurring at index i ∈1, b for some
string in W, and a set N ⊆[0, b] of candidate lengths of the strings in W. This
over-approximation, illustrated in Fig. 2, is called the bounded-length sequence
representation. A pair ⟨Ab, N⟩is referred to as a b-length sequence, and is an
abstract representation of the set of all strings that have a length ℓ∈N and a
character at each index i ∈[1, ℓ] taken from the set A[i] of symbols.
The domain of a string variable Xj, written dom (Xj), is a ﬁnite set of
strings; thus, the domain of any string variable Xj can be over-approximated
by a b-length sequence ⟨Ab
j, Nj⟩for some appropriately large value of b. In this
context, we will sometimes write dom(Ab
j, Nj) to mean the domain of a string
variable Xj that is represented by the b-length sequence ⟨Ab
j, Nj⟩; thus we have:
dom(Ab
j, Nj) =

ℓ∈Nj
{w ∈Σℓ|∀i ∈[1, ℓ] : w[i] ∈Aj[i]}
(1)
Bounded-length sequences are not unique: if the sequences ⟨Ab
1, N⟩and ⟨Ab
2, N⟩
diﬀer only in one or more pair of sets A1[i] and A2[i] where i > max(N),
then ⟨Ab
1, N⟩and ⟨Ab
2, N⟩represent the same set of strings. Uniqueness may
be imposed by a representation invariant that only allows bounded-length
sequences of a canonical form, in which a set A[i] of candidate characters is
the empty set if and only if the index i is greater than max(N). As illustrated in
Fig. 2, the set N of candidate lengths divides the sequence Ab into three regions:
the mandatory candidate characters, with an index at most min(N); the optional
candidate characters, with an index greater than min(N) but at most max(N);
and the forbidden candidate characters, with an index greater than max(N).
Propagation.
A properly designed variable implementation must take into
account the operation of propagators implementing constraints for the corre-
sponding variable type. For string variables, expected constraints include reg-
ular language membership (RegularL), string length (Len), reversal (Rev),

Design and Implementation of Bounded-Length Sequence Variables
55

A3[1], A3[2], A3[3], A3[4], A3[5], A3[6], . . .

N1 = {3} :

A1[1], A1[2], A1[3], A2[1], A2[2], A2[3], . . .

N1 = {4} :

A1[1], A1[2], A1[3], A1[4], A2[1], A2[2], . . .

N1 = {5} :

A1[1], A1[2], A1[3], A1[4], A1[5], A2[1], . . .

N1 = {6} :

A1[1], A1[2], A1[3], A1[4], A1[5], A1[6], . . .

⊆
∪
∪
∪
Fig. 3. The post-condition, implied by the constraint Cat(X1, X2, X3), on the set of
candidate characters A3[5], is determined by the four candidate lengths of X1, each
yielding a possible alignment between the three string variables.
and concatenation (Cat); many others are possible (see, e.g., [22]). The follow-
ing example provides a sample of the inference required by typical propagators
for such constraints, and gives context to the design decisions that follow.
Example 1 (Concatenation). The constraint Cat(X1, X2, X3) holds if the con-
catenation of string variables X1 and X2 is equal to the string variable X3.
Consider string variables X1, X2, and X3 with domains represented by the b-
length sequences ⟨Ab
1, N1, ⟨Ab
2, N2, and ⟨Ab
3, N3, each with maximum length
b = 15, and sets of candidate lengths N1 = [3, 6], N2 = [4, 7], and N3 = [5, 14].
A propagator is a contracting function on tuples of variable domains. A
propagator implementing the constraint Cat is a function of the following form:
Cat(dom(Ab
1, N1), dom(Ab
2, N2), dom(Ab
3, N3))
= ⟨dom(A′b
1 , N ′
1), dom(A′b
2 , N ′
2), dom(A′b
3 , N ′
3)⟩
(2)
The simplest inference relevant to the propagation of Cat is the initial arithmetic
adjustment of the candidate lengths. For example, every candidate length for X3
should be the sum of some candidate lengths for X1 and X2:
N ′
3 := {n3 ∈N3 | ∃n1 ∈N1, n2 ∈N2 : n3 = n1 + n2} = [7, 13]
(3)
For sets of candidate characters, there are dependencies not only upon sets of
candidate characters from the other string variables constrained by Cat, but
also on the sets of candidate lengths. For example, Fig. 3 illustrates the four
possible alignments of X1, X2, and X3, corresponding to the four candidate
lengths in the set N1 = [3, 6]. If, on the one hand, the length of X1 were ﬁxed
to either of the two smallest of these candidates, then any symbol in the set
A′
3[5] of candidate characters would also have to be an element in either A2[2]
or A2[1]. If, on the other hand, the length of X1 were ﬁxed to either of the two
largest of these candidates, then in either case any symbol in the set A′
3[5] of
candidate characters would have to be an element of the set A1[5]. Hence:
A′
3[5] := A3[5] ∩(A2[1] ∪A2[2] ∪A1[5])
(4)

56
J.D. Scott et al.
The resulting sets of candidate lengths also depend upon the sets of candidate
characters. For example, if the intersection of A2[2] and A3[5] were empty, then
the uppermost alignment illustrated in Fig. 3, where N1 = {3} could be ruled
out: for any combination of strings X1, X2, and X3 satisfying Cat(X1, X2, X3)
such that the length of X1 was 3, the maximum length of X3 would be 4 /∈[7, 13],
since the set A3[5] of candidate characters would be empty.
⊓⊔
Solver-Based Requirements. In a CP solver, the propagation loop described in
Sect. 2 is interleaved with a backtracking search. If that loop ends with some
variables as yet unassigned, then the search tree is grown by partitioning the
search space to create two or more subproblems, obtained by mutually exclusive
decisions on the domain of an unassigned variable; the propagation loop is then
executed on the chosen subproblem. On the other hand, if the propagation loop
results in a failed domain, then some prior decision is undone: the search returns
to a previously visited node, from which the tree is grown, if possible, by choosing
another of the mutually exclusive decisions that were determined at that node.
There are two main CP techniques for restoring a previously visited node
during backtracking [19]: trailing, in which changes to the variable domains dur-
ing search are stored on a stack, and backtracking consists of reconstructing a
previously visited node; and copying, in which the domains are copied before a
choice is applied, and every new node of the search tree begins with a fresh copy.
The choice of a copying solver has two main impacts on the design of a vari-
able type. First, under copying, the search procedure and the ﬁxpoint algorithm
are orthogonal, while under trailing all the solver components are aﬀected by
backtracking; thus, under copying, design choices can be made without reference
to the details of the search procedure. Second, for a copying system, memory
management is critical. The amount of memory required by a data structure
used to represent the domain of a variable (as well as data structures used o
store the states of other solver components) should therefore be minimal.
4
Data Structure
For a string variable Xwith a domain represented by a bounded-length sequence,
there are three largely orthogonal structural choices to be considered: the rep-
resentation of the set N of candidate lengths, the representation of the sets A[i]
of candidate characters, and the construction of the sequence ⟨A[1], . . . , A[b]⟩
itself. Lengths are natural numbers, and any ﬁnite alphabet may be mapped to
the natural numbers (upon imposing an ordering over the alphabet); therefore
the possible implementations for both the set N of candidate lengths and the
sets A[i] of candidate characters at all indices i of X are the same. However,
lengths and characters have diﬀerent characteristics and beneﬁt from diﬀerent
choices of representation. Each of these choices is now considered in turn.
The Set of Candidate Lengths. The set N of candidate lengths may be exactly
represented or over-approximated. An exact representation could utilize a data

Design and Implementation of Bounded-Length Sequence Variables
57
structure such as those used in CP solvers to exactly represent the domain of
an integer variable (e.g., a range sequence or a bitset). Alternately, the set of
candidate lengths can be over-approximated as an interval: a pair of integers
representing the lower and upper bounds of the set.
Recall from Example 1 that a propagator implementing the constraint Cat
must calculate sums of all candidate lengths for its string variables. This com-
putation is quadratic in the size of the set representation (i.e., in the number of
elements, ranges, or bounds used to represent the set). For the interval repre-
sentation, the representation size is constant, but for exact set representations it
is not. Example 1 also shows that the upper and lower bounds of N are slightly
more useful during propagation than interior values of N. The lower bound of
the set of candidate lengths deﬁnes the smallest sequence of sets of candidate
characters that an implementation should represent explicitly, as every candi-
date solution must have some symbol at those mandatory indices. In contrast,
all, some, or none of the other sets of candidate characters might be explicitly
represented; the upper bound of the set of candidate lengths divides these sets
of candidate characters into those at optional indices that participate in some of
the candidate solutions, and those at forbidden indices that participate in none.
An exact representation would allow some extra inference during the propa-
gation of string constraints, and could be sensible given enough string constraints
with eﬃcient propagators performing non-trivial reasoning on the interior values
of N. Otherwise, an interval representation appears to be most suitable.
Sets of Candidate Characters. The exact composition of an alphabet is problem
dependent; however, several properties generally apply. First, alphabet sizes are
typically manageable; this is certainly true for many interesting classes of string
constraint problems, such as occur in computational biology, natural-language
processing, and the veriﬁcation and validation of software. Second, the symbols
of an alphabet often have a known total ordering, in which case the strings of any
language on that alphabet have a corresponding lexicographic order; even when
no meaningful order exists, such an order may be imposed as needed. Third,
intervals of symbols often have little or no inherent meaning, as, in contrast
to numeric types, there is generally no logical relationship between consecutive
symbols in even a totally-ordered alphabet. Finally, as shown in Example 1, the
propagation of constraints over string variables with bounded-length sequence
domain representations depends heavily on standard set operations (i.e., union
and intersection) between sets of candidate characters.
Upon considering these properties, we propose to implement each set of can-
didate characters as a bitset. A bitset for a ﬁnite set S ⊂N is a sequence of
max(S) bits (i.e., values in {0, 1}) such that the i-th bit is equal to 1 if and only
if i ∈S. With word size w, a bitset representation of S requires k = ⌈max(S)/w⌉
words, or a total of kw bits. Bitsets allow for very fast set operations (typically, a
small constant number of operations per word is required): speciﬁcally, the com-
plexity of the union and intersection operations is linear in k. Furthermore, as
long as |Σ| is not signiﬁcantly larger than w, the memory requirement of a bitset
representation is competitive with other common exact set representations.

58
J.D. Scott et al.
Sequence. The sequence ⟨A[1], . . . , A[b]⟩of sets of candidate characters could
be implemented in two ways: as an array-based structure, requiring minimal
memory overhead and aﬀording direct access to the elements; or as a list-based
structure, allowing for better memory management of dynamic-sized lists, but
no direct access. A key observation is that the maximum length of any b-length
sequence is already given, namely b. We propose a hybrid array-list implemen-
tation: a ﬁxed length array of n pointers to blocks of m bitsets each, where
n = ⌈b/m⌉. This design oﬀers more eﬃcient access to individual nodes than a
traditional linked list, and easier memory (de)allocation than a static-sized array.
Blocks are allocated as needed: at a minimum, a number of blocks suﬃcient to
accommodate min(N) sets of candidate characters (i.e., the mandatory region)
is allocated; additional blocks are allocated when min(N) increases, or when a
restriction operation is applied to a set of candidate characters in the optional
region. Deallocation is performed as needed upon a decrease of max(N). For an
evaluation of alternate sequence implementations, see [22].
5
Domain Restriction Operations
As seen in Sect. 2, propagators interact with domains via restriction operations
that are exposed by the variable implementation. A restriction operation should
satisfy three important properties. First, the operation should be useful for
the implementation of some propagator. Second, the operation should be eﬃ-
cient when executed on the data structure that implements the variable domain.
Finally, the operation should be correct, meaning that the resulting domain is
actually restricted according to the semantics of the operation.
We now describe restriction operations appropriate for a BLS variable type.
The data structure chosen in Sect. 4 allows an eﬃcient implementation of these
operations. Interestingly, though, the correct semantics of BLS variable restric-
tion operations is not obvious, so before proceeding we deﬁne the semantics of
string equality and disequality that is most suited to string variables.
Restriction Operations on String Variables. Table 1 lists several restriction oper-
ations that might be provided by a string variable implementation. The opera-
tions are divided into two categories, aﬀecting either lengths or characters.
The ﬁrst category of operations works on sets of candidate lengths: the equal-
ity (leq) operation restricts the domain of the variable to strings of a given
length, and two inequality operations tighten the lower (lgr) or upper (lle)
bound of the set of candidate lengths. The second category of operations works
on sets of candidate characters: the equality and disequality operations restrict
the domain of the variable to strings with a given symbol at the speciﬁed index
(ceq) or to exclude all strings with a given symbol at the speciﬁed index (cnq),
two inequality operations tighten the lower (cgr) or upper (cle) bound of the
set of candidate characters, and the set intersection and set subtraction opera-
tions restrict the set of candidate characters to their intersection with a given
set of symbols cin) or to exclude a given set of symbols (cmi). Note that strict

Design and Implementation of Bounded-Length Sequence Variables
59
Table 1. Restriction operations for bounded length string variables.
Restriction
Operator
Length
Equality
leq(dom(X), ℓ) = {x ∈dom(X)||x| = ℓ}
≤
lle(dom(X), ℓ) = {x ∈dom(X)||x| ≤ℓ}
>
lgr(dom(X), ℓ) = {x ∈dom(X)||x| > ℓ}
Character Equality
ceq(dom(X), i, c) = {x ∈dom(X)|x[i] = c}
Disequality
cnq(dom(X), i, c) = {x ∈dom(X)|x[i] ̸= c}
≤
cle(dom(X), i, c) = {x ∈dom(X)|x[i] ≤c}
>
cgr(dom(X), i, c) = {x ∈dom(X)|x[i] > c}
Intersection cin(dom(X), i, C) = {x ∈dom(X)|x[i] ∈C}
Subtraction cmi(dom(X), i, C) = {x ∈dom(X)|x[i] /∈C}
greater than (>) and non-strict less than (≤) operations have been chosen for
each category based solely on the complementarity of the operations; the pairs
could just as well have been < and ≥, < and >, or ≤and ≥.
Table 1 omits restriction operations of length disequality, intersection, and
subtraction, as these would be incorrect here. For example, a length disequality
operation that attempts to remove an interior value from the set of candidate
lengths will result in no change to the domain. As sets of candidate characters are
assumed to be explicitly represented, the character disequality and intersection
operations are included.
Some of the restriction operations in Table 1 can be rewritten using the
other operations. For example, the character intersection and subtraction oper-
ations are equivalent to a series of character disequality operations. In practice,
an implementation using these decompositions may be ineﬃcient; however, for
the purpose of deﬁning the restriction operations for domains represented by
bounded-length sequences, a set of four required operations is suﬃcient: lle,
lgr, ceq, and cnq, allowing a propagator to increase the lower bound of the
string length (lgr), decrease the upper bound of the string length (lle), ﬁx the
character at an index i (ceq), or forbid a character at index i (cnq).
Restriction Operations as Update Rules.
Table 2 deﬁnes, for string variable
domains represented by a b-length sequence, the four chosen restriction operations.
They are given as a series of update rules on the components of the aﬀected b-length
sequence; for each restriction operation, only those components of the resulting
b-length sequence ⟨A′b, N ′⟩that diﬀer from the corresponding component of the
initial b-length sequence are deﬁned; all other components are unchanged.
Representation Invariant.
All of the restriction operations in Table 2 are
designed to respect the representation invariant of the b-length sequence rep-
resentation (see Sect. 3), which enforces the relationship between empty sets of
candidate characters and the upper bound of the set of candidate lengths. Hence,
the restriction operation lle(⟨Ab, N⟩, ℓ), which reduces the upper bound of N,

60
J.D. Scott et al.
Table 2. Restriction operations for a string variable with a domain represented by a
b-length sequence, expressed as update rules. Primed set identiﬁers (A′[i] and N ′) indi-
cate changed component values in the b-length sequence resulting from the restriction
operation; all other components in the resulting b-length sequence are identical to the
corresponding component in the original b-length sequence.
Restriction operation Update rule
lle(⟨Ab, N⟩, ℓ)
forall k ∈[ℓ+ 1, b] : A′[k] := ∅; N ′ := N ∩[0, ℓ]
lgr(⟨Ab, N⟩, ℓ)
N ′ := N ∩[ℓ+ 1, b]
ceq(⟨Ab, N⟩, i, c)
A′[i] := A[i] ∩{c}; N ′ := N ∩[i, b]
cnq(⟨Ab, N⟩, i, c)
A′[i] := A[i] \ {c}; ifA′[i] = ∅then
(N ′ := N ∩[0, i −1]; forall k ∈[i, b] : A′[k] := ∅)
also updates all sets of candidate characters at indices greater than ℓto be the
empty set. The restriction operation lgr(⟨Ab, N⟩, ℓ) requires no corresponding
update on sets of candidate characters with indices less than or equal to ℓ: if
there existed an empty set A[i] = ∅of candidate characters such that i was less
than or equal to ℓ, then the upper bound of N would already be at most i; hence
N ∩[ℓ+ 1, b] would also be the empty set, and the domain would be failed.
Equality Semantics. ceq and cnq have very diﬀerent eﬀects on the considered
set of candidate lengths: after ceq the set N ′ only contains lengths that are
at least i; but if A′[i] is not empty after cnq, then the set N ′ may contain
lengths greater than or equal to i. This appears to run counter to the intuition
that equality and disequality should be complementary operations. However, a
restriction operation is a function on the domain of a variable, not a function
on a component of that domain; the operations ceq and cnq are complementary
in that together they always deﬁne a partition of a string variable domain:
dom(ceq(⟨Ab, N⟩, i, c)) ∪dom(cnq(⟨Ab, N⟩, i, c)) = dom(⟨Ab, N⟩)
(5)
dom(ceq(⟨Ab, N⟩, i, c)) ∩dom(cnq(⟨Ab, N⟩, i, c)) = ∅
(6)
For full details on the equality semantics, see [22].
6
Propagation Events
As seen in Sect. 2, information about a domain restriction is communicated to a
propagator via a propagation event [19]. In practice, a propagation event should
be useful to some propagator; that is, the propagation event should distinguish
between a change to the domain that leaves the propagator at ﬁxpoint, versus
one that does not. The set of propagation events exposed by a variable implemen-
tation is called a propagation event system. Larger propagation event systems
come with a commensurate cost to eﬃciency; see [20] for a detailed analysis of
event-based propagation.

Design and Implementation of Bounded-Length Sequence Variables
61
val
lval
lmin
lmax
cval
cdom
(a)
val
lval
lmin
lmax
cval
cdom
(b)
Fig. 4. Implications in a minimal string variable event system (omitting transitive
implications). If the changes to the set of candidate lengths and the sets of candidate
characters are viewed independently (a), then the resulting propagation event system
is not monotonic. The corrected propagation event system (b) is monotonic.
A minimal event system for a string variable X with a domain represented
by a b length sequence ⟨Ab, N⟩is shown in Fig. 4. The val propagation event
indicates that the string variable domain has been reduced to a single string. The
next three propagation events indicate changes to the set N of candidate lengths:
either the lower bound has increased (lmin), or the upper bound has decreased
(lmax), or the set has become a singleton (lval). The remaining propagation
events indicate changes to a set A[i] of candidate characters: either some symbol
has been removed (cdom) or the symbol of the character at index i has become
known (cval). Note that cval indicates that the character at some index i is a
symbol c ∈Σ, and not that the set of candidate characters at i is a singleton. If i
is a mandatory index, then the two conditions are equivalent: for every string x
in the domain of X, the character x[i] is c. However, when an optional set of
candidate characters A[i] is a singleton, then there are additionally strings in
the domain for which the character at index i is undeﬁned.
Some propagation events are implied by others. Figure 4(a) shows an event
system in which length and character propagation events are isolated, making
it possible to report changes to the set of candidate lengths and changes to sets
of candidate characters independently. Unfortunately, this design violates one of
the properties required of propagation events [24]: the set of propagation events
generated by a sequence of restriction operations must be monotonic, that is, it
must not depend on the order of the restriction operations.
In a non-monotonic propagation event system, the set of events generated by
a series of restriction operations is dependent on their order, as shown now:
Example 2. Let X be a string variable with a domain represented by the b-length
sequence ⟨A4, N⟩= ⟨⟨{1, 2}, {1, 2}, {5, 6}, ∅⟩, [1, 3], where b = 4. The following
sequence of restriction operations restricts ﬁrst the set of candidate characters
at index 3, and then the set of candidate lengths:
⟨A′4, N ′⟩:= lle(cnq(⟨Ab, N⟩, 3, 6), 2)
:= lle(⟨⟨{1, 2}, {1, 2}, {5, 6} \ {6}, ∅⟩, [1, 3]⟩, 2)
:= lle(⟨⟨{1, 2}, {1, 2}, {5}, ∅⟩, [1, 3]⟩, 2)
:= ⟨⟨{1, 2}, {1,2}, {5} ∩∅, ∅⟩, [1, 3] ∩[0, 2]⟩= ⟨⟨{1, 2}, {1, 2}, ∅, ∅⟩, [1, 2]⟩

62
J.D. Scott et al.
The resulting set of propagation events is {cdom,lmax}: the set of candidate
characters at index 3 was restricted, followed by the set of candidate lengths. If
the order of the two restriction operations is reversed, then the resulting domain
representation is the same:
⟨A′′4, N ′′⟩:= cnq(lle(⟨Ab, N⟩, 2), 3, 6)
:= cnq(⟨⟨{1, 2}, {1, 2}, {5, 6} ∩∅, ∅⟩, [1, 3] ∩[0, 2]⟩, 3, 6)
:= cnq(⟨⟨{1, 2}, {1, 2}, ∅, ∅⟩, [1, 2]⟩, 3, 6)
:= ⟨⟨{1, 2}, {1, 2}, ∅\ {6}, ∅⟩, [1, 2]⟩= ⟨⟨{1, 2}, {1, 2}, ∅, ∅⟩, [1, 2]⟩
but the set of propagation events generated by this sequence of restriction opera-
tions is diﬀerent, namely only {lmax}: no cdom propagation event is generated,
because the cnq restriction operation did not change the domain.
⊓⊔
The monotonicity of event systems is important because propagation events
are used in the ﬁxpoint algorithm to schedule propagators for execution. A
non-monotonic event system makes propagator scheduling non-monotonic in the
sense that executing propagators in diﬀerent orders can generate diﬀerent prop-
agation events possibly resulting in diﬀerent amounts of pruning; see [21,24] for
a complete explanation of event systems and eﬃcient propagator scheduling.
Figure 4(b) shows a monotonic version of our propagation event system, in
which lmax implies cdom. Intuitively, this implication arises from the repre-
sentation invariant: any change to the upper bound of the set N of candidate
lengths must empty at least one set A[i] of candidate characters. A proof of the
monotonicity of this propagation event system is omitted for reasons of space:
see [22] for further discussion.
7
Experimental Evaluation
Experimental Methodology. Experiments were carried out on a VirtualBox 4.3.10
virtual client with 1024 MB of RAM, running Xubuntu 14.04. The host machine
was a 2.66 GHz Intel Core 2 Duo with 4 GB of RAM, running OpenSUSE 13.1.
Code for implemented propagators was written in C++ for the Gecode 4.4.0
constraint solving library, using 64-bit bitsets, and compiled with gcc 4.8.4.
For each problem including strings of unknown length, the same initial max-
imum length b was used for every string variable, and the experiments were run
for several possible values of b when possible. Timeout always was at 10 min.
Models and Implementations.
BLS variables are compared with two other
bounded length string representations for ﬁnite domain CP solvers.
The ﬁrst of these methods, the de facto standard for solving bounded-length
string constraint problems using a CP solver, is the padded-string method [12],
which requires no proper string variable type; instead, each string unknown in
a problem is modeled as a pessimistically large array of integer variables, allow-
ing multiple occurrences of a null or padding symbol at the end of each string.

Design and Implementation of Bounded-Length Sequence Variables
63
In the padded-string method there are no propagators implementing string con-
straints. Instead, each string constraint is modeled as a decomposition consist-
ing of a conjunction of reiﬁed constraints over the sequence of integer variables
corresponding to each string in the scope of the constraint, which express the
relationship between the length of the modeled string and the occurrences of the
padding character in the corresponding sequence.
The second method, the aggregate string method [23], is similar to the padded
string method, but an integer variable is added for each string unknown, mod-
eling its set of candidate lengths. Thus, the model of a string unknown in the
aggregate-string method is isomorphic to the bounded-length sequence represen-
tation in Sect. 3, modulo the inclusion of the padding character. The aggregate-
string method also diﬀers from the padded-string method in that each string
constraint is implemented by a single propagator. This method is implemented
with the aid of the indexical compiler [15].
All implementations, as well as the corresponding models for the benchmarks
listed below, can be found at https://bitbucket.org/jossco/gecode-string. Note
that these experiments do not use the machinery of the recent string extension [2]
of the MiniZinc modeling language [17].
Search. A serviceable, if not compelling, branching heuristic for string variables
applies a value selection heuristic for integer variables to either a set of candidate
characters or the set of candidate lengths. More interesting branching heuristics
for string variables should be explored; however, a simple heuristic is suﬃcient
for the purpose of comparing our string variable type with the two alternate
methods described above. We used the following heuristic: at each choice point,
the ﬁrst unassigned string variable is selected; the sets of candidate characters
at the mandatory indices are evaluated, and the set with the lowest cardinality
is selected; if there exist no mandatory indices, then the minimum of the set of
candidate lengths is increased instead. Character value selections are made by
splitting the selected set at its median element, and taking the lower half ﬁrst.
Benchmarks. The well-known string benchmarks of hampi [9], kaluza [18], and
sushi [8] have previously been shown to be trivial for CP solvers even without
sequence variables, see [12,23] for instance, hence we do not revisit them here.
Word Design for DNA Computing on Surfaces. This problem [6], with origins
in bioinformatics and coding theory, is to ﬁnd the largest set of strings S, each
of length 8 and with alphabet Σ = {A, T, C, G}, such that:
– Each string s ∈S contains exactly four characters from the set {C, G}.
– For all x, y ∈S such that x ̸= y, x and y diﬀer in at least four positions.
– For all x, y ∈S (including when x = y), the strings xrev and Comp(y) diﬀer
in at least four positions, where Comp is the permutation (AT)(CG).
Despite its name, this problem is actually rather a weak candidate for modeling
with bounded-length string variables. Every word in S has the same ﬁxed length,

64
J.D. Scott et al.
Table 3. Time, in seconds, either to ﬁnd a solution if one exists, or to prove b-bounded
unsatisﬁability otherwise; t indicates that the instance timed out (>10 min).
Padded
Aggregate
BLS
Benchmark
inst \ b 256
512
1024 256
512
1024 256 512
1024
SAT\UNSAT
WordDesign
80
216.0
221.3
25.8
sat
WordDesign
85
290.0
t
32.9
sat
WordDesign
112
t
t
92.5
sat
ChunkSplit
16
t
t
t
t
t
t
0.2
2.0
21.9
sat
ChunkSplit
25
t
t
t
t
t
t
1.4
15.0 215.2 sat
ChunkSplit
29
t
t
t
50.8 t
t
0.3
2.0
21.7
sat
Levenshtein
2
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
sat
Levenshtein
37
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
sat
Levenshtein
84
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
0.1
sat
anbn
89
2.0
t
t
0.2
0.5
1.7
0.1
0.3
2.3
sat
anbn
102
2.0
2.1
2.0
0.1
0.1
0.1
0.5
4.5
50.3
unsat
anbn
154
0.1
0.1
0.4
0.1
0.2
0.5
0.4
3.1
34.0
unsat
StringReplace
20
t
t
t
t
t
t
t
t
t
—
StringReplace
136
0.9
3.5
t
0.9
3.6
t
0.1
0.2
1.6
unsat
StringReplace
142
21.0 t
t
0.1
0.4
1.5
0.1
0.3
1.8
sat
Hamming
389
t
t
t
t
t
t
0.4
3.9
59.5
unsat
Hamming
1005
8.5
t
t
7.9
t
t
0.7
5.8
61.8
unsat
Hamming
1168
t
t
t
t
t
t
0.4
3.7
55.9
unsat
and most of the constraints are binary constraints on characters; hence, the
problem is easily modeled as an |S| × 8 matrix of integer variables. In such a
ﬁxed length string case, a proper string variable type seems to have relatively
little to oﬀer over a ﬁxed-length array of integer variables.
Experimental results for the Word Design problem are shown in Table 3. We
treat each value (shown in the instance column) of the cardinality of S as a sat-
isfaction problem. No implementation times out for |S| ≤80, only the aggregate
implementation times out for 80 < |S| ≤85, only the BLS implementation does
not time out for 85 < |S| ≤112, and all implementations time out for 112 < |S|.
As all strings in the problem are of ﬁxed length, there is nothing to be gained
by varying the maximum string length b for the problem; hence, we report only
one run for b = 8 per cardinality for each implementation.
The model using BLS variables performs comparatively well. As all strings
in the problem are of ﬁxed length, the padded and aggregate methods collapse
into a single method; the propagators provided by the aggregate model for the
purpose of treating ﬁxed-length arrays of variables as bounded-length strings
are unhelpful for true ﬁxed-length strings. In this ﬁxed-length context, the supe-
rior performance of BLS variables must be attributed to the choice of a bitset
representation for the alphabet.

Design and Implementation of Bounded-Length Sequence Variables
65
Benchmark of Norn. A set of approximately 1000 string constraint problems
were generated for the unbounded-length string solver norn [1]. These instances
do not require Unicode and have regular-language membership constraints, gen-
erated by a model checker, based on counter-example-guided abstraction reﬁne-
ment (CEGAR), for string-manipulating programs, as well as concatenation and
length constraints on string variables, and linear constraints on integer variables.
Instances of the benchmark of norn are written in the cvc4 dialect of the
smtlib2 language [14]. These instances were translated into Gecode models
using the three methods described above. Regular languages in the instances are
speciﬁed as regular expressions; these were directly translated into Gecode’s
regular-expression language and modeled by RegularL constraints, with the
exception of expressions of the form X ∈ϵ, which were instead modeled by
a constraint Len(X, 0). Approximately three quarters of all instances in the
benchmark include a negated regular expression. As Gecode does not imple-
ment negation for regular expressions, these instances were omitted from the
experiments as a matter of convenience; adding support for taking the comple-
ment of regular languages to Gecode is straightforward (e.g., [13]).
We solve the remaining 255 instances in a bounded-length context; our results
are therefore incomparable with those of an unbounded-length solver such as
norn. For satisﬁable instances, norn generates a language of satisfying assign-
ments for each string variable, whereas a CP-based method returns individual
satisfying strings; furthermore, norn can determine that an instance is unsat-
isﬁable for strings of any length, whereas CP solvers are limited to determining
b-bounded unsatisﬁability, for some practical upper bound b on string length.
Nevertheless, the benchmark of norn remains interesting in a bounded-length
context, as there are several challenging instances to be found. Complete results
for the 255 evaluated instances are omitted for space; for further discussion,
see [22]. Table 3 shows results for three instances in each of the ﬁve categories of
the benchmark of norn; these instances were selected as they appear to be the
hardest, in their respective categories, for solving in a bounded-length context.
As shown in Table 3, the BLS variable implementation is either signiﬁcantly
faster than the aggregate and padding implementations, or tied with them,
except (for reasons we failed so far to understand) on the unsatisﬁable instances
of the anbn benchmark. For large upper bounds on string sizes (b > 256), both of
the decomposition-based implementations are prone to time outs, mostly likely
as the search space is exhausting the available memory.
8
Conclusion
We have designed a new variable type, called bounded-length sequence (BLS)
variables. Implemented for the copying CP solver Gecode, BLS variables ease
the modeling of string constraint problems while simultaneously providing con-
siderable performance improvements over the alternatives. The described exten-
sion is agreed to become oﬃcial part of Gecode. It would be interesting to see
how our ideas transpose to a trailing CP solver.

66
J.D. Scott et al.
Bitsets are less appropriate for large alphabets, say when full Unicode (16 bit)
coverage is required: future work includes adapting the choice of character-set
representation based on alphabet size, possibly using BDDs, or a sparse-bitset
representation similar to that of [5].
BLS variables, together with propagators for string constraints [22], are part
of an emerging ecosystem of string solving in CP. The recent string extension [2]
of the MiniZinc modeling language [17] — for which we have extended the
FlatZinc interpreter of Gecode in order to support the BLS variable exten-
sion — can only serve to encourage further development.
Acknowledgements. We thank the anonymous referees for their helpful comments.
The authors based at Uppsala University are supported by the Swedish Research Coun-
cil (VR) under grant 2015-4910.
References
1. Abdulla, P.A., Atig, M.F., Chen, Y.-F., Hol´ık, L., Rezine, A., R¨ummer, P.,
Stenman, J.: Norn: an SMT solver for string constraints. In: Kroening, D.,
P˘as˘areanu, C.S. (eds.) CAV 2015. LNCS, vol. 9206, pp. 462–469. Springer, Cham
(2015). doi:10.1007/978-3-319-21690-4 29. http://user.it.uu.se/∼jarst116/norn
2. Amadini, R., Flener, P., Pearson, J., Scott, J.D., Stuckey, P.J., Tack, G.: MiniZ-
inc with strings. In: Hermenegildo, M., L´opez-Garc´ıa, P. (eds.) LOPSTR 2016.
LNCS, vol. 10184, pp. 52–67 (2016). Post-proceedings, Pre-proceedings Version at
Computing Research Repository. https://arxiv.org/abs/1608.03650
3. Bisht, P., Hinrichs, T., Skrupsky, N., Venkatakrishnan, V.N.: WAPTEC: white-
box analysis of web applications for parameter tampering exploit construction.
In: Chen, Y., Danezis, G., Shmatikov, V. (eds.) Computer and Communications
Security (CCS 2011), pp. 575–586. ACM (2011)
4. Bjørner, N., Tillmann, N., Voronkov, A.: Path feasibility analysis for string-
manipulating
programs.
In:
Kowalewski,
S.,
Philippou,
A.
(eds.)
TACAS
2009. LNCS, vol. 5505, pp. 307–321. Springer, Heidelberg (2009). doi:10.1007/
978-3-642-00768-2 27
5. Demeulenaere, J., Hartert, R., Lecoutre, C., Perez, G., Perron, L., R´egin, J.-C.,
Schaus, P.: Compact-table: eﬃciently ﬁltering table constraints with reversible
sparse bit-sets. In: Rueher, M. (ed.) CP 2016. LNCS, vol. 9892, pp. 207–223.
Springer, Cham (2016). doi:10.1007/978-3-319-44953-1 14
6. van Dongen, M.: CSPLib problem 033: word design for DNA computing on sur-
faces. http://www.csplib.org/Problems/prob033
7. Emmi, M., Majumdar, R., Sen, K.: Dynamic test input generation for data-
base applications. In: Rosenblum, D.S., Elbaum, S.G. (eds.) Software Testing and
Analysis (ISSTA 2007), pp. 151–162. ACM (2007)
8. Fu, X., Powell, M.C., Bantegui, M., Li, C.C.: Simple linear string constraints.
Formal Aspects Comput. 25, 847–891 (2013)
9. Ganesh, V., Kie˙zun, A., Artzi, S., Guo, P.J., Hooimeijer, P., Ernst, M.: HAMPI: a
string solver for testing, analysis and vulnerability detection. In: Gopalakrishnan,
G., Qadeer, S. (eds.) CAV 2011. LNCS, vol. 6806, pp. 1–19. Springer, Heidelberg
(2011). doi:10.1007/978-3-642-22110-1 1

Design and Implementation of Bounded-Length Sequence Variables
67
10. Gange, G., Navas, J.A., Stuckey, P.J., Søndergaard, H., Schachte, P.: Unbounded
model-checking with interpolation for regular language constraints. In: Piterman,
N., Smolka, S.A. (eds.) TACAS 2013. LNCS, vol. 7795, pp. 277–291. Springer,
Heidelberg (2013). doi:10.1007/978-3-642-36742-7 20
11. Golden, K., Pang, W.: A constraint-based planner applied to data processing
domains. In: Wallace, M. (ed.) CP 2004. LNCS, vol. 3258, p. 815. Springer,
Heidelberg (2004). doi:10.1007/978-3-540-30201-8 93
12. He, J., Flener, P., Pearson, J., Zhang, W.M.: Solving string constraints: the case
for constraint programming. In: Schulte, C. (ed.) CP 2013. LNCS, vol. 8124, pp.
381–397. Springer, Heidelberg (2013). doi:10.1007/978-3-642-40627-0 31
13. Hopcroft, J., Motwani, R., Ullman, J.D.: Introduction to Automata Theory, Lan-
guages, and Computation, 3rd edn. Addison Wesley, Redwood City (2007)
14. Liang, T., Reynolds, A., Tinelli, C., Barrett, C., Deters, M.: A DPLL(T) theory
solver for a theory of strings and regular expressions. In: Biere, A., Bloem, R.
(eds.) CAV 2014. LNCS, vol. 8559, pp. 646–662. Springer, Cham (2014). doi:10.
1007/978-3-319-08867-9 43
15. Monette, J.-N., Flener, P., Pearson, J.: Towards solver-independent propagators.
In: Milano, M. (ed.) CP 2012. LNCS, pp. 544–560. Springer, Heidelberg (2012).
doi:10.1007/978-3-642-33558-7 40.
http://www.it.uu.se/research/group/astra/
software\#indexicals
16. Negrevergne, B., Guns, T.: Constraint-based sequence mining using constraint
programming. In: Michel, L. (ed.) CPAIOR 2015. LNCS, vol. 9075, pp. 288–305.
Springer, Cham (2015). doi:10.1007/978-3-319-18008-3 20
17. Nethercote, N., Stuckey, P.J., Becket, R., Brand, S., Duck, G.J., Tack, G.:
MiniZinc: towards a standard CP modelling language. In: Bessi`ere, C. (ed.) CP
2007. LNCS, vol. 4741, pp. 529–543. Springer, Heidelberg (2007). doi:10.1007/
978-3-540-74970-7 38. http://www.minizinc.org
18. Saxena, P., Akhawe, D., Hanna, S., Mao, F., McCamant, S., Song, D.: A symbolic
execution framework for JavaScript. In: Security and Privacy (S&P 2010), pp.
513–528. IEEE Computer Society (2010)
19. Schulte, C., Carlsson, M.: Finite domain constraint programming systems. In:
Rossi, F., van Beek, P., Walsh, T. (eds.) Handbook of Constraint Programming,
pp. 495–526. Elsevier, Amsterdam (2006). Chap. 14
20. Schulte, C., Stuckey, P.J.: Eﬃcient constraint propagation engines. Trans. Pro-
gram. Lang. Syst. 31(1), 2:1–2:43 (2008)
21. Schulte, C., Tack, G.: Implementing eﬃcient propagation control. In: Proceedings
of TRICS: Techniques for Implementing Constraint programming Systems, a Con-
ference Workshop of CP 2010, St Andrews, UK (2010)
22. Scott, J.: Other things besides number: abstraction, constraint propagation, and
string variable types. Ph.D. thesis, Uppsala University, Sweden (2016). http://urn.
kb.se/resolve?urn=urn:nbn:se:uu:diva-273311
23. Scott, J.D., Flener, P., Pearson, J.: Constraint solving on bounded string variables.
In: Michel, L. (ed.) CPAIOR 2015. LNCS, vol. 9075, pp. 375–392. Springer, Cham
(2015). doi:10.1007/978-3-319-18008-3 26
24. Tack, G.: Constraint propagation: models, techniques, implementation. Ph.D. the-
sis, Saarland University, Germany (2009)
25. The Gecode Team: Gecode: a generic constraint development environment (2006).
http://www.gecode.org

In Search of Balance: The Challenge
of Generating Balanced Latin Rectangles
Mateo D´ıaz1(B), Ronan Le Bras2, and Carla Gomes2
1 Center for Applied Mathematics, Cornell University,
Ithaca, NY 14853, USA
md825@cornell.edu
2 Computer Science Department, Cornell University,
Ithaca, NY 14853, USA
{lebras,gomes}@cs.cornell.edu
Abstract. Spatially Balanced Latin Squares are combinatorial struc-
tures of great importance for experimental design. From a computational
perspective they present a challenging problem and there is a need for
eﬃcient methods to generate them. Motivated by a real-world applica-
tion, we consider a natural extension to this problem, balanced Latin
Rectangles. Balanced Latin Rectangles appear to be even more deﬁant
than balanced Latin Squares, to such an extent that perfect balance may
not be feasible for Latin rectangles. Nonetheless, for real applications, it
is still valuable to have well balanced Latin rectangles. In this work,
we study some of the properties of balanced Latin rectangles, prove the
nonexistence of perfect balance for an inﬁnite family of sizes, and present
several methods to generate the most balanced solutions.
Keywords: Latin Rectangles · Experimental design · Local search ·
Constraint satisfaction problem · Mixed-integer programming
1
Introduction
In recent years we have seen tremendous progress in search and optimization pro-
cedures, driven in part by hard challenging structured problems, such as those
from combinatorial design. As an example, Spatially Balanced Latin squares
(SBLSs) have led to the notion of streamlining constraints [7], in which addi-
tional, non-redundant constraints are added to the original problem to increase
constraint propagation and to focus the search on a small part of the subspace,
and other notions such as XOR constraints, a powerful general purpose stream-
lining technique, with applications in search, probabilistic reasoning, and sto-
chastic optimization [1,5,6,8], and local search procedures [9].
SBLSs have been studied in detail, e.g., [4,10,11,14]. In the recent work, by
combining streamlining reasoning and human-computation, Le Bras et al. [10,11]
discovered the ﬁrst polynomial time construction for the generation of SBLSs,
for any size n such that 2n + 1 is prime, and formally proved correctness of the
c
⃝Springer International Publishing AG 2017
D. Salvagnin and M. Lombardi (Eds.): CPAIOR 2017, LNCS 10335, pp. 68–76, 2017.
DOI: 10.1007/978-3-319-59776-8 6

Generating Balanced Latin Rectangles
69
procedure. This constructive solution is the ﬁrst example of the use of automated
reasoning and search to ﬁnd a general constructive algorithm for a combinatorial
design problem for an unlimited range of sizes. Previous automated reasoning
results in combinatorial mathematics were restricted to ﬁnding constructions for
a particular (ﬁxed) problem size.
The demand for balanced Latin squares arises from agronomic ﬁeld exper-
iments, where one wants to compare n treatments consisting of n fertilizers
applied in diﬀerent orderings. Such experiments can be modeled using a Latin
Square −a standard procedure in the ﬁeld of experimental design. However,
for agronomic experiments there is a caveat, geometric imbalance can poten-
tially bias the results, see for example [14]. In this paper, we propose as a new
benchmark problem Partially Balanced Latin Rectangles (PBLRs), a natural
extension SBLSs. PBLRs are also used in experimental design; in fact, in prac-
tice rectangular sizes are more common than squares, one of the key reasons why
we study them. Despite signiﬁcant progress concerning SBLSs, little is known
about PBLRs. Here, we present some of the properties of balanced Latin rectan-
gles, prove the nonexistence of perfect balance for an inﬁnite family of sizes, and
present several methods to generate the most balanced solutions. We also iden-
tify an interesting easy-hard-easy pattern in the run times of a MIP algorithm,
for proving optimality, and a CSP approach, for ﬁnding feasible solutions. We
believe that the PBLR problem is ideal as a benchmark for the study of diﬀerent
search and optimization approaches.
2
Preliminaries
Deﬁnition 1. Let k, n ∈N be positive numbers such that k ≤n and A a k × n
matrix. We say that A is a Latin rectangle if every cell of A contains a number
in {1, . . . , n} and there are no repetitions in a row or a column.
Let u, v ∈{1, . . . , n} be diﬀerent symbols and i ≤k. Deﬁne di(u, v) as
the distance between the symbols u and v in row i. We deﬁne the distance
d(u, v) := 
i≤k di(u, v).
Deﬁnition 2. We say that a k × n rectangle A is spatially balanced if all
the distances d(u, v) are the same. In order to abbreviate we refer to them as
SBLR(k, n).
The following proposition provides novel necessary and suﬃcient conditions for
the existence of SBLRs.
Proposition 1. If there exists a solution for SBLR(k, n) then the distance
between any pair of symbols is equal to k(n + 1)
3
.
Proof. If there is a solution for SBLR(k, n) then the sum of all the possible
distances divided by all the possible pairs of symbols must be integer. For a ﬁxed
row, we have n−d pairs with distance d. Since we have k rows, the sum of all the
distances is equal to k n
d=1 d(n−d) = k

n n
d=1 d −n
d=1 d2
= kn(n−1)(n+1)
6
.
Dividing this by the number of pairs,
n
2

, gives the result.

70
M. D´ıaz et al.
As a corollary, k ≡3 0 or n ≡3 2 are necessary conditions to have balance,
since the distances are integers, k(n + 1) must be multiple of 3 and one of the
two conditions must hold.
We note that two Latin rectangles are in the same isotopy class if it is possible
to obtain one from the other by permuting its rows, columns or symbol labels.
However, spatial balance is invariant only under row permutations or relabels of
the symbols. In order to avoid redundancy, we say that a Latin rectangle is in
its reduced form if the ﬁrst row is the sequence 1, 2, 3, . . . , n and the ﬁrst column
is in Lexicographic order, i.e. the digits are in ascending order.
2.1
Non-existence Results
Our experiments suggest that perfectly balanced rectangles do not exist, see
Sect. 4. The next results support this claim for small ks.
Proposition 2. For k = 2 the only n for which SBLR(2, n) is nonempty is 2.
Proof. When (k, n) = (2, 2), the only solution is perfectly balanced. Let n > 2
and assume that there is a solution for SBLR(k, n). By Proposition 1 we know
that n = 3l + 2, for some integer l > 0. Without loss of generality we may
assume that in the ﬁrst row the extremal symbols are 1 and n. Then, for the
pair (1, n) the smallest distance that a solution could achieve is n, as shown in
the rectangle.
1 . . . i i + 1 . . . n
∗. . . 1
n
. . . ∗
Hence, d(1, n) cannot satisfy the constraint in Proposition 1, since 2(n + 1)
3
=
2l + 2 < 3l + 2 = n ≤d(1, n). This contradiction completes the proof.
Proposition 3. For k = 3 the only n for which SBLR(3, n) is nonempty is 3.
Proof. For the case (k, n) = (3, 3), any solution is balanced. For the cases (k, n) =
(3, 4) and (k, n) = (3, 5), we used a complete approach to check all the possible
solutions and proved that none exists.
Suppose now that n > 5, by Proposition 1 we know that if A ∈SBLR(k, n)
then for every pair of symbols we have d(a, b) = n + 1. Again, without loss of
generality we may assume that the ﬁrst row of A is the sequence 1, 2, 3, . . . , n.
Then d1(1, n) = n−1, this forces the pair 1 and n to be next to each other in the
next two rows, otherwise d(1, n) ̸= n+1. Analogously, since d1(1, n−1) = n−2,
the pair (1, n −1) must have a distance of 1 in one of the remaining rows and 2
in the other, as it is shown in the rectangle.
1 . . .
∗
∗∗. . .
∗
∗∗∗. . . n −1 n
∗. . . n −1 1 n . . .
∗
∗∗∗. . .
∗
∗
∗. . .
∗
∗∗. . . n −1 ∗1 n . . .
∗
∗
Thus, we can bound d(n −1, n) = d1(n −1, n) + d2(n −1, n) + d3(n −1, n) ≤
1 + 3 + 2 = 6 < n + 1, which yields a contradiction, this completes the proof for
all the remaining cases.

Generating Balanced Latin Rectangles
71
Proposition 4. If there exists a solution for SBLR(k, n) then 6 −
18
n + 1 ≤k.
Proof. Suppose that A ∈SBLR(k, n), without loss of generality we may assume
that A is in its reduced form. With this setting we know that
d1(1, n −1) = n −2,
d1(1, n) = n −1
and
d1(n −1, n) = 1.
(1)
By the triangular inequality we have that for every i = 1, . . . , k, di(n −1, n) ≤
di(1, n −1) + di(1, n), summing over all the i ≥2,
k

i=2
di(n −1, n) ≤
k

i=2
di(1, n −1) +
k

i=2
di(1, n).
Using the distances that we know (1), we obtain d(n −1, n) −1 ≤d(1, n −1) −
(n−1)+di(1, n)−(n−2), since A is perfectly balanced, k(n+1)
3
≤2k(n+1)
3
−2n+4.
By simplifying this expression we get the stated result.
With our last result we proved that there are no solutions for k = 4 and
n > 8, and there are also no solutions for k = 5 and n > 17. Thus, at least
for small ks and for all the sizes that do not satisfy k ≡3 0 or n ≡3 2, perfect
balanced cannot be achieved.
2.2
Partially Balanced Latin Rectangles
These results suggest that SBLR(k, n) may be an infeasible problem when k < n.
For real applications if balance cannot be achieved, it is useful to have the
most balanced solution. To measure the balance of a rectangle R we deﬁne the
imbalance function, I(R) = 
i<j
d(i, j) −k(n+1)
3
 . Note that this function is
non-negative and if the rectangle is perfectly balanced I(R) = 0.
Algorithm 1. Random-restart hill climbing balanced Latin Rectangles
input : k, n
output: A well Balanced Latin Rectangle R
Generate a cyclic n × n Latin Square L and deﬁne R as a k × n rectangle of
zeros;
for a ﬁxed number of iterations do
Take k random rows of L and assign them to R;
for a ﬁxed number of iterations do
Draw a pair of columns (i, j) at random and assign to ¯R a copy of R
with the columns (i, j) switched;
if I( ¯R) < I(R) then
R = ¯R;
if I(R) = 0 then
Break all the loops, we found perfect balance;

72
M. D´ıaz et al.
Deﬁnition 3. Let R be a k×n Latin rectangle, we say that it is partially spatially
balanced if I(R) is minimum. For a ﬁxed size, we denote by PBLR(k, n) the set
of these rectangles.
Thus, if perfect balance is achieved, PBLR and SBLR are equivalent. This
presents a new challenge, since the minimum value of the imbalance is unknown
a priori. Now, we need to determine the minimum imbalance, that we call Ikn,
and at the same time generate the associated partially balanced rectangle.
3
Proposed Solutions
We developed three diﬀerent approaches for generating rectangles with minimum
imbalance: a local search algorithm, a constraint satisfaction encoding and a
mixed-integer programming encoding. In this section we describe these methods
and discuss some advantages and disadvantages of each encoding.
3.1
Local Search
The objective of the algorithm is to minimize the imbalance function, by perform-
ing random-restart hill climbing local search on the space of Latin rectangles.
Our algorithm starts with a cyclic solution, i.e., every row is obtained by shifting
by one position the previous row, and it was adapted from one of the best known
algorithms for balanced Latin squares proposed in [13]. One issue with this algo-
rithm is that it starts with an initial solution and never leaves its isotopic class.
So if there is no solution for that particular class, the algorithm will only explore
that class. Nevertheless, this technique was used before with successful results
for Latin squares. In that case, the stopping criterion is clear, I(R) = 0; but
for partially balance Latin rectangles the value of minR I(R) is unknown. Thus,
while this incomplete approach can only be used to ﬁnd upper bounds on the
minimum imbalance, it constitutes a relevant baseline and in practice, it is able
to ﬁnd good solutions fast.
3.2
Constraint Satisfaction Problem and Mixed-Integer
Programming Encodings
For any m ∈N deﬁne [m] = {1, . . . , m}. Let Δn be the set of possible pairs
(i, j) with i, j ∈[n] such that i < j. We consider a simple constraint satisfaction
problem that encodes the problem, where the variables Rij indicate the position
(column number) of the symbol j in row i. The ﬁrst set of alldiﬀerent constraints
guarantees that the symbols cover all the columns, while the second one guaran-
tees the non-repetition of symbols in columns. The last constraint assures that
the imbalance of solutions is upper bounded by the constant M.
Rij ∈[n]
∀i ∈[k], j ∈[n]
alldiﬀ(Ri1, ..., Rin)
∀i ∈[k]
alldiﬀ(R1j, ..., Rkj)
∀j ∈[n]

(i,j)∈Δn
 k(n+1)
3
−k
l=1 |Rli −Rlj|
 ≤M.
(2)

Generating Balanced Latin Rectangles
73
A classical approach to boost the performance of this algorithm is to use sym-
metry breaking constraints [2,3,12], which is done with the use of the following
constraints:
R1j = j
∀j ∈{1, . . . , n}
Ri1 < R(i+1)1
∀i ∈{1, . . . , k −1}
(3)
These constraints enforce that the Latin rectangle is in its reduced form, i.e., the
ﬁrst row is the sequence 1, 2, 3, . . . , n, and the ﬁrst column is in Lexicographic
order, i.e. the digits are in ascending order. By imposing these constraints we
are shrinking the size of the space of solutions by a factor of n!k! and only
eliminating equivalent elements from isotopic classes. For balanced squares, pre-
vious formulations use other streamlining constraints such as symmetry, that
have proven to be very eﬀective and, in fact, the solutions found with those
constraints served as basis to uncover the underlying pattern that gave rise to
the polynomial time construction in [10]. Unfortunately, the rectangular shape
prevents us from applying such constraints.
The main drawback with the CSP formulation is that its performance
depends on the scheme chosen for the values of M and it does not incremen-
tally learn from one iteration to the next. To tackle this issue we transform this
formulation into a mixed-integer program, using I(·) as an objective function.
The MIP formulation can be written as:
min 
(i,j)∈Δn Dij
s. t. Rij ∈[n]
∀i ∈[k], j ∈[n],
Dij =
 k(n+1)
3
−k
l=1 |Rli −Rlj|

∀(i, j) ∈Δn
Rih ̸= Ril
∀i ∈[k], (h, l) ∈Δn,
Ril ̸= Rjl
∀(i, j) ∈Δk, l ∈[n]
The formulation in this case is almost the same as in (2), with the only
diﬀerence that the left-hand side of the last constraint becomes the function
that we are minimizing. Also, note that the alldiﬀerent constraints translate to
a conjunction of binary disequalities.
Both the CSP and MIP encoding can be used to certify optimality, namely
they can prove that they ﬁnd the minimal imbalance Ikn. Nevertheless, the
MIP solver does not require an initial guess for the objective function bound.
In addition, through branch and cut methods, it generates lower bounds on the
optimal value of imbalance using a hierarchy of linear relaxations. In particular,
even before proving optimality, the MIP solver can certify that there is no perfect
balance if it ﬁnds a nontrivial lower bound, which could be potentially practical
even for the square case.
4
Experimental Results
We identiﬁed an interesting easy-hard-easy pattern in the runtimes of the MIP
solver, for proving optimality, and the CSP, for ﬁnding a PBLR given Ikn, when

74
M. D´ıaz et al.
k/n varies. For a ﬁxed n, the problem becomes harder as k increases; it is easier
when k = n, see Fig. 1.
All the experiments were run using 12 cores on a single compute server with
2.93 GHz Intel(R) Xeon(R) x5670 processors and 48 GB RAM. For the optimiza-
tion and satisﬁability problems we use IBM ILOG CPLEX-CP Optimizer.
Proving Optimality. To compare the CSP and MIP formulations, we ran
experiments for proving optimality, each method with 6 ≤n ≤8, 2 ≤k ≤n and
a time limit of 35 h. The MIP solver was able to solve the problem for all the
instances up to n = 8 and k = 4, see Table 1. To prove optimality using CSP
Fig. 1. Easy-hard-easy patterns in running times of the MIP solving for proving opti-
mality and the CSP proving satisﬁability of Ikn with n = 6, 7, 8.
Fig. 2. Bounds found by the local search algorithm and the MIP solver, from left to
right n = 10, 11, 12.
Table 1. Minimum imbalance found with our algorithms. The column corresponds to
k and the row to n. Grey cells indicate a proven optimal value.
2
3
4
5
6
7
8
9
10
11
12
4
2.¯6
4
5.¯3
5
8
6
8
0
6
16
12
13.¯3
16
0
7
28
22
22.¯6
22.¯6
20 18.¯6
8
40
36
32
30
24
28
0
9 65.¯3 56
56.¯6
56
52
66
60
0
10 92
86
92
66.¯6 102 100 99.¯3
80
40
11 124 120 122
122 126 136
132 128 110
0
12 168 158 162.¯6 170.¯6 120 183 184.¯6 178 174.¯6 147.¯3 0

Generating Balanced Latin Rectangles
75
we start with an initial feasible guess for the bound M and decrease it until the
problem is unsatisﬁable. A natural way to measure the time is to take the sum
of the last satisﬁable case and the ﬁrst infeasible one. The CSP solver was good
at proving optimality for the square cases where balance is feasible. In almost
any other case, if n ≤6 it takes more than the time limit to run the unsatisﬁable
case. Thus, the MIP seems to be a better method for proving optimality in
rectangles. However, the CSP formulation is fast at proving satisﬁability once
we have the minimum imbalance a priori. Finding well balanced solutions
for larger sizes. For larger sizes, it is very diﬃcult to prove optimality. As a
consequence, we use our local search algorithm to ﬁnd upper bounds. To compare
this approach with the MIP formulation, we ran the MIP solver with a time limit
of 1500 s and execute Algorithm 1 with 1000 iterations on both loops (it takes
less than 1200 s). For large sizes the pattern is similar, if k > 3 the local search
approach outperforms the MIP one, see Fig. 2.
5
Conclusions
We study the problem of generating balanced Latin rectangles. We conjecture
(and prove for several cases) that unlike squares, it is impossible to have perfect
balance for Latin rectangles. We examined the problem of ﬁnding the most
balanced solutions, namely partially balanced Latin rectangles. We developed
diﬀerent methods to prove optimality and to ﬁnd well balanced solutions when it
is too expensive to guarantee optimality. We identiﬁed an interesting easy-hard-
easy pattern in the run times of the MIP, for proving optimality, and CSP, for
ﬁnding a feasible solution. For larger problems, our simple local search approach
outperforms the MIP approach.
Finding Latin rectangles with minimum imbalance seems to be a very chal-
lenging computational problem. We believe that this problem is ideal as a bench-
mark for diﬀerent search and optimization approaches. We leave as an open
challenge for future researchers to improve the results in Table 1.
Acknowledgments. This work was supported by the National Science Foundation
(NSF Expeditions in Computing awards for Computational Sustainability, grants CCF-
1522054 and CNS-0832782, NSF Computing research infrastructure for Computational
Sustainability, grant CNS-1059284).
References
1. Ermon, S., Gomes, C.P., Sabharwal, A., Selman, B.: Low-density parity constraints
for hashing-based discrete integration. In: Proceedings of the 31th International
Conference on Machine Learning, ICML 2014, Beijing, China, 21–26 June 2014,
pp. 271–279 (2014)
2. Fahle, T., Schamberger, S., Sellmann, M.: Symmetry breaking. In: Walsh, T. (ed.)
CP 2001. LNCS, vol. 2239, pp. 93–107. Springer, Heidelberg (2001). doi:10.1007/
3-540-45578-7 7

76
M. D´ıaz et al.
3. Gent, I.P., Smith, B.M.: Symmetry breaking in constraint programming. In: Pro-
ceedings of ECAI-2000, pp. 599–603. IOS Press (2000)
4. Gomes, C., Sellmann, M., Van ES, C., Van Es, H.: The challenge of generating
spatially balanced scientiﬁc experiment designs. In: R´egin, J.-C., Rueher, M. (eds.)
CPAIOR 2004. LNCS, vol. 3011, pp. 387–394. Springer, Heidelberg (2004). doi:10.
1007/978-3-540-24664-0 28
5. Gomes, C.P., Hoﬀmann, J., Sabharwal, A., Selman, B.: Short XORs for model
counting: from theory to practice. In: Marques-Silva, J., Sakallah, K.A. (eds.) SAT
2007. LNCS, vol. 4501, pp. 100–106. Springer, Heidelberg (2007). doi:10.1007/
978-3-540-72788-0 13
6. Gomes, C.P., Sabharwal, A., Selman, B.: Model counting: a new strategy for
obtaining good bounds. In: Proceedings, The Twenty-First National Conference
on Artiﬁcial Intelligence and the Eighteenth Innovative Applications of Artiﬁcial
Intelligence Conference, Boston, Massachusetts, USA, 16–20 July 2006, pp. 54–61
(2006)
7. Gomes, C.P., Sellmann, M.: Streamlined constraint reasoning. In: Principles and
Practice of Constraint Programming - CP 2004, 10th International Conference, CP
2004, Toronto, Canada, 27 September–1 October 2004, Proceedings, pp. 274–289
(2004)
8. Gomes, C.P., van Hoeve, W.J., Sabharwal, A., Selman, B.: Counting CSP solutions
using generalized XOR constraints. In: AAAI, pp. 204–209 (2007)
9. Van Hentenryck, P., Michel, L.: Diﬀerentiable invariants. In: Principles and Prac-
tice of Constraint Programming - CP 2006, 12th International Conference, CP
2006, Nantes, France, 25–29 September 2006, Proceedings, pp. 604–619 (2006)
10. Le Bras, R., Gomes, C.P., Selman, B.: From streamlined combinatorial search to
eﬃcient constructive procedures. In: AAAI (2012)
11. Le Bras, R., Perrault, A., Gomes, C.: Polynomial time construction for spatially
balanced Latin squares (2012)
12. Rossi, F., Van Beek, P., Walsh, T.: Handbook of Constraint Programming. Elsevier,
Amsterdam (2006)
13. Smith, C., Gomes, C., Fernandez, C.: Streamlining local search for spatially bal-
anced Latin squares. In: IJCAI, vol. 5, pp. 1539–1541. Citeseer (2005)
14. Van Es, H., Van Es, C.: Spatial nature of randomization and its eﬀect on the
outcome of ﬁeld experiments. Agron. J. 85(2), 420–428 (1993)

Debugging Unsatisﬁable Constraint Models
Kevin Leo(B) and Guido Tack
Data 61/CSIRO, Faculty of IT, Monash University, Melbourne, Australia
{kevin.leo,guido.tack}@monash.edu
Abstract. The ﬁrst constraint model that you write for a new problem
is often unsatisﬁable, and constraint modelling tools oﬀer little support
for debugging. Existing algorithms for computing Minimal Unsatisﬁable
Subsets (MUSes) can help explain to a user which sets of constraints
are causing unsatisﬁability. However, these algorithms are usually not
aimed at high-level, structured constraint models, and tend to not scale
well for them. Furthermore, when used naively, they enumerate sets of
solver-level variables and constraints, which may have been introduced
by modelling language compilers and are therefore often far removed
from the user model.
This paper presents an approach for using high-level model structure
to, at the same time, speed up computation of MUSes for constraint mod-
els, present meaningful diagnoses to users, and enable users to identify
diﬀerent sources of unsatisﬁability in diﬀerent instances of a model. We
discuss the implementation of the approach for the MiniZinc modelling
language, and evaluate its eﬀectiveness.
1
Introduction
Modelling languages for constraint programming such as ESSENCE [6] and
MiniZinc [18] allow decision problems to be modelled in terms of high-level
constraints. Models are combined with instance data and are then compiled into
input programs for solving tools (solvers). The goal of these languages is to allow
users to solve constraint problems without expert knowledge of targeted solvers.
Unfortunately, real-world problems typically exhibit a level of complexity
that makes it diﬃcult to create a correct model. The ﬁrst attempt at modelling
a problem often results in an incorrect model. There are multiple ways in which
a model may be incorrect. In this paper we focus on the case of over-constrained
models where the conjunction of all constraints are unsatisﬁable for any instance.
When faced with unsatisﬁability, the user has few tools to help with debug-
ging. The main strategy usually consists in activating and deactivating con-
straints in an attempt to locate the cause of the problem, but this approach
is tedious and often impractical due to the fact that the fault may involve a
non-trivial combination of groups of constraints and instance data.
Several techniques for debugging unsatisﬁable constraint programs exist,
some are designed for debugging speciﬁc kinds of constraint programs [7,9,17],
Partly sponsored by the Australian Research Council grant DP140100058.
c
⃝Springer International Publishing AG 2017
D. Salvagnin and M. Lombardi (Eds.): CPAIOR 2017, LNCS 10335, pp. 77–93, 2017.
DOI: 10.1007/978-3-319-59776-8 7

78
K. Leo and G. Tack
while others focus on diagnosis of unsatisﬁability during search [11,19,21]. We
are concerned with approaches that are constraint-system agnostic [4,10,14].
Many of these approaches focus on the search for Minimal Unsatisﬁable Sub-
sets (MUSes): sets of constraints that together are unsatisﬁable, but become
satisﬁable with the removal of any one of the constraints. MUSes can therefore
help explain the sources of unsatisﬁability in a constraint program.
Existing tools for ﬁnding MUSes focus on program level constraints, i.e., the
constraints at the level of the solver. When combining MUS detection with high-
level modelling, this has two main drawbacks. Firstly, the solver-level program
typically contains hundreds or thousands of constraints, even for relatively simple
high-level models. MUS detection algorithms do not scale well to these problem
sizes1. Secondly, the user may ﬁnd it diﬃcult to interpret the resulting sets of
constraints, because they have lost all connection to the original model and may
involve variables that were introduced by the compilation.
The main contribution of this paper is an approach that uses high-level
model structure to guide the MUS detection algorithm. We show that
using the structure available in a high-level MiniZinc model can speed up the
search for MUSes. We also demonstrate how these can be presented to the user
in a meaningful and useful way, in terms of the high-level model instead of the
solver-level program. Finally, we show how MUSes found across a set of instances
can be expressed in terms of the high-level (parametric) model, allowing us to
generalise the detected conﬂicts and distinguish between genuine modelling bugs
and unsatisﬁability that arises from faulty instance data.
Structure.
The next section presents some of the background techniques.
Section 3 introduces a MUS detection algorithm that can take advantage of high-
level model structure. Section 4 discusses implementation aspects and presents
some experiments that show promising speed-ups. Section 5 shows how the addi-
tional information about model structure can be used to present more meaningful
diagnoses to the user, and Sect. 6 discusses how the presented approach can help
generalise the results found across several instances to the model-level. Finally,
Sect. 7 discusses related approaches and Sect. 8 concludes the paper.
2
Background
A constraint program is a formal representation of an instance of a decision or
optimisation problem. Constraint programs consist of a set of variables repre-
senting the decisions to be made, a set of domains representing the possible
assignments to these variables, and a set of constraints, in the form of predi-
cates which describe the relationship between variables. Optionally, an objective
function which is to be minimised or maximised can be deﬁned. A solution is a
set of assignments to the variables such that all constraints are satisﬁed and the
value of the objective function is optimal. If no assignment exists that satisﬁes
all constraints, the program is said to be unsatisﬁable.
1 [4] reports runtimes higher than 10 s for models of only 1000 constraints.

Debugging Unsatisﬁable Constraint Models
79
2.1
Constraint Models
MiniZinc is a high-level language for describing parametric models of problems.
These models can be combined with instance data and compiled into concrete
constraint programs that target speciﬁc solvers by the MiniZinc compiler.
In this paper we will use the Latin Square problem as a running example.
A Latin Square is an n by n matrix of integers where each row and column
contain permutations of the numbers 1 to n. Listing 1.1 presents a MiniZinc
model for this problem. Line 1 declares that the model has an integer parameter
‘n’, the dimension of the matrix. Line 2 creates the n×n matrix named X which
is a matrix of integer variables which must take values in the interval [1, n].
Line 4 introduces the ﬁrst set of constraints. It states that the variables in each
row of the matrix must take distinct values. Following this on line 5 the second
constraint states that the variables in each column must also take distinct values.
1
int: n;
2
array[1..n, 1..n] of var 1..n: X;
3
4
forall (r in 1..n) (alldifferent(row(X, r)));
5
forall (c in 1..n) (alldifferent(col(X, c)));
Listing 1.1. MiniZinc Model for the Latin Squares Problem
During compilation, MiniZinc performs a bottom-up translation, replacing
each argument to a predicate or function call with an auxiliary variable bound
to the result of the call. MiniZinc provides a set of standard decompositions
that encode constraints in terms of simpler predicates. Solver-speciﬁc MiniZinc
libraries can deﬁne custom decompositions, or declare a predicate as a built-in,
in which case it is simply added to the FlatZinc. For illustration purposes, a
compilation that decomposes the alldifferent constraint is shown, although
most MiniZinc solvers provide it as a built-in.
The following is an example trace through the compiler that introduces an
int ne predicate to the FlatZinc. Starting with line 4 from Lisiting 1.1 the
compiler starts to evaluate the forall predicate. The argument to the forall
constraint is an array comprehension. To evaluate the comprehension the com-
piler must loop through values for r in the set [1, n] and evaluate the expression
alldifferent(row(X, r)). With r set to 1 it evaluates row(X, 1) which returns an
array containing the variables corresponding to the ﬁrst row of the matrix X. Next
the compiler evaluates alldifferent(A) where A is the returned row array. The
standard library contains a deﬁnition for alldifferent with an array of inte-
ger variables, rewriting it to the more speciﬁc all different int(A) predicate.
The compiler must now evaluate this predicate call, resulting in a decomposition
into a set of not-equal constraints (int ne). The compiler then assigns 2 to r
and compilation proceeds. Compiling this model for n = 3 results in 18 int ne
constraints being added to the program.

80
K. Leo and G. Tack
2.2
Program Level Diagnosis
The Latin Squares model presented earlier has a set of symmetries which we
can break to ﬁnd solutions faster. One such symmetry is the ordering of values
in consecutive rows or columns. A naive user may try to break several of these
symmetries by adding the following constraints:
7
forall (r in 1..n-1) (lex_less(row(X, r), row(X, r+1)));
8
forall (c in 1..n-1) (lex_greater(col(X, c), col(X, c+1)));
Unfortunately, these constraints, combined with the alldifferent constraints
are in conﬂict since they enforce diﬀerent orderings on the rows and columns.
With these constraints added to the model, the decomposed, ﬂattened program
contains 58 constraints.
Current approaches for fault diagnosis in constraint programs work at the
level of individual constraints in a compiled program. These approaches typically
aim to enumerate all or some subset of Minimal Unsatisﬁable Subsets (MUSes).
Having a selection of MUSes gives the user a better chance of discovering the root
cause of unsatisﬁability, although in some cases a single MUS may be suﬃcient.
Enumerating the MUSes of a program can be achieved by exploring the
power-set, or all combinations, of constraints, performing a satisﬁability check
for each combination, and collecting all unsatisﬁable subsets, discarding all strict
supersets. The satisﬁability check is typically delegated to an external solver,
thus making the MUS detection algorithm itself agnostic to the concrete type of
problem that is being diagnosed.
Most MUS algorithms avoid enumerating the entire exponential-size power-
set in an attempt to minimise the number of satisﬁability checks required. For
example, they will avoid the exploration of any superset of an already discov-
ered MUS. A detailed survey of MUS enumeration approaches can be found
in [15]. Good techniques for pruning the search space can reduce the number
of satisﬁability checks considerably. However, depending on the time taken by
each satisﬁability check, this pruning may still not be enough to make these
approaches practical for large constraint programs.
When used with constraint programs generated by a compiler like MiniZinc,
the generated MUSes are likely to include constraints and variables introduced
during compilation. These can be diﬃcult to map back to their source in the
model. Presenting these MUSes to a user who is unfamiliar with the work-
ings of the compiler may not be conducive to ﬁxing modelling mistakes. A
MUS for the faulty Latin Squares model might include constraints such as
int lin le reif(...) and array bool or(...). Finding where these constraints
came from in the original model is possible but not simple.
3
Exploiting Model Structure for MUS Detection
This section introduces our approach to augment any existing MUS detection
algorithm so that it can take advantage of high-level model structure. Our app-
roach combines the idea to group constraints into a hierarchy, as explored in [11],

Debugging Unsatisﬁable Constraint Models
81
with an extension of the variable path concept ﬁrst introduced in [13], and the
idea to ﬁnd MUSes based on select groups of constraints from [1,16] in order to
speed up MUS detection and provide more meaningful diagnoses.
3.1
Constraint Paths
We introduced variable paths in [13] in order to match variables across diﬀer-
ent compilations of an instance, enabling the compiler to perform some whole-
program optimisation to produce simpler, more eﬃcient programs.
A variable path describes the path the compiler took from the model to
the point where a new variable is introduced during compilation. Paths contain
information on the location of each syntactic construct, as well as the bindings
of all loop variables that lead to the introduction of a variable.
For the purposes of this paper, we will extend the concept of variable paths to
also apply to constraints, in order to be able to identify the origins and high-level
structure of program-level constraints.
Listing 1.2 shows simpliﬁed paths for the ﬁrst 4 constraints in the com-
piled Latin Squares program (Listing 1.1), with a graphical representation of
the entire compilation in Fig. 1. The path for the ﬁrst int ne constraint encodes
that it came from a forall call on line 4 (4 : forall). Next we see the index
variable r was set to 1 (r=1). After this, the alldifferent call is replaced
with a call to all different int. At this point the compiler has decomposed
all different int into simpler constraints. It begins with a forall, for which
the index variables i and j are set to 1 and 2. Finally we see that the int ne
constraint was added for the binary disequality !=.
The amount of detail in the paths presented here has been reduced for illus-
trative purposes, actual MiniZinc paths retain information about what ﬁle a call
is in, the span of text the call covers, and the full name of each call.
int_ne(X[1],X[2]);




4:forall:r=1




all different int




forall:i=1 j=2




!=
int_ne(X[1],X[3]);




4:forall:r=1




all different int




forall:i=1 j=3




!=
int_ne(X[1],X[4]);




5:forall:c=1




all different int




forall:i=1 j=2




!=
int_ne(X[1],X[7]);




5:forall:c=1




all different int




forall:i=1 j=3




!=
Listing 1.2. Simpliﬁed MiniZinc paths with depth 2 preﬁxes marked in bold
Constraint paths make the model-level hierarchy explicit at the program
level. The root of this hierarchy (or depth 1) for our Latin Squares model would
have 2 abstract constraints with preﬁxes 4 : forall and 5 : forall, representing
the alldifferent constraints on rows or columns of X. The child constraints of
these (at depth 2) would be the n possible assignments for both r and c, giving
us the 2n individual alldifferent constraints. Jumping forward to a depth of
5 the preﬁxes ﬁnally distinguish between speciﬁc int ne constraints.
3.2
Grouping Constraints by Paths
The idea to perform diagnosis based on groups of constraints can be seen in [1,16]
where MUSes are found in a given set of grouped constraints. The hierarchical

82
K. Leo and G. Tack
Fig. 1. Constraint paths for the Latin Squares model
grouping of model constraints goes back to [11] where users deﬁne names for
groups of lower-level constraints so that more useful feedback can be presented
to users. We refer to these groupings as abstract constraints. Constraint paths
explicitly encode the hierarchy of constraints and give us access to model-level
structure in the solver-level programs. This enables us to group constraints
together automatically and intelligently when searching for MUSes. Instead of
relying on the user to deﬁne the abstract constraints, or the MUS detection work-
ing on the full set, we can automatically generate abstract constraints based on
the model structure, starting from the top-level constraints and iteratively reﬁn-
ing them down to individual solver-level constraints.
Take for example Listing 1.2. Grouping all constraints whose paths are equal
up to a depth of 1, a MUS enumeration algorithm will have to explore the power-
set of only two abstract constraints, a much smaller search space than that of
the full set of 18 program level constraints. Of course MUSes found at a depth
of 1 can only give a user a hint of what might be wrong with their model by
drawing their attention to the correct lines in their model. We may have to use
longer preﬁxes to ﬁnd more useful MUSes.
In Algorithm 1 the grouping is managed by a checker which is aware of
the constraints (C) and their depth in the hierarchy. The checker is initialised
by a call to create checker which takes arguments for the initial depth in the
hierarchy where the approach should start looking for MUSes (dmin) and also
the target depth of the procedure (dmax). The checker provides a procedure
called increase depth which takes a set of abstract constraints and splits these

Debugging Unsatisﬁable Constraint Models
83
constraints at the next depth (splitting them into a larger number of more spe-
ciﬁc constraints). This procedure returns false if the depth cannot be increased
any further (either due to dmax or simply reaching the program level constraints).
Algorithm 1 can be used with any solver-independent MUS detection algo-
rithm. We just assume functions detectMUSes.ﬁnished() that returns true when
the algorithm has fully enumerated the MUSes at the current depth and
detectMUSes.next() that returns the next MUS.
Algorithm 1. Procedure for reporting MUSes found at diﬀerent depths
1: procedure diagnose(C, dmin, dmax, maxMUSes, complete)
2:
checker ←create checker(C, dmin, dmax, complete)
3:
do MUSes ←∅
4:
while |MUSes| < maxMUSes and not detectMUSes(checker).ﬁnished( ) do
5:
MUSes ←MUSes ∪detectMUSes(checker).next( )
6:
checker.report(MUSes)
7:
while checker.increase depth(MUSes)
Selective Deepening. After detecting a set of MUSes for a certain grouping depth,
we may want to expand the depth to get more ﬁne-grained information. Of course
we could simply increase the grouping depth for all abstract constraints, which
will split each of them again according to the next part of the paths. However,
we do not need to increase the depth for abstract constraints that did not take
part in any MUS. This allows us to restrict the MUS detection at the next depth
to the abstract constraints that actually appear in MUSes at the current level.
This can speed up enumeration considerably since the abstract constraints that
are not involved in any MUS can be made up of very large sets of constraints.
In Algorithm 1 this is represented by the call to the checker.increase depth pro-
cedure on line 7 with the union of constraints occurring in MUSes.
Selective deepening allows for the discovery of MUSes at deep levels much
quicker than trying to ﬁnd MUSes directly at those depths. For example, if
running a MUS enumeration algorithm on a set of abstract constraints {a, b}
results in a MUS {a}, we need only increase the depth for a resulting in the new
set: {a1, a2, a3, b}. If b contains ﬁve lower level constraints this approach, without
aﬀecting completeness (since b is never removed), can lead to a signiﬁcant speed-
up when compared to searching in the set {a1, a2, a3, b1, b2, b3, b4, b5}.
Figure 2 shows several iterations of the approach when applied to a program
for the faulty Latin Squares model. The program has 58 constraints (the leaves
of the tree). In this ﬁgure, ovals represent abstract constraints that are to be
evaluated, with ﬁlled ovals representing a discovered MUS. Note that the frontier
only deepens under the abstract constraints that occurred partake in a MUS.
We see that the ﬁrst iteration can at most search through combinations of 4
abstract constraints. Upon ﬁnding a MUS, the depth is increased only for the
three responsible abstract constraints. At the next depth we see that the algo-
rithm must only search through combinations of 7 constraints. This continues
until, at the program level, the algorithm searches through just 36 constraints,
instead of the full set of 58 which traditional approaches would have to explore.

84
K. Leo and G. Tack
Fig. 2. Automatic deepening, from top: iterations 1, 2, 5 and 6
Incomplete enumeration.
A further optimisa-
tion that can improve scalability is to omit
the abstract constraints that did not occur in
any MUS at the current depth. However, this
removal may occasionally cut oﬀMUSes that can only be found at lower
depths. In the example shown here, it is clear that there are two MUSes:
{x<4, x>5} and {x>6, x<4}. A MUS detection algorithm searching at a depth
of 1, i.e., only looking at each constraint item as a whole, will ﬁnd the conﬂict
. However, this is not minimal as the set is still unsatisﬁable
when constraint x>6 is removed. Incomplete enumeration will then seek to ﬁnd
MUSes in the set {x<4, x>5} discarding constraint x>6 and thus cutting oﬀthe
MUS {x>6, x<4}. This approach sacriﬁces completeness for scalability allowing

Debugging Unsatisﬁable Constraint Models
85
users to quickly discover a subset of MUSes. In Algorithm 1, this behaviour is
enabled by setting complete to false.
4
Implementation and Evaluation
The approach was implemented by extending an existing implementation of an
enumeration algorithm called MARCO, developed and implemented by Liﬃton
and Malik [14,15], and evaluated on a set of MiniZinc benchmark models that
were augmented to make them unsatisﬁable.
4.1
Extending MARCO
Our proof of concept implementation consists of a FlatZinc satisfaction checker
that is aware of MiniZinc paths, and a new frontend to MARCO that controls
the grouping of FlatZinc constraints during the enumeration.
To enumerate MUSes the MARCO algorithm maintains a CNF formula
(called a map) that encodes the problem of selecting from the remaining un-
checked subsets of abstract constraints. The algorithm proceeds by ﬁnding a
solution to the map, which represents a subset of abstract constraints. If this
set is satisﬁable it is expanded by adding constraints that do not cause unsat-
isﬁability. The map is then updated to reﬂect that all subsets of this “grown”
subset are satisﬁable and should not be explored except in the presence of an
abstract constraint from outside this subset. If the subset was unsatisﬁable, a
“shrink” method is called which removes constraints that do not aﬀect the sub-
set’s unsatisﬁability. In this case the map is updated to mark all supersets as
explored. The algorithm continues as long as the map is satisﬁable, indicating
that unexplored subsets remain.
The frontend supports running MARCO on a set of abstract constraints of a
target depth, as well as starting at some depth and deepening after enumeration
of MUSes at that depth until a target depth is reached. Removing abstract
constraints that do not occur in a MUS at deeper enumerations (which makes
enumeration incomplete as discussed in the previous section) is also supported.
A further conﬁguration option for improving the performance of the tool is to
limit the number of MUSes to be discovered at each depth. This setting combined
with the setting for removing abstract constraints from the map allows the tool
to focus on discovering a speciﬁc fault as quickly as possible.
4.2
Experiments
To demonstrate the usefulness of the new approach two experiments were per-
formed. The ﬁrst explores the viability of the approach for ﬁnding sets of MUSes
at diﬀerent depths. The second experiment explores the amount of time it takes
to return a single MUS. Typically, MUS detection algorithms are evaluated on

86
K. Leo and G. Tack
sets of unsatisﬁable program-level instances2, not on parametric models. Cur-
rently there are no collections of models that contain bugs that make them
unsatisﬁable. Collections such as CSPLib [8] or the MiniZinc benchmarks3 con-
tain only ﬁnished, correct models for problems. We therefore introduced artiﬁcial
faults into models from the MiniZinc Challenge 2015 [22], similarly to how fault
injection was applied in [12]. The faults were selected to make the models unsat-
isﬁable in a non-trivial way (i.e., so that the compiler does not detect it during
compilation). Faults added include swapping arguments to global constraints,
changing operators (
to
), changing array index oﬀsets (
to
), using the wrong variables in constraints (
to
), removing negations (
to
), and changing
constants (
to
)). The instances used were selected at random.
Conﬁg dmin dmax complete
C2
2
2
true
C1→2
1
2
false
C3
3
3
true
C1→3
1
3
false
CMax
Max Max
true
C1→Max
1 Max
false
Fig. 3. Experiment setup
Six
diﬀerent
conﬁgurations
of
the
sys-
tem were evaluated, paired by their target
dmax. Each execution was given a timeout of
5 min (300 s). The columns show the duration
in seconds (T), the number of abstract con-
straint groups (G), and the number of conﬂicts
found (C). The conﬁgurations are presented
in Fig. 3. Conﬁgurations C2, C3, and CMax cor-
respond to a traditional approach to MUS enu-
meration at depths 2, 3, and at the depth of the
individual program-level constraints. Conﬁgura-
tions C1→2, C1→3 and C1→Max perform incomplete enumeration (setting com-
plete to false), enumerating MUSes ﬁrst at a depth of 1 and then increasing the
depth, discarding abstract constraints that are not involved in any MUS as it
proceeds.
MUS enumeration. Table 1 presents a comparison of the performance of the
diﬀerent conﬁgurations when the goal is to enumerate multiple MUSes.
Comparing conﬁgurations C2 and C1→2 we see that C1→2 is sometimes faster,
but in three of the eleven cases it cannot ﬁnd as many MUSes as the ﬁxed
depth C2 conﬁguration. For example, in the case of the “Free Pizza” model,
conﬁguration C2 returns 13 MUSes before timing out, while conﬁguration C1→2
can only yield a single MUS after discarding many of the other constraints. For
conﬁgurations C3 and C1→3, there are a few cases where conﬁguration C1→3
is faster. However, we also ﬁnd cases where just starting at a depth of 3 ﬁnds
more MUSes faster. Finally, in conﬁgurations CMax and C1→Max we see that the
deepening approach discovers MUSes at the level of individual program level
constraints much faster than the full enumeration approach on three occasions.
Again, we see an instance (MKnapsack) where increasing the depth does not
change the number of abstract constraints. Since the enumeration algorithm has
to essentially repeat enumeration at every depth it ends up being much slower.
2 For example, from the MaxSAT competition http://www.maxsat.udl.cat/.
3 https://github.com/MiniZinc/minizinc-benchmarks.

Debugging Unsatisﬁable Constraint Models
87
Table 1. Comparison of enumeration behaviour
Conﬁguration C2
C1→2
C3
C1→3
CMax
C1→Max
Model
T
G
C T
G
C T
G
C T
G C T
G
C T
G C
Costas array
300.0
56 88 300.0
54 11 300.0
83
0 300.0
0
0 300.0
83
0 300.0
0
0
CVRP
300.0
34
1
0.5
7
1 300.0
66
2
0.7
7
1 300.0
101
3
1.4 14
4
Free pizza
300.0
81 13
0.7
9
1 300.0
82 12
0.7
1
1 300.0
553
8
1.2
1
1
Mapping
0.4
24
1
0.3
2
1
28.9
69 70 10.9 28 66 300.0
254 70 300.0
0
0
MKnapsack
7.6
31 49
7.2
30 49
7.8
31 49
14.3 30 49
9.5
32 65
58.2 31 65
NMSeq
300.0
40
0 300.0
40
0 300.0
40
0 300.0
0
0 300.0 3240
0 300.0
0
0
Open stacks
300.0
802
5 300.0
800
5 300.0
841
4 300.0
0
0 300.0 4421
0 300.0
0
0
p1f
113.6
89 77 89.3
77 77 113.9
89 77 170.6 77 77 300.0
947 10 300.0
0
0
Radiation
0.5
5
1
0.5
4
1
12.0
68 12
4.1 25 12
73.1
388 12 20.4 12 12
Spot5
300.0 4998
0 300.0 4406
1 300.0 5227
0 300.0
0
0 300.0 5457
0 300.0
0
0
TDTSP
300.0
46
1 300.0
0
0 300.0
62
1 300.0
0
0 300.0
170
1 300.0
0
0
This indicates that the deepening approach may be less useful when most of the
constraints are in conﬂict.
Time to First MUS. In practice a user will often start trying to ﬁx the ﬁrst
few MUSes that are presented rather than waiting for a full enumeration. This
is similar to debugging in traditional programming languages, where one would
rarely try to ﬁx multiple reported errors at once but instead ﬁx one and then
check whether it was also the cause of the other errors. With this in mind the
second experiment, presented in Table 2, explores how quickly the conﬁgurations
can report the ﬁrst MUS found at a target depth.
Comparing conﬁgurations C2 and C1→2 in this case shows that the incom-
plete enumeration conﬁguration (C1→2) is almost always faster even with a dmax
of 2. For conﬁgurations C3 and C1→3 the eﬀect is even more pronounced with
C1→3 outperforming C3 in almost all cases. The impact of incomplete enumer-
ation can be seen in the results for the Free Pizza problem, where C2 must ﬁnd
MUSes given 82 abstract constraints at depth 2 whereas conﬁguration C1→2 has
narrowed the set of constraints to a single abstract constraint. Finally, in conﬁg-
urations CMax and C1→Max, we compare the traditional program-level approach
for ﬁnding a MUS against the incomplete approach. Here again we see the app-
roach outperforming CMax in most cases by honing in on a single MUS.
5
Displaying Diagnoses
The approach presented in Sect. 4 produces diagnoses as sets of MiniZinc paths.
While paths contain the information that is required for debugging, they are not
very easy to read. What a user may need to extract from a set of paths to help
interpret a diagnosis is just the set of syntactic positions that it relates to, and
the speciﬁc assignments to loop index variables during compilation that make

88
K. Leo and G. Tack
Table 2. First MUS found for diﬀerent depths
Conﬁguration C2
C1→2
C3
C1→3
CMax
C1→Max
Model
T
G
C T
G
C T
G
C T
G C T
G
C T
G C
Costas array
2.5
56 1
3.9
54 1 300.0
83 0
4.7 33 1 300.0
83 0
6.0 33 1
CVRP
0.5
34 1
0.3
7 1
0.9
66 1
0.4
7 1
1.3
101 1
0.8 14 1
Free pizza
1.6
81 1
0.6
9 1
1.7
82 1
0.7
1 1
9.4
553 1
0.9
1 1
Mapping
0.4
24 1
0.3
2 1
0.6
69 1
0.4 28 1
1.6
254 1
0.6
4 1
MKnapsack
0.5
31 1
0.5
30 1
0.5
31 1
0.5
1 1
0.6
32 1
0.7
1 1
NMSeq
300.0
40 0
300.0
40 0 300.0
40 0
300.0
0 0 300.0 3240 0
300.0
0 0
Open stacks
53.9
802 1
52.1
800 1
73.7
841 1
55.4
7 1 300.0 4421 0
73.4 14 1
p1f
3.9
89 1
1.9
11 1
4.2
89 1
2.1
1 1
29.6
947 1
2.4
1 1
Radiation
0.5
5 1
0.5
4 1
1.4
68 1
0.9 25 1
6.5
388 1
1.2
1 1
Spot5
300.0 4998 0 258.9 4406 1 300.0 5227 0 264.2
1 1 300.0 5457 0 264.2
1 1
TDTSP
1.2
46 1
0.5
1 1
1.4
62 1
0.4
1 1
3.0
170 1
0.7
1 1
up the diagnosis. To make things easier for the user, we can present diagnoses in
a more useful form. We have extended the MiniZinc IDE to be able to interpret
and display MiniZinc paths directly in the source code editor.
Case Study: Over-constrained Latin Squares
In Fig. 4 we see the Latin Squares model from Sect. 2 in the MiniZinc IDE with
some conﬂicting symmetry breaking constraints added. Enumerating MUSes for
this model instantiated with n = 3 produces 12 diagnoses. In Fig. 4 we show
how a selection of these are presented to the user. Each MUS is displayed in the
output section, with the number of abstract constraints involved and a list of
parameter values that lead to this diagnosis.
Looking at the highlighting of the model presented in Fig. 4a we see that
the selected diagnosis involves some combination of the row alldifferent con-
straints, the row lex less constraints, and the column lex greater constraints.
All of the MUSes for this problem involve some combination of the alldifferent
constraints and the lex constraints. Looking at the intersection of abstract con-
straints involved in MUSes it is easy to ﬁnd that at least 3 lex constraints are
involved in every MUS and must be the source of the bug.
If the user cannot immediately deduce what may be causing the issue they
can look at what speciﬁc rows and columns are involved by examining the list
of assignments to parameters. From the list in Fig. 4a we can see that the bug
involves the ﬁrst two iterations of the loop which introduce lex greater con-
straints for the ﬁrst three columns (
), the alldifferent constraints
on the ﬁrst two rows (
) and the lex less over the ﬁrst two rows
.
If the enumeration algorithm is conﬁgured to ﬁnd MUSes at greater depths,
the diagnoses will involve constraints introduced by decompositions of the model-
level constraints. This can be seen in Fig. 4b where a user has clicked on one such

Debugging Unsatisﬁable Constraint Models
89
(a) MUS with ﬁrst alldifferent
(b) MUS at library level
Fig. 4. Prototype MiniZinc IDE UI showing diﬀerent diagnoses.
diagnosis. This opens tabs for displaying the MiniZinc standard decompositions
of the lex less int and all different int constraints. The highlighting shows
a speciﬁc less-than-or-equal constraint that is unsatisﬁable. Tracking down con-
straints across multiple MiniZinc ﬁles is made much easier by this feature.
6
Generalising to the Model Level
MiniZinc paths were introduced to identify common structure across multiple
compilations of the same instance. With a slight modiﬁcation they can provide
insights into common structures found across instances, the intersection of which
can be considered to be instance-independent.
In some cases multiple instances of a problem will have MiniZinc paths in
common. For example, the paths for the ﬁrst few iterations of a forall loop will
often be the same. For example in the Latin Square model presented in Sect. 2 all
valid values for the parameter n result in the ﬁrst forall always being evaluated
for
. Diﬀerent instances will have similar paths for these constraints. Since
the instances are diﬀerent the constraints are not the same and so we cannot
perform any automatic reasoning on these. However, we will now show how
grouping the constraints can still provide useful insights.
6.1
Cross-Instance MUSes
The user may have a set of instances of their problem, some of which should be
satisﬁable, while others may be unsatisﬁable (even assuming a correct model).
If the user does not know which instances are unsatisﬁable to begin with, this

90
K. Leo and G. Tack
can make the process of developing a model quite diﬃcult as the user will have
to deduce whether unsatisﬁability is due to a bug in the model or the concrete
instance data. Using the techniques in this paper we can quickly discover the
conﬂicts arising from a set of instances and compare them, to give the user a
better idea of what may be happening in their model.
When examining a set of MUSes for several instances we often do not care
what speciﬁc iteration of a loop is buggy but whether at least one iteration
is buggy in every instance. To make it easier to analyse MUSes from multiple
instances, we can therefore generalise the paths to varying degrees. The easiest
option for generalising the paths is to simply remove the identifying information
that makes a path unique for a single FlatZinc constraint. This way, all iterations
of a loop get grouped together when grouping by path. These generalised paths
can still be grouped diﬀerently depending on depth though.
Just as we used the intersection of MUSes in a single instance to ﬁnd what bug
is common to all MUSes in Sect. 4, we can ﬁnd intersections of these generalised
paths to group instances by their MUSes, and ﬁnd MUSes that are common to
all instances, indicating a modelling bug.
Given a model and a set of instances (some of which are unsatisﬁable), we
can enumerate MUSes for each instance. The MUSes can then be presented to
the user, ranked by the number of instances that they occur in. Looking at this
ranked list, a user can discern whether a MUS occurs in all or most instances,
which indicates that it may be a bug in the model. Similarly the user can also
see the types of unsatisﬁability triggered by diﬀerent instances, allowing them
to classify their instances into diﬀerent groups.
6.2
Case Study: Cyclic-RCPSP
To demonstrate how this approach would work in practice we will look at a rel-
atively complex model along with a set of satisﬁable and unsatisﬁable instances.
The model we selected was the Cyclic Resource-Constrained Project Schedul-
ing Problem [2]. This problem is a project scheduling problem where tasks are
repeated inﬁnitely. The objective is to ﬁnd a cyclic schedule that ﬁrst minimises
the period of the schedule and then minimises the makespan.
We introduced a bug to the model by swapping arguments of the cumulative
constraints that represent start times and durations of tasks. Since both are
arrays of integer variables, the compiler does not detect this mistake.
The unsatisﬁable instances fall into two groups: two instances that have
unsatisﬁable resource capacities and two in which the precedences of some tasks
are cyclical (task a depends on task b which in turn depends on task a). In
addition to these unsatisﬁable instances there are three satisﬁable instances.
The FlatZinc programs for these instances can be quite large, making full
enumeration of MUSes a time consuming task even for a single instance. We
therefore use our selective deepening approach and instruct it to ﬁnish at the
relatively shallow depth of 8. Since we are interested in comparing diagnoses
across instances, we generalise the paths by removing all speciﬁc assignments
from them. This allows us to group larger sets of program constraints together

Debugging Unsatisﬁable Constraint Models
91
into even fewer abstract constraints, to avoid searching at an unnecessary level of
detail. In this model, this grouping means that the MUS enumeration algorithm
only needs to look at combinations of twelve abstract constraints regardless of
instance data. The instance data will only change which abstract constraints
fail. MUSes for each instance can be discovered very quickly with these settings
and can then be reported in terms of the model.
For these instances the algorithm discovers ﬁve distinct MUSes. These MUSes
occur in instances {0, 1, 2, 5, 6}, {3, 4}, {1, 5}, {2, 6} and {2}. The ﬁrst MUS
occurs in 5 of the 7 instances and as such is a strong candidate for being a model-
level bug. Indeed, this MUS involves the incorrect arguments to cumulative. The
instance that does not include this exact MUS also relates to cumulative but
it fails in a slightly diﬀerent way leading to a diﬀerent MUS. Once the bug has
been ﬁxed the user can run the analysis again which will show that there are
only two MUSes remaining. These remaining MUSes occur in instances {3, 4}
and {2, 6} which correspond to the instances with the two classes of faults.
7
Related Work
MUS enumeration at the program level has been researched extensively. Some
approaches focus on speciﬁc constraint systems, taking advantage of e.g. proper-
ties of linear systems in MIPs [9,17], or structure inherent to numerical CSPs [7].
Several algorithms have been proposed for constraint agnostic MUS enumer-
ation [15]. QuickXplain [10] attempts to discover MUSes using a divide and
conquer approach. Later approaches such as DAA [4] have more powerful tech-
niques for pruning the search space. DAA’s main drawback was that it has to
enumerate very large hitting sets and as a result had a high memory and time
cost. The MARCO algorithm [14] and the more recent MCS-MUS-BT [3] provide
much more eﬃcient approaches for ﬁnding MUSes (see Sect. 4).
At the modelling system level there has been some eﬀort to provide more
meaningful explanations of unsatisﬁability. In [11] users can explicitly group their
constraints, giving a user friendly name to each sub-group. When a constraint
system is found to be unsatisﬁable these names are used to provide feedback.
CPTEST [12] is a modelling system level framework that can aide a user in
correcting several types of faults in iterations of an initially correct model. Many
of these approaches could be combined with, and beneﬁt from, the hierarchical
structure that can be extracted from high-level models.
8
Conclusion
This paper presented a novel approach that can ﬁnd Minimal Unsatisﬁable Sub-
sets (MUSes) faster and present them to the user in a way that allows them
to quickly identify the source of the unsatisﬁability. Our approach is based on
automatically grouping related constraints together during MUS enumeration,
which reduces the search space and can speed up the discovery of diagnoses.

92
K. Leo and G. Tack
MiniZinc paths enable both this automatic grouping as well as the presentation
of the extracted MUSes in terms of the high-level model the user wrote.
We also presented a methodology for deducing, given a set of satisﬁable and
unsatisﬁable instances, whether the model has a bug. Further we can help a user
group unsatisﬁable instances by the types of unsatisﬁability that they introduce.
Future work. The approaches explored here will be better integrated into the
MiniZinc compiler and IDE, providing a more consistent interface for users.
Additionally, user studies focussing on the debugging of constraint models could
provide valuable insights into how these tools can be further improved. Finally,
integrating solvers based on Lazy Clause Generation [5,20] more tightly with
the MARCO algorithm seems a promising direction for further speed ups.
References
1. Andraus, Z.S., Liﬃton, M.H., Sakallah, K.A.: Reﬁnement strategies for veriﬁcation
methods based on datapath abstraction. In: Asia and South Paciﬁc Conference on
Design Automation, January 2006
2. Ayala, M., Benabid, A., Artigues, C., Hanen, C.: The resource-constrained modulo
scheduling problem: an experimental study. Comput. Optim. Appl. 54(3), 645–673
(2013)
3. Bacchus, F., Katsirelos, G.: Finding a collection of MUSes incrementally. In: Quim-
per, C.-G. (ed.) CPAIOR 2016. LNCS, vol. 9676, pp. 35–44. Springer, Cham (2016).
doi:10.1007/978-3-319-33954-2 3
4. Bailey, J., Stuckey, P.J.: Discovery of minimal unsatisﬁable subsets of constraints
using hitting set dualization. In: Hermenegildo, M.V., Cabeza, D. (eds.) PADL
2005. LNCS, vol. 3350, pp. 174–186. Springer, Heidelberg (2005). doi:10.1007/
978-3-540-30557-6 14
5. Feydy, T., Stuckey, P.J.: Lazy clause generation reengineered. In: Gent, I.P. (ed.)
CP 2009. LNCS, vol. 5732, pp. 352–366. Springer, Heidelberg (2009). doi:10.1007/
978-3-642-04244-7 29
6. Frisch, A.M., Grum, M., Jeﬀerson, C., Martnez, B., Miguel, H.I.: The design of
ESSENCE: a constraint language for specifying combinatorial problems. In: IJCAI
2007, pp. 80–87 (2007)
7. Gasca, R.M., Valle, C., G´omez-L´opez, M.T., Ceballos, R.: NMUS: structural analy-
sis for improving the derivation of all MUSes in overconstrained numeric CSPs. In:
Borrajo, D., Castillo, L., Corchado, J.M. (eds.) CAEPIA 2007. LNCS (LNAI), vol.
4788, pp. 160–169. Springer, Heidelberg (2007). doi:10.1007/978-3-540-75271-4 17
8. Gent, I.P., Walsh, T.: CSPlib: a benchmark library for constraints. In: Jaﬀar, J.
(ed.) CP 1999. LNCS, vol. 1713, pp. 480–481. Springer, Heidelberg (1999). doi:10.
1007/978-3-540-48085-3 36
9. Gleeson, J., Ryan, J.: Identifying minimally infeasible subsystems of inequalities.
INFORMS J. Comput. 2(1), 61–63 (1990)
10. Junker, U.: QuickXplain: Conﬂict detection for arbitrary constraint propagation
algorithms. In: IJCAI01 Workshop on Modelling and Solving problems with con-
straints (2001)
11. Jussien, N., Ouis, S.: User-friendly explanations for constraint programming. In:
Kusalik, A.J. (ed.) Proceedings of the Eleventh Workshop on Logic Programming
Environments (WLPE 2001), Paphos, Cyprus, 1 December 2001 (2001)

Debugging Unsatisﬁable Constraint Models
93
12. Lazaar, N., Gotlieb, A., Lebbah, Y.: A CP framework for testing CP. Constraints
17(2), 123–147 (2012)
13. Leo, K., Tack, G.: Multi-pass high-level presolving. In: Yang, Q., Wooldridge, M.
(eds.) Proceedings of the Twenty-Fourth International Joint Conference on Artiﬁ-
cial Intelligence, IJCAI 2015, Buenos Aires, Argentina, 25–31 July 2015, pp. 346–
352. AAAI Press (2015)
14. Liﬃton, M.H., Malik, A.: Enumerating infeasibility: ﬁnding multiple MUSes
quickly. In: Gomes, C., Sellmann, M. (eds.) CPAIOR 2013. LNCS, vol. 7874, pp.
160–175. Springer, Heidelberg (2013). doi:10.1007/978-3-642-38171-3 11
15. Liﬃton, M.H., Previti, A., Malik, A., Marques-Silva, J.: Fast, ﬂexible MUS enu-
meration. Constraints 21(2), 223–250 (2015)
16. Liﬃton, M.H., Sakallah, K.A.: Algorithms for computing minimal unsatisﬁable
subsets of constraints. J. Autom. Reasoning 40(1), 1–33 (2008)
17. van Loon, J.: Irreducibly inconsistent systems of linear inequalities. Eur. J. Oper.
Res. 8(3), 283–288 (1981)
18. Nethercote, N., Stuckey, P.J., Becket, R., Brand, S., Duck, G.J., Tack, G.:
MiniZinc: towards a standard CP modelling language. In: Bessi`ere, C. (ed.) CP
2007. LNCS, vol. 4741, pp. 529–543. Springer, Heidelberg (2007). doi:10.1007/
978-3-540-74970-7 38
19. O’Callaghan, B., O’Sullivan, B., Freuder, E.C.: Generating corrective explanations
for interactive constraint satisfaction. In: Beek, P. (ed.) CP 2005. LNCS, vol. 3709,
pp. 445–459. Springer, Heidelberg (2005). doi:10.1007/11564751 34
20. Ohrimenko, O., Stuckey, P.J., Codish, M.: Propagation = lazy clause generation.
In: Bessi`ere, C. (ed.) CP 2007. LNCS, vol. 4741, pp. 544–558. Springer, Heidelberg
(2007). doi:10.1007/978-3-540-74970-7 39
21. Ouis, S., Jussien, N., Boizumault, P.: k-relevant explanations for constraint pro-
gramming. In: Russell, I., Haller, S.M. (eds.) Proceedings of the Sixteenth Interna-
tional Florida Artiﬁcial Intelligence Research Society Conference, 12–14 May 2003,
St. Augustine, Florida, USA, pp. 192–196. AAAI Press (2003)
22. Stuckey, P., Becket, R., Fischer, J.: Philosophy of the MiniZinc challenge. Con-
straints 15(3), 307–316 (2010)

Learning Decision Trees with Flexible
Constraints and Objectives Using Integer
Optimization
Sicco Verwer1 and Yingqian Zhang2(B)
1 Delft University of Technology, Delft, The Netherlands
s.e.verwer@tudelft.nl
2 Eindhoven University of Technology, Eindhoven, The Netherlands
yqzhang@tue.nl
Abstract. We encode the problem of learning the optimal decision tree
of a given depth as an integer optimization problem. We show experi-
mentally that our method (DTIP) can be used to learn good trees up to
depth 5 from data sets of size up to 1000. In addition to being eﬃcient,
our new formulation allows for a lot of ﬂexibility. Experiments show that
we can use the trees learned from any existing decision tree algorithms as
starting solutions and improve the trees using DTIP. Moreover, the pro-
posed formulation allows us to easily create decision trees with diﬀerent
optimization objectives instead of accuracy and error, and constraints
can be added explicitly during the tree construction phase. We show
how this ﬂexibility can be used to learn discrimination-aware classiﬁca-
tion trees, to improve learning from imbalanced data, and to learn trees
that minimise false positive/negative errors.
1
Introduction
Decision trees [3] have gained increasing popularity these years due to their
eﬀectiveness in solving classiﬁcation and regression problems. As the problem
of learning optimal decision trees is a NP-complete problem [11], greedy based
heuristics such as CART [3] and ID3 [15] are widely used to construct sub-
optimal trees. Greedy decision tree algorithm builds a tree recursively starting
from a single node. At each decision node, an optimization problem is solved
to determine the locally best split decision based on a subset of the training
data such that the training data is further split into two subsets. Decisions are
determined in turn for each of these subsets on children nodes of the starting
node. The advantage of such a greedy approach is its computational eﬃciency.
The limitation is that the constructed trees may be far from optimal.
In this paper, we aim to build optimal decision trees by optimizing all deci-
sions concurrently. We formulate the problem of constructing the optimal deci-
sion tree of a given depth as an integer linear program. We call our method
DTIP. One beneﬁt of this formulation is that we can take advantage of the
powerful mixed-integer linear programming (i.e., MIP) solver to ﬁnd good trees.
c
⃝Springer International Publishing AG 2017
D. Salvagnin and M. Lombardi (Eds.): CPAIOR 2017, LNCS 10335, pp. 94–103, 2017.
DOI: 10.1007/978-3-319-59776-8 8

Learning Decision Trees with Flexible Constraints and Objectives
95
Researcher have previously investigated using solvers for learning diﬀerent kinds
of models and rules, see e.g., [4,6,8,10]. We are not the ﬁrst who try such an app-
roach for decision tree learning. Bennett and Blue [1] proposed a formulation to
solve the problem of constructing binary classiﬁcation trees with ﬁxed structure
and labels, where paths of the tree are encoded as disjunctive linear inequalities,
and non-linear objective functions are introduced to minimize errors. Norouzi
et al. [14] linked the decision tree optimization problem with the problem of
structured prediction with latent variables. To the best of our knowledge, our
method is the ﬁrst that encodes decision tree learning entirely in an integer pro-
gram. A similar approach for more general models is given in [2]. This method,
however, is quadratic instead of linear in the data set size and therefore requires
a lot of preprocessing in order to reduce the number of generated constraints. [4]
discusses modeling the problem of ﬁnding the smallest size decision tree that
perfectly separates the training data as a constraint program.
Section 2 shows the proposed encoding of learning optimal decision trees as
an integer optimization problem only requires O(2dn) constraints for regression
and O(nu+nv) constraints for classiﬁcation, where n is the size of the dataset, v
is the number of leafs, and u is the number of tree nodes. In addition, it requires
O(mu + nk + vy) variables, where k is the tree depth and y is the unique target
values in the dataset. This makes the encoding linear in the dataset size for ﬁxed
size trees. Moreover, the number of binary variables depends on the dataset size
up to a small constant factor (the tree depth). The formulated problem can be
directly solved by any MIP solver such as CPLEX. In Sect. 3, we show experi-
mentally that our method can be used to learn good trees up to depth 5 from
datasets of size up to 1000. In addition to being eﬃcient, our new formulation
allows for a lot of ﬂexibility. Our formulation enables that the trees obtained
from existing greedy algorithms from Scikit-learn can be used as starting solu-
tions for the CPLEX optimizer. Experiments with several real datasets show
that our method improves the starting solutions. Moreover, the proposed for-
mulation allows us to create decision trees with diﬀerent optimization objectives
other than standard objectives such as accuracy and error, and constraints can
be added explicitly when learning trees. We show how this ﬂexibility can be
used to learn discrimination-aware trees and to improve learning from imbal-
anced data. Starting from a solution given by Scikit-learn, our method can ﬁnd
trees of good performance that are discrimination-free, and trees that return
zero false-positives on training data.
2
Learning Decision Trees as Integer Programs (DTIP)
We assume the reader to be familiar with decision trees. We refer to [9] for
more information. The optimization problem that we aim to solve is to ﬁnd an
optimal classiﬁcation/regression tree of depth exactly k for a given dataset of
n rows (samples) and m features. The Boolean decisions and predictions are
variables and need to be set such that accuracy, absolute error, or any other
linear measures, is optimized. We solve this problem by translating/encoding it
entirely into linear constraints and providing this to an oﬀ-the-shelf MIP solver.

96
S. Verwer and Y. Zhang
Table 1. Feature values from ﬁrst few rows from the Iris data before (left) and after
(right) our data transform. The feature values are ﬁrst sorted, and then identical values
are mapped to integers in increasing order. Several feature values (such as SepLen {4.9,
4.7, 4.6}, PetLen, PetWid) are combined into a single integer because these only occur
for Iris-setosa ﬂowers. A SepWid value of 3.0 occurs most frequently in the data and
is mapped to 0. The target values are not transformed.
SepLen SepWid PetLen PetWid SepLen SepWid PetLen PetWid
5.1
3.5
1.4
0.2
−9.0
5.0
−3.0
−3.0
4.9
3.0
1.4
0.2
−11.0
0.0
−3.0
−3.0
4.7
3.2
1.3
0.2
−11.0
2.0
−3.0
−3.0
4.6
3.1
1.5
0.2
−11.0
1.0
−3.0
−3.0
5.0
3.6
1.4
0.2
−10.0
6.0
−3.0
−3.0
It has been shown in our previous work (e.g., [16]) that it is beneﬁcial to
keep encoding from machine learning models to linear constraints as small as
possible, thereby increasing the data size it can handle. In contrast to earlier
decision tree encodings (e.g. [1,4,16]), we therefore encode the leaf every data
row ends up in using a binary instead of a unary encoding, i.e., using k variables
instead of 2k. We start our encoding with a transformation of the input data.
2.1
Data Transformation
Earlier encodings of decision trees [1] or similar classiﬁcation/regression mod-
els [2] linearly scale the input data to the interval [0.0, 1.0]. There are good
reasons for doing so. For instance, this avoids large values in so-called big-M
formulations (a way to encode binary decisions in integer programming), which
can lead to numerical issues and long run-times. In spite of the beneﬁts of a
linear scaling, we advocate the use of a non-linear transform that assigns every
unique value of every feature to a unique integer, only maintaining the ordering
of these values. Using this transform:
– The thresholds in decision nodes can be represented by integer values instead
of continuous ones. This allows MIP solvers to branch on these values instead
of whether a certain row takes a left or right branch, reducing and balancing
the search tree used by these algorithms.
– When all rows having successive integers as feature values also have the same
class label, these values can be merged into a single integer.
– The most frequently occurring feature value can be mapped to the value 0,
reducing the number of non-zero coeﬃcients in the linear constraints.
– The ranges of feature values can all be centered around 0, see Table 1 for an
example. This reduces the size of M values used in the big-M formulations.
These beneﬁts all aﬀect the MIP solvers capacity to solve problems eﬃciently,
and therefore they are important considerations when encoding decision tree

Learning Decision Trees with Flexible Constraints and Objectives
97
Table 2. Summary of notation, constants, and variables used in the encoding.
Symbol
Type
Deﬁnition
n
Constant
Number of rows in data ﬁle
u
Constant
Number of nodes in tree, excluding leaf nodes
m
Constant
Number of features in data
v
Constant
Number of leaves in tree
k
Constant
Number of depths in tree
y
Constant
Number of unique target values in data
d(j)
Constant
Depth of node j of tree; the root node has depth 0
v(r, i)
Constant
Feature value for data row r and feature i
t(r)
Constant
Target value of data row r
LF, UF
Constant
Minimum, maximum feature value over all features
fi,j
Binary
Decision variable, feature i is used in decision rule of node j
cj
Integer
Decision variable, threshold of decision rule of node j
dh,r
Binary
Decision variable, path of data row r goes right/left at depth h
pl,t
Binary
Classiﬁer prediction of leaf l and target t
pl
Continuous
Regressor prediction of leaf l
er
Continuous
Prediction error for data row r
learning problems. Although a simple linear scaling can provide better results
for some problem instances, we have experienced signiﬁcant improvements in
the obtained solutions using the non-linear transform. In our experiments, we
demonstrate that our encoding is capable of producing good results when there
are 1000 rows in the input data, which is signiﬁcantly greater than previous
works on integer programming encodings for decision trees and would not have
been possible without this data transform.
2.2
Encoding Classiﬁcation Trees
Our encoding for classiﬁcation and regression trees is partly based on earlier
work where we translated already learned models into linear constraints in order
to deal with optimization under uncertainty [16]. Two key diﬀerences between
this work and our new encoding are: (1) the coeﬃcients denoting which constant
threshold and feature to use in a node’s binary decision are free variables, and
(2) the leaf that a data row ends up in is represented in binary instead of unary.
Table 2 summarizes the notation and variables that we use to encode trees. The
objective function minimizes the total prediction error for all data rows:
min

1≤r≤n
er,
for all r ∈[1, n]
(1)
where n is the number of input rows and er ∈R is the error for data row r,
deﬁned in Eq. 5. Every node j, excluding leaf nodes, in the tree needs a binary

98
S. Verwer and Y. Zhang
decision variable fi,j ∈{0, 1} to specify whether feature i ∈[1, m] is used in the
decision rule on node j:

1≤i≤m
fi,j = 1 for all j ∈[1, u]
(2)
Every node j requires an integer decision variable cj ∈[LF, UF] that repre-
sents the threshold, where LF = min{v(r, i)|i ∈[1, m], r ∈[1, n]}, UF =
max{v(r, i)|i ∈[1, m], r ∈[1, n]}, and v(r, i) denoting feature value for row r and
feature i. For each row r, we encode whether it takes the left or right branch of a
node using a variable dh,r ∈{0, 1} for every depth h, for all j ∈[1, u], r ∈[1, n] :

1≤h≤d(j)
Mrdlr(h, j, r) + Mrdd(j),r +

1≤i≤m
v(r, i)fi,j ≤Mrd(j) + cj

1≤h≤d(j)
M ′
rdlr(h, j, r) −M ′
rdd(j),r −

1≤i≤m
v(r, i)fi,j ≤M ′
r(d(j) −1) −cj (3)
where Mr = max{(v(r, i) −LF)|i ∈[1, m]} and M ′
r = max{(UF −v(r, i))|i ∈
[1, m]} are tight big-M values, d(j) is the depth of node j, and dlr(h, j, r) returns
the path directions from the root node required to reach node j:
dlr(h, j, r) =

dh,r
if the path to node j goes left at depth h
1 −dh,r
if the path to node j goes right at depth h
(4)
The formulation is essentially a big-M formulation for the constraint that if
row r takes the left (right) branch at depth d(j) of node j, denoted by dd(j),r,
and row r takes the path to node j (i.e., 
1≤h<d(j)(dlr(h, j, r)) = d(j) −1),
then the feature value v(r, i) for which fi,j is true has to be smaller (greater)
than threshold cj. This encodes all possible paths through the tree for all rows
using only O(nu) constraints and O(nk) binary variables, where k is the depth
of the tree. What remains is the computation of the classiﬁcation error:

1≤t≤y
pl,t = 1
for all l ∈[1, v]

1≤h≤k
dlr’(h, l, r) +

t̸=t(r)
pl,t ≤er + k
for all l ∈[1, v], r ∈[1, n]
(5)
where pl,t ∈{0, 1} is the prediction on leaf l, y is the number of unique target val-
ues (2 for binary classiﬁcation), v is the number of leafs in the tree, dlr’(h, l, r)
is the same as dlr(h, j, r) but for leafs l instead of internal nodes j, and t(r) is
the target value for row r. These constraints force that if a row ends in a leaf l
(setting dlr’(h, l, r) to 1 along the path to l), then the error er for row r is 1
if the leaf prediction type (for which pl,t is 1) is diﬀerent from the rows target
t(r). This adds O(vn) constraints and O(vy) to the encoding. This fully encodes
classiﬁcation tree learning in O(n(u + v)) constraints, O(mu + nk + vy) binary
variables, and u integers. A nice property is that the number of variables grows
with a very small constant factor (the depth of the tree k) in the dataset size n.

Learning Decision Trees with Flexible Constraints and Objectives
99
Fig. 1. The encoding of an example row r, with features values (5, 8, 10) and target
value 2 out of three possible targets {1, 2, 3}. We show the path taken by an assignment
of (d1,r, d2,r, d3,r) = (0, 1, 1), i.e., the row ﬁrst takes the right branch to node 3, and
then two left branches to node 6 and leaf 5. Observe that the big-M formulation forces
many of the path constraints to be satisﬁed, e.g., 5f1,1+8f2,1+10f3,1+d1,rMr ≤c1+Mr
reduces to 5f1,1 + 8f2,1 + 10f3,1 ≤c1 + Mr, which is always true. The same holds for
all constraints in subtrees rooted under a ⊤symbol. The constraint −5f1,1 −8f2,1 −
10f3,1 −d1,rM ′
r ≤−c1 reduces to −5f1,1 −8f2,1 −10f3,1 ≤−c1, which is true only if
the feature value of the feature type of node 1 (fi,1) is greater than the constraint value
for node 1 (c1). The same reasoning holds for the constraints on other depths. This
thus encodes the node constraints of a decision tree. The depth values (d1,r, d2,r, d3,r)
have a similar eﬀect on the leaf constraints. Only leaf 5 reached by r forces the error
er of row r to be 1 when the prediction type (pt,5) of leaf 5 is unequal to 2 (the target
type of r). All other leaf constraints are true for any error ≥0.
We strengthen the above encoding by bounding the node thresholds between
the minimum and maximum values of the features used in the binary decisions:

1≤i≤m
LF · fi,j ≤cj ≤

1≤i≤m
UF · fi,j
for all j ∈[1, u]
In addition, it does not make sense for the two leaf nodes l and l′ of the same
parent node to have the same values. The following breaks this symmetry:
pl,t + pl′,t = 1
for all t ∈[1, y] and all such pairs (l, l′).
Figure 1 shows the encoding for an example row.
2.3
Encoding Regression Trees
Our regression tree formulation is identical to the classiﬁcation tree formulation.
We only replace the error computation in Eq. 5 with the following constraints,
for all l ∈[1, v], r ∈[1, n] :

100
S. Verwer and Y. Zhang

1≤h≤k
Mtdlr’(h, l, r) + pl −t(r) ≤er + Mtk

1≤h≤k
M ′
tdlr’(h, l, r) + t(r) −pl ≤er + M ′
tk
(6)
where pl ∈[LT, UT] is the prediction value of leaf l, LT = min{t(r)|r ∈[1, n]},
UT = max{t(r)|r ∈[1, n]}, Mt = max{UT −t(r)|r ∈[1, n]} and M ′
t =
max{t(r) −LT|r ∈[1, n]}. This computes the absolute error for each row r
from the prediction value pl of the leaf it ends up in, depending on the path
variables from dlr’, using O(2vn) constraints.
3
Experiments
We conducted experiments on several benchmark datasets for both classiﬁca-
tion and regression tasks from the UCI machine learning repository [12]. We
compared the performance of the following three methods: (1) the classiﬁcation
and regression method from sciki-learn (i.e., optimized version of CART), (2)
the proposed decision tree as linear programs (DTIP) method that is solved
by CPLEX, and (3) DTIP solved by CPLEX with starting trees learned from
CART (DTIPs). The time limit for solving each problem is set to 30 min. We
learn decision trees of various depths, ranging from 1 to 5.
Classiﬁcation. We tested our method on three real datasets. The “Iris” data
have 4 attributes and 150 data points with 3 classes. The “Diabetes” data are
from the Pima Indian Diabetes database, which have 8 attributes and 768 data
points to two classes. The “Bank” data are from direct marketing campaigns of
a Portuguese banking institution [13]. The “Bank” dataset is considerable larger
than Iris and Diabetes, with 51 attributes and 4521 instances. As the purpose
of this paper is to demonstrate the performance of DTIP, we use all data points
for constructing the trees and use the classiﬁcation accuracy of the method on
all data points as the performance measurement. Table 3 reports the results.
With Iris data, CART is able to ﬁnd the optimal trees for depths 1, 2 and
5. Our proposed methods (DTIP and DTIPs) can always ﬁnd the optimal trees
with depths 1 to 5, no matter whether it starts with initial trees returned from
CART or not. For Diabetes, DTIP and DTIPs can construct the optimal trees
with depth 1. When the trees are larger, the encoded MIP models become more
and more diﬃcult to solve. This can be seen for depths 4 and 5, the performances
of DTIP are worse than CART when CPLEX tries to solve from scratch within
the limited running time (i.e., 30 min). This diﬃculty of CPLEX in solving large
instances becomes very obvious when DTIP builds the classiﬁcation trees of
depth 4 for the Bank data. The accuracy drops below 0.2, it is essentially still
preprocessing the data. However, when CPLEX starts with initial solutions,
DTIPs always improves the initial trees that are found by CART, resulting
higher or equal accuracies on all datasets and all diﬀerent sized trees.

Learning Decision Trees with Flexible Constraints and Objectives
101
Table 3. Classiﬁcation accuracy (top) and absolute error (bottom) of three regression
methods with trees of depths 1–5. The values with * indicate the optimal solutions.
d = 1
d = 2
d = 3
d = 4
d = 5
Iris
CART 0.6667* 0.96*
0.9733
0.9933 1*
DTIP
0.6667* 0.96*
0.9933* 1*
1*
DTIPs 0.6667* 0.96*
0.9933* 1*
1*
Diabetes
CART 0.7357
0.7721
0.776
0.7930 0.8372
DTIP
0.75*
0.7773
0.7969
0.7852 0.7852
DTIPs 0.75*
0.7773
0.7943
0.8255 0.8503
Bank
CART 0.8848
0.9009* 0.9044
0.9124 0.9206
DTIP
0.8929* 0.8956
0.8213
0.1152 0.1152
DTIPs 0.8929* 0.9009
0.9056
0.9129 0.9208
RedWine CART 780*
745
718
687
661
DTIP
780*
747
749
745
996
DTIPs 780*
745
715
686
661
Boston
CART 2518.1* 1755.6
1419.6
1230.7 1012
DTIP
2518.1* 1783.4
1410.2
1250.6 1200.6
DTIPs 2518.1* 1755.6
1413.6
1205
954
Regression. We used two real datasets. The ﬁrst one “RedWine” is from the wine
quality dataset [7]. It contains 1599 data points, each with 11 input variables,
and 1 output variable indicating the wine quality with scores between 0 and 10.
The “Boston” data have 13 input attributes and 1 output attribute containing
median value of owned houses in suburbs of Boston. There are 506 instances. The
bottom of Table 3 shows the performances, measured with absolute error. The
conclusions of this set of experiments are similar to those from the classiﬁcation
trees. The best performed one is DTIPs, where the regression trees learned from
CART are used as initial solutions to DTIP.
Discrimination-aware DTIP. In order to model the discrimination level of a
learned tree, we include a simple constraint that computes the diﬀerence in pos-
itive class probability for diﬀerent sets of rows (by summing and comparing
errors er). This diﬀerence is added to the objective function with a large multi-
plier (the data size), in this way the solver will try ﬁnd the most accurate tree
with zero discrimination. For this experiment, we assume that married is a sen-
sitive attribute in the bank data set. Since bank is too large to solve eﬃciently
using DTIP, we only use the top 1000 rows. After running DTIP for 15 min from
a Scikit-learn starting solution with accuracy 0.86 and 0.05 discrimination, we
obtain 0 discrimination for a depth 3 tree, with 0.85 accuracy. For comparison,
we also ran DTIP without discrimination constraints, which gives an accuracy
of 0.81 after 15 min. This result demonstrates the ﬂexibility of DTIP: adding a
single constraint gives solutions satisfying a diﬀerent objective.

102
S. Verwer and Y. Zhang
Imbalanced DTIP. For imbalanced data problems, such as the ﬁrst 1000 rows
from the bank set, depending on the problem context it can be important to ﬁnd
solutions with very few false positives or very few false negatives. In order to
demonstrate the ﬂexibility of DTIP, we again add a single constraint for counting
false positives or negatives (by summing er values) and add it to the objective
function with a large multiplier. We ran DTIP for 15 min, starting from the
Scikit-learn solution, and obtain a depth 3 tree with 0 false positives and 101
false negatives, or one with 672 false positives and 0 false negatives.
4
Conclusion
We give an eﬃcient encoding of decision tree learning in integer programming.
Experimental results demonstrate the strengths and limitations of our approach.
Decision trees of depth up to 5 can be learned from data sets of size up to 1000.
Larger data sets create to many constraints to be solved eﬀectively using a MIP
solver. We show how to use our approach to improve existing solutions provided
by a standard greedy approach. Moreover, we demonstrate the ﬂexibility of our
approach by modelling diﬀerent objective functions. In the future, we will inves-
tigate other objectives, integration with existing MIP models, and speeding up
the search by ﬁxing variables and using lazy constraints.
Acknowledgments. This work is partially funded by Technologiestichting STW
VENI project 13136 (MANTA).
References
1. Bennett, K.P., Blue, J.A.: Optimal decision trees. Technical report., R.P.I. Math
Report No. 214, Rensselaer Polytechnic Institute (1996)
2. Bertsimas, D., Shioda, R.: Classiﬁcation and regression via integer optimization.
Oper. Res. 55(2), 252–271 (2007)
3. Breiman, L., Friedman, J., Olshen, R., Stone, C.: Classiﬁcation and Regression
Trees. Wadsworth International Group, Belmont (1984)
4. Bessiere, C., Hebrard, E., O’Sullivan, B.: Minimising decision tree size as combi-
natorial optimisation. In: Gent, I.P. (ed.) CP 2009. LNCS, vol. 5732, pp. 173–187.
Springer, Heidelberg (2009). doi:10.1007/978-3-642-04244-7 16
5. Bruynooghe, M., Blockeel, H., Bogaerts, B., De Cat, B., De Pooter, S., Jansen, J.,
Labarre, A., Ramon, J., Denecker, M., Verwer, S.: Predicate logic as a modeling
language: modeling and solving some machine learning and data mining problems
with idp3. Theor. Pract. Logic Program. 15(06), 783–817 (2015)
6. Carrizosa, E., Morales, D.R.: Supervised classiﬁcation and mathematical optimiza-
tion. Comput. Oper. Res. 40(1), 150–165 (2013)
7. Cortez, P., Cerdeira, A., Almeida, F., Matos, T., Reis, J.: Modeling wine prefer-
ences by data mining from physicochemical properties. Decis. Support Syst. 47(4),
547–553 (2009)
8. De Raedt, L., Guns, T., Nijssen, S.: Constraint programming for data mining and
machine learning. In: AAAI, pp. 1671–1675 (2010)

Learning Decision Trees with Flexible Constraints and Objectives
103
9. Flach, P.: Machine Learning: The Art and Science of Algorithms that Make Sense
of Data. Cambridge University Press, Cambridge (2012)
10. Heule, M.J., Verwer, S.: Software model synthesis using satisﬁability solvers.
Empirical Softw. Eng. 18(4), 825–856 (2013)
11. Hyaﬁl, L., Rivest, R.L.: Constructing optimal binary decision trees is np-complete.
Inf. Proc. Lett. 5(1), 15–17 (1976)
12. Lichman, M.: UCI machine learning repository. http://archive.ics.uci.edu/ml
13. Moro, S., Cortez, P., Rita, P.: A data-driven approach to predict the success of
bank telemarketing. Decis. Support Syst. 62, 22–31 (2014)
14. Norouzi, M., Collins, M.D., Johnson, M., Fleet, D.J., Kohli, P.: Eﬃcient non-greedy
optimization of decision trees. In: NIPS, pp. 1729–1737. MIT Press (2015)
15. Quinlan, J.R.: Induction of decision trees. Mach. Learn. 1(1), 81–106 (1986)
16. Verwer, S., Zhang, Y., Ye, Q.C.: Auction optimization using regression trees and
linear models as integer programs. Artif. Intell. 244, 368–395 (2017)

Relaxation Methods for Constrained Matrix
Factorization Problems: Solving the Phase
Mapping Problem in Materials Discovery
Junwen Bai1, Johan Bjorck1(B), Yexiang Xue1, Santosh K. Suram2,
John Gregoire2, and Carla Gomes1
1 Department of Computer Science, Cornell University, Ithaca, NY 14850, USA
ujb225@cornell.edu
2 Joint Center for Artiﬁcial Photosynthesis, California Institute of Technology,
Pasadena, CA 91125, USA
Abstract. Matrix factorization is a robust and widely adopted tech-
nique in data science, in which a given matrix is decomposed as the
product of low rank matrices. We study a challenging constrained matrix
factorization problem in materials discovery, the so-called phase mapping
problem. We introduce a novel “lazy” Iterative Agile Factor Decomposi-
tion (IAFD) approach that relaxes and postpones non-convex constraint
sets (the lazy constraints), iteratively enforcing them when violations are
detected. IAFD interleaves multiplicative gradient-based updates with
eﬃcient modular algorithms that detect and repair constraint violations,
while still ensuring fast run times. Experimental results show that IAFD
is several orders of magnitude faster and its solutions are also in gen-
eral considerably better than previous approaches. IAFD solves a key
problem in materials discovery while also paving the way towards tack-
ling constrained matrix factorization problems in general, with broader
implications for data science.
Keywords: Constrained matrix factorization · Relaxation methods ·
Multiplicative updates · Phase-mapping
1
Introduction
Matrix factorization has become a ubiquitous technique in data analysis, with
applications in a variety of domains such as computer vision [10], topic modeling
[6], audio signal processing [11], and crystallography [12]. Often the phenomena
considered is naturally non-negative. In non-negative matrix-factorization, the
goal is to explain a non-negative signal as the product of (typically) two non-
negative low rank matrices. Nonnegative matrix factorization is known to be
NP-Hard [13], so a general algorithm for matrix factorization most likely scales
exponentially in the worst case.
We consider a challenging and central problem in materials discovery, so-
called phase-mapping, an inverse problem whose goal is to infer the materials’
c
⃝Springer International Publishing AG 2017
D. Salvagnin and M. Lombardi (Eds.): CPAIOR 2017, LNCS 10335, pp. 104–112, 2017.
DOI: 10.1007/978-3-319-59776-8 9

Relaxation Methods for Constrained Matrix Factorization Problems
105
crystal structure based on X-ray sample data, see Fig. 1(Left). Phase-mapping
was shown to be NP-Hard [5]. Existing approaches to phase mapping, discussed
in the next section, do not satisfy all the problem constraints. Furthermore,
approaches that explicitly try to incorporate the main problem constraints have
prohibitive run times on typical real-world data, hours or days, while still not
producing solutions that are completely physically meaningful.
We propose a novel Interleaved Agile Factor Decomposition (IAFD)
approach that “lazily” relaxes and postpones non-convex constraint sets (the
lazy constraints), iteratively enforcing them when violations are detected, see
Fig. 1(Right). IAFD uncovers the main underlying problem structure revealed
by the sample data by rapidly performing a large number of lightweight gradient-
based moves. In order to incorporate more intricate combinatorial constraints,
the algorithm interleaves the multiplicative gradient-based updates with eﬃ-
cient modular algorithms that detect and repair constraint violations, while still
ensuring fast run times, scaling up to large scale real-world problems. Our exper-
imental results show that IAFD is several orders of magnitude faster and its
solutions are also in general considerably better than previous approaches. Our
work provides an eﬃcient approach to solving a central problem in materials dis-
covery, while paving the way towards tackling constrained matrix factorization
problems in general, with broader implications for data science.
2
The Phase Mapping Problem
In search of new materials a common experimental method is to deposit several
elements onto a sample wafer at diﬀerent angles. The sample locations on the
wafer receive diﬀerent concentrations of the elements. As a result, distinct and
potentially undiscovered materials are formed at diﬀerent locations. All materials
can be characterized by a one-dimensional X-ray diﬀraction pattern F(q), which
can be measured at high energy accelerators. However, several phases might be
present at one sample location and the X-ray diﬀraction pattern at that location
then becomes a linear combination of a set of basis patterns, each corresponding
to the pattern of one pure phase. Figure 1(Left) illustrates this phenomenon.
In the mathematical model of the problem, a matrix A representing a set
of X-ray measurements on a sample wafer is obtained. Each column of A is a
vector representing the pattern F(q) obtained at one sample location, sampled
for Q ﬁxed values of q. The phase mapping problem entails factorizing A into
the product of W and H such that A ≈WH.
The matrix W encodes the characteristic patterns of pure phases while H
represents how much of the diﬀerent phases are present at individual sample
location. A complicating factor of the phase-mapping problem is that the laws of
thermodynamics induce a set of physical constraints on the possible underlying
low rank representation. The solutions must satisfy these constraints, deﬁned
below, and must additionally be nonnegative as the physical quantities described
by the matrices cannot be negative. Eﬃcient methods of solving this problem
accelerates materials science and enables automatic experimentation in search
of tomorrow’s semiconductor and photvoltaic materials.

106
J. Bai et al.
Fig. 1. (Left) The goal of the phase mapping problem is to explain observed X-ray
diﬀraction patterns at multiple sample locations in terms of the underlying phases or
crystal structures of the materials. Here the X-ray diﬀraction patterns of sample loca-
tions on the right edge of the triangle are shown in the middle plot. The top four sample
locations only have phase α, the bottom three only have phase β, while the middle four
sample locations have both α and β. In addition, the X-ray diﬀraction patterns of both
phase α and β are shifting to the right. (Right) At a high level, our Interleaved Agile
Factor Decomposition (IAFD) algorithm starts with solving a relaxed problem using
the multiplicative update rules of AgileFD [14], without enforcing combinatorial con-
straints. Violations of the Gibbs’ phase rule, the alloying rule, and the connectivity
constraint in the relaxed solutions are then addressed by eﬃcient modular algorithms,
in an interleaving manner. This procedure is iterated, creating a closed loop involving
AgileFD and the three modules.
Shifting. A phenomenon that complicates the matrix factorization is “shift-
ing”, where the X-ray patterns are changed in the sense F(q) →F(λkq), for
some real number λk that is ﬁxed for each phase k and column in A. For exam-
ple, the X-ray patterns in Fig. 1 are shifting to the right. The problem can
be circumvented by resampling the signal uniformly on a logarithmic scale,
where multiplicative shifts becomes additive. For ﬁxed m and k, the vector
(0, . . . , 0, W1,k, . . . , WQ−m,k)T formed by shifting the k-th column of W down
by m entries (and ﬁlling 0 for remaining entries) describes basis pattern of phase
k shifted by an amount controlled by m. We can then allow λk to attain M
diﬀerent discrete values by letting m ∈0, 1...M −1. By characterizing the H
matrix with three indices, one per phase k, sample point n, and allowed discrete
value of λk m, we can now express a linear combination of shifted basis patterns
as Aqn ≈
km Wq−m,kHkmn. Since this speciﬁc formulation will be used, the
constraints of the phase mapping problem will be given in terms of Wqk and
Hkmn, however other formulations of the rules are possible [3].
Gibbs’ Phase Rule. In a setting with three elements deposited, such as in
Fig. 1, Gibbs’ phase rule [1] states that the number of phases present at each
sample location is at most three. Mathematically, it is equivalent to constraining
the number of non-zero elements in vector (
m H1mn, 
m H2mn, . . .) for any
phase k to be no more than three. Thus, for ﬁxed n we have ∥
m Hkmn∥0 ≤3.

Relaxation Methods for Constrained Matrix Factorization Problems
107
Connectivity. The connectivity rule requires that the sample points where a
speciﬁc phase is present form a continuous domain on the sample wafer. For
example, in Fig. 1, each pattern occupies a continuous region. Mathematically,
since we have a discrete set of measurements we describe the constraint via a
graph G where sample points are nodes and nearby sample points are connected
with an edge. This graph is obtained through Delauney triangulation [7] of the
sample points. A continuous domain then corresponds to a connected component
on this graph, and we require that all sample points n with phase k present, i.e.

m Hkmn > 0, form a connected component on G.
Alloying Rule. The shifting parameter λk for phase k may shift continuously
across the sample points as a result of so called alloying. The alloying rule states
that for points where λk is changing, Gibbs’ phase rule becomes even stricter
and requires ∥
m Hkmn∥0 ≤2. In this discrete setting we interpret λk of a
point n as 
m Hnkmm/ 
m Hnkm, which can be thought of as the expectation
of m when we normalize Hkmn to a probability distribution. Two neighboring
sample points n and n′ with phase k present, which means 
m Hkmn > 0 and

m Hkmn′ > 0, are considered shifting if


m Hkmnm

m Hkmn
−

m Hkmn′m

m Hkmn′
 > ϵ,
(1)
The alloying rule states that if Eq. 1 is satisﬁed for any phase k and neighbouring
sample points n′ and n, then we must have ∥
m Hkmn∥0 ≤2.
2.1
Previous Approaches
Many algorithms have been proposed for solving the phase mapping problem,
for example [5,8,9]. Recently an eﬃcient algorithm called AgileFD [14], based
on coordinate descent using multiplicative updates, has been proposed. If we let
the matrix R represent the product of H and W, i.e. Rqn = 
m Wq−m,kHkmn
these updates are
Hkmn ←Hkmn

q Wq−m,k(Aqn/Rqn)

q Wq−m,k + γ
,
(2)
Wqk ←Wqk

mn
Aq+m,n
Rq+m,n
Hkmn + Wqk

q′nm HkmnWq′k

nk
Hkmn + Wqk

q′nm
Aq′+m,n
Rq′+m,n
HkmnWq′k
.
(3)
The algorithm relies on manual reﬁnement by domain experts to enforce combi-
natorial constraints, which makes it problematic to use in a scalable fashion.
Another approach called combiFD, able to express all constraints, has been
proposed [2]. It relies on a combinatorial factor decomposition formulation, where
iteratively H or W are frozen while the other is updated by solving a MIP. This
formulation allows all constraints to be expressed upfront, however solving the
complete MIP programs is infeasible in practice.

108
J. Bai et al.
3
Interleaved Agile Factor Decomposition
Given a non-negative Q-by-N measurement matrix A and the dimensions K
and M of the factorization, the phase mapping problem entails explaining A as
a generalized product of two low rank non-negative matrices W, H. The entire
mathematical formulation becomes:
min

qn
|Aqn −

mk
Wq−m,kHkmn|,
s.t. H ∈RK×M×N
+
,
W ∈RQ×K
+
,
H, W satisﬁes Gibbs’ phase rule, Connectivity, Alloying rule.
(4)
Representing the combinatorial rules as integer constraints has previously
been tried [2], however the resulting large MIP formulations are not feasible to
solve in practice. Instead, we propose a novel iterative framework that inter-
leaves eﬃcient multiplicative updates with compact subroutines able to address
speciﬁc constraints, called Interleaved Agile Factor Decomposition (IAFD). The
algorithm is illustrated, at a high level, in Fig. 1 (Right). The central insight is
that our constraints are too expensive to explicitly encode and maintain, however
ﬁnding and rectifying individual violations can be done eﬃciently. This motivates
a lazy approach that relaxes and postpones non-convex constraint sets (the lazy
constraints), iteratively enforcing them only as violations are detected. For each
constraint we provide an eﬃcient method to detect violations and repair them
through much smaller optimization problems.
The IAFD algorithm starts with solving the relaxed problem, with only the
convex non-negativity constraint, using the multiplicative updating rules (2) and
(3) of AgileFD [14]. This relaxed solution is then slightly reﬁned by three subrou-
tines which sample and rectify violations of Gibbs’ phase rule, the alloying rule,
and the connectivity constraint respectively, by solving small scale optimization
problems. The reﬁned solution is then relaxed again and improved through the
multiplicative updates. This process is repeated in an interleaving manner which
creates a closed loop involving AgileFD and the three reﬁning modules. A reason
why this interleaving can be expected to not produce much duplicate eﬀort is
due to the following observation:
Proposition 1. The number of non-zero entries in H : ∥{(n, k)| 
m Hnkm >
0}∥is nonincreasing under updates (2) of AgileFD.
This comes from the fact that every component is updated through multi-
plication with itself in (2), which ensures that zero-components stay zero. Thus,
if Gibbs’ phase rule is satisﬁed before the multiplicative updates, it will still be
satisﬁed after. We now describe the subroutines handling the constraints.
Gibbs’ Phase Rule Reﬁnement. After obtaining the matrix W and H, we
ﬁnd violations of Gibbs’ phase rule by scanning sample points and noting which
ones have more than three phases present. One key insight is that the problem of
enforcing Gibbs’ phase rule decouples between sample points once the matrix W
is ﬁxed. In order to represent the constraint that no more than three phases are

Relaxation Methods for Constrained Matrix Factorization Problems
109
present, we introduce a binary variable δkn denoting whether phase k is present
at sample location n (i.e., 
m Hkmn is nonzero). The constraint is now enforced
by solving the following mixed integer program with W ﬁxed for each violated
sample point, which results in a very light-weight reﬁnement:
min
δ,Hkmn∀k,m

q
|Aqn −

mk
Wq−m,kHkmn|,
s.t. ∀k, m Hkmn ≤Mδnk,

k
δnk ≤3.
(5)
Here, Hkmn ≤Mδnk is a big-M constraint, which enforces that phase k is
zero if δnk is zero. We use 
k δnk ≤3 to enforce that only three phases are
allowed. These compact programs typically contains two orders of magnitude
fewer variables then the complete program, and can be quickly solved in parallel.
Alloying Rule Reﬁnement. Violations of the alloying rule can be found by
comparing the shift parameter λk of some sample point n, here interpreted
as 
m Hkmnm/ 
m Hkmn, to that of its neighbors in graph G. This simply
amounts to a linear scan through all sample points. It is again possible to decou-
ple the constraint by taking W and n as ﬁxed, which allows for a compact mixed
integer program formulation. We ﬁx the violating sample point n, denote the set
of its neighbors as N(n), and then calculate λkn′ = 
m mHkmn′/ 
m Hkmn′
for all neighbors n′ ∈N(n) where phase k is present. In the MIP the binary vari-
able δkn is used to denote whether phase k is present at sample point n, another
binary variable τn is then introduced to denote whether the sample point under-
goes shift. By using a large M-constraint as in Gibbs’ phase rule module we can
encode that unless the sample point is shifting or doesn’t contain the phase k,
the λk has to be close to that of it’s neighbors as follows:
min
τ,δ,Hkmn∀k,m

q
|Aqn −

mk
Wq−m,kHkmn|,
s.t. |

m
Hkmnm −λkn′

m
Hkmn| ≤ϵ

m
Hkmn + Mτn + M(1 −δkn),
∀k, m, n′ ∈N(n),
Hkmn ≤Mδnk,

k
δnk + τn ≤3.
(6)
Connectivity Reﬁnement. While explicitly encoding the constraint is com-
putationally expensive, ﬁnding violations can be done in a lightweight manner.
For each phase k we ﬁnd all continuous regions containing phase k by simply
ﬁnding the connected components of our graph G where phase k is present.
To rectify the constraint, every connected component C is then weighted by
the total amount of present phase, which amounts to calculating the quantity

n∈C,m Hkmn. This weight corresponds to the amount of present signal. We
then zero out components in H corresponding to phase k and sample points in
the least weighted connected components. This procedure ensures that all the
phases correspond to a single contiguous regions, without deteriorating much (if
at all) the objective function in general.

110
J. Bai et al.
Fig. 2. (Left) Normalized L1 Loss of the diﬀerence between ground-truth and recon-
structed X-ray patterns for the algorithms on 8 real world systems. IAFD performs best,
with combiFD lagging behind the two other methods. (Right) Runtime for CombiFD,
AgileFD, IAFD to solve 8 real systems, note the logarithmic time scale. We can clearly
see that the heavy duty MIP formulation of combiFD results running times of hours,
while the two lightweight methods runs in a matter of minutes.
4
Experimental Results
IAFD is evaluated on several real world instances of the phase mapping problem,
available at [4]. We randomly initialize the matrices, and as the interleaving
with the connectivity-subroutine and the alloying-subroutine assumes structured
data, the whole algorithm starts with several rounds of AgileFD interleaving with
Gibbs’ rule followed by the other two subroutines. The diﬀraction patterns are
probed at around 200 locations of the respective wafers with approximately 1700
values of q sampled, we set K = 6 and M = 8 which gives us around two million
variables per problem. More rounds of interleaving lead to better results but of
course it takes more time. We chose to do three rounds of AgileFD interleaving
with Gibbs’ rule followed by enforcing the other two constraints to balance these
tradeoﬀs. Our method is compared against CombiFD [2], with a mipgap of 0.1
and 15 iterations. Due to its poor scaling properties only the Gibbs’ phase rule
is enforced for CombiFD. We also compare IAFD against AgileFD [14], with
termination constant set to 10−5.
The most important metric when comparing diﬀerent methods is the solution
quality, measured by L1 loss. Results shown in Fig. 2 (Left). It is evident that
CombiFD in [2] has subpar performance, while IAFD wins by a slight margin
over AgileFD. This suggests that enforcing the constraints actually improves
the reconstruction error. The area where we expect IAFD to perform the best
is in terms of enforcing the physical constraints, which is illustrated in Table 1.
Here IAFD consistently performs the best with zero violations, which results in
physically meaningful solutions to the phase mapping problem.
The smaller subroutines are evidently able to handle all constraints and
additionally provide a low loss, which might lead one to suspect that IAFD
has long run times. That is not the case. The run times can be viewed in Fig. 2
(Right). While AgileFD is slightly faster than IAFD, the diﬀerence is very small.

Relaxation Methods for Constrained Matrix Factorization Problems
111
CombiFD, which explicitly enforces the constraints [2], has prohibitive long run
times in practice, which suggests that a complete MIP encoding is both ineﬃ-
cient and unnecessary. These results show that IAFD can enforce all physical
rules, without sacriﬁcing much in either reconstruction error or running time.
Table 1. To the left we see the fraction of sample points violating the alloying rule for
diﬀerent algorithms, where IAFD consistently has no violations. The right side gives
the average number of connected components per phase, and here only IAFD always
contain a single continuous region as required by the connectivity constraint.
System
Alloying constraint
Connectivity constraint
CombiFD AgileFD IAFD CombiFD AgileFD IAFD
(Fe-Bi-V)Ox(I)
0.57
0.15
0.00
1.00
1.65
1.00
(Fe-Bi-V)Ox(II)
0.55
0.30
0.00
2.40
1.65
1.00
(Fe-Bi-V)Ox(III) 0.18
0.03
0.00
2.50
2.18
1.00
(Zn-Sn-Si)Nx(I)
0.06
0.01
0.00
1.00
2.38
1.00
(Zn-Sn-Si)Nx(II) 0.05
0.02
0.00
2.00
1.38
1.00
(W-Bi-V)Ox
0.54
0.08
0.00
1.67
2.31
1.00
(Ag-Bi-V)Ox
0.84
0.16
0.00
3.60
1.96
1.00
(Mo-Bi-V)Ox
0.46
0.08
0.00
1.60
1.72
1.00
5
Conclusions
We propose a novel Interleaved Agile Factor Decomposition (IAFD) framework
for solving the phase mapping problem, a challenging constrained matrix fac-
torization problem in materials discovery. IAFD is a lightweight iterative app-
roach that lazily enforces non-convex constraints. The algorithm is evaluated on
several real world instances and outperforms previous solvers both in terms of
run time and solution quality. IAFD’s approach, based on eﬃcient multiplicative
updates from unconstrained nonnegative matrix factorization and lazily enforced
constraints, performs much better compared to approaches that enforce all con-
straints upfront, using a large mathematical program. This approach opens up
a new angle for eﬃciently solving more general constrained factorization prob-
lems. We anticipate deploying IAFD at the Stanford Synchrotron Radiation
Lightsource in the near future to the beneﬁt of the materials science community.
Acknowledgements. We thank Ronan Le Bras and Rich Bernstein for fruitful dis-
cussion. This material is supported by NSF awards CCF-1522054, CNS-0832782, CNS-
1059284, IIS-1344201 and W911-NF-14-1-0498. Experiments were supported through
the Oﬃce of Science of the U.S. Department of Energy under Award No. DE-
SC0004993. Use of the Stanford Synchrotron Radiation Lightsource, SLAC National
Accelerator Laboratory, is supported by the U.S. Department of Energy, Oﬃce of Sci-
ence, Oﬃce of Basic Energy Sciences under Contract No. DE-AC02-76SF00515.

112
J. Bai et al.
References
1. Atkins, P., De Paula, J.: Atkins’ Physical Chemistry, p. 77. Oxford University
Press, New York (2006)
2. Ermon, S., Bras, R.L., Suram, S.K., Gregoire, J.M., Gomes, C., Selman, B.,
Van Dover, R.B.: Pattern decomposition with complex combinatorial constraints:
application to materials discovery. arXiv preprint arXiv:1411.7441 (2014)
3. Ermon, S., Bras, R., Gomes, C.P., Selman, B., Dover, R.B.: SMT-aided combinato-
rial materials discovery. In: Cimatti, A., Sebastiani, R. (eds.) SAT 2012. LNCS, vol.
7317, pp. 172–185. Springer, Heidelberg (2012). doi:10.1007/978-3-642-31612-8 14
4. Le Bras, R., Bernstein, R., Suram, S.K., Gregoire, J.M., Selman, B., Gomes, C.P.,
van Dover, R.B.: A computational challenge problem in materials discovery: syn-
thetic problem generator and real-world datasets (2014)
5. LeBras, R., Damoulas, T., Gregoire, J.M., Sabharwal, A., Gomes, C.P., Dover,
R.B.: Constraint reasoning and kernel clustering for pattern decomposition with
scaling. In: Lee, J. (ed.) CP 2011. LNCS, vol. 6876, pp. 508–522. Springer,
Heidelberg (2011). doi:10.1007/978-3-642-23786-7 39
6. Lee, D.D., Seung, H.S.: Learning the parts of objects by non-negative matrix fac-
torization. Nature 401(6755), 788–791 (1999)
7. Lee, D.T., Schachter, B.J.: Two algorithms for constructing a delaunay triangula-
tion. Int. J. Comput. Inf. Sci. 9(3), 219–242 (1980)
8. Long, C., Bunker, D., Li, X., Karen, V., Takeuchi, I.: Rapid identiﬁcation of struc-
tural phases in combinatorial thin-ﬁlm libraries using X-ray diﬀraction and non-
negative matrix factorization. Rev. Sci. Instrum. 80(10), 103902 (2009)
9. Long, C., Hattrick-Simpers, J., Murakami, M., Srivastava, R., Takeuchi, I., Karen,
V.L., Li, X.: Rapid structural mapping of ternary metallic alloy systems using
the combinatorial approach and cluster analysis. Rev. Sci. Instrum. 78(7), 072217
(2007)
10. Shashua, A., Hazan, T.: Non-negative tensor factorization with applications to sta-
tistics and computer vision. In: Proceedings of the 22nd International Conference
on Machine Learning, pp. 792–799. ACM (2005)
11. Smaragdis, P.: Non-negative matrix factor deconvolution; extraction of multiple
sound sources from monophonic inputs. In: Puntonet, C.G., Prieto, A. (eds.) ICA
2004. LNCS, vol. 3195, pp. 494–499. Springer, Heidelberg (2004). doi:10.1007/
978-3-540-30110-3 63
12. Suram, S.K., Xue, Y., Bai, J., Bras, R.L., Rappazzo, B., Bernstein, R., Bjorck,
J., Zhou, L., van Dover, R.B., Gomes, C.P., et al.: Automated phase mapping
with agilefd and its application to light absorber discovery in the V-Mn-Nb oxide
system. arXiv preprint arXiv:1610.02005 (2016)
13. Vavasis, S.A.: On the complexity of nonnegative matrix factorization. SIAM J.
Optim. 20(3), 1364–1377 (2009)
14. Xue, Y., Bai, J., Le Bras, R., Rappazzo, B., Bernstein, R., Bjorck, J., Longpre, L.,
Suram, S., van Dover, B., Gregoire, J., Gomes, C.: Phase mapper: an AI platform
to accelerate high throughput materials discovery. In: Twenty-Ninth International
Conference on Innovative Applications of Artiﬁcial Intelligence (2016)

Minimizing Landscape Resistance for Habitat
Conservation
Diego de U˜na1(B), Graeme Gange1, Peter Schachte1, and Peter J. Stuckey1,2
1 Department of Computing and Information Systems, The University of Melbourne,
Melbourne, Australia
{gkgange,schachte,pstuckey}@unimelb.edu.au,
d.deunagomez@student.unimelb.edu.au
2 Data 61, CSIRO, Melbourne, Australia
Abstract. Modeling ecological connectivity is an area of increasing
interest amongst biologists and conservation agencies. In the past few
years, diﬀerent modeling approaches have been used by experts in
the ﬁeld to understand the state of wildlife distribution. One of these
approaches is based on modeling land as a resistive network. The analy-
sis of electric current in such networks allows biologists to understand
how random walkers (animals) move across the landscape. In this paper
we present a MIP model and a Local Search approach to tackle the
problem of minimizing the eﬀective resistance in an electrical network.
This is then mapped onto landscapes in order to decide which areas need
restoration to facilitate the movement of wildlife.
1
Introduction
In the past decades, the natural habitat of diﬀerent species across the globe
have become disrupted and fragmented. This is considered to be a major threat
to the conservation of biodiversity by biologists and conservation experts [24].
As pointed out by Rosenberg et al. [24] and Pimm et al. [22] among other
experts, the isolation of groups of animals can easily lead to extinction. For
this reason, restoration needs to be undertaken in order to maintain a suitable
environment where wildlife can move, feed and breed. Landscape restoration is
part of Computational Sustainability, which has been increasingly attractive for
researchers in our community as it presents interesting computational problems
that happen to be NP-hard [10,14,17,26,27].
In ecology, patches of landscape are characterized by their resistance. This
corresponds to how hard it is for wildlife to traverse a land patch. For instance, a
low resistance value might be caused by soil of good quality that allows plants to
spread more easily, or an open ﬁeld might have high resistance for small rodents
(that may be targets for birds of prey). Of course, this measure depends on the
species being studied: some animals may be able to cross rivers more easily than
others. Lower resistance means the land patch is more suitable for the species
studied. Therefore, a suitable environment for wildlife would be one where their
c
⃝Springer International Publishing AG 2017
D. Salvagnin and M. Lombardi (Eds.): CPAIOR 2017, LNCS 10335, pp. 113–130, 2017.
DOI: 10.1007/978-3-319-59776-8 10

114
D. de U˜na et al.
core habitats are connected by low resistance patches, so that animals can freely
travel between core habitats to breed and feed.
To improve landscape connectivity, ecologists have used the idea of corridors
[7,15]. A corridor connects core habitats of animals using uninterrupted paths
through which the animals can move. A common measure of the quality of a
corridor is know as the Least-Cost Corridor (LCC) [1,6]. LCCs are chosen to
minimize the total sum of the resistance in the corridor. There has been extensive
work in the Constraint Programming, AI and OR communities in helping identify
the best corridors to be built (e.g. [10,17,27]).
Other work has addressed the problem of habitat conservation without
enforcing connectivity. For instance, Crossman et al. [9] wrote a MIP model for
habitat conservation. In their case, the intention was to minimize the number of
sites to be restored while keeping a desired area for the animals and maintaining
safe distances to roads and other dangers.
Although LCC is a valid model that is broadly used, it has been criticized for
over simplifying the actual movement of species [20]. McRae [19,21] proposed
the use of electric circuit theory to measure the total connectivity of a landscape.
This model is called Isolation By Resistance (IBR). In particular, it was shown
[20] how the IBR model better matches the empirical evidence of genetic distance
amongst a distributed species measured with two standard statistical models
(ﬁxation index and a step-wise mutation model). The IBR model has since been
used to study the eﬀects of habitat loss in birds [3,4], for instance.
In the IBR model, the land patches are modeled as nodes in an electric
circuit. The transition between contiguous patches is modeled by a branch of
the electric circuit carrying a resistor. The resistance of the resistor in Ohms
gives the resistance of moving between adjacent patches. The circuit is then
connected to a 1 A current source in a core habitat, while another core habitat
is connected to the ground (0 V). The eﬀective resistance between two habitats
in the electric circuit is the measure used for connectivity in the IBR model. It
physically corresponds to the real resistance between two points of the circuit.
An example of such circuit with 16 land patches (i.e. 16 nodes) can be seen on
the left of Fig. 2. Experts use the tool Circuitscape [25] for this model. Its task is
to compute the currents, voltages and eﬀective resistance that are then viewable
by experts in geographic visualization software. Nevertheless, Circuitscape does
not make conservation decisions, it only builds a linear system and solves it for
the experts.
The model is justiﬁed by the fact that the commute time between two nodes
s and t is given by 2mRst where m is the number of edges in the graph, and
Rst is the eﬀective resistance between s and t in the underlying electrical net-
work (Theorem 4.1 in [18]). The goal is therefore to lower the eﬀective resistance
between core habitats, as this directly translates into decreasing the commute
time between habitats. Previous work by Doyle and Snell [12] also proved this
property, and gave an interpretation of current ixy through a branch (x, y) of
the underlying electrical network. The current gives the net number of times a
random walker would walk from x to y using that branch. This is exactly what

Minimizing Landscape Resistance for Habitat Conservation
115
is used by biologists to detect areas where wildlife concentrates most in the IBR
model [21]. As an example, consider Fig. 1. On the right side the landscape is
plotted showing the conductance values in Ω−1 (inverse of resistance). On the
left, a heatmap of the current at each patch of land. We can observe that, when
moving out of their habitats, animals tend to walk using high conductance areas:
indeed the [0.2, 1] areas of the heatmap tend to coincide with high conductance
areas on the conductance map.
Fig. 1. Example of landscape. Left: The heatmap shows the current (in Amperes) in
the electric circuit. The three darkest/red patches correspond to core habitats. This
model predicts that animals will walk on the high current areas (⪆0.2 A, yellow/orange)
often and less often in low current (⪅0.2 A, green) areas. Right: Map of conductance
(in Ω−1). Darker/green areas are land where the resistance is low and easy for animals
to move across. (Color ﬁgure online)
For habitat and ecologic planning using the Isolation By Resistance model,
we are interested in minimizing the eﬀective resistance between habitats. To do
so, biologists need to decide where improving the habitat is more beneﬁcial. An
improvement could mean planning for reforestation, building highway bridges
for animals or improving the quality of the soil, among other actions. Our goal
here will be to choose the right spots for these investments, subject to a budget.
Note how the technical term eﬀective resistance is often substituted by resis-
tance distance in the biology literature. Also, in a non-reactive circuit with no
alternating current, eﬀective resistance and equivalent resistance are equal. For
consistency we will always refer to eﬀective resistance.
A similar problem was addressed by Ghosh et al. [13], but their problem was
continuous. The continuous variant does not apply in our habitat conservation
planning, as it is not possible to invest an inﬁnitesimal fraction of budget in one
location. In the real world, reforestation areas are typically well deﬁned discrete
interventions.

116
D. de U˜na et al.
Section 2 is a brief summary on some basic electric circuit theory that we
use later on. In Sect. 3 we discuss our Mixed-Integer Programming (MIP) model
in detail. Section 4 discusses the problems of our model and presents our Local
Search (LS) approach. Lastly, Sect. 5 presents our experimental results.
2
Preliminaries
The eﬀective resistance corresponds to the real resistance between two nodes in
the circuit. During the rest of this paper we will note Rxy the eﬀective resistance
between x and y.
In electric circuits, the eﬀective resistance can be computed by solving a
system of linear equations [2,11]. Such linear system can be obtained by doing
nodal analysis or mesh analysis. A more systematic way of obtaining the same
system of linear equations is by using the nodal admittance matrix [5], also
known by graph theoreticians as the Laplacian matrix. Let adj(n) be the set
of nodes adjacent to a node n in an electric circuit, and gij the conductance of
branch (i, j). For an electric circuit of n nodes, the Laplacian is an n × n matrix
deﬁned as follows:
Li,j =
⎧
⎪
⎨
⎪
⎩
−gij
if (i, j) is a branch of the circuit

k∈adj(i) gik
if i = j
0
otherwise
For any two nodes s and t in an electric circuit of Laplacian L, we can
calculate the eﬀective resistance Rst between those nodes by simply solving the
linear system given by Ltv = f where Lt is L without the tth row and column,
and f is a vector with all its elements equal to 0 except for the sth, which is
equal to 1. The eﬀective resistance we are after will be found in vs.
This is strictly equivalent to nodal analysis. In classic nodal analysis, we
would connect a source of 1 A to s and connect t to the ground. Then we obtain
an equation for each node of the form 
y∈adj(x) ixy = 0 (ﬂow conservation of the
current), except for the node s where the ﬂow of current is 1 since it’s connected
to the source, and the node t where the ﬂow is -1 for being connected to the
ground. Ohm’s law gives us that, for any branch, ixy = gxy(vx−vy) where vx and
vy are the voltages at nodes x and y and gxy is the conductance of the resistor
between x and y. Then any node can be selected as reference or datum node,
setting its voltage to 0. We select t as the datum node. Then by substituting
with Ohm’s law and the value of the datum node in the nodal analysis equations
we obtain the same system as with the Laplacian. Solving the system yields the
voltage of node s, among others. Since the source of current was 1 A, Ohm’s law
gives us that Rst = (vs −vt)/1 = vs. It is therefore only necessary to look at the
sth component of the solution vector to obtain the eﬀective resistance. This is
exactly the method implemented in Circuitscape [25], software used nowadays
for landscape planning by experts.
Note how the deﬁnition of eﬀective resistance only allows the computation
of one eﬀective resistance at a time: the current source needs to be connected

Minimizing Landscape Resistance for Habitat Conservation
117
to exactly one node, s, and only one node, t, must be connected to the ground.
Therefore, the eﬀective resistance between three or more nodes is undeﬁned.
Instead, we will consider the total eﬀective resistance for three nodes a, b and
c to be Rab + Rac + Rbc. These have to be computed by solving three diﬀerent
equation systems. In general, for f nodes, we will need f(f −1)/2 linear systems.
3
Problem Formulation
We deﬁne the problem of ﬁnding the Minimum Eﬀective Resistance in a Circuit
with Binary Investments (MERCBI) as follows.
We are given an electric network G∅= (N, E, g∅) where N are the nodes,
E are the edges (i.e. branches in circuit theory) and g∅: E →R+ is a function
giving the value of the original conductances that are placed on each edge. We
are also given a function gE : E →R+ of new improved conductances for each
edge. These functions are such that ∀e ∈E, g∅(e) ≤gE(e). Lastly, we are given
a set of pairs of focal nodes (i.e. core habitats) P = {⟨s1, t1⟩, ..., ⟨s|F |, t|F |⟩}, a
budget B, and a cost function c : E →R+. The pairs in P are the pairs of core
habitats between which we want to improve the resistance. It typically contains
all the possible pairs of habitats in the studied landscape, but not necessarily.
We note GA for A ⊆E the network given by GA = (N, E, gA) where gA is
a function deﬁned by g∅for all e ̸∈A and gE for all e ∈A. Similarly, RA
xy will
refer to the eﬀective resistance between x and y in GA.
Our goal is to ﬁnd a set S ⊆E such that we minimize RS = |F |
i=1 RS
siti while
keeping the sum of the investment costs below B. We say that the edges in S
are investments. The edges in E\S are wild edges. Note that it may be the case
that gS forms an open circuit (i.e. there is no way for current to go from some si
to some ti due to 0-conductance branches). In such case, RS = ∞by deﬁnition,
which can happen if there is not enough budget to ensure connectivity.
Formally, the model is translated into a Mixed Integer Program with the
following additional variables:
– be is a binary decision on whether the edge e of the circuit is part of the
selected solution S.
– v⟨s,t⟩
x
is the voltage at node x when s is connected to a 1 A source and t to
the ground.
– p⟨s,t⟩
(a,b),c is an intermediate variable for the product gS((a, b)) ∗v⟨s,t⟩
c
.
Minimize
|F |

i=1
v⟨si,ti⟩
si
(1)
s.t.
e=|E|

e=0
bec(e) ≤B
(2)
∀⟨si, ti⟩∈P,
∀x ∈N\{si, ti},

y∈adj(x)
p⟨si,ti⟩
(x,y),x −

y∈adj(x)\{ti}
p⟨si,ti⟩
(x,y),y = 0 (3)

118
D. de U˜na et al.
∀⟨si, ti⟩∈P,

y∈adj(si)
p⟨si,ti⟩
(si,y),si −

y∈adj(si)\{ti}
p⟨si,ti⟩
(si,y),y = 1
(4)
∀⟨si, ti⟩∈P,
∀e = (x, y) ∈E, ∀z ∈{x, y},
p⟨si,ti⟩
e,z
= begE(e)v⟨si,ti⟩
z
+ (1 −be)g∅(e)v⟨si,ti⟩
z
(5)
Equation 1 is our objective: minimize the sum of eﬀective resistances between
focal nodes. Equation 2 is our budget constraint. Equation 3 constrains the ﬂow
of current to be 0 in all nodes, except the nodes directly connected to the source.
Equation 4 indicates that the ﬂow at the source nodes has to be 1. Note how
the equations at the sinks with ﬂow −1 have been removed as they are linearly
dependant from the others. Equation 5 chooses the value of the p variables based
on the values of the Booleans.
In Eqs. 3 and 4, the ﬁrst sum correspond to the diagonal terms in the
Laplacian matrix, whereas the second sum is the non-diagonal terms of the
Laplacian matrix.
Furthermore, Eqs. 3, 4 and 5 are repeated for each pair of focal nodes in
P. Therefore, the equations obtained by nodal analysis are repeated C = |P|
times, one per pair in P. Nonetheless, these C systems are not equivalent, as
the source of 1 A is connected at diﬀerent nodes, and the voltages are therefore
diﬀerent in each system. That is, it is not necessarily the case that v⟨si,ti⟩
x
=
v⟨sj,tj⟩
x
, ∀i, j, x, i ̸= j. We say that our model contains C circuits.
3.1
Complexity
We now prove that the MERCBI problem is NP-hard by reduction from the
Steiner Tree Problem on graphs (STP). The STP, as formulated by Karp [16] in
his paper where he proved its NP-completeness, is: Given a graph G = (N, E),
a set R ⊆N, weighting function w on the edges and positive integer K, is there
a subtree of G weight ≤K containing the set of nodes in R?
We apply the following reduction:
– The electric circuit is the graph G.
– The cost function is w.
– The original resistance of edges is inﬁnite (i.e. g∅(e) = 0, ∀e ∈E).
– The resistance upon investment of all edges is 1 (i.e. gE(e) = 1, ∀e ∈E).
– The budget is K.
– The set of pairs of focal nodes P is the set of all pairs of distinct nodes in R,
which can be built in O(|R|2).
Assume we have an algorithm to solve the MERCBI problem that gives a
solution S. Clearly, by investing in the selected edges S we will obtain a resistance
RS ∈R iﬀthere is enough budget K, and RS = ∞otherwise:
1. If the resistance is ∞, then there is no Steiner Tree of cost less than K, since
we could not connect focal nodes with 1 Ω resistors.

Minimizing Landscape Resistance for Habitat Conservation
119
2. If the resistance is 0 then we can obtain a graph G∗by restricting GS to the
edges that have been invested. Clearly, G∗is of cost ≤K and a subgraph of
G that connects all pairs in P. Because P is the set of all pairs of nodes from
R, this means that all nodes in R are connected pairwise in G∗. Although
G∗may not be a tree, we can extract a tree T from it by breaking any cycle
G∗contains while maintaining its connectivity. The tree T is a Steiner tree
of cost at most K.
We have therefore shown that if we have a solver for the MERCBI problem,
we have one for the STP. Thus, the MERCBI problem is NP-hard.
4
Solving Approach
4.1
Greedy Algorithm
We ﬁrst devise a greedy algorithm for the problem based on the following obser-
vation: increasing the conductance of an edge with low current has little impact
in the overall eﬀective resistance. This intuition is easily justiﬁable: if the cur-
rent of an edge corresponds to the net number of times a random walker uses
that edge, the lower that number, the less impact improving that area would
have on the total commute time. Clearly edges near focal nodes or near low
resistance areas tend to concentrate more current and be used by more ran-
dom walkers. Thus increasing the conductance of those edges will likely have
a stronger impact in lowering the eﬀective resistance. Our greedy algorithm is
presented in Algorithm 1.
Algorithm 1. Greedy algorithm
1: procedure Greedy(G = (N, E), F, g∅, gE, c, B)
2:
lp ←BuildModel(G, F, g∅, gE, c, B)
▷Build the model from Sect. 3
3:
lp.setBinariesFalse(E)
▷Sets all the binaries to false
4:
lp.solve()
▷Solve the LP, with all binary variables ﬁxed
5:
i[e] ←0,
∀e ∈E
▷Array of currents
6:
for all (si, ti) ∈P do
7:
for all e = (x, y) ∈E do
8:
i[e] ←i[e] + |g∅(e) ∗(v⟨si,ti⟩
x
−v⟨si,ti⟩
y
)|
▷Ohm’s law
9:
se ←Reverse(SortBy(E, i))
▷Array of edges sorted by decreasing current
10:
S ←∅; b ←0; j ←0
11:
while b < B ∧j < |E| do
12:
if g∅(se[j]) < gE(se[j]) ∧b + c(se[j]) ≤B then
13:
S ←S ∪se[j]; b ←b + c(se[j])
▷Select edges with high current.
14:
j ←j + 1
return S
The algorithm solves the conductance LP once to ﬁnd the voltages at each
node, then calculates the currents in each edge (with no investments being made),

120
D. de U˜na et al.
and greedily selects the edges with most current to invest in, that ﬁt within the
budget.
As we will see in the experiments, this algorithm performs surprisingly well
despite not providing optimal solutions. In next sections we will try to obtain
better solutions than the ones provided by this algorithm.
4.2
Performance of the Pure MIP Model
The MIP model expressed in Sect. 3 is a direct mapping of the nodal analysis
performed in the electric circuit into a MIP model. In our implementation Eq. 5
was split into two indicator constraints which are supported by the IBM ILOG
CPLEX 12.4. solver as follows:
∀⟨si, ti⟩∈P,
∀e = (x, y) ∈E, ∀z ∈{x, y},
be =⇒p⟨si,ti⟩
e,z
= gE(e)v⟨si,ti⟩
z
(6)
¬be =⇒p⟨si,ti⟩
e,z
= g∅(e)v⟨si,ti⟩
z
(7)
Finding Bounds for the Variables. To help CPLEX tackle the problem, we
need to compute bounds on the variables. We apply basic circuit analysis to
ﬁnd these bounds. To do so, we need an initial assignment for the binary vari-
ables: this could be assigning all to false, or to the value of some initial solution
that respects the budget constraint (either a random solution, or obtained with
Algorithm 1). Without loss of generality, let us assume that all the binaries are
set to false, thus g∅gives the conductance for all edges.
t
s
1A
≡
t
INo
s
1A
Rst
≡
t
VT h
s
Rst
Fig. 2. Conversion from the circuit built for calculating eﬀective resistance to an equiv-
alent circuit with one resistor, and application of Th´evenin’s theorem.
When we are computing the eﬀective resistance between nodes s and t of a
circuit, we connect a current source between s and t as in Fig. 2. This can be
converted into an equivalent circuit as seen in the center of the ﬁgure. The value
of Rst is actually the value of the eﬀective resistance when all our binary variables
are ﬁxed. To obtain bounds for the voltages at the nodes marked •, we observe
that the circuit in the center is a Norton circuit, thus we can apply Th´evenin’s
theorem [8] to it to obtain an equivalent circuit with a voltage source instead of a

Minimizing Landscape Resistance for Habitat Conservation
121
current source (circuit on the right). Because the current in that Norton’s circuit
is 1A, the voltage of Thevenin’s equivalent source will be VT h = INoRst = Rst.
Thus, the upper bound of vs is Rst. Since t is connected to the ground, vt = 0V .
All the voltage at points between s and t are necessarily bounded by the voltages
at these points, because voltage can only drop. Therefore, the bounds for all
voltage variables are: ∀⟨si, ti⟩∈P, ∀x ∈N,
0 ≤v⟨si,ti⟩
x
≤R∅
siti. From these
bounds it is easy to derive bounds for the p variables: ∀⟨si, ti⟩∈P,
∀e =
(x, y) ∈E, ∀z ∈{x, y},
g∅(e)v⟨si,ti⟩
z
≤p⟨si,ti⟩
e,z
≤gE(e)v⟨si,ti⟩
z
.
Motivation for Local Search Approaches. We attempted to solve the MIP
model using IBM ILOG CPLEX 12.4. The major challenge we discovered while
running our experiments was that the linear relaxation of the model performed
poorly. The gap between the LP relaxation and the best integer solution found
in less than 5 hours was usually bigger than 40% even for small 10 × 10 grid-like
networks, as reported by CPLEX. As a small example, consider a 3 × 3 grid
with ∀e, g∅(e) = 0.01 and gE(e) = 1, a budget of 2 and 2 core habitats. The
proven optimal solution gives a resistance of 49.15, whereas the linear relaxation
achieves a resistance as low as 0.50. With such a big gap, the linear relaxation
cannot help pruning branches. We observed these gaps both when using indicator
constraints and without them (using an alternative encoding of the product of
continuous and binary variables as linear inequalities).
Since the linear relaxation is so weak, we decided to try a Local Search
approach.
4.3
Local Search
Although solving the MIP problem is time consuming, once all binary variables
are ﬁxed, the problem becomes a Linear Program, which can be solved eﬃciently.
The idea of local search is to repeatedly destroy part of the solution and build
a new one from the remaining solution.
Our Local Search (LS) moves are based on two stages: ﬁrst choose areas
where we invested but we are no longer interested in investing, then choose new
areas where we would like to invest. In this section we will discuss diﬀerent
techniques we use to make moves in the LS.
Destroying Investments. We start from a solution s where we have selected
a set S of edges to be investments. We call d the destruction rate of investments.
To destroy investments, we select a subset Sd of S such that |Sd| = d. We
will convert these d investments into wild edges, thus freeing part of the budget
to be used elsewhere. We implemented 3 ways of selecting those d edges:
1. InvRand: Choose d invested edges randomly.
2. InvLC: Choose the d invested edges with lowest current in s.
3. InvLCP: Choose d invested edges based on a probability distribution that
favors low current edges in s being selected.

122
D. de U˜na et al.
The way to compute the current of an edge is by simply using Ohm’s law, as
in line 8 of Algorithm 1. Because we are solving C linear systems for C circuits
at once, the current across an edge will be the sum of the currents of that edge
in the C diﬀerent circuits.
As for the probability distribution, the total current across an edge is
proportional to the inverse of its probability. Therefore edges with low cur-
rent will have a higher tendency to be chosen than edges with high current:
∀e = (x, y) ∈E,
Pr(e) ∝
 
⟨si,ti⟩∈P |gS(e) ∗(v⟨si,ti⟩
x
−v⟨si,ti⟩
y
)|
	−1.
Making New Investments. Let s′ be the solution s with investments S\Sd,
that is all investments of s except those selected for destruction. After destroying
investments Sd, we have gotten part b = Σe∈Sdc(e) of our budget back, so we
are ready to choose new places to invest. To do so, we have 4 diﬀerent strategies
to choose new edges to invest in:
1. WilRand: Choose a set W of wild edges randomly.
2. WilBFS: Choose a set W of wild edges by doing a Breath-First Search (BFS)
in the graph. The origin of the BFS is chosen according to a probability
distribution that favors high current nodes. This ensures that the chosen
edges are close to each other.
3. WilHC: Choose a set W of wild edges that have the highest current in s.
4. WilHCP: Choose a set W of wild edges based on a probability distribution
that favors high-current wild edges.
For all these strategies, we ensure that 
e∈W c(e) = b−ϵ and we select those
edges greedily to get the smallest slack ϵ.
The second strategy, WilBFS, requires computing the current at a node.
We deﬁne it as cx = 
(si,ti)∈P
1
2

y∈adj(x) |gS((x, y)) ∗(v⟨si,ti⟩
x
−v⟨si,ti⟩
y
)|. That
is, the sum of the accumulated current of the surrounding edges of the node.
As for the probability distributions used by both WilBFS and WilHCP, we
could not use a distribution as simple as for InvLCP. The reason is that, even
though the probability of choosing a particular node (or edge) of high current
is much higher than the probability of choosing particular node (or edge) of low
current, because there are many more nodes and edges with low current, the
probability of choosing a low current node or edge would be much higher.
To overcome this, we will only allow the choice of nodes or edges that have
a current higher than 10% of the current of the highest node or edge, plus
a certain number γ of low current elements (i.e. below that 10%). We choose
γ to be equal to the number of nodes or edges above 10% of the maximum
current. The probability of choosing an element with a current below 10% of
the maximum is now the same as the probability of choosing an element with a
current above that threshold.
Initial Solution. Local Search needs to start with an initial solution. Our
approach was to take the output of the greedy algorithm as the initial solution
for the LS.

Minimizing Landscape Resistance for Habitat Conservation
123
Using Simulated Annealing. As it is well known, Local Search can get stuck
into a local minimum if we are only accepting improving solutions. Therefore,
we implement the simulated annealing approach introduced by Ropke et al. for
the Pickup and Delivery Problem with Time Windows [23].
Here the idea is that we will always have 3 solutions at hand: sbest (for a set
of invested edges Sbest), s, and snew (for Snew). The ﬁrst is the solution found
so far with lowest resistance. The second is the current solution maintained
in the search (called accepted in [23]). The last, is the new solution which we
obtained by modifying the solution s using the destruction and investment phases
described earlier. If we accept a solution snew only when it is better than the
current solution we might get trapped in a local minimum. With simulated
annealing, we accept a solution snew with probability e−(snew−s)/T , where T is
the temperature. The temperature decreases at each iteration by a factor called
the cooling rate: Ti+1 = cr ∗Ti. This allows us to accept solutions that are worse
than our currently accepted solution, thus allowing us to get out of local minima.
Initially we will have a higher tendency to accept worsening solutions, and as
the temperature drops slowly over the iterations, we will only accept improving
solutions. We discuss the value of the initial temperature in Sect. 5.
Even using simulated annealing, it is easy to see how the combined usage of
InvLC and WilHC can perform the exact same destruction over and over once
it hits a local minimum. To avoid that, when we are using these two destruction
techniques together and we don’t accept a solution, we switch to WilHCP
for one iteration, which will put us in another neighborhood to explore (like a
restart).
5
Experiments
All the following experiments use IBM ILOG CPLEX 12.4 on an Intel R
⃝Core
TM
i7-4770 CPU machine @ 3.40 GHz with 15.6 GB of RAM running Linux 3.16.
5.1
Instances
We could not obtain real-world instances. Nonetheless we generated artiﬁcial
landscapes while trying to keep them realistic. In papers like [3,4] where the
IBR model is used on empirical data, high resistance areas have an approximate
resistance between 10 and 100 times bigger than areas with low resistance.
Our instances were built in the form of grids with 4 neighbor nodes (except
at the border of the landscape where nodes have 2 or 3 neighbors). The values
of conductances (i.e. inverse of resistances) are chosen following a probabilistic
beta-distribution with parameters α = 5 and β = 80 (Fig. 3).
Using this distribution, most conductances are in the desired range, still
allowing the unlikely possibility of having some high conductances scattered on
the map. Then, for each instance of size n×n we select n nodes, and their neigh-
bors, to be oases. All the edges coming out of oasis nodes have a conductance of
1, that is the same as if we had invested. This is to account for areas that are not

124
D. de U˜na et al.
0
0.2
0.4
0.6
0.8
1
0
5
10
15
α = 5,
β = 80
Fig. 3. Plot of beta distribution for α = 5, β = 80.
considered core habitats but are friendly to the animals: wild trees, fresh water
lakes, etc. Figure 1 shows, on the right, an example of such map in a 50×50 grid
(the scale being in Ω−1).
Regarding the budget cost, we created instances with homogeneous costs
(labelled Homo) where each edge costs 1. The reason for homogeneous costs is
that the main application for our problem is in wild land and speciﬁcally on
National Parks, where rehabilitation costs are typically fairly uniform across a
large area. Typically these kind of projects are used to recover land that has
been damaged by bush ﬁres, ﬂooding or landslides or that has been fragmented
by urbanization that cannot be torn down or purchased. We also constructed
instances where the cost of an investments is a uniform random number between
0.1 and 10 (labeled Rand).
Furthermore, the locations of core habitats are selected from a uniform dis-
tribution. For each size of grid and number of habitats, we generated 20 diﬀerent
instances, totalling 500 instances. Values reported are arithmetic means.
5.2
Improvement over the Initial Landscape
To measure the quality of a solution, we look at the ratio between the eﬀective
resistance in the solution and the original resistance when no investment is made.
Table 1 shows the average of these ratios. All the results reported on that table
had a budget equal to twice the side of a grid (e.g., for a grid of 50 × 50, we have
a budget of 100). The ratio of the budget to the number of edges where we could
invest is shown in the third column. The destruction rate here is ﬁxed to 10 for
instances of size 20 × 20 and 25 × 25, and 25 for bigger instances. The number of
iterations the LS does for these tests is 200. We do not show the results obtained
by random selections for destruction as they perform worse than the ones shown
here. The initial temperature is chosen such that a solution 50% worse than the
initial solution is accepted with a probability of 10%. The cooling rate is 0.98
for all experiments.
Our ﬁrst observation is that even with a very low budget, proportionally to
the mass of land, we manage to have signiﬁcant drops of resistance. We infer that
our approach is placing the investments in the right places. The greedy algorithm

Minimizing Landscape Resistance for Habitat Conservation
125
Table 1. Averages of the ratios of updated resistance over original resistance. Budgets
are always twice the side of a grid.
Cost type Size
Budget
ratio
Habitats Greedy InvLC
InvLCP
WilBFS WilHC WilHCP WilBFS WilHC WilHCP
Homo
20 × 20
0.11
2
0.30
0.24
0.25
0.25
0.29
0.26
0.27
Homo
25 × 25
0.09
2
0.30
0.22
0.23
0.24
0.28
0.24
0.26
Homo
25 × 25
0.09
3
0.38
0.29
0.30
0.29
0.37
0.31
0.35
Homo
30 × 30
0.07
2
0.37
0.26
0.27
0.28
0.35
0.29
0.31
Homo
30 × 30
0.07
3
0.42
0.33
0.33
0.33
0.42
0.35
0.41
Homo
50 × 50
0.04
2
0.44
0.34
0.35
0.36
0.44
0.37
0.40
Homo
50 × 50
0.04
3
0.48
0.42
0.40
0.41
0.48
0.43
0.48
Homo
50 × 50
0.04
4
0.52
0.46
0.44
0.45
0.52
r0.47
0.52
Homo
100 × 100 0.02
2
0.49
0.39
0.38
0.41
0.42
0.39
0.37
Homo
100 × 100 0.02
3
0.53
0.42
0.43
0.42
0.53
0.42
0.50
Homo
100 × 100 0.02
4
0.55
0.49
0.47
0.46
0.55
0.49
0.55
Homo
100 × 100 0.02
5
0.58
0.49
0.46
0.50
0.58
0.48
0.58
Rand
20 × 20
0.11
2
0.44
0.34
0.34
0.38
0.39
0.35
0.40
Rand
25 × 25
0.09
2
0.45
0.33
0.32
0.36
0.39
0.33
0.38
Rand
25 × 25
0.09
3
0.53
0.41
0.39
0.38
0.47
0.40
0.44
Rand
30 × 30
0.07
2
0.52
0.45
0.42
0.44
0.51
0.45
0.48
Rand
30 × 30
0.07
3
0.57
0.53
0.51
0.52
0.57
0.52
0.56
Rand
50 × 50
0.04
2
0.56
0.54
0.51
0.53
0.56
0.53
0.56
Rand
50 × 50
0.04
3
0.60
0.59
0.55
0.58
0.60
0.58
0.60
Rand
50 × 50
0.04
4
0.63
0.62
0.60
0.63
0.63
0.62
0.63
Rand
100 × 100 0.02
2
0.61
0.51
0.53
0.52
0.66
0.61
0.65
Rand
100 × 100 0.02
3
0.64
0.59
0.57
0.60
0.63
0.60
0.64
Rand
100 × 100 0.02
4
0.65
0.58
0.60
0.62
0.62
0.61
0.64
Rand
100 × 100 0.02
5
0.68
0.60
0.59
0.62
0.63
0.62
0.65
performs very well too. The Local Search is able to reduce the resistance of the
greedy algorithm by up to 11%.
We can observe how in general the InvLC approach to destroy bad invest-
ments outperforms InvLCP. Regarding the selection of new investment locations
it seems there is no clear winner, although WilBFS and WilHC tend to choose
investments in places nearby or directly connected, and that seems to give them
the advantage.
As an example, Fig. 4 shows the solution found to the instance of Fig. 1. On
the left, the heatmap shows the places of investment (marked lines) which redi-
rect most of the animals, as these become more suitable for them. On the right,
we see how the best investments tend to be aligned to connect low resistance
areas that already existed. Notice how they do not force a complete corridor:
some investments may not be used to maintain connectivity and are instead
moved to areas where they might be more useful.

126
D. de U˜na et al.
Fig. 4. Example of a solution obtained with InvLC+WilHC for the same instance as
shown in Fig. 1
5.3
Optimality Gap
In this section we look at the diﬀerence between the solution obtained and the
proven optimum for some instances. We could not obtain the optimal value for
instances of the size given in the previous subsection. For this reason, we ran
this experiment on 10 × 10 grids. These took, in some cases, up to 8 h to prove
optimality, even given the small size. It is clearly impractical to use the pure
MIP formulation.
Table 2. Average (across 20 instances) of ratio between LS and proven optimum for
10 × 10 instances (200 iterations)
InvLC
InvLCP
WilBFS WilHC WilHCP WilBFS WilHC WilHCP
Average 1.06
1.07
1.07
1.11
1.05
1.07
As it can be seen in Table 2, the LS approach does not deviate much from
the optimum. Once again we see no clear domination between the diﬀerent tech-
niques applied, although InvLCP+WilBFS seems slightly worse than others.
The LS even managed to ﬁnd optimal solution of two instances.
5.4
Number of Iterations
We now study the eﬀect of the number of iterations. We are interested to know
how quickly we get improvements in our solutions. To evaluate this, we looked
at the 50 × 50 instances with homogeneous investment cost and a budget of 100

Minimizing Landscape Resistance for Habitat Conservation
127
(same instances as in Table 1). We compute the ratio between the best solution
found so far at the xth iteration to the non-investment resistance. We average
this ratio across the 60 Homo 50 × 50 instances (2, 3 and 4 habitats) for each
iteration. The results are plotted in Fig. 5. Based on the results seen in Table 1
and this plot, we can see that InvLCP+WilBFS and InvLCP+WilHCP are
not only the ones that perform the worse, but we also only ﬁnd better solutions
very rarely. The InvLC+WilHC shows the quickest drop, but then slows down.
Since the drop happens in the ﬁrst 20 iterations, the temperature is still high,
thus suggesting that it does not get trapped in a local minimum, instead it is
most likely moving towards the global optimum (as we also saw on the optimality
gap results). All other techniques have a slower slope.
0
20
40
60
80
100
120
140
160
180
200
0.39
0.4
0.41
0.42
0.43
0.44
0.45
0.46
0.47
0.48
Iterations
BestSolution/InitialMap
InvLCP+WilBFS
InvLCP+WilHCP
InvLCP+WilHC
InvLC+WilBFS
InvLC+WilHCP
InvLC+WilHC
Fig. 5. Ratio between the best solution found and the value of the initial resistance
over the number of iterations for 6 destruction strategies (50 × 50 Homo grids, 2 to 4
habitats).
Regarding the time spent to perform these 200 iterations: no instance needed
more than 5 min in our machine. Therefore this number of iterations is also
suitable for use by habitat planners during their work.
5.5
Destruction Rate
In this subsection we will look at the eﬀects of the choice of destruction rate.
Recall, the destruction rate corresponds to how many edges are converted from
investments into wild edges for the next iteration of the LS. We ran these exper-
iments on the 50 × 50 instances for 2, 3 and 4 habitats with a budget of 100 and
homogeneous costs. Table 3 reports the average ratio between our solution and
the original resistance across the 60 instances depending on the destruction rate.
The number of iterations is once again 200.
As we can see in Table 3, the choice of destruction rate only marginally aﬀects
the quality of the solution. This shows that our local search approach is robust
and no matter what parameter is used (within a reasonable range), the algorithm
is likely to give similarly good solutions.

128
D. de U˜na et al.
Table 3. Ratio of solution found over initial landscape resistance for 50×50 grids with
diﬀerent destruction rates, over 200 iterations.
Destruction rate InvLC
InvLCP
WilBFS WilHC WilHCP WilBFS WilHC WilHCP
5
0.40
0.38
0.40
0.47
0.39
0.45
10
0.40
0.38
0.39
0.48
0.39
0.46
15
0.40
0.39
0.39
0.48
0.40
0.46
20
0.40
0.39
0.40
0.48
0.41
0.47
25
0.41
0.39
0.41
0.48
0.42
0.47
30
0.42
0.40
0.41
0.48
0.43
0.47
35
0.43
0.40
0.42
0.48
0.44
0.47
6
Conclusions and Future Work
In this paper we have introduced a new problem, the MERCBI problem, in
habitat conservation. We have clearly deﬁned it and given a MIP model. We
have provided a series of heuristics used to implement a Local Search. The results
show that this approach yields good quality solutions.
In future work we would like to investigate the idea of extending patches of
land to more than a node: a golf course, or a farm could cover more than one node
at a time and then we might consider purchasing part of them. Also, we could
take into account distance for patches with diﬀerent shapes where purchasing a
thin part of it may be more interesting than purchasing the entire land. Also, we
could look at having more than one species and take into account their predatory
behavior or possibly favor endangered species.
Acknowledgements. We would like to thank Julian Di Stefano and Holly Sitters
from the School of Ecosystems and Forest Sciences at the University of Melbourne as
well as Nevil Amos from the Department of Environment, Land, Water and Planning
of Victoria for meeting with us and introducing us to this problem.
References
1. Adriaensen, F., Chardon, J., De Blust, G., Swinnen, E., Villalba, S., Gulinck, H.,
Matthysen, E.: The application of ‘least-cost’ modelling as a functional landscape
model. Landscape Urban Plann. 64(4), 233–247 (2003)
2. Alexander, C.K., Sadiku, M.N.: Electric circuits (2000)
3. Amos, J.N., Bennett, A.F., Mac Nally, R., Newell, G., Pavlova, A., Radford,
J.Q., Thomson, J.R., White, M., Sunnucks, P.: Predicting landscape-genetic con-
sequences of habitat loss, fragmentation and mobility for multiple species of
woodland birds. PLoS One 7, 1–12 (2012)
4. Amos, J.N., Harrisson, K.A., Radford, J.Q., White, M., Newell, G., Nally, R.M.,
Sunnucks, P., Pavlova, A.: Species- and sex-speciﬁc connectivity eﬀects of habitat
fragmentation in a suite of woodland birds. Ecology 95(6), 1556–1568 (2014)

Minimizing Landscape Resistance for Habitat Conservation
129
5. Balabanian, N., Bickart, T.A., Seshu, S.: Electrical Network Theory. Wiley,
New York (1969)
6. Beier, P., Majka, D.R., Spencer, W.D.: Forks in the road: choices in proce-
dures for designing wildland linkages. Conserv. Biol. 22(4), 836–851 (2008).
http://dx.doi.org/10.1111/j.1523-1739.2008.00942.x
7. Beier, P., Noss, R.F.: Do habitat corridors provide connectivity? Conserv. Biol.
12(6), 1241–1252 (1998). http://dx.doi.org/10.1111/j.1523-1739.1998.98036.x
8. Brittain, J.E.: Thevenin’s theorem. IEEE Spectr. 27(3), 42 (1990)
9. Crossman, N.D., Bryan, B.A.: Systematic landscape restoration using integer pro-
gramming. Biol. Conserv. 128(3), 369–383 (2006)
10. Dilkina, B., Gomes, C.P.: Solving connected subgraph problems in wildlife conser-
vation. In: Lodi, A., Milano, M., Toth, P. (eds.) CPAIOR 2010. LNCS, vol. 6140,
pp. 102–116. Springer, Heidelberg (2010). doi:10.1007/978-3-642-13520-0 14
11. Dorf, R.C., Svoboda, J.A.: Introduction to Electric Circuits. Wiley, New York
(2010)
12. Doyle, P.G., Snell, J.L.: Random Walks and Electric Networks. Mathematical Asso-
ciation of America, Washington, D.C. (1984)
13. Ghosh, A., Boyd, S., Saberi, A.: Minimizing eﬀective resistance of a graph. SIAM
Rev. 50(1), 37–66 (2008)
14. Gomes, C.P.: Computational sustainability: computational methods for a sustain-
able environment, economy, and society. Bridge 39(4), 5–13 (2009)
15. Aars,
J.:
R.A.I.:
The
eﬀect
of
habitat
corridors
on
rates
of
transfer
and
interbreeding
between
vole
demes.
Ecology
80(5),
1648–1655
(1999).
http://www.jstor.org/stable/176553
16. Karp, R.M.: Reducibility among combinatorial problems. In: Miller, R.E.,
Thatcher, J.W., Bohlinger, J.D. (eds.) Complexity of Computer Computations.
The IBM Research Symposia Series, pp. 85–103. Springer, US (1972)
17. LeBras, R., Dilkina, B.N., Xue, Y., Gomes, C.P., McKelvey, K.S., Schwartz, M.K.,
Montgomery, C.A., et al.: Robust network design for multispecies conservation. In:
AAAI (2013)
18. Lov´asz, L.: Random walks on graphs. Combinatorics, Paul erdos is eighty 2, 1–46
(1993)
19. McRae,
B.H.:
Isolation
by
resistance.
Evolution
60(8),
1551–1561
(2006).
http://dx.doi.org/10.1111/j.0014-3820.2006.tb00500.x
20. McRae,
B.H.,
Beier,
P.:
Circuit
theory
predicts
gene
ﬂow
in
plant
and
animal
populations.
Proc.
Nat.
Acad.
Sci.
104(50),
19885–19890
(2007).
http://www.pnas.org/content/104/50/19885.abstract
21. McRae, B.H., Dickson, B.G., Keitt, T.H., Shah, V.B.: Using circuit theory to model
connectivity in ecology, evolution, and conservation. Ecology 89(10), 2712–2724
(2008). http://dx.doi.org/10.1890/07-1861.1
22. Pimm, S.L., Jones, H.L., Diamond, J.: On the risk of extinction. Am. Nat. 132(6),
757–785 (1988). http://dx.doi.org/10.1086/284889
23. Ropke, S., Pisinger, D.: An adaptive large neighborhood search heuristic for the
pickup and delivery problem with time windows. Transp. Sci. 40(4), 455–472
(2006). http://dx.doi.org/10.1287/trsc.1050.0135
24. Rosenberg, D.K., Noon, B.R., Meslow, E.C.: Biological corridors: form, function,
and eﬃcacy. BioScience 47(10), 677–687 (1997). http://bioscience.oxfordjournals.
org/content/47/10/677.short
25. Shah, V., McRae, B.: Circuitscape: a tool for landscape ecology. In: Proceedings
of the 7th Python in Science Conference, vol. 7, pp. 62–66 (2008)

130
D. de U˜na et al.
26. Urli, T., Brot´ankov´a, J., Kilby, P., Van Hentenryck, P.: Intelligent habitat restora-
tion under uncertainty. In: Thirtieth AAAI Conference on Artiﬁcial Intelligence
(2016)
27. Williams, J.C.: Delineating protected wildlife corridors with multi-objective pro-
gramming. Environ. Model. Assess. 3(1–2), 77–86 (1998)

A Hybrid Approach for Stator Winding
Design Optimization
Alessandro Zanarini(B) and Jan Poland
ABB Corporate Research Center, Baden-D¨attwil, Switzerland
{alessandro.zanarini,jan.poland}@ch.abb.com
Abstract. Large electrical machines (e.g. hydro generators or motors
for mining applications) are typically designed and built speciﬁcally for
their site of installation. Engineering these one-of-a-kind designs requires
a considerable amount of time and eﬀort, with no guarantees that the
ﬁnal blueprint is cost-optimal. One part of the design involves the stator
windings, i.e. the static part of an electric motor generating an electro-
magnetical ﬁeld. In order to achieve low harmonic losses during oper-
ation, fractional windings are preferred, where the number of slots per
phase in the stator is not a multiple of the number of poles. This renders
the layout of the stator winding a challenging combinatorial optimiza-
tion problem. The paper proposes a decomposition approach in which the
problem can be reformulated as a rich variant of the Travelling Salesman
Problem in which both MIP and CP are employed to tackle the problem.
1
Introduction
Three-phase electrical rotating machines used in diﬀerent industries are com-
posed by a non-moving part called stator, and a moving part known as rotor.
The stator has a cylindrical shape where the curved surface presents a set of
parallel bars running along the cylindrical surface in axial direction that con-
duct alternating electrical current in order to generate a magnetic ﬁeld capable
of moving the rotor. The rotor is also cylindrical in shape and concentric to
the stator and it rotates thanks to a number of magnetic poles that are sub-
ject to the magnetic ﬁeld generated by the stator. An example of such machines
are mills employed in the mining industry for crushing rocks from which, after
some intermediate steps, material is extracted. The conducting bars and their
connections (windings) made of copper contribute to the ﬁnal material cost for
constructing a stator. Depending on the stator design and machine dimensions,
the number of bars ranges from few dozens to few hundreds. As the number of
bars grows, the combinatorial complexity of the stator design (how to connect
the diﬀerent bars) becomes challenging for the engineer, leading to several hours
spent in the design with no guarantee of cost-optimality of the ﬁnal blueprint.
This paper formalizes the problem of designing the stator windings subject
to the electro-mechanical constraints with the objective of minimizing the mate-
rial cost. We show how this problem can be formulated as a rich variant of the
well-known Travelling Salesman Problem (TSP). It can be eﬃciently solved with
c
⃝Springer International Publishing AG 2017
D. Salvagnin and M. Lombardi (Eds.): CPAIOR 2017, LNCS 10335, pp. 131–146, 2017.
DOI: 10.1007/978-3-319-59776-8 11

132
A. Zanarini and J. Poland
Mixed Integer Programming and Constraint Programming. The solution devel-
oped has been successfully employed internally for the design of stator windings
leading both to savings in term of engineering labor and material costs.
The paper is organized as follows: Sect. 2 presents the literature background;
Sect. 3 formalizes the problem; Sect. 4 proposes a solution; experiments are
reported in Sect. 5. Conclusions are drawn in the last section.
2
Background
The three phases will be referred to as U, V , and W. Each stator bar is located
in a slot and each bar is assigned to a speciﬁc phase. The number of bars per
pole and phase is:
q = ns
3np
,
(1)
where ns is the number of slots, np is the number of poles, and we restrict to
three phase machines to simplify the presentation (note that our methods easily
generalize to arbitrarily many phases).
If q is integral, the design is referred to as integral-slot winding. If q is frac-
tional, the design is instead known as fractional-slot winding. Fractional-slot
windings are known to reduce the unwanted high-frequency harmonics in the
electromagnetical forces and give greater ﬂexibility to the designer in choosing
the number of slots to reach the target magnetic ﬂux density [1]. Each slot may
host a single bar (single-layer winding), two bars (two-layers winding) or more.
Two-layer windings have the advantages of easing the production and assembly
and also to reduce unwanted magnetomotive force harmonics [2]. In this paper,
we will focus on two-layer fractional-slot windings. In two-layer windings, the
two bars located in a slot are often referred to as top and bottom bars.
In Fig. 1, a typical stator winding blueprint is shown. In the example, the
stator presents 96 slots (numbered from 0 to 95), each one having two bars.
For simplicity, the drawing only shows phase U (bars 0, 1, 2 then 7, 8, 9 then
14, 15 and so on). The solid and dashed radial lines grouped by pair represent
respectively the top and bottom bars of each slot; Fig. 1b highlights the top and
bottom bars placed in slot 0. The number of poles is 14, therefore q = 2 + 2
7.
The connections drawn in the inner circle are located on one side of the stator,
and the connection drawn in the outer circle are located on the opposite side.
The former presents a regular structure and in the following it will be referred to
as the no jumper side, whereas the latter shows some regular patterns interleaved
by some long jumpers (referred to as jumper side). Figure 1a shows the jumper
and no jumper sides; examples of jumpers can be seen in the top left side or in
the bottom right side. Figure 1b highlights a coil (in the top right part of the
stator): a coil is a pair of top and bottom bars linked via a regular connection
in the no jumper side. The highlighted coil is formed by a top bar in slot 77 and
a bottom bar in slot 83 (referred in the following to as [XX]T([YY]B), in the
speciﬁc example 77T(83B)). In a conventional two layer winding, each top bar

A Hybrid Approach for Stator Winding Design Optimization
133
(a)
(b)
no jumper side
jumper side
jumper side
jumper side
jumper side
coil 77T(83B)
top bar phase U in slot 0
y1
y2/2
bottom bar phase U in slot 0
(c)
(d)
Direction of current
Fig. 1. Stator winding drawing of phase U: (a) no jumper side and jumper side are
shown; (b) top and bottom bars, coil and coil pitch are highlighted; (c) direction of
current for a sub-part of phase U; (d) current direction for all the bars of phase U
is connected with a bottom bar y1 slots away via the no jumper side, forming as
many coils as there are slots. The number y1 is referred to as coil pitch.
On the jumper side, there is more ﬂexibility. Basically, there are two diﬀer-
ent types of winding. In lap winding, adjacent coils are always connected. This
is easy to construct, but needs a large number of jumpers, in particular for frac-
tional windings. In this paper, we focus on wave winding, where each coil by
default connects to a coil of the subsequent group of coils. This gives rise to the
opportunity of solving a complex routing problem and thus signiﬁcantly reduc-
ing the winding length. Figure 1b shows a wave winding with coil pitch y1 = 6.
The complementary coil pitch on the jumper side is chosen as y2 = 8.
On the right side of Fig. 1c, two dangling connections are shown: these rep-
resent the plug to the external electrical power. The ﬁgure shows how the cur-
rent ﬂows starting from the external connector and continues clock-wise around
the stator. Only one full round is highlighted, however by following the overall

134
A. Zanarini and J. Poland
thread, starting from one power plug and ending in the other, it may be observed
that the overall connection goes a bit more than two full rounds clockwise, and
then it reverses direction (also referred to as ﬂip in the following) and it goes
counter-clockwise for about the same length (the jumper that causes this inver-
sion is shown in the bottom right part in dashed line). This is linked to the
fractional-winding nature of this design, q = 2 + 2
7. We will describe more in
details in the following sections the relation. As the alternating current ﬂows
around the stator, half of the bars (both top and bottom) will be traversed by
the current inwards, and the other half outwards. Figure 1d shows how the top
bars of phase U are partitioned in the two directions.
Previous work has focused on the optimal choice of ns, np, number of lay-
ers and assignment of phases to bars with the aim of optimizing torque den-
sity, torque ripple, losses or unwanted harmonics (see [3–5]). To the best of the
authors’ knowledge, no previous scientiﬁc literature looked at the minimization
of the material cost once the main design parameters of the machine have been
decided.
3
Problem Description
In this section, we describe the three main components for an optimal design of
a stator winding. At ﬁrst each coil (i.e. each top and bottom bar) needs to be
assigned to a phase and direction: this will deﬁne the electrical layout (Sect. 3.1).
Secondly, the bars of each phase needs to be linked together in order to form
a continuous thread (or route) going around the stator (Sect. 3.2). Finally, once
the three routes have been deﬁned, jumpers need to be selected to avoid any
conﬂict between the connections (Sect. 3.3).
3.1
Bar Assignment
Assigning the top and bottom bars to phases deﬁnes the main electrical prop-
erties of the machine, hence this step in the design is referred to as electrical
layout design. We restrict our focus to symmetric layouts between the phases,
i.e. where phase V and phase W follow rotated layouts of phase U. Moreover,
for a conﬁguration with q = a + b
c (with 0 < b < c), we restrict to layouts where
phase U is distributed to (c −b) many blocks of a contiguous slots and b many
blocks of (a + 1) contiguous slots. E.g., in Fig. 1, focusing on the top bars, we
will note the contiguous bars forming blocks of 3 and 2, respectively; starting
from slot 0, the pattern is “3 3 2 2 2 2 2”, that is, the bars belonging to phase U
are grouped as follows: 3 bars (slots 0,1,2) then another group of 3 (slots 7,8,9)
followed by 5 groups of 2 bars (14,15 then 21,22, and so on).
Symmetric layouts between the phases are constructed by rotating the layout
of phase U by c blocks and 2c blocks, respectively. This is illustrated in the
Table 1 for the example above.
The table is ﬁlled by ﬁrst ﬁlling phase U with the given pattern and
then traversing the table column by column, counting the number of elements

A Hybrid Approach for Stator Winding Design Optimization
135
Table 1. Illustration of the construction algorithm for an electrical layout which is
symmetric between the phases.
Phase U
3 3 2 2 2 2 2
Phase V
2 2 3 3 2 2 2
Phase W
2 2 2 2 3 3 2
(i.e. blocks) traversed and ﬁlling the pattern to phase V and W after c and
2c blocks have been traversed, respectively. This is illustrated by the bold face
entries which indicate starts of the pattern. Also, from this it is clear that c must
not be a multiple of 3, since otherwise after traversing c blocks, we would end
up again in phase U and not V or W.
The following combinatorial arguments answer the question of how many
possible electrical layouts exist modulo ﬂipping and rotation. The layout where
all (a + 1)-blocks are next to each other is referred to as the default electrical
layout. It often minimizes the number of jumpers needed for a wave winding and
therefore is a good candidate. If b = 1 or b = c −1, there is (modulo rotation)
only the default electrical layout. In all other fractional windings, there are more
electrical layouts.
In order to count these layouts, we distinguish between ﬂip-symmetric and
ﬂip-asymmetric ones. The ﬂip-symmetric layouts are those for which some ﬂip
exists that preserves the pattern. For example, in the layout above, “3 3 2 2 2 2
2”, is ﬂip symmetric, which becomes clear when we write it as “3 2 2 2 2 2 3”.
We can count the number of ﬂip-symmetric layouts: For odd c (as in our
example), it is
nsym =
 1
2(c −1)
⌊1
2b⌋

,
since the center block is here of length a + 1 if b is odd and length a if b is even,
the blocks right of the center are assigned freely, and the blocks left of the center
are ﬂipped copies. For even c, similar arguments show that
nsym =
⎧
⎨
⎩
c/2
b/2

for even b,
 c/2−1
(b−1)/2

for odd b.
Finally,
1
c
c
b

= 2nasym + nsym counts the asymmetric layouts twice and the
symmetric ones once (the factor 1
c accounts for rotational symmetry). Hence,
the total number of electrical layouts is
1
2
	
1
c
c
b

+ nsym

.
Once the electrical layout is ﬁxed, the coils belonging to phases U, V and W
are deﬁned and referred to as CU, CV , and CW , with CU = {cU
k , k = 1, . . . , 1
3ns}
etc., and cU
k ∈{0 . . . ns −1} being the slot number of the corresponding top bar.
We will also use the sign of the electrical current in each coil σU
k ∈{1, −1}.

136
A. Zanarini and J. Poland
Harmonics. The electrical layout has to be chosen in order to result in a large
fundamental harmonic and at the same time small higher order harmonics of the
magnetomotive force (MMF). For phase U (and by rotational symmetry, also the
other phases) and harmonic order h = 1, 2, . . ., the normalized harmonic content
is given by the absolute of a sum of complex vectors:
1
2
3ns

ns/3

n=1
σU
k exp
	
iπh · np
ns cU
k

−σU
k exp
	
iπh · np
ns (cU
k + y1)


,
where i = √−1. (The ﬁrst exp corresponds to the top bars, the second exp to
the bottom bars.)
Multiple Branches. Not every three phase electrical machine has three wind-
ings on the stator, some machines have windings with parallel branches. Dis-
tributing the bars of a phase to parallel branches adds one more criterion to the
optimization, namely the unbalanced magnetic pull in case of rotor eccentricities.
Optimizing multiple branches can be tackled with our methods, but we restrict
the presentation to single branch layouts in the following.
3.2
Coil Routing
Once each bar has been assigned to a phase and sign of the current, the problem
is then to connect1 all the coils of the same phase together while respecting the
direction their bars have been assigned to. The minimization of the total length
of one phase connection leads to reduced electrical losses and reduced material
consumption hence translating to a direct cost beneﬁt.
In the following, we will focus the description on phase U, analogous obser-
vations can be drawn about the other phases. By construction each coil cU
k is
formed by a top bar and a bottom bar; connecting the top bar (resp. bottom
bar) of cU
k to the bottom bar (resp. top bar) of another coil cU
j will keep the sign
of the current. Conversely, whenever the top bar (resp. bottom bar) of a coil cU
k
will be connected to the top bar (resp. bottom bar) of a coil cU
j , the sign of the
current will be reversed. A change in sign is referred to as a ﬂip. Depending on
the design and manufacturing constraints, the engineer may want to force only
one ﬂip per phase or allow multiple ﬂips.
Given a coil cU
k , in order to respect the phase and direction assignment, it
can be connected to a coil cU
j in an electrically valid way: top to bottom if their
sign coincides and top to top or bottom to bottom in case of diﬀerent signs.
For illustration, we consider the example shown in Fig. 2. The stator has
ns = 66 slots, the number of poles is np = 10, and the coil pitches are y1 =
6 and y2 = 7. Two sets of coils are emphasized with black dotted lines, the
1 For the sake of clarity, in this paper we focus at the case in which connections can
be placed only at the jumper side; the problem can be generalized to account also
connections at the opposite side.

A Hybrid Approach for Stator Winding Design Optimization
137
T
B
T
T
T
B
B
B
T
B
Fig. 2. Coils, jumpers and ﬂip connection for phase U.
corresponding bars are marked to be top (T) or bottom (B). The left group of
coils starts at coil 20T(26B), i.e. the coil which connects the top bar in slot 20
with the bottom bar in slot 26. It connects, in clockwise direction and without
a jumper, to coil 33T(39B). The following coil is 35T(41B), connected with a
jumper. In the right group of coils, we observe a ﬂip between coil 61T(1B) and
coil 0T(6B). That is, the current ﬂow in coil 0T is reversed wrt. coil 61T.
The problem to traverse each coil exactly once with the total minimum cost
clearly resembles to a Travelling Salesman Problem instance where coils repre-
sent cities and jumpers represent the travelling arcs between the cities. We will
formalize the model in Sect. 4.
Sum of Currents. The jumpers connecting coils have undesired losses and
decrease of eﬃciency. It is favorable to combine jumpers in a way that their
magnetic ﬁelds cancel as much as possible, thereby reducing the stray losses.
To formalize the problem, let
J = {JU ∪JV ∪JW } = {jp
k, k = 1, . . . , np
J, p = U, V, W}
be the set of jumpers of the three phases. Each jumper jp
k is associated with
its sign of the electrical current ˜σ(jp
k) ∈{1, −1} relative to clockwise rotation,
which follows from the signs of the coils it connects and the relative position of
these coils. For each jumper jp
k, we introduce the set of slots it spans over to be
S(jp
k). Then, conversely for each slot s, we deﬁne the set of jumpers going over
this slot to be J (s) = {j ∈J : s ∈S(j)}.
For each slot s, the relative amount of stray losses is approximated by the
absolute of the sum of complex current vectors corresponding to the jumpers in
J (s). Each jumper j = jp
k corresponds to a phase p and thus to a current vector

138
A. Zanarini and J. Poland
Fig. 3. Electric/magnetic vectors of the three phases in the complex plane.
v(j) = v(jp
k) = v(p) = exp(2πip/3), as shown in Fig. 3. The sum of the stray loss
terms over all slots yields the sum of currents value:
SumCur =
ns−1

s=0


j∈J (s)
˜σ(j)v(j)

(2)
In the optimization model described later, the sum of currents criterion may
either enter the cost function or be constrained.
3.3
Jumper Placement
Lastly, jumpers need to be placed in a conﬂict-free manner, where potential
conﬂicts arise from two or more jumpers of the same phase or diﬀerent phases,
which cross the same slot. An illustration is given in Fig. 4, where we revisit the
example from Fig. 1, now with all phases shown.
In the top left corner of the stator, three jumpers j1, j2, and j3 are highlighted
with thick black lines: j1 (dotted line) connects coil 50T(56B) and coil 48T(54B),
j2 (dashed line) connects coil 56T(62B) and coil 55T(61B), and j3 (solid line)
connects coil 66T(72B) and coil 64T(70B). The jumpers j1 and j2 belong to
phase U, j3 belongs to phase V , their respective jumper slot ranges are S(j1),
S(j2) and S(j3). Jumpers j1 and j3 alone never cause conﬂicts, but because of
the presence of j2, the potential conﬂicts in placement need to be resolved.
Conﬂicts are resolved by using diﬀerent radial and axial placement of the
jumpers in the 3-dimensional space. This is illustrated in Fig. 5 for two jumper
geometries. For the optimization problem, a ﬁxed set of axial and radial levels is
pre-deﬁned, depending on the electrical constraints (minimum distance between
jumpers because of voltage) and the manufacturing constraints.
In the example in Fig. 4, conﬂicts are resolved by using jumpers at diﬀerent
radial levels. This is indicated by dotted concentric circles outside the stator,
corresponding to the available radial levels: a jumper outside the innermost
dotted circle is placed on radial level 1, a jumper outside the second dotted
circle is placed on radial level 2.
The set of conﬂicting jumpers is deﬁned as:
Ω = {{jj, jk} | j ̸= k ∧S(jj) ∩S(jk) ̸= ∅}
(3)

A Hybrid Approach for Stator Winding Design Optimization
139
Level 1
Level 2
Fig. 4. Jumpers on diﬀerent radial levels
In other words two jumpers are conﬂicting if their respective slot ranges
intersect.
Ultimately, the jumper selection sub-problem consist in choosing the appro-
priate jumper geometries for all the pair of jumpers that have conﬂicts.
Fig. 5. Jumper geometries avoiding conﬂict: (a) using diﬀerent radial levels, where the
jumpers cross at diﬀerent axial levels, (b) using diﬀerent axial levels, where the longer
jumper bridges over the shorter one. The line styles indicate the axial level: axial level
0 = dotted, axial level 1 = solid, axial level 2 = dashed. For simplicity, the typical
curvature of the tangential part (left to right) of the jumpers is not shown.

140
A. Zanarini and J. Poland
4
Solution
In this section we will describe the solution approach. From a high level per-
spective, the problem is composed by the deﬁnition of the electrical layout (bar
assignment) and the optimization of the coil connections. In principle, the two
sub-problems could be solved in an integrated fashion, however there are two fun-
damental reasons why decomposing them: ﬁrst and foremost, the engineer prefers
a human in the loop optimization approach whereby he/she can decide which
electrical design(s) to optimize. Secondly, the problem becomes more tractable
from an optimization perspective.
Following this decomposition (electrical layout and coil connections), we
developed two methods: a model in which coil routing and jumper selection
is tackled in a single Mixed Integer Program (MIP) and a further decomposition
in which ﬁrstly the coil routing is optimized via MIP and then jumper selec-
tion is performed via MIP optionally fed with an initial solution computed via
Constraint Programming (CP). In the following we will describe the formulation.
4.1
Electrical Layout
As discussed in Sect. 3.1, there are possibly many candidate electrical layouts
to consider, depending on the number of slots and poles of the machine. Some
of these may already disqualify for their harmonics properties. Those which are
harmonically feasible may be completely enumerated, or Monte-Carlo sampled,
for further individual optimization. Alternatively, a multi-objective optimiza-
tion can be performed, oﬀering diﬀerent trade-oﬀs of harmonics properties and
material consumption to the engineer. These and other alternatives are out of
the technical scope of the paper. In the following, we will focus on solving the
routing and jumper placement eﬃciently for a given electrical layout.
4.2
Coil Routing
To solve the coil routing problem, we model the problem as a rich Travelling
Salesman Problem (TSP) on a directed graph. Each phase is treated as a separate
instance of a TSP. The graph for phase p is deﬁned by Gp = (V, A) where
each coil cp
i ∈Cp corresponds to a node vi ∈V with i = 1, . . . , nV . The set
of arcs A = aij (where aij represents the arc connecting coil cp
i with cp
j) is
composed by the electrically sound connections as deﬁned in Sect. 3.2. The set
of arcs is partitioned in two subset A = Ad ∪Af where Ad contains all the
arcs that keep the direction of the stator (i.e. bottom bar-top bar or top-bottom
bar connections), and Af contains all the ﬂip connectors (i.e. top-top bar or
bottom-bottom bar connections). Each arc aij has an associated weight waij
that is proportional to its length. The ﬁrst and last coils of the stator have
dangling endpoints as they will be connected to the power source. In the model,
we force these two coils to be connected together in order to form a closed loop.

A Hybrid Approach for Stator Winding Design Optimization
141
The TSP is formulated via a typical model where each arc is associated with
a binary decision variable xij that will be equal to one iﬀit will be part of the
ﬁnal solution.
The objective function is (with a slight abuse of notation):
min

aij∈A
wijxij
(4)
Here, weights wij are set to the exact material costs of the corresponding
jumper in the integrated MIP formulation, i.e. where TSP and jumper placement
are solved together. In the decomposed approach, the exact cost for the jumper
is not known at the TSP solving stage, so wij is assigned the cost of the cheapest
corresponding jumper. This may introduce small but perceivable sub-optimality
of the ﬁnal solution.
The set of constraints is listed below:
– the incoming degree and outgoing degree must be equal to one:

j
xij =

j
xji = 1
∀i = 1, . . . , nV
(5)
– the solution should not contain sub-tours. This constraint is handled by a typ-
ical cut generation procedure that add a linear constraint during the branch
and bound procedure to avoid the creation of sub-tours (see [6–8])
– optionally, the number of ﬂip should be equal to 1:

aij∈Af
xij = 1
(6)
– The sum of currents objective/constraint has been linearized: Instead of con-
sidering the complex absolute = Euclidean norm in (2), we use a hexagonal
norm where |v|hex is the radius of the scaled hexagon as depicted in Fig. 3 on
which v lies. Each |v|hex is implemented in the optimization by introducing a
corresponding slack variable and lower bounding it with the projection of v
onto all six vectors {exp(iπj/3) : 0 ≤j < 6}.
The model has been developed in SCIP 3.0 [8], using a customized variant
of the 3-opt heuristic for the solution process.
4.3
Jumper Placement
The jumper placement sub-problem is about choosing the correct conﬂict-free
jumper geometry for each connection that has been chosen by the coil rout-
ing. According to the jumper geometry deﬁnition, we introduce for each jumper
j ∈J its possible geometrical variants as Gj = {gq, q = 1, . . . , nGj}; each vari-
ant has an associated material cost wjq (with j ∈J and q ∈{1, . . . , nGj}).
Finally, for each pair of potentially conﬂicting jumpers {j′, j′′} ∈Ω, we intro-
duce the set of unordered pair of conﬂicting geometry variants as Λj′,j′′ =
{{q′, q′′} | iﬀgq′ conﬂicts with gq′′}.
In the following, we present two diﬀerent models: a MIP based formulation
and a CP based formulation.

142
A. Zanarini and J. Poland
MIP Model. We use binary variables yjq to represent the variant q of jumper
j that will be assigned value 1 iﬀthat variant is chosen.
The objective function is:
min

j∈J,q∈{1,...,nGj }
wjqyjq
(7)
The constraints are:
– for each jumper only one variant can be chosen:

q∈{1,...,nGj }
yjq = 1
∀j ∈J
(8)
– for each conﬂicting jumper pair {j′, j′′} ∈Ω, the respective geometrical vari-
ant should not collide:
yj′q′ ≤(1 −yj′′q′′)
∀{q′, q′′} ∈Λj′,j′′, ∀{j′, j′′} ∈Ω
(9)
CP Model. The CP model is employed optionally on those instances where
the feasibility aspect of the jumper selection is hard for the MIP model.
The CP model resembles closely the MIP model with the main diﬀerence
being the use of integer variables instead of binary ones. Speciﬁcally, each jumper
j is associated with a variable yj ∈{1, . . . , nGj}. For brevity, we do not report
the full model as it follows trivially from the MIP model.
The search strategy chooses the variable to branch on based on minimum
domain size, and it breaks ties based on the cost regret. The value heuristic
selects the jumper geometry variant based on the minimum cost. The ﬁrst search
phase is followed by a Large Neighborhood Search. The neighborhood is chosen
randomly, and the search strategy is the same as the one employed in the ﬁrst
phase, except that ties are broken randomly.
5
Experimental Evaluation
We develop the solution proposed using SCIP 3.0 [8] and Google Optimization
Tools (‘OR-Tools’) [9]. All the experiments have been conducted on a PC with
an Intel Core i5-5300U (2.3 GHz) and 16 GB of RAM.
5.1
Electrical Layout
We ﬁrstly tested the performance of the electrical layout generation by ﬁxing
the machine parameters for some stator setups and enumerating all the possible
layouts. Results are reported in Table 2; the ﬁrst four columns report the machine
parameters (the number of slots, number of poles, the coil pitch and the number
of slots per pole and phase); the last two columns show the total number of
layouts generated and the time (in milliseconds) to enumerate them all. Layout

A Hybrid Approach for Stator Winding Design Optimization
143
enumeration has been implemented in Python 2.7 with SciPy. The method takes
less than a second to enumerate hundreds of thousands of possible electrical
layouts. In practice, some of these layouts may be discarded a priori based on
the harmonics of the MMF and in a second stage a selection is presented to the
user. Finally, the user will decide which design(s) to optimize.
Table 2. Performance test for generating all the electrical layouts
ns
np y1 q = a + b
c
# layouts Time (ms)
102 14 8
2 + 3
7
10
4
264 34 8
2 + 10
17
5005
282
384 46 8
2 + 18
23
5985
251
480 58 8
2 + 22
29
296010
601
576 76 6
2 + 10
19
24310
770
5.2
Coil Routing
We tested the coil routing on a subset of the electrical layouts found in the pre-
vious section. Speciﬁcally, 20 instances were selected randomly for each machine
design deﬁned above (10 instances for the 102 slots machine). We then com-
pared the decomposition with and without the usage of CP, and the integrated
approach. We setup the jumper geometry in order to have 10 radial levels and 5
axial levels. The time limits were set to be of 90 s for the TSP problem and 30 s
for the jumper placement problem. For the integrated approach we set a time
limit of 300 s. For the MIP formulations, we additionally set the optimality gap
tolerance to 1%. As for the CP model (optionally used to feed a ﬁrst solution to
the jumper placement MIP model), the search is limited to 2 s (1 s to ﬁnd the
ﬁrst solution, plus an additional one for the Large Neighborhood Search).
In a ﬁrst test the sum of currents was left unconstrained. Results are reported
in Table 3. Each row presents aggregated results for a given subset of instances.
The ﬁrst column indicates the number of slots of the machine; next there are
three groups of columns summarizing the results for the decomposed MIP+CP
approach, the decomposed MIP approach and the integrated MIP approach.
Each group presents the average time and standard deviation, the value of the
objective function and the percentage of the solved instances. For the decom-
posed MIP and the integrated approach the objective function has been normal-
ized to the value obtained in the MIP+CP approach.
The MIP+CP shows clearly the best robustness in term of number of
instances solved, with a drawback of adding few seconds to the overall solution
process; the objective function is overall very similar.
The integrated MIP model manifests its limits: only small-size instances are
solved and the overall gain in the objective function is minimal. Please note that
the optimality gap reached was below the threshold of 1% except for 3 instances

144
A. Zanarini and J. Poland
Table 3. Results for coil routing and jumper placement with no sum of current con-
straint.
Decomposed MIP+CP
Decomposed MIP
MIP
ns
t (μ)
t (σ) ObjCP
%Sol
t (μ)
t (σ)
Obj
ObjCP
%Sol
t (μ)
t (σ)
Obj
ObjCP
%Sol
102
4.4
1.0
12.18
100%
2.4
1.2
100.0%
100%
177.6
112.2
98.2%
90%
264
28.6
28.7
23.57
100%
26.0
28.9
100.0%
95%
340.7
2.0
101.7%
5%
384
23.2
19.5
25.39
100%
19.4
19.4
99.9%
95%
342.1
3.2
–
0%
480
42.0
35.6
32.34
100%
38.8
34.8
100.1%
100%
339.8
2.2
–
0%
576
65.0
33.4
43.56
70%
60.4
32.7
99.8%
30%
341.2
2.4
–
0%
that timed out with a gap slightly higher than 2%. Out of the 10 instances
solved by the integrated approach only one showed a relevant objective function
improvement of 11% w.r.t. the decomposition. This suggests that the decompo-
sition is in most cases able to capture well the overall problem structure. One of
the main reason why the integrated MIP model struggles is that all the jumper
geometries need to be considered even for arcs that will not be part of the ﬁnal
solution, leading therefore to an explosion of number of variables.
In a second test, we run the same instance set from the previous experi-
mentation and constrained the sum of currents to be about half of what it was
initially found. In Table 4, we report for each instance set: the average sum of
current found by the decomposition in the ﬁrst benchmark, the constraint used
for the second benchmark, and the resulting average sum of current found in the
second benchmark when constrained.
Table 4. Average sum of currents found by the decomposition approach.
ns
SumCur (unconstrained) Upper bound SumCur (constrained)
102
2757.8
1370
130.6
264
8100.7
4050
304.5
384 13561.2
6780
343.6
480 16721.0
8360
458.9
576 11883.9
8490
492.2
The results of the second test are presented in Table 5. Observations are
similar to the instances with no sum of current constraint: the CP-supported
model is more consistent throughout the instance set, while adding little over-
head to the solution process. Note that for some instance sets (e.g. ns = 384) the
average objective function is slightly lower: this is due partly to the optimality
gap tolerance of 1% and partly to sub-optimality caused by the decomposition.
The integrated model is again bringing limited improvements in term of solution
quality while not being able to solve medium and large-size instances.

A Hybrid Approach for Stator Winding Design Optimization
145
Table 5. Results for coil routing and jumper placement with sum of current constraint.
Decomposed MIP+CP
Decomposed MIP
MIP
ns
t (μ)
t (σ) ObjCP
%Sol
t (μ)
t (σ)
Obj
ObjCP
%Sol
t (μ)
t (σ)
Obj
ObjCP
%Sol
102
4.9
1.0
12.18
100%
2.6
1.3
100.0%
100%
180.4
113.2
99.7%
100%
264
28.5
29.1
23.69
100%
25.8
29.7
99.9%
95%
345.9
21.2
100.9%
5%
384
23.4
19.4
25.29
100%
20.3
19.6
100.0%
95%
346.3
20.1
–
0%
480
43.7
33.5
33.41
95%
40.0
35.0
100.1%
95%
341.8
16.2
–
0%
576
68.1
34.7
42.43
75%
62.7
34.2
98.8%
45%
343.7
19.4
–
0%
6
Conclusion
In this paper, we formalized a design optimization problem for stator windings of
rotating machines. We proposed a hybrid approach that solves real-life instances
in a satisfactory manner. The most eﬃcient and robust solution decomposes the
problem in three parts: ﬁrstly, it generates the electrical layout, then it routes
optimally the connections of each phase, and lastly it selects jumpers in order
to avoid physical and electrical conﬂicts. The routing and jumper placement
sub-problems are solved via a MIP model supported by a CP model to boot-
strap the search for conﬂict-free jumpers. The MIP model for the TSP performs
remarkably well on this problem class and allows to enrich the TSP easily with
additional requirements, e.g. the sum of currents. The decomposition provides
a solution quality that is close to the one of a fully integrated model, with the
advantage of scaling to large-size instances, and being more eﬃcient. The tool
has been in use for the last two years in the engineering departments of ABB
for gearless mill drives and hydro-generators.
References
1. Stromberg, T.: Alternator voltage waveshape with particular reference to the higher
harmonics. ASEA J. 20, 139–148 (1947)
2. Jones, G.R.: Electrical Engineer’s Reference Book. Section 20, 3 edn. Elsevier (2013)
3. Fornasiero, E., Alberti, L., Bianchi, N., Bolognani, S.: Considerations on selecting
fractional-slot nonoverlapped coil windings. IEEE Trans. Ind. Appl. 49(3), 1316–
1324 (2013)
4. Alberti, L., Bianchi, N.: Theory and design of fractional-slot multilayer windings.
IEEE Trans. Ind. Appl. 49(2), 841–849 (2013)
5. Prieto, B.: Design and analysis of fractional-slot concentrated-winding multiphase
fault-tolerant permanent magnet synchronous machines. Ph.D. thesis, University of
Navarra, Spain (2015)
6. Gusﬁeld, D.: Very simple algorithms and programs for all pairs network ﬂow analy-
sis. SIAM J. Comput. 19(1), 143–155 (1990)
7. Gomory, R.E., Hu, T.C.: Multi-terminal network ﬂows. SIAM J. Appl. Math. 9,
551–570 (1961)

146
A. Zanarini and J. Poland
8. Gamrath, G., Fischer, T., Gally, T., Gleixner, A.M., Hendel, G., Koch, T., Maher,
S.J., Miltenberger, M., M¨uller, B., Pfetsch, M.E., Puchert, C., Rehfeldt, D.,
Schenker, S., Schwarz, R., Serrano, F., Shinano, Y., Vigerske, S., Weninger, D.,
Winkler, M., Witt, J.T., Witzig, J.: The SCIP Optimization Suite 3.2. ZIB Insti-
tute (2016)
9. Google Optimization Tools. https://developers.google.com/optimization/. Accessed
21 Nov 2016

A Distributed Optimization Method
for the Geographically Distributed Data
Centres Problem
Mohamed Wahbi(B), Diarmuid Grimes, Deepak Mehta, Kenneth N. Brown,
and Barry O’Sullivan
Insight Centre for Data Analytics, School of Computer Science and IT,
University College Cork, Cork, Ireland
{mohamed.wahbi,diarmuid.grimes,deepak.mehta,
ken.brown,barry.osullivan}@insight-centre.org
Abstract. The geographically distributed data centres problem (GDDC)
is a naturally distributed resource allocation problem. The problem
involves allocating a set of virtual machines (VM) amongst the data
centres (DC) in each time period of an operating horizon. The goal is
to optimize the allocation of workload across a set of DCs such that
the energy cost is minimized, while respecting limitations on data centre
capacities, migrations of VMs, etc. In this paper, we propose a distrib-
uted optimization method for GDDC using the distributed constraint
optimization (DCOP) framework. First, we develop a new model of the
GDDC as a DCOP where each DC operator is represented by an agent.
Secondly, since traditional DCOP approaches are unsuited to these types
of large-scale problem with multiple variables per agent and global con-
straints, we introduce a novel semi-asynchronous distributed algorithm
for solving such DCOPs. Preliminary results illustrate the beneﬁts of the
new method.
1
Introduction
Distributed constraint reasoning (DCR) gained an increasing interest in recent
years due to its ability to handle cooperative multi-agent problems that are
naturally distributed. DCR has been applied to solve a wide range of applica-
tions in multi-agent coordination such as distributed scheduling [20], distributed
planning [7], distributed resource allocation [24], target tracking in sensor net-
works [23], distributed vehicle routing [17], optimal dispatch in smart grid [22],
etc. These applications can be solved by a centralized approach once the knowl-
edge about the problem is delivered to a centralized authority. However, in such
applications, it may be undesirable or even impossible to gather the whole prob-
lem knowledge into a single authority. In general, this restriction is mainly due to
This work is funded by the European Commission under FP7 Grant 608826
(GENiC - Globally Optimised Energy Eﬃcient Data Centres).
This work is funded by Science Foundation Ireland (SFI) under Grant Number
SFI/12/RC/2289.
c
⃝Springer International Publishing AG 2017
D. Salvagnin and M. Lombardi (Eds.): CPAIOR 2017, LNCS 10335, pp. 147–166, 2017.
DOI: 10.1007/978-3-319-59776-8 12

148
M. Wahbi et al.
privacy and/or security requirements: constraints may represent strategic infor-
mation that should not be revealed to other agents that can be seen as competi-
tors, or even to a central authority. The cost or the inability of translating all
information to a single format may be another reason: in many cases, constraints
arise from complex decision processes that are internal to an agent and cannot be
articulated to a central authority. More reasons why distributed methods may be
desirable for such applications and often make a centralized process inadequate
have been listed in [11].
In DCR, a problem is expressed as a distributed constraint network. A dis-
tributed constraint network is composed of a group of autonomous agents where
each agent has control of some elements of information about the problem, that
is, variables and constraints. Each agent owns its local constraint network, and
variables in diﬀerent agents are connected by constraints. Traditionally, there
are two large classes of distributed constraint networks. The ﬁrst class con-
siders problems where all constraints are described by boolean relations (hard
constraint) on possible assignments of variables they involve. They are called
distributed constraint satisfaction problems (DisCSP). The second class of prob-
lems are distributed constraint optimization problems (DCOP) where constraints
are described by a set of cost functions for combinations of values assigned to
the variables they connect. In DisCSP, the goal is to ﬁnd assignments of values
to variables such that all (hard) constraints are satisﬁed while in DCOP the goal
is to ﬁnd assignments that minimize the objective function deﬁned by the sum
of all constraint costs.
Researchers in the DCR ﬁeld have developed a range of diﬀerent constraint
satisfaction and optimisation algorithms. The main algorithms and protocols
include synchronous [10,15,37], asynchronous [6,23,36,38] and semi-synchronous
[13,21,34] search, dynamic programming methods [25], and algorithms which
respect privacy and autonomy [8,35] versus those which perform local search
[16,40]. In order to simplify the algorithm speciﬁcation, most of these algorithms
assume that all constraints are binary, and that each agent controls exactly one
variable. Such assumptions simplify the algorithm speciﬁcation but represent
a limitation in the adoption of distributed constraint reasoning techniques in
real-world applications.
The ﬁrst assumption was justiﬁed by techniques which translated non-binary
problems into equivalent binary ones; however, recent research has demonstrated
the beneﬁts of handling non-binary constraints directly in distributed algo-
rithms [5,32]. The second assumption is justiﬁed by the fact that any DCR
problem with several variables per agent can be solved by those algorithms once
transformed using one of the following reformulations [9,37]: (i) compilation,
where for each agent we deﬁne a new variable whose domain is the set of solu-
tions to the original local problem; (ii) decomposition, where for each agent we
create a virtual agent for each of its variables. However, neither of these methods
scales up well as the size of the local problems increase, either (i) because of the
space and time requirements of the reformulation, or (ii) because of the extra
communication overhead and the loss of a complete view on the local problem
structure. Only a few algorithms for handling multiple local variables in DisCSP

A Distributed Optimization Method for the GDDC Problem
149
have been proposed [3,19,39]. These algorithms are speciﬁc to DisCSP, since
they reason about hard constraints, and cannot be applied directly to DCOP,
which are concerned with costs. Another limitation is that most DCOP algo-
rithms do not actively exploit hard constraints as they require all constraints to
be expressed as cost functions.
In this paper, we present a general distributed model for solving real-life
DCOPs, including hard constraints, in order to model the geographically dis-
tributed data centres problem [4,27–29]. After ﬁnding that traditional DCOP
approaches are unsuited to these types of large-scale problem with multiple
variables per agent and global (hard) constraints, we introduce, agac-ng, a
novel optimization distributed search algorithm for solving such DCOPs. In the
agac-ng algorithm agents assign their variables and generate a partial solu-
tion sequentially and synchronously following an ordering on agents. However,
generated partial solutions are propagated asynchronously by agents with unas-
signed variables. The concurrent propagation of partial solutions enables early
detection of a need to backtrack and saves a lot of search eﬀort. agac-ng can
perform bounded-error approximation while maintaining a theoretical guarantee
on solution quality. In agac-ng, a global upper bound which represents the cost
of the best known solution to the problem is maintained by all agents. When
propagating partial solutions agents ensure that the current lower bound on the
global objective augmented by the bounded-error distance does not exceed the
global upper bound.
Data centres consume considerable amounts of energy: an estimated 416.2
terawatt-hours was consumed in 2014 [2] with the US accounting for approx-
imately 91 terawatts [1]. While many industries and economies are dependent
on such infrastructure, the increase in high-computing requirements for cloud-
based services around internet-of-things, big data, etc. have lead to some experts
predicting that this consumption could treble in the next decade [2] unless sig-
niﬁcant breakthroughs are made in reducing consumption.
Geographically distributed data centres present many possible beneﬁts in
terms of reducing energy costs through global, rather than local, optimisation.
In particular each location may have diﬀerent unit energy costs, external weather
conditions, local renewable sources, etc. Therefore reasoning at a global level can
exploit these diﬀerences through optimally reallocating workload in each time
period through migration of virtual machines (VMs), subject to constraints on
number of migrations, virtual machine sovereignty, data centre capacities, etc.
Gathering the whole knowledge about the problem into a centralized location
may not be feasible in the geographically distributed data centres because of the
large amount of information (problem speciﬁcations) each data centre would
need to communicate to the centralized solver to solve the problem. In addition,
data centres may wish to keep some information about their local constraints,
costs, and topology conﬁdential and not share it with other data centres.
The maximum energy that can be consumed by a data centre and its running
capacity might also be conﬁdential information that data centres want to keep
private from operators of other data centres. Thus, the geographically distrib-
uted data centres is a naturally distributed resource allocation problem where

150
M. Wahbi et al.
data centres may not be willing to reveal their private information to other data
centres. In addition, sending the whole knowledge about the problem to a cen-
tralized location will create a bottleneck on the communication towards that
location. Thus, a distributed solving process is preferred for the geographically
distributed data centres problem.
This paper is structured as follows. Section 2 gives the necessary background
for distributed constraint reasoning and the general deﬁnition of the geographi-
cally distributed data centres. We present our model of the GDDC problem as
DCOP in Sect. 3. Section 4 introduces our new algorithm, agac-ng, for solving
DCOP with global hard constraints and multiple variables per agent. We report
experimental results in Sect. 5. Section 6 gives a brief overview of related works
on distributed constraint reasoning. Finally, we conclude the paper in Sect. 7.
2
Preliminaries/Background
2.1
Distributed Constraint Optimization Problem
A constraint satisfaction problem (CSP) has been deﬁned for a centralized archi-
tecture by a triple (X, D, C), where X = {x1, . . . , xp} is a set of p variables,
D = {d1, . . . , dp} is the set of their respective ﬁnite domains, and C is a set of
(hard) constraints. A constraint c(X) ∈C is a relation over the ordered subset of
variables X = (xj1, . . . , xjk), which deﬁnes those value combinations which may
be assigned simultaneously to the variables in X. |X| is the arity of c(X), and
X is its scope. A tuple τ = (vj1, . . . , vjk) ∈c(X) is a support for c(X), and τ[xi]
is the value of xi in τ. A solution to a CSP is an assignment to each variable of
a value from its domain, such that all constraints are satisﬁed.
A global constraint captures a relation over an arbitrary number of vari-
ables. For example, the AllDiff constraint states that the values assigned to
the variables in its scope must all be diﬀerent [30]. Filtering algorithms which
exploit the speciﬁc structure of global constraints are one of the main strengths
of constraint programming. During search, any value v ∈di that is not gener-
alized arc-consistent (gac) can be removed from di. A value vi ∈di, xi ∈X is
generalized arc-consistent with respect to a constraint c(X) iﬀthere exists a
support τ for c(X) such that vi = τ[xi], and for every xj ∈X, xi̸=xj, τ[xj] ∈dj.
Variable xi is gac if all its values are gac with respect to every constraint in C.
A CSP is gac if all its variables are gac.
A distributed constraint satisfaction problem (DisCSP) is a CSP where the
variables, domains and constraints of the underlying network are distributed over
a set of autonomous agents. Formally, a distributed constraint network (DisCSP)
is deﬁned by a 5-tuple (A, X, D, C, ϕ), where X, D and C are as above. A is a set of
n agents {a1, . . . , an}, and ϕ : X →A is a function specifying an agent to control
each variable. Each variable belongs to one agent and only the agent who owns a
variable has control of its value and knowledge of its domain. Let Xi denotes the
set of variables belonging to agent ai, i.e. Xi = {∀xj ∈X : ϕ(xj) = ai}. Agent ai
knows all constraints, Ci, involving its variables Xi. Xi can be partitioned in two
disjoint subsets Pi = {xj | ∀c(X) ∈Ci, xj ∈X →X ⊆Xi} and Ei = Xi\Pi. Pi

A Distributed Optimization Method for the GDDC Problem
151
is a set of private variables, which only share constraints with variables inside
ai. Conversely, Ei is a set of variables linked to the outside world and sometimes
referred to as external (or negotiation) variables.
This distribution of variables divides C in two disjoint subsets, Cintra which
are between variables of same agent (i.e., c(X) ∈Cintra, X ⊆Xi), and Cinter
which are between variables of diﬀerent agents called intra-agent (private) and
inter-agent constraint sets, respectively. An intra-agent constraint c(X) is known
by the agent owner of X, and it is unknown by other agents. Usually, it is
considered that an inter-agent constraint c(X) is known by every agent owning
a variable in X. Two agents are neighbours if they control variables that share
a constraint; we denote by Ni the set of neighbours of agent ai.
As in the centralized case, a solution to a DisCSP is an assignment to each
variable of a value from its domain, satisfying all constraints. A distributed con-
straint optimisation problem (DCOP) is deﬁned by a DisCSP (A, X, D, C, ϕ),
together with an objective function. A solution to a DCOP is a solution for the
DisCSP which is optimal with respect to the objective function.1
For the rest of the paper we consider a generic agent ai ∈A. Agent ai stores a
unique order on agents, i.e. an ordered tuple of all the agent IDs, denoted by ≺o.
≺o is called the current order of ai. Agents appearing before agent ai in ≺o are
the higher priority agents (predecessors) and conversely the lower priority agents
(successors) are agents appearing after ai in ≺o. For sake of clarity, we assume
that the order is the lexicographic ordering [1, 2, . . . , n]. Each agent maintains
a counter, and increments it whenever it changes its assignment. The current
value of the counter tags each generated assignment.
Deﬁnition 1. An assignment for an agent ai ∈A is an assignment for each
external variable of a value from its domain. That is, a tuple (⟨Ei, Vi⟩, ti) where
Vi ∈
×
xj∈Ei
dj, Vi[xj] ∈dj and ti is the tag value. When comparing two assign-
ments, the most up to date is the one with the greatest tag ti.
Deﬁnition 2. A current partial assignment cpa is an ordered set of assign-
ments of external variables [(⟨E1, V1⟩, t1), . . . , (⟨Ek, Vk⟩, tk)]. Two cpas are
compatible if the values of each common variable amongst the two cpas are
equal. ag(cpa) is the set of all agents with assigned variables in the cpa.
Deﬁnition 3. A timestamp associated with a cpa is an ordered list of coun-
ters [t1, . . . , tk] where ∀j ∈1..k, tj is the tag of the agent aj. When comparing
two cpas, the strongest one is that associated with the lexicographically greater
timestamp. That is, the cpa with greatest value on the ﬁrst counter on which
they diﬀer, if any, otherwise the longest one.
During search agents can infer inconsistent sets of assignments called
no-goods.
1 Cost functions can be implemented using element constraints [31].

152
M. Wahbi et al.
Deﬁnition 4. A no-good or conﬂict set is an assignment set of variables that
is not contained in any solution. A no-good is a clause of the form ¬[(xi = vi) ∧
(xj = vj) ∧. . . ∧(xk = vk)], meaning that these assignments cannot be extended
to a solution. We say that a no-good is compatible with a cpa if every common
variable is assigned the same value in both.
Agents use these no-goods to prune the search space. To stay polynomial
we only keep no-goods that are compatible with the current state of the search.
These no-goods can be a direct consequence of propagating constraints (e.g.,
any assignment set that violates a constraint is a no-good) or can be derived
from a set of no-goods. Literally, when all values of the variable xk are ruled
out by some no-good, they are resolved computing a new no-good as follows.
The new generated no-good is the conjunction of all these no-goods for values
of xk removing variable xk. If the generated no-good contains assignments of
local variables, agent ai uses this no-good locally to prune the search space.
Otherwise, agent ai reports the no-good to the agent having the lowest priority
among those having variables in the new generated no-good.
2.2
Geographically Distributed Data Centres Problem
The geographically distributed data centres (GDDC) problem considered here
can be deﬁned as follows. We are given a set of data centre locations L, and a
set of virtual machines that must be assigned to physical servers in each time
period of an operating horizon T. Each location has its own unit energy price
epl
t for each time period (sample real time prices are shown in Fig. 1). The cost
of executing a virtual machine (VM) in a location can diﬀer for each time period
for the same location, and for each location in the same time period. This cost
constitutes not only the electricity price from the local utility, but is also aﬀected
by external factors such as the outdoor temperature, equipment quality, etc. The
latter can be estimated using the Power Usage Eﬀectiveness (PUE) which is the
ratio of the total power consumption of a DC to the power consumption of the
IT equipment alone.
Fig. 1. Sample real time prices for 13 diﬀerent locations over the same 24 h period.

A Distributed Optimization Method for the GDDC Problem
153
Furthermore each location has an associated region k that a subset of sov-
ereign VMs can only be performed in (e.g., for security reasons). Therefore for
some VMs the set of locations where they can be performed is restricted. A data
centre has a maximum capacity on the IT power consumption pmaxl (aggre-
gated across all servers), and a maximum number of migrations in/out, mmaxl,
that can occur in each time period across all VM types.
There is a set of VM types V , where each type v has an associated power
consumption pv, and quantity nv that are running in each time period. The type
of a VM type can be further subdivided into those that can be run everywhere
and those that can only be run in a speciﬁc region, i.e. that can only be run at
its current location or in one of a limited set of other locations (e.g., a subset of
VMs can only be run in European data centres). Let Rl be the set of VM types
that can be run in location l.
We must then decide how many VMs of each type to allocate to each data
centre in each time period such that the total energy cost of performing the
VMs over the horizon, plus the costs of all migrations (where emiv/emov is the
energy cost of migrating a vm in/out), is minimized. This allocation is subject to
capacity restrictions in the data centres, limitations on the number of migrations
per data centre per time period, and limitations on migration of sovereign VMs.
3
GDDC as DCOP
The geographically distributed DCs can naturally be modeled as a DCOP as
follows. (For simplicity we consider the time periods to be of duration one hour
and thus energy and power values can be used interchangeably.)
Agents.
– Each DC is represented by an agent A = L.
(External) Variables.
– In each DC/agent there is an integer variable xl
vt that represents the number
of VMs of type v allocated to DC/agent al in period t.
– In each DC/agent there is an integer variable cl that indicates the total energy
cost for running and migrating VMs in Rl over all time-periods.
Note here that agents only have variables xl
vt if v ∈Rl. This is suﬃcient for
enforcing the sovereignty constraint.
(Private) Variables.
– In each DC/agent there is an integer variable mil
vt
that indicates how many VMs of type v are migrated in to the DC al in
time-period t.
– In each DC/agent there is an integer variable mol
vt
that indicates how many VMs of type v are migrated out of the DC al in
time-period t.

154
M. Wahbi et al.
– In each DC/agent there is a real variable ul
t that indicates the energy con-
sumption of DC l for time-period t.
– In each DC/agent there is an integer variable rl that indicates the total energy
cost of running VMs in Rl over the entire horizon.
– In each DC/agent there is an integer variable ml that indicates the total
energy cost for migrating VMs in Rl over all time-periods.
(Intra-agent) Constraints.
We present here the intra-agent constraints held by agent al ∈A. Only agent/DC
al is aware of the existence of its intra-agent constraints.
Allocation. The number of VMs of a given type running in time period are
equal to the number running in the previous time period plus the incomings,
minus the outgoings.
∀v∈Rl, ∀t∈T :
xl
v(t+1) = xl
vt + mil
vt −mol
vt
(3.1)
Capacity. The total energy consumed by a DC al for running VMs is bounded:
∀t∈T :
ul
t =

v∈Rl
xl
vt · pv ≤pmaxl
(3.2)
Migration. The amount of incoming/outgoing VMs per time period for each
DC is limited by a given threshold:
∀t∈T :

v∈Rl
mil
vt + mol
vt ≤mmaxl
(3.3)
We further add the following redundant constraint on migration in/out variable
pairs, enforcing that at least one must be 0 in every location in every time period
for every type:
∀v∈Rl, ∀t∈T :
mil
vt = 0 ∨mol
vt = 0
(3.4)
Running cost. The total energy cost of running VMs in al over the entire
horizon is:
rl =

t∈T
epl
t · ul
t
(3.5)
Migration cost. The total energy cost for migrating VMs in Rl over all time-
periods is:
ml =

v∈Rl

t∈T
(mil
vt · emiv + mol
vt · emov) · epl
t
(3.6)
Internal cost. The total energy cost for running and migrating VMs in Rl over
all time-periods is:
cl = rl + ml
(3.7)

A Distributed Optimization Method for the GDDC Problem
155
(Inter-agent) Constraints.
An inter-agent constraint is totally known by all agents owning variables it
involves.
Assignment. VMs of type v must all be assigned to DCs in each time-period
where v is in Rl:
∀v∈V , ∀t∈T :

l∈L|v∈Rl
xl
vt = nv
(3.8)
Objective. The global objective is to minimize the sum of the total energy cost
of running VMs in all DCs together with total energy cost for migrating VMs
over the entire horizon:
obj =

l∈L
cl
4
Nogood-Based Asynchronous Generalized
Arc-Consistency AGAC-ng
To solve a challenging distributed constraint optimization problem such as
GDDC, we propose a new DCOP algorithm, called agac-ng (nogood-based asyn-
chronous generalized arc-consistency). To the best of our knowledge, agac-ng is
the ﬁrst algorithm for solving DCOP with multiple variables per agent and non-
binary and hard constraints that can ﬁnd the optimal solution, or a solution within
a user-speciﬁed distance from the optimal using polynomial space at each agent.
When solving distributed constraint networks, the solution process is
restricted: each agent ai is only responsible for making decisions (assignments)
of the variables it controls (Xi). Thus, agents must communicate with each other
exchanging messages about their variable assignments and conﬂicts of constraints
in order to ﬁnd a global (optimal) solution. Several distributed algorithms for
solving the DCOP have been designed by the distributed constraint reason-
ing community. Regarding the manner on which assignments are processed and
search performed on these algorithms, they can be categorized into synchronous,
asynchronous, or semi-synchronous.
The ﬁrst category consists of those algorithms in which agents assign values
to their variables in a synchronous and sequential way. Although synchronous
algorithms do not exploit the parallelism inherent from the distributed system,
their agents receive consistent information from each other. The second cat-
egory consists of algorithms in which the process of proposing values to the
variables and exchanging these proposals is performed concurrently and asyn-
chronously between agents. Agents take advantage from the distributed formal-
ism to enhance the degree of concurrency. However, in asynchronous algorithms,
the global assignment state at any particular agent is in general inconsistent.
The third category is that of algorithms combining both sequential value assign-
ments by agents together with concurrent computation. Agents take advantage
from both the above-mentioned categories: they perform concurrent computa-
tion while exchanging consistent information between agents.

156
M. Wahbi et al.
In this section we propose a novel semi-synchronous search algorithm for
optimally solve DCOPs called agac-ng (nogood-based asynchronous generalized
arc-consistency). In agac-ng algorithm, agents assign their variables and gen-
erate a partial solution sequentially and synchronously following an ordering
on agents. However, generated partial solutions are propagated asynchronously
by neighbours with unassigned variables. The concurrent propagation of partial
solutions enables an early detection of a need to backtrack and saves search
eﬀort.
agac-ng incorporates an asynchronously generalized arc-consistency phase
in a synchronous search procedure. agac-ng follows an ordering on agents to
perform the sequential assignments by agents. Agents assign their variables only
when they hold the cpa. The cpa is a unique message (token) that is passed from
one agent to the next in the ordering. The cpa message carries the current partial
assignment that agents attempt to extend into a complete solution by assigning
their variables in it.2 When an agent succeeds in assigning its variables on the
cpa, it sends this cpa (token) to the next agent on the ordering. Furthermore,
copies of the cpa are sent to all neighbors whose assignments are not yet on the
cpa. These agents maintain the generalized arc-consistency asynchronously in
order to detect as early as possible inconsistent partial assignments. The gener-
alized arc-consistency process is performed as follows. When an agent receives
a cpa, it updates the domain of its variables and copies of neighbors variables,
removing all values that are not gac using the no-goods as justiﬁcation of value
deletions.
When an agent generates an empty domain as a result of maintaining gac,
it resolves the no-goods ruling out values from that domain, producing a new
no-good. Then, the agent backtracks to the agent with the lowest priority in
the conﬂict by sending the resolved no-good. Hence, multiple backtracks may
be performed at the same time coming from diﬀerent agents having an empty
domain. These backtracks are sent concurrently by these diﬀerent agents to dif-
ferent destinations. The reassignments of the destination agents then happen
simultaneously and generate several cpas. However, the strongest cpa coming
from the highest level in the agent ordering will eventually dominate all others.
Agents use timestamps attached to cpas to detect the strongest one (see Deﬁn-
ition 3). Interestingly, the search process of higher levels with stronger cpa can
use no-goods reported by the (killed) lower level processes, so that it beneﬁts
from their computational eﬀort.
4.1
The Algorithm Description
Each agent ai ∈A executes the pseudo-code shown in Fig. 2. The agent ai has
a local solver where it stores and propagates the most up-to-date assignments
received from higher priority agents (solver.cpa) w.r.t. the agent ordering (≺o).
In agac-ng, agents exchange the following types of messages (where ai is the
sender):
2 A complete solution here is a complete assignments of all agents’ external variables.

A Distributed Optimization Method for the GDDC Problem
157
Fig. 2. agac-ng algorithm running by agent ai.
ok?: agent ai passes on the current partial assignment (cpa) to a lower priority
agent. According to the ID of the agent that has the token attached to the
message by ai, the receiver will try to extend the cpa (when it is the next
agent on the ordering) or maintain generalized arc-consistency phase.

158
M. Wahbi et al.
ngd: agent ai reports the inconsistency to a higher priority agent. The incon-
sistency is reported by a no-good (i.e., a subset of the cpa).
sol: agent ai informs all other agents of the new best solution (cpa) and new
better bound.
stp: agent ai informs agents if either an optimal solution is found or the problem
is found to be unsolvable.
Agent ai running the agac-ng algorithm starts the search by calling proce-
dure initialize in which ai sets the upper bound to +∞and initializes its
local solver and sets the objective variable to minimize (lines 9 to 11) before
setting counter tags of other agents to zero (line 12). If agent ai is the initial-
ising agent (the ﬁrst agent in the agent ordering ≺o), it initiates the search by
calling procedure assign (line 15) after setting itself as the agent that has the
token (i.e., the privilege to make decisions) (line 14). Then, a loop considers the
receiving and processing of the possible message types (lines 2 to 8).
When calling procedure assign, agent ai tries to ﬁnd a local solution, which
is consistent with the assignments of higher agents (solver.cpa). If agent ai fails
to ﬁnd a consistent assignment, it calls procedure backtrack (line 23). If ai
succeeds, it increments its counter ti and generates a cpa from higher agents
assignments (solver.cpa) augmented by the assignments of its external variables
(Ei), lines 17 to 18.3 If the cpa includes assignments of all agents (ai is the last
agent in the order), agent ai calls procedure reportSolution() to report a new
solution (line 20). Otherwise, agent ai sends forward the cpa to every neighboring
agent (Ni\ag(cpa)) whose assignments are not yet on the cpa including the next
agent that will extend the cpa (i.e., the agent that will have the token) (lines
28 to 30) by calling procedure sendForward(), line 22.
When agent ai (the last agent) calls procedure reportSolution, a complete
assignment has been reached, with a new global cost of B (line 24). Agent ai
sends the full current partial assignment (cpa), i.e. solution, with the new global
cost to all other agents (line 25). Agent ai calls then procedure storeSolution()
(line 26) to set the upper bound to new global cost value and the best solution
to the newly found one, lines 54 to 55, and to post a new constraint requiring
that the global cost should not exceed the cost of the best solution found so far
(taking into account the error-bound). Finally, agent ai calls procedure assign()
to continue the search for new solutions with better cost (line 27). Whenever
agent ai receives a sol message it calls procedure storeSolution().
Whenever ai receives an ok? message, procedure processOK is called (line 5).
Agent ai checks if the received cpa is stronger than its current cpa (solver.cpa)
by comparing the timestamp of the received cpa to that stored locally, func-
tion compareTimeStamp call (line 31). If it is not the case, the received cpa is
discarded. Otherwise, ai sets the token to the newly received one and updates
the local counters of all agents in the cpa by those freshly received (lines 33
to 35). Next, agent ai updates its solver (solver.update) to include all assign-
ments of agents in the received cpa and propagates their eﬀects locally (line 36).
3 Only external variables linked to unassigned neighbors are needed.

A Distributed Optimization Method for the GDDC Problem
159
If agent ai generates an empty domain as a result of calling solver.update, ai
calls procedure backtrack (line 39), otherwise, ai checks if it has to assign its
variables (if it has the token) and then calls procedure assign if it is the case
(line 38).
When agent ai generates an empty domain after propagating its constraints,
the procedure backtrack is called to resolve the conﬂict. Agent ai requires its
local solver to explain the failure by generating a new no-good (ng), that is, the
subset of assignments that produced the inconsistency, (line 40). If the new no-
good (ng) is empty, agent ai terminates execution after sending a stp message
to all other agents in the system meaning that the last solution found is the
optimal one (line 42). Otherwise, agent ai sets the token to be the agent having
the lowest priority among those having variables in the newly generated no-good
ng. If the token is diﬀerent than ai, the no-good ng is reported to token through
a ngd message, line 46. Otherwise, ai has to seek a new local solution for its
variables by calling procedure assign after storing the generated no-good in its
local solver (lines 48 to 49).
When a ngd message is received by an agent ai, it checks the validity of
the received no-good (line 50). A no-good is valid if the assignments on it are
consistent with those stored locally in solver. If the received no-good (ng) is
valid, ai sets itself as the agent having the token (line 51). Next, agent ai stores
ng and propagates it in its local solver. The procedure assign is then called to
ﬁnd a new local solution for the variables of ai (line 53). Finally, when a stp
message is received, ai marks the end ﬂag as true to stop the main loop (line 8).
5
Evaluation of Distributed Approach
In this section we experimentally evaluate agac-ng, the distributed COP algo-
rithm we proposed in Sect. 4 for solving the DCOP model of the GDDC problem
presented in Sect. 3. All experiments were performed based on the DisChoco 2.0
platform4 [33], in which agents are simulated by Java threads that communicate
only through message passing. We use the Choco-4.0.0 solver as local solver of
each agent in the system [26].
5.1
Empirical Setup
Instances were generated for a scenario with 13 data centres across 3 continents,
each with a capacity deﬁned to be 40 MW. Real time price data was gathered for
each location for each hour for a set of ﬁve days. Dynamic PUE values of each
DC were generated as a function of the temperature across a sample day. There
were 5 VM types chosen with associated power consumption values of 20 W, 40 W,
60 W, 80 W and 100 W respectively. For each DC, VM creation petitions are ran-
domly generated until their consumption reaches a load percentage of 40% of the
DC capacity. Each VM creation petition is further randomly assigned a “sov-
ereignty”, which is either the continent the DC belongs to or the entire DC set.
4 http://dischoco.sourceforge.net.

160
M. Wahbi et al.
Finally, instances had a migration limit for each data centre stating what
percentage of average VMs per data centre can be migrated per time period. We
generated instances with a limit of 5% and with a limit of 10%. There were 3
instances generated with diﬀerent seeds for each migration limit and for each of
the ﬁve days of real time price data, producing a set of 30 instances. Therefore
instances typically had 10000 variables, with domain sizes ranging from 15 to
more than 500. For ci variables the domains are larger and domain sizes ranges
from 107 to more than 108. The number of constraints was approximately 7500
with maximum arity of 49 variables. The baseline subsequently used for compar-
ison involves the case where there are 0 migrations, i.e. each DC just performs
the load that it was initially assigned in time zero across all time periods.
5.2
Search Strategy
The agac-ng algorithm requires a total ordering on agents. This ordering is used
to pass on the token (the privilege of assigning variables) between agents. The
agent ordering can then aﬀect the behaviour of the algorithm and our empirical
results (Sect. 5.3) conﬁrms the eﬀect of agent ordering.
In the following we propose two agent orderings called o1 and o2. In both
orderings we use the same measure αi for an agent ai. For each agent ai, αi is
the diﬀerence between the most expensive and the cheapest price over all time
slots for the DC represented by agent ai. For o2, agents are sorted using an
increasing order over αi. For o1, the ﬁrst agent is selected to be the one with
median measure αi followed in increasing order by agents, say aj, having the
smallest distance to the measure αi, i.e. | αj −αi |. For example, let α1 = 22,
α2 = 10, α3 = 44, α4 = 55, α5 = 30, thus, o1 = [a5, a1, a3, a2, a4] and o2 =
[a2, a1, a5, a3, a4].
We investigated search strategies for the model of each agent. We are using
the migration in and migration out variables (mii
vt and moi
vt) as decision vari-
ables for the local solver of each agent/DC ai. Every agent/DC ai only com-
municates decisions about xi
vt and ci as they are the external variables of agent
ai while migration variables are private variables. Preliminary results suggested
the best approach for solving the problems was to choose decision variables (i.e.,
migration in and migration out variables) according to the domwdeg variant of
the weighted degree heuristic. Values of the migration in variables of the cheap-
est time slot and the migration out variables of the most expensive time slot are
selected in a decreasing order (the upper bound) and other migration variables
on increasing order (the lower bound).
5.3
Empirical Results
We evaluate the performance of the algorithm by communication load and solu-
tion quality. Communication load is measured by the total number of exchanged
messages among agents during algorithm execution (#msg) [18]. Results are
presented on Table 1. Solution quality is assessed by two measures: (i) in terms

A Distributed Optimization Method for the GDDC Problem
161
Table 1. Distributed: number of message exchanged by agents/DCs in solving each
instance.
Price Base euro agac-ng (o1)
agac-ng (o2)
Mig 5% Mig 10% Mig 5% Mig 10%
pr1
165.31
15,838
13,619
12,515
24,700
pr2
197.53
1,709
1,966
5,390
6,426
pr3
192.09
3,208
37,528
4,947
2,456
pr4
171.53
18,494
23,909
25,015
23,706
pr5
215.97
5,737
10,798
19,293
14,425
Table 2. Distributed: results in terms of average monetary cost (in Euros) for migration
limits at 5% and 10%.
Price Base euro agac-ng (o1)
agac-ng (o2)
Mig 5% Mig 10% Mig 5% Mig 10%
pr1
165.31
164.11
164.03
162.45
159.71
pr2
197.53
187.92
190.21
186.26
187.92
pr3
192.09
186.85
186.20
186.73
185.58
pr4
171.53
167.55
166.24
166.18
164.11
pr5
215.97
215.69
215.76
215.69
215.40
of average monetary cost in e (Table 2) and (ii) in terms of average percent-
age savings over baseline (Table 3). For all instances we use a timeout limit of
one hour per instance and present the average cost (respectively #msg) per
price/migration-limit instance type (across the three instances).
The results are given for the both static agent ordering presented in Sect. 5.2
for the three metrics Tables 1, 2 and 3.
Regarding the communication loads shown in Table 1, the agents exchange
few messages in solving the problems. In the worst case, agac-ng agents
exchanges 37, 528 messages to solve the problems. This represents a signiﬁcant
result regarding the complexity and the size of the instances solved by a complete
DCOP algorithm. This is mainly due to the expensive local ﬁltering and search
per each local solver and the topology of the instances solved: all instances solved
have a complete constraint network. Thus, agac-ng produces more chronological
backtracks than backjumps. This is explained by the behaviour of the algorithm
where only small improvements over the ﬁrst solution/UB were found. After
the ﬁrst solution, the agac-ng algorithm mainly backtracks from the last agent
on the ordering to the second last that returns the token to the last agent for
seeking new solutions and so on.
Comparing the solution quality (Tables 2 and 3), for instances pr5, agac-
ng improvement over the baseline is insigniﬁcant. The improvement over the
baseline ranges between 0.1–0.26%, which represents less than a euro per day.

162
M. Wahbi et al.
Table 3. Distributed: results in terms of average percentage savings over baseline for
migration limits at 5% and 10%.
Price Base euro agac-ng (o1)
agac-ng (o2)
Mig 5% Mig 10% Mig 5% Mig 10%
pr1
165.31
0.73%
0.77%
1.73%
3.39%
pr2
197.53
4.87%
3.71%
5.71%
4.87%
pr3
192.09
2.73%
3.07%
2.79%
3.39%
pr4
171.53
2.32%
3.08%
3.12%
4.33%
pr5
215.97
0.13%
0.10%
0.13%
0.26%
For other instances, this improvement is more signiﬁcant mainly for pr2 where
the improvement over the baseline ranges between 3.71% and 5.71%, which repre-
sents between 7.32 and 11.27 euros per day. This is an interesting result mainly
because agents are solving problems only by passing messages between them,
without sharing their constraints or their prices, and keeping their information
private.
Comparing two diﬀerent agents ordering, running agac-ng using agent order-
ing o2 always improves agac-ng using o1 for pricing. However, this is not always
the case for the number of message exchanges to solve the problem. The average
cost over all instances when running agac-ng using o2 is 366 and 369 when
using o1. For communication load, agac-ng (o2) requires an average of 27, 775
messages over all instances while agac-ng (o2) requires 26, 561 messages.
Regarding the migration limit, the results suggests that having a smaller
migration limit is better for the distributed solving process regarding the number
messages exchanges while the solution quality is slightly aﬀected by the change
on the migration limits. In the distributed problems having larger domains leads
to more messages when the ﬁltering power is limited.
6
Related Work
Many distributed algorithms for solving DisCSP/DCOP have been designed in
the last two decades. Synchronous Backtrack (SBT) is the simplest DisCSP
search algorithm. SBT performs assignments sequentially and synchronously. In
SBT, only the agent holding a current partial assignment (cpa) performs an
assignment or backtrack [41]. Meisels and Zivan [21] extended SBT to Asyn-
chronous Forward Checking (AFC), an algorithm in which the forward checking
propagation [14] is performed asynchronously [21]. In AFC, whenever an agent
succeeds to extend the cpa, it sends the cpa to its successor and it sends copies of
this cpa to the other unassigned agents in order to perform the forward checking
asynchronously. The Nogood-Based Asynchronous Forward Checking algorithm
(AFC-ng), which is an improvement of AFC, has been proposed in [34]. Unlike
AFC, AFC-ng uses no-goods as justiﬁcation of value removals and allows several

A Distributed Optimization Method for the GDDC Problem
163
simultaneous backtracks coming from diﬀerent agents and going to diﬀerent des-
tinations. AFC-ng was shown to outperform AFC. The pioneering asynchronous
algorithm for DisCSP was asynchronous backtracking (ABT) [6,38]. ABT is exe-
cuted autonomously by each agent, and is guaranteed to converge to a global
consistent solution (or detect inconsistency) in ﬁnite time.
The synchronous branch and bound (SyncBB) [15] is the basic systematic
search algorithm for solving DCOP. In SyncBB, only the agent holding the token
is allowed to perform an assignment while the other agents remain idle. Once it
assigns its variables, it passes on the token and then remains idle. Thus, SyncBB
does not make any use of concurrent computation. No-Commitment Branch
and Bound (NCBB) is another synchronous polynomial-space search algorithm
for solving DCOPs [10]. To capture independent sub-problems, NCBB arranges
agents in constraint tree ordering. NCBB incorporates, in a synchronous search,
a concurrent computation of lower bounds in non-intersecting areas of the search
space based on the constraint tree structure. Asynchronous Forward Bounding
(AFB) has been proposed in [12] for DCOP to incorporate a concurrent compu-
tation in a synchronous search. AFB can be seen as an improvement of SyncBB
where agents extend a partial assignment as long as the lower bound on its
cost does not exceed the global upper bound. In AFB, the lower bounds are
computed concurrently by unassigned agents. Thus, each synchronous extension
of the cpa is followed by an asynchronous forward bounding phase. Forward
bounding propagates the bounds on the cost of the partial assignment by send-
ing to all unassigned agents copies of the extended partial assignment. When
the lower bound of all assignments of an agent exceeds the upper bound, it per-
forms a simple backtrack to the previous assigned agent. Later, the AFB has
been enhanced by the addition of a backjumping mechanism, resulting in the
AFB BJ algorithm [13]. The authors report that AFB BJ, especially combined
with the minimal local cost value ordering heuristic performs signiﬁcantly better
than other DCOP algorithms. The pioneer complete asynchronous algorithm for
DCOP is Adopt [23]. Later on, the closely related BnB-Adopt [36] was presented.
BnB-Adopt changes the nature of the search from Adopt best-ﬁrst search to a
depth-ﬁrst branch-and-bound strategy, obtaining better performance.
7
Conclusions
In this paper we studied the geographically distributed data centres problem
where the objective is to optimize the allocation of workload across a set of
DCs such that the energy cost is minimized. We introduced a model of this
problem using the distributed constraint optimization framework. We presented
agac-ng, nogood-based asynchronous generalized arc-consistency, a new semi-
asynchronous algorithm for DCOPs with multiple variables per agent and with
non-binary and hard constraints. agac-ng can ﬁnd the optimal solution, or a
solution within a user-speciﬁed distance form the optimal using polynomial space
at each agent. We showed empirically the beneﬁts of the new method for solving
large-scale DCOPs.

164
M. Wahbi et al.
References
1. America’s Data Centres Consuming and Wasting Growing Amounts of Energy
(2015). https://www.nrdc.org/resources/americas-data-centers-consuming-and-wa
sting-growing-amounts-energy
2. Data centres to consume three times as much energy in next decade, experts warn
(2016). http://www.independent.co.uk/environment/global-warming-data-centres
-to-consume-three-times-as-much-energy-in-next-decade-experts-warn-a6830
086.html
3. Armstrong, A.A., Durfee, E.H.: Dynamic prioritization of complex agents in dis-
tributed constraint satisfaction problems. In: Proceedings of AAAI 1997/IAAI
1997, pp. 822–822 (1997)
4. Beloglazov, A., Buyya, R.: Energy eﬃcient resource management in virtualized
cloud data centers. In: Proceedings of the 2010 10th IEEE/ACM International
Conference on Cluster, Cloud and Grid Computing, pp. 826–831. IEEE Computer
Society (2010)
5. Bessiere, C., Brito, I., Gutierrez, P., Meseguer, P.: Global constraints in distributed
constraint satisfaction and optimization. Comput. J. 57(6), 906–923 (2013)
6. Bessiere, C., Maestre, A., Brito, I., Meseguer, P.: Asynchronous backtracking with-
out adding links: a new member in the ABT family. Artif. Intell. 161, 7–24 (2005)
7. Bonnet-Torr´es, O., Tessier, C.: Multiply-constrained DCOP for distributed plan-
ning and scheduling. In: AAAI Spring Symposium: Distributed Plan and Schedule
Management, pp. 17–24 (2006)
8. Brito, I., Meisels, A., Meseguer, P., Zivan, R.: Distributed constraint satisfaction
with partially known constraints. Constraints 14, 199–234 (2009)
9. Burke, D.A., Brown, K.N.: Eﬃcient handling of complex local problems in distrib-
uted constraint optimization. In: Proceedings of ECAI 2006, Riva del Garda, Italy,
pp. 701–702 (2006)
10. Chechetka, A., Sycara, K.: No-commitment branch and bound search for distrib-
uted constraint optimization. In: Proceedings of AAMAS 2006, pp. 1427–1429
(2006)
11. Faltings, B., Yokoo, M.: Editorial: introduction: special issue on distributed con-
straint satisfaction. Artif. Intell. 161(1–2), 1–5 (2005)
12. Gershman, A., Meisels, A., Zivan, R.: Asynchronous forward-bounding for distrib-
uted constraints optimization. In: Proceedings of ECAI 2006, pp. 103–107 (2006)
13. Gershman, A., Meisels, A., Zivan, R.: Asynchronous forward-bounding for distrib-
uted COPs. JAIR 34, 61–88 (2009)
14. Haralick, R.M., Elliott, G.L.: Increasing tree search eﬃciency for constraint satis-
faction problems. Artif. Intell. 14(3), 263–313 (1980)
15. Hirayama, K., Yokoo, M.: Distributed partial constraint satisfaction problem. In:
Smolka, G. (ed.) CP 1997. LNCS, vol. 1330, pp. 222–236. Springer, Heidelberg
(1997). doi:10.1007/BFb0017442
16. Hirayama, K., Yokoo, M.: The distributed breakout algorithms. Artif. Intell. 161,
89–116 (2005)
17. L´eaut´e, T., Faltings, B.: Coordinating logistics operations with privacy guarantees.
In: Proceedings of the IJCAI 2011, pp. 2482–2487 (2011)
18. Lynch, N.A.: Distributed Algorithms. Morgan Kaufmann Series (1997)
19. Maestre, A., Bessiere, C.: Improving asynchronous backtracking for dealing with
complex local problems. In: Proceedings of ECAI 2004, pp. 206–210 (2004)

A Distributed Optimization Method for the GDDC Problem
165
20. Maheswaran, R.T., Tambe, M., Bowring, E., Pearce, J.P., Varakantham, P.: Taking
DCOP to the real world: eﬃcient complete solutions for distributed multi-event
scheduling. In: Proceedings of AAMAS 2004, Washington, DC, USA, pp. 310–317.
IEEE Computer Society (2004)
21. Meisels, A., Zivan, R.: Asynchronous forward-checking for DisCSPs. Constraints
12(1), 131–150 (2007)
22. Miller, S., Ramchurn, S.D., Rogers, A.: Optimal decentralised dispatch of embed-
ded generation in the smart grid. In: Proceedings of AAMAS 2012, pp. 281–288.
International Foundation for Autonomous Agents and Multiagent Systems (2012)
23. Modi, P.J., Shen, W.-M., Tambe, M., Yokoo, M.: ADOPT: asynchronous distrib-
uted constraint optimization with quality guarantees. Artif. Intell. 161, 149–180
(2005)
24. Petcu, A., Faltings, B.: A value ordering heuristic for local search in distrib-
uted resource allocation. In: Faltings, B.V., Petcu, A., Fages, F., Rossi, F. (eds.)
CSCLP 2004. LNCS, vol. 3419, pp. 86–97. Springer, Heidelberg (2005). doi:10.
1007/11402763 7
25. Petcu, A., Boi Faltings, D.: A scalable method for multiagent constraint optimiza-
tion. In: Proceedings of IJCAI 2005, pp. 266–271 (2005)
26. Prud’homme, C., Fages, J.-G., Lorca, X.: Choco Documentation. TASC, INRIA
Rennes, LINA CNRS UMR 6241, COSLING S.A.S. (2016)
27. Qureshi, A., Weber, R., Balakrishnan, H., Guttag, J., Maggs, B.: Cutting the elec-
tric bill for internet-scale systems. In: ACM SIGCOMM Computer Communication
Review, vol. 39, pp. 123–134. ACM (2009)
28. Rahman, A., Liu, X., Kong, F.: A survey on geographic load balancing based data
center power management in the smart grid environment. IEEE Commun. Surv.
Tutorials 16(1), 214–233 (2014)
29. Rao, L., Liu, X., Xie, L., Liu, W.: Minimizing electricity cost: optimization of
distributed internet data centers in a multi-electricity-market environment. In:
INFOCOM, 2010 Proceedings IEEE, pp. 1–9. IEEE (2010)
30. R´egin, J.-C.: A ﬁltering algorithm for constraints of diﬀerence in CSPs. In: Pro-
ceedings of AAAI 1994, pp. 362–367 (1994)
31. Van Hentenryck, P., Deville, Y., Teng, C.-M.: A generic arc-consistency algorithm
and its specializations. Artif. Intell. 57(2–3), 291–321 (1992)
32. Wahbi, M., Brown, K.N.: Global constraints in distributed CSP: concurrent GAC
and explanations in ABT. In: O’Sullivan, B. (ed.) CP 2014. LNCS, vol. 8656, pp.
721–737. Springer, Cham (2014). doi:10.1007/978-3-319-10428-7 52
33. Wahbi, M., Ezzahir, R., Bessiere, C., Bouyakhf, E.H.: DisChoco 2: a platform for
distributed constraint reasoning. In: Proceedings of Workshop on DCR 2011, pp.
112–121 (2011)
34. Wahbi, M., Ezzahir, R., Bessiere, C., Bouyakhf, E.H.: Nogood-based asynchronous
forward-checking algorithms. Constraints 18(3), 404–433 (2013)
35. Wallace, R.J., Freuder, E.C.: Constraint-based reasoning and privacy/eﬃciency
tradeoﬀs in multi-agent problem solving. Artif. Intell. 161, 209–228 (2005)
36. Yeoh, W., Felner, A., Koenig, S.: BnB-ADOPT: an asynchronous branch-and-
bound DCOP algorithm. J. Artif. Intell. Res. (JAIR) 38, 85–133 (2010)
37. Yokoo, M.: Distributed Constraint Satisfaction: Foundation of Cooperation in
Multi-agent Systems. Springer, Berlin (2001)
38. Yokoo, M., Durfee, E.H., Ishida, T., Kuwabara, K.: Distributed constraint satis-
faction for formalizing distributed problem solving. In: Proceedings of 12th IEEE
International Conference on Distributed Computing Systems, pp. 614–621 (1992)

166
M. Wahbi et al.
39. Yokoo, M., Hirayama, K.: Distributed constraint satisfaction algorithm for complex
local problems. In: International Conference on Multi Agent Systems, pp. 372–379
(1998)
40. Zhang, W., Wang, G., Xing, Z., Wittenburg, L.: Distributed stochastic search
and distributed breakout: properties, comparison and applications to constraint
optimization problems in sensor networks. Artif. Intell. 161, 55–87 (2005)
41. Zivan, R., Meisels, A.: Synchronous vs Asynchronous search on DisCSPs. In: Pro-
ceedings of EUMAS 2003 (2003)

Explanation-Based Weighted Degree
Emmanuel Hebrard1(B) and Mohamed Siala2
1 LAAS-CNRS, Universit´e de Toulouse, CNRS, Toulouse, France
hebrard@laas.fr
2 Insight Centre for Data Analytics, Department of Computer Science,
University College Cork, Cork, Ireland
mohamed.siala@insight-centre.org
Abstract. The weighted degree heuristic is among the state of the art
generic variable ordering strategies in constraint programming. However,
it was often observed that when using large arity constraints, its eﬃ-
ciency deteriorates signiﬁcantly since it loses its ability to discriminate
variables. A possible answer to this drawback is to weight a conﬂict set
rather than the entire scope of a failed constraint.
We implemented this method for three common global constraints
(AllDiﬀerent, Linear Inequality and Element) and evaluate it on
instances from the MiniZinc Challenge. We observe that even with sim-
ple explanations, this method outperforms the standard Weighted Degree
heuristic.
1
Introduction
When solving a constraint satisfaction problem with a backtracking algorithm,
the choice of the next variable to branch on is fundamental. Several heuristics
have been designed to make this choice, often relying on the concept of fail
ﬁrstness: “To succeed, try ﬁrst where you are most likely to fail ” [4].
The Weighted Degree heuristic (wdeg) [1] is still among the state of the art
for general variable ordering search heuristics in constraint programming. Its
principle is to keep track of how many times each constraint has failed in the
past, with the assumption that constraints that were often responsible for a fail
will most likely continue this trend. It is very simple and remarkably robust,
which often makes it the heuristic of choice when no dedicated heuristic exists.
Depending on the application domain, other generic heuristics may be more
eﬃcient, and it would be diﬃcult to make a strong claim that one dominates
the other outside of a restricted data set. Empirical evidence from the MiniZinc
Challenge and from the CSP Solver Competition,1 however, show that wdeg
is among the best generic heuristics for classical CSP solvers (see [16] for an
analysis of the reasons of its eﬃciency). It is also interesting to observe that SAT
and clause-learning solvers often use Variable State Independent Decaying Sum
M. Siala—Supported by SFI under Grant Number SFI/12/RC/2289.
1 http://www.minizinc.org/challenge.html, http://www.cril.univ-artois.fr/CSC09.
c
⃝Springer International Publishing AG 2017
D. Salvagnin and M. Lombardi (Eds.): CPAIOR 2017, LNCS 10335, pp. 167–175, 2017.
DOI: 10.1007/978-3-319-59776-8 13

168
E. Hebrard and M. Siala
(V SIDS) [10], which has many similarities with wdeg, whilst taking advantage
of the conﬂicts computed by these algorithms.
One drawback of Weighted Degree, however, is that the weight attributed to
constraints of large arity has a weak informative value. In the extreme case, a
failure on a constraint involving all variables of the problem does not give any
useful information to guide search.
We empirically evaluated a relatively straightforward solution to this issue.
We propose to weight the variables involved in a minimal conﬂict set rather than
distributing it equally among all constrained variables. Such conﬂict sets can be
computed in the same way as explanations within a clause learning CSP solver.
2
Background
A constraint network is a triplet ⟨X, D, C⟩where X is a n-tuple of variables
⟨x1, . . . , xn⟩, D is a mapping from variables to ﬁnite sets of integers, and C is a
set of constraints. A constraint C is a pair (X(C), R(C)), where X(C) is tuple of
variables and R(C) ∈Zk. The assignment of the k-tuple of values σ to the k-tuple
of variables X satisﬁes the constraint C iﬀ∃τ ∈R(C) such that ∀i, j X[i] =
X(C)[j]
=⇒
σ[i] = τ[j]. A solution of a constraint network ⟨X, D, C⟩is an
assignment of a n-tuple σ to X satisfying all constraints in C with σ[i] ∈D(xi)
for all 1 ≤i ≤n. The constraint satisfaction problem (CSP) consists in deciding
whether a constraint network has a solution.
Algorithm 1. Solve(P = ⟨X, D, C⟩)
1 if not consistent(P) then return False;
2 if ∀x ∈X, |D(x)| = 1 then return True;
3 select x ∈{xi ∈X, |D(xi)| > 1} and v ∈D(x);
4 return Solve(P|x=v) or Solve(P|x̸=v);
We consider the class of backtracking algorithms, which can be deﬁned recur-
sively as shown in Algorithm 1. In Line 1, a certain property of consistency of the
network is checked, through constraint propagation. This process yields either
some domain reductions or, crucially, a failure. In the latter case, the last con-
straint that was processed before a failure is called the culprit. Line 2 detects
that a solution was found. In Line 3, a heuristic choice of variable and value is
made to branch on in Line 4, where P|x=v (resp. P|x̸=v) denotes the constraint
network equal to the current P except for D(x) = {v} (resp. D(x) = D(x)\{v}).
3
General Purpose Heuristics
We ﬁrst brieﬂy survey the existing general-purpose variable ordering heuristics
against which we compare the proposed improvement of Weighted Degree.
Impact-Based Search (IBS) [11] selects the variable with highest expected
impact. The impact I(x = a) of a decision x = a corresponds to the reduction in

Explanation-Based Weighted Degree
169
potential search space resulting from this decision. It is deﬁned as 1−SP /SP |x=a
where SP is the size of the Cartesian product of the domains of P after constraint
propagation, and P|x=a denotes the problem P with D(x) restricted to {a}. This
value is updated on subsequent visits of the same decision. Moreover, in order
to favor recent probes, the update is biased by a parameter α: Ia(x = a) ←
((α −1)Ib(x = a) + Ip(x = a))/α, where Ip, Ib and Ia refer to, respectively,
the last recorded impact, the impact stored before, and after the decision. The
preferred variable minimizes 
a∈D(x)(1−I(x = a)), the sum of the complement-
to-one of each value’s impact, i.e., the variable with fewest options, and of highest
impact. This idea also deﬁnes a branching strategy since the value with lowest
impact can be seen as more likely to lead to a solution. Therefore, once a variable
x has been chosen, the value a with the lowest I(x = a) is tried ﬁrst.
Activity-Based Search (ABS) [9] selects the variable whose domain was most
often reduced during propagation. It maintains a mapping A from variables
to reals. After Line 1 in Algorithm 1, for every variable x whose domain has
been somehow reduced during the process, the value of A(x) is incremented. In
order to favor recent activity, for every variable x, A(x) is multiplied by factor
0 ≤γ ≤1 before2. The variable x with lowest ratio |D(x)|/A(x) is selected ﬁrst.
Similarly to IBS, one can use the activity to select the value to branch on. The
activity of a decision is deﬁned as the number of variables whose domain was
reduced as its consequence. It is updated as in IBS, using the same bias α.
The Last Conﬂict heuristic (LC) [7] relies on a completely diﬀerent con-
cept. Once both branches (P|x=a and P|x̸=a) have failed, the variable x of this
choice point is always preferred, until it is successfully assigned, that is, a branch
P|x=b (with possibly b = a) is tried and the following propagation step succeeds.
Indeed, a non-trivial subset of decisions forming a nogood must necessarily con-
tain x. Moreover, if there is a nogood that does not contain the previous decision
(or the next, etc.) then this procedure will automatically “backjump” over it.
Often, no such variable exists, for instance when the search procedure dives
through left branches and therefore a default selection heuristic is required.
Conﬂict Ordering Search (COS) [2] also tries to focus on variables that failed
recently. Here, for every failure (Line 1, Algorithm 1 returning False), the vari-
able selected in the parent recursion is stamped by the total number of failures
encountered so far. The variable with the highest stamp is selected ﬁrst. If there
are several variables with equal stamp (this can only be 0, i.e., variables which
never caused a failure), then a default heuristic is used instead.
Last, Weighted Degree (wdeg) [1] maintains a mapping w from constraints
to reals. For every failure with culprit constraint C, a decay factor γ is applied
exactly as in ABS and w(C) is incremented. The weight wdeg(xi) of a variable
xi is the sum of the weight of its active neighboring constraints, i.e., those con-
straining at least another distinct unassigned variable: wdeg(xi) = 
C∈Ci w(C)
where Ci = {C | C ∈C ∧xi ∈X(C) ∧∃xj ̸= xi ∈X(C) s.t. |D(xj)| > 1}. The
variable x with lowest ratio |D(x)|/wdeg(x) is selected ﬁrst.
2 In practice, the increment value is divided by γ, and A is scaled down when needed.

170
E. Hebrard and M. Siala
4
Explanation-Based Weight
We propose to adapt the weighting function of wdeg to take into account the
fact that not every variables in the scope a constraint triggering a failure may
be involved in the conﬂict.
Consider for instance the constraint n
i=1 xi ≤k where the initial domains
are all in {0, 1}. When a failure is triggered, the weight of every variable is incre-
mented. However, variables whose domain is equal to {1} are sole responsible
for this failure. It would be more accurate to increment only their weight.
When a failure occurs for a constraint C with scope ⟨x1, . . . , xk⟩, it means
that under the current domain D, there is no tuple satisfying C in the cartesian
product D(x1)×, . . . , ×D(xk). It follows that there is no solution for the CSP
under the current domain. Any subset E = {i1, . . . , im} of {1, . . . , k} such that
no tuple in D(xi1)×, . . . , ×D(xim) satisﬁes C is an explanation of the failure of
C under the domain D. The set {1, . . . , k} is the trivial explanation, however
explanations that are strict subsets represent a valuable information and have
been used to develop highly successful search algorithms. The goal in these
algorithms (e.g., Nogood Recording [12] and CDCL [6,13,14]) is to use such
explanations to derive a nogood, which also entails the failure under the current
domain when added to the CSP, however without falsifying individually any
constraint. Another goal is to make this nogood as weak as possible since we
can add the negation of the nogood as an implied constraint. For this reason, as
well as other practical reasons, a more restrictive language of literals is used to
represent explanations and nogoods (typically x = a, x ̸= a, x ≤a and x > a).
Our purpose is simpler and more modest: we simply aim at computing an
explanation E as small as possible, and do not care about the unary domain
constraints. Indeed we use this explanation only to weight the variables whose
indices are in E. We keep a weight function for variables instead of constraints.
When a constraint C triggers a failure, we apply a decay factor γ and by default
we increment the weight w(x) of every variable x ∈X(C). However, for a small
set of constraints (AllDifferent, Element, and Linear Inequality) for
which we have implemented an algorithm for computing an explanation, we
update the weight only of the variables involved in the explanation.
There is one diﬀerence with wdeg concerning inactive constraints, i.e., with
at most one variable currently unassigned. The weight of an inactive constraint
does not contribute to the selection of its last variable. We decided to ignore
this, and we count the weight of active and inactive constraint alike as it did
not appear to be critical. However, one can implement this easily by keeping, for
each constraint C and each variable x ∈X(C), the quantity of weight w(C, x)
due to C. When a constraint becomes inactive and its last unassigned variable
is x, then w(C, x) is subtracted from w(x), and added back upon backtrack.
Notice that since we use these conﬂicts for informing the heuristic only, they
do not actually need to be minimal if, for instance, extracting a minimal conﬂict
is computationally hard for the constraint. Similarly, the explanation does not
even need to be correct. The three very straightforward procedures that we
implemented and described below produce explanations that are correct but not

Explanation-Based Weighted Degree
171
necessarily minimal. We chose to use fast and easy to implement explanations
and found that it was suﬃcient to improve the behavior of wdeg:
AllDifferent(⟨x1, . . . , xk⟩) ⇔∀1 ≤i < j ≤k, xi ̸= xj, where ⟨x1, . . . , xk⟩
is a tuple of integer variables.
We used in our experiment two propagators for that constraint. First, with
highest priority, arc consistency is achieved on binary inequalities. If a failure is
obtained when processing the inequality xi ̸= xj, it means that both variables
are ground and equal, hence we use the conﬂict set {xi, xj}.
Then, with lower priority, we use the bounds consistency propagation algo-
rithm described in [8]. This algorithm fails if it ﬁnds a Hall interval, i.e., a set
of at least b −a + 2 variables whose domains are included in {a, . . . , b}. When
this happens we simply use this set of variables as conﬂict.
Linear Inequality(⟨x1, . . . , xk⟩, ⟨a1, . . . , ak⟩, b) ⇔k
i=1 aixi ≤b, where b
is an integer, ⟨x1, . . . , xk⟩a tuple of integer variables and ⟨a1, . . . , ak⟩of integers.
The constraint fails if and only if the lower bound of the sum is strictly larger
than b. When this is true, a possible conﬂict is the set containing every variable
xi such that either ai is positive and min(D(xi)) is strictly larger than its initial
value, or ai is negative and max(D(xi)) is strictly lower than its initial value.
Element(⟨x1, . . . , xk⟩, n, v) ⇔xn = v, where ⟨x1, . . . , xk⟩is a tuple of inte-
ger variables, n, v two integer variables.
We use the conﬂict set {n, v} ∪{xi | i ∈D(n)}, that is, we put weight only
on the “index” and “value” variables n and v, as well as every variable of the
array pointed to by the index variable n.
5
Experimental Evaluation
Fig. 1a illustrates the diﬀerence that explanation can make. The data comes from
instance 3-10-20.dzn of the ghoulomb.mzn model. We plot, for each variable
x, the number of failures of a constraints involving x (blue crosses) and the
number of explanations of failures involving x (red dot). We observe a much
wider distribution of the weight when using explanations. As a result, the optimal
solution could be proven with e-wdeg in 15 s whereas the best upper bound found
with wdeg after 1200 s was 1.83 times larger than the optimal.
Next, we experimentally evaluated the proposed variant of wdeg, denoted
e-wdeg, against the state-of-the-art general-purpose branching heuristics wdeg,
ABS, IBS, COS and LC with wdeg and e-wdeg as default heuristic for the two
latter. We used lexicographic value ordering for every heuristic, except IBS and
ABS since their default branching strategies were slightly better.3 The decay
factor γ was set to 0.95 for ABS, wdeg and e-wdeg, and the bias α was set
to 8 for ABS and IBS. No probes were used to initialize weights. In all cases
the initial values were set up to emulate the “minimum domain over degree”
strategy. All the methods were implemented in Mistral-2.0 [5] and the same
geometric restarts policy was used in all cases.
3 The extra space requirement was an issue for 5 optimization instances. However the
impact on the overall results is quasi null.

172
E. Hebrard and M. Siala
(a) Weight distribution
0
50
100
150
200
250
100
101
102
103
104
105
Variable x
Involved in y conﬂicts
Constraint’s Scope
Explanation
(b) Ratio of solved instances
0.2
0.4
0.6
0.8
0
500
1,000
1,500
Solved ratio
CPU Time
e-wdeg
LC (e-wdeg)
wdeg
LC (wdeg)
ABS
IBS
COS (wdeg)
COS (e-wdeg)
Fig. 1. Weight distribution & search eﬃciency on satisfaction instances (Color ﬁgure
online)
We used all the instances of the Minizinc challenge from 2012 to 2015 [15].
This data set contains 399 instances, 323 are optimization problems and 76 are
satisfaction problems. In this set, 72 instances have at least one AllDifferent,
147 have at least one Element and all have at least one Linear Inequality.
However, Element is often posted on arrays of constants and LinearInequal-
ity may be used to model clauses, which is not favorable to e-wdeg since the
explanations are trivial. All the tests ran on Intel Xeon E5430 processors with
Linux. The time cutoﬀwas 1500 s for each instance excluding the parsing time.
Heuristics were randomized by choosing uniformly between the two best choices,
except COS and LC for which only the default heuristics were randomized in
the same way. Each conﬁguration was given 5 randomized runs.
5.1
Satisfaction Problems
We ﬁrst report the results on satisfaction instances, where we plot, for every
heuristic, the ratio of runs (among 76 × 5) in which the instance was solved
(x-axis) over time (y-axis) in Fig. 1b. The results are clear. Among previous
heuristics, wdeg is very eﬃcient, only outperformed by LC (using wdeg as default
heuristic when the testing set is empty). Conﬂict Ordering Search comes next,
followed by IBS and ABS. Notice that IBS and ABS are often initialised using
probing. However, this method could be used for other heuristics (see [3]), and
is unlikely to be suﬃcient to close this gap. Whether used as default for COS
or LC, or as stand alone, e-wdeg always solves more instances than wdeg. The
diﬀerence is not signiﬁcant below a 500 s time limit, but patent above this mark.
Overall, e-wdeg solves 4.5%, 3.7% and 2.9% more instances than wdeg when
used as default for LC, COS or as stand alone, respectively.

Explanation-Based Weighted Degree
173
5.2
Optimization Problems
Second, we report the results for the 323 optimization instances (1615 runs). We
ﬁrst plot, for every heuristic, the ratio of instances proven optimal (x-axis) over
time (y-axis) in Fig. 2a. Here the gain of explanation-based weights is less clear.
e-wdeg can prove 2.3% more instances, however LC (e-wdeg) ﬁnds only 0.6%
more proofs than LC and COS (e-wdeg) 1.6% more than COS.
(a) Number of proofs
0.1
0.2
0.3
0.4
0
500
1,000
1,500
Optimality ratio
CPU Time
e-wdeg
LC (e-wdeg)
wdeg
LC (wdeg)
ABS
IBS
COS (wdeg)
COS (e-wdeg)
(b) Objective value
0.2
0.4
0.6
0.8
0
500
1,000
1,500
Objective ratio
CPU Time
e-wdeg
LC (e-wdeg)
wdeg
LC (wdeg)
ABS
IBS
COS (wdeg)
COS (e-wdeg)
Fig. 2. Search eﬃciency, optimization instances
In Fig. 2b we plot the normalized objective value of the best solution found
by heuristic h (x-axis) after a given time (y-axis). Let h(I) be the objective
value of the best solution found using heuristic h on instance I and lb(I) (resp.
ub(I)) the lowest (resp. highest) objective value found by any heuristic on I.
The formula below gives a normalized score in the interval [0, 1]:
score(h, I) =
⎧
⎨
⎩
h(I)−lb(I)+1
ub(I)−lb(I)+1,
if I is a maximization instance
ub(I)−h(I)+1
ub(I)−lb(I)+1,
otherwise
Notice that we add 1 to the actual and maximum gap. Moreover, if an heuris-
tic h does not ﬁnd any feasible solution for instance I, we arbitrarily set h(I) to
lb(I)−1 for a maximization problem, and ub(I)+1 for a minimization problem.
It follows that score(h, I) is equal to 1 if h has found the best solution for this
instance among all heuristics, decreases as h(I) gets further from the optimal
objective value, and is equal to 0 if and only if h did not ﬁnd any solution for I.
We observe that using e-wdeg, the solver ﬁnds signiﬁcantly better solutions
faster than using wdeg. The same observation can be made for Last Conﬂict
and Conﬂict Ordering Search using e-wdeg outperforming their counterparts
relying on wdeg, although the gap is slightly less important in these cases. The
overall improvement (respectively 3.2%, 1.6% and 0.8%) is modest, however, the
number of data points makes it statistically signiﬁcant.

174
E. Hebrard and M. Siala
(a) Objective
0
0.2
0.4
0.6
0.8
0
500
1,000
1,500
CPU Time
e-wdeg
LC (e-wdeg)
wdeg
LC (wdeg)
ABS
IBS
COS (wdeg)
COS (e-wdeg)
(b) Optimality
0
0.1
0.2
0.3
0.4
(c) Satisfaction
0
0.2
0.4
0.6
0.8
Fig. 3. Search eﬃciency, branching on auxiliary variables
Overall, the wdeg heuristic is clearly very competitive when considering
a large sample of instances, and indeed it dominates IBS and ABS on the
MiniZinc instances. Last Conﬂict and Conﬂict Ordering Search seem even more
eﬃcient, however, they rely on wdeg as default heuristic. These experiments
show that computing speciﬁc conﬂict sets for constraints signiﬁcantly boosts
wdeg. Although the gain is less straightforward in the case of LC and COS
(especially for the latter which relies less heavily on the default heuristic), this
approach can be useful in those cases too, and in any case never hinders search
eﬃciency.
Notice that we restricted the heuristic selection to the decision variables
speciﬁed in the MiniZinc model. We also tried to let the heuristic branch on
auxiliary variables, created internally to model expressions. The results, shown
in Fig. 3 are not exactly as cleanly cut in this case, and actually diﬃcult to
understand. The only two heuristics to actually beneﬁt from this are wdeg and
e-wdeg, by 6.7% and 8.3%, respectively for the objective value criteria. Two
heuristics perform much worse, again for the objective value: ABS loses 15.2%
and COS (e-wdeg) 4.7%. Other heuristics are all marginally worse in this setting.
On other criteria, such as the number of optimality proofs, there is a similar,
but not identical trend, with LC gaining the most from the extra freedom.
6
Conclusion
We showed that the wdeg heuristic can be made more robust to instances with
large arity constraints through a relatively simple method. Whereas wdeg dis-
tributes weights equally among all variables of the constraint that triggered a
failure, we propose to weight only the variables of that constraint participating
in an explanation of the failure. Our empirical analysis shows that this technique
improves the performance of wdeg. In particular, the Last Conﬂict heuristic [7]
using the improved version of Weighted Degree, e-wdeg, is the most eﬃcient
overall on the benchmarks from the previous MiniZinc challenges.

Explanation-Based Weighted Degree
175
References
1. Boussemart, F., Hemery, F., Lecoutre, C., Sais, L.: Boosting systematic search
by weighting constraints. In: Proceedings of the 16th European Conference on
Artiﬁcial Intelligence - ECAI, pp. 146–150 (2004)
2. Gay, S., Hartert, R., Lecoutre, C., Schaus, P.: Conﬂict ordering search for schedul-
ing problems. In: Pesant, G. (ed.) CP 2015. LNCS, vol. 9255, pp. 140–148. Springer,
Cham (2015). doi:10.1007/978-3-319-23219-5 10
3. Grimes, D., Wallace, R.J.: Sampling strategies and variable selection in weighted
degree heuristics. In: Bessi`ere, C. (ed.) CP 2007. LNCS, vol. 4741, pp. 831–838.
Springer, Heidelberg (2007). doi:10.1007/978-3-540-74970-7 61
4. Haralick, R.M., Elliott, G.L.: Increasing tree search eﬃciency for constraint sat-
isfaction problems. In: Proceedings of the 6th International Joint Conference on
Artiﬁcial Intelligence - IJCAI, pp. 356–364 (1979)
5. Hebrard, E.: Mistral, a constraint satisfaction library. In: Proceedings of the CP-08
Third International CSP Solvers Competition, pp. 31–40 (2008)
6. Bayardo Jr., R., Schrag, R.C.: Using CSP look-back techniques to solve real-world
SAT instances. In: Proceedings of the 14th National Conference on Artiﬁcial Intel-
ligence - AAAI, pp. 203–208 (1997)
7. Christophe, L., Sas, L., Tabary, S., Vidal, V.: Reasoning from last conﬂict(s) in
constraint programming. Artif. Intell. 173(18), 1592–1614 (2009)
8. Lopez-Ortiz, A., Quimper, C.G., Tromp, J., Beek, P.V.: A fast and simple algo-
rithm for bounds consistency of the all diﬀerent constraint. In: Proceedings of the
18th International Joint Conference on AI - IJCAI, pp. 245–250 (2003)
9. Michel, L., Hentenryck, P.: Activity-based search for black-box constraint pro-
gramming solvers. In: Beldiceanu, N., Jussien, N., Pinson, ´E. (eds.) CPAIOR
2012. LNCS, vol. 7298, pp. 228–243. Springer, Heidelberg (2012). doi:10.1007/
978-3-642-29828-8 15
10. Moskewicz, M.W., Madigan, C.F., Zhao, Y., Zhang, L., Malik, S.: Chaﬀ: engineer-
ing an eﬃcient SAT solver. In: Proceedings of the 38th Annual Design Automation
Conference - DAC, pp. 530–535 (2001)
11. Refalo, P.: Impact-based search strategies for constraint programming. In: Wallace,
M. (ed.) CP 2004. LNCS, vol. 3258, pp. 557–571. Springer, Heidelberg (2004).
doi:10.1007/978-3-540-30201-8 41
12. Schiex, T., Verfaillie, G.: Nogood recording for static and dynamic constraint sat-
isfaction problems. In: ICTAI, pp. 48–55 (1993)
13. Silva, J.P.M., Sakallah, K.A.: GRASP - a new Search algorithm for satisﬁability.
In: Proceedings of the IEEE/ACM International Conference on Computer-aided
Design - ICCAD, pp. 220–227 (1996)
14. Silva, M.J.P., Sakallah, K.A.: GRASP: a search algorithm for propositional satis-
ﬁability. IEEE Trans. Comput. 48(5), 506–521 (1999)
15. Stuckey, P.J., Feydy, T., Schutt, A., Tack, G., Fischer, J.: The minizinc challenge
2008–2013. AI Mag. 35(2), 55–60 (2014)
16. Wallace, R.J., Grimes, D.: Experimental studies of variable selection strategies
based on constraint weights. J. Algorithms 63(1), 114–129 (2008)

Counting Weighted Spanning Trees to Solve
Constrained Minimum Spanning Tree Problems
Antoine Delaite1 and Gilles Pesant1,2(B)
1 ´Ecole Polytechnique de Montr´eal, Montreal, Canada
{antoine.delaite,gilles.pesant}@polymtl.ca
2 CIRRELT, Universit´e de Montr´eal, Montreal, Canada
Abstract. Building on previous work about counting the number of
spanning trees of an unweighted graph, we consider the case of edge-
weighted graphs. We present a generalization of the former result to
compute in pseudo-polynomial time the exact number of spanning trees
of any given weight, and in particular the number of minimum span-
ning trees. We derive two ways to compute solution densities, one of
them exhibiting a polynomial time complexity. These solution densities
of individual edges of the graph can be used to sample weighted spanning
trees uniformly at random and, in the context of constraint programming,
to achieve domain consistency on the binary edge variables and, more
importantly, to guide search through counting-based branching heuris-
tics. We exemplify our contribution using constrained minimum spanning
tree problems.
1
Introduction
Counting-based search [16] in cp relies on computing the solution density
of each variable-value assignment for a constraint in order to build an inte-
grated variable-selection and value-selection heuristic to solve constraint sat-
isfaction problems. Given a constraint c(x1, . . . , xℓ), its number of solutions
#c(x1, . . . , xℓ), respective ﬁnite domains Di 1≤i≤ℓ, a variable xi in the scope
of c, and a value d ∈Di, we call
σ(xi, d, c) = #c(x1, . . . , xi−1, d, xi+1, . . . , xℓ)
#c(x1, . . . , xℓ)
the solution density of pair (xi, d) in c. It measures how often a certain assign-
ment is part of a solution to c. We can exploit the combinatorial structure
of the constraint to design eﬃcient algorithms computing solution densities.
Such algorithms have already been designed for several families of constraints
such as alldifferent, gcc, regular, and knapsack [12], dispersion [10], and
spanningTree for unweighted graphs [3].
When faced with optimization problems however, solutions should not be
considered on an equal footing since they will have diﬀerent costs. Let f :D→R
c
⃝Springer International Publishing AG 2017
D. Salvagnin and M. Lombardi (Eds.): CPAIOR 2017, LNCS 10335, pp. 176–184, 2017.
DOI: 10.1007/978-3-319-59776-8 14

Counting Weighted Spanning Trees
177
(with
D = D1 × · · · × Dℓ) associate a cost to each combination of val-
ues for variables x1, . . . , xℓand z ∈[mint∈D f(t), maxt∈D f(t)] be a bounded-
domain continuous variable. An optimization constraint c⋆(x1, . . . , xℓ, z, f) holds
if c(x1, . . . , xℓ) is satisﬁed and z = f(x1, . . . , xℓ). This is a useful concept if the
objective function of the problem can be expressed using the z variables of
some optimization constraints (or, even better, a single one). Without loss of
generality consider minimization problems. Pesant [11] recently generalized the
concept of solution density to that of cost-based solution density. Let ϵ ≥0 be
a small real number and #c⋆
ϵ(x1, . . . , xℓ, z, f) denote the number of solutions to
c⋆(x1, . . . , xℓ, z, f) with z ≤(1 + ϵ) · mint∈c(x1,...,xℓ) f(t). We call
σ⋆(xi, d, c⋆, ϵ) = #c⋆
ϵ(x1, . . . , xi−1, d, xi+1, . . . , xℓ, z, f)
#c⋆ϵ(x1, . . . , xℓ, z, f)
the cost-based solution density of pair (xi, d) in c⋆. A value of ϵ = 0 corresponds
to the solution density over the optimal solutions to the constraint with respect
to f and if there is a single optimal solution then this identiﬁes the correspond-
ing assignment to xi with a solution density of 1. A positive ϵ gives a mar-
gin to include close-to-optimal solutions since there are likely other constraints
to satisfy that will rule out the optimal ones with respect to that single opti-
mization constraint. Algorithms to compute cost-based solution densities have
already been given for the optimization versions of the alldifferent, regular,
and dispersion constraints [11]. In this spirit we present an algorithm that,
given a weighted graph, computes the exact number of spanning trees of any
given weight and we contribute cost-aware solution densities for a spanningTree
constraint.
Section 2 reviews related work on tree constraints in cp. Section 3 presents our
counting algorithm for weighted spanning trees. Section 4 derives ways to com-
pute solution densities eﬃciently. Section 5 gives applications of solution density
information. Section 6 reports on the usefulness of this work to solve constrained
minimum spanning tree problems.
2
Related Work
Previous research in cp on imposed tree structures has focused mostly on ﬁltering
algorithms but also includes branching heuristics and explanations. Beldiceanu
et al. [2] introduce the tree constraint, which addresses the digraph partitioning
problem from a constraint programming perspective. In their work a constraint
that enforces a set of vertex-disjoint anti-arborescences is proposed. They achieve
domain consistency in O(nm) time, where n is the number of vertices and m
is the number of edges in the graph. Dooms and Katriel [6] introduce the MST
constraint, requiring the tree variable to represent a minimum spanning tree
of the graph on which the constraint is deﬁned. The authors propose polytime
bound consistent ﬁltering algorithms for several restrictions of this constraint.
Later Dooms and Katriel [7] propose a weighted spanning tree constraint, in
which both the tree and the weight of the edges are variables, and consider

178
A. Delaite and G. Pesant
several ﬁltering algorithms. The latter is improved by R´egin [13], who proposes
an incremental domain consistency algorithm running in O(m + n log n) time.
Subsequently, R´egin et al. [14] improve the time complexity further and also con-
sider mandatory edges. De U˜na et al. [5] generate explanations for cost-based
failure and ﬁltering that occurs in the weighted spanning tree constraint and
feed them to a sat solver. They show empirically that such an approach greatly
improves our ability to solve diameter-constrained minimum spanning tree prob-
lems in cp. Brockbank et al. [3] build on Kirchhoﬀ’s Matrix-Tree Theorem to
derive solution densities for the edges of an unweighted graph and use them
in a counting-based branching heuristic. In the next sections we generalize this
approach for edge-weighted graphs.
3
Counting Weighted Spanning Trees
In the case of an unweighted graph G on n vertices and m edges Kirchhoﬀ’s
Matrix-Tree Theorem [15] equates the number of spanning trees of G to the
absolute value of the determinant of a sub-matrix (the (i, j)-minor) of the Lapla-
cian matrix of G, obtained by subtracting the adjacency matrix of G from the
diagonal matrix whose ith entry is equal to the degree of vertex vi in G. Hence
the number of spanning trees can be computed as the determinant of a (n −1)
× (n −1) matrix, in O(n3) time.
Let G now be an edge-weighted multigraph with vertex set V, edge set E, and
weight function w : E →N. Following in Kirchhoﬀ’s footsteps Broder and Mayr
[4] introduce a derived matrix M = (mij)1≤i,j≤n whose elements are univariate
polynomials built from the edges of G:1
mij =

e=(vi,vj)∈E −xw(e),
i ̸= j

e=(vi,vk)∈E
xw(e),
i = j
(1)
Oﬀ-diagonal entries mij add one monomial per edge between distinct vertices
vi and vj (there may be several since we consider multigraphs) whereas entries
mii on the diagonal add one monomial per edge incident with vertex vi.
Figure 1 shows a small graph and its derived matrix M. Observe that if all
weights are 0 (or alternatively if we set x to 1) matrix M simpliﬁes to the previ-
ously deﬁned Laplacian matrix: it is thus a true generalization of the unweighted
case. If we remove the ith row and column of M for any i and compute the deter-
minant of the remaining matrix, we obtain a unique polynomial with very useful
characteristics: each one of its monomials akxk indicates the number ak of span-
ning trees of weight k. For example say we remove the ﬁrst row and column of
M in Fig. 1 and compute the determinant:

x2 + 2x1 −x1
−x1
0
−x1
2x1
−x1
0
−x1
−x1 2x3 + 2x1
−x3
0
0
−x3
x1 + x3

= 2x9 + 3x8 + 7x7 + 6x6 + 3x5
1 We generalize slightly their deﬁnition in order to include multigraphs, which may
occur when we contract edges in the context of cp search.

Counting Weighted Spanning Trees
179
v1
v2
v5
v4
v3
2
3
1
1
1
1
3
M =
⎛
⎜
⎜
⎜
⎜
⎝
x2 + x3 + x1
−x2
0
−x3
−x1
−x2
x2 + 2x1 −x1
−x1
0
0
−x1
2x1
−x1
0
−x3
−x1
−x1 2x3 + 2x1
−x3
−x1
0
0
−x3
x1 + x3
⎞
⎟
⎟
⎟
⎟
⎠
Fig. 1. A graph and its derived matrix.
Thus we discover that there are two spanning trees of weight 9, three of weight 8,
seven of weight 7, six of weight 6, and three of (minimum) weight 5. We will call
this polynomial the wst-polynomial of G and denote it PG(x). It directly gives
us the number of spanning trees of each weight though computing it requires
Θ(n3W max(n, W)) time in the worst case, where W = max{w(e) : e ∈E} [8].
Thus it is dependent on the magnitude of the edge weights.
Broder and Mayr [4] were interested in improving the eﬃciency of counting
the number of minimum spanning trees, say of weight k⋆, and for that purpose
they provide an algorithm that, guided by an arbitrary spanning tree, takes linear
combinations of some of the columns in order to factor out the corresponding
power xk⋆, and then evaluates the remaining determinant at x = 0 thus isolating
the coeﬃcient ak⋆. The time complexity of their algorithm is the same as that
of matrix multiplication (for n × n matrices).
4
Computing Cost-Aware Solution Densities
We are interested in counting spanning trees because they are instrumental in
establishing the solution density of an edge e ∈E given a spanningTree con-
straint on G. A natural way to go about this is to divide the number of spanning
trees using that edge by the total number of spanning trees. We ﬁrst discuss how
to compute cost-based solution densities exactly and then present an adaptation
of that concept which allows us to lower the time complexity signiﬁcantly.
4.1
Exact Cost-Based Solution Densities
What is the solution density of a given edge e among all spanning trees of weight
k? We can compute the diﬀerence between the wst-polynomial of G and that
of the same graph without that edge, PG(x) −PG−e(x). The result is another
polynomial giving the number of spanning trees of each weight that use e. From
it we can extract the number of spanning trees of weight k which contain e and
the corresponding solution densities.
Consider Fig. 2 in which we removed edge (v2, v3) from G. We have
PG−(v2,v3)(x) = x9 + x8 + 3x7 + 2x6 + x5
and therefore

180
A. Delaite and G. Pesant
v1
v2
v5
v4
v3
2
3
1
1
1
3
M ′ =
⎛
⎜
⎜
⎜
⎜
⎝
x2 + x3 + x1
−x2
0
−x3
−x1
−x2
x2 + x1
0
−x1
0
0
0
x1
−x1
0
−x3
−x1
−x1 2x3 + 2x1
−x3
−x1
0
0
−x3
x1 + x3
⎞
⎟
⎟
⎟
⎟
⎠
Fig. 2. The graph at Fig. 1 with edge (v2, v3) removed and its derived matrix.
PG(x) −PG−(v2,v3)(x) = x9 + 2x8 + 4x7 + 4x6 + 2x5.
So for example edge (v2, v3) is used in four spanning trees of weight 7 and the
corresponding solution density is
4
7 since according to PG(x) there are seven
spanning trees of that weight. For cost-based solution densities, we need to take
into account the trees of weight at most (1 + ϵ)k⋆. In our example k⋆= 5 and
say we take ϵ = 0.2: there are 4 + 2 = 6 spanning trees of weight at most
(1 + 0.2) · 5 = 6 using edge (v2, v3) and 6 + 3 = 9 such trees according to PG(x)
(using that edge or not), yielding a cost-based solution density equal to 6
9, or 2
3.
4.2
Cost-Damped Solution Densities
Computing solution densities via wst-polynomials may be too time-consuming
for some uses such as in a branching heuristic that is called at every node of
the search tree. The success of Broder and Mayr [4] in eﬃciently computing the
number of minimum spanning trees came in large part from the evaluation of the
determinant for a ﬁxed x, thus working with scalar values instead of polynomials.
Consider applying an exponential decay of sorts to the number ak of span-
ning trees of weight k according to the diﬀerence between that weight and
that of a minimum spanning tree, k⋆, thus giving more importance to close-
to-optimal trees. We do this by choosing an appropriate value for x. Con-
sider the wst-polynomial PG(x) = kmax
k=k⋆akxk: evaluating it at x = 1 yields
PG(1) = kmax
k=k⋆ak, the total number of spanning trees regardless of their weight.
Using instead some 0 < x < 1 will have the desired damping eﬀect on the coeﬃ-
cients by applying an additional xk−k⋆factor to ak relative to ak⋆— the further
weight k is from the minimum weight, the smaller the factor. As an illustra-
tion if we apply this approach to the example of the previous section with any
x ∈[0.3, 0.9] we get a cost-damped solution density in the range [0.62, 0.66]
(compared to 2
3 with the previous approach).
To build our matrix M we need to elevate x up to the W th power, which can
be done in Θ(log W), and add the appropriate power in M for each edge, taking
Θ(m log W) time overall. Then we have a scalar matrix from which we proceed as
in [3], requiring Θ(n3) time to compute the solution density of all the edges inci-
dent to a given vertex. So the overall time is in O(n2 max(n, log W)) globally for
these incident edges, which is truly polynomial, compared to Θ(n3W max(n, W))

Counting Weighted Spanning Trees
181
for a single edge if we work directly with the wst-polynomials as described in
Sect. 4.1, and which is pseudo-polynomial. And cost-damped solution densities
can be updated incrementally as edges are selected or removed from G, in Θ(n2)
time instead of Θ(n3) (see [3]).
5
Applications of Solution Densities
Filtering. As reviewed in Sect. 2 several ﬁltering algorithms were previously pro-
posed for spanning tree constraints. But note that if we already spend time
computing solution densities we can achieve domain consistency at no further
expense: a solution density of 0 means that the edge can be ﬁltered; a solution
density of 1 means that the edge can be ﬁxed. This holds as well for cost-damped
solution densities since every solution is still taken into account. With cost-based
solution densities we can apply cost-based ﬁltering.
Sampling. The ability to compute solution densities exactly allows us to sample
uniformly at random the combinatorial structures on which they are computed.
In particular if we compute cost-based solution densities on spanning trees of
weighted graphs, we can sample spanning trees of a given weight. We simply
consider one edge at a time, deciding whether to include it or not according to
its solution density (viewed as a probability), and then updating the solution
densities accordingly.
Branching. The initial motivation for this work was to provide solution-counting
information from constraints in order to branch using a heuristic relying on
such information (i.e. counting-based search [12]). Among them, maxSD and its
optimization counterpart, maxSD⋆, select the variable-value pair with the highest
solution density among all constraints. In the next section we report on some
experiments using the latter.
6
Experiments
To demonstrate the eﬀectiveness of using solution density information from a
spanning tree optimization constraint to guide a branching heuristic, we use it
to solve some degree-constrained and diameter-constrained minimum spanning
tree problems. Our graph instances are the same as [5], with a number of vertices
ranging from 15 to 40 and an edge density ranging from 0.13 to 1 (i.e. a complete
graph). We used the IBM ILOG CP v1.6 solver for our implementation and
performed our experiments on a AMD Opteron 2.2 GHz with 1 GB of memory.
We applied a 20-minute timeout throughout our experiments.
We introduce one binary variable xe ∈{0, 1} per edge of the graph and a cost
variable k corresponding to the weight of the spanning tree, which we minimize.
On these we post optimization constraint spanningTree(G, {xe : e ∈E}, k, w)
equipped to answer queries about solution densities and enforcing

182
A. Delaite and G. Pesant

e∈E
w(e) · xe = k

e∈E
xe = n −1

e=(v,v′)∈E
xe ≥1
∀v ∈V
σ⋆(xe, 1, spanningTree) = 1
⇒
xe = 1
∀e ∈E
σ⋆(xe, 1, spanningTree) = 0
⇒
xe = 0
∀e ∈E
The last two constraints are only present if we use counting-based search and
hence need solution densities. We omit parameter ϵ in σ⋆because it is not rele-
vant for cost-damped solution densities. We also compute a lower bound at each
node of the search tree by running Kruskal’s minimum spanning tree algorithm
on G updated to reﬂect the edges that have been decided.
We compare counting-based branching heuristic maxSD⋆fed with cost-
damped solution densities (using x = 0.9; the algorithm is not particularly sen-
sitive to that choice) to a reasonable dedicated heuristic that selects edges by
increasing weight.
6.1
Degree-Constrained Minimum Spanning Tree Problem
The Degree-Constrained MST Problem requires that we ﬁnd a minimum span-
ning tree of G whose vertices have degree at most d [9]. We add to our model

e=(v,v′)∈E
xe ≤d
∀v ∈V
We attempt to solve to optimality the previously-mentioned instances with d =
2, 3, and 4. Our results are summarized at Fig. 3: the dedicated heuristic performs
almost as well as maxSD⋆on the d = 2 instances (which corresponds to ﬁnding a
minimum Hamiltonian path) but it is clearly outperformed on the larger degree
bounds.
6.2
Diameter-Constrained Minimum Spanning Tree Problem
The Diameter-Constrained MST Problem requires that we ﬁnd a minimum span-
ning tree of G such that for any two vertices the path joining them has at most p
edges [1]. So in addition to the spanningTree constraint we maintain incremen-
tally the length of a shortest path between each pair of vertices and backtrack
if any exceed p. We attempt to solve to optimality the previously-mentioned
instances with a diameter p ranging from 4 to 10. Our results are summarized
at Fig. 4. The heuristic selecting edges by increasing weight did not solve any
instance so it does not appear in the ﬁgure. maxSD⋆manages to solve about
half of the instances but is not competitive in terms of computation time and
eﬀectiveness with EXPL, the explanation-generating approach of [5], though on

Counting Weighted Spanning Trees
183
Fig. 3. Percentage of Degree-Constrained MST instances solved to optimality after a
given time per instance.
Fig. 4. Percentage of Diameter-Constrained MST instances solved to optimality after
a given time (left) or size of search tree (right) per instance.
some instances it yields smaller search trees. Here the cp models are also quite
diﬀerent: the other one is tailored to the Diameter-Constrained MST with parent
and height variables, and a dedicated branching heuristic.
7
Conclusion
We presented new algorithms to compute cost-aware solution densities for span-
ning tree constraints on weighted graphs and showed how they can be used in
counting-based branching heuristics to improve our ability to solve constrained
minimum spanning tree problems in cp. Such solution densities are also useful
for domain ﬁltering and uniform sampling.
Financial support for this research was provided by NSERC Grant
218028/2012.

184
A. Delaite and G. Pesant
References
1. Achuthan, N.R., Caccetta, L., Caccetta, P., Geelen, J.F.: Algorithms for the min-
imum weight spanning tree with bounded diameter problem. Optim. Tech. Appl.
1(2), 297–304 (1992)
2. Beldiceanu, N., Flener, P., Lorca, X.: The tree constraint. In: Bart´ak, R., Milano,
M. (eds.) CPAIOR 2005. LNCS, vol. 3524, pp. 64–78. Springer, Heidelberg (2005).
doi:10.1007/11493853 7
3. Brockbank,
S.,
Pesant,
G.,
Rousseau,
L.-M.:
Counting
spanning
trees
to
guide search in constrained spanning tree problems. In: Schulte, C. (ed.) CP
2013. LNCS, vol. 8124, pp. 175–183. Springer, Heidelberg (2013). doi:10.1007/
978-3-642-40627-0 16
4. Broder, A.Z., Mayr, E.W.: Counting minimum weight spanning trees. J. Algo-
rithms 24(1), 171–176 (1997)
5. de U˜na, D., Gange, G., Schachte, P., Stuckey, P.J.: Weighted spanning tree con-
straint with explanations. In: Quimper, C.-G. (ed.) CPAIOR 2016. LNCS, vol.
9676, pp. 98–107. Springer, Cham (2016). doi:10.1007/978-3-319-33954-2 8
6. Dooms, G., Katriel, I.: The Minimum Spanning Tree constraint. In: Benhamou, F.
(ed.) CP 2006. LNCS, vol. 4204, pp. 152–166. Springer, Heidelberg (2006). doi:10.
1007/11889205 13
7. Dooms, G., Katriel, I.: The “Not-Too-Heavy Spanning Tree” constraint. In: Hen-
tenryck, P., Wolsey, L. (eds.) CPAIOR 2007. LNCS, vol. 4510, pp. 59–70. Springer,
Heidelberg (2007). doi:10.1007/978-3-540-72397-4 5
8. Hromˇc´ık, M., ˇSebek, M.: New algorithm for polynomial matrix determinant based
on FFT. In: Proceedings of the 5th European Control Conference (ECC99), Sep-
tember 1–3, Karlsruhe, Germany, (1999)
9. Subhash, S.C., Ho, C.A.: Degree-constrained minimum spanning tree. Comput.
Oper. Res. 7(4), 239–249 (1980)
10. Pesant, G.: Achieving domain consistency and counting solutions for dispersion
constraints. INFORMS J. Comput. 27(4), 690–703 (2015)
11. Pesant, G.: Counting-based search for constraint optimization problems. In: Schu-
urmans, D., Wellman, M.P. (eds.) AAAI, pp. 3441–3448. AAAI Press, Palo Alto
(2016)
12. Pesant, G., Quimper, C.-G., Zanarini, A.: Counting-based search: branching heuris-
tics for constraint satisfaction problems. J. Artif. Intell. Res. 43, 173–210 (2012)
13. R´egin, J.-C.: Simpler and incremental consistency checking and arc consistency
ﬁltering algorithms for the weighted spanning tree constraint. In: Perron, L., Trick,
M.A. (eds.) CPAIOR 2008. LNCS, vol. 5015, pp. 233–247. Springer, Heidelberg
(2008). doi:10.1007/978-3-540-68155-7 19
14. R´egin, J.-C., Rousseau, L.-M., Rueher, M., Hoeve, W.-J.: The weighted span-
ning tree constraint revisited. In: Lodi, A., Milano, M., Toth, P. (eds.) CPAIOR
2010. LNCS, vol. 6140, pp. 287–291. Springer, Heidelberg (2010). doi:10.1007/
978-3-642-13520-0 31
15. Tutte, W.T.: Graph Theory. Encyclopedia of Mathematics and Its Applications.
Cambridge University Press, New york (2001)
16. Zanarini, A., Pesant, G.: Solution counting algorithms for constraint-centered
search heuristics. In: Bessi`ere, C. (ed.) CP 2007. LNCS, vol. 4741, pp. 743–757.
Springer, Heidelberg (2007). doi:10.1007/978-3-540-74970-7 52

The Weighted Arborescence Constraint
Vinasetan Ratheil Houndji1,2(B), Pierre Schaus1,
Mahouton Norbert Hounkonnou2, and Laurence Wolsey1
1 Universit´e catholique de Louvain, Louvain-la-Neuve, Belgium
{vinasetan.houndji,pierre.schaus,laurence.wolsey}@uclouvain.be,
ratheil.houndji@ifri.uac.bj
2 Universit´e d’Abomey-Calavi, Abomey-Calavi, Benin
norbert.hounkounnou@cipma.uac.bj
Abstract. For a directed graph, a Minimum Weight Arborescence
(MWA) rooted at a vertex r is a directed spanning tree rooted at r
with the minimum total weight. We deﬁne the MinArborescence con-
straint to solve constrained arborescence problems (CAP) in Constraint
Programming (CP). A ﬁltering based on the LP reduced costs requires
O(|V |2) where |V | is the number of vertices. We propose a procedure to
strengthen the quality of the LP reduced costs in some cases, also run-
ning in O(|V |2). Computational results on a variant of CAP show that
the additional ﬁltering provided by the constraint reduces the size of the
search tree substantially.
1
Introduction
In graph theory, the problem of ﬁnding a Minimum Spanning Tree (MST) [13]
is one of the most known problem for undirected graphs. The corresponding
version for directed graphs is called Minimum Directed Spanning Tree (MDST)
or Minimum Weight Arborescence (MWA). It is well known that the graphs
are good structures to model some real life problems. The MWA problem has
many practical applications in telecommunication networks, computer networks,
transportation problems, scheduling problems, etc. It can also be considered as a
subproblem in many routing and scheduling problems [9]. For example, Fischetti
and Toth ([8]) used MWA problem as a relaxation of Asymmetric Travelling
Salesman Problem (ATSP).
Let us formally deﬁne the MWA problem. Consider a directed graph G =
(V, E) in which V = {v1, v2, . . . , vn} is the vertex set and E = {(i, j) : i, j ∈V } is
the edge set. We associate a weight w(i, j) to each edge (i, j) and we distinguish
one vertex r ∈V as a root. An arborescence A rooted at r is a directed spanning
tree rooted at r. So A is a spanning tree of G if we ignore the direction of
edges and there is a directed path in A from r to each other vertex v ∈V .
An MWA A(G)⋆of the graph G is an arborescence with the minimum total
cost. Without loss of generality, we can remove any edge entering in the root
r. Consider a subset of vertices S ⊆V . Let δin
S
be the set of edges entering
S: δin
S = {(i, j) ∈E : (i ∈V \ S) ∧(j ∈S)}. For a vertex k ∈V , δin
k
is the
c
⃝Springer International Publishing AG 2017
D. Salvagnin and M. Lombardi (Eds.): CPAIOR 2017, LNCS 10335, pp. 185–201, 2017.
DOI: 10.1007/978-3-319-59776-8 15

186
V.R. Houndji et al.
set of edges that enter in k. Let V ′ be the set of vertices without the root r:
V ′ = V \ {r}. The MWA problem can be formulated as follows [10]:
w(A(G)⋆) = min

(i,j)∈E
w(i, j) · xi,j
(1)
(MWA)

(i,j)∈δin
j
xi,j = 1, ∀j ∈V ′
(2)

(i,j)∈δin
S
xi,j ≥1, ∀S ⊆V ′ : |S| ≥2
(3)
xi,j ∈{0, 1}, ∀(i, j) ∈E
(4)
in which xi,j = 1 if the edge (i, j) is in the optimal arborescence A(G)⋆and
xi,j = 0 otherwise. The ﬁrst group of constraints imposes that exactly one edge
enters in each vertex j ∈V ′ and the constraints (3) enforce the existence of a path
from the root r to all other vertices. Without loss of generality [9], we assume
that w(i, i) = ∞, ∀i ∈V and w(i, j) > 0, ∀(i, j) ∈E. Then the constraints (4)
can be relaxed to xi,j ≥0, ∀i, j(5) and the constraints (2) become redundant
(see [6]).
Here, we address the Constrained Aborescence Problem (CAP) - that requires
one to ﬁnd an arborescence that satisﬁes other side constraints and is of mini-
mum cost - and show how one can handle them in CP. After some theoretical
preliminaries, we deﬁne the MinArborescence constraint and show how to ﬁlter
the decision variables. Then we describe how Linear Programming (LP) reduced
costs can be improved. Finally, we show some experimental results and conclude.
2
Background
Algorithms to compute an MWA A(G)⋆of a given graph G were proposed
independently by Chu and Liu ([3]), Edmonds ([6]) and Bock ([2]). A basic
implementation of that algorithm is in O(|V ||E|). The associated algorithm is
often called Edmonds’ Algorithm. An O(min{|V |2, |E| log |V |}) implementation
of the Edmonds’ algorithm is proposed by [23]. More sophisticated implementa-
tions exist (see for example [12,19]). Fischetti and Toth [9] proposed an O(|V |2)
implementation to compute an MWA and also the associated linear program-
ming reduced costs. We rely on this algorithm for ﬁltering the MinArborescence
constraint.
An MWA has two important properties that are used to construct it [16].
Proposition 1. A subgraph A = (V, F) of the graph G is an arborescence rooted
at r if and only if A has no cycle, and for each vertex v ̸= r, there is exactly one
edge in F that enters v.
Proposition 2. For each v ̸= r, select the cheapest edge entering v (breaking ties
arbitrarily), and let F ⋆be this set of n −1 edges. If (V, F ⋆) is an arborescence,
then it is a minimum cost arborescence, otherwise w(V, F ⋆) is a lower bound on
the minimum cost arborescence.

The Weighted Arborescence Constraint
187
The LP dual problem DMWA of MWA is [9]:
max

S⊆V ′
uS
(DMWA)
w(i, j) −

(i,j)∈δin
S ,∀S⊆V ′
uS ≥0, ∀(i, j) ∈E
uS ≥0, ∀S ⊆V ′
in which uS is the dual variable associated to the subset of edges S ⊆V ′.
Let rc(i, j) be the LP reduced cost associated to the edge (i, j). The necessary
and suﬃcient conditions for the optimality of MWA (with primal solution x⋆
i,j)
and DMWA (with dual solution u⋆
S) are [9]:
1. primal solution x⋆
i,j satisﬁes the constraints (3) and (5)
2. u⋆
S ≥0 for each S ⊆V ′
3. reduced cost rc(i, j) = w(i, j) −
(i,j)∈δin
S ,∀S⊆V ′ u⋆
S ≥0 for each (i, j) ∈E
4. rc(i, j) = 0 for each (i, j) ∈E such that x⋆
i,j > 0
5. 
(i,j)∈δin
S x⋆
i,j = 1 for each S ⊆V ′ such that u⋆
S > 0
Algorithm 1 [9] describes the global behavior of algorithms for computing an
MWA A(G)⋆for a graph G rooted at a given vertex r. Note that the optimality
condition 5. implies that: for each S ⊆V ′, u⋆
S > 0 =⇒
(i,j)∈δin
S x⋆
i,j = 1 and

(i,j)∈δin
S x⋆
i,j > 1 =⇒u⋆
S = 0 (because u⋆
S ≥0). The diﬀerent values of dual
variables uS, ∀S ⊆V ′ are obtained during the execution of Edmonds algorithm.
Actually, for each vertex k ∈V : u⋆
k = arg min(v,k)∈δin
k w(v, k) (line 8 of Algo-
rithm 1). If a subset S ⊆V ′ : |S| ≥2 is a strong component1, then u⋆
S is the
minimum reduced cost of edges in δin
S . Only these subsets can have u⋆
S > 0. All
other subsets (S ⊆V ′ : |S| ≥2 and S is not a strong component) have u⋆
S = 0.
So there are O(|V |) subsets S ⊆V ′ : |S| ≥2 that can have u⋆
S > 0. A straight-
forward algorithm can compute rc(i, j) = w(i, j) −
(i,j)∈δin
S ,∀S⊆V ′ u⋆
S, ∀(i, j)
in O(|V |3) by considering, for each edge, the O(|V |) subsets that are directed
cycles. Fischetti and Toth [9] proposed an O(|V |2) algorithm to compute
rc(i, j), ∀(i, j) ∈E.
Consider the graph G1 = (V1, E1) in Fig. 1 with the vertex 0 as root.
Figures 2, 3 and 4 show the diﬀerent steps needed to construct A(G1)⋆.
Related works in CP. Many works focused on ﬁltering the Minimum Span-
ning Tree (MST) constraints on undirected weighted graphs (see for examples
[4,5,21,22]). About directed graphs, Lorca ([17]) proposed some constraints
on trees and forests. In particular, the tree constraint [1,17] is about anti-
arborescence on directed graph. By considering a set of vertices (called resource),
1 A strong component of a graph G is a maximal (with respect to set inclusion) vertex
set S ⊆V such that (i) |S| = 1 or (ii) for each pair of distinct vertices i and j in S,
at least one path exists in G from vertex i to vertex j [9].

188
V.R. Houndji et al.
Algorithm 1. Computation of a minimum weight arborescence A(G)⋆
rooted at vertex r
Input: G = (V, E) ; r ∈V ; w(e), ∀e ∈E
// all primal and dual variables x⋆
i,j and u⋆
S are assumed to be zero
1 foreach each edge (i, j) ∈E do
2
rc(i, j) ←w(i, j)
3 A0 ←∅; h ←0
// Phase 1:
4 while G0 = (V, A0) is not r-connected do
// A graph is r-connected iff there is a path from the vertex r
to each other vertex v in V.
5
h ←h + 1
6
Find any strong component Sh of G0 such that r ̸∈Sh and A0 ∩δin
Sh = ∅
// If |Sh| > 1, then Sh is a directed cycle
7
Determine the edge (ih, jh) in δin
Sh such that rc(ih, jh) ≤rc(e), ∀e ∈δin
Sh
8
u⋆
Sh ←rc(ih, jh) // dual variable associated to Sh
9
x⋆
ih,jh ←1;
A0 ←A0 ∪{ih, jh}
10
foreach each edge (i, j) ∈δin
Sh do
11
rc(i, j) ←rc(i, j) −u⋆
Sh
// Phase 2:
12 t ←h
13 while t ≥1 do
// Extend A0 to an arborescence by letting all but one edge of
each strong component S
14
if x⋆
it,jt = 1 then
15
foreach each q < t such that jt ∈Sq do
16
x⋆
iq,jq ←0
17
A0 ←A0 \ {(iq, jq)}
18
t ←t −1
0
1
2
3
4
5
35
38
8
20
33
33
45
10
6
6
20
5
15
11
41
Fig. 1. Initial graph G1

The Weighted Arborescence Constraint
189
0
1
2
3
4
5
29
23
2
15
18
22
39
5
0
0
14
0
0
0
30
Fig. 2. Computation of A(G1)⋆: Phase 1, after selection of all single vertices. A0 =
{(2, 1), (4, 2), (4, 3), (2, 4), (3, 5)}.
0
1
3
5
S1 = {2, 4}
2 →0
18 →16
0
0
14 →12
0
Fig. 3. Computation of A(G1)⋆: Phase 1, after the detection of the size 2 strong com-
ponent {2, 4}. A0 = {(2, 1), (4, 2), (4, 3), (2, 4), (3, 5), (0, 4)}.
0
1
2
3
4
5
8
6
5
15
11
Fig. 4. Computation of A(G1)⋆: Phase 2. A0 = {(2, 1), (4, 2), (4, 3), (3, 5), (0, 4)}. (2, 4)
is removed.

190
V.R. Houndji et al.
the tree constraint partitions the vertices of a graph into a set of disjoint anti-
arborescences such that each anti-arborescence points to a resource vertex. The
author proposed a GAC ﬁltering algorithm in O(|E||V |) that is in O(|V |3). Fur-
ther, Lorca and Fages [18] propose an O(|E| + |V |) ﬁltering algorithm for this
constraint.
Here, we focus on an optimization oriented constraint for MWA mentioned
in [11]. The authors consider MWA as a relaxation of the Traveling Salesman
Problem (TSP) and use LP reduced costs to ﬁlter inconsistent values. In this
paper, we formally deﬁne the optimization constraint for MWA and propose an
algorithm to improve the LP reduced costs of MWA.
3
The MinArborescence Constraint
To deﬁne the MinArborescence constraint, we use the predecessor variable rep-
resentation of a graph. The arborescence is modeled with one variable Xi for
each vertex i of G representing its predecessor. The initial domain of a variable
Xi is thus the neighbors of i in G: j ∈D(Xi) ≡(j, i) ∈E. The constraint
MinArborescence(X, w, r, K) holds if the set of edges {(Xi, i) : i ̸= r} is a
valid arborescence rooted at r with 
i̸=r w(Xi, i) ≤K.
The consistency of the constraint is achieved by computing an exact MWA
A(G)⋆rooted at r and verifying that w(A(G)⋆) ≤K. The value w(A(G)⋆) is an
exact lower bound for the variable K: K ≥w(A(G)⋆). The ﬁltering of the edges
can be achieved based on the reduced costs. For a given edge (i, j) ̸∈A(G)⋆, if
w(A(G)⋆) + rc(i, j) > K, then Xi ←j is inconsistent. In Sect. 4, we propose a
procedure to strengthen the quality of the LP reduced costs in O(|V |2) in some
cases.
Note that, a GAC ﬁltering would require exact reduced costs, that to the best
of our knowledge can only be obtained by recomputing an MWA from scratch
in O(|E||V |2) which is in O(|V |4).
The basic decomposition of the MinArborescence constraint does not scale
well due to the exponential number of constraints in Eq. (3). We propose a
light constraint (called the Arborescence constraint) to have a scalable baseline
model for experiments.
Decomposing MinArborescence. The constraint Arborescence(X, r) holds if
the set of edges {(Xi, i) : ∀i ∈V ′} is a valid arborescence rooted at r. We
introduce an incremental forward checking like incremental ﬁltering procedure
for this constraint in Algorithm 2. Our algorithm is inspired by the ﬁltering
described in [20] for enforcing a Hamiltonian circuit. During the search, the
bound variables form a forest of arborescences. Eventually when all the variables
are bound, a unique arborescence rooted at r is obtained. The ﬁltering maintains
for each node:

The Weighted Arborescence Constraint
191
1. a reversible integer value localRoot[i] deﬁned as localRoot[i] = i if Xi is not
bound, otherwise it is recursively deﬁned as localRoot[Xi], and
2. a reversible set leafNodes[i] that contains the leaf nodes of i in the forest
formed by the bound variables.
The ﬁltering prevents cycles by removing from any successor variable Xv all
the values corresponding to its leaf nodes (i.e. the ones in the set leafNodes[v]).
Algorithm 2 registers the ﬁltering procedure to the bind events such that bind(i)
is called whenever the variable Xi is bind. This bind procedure then ﬁnds the
root lr of the (sub) arborescence at line 15. Notice that j is not necessarily the
root as vertex j might well have been the leaf node of another arborescence that
is now being connected by the binding of Xi to value j. This root lr then inherits
all the leaf nodes of i. None of these leaf nodes is allowed to become a successor
of lr, otherwise a cycle would be created.
Algorithm 2. Class Arborescence
1 X: array of |V | variables;
2 localRoot: array of |V | reversible int;
3 leafNodes: array of |V | reversible set of int;
4 Method init()
5
Xr ←r ;
// self loop on r
6
foreach each vertex v ∈V ′ do
7
leafNodes[v].insert(v) ;
8
Xv.removeV alue(v) ;
// no self loop
9
if Xv.isBound then
10
bind(v);
11
else
12
Xv.registerOnBindChanges() ;
13 Method bind(i: int)
14
j ←Xi ;
// edge (j, i) ∈arborescence
15
lr ←localRoot[j] ;
// local root of j
16
foreach each v ∈leafNodes[i] do
17
localRoot[v] ←lr ;
18
leafNodes[lr].insert(v) ;
19
Xlr.removeV alue(v) ;
The MinArborescence(X, w, r, K) constraint can be decomposed as the
Arborescence(X, r) constraint plus 
i̸=r w(Xi, i) ≤K, a sum over element
constraints.

192
V.R. Houndji et al.
4
Improved Reduced Costs
Let A(G)⋆
i→j be an MWA of the graph G when the edge (i, j) is forced to be
in it. We know that the LP reduced cost rc(i, j) gives a lower bound on the
associated cost increase: w(A(G)⋆
i→j) ≥w(A(G)⋆)+rc(i, j). However, this lower
bound on w(A(G)⋆
i→j) can be improved in some cases.
Let us use the following notation to characterize an MWA A(G)⋆.
Let pred[v], ∀v ∈V ′, be the vertex in V such that the edge (pred[v], v) ∈A(G)⋆:
x⋆
pred[v],v = 1. For example, in the graph G1, pred[1] = 2 and pred[5] = 3.
Let parent[S] be the smallest cycle strictly containing the subset S ⊆V ′:
parent[S] is the smallest subset > |S| such that 
(i,j)∈δin
parent[S] x⋆
i,j = 1 and
S ⊂parent[S]. In other words, parent[S] is the ﬁrst directed cycle that includes
the subset S found during the execution of the Edmonds’ algorithm. We assume
that parent[S] = ∅if there is no such cycle and parent[∅] = ∅. In the graph G1,
parent[1] = parent[3] = parent[5] = ∅and parent[2] = parent[4] = {2, 4}. Here,
parent[parent[k]] = ∅, ∀k ∈V ′.
The next three properties give some information to improve the LP reduced
costs when all vertices involved do not have a parent.
Property 1. Assume that there is a path P = (j, . . . , i) from the vertex j to
vertex i in A(G)⋆such that ∀k ∈P : parent[k] = ∅. If the edge (i, j) is forced to
be in A(G)⋆, then the cycle c = (k : k ∈P) will be created during the execution
of Edmonds’ algorithm.
Proof. parent[k]
=
∅means that pred[k] is such that w(pred[k], k)
=
min{w(v, k) : (v, k) ∈δin
k } and then pred[k] will be ﬁrst selected by Edmonds’
algorithm ∀k ∈P \ {j}. On the other hand, if the edge (i, j) is forced into the
MWA it implies that all other edges entering j are removed. Consequently, the
cycle c = (k : k ∈P) will be created.
⊓⊔
Let us use the following notations to evaluate the improved reduced costs. Let
min1[k] = arg min(v,k)∈δin
k w(v, k) be the minimum cost edge entering the vertex
k. If there is more than one edge with the smallest weight, we choose one of them
arbitrarily. Also, let min2[k] = arg min(v,k)∈δin
k ∧(v,k)̸=min1 w(v, k) be the second
minimum cost edge entering k. For each vertex k ∈V , let bestTwoDiff[k] be
the diﬀerence between the best two minimum costs of edges entering the vertex
k: ∀k ∈V, bestTwoDiff[k] = w(min2[k]) −w(min1[k]). For instance, in the
graph G1: min1[5] = (3, 5), min2[5] = (1, 5) and bestTwoDiff[5] = 10 −5 = 5.
Property 2. Consider the cycle c = (i, . . . , j) obtained by forcing the edge (i, j)
such that parent[k] = ∅, ∀k ∈c (see Property 1). The minimum cost increase if
the cycle is broken/connected by the vertex k′ ∈c is bestTwoDiff[k′].
Proof. For a given k′ ∈c, parent[k′] = ∅implies that the edge (pred[k′], k′) =
min1[k′]. Then the cheapest way to break the cycle by k′ is to use the edge with
the second minimum cost min2[k′]. Hence the minimum cost increase if the cycle
is broken by the vertex k′ is w(min2[k′]) −w(min1[k′]) = bestTwoDiff[k′]. ⊓⊔

The Weighted Arborescence Constraint
193
When parent[j] = ∅, the LP reduced costs rc(i, j) are simple and can be
easily interpreted.
Property 3. Consider a vertex j ∈V ′ such that parent[j] = ∅. For all i ∈V \{j}
with (i, j) ̸∈A(G)⋆: rc(i, j) = w(i, j) −w(pred[j], j).
Proof. We know that if parent[j] = ∅, then for each S ⊆V ′ with j ∈S and
|S| ≥2, u⋆
S = 0 (because none of them is a directed cycle). Then rc(i, j) =
w(i, j) −u⋆
j. On the other hand, since parent[j] = ∅, the edge min1[j] is the one
used to connect j in A(G)⋆. So u⋆
j = w(min1[j]) = w(pred[j], j) and rc(i, j) =
w(i, j) −w(pred[j], j).
⊓⊔
By considering an MWA A(G)⋆, the interpretation of rc(i, j) when
parent[j] = ∅is that the edge (i, j) is forced into A(G)⋆and the edge (pred[j], j)
that is used to connect j is removed. Intuitively, if this process induces a new
cycle, this latter has to be (re)connected to the rest of the arborescence from
a vertex in the cycle diﬀerent from j. Proposition 3 established below, gives a
ﬁrst improved reduced cost expression when a new cycle c is created by forcing
(i, j) into the arborescence and ∀k ∈c, parent[k] = ∅. Note that such new cycle
will be created only if there is already a path in A(G)⋆from the vertex j to the
vertex i.
Proposition 3. Assume that there is a path P = (j, . . . , i) from the vertex j to
vertex i in A(G)⋆such that ∀k ∈P : parent[k] = ∅. We have: w(A(G)⋆
i→j) ≥
w(A(G)⋆) + rc(i, j) + mink∈P\{j}{bestTwoDiff[k]}.
Proof. Without loss of generality, we assume that the cycle c = (k : k ∈P)
is the ﬁrst one created by the Edmonds’ algorithm (see Property 1). After this
step, the new set of vertices is V ′ = {c} ∪{v ∈V : v ̸∈c}. In A(G)⋆, the
edges assigned to vertices in c do not inﬂuence the choice of edges for each
vertex v ∈V : v ̸∈c (because parent[k] = ∅, ∀k ∈c). So w(A(G)⋆
i→j) ≥

k∈V ∧k̸∈c w(pred[k], k) + w(c) in which w(c) is the minimum sum of costs of
edges when exactly one edge is assigned to each vertex in c without cycle. The
cheapest way to connect all vertices in c such that each one has exactly one
entering edge is to use all cheapest entering edges (min1[k], ∀k ∈c). The cycle
obtained must be broken in the cheapest way. To do so, the vertex used: (1)
must be diﬀerent from j (because (i, j) is already there) and (2) have to induce
the minimal cost increase. Then, from Property 2, a lower bound on the mini-
mum cost increase is mink∈P\{j}{bestTwoDiff[k]}. In addition, we have to add
the cost of the forced edge (i, j) and remove the cost of the edge in A(G)⋆that
enters in j: w(c) ≥
k∈c w(pred[k], k)+mink∈c\{j}{bestTwoDiff[k]}+w(i, j)−
w(pred[j], j). We know that 
k∈c w(pred[k], k) + 
k∈V ∧k̸∈c w(pred[k], k) =
w(A(G)⋆) and rc(i, j) = w(i, j) −w(pred[j], j) (see Property 3).
Thus w(A(G)⋆
i→j) ≥w(A(G)⋆) + rc(i, j) + mink∈P\{j}{bestTwoDiff[k]}. ⊓⊔

194
V.R. Houndji et al.
0
1
2
3
4
5
8
6
5
15
41
(a) G1 with rc(5, 3)
0
1
2
3
4
5
8
10
6
15
41
(b) G1 with irc(5, 3)
Fig. 5. G1 with rc(5, 3) and irc(5, 3)
Example 1. Consider the graph G1 presented in Fig. 1 and its MWA (Fig. 4). We
want to force (5, 3) to be into the MWA:
– rc(5, 3) = w(5, 3) −w(4, 3) = 41 −11 = 30. This operation leads to the graph
shown in Fig. 5(a). We can see that the new cycle created c = (5, 3) must be
broken from a vertex diﬀerent from 3.
– irc(5, 3) = rc(5, 3) + bestTwoDiff[5] = 30 + 5 = 35. The corresponding
graph is shown in Fig. 5(b), that actually is the new MWA.
Property 2 and Proposition 3 can be generalized into Property 4 and Propo-
sition 4 below to include some vertices that have one parent.
Property 4. Consider an ordered set of vertices (k1, k2, . . . , kn) in A(G)⋆such
that c = (k1, k2, . . . , kn) is a directed cycle connected (broken) by the vertex k⋆
and parent[c] = ∅. The minimum cost increase if the cycle is broken by another
vertex k′ ∈c \ {k⋆} is ≥bestTwoDiff[k′] −u⋆
c.
Proof. We know that parent[c] = ∅implies u⋆
c = min{w(min2[k])−w(min1[k]) :
k ∈c} = w(min2[k⋆])−w(min1[k⋆]). So if the cycle is now connected by another
vertex than k⋆, the edge min1[k⋆] can be used instead of min2[k⋆] and decreases
the cost by w(min2[k⋆]) −w(min1[k⋆]) = u⋆
c. On the other hand, ∀ki ∈c \ {k⋆},
(pred[ki], ki) = min1[ki]. The cheapest way to use k′ to connect c is to use
min2[k′]. Hence, a lower bound on the total cost induced is min2[k′]−min1[k′]−
u⋆
c = bestTwoDiff[k′] −u⋆
c.
⊓⊔
Now the improved reduced costs can be formulated as follows.
Proposition 4. Assume that there is a path P = (j, . . . , i) from the vertex j to
vertex i in A(G)⋆such that ∀k ∈P : parent[parent[k]] = ∅. Then w(A(G)⋆
i→j) ≥
w(A(G)⋆) + rc(i, j) + mink∈P\{j}{bestTwoDiff[k] −u⋆
parent[k]}.
Proof. Note that if ∀k ∈P : parent[k] = ∅(that implies that u⋆
parent[k] = 0),
the formula is the same as the one of Proposition 3. Let Z denote the set of
vertices in P and in all other cycles linked to P. Formally, Z = {v ∈V : v ∈
P} ∪{k ∈V : ∃v ∈P ∧parent[k] = parent[v]}. We know that, in A(G)⋆, the
edges assigned to vertices in Z do not inﬂuence the choice of edges for each vertex

The Weighted Arborescence Constraint
195
k ∈V \ Z. So w(A(G)⋆
i→j) ≥
k∈V \Z w(pred[k], k) + w(Z) in which w(Z) is
the minimum sum of the costs of edges when we assign exactly one edge to each
vertex in Z without cycle. The reasoning is close to the proof of Proposition 3.
The diﬀerences here are:
1. ∃k
∈P \ {j} : parent[k] ̸= ∅. Assume that we want to break the
cycle by one vertex v⋆in P \ {j}. If the vertex v⋆used is such that
parent[v⋆] = ∅, then the minimum cost to pay is ≥bestTwoDiff[v⋆] (here
u⋆
parent[v⋆] = 0 because parent[v⋆] = ∅). If v⋆is such that parent[v⋆] ̸=
∅∧parent[parent[v⋆]] = ∅, then from Property 4, the cost to pay is ≥
bestTwoDiff[v⋆] −u⋆
parent[v⋆]. By considering all vertices in P \ {j}, the
cost to pay is then ≥mink∈P\{j}{bestTwoDiff[k] −u⋆
parent[k]}.
2. the vertex j may have one parent. Let connect be the vertex that is used to
connect the cycle parent[j] in A(G)⋆.
Case 1: parent[i] ̸= parent[j]. If we force the edge (i, j), then the cycle
parent[j] should not be created because it is as if all edges entering j but
(i, j) are removed. First, assume that j ̸= connect. The edge in parent[j] not
in A(G)⋆should be used and the cost won is u⋆
parent[j] (as in the proof of
Property 4). Thus, a lower bound on the cost to break the cycle parent[j] by
j is: w(i, j) −w(pred[j], j) −u⋆
parent[j]. This lower bound is equal to rc(i, j)
because w(pred[j], j) = w(min1[j]). Now assume that j = connect. In this
case (pred[j], j) = min2[j] (because parent[parent[j]] = ∅). Using the edge
(i, j) instead of (pred[j], j) induces the cost w(i, j) −w(min2[j]) = w(i, j) −
w(min1[j]) −w(min2[j]) + w(min1[j) = w(i, j) −w(min1[j]) −u⋆
parent[j] =
rc(i, j).
Case 2: parent[i] = parent[j] ̸= ∅. This means that the edge (i, j) is the one
of the cycle that is not in the MWA. In this case rc(i, j) = 0, and the new
cycle created should be broken as described above (1.).
Hence, a lower bound on w(Z) is

k∈Z
w(pred[k], k) +
min
k∈P\{j}{bestTwoDiff[k] −u⋆
parent[k]} + rc(i, j)
and w(A(G)⋆
i→j) ≥w(A(G)⋆) + mink∈P\{j}{bestTwoDiff[k] −u⋆
parent[k]} +
rc(i, j).
⊓⊔
Note that parent[parent[k]] = ∅if k is not in a cycle or k is in a cycle that
is not contained in a larger cycle. The formula of Proposition 4 is available only
if ∀k ∈P : parent[parent[k]] = ∅. Let irc(i, j) denote the improved reduced
cost of the edge (i, j): irc(i, j) = rc(i, j) + max{mink∈P∧k̸=j{bestTwoDiff[k] −
u⋆
parent[k]}, 0} if the assumption of Proposition 4 is true and irc(i, j) = rc(i, j)
otherwise.

196
V.R. Houndji et al.
0
1
2
3
4
5
33
6
6
5
11
(a) G1 with rc(1, 2)
0
1
2
3
4
5
35
33
6
5
11
(b) G1 with irc(1, 2)
Fig. 6. G1 with rc(1, 2) and irc(1, 2)
Example 2. Consider the graph G1 in Fig. 1 and its MWA A(G1)⋆in Fig. 4. For
the construction of A(G1)⋆, the cycle c1 = {2, 4} is created. We want to force
into the MWA the edge:
1. (1, 2): rc(1, 2) = w(1, 2) −u⋆
2 −u⋆
c1 = w(1, 2) −w(4, 2) −(w(0, 4) −w(2, 4)).
rc(1, 2) = 16. The corresponding graph is presented in Fig. 6(a). Of course,
the new cycle (1, 2) created must be broken from the vertex 1. irc(1, 2) =
rc(1, 2) + (w(0, 1) −w(2, 1)) = 16 + 29 = 45. Actually, that is the exact
reduced cost since the new graph obtained is an arborescence (see Fig. 6(b)).
2. (1, 4): rc(1, 4) = w(1, 4) −u⋆
4 −u⋆
c1 = w(1, 4) −w(2, 4) −(w(0, 4) −w(2, 4)).
rc(1, 4) = 37. The corresponding graph is presented in Fig. 7 (a). But
irc(1, 4) = rc(1, 4)+min{w(0, 1)−w(2, 1) = 29, w(1, 2)−w(4, 2)−u⋆
c1 = 16}.
So irc(1, 4) = 37 + 16 = 53. Here (see Fig. 7 (b)), the graph obtained is not
an arborescence and irc(1, 4) is a lower bound.
0
1
2
3
4
5
45
6
5
15
(a) G1 with rc(1, 4)
0
1
2
3
4
5
33
45
6
5
11
(b) G1 with irc(1, 4)
Fig. 7. G1 with rc(1, 4) and irc(1, 4)
Algorithm 3 computes irc(i, j), ∀(i, j) in O(|V |2). First, it initializes each
irc(i, j) to rc(i, j), ∀(i, j) ∈E. Then, for each edge (i, j) involved in the assump-
tion of Proposition 4 (Invariant (a)), Algorithm 3 updates its irc(i, j) according
to the formula of Proposition 4.

The Weighted Arborescence Constraint
197
Algorithm 3. Computation of improved reduced costs irc(i, j), ∀(i, j) ∈E
Input: parent[k], ∀k ∈V ; pred[k], ∀k ∈V ; u⋆
ci, ∀ci ∈C and
bestTwoDiff[k], ∀k ∈V that can be computed in O(|V |2)
Output: irc(i, j), ∀(i, j) ∈E
1 foreach each edge (i, j) ∈E do
2
irc(i, j) ←rc(i, j)
3 foreach each vertex i ∈V do
4
if parent[parent[i]] = ∅then
5
min ←bestTwoDiff[i] −u⋆
parent[i]
6
j = pred[i]
7
while (parent[parent[j]] = ∅)∧min > 0 ∧j ̸= r do
// Invariant (a): there is a path P from j to i such that
∀k ∈P : parent[parent[k]] = ∅
// Invariant (b):
min = mink∈P\{j}{bestTwoDiff[k] −uparent[k]⋆}
8
irc(i, j) ←irc(i, j) + min
9
if bestTwoDiff[j] −uparent[j]⋆< min then
10
min ←bestTwoDiff[j] −u⋆
parent[j]
11
j = pred[j]
5
Experimental Results
As a ﬁrst experiment, we evaluate the proportion of reduced costs aﬀected by
Proposition 4. Therefore we randomly generated two classes of 100 instances
w(i, j) ∈[1, 100] with diﬀerent values of the number of vertices:
– class1: for each i ∈V , parent[parent[i]] = ∅;
– class2: many vertices i ∈V are such that parent[parent[i]] ̸= ∅.
The class1 was obtained by ﬁltering out the random instances not satisfying
the property. Let exactRC(i, j) be the exact reduced cost associated to the
edge (i, j) ∈E. Table 1 shows, for each class of instances (with respectively
|V | = 20, |V | = 50 and |V | = 100), the proportion of instances of each class
and for each group of instances: (1) the proportion of edges (i, j) ∈E such that
rc(i, j) < exactRC(i, j); (2) the proportion of edges that have irc(i, j) > rc(i, j);
and (3) the proportion of edges such that irc(i, j) = exactRC(i, j). Note that,
for this benchmark, at least 37% of 300 instances are class1 instances and at
least 45% of LP reduced costs (with rc(i, j) < exactRC(i, j)) are improved for
these instances. Of course, the results are less interesting for the class2 instances.
To test the MinArborescence constraint, experiments were conducted on an
NP-Hard variant of CAP: the Resource constrained Minimum Weight Arbores-
cence Problem (RMWA) [10,14]. The RMWA problem is to ﬁnd an MWA
under the resource constraints for each vertex i ∈V : 
(i,j)∈δ+
i ai,j · xi,j ≤bi
where δ+
i
is the set of outgoing edges from i, ai,j is the amount of resource

198
V.R. Houndji et al.
Table 1. Proportion of reduced costs aﬀected by Proposition 4
|V | = 20
|V | = 50
|V | = 100
Class1 Class2 Class1 Class2 Class1 Class2
%: instances of classk (k ∈{1, 2}) 48
52
31
69
32
68
%: rc(i, j) < exactRC(i, j)
19.3
38.1
9.8
26.7
2.1
16.6
%: irc(i, j) > rc(i, j)
12.6
1.9
4.6
0.9
1.2
0.2
%: irc(i, j) = exactRC(i, j)
9.9
1.17
3.49
0.79
1.19
0.2
uses by the edge (i, j) and bi is the resource available at vertex i. RMWA
can be modeled in CP with a MinArborescence constraint (or one of its
decompositions) for the MWA part of problem and the binaryKnapsack con-
straint [7] together with weightedSum constraint for the resource constraints.
We have randomly generated the diﬀerent costs/weights as described in [14]:
ai,j ∈[10, 25], w(i, j) ∈[5, 25]. To have more available edges to ﬁlter, we have
used bi = 2 · ⌊

(i,j)∈δ+
i
ai,j
|δ+
i |
⌋(instead of bi = ⌊

(i,j)∈δ+
i
ai,j
|δ+
i |
⌋) and 75% graph
density.
In order to avoid the eﬀect of the dynamic ﬁrst fail heuristic interfering with
the ﬁltering, we use the approach described in [25] to evaluate global constraints.
This approach consists in recording the search tree with the weakest ﬁltering as
a baseline. It is obtained with the decomposition model using the Arborescence
constraint. This search tree is then replayed with the stronger reduced cost based
ﬁltering for MinArborescence. The recorded search tree for each instance corre-
sponds to an exploration of 30 s. As an illustration for the results, Table 2 details
the computational results for MinArborescence constraint with ﬁltering based
on improved reduced costs (MinArbo IRC), reduced costs (MinArbo RC), the
decomposition with Arborescence constraint (Arbo) and Arbo+ﬁltering only
based on lower bound on MWA (Arbo+LB) on 4 (arbitrarily chosen) out of the
100 randomly instances with |V | = 50. We also report the average results for the
100 instances. On average, the search space is divided by ≈460 with the reduced
costs based ﬁltering MinArborescence constraint (wrt Arbo) and by ≈81 with
Arbo+LB. This demonstrates the beneﬁts brought by the MinArborescence
global constraints described in this paper.
To
further
diﬀerentiate
the
ﬁltering
of
MinArbo IRC,
we
now
use
MinArbo RC as a baseline ﬁltering for recording the search tree on another
set of 100 randomly generated instances of class1 with |V | = 50. Figure 8 shows
the corresponding performance proﬁles wrt the number of nodes visited and the
time used respectively. For ≈30% of instances the search space is divided by
at least 1.5 and for ≈7% the search space is divided by at least 4. On the
other hand, the average gain for MinArbo IRC (wrt MinArbo RC) is 1.7 wrt
the number of nodes visited and 1.4 wrt time. Unfortunately, as was expected,
the average gain is limited as only ≈5% of LP reduced costs can be improved.

The Weighted Arborescence Constraint
199
Table 2. Results: MinArbo IRC, MinArbo RC, Arbo+LB and Arbo
Instance
MinArbo IRC
MinArbo RC Arbo+LB
Arbo
Nodes
Time(s) Nodes
Time Nodes
Time Nodes
Time
1
20259
0
20259
0
40061
1
5879717
28
2
12552
0
12552
0
16794
0
6033706
27
3
13094
0
13094
0
121290 2
6383651
28
4
62607
0
62607
0
283854 6
7316899
29
Average 14385 0
14385 0
81239
1.4
6646748 28
Fig. 8. Performance proﬁles wrt number of nodes visited and time
The implementations and tests have been realized within the OscaR open
source solver [24]. Our source-code and the instances are available at [15]. Our
CP model is able to solve and prove optimality of RMWA instances with up to
|V | = 50. Similar instances can be solved using the Lagrangian decomposition
approach of [14]. The branch and cut algorithm of [10] reports results solving
instances with up to |V | = 500. We believe this lack of performance of CP wrt to
the branch and cut approach is due to the |V | independent knapsack constraints
inducing a weaker pruning. We do hope this ﬁrst result will trigger more research
in the future to make CP more competitive on this challenging problem.
6
Conclusion
We have deﬁned the MinArborescence constraint based on the reduced costs to
ﬁlter the edges for a constrained arborescence problem. We have proposed an
algorithm to improve the LP reduced costs of the minimum weighted arbores-
cence in some cases. Finally, we have demonstrated experimentally the interest
of improved reduced costs in some particular graphs and the eﬃciency of the
cost-based ﬁltering on the resource constrained arborescence problem. As future
works, we would like to: (1) think about a global constraint wrt resource con-
straints (2) study the incremental aspects of the MinArborescence constraint
and (3) propose specialized search heuristics.

200
V.R. Houndji et al.
References
1. Beldiceanu, N., Flener, P., Lorca, X.: The tree Constraint. In: Bart´ak, R., Milano,
M. (eds.) CPAIOR 2005. LNCS, vol. 3524, pp. 64–78. Springer, Heidelberg (2005).
doi:10.1007/11493853 7
2. Bock, F.: An algorithm to construct a minimum directed spanning tree in a directed
network. Dev. Oper. Res. 1, 29–44 (1971)
3. Chu, Y.J., Liu, T.H.: On the shortest arborescence of a directed graph. Sci. Sin.
Ser. A 14, 1396–1400 (1965)
4. Dooms, G., Katriel, I.: The minimum spanning tree constraint. In: Benhamou, F.
(ed.) CP 2006. LNCS, vol. 4204, pp. 152–166. Springer, Heidelberg (2006). doi:10.
1007/11889205 13
5. Dooms, G., Katriel, I.: The “not-too-heavy spanning tree” constraint. In:
Hentenryck, P., Wolsey, L. (eds.) CPAIOR 2007. LNCS, vol. 4510, pp. 59–70.
Springer, Heidelberg (2007). doi:10.1007/978-3-540-72397-4 5
6. Edmonds, J.: Optimum branchings. J. Res. Nat. Bur. Stand. B 71(4), 125–130
(1967)
7. Fahle, T., Sellmann, M.: Cost based ﬁltering for the constrained knapsack problem.
Ann. Oper. Res. 115(1–4), 73–93 (2002)
8. Fischetti, M., Toth, P.: An additive bounding procedure for asymmetric travelling
salesman problem. Math. Program. 53, 173–197 (1992)
9. Fischetti, M., Toth, P.: An eﬃcient algorithm for min-sum arborescence problem
on complete digraphs. Manage. Sci. 9(3), 1520–1536 (1993)
10. Fischetti, M., Vigo, D.: A branch-and-cut algorithm for the resource-constrained
minimum-weight arborescence problem. Network 29, 55–67 (1997)
11. Focacci, F., Lodi, A., Milano, M., Vigo, D.: Solving TSP through the integration
of OR and CP techniques. Electron. Notes Discrete Math. 1, 13–25 (1999)
12. Gabow, H.N., Galil, Z., Spencer, T.H., Tarjan, R.E.: Eﬃcient algorithms for ﬁnding
minimum spanning trees in undirected and directed graphs. Combinatorica 6(3),
109–122 (1986)
13. Graham, R.L., Hell, P.: On the history of the minimum spanning tree problem.
Hist. Comput. 7, 13–25 (1985)
14. Guignard, M., Rosenwein, M.B.: An application of lagrangean decomposition to
the resource-constrained minimum weighted arborescence problem. Network 20,
345–359 (1990)
15. Houndji, V.R., Schaus, P.: Cp4cap: Constraint programming for constrained
arborescence problem. https://bitbucket.org/ratheilesse/cp4cap
16. Kleinberg, J., Tardos, E.: Minimum-cost arborescences: a multi-phase greedy algo-
rithm. In: Algorithm Design, Tsinghua University Press (2005)
17. Lorca, X.: Contraintes de Partitionnement de Graphe. Ph. D. thesis, Universit´e de
Nantes (2010)
18. Fages, J.-G., Lorca, X.: Revisiting the tree constraint. In: Lee, J. (ed.) CP
2011. LNCS, vol. 6876, pp. 271–285. Springer, Heidelberg (2011). doi:10.1007/
978-3-642-23786-7 22
19. Mendelson, R., Tarjan, R.E., Thorup, M., Zwick, U.: Melding priority queues. In:
Hagerup, T., Katajainen, J. (eds.) SWAT 2004. LNCS, vol. 3111, pp. 223–235.
Springer, Heidelberg (2004). doi:10.1007/978-3-540-27810-8 20
20. Pesant, G., Gendreau, M., Potvin, J.-Y., Rousseau, J.-M.: An exact constraint logic
programming algorithm for the traveling salesman problem with time windows.
Transp. Sci. 32(1), 12–29 (1998)

The Weighted Arborescence Constraint
201
21. R´egin, J.-C.: Simpler and incremental consistency checking and arc consistency
ﬁltering algorithms for the weighted spanning tree constraint. In: Perron, L., Trick,
M.A. (eds.) CPAIOR 2008. LNCS, vol. 5015, pp. 233–247. Springer, Heidelberg
(2008). doi:10.1007/978-3-540-68155-7 19
22. R´egin, J.-C., Rousseau, L.-M., Rueher, M., van Hoeve, W.-J.: The weighted span-
ning tree constraint revisited. In: Lodi, A., Milano, M., Toth, P. (eds.) CPAIOR
2010. LNCS, vol. 6140, pp. 287–291. Springer, Heidelberg (2010). doi:10.1007/
978-3-642-13520-0 31
23. Tarjan, R.E.: Finding optimum branchings. Networks 7(3), 25–35 (1977)
24. OscaR Team. Oscar: Scala in or (2012). https://bitbucket.org/oscarlib/oscar
25. Van Cauwelaert, S., Lombardi, M., Schaus, P.: Understanding the potential of
propagators. In: Michel, L. (ed.) CPAIOR 2015. LNCS, vol. 9075, pp. 427–436.
Springer, Cham (2015). doi:10.1007/978-3-319-18008-3 29

Learning When to Use a Decomposition
Markus Kruber1(B)
, Marco E. L¨ubbecke1
, and Axel Parmentier2
1 Chair of Operations Research, RWTH Aachen University,
Kackertstrasse 7, 52072 Aachen, Germany
{kruber,luebbecke}@or.rwth-aachen.de
2 CERMICS, ´Ecole des Ponts Paristech, Universit´e Paris Est,
6 et 8 Avenue Blaise Pascal, 77420 Champs sur Marne, France
axel.parmentier@enpc.fr
Abstract. Applying a Dantzig-Wolfe decomposition to a mixed-integer
program (MIP) aims at exploiting an embedded model structure and
can lead to signiﬁcantly stronger reformulations of the MIP. Recently,
automating the process and embedding it in standard MIP solvers have
been proposed, with the detection of a decomposable model structure as
key element. If the detected structure reﬂects the (usually unknown)
actual structure of the MIP well, the solver may be much faster on
the reformulated model than on the original. Otherwise, the solver may
completely fail. We propose a supervised learning approach to decide
whether or not a reformulation should be applied, and which decomposi-
tion to choose when several are possible. Preliminary experiments with a
MIP solver equipped with this knowledge show a signiﬁcant performance
improvement on structured instances, with little deterioration on others.
Keywords: Mixed-integer programming · Branch-and-price · Column
generation · Automatic Dantzig-Wolfe decomposition · Supervised
learning
1
Setting and Approach
Dantzig-Wolfe (DW) reformulation of a mixed-integer program (MIP) became
an indispensable tool in the computational mathematical programming bag of
tricks. On the one hand, it may be the key to solving specially structured MIPs.
On the other hand, successfully applying the technique may require a solid back-
ground, experience, and a non-negligible implementation eﬀort.
In order to make DW reformulation more accessible also to non-specialists,
general solvers were developed that make use of the method. One such solver is
GCG [4], an extension to the well-established MIP solver SCIP [1]. Several detectors
ﬁrst look for possible DW reformulations of the original MIP model. Diﬀerent
types of reformulations are used by practitioners, and even if most MIPs can
be forced into each of these types [2], their relevance highly depends on the
model structure. For instance, staircase forms suit well to temporal knapsack
problems [3] while bordered block diagonal forms work well on vehicle rout-
ing problems. As in general solvers, we do not know a priori the structure of
c
⃝Springer International Publishing AG 2017
D. Salvagnin and M. Lombardi (Eds.): CPAIOR 2017, LNCS 10335, pp. 202–210, 2017.
DOI: 10.1007/978-3-319-59776-8 16

Learning When to Use a Decomposition
203
the original model, and many decompositions of each type are detected. These
decompositions are then evaluated: if they are of “good quality,” the MIP is
reformulated according to a “best suited” decomposition.
If one ﬁnds and selects a decomposition (DEC) that captures a structure
underlying the original model, the reformulated model may be solved much faster
than the original one. However, from the many diﬀerent decompositions, we may
select one that does not reﬂect the actual underlying model structure; and in this
case the solver may completely fail. We currently do not have any consistently
reliable a priori measure to distinguish between these cases, except heuristic
proxies, see e.g., [2]. What is more, the latter case is not uncommon, and GCG,
presented with an arbitrary MIP, successfully detects a decomposition that leads
to an improved performance over SCIP on the original model only in a small
fraction of cases. This is to be expected, but will not render GCG (in its present
form) a competitive general MIP solver. Our work thus aims at providing a
mean to decide whether it pays to DW reformulates a given MIP model or not.
Figure 1 illustrates how the result may look like.
MIP
Detection
DEC
DW?
Select
GCG
SCIP
no
yes
Fig. 1. Multiple detectors DW solver GCG with “SCIP exit strategy”
Literature. Decomposable model structure may be detected directly from the
MIP, see e.g., [2,12] and the references therein. Machine learning techniques have
been recently used in computational mathematical optimization, e.g., automated
MIP solver conﬁguration [13], load balancing in parallel branch-and-bound [10],
or variable selection in branching decisions [6,9]. We are not aware of works that
try to learn MIP model structure to be exploited in decompositions.
Our Supervised Learning Approach. We would like to learn an answer to the
question: Given a MIP P, a DW decomposition D, and a time limit τ, will GCG
using D optimally solve P faster than SCIP (or have a smaller optimality gap at
time τ)? We deﬁne a mapping φ that transforms a tuple (P, D, τ) into a vector
of suﬃcient statistics or features φ(P, D, τ) ∈Rd. Thanks to this mapping φ, the
question above becomes a standard binary classiﬁcation problem on Rd. We can
therefore train a standard classiﬁer f : Rd →{0, 1} to solve this problem. Given
an instance (P, D, τ), the quantity f ◦φ(P, D, τ) is equal to one iﬀthe predicted
answer to the question above is positive. Practically, we have built a database of
SCIP and GCG runs for tuples (P, D, τ), a mapping φ, and have trained classiﬁers
f from the scikit-learn library [11] on the instances φ(P, D, τ). Answers to
the probabilistic versions g : Rd →[0, 1] of these classiﬁers can be interpreted as
the probability that GCG using D outperforms SCIP if the time limit is τ.

204
M. Kruber et al.
GCG starts by detecting decompositions D1, . . . , Dk for P. This detection takes
time τdet. We then decide how to make use of the remaining time τ −τdet:
Continue GCG if max
i=1,...,k g ◦φ(P, Di, τ −τdet) ≥α. Otherwise run SCIP.
(1)
The threshold α reﬂects our level of conservatism towards solving P using a
DW reformulation. If we decide to continue the run with GCG, we
use decomposition D with maximum g ◦φ(P, D, τ −τdec),
(2)
that is, one with largest predicted probability that GCG beats SCIP.
There are three key elements for such an approach to perform well: First,
the features must catch relevant information, see Sect. 2.2. Second, for training
a classiﬁer we need to present it data, i.e., tuples (P, D, τ) in the learning phase
that are similar to those we expect to see later when using the classiﬁer. Our
learning dataset contains instances from a wide range of families of structured
and non-structured models. Third, an appropriate binary classiﬁer must be used.
It is our working hypothesis that a decomposition is likely to work if a similar
decomposition works on a similar model. We thus tested classiﬁers whose answer
depends only on the distance of the feature vector of the instance considered
to those of the instances in the training set: nearest neighbors, support vector
machines with an RBF kernel because such a kernel is stationary [5], and random
forests because they can be seen as a weighted nearest neighbor scheme [8].
2
Decompositions and Features
2.1
Bird’s View on Dantzig-Wolfe Reformulation
We would like to solve what we call the original MIP
min

ctx, Ax ≥b, x ∈Zn
+ × Qq
+

.
(3)
Today, the classical approach to solve (3) is branch-and-cut, implemented e.g.,
in the SCIP solver [1]. Without going into much detail, the data in (3) can
sometimes be re-arranged such that a particular structure in the model becomes
visible. Among several others, one such structure is the so-called arrowhead or
double-bordered block diagonal form. It consists of partitioning the variables
x = [x1, . . . , xκ, xℓ]t and right hand sides b = [b1, . . . , bκ, bℓ]t to obtain
min ctx
s.t.
⎡
⎢⎢⎢⎢⎢⎣
D1
F 1
D2
F 2
...
...
Dκ F κ
A1 A2 · · · Aκ G
⎤
⎥⎥⎥⎥⎥⎦
·
⎡
⎢⎢⎢⎢⎢⎣
x1
x2
...
xκ
xℓ
⎤
⎥⎥⎥⎥⎥⎦
≥
⎡
⎢⎢⎢⎢⎢⎣
b1
b2
...
bκ
bℓ
⎤
⎥⎥⎥⎥⎥⎦
x ∈Zn
+ × Qq
+.
(4)

Learning When to Use a Decomposition
205
with sub-matrices of appropriate dimensions. Such a re-arrangement is called a
decomposition of (3) and ﬁnding it is called detection. Matrices Di are called
blocks, variables xℓare linking variables and (A1 · · · Aκ G)x ≥bℓare linking con-
straints. Such forms are interesting because a Dantzig-Wolfe reformulation can
be applied, leading to a solution of the resulting MIP by branch-and-price. Its
characteristic is that the linear relaxations in each node of the search tree are
solved by column generation, an alternation between solving pricing problems,
i.e., integer programs over Dixi ≥bi, and a linear program (the so-called master
problem) involving exponentially many variables and constraints systematically
derived from the linking constrains. See [4] for details (which are not necessary
for the following). The algorithmic burden is considerable but may pay oﬀif the
pricing problems capture a well solvable sub-structure of the model (3). In such
a case we call the decomposition good. When we know some good decomposi-
tion of a MIP model we call the model structured, if we do not know a good
one the model is “non-structured.” Automatic detection of decompositions, DW
reformulation, and branch-and-price is implemented in the GCG solver [4].
2.2
Features Considered
We now give an idea of the feature map φ that turns a MIP, decomposition,
and time limit into a vector φ(P, D, τ) ∈Rd that we give as input to supervised
learning classiﬁers. We deﬁne a large number of features (more than 100) to
catch as much information as possible, and then use a regularization approach
to avoid overﬁtting. We only sketch the main types, without being exhaustive.
Our ﬁrst features are instance statistics, like the number of variables, con-
straints, and non-zeros, the proportion of binary, integer, and continuous vari-
ables, or of certain types of constraints, such as knapsack or set covering. We
collect decomposition based statistics like the number κ of blocks, or the propor-
tion of non-zeros or variable/constraint types per block. As the dimension d of
φ is ﬁxed and κ varies across decompositions, we consider block statistics via
their average, variance, and quantiles. A small number of linking constraints and
variables is empirically considered good on “non-structured” MIPs [2].
Richer decomposition based features come from adjacencies, e.g., from the
bipartite graph with a vertex for each row i of A and each column j of A, and
an edge (i, j) iﬀaij ̸= 0. Blocks can be seen as clusters of vertices in this graph.
We can then build features inspired from graph clustering.
Many features can be obtained from the detectors themselves. The simplest
is the detector indicator feature 1t(D), which is a binary feature equal to one
iﬀdecomposition D was found by detector t (“D is of type t”). Some detectors
use metrics to evaluate the quality of their decomposition, for instance, whether
blocks are “equal.” These metrics can occur in φ. Some features mentioned above
play diﬀerent roles in diﬀerent types of decompositions. Type speciﬁc behavior
can be captured by products of detector indicators 1t with other features.
Finally, as the detection time varies from one instance to another, an impor-
tant feature is the time remaining τ −τdec after detection. Functions of this time
feature can be used within products with other features.

206
M. Kruber et al.
3
Preliminary Computational Results
3.1
Experimental Setup
As Dantzig-Wolfe decomposition leverages embedded structure in MIP instances,
we want to learn from a wide range of model structures. At the same time,
using a non-appropriate decomposition (i.e., assuming a “wrong” structure)
almost certainly leads to poor solver performance. It is therefore most impor-
tant to detect the “absence” of structure (or structure currently not exploitable
by GCG). We have therefore built a dataset of 300 “structured” (instances for
which an intuitive decomposition is known, e.g. from literature) and 100 “non-
structured” instances. We underline the fact that the information whether an
instance contains structure or not is not part of the input features. We considered
the following families of structured instances: vertex coloring (clr), set covering
(stcv), capacitated p-median (cpmp), survivable ﬁxed telecommunication net-
work design (sdlb), cutting stock (ctst), generalized assignment (gap), network
design (ntlb), lot sizing (ltsz), bin packing (bp), resource allocation (rap), sta-
ble set (stbl), and capacitated vehicle routing (cvrp) problems. The instances
assumed “non-structured” are randomly chosen from MIPLIB 2010 [7]. GCG
detected decompositions for each instance, leading to a total of 1619 decomposi-
tions. We launched SCIP and GCG with a time limit of two hours on each. Table 1
provides the number of instances per family and their diﬃculty, for which we use
as proxy the solution status of SCIP after 2 h. All experiments were performed
on a i7-2600 3.4 GHz PC, 8 MB cache, and 16 GB RAM, running OpenSuSE
Linux 13.1. We used the binary classiﬁers of the python scikit-learn library
[11], SCIP in version 3.2.1 [1], and GCG in version 2.1.1 [4].
Table 1. Number of instances listed per problem class and solution status of SCIP
All
clr stcv cpmp sdlb ctst gap ntlb ltsz bp rap stbl cvrp miplib
Instances 400
25
25
25
25
25
25
25
25
25
25
25
25
100
Opt. sol.
65.5% 19
3
18
10
25
23
25
25
6
12
22
6
68
Feas. sol. 31.5%
6
21
7
11
–
2
–
–
19
12
3
19
26
No sol.
3.0%
–
1
–
4
–
–
–
–
–
1
–
–
6
3.2
Overall Computational Summary of Our Experiment
We have randomly split our dataset into a training set containing 3/5 of the
instances and a test set containing the remaining. Table 2 aggregates the overall
results of our experiments when using a k-nearest neighbors (KNN) classiﬁer. We
report statistics for four solvers: the standard branch-and-cut MIP solver SCIP;
GCG that tries to detect a decomposition and perform a DW reformulation accord-
ingly; columns SL correspond to GCG with the learned classiﬁers (1) and (2) built
in (i.e., the proposal of this paper); and ﬁnally, columns OPT list the results we
would obtain if we always selected the best solver. The rows show, for the sets of

Learning When to Use a Decomposition
207
all, structured, and non-structured instances, the number of instances that could
not be solved to optimality, the total time needed on the respective entire test set,
and the geometric mean of the CPU times per instance. As deﬁned in (1) and (2)
the detection time is included in the timelimit for GCG, SL and OPT.
Table 2. Aggregated statistics on a test set of 131 instances, 99 structured and 32
non-structured. Overall GCG achieves a better performance than SCIP in 34 cases.
Instances
All
Structured
Non-structured
Solver
SCIP
GCG
SL
OPT
SCIP
GCG
SL
OPT SCIP
GCG
SL
OPT
No opt. sol.
52
66
44
39
39
37
31
26
13
29
14
13
CPU time (h) 111.3 142.6 93.1 85.7
83.5
82.2
65.9 58.5
27.8
56.8
29.2
27.2
Geo. mean (s) 127.1 370.4 78.6 67.8
73.4
146.9 39.2 32.2
672.9 5145.0 766.0 646.5
The OPT columns show that there is potential in applying a DW reformu-
lation on structured instances. However, we see that always using the current
default GCG as a standalone solver is not an option. It needs to be complemented
with an option to run SCIP in the case that a DW reformulation does not appear
to be promising. This is what we do with our supervised learning approach.
The resulting solver SL performs better than SCIP on structured instances with
only little performance deterioration on “non-structured” instances. This trend
should be conﬁrmed on a larger test set. The performance of SL relies on the
quality of the decisions taken by classiﬁers (1) and (2), that we now explain
in detail.
3.3
Deciding Whether to Continue Running GCG or to Start SCIP
Table 3 shows the ability of classiﬁer (1) to choose between SCIP and GCG given
the decompositions provided by GCG’s detectors and a time limit. The ﬁrst row
lists the share of instances where GCG and SCIP are respectively the best options.
The 2 × 2 cells below give the confusion matrix, based on the prediction of the
respective classiﬁers on the left side (rows) and on the true classes on the top
(columns). The diagonal of each cell corresponds to cases where the classiﬁer
has predicted the right answer. The top right corner of each cell corresponds to
false negatives (GCG is better, but we predict SCIP) and the bottom left to false
positives (SCIP is better but we predict GCG). As GCG may perform very poorly
when not a “right” structure is detected we try to keep the false positives low,
even if this implies accepting more false negatives and thus not exploiting the
full potential of GCG. The conﬁdence threshold α in (1) was set to 0.5 here.
Even if the size of the test set is too small to arrive at ﬁnal conclusions,
we identify some trends. The support vector machine with radial basis function
(RBF) classiﬁer shows a highly risk-averse behaviour with predicting GCG only
in 7.6% of all cases. Only half of its predictions are correct, which is a poor
performance. KNN and random forrests (RF) classiﬁers are willing to take more
risk and predict that GCG is the best option in 20.6% to 29.0% of all instances.

208
M. Kruber et al.
Table 3. Accuracy of solver selection, i.e., classiﬁer (1)
All instances
Structured
Non-structured
SCIP
GCG
SCIP
GCG
SCIP
GCG
Classiﬁer Pred
74.0% 26.0% 68.7% 31.3% 90.6% 9.4%
RBF
SCIP 73.3% 19.1% 66.7% 23.2% 93.8% 6.3%
Unbal.
GCG
3.8%
3.8%
5.1%
5.1%
0.0% 0.0%
KNN
SCIP 69.5%
9.9% 64.6% 11.1% 84.4% 6.3%
distance
GCG
6.9% 13.7%
7.1% 17.2%
6.3% 3.1%
RF
SCIP 63.4% 11.5% 55.6% 13.1% 87.5% 6.3%
Unbal.
GCG 10.7% 14.5% 13.1% 18.2%
3.1% 3.1%
RF
SCIP 60.3% 10.7% 50.5% 11.1% 90.6% 9.4%
Bal
GCG 13.7% 15.3% 18.2% 20.2%
0.0% 0.0%
With a true predictions over all GCG predictions ratio of around 2/3, KNN shows
the best precision. This explains that our SL scheme with KNN catches roughly
2/3 of the improvement potential of OPT with respect to SCIP in Table 2.
3.4
Selecting a Best Decomposition in GCG
Table 4 shows the percentage of instances on which classiﬁer (2) predicts the right
decomposition on the entire test set, on the subset of instances for which GCG
is selected by classiﬁer (1), and on the subset of instances such that GCG on the
best decomposition actually outperforms SCIP. The classiﬁer predicts the right
answer only on about half of the instances. However, in practice, this classiﬁer
is called only if GCG has been selected by classiﬁer (1). And on these instances,
the right answer is predicted on around 80% of the cases. This diﬀerence of
performance can be explained by the fact that there is no information on the
relative performance of GCG on instances where SCIP performs better than GCG on
all decompositions in the training dataset. A direction to include this information
and improve the performance is to use a one-versus-one approach, where binary
classiﬁers take two decompositions in input and predict which one is better,
instead of using our “one-versus-SCIP” approach.
Table 4. Accuracy of best decomposition selection, i.e., classiﬁer (2)
Classiﬁer
All instances GCG predicted by (1) GCG better
RBF
42.7%
80.0%
76.7%
KNN
58.8%
88.9%
77.4%
RF unbalanced 51.1%
72.7%
76.5%
RF balanced
64.9%
71.1%
79.4%

Learning When to Use a Decomposition
209
4
Discussion and Future Work
It is our understanding that DW reformulation will be successful in terms of
solver performance only on a small fraction of MIPs. It is therefore crucial to reli-
ably decide whether a reformulation will pay oﬀon a given input, or not. Several
factors inﬂuence the success of the reformulation, among them the strength of
the relaxation, an acceptable computational burden of repeatedly solving many
subproblems, etc. An experienced modeler may have a sense for such factors, and
this work aims equipping the GCG solver with a sort of such a sense. Our prelim-
inary computational experience suggests that a supervised learning approach be
promising. There are three immediate next steps.
1. We are currently completely re-designing the detection loop in GCG, lead-
ing to many more potential decompositions, and thus a much richer training
set for our learning algorithms. 2. Detection can be very time consuming. One
should predict before detection if the decomposition detected will be worth being
used. This may save detection time in particular on “non-structured” instances
where GCG is currently unable to ﬁnd a good decomposition. 3. The quality of
the features given as input to a machine learning classiﬁer determines its perfor-
mance. A direction to improve feature quality is to use run time, i.e., a posteriori
information about a decomposition. One could run column generation on each
decomposition for a limited amount of time in order to collect solution time and
integrality gap of pricing problems, degeneracy of and dual bound provided by
the master problem, etc. Figure 2 illustrates such a “strong detection” solution
scheme. Reporting about this ongoing work is postponed to the full version of
this short paper and will be implemented in a near future release of GCG.
MIP
Dec?
Detection
DEC
DW?
yes
timelimit
Status
yes
Cont?
GCG
SCIP
no
no
yes
no
Fig. 2. Three decisions taken along a “strong detection” solution scheme
References
1. Achterberg, T.: SCIP: solving constraint integer programs. Math. Program. Com-
put. 1(1), 1–41 (2009)
2. Bergner, M., Caprara, A., Ceselli, A., Furini, F., L¨ubbecke, M.E., Malaguti, E.,
Traversi, E.: Automatic Dantzig-Wolfe reformulation of mixed integer programs.
Math. Program. 149(1–2), 391–424 (2015)
3. Caprara, A., Furini, F., Malaguti, E.: Uncommon Dantzig-Wolfe reformulation for
the temporal knapsack problem. INFORMS J. Comput. 25(3), 560–571 (2013)

210
M. Kruber et al.
4. Gamrath, G., L¨ubbecke, M.E.: Experiments with a generic Dantzig-Wolfe decom-
position for integer programs. In: Festa, P. (ed.) SEA 2010. LNCS, vol. 6049, pp.
239–252. Springer, Heidelberg (2010). doi:10.1007/978-3-642-13193-6 21
5. Genton, M.G.: Classes of kernels for machine learning: a statistics perspective. J.
Mach. Learn. Res. 2, 299–312 (2001)
6. Khalil, E., Le Bodic, P., Song, L., Nemhauser, G., Dilkina, B.: Learning to branch
in mixed integer programming. In: Proceedings of the 30th AAAI Conference on
Artiﬁcial Intelligence (2016)
7. Koch, T., Achterberg, T., Andersen, E., Bastert, O., Berthold, T., Bixby, R.E.,
Danna, E., Gamrath, G., Gleixner, A.M., Heinz, S., Lodi, A., Mittelmann, H.,
Ralphs, T., Salvagnin, D., Steﬀy, D.E., Wolter, K.: MIPLIB 2010. Math. Program.
Comput. 3(2), 103–163 (2011)
8. Lin, Y., Jeon, Y.: Random forests and adaptive nearest neighbors. J. Am. Stat.
Assoc. 101(474), 578–590 (2006)
9. Marcos Alvarez, A., Louveaux, Q., Wehenkel, L.: A machine learning-based approx-
imation of strong branching. INFORMS J. Comput. 29(1), 185–195 (2014)
10. Marcos Alvarez, A., Wehenkel, L., Louveaux, Q.: Machine learning to balance the
load in parallel branch-and-bound (2015)
11. Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O.,
Blondel, M., Prettenhofer, P., Weiss, R., Dubourg, V., Vanderplas, J., Passos, A.,
Cournapeau, D., Brucher, M., Perrot, M., Duchesnay, E.: Scikit-learn: machine
learning in python. J. Mach. Learn. Res. 12, 2825–2830 (2011)
12. Wang, J., Ralphs, T.: Computational experience with hypergraph-based methods
for automatic decomposition in discrete optimization. In: Gomes, C., Sellmann, M.
(eds.) CPAIOR 2013. LNCS, vol. 7874, pp. 394–402. Springer, Heidelberg (2013).
doi:10.1007/978-3-642-38171-3 31
13. Xu, L., Hutter, F., Hoos, H.H., Leyton-Brown, K.: Hydra-MIP: automated algo-
rithm conﬁguration and selection for mixed integer programming. In: RCRA Work-
shop on Experimental Evaluation of Algorithms for Solving Problems with Combi-
natorial Explosion at the International Joint Conference on Artiﬁcial Intelligence
(IJCAI), July 2011

Experiments with Conﬂict Analysis in Mixed
Integer Programming
Jakob Witzig1(B), Timo Berthold2, and Stefan Heinz2
1 Zuse Institute Berlin, Takustr. 7, 14195 Berlin, Germany
witzig@zib.de
2 Fair Isaac Germany GmbH, Takustr. 7, 14195 Berlin, Germany
{timoberthold,stefanheinz}@fico.com
Abstract. The analysis of infeasible subproblems plays an important
role in solving mixed integer programs (MIPs) and is implemented in
most major MIP solvers. There are two fundamentally diﬀerent con-
cepts to generate valid global constraints from infeasible subproblems.
The ﬁrst is to analyze the sequence of implications, obtained by domain
propagation, that led to infeasibility. The result of the analysis is one or
more sets of contradicting variable bounds from which so-called conﬂict
constraints can be generated. This concept has its origin in solving satis-
ﬁability problems and is similarly used in constraint programming. The
second concept is to analyze infeasible linear programming (LP) relax-
ations. The dual LP solution provides a set of multipliers that can be
used to generate a single new globally valid linear constraint. The main
contribution of this short paper is an empirical evaluation of two ways
to combine both approaches. Experiments are carried out on general
MIP instances from standard public test sets such as Miplib2010; the
presented algorithms have been implemented within the non-commercial
MIP solver SCIP. Moreover, we present a pool-based approach to man-
age conﬂicts which addresses the way a MIP solver traverses the search
tree better than aging strategies known from SAT solving.
1
Introduction: MIP and Conﬂict Analysis
In this paper we consider mixed integer programs (MIPs) of the form
c⋆= min{ctx | Ax ≥b, ℓ≤x ≤u, x ∈Zk × Rn−k},
(1)
with objective coeﬃcient vector c ∈Rn, constraint coeﬃcient matrix A ∈Rm×n,
constraint left-hand side b ∈Rm, and variable bounds ℓ, u ∈R
n, where R : = R∪
{±∞}. Furthermore, let N = {1, . . . , n} be the index set of all variables.
When omitting the integrality requirements, we obtain the linear program
(LP)
c⋆
LP = min{ctx | Ax ≥b, ℓ≤x ≤u, x ∈Rn}.
(2)
The linear program (2) is called the LP relaxation of (1). The LP relaxation pro-
vides a lower bound on the optimal solution value of the MIP (1), i.e., c⋆
LP ≤c⋆.
c
⃝Springer International Publishing AG 2017
D. Salvagnin and M. Lombardi (Eds.): CPAIOR 2017, LNCS 10335, pp. 211–220, 2017.
DOI: 10.1007/978-3-319-59776-8 17

212
J. Witzig et al.
In LP-based branch-and-bound [11,18], the most commonly used method to
solve MIPs, the LP relaxation is used for bounding. Branch-and-bound is a
divide-and-conquer method that splits the search space sequentially into smaller
subproblems, which are (hopefully) easier to solve. During this procedure, we
may encounter infeasible subproblems. Infeasibility can be detected by contra-
dicting implications, e.g., derived by domain propagation, or by an infeasible LP
relaxation. Modern MIP solvers try to ‘learn’ from infeasible subproblems, e.g.,
by conﬂict analysis. Conﬂict analysis for MIP has its origin in solving satisﬁa-
bility problems (SAT) and goes back to [21]. Similar ideas are used in constraint
programming, e.g., see [14,15,25]. The ﬁrst suggestions of using conﬂict analy-
sis techniques in MIP were by [2,12,24]. Further publications suggested the use
conﬂict information for variable selection in branching to tentatively generate
conﬂicts before branching [3,16] and to analyze infeasibility detected in primal
heuristics [6,7].
Today, conﬂict analysis is widely established in solving MIPs. The principal
idea of conﬂict analysis, in MIP terminology, can be sketched as follows.
Given an infeasible node of the branch-and-bound tree deﬁned by the sub-
problem
min{ctx | Ax ≥b, ℓ′ ≤x ≤u′, x ∈Zk × Rn−k}
(3)
with local bounds ℓ≤ℓ′ ≤u′ ≤u. In LP-based branch-and-bound, the infeasi-
bility of a subproblem is typically detected by an infeasible LP relaxation (see
next section) or by contradicting implications.
In the latter case, a conﬂict graph gets constructed which represents the logic
of how the set of branching decisions led to the detection of infeasibility. More
precisely, the conﬂict graph is a directed acyclic graph in which the vertices
represent bound changes of variables and the arcs (v, w) correspond to bound
changes implied by propagation, i.e., the bound change corresponding to w is
based (besides others) on the bound change represented by v. In addition to
these inner vertices which represent the bound changes from domain propaga-
tion, the graph features source vertices for the bound changes that correspond
to branching decisions and an artiﬁcial sink vertex representing the infeasibil-
ity. Then, each cut that separates the branching decisions from the artiﬁcial
infeasibility vertex gives rise to a valid conﬂict constraint. A conﬂict constraint
consists of a set of variables with associated bounds, requiring that in each feasi-
ble solution at least one of the variables has to take a value outside these bounds.
Note that in general, this is not a linear constraint and that by using diﬀerent
cuts in the graph, several diﬀerent conﬂict constraints might be derived from
a single infeasibility. A variant of conﬂict analysis close to the one described
above is implemented in SCIP, the solver in which we will conduct our compu-
tational experiments. Also, a similar implementation is available in the FICO
Xpress-Optimizer.
This short paper consists of two parts which are independent but complement
each other in practice. The ﬁrst part of this paper (Sect. 2) focuses on a MIP
technique to analyze infeasibility based on LP theory. We discuss the interaction,

Experiments with Conﬂict Analysis in Mixed Integer Programming
213
diﬀerences, and commonalities between conﬂict analysis and the so-called dual
ray analysis. Although both techniques have been known before, e.g., [2,22], this
will be, to the best of our knowledge, the ﬁrst published direct comparison of the
two. In the second part (Sect. 3), we present a new approach to drop conﬂicts
that do not lead to variable bound reductions frequently. This new concept
is an alternative to the aging scheme known from SAT. Finally, we present
computational experiments comparing the techniques described in Sects. 2 and 3.
Applying a combined approach of conﬂict and dual ray analysis is the main
novelty of this paper.
2
Analyzing Dual Unbounded Solutions
The idea of conﬂict analysis is tightly linked to domain propagation: conﬂict
analysis studies a sequence of variable bound implications made by domain prop-
agation routines. Besides domain propagation, there is another important sub-
routine in MIP solving which might prove infeasibility of a subproblem: the LP
relaxation. The proof of LP infeasibility comes in the form of a so-called “dual
ray”, which is a list of multipliers on the model constraints and the variable
bounds. Those give rise to a globally valid constraint that can be used simi-
larly to a conﬂict constraint. In this section, we discuss the analysis of the LP
infeasibility proof in more detail.
2.1
Analysis of Infeasible LPs: Theoretical Background
Consider a node of the branch-and-bound tree and the corresponding subproblem
of type (3) with local bounds ℓ≤ℓ′ ≤u′ ≤u. The dual LP of the corresponding
LP relaxation of (3) is given by
max{ytb + rtℓ′ + rtu′ | Aty + r + r = c, y, r ∈Rn
≥0, r ∈Rn
≤0},
(4)
where A·i is the i-th column of A, ri = max{0, ci −ytA·i}, and ri = min{0, ci −
ytA·i}. By LP theory each unbounded ray (γ, r, r) of (4) proves infeasibility
of (3). A ray is called unbounded if multiplying the ray with an arbitrary scalar
α > 0 will not change the feasibility. Note, in this case it holds
ri = max{0, −ytA·i}
and
ri = min{0, −ytA·i}.
Moreover, the Lemma of Farkas states that exactly one of the following two
systems is satisﬁable
(F1)
Ax ≥b
ℓ′ ≤x ≤u′

˙∨

γtA + r + r = 0
γtb + rtℓ′ + rtu′ > 0
(F2)
It follows immediately, that if F1 is infeasible, there exists an unbounded ray
(γ, r, r) of (4) satisfying F2. An infeasibility proof of (3) is given by a single
constraint
γtAx ≥γtb,
(5)

214
J. Witzig et al.
which is an aggregation of all rows Aj· for j = 1, . . . , m with weight γj >
0. Constraint (5) is globally valid but violated in the local bounds [ℓ′, u′] of
subproblem (3). In the following, this constraint will be called a proof-constraint.
2.2
Conﬂict Analysis of Infeasible LPs
The analysis of an infeasible LP relaxation, as it is implemented in SCIP, is a
hybrid of the theoretical considerations made in Sect. 2.1 and the analysis of
the conﬂict graph known from SAT. To use the concept of a conﬂict graph,
all variables with a non-zero coeﬃcient in the proof-constraint are converted to
vertices of the conﬂict graph representing bound changes; global bound changes
are omitted. Those vertices, called the initial reason, are then connected to the
artiﬁcial sink representing the infeasibility. This neat idea was introduced in [1].
From thereon, conﬂict analysis can be applied as described in Sect. 1.
In practice, the proof-constraint is often quite dense, and therefore, it might
be worthwhile to search for a sparser infeasibility proof. This can be done by a
heuristic that relaxes some of the local bounds [ℓ′, u′] that appear in the proof-
constraint. Of course, the relaxed local bounds [ℓ′′, u′′] with ℓ< ℓ′′ ≤ℓ′ ≤u′ ≤
u′′ < u still need to fulﬁll γtb + rtℓ′′ + rtu′′ > 0. The more the bounds can be
relaxed by this approach, the smaller becomes the initial reason. Consequently,
the derived conﬂict constraints become stronger. Note again that these con-
straints do not need to be linear, if general integer or continuous variables are
present.
2.3
Dual Ray Analysis of Infeasible LPs
The proof-constraint is globally valid but infeasible within the local bounds. It
follows immediately by the Lemma of Farkas that the maximal activity
Δmax(γtA, ℓ′, u′) :=

i∈N : γtAi>0
(γtAi)u′
i +

i∈N : γtAi<0
(γtAi)ℓ′
i
of γtAx w.r.t. variable bounds [ℓ′, u′] is strictly less than the corresponding left-
hand side γtb.
Instead of creating an “artiﬁcial” initial reason, the proof-constraint might
also be used directly for domain propagation in the remainder of the search.
It is a conical combination of global constraints, i.e., it is itself a valid (but
redundant) global constraint. In contrast to the method described in Sect. 2.2,
using a dual unbounded ray as a set of weights to aggregate model constraints
yields exactly one linear constraint.
The proof-constraint along with an activity argument can be used to deduce
local lower and upper variable bounds [2]. Consider a subproblem with local
bounds [ℓ′, u′]. For any i ∈N with a non-zero coeﬃcient in the proof-constraint
the maximal activity residual is given by
Δi
max(γtA, ℓ′, u′) :=

j∈N\i: γtAj>0
(γtAj)u′
j +

j∈N\i: γtAj<0
(γtAj)ℓ′
j,

Experiments with Conﬂict Analysis in Mixed Integer Programming
215
i.e., the maximal activity over all variables but xi. Hence, valid local bounds are
given by
γtb −Δi
max(γtA, ℓ′, u′)
ai
≤
≥

xi
if ai > 0
if ai < 0 .
This is the bound tightening procedure [9] which is widely used in all major MIP
solvers for all kinds of linear constraints.
Just like the dual ray might be heuristically reduced to get a short initial rea-
son for conﬂict analysis, it might be worthwhile to alter the proof-constraint itself
before using it for propagation. This can include the application of presolving
steps such as coeﬃcient tightening to the constraint, projecting out continuous
variables or applying mixed-integer rounding to get an alternative globally valid
constraint that might be more powerful in propagation.
Finally, instead of generating a valid constraint from the dual ray, one could
equivalently use the ray itself to simply check for infeasibility [22,23] or to esti-
mate the objective change during branch-and-bound and to derive branching
decisions therefrom. While in Sect. 2.2, we described a way to reduce LP infeasi-
bility analysis to conﬂict analysis based on domain propagation, one could also
try to generate a dual ray by solving the LP relaxation after having detected
infeasibility by propagation.
3
Managing of Conﬂicts in a MIP Solver
Maintaining and propagating large numbers of conﬂict constraints might have
a negative impact on solver performance and create a big burden memory-wise.
For instances with a high throughput of branch-and-bound nodes, a solver like
SCIP might easily create hundreds of thousands of conﬂicts within an hour of
running time. In order to avoid a slowdown or memory short-coming, an aging
mechanism is used within SCIP. Once again, aging is a concept inspired by
SAT and CP solving. Every time a conﬂict constraint is considered for domain
propagation an age counter (individually for each constraint) is increased if no
deduction was found. If a deduction is found, the age will be reset to 0. If the age
reaches a predeﬁned threshold the conﬂict constraint is permanently deleted.
In SAT and CP, this mechanism is a well-established method to drop con-
ﬂict constraints that are not frequently propagated. In the case of MIP solving,
there are two main diﬀerences concerning the branch-and-bound search. First,
domain propagation is most often not the most expensive part of node process-
ing. Second, SAT and CP solvers often use a pure depth-ﬁrst-search (DFS) node
selection, while state-of-the-art MIP solvers use some hybrid between DFS and
best-estimate-search or best-ﬁrst-search (see, e.g., [2,5,20]). Therefore, it fre-
quently happens that the node processed next is picked from a diﬀerent part of
the tree.
In the following, we describe a pool-based approach to manage conﬂict con-
straints. Here, a pool refers to a ﬁxed-size array that allows direct access to a
particular element and is independent of the model itself. The conﬂict pool is

216
J. Witzig et al.
used to manage all conﬂict constraints, independently whether they were derived
from domain propagation or an infeasible LP relaxation. The number of con-
straints that can be stored within the conﬂict pool at the same is limited. In
our implementation the maximal size of the conﬂict pool depends on the num-
ber of variables and constraints of the presolved problem. However, the pool
provides space for at least 1 000 and at most 50 000 conﬂict constraints at the
same time. The conﬂict pool allows a central management of conﬂict constraints
independently from the model constraints, i.e., they can be propagated, checked
or deleted separately, without the need to traverse through all constraints.
To drop conﬂict constraints that don’t frequently lead to deductions we
implemented an update-routine that checks the conﬂict pool regularly, e.g., any
time we create the ﬁrst new conﬂict at a node. Moreover, we still use the concept
of aging to determine the conﬂict constraints that are rarely used in propagation.
Within this update procedure the oldest conﬂict constraints are removed.
Beside the regular checks, the conﬂict pool is updated every time a new
improving incumbent solution is found. Conﬂict constraints might depend on a
(previous) best known solution, e.g., when the conﬂict was created from an LP
whose infeasibility proof contained the objective cutoﬀ. Such conﬂicts become
weaker whenever a new incumbent is found and the chance that they lead to
deductions becomes smaller the more the incumbent improves. Due to this, for
each conﬂict constraint involving an incumbent solution we store the correspond-
ing objective value. If this value is suﬃciently worse than the new objective value,
the conﬂict constraint will be permanently deleted. In our computational exper-
iments (cf. Sect. 4) we use a threshold of 5%.
4
Computational Experiments
In our computational experiments, we compare combinations of the techniques
presented in this paper: conﬂict analysis and dual ray analysis. To the best of
our knowledge, most major MIP solvers either use conﬂict analysis of infeasible
LPs and domain propagation (e.g., SCIP, FICO Xpress-Optimizer) or they employ
dual ray analysis (e.g., Gurobi, SAS). We will refer to the former as the conflict
setting and to the latter as the dualray setting. We compare those to a setting
that uses conﬂict analysis and dual ray analysis simultaneously, the combined
setting. Finally, we consider an extension of the combined setting that uses a
pool for conﬂict management, the setting combined+pool.
All experiments were performed with the non-commercial MIP solver
SCIP [13] (git hash 60f49ab, based on SCIP 3.2.1.2), using SoPlex 2.2.1.3 as
LP solver. The experiments were run on a cluster of identical machines, each
with an Intel Xeon Quad-Core with 3.2 GHz and 48 GB of RAM; a time limit of
3600 s was set.
We used two test sets: the Miplib2010 [17] benchmark test set and a selec-
tion of instances taken from the Miplib [8], Miplib2003 [4], Miplib2010, the
Cor@l [19] collection, the Alu1, and the Markshare [10] test sets. From these
1 The instances are part of the contributed section of Miplib2003.

Experiments with Conﬂict Analysis in Mixed Integer Programming
217
we selected all instances for which (i) all of the above settings need at least 100
nodes, (ii) at least one setting ﬁnishes within the time limit of 3600 s, and (iii)
at least one setting analyzes more than 100 infeasible subproblems successfully.
We refer to this test set as the Conflict set, since it was designed to contain
instances for which conﬂict or dual ray analysis is frequently used.
Aggregated results on the number of generated nodes and needed solving
time can be found in Table 1. Detailed results for each instance and test set
separately can be found in the appendix of [26]. We use the conflict setting
as a base line (since it used to be the SCIP default), for which we give actual
means of branch-and-bound nodes and the solving time. For all other settings,
we instead give factors w.r.t. the base line. A number greater than one implies
that the setting is inferior and a number less than one implies that the setting
is superior to the conflict setting.
First of all, we observe that solely using dual ray analysis is inferior to using
conﬂict analysis on both test sets w.r.t. both performance measures. Note that
we used a basic implementation of dual ray analysis; a solver that solely relies
on it might implement further extensions that decrease this diﬀerence in perfor-
mance, see also Sect. 5. However, the combination of conﬂict and dual ray analy-
sis showed some signiﬁcant performance improvements. We observed a speed-up
of 3% and 18% on Miplib2010 and Conflict, respectively. Moreover, the num-
ber of generated nodes could be reduced by 5% and 25%, respectively. Finally,
on the Conflict test set, the combined setting solved one instance more than
the conflict setting and ﬁve more than the dualray setting. We take those
results as an indicator that the two techniques complement each other well. In
an additional experiment, we also tested applying conﬂict analysis solely from
domain propagation or solely from infeasible LPs. Both variants were inferior to
the conflict setting and are therefore not discussed in detail. Detailed results
for each instance and test set separately can be found in the appendix of [26].
To partially explain the diﬀerent extent of the improvements on both test
sets, we would like to point out that in the Miplib2010 benchmark set, there
are only 31 instances which fulﬁll the ﬁltering criteria mentioned above for the
Conflict set. On those, the combined setting is 7.2% faster and needs 15.6%
less nodes than the conflict setting.
Looking at individual instances, there are a few cases for which the combined
setting is the clear winner, e.g., neos-849702 or bnatt350. For neos-849702 and
bnatt350, the dualray setting has a timeout, while the conflict setting is a
factor of 6.2 and 1.83 slower, respectively, than the combined setting. At the
same time, ns1766074 shows the largest deterioration from using a combined
setting, being a factor of 1.63 slower than conflict and a factor of 1.06 slower
than dualray. Generally, we observed that for infeasible MIPs (like ns1766074),
the conflict setting was preferable over combined.
As can be seen in Table 1, using a conﬂict pool in addition to an
aging system makes hardly any diﬀerence w.r.t. the overall performance
in combination with the combined setting. However, looking at individual
instances, there are cases where using a pool leads to a speed up of 17%

218
J. Witzig et al.
Table 1. Aggregated computational results. Columns marked with # show the number
of solved instances. Columns 3 and 4 show the shifted geometric mean of absolute
numbers of generated nodes (n, shift = 100) and needed solving time in seconds (t,
shift = 10), respectively. All remaining columns show the relative number of generated
nodes (nQ) and needed solving time (tQ) w.r.t. Columns 3 and 4, respectively.
conflict
dualray
combined
combined+pool
Test set
#
n
t
#
nQ
tQ
#
nQ
tQ
#
nQ
tQ
Miplib2010
60
14382
686
57
1.365 1.167
60
0.955
0.977
60
0.957
0.975
Conflict
105
16769
143
101
1.616 1.256 106
0.755
0.827 106
0.759 0.829
(e.g., ns1766074) and 20% (e.g., neos-1620807) compared to the combined
setting on the Miplib2010 and Conflict test set, respectively. Moreover, using
the presented pool-based approach to manage conﬂicts in combination with the
conflict setting shows a decrease of solving time of 8.5% (Miplib2010) and
5.5% (Conflict) on instances where more than 50 000 infeasible LPs were ana-
lyzed successfully.
5
Conclusion and Outlook
In this short paper we discussed the similarities and diﬀerences of conﬂict analy-
sis and dual ray analysis in solving MIPs. Our computational study indicates
that a combination of both approaches can signiﬁcantly enhance the perfor-
mance of a state-of-the-art MIP solver and help each other. In our opinion both
approaches complement each other well and on instances where the analysis of
infeasible subproblems succeeds frequently, the solving time improved by 17.3%
and the number of branch-and-bound nodes by 24.5%. In contrast, using a pool-
based approach in addition to an aging mechanism to manage conﬂict constraints
showed very little impact.
There are several instances for which using either dual ray analysis or conﬂict
analysis exclusively outperformed the combination of both. Thus, we will plan to
investigate a dynamic mechanism to switch between both techniques. Further-
more, applying dual ray analysis for infeasibility deduced by domain propagation
as well as using more preprocessing (e.g., mixed integer rounding, projecting out
continuous variables, etc.) techniques to modify constraints derived from dual
ray analysis appear as promising directions for future research.
Acknowledgments. The work for this article has been conducted within the Research
Campus Modal funded by the German Federal Ministry of Education and Research
(fund number 05M14ZAM). We thank the anonymous reviewers for their valuable
suggestions and helpful comments.

Experiments with Conﬂict Analysis in Mixed Integer Programming
219
References
1. Achterberg, T.: Conﬂict analysis in mixed integer programming. Discrete Optim.
4(1), 4–20 (2007)
2. Achterberg, T.: Constraint integer programming (2007)
3. Achterberg, T., Berthold, T.: Hybrid branching. In: Hoeve, W.-J., Hooker, J.N.
(eds.) CPAIOR 2009. LNCS, vol. 5547, pp. 309–311. Springer, Heidelberg (2009).
doi:10.1007/978-3-642-01929-6 23
4. Achterberg, T., Koch, T., Martin, A.: MIPLIB 2003. Oper. Res. Lett. 34(4), 361–
372 (2006)
5. B´enichou, M., Gauthier, J.-M., Girodet, P., Hentges, G., Ribi`ere, G., Vincent, O.:
Experiments in mixed-integer linear programming. Math. Program. 1(1), 76–94
(1971)
6. Berthold, T., Gleixner, A.M.: Undercover: a primal MINLP heuristic exploring a
largest sub-MIP. Math. Program. 144(1–2), 315–346 (2014)
7. Berthold, T., Hendel, G.: Shift-and-propagate. J. Heuristics 21(1), 73–106 (2015)
8. Bixby, R.E., Boyd, E.A., Indovina, R.R.: MIPLIB: a test set of mixed integer
programming problems. SIAM News 25, 16 (1992)
9. Brearley, A., Mitra, G., Williams, H.: Analysis of mathematical programming prob-
lems prior to applying the simplex algorithm. Math. Program. 8, 54–83 (1975)
10. Cornu´ejols, G., Dawande, M.: A class of hard small 0-1 programs. In: Bixby, R.E.,
Boyd, E.A., R´ıos-Mercado, R.Z. (eds.) IPCO 1998. LNCS, vol. 1412, pp. 284–293.
Springer, Heidelberg (1998). doi:10.1007/3-540-69346-7 22
11. Dakin, R.J.: A tree-search algorithm for mixed integer programming problems.
Comput. J. 8(3), 250–255 (1965)
12. Davey, B., Boland, N., Stuckey, P.J.: Eﬃcient intelligent backtracking using linear
programming. INFORMS J. Comput. 14(4), 373–386 (2002)
13. Gamrath, G., Fischer, T., Gally, T., Gleixner, A.M., Hendel, G., Koch, T., Maher,
S.J., Miltenberger, M., M¨uller, B., Pfetsch, M.E., Puchert, C., Rehfeldt, D.,
Schenker, S., Schwarz, R., Serrano, F., Shinano, Y., Vigerske, S., Weninger, D.,
Winkler, M., Witt, J.T., Witzig, J.: The SCIP optimization suite 3.2. Technical
Report 15–60, ZIB, Takustr. 7, 14195 Berlin (2016)
14. Ginsberg, M.L.: Dynamic backtracking. J. Artif. Intell. Res. 1, 25–46 (1993)
15. Jiang, Y., Richards, T., Richards, B.: No-good backmarking with min-conﬂict
repair in constraint satisfaction and optimization. In: PPCP, vol. 94, pp. 2–4.
Citeseer (1994)
16. Kılın¸c Karzan, F., Nemhauser, G.L., Savelsbergh, M.W.P.: Information-based
branching schemes for binary linear mixed-integer programs. Math. Program. Com-
put. 1(4), 249–293 (2009)
17. Koch, T., Achterberg, T., Andersen, E., Bastert, O., Berthold, T., Bixby, R.E.,
Danna, E., Gamrath, G., Gleixner, A.M., Heinz, S., Lodi, A., Mittelmann, H.,
Ralphs, T., Salvagnin, D., Steﬀy, D.E., Wolter, K.: MIPLIB 2010. Math. Program.
Comput. 3(2), 103–163 (2011)
18. Land, A.H., Doig, A.G.: An automatic method of solving discrete programming
problems. Econometrica 28(3), 497–520 (1960)
19. Linderoth, J.T., Ralphs, T.K.: Noncommercial software for mixed-integer linear
programming. Integer Program.: Theor. Pract. 3, 253–303 (2005)
20. Linderoth, J.T., Savelsbergh, M.W.: A computational study of search strategies
for mixed integer programming. INFORMS J. Comput. 11(2), 173–187 (1999)

220
J. Witzig et al.
21. Marques-Silva, J.P., Sakallah, K.: Grasp: a search algorithm for propositional sat-
isﬁability. IEEE Trans. Comput. 48(5), 506–521 (1999)
22. P´olik, I.: (Re)using dual information in MILP. In: INFORMS Computing Society
Conference, Richmond, VA (2015)
23. P´olik, I.: Some more ways to use dual information in MILP. In: International
Symposium on Mathematical Programming, Pittsburgh, PA (2015)
24. Sandholm, T., Shields, R.: Nogood learning for mixed integer programming. In:
Workshop on Hybrid Methods and Branching Rules in Combinatorial Optimiza-
tion, Montr´eal (2006)
25. Stallman, R.M., Sussman, G.J.: Forward reasoning and dependency-directed back-
tracking in a system for computer-aided circuit analysis. Artif. Intell. 9(2), 135–196
(1977)
26. Witzig, J., Berthold, T., Heinz, S.: Experiments with conﬂict analysis in mixed
integer programming. Technical report 16–63, ZIB, Takustr. 7, 14195 Berlin (2016)

A First Look at Picking Dual Variables
for Maximizing Reduced Cost Fixing
Omid Sanei Bajgiran1,3(B), Andre A. Cire1, and Louis-Martin Rousseau2,3
1 Department of Management, University of Toronto Scarborough, Toronto, Canada
omid.saneibajgiran@rotman.utoronto.ca
2 Department of Mathematics and Industrial Engineering,
Polytechnique Montr´eal, Montreal, Canada
3 Interuniversity Research Center on Enterprise Networks,
Logistics and Transportation (CIRRELT), Montreal, Canada
Abstract. Reduced-cost-based ﬁltering in constraint programming and
variable ﬁxing in integer programming are techniques which allow to cut
out part of the solution space which cannot lead to an optimal solution.
These techniques are, however, dependent on the dual values available
at the moment of pruning. In this paper, we investigate the value of
picking a set of dual values which maximizes the amount of ﬁltering (or
ﬁxing) that is possible. We test this new variable-ﬁxing methodology
for arbitrary mixed-integer linear programming models. The resulting
method can be naturally incorporated into existing solvers. Preliminary
results on a large set of benchmark instances suggest that the method
can eﬀectively reduce solution times on hard instances with respect to a
state-of-the-art commercial solver.
Keywords: Mixed-integer programming · Variable ﬁxing methodology ·
Reduced-cost based ﬁltering
1
Introduction
A key feature of modern mathematical programming solvers refers to the wide
range of techniques that are applied to simplify an instance. Typically considered
during a preprocessing stage, these techniques aim at ﬁxing variables, eliminating
redundant constraints, and identifying structure that can either lead to speed-
ups in solution times or provide useful information about the model at hand.
Examples of valuable information include, e.g., potential numerical issues or
which subset of inequalities and variables may be responsible for the infeasibility
[12], if that is the case. These simpliﬁcation methods alone reduce solution times
by half in state-of-the-art solvers such as CPLEX, SCIP, or Gurobi [4], thereby
constituting an important tool in the use of mixed-integer linear programming
(MILP) in practical real-world problems [11].
In this paper we investigate a new simpliﬁcation technique that expands
upon the well-known reduced cost ﬁxing method, ﬁrst mentioned by Balas and
c
⃝Springer International Publishing AG 2017
D. Salvagnin and M. Lombardi (Eds.): CPAIOR 2017, LNCS 10335, pp. 221–228, 2017.
DOI: 10.1007/978-3-319-59776-8 18

222
O.S. Bajgiran et al.
Martin [2] and largely used both in the mathematical programming and the con-
straint programming (CP) communities. The underlying idea of the method is
straightforward: Given a linear programming (LP) model and any optimal solu-
tion to such a model, the reduced cost of a variable indicates the marginal linear
change in the objective function when the value of the variable in that solution
is increased [5]. In cases where the LP encodes a relaxation of an arbitrary opti-
mization problem, we can therefore ﬁlter all values from a variable domain that,
based on the reduced cost, incur a new objective function value that is worse
than a known solution to the original problem. The result is a tighter variable
bound which can then trigger further variable ﬁxing and other simpliﬁcations.
This simple but eﬀective technique is widely applied in MILP presolving
[4,11,12] and plays a key role in a variety of propagation methods for global con-
straints in CP [7–9]. It can be easily incorporated into solvers since the reduced
costs are directly derived from any optimal set of duals, which in turn can be
eﬃciently obtained by solving an LP once. The technique is also a natural way of
exploiting the strengths of MILP within a CP framework, since the dual values
incorporate a global bound information that is potentially lost when processing
constraints one at a time (a concept that is explored, e.g., in [3,17,19]).
However, in all cases typically only one reduced cost per variable is consid-
ered, that is, the one obtained after solving the LP relaxation of a MILP. In
theory, any set of feasible duals provides valid reduced costs that may lead, in
turn, to quite diﬀerent variable bound tightenings. This question was originally
raised by Sellmann [16], who demonstrated that not only distinct dual vectors
would result in signiﬁcantly diﬀerent ﬁltering behaviors, but that potentially
sub-optimal dual vectors could yield much more pruning than the optimal ones.
Our goal in this work is to investigate the potential eﬀect of picking the
dual vector that maximizes reduced-cost-based ﬁltering. By doing so, we revisit
the notion of relaxed consistency for reduced costs ﬁxing; that is, we wish to
inﬂuence the choice of the dual values given by a relaxation so as to maximize
the amount of pruning that can be performed. We view the proposed techniques
as a ﬁrst direction towards answering some of the interesting questions raised
in the ﬁeld of CP-based Lagrangian relaxation [3,16], in particular related to
how to select the dual variables (or, equivalently, the Lagrangian multipliers) to
maximize propagation.
The contribution of this paper is to formulate the problem of ﬁnding the
dual vectors that maximize the number of reductions as an optimization problem
deﬁned over the space of optimal (or just feasible) dual values. We compare this
approach to achieving full relaxed consistency, which can be obtained by solving
a large (but polynomial) number of LP problems. The resulting technique can
be seamlessly incorporated into existing solvers, and preliminary results over the
MIPLIB indicate that it can signiﬁcantly reduce solution time as well as the size
of the branching tree when proving the optimality of a primal bound. We hope
to motivate further research on the quality of the duals used within both ILP
and CP technology.

Maximizing Reduced Cost Fixing
223
For the sake of clarity and without loss of generality, the proposed approaches
will be detailed in the context of integer linear programs (ILPs), i.e., where all
variables are integers, as opposed to mixed-integer linear programming models.
This technique is also applicable in the context of CP, if one can derive a (partial)
linear relaxation of the model [15].
The paper is organized as follows. Section 2 introduces the necessary notation
and the basic concepts of reduced cost ﬁxing and the related consistency notions.
Next, we discuss one alternative to obtain an approximate consistency in Sect. 3.
Finally, we present a preliminary numerical study in Sect. 4 and conclude in
Sect. 5.
2
Reduced Cost Fixing and Relaxed-Consistency
For the purposes of this paper, consider the problem
zP := min{cT x : Ax ≥b, x ≥0}
(P)
with A ∈Rn×m and b, c ∈Rn for some n, m ≥1. We assume that (P) represents
the LP relaxation of an ILP problem PS with an optimal solution value of
z∗≥zP and where variables {xi : i ∈S} are subject to integrality constraints.
The dual of the problem (P) can be written as
zD := max{uT b : uT A ≤cT , u ≥0}
(D)
where u ∈Rm is the vector of dual variables. We assume for exposition that PS,
(P), and (D) are bounded (the results presented here can be easily generalized
when that is not the case).
We have zP = zD (strong duality) and for every optimal solution x∗of
(P), there exists an optimal solution u∗to (D) such that u∗T (b −Ax∗) = 0
(complementary slackness). Moreover, for some j such that x∗
j = 0, the quantity
cj = cj −u∗T Aj
(RC)
is the reduced cost of variable xj and yields the marginal increase in the objective
function if x∗
j moves away from its lower bound. Thus, if a given known feasible
solution with value zUB ≥z∗is available to the original ILP, the reduced cost
ﬁxing technique consists of ﬁxing x∗
j = 0 if
zP + cj ≥zUB,
(RCF)
since any solution with x∗
j > 0 can never improve upon the existing upper
bound zUB. We remark in passing that the condition (RCF) can be generalized
to establish more general bounds on a variable. That is, we can use the reduced
cost cj to deduce values lj and uj such that either x∗
j ≥lj or x∗
j ≤uj in
any optimal solution (see, e.g., [10]). In this paper we restrict our attention to
the classical case described above. We refer to Wolsey [18] and Nemhauser and
Wolsey [13] for the formal proofs of correctness.

224
O.S. Bajgiran et al.
The dual variables u∗for the computation of (RC) can be obtained with
very little computational eﬀort after ﬁnding an optimal solution x∗to (P) (e.g.,
they are computed simultaneously to x∗when using the Simplex method). In
the most of practical known implementations concerning ILP presolving and CP
propagation methods, the reduced cost ﬁxing is typically carried out using the
single u∗computed after solving every LP relaxation [7,12]. Note, however, that
(D) may contain multiple optimal solutions, each potentially yielding a diﬀerent
reduced cost cj that may or may not satisfy condition (RCF).
One therefore does not need to restrict its attention to a unique u∗, and
can potentially improve the number of variables that are ﬁxed if the whole dual
space is considered instead. This would mean that if there exists a reduced cost
cj such that variable xj can be ﬁxed to 0 with respect to zUB, then there exist a
dual vector u such that uT b + (cj −uT Aj) ≥zUB. This was ﬁrst demonstrated
by [16] in the context of CP-based Lagrangian Relaxation, which also pointed
out that often more ﬁltering occurs when u is not an optimal dual vector and
therefore we might have that uT b < zP .
Achieving the notion of Relaxed Consistency, as deﬁned in [6], is thus quite
consuming in such a condition. This form of consistency can be casted, in the
context of ILP, as follows.
Deﬁnition 1. Let PS be an ILP model with linear programming relaxation (P)
and its corresponding dual (D). The model PS is relaxed consistent (or relaxed-
P-consistent) with respect to an upper bound zUB to PS if for any dual feasible
u∗to (D) and its associated reduced cost c vector, condition (RCF) is never
satisﬁed, i.e., zP + cj < zUB for all j = 1, . . . , n.
If a model is relaxed consistent according to Deﬁnition 1, then it is not
possible to ﬁx any variable xj via reduced costs.
Consistent with the theory presented in [6,16], any ILP formulation can be
eﬃciently converted into a relaxed consistent formulation in polynomial time.
Given an ILP model PS and the primal (P) and dual (D) of its associated linear
programming relaxation, the set of optimal dual solution coincides with the
polyhedral set D = {u ∈Rm : uT A ≥c, u ≥0}. Thus, a variable xj can be ﬁxed
to zero if the optimal solution c∗
j of the problem c∗
j = max{cj −uT Aj : u ∈D}
is such that zP + c∗
j ≥zUB. This means that the complexity of the procedure is
dominated by the cost of solving O(n) LP models, each of which can be done in
weakly polynomial time [18].
3
Dual Picking for Maximum Reduced Cost Fixing
Establishing relaxed consistency by solving O(n) LP models is impractical when
a model has any reasonably large number of variables. We thus propose a simple
alternative model that exploits the underlying concept of searching in the dual
space for maximizing ﬁltering. Namely, as opposed to solving an LP for each

Maximizing Reduced Cost Fixing
225
variable, we will search for the dual variables that together maximize the number
of variables that can be ﬁxed. This can be written as the following MILP model:
max
n

i=1
yi
(DP-RCF)
s.t.
uT A ≤c
(1)
uT b = zP
(2)
uT b + (cj −uT Aj) ≥zUB −(1 −yj)M
∀j
(3)
u ≥0
(4)
y ∈{0, 1}n
(5)
In the model (DP-RCF) above, we are searching for the dual variables u, on
the optimal dual face, that maximize the number of variables ﬁxed. Speciﬁcally,
we will deﬁne a binary variable yi in such a way that yi = 1 if and only if we ﬁx
it allows to deduce that xj can be ﬁxed to 0.
To enforce this, let M be a suﬃciently large number, and 1 an n-dimensional
vector containing all ones. Constraints (1), and (4) ensure that u∗is dual feasible.
If yi = 1, then inequality (3) reduces to condition (RCF) and the associated xi
should be ﬁxed to 0. Otherwise, the right-hand side of (3) is arbitrary small (in
particular to account for arbitrarily small negative reduced costs). Constraint (2)
enforces strong duality and the investigation of optimal dual vectors only, it can
be omitted in order to explore the whole dual feasible space (as sub-optimal dual
vectors can perhaps ﬁlter more [16]). Finally, constraint (5) deﬁnes the domain
of the y variable and the objective maximizes the number of variables ﬁxed.
The model (DP-RCF) does not necessarily achieve relaxed consistency as it
yields a single reduced cost vector and it is restricted to the optimal dual face.
However, our experimental results indicate that the model can be solved quite
eﬃciently and yields interesting bounds. Notice also that any feasible solution to
(DP-RCF) corresponds to a valid set of variables to ﬁx, and hence any solutions
found during search can be used to our purposes as well.
4
Preliminary Numerical Study
We present a preliminary numerical study of our technique on a subset of the
MIPLIB 2010 benchmark [1]. All experiments were performed using IBM ILOG
CPLEX 12.6.3 on a single thread of an Intel Core i7 CPU 3.40 GHz with 8.00 GB
RAM.
Our goal for these experiments is twofold: We wish ﬁrst to evaluate the
ﬁltering achieved by the dual picking models (DP-RCF) in comparison to the
full relaxed-consistent model, and next verify what is the impact of ﬁxing these
variables in CPLEX when proving optimality. For the ﬁrst criteria, we considered
the number of ﬁxed variables according to three diﬀerent approaches:

226
O.S. Bajgiran et al.
1. The model (DP-RCF) with a time limit of 10 min, denoted by DP.
2. A modiﬁed version of the model (DP-RCF) without constraint (2), i.e., we
increased our search space by considering any feasible dual solution, also ﬁxing
a time limit of 10 min. We denote this approach by DP-M.
3. Solving the O(n) LP models to achieve relaxed consistency in view of
Deﬁnition 1, with no time limit. This method provides the maximum number
of variables that can be ﬁxed thought RCF and thus an upperbound for both
DP and DP-M, it is denoted by RCC (relaxed reduced-cost consistency).
In all the cases above, we considered the optimal solution value of the instance
as the upper bound zUB for the (RCF) condition, which results in the strongest
possible ﬁltering for a ﬁxed set of duals.
Next, to assess the impact of ﬁxing variables in the ILP solution process, we
ran the default CPLEX for each approach above, speciﬁcally setting the ﬁxed
variables to zero and providing the optimal solution value of each instance to the
solver. We have also ran default CPLEX without any variable ﬁxing and with
the optimal value as an input, in order to evaluate the impact of variable ﬁxing
in proving the optimality of a particular bound.
As a benchmark we considered all “easy” instances of the MIPLIB that could
be solved within our memory or limit of 8 GB and a time limit of 60,000 s. We
have also eliminated all instances where relaxed consistency could not ﬁx any
variable. This resulted in 36 instances.
The results are depicted in Table 1, where Aux stands for the CPU time
required to solve the auxiliary model (DP-RCF) and Vars is the number of
variables which could be ﬁxed. Moreover, Cons and Vars in the Dimension cat-
egory indicate the number of constraints and decision variables of each instance,
respectively, and DC shows the number of variables that default CPLEX can
ﬁx as a result of the ﬁnal dual solution calculated by ourselves. Omitted rows
indicates the auxiliary problem reached its time limit. Due to space restric-
tions, instances neos-16..., neos-47..., neos-93..., neos-13..., neos-84..., rmatr-p5,
rmatr-p10, core253..., neos-93..., and sat... represent instances neos-1601936,
neos-476283, neos-934278, neos-1337307, neos-849702, rmatr100-p5, rmatr100-
p10, core2536-691, neos-934278, and satellites1-25, respectively.
We ﬁrst notice that achieving full relaxed consistency is quite time consuming
and not practical with respect to the default solution time of CPLEX. When
looking at both DP models, it is obvious that restricting the search to the optimal
dual face, rather than the whole dual feasible space, yields practically the same
amount of ﬁltering while being orders of magnitude faster. In fact, in many cases
the (DP-RCF) model can be solved in less than a second.
To determine whether such ﬁltering is worth the extra eﬀort, we compare
the solution time of DP against the default solution time of CPLEX. For each
instance we compute the speedup as the solution time of CPLEX divided by the
sum of both solution and dual picking (i.e., solving (DP-RCF)) time of DP. We
then compute the geometric mean of all speedups, which yields an average speed
up of 20% when using our dual picking methodology over the default CPLEX.

Maximizing Reduced Cost Fixing
227
Table 1. General results
Instance
Dimension
DC
CPLEX Default
DP
DP-M
RCC
Cons Vars
Vars Time Nodes
Time Nodes
Aux Vars
Time Nodes Aux Vars Time Nodes Aux
Vars
MILP instances
30n20b8
578 18380
0 3
260
3
260
0.5
7282
3 90
494
13603
aflow40b
1442
2728
33 187
17461
1961
200477 0.03 499
281 21635 42
532
binkar10 1
1026
2298
165 7
1567
8
2135
0.1
200
5 902
51
281
core253...
2539 15293
2494 38304 239195 2414
8816
600 3046
4464 72010 79173
4818
biella1
1203
7328
219 942
1477
257
1107
52
429
204 1089
18220
1540
gmu-35-40
424
1205
0 68
351765 68
351765 0.02 0
124
≈6e5
9
485
26 94209 19
493
mik-...
151
251
50 1
2205
0.7
1115
0.01 50
0.7
1115
0.5
50
0.7 1115
0.8
50
mzzv11
9499 10240
29 19
57
17
46
26
213
26 329
18679
678
neos13
20852
1827
8 6
485
5
399
1
8
5
399
68
8
6 7
588
179
neos-16...
3131
4446
867 327
4264
137
2259
2
1766
377 7549
7747
1782
neos-47
10015 11915
36 1054
1565
195
1129
2
69
195 1129
38293
69
neos-93...
11495 23123 15843 430
97
74
12
3
15981
132 39
86254 16088
net12
14021 14115
0 162
440
16
50
1
32
150 880
3057
334
ns1208400
4289
2883
2 21
860
21
860
0.08 286
21
860
107
286
32 1465
1286
592
ns1830653
2932
1629
0 208
24606
74
9185
0.2
850
217
31211 952
853
110 5203
65
915
pw-myciel4
8164
1059
0 0.4
17
0.4
117
0.05 0
0.4
117
6
0
1 580
54
160
rmatr-p10
7260
7359
100 20
455
27
345
0.2
100
27 345
3260
100
rmatr-p5
8685
8784
100 3
2
5
6
2
100
5 6
9601
100
rococo...
1293
3117
0 523
13193
4475
54077
0.3
511
531 15979 35
553
roll3000
2295
1166
0 2645
177058 2645
177058 0.08 120
1015 61271 50
149
sat...
5996
9013
99 220
442
25
497
2
2499
11 758
13845
4989
sp98ir
1531
1680
110 244
8607
241
13057
1.5
285
204 18659 86
524
timtab1 171
397
0
2393 ≈1e6 2393
≈1e6 0.02
13
2393
≈1e6 4
13
2393 ≈1e6 1
13
Binary-Only Problems
acc-tight5
3052
1339
1339 97
2262
97
2262
0
1339
97
2262
0
1339
97 2262
0
1339
air04
823
8904
0 7
493
7
46
6
64
5 54
1500
126
bab5
4964 21600
0 8313
49311
2903
12717
369 373
883 7904
18563
2309
eil33-2
32
4516
1004 112
15041
18
6825
6
1035
0.3 305
33
4013
eilB101
100
2818
45 595
26753
318
21666
1
125
84 10309 115
1588
n3div36
4484 22120
4755 57917 ≈2.7e6 57017 ≈2.7e6 1
4755
3518 83900 6568
6372
neos-13...
5687
2840
14 9
300
6
110
5
77
5 30
1098
150
neos18
11402
3312
0 0.4
1
0.4
1
0.08 84
0.4 1
142
84
neos-84...
1041
1737
1737 105
8223
105
8223
0
1737
105
8223
0
1737
105 8223
0
1737
ns1688347
4197
2685
0 8
260
5
1310
0.25 267
1 1
56
716
opm2-z7-s2 31798
2023
0 230
1000
230
1000
1
0
230
1000
35
0
25 260
2484
459
rmine6
7078
1096
0 69
15233
31
5920
0.25 1
31
5920
10
1
52 10497 116
66
sp98ic
825 10894
6902 29041 149629 40306 466555 62
7098
345 35647 4724
8488
5
Conclusion
In this paper, we revisited the notion of reduced-cost based ﬁltering and variable
ﬁxing, which are known to be dependent on the available dual information.
We deﬁned the problem of identifying the set of dual values that maximize
the number of variables which can be ﬁxed as an optimization problem. We
demonstrated that looking for a good set of such dual on the optimal dual face
is considerably faster and ﬁlter almost as many variables as when considering
the full feasible dual space. In many cases ﬁxing more variable lead to a reduced
search tree that can be explored faster. However, in a good number of cases,
solution time increases when more variables are ﬁxed, which is probably due to
the fact that early in the tree the search takes a diﬀerent path.
Future research will consider dual picking during search, so as to try to ﬁx
variables when the relative gap becomes small enough in a subtree, as well as
applying the techniques in the context of constraint programming.

228
O.S. Bajgiran et al.
References
1. MIPLIB2010. http://miplib.zib.de/miplib2010-benchmark.php
2. Balas, E., Martin, C.H.: Pivot and complement-a heuristic for 0–1 programming.
Manage. Sci. 26(1), 86–96 (1980)
3. Bergman, D., Cire, A.A., Hoeve, W.-J.: Improved constraint propagation via
lagrangian decomposition. In: Pesant, G. (ed.) CP 2015. LNCS, vol. 9255, pp.
30–38. Springer, Cham (2015). doi:10.1007/978-3-319-23219-5 3
4. Bixby, E.R., Fenelon, M., Gu, Z., Rothberg, E., Wunderling, R.: MIP: theory
and practice — closing the gap. In: Powell, M.J.D., Scholtes, S. (eds.) CSMO
1999. ITIFIP, vol. 46, pp. 19–49. Springer, Boston, MA (2000). doi:10.1007/
978-0-387-35514-6 2
5. Chv´atal, V.: Linear Programming. Freeman, New York (1983). Reprints: (1999),
(2000), (2002)
6. Fahle, T., Sellmann, M.: Cost-based ﬁltering for the constrained knapsack problem.
Ann. Oper. Res. 115, 73–93 (2002)
7. Focacci, F., Lodi, A., Milano, M.: Cost-based domain ﬁltering. In: Jaﬀar, J. (ed.)
CP 1999. LNCS, vol. 1713, pp. 189–203. Springer, Heidelberg (1999). doi:10.1007/
978-3-540-48085-3 14
8. Focacci, F., Lodi, A., Milano, M., Vigo, D.: Solving TSP through the integration
of OR and CP techniques. Electron. Notes Discrete Math. 1, 13–25 (1999)
9. Focacci, F., Milano, M., Lodi, A.: Solving TSP with time windows with con-
straints. In: Proceedings of the 1999 International Conference on Logic program-
ming, Massachusetts Institute of Technology, pp. 515–529 (1999)
10. Klabjan, D.: A new subadditive approach to integer programming. In: Cook, W.J.,
Schulz, A.S. (eds.) IPCO 2002. LNCS, vol. 2337, pp. 384–400. Springer, Heidelberg
(2002). doi:10.1007/3-540-47867-1 27
11. Lodi, A.: Mixed integer programming computation. In: J¨unger, M., Liebling, T.M.,
Naddef, D., Nemhauser, G.L., Pulleyblank, W.R., Reinelt, G., Rinaldi, G., Wolsey,
L.A. (eds.) 50 Years of Integer Programming 1958–2008, pp. 619–645. Springer,
Heidelberg (2010)
12. Mahajan, A.: Presolving mixed-integer linear programs. In: Wiley Encyclopedia of
Operations Research and Management Science (2010)
13. Nemhauser, G.L., Wolsey, L.A.: Integer Programming and Combinatorial Opti-
mization (1988)
14. Chichester, W., Nemhauser, G.L., Savelsbergh, M.W.P., Sigismondi, G.S.: Con-
straint Classiﬁcation for Mixed Integer Programming Formulations. COAL Bul-
letin, vol. 20, pp. 8–12 (1992)
15. Refalo, P.: Linear formulation of constraint programming models and hybrid
solvers. In: Dechter, R. (ed.) CP 2000. LNCS, vol. 1894, pp. 369–383. Springer,
Heidelberg (2000). doi:10.1007/3-540-45349-0 27
16. Sellmann, M.: Theoretical foundations of CP-based lagrangian relaxation. In: Wal-
lace, M. (ed.) CP 2004. LNCS, vol. 3258, pp. 634–647. Springer, Heidelberg (2004).
doi:10.1007/978-3-540-30201-8 46
17. Thorsteinsson, E.S., Ottosson, G.: Linear relaxations and reduced-cost based prop-
agation of continuous variable subscripts. Ann. Oper. Res. 115(1), 15–29 (2002)
18. Wolsey, L.A.: Integer Programming, vol. 4. Wiley, New York (1998)
19. Yunes, T., Aron, I.D., Hooker, J.N.: An integrated solver for optimization problems.
Oper. Res. 58(2), 342–356 (2010)

Experimental Validation of Volume-Based
Comparison for Double-McCormick Relaxations
Emily Speakman, Han Yu, and Jon Lee(B)
Department of Industrial and Operations Engineering,
University of Michigan, Ann Arbor, MI, USA
{eespeakm,yuha,jonxlee}@umich.edu
Abstract. Volume is a natural geometric measure for comparing poly-
hedral relaxations of non-convex sets. Speakman and Lee gave volume
formulae for comparing relaxations of trilinear monomials, quantifying
the strength of various natural relaxations. Their work was motivated
by the spatial branch-and-bound algorithm for factorable mathematical-
programming formulations. They mathematically analyzed an important
choice that needs to be made whenever three or more terms are multi-
plied in a formulation. We experimentally substantiate the relevance of
their main results to the practice of global optimization, by applying it
to diﬀerent relaxations of diﬃcult box cubic problems (boxcup). In doing
so, we ﬁnd that, using their volume formulae, we can accurately predict
the quality of a relaxation for boxcups based on the (box) parameters
deﬁning the feasible region. Speciﬁcally, we are able to conclude from our
experiments that all of the relevant relaxations are round enough so that
average objective gaps for a pair of boxcup relaxations can be predicted
by appropriately combining the volumes of relaxations of the individual
trilinear monomials.
Keywords: Mccormick inequalities · Mixed-integer non-linear opti-
mization · Global optimization · Trilinear monomials · Spatial branch-
and-bound
1
Introduction
1.1
Measuring Relaxations via Volume in Mathematical
Optimization
In the context of mathematical optimization, there is often a natural tradeoﬀ
in the tightness of a convexiﬁcation (i.e., a convex relaxation) and the diﬃ-
culty of optimizing over it. This idea was emphasized by [11] in the context
of mixed-integer non-linear programming (MINLP) (see also the recent work
[7]). Of course this is also a well-known phenomenon for diﬃcult 0/1 linear-
optimization problems, where very tight relaxations are available via extremely
heavy semideﬁnite-programming relaxations (e.g., the Lasserre hierarchy), and
the most eﬀective relaxation for branch-and-bound/cut may well not be the
c
⃝Springer International Publishing AG 2017
D. Salvagnin and M. Lombardi (Eds.): CPAIOR 2017, LNCS 10335, pp. 229–243, 2017.
DOI: 10.1007/978-3-319-59776-8 19

230
E. Speakman et al.
tightest. Earlier, again in the context of mathematical optimization, [12] intro-
duced the idea of using volume as a measure of the tightness of a convex relax-
ation (for ﬁxed-charge and vertex packing problem). Most of that mathematical
work was asymptotic, seeking to understand the quality of families of relax-
ations with a growing number of variables, but some of it was also substantiated
experimentally in [11].
1.2
Spatial Branch-and-Bound
The workhorse algorithm for global optimization of so-called factorable MINLPs
(see [13]) is spatial branch-and-bound (sBB) (see [1,18,20]). sBB decomposes
model functions against a library of basic functions (e.g., x1x2, x1x2x3, xx2
1 ,
log(x), sin(x), x3, √x, arctan(x)). We assume that each basic function is a func-
tion of no more than 3 variables, and that we have a convex outer-approximation
of the graph of each such basic function on box domains. sBB realizes convex-
iﬁcations of model functions by composing convexiﬁcations of basic functions.
sBB subdivides the domains of variables, and re-convexiﬁes, obtaining stronger
bounds on model functions. Prominent software for sBB includes: ANTIGONE
[16], BARON [19], Couenne [3], SCIP [26], and αBB [1].
1.3
Using Volume to Guide Decompositions for Spatial
Branch-and-Bound
[23] applied the idea of [12], but now in the context of the low-dimensional
relaxations of basic functions that arise in sBB. Speciﬁcally, [23] considered the
basic function f = x1x2x3 on box domains; that is the graph
G :=

(f, x1, x2, x3) ∈R4 : f = x1x2x3, xi ∈[ai, bi], i = 1, 2, 3

,
where 0 ≤ai < bi are given constants. It is important to realize that G is relevant
for any model where any three quantities (which could be complicated functions
themselves) are multiplied. Furthermore, the case of nonzero lower bounds (i.e.,
ai > 0) is particularly relevant, especially when the multiplied quantities are
complicated functions of model variables. We let Ph denote the convex hull of G.
Though in fact polyhedral, the relaxation Ph has a rather complicated inequality
description. Often lighter relaxations are used by modelers and MINLP software.
[23] considered three natural further relaxations of Ph. Thinking of the product
as x1(x2x3) or x2(x1x3) or x3(x1x2), and employing the so-called McCormick
relaxation twice, leads to three diﬀerent double-McCormick relaxations. [23]
derive analytic expressions for the volume of Ph as well as all three of the natural
relaxations. The expressions are formulae in the six constants 0 ≤ai < bi,
i = 1, 2, 3. In doing so, they quantify the quality of the various relaxations and
provide recommendations for which to use.
It is important to be aware that sBB software such as ANTIGONE and
BARON exclusively use the relatively complicated inequality description of

Experimental Validation of Volume-Based Comparison
231
Ph, while others such as Couenne, SCIP and αBB use an arbitrary double-
McCormick relaxation. Much more discussion and mathematical guidance con-
cerning the tradeoﬀs between these and related (extended-variable) relaxations
can be found in [23, Sect. 3].
1.4
Our Contribution
The results of [23] are theoretical. Their utility for guiding modelers and sBB
implementers depends on the belief that volume is a good measure of the quality
of a relaxation. Morally, this belief is based on the idea that with no prior infor-
mation on the form of an objective function, the solution of a relaxation should
be assumed to occur with a uniform density on the feasible region. Our contribu-
tion is to experimentally validate the robustness of this theory in the context of
a particular use case, optimizing multilinear cubics over boxes (‘boxcup’). There
is considerable literature on techniques for optimizing quadratics, much of which
is developed and validated in the context of so-called ‘boxqp’ problems, where
we minimize 
i,j qijxixj over a box domain in Rn. So our boxcup problems, for
which we minimize 
i,j qijkxixjxk over a box domain in Rn, are natural and
deﬁned in the same spirit and for the same purpose as the boxqp problems.
A main result of [23] is an ordering of the three natural relaxations of individ-
ual trilinear monomials by volume. But their formulae are for n = 3. Our exper-
iments validate their theory as applied to our use case. We demonstrate that
in the setting of ‘boxcup’ problems, the average objective discrepancy between
relaxations very closely follows the prediction of the theory of [23], when volumes
are appropriately combined (summing the 4-th root of the volume, across the
chosen relaxations of each trilinear monomials). Moreover and very importantly,
we are able to demonstrate that these results are robust against sparsity of the
cubic forms.
[12] deﬁned the idealized radius of a polytope in Rd as essentially the d-th
root of its volume (up to some constants depending on d). For a polytope that
is very much like a ball in shape, we can expect that this quantity is (propor-
tional to) the “average width” of the polytope. The average width arises by
looking at ‘max minus min’, averaged over all normalized linear objectives. So,
the implicit prediction of [23] is that the idealized radius should (linearly) predict
the expected ‘max minus min’ for normalized linear objectives. We have vali-
dated this experimentally, and looked further into the idealized radial distance
between pairs of relaxations, ﬁnding an even higher degree of linear association.
Finally, in the important case a1 = a2 = 0, b1 = b2 = 1, [23] found that
the two worst relaxations have the same volume, and the greatest diﬀerence in
volume between Ph and the (two) worst relaxations occurs when a3 = b3/3.
We present results of experiments that clearly show that these predictions via
volume are again borne out on ‘boxcup’ problems.
All in all, we present convincing experimental evidence that volume is a
good predictor for quality of relaxation in the context of sBB. Our results
strongly suggest that the theoretical results of [23] are important in devising

232
E. Speakman et al.
decompositions of complex functions in the context of factorable formulations
and therefore our results help inform both modelers and implementers of sBB.
1.5
Literature Review
Computing the volume of a polytope is well known to be strongly #P-hard (see
[4]). But in ﬁxed dimension, or, in celebrated work, by seeking an approximation
via a randomized algorithm (see [9]), positive results are available. Our work
though is motivated not by algorithms for volume calculation, but rather in
certain situations where analytic formulae are available.
Besides [12,23] (and the follow-on [22]), there have been a few papers on
analytic formulae for volumes of polytopes that naturally arise in mathematical
optimization; see [2,5,10,24,25]. But none of these works has attempted to apply
their ideas to the low-dimensional polytopes that naturally arise in sBB, or even
to apply their ideas to compare relaxations. One notable exception is [6], which
is a mostly-computational precursor to [23], focusing on quadrilinear functions
(i.e., f = x1x2x3x4). [6] identiﬁes that the quality of a repeated-McCormick
relaxation depends quite a lot on the order of grouping, but there was no ﬁrm
theoretical result grounding the choice of repeated-McCormick relaxation. In
contrast, we have ﬁrm theoretical grounding, via [23], and here we go a step
further to see that the theory of [23] can be used to rather accurately predict
the quality (as measured by objective gap) of an aggregate relaxation built from
the diﬀerent relaxations of individual trilinear monomials.
There are many implementations of sBB. E.g., BARON [19], Couenne [3],
SCIP [26], ANTIGONE [16], and αBB [1]. Both BARON and ANTIGONE use
the complete linear-inequality description of Ph, while Couenne, SCIP and αBB
use an arbitrary double-McCormick relaxation. Our results indicate that there
are situations where the choice of BARON and ANTIGONE may be too heavy,
and certainly even restricting to double-McCormick relaxations, Couenne, SCIP
and αBB do not systematically choose the best one.
There is a large literature on convexiﬁcation of multilinear functions. Most
relevant to our work are: the polyhedral nature of the convexiﬁcation of the
graphs of multilinear functions on box domains (see [17]); the McCormick
inequalities describing giving the complete linear-inequality description for bilin-
ear functions on a box domain (see [13]); the complete linear-inequality descrip-
tion of Ph (see [14,15]).
2
Preliminaries
2.1
From Volumes to Gaps
For a convex body C ⊂Rd, we denote its volume (i.e., Lebesgue measure) by
vol(C). Volume seems like an awkward measure to compare relaxations, when
typically we are interested in objective-function gaps. Following [12], the idealized
radius of a convex body C ⊂Rd is
ρ(C) := (vol(C)/vol(Bd))1/d ,

Experimental Validation of Volume-Based Comparison
233
where Bd is the (Euclidean) unit ball in Rd. ρ(C) is simply the radius of a
ball having the same volume as C. The idealized radial distance between convex
bodies C1 and C2 is simply |ρ(C1) −ρ(C2)|. If C1 and C2 are concentric balls,
say with C1 ⊂C2, then the idealized radial distance between them is the (radial)
height of C2 above C1. The mean semi-width of C is simply
1
2

∥c∥=1

max
x∈C c′x −min
x∈C c′x

dψ,
where ψ is the (d −1)-dimensional Lebesgue measure on the boundary of Bd,
normalized so that ψ on the entire boundary is unity. If C is itself a ball, then
(i) its idealized radius is in fact its radius, and (ii) its width in any unit-norm
direction c is constant, and so (iii) its (idealized) radius is equal to its mean
semi-width.
Key point: What we can hope is that our relaxations of individual trilinear
monomials (in a model with many overlapping trilinear monomials) are round
enough so that choosing them to be of small volume (which is proportional to
and monotone increasing in its idealized radius raised to the power d) is a good
proxy for choosing the overall relaxation by mean width (which is the same as
mean objective-value range). It is this that we investigate experimentally.
2.2
Double-McCormick Convexiﬁcations of Trilinear Momomials
Without loss of generality (see [23]), we can relabel the three variables so that
a1b2b3 + b1a2a3 ≤b1a2b3 + a1b2a3 ≤b1b2a3 + a1a2b3.
(Ω)
is satisﬁed by the variable bounds. Given f := x1x2x3 satisfying (Ω), there are
three choices of double-McCormick convexiﬁcations depending on the bilinear
sub-monomial we convexify ﬁrst. We could ﬁrst group x1 and x2 and convexify
w = x1x2; after this, we are left with the monomial f = wx3 which we again
convexify using McCormick. Alternatively, we could ﬁrst group variables x1 and
x3, or variables x2 and x3.
To see how to perform these convexiﬁcations in general, we exhibit the
double-McCormick convexiﬁcation that ﬁrst groups the variables xi and xj.
Therefore we have f = xixjxk, and we let wij = xixj, so f = wijxk. Convexify-
ing wij = xixj using the standard McCormick inequalities, and then convexifying
f = wijxk, again using the standard McCormick inequalities, we obtain the 8
inequalities:
wij −ajxi −aixj + aiaj ≥0,
f −akwij −aiajxk + aiajak ≥0,
−wij + bjxi + aixj −aibj ≥0,
−f + bkwij + aiajxk −aiajbk ≥0,
−wij + ajxi + bixj −biaj ≥0,
−f + akwij + bibjxk −bibjak ≥0,
wij −bjxi −bixj + bibj ≥0,
f −bkwij −bibjxk + bibjbk ≥0.

234
E. Speakman et al.
Using Fourier-Motzkin elimination (i.e., projection), we then eliminate the
variable wij to obtain the following system in our original variables f, xi, xj, xk.
xi −ai
≥0,
(1)
xj −aj
≥0,
(2)
f −ajakxi −aiakxj −aiajxk + 2aiajak
≥0,
(3)
f −ajbkxi −aibkxj −bibjxk + aiajbk + bibjbk
≥0,
(4)
−xj + bj
≥0,
(5)
−xi + bi
≥0,
(6)
f −bjakxi −biakxj −aiajxk + aiajak + bibjak
≥0,
(7)
f −bjbkxi −bibkxj −bibjxk + 2bibjbk
≥0,
(8)
−f + bjbkxi + aibkxj + aiajxk −aiajbk −aibjbk
≥0,
(9)
−f + ajbkxi + bibkxj + aiajxk −aiajbk −biajbk
≥0,
(10)
−xk + bk
≥0,
(11)
−f + bjakxi + aiakxj + bibjxk −aibjak −bibjak
≥0,
(12)
−f + ajakxi + biakxj + bibjxk −biajak −bibjak
≥0,
(13)
xk −ak
≥0,
(14)
f −aiajxk
≥0,
(15)
−f + bibjxk
≥0.
(16)
It is easy to see that the inequalities (15) and (16) are redundant: (15) is
ajak (1) + aiak (2) + (3), and (16) is bjak (6) + aiak (5) + (12).
We use the following notation in what follows. For ℓ= 1, 2, 3, system ℓis
deﬁned to be the system of inequalities obtained by ﬁrst grouping the pair of
variables xj and xk, with j and k diﬀerent from ℓ. For ℓ= 1, 2, 3, the polytope
Pℓis deﬁned to be the solution set of system ℓ, while
Ph := conv ({(f, x1, x2, x3) : f = x1x2x3, xi ∈[ai, bi], i = 1, 2, 3}) .
2.3
Volumes of Convexiﬁcations
The main results of [23] are as follows.
Theorem 1 (Volume of the convex hull; see [23])
vol(Ph) = (b1 −a1)(b2 −a2)(b3 −a3) ×
(b1(5b2b3 −a2b3 −b2a3 −3a2a3) + a1(5a2a3 −b2a3 −a2b3 −3b2b3))/24.
Theorem 2 (Volume of the double McCormick with x1(x2x3); see [23])
vol(P1) = vol(Ph) + (b1 −a1)(b2 −a2)2(b3 −a3)2 ×
3(b1b2a3 −a1b2a3 + b1a2b3 −a1a2b3) + 2(a1b2b3 −b1a2a3)
24(b2b3 −a2a3)
.

Experimental Validation of Volume-Based Comparison
235
Theorem 3 (Volume of the double McCormick with x2(x1x3); see [23])
vol(P2) = vol(Ph) + (b1 −a1)(b2 −a2)2(b3 −a3)2 
5(a1b1b3 −a1b1a3) + 3(b2
1a3 −a2
1b3)

24(b1b3 −a1a3)
.
Theorem 4 (Volume of the double McCormick with x3(x1x2); see [23])
vol(P3) = vol(Ph) + (b1 −a1)(b2 −a2)2(b3 −a3)2 
5(a1b1b2 −a1b1a2) + 3(b2
1a2 −a2
1b2)

24(b1b2 −a1a2)
.
Corollary 1 (Ranking the relaxations by volume; see [23])
vol(Ph) ≤vol(P3) ≤vol(P2) ≤vol(P1).
Corollary 2 (A worst case; see [23]). For the special case of a1 = a2 = 0,
b1 = b2 = 1, and ﬁxed b3, the greatest diﬀerence in volume for P3(= Ph) and P2
(or P1) occurs when a3 = b3/3.
3
Computational Experiments
3.1
Box Cubic Programs and 4 Relaxations
Our experiments are aimed at the following natural problem which is concerned
with optimizing a linear function on trinomials. Let H be a 3-uniform hypergraph
on n vertices. Each hyperedge of H is a set of 3 vertices, and we denote the set
of hyperedges by E(H). If H is complete, then |E(H)| =
n
3
	
. We associate
with each vertex i a variable xi ∈[ai, bi], and with each hyperedge (i, j, k) the
trinomial xixjxk and a coeﬃcient qijk (1 ≤i < j < k ≤n). We now formulate
the associated boxcup (‘box cubic problem’):
min
x∈Rn
⎧
⎨
⎩

(i,j,k)∈E(H)
qijk xi xj xk : xi ∈[ai, bi], i = 1, 2, . . . , n
⎫
⎬
⎭.
(BCUP)
The name is in analogy with the well-known boxqp, where just two terms (rather
than three) are multiplied (‘box’ refers to the feasible region and ‘qp’ refers to
‘quadratic program’).
(BCUP) is a diﬃcult nonconvex global-optimization problem. Our goal here
is not to solve instances of this problem, but rather to solve a number of diﬀerent
relaxations of the problem and see how the results of these experiments correlate
with the volume results of [23]. In this way, we seek to determine if the guidance
of [23] is relevant to modelers and those implementing sBB.
We have seen how for a single trilinear term f = xixjxk, we can build four
distinct relaxations: the convex hull of the feasible points, Ph, and three relax-
ations arising from double McCormick: P1, P2 and P3. To obtain a relaxation of
(BCUP), we choose a relaxation Pℓ, for some ℓ= 1, 2, 3, h and apply this same

236
E. Speakman et al.
relaxation method to each trinomial of (BCUP). We therefore obtain 4 distinct
linear relaxations of the form:
min
(x,f)∈Pℓ
⎧
⎨
⎩

(i,j,k)∈E(H)
qijk fijk
⎫
⎬
⎭.
where Pℓ, ℓ= 1, 2, 3, h is the polytope in dimension |E(H)| + n arising from
using relaxation Pℓon each trinomial. This linear relaxation is a linear inequality
system involving the n variables xi (i = 1, 2, . . . , n), and the |E(H)| new ‘function
variables’ fijk. Each such ‘function variable’ models a product xi xj xk.
For our experiments, we randomly generate box bounds [ai, bi] on xi, for each
i = 1, . . . , n independently, by choosing (uniformly) random pairs of integers
0 ≤ai < bi ≤10. With each realization of these bounds, we get relaxation
feasible regions Pℓ, for ℓ= 1, 2, 3, h.
3.2
Three Scenarios for the Hypergraph H
We designed our experiments with the idea of gaining some understanding of
whether our conclusions would depend on how much the monomials overlap.
So we looked at three scenarios for the hypergraph H of (BCUP), all with
|E(H)| = 20 monomials:
– Our dense scenario has H being a complete 3-uniform hypergraph on n = 6
vertices (
6
3
	
= 20). We note that each of the n = 6 variables appears in
6−1
3−1
	
= 10 of the 20 monomials, so there is considerable overlap in variables
between trinomials.
– Our
sparse
scenario
has
hyperedges:
{1, 2, 3},
{2, 3, 4},
{3, 4, 5}
. . .
{18, 19, 20}, {19, 20, 1}, {20, 1, 2}. Here we have n = 20 variables and each
variable is in only 3 of the trinomials.
– Our very sparse scenario has n = 30 variables and each variable is in only
2 of the trinomials.
For each scenario, we generate 30 sets of bounds [ai, bi] on xi (i = 1, . . . , n). To
control the variation in our results, and considering that the scaling of
Q := {qijk : {i, j, k} ∈E(H)}
is arbitrary, we generate 100,000 random Q with |E(H)| entries, uniformly dis-
tributed on the unit sphere in R|E(H)|. Then, for each Q, we both minimize and
maximize 
i<j<k qijk fijk over each Pℓ, ℓ= 1, 2, 3, h and each set of bounds.
3.3
Quality of Relaxations
For each Q we take the diﬀerence in the optimal values, i.e. the maximum value
minus the minimum value; this can be thought of as the width of the polytope in

Experimental Validation of Volume-Based Comparison
237
the direction Q. We then average these widths for each Pℓ, ℓ= 1, 2, 3, h, across
the 100,000 realizations of Q (which results in very small standard errors), and
we refer to this quantity ω(Pℓ) as the quasi mean width of the relaxation. It is
not quite the geometric mean width, because we do not have objective terms for
all variables in (BCUP) (i.e., we have no objective terms n
i=1 cixi).
We seek to investigate how well the volume formulae of [23], comparing the
volumes of the polytopes Pℓ(ℓ= 1, 2, 3, h), can be used to predict the quality
of the relaxations Pℓ(ℓ= 1, 2, 3, h) as measured by their quasi mean width.
Figure 1 consists of a plot for each scenario: dense, sparse, and very sparse.
Each plot illustrates the diﬀerence in quasi mean width between P3 (using
the ‘best’ double McCormick) and each of the other relaxations. Each point
represents a choice of bounds and the instances are sorted by ω(P1) −ω(Ph).
In all three plots ω(Ph)−ω(P3) is non-positive, which is to be expected because
Ph is contained in each of the three double-McCormick relaxations. Furthermore
the plots illustrate that the general trend is for ω(P2) −ω(P3) and ω(P1) −
ω(P3) to be positive and also for ω(P1) −ω(P3) to be greater than ω(P2) −
ω(P3). This agrees with Corollary 1 and gives strong validation for the use of
volume to measure the strength of diﬀerent relaxations. It conﬁrms that given a
choice of the double-McCormick relaxations, P3 is the one to choose.
However, there are a few exceptions to the general trend and these exceptions
are most apparent in the very sparse case. In both the dense case and the sparse
case we only see a deviation from the trend on a small number of occasions when
P2 is very slightly better than P3. In each of these cases, the diﬀerence seems
to be so small that we can really regard P3 and P2 as being equivalent from a
practical viewpoint. In the very sparse case, the general trend is still followed,
but we see a few more cases where P2 is slightly better than P3. We also see
that in a few instances, P1 is better than P2 and occasionally even slightly
better than P3.
However it is important to note that when we consider the sparse and very
sparse cases, the diﬀerences in quasi mean width between any two of the relax-
ations is much smaller than these diﬀerences in the dense case. If we were to take
the sparsity of H to the extreme and run our experiments with n = 60 and each
variable only in one trinomial, the diﬀerence in quasi mean width between any
two of the polytopes will become zero for these boxcup problems. Therefore, it
is not surprising that our results diverge from the general trend as H becomes
sparser.
Using the common technique of ‘performance proﬁles’ (see [8]), we can illus-
trate the diﬀerences in quasi mean width of the three double-McCormick relax-
ations in another way. We obtained the matlab code “perf.m” which was adapted
to create these plots from the link contained in [21]. Figure 2 shows a performance
proﬁle for each of the dense, sparse and very sparse scenarios. For each choice of
bounds, Ph gives the least quasi mean width (because it is contained in each of
the other relaxations). Our performance proﬁles display the fraction of instances
where the quasi mean width of Pℓis within a factor α of the mean width of
Ph, for ℓ= 1, 2, 3. The plots are natural log plots where the horizonal axis is

238
E. Speakman et al.
(a) dense case
(b) sparse case
(c) very-sparse case
Fig. 1. Quasi-mean-width diﬀerences
τ := ln(α). Using this measure, we see that the trend in all cases is that P3
dominates P2 which in turn dominates P1. In the very sparse case, we see that
P3 and P2 are very close for small factors α. In general, all three relaxations are
within a small factor of the hull. Displaying the results in this manner gives us a
way to see quickly which relaxation performs best for the majority of instances.
Again, we see agreement with the prediction of Corollary 1 and conﬁrmation
that P3 is the best double-McCormick relaxation.

Experimental Validation of Volume-Based Comparison
239
(a) dense case
(b) sparse case
(c) very-sparse case
Fig. 2. Quasi-mean-width performance proﬁles. Displays the fraction of instances
where the quasi mean width of Pℓis within a factor α = eτ of the mean width of
Ph. Note that for small τ, eτ ≈1 + τ.

240
E. Speakman et al.
3.4
Validating the Relationship Between Volume and Mean Width
Using the [23] formulae, we calculate the volume of the relaxation for each indi-
vidual trinomial, Pℓ. We then take the fourth root of these volumes and sum
over all |E(H)| trinomials to obtain a kind of ‘aggregated idealized radius’ for
each relaxation and each set of bounds. Restricting our attention to the dense
scenario, in Fig. 3, we compare these aggregated idealized radii with quasi mean
width, across all relaxations Pℓ, ℓ= 1, 2, 3, h and each set of bounds (each point
in each scatter plot corresponds to a choice of bounds). We see a high R2 coeﬃ-
cient in all cases, so we may conclude that volume really is a good predictor of
relaxation width.
We also compute the diﬀerence in width between polytope pairs: Ph and
P3, P3 and P2, P2 and P1 for each direction Q. We then average these
width diﬀerences for each polytope and each set of bounds, across the 100,000
realizations of Q. We refer to this result as the quasi mean width diﬀerence
of the pair of polytopes. In Fig. 4, we similarly compare aggregated idealized
radial diﬀerences with quasi mean width diﬀerences. We see even higher R2
coeﬃcients, validating volume as an excellent predictor of average objective gap
between pairs of relaxations.
Fig. 3. Idealized radius predicting quasi mean width

Experimental Validation of Volume-Based Comparison
241
Fig. 4. Idealized radial distance predicting quasi mean width diﬀerence
3.5
A Worst Case
Our ﬁnal set of experiments relate to a ‘worst case’ as described in [23]. In the
important special case of a1 = a2 = 0 and b1 = b2 = 1, the two ‘bad’ double-
McCormick relaxations have the same volume and the ‘good’ double McCormick
is exactly the hull. In addition, the greatest diﬀerence in volume between the
hull and the bad relaxations occurs when a3 = b3/3.
We compute the same results as we have discussed before (i.e. the diﬀerences
in quasi mean width between the relaxations) with n = 6, but now instead of
using random bounds, we ﬁx a1 = a2 = 0 and b1 = b2 = 1. We also ﬁx b3 and
run the experiments for a3 = 1, 2, . . . , b3 −1. Here, we only consider the
5
3
	
= 10
trinomials that have the form xjxkx6.
Figure 5 displays a plot of these results for b3 = 30, 60, 90, 120 and 150.
From the inequality systems we know that Ph is exactly P3, therefore we are
interested in the comparison between: P2 and P3, and P2 and P1. From
the plots of these diﬀerences, we see exactly what we would expect given the
volume formulae. The diﬀerence in mean width between P2 and P1 is very
small; from a practical standpoint it is essentially zero. The diﬀerence in mean
width between P2 and P3 is always positive, indicating again that P3 is the
best choice of double-McCormick relaxation. In addition, we observe that the
maximum diﬀerence falls close to a3 = b3/3 in all cases, demonstrating again
that volume is a good predictor of how well a relaxation behaves.

242
E. Speakman et al.
Fig. 5. Worst-case analysis for a3 (a1 = a2 = 0, b1 = b2 = 1)
Acknowledgments. Supported in part by ONR grant N00014-14-1-0315.
References
1. Adjiman, C., Dallwig, S., Floudas, C., Neumaier, A.: A global optimization method,
αBB, for general twice-diﬀerentiable constrained NLPs: I. Theoretical advances.
Comput. Chem. Eng. 22(9), 1137–1158 (1998)
2. Ardila, F., Benedetti, C., Doker, J.: Matroid polytopes and their volumes. Discrete
Comput. Geom. 43(4), 841–854 (2010)
3. Belotti, P., Lee, J., Liberti, L., Margot, F., W¨achter, A.: Branching and bounds
tightening techniques for non-convex MINLP. Optim. Methods Softw. 24(4–5),
597–634 (2009)
4. Brightwell, G., Winkler, P.: Counting linear extensions is #P-complete. In: Pro-
ceedings of the Twenty-third Annual ACM Symposium on Theory of Computing,
STOC 1991, pp. 175–181. ACM, New York (1991)
5. Burggraf, K., De Loera, J., Omar, M.: On volumes of permutation polytopes. In:
Bezdek, K., Deza, A., Ye, Y. (eds.) Discrete Geometry and Optimization, vol. 69,
pp. 55–77. Springer, Heidelberg (2013)

Experimental Validation of Volume-Based Comparison
243
6. Caﬁeri, S., Lee, J., Liberti, L.: On convex relaxations of quadrilinear terms. J.
Global Optim. 47, 661–685 (2010)
7. Dey, S.S., Molinaro, M., Wang, Q.: Approximating polyhedra with sparse inequal-
ities. Math. Program. 154(1), 329–352 (2015)
8. Dolan, D.E., Mor´e, J.J.: Benchmarking optimization software with performance
proﬁles. Math. Program. 91(2), 201–213 (2002)
9. Dyer, M., Frieze, A., Kannan, R.: A random polynomial-time algorithm for approx-
imating the volume of convex bodies. J. ACM 38(1), 1–17 (1991)
10. Ko, C.W., Lee, J., Steingr´ımsson, E.: The volume of relaxed Boolean-quadric and
cut polytopes. Discrete Math. 163(1–3), 293–298 (1997)
11. Lee, J.: Mixed integer nonlinear programming: some modeling and solution issues.
IBM J. Res. Dev. 51(3/4), 489–497 (2007)
12. Lee, J., Morris, W.: Geometric comparison of combinatorial polytopes. Discrete
Appl. Math. 55, 163–182 (1994)
13. McCormick, G.: Computability of global solutions to factorable nonconvex pro-
grams: Part I. Convex underestimating problems. Math. Program. 10, 147–175
(1976)
14. Meyer, C., Floudas, C.: Trilinear monomials with mixed sign domains: facets of
the convex and concave envelopes. J. Global Optim. 29, 125–155 (2004)
15. Meyer, C., Floudas, C.: Trilinear monomials with positive or negative domains:
facets of the convex and concave envelopes. In: Floudas C.A., Pardalos P. (eds)
Frontiers in Global Optimization. NOIA, vol. 74, pp. 327–352. Springer, Boston
(2004)
16. Misener, R., Floudas, C.A.: ANTIGONE: Algorithms for coNTinuous/Integer
Global Optimization of Nonlinear Equations. J. Global Optim. 59(2), 503–526
(2014). doi:10.1007/s10898-014-0166-2
17. Rikun, A.: A convex envelope formula for multilinear functions. J. Global Optim.
10, 425–437 (1997)
18. Ryoo, H., Sahinidis, N.: A branch-and-reduce approach to global optimization. J.
Global Optim. 8(2), 107–138 (1996)
19. Sahinidis, N.: BARON 15.6.5: Global Optimization of Mixed-Integer Nonlinear
Programs, User’s Manual (2015)
20. Smith, E., Pantelides, C.: A symbolic reformulation/spatial branch-and-bound
algorithm for the global optimisation of nonconvex MINLPs. Comput. Chem. Eng.
23, 457–478 (1999)
21. Soﬁ, A., Mamat, M., Mohid, S., Ibrahim, M., Khalid, N.: Performance proﬁle com-
parison using matlab. In: Proceedings of International Conference on Information
Technology & Society 2015 (2015)
22. Speakman, E., Lee, J.: On sBB branching for trilinear monomials. In: Rocha, A.,
Costa, M., Fernandes, E. (eds.) GOW 2016, pp. 81–84 (2016). ISBN: 978-989-20-
6764-3
23. Speakman, E., Lee, J.: Quantifying double McCormick. To appear in: Math. Oper.
Res. (2017)
24. Stanley, R.: Two poset polytopes. Discrete Comput. Geom. 1(1), 9–23 (1986)
25. Steingr´ımsson, E.: A decomposition of 2-weak vertex-packing polytopes. Discrete
Comput. Geom. 12(4), 465–479 (1994)
26. Vigerske, S., Gleixner, A.: Scip: Global optimization of mixed-integer nonlinear
programs in a branch-and-cut framework. Technical report 16-24, ZIB, Takustr. 7,
14195, Berlin (2016)

Minimum Makespan Vehicle Routing Problem
with Compatibility Constraints
Miao Yu
, Viswanath Nagarajan
, and Siqian Shen(B)
University of Michigan, Ann Arbor, MI, USA
{miaoyu,viswa,siqian}@umich.edu
Abstract. We study a multiple vehicle routing problem, in which a
ﬂeet of vehicles is available to serve diﬀerent types of services demanded
at locations. The goal is to minimize the makespan, i.e. the maximum
length of any vehicle route. We formulate it as a mixed-integer linear pro-
gram and propose a branch-cut-and-price algorithm. We also develop an
eﬃcient O(log n)-approximation algorithm for this problem. We conduct
numerical studies on Solomon’s instances with various demand distribu-
tions, network topologies, and ﬂeet sizes. Results show that the approx-
imation algorithm solves all the instances very eﬃciently and produces
solutions with good practical bounds.
Keywords: Vehicle routing · Compatibility constraints · Branch-cut-
and-price · Approximation algorithm
1
Introduction
Vehicle routing problems (VRPs), with a goal of ﬁnding the optimal routing
assignment for a ﬂeet of vehicles to serve demands at various locations, are
classical and well studied combinatorial optimization problems. Starting from
Dantzig and Ramser [7], many variants of VRPs have been considered, including
VRP with time windows, Capacitated VRP, VRP with heterogeneous ﬂeet, VRP
with multiple depots, as well as hybrid versions of these variants, all of which
are discussed in detail by Golden et al. [15] and Toth and Vigo [24].
In this paper, we consider a minimum makespan VRP with compatibility
constraints (VRPCC). We assume that multiple types of services are demanded
at various locations of a given network and each type of service can only be
served by certain vehicles. The goal of the problem is to minimize the maxi-
mum traveling time of all the routes designed for fulﬁlling the demand, i.e., the
makespan for ﬁnishing all services. The motivation of studying the minimization
of makespan comes from applications that focus on balancing routing assign-
ment for all vehicles and minimizing the time to ﬁnish serving all customers. We
consider a salient application as deploying shared vehicles for serving patient
medical home care service demand, distributed in a geographical network, in
which we aim to balance workload of diﬀerent medical staﬀteams dispatched
c
⃝Springer International Publishing AG 2017
D. Salvagnin and M. Lombardi (Eds.): CPAIOR 2017, LNCS 10335, pp. 244–253, 2017.
DOI: 10.1007/978-3-319-59776-8 20

Minimum Makespan Vehicle Routing Problem
245
together with the vehicles. The solution methods investigated in this paper can
help scheduling and routing for medical home care delivery.
We review the main VRP literature focusing on diﬀerent solution meth-
ods. To exactly optimize VRPs, branch-and-cut was the dominant approach
before 2000s, and the related research (see, e.g., [1,3,19,20,22]) develops eﬀective
valid inequalities to improve solution eﬃciency. Following Fukasawa et al. [13],
branch-cut-and-price (BCP) became the best performing exact solution method
for (capacitated) VRP: this combines branch-and-cut with column generation.
There have also been many approximation algorithms for VRPs with a minimum
makespan objective (see, e.g., [4,8,12]) that provide polynomial time algorithms
with a provable performance guarantee. To the best of our knowledge, no prior
work has provided either exact solution methods or eﬃcient approximation algo-
rithms for VRPCC.
In this paper, we develop the following algorithms for VRPCC: (i) an exact
algorithm based on the BCP approach and (ii) an O(log n)-approximation algo-
rithm based on a budgeted covering approach. We provide preliminary numerical
experiments for both algorithms. Our results show that the approximation algo-
rithm solves the problem eﬃciently and yields a practical approximation ratio
of at most two (on the test instances).
The remainder of the paper is organized as follow. In Sect. 2, we formally
deﬁne the problem and present a mixed-integer linear programming formula-
tion. In Sects. 3 and 4, we propose a BCP algorithm and an approximation algo-
rithm for VRPCC, respectively. In Sect. 5, we test our algorithms on Solomon’s
instances with diverse graph sizes, vehicle ﬂeet sizes, and demand distributions.
We present the numerical results for our proposed solution methods. In Sect. 6,
we conclude our ﬁndings and state future research.
2
Problem Statement and Formulation
Consider an undirected graph G = (V, E), where V = {0, 1, . . . , n} is the set of
n+1 nodes. Node 0 represents the depot and set V + = {1, 2, . . . , n} corresponds
to customer locations. Each edge (i, j) ∈E, ∀i, j ∈V has an associated distance
dij satisfying the triangle inequality such that dij ≤dik + dkj, ∀i, k, j ∈V .
A ﬂeet K of vehicles, with K = {1, 2, . . . , m}, initially located at the depot
can serve demand from customers. Each vehicle k ∈K can only visit a subset
Vk ⊂V +, based on matches of vehicles and service types. In our problem, a
routing decision assigns each vehicle a route such that: (a) the nodes visited by
vehicle k ∈K are in the set Vk; (b) each node must be visited exactly once; and
(c) the maximum distance of all assigned routes is minimized.
We deﬁne the vector x = (xk
ij, (i, j) ∈E, k ∈K)T where xk
ij takes value 1 if
edge (i, j) ∈E is used in the route for vehicle k, and 0 otherwise. Consider the
binary parameter u = (uk
i , k ∈K, i ∈V +)T where uk
i takes value 1 if i ∈Vk
for vehicle k ∈K, and 0 otherwise. Let τ represent the maximum distance of all
the routes. We formulate a mixed-integer program for the VRPCC as follows.

246
M. Yu et al.
(MIP)
minimize
x,τ
τ
(1)
subject to

(i,j)∈E
dijxk
ij ≤τ
∀k ∈K,
(2)

j:(v,j)∈E
xk
vj −

i:(i,v)∈E
xk
iv = 0
∀v ∈V, ∀k ∈K,
(3)

k∈K

i:(i,j)∈E
xk
ij = 1
∀j ∈V +,
(4)

i:(i,j)∈E
xk
ij ≤uk
j
∀j ∈V +, ∀k ∈K,
(5)

(i,j)∈E,i,j∈S
xk
ij ≤|S| −1
∀S ⊂V +, ∀k ∈K,
(6)
xk
ij ∈{0, 1}
∀(i, j) ∈E, ∀k ∈K, (7)
where constraints (2) ensure that τ equals to the maximum distance of all the
routes, which is minimized in the objective function (1); constraints (3) enforce
ﬂow balance at each node for routing each vehicle; constraints (4) ensure that
each node is visited exactly once; constraints (5) ensure that vehicle k can only
visit nodes in Vk; constraints (6) are sub-tour elimination constraints.
3
Branch-Cut-and-Price Algorithm
We describe a BCP approach for VRPCC based on a set partition formulation
where each decision variable represents a feasible route [18]. Let P k be the set
containing all feasible routes for vehicle k ∈K. We deﬁne the binary decision
variable λ = (λp, p ∈P k, k ∈K)T where λp takes value 1 if route p ∈P k is
used by vehicle k and 0 otherwise. Denote the binary parameter a = (aip, i ∈
V, p ∈P k, k ∈K)T where aip takes value 1 if p ∈P k visits node i ∈V , and 0
otherwise. We consider cp as the cost of route p ∈P k, k ∈K. The set partition
formulation is given by
(SP)
minimize
λ,τ
τ
(8)
subject to

k∈K

p∈P k
ajpλp = 1
∀j ∈V +,
(9)

p∈P k
cpλp ≤τ
∀k ∈K,
(10)
λp ∈{0, 1}
∀p ∈P k, ∀k ∈K,
(11)
where constraints (9) enforce that each node is visited exactly once and con-
straints (10) enforce that τ is the maximum cost of all routes. Due to the expo-
nential size of P k, k ∈K, we exploit the BCP method to solve SP.

Minimum Makespan Vehicle Routing Problem
247
3.1
Column Generation
The idea of column generation is to maintain a subset ¯P k ⊂P k for each k ∈K.
We solve SP with P k replaced by ¯P k and detect whether any p ∈P k\ ¯P K can
improve the solution: if yes, we add those favorable routes p to ¯P k and repeat the
process; otherwise we claim optimality. We deﬁne a restricted master problem
(RMP) as linear relaxation of SP with P k replaced by ¯P k for all k ∈K.
Clearly, any feasible primal solution to RMP is feasible to the linear relax-
ation of SP, but this is not necessarily true for their dual solutions. For each
vehicle k ∈K, we want to ﬁnd if there exists some favorable route p ∈P k\ ¯P k
that can improve the objective value of RMP. We deﬁne α, β as the dual variables
corresponding to constraints (9) and (10), respectively, and deﬁne the decision
variable y = (yijp, (i, j) ∈E, p ∈P k, k ∈K)T where yijp takes value 1 if
(i, j) ∈p and 0 otherwise. For each vehicle k ∈K, with ˆα, ˆβ being current
optimal dual solution, the problem of pricing out a new route p (if exists) is
(PP)
zk
PP(ˆα, ˆβ) = min
y
⎧
⎨
⎩
ˆβk

(i,j)∈E
dijyk
ijp −

(i,j)∈E,j̸=0
ˆαjyk
ijp | p ∈P k
⎫
⎬
⎭(12)
If the optimal objective value of PP is less than 0, then route p can poten-
tially improve the current best solution to RMP. For each vehicle k ∈K, PP
is equivalent to the problem ﬁnding the shortest path in G(V, E), where the
cost of the edge is replaced by its reduced cost ¯cij = ˆβkdij −ˆαj for each edge
(i, j) ∈E, j ∈V +, and ¯ci0 = ˆβkdi,0 for each edge (i, 0) ∈E. Since edge costs
can be negative in PP, this problem is NP-hard [2]. Solution approaches for PP
have been studied in [5,10]. In this paper, we identify and generate the so-called
“ng-route” by implementing the following procedure. Suppose that we assign a
neighbor set to each node. An ng-route is a route where a node i ∈V + can be
revisited after a vehicle visits a node whose neighbor set does not contain node i.
A label-setting algorithm, as shown in [5], can solve ng-route relaxation to PP
that would improve the solution time of RMP.
3.2
Cutting Planes
Valid inequalities help improving the quality of the solution produced by RMP.
We add subset-row inequalities from [17] as valid cuts when solving RMP. For any
set S = {i1, i2, i3} containing three vertices i1, i2, i3 ∈V +, the corresponding
subset-row inequalities ensure that the sum of corresponding variables of all
selected routes that visit at least two vertices in S is at most 1. With set IS
containing all the routes in 	
k∈K ¯P k that visit at least two nodes in S, subset-
row inequalities are in the form of:

k∈K

p∈IS
λp ≤1
∀S ⊆V, |S| = 3.
(13)
We use the algorithm from [21] to solve PP with additional dual variables
associated with the added (13).

248
M. Yu et al.
3.3
Branching Rule
We adopt the following branching rules from [9]: we calculate the sum of ﬂows for
each edge as f k
ij = 
p∈P k aijpλp for all (i, j) ∈E and k ∈K. If f k
ij is fractional,
we generate two branches: f k
ij = 1, where vehicle k has to use edge (i, j), and
f k
ij = 0, where we exclude edge (i, j) from any route traveled by vehicle k ∈K.
4
Approximation Algorithm
Our later computational results show that a straightforward implementation of
BCP is time consuming for VRPCC. Here we propose an O(log n)-approximation
algorithm, where we recall that n is the number of customer locations. The
algorithm is based on the solution approach for the following problem deﬁned
on a network G = (V, E).
Problem 1. Maximum Budgeted Cover Problem
Input: A node subset X ⊂V , a ﬂeet K of vehicles and a budget B ≥0.
Output: A set H of routes, one for each vehicle k ∈K, where each route has
cost less than B.
Objective: Maximize |H ∩X|.
A greedy 2-approximation algorithm for this problem follows from [6]. This
algorithm was based on the idea of iteratively picking a route that covers the
maximum number of remaining nodes for the current node subset X. We propose
a variant of this algorithm which is faster and still achieves a 2-approximation.
Our greedy algorithm works with an oracle for the orienteering prob-
lem, where O(X, B, i) outputs a route, with cost less than B, for vehicle
i ∈K, which covers the maximum number of nodes in X. The greedy algo-
rithm, Greedy(X, B), for the maximum budgeted cover problem is described in
Algorithm 1.
Algorithm 1. A greedy algorithm for maximum budgeted cover
input : A ﬂeet K of vehicles, a subset X ⊂V and a budget of route B
output: A set H of routes with cost less than B, one route for each vehicle
1 H ←∅, X′ ←X
2 for i in K do
3
Ai = O(X′, B, i)
4
H ←H ∪{Ai}, X′ ←X′\Ai
5 end
6 return H
We propose the following lemmas to analyze the above algorithm. The proofs
are deferred to a full version.

Minimum Makespan Vehicle Routing Problem
249
Lemma 1. Algorithm 1 is a 2-approximation algorithm for the maximum bud-
geted cover problem.
Lemma 2. Greedy Algorithm needs to be executed at most log |X| + 1 times to
cover all nodes in X with suﬃcient large budget B.
Next, we propose an approximation algorithm for VRPCC based on
Algorithm 1. We set X = V +, where recall that V + is the set of customer
locations. If budget B is suﬃciently large that an optimal solution of maximum
budgeted cover problem covers every node in X, then by Lemma 2, we can use
Algorithm 1 to produce a feasible solution to VRPCC, where each vehicle carries
routes with total cost at most (log n + 1)B. We apply binary search to ﬁnd the
smallest budget B. We detail the algorithmic steps in Algorithm 2.
Algorithm 2. An approximation algorithm for VRPCC
input : A network G = (V, E), a ﬂeet set of vehicles K, an budget B
output: Routing assignment for each vehicle in k ∈K
1 Initialize Sk = ∅for all k ∈K, X ←V +, Solve ←true
2 while X ̸= ∅do
3
H ←output of Algorithm 1 with input K, X, B
4
n = |X|, X ←X\H
5
if |X| > n
2 then Solve ←false, go to Step 8
6
Update Sk with Ak ∈H for all k ∈K
7 end
8 Apply binary search to ﬁnd optimal B based on the result of Solve, go to
Step 1 if B is not optimal
9 Shortcut the routes in Sk, k ∈K to produce solution for VRPCC
The approximation factor of Algorithm 2 depends on the oracle for orien-
teering problem in Algorithm 1. If we use a “bicriteria” approximation for ori-
enteering that covers the optimal number of nodes with cost at most βB then
Algorithm 2 yields an approximation factor of β(log n + 1) following the result
from Lemma 2. In particular, we use a procedure from [14] which implies β = 3.
5
Numerical Results
We conduct numerical experiments to evaluate the performance of directly com-
puting the MIP, the BCP algorithm, and the approximation algorithm. We con-
duct experiments on Solomon’s instances [23] adapted to VRPCC settings. The
Solomon’s instances are classiﬁed into three classes according to diﬀerent distrib-
utions of customer locations: random distribution (R), clustered distribution (C),
and a mix of both (RC). The customer locations are distributed on a [0, 100]2
square. In the Solomon’s instances, there are 100 locations for each instance. We
pick the ﬁrst n locations to test our algorithms, where n ∈{10, 15, 20, 25, 30}.
We also test the performance of our algorithms on diﬀerent ﬂeet sizes m ∈{3, 5}.

250
M. Yu et al.
For each location v ∈V +, we randomly pick two vehicles that are capable of
serving the corresponding customer. In practice, the average productivity of a
medical home crew ranges from 4–6 visits per day [11]. Therefore, the sizes of
our test instances are close to real-world medical home care instances.
For each test instance, we ﬁrst solve the problem with model MIP and use the
result as a benchmark. We use Gurobi 6.5 [16] as the optimization solver. For the
BCP algorithm, we use Gurobi as a linear programming solver of RMP at each
branched node, and managing the branching process by following the branching
rule in Sect. 3.3. For the approximation algorithm, we apply a procedure by
Garg [14] that solves orienteering problem for optimal number of nodes but with
a budget 5B in Algorithm 1. We set the running time limit for all programs as
1,200 s. We use Java and perform numerical test on a Dell desktop with an Intel
i7-3770 processor and 16 GB memory.
For MIP and BCP, we report their best upper bounds (“UB”) and
lower bounds (“LB”) achieved, their gaps (“Gap”) and computational time
(“Time(s)”); for approximation algorithm, we report the objective values of its
solutions (“Obj”) and computational time (“Time(s)”). Tables 1 and 2 summa-
rize the numerical performances of the three methods. We use “–” to indicate
that time limit for the computation of the instance is reached.
Table 1. Numerical results with m = 3
Type n
MIP
BCP
Approx.
LB
UB
Gap
Time(s) LB
UB
Gap
Time(s) Obj
Time(s)
R
10
87.00
87.00
0.00% 0.15
87.00
87.00
0.00% 4.26
147.00 0.16
15 100.00 100.00
0.00% 3.77
100.00 100.00
0.00% 288.20
155.00 0.19
20 114.00 114.00
0.00% 26.94
104.00 143.00 27.27% –
200.00 0.57
25 140.00 140.00
0.00% 912.77
122.00 223.00 45.29% –
230.00 1.10
30 138.00 153.00
9.80% –
132.58 252.00 47.39% –
247.00 1.83
C
10
40.00
40.00
0.00% 1.12
40.00
40.00
0.00% 6.58
48.00 0.18
15
59.00
79.00 25.32% –
40.84
80.00 48.95% –
93.00 0.20
20
42.00
87.00 51.72% –
53.00
93.00 43.01% –
102.00 0.58
25
44.00
93.00 52.69% –
57.71 109.00 47.06% –
111.00 1.20
30
44.00
94.00 53.19% –
62.41 136.00 54.11% –
140.00 3.48
RC
10
87.00
87.00
0.00% 0.63
87.00
87.00 0.00%
5.90
131.00 0.14
15 119.00 119.00
0.00% 1137.84
71.02 120.00 40.82% –
171.00 0.27
20 112.00 132.00 15.15% –
86.48 132.00 34.48% –
199.00 0.43
25
92.00 157.00 41.40% –
105.27 192.00 45.17% –
261.00 1.40
30 137.00 218.00 37.16% –
126.90 332.00 61.78% –
282.00 1.88
In Table 1, when m = 3, we see that MIP performs well for instances of type
R, in which it can solve up to 25-node instances. For instances of types C and
RC, we can no longer use MIP to solve instances with more than 15 nodes. This
shows that clustered distribution of nodes is more challenging to handle. At the

Minimum Makespan Vehicle Routing Problem
251
Table 2. Numerical results with m = 5
Type n
MIP
BCP
Approx.
LB
UB
Gap
Time(s) LB
UB
Gap
Time(s) Obj
Time(s)
R
10
69.00
69.00
0.00% 0.02
69.00
69.00
0.00% 0.74
119.00 0.04
15
97.00
97.00
0.00% 0.23
97.00
97.00
0.00% 115.94
162.00 0.09
20 106.00 106.00
0.00% 3.75
93.00 116.00 19.83% –
210.00 0.25
25 120.00 120.00
0.00% 29.45
98.00 142.00 30.99% –
220.00 0.50
30 123.00 123.00
0.00% 59.36
103.92 148.00 29.78% –
193.00 0.78
C
10
40.00
40.00
0.00% 0.04
40.00
40.00
0.00% 0.99
42.00 0.03
15
78.00
78.00
0.00% 4.71
78.00
78.00
0.00% 465.68
85.00 0.10
20
82.00
82.00
0.00% 914.80
50.00
85.00 41.18% –
96.00 0.22
25
62.00
85.00 27.06% –
52.38 123.00 57.41% –
127.00 0.45
30
61.00
87.00 29.89% –
55.43 106.00 47.71% –
110.00 1.00
RC
10
83.00
83.00
0.00% 0.09
83.00
83.00
0.00% 2.08
115.00 0.03
15 105.00 105.00
0.00% 7.00
105.00 105.00
0.00% 344.88
131.00 0.12
20 130.00 130.00
0.00% 25.76
94.33 169.00 44.18% –
197.00 0.28
25 137.00 139.00
1.44% –
95.28 202.00 52.83% –
246.00 0.50
30 167.00 171.00
2.34% –
109.27 328.00 66.69% –
307.00 0.82
same time, BCP does not outperform MIP, and we can only use BCP to solve
instances with 15 nodes or fewer. Although BCP algorithm cannot be solved very
eﬃciently yet, it can produce better lower bound for instances that both MIP
and BCP cannot solve to optimality, e.g., instances with 20, 25, and 30 nodes of
type C. On the other hand, approximation algorithm solves all the instances very
quickly. The computation can be ﬁnished within 4 s for all instances, and there
is no signiﬁcant diﬀerence in computational time when solving instances of type
C/RC than type R. Moreover, despite that the theoretical approximation bound
of our algorithm could be large when n increases, the practical approximation
ratio is within a factor of 2 when comparing the objective values of solutions
produced by the approximation algorithm with the best lower bounds produced
by MIP and BCP for all instances.
Table 2 summarizes the numerical results for instances with larger ﬂeet
size m = 5. Results show that the computational time has been signiﬁcantly
improved and more instances can be solved by MIP and BCP. Our approxi-
mation algorithm can solve all instances within one second, and the practical
approximation ratio is still within a factor of 2 comparing its objective values of
solutions with the best lower bounds of MIP and BCP.
To summarize, we recognize that diﬀerent distributions of demand locations
could aﬀect the computational time of our proposed exact solution methods.
Despite lacking eﬃcient exact solution approaches for VRPCC, our proposed
approximation algorithm can eﬃciently solve the problem and provides good
practical solutions.

252
M. Yu et al.
6
Conclusions and Future Research
In this paper, we formulated a minimum makespan routing problem with com-
patibility constraints. We proposed three solution approaches for the problem:
MIP, BCP, and an approximation algorithm. Numerical results showed that
MIP and BCP could not solve many of these instances to optimality (within our
time limit), whereas the approximation algorithm obtained good quality solu-
tions within seconds. Moreover, our approximate solutions (for these instances)
are within two times the best lower bounds from MIP or BCP. Future research
includes further investigation on BCP to improve its eﬃciency and designing
good implementations to improve the practical approximation factor of the
approximation algorithm.
References
1. Achuthan, N.R., Caccetta, L., Hill, S.P.: An improved branch-and-cut algorithm
for the capacitated vehicle routing problem. Transp. Sci. 37(2), 153–169 (2003)
2. Ahuja, R.K., Magnanti, T.L., Orlin, J.B.: Network Flows: Theory, Algorithms, and
Applications. Prentice Hall, Upper Saddle River (1993)
3. Applegate, D., Cook, W., Dash, S., Rohe, A.: Solution of a min-max vehicle routing
problem. INFORMS J. Comput. 14(2), 132–143 (2002)
4. Arkin, E.M., Hassin, R., Levin, A.: Approximations for minimum and min-max
vehicle routing problems. J. Algorithms 59(1), 1–18 (2006)
5. Baldacci, R., Mingozzi, A., Roberti, R.: New route relaxation and pricing strategies
for the vehicle routing problem. Oper. Res. 59(5), 1269–1283 (2011)
6. Chekuri, C., Kumar, A.: Maximum coverage problem with group budget con-
straints and applications. In: Jansen, K., Khanna, S., Rolim, J.D.P., Ron, D.
(eds.)
APPROX/RANDOM
2004.
LNCS,
vol.
3122,
pp.
72–83.
Springer,
Heidelberg (2004). doi:10.1007/978-3-540-27821-4 7
7. Dantzig, G.B., Ramser, J.H.: The truck dispatching problem. Manage. Sci. 6(1),
80–91 (1959)
8. Even, G., Garg, N., K¨onemann, J., Ravi, R., Sinha, A.: Min-max tree covers of
graphs. Oper. Res. Lett. 32(4), 309–315 (2004)
9. Feillet, D.: A tutorial on column generation and branch-and-price for vehicle rout-
ing problems. Q. J. Oper. Res. 8(4), 407–424 (2010)
10. Feillet, D., Dejax, P., Gendreau, M., Gueguen, C.: An exact algorithm for the
elementary shortest path problem with resource constraints: application to some
vehicle routing problems. Networks 44(3), 216–229 (2004)
11. National Association for Home Care & Hospice. Basic Statistics About Home Care,
pp. 1–14. National Association for Home Care & Hospice, Washington, DC (2010)
12. Frederickson, G.N., Hecht, M.S., Kim, C.E.: Approximation algorithms for some
routing problems. In: 17th Annual Symposium on Foundations of Computer Sci-
ence, pp. 216–227. IEEE (1976)
13. Fukasawa, R., Longo, H., Lysgaard, J., de Arag˜ao, M.P., Reis, M., Uchoa, E.,
Werneck, R.F.: Robust branch-and-cut-and-price for the capacitated vehicle rout-
ing problem. Math. Program. 106(3), 491–511 (2006)
14. Garg, N.: A 3-approximation for the minimum tree spanning k vertices. In: Pro-
ceedings of the 37th Annual Symposium on Foundations of Computer Science,
FOCS 1996, pp. 302–309. IEEE Computer Society, Washington, DC (1996)

Minimum Makespan Vehicle Routing Problem
253
15. Golden, B.L., Raghavan, S., Wasil, E.A.: Problem, The Vehicle Routing: Latest
Advances and New Challenges. Springer Science & Business Media, New York
(2008)
16. Gurobi Optimization, Inc., Gurobi optimizer reference manual (2016). http://www.
gurobi.com
17. Jepsen, M., Petersen, B., Spoorendonk, S., Pisinger, D.: Subset-row inequalities
applied to the vehicle-routing problem with time windows. Oper. Res. 56(2), 497–
511 (2008)
18. Kallehauge, B., Larsen, J., Madsen, O.B., Solomon, M.M.: Vehicle routing prob-
lem with time windows. In: Desaulniers, G., Desrosiers, J., Solomon, M.M. (eds.)
Column Generation, pp. 67–98. Springer, New York (2005)
19. Letchford, A.N., Eglese, R.W., Lysgaard, J.: Multistars, partial multistars and the
capacitated vehicle routing problem. Math. Program. 94(1), 21–40 (2002)
20. Lysgaard, J., Letchford, A.N., Eglese, R.W.: A new branch-and-cut algorithm for
the capacitated vehicle routing problem. Math. Program. 100(2), 423–445 (2004)
21. Pecin, D., Pessoa, A., Poggi, M., Uchoa, E.: Improved branch-cut-and-price for
capacitated vehicle routing. In: Lee, J., Vygen, J. (eds.) IPCO 2014. LNCS, vol.
8494, pp. 393–403. Springer, Cham (2014). doi:10.1007/978-3-319-07557-0 33
22. Ralphs, T.K., Kopman, L., Pulleyblank, W.R., Trotter, L.E.: On the capacitated
vehicle routing problem. Math. Program. 94(2–3), 343–359 (2003)
23. Solomon, M.M.: Algorithms for the vehicle routing and scheduling problems with
time window constraints. Oper. Res. 35(2), 254–265 (1987)
24. Toth, P., Vigo, D., Routing, V.: Problems, Methods, and Applications. SIAM,
Philadelphia (2014)

Solving the Traveling Salesman Problem
with Time Windows Through Dynamically
Generated Time-Expanded Networks
Natashia Boland1, Mike Hewitt2(B), Duc Minh Vu2, and Martin Savelsbergh1
1 Georgia Institute of Technology, Atlanta, USA
2 Loyola Chicago University, Chicago, USA
mhewitt3@luc.edu
Abstract. The Traveling Salesman Problem with Time Windows is the
problem of ﬁnding a minimum-cost path visiting each of a set of cities
exactly once, where each city must be visited within a speciﬁed time
window. It has received signiﬁcant attention because it occurs as a sub-
problem in many real-life routing and scheduling problems. We explore
an approach in which the strength of a time-expanded integer linear pro-
gramming (IP) formulation is exploited without ever explicitly creating
the complete formulation. The approach works with carefully designed
partially time-expanded networks, which are used to produce upper as
well as lower bounds, and which are iteratively reﬁned until optimality
is reached. Preliminary computational results illustrate the potential of
the approach as, for almost all instances tested, optimal solutions can be
identiﬁed in only a few iterations.
Keywords: Traveling Salesman Problem · Time Windows · Time-
expanded networks · Mixed integer programming · Dynamic discretiza-
tion discovery
1
Introduction
The Traveling Salesman Problem with Time Windows (TSPTW) is deﬁned as
follows. Let (N, A) be a complete directed graph with node set N = {0, 1, 2, ..., n}
representing the set of cities that must be visited by the salesman, starting and
ending in designated city 0, and with arc set A ⊆N×N representing the roads on
which the salesman can travel. Each arc (i, j) ∈A has an associated travel time,
denoted by τij, and an associated cost, denoted by cij. Furthermore, each city
i ∈N can only be visited during a speciﬁed time interval [ei, li], referred to as
the city’s time window. The Traveling Salesman Problem with Time Windows
seeks a minimum-cost tour that departs city 0 at or after e0, departs city i
(i ∈N \{0}) within its time window [ei, li], and returns to city 0 at or before l0.
Note that arriving at city i (i ∈N \ {0}) before ei is allowed, but implies that
the salesman has to wait. Evaluating whether a given city sequence results in
a feasible solution can be done in linear time (by departing every city as early
c
⃝Springer International Publishing AG 2017
D. Salvagnin and M. Lombardi (Eds.): CPAIOR 2017, LNCS 10335, pp. 254–262, 2017.
DOI: 10.1007/978-3-319-59776-8 21

Solving TSPTW Through Dynamically Generated Time-Expanded Networks
255
as possible), but ﬁnding a city sequence that results in a feasible solution is
NP-hard [8].
There is a large body of work available on exact solution of the TSPTW, from
both the operations research and constraint programming communities, e.g.,
[2,4–6,9,11,12]. The TSPTW is notoriously diﬃcult to solve. Compact integer
programming (IP) formulations that use continuous variables to model time have
weak linear programming (LP) relaxations. Their solution with current IP solver
technology is limited to only small instances. Extended IP formulations, with
binary variables indexed by time, have much stronger relaxations, but (tend to)
have a huge number of variables. In this paper, we explore an approach in which
the strength of an extended IP formulation is exploited without ever explicitly
creating the complete formulation. Preliminary work using this approach for a
service network design problem arising in less-than-truckload transportation has
been very promising: it is able to solve much larger instances than is possible
with other techniques [3]. The key to the approach is that it discovers exactly
which times are needed to obtain an optimal, continuous-time solution, in an
eﬃcient way, by solving a sequence of (small) IPs. The IPs are constructed as a
function of a subset of times, with variables indexed by times in the subset. They
are carefully designed to be tractable in practice, and to yield a lower bound (it
is a cost minimization problem) on the optimal continuous-time value. Once the
right (very small) subset of times is discovered, the resulting IP model yields
the continuous-time optimal value. In this paper, we explore whether such an
approach can also be used to solve instances of the TSPTW.
Regarding the existing literature, the method of [4] also dynamically gener-
ates a time-expanded network in the context of solving the TSPTW. However,
they do so as a preprocessing scheme for a branch-and-cut solution approach.
As such, in their approach, the time-expanded network is never changed dur-
ing the branch-and-cut search, whereas we continue to reﬁne the discretization
until the problem is solved. Similarly, the method of [4] uses information from
the solution to an LP to heuristically reﬁne the set of time points, whereas we
use information from the solution to an IP to carefully reﬁne the set of time
points to guarantee convergence to an optimal solution to the TSPTW instance.
From a computational perspective, our approach outperforms nearly all existing
methods on nearly all instances. While the method of [2] performs better than
ours, we believe the performance of our approach can be greatly improved by
constraint programming-type ideas.
2
Problem Formulation and Solution Approach
Because of the time dimension, it is natural to use a time-expanded network
to model the TSPTW. Assuming integer travel times and time windows, the
time-expanded network for the TSPTW has, for each city i ∈N, a timed node,
(i, t) ∈N, for each t ∈[ei, li] = {ei, ei+1, . . . , li}. A timed arc a = ((i, t)(j, t′)) ∈
A ⊆N × N represents travel from city i departing at time t to city j arriving
at time t′ = max{ej, t + τij}, deﬁned if t′ ≤lj. The cost of timed arc a =
((i, t), (j, t′)) ∈A is denoted by ca and taken to be cij.

256
N. Boland et al.
An integer programming model of the TSPTW based on the time-expanded
network, (N, A), is as follows. Let xa for arc a ∈A be a binary variable indicating
whether the timed arc is used (xa = 1) or not (xa = 0). Then the following
integer program solves the TSPTW:
min

a∈A
caxa
(1)

a=((i,t)(j,t′))∈A
xa = 1
∀i ∈N,
(2)

a∈δ−((i,t))
xa −

a∈δ+((i,t))
xa = 0
∀(i, t) ∈N, i ̸= 0,
(3)
xa ∈{0, 1}
for all a ∈A,
(4)
where δ−() and δ+() denote the set of timed arcs coming into and going out of
a timed node. Constraints (2) force one unit of ﬂow to leave city i for all i ∈N,
whereas constraints (3) are ﬂow balance constraints that ensure that the ﬂow
into a timed node equals the ﬂow out of the node at all cities except city 0.
Note that this model embeds the assumption, which may be made without loss
of generality, that any waiting in the tour occurs prior to the start of a time
window; the salesman will either depart a city as soon as he arrives, or at the
start of the city’s time window, whichever is later.
The TSPTW formulation (1)–(4) deﬁned on the time-expanded network
needs no subtour elimination constraints, and we found, in our experiments, that
its LP relaxation yields a very strong lower bound. However, it has a huge num-
ber of variables because of the time dimension, making solution with IP solvers
impractical for all but the smallest instances. In the next section, we show how
we iteratively build and update small partially time-expanded networks to solve
TSPTW instead of using the whole time-expanded network.
We derive a partially time-expanded network, DT = (NT , AT ), from a given
subset of the timed nodes, NT ⊆N. The arc set, AT ⊆NT × NT , of a partially
time-expanded network consists of arcs of the form ((i, t), (j, t′)), where (i, j) ∈
A. The cost of arc a = ((i, t), (j, t′)) ∈AT is cij. We do not require that arc
((i, t), (j, t′)) satisﬁes t′ = max{ej, t + τij}. In fact, the ﬂexibility to introduce
timed arcs with travel time diﬀerent to the actual travel time between the cities
at their start and end is an essential feature of our approach. Let TSPTW(DT )
denote the IP formulation (1)–(4) deﬁned with respect to DT instead of (N, A).
By careful design of DT , TSPTW(DT ) can be guaranteed to yield a lower bound
on the value of the TSPTW, or can be guaranteed, if feasible, to yield an upper
bound. We ﬁrst discuss properties of DT that ensure a lower bound.
Property 1. For every i ∈N, the nodes (i, ei) and (i, li) are in NT .
Property 2. For every node (i, t) ∈NT and for every (i, j) ∈A with t +
τij ≤lj, there is an arc of the form ((i, t)(j, t′)) ∈AT . Furthermore, every
arc ((i, t), (j, t′)) ∈AT must have either (1) t + τij < ej and t′ = ej or (2)
ej ≤t′ ≤t + τij.

Solving TSPTW Through Dynamically Generated Time-Expanded Networks
257
This ensures that the travel time on arc ((i, t), (j, t′)) ∈AT is either accurate, or
is underestimated: t′ ≤max{ej, t+τij}. We say the arc is too short if t′ < t+τij.
Lemma 1. If DT satisﬁes Properties 1 and 2, then the value of TSPTW(DT )
is a lower bound on the value of TSPTW.
Corollary 1. If
the
city
sequence
speciﬁed
by
an
optimal
solution
to
TSPTW(DT ) is feasible with actual travel times, then it is an optimal tour for
TSPTW. Otherwise, the solution contains at least one arc that is too short.
In fact, at most one arc in AT is required for each (i, t) ∈NT and each arc
(i, j) ∈A with t + τij ≤lj, and using the one with the longest possible travel
time, consistent with the above two properties, yields the best lower bound.
Property 3. If arc ((i, t), (j, t′)) ∈AT , then there is no (j, t′′) ∈NT with
t′ < t′′ ≤t + τij. We refer to this property as the longest-feasible-arc property.
Lemma 2. For a ﬁxed NT , the partially time-expanded network DT with the
longest-feasible-arc property induces an instance of TSPTW(DT ) with the largest
optimal objective function value.
Given
a
partially
time-expanded
network,
DT ,
containing
an
arc,
((i, t), (j, t′)), that is too short, we can “correct” the resulting travel time under-
estimation using procedure Lengthen-arc((i, t), (j, t′)). In this procedure, a
new timed node is added to NT to reﬂect the correct arrival time at j if depart-
ing i at time t, and the arcs in AT are updated so as to maintain Properties 1–3.
Algorithm 1. Lengthen-arc((i, t), (j, t′))
Require: Arc ((i, t), (j, t′)) ∈AT with t′ < t + τij
1: Add the new node (j, t′′) with t′′ = t + τij to NT .
2: Add new arcs out of node (j, t′′) so as to satisfy Properties 2 and 3.
3: For any too short arc ((h, u), (j, t′)), if u + τhj ≥t′′, replace this arc by the arc
((h, u), (j, t′′)) (maintain Property 3).
Lemma 3. If D′
T is formed from DT by lengthening an arc, then the optimal
objective function value of TSPTW(D′
T ) is at least as large as the optimal objec-
tive function value of TSPTW(DT ). Furthermore, if DT satisﬁes Properties 1–3,
then so does D′
T .
The above properties and lemmas provide the building blocks for an iterative
reﬁnement algorithm for solving TSPTW, which is found in Algorithm 2.
By Lemma 1, Corollary 1, Lemma 3, and since the number of arcs is ﬁnite,
we have the following.
Theorem 1. Solve-TSPTW terminates with an optimal solution.

258
N. Boland et al.
Algorithm 2. Solve-TSPTW
Require: TSPTW instance (N, A), e, l, τ and c
1: Create a partially time-expanded network DT satisfying Properties 1–3.
2: while not solved do
3:
Solve TSPTW(DT )
4:
Determine whether the tour speciﬁed by the solution to TSPTW(DT ) is
feasible with actual travel times
5:
if it is feasible then
6:
Stop: the tour is optimal for TSPTW
7:
end if
8:
Reﬁne DT by correcting an arc in AT that is too short: for at least one
a = ((i, t), (j, t′)) ∈AT with t′ < t + τij, run Lengthen-Arc(a)
9: end while
To improve the overall performance of Algorithm 2, we ﬁrst use well known
preprocessing steps, such as described in [4], to tighten the time windows and to
remove unnecessary arcs. We also add valid inequalities to the TSPTW(DT ) for-
mulation to eliminate subtours and bad paths (paths that violate time windows
if traversed using actual travel times) as they are encountered in the algorithm.
We use the simplest form of such inequalities, adapted to the time-expanded
setting. Speciﬁcally, if M ⊂N, we eliminate tours on M with the constraint

(i,j)∈A∩(M×M)

t,t′:((i,t),(j,t′))∈AT
xa ≤|M| −1.
Similarly, if P ⊂A is a bad path, we eliminate its use with the inequality

(i,j)∈P

t,t′:((i,t),(j,t′))∈AT
xa ≤|P| −1.
We detect node sets that give rise to subtours and bad paths found in integer
solutions generated in the course of the algorithm, and add their elimination
constraints to the TSPTW(DT ) models solved subsequently. We found it helpful
to add, a priori, all subtour elimination constraints for node sets of size 2.
Finally, we use two heuristics to speed up the search of feasible solutions.
Both use NT to create a partially time-expanded network, diﬀerent from DT ,
which are designed to generate feasible solutions. Both networks, denoted by
D1
T = (NT , A1
T ) and D2
T = (NT , A2
T ), only contain timed arcs that have positive
travel time, and so integer solutions on these networks cannot include subtours.
The ﬁrst is more conservative: A1
T is created by taking each (i, t) ∈NT and each
(i, j) ∈A with t + τij ≤lj, ﬁnding the earliest time, t′, with (j, t′) ∈NT and
t′ ≥t+τij, and adding arc ((i, t), (j, t′)) to A1
T . Note that such an arc must exist,
by Property 1 ((i, li) ∈NT for all i ∈N). Now any integer feasible solution to
TSPTW(D1
T ) must be a feasible TSPTW tour. The second network may still
contain arcs that are too short. Its arcs are created by taking each (i, t) ∈NT
and each (i, j) ∈A with t + τij ≤lj, ﬁnding the latest time, t′, with (j, t′) ∈NT

Solving TSPTW Through Dynamically Generated Time-Expanded Networks
259
Algorithm 3. Solve-TSPTW-Enhanced
Require: TSPTW instance (N, A), e, l, τ and c, and optimality tolerance ϵ
1: Perform preprocessing, updating A, e and l.
2: Create a partially time-expanded network DT .
3: S = ∅{S contains feasible solutions to the TSPTW that have been found}
4: while not solved do
5:
Build and solve TSPTW(DT ), harvest integer solutions, ¯S, and lower bound, z.
6:
if S = ∅then
7:
Build and solve TSPTW(D1
T ), harvest integer solutions and add to S.
8:
end if
9:
Build and solve TSPTW(D2
T ), harvest integer solutions and add to ¯S.
10:
for all ¯s ∈¯S do
11:
if ¯s can be converted to a feasible solution s then
12:
Add s to S.
13:
end if
14:
Remove ¯s from ¯S.
15:
end for
16:
Compute gap δ between best solution in S and lower bound, z.
17:
if δ ≤ϵ then
18:
Stop: best solution in S is ϵ−optimal for TSPTW.
19:
end if
20:
Reﬁne DT by lengthening arcs that are too short.
21: end while
and t′ ≤t + τij, and then checking two cases: (1) t′ > t: in this case, we add
((i, t), (j, t′)) to A2
T ; (2) t′ ≤t: in this case, ﬁnd the earliest t′′ with (j, t′′) ∈NT
and t′′ > t, and add ((i, t), (j, t′′)) to A2
T . Solutions to TSPTW(D2
T ) may have
bad paths, so we also add all bad path elimination inequalities to the formulation
before solving it. An integer feasible solution to TSPTW(D2
T ) may, or may not,
be feasible for the TSPTW.
Preliminary experiments showed that TSPTW(D1
T ) was helpful for ﬁnding
an initial feasible TSPTW tour sooner, but rarely gave better solutions than
could be found with other IP formulations afterwards, and so is disabled after
the ﬁrst feasible TSPTW tour is found. TSPTW(D2
T ) is solved at every iteration.
For every IP solved, (TSPTW(DT ), TSPTW(D1
T ) or TSPTW(D2
T )) we har-
vest all integer solutions found by the solver. All are checked for feasibility to
the TSPTW, and any subtours and bad paths discovered, in the case that the
solution is not feasible, are recorded. This can be done in linear time. Any new
best feasible solution is saved. Also, all too short arcs used in one of these solu-
tions, if they led to infeasibility with actual travel times, are lengthened in the
iteration in which they are found.
Algorithm 3 summarizes how we solve the TSPTW using partially time-
expanded networks.

260
N. Boland et al.
3
Computational Results
The algorithm is coded in C++ and uses Gurobi 6.5.0 with default settings (and
a single thread) to solve IP problems. All computational results are obtained
on a workstation with a Intel(R) Xeon (R) CPU E5-4610 v2 2.30 GHz processor
running the Ubutu Linux 14.04.3 Operating System. Algorithm performance is
evaluated on 5 classes of instances1, with a total of 337 instances: 32 “easy” and
18 “hard” instances from [1], (AFG-Easy and AFG-Hard), 135 instances from
[5], (DDGS), 125 from [10], (SU), and 27 instances from [7], (SPGPR).
The partially time-expanded networks are initialized with NT = {(i, ei) : i ∈
N} ∪{(i, li) : i ∈N}. When solving IPs at lines 6, 8 and 10 of Algorithm 3, the
IP solver is given an optimality tolerance of ϵ and a time limit of 10 min. At later
iterations, as the partially time-expanded network becomes larger, it is possible
that the IP solver does not return a solution to TSPTW(DT ) within 10 min. If
it happens, we let the solver run until it ﬁnds an integer solution. An overall
time limit of 5 h is set; if Algorithm 3 does not stop at line 18 in that time, the
instance is deemed to have “timed out”. We assess
1. the average performance of Solve-TSPTW in term of running time, optimal
gap, and gap versus best solutions in literature,
2. in the case of time-out, the Solve-TSPTW solution quality with respect to
the best in literature, and
3. the size of the partially time-expanded network the algorithm dynamically
discovers versus the size of the full time-space network.
The ﬁrst two points are addressed in Table 1, where in the top half we report
results for all instances and in the bottom half we report results for instances
that timed out. Values in columns 2–7 in the top half and columns 3–5 in the
bottom half represent averages over all instances in a class. The column with
heading “# opt” gives the number of instances that were solved to within a gap
of ϵ (the algorithm terminated at line 18) and the column with heading “# ≤
best” gives the number of instances for which the best feasible solution found
by Solve-TSPTW has value within 0.01% of the best known value reported
in the literature. The algorithm proved optimality for 313 out of 337 instances
and produced 329 solutions of equal or better value than best-known solutions.
Except for class SPGPR, the instances that timed out have small optimal gaps,
which indicates that the solutions are optimal or close to optimal. In all cases,
they are very close to the best solutions reported in the literature. We note that
the algorithm produced new best solutions for 41 instances in class SU.
Finally, Table 2 shows that Algorithm 3 requires only a very small num-
ber of nodes and arcs to produce high-quality solutions (compared to the full
time-expanded network). In summary, the computational results show that a
dynamic discretization discovery algorithm using carefully designed partially
1 http://lopez-ibanez.eu/tsptw-instances,
http://homepages.dcc.ufmg.br/∼rfsilva/
tsptw/.

Solving TSPTW Through Dynamically Generated Time-Expanded Networks
261
Table 1. Performance of Algorithm 3
Instance
# Time
# Gap at Gap at Gap vs
# # ≤
class
solns
(s) iter 1st iter last iter
best opt best
AFG-Easy
3.97
23
9
0.56%
0.00%
0.00%
32
32
AFG-Hard 9.00 7,122
23
1.02%
0.09%
0.01%
14
15
DDGS
2.03
605
11
4.69%
0.00%
0.01% 132
134
SU
3.34 2,846
14
8.31%
0.06% -0.06% 112
123
SPGPR
3.78 2,989
9
9.01%
0.73%
0.03%
23
25
Instance
# timed
Ave
Max Gap vs
class
out
gap
gap
best
AFG-Hard
4 0.35% 0.64%
0.15%
DDGS
3 0.66% 1.01%
0.06%
SU
13 0.59% 1.16% -0.16%
SPGPR
4 4.89% 9.01%
0.22%
No instances in class AFG-Easy timed out
Table 2. Time-expanded network size.
Instances
Full time-expanded network Final partially time-expanded network
#Nodes
#Arcs
#Nodes #Arcs
AFG-Easy
69 k
1,336 k
232
3,549
AFG-Hard 133 k
10,441 k
2,190
50,100
DDGS
3 k
185 k
400
4,773
SU
45 k
7,195 k
1,292
8,512
SPGPR
107,672 k 2,631,490 k
564
14,308
time-expanded networks can produce high-quality solutions to the TSPTW. Fur-
ther algorithm engineering, e.g., improved cut generation and management and
more sophisticated schemes for choosing arcs to lengthen, can enhance its per-
formance. It is easy to modify the algorithm to handle TSPTW variants, e.g.,
TSPTW with time-dependent travel times.
References
1. Ascheuer, N., Fischetti, M., Gr¨otschel, M.: A polyhedral study of the asymmetric
traveling salesman problem with time windows. Networks 36(2), 69–79 (2000)
2. Baldacci, R., Mingozzi, A., Roberti, R.: New state-space relaxations for solving
the traveling salesman problem with time windows. INFORMS J. Comput. 24(3),
356–371 (2012)
3. Boland, N., Hewitt, M., Marshall, L., Savelsbergh, M.: The continuous time service
network design problem. Optimization Online 2015–01-4729 (2015)
4. Dash, S., G¨unl¨uk, O., Lodi, A., Tramontani, A.: A time bucket formulation for
the traveling salesman problem with time windows. INFORMS J. Comput. 24(1),
132–147 (2012)

262
N. Boland et al.
5. Dumas, Y., Desrosiers, J., G´elinas, ´E., Solomon, M.M.: An optimal algorithm for
the traveling salesman problem with time windows. Oper. Res. 43(2), 367–371
(1995)
6. Focacci, F., Lodi, A., Milano, M.: A hybrid exact algorithm for the TSPTW.
INFORMS J. Comput. 14(4), 403–417 (2002)
7. Pesant, G., Gendreau, M., Potvin, J., Rousseau, J.: An exact constraint logic
programming algorithm for the traveling salesman problem with time windows.
Transp. Sci. 32(1), 12–29 (1998)
8. Savelsbergh, M.W.P.: Local search in routing problems with time windows. Ann.
Oper. Res. 4(1), 285–305 (1985)
9. Savelsbergh, M.W.P.: The vehicle routing problem with time windows: minimizing
route duration. INFORMS J. Comput. 4(2), 146–154 (1992)
10. da Silva, R.F., Urrutia, S.: A general VNS heuristic for the traveling salesman
problem with time windows. Discrete Optim. 7(4), 203–211 (2010)
11. Wang, X., Regan, A.: Local truckload pickup and delivery with hard time window
constraints. Transp. Res. Part B 36, 97–112 (2002)
12. Wang, X., Regan, A.: On the convergence of a new time window discretization
method for the traveling salesman problem with time window constraints. Comput.
Ind. Eng. 56, 161–164 (2009)

A Fast Prize-Collecting Steiner Forest Algorithm
for Functional Analyses in Biological Networks
Murodzhon Akhmedov1,2,3, Alexander LeNail1, Francesco Bertoni3,
Ivo Kwee2,3, Ernest Fraenkel1, and Roberto Montemanni2(B)
1 Department of Biological Engineering, Massachusetts Institute of Technology,
Cambridge, MA, USA
{akhmedov,lenail,fraenkel-admin}@mit.edu
2 Dalle Molle Institute for Artiﬁcial Intelligence Research (IDSIA-USI/SUPSI),
Manno, Switzerland
{murodzhon,roberto}@idsia.ch
3 Institute of Oncology Research (IOR), Bellinzona, Switzerland
{francesco.bertoni,ivo.kwee}@ior.iosi.ch
Abstract. The Prize-collecting Steiner Forest (PCSF) problem is NP-
hard, requiring extreme computational eﬀort to ﬁnd exact solutions for
large inputs. We introduce a new heuristic algorithm for PCSF which pre-
serves the quality of solutions obtained by previous heuristic approaches
while reducing the runtime by a factor of 10 for larger graphs. By decreas-
ing the draw on computational resources, this algorithm aﬀords systems
biologists the opportunity to analyze larger biological networks faster
and narrow their analyses to individual patients.
Keywords: Prize-collecting Steiner Forest · Biological networks
1
Introduction
The Prize-collecting Steiner Tree Problem (PCST) is a widely studied problem
in the combinatorial optimization literature [2]. In PCST, we are given a graph
G = (V, E) such that vertices are assigned prizes ∀v ∈V : p(v) ∈R+ and edges
are weighted with costs ∀e ∈E : c(e) ∈R+. The objective is to ﬁnd a tree
T = (Vt, Et) which minimizes:
f(T) =

e∈Et
c(e) +

v̸∈Vt
p(v)
(1)
Vertices with prize p(v) = 0 are referred to as Steiner nodes, and vertices with
p(v) > 0 are called terminal nodes [7].
In contrast, the Prize-collecting Steiner Forest (PCSF) problem searches for
a forest over input graph G. The PCSF is more general form of the PCST,
and both problems are proven to be NP-hard [6,21]. The forest solution for the
PCSF can be obtained by slightly modifying an input graph, and solving it by
PCST algorithms. By adding an artiﬁcial root node to the input graph with
c
⃝Springer International Publishing AG 2017
D. Salvagnin and M. Lombardi (Eds.): CPAIOR 2017, LNCS 10335, pp. 263–276, 2017.
DOI: 10.1007/978-3-319-59776-8 22

264
M. Akhmedov et al.
edges from root to every other node with weight ω, we construct an augmented
graph G′ = (V ∪root, E ∪Eroot), where Eroot = e(root,v∈V ) with associated cost
function ∀e ∈Eroot : c′(e) = ω. Then, the PCST is solved on G′ with a slightly
modiﬁed objective function [8]:
f(T) =

e∈Et\Eroot
c(e) +

v /∈Vt
p(v) +

e∈Et∩Eroot
c′(e)
(2)
The ﬁnal forest solution is obtained by removing the root from T as well as the
edges emanating from it, such that the single tree solution connected by root
becomes a solution of disconnected components. The parameter ω regulates the
number of selected outgoing edges from the root, which determines the number
of trees in the forest. A forest with distinct sizes and diﬀerent number of trees
can be obtained by running the algorithm for diﬀerent values of ω.
The PCSF problem from combinatorial optimization maps nicely onto the
biological problem of ﬁnding diﬀerentially enriched sub-networks in the interac-
tome of a cell. An interactome is a graph in which vertices represent biomolecules
and edges represent the known physical interactions of those biomolecules. We
can assign prizes to vertices based on measurements of diﬀerential expression of
those cellular quantities in a patient sample and costs to edges from conﬁdence
scores for those intra-cellular interactions from experimental observation (high
conﬁdence means low edge cost), yielding a viable input to the PCSF prob-
lem. Vertices of the interactome which are not observed in patient data are not
assigned a prize and become the Steiner nodes.
An algorithm to approximately solve the Prize-Collecting Steiner Forest
problem can then be applied against this augmented interactome, resulting in
a set of subgraphs corresponding to subsections of the interactome in which
functionally related biomolecules may play an important concerted role in the
diﬀerentially active biological process of interest. Thus, PCSF, when applied in
a biological context, can be used to predict neighborhoods of the interactome
belonging to the key dysregulated pathways of a disease.
Since the PCSF problem is NP-hard, it requires tremendous amounts of
computation to determine exact solutions for large inputs. In biology, large net-
works are the norm. For example, the human Protein-Protein Interaction (PPI)
network has some 15,000 nodes and 175,000 edges [1]. Adding metabolites for a
more complete interactome yields inputs of some 36,000 nodes and over 1,000,000
edges. We require eﬃcient algorithms to investigate these huge biological graphs
for each patient, which requires solving PCSF many times.
We present a heuristic algorithm for PCSF which outperforms other heuristic
approaches from the literature in computational eﬃciency for larger graphs by
a factor of 10, while preserving the quality of the results. Our algorithm, MST-
PCSF, builds from the ideas presented in [24,25]. In contrast to [24,25], MST-
PCSF searches for a forest structure rather than a tree in the network, which
represents the underlying biological problem better. Moreover, MST-PCSF uses
more greedy clustering preprocessing step which divides large input graph into
smaller clusters, and a heuristic to ﬁnd approximate solutions for each cluster.

A Fast Prize-Collecting Steiner Forest Algorithm
265
2
Related Work
The authors in [2] initially studied the prize-collecting traveling salesman
problem, and proposed a 3-approximation algorithm. A 2-approximation with
O(n3logn) running time is proposed in [3] by using the primal-dual method,
improved by [4] to O(n2logn) execution time. That runtime was maintained in
[5], which improved the approximation factor to 2 −2
n.
Exact methods were devised in [6,7] using mixed integer linear programming,
and a branch-and-cut algorithm based on directed edges was proposed to solve
the model. An exact row generation approach was presented in [9] using a new
set of valid inequalities.
A relax-and-cut algorithm was studied in [10] to develop eﬀective Lagrangian
heuristic. A multi-start local search algorithm with perturbations was integrated
with variable neighborhood search in [11]. Seven diﬀerent variations of PCST
were studied in [12], and polynomial algorithms were designed for four of them.
Some lower bound and polyhedral analyses were performed in [13–16]. A tabu-
search metaheuristic, and a combination of memetic algorithms and integer pro-
gramming were employed in [17,18] for PCST.
The application of the PCST approach in Biology has led to important
results. The speciﬁc biological problems were to identify functions of molecules
[19], to ﬁnd protein associations [20], and to reconstruct multiple signaling path-
ways [21]. We have been developing heuristic and matheuristic PSCT algorithms
[24,25] for similar applications, and in this study we aim to extend these ideas
to a forest approach in order to make more realistic biological inferences.
3
Algorithm: MST-PCSF, A Fast Heuristic Algorithm
for the PCSF Problem
The proposed MST-PCSF heuristic algorithm is composed of two distinct phases.
First, we cluster the input graph to transform a global problem into a set of
smaller local problems. Second, we bypass the intractability of PCSF by instead
solving the Minimum Spanning Tree Problem (MST) on an altered representa-
tion of the input graph. These simple ideas dramatically reduce the time needed
to obtain high quality solutions to the Prize-collecting Steiner Forest Problem.
3.1
Greedy Clustering Transforms a Global Problem
into a Set of Local Problems
The intuition behind the clustering phase of the algorithm is that, in the context
of biological interaction networks, we anticipate ﬁnding groups of dysregulated
terminal nodes within the expression data, which are joined by moderately high
conﬁdence edges and Steiner nodes, forming independent high-prize and low-cost
patches in the input graph. Simply put, we anticipate the clusters exist quite
strongly in the data, and constituent trees from the optimal solution forest will
be split between these clusters, but will not span multiple clusters. Clustering
therefore is a sensible ﬁrst step.

266
M. Akhmedov et al.
Fig. 1. Clustering phase of MST-PCSF. (A) The original network, such that
terminal nodes are represented in yellow, Steiner nodes are in brown, edges are in blue
and edge thickness relates to cost. (B) Compute All-Paths Shortest Path from every
terminal to every terminal on the graph, resulting in a matrix of shortest path distances.
(C) Choose a terminal at random and evaluate which terminals are reachable, assign
them to a cluster. (D) Iteratively select nodes assigned to the cluster and ﬁnd additional
nodes which satisfy the clustering criterion with respect to other nodes in the cluster.
(E) Cluster is full when no more terminals satisfy the criterion. Start a new cluster with
an unclustered node. (F) Repeat steps B–E until there are no remaining unclustered
nodes from the graph in A. (G) The raw clustering of the graph in A may have
many singletons and doubletons. (H) Merge singleton and doubleton clusters with
their nearest cluster, return the ﬁnal clustering (Color ﬁgure online)

A Fast Prize-Collecting Steiner Forest Algorithm
267
PCSF has proven more apt for the problem of highlighting disease-relevant
networks [19,20] than PCST because in a tumorous cell, for example, multiple
functional pathways may be simultaneously active and dysregulated, but non-
overlapping. If we seek to ﬁnd a tree, we are forced to incorporate spurious edges
in our solution. If we seek to ﬁnd a set of disconnected trees, however, we only
select the edges needed to connect the high-prize terminals.
Algorithm 1. MST-PCSF, Clustering phase
Initizalization:
Set U = {v : p(v) > 0}, the set of ‘unassigned terminals’.
Set A = ∅, the set of ‘assigned terminals’.
Vector C such that |C| = |U| and Ci ←0 for all i : 1...U
Matrix D ∈R|U|×|U| such that Di,j is the weighted distance of the shortest path
from vi to vj for all pairs vi, vj ∈U
clusterID ←0
Algorithm
while U is not ∅do
clusterID ←clusterID +1;
Remove arbitrary vertex vi from U and insert it into A.
while A is not ∅do
Remove arbitrary vertex va from A.
Ca = ClusterID
for each vu ∈U do
if (Da,u < p(va) & Da,u < p(vu) then
Remove vu from U and insert it into A.
end if
end for
end while
end while
for each singleton and doubleton cluster Gk = (Vk, Ek) do
Let Gmink = min 
vi∈Vk

vj /∈Vk Dij, the closest subgraph to Gk.
Consolidate Gk with the nearest cluster Gmink
end for
Output terminals set A, assignment vector C representing the ﬁnal clustering.
The pseudocode of the heuristic clustering phase is shown in Algorithm 1.
The clustering is performed by considering the pairwise relationship of termi-
nal nodes. Given an input network (Fig. 1A), the algorithm computes all-pairs
shortest path distance matrix Dij among the terminal nodes (Fig. 1B). Two
terminal nodes i and j are clustered together if they satisfy the strict cluster-
ing criterion: Di,j < p(i) & Di,j < p(j), which worked best of those we tested
for biological networks. At the ﬁrst iteration, the algorithm arbitrarily selects a
terminal node, evaluates other terminals satisfying the clustering criterion with
respect to the selected node, and assigns them into the same cluster (Fig. 1C).

268
M. Akhmedov et al.
Each terminal node newly added into the cluster is iteratively analyzed according
to the clustering criterion, and additional terminal nodes satisfying the criterion
are incorporated into the cluster. This process corresponds to the growth phase
of the cluster (Fig. 1D). The heuristic obtains the ﬁrst cluster when there is no
more terminal node outside of that cluster which satisﬁes the clustering crite-
rion (Fig. 1E). Then, the algorithms continues to build new clusters (Fig. 1H) by
repeating the same steps in (Fig. 1C–E), and it is terminated when all terminal
nodes of the input network are clustered (Fig. 1G).
After clustering the graph, singleton and doubleton clusters are merged with
their nearest neighbor clusters (Fig. 1H), since those subgraphs harbor very little
biological information on their own.
3.2
Solving MST on an Altered Representation of the Input Graph
Bypasses the Complexity of PCSF
In the second phase of the algorithm, we bypass the complexity of PCSF by
ﬁnding instead the tree covering every terminal node with minimum total edge
costs, which is equivalent to solve the MST. The pseudocode of the second phase
is shown in Algorithm 2.
Fig. 2. MST-Phase of MST-PCSF. (A) Construct the complete graph from the
terminal nodes in the clustered subgraphs. (B) Add an artiﬁcial root node with an
edge from that root to every other nodes with cost ω. or use the already-existing node
speciﬁed as the root by the user. (C) Determine the MST of this augmented graph
by Prim’s algorithm. (D) Interpolate the Steiner nodes from the original graph. (E)
Prune all leaves for which the prize of the leaf is less than the cost of the edge. (F)
Output a forest of subgraphs for further investigation.

A Fast Prize-Collecting Steiner Forest Algorithm
269
The altered representation of the original network is the set of complete sub-
graphs composed exclusively of terminals from each cluster, in which each edge is
weighted with the shortest path distance between the terminal nodes (Fig. 2A).
We can solve MST on this representation quickly (Fig. 2C) and then project the
solution back into the original graph (Fig. 2D), and ﬁnally, disconnecting nodes
too expensive to retain in the solution (Fig. 2E).
Algorithm 2. MST-PCSF, MST phase
Initizalization:
For each subgraph Gs, construct the complete subgraph of the terminal nodes in
Gs called G′
s=(V ′
s, E′
s), such that each edge ∈E′
s is weighted with the cost of the
shortest path between the terminals it connects in Gs.
Algorithm
Add a vertex called Root and edges from Root to each terminal with cost ω.
This produces a new graph which is the union of each of the G′
s subgraphs and Root.
Find the minimum spanning tree of the new graph to obtain MST = (Vmst, Emst)
Interpolate the omitted Steiner nodes in the edges of MST from G
for each leaf node v ∈MST and associated parent node v′ do
if p(v) < connection cost then
remove v from MST ;
end if
end for
Remove Root from MST leaving behind a forest of subgraphs we call F.
Output F, an approximate solution to the PSCF problem on graph G.
The algorithmic steps in (Fig. 2A–D) can be repeated several times in order
to decrease the tree cost further, by adding the Steiner nodes incorporated in
the solution tree (Fig. 2-D) into the cliques constructed in the next iteration
(Fig. 2-A), yielding better results at each iteration until convergence. However,
we only perform a single iteration of those steps for our results in this work.
4
Results
We compare the performances of MST-PCSF and the message passing algorithm
(MSGP), a broadly used heuristic algorithm for PCST and PCSF. The MSGP
algorithm has been used to predict unknown protein associations [20], ﬁnd hid-
den components of regulatory networks [22], and reconstruct cell-signaling path-
ways [21]. We test the performances of these algorithms on small benchmark
instances from the literature, as well as medium and large networks generated
from real biological data. We use the default parameters for MSGP, except the
reinforcement parameter g, which is set to 0.001 as in [21]. The computational
studies are performed on a server equipped with an AMD Opteron(tm) Proces-
sor 6320 and shared memory of 256 GB. A single core is used while running the

270
M. Akhmedov et al.
Table 1. Performances of MST-PCSF and MSGP for the D instances [21]. The perfor-
mance of the message passing algorithm [20] and the proposed heuristic are displayed
under MSGP and MST-PCSF for ω = {5, 8}. We report the upper bounds obtained
from the methods under OBJ column. The running times of the methods are provided
in seconds under t(s) column. We provide the average statistics of 10 runs obtained by
both methods for each instance.
Instance
V
E
T
ω = 5
ω = 8
MSGP
MST-PCSF
MSGP
MST-PCSF
OBJ
t(s)
OBJ
t(s)
OBJ
t(s)
OBJ
t(s)
D01-A
1001
1255
5
21
0.44
21
0.03
26
0.45
26
0.03
D01-B
1001
1255
5
25
0.44
25
0.03
40
0.44
40
0.03
D02-A
1001
1260
10
46
0.45
46
0.04
58
0.44
58
0.04
D02-B
1001
1260
10
50
0.45
50
0.05
80
0.44
80
0.04
D03-A
1001
1417
83
630
0.66
630
0.52
787
0.7
794
0.51
D03-B
1001
1417
83
782
0.65
782
0.53
1131
0.68
1155
0.52
D04-A
1001
1500
250
928
0.63
937
0.81
1164
0.63
1170
0.79
D04-B
1001
1500
250
1129
0.66
1137
0.84
1574
0.8
1596
0.84
D05-A
1001
1750
500
1774
0.84
1791
1.86
2130
0.82
2195
1.74
D05-B
1001
1750
500
2108
0.77
2123
2.05
2787
0.82
2839
2.06
D06-A
1001
2005
5
21
0.61
21
0.05
26
0.61
26
0.04
D06-B
1001
2005
5
25
0.61
25
0.04
40
0.61
40
0.04
D07-A
1001
2010
10
46
0.67
46
0.06
58
0.67
58
0.06
D07-B
1001
2010
10
50
0.67
50
0.06
80
0.68
80
0.06
D08-A
1001
2167
83
630
0.82
634
0.62
763
0.89
789
0.61
D08-B
1001
2167
83
753
0.84
760
0.64
994
1.54
1023
0.64
D09-A
1001
2250
250
918
1.75
930
0.95
1080
0.95
1126
0.94
D09-B
1001
2250
250
1099
1.28
1116
1.01
1367
1.03
1414
1.01
D10-A
1001
2500
500
1569
1.2
1601
2.3
1705
1.36
1747
2.29
D10-B
1001
2500
500
1812
1.12
1836
2.38
2058
1.25
2078
2.37
D11-A
1001
5005
5
21
1.52
21
0.07
26
4.09
26
0.07
D11-B
1001
5005
5
24
1.56
24
0.07
36
4.09
36
0.07
D12-A
1001
5010
10
42
1.61
42
0.1
50
3.61
50
0.1
D12-B
1001
5010
10
43
1.55
44
0.1
50
2.54
50
0.1
D13-A
1001
5167
83
454
4.7
469
1.06
463
2.46
477
1.07
D13-B
1001
5167
83
501
2.42
516
1.06
510
3.09
526
1.07
D14-A
1001
5250
250
615
3.32
642
1.62
624
3.86
651
1.64
D14-B
1001
5250
250
673
4.16
697
1.62
690
2.09
712
1.64
D15-A
1001
5500
500
1057
3.68
1072
3.58
1076
2.41
1085
3.65
D15-B
1001
5500
500
1124
2.66
1140
3.56
1147
2.69
1154
3.64
D16-A
1001
25005
5
18
31.16
20
0.27
22
24.65
22
0.26
D16-B
1001
25005
5
19
23.26
20
0.28
22
26.69
23
0.27
D17-A
1001
25010
10
28
28.83
29
0.37
31
20.98
33
0.37
D17-B
1001
25010
10
28
29.41
29
0.37
31
31.95
32
0.38
D18-A
1001
25167
83
225
17.53
247
3.81
231
30.31
250
3.81
D18-B
1001
25167
83
233
20.83
256
3.81
238
24.62
259
3.81
D19-A
1001
25250
250
314
17.69
346
5.67
321
33.38
348
5.69
D19-B
1001
25250
250
320
31.16
353
5.68
323
30.83
356
5.67
D20-A
1001
25500
500
542
31.36
547
11.56
545
28.63
549
11.53
D20-B
1001
25500
500
543
21.46
548
11.6
545
34.55
551
11.52
mean
531
7.39
541
1.78
623
8.33
638
1.78

A Fast Prize-Collecting Steiner Forest Algorithm
271
Table 2. The comparison results of the methods for the Breast Cancer network
instances generated based on phosphoproteomic data in [23]. The performance of the
message passing algorithm [20] and the proposed heuristic are displayed under MSGP
and MST-PCSF for ω = {1, 2}. The average statistics of 10 runs provided by both
algorithms are reported for each instance.
Instance
V
E
T
ω = 1
ω = 2
MSGP
MST-PCSF
MSGP
MST-PCSF
OBJ
t(s)
OBJ
t(s)
OBJ
t(s)
OBJ
t(s)
A2-A0CM
36892
1016411
122
35.71
1726
35.64
130
36.69
1189
36.89
128
A2-A0D2
36892
1016411
226
66.38
1838
66.50
234
67.53
1987
67.50
231
A2-A0EV
36892
1016411
69
18.29
1482
18.35
78
19.55
1844
19.60
77
A2-A0EY
36892
1016411
118
40.56
2107
40.70
127
41.77
2273
41.91
128
A2-A0SW
36892
1016411
60
14.45
1655
14.50
69
15.45
1637
15.50
68
A2-A0T6
36892
1016411
92
30.92
1697
31.05
100
32.18
1737
32.31
101
A2-A0YC
36892
1016411
182
48.04
1766
48.17
188
49.04
1761
49.17
190
A2-A0YD
36892
1016411
55
17.92
1593
17.92
64
18.92
1656
18.92
63
A2-A0YF
36892
1016411
165
48.31
1783
48.27
174
49.54
847
49.65
177
A2-A0YM
36892
1016411
236
61.89
2080
62.07
245
62.91
1707
63.19
243
A7-A0CE
36892
1016411
142
38.35
1604
38.42
150
39.61
1719
39.68
149
A7-A13F
36892
1016411
139
43.42
1049
43.57
148
44.42
1471
44.57
145
A8-A06Z
36892
1016411
112
36.24
1394
36.42
119
37.76
1524
37.94
121
A8-A079
36892
1016411
77
25.77
1609
25.96
85
26.77
1691
26.96
87
A8-A09G
36892
1016411
186
46.65
1985
46.61
193
47.61
2229
47.73
196
AN-A04A
36892
1016411
208
63.40
1854
63.41
215
64.58
1916
64.41
216
AN-A0FK
36892
1016411
81
21.82
1582
21.89
89
22.99
1404
23.06
88
AN-A0FL
36892
1016411
126
35.63
1677
35.87
136
36.75
1782
36.99
134
AO-A0JC
36892
1016411
76
22.32
1734
22.39
85
23.32
1843
23.39
83
AO-A0JE
36892
1016411
116
31.60
2362
31.70
124
32.88
2033
32.97
124
AO-A0JM
36892
1016411
216
59.34
2563
59.61
225
60.79
2248
61.09
226
AO-A126
36892
1016411
51
16.26
1708
16.30
59
17.26
1835
17.30
59
AO-A12B
36892
1016411
148
43.94
2101
43.94
157
45.88
1613
45.72
155
AO-A12D
36892
1016411
211
54.94
1671
55.09
221
56.00
1857
56.18
218
AO-A12E
36892
1016411
146
39.68
2178
39.78
154
40.98
2238
40.81
155
AO-A12F
36892
1016411
167
44.88
2150
45.16
176
46.02
2054
46.30
176
AR-A0TR
36892
1016411
120
32.08
1920
32.28
127
33.08
1771
33.28
128
AR-A0TT
36892
1016411
164
43.05
1755
43.33
174
44.22
1826
44.50
172
AR-A0TV
36892
1016411
201
57.08
2453
57.27
210
58.20
3565
58.44
209
AR-A0TX
36892
1016411
169
46.36
2194
46.47
177
47.73
2171
47.74
176
AR-A0U4
36892
1016411
154
41.16
2111
41.39
163
42.48
2065
42.70
160
AR-A1AP
36892
1016411
109
32.49
1568
32.50
117
33.52
1754
33.54
117
AR-A1AS
36892
1016411
181
48.69
2344
48.91
188
49.93
2245
50.09
191
AR-A1AW
36892
1016411
116
31.48
1616
31.72
128
32.65
1849
32.89
127
BH-A0AV
36892
1016411
137
40.06
1983
40.20
144
41.84
1685
41.98
146
BH-A0BV
36892
1016411
220
57.43
1755
57.58
229
58.52
1466
58.59
228
BH-A0DG
36892
1016411
209
62.49
1757
62.58
219
63.56
2020
63.70
218
BH-A0E9
36892
1016411
107
35.57
1808
35.45
114
36.35
1339
36.45
115
BH-A18N
36892
1016411
176
47.57
1608
47.58
183
48.73
2310
48.87
181
BH-A18U
36892
1016411
72
23.87
1627
23.91
81
24.87
1428
24.91
79
C8-A12T
36892
1016411
87
20.25
1646
20.31
95
21.51
1745
21.57
96
C8-A12U
36892
1016411
87
27.03
1561
27.15
95
28.02
1576
28.15
94
C8-A12V
36892
1016411
94
26.38
2330
26.54
101
27.38
2267
27.54
101
C8-A130
36892
1016411
84
26.88
1229
27.06
93
27.88
1363
28.06
92
C8-A131
36892
1016411
187
53.48
1753
53.77
198
54.57
2177
54.80
197
C8-A138
36892
1016411
191
51.18
1930
51.14
199
52.13
1792
52.19
199
D8-A142
36892
1016411
135
41.35
1539
41.20
143
42.03
1142
42.20
142
E2-A154
36892
1016411
145
42.36
1770
42.50
152
43.47
1715
43.61
152
E2-A158
36892
1016411
169
49.12
2135
49.18
175
50.37
2117
50.38
176
E2-A15A
36892
1016411
176
51.55
1599
51.80
182
52.58
1601
52.83
183
mean
39.91
1819
40.02
149
41.06
1822
41.18
148

272
M. Akhmedov et al.
Table 3. Performances of MST-PCSF and MSGP for the Glioblastoma network
instances generated from phosphoproteomic data from [21]. The ﬁrst four columns
provide instance name, number of total nodes, edges, and terminals for each network.
The performance of the message passing algorithm [20] and the proposed heuristic are
displayed under MSGP and MST-PCSF for ω = {1, 2}. We report the upper bounds
obtained from the methods under OBJ column. The running times of the methods
are provided in seconds under t(s) column. We report the average statistics of 10 runs
obtained by both algorithms for each instance.
Instance
V
E
T
ω = 1
ω = 2
MSGP
MST-PCSF
MSGP
MST-PCSF
OBJ
t(s)
OBJ
t(s)
OBJ
t(s)
OBJ
t(s)
GBM6
15357 175792 108 26.35 305 26.41
22
28.61 262 28.66
22
GBM8
15357 175792 115 25.45 246 25.50
23
27.45 211 27.50
23
GBM10
15357 175792 132 32.85 247 32.91
26
36.96 206 37.02
26
GBM12
15357 175792 135 32.50 240 32.58
27
35.03 253 35.11
27
GBM15
15357 175792 131 30.49 248 30.58
26
33.62 256 33.70
26
GBM26
15357 175792 122 28.42 274 28.46
24
31.02 269 31.04
24
GBM39
15357 175792 123 29.51 170 29.56
24
31.68 170 31.72
24
GBM59
15357 175792 123 26.54 258 26.53
24
29.56 275 29.55
24
GBM-pool 15357 175792 161 42.90 299 42.98
31
50.90 267 50.97
31
mean
30.56 254 30.61
25
33.87 241 33.92
25
experiments. We run the algorithms for values of ω = {1, 2} for medium and
large instances, and we used ω = {5, 8} for small instances due to their higher
edge costs. In order to have a fair and robust comparison baseline, we report the
average statistics of 10 runs provided by both algorithms for each instance.
The ﬁrst of these domains is a set of benchmark instances for PCST algo-
rithms from the literature [11] called the D instance set, which is composed of
smaller networks of roughly 1,000 nodes and 25,000 edges. However, since the
algorithms we compare solve PCSF, we modify the D instances by adding a root
node beforehand, and specify to use that node as their root. The computational
performances of the algorithms are displayed in Table 1. For these small inputs,
MSGP provides a slightly better upper bounds for both values of ω. On the
other hand, MST-PCSF outperforms MSGP in average running time.
The second domain is phosphoproteomic data from the Glioblastoma patients
in [21]. These graphs are obtained by using diﬀerentially expressed genes as
terminals by mapping them onto the human PPI network. As our PPI, we use
STRING (version13), in which the network edges have a experimentally-derived
conﬁdence score s(e) [1]. The low conﬁdence edges with s(e) < 0.5 are removed
to improve the reliability of the ﬁndings. We convert edge conﬁdence into edge
cost: c(e) = max(0.01, 1 −s(e)). The performance statistics of the methods are
reported in Table 3. For these medium sized networks, the upper bounds provided

A Fast Prize-Collecting Steiner Forest Algorithm
273
Fig. 3. A visual representation of the relationship between the solutions obtained by
the MST-PCSF and MSGP algorithms on Gliablastoma patient data.
by MST-PCSF and MSGP are comparable for both values of ω. However, MST-
PCSF has an order of magnitude speed up in running time on average.
Our largest graphs are generated by mapping the phosphoproteomic data
from the Breast Cancer patients in [23] onto an integrated interactome of proteins
and metabolites, resulting in networks with 36,897 nodes and 1,016,288 edges.
Here as well, each network represents a single patient. The results provided by
both algorithms are displayed in Table 2. For these large network instances, MST-
PCSF provided very similar quality results compared to MSGP. In addition,
MST-PCSF is approximately ten times faster in execution time on average.

274
M. Akhmedov et al.
The primary reason for MST-PCSF providing slightly worse solutions in gen-
eral could be the value of its simplicity: it considers only pairwise distance rela-
tion of terminal nodes while clustering, and solving the MST afterwards. Nev-
ertheless, this simplicity leads to tenfold running time speed up in large real
biological graphs. For example, the average running time of MSGP for a sin-
gle graph instance generated from the Breast Cancer patient is around half an
hour. In order to perform robust biological inferences out of network, it is rec-
ommended to solve the PCSF iteratively on the same graph while introducing
some noise to edge costs and node prizes at each iteration. Higher the number of
iterations, more robust the inference output is, which requires tremendous com-
putational time. Therefore, we think that MST-PCSF can be useful to analyze
large biological networks in this context.
Finally, we compare the results provided by MST-PCSF and MSGP in
Glioblastoma patient networks for ω = 1. We excluded the UBC gene from
the interactome due to its high node degree. We took the union of output forests
for these 9 instances for each algorithm. MST-PCSF provided a subgraph with
269 nodes and 301 edges and MSGP provided a subgraph of 286 nodes and 364
edges. We merged the subgraphs into one network, demonstrating the overlap
between the solutions (Fig. 3). The methods obtained solutions with 255 com-
mon nodes and 251 common edges. MST-PCSF recovered 89% of nodes and
83% of edges of the solution provided by MSGP. This result demonstrates that
our algorithm does not merely recover similar quality solutions, but in fact, very
similar solutions.
5
Conclusion
We present a new heuristic algorithm for the Prize-collecting Steiner Forest prob-
lem which supersedes the existing algorithms of which we are aware, particularly
on larger-scale graphs common in the application-space of biology. The PCSF
approach is well suited to the problem of predicting disease-relevant subnetworks
from an interactome conditional on observed data gathered from patients. Our
algorithm reduces the requisite computing time to solve PCSF which expedites
existing research and also provides the capacity to analyze these data patient-by-
patient, an operation which previously has been prohibitively computationally
expensive.
Our algorithm accelerates the pace of relevant subnetwork imputation, which
we hope will be a boon for all who apply the Prize-collecting Steiner Forest
approach, in biology and elsewhere.
Acknowledgments. M.A. was supported by the Swiss National Science Foundation
through the project 205321-147138/1: Steiner trees for functional analysis in cancer
system biology. M.A. (partially) and A.L. were supported by the National Institute of
Health through the project U54-NS-091046 and U01-CA-184898.

A Fast Prize-Collecting Steiner Forest Algorithm
275
References
1. Szklarczyk, D., Franceschini, A., Kuhn, M., Simonovic, M., Roth, A., Minguez, P.,
Doerks, T., Stark, M., Muller, J., Bork, P., Jensen, L.J., van Mering, C.: STRING
8 - a global view on proteins and their functional interactions in 630 organisms.
Nucleic Acids Res. 37, D412–D416 (2011)
2. Bienstock, D., Goemans, M.X., Simchi-Levi, D., Williamson, D.: A note on the
prize-collecting traveling salesman problem. Math. Program. 59, 413–420 (1993)
3. Goemans, M.X., Williamson, D.P.: The primal-dual method for approximation
algorithms and its application to network design problems. In: Approximation
Algorithms for NP-Hard Problems, pp. 144–191 (1996)
4. Johnson, D.S., Minkoﬀ, M., Phillips, S.: The prize-collecting Steiner tree problem:
theory and practice. In: Proceedings of 11th ACM-SIAM Symposium on Discrete
Algorithms, pp. 760–769 (2000)
5. Feoﬁloﬀ, P., Fernandes, C.G., Ferreira, C.E., Pina, J.C.: Primal-dual approximation
algorithms for the prize-collecting Steiner tree problem. Inf. Process. Lett. 103(5),
195–202 (2007)
6. Ljubic, I., Weiskircher, R., Pferschy, U., Klau, G., Mutzel, P., Fischetti, M.: Solving
the prize-collecting Steiner tree problem to optimality. In: Seventh Workshop on
Algorithm Engineering and Experiments, pp. 68–76 (2005)
7. Ljubic, I., Weiskircher, R., Pferschy, U., Klau, G., Mutzel, P., Fischetti, M.: An
algorithmic framework for the exact solution of the prize-collecting Steiner tree
problem. Math. Progam. 105(2), 427–449 (2006)
8. Bechet, M.B., Bradde, S., Braunstein, A., Flaxman, A., Foini, L., Zecchina, R.:
Clustering with shallow trees. J. Stat. Mech. 2009, 12010 (2009)
9. Haouari, M., Layeb, S.B., Sherali, H.D.: Algorithmic expedients for the prize col-
lecting Steiner tree problem. Discrete Optim. 7, 32–47 (2010)
10. Cunha, A.S., Lucena, A., Maculan, N., Resende, M.G.C.: A relax-and-cut algo-
rithm for the prize-collecting Steiner problem in graphs. Discrete Appl. Math.
157, 1198–1217 (2009)
11. Canuto, S.A., Resende, M.G.C., Ribeiro, C.C.: Local search with perturbation for
the Prize-collecting Steiner tree problem in graphs. Networks 38, 50–58 (2001)
12. Chapovska, O., Punnen, A.P.: Variations of the prize-collecting Steiner tree prob-
lem. Networks 47(4), 199–205 (2006)
13. Beasley, J.E.: An SST-based algorithm for the Steiner problem in graphs. Networks
19, 1–16 (1989)
14. Duin, C.W., Volgenant, A.: Some generalizations of the Steiner problem in graphs.
Networks 17(2), 353–364 (1987)
15. Fischetti, M.: Facets of two Steiner arborescence polyhedra. Math. Program. 51,
401–419 (1991)
16. Lucena, A., Resende, M.G.C.: Strong lower bounds for the prize-collecting Steiner
problem in graphs. Discrete Appl. Math. 141, 277–294 (2004)
17. Fu, Z.H., Hao, J.K.: Knowledge guided tabu search for the prize collecting Steiner
tree problem in graphs. In: 11th DIMACS Challenge Workshop (2014)
18. Klau, G.W., Ljubi´c, I., Moser, A., Mutzel, P., Neuner, P., Pferschy, U., Raidl,
G., Weiskircher, R.: Combining a memetic algorithm with integer programming
to solve the prize-collecting Steiner tree problem. In: Deb, K. (ed.) GECCO
2004. LNCS, vol. 3102, pp. 1304–1315. Springer, Heidelberg (2004). doi:10.1007/
978-3-540-24854-5 125

276
M. Akhmedov et al.
19. Dittrich, M.T., Klau, G.W., Rosenwald, A., Dandekar, T., Mueller, T.: Identify-
ing functional modules in proteinprotein interaction networks: an integrated exact
approach. Bioinformatics 26, 223–231 (2008)
20. Bechet, M.B., Borgs, C., Braunstein, A., Chayesb, J., Dagkessamanskaia, A., Fra-
nois, J.M., Zecchina, R.: Finding undetected protein associations in cell signaling
by belief propagation. PNAS 108, 882–887 (2010)
21. Tuncbag, N., Braunstein, A., Pagnani, A., Huang, S.C., Chayes, J., Borgs, C.,
Zecchina, R., Fraenkel, E.: Simultaneous reconstruction of multiple signaling path-
ways via the prize-collecting Steiner forest problem. J. Comput. Biol. 20(2), 124–
136 (2013)
22. Tuncbag, N., McCallum, S., Huang, S.C., Fraenkel, E.: SteinerNet: a web server
for integrating omic data to discover hidden components of response pathways.
Nucleic Acids Res. 40, 1–5 (2012)
23. Mertins, P., Mani, D.R., Ruggles, K.V., Gillette, M.A., Clauser, K.R., Wang,
P., Wang, X., Qiao, J.W., Cao, S., Petralia, F., Kawaler, E., Mundt, F., Krug,
K., Tu, Z., Lei, J.T., Gatza, M.L., Wilkerson, M., Perou, C.M., Yellapantula,
V., Huang, K., Lin, C., McLellan, M.D., Yan, P., Davies, S.R., Townsend, R.R.,
Skates, S.J., Wang, J., Zhang, B., Kinsinger, C.R., Mesri, M., Rodriguez, H., Ding,
L., Paulovich, A.G., Feny, D., Ellis, M.J., Carr, S.A., NCI CPTAC: Proteoge-
nomics connects somatic mutations to signalling in breast cancer. Nature 534,
55–62 (2016)
24. Akhmedov, M., Kwee, I., Montemanni, R.: A divide and conquer matheuristic
algorithm for the prize-collecting Steiner tree problem. Comput. Oper. Res. 70,
18–25 (2016)
25. Akhmedov, M., Kwee, I., Montemanni, R.: A fast heuristic for the prize-collecting
Steiner tree problem. Lect. Not. Manag. Sci. 6, 207–216 (2014)
26. Prim, R.C.: Shortest connection networks and some generalizations. Bell Syst.
Techn. J. 36(6), 1389–1401 (1957)

Scenario-Based Learning for Stochastic
Combinatorial Optimisation
David Hemmi1,2(B), Guido Tack1,2(B), and Mark Wallace1(B)
1 Faculty of IT, Monash University, Melbourne, Australia
{david.hemmi,guido.tack,mark.wallace}@monash.edu
2 Data61/CSIRO, Melbourne, Australia
Abstract. Combinatorial optimisation problems often contain uncer-
tainty that has to be taken into account to produce realistic solutions.
This uncertainty is usually captured in scenarios, which describe diﬀerent
potential sets of problem parameters based on random distributions or his-
torical data. While eﬃcient algorithmic techniques exist for speciﬁc prob-
lem classes such as linear programs, there are very few approaches that can
handle general Constraint Programming formulations with uncertainty.
This paper presents a generic method for solving stochastic combinato-
rial optimisation problems by combining a scenario-based decomposition
approach with Lazy Clause Generation and strong scenario-independent
nogoods over the ﬁrst stage variables. The algorithm can be implemented
based on existing solving technology, is easy to parallelise, and is shown
experimentally to scale well with the number of scenarios.
1
Introduction
Reasoning under uncertainty is important in combinatorial optimisation, since
uncertainty is inherent to many problems in an industrial setting. An example
of this especially hard class of optimisation problems is the generalised assign-
ment problem [2], where jobs with an uncertain duration have to be assigned
to computers in a network, or the assignment of tasks to entities when develop-
ing a project plan. Other examples include the stochastic Steiner tree problem
with uncertainty in the set of terminals [12]. At ﬁrst, edges in the graph can be
purchased at a low price. Once the set of terminals is revealed, the Steiner tree
must be completed by purchasing edges at an increased price. Various forms of
the stochastic set covering problem [11] and stochastic vehicle routing problems
[21] are of similar form.
The focus of this paper is on stochastic optimisation problems with a combi-
natorial structure. The random variables describing the uncertainty have ﬁnite
support. Each random variable has an underlying discrete probability distribu-
tion. A scenario describes the stochastic problem when all the random variables
are ﬁxed. Each scenario has a probability of occurrence. Stochastic problems
are composed of a ﬁrst and subsequent stages, the simplest being a two stage
problem. The ﬁrst stage denotes the problem before information about the ran-
dom variables is revealed and ﬁrst stage decisions are taken with respect to all
scenarios. Second stage decisions are made once the random variables are ﬁxed.
c
⃝Springer International Publishing AG 2017
D. Salvagnin and M. Lombardi (Eds.): CPAIOR 2017, LNCS 10335, pp. 277–292, 2017.
DOI: 10.1007/978-3-319-59776-8 23

278
D. Hemmi et al.
In constraint programming (CP), modeling frameworks that allow solver
agnostic modelling have been developed. Examples include MiniZinc [15] and
Essence [10]. Problems described in one of these languages can be solved with
a range of CP, MIP or SAT solvers without the user having to tailor the model
for speciﬁc solvers. Stochastic MiniZinc [17] is an extension of the MiniZinc
modelling language that supports uncertainty. Stochastic MiniZinc enables mod-
ellers to express combinatorial stochastic problems at a high level of abstraction,
independent of the stochastic solving approach. Stochastic MiniZinc uses the
standard language set of MiniZinc and admits models augmented with annota-
tions describing the ﬁrst and second stage. Stochastic MiniZinc automatically
transforms the model into a structure that can be solved by standard MiniZinc
solvers. At present, stochastic MiniZinc translates a two-stage model into the
deterministic equivalent and uses a standard CP, MIP or SAT solver for the
search. However, standard solver technology cannot exploit the special model
structure of scenario based stochastic optimisation problems. As a result, the
search performance is poor and it is desirable to develop a solver that can handle
this class of problems without requiring the modeler to have expert knowledge
in stochastic optimisation.
The literature on stochastic optimisation problems containing integer vari-
ables is diverse, see ([1,4,6,13,18]). Previous works have been concerned with
problems that have a speciﬁc mathematical structure, for example linear models
with integrality requirements. Furthermore, these methods are not available as
standalone solvers and it is not possible to easily transform a model deﬁned in
a CP framework into a form that can be solved by one of these algorithms, in
particular when a CP model contains non-linear constraints that would require
(potentially ineﬃcient) linearisation. In addition, methods to solve stochastic
constraint satisfaction problems, inspired by the stochastic satisﬁability prob-
lem, have been introduced, see ([3,14]) for an overview.
This paper proposes an algorithm to solve combinatorial stochastic optimisa-
tion problems. The work is based on Lazy Clause Generation (LGC) and trans-
lates the scenario decomposition algorithm for 0–1 stochastic programs introduced
by Ahmed [1] into the CP setting. Our main contributions are the following: ﬁrst,
a search strategy that in combination with Ahmed’s algorithm produces scenario-
independent nogoods; secondly, an eﬀective use of nogoods in the sub-problems
when using decomposition algorithms; thirdly, a scenario bundling method to fur-
ther improve the performance. Also, the benchmark instances we are using have
been made publicly available on the CSPLib.
In contrast to the methods in the literature, the proposed algorithm can
be used directly as a back-end solver to address problems formulated in
a CP framework. No sophisticated reformulation of the model nor problem
speciﬁc decompositions are required. The paper concludes with an experiment
section.

Scenario-Based Learning for Stochastic Combinatorial Optimisation
279
2
Background and Related Work
This section introduces the basic notation, and then discusses relevant literature.
2.1
Basic Deﬁnitions
We will base our deﬁnition of stochastic problems on the deterministic case.
Deﬁnition 1. A (deterministic) constraint optimisation problem (COP)
is a four-tuple P deﬁned as follows:
P = < V, D, C, f >
where V is a set of decision variables, D is a function mapping each element of V
to a domain of potential values, and C is a set of constraints. A constraint c ∈C
acts on variables xi, ..., xj, termed scope(c) and speciﬁes mutually-compatible
variable assignments σ from the Cartesian product D(xi) × ... × D(xj). The
quality of a solution is measured using the objective function f. We write σ(x)
for the value of x in assignment σ; σ|X for σ restricted to the set of variables
X; and σ ∈D to mean ∀x : σ(x) ∈D(x). We deﬁne the set of solutions of
a COP as the set of assignments to decision variables from the domain D that
satisﬁes all constraints in C:
sol(P) = {σ ∈D | ∀c ∈C : σ|scope(c) ∈c}
Finally, an optimal solution is one that minimises the objective function:
min f(σ)
σ∈sol(P )
On the basis of a COP we can deﬁne a Stochastic Constraint Optimisation
Problem (SCOP). We restrict ourselves to two-stage problems, although the
deﬁnitions and algorithms can be generalised to multi-stage problems. In a two-
stage SCOP, a set of decisions, the ﬁrst stage decisions, is taken before the values
for the random variables are known. Once the random variables are ﬁxed, further
decisions are made for the subsequent stages. To enable reasoning before the
random variables are revealed, we assume that their values can be characterised
by a ﬁnite set of scenarios, i.e., concrete instantiations of the random variables
based on historical data or forecasting. Our task is then to make ﬁrst stage
decisions that are optimal on average over all scenarios.
The idea behind our deﬁnition of an SCOP is to regard each scenario as
an individual COP (ﬁxing all random variables), with the additional restriction
that all scenario COPs share a common set of ﬁrst stage variables V . The goal
is to ﬁnd solutions to the scenarios that agree on the ﬁrst stage variables, and
for which the weighted average over all scenario objective values is minimal.

280
D. Hemmi et al.
Deﬁnition 2. A stochastic constraint optimisation problem (SCOP) is a
tuple as follows:
P
= < V, P1, . . . , Pk, p1, . . . , pk >
with Pi = < Vi, Di, Ci, fi >
∀i ∈1..k : V ⊆Vi
where each Pi is a COP, each pi is the weight (e.g. the probability) of scenario
i, and the set V is the set of ﬁrst stage variables shared by all Pi.
The set of solutions of an SCOP is deﬁned as tuples of solutions of the Pi
that agree on the variables in V :
sol(P) = {σ|σ =< σi, .., σk >, σi ∈sol(Pi),
σi(x) = σj(x)
∀i, j ∈1..k, x ∈V }
An optimal solution to an SCOP minimises the weighted sum of the indi-
vidual objectives:
min
σ∈sol(P )
k
i=1 pifi(σi)
Multiple methods to solve stochastic programs have been developed in the past.
The most direct option is to formulate the SCOP as one large (deterministic)
COP, called the deterministic equivalent (DE). Starting from Deﬁnition 2, the
DE is constructed simply by taking the union of all the Pi (assuming an appro-
priate renaming of the second stage variables), and using the weighted sum as the
objective function. The number of variables and constraints in the DE increases
linearly with the number of scenarios. However, in case of a combinatorial second
stage, the solving time may increase exponentially. As a result the DE approach
lacks scalability.
An alternative to the DE is to decompose the SCOP, in one of two ways.
Firstly, the problem can be relaxed by time stages. In the vertical decomposi-
tion a master problem describes the ﬁrst stage and contains an approximation
of the second stage. A set of sub-problems captures the second stages for each
scenario. Complete assignments to the master problem are evaluated against the
sub-problems. The L-shaped method [4], which is similar to Benders decompo-
sition, is an example for an algorithm that works on the basis of the vertical
decomposition. However, the L-Shaped method is limited to linear, continuous
SCOPs, with extensions available for problems with integrality requirements.
Secondly, the problem can be decomposed horizontally, by scenarios. The rest
of this paper is based on this scenario decomposition, which is introduced in the
next section.
2.2
Scenario-Based Decomposition
A straightforward decomposition for SCOPs is the scenario decomposition, where
the Pi of an SCOP are treated as individual problems. A feasible solution to the
stochastic program requires the shared variables to agree across all scenarios.

Scenario-Based Learning for Stochastic Combinatorial Optimisation
281
To ensure feasibility, an additional consistency constraint σi(x) = σj(x)∀i, j ∈
1 . . . k, x ∈V is required. Note how this constraint was built into our deﬁnition
of SCOP, but now has become an external requirement that can be relaxed.
To make this more formal, we introduce the notion of a pre-solution, which
represents tuples of individual scenario solutions that may violate the consistency
constraint.
Deﬁnition 3. A pre-solution to a SCOP P is a tuple
pre sol(P) = {σ|σ =< σi, .., σk >, σi ∈sol(Pi), ∀i ∈1..k}
We can redeﬁne the set of solutions as exactly those pre-solutions that agree on
the ﬁrst stage variables:
sol(P) = {σ|σ ∈pre sol(P) ∧σi(x) = σj(x) ∀i, j ∈1 . . . k, x ∈V }
Algorithms based on the scenario decomposition generate pre-solutions and iter-
atively enforce convergence (i.e., consistency) on the ﬁrst stage variables V , using
diﬀerent methods as described below. Any converged pre-solution is a feasible
solution of the SCOP, and yields an upper bound on the stochastic objective
value. As pre-solutions relax the consistency constraints, any pre-solution that
minimises the individual objective functions represents a lower bound to the
stochastic objective:
min
σ∈pre sol(P )
k
i=1 pifi(σi) = k
i=1 pi
min
σ∈sol(Pi)fi(σ) ≤
min
σ∈sol(P )
k
i=1 pifi(σi)
A number of algorithms have been developed on the basis of iteratively solving
the scenarios. In the following, we will introduce the most relevant methods.
Progressive Hedging (PH) ﬁnds convergence over the shared variables by penal-
ising the scenario objective functions. The added penalty terms represent the
Euclidean distance between the averaged ﬁrst stage variables and each scenario.
Gradually, by repetitively solving the scenarios and updating the penalty terms,
convergence over the shared variables is achieved. Originally PH was introduced
by Rockafellar and Wets [18] for convex stochastic programs, for which conver-
gence on the optimal solution can be guaranteed. However, PH was successfully
used as a heuristic for stochastic MIPs, where optimality guarantees do not hold
due to non-convexity, such as the stochastic inventory routing problem in [13]
or resource allocation problems in [22] to name just a few.
An alternative scenario-based decomposition method was proposed by Carø
and Schultz [6], called Dual Decomposition (DD), a branch and bound algo-
rithm for stochastic MIPs. To generate lower bounds, the DD uses the dual of
the stochastic problem, obtained by relaxing the consistency constraints using
Lagrangian multipliers. To generate upper bounds, the solutions to the sub-
problems are averaged and made feasible using a rounding heuristic. The fea-
sible region is successively partitioned by branching over the shared variables.
The eﬃciency of the DD strongly depends on the update scheme used for the
Lagrangian multipliers.

282
D. Hemmi et al.
The algorithms introduced so far, including PH, DD and the L-Shaped
method, only work for linear programs, with some extensions being available
for certain classes of integer linear programs. As a result these approaches may
require a solid understanding of the problem and the algorithm, in order to
reformulate a given problem to be solved or to adapt the algorithm.
2.3
Evaluate and Cut
In contrast to the specialised approaches, Ahmed [1] proposes a scenario decom-
position algorithm for SCOPs with binary ﬁrst stage variables, which does not
rely on the constraints being linear or the problem convex. Ahmed’s algorithm
solves each scenario COP independently to optimality. This yields a pre-solution
to the SCOP and according to the discussion above a lower bound. In addition,
each scenario ﬁrst stage solution is evaluated against all other scenarios. The
result is a candidate solution to the SCOP and therefore an upper bound. The
evaluated candidates are added to the scenarios as nogoods (or cuts), forcing
them to return diﬀerent solutions in every iteration. As long as the ﬁrst stage
variables have ﬁnite domains, this process is guaranteed to ﬁnd the optimal
SCOP solution and terminates, either when the lower bound exceeds the upper
bound or no additional candidate solutions are found. The scenarios can be
evaluated separately, allowing highly parallelised implementations.
Algorithm 1 implements Ahmed’s [1] ideas using our SCOP notation. We
call this algorithm evaluateAndCut, as it evaluates each candidate against
all scenarios and adds nogoods that cut the candidates from the rest of the
search. In line 7, each scenario i is solved independently, resulting in a variable
assignment to the ﬁrst and second stage σ, and a value obj of the objective
function fi. The lower bound is computed incrementally as the weighted sum
(line 8). Furthermore, a set S of all ﬁrst stage scenario solutions is constructed
(line 9, recall that σ|V means the restriction of σ to the ﬁrst stage variables
V ). Each candidate solution σ|V in S is evaluated against all scenarios Pi (line
14). This yields a feasible solution to the SCOP, and the tentative upper bound
tUB is updated. Once the candidate σ|V is evaluated, a nogood excluding σ|V
from the search is added to the scenario problems Pi (line 16). We call this
the candidate nogood. If the tentative upper bound is better than the global
upper bound UB, a new incumbent is found (lines 17–19).
Each candidate σ|V is an optimal solution to one scenario problem Pi; exclud-
ing it from all scenarios in line 16 implies a monotonically increasing lower bound.
The algorithm terminates when the lower bound exceeds the upper bound. Note,
the call to solve in line 7 will return inﬁnity if any scenario becomes infeasible.
Line 21 will be used in our extensions of the algorithm presented in Sect. 3 and
can be ignored for now.
Ahmed [1] reports on an implementation of this algorithm using MIP technol-
ogy, and evaluates it on benchmarks with a binary ﬁrst and linear second stage.
The paper claims that this algorithm can ﬁnd high quality upper bounds quickly.
For linear SCOPs, improvements to the algorithm are introduced in [20], taking

Scenario-Based Learning for Stochastic Combinatorial Optimisation
283
Algorithm 1. A scenario decomposition algorithm for 0–1 stochastic programs
1: procedure evaluateAndCut
2:
Initialise: UB = ∞, LB = –∞, sol = NULL
3:
while LB < UB do
4:
LB = 0, S= ∅
5:
% Obtain lower bound and ﬁnd candidate solutions
6:
for i in 1..k do
7:
< σ,obj> = solve(Pi)
8:
LB += pi * obj
9:
S ∪= σ|V
10:
% Check candidate solutions and obtain upper bound
11:
for σV ∈S do
12:
tUB = 0
13:
for i in 1..k do
14:
< ,obj> = solve(Pi[C ∪= {σV }])
15:
tUB += pi * obj
16:
Pk = Pk[C ∪= {¬σV }]
17:
if tUB < UB then
18:
sol = σV
19:
UB = tUB
20:
% Evaluate partial ﬁrst stage assignments
21:
dive(P,UB,S,sol)
22:
return sol
advantage of a linear relaxation to improve the lower bounds, and additional
optimality cuts based on the dual of the second stage problem.
Discussion: The evaluateAndCut algorithm has the distinct advantage over
other decomposition approaches that it can be applied to arbitrary SCOPs. This
makes it an ideal candidate as a backend for solver-independent stochastic mod-
elling languages such as Stochastic MiniZinc. However, in the worst case the algo-
rithm requires O(k2) checks for k scenarios in each iteration. This is not pro-
hibitive if evaluating the candidates is cheap. However, for problems that have
many scenarios and a combinatorial second stage, this quadratic behaviour may
dominate the solving time. In the following section, we propose a technique to
decrease the time to solve and evaluate the individual candidates. Furthermore, we
propose a method to reduce the number of iterations required to solve the SCOP.
3
Scenario-Based Algorithms for CP
This section introduces three modiﬁcations to evaluateAndCut that improve
its performance: using Lazy Clause Generation solvers for the scenario sub-
problem; using dives to limit the number of candidate veriﬁcations and iterations
required; and scenario bundling as a hybrid between the DE and scenario-based
decomposition.

284
D. Hemmi et al.
Traditional CP solvers use a combination of propagation and search. Prop-
agators reduce the variable domains until no further reduction is possible or a
constraint is violated. Backtracking search, usually based on variable and value
selection heuristics, explores the options left after propagation has ﬁnished. In
contrast to traditional CP solvers, Lazy Clause Generation (LCG) [16] solvers
learn during the search. Every time a constraint is violated the LCG solver
analyses the cause of failure and adds a constraint to the model that prevents
the same failure from happening during the rest of the search. The added con-
straints are called nogoods, and they can be seen as the CP equivalent of cutting
planes in integer linear programming – they narrow the feasible search space.
Chu and Stuckey showed how nogoods can be reused across multiple instances
of the same model if the instances are structurally similar [7]. This technique
is called inter-instance learning. The nogoods learned during the solving of one
instance can substantially prune the search space of future instances. Empirical
results published in [7] indicate that for certain problem classes, a high similarity
between models yields a high resusability of nogoods and therefore increased
performance.
The evaluateAndCut algorithm repeatedly solves very similar instances.
In each iteration, every scenario is solved again, with the only diﬀerence being
the added nogoods or the projection onto the ﬁrst stage variables to evaluate
candidates (see line 16 in Algorithm 1). As a consequence, inter-instance learning
can be applied to speed up the search within the same scenario. We call this
concept vertical learning. No changes are required to Algorithm 1, except that we
assume the calls to solve in line 14 to be incremental and remember the nogoods
learned for each Pi in the previous iteration. To the best of our knowledge, this
is the ﬁrst use of inter-instance learning in a vertical fashion.
3.1
Search Over Partial Assignments
As introduced earlier, the evaluateAndCut algorithm quickly ﬁnds high qual-
ity solutions, by checking the currently best solution for an individual scenario
(the candidate) against all other scenarios. However, in order to prove optimality,
evaluateAndCut relies on the lower bound computed from the pre-solution
found in each iteration. The quality of the lower bound, and the number of
iterations required for it to reach the upper bound and thus prove optimality,
crucially depends on the candidate nogoods added in each iteration.
Compared to nogoods as computed during LCG search, candidate nogoods
are rather weak: They only cut oﬀa single, complete ﬁrst stage assignment. Fur-
thermore, in the absence of Lagrangian relaxation or similar methods, candidate
nogoods are the only information about the global lower bound that is available
to each scenario problem Pi.
Stronger nogoods: To illustrate the candidate nogoods produced by eval-
uateAndCut, consider a set of shared variables of size 5, and a ﬁrst stage
candidate solution x = [3, 6, 1, 8, 3]. The resulting candidate nogood added to

Scenario-Based Learning for Stochastic Combinatorial Optimisation
285
the constraint set of each scenario subproblem Pi is:
Pi = Pi[C∪= {x1 ̸= 3 ∨x2 ̸= 6 ∨x3 ̸= 1 ∨x4 ̸= 8 ∨x5 ̸= 3}]
The added constraint cuts oﬀexactly one solution. A nogood composed of only
a subset of shared variables would be much stronger. For example, assume that
we can prove that even the partial assignment x1 = 3 ∧x2 = 6 ∧x3 = 1 cannot
be completed to an optimal solution to the stochastic problem. We could add
the following nogood:
Pi = Pi[C∪= {x1 ̸= 3 ∨x2 ̸= 6 ∨x3 ̸= 1}]
In contrast to the original candidate nogood that pruned exactly one solution,
the new, shorter nogood can cut a much larger part of the search space. We call
this stronger kind of nogood a partial candidate nogood.
Diving: We now develop a method for ﬁnding partial candidate nogoods based
on diving. The main idea is to iteratively ﬁx ﬁrst stage variables across all scenar-
ios and compute a pre-solution, until the lower bound exceeds the global upper
bound. In that case, the ﬁxed variables can be added as a partial nogood.
The modiﬁcation to Algorithm 1 consists of a single added call to a procedure
dive in line 21, which is executed in each iteration after the candidates have been
checked. The deﬁnition of dive is described in Algorithm 2.
Line 5 constructs a constraint c that ﬁxes a subset of the ﬁrst stage variables,
based on the current set of scenario solutions S. This is done according to a
heuristic that is introduced later. The loop in lines 7–10 is very similar to the
computation of a pre-solution in Algorithm 1, except that all scenarios are forced
to agree on the selected subset of shared variables by adding the constraint c. As
in Algorithm 1, we compute a lower bound based on the pre-solution. However,
since we have arbitrarily forced the scenario sub-problems to agree using c, this
lower bound is not valid globally – there can still be better overall solutions with
diﬀerent values for the variables σ|V .
In essence, the algorithm is adding speciﬁc consistency constraints one by
one. Adding such constraint can lead to one of three states:
1. The lower bound does not exceed the upper bound, but all scenarios agree on
a common ﬁrst stage assignment (even if c does not constrain all ﬁrst stage
variables). This means that a new incumbent solution is found (lines 11–17)
and the constraint c can be added as a partial candidate nogood.
2. The lower bound meets or exceeds the upper bound. In this case, the partial
consistency constraint c cannot be extended to any global solution that is
better than the incumbent (lines 18–22). We can therefore add c as a partial
candidate nogood.
3. The lower bound is smaller than the upper bound, and the scenarios have not
converged on the ﬁrst stage variables. In this case, an additional constraint
is added to the partial consistency constraint c.

286
D. Hemmi et al.
Algorithm 2. Searching for partial candidate nogoods using diving
1: procedure dive(P,UB,S,sol)
2:
tLB = –∞
3:
while tLB < UB do
4:
tLB = 0
5:
c = selectFixed(S,V) % Select ﬁrst stage variables to ﬁx
6:
S = ∅
7:
for i in 1..k do
8:
<σ,obj> = solve(Pi[C ∪= {c}])
9:
tLB += pi * obj
10:
S ∪= σ|V
11:
if tLB < UB ∧S = {σV } then
12:
UB = tLB
13:
sol = σV
14:
% Add partial candidate nogood
15:
for i in 1..k do
16:
Pi = Pi[C ∪= {¬c}]
17:
return
18:
if tLB >= UB then
19:
% Add partial candidate nogood
20:
for i in 1..k do
21:
Pi = Pi[C ∪= {¬c}]
22:
return
The dive procedure terminates because in each iteration, selectFixed ﬁxes at
least one additional ﬁrst stage variable, which means that either case (1) or (2)
above must eventually hold.
Note, we do not evaluate the scenario solutions constructed during diving
against all scenarios. The rationale is that the additional consistency constraints
are likely to yield non-optimal overall solutions (i.e., worse than the current
incumbent). The O(k2) evaluations would therefore be mostly useless.
Diving heuristic: Let us now deﬁne a heuristic for choosing the consistency
constraints to be added in each iteration during a dive. The goal is to produce
short, relevant partial candidate nogoods and achieve convergence across all
scenarios quickly. Our heuristic focuses on the ﬁrst stage variables that have
already converged in the current pre-solution. The procedure selectAndFix in
Algorithm 3 implements the heuristic. First, it picks all the ﬁrst stage variables
that have already converged, including those ﬁxed in previous steps of this dive
(line 5 right side of ∧). Secondly, an additional ﬁrst stage variable assignment
is chosen (line 5 left side of ∧), based on the variable/value combination that
occurs most often over all scenarios. Given the current pre-solution S, it ﬁrst
constructs a mapping Count from variables to multisets of their assignments
(line 3). Thereafter a variable/value combination is picked that occurs most
often, but is not converged yet (line 4). In example, if variable x3 is assigned
to the value 4 in three scenario solutions, and to the value 7 in another two,

Scenario-Based Learning for Stochastic Combinatorial Optimisation
287
then Count would contain the pair < x3, {4, 4, 4, 7, 7} >. The value 4 would be
assigned to x3 assuming it is the most prevalent variable/value combination over
all ﬁrst stage variables. Finally, we construct a constraint that assigns xe to ve,
in addition to assigning all variables that have converged across all scenarios
(line 5).
Algorithm 3. Diving heuristic
1: procedure selectFixed(S,V)
2:
V als = < {σi(x) : σi ∈S} : x ∈V >
3:
Count = < card({i : σi(x) = val, σi ∈S}) :< x ∈V, val ∈V alsx >>
4:
< xe, ve > =
arg max
<x,v>
val∈V alsx
Count<x,v>< k
Count<x,val>
5:
c = (xe = ve ∧

x∈V
val∈V alsx
Cout<x,v>= k
x = v)
6:
return c
Scenario Bundling: The ﬁnal extension of Ahmed’s algorithm is based on the
observation that the sub-problems in any scenario decomposition method can in
fact comprise multiple scenarios, as long as we can ﬁnd the optimal stochastic
solution for that subset of scenarios in each iteration. We call this scenario
bundling.
Since evaluateAndCut has O(k2) behaviour for k scenarios, bundling can
have a positive eﬀect on the runtime, as long as the solving time for each bundle
is not signiﬁcantly higher than that for an individual scenario. Furthermore,
the bundling of scenarios yields better lower bounds, since each component of
a pre-solution is now an optimal stochastic solution for a subset of scenarios,
which is guaranteed to be worse than the individual scenario objectives. By
bundling scenarios in this way, we can therefore combine the fast convergence of
evaluateAndCut for large numbers of scenarios with the good performance
of methods such as the DE on low numbers of scenarios. Scenario bundling has
been applied to progressive hedging [9] and to evaluateAndCut in [19].
4
Experiments
This section reports on our empirical evaluation of the algorithms discussed
above. As a benchmark set we use a stochastic assignment problem with recourse
similar to the stochastic generalised assignment problem (SGAP) described in
[2]. A set of jobs, each composed of multiple tasks, is to be scheduled on a set
of machines. Precedence constraints ensure that the tasks in a job are executed
sequentially. Furthermore, tasks may be restricted to a sub-set of machines.

288
D. Hemmi et al.
The processing time of the tasks varies across the set of machines, and in the
stochastic version of the problem, this is a random variable. In the ﬁrst stage,
tasks must be assigned to machines. An optimal schedule with respect to the
random variables is created in the second stage. The objective is to ﬁnd a task
to machine assignment minimizing the expected makespan over all scenarios.
Work on the SGAP with uncertainty on whether a job must be executed
is described in [2]. However, to the best of our knowledge, there are no public
benchmarks for the SGAP with uncertain processing times. Benchmarks for our
experiments are created as described for deterministic ﬂexible job shop instances
in [5]. The scenarios are created by multiplying the base task durations by a
number drawn from a uniform distribution with mean 1.5, variance 0.3, a lower
limit of 0.9 and upper limit of 2.
We modelled the benchmark problems in MiniZinc. Each scenario is described
in a separate MiniZinc data ﬁle, and compiled separately. This enables the
MiniZinc compiler to optimise the COPs individually before solving. The models
use a ﬁxed search strategy (preliminary studies using activity based search did
not improve the performance). The solver is implemented using Chuﬀed [8] and
Python 2.7. The scenarios are solved using Chuﬀed, and learned nogoods are
kept for subsequent iterations to implement vertical learning. A Python script
coordinates the scenarios and dives. Up to twenty scenarios are solved in paral-
lel. The experiments were carried out on a 2.9 GHz Intel Core i5, Desktop with
8 GB running OSX 10.12.1. A timeout of 3600 s was used.
4.1
Results
Table 1 contains the results for 9 representative problem instances with a range
of 20 to 400 scenarios. In every instance, the optimal solution is found within the
ﬁrst few iterations and the remaining time is used to prove optimality. No results
for the deterministic equivalent are presented as the run time is not competitive
once the number of scenarios exceeds 20.
The impact of diving: The ﬁrst two rows per instance contain the time it
takes to solve the problem instances without scenario bundling. No substantial
time diﬀerence can be reported for ﬁnding the optimal solution when dives are
enabled. However, using dives improves the overall search performance in every
instance. Figure 1 contains plots displaying the impact of dives when solving 100
scenarios without bundling.
The monotonically increasing graphs are the lower bounds. The horizontal
black line is the optimal solution. The upper bound progress is not displayed,
as the optimal solution is always found within a few seconds. Once the lower
bound meets the upper bound, optimality is proven and the search terminates.
The lower bound increases quickly at the beginning of the search. Over time the
evaluate and cut method without diving ﬂattens and the lower bound converges
slowly towards upper bound. In strong contrast is the progress of the lower bound
when using diving. At ﬁrst during the initial iterations, the partial candidate
nogoods are not showing any eﬀects and the two curves are similar. However,

Scenario-Based Learning for Stochastic Combinatorial Optimisation
289
Fig. 1. The impact of dives
after the initial phase, generated strong partial candidate nogoods are paying oﬀ
and the lower bound jumps drastically.
Using DE to proof optimality: The third row in each instance displays the
times it takes to prove optimality using the deterministic equivalent. The upper
bounds obtained from evaluateAndCut are used to constrain the stochastic
objective function in the DE. Similarly to using the DE directly, it does not scale
with an increased number of scenarios. Furthermore, in contrast to evaluate-
AndCut no optimality gap is produced.
Scenario bundling: The last two rows per instance contain the time it takes
to solve the instances using scenario bundling. Four scenarios are randomly
grouped to form a DE. Each scenario group becomes a sub-problem solved with
evaluateAndCut with and without diving enabled. As a result, the run time
decreases in every instance. For the benchmark instances, diving is less powerful
when using scenario bundles. This can be explained by the decreasing number

290
D. Hemmi et al.
Table 1. Time to prove optimality [sec]
Instance
Algorithm
20
40
80
100
200
300
400
dh 5 17
E&C
24
343
1748
2060
Dive
12
152
584
716
DEupperBound
7
215
1342
1923
-
-
-
E&Cbundle
1
8
19
17
27
49
46
Divebundle
1
11
20
17
26
45
64
dh 5 16
E&C
36
111
436
638
Dive
23
56
212
318
DEupperBound
20
131
877
1136
-
-
-
E&Cbundle
2
5
9
11
34
69
91
Divebundle
3
7
12
13
41
72
93
dh 5 20
E&C
338
196
-
-
Dive
87
880
713
1304
DEupperBound
43
293
2355
-
-
-
-
E&Cbundle
3
17
36
72
240
423
645
Divebundle
7
18
43
71
190
328
510
dh 5 18
E&C
163
719
-
-
Dive
50
179
984
1566
DEupperBound
3
50
1547
3467
-
-
-
E&Cbundle
7
31
162
226
407
712
1233
Divebundle
8
32
157
218
401
629
873
dh 6 17 1
E&C
94
735
2328
3242
Dive
33
152
447
712
DEupperBound
6
62
647
1073
-
-
-
E&Cbundle
2
3
15
15
59
91
156
Divebundle
2
3
14
15
52
75
114
dh 6 15
E&C
10
35
209
338
Dive
5
14
95
147
DEupperBound
5
69
495
1142
-
-
-
E&Cbundle
1
3
16
21
67
81
138
Divebundle
1
4
19
25
61
66
183
dh 6 17 2
E&C
253
755
3345
-
Dive
58
153
725
1014
DEupperBound
0
31
477
386
2159
-
-
E&Cbundle
7
8
33
49
89
182
303
Divebundle
6
7
24
24
48
121
178
dh 6 16
E&C
21
69
141
214
Dive
13
36
71
97
DEupperBound
12
111
777
1921
-
-
-
E&Cbundle
11
24
29
42
125
206
336
Divebundle
11
25
37
54
76
122
199
dh 6 18
E&C
134
672
2140
3548
Dive
44
150
454
645
DEupperBound
9
157
1790
3323
-
-
-
E&Cbundle
7
11
19
29
120
219
338
Divebundle
9
20
24
31
131
212
310

Scenario-Based Learning for Stochastic Combinatorial Optimisation
291
Table 2. Speedup using vertical learning
of iterations required to ﬁnd the optimal solution. More time is spend solving
the sub-problems and less eﬀort is required to coordinate the scenarios.
Vertical learning: Table 2 shows the speed-up when using vertical learning.
Each column displays the speed-up over a scenario group. Overall the solving
time decreased by 19.3% with 10.7% variance with an increase of at most 72%.
5
Conclusion
This paper has presented the ﬁrst application of Ahmed’s scenario decompo-
sition algorithm [1] in a CP setting. Furthermore, we have introduced multiple
algorithmic innovations that substantially improve the evaluateAndCut algo-
rithm. The most signiﬁcant improvement is the partial search to create strong
candidate nogoods. All our algorithmic innovations can be implemented in a
parallel framework.
To further strengthen the evaluateAndCut algorithm we will continue to
work on the following ideas. First, the heuristic used to determine the variables
to be ﬁxed strongly impacts the performance of the search. A good heuristic
is able to produce strong, relevant nogoods. Introducing a master problem that
enables a tree search and improved coordination is worthwhile exploring. For the
results we have used the standard system to manage nogoods within Chuﬀed.
Further analysing to role of nogoods will help us understand their impact and
how we can use nogoods to improve vertical learning and eﬃciently incorporate
inter-instance learning.
References
1. Ahmed, S.: A scenario decomposition algorithm for 0–1 stochastic programs. Oper.
Res. Lett. 41(6), 565–569 (2013)
2. Albareda-Sambola, M., Van Der Vlerk, M.H., Fern´andez, E.: Exact solutions to
a class of stochastic generalized assignment problems. Eur. J. Oper. Res. 173(2),
465–487 (2006)

292
D. Hemmi et al.
3. Balafoutis, T., Stergiou, K.: Algorithms for stochastic CSPs. In: Benhamou, F.
(ed.) CP 2006. LNCS, vol. 4204, pp. 44–58. Springer, Heidelberg (2006). doi:10.
1007/11889205 6
4. Birge, J.R., Louveaux, F.: Introduction to Stochastic Programming. Springer
Science & Business Media, New York (2011)
5. Brandimarte, P.: Routing and scheduling in a ﬂexible job shop by tabu search.
Ann. Oper. Res. 41(3), 157–183 (1993)
6. CarøE, C.C., Schultz, R.: Dual decomposition in stochastic integer programming.
Oper. Res. Lett. 24(1), 37–45 (1999)
7. Chu, G., Stuckey, P.J.: Inter-instance nogood learning in constraint programming.
In: Milano, M. (ed.) CP 2012. LNCS, pp. 238–247. Springer, Heidelberg (2012).
doi:10.1007/978-3-642-33558-7 19
8. Chu, G.G.: Improving combinatorial optimization. Ph.D. thesis, The University of
Melbourne (2011)
9. Crainic, T.G., Hewitt, M., Rei, W.: Scenario grouping in a progressive hedging-
based meta-heuristic for stochastic network design. Comput. Oper. Res. 43, 90–99
(2014)
10. Frisch, A.M., Harvey, W., Jeﬀerson, C., Mart´ınez-Hern´andez, B., Miguel, I.:
Essence: a constraint language for specifying combinatorial problems. Constraints
13(3), 268–306 (2008)
11. Goemans, M., Vondr´ak, J.: Stochastic covering and adaptivity. In: Correa, J.R.,
Hevia, A., Kiwi, M. (eds.) LATIN 2006. LNCS, vol. 3887, pp. 532–543. Springer,
Heidelberg (2006). doi:10.1007/11682462 50
12. Hokama, P., San Felice, M.C., Bracht, E.C., Usberti, F.L.: A heuristic approach
for the stochastic steiner tree problem (2014)
13. Hvattum, L.M., Løkketangen, A.: Using scenario trees and progressive hedging for
stochastic inventory routing problems. J. Heuristics 15(6), 527–557 (2009)
14. Manandhar, S., Tarim, A., Walsh, T.: Scenario-based stochastic constraint pro-
gramming. arXiv preprint arXiv:0905.3763 (2009)
15. Nethercote, N., Stuckey, P.J., Becket, R., Brand, S., Duck, G.J., Tack, G.:
MiniZinc: towards a standard CP modelling language. In: Bessi`ere, C. (ed.) CP
2007. LNCS, vol. 4741, pp. 529–543. Springer, Heidelberg (2007). doi:10.1007/
978-3-540-74970-7 38
16. Ohrimenko, O., Stuckey, P.J., Codish, M.: Propagation via lazy clause generation.
Constraints 14(3), 357–391 (2009)
17. Rendl, A., Tack, G., Stuckey, P.J.: Stochastic MiniZinc. In: O’Sullivan, B. (ed.)
CP 2014. LNCS, vol. 8656, pp. 636–645. Springer, Cham (2014). doi:10.1007/
978-3-319-10428-7 46
18. Rockafellar, R.T., Wets, R.J.B.: Scenarios and policy aggregation in optimization
under uncertainty. Math. Oper. Res. 16(1), 119–147 (1991)
19. Ryan, K., Ahmed, S., Dey, S.S., Rajan, D.: Optimization driven scenario grouping
(2016)
20. Ryan, K., Rajan, D., Ahmed, S.: Scenario decomposition for 0–1 stochastic pro-
grams: improvements and asynchronous implementation. In: Parallel and Distrib-
uted Processing Symposium Workshops, pp. 722–729. IEEE (2016)
21. Toth, P., Vigo, D.: Vehicle Routing: Problems, Methods, and Applications, vol. 18.
Siam, Philadelphia (2014)
22. Watson, J.P., Woodruﬀ, D.L.: Progressive hedging innovations for a class of sto-
chastic mixed-integer resource allocation problems. Comput. Manage. Sci. 8(4),
355–370 (2011)

Optimal Stock Sizing in a Cutting Stock
Problem with Stochastic Demands
Alessandro Zanarini(B)
ABB Corporate Research Center, Baden-D¨attwil, Switzerland
alessandro.zanarini@ch.abb.com
Abstract. One dimensional cutting stock problems arise in many man-
ufacturing domains such as pulp and paper, textile and wood. In this
paper, a new real life variant of the problem occuring in the rubber
mold industry is introduced. It integrates both operational and strate-
gical planning optimization: on one side, items need to be cut out of
stocks of diﬀerent lengths while minimizing trim loss, excess of produc-
tion and the number of required cutting operations. Demands are how-
ever stochastic therefore the strategic choice of which mold(s) to build
(i.e. which stock lengths will be available) is key for the minimization of
the operational costs. A deterministic pattern-based formulation and a
two-stage stochastic problem are presented. The models developed are
solved with a mixed integer programming solver supported by a con-
straint programming procedure to generate cutting patterns. The app-
roach shows promising experimental results on a set of realistic industrial
instances.
1
Introduction
Classical one-dimensional Cutting Stock Problems (CSP) consist of minimizing
the trim loss in the manufacturing process of cutting small items out of a set
of larger size stocks. Each item (also referred to as small object) i = 1, . . . , I
has an associated demand di, and a length li; the stock (also referred to as
large object or roll) has a length L out of which one or more items can be cut
out. The objective is to utilize the minimum amount of stocks, i.e. minimize
the trim loss (problem type 1/V/I/R, according to Dyckhoﬀ’s classiﬁcation [1]).
Many diﬀerent variations of the classical CSP have been proposed since the
ﬁrst formulation was introduced by Kantorovich in 1939 [2], including stock
of diﬀerent lengths, setup costs, open stock minimization, number of pattern
minimization, to name just a few (see [3] for a survey).
In this paper, we present a new variation of the problem arising in the produc-
tion of a set of speciﬁc items in the rubber mold industry. In this manufacturing
process, a mold is used to create one stock typology of a given length. From
the mold, several stocks can be produced and they are then cut by an operator
(with the help of a cutting machine) in multiple pieces to meet the individual
item demands. Possibly, the left-over of the initial stock may be discarded (trim
loss) in case its trim length does not correspond to the length of any required
c
⃝Springer International Publishing AG 2017
D. Salvagnin and M. Lombardi (Eds.): CPAIOR 2017, LNCS 10335, pp. 293–301, 2017.
DOI: 10.1007/978-3-319-59776-8 24

294
A. Zanarini
items; alternatively, if the remaining length matches one of the item lengths and
the demand for that particular item is already met, it is stored in inventory (with
an associated cost) as excess of production for future use. Due to process con-
straints, inventory items cannot be further reworked, i.e. they cannot be re-cut
to meet future demands of smaller items. The operational planning optimization
consists of minimizing the trim loss, the over-production (i.e., inventory costs)
and the number of cuts to be performed by the operator.
The peculiarity of the problem at hand comes from the fact that the mold(s)
also needs to be built; that is, the stock length(s) has to be decided up-front in
order to meet the future demands. In the unrealistic case of inﬁnite budget and
capacity, the trivial optimal solution would be to produce one mold for each item
length. In reality, however, the number of molds that can be manufactured is
limited. This leads to an integrated strategical and operational real-life planning
problem that, to the best of the author’s knowledge, has not been explored before
in the literature.
The contributions of the paper are: the introduction and formalization of a
new real-life cutting stock problem and its solution via an integrated strategical
and operational CSP model with either deterministic or stochastic demands.
The problems have been solved with a Mixed Integer Programming (MIP) solver
supported by a Constraint Programming (CP) procedure to generate diﬀerent
cutting patterns.
The remainder of the paper is organized as follows: Sect. 2 summarizes the
relevant literature; Sect. 3 formalizes the strategical and operational planning
problem; experimental results are shown in Sect. 4; Sect. 5 draws some conclu-
sions and describes future work.
2
Background
One-dimensional cutting stock problems have been extensively studied in the
literature; several approaches have been proposed such as approximation algo-
rithms, heuristics and meta-heuristics, population-based methods, constraint
programming, dynamic programming and mathematical programming (please
refer to the recent survey in [4] for an ample review and comparison). Among
many, the most employed mathematical formulations are either item-based (orig-
inally introduced in [2]) or pattern-based (see [5]). In the former, a binary variable
is used to indicate whether item i is cut out of stock j. In the latter, a cutting
pattern deﬁnes a priori how many items and which item types are cut out of
a single stock; then an integer variable i represents the number of stocks that
are cut according to a given pattern i. As the number of cutting patterns grows
exponentially with the average number of items that can ﬁt into a stock, the
problem is typically solved with column generation approaches. Pattern-based
branch-and-price and branch-and-cut-and-price algorithms are considered the
state-of-the-art complete methods (see [4,6]).
Very few research papers investigate uncertainty of demands arising in cut-
ting stock problems. Kallrath et al. [8] studied a real-life problem arising in

Optimal Stock Sizing in a Cutting Stock Problem with Stochastic Demands
295
the paper industry where they minimize the trim loss and the number of pat-
terns used, while not allowing over production. They solved the problem using
a column generation approach that falls back to column enumeration in certain
cases (i.e. precomputing all the columns explicitly). The column enumeration
is claimed to be easily applicable to MILP problems, easier to implement and
maintain and at times more eﬃcient when the pricing problem becomes diﬃcult
or when the number of columns is relatively small (see also [9]). In the same
paper, they also introduced a two-stage stochastic version of the problem with
uncertain demands: the ﬁrst stage variables represent the patterns employed,
and in the second stage the related production is deﬁned. They employ column
enumeration in a sample average approximation framework; unfortunately, no
experimental results are shown for the stochastic problem. Beraldi et al. studied
in [10] a similar problem for which they proposed an ad-hoc approach designed to
exploit the speciﬁc problem structure. Alem et al. in [7] also considered stochas-
tic CSP, where in the ﬁrst stage the patterns and production plans are decided
and the second stage decision determines over and under production.
The main diﬀerences between the problems already studied in the literature
and this contribution are: ﬁrstly, the main cost drivers are the trim loss and
the allowed over production; as a secondary objective the number of cuts to be
performed (as opposed to the numbers of patterns utilized); secondly, the molds
to be built are the key initial investment and they determine the available stock
lengths (as opposed to predeﬁned stock lengths).
In order to clarify the diﬀerence in solutions between minimizing the total
number of cuts or alternatively the number of patterns, let’s consider an example
in which the stock length is equal to 16 and two items, of lengths 7 and 8
respectively, need to be produced with a demand of 2 each. All the solutions that
are optimal in term of trim loss require 2 stocks. The solution that minimizes
the number of patterns employs only one pattern (8, 7); it will be used twice to
meet the demand and the production will require a total of 4 cutting operations.
Oppositely, the minimization of the number of cuts will make use of one pattern
(8, 8) and one pattern (7, 7); the ﬁrst pattern requires only one cut to produce
two items, whereas the second two cuts.
3
Problem Formulation
In this section the problem is formalized: at ﬁrst the deterministic operational
planning problem is shown followed by the integrated strategical and operational
planning problem. All the approaches use a pattern-based modelling1.
1 For comparison, an item-based model was also developed; it showed quickly its lim-
itations even in relatively small instances as soon as included the over production.
For brevity, we omit the item-based model and results.

296
A. Zanarini
3.1
Operational Planning with Deterministic Demands
In this section, the lengths of the molds is assumed to be already deﬁned and
the problem is to ﬁnd the optimal production plan fulﬁlling some deterministic
demands. We will use the following notation:
Parameters and Variables
I
The total number of diﬀerent items to be produced; the items are indexed
by i = 1, . . . , I
li
The length of the item i
di
The demand for item i
ci
The cost for over producing item i
M
The total number of molds; molds are indexed by m = 1, . . . , M
Lm
The length of the stock produced with mold m
Pm
The total number of patterns related to the stock m; the patterns are
indexed by j = 1, . . . , Pm
pmji The number of items i produced with pattern j of mold m
wmj
The amount of trim loss caused by pattern j of mold m
omj
The number of cutting operations needed for pattern j of mold m
W
The trim loss cost per unit of length
α
The weight on the secondary objective function, i.e. number of cutting
costs (α ≪1)
xmj
The integer non-negative variable indicating the number of times pattern
j of mold m is used in the ﬁnal solution
Note that the cost of over production is linked to the length li (the longer
the higher will be the cost) and to the demand of item i, i.e. the higher is
its demand, the lower will be the inventory cost; in general: W · li ≥ci ≥0.
The real industrial problem analyzed in this paper exhibits integer item and
stock lengths. The following mathematical model formalizes the problem for the
optimal production planning:
min
M

m=1
Pm

j=1

W · wmj · xmj + α · omj · xmj

+
I

i=1
ci ·

M

m=1
Pm

j=1
pmji · xmj −di

(1)
M

m=1
Pm

j=1
pmji · xmj ≥di,
i = 1, . . . , I
(2)
xmj ∈Z
(3)
The objective function in Eq. (1) is composed by three main elements: the
waste produced (wmj · xmj), the number of cuts required (omj · xmj) and lastly,
the over production. Equation (2) constrains the production of item i to meet
its demand di.

Optimal Stock Sizing in a Cutting Stock Problem with Stochastic Demands
297
Pattern Enumeration. An early analysis of the industrial instances showed
that for relevant mold sizes, the number of feasible cutting patterns is relatively
contained (about up to 50). For this reason and for the simplicity of the approach,
we decided to enumerate exhaustively all the patterns (see [8,9]). The pattern
enumeration is a pure constraint satisfaction problem that has been solved with
Constraint Programming.2 Once all the patterns have been generated, they are
fed as input to the mathematical model deﬁned in Sect. 3.1. The parameters
pmji, wmj and omj of the previous section become variables for this sub-problem,
respectively:
– Zi ∈{0, . . . , ⌊Lm/li⌋} represents the number of units produced for item i
– Q ∈{0, . . . Lm} represents the amount of trim loss for the pattern
– O ∈{0, . . . Lm −1} indicates the number of cut operations required
The model follows:
Lm = Q +
I

i=1
li · Zi
(4)
Q ̸= q · li,
q = 1, . . . , ⌊Lm/li⌋,
i = 1, . . . , I
(5)
O =
I

i=1
Zi −1 + (Q > 0)
(6)
Equation (4) constrains the waste and the sum of the item lengths to be
equal to the mold size. The set of Eq. (5) avoids that the waste is equal to (or
a multiple of) the size of one of the items in demand. This restricts the number
of feasible patterns and it is valid as long as ci ≤W · li; that is, the inventory
cost of an item of length li is at most as expensive as throwing away the same
quantity.3 Finally, Eq. (6) deﬁnes the number of cuts to be equal to the number
of items in the pattern minus 1; an additional cut is required if there is also a
ﬁnal trim (reiﬁed constraint).
3.2
Strategical and Operational Planning Under Stochastic
Demands
In this industrial context the main practical challenge is that an initial invest-
ment is required for the construction of the molds that will be used to meet future
stochastic demands. The problem is a two-stage stochastic problem in which the
ﬁrst stage decision variables are the mold to be built, and the second-stage is
the actual production plan once the demands are known.
We follow a sample average approximation approach in which a set of scenar-
ios s ∈Ω with corresponding probabilities 
s∈Ω ps = 1 is Monte Carlo sampled.
The vector of demands ds
i now depends on the speciﬁc scenario realization. We
2 For comparison an equivalent MIP model was also developed, however it performed
orders of magnitude slower for enumerating all the solutions.
3 This was the case in the real industrial context examined.

298
A. Zanarini
further introduce the binary variables ym indicating whether mold m is produced
or not. Due to manufacturing and operational constraints the molds considered
have to be limited to a maximum length L (Lm ≤L). The model follows:
min

s∈Ω
ps

M

m=1
Pm

j=1

W · wmj · xmjs + α · omj · xmjs

+
I

i=1
ci ·

M

m=1
Pm

j=1
pmji · xmjs −dis

M

m=1
ym ≤M
(7)
xmjs ≤B · ym,
s ∈Ω, m = 1, . . . , M, j = 1, . . . , Pm
(8)
Pm

j=1
xmjs ≥ym,
s ∈Ω, m = 1, . . . , M
(9)
M

m=1
Pm

j=1
pmji · xmjs ≥dis,
s ∈Ω, i = 1, . . . , I
(10)
ym ∈{0, 1},
xmjs ∈Z
(11)
where M is the maximum number of molds that can be built (Eq. (7)). Equations
(8) and (9) indicate that if a mold is not built then none of its pattern can be
employed, and vice versa, if only one pattern is used then the associated mold
must be built (B is a large number). Finally, Eq. (10) constrains the production
to meet the demand for each possible scenario.
For pure cutting stock problems minimizing the number of stocks used, a still
open conjecture, the Modiﬁed Integer Round-Up Property (MIRUP), states that
the integral optimal solution is close to the linear relaxation: zopt −⌈LLP ⌉≤1
(see [4] and [11]). In order to render the second stage sub-problem computation-
ally tractable, the integrality constraints on xmjs have been lifted (ym are kept
binary).
4
Experimental Results
The MIP and CP model have been developed using Google OR-Tools. The MIP
solver employed is the open source CBC. All the experiments have been con-
ducted on a MacBook Pro with an Intel Dual-core i5-5257U and 8 GB of RAM.
Pattern Enumeration. Creating all the possible patterns for all the molds ranging
from length 1 to 16 (lengths relevant in the studied industrial context) takes
30 ms for a total combined number of patterns of about 200. For reference, the
enumeration of the patterns for molds of lengths 15, 25 and 35 takes respectively
2, 28 and 300 ms, to enumerate resp. 40, 328 and 1995 patterns.

Optimal Stock Sizing in a Cutting Stock Problem with Stochastic Demands
299
Operational Planning with Deterministic Demands. We generated 2250 synthetic
instances with L = 20, and random demands (the ratio between the maximum
demand and the minimum demand -
maxi di
mini di - ranged from 3 to 20); in this
deterministic version the molds are predeﬁned (M ranged from 1 to 3).
For brevity, we will not report the detailed results but just some observations.
Firstly, all the generated instances but one were solved to optimality within one
second (with all the integrality constraints); evidently, the item and stock lengths
at play in this domain are not challenging for the MIP solver.
Secondly, we compared the optimal objective value of the integer solution
with the one from the linear relaxation: the diﬀerence between the two is at
most 0.03%. This is reassuring for the two-stage stochastic approach in which
the integrality constraints on the second-stage decision variables have been lifted.
Thirdly, we used the deterministic model to analyze the variability of the
number of cutting operations on a real industrial instance after the trim loss and
over production had been ﬁxed to their respective optimal values. Despite being
a secondary objective, the best and worst solutions showed as much as almost
10% of diﬀerence in the number of cutting operations; in the analyzed industrial
context, this translates to about 50 h of reduced time in cutting operations for
producing the same quantity of items (about 150 thousands).
Strategical and Operational Planning Under Stochastic Demands. We generated
the stochastic instances starting from a real industrial demand vector. We per-
turbated it using a gaussian distribution in order to create 20 diﬀerent demand
vectors. Each of them represents the mean demand vector, on which another set
of gaussian distributions are centered for the scenario generation; their standard
deviations is proportional to the demands: σi = di
k . We tested diﬀerent standard
deviation with k ∈{1, 3, 5}, scenario set cardinalities, |Ω| ∈{10, 20, 50}, and
number of molds to be constructed, M ∈{3, 4}; L and maxi{di} are both set
to 16, as per the industrial setting. The total number of tests amount to 360
instances.
In order to evaluate the quality of the stochastic approach, we computed the
Expected Value of Perfect Information (EVPI) and the Value of the Stochastic
Solution (VSS). The EVPI represents how much one could gain with a wait-and-
see strategy, i.e. the expected decrease of the objective value in case of a priori
knowledge of the stochastic variable realizations; a high EVPI connotes that
the stochastic approach is not capable of capturing well the uncertainty on the
demands. The VSS indicates the expected increase of the objective value when
using a deterministic optimization fed with the expected values of the stochastic
variables (see [12] for a general procedure to compute it); a high VSS means that
capturing uncertainty with the stochastic approach is actually bringing a beneﬁt;
oppositely, a low VSS shows that a deterministic optimization using expected
values leads to a solution that is similar to the one computed by the stochastic
approach.
The results are presented in Table 1: each row reports aggregated values for 20
instances; the ﬁrst three columns represent the instance parameters (described at
the beginning of the paragraph); four pairs of columns follow reporting averages

300
A. Zanarini
Table 1. Aggregated results for 360 instances.
Time (secs) Obj
EVPI
VSS
M k | Ω|
μ
σ
μ
σ
μ
σ
μ
σ
3
1 10
2.7 0.2
25769.8 5042.6 10818.1 3484.8
4185.0
8861.5
3
1 20
6.2 0.6
28142.6 5993.8 13942.7 4252.7
8975.3 11289.0
3
1 50
21.2 2.1
26616.4 4261.9 13106.8 3046.9
9861.5 13553.5
3
3 10
2.8 0.5
13600.8 4601.2
2392.0 1861.2
457.8
1602.4
3
3 20
7.0 1.1
13560.2 4650.3
3185.7 2031.3
375.0
1148.8
3
3 50
23.6 2.8
13741.9 4276.0
3415.7 2146.0
977.4
2861.5
3
5 10
2.8 0.5
11991.5 4517.3
1146.3 1363.7
66.3
215.6
3
5 20
7.3 0.9
11777.4 4502.6
1544.0 1474.0
16.0
69.6
3
5 50
26.2 4.4
11793.1 4274.3
1553.6 1494.7
252.1
876.7
4
1 10
2.3 0.2
9628.9 3423.0
5796.7 2226.7
3321.9
5148.8
4
1 20
5.7 0.4
11546.6 3550.4
8101.1 2601.3 11531.5 11090.0
4
1 50
21.4 2.3
11492.7 2635.8
7864.9 1764.7 11942.2 12523.1
4
3 10
2.4 0.2
4383.6 2217.2
1006.9
823.7
231.7
945.6
4
3 20
6.1 0.6
4738.9 2240.3
1626.1 1091.7
490.6
1265.9
4
3 50
22.6 2.1
4669.2 1995.9
1649.8 1003.8
1071.8
2142.2
4
5 10
2.4 0.2
3748.4 2018.3
441.2
517.9
17.1
74.4
4
5 20
6.0 0.6
3857.9 1970.7
745.8
626.0
149.2
363.7
4
5 50
20.9 3.0
3812.3 1826.7
783.7
607.0
164.1
492.4
and standard deviations of respectively the solution time, the objective value,
the EVPI and the VSS.
As expected, by decreasing the level of stochasticity (lower standard devia-
tion of the scenario generation, k = 5), the VSS drops signiﬁcantly as well as
the EVPI. Oppositely, as the scenario set cardinality grows, the VSS gets big-
ger in proportion than the EVPI. Finally, increasing M allows to signiﬁcantly
decrease the value of the objective function, though the VSS, in proportion,
increases. From a computational standpoint, for reference, should the integral-
ity constraints be kept, an instance takes more than one minute to solve (with
|Ω| = 10). The parameters that impact the most the solution time are |Ω| and L.
Increasing |Ω| to 100, 200 and 500 leads to a solution time of respectively about
65, 189 and 831 s. Similarly, increasing L (the real life problem presents L = 16)
to 20, 25 and 30 (with |Ω| = 10) results to a solution time of respectively about
17, 162 and 852 s.
5
Conclusion
In this paper, we introduced a new two-stage stochastic problem arising in the
production of some rubber elements in the rubber mold industry. The uniqueness

Optimal Stock Sizing in a Cutting Stock Problem with Stochastic Demands
301
comes from deciding the stock lengths before knowing the actual production
demand. We believe this problem setup can be relevant for other domains as
well where stock purchase orders need to be placed despite uncertainty on the
demands. We formalized the problem and developed a solution that was able
to solve real-life instances to optimality within an acceptable time. Future work
includes the exploration of other optimization techniques to improve scalability,
the integration of other real-life constraints and explore a multi-stage stochastic
setup for planning the production and inventory for multiple time slots.
Acknowledgements. The author would like to thank Davide Zanarini for bringing
to his attention this industrial problem.
References
1. Dyckhoﬀ, H.: A typology of cutting and packing problems. Eur. J. Oper. Res. 44,
145–159 (1990)
2. Kantorovich, L.V.: Mathematical methods of organizing and planning production.
Manag. Sci. 6(4), 366–422 (1960)
3. W¨ascher, G., Hauner, H., Schumann, H.: An improved typology of cutting and
packing problems. Eur. J. Oper. Res. 183, 1109–1130 (2007)
4. Delorme, M., Iori, M., Martello, S.: Bin packing and cutting stock problems: math-
ematical models and exact algorithms. Eur. J. Oper. Res. 255(1), 1–20 (2016)
5. Gilmore, P.C., Gomory, R.E.: A linear programming approach to the cutting-stock
problem. Oper. Res. 9(6), 849–859 (1961)
6. Belov, G., Scheithauer, G.: A branch-and-cut-and-price algorithm for one-
dimensional stock cutting and two-dimensional two-stage cutting. Eur. J. Oper.
Res. 171(1), 85–106 (2006)
7. Alem, D.J., Munari, P.A., Arenales, M.N., Ferreira, P.A.: On the cutting stock
problem under stochastic demand. Ann. Oper. Res. 179(1), 169–186 (2010)
8. Kallrath, J., Rebennack, S., Kallrath, J., Kusche, R.: Solving real-world cutting
stock-problems in the paper industry: mathematical approaches, experience and
challenges. Eur. J. Oper. Res. 238(1), 374–389 (2014)
9. Rebennack, S., Kallrath, J., Pardalos, P.M.: Column enumeration based decom-
position techniques for a class of non-convex MINLP problems. J. Global Optim.
43(2–3), 277–297 (2009)
10. Beraldi, P., Bruni, M.E., Conforti, D.: The stochastic trim-loss problem. Eur. J.
Oper. Res. 197(1), 42–49 (2009)
11. Scheithauer, G., Terno, J.: Theoretical investigations on the modiﬁed integer
round-up property for the one-dimensional cutting stock problem. Oper. Res. Lett.
20(2), 93–100 (1997)
12. Escudero, L.F., Garn, A., Merino, M., Prez, G.: The value of the stochastic solution
in multistage problems. TOP 15(1), 48–64 (2007)

Stochastic Task Networks
Trading Performance for Stability
Kiriakos Simon Mountakis1, Tomas Klos2, and Cees Witteveen1(B)
1 Delft University of Technology, Delft, The Netherlands
C.Witteveen@tudelft.nl
2 Utrecht University, Utrecht, The Netherlands
Abstract. This paper concerns networks of precedence constraints
between tasks with random durations, known as stochastic task net-
works, often used to model uncertainty in real-world applications. In
some applications, we must associate tasks with reliable start-times
from which realized start-times will (most likely) not deviate too far.
We examine a dispatching strategy according to which a task starts as
early as precedence constraints allow, but not earlier than its correspond-
ing planned release-time. As these release-times are spread farther apart
on the time-axis, the randomness of realized start-times diminishes (i.e.
stability increases). Eﬀectively, task start-times becomes less sensitive
to the outcome durations of their network predecessors. With increas-
ing stability, however, performance deteriorates (e.g. expected makespan
increases). Assuming a sample of the durations is given, we deﬁne an
LP for ﬁnding release-times that minimize the performance penalty of
reaching a desired level of stability. The resulting LP is costly to solve,
so, targeting a speciﬁc part of the solution-space, we deﬁne an associated
Simple Temporal Problem (STP) and show how optimal release-times
can be constructed from its earliest-start-time solution. Exploiting the
special structure of this STP, we present our main result, a dynamic pro-
gramming algorithm that ﬁnds optimal release-times with considerable
eﬃciency gains.
Keywords: Activity
network ·
Stochastic
scheduling ·
Solution
robustness
1
Introduction
A stochastic task network is a directed acyclic graph G(V, E) with each node
in V = {1, . . . , n} representing a task with a random duration and each arc
(i, j) ∈E representing a precedence-constraint between tasks i and j, specifying
that task j cannot start unless task i has ﬁnished. Such networks appear in
several domains like project scheduling [16], parallel computing [22], or even
digital circuit design [4], where there is a need to model a partial order of events
with uncertain durations. Postulating that a model of uncertainty is known, task
durations are described by a random vector D = (D1, . . . , Dn) with a known
c
⃝Springer International Publishing AG 2017
D. Salvagnin and M. Lombardi (Eds.): CPAIOR 2017, LNCS 10335, pp. 302–311, 2017.
DOI: 10.1007/978-3-319-59776-8 25

Stochastic Task Networks
303
probability distribution. In project scheduling, for example, the duration Di of
task i may turn out to be shorter or longer than a nominal value according to a
certain distribution.
A given task network is typically mapped to a realized schedule (i.e. an assign-
ment of start-times to tasks) via earliest-start dispatching; i.e. observing outcome
durations and starting a task immediately when precedence-constraints allow
(i.e. not later than the maximum ﬁnish-time of its network predecessors). Ran-
dom durations make the realized start-time of a task (and the overall realized
schedule makespan) also random. Since PERT networks [17], a large body of
literature focused on the problem of determining the makespan distribution [1],
eventually shown to be a hard problem [11]. A variety of eﬃcient heuristics have
been developed so far (see [4]), among which Monte Carlo sampling remains,
perhaps, the most practical.
Consider, for example, the stochastic task network in Fig. 1, detailing the plan
of a house construction project, assuming task durations are random variables
that follow the uniform distribution within respective intervals. With earliest-
start dispatching, the overall duration of the project (i.e. the realized schedule
makespan) will range between 12 and 20 days with an expected value of a little
over 16 days.
Fig. 1. A motivating example.
This paper addresses a problem which, to our knowledge, has not been
addressed in existing literature. To motivate our problem, let us return to the
earlier example and suppose task 7 (“Paint interior”) is assigned to a painting
crew charging $100 per day. Assume we are willing to hire them for at least 4 days
(the maximum number of days they will need) and for at most 6 days; i.e. we
have a budget of $600 for painting. With earliest-start dispatching, 7 may start
within 8 to 15 days from the project start (the start-date of task 1). A challenge
that arises in this situation is deciding when to hire the painting crew, because
to allow for an expected makespan of a little over 16 days (as mentioned earlier),
we must book the painting crew from the 8-th day and until the 19-th day, at
the excessive cost of $1100. The solution we examine here, is to use a diﬀerent

304
K.S. Mountakis et al.
dispatching strategy, associating task 7 with a planned release-time, t7, before
which it may not start even if installing plumbing and electricity are ﬁnished
earlier than t7. If we choose that 7 may not start earlier than, e.g., t7 = 13 days
from the project start, we only need to book the painting crew on the 13-th day
until the 19-th day, for an acceptable cost of $600. However, the price to pay for
this stability is an expected makespan increase to a little over 17 days.
Now suppose that after assessing our budget carefully it turns out that each
task may deviate at most, say w days, from its respective planned release-time.
The emerging question addressed in this paper is:
Which planned release-times reach the desired level of stability1 while
minimizing the incurred performance penalty?
This problem does not involve resource-constraints. However, task networks
are often used in the area of resource-constrained scheduling under uncertainty
(see [2,12]) to represent solutions (e.g. the earliest-start policy [13], the partial-
order schedule [5,10,20]). Thus, our work is expected to be useful in dealing with
associated problems, such as distributing slack in a resource-feasible schedule to
make it insensitive to durational variability [8].
Organization. A formal problem statement and its LP formulation are pre-
sented in Sect. 2. As the resulting LP can be quite costly to solve, Sect. 3 presents
our main result, an eﬃcient dynamic programming algorithm. Section 4 con-
cludes the paper and outlines issues to be addressed in future work.
2
Problem Deﬁnition
We are given a task network G(V, E) and a stochastic vector D = (D1, . . . , Dn)
describing task durations. Let Q index the space of all possible realization sce-
narios for D such that dip denotes the realized duration of task i in scenario
p ∈Q. We assume to know the probability distribution of D; i.e. the probability
P[D = (d1p, . . . , dnp)] for all p ∈Q. To limit the unpredictability of the real-
ized schedule, we want to associate tasks with respective planned release-times
t = (t1, . . . , tn) such that the realized schedule is formed by starting a task as
early as permitted by precedence-constraints, but not earlier than its release-
time. That is, the start-time sjp of task j in scenario p will be determined as:
sjp = max[ max
(i,j)∈E(sip + dip), tj]
(1)
Given a sample P ⊆Q of size m of the stochastic durations vector, this paper
is devoted to the following problem:
1 As in Bidot et al. [3], stability here refers to the extent that a predictive sched-
ule (planned release-times in our case) is expected to remain close to the realized
schedule.

Stochastic Task Networks
305
min
t≥0
F :=

j∈V,p∈P
sjp
(P)
subject to
sjp = max[ max
(i,j)∈E(si + di), tj]
∀j ∈V, p ∈P
(2)
sjp −tj ≤w
∀j ∈V, p ∈P
(3)
This problem tries to optimize a trade-oﬀbetween stability and performance:
release-times are sparsely spread in time in order to form a stable schedule, i.e.
such that in every considered scenario a realized start-time will stay within w
time-units from the corresponding release-time.
Since the whole space of possible duration realizations, Q, may be too large,
or even inﬁnite, we only consider a manageable sample P ⊆Q during opti-
mization.2 At the same time, we want to ensure a minimal performance penalty
F −F ∗where F ∗denotes the throughput of earliest-start dispatching with no
release-times.3
Instead of minimizing a standard performance criterion like expected
makespan, we choose to maximize expected throughput,
1
m
n

j,p sjp , which equals
the average rate at which tasks ﬁnish over all scenarios. It can be shown that
a schedule of maximum throughput is one of minimum makespan and/or tar-
diness (in case tasks are associated with deadlines). We maximize throughput
indirectly by minimizing its inverse, with the constant m
n omitted for simplicity.
LP Formulation. The resulting problem is not easy to handle due to the
equality constraint, but using a standard trick it can be rewritten as the following
linear program (LP):
min
s,t≥0
F :=

j∈V,p∈P
sjp
(P)
subject to
sjp ≥sip + dip
(i, j) ∈E, p ∈P
(4)
sjp ≥tj
j ∈V, p ∈P
(5)
sjp −tj ≤w
j ∈V, p ∈P
(6)
Note that the solution-space of the resulting LP encompasses that of the original
formulation. However, it is easy to show that both problems have the same set of
optimal solutions, because a solution (s, t) for the LP cannot be optimal unless
it satisﬁes (2).
Currently, the best (interior-point) LP solvers have a complexity of O(N 3M)
where N is the number of variables and M the input complexity [21]. Thus,
letting δ ≤n denote the max in-degree in G(V, E), the cost of solving (P) as an
LP with nm variables and O(nδm) constraints can be bounded by O(n4m4δ) ⊆
2 Knowing the distribution of D, we assume to be able to draw P.
3 The reader can easily recognize the similarity of the proposed LP with a so-called
Sample Average Approximation (SAA) of a stochastic optimization problem [14].

306
K.S. Mountakis et al.
O(n5m4), which can be daunting even for small instances. Fortunately, as shown
in the following section, we manage to obtain the substantially tighter bound of
O(n2m) for solving (P), by exploiting its simple structure to devise a dynamic
programming algorithm.
3
Fast Computation of Planned Release-Times
We ﬁrst show that a ﬁxed relationship between variables sjp and tj can be
assumed while looking for an optimal (s, t). Based on this, a problem (P ′) is
deﬁned which can be solved instead of (P).
A Tighter Formulation. Begin by rewriting (6) as tj ≥maxp sjp −w , ∀j ∈V .
Now, let Λ denote the set of all feasible (s, t) for problem (P) and let Λ∗⊆Λ be
that part of the solution-space that only contains (s, t) for which tj = maxp sjp−
w for all j.
Lemma 1. For every feasible (s, t) ∈Λ \ Λ∗there exists (s′, t′) ∈Λ∗with equal
objective value.
Proof. Consider feasible (s, t) with tj = maxp sj∗p−w+c with c > 0 for some j∗.
Construct t′ by letting t′
j = tj for all j ̸= j∗and t′
j∗= tj∗−c = maxp sj∗p −w.
Trivially, if (s, t) is feasible, so is (s, t′), with the same objective value. Keeping
s ﬁxed, we may repeat this construction to enforce that tj = maxp sjp −w for all
j and have (s, t′) ∈Λ∗.
The previous result allows us to consider the following problem, obtained by
substituting maxp′∈P sjp′ −w for tj in (P):
min
s≥0

p
snp
(P ′)
subject to
sjp ≥sip + dip
(i, j) ∈E, p ∈P
(7)
sjp ≥max
p′∈P sjp′ −w
j ∈V, p ∈P
(8)
sjp −(max
p′∈P sjp′ −w) ≤w
j ∈V, p ∈P
(9)
Clearly, (s, t) ∈Λ∗iﬀs is feasible for (P ′).4 In other words, the solution-space of
P ′ comprises only those s that can be paired with t by letting tj = maxp sjp −w
to form a feasible (s, t) for (P). By Lemma 1, if s is optimal for (P ′), then (s, t)
is optimal for (P). Also, if (P) has a solution (i.e. if G(V, E) is acyclic), then
(P ′) also has a solution.
4 Since (s, t) ∈Λ∗implies maxp′ sjp′ −w = tj for all j.

Stochastic Task Networks
307
The Resulting STP. Formulation (P ′) is useful because it can be cast as a
certain type of Temporal Constraint Satisfaction Problem (TCSP) [9]. We start
by noting that (9) is always true and can be omitted. Moreover, (8) can be
rewritten as (11), to obtain the following reformulation:
min
s≥0

p
snp
(P ′)
subject to
sip −sjp ≤−dip
(i, j) ∈E, p ∈P
(10)
sjp −sjp′ ≤w
(p, p′) ∈P2, j ∈V
(11)
Constraints (10) and (11) eﬀectively represent the solution-space of a Simple
Temporal Problem (STP) [9] with temporal variables {sjp : j ∈V, p ∈P}.
The structure of the resulting STP (speciﬁcally, of its distance graph [9]) is
demonstrated in Fig. 2.
Fig. 2. Example task network (a) and resulting STP (b) for a sample P = {p, p′′}.
The earliest start time (est) solution of any given STP (assuming it is con-
sistent) assigns to each variable the smallest value it may take over the set of
feasible solutions. Therefore, the est solution of the resulting STP optimally
solves (P ′), leading us to the following observation.
Observation 1. By Lemma 1, an optimal solution (s, t) for (P) can be formed
by ﬁnding the earliest start time solution s of the resulting STP and pairing it
with t formed by letting tj = maxp∈P sjp −w for all j.

308
K.S. Mountakis et al.
Algorithm 1
1: l(s1p) ←0 for all p ∈P
2: for each tier j in a topological sort of G(V, E) do
3:
kjp ←min{l(sip) −dip : (i, j) ∈E} for all p ∈P
4:
p∗←arg min{kjp : p ∈P}
5:
l(sjp) ←max{kjp, kjp∗+ w} for all p ∈P
6: end for
7: sjp ←l(sjp) for all j ∈V, p ∈P
8: tj ←maxp∈P sjp −w for each j ∈V
The est value of sjp is the length of the shortest-path (in the distance graph)
from (the node corresponding to) sjp to the special-purpose variable z which is
ﬁxed to zero. Those values can be found with a single-source shortest-path algo-
rithm (e.g. Bellman-Ford [19]) in time O(NM) where N is the number of nodes
and M the number of arcs. In our case, N = nm and M = O(nmδ), yielding
O(n3m2); already a better bound than that of solving (P) as an LP. However,
in the following we obtain an even better bound with a dynamic programming
algorithm.
Computing the est Solution by Dynamic Programming. Let us associate
each task j ∈V with a corresponding tier including all nodes {sjp : p ∈P} of
the STP distance graph. A few remarks on the structure of the STP are in order.
First, due to (11) the resulting STP is not acyclic, but each cycle only includes
nodes that belong to the same tier. Second, due to (10) there is a path from each
node in tier j to each node in tier i if and only if there is a path from task i to
j in G(V, E).
Let l(sjp) denote the shortest-path length from sjp to z (i.e. the value of
variable sjp in an optimal solution of (P ′)). From the structure of the resulting
STP, we have:
l(sjp) = min

min
(i,j)∈E(l(sip) −dip), min
p′̸=p l(sjp′) + w

(12)
The existence of cycles complicates solving subproblem l(sjp) as it depends
on (and is a dependency of) other subproblems l(sjp′) in the same tier. However,
we can “break” dependencies between subproblems in the same tier as shown
below.
Deﬁne kjp := min(i,j)∈E(l(sip) −dip) and p∗:= arg minp∈P kjp.
Lemma 2. l(sjp) = min{kjp, kjp∗+ w}
Proof. Begin by noting that the shortest-path from sjp to z visits at most one node
sjp′ from the same tier. As such, for every sjp we have that: either l(sjp) = kjp,
or l(sjp) = kjp′ + w < kjp for some p′ ̸= p.

Stochastic Task Networks
309
Now, note that l(sjp∗) = kjp∗, since if not (i.e. if l(sjp∗) ̸= kjp∗), then
l(sjp∗) = kjp′ + w < kjp∗with p′ ̸= p∗, which contradicts the deﬁnition of p∗.
Last, we show that if l(sjp) ̸= kjp then l(sjp) = kjp∗+ w. Suppose not.
Since l(sjp) ̸= kjp then according to (12), l(sjp) = l(sjp′) + w but with p′ ̸= p∗.
Expanding l(sjp′) according to (12),
min{kjp′, min
p′′̸=p′ l(sjp′′) + w} + w < l(sjp∗) + w = kjp∗+ w
and since kjp′ ≥kjp∗,
min
p′′̸=p′ l(sjp′′) + w < kjp∗
⇔l(sjp∗) + w < kjp∗
which contradicts that l(sjp∗) = kjp∗.
The resulting recursion suggests a dynamic programming approach, summa-
rized in Algorithm 1. It involves solving the subproblems of one tier at a time,
visiting tiers according to a topological sort of G(V, E) (recall that tiers corre-
spond to tasks j ∈V ). Finding a topological sort takes O(nδ) [23], recalling that
δ denotes the max in-degree of a task in the network. The overall complexity of
Algorithm 1 is therefore O(nmδ) ⊆O(n2m).
4
Conclusion
Given a stochastic task network with n tasks we consider dispatching the tasks
as early as possible, subject to (planned) release-times. Assuming a sample with
m realizations of the stochastic durations vector is drawn, we deﬁned an LP
for ﬁnding optimal release-times; i.e. that minimize the performance penalty
of reaching a desired level of stability. The resulting LP is costly to solve, so
pursuing a more eﬃcient solution method we managed to show that optimal
release-times can be expressed as a function of the earliest start time solution of
an associated Simple Temporal Problem. Exploiting the structure of this STP,
we were able to deﬁne a dynamic programming algorithm for ﬁnding optimal
release-times with considerable eﬃciency, in time O(n2m).
Future Work. Since we optimize according to a manageable sample P, there
is a (potentially non-zero) probability Pv that the realized start-time of a task
deviates further than w time-units from its planned release-time. The question
of how Pv (or E[Pv] as in [6]) depends on m (the size of P) should be addressed
in future work. Furthermore, in an earlier paper [18], an LP similar to (P) was
used it in a two-step heuristic for a ﬂavor of the stochastic resource constrained
project scheduling problem (stochastic RCPSP) [15,24]. Given a resource alloca-
tion determined in a ﬁrst step, in a second step a LP was used to ﬁnd planned
release-times that minimize the total expected deviation of the realized sched-
ule from those release-times. This heuristic was found to outperform the state-
of-the-art in the area of proactive project scheduling. In future work, we shall

310
K.S. Mountakis et al.
investigate using the algorithm presented here in order to stabilize the given
resource-allocation, expecting gains in both eﬃciency and eﬀectiveness. Finally,
a potentially related problem, namely PERTCONVG, is studied by Chr´etienne
and Sourd in [7], which involves ﬁnding start-times for a task network so as to
minimize the sum of convex cost functions. In fact, their algorithm bears struc-
tural similarities to ours, since subproblems are solved in a topological order. It
would be worth investigating if their analysis can be extended in order to enable
casting the problem studied here as an instance of that problem.
References
1. Adlakha, V., Kulkarni, V.G.: A classiﬁed bibliography of research on stochastic
pert networks: 1966–1987. INFOR 27, 272–296 (1989)
2. Beck, C., Davenport, A.: A survey of techniques for scheduling with uncertainty
(2002)
3. Bidot, J., Vidal, T., Laborie, P., Beck, J.C.: A theoretic and practical framework
for scheduling in a stochastic environment. J. Sched. 12, 315–344 (2009)
4. Blaauw, D., Chopra, K., Srivastava, A., Scheﬀer, L.: Statistical timing analysis:
from basic principles to state of the art. IEEE Trans. Comput. Aided Des. Integr.
Circuits Syst. 27, 589–607 (2008)
5. Bonﬁetti, A., Lombardi, M., Milano, M.: Disregarding duration uncertainty in
partial order schedules? Yes, we can!. In: Simonis, H. (ed.) CPAIOR 2014. LNCS,
vol. 8451, pp. 210–225. Springer, Cham (2014). doi:10.1007/978-3-319-07046-9 15
6. Calaﬁore, G., Campi, M.C.: Uncertain convex programs: randomized solutions and
conﬁdence levels. Math. Program. 102, 25–46 (2005)
7. Chr´etienne, P., Sourd, F.: Pert scheduling with convex cost functions. Theoret.
Comput. Sci. 292, 145–164 (2003)
8. Davenport, A., Geﬄot, C., Beck, C.: Slack-based techniques for robust schedules.
In: Sixth European Conference on Planning (2014)
9. Dechter, R., Meiri, I., Pearl, J.: Temporal constraint networks. Artif. Intell. 49,
61–95 (1991)
10. Godard, D., Laborie, P., Nuijten, W.: Randomized large neighborhood search for
cumulative scheduling. In: ICAPS. vol. 5 (2005)
11. Hagstrom, J.N.: Computing the probability distribution of project duration in a
pert network. Networks 20, 231–244 (1990)
12. Herroelen, W., Leus, R.: Project scheduling under uncertainty: survey and research
potentials. EJOR 165, 289–306 (2005)
13. Igelmund, G., Radermacher, F.J.: Preselective strategies for the optimization of
stochastic project networks under resource constraints. Networks 13, 1–28 (1983)
14. Kleywegt, A.J., Shapiro, A., Homem-de Mello, T.: The sample average approxi-
mation method for stochastic discrete optimization. SIAM J. Optim. 12, 479–502
(2002)
15. Lamas, P., Demeulemeester, E.: A purely proactive scheduling procedure for the
resource-constrained project scheduling problem with stochastic activity durations.
J. Sched. 19, 409–428 (2015)
16. Leus, R.: Resource allocation by means of project networks: dominance results.
Networks 58, 50–58 (2011)
17. Malcolm, D.G., Roseboom, J.H., Clark, C.E., Fazar, W.: Application of a technique
for research and development program evaluation. Oper. Res. 7, 646–669 (1959)

Stochastic Task Networks
311
18. Mountakis, S., Klos, T., Witteveen, C., Huisman, B.: Exact and heuristic methods
for trading-oﬀmakespan and stability in stochastic project scheduling. In: MISTA
(2015)
19. Pallottino, S.: Shortest-path methods: complexity, interrelations and new proposi-
tions. Networks 14, 257–267 (1984)
20. Policella, N., Oddi, A., Smith, S.F., Cesta, A.: Generating robust partial order
schedules. In: Wallace, M. (ed.) CP 2004. LNCS, vol. 3258, pp. 496–511. Springer,
Heidelberg (2004). doi:10.1007/978-3-540-30201-8 37
21. Potra, F.A., Wright, S.J.: Interior-point methods. J. Comput. Appl. Math. 124,
281–302 (2000)
22. Shestak, V., Smith, J., Maciejewski, A.A., Siegel, H.J.: Stochastic robustness metric
and its use for static resource allocations. J. Parallel Distrib. Comput. 68, 1157–
1173 (2008)
23. Tarjan, R.E.: Edge-disjoint spanning trees and depth-ﬁrst search. Acta Informatica
6, 171–185 (1976)
24. Van de Vonder, S., Demeulemeester, E., Herroelen, W.: Proactive heuristic proce-
dures for robust project scheduling: an experimental analysis. EJOR 189, 723–733
(2008)

Rescheduling Railway Traﬃc on Real Time
Situations Using Time-Interval Variables
Quentin Cappart(B) and Pierre Schaus
Universit´e catholique de Louvain, Louvain-la-Neuve, Belgium
{quentin.cappart,pierre.schaus}@uclouvain.be
Abstract. In the railway domain, the action of directing the traﬃc in
accordance with an established timetable is managed by a software. How-
ever, in case of real time perturbations, the initial schedule may become
infeasible or suboptimal. Subsequent decisions must then be taken man-
ually by an operator in a very limited time in order to reschedule the
traﬃc and reduce the consequence of the disturbances. They can for
instance modify the departure time of a train or redirect it to another
route. Unfortunately, this kind of hazardous decisions can have an unpre-
dicted negative snowball eﬀect on the delay of subsequent trains. In this
paper, we propose a Constraint Programming model to help the oper-
ators to take more informed decisions in real time. We show that the
recently introduced time-interval variables are instrumental to model this
scheduling problem elegantly. We carried experiments on a large Belgian
station with scenarios of diﬀerent levels of complexity. Our results show
that the CP model outperforms the decisions taken by current greedy
strategies of operators.
1
Introduction
Since the dawn of the nineteenth century, development of railway systems has
taken a huge importance in many countries. Over the years, the number of trains,
the number of tracks, the complexity of networks increase and are still increasing.
In this context, the need of an eﬃcient and reliable train schedule is crucial.
Indeed, a bad schedule can cause train conﬂicts, unnecessary delays, ﬁnancial
losses, and a passenger satisfaction decrease. While earliest train schedules could
be built manually without using optimisation or computer based methods, it is
not possible anymore. Plenty of works deal with this problematic of building the
most appropriate schedule for general [1–3], or speciﬁc purposes [4,5].
Practical schedules must also deal with real time perturbations. Disturbances,
technical failures or simply consequences of a too optimistic theoretical schedule
can cause delays which can be propagated on other trains. The initial sched-
ule may then become infeasible. Real-time modiﬁcations of the initial schedule
therefore may be required. Several works already tackle this problem. A recent
survey (2014) initiated by Cacchiani et al. [6] recaps the diﬀerent trends on mod-
els and algorithms for real-time railway disturbance management. For instance,
Fay et al. [7] propose an expert system using fuzzy rules and Petri Net for the
c
⃝Springer International Publishing AG 2017
D. Salvagnin and M. Lombardi (Eds.): CPAIOR 2017, LNCS 10335, pp. 312–327, 2017.
DOI: 10.1007/978-3-319-59776-8 26

Rescheduling Railway Traﬃc on Real Time Situations
313
modelling. Such a method requires to deﬁne the rules, which can diﬀer according
to the station. Higgins et al. [8] propose to use local search methods in order
to solve conﬂicts. However, this work does not take into account the alterna-
tive routes that trains can have in order to reach their destination, and that the
planned route remains not always optimal in case of real time perturbations. For
that, D’Ariano et al. [9] model the problem with an alternative graph formulation
and propose a branch and bound algorithm to solve it. They suggest to enrich
their method with a local search algorithm for rerouting optimization purposes
[10]. Besides, some works consider the passenger satisfaction [11–13] in their
model. It is also known as the Delay Management Problem. Generally speaking,
most of the methods dealing with train scheduling are based on Mixed Integer
Programming [14–17]. However, the performance of Mixed Integer Programming
models for solving scheduling problems is known to be highly dependant of the
granularity of time chosen.
Constraint Based Scheduling [18], or in other words, applying Constraint Pro-
gramming on scheduling problems seems to be a good alternative over Mixed
Integer Programming. According to the survey of Bartak et al. [19], Constraint
Programming is particularly well suited for real-life scheduling applications. Fur-
thermore, several works [20–22] show that Constraint Programming can be used
for solving scheduling problems on large and realistic instances. By following this
trend, Rodriguez [23] proposes a Constraint Programming model for real-time
train scheduling at junctions. However, despite the good performances obtained,
this model can be improved. Firstly, the modelling do no use the strength of
global constraints which can provide a better propagation. Secondly, the search
can also be improved with heuristics and the use of Local Search techniques.
Finally, the objective function is only deﬁned in function of train delays without
considering the passengers or the diﬀerent categories of trains.
In this paper, we propose a new model for rescheduling the railway traﬃc in
a real time context. The contributions are as follows:
• A Constraint Programming model for rescheduling the traﬃc through real
time disturbances on the railway network. In addition to being a relevant
application for railway companies, the proposed model scales on a realistic
instance, the station of Courtrai (Belgium).
• The application of state of the art scheduling algorithms and relevant global
constraints in order to achieve a better propagation and a faster search. Con-
cretely, the conditional time-intervals introduced by Laborie et al. [24,25] as
well as their dedicated global constraints have been used. Furthermore, the
exploration of the state space has been carried out using Failure Directed
Search together with Large Neighbourhood Search [26].
• The formulation of an objective function aiming at the same time to minimise
the total delay and to maximise the overall passenger satisfaction. Further-
more, the objective function also considers the heterogeneity of the traﬃc
and diﬀerent levels of priority between trains through a deﬁned lexicograph-
ical order. For instance a freight train has a lower priority than a passenger
train.

314
Q. Cappart and P. Schaus
The implementation of the model has been performed with IBM ILOG CP
Optimizer V12.6.3 [27] which is particularly ﬁtted for designing scheduling mod-
els [28–30]. The next section describes the signalling principles and the com-
ponents considered for the modelling. Section 3 introduces the model and its
speciﬁcities. Experiments and discussions about its performances are then car-
ried in Sect. 4.
2
Signalling Principles
In the railway domain, the goal of a signalling system is to ensure a safe control of
the traﬃc [31]. Such a task encompasses the regulation of the traﬃc. In Belgium,
it is mainly performed by a software, called the Traﬃc Management System, that
automatically regulates the traﬃc according to predeﬁned rules. Let us consider
the ﬁctive station presented in Fig. 1. Several components are depicted on it:
Fig. 1. Track layout of a ﬁctive station with two Routes (R1 and R2). (Color ﬁgure
online)
• The track segments (e.g. T1) are the portions of the railway structure where
a train can be detected. They are delimited by the joints (⊣⊢).
• The signals (e.g. S1) are the devices used to control the train traﬃc. They
are set on a proceed state (green) if a train can safely move into the station
or in a stop state (red) otherwise.
Besides these physical components, signalling also involves logical structures:
• The routes correspond to the paths that trains can follow inside a station in
order to reach a destination. They are expressed in term of track segments
and signals. For instance, R1 is a route going from T4 to T7 by following the
path [T4, S2, T5, T2, T6]. The ﬁrst track segment of a route is always in front
of a signal which is initially at a stop state and which turns green when the
Traﬃc Management System allows the train to proceed. The track segment

Rescheduling Railway Traﬃc on Real Time Situations
315
used for the destination is not a part of the route. This action is called a
route activation. At this step, all the track segments forming the route
after the start signal are reserved and cannot be used by another train. Once
a route has been activated, the train can move through it in order to reach its
destination. For instance, once a train following route R1 has reached T6 and
is not on the previous track segments anymore, T5 and T2 can be released
in order to allow other trains to use them. Detailed explanations about the
route management is provided in [32].
• The itineraries correspond to a non physical path from a departure point
to an arrival point. An itinerary can be constituted of one or several routes
which can be alternative. For instance, as depicted in Fig. 1, two routes (R1
and R2) are possible in order to accomplish itinerary IT1 from T4 to T7.
In normal situations, a route is often preferred than others, but in case of
perturbations, the route causing the less conﬂicts is preferred.
According to an established timetable, the Traﬃc Management System acti-
vates routes in order to ensure the planned traﬃc. However, in case of real time
perturbations, signalling must be handled manually and the actions to perform
are decided by human operators. The procedure follows this pattern:
1. The Traﬃc Management System has predicted a future conﬂict caused by
perturbations on the traﬃc.
2. The operators controlling the traﬃc analyse the situation and evaluate the
possible actions to do in order to minimise the consequences of the perturba-
tions. Besides the safety, the criterion considered for the decision is the sum
of delays per passenger. In other words, the objective is to minimise the sum
of delays of trains pondered by their number of passenger. Furthermore, some
trains can have a higher priority than others.
3. According to the situation, they perform some actions (stopping a train,
changing its route, etc.) or do nothing.
In this work, we are interested by the actions of operators facing up real time
perturbations. In such situations, they must deal with several diﬃculties:
• The available time for analysing the situation and taking a decision can be
very short according to the criticality of the situation (less than one minute).
• For large stations with a dense traﬃc, particularly during the peak hours, the
eﬀects of an action are often diﬃcult to predict.
• The number of parameters that must be considered (type of trains, number
of passengers, etc.) complicates the decision.
Such reasons can lead railway operators to take decisions that will not
improve the situation, or worse, will degrade it. Most of the time, the deci-
sion taken is to give the priority either to the ﬁrst train arriving at a signal, or
to the ﬁrst one that must leave the station [9]. However, it is not always the
best decision. It is why we advocate the use of a Decision Support Tool based
on optimisation in order to assist operators in their decisions. The requirement
for this software is then to provide a good solution to this rescheduling problem
within a short and parametrisable computation time.

316
Q. Cappart and P. Schaus
3
Modelling
This section presents how we model the problem. Basically, the goal is to schedule
adequately trains in order to bring them to their destination. The decision is then
to chose, for each train, which route must be activated and at what time. Each
track segment can host at most one train at a time. Furthermore, they can also
be reserved only for one train. An inherent component of scheduling problems
are the activities. Roughly speaking, a classical activity A is modelled with three
variables, a start date s(A), a duration d(A) and an end date e(A). The activity
is ﬁxed if the three variables are reduced to a singleton. Our model contains
three kinds of activities that are linked together:
• The itinerary activities deﬁne the time interval when a train follows a
particular itinerary. Each train has one and only one itinerary activity. We
deﬁne At,it as the activity for itinerary it of train t.
• The route activities deﬁne the interval when a train follows a particular
route of an itinerary. We deﬁne At,it,r as the activity for route r of Itinerary
it related to train t.
• The train activities correspond to the movements of a train through the
station in order to complete a route. Such activities use the track segments as
resources. We deﬁne At,it,r
i
as the ith train activity of route r of itinerary it and
related to train t. Inside a same route, there are as many train activities as the
number of elements on the route path. The element can be a track segment
or a signal. For instance, by following the example of Fig. 1, At,IT 1,R1
1
is a
train activity related to track segment T4, At,IT 1,R1
2
to signal S2, At,IT 1,R1
3
to T5, etc.
One particularity of our problem is that some activities are optional. In other
words, they may or may not be executed in the ﬁnal schedule. For instance, let
us assume that a train has to accomplish an itinerary from T4 to T7. To do so,
it can follow either R1, or R2. If R1 is chosen, the activity related to R2 will not
be executed. For that, we model optional activities with the conditional time-
interval variables introduced by Laborie et al. [24,25] that implicitly encapsulate
the notion of optionality. It also allows an eﬃcient propagation through dedicated
global constraints (alternative and span for instance) as well as an eﬃcient
search. Roughly speaking, when an activity is ﬁxed, it can be either executed, or
non executed. If the activity is executed, it behaves as a classical activity that
executes on its time interval, otherwise, it is not considered by any constraint.
This functionality is modelled with a new variable x(A) ∈{0, 1} for each activity
A such that x(A) = 1 if the activity is executed and x(A) = 0 otherwise.
Figure 2 presents how the diﬀerent activities are organised for the case study
shown in Fig. 1 for a train t. Activity At,IT 1 is mandatory, it models the fact that
the train has to reach its destination. Alternative constraint ensures that the
train can follow only one route to do so, R1 or R2. The other route activity is
not executed. Furthermore, the start date and the end date of the chosen route
is synchronised with the itinerary activity. Span constraint enforces the route

Rescheduling Railway Traﬃc on Real Time Situations
317
activity to be synchronised with the start date of the ﬁrst track segment activity
and with the end date of the last track segment activity. For instance, At,IT 1,R1
is synchronised with the start date of At,IT 1,R1
1
and the end date of At,IT 1,R1
5
.
The detailed explanations of alternative and span constraints are provided
thereafter.
Fig. 2. Breakdown structure of the model using alternative and span constraints.
As we will see, conditional time-interval variables facilitate the construction
of our model. Let us now deﬁne the diﬀerent components of the model.
Parameters. Two entities are involved in our model: the trains and the track
segments. Table 1 recaps the parameters considered. The speed, number of pas-
sengers, and length are straightforward to understand. The estimated arrival
time of a train is a prediction of its arrival time at the station. The earliest start
time deﬁnes a lower bound on the starting time of a train. In other words, a
train cannot start its itinerary before this time. Indeed, a train cannot leave its
platform before the time announced to the passengers. The planned completion
time is the time announced on the initial schedule. It deﬁnes when the train is
supposed to arrive at a platform. It is used in the objective function in order
to compute the delays generated. The category deﬁnes the nature of the train.
More explanations about the category is provided in Sect. 3.
Decision Variables. As previously said, the problem is to chose, for each
train, which route must be activated and at what time. Such a problem can
be seen as a slightly variant of a job shop scheduling problem [33] where the
machines represent the track segments and the jobs represent train activities.
The diﬀerence are as follows:
• Some activities are optional. In other words, they may or not be executed in
the ﬁnal schedule.
• The end of a train activity must be synchronised by the start of the next one.

318
Q. Cappart and P. Schaus
Table 1. Parameters related to a train t or a track segment ts.
Entity
Parameter
Name Meaning
Train
Speed
sptt
Speed of t
Passengers
pt
Number of passengers of t
Estimated arrival time
eatt
When t arrives to the station
Earliest start time
estt
Lower bound on start time of t
Planned completion time pctt
When t must complete its journey
Category
catt
Category of t
Track segment Length
lgtts
Length of ts
• A train activity can use more than one resource. When a train is on a particu-
lar track segment, its current activity uses the current track segment as well as
the next ones that are reserved for the route. For instance, let us consider the
route activity At,IT 1,R1. The related train activities with their resources are
At,IT 1,R1
1
, At,IT 1,R1
2
, At,IT 1,R1
3
, At,IT 1,R1
4
and At,IT 1,R1
5
. The resources used by
the activities are (T4), (T4, S2), (T5, T2, T6), (T2, T6) and (T6) respectively.
Let us remember from Sect. 2 that only the track segments located after the
start signal are reserved through the route activation. Concerning the initial
track segment T4, it is released after the train has passed the start signal.
From a Constraint Based Scheduling approach, the problem is to assign a
unique value to each train activity. The decision variables and their domain are,
for all trains t, itineraries it, routes r and indexes i:
s(At,it,r
i
)

∈[eatt, horizon]
if t on track segment ts
∈[estt, horizon]
if t in front of a signal
(1)
d(At,it,r
i
)

= lgtts/spdt
if t on track segment ts
∈[0, horizon]
if t in front of a signal
(2)
e(At,it,r
i
) = s(At,it,r
i
) + d(At,it,r
i
)
(3)
x(At,it,r
i
) ∈{0, 1}
(4)
The domain is determined in order to be as restricted as possible without
removing a solution. Eq. (1) indicates that an activity cannot begin before the
estimated arrival time of t. The upper bound of the start date is deﬁned by
the time horizon considered. More details about the horizon chosen is provided
in Sect. 4. Eq. (2) models the time required to achieve the activity. If t is on a
track segment, the duration is simply the length of the track segment divided
the speed of t. Otherwise, the time that t will have to wait is unknown. Eq. (3)
is an implicit constraint of consistency. Finally, Eq. (4) states that the activity
is optional. Concerning route and itinerary activities, they are linked to train
activities through constraints.

Rescheduling Railway Traﬃc on Real Time Situations
319
Constraints. This section describes the diﬀerent constraints considered. Most
of them are expressed in term of a train, an itinerary and a route. Let us express
T as the set of trains, ITt as the set of possible itineraries for t, Rit the set of
possible routes for it ∈ITt and Nr as the number of train activities of a route
r ∈Rit. Furthermore, let us state TS as the set of all the track segments in the
station.
Precedence. This constraint (Eq. (5)) ensures that train activities must be exe-
cuted in a particular order. It links the end of a train activity At,it,r
i
with the
start of At,it,r
i+1 .
e(At,it,r
i
) = s(At,it,r
i+1 )
∀t ∈T, ∀it ∈ITt, ∀r ∈Rit, ∀i ∈[1, Nr[
(5)
All precedence constraints are aggregated into a temporal network in order
to have a better propagation [24,34]. Instead of having a bunch of independent
constraints, they are considered as a global constraint.
Execution Consistency. As previously said, some activities are alternative. If
a route is not chosen for a train, none of activities At,it,r
i
will be executed.
Otherwise, all of them must be executed. Eq. (6) states that all the train activities
related to the same environment must have the same execution status.
x(At,it,r
1
) ≡x(At,it,r
i
)
∀t ∈T, ∀it ∈ITt, ∀r ∈Rit, ∀i ∈]1, Nr]
(6)
Alternative.
Introduced by Laborie and Rogerie [24], this constraint mod-
els an exclusive alternative between a bunch of activities. It is expressed in
Eq. (7).
alternative

At,it,

At,it,rr ∈Rit

∀t ∈T, ∀it ∈ITt
(7)
It means that when At,it is executed, then exactly one of the route activities
must be executed. Furthermore, the start date and the end date of At,it must
be synchronised with the start and end date of the executed route activity. if
At,it is not executed, none of the other activities can be executed. In our model,
At,it is an mandatory activity. It models the fact that each train must reach its
destination through an itinerary but for that, it must follow exactly one route.
Span. Also introduced in [24], this constraint states that an executed activity
must span over a bunch of other executed activities by synchronising its start
date with the earliest start date of other executed activities and its end date
with the latest end date. It is expressed in Eq. 8.
span

At,it,r,

At,it,r
i
i ∈[1, Nr]

∀t ∈T, ∀it ∈ITt, ∀r ∈Rit
(8)
It models the fact that the time taken by a train to complete a route is
equal to the time required for crossing each of its components. If the route is not
chosen, then none activity will be executed. A representation of span constraint
is shown in Fig. 2.

320
Q. Cappart and P. Schaus
Unary Resource. An important constraint is that trains cannot move or reserve
a track segment that is already used for another train. It is a unary resource
constraint (Eq. (9)). Each track segment can then be reserved only once at a time.
Let us state ACTts as the set of all the train activities using track segment ts.
noOverlap

A
A ∈ACTts

∀ts ∈TS
(9)
The semantic of this constraint, as well as comparisons with existing frame-
works, is presented in [25] which extends state of the art ﬁltering methods in
order to handle conditional time-interval variables.
Train Order Consistency. This last constraint ensures that trains cannot over-
take other trains if they are on the same track segment. An illustration of this
scenario is presented in Fig. 3. Even if train t2 has a higher priority than t1, it
cannot begin its activities before t1 because t1 has an earlier estimated arrival
time. In other words, on each track segment in front of a signal, the start date
of the ﬁrst activity of each train is sorted by their estimated arrival time.
t1t1
t2t2
Fig. 3. Two trains waiting on the same track segment.
Let us state TSB ⊂TS as the set of all ﬁrst track segments, Ntsb as the
number of trains beginning their itinerary on track segment tsb, and (ABtsb
i
) as
the sequence of the ﬁrst activities of trains ti beginning on tsb with i ∈[1, Ntsb].
The sequence is ordered by the estimated arrival time of trains ti. Then, train
order consistency constraint can be expressed through Eq. (10).
s(ABtsb
i−1) < s(ABtsb
i
)
∀tsb ∈TSB, ∀i ∈]1, Ntsb]
(10)
These constraints are also considered in the temporal network.
Objective Function. The criterion frequently used for the objective function
is the sum of train delays [23]. Let us state jctt as the journey completion time
of a train t. It corresponds to the end date of the last train activity of t. The
delay dt of t is expressed in Eq. 11.
dt = max

0, jctt −pctt

(11)
The max function is used to nullify the situation where t is in advance on its
schedule. Eq. 12 presents a ﬁrst objective function.

Rescheduling Railway Traﬃc on Real Time Situations
321
min
 
t∈T
dt

(12)
However, in real circumstances, railway operators must also consider other
parameters such as the number of passengers and the priority of trains. Section 3
introduced the category parameter, here are the diﬀerent categories considered:
1. Maintenance or special vehicles (C1).
2. Passenger trains with a correspondence (C2).
3. Simple passenger trains (C3).
4. Freight trains (C4).
Such categories are sorted with a decreasing order according to their priority.
Special vehicles have then the highest priority and freight trains the lowest.
Furthermore, if a passenger train has more passengers than another one, the cost
of the delay will be more important. The objective function is then threefold:
• Scheduling trains according to their priority. For instance, a maintenance
vehicle must be scheduled before a passenger train, if possible.
• Minimising the sum of delays.
• Maximising the overall passenger satisfaction. The passenger satisfaction
decreases if its train is late.
A second objective function can then be expressed (Eq. 13).
lex min
	 
t∈C1
dt,

t∈C2
pt × dt,

t∈C3
pt × dt,

t∈C4
dt

(13)
Where pt corresponds to the number of passengers of train t, as deﬁned in
Table 1. This equation gives a lexicographical ordering of trains according to
their category from C1 to C4. For passenger categories (C2 and C3) the delay
is expressed by passengers. In this way, the more is the number of passenger,
the greater will be the penalty for delays. The objective is then to minimise this
expression with regard to its lexicographical ordering.
Search Phase. The exploration of the state space is performed with the algo-
rithm of Vilim et al. [26] which combines a Failure-Directed Search with Large
Neighborhood Search. The implementation proposed on CP Optimizer V12.6.3
[27] is particularly ﬁtted to deal with conditional time-interval variables, prece-
dence constraints, and optional resources. The search is performed on execution
and on start date variables. Concerning the variable ordering, trains are sorted
according to their category. For instance, activities related to passenger trains
will be assigned before activities of freight trains. The value ordering is let by
default.

322
Q. Cappart and P. Schaus
4
Experiments
This section evaluates the performance of the Constraint Programming model
through diﬀerent experiments. Concretely, we compare our solution with the
solutions obtained with classical dispatching methods on a realistic station:
Courtrai. Its track layout is presented in Fig. 4. It contains 23 track segments,
14 signals, 68 itineraries and 84 possible routes. Three systematic strategies are
commonly used in practice:
• First Come First Served (FCFS) strategy gives the priority to the trains
according to their estimated arrival time.
• Highest Delay First Served (HDFS) strategy gives the priority to the
train that has the earliest planned completion time.
• Highest Priority First Served (HPFS) strategy gives the priority to a
train belonging to the category with the highest priority. The planned com-
pletion time is then used as a tie breaker for trains of a same category.
Fig. 4. Track layout of Courtrai.
Furthermore, three meta-parameters must be considered for the experiments,
the time horizon, the decision time and the number of trains. The time horizon
deﬁnes an upper bound on the estimated arrival time of trains. According to
D’Ariano et al. [9], the practical time horizon for railway managers is usually less
than one hour. In our experiments, we considered three time horizons (30 min,
one hour and two hours). The decision time is the time that railway operators
have at disposal for taking a decision. It is highly dependant to the criticality of
the current situation. However, according to Rodriguez [23], the system must be

Rescheduling Railway Traﬃc on Real Time Situations
323
able to provide an acceptable solution within 3 min for practical uses. Concerning
the number of trains, we considered scenarios having 5, 10, 15, 20, 25 and 30
trains.
Experiments have be done under the assumption that we allow a feasible
schedule. This responsibility is beyond the scope of the station. Two situations
are considered: a homogeneous and a heterogeneous traﬃc.
All the experiments have been realised on a MacBook Pro with a 2.6 GHz
Intel Core i5 processor and with a RAM of 16 Go 1600 MHz DDR3 using a 64-Bit
HotSpot(TM) JVM 1.8 on El Capitan 10.11.15. The model has been designed
with CP Optimizer V12.6.3 and the optimisation is performed with four workers.
Homogeneous Traﬃc. In this ﬁrst situation, the number of passengers and
the category are not considered. Each train has then the same priority and Eq. 12
is the objective function used. Table 2 recaps the experiments performed. Each
scenario is repeated one hundred times with a random schedule. The diﬀerent
values of the schedule are generated randomly with uniform distributions. For
instance, let us consider a schedule from 1 pm to 3 pm with 10 trains. For each
train, we randomly choose its itinerary among the set of possible itineraries, its
departure time and its expected arrival time in the interval [1 pm, 3 pm]. The
delay indicated for each strategy corresponds to the arithmetic mean among
all the tests. For each scenario, the average, the minimum and the maximum
improvement ratio of the CP solution in comparison to the best solution obtained
with classical methods is also indicated. POS indicates the number of tests where
the CP approach has improved the solution while OPT indicates the number of
tests where CP has reached the optimum.
As we can see, CP improves the solution for almost all the tests. The average
improvement ratio is above 20% in all the scenarios. Optimum is often reached
(more than 75% of the instances) when 10 trains or less are considered.
Heterogeneous Traﬃc. In this second situation, we use Eq. 13 for the objec-
tive function. The number of passengers and the category are then considered.
As for the heterogeneous case, such values are chosen randomly with a uniform
distribution. Among the classical approaches, only HPFS deals with an het-
erogeneous traﬃc. Table 3 recaps the experiments performed. Optimisations are
performed sequentially for each category. The time is allocated according to the
priority of categories. The allocation is then not done a priori but dynamically
according to the time taken by the successive optimisations. Unlike the previous
experiments, we do not compute the improvement ratio, but the number of expe-
riences where CP has improved the solution obtained with HPFS. Our choice
was motivated by the subjective aspect of deﬁning an improvement ratio for a
heterogeneous traﬃc. For instance there is no clear preference between decreas-
ing the delay of one train of category C1 and of 10 trains with lower priorities.
This kind of questions usually requires the consideration of signalling operators.
We consider that CP has improved the solution when the sum of delay per cate-
gory is lexicographically lower than the result provided by HDFS. For the three

324
Q. Cappart and P. Schaus
Table 2. Comparison between CP and classical scheduling approaches for an homoge-
neous traﬃc with a decision time of 3 min.
Horizon
# Trains
Average delay (min.)
Improvement ratio (%) POS
OPT
FCFS
HDFS
CP
Mean
Min
Max
(x/100)
(x/100)
2 h
5
192.06
191.10
148.91
22.08
-7.69
100.00
99
97
10
800.40
797.82
575.04
27.92
5.13
66.08
100
85
15
1917.95
1896.64
1341.53
29.27
1.16
58.549
100
28
20
3457.45
3397.03
2414.45
28.92
10.26
53.29
100
0
25
5581.10
5632.69
3993.04
28.45
8.58
48.37
100
0
30
8004.84
8018.76
5714.69
28.61
14.35
43.61
100
0
1 h
5
225.36
228.75
72.06
23.65
0.71
100.00
100
96
10
911.01
906.32
664.18
26.72
3.96
62.41
100
83
15
2128.34
2104.95
1494.67
28.99
5.93
51.29
100
17
20
3675.72
3680.00
2612.6
28.92
8.25
55.86
100
1
25
5971.54
6004.81
4246.55
28.89
9.41
53.67
100
0
30
8595.56
8576.92
6085.51
29.05
7.89
45.51
100
0
30 min
5
231.16
229.06
173.95
24.06
1.49
100.00
100
94
10
950.30
929.71
692.18
25.55
3.12
64.32
100
83
15
2145.36
2161.20
1535.86
28.41
7.927
51.61
100
14
20
3858.67
3888.29
2728.0
29.30
6.63
52.50
100
0
25
6137.02
6135.59
4320.14
29.59
7.75
53.38
100
0
30
8863.49
8775.84
6357.08
27.56
8.72
49.08
100
0
horizons considered and for 100 tests per scenario, CP improves the solution for
almost all the tests, even when the optimum is not reached.
Scalability. This experiment deals with the scalability of the CP model in func-
tion with the decision time. We observed that setting the decision time to 10 min
instead of 3 min do not increase signiﬁcantly the performances. The gain of the
improvement ratio is less than 1% for 60 random instances on an homogeneous
traﬃc of 5, 10, 15, 20, 25 or 30 trains (10 instances per conﬁguration). We can
then conclude that even if the CP approach gives a feasible and competitive
solution within 3 min, the quality of the solution do not increase signiﬁcantly
with time.
Criticality. In some cases, railway operators do not have an available decision
time of 180 s, they have to react almost instantly because of the criticality of
the situation. For this reason, we analysed how the CP model performs with a
decision time lower than 10 s. To do so, we recorded the number of experiments
where the CP approach has improved the solution in comparison to FCFS and
HDFS strategies. For the scenarios depicted in Table 2, we observed that CP
provides a same or better solution in more than 99% of the cases and can then
also be used to deal with critical situations.

Rescheduling Railway Traﬃc on Real Time Situations
325
Table 3. Comparison between CP and HPFS approach for an heterogeneous traﬃc
with a decision time of 3 min.
Horizon # Trains POS (x/100) OPT (x/100)
2 h
5
100
99
10
98
87
15
100
64
20
100
51
25
100
13
30
100
5
1 h
5
100
99
10
99
89
15
100
75
20
100
51
25
100
22
30
100
8
30 min
5
100
99
10
99
87
15
100
75
20
100
52
25
100
26
30
100
7
Reproducibility. A shortcoming in the literature about this ﬁeld of research
is the lack of reproducibility. To overcome this lack, we decided to provide infor-
mation about our instances and the tests performed1. The information provided
are enough to build a model, perform experiments and to compare them with
ours.
5
Conclusion
Nowadays, railway operators must deal with the problem of rescheduling the
railway traﬃc in case of real time perturbations in the network. However, the
systematic and greedy strategies (FCFS, HDFS and HPFS) currently used for
this purpose often give a suboptimal decision. In this paper, we presented a CP
model for rescheduling the railway traﬃc on real time situations. The modelling
is based on the recently introduced time-interval variables. Such a structure
allows to design the model elegantly with variables and global constraints espe-
cially dedicated for scheduling. Finally, an objective function taking into account
the heterogeneity of the traﬃc is presented. Experiments have shown that a dis-
patching better than the classical approaches is obtained in less than 3 min in
1 Available at https://bitbucket.org/qcappart/qcappart opendata.

326
Q. Cappart and P. Schaus
almost all the situations that can occur in a large station, even when the opti-
mum is not reached.
Two aspects are considered for our future works: improving the model and its
scalability. Firstly, we plan to modify the model and analyse the consequences.
For instance, we will use a xor instead of alternative constraint. Secondly, we
will consider experiments on larger areas covering several stations.
Acknowledgments. This research is ﬁnanced by the Walloon Region as part of the
Logistics in Wallonia competitiveness pole. Experiments have been performed with
ILOG CP Optimizer through the academic initiative of IBM.
References
1. Higgins, A., Kozan, E., Ferreira, L.: Optimal scheduling of trains on a single line
track. Transp. Res. Part B Methodol. 30, 147–161 (1996)
2. Ghoseiri, K., Szidarovszky, F., Asgharpour, M.J.: A multi-objective train schedul-
ing model and solution. Transp. Res. Part B Methodol. 38, 927–952 (2004)
3. Zhou, X., Zhong, M.: Bicriteria train scheduling for high-speed passenger railroad
planning applications. Eur. J. Oper. Res. 167, 752–771 (2005)
4. Harabor, D., Stuckey, P.J.: Rail capacity modelling with constraint programming.
In: Quimper, C.-G. (ed.) CPAIOR 2016. LNCS, vol. 9676, pp. 170–186. Springer,
Cham (2016). doi:10.1007/978-3-319-33954-2 13
5. Senthooran, I., Wallace, M., Koninck, L.: Freight train threading with diﬀerent
algorithms. In: Michel, L. (ed.) CPAIOR 2015. LNCS, vol. 9075, pp. 393–409.
Springer, Cham (2015). doi:10.1007/978-3-319-18008-3 27
6. Cacchiani, V., Huisman, D., Kidd, M., Kroon, L., Toth, P., Veelenturf, L.,
Wagenaar, J.: An overview of recovery models and algorithms for real-time railway
rescheduling. Transp. Res. Part B Methodol. 63, 15–37 (2014)
7. Fay, A.: A fuzzy knowledge-based system for railway traﬃc control. Eng. Appl.
Artif. Intell. 13, 719–729 (2000)
8. Higgins, A., Kozan, E., Ferreira, L.: Heuristic techniques for single line train
scheduling. J. Heuristics 3, 43–62 (1997)
9. Dariano, A., Pacciarelli, D., Pranzo, M.: A branch and bound algorithm for schedul-
ing trains in a railway network. Eur. J. Oper. Res. 183, 643–657 (2007)
10. D’Ariano, A., Corman, F., Pacciarelli, D., Pranzo, M.: Reordering and local rerout-
ing strategies to manage train traﬃc in real time. Transp. Sci. 42, 405–419 (2008)
11. Schachtebeck, M., Sch¨obel, A.: To wait or not to wait and who goes ﬁrst? delay
management with priority decisions. Transp. Sci. 44, 307–321 (2010)
12. Dollevoet, T., Huisman, D., Schmidt, M., Sch¨obel, A.: Delay management with
rerouting of passengers. Transp. Sci. 46, 74–89 (2012)
13. Dollevoet, T., Huisman, D., Kroon, L., Schmidt, M., Sch¨obel, A.: Delay manage-
ment including capacities of stations. Transp. Sci. 49, 185–203 (2014)
14. Corman, F., Goverde, R.M.P., D’Ariano, A.: Rescheduling dense train traﬃc over
complex station interlocking areas. In: Ahuja, R.K., M¨ohring, R.H., Zaroliagis,
C.D. (eds.) Robust and Online Large-Scale Optimization. LNCS, vol. 5868, pp.
369–386. Springer, Heidelberg (2009). doi:10.1007/978-3-642-05465-5 16
15. Foglietta, S., Leo, G., Mannino, C., Perticaroli, P., Piacentini, M.: An optimized,
automatic TMS in operations in Roma Tiburtina and monfalcone stations. WIT
Trans. Built Environ. 135, 635–647 (2014)

Rescheduling Railway Traﬃc on Real Time Situations
327
16. Araya, S., Abe, K., Fukumori, K.: An optimal rescheduling for online train traﬃc
control in disturbed situations. In: The 22nd IEEE Conference on Decision and
Control, 1983, pp. 489–494. IEEE (1983)
17. Lamorgese, L., Mannino, C.: An exact decomposition approach for the real-time
train dispatching problem. Oper. Res. 63, 48–64 (2015)
18. Baptiste, P., Le Pape, C., Nuijten, W.: Constraint-Based Scheduling: Applying
Constraint Programming to Scheduling Problems, vol. 39. Springer Science & Busi-
ness Media, US (2012)
19. Bart´ak, R., Salido, M.A., Rossi, F.: New trends in constraint satisfaction, planning,
and scheduling: a survey. Knowl. Eng. Rev. 25, 249–279 (2010)
20. Kelareva, E., Brand, S., Kilby, P., Thi´ebaux, S., Wallace, M., et al.: CP and MIP
methods for ship scheduling with time-varying draft. In: ICAPS (2012)
21. Kelareva, E., Tierney, K., Kilby, P.: CP methods for scheduling and routing
with time-dependent task costs. In: Gomes, C., Sellmann, M. (eds.) CPAIOR
2013. LNCS, vol. 7874, pp. 111–127. Springer, Heidelberg (2013). doi:10.1007/
978-3-642-38171-3 8
22. Ku, W.Y., Beck, J.C.: Revisiting oﬀ-the-shelf mixed integer programming and con-
straint programming models for job shop scheduling. Dept Mech. Ind. Eng., Univ.
Toronto, Toronto, ON, Canada, Technical report. MIE-OR-TR2014-01 (2014)
23. Rodriguez, J.: A constraint programming model for real-time train scheduling at
junctions. Transp. Res. Part B Methodol. 41, 231–245 (2007)
24. Laborie, P., Rogerie, J.: Reasoning with conditional time-intervals. In: FLAIRS
Conference, pp. 555–560 (2008)
25. Laborie, P., Rogerie, J., Shaw, P., Vil´ım, P.: Reasoning with conditional time-
intervals. Part II: an algebraical model for resources. In: FLAIRS Conference (2009)
26. Vil´ım, P., Laborie, P., Shaw, P.: Failure-directed search for constraint-based
scheduling. In: Michel, L. (ed.) CPAIOR 2015. LNCS, vol. 9075, pp. 437–453.
Springer, Cham (2015). doi:10.1007/978-3-319-18008-3 30
27. Laborie, P.: IBM ILOG CP optimizer for detailed scheduling illustrated on three
problems. In: Hoeve, W.-J., Hooker, J.N. (eds.) CPAIOR 2009. LNCS, vol. 5547,
pp. 148–162. Springer, Heidelberg (2009). doi:10.1007/978-3-642-01929-6 12
28. Vil´ım, P.: Max energy ﬁltering algorithm for discrete cumulative resources. In:
Hoeve, W.-J., Hooker, J.N. (eds.) CPAIOR 2009. LNCS, vol. 5547, pp. 294–308.
Springer, Heidelberg (2009). doi:10.1007/978-3-642-01929-6 22
29. Salido, M.A., Escamilla, J., Barber, F., Giret, A., Tang, D., Dai, M.: Energy-aware
parameters in job-shop scheduling problems. In: GREEN-COPLAS 2013: IJCAI
2013 Workshop on Constraint Reasoning, Planning and Scheduling Problems for
a Sustainable Future, pp. 44–53(2013)
30. Hait, A., Artigues, C.: A hybrid CP/MILP method for scheduling with energy
costs. Eur. J. Ind. Eng. 5, 471–489 (2011)
31. Theeg, G.: Railway Signalling & Interlocking: International Compendium. Eurail-
press (2009)
32. Cappart, Q., Schaus, P.: A dedicated algorithm for veriﬁcation of interlocking
systems. In: Skavhaug, A., Guiochet, J., Bitsch, F. (eds.) SAFECOMP 2016. LNCS,
vol. 9922, pp. 76–87. Springer, Cham (2016). doi:10.1007/978-3-319-45477-1 7
33. Yamada, T., Nakano, R.: Job shop scheduling. IEE Control Engineering Series,
p. 134 (1997)
34. Dechter, R., Meiri, I., Pearl, J.: Temporal constraint networks. Artif. Intell. 49,
61–95 (1991)

Dynamic Temporal Decoupling
Kiriakos Simon Mountakis1, Tomas Klos2, and Cees Witteveen1(B)
1 Delft University of Technology, Delft, The Netherlands
C.Witteveen@tudelft.nl
2 Utrecht University, Utrecht, The Netherlands
Abstract. Temporal decoupling is a method to distribute a temporal
constraint problem over a number of actors, such that each actor can
solve its own part of the problem. It then ensures that the partial solu-
tions provided can be always merged to obtain a complete solution. This
paper discusses static and dynamic decoupling methods oﬀering maximal
ﬂexibility in solving the partial problems. Extending previous work, we
present an exact O(n3) ﬂexibility-maximizing static decoupling method.
Then we discuss an exact O(n3) method for updating a given decoupling,
whenever an actor communicates a commitment to a particular set of
choices for some temporal variable. This updating method ensures that:
(i) the ﬂexibility of the decoupling never decreases and (ii) every com-
mitment once made is respected in the updated decoupling. To ensure
an eﬃcient updating process, we introduce a fast heuristic to construct
a new decoupling given an existing decoupling in nearly linear time. We
present some experimental results showing that, in most cases, updating
an existing decoupling in case new commitments for variables have been
made, signiﬁcantly increases the ﬂexibility of making commitments for
the remaining variables.
Keywords: Simple Temporal Problem · Decoupling · Flexibility
1
Introduction
In quite a number of cases a joint task has to be performed by several actors,
each controlling only a part of the set of task constraints. For example, consider
making a joint appointment with a number of people, constructing a building
that involves a number of (sub)contractors, or ﬁnding a suitable multimodal
transportation plan involving diﬀerent transportation companies. In all these
cases, some task constraints are under the control of just one actor (agent), while
others require more than one agent setting the right values to the variables to
satisfy them. Let us call the ﬁrst type of constraints intra-agent constraints and
the second type inter-agent constraints.
If there is enough time, solving such multi-actor constraint problems might
involve consultation, negotiation or other agreement technologies. Sometimes,
however, we have to deal with problems where communication between agents
during problem solving is not possible or unwanted. For example, if the agents
are in competition, there are legal or privacy issues preventing communication,
or there is simply no time for communication.
c
⃝Springer International Publishing AG 2017
D. Salvagnin and M. Lombardi (Eds.): CPAIOR 2017, LNCS 10335, pp. 328–343, 2017.
DOI: 10.1007/978-3-319-59776-8 27

Dynamic Temporal Decoupling
329
In this paper we use an approach to solve multi-actor constraint problems in
the latter contexts: instead of using agreement technologies, we try to avoid them
by providing decoupling techniques. Intuitively, a decoupling technique modiﬁes
a multi-actor constraint system such that each of the agents is able to select a
solution for its own set of (intra-agent) constraints and a simple merging of all
individual agent solutions always satisﬁes the total set of constraints. Usually,
this is accomplished by tightening intra-agent constraints such that inter-agent
constraints are implied. Examples of real-world applications of such decoupling
techniques can be found in e.g. [2,8].
Quite some research focuses on ﬁnding suitable decoupling techniques
(e.g., [1,2,7]) for Simple Temporal Problems (STPs) [3]. An STP S = (T, C)
is a constraint formalism where a set of temporal variables T = {t0, t1, . . . , tn}
are subject to binary diﬀerence constraints C and solutions can be found in low
polynomial (O(n3)) time. Since STPs deal with temporal variables, a decoupling
technique applied to STPs is called temporal decoupling. The quality of a tempo-
ral decoupling technique depends on the degree to which it tightens intra-agent
constraints: the more it restricts the ﬂexibility of each individual agent in solving
their own part of the constraints, the less it is preferred. Therefore, we need a
ﬂexibility metric to evaluate the quality of a temporal decoupling.
Originally, ﬂexibility was measured by summing the diﬀerences between the
highest possible (latest) and the lowest possible (earliest) values of all variables
in the constraint system after decoupling ([6,7]). This so-called naive ﬂexibility
metric has been criticized, however, because it does not take into account the
dependencies between the variables and, in general, seriously overestimates the
“real” amount of ﬂexibility. An alternative metric, the concurrent ﬂexibility met-
ric has been proposed in [15]. This metric accounts for dependencies between
the variables and is based on the notion of an interval schedule for an STP S:
For each variable ti ∈T an interval schedule deﬁnes an interval [li, ui], such that
choosing a value within [li, ui] for each variable ti, always constitutes a solution
for S. The sum n
i=1(ui −li) of the widths of these intervals determines the
ﬂexibility of S. The concurrent ﬂexibility of S then is deﬁned as the maximum
ﬂexibility we can obtain for an interval schedule of S and can be computed in
O(n3) (see [9]).
As shown in [15], the concurrent ﬂexibility metric can also be used to obtain
an optimal (i.e. maximum ﬂexible) temporal decoupling of an STP. This decou-
pling is a total decoupling, that is, a decoupling where the n variables are evenly
distributed over n independent agents and thus every agent controls exactly one
variable. It has been shown that this total decoupling is optimal for every parti-
tioning of the set of temporal variables if one considers the concurrent ﬂexibility
metric as the ﬂexibility metric to be used. In this paper, we concentrate on such
(optimal) total decouplings of an STP.
In all existing approaches, a single temporal decoupling is computed in
advance and is not changed, even if later on some agents announce their
commitment to a speciﬁc value (or range of values) for a variable they control.
Intuitively, however, we can use such additional information for the beneﬁt of all

330
K.S. Mountakis et al.
agents, by possibly increasing the ﬂexibility of variables they are not yet com-
mitted to. Speciﬁcally, when an agent announces a commitment to a sub-range
of values within the given interval schedule (that represents the current decou-
pling), we are interested in updating the decoupling such that the individual
ﬂexibility of no agent is aﬀected negatively.
More precisely, the overall aim of this paper is to show that a decoupling
update method with the following nice properties do exist: ﬁrst of all, it never
aﬀects the current ﬂexibility of the agents negatively, and, secondly, it never
decreases (and possibly increases) the individual ﬂexibility of the variables not
yet committed to. We will also show that updating a temporal decoupling as the
result of a commitment for a single variable can be done in almost linear (amor-
tized) time (O(n log n)), which compares favourably with the cost of computing
a new optimal temporal decoupling (O(n3)).
Organisation. In Sect. 2 we discuss existing relevant work on STPs and tempo-
ral decoupling (TD). Then, in Sect. 3, extending some existing work, we brieﬂy
show how a total TD can be computed in O(n3), using a minimum matching
approach. In Sect. 4, we ﬁrst provide an exact approach to update an existing
decoupling after commitments to variables have been made. We conclude, how-
ever, that this adaptation is computationally quite costly. Therefore, in Sect. 5
we oﬀer an alternative, heuristic method, that is capable to adapt a given tem-
poral decoupling in almost linear time per variable commitment. To show the
beneﬁts of adapting the temporal decoupling, in Sect. 6 we present the results
of some experiments using STP benchmarks sets with the heuristic decoupling
update and compare the results with an exact, but computationally more inten-
sive updating method. We end with stating some conclusions and suggestions
for further work.
2
Preliminaries
Simple Temporal Problems.
A Simple Temporal Problem (STP) (also
known as a Simple Temporal Network (STN)) is a pair S = (T, C), where
T = {t0, t1, . . . , tn} is a set of temporal variables (events) and C is a ﬁnite
set of binary diﬀerence constraints tj −ti ≤cij, for some real number cij.1 The
problem is to ﬁnd a solution, that is a sequence (s0, s1, s2, . . . , sn) of values such
that, if each ti ∈T takes the value si, all constraints in C are satisﬁed. If such a
solution exists, we say that the STN is consistent. 2 In order to express absolute
time constraints, the time point variable t0 ∈T, also denoted by z, is used. It
represents a ﬁxed reference point on the timeline, and is always assigned the
value 0.
1 If both tj −ti ≤cij and ti −tj ≤cji are speciﬁed, we sometimes use the more
compact notations −cji ≤tj −ti ≤cij or tj −ti ∈[−cji, cij].
2 W.l.g., in the remainder of the paper we simply assume an STN to be consistent.
Consistency of an STN can be determined in low-order polynomial time [4].

Dynamic Temporal Decoupling
331
Example 1. Consider two trains 1 and 2 arriving at a station. Train 1 has to
arrive between 12:05 and 12:15, train 2 between 12:08 and 12:20. People traveling
with these trains need to change trains at the station. Typically, one needs at
least 3 min for changing trains. Train 1 will stay during 5 min at the station
and train 2 stays 7 min before it departs. Let ti denote the arrival time for train
i = 1, 2. Let t0 = z = 0 represent noon (12:00). To ensure that all passengers have
an opportunity to change trains, we state the following STP S = (T, C) where:
T = {z, t1, t2}, and C = {5 ≤t1 −z ≤15, 8 ≤t2 −z ≤20, −2 ≤t2 −t1 ≤4}.
As the reader may verify, two possible solutions3 for this STP are s = (0, 10, 10)
and s′ = (0, 15, 17). That is, there is a solution when both trains arrive at 12:10,
and there is also a solution where train 1 arrives at 12:15, while train 2 arrives
at 12:17.
■
An STP S = (T, C) can also be speciﬁed as a directed weighted graph
GS = (TS, ES, lS) where the vertices TS represent variables in T and for every
constraint tj −ti ≤cij in C, there is a directed edge (ti, tj) ∈ES labeled by its
weight lS(ti, tj) = cij. The weight cij on the arc (ti, tj) can be interpreted as the
length of the path from ti to tj. Using a shortest path interpretation of the STP
S = (T, C), there is an eﬃcient method to ﬁnd all shortest paths between pairs
(ti, tj) using e.g. Floyd and Warshall’s all-pairs shortest paths algorithm [5]. A
shortest path between time variables ti and tj then corresponds to a tightest
constraint between ti and tj. These tightest constraints can be collected in an
n×n minimum distance matrix D = [dij], containing for every pair of time-point
variables ti and tj the length dij of the shortest path from ti to tj in the distance
graph. In particular, the ﬁrst row and the ﬁrst column of the distance matrix
D contain useful information about the possible schedules for S: The sequence
lst = (d00, d01, . . . , d0n) speciﬁes the latest starting time solution for each time
point variable ti. Analogously, est = (−d00, −d10, . . . , −dno) speciﬁes the earliest
starting time solution.
Example 2. Continuing Example 1, the minimum distance matrix D of S equals
D =
⎡
⎣
0 15 19
−5 0 4
−8 2 0
⎤
⎦
The earliest time solution therefore is est = (0, 5, 8), and the latest time solution
lst = (0, 15, 19).
■
Given S, the matrix D can be computed in low-order polynomial (O(n3))
time, where n = |T|, see [3].4 Hence, using the STP-machinery we can ﬁnd
earliest and latest time solutions quite eﬃciently.
3 Of course, there are many more.
4 An improvement by Planken et al. [12] has shown that a schedule can be found in
O(n2wd)-time where wd is the graph width induced by a vertex ordering.

332
K.S. Mountakis et al.
Temporal Decoupling. In order to ﬁnd a solution for an STP S = (T, C) all
variables ti ∈T should be assigned a suitable value. Sometimes these variables
are controlled by diﬀerent agents. That is, T −{z} = {t1, . . . , tn} is partitioned
into k non-empty and non-overlapping subsets T1, . . . , Tk of T, each Tj corre-
sponding to the set of variables controlled by agent aj ∈A = {a1, a2, . . . , ak}.
Such a partitioning of T −{z} will be denoted by [Ti]k
i=1. In such a case, the
set of constraints C is split in a set Cintra = k
i=1 Ci of intra-agent constraints
and a set Cinter = C −Cintra of inter-agent constraints. Here, a constraint
ti −tj ≤cji is an intra-constraint occurring in Ch if there exists a subset Th
such that ti, tj ∈Th, else, it is an inter-agent constraint. Given the partitioning
[Tj]k
j=1, every agent ai completely controls the (sub) STP Si = (Ti ∪{z}, Ci),
where Ci is its set of intra-agent constraints, and determines a solution si for
it, independently from the others. In general, however, it is not the case that,
using these sub STPs, merging partial solutions si will always constitute a total
solution to S:
Example 3. Continuing Example 1, let train i be controlled by agent ai for i =
1, 2 and assume that we have computed the set of tightest constraints based on
C. Then S1 = ({z, t1}, {5 ≤t1 −z ≤15}) and S2 = ({z, t2}, {8 ≤t2 −z ≤19}).
Agent a1 is free to choose a time t1 between 5 and 15 and suppose she chooses
10. Agent a2 controls t2 and, therefore, can select a value between 8 and 19.
Suppose he chooses 16. Now clearly, both intra-agent constraints are satisﬁed,
but the inter-agent constraint t2 −t1 ≤4 is not, since 16 −10 > 4. Hence, the
partial solutions provided by the agents are not conﬂict-free.
■
The temporal decoupling problem for S = (T, C), given the partitioning [Tj]k
j=1,
is to ﬁnd a suitable set C′
intra = k
i=1 C′
i of (modiﬁed) intra-agent constraints
such that if s′
i is an arbitrary solution to S′
i = (Ti ∪{z}, C′
i), it always can be
merged with other partial solutions to a total solution s′ for S.5
We are interested, however, not in arbitrary decouplings, but an optimal
decoupling of the k agents, allowing each agent to choose an assignment for its
variables independently of others, while maintaining maximum ﬂexibility. This
optimal decoupling problem has been solved in [14] for the total decoupling
case, that is the case where k = n and each agent ai controls a single time point
variable ti. In this case, the decoupling results in a speciﬁcation of a lower bound
li and an upper bound ui for every time point variable ti ∈T, such that ti can
take any value vi ∈[li, ui] without violating any of the intra- or inter-agent
constraints. This means that if agent ai controls Ti then her set of intra-agent
constraints is Ci = {lj ≤tj ≤uj | tj ∈Ti}.
The total ﬂexibility the agents have, due to this decoupling, is determined by
the sum of the diﬀerences (ui −li). Therefore, the decoupling bounds l = (li)n
i=1
and u = (ui)n
i=1 are chosen in such a way that the ﬂexibility 
i(ui −li) is
maximized. It can be shown (see [14]) that such a pair (l, u) can be obtained as
a maximizer of the following LP:
5 In other words, the set k
i=1 C′
i logically implies the set C of original constraints.

Dynamic Temporal Decoupling
333
max
l,u

i
(ui −li)
(TD(D))
s.t.
l0 = u0 = 0
(1)
li ≤ui
∀i ∈T
(2)
uj −li ≤dij
∀i ̸= j ∈T
(3)
where D is the minimum distance matrix associated with S.
Example 4. Consider the matrix D in Example 2 and the scenario sketched in
Example 3. Then the LP whose maximizers determine a maximum decoupling
is the following:
max
l,u

i
(ui −li)
s.t.
u0 = l0 = 0,
l1 ≤u1,
l2 ≤u2
u1 −l0 ≤15,
u2 −l0 ≤19
u0 −l1 ≤−5,
u2 −l1 ≤4
u0 −l2 ≤−8,
u1 −l2 ≤2
Solving this LP, we obtain 2
i=0(ui −li) = 6 with maximizers l = (0, 15, 13)
and u = (0, 15, 19). This implies that in this decoupling (l, u) agent a1 is
forced to arrive at 12:15, while agent a2 can choose to arrive between 12:13 and
12:19.
■
Remark. Note that the total decoupling solution (l, u) for S also is a solution
for a decoupling based on an arbitrary partitioning [Ti]k
i=1 of S. Observe that
(l, u) is a decoupling, if for any value vi chosen by ah and any value wj chosen
by ah′ for every 1 ≤h ̸= h′ ≤k, we have vi −wj ≤dji and wj −vi ≤dij.
Since (l, u) is a total decoupling, it satisﬁes the conditions of the LP TD(D).
Hence, ui −lj ≤dij and uj −li ≤dji. Since vi ∈[li, ui] and wj ∈[lj, uj],
we then immediately have vi −wj ≤ui −lj ≤dji and wj −vi ≤uj −li ≤
dji. Therefore, whatever choices are made by the individual agents satisfying
their local constraints, these choices always will satisfy the original constraints,
too.
■
Remark. In [14] the (l, u) bounds found by solving the LP TD(D) are used to
compute the concurrent ﬂexibility ﬂex(S) = n
i (ui−li) of an STP S. Taking the
concurrent ﬂexibility as our ﬂexibility metric, the (l, u) bounds for decoupling are
always optimal, whatever partitioning [Ti]n
i=1 is used: ﬁrst, observe that due to
a decoupling, the ﬂexibility of an STN can never increase. Secondly, if the (l, u)
bounds for a total decoupling are used, by deﬁnition, the sum k
i=1 ﬂex(Si) of
the (concurrent) ﬂexibilities of the decoupled subsystems equals the ﬂexibility
ﬂex(S) of the original system. Hence, the total decoupling bounds (l, u) are
optimal, whatever partitioning [Ti]n
i=1 used.
■

334
K.S. Mountakis et al.
In this paper, we will consider concurrent ﬂexibility as our ﬂexibility metric.
Hence, a total decoupling is always an optimal decoupling for any partitioning
of variables. Therefore, in the sequel, we concentrate on total decouplings.
3
Total Decoupling by Minimum Matching
In the introduction we mentioned that an (optimal) total decoupling can be
achieved in O(n3) time. In the previous section, we presented an LP to compute
such a decoupling. If the STP has n variables, the LP to be solved has 2n
variables and n2 constraints. Modern interior-point methods solve LPs with n
variables and m constraints in roughly O(n3m) [13]. Thus, the complexity of
solving total decoupling as an LP might be as high as O(n5). In a previous
paper ([9]), we have shown that computing the concurrent ﬂexibility of an STP
can be reduced to a minimum matching problem (see [10]) using the distance
matrix D to construct a weighted cost matrix D∗for the latter problem.
This reduction, however, does not allow us to directly compute the corre-
sponding ﬂexibility maximizers (l, u). In this section we therefore show that
there is a full O(n3) alternative method for the LP-based ﬂexibility method
to compute the concurrent ﬂexibility and the corresponding maximizers (l, u),
thereby showing that an optimal total decoupling can be computed in O(n3).
Flexibility and Minimum Matching. Given an STN S = (T, C) with mini-
mum distance matrix D, let ﬂex(S) be its concurrent ﬂexibility, realised by the
maximizers (l, u). Hence, ﬂex(S) = f(l, u) = n
i=1(ui −li). Unfolding this sum
–as was noticed in [9]– we obtain
f(l, u) =

i∈T
(ui −li) =

i∈T
(uπi −li)
(4)
for every permutation π of T.6 Since (l, u) is a total decoupling, we have
uj −li ≤dij
∀i ̸= j ∈T
(5)
uj −lj = (uj −z) + (z −lj) ≤d0j + dj0
∀j ∈T
(6)
Hence, deﬁning the modiﬁed distance matrix (also called weight matrix) D∗=
[d∗
ij]n×n by
d∗
ij =
	
dij,
1 ≤i ̸= j ≤n
d0i + di0,
1 ≤i = j ≤n
we obtain the following inequality:
f(l, u) ≤min{

i∈T
d∗
iπi : π ∈Π(T)}
(7)
6 To avoid cumbersome notation, we will often use i ∈T as a shorthand for ti ∈T.

Dynamic Temporal Decoupling
335
where Π(T) is the space of permutations of T. Equation (7) states that the maxi-
mum ﬂexibility of an STN is upper bounded by the value of a minimum matching
in a bipartite graph with weight matrix D∗. The solution of such a matching
problem consists in ﬁnding for each row i in D∗a unique column πi such that
the sum 
i∈T d∗
iπi is minimized. As we showed in [9], by applying LP-duality
theory, Eq. (7) can be replaced by an equality: f(l, u) = minπ∈Π(T )

i∈T d∗
iπi, so
the concurrent ﬂexibility ﬂex(S) = f can be computed by a minimum matching
algorithm as e.g. the Hungarian algorithm, running in O(n3) time.
Finding a Maximizer (l, u) Using Minimum Matching. With the fur-
ther help of LP-duality theory i.c., complementary slackness conditions [10], the
following result is immediate:
Observation 1. If π is a minimum matching with value m for the weight matrix
D∗, then there exists a maximizer (l, u) for the concurrent ﬂexibility ﬂex(S) of
S, such that ﬂex(S) = m and for all i ∈T, uπi −li = d∗
iπi.
Now observe that the inequalities stated in the LP-speciﬁcation TD(D) and
the inequalities uπi −li = d∗
iπi in Observation 1 all are binary diﬀerence con-
straints. Hence, the STP S′ = (T ′, C′), where
T ′ = L ∪U = {li | i ∈T} ∪{ui | i ∈T},
C′ = {ui −lj ≤d∗
ij | i, j ∈T} ∪{li −uπi ≤−d∗
iπi | i ∈T} ∪{li −ui ≤0 | i ∈T}
is an STP7 and every solution s′ = (l1, . . . ln, u1, . . . , un) of S′ in fact is a max-
imizer (l, u) for the original STP S, since the ﬂexibility associated with such a
solution (l, u) satisﬁes ﬂex(S) ≥f(l, u) = 
i∈T (ui −li) ≥
i∈T d∗
iπi = ﬂex(S).
Hence, this pair (l, u) is a maximizer realizing ﬂex(S).
In particular, the earliest and latest solutions of S′ have this property. Hence,
since (i) D∗can be obtained in O(n3) time, (ii) a minimum matching based on
D∗can be computed in O(n3), and (iii) the STN S′ together with a solution
s = (l, u) for it can be computed in O(n3), we obtain the following result:
Corollary 1. Given an STN S = (T, C) with distance matrix D, an optimal
total decoupling (l, u) for S can be found in O(n3).
4
Dynamic Decoupling by Updating
A temporal decoupling allows agents to make independent choices or commit-
ments to variables they control. As pointed out in the introduction, we want to
adapt an existing (total) temporal decoupling (l, u) whenever an agent makes
a new commitment to one or more temporal variables (s)he controls. To show
that such a commitment could aﬀect the ﬂexibility other agents have in making
their possible commitments, consider our leading example again:
7 We should note that this STP has two external reference variables u0 = l0 = 0.

336
K.S. Mountakis et al.
Example 5. In Example 3 we obtained a temporal decoupling (l, u) = ((0, 15, 13),
(0, 15, 19)). Here, agent 1 was forced to arrive at 12:15, but agent 2 could
choose to arrive between 12:13 and 12:19. Suppose agent 2 commits to arrive
at 12:13. As a result, agent 1 is able to arrive at any time in the interval
[9, 15]. Then, by adapting the decoupling to the updated decoupling (l′, u′) =
((0, 9, 13), (0, 15, 13)), the ﬂexibility of agent 1 could increase from 0 to 6, taking
into account the new commitment agent 1 has made. If the existing commitment
(l, u), however, is not updated, agent 1 still has to choose 12:15 as its time of
arrival.
■
In order to state this dynamic decoupling or decouple updating problem more
precisely, we assume that at any moment in time the set T consists of a subset
Tc of variables committed to and a set of not committed to, or free, variables Tf.
The commitments for variables ti ∈Tc are given as bounds [lc
i, uc
i], specifying
that for i ∈Tc, ti is committed8 to choose a value in the interval [lc
i, uc
i]. The
total set of commitments in Tc is given by the bounds (lc, uc). We assume that
these committed bounds do not violate decoupling conditions.
Whenever (l, u) is a total decoupling for S = (T, C), where T = Tc ∪Tf,
we always assume that (l, u) respects the commitments, i.e. [li, ui] = [lc
i, uc
i] for
every ti ∈Tc, and (l, u) is an optimum decoupling for the free variables in Tf,
given these commitments. Suppose now an agent makes a new commitment for
a variable ti ∈Tf. In that case, such a commitment [vi, wi] should satisfy the
existing decoupling, that is li ≤vi ≤wi ≤ui, but as a result, the new decoupling
where [li, ui] = [vi, wi], T ′
c = Tc ∪{ti}, and T ′
f = Tf −{ti} might no longer be
an optimal decoupling w.r.t. T ′
f (e.g. see Example 5). In that case we need to
update (l, u) and to ﬁnd a better decoupling (l′, u′).
The decoupling updating problem then can be stated as follows:
Given a (possibly non-optimal) total decoupling (l, u) for an STP S =
(T, C), with T = Tc ∪Tf, ﬁnd a new total decoupling (l′, u′) for S such
that
1. no individual ﬂexibility of free variables is negatively aﬀected, i.e., for
all j ∈Tf, [lj, uj] ⊑[l′
j, u′
j]9;
2. all commitments are respected, that is, for all j ∈Tc, [l′c
j , u′c
j ] = [lc
j, uc
j];
3. the ﬂexibility realized by (l′, u′), given the commitments (l′c, u′c), is
maximum.
Based on the earlier shown total decoupling problem TD(D), this decoupling
update problem can also be stated as the following LP:
max

j
(u′
j −l′
j)
(DTD(D, Tc, Tf, (l, u))
8 Note that this is slightly more general concept than a strict commitment of a variable
ti to one value vi.
9 That is, the interval [l′
j, u′
j] contains [lj, uj].

Dynamic Temporal Decoupling
337
s.t.
u′
0 = l′
0 = 0
(8)
u′
j −l′
i ≤dij
∀i ̸= j ∈T
(9)
uj ≤u′
j, l′
j ≤lj
∀j ∈Tf
(10)
l′
j = lj, u′
j = uj
∀j ∈Tc
(11)
Here, (l, u) is the existing total decoupling.
In fact, by transforming DTD-instances into TD-instances, we can show
that the dynamic decoupling (decoupling updating) problem can be reduced to
a minimum-matching problem and can be solved in O(n3), too.10
5
A Fast Heuristic for Updating
Although the dynamic total decoupling problem can be reduced to the static tem-
poral decoupling problem, in practice, the computational complexity involved
might be too high. While an initial decoupling might be computed oﬀ-line, an
update of a decoupling requires an on-line adaptation process. Since the costs
of solving such a dynamic matching problem are at least O(n2) per update, we
would like to alleviate this computational investment. Therefore, in this section
we discuss a fast heuristic for the decoupling updating problem. Using this heuris-
tic, we show that an updated decoupling can be found in (amortized) O(n log n)
per update step if O(n) updates are assumed to take place.
The following proposition is a simple result we base our heuristic on:
Proposition 1. If (l, u) is a total decoupling for S with associated weight matrix
D∗, then, for all j ∈T, lj = maxk∈T (uk −d∗
kj) and uj = mink∈T (lk + d∗
jk).
Proof. Since (l, u) is a maximizer of the LP TD(D), ui−lj ≤d∗
ij, for every i, j ∈
T. Hence, for each i, j ∈T, we have lj ≥maxk∈T (uk−d∗
kj) and ui ≤mink∈T (lk+
d∗
ik). Now, on the contrary, assume that for some i ∈T, li > mink∈T (uk −d∗
ki).
Then the bounds (l′, u) where l′ = l, except for l′
i = mink∈T (uk −d∗
ki), would
satisfy the constraints ui −l′
j ≤d∗
ij for every i, j ∈T, as well as l′
j ≤uj for
every j ∈T. Hence, (l′, u) is a decoupling as well. But then (l′, u) satisﬁes the
conditions of the LP TD(D) but 
j(uj −l′
j) > 
j(uj −lj), contradicting the
fact that (l, u) is a maximizer of this LP. Hence, such an i ∈T cannot exist. The
proof for ui < mink∈T (lk + dik) goes along the same line and the proposition
follows.
■
The converse, however, of this proposition is not true: not every solution (l, u)
satisfying the two equalities is a maximum decoupling. It can only be shown that
in such a case (l, u) is a maximal total decoupling. That means, if (l, u) satisﬁes
10 As has been observed by a reviewer, there exists an O(n2) algorithm for the dynamic
variant of the minimum-matching problem. It is likely that our dynamic decoupling
problem can be solved by such a dynamic minimum matching algorithm in O(n2)
time, too.

338
K.S. Mountakis et al.
the equations above, there does not exist a decoupling (l′, u′) containing (l, u)
that has a higher ﬂexibility.
It turns out that these maximal total decouplings and their updates can be
computed very eﬃciently: given a (non-maximal) total decoupling (l, u), and a
set T = Tc ∪Tf of committed and free variables, there exists a very eﬃcient
algorithm to compute a new total decoupling [l′, u′] such that
1. (l′, u′) preserves the existing commitments: for all j ∈Tc, [l′
j, u′
j] = [lj, uj];
2. [l′, u′] respects the existing bounds of the free variables: for all j ∈Tf,
[lj, uj] ⊑[l′
j, u′
j];
3. (l′, u′) satisﬁes the conditions stated in Proposition 1 above w.r.t. the free
variables, i.e. (l′, u′) is a maximal decoupling with respect to variables in Tf.
The following surprisingly simple algorithm ﬁnds a maximal ﬂexible total
decoupling for a set Tf ∪Tc of free and committed temporal variables that
satisﬁes these conditions. The algorithm iteratively updates the existing (l, u)-
decoupling bounds for the variables in Tf until all free variables satisfy the
equations stated in Proposition 1.
Algorithm 1. Finding an update (l′, u′) of an existing total decoupling (l, u)
Require: (l, u) is a total decoupling for S; D∗= [d∗
ij] is the weight matrix; T = Tf ∪Tc;
1: l′ = l ; u′ = u;
2: for i = 1 to |Tf| do
3:
mini := maxk∈T (u′
k −d∗
ik);
4:
maxi := mink∈T (l′
k + d∗
kj);
5:
if l′
i > mini then
6:
l′
i ←mini
7:
if u′
i < maxi then
8:
u′
i ←maxi
9: return (l′, u′);
It is easy to see that the algorithm preserves the existing commitments for
variables in Tc, since only bounds of free variables in Tf are updated.
It is also easy to see that the existing bounds [lj, uj] of free variables j ∈Tf
are respected: For every j it holds that either uj < maxj (l′
j > minj, respectively)
or uj = maxj (l′
j > minj, respectively). Hence, u′
j ≥uj and l′
j ≤lj.
To show that the obtained decoupling (l′, u′) is maximal with respect to the
free variables, we state two key observations: ﬁrst of all, in every step i it holds
that l′
i ≥mini and u′
i ≤maxi, because (l, u) is a decoupling and the (mini, maxi)
bounds are not violated during updating. Secondly, once the bounds (l′
i, u′
i) for
a variable i ∈Tf have been updated to mini and maxi, (l′
i, u′
i) will never need
to be updated again, since mini depends on values u′
k that might increase or
stay the same; hence mini cannot decrease in subsequent steps. Likewise, maxi
depends on values l′
k that might decrease or stay the same. Therefore, maxi
cannot increase in later steps. Therefore, it is suﬃcient to update the bounds for
the variables only once.

Dynamic Temporal Decoupling
339
As a result, at the end all free variables have been updated and achieved their
minimal/maximal bound. Then a maximal total decoupling has been obtained,
since all variables in Tf will satisfy the equations stated in Proposition 1.
Example 6. Continuing our example, notice that the weight matrix D∗obtained
via the minimum distance matrix D (see Example 2) equals
D∗=
⎡
⎣
0 15 19
−5 10 4
−8 2 6
⎤
⎦
Given the decoupling (l, u) = ((0, 15, 13), (0, 15, 19)) with Tf = {t1, t2}, let
agent 2 commit to t2 = 13. We would like to compute a new decoupling
maximizing the ﬂexibility for t1. Note that min1 = max{5, 5, 9} = 9 and
max1 = min{15, 25, 15} = 15. Hence, the heuristic ﬁnds an updated decoupling
(l′, u′) = ((0, 9, 13), (0, 15, 13)).
Complexity of the Heuristic. As the reader quickly observes, a naive imple-
mentation of Algorithm 1 would require O(n2)-time to obtain an updated decou-
pling: To update a single variable i ∈Tf, we have to compute mini and maxi.
This costs O(n) per variable and there may be n variables to update.
Fortunately, if there are O(n) updating steps, the computational cost per
step can be signiﬁcantly reduced: First compute, for each i ∈T, a priority
queue Qmin(i) of the entries uk −d∗
ik, k ∈T, and a priority queue Qmax(i) of
the entries lk + d∗
ki, k ∈T. The initialisation of these priority queues will cost
O(n . n log n) = O(n2. log n).
After a new commitment for a variable j, say [vj, wj], we have to compute a
decoupling update. It suﬃces to update the priority queues Qmin(i) and Qmax(i)
for every i ∈Tf. In this case, the element uj −d∗
ij in the queue Qmin(i) has to
be changed to wj −d∗
ij and the element lj in queue Qmax(i) has to be changed to
vj + d∗
ji. Maintaining the priority order in the priority queues will cost O(log n)
per variable. Hence, the total cost for computing a new decoupling are O(n log n).
If there are O(n) updates, the total cost of performing these O(n) updates are
O(n2. log n) + O(n).O(n. log n) = O(n2. log n). Hence, the amortized time costs
per update are O(n. log n).
In the next section we will verify the quality of such maximal ﬂexible decou-
plings experimentally, comparing them with maximum ﬂexible total decouplings
and a static decoupling.
6
Experimental Evaluation
We discuss an experimental evaluation of the dynamic total decoupling method.
These experiments have been designed (i) to verify whether updating a decou-
pling indeed signiﬁcantly increases the decoupling ﬂexibility compared to using
a static decoupling and (ii) to verify whether the heuristic signiﬁcantly reduces
the computational investments in updating as expected.

340
K.S. Mountakis et al.
Material. We applied our updating method to a dataset of STP benchmark
instances (see [11]). This dataset contains a series of sets of STP-instances with
STPs of varying structure and number of variables. Table 1 contains the main
characteristics of this benchmark set. The size of the STP-instances in this
dataset varies from instances with 41 variables with 614 constraints to instances
with 4082 nodes and 14110 constraints. In total, these sets contain 1122 STP-
instances. We used MATLAB (R2016b) on an iMac 3.2 Ghz, Intel Core i5, with
24 Gb memory to perform the experiments.
Table 1. STP Benchmarksets used in the experiments.
Benchmark set
NR instances Min vars AV vars Max vars
bdh
300
41
207
401
Diamonds
130
111
857
2751
Chordalgraphs
400
200
200
200
NeilYorkeExamples 180
108
1333
4082
sf-ﬁxedNodes
112
150
150
150
Method. For each instance in the benchmark set we ﬁrst computed a maximum
decoupling (l, u) with its ﬂexibility flex = n
j=1(uj −lj), using the minimum
matching method. Then, according to an arbitrary ordering < t1, t2, . . . tn >
of the variables in T, each variable ti is iteratively committed to a ﬁxed value
vi, where vi is randomly chosen in an interval (li, ui) belonging to the current
temporal decoupling (l, u). After each such a commitment, we compute a new
updated decoupling (li, ui) where Tc = {t1, . . . ti} and Tf = {ti+1, . . . tn}. The
total ﬂexibility of the new decoupling (li, ui) is now dependent upon the n−i free
variables in Tf and will be denoted by f i
h. We initially set f 0
h = flex. In order
to account for the decreasing number of free variables after successive updates,
we compute after each update the heuristic update ﬂexibility per free variable in
Tf: f i
h/(n−i). To compare these ﬂexibility measures with the static case, for the
latter we take the total ﬂexibility of the free variables ﬂex i = n
j=i+1(uj−lj) and
then compute the ﬂexibility per free time variable without updating: ﬂex i/(n−i).
As a summary result for each benchmark instance k, we compute
1. the average over all updates of the static ﬂexibility per free variable: av stat =
n
i=1 ﬂex i/((n −i) · n)
2. the average over all updates of the heuristic update ﬂexibility per free variable:
av h = n
i=1 f i
h/((n −i) · n)
Note that the ratio rel ﬂex h = av h/av stat indicates the impact of the
heuristic updating method for a particular instance: a value close to 1 indi-
cates almost no added ﬂexibility (per time variable) by updating, but a value
of 2 indicates that the ﬂexibility (per time variable) doubled by updating the
decoupling.

Dynamic Temporal Decoupling
341
Finally, we collected the rel ﬂex h results per instance for each benchmark set
and measured their minimum, mean and maximum values per benchmark set.
Results. The rel ﬂex h results are grouped by benchmark set and their mini-
mum, mean, and maximum per benchmark set are listed in Table 2.
Table 2. Statistics of ﬂexibility ratio’s rel ﬂex h of decoupling updates vss static decou-
pling per benchmark set.
Benchmark set
Min Mean Max
bdh-agent-problems 1.00 1.00
1.002
Diamonds
1.34 1.95
2.39
Chordalgraphs
1.08 1.31
1.65
NeilYorkeExamples
1.20 1.74
2.39
sf-ﬁxedNodes
1.21 1.38
1.78
As can be seen, except for bdh-agent-problems, dynamic updating of a tem-
poral decoupling increases the mean ﬂexibility per variable rather signiﬁcantly:
For example, in the diamonds and NeilYorke benchmark sets, updating almost
doubled the ﬂexibility per free time variable.11
We conclude that, based on this set of benchmarks, one might expect a sig-
niﬁcant increase in ﬂexibility if a decoupling is updated after changes in commit-
ments have been detected, compared to the case where no updating is provided.
To verify whether the heuristic was able to reduce the computation time sig-
niﬁcantly, unfortunately, we can only present partial results. The reason is that
for the more diﬃcult instances in these benchmark sets, computing the updates
with an exact minimum matching method was simply too time-consuming.12
We therefore selected from each benchmark set the easy13 instances and col-
lected them in the easy-<benchmark> variants of the original benchmark sets.
We then measured the mean computation time per benchmark set for both
the exact and heuristic updating method and also the mean performance ratio
rel flex/rel flexh of the exact versus the heuristic update method. The latter
metric indicates how much the exact method outperforms the heuristic method
in providing ﬂexibility per time variable. The results are collected in Table 3.
11 The reason that in the bdh-agent problems the updating did not increase, is prob-
ably due to the fact that in these instances, as we observed, the ﬂexibility was
concentrated in a very few time point variables. Eliminating the ﬂexibilities by com-
mitments of these variables did not aﬀect the ﬂexibility of the remaining variables
that much.
12 Some of the more diﬃcult benchmark problems even did not ﬁnish within 36 h.
13 A benchmark problem instance in [11] was considered to be “easy” if the exact
update method ﬁnished in 15 min. For the bdh-agent set we collected the instances
until easybdh 8 10 50 350 49, for the diamonds set all instances up to diamonds-38-
5.0, for the chordal graphs all instances until chordal-ﬁxedNodes-150,5,1072707262,
for the NeilYorkeExamples all instances until ny-419,10, and for the sf-ﬁxedNodes
the instances until sf-ﬁxedNodes-150,5,1072707262.

342
K.S. Mountakis et al.
Table 3. Comparing the time (sec.) and performance ratio of the exact and heuristic
updating methods (easy variants of benchmark instances)
Benchmark set
CPU heuristic CPU exact Performance ratio
easy-bdh-agent-problems 0.21
2.75
1.00
easy-diamonds
0.38
67.65
1.05
easy-chordalgraphs
0.78
12.6
1.06
easy-NeilYorkeExamples
0.61
135.62
1.01
easy-sf-ﬁxedNodes
0.13
4.71
1.03
From these results we conclude that even for the easy cases, the heuristic
clearly outperforms the exact update method, being more than 10 times and
sometimes more than 200 times faster. We also observe that the heuristic method
does not signiﬁcantly lose on ﬂexibility: The performance ratio’s obtained are
very close to 1.
7
Conclusions and Discussion
We have shown that adapting a decoupling after a new variable commitment
has been made in most cases signiﬁcantly increases the ﬂexibility of the free,
non-committed, variables. We also showed that this updating method did not
induce any disadvantage to the actors involved: every commitment is preserved
and the current decoupling bounds are never violated. We introduced a simple
heuristic that replaces the rather costly computation of a decoupling with maxi-
mum ﬂexibility by a computation of a decoupling with maximal ﬂexibility. This
heuristic reduces the computation time per update from O(n3) to O(n · log n)
(amortized) time. As we showed experimentally, the computational investments
for the heuristic are signiﬁcantly smaller, while we observed almost no loss of
ﬂexibility compared to the exact method.
In the future, we want to extend this work to computing ﬂexible schedules
for STPs: Note that the update heuristic also can be used to ﬁnd a maximal
ﬂexible schedule given a solution s of an STP. Such a solution is nothing more
than a non-optimal decoupling (s, s) for which, by applying our heuristic, an
O(n log n) procedure exists to ﬁnd an optimal ﬂexible schedule. Furthermore,
we are planning to construct dynamic decoupling methods in a distributed way,
like existing approaches to static decoupling methods have done.
References
1. Boerkoel, J., Durfee, E.: Distributed reasoning for multiagent simple temporal
problems. J. Artif. Intell. Res. (JAIR) 47, 95–156 (2013)
2. Brambilla, A.: Artiﬁcial Intelligence in Space Systems: Coordination Through
Problem Decoupling in Multi Agent Planning for Space Systems. Lambert
Academic Publishing, Germany (2010)

Dynamic Temporal Decoupling
343
3. Dechter, R.: Constraint Processing. Morgan Kaufmann, USA (2003)
4. Dechter, R., Meiri, I., Pearl, J.: Temporal constraint networks. Artif. Intell. 49,
61–95 (1991)
5. Floyd, R.: Algorithm 97: shortest path. Commun. ACM 5(6), 345 (1962)
6. Hunsberger, L.: Algorithms for a temporal decoupling problem in multi-agent plan-
ning. In: Proceedings of the Association for the Advancement of Artiﬁcial Intelli-
gence (AAAI-02), pp. 468–475 (2002)
7. Hunsberger, L.: Group decision making and temporal reasoning. Ph.D. thesis,
Harvard University, Cambridge, Massachusetts (2002)
8. van Leeuwen, P., Witteveen, C.: Temporal decoupling and determining resource
needs of autonomous agents in the airport turnaround process. In: Proceedings of
the 2009 IEEE/WIC/ACM International Joint Conference on Web Intelligence and
Intelligent Agent Technology, WI-IAT 2009, vol. 02, pp. 185–192. IEEE Computer
Society, Washington, DC, USA (2009)
9. Mountakis, S., Klos, T., Witteveen, C.: Temporal ﬂexibility revisited: maximizing
ﬂexibility by computing bipartite matchings. In: Proceedings Twenty-Fifth Inter-
national Conference on Automated Planning and Scheduling (2015). http://www.
aaai.org/ocs/index.php/ICAPS/ICAPS15/paper/view/10610
10. Papadimitriou, C.H., Steiglitz, K.: Combinatorial Optimization: Algorithms and
Complexity. Prentice-Hall Inc., Upper Saddle River (1982)
11. Planken, L.: Algorithms for simple temporal reasoning. Ph.D. thesis, Delft
University of Technology (2013)
12. Planken, L., de Weerdt, M., van der Krogt, R.: Computing all-pairs shortest paths
by leveraging low treewidth. J. Artif. Intell. Res. 43(1), 353–388 (2012). http://dl.
acm.org/citation.cfm?id=2387915.2387925
13. Potra, F.A., Wright, S.J.: Interior-point methods. J. Comput. Appl. Math.
124(1–2),
281–302
(2000).
http://www.sciencedirect.com/science/article/pii/
S0377042700004337. Numerical Analysis 2000. Vol. IV: Optimization and Non-
linear Equations
14. Wilson, M., Klos, T., Witteveen, C., Huisman, B.: Flexibility and decoupling in
the simple temporal problem. In: Rossi, F. (ed.) Proceedings International Joint
Conference on Artiﬁcial Intelligence (IJCAI), pp. 2422–2428. AAAI Press, Menlo
Park, CA (2013). http://ijcai.org/papers13/Papers/IJCAI13-356.pdf
15. Wilson, M., Klos, T., Witteveen, C., Huisman, B.: Flexibility and decoupling in
simple temporal networks. Artif. Intell. 214, 26–44 (2014)

A Multi-stage Simulated Annealing Algorithm
for the Torpedo Scheduling Problem
Lucas Kletzander(B) and Nysret Musliu
TU Wien, Karlsplatz 13, 1040 Wien, Austria
{lkletzan,musliu}@dbai.tuwien.ac.at
Abstract. In production plants complex chains of processes need to be
scheduled in an eﬃcient way to minimize time and cost and maximize
productivity. The torpedo scheduling problem that deals with optimiz-
ing the transport of hot metal in a steel production plant was proposed
as the problem for the 2016 ACP (Association for Constraint Program-
ming) challenge. This paper presents a new approach utilizing a multi-
stage simulated annealing process adapted for the provided lexicographic
evaluation function. It relies on two rounds of simulated annealing each
using a speciﬁc objective function tailored for the corresponding part of
the evaluation goals with an emphasis on eﬃcient moves. The proposed
algorithm was ranked ﬁrst (ex aequo) in the 2016 ACP challenge and
found the best known solutions for all provided instances.
Keywords: Torpedo scheduling · Simulated annealing · Lexicographic
evaluation function
1
Introduction
Production plants have a wide range of complex chains of processes that raise
the need for optimization. To be competitive, cost needs to be minimized and
eﬃciency and productivity need to be maximized. A selected scheduling problem
in steel production was chosen for the 2016 ACP (Association for Constraint
Programming) challenge [12]. The aim was to provide the best solutions for the
torpedo scheduling problem where the transport of hot metal across various zones
needs to optimized. The goal is to use as few transport vehicles (torpedoes) as
possible and keep the time spent for a chemical process called desulfurization low
while satisfying all deadlines and moving through the zones respecting capacity
and duration constraints.
So far we know one other approach to the same problem by Geiger [4] ranking
third in the competition. He used a branch and bound procedure to traverse feasi-
ble transport assignments and solved a resource-constrained scheduling problem
based on variable neighborhood search to minimize desulfurization times. Other
optimization problems in steel production have been researched in the past years.
The molten iron allocation problem, modeled as a parallel machine scheduling
problem [13] and the molten iron scheduling problem modeled as a ﬂow shop
c
⃝Springer International Publishing AG 2017
D. Salvagnin and M. Lombardi (Eds.): CPAIOR 2017, LNCS 10335, pp. 344–358, 2017.
DOI: 10.1007/978-3-319-59776-8 28

Multi-stage Simulated Annealing for Torpedo Scheduling
345
problem [6,9] deal with assignments of torpedoes to machines. Various torpedo
scheduling problems are deﬁned for planning the transport of hot metal with the
focus on vehicle routing across a network of rails and the objective to minimize
transportation times [2,7,10]. Further production stages of steel making are also
considered, e.g. steelmaking-continuous casting [14] which comes after the stage
considered in this paper.
The approach we used is to utilize a multi-stage simulated annealing process
adapted for the provided lexicographic evaluation function. The technique to
simulate the physical process of annealing was introduced in [8] and is a widely
used technique in many applications [3,11]. Applications of multi-stage algo-
rithms can be found in the domain of vehicle routing problems with time win-
dows. These applications range back to [5] and also include application of simu-
lated annealing [1], however, only for one stage. Several of these problems share
a primary goal to minimize a number of vehicles, but pursue very diﬀerent sec-
ondary objectives.
In our approach the ﬁrst round of simulated annealing focuses on the primary
goal of optimizing the number of torpedoes while the second round deals with the
secondary goal of reducing the desulfurization time. The algorithm is designed
to try a large number of moves in a short time by emphasis on eﬃcient move
calculations. Various parameters for the algorithm were determined by empiric
evaluation. With this process it was possible to obtain the best solutions found
in the competition for all given instances.
2
Problem Deﬁnition
The selected problem represents a part of the steel production process in a steel
production plant. The blast furnace (BF) continually produces hot metal with
a certain level of sulfurization that needs to be picked up at certain deadlines.
From there the metal is transported via torpedoes which are cars suited for the
transport of hot metal. There are two possible routes the torpedoes can take.
The ﬁrst and standard route is to transition to a buﬀer zone (full buﬀer, FB)
where torpedoes can wait for an arbitrary amount of time. Afterwards they reach
the desulfurization zone (D) where the sulfurization level of the hot metal can be
lowered. The time that needs to be spend at this station is proportional to the
desired decrease in sulfurization levels. The delivery zone is at the converter (C)
where the hot metal is unloaded from the torpedoes. The hot metal is required at
certain deadlines at the converter. However, there is a maximum sulfuriziation
level set for each deadline that the delivered hot metal must respect. Finally,
the torpedo returns to another buﬀer zone (empty buﬀer, EB) where empty
torpedoes wait for their next turn.
The other route can be used in order to get rid of excess hot metal that is not
needed at the converter. It consists of a transition to the emergency pit where
the torpedo disposes the hot metal and then returns to the buﬀer zone for empty
torpedoes (EB). This route takes a ﬁxed amount of time.

346
L. Kletzander and N. Musliu
2.1
Instance Representation
For each instance of the problem several global parameters specifying the details
of the production plant are given. durBF speciﬁes the duration of ﬁlling a torpedo
at the blast furnace. durConverter speciﬁes the duration of unloading a torpedo
at the converter.
The sulfurization level of hot metal is assumed to have ﬁve possible levels from
1 (low) to 5 (high). durDesulf speciﬁes the time needed at the desulfurization
zone to lower the sulfurization level by 1.
Further the three parameters specifying the amount of available slots in indi-
vidual zones are nbSlotsFullBuﬀer, nbSlotsDesulf and nbSlotsConverter. These
denote the maximum number of torpedoes that are allowed to stay at the zone
at any given time. For the blast furnace and any transition between stations
only one torpedo is allowed at the same time. There are no limits for the empty
buﬀer and the emergency pit.
Finally the minimum transition times between any two adjacent zones are
speciﬁed via the values tt⟨Zone1⟩To⟨Zone2⟩.
The goal of the algorithm is to schedule the routes of the torpedoes for an
extended period of time. For this purpose, a series of blast furnace deadlines and
converter deadlines is included in every instance.
Blast furnace deadlines are described as BF ⟨id⟩⟨time⟩⟨sulf ⟩where ⟨id⟩iden-
tiﬁes the individual deadline. ⟨time⟩denotes the exact point in time when the
hot metal will be released, therefore at this point in time a torpedo must be
waiting to receive the metal. ⟨sulf ⟩denotes the sulfurization level the metal will
have.
Converter deadlines are similarly described as C ⟨id⟩⟨time⟩⟨maxSulf ⟩where
⟨id⟩identiﬁes the individual deadline. ⟨time⟩denotes the exact point in time
when the hot metal has to be provided by a torpedo located at the converter.
⟨maxSulf ⟩denotes the maximum sulfurization level that can be accepted.
The instance ﬁles for the competition can be found on the web page.1
2.2
Solution Representation
The solution format contains the used number of torpedoes nbTorpedoes, fol-
lowed by a list of torpedo tours. Each tour ﬁrst contains three IDs. idTorpedo
speciﬁes the torpedo used for this tour in the range [0, nbTorpedoes −1]. idBF
speciﬁes the ID of the blast furnace output the torpedo receives on this tour and
idC denotes the ID of the converter demand the torpedo satisﬁes on this tour
or −1 in case the metal is disposed at the emergency pit.
Next for each tour a set of time points is given, determining the exact
sequence of the tour. For each zone start⟨Zone⟩and end⟨Zone⟩are speciﬁed
according to the order the torpedo passes through the zones. The time points
from startFB to endC are only speciﬁed for regular tours, but are missing for
tours using the emergency pit.
1 http://cp2016.a4cp.org/program/acp-challenge-app/instance.

Multi-stage Simulated Annealing for Torpedo Scheduling
347
2.3
Evaluation Function
The function given for the evaluation of the solution quality considers the neces-
sary number of torpedoes and the time spent in the desulfurization zone. First
a solution is only valid if all deadlines at the blast furnace and the converter are
met, the maximum sulfurization levels are respected, torpedoes move through
the system correctly according to the speciﬁed transition and duration times and
capacity constraints in the various zones are respected.
Then valid solutions are evaluated according to the lexicographic evaluation
function that ranks solutions ﬁrst based on the number of torpedoes and then
on the total desulfurization time timeDesulf . More precisely the function
cost = nbTorpedoes + timeDesulf /(upperBoundTorpedo · durDesulf )
(1)
is used and the goal is to minimize the cost. Here upperBoundTorpedo is four
times the number of converter deadlines as in the worst case every single torpedo
tour to the converter needs to lower the sulfurization level from 5 (maximum)
to 1 (minimum), therefore spending 4 · durDesulf in this zone each time. This
way for any reasonable solution the desulfurization time is normalized to the
interval [0, 1), the integer value of the cost represents the number of torpedoes
and the part after the comma represents the desulfurization time, enforcing the
lexicographic order.
3
Solution Approach
To solve the given problem we introduce a two-stage simulated annealing based
algorithm. We ﬁrst formulate solution candidates and a set of equations to eﬃ-
ciently track data determining the solution quality. We then provide the overview
of the algorithm and describe the motivation for the design of individual parts
and their contributions for improving the solution quality. Emphasis is put on
the design of the objective functions to use the power of the two-stage approach.
3.1
Representing a Solution Candidate
We model a possible solution with the concept of a torpedo run. Such a run
represents one round trip of a torpedo and is tied to exactly one blast furnace
deadline and either one converter deadline or the emergency pit. This is very
similar to a tour in the required solution representation except that no speciﬁc
torpedo ID is assigned. It allows to determine the amount of such runs directly
from the given instance. The total amount of runs is equal to the number of
blast furnace deadlines. The amount of runs targeting the converter is equal to
the number of converter deadlines and the rest is targeting the emergency pit.
As a solution candidate we use an array of such torpedo runs that contains the
correct number of runs targeting the converter and emergency pit. The algorithm
is designed to respect the order of zones, prevent violations of zone durations
and transition times and to always meet the blast furnace deadlines. All other

348
L. Kletzander and N. Musliu
violations of the constraints previously described are possible in the algorithm.
Instead of preventing them they are monitored and penalized by the objective
functions used in the simulated annealing algorithm.
Monitoring Constraint Violations. Constraint violations are tracked by
maintaining an array of 10 values called badness each representing a certain
kind of violation that can be individually weighted in the objective function.
The ﬁrst set of constraint violations regarding the converter demands can
be described as follows given the notation that RC is the set of torpedo runs
targeting the converter and BF and C hold the corresponding blast furnace and
converter deadline objects:

i∈RC
max{startC i −C[idC i] . time, 0}
(2)
diﬀi = BF[idBF i] . sulf −
endDi −startDi
durD

−C[idC i] . maxSulf
(3)

i∈RC
max{diﬀi, 0}
(4)
Equation (2) counts the total converter deadline miss time by summing up
the delay across all runs i that miss their deadline, in which case the start time
at the converter startC i will be after the time of the assigned converter deadline.
Equation (3) ﬁrst calculates the ﬁnal sulfurization level for torpedo run i
using the level at the blast furnace and the amount of time spend in the desul-
furization zone. By subtracting the maximum allowed level at the converter it
calculates the diﬀerence diﬀi between the maximum allowed level and the actual
level. Values below 0 mean that the hot metal has lower sulfurization level than
the allowed maximum indicating a potentially wasteful, but feasible pairing. A
value of 0 exactly meets the requirement and values above 0 indicate sulfuriza-
tion level misses and therefore constraint violations. Equation (4) sums up this
diﬀerence in sulfurization levels across all runs that miss this requirement.
For each of the remaining capacity constraints the algorithm maintains an
array with the size T equal to the amount of time units in the whole planning
period. One such array is maintained for each capacitated zone (cBF, cFB, cD
and cC) and for the corresponding transitions (cBFtoFB, cFBtoD, cDtoC and
cCtoE). Each element of such an array counts the amount of torpedo runs using
the respective zone or transition at the given point in time.
To track the violations for each of these arrays X the corresponding badness
value is calculated by

0≤t<T
max{X[t] −maxOccupationX, 0}
(5)
where maxOccupationX is either one of the given capacities nbSlotsFullBuﬀer,
nbSlotsDesulf , nbSlotsConverter for the corresponding arrays or 1 in all other
cases.

Multi-stage Simulated Annealing for Torpedo Scheduling
349
Monitoring Optimization Goals. To keep track of the number of torpedos
another array spanning across the whole time span of the planning period is
used. The array occupation counts for each point in time how many torpedo
runs are currently active. Therefore the maximum value of this array represents
the current value of the main evaluation goal.
However, for this array tracking the sum of all values is not good enough for
use in the objective functions. Therefore the array occupationCount counts the
number of elements of occupation having a certain value by
occupationCount[i] = count{t : occupation[t] = i},
(6)
e.g. occupationCount[1] counts the amount of time points where exactly one
torpedo run is active. This concept allows to individually weight speciﬁc levels
of occupation.
Finally the desulfurization is tracked by timeDesulf holding the total amount
of desulfurization time for all torpedo runs. Further the array desulfMismatch
keeps track of the diﬀerence between the sulfurization levels of the metal provided
at the blast furnace compared to the required levels at the converter. It calculates
desulfMismatch[i] = count{j : BF[idBF j] . sulf −C[idC j] . maxSulf = i}
(7)
for 0 ≤i < 5, e.g. desulfMismatch[3] counts the amount of torpedo runs that
need at least 3 desulfurization steps in order to be feasible. This again allows
individual weights for speciﬁc diﬀerence values.
3.2
The Simulated Annealing Algorithm
The simulated annealing process is done in two rounds using almost the same
parameters, but very diﬀerent objective functions. The general design of each
round uses the usual process of simulated annealing:
generateInitialSolution();
for round = 0...1 {
value = evaluateSolution();
t = value/10;
for outer = 0...10000 {
for inner = 0...innerIterations() {
chooseMove();
newValue = evaluateSolution();
if (shouldAccept()) {
acceptMove();
} else {
abortMove();
}
}
t *= 0.998;
}
}

350
L. Kletzander and N. Musliu
Heuristic Generation of the Initial Solution Candidate. The concept of
generateInitialSolution() is to take the ordered lists of blast furnace and con-
verter deadlines and pair them according to this ordering. The torpedo runs are
initialized to spend the minimum amount of time in the desulfurization zone
that allows them to meet the sulfurization level requirement of the assigned con-
verter. They arrive at the converter just at the time of the deadline or too late
in case this pairing is actually not feasible and spend any required waiting time
in the full buﬀer.
As long as emergency pit runs are available, they are set whenever it is
possible to put the current blast furnace output to the emergency pit and still
transport the next blast furnace output to the current converter deadline in time.
As emergency pit runs tend to get scheduled a bit too early by this approach
and cause converter deadline misses a few runs later, the algorithm includes
backtracking on such deadline misses to remove earlier emergency pit runs again
and schedule those later.
At ﬁrst time and space proportional to the total length of the planning period
are required as all the capacity tracking arrays need to be initialized. The eﬀort
of constructing the initial solution is proportional to the number of blast furnace
deadlines (actually not linear due to the backtracking, but in practice still very
close) times the duration of a run as each run needs to be added to the capacity
tracking system.
Note that in most cases the initial solution will not be feasible as some degree
of constraint violation in regard to missing deadlines and capacity constraints
is to be expected. However, it is designed to have a structure that produces a
small amount of constraint violations while still keeping the execution very fast.
Parameter Tuning. The parameters for the algorithm were carefully chosen
by experimental evaluation of various combinations of parameters to increase the
performance. In the following the best values used in the ﬁnal computation for
the competition are presented. Reasons for the provided choice are given as well
as problems encountered with diﬀerent values. Unless otherwise stated, changes
in most parameters only resulted in small changes in the results.
Iteration Parameters. For each round the algorithm uses a ﬁxed number of
outer iterations that was set to 10000. This value ensures that the algorithm
converges to a stable result.
The temperature was set to start with a value of one tenth of the initial value
of the objective function. In each outer iteration the temperature is decreased
by multiplying with the factor 0.998. This choice, especially combined with the
starting temperature and the number of inner iterations was one of the more
critical choices for the quality of the results and therefore subject of detailed
empirical evaluation. The initial solution is constructed in a way to already have
a structure limiting the amount of constraint violations. While a certain increase
in such violations is expected in the early stage of the simulated annealing process
to prevent getting stuck too close to the initial solution, keeping the temperature

Multi-stage Simulated Annealing for Torpedo Scheduling
351
high for too long destroys the structure of the initial solution requiring extensive
amounts of repair at lower temperatures that make the overall result worse. On
the other hand, dropping the temperature too fast leads to getting stuck in local
optima.
The number of inner iterations innerIterations() depends on the size of the
instance, more precisely it is the number of blast furnace deadlines. This choice
was made as the number of possible moves also depends on this value. For the
second round experiments showed an increase by a factor of 4 to be beneﬁcial.
Eﬃcient Moves. We proposed three moves that are chosen randomly with
certain probabilities in chooseMove(). The ﬁrst move is a switch between the
assigned converter deadlines for two torpedo runs. It is chosen with a probability
of 0.4 in the ﬁrst round and 0.6 in the second round. The rather high probability
is due to the fact that this is the move with the highest impact on the structure
of the solution. The selection of the two runs is not randomized, but actually a
sensitive choice. The reason is that choosing two runs at very diﬀerent time points
in the whole planning period will likely not be a good move as large deadline
misses can be expected. On the other hand, only switching closely adjacent runs
will likely end up in local optima too soon. Therefore selecting the distance of the
two runs when sorted by their start times randomly between 1 and 10 showed
to provide good results.
Additionally runs are locally improved after such a move to reduce the
amount of converter deadline misses and sulfurization level misses. First the
time spent at the desulfurization station is set to exactly the amount of time
needed to pass the required maximum sulfurization level at the converter. Sec-
ond if the converter deadline is missed the time at the full buﬀer is reduced just
enough to get the deadline, or to 0 if the deadline miss is larger than the full
buﬀer time.
The other moves consist of changing the time spend at the full buﬀer (prob-
ability 0.4 in the ﬁrst round and 0.2 in the second), the time spend at the
desulfurization zone (probability 0.1) or the time spend at the converter (proba-
bility 0.1). As these moves are intended to change the internal structure of a run
towards a good feasible solution, new values for the respective times are chosen
randomly within limits preventing a converter deadline miss if possible.
The key concept in tracking the capacity and goal data is to allow very fast
calculation of changes triggered by moves in the algorithm and therefore be able
to try a lot of moves in a short amount of time. For the moves it is necessary
to ﬁrst compute the eﬀects of the move and then either accept or reject it.
Therefore, every move is reﬂected by ﬁrst creating copies of the torpedo runs
that are aﬀected by the move. Then all tracking data is updated by removing
the original runs and adding the changed copies. In case the move is rejected, the
copies are removed again and the originals added back to the tracking data. In
case it is accepted, the copies replace the originals in the array of torpedo runs.
The important aspect is that all badness and goal tracking data can be
updated incrementally. The sums or counts in (2), (4) and (7) allow easy removal

352
L. Kletzander and N. Musliu
and addition of torpedo runs. For (5) and (6) only the parts of the arrays X and
occupation aﬀected by the currently changed torpedo runs need to be updated,
the eﬀects on the sum and the count can easily be computed incrementally again.
Using this principle it is possible to update all tracked data in time that
just depends on the duration of the respective torpedo runs that are removed or
added. This duration is usually very small compared to the whole time span of
the planning period and is independent of the number of torpedo runs.
Moves are accepted by shouldAccept() if their evaluation yields a better
or equal result according to evaluateSolution() or else with a probability of
exp
 value−newValue
t

.
Selection Bias. As the main objective in the ﬁrst round is to reduce the maxi-
mum number of torpedoes used, optimization in areas with already low numbers
of torpedoes might not be relevant for the result at all while a single point with
a high number determines the value of the result. Therefore, the selection for a
move is biased to prefer runs in areas with a high number of torpedoes in use. On
the other hand, for the desulfurization time the total sum is relevant, therefore
every optimization matters and no selection bias is used in the second round.
3.3
Objective Functions
To use the power of the two-stage approach we proposed speciﬁc objective func-
tions for evaluateSolution() for each round tailored to the respective goals:

0≤i<10
w1[i] · badness[i] +

i≥0
occupationCount[i] · i4 + cost
(8)

0≤i<10
w2[i] · badness[i] +

i>ﬁxed
1000000 · occupationCount[i] · i4 +

0≤i<5
10000 · desulfMismatch[i] · i + timeDesulf
(9)
Equation (8) denotes the objective function for the ﬁrst round and (9) the
objective function for the second round. Again, the weights were chosen by exper-
imental evaluation.
Constraint Violations. First, both objective functions take into account the
constraint violations maintained by badness, however, with diﬀerent weight vec-
tors w1 and w2.
Both objective functions use a weight of 100000 for the total converter dead-
line miss time as missing such deadlines potentially indicates structural problems
of the solution and therefore is considered a priority for optimization.
For optimization of the sulfurization level misses the ﬁrst round again uses
a weight of 100000 as it focuses on ﬁnding a feasible solution with the least

Multi-stage Simulated Annealing for Torpedo Scheduling
353
possible amount of torpedoes. The high value ensures the focus on feasibility.
For the second round, however, the weight is only 1000 as there is a special part
of the objective function that also deals with the sulfurization level misses in
more detail.
Capacity constraint violations are all penalized by a weight of 10000 in the
ﬁrst round. Again the values were chosen rather high to focus on feasibility. For
the second run weights for capacity misses at speciﬁc zones are only weighted
by 10, for transitions by 1000. Transitions need to be weighted higher as their
maximum occupation of 1 leaves less margin in general, but the weights for the
zones were chosen very low to prevent the algorithm to get stuck in local optima.
This actually introduces a small probability for constraint violations still present
in the ﬁnal result, however, higher values focused the process more on these
constraints in the ﬁrst place and only afterwards optimizing the desulfurization
times within the limits already set by the constraints while the low values allow a
kind of parallel optimization of both desulfurization times and capacity violations
at a similar pace.
Number of Torpedoes. The next part of the objective functions uses the
occupationCount array. For the ﬁrst round each element occupationCount[i] is
weighted by i4. This polynomial weighting strategy ensures a strong optimization
towards a small number of currently active torpedo runs at any point in time
with a special emphasis on eliminating areas using a high number of torpedoes.
This showed to be a successful approach to optimize the ﬁrst objective.
For the second round the goal of this part of the objective function is com-
pletely changed. This part is the reason for choosing to use two separate rounds
of optimization in the ﬁrst place. The key point is the structure of the solution
created in the ﬁrst round. The number of currently active torpedoes is kept as
low as possible across the whole time span in order for the optimization to work.
However, as only the maximum number of torpedoes counts for the value of the
solution and this maximum value might only be reached at a small part of the
whole process, this kind of optimization restricts the possibilities to optimize the
desulfurization time more than necessary. Section 4.1 will highlight this in the
results.
Therefore, the second round memorizes the amount of torpedoes reached in
the ﬁrst round (ﬁxed) and sets the weight for every i up to this value to 0.
For all i above this value previous weights are increased by a factor of 1000000.
This allows free use of any number of torpedoes up to the set limit allowing
much more ﬂexibility for the reduction of desulfurization times by utilization of
torpedoes that would otherwise be on standby. On the other hand the excessive
weights for going beyond this limit ensure that the optimization result from the
ﬁrst round is kept throughout the second round.
Desulfurization. In the ﬁrst round the objective function is completed by
adding the actual evaluation function used for the ﬁnal solution as given by (1).
This adds the optimization of desulfurization times as a low priority goal to the
process.

354
L. Kletzander and N. Musliu
In the second round the desulfurization times are included in more detail to
encourage better matching of blast furnace and converter deadlines with respect
to their sulfurization levels. To incorporate the diﬀerence in initial sulfurization
levels compared to the actual converter level demands the array desulfMismatch
is used. A level miss is weighted by 10000 · i where i counts the number of
missed levels, therefore linearly penalizing the distance to the required level.
Here a range of other methods was tried as well, in particular using polynomial
strategies like for the number of torpedoes or also penalizing levels that are lower
than required in order to reduce potentially wasteful situations where torpedo
runs with low level are used for converter demands with high maximum level.
However, none of these strategies gave better results than the one described
above.
Finally, the total desulfurization time is added to the objective function as
well. Using a weight of 1, this (actually the overall optimization goal of the sec-
ond round) is a rather low priority target in the optimization process. This is
because putting more emphasis on parts like the sulfurization level mismatch
works towards producing an optimal structure of the solution earlier. This is
important especially regarding the assignment of converter deadlines to the tor-
pedo runs. As such switches can easily produce at least temporary constraint
violations it is beneﬁcial to work on an optimized assignment while the temper-
ature is still high and then focus on optimizations within runs by shifting times
to reduce desulfurization times locally at a later point in the process.
4
Results
The algorithm was executed on an Intel Core i7-6700K with 4 × 4.0 GHz. Table 1
shows the main characteristics of the competition instances. The duration and
capacity constraints were rather similar for the given instances while the main
diﬀerences were in the number of blast furnace and converter deadlines and the
time span covered. A single run for each instance used only 4 to 10 min on a
single CPU core for all instances except 6. Here, even though the instance size
is not that diﬀerent, one run takes almost 30 min due to the structure of the
instance creating longer torpedo runs.
4.1
Structure of a Solution
To see the importance of using two rounds of simulated annealing, data collected
from one particular computation of instance 1 is presented after the creation of
the initial solution and after each round of simulated annealing. The resulting
distribution is similar in all instances, therefore it is only described for instance
1 to highlight the way the algorithms transforms the solution.
Table 2 shows the elements of occupationCount and timeDesulf for instance 1.
All values occupationCount[i] with i > 5 are 0.
The initial solution generated by the heuristic typically uses only few tor-
pedoes more than the ﬁnal result, in this case 5 torpedoes are used. However,

Multi-stage Simulated Annealing for Torpedo Scheduling
355
Table 1. Instance characteristics
Instance
1
2
3
4
5
6
Blast furnace deadlines
850
1500
2200
1000
1800
2500
Converter deadlines
800
1400
2100
1000
1780
2350
Time span
131100
144165
394723
133798
256216
251460
Table 2. Objective values in various stages of the algorithm (instance 1)
Value
[0]
[1]
[2]
[3]
[4]
[5]
timeDesulf
Initial
41077
58754
25050
6091
239
11
18333
Round 0 68141
49830
12136
1053
62
0
18512
Round 1
681
17303
55387
46254
11597
0
7776
as to be expected, this solution is not feasible, capacity constraints are slightly
violated at the transitions FBtoD, DtoC and CtoE as well as at the desulfur-
ization zone. Figure 1 shows the distribution of the occupation values. The most
frequent occupation at this stage is to have 0 or 1 torpedo active.
After the ﬁrst round of simulated annealing the number of torpedoes was
lowered to 4, however, the desulfurization time slightly increased despite the
fact that desulfurization is added as a low level optimization goal to this stage.
This solution is already feasible and ﬁxes the number of torpedoes used in the
ﬁnal solution. As the ﬁgure shows, the distribution for occupationCount is shifted
as far as possible to the lower indices. The highest number of torpedoes is only
used at a very small number of time points. In fact, for almost half the time no
torpedoes are active at all.
In the second round of simulated annealing this distribution completely
changes as the algorithm now permits free use of any number of torpedoes up to
Fig. 1. Objective values in various stages of the algorithm (instance 1)

356
L. Kletzander and N. Musliu
4 in order to optimize the desulfurization time as much as possible. The results
show that there is almost no time left without any torpedo on the move while
the most frequent occupation shifts to 2 and 3. This allows much more ﬂexibil-
ity for optimizing the desulfurization time and results in less than half the time
compared to the ﬁrst round.
4.2
Results for Competition Instances
For the competition 50 runs were performed for each instance and the best result
was handed in. There were no restrictions on computation times.
Table 3 presents the results obtained by the 50 runs for each instance. Some
runs did not ﬁnd the best number of torpedoes. In case a previous run already
got less torpedoes, the run was aborted after the ﬁrst round of simulated anneal-
ing (Abort 1). Some runs resulted in minor levels of constraint violations and
were discarded (Abort 2). The amount of times the best result (Minimum) was
produced is also shown (Freq.). Maximum, mean and standard deviation are
calculated from runs that were not aborted only.
Table 3. Computation results (50 runs for each instance)
Instance Abort 1 Abort 2 Minimum
Freq. Maximum
Mean
Std. dev
1
1
4
4.0890625
3
4.09094907 4.089995 0.000484
2
1
0
4.08607143 16
4.08712662 4.086279 0.000217
3
16
0
3.12928571
5
3.12964286 3.129415 0.000080
4
0
3
3.157
47
3.157
3.157
0
5
6
1
4.08483146
1
4.08609551 4.085374 0.000282
6
1
0
4.075
1
4.07585106 4.075358 0.000172
The results show that in general the algorithm produces very stable solutions
that do not diﬀer much. This is important for practical use as the calculation
could be done with only few runs in a short time while still staying within a short
distance to the best solutions the algorithm might ﬁnd given more runs. For the
competition, however, distances between the participants were small leading to
the need to obtain the best results the algorithm can oﬀer. With 50 runs per
instance we were able to obtain the ﬁrst place (ex aequo) on all instances.
The best result was produced in at least 3 out of the 50 runs for the ﬁrst
four instances making those results easily reproducible and increasing the con-
ﬁdence that these are the best results the algorithm can produce. For the last
two instances, however, the best solution was only found in 1 out of 50 runs.
These two turned out to be the most diﬃcult instances in the competition. To
gain more conﬁdence, several recomputations of the whole 50 runs on these two
instances were able to reproduce these results, even though only about every
second recomputation for instance 5, but did not ﬁnd any better results.

Multi-stage Simulated Annealing for Torpedo Scheduling
357
Constraint violations in the results were only encountered in a minor fraction
of the runs (maximum of 4 out of 50). Moreover, for all aborted instances the
violations only occurred at most at 2 time units across the whole planning period.
Given the fact that penalties for capacity violations were deliberately set low in
the second round of simulated annealing to focus further on the optimization of
desulfurization times this is considered a good result.
Instance 3, a large instance with the longest time span, yet a very small
amount of torpedoes in the solution, showed to be the hardest instance for the
ﬁrst round. Here 32% of runs did not ﬁnd the best amount of torpedoes compared
to at most 12% for any other instance.
Instance 4 was clearly the easiest instance, being one of the smaller instances,
but also providing equal amounts of blast furnace and converter deadlines. This
eliminates the need for emergency pit runs. In fact, 47 out of 50 runs on this
instance produced the best found result.
5
Conclusion
This paper presented an approach using a multi-stage simulated annealing
process to solve a scheduling problem in steel production plants. Utilizing eﬃ-
cient moves it optimizes results for the lexicographic evaluation function using
two diﬀerent objective functions each tailored for optimal progress towards the
corresponding part of the evaluation in a two-stage simulated annealing process.
The results show that the approach is a valid and competitive method to
solve the given problem. As the general idea is generic, it can also be adapted
to various other problems utilizing lexicographic evaluation functions. Further
the framework to track changing capacity violations in a fast manner showed to
improve eﬃciency of the algorithm.
Future work could include the adaption of the approach to various other
problems in this domain to see how well it competes with diﬀerent approaches.
Further the selection of critical parameters for simulated annealing, especially
regarding automated parameter selection, could be the goal of research.
Acknowledgments. This work was supported by the Austrian Science Fund (FWF):
P24814-N23.
References
1. Bent, R., Van Hentenryck, P.: A two-stage hybrid local search for the vehicle
routing problem with time windows. Transp. Sci. 38(4), 515–530 (2004)
2. Deng, M., Inoue, A., Kawakami, S.: Optimal path planning for material and prod-
ucts transfer in steel works using ACO. In: The 2011 International Conference on
Advanced Mechatronic Systems, pp. 47–50. IEEE (2011)
3. Dowsland, K.A., Thompson, J.M.: Simulated annealing. In: Rozenberg, G., Bck,
T., Kok, J.N. (eds.) Handbook of Natural Computing, pp. 1623–1655. Springer,
Heidelberg (2012)

358
L. Kletzander and N. Musliu
4. Geiger, M.J.: Optimale Torpedo-Einsatzplanung - Analyse und L¨osung eines
Ablaufplanungsproblems der Stahlindustrie. In: Entscheidungsunterst¨utzung in
Theorie und Praxis - Tagungsband des gemeinsamen Workshops der GOR-
Arbeitsgruppen “Entscheidungstheorie und -praxis”, “Fuzzy Systeme, Neuronale
Netze und K¨unstliche Intelligenz” sowie “OR im Umweltschutz” am 10. und 11.
M¨arz 2016 in Magdeburg. Springer (in press)
5. Homberger, J., Gehring, H.: Two evolutionary metaheuristics for the vehicle rout-
ing problem with time windows. INFOR: Inf. Syst. Oper. Res. 37(3), 297–318
(1999)
6. Huang, H., Chai, T., Luo, X., Zheng, B., Wang, H.: Two-stage method and appli-
cation for molten iron scheduling problem between iron-making plants and steel-
making plants. IFAC Proc. Volumes 44(1), 9476–9481 (2011)
7. Kikuchi, J., Konishi, M., Imai, J.: Transfer planning of molten metals in steel
worksby decentralized agent. Memoirs Fac. Eng. Okayama Univ. 42(1), 60–70
(2008)
8. Kirkpatrick, S., Gelatt, C.D., Vecchi, M.P.: Optimization by simulated annealing.
Science 220(4598), 671–680 (1983)
9. Li, J., Pan, Q., Duan, P.: An improved artiﬁcial bee colony algorithm for solving
hybrid ﬂexible ﬂowshop with dynamic operation skipping. IEEE Trans. Cybern.
46(6), 1311–1324 (2016)
10. Liu, Y.Y., Wang, G.S.: The mix integer programming model for torpedo car
scheduling in iron and steel industry. In: International Conference on Computer
Information Systems and Industrial Applications, pp. 731–734. Atlantis Press
(2015)
11. Pham, D., Karaboga, D.: Intelligent Optimisation Techniques: Genetic Algorithms,
Tabu Search, Simulated Annealing and Neural Networks. Springer Science & Busi-
ness Media, London (2012)
12. Schaus, P., Dejemeppe, C., Mouthuy, S., Mouthuy, F.-X., Allouche, D., Zytnicki,
M., Pralet, C., Barnier, N.: The torpedo scheduling problem: description (2016).
http://cp2016.a4cp.org/program/acp-challenge/problem.html. Accessed: 02 Feb
2017
13. Tang, L., Wang, G., Liu, J.: A branch-and-price algorithm to solve the molten
iron allocation problem in iron and steel industry. Comput. Oper. Res. 34(10),
3001–3015 (2007)
14. Tang, L., Zhao, Y., Liu, J.: An improved diﬀerential evolution algorithm for practi-
cal dynamic scheduling in steelmaking-continuous casting production. IEEE Trans.
Evol. Comput. 18(2), 209–225 (2014)

Combining CP and ILP in a Tree
Decomposition of Bounded Height for the Sum
Colouring Problem
Ma¨el Minot1,3(B), Samba Ndojh Ndiaye1,2, and Christine Solnon1,3
1 Universit´e de Lyon - LIRIS, Lyon, France
{mael.minot,samba-ndojh.ndiaye,christine.solnon}@liris.cnrs.fr
2 Universit´e Lyon 1, LIRIS, UMR5205, 69622 Lyon, France
3 INSA-Lyon, LIRIS, UMR5205, 69621 Lyon, France
Abstract. The Sum Colouring Problem is an NP-hard problem derived
from the well-known graph colouring problem. It consists in ﬁnding a
proper colouring which minimizes the sum of the assigned colours rather
than the number of those colours. This problem often arises in scheduling
and resource allocation. In this paper, we conduct an in-depth evaluation
of ILP and CP’s capabilities to solve this problem, with several improve-
ments. Moreover, we propose to combine ILP and CP in a tree decom-
position with a bounded height. Finally, those methods are combined in
a portfolio approach to take advantage from their complementarity.
1
Introduction
The Sum Colouring Problem (SCP) is an NP-hard problem derived from the
well-known graph colouring problem. It consists in ﬁnding a proper colouring
(i.e. an assignment of colours to vertices such that neighbour vertices have dif-
ferent colours) which minimizes the sum of the assigned colours rather than the
number of those colours. This problem arises in a variety of real-world prob-
lems, especially in scheduling and resources allocation [21]. Many incomplete
approaches have been proposed [12], whereas only few complete approaches have
been proposed: mainly Integer Linear Programming (ILP) in [12], Branch and
Bound (B&B), SAT and Constraint Programming (CP) in [22]. In this paper,
we propose to conduct a more in-depth evaluation of ILP and CP’s capabilities
to solve the SCP, with several improvements. Moreover, we use tree decompo-
sition to improve the solution process by decomposing SCPs into independent
subproblems which are solved by ILP and CP, with promising results. Finally,
we show that a portfolio approach can take advantage of the complementarity
of the diﬀerent approaches.
Section 2 deﬁnes the SCP and gives an overview of existing approaches.
Sections 3 and 4 introduce improvements for CP and ILP models. Section 5
explains how CP and ILP may be combined by means of a tree decomposition,
and Sect. 6 introduces a portfolio approach.
c
⃝Springer International Publishing AG 2017
D. Salvagnin and M. Lombardi (Eds.): CPAIOR 2017, LNCS 10335, pp. 359–375, 2017.
DOI: 10.1007/978-3-319-59776-8 29

360
M. Minot et al.
2
The Sum Colouring Problem
An undirected graph G = (V, E) is deﬁned by a set V of nodes and a set
E ⊆V×V of edges. Each edge of G is an undirected pair of nodes. We note
deg(v) the degree of a vertex v, i.e. deg(v) = |

u ∈V, {u, v} ∈E

|, and Δ(G)
the largest degree in the graph, i.e. Δ(G) = max{deg(v), v ∈V }.
A legal (or “proper”) k-colouring of a graph G = (V, E) is a mapping c : V →
[1, k] such that ∀{x, y} ∈E, c(x) ̸= c(y). Classic graph colouring aims at ﬁnding a
proper k-colouring that minimizes k, whereas the SCP aims at ﬁnding a proper k-
colouring that minimizes the sum of assigned colours, i.e., 
x∈V c(x). The lowest
achievable sum for G is called the chromatic sum and is denoted Σ(G).
Existing Bounds. In [30], it is shown that ⌈

8|E|⌉≤Σ(G) ≤⌊3(|E|+1)
2
⌋.
In [21], it is demonstrated that Σ(G) ≤|V | + |E|, and that an optimal sum
colouring will never use strictly more than Δ(G) + 1 colours. Finally, in [24,33]
a lower bound is deﬁned with respect to a clique decomposition of the graph.
Deﬁnition 1. A clique is a subset of nodes which are all linked pairwise. A
clique decomposition of a graph is a partition C of its vertices such that, for each
set Ci ∈C, the subgraph induced by Ci is a clique.
Given a clique decomposition C of G, we have Σ(G) ≥
Ci∈C |Ci|·(|Ci|+1)/2,
as all vertices in a same clique must have diﬀerent colours.
Dominant Colourings. Dominant colourings were discussed in [22]: a k-colou-
ring may be seen as an ordered partition on the vertices of the coloured graph,
such that the i-th set Si contains all vertices using colour i (with 1 ≤i ≤k).
A colouring is dominated when the vertex colour sum can be lowered simply
by reassigning the indices of these sets (i.e., swapping colours) without actually
altering the partition. The dominant colouring of a k-colouring c is the colouring
obtained by ordering the partition deﬁned by c by decreasing size of sets, so that
S1 is the largest set, and Sk the smallest. This dominant colouring has the lowest
vertex colour sum among all possible colour swappings of c.
Incomplete Approaches. Many incomplete approaches were used to ﬁnd
approximate solutions for the SCP. A review of most of these approaches may
be found in [12]. It classiﬁes main contributions in three classes: greedy algo-
rithms [32,33], local search heuristics [3,7] and evolutionary algorithms [11,13,
23]. None of these algorithms are able to reach all best known upper and lower
bounds. The percentage of instances on which the best known upper bound is
reached ranges from 46% [32,33] to 90% [11] on tested graphs. Such approaches
can prove the optimality of a solution if the lowest upper bound happens to
reach the highest lower bound. However, such proofs were only made on 21
instances out of 94, even when using all the bounds found by every methods in
[12] simultaneously.

Combining CP and ILP in a Tree Decomposition of Bounded Height
361
CP. A basic CSP model was proposed in [22]. For each node u ∈V , this
model includes a variable xu whose domain is D(xu) = [1, Δ(G) + 1]. There is
a disequality constraint for each edge of the graph, i.e., ∀{u, v} ∈E, xu ̸= xv.
The objective is to minimize the sum of all variables, i.e., 
u∈V xu. This model
was evaluated using Choco [14]. The results were not competitive with state-of-
the-art approaches: solution times on rather easy instances were long [22].
B&B. In [22], a B&B approach is described. At each node of the search, a
lower bound is obtained by computing a clique decomposition C of the subgraph
induced by uncoloured vertices. However, instead of bounding the colour sum for
each clique Ci ∈C by |Ci|·(|Ci|+1)/2 (as proposed in [24,33]) the authors bound
it by the sum of the |Ci| smallest available colours among the vertices of Ci. This
new bound is tighter since it takes – to some degree – the availability of colours
into account. Besides, each time a proper colouring is found, its corresponding
dominant colouring is computed to improve the bound. This approach obtains
better results than the basic CP model, but remains limited to small graphs.
SAT. Diﬀerent SAT encodings for the SCP are described in [22]. They are exper-
imentally compared (using diﬀerent SAT solvers) with B&B and CP, on randomly
generated graphs and on six DIMACS instances. On these instances, the proposed
B&B and CP approaches are not competitive with the SAT portfolio ISAC [15],
which obtains the best results. In this paper, we introduce new CP models which
are competitive with these SAT models, and an ILP model which outperforms them
on the six DIMACS instances for which results were given.
ILP. An ILP model was proposed in [31]. It associates a binary variable xuk
with every pair (u, k) ∈V×[1, Δ(G) + 1], so that xuk equals 1 iﬀnode u uses
colour k. The objective is to minimize the sum of the integers corresponding to
used colours, i.e. |V |
u=1
Δ(G)+1
k=1
k · xuk so that each node u ∈V is assigned
one colour, i.e., Δ(G)+1
k=1
xuk = 1, and for each edge {u, v} ∈E, u and v
have diﬀerent colours, i.e., xuk + xvk ≤1, ∀k ∈[1, Δ(G) + 1]. This model was
evaluated with CPLEX, showing that it is very eﬃcient for small graphs, but
that the memory cost was too high for larger ones.
3
New CP Models for the SCP
The CP model of [22] is very limited, as it only propagates binary diﬀerence
constraints, and bounds the objective function with the sum of minimal values
in domains. In this section, we propose and compare several improvements.
3.1
Initial Domain Reduction
Instead of using the same domain [1, Δ(G) + 1] for all variables, we propose to
tighten domains by using the following property:

362
M. Minot et al.
Property 1. For every optimal sum colouring c of a graph G = (V, E), we have
∀v ∈V, c(v) ≤deg(v) + 1.
To prove this property, let us suppose that it does not hold for a given optimal
colouring c of a graph G. It follows that there exists a vertex v in V such that
c(v) > deg(v) + 1. In such a case, there exists x ∈[1, deg(v) + 1] such that every
neighbour of v has a colour diﬀerent from x (since v only has deg(v) neighbours).
As a consequence, a better colouring than c can be obtained by colouring v with
x instead of c(v). Therefore, c is not optimal, which contradicts our initial claim.
Hence, we deﬁne D(xu) = [1, deg(u) + 1] for each u ∈V . This is a minor but
natural improvement, with a negligible cost.
3.2
Dominant Colourings
As pointed out in [22] and recalled in Sect. 2, colourings found during the search
may be dominated, and can be improved simply by swapping colours. This makes
the upper bound go down faster at a low computing cost. Besides, these swap-
pings break symmetries by forbidding, thanks to the update of the upper bound,
the computation of colourings that are dominated by the ones already found.
3.3
AllDifferent Constraints
Instead of using only binary disequality constraints to prevent neighbour vertices
from being assigned the same colour, we propose to use AllDifferent constraints.
This may be done in several diﬀerent ways. A ﬁrst possibility is to compute a clique
decomposition of the graph, as deﬁned in Deﬁnition 1. In this case, we post a global
AllDifferent constraint for each clique, and a binary disequality constraint for each
edge such that no clique contains its two endpoints. A second possibility is to com-
pute a set of maximal cliques such that, for each edge, there exists at least one
clique that contains its two endpoints. In this case, we post a global AllDifferent
constraint for each maximal clique. This introduces redundancies in AllDifferent
constraints and may prune more values, but at a higher cost.
For both approaches, we may consider diﬀerent heuristics to build cliques.
Several tests (not reported due to lack of space) showed us that a simple greedy
construction of maximal cliques yields a good tradeoﬀbetween the time spent
building the cliques, the time spent to propagate AllDifferent constraints, and
the reduction of the search space. More precisely, for each vertex of the graph,
we build a maximal clique in a greedy way: starting from a clique that contains
this vertex, we iteratively choose the vertex with the largest degree among the
vertices that can correctly extend the clique, until no such vertex exists. We then
post a global AllDifferent constraint for each of these maximal cliques.
3.4
Lower Bound
The main drawback of the CP model proposed in [22] is due to the poor lower
bound, which is the sum of minimal values in domains. This lower bound does not
take into account the disequality constraints. A better lower bound is obtained

Combining CP and ILP in a Tree Decomposition of Bounded Height
363
by using a clique decomposition C, as proposed in the B&B approach of [22]: it
is deﬁned by the sum, for each clique Ci of C, of the sum of the |Ci| smallest
values in the union of the domains of the variables associated with vertices
of Ci. This new bound takes into account some disequality constraints (those
between pairs of variables that belong to a same clique). However, the sum
of the |Ci| smallest values may be a bad approximation of the chromatic sum
of the subgraph induced by Ci when variables of Ci have diﬀerent domains.
Let us consider for example a clique Ci = {a, b, c} with D(a) = {1, 2, 3} and
D(b) = D(c) = {7, 8}. The sum of the 3 smallest values in D(a) ∪D(b) ∪D(c)
is 1 + 2 + 3 = 6, whereas the chromatic sum of the subgraph induced by {a, b, c}
is 1 + 7 + 8 = 16.
Combining AllDiﬀerent and Sum Constraints. Better bounds may be
computed by considering the global constraint that combines an AllDifferent
constraint with a sum constraint on the same set of variables [2]. In particular,
[2] proposed a bound consistency algorithm for this global constraint. The main
idea relies on the notion of “blocks” of variables, deﬁned in such a way that, for
a given AllDifferent constraint, variables of the same block are interchangeable.
Each block also has a set of values. An initial lower bound is computed in
O(n log(n)), where n is the number of variables in the AllDifferent contraint.
During the search, if a variable of a block is assigned with a value of this block,
the lower bound is left unchanged, otherwise, it is updated in O(1).
Note that the clique decomposition used to compute lower bounds is diﬀerent
from the set of maximal cliques used to propagate disequality constraints as
proposed in Sect. 3.3. In the clique decomposition used to compute lower bounds,
some disequality constraints are missing (those between vertices that belong to
diﬀerent cliques). Hence, the propagation of the conjunctions of AllDifferent
and sum global constraints (to compute the lower bound) does not ensure a
proper colouring. It must be combined either with binary disequality constraints
between neighbour vertices that belong to diﬀerent cliques, or with AllDifferent
constraints as deﬁned in Sect. 3.3.
Computation of the Clique Decomposition. We may consider diﬀerent
heuristics to build clique decompositions, and diﬀerent clique decompositions
may lead to diﬀerent bounds. To build a clique decomposition, we consider a
basic greedy approach very similar to the one described in Sect. 3.3 to compute
a set of maximal cliques: the only diﬀerence is that each time a vertex is added
to a clique, it is removed from the graph so that it cannot be selected for another
clique. A key point to obtain a good tradeoﬀbetween the time spent to compute
bounds and the reduction of the search space lies in the frequency of the com-
putation of a clique decomposition. In the B&B approach of [22], a new clique
decomposition is computed at each node of the search tree, on the subgraph
induced by uncoloured vertices. This allows to compute more accurate bounds,
but clique decomposition computations are rather expensive. Hence, we propose
to compute a clique decomposition only once, at the root of the search tree. This
partition is then used at each node of the tree to compute a new bound.

364
M. Minot et al.
Triggering of the Bound Computation. Trying too early to prune branches
with bound computations often leads to a loss of eﬃciency: when only a few
vertices are coloured, we do not have enough information as to how the colouring
will turn out. The computed lower bound is thus too low to be of any use. To
prevent unnecessary computations, we set a lower limit for the triggering of
the bound computation: if the distance between the sum of currently assigned
colours and the current upper bound amounts to more than gap %, we refrain
from computing the lower bound. In addition to this lower triggering limit, we
added an upper one: we refrain from using this bound when unc or less vertices
are uncoloured. The reason behind this is that when only a few vertices are left
uncoloured, it may be faster to explore what remains in this part of the search
space rather than using a bound to try to prune this very small branch.
3.5
Experimental Comparison
Experimental Setup and Benchmark. Programs are executed on an Intel R
⃝
Xeon R
⃝CPU E5-2670 at 2.60 GHz processor, with 0480 KB of cache memory
and 4 GB of RAM. We consider 126 instances which are classically used for sum
colouring, as in [12,31]. Some are from COLOR02/03/041, but most of them are
DIMACS instances designed for the classical colouring problem2. The timeout
was set to 24 h. For each instance, the reference solution is the best known upper
bound, either available in the literature (mainly [12,31]), or previously computed
by one of our approaches. It gives an overview of the state of the art. Tables
also give detailed results for a set of ten instances that we chose to highlight the
peculiarities of each approach.
Conﬁgurations. We implemented CP models in Gecode (version 4.2.1) [29].
We cannot report results for all possible combinations of the diﬀerent improve-
ments described in Sects. 3.1, 3.2, 3.3 and 3.4. We have chosen the following
conﬁgurations:
– Base: Basic model, with binary disequality constraints, a lower bound deﬁned
as the sum of smallest values in variable domains, and bound consistency
ensured.
– AllDiﬀ+Bound: Model with AllDifferent constraints (as deﬁned in Sect. 3.3),
a lower bound deﬁned by using a clique partition and computing for each
clique Ci the sum of the |Ci| smallest values in variables’ domains, and bound
consistency ensured. Parameters gap and unc are set to 20% and 5, respec-
tively. Experiments not detailed in this paper showed these values oﬀer the
best compromise.
– AllDiﬀ+Bound+Swap: Same as AllDiﬀ+Bound, but with colour swapping.
– AllDiﬀ+Bound+Swap+Dom:
Same
as
AllDiﬀ+Bound+Swap,
but
with
domain consistency instead of bound consistency.
1 http://mat.gsia.cmu.edu/COLOR02.
2 ftp://dimacs.rutgers.edu/pub/challenge/graph/benchmarks/color/.

Combining CP and ILP in a Tree Decomposition of Bounded Height
365
– AllDiﬀ+SumBound+Swap: Same as AllDiﬀ+Bound+Swap, but lower bound
computation is done by using the bound consistency algorithm of [2]. gap and
unc are respectively set to 50% and 0 (the best setting for this conﬁguration).
For these ﬁve conﬁgurations, the Branch and Bound (BAB) search engine
was selected. As the goal is to minimize the sum of the variables, the value
ordering heuristic chooses the smallest value. We have designed and compared
diﬀerent variable ordering heuristics (including well-known ones such as Activity
and wDeg) and the best results are obtained with minElim, that chooses the
variable that has the smallest value in its domain, and break ties by choosing
the variable for which this smallest value would be removed from the fewest
domains. Luby was used as a restart policy, with a scale of 500.
Results. Table 1 compares CP models on 10 representative instances, and then
gives global results for the whole benchmark. AllDiﬀ+Bound outperforms Base on
6 instances out of the 10, and adding colour swapping (+Swap) allows an overall
improvement of bounds and generally faster proofs. Replacing bound consistency
(in AllDiﬀ+Bound+Swap) with domain consistency (in AllDiﬀ+Bound+Swap-
+Dom) pays oﬀon some instances, but degrades the solution process on some oth-
ers. Finally, using the global constraint that combines a sum and an AllDifferent
constraint improves the solution process on some instances, but also often degrades
it. Actually, we noticed that, in many cases, all variables in the global constraint
have very similar domains. Therefore, the bound computed for their sum is very
close to the sum of the smallest values in the union of the domains.
As a conclusion, none of the proposed CP model appears to be competitive
with state-of-the-art incomplete approaches, as the best model (AllDiﬀ+Bound-
+Swap) is able to reach the reference solution for only 49 instances.
Table 1. Comparison of CP models. The ﬁrst ten lines detail results for ten representative
instances: best upper bound found within the time limit (UB), time needed to ﬁnd UB
(tUB) and to prove optimality (tproof), if optimality has been proven. The last three lines
give the average distance between UB and the reference solution (in percentage), and the
number of instances for which the reference solution has been found (# Ref. sol.), and
optimality has been proven (# Optim. proofs) for the 126 instances.

366
M. Minot et al.
Making Proofs with CP. None of the conﬁgurations considered above are
good at proving optimality, as proofs were only made for 11 instances. This may
be due to the variable ordering heuristic minElim, which aims at quickly ﬁnding
good solutions. Hence, we conducted experiments with a new CP conﬁguration,
designed to prioritize proof-making. It employs a hybrid restart policy, coupled
with a scheduled heuristic change. In this conﬁguration, we use the same setting
as for AllDiﬀ+Bound+Swap+Dom but we change the variable ordering heuristic
during the search: as soon as the search endured 50 consecutive restarts without
having improved the global upper bound, minElim is replaced by a heuristic that
aims at proving optimality (it chooses the variable that has the highest number
of uncoloured neighbours, and break ties by ﬁrst choosing the variable that has
the smallest value in its domain, and then the smallest current domain). When
the variable ordering heuristic is changed, we also change the restart policy for
a geometric policy, with a scale value of 100 and a base of 2.
Using this conﬁguration, we are able to prove optimality for 15 instances
instead of 11. Though this is an improvement of 36%, this is still far from the
state of the art. For example, using all known bounds computed with heuristic
approaches, optimality was proven for 21 of the 94 instances considered in [12].
4
Integer Linear Programming
Two improvements introduced in the previous section for CP may be easily
adapted to ILP. Firstly, initial domain reduction is enacted simply by declaring
less variables and by removing diﬀerence constraints between nodes when the
considered colour is not in the domains of both nodes. Secondly, AllDifferent
constraints are modeled by adding the constraint 
v∈Ci xvk ≤1, for each clique
Ci and each colour k (instead of binary disequalities).
Implementation. We used ILOG CPLEX (version 12.6.2) [4]. To help CPLEX
to avoid running out of memory, the two following parameters were added:
Depth-First Search was forced as a node selection strategy, and the cuts fac-
tor was set to 1.5. Previous experiments showed us that these parameters did
not signiﬁcantly lessen CPLEX’s ability to solve the instances we use.
Experimental Results. Table 2 compares results of the initial ILP model as
proposed in [31], denoted ILP, with the ILP model that includes the two improve-
ments, denoted ILP+. The most notable improvement is due to domain reduc-
tion, since reducing domains for ILP also removes variables and constraints.
Overall, improvements allowed us to increase the number of optimality proofs
from 61 to 65, and the number of times the reference solution has been found
from 66 to 73. Besides, the number of memory outs goes down from 28 to 23.
When comparing ILP+ to CP, we note that they perform very diﬀerently. For
four instances (DSJC* and school*), ILP+ ran out of memory. Therefore, the
best solution found is far from the reference solution, and from the best solution

Combining CP and ILP in a Tree Decomposition of Bounded Height
367
found with CP models. For qg.order60, the best solution found by ILP is far
from optimality, whereas all CP models are able to reach the optimum, with two
of them even proving optimality. However, for the ﬁve remaining instances, ILP
either ﬁnds better solutions (ash331GPIA, le450 5b), proves optimality quicker
(3-Insertions 3, r125.1), or proves optimality while CP cannot (inithx.i.3).
When considering global results (on the whole benchmark), ILP+ is able to ﬁnd
reference solutions and to prove optimality much more often than CP, but the
average distance to reference solutions is much larger, mostly because of the
times it ran out of memory.
5
Combining CP and ILP
Experiments reported previously showed us that CP and ILP have complemen-
tary abilities: ILP is very eﬃcient to solve small instances, but it runs out of
memory for 23 instances; it never occurs with CP, but its solutions are often
far from the optimum, despite a rather good average distance. We therefore
propose to decompose the problem into smaller independent subproblems, and
to combine CP and ILP to solve these subproblems. The goal is to identify a
subset Kr ⊆V of nodes, for which we compute all proper colourings with CP.
From each of these colourings, we derive an independent subproblem. Given the
optimal sum colouring of each of these independent subproblems, we deduce the
optimal solution of the original problem in a straightforward way. If the sub-
problems are small enough, they can be solved with ILP. Furthermore, when
instances are well structured, we may choose the subset Kr so that, for each
colouring of Kr, we obtain several independent subproblems (instead of a single
one), even easier to solve with ILP. This idea is reminiscent of the approach
called Backtracking on Tree Decomposition (BTD) [9].
In Sect. 5.1, we recall the basic principles of BTD and show how to use it to
solve the SCP. In Sect. 5.2, we introduce a new decomposition, as well as a way
to use it. This approach is called BFD, and can be employed to combine CP and
ILP. Decomposition methods are experimentally compared in Sect. 5.3.
5.1
Tree Decomposition
Deﬁnition 2. [27] A tree decomposition of a graph (V, E) is a couple (K, T) where
T = (I, F) is a tree, and K : I →P(V ) is a function which associates a subset of
variables Ki ⊆V (called a cluster) with every node i ∈I such that the following
conditions are satisﬁed: (i) ∪i∈IKi = V ; (ii) for each edge (vj, vk) ∈E, there exists
a node i ∈I such that {vj, vk} ⊆Ki; and (iii) for all i, j, k ∈I, if k is in a path from
i to j in T, then Ki∩Kj ⊆Kk. The width of a tree decomposition is maxi∈I |Ki|−1.
Intersections of neighbour clusters are called separators.
BTD is a generic approach that exploits a tree decomposition of the con-
straint (hyper)graph of a CSP to identify independent subproblems which are
solved separately. More precisely, given a tree decomposition (K, T) and a root

368
M. Minot et al.
node r ∈I, BTD ﬁrst assigns the variables of the root cluster Kr. Then, BTD
recursively solves, for each child i of r, the independent subproblem that con-
tains all variables occurring in the clusters associated with the subtree rooted
in i. To avoid the repeated exploration of same parts of the search space, BTD
records structural (no)goods, that allow to reduce time complexity to O(ndw+1)
with n the number of variables, d the maximum domain size and w the width
of T. The space complexity is O(nsds) with s the size of the largest separator.
BTD may be used to solve optimization problems provided that the objective
function is decomposable, i.e., once all variables of a root cluster r are assigned,
the optimal solution that extends this partial assignment may be obtained by
computing separately, for each child of r, the optimal solution of the subproblem
associated with this child [5,9]. In this case, we have to record structural valued
goods, i.e., pairs composed by an assignment of the variables of a separator
and the optimal solution of the subproblem associated with this child for this
assigment. To avoid solving to optimality a subproblem if it is obvious its optimal
solution cannot be extended to a global solution, an upper bound is added to the
subproblem. As soon as it is proved that the optimal solution of the subproblem
cannot be lower than the upper bound, the solving of the subproblem is stopped.
BTD may be used to solve the SCP as its objective function is decomposable:
given a tree decomposition of the graph to colour, once the nodes of a root cluster
r are coloured, the best sum colouring that extends this partial colouring may
be obtained by searching separately the best sum colouring of each child of r.
Note that this is not the case, for example, of the classical colouring problem, as
we cannot colour children separately when the goal is to minimize the number
of used colours (as we must know the colours used by other clusters).
However, experiments on our 126 instances have shown us that many instances
are poorly structured: when computing a tree decomposition with MinFill [16], 65
instances are not decomposed at all (i.e., there is only one cluster, which contains
all variables), and 103 instances have at least one cluster that contains more than
90% of the variables. For these instances, BTD behaves poorly.
5.2
Backtracking with Flower Decomposition (BFD)
Even when the tree decomposition only contains one cluster, we may decompose
it into two subsets (Kr and V \Kr): we use CP to enumerate all proper colouring
of Kr, and then, for each of these colourings, we use ILP to ﬁnd the optimal
sum colouring of V \ Kr (given the colours assigned to Kr).
The idea of BFD is to exploit instance structure so that, for each colouring of
Kr, we may split V \Kr into several subsets that may be solved to optimality with
ILP independently. In other words, we propose to compute a tree decomposition
with a height of 1, composed of a root cluster Kr, and a set of leaf clusters which
are all children of Kr. The key point is to choose the nodes of Kr so that we
obtain leaves that are small enough to be solved to optimality with ILP. To this
end, we introduce a parameter l, that enforces a limit on the size of the leaves.
More precisely, for each leaf cluster Ki, we ensure that |Ki \ Kr| ≤l × |V |.
Besides this hard constraint on the leaf sizes, we also aim at favoring small roots
(as we have to enumerate all its proper colourings).

Combining CP and ILP in a Tree Decomposition of Bounded Height
369
This ﬂower decomposition is built as follows. We ﬁrst build a tree decompo-
sition (K, T = (I, F)) of the graph G = (V, E) with MinFill. Let S be the set
of all separators, i.e., S = {Ki1 ∩Ki2|{i1, i2} ∈F}. Given a subset S′ ⊆S, we
deﬁne a ﬂower decomposition whose root is the cluster K′
r = ∪s∈S′s. The other
clusters of the ﬂower (the leaves) are deﬁned by the connected components of
the subgraph of G induced by V \ K′
r. Each leaf cluster K′
i is then extended
by adding to it any vertex of K′
r adjacent to a vertex of K′
i. A similar process
was employed in [10], where it was also demonstrated that it results in a correct
tree decomposition. Of course, the quality of the obtained ﬂower decomposition
depends on the initial subset S′. The goal is to ﬁnd the subset S′ such that
the resulting ﬂower decomposition satisﬁes the size limit l on the leaf clusters
while minimizing the size of K′
r. We use Gecode to solve this problem. As it is
NP-hard, we limit the CPU time for computing it to 15 min, and use the best
ﬂower decomposition computed within this time limit. If Gecode has not found
any ﬂower decomposition that satisﬁes the size limit l on the leaf clusters, we
build a ﬂower decomposition which only contains two clusters and such that the
root cluster contains the |V | × (1 −l) vertices with largest degrees.
As with BTD, we also enforce a limit on the size of cluster separators. In the
context of BTD, this is done by merging clusters whose separators contain too
many variables. In our case, however, we keep the clusters as they are: the only
eﬀect of the limit is that no valued good is recorded on the separators which
exceed the limit, since doing so would be very likely to consume a large amount
of memory. Moreover, if a separator corresponds to the full root cluster, there is
no need to record any valued good on it, since aﬀectations on such a separator
cannot be produced more than once.
5.3
Experimental Evaluation
We compare two approaches, denoted BTD and BFD. BTD refers to the classical
BTD approach. In this case, the tree decomposition is built using the MinFill
algorithm and CP is used to solve subproblems: leaf clusters are solved with the
Gecode conﬁguration AllDiﬀ+Bound+Swap (that uses restarts), whereas non-
leaf clusters are solved with the same conﬁguration, but without restarts, as we
need to enumerate all solutions.
BFD refers to our new approach, based on a ﬂower decomposition. CP (All-
Diﬀ+Bound+Swap without restarts) is used to enumerate the solutions of the
non-leaf clusters and ILP+ to solve the subproblems induced by the leaves for
each assignment of the separators. The maximal size for the separators is set
to 30. We report results with two values for the “l” parameter (that limits the
size of leaf clusters): 75% (denoted BFD 75) and 90% (denoted BFD 90).
Table 2 reports experimental results of BTD, BFD 90 and BFD 75. When
looking at the detailed results on our ten representative instances, we note that
they have complementary results: BTD is better than BFD 90 and BFD 75 on
DSJC250.5 and school1, BFD 90 is better on ash331GPIA, 3-Insertions 3,
inithx.i.3, school1 nsh and r125.1, and BFD 75 is better on DSJC1000.1,
le450 5b, qg.order60 and r125.1. For two of these instances (r125.1 and

370
M. Minot et al.
Table 2. Comparison of ILP, ILP+, BTD, BFD 90 and BFD 75. The ﬁrst ten lines
detail results for as many representative instances. “#M#” in tproof means a memory
out occurred. The last four lines give, for the 126 instances: the average distance from
UB to the reference solution (in percentage of the reference solution); the number
of instances for which the reference solution was found (# Ref. sol.); the number of
instances for which optimality was proved (# Optim. proofs); the number of times
search was aborted due to a lack of memory (# Out of memory).
school1 nsh), the best results, over all considered approaches, are actually
obtained by BFD 90.
When looking at global results over the whole benchmark, BTD is able to
ﬁnd the reference solution for only 17 instances (instead of 46 and 48 for BFD 90
and BFD 75), and it proves optimality for 13 instances only (instead of 39 and
26). Actually, as pointed out previously, most instances have no structure at all,
or only a very poor structure, and BTD is generally outperformed on them by
the CP approaches introduced in Sect. 3.
BFD 90 and BFD 75 are able to ﬁnd reference solutions and to prove opti-
mality for much more instances than BTD. Actually, even if the instance is
not structured at all (i.e., there is only one cluster in the tree decomposition,
which happens 40 times for our 126 instances), BFD is still able to build a ﬂower
decomposition with one root cluster (that contains |V |·(1−l) nodes) and one leaf
(that contains the remaining |V | · l variables). In this case, BFD often behaves
much better than BTD. However, BFD also suﬀers from a relatively high average
distance to reference solutions. Actually, on some instances, BFD spends a lot of
time to enumerate colourings for the root cluster which cannot be extended to
good solutions. However, for each of these colourings, BFD wastes a lot of time
solving to optimality useless subproblems.
Comparing BFD 90 and BFD 75 proves that allowing larger leaf clusters
increases the memory needs but also eases the computation of upper bounds, as
it makes the root cluster smaller (less enumeration) and gives ILP a more global
view of the problem, preventing it in some cases to spend too much time solving a
useless subproblem to optimality. When comparing BFD with the CP approaches
of Sect. 3, we note an increase in the number of proofs (39 and 26 instead of
11), but the average distance to the reference solution is larger. Compared with

Combining CP and ILP in a Tree Decomposition of Bounded Height
371
ILP+, BFD ﬁnds the reference solution less often, as with optimality proofs,
but there also are less failures due to memory. Actually, BFD and ILP+ have
complementary performance: BFD 90 (resp. BFD 75) performs strictly better
than CPLEX on 21 (resp. 28) instances, and strictly worse on 91 (resp. 85)
instances.
6
Portfolio Approach
We have introduced diﬀerent approaches for the SCP in the previous sections.
Some of them are dominated, in the sense that, for each instance, there is always
another approach that performs better on this instance (it ﬁnds the same solu-
tion quicker, or a better solution). This is the case of the CP conﬁgurations
Base and AllDiﬀ+Bound, as well as ILP, BTD and BFD 75. The ﬁve other
approaches (namely, the three remaining CP conﬁgurations, BFD 90 and ILP+)
are complementary, and we propose to combine them in a portfolio approach.
More precisely, given a solver portfolio [6,8], the per-instance algorithm selec-
tion problem [26] consists in selecting the solver of the portfolio which is expected
to perform best on a given instance. Algorithm selection systems usually build
machine learning models to forecast which solver should be used in a particu-
lar context. Using the predictions, one or more solvers from the portfolio may
be selected to be run sequentially or in parallel. In our SCP context, solver
performance is highly constrained by memory bandwidth, in particular for ILP.
Therefore, we cannot simply run our diﬀerent solvers in parallel, and we consider
the case where exactly one solver is selected.
One of the most prominent and successful systems that employs this app-
roach is SATzilla [34], which deﬁned the state of the art in SAT solving for a
number of years. Other application areas include constraint solving [25], the trav-
elling salesperson problem [19], subgraph isomorphism [20] and AI planning [28].
The reader is referred to a survey [18] for additional information on algorithm
selection.
The selection process is composed of two steps: given an SCP instance to be
solved, we ﬁrst extract features from instances; then, we run algorithm selection
to choose a solver. Finally we run the selected solver on the instance.
Feature Extraction. Given a graph G = (V, E) for which we are looking for
the chromatic sum, we compute the following features (a “*” denoting the use
of the minimum, maximum, mean and standard deviation): number of nodes |V |
and edges |E|, degrees of the vertices in V *, size of connected components in G*,
number of constraints and variables in the ILP+ model, number of AllDifferent
constraints (arity of more than 2) in the AllDiﬀ+Bound CP model, arity of these
AllDifferent constraints*. We also added features computed from the largest
connected component G′ of G: density, theoretical upper and lower bounds of
Σ(G′), number of clusters in the tree decomposition computed with MinFill.
Moreover, this tree decomposition is used to compute a ﬂower decomposition
(with l = 90), which gives additional features: size of the root cluster, Cartesian

372
M. Minot et al.
product of the sizes of the domains in the root cluster, number of clusters,
distance between the theoretical upper and lower bounds of the root cluster,
density of the root cluster, density of leaf clusters*, number of proper variables in
clusters*, separator density*, separator sizes*, the distance between theoretical
upper and lower bounds on leaf clusters*, the number of binary variables* and
constraints* in the ILP+ model associated with leaf clusters.
Selection Model. We use Llama [17] to build our solver selection model.
Llama supports the most common algorithm selection approaches used in the
literature. We performed a set of preliminary experiments to determine the app-
roach that works best here, i.e., a pairwise regression approach with random
forest regression. This approach trains a model that predicts the performance
diﬀerence between every pair of solvers in the portfolio, similarly to what is
done in [34]: if the ﬁrst solver is better than the second, the diﬀerence is pos-
itive, otherwise negative. The solver with the highest cumulative performance
diﬀerence (i.e., the most positive diﬀerence over all other solvers) is chosen to be
run. As this approach already gives very good performance, we did not tune the
parameters of the random forest machine learning algorithm. It is possible that
overall performance can be improved by doing so, and we make no claims that
the particular solver selection approach we use in this paper cannot be improved.
Experimental Results. We use leave-one-out cross-validation to determine
the performance of our portfolio approach, as we only have 126 instances in our
benchmark (which is not much for training a learning model): for each instance
i, we train the selection model on all instances but i, and evaluate it on i.
The set of features is computed in 5.4 min in average, with 110 of our
instances actually being under 5 min. Some instances, such as latin square 10,
DSJC1000.9 or flat1000 50 0 take a prohibitive amount of time when comput-
ing our set of features, mostly because of the two decompositions needed.
Table 3 shows us that our portfolio approach obtains results that are close
to those of the Virtual Best Solver (VBS), which considers the best solver for
each instance separately. It often selects either the best solver, or a solver which
behaves well. The VBS (resp. our portfolio approach) uses ILP+ for 67 (resp.
78) instances, BFD 90 for 10 (resp. 13) instances, AllDiﬀ+Bound+Swap for 20
(resp. 11) instances, AllDiﬀ+Bound+Swap+Dom for 14 (resp. 11) instances, and
AllDiﬀ+SumBound+Swap for 15 (resp. 13) instances. Even when the portfolio
selects the best solver, the solving time is increased by the time needed to com-
pute features (which may be large on some instances). Note that the time needed
by the model to select a solver is negligible (0.15 s on average).
Our portfolio is able to prove optimality for more than half of the 126
instances. In [12], best upper and lower bounds are reported for a set of state-of-
the-art heuristic approaches, on a subset of 92 instances. Using the best of these
bounds (computed with diﬀerent heuristic approaches), they can prove optimal-
ity for 21 of these instances, whereas our portfolio approach is able to prove
optimality for 42 of these 92 instances, i.e., twice more. Finally, our portfolio

Combining CP and ILP in a Tree Decomposition of Bounded Height
373
Table 3. Detailed results, for the virtual best solver and our portfolio approach. tfeat.
is the time needed to compute the features. For each method, “Algo” gives the chosen
algorithm. Gec1 denotes AllDiﬀ+Bound+Swap, Gec2 AllDiﬀ+Bound+Swap+Dom, and
Gec3 AllDiﬀ+SumBound+Swap. Times tUB and tproof for the portfolio include tfeat..
Name
Ref. sol.
Virtual best solver
Portfolio approach
Algo
UB
tUB
tproof
tfeat.
Algo
UB
tUB
tproof
DSJC250.5
3210
Gec2
3540
51076
6
Gec1
3591
845
DSJC1000.1
8991
Gec3
10295
66842
126
Gec1
10328
3466
ash331GPIA
1432
ILP+
1432
29870
20
ILP+
1432
29890
3-Insert. 3
92
ILP+
92
0
0
0
ILP+
92
0
0
le450 5b
1350
ILP+
1398
22554
4
ILP+
1398
22558
qg.order60
109800
Gec2
109800
138
139
4866
Gec1
109800
5070
5070
r125.1
257
BFD
257
0
0
0
BFD
257
0
0
inithx.i.3
1986
ILP+
1986
9
20
8
BFD
1986
114
694
school1
2674
Gec1
3531
75519
18
Gec3
3648
1149
school1 nsh
2392
BFD
2539
86400
10
Gec3
2992
14166
Average dist. (%)
4.25
5.51
# Ref. sol.
83
76
# Optim. proofs
66
65
# Out of memory
0
0
approach has improved (resp. reached) the best upper bounds reported in [12]
for 2 (resp. 48) of the 92 instances: for DSJR500.1 the new upper bound is 2142
instead of 2156, and for le450 25b it is 3349 instead of 3365.
7
Conclusion and Future Work
We proposed some improvements for solving the SCP with CP and ILP, and
demonstrated that they have complementary advantages: ILP is eﬃcient on
small instances, but fails to solve large instances due to its large memory needs;
CP never runs out of memory but is not able to compute as good solutions
as ILP on small instances. We proposed a CP/ ILP combination that may be
used as a compromise between CP and ILP. Besides, since it makes use of tree
decomposition, this combination is better than CP or ILP alone to solve some
well-structured instances. We combined those methods in a portfolio approach
which obtains results close to those of the virtual best solver. It has been able
to prove optimality for more than half of the considered instances. It has also
been able to improve the best known upper bounds for two instances.
In the future, the use of a dedicated decomposition algorithm might be stud-
ied, in order to build a ﬂower decomposition from scratch rather than resorting
to an initial tree decomposition. The goal would be to make it more straightfor-
ward to obtain a balanced decomposition. Being able to automatically ﬁne-tune
BFD’s parameters (l as well as the maximal size of separators) could also prove
useful.

374
M. Minot et al.
A major drawback of BFD is that some subproblems are solved to optimality
even if they are useless due to poor assignments in the root. An interesting
improvement that we shall investigate in further research would be to prevent
ILP from spending more that a set amount of time on a leaf cluster, and to ask
for a new assignment on the root if necessary. It could be seen as another form
of restarts, as seen in [1].
Acknowledgements. This work has been supported by the ANR project SoLStiCe
(ANR-13-BS02-0002-01).
References
1. Allouche, D., Givry, S., Katsirelos, G., Schiex, T., Zytnicki, M.: Anytime hybrid
best-ﬁrst search with tree decomposition for weighted CSP. In: Pesant, G. (ed.)
CP 2015. LNCS, vol. 9255, pp. 12–29. Springer, Cham (2015). doi:10.1007/
978-3-319-23219-5 2
2. Beldiceanu, N., Carlsson, M., Petit, T., R´egin, J.C.: An o(nlog n) bound consistency
algorithm for the conjunction of an alldiﬀerent and an inequality between a sum
of variables and a constant, and its generalization. In: ECAI, vol. 12, pp. 145–150
(2012)
3. Benlic, U., Hao, J.-K.: A study of breakout local search for the minimum sum
coloring problem. In: Bui, L.T., Ong, Y.S., Hoai, N.X., Ishibuchi, H., Suganthan,
P.N. (eds.) SEAL 2012. LNCS, vol. 7673, pp. 128–137. Springer, Heidelberg (2012).
doi:10.1007/978-3-642-34859-4 13
4. Cplex, I.: High-performance software for mathematical programming and optimiza-
tion (2005)
5. De Givry, S., Schiex, T., Verfaillie, G.: Exploiting tree decomposition and soft local
consistency in weighted CSP. In: AAAI, vol. 6, pp. 1–6 (2006)
6. Gomes, C.P., Selman, B.: Algorithm portfolios. Artif. Intell. 126(1–2), 43–62 (2001)
7. Helmar, A., Chiarandini, M.: A local search heuristic for chromatic sum. In: Pro-
ceedings of the 9th Metaheuristics International Conference, vol. 1101, pp. 161–170
(2011)
8. Huberman, B.A., Lukose, R.M., Hogg, T.: An economics approach to hard com-
putational problems. Sci. 275(5296), 51–54 (1997)
9. J´egou, P., Terrioux, C.: Hybrid backtracking bounded by tree-decomposition of
constraint networks. Artif. Intell. 146, 43–75 (2003)
10. J´egou, P., Kanso, H., Terrioux, C.: An algorithmic framework for decomposing
constraint networks. In: 2015 IEEE 27th International Conference on Tools with
Artiﬁcial Intelligence (ICTAI), pp. 1–8. IEEE (2015)
11. Jin, Y., Hao, J.K.: Hybrid evolutionary search for the minimum sum coloring
problem of graphs. Inf. Sci. 352, 15–34 (2016)
12. Jin, Y., Hamiez, J.P., Hao, J.K.: Algorithms for the minimum sum coloring prob-
lem: a review (2015). arXiv:1505.00449
13. Jin, Y., Hao, J.K., Hamiez, J.P.: A memetic algorithm for the minimum sum
coloring problem. Comput. Oper. Res. 43, 318–327 (2014)
14. Jussien, N., Rochart, G., Lorca, X.: Choco: an open source java constraint program-
ming library. In: CPAIOR 2008 Workshop on Open-Source Software for Integer and
Contraint Programming (OSSICP 2008), pp. 1–10 (2008)

Combining CP and ILP in a Tree Decomposition of Bounded Height
375
15. Kadioglu, S., Malitsky, Y., Sellmann, M., Tierney, K.: Isac-instance-speciﬁc algo-
rithm conﬁguration. In: ECAI, vol. 215, pp. 751–756 (2010)
16. Kjaerulﬀ, U.: Triangulation of graphs - algorithms giving small total state space.
Technical report, Judex R.R. Aalborg, Denmark (1990)
17. Kotthoﬀ, L.: LLAMA: Leveraging learning to automatically manage algorithms.
Technical report, June 2013. http://arxiv.org/abs/1306.1031
18. Kotthoﬀ, L.: Algorithm selection for combinatorial search problems: a survey. AI
Mag. 35(3), 48–60 (2014)
19. Kotthoﬀ, L., Kerschke, P., Hoos, H., Trautmann, H.: Improving the state of the
art in inexact TSP solving using per-instance algorithm selection. In: Dhaenens,
C., Jourdan, L., Marmion, M.-E. (eds.) LION 2015. LNCS, vol. 8994, pp. 202–217.
Springer, Cham (2015). doi:10.1007/978-3-319-19084-6 18
20. Kotthoﬀ, L., McCreesh, C., Solnon, C.: Portfolios of subgraph isomorphism algo-
rithms. In: Festa, P., Sellmann, M., Vanschoren, J. (eds.) LION 2016. LNCS, vol.
10079, pp. 107–122. Springer, Cham (2016). doi:10.1007/978-3-319-50349-3 8
21. Kubale, M.: Graph Colorings, vol. 352. American Mathematical Society, Canada
(2004)
22. Lecat, C., Li, C.M., Lucet, C., Li, Y.: Exact methods for the minimum sum col-
oring problem. In: DPCP-2015, Cork, Ireland, Iran, pp. 61–69 (2015). https://hal.
archives-ouvertes.fr/hal-01323741
23. Moukrim, A., Sghiouer, K., Lucet, C., Li, Y.: Upper and lower bounds for the
minimum sum coloring problem (Submitted for Publication)
24. Moukrim, A., Sghiouer, K., Lucet, C., Li, Y.: Lower bounds for the minimal sum
coloring problem. Electron. Notes Discrete Math. 36, 663–670 (2010)
25. O’Mahony, E., Hebrard, E., Holland, A., Nugent, C., O’Sullivan, B.: Using case-
based reasoning in an algorithm portfolio for constraint solving. In: Proceedings of
the 19th Irish Conference on Artiﬁcial Intelligence and Cognitive Science, January
2008
26. Rice, J.R.: The algorithm selection problem. Adv. Comput. 15, 65–118 (1976)
27. Robertson, N., Seymour, P.: Graph minors II: algorithmic aspects of tree-width.
J. Algorithms 7, 309–322 (1986)
28. Seipp, J., Braun, M., Garimort, J., Helmert, M.: Learning portfolios of automati-
cally tuned planners. In: ICAPS (2012)
29. Team, G.: Gecode: generic constraint development environment (2006, 2008)
30. Thomassen, C., Erd¨os, P., Alavi, Y., Malde, P.J., Schwenk, A.J.: Tight bounds on
the chromatic sum of a connected graph. J. Graph Theor. 13(3), 353–357 (1989)
31. Wang, Y., Hao, J.K., Glover, F., L¨u, Z.: Solving the minimum sum coloring problem
via binary quadratic programming (2013). arXiv:1304.5876
32. Wu, Q., Hao, J.K.: An eﬀective heuristic algorithm for sum coloring of graphs.
Comput. Oper. Res. 39(7), 1593–1600 (2012)
33. Wu, Q., Hao, J.K.: Improved lower bounds for sum coloring via clique decomposi-
tion. arXiv preprint (2013). arXiv:1303.6761
34. Xu, L., Hutter, F., Hoos, H.H., Leyton-Brown, K.: SATzilla: portfolio-based algo-
rithm selection for SAT. J. Artif. Intell. Res. 32, 565–606 (2008)

htd – A Free, Open-Source Framework
for (Customized) Tree Decompositions
and Beyond
Michael Abseher(B), Nysret Musliu, and Stefan Woltran
Institute of Information Systems, TU Wien,
184/2, Favoritenstraße 9–11, 1040 Vienna, Austria
{abseher,musliu,woltran}@dbai.tuwien.ac.at
Abstract. Decompositions of graphs play a central role in the ﬁeld of
parameterized complexity and are the basis for many ﬁxed-parameter
tractable algorithms for problems that are NP-hard in general. Tree
decompositions are the most prominent concept in this context and sev-
eral tools for computing tree decompositions recently competed in the
1st Parameterized Algorithms and Computational Experiments Chal-
lenge. However, in practice the quality of a tree decomposition cannot
be judged without taking concrete algorithms that make use of tree
decompositions into account. In fact, practical experience has shown
that generating decompositions of small width is not the only crucial
ingredient towards eﬃciency. To this end, we present htd, a free and
open-source software library, which includes eﬃcient implementations of
several heuristic approaches for tree decomposition and oﬀers various
features for normalization and customization of decompositions. The
aim of this article is to present the speciﬁcs of htd together with an
experimental evaluation underlining the eﬀectiveness and eﬃciency of
the implementation.
Keywords: Tree decompositions · Dynamic programming · Software
library
1
Introduction
Graph decompositions are an important concept in the ﬁeld of parameterized
complexity and a wide variety of such approaches can be found in the litera-
ture including tree decompositions [11,23,37], branch decompositions [38], and
hypertree decompositions [22] (of hypergraphs), to mention just a few. The con-
cept of tree decompositions gained special attention since many NP-hard search
problems become tractable when the parameter treewidth is bounded by some
constant k [8,12,36]. A problem that exhibits tractability by bounding some
problem-inherent constant is also called ﬁxed-parameter tractable (FPT) [18].
The standard technique for solving a given problem using this concept is the
computation of a tree decomposition followed by a dynamic programming (DP)
algorithm that traverses the nodes of the decomposition and consecutively solves
the respective sub-problems [36]. For problems that are FPT w.r.t. treewidth,
c
⃝Springer International Publishing AG 2017
D. Salvagnin and M. Lombardi (Eds.): CPAIOR 2017, LNCS 10335, pp. 376–386, 2017.
DOI: 10.1007/978-3-319-59776-8 30

htd – A Free, Open-Source Framework for (Customized)
377
the general run-time of such algorithms for an instance of size n is f(k) · nO(1),
where f is an arbitrary function over width k of the used tree decomposition. In
fact, this approach has been used for several applications including the solving
of inference problems in probabilistic networks [32], frequency assignment [30],
computational biology [42], logic programming [33], routing problems [17], and
solving of quantiﬁed boolean formulae [14].
From a theoretical point of view the actual width k is the crucial parame-
ter towards eﬃciency for FPT algorithms that use tree decompositions. In the
literature various approaches for optimizing width when computing decompo-
sitions have been proposed (see Sect. 2), but to the best of our knowledge no
software frameworks exist which oﬀer the feature to customize tree decomposi-
tions by other criteria than just minimizing the plain width. However, experience
shows that even decompositions of the same width can lead to signiﬁcant dif-
ferences in the run-time of DP algorithms and recent results conﬁrm that the
width is indeed not the only important parameter that has a signiﬁcant inﬂuence
on the performance [27,33]. In particular, [4,6] has underlined that considering
such additional criteria is highly beneﬁcial. Nevertheless, a post-processing phase
based on machine learning was needed to determine “good” tree decompositions.
Therefore we see a strong need to oﬀer a specialized decomposition framework
that allows for directly constructing customized decompositions, i.e., decomposi-
tions which reﬂect certain preferences of the developer, in order to optimally ﬁt
to the DP algorithm in which they are used. In this paper we present a free, open-
source framework (htd) which supports a vast amount of input graph types and
diﬀerent types of decompositions. htd includes eﬃcient implementations of sev-
eral heuristic approaches for computing tree decompositions. Furthermore, htd
provides various built-in customization and manipulation operations in order
to ﬁt the needs of developers of DP algorithms. These include normalizations,
optimization of tree decompositions, computation of induced edges and labeling
operations (see Sect. 3).
Just recently, htd participated in the “First Parameterized Algorithms and
Computational Experiments Challenge” (“PACE16”)1 where it was ranked at
the third place in the heuristics track. Although htd provides lots of additional
convenience functions, the results of htd with respect to the optimization of
width are very close to those of the heuristic approaches ranked at the ﬁrst two
places. In Sect. 4, we will present some complementing experimental evaluation
that also sheds light on the eﬀect of customization when decompositions are used
in a speciﬁc DP algorithm.
htd has been already used successfully in diﬀerent projects, like D-FLAT [2],
a framework for rapid-prototyping of dynamic programming algorithms on tree
decompositions, or dynQBF [14], a DP-based solver for quantiﬁed boolean for-
mulae. Our framework is available for download as free, open-source software
at https://github.com/mabseher/htd. We consider htd as a potential starting
point for researchers to contribute their algorithms in order to provide a new
1 See https://pacechallenge.wordpress.com/track-a-treewidth/ for more details.

378
M. Abseher et al.
framework for all diﬀerent types of graph decompositions. A detailed report on
htd and all its features can be found in [5].
2
Background
The intuition underlying tree decompositions is to obtain a tree from a (poten-
tially cyclic) graph by subsuming multiple vertices in one node and thereby
isolating the parts responsible for the cyclicity. Formally, the notions of tree
decomposition and treewidth are deﬁned as follows [13,37].
Deﬁnition 1. Given a graph G = (V, E), a tree decomposition of G is a pair
(T, χ) where T = (N, F) is a tree and χ : N →2V assigns to each node a set of
vertices (called the node’s bag), such that the following conditions hold:
1. For each vertex v ∈V , there exists a node i ∈N such that v ∈χi.
2. For each edge (v, w) ∈E, there exists an i ∈N with v ∈χi and w ∈χi.
3. For each i, j, k ∈N: If j lies on the path between i and k then χi ∩χk ⊆χj.
The width of a given tree decomposition is deﬁned as max i∈N|χi| −1 and the
treewidth of a graph is the minimum width over all its tree decompositions.
Note that the tree decomposition of a graph is in general not unique. In the
following we consider rooted tree decompositions.
Deﬁnition 2. A normalized (or nice) tree decomposition of a graph G is a
rooted tree decomposition (T, χ) where each node i ∈N is of one of the following
types: (1) Leaf: i has no child nodes;(2) Introduce Node: i has one child j with
χj ⊂χi and |χi| = |χj|+1; (3) Forget Node: i has one child j with χj ⊃χi and
|χi| = |χj| −1;(4) Join Node: i has two children j, k with χi =χj =χk.
Each tree decomposition can be transformed into a normalized one in linear
time without increasing the width [29]. Apart from nice tree decompositions,
one can ﬁnd numerous other normalizations in the literature [3].
While the problem of constructing an optimal tree decomposition, i.e. a
decomposition with minimal width, is intractable [7], researchers have proposed
several exact methods for small graphs and eﬃcient heuristic approaches that
usually construct tree decompositions of almost optimal width for larger graphs.
Examples of exact algorithms for tree decompositions are [9,21,39]; greedy
heuristic algorithms include Minimum Degree heuristic [10], Maximum Cardi-
nality Search (MCS) [40], and Min-Fill heuristic [16]. Metaheuristic techniques
have been provided in terms of genetic algorithms [31,35], ant colony optimiza-
tion [25], and local search based techniques [15,28,34]. Recent surveys [13,26]
provide further details.
Several software frameworks that implement some of the tree decomposi-
tion algorithms mentioned above are publicly available. These libraries include
QuickBB [21]2, htdecomp [19]3 and Jdrasil4. Very recently, also an open database
for computation, storage and retrieval of tree decompositions was initiated [41].
2 Available at http://www.hlt.utdallas.edu/∼vgogate/quickbb.html.
3 Available at http://www.dbai.tuwien.ac.at/proj/hypertree/downloads.html.
4 Available at https://github.com/maxbannach/Jdrasil.

htd – A Free, Open-Source Framework for (Customized)
379
3
A Framework for (Customized) Tree Decompositions
The aforementioned tree decomposition software frameworks focus mainly on
computing tree decompositions of small width; optimizing the resulting decom-
position with respect to a concrete DP algorithm to be applied is therefore
left to the user. Our system, htd, provides an eﬃcient implementation of sev-
eral algorithms that compute tree decompositions of small width and aims for
much richer customizations of the resulting tree decomposition: this includes
normalizations, further optimization criteria (for instance, minimizing the total
number of children of join nodes), or augmenting the information in the bags
(for instance, including the subgraph induced by the vertices of the bag) of the
provided decomposition. In what follows we highlight the main features.
Computing Tree Decompositions: The framework provides eﬃcient implemen-
tations of several heuristic approaches for tree decompositions including MCS,
Minimum Degree heuristic, Min-Fill heuristic and their combination. The default
tree decomposition strategy in htd is Min-Fill.
Manipulation Operations and Normalizations: Manipulations in the context of
htd refer to all operations which alter a given decompositions, e.g., by ensur-
ing that the maximum number of children of decomposition nodes is limited.
Normalizations are manipulations which combine multiple base manipulation
operations, e.g., one can use them to request only nice tree decompositions.
Optimization: In the context of htd, the term “optimization” is used to refer to
operations which improve certain properties of a decomposition. Via optimiza-
tion the obtained decomposition adheres to certain preferences. htd oﬀers the
ability to optimize by searching for the optimal root node and also by choosing
the best tree decomposition based on a custom (potentially multi-level) criterion.
Combining these two strategies might yield even better results.
Computation of Induced Edges: Any DP algorithm needs precise knowledge
about the (hyper-)edges of the problem instance’s graph representation induced
by a bag because this is the information which actually matters when the prob-
lem at hand has to be solved. Computing this information inside the DP step
is not only time-consuming, it also potentially destroys the property of ﬁxed-
parameter tractability in practice because in each bag one might need to scan
the whole edge set of the input. htd computes and delivers this information very
eﬃciently by extending decomposition algorithms appropriately.
Labels and Labeling Operations: Labels generalize the concept that is used for
the induced edges to arbitrary information. They can be used to take care of
supplemental information of the input instance and they can enhance the knowl-
edge base represented by a decomposition. In both cases, one can deﬁne labeling
functions and operations which automate the process of assigning labels.
All these customization features can be applied directly in the context of the
decomposition procedure and no further code changes are necessary. The basic
work-ﬂow of how to obtain a customized decomposition of a problem instance
using htd is the following: At ﬁrst, the input speciﬁcation which represents an

380
M. Abseher et al.
instance of a given problem must be transformed to its corresponding instance
graph. Depending on the actual need, a developer can choose from several built-
in graph types, like directed and undirected ones as well as hypergraphs. In order
to be able to manage additional information about the input instance, htd also
oﬀers labeled and so-called named counterparts for each graph type. A labeled
graph type allows to add arbitrary labels to the vertices and edges of the graph.
Named graph types extend this helpful feature by providing a bi-directional
one-to-one mapping between labels and the vertices or edges, respectively. This
can be useful in cases where one wants to access vertices and edges not only by
their identiﬁers but also by a custom “name” which can be of arbitrary data
type. Importers for graph formats like DIMACS and GraphML are provided by
htd, but one can also construct the graph manually using a custom importer.
After all information of the input instance is parsed, the next step for a
developer is to decide for the decomposition algorithm to use and (optionally)
choose from the wide range of built-in manipulation operations or to implement
its own, custom manipulation. By running the decomposition algorithm with
the instance graph as its input, we obtain a customized decomposition which
then can be used in the dynamic programming algorithm. Alternatively, the
decomposition can be exported or printed using built-in functionality.
4
Experimental Evaluation
In this section we give ﬁrst results regarding the performance characteristics of
our framework. The experiments in this paper are based on two investigations.
At ﬁrst, we have a look at the actual eﬃciency of htd in terms of the minimum
width achieved within a ﬁxed time period. Afterwards we highlight the usefulness
of htd’s ability to optimize tree decompositions based on custom criteria.
All our experiments were performed on a single core of an Intel Xeon E5-
2637@3.5 GHz processor running Debian GNU/Linux 8.3. For repeatability of
the experiments we provide the complete testbed and the results under the
following link: www.dbai.tuwien.ac.at/proj/dﬂat/evaluation htd100b1.zip
4.1
Eﬃcient Computation of Minimum-Width Decompositions
In order to have an indication for htd’s actual competitiveness, we compare htd
1.0.0-beta1 [1] to the participants of Track A (“Treewidth”) of the Parameterized
Algorithms and Computational Experiments Challenge 2016 (“PACE16”).
The following list of algorithms contains htd and all further participants of
the sequential heuristics track of the PACE treewidth challenge in the variant in
which they were submitted. For each of the algorithms we provide the name of
the binary, the ID in the PACE challenge (if applicable), the location of its source
code and the exact identiﬁer of the program version in the GitHub repository.
Most of the algorithms use Min-Fill as basic decomposition strategy.
– htd 0.9.9: “htd gr2td minﬁll exhaustive.sh” (PACE-ID: 5)
Available at https://github.com/mabseher/htd
GitHub-Commit-ID: f4f9b8907da2025c4c0c6f24a47ﬀ4dd0bde1626

htd – A Free, Open-Source Framework for (Customized)
381
– htd 1.0.0-beta1: “htd gr2td exhaustive.sh” (No participant of PACE)
Available at https://github.com/mabseher/htd
GitHub-Commit-ID: f04bfd256e0be0eb536ef04410b541b15206255d
– “tw-heuristic” (PACE-ID: 1)
Available at https://github.com/mrprajesh/pacechallenge.
GitHub-Commit-ID: 6c29c143d72856f649de99846e91de185f78c15f
– “tw-heuristic” (PACE-ID: 6)
Available at https://github.com/maxbannach/Jdrasil
GitHub-Commit-ID: fa7855e4c9f33163606a0677485a9e51d26d7b0a
– “tw-heuristic” (PACE-ID: 9)
Available at https://github.com/elitheeli/2016-pace-challenge
GitHub-Commit-ID: 2f4acb30b5c48608859ﬀ27b5f4e217ee8346ca5
– “tw-heuristic” (PACE-ID: 10) [20]
Available at https://github.com/mfjones/pace2016
GitHub-Commit-ID: 2b7f289e4d182799803a014d0ee1d76a4de70c1f
– “ﬂow cutter pace16” (PACE-ID: 12) [24]
Available at https://github.com/ben-strasser/flow-cutter-pace16
GitHub-Commit-ID: 73df7b545f694922dcb873609ae2759568b36f9f
htd already proved its eﬃciency on the instances of the PACE challenge
by achieving the third place in the competition5. Because these instances are
relatively small – htd is able to decompose any of the instances in less than two
seconds – we want to present here also results for larger instances.
For this purpose we consider in our experiments instances which are used in
competitions for quantiﬁed boolean formulas (QBFs). In fact, we decompose here
the Gaifman graph underlying the CNF matrix of the QBFs thus ignoring the
actual quantiﬁer preﬁx (DP-based solvers for QBFs like dynQBF [14] handle the
quantiﬁer information internally and only require a decomposition of the matrix
as input). We used DataSet 1 from the QBFEVAL’16 competition6. The data set
contains 825 instances, most of them being signiﬁcantly larger than the instances
of the PACE challenge. In the experiments, each test run was limited to a run-
time of at most 100 s and 32 GB of main memory. For the actual evaluation we
use the testbed of the PACE challenge7.
In Fig. 1 we present the outcome of our experiments in a plot of the cumula-
tive frequency of the obtained widths for each of the algorithms (IDs are those
of the PACE challenge). A point (x, y) in the diagram indicates that y decompo-
sitions have a width of x or less. This means, an algorithm is better in terms of
decomposition width when its line chart reaches the top with minimal width. We
can see that already the “old” version of htd used in the PACE challenge out-
performs its competitors in the region between width 250 and 1000. The recent
version of htd improves upon these results and shows that only in regions with
very high width, two of its competitors are able to decompose more instances.
5 See
https://pacechallenge.wordpress.com/2016/09/12/here-are-the-results-of-the-
1st-pace-challenge/.
6 Available at http://www.qbﬂib.org/TS2016/Dataset 1.tar.gz.
7 Available at https://github.com/holgerdell/PACE-treewidth-testbed.

382
M. Abseher et al.
Fig. 1. Decomposition quality for instances of QBFEVAL2016 challenge
Table 1. Results for dynQBF using tree decompositions with low join node complexity
Iterations Solved instances (Total) Solved instances (Mean) Total user-time
1
491
98.2
5904.54
5
512
102.4
6375.08
10
512
102.4
5908.78
4.2
Using Customized Tree Decompositions to Increase Eﬃciency
Next, we highlight the usefulness of htd’s ability to optimize tree decompositions
via an application scenario using the QBF solver dynQBF [14]8. dynQBF makes
extensive use of htd’s features (especially concerning induced edges) and beneﬁts
from customized tree decompositions as provided by htd.
For the following experiments we consider the 200 instances of the 2QBF
track of the QBFEVAL2014 competition9. These instances diﬀer from those
used in Sect. 4.1 due to the fact that after decomposing we still need to run the
dynamic programming algorithm. For each of the test runs we allow a single
thread, 10 min execution time and 16 GB of main memory. This time, we only
consider htd as decomposition library as, to the best of our knowledge, currently
no other framework considers custom preferences for optimization.
Table 1 shows the results of the experiments running dynQBF (using htd
1.0.0-beta1). For each instance, ﬁve diﬀerent tree decompositions were generated
using the Min-Fill heuristics with diﬀerent seeds. The ﬁrst column shows the
number of optimization iterations, i.e., the number of iterations after which the
best known decomposition is returned. The second and third column show the
total and average number of instances over the ﬁve repetitions which were solved
8 Available at https://github.com/gcharwat/dynqbf/releases/tag/v0.3-beta.
9 Available at http://www.qbﬂib.org/TS2010/2QBF.tar.gz.

htd – A Free, Open-Source Framework for (Customized)
383
successfully using the best decomposition found and the last column shows the
total amount of solving time, restricted to the instances successfully solved.
As ﬁtness function for the optimization we aim at minimizing the complexity
of join nodes given by the formula 
j∈J

c∈Cj |χc| where J is the set of join
nodes and Cj is the set of children of node j. That is, we want the total sum of
the products of the join node children’s bag sizes to be as small as possible. The
intuition is that when the given measure is small, the dynamic programming
algorithm is more eﬃcient in join nodes.
We can see that with a single iteration, i.e., without optimization, dynQBF
solves about 98 out of 200 instances in average. The table illustrates that the
number of solved instances in our example scenario increases when we use ﬁve
iterations for the optimization phase. Therefore the total solving time increases.
An interesting observation is the fact that using ten optimization iterations we
need almost the same amount of time as without optimization, but we still
can solve more instances.10 That means that the (on average) four additional
instances come for free in our scenario.
Note that with a statistical signiﬁcance of over 99.95%, the width of the
obtained decompositions does not change with the number of iterations, i.e., the
customized tree decompositions indeed increase the eﬃciency of the dynQBF
algorithm. Hence, by using an optimization function of not more than ten lines
of code, one can already achieve improvements using htd.
5
Conclusion
In this paper we presented a new open-source framework for tree decompositions
called htd. To the best of our knowledge, htd is the ﬁrst software framework
which aims for optimizing tree decompositions by other criteria than just the
plain width. We gave an overview over its features and provided an introduction
on how to use the library (for more details, see [5]). Moreover, we evaluated
our approach by comparing the performance of htd and other participants of
the Parameterized Algorithms and Computational Experiments Challenge 2016.
The outcome of the evaluation indicates that the performance characteristics of
the new framework are indeed encouraging. Furthermore, we showed that cus-
tomizing tree decompositions is a powerful feature which can improve eﬃciency
of dynamic programming algorithms using those decompositions.
For future work we want to further improve the built-in heuristics and algo-
rithms in order to enhance the capabilities for generation of customized decom-
positions of small width. Furthermore we are currently working on making some
exact algorithms for tree decompositions amenable to customization. Last, but
not least, we invite researchers and software developers to contribute to the
10 When we use a pool of ten decompositions to choose from, the chance for obtaining an
even better decomposition increases. However, no additional instance is solved when
we change from ﬁve to ten iterations, but the run-time for the solved instances further
decreases (compensating the time required for computing more decompositions).

384
M. Abseher et al.
library as we try to initiate a joint collaboration on a powerful framework for
graph decompositions and any input is highly appreciated.
Acknowledgments. This work has been supported by the Austrian Science Fund
(FWF): P25607-N23, P24814-N23, Y698-N23.
References
1. Abseher, M.: htd 1.0.0-beta1 (2016). http://github.com/mabseher/htd/tree/v1.0.
0-beta1
2. Abseher, M., Bliem, B., Charwat, G., Dusberger, F., Hecher, M., Woltran, S.: The
D-FLAT system for dynamic programming on tree decompositions. In: Ferm´e, E.,
Leite, J. (eds.) JELIA 2014. LNCS, vol. 8761, pp. 558–572. Springer, Cham (2014).
doi:10.1007/978-3-319-11558-0 39
3. Abseher, M., Bliem, B., Charwat, G., Dusberger, F., Hecher, M., Woltran, S.:
D-FLAT: progress report. Technical report, DBAI-TR-2014-86, TU Wien (2014).
http://www.dbai.tuwien.ac.at/research/report/dbai-tr-2014-86.pdf
4. Abseher, M., Dusberger, F., Musliu, N., Woltran, S.: Improving the eﬃciency of
dynamic programming on tree decompositions via machine learning. In: Proceed-
ings of IJCAI, pp. 275–282. AAAI Press (2015)
5. Abseher, M., Musliu, N., Woltran, S.: htd - A free, open-source framework for
tree decompositions and beyond. Technical report, DBAI-TR-2016-96, TU Wien
(2016). http://www.dbai.tuwien.ac.at/research/report/dbai-tr-2016-96.pdf
6. Abseher, M., Musliu, N., Woltran, S.: Improving the eﬃciency of dynamic pro-
gramming on tree decompositions via machine learning. Technical report, DBAI-
TR-2016-94, TU Wien (2016). http://www.dbai.tuwien.ac.at/research/report/
dbai-tr-2016-94.pdf
7. Arnborg, S., Corneil, D.G., Proskurowski, A.: Complexity of ﬁnding embeddings
in a k-tree. J. Algebraic Discrete Methods 8(2), 277–284 (1987)
8. Arnborg, S., Proskurowski, A.: Linear time algorithms for NP-hard problems
restricted to partial k-trees. Discrete Appl. Math. 23(1), 11–24 (1989)
9. Bachoore, E.H., Bodlaender, H.L.: A branch and bound algorithm for exact,
upper, and lower bounds on treewidth. In: Cheng, S.-W., Poon, C.K. (eds.) AAIM
2006. LNCS, vol. 4041, pp. 255–266. Springer, Heidelberg (2006). doi:10.1007/
11775096 24
10. Berry, A., Heggernes, P., Simonet, G.: The minimum degree heuristic and the
minimal triangulation process. In: Bodlaender, H.L. (ed.) WG 2003. LNCS, vol.
2880, pp. 58–70. Springer, Heidelberg (2003). doi:10.1007/978-3-540-39890-5 6
11. Bertel`e, U., Brioschi, F.: On non-serial dynamic programming. J. Comb. Theor.
Ser. A 14(2), 137–148 (1973)
12. Bodlaender, H.L., Koster, A.M.C.A.: Combinatorial optimization on graphs of
bounded treewidth. Comput. J. 51(3), 255–269 (2008)
13. Bodlaender, H.L., Koster, A.M.C.A.: Treewidth computations I. Upper bounds.
Inf. Comput. 208(3), 259–275 (2010)
14. Charwat, G., Woltran, S.: Dynamic programming-based QBF solving. In: Proceed-
ings of the 4th International Workshop on Quantiﬁed Boolean Formulas, vol. 1719,
pp. 27–40. CEUR Workshop Proceedings (2016)
15. Clautiaux, F., Moukrim, A., N´egre, S., Carlier, J.: Heuristic and meta-heuristic
methods for computing graph treewidth. RAIRO Oper. Res. 38, 13–26 (2004)

htd – A Free, Open-Source Framework for (Customized)
385
16. Dechter, R.: Constraint Processing. Morgan Kaufmann, USA (2003)
17. Dourisboure, Y.: Compact routing schemes for generalised chordal graphs. J. Graph
Algorithms Appl. 9(2), 277–297 (2005)
18. Downey, R.G., Fellows, M.R.: Parameterized Complexity. Monographs in Com-
puter Science. Springer, New York (1999)
19. Ganzow, T., Gottlob, G., Musliu, N., Samer, M.: A CSP hypergraph library. Tech-
nical report, DBAI-TR-2005-50, TU Wien (2005). http://www.dbai.tuwien.ac.at/
proj/hypertree/csphgl.pdf
20. Gaspers, S., Gudmundsson, J., Jones, M., Mestre, J., R¨ummele, S.: Turbocharging
treewidth heuristics. In: Proceedings of IPEC (2016, to appear)
21. Gogate, V., Dechter, R.: A complete anytime algorithm for treewidth. In: Proceed-
ings of UAI, pp. 201–208. AUAI Press (2004)
22. Gottlob, G., Leone, N., Scarcello, F.: Hypertree decompositions and tractable
queries. J. Comput. Syst. Sci. 64(3), 579–627 (2002)
23. Halin, R.: S-functions for graphs. J. Geom. 8, 171–186 (1976)
24. Hamann, M., Strasser, B.: Graph bisection with pareto-optimization. In: Proceed-
ings of ALENEX, pp. 90–102. SIAM (2016)
25. Hammerl, T., Musliu, N.: Ant colony optimization for tree decompositions. In:
Cowling, P., Merz, P. (eds.) EvoCOP 2010. LNCS, vol. 6022, pp. 95–106. Springer,
Heidelberg (2010). doi:10.1007/978-3-642-12139-5 9
26. Hammerl, T., Musliu, N., Schafhauser, W.: Metaheuristic algorithms and tree
decomposition. In: Kacprzyk, J., Pedrycz, W. (eds.) Springer Handbook of Com-
putational Intelligence, pp. 1255–1270. Springer, Heidelberg (2015). doi:10.1007/
978-3-662-43505-2 64
27. J´egou, P., Terrioux, C.: Bag-connected tree-width: a new parameter for graph
decomposition. In: Proceedings of ISAIM, pp. 12–28 (2014)
28. Kjaerulﬀ, U.: Optimal decomposition of probabilistic networks by simulated
annealing. Stat. Comput. 2(1), 2–17 (1992)
29. Kloks, T.: Treewidth, Computations and Approximations. LNCS, vol. 842.
Springer, Heidelberg (1994)
30. Koster, A.M.C.A., van Hoesel, S.P.M., Kolen, A.W.J.: Solving frequency assign-
ment problems via tree-decomposition 1. Electr. Notes Discrete Math. 3, 102–105
(1999)
31. Larranaga, P., Kujipers, C.M., Poza, M., Murga, R.H.: Decomposing bayesian net-
works: triangulation of the moral graph with genetic algorithms. Stat. Comput.
7(1), 19–34 (1997)
32. Lauritzen, S.L., Spiegelhalter, D.J.: Local computations with probabilities on
graphical structures and their application to expert systems. J. R. Stat. Soc. Ser.
B 50, 157–224 (1988)
33. Morak, M., Musliu, N., Pichler, R., R¨ummele, S., Woltran, S.: Evaluating tree-
decomposition based algorithms for answer set programming. In: Hamadi, Y.,
Schoenauer, M. (eds.) LION 2012. LNCS, pp. 130–144. Springer, Heidelberg (2012).
doi:10.1007/978-3-642-34413-8 10
34. Musliu, N.: An iterative heuristic algorithm for tree decomposition. In: Cotta, C.,
van Hemert, J. (eds.) Recent Advances in Evolutionary Computation for Combina-
torial Optimization. Studies in Computational Intelligence, vol. 153, pp. 133–150.
Springer, Heidelberg (2008)
35. Musliu, N., Schafhauser, W.: Genetic algorithms for generalized hypertree decom-
positions. Eur. J. Ind. Eng. 1(3), 317–340 (2007)
36. Niedermeier, R.: Invitation to Fixed-Parameter Algorithms. Oxford Lecture Series
in Mathematics and Its Applications. Oxford University Press, Oxford (2006)

386
M. Abseher et al.
37. Robertson, N., Seymour, P.: Graph minors. III. Planar tree-width. J. Comb. Theor.
Ser. B 36(1), 49–64 (1984)
38. Robertson, N., Seymour, P.: Graph minors. X. Obstructions to tree-decomposition.
J. Comb. Theor. Ser. B 52(2), 153–190 (1991)
39. Shoikhet, K., Geiger, D.: A practical algorithm for ﬁnding optimal triangulations.
In: Proceedings of AAAI/IAAI, pp. 185–190. AAAI Press/The MIT Press (1997)
40. Tarjan, R.E., Yannakakis, M.: Simple linear-time algorithm to test chordality of
graphs, test acyclicity of hypergraphs, and selectively reduce acyclic hypergraphs.
SIAM J. Comput. 13, 566–579 (1984)
41. van Wersch, R., Kelk, S.: Toto: an open database for computation, storage and
retrieval of tree decompositions. Discrete Appl. Math. 217, 389–393 (2017)
42. Xu, J., Jiao, F., Berger, B.: A tree-decomposition approach to protein structure
prediction. In: Proceedings of CSB, pp. 247–256 (2005)

The Nemhauser-Trotter Reduction and Lifted
Message Passing for the Weighted CSP
Hong Xu(B)
, T.K. Satish Kumar, and Sven Koenig
University of Southern California, Los Angeles, CA 90089, USA
{hongx,skoenig}@usc.edu, tkskwork@gmail.com
Abstract. We study two important implications of the constraint com-
posite graph (CCG) associated with the weighted constraint satisfac-
tion problem (WCSP). First, we show that the Nemhauser-Trotter (NT)
reduction popularly used for kernelization of the minimum weighted ver-
tex cover (MWVC) problem can also be applied to the CCG of the
WCSP. This leads to a polynomial-time preprocessing algorithm that
ﬁxes the optimal values of a large subset of the variables in the WCSP.
Second, belief propagation (BP) is a well-known technique used for
solving many combinatorial problems in probabilistic reasoning, artiﬁ-
cial intelligence and information theory. The min-sum message passing
(MSMP) algorithm is a simple variant of BP that has also been suc-
cessfully employed in several research communities. Unfortunately, the
MSMP algorithm has met with little success on the WCSP. We revive
the MSMP algorithm for solving the WCSP by applying it on the CCG
of a given WCSP instance instead of its original form. We refer to this
new MSMP algorithm as the lifted MSMP algorithm for the WCSP. We
demonstrate the eﬀectiveness of our algorithms through experimental
evaluations.
1
Introduction
The weighted constraint satisfaction problem (WCSP) is a combinatorial opti-
mization problem. It is a generalization of the constraint satisfaction problem
(CSP) in which the constraints are no longer “hard”. Instead, each tuple in a
constraint—i.e., an assignment of values to all variables in that constraint—is
associated with a non-negative weight (sometimes referred to as “cost”). The
goal is to ﬁnd an assignment of values to all variables from their respective
domains such that the total weight is minimized [1].
More formally, the WCSP is deﬁned by a triplet B = ⟨X, D, C⟩, where
X = {X1, X2, . . . , XN} is a set of N variables, D = {D1, D2, . . . , DN} is a
set of N domains with discrete values, and C = {C1, C2, . . . , CM} is a set of
M weighted constraints. Each variable Xi ∈X can be assigned a value in its
associated domain Di ∈D. Each constraint Ci ∈C is deﬁned over a certain
subset of the variables Si ⊆X, called the scope of Ci. Ci associates a non-
negative weight with each possible assignment of values to the variables in Si.
(For notational convenience, we use Si and Ci interchangeably throughout this
c
⃝Springer International Publishing AG 2017
D. Salvagnin and M. Lombardi (Eds.): CPAIOR 2017, LNCS 10335, pp. 387–402, 2017.
DOI: 10.1007/978-3-319-59776-8 31

388
H. Xu et al.
paper when referring to the variables participating in a weighted constraint, e.g.,
Xk ∈Ci ≡Xk ∈Si.) The goal is to ﬁnd an assignment of values to all variables in
X from their respective domains that minimizes the sum of the weights speciﬁed
by each weighted constraint in C [1]. This combinatorial task can equivalently
be characterized by having to compute
arg min
a∈A(X)

Ci∈C
ECi(a|Ci),
(1)
where A(X) represents the set of all |D1|×|D2|×. . .×|DN| complete assignments
to all variables in X. a|Ci represents the projection of a complete assignment a
onto the subset of variables in Ci. ECi is a function that maps each a|Ci to its
associated weight in Ci.
The Boolean WCSP is the WCSP in which each domain Di ∈D has its
cardinality restricted to be 2. Despite this restriction, the Boolean WCSP is
representationally as powerful as the WCSP, and it is also NP-hard to solve
in general. The (Boolean) WCSP can be used to model a wide range of use-
ful combinatorial problems arising in a large number of real-world application
domains. For example, in artiﬁcial intelligence, it can be used to model user
preferences [2] and combinatorial auctions. In bioinformatics, it can be used to
locate RNA motifs [21]. In statistical physics, the energy minimization problem
on the Potts model is equivalent to that on its corresponding pairwise Markov
random ﬁeld [20], which in turn can be modeled as the WCSP. In computer
vision, it can be used for image restoration and panoramic image stitching [3,8].
The constraint composite graph (CCG) is a combinatorial structure asso-
ciated with an optimization problem posed as the WCSP. The CCG provides
a unifying framework for simultaneously exploiting the graphical structure of
the variable-interactions in the WCSP as well as the numerical structure of the
weighted constraints in it. The task of solving the WCSP can be reformulated
as the task of ﬁnding a minimum weighted vertex cover (MWVC) on its associ-
ated CCG [9–11]. CCGs can be constructed in polynomial time and are always
tripartite [9–11]. A subclass of the WCSP has instances with bipartite CCGs.
This subclass is tractable since the MWVC problem can be solved in polynomial
time on bipartite graphs using a staged maxﬂow algorithm [4].
Despite its theoretical importance, the CCG still remains largely understud-
ied. In this paper, we study two important implications of the CCG. First, we
show that the Nemhauser-Trotter (NT) reduction popularly used for kerneliza-
tion of the MWVC problem [16] can also be applied to the CCG of the WCSP.
This leads to a polynomial-time preprocessing algorithm that ﬁxes the optimal
values of a subset of the variables in the WCSP; and this subset is often the set
of all variables. As a consequence, many WCSP instances can be solved by the
polynomial-time NT reduction without search. Experimental evaluations of the
NT reduction on the Boolean WCSP benchmark instances show that about 1/8th
of these benchmark instances have a kernel of size 0. In other words, about 1/8th
of these benchmark instances can be solved without search, simply by using the
power of the preprocessing algorithm that encapsulates the NT reduction.

The Nemhauser-Trotter Reduction and Lifted Message Passing
389
Second, belief propagation (BP) is a well-known technique used for solving
many combinatorial problems in probabilistic reasoning, artiﬁcial intelligence
and information theory. The min-sum message passing (MSMP) algorithm is a
simple variant of BP that has also been successfully employed in several research
communities. Unfortunately, the MSMP algorithm has met with little success on
the WCSP. We revive the MSMP algorithm for solving the WCSP by applying
it on the CCG of a given WCSP instance instead of its original form. We refer to
these algorithms as the lifted MSMP algorithm (since the CCG is a lifted repre-
sentation of the WCSP [9–11]) and the original MSMP algorithm, respectively.
Intuitively, the lifted MSMP algorithm outperforms the original MSMP algo-
rithm in terms of eﬀectiveness since the CCG associated with the WCSP makes
the numerical structure of its weighted constraints explicit using a tripartite
graphical representation. We demonstrate the eﬀectiveness of the lifted MSMP
algorithm through experimental evaluations on the Boolean WCSP benchmark
instances. We show that it outperforms the original MSMP algorithm on these
benchmark instances in terms of solution quality.
2
The Constraint Composite Graph
Given an undirected graph G = ⟨V, E⟩, a vertex cover of G is deﬁned as a
set of vertices S ⊆V such that every edge in E has at least one of its endpoint
vertices in S. A minimum vertex cover (MVC) of G is a vertex cover of minimum
cardinality. When G is vertex-weighted—i.e., each vertex vi ∈V has a non-
negative weight wi associated with it—its MWVC is deﬁned as a vertex cover
of minimum total weight of its vertices. The MWVC problem is to compute an
MWVC on a given vertex-weighted undirected graph.
For a given graph G, the concept of the MWVC problem can be extended to
the notion of projecting MWVCs onto a given independent set (IS) U ⊆V . (An IS
is deﬁned as a set of vertices in which no two of them are connected by an edge.)
The input to such a projection is the graph G as well as an IS U = {u1, u2, . . . , uk}.
The output is a table of 2k numbers. Each entry in this table corresponds to a k-
bit vector. We say that a k-bit vector t imposes the following restrictions: (a) if
the ith bit ti is 0, the vertex ui has to be excluded from the MWVC; and (b) if
the ith bit ti is 1, the vertex ui has to be included in the MWVC. The projection
of the MWVC problem onto the IS U is then deﬁned to be a table with entries
corresponding to each of the 2k possible k-bit vectors t(1), t(2), . . . , t(2k). The value
of the entry corresponding to t(j) is equal to the weight of the MWVC conditioned
on the restrictions imposed by t(j). Figure 1 in [11] presents a simple example to
illustrate this projection in a vertex-weighted undirected graph.
The table of numbers produced above can be viewed as a weighted constraint
over |U| Boolean variables. Conversely, given a (Boolean) weighted constraint, we
design a lifted representation for it so as to be able to view it as the projection of
MWVCs onto an IS in some intelligently constructed vertex-weighted undirected
graph [9,10]. The beneﬁt of constructing these representations for individual
constraints lies in the fact that the lifted representation for the entire WCSP,
called the CCG of the WCSP, can be obtained simply by “merging” them.

390
H. Xu et al.
Figure 2 in [11] shows an example WCSP instance over 3 Boolean variables
to illustrate the construction of the CCG. Here, there are 3 unary weighted
constraints and 3 binary weighted constraints. Their lifted representations are
shown next to them. The ﬁgure also illustrates how the CCG is obtained from
the lifted representations of the weighted constraints: In the CCG, vertices that
represent the same variable are simply “merged”—along with their edges—and
every “composite” vertex is given a weight equal to the sum of the individual
weights of the merged vertices. Computing the MWVC for the CCG yields a
solution for the WCSP instance; namely, if Xi is in the MWVC, then it is
assigned the value 1 in the WCSP instance, otherwise it is assigned the value 0 in
the WCSP instance. The CCG of the WCSP can be constructed in polynomial
time using the algorithms suggested in [9,10].
3
Kernelization of the WCSP: NT Reduction on CCGs
The NT reduction is a polynomial-time kernelization procedure that reduces
the size of a given MWVC problem instance [16]. It can be potentially applied
on CCGs as well. It is based on the observation that the MWVC problem is a
half-integral problem. This means that its Integer Linear Programming (ILP)
formulation exhibits the following property. Given a graph G = ⟨V, E⟩, let wi be
the non-negative weight associated with vertex vi. In the ILP formulation of the
MWVC problem instance on G, a Boolean decision variable Zi is ﬁrst associated
with the presence of vertex vi in the MWVC. Then, the ILP formulation is
Minimize
|V |

i=1
wiZi,
∀vi ∈V :
Zi ∈{0, 1},
∀(vi, vj) ∈E :
Zi + Zj ≥1.
(2)
If we relax the integrality constraints Zi ∈{0, 1} for all i ∈{1, 2, . . . , |V |} and
solve the relaxed LP, the optimal solution of the LP is guaranteed to be half-
integral—i.e., ∀i ∈{1, 2, . . . , |V |} : Zi ∈{0, 1
2, 1}. There then exists an MWVC
on G that includes vi if Zi = 1 and excludes vi if Zi = 0. Therefore, one can
kernelize the MWVC problem instance on G to an MWVC problem instance on
a subgraph of G by retaining only those vertices whose Boolean variables in the
optimal solution of the LP are 1
2.
The half-integrality property can be further exploited to solve the LP relax-
ation of the MWVC problem with a maxﬂow algorithm instead of a general LP
solver [4]. We ﬁrst transform G to a vertex-weighted undirected bipartite graph
Gb = ⟨V L
Gb, V R
Gb, EGb⟩as follows. For each vertex vi ∈V , we create two vertices
vL
i ∈V L
Gb and vR
i ∈V R
Gb, both with weight wi. For each edge (vi, vj) ∈E, we
create two edges (vL
i , vR
j ) ∈EGb and (vL
j , vR
i ) ∈EGb. The MWVC problem
can be solved in polynomial time on the bipartite graph Gb using a maxﬂow
algorithm [4]; and the half-integral solution of the above LP relaxation can be

The Nemhauser-Trotter Reduction and Lifted Message Passing
391
retrieved as follows. If both vL
i and vR
i
are in the MWVC of Gb, then Zi = 1
and vi can be safely included in the MWVC of G; if neither vL
i nor vR
i is in the
MWVC of Gb, then Zi = 0 and vi can be safely excluded from the MWVC of G;
if exactly one of vL
i or vR
i is in the MWVC of Gb, then Zi = 1
2 and vi is retained
in the kernel of the MWVC problem instance posed on G.
4
Min-Sum Message Passing on the WCSP and CCGs
BP is a well-known technique for solving many combinatorial problems across
a wide range of ﬁelds such as probabilistic reasoning, artiﬁcial intelligence and
information theory. It can be used to solve hard inference problems that arise
in statistical physics, computer vision, error-correcting coding theory or, more
generally, on graphical models such as Bayesian Networks and Markov random
ﬁelds [20]. BP is an eﬃcient algorithm that is based on local message passing.
Although a complete theoretical analysis of its convergence and correctness is
elusive, it works well in practice on many important combinatorial problems.
While BP performs message passing for the objective of marginalization over
probabilities, the MSMP algorithm is a variant of BP that is used to ﬁnd an
assignment of values to all variables in X that minimizes functions of the form
E(X) =

i
Ei(Xi),
(3)
where X is the set of all variables in the global function E; Ei is a local function
constituting the ith term of E; and Xi is a subset of X containing all variables
that participate in Ei.
To minimize the function E(X), the MSMP algorithm ﬁrst builds a factor
graph, i.e., an undirected bipartite graph with one partition containing vertices
that represent the variables in X and the other partition containing vertices that
represent the local functions Ei for all i. An edge represents the participation
of a variable in a local function. Furthermore, a message is associated with
each direction of each edge. Intuitively, messages represent interactions between
individual variables and local functions. The value of Ei is the potential of its
corresponding vertex because it is indicative of its “potential” to aﬀect other
vertices. Messages are updated iteratively until convergence. In each iteration,
the message from vertex u to vertex v is inﬂuenced by incoming messages to
u as well as u’s potential if it represents a local function. Upon convergence, a
solution can be extracted from the messages.
The MSMP algorithm converges and produces an optimal solution if the fac-
tor graph is a tree [12]. This is, however, not necessarily the case if the factor
graph is loopy [12]. Although the clique tree algorithm alleviates this problem to
a certain extent by ﬁrst converting loopy graphs to trees [7], the technique only
scales to graphs with low treewidths. If the MSMP algorithm operates directly
on loopy graphs, the theoretical underpinnings of its convergence and optimality
properties still remain poorly understood. Nonetheless, it works well in practice

392
H. Xu et al.
Fig. 1. Illustrates the factor graph of a Boolean WCSP instance with 3 variables
{X1, X2, X3} and 3 constraints {C12, C13, C23}. Here, X1, X2 ∈C12, X1, X3 ∈C13
and X2, X3 ∈C23. The circles are variable vertices, and the squares are constraint
vertices. νX1→C12 and ˆνC12→X1 are the messages from X1 to C12 and from C12 to X1,
respectively. Such a pair of messages annotates each edge (not all are explicitly shown).
on a number of important combinatorial problems in artiﬁcial intelligence, sta-
tistical physics and signal processing [12,14]. Examples include the CSP [15],
K-satisﬁability [13] and the MVC problem [18]. Unfortunately, the MSMP algo-
rithm has met with little success on the WCSP. In this section, we show how to
revive the MSMP algorithm for the WCSP by using CCGs.
4.1
The MSMP Algorithm Applied Directly on the WCSP
We now describe how the MSMP algorithm can be applied directly to solve the
Boolean WCSP deﬁned by ⟨X, D, C⟩. We refer to this as the original MSMP
algorithm. As explained before, we ﬁrst construct its factor graph. We create a
vertex for each variable in X (variable vertex) and for each weighted constraint
in C (constraint vertex). A variable vertex Xi and a constraint vertex Cj are
connected by an edge if and only if Cj contains Xi. Figure 1 shows an example.
After the factor graph is constructed, a message (two real numbers) for each
of the two directions along each edge is initialized, for instance, to zeros. A pair
of messages νX1→C12 and ˆνC12→X1 is illustrated in Fig. 1. The messages are then
updated iteratively by using the min-sum update rules given by
ν(t)
Xi→Cj(Xi = xi) =

Ck∈∂Xi\{Cj}

ˆν(t−1)
Ck→Xi(Xi = xi)

+ c(t)
Xi→Cj
(4)
ˆν(t)
Cj→Xi(Xi = xi) =
min
a∈A(∂Cj\{Xi})
⎡
⎣ECj(a|Xj) +

Xk∈∂Cj\{Xi}
ν(t)
Xk→Cj(a|{Xk})
⎤
⎦(5)
+ ˆc(t)
Cj→Xi

The Nemhauser-Trotter Reduction and Lifted Message Passing
393
for all Xi ∈X, Cj ∈C and xi ∈{0, 1} until convergence [12], where
– ˆν(t)
Cj→Xi(Xi = xi) for both xi ∈{0, 1} are the two real numbers of the message
that is passed from the constraint vertex Cj to the variable vertex Xi in the
tth iteration,
– ν(t)
Xi→Cj(Xi = xi) for both xi ∈{0, 1} are the two real numbers of the message
that is passed from the variable vertex Xi to the constraint vertex Cj in the
tth iteration,
– ∂Xi and ∂Cj are the sets of neighboring vertices of Xi and Cj, respectively,
– Xj is the set of all variables in the constraint Cj, and
– c(t)
Xi→Cj and ˆc(t)
Cj→Xi are normalization constants such that
min

ν(t)
Xi→Cj(Xi = 0),
ν(t)
Xi→Cj(Xi = 1)

= 0
(6)
min

ˆν(t)
Cj→Xi(Xi = 0),
ˆν(t)
Cj→Xi(Xi = 1)

= 0.
(7)
The message update rules can be understood as follows. Each message from
a variable vertex Xi to a constraint vertex Cj is updated by summing up all
Xi’s incoming messages from its other neighboring vertices. Each message from
a constraint vertex Cj to a variable vertex Xi is updated by ﬁnding the minimum
of the constraint function ECj plus the sum of all Cj’s incoming messages from
its other neighboring vertices. The messages can be updated in various orders.
We use the superscript ∞to indicate the values of messages upon conver-
gence. The ﬁnal assignment of values to variables in X = {X1, X2, . . . , XN} can
then be found by computing
EXi(Xi = xi) ≡

Ck∈∂Xi
ˆν(∞)
Ck→Xi(Xi = xi)
(8)
for all Xi ∈X and xi ∈{0, 1}. Here, EXi(Xi = 0) and EXi(Xi = 1) can be
proven to be equal to the minimum values of the total weights conditioned on
Xi = 0 and Xi = 1, respectively. By selecting the value of xi that leads to a
smaller value of EXi(Xi = xi), we obtain the ﬁnal assignment of values to all
variables in X.
4.2
The MSMP Algorithm Applied on CCGs
To solve a given WCSP instance, we can ﬁrst transform it to an MWVC problem
instance on its CCG. We can then apply the MSMP algorithm on the CCG. We
refer to this procedure as the lifted MSMP algorithm.
The MWVC problem on ⟨V, E, w⟩—where V is the set of vertices, E is the set
of edges, and w is the set of non-negative weights of the vertices—is a subclass
of the Boolean WCSP. Throughout this subsection, we use the variable Xi to
represent the ith vertex in V : Xi = 1 means the ith vertex is selected in the
MWVC, and Xi = 0 means the ith vertex is not selected in the MWVC. The
MWVC problem can therefore be rewritten as a subclass of the Boolean WCSP
with only the following two types of constraints:

394
H. Xu et al.
– Unary weighted constraints: Each of these weighted constraints corresponds
to a vertex in the MWVC problem. We use CV
i
to denote the weighted con-
straint that corresponds to the ith vertex. CV
i therefore only has one variable
Xi. In the weighted constraint CV
i , the tuple in which Xi = 1 has weight
wi ≥0 and the other tuple has weight zero. This type of weighted constraints
represents the minimization objective of the MWVC problem.
– Binary weighted constraints: Each of these weighted constraints corresponds
to an edge in the MWVC problem. We use CE
j to denote the weighted con-
straint that corresponds to the jth edge. The indices of the endpoint vertices
of this edge are denoted by j(+1) and j(−1). CE
j
therefore has two vari-
ables Xj(+1) and Xj(−1). In the weighted constraint CE
j , the tuple in which
Xj(+1) = Xj(−1) = 0 has weight inﬁnity and the other tuples have weight
zero. This type of weighted constraints represents the requirement that at
least one endpoint vertex must be selected for each edge.
Given that the MWVC problem is a subclass of the Boolean WCSP, Eqs. 4,
5 and 8 can be reused for the MSMP algorithm on it. For the MWVC problem,
these equations can be further simpliﬁed. For notational convenience, we omit
normalization constants in the following derivation.
For each of the unary weighted constraints CV
i , we have
– the added weight for selecting a vertex:
ECV
i (Xi = xi) =

wi
xi = 1
0
xi = 0 ,
(9)
– and exactly one variable in CV
i :
∂CV
i \ {Xi} = ∅.
(10)
By plugging Eqs. 9 and 10 into Eq. 5 for t = ∞, we have
ˆν(∞)
CV
i →Xi(Xi = xi) =

wi
xi = 1
0
xi = 0
(11)
for all CV
i . Note that here we do not need Eq. 4 for CV
i
since it has only one
variable and thus the message passed to it does not aﬀect the ﬁnal solution.
For each of the binary weighted constraints CE
j , we have
– the requirement that at least one endpoint vertex must be selected for each
edge:
ECE
j (Xj(+1) = xj(+1), Xj(−1) = xj(−1)) =

+∞
xj(+1) = xj(−1) = 0
0
otherwise
,
(12)
– and exactly two variables in CE
j :
∂CE
j \{Xj(ℓ)} = {Xj(−ℓ)}
∀ℓ∈{+1, −1}.
(13)

The Nemhauser-Trotter Reduction and Lifted Message Passing
395
By plugging Eqs. 11, 12 and 13 into Eqs. 4 and 5 along with the fact that
there exist only unary and binary weighted constraints, we have
ν(t)
Xj(ℓ)→CE
j (Xj(ℓ) = 1) =

Ck∈∂Xj(ℓ)\{CV
j(ℓ),CE
j }

ˆν(t−1)
Ck→Xj(ℓ)(Xj(ℓ) = 1)

+ wj(ℓ)
(14)
ν(t)
Xj(ℓ)→CE
j (Xj(ℓ) = 0) =

Ck∈∂Xj(ℓ)\{CV
j(ℓ),CE
j }
ˆν(t−1)
Ck→Xj(ℓ)(Xj(ℓ) = 0)
(15)
ˆν(t)
CE
j →Xj(ℓ)(Xj(ℓ) = 1) =
min
a∈{0,1} ν(t)
Xj(−ℓ)→CE
j (Xj(−ℓ) = a)
(16)
ˆν(t)
CE
j →Xj(ℓ)(Xj(ℓ) = 0) = ν(t)
Xj(−ℓ)→CE
j (Xj(−ℓ) = 1)
(17)
for all CE
j
and both ℓ∈{+1, −1}. By plugging Eqs. 14 and 15 into Eqs. 16
and 17, we have
ˆν(t)
CE
j →Xj(ℓ)(Xj(ℓ) = 1)
=
min
a∈{0,1}
⎡
⎢⎣

Ck∈∂Xj(−ℓ)\{CV
j(−ℓ),CE
j }

ˆν(t−1)
Ck→Xj(−ℓ)(Xj(−ℓ) = a)

+ wj(−ℓ) · a
⎤
⎥⎦
(18)
ˆν(t)
CE
j →Xj(ℓ)(Xj(ℓ) = 0)
=

Ck∈∂Xj(−ℓ)\{CV
j(−ℓ),CE
j }

ˆν(t−1)
Ck→Xj(−ℓ)(Xj(−ℓ) = 1)

+ wj(−ℓ)
(19)
for all CE
j and both ℓ∈{+1, −1}, where ˆν(t)
CE
j →Xj(ℓ)(Xj(ℓ) = b) for both b ∈{0, 1}
are the two real numbers of the message that is passed from the jth edge to the
j(ℓ)th vertex. Since each edge has exactly two endpoint vertices, the message
from an edge to one of its endpoint vertices can be viewed as a message from
the other endpoint vertex to it. Formally, for the jth edge, we deﬁne the message
from the j(+1)th vertex to the j(−1)th vertex in the tth iteration as
μ(t)
j(+1)→j(−1) ≡ˆν(t)
CE
j →Xj(−1).
(20)
By plugging in Eq. 20 and substituting j(ℓ) with i and j(−ℓ) with j, Eqs. 18
and 19 can be rewritten (with normalization constants) in the form of messages
between vertices as
μ(t)
j→i(Xi = 1) =
min
a∈{0,1}
⎡
⎣

k∈N(j)\{i}
μ(t−1)
k→j (Xj = a) + wj · a
⎤
⎦+ c(t)
j→i
(21)
μ(t)
j→i(Xi = 0) =

k∈N(j)\{i}
μ(t−1)
k→j (Xj = 1) + wj + c(t)
j→i
(22)

396
H. Xu et al.
for all i and j such that the ith and jth vertices are connected by an edge in E.
Here, N(j) is the set of neighboring vertices of the jth vertex in V and c(t)
j→i repre-
sents the normalization constant such that min

μ(t)
j→i(Xi = 1), μ(t)
j→i(Xi = 0)

=
0. Equations 21 and 22 are the message update rules of the MSMP algorithm
adapted to the MWVC problem.
If the messages converge, by plugging Eqs. 11 and 20 into Eq. 8, the ﬁnal
assignment of values to variables can be found by computing
EXi(Xi = xi) =

j∈N(i)

μ(∞)
j→i(Xi = xi)

+ wixi,
(23)
where the meaning of EXi(Xi = xi) is similar to that in Eq. 8.
5
Experimental Evaluation
In this section, we present experimental evaluations of the NT reduction and
the lifted MSMP algorithm. We used two sets of Boolean WCSP benchmark
instances for our experiments. The ﬁrst set of benchmark instances is from the
UAI 2014 Inference Competition1. Here, maximum a posteriori (MAP) inference
queries with no evidence on the PR and MMAP benchmark instances can be
reformulated as Boolean WCSP instances by ﬁrst taking the negative logarithms
of the probabilities in each factor and then normalizing them. The second set
of benchmark instances is from [6]2. This set includes the Probabilistic Infer-
ence Challenge 2011, the Computer Vision and Pattern Recognition OpenGM2
benchmark, the Weighted Partial MaxSAT Evaluation 2013, the MaxCSP 2008
Competition, the MiniZinc Challenge 2012 & 2013 and the CFLib (a library of
cost function networks). The experiments were performed on those benchmark
instances that have only Boolean variables.3
The optimal solutions of the benchmark instances in [6] were computed using
toulbar2 [6]. Since toulbar2 cannot solve WCSP instances with non-integral
weights, the optimal solutions of the benchmark instances from the UAI 2014
Inference Competition were computed by ﬁnding MWVCs on their CCGs. For
each benchmark instance, the MWVC problem was solved by ﬁrst kernelizing it
using the NT reduction, then reformulating it as an ILP [19] and ﬁnally solving
the ILP using the Gurobi optimizer [5] with a running time limit of 5 min.
For our experiments, we implemented the NT reduction using the Gurobi
optimizer [5] as the LP solver.4 For the MSMP algorithms, we set the initial
values of all messages to zeros. If no message changed by an amount more than
1 http://www.hlt.utdallas.edu/∼vgogate/uai14-competition/index.html.
2 http://genoweb.toulouse.inra.fr/∼degivry/evalgm/.
3 As shown in [10], our techniques can also be generalized to the WCSP with larger
domain sizes of the variables. However, for a proof of concept, this paper focuses on
the Boolean WCSP.
4 We could have also implemented the NT reduction using a more eﬃcient maxﬂow
algorithm [4]; but once again, we focus only on the proof of concept in this paper.

The Nemhauser-Trotter Reduction and Lifted Message Passing
397
10−6 in any iteration, we declared convergence. We used the synchronous mes-
sage updating order, i.e., messages were updated in parallel in each iteration.
This standardized the comparison between the two MSMP algorithms, factoring
out the eﬀects of diﬀerent message updating orders within each iteration. In case
of failure to converge within the time limit (5 min) for any benchmark instance,
we reported the solution produced by the MSMP algorithm on that benchmark
instance at the end of that time limit. The CCG construction algorithm and the
MSMP algorithms were implemented in C++ using the Boost graph library [17]
and were compiled by gcc 4.9.2 with the “–O3” option. Our experiments were
performed on a GNU/Linux workstation with an Intel Xeon processor E3-1240
v3 (8MB Cache, 3.4 GHz) and 16 GB RAM.
Figure 2 shows the eﬀectiveness of the NT reduction on the benchmark
instances. The polynomial-time NT reduction solved about 1/8th of these bench-
mark instances yielding empty kernels. Being able to solve this many benchmark
Fig. 2. Shows the eﬀectiveness of the NT reduction. The x-axis shows the fraction of
variables that are eliminated by the NT reduction. The y-axis shows the number of
benchmark instances on which this happens for a fraction range.

398
H. Xu et al.
Fig. 3. Shows the qualities of the solutions (total weights) produced by the original and
the lifted MSMP algorithms in comparison to the optimal solutions (for benchmark
instances with known optimal solutions). The x-axis shows the suboptimality of the
MSMP solutions. The y-axis shows the number of benchmark instances for a range of
suboptimality. Higher bars on the left are indicative of better solutions. (Color ﬁgure
online)
instances without search is indicative of the potential usefulness of the NT reduc-
tion for solving structured real-world problems.
Figure 3 shows the qualities of the solutions (total weights) produced by
the original MSMP algorithm versus the lifted MSMP algorithm in comparison
to the optimal solutions. A signiﬁcant fraction of the solutions produced by
the lifted MSMP algorithm are very close to the optimal solutions. However,
both MSMP algorithms produced solutions that are highly suboptimal in the
> 30% suboptimality range. Therefore, Fig. 4 presents a direct comparison of
the qualities of the solutions produced by the two MSMP algorithms. From this

The Nemhauser-Trotter Reduction and Lifted Message Passing
399
Fig. 4. Shows the qualities of the solutions produced by the original MSMP algorithm
in direct comparison to those produced by the lifted MSMP algorithm for both sets of
benchmark instances. Each point in these plots represents a benchmark instance. The
x and y coordinates of a benchmark instance represent the solution qualities produced
by the lifted MSMP algorithm and the original MSMP algorithm, respectively. Bench-
mark instances above (red)/below (blue) the diagonal dashed line have better/worse
solution qualities when using the lifted MSMP algorithm instead of the original MSMP
algorithm. Benchmark instances whose MSMP solution qualities diﬀer by only 1% are
considered close (green) to the diagonal dashed line. (Color ﬁgure online)

400
H. Xu et al.
ﬁgure, it is evident that solution qualities of the lifted MSMP algorithm are
signiﬁcantly better than those of the original MSMP algorithm.
Table 1 shows the number of benchmark instances on which each MSMP algo-
rithm converged within the time limit. Table 2 shows the convergence time and
number of iterations for those benchmark instances on which both algorithms
converged. Although the original MSMP algorithm converged more frequently
and faster, the lifted MSMP algorithm produced better solutions in general. In
addition, both MSMP algorithms are anytime and can be easily implemented in
distributed settings. Therefore, the comparison of the qualities of the solutions
produced is more important than that of the frequency and speed of convergence.
Table 1. Shows the number of benchmark instances on which each MSMP algorithm
converged. The column “Neither”/“Both” indicates the number of benchmark instances
on which neither/both of the MSMP algorithms converged within the time limit of
5 min. The column “Original”/“Lifted” indicates the number of benchmark instances
on which only the original/lifted MSMP algorithm converged.
Benchmark instance set
Neither Both Original Lifted
UAI 2014 inference competition
25
4
124
0
[6]
258
7
44
0
Table 2. Shows the number of iterations and running time for each of the benchmark
instances on which both MSMP algorithms converged within the time limit of 5 min.
The column “Benchmark Instance” indicates the name of each benchmark instance.
The “U:” and “T:” at the beginning of the names indicate that they are from the
UAI 2014 Inference Competition and [6], respectively. The columns “Iterations” and
“Running Time” under “The Original MSMP” and “The Lifted MSMP” indicate the
number of iterations and running time (in seconds) after which the original MSMP
algorithm and the lifted MSMP algorithm converged, respectively. With a few excep-
tions, the number of iterations and running time for the original MSMP algorithm are
in general smaller than those of the lifted MSMP algorithm.
Benchmark Instance
The Original MSMP
The Lifted MSMP
Iterations
Running Time
Iterations
Running Time
U:PR/relational 2
5
0.84
9
4.00
U:PR/ra.cnf
1
0.35
6
0.34
U:PR/relational 5
5
1.18
3
0.76
U:PR/Segmentation 12
9
0.04
44
0.14
T:MRF/Segmentation/4 30 s.binary
31
0.10
60
0.13
T:MRF/Segmentation/2 28 s.binary
9
0.05
44
0.11
T:MRF/Segmentation/18 10 s.binary
15
0.07
102
0.18
T:MRF/Segmentation/12 20 s.binary
31
0.13
50
0.14
T:MRF/Segmentation/11 3 s.binary
47
0.15
176
0.24
T:MRF/Segmentation/1 28 s.binary
35
0.11
60
0.14
T:MRF/Segmentation/3 20 s.binary
31
0.12
54
0.14

The Nemhauser-Trotter Reduction and Lifted Message Passing
401
6
Conclusions and Future Work
We studied two important implications of the CCG associated with the WCSP.
First, we showed that the NT reduction popularly used for kernelization of the
MWVC problem can also be applied to the CCG of the WCSP. This leads to a
polynomial-time preprocessing algorithm that ﬁxes the optimal values of a subset
of variables in a WCSP instance. This subset is often the set of all variables:
We observed that the NT reduction could determine the optimal values of all
variables for about 1/8th of the benchmark instances without search.
Second, we revived the MSMP algorithm for solving the WCSP by apply-
ing it on its CCG instead of its original form. We observed not only that the
lifted MSMP algorithm produced solutions that are close to optimal for a large
fraction of benchmark instances, but also that, in general, it produced signiﬁ-
cantly better solutions than the original MSMP algorithm. Although the lifted
MSMP algorithm requires slightly more work in each iteration since the CCG
is constructed using auxiliary variables, the size of the CCG is only linear in
the size of the tabular representation of the WCSP [9–11], and the lifted MSMP
algorithm has the beneﬁt of producing better solutions. Both MSMP algorithms
employ local message passing techniques that avoid an exponential amount of
computational eﬀort and can be readily adapted to distributed settings as well.
There are many avenues for future work. One is to extend the lifted MSMP
algorithm to handle the WCSP with variables of larger domain sizes. (This
extension already exists in theory [10].) Another one is to develop a distributed
version of the lifted MSMP algorithm using grid/cloud computing facilities. And
a third one is to explore the usefulness of constructing the CCG recursively, i.e.,
constructing the CCG of the MWVC problem instance on the CCG associated
with a WCSP instance, and so on.
Acknowledgement. The research at the University of Southern California was sup-
ported by the National Science Foundation (NSF) under grant numbers 1409987 and
1319966. The views and conclusions contained in this document are those of the authors
and should not be interpreted as representing the oﬃcial policies, either expressed or
implied, of the sponsoring organizations, agencies or the U.S. government.
References
1. Bistarelli, S., Montanari, U., Rossi, F., Schiex, T., Verfaillie, G., Fargier, H.:
Semiring-based CSPs and valued CSPs: frameworks, properties, and comparison.
Constraints 4(3), 199–240 (1999)
2. Boutilier, C., Brafman, R.I., Domshlak, C., Hoos, H.H., Poole, D.: CP-nets: a tool
for representing and reasoning with conditional ceteris paribus preference state-
ments. J. Artif. Intell. Res. 21, 135–191 (2004)
3. Boykov, Y., Veksler, O., Zabih, R.: Fast approximate energy minimization via
graph cuts. IEEE Trans. Pattern Anal. Mach. Intell. 23(11), 1222–1239 (2001)
4. Goldberg, A.V., Tarjan, R.E.: A new approach to the maximum-ﬂow problem. J.
ACM 35(4), 921–940 (1988)

402
H. Xu et al.
5. Gurobi Optimization Inc.: Gurobi optimizer reference manual (2016). http://www.
gurobi.com
6. Hurley, B., O’Sullivan, B., Allouche, D., Katsirelos, G., Schiex, T., Zytnicki, M.,
de Givry, S.: Multi-language evaluation of exact solvers in graphical model discrete
optimization. Constraints 21(3), 413–434 (2016)
7. Koller, D., Friedman, N.: Probabilistic Graphical Models: Principles and Tech-
niques. MIT Press, Cambridge (2009)
8. Kolmogorov, V.: Primal-dual algorithm for convex markov random ﬁelds. Technical
report, MSR-TR-2005-117, Microsoft Research (2005)
9. Kumar, T.K.S.: A framework for hybrid tractability results in boolean weighted
constraint satisfaction problems. In: Stuckey, P.J. (ed.) CP 2008. LNCS, vol. 5202,
pp. 282–297. Springer, Heidelberg (2008). doi:10.1007/978-3-540-85958-1 19
10. Kumar, T.K.S.: Lifting techniques for weighted constraint satisfaction problems.
In: the International Symposium on Artiﬁcial Intelligence and Mathematics (2008)
11. Kumar, T.K.S.: Kernelization, generation of bounds, and the scope of incremental
computation for weighted constraint satisfaction problems. In: The International
Symposium on Artiﬁcial Intelligence and Mathematics (2016)
12. M´ezard, M., Montanari, A.: Information, Physics, and Computation. Oxford
University Press, New York (2009)
13. M´ezard, M., Zecchina, R.: Random k-satisﬁability problem: from an analytic solu-
tion to an eﬃcient algorithm. Phys. Rev. E 66(5), 056126 (2002)
14. Moallemi, C.C., Roy, B.V.: Convergence of min-sum message-passing for convex
optimization. IEEE Trans. Inf. Theor. 56(4), 2041–2050 (2010)
15. Montanari, A., Ricci-Tersenghi, F., Semerjian, G.: Solving constraint satisfaction
problems through belief propagation-guided decimation. In: The Annual Allerton
Conference, pp. 352–359 (2007)
16. Nemhauser, G.L., Trotter, L.E.: Vertex packings: structural properties and algo-
rithms. Math. Program. 8(1), 232–248 (1975)
17. Siek, J., Lee, L.Q., Lumsdain, A.: The Boost Graph Library: User Guide and
Reference Manual. Addison-Wesley, Boston (2002)
18. Weigt, M., Zhou, H.: Message passing for vertex covers. Phys. Rev. E 74(4), 046110
(2006)
19. Xu, H., Kumar, T.K.S., Koenig, S.: A new solver for the minimum weighted vertex
cover problem. In: Quimper, C.-G. (ed.) CPAIOR 2016. LNCS, vol. 9676, pp. 392–
405. Springer, Cham (2016). doi:10.1007/978-3-319-33954-2 28
20. Yedidia, J.S., Freeman, W.T., Weiss, Y.: Understanding belief propagation and its
generalizations. Exploring Artif. Intell. New Millennium 8, 236–239 (2003)
21. Zytnicki, M., Gaspin, C., Schiex, T.: DARN! A weighted constraint solver for RNA
motif localization. Constraints 13(1), 91–109 (2008)

A Local Search Approach for Incomplete Soft
Constraint Problems: Experimental Results
on Meeting Scheduling Problems
Mirco Gelain1, Maria Silvia Pini1(B), Francesca Rossi2,
Kristen Brent Venable3, and Toby Walsh4
1 University of Padova, Padua, Italy
mirco@gelain.it, pini@dei.unipd.it
2 IBM T.J. Watson Research Center, Yorktown Heights, NY, USA
frossi@math.unipd.it
3 IHMC, Tulane University, New Orleans, USA
kvenabl@tulane.edu
4 UNSW and Data61 (formerly NICTA), Sydney, Australia
toby.walsh@nicta.com.au
Abstract. We consider soft constraint problems where some of the pref-
erences may be unspeciﬁed. In practice, some preferences may be missing
when there is, for example, a high cost for computing the preference val-
ues, or an incomplete elicitation process. Within such a setting, we study
how to ﬁnd an optimal solution without having to wait for all the pref-
erences. In particular, we deﬁne a local search approach that interleaves
search and preference elicitation, with the goal to ﬁnd a solution which is
“necessarily optimal”, that is, optimal no matter the missing data, whilst
asking the user to reveal as few preferences as possible. Previously, this
problem has been tackled with a systematic branch & bound algorithm.
We now investigate whether a local search approach can ﬁnd good qual-
ity solutions to such problems with fewer resources. While the approach
is general, we evaluate it experimentally on a class of meeting schedul-
ing problems with missing preferences. The experimental results show
that the local search approach returns solutions which are very close
to optimality, whilst eliciting a very small percentage of missing prefer-
ence values. In addition, local search is much faster than the systematic
approach, especially as the number of meetings increases.
1
Introduction
Many real-life problems such as scheduling, planning, and resource allocation
problems can be modeled as constraint satisfaction problems and thus solved
eﬃciently using constraint programming techniques [4,14]. A constraint satisfac-
tion problem (CSP) is represented by a set of variables, each with a domain of
F. Rossi—On leave from University of Padova
c
⃝Springer International Publishing AG 2017
D. Salvagnin and M. Lombardi (Eds.): CPAIOR 2017, LNCS 10335, pp. 403–418, 2017.
DOI: 10.1007/978-3-319-59776-8 32

404
M. Gelain et al.
values, and a set of constraints. A solution is an assignment of values to the vari-
ables which satisﬁes all constraints and which optionally maximizes/minimizes
an objective function. For example, let us consider the meeting scheduling prob-
lem, which is the problem of scheduling meetings attended by a number of agents.
This can be modeled as a CSP [15] where the variables represent the meetings,
the domains represent the time slots, and constraints between two variables, that
represent diﬀerent meetings, model the fact that one or more agents must par-
ticipate in both meetings. Every constraint is satisﬁed by all pairs of time slots
that allow participation in both meetings according to the time needed to go
between the corresponding locations. A solution is a schedule, i.e., an allocation
of a time slot to each meeting, such that all agents can attend all their meetings.
Soft constraints [13] can model optimization problems by allowing for several
levels of satisﬁability in a constraint via the use of preference or cost values.
These represent how much we like a particular instantiation of the variables of
a constraint. For example, in meeting scheduling problems, each agent may give
preferences to the time slots as well as to the combination of time slots for the
meetings they will participate in, and the goal is to ﬁnd a schedule which allows
all agents to attend all their meetings and makes the agents as happy as possible
in terms of their preferences.
Incomplete soft constraint problems [8–10] can model situations in which
preferences are not completely known before the solving process starts. In the
context of meeting scheduling problems, some agent may not specify preferences
over every time slot due to computational costs, or if there is an ongoing elicita-
tion process. In this scenario, we aim at ﬁnding a solution which is necessarily
optimal, that is, optimal no matter what the missing preferences are, while ask-
ing the user to reveal as few preferences as possible. Previously this problem has
been tackled with a branch and bound algorithm which was guaranteed to ﬁnd a
solution with this property [10]. We now show that a simple local search method
can solve such problems optimally, or close to optimally, with fewer resources
and can be eﬀective in terms of both elicitation and scalability.
Our local search algorithm, deﬁned for any kind of incomplete soft constraint
problem, starts from a randomly chosen assignment to all the variables and, at
each step, moves to the most promising assignment in its neighborhood, obtained
by changing the value of one variable. The variable to reassign and its new value
are chosen so to maximize the quality of the new assignment. To make this
choice, we elicit some of the preferences missing in the neighboring assignments.
To evaluate the quality of the obtained solution and the performance of our
local search approach, we compare it with the performance of the systematic
method shown in [10]. We have decided to evaluate the approaches on classes of
incomplete fuzzy meeting scheduling problems, and not on classes of randomly
generated incomplete fuzzy problems, since structured problems are more real-
istic. Since in [10] tests for the systematic approach have been performed on
randomly generated classes of incomplete fuzzy constraints, in this paper we ﬁrst
evaluate how the approach in [10] works on classes of incomplete fuzzy meet-
ing scheduling problems. This is necessary to state which is the best systematic

A Local Search Approach for Incomplete Soft Constraint Problems
405
algorithm when we consider the class of incomplete fuzzy meeting scheduling
problems. The aim of the paper is to answer all the following research ques-
tions: can we eﬃciently tackle incomplete meeting scheduling problems with
local search? How much elicitation is needed? Do we need a sophisticated local
algorithm to compete with the existing systematic approach?
Experimental results show that our local search approach returns very high
quality solutions, which have a preference very close to the optimal one, whilst
eliciting a small percentage of missing preferences (always smaller than 5%).
Moreover, they show that a local search approach is much faster than the sys-
tematic approach when the number of meetings (i.e., the number of variables)
increases (for example, 2 s vs. 325 s when there are 14 meetings). Therefore, it
is not necessary to develop a complicated local search algorithm to obtain good
results. Even if the local search approach is general, i.e., it holds for any kind of
preference, to evaluate its performance experimentally, we consider incomplete
meeting scheduling problems with fuzzy preferences.
The paper is structured as follows. In Sect. 2 we deﬁne incomplete soft con-
straint problems (ISCSPs) and an existing branch and bound approach to solve
them, as well as meeting scheduling problems. In Sect. 3 we present our local
search algorithm for ISCSPs that interleaves search and preference elicitation.
In Sect. 4 we deﬁne incomplete soft meeting scheduling problems (IMSPs), that
are meeting scheduling problems where constraints are replaced by preferences
and some preferences may be missing, and we describe the problem generator
used in the experimental studies. Next, in Sect. 5 we discuss the experimental
results of the branch and bound and local search approaches on classes of fuzzy
IMSPs. In Sect. 6 we compare our approach to other existing approaches to deal
with incompletely speciﬁed constraint optimization problems. Finally, in Sect. 7
we summarize the results contained in this paper.
A preliminary version of some of the results of this paper and only some
experiments on randomly generated fuzzy CSPs with missing preferences are
contained in a short paper appeared in [11].
2
Background
We give some basic notions on incomplete soft constraint problems and meeting
scheduling problems.
2.1
Incomplete Soft Constraint Problems
A soft constraint [13] involves a set of variables and it is deﬁned by a preference
function that associates a preference value from a (totally or partially ordered)
set to each instantiation of its variables. This preference value is taken from a
c-semiring, which is deﬁned by ⟨A, +, ×, 0, 1⟩, where A is the set of preference
values, + is a commutative, associative, and idempotent operator, × is used
to combine preference values and is associative, commutative, and distributes
over +, 0 is the worst element, and 1 is the best element. The c-semiring S

406
M. Gelain et al.
induces a partial or total order ≤S over the preference values, where a ≤S b iﬀ
a + b = b. A soft constraint satisfaction problem (SCSP) is a tuple ⟨V, D, C, S⟩
where V is a set of variables, D is the domain of the variables and C is a
set of soft constraints over V associating values from A. An instance of the
SCSP framework is obtained by choosing a particular c-semiring. For instance,
choosing ⟨[0, 1], max, min, 0, 1⟩means that preferences are in [0, 1] and we want
to maximize the minimum preference. This deﬁnes the so-called fuzzy CSPs
(FCSPs). Weighted CSPs (WCSPs) are instead deﬁned by the c-semiring ⟨R+,
min, +, +∞, 0⟩: preferences are interpreted as costs, they are in R+, and the
goal is to minimize the sum of the costs.
Incomplete soft constraint satisfaction problems (ISCSPs) [10] generalize
SCSPs by allowing incomplete soft constraints, i.e., soft constraints with miss-
ing preferences. More precisely, an incomplete soft constraint involves a set of
variables and associates a value from the preference set A or ?. All tuples (i.e.,
partial assignments to the variables) associated with ? are called incomplete
tuples meaning that their preference is unspeciﬁed. Given an assignment s to all
the variables of an ISCSP P, the preference of s in P, denoted by pref(P, s) (aka
pref(s)), is the combination (which corresponds to the minimum in the fuzzy
case and to the sum in the weighted case) of the known preferences associated to
the sub-tuples of s in the constraints of P. Incomplete fuzzy constraint satisfac-
tion problems (IFCSPs) are ISCSPs where the preferences are fuzzy. Similarly,
incomplete weighted constraint satisfaction problems (IWCSPs) are weighted
CSPs where some preferences may be missing.
Fig. 1. An example of IFCSP.
Figure 1 shows an example of an IFCSP with three variables A, B, and C,
with domains D(A) = {a, b, c}, D(B) = {d, e}, and D(C) = {r, s}. In this
IFCSP there are three unary constraints on A, B, and C speciﬁed resp. by
preference functions idef2, idef3, and idef4, a binary constraint involving A and

A Local Search Approach for Incomplete Soft Constraint Problems
407
C speciﬁed by the preference function idef1, and a binary constraint involving
A and B speciﬁed by the preference function idef. The presence of the question
marks identiﬁes the missing preference values. The preference of the assignment
s = ⟨A = a, B = e, C = r⟩, i.e., pref(P, s) is min(0.3, 0.4, 1, 0.9) = 0.3. Since
this assignment involves some missing preferences, such as the one for the tuple
⟨A = a, B = e⟩, its preference should be interpreted as an upper bound of the
actual preference for this assignment.
In an SCSP a complete assignment of values to all the variables is an optimal
solution if its preference is the best possible. This notion is generalized to ISCSPs
via the notion of necessarily optimal solutions, that is, complete assignments
which have the best preference no matter the value of the unknown preferences.
In the example of Fig. 1, the assignment ⟨A = a, B = e, C = r⟩is not
necessarily optimal. In fact, if all missing preferences are 1, this assignment has
preference 0.3, while the assignment ⟨A = b, B = d, C = r⟩has preference 0.6.
In this example, there are no necessarily optimal solutions. Consider now the
same example where the preferences of both A = b and A = c are set to 0.2. In
this new IFCSP, the assignment ⟨A = a, B = d, C = s⟩has preference 0.3 and is
necessarily optimal. In fact, whatever values are given to the missing preferences,
all other assignments have preference at most 0.2 (if A = b or c) or 0.3 (if A = a).
In [10] several complete algorithms are proposed to ﬁnd a necessarily optimal
solution of an ISCSP. All these algorithms follow a branch and bound schema
where search is interleaved with elicitation. Elicitation is needed since the given
problem may have an empty set of necessarily optimal solutions. By eliciting
some preferences, this set eventually becomes non-empty. Several elicitation
strategies are considered and tested on randomly generated IFCSPs. The goal
was to elicit as little as possible before ﬁnding a necessarily optimal solution.
In particular, the elicitation strategies depended on three parameters: when to
elicit, what to elicit, and who chooses the value to be assigned to the next vari-
able. For example, one may only elicit missing preferences after running branch
and bound to exhaustion, or at the end of every complete branch, or at every
node in the search tree. Also, one may elicit all missing preferences related to the
candidate solution, or one might just ask the user for the worst preference among
some missing ones. Finally, when choosing the value to assign to a variable, the
system can make the choice randomly, or it can ask the user, who knows or can
compute (all or some of) the missing preferences, for help. All of the details of
the systematic approach can be found in [10].
2.2
Meeting Scheduling Problems
The meeting scheduling problem (MSP) is a well known benchmark for constraint
satisfaction problems (CSPs) [15]. It is the problem of scheduling some meetings
by allowing the participants to attend all the meetings they are involved in.
More formally, a meeting scheduling problem can be described by: (i) a set of
agents, (ii) a set of meetings, each with a location and a duration, (iii) a set
of time slots where meetings can take place, (iv) for each meeting, a subset of
agents that are supposed to attend such a meeting, (v) for each pair of locations,

408
M. Gelain et al.
the time to go from one location to the other one (the so-called distance table).
Some typical simplifying assumptions are making all meeting durations equal,
and having the same number of meetings for each agent. Solving a MSP means
allocating each meeting to a time slot in a way that all agents can participate
in their meetings. An agent cannot participate in two meetings if they overlap
in time, or if the time to get between locations prevents him from getting to
the second meeting. The MSP can be modelled as a CSP [15] with variables
representing the meetings and domains representing the time slots. We have a
constraint between two variables when one or more agents participate in the
two meetings. Each constraint is satisﬁed by all pairs of time slots that allow
the participation to both meetings taking into account the time needed to get
between the two locations.
3
Local Search on ISCSPs
A local search approach has been deﬁned in [1,3] for soft constraint problems.
Here we adapt it to deal with incompleteness.
Our local search algorithm for ISCSPs interleaves elicitation with search.
To start, we randomly generate an assignment of all the variables. To assess the
quality of such an assignment, we compute its preference. Since some preferences
involved in the chosen assignment may be missing, we ask the user to reveal them.
At each step, we have a current assignment (v1, . . . , vn) for variables X1, . . . , Xn
and we choose a variable to change its value. To do this, we compute a ‘local’
preference for each pair (Xi, vi), by combining all the speciﬁed preferences of the
tuples containing Xi = vi in all the constraints. If the pair (Xi, vi) is the one
with the worst local preference, we choose another value for Xi.
Consider again the example in Fig. 1 and the assignment ⟨A = a, B = e, C =
r⟩. In this assignment, the local preference of variable A is min(0.3, 0.4, 0.9) =
0.3, while for B is 0.4, and for C is min(0.9, 1) = 0.9. Thus our algorithm would
choose variable A.
To choose the new value for the selected variable, we compute the preferences
of the variable assignments obtained by choosing the other values for this vari-
able. We then choose the value which is associated to the best preference. If two
values are associated to assignments with the same preference, we choose the
one associated to the assignment with the smaller number of incomplete tuples.
In this way, we aim at moving to a new assignment which is better than the
current one and has the fewest number of missing preferences.
In the running example, from assignment ⟨A = a, B = e, C = r⟩, once we
know that variable A will be changed, we compute pref(⟨A = b, B = e, C =
r⟩) = 0.4 and pref(⟨A = c, B = e, C = r⟩) = 0.3. Thus, we would select the
value b for A.
Since the new assignment, say s′, could have incomplete tuples, we ask the
user to reveal enough of this data to compute the actual preference of s′. We call
ALL the elicitation strategy that elicits all the missing preferences associated to
the sub-tuples of s′ in the constraints.

A Local Search Approach for Incomplete Soft Constraint Problems
409
For fuzzy constraints, we also consider another elicitation strategy, called
WORST, that asks the user to reveal only the worst preference among the miss-
ing ones, if it is less than the worst known preference. In the fuzzy case, this
is enough to compute the actual preference of s′, since the preference of an
assignment coincides with the worst preference in its constraints.
For weighted constraints, we consider the following three strategies (besides
ALL):
– WW: we elicit the worst missing cost (that is, the highest) until either all the
costs are elicited or the current global cost of the assignment is higher than
the preference of the best assignment found so far;
– BB: we elicit the best (i.e., the minimum) cost with the same stopping con-
dition as for WW;
– BW: we elicit the best and the worst cost in turn, with the same stopping
condition as for WW.
As in many classical local search algorithms, to avoid stagnation in local
minima, we employ tabu search and random moves. Our algorithm has two
parameters: p, which is the probability of a random move, and t, which is the
tabu tenure. When we have to choose a variable to re-assign, the variable is either
randomly chosen, with probability p or, with probability (1-p), we perform the
procedure described above. Also, if no improving move is possible, i.e., all new
assignments in the neighborhood are worse than or equal to the current one,
then the chosen variable is marked as tabu and not used for t steps.
Notice that, while in classical local search scenarios the underlying problem
is always the same, and we just move from one of its solutions to another one, in
our scenario we also change the problem via the elicitation strategies. Since the
change involves only the preference values, the solution set remains the same,
although the preferences of the solutions may decrease over time.
During search, the algorithm maintains the best solution found so far, which
is returned when the maximum number of allowed steps is reached. In the ideal
case, the returned solution is a necessarily optimal solution of the initial problem
with the preferences added by the elicitation. However, there is no formal guar-
antee that this is so: we can reach a problem with necessarily optimal solutions,
but the algorithm may fail to ﬁnd one of those. Our experimental results on
classes of meeting scheduling problems with missing preferences will show that,
even when the returned solutions are not necessarily optimal, their quality is
very close to that of the necessarily optimal solutions.
Notice that systematic and local search solvers are completely re-usable, i.e.,
the algorithms are applicable to other problems as well.
4
Experimental Setting and Problem Generator
We now consider a generalization of the MSP, called IMSP, where constraints are
replaced by soft constraints with preferences and each agent may decide to give
only some of the preferences over the meetings he would like to attend. We recall

410
M. Gelain et al.
that in these problems the variables are meetings and the variables’ domains
contain all the time slots in which the meeting can be allocated. Preferences
associated to each variable assignment (i.e., to each time slot for each meeting)
model the aggregation of the participants’ preferences and they are computed
as follows. For every meeting and for each time slot of this meeting the system
asks the agents to reveal their preferences over this time slot. However, some
agents may decide to not reveal them. The system then collects all the received
preferences for that time slot and, if the majority of agents involving in this
meeting express a preference over it, then it assigns to this time slot the average
preference value, otherwise it marks that preference as unknown.
Similarly, in every binary soft constraint the preferences of every pairs of
time slots, which are compatible with the distance table, is computed as follows:
the system collects the preferences given by the agents and it sets the preference
value of the pair as unknown, if one of the values has an associated incomplete
preference, otherwise it takes the average preference.
Notice that other measures, beside the average, can be used to compute a
collective preference, such as for example the minimum or the maximum [2], and
there are also other criteria that can be considered to decide when a preference is
unknown, for example when a certain number of agents have not revealed their
preferences.
Solving an IMSP concerns then ﬁnding time slots for the meetings such that
all agents can participate and, among all possible solutions, the solution has the
best preference.
In this context, given an IMSP P, the necessarily optimal solutions are sched-
ules that are optimal no matter how the missing preferences are revealed. Thus,
if there is at least one such solution, this is certainly preferred to any other.
For eliciting a missing preference, a query is submitted to the participating
community. We get a preference value by asking all those participants that didn’t
give their preference upfront. The user’s eﬀort shown in the paper is therefore
the eﬀort of such a community. More precisely, the user’s eﬀort is the number
of preference values the users are required to consider to answer the elicitation
queries.
Our generator of incomplete meeting scheduling problems has the following
parameters:
– m: number of meetings;
– n: number of agents;
– k: number of meetings per agent;
– l: number of time slots;
– min and max: minimal and maximal distance in time slots between two
locations;
– i: percentage of incomplete preferences.
We randomly generate an IMSP with m variables, representing the meetings,
each with domain in {1, . . . , l} to represent the time slots from 1 to l. Such time
slots are assumed to be one time unit long and to be adjacent to each other.

A Local Search Approach for Incomplete Soft Constraint Problems
411
Given two time slots a and b, they can be used for two meetings only if the time
needed to go from a location to the other one is at most b −a −1.
For each of the n agents, we generate randomly k integers between 1 and m,
representing the meetings in which he participates. Also, for each pair of time
slots, we randomly generate an integer between min and max that represents
the time needed to go from one location to the other one. These integers will
form what we call the distance table. Notice that min = 1 means that the time
distance between two meetings’ locations is at least 1 time slot.
Given two meetings, if there is at least one agent who needs to participate in
both, we generate a binary constraint between the corresponding variables. Such
a constraint is satisﬁed by all pairs of time slots that are compatible according
to the distance table.
We then generate the fuzzy preferences over the domain values according to
parameter i. We set the preference of each compatible pair in the binary con-
straints by assigning the average preference that the values in the pair have
in their domain when both preferences are known. Otherwise, we assign an
unknown preference.
As an example, assume m = 5, n = 3, k = 2, l = 5, min = 1, max = 2, and
i = 30. According to these parameters, we generate an IMSP with the following
features:
– 5 meetings: m1, m2, m3, m4, and m5;
– 3 agents: a1, a2, and a3;
– participation in meetings: we randomly generate 2 meetings for each agent,
for example: a1 must participate in meetings m1 and m2, a2 must participate
in meetings m4 and m5, and a3 must participate in meetings m2 and m3;
– 5 time slots: t1, . . . , t5;
– distance table: we randomly generate its values;
– we randomly generate the fuzzy preferences associated to the variable values
in a way that 30% of the preferences are missing. Then, we compute the
preferences of the compatible pairs in the binary constraints (or we state that
the preference is missing) as stated before.
In our experimental evaluation we randomly generate IMSPs by varying the
number of meetings m from 5 to 14, the number of agents n from 4 to 10, the
number of meetings per agent k from 2 to 5, and the percentage of incompleteness
i from 10% to 100%. The distance distribution is uniform random. Moreover, we
will focus on IMSPs where the preferences are fuzzy.
5
Experimental Evaluation
In this section we show how the branch and bound and local search algorithms
perform on classes of fuzzy IMSPs. All our experiments have been performed on
an AMD Athlon 64 × 2 2800+, with 1 Gb RAM, Linux operating system, and
using JVM 6.0.1. Results are averaged over 100 problem instances.

412
M. Gelain et al.
Fig. 2. Systematic search on fuzzy IMSPs.
5.1
The Systematic Approach
We implemented all the algorithms presented in [10], which have been obtained
by instantiating in all the possible ways the parameters who, when, and what.

A Local Search Approach for Incomplete Soft Constraint Problems
413
 0
 0.01
 0.02
 0.03
 0.04
 0.05
 0.06
 0.07
 10
 20
 30
 40
 50
 60
 70
 80
 90
 100
preference error
incompleteness
ALL
WORST
(a)
 0
 0.01
 0.02
 0.03
 0.04
 0.05
 0.06
 0.07
 4
 5
 6
 7
 8
 9
 10
preference error
agents
ALL
WORST
(b)
 0
 0.01
 0.02
 0.03
 0.04
 0.05
 0.06
 0.07
 5
 6
 7
 8
 9
 10
 11
 12
 13
 14
preference error
meetings
ALL
WORST
(c)
 0
 0.01
 0.02
 0.03
 0.04
 0.05
 0.06
 0.07
 0.08
 0.09
 2
 2.5
 3
 3.5
 4
 4.5
 5
preference error
meetings per agent
ALL
WORST
(d)
 0.5
 1
 1.5
 2
 2.5
 3
 3.5
 4
 4.5
 5
 10
 20
 30
 40
 50
 60
 70
 80
 90
 100
percentage of elicited preferences
incompleteness
WORST
FBB
(e)
 0
 0.5
 1
 1.5
 2
 2.5
 3
 3.5
 4
 4
 5
 6
 7
 8
 9
 10
percentage of elicited preferences
agents
WORST
FBB
(f)
 0.5
 1
 1.5
 2
 2.5
 3
 3.5
 5
 8
 11
 14
percentage of elicited preferences
meetings
WORST
FBB
(g)
 0
 1
 2
 3
 4
 5
 6
 7
 8
 9
 2
 3
 4
 5
percentage of elicited preferences
meetings per agent
WORST
FBB
(h)
 0
 5000
 10000
 15000
 20000
 25000
 30000
 35000
 5
 8
 11
 14
execution time (msec.)
meetings
ALL
WORST
FBB
(i)
Fig. 3. Local search on fuzzy IMSPs.

414
M. Gelain et al.
While in [10] these algorithms have been tested on randomly generated IFCSPs,
we now test their performance on the classes of fuzzy IMSPs deﬁned above.
In all the experimental results for the systematic approach, the association
between an algorithm name and a line symbol is shown in Fig. 2(a). The names
of the algorithms coincide with those used in the literature [10]. Every acronym
describes the elicitation strategy that is used, i.e., who chooses the value to be
assigned to the next variable, what to elicit, and when to elicit. For example, one
may only elicit missing preferences after running branch and bound to exhaustion
(TREE), or at the end of every complete branch (BRANCH), or at every node
in the search tree (NODE). Then, one may elicit all missing preferences (ALL)
related to the candidate solution, or one might just ask the user for the worst
preference (WORST) among some missing ones. Finally, when choosing the value
to assign to a variable, the system can assign this value in decreasing order (DP
and DPI) or one might ask the user, who knows or can compute (all or some
of) the missing preferences, for help in a lazy or a smart way (LU and SU).
Thus, each name has the form X.Y.Z, where X ∈{TREE, BRANCH, NODE},
Y ∈{ALL, WORST}, and Z ∈{DPI, DP, LU, SU}.
To evaluate the performance of the algorithms, as in [10], we measured
the percentage of the elicited tuples, i.e., the percentage of missing preferences
revealed by the agents, and the user’s eﬀort, i.e., the percentage of missing pref-
erences that the user has to consider to answer the queries of the system. In
these experimental tests the default values are m = 12, n = 5, k = 3, l = 10,
min = 1, and max = 2.
As expected, as the incompleteness in the problem rises, the percentage of
elicited tuples tends to increase slightly (see Fig. 2(b)). The algorithm that elicits
fewest tuples in this context is LU.WORST.BRANCH (less than 1% of incom-
plete tuples), among the ones that need the user’s help to choose the values
to instantiate, and DPI.WORST.BRANCH (less than 5% of incomplete tuples)
among the others. The common features of these algorithms are that they elicit
the worst preference every time they reach a complete assignment.
Moreover, these two best algorithms tend to elicit fewer tuples as the meetings
per agent increases, as shown in Fig. 2(c). They reach close to 0% at 5 meetings
per agent because, in that setting, the optimal preference is 0 and so they need
to ask only for the missing preferences to ﬁx the lower bound in the ﬁrst branch
and then they do not reach a complete assignment again. They need more or
less the same amount of elicitation in percentage terms as the number of these
meetings per agent increases, while eliciting almost always less than 5% of the
missing tuples. Moreover, they are the best ones in terms of user’s eﬀort which
is always smaller than 20% (Figs. 2(d) and (e)).
Figure 2(g) shows the results of LU.WORST.BRANCH. It elicits a very small
number of preferences also in totally incomplete settings. The elicitation is always
less than 1% and the user has to consider only around 10% of the incomplete
tuples to give the preferences asked by the system. This behaviour is due to the
fact that the search is guided by the user that selects the value to instantiate.

A Local Search Approach for Incomplete Soft Constraint Problems
415
If we are working in settings where the system decides how to instantiate the
domain values, the best algorithm is DPI.WORST.BRANCH which, as shown
in Fig. 2(f), elicits a percentage of preferences which remains always under 5%
so it gives very good results. Also, the eﬀort required to the user is very small:
it is at most 23% even if the incompleteness is 100% and the percentage of the
elicited preferences is smaller than 5%. We will consider this algorithm, which
we will call FBB (which stands for fuzzy Branch and Bound), when comparing
it to our local search algorithms.
5.2
The Local Search Approach
We test the performance of our local search algorithm, instantiated with both
ALL and WORST elicitation strategies, on fuzzy IMSPs by considering the
quality of the returned solution, the percentage of elicited preferences, and the
execution time needed to ﬁnd a solution. Moreover, we compare our best local
search algorithm to FBB on the same test set in terms of percentage of elicited
preferences and execution time. In these experimental tests we use a step limit
of 100000, a random walk probability of 0.2, and a tabu tenure of 1000. Also, we
consider an average on 100 problems and the default values are m = 10, n = 5,
k = 3, l = 10, i = 30%, min = 1, max = 2.
First, we measure the quality of the returned solution in terms of the prefer-
ence error: this is the distance, in percentage over the whole range of preference
values, between the preference of the solution returned by the local search algo-
rithm and the preference of a necessarily optimal solution. When we vary the
amount of incompleteness in the problem, surprisingly, the preference error is
very small, and always less than 2.5% both for ALL and WORST elicitation
strategies (see Fig. 3(a)). When we vary the number of agents, meetings, and
meetings per agent, the preference error rises as we increase each parameter,
though it remains in the worst case below 10%. The WORST strategy is the
best elicitation strategy in all settings (see Figs. 3(b), (c) and (d). Note that for
5 meetings per agent in Fig. 3(d) the problems almost all have solution prefer-
ences equal to 0 and so the error is 0 as well.
Next, we consider the percentage of elicited preferences of our best local
search algorithm (i.e., with the WORST elicitation strategy) and compare it to
FBB (see Figs. 3(e), (f), (g) and (h)). We omit the results of the ALL strategy
as it elicits from 60% to 80% of the incomplete preferences, while the WORST
strategy and FBB elicit always less than 10%. In all settings, the WORST strat-
egy elicits more preferences than FBB. However, the local search approach is
more eﬃcient in terms of execution time when the number of meetings increases
(see Fig. 3(i)). In fact, for more than 11 meetings, FBB takes much more time
than the WORST and ALL local search strategies.
To better evaluate the performance of our best local search algorithm (i.e.,
with the WORST strategy), we consider also its runtime behavior (Fig. 4). The
left part of the ﬁgure shows the percentage of elicited preferences, where one
of the parameters among i, n, m, k, is not ﬁxed, in turn. The same holds in

416
M. Gelain et al.
 0
 0.5
 1
 1.5
 2
 2.5
 3
 3.5
 4
 4.5
 5
 0
 20000
 40000
 60000
 80000
 100000
percentage of elicited preferences
steps
i=10
i=20
i=30
i=40
i=50
i=60
i=70
i=80
i=90
i=100
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0
 20000
 40000
 60000
 80000
 100000
distance to necessary optimal
steps
i=10
i=20
i=30
i=40
i=50
i=60
i=70
i=80
i=90
i=100
 0
 0.5
 1
 1.5
 2
 2.5
 3
 3.5
 4
 0
 20000
 40000
 60000
 80000
 100000
percentage of elicited preferences
steps
n=4
n=5
n=6
n=7
n=8
n=9
n=10
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0
 20000
 40000
 60000
 80000
 100000
distance to necessary optimal
steps
n=4
n=5
n=6
n=7
n=8
n=9
n=10
 0
 0.5
 1
 1.5
 2
 2.5
 3
 3.5
 0
 20000
 40000
 60000
 80000
 100000
percentage of elicited preferences
steps
m=5
m=8
m=11
m=14
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0
 20000
 40000
 60000
 80000
 100000
distance to necessary optimal
steps
m=5
m=8
m=11
m=14
 0
 1
 2
 3
 4
 5
 6
 7
 8
 9
 0
 20000
 40000
 60000
 80000
 100000
percentage of elicited preferences
steps
k=2
k=3
k=4
k=5
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0
 20000
 40000
 60000
 80000
 100000
distance to necessary optimal
steps
k=2
k=3
k=4
k=5
Fig. 4. Runtime behavior of WORST in fuzzy IMSPs.
the right part of the ﬁgure, which shows the distance to the preference of the
necessarily optimal solution.
As for other classes of incomplete soft constraint problems, WORST elicits
all the preferences needed to reach a solution during the ﬁrst steps. Moreover,

A Local Search Approach for Incomplete Soft Constraint Problems
417
the distance to the necessarily optimal preference decreases very rapidly during
the ﬁrst 20000 steps and then decreases slightly as the search continues. This
behavior is the same no matter which parameter is varying. Thus we could stop
the algorithm after only 20000–30000 steps whilst still ensuring good solutions
and a fast execution time.
6
Related Work
Some works have addressed issues similar to those considered in this paper by
allowing for open settings in CSPs: both open CSPs [6,7] and interactive CSPs
[12] work with domains that can be partially speciﬁed, and in dynamic CSPs [5]
variables, domains, and constraints may change over time.
While the main goal of the work on interactive CSPs is to minimize the run
time of the solver, we emphasize the minimization of the number of queries to
the user and/or of the user eﬀort. The work closest to ours is the one on open
CSPs. An open CSP is a possibly unbounded, partially ordered set of constraint
satisfaction problems, each of which contains at least one more domain value than
its predecessor. Thus, in [7] the approach is to solve larger and larger problems
until a solution is found, while minimizing the number of variable values asked
to the user. To compare it to our setting, we assume all the variable values are
known from the beginning, while some of the preferences may be missing, and
our algorithms work on diﬀerent completions of the given problem. Also, open
CSPs exploit a Monotonicity Assumption that each agent provides values for
variables in strictly non-decreasing order of preference. Even when there are no
preferences, each agent gives only values for variables that are feasible. Working
under this assumption means that the agent that provides new values/costs for
a variable must know the bounds on the remaining possible costs, since they
are provided best value ﬁrst. If the bound computation is expensive or time
consuming, then this is not desirable. This is not needed in our setting, where
single preferences are elicited.
Algorithms that interleave elicitation and solving for incomplete problems
have been considered also in [16]. However, we consider incomplete soft constraint
problems, while in [16] they consider incomplete constraint problems and they
assume to have probabilistic information regarding whether an unknown tuple
satisﬁes a constraint. Also, we apply local search.
7
Conclusions
We deﬁned a local search approach for solving incomplete soft constraint prob-
lems and we compared it to an existing branch and bound approach with the
same goal. The comparison was done on classes of incomplete fuzzy meeting
scheduling problems. Experimental results show that our local search approach
returns solutions which are very close to optimality, whilst eliciting a very small
percentage of missing preference values. In addition, our simple local search is

418
M. Gelain et al.
much faster than the systematic approach, especially as the number of meet-
ings increases. Therefore, it is not necessary to use a sophisticated algorithm to
compete with the complete method.
References
1. Aglanda, A., Codognet, P., Zimmer, L.: An adaptive search for the NSCSPs. In:
Proceedings of CSCLP 2004 (2004)
2. Arrow, K.J., Sen, A.K., Suzumura, K.: Handbook of Social Choice and Welfare.
North Holland, Elsevier (2002)
3. Codognet, P., Diaz, D.: Yet another local search method for constraint solving. In:
Steinh¨ofel, K. (ed.) SAGA 2001. LNCS, vol. 2264, pp. 73–90. Springer, Heidelberg
(2001). doi:10.1007/3-540-45322-9 5
4. Dechter, R.: Constraint Processing. Morgan Kaufmann, San Francisco (2003)
5. Dechter, R., Dechter, A.: Belief maintenance in dynamic constraint networks. In:
AAAI 1988, pp. 37–42 (1988)
6. Faltings, B., Macho-Gonzalez, S.: Open constraint satisfaction. In: Hentenryck, P.
(ed.) CP 2002. LNCS, vol. 2470, pp. 356–371. Springer, Heidelberg (2002). doi:10.
1007/3-540-46135-3 24
7. Faltings, B., Macho-Gonzalez, S.: Open constraint programming. AI J. 161(1–2),
181–208 (2005)
8. Gelain, M., Pini, M.S., Rossi, F., Venable, K.B.: Dealing with incomplete prefer-
ences in soft constraint problems. In: Bessi`ere, C. (ed.) CP 2007. LNCS, vol. 4741,
pp. 286–300. Springer, Heidelberg (2007). doi:10.1007/978-3-540-74970-7 22
9. Gelain, M., Pini, M.S., Rossi, F., Venable, K.B., Walsh, T.: Elicitation strategies for
fuzzy constraint problems with missing preferences: algorithms and experimental
studies. In: Stuckey, P.J. (ed.) CP 2008. LNCS, vol. 5202, pp. 402–417. Springer,
Heidelberg (2008). doi:10.1007/978-3-540-85958-1 27
10. Gelain, M., Pini, M.S., Rossi, F., Venable, K.B., Walsh, T.: Elicitation strategies
for soft constraint problems with missing preferences: properties, algorithms and
experimental studies. AI J. 174(3–4), 270–294 (2010)
11. Gelain, M., Pini, M.S., Rossi, F., Venable, K.B., Walsh, T.: A local search approach
to solve incomplete fuzzy CSPs. In: Proceedings of ICAART 2011, Poster Paper
(2011)
12. Lamma, E., Mello, P., Milano, M., Cucchiara, R., Gavanelli, M., Piccardi, M.:
Constraint propagation and value acquisition: why we should do it interactively.
In: IJCAI, pp. 468–477 (1999)
13. Meseguer, P., Rossi, F., Schiex, T.: Soft constraints. In: Rossi, F., Van Beek,
P., Walsh, T. (eds.) Handbook of Constraint Programming. Elsevier, Amsterdam
(2006)
14. Rossi, F., Van Beek, P., Walsh, T.: Handbook of Constraint Programming. Elsevier,
Amsterdam (2006)
15. Shapen, U., Zivan, R., Meisels, A.: Meeting scheduling problem (MSP). http://
www.cs.st-andrews.ac.uk/∼ianm/CSPLib/prob/prob046/index.html (2010)
16. Wilson, N., Grimes, D., Freuder, E.C.: Interleaving solving and elicitation of con-
straint satisfaction problems based on expected cost. Constraints 15(4), 540–573
(2010)

Author Index
Abseher, Michael
376
Akhmedov, Murodzhon
263
Bai, Junwen
104
Bajgiran, Omid Sanei
221
Bardin, Sébastien
3
Beldiceanu, Nicolas
21
Bergman, David
41
Berthold, Timo
211
Bertoni, Francesco
263
Bjorck, Johan
104
Bobot, François
3
Boland, Natashia
254
Bras, Ronan Le
68
Brown, Kenneth N.
147
Cappart, Quentin
312
Carlsson, Mats
21
Chihani, Zakaria
3
Cire, Andre Augusto
41, 221
de Uña, Diego
113
Delaite, Antoine
176
Derrien, Alban
21
Díaz, Mateo
68
Flener, Pierre
51
Fraenkel, Ernest
263
Gange, Graeme
113
Gelain, Mirco
403
Gomes, Carla
68, 104
Gregoire, John
104
Grimes, Diarmuid
147
Hebrard, Emmanuel
167
Heinz, Stefan
211
Hemmi, David
277
Hewitt, Mike
254
Houndji, Vinasetan Ratheil
185
Hounkonnou, Mahouton Norbert
185
Kletzander, Lucas
344
Klos, Tomas
302, 328
Koenig, Sven
387
Kruber, Markus
202
Kwee, Ivo
263
Lee, Jon
229
LeNail, Alexander
263
Leo, Kevin
77
Lübbecke, Marco E.
202
Marre, Bruno
3
Mehta, Deepak
147
Minot, Maël
359
Montemanni, Roberto
263
Mountakis, Kiriakos Simon
302, 328
Musliu, Nysret
344, 376
Nagarajan, Viswanath
244
Ndiaye, Samba Ndojh
359
O’Sullivan, Barry
147
Parmentier, Axel
202
Pearson, Justin
51
Perez, Guillaume
30
Pesant, Gilles
176
Pini, Maria Silvia
403
Poland, Jan
131
Prud’homme, Charles
21
Régin, Jean-Charles
30
Rossi, Francesca
403
Rousseau, Louis-Martin
221
Satish Kumar, T.K.
387
Savelsbergh, Martin
254
Schachte, Peter
113
Schaus, Pierre
185, 312
Schulte, Christian
51
Schutt, Andreas
21
Scott, Joseph D.
51
Shen, Siqian
244

Siala, Mohamed
167
Solnon, Christine
359
Speakman, Emily
229
Stuckey, Peter J.
21, 113
Suram, Santosh K.
104
Tack, Guido
77, 277
Venable, Kristen Brent
403
Verwer, Sicco
94
Vu, Duc Minh
254
Wahbi, Mohamed
147
Wallace, Mark
277
Walsh, Toby
403
Witteveen, Cees
302, 328
Witzig, Jakob
211
Wolsey, Laurence
185
Woltran, Stefan
376
Xu, Hong
387
Xue, Yexiang
104
Yu, Han
229
Yu, Miao
244
Zanarini, Alessandro
131, 293
Zhang, Yingqian
94
420
Author Index

