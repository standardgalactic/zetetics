Representation Learning Beyond Linear Prediction Functions
Ziping Xu
Department of Statistics
University of Michigan, Ann Arbor
zipingxu@umich.edu
Ambuj Tewari
Department of Statistics
University of Michigan, Ann Arbor
tewaria@umich.edu
June 1, 2021
Abstract
Recent papers on the theory of representation learning has shown the importance of a quantity called
diversity when generalizing from a set of source tasks to a target task. Most of these papers assume that
the function mapping shared representations to predictions is linear, for both source and target tasks. In
practice, researchers in deep learning use diﬀerent numbers of extra layers following the pretrained model
based on the diﬃculty of the new task. This motivates us to ask whether diversity can be achieved when
source tasks and the target task use diﬀerent prediction function spaces beyond linear functions. We show
that diversity holds even if the target task uses a neural network with multiple layers, as long as source
tasks use linear functions. If source tasks use nonlinear prediction functions, we provide a negative result
by showing that depth-1 neural networks with ReLu activation function need exponentially many source
tasks to achieve diversity. For a general function class, we ﬁnd that eluder dimension gives a lower bound
on the number of tasks required for diversity. Our theoretical results imply that simpler tasks generalize
better. Though our theoretical results are shown for the global minimizer of empirical risks, their qualitative
predictions still hold true for gradient-based optimization algorithms as veriﬁed by our simulations on deep
neural networks.
1
Introduction
It has become a common practice (Tan et al., 2018) to use a pre-trained network as the representation for a
new task with small sample size in various areas including computer vision (Marmanis et al., 2015), speech
recognition (Dahl et al., 2011; Jaitly et al., 2012; Howard and Ruder, 2018) and machine translation (Weng
et al., 2020). Most representation learning is based on the assumption that the source tasks and the target task
share the same low-dimensional representation.
In this paper, we follow the previous work (Tripuraneni et al., 2020) on representation learning by assuming
each task, labeled by t, generates observations noisily from the mean function f ∗
t ◦h∗, where h∗is the true
representation shared by all the tasks and f ∗
t is called the prediction function. We assume h∗∈H, the
representation function space and ft ∈Ft, the prediction function space. Tripuraneni et al. (2020) proposed a
diversity condition which guarantees that the learned representation will generalize to target tasks with any
prediction function. Assume we have T source tasks labeled by 1, . . . , T and F1 = · · · = FT = Fso. We denote
the target task by ta and let Et(f, h) ∈R be the excess error of f, h ∈Ft × H for task t. The diversity condition
can be stated as follows: there exists some ν > 0, such that for any f ∗
ta ∈Fta and any h ∈H,
inf
fta∈Fta Eta(fta, h)
Excess error given h for target task
≤ν
inf
ft∈Fso,t=1,...,T
1
T
T
X
t=1
Et(ft, h)
Excess error given h for source tasks
.
1
arXiv:2105.14989v1  [stat.ML]  31 May 2021

The diversity condition relates the excess error for source tasks and the target task with respect to any ﬁxed
representation h. A smaller ν indicates a better transfer from source tasks to the target task. Generally, we say
T source tasks achieve diversity over Fta when ν is ﬁnite and relatively small.
The number of source tasks plays an important role here. To see this, assume Fso = Fta = F is a discrete
function space and each function can be arbitrarily diﬀerent. In this case, we will need the target task to be
the same as at least one source task, which in turn requires T ≥|F| and ν ≥|F|. So far, it has only been
understood that when both source tasks and target task use linear prediction functions, it takes at least d
source tasks to be diverse, where d is the number of dimension of the linear mappings. This paper answers the
following two open questions with a focus on the deep neural network (DNN) models:
How does representation learning work when Fso ̸= Fta?
How many source tasks do we need to achieve diversity with nonlinear prediction functions?
There are strong practical motivations to answer the above two questions. In practice, researchers use
representation learning despite the diﬀerence in diﬃculty levels of source and target tasks. We use a more
complex function class for substantially harder target task, which means Fso ̸= Fta. This can be reﬂected as
extra layers when a deep neural network model is used as prediction functions. On the other hand, the source
task and target task may have diﬀerent objectives. Representation pretrained on a classiﬁcation problem, say
ImageNet, may be applied to object detection or instance segmentation problems. For instance, Oquab et al.
(2014) trained a DNN on ImageNet and kept all the layers as the representation except for the last linear
mapping, while two fully connected layers are used for the target task on object detection.
Another motivation for our work is the mismatch between recently developed theories in representation
learning and the common practice in empirical studies. Recent papers on the theory of representation learning
all require multiple sources tasks to achieve diversity so as to generalize to any target task in F (Maurer et al.,
2016; Du et al., 2020; Tripuraneni et al., 2020). However, most pretrained networks are only trained on a
single task, for example, the ImageNet pretrained network. To this end, we will show that a single multi-class
classiﬁcation problem can be diverse.
Lastly, while it is common to simply use linear mapping as the source prediction function, there is no clear
theoretical analysis showing whether or not diversity can be achieved with nonlinear prediction function spaces.
Main contributions.
We summarize the main contributions made by this paper.
1. We show that diversity over Fso implies diversity over Fta, when both Fso and Fta are DNNs and Fta
has more layers. More generally, the same statement holds when Fta is more complicated than Fso, in
the way that Fta = F′
ta ◦(F⊗m
so ) for some positive integer m1 and function class F′
ta.
2. Turning our attention to the analysis of diversity for non-linear prediction function spaces, we show that
for a depth-1 NN, it requires Ω(2d) many source tasks to establish diversity with d being the representation
dimension. For general Fso, we provide a lower bound on the number of source tasks required to achieve
diversity using the eluder dimension (Russo and Van Roy, 2013) and provide a upper bound using the
generalized rank (Li et al., 2021).
3. We show that, from the perspective of achieving diversity, a single source task with multiple outputs can
be equivalent to multiple source tasks. While our theories are built on empirical risk minimization, our
simulations on DNNs for a multi-variate regression problem show that the general statement still hold
when a stochastic gradient descent is used for optimization.
2
Preliminaries
We ﬁrst introduce the mathematical setup of the problem studied in this paper along with the two-phase
learning method that we will focus on.
Problem setup.
Let X denote the input space. We assume the same input distribution PX for all tasks, as
covariate shift is not the focus of this work. In our representation learning setting, there exists a generic feature
representation function h∗∈H : X 7→Z that is shared across diﬀerent tasks, where Z is the feature space and
1F⊗T is the T times Cartesian product of F.
2

H is the representation function space. Since we only consider the diﬀerent prediction functions, each task,
indexed by t, is deﬁned by its prediction function f ∗
t ∈Ft : Z 7→Yt, where Ft is the prediction function space
of task t and Yt is the corresponding output space. The observations Yt = f ∗
t ◦h∗(X) + ϵ are generated noisily
with mean function f ∗
t ◦h∗, where X ∼PX and ϵ is zero-mean noise that is independent of X.
Our representation is learned on source tasks f ∗
so := (f ∗
1 , . . . , f ∗
T ) ∈F1 × · · · × FT for some positive integer
T. We assume that all the prediction function spaces Ft = Fso are the same over the source tasks. We denote
the target task by f ∗
ta ∈Fta, where Fta is the target prediction function space. Unlike the previous papers (Du
et al., 2020; Tripuraneni et al., 2020; Maurer et al., 2016), which all assume that the same prediction function
space is used for all tasks, we generally allow for the possibility that Fso ̸= Fta.
Learning algorithm.
We consider the same two-phase learning method as in Tripuraneni et al. (2020). In
the ﬁrst phase (the training phase), n = (n1, . . . , nT ) samples from each task are available to learn a good
representation. In the second phase (the test phase), we are presented nta samples from the target task to
learn its prediction function using the pretrained representation learned in the training phase.
We denote a dataset of size n from task ft by Sn
t = {(xti, yti)}n
i=1. We use empirical risk minimization
(ERM) for both phase. In the training phase, we minimize average risks over {Snt
t }T
t=1:
ˆR(f, h | f ∗
so) :=
1
P
t nt
T
X
t=1
nt
X
i=1
lso(ft ◦h(xti), yti),
where lso : Y × Y 7→R is the loss function for the source tasks and f = (f1, . . . , fT ) ∈F⊗T
so . The estimates are
given by ( ˆfso, ˆh) ∈arg minf,h ˆR(f, h | f ∗
so). In the second phase, we obtain the dataset {xta,i, yta,i}nta
i
from
the target task and our predictor ˆfta is given by
arg min
f∈Fta
ˆR(f, ˆh | f ∗
ta) :=
1
nta
nta
X
i=1
lta(f ◦ˆh(xta,i), yta,i),
for some loss function lta on the target task. We also use R(·, · | ·) for the expectation of the above empirical risks.
We denote the generalization error of certain estimates f, h by E(f, h | ·) := R(f, h | ·)−minf ′∈F,h′∈H R(f ′, h′ | ·),
where F can be either F⊗T
so
or Fta depending on the tasks. Our goal is to bound the generalization error of the
target task.
For simplicity, our results are presented under square loss functions. However, we show that our results can
generalize to diﬀerent loss functions in Appendix G. We also assume n1 = · · · = nT = nso for some positive
integer nso.
2.1
Model complexity
As this paper considers general function classes, our results will be presented in terms of the complexity
measures of classes of functions. We follow the previous literature (Maurer et al., 2016; Tripuraneni et al., 2020)
which uses Gaussian complexity. Note that we do not use the more common Rademacher complexity as the
proofs require a decomposition theorem that only holds for Gaussian complexity.
For a generic vector-valued function class Q containing functions q : Rd 7→Rr and N data points,
XN = (x1, . . . , xN)T , the empirical Gaussian complexity is deﬁned2 as
ˆGN(Q) = Eg
"
sup
q∈Q
1
√
N
N
X
i=1
gT
i q(xi)
#
,
gi ∼N(0, Ir)
i.i.d.
The corresponding population Gaussian complexity is deﬁned as GN(Q) = EXN
h
ˆGN(Q)
i
, where the expectation
is taken over the distribution of XN.
2Note that the standard deﬁnition has a 1/N factor instead of 1/
√
N. We use the variant so that ˆGN does not scale with N in
most of the cases we consider and only reﬂects the complexity of the class.
3

2.2
Diversity
Source tasks have to be diverse enough, to guarantee that the representation learned from source tasks can be
generalized to any target task in Fta. To measure when transfer can happen, we introduce the following two
deﬁnitions, namely that of transferability and diversity.
Deﬁnition 1. For some ν, µ > 0, we say the source tasks f ∗
1 , . . . , f ∗
T ∈Fso are (ν, µ)-transferable to task f ∗
ta if
sup
h∈H
inff∈Fta E(f, h | f ∗
ta)
inff∈F⊗T
so E(f, h | f ∗so) + µ/ν ≤ν.
Furthermore, we say they are (ν, µ)-diverse over Fta if above ratio is bounded for any true target prediction
functions f ∗
ta ∈Fta, i.e.
sup
f ∗
ta∈Fta
sup
h∈H
inff∈Fta E(f, h | f ∗
ta)
inff∈F⊗T
so E(f, h | f ∗so) + µ/ν ≤ν.
When it is clear from the context, we denote E(f, h | f ∗
ta) and E(f, h | f ∗
so) by Eta(f, h) and Eso(f, h),
respectively. We will call ν the transfer component and µ, the bias introduced by transfer. The deﬁnition of
transferable links the generalization error between source tasks and the target task as shown in Theorem 1.
The proof can be found in Appendix A.
Theorem 1. If source tasks f ∗
1 , . . . , f ∗
T are (ν, µ)-diverse over Fta, then for any f ∗
ta ∈Fta, we have
Eta( ˆfta, ˆh) ≤νEso( ˆfso, ˆh) + µ +
√
2π ˆGnta

Fta ◦ˆh

√nta
+
s
9 ln(2/δ)
2nta
.
The ﬁrst term in Theorem 1 can be upper bounded using the standard excess error bound of Gaussian
complexity. The beneﬁt of representation learning is due to the decrease in the third term from ˆGnta(Fta ◦
H)/√nta without representation learning to ˆGnta(Fta ◦ˆh)/√nta in our case. For the problem with complicated
representations, the former term can be extremely larger than the later one.
In the rest of the paper, we discuss when we can bound (ν, µ), for nonlinear and nonidentical Fso and Fta.
3
Negative transfer when source tasks are more complex
Before introducing the cases that allow transfer, we ﬁrst look at a case where transfer is impossible.
Let the source task use a linear mapping following the shared representation and the target task directly
learns the representation. In other words, f ∗
ta is identical mapping and known to the learner. We further
consider Fso = {z 7→wT z : w ∈Rp} and H = {x 7→Hx : H ∈Rp×d}. The interesting case is when p ≪d.
Let the optimal representation be H∗and the true prediction function for each source task be w∗
1, . . . , w∗
T . In
the best scenario, we assume that there is no noise in the source tasks and each source task collects as many
samples as possible, such that we will have an accurate estimation on each w∗T
t H∗∈Rp×d which we denote by
W ∗
t .
However, the hypothesis class given the information from source tasks are
{H ∈Rp×d : ∃w ∈Rp, wT H = Wt for all t = 1, . . . , T}.
As H∗is in the above class, any QH∗for some rotation matrix Q ∈Rp×p is also in the class. In other
words, a non-reducible error of learnt representation is maxQ ∥H∗−QH∗∥2
2 in the worst case.
More generally, we give a condition for negative transfer.
Consider a single source task f ∗
so.
Let
H∗
so := arg minh∈H minf∈Fso Eso(f, h) and H∗
ta := arg minh∈H minf∈Fta Eta(f, h).
In fact, if one hopes to
get a representation learning beneﬁt with zero bias (µ = 0), we need all h ∈H∗
so to satisfy inff∈Fta Eta(f, h) = 0,
i.e. any representation that is optimal in the source task is optimal in the target task as well. Equivalently, we
will need H∗
so ⊂H∗
ta. As shown in Proposition 1, the deﬁnition of transferable captures the case well.
Proposition 1. If there exists a h′ ∈H∗
so such that h′ /∈H∗
ta, then minfta Eta(fta, h′) > 0. Furthermore, there
is no ν < ∞, such that f ∗
so is (ν, 0)-transferable to f ∗
ta.
Proof. The ﬁrst statement is by deﬁnition. Plugging g′ into the Deﬁnition 1, we will have ν = ∞
The negative transfer happens when the optimal representation for source tasks may not be optimal for
the target task. As the case in our linear example, this is a result of more complex Fso, which allows more
ﬂexibility to reduce errors. This inspires us to consider the opposite case where Fta is more complex than Fso.
4

4
Source task as a representation
Before discussing more general settings, we ﬁrst consider a single source task, which we refer to so. Assume
that the source task itself is a representation of the target task. Equivalently, the source task has a known
prediction function f ∗
so(x) = x. This is a commonly-used framework when we decompose a complex task into
several simple tasks and use the output of simple tasks as the input of a higher-level task.
A widely-used transfer learning method, called oﬀset learning, which assumes f ∗
ta(x) = f ∗
so(x) + w∗
ta(x) for
some oﬀset function w∗
ta, also applies here. The oﬀset method enjoys its beneﬁts when wta has low complexity
and can be learnt with few samples. It is worth mentioning that our setting covers a more general setting in
Du et al. (2017), which assumes f ∗
ta(x) = G(f ∗
so(x), w∗
ta(x)) for some known transformation function G and
unknown w∗
ta.
We show that a simple Lipschitz condition on Fta gives us a bounded transfer-component.
Assumption 1 (Lipschitz assumption). Any fta ∈Fta is L-Lipschitz with respect to L2 distance.
Theorem 2. If Assumption 1 holds, task f ∗
so is (L, 0)-transferable to task f ∗
ta and we have with a high probability,
Eta( ˆfta, ˆh) = ˜O
 
LˆGnso(H)
√nso
+
ˆGnta(Fta ◦ˆh)
√nta
!
.
Theorem 2 bounds the generalization error of two terms. The ﬁrst term that scales with 1/√nso only depends
on the complexity of H. Though the second term scales with 1/√nta, it is easy to see that ˆGnta(Fta ◦ˆh) =
ˆGnta(Fta) ≪ˆGnta(Fta ◦H) with the dataset {ˆh(xta,i)}nta
i=1.
4.1
General case
Previously, we assume that a single source task is a representation of the target task. Now we consider a
more general case: there exist functions in the source prediction space that can be used as representations of
the target task. Formally, we consider Fta = F′
ta ◦(F⊗m
so ) for some m > 0 and some target-speciﬁc function
space F′
ta : Y⊗m
so
7→Yta. Note that Fta is strictly larger than Fso when m = 1 and the identical mapping
x 7→x ∈F′
ta. In practice, ResNet (Tai et al., 2017) satisﬁes the above property.
Assumption 2. Assume any f ′
ta ∈F′
ta is L′-Lipschitz with respect to L2 distance.
Theorem 3. If Assumption 2 holds and the source tasks are (ν, µ)-diverse over its own space Fso, then we
have
Eta

ˆfta, ˆh

= ˜O

L′m
 
ν ˆGT nso(F⊗T
so
◦H)
√Tnso
+ µ
!
+
ˆGnta

Fta ◦ˆh

√nta

.
It is shown in Tripuraneni et al. (2020) that ˆGP
s ns(F⊗S
so
◦H) can be bounded by
˜O(ˆGT nso(H) +
√
T ˆGnso(Fso)). Thus, the ﬁrst term scales with ˆGT nso(H)/√Tnso + ˆGnso(Fso)/√nso. Again the common
part shared by all the tasks decreases with √Tnso, while the task-speciﬁc part scales with √nso or √nta.
4.2
Applications to deep neural networks
Theorem 3 has a broad range of applications. In the rest of the section, we discuss its application to deep
neural networks, where the source tasks are transferred to a target task with a deeper network.
We ﬁrst introduce the setting for deep neural network prediction function. We consider the regression
problem with Yso = Yta = R and the representation space Z ⊂Rp. A depth-K vector-valued neural network is
denoted by
f(x) = σ (WK (σ (. . . σ (W1x)))) ,
where each Wk is a parametric matrix of layer k and σ is the activation function. For simplicity, we let
Wk ∈Rp×p for k = 1, . . . K. The class of all depth-K neural network is denoted by MK. We denote the linear
class by L = {x 7→αT x + β : ∀α ∈Rp, ∥α∥2 ≤M(α)} for some M(α) > 0. We also assume
max{∥Wk∥∞, ∥Wk∥2∥} ≤M(k).
where ∥· ∥∞and ∥· ∥2 are the inﬁnity norm and spectral norm. We assume any z ∈Z, ∥z∥∞≤DZ.
5

Deeper network for the target task.
We now consider the source task with prediction function of a
depth-Kso neural network followed by a linear mapping and target task with depth-Kta neural network. We let
Kta > Kso. Then we have
Fta = L ◦MKta−Kso ◦MKso and Fso = L ◦MKso.
Using the fact that M1 = σ(L⊗p), we can write Fta as L ◦MKta−Kso−1 ◦σ ◦(F⊗p
so ). Thus, we can apply
Theorem 3 and the standard Gaussian complexity bound for DNN models, which gives us Corollary 1.
Corollary 1. Let Fso be depth-Kso neural network and Fta be depth-Kta neural network. If source tasks are
(ν, 0)-diverse over Fso, we have
Eta

ˆfta, ˆh

=
˜O
 
pνM(α)ΠKta
k=1M(k)
 ˆGT nso(H)
√Tnso
+ DZ
√Kso
√nso
!
+ DZ
√Kta · M(α)ΠKta
k=1M(k)
√nta
!
.
(1)
Note that the terms that scales with 1/√nso and 1/√nta have similar coeﬃcients M(α)ΠKta
k=1M(k), which
do not depends on the complexity of H. The term that depends on ˆGT nso(H) scales with 1/√Tnso as we
expected.
5
Diversity of non-linear function classes
While Corollary 1 considers the nonlinear DNN prediction function space under a diversity condition, it is not
clearly understood how diversity can be achieved for nonlinear spaces. In this section, we ﬁrst discuss a speciﬁc
non-linear prediction function space and show a fundamental barrier to achieving diversity. Then we extend
our result to general function classes by connecting eluder dimension and diversity. We end the section with
positive results for achieving diversity under a generalized rank condition.
We consider a subset of depth-1 neural networks with ReLu activation function: F = {x 7→[⟨x, w⟩−
(1 −ϵ/2)]+ : ∥x∥2 ≤1, ∥w∥≤1} for some ϵ > 0. Our lower bound construction is inspired by the similar
construction in Theorem 5.1 of Dong et al. (2021).
Theorem 4. Let T = {f1, . . . , fT } be any set of depth-1 neural networks with ReLu activation in F. For
any ϵ > 0, if T ≤2d log(1/ϵ)−1, there exists some representation h∗, h′ ∈H, some distribution PX and a target
function f ∗
ta ∈F, such that
inf
f∈F Eso(f, h′) = 0,
while inf
f∈F Eta(f, h′) = ϵ2/32.
Theorem 4 implies that we need at least Ω(2d log(1/ϵ)) source tasks to achieve diversity. Otherwise, we
can always ﬁnd a set of source tasks and a target task such that the generalization error in source tasks are
minimized to 0 while that in target task is ϵ2/32. Though ReLu gives us a more intuitive result, we do show
that similar lower bounds can be shown for other popular activation functions, for example, sigmoid function
(see Appendix E).
5.1
Lower bound using eluder dimension
We extend our result by considering a general function space F and build an interesting connection between
diversity and eluder dimension (Russo and Van Roy, 2013). We believe we are the ﬁrst to notice a connection
between eluder dimension and transfer learning.
Eluder dimension has been used to measure the sample complexity in the Reinforcement Learning problem.
It considers the minimum number of inputs, such that any two functions evaluated similarly at these inputs
will also be similar at any other input. However, diversity considers the minimum number of functions such
that any two representations with similar outputs for these functions will also have similar output for any other
functions. Thus, there is a kind of duality between eluder dimension and diversity.
We ﬁrst formally deﬁne eluder dimension. Let F be a function space with support X and let ϵ > 0.
6

Deﬁnition 2 ((F, ϵ)-dependence and eluder dimension (Osband and Van Roy (2014))). We say that x ∈X is
(F, ϵ)-dependent on {x1, . . . , xn} ⊂X iﬀ
∀f, ˜f ∈F,
n
X
i=1
f (xi) −˜f (xi)

2
2 ≤ϵ2 =⇒∥f(x) −˜f(x)∥2 ≤ϵ.
We say x ∈X is (F, ϵ)-independent of {x1, . . . , xn} iﬀit does not satisfy the deﬁnition of dependence.
The eluder dimension dimE(F, ϵ) is the length of the longest possible sequence of elements in X such that
for some ϵ′ ≥ϵ every element is (F, ϵ′)-independent of its predecessors.
We slightly change the deﬁnition and let dims
E(F, ϵ) be the shortest sequence such that any x ∈X is
(F, ϵ)-dependent on the sequence.
Although dims
E(F, ϵ) ≤dimE(F, ϵ), it can be shown that in many regular cases, say linear case and
generalized linear case, dimE(F, ϵ) is only larger then dims
E(F, ϵ) up to a constant.
Theorem 5. For any function class F : X 7→R, and some ϵ > 0, let F∗be the dual class of F. Let
dE = dims
E(F∗, ϵ). Then for any sequence of tasks f1, . . . , ft, t ≤dE −1, there exists a task ft+1 ∈F such
that for some data distribution PX and two representations h, h∗,
inff ′
t+1∈F EX∥f ′
t+1(h(X)) −ft+1(h∗(X))∥2
2
1
t inff ′
1,...,f ′
t
Pt
i=1 EX∥f ′
i(h(X)) −fi(h∗(X))∥2
2
≥t/2.
Theorem 5 formally describes the connections between eluder dimension and diversity. To interpret the
theorem, we ﬁrst discuss what are good source tasks. Any T source tasks that are diverse could transfer well
to a target task if the parameter ν could be bounded by some ﬁxed value that is not increasing with T. For
instance, in the linear case, ν = O(d) no matter how large T is. While for a ﬁnite function class, in the worst
case, ν will increase with T before T reaches |F|. Theorem 5 states that if the eluder dimension of the dual
space is at least dE, then ν scales with T until T reaches dE, as if the function space F is discrete with dE
elements.
Note that this result is consistent with what is shown in Theorem 9 as eluder dimension of the class discussed
above is lower bounded by Ω(2d) as well Li et al. (2021).
5.2
Upper bound using approximate generalized rank
Though we showed that diversity is hard to achieve in some nonlinear function class, we point out that diversity
can be easy to achieve if we restrict the target prediction function such that they can be realized by linear
combinations of some known basis. Tripuraneni et al. (2020) has shown that any source tasks f1, . . . , fT ∈F
are (1/T, µ)-diverse over the space {f ∈F : ∃˜f ∈conv(f1, . . . , fT ) such that supz ∥f(z) −˜f(z)∥≤µ}, where
conv(f1, . . . , fT ) is the convex hull of {f1, . . . , fT }. This can be characterized by a complexity measure, called
generalized rank. Generalized rank is the smallest dimension required to embed the input space such that all
hypotheses in a function class can be realizable as halfspaces. Generalized rank has close connections to eluder
dimension. As shown in Li et al. (2021), eluder dimension can be upper bounded by generalized rank.
Deﬁnition 3 (Approximate generalized rank with identity activation). Let Bd(R) := {x ∈Rd | ∥x∥2 ≤R}.
The µ-approximate id-rk(F, R) of a function class F : X 7→R at scale R is the smallest dimension d for which
there exists mappings φ : X 7→Bd(1) and w : F 7→Bd(R) such that
for all (x, f) ∈X × F : |f(x) −⟨w(f), φ(x)⟩| ≤µ.
Proposition 2. For any F with µ-approximate id-rank(F, R) ≤di for some R > 0, there exists no more than
di functions, f1, . . . , fdi, such that w(f1), . . . w(fdi) span Rddi. Then f1, . . . , fdi are (di, µ)-diverse over F.
Proposition 2 is a direct application of Lemma 7 in Tripuraneni et al. (2020). Upper bounding id-rank is
hard for general function class. However, this notation can be useful for those function spaces with a known set
of basis functions. For example, any function space F that is square-integrable has a Fourier basis. Though the
basis is an inﬁnite set, we can choose a truncation level d such that the truncation errors of all functions are
less than µ. Then the hypothesis space F has µ-approximate id-rank less than d.
7

6
Experiments
In this section, we use simulated environments to evaluate the actual performance of representation learning on
DNNs trained with gradient-based optimization methods. We also test the impact of various hyperparameters.
Our results indicate that even though our theory is shown to hold for ERM estimators, their qualitative
theoretical predictions still hold when Adam is applied and the global minima might not be found. Before
we introduce our experimental setups, we discuss a diﬀerence between our theories and experiments: our
experiments use a single regression task with multiple outputs as the source task, while our analyses are built
on multiple source tasks.
6.1
Diversity of problems with multiple outputs
Though we have been discussing achieving diversity from multiple source tasks, we may only have access to a
single source task in many real applications, for example, ImageNet, which is also the case in our simulations.
In fact, we can show that diversity can be achieved by a single multiclass classiﬁcation or multivariate regression
source task. The diversity for the single multi-variate regression problem with L2 loss is trivial as the loss
function is decomposable. For multi-class classiﬁcation problems or regression problems with other loss functions,
we will need some assumptions on boundedness and continuous. We refer the readers to Appendix H for details.
6.2
Experiments setup
Our four experiments are designed for the following goals. The ﬁrst two experiments (Figure 1, a and b) target
on the actual dependence on nso, nta, Kso and Kta in Equation (1) that upper bounds the errors of DNN
prediction functions. Though the importance of diversity has been emphasized by various theories, no one has
empirically shown its beneﬁts over random tasks selection, which we explore in our third experiment (Figure
1, c). Our fourth experiment (Figure 1, d) veriﬁes the theoretical negative results of nonlinearity of source
prediction functions showed in Theorem 4 and 5.
Though the hyper-parameters vary in diﬀerent experiments, our main setting can be summarized below.
We consider DNN models of diﬀerent layers for both source and target tasks. The ﬁrst K layers are the shared
representation. The source task is a multi-variate regression problem with output dimension p and Kso layers
following the representation. The target task is a single-output regression problem with Kta layers following the
representation. We used the same number of units for all the layers, which we denote by nu. A representation
is ﬁrst trained on the source task using nso random samples and is ﬁxed for the target task, trained on nta
random samples. In contrast, the baseline method trains the target task directly on the same nta samples
without the pretrained network. We use Adam with default parameters for all the training. We use MSE
(Mean Square Error) to evaluate the performance under diﬀerent settings.
6.3
Results
Figure 1 summaries our four experiments, with all the Y axes representing the average MSE’s of 100 independent
runs with an error bar showing the standard deviation. The X axis varies depending on the goal of each
experiment. In subﬁgure (a), we test the eﬀects of the numbers of observations for both source and target tasks,
while setting other hyperparameters by default values. The X axis represents nso and the colors represents
nta. In subﬁgure (b), we test the eﬀects of the number of shared representation layers K. To have comparable
MSE’s, we keep the sum K + Kta = 6 and run K = 1, . . . , 5 reﬂected in the X axis, while keeping Kso = 1. In
subﬁgure (c), we test the eﬀects of diversity. The larger p we have, the more diverse the source task is. We
keep the actual number of observations nso · p = 4000 for a fair comparison. Lastly, in subﬁgure (d), we test
whether diversity is hard to achieve when the source prediction function is nonlinear. The X axis is the number
of layers in source prediction function Kso. The nonlinearity increases with Kso. We run Kso = 1, 2, 3 and add
an activation function right before the output such that the function is nonlinear even if Kso = 1.
In Figure 1 (a), MSE’s decrease with larger numbers of observations in both source and target tasks, while
there is no signiﬁcant diﬀerence between nso = 1000 and 10000. The baseline method without representation
learning performs worst and it performs almost the same when nta reaches 1000. In (b), there are positive
beneﬁts for all diﬀerent numbers of the shared layers and the MSE is the lowest at 5 shared layers. As shown
in Figure 1 (c), the MSE’s and their variances are decreasing when the numbers of outputs increase, i.e., higher
diversity. Figure 1 (d) shows that there is no signiﬁcant diﬀerence between baseline and Kso = 1, 2, 3. When
Kso = 2, 3 there is a negative eﬀects.
8

0.00
0.05
0.10
100
1000
10000
Baseline
Source
Mean Square Error
Target
10
100
1000
(a) Eﬀects of sample sizes
0.010
0.015
0.020
1
2
3
4
5
baseline
Number of Layers in the Representation
Mean Square Error
(b) Eﬀects of K
0.1
0.2
0.3
1
2
3
4
Baseline
Output Dimensions
Mean Square Error
(c) Eﬀects of diversity
0.005
0.010
0.015
0.020
0.025
1
2
3
Baseline
Number of Layers in the Source Prediction Functions
Mean Square Error
(d) Eﬀects of Kso
Figure 1: (a) Eﬀects of the numbers of observations for both source (nso) and target tasks (nta). (b) Eﬀects of
the number of shared representation layers K. (c) Eﬀects of diversity determined by the output dimensions p.
We keep the actual number of observations nso · p = 4000. (d) Eﬀects of nonlinearity of the source prediction
function. Higher Kso indicates higher nonlinearity.
7
Discussions
In this paper, we studied representation learning beyond linear prediction functions. We showed that the
learned representation can generalize to tasks with multi-layer neural networks as prediction functions as long
as the source tasks use linear prediction functions. We show the hardness of being diverse when the source
tasks are using nonlinear prediction functions by giving a lower bound on the number of source tasks in terms
of eluder dimension. We further give an upper bound depending on the generalized rank.
Focusing on future work, we need better tools to understand the recently proposed complexity measure
generalized rank. Our analyses rely on the ERM, while in practice as well as in our simulations, gradient-based
optimization algorithms are used. Further analyses on the beneﬁts of representation learning that align with
practice on the choice of optimization method should be studied. Our analyses assume arbitrary source tasks
selection, while, in real applications, we may have limited data from groups that are under represented, which
may lead to potential unfairness. Algorithm that calibrates this potential unfairness should be studied.
9

References
Bartlett, P. L. and Mendelson, S. (2002). Rademacher and gaussian complexities: Risk bounds and structural
results. Journal of Machine Learning Research, 3(Nov):463–482.
Dahl, G. E., Yu, D., Deng, L., and Acero, A. (2011). Context-dependent pre-trained deep neural networks
for large-vocabulary speech recognition. IEEE Transactions on audio, speech, and language processing,
20(1):30–42.
Dong, K., Yang, J., and Ma, T. (2021). Provable model-based nonlinear bandit and reinforcement learning:
Shelve optimism, embrace virtual curvature. arXiv preprint arXiv:2102.04168.
Dragomir, S. S. and Gluscevic, V. (2000). Some inequalities for the kullback-leibler and x2- distances in
information theory and applications. RGMIA research report collection, 3(2):199–210.
Du, S. S., Hu, W., Kakade, S. M., Lee, J. D., and Lei, Q. (2020).
Few-shot learning via learning the
representation, provably. arXiv preprint arXiv:2002.09434.
Du, S. S., Koushik, J., Singh, A., and Póczos, B. (2017). Hypothesis transfer learning via transformation
functions. In Advances in neural information processing systems, pages 574–584.
Fazlyab, M., Robey, A., Hassani, H., Morari, M., and Pappas, G. J. (2019). Eﬃcient and accurate estimation
of lipschitz constants for deep neural networks. arXiv preprint arXiv:1906.04893.
Golowich, N., Rakhlin, A., and Shamir, O. (2018). Size-independent sample complexity of neural networks. In
Conference On Learning Theory, pages 297–299. PMLR.
Howard, J. and Ruder, S. (2018). Universal language model ﬁne-tuning for text classiﬁcation. arXiv preprint
arXiv:1801.06146.
Jaitly, N., Nguyen, P., Senior, A., and Vanhoucke, V. (2012). Application of pretrained deep neural networks
to large vocabulary speech recognition.
Jin, C., Netrapalli, P., Ge, R., Kakade, S. M., and Jordan, M. I. (2019). On nonconvex optimization for machine
learning: Gradients, stochasticity, and saddle points. arXiv preprint arXiv:1902.04811.
Li, G., Kamath, P., Foster, D. J., and Srebro, N. (2021). Eluder dimension and generalized rank. arXiv preprint
arXiv:2104.06970.
Marmanis, D., Datcu, M., Esch, T., and Stilla, U. (2015). Deep learning earth observation classiﬁcation using
imagenet pretrained networks. IEEE Geoscience and Remote Sensing Letters, 13(1):105–109.
Maurer, A., Pontil, M., and Romera-Paredes, B. (2016). The beneﬁt of multitask representation learning. The
Journal of Machine Learning Research, 17(1):2853–2884.
Oquab, M., Bottou, L., Laptev, I., and Sivic, J. (2014). Learning and transferring mid-level image representations
using convolutional neural networks. In Proceedings of the IEEE conference on computer vision and pattern
recognition, pages 1717–1724.
Osband, I. and Van Roy, B. (2014). Model-based reinforcement learning and the eluder dimension. arXiv
preprint arXiv:1406.1853.
Russo, D. and Van Roy, B. (2013). Eluder dimension and the sample complexity of optimistic exploration. In
NIPS, pages 2256–2264. Citeseer.
Scaman, K. and Virmaux, A. (2018). Lipschitz regularity of deep neural networks: analysis and eﬃcient
estimation. arXiv preprint arXiv:1805.10965.
Tai, Y., Yang, J., and Liu, X. (2017). Image super-resolution via deep recursive residual network. In Proceedings
of the IEEE conference on computer vision and pattern recognition, pages 3147–3155.
Tan, C., Sun, F., Kong, T., Zhang, W., Yang, C., and Liu, C. (2018). A survey on deep transfer learning. In
International conference on artiﬁcial neural networks, pages 270–279. Springer.
10

Tripuraneni, N., Jordan, M., and Jin, C. (2020). On the theory of transfer learning: The importance of task
diversity. Advances in Neural Information Processing Systems, 33.
Weng, R., Yu, H., Huang, S., Cheng, S., and Luo, W. (2020). Acquiring knowledge from pre-trained model to
neural machine translation. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence, volume 34,
pages 9266–9273.
11

A
Proof of Theorem 1
Proof. Note that the second phase is to ﬁnd the best function within the class Fta ◦ˆh. We ﬁrst apply the
standard bounded diﬀerence inequality (Bartlett and Mendelson, 2002) as shown in Theorem 6.
Theorem 6 (Bartlett and Mendelson (2002)). With a probability at least 1 −δ,
sup
f∈Fta
|Rta(f ◦ˆh) −ˆRta(f ◦ˆh)| ≤
√
2π ˆGnta(Fta ◦ˆh)
√nta
+
s
9 ln(2/δ)
2nta
=: ϵ(ˆh, nta, δ),
furthermore, the total generalization error can be upper bounded by
Eta( ˆfta ◦ˆh) ≤
inf
f∈Fta Eta(f, ˆh)
|
{z
}
approximation error
+
ϵ(ˆh, nta, δ)
|
{z
}
generalization error over Fta◦ˆh
.
(2)
The result follows by the deﬁnition,
inf
f∈Fta Eta(f, ˆh) ≤
inffta∈Fta Eta(fta, ˆh)
inffso∈F⊗S
so Eso(fso, ˆh) + µ/ν
(Eso( ˆfso, ˆh) + µ/ν) ≤νEso( ˆfso, ˆh) + µ.
B
Proof of Theorem 2
Proof. To show f ∗
so is (L, 0)-transferable to f ∗
ta, we bound the approximation error of the target task given any
ﬁxed h ∈H.
Eta(f ∗
ta, h)
= EX,Y [lta(f ∗
ta ◦h(X), Y ) −lta(f ∗
ta ◦h∗(X), Y )]
= EX∥f ∗
ta ◦h(X) −f ∗
ta ◦h∗(X)∥2
2
≤LEX∥h(X) −h∗(X)∥2
2.
(3)
Now using Assumption 1, we have
Eso(h) = EX,Y [lso(h(X), Y ) −lso(h∗(X), Y )]
= EX∥h∗(X) −h(X)∥2
2
Combined with Equation (3), we have suph∈H[Eta(f ∗
ta, h)/Eso(h)] ≤L.
Firstly, using Theorem 6 on source tasks solely, we have Eso(ˆh) = ˜O(ˆGnso(G)/√nso). Deﬁnition 1 gives us
inf
f∈Fta Eta(f, ˆh) ≤LEso(ˆh) = ˜O(LˆGnso(G)/√nso).
Combined with (2), we have
Eta

ˆfta, ˆh

= ˜O

L
ˆGnso(G)
√nso
+
ˆGnta

Fta ◦ˆh

√nta

.
C
Proof of Theorem 3
Proof. Let f ∗
ta(x) = f ′∗
ta(f ∗
1 ◦h∗(x), . . . f ∗
m ◦h∗(x)). Deﬁne new tasks t1, . . . tm. Each ti has the prediction
function f ∗
i .
12

By Deﬁnition 1, the source tasks are (ν, µ)-transferable to each ti. By Theorem 1, we have
inf
fi∈Fso Eti(fi, ˆh) ≤νEso( ˆfso, ˆh) + µ.
Since we use L2 loss,
inf
fi∈Fso EX∥fi ◦ˆh(X) −f ∗
i ◦h∗(X)∥2
2 =
inf
fi∈Fso Eti(fi, ˆh) ≤νEso( ˆfso, ˆh) + µ.
As this holds for all i ∈[m], we have
inf
f1,...,fm∈Fso EX∥(f1 ◦ˆh(X), . . . , fm ◦ˆh(X)) −(f ∗
1 ◦h∗(X), . . . , f ∗
m ◦ˆh∗(X))∥2
2 ≤m(νEso( ˆfso, ˆh) + µ).
Using Assumption 2, we have
inf
f∈Fta Eta(f, ˆh) ≤L′m(νEso( ˆfso, ˆh) + µ).
Theorem 3 follows by plugging this into (2).
D
Proof of Corollary 1
This section we prove Corollary 1 using Theorem 3, the standard bound for Gaussian complexity of DNN model
and the Gaussian complexity decomposition from Tripuraneni et al. (2020).
The following theorem bounds the Rademacher complexity of a deep neural network model given an input
dataset XN = (x1, . . . , xN)T ∈RN×d.
Theorem 7 (Golowich et al. (2018)). Let σ be a 1-Lipschitz activation function with σ(0) = 0. Recall that MK
is the depth K neural network with d-dimensional output with bounded input ∥xji∥≤DZ and ∥Wk∥∞≤M(k)
for all k ∈[K]. Recall that L =

x 7→αT x + β : ∀α ∈Rp, ∥α∥2 ≤M(α)
	
is the linear class following the
depth-K neural network. Then,
Rn(L ◦MK; XN) ≤2DZ
√K + 2 + log d · M(α)ΠK
k=1M(k)
√n
.
Since for any function class F, ˆGn(F) ≤2√log n · ˆRn(F), we also have the bound for the Gaussian
complexity under the same conditions.
Applying Theorem 7, we have an upper bound for the second term in Theorem 3:
ˆGnta

Fta ◦ˆh

√nta
≤2DZ
p
log(nta)
p
Kta + 2 + log(d)MαΠKta
k=1M(k)
√nta
= ˜O
 
DZsqrtKtaM(α)ΠKta
k=1M(k)
√nta
!
.
It only remains to bound ˆGT nso
 F⊗T
so
◦H

/√Tnso in Theorem 3. To proceed, we introduce the decompo-
sition theorem for Gaussian complexity (Tripuraneni et al., 2020).
Theorem 8 (Theorem 7 in Tripuraneni et al. (2020)). Let the function class F consist of functions that
are L(F)-Lipschitz and have boundedness parameter DX = supf,f ′,x,x′ ∥f(x) −f ′(x′)∥2.
Further, deﬁne
Q = {h( ¯X) : h ∈H, ¯X ∈∪T
j=1{Xj}}. Then the Gaussian complexity of the function class F⊗T (H) satisﬁes,
ˆGX
 F⊗T (H)

≤
4DX
(nT)3/2 + 128C
 F⊗T (H)

· log(nT),
where C (F⊗t(H)) = L(F)ˆGX(H) + maxq∈Q ˆGq(F).
With Theorem 8 applied, we have
ˆGT nso
 F⊗T
so
◦H

√Tnso
≤
8DX
(Tnso)2 +
128

L(Fso)ˆGT nso(H) + maxq∈Q ˆGq(Fso)

· log(Tnso)
√Tnso
.
(4)
The second term relies on the Lipschitz constant of DNN, which we bound with the following lemma. Similar
results are given by Scaman and Virmaux (2018); Fazlyab et al. (2019).
13

Lemma 1. If the activation function is 1-Lipschitz, any function in L ◦MK is M(α)ΠK
k=1M(k)-Lipschitz
with respect to L2 distance.
Proof. The linear mapping x 7→Wkx is ∥Wk∥2-Lipschitz. Combined with the Lipschitz of activation function
we have σ(Wkx) is also ∥Wk∥2-Lipschitz. Then the composition of diﬀerent layers has Lipschitz constant ΠkMk.
The Lemma follows by adding the Lipschitz of the last linear mapping.
Thus, we have
L(Fso) ≤M(α)ΠKso
k=1M(k).
By Theorem 7,
max
q∈Q
ˆGq(Fso) = ˜O(DZ
√KsoM(α)ΠKso
k=1M(k)
√nso
).
Plug the above two equations into (4), we have
ˆGT nso
 F⊗T
so
◦H

√Tnso
= ˜O
 
DX
(Tnso)2 + M(α)ΠKso
k=1M(k)(
ˆGT nso(H)
√Tnso
+ DZ
√Kso
√nso
)
!
,
where DX = sup(h,f,x),(h′,f ′,x′)∈H×Fso×X ∥h ◦f(x) −h′ ◦f ′(x′)∥.
Lemma 2. The boundedness parameter DX satisﬁes DX ≤DZM(α)ΠKso
k=1M(k).
Proof. The proof is given by induction. Let rk denote the vector-valued output of the k-th layer of the prediction
function. First note that
DX ≤2
sup
f∈Fso,z∈Z
∥f(z)∥2 ≤2M(α)∥rKso∥2.
For each output of the k-th layer, we have
∥rk∥2 = ∥σ(Wkrk−1)∥2 ≤∥Wkrk−1∥2
2 ≤∥Wk∥2
2∥rk−1∥2
2,
where the ﬁrst inequality is by the 1-Lipschitz of the activation function. By induction, we have
DX ≤2DZM(α)ΠKso
k=1M(k).
Recall that Fta = L ◦MKta−Kso−1 ◦(F⊗p
so ) and the Lipschitz constant L′ ≤M(α)ΠKta
Kso+2M(k). Using
Theorem 3 and apply Lemma 2, we have
Eta

ˆfta, ˆh

=
˜O
 
pνΠKta
k=Kso+2M(k)
 
M(α)ΠKso
k=1M(k)(
ˆGT nso(H)
√Tnso
+ DZ
√Kso
√nso
)
!
+ DZ
√Kta · M(α)ΠKta
k=1M(k)
√nta
!
.
E
Lower bound results for the diversity of depth-1 NN
We ﬁrst give the proof using ReLu activation function (Theorem 4), as the result is more intuitive before we
extend the similar results to other activation functions.
Proof. As we consider arbitrary representation function and covariate distribution, for simplicity we write
X′ = h∗(X) and Y ′ = h(X).
We consider a subset of depth-1 neural networks with ReLu activation function: F = {x 7→[⟨x, w⟩−(1 −
ϵ/4)]+ : ∥x∥2 ≤1, ∥w∥≤1}. Let fw be the function with parameter w. Consider U ⊂{x : ∥x∥2 = 1} such that
⟨u, v⟩≤1 −ϵ for all u, v ∈U, u ̸= v.
Lemma 3. For any T ⊂F, |T | ≤⌊|U|/2⌋, there exists a V ⊂U, |V | ≥⌊|U|/2⌋such that any f ∈T , f(v) = 0
for all v ∈V .
14

Proof. For any set T , let UT = {u : ∃t ∈T , u ∈arg maxu∈U⟨u, ft⟩} be a subset of U. Thus, |UT | ≤T ≤⌊|U|/2⌋.
Let V = U \ UT .
For any f ∈T , let uf be its closed point in U. Let vf be its closed point in V . Let θf be the angle between
uf and vf. By the deﬁnition of U, we have cos(θf) = ⟨uf, vf⟩≤1 −ϵ. We will show that ⟨f, vf⟩≤1 −ϵ/4.
Note that since ⟨f, vf⟩≤⟨f, uf⟩, we have the angle between f and v is larger than θf/2. By the simple
fact that cos(θf/2) ≤1 −(1 −cos(θf))/4, we have ⟨f, vf⟩≤1 −ϵ/4. Thus, f(vf) = 0 and f(v) = 0 for all
v ∈V .
For any set of prediction functions in source tasks, let V be the set deﬁned in the above lemma. Consider
any u ∈U \ V and let V ′ = U \ (V ∪u). By this construction, we have fu(u) = ϵ/4, while all f ∈T , f(u) = 0.
Note that
inf
f∈F
1
|V ′|
X
x∈V ′
(fu(u) −f(x))2 ≥|V ′| −1
16|V ′| ϵ2 ≥1
32ϵ2,
while
X
f∈T
1
|V ′|
X
x∈V ′
(f(u) −f(x))2 = 0.
Thus, we let X′ = u almost surely and Y ′ follows a uniform distribution over V ′. This is true when the
covariate distribution the same as Y ′ and h = x 7→x and h∗= x 7→u. Recalling the deﬁnition of diversity, we
have
inf
f∈F EX′,Y ′(fu(X′) −f(Y ′))2 = ϵ2/32 and 1
T
X
ft∈T
inf
f ′
t∈F EX′,Y ′(fs(X′) −f ′
s(Y ′))2 = 0.
Note that the same result holds when the bias b ≤−(1 −ϵ/4). For general bounded ∥b∥2 ≤1, one can add
an extra coordinate in x as an oﬀset.
In Theorem 4, we show that in depth-1 neural network with ReLu activation function, we will need
exponentially many source tasks to achieve diversity.
Similar results can be shown for other non-linear
activation functions that satisﬁes the following condition:
Assumption 3. Let σ : R 7→R be an activation function. We assume there exists x1, x2 ∈R, x1 > x2, such
that |σ(x1)| ≥supx≤x2 |σ(x)|M for some M > 0.
ReLu satisﬁes the assumption with any M > 0 for any x1 > 0 and x2 ≤0. Also note that any continuous
activation function that is lower bounded and increasing satisﬁes this assumption.
Theorem 9. Let σ satisﬁes the above assumption with M for some x1 and x2. Let F = {x 7→σ(8(x1 −x2)⟨x, w⟩−
7x1 + 8x2)) : ∥x∥2 ≤1, ∥w∥2 ≤1}. Let T = {f1, . . . , fT } be any set of depth-1 neural networks with ReLu
activation in F. If T ≤2d log(2)−1, there exists some representation h∗, h′ ∈H, some distribution PX and a
target function f ∗
ta ∈F, such that
inff∈F Eta(f, h′)
inff∈F Eso(f, h′) ≥(M −1)2
8
.
Proof. We follow the construction in the proof of Theorem 9, ﬁx an ϵ = 1/2 and let U ⊂{x : ∥x∥2 = 1} such
that ⟨u, v⟩≤1 −ϵ for all u, v ∈U, u ̸= v.
For any source tasks set T , let UT = {u : ∃t ∈T , u ∈arg maxu∈U⟨u, ft⟩} be a subset of U.
Thus,
|UT | ≤T ≤⌊|U|/2⌋. Let V = U \ UT . For any f ∈T , v ∈V , similarly to the previous argument, we have
⟨f, v⟩≤1 −ϵ/4 = 1/8. Therefore, ⟨f, v⟩≤1 −ϵ/4 = 1/8 ≤x2.
For any set of prediction functions in source tasks, let V be the set deﬁned in the above lemma. Consider any
u ∈U \ V and let V ′ = U \ (V ∪u). By this construction, we have fu(u) = σ(x1), while all f ∈T , f(u) = σ(x2).
Note that
inf
f∈F
1
|V ′|
X
x∈V ′
(fu(u) −f(x))2 ≥|V ′| −1
|V ′|
≥1
2(σ(x1) −σ(x2))2,
while
X
f∈T
1
|V ′|
X
x∈V ′
(f(u) −f(x))2 ≤4 sup
x≤x2
σ(x2)2.
15

Thus
inff∈F
1
|V ′|
P
x∈V ′(fu(u) −f(x))2
P
f∈T
1
|V ′|
P
x∈V ′(f(u) −f(x))2 ≥(M −1)2
8
F
Proof of Theorem 5
Proof. Since dims(F∗) is at least dE, for any set {f1, . . . , ft}, there exists a ft+1 that is (F∗, ϵ)-independent of
{f1, . . . , ft}. By deﬁnition, we have
∃x1, x2 ∈X,
t
X
i=1
∥fi(x1) −fi(x2)∥2
2 ≤ϵ2, while ∥ft+1(x1) −ft+1(x2)∥2
2 ≥ϵ2.
We only need to construct appropriate data distribution PX and representation g, g∗to ﬁnish the proof. As we
do not make any assumption on g, g∗and PX, it would be simple to let X1 = g(X) and X2 = g∗(X).
We let the distribution of X1 be the point mass on x1. Let X2 be the uniform distribution over {x1, x2}.
For the excess error of source tasks, we have
inf
f ′
1,...,f ′
t
t
X
i=1
EX1,X2∥f ′
i(X1) −fi(X2)∥2
2
≤
t
X
i=1
EX1,X2∥fi(X1) −fi(X2)∥2
2
=
t
X
i=1
1
2∥fi(x1) −fi(x2)∥2
2 ≤ϵ2
2 .
For the excess error of the target task ft+1, we have
inf
f ′
t+1∈F EX∥f ′
t+1(X1) −ft+1(X2)∥2
2
=
inf
f ′
t+1∈F[1
2∥f ′
t+1(x1) −ft+1(x2)∥2
2 + 1
2∥f ′
t+1(x1) −ft+1(x1)∥2
2]
≥inf
a∈R[1
2∥a −ft+1(x2)∥2
2 + 1
2∥a −ft+1(x1)∥2
2]
= 1
4∥ft+1(x1) −ft+1(x2)∥2
2 ≥ϵ2
4 .
The statement follows.
G
Extending to general loss functions
In all the above analyses, we assume the square loss function for both source and target tasks. We ﬁrst show
that diversity under square loss implies diversity under any convex loss function. Let ∇l(x, y) be the gradient
of function ∇l(·, y) evaluated at x.
Lemma 4. Any task set F that is (ν, µ)-diverse over any prediction space under square loss is also (ν/c1, µ/c1)-
diverse over the same space under loss l, if l is c1 strongly-convex and for all x ∈X
E[∇l(g∗(X), Y ) | X = x] = 0
(5)
Proof. Using the deﬁnition of the strongly convex and (5),
EX,Y [l(ft ◦h(X), Y ) −l(f ∗
t ◦h∗(X), Y )]
≥EX,Y [∇l (f ∗
t ◦h∗(X), Y )T (f ∗
t ◦h∗(X) −ft ◦h(X)) + c1 ∥f ∗
t ◦h∗(X) −ft ◦h(X)∥2
2]
= c1EX,Y [∥f ∗
t ◦h∗(X) −ft ◦h(X)∥2
2],
which is the generalization error under the square loss.
16

Note that Equation (5), is a common assumption made in various analyses of stochastic gradient descent
(Jin et al., 2019).
On the other direction, we show that any established diversity over the target task with square loss also
implies the diversity over the same target task with any loss l if ∇2l ≻c2I for some c2 > 0.
Lemma 5. Any task set F that is (ν, µ)-diverse over a target prediction space under square loss is also
(νc2, µc2)-diverse over the same space under loss l, if ∇2l(·, y) ≻c2I for all y ∈Yta and for all x ∈X we have
E[∇l(g∗(X), Y ) | X = x] = 0.
Proof. The proof is the same as the proof above except for changing the direction of inequality. Using the
deﬁnition of the strongly convex and (5),
EX,Y [l(ft ◦h(X), Y ) −l(f ∗
t ◦h∗(X), Y )]
≤EX,Y [∇l (f ∗
t ◦h∗(X), Y )T (f ∗
t ◦h∗(X) −ft ◦h(X)) + c2 ∥f ∗
t ◦h∗(X) −ft ◦h(X)∥2
2]
= c2EX,Y [∥f ∗
t ◦h∗(X) −ft ◦h(X)∥2
2],
which is the generalization error under the square loss.
H
Missing proofs in Section 6
Assume we have T tasks, which is (ν, µ)-diverse over Fso, and Yso ⊂R. Then we can construct a new source
task so with multivariate outputs, i.e. Yso ⊂RT , such that Hso = F⊗T
so
and each dimension k on the output,
given an input x, is generated by
Yk(X) = f ∗
k ◦h∗(X) + ϵ.
Intuitively, this task is equivalent to T source tasks of a single output, which is formally described in the
following Theorem.
Theorem 10. Let so be a source task with Yso ⊂RK and f ∗
so(·) = (m∗
1(·), . . . , m∗
K(·)) for some class
M : Z 7→R. Then if the task set t1, . . . , tK with prediction functions m∗
1, . . . , m∗
K from hypothesis class M is
(ν, µ)-diverse over M, then so is ( ν
K , µ
K )-diverse over the same class.
Proof. This can be derived directly from the deﬁnition of diversity. We use t to denote the new task. By
deﬁnition,
inf
fso∈Fso Eso(fso, h) = inf
hso EX∥(m1 ◦(X), . . . , mK ◦h(X)) −(m∗
1 ◦h∗(X), . . . , m∗
K ◦h∗(X))∥2
2
=
K
X
k=1
inf
mk∈M ∥mk ◦h(X) −m∗
k ◦h∗(X)∥2
2
=
K
X
k=1
inf
mk∈M Etk(ftk, h)
As (t1, . . . , tK) is (ν, µ)-diverse, we have
supm∗∈M infm∈M Em∗(m, h)
infhso∈Fso Eso(fso, h) + µ/ν = 1
K
supm∗∈M infm∈M Em∗(m, h)
1
K
PK
k=1 infmk∈M Etk(hk, h) +
µ
νK
≤ν
K .
For the multiclass classiﬁcation problem, we try to explain the success of the pretrained model on ImageNet,
a single multi-class classiﬁcation task. For a classiﬁcation problem with K-levels, a common way is to train
a model that outputs a K-dimensional vector, upon which a Softmax function is applied to give the ﬁnal
classiﬁcation result. A popular choice of the loss function is the cross-entropy loss.
Now we formally introduce our model. Let the Softmax function be q : RK 7→[0, 1]K. Assume our response
variable y ∈RK is sampled from a multinomial distribution with mean function q(f ∗
so ◦h∗(x)) ∈[0, 1]K,
where h∗∈H : X 7→Z and f ∗
so ∈Fso : Z 7→RK. We use the cross-entropy loss l : [0, 1]K × [0, 1]K 7→R,
l(p, q) = −PK
k=1 pk log(qk).
17

Assumption 4. [Boundedness] We assume that any f ◦h(x) ∈Fso × H is bounded in [−log(B), log(B)] for
some constant positive B. We also assume the true function mink U(f ∗◦h∗(x))k ≥1/B∗for some B∗> 0.
Theorem 11. Under Assumption 4, a K-class classiﬁcation problem with f ∗
so(·) = (m∗
1(·), . . . , m∗
K(·)) for some
m∗
1, . . . , m∗
K ∈M and Softmax-cross-entropy loss function is (2B2
∗B4ν, B2
∗µ)-diverse over any the function
class M as long as f ∗
so with L2 loss is (ν, µ)-diverse over M.
Proof. We consider any target task with prediction function from M⊗K′. Let U : RK′ 7→[0, 1]K′ be the
softmax function. We ﬁrst try to remove the cross-entropy loss. By deﬁnition, the generalization error of any
f ◦h ∈M⊗K′ × H is
Eta(f ◦h) −Eta(h∗◦h∗)
= EX,Y [−
K′
X
i=1
1(Y = i) log( U(f ◦h)
U(f ∗◦h∗))]
= EX[−
K′
X
i=1
U(f ∗◦h∗) log( U(f ◦h)
U(f ∗◦h∗))],
(6)
which gives us the KL-divergence between two distributions U(f ◦h) and U(f ∗◦h∗).
Lemma 6. For any two discrete distributions p, q ∈[0, 1]K, we have
KL(p, q) ≥1
2(
K
X
i=1
|p −q|)2 ≥1
2
K
X
i=1
(pi −qi)2.
On the other hand, if mini pi ≥b for some positive b, then
KL(p, q) ≤1
b2
K′
X
i=1
(pi −qi)2.
Proof. The ﬁrst inequality is from Theorem 2 in Dragomir and Gluscevic (2000). The second inequality is by
simple calculus.
By the assumption 4, we have that for any h, g, x,
U(f ◦h(x))i ∈[
1
KB2 ,
1
1 + (K −1)/B2 ].
We also have PK
i=1 exp(f ◦h(x)i) ∈[K/B, KB]. To proceed,
supf ∗
ta∈M⊗Kta inf ˆ
fta Eta( ˆfta ◦h) −Eta(f ∗
ta ◦h∗)
inf ˆ
fso∈M⊗Kso Eso( ˆfso ◦h) −Eso(f ∗so ◦h∗) +
B2∗µ
2B2∗B4ν
(Applying (6) and Lemma 6)
≤2B2
∗
supf ∗
ta∈M⊗Kta inf ˆ
fta EX∥U( ˆfta ◦h(X)) −U(f ∗
ta ◦h∗(X))∥2
2
inf ˆ
fso∈M⊗Kso EX∥U( ˆfso ◦h(X)) −U(f ∗so ◦h∗(X))∥2
2 +
µ
B4ν
(Using the boundedness of
K
X
i=1
exp (f ◦h(x)i))
≤2B2
∗B2 supf ∗
ta∈M⊗Kta inf ˆ
fta EX∥exp( ˆfta ◦h(X)) −exp(f ∗
ta ◦h∗(X))∥2
2
inf ˆ
fso∈M⊗Kso EX∥exp( ˆfso ◦h(X)) −exp(f ∗so ◦h∗(X))∥2
2 +
µ
B2ν
(Using the Lipschitz and convexity of exp)
≤2B2
∗B4 supf ∗
ta∈M⊗Kta inf ˆ
fta EX∥ˆfta ◦h(X) −f ∗
ta ◦h∗(X)∥2
2
inf ˆ
fso∈M⊗Kso EX∥ˆfso ◦h(X) −f ∗so ◦h∗(X)∥2
2 + µ/ν
≤2B2
∗B4ν.
The diversity follows.
18

I
Experimental details
Each dimension of inputs is generated from N(0, 1). We use Adam with default parameters for all the training
with a learning rate 0.001. We choose ReLu as the activation function.
True parameters.
The true parameters are initialized in the following way. All the biases are set by 0. The
weights in the shared representation are sampled from N(0, 1/√nu). The weights in the prediction function for
the source task are set to be orthonormal when Kso = 1 and p ≤nu. For the target prediction function or
source prediction function if Kso > 1, the weights are sampled from N(0, 1/√nu) as in the representation part.
Hyperparameters.
Without further mentioning, we use the number of hidden units, nu = 4, input dimension
p = 4, K = 5, Kta = Kso = 1, the number of observations nso = 1000 and nta = 100 by default. Note that
since p is set to be 4 by default, equivalently we will have nso · p = 4000 observations.
Computation resources.
MacBook Pro (13-inch, 2019, Two Thunderbolt 3 ports); Processor: 1.4 GHz
Quad-Core Intel Core i5. Memory: 16 GB 2133 MHz LPDDR3.
19

