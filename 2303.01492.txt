An Improved Classical Singular Value Transformation
for Quantum Machine Learning
Ainesh Bakshi
ainesh@mit.edu
MIT
Ewin Tang
ewint@cs.washington.edu
University of Washington
March 7, 2023
Abstract
Quantum machine learning (QML) has shown great potential to produce large quantum
speedups for computationally intensive linear algebra tasks. The quantum singular value
transformation (QSVT), introduced by Gilyén, Su, Low and Wiebe [GSLW19], is a unifying
framework to obtain QML algorithms. We provide a classical algorithm that matches the
performance of QSVT on low-rank inputs, up to a small polynomial overhead. Under
efﬁcient quantum-accessible memory assumptions, given a bounded matrix A ∈Cm×n, a
vector b ∈Cn, and a bounded degree-d polynomial p, QSVT can output a measurement
from the state |p(A)b⟩in O(d∥A∥F) time after linear-time pre-processing. We show that, in
the same setting, for any ε > 0, we can output a vector v such that ∥v −p(A)b∥⩽ε∥b∥in
O(d9∥A∥4
F/ε2) time after linear-time pre-processing. This improves upon the best known
classical algorithm [CGL+20a], which requires O(d22∥A∥6
F/ε6) time.
Instantiating the aforementioned algorithm with different polynomials, we obtain fast
quantum-inspired algorithms for regression, recommendation systems, and Hamiltonian
simulation. We improve in numerous parameter settings on prior work, including those that
use problem-specialized approaches, for quantum-inspired regression [CGL+20b, CGL+20a,
SM21, GST22, CCH+22] and quantum-inspired recommendation systems [Tan19, CGL+20a,
CCH+22].
Our key insight is to combine the Clenshaw recurrence, an iterative method for computing
matrix polynomials, with sketching techniques to simulate QSVT classically. The tools we
introduce in this work include (a) a non-oblivious matrix sketch for approximately preserving
bi-linear forms, (b) a non-oblivious asymmetric approximate matrix product sketch based
on ℓ2
2 sampling, (c) a new stability analysis for the Clenshaw recurrence, and (d) a new
technique to bound arithmetic progressions of the coefﬁcients appearing in the Chebyshev
series expansion of bounded functions, each of which may be of independent interest.
arXiv:2303.01492v2  [quant-ph]  6 Mar 2023

Contents
1
Introduction
3
2
Our Results
4
2.1
Applications: Dequantizing QML
. . . . . . . . . . . . . . . . . . . . . . . . . . .
5
3
Technical overview
6
4
Related work
12
5
Preliminaries
14
5.1
Linear algebra . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
14
5.2
Polynomials and the Chebyshev Basis . . . . . . . . . . . . . . . . . . . . . . . . .
14
5.3
Sampling and query access
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
16
6
Extending the Sketching Toolkit
18
6.1
The Bi-Linear Entry-wise Sampling Transform . . . . . . . . . . . . . . . . . . . .
18
6.2
Approximate Matrix Product via ℓ2
2 Sampling . . . . . . . . . . . . . . . . . . . . .
20
7
Sums of Chebyshev coefﬁcients
22
8
Properties of the Clenshaw recursion
28
8.1
Deriving the Clenshaw recursions
. . . . . . . . . . . . . . . . . . . . . . . . . . .
28
8.2
Evaluating even and odd polynomials . . . . . . . . . . . . . . . . . . . . . . . . .
29
9
Stability of the scalar Clenshaw recursion
30
9.1
Analyzing error propagation
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
31
9.2
Bounding the iterates of the Clenshaw recurrence
. . . . . . . . . . . . . . . . . .
33
10 Computing matrix polynomials
35
10.1 Computing odd matrix polynomials . . . . . . . . . . . . . . . . . . . . . . . . . .
35
10.2 Generalizing to even polynomials . . . . . . . . . . . . . . . . . . . . . . . . . . . .
40
10.3 Bounding iterates of singular value transformation . . . . . . . . . . . . . . . . . .
43
11 Dequantizing algorithms
44
11.1 Recommendation systems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
44
11.2 Linear regression
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
45
11.3 Hamiltonian simulation
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
45
Acknowledgments
46
References
46

1
Introduction
Quantum machine learning (QML) has rapidly developed into a highly active ﬁeld of study,
with numerous proposals for speeding up machine learning tasks on quantum computers
([HHL09, RML14, KP17, WKS14, WKS16, BKL+19], and see [DW20, CHI+18] for a survey).
These proposals include quantum algorithms for several basic tasks in machine learning,
including regression, principal component analysis, support vector machines, recommendation
systems, Hamiltonian simulation and semi-deﬁnite programming. The central goal in QML is
to demonstrate a problem on which quantum computers obtain a substantial practical speedup
over classical computers. A successful resolution of this goal would provide compelling
motivation to further invest resources into developing scalable quantum computers (i.e. be a
killer app for quantum computing).
The quantum singular value transformation (QSVT) framework [LC17, CGJ19, GSLW19] uses
ideas from signal processing to present a uniﬁed approach to design algorithms for quantum
linear algebra and, by extension, QML. This framework is known to capture essentially all
linear algebraic QML techniques [MRTC21] (and, more generally, the most prominent examples
of quantum advantage), so it will be the focus of our investigation in this work. Informally,
the QSVT framework deﬁnes a central primitive known as the block-encoding, and shows
that, given a matrix A ∈Cm×n with bounded operator norm as a block-encoding, one can
generate a block-encoding for a degree-d polynomial applied to that matrix, p(A), deﬁned in
the appropriate sense, with only O(d log(mn)) overhead in gate complexity.1 Efﬁcient block-
encodings do not exist in general, but they do exist for two broad classes of matrices, assuming
appropriately strong forms of coherent access: matrices with low sparsity (a typical setting for
quantum simulation) and matrices with low stable rank (a typical setting for quantum machine
learning). We treat the latter case; speciﬁcally, we assume that the classical input data is in a
quantum-accessible data structure, which allows for efﬁcient (as in low-depth) preparation of a
block-encoding of A/∥A∥F, where ∥A∥F = ∑i,j
Ai,j
2 denotes the Frobenius norm of A.
This type of block-encoding is the one commonly used for quantum linear algebra algorithms on
classical data [KLLP19, CdW21], since it works for arbitrary matrices and vectors, paying only
a ∥A∥F/∥A∥in sub-normalization. The main consequence of the QSVT framework for QML is
that, given a matrix A ∈Cm×n with bounded operator norm, a vector b ∈Cn of unit norm, and
a degree-d polynomial p with |p(x)| ⩽1 for x ∈[−1, 1], we can output a sample2 from the state
|p(
A
∥A∥F )b⟩in time O(d log(mn)). Since in applications we care about applying polynomials to
the singular values of A, which in
A
∥A∥F lie in the range [0, ∥A∥
∥A∥F ], we need to pay an additional
overhead of ∥A∥F
∥A∥to amplify these singular values [GSLW19, Theorem 30] to the full region [0, 1],
making the gate complexity to produce a state ε-close to |p(A)b⟩, O(d ∥A∥F
∥A∥log(mn) log(1/ε)).
Note that, though the quantum algorithm has only logarithmic dependence on ε, if we wish to
determine a property of the output state (say, whether its norm is large or whether it has high
overlap with another state), this incurs an O(1/ε), since distinguishing a state from one ε-far
in trace distance requires Ω(1/ε) additional overhead, even when given an oracle efﬁciently
1We will generally not concern ourselves with log(mn) factors, since quantum algorithms typically count bit
complexity where classical algorithms count word RAM complexity, which muddles any comparison of such
log(mn) and log 1
ε factors.
2There is a cost of post-selection which we ignore in this exposition. More formally, we consider |p(
A
∥A∥F )⟩to be
an un-normalized state, and deﬁne a sample from such a state |ψ⟩to be a measurement of |ψ⟩in the computational
basis with probability ∥ψ∥2, and “failure” otherwise.
3

preparing that state. Therefore, the effective running time for QSVT is Ω

d∥A∥F
∥A∥ε log(mn)

.
At ﬁrst glance, it appears that QSVT obtains an exponential speed-up over classical algorithms,
since it scales logarithmically with input size. However, as ﬁrst argued in the work of [Tan19],
to obtain a fair comparison, the classical algorithms should be equipped with a ℓ2
2 sampling data
structure to access the input (see Section 5.3 for the precise model). Given access to such a data
structure, and an accuracy parameter ε > 0, Chia, Gilyén, Li, Lin, Tang and Wang [CGL+20a]
showed that there exists a classical algorithm that outputs a vector v such that ∥p(A)b −v∥⩽ε,
in O(d22∥A∥6
F/ε6) time. As a result, they obtain quantum-inspired algorithms for quantum-
inspired versions of several fundamental problems in machine learning, albeit with a running
time that is a large polynomial in the degree of the polynomial p, the Frobenius norm of A, and
the accuracy parameter ε.
Multiple papers [KLLP19, KP22] have conjectured that the large polynomial running time
(signiﬁcantly larger than quartic) for classical algorithms may be inherent, and thus can demon-
strate a practical speedup for several problems in QML. This is borne out in prior work on
quantum-inspired algorithms, which is dominated by the cost of computing a singular value
decomposition of a matrix with Ω(( ∥A∥F
∥A∥ε )2) rows and columns, immediately incurring a power-
six dependence in ε and the Frobenius norm of A. Therefore, the central question we address in
this paper is as follows:
Is the large polynomial running time for dequantizing QSVT inherent?
2
Our Results
We answer the central question above in the negative for all parameters except polynomial
degree, and show that there are indeed better classical algorithms that simulate QSVT. Our
main result is as follows:
Theorem 2.1 (Classical Singular Value Transformation, informal Theorem 10.1). Suppose we are
given A ∈Cm×n with ∥A∥⩽1 and b ∈Cn, and an accuracy parameter ε. Then, after O(nnz(A) +
nnz(b)) preprocessing time to create a data structure3, for an even or odd degree-d polynomial p such
that |p(x)| ⩽1 for x ∈[−1, 1], we can output a description of a vector y ∈Cn such that with
probability at least 0.9, ∥y −p(A)b∥⩽ε∥b∥in O

d9 log4(d)∥A∥4
F log(n)/ε2
time. This description
gives y as either Ax or A†x for a sparse vector x, depending on the parity of p, and allows us to compute
entries of y in ˜O

d4∥A∥2
F/ε2
time or obtain an ℓ2
2 sample from y in ˜O

d6∥A∥4
F/(ε2∥y∥2)

time.
Remark 2.2 (No large speedup for low-degree QSVT circuits). Recall the setting of QSVT for
QML, where we have A with bounded operator norm in a quantum-accessible data structure,
can apply a QSVT circuit, and wish to learn a simple linear algebraic property of the output
state (say, the inner product ⟨v|p(A)b⟩for a given vector v). As discussed, the quantum gate
complexity of this QSVT algorithm4 is Ω(d∥A∥F/ε).
3If we are already given A and b in the QRAM data structures needed to prepare a block-encoding of A/∥A∥F
and a quantum state of b/∥b∥, this preprocessing can be done in O(d8 log8(d) log2(n)∥A∥4
F/ε4) time.
4This is not a “lower bound” statement, merely a statement about the gate complexity of QSVT. Note that it’s
possible to embed quantumly hard problems into even this sort of QSVT, such that, for very particular linear algebraic
properties, there is a large quantum speedup. For example, one could ask to compute the Forrelation [AA18] of
the output vector with a given vector; this would require Ω(n) queries to the input classically but runs in time
polynomial in log(n).
4

Classically, we can use Theorem 2.1 to compute a description for p(A)b in O(d9 log4(d)∥A∥4
F log(n)/ε2)
time, which we can then use to compute an entry or estimate an overlap ⟨v|p(A/∥A∥F)b⟩. The
runtime is dominated by the cost of the initial algorithm, and so the gap between quantum and
classical is 1-to-9 for the degree d, 1-to-4 for ∥A∥F (which we think of as square root of stable
rank ∥A∥F
∥A∥), and 1-to-2 for ε. So, considering constant degree, the gap is merely quartic, which is
the type of speedup that prior work suggests might just sufﬁce to achieve a quantum advantage
for intermediate-term quantum computers [BMN+21].
Remark 2.3 (Comparison to [CGL+20a, JLGS20, GLG22]). There are three papers that get
similar results about “dequantizing the quantum singular value transformation”. The work
of Chia, Gilyén, Li, Lin, Tang, and Wang [CGL+20a] gives a runtime of eO(d22∥A∥6
F/ε6) (after
renormalizing so that ∥A∥= 1 instead of ∥A∥F = 1). We improve in all parameters over this
work: degree of the polynomial p, Frobenius norm of A, and accuracy parameter ε.
The work of Jethwani, Le Gall, and Singh [JLGS20] provides two algorithms for applying
f (A)b for Lipschitz-continuous f and A with condition number κ. (Standard results in function
approximation state that such functions can be approximated by polynomials of degree O(L),
where L is the Lipschitz constant of f and the tail is either polynomial or exponential depending
on how smooth f is [Tre19].) In particular, they achieve a runtime of O(∥A∥6
Fκ20(d2 + κ)6/ε6)
to apply a degree-d polynomial. Again, we improve in all parameters over this work: degree
of the polynomial p, Frobenius norm of A, and accuracy parameter ε. We also do not incur
condition number dependence.
Finally, the work of Gharibian and Le Gall [GLG22] considers QSVT when the input matrices are
sparse, which is the relevant regime for quantum chemistry and other problems in many-body
systems. Here, the matrices of interest are local Hamiltonians, which are row- and column-
sparse. Their main contribution is distinguishing between constant-degree QSVT circuits, which
can be simulated in polynomial time classically, and polynomial-size QSVT circuits (in the
number of qubits), which can solve BQP-complete problems. We deal with a different setting,
where all circuits can be simulated efﬁciently in polynomial time.
Next, we describe the implications of Theorem 2.1 to speciﬁc problems in QML and show that
we obtain faster quantum-inspired algorithms for several such problems.
2.1
Applications: Dequantizing QML
We begin by stating the de-quantization result we obtain for regression.
Corollary 2.4 (De-quantizing Regression, informal Corollary 11.5). Given an ℓ2
2-sampling oracle
for A ∈Cm×n and b ∈Cn such that ∥A∥, ∥b∥⩽1, a parameter 0 < σ < 1, and an accuracy
parameter ε > 0, we can output the representation of a vector y ∈Cn such that with probability
at least 0.9,
y −A+
⩾σb
 ⩽ε, where A+
⩾σ denotes the function on A that is the inverse for singular
values that are ⩾σ and smoothly thresholds away all singular values below σ. This algorithm runs in
˜O

∥A∥4
F/(ε2σ11)

time, where σ is the chosen threshold for A. We can also output a sample from y in
the same running time.
Remark 2.5 (Comparison with [CGL+20b, CCH+22, SM21]). We note that Chia et. al. [CGL+20b]
get a running time of O

∥A∥6
F/(ε6σ28)

. We improve over this result in all parameters.
Chepurko et. al. [CCH+22] get a running time of ˜O(∥A∥4
F log(d)/(σ8ε4)), where σ is the mini-
mum singular value of A, assuming that we perform regression with some sizable regularization
5

λ > 0. We improve over their work in ε dependence, and match it in ∥A∥F dependence. As for
the σ dependence, this achieves error to ε∥A∥∥b∥error, which is worse than the ε∥x∗∥bound
that we actually achieve; making the conversion costs an additional 1/σ4 in overhead. So, for
this low-error setting, we achieve better σ dependence, but if only worse error is required, their
algorithm performs better.
Shao and Montanaro [SM21] get a running time of O
 ∥A∥6
F/σ8ε2
which matches our ε-
dependence, obtains a better σ-dependence and a worse Frobenius norm dependence. Addi-
tionally they requires that b is in the image of A.
In the context of Recommendation systems, the goal is to output a sample from the rows of
a low-rank approximation to A, denoted by A⩾σ, where we zero out all the singular values
smaller than σ (see Section 11.1 for a formal problem description). We then obtain the following
corollary:
Corollary 2.6 (De-quantizing Recommendation Systems, informal Corollary 11.3). Given a
matrix A ∈Cm×n such that ∥A∥⩽1, an accuracy parameter ε, and an i ∈[n], we can produce a data
structure in O(nnz(A)) time such that, we can compute a vector xi such that with probability at least
0.9,
A†x −[A⩾σ]i,∗
 ⩽ε∥Ai,∗∥in ˜O
 ∥A∥4
F/
 σ9ε2
time. Further, we can ℓ2
2-sample from xi in
˜O

∥A∥4
F/

σ6ε2A†x
2
time.
Remark 2.7 (Comparison to [CGL+20a, CCH+22]). Chia et. al. [CGL+20a] achieve a runtime
of ˜O( ∥A∥6
F
σ16ε6 ). We improve upon it in every parameter, including error ε, the threshold σ, and the
Frobenius norm ∥A∥F.
Chepurko et. al [CCH+22, Theorem 26] achieves a runtime that is at least Ω(k3/ε6) to get
a low-rank approximation of an input vector M with the guarantee that ∥A −M∥2
F ⩽(1 +
ε)∥A −Ak∥2
F. The authors use that ℓ2
2 importance sampling sketches oversample ridge leverage
score sketch in certain parameter regimes, so this requires certain addition assumptions on the
size of ∥Ak∥F and the residual ∥A −Ak∥F. Work on quantum recommendation systems [KP17,
GSLW19] require a singular value threshold σ instead of a rank threshold k, and the standard
way to convert between this “sketching”-style error bound and the “QSVT”-style error bound
is to bound k ⩽∥A∥2
F/σ2. Upon doing this, we see that the runtime is ˜O( ∥A∥6
F
σ6ε6 ). Our work
improves upon this in the ∥A∥F and ε parameters, but we lose a factor of σ3.
Next, we state the de-quantization result we obtain for Hamiltonian simulation:
Corollary 2.8 (Hamiltonian Simulation, informal Corollary 11.8). Given a symmetric Hamiltonian
H ∈Cn×n with ∥H∥⩽1 and a vector b ∈Cn, we can output a description of a vector v such that, with
probability ⩾0.9,
v −eiHtb
 ⩽ε∥b∥in ˜O(t9∥H∥4
F/ε2).
We note that the only prior work [CGL+20a] we are aware of in the low-rank regime obtains a
running time O

t16∥H∥6
F/ε6
, and we improve upon it in every parameter.
3
Technical overview
In this section, we describe our classical framework for simulating QSVT and provide an
overview of our key new contributions. In brief: our main conceptual contribution, using an
iterative method (the Clenshaw recurrence) instead of a pure sketching algorithm, is enough
to achieve O(d13∥A∥4
F/ε4), corresponding to d iterations of matrix-vector products of size
∥A∥2
F/(ε/d3)2 by ∥A∥2
F/(ε/d3)2. With insights into the stability of the Clenshaw recurrence
6

and sums of Chebyshev coefﬁcients, we improve the ε/d3 to an ε/(d2 log2(d)). In the worst
case, we may need to rescale ε to ε/d2; in other words, our stability analysis is tight up to log(d)
factors. With insights into matrix sparsiﬁcation, we improve the ε4 to an ε2, which is clearly
tight. Together,5 this gives the ﬁnal runtime of O(d9 log4(d)∥A∥4
F/ε2). We begin by providing
an informal description of the input access model.
Input model: Oversampling and query access.
Our goal is to give a classical version of the
quantum singular value transformation. In the quantum setting, we have a matrix A ∈Cm×n
with ∥A∥⩽1 (given as a block-encoding) and a vector b with ∥b∥= 1 (given encoded into the
amplitudes of a quantum state), and wish to compute f (A)b, where f : [−1, 1] →R is an
appropriately smooth function. The block-encoding access model is inherently quantum, but
there are standard ways to construct a block-encoding for a matrix from classical data, including
by assuming the matrix is sparse with efﬁciently computable entries [GSLW19, Lemma 48] and
given as a quantum circuit [GSLW19, Deﬁnition 44]. Both of these support universal quantum
computation, and therefore cannot be simulated classically unless BPP=BQP.
However, block-encodings can also be achieved given an arbitrary matrix in a data structure
placed in quantum random access memory [GSLW19, Lemma 50], a proposal for a quantum
hardware architecture theorized to give the ability for quantum computers to efﬁciently access
stored memory in superposition [GLM08]. As noted in prior work, this model incurs a square
root of stable rank, i.e. O(∥A∥F/∥A∥) overhead, allowing it to be simulated classically with
only polynomial overhead with sketching algorithms [CGL+20a].
Chia et. al. [CGL+20a] proceed by introducing the access model of oversampling and query access,
which can be interpreted as the classical analogue of the block-encoding and amplitude-encoded
quantum state in the quantum setting. In particular, given a vector v ∈Cn, oversampling and
query access corresponds to: (1) given an index i ∈[n], outputting vi, (2) sampling an index j with
probability |v(j)|2/∥v∥2, and (3) outputting ∥v∥2. Similarly for a matrix A ∈Cm×n, oversampling
and query access corresponds to having oversampling and query access to all rows of A, as well as
the vector of the row norms. We point the reader to [CGL+20a] for an explanation of why this
model is the right classical analogue to benchmark quantum algorithms against. In short, this
model has closure properties very similar to that of the block-encoding, and can be achieved
whenever quantum states and generic block-encodings can be prepared efﬁciently.
Computing polynomials of matrices.
The main primitive of QSVT is to take a block-encoding
of a matrix A and give a block-encoding of an even or odd polynomial of that matrix, p(A), with
an overhead of deg(p). When A is asymmetric, we can interpret QSVT as applying the matrix
function that applies p to each singular value of A (Deﬁnition 5.1). In this way, quantum linear
algebra algorithms can apply generic functions to the singular values of a matrix, provided that
they are smooth enough to be approximated well by a low-degree polynomial.
In order to simulate QSVT classically, given a matrix A ∈Cm×n, a vector b ∈Cn, and
a polynomial p : [−1, 1] →R, our goal is to compute some description p(A)b. Speciﬁ-
cally, we aim for our algorithm to run in poly(∥A∥F, 1
ε , d, log(mn)) time after O(nnz(A) +
nnz(b)) preprocessing, and to output sampling and query access to x, where ∥x −p(A)b∥⩽
5It is natural to wonder here why the complexity is not something like d∥A∥4
F/(ε/(d2 log2(d)))2
=
d5 log2(d)∥A∥4
F/ε2. Such a runtime is conceivable, but our analysis essentially replaces two factors of 1/ε with
factors of 1/d2, so our sketching ideas do not save any factors of d.
7

ε∥p∥sup∥b∥, where ∥p∥sup = maxx∈[−1,1]|p(x)|. We note that as a byproduct, we also obtain a
O(nnz(A) + nnz(b) + poly(∥A∥F, 1/ε, d, log(mn))) algorithm for the task of outputting x.
We require the running time of the algorithm to be independent of input dimension (after the
preprocessing) and therefore are compelled to create sketches of A and b and work with these
sketches. We note that prior work [Tan19, Tan21, CGL+20a] stops here, and directly computes a
SVD of the sketches, and applies the relevant polynomial p to the singular values of the sketch.
As noted in the previous section, this approach loses large polynomial factors in the relevant
parameters.
Combining sketches with iterative algorithms.
Our main conceptual insight is to run itera-
tive algorithms on the resulting sketches of A and b in order to approximate matrix polynomials.
In the canonical numerical linear algebra regime (working with matrices directly, instead
of sketches), there are two standard methods to achieve this goal: (1) compute the polyno-
mial explicitly through something like a Clenshaw recurrence [Cle55], or (2) use the Lanczos
method [Lan50] to create a roughly-orthogonal basis for the Krylov subspace {b, Ab, A2b, . . .}
and then apply the function exactly to the matrix in this basis, in which A is tridiagonal, implic-
itly using a polynomial approximation in the analysis. We note that in a world where we are
allowed to pay O(nnz(A)) (or even O(m + n)) time per-iteration, we can simply run either of
these algorithms and call it a day. The main challenge in the setting we consider is that each
iterative step must run in time that is dimension-independent.
Given that we sketch each iterate down to a size that is dimension-independent, we introduce
additive error at each step. So, in essence, we must perform a stability analysis of an iterative
method, where the error introduced in each iteration is from truncating the iterates. Finite-
precision/stability analysis of Clenshaw and Lanczos iteration are well-understood [Cle55,
Pai76, MMS18], and therefore one might expect to use these analysis in a black-box manner.
However, unlike typical ﬁnite-precision analyses, which is concerned with error either at the
granularity of “number of bits to maintain”, and so is ﬁne with polynomial loss, our ﬁnal
runtime depends polynomially on the quality of our error analysis. This constraint requires us
to form a more intimate understanding of these iterative methods.
Folklore intuition suggests that Lanczos is a stabler algorithm for applying matrix functions,
but state-of-the-art analyses of it rely on the stability of the Clenshaw recurrence as a subroutine
(see [MMS18]), and therefore gives strictly worse error-accumulation bounds than the Clenshaw
recurrence. In particular, if we wish to compute a generic p(A)b to ε error in the regime where
every matrix-vector product Ax incurs an error of ε∥A∥∥x∥using Lanczos, the stability analysis
of Musco, Musco, and Sidford suggests that the error of the output is O(d5.5ε), which would
introduce a d11 in our setting [MMS18].6 To incur less error, we do not use Lanczos and analyze
the Clenshaw recurrence directly.
6This computation arises from taking Lemma 9 of [MMS18] to evaluate a degree-d polynomial, say, bounded
by 1 in [−1, 1]. A generic example of such polynomial is only by a constant in [−1 −η, 1 + η] when η = O(1/d2)
(Lemma 5.4), and has Chebyshev coefﬁcients bounded only by a constant, without decaying. Thus, the bound from
Lemma 9 becomes O(d5∥E∥). Continuing the analysis into [Pai76], E is the matrix whose ith column is the error
incurred in the ith iteration; each column has norm ε∥A∥∥vj+1∥= ε in our setting where we only incur error in
matrix-vector products, since ∥A∥= 1 by normalizing and ∥vj+1∥= 1 because the algorithm normalizes it to unit
norm, and we assume that scalar addition and multiplication can be performed exactly. We have no control over the
direction of error, so we can at best bound ∥E∥with the column-wise bound, giving ∥E∥⩽∥E∥F ⩽
√
kε. So, our
version of [MMS18, Equation 16] gives ∥E∥⩽
√
dε, which gives us the asserted O(d5.5ε) bound.
8

Stability of the scalar Clenshaw recurrence.
The Chebyshev polynomials {Tℓ(x)}ℓform a
basis and therefore any degree-d polynomial p(x) can be written as a linear combination of
Chebyshev polynomials, i.e. p(x) = ∑d
ℓ=0 aℓTℓ(x). The Clenshaw recurrence computes p(x)
through the iteration computing qd through to q0:
qd+1, qd+2 := 0;
qk := 2xqk+1 −qk+2 + ak;
p(x) = 1
2(a0 + q0 −q2).
For example, in the randomized numerical linear algebra (RNLA) literature, this is often applied
in the case where ad = 1 and ak = 0 otherwise, to evaluate a degree-d Chebyshev polynomial
Td(x). Note that by Markov’s inequality, a bounded polynomial has derivative bounded by
d2 [Sch41]. This is achieved for a Chebyshev polynomial, p(x) = Td(x). So, if our error was
only in changing x to some value in (x −ε, x + ε), this error must cascade to a O(d2ε) worst-case
error in the output. This rough argument suggests that, in some sense, a O(d2) overhead is the
best we could hope for.
Our ﬁrst technical contribution is an analysis showing that the Clenshaw algorithm gives this
optimal overhead, up to a logarithmic overhead. This proceeds by showing that the overhead
can be upper bounded by the size of the largest Clenshaw iterate |qk| (see Proposition 9.1),
and then bounding the size of iterate |qk| by O(d log(d)∥p∥sup) (see Theorem 9.4). The main
lemma we prove states that for a bounded polynomial p(x) = ∑d
ℓ=0 aℓTℓ(x), sums of the form
aℓ+ aℓ+2 + · · · are all bounded by O(log(ℓ)∥p∥sup) (Fact 7.3). This statement follows from facts
in Fourier analysis (in particular, this log(ℓ) is the same log as the one occurs when bounding
the L1 norm of the Dirichlet kernel). Finally, we note that just using individual bounds on
coefﬁcients does not sufﬁce, since this would only give a bound of O(d) on these sums, which
can be exponentially worse than the bound we obtain, and would give O(d3) overhead (and a
d13 in the ﬁnal runtime instead of a d9).
We note that we are not aware of any prior work where such a sharp analysis appears in
the literature. The standard literature either considers an additive error (where, unlike usual
models like ﬂoating-point arithmetic, each multiplication incurs identical error regardless
of magnitude) [Cle55, FP68, MH02] or eventually boils down to bounding |ai| (since their
main concern is dependence on x) [Oli77, Oli79], which is insufﬁcient to get our d2 log(d)
stability bound. The modern work we are aware of shows a O(d2) bound only for Chebyshev
polynomials [BKM22], sometimes used to give a O(d3) bound for computing generic bounded
polynomials [MMS18], since a degree-d polynomial can be written as a linear combination of
Tk(x) with bounded coefﬁcients.
Extending Clenshaw to compute matrix polynomials.
So far, we have only considered the
Clenshaw recurrence for a scalar x. Generalizing this to matrices is fairly straightforward:
for a matrix A (not necessarily square), we wish to apply an even or odd polynomial to it,
and then apply it to a vector b. Then, we can use a corresponding variant of the Clenshaw
recurrence to compute it. For example, the matrix Clenshaw recurrence for computing p(x) =
∑d
ℓ=0 a2ℓ+1T2ℓ+1(x) is
ud+1, ud+2 := 0;
uk := 2(2AA† −I)uk+1 −uk+2 + 2a2k+1Ab;
p(A)b = u := 1
2(u0 −u1).
9

The question becomes how to perform this iteration efﬁciently and stably, in a dimension-
independent regime. We begin by sketching down our matrix and vector: we show that it
sufﬁces to maintain a sparse description of uk of the form uk = Avk where vk is sparse. In
particular, we produce sketches S ∈Cn×s and T ∈Ct×m such that
1. ∥AS(AS)† −AA†∥⩽ε,
2. ∥ASS†b −Ab∥⩽ε,
3. ∥TAS(TAS)† −AS(AS)†∥⩽ε
Sketches that satisfy the above type of guarantees are called approximate matrix product (AMP)
sketches, and are standard in the quantum-inspired algorithms literature [CGL+20a].
We
observe that if we have sampling and query access to A and b, then we can produce these
sketches of size s, t = O( ∥A∥2
F
ε2
log(n) log 1
δ), and then compute TAS in O(st) time. We also
note that the ﬁrst two guarantees follow from observing that AMP sketches oversample the
symmetric approximate matrix product sketch by a factor of 2, and thus both guarantees hold
simultaneously. The third guarantee is straight-forward and does not require asymmetry. Using
these guarantees we can sketch the iterates as follows:
uk = 2(2AA† −I)uk+1 −uk+2 + 2a2k+1Ab
= 4AA†Avk+1 −2Avk+1 −Avk+2 + 2a2k+1Ab
≈AS[4(TAS)†(TAS)vk+1 −2vk+1 −vk+2 + 2a2k+1S†b].
Therefore, we can interpret Clenshaw iteration as the recursion on the dimension-independent
term vk ≈4(TAS)†(TAS)vk+1 −2vk+1 −vk+2 + 2a2k+1S†b, and then applying AS on the left to
lift it back to m dimensional space. We can then analyze the sketched recursion to obtain an
algorithm that runs in O(∥A∥4
F/ε4) time per-iteration, not including the loss from rescaling ε,
whereas we wish to achieve a O(1/ε2) dependence per-iteration.
Though so far we have only used a very limited subset of the sketching toolkit—namely, ℓ2
2 im-
portance sampling—we remark that it’s not clear how, for example, oblivious sketches [NN13]
or the connection between importance sampling and leverage score sampling [CCH+22] help
us, since our choices of sketches are optimal up to log factors for the guarantees we desire. To
get the additional improvement, we need a new sketching technique.
Improving the ε-dependence.
A natural next step to improve per-iteration runtime is to
sparsify the matrix TAS, in order to make matrix-vector products more efﬁcient. If we can
sparsify TAS to O(1/ε2) non-zero entries, then we get the desired quadratic savings in per-
iteration cost.
There is signiﬁcant literature on sparsifying the entries of a matrix [AM07, KD14, BKKS21].
However, it does not sufﬁce for us to use these as a black box. For example, consider the sketch
given by Drineas and Zouzias [DZ11]: for a matrix M ∈Rn×n, zero out every entry smaller
than
ε
2n, then sample entries proportional to their ℓ2
2 magnitude, and consider the corresponding
unbiased estimator of M, denoted ˜M. The guarantee is that the operator norms are close,
M −˜M
 ⩽ε, and the sparsity is O(n log(n) ∥A∥2
F
ε2 ). This is precisely the guarantee we need
and the sketch can be performed efﬁciently; however, this does not sparsify the matrix for us,
since in our setting, our matrices TAS have dimension ∥A∥2
F/ε2, so the sparsity guarantee is
only O(∥A∥4
F/ε4 ln(n)) = O(st ln(n)). In other words, this sketch gives us no improvement on
sparsity!
10

Bi-linear entry-wise sampling transform.
Our second main technical contribution is to by-
pass this barrier by noticing that we don’t need a guarantee as strong as a spectral norm bound.
Instead, we only need to achieve approximations of the form
ASMv −AS ˜Mxk
 ⩽ε,
(1)
for various different choices of xk. So, we only need our sketch ˜M to approximate M in some
directions with good probability, instead of approximating it in all directions. We deﬁne a
simple sketch, which we call the Bilinear Entry Sampling Transform (BEST7), that is an unbiased
estimator for A linearly (rather than the product A†A) and achieves the aforementioned guar-
antee (Equation (1)). This sketch is sampled from the same distribution as the one of Drineas
and Zouzias from above, but without the zero-ing out small entries. In particular, we deﬁne
M(k) = 1
pi,j
Ai,jeie†
j
with probability pi,j =
Ai,j

∥A∥2
F
.
Then, to get the sketch with sparsity r, we take the average of r independent copies of the above
random matrix:
BEST(A) = 1
r ∑
k∈[r]
M(k).
This deﬁnition is not new: for example, one of the earliest papers on matrix sparsiﬁcation for
linear algebra algorithms brieﬂy considers this sketch [AM07], and this sketch has been used
implicitly in prior work on dequantizing QML algorithms [Tan21]. However, as far as we know,
our analysis of this sketch for preserving bi-linear forms and saving a factor of 1/ε2 is novel.
We show that BEST(A) satisﬁes the following guarantees: taking r = Ω(∥M∥2
F/ε2), this sketch
preserves the bilinear form u†Mv to ε∥u∥∥v∥error with probability ⩾0.9 (Lemma 6.2). Second,
taking r = Ω(∥M∥2
Fn), this sketch preserves the norm ∥Mv∥to 0.1∥v∥error with probability
⩾0.9. So, by taking r = Ω(∥M∥2
F(n + 1
ε2 )), we can get both properties. However, with this
choice of r, BEST(M) does not preserve the spectral norm ∥M∥, even to constant error.8 This
sketch can be interpreted as a relaxation of the approximate matrix product and the sparsiﬁcation
sketches above, since we use it in regimes where ∥BEST(A)∥is unbounded and ∥BEST(A)b∥
is not ε-close to ∥Ab∥. However, if we use it in the “interior” of a matrix product expression,
as a proxy for approximate matrix product, it can be used successfully to improve the n/ε2
dependence of spectral norm entry sparsiﬁcation to something like n + 1/ε2. In our setting, this
corresponds to an improvement of 1/ε4 to 1/ε2.
Applying BEST to Clenshaw Iteration.
Returning to our odd Clenshaw iteration, we had our
approximate iterate
uk ≈AS[4(TAS)†(TAS)vk+1 −2vk+1 −vk+2 + 2a2k+1S†b].
We can approximate this by taking B = BEST(TAS) and B† = BEST((TAS)†) with sparsity
r = Θ(∥A∥4
F/ε2) to get that the bilinear forms like [AS]i,∗(TAS)†((TAS)vk+1) are preserved.
This allows us to successfully approximate with sparsiﬁed matrices,
≈AS[4B†Bvk+1 −2vk+1 −vk+2 + 2a2k+1S†b].
7That is, we pronounce BEST(A) as “best of A”. We make no normative claim about our sketch vis-a-vis other
sketches.
8At this point, one might wonder whether one can simply zero out small entries to get these guarantees with the
additional spectral norm bound. This is not the case; if we only zero out entries smaller than ε/(2n), this threshold
is too small to improve the matrix Bernstein tail bound, whereas if we zero out entries smaller than, say, 1/(100n),
the matrix Bernstein tail bound goes through, but the bilinear form is not ε-preserved.
11

so vk ≈4B†Bvk+1 −2vk+1 −vk+2 + 2a2k+1S†b.
This recurrence in vk will be our algorithm (Algorithm 10.4): compute S and T in the pre-
processing phase, then compute the iteration of vk’s, pulling fresh copies of B and B† each time,
and output it as the description of the output u ≈p(A)b. What remains is the error analysis,
which similar to the scalar case, requires bounding the size of the iterates ∥vk∥. We used the
ε-approximation of the bilinear form to show that the sparsiﬁcations B and B† successfully
approximate uk; we use the constant-error approximation of the norm to show that the ∥vk∥’s
are bounded.
Challenges in extending ﬁnite-precision Clenshaw iteration.
We note here that the above
error does not directly follow from the error of the sketches along with the ﬁnite-precision scalar
Clenshaw iteration. The error we incur in iteration k is not ε∥uk+1∥, but ε∥vk+1∥, so our error
analysis requires understanding both the original Clenshaw iteration along with the “dual”
Clenshaw iteration vk, which requires a separate error analysis.
Sums of Chebyshev coefﬁcients.
One ﬁnal technical wrinkle remains, which is to bound the
error accumulation of the matrix Clenshaw recurrences. In the same way that the original scalar
Clenshaw algorithm required bounding arithmetic progressions of Chebyshev coefﬁcients with
step size two, aℓ+ aℓ+2 + · · · by the norm of the corresponding polynomial p = ∑d
ℓ=0 aℓTℓ(x),
to prove stability for the even and odd versions, we need to bound arithmetic progressions with
step size four. Surprisingly, this is signiﬁcantly more challenging.
We give a thorough explanation for this in Remark 7.8, but in brief, some arithmetic progressions
of Chebyshev coefﬁcients arise naturally as linear combinations of polynomial evaluations. For
example, ∑k⩾0 a2k = 1
2(p(−1) + p(1)), so we can conclude that
∑k⩾0 a2k
 ⩽∥p∥sup. Through
Fourier analysis, this can be generalized to progressions with different step sizes, but breaks
for progressions with certain offsets. The sum ∑k⩾0 a4k+1 is one such example, and this is a
quantity we need to bound to give a ˜O(d2) bound on the iterates of odd matrix Clenshaw. A
naïve bound of O(d) follows from bounding each coefﬁcient separately, but this results in a
signiﬁcantly worse running time down the line.
In Lemma 7.4, we show that we can bound the above sum ∑k⩾0 a4k+1 by O(log2(d)∥p∥sup).
This shows that this sum has near-total cancellation: despite our guarantee on coefﬁcient
being merely that it is bounded by a constant,9 the sum of O(d) of these coefﬁcients is only
poly-logarithmic in magnitude. The proof proceeds by considering many different polynomial
evaluations p(x0), p(x1), . . . , p(xd), and trying to write the arithmetic progression as a linear
combination of these evaluations ∑ckp(xk). This can be thought of as a linear system in the ck’s,
which we can then prove has a solution, and then bound the ℓ1 norm of this solution. To do
this, we argue that we can bound its solution A−1b by the solution of a different system, C−1b,
where C is an matrix that is entrywise larger than A. Since this is not true generally, we use
strong properties of the particular matrix A in the linear system at hand to prove this.
4
Related work
Quantum machine learning.
Starting with the breakthrough work of Harrow, Hassidim and
Lloy [HHL09] for solving sparse linear systems, QML has seen a growing number of pro-
9In fact, for any degree d, there are polynomials of that degree which are bounded and yet ∑d
ℓ=0|aℓ| = Ω(d)
[Tre19, Theorems 8.2 and 8.3].
12

posals. Several works assume access to quantum random access memory and provide quantum
algorithms for Principal Component Analysis [LMR14], support vector machines [RML14],
k-means clustering [LMR13], quantum recommendation systems [KP17], and neural net-
works [WKS16, WKS14]. We refer the reader to [BWP+17] for a comprehensive survey. In
particular, we note that the current proposals that resist de-quantization and potentially obtain
a super-polynomial quantum speedup include Zhao, Fitzsimons, and Fitzsimons on Gaussian
process regression [ZFF19] and Lloyd, Garnerone, and Zanardi on topological data analy-
sis [LGZ16].
Quantum-inspired algorithms.
Since the breakthrough work of Tang [Tan19] for recommen-
dation systems, there has been a ﬂurry of work obtaining quantum-inspired classical algorithms
for various problems in machine learning and numerical linear algebra. The works most closely
related to ours are the quantum-inspired framework for QSVT by Chia, Gilyén, Li, Lin, Tang
and Wang [CGL+20a], and a de-quantization of QSVT when the input matrices are sparse
by Gharibian and Le Gall [GLG22]. A series of papers also focus on dequantizing speciﬁc
problems, such as regression [CGL+20b, CGL+20a, SM21, GST22, CCH+22], recommendation
system [Tan19, CGL+20a, CCH+22] and principal component analysis [Tan21]. Classical algo-
rithms for quantum simulation include Van den Nest’s work on simulating restricted classes of
quantum circuits using probabilistic methods [VdN11] and Rudi, Wossnig, Ciliberto, Rocchetto,
Pontil, and Severini’s work on using the Nyström method to simulate a sparse Hamiltonian H
on a sparse input state [RWC+20]. .
Randomized numerical linear algebra.
Our work is draws upon several ideas from the
randomized numerical linear algebra literature and we refer the reader to the following surveys:
[Mah11, Woo14] for the relevant background. The asymmetric approximate matrix product
sketch we introduce is closely related to the approximate matrix product sketches considered
in [MI11, MZ11, CEM+15] (non-oblivious) and [CNW15] (oblivious). We note that in our setting,
we cannot afford to use oblivious sketches, since we lose the ability to maintain sampling
and query access to the resulting vectors. Cohen, Nelson, and Woodruff obtain an AMP
guarantee in the symmetric case using ridge leverage score sampling, whereas we work with
asymmetric matrices and would need to pay condition number dependent oversampling factors
to sample from the ridge leverage score distribution. Magdon-Ismail [MI11] and Magen and
Zouzias [MZ11] construct asymmetric approximate matrix products, but require sampling
from
n
∥Ai∥2 + ∥Bi∥2/(∑i∈[n]∥Ai∥2 + ∥Bi∥2)
o
and
n
∥Ai∥∥Bi∥/ ∑i∈[n]∥Ai∥∥Bi∥
o
respectively.
We note that we do not have efﬁcient access to these distributions in the sampling and query
access model. Drineas, Kannan and Mahoney [DKM06] use ℓ2
2 sampling to get the assymetric
approximate matrix product guarantee, but under the Frobenius norm instead of spectral norm,
which yet again does not sufﬁce for our purposes. Regarding the bi-linear entry-wise sampling
transform, creating sketches by sampling entries proportional to their squared-magnitude are
well-known in the literature but are typically used to bound the operator norm of the matrix
rather than any particular bi-linear form. Bounding the operator norm directly happens to be too
restrictive, as we discussed in the technical overview. Regardless, entry-wise sampling sketches
were introduced by Achlioptas and McSherry [AM07] to speed up low-rank approximation.
Arora, Hazan and Kale used them in the context of faster algorithms for SDP solving [AHK06].
Since then, a series of works have provided sharper guarantees for such sketches [GT09,
DZ11, KD14, KDMI17, BKKS21]. Finally, there has been a recent ﬂurry of work on sublinear
time algorithms for low-rank approximation under various structural assumptions on the
13

input [MW17, BW18, IVWW19, BCW20], however all such algorithms still incur at least a linear
dependence on the number of rows and columns of the input matrix.
In addition to the usual sketching and sampling toolkit, we also use ideas from iterative
algorithms in the numerical linear algebra literature. Iterative methods, such as power iteration,
Krylov subspace methods, Golub-Kahan Bidiagonalization, Arnoldi iteration, and the Lanczos
iteration are ubiquitous in scientiﬁc computing and are used for matrix inversion, solving linear
systems, linear programming, low-rank approximation, and numerous other fundamental linear
algebra primitives [Saa81, TBI97]. Our work is closely related to iterative algorithms that use the
sparsiﬁcation sketches and exploit singular values gaps, such as [AZL16, GS18, MM15, BCW22],
however these results incur dimension dependent factors, which are crucial to avoid in our
setting.
5
Preliminaries
We use the notation f ≲g to denote the ordering f = O(g) (and respectively for ≳and ≂).
5.1
Linear algebra
For vectors v ∈Cn, ∥v∥denotes standard Euclidean norm (so ∥v∥:= (∑n
i=1|vi|2)1/2). For a
matrix A ∈Cm×n, the Frobenius norm of A is ∥A∥F := (∑m
i=1 ∑n
j=1
Ai,j
2)1/2 and the spectral norm
of A is ∥A∥:= supx∈Cn,∥x∥=1 ∥Ax∥.
A singular value decomposition (SVD) of A is a representation A = UDV†, where for N :=
min(m, n), U ∈Cm×N and V ∈Cn×N are isometries and D ∈RN×N is diagonal with σi := Di,j
and σ1 ⩾σ2 ⩾· · · ⩾σN ⩾0. We can also write this decomposition as A = ∑N
i=1 σiuiv†
i , where
ui := U∗,i and vi := V∗,i.
We denote the set of singular values of A by Spec(A) := {σk}k∈[N]. For a Hermitian matrix
A ∈Cn×n and a function f : R →C, f (A) refers to applying f to the eigenvalues of A.
5.2
Polynomials and the Chebyshev Basis
We consider polynomials with complex coefﬁcients, p ∈C[x]. For a Hermitian matrix A, p(A)
refers to evaluating the polynomial with x replacing A; this is equivalent to applying p to the
eigenvalues of A. The right deﬁnition for applying p to a general non-square matrix is subtle; as
done in QSVT, we restrict to settings where the matrix formed by evaluating p on the singular
values of A coincides with the evaluation of a corresponding polynomial in A.
Deﬁnition 5.1 (Deﬁnition 6.1 of [CGL+20a]). For a matrix A ∈Cm×n and degree-d polynomial
p(x) ∈C[x] of parity-d (i.e., even if d is even and odd if d is odd), we deﬁne the notation p(A)
in the following way:
1. If p is even, meaning that we can express p(x) = q(x2) for some polynomial q(x), then
p(A) := q(A†A) = p(
√
A†A).
2. If p is odd, meaning that we can express p(x) = x · q(x2) for some polynomial q(x), then
p(A) := A · q(
√
A†A).
14

For example, if p(x) = x2 + 1, then p(QV)(A) = A†A + I, and if p(x) = x3 + x, then p(QV)(A) =
AA†A + A. Looking at a singular value decomposition A = ∑σiuiv†
i , p(QV)(A) = ∑p(σi)uiv†
i
when p is odd and p(QV)(A) = ∑p(σi)viv†
i when p is even, thus making this deﬁnition coincide
with the singular value transformation as given in [GSLW19, Deﬁnition 16].
We work in the Chebyshev basis of polynomials throughout. Let Tℓ(x) and Uℓ(x) denote
Chebyshev polynomials of the ﬁrst and second kind, respectively. They can be deﬁned on
[−1, 1] via
Tk(cos(θ)) = cos(nθ)
(2)
Uk(cos(θ)) = sin((n + 1)x)/ sin(x),
(3)
but we will give attention to their recursive deﬁnitions, since we will use them for computation.
T0(x) = 1
U0(x) = 1
T1(x) = x
U1(x) = 2x
(4)
Tk(x) = 2x · Tk−1(x) −Tk−2(x)
Uk(x) = 2x · Uk−1(x) −Uk−2(x)
For a function f : [−1, 1] →R, we denote ∥f ∥sup := supx∈[−1,1]| f (x)|. In this norm, the
Chebyshev polynomials have ∥Tk(x)∥sup = 1 and ∥Uk(x)∥sup = n + 1.
Any Lipschitz continuous function10 f : [−1, 1] →R can be written as a (unique) linear
combination of Chebyshev polynomials, f (x) = ∑ℓaℓTℓ(x) (where we interpret Tℓ(x) ≡0
for negative ℓ). When f is a degree-d polynomial, then aℓ= 0 for all ℓ> d. A common
way to approximate a function is by truncating the polynomial expansion; we denote this
operation by fk(x) = ∑k
ℓ=0 aℓTℓ(x), and we denote the remainder to be ¯fk(x) = f (x) −fk(x) =
∑∞
ℓ=k+1 aℓTℓ(x).
We use the following well-known properties of Chebyshev polynomials from Mason and
Handscomb [MH02].
Ti(x) = 1
2(Ui(x) −Ui−2(x))
(5)
Ui(x) = ∑
j⩾0
Ti−2j(x)(1 + Ji −2j ̸= 0K)
(6)
Tjk(x) = Tj(Tk(x))
(7)
U2k+1(x) = Uk(T2(x))U1(x) = Uk(T2(x))2x
(8)
d
dx Tk(x) = kUk−1(x)
(9)
We recommend the book by Trefethen, and rely on its results to prove our bounds. We use a
couple of standard results about Chebyshev polynomials.
Lemma 5.2 (Coefﬁcient bound, consequence of [Tre19, Eq. (3.12)]). Let f : [−1, 1] →R be a
Lipschitz continuous function. Then all its Chebyshev coefﬁcients ak are bounded: |ak| ⩽2∥f ∥sup.
Lemma 5.3 ([Tre19, Theorems 8.1 and 8.2]). Let a function f analytic in [−1, 1] be analytically
continuable to the open Bernstein ellipse Eρ = { z+z−1
2
| |z| < ρ}, where it satisﬁes | f (x)| ⩽M for
some M. Then its Chebyshev coefﬁcients satisfy |a0| ⩽M and
|ak| ⩽2Mρ−k,
k ⩾1.
10We call a function f : [−1, 1] →R Lipschitz continuous if there exists a constant C such that | f (x) −f (y)| ⩽
C|x −y| for x, y ∈[−1, 1].
15

Consequently,
∥¯fn∥sup ⩽2Mρ−n
ρ −1 .
Lemma 5.4. For a degree-d polynomial p, and δ =
1
4d2 ,
sup
x∈[−1−δ,1+δ]
|p(x)| ⩽e∥p∥sup.
Proof. Without loss of generality, take ∥p∥sup = 1. By Proposition 2.4 in Sachadeva and Vish-
noi [SV+14], and basic properties of Chebyshev polynomials,
sup
x∈[−1−δ,1+δ]
|p(x)| ⩽
sup
x∈[−1−δ,1+δ]
|Td(x)| = Td(1 + δ).
Further, by Proposition 2.5 in [SV+14], we can evaluate Td(1 + δ) via the formula
Td(x) = 1
2

x +
p
x2 −1
d
+ 1
2

x −
p
x2 −1
d
Td(1 + δ) = 1
2

1 + δ +
p
2δ + δ2
d
+ 1
2

1 + δ −
p
2δ + δ2
d
⩽exp

d(δ +
p
2δ + δ2)

⩽exp

1
4d +
q
1
2 +
1
16d2

⩽e
5.3
Sampling and query access
We now introduce the “quantum-inspired” access model, following prior exposition [CGL+20a].
We refer the reader there for a more thorough investigation of this access model. From a
sketching perspective, this model encompasses “the set of algorithms that can be performed
in time independent of input dimension, using only ℓ2
2 sampling”, and is a decent classical
analogue for the input given to a quantum machine learning algorithms operating on classical
data.
Deﬁnition 5.5 (Sampling and query access to a vector, [CGL+20a, Deﬁnition 3.2]). For a vector
v ∈Cn, we have SQ(v), sampling and query access to v, if we can:
1. query for entries of v;
2. obtain independent samples i ∈[n] where we see i with probability |v(i)|2/∥v∥2;
3. query for ∥v∥.
These samples are called something like “ℓ2
2 importance samples” in the randomized numerical
linear algebra literature. Quantumly, these can be considered to be simulated measurements of
the quantum state |v⟩:=
1
∥v∥∑vi|i⟩in the computational basis. Sampling and query access is
closed under taking linear combinations, once we introduce slack in the form of oversampling.
Deﬁnition 5.6 (Oversampling and query access, [CGL+20a, Deﬁnition 3.4]). For v ∈Cn and
φ ⩾1, we have SQφ(v), φ-oversampling and query access to v, if we have the ability to query v
and SQ( ˜v) for ˜v ∈Cn a vector satisfying ∥˜v∥2 = φ∥v∥2 and | ˜v(i)|2 ⩾|v(i)|2 for all i ∈[n].
16

The ℓ2
2 distribution over ˜v φ-oversamples the distribution over v:
| ˜vi|2
∥˜v∥2 = | ˜vi|2
φ∥v∥2 ⩾1
φ
|vi|2
∥v∥2 .
Intuitively speaking, estimators that use Dv can also use D ˜v via rejection sampling at the expense
of a factor φ increase in the number of utilized samples. From this observation we can prove
that oversampling access implies an approximate version of the usual sampling access:
Lemma 5.7 (Oversampling to sampling, [CGL+20a, Lemma 3.5]). Suppose we are given SQφ(v)
and some δ ∈(0, 1]. We can sample from Dv with probability ⩾1 −δ in O
 φ log 1
δ

queries to SQφ(v).
We can also estimate ∥v∥to ν multiplicative error for ν ∈(0, 1] with probability ⩾1 −δ in O

φ
ν2 log 1
δ

queries. Both of these algorithms take linear time in the number of queries.
Generally, compared to a quantum algorithm that can output (and measure) a desired vector
|v⟩, our algorithms will output SQφ(u) such that ∥u −v∥is small. So, to dequantize a quantum
algorithm, we will need to bound φ to show that we can output samples from |v⟩. As for error,
bounds on ∥u −v∥imply that measurements from u and v follow distributions that are close
in total variation distance [Tan19, Lemma 4.1]. Now, we show that oversampling and query
access of vectors is closed under taking small linear combinations.
Lemma 5.8 (Linear combinations, [CGL+20a, Lemma 3.6]). Given SQϕt(vt) ∈Cn and λt ∈C
for all t ∈[τ], we have SQφ(∑τ
t=1 λtvt) for φ = τ ∑ϕt∥λtvt∥2
∥∑λtvt∥2 . After paying the pre-processing cost of
querying for each of the norms of the ˜vt’s, the cost of any query is equal to the cost of sampling from any
of the vt’s plus the cost of querying an entry from all of the vt’s.
Deﬁnition 5.9 (Oversampling and query access to a matrix, [CGL+20a, Deﬁnition 3.7]). For a
matrix A ∈Cm×n, we have SQ(A) if we have SQ(A(i, ·)) for all i ∈[m] and SQ(a) for a ∈Rm
the vector of row norms (a(i):=∥A(i, ·)∥).
We have SQφ(A) if we have Q(A) and SQ( ˜A) for ˜A ∈Cm×n satisfying ∥˜A∥2
F = φ∥A∥2
F and
 ˜A(i, j)
2 ⩾|A(i, j)|2 for all (i, j) ∈[m] × [n].
Remark 5.10. QML algorithms achieve their largest speedups over classical algorithms when
given state preparation in polylog(mn) time. So, we concern ourselves with this setting.
We can get SQ access to input matrices and vectors in input-sparsity time. Given v ∈Cn in the
standard RAM model, the alias method [Vos91] takes Θ(nnz(v)) pre-processing time to output
a data structure that uses Θ(nnz(v)) space and can sample from v in Θ(1) time. In other words,
we can get SQ(v) with constant-time queries in O(nnz(v)) time, and by extension, for a matrix
A ∈Cm×n, SQ(A) with constant-time queries in O(nnz(v)) time.11.
Therefore, the quantum-inspired setting can be directly translated to a basic randomized
numerical linear algebra algorithm. More precisely, with this data structure, a fast quantum-
inspired algorithm (say, one running in time O(T sq(A)) for T independent of input size)
implies an algorithm in the standard computational model (running in O(nnz(A) + T) time).
Corollary 5.11. Suppose we are given sampling and query access to a matrix A ∈Cm×n and a vector
b ∈Cn, where we can respond to queries in O(1) time. Further suppose we have a vector u ∈Cn
11This holds in the word-RAM model, with an additional log overhead when considering bit complexity.
17

implicitly represented by v ∈Cm and η, with u = A†v + ηb. Then by Lemma 5.8, we have SQφ(u) for
φ = (∥v∥0 + 1)∑k ∥vkAk∥2 + ∥ηb∥2
∥u∥2
and a query cost of O(∥v∥0). In particular, by Lemma 5.7 we can draw one sample from u with probability
⩾1 −δ in O(∥v∥0φ log 1
δ) time.
This factor is only large when the linear combination has signiﬁcantly smaller norm than the
components vt in the sum suggest. Usually, in our applications, we can intuitively think about
this overhead being small when the desired output vector mostly lies in a subspace spanned
by singular vectors with large singular values in our low-rank input. Quantum algorithms
also have the same kind of overhead. Namely, the QSVT framework encodes this in the
subnormalization constant α of block-encodings, and the overhead from the subnormalization
appears during post-selection [GSLW19]. When this cancellation is not too large, the resulting
overhead typically does not affect too badly the runtime of our applications.
6
Extending the Sketching Toolkit
In this section, we show how to extend the modern sketching toolkit (see e.g. [Woo14]) in two
ways: (a) we provide a sub-sampling sketch that preserves bi-linear forms with only a inverse
quadratic dependence on ε and (b) a non-oblivious, ℓ2
2 sampling based asymmetric approximate
matrix product sketch.
6.1
The Bi-Linear Entry-wise Sampling Transform
Deﬁnition 6.1 (Bi-linear Entry-wise Sparsifying Transform). For a matrix A ∈Cm×n, the BEST
of A with parameter T is a matrix sampled as follows: for all k ∈[T],
M(k) = 1
pi,j
Ai,jeie†
j
with probability pi,j =
Ai,j

∥A∥2
F
Then,
BEST(A) = 1
T ∑
k∈[T]
M(k).
Lemma 6.2 (Basic Properties of the Bi-Linear Entry-wise Sparsifying Transform). For a matrix
A ∈Cm×n, let M = BEST(A) with parameter T. Then, for X ∈Cm×m, u ∈Cm and v ∈Cn, we have
nnz(M) ⩽T
(10)
E [M] = A
(11)
E
h
M†XM −A†XA
i
= 1
T

Tr(X)∥A∥2
FI −A†XA

(12)
E

u†Mv −E
h
u†Mv
i2
= 1
T

∥A∥2
F∥u∥2∥v∥2 −(u†Av)2
(13)
Proof. Observe, since M is an average of T sub-samples, each of which are 1-sparse, M has at
most T non-zero entries. Next,
E [M] = 1
T ∑
k∈T
E
h
M(k)i
= ∑
i∈[m] ∑
j∈[n]
pi,j
Ai,j
pi,j
eie†
j = A
(14)
18

Similarly,
E
h
M†XM
i
= 1
T2 E


 
∑
k∈[T]
M(k)
!†
X
 
∑
k∈[T]
M(k)
!

= 1
T2 E
" 
∑
k,k′∈[T]

M(k)†
XM(k′)
!#
= 1
T2
  
∑
k̸=k′∈[T]
E
h
M(k)i†
· X · E
h
M(k′)i!
+
 
∑
k∈[T]
E

M(k)†
XM(k)
!!
=

1 −1
T

A†XA + 1
T
∑
i∈[m],j∈[n]
pi,j
A2
i,j
p2
i,j
eje†
i Xeie†
j
=

1 −1
T

A†XA + ∥A∥2
F
T
∑
i∈[m],j∈[n]
Xi,ieje†
j
=

1 −1
T

A†XA + ∥A∥2
F Tr(X)
T
I.
(15)
Finally, observe, for i, i′ ∈[m] and j, j′ ∈[n],
M(k)
i,j M(k)
i′,j′ =
(
M2
i,j
if (i, j) = (i′, j′)
0
otherwise
,
and therefore,
E

u†Mv −E
h
u†Mv
i2
= E

u†(M −A)v
2
= E


∑
i,i′∈[m],j,j′∈[n]
(M −A)i,j(M −A)i′,j′uivjui′vj′


= E


∑
i∈[m],j∈[n]
(M)2
i,ju2
i v2
j −

u†Av
2


= 1
T

∥A∥2
F∥u∥2∥v∥2 −

u†Av
2
(16)
We list a simple consequence of these bounds that we use later.
Corollary 6.3. For a matrix A ∈Cm×n, let M = BEST(A) with parameter T. Then, for matrices
X ∈Cℓ×m and Y ∈Cn×d,
Pr
h
∥XMY∥F ⩾∥XAY∥F + ∥X∥F∥A∥F∥Y∥F
√
δT
i
⩽δ
19

6.2
Approximate Matrix Product via ℓ2
2 Sampling
In this subsection, we extend the well-known approximate matrix product (see for instance
[Woo14]) to the setting where we have an ℓ2
2-sampling oracle. Typically, the approximate
matrix product guarantee is achieved in the oblivious sketching model, however, we cannot
extend the oblivious sketching to the quantum setting. Quantum-inspired algorithms follows
row subsampling literature, which can be performed in this setting. Formally, we show the
following:
Deﬁnition 6.4. Given two matrices A ∈Cm×n and B ∈Cn×d, along with a probability distribu-
tion p ∈Rn
⩾0, we deﬁne the Asymmetric Approximate Matrix Product of sketch size s, denoted
AMPs(A, B, p), to be the n × s matrix whose columns are i.i.d. sampled according to the law
[AMPs(A, B, p)]∗,j =
ek
√s · pk
with probability pk
For an S = AMPs(A, B, p), we will typically consider the expression ASS†B, which can be
written as the sum of independent rank-one matrices
1
s·pk A∗,kBk,∗.
Theorem 6.5 (Asymmetric Approximate Matrix Multiplication). Given matrices A ∈Rm×n
and B ∈Rn×d such that ∥A∥= 1, ∥B∥= 1, let AMP(A, B) ∈Rm×d denote a sketch, where
AMP(A, B) = 1
k ∑i∈[k] xi ⊗yi such that xi ⊗yi = 1
pi ai ⊗bi with probability pi ⩾c∥ai∥2/(2∥A∥2
F) +
∥bi∥2/(2∥B∥2
F), for a ﬁxed constant c. Then, with probability at least 1 −δ
∥˜
AB −AB∥⩽
s
c′ log(n) log(1/δ)
 ∥A∥2
F + ∥B∥2
F

k
,
for a ﬁxed universal constant c′.
To obtain this result, we prove the following key lemma:
Lemma 6.6 (Concentration of Asymmetric Random Outer Products). Let (x, y) be a tuple of
random vectors, where x ∈Rm and y ∈Rd such that

max
i∈[n]∥xi∥2E
h
yy⊤i + max
i∈[n]∥yi∥2E
h
xx⊤i

⩽M2.
Let {(xi, yi)}i∈[n] be independent copies of the random variable (x, y). Then, for any t ∈(0, 1),
Pr
"
1
n ∑
i∈[n]
xi ⊗yi −E [x ⊗y]
 ⩾t
#
⩽(m + d) exp
 
−
ct2n
M2 + maxi∈[n]∥xi ⊗yi∥t
!
Proof. For i ∈[n], let Zi = 1
n(xi ⊗yi −E [x ⊗y]). Further, ∥Zi∥⩽2
n∥xi ⊗yi∥⩽2M
n . Next, we
bound the variance :
σ2 = max







 ∑
i∈[n]
E
h
ZiZ⊤
i
i
|
{z
}
(i)
,
 ∑
i∈[n]
E
h
Z⊤
i Zi
i
|
{z
}
(ii)







20

We bound term (i) as follows:
 ∑
i∈[n]
E
h
ZiZ⊤
i
i =

1
n E

xiy⊤
i −E [x ⊗y]

xiy⊤
i −E [x ⊗y]
⊤
⩽2
n
E
h
∥yi∥2xix⊤
i
i
⩽
maxi∈[n]∥yi∥2
n
E
h
xx⊤i
(17)
We bound term (ii) as follows:
 ∑
i∈[n]
E
h
Z⊤
i Zi
i =

1
n E

xiy⊤
i −E [x ⊗y]
⊤
xiy⊤
i −E [x ⊗y]

⩽2
n
E
h
∥xi∥2yiy⊤
i
i
⩽
maxi∈[n]∥xi∥2
n
E
h
yy⊤i
(18)
Let M2 ⩾

maxi∈[n]∥xi∥2E

yy⊤ + maxi∈[n]∥yi∥2E

xx⊤

. Applying Matrix Bernstein
(see Fact 6.7),
Pr
" ∑
i∈[n]
Zi
 ⩾t
#
= Pr
"
1
n ∑
i∈[n]
xi ⊗yi −E [x ⊗y]
 ⩾t
#
⩽(m + d) exp
 
−
ct2n
M2 + maxi∈[n]∥xi ⊗yi∥t
!
,
for a ﬁxed constant c ⩾1.
Fact 6.7 (Matrix Bernstein, Theorem 1.6 [Tro12]). Given a sequence of independent d1 × d2 random
matrices { Zi }i∈[k] such that for all i ∈[k], E [Zi] = 0 and ∥Zi∥⩽L almost surely, let
σ2 = max
  ∑
i∈[k]
E
h
ZiZ⊤
i
i,
 ∑
i∈[k]
E
h
Z⊤
i Zi
i
!
.
Then, for any t ⩾0,
Pr
" ∑
i∈[k]
Zi
 ⩾t
#
⩽(d1 + d2) exp

−
t2/2
σ2 + Lt/3

It is now straight-forward to prove Theorem 6.5 using the aforementioned lemma:
Proof of Theorem 6.5. Setting xi =
1
√pi · ai and yi =
1
√pi · bi, where pi =
∥ai∥2
2∥A∥2
F + ∥bi∥2
2∥B∥2
F , we have
∥xi∥⩽2∥A∥F
∥yi∥⩽2∥B∥F
∥xi ⊗yi∥1/2 ⩽

∥A∥F∥B∥F
∥ai∥∥bi∥ai ⊗bi

1/2
⩽
q
∥A∥F · ∥B∥F.
21

Further,
E [x ⊗x] = ∑
i∈[n]
1
2
 
∥ai∥2
∥A∥2
F
+ ∥bi∥2
∥B∥2
F
!
· ai ⊗ai
pi
= AA⊤,
(19)
and similarly
E [y ⊗y] = ∑
i∈[n]
1
2
 
∥ai∥2
∥A∥2
F
+ ∥bi∥2
∥B∥2
F
!
· bi ⊗bi
pi
bi ⊗bi
= BB⊤.
(20)
Observe, setting M2 = 2

∥A∥2
F + ∥B∥2
F

sufﬁces, and applying Matrix Bernstein, we have
Pr
AB −˜
AB
 ⩾t
 ⩽(m + d) exp
 
−
ct2k
∥A∥2
F + ∥B∥2
F
!
.
(21)
Setting t =
r
c log(1/δ) log(m+d)(∥A∥2
F+∥B∥2
F)
k
, we know that with probability at least 1 −δ
AB −˜
AB
 ⩽
v
u
u
tc log(1/δ) log(m + d)

∥A∥2
F + ∥B∥2
F

k
,
as desired.
7
Sums of Chebyshev coefﬁcients
To give improved stability bounds for the Clenshaw recurrence, we need to bound various sums
of Chebyshev coefﬁcients. Since we aim to give bounds that hold for all degree-d polynomials,
we use no property of the function beyond that it has a unique Chebyshev expansion; of course,
for any particular choice of function f, the bounds in this section can be improved by explicitly
computing its Chebyshev coefﬁcients, or in some cases, by using smoothness properties of the
function [Tre19, Theorems 7.2 and 8.2].
Let f : [−1, 1] →R be a Lipschitz continuous function. Then it can be expressed uniquely
as a linear combination of Chebyshev polynomials f (x) = ∑∞
i=0 aiTi(x). A broad topic of
interest in approximation theory is bounds for linear combinations of these coefﬁcients, ∑aici,
in terms of ∥f ∥sup; this was one motivation of Vladimir Markov in proving the Markov brothers’
inequality [Sch41, p575]. Our goal for this section will be to investigate this question in the
case where these sums are arithmetic progressions of step four. This will be necessary for later
stability analyses, and is one of the ﬁrst non-trivial progressions to bound. We begin with some
straightforward assertions (see [Tre19] for background).
Fact 7.1. Let f : [−1, 1] →R be a Lipschitz continuous function. Then its Chebyshev coefﬁcients {aℓ}ℓ
22

satisfy
∑
ℓ
aℓ
 = | f (1)| ⩽∥f ∥sup
∑
ℓ
(−1)ℓaℓ
 = | f (−1)| ⩽∥f ∥sup
∑
ℓ
aℓJℓis evenK
 =
∑
ℓ
aℓ
1
2(1 + (−1)ℓ)
 ⩽∥f ∥sup
∑
ℓ
aℓJℓis oddK
 =
∑
ℓ
aℓ
1
2(1 −(−1)ℓ)
 ⩽∥f ∥sup
We use the following result on Lebesgue constants to bound truncations of the Chebyshev
coefﬁcient sums.
Lemma 7.2 ([Tre19, Theorem 15.3]). Let f : [−1, 1] →R be a Lipschitz continuous function, let
fk(x) = ∑k
ℓ=0 aℓTℓ(x), and let optimal degree-k approximating polynomial to f be denoted f ∗
k . Then
∥f −fk∥sup ⩽

4 + 4
π2 log(k + 1)

∥f −f ∗
k ∥sup
⩽

4 + 4
π2 log(k + 1)

∥f ∥sup.
Similarly,
∥fk∥sup ⩽∥f −fk∥sup + ∥f ∥sup ⩽

5 + 4
π2 log(k + 1)

∥f ∥sup.
This implies bounds on sums of coefﬁcients.
Fact 7.3. Consider a function f (x) = ∑ℓaℓTℓ(x). Then

∞
∑
ℓ=k
aℓJℓ−k is evenK
 ⩽∥f −fk−2∥sup ⩽

4 + 4
π2 log(k −1)

∥f ∥sup,
where the inequalities follow from Fact 7.1 and Lemma 7.2. When k = 0, 1, then the sum is bounded by
∥f ∥sup, as shown in Fact 7.1.
Now, we prove similar bounds in the case that f (x) is an odd function. In particular, we want
to obtain a bound on alternating signed sums of the Chebshyev coefﬁcients and we incur a
blowup that scales logarithmically in the degree.
Lemma 7.4. Let f : [−1, 1] →R be an odd Lipschitz continuous function with Chebyshev coefﬁcients
{aℓ}ℓ, so that ak = 0 for all even k. Then the Chebyshev coefﬁcient sum is bounded as

d
∑
ℓ=0
(−1)ℓa2ℓ+1
 ⩽(ln(d) + 2)
max
0⩽k⩽2d+1∥fk∥sup
⩽(ln(d) + 2)

5 + 4
π2 ln(2d + 2)

∥f ∥sup
⩽

16 + 4 ln2(d + 1)

∥f ∥sup.
We ﬁrst state the following relatively straight-forward corollary:
23

Corollary 7.5. Lemma 7.4 gives bounds on arithmetic progressions with step size four. Let f : [−1, 1] →
R be a Lipschitz continuous function, and consider nonnegative integers c ⩽d. Then

d
∑
ℓ=c
aℓJℓ−c ≡0 (mod 4)K
 ⩽(32 + 8 ln2(d + 1))∥f ∥sup
Proof. Deﬁne f odd := 1
2( f (x) −f (−x)) and f even := 1
2( f (x) + f (−x)) to be the odd and even
parts of f respectively. Triangle inequality implies that ∥f odd∥sup, ∥f even∥sup ⩽∥f ∥sup. Suppose
c, d are odd. Then

d
∑
ℓ=c
aℓJℓ−c ≡0 (mod 4)K
 = 1
2

⌊(d−c)/2⌋
∑
ℓ=0
ac+2ℓ(1 ± (−1)ℓ)

⩽1
2

⌊(d−c)/2⌋
∑
ℓ=0
ac+2ℓ
 +

⌊(d−c)/2⌋
∑
ℓ=0
(−1)ℓac+2ℓ


⩽1
2

∥f odd
c
∥sup + ∥f odd
d
∥sup + 2(ln(d) + 2) max
0⩽k⩽d∥f odd
k
∥sup

⩽(32 + 8 ln2(d + 1))∥f odd∥sup
⩽(32 + 8 ln2(d + 1))∥f ∥sup
The case when c is even is easy, and it follows from Fact 7.3: for , we know that
∑
ℓ
a2ℓTℓ(x)

sup =
∑
ℓ
a2ℓTℓ(T2(x))

sup =
∑
ℓ
a2ℓT2ℓ(x)

sup =
 f even(x)

sup ⩽∥f ∥sup,
so
∑
ℓ⩾c
aℓJℓ−c ≡0 (mod 4)K
 =
 ∑
ℓ⩾c/2
a2ℓJℓ−c/2 is evenK

⩽

4 + 4
π2 log(c/2 −1)
∑
ℓ
a2ℓTℓ(x)

sup
⩽

4 + 4
π2 log(c/2 −1)

∥f ∥sup,
and combining these two bounds for c and d gives the desired statement.
We note that Lemma 7.4 will be signiﬁcantly harder to prove. See Remark 7.8 for an intuitive
explanation why. We begin with two structural lemmas on how the solution to a unitriangular
linear system behaves, which might be on independent interest.
Lemma 7.6 (An entry-wise positive solution). Suppose that A ∈Rd×d is an upper unitriangular
matrix such that, for all i ⩽j, aij > 0, aij > ai−1,j. Then A−1⃗1 is a vector with positive entries.
The same result holds when A is a lower unitriangular matrix such that, for all i ⩾j, aij > 0, aij > ai+1,j.
Proof. Let x = A−1⃗1. Then xd = 1 ⩾0. The result follows by induction:
xi = 1 −
d
∑
j=i+1
Aijxj
=
d
∑
j=i+1
(Ai+1,j −Aij)xj + 1 −
d
∑
j=i+1
Ai+1,jxj
24

=
d
∑
j=i+1
(Ai+1,j −Aij)xj + 1 −[Ax]i+1
=
d
∑
j=i+1
(Ai+1,j −Aij)xj
> 0
For lower unitriangular matrices, the same argument follows. The inverse satisﬁes x1 = 1 and
xi = 1 −
i−1
∑
j=1
Aijxj
=
d
∑
j=i+1
(Ai−1,j −Aij)xj + 1 −
d
∑
j=i+1
Ai−1,jxj > 0
Next, we characterize how the solution to a unitriangular linear system behaves when we
consider a partial ordering on the matrices.
Lemma 7.7. Let A be a nonnegative upper unitriangular matrix such that Aij > Ai−1,j and Aij >
Ai,j+1 for all i ⩽j. Let B be a matrix with the same properties, such that A ⩾B entrywise. By Lemma 7.6,
x(A) = A−1⃗1 and x(B) = B−1⃗1 are nonnegative. It further holds that ∑d
i=1[A−1⃗1]i ⩽∑d
i=1[B−1⃗1]i.
Proof. We consider the line between A and B, A(t) = A(1 −t) + Bt for t ∈[0, 1]. Let x(t) =
A−1⃗1; we will prove that⃗1Tx(t) is monotonically increasing in t. The gradient of x(t) has a
simple form [Tao13]:
A(t)x(t) =⃗1
∂[A(t)x(t)] = ∂t[⃗1]
(B −A)x(t) + A(t)∂tx(t) = 0
∂tx(t) = A−1(t)(A −B)x(t).
So,
⃗1T∂tx(t) =⃗1TA−1(t)(A −B)A−1(t)⃗1
= [A−T(t)⃗1]T(A −B)[A−1(t)⃗1].
Since A and B satisfy the entry constraints, so do every matrix along the line. Consequently, the
column constraints in Lemma 7.6 are satisﬁed for both A and AT, so both A−T(t)⃗1 and A−1(t)⃗1
are positive vectors. Since A ⩾B entrywise, this means that⃗1T∂tx(t) is positive, as desired.
Proof of Lemma 7.4. We ﬁrst observe that the following sorts of sums are bounded. Let xk :=
cos( π
2 (1 −
1
2k+1)). Then, using that Tℓ(cos(x)) = cos(ℓx),
f2k+1(xk) =
2k+1
∑
ℓ=0
aℓTℓ(xk)
=
k
∑
ℓ=0
a2ℓ+1T2ℓ+1(xk)
25

=
k
∑
ℓ=0
a2ℓ+1 cos
π
2

2ℓ+ 1 −2ℓ+ 1
2k + 1

=
k
∑
ℓ=0
(−1)ℓa2ℓ+1 sin
π
2
2ℓ+ 1
2k + 1

.
We have just shown that

k
∑
ℓ=0
a2ℓ+1(−1)ℓsin
π
2
2ℓ+ 1
2k + 1
 ⩽∥f2k+1∥sup.
(22)
We now claim that there exist non-negative ck for k ∈{0, 1, . . . , d} such that
d
∑
ℓ=0
(−1)ℓa2ℓ+1 =
d
∑
k=0
ck f2k+1(xk).
(23)
The f2k+1(xk)’s can be bounded using Lemma 7.2. The rest of the proof will consist of showing
that the ck’s exist, and then bounding them.
To do this, we consider the coefﬁcient of each a2ℓ+1 separately; let A(k) ∈[0, 1]d+1 (index starting
at zero) be the vector of coefﬁcients associated with p2k+1(xk):
A(k)
ℓ
= sin
π
2
2ℓ+ 1
2k + 1

for 0 ⩽ℓ⩽k, 0 otherwise
(24)
Note that the A(k)
ℓ
is always non-negative and increasing with ℓup to A(k)
k
= 1. Then Eq. (23)
holds if and only if
c0A(0) + · · · + cdA(d) =⃗1,
or in other words, the equation Ac = ⃗1 is satisﬁed, where A is the matrix with columns A(k)
and c is the vector of cℓ’s. Since A is upper triangular (in fact, with unit diagonal), this can be
solved via backwards substitution: cd = 1, then cd−1 can be deduced from cd, and so on. More
formally, the sth row gives the following constraint that can be rewritten as a recurrence.
d
∑
t=s
sin
π
2
2s + 1
2t + 1

ct = 1
(25)
cs = 1 −
d
∑
t=s+1
sin
π
2
2s + 1
2t + 1

ct
(26)
Because the entries of A increase in ℓ, the cℓ’s are all positive.
Invoking Lemma 7.6 with the matrix A establishes that such cs exist; our goal now is to bound
them. Doing so is not as straightforward as it might appear: since the recurrence Eq. (26)
subtracts by ct’s, an upper bound on ct for t ∈[s + 1, d] does not give an upper bound on cs; it
gives a lower bound. So, an induction argument to show bounds for cs’s fails. Further, we were
unable to ﬁnd any closed form for this recurrence. However, since all we need to know is the
sum of the cs’s, we show that we can bound this via a generic upper bound on the recurrence.
Here, we apply Lemma 7.7 to A as previously deﬁned, and the bounding matrix is (for i ⩽j)
Bij = i
j ⩽2i + 1
2j + 1 ⩽sin
π
2
2i + 1
2j + 1

= Aij,
26

using that sin( π
2 x) ⩾x for x ∈[0, 1]. Let ˆc = B−1⃗1. Then ˆci =
1
i+1 for i ̸= d and ˆcd = 1.
[Bˆc]i =
d
∑
j=i
Bij ˆcj =
d−1
∑
j=i
i
j
1
j + 1 + i
d = i
d−1
∑
j=i
1
j −
1
j + 1

+ i
d = i
1
i −1
d) + i
d = 1
By Lemma 7.7, ∑i ci ⩽∑i ˆci ⩽ln(d) + 2. So, altogether, we have

d
∑
ℓ=0
(−1)ℓa2ℓ+1
 =

d
∑
k=0
ck f2k+1(xk)

⩽
d
∑
k=0
ck∥f2k+1∥sup
⩽

d
∑
k=0
ck

max
0⩽k⩽d∥f2k+1∥sup
=

d
∑
k=0
ck

max
0⩽k⩽2d+1∥fk∥sup
⩽(ln(d) + 2)
max
0⩽k⩽2d+1∥fk∥sup
Remark 7.8. A curious reader will (rightly) wonder whether this proof requires this level of
difﬁculty. Intuition from the similar Fourier analysis setting suggests that arithmetic progres-
sions of any step size at any offset are easily bounded. We can lift to the Fourier setting by
considering, for an f : [−1, 1] →R, a corresponding 2π-periodic g : [0, 2π] →R such that
g(θ) := f (cos(θ)) =
∞
∑
k=0
akTk(cos(θ)) =
∞
∑
k=0
ak cos(kθ) =
∞
∑
k=0
ak
eikθ + e−ikθ
2
This function has the property that |g(θ)| ⩽∥f ∥sup and bg(k) = a|k|/2 (except bg(0) = a0).
Consequently,
1
t
t−1
∑
j=0
f

cos(2πj
t )

= 1
t
t−1
∑
j=0
g
2πj
t

= 1
t
t−1
∑
j=0
∞
∑
k=−∞
bg(k)e2πijk/t =
∞
∑
k=−∞
bg(k)
t−1
∑
j=0
1
t e2πijk/t
=
∞
∑
k=−∞
bg(k)Jk is divisible by tK =
∞
∑
k=−∞
bg(kt),
so we can bound arithmetic progressions |∑k bg(kt)| ⩽∥f ∥sup, and this generalizes to other
offsets, to bound |∑k bg(kt + o)| for some o ∈[t −1]. Notably, though, this approach does not
say anything about sums like ∑k a4k+1. The corresponding progression of Fourier coefﬁcients
doesn’t give it, for example, since we pick up unwanted terms from the negative Fourier
coefﬁcients.12
∑
k
bg(4k + 1) = (bg(1) + bg(5) + bg(9) + · · ·) + (bg(−3) + bg(−7) + bg(−11) + · · ·)
= 1
2(a1 + a5 + a9 + · · ·) + 1
2(a3 + a7 + a11 + · · ·) = ∑
k⩾0
a2k+1.
12These sums are related to the Chebyshev coefﬁcients one gets from interpolating a function at Chebyshev points
[Tre19, Theorem 4.2].
27

In fact, by inspection of the distribution13 D(x) = ∑∞
k=0 T4k+1(x), it appears that this arithmetic
progression cannot be written as a linear combination of evaluations of f (x). Since the shape
of the distribution appears to have 1/x behavior near x = 0, we conjecture that our analysis
losing a log factor is, in some respect, necessary.
Conjecture 7.9. For any step size t > 1 and offset o ∈[t −1] such that o ̸= t/2, there exists a function
f : [−1, 1] →R such that ∥f ∥sup = 1 but |∑n
k=0 atk+o| = Ω(log(n)).
8
Properties of the Clenshaw recursion
8.1
Deriving the Clenshaw recursions
Suppose we are given as input a degree-d polynomial as a linear combination of Chebyshev
polynomials:
p(x) =
d
∑
k=0
akTk(x).
(27)
Then this can be computed with the Clenshaw algorithm, which gives the following recurrence.
qd+1 = qd+2 = 0
qk = 2xqk+1 −qk+2 + ak
(Clenshaw)
˜p = 1
2(a0 + q0 −q2)
Lemma 8.1. The recursion in Eq. (Clenshaw) computes p(x). That is, in exact arithmetic, ˜p = p(x).
In particular,
qk =
d
∑
i=k
aiUi−k(x).
(28)
Proof. We show Eq. (28) by induction.
qk = 2xqk+1 −qk+2 + ak
= 2x

d
∑
i=k+1
aiUi−k−1(x)

−

d
∑
i=k+2
aiUi−k−2(x)

+ ak
= ak + 2xak+1U0(x) +
d
∑
i=k+2
ai(2xUi−k−1(x) −Ui−k−2(x))
=
d
∑
i=k
aiUi−k(x).
(29)
13This is the functional to integrate against to compute the sum,
2
π
R 1
−1 f (x)D(x)/
√
1 −x2 = ∑a4k+1. The
distribution is not a function, but can be thought of as the limit object of Dn(x) = ∑n
k=0 T4k+1(x) as n →∞,
analogous to Dirichlet kernels and the Dirac delta distribution.
28

Consequently, we have
1
2(a0 + u0 −u2) = 1
2

a0 +
d
∑
i=0
aiUi(x) −
d
∑
i=2
aiUi−2(x)

= a0 + a1x +
d
∑
i=2
ai
2 (Ui(x) −Ui−2(x))
=
d
∑
i=0
aiTi(x).
Remark 8.2. Though the aforementioned discussion is specialized to the scalar setting, it
extends to the the matrix setting almost entirely syntactically: consider a Hermitian A ∈Cn×n
and b ∈Cn with ∥A∥, ∥b∥⩽1. Then p(A)b can be computed in the following way:
ud+1 =⃗0
ud = adb
uk = 2Auk+1 −uk+2 + akb
u := p(A)b = 1
2(a0b + u0 −u2)
(30)
The proof that this truly computes p(A)b is the same as the proof of correctness for Clenshaw’s
algorithm shown above.
8.2
Evaluating even and odd polynomials
We will be considering evaluating odd and even polynomials. We again focus on the scalar
setting and note that this extends to the matrix setting in the obvious way. The previous
recurrence Eq. (Clenshaw) can work in this setting, but it’ll be helpful for our analysis if the
recursion multiplies by x2 each time, instead of x [MH02, Chapter 2, Problem 7]. So, in the case
where the degree-(2d + 1) polynomial p(x) is odd (so a2k = 0 for every k), it can be computed
with the iteration
qd+1 = qd+2 = 0
qk = 2T2(x)qk+1 −qk+2 + a2k+1U1(x)
(Odd Clenshaw)
˜p = 1
2(q0 −q1)
When p(x) is a degree-(2d) even polynomial (so a2k+1 = 0 for every k), it can be computed via
the same recurrence, replacing a2k+1U1(x) with a2k. However, we will use an alternative form
that’s more convenient for us (since we can reuse the analysis of the odd case).
˜a2k := a2k −a2k+2 + a2k+4 −· · · ± a2d
(31)
qd+1 = qd+2 = 0
qk = 2T2(x)qk+1 −qk+2 + ˜a2k+2U1(x)2
(Even Clenshaw)
˜p = ˜a0 + 1
2(q0 −q1)
These recurrences correctly compute p follows from a similar analysis to the standard Clenshaw
algorithm, formalized below.
29

Lemma 8.3. The recursions in Eq. (Odd Clenshaw) and Eq. (Even Clenshaw) correctly compute
p(x) for even and odd polynomials, respectively. That is, in exact arithmetic, ˜p = p(x). In particular,
qk =
d
∑
i=k
aiUi−k(x).
(32)
Proof. We can prove these statements by applying Eq. (28). In the odd case, Eq. (Odd Clenshaw)
is identical to Eq. (Clenshaw) except that x is replaced by T2(x) and ak is replaced by a2k+1U1(x),
so by making the corresponding changes in the iterate, we get that
qk =
d
∑
i=k
a2i+1U1(x)Ui−k(T2(x)) =
d
∑
i=k
a2i+1U2(i−k)+1(x)
by Eq. (8)
˜p = 1
2(q0 −q1) =
d
∑
i=0
a2i+1
2

U2i+1(x) −U2i−1(x)

= p(x).
by Eq. (5)
Similarly, in the even case, Eq. (Even Clenshaw) is identical to Eq. (Clenshaw) except that x is
replaced by T2(x) and ak is replaced by 4˜a2kx2 (see Deﬁnition 31), so that
qk =
d
∑
i=k
˜a2i+2U1(x)2Ui−k(T2(x))
(33)
=
d
∑
i=k
˜a2i+2U1(x)U2(i−k)+1(x)
by Eq. (8)
=
d
∑
i=k
˜a2i+2(U2(i−k)(x) + U2(i−k+1)(x))
by Eq. (4)
=
d+1
∑
i=k
˜a2i+2U2(i−k)(x) +
d+1
∑
i=k+1
˜a2iU2(i−k)(x)
noticing that ˜a2d+2 = 0
= ˜a2k+2 +
d+1
∑
i=k+1
(˜a2i + ˜a2i+2)U2(i−k)(x)
(34)
= ˜a2k+2 +
d
∑
i=k+1
a2iU2(i−k)(x)
(35)
= −˜a2k +
d
∑
i=k
a2iU2(i−k)(x)
(36)
Finally, observe
˜a0 + 1
2(q0 −q1) = ˜a0 + 1
2(a0 −˜a0 + ˜a2) +
d
∑
i=1
a2i
2 (U2i(x) −U2i−2(x)) = p(x).
(37)
9
Stability of the scalar Clenshaw recursion
Before we move to the matrix setting, we warmup with a stability analysis of the scalar Clenshaw
recursion. Suppose we perform Eq. (Clenshaw) to compute a degree-d polynomial p, except
every addition, subtraction, and multiplication incurs ε relative error. Typically, this has been
30

analyzed in the ﬁnite precision setting, where the errors are caused by truncation. These stan-
dard analyses show that this ﬁnite precision recursion gives p(x) to d2(∑|ai|)ε = O(d3∥p∥supε)
error. A use of Parseval’s formula [MH02, Theorem 5.3] ∑|ai| ⩽
√
d
q
∑a2
i = O(
√
d∥p∥sup)
reduces this by a factor of
√
d.
However, this bound on ∑|ai| is essentially tight, and moreover, it is tight for a relevant class
of polynomials: truncations of well-behaved functions. Such polynomials generally have
the property that |ak| = Θ((1 −log(1/ε)/d)−k) (Lemma 5.2), so most of the coefﬁcients are
constant-sized, eventually decaying to ε.
We improve on prior stability analyses to show that the Clenshaw recurrence for Chebyshev
polynomials only incurs an error overhead of d2 log(d). This is tight up to a logarithmic factor.
This, for example, could be used to improve the bound in [MMS18, Lemma 9] from k3 to
k2 log(k) (where in that paper, k denotes degree).
9.1
Analyzing error propagation
The following is a simple analysis of Clenshaw, with some rough resemblance to an analysis of
Oliver [Oli79].
Proposition 9.1 (Stability Analysis for Scalar Clenshaw). Consider a degree-d polynomial p :
[−1, 1] →R with Chebyshev coefﬁcients p(x) = ∑d
k=0 akTk(x). Let ⊕, ⊖, ⊙: C × C →C be binary
operations representing addition, subtraction, and multiplication to µε relative error, for 0 < ε < 1:
|(x ⊕y) −(x + y)| ⩽µε(|x| + |y|)
|(x ⊖y) −(x −y)| ⩽µε(|x| + |y|)
|x ⊙y −x · y| ⩽µε|x||y| = µε|xy|.
Given an x ∈[−1, 1], consider performing the Clenshaw recursion with these noisy operations:
˜qd+1 = ˜qd+2 = 0
˜qk = (2 ⊙x) ⊙˜qk+1 ⊖( ˜qk+2 ⊖ak)
(Finite-Precision Clenshaw)
˜˜p = 1
2 ⊙((a0 ⊕q0) ⊖q2)
Then Eq. (Finite-Precision Clenshaw) outputs p(x) up to 50ε∥p∥sup error14, provided that µ > 0
satisﬁes the following three criterion.
(a) µε ⩽
1
50(d+2)2 ;
(b) µ ∑d
i=0|ai| ⩽∥p∥sup;
(c) µ|qk| = µ|∑d
i=k aiUi−k(x)| ⩽1
d∥p∥sup for all k ∈{0, . . . , d}.
This analysis shows that arithmetic operations incurring µε error result in computing p(x) to
ε error. In particular, the stability of the scalar Clenshaw recurrence comes down to under-
standing how small we can take µ. Note that if we ignored coefﬁcient sign, |∑d
i=k aiUi−k(x)| ⩽
|∑d
i=k|ai|Ui−k(x)| = ∑d
i=k(i −k + 1)|ai|, this would require setting µ = Θ(1/d3). We show in
Section 9.2 that we can set µ = Θ((d2 log(d))−1) for all x ∈[−1, 1] and polynomials p.
Lemma 9.2. In Proposition 9.1, it sufﬁces to take µ = Θ((d2 log(d))−1).
14We did not attempt to optimize the constants for this analysis.
31

Proof of Proposition 9.1. We will expand out these ﬁnite precision arithmetic to get error intervals
for each iteration.
˜qd+1 = ˜qd+2 = 0,
(38)
and
˜qk = (2 ⊙x) ⊙˜qk+1 ⊖( ˜qk+2 ⊖ak)
= (2x ± 2µε|x|) ⊙˜qk+1 ⊖( ˜qk+2 −ak ± µε(| ˜qk+2| + |ak|))
= ((2x ˜qk+1 ± 2µε|x| ˜qk+1) ± µε|(2x ± 2µε|x|) ˜qk+1|) ⊖( ˜qk+2 −ak ± µε(| ˜qk+2| + |ak|))
∈(2x ˜qk+1 ± (2µε + µ2ε2)2|x ˜qk+1|) ⊖( ˜qk+2 −ak ± µε(| ˜qk+2| + |ak|))
∈(2x ˜qk+1 ± 6µε|x ˜qk+1|) ⊖( ˜qk+2 −ak ± µε(| ˜qk+2| + |ak|))
= 2x ˜qk+1 −˜qk+2 + ak ± µε(6|x ˜qk+1| + | ˜qk+2| + |ak|)
+ µε|2x ˜qk+1 ± 6µε|x ˜qk+1|| + µε| ˜qk+2 −ak ± µε(| ˜qk+2| + |ak|)|
∈2x ˜qk+1 −˜qk+2 + ak ± µε(14|x ˜qk+1| + 3| ˜qk+2| + 3|ak|),
(39)
and,
˜˜p = 1
2 ⊙((a0 ⊕q0) ⊖q2)
= 1
2 ⊙((a0 + q0 ± µε(|a0| + |q0|)) ⊖q2)
= 1
2 ⊙((a0 + q0 −q2 ± µε(|a0| + |q0|)) ± µε(|a0 + q0 ± µε(|a0| + |q0|)| + |q2|))
∈1
2 ⊙(a0 + q0 −q2 ± µε(3|a0| + 3|q0| + |q2|))
= 1
2(a0 + q0 −q2 ± µε(3|a0| + 3|q0| + |q2|)) ± µε 1
2|a0 + q0 −q2 ± µε(3|a0| + 3|q0| + |q2|)|
∈1
2(a0 + q0 −q2) ± 1
2µε(7|a0| + 7|q0| + 3|q2|).
(40)
To summarize, we have
˜qd+1 = ˜qd+2 = 0
˜qk = 2x ˜qk+1 −˜qk+2 + ak + δk,
where |δk| ⩽µε(14|x ˜qk+1| + 3| ˜qk+2| + 3|ak|)
(41)
˜˜p = 1
2(a0 + q0 −q2) + δ,
where |δ| ⩽1
2µε(7|a0| + 7|q0| + 3|q2|)
(42)
By Lemma 8.1, this recurrence satisﬁes
˜qk =
d
∑
i=k
Ui−k(x)(ai + δi)
qk −˜qk =
d
∑
i=k
Ui−k(x)δi
q −˜q = δ + 1
2

d
∑
i=0
Ui(x)δi −
d
∑
i=2
Ui−2(x)δi

= δ + 1
2δ0 +
d
∑
i=1
Ti(x)δi
|q −˜q| ⩽|δ| + 1
2|δ0| +
d
∑
i=1
|Ti(x)δi| ⩽|δ| +
d
∑
i=0
|δi|.
(43)
This analysis so far has been fully standard. Let’s continue bounding.
⩽µε

7
2|a0| + 7
2|q0| + 3
2|q2| +
d
∑
i=0
(14|x ˜qi+1| + 3| ˜qi+2| + 3|ai|)

32

⩽µε
d
∑
i=0
(20| ˜qi| + 10|ai|).
(44)
Now, we will bound all of the δk’s. Combining previous facts, we have
| ˜qk| =

d
∑
i=k
Ui−k(x)(ai + δi)

⩽

d
∑
i=k
Ui−k(x)ai
 +
d
∑
i=k
Ui−k(x)δi

⩽

d
∑
i=k
Ui−k(x)ai
 +
d
∑
i=k
(i −k + 1)|δi|
⩽

d
∑
i=k
Ui−k(x)ai
 + µε
d
∑
i=k
(i −k + 1)(14| ˜qi+1| + 3| ˜qi+2| + 3|ai|)
⩽
 1
µd + 3µεd −k + 1
µ

∥p∥sup + µε
d
∑
i=k
(i −k + 1)(14| ˜qi+1| + 3| ˜qi+2|)
⩽1.5
µd ∥p∥sup + µε
d
∑
i=k
(i −k + 1)(14| ˜qi+1| + 3| ˜qi+2|)
Note that | ˜qk| ⩽ck, where
cd = 0;
ck = 1.5
µd ∥p∥sup + µε
d
∑
i=k
(i −k + 1)(14ci+1 + 3ci+2)
(45)
Solving this recurrence, we have that ck ⩽
2
µd∥p∥sup, since by strong induction,
ck ⩽
1.5
µd + µε
d
∑
i=k
(i −k + 1)17 2
µd

∥p∥sup
=
1.5
µd + 17µε 1
µd(d −k + 1)(d −k + 2)

∥p∥sup ⩽2
µd∥p∥sup
(46)
Returning to Equation (44):
|q −˜q| ⩽µε
d
∑
i=0
(20| ˜qi| + 10|ai|) ⩽µε
d
∑
i=0
(20ci + 10|ai|)
(47)
⩽40ε∥p∥sup + 10µε
d
∑
i=0
|ai| ⩽50ε∥p∥sup
9.2
Bounding the iterates of the Clenshaw recurrence
The goal of this section is to prove Lemma 9.2. In particular, we wish to show that for µ =
Θ((d2 log(d))−1), the following criteria hold:
(a) µε ⩽
1
50(d+2)2 ;
(b) µ ∑d
i=0|ai| ⩽∥p∥sup;
(c) µ|qk| = µ|∑d
i=k aiUi−k(x)| ⩽1
d∥p∥sup for all k ∈{0, . . . , d}.
33

For this choice of µ, (a) is clearly satisﬁed, and since |ai| ⩽2∥p∥sup (Lemma 5.2), µ ∑d
i=0|ai| ⩽
2(d + 1)∥p∥sup ⩽∥p∥sup, so (b) is satisﬁed. In fact, both of these criterion are satisﬁed for
µ = O(1/d), provided ε is sufﬁciently small.
Showing (c) requires bounding ∥∑d
ℓ=k aℓUℓ−k(x)∥sup for all k ∈[d]. These expressions are also
the iterates of the Clenshaw algorithm (Lemma 8.1), so we are in fact trying to show that in
the process of our algorithm we never produce a value that’s much larger than the ﬁnal value.
From testing computationally, we believe that the following holds true.
Conjecture 9.3. Let p(x) be a degree-d polynomial with Chebyshev expansion p(x) = ∑d
ℓ=0 aℓTℓ(x).
Then, for all k from 0 to d,

d
∑
ℓ=k
aℓUℓ−k(x)

sup ⩽(d −k + 1)∥p∥sup,
maximized for the Chebyshev polynomial p(x) = Td(x).
Conjecture 9.3 would imply that it sufﬁces to take µ = Θ(1/d2). We prove it up to a log factor.
Theorem 9.4. For a degree-d polynomial p, consider the degree-(d −k) polynomial qk(x) = ∑d
ℓ=k aℓUℓ−k(x).
Then
∥qk∥sup ⩽(d −k + 1)

16 + 16
π2 log(d)

∥p∥sup.
Proof. We proceed by carefully bounding the Chebyshev coefﬁcients of qk, which turn out to be
arithmetic progressions of the ak’s which we bounded in Section 7.
qk(x) = ∑
i
aiUi−k(x)
= ∑
i ∑
j⩾0
aiTi−k−2j(x)(1 + Ji −k −2j ̸= 0K)
= ∑
i ∑
j⩾0
ai+k+2jTi(x)(1 + Ji ̸= 0K)
= ∑
i
Ti(x)(1 + Ji ̸= 0K) ∑
j⩾0
ai+k+2j
|qk(x)| ⩽∑
i
Ji ⩾0K(1 + Ji ̸= 0K)
∑
j⩾0
ai+k+2j

⩽2
d−k
∑
i=0
∑
j⩾0
ai+k+2j

⩽4
d−k
∑
i=0

4 + 4
π2 log(i + k −1)

∥p∥sup
by Fact 7.3
⩽(d −k + 1)

16 + 16
π2 log(d)

∥p∥sup.
Remark 9.5. We spent some time trying to prove Conjecture 9.3, since its form is tantalizingly
close to that of the Markov brothers’ inequality [Sch41] ∥d
dx p(x)∥sup = ∥∑d
ℓ=0 aℓℓUℓ−1(x)∥sup ⩽
d2∥p(x)∥sup, except with the linear differential operator
d
dx : Tℓ7→ℓUℓ−1 replaced with the
linear operator Tℓ7→Uℓ−k. However, calculations suggest that the variational characterization
of max∥p∥sup=1| d
dx p(x)| underlying proofs of the Markov brothers’ inequality [Sha04] does not
hold here, and from our shallow understanding of these proofs, it seems that they strongly use
properties of the derivative.
34

10
Computing matrix polynomials
Our goal is to prove the following theorem:
Theorem 10.1. Suppose we are given sampling and query access to A ∈Cm×n and b ∈Cn, and
suppose we are given an even or odd degree-d polynomial. Then we can output a description of a vector
y ∈Cn such that ∥y −p(A)b∥⩽ε∥p∥Spec(A) with probability ⩾0.9 in time
O

min

nnz(A), d8∥A∥4
F
ε4
log8(d) log2(n)

+ d9∥A∥4
F
ε2
log4(d) log(n)

.
We can access the output description in the following way:
(a) Compute entries of y in O
 d4∥A∥2
F
ε2
log4(d) log(n)

time;
(b) Sample i ∈[n] with probability |yi|2
∥y∥2 in O
 d6∥A∥4
F
(ε∥y∥)2 log8(d) log(n)

time;
(c) Estimate ∥y∥2 to ν relative error in O
 d6∥A∥4
F
(νε∥y∥)2 log8(d) log(n)

time.
We now extend the error analysis in the scalar setting from Section 9 to the matrix setting. For
simplicity, we treat the odd case, and in Section 10.2, describe what changes to prove the result
for even polynomials.
10.1
Computing odd matrix polynomials
The main theorem we obtain is as follows:
Theorem 10.2. Suppose we are given sampling and query access to A ∈Cm×n and b ∈Cn with
∥A∥, ∥b∥⩽1, and suppose we are given a (2d + 1)-degree odd polynomial, written in its Chebyshev
coefﬁcients as
p(x) =
d
∑
i=0
a2i+1T2i+1(x).
Then we can output a vector x ∈Cn such that ∥Ax −p(A)b∥⩽ε∥p∥Spec(A) with probability ⩾0.9 in
time
O

min

nnz(A), ∥A∥4
F
(µε)4 log2(n)

+ d5∥A∥4
F
(µε)2
log(n)

,
assuming µ satisﬁes the following bounds:
(a) µε ⩽
1
100d2 ;
(b) µ ∑d
i=0|a2i+1| ⩽1;
(c) µ∥∑d
i=k a2i+1Ui−k(T2(x))∥sup ⩽d−k+1
d2
for all 0 ⩽k ⩽d;
The output description has the additional properties
∑
i
∥Ai,∗∥2|xi|2 ≲
ε2
log(n)d2
∥x∥0 ≲∥A∥2
F
(µε)2 log(n),
so that by Corollary 5.11, we can
(a) Compute entries of Ax in ∥x∥0 = O
 ∥A∥2
F
(µε)2 log(n)

time;
35

(b) Sample i ∈[n] with probability |yi|2
∥y∥2 in O

∥A∥F
ε2µ2d2∥y∥2

time;
(c) Estimate ∥y∥2 to ν relative error in O
 ∥A∥4
F log(n)
(νµ2εd∥y∥)2 log(n)

time.
The criterion for what µ need to be are somewhat non-trivial; essentially, these bounds are
what’s necessary for Eq. (Odd Clenshaw) to be numerically stable when computing p(x)/x.
This is necessary because we primarily work in the “dual” space, maintaining our Clenshaw
iterate uk as Avk where vk is a sparse vector. For any bounded polynomial, though, we can
always take µ to be Ω((d2 log2(d))−1).
Proposition 10.3. In Theorem 10.2, we can always take 1/µ ≲d2 log2(d)∥p∥Spec(A).
We are now ready to dive into the proof of Theorem 10.2.
Algorithm 10.4 (Odd singular value transformation).
Input: A matrix A ∈Cm×n, vector b ∈Cn, ε, δ, µ > 0, a degree 2d + 1 polynomial p, and
the corresponding coefﬁcients in the Chebyshev basis, i.e. p(x) = ∑d
i=0 a2i+1T2i+1(x).
Preprocessing sketches: Let s, t = Θ
 ∥A∥2
F
(µε)2 log( n
δ )

.
P1. If SQ(A†) and SQ(b) are not given, compute data structures to simulate them in
O(1) time;
P2. Sample S ∈Cn×s to be AMP

A, b, { 1
2( ∥A∗,i∥2
∥A∥2
F + |bi|2
∥b∥2 )}

(Theorem 6.5) with
sketch size s;
P3. Sample T ∈Ct×m to be AMP
 S†A†, AS

(Theorem 6.5) with sketch size t;
P4. Compute a data structure that can respond to SQ(TAS) queries in O(1) time;
Clenshaw iteration: Let r = Θ(d4∥A∥2
F(s + t)) = Θ
 d4∥A∥4
F
(µε)2 log n
δ

. Then starting with
vd+1 = vd+2 =⃗0s and going until v0,
I1. Let B(k) = BEST(TAS) and B(k)
†
= BEST((TAS)†) with parameter r;
I2. Compute vk = 2(2B(k)
† B(k) −I)vk+1 −vk+2 + a2k+1Sb.
Output: Output x = 1
2S(v0 −v1) that satisﬁes ∥Ax −p(A)b∥⩽ε.
Preprocessing sketches.
Given a matrix A ∈Cm×n and a vector b ∈Cn, the pre-processing
phase of Algorithm 10.4 can be performed in O(nnz(A)) time. If O(Q)-time SQ(A) and SQ(b)
are already given, then this can be performed in O(Qst) time.
The way to do this is straightforward. Finally, we query all of TAS in O(min(nnz(A), st)) time.
Using this, we can prepare the data structure for SQ(TAS).
We list the guarantees of the sketch that we will use in the error analysis, and point to where
they come from in Section 6.2. The guarantees of Theorem 6.5 individually fail with probability
O(δ), so we will rescale to say that they all hold with probability ⩾1 −δ.
∥[AS]∗,j∥2 ⩽2∥A∥2
F/s for all j ∈[s]
by Deﬁnition 6.4
(AS col bound)
∥S†b∥2 ⩽2∥b∥2
by Deﬁnition 6.4
(∥S†b∥bound)
∥AA† −AS(AS)†∥⩽µε
by Theorem 6.5
(AA† AMP)
36

∥Ab −ASS†b∥⩽µε
by Theorem 6.5
(Ab AMP)
∥TAS∥2
F = ∥AS∥2
F ⩽2∥A∥2
F
by Deﬁnition 6.4
(∥TAS∥F bound)
∥(AS)†(AS) −(TAS)†TAS∥⩽µε
by Theorem 6.5
((AS)†AS AMP)
A direct consequence of Eq. (AA† AMP) and Eq. ((AS)†AS AMP) is that AS and TAS have
bounded spectral norms.
∥AS∥2 = ∥AS(AS)†∥⩽∥AA†∥+ µε = ∥A∥2 + µε
∥TAS∥2 = ∥(TAS)†(TAS)∥⩽∥(AS)†(AS)∥+ µε = ∥AS∥2 + µε ⩽∥A∥2 + 2µε
(∥TAS∥bound)
Remark 10.5. In the streaming model, this procedure can be done in a constant number of
passes. Further, only need sampling and query access to b to give a log 1
δ failure probability: if we
only have query access to b, then sketch of S can use the probability distribution corresponding
to ∥A∗,i∥2
∥A∥2
F sufﬁces to give the necessary guarantee with probability ⩾0.99.
One Clenshaw iteration.
We are trying to perform the odd Clenshaw recurrence deﬁned in
Eq. (Odd Clenshaw). The matrix analogue of this is
uk = 2(2AA† −I)uk+1 −uk+2 + 2a2k+1Ab,
(Odd Matrix Clenshaw)
u = 1
2(u0 −u1).
We now show how to compute the next iterate uk given the previous two iterates in the form
uk+1 = (SA)†vk+1 and uk+2 = (SA)†vk+2, along with b, for all k ⩾0. The analysis begins by
showing that uk is ε∥vk+1∥-close to the output of an exact, zero-error iteration. Next, we show
that ∥vk∥is O(d)-close to its zero-error iteration value. Finally, we show that these errors don’t
accumulate too much towards the ﬁnal outcome.
First, we note that the runtime of each iteration is O(r), since this is the cost of producing the
sketches B(k) and B(k)
† , and actually computing the iteration costs O(r + s + t) = O(r).
Starting from Eq. (Odd Matrix Clenshaw), we take the following series of approximations by
applying intermediate sketches:
4AA†uk+1−2uk+1 −uk+2 + a2k+1Ab
≈1 4AS(AS)†uk+1 −2uk+1 −uk+2 + a2k+1Ab
= AS

4(AS)†(AS)vk+1 −2vk+1 −vk+2

+ a2k+1Ab
≈2 AS

4(TAS)†(TAS)vk+1 −2vk+1 −vk+2

+ a2k+1Ab
≈3 AS

4(TAS)†(TAS)vk+1 −2vk+1 −vk+2 + a2k+1S†b

≈4 AS(4(TAS)†B(k)vk+1 −2vk+1 −vk+2 + a2k+1S†b)
≈5 AS(4B(k)
† B(k)vk+1 −2vk+1 −vk+2 + a2k+1S†b).
We note that sketching without the BEST sufﬁces to get a d∥A∥4
F/(µε)4 runtime, since we can
now compute the expression 4(TAS)†(TAS)vk+1 −2vk+1 −vk+2 + a2k+1S†b and call it vk. We
improve on this using the BEST sketch from Section 6.1.
37

As for approximation error, let ε1, ε2, ε3, ε4 and ε5 be the errors introduced in the corresponding
approximation above. Using the previously established bounds on S and T,
ε1 ⩽4∥AA† −AS(AS)†∥∥uk+1∥⩽4µε∥uk+1∥
by Eq. (AA† AMP)
ε2 ⩽4∥AS∥∥((AS)(AS)† −(TAS)(TAS)†)vk+1∥⩽5µε∥vk+1∥
by Eq. ((AS)†AS AMP)
ε3 ⩽|a2k+1|∥Ab −ASS†b∥⩽|a2k+1|µε
by Eq. (Ab AMP)
The bounds on ε4 and ε5 follow from the bounds in Section 6.1 applied to TAS. With probability
⩾1 −1/(100d), the following hold:
ε4 ⩽4∥AS(TAS)†(TAS −B(k))vk+1∥
⩽4
√
100d/r∥AS(TAS)†∥F∥TAS∥F∥vk+1∥
by Corollary 6.3
⩽d−3/2µε∥vk+1∥
Similarly,
ε5 ⩽∥4AS((TAS)† −B(k)
† )B(k)vk+1∥
⩽4
√
100d/r∥AS∥F∥B(k)vk+1∥
by Corollary 6.3
⩽4
√
100d/r∥AS∥F

∥TASvk+1∥+
√
100d/r∥It∥F∥TAS∥F∥vk+1∥

by Corollary 6.3
⩽d−3/2µε∥vk+1∥.
by taking r = Θ(d4(s + t)) = Θ( d4∥A∥4
F
(µε)2 log n
δ ).
Call the cumulative error for this round ε(k).
ε(k) := ε1 + ε2 + ε3 + ε4 + ε5 ≲µε

∥vk∥+ |a2k+1|

.
Error accumulation across iterations.
A simple argument shows that the error of the full
algorithm is the sum of the errors for each round. In other words, after completing the iteration,
we have a vector u such that
∥u −p(A)b∥⩽
d
∑
k=0
ε(k) ≲µε
d
∑
k=0
(∥vk∥+ |a2k+1|) ⩽ε +
d
∑
k=0
µε∥vk∥.
(48)
So, it sufﬁces to bound the vk’s. The recursions deﬁning them is
vk = 4R(k)
† R(k)vk+1 −2vk+1 −vk+2 + a2k+1S†b
= 2(2(TAS)†(TAS) −I)vk+1 −vk+2 + a2k+1S†b + 4(B(k)
† B(k) −(TAS)†(TAS))vk+1
This solves to
vk =
d
∑
i=k
Ui−k(2(TAS)†(TAS) −I)

a2i+1S†b + 4(B(i)
† B(i) −(TAS)†(TAS))vi+1

.
We bound the second moment (averaging over the randomness of the B(k)’s), using that the
B(k), B(k)
†
and B(k′) are all drawn independently. First, notice that the means are bounded.
¯vk = E
[k,d] [vk] =
d
∑
i=k
Ui−k(2(TAS)†(TAS) −I)a2k+1Sb.
38

Using sub-multiplicativity of the operator norm,
∥¯vk∥⩽

d
∑
i=k
Ui−k(2(TAS)†(TAS) −I)a2k+1

S†b

=

d
∑
i=k
Ui−k(T2(x))a2k+1

Spec(TAS)
S†b

⩽e

d
∑
i=k
Ui−k(2x2 −1)a2k+1

sup
S†b

by Lemma 5.4 and Eq. (∥TAS∥bound)
⩽2e∥
d
∑
i=k
Ui−k(2x2 −1)a2k+1∥sup
⩽2ed −k + 1
µd2
by Item 10.2(c)
We now compute the second moment of vk.
E
[k,d]
h
∥vk∥2i
= ∥¯vk∥2 + E
[k,d]
h
∥vk −¯vk∥2i
= ∥¯vk∥2 + E
[k,d]



d
∑
i=k
Ui−k(2(TAS)†(TAS) −I)4(B(i)
† B(i) −(TAS)†(TAS))vi+1

2

We use independence of the B(i)’s to note that the expectation of cross terms is 0, and therefore,
= ∥¯vk∥2 + E
[k,d]
"
d
∑
i=k
Ui−k(2(TAS)†(TAS) −I)4(B(i)
† B(i) −(TAS)†(TAS))vi+1

2
#
⩽∥¯vk∥2 +
d
∑
i=k
16(i −k + 1)2 E
[i,d]
(B(i)
† B(i) −(TAS)†(TAS))vi+1

2
⩽∥¯vk∥2 +
d
∑
i=k
16(i −k + 1)2
d4
E
[i+1,d]
h
∥vi+1∥2i
,
where the last line follows from the computation
E
k
(B(k)
† B(k) −(TAS)†(TAS))vk+1

2
= E
k
B(k)
† B(k)vk+1

2
−
(TAS)†(TAS)vk+1

2
= E
k
h
(B(k)vk+1)†(B(k)
† )†B(k)
† (B(k)vk+1)
i
−
(TAS)†(TAS)vk+1

2
⩽E
k

(B(k)vk+1)†((TAS)(TAS)† + 1
r Tr(Is)∥TAS∥2
FIt)(B(k)vk+1)

−
(TAS)†(TAS)vk+1

2
by Eq. (12)
⩽E
k
h
v†
k+1(TAS)†((TAS)(TAS)† + s
r∥TAS∥2
FIt)(TAS)vk+1
+ v†
k+1
1
r Tr((TAS)(TAS)† + s
r∥TAS∥2
FIt)∥TAS∥2
Fvk+1
i
−
(TAS)†(TAS)vk+1

2
by Eq. (12)
= s
r∥TAS∥2
F∥TASvk+1∥2 + 1
r ∥TAS∥4
F∥vk+1∥2 + st
r2 ∥TAS∥4
F∥vk+1∥2
39

≲
s∥A∥2
F
r
+ ∥A∥4
F
r
+ st∥A∥4
F
r2

∥vk+1∥2
by Eqs. (∥TAS∥F bound) and (∥TAS∥bound)
= ∥A∥2
F
r

s + ∥A∥2
F + t∥A∥2
F
r

∥vk+1∥2 ⩽1
d4 ∥vk+1∥2
Now, we can bound the second moments of all ∥vk∥by bounding the recurrence. We deﬁne
the following recurrence ck to satisfy E[k,d][∥vk∥2] ⩽ck, and then inductively show that ck ⩽
∥¯vk∥2 + (d−k+1)2
µ2d5
⩽30 (d−k+1)2
µ2d4
:
ck = ∥¯vk∥2 +
d
∑
i=k
(i −k + 1)2
d4
ci+1
⩽∥¯vk∥2 +
d
∑
i=k
(i −k + 1)2
d4
30(d −i + 1)2
µ2d4
⩽∥¯vk∥2 + 30
µ2d8
d
∑
i=k
(i −k + 1)2(d −i + 1)2
⩽∥¯vk∥2 + (d −k + 1)5
µ2d8
⩽∥¯vk∥2 + (d −k + 1)2
µ2d5
We have shown that E[∥vk −¯vk∥2] ⩽1
d( (d−k+1)
µd2
)2. By Markov’s inequality, with probability
⩾0.999, we have that for all k, ∥vk∥≲d−k+1
µd2 . Returning to the ﬁnal error bound Eq. (48),
d
∑
k=0
ε(k) ≲ε +
d
∑
k=0
µε∥vk∥≲ε +
d
∑
k=0
ε(d −k + 1)/d2 ≲ε.
(49)
This concludes the error analysis.
Output description properties.
After the iteration concludes, we can compute u by comput-
ing x = 1
2S(v0 −v1) in linear O(s) time. Then, u = 1
2(u0 −u1) = 1
2 AS(v0 −v1) = Ax. Note
that though x ∈Cn, its sparsity is at most the sparsity of x, which is bounded by s.
Further, using the prior bounds on v0 and v1, we have that
n
∑
j=1
A∗,j
2|xi|2 =
s
∑
j=1
[SA]∗,j
2 1
2(v0 −v1)i
2
⩽
s
∑
j=1
2
s ∥A∥2
F
 1
2(v0 −v1)i
2
⩽2
s ∥A∥2
F
 1
2(v0 −v1)
2
⩽2
s ∥A∥2
F(∥v0∥2 + ∥v1∥2)
⩽2∥A∥2
F/(√
sµd)2.
10.2
Generalizing to even polynomials
We also obtain an analogous result for even polynomials. For the most part, changes are
superﬁcial; the red text indicates differences from Theorem 10.2. The major difference is that the
representation of the output is A†x + ηb instead of Ax, which results from a difference in the
dimension of the output, and the allowing for constant terms in p.
40

Theorem 10.6. Suppose we are given sampling and query access to A ∈Cm×n and b ∈Cn with
∥A∥, ∥b∥⩽1, and suppose we are given a (2d)-degree even polynomial, written in its Chebyshev
coefﬁcients as
p(x) =
d
∑
i=0
a2iT2i(x).
Then we can output a vector x ∈Cm and η ∈C such that ∥A†x + ηb −p(A)b∥⩽ε∥p∥Spec(A) in
time
O

min

nnz(A), ∥A∥4
F
(µε)4 log2(n)

+ d5∥A∥4
F
(µε)2
log(n)

,
assuming µ satisﬁes the following bounds:
(a) µε ⩽
1
100d2 ;
(b) µ ∑d
i=0|˜a2i| ⩽1;
(c) µ∥∑d
i=k 4˜a2i+2x · Ui−k(T2(x))∥sup ⩽d−k+1
d2
for all 0 ⩽k ⩽d.
The output description has the additional properties that
∑
i
∥Ai,∗∥2|xi|2 ≲
ε2
log(n)d2
∥x∥0 ≲∥A∥2
F
(µε)2 log(n),
so that by Corollary 5.11, we can
(a) Compute entries of Ax in ∥x∥0 = ∥A∥F/ε2 time;
(b) Sample i ∈[n] with probability |yi|2
∥y∥2 in ( ∥A∥2
F
(µd)2 +p(0)2) ∥A∥2
F log(n)
(µε∥y∥)2
time;
(c) Estimate ∥y∥2 to ν relative error in ( ∥A∥2
F
(µd)2 +p(0)2) ∥A∥2
F log(n)
(νµε∥y∥)2 time.
Proposition 10.7. In Theorem 10.6, we can always take 1/µ ≲d2log(d)∥p∥Spec(A).
Algorithm 10.8 (Even singular value transformation).
Input: A matrix A ∈Cm×n, vector b ∈Cn, ε > 0, a degree-2d polynomial p, and the
corresponding coefﬁcients in the Chebyshev basis, i.e. p(x) = ∑d
i=0 a2iT2i(x).
Preprocessing sketches: Let s, t = Θ( ∥A∥2
F
(µε)2 log( n
δ )). Compute all ˜a2k = a2k −a2k+2 + · · · ±
a2d.
P1. If SQ(A) and SQ(b) are not given, compute data structures to simulate them in
O(1) time;
P2. Sample S ∈Cs×m to be AMP(A†, A, { ∥A∗,i∥2
∥A∥2
F }) (Theorem 6.5) with sketch size s;
P3. Sample T ∈Cn×t to be AMP(SA, A†S†, { 1
2( ∥[SA]∗,i∥2
∥SA∥2
F
+ |bi|2
∥b∥2 )}) (Theorem 6.5)
with sketch size t;
P4. Compute a data structure that can respond to SQ(SAT) queries in O(1) time;
Clenshaw iteration: Let r = Θ(d4∥A∥2
F(s + t)) = Θ( d4∥A∥4
F
(µε)2 log n
δ ). Then starting with
vd+1 = vd+2 =⃗0s and going until v0,
41

I1. Let B(k) = BEST(SAT) and B(k)
†
= BEST((SAT)†) with parameter r;
I2. Compute vk = 2(2B(k)B(k)
†
−I)vk+1 −vk+2 + 4˜a2k+2B(k)b.
Output: Output x = 1
2S†(v0 −v1) and η = ˜a0 that satisﬁes
A†x + ηb −p(A)b
 ⩽ε.
Recall the odd and even recurrences deﬁned in Eqs. (Odd Clenshaw) and (Even Clenshaw).
uk = 2(2AA† −I)uk+1 −uk+2 + 2a2k+1Ab,
(Odd Matrix Clenshaw)
u = 1
2(u0 −u1).
The matrix analogue of the even recurrence is identical except for the ﬁnal term is 4˜a2k+2A†Ab
instead of 2a2k+1Ab.
˜a2k := a2k −a2k+2 + a2k+4 −· · · ± a2d
uk = 2(2A†A −1)uk+1 −uk+2 + 4˜a2k+2A†Ab,
(Even Matrix Clenshaw)
u = ˜a0b + 1
2(u0 −u1).
So, a roughly identical analysis works upon making the appropriate changes.
One (even) Clenshaw iteration.
We maintain uk as (SA)†vk. The error analysis proceeds by
bounding
4A†Auk+1−2uk+1 −uk+2 + 4˜a2k+2A†Ab
≈1 4(SA)†(SA)uk+1 −2uk+1 −uk+2 + 4˜a2k+2A†Ab
= (SA)†
4(SA)(SA)†vk+1 −2vk+1 −vk+2

+ 4˜a2k+2A†Ab
≈2 (SA)†
4(SAT)(SAT)†vk+1 −2vk+1 −vk+2

+ 4˜a2k+2A†Ab
≈3 (SA)†
4(SAT)(SAT)†vk+1 −2vk+1 −vk+2 + 4˜a2k+2SATT†b

≈4 (SA)†
4(SAT)B(k)
† vk+1 −2vk+1 −vk+2 + 4˜a2k+2SATT†b

≈5 (SA)†
4B(k)B(k)
† vk+1 −2vk+1 −vk+2 + 4˜a2k+2SATT†b

≈6 (SA)†
4B(k)B(k)
† vk+1 −2vk+1 −vk+2 + 4˜a2k+2B(k)T†b

Using the same approaches as in the odd case, we can show that the error incurred to compute
uk is O(µε(∥vk+1∥+ |˜a2k+2|)).
Error accumulation across iterations.
The corresponding recurrence for vk is
vk = 4B(k)B(k)
† vk+1 −2vk+1 −vk+2 + 4˜a2k+2B(k)T†b
= (4(SAT)(SAT)† −2I)vk+1 −vk+2
+
h
4˜a2k+2SATT†b + 4(SAT(SAT)† −B(k)B(k)
† )vk+1 + (B(k) −SAT)T†b
i
This solves to
vk =
d
∑
i=k
Ui−k(T2(SAT))
h
4˜a2k+2SATT†b + 4(SAT(SAT)† −B(k)B(k)
† )vk+1 + (B(k) −SAT)T†b
i
From here, the identical analysis applies. The bound from Item 10.6(c) corresponds to ∥E[vk]∥
being bounded by d−k+1
µd2 .
42

Output description properties.
The main change is that the description is A†x + ˜a0b, with an
additional scalar b term. We ﬁrst use the prior analysis to get SQφ(A†x) for φ = ∥x∥0
∑i∥Ai,∗∥2|xi|2
∥A†x∥2
=
∥A∥2
F
(µd)2∥A†x∥2 . Then we use Lemma 5.8 to get SQϕ(A†x + ηb) for φ = 2( ∥A∥2
F
(µd)2 + η2∥b∥2)/
A†x + ηb
2.
Finally, we note that ˜a0 = p(0).
10.3
Bounding iterates of singular value transformation
Theorem 10.9. Let p(x) be a odd degree 2d + 1 polynomial such that ∥p∥sup ⩽1. Then it sufﬁces for
Theorem 10.2 to take
µ = O(d2 log(d)).
Proof. All there is to prove is that

d
∑
i=k
aiUi−k(T2(x))

sup ⩽(d −k + 1) log2(d + 1)
First, we note that it sufﬁces to prove the bound for ∑d
i=k aiUi−k(y), since we get the bound by
setting y = T2(x). The strategy to proving this is the same as the one we discussed before:
∑
i
aiUi−k(T2(x))
= ∑
i
ai ∑
j⩾0
Ti−k−2j(y)(1 + Ji −k −2j ̸= 0K)
= ∑
j⩾0∑
i
aiTi−k−2j(y)(1 + Ji −k −2j ̸= 0K)
= ∑
j⩾0∑
i
ai+k+2jTi(x)(1 + Ji ̸= 0K)
= ∑
i
Ti(x)(1 + Ji ̸= 0K) ∑
j⩾0
ai+k+2j
⩽∑
i
(1 + Ji ̸= 0K)
∑
j⩾0
ai+k+2j

⩽∑
i
(1 + Ji ̸= 0K)(32 + 8 ln2(d + 1))Ji ⩽d −kK
≲(d −k + 1) log2(d + 1)
Theorem 10.10. Let p(x) be a even degree-(2d) polynomial such that ∥p∥sup ⩽1. Then it sufﬁces for
Theorem 10.6 to take
µ = O(d2 log2(d))
Proof. All there is to prove is that

d
∑
i=k
4˜a2i+2xUi−k(T2(x))

sup =

d
∑
i=k
2˜a2i+2U2(i−k)+1(x)

sup ⩽(d −k + 1) log2(d + 1)
The strategy to proving this is the same as the one we discussed before:
∑
i
˜a2i+2U2(i−t)+1(x)
43

= ∑
i ∑
k⩾0
(−1)ka2(i+k+1)
i−t
∑
j=0
T2j+1(x)
= ∑
i,j,k
(−1)ka2(i+k+1)T2j+1(x)Jj, k ⩾0KJj ⩽i −tK
= ∑
j⩾0
T2j+1 ∑
i,k
(−1)ka2(i+k+1)Jk ⩾0KJj ⩽i −tK
= ∑
j⩾0
T2j+1 ∑
i,k,ℓ
(−1)ka2ℓJℓ= i + k + 1KJk ⩾0KJj ⩽i −tK
= ∑
j⩾0
T2j+1 ∑
ℓ
a2ℓ∑
k
(−1)kJk ⩾0KJj ⩽ℓ−k −1 −tK
= ∑
j⩾0
T2j+1 ∑
ℓ
a2ℓ∑
k
(−1)kJ0 ⩽k ⩽ℓ−j −1 −tK
= ∑
j⩾0
T2j+1 ∑
ℓ
a2ℓJℓ−j −1 −t ∈2Z⩾0K
= ∑
j⩾0
T2j+1 ∑
ℓ⩾0
a2(2ℓ+j+1+t)
⩽
d
∑
j⩾t+1
∑
ℓ⩾0
a2(2ℓ+j)

⩽(d −t)(32 + 8 ln2(d + 1))
≲(d −k + 1) log2(d + 1)
11
Dequantizing algorithms
11.1
Recommendation systems
Lemma 11.1 (Polynomial approximations of the sign function (Lemma 25 of [GSLW19])). For
all δ > 0 and ε ∈(0, 1
2) there exists an efﬁciently computable odd polynomial p ∈R[x] with deg(p) =
O( 1
δ log 1
ε )
• for all x ∈[−2, 2], |p(x)| ⩽1
• for all x ∈[−2, −δ] ∪[δ, 2], |p(x) −sign(x)| ⩽ε
Lemma 11.2. For ς, η ∈(0, 1
2) and σ ∈[−1, 1], there exists an even polynomial p(x) ∈R[x] of degree
O( 1
ησ log 1
ς) such that p(x) ∈[0, 1] for all x ∈[−1, 1] and
p(x) ∈







[1 −ς, 1]
for 1 ⩽x ⩽−(1 + η)σ
[0, ς]
for −(1 −η)σ ⩽x ⩽(1 −η)σ
[1 −ς, 1]
for (1 + η)σ ⩽x ⩽1
Proof. Let s(x) be the approximation in Lemma 11.1 with error paramters δ ←ησ and ε ←ς
2.
Then
p(x) := (1 −ς)

1 + s(x −σ) + s(−x −σ)
2

satisﬁes the desired parameters.
44

Corollary 11.3 (De-quantizing Recommendation Systems). The recommendation systems problem
is to sample from [A⩾σ, 1
6 ]i,∗. Using Theorem 10.6 on the polynomial from Lemma 11.2, given nnz(A)
preprocessing time to make a data structure, we can compute an x such that
A†x −A†
⩾σ, 1
3 ei
 ⩽ε in
eO
 ∥A∥4
F
σ9ε2

time, and sample from this x in eO

∥A∥4
F
σ6ε2∥A†x∥2

time.
11.2
Linear regression
Lemma 11.4 (Polynomial approximations of 1/x, [GSLW19, Lemma 40], following [CKS17]).
Let κ > 1 and 0 < ε ⩽δ ⩽1
2. There is an odd polynomial p(x) such that
p(x) −δ
2x
 ⩽ε
is ε-close to 1/x on the domain [−1, 1] \ (−1
κ, 1
κ). Let J :=
lp
b log(4b/ε)
m
, then the O(κ log κ
ε )-degree
odd real polynomial
g(x) := 4
J
∑
j=0
(−1)j
"
∑b
i=j+1 ( 2b
b+i)
22b
#
T2j+1(x)
is ε-close to f (x) on the interval [−1, 1], moreover |g(x)| ⩽4J = O(κ log( κ
ε )) on this interval.
This is the polynomial that is applied to perform regression in the QSVT framework, with slight
modiﬁcations to make the polynomial suitably bounded.
Corollary 11.5. If we have SQ(A) and SQ(b), we can apply Theorem 10.2 on g(x)/κ to error ε/κ,
where g(x) is the polynomial in Lemma 11.4. Then after O(nnz(A)) or O(κ12∥A∥4
Fε−4 polylog(nκ/ε)),
we compute This allows us to get a representation of a vector y such that ∥y −g(A)b∥⩽ε∥b∥in
O
κ11∥A∥4
F
ε2
polylog(nκ/ε)

time. We can also sample and query from the output in this amount of time.
11.3
Hamiltonian simulation
In this Subsection, we provide the corollary for de-quantizing Hamiltonian simulation. We
begin by recalling the following deﬁnition:
Deﬁnition 11.6 (Bessel functions of the ﬁrst kind). Given i ∈N,
Ji(x) =
∞
∑
m=0
(−1)m
m!Γ(i + m + 1)
 x
2
2m+i
,
where Γ is the gamma function.
We also need the following polynomial approximation to trigonometric functions:
Lemma 11.7 (Polynomial approximation to trigonometric functions, [GSLW19, Lemma 57]).
Given t ∈R and ε ∈(0, 1/e), let r = Θ

t +
log(1/ε)
log log(1/ε)

. Then, the following degree 2r and 2r + 1
45

satisfy that for all x ∈[−1, 1],
cos(tx) −J0(t) + 2 ∑
i∈[1,r]
(−1)iJ2i(t)T2i(x)

sup
⩽ε,
sin(tx) −2 ∑
i∈[0,r]
(−1)iJ2i+1(t)T2i+1(x)

sup
⩽ε,
where Ji is the i-th Bessel function of the ﬁrst kind.
Corollary 11.8 (Hamiltonian simulation). Given a symmetric Hamiltonian H ∈Cn×n with ∥H∥⩽1
and a vector b ∈Cn, after O(nnz(A)) preprocessing, we can output a description of a vector v such
that, with probability ⩾0.9,
v −eiHtb
 ⩽ε|b| in O(t9∥H∥4
F log(1/ε)9/(ε2 log log(1/ε)9)).
Proof. Let pcos = J0(t) −2 ∑i∈[1,r](−1)iJ2i(t)T2i(x) and let psin = 2 ∑i∈[0,r](−1)iJ2i+1(t)T2i+1(x).
We apply Theorem 10.6 to get a description of c such that ∥c −pcos(tH)b∥⩽ε∥b∥and apply
Theorem 10.2 to get a description of s such that ∥s −psin(tH)b∥⩽ε∥b∥. Then
eiHtb = cos(Ht)b + i sin(Ht)b ≈ε∥b∥pcos(Ht)b + ipsin(Ht)b ≈ε∥b∥c + is.
This gives us a description O(ε)-close to eiHt. Using Corollary 5.11, we can get a sample from
this output in O
 t6∥A∥4
F log(n) 1
ε2 polylog( 1
ε )

time.
Acknowledgments
ET and AB thank Nick Trefethen, Simon Foucart, Alex Townsend, Sujit Rao, and Victor Reis for
helpful discussions. ET thanks t.f. for the support. AB is supported by Ankur Moitra’s ONR
grant. ET is supported by the NSF GRFP (DGE-1762114). Work on this paper was initiated at
the Simon Institute’s “Probability, Geometry, and Computation in High Dimensions” program
in 2020; we thank the institute for its hospitality.
References
[AA18]
Scott Aaronson and Andris Ambainis. Forrelation: A problem that optimally
separates quantum from classical computing. SIAM Journal on Computing, 47(3):982–
1038, 2018. 4
[AHK06]
Sanjeev Arora, Elad Hazan, and Satyen Kale. A fast random sampling algorithm
for sparsifying matrices. In Approximation, Randomization, and Combinatorial Opti-
mization. Algorithms and Techniques, pages 272–279. Springer, 2006. 13
[AM07]
Dimitris Achlioptas and Frank McSherry. Fast computation of low-rank matrix
approximations. Journal of the ACM (JACM), 54(2):9–es, 2007. 10, 11, 13
[AZL16]
Zeyuan Allen-Zhu and Yuanzhi Li. Lazysvd: Even faster svd decomposition yet
without agonizing pain. Advances in neural information processing systems, 29, 2016.
14
[BCW20]
Ainesh Bakshi, Nadiia Chepurko, and David P Woodruff. Robust and sample
optimal algorithms for psd low rank approximation. In 2020 IEEE 61st Annual
46

Symposium on Foundations of Computer Science (FOCS), pages 506–516. IEEE, 2020.
14
[BCW22]
Ainesh Bakshi, Kenneth L Clarkson, and David P Woodruff. Low-rank approxima-
tion with 1/ε1/3 matrix-vector products. pages 1130–1143, 2022. 14
[BKKS21]
Vladimir Braverman, Robert Krauthgamer, Aditya R Krishnan, and Shay Sapir.
Near-optimal entrywise sampling of numerically sparse matrices. In Conference on
Learning Theory, pages 759–773. PMLR, 2021. 10, 13
[BKL+19]
Fernando G. S. L. Brandão, Amir Kalev, Tongyang Li, Cedric Yen-Yu Lin, Krysta M.
Svore, and Xiaodi Wu. Quantum SDP solvers: Large speed-ups, optimality, and
applications to quantum learning. In Proceedings of the 46th International Colloquium
on Automata, Languages, and Programming (ICALP), pages 27:1–27:14, 2019. arXiv:
1710.02581 3
[BKM22]
Vladimir Braverman, Aditya Krishnan, and Christopher Musco. Sublinear time
spectral density estimation. In Proceedings of the 54th Annual ACM SIGACT Sym-
posium on Theory of Computing, STOC 2022, page 1144–1157, New York, NY, USA,
2022. Association for Computing Machinery. 9
[BMN+21] Ryan Babbush, Jarrod R McClean, Michael Newman, Craig Gidney, Sergio Boixo,
and Hartmut Neven. Focus beyond quadratic speedups for error-corrected quan-
tum advantage. PRX Quantum, 2(1):010103, 2021. 5
[BW18]
Ainesh Bakshi and David Woodruff. Sublinear time low-rank approximation of
distance matrices. Advances in Neural Information Processing Systems, 31, 2018. 14
[BWP+17]
Jacob Biamonte, Peter Wittek, Nicola Pancotti, Patrick Rebentrost, Nathan Wiebe,
and Seth Lloyd. Quantum machine learning. Nature, 549(7671):195–202, 2017. 13
[CCH+22]
Nadiia Chepurko, Kenneth Clarkson, Lior Horesh, Honghao Lin, and David
Woodruff. Quantum-inspired algorithms from randomized numerical linear alge-
bra. In International Conference on Machine Learning, pages 3879–3900. PMLR, 2022.
1, 5, 6, 10, 13
[CdW21]
Yanlin Chen and Ronald de Wolf. Quantum algorithms and lower bounds for
linear regression with norm constraints, 2021. 3
[CEM+15]
Michael B Cohen, Sam Elder, Cameron Musco, Christopher Musco, and Madalina
Persu. Dimensionality reduction for k-means clustering and low rank approxima-
tion. In Proceedings of the forty-seventh annual ACM symposium on Theory of computing,
pages 163–172, 2015. 13
[CGJ19]
Shantanav Chakraborty, András Gilyén, and Stacey Jeffery. The power of block-
encoded matrix powers: Improved regression techniques via faster Hamiltonian
simulation. In Proceedings of the 46th International Colloquium on Automata, Languages,
and Programming (ICALP), pages 33:1–33:14, 2019. arXiv: 1804.01973 3
[CGL+20a] Nai-Hui Chia, András Gilyén, Tongyang Li, Han-Hsuan Lin, Ewin Tang, and
Chunhao Wang. Sampling-based sublinear low-rank matrix arithmetic framework
for dequantizing quantum machine learning. In Proceedings of the 52nd Annual ACM
SIGACT Symposium on Theory of Computing - STOC 2020. ACM Press, 2020. 1, 4, 5, 6,
7, 8, 10, 13, 14, 16, 17
47

[CGL+20b] Nai-Hui Chia, András Gilyén, Han-Hsuan Lin, Seth Lloyd, Ewin Tang, and Chun-
hao Wang. Quantum-inspired algorithms for solving low-rank linear equation
systems with logarithmic dependence on the dimension. In Proceedings of the 31st
International Symposium on Algorithms and Computation (ISAAC), pages 47:1–47:17,
2020. 1, 5, 13
[CHI+18]
Carlo Ciliberto, Mark Herbster, Alessandro Davide Ialongo, Massimiliano Pontil,
Andrea Rocchetto, Simone Severini, and Leonard Wossnig. Quantum machine
learning: a classical perspective. Proceedings of the Royal Society A: Mathematical,
Physical and Engineering Sciences, 474(2209):20170551, 2018. 3
[CKS17]
Andrew M. Childs, Robin Kothari, and Rolando D. Somma. Quantum algorithm for
systems of linear equations with exponentially improved dependence on precision.
SIAM Journal on Computing, 46(6):1920–1950, 2017. 45
[Cle55]
C. W. Clenshaw. A note on the summation of Chebyshev series. Math. Tables Aids
Comput., 9:118–120, 1955. 8, 9
[CNW15]
Michael B Cohen, Jelani Nelson, and David P Woodruff. Optimal approximate
matrix product in terms of stable rank. arXiv preprint arXiv:1507.02268, 2015. 13
[DKM06]
P. Drineas, R. Kannan, and M. Mahoney. Fast Monte Carlo algorithms for matrices
II: Computing a low-rank approximation to a matrix. SIAM Journal on Computing,
36(1):158–183, January 2006. 13
[DW20]
Vedran Dunjko and Peter Wittek. A non-review of quantum machine learning:
trends and explorations. Quantum Views, 4:32, 2020. 3
[DZ11]
Petros Drineas and Anastasios Zouzias. A note on element-wise matrix sparsi-
ﬁcation via a matrix-valued bernstein inequality. Information Processing Letters,
111(8):385–389, 2011. 10, 13
[FP68]
L. Fox and I. B. Parker. Chebyshev polynomials in numerical analysis. Oxford University
Press, London-New York-Toronto, Ont., 1968. 9
[GLG22]
Sevag Gharibian and François Le Gall. Dequantizing the quantum singular value
transformation: Hardness and applications to quantum chemistry and the quantum
pcp conjecture. In Proceedings of the 54th Annual ACM SIGACT Symposium on Theory
of Computing, pages 19–32, 2022. 5, 13
[GLM08]
Vittorio Giovannetti, Seth Lloyd, and Lorenzo Maccone. Quantum random access
memory. Physical review letters, 100:160501, Apr 2008. 7
[GS18]
Neha Gupta and Aaron Sidford. Exploiting numerical sparsity for efﬁcient learning:
faster eigenvector computation and regression. Advances in Neural Information
Processing Systems, 31, 2018. 14
[GSLW19]
András Gilyén, Yuan Su, Guang Hao Low, and Nathan Wiebe. Quantum singular
value transformation and beyond: Exponential improvements for quantum matrix
arithmetics. In Proceedings of the 51st ACM Symposium on the Theory of Computing
(STOC), pages 193–204, 2019. arXiv: 1806.01838 1, 3, 6, 7, 15, 18, 44, 45
[GST22]
András Gilyén, Zhao Song, and Ewin Tang. An improved quantum-inspired
algorithm for linear regression. Quantum, 6:754, 2022. 1, 13
48

[GT09]
Alex Gittens and Joel A Tropp. Error bounds for random matrix approximation
schemes. arXiv preprint arXiv:0911.4108, 2009. 13
[HHL09]
Aram W. Harrow, Avinatan Hassidim, and Seth Lloyd. Quantum algorithm for
linear systems of equations. Physical Review Letters, 103(15):150502, 2009. arXiv:
0811.3171 3, 12
[IVWW19] Pitor Indyk, Ali Vakilian, Tal Wagner, and David P Woodruff. Sample-optimal
low-rank approximation of distance matrices. In Conference on Learning Theory,
pages 1723–1751. PMLR, 2019. 14
[JLGS20]
Dhawal Jethwani, François Le Gall, and Sanjay K. Singh.
Quantum-inspired
classical algorithms for singular value transformation. In Proceedings of the 45th
International Symposium on Mathematical Foundations of Computer Science (MFCS),
pages 53:1–53:14, 2020. arXiv: 1910.05699 5
[KD14]
Abhisek Kundu and Petros Drineas. A note on randomized element-wise matrix
sparsiﬁcation. arXiv preprint arXiv:1404.0320, 2014. 10, 13
[KDMI17]
Abhisek Kundu, Petros Drineas, and Malik Magdon-Ismail. Recovering pca and
sparse pca via hybrid-(l1, l2) sparse sampling of data elements. The Journal of
Machine Learning Research, 18(1):2558–2591, 2017. 13
[KLLP19]
Iordanis Kerenidis, Jonas Landman, Alessandro Luongo, and Anupam Prakash.
q-means: A quantum algorithm for unsupervised machine learning. Advances in
Neural Information Processing Systems, 32, 2019. 3, 4
[KP17]
Iordanis Kerenidis and Anupam Prakash. Quantum recommendation systems. In
Proceedings of the 8th Innovations in Theoretical Computer Science Conference (ITCS),
pages 49:1–49:21, 2017. arXiv: 1603.08675 3, 6, 13
[KP22]
Iordanis Kerenidis and Anupam Prakash. Quantum machine learning with sub-
space states. arXiv preprint arXiv:2202.00054, 2022. 4
[Lan50]
Cornelius Lanczos. An iteration method for the solution of the eigenvalue problem
of linear differential and integral operators. J. Research Nat. Bur. Standards, 45:255–
282, 1950. 8
[LC17]
Guang Hao Low and Isaac L. Chuang.
Optimal Hamiltonian simulation by
quantum signal processing. Physical Review Letters, 118(1):010501, 2017. arXiv:
1606.02685 3
[LGZ16]
Seth Lloyd, Silvano Garnerone, and Paolo Zanardi. Quantum algorithms for
topological and geometric analysis of data. Nature communications, 7(1):1–7, 2016.
13
[LMR13]
Seth Lloyd, Masoud Mohseni, and Patrick Rebentrost. Quantum algorithms for
supervised and unsupervised machine learning. arXiv, 2013. 13
[LMR14]
Seth Lloyd, Masoud Mohseni, and Patrick Rebentrost. Quantum principal compo-
nent analysis. Nature Physics, 10(9):631–633, July 2014. 13
[Mah11]
Michael W. Mahoney. Randomized algorithms for matrices and data. Foundations
and Trends® in Machine Learning, 3(2):123–224, 2011. 13
49

[MH02]
John C Mason and David C Handscomb. Chebyshev polynomials. Chapman and
Hall/CRC, 2002. 9, 15, 29, 31
[MI11]
Malik Magdon-Ismail. Using a non-commutative bernstein bound to approximate
some matrix algorithms in the spectral norm. arXiv preprint arXiv:1103.5453, 2011.
13
[MM15]
Cameron Musco and Christopher Musco. Randomized block krylov methods for
stronger and faster approximate singular value decomposition. Advances in neural
information processing systems, 28, 2015. 14
[MMS18]
Cameron Musco, Christopher Musco, and Aaron Sidford. Stability of the lanczos
method for matrix function approximation. In Proceedings of the Twenty-Ninth
Annual ACM-SIAM Symposium on Discrete Algorithms, pages 1605–1624. Society for
Industrial and Applied Mathematics, January 2018. 8, 9, 31
[MRTC21]
John M. Martyn, Zane M. Rossi, Andrew K. Tan, and Isaac L. Chuang. Grand
uniﬁcation of quantum algorithms. Physical Review X, 2(4):040203, 2021. arXiv:
2105.02859 3
[MW17]
Cameron Musco and David P Woodruff. Sublinear time low-rank approximation of
positive semideﬁnite matrices. In 2017 IEEE 58th Annual Symposium on Foundations
of Computer Science (FOCS), pages 672–683. IEEE, 2017. 14
[MZ11]
Avner Magen and Anastasios Zouzias. Low rank matrix-valued chernoff bounds
and approximate matrix multiplication. In Proceedings of the twenty-second annual
ACM-SIAM symposium on Discrete Algorithms, pages 1422–1436. SIAM, 2011. 13
[NN13]
Jelani Nelson and Huy L. Nguyen. OSNAP: Faster numerical linear algebra algo-
rithms via sparser subspace embeddings. In 2013 IEEE 54th Annual Symposium on
Foundations of Computer Science. IEEE, October 2013. 10
[Oli77]
J. Oliver. An error analysis of the modiﬁed Clenshaw method for evaluating
Chebyshev and Fourier series. J. Inst. Math. Appl., 20(3):379–391, 1977. 9
[Oli79]
J. Oliver. Rounding error propagation in polynomial evaluation schemes. J. Comput.
Appl. Math., 5(2):85–97, 1979. 9, 31
[Pai76]
C. C. Paige. Error analysis of the lanczos algorithm for tridiagonalizing a symmetric
matrix. IMA Journal of Applied Mathematics, 18(3):341–349, 1976. 8
[RML14]
Patrick Rebentrost, Masoud Mohseni, and Seth Lloyd. Quantum support vector
machine for big data classiﬁcation. Physical review letters, 113:130503, September
2014. 3, 13
[RWC+20] Alessandro Rudi, Leonard Wossnig, Carlo Ciliberto, Andrea Rocchetto, Massimil-
iano Pontil, and Simone Severini. Approximating hamiltonian dynamics with the
nyström method. Quantum, 4:234, 2020. 13
[Saa81]
Yousef Saad. Krylov subspace methods for solving large unsymmetric linear
systems. Mathematics of computation, 37(155):105–126, 1981. 14
[Sch41]
A. C. Schaeffer. Inequalities of A. Markoff and S. Bernstein for polynomials and
related functions. Bull. Amer. Math. Soc., 47:565–579, 1941. 9, 22, 34
50

[Sha04]
Aleksei Shadrin. Twelve proofs of the Markov inequality. In Approximation theory:
a volume dedicated to Borislav Bojanov, pages 233–298. Prof. M. Drinov Acad. Publ.
House, Soﬁa, 2004. 34
[SM21]
Changpeng Shao and Ashley Montanaro. Faster quantum-inspired algorithms for
solving linear systems. arXiv, 2021. 1, 5, 6, 13
[SV+14]
Sushant Sachdeva, Nisheeth K Vishnoi, et al. Faster algorithms via approximation
theory. Foundations and Trends® in Theoretical Computer Science, 9(2):125–210, 2014.
16
[Tan19]
Ewin Tang. A quantum-inspired classical algorithm for recommendation systems.
In Proceedings of the 51st Annual ACM SIGACT Symposium on Theory of Computing -
STOC 2019. ACM Press, 2019. 1, 4, 8, 13, 17
[Tan21]
Ewin Tang. Quantum principal component analysis only achieves an exponential
speedup because of its state preparation assumptions. Phys. Rev. Lett., 127:060503,
Aug 2021. 8, 11, 13
[Tao13]
T Tao.
Matrix identities as derivatives of determinant identities, 2013.
URL https://terrytao. wordpress. com/2013/01/13/matrix-identities-as-derivatives-of-
determinant-identities, 2013. 25
[TBI97]
Lloyd N Trefethen and David Bau III. Numerical linear algebra, volume 50. Siam,
1997. 14
[Tre19]
Lloyd N. Trefethen. Approximation Theory and Approximation Practice, Extended
Edition. Society for Industrial and Applied Mathematics, Philadelphia, PA, 2019. 5,
12, 15, 22, 23, 27
[Tro12]
Joel A Tropp. User-friendly tail bounds for sums of random matrices. Foundations
of computational mathematics, 12(4):389–434, 2012. 21
[VdN11]
Maarten Van den Nest. Simulating quantum computers with probabilistic methods.
Quantum Information and Computation, 11(9&10):784–812, 2011. arXiv: 0911.1624 13
[Vos91]
Michael D Vose. A linear algorithm for generating random numbers with a given
distribution. IEEE Transactions on software engineering, 17(9):972–975, 1991. 17
[WKS14]
Nathan Wiebe, Ashish Kapoor, and Krysta M Svore. Quantum deep learning. arXiv
preprint arXiv:1412.3489, 2014. 3, 13
[WKS16]
Nathan Wiebe, Ashish Kapoor, and Krysta M. Svore. Quantum perceptron models.
In Advances in Neural Information Processing Systems 29 (NIPS), pages 3999–4007,
2016. arXiv: 1602.04799 3, 13
[Woo14]
David P. Woodruff. Sketching as a tool for numerical linear algebra. Foundations
and Trends® in Theoretical Computer Science, 10, 2014. 13, 18, 20
[ZFF19]
Zhikuan Zhao, Jack K. Fitzsimons, and Joseph F. Fitzsimons. Quantum-assisted
Gaussian process regression.
Physical Review A, 99(5):052331, 2019.
arXiv:
1512.03929 13
51

