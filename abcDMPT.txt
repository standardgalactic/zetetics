A Brief Compilation
of Guides, Walkthroughs, and Problems
Discrete Mathematics and
Probability Theory
at the University of California, Berkeley
Alvin Wan

Contents
0.1
Purpose . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
4
0.1.1
Contributors
. . . . . . . . . . . . . . . . . . . . . . . . .
4
0.1.2
Structure
. . . . . . . . . . . . . . . . . . . . . . . . . . .
4
0.1.3
Breakdown
. . . . . . . . . . . . . . . . . . . . . . . . . .
4
0.1.4
Resources . . . . . . . . . . . . . . . . . . . . . . . . . . .
4
1
Modular Arithmetic and Polynomials
5
1.1
Guide
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
5
1.1.1
Modular Arithmetic . . . . . . . . . . . . . . . . . . . . .
5
1.1.2
Polynomial Properties . . . . . . . . . . . . . . . . . . . .
5
1.1.3
Fermat’s Little Theorem . . . . . . . . . . . . . . . . . . .
5
1.1.4
Lagrange Interpolation . . . . . . . . . . . . . . . . . . . .
6
1.1.5
Error-Correcting Codes
. . . . . . . . . . . . . . . . . . .
6
1.1.6
RSA . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
6
1.1.7
Secret Sharing
. . . . . . . . . . . . . . . . . . . . . . . .
6
1.2
Secret-Sharing Walkthrough . . . . . . . . . . . . . . . . . . . . .
7
2
Counting
9
2.1
Guide
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
9
2.1.1
Fundamental Properties . . . . . . . . . . . . . . . . . . .
9
2.1.2
Stars and Bars . . . . . . . . . . . . . . . . . . . . . . . .
9
2.1.3
Order, Replacement, and Distinguishability . . . . . . . .
9
2.1.4
Combinatorial Proofs
. . . . . . . . . . . . . . . . . . . .
10
2.1.5
Inclusion Exclusion Principle . . . . . . . . . . . . . . . .
10
2.2
Stars and Bars Walkthrough
. . . . . . . . . . . . . . . . . . . .
11
2.3
Problems
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
13
3
Probability
14
3.1
Guide
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
14
3.1.1
Random Variables . . . . . . . . . . . . . . . . . . . . . .
14
3.1.2
Law of Total Probability . . . . . . . . . . . . . . . . . . .
14
3.1.3
Conditional Probability . . . . . . . . . . . . . . . . . . .
14
3.1.4
Bayes’ Rule . . . . . . . . . . . . . . . . . . . . . . . . . .
15
3.1.5
Independence . . . . . . . . . . . . . . . . . . . . . . . . .
15
3.1.6
Symmetry . . . . . . . . . . . . . . . . . . . . . . . . . . .
15
2

Discrete Mathematics and Probability Theory
aaalv.in/abcDMPT
3.2
Symmetry Walkthrough . . . . . . . . . . . . . . . . . . . . . . .
16
3.3
Problems
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
18
4
Expectation
19
4.1
Guide
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
19
4.1.1
Expectation Deﬁnition . . . . . . . . . . . . . . . . . . . .
19
4.1.2
Linearity of Expectation . . . . . . . . . . . . . . . . . . .
20
4.1.3
Conditional Expectation . . . . . . . . . . . . . . . . . . .
20
4.1.4
Law of Total Expectation . . . . . . . . . . . . . . . . . .
20
4.2
Linearity of Expectation Walkthrough . . . . . . . . . . . . . . .
21
4.3
Dilution Walkthrough
. . . . . . . . . . . . . . . . . . . . . . . .
24
4.4
Problems
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
28
5
Distributions and Estimation
29
5.1
Guide
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
29
5.1.1
Important Distributions . . . . . . . . . . . . . . . . . . .
29
5.1.2
Combining Distributions . . . . . . . . . . . . . . . . . . .
29
5.1.3
Variance . . . . . . . . . . . . . . . . . . . . . . . . . . . .
29
5.1.4
Covariance
. . . . . . . . . . . . . . . . . . . . . . . . . .
30
5.1.5
Linearity of Variance . . . . . . . . . . . . . . . . . . . . .
30
5.1.6
Linear Regression . . . . . . . . . . . . . . . . . . . . . . .
30
5.2
Variance Walkthrough . . . . . . . . . . . . . . . . . . . . . . . .
31
6
Bounds
33
6.1
Guide
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
33
6.1.1
Markov’s Inequality
. . . . . . . . . . . . . . . . . . . . .
33
6.1.2
Chebyshev’s Inequality . . . . . . . . . . . . . . . . . . . .
33
6.1.3
Law of Large Numbers . . . . . . . . . . . . . . . . . . . .
33
6.2
Conﬁdence Intervals Walkthrough
. . . . . . . . . . . . . . . . .
34
7
Markov Chains
38
7.1
Guide
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
38
7.1.1
Deﬁnition . . . . . . . . . . . . . . . . . . . . . . . . . . .
38
7.1.2
Characterization . . . . . . . . . . . . . . . . . . . . . . .
38
7.1.3
Transition Probability Matrices . . . . . . . . . . . . . . .
38
7.1.4
Balance Equations . . . . . . . . . . . . . . . . . . . . . .
39
7.1.5
Important Theorems . . . . . . . . . . . . . . . . . . . . .
39
7.2
Hitting Time Walkthrough
. . . . . . . . . . . . . . . . . . . . .
40
8
Solutions
44
8.1
Counting
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
45
8.2
Probability
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
48
8.3
Expectation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
54
Page 3

Discrete Mathematics and Probability Theory
aaalv.in/abcDMPT
0.1
Purpose
This compilation is (unoﬃcially) written for the Spring 2016 CS70: Discrete
Mathematics and Probability Theory class taught by Professor Satish Rao
and Professor Jean Walrand at UC Berkeley. It’s primary purpose is to oﬀer
additional practice problems and walkthroughs to build intuition, as a supplement
to oﬃcial course notes and lecture slides. Including more diﬃcult problems in
walkthroughs, there are over 35 exam-level problems.
0.1.1
Contributors
A Special Thanks to Sinho Chewi for spending many hours suggesting improvements,
catching bugs, and discussing ideas and solutions for problems with me. Additionally,
thanks to Dibya Ghosh and Blake Tickell, who helped review problems for clarity
and correctness.
0.1.2
Structure
Each chapter is structured so that this book can be read on its own.
A
minimal guide at the beginning of each section covers essential materials and
misconceptions but does not provide a comprehensive overview.
Each guide
is then followed by walkthroughs covering classes of diﬃcult problems and 3-5
exam-level (or harder) problems that I’ve written speciﬁcally for this book.
Note: As of Spring 2016, not all chapters have problems. However, all chapters
have at least a walkthrough. This will be amended in Fall 2016.
0.1.3
Breakdown
For the most part, guides are “cheat sheet”s for select chapters from oﬃcial
course notes, with additional comments to help build intuition.
For more diﬃcult parts of the course, guides may be accompanied by breakdowns
and analyses of problem types that might not have been explicitly introduced
in the course. These additional walkthroughs will attempt to provide a more
regimented approach to solving complex problems.
Problems are divvied up into two parts: (1) walkthroughs - a string of problems
that ”evolve” from the most basic to the most complex - and (2) exam-level
questions, erring on the side of diﬃculty where needed. The hope is that with
walkthroughs, students can reduce a relatively diﬃcult problem into smaller,
simpler subproblems.
0.1.4
Resources
Additional resources, including 20+ quizzes with
80 practice questions, and
other random worksheets and problems are posted online at alvinwan.com/cs70.
Page 4

Chapter 1
Modular Arithmetic and
Polynomials
1.1
Guide
1.1.1
Modular Arithmetic
In modulo p, only the numbers {0, 1, ..., p −1} exist. Additionally, division is
not well-deﬁned.
Instead, we deﬁne a multiplicative inverse.
We know that
outside a modulo ﬁeld, for any number n, an inverse n−1 multiplied by itself is
1. (n · n−1 = 1) Thus, we extend the deﬁnition of an inverse to the modulo ﬁeld
in this manner, where for any number n,
n · n−1 = 1
(mod p)
Do not forget that division, and thus, fractions do not exist in a modulo.
1.1.2
Polynomial Properties
For polynomials, we have two critical properties.
1. A polynomial of degree d has at most d roots.
2. A polynomial of degree d is uniquely identiﬁed by d + 1 distinct points.
Note that taking a polynomial over a Galois Field of modulo p (denoted GF(p))
simply means that all operations and elements in that ﬁeld are (mod p).
1.1.3
Fermat’s Little Theorem
Fermat’s Little Theorem states that if p is prime, for any a, ap = a (mod p). If
p does not divide a, we additionally know the following.
5

Discrete Mathematics and Probability Theory
aaalv.in/abcDMPT
ap−1 = 1
(mod p)
Applying Fermat’s Little Theorem repeatedly until the exponent of a is less
than p −1 gives us an interesting corollary: ay = ay mod p−1 (mod p)
1.1.4
Lagrange Interpolation
For a given set of points, compute ﬁrst the ∆for each coordinate, where
∆i = Πi̸=j
(x −xj)
(xi −xj)
Then, to recover the original polynomial, multiply all ∆i by the respective yis.
P(x) =
X
i
∆iyi
1.1.5
Error-Correcting Codes
Across a lossy channel, where at most k packets are lost, send n + k packets.
Across a corruption channel, where at most k packets are corrupted, send n+2k
packets. To recover a P across a corruption channel, apply Berkelamp-Welsh.
1.1.6
RSA
In RSA, we have a public key (N, e), where N is a product of two primes, p and
q, and e is co-prime to (p −1)(q −1). Here are Encrypt and Decrypt.
E(x) = xe
(mod N)
D(y) = yd = x
(mod N)
Why are they deﬁned this way? We have that y = E(x), so we plug in:
D(E(x)) = E(x)d = (xe)d = xed = x1
(mod N)
If the above equation xed = x is satisﬁed, then D(y) returns the original message.
How do we generate d? By Fermat’s Little Theorem’s corollary, we know ed = 1
(mod (p −1)(q −1)). Given we have e, we see that we can compute d if and
only if we know p and q. Thus, breaking RSA equates factorizing N into p, q.
1.1.7
Secret Sharing
In a secret-sharing problem, our goal is to create a secret such that only a group
meeting speciﬁc requirements can uncover it. We will explore secret sharing
problems in 3.2 Secret-Sharing Walkthrough.
Page 6

Discrete Mathematics and Probability Theory
aaalv.in/abcDMPT
1.2
Secret-Sharing Walkthrough
We begin with the most elementary form of secret-sharing, which requires
consensus from some subset of k people before the secret can be revealed.
Question: Basic
Construct a scheme that requires that at least k of n people to come together,
in order to unlock the safe.
Answer Polynomial of degree k −1.
We need at least k people, meaning this polynomial should require k points.
Thus, we create a polynomial of degree k −1, and distribute n distinct points
along this polynomial to n people. The secret is this polynomial evaluated at 0.
Question: Combining Polynomials
Develop a scheme that requires x1 people from group A and x2 people from B.
Answer polynomials of x1 −1, x2 −1 degrees, and a 1-degree polynomial using P1(0), P2(0)
Create a polynomial p1 of degree x1 −1 for A and a second polynomial p2 of
degree x2 −1 for B. Use the secrets of p1 and p2 (p1(0) and p2(0)) to create a
third polynomial p3 of degree 1. The secret is p3(0).
Question: Combining Polynomials Generalized
Construct a scheme that requires xi from each of the n groups of people.
Answer n Pi’s of xi −1 degree, and 1 n −1-degree polynomial using Pi(0) for all i.
Create n polynomials with degree xi −1 for the ith group. Use the secrets
(∀i, pi(0)) of all n polynomials to create an n + 1th polynomial of degree n −1.
The root of this n + 1th polynomial is the secret.
Question: Re-weighting
Each group elects oi oﬃcials. Construct a scheme that requires ai ≤oi oﬃcials
from each group, where 10 citizens can replace an oﬃcial.
Answer 10ai −1 polynomials, where each official gets 10 points and each citizen gets 1
Create a polynomial of degree 10ai −1, and give each of the ai oﬃcials 10 points
each. Then, give each citizen 1 point each. Use the secrets (∀i, pi(0)) of all n
polynomials to create an n + 1th polynomial of degree n −1. Since each oﬃcial
has 10 times the number of packets for the same polynomial, any 10 citizens
can ”merge” to become a single oﬃcial.
Question: Erasure Channel
Construct a scheme that requires k workers to unlock the safe. Make sure to
specify, if at most m workers do not respond to requests, how many workers
we need to ensure we can unlock the safe?
Answer k + m, one k −1-degree polynomial
Page 7

Discrete Mathematics and Probability Theory
aaalv.in/abcDMPT
The intuitive response to simply request the number of people that may not
respond in addition to the number of people we need to unlock the safe. Thus,
we need to request k+m workers to reconstruct a degree k−1-degree polynomial.
Question: Corruption Channel
Construct a scheme that requires k workers to unlock the safe. Make sure to
specify, if at most m workers mis-remember information, how many people we
need to ensure we can unlock the safe.
Answer k + 2m, one k −1-degree polynomial
Per our knowledge of corruption channels, we need to n + 2k packets, or in this
case, k + 2m workers, where again, we construct a degree k −1 polynomial.
Page 8

Chapter 2
Counting
2.1
Guide
Counting bridges discrete mathematics and probability theory, to some degree
providing a transition from one to the next. Albeit a seemingly trivial topic,
this section provides foundations for probability.
2.1.1
Fundamental Properties
We have two counting properties, as follows:
1. If, for each of k items, we have {n1, n2, ..., nk} options, the total number
of possible combinations is n1 · n2 · · · nk = Πini.
2. To ﬁnd the total number of un-ordered combinations, divide the number
of ordered combinations by the number of orderings.
 n
k

=
n!
(n−k)!k!
2.1.2
Stars and Bars
Given n balls and k bins, count all the ways to distribute n balls among k bins.
Given a list of n balls, we need to slice this list in k−1 places to get k partitions.
In other words, given all of our stars (balls, n) and bars (“slices,” k −1), we
have n + k −1 total items. We can either choose to place our k −1 bars or our
n stars. Thus, the total number of ways to distribute n balls among k bins:
n + k −1
n

=
n + k −1
k −1

2.1.3
Order, Replacement, and Distinguishability
This is a list of translations, from “counting” to “English”. One of the most
common mistakes over-counting or under-counting the number of combinations.
9

Discrete Mathematics and Probability Theory
aaalv.in/abcDMPT
• Indistinguishable balls =⇒Order doesn’t matter, combinations
• Distinguishable balls =⇒Order does matter, permutations
• Only one ball in each bin =⇒Without replacement
• Multiple balls allowed in each bin =⇒With replacement
2.1.4
Combinatorial Proofs
Several of the following are broadly applicable, for all sections in probability.
However, we will introduce them here, as part of a set of approaches you can
use to tackle combinatorial proofs.
• Addition is Or, and multiplication is And.
– ”Expand” coeﬃcients. 2
 n
2

=
 n
2

+
 n
2

, so consider all pairs from n
Or consider all pairs from (another) n.
– Distribute quantities.
 n
2
a+b =
 n
2
a n
2
b, so we consider all pairs
from n, a times And consider all pairs from (another) n, b times.
• Switch between equivalent forms for combinations, to see which makes
more sense.
– Rewrite quantities as ”choose 1”. n =
 n
1

, so we pick one from n
items.
– Toggle between the two quantities.
 a+b
b

=
 a+b
a

, as choosing b is
the same as choosing a from a + b.
• Try applying the ﬁrst rule of counting as well.
– 2n is the equivalent of picking all (possibly empty) subsets. In other
words, we consider 2 possibilities for all n items {Include, Don’t Include}.
–
 n
k

k! =
n!
(n−k)!, which is k samples from n items without replacement.
• Make sure to not prove equality mathematically, or attempt to just write
in words what happens mathematically.
2.1.5
Inclusion Exclusion Principle
This section prevents over-counting; we can visualize this as Venn Diagrams.
For, |A ∪B|, note that the area denoting A ∩B is duplicated. So, we subtract
the intersection to get the space of all A and B. Thus,
|A ∪B| = |A| + |B| −|A ∩B|
|A ∪B ∪C| = |A| + |B| + |C| −|A ∩B| −|B ∩C| −|C ∩A| + |A ∩B ∩C|
Page 10

Discrete Mathematics and Probability Theory
aaalv.in/abcDMPT
2.2
Stars and Bars Walkthrough
Stars and bars can come in many diﬀerent types and ﬂavors. The following
build on one case, building in complexity. Here are several other stars and bars
variations:
• All possible phone numbers, given that the digits sum to a particular
value.
• Distributing chairs among rooms, given we have only a particular number
of chairs.
• Bringing at least k of each sock, given you can only ﬁt n socks in your
suitcase.
Notice that in all of the mentioned scenarios, there is some “bound” on the
number of items we can distribute. That should immediately trigger “stars and
bars”.
Question: Basic
How many ways are there to sprinkle 10 oreo pieces on our 3 scoops of ice
cream? Assume that each scoop is a diﬀerent ﬂavor of ice cream. (i.e., Each
scoop is distinguishable.)
Answer
12
2

This reduces to a stars and bars problem, where we have 3 possible ”buckets”
(2 bars) and 10 stars. Note that to specify 3 buckets, we need only 2 bars to
separate the 10 stars. Thus, we can choose the 2 bars or the 10 balls from 12
slots.
Question: At Least
How many ways are there to sprinkle 10 sprinkles on 3 scoops, such that the
ﬁrst scoop gets at least 5 pieces?
Answer
7
2

This reduces to a second stars and bars problem. Simply, we ﬁrst give the ﬁrst
scoop 5 pieces. Why do this? This means that regardless of however many
additional pieces we distribute it, the ﬁrst scoop will have at least 5 pieces.
We are then left with a sub-stars-and-bars problem, with 10 −5 = 5 sprinkles
and 3 scoops. We proceed as usual, noting this is 5 stars and 2 bars.
Question: At Most
Assume that each scoop can only hold a maximum of 8 pieces. How many
ways are there to sprinkle 10 sprinkles on 3 scoops?
Answer
12
2

−
3
1
2
1

−
3
1

Page 11

Discrete Mathematics and Probability Theory
aaalv.in/abcDMPT
First, we count all the possible ways to distribute 10 oreo sprinkles among 3
scoops. This is the answer to the last quiz, problem 3:
 12
2

Then, we count the number of invalid combinations. The only invalid combinations
are when a scoop has 9 or more sprinkles. We consider each case:
1. One scoop has 9 sprinkles. There are
 3
1

ways to pick this one scoop with
9 sprinkles. Then, there are two other scoops to pick from, to give the
ﬁnal sprinkle, making
 2
1

ways to “distribute” the last sprinkle.
2. One scoop has 10 sprinkles. There are
 3
1

ways to pick this one scoop.
There are no more sprinkles for the other scoops.
Thus, we take all combinations and then substract both invalid combinations.
We note that the invalid combinations are mutually exclusive.
Question: At Least and At Most
Assume that each scoop can only hold a maximum of 8 pieces and a minimum
of 2. How many ways are there to sprinkle 14 sprinkles on our 3 scoops of ice
cream?
Answer
10
2

−
3
1
2
1

−
3
1

Given the ﬁrst problem in this walkthrough, we know that we can reduce the
problem to a sub-stars-and-bars problem. We ﬁrst distribute 2 sprinkles to each
scoop, guaranteeing that each scoop will have at least 2 sprinkles distributed to
it.
Then, we count all the possible ways to distribute 8 oreo sprinkles among 3
scoops. This is - by stars and bars -
 10
2

.
Then, we count the number of invalid combinations. Since each scoop already
has 2 sprinkles, it can take at most 6 more, to satisfy the 8-sprinkles maximum.
Thus, the invalid combinations are when we distribute 7 or more sprinkles to a
single scoop.
We consider each case:
1. One scoop has 7 sprinkles. There are
 3
1

ways to pick this one scoop with
7 sprinkles. Then, there are two other scoops to pick from, to give the
ﬁnal sprinkle, making
 2
1

ways to “distribute” the last sprinkle.
2. One scoop has 8 sprinkles. There are
 3
1

ways to pick this one scoop.
There are no more sprinkles for the other scoops.
Thus, we take all combinations and then substract both invalid combinations.
We note that the invalid combinations are mutually exclusive, making
 10
2

−
 3
1
 2
1

−
 3
1

.
Page 12

Discrete Mathematics and Probability Theory
aaalv.in/abcDMPT
2.3
Problems
1. If we roll a standard 6-sided die 3 times, how many ways are there to roll
a sum total of 14 pips where all rolls have an even number of pips?
2. Given a standard 52-card deck and a 5-card hand, how many unique hands
are there with at least 1 club and no aces?
3. Given a standard 52-card deck and a 5-card hand, how many unique hands
are there with at least 1 club or no aces?
4. Given a standard 52-card deck and a 3-card hand, how many unique
hands are there with cards that sum to 15? (Hint: Each card is uniquely
identiﬁed by both a number and a suit. This problem is more complex than
phone numbers.)
Page 13

Chapter 3
Probability
3.1
Guide
3.1.1
Random Variables
Let Ωbe the sample space.
A random variable is by deﬁnition a function
mapping events to real numbers. X : Ω→R, X(ω) ∈R. An indicator variable is
a random variable that only assumes values {0, 1} to denote success or failure for
a single trial. Note that for an indicator, expectation is equal to the probability
of success:
E[Xi] = 1 · P[Xi = 1] + 0 · P[Xi = 0] = P[Xi = 1]
3.1.2
Law of Total Probability
The law of total probability states that Pr[A] = Pr[A|B]Pr[B]+Pr[A| ¯B]Pr[ ¯B],
if the only values of B are B and ¯B. More generally speaking, for a set of Bi
that partition Ω,
Pr[A] =
X
i
Pr[A|Bi]Pr[Bi]
Do not forget this law. On the exam, students often forget to multiply by Pr[Bi]
when computing Pr[A].
3.1.3
Conditional Probability
Conditional probability gives us the probability of an event given priors. By
deﬁnition, the probability of A given B is deﬁned to be
Pr[A|B] = Pr[A ∩B]
Pr[B]
14

Discrete Mathematics and Probability Theory
aaalv.in/abcDMPT
3.1.4
Bayes’ Rule
Bayes’ expands on this idea, using the Law of Total Probability.
Pr[A|B] = Pr[B|A]Pr[A]
Pr[B]
3.1.5
Independence
Note that if the implication only goes in one direction, the converse is not
necessarily true. This is a favorite for exams, where the crux of a True-False
question may be rooted in a converse of one of the following implications.
X, Y independent ⇔(Pr[XY ] = Pr[X]Pr[Y ])
X, Y independent ⇒(E[XY ] = E[X]E[Y ])
X, Y independent ⇒(Var(X + Y ) = Var(X) + Var(Y ))
X, Y independent ⇒(Cov(X, Y ) = 0)
Using the above deﬁnition for independence with probabilities, we have the
following corollary.
X, Y independent ⇔(Pr[X|Y ] = Pr[X, Y ]
Pr[Y ]
= Pr[X]Pr[Y ]
Pr[Y ]
= Pr[X])
3.1.6
Symmetry
Given a set of trials, the principle of symmetry states that the probability of
each trial is independent of other trials, without additional information. See 3.2
Symmetry Walkthrough for more details and concrete examples.
Page 15

Discrete Mathematics and Probability Theory
aaalv.in/abcDMPT
3.2
Symmetry Walkthrough
Let us take 10 marbles from a bag of N marbles, of which r > 10 are red,
without replacement. Let Xi be an indicator equal to 1 if and only if the ith
marble is red.
Question: Single Unconditioned, None Conditioned
With what probability is the ﬁrst marble red? The second? Third? The tenth?
Answer All:
r
N
The probability of the ﬁrst marble being red is
r
N . However, by symmetry, the
probability of each marble being red is also
r
N ! We know this is true, because
we are not given any information about any of the marbles. With or without
replacement, symmetry applies. However, as we will see, symmetry breaks down
or applies to a limited degree when we are given information and condition our
probabilities.
Question: Single Unconditioned, Single Conditioned
Given that the ﬁrst marble is red, with what probability is the second marble
red? The third? The tenth?
Answer ∀1 < i ≤10, r−1
N−1
We are given that the ﬁrst marble is red. As a result, we are actually computing
Pr[Xi = 1|X1 = 1]. Since one red marble has already been removed, we have
that for the second marble, Pr[X2 = 1|X1 = 1] =
r−1
N−1. Again, by symmetry,
we in fact know that this is true for all i > 1, as we do not have additional
information that would tell us otherwise.
Question: Single Unconditioned, Multiple Conditioned
Given that the ﬁrst and ﬁfth marbles are red, with what probability is the
second marble red? The tenth?
Answer
r−2
N−2
The ”position” of the marble that we have information about, does not matter.
Thus, again applying symmetry, we have that all remaining marbles have probability
r−2
N−2 of being red.
Question: Multiple Unconditioned, “None” Conditioned
What is the probability that the ﬁrst two marbles are red? The second and
third? The ninth and tenth?
Answer
r
N
r−1
N−1
Again, by symmetry, we can argue that regardless of which two marbles, the
probability that any pair is red, is the probability that one pair is red. Note
that symmetry doesn’t apply within the pair, however. When considering the
“second” marble, we know that the “ﬁrst” marble is necessarily red.
When
computing the probability that the ﬁrst and second marbles are red, we are
then computing Pr[X1]Pr[X2|X1], which is
r
N
r−1
N−1.
Page 16

Discrete Mathematics and Probability Theory
aaalv.in/abcDMPT
Question: Multiple Unconditioned, Multiple Conditioned
Given that the ﬁrst marble is red and the second is not red, what is the
probability that the seventh marble is red and the ninth marble is not red?
Answer
r−1
N−2
N−r−1
N−3
At this point, it should be apparent that we could have asked for the probability
that any two marbles are red. For the seventh marble, which is red, we consider
the number of red marbles we have no information about, which is r −1 and
the total number of marbles we have no information about, N −2. This makes
the probability that the seventh marble is red,
r−1
N−2.
For the tenth marble, there are now N −3 marbles we do not have information
about. There are r −2 red marbles we do not have information about. Thus,
the numebr of remaining non-red marbles is (N −3) −(r −2) = N −r −1,
making the probability that the tenth marble is not red, N−r−1
N−3 .
Page 17

Discrete Mathematics and Probability Theory
aaalv.in/abcDMPT
3.3
Problems
1. We sample k times at random, without replacement, a coin from our wallet.
There are p pennies, n nickels, d dimes, and q quarters, making N total
coins. Given that the ﬁrst three coins are pennies, what is the probability
that we will sample 2 nickels, 2 pennies, and 1 dime next, in any order?
2. We are playing a game with our apartment-mate Wayne. There are three
coins, one biased with probability p of heads and the other two fair coins.
First, each player is assigned one of three coins uniformly at random.
Players then ﬂip simultaneously, where each player earns h points per
head. The winning player is the one with the most points. If Wayne earns
k points after n coin ﬂips, what is the probability that Wayne has the
biased coin?
3. Let X and Y be the results from two numbers, chosen uniformly randomly
in the range {1, 2, 3, ..., k}. Deﬁne Z = |X −Y |.
(a) Find the probability that Z < k −2.
(b) Find the probability that Z ≥2.
4. Consider a 5 in. × 3 in. board, where each square inch is occupied by a
single tile. A monkey hammers away at the board, choosing a position
uniformly at random; assume a single strike completely removes a tile from
that position. (Note that the monkey can choose to strike a position with
no tiles.) By knocking oﬀtiles, the monkey can create digits. For example,
the following would form the digit “3”, where 0 denotes a missing tile and
1 denotes a tile.


0
0
0
1
1
0
0
0
0
1
1
0
0
0
0


(a) Given the monkey strikes n times, where n > 15, what is the probability
that the monkey knocks out tiles, such that the board forms “8”?
(b) Given the monkey strikes n times, where n > 15, what is the probability
that the monkey knocks out tiles, such that the board forms “2”?
(c) Given the monkey strikes n times, where n > 15, what is the probability
that the monkey knocks out tiles, such that the board forms an even
number?
(d) Which digit is most likely to occur?
Page 18

Chapter 4
Expectation
4.1
Guide
With expectation, we begin to see that some quantities no longer make sense.
Expressions that we compute the expectation for may in fact be far detached
from any intuitive meaning. We will speciﬁcally target how to deal with these,
in the below regurgitations of expectation laws and deﬁnitions.
4.1.1
Expectation Deﬁnition
Expectation is, intuitively, the mean. We multiply all values by the probability
that that value occurs. The formula is as follows:
E[X] =
X
x∈R
xPr[X = x]
However, we also know the following.
E[g(X)] =
X
x∈R
g(x)Pr[X = x]
Said another way, the expression in E[...], which we will call g(X) can be
needlessly complex. To solve such an expectation, simply plug in the expression
into the summation. For example, say we need to solve for E[X2/5]. This makes
little intuitive sense. However, we know the expression in terms of X aﬀects
only the value.
E[X2/5] =
X
x2/5Pr[X = x]
This also extends to multiple random variables, so
19

Discrete Mathematics and Probability Theory
aaalv.in/abcDMPT
E[g(X, Y )] =
X
x,y∈R
g(x, y)Pr[X = x, Y = y]
For example, we have that
E[(X + Y )2] =
X
x,y∈R
(x + y)2Pr[X = x, Y = y]
4.1.2
Linearity of Expectation
Regardless of independence, the linearity of expectation always holds.
Said
succinctly, it is true that E[P
i aiXi] = P
i aiE[Xi]. Said once more in a less
dense format, using constants ai and random variables xi:
E[a1X1 + a2X2 + · · · + anXn] = a1E[X1] + a2E[X2] + · · · + anE[Xn]
Given a more complex combination of random variables, apply linearity of
expectation to solve.
4.1.3
Conditional Expectation
Here, we expand our deﬁnition of expectation.
E[Y |X = x] =
X
y
yPr[Y = y|X = x]
We know how to solve for Pr[Y = y|X = x], using deﬁnitions from the last
chapter.
4.1.4
Law of Total Expectation
The Law of Total Expectation states simply that
E[E[Y |X]] = E[Y ]
However, the projection property provides a more general form, showing that
the law of total expectation is actually a special case where f(x) = 1.
E[E[Y |X]f(X)] = E[Y f(X)]
Page 20

Discrete Mathematics and Probability Theory
aaalv.in/abcDMPT
4.2
Linearity of Expectation Walkthrough
Let us begin with a simple linearity of expectation problem, where of the random
variables are independent.
However, as we will see, linearity of expectation
always holds, regardless of dependence between the involved random variables.
From this walkthrough, you should learn:
• When a question asks “how many”, immediately think “indicator variable”.
The indicator variable is then used to indicate whether or not the ith trial
is a success. Sum over all indicators to compute the number of successes.
• Option 1 for computing expectation: Directly compute a complex
expression for random variables, using the fact that E[g(X, Y )] = P g(x, y)Pr[X =
x, Y = y]
• Option 2 for computing expectation: We can expand an expectation
that makes little intuitive sense, such as E[(X+Y )2] = E[X2+2XY +Y 2].
Using linearity of expectation, we can then compute each term separately.
Question: Variables
Knowing that E[X] = 3, E[Y ] = 2, E[Z] = 1, compute E[1X + 2Y + 3Z].
Answer 10
Use linearity of expectation to expand E[X+2Y +3Z] = E[X]+2E[Y ]+3E[Z] =
3 + 2(2) + 3(1) = 10.
Question: Independence
Let Z = X1 + X2 + X3 + X4, where each Xi is the number of pips for a dice
roll. What is E[Z]?
Answer 14
We begin by computing E[Z] = E[X1 + X2 + X3 + X4], which, by linearity of
expectation, is the same as E[X1] + E[X2] + E[X3] + E[X4]. All of the Xis are
the same, making ∀i, E[Xi] = 7
2. This makes E[Z] = 4E[X1] = 4 7
2 = 14.
We will now see a more surprising application of linearity of expectation, where
the random variables are dependent.
Question: Dependence
Consider a bag of k red marbles and k blue marbles. Without replacement, we
pull 4 marbles from the bag in sequential order; what is the expected number
of red marbles?
Answer 2
The question asks for ”how many,” so we know we need indicators to count
successes. Let us deﬁne Z = P4
i=1 Xi, where each Xi is 1 if the ith marble is
red. We know, E[Z] = E[P4
i=1 Xi], and by linearity of expectation
Page 21

Discrete Mathematics and Probability Theory
aaalv.in/abcDMPT
E[Z] =
4
X
i=1
E[Xi]
For all indicators, E[X] = P[X = 1].
=
4
X
i=1
P[Xi = 1]
=
4
X
i=1
1
2
= 41
2 = 2
As linearity of expectation shows, the probability of a red marble on the ﬁrst
marble is the same as the probability of a red marble on the fourth.
This
symmetry applies to any set of samples where we are not given additional
information about the samples drawn.
Question: Algebra with Independence
Consider three random variables X, Y , and Z, where X is the outcome of a
dice roll, Y is the outcome of another dice roll, and Z = Y + X (Z be the sum
of the two dice roll outcomes). Compute E[Z] and E[Z2]. Is E[Z2] = E[Z]2?
Answer 7, 38.5, No
First, we’ll compute E[Z].
E[Z] = E[X + Y ]
= E[X] + E[Y ]
= 7
2 + 7
2
= 7
E[Z2] is not guaranteed to be 49. E[Z2] ̸= E[Z]2 necessarily. Whereas it is
possible, it is not guaranteed.
E[Z2] = E[(X + Y )2]
= E[X2 + 2XY + Y 2]
= E[X2] + 2E[XY ] + E[Y 2]
We will compute each term separately, to provide clarity.
First, we compute 2E[XY ]. Since, X and Y are independent, we can rewrite
this as
2E[X]E[Y ] = 2(7
2)(7
2) = 49
2
Page 22

Discrete Mathematics and Probability Theory
aaalv.in/abcDMPT
Second, we compute E[X2]. This is given in the previous part.
Third, note that E[X2] = E[Y 2], since these are both dice rolls. Thus,
E[X2] + 2E[XY ] + E[Y 2]
= 91
6 + 49
2 + 91
6
= 329
6
This makes E[Z2] = 54.8, which is not E[Z]2 = 49.
Question: More Dependence
Angie goes to the music building daily to practice singing. Each day, she
chooses one of n pieces at random. Given some m < n of the pieces are arias,
let us impose an absolute ordering on these arias. If Angie practices for k
days, how many times will Angie practice all arias in sequential order, without
repeating an aria? (Note that this means Angie will necessarily spend m days,
practicing one aria a day, to ﬁnish one sequence.)
Answer (k −m + 1)
1
nm
The question asks for ”how many,” so we know we need indicators to count
successes. We deﬁne X to be the total number of sequences and X = P
i Xi
where Xi is be 1 iﬀAngie begins a sequence on day i. This means that the
last day Angie can begin a sequence is k −m + 1. Thus, we actually consider
k −m + 1 trials.
Now, we compute the probability that, of m trials, Angie picks exactly the right
m arias in sequential order. Thus, the probability for a particular Xi is
1
nm .
E[X] = E[
X
i
Xi]
=
X
i
E[Xi]
= (k −m + 1)E[Xi]
= (k −m + 1) 1
nm
Page 23

Discrete Mathematics and Probability Theory
aaalv.in/abcDMPT
4.3
Dilution Walkthrough
For the following problems, some answers are so complex syntactically that
placing an upside-down miniature version would render the answer completely
illegible. As a consequence, most answers in this section are placed right-side
up in plain sight.
It is now year 3000 where watermelon is a sacred fruit. Everyone receives N
watermelons at birth. However, citizens of this future must participate in the
watermelon ceremonies, annually. At this ritual, citizens can choose to pick 1
melon at random, to replace with a cantaloupe.
Question: Two Cases
Given that a citizen has m watermelons at the nth year, what are all the
possible number of watermelons that this citizen can have in the n + 1th year,
and what is the probability that each of these situations occur?
Xn+1 =
(
m
w.p.1 −m
N
m −1
w.p. m
N
In the ﬁrst case, our number of watermelons does not change. This only occurs
if our pick is a cantaloupe. Since there are m watermelons, there are N −m
cantaloupes. Thus, the probability of picking a single cantaloupe is N−m
N
=
1 −m
N .
The second case falls out, as there are m watermelons, making m
N .
Question: Three Cases
Let us suppose that a citizen now picks two watermelons at random, at this
ritual. Given that a citizen has m watermelons at the nth year, what are all
the possible number of watermelons that this citizen can have in the n + 1th
year, and what is the probability that each of these situations occur?
Xn+1 =







m
w.p. (N−m)(N−m−1)
N(N−1)
m −1
w.p. 2m(N−m)
N(N−1)
m −2
w.p. m(m−1)
N(N−1)
In the ﬁrst case, our number of watermelons does not change. This only occurs
if both of our picks are cantaloupes. Since there are m watermelons, there are
N −m cantaloupes. Thus, the probability of picking a single cantaloupe is N−m
N
.
The probability of picking two cantaloupes is (N−m)(N−m−1)
N(N−1)
.
Likewise, if the number of watermelons decreases by 1, we have chosen one
watermelon and one cantaloupe. This means we either chose the watermelon
second and the cantaloupe ﬁrst N−m
N
m
N−1, or we chose the watermelon ﬁrst and
the cantaloupe second m
N
N−m
N−1 . Summed together, we have that the probability
of one watermelon and one cantaloupe is 2m(N−m)
N(N−1) .
Page 24

Discrete Mathematics and Probability Theory
aaalv.in/abcDMPT
Finally, the probability of picking two watermelons is m(m−1)
N(N−1).
Question: Conditional Expectation, Two Cases
Again, let us consider the original scenario, where each citizen picks only 1
melon at random at the ritual. Given that a citizen has m watermelons at the
nth year, how many watermelons will a citizen then have in year n + 1, on
average?
Answer Xn(1 −
1
N )
We are eﬀectively computing E[Xn+1|Xn = m].
We already considered all
possible values of Xn+1 with their respective probabilities. So,
E[Xn+1|Xn = m] =
X
x
E[Xn+1 = x|Xn = m]
=
X
x
xPr[Xn+1 = x|Xn = m]
= m(1 −m
N ) + (m −1)m
N
= m −m2
N + m2
N −m
N
= m −m
N
= m(1 −1
N )
Since Xn = m, we substitute it in.
= Xn(1 −1
N )
Question: Conditional Expectation, Three Cases
Let each citizen pick two melons per ritual. Given that a citizen has m
watermelons at the nth year, how many watermelons will a citizen then have
in year n + 1, on average?
Answer m(1 −
2
N )
We are eﬀectively computing E[Xn+1|Xn = m].
We already considered all
possible values of Xn+1 with their respective probabilities. So,
E[Xn+1|Xn = m] =
X
x
xPr[Xn+1 = x|Xn = m]
= m(N −m)(N −m −1) + (m −1)2m(N −m) + (m −2)m(m −1)
N(N −1)
= m(N −1)(N −2)
N(N −1)
= mN −2
N
Page 25

Discrete Mathematics and Probability Theory
aaalv.in/abcDMPT
Since Xn = m, we substitute it in.
E[Xn+1|Xn] = Xn(1 −2
N )N
Question: Expectation
Let each citizen pick two melons per ritual. After n years, compute the
average number of watermelons a particular citizen will have left.
Answer (1 −
2
N )n−1N
We are now computing E[Xn]. First, we note that the law of total expectation
allows us to conclude the following.
E[Xn+1] = E[E[Xn+1|Xn]]
= E[Xn(1 −2
N )]
= (1 −2
N )E[Xn]
We have the following relationship.
E[Xn] = (1 −2
N )E[Xn−1]
Since E[Xn] is recursively deﬁned, we see that the constant in front of E[Xn−1]
will simply be multiplied repeatedly.
Thus, we can express this in terms of
E[X1].
E[Xn] = (1 −2
N )n−1E[X1]
Finally, we note that we began with N watermelons, so E[X1] = N.
E[Xn] = (1 −2
N )n−1N
Question: Algebra
Let each citizen pick two melons per ritual. If all citizens begin with 100
watermelons, what is the minimum number of years such that the expected
number of cantaloupes a citizen has is at least 99?
Answer 229
Just plug into our expression for E[Xn]. 99 cantaloupes means 1 watermelon.
Thus, we are solving for E[Xn] = 1.
E[Xn] = (1 −
2
100)n−1100 = 1
Page 26

Discrete Mathematics and Probability Theory
aaalv.in/abcDMPT
(49
50)n−1 =
1
100
log (49
50)n−1 = log 1
100
Using the log rule, log an = n log a, we get:
(n −1) log 99
100 = log 1
100
n −1 = log
1
100
log 49
50
n = 1 + log
1
100
log 49
50
n ≈229
It will take approximately 229 years.
Page 27

Discrete Mathematics and Probability Theory
aaalv.in/abcDMPT
4.4
Problems
1. Consider a set of books B. The spine of each book has a single character
inscribed, and this character is the only distinguishing characteristic. Placed
side-by-side in the right order, the spines of all books in B spell some string
s, ”RaoWalrandRule”. What is the expected number of times s shows
up, if a monkey picks n books from B uniformly at random and places
them, uniformly at random, on a shelf? The books may be upside-down,
but the spines are always facing out. Hint: Do repeating characters aﬀect
the probability?
2. Four square inch tiles make up a 2 in. × 2 in. insignia, and each tile is
distinct. We have 2572 such tiles. When the tiles are randomly arranged
in a 257 × 257 grid, what is the expected number of properly-formed
insignias? Assume that tiles can be rotated in any of 4 orientations but
not ﬂipped.
3. Let Xi be the event that the ith dice roll has an even number of pips. Let
Z be the product of all Xi, from 1 to 10. Formally, Z = Π10
i=1Xi. Let Y
be the sum of all Xi, from 1 to 10. Formally, Y = P10
i=1 Xi.
(a) Compute E[Y |Z]. Remember that your result is a function of Z.
(b) Using your derivation from part (a), compute E[Y |Z > 0]. Explain
your answer intuitively.
(c) Using your derivation from part (a), compute E[Y |Z = 0].
Page 28

Chapter 5
Distributions and
Estimation
5.1
Guide
Distributions help us model common patterns in real-life situations.
In the
end, being able to recognize distributions quickly and eﬀectively is critical to
completing diﬃcult probability problems.
5.1.1
Important Distributions
Here are several of the most important distributions and when to use them.
• Binomial Distribution: Number of successes in n trials, where each
trial has probability p of success
• Geometric Distribution: Number of trials until the ﬁrst success, where
each trial is independent and each trial has probability p of success
• Poisson Distribution: The probability of k successes per n trials or a
unit of time, given some average number of successes
5.1.2
Combining Distributions
• The minimum across k geometric distributions, each with the same parameter
p, is X′ ∼Geo((1 −p)k).
• The sum of any k Poisson distributions is another Poisson distribution
X′ ∼Pois(Pk
i λi).
5.1.3
Variance
Variance is by deﬁnition equal to E[(X −E[X])2]. After a brief derivation, we
get that Var(X) is then the following.
29

Discrete Mathematics and Probability Theory
aaalv.in/abcDMPT
Var(X) = E[X2] −E[X]2
Now, we discuss how to algebraically manipulate variance.
First, note that
shifting all values of X by a constant c will not change the variance. Second,
pulling out constants from inside the variance squares the value.
Var(X + c) = Var(X)
Var(aX) = a2Var(X)
5.1.4
Covariance
Covariance is by deﬁnition equal to E[(X −E[X])(Y −E[Y ])]. After a brief
derivation, we get that Cov(X, Y ) is then equal to the following.
Cov(X, Y ) = E[XY ] −E[X]E[Y ]
Now, we discuss how to algebraically manipulate Cov. First, we can split sums.
Second, we can move constants out and apply the constant to either variable.
Cov(X1 + X2, Y ) = Cov(X1, Y ) + Cov(X2, Y )
aCov(X, Y ) = Cov(aX, Y ) = Cov(X, aY )
5.1.5
Linearity of Variance
The variance of two random variables summed together is.
Var(X + Y ) = Var(X) + 2Cov(X, Y ) + Var(Y )
However, if X and Y are independent, we have that Cov(X, Y ) = 0 and
Var(X + Y ) = Var(X) + Var(Y ). More generally, if all Xis are independent,
the linearity of variance holds Var(X1 +...Xn) = Var(X1)+...+Var(Xn), or:
Var(
X
i
Xi) =
X
i
Var(Xi)
If all Xi share a common distribution with identical parameters, then we also
know P
i Var(Xi) = n · Var(Xi).
5.1.6
Linear Regression
L[Y |X] = E[Y ] + Cov(X, Y )
Var(X) (X −E[X])
Page 30

Discrete Mathematics and Probability Theory
aaalv.in/abcDMPT
5.2
Variance Walkthrough
We will begin with the most basic form of variance, with independent random
variables. There are several important takeaways from this walkthrough.
1. If events Xi are mutually independent, then var(P
i Xi) = P
i var(Xi).
2. The converse of statement is not necessarily true.
3. Otherwise, we apply the more general deﬁnition of variance, that var(X) =
E[X2] −E[X]2.
Question: Variance Algebra
Let X be the number of pips for a single dice roll. Compute Var(6X2 + 3).
Answer 5369
Shifting a distribution (or similarly, a random variable’s values) by a constant
does not aﬀect the variance.
Var(6X2 + 3) = Var(6X2)
We pull out the constant, and substitute the equation in.
= 36Var(X2)
= 36(E[X4] −E[X2]2)
We know that E[X2] = 12+22+32+42+52+62
6
= 91
6 . Likewise, E[X4] = 14+24+34+44+54+64
6
=
2275
6 . Finally, plug in and simplify.
= 36(2275
6
−(91
6 )2)
= 36(2275
6
−8281
36 )
= 6 · 2275 −8281
= 5369
Question: Independent Events
Let X and Y be the number of pips for two separate dice rolls. Compute
Var(
√
6X −
√
6Y + 3).
Answer 35
We ﬁrst know that the shift does not aﬀect variance. We apply linearity of
variance, as X and Y are independent. Thus,
Page 31

Discrete Mathematics and Probability Theory
aaalv.in/abcDMPT
Var(
√
6X −
√
6Y + 3) = Var(
√
6X −
√
6Y )
= Var(
√
6X) + Var(−
√
6Y )
= 6(Var(X) + Var(Y ))
We can compute Var(X) = E[X2] −E[X]2 separately. We know that E[X] =
1+2+3+4+5+6
6
= 7
2. Likewise, E[X2] = 12+22+32+42+52+62
6
= 91
6 . Thus,
Var(X) = E[X2] −E[X]2
= 91
6 −(7
2)2
= 91
6 −49
4
= 35
12
Now, we substitute in.
Var(
√
6X −
√
6Y + 3) = 6(Var(X) + Var(Y ))
= 6(35
12 + 35
12)
= 635
6
= 35
Page 32

Chapter 6
Bounds
6.1
Guide
6.1.1
Markov’s Inequality
Markov’s inequality oﬀers a bound in one direction. Intuitively, it gives us an
upper bound on the probability we are greater than some value. Keep in mind
that a ≥0, and X cannot take negative values.
Pr[X > a] ≤E[X]
a
More generally, for a strictly non-negative, monotonically increasing function f,
Pr[X > a] ≤E[f(X)]
f(a)
6.1.2
Chebyshev’s Inequality
Keep in mind that a ≥0, and intuitively, it gives an upper bound on the
probability that we are more than a distance a from the mean.
Pr[|X −E[X]| ≥α] ≤Var(X)
α2
However, we may be interested in a lower bound on the probability that we are
less than a distance a from the mean. Thus, if Chebyshev’s oﬀers a bound of p,
we are actually interested in 1 −p.
6.1.3
Law of Large Numbers
Let E[ ¯X] be the average across all samples of data. Let E[X] be the actual
average.
The Law of Large Numbers states that with many i.i.d.
random
variables, E[ ¯X] approaches E[X].
33

Discrete Mathematics and Probability Theory
aaalv.in/abcDMPT
6.2
Conﬁdence Intervals Walkthrough
This question was taken from the Spring 2016 CS70 Discussion 12.
On the planet Vegas, everyone carries a coin. Many people are honest and carry
a fair coin (heads on one side and tails on the other), but a fraction p of them
cheat and carry a trick coin with heads on both sides. You want to estimate p
with the following experiment: you pick a random sample of n people and ask
each one to ﬂip his or her coin. Assume that each person is independently likely
to carry a fair or a trick coin.
Question: Estimation
Given the results of your experiment, how should you estimate p?
We are looking for ˜p, the fraction of people with trick coins, so let us begin by
assuming that the fraction of people with trick coins is ˜p.
Let ˜q be the fraction of people we observe with heads, in terms of ˜p.
˜q = (1)˜p + 1
2(1 −˜p)
This implies that
2˜q = 2˜p + (1 −˜p)
˜p = 2˜q −1
Note that ˜p is the fraction of people we think have trick coins. This is diﬀerent
from p, which is the actual fraction of people with trick coins. To express the
actual p in terms of our actual q, we rewrite the tilde expressions. Note that this
is in theory dependent on the people we sample, so this is only an approximate
equality that should be true as we approach an inﬁnite number of samples.
p ≈2q −1
Question: Chebyshev’s
How many people do you need to ask to be 95% sure that your answer is oﬀ
by at most 0.05?
We are looking for the diﬀerence between ˜p and p to be less than 0.05 with
probability 95%. We ﬁrst note that Chebyshev’s inequality naturally follows,
as Chebyshev’s helps us ﬁnd distance from the mean with a certain probability.
Formally, this is Chebyshev’s:
Pr[|X −µ| ≥a] ≤var(X)
a2
Page 34

Discrete Mathematics and Probability Theory
aaalv.in/abcDMPT
However, we are interested in ﬁnding an n so that we are oﬀby at most 0.05 with
probability 95%. This is equivalent to being oﬀby at least 0.05 with probability
5%. The latter is answerable by Chebyshev’s.
Then, we follow three steps.
Step 1 : Fit to |X −µ| ≥a
We ﬁrst only deal with ”your answer is oﬀby at most 0.05”. We can re-express
this mathematically, with the following:
|˜p −p| < 0.05
We don’t have ˜p, however, so we plug in q for our ˜p.
|(2˜q −1) −(2q −1)| ≤0.05
|2˜q −2q| ≤0.05
|˜q −q| ≤0.025
First, note that with inﬁnitely many samples, the fraction ˜q should naturally
converge to become the fraction q.
µ˜q = q
We can thus transform this to something closer to our form!
|˜q −µ˜q| ≤0.025
.
However, we need to incorporate the number of people we are sampling. So, we
multiply all by n.
|˜qn −µ˜qn| ≤0.025n
Let us consider again: what is ˜q? We know that ˜q was previously deﬁned to be
the fraction of people that we observe to have heads. We are inherently asking
for the number of heads in n trials. In other words, we want k successes among
n trials, so this sounds calls for a Bernoulli random variable! We will deﬁne Xi
to be 1 if the ith person tells us heads. This makes
˜q = 1
n
n
X
i=1
Xi
To make our life easier, let us deﬁne another random variable Y = ˜qn.
Page 35

Discrete Mathematics and Probability Theory
aaalv.in/abcDMPT
Y = ˜qn =
n
X
i=1
Xi
.
Seeing that this now matches the format we need, our α is 0.025. Our ﬁnal form
is
Pr[|Y −qn| ≥0.025n] ≤
var(Y )
(n0.025)2
Equivalently,
Pr[|Y −qn| < 0.025n] ≥1 −
var(Y )
(n0.025)2
Step 2 : Compute var(Y )
a2
We ﬁrst compute var(Xi).
var(Xi) = E[X2
i ] −E[Xi]2
= q −q2
= q(1 −q)
We then compute var(Y ).
var(Y ) = var(
n
X
i=1
Xi)
=
n
X
i=1
var(Xi)
= nvar(Xi)
= nq(q −1)
Thus, we have the value of our right-hand-side.
var(Y )
a2
= nq(1 −q)
(n0.025)2
= q(1 −q)
n(0.025)2
Step 3 : Compute Bound
We now consider the remainder of our question: ”How many people do you need
to ask to be 95% sure...”. Per the ﬁrst paragraph right before step 1, we are
actually interested in the probability of 5%. Thus, we want the following.
Page 36

Discrete Mathematics and Probability Theory
aaalv.in/abcDMPT
var(Y )
a2
= q(1 −q)
n(0.025)2 = 0.05
.
We have an issue however: there are two variables, and we don’t know q.
However, we can upper bound the quantity q(1−q). Since Chebyshev’s computes
an upper bound for the probability, we can substitute q(1 −q) for its maximum
value.
q(1 −q) = q2 −q
To ﬁnd it’s maximum, we take the derivative and set equal to 0.
q′ = 2q −1 = 0
q = 1
2
This means that q(1 −q) is maximized at q = 1
2, making the maximum value
for q(1 −q), 1
2(1 −1
2) = 1
2( 1
2) = 1
4. We now plug in 1
4.
1 −q(1 −q)
n(0.025)2 = 0.95
q(1 −q)
n(0.025)2 = 0.05
1/4
n(0.025)2 = 0.05
1
4n(0.025)2 = 1
20
5
(0.025)2 = n
n = 8000
Page 37

Chapter 7
Markov Chains
7.1
Guide
Markov Chains are closely tied to both linear algebra and diﬀerential equations.
We will explore connections with both to build a better sense of how markov
chains work.
7.1.1
Deﬁnition
Formally, a Markov Chain is a countable set of random variables that satisfy
the memoryless (Markov) property, where transitions to the next state depend
only on the current state.
7.1.2
Characterization
We are interested in three properties of Markov Chains: (1) reducibility, (2)
periodicity, and (3) transience.
• A Markov chain is irreducible if it can go from every state i to every other
state j, possibly in multiple steps.
• d(i) := g.c.d{n > 0 | P n(i, i) = Pr[Xn = i|X0 = i] > 0}, i ∈X where
d(i) = 1 if and only if the state i is aperiodic.
The Markov chain is
aperiodic if and only if all states are aperiodic.
• A distribution π is invariant for the transition probability matrix P if it
satisﬁes the following balance equation: π = πP. If a time-dependent
distribution converges limn→∞πn = π, the resulting distribution is then
called the stationary distribution or steady state distribution.
7.1.3
Transition Probability Matrices
The Transition Probability Matrix (TPM) is written so that the rows sum to 1.
Each i, jth entry corresponds to the probability that we transition from state i
(row) to state j (column).
38

Discrete Mathematics and Probability Theory
aaalv.in/abcDMPT
7.1.4
Balance Equations
Balance equations consider incoming edges in the Markov chain, where each
π(i) is the sum of all previous states π(h) multiplied by the probability that h
transitions to i. More succinctly,
∀i, π(i) =
X
h
Phi · π(h)
Note that we can obtain the balance equations by left-multiplying the TPM by
⃗π = [π(0)π(1) . . . π(n)]T
7.1.5
Important Theorems
• Any ﬁnite, irreducible Markov chain has a unique invariant distribution.
• Any irreducible, aperiodic Markov chain has a unique invariant distribution
that it will converge to, independent of the chain’s initial state.
Page 39

Discrete Mathematics and Probability Theory
aaalv.in/abcDMPT
7.2
Hitting Time Walkthrough
“Hitting Time” questions ask for the expected amount of time to “hit” a state.
Note that for in CS70, we consider only discrete-time Markov chains. The ﬁrst
3 variants introduced below reduce to solving a system of linear equations.
s0
s1
s2
s3
1/4
1/4
1/4
1/2
1/4
1
1/2
1
Question: Linear Algebra with Transition Probability
Let Xi be the number of steps needed to reach X3. Compute E[X0].
Answer 16
Begin by writing the First Step equations.
β(s0) = 1 + 1
4β(s0) + 1
4β(s1) + 1
2β(s2)
β(s1) = 1 + 1
4β(s0) + 1
2β(s1) + 1
4β(s3)
β(s2) = 1 + β(s0)
β(s3) = 0
Bring all constants to one side and all β(si) to the right.
−1 = −3
4β(s0) + 1
4β(s1) + 1
2β(s2)
−1 = 1
4β(s0) −1
2β(s1) + 1
4β(s3)
−1 = β(s0) −β(s2)
0 = β(s3)
Write the system of equations as a matrix.


−3/4
1/4
1/2
0
−1
1/4
−1/2
0
1/4
−1
1
0
−1
0
−1
0
0
0
1
0


Page 40

Discrete Mathematics and Probability Theory
aaalv.in/abcDMPT
Finally, solve the system of equations.


0
0
0
0
16
0
0
0
0
10
0
0
0
0
17
0
0
0
1
0


This means β(s0) = 16.
s0 (4)
s1 (1)
s2 (2)
s3
1/4
1/4
1/4
1/2
1/4
1
1/2
1
Question: Linear Algebra with Transition Probability, Wait Time
Let Xi be the number of steps needed to reach X3. In the Markov Chain
above, the number in parentheses for a state represents the number of steps
needed to “pass through” that state. Compute E[X0].
Begin by writing the First Step equations.
β(s0) = 4 + 1
4β(s0) + 1
4β(s1) + 1
2β(s2)
β(s1) = 1 + 1
4β(s0) + 1
2β(s1) + 1
4β(s3)
β(s2) = 2 + β(s0)
β(s3) = 0
Bring all constants to one side and all β(si) to the right.
−4 = −3
4β(s0) + 1
4β(s1) + 1
2β(s2)
−1 = 1
4β(s0) −1
2β(s1) + 1
4β(s3)
−2 = β(s0) −β(s2)
0 = β(s3)
Write the system of equations as a matrix.
Page 41

Discrete Mathematics and Probability Theory
aaalv.in/abcDMPT


−3/4
1/4
1/2
0
−4
1/4
−1/2
0
1/4
−1
1
0
−1
0
−2
0
0
0
1
0


Finally, solve the system of equations.


0
0
0
0
44
0
0
0
0
24
0
0
0
0
46
0
0
0
1
0


This means β(s0) = 44.
s0 (4)
s1 (1)
s2 (2)
s3
1/4
1/4 (4)
1/4
1/2
1/4 (4)
1
1/2
1 (4)
Question: Lin. Alg. with Trans. Prob., Wait Time, Trans. Time
Let Xi be the number of steps needed to reach X3. In the Markov Chain
above, the number in parentheses for a state represents the number of steps
needed to “pass through” that state. The numbers in parentheses for an edge
represent the number of steps needed to “pass through” that edge. If no
number is speciﬁed, the edge takes only 1 step. Compute E[X0].
Begin by writing the First Step equations.
β(s0) = 4 + 1
4β(s0) + 1
4(β(s1) + 4) + 1
2β(s2)
β(s1) = 1 + 1
4β(s0) + 1
2β(s1) + 1
4(β(s3) + 4)
β(s2) = 2 + (β(s0) + 4)
β(s3) = 0
Simplify all constants.
β(s0) = 4 + 1
4β(s0) + 1
4β(s1) + 1 + 1
2β(s2)
Page 42

Discrete Mathematics and Probability Theory
aaalv.in/abcDMPT
β(s1) = 1 + 1
4β(s0) + 1
2β(s1) + 1
4β(s3) + 1
β(s2) = 2 + β(s0) + 4
β(s3) = 0
Bring all constants to one side and all β(si) to the right.
−5 = −3
4β(s0) + 1
4β(s1) + 1
2β(s2)
−2 = 1
4β(s0) −1
2β(s1) + 1
4β(s3)
−6 = β(s0) −β(s2)
0 = β(s3)
Write the system of equations as a matrix.


−3/4
1/4
1/2
0
−5
1/4
−1/2
0
1/4
−2
1
0
−1
0
−6
0
0
0
1
0


Finally, solve the system of equations.


0
0
0
0
72
0
0
0
0
48
0
0
0
0
78
0
0
0
1
0


This means β(s0) = 72.
Page 43

Chapter 8
Solutions
This section contains completely-explained solutions for each of the problems
provided. Each one of these problems is designed to be at exam-level or harder,
err’ing on the side of diﬃculty. The goal is touch on all major topics presenting
in that chapter. In each of the following solutions, we identify ”Takeaways” for
every question at the bottom. You should understand just how the solution
appeals to those takeaways, and on the exam, be prepared to apply tips and
tricks presented here.
44

Discrete Mathematics and Probability Theory
aaalv.in/abcDMPT
8.1
Counting
1. If we roll a standard 6-sided die 3 times, how many ways are there to roll
a sum total of 14 pips where all rolls have an even number of pips?
Solution:
We can reduce all of our dice to ”3-sided die” that contain
only even numbers. Additionally, we can consider a reduced subproblem.
In the original problem, we can only combine any number of 2, 4, 6 for a
total of 14. This is the same as counting the number of ways to combine
1, 2, 3 for a total of 7.
Distributing x to a dice roll is the same as assigning it 2x pips. Since a
dice roll can have at most 6 pips, x is at most 3 for a single roll. Since
dice do not have a 0 side, x is at least 1. Thus, we are distributing 7 balls
among 3 bins with at most 3 balls and at least 1 ball for a single bin.
By 2.2 Stars and Bars Walkthrough: At Least, we ﬁrst distribute 1 ball to
each bin, reducing the problem to 4 balls and 3 bins, for
 6
2

By 2.2 Stars and Bars Walkthrough: At Most, we can identify two classes
of invalid combinations:
• Distribute all 4 balls to one bin:
 3
1

.
• Distribute 3 balls to one bin. There are then 2 other bins to pick
from, for the last ball:
 3
1
 2
1

In sum, we then have (1) all ways to distribute 7 balls, with at least 1 in
each bin Minus (2) all the ways to get more than 3 balls in a single bin.
6
2

−
3
1

−
3
1
2
1

Takeaway: Reduce complex stars and bars to the most basic form.
Alternate Solution:
First, we distribute 1 ball to each bin, reducing the problem to distributing
4 balls among 3 bins such that each bin contains no more than 2 balls.
The possibilities can be enumerated: 2 + 2 + 0, 2 + 0 + 2, 0 + 2 + 2, 1 + 1 +
2, 1 + 2 + 1, 2 + 1 + 1. Hence, there are 6 total ways.
Takeaway: When the options are few enough, enumerate.
2. Given a standard 52-card deck and a 5-card hand, how many unique hands
are there with at least 1 club and no aces?
Solution:
Let A be the event, “at least 1 club”, and B be the event “no
aces”. We are looking for |A ∩B|.
Note that computing |A∩B| is potentially tedious. Instead of considering
A, “at least 1 club”, it is simpler to consider ¯A, “no clubs”. Thus, we
rewrite |A ∩B| = |B| −| ¯A ∩B|. To compute | ¯A ∩B|, we examine all
combinations with no aces and no clubs. We are drawing from 52−4−12 =
Page 45

Discrete Mathematics and Probability Theory
aaalv.in/abcDMPT
36 cards, making | ¯A ∩B| =
 36
5

. Consider all hands with no aces. This is
|B| =
 48
5

. Thus, we have
|A ∩B| = |B| −| ¯A ∩B|
=
48
5

−
36
5

3. Given a standard 52-card deck and a 5-card hand, how many unique hands
are there with at least 1 club or no aces?
Solution:
Again, let A be the event, “at least 1 club”, and B be the
event “no aces”. We are looking for |A ∪B| = |A| + |B| −|A ∩B|.
From the previous part, we have |A ∩B| = |B| −| ¯A ∩B|. Thus, we ﬁrst
simplify the original |A ∪B| algebraically.
|A ∪B| = |A| + |B| −|A ∩B|
= |A| + |B| −(|B| −| ¯A ∩B|)
= |A| + | ¯A ∩B|
We now compute |A|. Again, | ¯A| is easier to compute, so we consider
|A| = |Ω|−| ¯A|. | ¯A|, or combinations without any clubs, is
 39
5

. Thus, the
number of combinations with at least one club is |A| =
 52
5

−
 39
5

. We
now compute |A ∪B| = |A| + | ¯A ∩B|
52
5

−
39
5

+
36
5

Takeaway: Draw a Venn Diagram, and compute simpler portion.
4. Given a standard 52-card deck and a 3-card hand, how many unique
hands are there with cards that sum to 15? (Hint: Each card is uniquely
identiﬁed by both a number and a suit. This problem is more complex than
phone numbers.)
Solution:
Numbers
First, a card has a maximum value of 13. Implicitly, we are thus looking
for all the ways to distribute 15 among 3 bins, where any bin has at most
13 and at least 1. First, we distribute a value of 1 to each bin, so the
problem reduces down to distributing 12 among 3 bins, where each bin is
at most 12. By stars and bars, the answer here is just
 14
2

.
Suits
There are a total of
 4
1
3 ways to pick suits. The only problem is when
we assign the same number and suit to two diﬀerent cards. We will thus
consider all invalid combinations.
Page 46

Discrete Mathematics and Probability Theory
aaalv.in/abcDMPT
• It is possible that each card is a 5. There are 43 total assignments
of suits to the cards, and only 4!/1! = 24 of them are valid. Hence,
there are 43 −24 invalid combinations here.
• Let us count the number of invalid combinations in which exactly 2 of
the cards are assigned the same number. The number that the cards
are assigned can range from 1 to 7, excluding 5 (since if two cards
are 5, this forces the third card to also be 5, which corresponds to
the previous case of all 3 cards having the same value). There are
 3
2

ways to choose the locations of the numbers,
 6
1

ways to choose the
number which is repeated, 4 ways to choose the suit of the repeated
numbers (in an invalid combination, the repeated numbers have the
same suit), and 4 ways to choose the suit of the last number, for a
total of 3 · 6 · 4 · 4 = 288 invalid combinations.
We thus have (1) all ways to distribute 15 validly Multiplied by (2) all
possible suit choices and ﬁnally, Minus invalid suit and number assignments.
14
2
4
1
3
−(43 −24) −288 = 5496
Takeaway: Beware of over-counting.
Page 47

Discrete Mathematics and Probability Theory
aaalv.in/abcDMPT
8.2
Probability
1. We sample k times at random, without replacement, a coin from our wallet.
There are p pennies, n nickels, d dimes, and q quarters, making N total
coins. Given that the ﬁrst three coins are pennies, what is the probability
that we will sample 2 nickels, 2 pennies, and 1 dime next, in any order?
Solution:
In a previous version, this problem was more complex but so
was the answer. The question is now simpliﬁed.
We have a
n
N−3 of picking a nickel and likewise
n
N−3 of picking another
nickel. We repeat for the pennies to obtain
p−4
N−5 (as the ﬁrst three coins
are given to be pennies) and
p−5
N−6. Finally, the probability of picking a
dime next is
d
N−7.
Now, we need to discount order. There are a total of
 5
2

ways to pick
slots for pennies, and from the remaining 3 slots,
 3
1

ways to pick a slot
for the dime. Multiplying the two together, we obtain our ﬁnal answer.
5
2
3
1

n
N −3
n −1
N −4
p −4
N −5
p −5
N −6
d
N −7
2. We are playing a game with our apartment-mate Wayne. There are three
coins, one biased with probability p of heads and the other two fair coins.
First, each player is assigned one of three coins uniformly at random.
Players then ﬂip simultaneously, where each player earns h points per
head. The winning player is the one with the most points. If Wayne earns
k points after n coin ﬂips, what is the probability that Wayne has the
biased coin?
Solution:
Let X be the number of heads that Wayne obtains. Let Y be
the event that Wayne has the biased coin. By Bayes’ Rule, we know the
following.
Pr[Y |X] =
Pr[X|Y ]Pr[Y ]
Pr[X|Y ]Pr[Y ] + Pr[X| ¯Y ]Pr[ ¯Y ]
We will ﬁrst compute the easiest terms. We know the following probabilities.
Given that we have three coins with one biased, the probability of a biased
coin is 1
3 and for an unbiased coin, 2
3.
Pr[Y ] = 1
3
Pr[ ¯Y ] = 2
3
We will now compute both conditionals, given Y and ¯Y .
Restated in
English, we are computing the probability of k
h success given n trials. The
number of successes follows the Bin(n, p) distribution, and the general
formula is
Page 48

Discrete Mathematics and Probability Theory
aaalv.in/abcDMPT
Pr[Bin(n, p) = k] =
n
k

(1 −p)n−kpk
For the biased coin, the probability of heads is p (Y ). For the fair coin,
the probability of heads is 1
2 ( ¯Y ). To simplify, we will deﬁne k′ = k
h, which
gives us the number of heads that Wayne obtained.
Pr[X = k′|Y ] =
n
k′

(1 −p)n−k′pk′
Pr[X = k′| ¯Y ] =
n
k′
 1
2n
We have computed all values, so we plug into Bayes’ Rule and simplify.
Pr[Y |X] =
Pr[X|Y ]Pr[Y ]
Pr[X|Y ]Pr[Y ] + Pr[X| ¯Y ]Pr[ ¯Y ]
=
Pr[X|Y ] 1
3
Pr[X|Y ] 1
3 + Pr[X| ¯Y ] 2
3
=
Pr[X|Y ]
Pr[X|Y ] + Pr[X| ¯Y ]2
=
  n
k′

(1 −p)n−k′pk′
  n
k′

(1 −p)n−k′pk′ +
  n
k′
 1
2n 2
=
(1 −p)n−k′pk′
(1 −p)n−k′pk′ + 21−n
Takeaway: Remember Bayes’ Rule.
3. Let X and Y be the results from two numbers, chosen uniformly randomly
in the range {1, 2, 3, ..., k}. Deﬁne Z = |X −Y |.
(a) Find the probability that Z < k −2.
Solution:
Instead of computing Pr[Z < k −2], it is easier to
compute 1 −Pr[Z < k −2] = Pr[Z ≥k −2], as this includes only
two possible values for Z.
Pr[Z ≥k −2] = Pr[Z = k −2] + Pr[Z = k −1]
We can now compute each probability independently. There are a
total of k2 possible combinations:
• For Pr[Z = k −1], we have 1 possible combination of numbers
(k, 1), with 2 ways of rolling those combinations (k, 1) or (1, k),
making 2 total combinations.
2
k2
Page 49

Discrete Mathematics and Probability Theory
aaalv.in/abcDMPT
• For Pr[Z = k −2], we have 2 possible combinations of numbers
((k, 2), (k−1, 1)) with 2 ways to roll each, making 4 total combinations.
4
k2
Pr[Z ≥k −2] = 4
k2 + 2
k2 = 6
k2
We thus solve for Pr[Z < k −2].
1 −Pr[Z < k −2] = Pr[Z ≥k −2]
Pr[Z < k −2] = 1 −Pr[Z ≥k −2]
Now, we can plug in to get our ﬁnal expression.
Pr[Z < k −2] = 1 −Pr[Z ≥k −2]
= 1 −6
k2
Takeaway: Use counting where applicable.
(b) Find the probability that Z ≥2.
Solution:
Again, we apply the same trick in the previous part. It
is easier to compute the probability for Pr[Z < 2] = 1 −Pr[Z ≥2].
Thus, we have only two possible values of Z to account for.
Pr[Z < 2] = Pr[Z = 0] + Pr[Z = 1]
There are a total of k2 combinations.
• We know that Z = 0 implies that both rolls yielded the same
number. Thus, the ﬁrst roll has k options and for each value the
ﬁrst roll assumes, the second has 1, making k total combinations.
k
k2
• We know that Z = 1 implies that the rolls are within 1 of each
other. Let us consider two cases, (1) X ∈{1, k} or (2) 1 < X <
k. If the ﬁrst roll is 1 or k, then the second roll has only one
option each, 2 or k −1, respectively. If the ﬁrst roll 1 < X < k
(k-2 possibilities), then Y has two options each, making 2(k −2)
possible combinations.
2+2(k−2)
k2
= 2(k−1)
k2
Pr[Z < 2] = k
k2 + 2(k −1)
k2
= 3k −2
k2
We thus solve for Pr[Z ≥2].
Pr[Z < 2] = 1 −Pr[Z ≥2]
Pr[Z ≥2] = 1 −Pr[Z < 2]
Now, plug in our solution for Pr[Z < 2].
Page 50

Discrete Mathematics and Probability Theory
aaalv.in/abcDMPT
Pr[Z ≥2] = 1 −Pr[Z < 2]
= 1 −3k −2
k2
= k2 −3k + 2
k2
= (k −2)(k −1)
k2
4. Consider a 5 in. × 3 in. board, where each square inch is occupied by a
single tile. A monkey hammers away at the board, choosing a position
uniformly at random; assume a single strike completely removes a tile from
that position. (Note that the monkey can choose to strike a position with
no tiles.) By knocking oﬀtiles, the monkey can create digits. For example,
the following would form the digit “3”, where 0 denotes a missing tile and
1 denotes a tile.


0
0
0
1
1
0
0
0
0
1
1
0
0
0
0


(a) Given the monkey strikes n times, where n > 15, what is the probability
that the monkey knocks out tiles, such that the board forms “8”?
Solution:
To knock out an 8, the monkey needs to achieve the
following board.


0
0
0
0
1
0
0
0
0
0
1
0
0
0
0


We can consider this as balls and bins, where we have n balls being
thrown into 15 bins. Except, all balls are distinguishable. The total
number of ways to strike 15 tiles is 15n.
We wish to avoid the 2 tiles and to strike all other 13 tiles. Thus,
we are throwing n balls into 13 bins, where each bin has at least 1.
Since order matters, we cannot apply stars and bars directly.
i. Consider all the ways to distribute n balls among 13 bins, 13n
ii. Subtract all cases where 1 bin is left empty. First, we choose the
bin that is left empty
 13
1

, then we distribute the n balls among
the remaining 12 bins, 12n. Together, this is −
 13
1

12n.
Page 51

Discrete Mathematics and Probability Theory
aaalv.in/abcDMPT
iii. By inclusion-exclusion, we have double-counted all cases where 2
bins are left empty. Choose the 2 bins that are left empty,
 13
2

.
Distribute to the remaining bins, 11n. This is +
 13
2

11n.
We notice a pattern, which is that for 0 ≤i ≤13, we select i to
be the number of bins that are empty. Then, we distribute n balls
among the remaining i bins. By inclusion-exclusion, we include and
exclude alternately. The total number of ways to distribute n balls
among 13 bins, where each bin receives at least one ball is thus
13
X
i=0
(−1)i
13
i

(13 −i)n
The probability that this digit occurs is thus
1
15n
13
X
i=0
(−1)i
13
i

(13 −i)n
Generally, let a be the number of 0s in the matrix. The probability
that the monkey forms this particular number is
1
15n
a
X
i=0
(−1)i
a
i

(a −i)n
Takeaway: Don’t overthink, and consider counting.
(b) Given the monkey strikes n times, where n > 15, what is the probability
that the monkey knocks out tiles, such that the board forms “2”?
Solution:
To knock out a 2, the monkey needs to achieve one of
two boards.


0
0
0
1
1
0
0
0
0
0
1
1
0
0
0


By the same logic as the previous part, we can see that there are 4
positions we avoid and 11 positions we must hit. Using the general
form above, we see that the number of 0s (a) is 11.
Thus, the
probability of a 2 is:
1
15n
11
X
i=0
(−1)i
11
i

(11 −i)n
(c) Given the monkey strikes n times, where n > 15, what is the probability
that the monkey knocks out tiles, such that the board forms an even
number?
Page 52

Discrete Mathematics and Probability Theory
aaalv.in/abcDMPT
Solution:
First, let us count the number of 0s required to form
each digit.
• digit 0 : 12 zeros
• digit 2 : 11 zeros
• digit 4 : 9 zeros
• digit 6 : 12 zeros
• digit 8 : 13 zeros
Let the probability that a digit with a zeros occurs be
pa =
1
15n
a
X
i=0
(−1)i
a
i

(a −i)n
Applying the pa, we see that the probability that any of these even
numbers occurs is:
2p12 + p11 + p9 + p13
(d) Which digit is most likely to occur?
Solution:
The probability that a digit occurs is dependent solely
upon the number of zeros present in that digit.
Thus, we look
for the digit with the most zeros. We enumerate all numbers and
corresponding 1s and 0s.
Digit
ones
zeros
0
3
12
1
6
9
2
4
11
3
4
11
4
6
9
5
4
11
6
3
12
7
6
9
8
2
13
9
3
12
We note that 8 has the most zeros. Thus, the digit 8 is most likely
to occur.
Page 53

Discrete Mathematics and Probability Theory
aaalv.in/abcDMPT
8.3
Expectation
1. Consider a set of books B. The spine of each book has a single character
inscribed, and this character is the only distinguishing characteristic. Placed
side-by-side in the right order, the spines of all books in B spell some string
s, ”RaoWalrandRule”. What is the expected number of times s shows
up, if a monkey picks n books from B uniformly at random and places
them, uniformly at random, on a shelf? The books may be upside-down,
but the spines are always facing out. Hint: Do repeating characters aﬀect
the probability?
Solution:
This is similar to the last problem in the 4.2 Linearity of
Expectation Walkthrough. However, we are drawing letters from s instead
of the entire alphabet. We realize the question the is asking for ”expected
number of (successes)” once more, so we immediately deﬁne our indicator
variable, Xi to be 1 if one instance of s ends at the ith position along the
shelf.
The string is 14 letters long, meaning that the ﬁrst 13 books on the shelf
cannot be the position where an instance of s ends. This makes X =
Pn
i=14 Xi the total number of times s appears on the shelf. By linearity
of expectation:
E[X] = E[
n
X
i=14
Xi]
=
n
X
i=14
E[Xi]
=
n
X
i=14
Pr[Xi = 1]
We now compute the probability of success, which is the probability that
s is spelled, given some random sampling of letters from s. For the ﬁrst
letter, sampling randomly from s give us a
3
14 probability of retrieving an
R, as there are 14 total letters, 3 of which are R. We proceed to compute
the product of these probabilities, one for each letter.
=
n
X
i=14
3 · 3 · 1 · 1 · 3 · 2 · 3 · 3 · 1 · 1 · 3 · 1 · 2 · 1
1414
=
n
X
i=14
36 · 22
1414
= (n −13)36 · 22
1414
Takeaway: Be clever with how you deﬁne ”success” for your indicator
variables. Remember linearity of expectation always holds.
Page 54

Discrete Mathematics and Probability Theory
aaalv.in/abcDMPT
2. Four square inch tiles make up a 2 in. × 2 in. insignia, and each tile is
distinct. We have 2572 such tiles. When the tiles are randomly arranged
in a 257 × 257 grid, what is the expected number of properly-formed
insignias? Assume that tiles can be rotated in any of 4 orientations but
not ﬂipped.
Solution:
There are 256 · 256 = 48 interior vertices. Let Xi be 1 if the
ith interior vertex is at the center of a valid insignia. Thus, we know that
X = P48
i Xi is the total number of valid insignias. We can apply linearity
of expectation, and then invoke properties of an indicator variable.
E[X] = E[
48
X
i
Xi]
=
48
X
i
E[Xi]
= 48E[Xi]
= 48Pr[Xi = 1]
We can use counting to compute the probability that vertex i is the center
of a valid insignia. First, there are 4 valid orientations for the insignia.
Second, we now compute all possible combinations of 4 tiles.
Each of
the four tiles is chosen at random and then rotated at random, making
4 possible tiles with 4 possible orientations each (16), for each tile. Thus
there are a total of 164 = 48 combinations.
Pr[Xi = 1] = 4
48 = 1
47 = 1
47
We now plug it back in to solve for E[X].
E[X] = 48Pr[Xi = 1]
= 48 1
47
= 4
The takeaway below was mentioned prior, but it is worth mentioning
again.
Takeaway: Deﬁne your indicator variables cleverly.
3. Let Xi be the event that the ith dice roll has an even number of pips. Let
Z be the product of all Xi, from 1 to 10. Formally, Z = Π10
i=1Xi. Let Y
be the sum of all Xi, from 1 to 10. Formally, Y = P10
i=1 Xi.
Page 55

Discrete Mathematics and Probability Theory
aaalv.in/abcDMPT
(a) Compute E[Y |Z]. Remember that your result is a function of Z.
Solution: We will begin by making algebraic manipulations. First,
we plug in Y . Then, by linearity of expectation, we move the summation
out.
E[Y |Z] = E[
10
X
i=1
Xi|Z]
=
10
X
i=1
E[Xi|Z]
= 10E[Xi|Z]
Separately, we will now compute E[Xi|Z].
We note that for an
indicator variable i, E[Xi] = Pr[Xi = 1]. The analogous, conditional
form states the following.
E[Xi|Z] = Pr[Xi = 1|Z]
Applying Bayes’ Rule, we then get the following.
=
Pr[Z|Xi = 1]Pr[Xi = 1]
Pr[Z|Xi = 1]Pr[Xi = 1] + Pr[Z|Xi = 0]Pr[Xi = 0]
First, note that Pr[Xi = 1] - the probability of rolling an even number
of pips - is 1
2. The probability of rolling an odd, Pr[Xi = 0] is also
1
2. Thus, this simpliﬁes
=
Pr[Z|Xi = 1] 1
2
Pr[Z|Xi = 1] 1
2 + Pr[Z|Xi = 0] 1
2
=
Pr[Z|Xi = 1]
Pr[Z|Xi = 1] + Pr[Z|Xi = 0]
We will additionally assign Z to a value k. After solving E[Xi|Z = k],
we can then substitute all instances of k with Z for E[Xi|Z].
E[Xi|Z = k] =
Pr[Z = k|Xi = 1]
Pr[Z = k|Xi = 1] + Pr[Z = k|Xi = 0]
We now substitute Z in.
For Pr[Z = k|Xi = 0], since Z is the
product of all Xj, including Xi = 0, then Z = 0. Thus, Pr[Z =
k|Xi = 0] = Pr[k = 0].
E[Xi|Z = k] =
Pr[Π10
j=1Xj = k|Xi = 1]
Pr[Π10
j=1Xj = k|Xi = 1] + Pr[k = 0]
Consider the probability speciﬁed in the numerator. Pr[Xi = 1|Π10
j=1Xj].
We know that since Xi is 1, it does not aﬀect the product. Thus,
this probability reduces to Pr[Π10
j=1,i̸=jXj].
Page 56

Discrete Mathematics and Probability Theory
aaalv.in/abcDMPT
=
Pr[Π10
j=1,i̸=jXj = k]
Pr[Π10
j=1,i̸=jXj = k] + Pr[k = 0]
Finally, to convert E[Xi|Z = k] back to E[Xi|Z], we substitute Z
for all k. After that, we use our ﬁrst result to obtain an expression
for E[Y |Z]. We can continue to reason about the numerator, but we
will leave that for part (c); a more intuition-based approach is shown
in the alternate solution that follows.
E[Xi|Z] =
Pr[Π10
j=1,i̸=jXj = Z]
Pr[Π10
j=1,i̸=jXj = Z] + Pr[Z = 0]
E[Y |Z] = 10
Pr[Π10
j=1,i̸=jXj = Z]
Pr[Π10
j=1,i̸=jXj = Z] + Pr[Z = 0]
Takeaway: Be able to switch between (1) algebraic manipulations
and (2) intuition.
Alternate Solution:
Since Z can only be 0 or 1, let us consider the two cases. First, when
Z = 1, then every Xi must be 1, which means that the sum of Xi’s
must be 10. Therefore, E[Y |Z = 1] = 10.
Otherwise, Z = 0. Computing this case requires a bit more work.
From the deﬁnition:
E[Y ] =
10
X
y=0
yPr[Y = y]
Conditioned on Z = 0, we know that at least one of the Xi’s is 0.
This rules out the possibility that every Xi = 1, which occurs with
probability 2−10. The probability that at least one Xi is 0 is 1−2−10.
The conditional distribution Pr(Y = y|Z = 0) therefore has to be
“rescaled”:
Pr[Y = y|Z = 0] = Pr[Y = y, Z = 0]
Pr[Z = 0]
= Pr[Y = y]
1 −2−10 ,
0 ≤y ≤9
Page 57

Discrete Mathematics and Probability Theory
aaalv.in/abcDMPT
Therefore, the conditional expectation is
E[Y |Z = 0] =
9
X
y=0
yPr[Y = y|Z = 0]
=
1
1 −2−10
9
X
y=0
y Pr[Y = y]
=
1
1 −2−10
 10
X
y=0
y Pr[Y = y] −10 · 1
210
!
=
1
1 −2−10

E[Y ] −10 · 1
210

=
1
1 −2−10

5 −10 · 1
210

We have obtained the two diﬀerent cases for E[Y |Z = z], which is
enough to specify E[Y |Z] as a function of Z. Just for fun, we can
“stitch” the two expressions into one:
E[Y |Z] = 10Z +
1
1 −2−10

5 −10 · 1
210

(1 −Z)
(b) Using your derivation from part (a), compute E[Y |Z > 0]. Explain
your answer intuitively.
Solution: To gain some insight, we then evaluate this expression for
Z > 0. Notice that if Z > 0, then Pr[Z = 0] = 0. Thus,
E[Y |Z > 0] = 10
Pr[Π10
j=1,i̸=jXj = Z]
Pr[Π10
j=1,i̸=jXj = Z] + 0 = 10
This makes sense, as if Z > 0, then ∀i, Xi ̸= 0. Thus, all ∀i, Xi = 1,
making Y = P10
i=1 Xi = 10.
(c) Using your derivation from part (a), compute E[Y |Z = 0].
Solution: Next, we evaluate this expression at Z = 0. Note that
Pr[Z = 0] = 1.
E[Y |Z = 0] = 10
Pr[Π10
j=1,i̸=jXj = 0]
Pr[Π10
j=1,i̸=jXj = 0] + 1
We will now compute Pr[Π10
j=1,i̸=jXj = 0] separately. Note that the
probability the product is 0, is the probability that at least one of
the Xj, i ̸= j is 0. As a result, we consider the probability that none
of the Xj are 0, and subtract that from 1. Formally,
Pr[Π10
j=1,i̸=jXj = 0]
= Pr[∪10
j=1,i̸=jXj = 0]
Page 58

Discrete Mathematics and Probability Theory
aaalv.in/abcDMPT
= 1 −Pr[∩10
j=1,i̸=jXj ̸= 0]
Since the probability of all Pr[Xj ̸= 0] are independent, we can
multiply all of the probabilities together.
= 1 −Π10
j=1,i̸=jPr[Xj ̸= 0]
For all Xj, Pr[Xj ̸= 0] is the probability that a dice roll is not even,
which is 1
2.
= 1 −Π10
j=1,i̸=j
1
2
= 1 −1
29
Plugging it back in, we get that
E[Y |Z = 0] = 10
1 −1
29
1 −1
29 + 1
= 101 −1
29
2 −1
29
= 10 29 −1
210 −1
Takeaway: Apply DeMorgan’s to handle grotesque probabilities.
Page 59

