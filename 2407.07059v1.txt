Differentiable Optimization of Similarity Scores
Between Models and Brains
Nathan Cloos
MIT
nacloos@mit.edu
Moufan Li
NYU
moufan.li@nyu.edu
Markus Siegel
HIH TÃ¼bingen
markus.siegel@uni-tuebingen.de
Scott L. Brincat
MIT
sbrincat@mit.edu
Earl K. Miller
MIT
ekmiller@mit.edu
Guangyu Robert Yang
MIT
yanggr@mit.edu
Christopher J. Cueva
MIT
ccueva@gmail.com
Abstract
What metrics should guide the development of more realistic models of the brain?
One proposal is to quantify the similarity between models and brains using methods
such as linear regression, Centered Kernel Alignment (CKA), and angular Pro-
crustes distance. To better understand the limitations of these similarity measures
we analyze neural activity recorded in five experiments on nonhuman primates, and
optimize synthetic datasets to become more similar to these neural recordings. How
similar can these synthetic datasets be to neural activity while failing to encode task
relevant variables? We find that some measures like linear regression and CKA,
differ from angular Procrustes, and yield high similarity scores even when task
relevant variables cannot be linearly decoded from the synthetic datasets. Synthetic
datasets optimized to maximize similarity scores initially learn the first principal
component of the target dataset, but angular Procrustes captures higher variance
dimensions much earlier than methods like linear regression and CKA. We show in
both theory and simulations how these scores change when different principal com-
ponents are perturbed. And finally, we jointly optimize multiple similarity scores
to find their allowed ranges, and show that a high angular Procrustes similarity, for
example, implies a high CKA score, but not the converse.
Project page: https://diffscore.github.io
Code: https://github.com/diffscore/diffscore
Similarity package: https://github.com/diffscore/similarity-repository
1
Introduction
Natural intelligence entails the interactions between many systems - such as different sensory, memory,
and motor systems. Therefore, ultimately, many of the open questions in systems neuroscience will
require models that bridge these systems. However, there are many challenges as we work towards
these multi-system models of the brain. We need guidance to search through this huge design space
of potential models. It would be helpful to have metrics that allow us to extract general principles
that are applicable across many models, or alternatively, highlight the power of heterogeneity, and
shine a spotlight on combinations of tasks, datasets, and models that are not currently well explained.
Preprint. Under review.
arXiv:2407.07059v1  [q-bio.NC]  9 Jul 2024

Angular Procrustes, Linear Regression, CKA, Angular CKA, NBS
Neuron 1
Neuron 2
Mante 2013
Siegel 2015
Hatsopoulos 2007
Freeman 2013
Majaj 2015
Reference dataset X
Optimize dataset Y
Scores
Neural datasets
Dots
Dots
Reaching
Texture
Object
+ others
Figure 1: (a) To better understand the properties of similarity measures we optimize synthetic
datasets to become more similar to a reference dataset, for example, neural recordings. (b) We
analyzed similarity scores between artificial datasets and electrode recordings from five experiments
on nonhuman primates spanning a diverse range of behaviors and brain regions.
Similarity measures have become a cornerstone in evaluating representational alignment across
different models [Kornblith et al., 2019], different biological systems [Kriegeskorte et al., 2008],
and across both artificial and biological systems. Researchers have employed diverse methods to
compare model representations with, for example, brain activity, aiming to identify models that
exhibit brain-like representations [Yamins et al., 2014, Sussillo et al., 2015, Schrimpf et al., 2018,
Nayebi et al., 2018]. However, while these measures are actively used and provide an efficient
way to compare structure across complex systems, it is not clear that they adequately represent the
computational properties of interest, and there is a need to better understand their limitations. The
field lacks clear guidelines for selecting the most appropriate measure for a given context.
In this work we study several popular methods that have been proposed to quantify the similarity
between models and neural data, in particular, linear regression [Yamins et al., 2014, Schrimpf et al.,
2018], Centered Kernel Alignment (CKA) [Kornblith et al., 2019], and angular Procrustes distance
[Williams et al., 2021, Ding et al., 2021]. See appendix B.1 for a brief overview. We analyzed
neural data from studies on nonhuman primates (Figure 1) and compared the neural responses to
task-optimized recurrent neural networks (RNNs), or synthetic datasets, with different similarity
scores. In order to study what drives high similarity scores we directly optimize the synthetic datasets
to maximize their similarity to the neural datasets as assessed by different methods, for example,
linear regression, CKA, or angular Procrustes distance.
Comparing similarity scores across studies is challenging, primarily due to variability in naming
and implementation conventions. As part of our contribution to the research community we have
created, and are continuing to develop, a Python package that benchmarks and standardizes similarity
measures.
Disagreement between similarity measures. An example application of similarity measures is to
quantify how brain-like models are. However, when we compare task-optimized RNNs to two neural
datasets, as shown in Figure 2, we find that different similarity measures do not agree about which
models are more similar to the data, or even about whether the models are more or less similar than
two baseline scores that compare modified versions of the neural data to the original neural data. Are
most of the models generally performing well or not, i.e. achieving model-data similarities above
one or both of these baseline scores? Different measures give different answers. These similarity
measures lack consistency and do not present a consensus interpretation. This is not an irrelevant
exercise as all of these similarity measures have been used in the literature to compare models to
neural data.
2
Related Work
Reviews. Recent reviews by Sucholutsky et al. [2023] and Klabunde et al. [2023] provide compre-
hensive overviews of representational similarity measures and their theoretical properties. While
these reviews highlight the diversity of available metrics, they offer limited practical guidance on
metric selection.
2

0.0
0.2
0.4
0.6
0.8
1.0
0.0
0.2
0.4
0.6
0.8
1.0
0.0
0.2
0.4
0.6
0.8
1.0
0.0
0.2
0.4
0.6
0.8
1.0
0.0
0.2
0.4
0.6
0.8
1.0
0.0
0.2
0.4
0.6
0.8
1.0
a
c
b
d
Angular Procrustes
CKA
CKA
Ridge Regression CV
Ridge Regression CV
Choice
Choice
Dot motion
coherence
Dot color
coherence
Mante 2013
Siegel 2015
0
1
Time (s)
0
25
0
1
Time (s)
0
20
Firing rate
(spikes/s)
Cue
Stim
Neuron 1
Neuron 2
Angular Procrustes
Neuron-split
Cross-condition
average
Score
Score
better
better
worse
worse
Figure 2: Different similarity measures are not consistent when comparing task-optimized
recurrent neural networks to neural datasets. We consider two neural datasets from prefrontal
cortex (PFC) [Mante et al., 2013] (a) and Frontal Eye Field (FEF) [Siegel et al., 2015] (b) in
monkeys performing an experimental task that required the animal to attend to either color or motion
information while ignoring the non-cued feature of the stimuli. On each trial, a field of colored
moving dots is shown. Monkeys are given a cue at the beginning of the trial to determine whether the
dots in the stimulus are moving left vs right, or are red vs green. The monkey reported its choice
with a saccade to one of two visual targets. In both datasets, we analyzed neural activity taken when
the dot stimulus was presented. In panel a neural activity is visualized in a low-dimensional space
capturing task-relevant dynamics using the targeted dimensionality reduction method from Mante et
al. 2013. Each curve shows the average neural activity for a different experimental condition. See
Mante et al. 2013 for a detailed description of the analysis. This visualization highlights features of
the data but the similarity scores were computed using the firing rates from the electrode recordings
before any dimensionality reduction. In panel b neural firing rates for two example neurons are shown
with the colors denoting average activity for different experimental conditions. (c, d) RNNs with
three different architectures, CTRNN, LowPassCTRNN, LSTM and three different nonlinearities,
ReLU, ReTanh, Tanh are compared to neural datasets. The Neuron-split baseline score was obtained
by dividing the neurons from a single dataset into disjoint sets and then comparing. If the model-data
scores are equal to the neuron-split scores this indicates that model activity is indistinguishable from
the neural activity of other recorded neurons. The Condition-average baseline score was obtained by
averaging the neural activity along the trial and condition dimensions and comparing it to the original
dataset. Each neuron in this condition-averaged dataset still has a unique time-varying firing rate.
This strong baseline shows the similarity to the original data that one can obtain by only keeping the
condition-independent neural dynamics. The different similarity measures do not agree on the
relative rankings of the models, or on a more global scale, how similar the models are relative
to the two baseline scores.
Our work addresses this gap by proposing a general framework for comparing similarity measures on
experimental neural datasets. By directly optimizing synthetic datasets to maximize their similarity to
neural recordings, we can systematically investigate how different metrics prioritize various aspects
of the data, such as specific principal components or task-relevant information.
Mathematical properties. This work builds upon several important theoretical and empirical
contributions. Kornblith et al. [2019] discussed the invariance properties of similarity measures
and their implications for comparing neural representations. Williams et al. [2021] advocated for
similarity measures that satisfy the axioms of a metric distance. Harvey et al. [2023] established
a duality between Normalized Bures Similarity (NBS) and Procrustes distance, shedding light on
their mathematical relationship to CKA. We leverage these theoretical insights to provide a deeper
interpretation of our empirical findings.
Comparison with functional measures. Our approach shares similarities with the work of Ding
et al. [2021], who evaluated metrics based on their correlation with functional behavioral measures.
They analyzed a collection of trained models and observed how variations in the models, such as
the removal of principal components, impacted both the similarity scores and the performance on
3

task-specific probes. They identified that CKA exhibited sensitivity primarily to the top principal
components, while orthogonal Procrustes demonstrated more robust performance.
However, our approach differs significantly in how we construct a diverse set of datasets with varying
behavioral characteristics. Instead of relying on pre-trained models, we begin with unstructured
noise and directly optimize it to maximize similarity to neural recordings. This allows us to address
questions such as whether high similarity scores can be achieved without encoding task-relevant
variables. Our optimization-based approach reveals how different similarity measures guide the
emergence of task-relevant information within the synthetic datasets, offering a dynamic perspective
on their properties.
In contrast to the majority of previous work, our method is model-agnostic, focusing on the properties
of similarity measures themselves rather than specific model architectures. We demonstrate the
generalizability of our findings by applying our framework to multiple neural datasets, revealing
consistent patterns in how different metrics prioritize data features. Interestingly, we find that the
optimization dynamics observed with neural data are closely predicted when using Gaussian datasets
with matched variance distributions. This suggests that our insights extend beyond the specifics of
individual neural datasets and reflect fundamental properties of the similarity measures themselves.
3
Method
To evaluate the similarity of representations between two systems, we extract feature representations
such as activity in a brain area or model layer in response to stimuli. Our objective is to quantify the
alignment between these representations using a similarity score. Assume two datasets X and Y
represent these features with dimensions (sample, feature) and mean-centered columns. Datasets with
temporal dynamics are reshaped from (time, sample, feature) to (time*sample, feature). We define a
scoring function score(X, Y ) as a measure that increases with similarity, achieving a maximum of 1
when X = Y .
We show results for the following three common similarity measures (see appendix B.1 for details):
â¢ Linear Regression finds the best linear mapping B that predicts a reference dataset X from a
dataset Y [Yamins et al., 2014, Schrimpf et al., 2018]. We measure the goodness of fit using R2
and use ridge regularization with parameter Î» = 100 as well as 5-fold cross-validation that tests
generalization across different experimental conditions.
R2
LR = 1 âminB â¥X âY Bâ¥2
F + Î»â¥Bâ¥F
â¥Xâ¥2
F
â¢ Centered Kernel Alignment (CKA) measures the correlation between the kernels of two
datasets X and Y [Kornblith et al., 2019]. We consider here linear CKA where the kernels are
XXT and Y Y T .
CKA(X, Y ) = â¨vec(XXT ), vec(Y Y T )â©
â¥XXT â¥F â¥Y Y Y â¥F
We also consider the angular version of CKA [Williams et al., 2021, Lange et al., 2022].
â¢ Angular Procrustes finds the optimal orthogonal linear alignment between X and Y to maxi-
mize their correlation [Williams et al., 2021, Ding et al., 2021].
max
QâO
â¨X, QY â©
â¥Xâ¥F â¥Y â¥F
where O is the group of orthogonal linear transformations. Williams et al. [2021] proposed
taking the arccosine to satisfy the axioms of a distance metric. Here we rescale the angular
Procrustes distance to obtain a scoring measure between 0 and 1, where 1 is perfect similarity.
4
Results
4.1
High similarity scores do not guarantee encoding of task relevant variables
Question. We start by asking if synthetic datasets with high similarity scores relative to the neural
data, encode task relevant variables, for example, the stimulus features or the response of the monkey,
4

in the same way as the neural data. More specifically, is it possible for the synthetic datasets to have a
high similarity score while failing to encode task relevant variables?
Problem with CKA and linear regression. Surprisingly we find that for linear regression and CKA
the answer is yes, a high similarity score does not necessarily mean the synthetic datasets encode
task relevant variables like the neural data. Figures 3a and 3b show the decode accuracy of a linear
classifier trained to decode task relevant variables (cross-validated across different conditions) as the
similarity score increases. Before optimization, the synthetic datasets initially consisted of Gaussian
noise and the decode accuracy was near the baseline chance level of 0.5 as expected for the binary
classifier used in this analysis.
Consider the synthetic data optimized towards the Siegel 2015 neural recordings using CKA similarity
(second row and column in Figure 3). When the synthetic dataset has a high similarity score of 0.9 the
decode accuracy for all the task variables is still less than that found in the neural activity (horizontal
dashed lines). This is in contrast to the case where synthetic data is optimized to maximize angular
Procrustes similarity (first column) and a similarity score of near 0.9 yields a dataset that encodes task
variables to the same degree as the neural recordings. Note that both CKA and angular Procrustes
have the same similarity scale ranging between 0 and 1 (perfect similarity). See appendix B.1 for
more details.
As another example, consider the decode accuracy for the color of the stimulus that was shown in
the experiments (green curves). For linear regression and CKA the similarity score can be high even
when the data does not encode this task variable at the level found in the neural activity, suggesting
there is a mismatch between the datasets with a high similarity score and features of the neural activity.
In contrast, for the angular Procrustes distance the synthetic datasets that have a high similarity score
relative to the neural activity also encode the task relevant variables.
How high should a good score be? As a final more optimistic note, for all the similarity measures,
we can see a general trend that optimizing the synthetic data towards higher similarity eventually
leads the data to encode task relevant variables above the baseline chance level of 0.5 accuracy. One
perspective on this decoding analysis is that it gives a window into what is a high similarity score. If
a high score is one where the data is encoding task relevant variables then these similarity numbers
will need to be interpreted differently based on the similarity measure.
High variability in the condition average baseline across metrics. A specific number for the
similarity score, 0.9 for example, can have different interpretations depending on the measure that is
used, as we have seen in the examples above contrasting CKA and angular Procrustes. Conversely if
two models or datasets are compared then this comparison may result in different scores depending
on the similarity measure. Prior work has used the condition average baseline as a reference score
for benchmarking models against neural data [Cloos et al., 2022]. The condition average baseline is
obtained by averaging each neuronâs activity across trials from all experimental conditions, while
still allowing each neuron to have its own unique time-varying firing rate, and then comparing this
condition-averaged dataset to the original neural data. This baseline shows the similarity one can
obtain by discarding the condition specific activity to keep only the condition independent dynamics
of the neural activity. The condition average baseline is shown in Figures 3a and 3b as vertical dashed
lines. For a single dataset, we are always comparing the same two quantities to obtain the condition
average baseline but the scores are quite different depending on the similarity measure. For the
angular Procrustes measure, as opposed to the linear regression similarity measure, this baseline
score is closer to the point where task relevant information starts to be encoded.
4.2
Optimization dynamics of similarity scores
4.2.1
Low-dimensional synthetic datasets
Question. How much of the neural data must be captured by a synthetic dataset or model before the
decode accuracy reaches the level seen in the neural dataset itself? One perspective on this question is
to decode the task variables from neural data after projecting onto principal components 1 through N,
where principal component 1 captures the most variance. In order to capture all the information about
the task variables, at least several principal components must be included in the decode as shown in
Figures 3c and 3d. This motivates the following hypothesis.
5

0.4
0.6
0.8
1.0
0.5
0.6
0.7
0.8
0.9
0.2
0.4
0.6
0.8
1.0
0.5
0.6
0.7
0.8
0.9
0.0
0.2
0.4
0.6
0.8
1.0
0.4
0.6
0.8
0.4
0.6
0.8
1.0
0.6
0.8
1.0
0.2
0.4
0.6
0.8
1.0
0.6
0.8
1.0
0.0
0.2
0.4
0.6
0.8
1.0
0.6
0.8
5
10
15
20
0.5
0.6
0.7
0.8
0.9
5
10
15
20
0.6
0.8
response
direction
color
context
response
direction
color
context
Mante 2013
Siegel 2015
Optimized score
Optimized score
Optimized score
Optimized score
Optimized score
Optimized score
Decoding accuracy
Decoding accuracy
a
b
CKA
Angular Procrustes
Ridge Regression CV
Condition
average
Number of PCs included
Number of PCs included
Decoding accuracy
Decoding accuracy
c
d
Figure 3: (a, b) Decode accuracy for experimental variables versus similarity scores. Decode is
from synthetic data optimized towards greater similarity with the neural data from (a) Mante et al.
[2013] and (b) Siegel et al. [2015]. Horizontal dashed lines indicate the decode accuracy from the
neural data. (c, d) Decode accuracy from neural data versus number of principal components included
in the decode. Decode uses data from (c) prefrontal cortex from Mante et al. 2013 and (d) FEF from
Siegel et al. 2015.
Hypothesis. Perhaps the reason that linear regression and CKA similarity scores can be so high while
the synthetic data fails to encode task variables is because these similarity measures preferentially
rely on the top few principal components.
We explore this hypothesis in the following set of analyses with a synthetic dataset based on the
neural recordings from Mante et al. 2013.
Figure 4a shows the reference dataset (compare to Figure 2a). We can think of this reference dataset
as a low-dimensional neural trajectory summarizing the population activity of many neurons, or
alternatively, as the firing rates of two neurons over time (shown here encoding the two task variables
of choice and dot motion coherence), recorded during six different experimental conditions, with the
color in Figure 4a denoting the condition.
Figure 4b shows the transformation of an initially random Gaussian noise dataset as it is optimized to
maximize either the angular Procrustes or CKA similarity score with respect to the reference dataset.
The score increases from an initial value near 0 to a maximum near 1 as optimization progresses,
with the insets at the top of the figure showing the optimized noise dataset at various points during
this procedure. The yellow curve shows how well the optimized dataset captures the first principal
component of the reference dataset, as quantified by R2, throughout optimization (see appendix B.2
for details). Notice that the second principal component, shown in purple, is only captured at a much
higher optimization score for CKA versus angular Procrustes.
Figure 4c shows the same results when a synthetic Gaussian noise dataset is optimized towards the
reference dataset using either linear regression similarity or angular CKA similarity [Williams et al.,
2021].
Dependence on the variance distribution. The optimization dynamics not only depend on the
similarity measure but also on the variance distribution of the reference dataset. Figure 4d shows four
reference datasets with the same variance along the first principal component but decreasing variance
along the second. If both dimensions have approximately equal variance then angular Procrustes,
CKA, and linear regression will learn both dimensions similarly during optimization as shown by
the white curve in Figure 4e. The curves are colored to indicate the fraction of variance the second
principal component has relative to the first, so 1 indicates both principal components have the same
variance. As the asymmetry between the principal components grows, the optimization to maximize
angular Procrustes similarity still prioritizes the lower variance principal component (first column,
second row), while CKA and linear regression do not capture the second principal component of the
reference dataset until much later during optimization.
6

0.0
0.5
1.0
0
1
0.0
0.5
1.0
0
1
0.0
0.5
1.0
0
1
0.0
0.2
0.4
0.6
0.8
1.0
0.00
0.25
0.50
0.75
1.00
0.2
0.4
0.6
0.8
1.0
PC
ti
0.0
0.5
1.0
0
1
0
1
0.5
1.0
0
1
0
1
0.2
0.4
0.6
0.8
1.0
0.00
0.25
0.50
0.75
1.00
b
a
d
e
Ridge Regression CV
Gaussian
noise init
Reference
dataset
PC 2
PC 1
0.90
0.50
0.50
0.90
Captured variance
Captured variance
Angular Procrustes
CKA
Optimized score
c
Decrease variance of PC2
PC1
PC2
CKA
Angular Procrustes
Ridge Regression CV
Optimized score
Optimized score
Reference dataset
PC2 var
Angular CKA
Optimized score
Captured variance
6 conditions
t&
Right
choice
Left
choice
Dot motion
coherence
0.0
0.5
1.0
0
1
Figure 4: Different similarity measures differentially prioritize learning principal components
of the data. (a) Reference dataset used as a target during optimization. (b, c) Initial Gaussian random
noise data is updated to maximize similarity with the reference dataset, as quantified by one of the
similarity measures. The transformation of the random noise dataset is shown at the top of panel b.
The first principal component of the reference dataset is increasingly well captured by the optimized
data as the similarity scores increase (yellow curves). The second, lower variance, component is
also learned when maximizing the angular Procrustes similarity but is only captured at high
similarity scores when maximizing linear regression, CKA, and angular CKA similarity. (d)
Four reference datasets with decreasing variance along the second principal component. (e) Similarity
measures capture both principal components when their variance is approximately equal. However,
when the variance differs, CKA and linear regression have a much more pronounced preference for
learning the high variance component as opposed to angular Procrustes which causes the optimized
dataset to also capture the low variance dimension (curves colored according to asymmetry of variance
distribution).
4.2.2
Neural datasets
The optimization dynamics revealed in Figure 4a for a two-dimensional dataset also holds on real
neural datasets (A.1). We now consider various neural datasets as the reference dataset for the
optimization procedure. Figure 5a shows the optimization dynamics when a random noise Gaussian
dataset is optimized towards the Siegel et al. 2015 dataset by maximizing angular Procrustes similarity
(top) or CKA similarity (bottom). Each curve shows how the optimized dataset captures a single
principal component of the reference dataset during the course of optimization. Similar to the
results in Figure 4, optimizing for angular Procrustes similarity captures more of the lower variance
components in the data for a given similarity score.
A convenient way of summarizing these curves is to note the similarity score required to capture
a given principal component above some threshold, defined here (and shown in Figure 5a) as the
centerpoint between the maximum and initial R2 value for a given principal component. Figure
5b shows the score required to reach the principal component threshold for the different principal
components in the Mante et al. 2013 dataset. Figures 5c and 5d show the same curves when the
reference datasets are now the electrode recordings from Siegel et al. 2015 and Majaj et al. 2015.
Angular CKA. Williams et al. [2021] proposed to take the arccosine of CKA to obtain a measure
that satisfies the axioms of a distance metric. Figures 5b, 5c, and 5d additionally show that the metric
version of CKA (referred to as angular CKA) improves its sensitivity to lower variance components.
Neural curves predicted by matched Gaussians. Surprisingly, the optimization dynamics shown in
these figures is well matched when the reference neural dataset is replaced by a new reference dataset
that consists of random Gaussian numbers with variances for each principal component matched to
7

10â3
10â2
10â1
0.0
0.5
1.0
10â4
10â3
10â2
10â1
0.0
0.2
0.4
0.6
0.8
1.0
10â4
10â3
10â2
10â1
0.0
0.5
1.0
10â4
10â3
10â2
10â1
0.4
0.6
0.8
0.4
0.6
0.8
1.0
Angular Procrustes Score
0.6
0.8
1.0
â3
â2
â1
log(PC explained variance)
0.25
0.50
0.75
1.00
0.6
0.8
1.0
Angular CKA
CKA
Angular Procrustes
NBS
Ridge Regression CV
Linear Regression
Angular Procrustes
Mante 2013
PC explained variance
Score to reach PC threshold
Angular Procrustes
NBS
Angular CKA
CKA
Ridge Regression CV
Siegel 2015
Mante 2013
MajajHong 2015
Siegel 2015
PC explained variance
Optimized score
Reference dataset captured variance
a
b
c
d
e
PC1
threshold
Init
Max
CKA
Score to reach
PC1 threshold
10â2
10â1
0.25
0.50
0.75
1.00
10â4
10â3
10â2
10â1
0.0
0.5
1.0
10â4
10â3
10â2
10â1
0.6
0.8
1.0
Matched Gaussian
Neural dataset
10â4
10â3
10â2
10â1
0.25
0.50
0.75
1.00
10â4
10â3
10â2
10â1
0.4
0.6
0.8
1.0
10â4
10â3
10â2
10â1
0.4
0.6
0.8
1.0
10â3
10â2
10â1
0.25
0.50
0.75
1.00
10â2
10â1
0.25
0.50
0.75
1.00
10â4
10â3
10â2
10â1
0.0
0.5
1.0
Score to reach PC threshold
PC explained variance
Figure 5: (a) A randomly initialized synthetic dataset is updated to maximize the similarity
with a neural dataset, taken here to be the FEF dataset from [Siegel et al., 2015]. The principal
components (PCs) of this reference dataset are captured by the optimized dataset at different similarity
scores, which in subsequent figures we call the score to reach the PC threshold. (b) The score to reach
the PC threshold for the [Mante et al., 2013] dataset is shown as a function of the variance explained
by each PC. The highest variance PC is learned first during optimization at the lowest similarity score
(bottom right of figure). A vertical slice through the figure shows the similarity score required to
capture a specific PC. For example, to capture the PC at 10â2 requires a much lower similarity score
when maximizing angular Procrustes versus CKA (light blue curve is below the red curve). (c, d)
The reference dataset used as a target during optimization is the neural activity from [Siegel et al.,
2015] FEF (panel c) and [Majaj et al., 2015] (panel d). (e) The neural data points are the same as in
panels c and d (colored dots). The similarity scores at which PCs of this neural activity are learned,
is well predicted by replacing neural activity with random Gaussian datasets that have a matching
distribution of variances for each PC (black curves).
the neural data (Figure 5e). The variance distribution of the reference dataset strongly determines
when each principal component is captured during optimization.
4.3
Theoretical analysis of CKA and NBS
Ding et al. [2021] briefly noted in their discussions that in the simplified case where two datasets
have the same eigenvectors, CKA reduces to products of squared singular values of each dataset,
whereas Procrustes distance doesnât square the singular values. Here we provide a more formal way
to derive this result, leveraging a duality relationship discovered by Harvey et al. [2023].
Duality of Bures and shape distances. Harvey et al. [2023] showed that angular Procrustes is
related to another measure, Normalized Bures Similarity (NBS) [Tang et al., 2020], by just taking the
arccosine. Similar to angular CKA, we find that the arccosine increases sensitivity to lower variance
components (Figures 5b, 5c, and 5d). This can be understood by the nonlinear mapping of arccosine
that expands values close to one.
To understand the increased sensitivity to low variance principal components of NBS compared to
CKA (dark blue and red curves in Figures 5b, 5c, and 5d), we analyze their matrix norm formulation.
As shown in Harvey et al. [2023], CKA and NBS can be formulated as follow (see appendix B.1 for
8

0.0
0.5
1.0
0
1
0
1
0
1
0
1
0
1
0
1
Score A
Angular Procrustes
CKA
Linear Regression
Ridge Regression CV
0.9
0.9
Score B
Score B
Score B
A
A
B
B
A
B
Independent
Coupled
Asymmetric
0.00
0.02
0.04
0.06
0.08
0.8
0.9
1.0
Variance of perturbed PC
Score
CKA
NBS
Angular Procrustes
Theory
a
b
c
0.0
0.5
1.0
0.0
0.5
1.0
0.0
0.5
1.0
Figure 6: (a) The similarity score between a synthetic dataset and a modified version of the
same dataset when a single principal component is perturbed. For all similarity measures,
when small variance components are perturbed the similarity score is near 1. However, when high
variance components are perturbed the angular Procrustes score drops much more than for CKA
and Normalized Bures Similarity (NBS). (b, c) We jointly optimized the values of both angular
Procrustes and CKA or linear regression to illustrate the allowed ranges of both similarity
scores (gray region enclosed by the solid lines). The pink dots in panel c indicate the target values for
optimization, and the blue curves indicate the optimization trajectories of the two similarity measures.
If angular Procrustes has a high score of 0.9 (horizontal dashed line) then linear regression will have
a value above this. In contrast, a high linear regression score of 0.9 (vertical dashed line) does not
imply a high angular Procrustes score, and a wide range of angular Procrustes scores are possible.
details):
CKA(X, Y ) =
â¥XT Y â¥2
F
â¥XXT â¥F â¥Y Y T â¥F
NBS(X, Y ) =
â¥XT Y â¥â
p
â¥XXT â¥ââ¥Y Y T â¥â
NBS quantifies similarity using the nuclear norm, which involves a sum of singular values, whereas
CKA uses the Frobenious norm, which sums the square of the singular values. The additional square
operation in CKA significantly increases the contribution of large variance components. We show this
in theory by considering how CKA and NBS change when perturbing a single principal component
of the data. The result is that CKA depends quadratically on the variance of the perturbed principal
component, whereas NBS has a linear dependence (see proof in appendix B.3), as empirically
validated in Figure 6a.
CKA(X, ËXk) â
P
iÌ¸=k(Î»i
X)2
P
i(Î»i
X)2
NBS(X, ËXk) â
P
iÌ¸=k Î»i
X
P
i Î»i
X
where ËXk is equal to X where only the kth principal component is modified.
4.4
Are metrics mutually independent?
A natural question that arises is whether these different similarity metrics are independent of each
other, or if they exhibit consistent relationships. To address this, we consider three possible relation-
ships between any two given similarity metrics (Figure 6b):
â¢ Independent: The two metrics are independent. A high score on one metric offers no guarantee
of a high score on the other.
â¢ Coupled: The two metrics are tightly coupled. A high score on one metric implies a high score
on the other, and vice versa.
â¢ Asymmetric: One metric subsumes the other. A high score on the first metric guarantees a high
score on the second, but not the other way around.
We jointly optimize multiple similarity scores to find their allowed ranges (Figure 6c) and show that a
high angular Procrustes similarity implies a high CKA score, but not the converse. A high value
of angular Procrustes implies a high score for unregularized linear regression but linear regression
that is regularized and cross-validated across experimental conditions can take independent values.
5
Discussion
Our study reveals critical limitations of commonly used similarity measures for comparing models
and neural datasets. While these measures offer a seemingly straightforward way to quantify
9

representational alignment, our optimization-based approach shows that high similarity scores,
particularly for CKA and linear regression, do not guarantee that synthetic datasets encode task-
relevant information in a manner consistent with neural data. Specifically, we demonstrate that
measures like CKA are heavily influenced by the top principal components of the data, often
achieving high scores even when lower variance components, which might carry crucial task-related
information, remain poorly captured.
Limitations & future work. Our study focused on differentiable, geometry-based similarity measures
and their optimization dynamics. Future work should investigate how it extends to other types of
measures. Additionally, a comprehensive analysis of computational efficiency and scalability for
different metrics will be crucial as datasets continue to grow in size. Moving forward, our python
package aims to standardize similarity measures, facilitating a more cumulative scientific approach
by centralizing findings related to these measures, and enabling more integrated benchmarking and
comparisons.
10

References
Nathan Cloos, Moufan Li, Guangyu Robert Yang, and Christopher J. Cueva. Scaling up the evalu-
ation of recurrent neural network models for cognitive neuroscience. Conference on Cognitive
Computational Neuroscience (CCN), 2022.
Frances Ding, Jean-Stanislas Denain, and Jacob Steinhardt. Grounding representation similarity
through statistical testing. In Advances in Neural Information Processing Systems, volume 34,
2021.
Jeremy Freeman, Corey M Ziemba, David J Heeger, Eero P Simoncelli, and J Anthony Movshon. A
functional and perceptual signature of the second visual area in primates. Nature Neuroscience, 16
(7):974â981, July 2013. ISSN 1546-1726.
Sarah E Harvey, Brett W. Larsen, and Alex H Williams. Duality of bures and shape distances with
implications for comparing neural representations. In UniReps: the First Workshop on Unifying
Representations in Neural Models, 2023.
Nicholas G. Hatsopoulos, Qingqing Xu, and Yali Amit. Encoding of movement fragments in the
motor cortex. Journal of Neuroscience, 27(19):5105â5114, 2007.
Sepp Hochreiter and JÃ¼rgen Schmidhuber. Long short-term memory. Neural computation, 9:1735â80,
12 1997.
Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization, 2017.
Max Klabunde, Tobias Schumacher, Markus Strohmaier, and Florian Lemmerich. Similarity of neural
network models: A survey of functional and representational measures, 2023.
Simon Kornblith, Mohammad Norouzi, Honglak Lee, and Geoffrey Hinton. Similarity of neural
network representations revisited, 2019.
Nikolaus Kriegeskorte, Marieke Mur, Douglas A. Ruff, Roozbeh Kiani, Jerzy Bodurka, Hossein
Esteky, Keiji Tanaka, and Peter A. Bandettini. Matching categorical object representations in
inferior temporal cortex of man and monkey. Neuron, 60(6):1126â1141, 12 2008.
Richard D. Lange, Devin Kwok, Jordan Matelsky, Xinyue Wang, David S. Rolnick, and Konrad P.
Kording. Neural networks as paths through the space of representations, 2022.
Najib J. Majaj, Ha Hong, Ethan A. Solomon, and James J. DiCarlo. Simple learned weighted
sums of inferior temporal neuronal firing rates accurately predict human core object recognition
performance. Journal of Neuroscience, 35(39):13402â13418, 2015. doi: 10.1523/JNEUROSCI.
5181-14.2015.
Valerio Mante, David Sussillo, Krishna V. Shenoy, and William T. Newsome. Context-dependent
computation by recurrent dynamics in prefrontal cortex. Nature, 503:78â84, Nov 2013.
Kenneth D. Miller and Francesco Fumarola. Mathematical equivalence of two common forms of
firing rate models of neural networks. Neural Computation, 24(1):25â31, 2012.
Aran Nayebi, Daniel Bear, Jonas Kubilius, Kohitij Kar, Surya Ganguli, David Sussillo, James J
DiCarlo, and Daniel L Yamins. Task-driven convolutional recurrent models of the visual system.
Advances in neural information processing systems, 31, 2018.
Martin Schrimpf, Jonas Kubilius, Ha Hong, Najib J. Majaj, Rishi Rajalingham, Elias B. Issa, Kohitij
Kar, Pouya Bashivan, Jonathan Prescott-Roy, Franziska Geiger, Kailyn Schmidt, Daniel L. K.
Yamins, and James J. DiCarlo. Brain-score: Which artificial neural network for object recognition
is most brain-like? bioRxiv preprint, 2018.
Martin Schrimpf, Jonas Kubilius, Michael J Lee, N Apurva Ratan Murty, Robert Ajemian, and James J
DiCarlo. Integrative benchmarking to advance neurally mechanistic models of human intelligence.
Neuron, 2020. URL https://www.cell.com/neuron/fulltext/S0896-6273(20)30605-X.
Markus Siegel, Timothy J. Buschman, and Earl K. Miller. Cortical information flow during flexible
sensorimotor decisions. Science, 348(6241):1352â1355, 2015. doi: 10.1126/science.aab0551.
11

Ilia Sucholutsky, Lukas Muttenthaler, Adrian Weller, Andi Peng, Andreea Bobu, Been Kim, Bradley C.
Love, Erin Grant, Iris Groen, Jascha Achterberg, Joshua B. Tenenbaum, Katherine M. Collins,
Katherine L. Hermann, Kerem Oktar, Klaus Greff, Martin N. Hebart, Nori Jacoby, Qiuyi Zhang,
Raja Marjieh, Robert Geirhos, Sherol Chen, Simon Kornblith, Sunayana Rane, Talia Konkle,
Thomas P. OâConnell, Thomas Unterthiner, Andrew K. Lampinen, Klaus-Robert MÃ¼ller, Mariya
Toneva, and Thomas L. Griffiths. Getting aligned on representational alignment, 2023.
David Sussillo, Mark M Churchland, Matthew T Kaufman, and Krishna V Shenoy. A neural network
that finds a naturalistic solution for the production of muscle activity. Nature neuroscience, 18(7):
1025â1033, Jul 2015.
Shuai Tang, Wesley J. Maddox, Charlie Dickens, Tom Diethe, and Andreas Damianou. Similarity of
neural networks with gradients, 2020.
Alex H. Williams, Erin Kunz, Simon Kornblith, and Scott W. Linderman. Generalized shape metrics
on neural representations. In Advances in Neural Information Processing Systems, volume 34,
2021.
Daniel L. K. Yamins, Ha Hong, Charles F. Cadieu, Ethan A. Solomon, Darren Seibert, and James J.
DiCarlo. Performance-optimized hierarchical models predict neural responses in higher visual
cortex.
Proceedings of the National Academy of Sciences, 111(23):8619â8624, 2014.
doi:
10.1073/pnas.1403112111.
12

A
Neural Datasets and Models
A.1
Datasets
We analyzed neural data from five studies on nonhuman primates:
â¢ Mante et al. [2013]: Prefrontal cortex (PFC) electrode recordings during a contextual decision-
making task involving colored moving dots. Data link1.
â¢ Siegel et al. [2015]: Frontal Eye Field (FEF) electrode recordings during a contextual decision-
making task involving colored moving dots, similar to Mante et al. (2013).
â¢ Hatsopoulos et al. [2007]: Primary motor (M1) electrode recordings during a center-out reaching
task. Data link2.
â¢ Majaj et al. [2015]: Inferior temporal (IT) electrode recordings during object image presenta-
tions.
â¢ Freeman et al. [2013]: Primary (V1) and secondary (V2) visual area electrode recordings during
texture and noise image presentations.
We use the BrainScore3 library [Schrimpf et al., 2020] for the Majaj et al. [2015], Freeman et al.
[2013] datasets.
A.2
Recurrent Neural Network (RNN) models
We show results for three commonly used RNN architectures: LSTMs [Hochreiter and Schmidhuber,
1997] and two choices for continuous time recurrent neural networks (CTRNNs), which differ by the
position of the nonlinearity [Miller and Fumarola, 2012]. A first alternative is given by the following
equations.
(
Ï dh
dt = âh + Wr + Bu + b
r = f(h) + Î¾
where u is the input, h represents the membrane potential, Î¾ is a Gaussian white noise, and r is the
firing rate produced by the model and is used to compare the model with the neural data. The second
alternative is given by the following equation.
Ï dr
dt = âr + f(Wr + Bu + b) + Î¾
We call it the LowPassCTRNN since it can be shown that its firing rate is a low-pass filter of the
firing rate of the first architecture, which we call CTRNN [Miller and Fumarola, 2012]. The RNNs
are trained using supervised learning on simplified task inputs and outputs.
A.3
Condition average baseline:
The condition average baseline is obtained by averaging the neural activity along the condition
dimension, while still allowing each neuron to have its own unique time-varying firing rate, and
then comparing this condition-averaged dataset to the original neural data. This baseline shows the
similarity one can obtain by discarding the condition specific activity to keep only the condition
independent dynamics of the neural activity.
B
Similarity Measures
B.1
Definitions
Several methods have been proposed to measure similarity between models and neural data (see
[Sucholutsky et al., 2023], [Klabunde et al., 2023] for comprehensive reviews). While some efforts
1https://www.ini.uzh.ch/en/research/groups/mante/data.html
2https://datadryad.org/stash/dataset/doi:10.5061/dryad.xsj3tx9cm
3https://github.com/brain-score/vision
13

have been made to characterize these metrics mathematically, for instance, by examining their
invariance properties, clear guidance on selecting the most appropriate metric for a given scenario
remains limited. To address this gap, we introduce a novel method for analyzing the specific aspects
of data prioritized by different similarity metrics. Our method leverages the differentiability of these
metrics and is applicable to a wide range of measures. We focus here on three commonly used
metrics and their variants, linear regression [Yamins et al., 2014, Schrimpf et al., 2018], Centered
Kernel Alignment (CKA) [Kornblith et al., 2019], and Procrustes distance [Williams et al., 2021,
Ding et al., 2021]. These methods quantify similarity based on the goodness of fit after aligning the
representations. This alignment transformation allows for greater flexibility by making similarity
measures invariant to specific transformations. For example, instead of demanding a one-to-one
mapping of neurons between datasets, these methods can accommodate scenarios where neurons in
one dataset correspond to linear combinations of neurons in the other.
Consider two datasets X and Y with shape (sample, feature) and mean-centered columns. Datasets
with dynamics are shaped from (time, sample, feature) to (time*sample, feature).
Assuming X as our reference dataset (e.g., neural data), linear regression seeks the optimal linear
mapping B to predict X from Y . We use the R2 coefficient to evaluate goodness of fit and ridge
regularization with parameter Î» = 100, as well as 5-fold cross-validation. Note that we apply
cross-validation after reshaping the data from (time, sample, feature) to (time*sample, feature).
R2
LR = 1 âminB â¥X âY Bâ¥2
F + Î»â¥Bâ¥F
â¥Xâ¥2
F
Itâs important to note that this score is not symmetric. Applying linear regression on Y , X instead of
X, Y may yield significantly different scores. This asymmetry arises because one dataset might be
highly predictive of the other, while the reverse might not hold true.
Unlike linear regression, CKA and Procrustes are symmetric, meaning the metric yields the same
result whether applied to X, Y or Y, X. Moreover, they exhibit a different class of invariance. While
linear regression is invariant under invertible linear transformations, CKA and Procrustes are invariant
under orthogonal linear transformations. This stricter invariance to orthogonal transformations
potentially reduces sensitivity to noise [Kornblith et al., 2019].
CKA measures the correlation between the kernels of two datasets X and Y [Kornblith et al., 2019].
We consider here linear CKA where the kernels are XXT and Y Y T .
CKA(X, Y ) = â¨vec(XXT ), vec(Y Y T )â©
â¥XXT â¥F â¥Y Y Y â¥F
=
â¥XT Y â¥2
F
â¥XXT â¥F â¥Y Y T â¥F
CKA scores range from 0 to 1, with 1 indicating perfect similarity. We also consider the arccosine
of CKA, a metric satisfying the axioms of a distance metric (e.g., the triangle inequality) with 0
signifying perfect similarity [Williams et al., 2021]. This metric, also known as Angular CKA [Lange
et al., 2022], is then normalized to a similarity score between 0 and 1 for direct comparison with other
scoring methods. This normalization involves dividing the angular distance by Ï/2 and subtracting
the result from 1, ensuring the measure increases with similarity.
AngularCKAScore(X, Y ) := 1 âarccos(CKA(X, Y ))
Ï/2
Procrustes distance provides another approach for quantifying similarity [Williams et al., 2021, Ding
et al., 2021]. This metric identifies the optimal orthogonal alignment to maximize the correlation
between X and Y :
max
QâO
â¨X, QY â©
â¥Xâ¥F â¥Y â¥F
where O is the group of orthogonal linear transformations. An angular distance metric can be obtained
by taking the arccosine of this quantity.
As shown in Harvey et al. [2023], Procrustes distance is closely related to Normalized Bures Similarity
(NBS) [Tang et al., 2020], with Procrustes distance equating the arccosine of NBS. NBS is defined as:
NBS(X, Y ) =
â¥XT Y â¥â
p
â¥XXT â¥ââ¥Y Y T â¥â
14

This definition resembles CKA but utilizes the nuclear matrix norm instead of the Frobenius matrix
norm. The Frobenius norm, denoted by â¥Aâ¥F for a matrix A, is calculated as the square root of the
sum of squared singular values. The nuclear norm, â¥Aâ¥â, is simply the sum of singular values. The
implications of this difference in matrix norms for the weighting of principal components by CKA
and NBS are further explored in B.3.
Similar to the score version of CKA distance, we define a score version of Procrustes distance,
ranging from 0 to 1, where 1 represents perfect similarity.
AngularProcrustesScore(X, Y ) := 1 âarccos(NBS(X, Y ))
Ï/2
B.2
Optimizing similarity scores
To better characterize similarity measures we optimize synthetic datasets Y to become more similar
to a reference dataset X. We initialize the synthetic dataset Y by randomly sampling from a standard
Gaussian distribution with the same shape as X. We use Adam [Kingma and Ba, 2017] to optimize
Y to maximize the similarity score with X, leveraging the differentiability of the similarity measures,
and stop the optimization when the score reaches a fixed threshold.
As we optimize Y towards greater similarity with X, we simultaneously evaluate how well task-
relevant variables can be linearly decoded from the evolving synthetic data. This decoding analysis
employs logistic regression with stratified 5-folds cross-validation. For dataset with temporal dynam-
ics, we fit a separate decoder for each time step and report the average accuracy across time.
We also evaluate how each principal component (PC) of the reference dataset X is captured as Y is
optimized for similarity. Specifically, for each PC vi of X, we use linear regression to find the optimal
linear combination of columns of Y that best predicts vi. The goodness of fit is then quantified using
the R2 coefficient:
R2
P Ci := 1 ââ¥(X âËX)viâ¥2
â¥Xviâ¥2
where ËX = Y ËB and ËB = argminBâ¥X âY Bâ¥2
F .
All experiments were conducted on a NVIDIA GeForce GTX TITAN X and required less than a day
of compute time.
B.3
Proof: theoretical analysis of CKA and NBS
Our results show that similarity in the high variance components seem to dominate the Centered
Kernel Alignemnt (CKA) score. In contrast metrics like Normalized Bures Distance (NBS) seem to
be more sensitive to changes in lower variance components. We explain this difference by showing,
in theory, how CKA and NBS changes when perturbing a single principal component of the data.
The result is that CKA depends quadratically on the variance of the perturbed principal component,
whereas NBS has a linear dependence. We start the derivation from the matrix norm definitions of
CKA and NBS.
NBS(X, Y ) =
â¥XT Y â¥â
p
â¥XXT â¥ââ¥Y Y T â¥â
CKA(X, Y ) =
â¥XT Y â¥2
F
â¥XXT â¥F â¥Y Y T â¥F
Let X be a dataset, and Y be a perturbed version of X, denoted ËXk, where only the kth principal
component is modified. Specifically, define Y such that its jth left singular vector uj
Y is equal to the
jth left singular vector uj
X of X for all j Ì¸= k, and define uk
Y to be a perturbation of uk
X that preserves
variance. Here we randomly sample uk
Y from a Gaussian distribution with the same variance as
the variance of uk
X so that Ïk
Y = Ïk
X. The right singular vectors of Y are the same as the right
singular vectors of X. Intuitively this perturbation affects only the projection of the data along the
kth principal component.
15

We first show how CKA(X, ËXk) depends on the variance of the perturbed principal component. As
shown in [Kornblith et al., 2019], CKA can be rewritten in terms of dot products of the left singular
vectors.
CKA(X, Y ) =
P
i,j Î»i
XÎ»j
Y â¨ui
X, uj
Y â©2
pP
i(Î»i
X)2
qP
j(Î»j
Y )2
where X = UXÎ£XV T
X , Y = UY Î£Y V T
Y are the SVD decompositions of X and Y respectively,
and Î»i
X = (Ïi
X)2, Î»i
Y = (Ïi
Y )2. Since uj
Y = uj
X for all j Ì¸= k and since UX is an orthogonal
matrix, the dot products â¨ui
X, uj
Y â©are equal to 0 for all j Ì¸= i whenever j Ì¸= k. The dot products
â¨ui
X, uk
Y â©are not guaranteed to be zero since randomly sampling uk
Y doesnât guarantee to preserve
the orthogonality with the other left singular vectors. However, when the number of samples in
the data is large the projection uk
Y on the other singular vectors will be relatively small. With the
assumption that â¨ui
X, uk
Y â©â0, CKA can be written as:
CKA(X, ËXk) â
P
iÌ¸=k(Î»i
X)2
P
i(Î»i
X)2
This shows that CKA scores between the original data and the perturbed data depend quadratically
on the variance of the perturbed principal component. As shown in Figure 6, our theoretical
approximation closely matches simulations.
As opposed to CKA, NBS cannot be directly rewritten as a sum of left singular vector dot products.
However, we show that NBS reduces to a simple form when Y is defined as X perturbed along a
single principal component and when â¨ui
X, uk
Y â©â0 is assumed. We start by expressing the nuclear
norm in the numerator of NBS in terms of the SVD decomposition of X and Y .
â¥XT Y â¥â= â¥VXÎ£XU T
XUY Î£Y V T
Y â¥â= â¥Î£XU T
XUY Î£Y â¥â
The individual entries of the product of matrices inside the nuclear norm corresponds to the dot
products between the left singular vectors of X and Y weighted by the corresponding singular values.
[Î£XU T
XUY Î£Y ]ij = Ïi
XÏj
Y â¨ui
X, uj
Y â©
By definition of Y â¡ËXk,
Ïi
XÏj
Y â¨ui
X, uj
Y â©=
(Ïi
X)2Î´ij
if j Ì¸= k
Ïi
XÏk
Xâ¨ui
X, uk
Y â©
if j = k
With our assumption that â¨ui
X, uk
Y â©â0 and the nuclear norm property â¥Aâ¥â= Tr
hâ
AT A
i
, we
find:
â¥XT Y â¥ââ
X
iÌ¸=k
(Ïi
X)2
Since our perturbation preserves the variance, we have â¥XXT â¥â= â¥Y Y T â¥â= P
i(Ïi
X)2. We
finally obtain the following approximation, which explicitly reveals the linear dependence of NBS on
the variance of the principal components.
NBS(X, ËXk) â
P
iÌ¸=k(Ïi
X)2
P
i(Ïi
X)2
=
P
iÌ¸=k Î»i
X
P
i Î»i
X
16

