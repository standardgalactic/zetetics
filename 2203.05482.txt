Model soups: averaging weights of multiple ﬁne-tuned models
improves accuracy without increasing inference time
Mitchell Wortsman 1 Gabriel Ilharco 1 Samir Yitzhak Gadre 2 Rebecca Roelofs 3 Raphael Gontijo-Lopes 3
Ari S. Morcos 4 Hongseok Namkoong 2 Ali Farhadi 1 Yair Carmon * 5 Simon Kornblith * 3 Ludwig Schmidt * 1
Abstract
The conventional recipe for maximizing model
accuracy is to (1) train multiple models with var-
ious hyperparameters and (2) pick the individ-
ual model which performs best on a held-out
validation set, discarding the remainder. In this
paper, we revisit the second step of this proce-
dure in the context of ﬁne-tuning large pre-trained
models, where ﬁne-tuned models often appear
to lie in a single low error basin. We show that
averaging the weights of multiple models ﬁne-
tuned with different hyperparameter conﬁgura-
tions often improves accuracy and robustness. Un-
like a conventional ensemble, we may average
many models without incurring any additional
inference or memory costs—we call the results
“model soups.” When ﬁne-tuning large pre-trained
models such as CLIP, ALIGN, and a ViT-G pre-
trained on JFT, our soup recipe provides signiﬁ-
cant improvements over the best model in a hy-
perparameter sweep on ImageNet. The result-
ing ViT-G model, which attains 90.94% top-1
accuracy on ImageNet, achieved a new state of
the art. Furthermore, we show that the model
soup approach extends to multiple image clas-
siﬁcation and natural language processing tasks,
improves out-of-distribution performance, and im-
proves zero-shot performance on new downstream
tasks. Finally, we analytically relate the perfor-
mance similarity of weight-averaging and logit-
ensembling to ﬂatness of the loss and conﬁdence
of the predictions, and validate this relation em-
pirically. Code is available at https://github.
com/mlfoundations/model-soups.
*Equal contribution 1University of Washington 2Columbia Uni-
versity 3Google Research, Brain Team 4Meta AI Research 5Tel
Aviv University. Correspondence to: <mitchnw@uw.edu>.
Proceedings of the 39 th International Conference on Machine
Learning, Baltimore, Maryland, USA, PMLR 162, 2022. Copy-
right 2022 by the author(s).
75
76
77
78
79
80
81
ImageNet Accuracy (top-1, %)
35
40
45
50
55
Avg. accuracy on 5 distribution shifts
Greedy Soup
Uniform Soup
Initialization
Various
hyperparameters
Figure 1: Model soups improve accuracy over the best individual
model when performing a large, random hyperparameter search
for ﬁne-tuning a CLIP ViT-B/32 model on ImageNet. The uniform
soup (blue circle) averages all ﬁne-tuned models (green diamonds)
in a random hyperparameter search over learning rate, weight-
decay, iterations, data augmentation, mixup, and label smoothing.
The greedy soup adds models sequentially to the model soup,
keeping a model in the soup if accuracy on the held-out validation
set does not decrease.
Method
ImageNet acc.
Distribution
(top-1, %)
shifts
ViT-G (Zhai et al., 2021)
90.45
–
CoAtNet-7 (Dai et al., 2021)
90.88
–
Our models/evaluations based on ViT-G:
ViT-G (reevaluated)
90.47
82.06
Best model in
90.78
84.68
hyperparam search
Greedy soup
90.94
85.02
Table 1: Model soups improve accuracy over the best individual
model when ﬁne-tuning a JFT-3B pre-trained ViT-G/14 model on
ImageNet. Instead of selecting the best model from a hyperparam-
eter sweep during ﬁne-tuning, model soups average the weights
of multiple ﬁne-tuned models. To evaluate performance under
distribution shift we consider average accuracy on ImageNet-V2,
ImageNet-R, ImageNet-Sketch, ObjectNet, and ImageNet-A. Ad-
ditional details are provided by Table 4 and Section 3.3.2, while
analogous results for BASIC (Pham et al., 2021) are in Appendix C.
arXiv:2203.05482v3  [cs.LG]  1 Jul 2022

Model soups: averaging weights of multiple ﬁne-tuned models improves accuracy without increasing inference time
1. Introduction
In recent years, research has shown that models pre-trained
on large and diverse datasets learn representations that trans-
fer well to a variety of tasks. As a result, machine learning
practitioners now commonly develop solutions for down-
stream tasks by ﬁne-tuning large pre-trained models (Gir-
shick et al., 2014; Yosinski et al., 2014; Kornblith et al.,
2019; Kolesnikov et al., 2020). Typically, the ﬁne-tuning
process involves two steps: (1) ﬁne-tune models with a va-
riety of hyperparameter conﬁgurations, and (2) select the
model which achieves the highest accuracy on the held-out
validation set. The remaining models are then discarded.
Selecting a single model and discarding the rest has several
downsides. For one, ensembling outputs of many models
can outperform the best single model, albeit at a high com-
putational cost during inference. For another, ﬁne-tuning a
model on downstream tasks can sometimes reduce out-of-
distribution performance (Radford et al., 2021; Andreassen
et al., 2021; Wortsman et al., 2021; Pham et al., 2021), and
the best single model on the target distribution may not be
the best model on out-of-distribution data.
In this work, we propose a more accurate and robust alter-
native to the second step of the conventional recipe in the
context of ﬁne-tuning a large pre-trained model. Instead of
selecting the individual ﬁne-tuned model which achieves the
highest accuracy on the held-out validation set, we average
the weights of models ﬁne-tuned independently, and refer to
the result as a model soup. Given the results of the ﬁrst step—
a hyperparameter sweep over ﬁne-tuned models—averaging
several of these models to form a model soup requires no
additional training and adds no cost at inference time.
Since the loss landscape of neural network training is non-
convex with many solutions in different loss basins, it is per-
haps surprising that averaging the weights of independently
ﬁne-tuned models achieves high performance. However, re-
cent work (Neyshabur et al., 2020) observes that ﬁne-tuned
models optimized independently from the same pre-trained
initialization lie in the same basin of the error landscape,
inspiring our method. Weight averaging along a single train-
ing trajectory has previously been shown to improve the
performance of models in non-transfer settings (Szegedy
et al., 2016; Izmailov et al., 2018). Our approach extends
weight averaging to the context of ﬁne-tuning, where we ﬁnd
that it also works across many independent runs with varied
hyperparemeter conﬁgurations. Our use of a diverse set of
ﬁne-tuned models is inspired by Gontijo-Lopes et al. (2022)
who observe that ensembling independent runs trained with
different hyperparameters improves performance.
We perform a comprehensive experimental study of ﬁne-
tuning to understand the behavior of model soups. For
our main results we ﬁne-tune CLIP (Radford et al., 2021)
and ALIGN (Jia et al., 2021), which are pre-trained with
a contrastive loss on image-text pairs, and a ViT-G model
pre-trained on JFT (Zhai et al., 2021). Our results show that
model soups often outperform the best individual model on
both the in-distribution and natural distribution shift test sets
(Table 1, Figure 1, Figure 5). A model soup composed of
ViT-G models achieves 90.94% on ImageNet (Deng et al.,
2009), surpassing the previous state of the art of 90.88%
attained by the CoAtNet model (Dai et al., 2021) while
requiring 25% fewer FLOPs at inference time.1 In general,
model soups can approach the performance of ensembling,
with no additional computational cost or memory relative
to a single model during inference. Beyond ImageNet and
associated distribution shifts, our results show that model
soups are applicable when ﬁne-tuning on tasks from the
WILDS (Koh et al., 2021) benchmark, and when ﬁne-tuning
transformer models (Vaswani et al., 2017; Devlin et al.,
2019a; Raffel et al., 2020b) for text classiﬁcation.
While the most straightforward approach to making a model
soup is to average all the weights uniformly, we ﬁnd that
greedy soups, where models are sequentially added to the
soup if they improve accuracy on held-out data, outperforms
uniform averaging. Greedy soups avoid adding in models
which may lie in a different basin of the error landscape,
which could happen if, for example, models are ﬁne-tuned
with high learning rates.
In addition to empirical observations, we analytically re-
late the similarity in loss between weight-averaging and
logit-ensembling to the ﬂatness of the loss (i.e., its second
derivative on a line between models) and conﬁdence of the
predictions (expressed via the variance of a logits difference
drawn from the weight-average softmax). We empirically
validate our approximation on a subset of the models we
train and show that it is strongly correlated with the true av-
eraging vs. ensembling performance difference, particularly
in the learning rate regimes where soups are effective and
models achieve higher accuracy.
Paper outline. Our method of model soups is presented and
evaluated in Sections 2 and 3, respectively. Next, Section 4
includes our analysis relating model soups and ensembles,
Section 5 details the scope and limitations of the proposed
method, and Section 6 contextualizes model soups by re-
viewing related work.
2. Method
This section highlights three recipes for model souping, the
uniform, greedy, and learned soup, though the greedy soup
is our central method. We summarize the methods described
1Since our initial submission, we attain 90.98% with BA-
SIC (Pham et al., 2021), which ties the newer CoCa model (Yu
et al., 2022) to their reported precision; see Appendix C.

Model soups: averaging weights of multiple ﬁne-tuned models improves accuracy without increasing inference time
Table 2: The primary methods contrasted in this work. Each θi is a
model found through ﬁne-tuning from a shared initialization. Cost
refers to the memory and compute requirements during inference
relative to a single model. All methods require the same training.
Method
Cost
Best on val. set
f(x, arg maxi ValAcc(θi))
O(1)
Ensemble
1
k
Pk
i=1 f(x, θi)
O(k)
Uniform soup
f

x, 1
k
Pk
i=1 θi

O(1)
Greedy soup
Recipe 1
O(1)
Learned soup
Appendix I
O(1)
in this section in Table 2.
We consider a neural network f(x, θ) with input data x
and parameters θ ∈Rd. Fine-tuning is analogous to stan-
dard neural network training but includes an important dis-
tinction: the parameters are initialized to those found via
pre-training.
Let θ = FineTune(θ0, h) denote the parameters obtained by
ﬁne-tuning with pre-trained initialization θ0 and hyperpa-
rameter conﬁguration h. The hyperparameter conﬁguration
can include the choice of optimizer, data augmentation,
training iterations, and a random seed which will determine
data order.
For hyperparameter conﬁgurations h1, ..., hk let θi
=
FineTune(θ0, hi). Conventionally, the parameters θj which
attain the highest accuracy on a held out validation set
are selected, and the remaining parameters are discarded.
Instead, model soups f(x, θS) use an average of θi, i.e.,
θS =
1
|S|
P
i∈S θi where S ⊆{1, ..., k}. The uniform soup
is constructed by averaging all ﬁne-tuned models θi and so
S = {1, ..., n}.
There are settings in which a hyperparameter conﬁguration
can produce a model with low accuracy that results in a low
accuracy uniform soup. This issue can be circumvented with
a greedy soup (Recipe 1). The greedy soup is constructed by
sequentially adding each model as a potential ingredient in
the soup, and only keeping the model in the soup if perfor-
mance on a held out validation set (disjoint from the training
and test sets) improves. Before running this procedure we
sort the models in decreasing order of validation set accu-
racy, and so the greedy soup can be no worse than the best
individual model on the held-out validation set. We also
explore a more advanced learned soup recipe that optimizes
model interpolation weights by gradient-based minibatch
optimization (see Appendix I for details). This procedure re-
quires simultaneously loading all models in memory which
currently hinders its use with large networks.
Recipe 1 GreedySoup
Input: Potential soup ingredients {θ1, ..., θk} (sorted in
decreasing order of ValAcc(θi)).
ingredients ←{}
for i = 1 to k do
if ValAcc(average(ingredients ∪{θi})) ≥
ValAcc(average(ingredients)) then
ingredients ←ingredients ∪{θi}
return average(ingredients)
3. Experiments
This section presents our key experimental ﬁndings. We
begin with experimental setup (Section 3.1) then provide
intuition for model soups by examining error landscape vi-
sualizations (Section 3.2). Next we present our main results
(Section 3.3), using model soups as an alternative to select-
ing the best performing individual model. The appendix
includes additional results on model soups in the context
of robust ﬁne-tuning (Appendix D) and model soups con-
structed by ﬁne-tuning on different datasets (Appendix E).
3.1. Experimental setup
Our experiments explore the application of model soups
when ﬁne-tuning various models. The primary models we
ﬁne-tune are the CLIP (Radford et al., 2021), ALIGN (Jia
et al., 2021), and BASIC (Pham et al., 2021) models pre-
trained with contrastive supervision from image-text pairs,
a ViT-G/14 model pre-trained on JFT-3B (Zhai et al., 2021),
and transformer models for text classiﬁcation (Devlin et al.,
2019a; Raffel et al., 2020a). Unless otherwise mentioned,
experiments use the CLIP ViT-B/32 model. Fine-tuning is
performed end-to-end (all parameters are modiﬁed) which
typically results in better accuracy than training only the
ﬁnal linear layer (Kornblith et al., 2019; Agrawal et al.,
2014; Chatﬁeld et al., 2014; Azizpour et al., 2015).
We consider two different methods for initializing the ﬁnal
linear layer before ﬁne-tuning. The ﬁrst method initializes
the model from a linear probe (LP), as described in Kumar
et al. (2022), and we refer to this method as LP initializa-
tion. The second method uses the zero-shot initialization,
e.g., using the classiﬁer produced by the text tower of CLIP
or ALIGN as the initialization. Both methods for initializ-
ing the model produce similar trends when applicable, and
unless otherwise stated we use the LP initialization.
For the ensemble baselines (Dietterich, 2000; Lakshmi-
narayanan et al., 2017) we ensemble the logits (unormalized
outputs) of models as in Gontijo-Lopes et al. (2022). Fine-
tuning uses a supervised cross-entropy loss and, unless oth-
erwise mentioned, is conducted on ImageNet (Deng et al.,
2009). When ﬁne-tuning on ImageNet we also evaluate on

Model soups: averaging weights of multiple ﬁne-tuned models improves accuracy without increasing inference time
0
20
40
60
80
100
120
0
5
10
15
20
0
Fine-tuning
ImageNet train loss
0
20
40
60
80
100
120
0
5
10
15
20
0
ImageNet test error
0
20
40
60
80
100
120
0
5
10
15
20
0
Avg. error on 5 distribution shifts
0
20
40
60
80
100
120
0
1
2
3
4
0
ImageNet train loss
Initialization
LR = 3 10
5 (seed 0)
LR = 3 10
5 (seed 1)
LR = 3 10
6
0
20
40
60
80
100
120
0
1
2
3
4
0
ImageNet test error
0
20
40
60
80
100
120
0
1
2
3
4
0
Avg. error on 5 distribution shifts
3.6e-04
8.7e-04
1.1e-03
1.6e-03
2.2e-03
3.2e-03
4.8e-03
7.2e-03
1.1e-02
> 1.1e-02
21.5
21.9
22.4
23.3
25.2
29.1
37.2
54.2
89.4
> 89.4
46.2
46.6
47.0
47.7
49.2
52.1
57.9
69.4
92.2
> 92.2
3.6e-04
8.7e-04
1.1e-03
1.6e-03
2.2e-03
3.2e-03
4.8e-03
7.2e-03
1.1e-02
> 1.1e-02
21.5
21.9
22.4
23.3
25.2
29.1
37.2
54.2
89.4
> 89.4
46.2
46.6
47.0
47.7
49.2
52.1
57.9
69.4
92.2
> 92.2
Figure 2: The solution with the highest accuracy is often not a ﬁne-tuned model but rather lies between ﬁne-tuned models. This ﬁgure
shows loss and error on a two dimensional slice of the loss and error landscapes. We use the zero-shot initialization θ0 and ﬁne-tune twice
(illustrated by the gray arrows), independently, to obtain solutions θ1 and θ2. As in Garipov et al. (2018), we obtain an orthonormal basis
u1, u2 for the plane spanned by these models, and the x and y-axis show movement in parameter space in these directions, respectively.
20
30
40
50
60
70
80
Angle between models 
0
1
2
3
4
5
Accuracy gain
Acc(
1
2
1 + 1
2
2)
1
2(Acc(
1) + Acc(
2))
Vary seed
Vary learning rate
Vary augmentation
Figure 3: The advantage of averaging solutions (y-axis) is corre-
lated with the angle φ between between solutions, while varying
hyperparameter conﬁgurations between pairs enables a larger φ.
Each point corresponds to a pair of models θ1, θ2 that are ﬁne-
tuned independently from a shared initialization θ0 with different
hyperparameter conﬁgurations. The angle φ between between
solutions refers to the angle between θ1 −θ0 and θ2 −θ0 (i.e., the
initialization is treated as the origin). Accuracy is averaged over
ImageNet and the ﬁve distribution shifts described in Section 3.1.
the ﬁve natural distribution shifts: ImageNetV2 (Recht et al.,
2019), ImageNet-R (Hendrycks et al., 2021a), ImageNet-
Sketch (Wang et al., 2019), ObjectNet (Barbu et al., 2019),
and ImageNet-A (Hendrycks et al., 2021b). We often report
results averaged over these ﬁve distribution shifts. Since the
ofﬁcial ImageNet validation set is typically used as the test
set, we use roughly 2% of the ImageNet training set as a
held-out validation set for constructing greedy soups.
3.2. Intuition and motivation
Error landscape visualizations. To provide intuition, we
visualize a two dimensional slice of the training loss and
test error landscape when ﬁne-tuning CLIP on ImageNet.
In these experiments, we use the zero-shot initialization
θ0 ∈Rd and ﬁne-tune twice, independently, to produce
solutions θ1 and θ2. The points θ0, θ1 and θ2 deﬁne a plane
in parameter space, and we evaluate the ImageNet train
loss, ImageNet test error, and the test error on the ﬁve afore-
mentioned distribution shifts on this plane. The results are
illustrated in Figure 2 where the zero-shot initialization (θ0)
is shown as a star and a solution ﬁne-tuned with learning
rate 3 · 10−5 (θ1) is shown as a blue square. For θ2 we
either use the same learning rate as θ1 (but vary the random
seed) or learning rate 3 · 10−6. For both the in-distribution
and out-of-distribution test sets, the loss/error contours are
basin-shaped, and none of the three points is optimal.
These results suggest that (1) interpolating the weights of
two ﬁne-tuned solutions can improve accuracy compared
to individual models and (2) more uncorrelated solutions—
models that form an angle2 closer to 90 degrees—may lead
to higher accuracy on the linear interpolation path.
To investigate the correlation between accuracy improve-
ment and angle, we consider a series of models trained with
different seeds, learning rates, and data augmentation. For
each pair θ1, θ2, we compare the accuracy of their average
with the average of their accuracies, Acc
  1
2θ1 + 1
2θ2

−
1
2 (Acc (θ1) + Acc (θ2)), which we refer to as the interpo-
2In particular, the angle φ between θ1 −θ0 and θ2 −θ0, i.e.,
the angle between the arrows shown in Figure 2.

Model soups: averaging weights of multiple ﬁne-tuned models improves accuracy without increasing inference time
75
76
77
78
79
80
81
ImageNet Accuracy of two model soup
75
76
77
78
79
80
81
ImageNet Accuracy of two model ensemble
40
45
50
55
Avg. accuracy of two model soup
on distribution shifts
40
45
50
55
Avg. accuracy of two model ensemble
on distribution shifts
y = x
Max learning rate in model pair
(log scale)
Figure 4: Ensemble performance is correlated with model soup performance. Each point on the scatter plot is a model pair with different
hyperparameters. The x-axis is the accuracy when the weights of the two models are averaged (i.e., the two model soup) while the y-axis
is the accuracy of the two model ensemble. Ensembles often perform slightly better than soups on ImageNet (left) while the reverse is true
on the distribution shifts (right). Each model pair consists of two random greed diamonds from Figure 1.
84
85
86
87
88
ImageNet (top-1, %)
66
68
70
72
74
76
Avg. accuracy on 5 distribution shifts (top-1, %)
Individual models with
various hyperparameters
Initialization
Uniform soup
Greedy soup
Figure 5: Model soups improve accuracy when ﬁne-tuning ALIGN.
lation advantage. Figure 3 illustrates the results, in which
we observe that the interpolation advantage is correlated
with the angle φ and that varying the learning rate, seed, or
data augmentation can produce solutions which are more
orthogonal. Experimental details and discussion of high
learning rates provided in Appendix J.1.
Ensemble comparison. Figure 4 observes that ensemble
performance is correlated with soup performance for mod-
erate and small learning rates. We consider pairs of models
selected at random from the individual solutions in Figure 1,
and ﬁnd that the maximum learning rate of the models in the
pair is indicative of the ensemble accuracy, soup accuracy,
and their relation: When learning rate is small, ensemble
accuracy and soup accuracy are similar, but both are subop-
timal. For moderate learning rate values, ensemble accuracy
and soup accuracy are both high. For high learning rate
values, ensemble performance exceeds soup performance,
but ensembles/soups with moderate learning rates perform
better. Overall, ensembles achieve higher accuracy on Ima-
geNet while the reverse is true on the distribution shifts.
One dimensional hyperparameter grids. Finally, in Ap-
pendix F we ask the question: for a one dimensional grid
of hyperparameters {ha, ..., hb}, how does averaging the
models ﬁne-tuned with hyperparameter conﬁgurations ha
and hb corresponding to the endpoints compare with picking
the best individual model ﬁne-tuned with hyperparameter
conﬁguration h ∈{ha, ..., hb}? The hyperparameters we
vary are optimizer, augmentation, and learning rate. For
the majority of grid searches, the average of the endpoints
outperforms the best individual model in the grid.
3.3. Model soups
With the gains of averaging two ﬁne-tuned models in mind,
we turn our attention to averaging many models with differ-
ent hyperparameters: this section presents our main results,
which show that averaging ﬁne-tuned models can be used as
an alternative to the conventional procedure of selecting the
single model which performs best on the held-out validation
set. We explore CLIP (Radford et al., 2021) and ALIGN (Jia
et al., 2021) ﬁne-tuned on ImageNet (Deng et al., 2009) (Sec-
tion 3.3.1), ViT-G pre-trained on JFT-3B (Zhai et al., 2021)
and ﬁne-tuned on ImageNet (Section 3.3.2), and transformer
models ﬁne-tuned on text classiﬁcation tasks (Section 3.3.3).
Appendix G additionally explores (1) CLIP ViT-L ﬁne-tuned
on WILDS (Koh et al., 2021) and CIFAR-10 and (2) an
ImageNet-22k-pretrained ViT-B ﬁne-tuned on ImageNet.
Moreover, Appendix C shows that model soups improve
accuracy when ﬁne-tuning BASIC (Pham et al., 2021).

Model soups: averaging weights of multiple ﬁne-tuned models improves accuracy without increasing inference time
Table 3: Ablation on multiple methods from Table 2 and their
variants when when ﬁne-tuning CLIP ViT-B/32 with the random
hyperparameter search described in Section 3.3.1. For “Greedy
soup (random order)”, we try three random model orders when
running the greedy soup procedure (by default, models are sorted
by decreasing held-out val accuracy). The “Learned soup” and its
variants are descried in Appendix I. The best in best individual
model refers to ImageNet accuracy.
ImageNet
Dist. shifts
Best individual model
80.38
47.83
Second best model
79.89
43.87
Uniform soup
79.97
51.45
Greedy soup
81.03
50.75
Greedy soup (random order)
80.79 (0.05)
51.30 (0.16)
Learned soup
80.89
51.07
Learned soup (by layer)
81.37
50.87
Ensemble
81.19
50.77
Greedy ensemble
81.90
49.44
3.3.1. FINE-TUNING CLIP AND ALIGN
We begin our study of model soups by considering two-
pretrained models, CLIP ViT-B/32 and ALIGN EfﬁcientNet-
L2, and performing a hyperparameter sweep for the ﬁne-
tuning each model on ImageNet. For CLIP we use a random
hyperparameter search over learning rate, weight decay,
training epochs, label smoothing, and data augmentation,
obtaining 72 ﬁne-tuned models (details in Appendix J.2.1).
For ALIGN we use a grid search over learning rate, data
augmentation, and mixup, obtaining 12 ﬁne-tuned models
(details in Appendix J.2.2). To form our greedy soups, we
sort models in order of decreasing accuracy on the held-out
validation set before applying Recipe 1. For both CLIP and
ALIGN, the greedy soup selects 5 models. Figure 1 and
5 show the performance of the resulting models and their
uniform and greedy soups for CLIP and ALIGN. The greedy
soup improves on over the best model in the hyperparameter
sweep by 0.7 and 0.5 percentage points, respectively.
Furthermore, we show that, for essentially any number of
models, the greedy soup outperforms the best single model
on both the ImageNet and the out-of-distribution test sets.
We consider an additional setting where we prepare a se-
quence of soups by sequentially adding CLIP models from
the hyperparameter sweep in random order. Appendix Fig-
ure B.1 shows the performance of the uniform and greedy
soup, as well as the best single model so far and a logit
ensemble, as a function of the number of models considered.
The greedy soup is better than the uniform soup on Ima-
geNet and comparable to it out-of-distribution. The logit
ensemble is better than the greedy soup on ImageNet, but
worse out-of-distribution.
Table 3 lists the performance of the CLIP soups and base-
lines described above, as well as additional soup variants
described in Appendix I.
To further establish the generality of the model soup, we
replicate the CLIP hyperparameter sweep experiment on
two image classiﬁcation tasks from WILDS (Koh et al.,
2021), namely FMoW (Christie et al., 2018) and iWild-
Cam (Beery et al., 2021). Appendix Figure G.1 shows
results qualitatively similar to our ImageNet experiment,
and Appendix J.2.1 describes experimental details.
We report several additional variants and baselines for the ex-
periment described above. In Appendix H we present results
for different hyperparameter sweeps and ﬁne-tuning initial-
izations, when ﬁne-tuning CLIP on ImageNet. For instance,
we try a standard grid search which is similar to the grid
search described for ALIGN above, and an extreme grid
search which includes solutions ﬁne-tuned with extreme
hyperparameters that result in badly performing models (de-
tails in Appendix J.2.1). Moreover, Appendix L compares
model soups with additional baselines, including distillation
from an ensemble as in Hinton et al. (2014), exponential
moving averaging (Szegedy et al., 2016), stochastic weight
averaging (Izmailov et al., 2018), and sharpness aware mini-
mization (Foret et al., 2021).
We highlight a few interesting takeaways from these experi-
ments: (1) The greedy soup outperforms the best individual
model—with no extra training and no extra compute during
inference, we were able to produce a better model. (2) While
the uniform soup can outperform the best individual model,
we only observe this when all individual models achieve
high accuracy (e.g., when ﬁne-tuning ALIGN in Figure 1);
unlike the examples in Figure 2, there can be an error barrier
between ﬁne-tuned models. We mainly observe this when
ﬁne-tuning with high learning rates (this is illustrated in
Appendix J.1, Figure J.1). However, these high learning
rate models also have a lower accuracy, and are therefore
excluded by the greedy soup.
3.3.2. FINE-TUNING A VIT-G MODEL PRE-TRAINED ON
JFT-3B
To test whether the gains obtained by model soups are ad-
ditive with other techniques used to obtain state-of-the-art
models, we applied our greedy soup technique to 58 ViT-
G/14 models ﬁne-tuned on ImageNet. We vary the learning
rate, decay schedule, loss function, and minimum crop size
in the data augmentation, and optionally apply RandAug-
ment (Cubuk et al., 2020), mixup (Zhang et al., 2017), or
CutMix (Yun et al., 2019). We also train four models with
sharpness-aware minimization (SAM) (Foret et al., 2021).
For further details of our hyperparameter sweep, see Ap-
pendix J.2.3. For each model training run, we save exponen-
tial moving averages (EMA) of the weights (Szegedy et al.,
2016) computed with decay factors of 0.999 (low EMA)
and 0.9999999 (high EMA). Whereas high EMA generally
provides the best single-model accuracy, both greedy soup

Model soups: averaging weights of multiple ﬁne-tuned models improves accuracy without increasing inference time
Table 4: Greedy soup improves over the best individual models obtained in a hyperparameter sweep for ViT-G/14 pre-trained on JFT-3B
and ﬁne-tuned on ImageNet, both in- and out-of-distribution. Accuracy numbers not signiﬁcantly different from the best are bold-faced.
Statistical comparisons are performed using an exact McNemar test or permutation test at α = 0.05. Avg shift accuracy of the best model
on each test set is the best average accuracy of any individual model. Analogous results when ﬁne-tuning BASIC-L are available in
Appendix C.
ImageNet
Distribution shifts
Method
Top-1
ReaL
Multilabel
IN-V2
IN-R
IN-Sketch
ObjectNet
IN-A
Avg shifts
ViT/G-14 (Zhai et al., 2021)
90.45
90.81
–
83.33
–
–
70.53
–
–
CoAtNet-7 (Dai et al., 2021)
90.88
–
–
–
–
–
–
–
–
Our models/evaluations based on ViT-G/14:
ViT/G-14 (Zhai et al., 2021) (reevaluated)
90.47
90.86
96.89
83.39
94.38
72.37
71.16
89.00
82.06
Best model on held out val set
90.72
91.04
96.94
83.76
95.04
73.16
78.20
91.75
84.38
Best model on each test set (oracle)
90.78 91.78
97.29
84.31
95.04
73.73
79.03 92.16
84.68
Greedy ensemble
90.93 91.29
97.23
84.14
94.85
73.07
77.87
91.69
84.33
Greedy soup
90.94 91.20
97.17
84.22
95.46
74.23
78.52 92.67
85.02
Table 5: Performance of model soups on four text classiﬁcation datasets from the GLUE benchmark (Wang et al., 2018).
Model
Method
MRPC
RTE
CoLA
SST-2
BERT (Devlin et al., 2019b)
Best individual model
88.3
61.0
59.1
92.5
Greedy soup
88.3 (+0.0)
61.7 (+0.7)
59.1 (+0.0)
93.0 (+0.5)
T5 (Raffel et al., 2020b)
Best individual model
91.8
78.3
58.8
94.6
Greedy soup
92.4 (+0.6)
79.1 (+0.8)
60.2 (+0.4)
94.7 (+0.1)
and greedy ensembling attain higher validation accuracy
when applied to parameters with low EMA. We report the
highest single model accuracy numbers obtained with either
EMA decay value, but perform greedy soup and ensembling
with models trained with EMA decay of 0.999. For each
combination of training run and EMA decay rate, we evalu-
ate accuracy on our held out validation set every 1000 steps.
We use these accuracy values to pick the best checkpoint for
ensembling, souping, and subsequent evaluation.
In Table 4, we report results on the ImageNet validation set
and the ﬁve distribution shift datasets studied above as well
as two relabeled ImageNet validation sets, ReaL (Beyer
et al., 2020) and multilabel (Shankar et al., 2020). Our
greedy soup procedure selects 14 of the 58 models ﬁne-
tuned as part of our hyperparameter sweep, and this soup
performs statistically signiﬁcantly better than the best indi-
vidually ﬁne-tuned model selected based on our held out
validation set on all datasets except for ObjectNet. Even
when we give an unfair advantage to individually ﬁne-tuned
models by selecting them based on their performance on
each test set (denoted “oracle” in Table 4), the greedy soup,
which was selected using only in-distribution data, remains
superior on most datasets. Only on ReaL and ObjectNet
does there exist an individual model that performs statisti-
cally signiﬁcantly better than the soup, and the best model
differs between those two datasets. Greedy ensembling per-
forms similarly to the greedy soup in terms of ImageNet top-
1 and multilabel accuracy, and slightly better on ReaL, but
signiﬁcantly worse on all distribution shift datasets except
for ImageNet-V2. Thus, greedy soup can provide additional
gains on top of standard hyperparameter tuning even in the
extremely high accuracy regime.
3.3.3. FINE-TUNING ON TEXT CLASSIFICATION TASKS
To test whether the gains obtained by model soups extend
to domains beyond image classiﬁcation, we conduct prelim-
inary experiments with natural language processing (NLP).
While more investigation is warranted to establish the appli-
cability of model soups for NLP, we believe our experiments
are a promising initial step. In particular, we ﬁne-tune BERT
(Devlin et al., 2019b) and T5 (Raffel et al., 2020b) models
on four text classiﬁcation tasks from the GLUE benchmark
(Wang et al., 2018): MRPC (Dolan and Brockett, 2005),
RTE (Dagan et al., 2005; Bar-Haim et al., 2006; Giampic-
colo et al., 2007; Bentivogli et al., 2009), CoLA (Warstadt
et al., 2019) and SST-2 (Socher et al., 2013), as in (Dodge
et al., 2020). We use the standard metric for each dataset:
average of accuracy and F1 score for MRPC, accuracy for
RTE, Matthews correlation for CoLA (Matthews, 1975) and
accuracy for SST-2. Details are provided in Appendix J.4.
We ﬁne-tune 32 models for each dataset with a random
hyper-parameter search over learning rate, batch size, num-
ber of epochs and random seed. Table 5 reports the corre-
sponding metric on the validation set for BERT-base un-
cased (Devlin et al., 2019a) and T5-base (Raffel et al.,
2020b). Additional experimental details and results for
more models are provided in Appendix J.5. While the im-
provements are not as pronounced as in image classiﬁcation,
the greedy soup can improve performance over the best
individual model in many cases.

Model soups: averaging weights of multiple ﬁne-tuned models improves accuracy without increasing inference time
4. Analytically comparing soups to ensembles
The goal of this section is to obtain complementary ana-
lytical insight into the effectiveness of model soups. For
simplicity, we consider a soup consisting of only two mod-
els with parameters θ0 and θ1. For weighting parameter
α ∈[0, 1] we let θα = (1 −α)θ0 + αθ1 denote the weight-
averaged soup. We would like to understand when the soup
error, errα := Ex,y1{arg maxi fi(x; θα) ̸= y}, would be
lower that the best of both endpoints, min{err0, err1}.
Note that just convexity of errα in α does not by itself imply
superiority of the soup to both endpoints, as the minimum
of errα over α may be obtained at the endpoints even when
errα is convex. To get further leverage on the problem, we
compare the soup to the logit-level ensemble f ens
α (x) = (1−
α)f(x; θ0) + αf(x; θ1). The rich literature on ensembles
(see Sec. 6) tells us that the expected error of the ensemble,
errens
α , is often strictly below min{err0, err1} for neural
networks. Therefore, whenever errα ≈errens
α
we expect
the soup to outperform both endpoint models.
To analytically compare the soup and the ensemble, we
replace the 0-1 loss with a differentiable surrogate. Specif-
ically, we consider the cross-entropy loss ℓ(f, y)
=
log
P
y′ efy′−fy

. We let Lsoup
α
= Ex,yℓ(βf(x; θα), y)
denote the β-calibrated expected loss of the soup, and sim-
ilarly deﬁne Lens
α
= Ex,yℓ(βf ens
α (x), y) for the ensemble.
We derive the following approximation for the loss differ-
ence:
Lsoup
α
−Lens
α
≈α(1 −α)
2

−d2
dα2 Lsoup
α
+ β2ExVarY ∼psftmx(βf(x;θα)) [∆fY (x)]

,
(1)
where [psftmx(f)]i = efi/ P
j efj is the standard “softmax”
distribution and ∆f(x) = f(x; θ1) −f(x; θ0) is the differ-
ence between the endpoint logits. We obtain our approxi-
mation in the regime where the logits are not too far from
linear; see Appendix K.3 for a detailed derivation.
The ﬁrst term in approximation (1) is negatively propor-
tional to the second derivative of the loss along the trajec-
tory: when the approximation holds, convexity of the loss
indeed favors the soup. However, the second term in the
approximation does not follow from the “convex basin” in-
tuition. This term always favors the ensemble, but is small
in one of two cases: (a) the somewhat trivial case when the
endpoint models are similar (so that ∆f is small) and (b)
when the soup produces conﬁdent predictions, implying that
psftmx(βf(x; θα)) is close to a point mass and consequently
the variance term is small.
To test our approximation, we evaluate it over of set of ﬁne-
tuned models with different learning rates, augmentation
strategies, random seeds and α values. We set β to calibrate
the soup model, and ﬁnd that it improves the ability of our
approximation to predict the soup/ensemble error difference;
see Appendix K.4 for detailed description of our setup.
Figure K.1 summarizes the results of our empirical eval-
uations. When excluding the high learning rate of 10−4
(center and right panels),3 we see that the approximation
is strongly correlated with both the true difference in loss
as well as the difference in error, and the approximation
and true loss difference generally agree in sign. Additional
details are provided in Appendix K.
5. Scope and limitations
While this work has so far demonstrated that averaging
many ﬁne-tuned models is a useful technique for improving
accuracy, this section explores two limitations of the ap-
proach. The ﬁrst is the applicability of model soups, and the
second is the failure of model soups to substantially improve
calibration.
Applicability. So far our experiments have mainly explored
models pre-trained on large, heterogeneous datasets. In
Appendix G we also explore model soups for an ImageNet-
22k pre-trained model. While the greedy soup still provides
improvements on ImageNet, these improvements are less
substantial compared to those observed when ﬁne-tuning
CLIP and ALIGN.
Calibration.
While ensembles improve model calibra-
tion (Guo et al., 2017; Roelofs et al., 2020), model soups do
not have the same effect. As hyperparameters can also have
an effect on calibration, we consider the ensemble and soup
of 20 models which are identical other than random seed.
Results are illustrated in Figure B.2 using the calibration
metrics of Roelofs et al. (2020).
6. Related work
Averaging model weights.
Averaging the weights of
models is a popular approach in convex optimization and
deep learning. Most applications study models along the
same optimization trajectory, e.g. (Ruppert, 1988; Polyak,
1990; Szegedy et al., 2016; Izmailov et al., 2018; Zhang
et al., 2019; Kaddour et al., 2022; Junczys-Dowmunt et al.,
2016). By contrast, Nagarajan and Kolter (2019); Frankle
et al. (2020); Neyshabur et al. (2020); Von Oswald et al.
(2020) and Matena and Raffel (2021) weight-average mod-
els which share an initialization but are optimized indepen-
dently. Nagarajan and Kolter (2019) observed that models
trained on MNIST (LeCun, 1998) from the same random
3Fine-tuned models with learning rate 10−4 are far in weight
space from the initial model and are often rejected when forming
greedy soups. Therefore, we do not expect our approximation to
be tight for these learning rates.

Model soups: averaging weights of multiple ﬁne-tuned models improves accuracy without increasing inference time
initialization are connected in weight space by a linear path
of high accuracy. Frankle et al. (2020) ﬁnd that, when train-
ing a pair of models from scratch on harder datasets such as
ImageNet with the same hyperparameter conﬁguration and
initialization but different data order, interpolating weights
achieves no better than random accuracy. However, Fran-
kle et al. (2020) showed that when the two models share
a portion of their optimization trajectory, accuracy does
not drop when they are averaged. Analogously, Neyshabur
et al. (2020) demonstrate that when two models are ﬁne-
tuned with the same pre-trained initialization, the interpo-
lated model attains at least the accuracy of the endpoints.
Unlike Nagarajan and Kolter (2019); Frankle et al. (2020);
Neyshabur et al. (2020) we consider averaging many models
with varied hyperparameter conﬁgurations.
In the late phases of training, Von Oswald et al. (2020) make
copies of a subset of the neural network parameters (e.g, the
batch norm weights, the classiﬁcation layer, etc.). These
parameters are then optimized independently and subse-
quently averaged. In contrast to Von Oswald et al. (2020),
a) we average across independent runs with hyperparemter
diversity, b) we modify all weights in the network, and c)
we consider the transfer setting. Matena and Raffel (2021)
merge models with the same pre-trained initialization that
are ﬁne-tuned on different text classiﬁcation tasks. They
also propose Fisher information as an alternative technique
for model merging. We experiment with averaging mod-
els which are trained on different datasets in Appendix E,
however, in contrast to Matena and Raffel (2021) we do not
use data from the target distribution. Wortsman et al. (2021)
average zero-shot and ﬁne-tuned models, ﬁnding improve-
ments in- and out-of-distribution. In contrast to Wortsman
et al. (2021), we average models across many independent
runs which provides more substantial improvements.
Stochastic Weight Averaging (SWA) (Izmailov et al., 2018),
which averages weights along a single optimization trajec-
tory, is also motivated by the relation between ensembling
model outputs and averaging model weights. In contrast, the
averaging we propose is across independent runs. Moreover,
while their analysis relates the averaged network outputs
(i.e., the logit ensemble) to the output of the a network with
the averaged weights, our analysis (Section 4) goes a step
further and relates the classiﬁcation losses associated with
these two vectors.
Pre-training and ﬁne-tuning.
In computer vision and
natural language processing, the best performing models
are often pre-trained on a large dataset before being ﬁne-
tuned on data from the target task (Donahue et al., 2014;
Yosinski et al., 2014; Sharif Razavian et al., 2014; Girshick
et al., 2014; Mahajan et al., 2018; Kornblith et al., 2019;
Yalniz et al., 2019; Kolesnikov et al., 2020; Bommasani
et al., 2021). This paradigm is also referred to as trans-
fer learning. Recently, image-text pre-training has become
increasingly popular in computer vision as a pre-training
task (Radford et al., 2021; Jia et al., 2021; Mu et al., 2021;
Pham et al., 2021; Yu et al., 2022). Recent work has ex-
plored alternative strategies for adapting these models to
speciﬁc target tasks (Zhou et al., 2021; Gao et al., 2021;
Zhang et al., 2021), for instance via a lightweight residual
feature adapter. In contrast, our work explores standard
end-to-end ﬁne-tuned models. Other work has attempted
to improve transfer learning by regularizing models toward
their initialization (Xuhong et al., 2018), choosing layers to
tune on a per-example basis (Guo et al., 2019), reinitializing
layers over the course of training (Li et al., 2020), or using
multiple pretrained models with data-dependent gating (Shu
et al., 2021).
Ensembles.
Combining the outputs of many models is
a foundational technique for improving the accuracy and
robustness of machine learning models (Dietterich, 2000;
Bauer and Kohavi, 1999; Breiman, 1996; Friedman et al.,
2001; Lakshminarayanan et al., 2017; Freund and Schapire,
1997). Ovadia et al. (2019) show that ensembles exhibit
high accuracy under distribution shift. Mustafa et al. (2020)
propose a method for identifying subsets of pre-trained
models for ﬁne-tuning and later ensembling them, ﬁnding
strong in-distribution accuracy and robustness to distribu-
tion shift. Gontijo-Lopes et al. (2022) conduct a large-scale
study of ensembles, ﬁnding that higher divergence in train-
ing methodology leads to uncorrelated errors and better
ensemble accuracy. Finally, previous work has explored
building ensembles of models produced by hyperparameter
searches (Snoek et al., 2015; Mendoza et al., 2016; Saikia
et al., 2020), including greedy selection strategies (Caru-
ana et al., 2004; 2006; L´evesque et al., 2016; Wenzel et al.,
2020). Importantly, ensembles require a separate inference
pass through each model, which increases computational
costs. When the number of models is large, this can be
prohibitively expensive. Unlike ensembles, model soups
require no extra compute at inference time.
7. Conclusion
Our results challenge the conventional procedure of select-
ing the best model on the held-out validation set when ﬁne-
tuning. With no extra compute during inference, we are of-
ten able to produce a better model by averaging the weights
of multiple ﬁne-tuned solutions.
Acknowledgements
We thank Ting Chen, Jesse Dodge, Ben Eysenbach, David
Fleet, Pieter-Jan Kindermans, Mohammad Norouzi, Sarah
Pratt and Vivek Ramanujan for helpful discussions and draft
feedback, Lucas Beyer and Xiaohua Zhai for assistance
with ViT-G/14 ﬁne-tuning, and Hyak at UW for comput-
ing support. YC was supported in part by the Israeli Sci-

Model soups: averaging weights of multiple ﬁne-tuned models improves accuracy without increasing inference time
ence Foundation (ISF) grant no. 2486/21, the Len Blavat-
nik and the Blavatnik Family foundation, and The Yandex
Initiative for Machine Learning. This work is in part sup-
ported by the NSF AI Institute for Foundations of Machine
Learning (IFML), Open Philanthropy, NSF IIS 1652052, IIS
17303166, DARPA N66001-19-2-4031, DARPA W911NF-
15-1-0543 and gifts from Allen Institute for AI.
References
Pulkit Agrawal, Ross Girshick, and Jitendra Malik. Analyzing
the performance of multilayer neural networks for object recog-
nition.
In European conference on computer vision, pages
329–344. Springer, 2014.
Anders Andreassen, Yasaman Bahri, Behnam Neyshabur, and Re-
becca Roelofs. The evolution of out-of-distribution robustness
throughout ﬁne-tuning, 2021. https://arxiv.org/abs/
2106.15831.
Hossein Azizpour, Ali Sharif Razavian, Josephine Sullivan, Atsuto
Maki, and Stefan Carlsson.
From generic to speciﬁc deep
representations for visual recognition. In Proceedings of the
IEEE conference on computer vision and pattern recognition
workshops, pages 36–45, 2015.
Hessam Bagherinezhad, Maxwell Horton, Mohammad Raste-
gari, and Ali Farhadi.
Label reﬁnery:
Improving ima-
genet classiﬁcation through label progression. arXiv preprint
arXiv:1805.02641, 2018.
Roy Bar-Haim, Ido Dagan, Bill Dolan, Lisa Ferro, Danilo Gi-
ampiccolo, Bernardo Magnini, and Idan Szpektor. The second
pascal recognising textual entailment challenge. In Proc. of the
II PASCAL challenge, 2006.
Andrei Barbu, David Mayo, Julian Alverio, William Luo, Christo-
pher Wang, Dan Gutfreund, Josh Tenenbaum, and Boris Katz.
Objectnet: A large-scale bias-controlled dataset for pushing the
limits of object recognition models. In Advances in Neural In-
formation Processing Systems (NeurIPS), 2019. URL https:
//proceedings.neurips.cc/paper/2019/file/
97af07a14cacba681feacf3012730892-Paper.
pdf.
Eric Bauer and Ron Kohavi. An empirical comparison of voting
classiﬁcation algorithms: Bagging, boosting, and variants. Ma-
chine learning, 1999. https://link.springer.com/
article/10.1023/A:1007515423169.
Sara Beery, Arushi Agarwal, Elijah Cole, and Vighnesh Birodkar.
The iwildcam 2021 competition dataset. In Conference on Com-
puter Vision and Pattern Recognition (CVPR) FGVC8 Work-
shop, 2021. https://arxiv.org/abs/2105.03494.
Luisa Bentivogli, Peter Clark, Ido Dagan, and Danilo Giampiccolo.
The ﬁfth pascal recognizing textual entailment challenge. In
TAC, 2009.
Lucas Beyer, Olivier J H´enaff, Alexander Kolesnikov, Xiaohua
Zhai, and A¨aron van den Oord. Are we done with imagenet?
arXiv preprint arXiv:2006.07159, 2020.
Lucas Beyer, Xiaohua Zhai, Am´elie Royer, Larisa Markeeva,
Rohan Anil, and Alexander Kolesnikov.
Knowledge distil-
lation: A good teacher is patient and consistent, 2021. URL
https://arxiv.org/abs/2106.05237.
Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Alt-
man, Simran Arora, Sydney von Arx, Michael S Bernstein,
Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et al.
On the opportunities and risks of foundation models, 2021.
https://arxiv.org/abs/2108.07258.
Lukas Bossard, Matthieu Guillaumin, and Luc Van Gool.
Food-101–mining discriminative components with random
forests.
In European Conference on Computer Vision
(ECCV), 2014. https://data.vision.ee.ethz.ch/
cvl/datasets_extra/food-101/.
Leo Breiman.
Bagging predictors.
Machine learning,
1996.
https://link.springer.com/article/10.
1007/BF00058655.
Rich Caruana, Alexandru Niculescu-Mizil, Geoff Crew, and Alex
Ksikes. Ensemble selection from libraries of models. In Pro-
ceedings of the twenty-ﬁrst international conference on Machine
learning, page 18, 2004.
Rich Caruana, Art Munson, and Alexandru Niculescu-Mizil. Get-
ting the most out of ensemble selection. In Sixth International
Conference on Data Mining (ICDM’06), pages 828–833. IEEE,
2006.
Ken Chatﬁeld, Karen Simonyan, Andrea Vedaldi, and Andrew
Zisserman. Return of the devil in the details: Delving deep
into convolutional nets. In British Machine Vision Conference,
2014.
Gordon Christie, Neil Fendley, James Wilson, and Ryan Mukher-
jee. Functional map of the world. In Conference on Com-
puter Vision and Pattern Recognition (CVPR), 2018. https:
//arxiv.org/abs/1711.07846.
Mircea Cimpoi, Subhransu Maji, Iasonas Kokkinos, Sammy Mo-
hamed, and Andrea Vedaldi. Describing textures in the wild.
In Conference on Computer Vision and Pattern Recognition
(CVPR), 2014. https://arxiv.org/abs/1311.3618.
Ekin D Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V Le.
RandAugment: Practical automated data augmentation with a
reduced search space. In Conference on Computer Vision and
Pattern Recognition (CVPR), 2020. https://arxiv.org/
abs/1909.13719.
Ido Dagan, Oren Glickman, and Bernardo Magnini. The pascal
recognising textual entailment challenge. In Machine Learning
Challenges Workshop, 2005.
Zihang Dai, Hanxiao Liu, Quoc Le, and Mingxing Tan. CoAtNet:
Marrying convolution and attention for all data sizes. Advances
in Neural Information Processing Systems, 34, 2021.
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-
Fei.
Imagenet: A large-scale hierarchical image database.
In Conference on Computer Vision and Pattern Recognition,
2009. https://ieeexplore.ieee.org/document/
5206848.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina
Toutanova. BERT: Pre-training of deep bidirectional transform-
ers for language understanding. In North American Chapter of
the Association for Computational Linguistics (NAACL), 2019a.
URL https://aclanthology.org/N19-1423.

Model soups: averaging weights of multiple ﬁne-tuned models improves accuracy without increasing inference time
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina
Toutanova. BERT: Pre-training of deep bidirectional trans-
formers for language understanding. In Proceedings of the
2019 Conference of the North American Chapter of the As-
sociation for Computational Linguistics: Human Language
Technologies, Volume 1 (Long and Short Papers), pages 4171–
4186, Minneapolis, Minnesota, June 2019b. Association for
Computational Linguistics. doi: 10.18653/v1/N19-1423. URL
https://aclanthology.org/N19-1423.
Thomas G Dietterich.
Ensemble methods in machine learn-
ing. In International workshop on multiple classiﬁer systems,
2000.
https://link.springer.com/chapter/10.
1007/3-540-45014-9_1.
Jesse Dodge, Gabriel Ilharco, Roy Schwartz, Ali Farhadi, Han-
naneh Hajishirzi, and Noah Smith. Fine-tuning pretrained lan-
guage models: Weight initializations, data orders, and early
stopping. arXiv preprint arXiv:2002.06305, 2020.
Bill Dolan and Chris Brockett. Automatically constructing a corpus
of sentential paraphrases. In Proc. of IWP, 2005.
Jeff Donahue, Yangqing Jia, Oriol Vinyals, Judy Hoffman, Ning
Zhang, Eric Tzeng, and Trevor Darrell. Decaf: A deep con-
volutional activation feature for generic visual recognition. In
International conference on machine learning, pages 647–655.
PMLR, 2014.
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk
Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa
Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly,
Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16
words: Transformers for image recognition at scale. In Interna-
tional Conference on Learning Representations (ICLR), 2021.
https://arxiv.org/abs/2010.11929.
Pierre Foret, Ariel Kleiner, Hossein Mobahi, and Behnam
Neyshabur. Sharpness-aware minimization for efﬁciently im-
proving generalization. In International Conference on Learn-
ing Representations, 2021. https://openreview.net/
forum?id=6Tm1mposlrM.
Jonathan Frankle, Gintare Karolina Dziugaite, Daniel Roy, and
Michael Carbin. Linear mode connectivity and the lottery ticket
hypothesis. In International Conference on Machine Learn-
ing (ICML), 2020.
https://arxiv.org/abs/1912.
05671.
Yoav Freund and Robert E Schapire.
A decision-theoretic
generalization
of
on-line
learning
and
an
application
to boosting.
Journal of Computer and System Sci-
ences, 1997.
https://www.sciencedirect.com/
science/article/pii/S002200009791504X.
Jerome Friedman, Trevor Hastie, Robert Tibshirani, et al. The
elements of statistical learning. Springer series in statistics New
York, 2001.
Peng Gao, Shijie Geng, Renrui Zhang, Teli Ma, Rongyao Fang,
Yongfeng Zhang, Hongsheng Li, and Yu Qiao. Clip-adapter:
Better vision-language models with feature adapters. arXiv
preprint arXiv:2110.04544, 2021.
Timur Garipov, Pavel Izmailov, Dmitrii Podoprikhin, Dmitry
Vetrov, and Andrew Gordon Wilson. Loss surfaces, mode con-
nectivity, and fast ensembling of dnns. In Advances in Neural
Information Processing Systems (NeurIPS), 2018.
https:
//arxiv.org/abs/1802.10026.
Danilo Giampiccolo, Bernardo Magnini, Ido Dagan, and Bill
Dolan. The third pascal recognizing textual entailment chal-
lenge. In Proc. of the ACL-PASCAL workshop on textual entail-
ment and paraphrasing, 2007.
Ross Girshick, Jeff Donahue, Trevor Darrell, and Jitendra Malik.
Rich feature hierarchies for accurate object detection and se-
mantic segmentation. In Proceedings of the IEEE conference on
computer vision and pattern recognition, pages 580–587, 2014.
Raphael Gontijo-Lopes, Yann Dauphin, and Ekin Dogus Cubuk.
No one representation to rule them all: Overlapping features
of training methods. In International Conference on Learning
Representations, 2022. URL https://openreview.net/
forum?id=BK-4qbGgIE3.
Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q Weinberger.
On calibration of modern neural networks. In International
Conference on Machine Learning (ICML), 2017.
https:
//arxiv.org/abs/1706.04599.
Yunhui Guo, Honghui Shi, Abhishek Kumar, Kristen Grauman,
Tajana Rosing, and Rogerio Feris. Spottune: transfer learning
through adaptive ﬁne-tuning. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition, pages
4805–4814, 2019.
Dan Hendrycks, Steven Basart, Norman Mu, Saurav Kadavath,
Frank Wang, Evan Dorundo, Rahul Desai, Tyler Zhu, Samyak
Parajuli, Mike Guo, Dawn Song, Jacob Steinhardt, and Justin
Gilmer. The many faces of robustness: A critical analysis of
out-of-distribution generalization. International Conference on
Computer Vision (ICCV), 2021a. https://arxiv.org/
abs/2006.16241.
Dan Hendrycks, Kevin Zhao, Steven Basart, Jacob Steinhardt,
and Dawn Song. Natural adversarial examples. Conference
on Computer Vision and Pattern Recognition (CVPR), 2021b.
https://arxiv.org/abs/1907.07174.
Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Dark knowledge,
2014. https://www.ttic.edu/dl/dark14.pdf.
Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the
knowledge in a neural network. In Advances in Neural Informa-
tion Processing Systems (NeurIPS) Deep Learning Workshop,
2015. https://arxiv.org/abs/1503.02531.
Pavel Izmailov, Dmitrii Podoprikhin, Timur Garipov, Dmitry
Vetrov, and Andrew Gordon Wilson. Averaging weights leads
to wider optima and better generalization. In Conference on
Uncertainty in Artiﬁcial Intelligence (UAI), 2018. https:
//arxiv.org/abs/1803.05407.
Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu
Pham, Quoc V Le, Yunhsuan Sung, Zhen Li, and Tom Duerig.
Scaling up visual and vision-language representation learning
with noisy text supervision. In International Conference on
Machine Learning (ICML), 2021. https://arxiv.org/
abs/2102.05918.
Marcin Junczys-Dowmunt, Tomasz Dwojak, and Rico Sennrich.
The amu-uedin submission to the wmt16 news translation task:
Attention-based nmt models as feature functions in phrase-based
smt. arXiv preprint arXiv:1605.04809, 2016.

Model soups: averaging weights of multiple ﬁne-tuned models improves accuracy without increasing inference time
Jean Kaddour, Linqing Liu, Ricardo Silva, and Matt J Kusner.
Questions for ﬂat-minima optimization of modern neural net-
works. arXiv preprint arXiv:2202.00661, 2022.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic
optimization. In International Conference on Learning Rep-
resentations (ICLR), 2014. https://arxiv.org/abs/
1412.6980.
Pang Wei Koh, Shiori Sagawa, Henrik Marklund, Sang Michael
Xie, Marvin Zhang, Akshay Balsubramani, Weihua Hu, Michi-
hiro Yasunaga, Richard Lanas Phillips, Irena Gao, Tony Lee,
Etienne David, Ian Stavness, Wei Guo, Berton A. Earnshaw,
Imran S. Haque, Sara Beery, Jure Leskovec, Anshul Kundaje,
Emma Pierson, Sergey Levine, Chelsea Finn, and Percy Liang.
WILDS: A benchmark of in-the-wild distribution shifts. In
International Conference on Machine Learning (ICML), 2021.
https://arxiv.org/abs/2012.07421.
Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Joan
Puigcerver, Jessica Yung, Sylvain Gelly, and Neil Houlsby. Big
transfer (bit): General visual representation learning. In Euro-
pean Conference on Computer Vision (ECCV), 2020. https:
//arxiv.org/abs/1912.11370.
Simon Kornblith, Jonathon Shlens, and Quoc V Le. Do better
imagenet models transfer better? In Conference on Computer
Vision and Pattern Recognition (CVPR), 2019. https://
arxiv.org/abs/1805.08974.
Jonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei. 3d
object representations for ﬁne-grained categorization. In Inter-
national Conference on Computer Vision (ICCV) Workshops,
2013. https://ieeexplore.ieee.org/document/
6755945.
Alex
Krizhevsky,
Geoffrey
Hinton,
et
al.
Learn-
ing
multiple
layers
of
features
from
tiny
images,
2009.
https://www.cs.toronto.edu/˜kriz/
learning-features-2009-TR.pdf.
Ananya Kumar, Aditi Raghunathan, Robbie Matthew Jones,
Tengyu Ma, and Percy Liang. Fine-tuning can distort pretrained
features and underperform out-of-distribution. In International
Conference on Learning Representations, 2022. URL https:
//openreview.net/forum?id=UYneFzXSJWh.
Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blun-
dell. Simple and scalable predictive uncertainty estimation
using deep ensembles. In Advances in Neural Information Pro-
cessing Systems (NeurIPS), 2017. https://arxiv.org/
abs/1612.01474.
Yann LeCun. The mnist database of handwritten digits. http://yann.
lecun. com/exdb/mnist/, 1998.
Julien-Charles L´evesque, Christian Gagn´e, and Robert Sabourin.
Bayesian hyperparameter optimization for ensemble learning.
arXiv preprint arXiv:1605.06394, 2016.
Xingjian Li, Haoyi Xiong, Haozhe An, Cheng-Zhong Xu, and
Dejing Dou. Riﬂe: Backpropagation in depth for deep transfer
learning through re-initializing the fully-connected layer. In
International Conference on Machine Learning, pages 6010–
6019. PMLR, 2020.
Ilya Loshchilov and Frank Hutter. Sgdr: Stochastic gradient de-
scent with warm restarts. In International Conference on Learn-
ing Representations (ICLR), 2016. https://arxiv.org/
abs/1608.03983.
Ilya Loshchilov and Frank Hutter. Decoupled weight decay regu-
larization. In International Conference on Learning Repre-
sentations (ICLR), 2019.
https://openreview.net/
forum?id=Bkg6RiCqY7.
Edward
Ma.
Nlp
augmentation.
https://github.com/makcedward/nlpaug, 2019.
Dhruv Mahajan, Ross Girshick, Vignesh Ramanathan, Kaiming
He, Manohar Paluri, Yixuan Li, Ashwin Bharambe, and Laurens
Van Der Maaten. Exploring the limits of weakly supervised pre-
training. In European Conference on Computer Vision (ECCV),
2018. https://arxiv.org/abs/1805.00932.
Michael Matena and Colin Raffel. Merging models with ﬁsher-
weighted averaging, 2021.
https://arxiv.org/abs/
2111.09832.
Brian W Matthews. Comparison of the predicted and observed
secondary structure of t4 phage lysozyme. Biochimica et Bio-
physica Acta (BBA)-Protein Structure, 1975.
Hector Mendoza, Aaron Klein, Matthias Feurer, Jost Tobias Sprin-
genberg, and Frank Hutter. Towards automatically-tuned neural
networks. In Workshop on Automatic Machine Learning, pages
58–65. PMLR, 2016.
Norman Mu, Alexander Kirillov, David Wagner, and Saining Xie.
Slip: Self-supervision meets language-image pre-training. arXiv
preprint arXiv:2112.12750, 2021.
Basil Mustafa, Carlos Riquelme, Joan Puigcerver, Andr´e Susano
Pinto, Daniel Keysers, and Neil Houlsby. Deep ensembles for
low-data transfer learning, 2020. https://arxiv.org/
abs/2010.06866.
Vaishnavh Nagarajan and J. Zico Kolter.
Uniform conver-
gence may be unable to explain generalization in deep
learning.
In H. Wallach, H. Larochelle, A. Beygelz-
imer, F. d'Alch´e-Buc, E. Fox, and R. Garnett, editors,
Advances in Neural Information Processing Systems, vol-
ume 32. Curran Associates, Inc., 2019.
URL https:
//proceedings.neurips.cc/paper/2019/file/
05e97c207235d63ceb1db43c60db7bbb-Paper.
pdf.
Behnam Neyshabur, Hanie Sedghi, and Chiyuan Zhang. What is
being transferred in transfer learning? In Advances in Neural
Information Processing Systems (NeurIPS), 2020.
https:
//arxiv.org/abs/2008.11687.
Yaniv Ovadia, Emily Fertig, Jie Ren, Zachary Nado, David Sculley,
Sebastian Nowozin, Joshua V Dillon, Balaji Lakshminarayanan,
and Jasper Snoek. Can you trust your model’s uncertainty?
evaluating predictive uncertainty under dataset shift. In Ad-
vances in Neural Information Processing Systems (NeurIPS),
2019. https://arxiv.org/abs/1906.02530.
Hieu Pham, Zihang Dai, Golnaz Ghiasi, Hanxiao Liu, Adams Wei
Yu, Minh-Thang Luong, Mingxing Tan, and Quoc V. Le. Com-
bined scaling for zero-shot transfer learning, 2021. https:
//arxiv.org/abs/2111.10050.

Model soups: averaging weights of multiple ﬁne-tuned models improves accuracy without increasing inference time
Boris Teodorovich Polyak. New method of stochastic approxima-
tion type. Automation and remote control, 1990.
Oﬁr Press and Lior Wolf. Using the output embedding to improve
language models. In Proceedings of the 15th Conference of
the European Chapter of the Association for Computational
Linguistics: Volume 2, Short Papers, pages 157–163, Valencia,
Spain, April 2017. Association for Computational Linguistics.
URL https://aclanthology.org/E17-2025.
Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh,
Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell,
Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya
Sutskever.
Learning transferable visual models from natu-
ral language supervision.
In International Conference on
Machine Learning (ICML), 2021. https://arxiv.org/
abs/2103.00020.
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sha-
ran Narang, Michael Matena, Yanqi Zhou, Wei Li, and Pe-
ter J. Liu.
Exploring the limits of transfer learning with a
uniﬁed text-to-text transformer. Journal of Machine Learn-
ing Research, 2020a. http://jmlr.org/papers/v21/
20-074.html.
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sha-
ran Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J.
Liu. Exploring the limits of transfer learning with a uniﬁed text-
to-text transformer. Journal of Machine Learning Research,
21(140):1–67, 2020b. URL http://jmlr.org/papers/
v21/20-074.html.
Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal
Shankar. Do ImageNet classiﬁers generalize to ImageNet? In
International Conference on Machine Learning (ICML), 2019.
https://arxiv.org/abs/1902.10811.
Rebecca Roelofs, Nicholas Cain, Jonathon Shlens, and Michael C
Mozer. Mitigating bias in calibration error estimation, 2020.
https://arxiv.org/abs/2012.08668.
David
Ruppert.
Efﬁcient
estimations
from
a
slowly
convergent
robbins-monro
process,
1988.
https:
//ecommons.cornell.edu/bitstream/handle/
1813/8664/TR000781.pdf.
Tonmoy Saikia, Thomas Brox, and Cordelia Schmid. Optimized
generic feature learning for few-shot classiﬁcation across do-
mains. arXiv preprint arXiv:2001.07926, 2020.
Vaishaal Shankar, Rebecca Roelofs, Horia Mania, Alex Fang,
Benjamin Recht, and Ludwig Schmidt. Evaluating machine
accuracy on imagenet. In International Conference on Ma-
chine Learning (ICML), 2020.
http://proceedings.
mlr.press/v119/shankar20c/shankar20c.pdf.
Ali Sharif Razavian, Hossein Azizpour, Josephine Sullivan, and
Stefan Carlsson. Cnn features off-the-shelf: an astounding
baseline for recognition. In Proceedings of the IEEE conference
on computer vision and pattern recognition workshops, 2014.
https://arxiv.org/abs/1403.6382.
Noam Shazeer and Mitchell Stern. Adafactor: Adaptive learning
rates with sublinear memory cost. In International Conference
on Machine Learning, pages 4596–4604. PMLR, 2018.
Yang Shu, Zhi Kou, Zhangjie Cao, Jianmin Wang, and Mingsheng
Long. Zoo-tuning: Adaptive transfer from a zoo of models. In
International Conference on Machine Learning, pages 9626–
9637. PMLR, 2021.
Jasper Snoek, Oren Rippel, Kevin Swersky, Ryan Kiros, Nadathur
Satish, Narayanan Sundaram, Mostofa Patwary, Mr Prabhat, and
Ryan Adams. Scalable bayesian optimization using deep neural
networks. In International conference on machine learning,
pages 2171–2180. PMLR, 2015.
Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christo-
pher D Manning, Andrew Ng, and Christopher Potts. Recursive
deep models for semantic compositionality over a sentiment
treebank. In Proceedings of EMNLP, 2013.
Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens,
and Zbigniew Wojna. Rethinking the inception architecture
for computer vision. In Proceedings of the IEEE conference
on computer vision and pattern recognition, pages 2818–2826,
2016.
Tijmen Tieleman and Geoffrey Hinton. Lecture 6.5-rmsprop: Di-
vide the gradient by a running average of its recent magnitude.
COURSERA: Neural networks for machine learning, 4(2):26–
31, 2012.
Hugo Touvron, Andrea Vedaldi, Matthijs Douze, and Herve Jegou.
Fixing the train-test resolution discrepancy. In Advances in Neu-
ral Information Processing Systems (NeurIPS), 2019. https:
//proceedings.neurips.cc/paper/2019/file/
d03a857a23b5285736c4d55e0bb067c8-Paper.
pdf.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit,
Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polo-
sukhin. Attention is all you need. Advances in neural informa-
tion processing systems, 30, 2017.
Johannes Von Oswald, Seijin Kobayashi, Joao Sacramento, Alexan-
der Meulemans, Christian Henning, and Benjamin F Grewe.
Neural networks with late-phase weights.
arXiv preprint
arXiv:2007.12927, 2020.
Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer
Levy, and Samuel R Bowman. Glue: A multi-task benchmark
and analysis platform for natural language understanding. arXiv
preprint arXiv:1804.07461, 2018.
Haohan Wang, Songwei Ge, Zachary Lipton, and Eric P Xing.
Learning robust global representations by penalizing local pre-
dictive power. In Advances in Neural Information Process-
ing Systems (NeurIPS), 2019. https://arxiv.org/abs/
1905.13549.
Alex Warstadt, Amanpreet Singh, and Samuel R. Bowman. Neural
network acceptability judgments. TACL, 7:625–641, 2019.
Jason Wei and Kai Zou. Eda: Easy data augmentation techniques
for boosting performance on text classiﬁcation tasks. arXiv
preprint arXiv:1901.11196, 2019.
Florian Wenzel, Jasper Snoek, Dustin Tran, and Rodolphe Jenat-
ton. Hyperparameter ensembles for robustness and uncertainty
quantiﬁcation. arXiv preprint arXiv:2006.13570, 2020.
Ross Wightman. Pytorch image models. https://github.
com/rwightman/pytorch-image-models, 2019.

Model soups: averaging weights of multiple ﬁne-tuned models improves accuracy without increasing inference time
Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond,
Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault,
R´emi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer,
Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Can-
wen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame,
Quentin Lhoest, and Alexander M. Rush. Transformers: State-
of-the-art natural language processing.
In Proceedings of
the 2020 Conference on Empirical Methods in Natural Lan-
guage Processing: System Demonstrations, pages 38–45, On-
line, October 2020. Association for Computational Linguis-
tics.
URL https://www.aclweb.org/anthology/
2020.emnlp-demos.6.
Mitchell Wortsman, Gabriel Ilharco, Jong Wook Kim, Mike Li,
Simon Kornblith, Rebecca Roelofs, Raphael Gontijo-Lopes,
Hannaneh Hajishirzi, Ali Farhadi, Hongseok Namkoong, and
Ludwig Schmidt. Robust ﬁne-tuning of zero-shot models. 2021.
https://arxiv.org/abs/2109.01903.
Jianxiong Xiao, Krista A Ehinger, James Hays, Antonio Torralba,
and Aude Oliva. Sun database: Exploring a large collection
of scene categories. International Journal of Computer Vi-
sion, 2016. https://link.springer.com/article/
10.1007/s11263-014-0748-y.
LI Xuhong, Yves Grandvalet, and Franck Davoine. Explicit in-
ductive bias for transfer learning with convolutional networks.
In International Conference on Machine Learning, pages 2825–
2834. PMLR, 2018.
I Zeki Yalniz, Herv´e J´egou, Kan Chen, Manohar Paluri, and Dhruv
Mahajan. Billion-scale semi-supervised learning for image
classiﬁcation, 2019.
https://arxiv.org/abs/1905.
00546.
Jason Yosinski, Jeff Clune, Yoshua Bengio, and Hod Lipson. How
transferable are features in deep neural networks? In Advances
in Neural Information Processing Systems (NeurIPS), 2014.
https://arxiv.org/abs/1411.1792.
Jiahui Yu, Zirui Wang, Vijay Vasudevan, Legg Yeung, Mojtaba
Seyedhosseini, and Yonghui Wu.
Coca: Contrastive cap-
tioners are image-text foundation models.
arXiv preprint
arXiv:2205.01917, 2022.
Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk Chun,
Junsuk Choe, and Youngjoon Yoo. Cutmix: Regularization
strategy to train strong classiﬁers with localizable features. In
Proceedings of the IEEE/CVF international conference on com-
puter vision, pages 6023–6032, 2019.
Xiaohua Zhai, Alexander Kolesnikov, Neil Houlsby, and Lucas
Beyer. Scaling vision transformers, 2021. https://arxiv.
org/abs/2106.04560.
Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David
Lopez-Paz. mixup: Beyond empirical risk minimization. 2017.
https://arxiv.org/abs/1710.09412.
Michael R Zhang, James Lucas, Geoffrey Hinton, and Jimmy
Ba. Lookahead optimizer: k steps forward, 1 step back. In
Advances in Neural Information Processing Systems (NeurIPS),
2019. https://arxiv.org/abs/1907.08610.
Renrui Zhang, Rongyao Fang, Peng Gao, Wei Zhang, Kunchang Li,
Jifeng Dai, Yu Qiao, and Hongsheng Li. Tip-adapter: Training-
free clip-adapter for better vision-language modeling. arXiv
preprint arXiv:2111.03930, 2021.
Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei
Liu. Learning to prompt for vision-language models, 2021.
https://arxiv.org/abs/2109.01134.

Model soups: averaging weights of multiple ﬁne-tuned models improves accuracy without increasing inference time
A. Overview
The appendix is organizes via the following contributions:
• Appendix B (Additional ﬁgures) supplements the main text with additional ﬁgures.
• Appendix C (BASIC) presents additional experiments exploring model soups for BASIC (Pham et al., 2021).
• Appendix D (Robust ﬁne-tuning) compares model soups with WiSE-FT (Wortsman et al., 2021), a technique for
ﬁne-tuning while preserving robustness.
• Appendix E (Cross-dataset soups) explores soups for models which are trained on different datasets to improve
zero-shot transfer.
• Appendix F (Analysis of 1D hyperparameter grids) compares the performance of averaging endpoints with intermediate
solutions for hyperparemters on a one dimensional grid.
• Appendix G (Additional ﬁne-tuning and pre-training datasets) explores model soups for additional datasets.
• Appendix H (Additional grid searches and initializations) supplements the results in the main text with other hyperpa-
rameter sweeps and model initializations (i.e., zero-shot instead of LP initialization).
• Appendix I (Learned soup) describes the more advanced souping procedure where we learn the soup mixing coefﬁcients
with gradient based optimization on the held-out validation set.
• Appendix J (Experimental details) provides additional details for the experiments.
• Appendix K (Analytical comparison details) supplements Section 4 in analytically comparing soups and ensembles.
• Appendix L (Additional baselines) compares soups with additional baselines including stochastic weight averaging (Iz-
mailov et al., 2018) and sharpenss aware minimization (Foret et al., 2021).
B. Additional ﬁgures
0
10
20
30
40
50
60
70
Number of models
77.0
77.5
78.0
78.5
79.0
79.5
80.0
80.5
81.0
81.5
ImageNet (top-1, %)
Greedy soup
Uniform soup
Best individual model on ImageNet held-out val set
Ensemble (more compute)
Avg. individual model
0
10
20
30
40
50
60
70
Number of models
45
46
47
48
49
50
51
52
Avg. accuracy on 5 distribution shifts
Figure B.1: For essentially any number of models, the greedy soup outperforms the best single model on both ImageNet and the
out-of-distribution test sets. On the x-axis we show the number of models considered in a random search over hyperparameters while the
y-axis displays the accuracy of various methods for model selection which are summarized in Table 2. All methods require the same
amount of training and compute cost during inference with the exception of the ensembles, which require a separate pass through each
model. Results are for ﬁne-tuning CLIP ViT-B/32, averaged over three random orders (shown with faded lines).

Model soups: averaging weights of multiple ﬁne-tuned models improves accuracy without increasing inference time
10
20
Number of models
0.77
0.78
ImageNet (top-1, %)
10
20
Number of models
0.44
0.46
Distr. shifts (top-1, %)
10
20
Number of models
0.05
0.10
0.15
ImageNet (ECE)
10
20
Number of models
0.2
0.3
Distr. shifts (ECE)
Soup
Ensemble
Calibrated Soup
Calibrated Ensemble
Figure B.2: Like model ensembling, model soups improve accuracy, but unlike model ensembling, model soups do not improve calibration.
Expected calibration error (ECE) is computed using equal-mass binning. The soup in this ﬁgure is the uniform soup, and all models in
this experiment are ﬁne-tuned CLIP ViT-B/32 models with the same hyperparameters but different random seeds. The calibrated soup and
calibrated ensemble refer to a soup and ensemble composed of models which are calibrated through temperature scaling (Guo et al., 2017).
Calibrating models before ensembling or souping had no effect on accuracy and so these curves are omitted from the plots on the left.
C. BASIC
After our initial submission we tested model soups when ﬁne-tuning BASIC-L (Pham et al., 2021). Due to memory
constraints, we ﬁne-tune with a batch size of 64 instead of 512. We initialize with the zero-shot classiﬁcation head and train
for 8 epochs using the Adafactor optimizer (Shazeer and Stern, 2018) at a resolution of 500 × 500. We sweep over a grid of
learning rates (1·10−5 or 2·10−5) and 10 data augmentation settings, resulting in 20 different models. We use random crops
and ﬂips with a minimum crop size of 90% of the image together with mixup (Zhang et al., 2017) or CutMix (Yun et al., 2019)
with α ∈{0.2, 0.4}, AutoAugment with (num_layers, magnitude) ∈{(2, 10), (2, 15), (2, 20), (2, 25), (3, 10)}.
We additionally train models with random crops and ﬂips with minimum crop sizes of 5% and 90% without additional
augmentation.
As in our ViT-G/14 experiments (Section 3.3.2), we save exponential moving averages with low and high EMA decay
factors, and ﬁnd that low EMA weights provide better performance for greedy souping and greedy ensembling whereas
high EMA weights provide better single-model performance. We adjust the EMA factors for the difference in batch size
and thus use a decay factor of 0.9991/8 for our low EMA conﬁguration and 0.99999991/8 for our high EMA conﬁguration.
During each training run, for each set of EMA weights, we evaluate accuracy on our held out validation set every 5000 steps
and use the best checkpoint for ensembling, souping, and subsequent evaluation. We resize the full image to 500 × 500 for
evaluation.
Results are shown in Table C.1. The greedy soup consistently outperforms the individual model with highest accuracy on
the held-out validation set. The best BASIC-L model on each individual test set sometimes outperforms the greedy soup, but
selecting the model on the test set will generally overestimate its true accuracy.
Table C.1: Greedy soup improves over the best individual model on the held-out validation set when ﬁne-tuning BASIC-L (Pham et al.,
2021). Among the best model on the held out val set, the greedy ensemble, and the greedy soup, numbers not signiﬁcantly different from
the best are bold-faced. Statistical comparisons are performed using an exact McNemar test or permutation test at α = 0.05. Avg shift
accuracy of the best model on each test set is the best average accuracy of any individual model. For CoCa (Yu et al., 2022), a model
which was introduced after our initial submission, evaluations were only available to one decimal place.
ImageNet
Distribution shifts
Method
Top-1
ReaL
Multilabel
IN-V2
IN-R
IN-Sketch
ObjectNet
IN-A
Avg shifts
ViT/G-14 (Zhai et al., 2021)
90.45
90.81
–
83.33
–
–
70.53
–
–
CoAtNet-7 (Dai et al., 2021)
90.88
–
–
–
–
–
–
–
–
BASIC-L (zero-shot) (Pham et al., 2021)
85.70
–
–
80.60
95.70
76.10
82.30
85.60
84.06
CoCa (zero-shot) (Yu et al., 2022)
86.30
–
–
80.70
96.50
77.60
82.70
90.20
85.54
CoCa (ﬁne-tuned) (Yu et al., 2022)
91.00
–
–
–
–
–
–
–
–
ViT-G/14 greedy soup (Table 4)
90.94
91.20
97.17
84.22
95.46
74.23
78.52
92.67
85.02
Our models/evaluations with ﬁne-tuned BASIC-L:
Best model on held out val set
90.83
90.84
98.16
84.42 95.50
76.98
78.09
93.13
85.63
Greedy ensemble
91.02
91.11
98.46
84.65 95.79
76.63
79.91
94.05
86.20
Greedy soup
90.98 91.03
98.37
84.63
96.10
77.18
79.94
94.17
86.40
Best model on each test set (oracle)
90.87
91.24
98.41
84.84
95.89
77.30
80.94
94.47
86.54

Model soups: averaging weights of multiple ﬁne-tuned models improves accuracy without increasing inference time
75
76
77
78
79
80
81
ImageNet Accuracy (top-1, %)
35
40
45
50
55
Avg. accuracy on 5 distribution shifts
60
65
70
75
80
ImageNet Accuracy (top-1, %)
44
46
48
50
52
54
Avg. accuracy on 5 distribution shifts
Greedy Soup
Uniform soup
Initialization
Individual models with
various hyperparameters
Interpolate with initialization
(WiSE-FT)
Minimal data aug
Various stronger aug
(with and without mixup)
Minimal data aug with mixup
Figure D.1: Model soups compared to baselines for robust ﬁne-tuning. WiSE-FT (Wortsman et al., 2021) improves the robustness of
model θ1 ﬁne-tuned from initialization θ0 by interpolating between θ1 and θ0. Above we display the accuracy of models along these
interpolation curves both for regular ﬁne-tuned models and model soups (left: random hyperparameter search using the LP initialization,
right: grid search using the zero-shot initialization). The model soups lie beyond the WiSE-FT curves generated by any individual model,
and accuracy can be improved on the distribution shifts by applying WiSE-FT to the model soups.
D. Robust ﬁne-tuning
Wortsman et al. (2021) introduce WiSE-FT, a method for improving the robustness of a model θ1 which is ﬁne-tuned from
initialization θ0 by linearly interpolating θ1 and θ0. An intriguing observation was that, once the data augmentation is ﬁxed,
interpolating between θ1 and θ0 often traces a similar curve regardless of hyperparameters.4 In other words, a reasonable
hypothesis was that this curve is Pareto optimal—no hyperparameter conﬁguration would surpass it. In Figure D.1, we trace
the curves when interpolating between θ1 and θ0 for a random hyperparameter search (left) and the standard grid search
described in Appendix J.2.1 (right) when ﬁne-tuning CLIP ViT-B/32. We ﬁnd that the uniform soup and greedy soup lie
beyond these interpolation curves. Moreover, we ﬁnd interpolating between these soups and the initialization also provides
additional accuracy improvements on the distribution shifts.
E. Cross-dataset soups
So far, our experiments have studied soups of models ﬁne-tuned on the same dataset with different hyperparameters. In this
section, we prepare soups containing models ﬁne-tuned on different datasets. We evaluate the resulting soups on a held-out
dataset, from which no labeled training data is used (i.e., zero-shot evaluation).
Concretely, we consider soups based on the CLIP zero-shot initialization along with six models ﬁne-tuned independently on
CIFAR-10 (Krizhevsky et al., 2009), Describable Textures (Cimpoi et al., 2014), Food-101 (Bossard et al., 2014), SUN397
(Xiao et al., 2016), Stanford Cars (Krause et al., 2013) and ImageNet (Deng et al., 2009). We evaluate on CIFAR-100
(Krizhevsky et al., 2009), which does not share classes with CIFAR-10. Since each task has a different set of classes, the last
layers cannot be part of the soup. Hence, during ﬁne-tuning, we freeze the linear head produced by CLIP’s text tower so that
task-speciﬁc learning is captured only in the backbone weights. At test time, we use the “backbone soup” with a zero-shot
head constructed from CLIP’s text tower and the CIFAR-100 class names with the prompt-ensembling used for ImageNet
by Radford et al. (2021). Figure E.1 (left) shows that a model soup containing models trained on each of these datasets
and the zero-shot model improves zero-shot performance on CIFAR-100 by 6.4 percentage points over the CLIP baseline.
Moreover, Figure E.1 (right) shows that the choice of which ﬁne-tuned models to include can have a substantial impact on
the accuracy of the resulting soup. See Appendix J.3 for additional details.
F. Analysis of 1D hyperparameter grids
This section asks: for a one dimensional grid of hyperparameters {ha, ..., hb}, how does averaging the models ﬁne-tuned
with hyperparameter conﬁgurations ha and hb corresponding to the endpoints compare with picking the best individual
model ﬁne-tuned with hyperparameter conﬁguration h ∈{ha, ..., hb}?
The results are illustrated in Figure F.1, where each square represents a grid {ha, ..., hb}. The average of the endpoints often
4This is visible in Figure D.1 (right) where different data augmentations are shown with different colors. On the other hand, in
Figure D.1 (left) there are many different methods of data augmentation as we conduct a random hyperparameter search.

Model soups: averaging weights of multiple ﬁne-tuned models improves accuracy without increasing inference time
1
2
3
4
5
6
7
Number of ingredients (models)
−4
−2
0
2
4
6
8
10
Percentage point ∆from CLIP zero-shot
Cross-dataset soup
Zero-shot CLIP
0
2
4
6
8
Average percentage point ∆when including soup ingredient
CIFAR10
ImageNet
SUN397
Food101
Cars
DTD
Figure E.1: Model soups can improve zero-shot performance on new downstream tasks. (left) Starting with zero-shot CLIP we create a
soup by adding models ﬁne-tuned on ImageNet, CIFAR-10, Food101, SUN397, DTD, and Cars, and evaluate on CIFAR-100. Different
orders for adding models are shown with faded lines. (right) The average change in CIFAR-100 accuracy when a model ﬁne-tuned on the
dataset listed in the y-axis is added to the model soup.
SGD
RMSprop
Adam
AdamW
SGD
RMSprop
Adam
AdamW
0
1.3
1.2
1.4
0
1.6
1.7
0
1.7
0
Choice of optimizer
0
1
2
3
4
5
0
1
2
3
4
5
0
1.6
1.8
1.8
1.8
1.7
0
2.8
2
2.6
2
0
1.2
1.1
1.2
0
1.4
0.8
0
1.3
0
Choice of augmentation strength
1e-4
3e-5
2e-5
1e-5
3e-6
1e-6
1e-7
1e-4
3e-5
2e-5
1e-5
3e-6
1e-6
1e-7
0
-1.1
-2
-2.8
-3.2
-3.2
-3.3
0
0.96
0.86
1.3
1.5
1.5
0
0.64
1.3
1.6
1.6
0
0.63
1.1
1.2
0
0.22
0.15
0
-0.56
0
Choice of learning rate
0.0
0.2
0.4
0.6
0.8
1.0
1.2
1.4
1.6
0.0
0.5
1.0
1.5
2.0
2.5
−3
−2
−1
0
1
Acc
 1
2θa + 1
2θb

−maxθ∈{θa,...,θb} Acc (θ)
Figure F.1: Analysis of 1D hyperparameter grids, where the average of models at the endpoints often outperforms the best individual
model in the grid. In particular, colors and numbers indicate the percentage point improvement obtained by averaging the models on the x
and y axis versus taking the best individual model in the range between them. Results are shown for the CLIP ViT-B/32 model ﬁne-tuned
on ImagNet.
outperforms the best individual model in the grid. A notable exception is when the learning rate 10−4 is the left endpoint of
the grid. As this experiment uses AdamW, this learning rate is too high for ﬁne-tuning and, unlike the examples in Figure 2,
there is a high error barrier between the two ﬁne-tuned solutions (see Figure J.1, lower right for example).
When varying optimizer we use minimal data augmentation and LR 3 · 10−5 for RMSProp (Tieleman and Hinton, 2012),
Adam (Kingma and Ba, 2014), and AdamW (Loshchilov and Hutter, 2019). SGD requires a larger learning rate, and so we
use 0.1. When varying augmentation strength, we use minimal data augmentation and LR 3 · 10−5.
G. Additional ﬁne-tuning and pre-training datasets
In this section we explore ﬁne-tuning or pre-training on additional datasets. First, Figure G.1 displays results when ﬁne-
tuning a CLIP ViT-L model on two datasets included in the WILDS (Koh et al., 2021) challenge, FMoW (Christie et al.,
2018) and iWildCam (Beery et al., 2021).
Next, Figure G.2 displays results for ﬁne-tuning a CLIP ViT-L model on CIFAR-10 (Krizhevsky et al., 2009). The y-axis of
Figure G.2 displays accuracy on CIFAR-10.1 (Recht et al., 2019), a reproduction of CIFAR-10 with a distribution shift. The
individual models are ﬁne-tuned with the random hyperparameter search described in Section J.2.1.
In addition, Figure G.3 shows results when ﬁne-tuning a ViT-B/32 (Dosovitskiy et al., 2021) model pre-trained on ImageNet-
22k (Deng et al., 2009) and ﬁne-tuned on ImageNet. This differs from many of our other experiments as the dataset used for

Model soups: averaging weights of multiple ﬁne-tuned models improves accuracy without increasing inference time
60
65
70
75
In-distribution test accuracy
35
40
45
50
OOD Worst Region Accuracy
WILDS-FMoW
45
50
55
ID test macro F1
35
40
45
OOD test macro F1
WILDS-iWildCam
Greedy soup
Uniform soup
Various hyperparameters
Figure G.1: Model soups improve accuracy when ﬁne-tuning on the diverse classiﬁcation tasks WILDS-FMoW (Koh et al., 2021; Christie
et al., 2018) and WILDS-iWildCam (Koh et al., 2021; Beery et al., 2021). Results are shown for the CLIP ViT-L/14 model and a random
hyperparameter search over learning rate, weight-decay, iterations, data augmentation, mixup, and label smoothing.
pre-training is smaller and less diverse. While the greedy soup offers an improvement, the improvement is less substantial
than Figure 1 which uses the same model and hyperparameter search but a different pre-training dataset.
Finally, we ﬁne-tune a ViT-B/32 model ﬁve times on ImageNet, using the best hyperparameters found by the hyperparameter
sweep, varying only the random seed. This experiment is conducted both for a model pre-trained on ImageNet-22k (Deng
et al., 2009) and a pre-trained CLIP model. The results are shown in Figure G.4, comparing, for an experimental budget of
1 ≤k ≤5 models: (i) the individual model with random seed k, (ii) the model soup composed of models with random seeds
1 through k, and (iii) the ensemble composed of models with random seeds 1 through k. The performance of the model soup
appears correlated with the performance of the ensemble. Moreover, we ﬁnd that CLIP models are more amenable to both
ensembling and souping than models pre-trained on ImageNet-22k.

Model soups: averaging weights of multiple ﬁne-tuned models improves accuracy without increasing inference time
98.0
98.5
99.0
99.5
CIFAR-10 Accuracy (top-1, %)
95.0
96.0
97.0
98.0
99.0
CIFAR-10.1 Accuracy (top-1, %)
Uniform soup
Greedy soup
Various hyperparameters
Figure G.2:
Fine-tuning a CLIP ViT-L model on CIFAR-
10 (Krizhevsky et al., 2009) with the random hyperparameter
search described in Section J.2.1. The y-axis displays accuracy
on CIFAR-10.1 (Recht et al., 2019), a reproduction of CIFAR-10
with a distribution shift.
77
78
79
80
ImageNet Accuracy (top-1, %)
38
40
42
44
46
Avg. accuracy on 5 distr. shifts
Uniform soup
Greedy soup
Various hyperparameters
Figure G.3: Fine-tuning on ImageNet, using a ViT-B/32 (Doso-
vitskiy et al., 2021) pre-trained on ImageNet-22k (Deng et al.,
2009).
1
2
3
4
5
Random seed / Number of models
79.5
80.0
80.5
ImageNet (top-1, %)
Individual models (IN-22k pre-training)
Individual models (CLIP pre-training)
Uniform soup (IN-22k pre-training)
Uniform soup (CLIP pre-training)
Ensemble (IN-22k pre-training)
Ensemble (CLIP pre-training)
1
2
3
4
5
Random seed / Number of models
43
44
45
46
47
48
49
Avg. accuracy on 5 distribution shifts
Figure G.4: For a CLIP and ImageNet-22k pre-trained ViT-B/32 model, we use the best hyperparameters found by the hyperparameter
sweep to ﬁne-tune multiple times, varying only the random seed. For an experimental budget of 1 ≤k ≤5 models, we show: (i) the
individual model with random seed k, (ii) the model soup composed of models with random seeds 1 through k, and (iii) the ensemble
composed of models with random seeds 1 through k.
H. Additional grid searches and initializations
This section recreates Figure B.1 with different initializations (linear probe initialization or zero-shot) and different grid
searches (standard and extreme grid) when ﬁne-tuning CLIP ViT-B/32. The standard and extreme grid searches are described
in Section J.2.1.
Figure H.1 considers the linear probe (LP) initialization and the standard grid. Figure H.2 considers the linear probe (LP)
initialization and the extreme grid. Figure H.3 considers the zero-shot initialization and the standard grid. Figure H.4
considers the zero-shot initialization and the extreme grid.

Model soups: averaging weights of multiple ﬁne-tuned models improves accuracy without increasing inference time
0
10
20
30
40
Number of models
78
79
80
81
ImageNet (top-1, %)
Greedy soup
Uniform soup
Best individual model on ImageNet held-out val set
Ensemble (more compute)
Avg. individual model
0
10
20
30
40
Number of models
44
46
48
50
Avg. accuracy on 5 distribution shifts
Figure H.1: Replicating Figure B.1 with the LP initialization and the standard grid hyperparameter search.
0
10
20
30
40
50
Number of models
66
68
70
72
74
76
78
80
ImageNet (top-1, %)
Greedy soup
Uniform soup
Best individual model on ImageNet held-out val set
Ensemble (more compute)
Avg. individual model
0
10
20
30
40
50
Number of models
35.0
37.5
40.0
42.5
45.0
47.5
50.0
52.5
Avg. accuracy on 5 distribution shifts
Figure H.2: Replicating Figure B.1 with the LP initialization and the extreme grid hyperparameter search.

Model soups: averaging weights of multiple ﬁne-tuned models improves accuracy without increasing inference time
0
10
20
30
40
Number of models
76
77
78
79
80
81
ImageNet (top-1, %)
Greedy soup
Uniform soup
Best individual model on ImageNet held-out val set
Ensemble (more compute)
Avg. individual model
0
10
20
30
40
Number of models
44
46
48
50
52
Avg. accuracy on 5 distribution shifts
Figure H.3: Replicating Figure B.1 with the zero-shot initialization and the standard grid hyperparameter search.
0
10
20
30
40
50
Number of models
60
65
70
75
80
ImageNet (top-1, %)
Greedy soup
Uniform soup
Best individual model on ImageNet held-out val set
Ensemble (more compute)
Avg. individual model
0
10
20
30
40
50
Number of models
30
35
40
45
50
55
Avg. accuracy on 5 distribution shifts
Figure H.4: Replicating Figure B.1 with the zero-shot initialization and the extreme grid hyperparameter search.

Model soups: averaging weights of multiple ﬁne-tuned models improves accuracy without increasing inference time
I. Learned soup
In addition to the greedy soup method described in the text, we also explore a more advanced souping procedure, which
removes the sequential constraint from the greedy soup and requires only a single pass through the held out validation set.
We refer to this method as the learned soup, as it involves learning the soup mixing coefﬁcients for each of the ingredients
on the held-out validation set. However, the learned soup has the downside of requiring all models to be simultaneously
loaded in memory. In practice we combine the models on CPU before moving the parameters to GPU for each batch. For
loss ℓand validation set {(xi, yi)}n
i=1, we ﬁnd mixing coefﬁcients α ∈Rk and temperature scaling parameter β via
arg min
α∈Rk,β∈R
n
X
j=1
ℓ
 
β · f
 
xj,
k
X
i=1
αiθi
!
, yj
!
.
(2)
In practice we ﬁnd better results when α is parameterized as the output of a softmax, so that each αi is positive and values
sum to one. We optimizer the aforementioned equation with gradient based mini-batch optimization for three epochs over
the held-out validation set with the AdamW otpimizer and constant learning rate 0.1.
As presented in Table 3, we also try a “by layer” variant of the learned soup. For this we learn a separate α for each layer of
the network. Finally, another way to get non-uniform mixing coefﬁcients is to sample with replacement in the greedy soup
procedure.
J. Experimental details
J.1. Error landscape visualizations
To supplement Figure 2, we provide an identical experiment but with a 10x bigger learning rate instead of 10x smaller.
Results are illustrated in Figure J.1 with linear instead of log scaling for the contour lines. Since the error difference is more
substantial, linear scaling was more clear. When ﬁne-tuning with a larger learning rate, error increases on the path between
the two ﬁne-tuned solutions. All error landscape visualizations use CLIP ViT-B/32 ﬁne-tuned on ImageNet for 10 epochs
with minimal data augmentation, as used by CLIP during pre-training. When computing angles between the two ﬁne-tuned
solutions, as in Figure 3, we use the repeated weights which constitute the majority of the network parameters. We ignore
gain terms which tend to skew positive if occurring before ReLU activations.
In Figure 3 we consider solutions ﬁne-tuned with learning rates less that 10−4. As in Figure J.1, if a learning rate that is
large is used accuracy will decrease on the path in weight space between the two models.
J.2. Model soups
This section describes the set of hyperparameters used for searches. For all ImageNet experiments, we withhold 2% of the
training set and use these examples as the held-out validation set for model selection in greedy and learned soup.
J.2.1. CLIP EXPERIMENTS
Unless otherwise mentioned, all experiments used the AdamW optimizer (Loshchilov and Hutter, 2019) with cosine
annealing learning rate schedule (Loshchilov and Hutter, 2016) for 10 epochs at batch size 512 at a resolution of 224×224.
When necessary we discretize augmentation strength into minimal, medium, and strong. Minimal augmentation uses only a
random crop consisting of 90%-100% of the total image area. Medium is the default augmentation used by the timm library
(Wightman, 2019). Strong refers to RandAugment (Cubuk et al., 2020) (N = 2, M = 15).
We now provide the low level details for the hyperparemter searches, which are standard grid, extreme grid, and random
search. The standard grid includes learning rates 3 · 10−5, 2 · 10−5, 1 · 10−5, 3 · 10−6, where 2 · 10−5, 1 · 10−5 typically
perform the best. Augmentation strengths are minimal, medium, or strong. Mixup is either off or on at α = 0.5. We consider
all combinations of the above, running each hyperparameter conﬁguration with two random seeds.
The extreme grid considers learning rates 3 · 10−4, 1 · 10−4, 3 · 10−5, 2 · 10−5, 1 · 10−5, 3 · 10−6, 1 · 10−6, 1 · 10−7, where
2 · 10−5, 1 · 10−5 typically perform the best. Augmentation strengths are minimal, medium, or strong. Mixup is either off or
on at α = 0.5. Moreover, we include the initialization in this search, which often outperforms some of the extreme learning
rates but is far from the most accurate model.

Model soups: averaging weights of multiple ﬁne-tuned models improves accuracy without increasing inference time
0
20
40
60
80
100
120
0
5
10
15
20
Fine-tuning
ImageNet train loss
0
20
40
60
80
100
120
0
5
10
15
20
ImageNet test error
0
20
40
60
80
100
120
0
5
10
15
20
Avg. error on 5 distribution shifts
0
200
400
600
800
1000
0
5
10
15
20
25
30
ImageNet train loss
Initialization
LR = 3 10
5 (seed 0)
LR = 3 10
5 (seed 1)
LR = 3 10
4 (seed 0)
0
200
400
600
800
1000
0
5
10
15
20
25
30
ImageNet test error
0
200
400
600
800
1000
0
5
10
15
20
25
30
Avg. error on 5 distribution shifts
0.000356
0.00166
0.00297
0.00428
0.00558
0.00689
0.00819
> 0.00819
0.215
0.3
0.385
0.47
0.555
0.64
0.725
> 0.725
0.462
0.52
0.577
0.635
0.692
0.75
0.807
> 0.807
0.000385
0.00299
0.0056
0.00821
0.0108
0.0134
0.016
> 0.016
0.226
0.323
0.42
0.516
0.613
0.71
0.806
> 0.806
0.475
0.541
0.606
0.671
0.736
0.802
0.867
> 0.867
Figure J.1: Replicating Figure 2 with a 10x larger learning rate instead of 10x smaller in the second row.
The random search chooses learning rate 10−λ1 where λ1 is selected uniformly at random from 4 to 6. Weight decay is
chosen randomly as 10−λ2 where λ2 is selected uniformly at random from 0.2 to 4. With probability 0.5, label smoothing
is set to 0 and otherwise it is selected uniformly at random between 0 and 0.25. Fine-tuning epochs are chosen randomly
between four and sixteen. Mixup is 0 with probability 0.5, and otherwise is chosen uniformly at random from 0 to 0.9. With
probability 1/3 we use minimal augmentation, otherwise we use randaug where M and N are chosen uniformly at random
between 0 and 20 and 0 and 2 respectively.
When ﬁne-tuning on WILDS-FMoW and WILDS-iWildCam for Figure G.1, we use the same random search as when we
ﬁne-tune CLIP on ImageNet. The only difference is that we are able to use a larger ViT-L/14 model as the datasets are
smaller. This also requires us to change the default batch size from 512 to 128.
J.2.2. ALIGN EXPERIMENTS
We ﬁne-tuned ALIGN EfﬁcientNet-L2 models using AdamW with weight decay of 0.1 at a resolution of 289 × 289 for
25 epochs, with the ﬁnal layer initialized from a linear probe without data augmentation. We ﬁne-tuned 5 models with
standard Inception-style random crops (consisting of 5% to 100% of the total image area with an aspect ratio between 0.75
and 1.33) and different learning rates (1 · 10−6, 2 · 10−6, 5 · 10−6, 1 · 10−5, and 2 · 10−5). We also ﬁne-tuned 7 additional
models at a learning rate of 5 · 10−6 with different data augmentation strategies. Speciﬁcally, we varied the random cropping
strategy (either Inception-style crops or less aggressive crops consisting of 90% to 100% of the total image area with an
aspect ratio between 0.95 and 1.05), the use of RandAugment (Cubuk et al., 2020) (off or N = 2, M = 15), and the use of
mixup (Zhang et al., 2017) (off or α = 0.5) and trained models with all combinations of these strategies. Our soups are
obtained by considering these 12 models as well as the linear probe initialization. We perform evaluation at 360 × 360
resolution using a square center crop from images. The accuracy we attain with greedy soup approaches that reported by Jia
et al. (2021), which evaluated at 600 × 600 resolution.
J.2.3. VIT-G/14 EXPERIMENTS
These models are initialized with a backbone that was pretrained on the JFT-3B dataset (Zhai et al., 2021) and linear
probes obtained at either the 224 × 224 resolution at which the ViT-G/14 was pretrained or at the 518 × 518 resolution

Model soups: averaging weights of multiple ﬁne-tuned models improves accuracy without increasing inference time
used for ﬁne-tuning. Models are ﬁne-tuned at a batch size of 512 for either 10,000 or 20,000 steps (approximately 4 or 8
epochs) using the Adafactor optimizer (Shazeer and Stern, 2018) with learning rates of 3 · 10−5 or 5 · 10−5; a constant or
cosine decay learning rate schedule; and softmax or binary cross-entropy loss. When ﬁne-tuning with binary cross-entropy
loss, we use a linear probe that is also trained with binary cross-entropy loss. We vary data augmentation, applying
RandAugment (Cubuk et al., 2020), mixup (Zhang et al., 2017), or CutMix (Yun et al., 2019) of varying strengths and
random cropping with a minimum crop size of 5%, 70%, 90%, or 100% of the full image. When applying SAM, we
consider models with perturbations either synchronized or unsynchronized across accelerators, including one model with
synchronized perturbations and a combination of CutMix and SAM. All models are ﬁne-tuned at 518 × 518 resolution and
evaluated by rescaling test images to 550 × 550 (without preserving the aspect ratio) and taking a 518 × 518 central crop.
We manually tuned hyperparameters with the goal of maximizing single-model accuracy. After settling on the use of
Adafactor as the optimizer, we included all subsequently trained models in the pool of models to be used for greedy soup.
The model that performs best on the holdout set is initialized with a 224 × 224 linear probe and ﬁne-tuned with a learning
rate of 3e-5 and a constant learning rate decay schedule, with softmax cross-entropy loss, a minimum crop size of 90%, and
CutMix with α = 0.2. The model that performs best on the ofﬁcial ImageNet validation set is initialized with a 518 × 518
linear probe and ﬁne-tuned at a learning rate of 3e-5 and a constant learning rate decay schedule, with softmax cross-entropy
loss, a minimum crop size of 90%, CutMix with α = 0.2, and SAM. The greedy soup contains models trained with a wide
range of different hyperparameter values including different learning rates, linear probes, loss functions, and every form of
data augmentation and minimum crop size investigated. Notably, although models trained with SAM with synchronized
perturbations are included in the greedy soup, the greedy soup process skips over the models trained with SAM with
unsynchronized perturbations because adding them produces a large drop in holdout accuracy.
J.3. Cross-dataset soups details
When ﬁne-tuning we initialize with CLIP ViT-B/32 and use learning rate 3 · 10−5 for 10 epochs with mini-batch size of 512.
We train with minimal augmentation.
J.4. Text classiﬁcation datasets
We study four text classiﬁcation datasets from the GLUE benchmark (Wang et al., 2018).
Microsoft Research Paraphrase Corpus
(MRPC; (Dolan and Brockett, 2005)) contains pairs of sentences, labeled as
either nearly semantically equivalent, or not. The dataset is evaluated using the average of F1 and accuracy. The training set
consists of 3.7 thousand samples and the validation set of 409 samples.
Recognizing Textual Entailment
(RTE; (Wang et al., 2018)) contains pair of sentences, and the task is to predict whether
the ﬁrst sentence (the premise) entails or contradicts the second sentence (the hypothesis). The data is originally from a
series of datasets (Dagan et al., 2005; Bar-Haim et al., 2006; Giampiccolo et al., 2007; Bentivogli et al., 2009). The dataset
is evaluated using classiﬁcation accuracy. The training set consists of 2.5 thousand samples and the validation set of 277
samples.
Corpus of Linguistic Acceptability
(CoLA; (Warstadt et al., 2019)) contains sentences labeled as either grammatical or
ungrammatical. Models are evaluated on Matthews correlation (MCC; (Matthews, 1975)), which ranges between −1 and 1.
The training set consists of 8.6 thousand samples and the validation set consists of 1043 samples.
Stanford Sentiment Treebank
(SST-2; (Socher et al., 2013)) contains sentences labelled as expressing positive or
negative sentiment, collected from movie reviews. The dataset is evaluated using classiﬁcation accuracy. The training set
consists of 67 thousand samples and the validation set consists of 873 samples.
J.5. Fine-tuning details for text classiﬁcation tasks
Each model is ﬁne-tuned 32 times on each dataset, performing a random hyperparameter search. The learning rate is chosen
uniformly in log space over [10−6, 10−3], the batch size is chosen uniformly from {8, 16, 32, 64} and the number of epochs
from {2, 3, 5}. Evaluation is conducted once at the end of training, without early stopping. We use a maximum sequence
length of 128 tokens and train with Adam (Kingma and Ba, 2014) using β1 = 0.9, β2 = 0.999 and ϵ = 10−8, gradient

Model soups: averaging weights of multiple ﬁne-tuned models improves accuracy without increasing inference time
Table J.1: Performance of model soups on four text classiﬁcation datasets from the GLUE benchmark (Wang et al., 2018).
Model
Method
MRPC
RTE
CoLA
SST-2
BERT-base (Devlin et al., 2019b)
Best individual model
88.3
61.0
59.1
92.5
Uniform soup
76.0
52.7
0.0
89.9
Greedy soup
88.3
61.7
59.1
93.0
BERT-large (Devlin et al., 2019b)
Best individual model
88.8
56.7
63.1
92.2
Uniform soup
15.8
52.7
1.90
50.8
Greedy soup
88.8
56.7
63.1
92.3
T5-small (Raffel et al., 2020b)
Best individual model
89.7
70.0
42.2
91.7
Uniform soup
82.7
61.7
10.4
91.1
Greedy soup
89.7
70.0
43.0
91.7
T5-base (Raffel et al., 2020b)
Best individual model
91.8
78.3
58.8
94.6
Uniform soup
86.4
71.8
12.3
94.6
Greedy soup
92.4
79.1
60.2
94.7
T5-large (Raffel et al., 2020b)
Best individual model
93.4
82.7
61.7
96.3
Uniform soup
74.8
50.2
0.00
96.0
Greedy soup
93.4
84.8
62.7
96.3
clipping of 1.0, no weight decay, and with the learning rate being decayed linearly to zero at the end of training. We use
pre-trained weights from the Huggingface Transformers library (Wolf et al., 2020). For BERT models, we use the uncased
version.
Fine-tuning occurs without any additional parameters to avoid distorting the features from the pre-trained mod-
els (Kumar et al., 2022).
For such, the classiﬁcation tasks are adapted to be suited to the pre-training ob-
jective of BERT and T5.
For T5, the tasks are cast as a sequence-to-sequence problem.
For instance, for
sentiment analyses, an example is to predict “A) positive” from “sentence:
The best movie I’ve
ever seen!
| options:
A) positive B) negative | label:”.
For BERT, the tasks are cast
as a masked language modeling problem.
For instance, for linguistic acceptability, an example is to predict
“A) acceptable” for the inputs “sentence:
model soups are grammatical.
| options:
A)
acceptable B) unacceptable | label:
[MASK] [MASK] [MASK]”. For evaluation, we select which
of the options is given the highest probability according to the model.
The full set of results is shown in Table J.1. On 10 out of the 20 combinations of models and datasets, the greedy soup shows
better performance than the best individual model from the hyperparameter search. Uniform soups show worse performance
than the best individual model on all experiments, which could be an artifact of the broad range of hyperparameters used in
the search. While the experiments varied only basic hyperparameters such as learning rate and batch size, we hypothesize
that a broader set of hyperparameter choices (e.g. data augmentation (Wei and Zou, 2019; Ma, 2019)) could lead to more
diverse models and better soups.
Finally, as a word of caution for practitioners, we remind readers that many recent language models have tied weights on
the output and embedding layers (Press and Wolf, 2017). For this reason, caution is needed when writing code to average
models in-place.
K. Analytical comparison details
K.1. Notation and preliminaries
We begin by restating and adding to the notation used in Section 4. For a model with parameter vector θ ∈Rd and input
vector x, we let f(x; θ) ∈RC denote the model’s logit output for C-way classiﬁcation. Throughout, we ﬁx two endpoint
models θ0 and θ1, and for an interpolation parameter α ∈[0, 1] deﬁne
θα := (1 −α)θ0 + αθ1, and f soup
α
(x) := f(x; θα)

Model soups: averaging weights of multiple ﬁne-tuned models improves accuracy without increasing inference time
to be the “soup” weight averaged model and its corresponding logits. We also write
f ens
α (x) := (1 −α)f(x; θ0) + αf(x; θ1)
for the logits of the ensemble model. We write
δ = θ1 −θ0
for the difference of the two endpoints.
For a logit vector f ∈RC and a ground-truth label y, denote the cross-entropy loss by
ℓ(f; y) = log

X
y′
exp{fy′ −fy}

.
For some distribution over x, y we write the expected β-calibrated log losses of the soup and ensemble as
Lsoup
α
= Ex,yℓ(βf(x; θα), y) and Lens
α
= Ex,yℓ(βf ens
α (x), y),
respectively.
We have the following expression for the derivatives of cross entropy w.r.t. logits. The gradient is
∇fℓ(f, y) = psftmx(f) −e(y),
where e(i) is the ith standard basis vector and psftmx(f) ∈RC has efi/ P
j efj in its ith entry. The Hessian is
∇2
fℓ(f, y) = diag (psftmx (f)) −[psftmx(f)][psftmx(f)]T ,
so that for any v ∈RC, we have
vT ∇2
fℓ(f, y) v = VarY ∼psftmx(f)[vY ].
Finally, we use δT ∇f(x; θ) to denote a vector in RC whose ith entry is δT ∇[f(x; θ)]i. Similarly, δT ∇2f(x; θ)δ denotes a
vector in RC whose ith entry is δT [∇2f(x; θ)]iδ, where gradients and Hessian are with respect to θ.
K.2. An exact expression for logit difference
We use the fundamental theorem of calculus and elemntary algebraic manipulation to obtain an exact integral form for the
difference between the soup and ensemble logits. To streamline notation we drop the dependence of the logits on the input x.
f ens
α
−f soup
α
= (1 −α) [f (θ0) −f (θα)] + α [f (θ1) −f (θα)]
= −(1 −α)
Z α
0
δT ∇f (θt) dt + α
Z 1
α
δT ∇f (θt) dt
= −(1 −α)
Z α
0

δT ∇f (θα) +
Z t
α
δT ∇f (θτ) δdτ

dt + α
Z 1
α

δT ∇f (θα) +
Z t
α
δT ∇f (θτ) δdτ

dt
= −(1 −α)
Z α
0
Z t
α
 δT ∇2f (θτ) δ

dτdt + α
Z 1
α
Z t
α
 δT ∇2f (θτ) δ

dτdt
= (1 −α)
Z α
0
Z α
t
 δT ∇2f (θτ) δ

dτdt + α
Z 1
α
Z t
α
 δT ∇2f (θτ) δ

dτdt
= (1 −α)
Z α
0
 δT ∇2f (θτ) δ

dτ
Z τ
0
dt + α
Z 1
α
 δT ∇2f (θτ) δ

dτ
Z 1
τ
dt
=
Z 1
0
 δT ∇2f (θτ) δ

wα (τ) dτ,
(3)

Model soups: averaging weights of multiple ﬁne-tuned models improves accuracy without increasing inference time
where
wα (τ) =
(
(1 −α) τ
τ ≤α
α (1 −τ)
otherwise = min {(1 −α) τ, α (1 −τ)} .
Note that
Z 1
0
wα(τ)dτ = α(1 −α)
2
.
K.3. Derivation of approximation
We continue to suppress the dependence on x in order to simplify notation. We begin with the following ﬁrst order
approximation of the pointwise log-loss difference between the ensemble and soup, which is also a lower bound due to
convexity.
ℓ(f ens
α ; y) −ℓ(f soup
α
; y) ≈[∇fℓ(f ens
α ; y)]T (f ens
α
−f soup
α
) + O
 ((f ens
α
−f soup
α
)2)

.
Now, we approximate the ensemble and soup logit difference using eq. 3 by assuming that δT ∇2f (θτ) δ ≈δT ∇2f (θα) δ
for all τ ∈[0, 1]; this holds when the logits are approximately quadratic along the line between the checkpoints. The
resulting approximation is
f ens
α
−f soup
α
≈cα δT ∇2f (θα) δ + O

max
τ∈[0,1] |∇3f(θτ)[δ⊗3]|

, where cα := (1 −α)α
2
.
Combining the two approximation above, we obtain
ℓ(f ens
α ; y) −ℓ(f soup
α
; y) ≈cα [∇fℓ(f ens
α ; y)]T δT ∇2f (θα) δ.
To relate this expression to the Hessian of the loss with respect to the parameters, we note that for any θ (by the chain rule)
δT ∇2
θℓ(f(θ); y)δ = [δT ∇f(θ)]T ∇2
fℓ(f(θ); y)[δT ∇f(θ)] + ∇fℓ(f(θ); y)]T δT ∇2f(θ)δ.
When setting θ = θα, we note that the second term on the RHS is (up to a constant) our approximation for the loss
difference). Recalling the expression for the cross-entropy Hessian, the ﬁrst term is
[δT ∇f(θ)]T ∇2
fℓ(f(θ); y)[δT ∇f(θ)] = VarY ∼psftmx(f(θ))

δT ∇f(θ)

.
As a ﬁnal approximation, we let
δT ∇f(θα) ≈f(θ1) −f(θ0) + O
 δT ∇2f(θ)δ

;
this holds when logits are too far from linear in θ.
Substituting back and making x explicit, we obtain
ℓ(f soup
α
(x); y) −ℓ(f ens
α (x); y) ≈−cα
d2
dα2 ℓ(f soup
α
(x); y) + cα VarY ∼psftmx(f soup
α
(x)) [f(x; θ1) −f(x; θ0)] ,
where we have used
δT ∇2
θℓ(f soup
α
(x); y)δ = d2
dα2 ℓ(f soup
α
(x); y).
Scaling all logits by β, the approximation becomes
ℓ(βf soup
α
(x); y) −ℓ(βf ens
α (x); y) ≈−cα
d2
dα2 ℓ(βf soup
α
(x); y) + cα β2VarY ∼psftmx(βf soup
α
(x)) [f(x; θ1) −f(x; θ0)] .
Averaging the result over x, we arrive at the approximation (1), which we repeat here for ease of reference:
Lsoup
α
−Lens
α
≈−cα
d2
dα2 Lsoup
α
+ cα β2ExVarY ∼psftmx(βf soup
α
(x)) [f(x; θ1) −f(x; θ0)] .

Model soups: averaging weights of multiple ﬁne-tuned models improves accuracy without increasing inference time
−0.5
−0.4
−0.3
−0.2
−0.1
0.0
0.1
0.2
Diﬀerence in loss
between ensembles and interpolation
−1.0
−0.8
−0.6
−0.4
−0.2
0.0
0.2
0.4
Predicted diﬀerence by approximation
All learning rates
−0.125
−0.100
−0.075
−0.050
−0.025
0.000
0.025
Diﬀerence in loss
between ensembles and interpolation
−0.15
−0.10
−0.05
0.00
0.05
Predicted diﬀerence by approximation
Max LR < 10−4
−0.020
−0.015
−0.010
−0.005
0.000
0.005
0.010
Diﬀerence in error
between ensembles and interpolation
−0.15
−0.10
−0.05
0.00
0.05
Predicted diﬀerence by approximation
Max LR < 10−4
Linear trend
ImageNet
Distribution shifts
0.0
0.2
0.4
0.6
0.8
1.0
1.2
Diﬀerence in loss
between ensembles and interpolation
−1.0
−0.5
0.0
0.5
1.0
1.5
Predicted diﬀerence by approximation
All learning rates
0.00
0.05
0.10
0.15
0.20
0.25
Diﬀerence in loss
between ensembles and interpolation
−0.10
−0.05
0.00
0.05
0.10
0.15
0.20
0.25
0.30
Predicted diﬀerence by approximation
Max LR < 10−4
−0.020
−0.015
−0.010
−0.005
0.000
0.005
0.010
Diﬀerence in error
between ensembles and interpolation
−0.10
−0.05
0.00
0.05
0.10
0.15
0.20
0.25
0.30
Predicted diﬀerence by approximation
Max LR < 10−4
Linear trend
ImageNet
Distribution shifts
Figure K.1: Validation of the analytical approximation (1) for the performance difference of a 2-model soup and ensemble. Each marker
on the scatter plots represent a different choice of endpoint models (θ0, θ1) and interpolation weight α. In every scatter plot, the vertical
axis shows the true performance difference between the soup and ensemble (in loss for the left and center panes, and error for the right
pane), where a positive value indicates the ensemble is better. The horizontal axis shows our approximation for the loss difference. The
top row shows results with inverse temperature β chosen to calibrate the soup, and the bottom row shows results for β ﬁxed to 1.
K.4. Detailed empirical evaluations
Evaluation setup.
We evaluated our bounds on checkpoints from the ViT-B/32 ﬁne-tuning experiments from the extreme
grid search described in Section J.2.1. We selected three learning rate values (10−6, 10−5 and 10−4), two levels augmentation
(none and RandAugment+MixUp), and considered two different random seeds (0 and 1). From these checkpoints (as well
as the initialization) we constructed the following (θ0, θ1) pairs:
• All pairs with different learning rate, the same augmentation level and seed 0,
• All pairs with the same learning rate, different augmentation level and seed 0,
• All pairs with the same learning rate and augmentation level, but different seeds,
• All checkpoints with seed 0 coupled with the initialization.
This results in 21 pairs overall. For each pair and each α ∈{0, 0.1, . . . , 0.9, 1.0} we evaluated errα, errens
α , Lsoup
α
, Lens
α , as
well as the approximation (1). We performed this evaluation on the ImageNet validation set as well as on the 5 OOD test
sets considered throughout this paper.
The effect of temperature calibration.
Since our ultimate goal is to accurately predict the difference in error rather than
the difference in loss, we introduce the inverse-temperature parameter β to the loss, and tune it to calibrate the soup model.
Speciﬁcally, for every model pair, value of α and test set, we take β = arg minβ Ex,yℓ(βf soup
α
(x); y).
While choosing β based on the soup rather the ensemble might skew the loss in favor of the soup, it has no effect on the
difference in prediction error. Moreover, in preliminary experiments calibrating the ensemble produced very similar results.
In contrast, as shown in Figure K.1, ﬁxing β = 1 throughout results in far poorer prediction of the difference in error.

Model soups: averaging weights of multiple ﬁne-tuned models improves accuracy without increasing inference time
L. Additional baselines
This section explores additional baselines for model soups, including distillation from an ensemble as in Hinton et al. (2014)
(Table L.1), ﬁx-augmentation as in Touvron et al. (2019) (Table L.2), weight-averaging along a trajectory as in Szegedy et al.
(2016); Izmailov et al. (2018) (Figures L.1 and L.2), and Sharpness Aware Minimization as in Foret et al. (2021) (Table L.3).
Unless otherwise mentioned, we ﬁne-tune CLIP ViT-B/32 models with AdamW (Loshchilov and Hutter, 2019) and cosine
annealing learning rate (Loshchilov and Hutter, 2016) for 10 epochs on ImageNet with a learning rate of 2e-5 and medium
augmentation (data augmentation policies are discussed in more detail in Section J.2.1).
We explore the baseline of distillation (Hinton et al., 2014; 2015) from the ensemble of three models trained with different
data augmentation. As previously reported (Bagherinezhad et al., 2018; Beyer et al., 2021), we ﬁnd that it improves accuracy
to run distillation with data augmentation. Unfortunately, this substantially increases the computational resources necessary
to distill from the ensemble. As we cannot cache the predictions of the models in the ensemble, it is necessary to perform a
forward pass for each model in the ensemble at each step of ﬁne-tuning. This makes distilling from an ensemble similarly
expensive as training the models which constitute the ensemble. Nevertheless, as illustrated in Table L.1, model soups still
perform favorably.
Table L.1 also introduces stochastic augmentation. For each data point, stochastic augmentation randomly applies minimal,
medium, or strong data augmentation. Additionally, Table L.2 explores an alternative method for merging augmentations
together. This augmentation policy, which we refer to as ﬁx-aug, is introduced by Touvron et al. (2019). For ﬁx-aug, strong
augmentation is used for all but the ﬁnal epoch, which uses minimal augmentation.
Figure L.1 and Figure L.2 apply model soups to solutions which already average along the ﬁne-tuning trajectory. Methods
for averaging along an individual optimization trajectory include exponential moving averages (EMA) (Szegedy et al., 2016)
and stochastic weight averages (SWA) (Izmailov et al., 2018). We ﬁnd that EMA and SWA can improve the accuracy of a
single model but that model soups provide improvements even when applied to models which have weight-averaging along
their trajectory. We try learning rates 10−5 and 3 · 10−5 and three learning rate schedulers: constant, cosine annealing with
restarts, and cosine annealing (all schedules have a short warm up period). In Figure L.1 we ﬁne-tune a CLIP pre-trained
ViT-B/32, while Figure L.2 ﬁne-tunes an ImageNet-21k pre-trained ViT-B/32.
Table L.3 explores the relation between model soups and sharpness-aware minimization (SAM) (Foret et al., 2021). In line
with previous results, we ﬁnd that SAM improves accuracy over vanilla ﬁne-tuning. Souping two models trained with SAM
improves over either individual model, although the magnitude of the gain is smaller than for vanilla ﬁne-tuning. Souping
models trained with and without SAM yields higher accuracy than souping models trained only with vanilla ﬁne-tuning or
only with SAM.
As a ﬁnal comparison that is potentially useful, we augment Figure 1 with additional comparisons from Table 3. Results are
shown in Figure L.3

Model soups: averaging weights of multiple ﬁne-tuned models improves accuracy without increasing inference time
Table L.1: Comparing model soups to network distillation from an ensemble of models trained with different data augmentations.
Stochastic data augmentation randomly applies minimal, medium, or strong data augmentation.
ImageNet
Distribution shifts
Individual model (LR 3e-05, minimal aug)
76.42
43.21
Individual model (LR 3e-05, medium aug)
78.83
43.55
Individual model (LR 3e-05, strong aug)
79.08
43.75
Individual model (LR 3e-05, stochastic aug)
78.94
45.04
Individual model (LR 3e-05, stochastic aug 3x epochs)
78.38
42.18
Distillation from the ensemble (LR 3e-05, no aug)
78.59
43.45
Distillation from the ensemble (LR 3e-05, stochastic aug)
79.79
45.63
Soup minimal, medium, and strong aug (LR 3e-05)
80.24
47.97
Ensemble minimal, medium, and strong aug (LR 3e-05)
80.19
46.33
Individual model (LR 1e-05, minimal aug)
77.19
47.98
Individual model (LR 1e-05, medium aug)
79.51
46.74
Individual model (LR 1e-05, strong aug)
79.33
46.62
Individual model (LR 1e-05, stochastic aug)
79.48
48.07
Individual model (LR 1e-05, stochastic aug 3x epochs)
79.59
46.89
Distillation from the ensemble (LR 1e-05, no aug)
79.13
47.28
Distillation from the ensemble (LR 1e-05, stochastic aug)
79.88
47.49
Soup minimal, medium, and strong aug (LR 1e-05)
80.08
49.75
Ensemble minimal, medium, and strong aug (LR 1e-05)
80.17
49.36
Table L.2: Comparing models soups of different augmentations with another method which combines different augmentation strategies—
ﬁx aug, as described in Touvron et al. (2019). For ﬁx aug we use strong data augmentation for all except the ﬁnal epoch for which we
apply minimal aug.
ImageNet
Distribution shifts
Individual model (LR 3e-05, minimal aug)
76.42
43.21
Individual model (LR 3e-05, medium aug)
78.83
43.55
Individual model (LR 3e-05, strong aug)
79.08
43.75
Individual model (LR 3e-05, ﬁx aug)
79.43
45.46
Individual model (LR 3e-05, ﬁx aug 4x epochs)
78.57
41.53
Soup minimal, medium, and strong aug (LR 3e-05)
80.24
47.97
Soup minimal, medium, strong, and ﬁx aug (LR 3e-05)
80.41
48.14
Individual model (LR 1e-05, minimal aug)
77.19
47.98
Individual model (LR 1e-05, medium aug)
79.51
46.74
Individual model (LR 1e-05, strong aug)
79.33
46.62
Individual model (LR 1e-05, ﬁx aug)
79.70
48.18
Individual model (LR 1e-05, ﬁx aug 4x epochs)
79.96
45.86
Soup minimal, medium, and strong aug (LR 1e-05)
80.08
49.75
Soup minimal, medium, strong, and ﬁx aug (LR 1e-05)
80.17
49.71
Table L.3: Applying model soups to models trained with sharpness aware minimization (SAM) (Foret et al., 2021).
ImageNet
Distribution shifts
Vanilla ﬁne-tuning (seed 0)
79.32
45.09
Vanilla ﬁne-tuning (seed 1)
79.16
45.12
SAM ﬁne-tuning (seed 0)
79.61
43.78
SAM ﬁne-tuning (seed 1)
79.59
43.79
Soup (vanilla ﬁne-tuning, seeds 0 and 1)
79.78
46.46
Soup (SAM ﬁne-tuning, seeds 0 and 1)
79.85
44.44
Soup (vanilla ﬁne-tuning and SAM ﬁne-tuning, seed 0)
80.04
45.38

Model soups: averaging weights of multiple ﬁne-tuned models improves accuracy without increasing inference time
No EMA
β=0.99
β=0.999
β=0.9999 β=0.99999 β=0.999999
SWA
SWA (70%)
76
77
78
79
80
ImageNet (top-1, %)
LR = 1e-05, LR schedule = constant
No EMA
β=0.99
β=0.999
β=0.9999 β=0.99999 β=0.999999
SWA
SWA (70%)
76
77
78
79
80
ImageNet (top-1, %)
LR = 3e-05, LR schedule = constant
No EMA
β=0.99
β=0.999
β=0.9999 β=0.99999 β=0.999999
SWA
SWA (70%)
76
77
78
79
80
ImageNet (top-1, %)
LR = 1e-05, LR schedule = cosine annealing with restarts
No EMA
β=0.99
β=0.999
β=0.9999 β=0.99999 β=0.999999
SWA
SWA (70%)
76
77
78
79
80
ImageNet (top-1, %)
LR = 3e-05, LR schedule = cosine annealing with restarts
No EMA
β=0.99
β=0.999
β=0.9999 β=0.99999 β=0.999999
SWA
SWA (70%)
76
77
78
79
80
ImageNet (top-1, %)
LR = 1e-05, LR schedule = cosine annealing
No EMA
β=0.99
β=0.999
β=0.9999 β=0.99999 β=0.999999
SWA
SWA (70%)
76
77
78
79
80
ImageNet (top-1, %)
LR = 3e-05, LR schedule = cosine annealing
Soup
Minimal aug
Medium aug
Strong aug
Figure L.1: The improvements offered by model soups are additive with weight-averaging along a trajectory (by SWA or EMA with decay
β). The soup is the average of the model with minimal, medium and strong data aug. Results are shown for a CLIP ViT-B/32 model
ﬁne-tuned on ImageNet. For SWA, we average checkpoints which are saved after each of the 10 epochs, while SWA 70% only averages
checkpoints after ﬁne-tune is 70% complete.

Model soups: averaging weights of multiple ﬁne-tuned models improves accuracy without increasing inference time
No EMA
β=0.99
β=0.999
β=0.9999 β=0.99999 β=0.999999
SWA
SWA (70%)
77.5
78.0
78.5
79.0
ImageNet (top-1, %)
LR = 1e-05, LR schedule = constant
No EMA
β=0.99
β=0.999
β=0.9999 β=0.99999 β=0.999999
SWA
SWA (70%)
77.5
78.0
78.5
79.0
ImageNet (top-1, %)
LR = 3e-05, LR schedule = constant
No EMA
β=0.99
β=0.999
β=0.9999 β=0.99999 β=0.999999
SWA
SWA (70%)
77.5
78.0
78.5
79.0
ImageNet (top-1, %)
LR = 1e-05, LR schedule = cosine annealing with restarts
No EMA
β=0.99
β=0.999
β=0.9999 β=0.99999 β=0.999999
SWA
SWA (70%)
77.5
78.0
78.5
79.0
ImageNet (top-1, %)
LR = 3e-05, LR schedule = cosine annealing with restarts
No EMA
β=0.99
β=0.999
β=0.9999 β=0.99999 β=0.999999
SWA
SWA (70%)
77.5
78.0
78.5
79.0
ImageNet (top-1, %)
LR = 1e-05, LR schedule = cosine annealing
No EMA
β=0.99
β=0.999
β=0.9999 β=0.99999 β=0.999999
SWA
SWA (70%)
77.5
78.0
78.5
79.0
ImageNet (top-1, %)
LR = 3e-05, LR schedule = cosine annealing
Soup
Minimal aug
Medium aug
Strong aug
Figure L.2: The improvements offered by model soups are additive with weight-averaging along a trajectory (by SWA or EMA with decay
β). The soup is the average of the model with minimal, medium and strong data aug. Results are shown for a ImageNet-21k pre-trained
ViT-B/32 model ﬁne-tuned on ImageNet. For SWA, we average checkpoints which are saved after each of the 10 epochs, while SWA 70%
only averages checkpoints after ﬁne-tune is 70% complete.

Model soups: averaging weights of multiple ﬁne-tuned models improves accuracy without increasing inference time
75
76
77
78
79
80
81
ImageNet Accuracy (top-1, %)
35
40
45
50
55
Avg. accuracy on 5 distribution shifts
Greedy Soup
Uniform Soup
Greedy Ensemble
(more compute)
Ensemble
(more compute)
Initialization
Various
hyperparameters
Figure L.3: Adding additional results from Table 3 to Figure 1.

