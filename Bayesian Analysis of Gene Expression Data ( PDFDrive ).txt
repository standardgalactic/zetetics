
Bayesian Analysis of
Gene Expression Data


Bayesian Analysis of
Gene Expression Data
Bani K. Mallick,
Texas A&M University, USA
David Lee Gold,
University at Buffalo, The State University of New York, USA
and
Veerabhadran Baladandayuthapani,
University of Texas MD Anderson Cancer Center, USA
A John Wiley and Sons, Ltd., Publication

This edition ﬁrst published 2009
2009, John Wiley & Sons, Ltd
Registered ofﬁce
John Wiley & Sons Ltd, The Atrium, Southern Gate, Chichester, West Sussex, PO19 8SQ, United Kingdom
For details of our global editorial ofﬁces, for customer services and for information about how to apply for
permission to reuse the copyright material in this book please see our website at www.wiley.com.
The right of the author to be identiﬁed as the author of this work has been asserted in accordance with the
Copyright, Designs and Patents Act 1988.
All rights reserved. No part of this publication may be reproduced, stored in a retrieval system, or transmitted,
in any form or by any means, electronic, mechanical, photocopying, recording or otherwise, except as
permitted by the UK Copyright, Designs and Patents Act 1988, without the prior permission of the publisher.
Wiley also publishes its books in a variety of electronic formats. Some content that appears in print may not
be available in electronic books.
Designations used by companies to distinguish their products are often claimed as trademarks. All brand
names and product names used in this book are trade names, service marks, trademarks or registered
trademarks of their respective owners. The publisher is not associated with any product or vendor mentioned
in this book. This publication is designed to provide accurate and authoritative information in regard to the
subject matter covered. It is sold on the understanding that the publisher is not engaged in rendering
professional services. If professional advice or other expert assistance is required, the services of a competent
professional should be sought.
Library of Congress Cataloguing-in-Publication Data:
Bayesian analysis of gene expression data / edited by Bani Mallick, David Gold, and Veera
Baladandayuthapani.
p. cm.
Includes bibliographical references and index.
ISBN 978-0-470-51766-6 (cloth)
1.
Gene expression–Statistical methods. 2.
Bayesian statistical decision theory. I.
Mallick, Bani K.,
1965- II.
Gold, David, 1970- III.
Baladandayuthapani, Veerabhadran, 1976-
QH450.B38 2009
572.8′6501519542–dc22
2009022671
A catalogue record for this book is available from the British Library.
ISBN 978-0-470-51766-6 (HB)
Typeset in 10/12pt Times by Laserwords Private Limited, Chennai, India.
Printed and bound Great Britain by TJ International Ltd, Padstow, Cornwall.

To parents, Koushambi Nath and Bharati Mallick and his wife, Mou
Bani K. Mallick
To Marlene S. Gold
David Lee Gold
To Upali, Aarith, Aayush and my parents
Veerabhadran Baladandayuthapani


Contents
Table of Notation
xi
1
Bioinformatics and Gene Expression Experiments
1
1.1
Introduction
1
1.2
About This Book
3
2
Gene Expression Data: Basic Biology and Experiments
5
2.1
Background Biology
5
2.1.1
DNA Structures and Transcription
6
2.2
Gene Expression Microarray Experiments
9
2.2.1
Microarray Designs
11
2.2.2
Work Flow
11
2.2.3
Data Cleaning
15
3
Bayesian Linear Models for Gene Expression
21
3.1
Introduction
21
3.2
Bayesian Analysis of a Linear Model
22
3.2.1
Analysis via Conjugate Priors
23
3.2.2
Bayesian Variable Selection
25
3.2.3
Model Selection Priors
26
3.2.4
Priors on Regression Coefﬁcients
27
3.2.5
Sparsity Priors
29
3.3
Bayesian Linear Models for Differential Expression
30
3.3.1
Relevant Work
32
3.4
Bayesian ANOVA for Gene Selection
34
3.4.1
The Basic Bayesian ANOVA Model
35
3.4.2
Differential Expression via Model Selection
36
3.5
Robust ANOVA model with Mixtures of Singular Distributions
38
3.6
Case Study
40
3.7
Accounting for Nuisance Effects
43
3.8
Summary and Further Reading
49
4
Bayesian Multiple Testing and False Discovery Rate Analysis
51
4.1
Introduction to Multiple Testing
51

viii
CONTENTS
4.2
False Discovery Rate Analysis
53
4.2.1
Theoretical Developments
53
4.2.2
FDR Analysis with Gene Expression Arrays
57
4.3
Bayesian False Discovery Rate Analysis
60
4.3.1
Theoretical Developments
60
4.4
Bayesian Estimation of FDR
61
4.5
FDR and Decision Theory
65
4.6
FDR and bFDR Summary
65
5
Bayesian Classiﬁcation for Microarray Data
69
5.1
Introduction
69
5.2
Classiﬁcation and Discriminant Rules
71
5.3
Bayesian Discriminant Analysis
72
5.4
Bayesian Regression Based Approaches to Classiﬁcation
74
5.4.1
Bayesian Analysis of Generalized Linear Models
75
5.4.2
Link Functions
75
5.4.3
GLM using Latent Processes
76
5.4.4
Priors and Computation
76
5.4.5
Bayesian Probit Regression using Auxiliary Variables
77
5.5
Bayesian Nonlinear Classiﬁcation
79
5.5.1
Classiﬁcation using Interactions
79
5.5.2
Classiﬁcation using Kernel Methods
82
5.6
Prediction and Model Choice
84
5.7
Examples
85
5.8
Discussion
87
6
Bayesian Hypothesis Inference for Gene Classes
89
6.1
Interpreting Microarray Results
89
6.2
Gene Classes
90
6.2.1
Enrichment Analysis
92
6.3
Bayesian Enrichment Analysis
94
6.4
Multivariate Gene Class Detection
95
6.4.1
Extending the Bayesian ANOVA Model
98
6.4.2
Bayesian Decomposition
105
6.5
Summary
107
7
Unsupervised Classiﬁcation and Bayesian Clustering
109
7.1
Introduction to Bayesian Clustering for Gene Expression Data
109
7.2
Hierarchical Clustering
111
7.3
K-Means Clustering
112
7.4
Model-Based Clustering
114
7.5
Model-Based Agglomerative Hierarchical Clustering
115
7.6
Bayesian Clustering
116
7.7
Principal Components
117
7.8
Mixture Modeling
119

CONTENTS
ix
7.8.1
Label Switching
124
7.9
Clustering Using Dirichlet Process Prior
126
7.9.1
Inﬁnite Mixture of Gaussian Distributions
131
8
Bayesian Graphical Models
137
8.1
Introduction
137
8.2
Probabilistic Graphical Models
138
8.3
Bayesian Networks
138
8.4
Inference for Network Models
141
8.4.1
Multinomial-Dirichlet Model
143
8.4.2
Gaussian Model
145
8.4.3
Model Search
146
8.4.4
Example
148
9
Advanced Topics
151
9.1
Introduction
151
9.2
Analysis of Time Course Gene Expression Data
151
9.2.1
Gene Selection
152
9.2.2
Functional Clustering
153
9.2.3
Dynamic Bayesian Networks
155
9.3
Survival Prediction Using Gene Expression Data
155
9.3.1
Gene Selection for Time-to-Event Outcomes
156
9.3.2
Weibull Regression Model
156
9.3.3
Proportional Hazards Model
157
9.3.4
Accelerated Failure Time Model
158
Appendix A: Basics of Bayesian Modeling
159
A.1
Basics
159
A.1.1
The General Representation Theorem
160
A.1.2
Bayes’ Theorem
161
A.1.3
Models Based on Partial Exchangeability
162
A.1.4
Modeling with Predictors
162
A.1.5
Prior Distributions
163
A.1.6
Decision Theory and Posterior and Predictive Inferences
165
A.1.7
Predictive Distributions
168
A.1.8
Examples
168
A.2
Bayesian Model Choice
172
A.3
Hierarchical Modeling
175
A.4
Bayesian Mixture Modeling
181
A.5
Bayesian Model Averaging
183
Appendix B: Bayesian Computation Tools
185
B.1
Overview
185
B.2
Large-Sample Posterior Approximations
186
B.2.1
The Bayesian Central Limit Theorem
186

x
CONTENTS
B.2.2
Laplace’s Method
188
B.3
Monte Carlo Integration
191
B.4
Importance Sampling
192
B.5
Rejection Sampling
193
B.6
Gibbs Sampling
195
B.7
The Metropolis Algorithm and Metropolis–Hastings
198
B.8
Advanced Computational Methods
202
B.8.1
Block MCMC
203
B.8.2
Truncated Posterior Spaces
204
B.8.3
Latent Variables and the Auto-Probit Model
204
B.8.4
Bayesian Simultaneous Credible Envelopes
205
B.8.5
Proposal Updating
206
B.9
Posterior Convergence Diagnostics
207
B.10 MCMC Convergence and the Proposal
208
B.10.1 Graphical Checks for MCMC Methods
211
B.10.2 Convergence Statistics
212
B.10.3 MCMC in High-throughput Analysis
213
B.11 Summary
214
References
217
Index
237

Table of Notation
θ ∈
unobserved scalar parameter
θ = (θ1, . . . , θp)
unobserved parameter vector
β ∈B
unobserved regression parameter
β = (β1, . . . , βp)
unobserved regression parameter vector
p
dimensionality of unobserved parameter
ˆθ, ˜θ
point estimators of θ, as described in text
ϵ
unobserved residual
ϵ = (ϵ1, . . . , ϵn)
unobserved residual vector
(θ1, θ2, . . . , θp)
denotes elements arranged in a vector, or vectors
in columns
m ∈M
index of assumed model
g = 1, . . . , G
index of gene g
i = 1, . . . , n
index of subjects or arrays
k = 1, . . . , K
index of groups, clusters, classes, treatments
c = 1, . . . , C
index of iteration in Monte Carlo chain
y
observed scalar response
y = (y1, . . . , yn)
observed response vector
xi
vector of observed covariates for ith subject
X = (x1, x2, . . . , xn)
design matrix of observed covariates
XT
matrix transpose
µ
mean response
P (A|B)
probability of event A given B
P (y|θ)
likelihood of observed response y given
unobserved parameter θ
P (θ)
prior density of θ
π
unobserved mixture weight, π ∈(0, 1)
π = (π1, . . . , πK)
vector of unobserved mixture weights,
K
k=1 πk = 1
σ 2
unobserved dispersion parameter

unobserved dispersion matrix
(¯x, ˆσ 2
x )
sample mean and sample variance of observed
vector x
tg
t-statistic for gene g
τ
unobserved precision parameter τ = σ −2
N(µ, σ 2)
Gaussian density, with mean µ and dispersion σ 2

xii
TABLE OF NOTATION
Gamma(a, b)
gamma density, with shape a and rate b
IG(a, b)
inverse gamma density, with shape a and scale b
NIG(µ, σ 2, a, b)
normal inverse gamma density
U(a, b)
uniform density, with lower and upper bounds
(a, b)
MixNorm(µ, σ 2, π)
mixture Gaussian density, with mixture weights
π1, . . . , πK
Dir(α)
Dirichlet density with parameters α
(θ1, θ2, . . . , θp)T
denotes p elements arranged in a vector, transposed
S(µ, , df )
multivariate t density, location µ, dispersion ,
and degrees of freedom df
r,g
channel index, red and green
G
collection of genes, g1, g2, . . .
J
j=1 aj
a1 + a2 + . . .
Beta(a, b)
beta density with parameters a,b
Dir(α)
Dirichlet density, with parameters α={α}K
k=1
DP(α, 
)
Dirichlet Process density, with parameters α, 
Bernoulli(π)
Bernoulli density, with success probability π
Wishart(α, R−1)
Wishart density, with scale α and p × p matrix R
InverseWishart(α, R)
inverse Wishart density, with scale α and p × p
matrix R
Multinomial(K, π)
multinomial density, with parameters K and π
K
kernel function
p
p-value

1
Bioinformatics and Gene
Expression Experiments
1.1
Introduction
The ﬁeld of genomics has witnessed rapid growth since 1953, when Crick and
Watson were credited with the discovery of the double helix structure of DNA
in the cell nucleus, a discovery that opened the ﬂoodgates for great advances
in research. This new understanding of the molecular basis for the code of life
has enabled reﬁnements to earlier concepts of genetics and evolution, propelling
scientiﬁc discovery at a rate and scale unseen in recorded history. The last 100
years have been a golden age for science as a whole. While much remains
to be learned, concurrent efforts in functional genomics are gradually piecing
together an explanation of how genomic diversity is linked with the diverse
characteristics of organisms. New discoveries are being made every day. The
potential, for example, to treat patients with personalized medicine, or improve
agricultural yields, has never been greater. These technologies are changing the
way we live, and how we use them can ultimately determine how we will survive
as a species in a changing environment. We must adapt and learn in a climate
of global bio-uncertainty. Bioinformatics is at the crossroads of these efforts,
borrowing ideas from computer science, mathematics and statistics, engineering,
biology and genetics, to translate vast amounts of genomic data into quantiﬁable
information that can change the way we process decisions and adapt to competing
forces in our diverse and changing environment.
The early work in bioinformatics largely focused on string processing
(sequencing) the genomes of bacteria and other microbial life forms of interest
Bayesian Analysis of Gene Expression Data
B. Mallick, D. Gold, and V. Baladandayuthapani
2009 John Wiley & Sons, Ltd

2
BAYESIAN ANALYSIS OF GENE EXPRESSION DATA
to researchers in public health. Larger and more ambitious efforts emerged
to complete the sequencing of the human genome, with intense competition
between private industry, dominated by Celera Corporation led by CEO Craig
Venter, and the Human Genome Project, an international project largely funded
by governments and universities. Neither of these emerged as the clear winner,
as both left us with wide gaps in our understanding of the human genome,
although both claimed some degree of success. Private and public efforts can be
credited with greatly increasing our abilities to collect, store and process great
amounts of genomic information on many species.
With completion of the human genome looming, attention has turned to inves-
tigating the links between phenotypic traits and genomic events, e.g. improving
agricultural production, and to complex diseases, with the ultimate goal of map-
ping their molecular proﬁles and improving therapies by what has been come
to be known as personalized medicine. Ever larger genome-wide association
studies are rapidly emerging as design standards to draw inference of genetic
susceptibility to disease. These efforts beneﬁt from large investments in public
infrastructure, such as the National Center for Biotechnology Information (NCBI)
and the European Bioinformatics Institute.
While technological advances offer tremendous quantities of genomic
information, mapping out the genomic links between phenotypes would be very
expensive, even prohibitive, without the aid of high-throughput experiments,
capable of screening thousands of transcripts in unison (Ramsay, 1998). These
large scale experiments are made possible by the ever increasing precision
of micro-technologies, enabling the measurement of more and more genes
per experiment. One difﬁculty in printing and measuring thousands of probe
sequences on a single microarray chip is that the experimental conditions for
any one probe sequence are less than optimal. More costly and time-consuming
technologies offer greater accuracy in measurements. Reﬁnements in gene
expression
microarray
experiments
have
beneﬁted
from
experience
and
multidisciplinary approaches, making next-generation arrays more accurate and
consistent. High-throughput gene expression microarrays are known to yield
noisy and sometimes biased measurements, and statistical modeling issues are
well documented in the literature (Zhang and Zhao, 2000; Alizadeh et al., 2001;
Baggerly et al., 2001; Dougherty, 2001; Hess et al., 2001; Kerr and Churchill,
2001a; Ramaswamy et al., 2001; West et al., 2001; Yang et al., 2001, 2002a).
Gene expression microarray experiments have been conducted for many rea-
sons. Basic biology interests range from functional genomics to gene discovery,
e.g. infer activation/suppression of gene pathways, a topic of ongoing interest in
the ﬁeld of gene expression microarray analysis (Curtis et al., 2005; Jiang and
Gentleman, 2007; Subramanian et al., 2007). On the clinical side, biomarker
discovery is advancing rapidly, with, for example, accounts of validated cancer
signatures receiving much attention (Desmedt et al., 2008; Millikan et al.,
2008). Biomarkers of interest range from diagnostic, i.e. to yield improvements
in early disease detection, to prognostic, i.e. choosing therapies with the aid of

BIOINFORMATICS AND GENE EXPRESSION EXPERIMENTS
3
informative genetic signatures (Zhou et al., 2008). Advances in microarray
technologies have supported and beneﬁted from clinical trials, with routine
protocols to collect and analyze candidate gene expression signatures, as part of
large cohort studies (Fu and Jeffrey, 2007; Koscielny, 2008).
The future of gene expression microarray experiments is quite promising.
Whole genome gene expression microarray chips make it possible to scan the
entire genome of an organism for transcriptional variation. Translational research
is aimed at discovering biological mechanisms, and ultimately developing new
technologies from these discoveries to improve the way we live together as a
species and in balance with our environment. From industry to medical engi-
neering and bioinformatics, multidisciplinary efforts are leading to new biotech-
nologies that will one day replace outmoded technologies, enabling the human
race to achieve a better understanding of itself and to succeed sustainably in a
changing environment. Foreseen challenges include computational hurdles, deal-
ing with the vast amount of information available, and development of tools to
mine and analyze the data, to synthesize the data in a way that is useful for
learning. Overcoming differences in the way diverse information is stored and
analyzed is a major task, necessary for success.
1.2
About This Book
Our purpose in writing this book is to describe the existing Bayesian statistical
methods in the analysis of gene expression data. Bayesian methods offer a num-
ber of advantages over more conventional statistical techniques that make them
particularly appropriate to analyze these sorts of complex data. Bayesian hierar-
chical models allow us to borrow strength among units across a whole data set
in order to improve inference. For example, gene expression experiments used
by biologists to study fundamental processes of activation/suppression frequently
involve genetically modiﬁed animals or speciﬁc cell lines, and such experiments
are typically carried out with only a small number of biological samples. It is
clear that this amount of replication makes standard estimates of gene variability
unstable. By assuming exchangeability across the genes, inference is strengthened
by borrowing information from comparable units.
Another strength of the Bayesian framework is the propagation of uncertainty
through the model. Gene expression data is often processed through a series of
steps, each time ignoring the uncertainty associated with the previous step. The
end result of this process can be overconﬁdent inference. For example, a typical
analysis may begin with some kind of normalization process. The normalized
data will then used for statistical analysis for gene selection, say, which ignores
any uncertainty in the normalizing process. Furthermore, the selected genes may
be used in a classiﬁer, ignoring the uncertainty in the selection process. This way
the analysis is usually broken down into a collection of distinct steps that fail
to correctly propagate uncertainty. In a Bayesian model it is straightforward to
develop integrated models which include each of these effects simultaneously,

4
BAYESIAN ANALYSIS OF GENE EXPRESSION DATA
thus retaining the correct level of uncertainty on the ﬁnal estimates. Detailed
modeling combined with a carefully designed experiment can allow coherent
inference about the unknowns (Shoemaker et al., 1999; Broet et al., 2002; Beau-
mont and Rannala, 2004; Wilkinson, 2007).
This book attempts to describe these hierarchical Bayesian models to analyze
gene expression data. We begin in Chapter 2 with some basic biology relating to
our area of interest. In particular, we begin with the principles behind microarray
experiments. We discuss DNA, cDNA, RNA, and mRNA structures. We also
discuss the basic microarray technology including explaining the steps involved
in generating experimental microarray data.
Chapter 3 discusses inference about differential expression based on hierar-
chical mixture models. It starts with the Bayesian linear model which is the basic
tool to develop these hierarchical models. It also describes integrated Bayesian
hierarchical models, including ﬂexible model based normalization as a part of
the model itself.
In Chapter 4 we consider the problem of Bayesian multiple testing and false
discovery rate analysis. We explain it within the context of decision theory and
illustrate how the choice of loss functions can develop different tests.
Chapter 5 provides a review of Bayesian classiﬁcation methods to classify dis-
eases using gene expression data. We discuss both linear and nonlinear classiﬁers
to develop ﬂexible models to relate gene expression and disease status.
Chapter 6 concerns Bayesian hypothesis inference for gene classes.
Chapter 7 explores clustering models for gene expression data. We review
different Bayesian clustering methods based on principal components analysis,
mixture of Gaussians, as well as Dirichlet processes.
Chapter 8 discusses the development of Bayesian networks to uncover under-
lying relationships between the genes. For example, we develop a network of
dependencies between the differentially expressed genes. In this chapter, we
carry out further investigation to develop probabilistic models to relate genes
based on microarray data.
Chapter 9 contains some advanced topics such as the analysis of time course
gene expression data and survival prediction using gene expression data. Finally,
for scientists who do not have Bayesian training, there are two appendices which
provide basic information on Bayesian analysis and Bayesian computation.
Our book presents state-of-the-art Bayesian modeling techniques for modeling
gene expression data. It is intended as a research textbook as well as valuable
desktop reference. We have used much of the book to teach a three credit hour,
single semester course in the Department of Statistics, Texas A&M University.

2
Gene Expression Data: Basic
Biology and Experiments
2.1
Background Biology
This section contains a brief introduction to the biology of gene expression for
readers with little or no biological background. For detailed information with
illustrations, see Watson et al. (1987), Nguyen et al. (2002), and T¨ozeren and
Byers (2004).
The basic unit of life in all organisms is the cell. In order to survive, several
processes must be carried out by all cells, including the acquisition and assimila-
tion of nutrients, the synthesis of new cellular material, movement and replication.
Each cell possesses the entire genetic information of the parent organism. This
information, stored in a speciﬁc type of nucleic acid known deoxyribonucleic
acid (DNA), is passed on to daughter cells during cell division. All cells perform
some common activities known as ‘housekeeping processes’. Additionally, some
speciﬁc activities are carried out by specialized cells. For example, muscle cells
have mechanical properties, while red blood cells can carry oxygen. All these cell
types have identical genes, but they differ from each other based on expressed
genes.
A gene is a speciﬁc segment of a DNA molecule that contains all the coding
information necessary to instruct the cell for the creation (synthesis) of functional
structures called proteins, necessary for cell life processes. Hence the primary
biological processes can be observed as information transfer processes, and this
is crucial to mediate the characteristic features or phenotype of the cells (like
cancer and normal cells).
There is another type of nucleic acid known as ribonucleic acid (RNA). RNA
molecules have chemical compositions that complement DNA and are involved
Bayesian Analysis of Gene Expression Data
B. Mallick, D. Gold, and V. Baladandayuthapani
2009 John Wiley & Sons, Ltd

6
BAYESIAN ANALYSIS OF GENE EXPRESSION DATA
in the synthesis of protein. The ﬂow of information starts from genes encoded
by DNA to a particular type of RNA known as messenger RNA (mRNA) by
the transcription process and from mRNA to protein by the translation process.
Hence, the kind and amount of protein present in the cell depends on the genotype
of the cell. In this way genes determine the phenotype of cells and hence the
organism. This simpliﬁed model to relate gene to phenotype is illustrated below:
DNA →mRNA →Amino : acid →Protein →Cell : phenoype
→Organism : phenotype
A gene is expressed if its DNA has been transcribed to RNA, and gene
expression is the level of transcription of the DNA of the gene. This is known
as transcription level gene expression and microarrays measure this gene expres-
sion. We can also obtain gene expression at the protein level where the mRNA
is translated to protein, and protein arrays have also been developed (Haab
et al., 2001). Southern blotting and other methods can detect mRNA expres-
sion of a single or a few genes. The novelty of microarrays is that they quantify
transcript levels (mRNA expression levels) on a global scale by quantifying the
transcript abundance of thousands of genes simultaneously. Hence, this technique
has allowed biologists to take a ‘global perspective on life processes – to study
the role of all genes or all proteins at once’ (Lander and Weinberg, 2000).
There are three primary information transfer processes in functioning organ-
isms: (1) replication, (2) transcription and (3) translation. We concentrate on
transcription in what follows as it is directly relevant to DNA microarray tech-
nology.
2.1.1
DNA Structures and Transcription
DNA consists of four primary types of nucleotides: adenine (A), guanine (G),
cytosine (C) and thymine (T). DNA exists as a double helix (Watson and Crick,
1953) where each strand is made of different combinations of the same four
molecular beads, represented by A, C, G and T (Figure 2.1). Also the bonds
joining the nucleotides in DNA are directional, with what are referred to as
a 5′ end and a 3′ end. The beads in opposing strands complement each other
according to base pairing combinations. The two strands of the helical chains are
held together by hydrogen bonding between nucleotides at the same positions in
opposite strands. The same position refers to an identical number of nucleotides
from one end of the DNA. The A nucleotide on one strand always pairs with T
on the other strand at the same position. The nucleotides C and G are similarly
paired. Hence, the pairs (A, T) and (C, G) are known as complementary base
pairs. This complementary base pair rule assures that the information stored
in DNA is in duplicate. In this way we obtain complementary DNA strands.
For example, if one DNA strand contains the sequence 5′ AACTTG 3′ at a
certain location, the complementary strand will have the sequence 3′ TTGAAC
5′ at the same position. To generate new copies of DNA, each single strand of

GENE EXPRESSION DATA: BASIC BIOLOGY AND EXPERIMENTS
7
Adenine
Sugar
Phosphate
Backbone
Base pair
Nitrogenous
base
Thymine
Guanine
Cytisine
G
C
G
G
T
T
A
A
T
A
T
A
A
A
C
C
G
C
C
G
T
T
A
A
Figure 2.1
Double stranded helix structure of DNA. The types of base pairs
found in DNA are restricted to those shown in the ﬁgure. (Modiﬁed from
http://www.nhgri/nih.gov/DIR/VIP/Glossary.)
DNA becomes a template to produce a complementary strand. The hereditary
information of an organism is distributed along both strands of a DNA molecule.
The term ‘gene’ generally refers to those segments of one strand of DNA that are
essential for the synthesis of a functional protein. It also refers to the segments
of a DNA strand that encodes an RNA molecule.
RNA is a single-stranded nucleic acid containing uracil (U) in place of
thymine (Figure 2.2). This uracil forms a hydrogen bond with adenine (A). DNA
transcription is the information transfer process directly relevant to DNA microar-
ray experiments because quantiﬁcation of the type and amount of this copied
information is the goal of the microarray experiment. In the transcription phase,
the DNA sequence that ultimately encodes a protein is copied (transcribed) into
RNA and this resulting RNA molecule is known as messenger RNA as it carries
the information contained in DNA. There are three stages in the transcriptional
process involving RNA chain: (1) initiation, (2) elongation, (3) termination.
Promoter regions are those parts of DNA that signal the initiation of tran-
scription. Speciﬁc DNA sequences in the promoter region generate an enzyme
called RNA polymerase II at the transcription initiation site. This RNA poly-
merase moves along the DNA and extends the RNA chain by adding nucleotides
with base A, G, C, or U where T, C, G, or A is found in the DNA template
strand, respectively. This complimentary pair rule is illustrated by the sequence

8
BAYESIAN ANALYSIS OF GENE EXPRESSION DATA
Cytosine C
Cytosine
C
Guanine G
Guanine
G
Adenine
A
Adenine
A
Nitrogenous
Bases
Nitrogenous
Bases
Ribonucleic acid
Deoxyribonucleic acid
Nitrogenous
Bases
Base pair
Sugar
phosphate
backbone
replaces Thymine in RNA
Uracil
U
Thymine
T
RNA
DNA
AUCG’S
ATCG’S
H
H
H
H
H
H
CH3
N
N
N
N
N
H
H
H
N
H N
N
N
N
N
H
H
H
H N
N
N
N
N
H
H
H
H
H
N
N
H
O
CH3
H
H
N
O
N
O
N
O
H
H
CH3
N
N
N
H
H
N
N
O
N
O
H
H
H
H
N
N
N
O
Figure 2.2
Comparison of RNA and DNA structures. (Modiﬁed from http://www.
nhgri/nih.gov/DIR/VIP/Glossary.)
3′: TACCGAAATGAATGCGCTTA : 5′ :: DNA
5′: AUGGCUUUACUCACGCGAAU : 3′:: mRNA
RNA polymerase enzyme recognizes signals in the DNA sequences for chain
termination, resulting in the release of the newly synthesized RNA from the
DNA template. The resulting mRNA leaves the nucleus and associates with a
ribosome, where translation occurs.
Some preprocessing usually take place before transportation of the message.
For example, an enzyme called polyadenylase adds a sequence of As known as
poly(A) tail to the RNA strand. The poly(A) tail plays a key role in microarray
experiments. RNA splicing is another important part of mRNA formation. The

GENE EXPRESSION DATA: BASIC BIOLOGY AND EXPERIMENTS
9
DNA segments that encode for a protein are known as exons, while noncoding
segments are called introns. RNA splicing is a series of splicing reactions that
remove the intron regions and fuse the remaining exon regions together. Thus,
the resulting mRNA contains only the coding sequences (exons) and can be
identiﬁed by the poly(A) tail.
A spliced mature mRNA molecule exists in the nucleus and is transported to a
ribosome in the cytoplasm where it directs the synthesis of a molecule of protein.
At the ribosome, the mRNA molecule directs the synthesis of a protein molecule
via a special genetic code. It is based on triplets of contiguous nucleotides, called
codons, which correspond to speciﬁc amino acids. This process of converting an
mRNA molecule to a protein is called translation, and involves transfer RNA
(tRNA) molecules. A unique tRNA molecule exists for each possible codon.
A tRNA molecule has an anticodon at one end, which binds to its corresponding
codon. At the other end, it has the corresponding amino acid. The anticodons
of tRNA molecules bind to the corresponding codons of the mRNA molecule to
bring the correct sequence of amino acid together. We illustrate this process in
Figure 2.3.
It is clear that a correspondence exists between protein molecules and mature
mRNA molecules. Mature mRNA molecules are also known as mRNA tran-
scripts because they are synthesized by transcription of DNA. Each synthesized
molecule of protein requires and consumes one transcript, hence the rate of
synthesis of a protein can be estimated by quantifying the abundance of corre-
sponding transcripts. DNA microarrays are assays for measuring the abundance
of mRNA transcripts corresponding to thousands of genes in a collection of
cells.
2.2
Gene Expression Microarray Experiments
Gene expression microarray technologies yield discrete measurements of thou-
sands of transcripts at a fractional per gene basis compared to other methods,
facilitating genome-wide discovery of the relative fold change in gene expression
in, for example, diseased versus normal tissues (Alizadeh et al., 1999), discrete
time course in the progression of disease (Richardson et al., 2007) or in different
disease tissue types (Yeang et al., 2001). Moreover, these technologies offer the
potential to discover molecular signatures that can differentiate what was once
thought to be the same disease, i.e. to differentiate pathologically similar though
molecularly distinct disease subtypes (Bhattacharjee et al., 2001; DeRisi et al.,
1996; Weigelt et al., 2008; Eisen et al., 1998). Clinical applications include but
are not limited to diagnostic and prognostic indicators (Cardoso et al., 2008;
Bueno-de-Mesquita et al., 2007; Fan et al., 2006; Winter et al., 2007).
The tradeoff of measuring so many variables at once is quality. The conditions
for measuring gene expression are not optimized for any one transcript, as in more
expensive per gene methods. Challenges include technical artifacts, occasional
poor arrays, or transcript-speciﬁc irregularities. In this section, we will examine

10
BAYESIAN ANALYSIS OF GENE EXPRESSION DATA
Mature mRNA
Transport to cytoplasm
Translation
Amino acid
chain (protein)
tRNA
Amino acid
Anti-codon
Codon
mRNA
Ribosome
mRNA Transcription
DNA
Nuclear membrane
Figure 2.3
Information ﬂow in a cell from DNA to RNA to protein. (Modiﬁed
from http://www.nhgri/nih.gov/DIR/VIP/Glossary.)
some of the developments in microarray technologies, and offer insights into how
the experiments are conducted.
The ﬁrst account of expression proﬁling on a microarray was given by Schena
et al. (1995). Since then, rapid investment and production of microarray exper-
iments has been under way (Shalon et al., 1996; Schena et al., 1998; Ramsay
1998). Early experiments with gene expression microarrays largely demonstrated
proof of principle (Ramaswamy et al., 2003; Shipp et al., 2002; Golub et al.,
1999; Alon et al., 1999; Yeang et al., 2001). While offering promising results,
early studies were plagued with technical design problems, poor array qual-
ity, and lacked reproducibility (Kerr et al., 2000; Zhang et al., 2008). Among
other challenges, the genomes of many organisms were not yet fully annotated,
contributing to confusion in analyzing and interpreting results.

GENE EXPRESSION DATA: BASIC BIOLOGY AND EXPERIMENTS
11
Conventional experiments with microarrays depend on sophisticated robotics
and micro-technologies. Since the advent of gene expression microarrays, pow-
erful technologies have emerged, some commercial (e.g. Affymetrix, Agilent,
Illumina) and others academic (the Stanford Brown Lab). Each technology pro-
vides its own unique results, corresponding to very different array designs and
manufacturing steps. Everything from estimation of relative gene expression to
high-dimensional data analysis depends on the technological platform.
2.2.1
Microarray Designs
Depending on the scientiﬁc investigator’s needs, there are many microarray prod-
ucts to choose from, each designed to measure gene expression under different
conditions, for example in Saccharomyces cerevisiae, Drosophila melanogaster,
or Homo sapiens. Some proprietary manufacturers offer custom arrays, although
in practice ready-made arrays are more commonly applied; see, for example,
the Affymetrix HT Mouse Genome 430 Array or the HT Human Genome U133
Array series (http://www.affymetrix.com).
The surface of an array, glass or silicon, is made up of a grid of features, or
spots, each corresponding to a gene or nucleotide sequence. Very small amounts
of a DNA sequence, of the order of picomolars, of a gene are demobilized at
a respective spot on the array surface. Note that the entire nucleotide sequence
of each gene is not ligated to the respective spots, rather, shorter nucleotide
sequences, or probes, unique to a particular chromosomal locus of a gene, are
chemically attached to the chip surface. Some platforms include more than one
probe per gene, and replicated probes. The genes and probes must be care-
fully selected in advance, requiring trial and error with clone libraries for each
sequence. As some probes are better suited to microarray experimentation than
others, the experimental conditions for any one probe cannot be optimized.
On an Affymetrix microarray chip, each gene is represented by a probe
set, and in some cases more than one probe set per gene. Each probe set
is a collection of between 14 and 20 probes. Each probe represents a short
sequence, or oligonucleotide, of 25 bases called the perfect match (PM) probe.
A mismatch (MM) probe, identical to the PM, with the exception of a single
switch in base at the center of the nucleotide, is included for control rea-
sons discussed below. For example, suppose that the PM probe sequence is
GCACAGCTTGCAAAGGATATTGCCA. Switching the middle base from A to
T, the MM probe sequence is GCACAGCTTGCATAGGATATTGCCA.
2.2.2
Work Flow
Each stage of a microarray experiment is important, relevant to the analysis, and
contributes to total variation. Some steps are platform-dependent, while in general
most microarray experiments include (1) sample collection, (2) preparation, (3)
hybridization, and (4) ﬂuorescent scanning and image processing. These stages

12
BAYESIAN ANALYSIS OF GENE EXPRESSION DATA
of a microarray experiment are described below, with special attention to the
Affymetrix platform due to its widespread application.
1. Sample Collection
Sample mRNA collection methods depend on the scientiﬁc problem, for example,
from a patient biopsy, or from immortalized cell lines. A minimum of 5 mg of
usable mRNA is generally considered sufﬁcient and necessary for each microar-
ray. In cases where sample is in short supply, each small quantity must be
preserved. Sample testing of quality and other tissue tests for purity are unfor-
tunately forgone in some cases, or performed on only a subsample, for want of
supply. In some cases less total RNA might be considered sufﬁcient, depending
on the scientiﬁc relevance of the experiment and on the circumstances that make
collection difﬁcult or expensive.
Accepted media for tissue range from fresh or frozen tissue to parafﬁn embed-
ded cores. Note that one of the inevitable challenges in working with resected
tumor tissues is that the cellular makeup of the sample can vary. For example,
in cancer studies, diseased tissue contains a mixture of cells, with variability in
the percentage of tumor tissue present in the sample, confounding the results.
Determination of the cellular content is often expensive, time-consuming and not
achievable for gene expression microarray experiments. As a guide, the sample
should be as homogeneous a collection of cells as possible, although in prac-
tice this cannot be guaranteed. For an in-depth study of the variability in gene
expression attributable to such causes in prostate cancer, see Stuart et al. (2004).
2. Sample Preparation
The tissue sample must be prepared before it is deposited on a microarray.
The mRNA is collected in a solution, fragmented with restriction enzymes, and
reverse transcribed from mRNA to cloned DNA (cDNA) by reverse transcriptase
catalysts. In its natural form, mRNA is relatively unstable, while cDNA is much
better preserved. In some instances, ampliﬁcation of the sample is performed by
polymerase chain reaction. Ampliﬁcation is an accepted practice for gene expres-
sion microarray experiments, although with small amounts of starting material,
e.g. as little as 100 µg, experimental sensitivity is reduced (Gold et al., 2004).
The cDNA fragments are denatured with heat, so that the complementary
sequences of the DNA double helix structure, see Figure 2.1, are chemically
unbound, and available to bind or hybridize to their respective complementary
oligonucleotides spotted on the array. In order to measure the relative quantity
of cDNA that has hybridized at a spot, the cDNA fragments are labeled with a
ﬂuorescent dye capable of emitting a frequency pulse when activated by a laser.
In two-channel experiments, sample is collected for two tissues of different ori-
gin, e.g. diseased and normal tissue, each labeled with a different ﬂuorescent
dye, Cy3 and Cy5 for example. Different dyes emit a different wavelength when
activated, enabling measurement of the relative ﬂuorescence at each spot. Unfor-
tunately, different dyes are known to decay at different rates, have different
dynamic ranges, and, if unaccounted for, can confound a study (Churchill 2002).

GENE EXPRESSION DATA: BASIC BIOLOGY AND EXPERIMENTS
13
Figure 2.4
Affymetrix GeneChipprobe array. Image courtesy of Affymetrix.
Affymetrix microarray experiments are one-channel experiments. Short oligo
sequences are labeled as cRNA rather than cDNA. The cDNAs are produced as
before, but used to make target cRNAs with biotinylated nucleotides. After the
sample is deposited, the chip is washed and incubated with a ﬂuorescent dye hat
binds to the biotins on the cRNAs. Dye bias is not an issue in single channel
experiments.
3. Hybridization
A central premise underlying gene expression microarray technology is that
labeled cDNA fragments will hybridize to the correct complimentary fragment
on the array, a process called complementary hybridization. Hybridization is
nature’s way of attraction to and binding of complementary DNA sequences.
The physics of hybridization is quite complicated. Technical spot-to-spot vari-
ability can be attributed to more than just differences in the quantity of cDNA
deposited on an array or at a spot. Physical differences in the binding efﬁcien-
cies of different DNA fragments are well known (Zhang et al., 2003) in target
probes, as well as dye incorporation and sequence errors. Non-speciﬁc binding,
whereby hybridization occurs between unintended and inexact complements, is
also a confounding factor in high-throughput gene expression experiments. For
this reason, Affymetrix includes a PM sequence and a MM sequence for each
25-mer oligo probe on the chip. The 25-mer mismatch is identical to the PM,
except for a base pair change at the central position in the sequence. The MM
signal can be used as a control to normalize the PM signal, and account for such

14
BAYESIAN ANALYSIS OF GENE EXPRESSION DATA
biases due to nonspeciﬁc binding. In practice, though, the beneﬁt of using the
mismatch probes has been called in to question, and is suspected of offering an
unfavorable bias–variance tradeoff (Wang et al., 2007).
4. Scanning and Image Processing
The image data for a single microarray consists of pixel intensities correspond-
ing to probes/genes as arranged on the array. When a ﬂuorescent dye absorbs
light, ﬂuorescence is emitted, and measured by optical scanning equipment, for
example the Affymetrix Scanner 3000 7G, Agilent’s DNA microarray scanner
with SureScan High-Resolution Technology, or Axon GenePix scanners. The
scans provide a high-resolution map of the ﬂuorescence over the array. Dur-
ing scanning, focused laser excitation of the dyes provides illumination over the
array surface. Each pixel intensity value is stored in bits, called color depth.
A color depth of, for example, 16 bits per pixel (as is common in microarray
scanners) means that the intensity value of each pixel is an integer between 0 and
65 535(= 216 −1). The number of pixels contained in a digital image is called
its resolution. The high-resolution gray-scale TIFF image produced by the scan
is used for later analysis. Note that for any given dye, there is a range of possible
wavelengths. If the excitation wavelength is too close to the emission peak, the
signal will be low. Too much excitation can damage the sample, resulting in a
highly saturated signal, i.e. a ‘bright’ array. In practice, balancing the laser gain
setting to optimize efﬁciency requires experience.
Imaging software capable of processing the array images is used to generate
a detailed map of the ﬂuorescent signal at each spot, between the spots, and
over the entire array. Such software relies on sophisticated algorithms, compu-
tational signal processing tools that can identify the features on the array. Array
localization, which is usually software-driven, involves delineating the spots cor-
responding to the genes in the image. Ideally, every spot should be circular in
shape and all spots should have consistent diameters. This is rarely the case. The
observed spots deviate from the circle in having a donut, sickle, oval or pear
shape. The image analysis software rectiﬁes these spatial problems by capturing
the true shape. Other image analysis techniques use pixel distributions such as
histograms to deﬁne spots. Hybrid approaches have been suggested that combine
both the spatial and distributional approaches. Users may aid the software by out-
lining grids and providing information about spot size and the number of rows
and columns spotted on the slide. Manual adjustments could be incorporated to
improve upon automated spot identiﬁcations.
The pixel intensities at a spot, or feature, are a compilation of ﬂuorescence
signals from speciﬁc hybridization and all other sources. Noise in ﬂuorescence
can come from many sources, including the array surface itself, any treatments
on the slide, and unintended material on the array surface. These ﬂuorescence
sources of noise must be accounted for. Only the ﬂuorescent emission due to
the biology (of scientiﬁc interest) should be included in downstream analysis.
Image segmentation techniques are used to classify each pixel in the target area
as either foreground (spot signal) or background. There are variety of proprietary

GENE EXPRESSION DATA: BASIC BIOLOGY AND EXPERIMENTS
15
commercial approaches available to this end. The spot ﬂuorescent intensity is
composed of both the biological signal we are interested in, and the background
ﬂuorescent noise, that we would like to remove. Background corresponds to the
ﬂuorescence that may contribute to the spot pixel intensities that are not due to
the target molecules such as dust particles, stray molecules and the slide itself.
Background areas vary across slides so most software attempts to measure local
background by quantifying pixel intensities around each spot. For example, the
pixel intensities between concentric circles around the target spot can be used as
background. See Yang et al. (2002b) for a comparison of image analysis methods
for cDNA microarrays. Systematic trends in the background ﬂuorescence can be
accounted for with spatial techniques (Wilson et al., 2003).
Having deﬁned the spot signal and background areas, the pixels in these areas
are now used to compute the spot and background intensities. Some summary
statistics are used to represent the intensities for all the pixels in the respective
areas. Examples include the mean (average of all pixel intensities), median, mode
(peak of histogram), area (number of pixels) and total intensity (sum of pixel
intensities).The best measure of intensity remains an open question, with the
mean, median or some quantile of the intensity being the most common.
The imaging software used for quantiﬁcation also outputs some spot quality
statistics from which the reliability of the spots can be inferred. Different image
analysis software includes GenePix, SPOT, ScanAlyze, UCSG Spot, Agilent’s
Feature Extraction software and Imagene. Many quality statistics have been pro-
posed for measuring the quality of the signal measured at each spot. For example,
the ratio of the sample mean to the sample standard deviation of the signal inten-
sities at, i.e. pixels within or near, a spot provide a measure of the signal-to-noise
for ﬂagging weak or outlier spots.
The Affymetrix imaging software uses a griding procedure based special align-
ment features located in the four corners of the chip, to segment the spots. The
spots, or features, on Affymetrix chips are rectangular in shape, with sides about
5–8 pixels in length. Affymetrix uses a special software to determine the locations
the features on the array. The intensity value for a feature is computed by default
as the 75th percentile of the pixel intensities for the whole interior of the feature,
excluding the boundary pixels. If the segmentation algorithm fails to properly
align the features on the chip, the signal estimates of the true features on the chip
can be seriously biased. For more discussion on image processing for Affymetrix
chips, see Arteaga-Salas et al. (2008) and Schadt et al. (2001) (Figs. 2.5–2.7).
2.2.3
Data Cleaning
Data cleaning protocols are universally accepted as essential for producing rea-
sonable analyses with gene expression microarray data, although the analytical
application and procedures are to a certain extent platform-speciﬁc, while gen-
eral principles and methodologies can offer guidance. Nonbiological sources of
variation on microarrays include technical replicate variation, dye-to-dye varia-
tion, inter-operator, array-to-array, day-to-day and inter-lab variation. Replication

16
BAYESIAN ANALYSIS OF GENE EXPRESSION DATA
RNA fragment hybridizes with DNA on GeneChip® array
RNA fragments with fluorescent tags from sample to be tested
Figure 2.5
Cartoon depicting hybridization of tagged probes to Affymetrix
GeneChipmicroarray. Image courtesy of Affymetrix.
Figure 2.6
Affymetrix GeneChipScanner 3000. Image courtesy of Affymetrix.

GENE EXPRESSION DATA: BASIC BIOLOGY AND EXPERIMENTS
17
Figure 2.7
Affymetrix Array WorkStation. Image courtesy of Affymetrix.
of microarray experimental samples can provide some evidence of anomalies.
Absent expensive validation techniques, there is no way of knowing if the gene
expression measure for a particular gene in a given experiment is too noisy to
provide information about important of biological changes.
It is almost universal practice to correct the foreground intensities by
subtracting the background intensity near the probe feature. The motivation
for background adjustment is the belief that a spot’s measured intensity
includes a contribution not speciﬁcally due to the hybridization of the target
to the probe, e.g. cross-hybridization and other chemicals on the glass. An
undesirable side-effect of background correction is that negative intensities
may be produced for some spots and hence missing values if log-intensities
are computed, resulting in loss of information associated with low channel
intensities. Moreover, such a background correction tends to inﬂate the noise
in the expression values for low expressing genes (Bolstad, 2006). A better
approach is to use a smoother estimate of the background. Yang et al. (2002b)
recommend a morphological background such as those produced using Spot
and recent versions of GenePix. In summary, all background correction tends to
increase the noise especially in low expressed genes, even if marginally, hence
some users choose to ignore any sort of background correction.
Another critical step before analyzing any microarray data is array
normalization. The purpose of normalization is to adjust for any bias arising
from variation in the experimental steps rather than from biological differences
between the RNA samples. Such systematic bias can arise, inter alia, from
red-green bias due to differences in target labeling methods, scanning properties
of two channels perhaps by the use of different scanner settings or variation

18
BAYESIAN ANALYSIS OF GENE EXPRESSION DATA
over the course of the print run or nonuniformity in the hybridization. Thus
it is necessary to normalize the intensities before any subsequent analysis is
carried out. Normalization can be carried out within each array or between
arrays. There are several methods available for array normalization such as
global mean methods, iterative linear regression, curvilinear methods (e.g.
lowess) and variance model methods. The simplest and most widely used within
array normalization assumes that the red-green bias is constant with respect to
log-scale across the array (see Ideker et al., 2000; Chen et al., 1997).
Of speciﬁc interest within a slide are systematic differences due to intensity-
and location-dependent dye biases. One way to compare red and green channel
measurements for each spot on an array is an MA plot. The M value on the
vertical axis is the log2-ratio of the red and green channel intensities i.e. the fold
change on log-scale between the samples. The A value on the horizontal axis is
the average of the log2 intensities. In principle at least, it is typically expected
that, free of bias, an MA plot should be centered around 0, although this is rarely
that case. Trends in M versus A can be signs of channel bias. A smoother, such
as a loess smoother, is usually used to give an intensity-dependent correction,
such that the M values are now centered around 0. This removes any global
intensity-dependent dye biases and is sometimes called the lowess normalization
method. Although the lowess normalization method adjusts for location, it does
not account for spatial/regional differences in variability, i.e. M values have
different variability depending upon the grid they lie in. Yang et al. (2002a)
proposed normalizing the median of the absolute deviation of M values.
There are various algorithms for normalization for Affymetrix data. Bolstad
et al. (2003) propose a quantile normalization where the goal is to make the
empirical distribution of intensities the same across arrays. The target distribution
is found by averaging the quantiles for each of the arrays in the data set. Quantile
normalization changes expression over many slides, i.e. changes the correlation
structure of the data, and may affect subsequent analysis. A thorough comparison
of quantile normalization with other methods, and its effects on variability and
bias, can be found in Bolstad et al. (2003).
Another very popular method for normalization of Affymetrix array is
robust multichip analysis (RMA); see Irizarry et al. (2003). It is implemented
in the R package affy and is available for download from http://www.
bioconductor.org/. The RMA algorithm only use the PM intensities and ignores
the MM intensities from an array. The PM intnesities are background corrected
on the raw intensity scale, to yield yij = log2(PMij −BGij), where BGij is an
estimate of the background signal of the jth probe on the ith array . The RMA
expression measure is then based on the following model, called the multi-array
probe-level model (PLM):
yij = βi + αj + ϵij
where i and j index the array and probe, respectively. The parameter βj is the
expression of the probe set in array i, αj is the probe afﬁnity effect for the jth

GENE EXPRESSION DATA: BASIC BIOLOGY AND EXPERIMENTS
19
probe in the probe set and the ϵij are the residuals. The parameters βi and αj) are
then estimated using robust methods such as the median Polish method (quicker)
or via robust linear models.
RMA is not the only expression measure possible. A popular modiﬁcation
of the algorithm is known as GCRMA (Wu et al., 2004), which incorporates
probe sequence information into the background correction algorithm. Li and
Wong’s (2001) dChip MBEI also uses a multi-array model which is multiplica-
tive with additive errors ﬁtted on the natural scale and a different nonlinear
algorithm is used. Affymetrix also provides MAS 5.0 expression measures and
more recently an algorithm called PLIER. MAS 5.0 values are typically noisy in
the low-intensity range and use a simple linear scaling normalization.
Data cleaning methods, while essential, are not the focus of this book. Rather,
we take the gene expression measurement from a preprocessed experiment as
given, and proceed to the challenging tasks of extracting and interpreting infor-
mation from the results. For further reading on data cleaning procedures, see
Speed (2003), Bolstad et al. (2003), Gentleman et al. (2004), Brettschneider
et al. (2008), and Fan et al. (2004). For comparisons of array technology plat-
forms, see Thompson and Pine (2009), McCall and Irizarry (2008), and Yauk
et al. (2004). Lab-to-lab variation is discussed in Irizarry et al. (2005), Fare et al.
(2003), Dobbin et al. (2005), and MAQC Consortium (2006). For clinical appli-
cations, see Olson (2004), Pusztai and Hess (2004), and Ramaswamy and Golub
(2002). For more infomation on Affymetrix microarrays and technical aspects of
low-level data processing, see the dChip software (http://www.dchip.org; Li and
Wong, 2001) and the Affymetrix white pages available at http://www.affymetrix.
com/support/technical/whitepapers/sadd_whitepaper.pdf.


3
Bayesian Linear Models
for Gene Expression
3.1
Introduction
The linear model is perhaps the most fundamental of all applied statistical models,
encompassing a wide variety of models such as analysis of variance (ANOVA),
regression, analysis of covariance (ANCOVA), and mixed effect models. A large
number of such statistical models, when appropriately parameterized, can be
reduced to a linear model, hence making it a rich class of models. Usually the
aim of such statistical models is often to characterize the dependencies among
several observed quantities. For example, how does the expression of given gene
or group of genes affect a clinical phenotype, or how do gene expressions vary
from the diseased group to the non-diseased group? A variety of such questions
can be answered using variations and extensions of the basic linear model. In
general, the ultimate goal of these analyses is to ﬁnd a proper representation of
the conditional distribution, P (y|x), of an observed variable y, given a vector
of observations, x based on random samples of y and x. Although a complete
characterization of the density P (y|x) is generally difﬁcult, estimation usually
proceeds by restricting the space of densities to a parametric family, indexed
by parameters, a route we shall follow here. We start with a brief introduction
to linear models, especially in the Bayesian context, before delving into the
application of linear models to gene expression data.
The basic ingredients of a linear model include a response or outcome variable
y which can be continuous or discrete. The variables X = (x1, . . . , xp) are called
the explanatory variables (p in number) and can be continuous or discrete or
Bayesian Analysis of Gene Expression Data
B. Mallick, D. Gold, and V. Baladandayuthapani
2009 John Wiley & Sons, Ltd

22
BAYESIAN ANALYSIS OF GENE EXPRESSION DATA
a combination of both. For example, in typical settings discussed in this book
X might represent the gene expression of p genes from a given microarray.
The distribution of y given X is typically studied in the context of a set of
units or experimental subjects, i = 1, . . . , n, in which yi and xi1, . . . , xip are
measured. We denote y = (y1, . . . , yn) as the n × 1 vector of response variables
and X = (x1, . . . , xp) as the n × p matrix of explanatory variables.
A variety of models with various dependence structures can be posited on the
response and explanatory variables, and we consider the simplest case here: the
normal linear model1 where the distribution of y given X is normal with mean
a linear function of X admitting the model
y = Xβ + ϵ
(3.1)
in which β = (β1, . . . , βp)T is a p × 1 vector of regression coefﬁcients and the
error process, in the simplest case, is assumed to be N(0, σ 2In). More com-
plicated error structures can be accommodated by this model depending on
application, and we shall defer this to later. Thus, the parameters to estimate
in this model are θ = (β, σ 2). Before we get into estimation, we refer to two
implicit assumptions. First, we assume that the response variables and explana-
tory variables are clearly deﬁned (with appropriate transformations if required).
Second, we assume that the explanatory variables are observed without error.
Further modeling mechanisms need to be in place to account for such errors, but
these lie outside the scope of this book. The main practical advantage of making
these assumptions is that it is much easier to specify a realistic conditional dis-
tribution on one variable y rather than a joint distribution on p + 1 variables. We
note there that there is a huge literature on Bayesian analysis of linear models and
it is not our intention to reproduce that here; rather, we present those aspects of
estimation and inference that are pertinent to our case studies on gene expression
data.
3.2
Bayesian Analysis of a Linear Model
Before discussing the Bayesian approach in detail, we brieﬂy discuss the classical
estimation of the linear model (e.g. maximum likelihood estimation), and relate
the results from a Bayesian perspective. Under the Gaussian speciﬁcation above,
the likelihood of a simple linear model can be written as
l(β, σ 2|y, X) ∝(σ 2)−n/2 exp
 1
2σ 2 (y −Xβ)T (y −Xβ)

.
(3.2)
The maximum likelihood estimator of β is then obtained by maximizing the
above likelihood (or its logarithm, the log-likelihood), and is given by β =
(XT X)−1XT y. This is the ordinary least squares (OLS) estimator and can be
1We shall use the terms ‘linear model’ and ‘linear regression’ interchangeably while referring to
same underlying model.

BAYESIAN LINEAR MODELS
23
viewed as the orthogonal projection of y on the linear subspace spanned by the
columns of X. In order to avoid non-identiﬁability and uniqueness problems,
we assume that X is of full rank, i.e., rank(X) = p + 1. In addition, we assume
p + 1 < n, in order for the proper estimates of β to exist, since XT X is not
invertible if the condition does not hold.
Similarly, an unbiased estimator of the error variance σ 2 is given by
σ 2 =
1
n −p(y −Xβ)T (y −Xβ)
(3.3)
and σ 2(XT X)−1 approximates the covariance matrix of β.
3.2.1
Analysis via Conjugate Priors
Bayesian inference and estimation of the above linear model proceeds by eliciting
the prior distribution of the parameters θ = (β, σ 2) (Lindley and Smith, 1972).
A variety of choices are at our disposal here. A class of popular priors used for
building Bayesian linear models are conjugate priors. Noting that the likelihood
function in (3.2) has Gaussian kernel linked with β and the inverse-gamma kernel
for σ 2, a conjugate class of priors is speciﬁed as
β|σ 2, X ∼N

µβ, σ 2Vβ

,
σ 2|X ∼IG (a, b) ,
a, b > 0,
(3.4)
where µβ is a p-dimensional vector and Vβ is a p × p positive deﬁnite symmetric
matrix. We call this the normal-inverse-gamma (NIG) prior and denote it by
NIG(µβ, Vβ, a, b), which is deﬁned by the joint probability distribution of the
vector β and the scalar σ 2.
The posterior distribution from the NIG prior is obtained by combining the
likelihood in (3.2) and the priors speciﬁed in (3.4). This leads to the posterior
distribution
P (β|σ 2, y, X) ∼N{µ∗
β, V ∗
β },
P (σ 2|y, X) ∼IG(a∗, b∗),
(3.5)
where
µ∗
β = (V −1
β
+ XT X)−1(Vβµβ + XT )−1,
V ∗
β = (V −1
β
+ XT X)−1,
a∗= a + n
2,
b∗= b + 1
2

µT
β V −1
β µβ + yT y −µ∗
βV −∗1
β
µ∗
β


24
BAYESIAN ANALYSIS OF GENE EXPRESSION DATA
Further, the marginal posterior distribution of β can be obtained by integrating
out σ 2 from the NIG joint posterior as
P (β|X, y) =
	
P (β, σ 2|X, y)dσ 2,
which results in the marginal posterior distribution being a multivariate t-density
with degrees of freedom 2a∗, mean µ∗
β and covariance (b∗/a∗)V ∗
β . The probabil-
ity density function of a multivariate t-density with ν degrees of freedom, mean
µ and covariance V is given by
P (t|ν, µ, V ) = 
((ν + p)/2)
(ν/2)
πp/2|νV |1/2

1 + (t −µ)T V −1(t −µ))
ν
(ν+p)/2
.
All the information for modeling the behavior of the response as a function
of the covariates is contained in these posterior distributions. Apart from the
goal of understanding the behavior of the response variable y, given X, another
common objective of regression analysis is prediction of the response given a
new or future set of observations. Suppose we have observed a new m × p matrix
of regressors X, and we wish to predict the corresponding outcome y. Observe
that if β and σ 2 are known, then the probability law for the predicted outcomes
would be described as y ∼N(Xβ, σ 2) However, these parameters are not known
and information about them is summarized through our posterior distributions
in (3.5). Hence, all predictions for the data follow from the posterior predictive
distribution as
P (y|y) =
	
P (y|β, σ 2)P (β, σ 2|y)dβdσ 2,
which, using NIG(µ∗
β, V ∗
β , a∗, b∗) in (3.5) and after some algebra, can be shown
to follow a multivariate t-density with 2a∗degrees of freedom, mean Xµ∗
β and
variance (b∗/a∗)(I + XV ∗
β XT ). Notice that there are two sources of uncertainty in
the posterior predictive distribution. First, the fundamental source of variability in
the model due to σ 2, unaccounted for by Xβ, and second, the posterior uncertainty
in β and σ 2 as a result of their estimation from a ﬁnite sample y. It can be shown
that, as the sample size n →∞, the variance due to the posterior uncertainty
disappears, but the predictive uncertainty remains.
What we have discussed so far is a general setting for conducting Bayesian
inference on linear models by eliciting conjugate priors. As with any Bayesian
analysis, the amount of information one wishes to impart via the prior is in some
sense subjective. We discuss some other choices of priors below, varying by the
amount of information imparted to the posterior analysis.
A general class of noninformative priors are Jeffrey’s priors, which have a
close connection to the classical estimation theory. These priors can be obtained
by letting Vβ →0 (i.e. the null matrix, essentially no prior information) and

BAYESIAN LINEAR MODELS
25
a →−p/2 and b →0. This leads to the noninformative priors P (β) ∝1 and
P (σ 2) ∝σ −2 or, equivalently,
P (β, σ 2|X) ∝σ −2.
One can easily show that this prior corresponds to Jeffrey’s prior with respect
to the parameters (we leave it as an exercise for the reader). Note that the two
distributions above are not valid probabilities, since they do not integrate to any
ﬁnite number and hence are improper priors.2 However, the posterior distribution
is proper. It can be shown the the posterior distributions of β and σ 2 are
P (β|σ 2, y, X) ∼N

β, σ 2(XT X)−1
,
P (σ 2|y, X) ∼IG
(n −p)
2
, (n −p)σ 2
2

,
where β is the standard least squares estimator deﬁned above, IG is the
inverse-gamma distribution and σ 2 is as deﬁned in (3.3). There are striking
similarities between Bayesian estimation with Jeffrey’s prior and the classical
estimation. First, note that the posterior expectation of β is E[β|y, X] = β,
which is exactly the OLS estimate, and that the (conditional) variance is given
by σ 2(XT X)−1, which can be approximated by setting σ 2 = σ 2. Second, the
distribution of σ 2 is characterized as (n −p)σ 2/σ 2 following a chi-square
distribution.
A middle-ground solution between informative and noninformative priors can
be obtained by setting Vβ = c(XT X)−1 and P (σ 2) ∝σ −2, which is commonly
referred to as Zellner’s g-prior: a (conditional) Gaussian prior on β and improper
prior on σ 2. Note that it appears the the prior is data-dependent, but this is not the
case since the entire model is conditional on X. The constant c here is interpreted
as a measure of the amount of information available in the prior relative to the
sample. For example, setting 1/c = 0.5 gives the prior the same weight as 50%
of the sample.
3.2.2
Bayesian Variable Selection
In an ideal setting, when building a regression model, one should include all
relevant pieces of information available from the data. This includes, in the
regression context, all the predictor variables (X) that might possibly explain the
variability in the response variable y. In a gene expression data context we have
potentially hundreds of thousands of genes as predictor variables, which in almost
all cases is much greater than the number of samples/arrays, i.e. p ≫n. There
are several potential drawbacks when using a plain vanilla regression model with
many many predictors. First, the constraint p < n is of course violated when,
2Improper priors are the class of of priors which do not integrate to a ﬁnite number i.e.

(θ)dθ =
∞with respect to a σ-ﬁnite measure.

26
BAYESIAN ANALYSIS OF GENE EXPRESSION DATA
say, using noninformative priors. Second and more importantly, including such
a large number of predictors leaves little information available to obtain precise
estimators of the regression coefﬁcients with little gain in explanatory power.
As is often the case, most of the genes/probes are mostly ‘housekeeping’ genes
and have little effect on the response, and including them in our regression
model results in little or almost no gain in power while making it harder to
estimate the regression parameters. Thus it is important to be able to decide
which variables to include from this potentially large pool of genes in order to
balance good explanatory power with good estimation performance, a problem
classically referred to as variable selection.
The variable selection problem is usually posed as a special case of the
model selection problem, where each model under consideration corresponds
to a distinct subset of X = (x1, . . . , xp). In this section we discuss variable
selection approaches in the context of linear multiple regression with normal
likelihood speciﬁcations. Many problems of interest can be posed as linear
variable selection problems. For example, in nonparametric function estimation,
the values of the unknown function are represented by y, and linear bases such
as wavelets or splines are represented by X = (X1, . . . , Xp), The problem of
then ﬁnding a parsimonious approximation to the function is then the linear
variable selection problem.
Variable selection is essentially a decision problem, in which all potential
models have to be considered in parallel against a criterion that ranks them.
Formally, with p predictor variables we have potentially 2p models to choose
from, each having q predictor variables (including the null model with q = 0).
Formally, assuming the linear regression setup as in (3.1) with y ∼N(Xβ, σ 2I),
where X = (X1, . . . , Xp), β is p × 1 vector of unknown regression coefﬁcients,
and σ 2 is an unknown positive scalar. The variable selection problem then pro-
ceeds to identify subsets of predictors with regression coefﬁcients small enough
to ignore them. We shall describe below different Bayesian formulations of this
problem distinguished by their interpretation of how small a regression coefﬁ-
cient must be to ignore Xj, the jth predictor (or gene). A convenient way of
representing each of the 2p subsets is via the vector
γ = (γ1, . . . , γp)T ,
where γj = 1 or 0 according to whether βj is included or excluded from the
model. Thus qγ = p
j=1 γj indexes the dimensionality of the model implied by
the vector γ.
3.2.3
Model Selection Priors
For the speciﬁcation of the model space prior, most Bayesian approaches use
independence priors of the form
P (γ) =

j
ω
γj
j (1 −ωj)1−γj .

BAYESIAN LINEAR MODELS
27
Under this prior, each predictor Xj enters the model independently of the oth-
ers, with probability p(γj = 1) = ωj = 1 −P (γj = 0). The parameter ωj can
be viewed as the (prior) weight for variable Xj, with small ωj downweighting
predictors which are of little interest. Moreover, such priors are easy to specify
and can substantially reduce the computational burden since the resulting poste-
riors are often obtained in closed form. See, for example, Clyde et al. (1996),
George and McCulloch (1993, 1997), Raftery et al. (1997), and Smith and Kohn
(1996). A more useful reduction is obtained by setting ωi ≡ω, yielding
P (γ) = ωqγ (1 −ω)p−qγ ,
(3.6)
in which case the hyperparameter ω is the a priori expected proportion of Xis
in the model. Setting ω to a small number yields parsimonious models. Alter-
natively, one could assume a prior on ω. A popular choice is a beta prior,
ω ∼Beta(a, b), which yields
P (γ) = B(a + qγ , b + p −qγ )
B(a, b)
where B(a, b) is the beta function. A limiting case can be obtained by setting
ω = 1/2, giving P (γ) = 1/2p, which imposes a uniform prior, and puts most of
its mass near models of size qγ = p/2. More generally, one could simply put a
prior g(qγ ) on the model dimension,
P (γ) =
 p
qγ
−1
g(qγ ),
of which the above two are special cases. Note that under this prior the com-
ponents of γ are exchangeable but not independent, except in the special case
of (3.6).
3.2.4
Priors on Regression Coefﬁcients
Conditional on γ , the regression model (3.1) can now be written as
P (y|βγ , σ 2, γ) = N(Xγ βγ , σ 2I)
(3.7)
where Xγ is the n × qγ matrix wholes columns correspond to the γ th subset
of X1, . . . , Xp, βγ is a qγ -dimensional vector of unknown coefﬁcients, and σ 2
is the residual error variance. Note that the same σ 2 is shared by all potential
models indexed by γ, but this is something of a mathematical trick rather than
being justiﬁed by the model; the independence of σ 2 and γ allows convenient
posterior calculations. We now concentrate on eliciting priors on the regression
parameters βγ and σ 2.
The
most
commonly
applied
prior
form
for
this
setup,
especially
for high-dimensional problems, is the conjugate NIG prior discussed in

28
BAYESIAN ANALYSIS OF GENE EXPRESSION DATA
Section 3.2.1. Speciﬁcally, the prior consists of a conjugate qγ -dimensional
normal prior on βγ and an inverse-gamma prior on σ 2:
P (βγ |σ 2, γ) = N(µγ , σ 2Vγ ),
P (σ 2|γ ) ∼IG(a, b),
a, b > 0.
(3.8)
Note that this prior, coupled with the prior P (γ ) deﬁned in the previous section,
can be viewed as mixture prior on the elements of β. It implicitly assigns a
point mass at zero for the coefﬁcients that are not contained in βγ , and a normal
prior for the nonzero coefﬁcients. Such a setup has been used in various variable
selection settings (see Chipman et al., 2001) . Also one of the ﬁrst Bayesian
variable selection treatments of the setup in (3.7) was used by Mitchell and
Beauchamp (1988) who proposed the spike-and-slab priors, obtained by replacing
the point mass at zero prior by a normal distribution with a small variance. This
parametric variable selection setup was further extended by Ishwaran and Rao
(2003, 2005) for gene expression data, details of which are given in Section 3.4.
A desirable property of the prior speciﬁcation in (3.8) is that the full condi-
tionals of the regression parameters βγ and σ 2 given γ are available in closed
form. Furthermore, we can analytically integrate them out from the joint posterior
P (βγ , σ 2, γ|y) to yield the marginal density of the data in closed form,
P (y|γ) ∝|XT
γ Xγ + V −1
γ |−1/2|Vγ |−1/2(b + S2
γ )−(n+a)/2,
where
S2
γ = yT y −yT Xγ (XT
γ Xγ + V −1
γ )−1XT
γ y.
As
we
will
show
in
subsequent
chapters,
the
use
of
closed-
form
expressions
can
substantially
improve
posterior
evaluation
and
speed up Markov chain Monte Carlo (MCMC) calculations. What now remains
is the elicitation of the hyperparameters, µγ , Vγ , a, b which are crucial to
the performance of the Bayesian variable selection approach described above.
For small numbers of predictors sometimes prior subjective knowledge is
available to elicit these parameters (Garthwaite and Dickey, 1996). But for the
cases considered in this book, involving gene expression data, the number of
predictors is very large and in most cases little prior knowledge is available.
For the prior mean on the regression coefﬁcients µγ , it is common to set
µγ = 0, which corresponds to the standard Bayesian approaches to testing point
null hypotheses, where under the alternative the prior is typically centered at
the point null value. For choosing the prior covariance matrix, Vγ , a com-
mon speciﬁcation employed is Vγ = c(XT
γ Xγ )−1 or Vγ = cIqγ , where Iqγ is
an identity matrix of dimension qγ and c is a positive scalar. The former choice
Vγ = c(XT
γ Xγ )−1 serves to replicate the covariance structure of the likelihood,
and yields the common g-prior recommended by Zellner (1986). The latter
choice, Vγ = cIqγ , corresponds to the belief that a priori the components of βγ

BAYESIAN LINEAR MODELS
29
are conditionally independent. Having ﬁxed Vγ , c should be chosen large enough
so that the prior is relatively ﬂat over the region of plausible values of βγ . With
Vγ = c(XT
γ Xγ )−1, Smith and Kohn (1996) recommend c = 100 and report that
the performance was insensitive to values of c between 10 and 10 000. This prior
was also used by Lee et al. (2003) for variable selection in a classiﬁcation context
for gene expression data.
The above likelihood and prior setup, (3.7) and (3.8), allows for analytical
marginalization of β and σ 2 due to their conjugate formulations. Speciﬁcally,
we can integrate out β and σ 2 from the joint posterior P (β, σ 2, γ|y) to yield
the marginal distribution of the data which is proportional to P (y|γ), avail-
able in closed form. Speciﬁcally, the marginal distribution P (γ|rest) can be
obtained as
P (γ|·) ∝P (y|γ ) ∝P (γ|y)
With the above prior formulation and with the choice of Vγ = c(XT
γ Xγ )−1, it
can shown that
P (γ|rest) = (1 + c)qγ /2{b + yT y −(1 + 1/c)−1UT U}(n+a)/2P (γ)
where U = VT XT
γ y for upper triangular V such that VT V = XT
γ Xγ which can
be obtained by Cholesky’s decomposition. The availability of this posterior in
closed form considerably speeds up subsequent MCMC calculations.
3.2.5
Sparsity Priors
The key to inducing sparsity in the number of genes selected is how we model
the prior covariance matrix, Vβ, or in fact the elements on its diagonal, since they
correspond to the conditional variances of β. With a prior mean set to zero, small
variances essentially shrink the regression coefﬁcients to zero, thus inducing spar-
sity. Denote the prior on P (β) ∼N(0, Vβ), where 0 is a p-dimensional vector of
0s. Assume independence among the βs and hence write Vβ = diag(λ1, . . . , λp)
as a a diagonal matrix with λj denoting the variance of βj. Assigning different
choices of prior distributions to the λs generates different models with different
degrees of sparsity to select the number of genes used. We discuss three choice
of priors below.
The simplest conjugate prior formulation is obtained by setting λj = IG(a, b).
This model is equivalent to the automatic relevance determination model of Li
et al. (2002). The hyperparameters a and b are set such that the the variance of
λj is large. Assuming independence among the λj, the joint prior distribution is
given by
Vβ =
p

i=1
IG(a, b).

30
BAYESIAN ANALYSIS OF GENE EXPRESSION DATA
Another prior that induces sparsity is the Laplace prior on β, which, as
opposed to the Gaussian prior, sets nonrelevant regression coefﬁcients exactly
to zero, i.e. selection rather than shrinkage. The Laplace prior can be written
as a scale mixture of normals, and can be expressed as a zero-mean Gaussian
prior with an independently exponentially distributed variance. Speciﬁcally, if
P (βj|λj) = N(0, λj) and P (λj|γ ) = exp(γ ), where the exponential distribution
is parameterized as x ∼γ
2 exp( γ x
2 ), we obtain
P (βj|γ ) =
	 ∞
0
P (βj|λj)P (λj|γ )dλj.
Simple integral calculation yields
P (βj|γ ) = Laplace

0, 1
√γ

.
This is essentially a Bayesian version of the lasso method of Tibshirani (1996),
with an added ﬂexibility due the choices of multiple variances (λs) rather than
global penalty parameters as is done in classical lasso regression.
In the absence of any information regarding the regression coefﬁcients, a
noninformative prior is Jeffrey’s prior,
Vβ = |I(Vβ)|1/2 =
p

j
1
λj
,
where I(•) is Fisher’s information matrix. This prior has been shown to strongly
induce sparseness and yields good performance (Bae and Mallick, 2004).
Having armed ourselves with these different kinds of modeling strategies for
a Bayesian linear model, we now turn our attention to applying them to gene
expression data.
3.3
Bayesian Linear Models for Differential
Expression
The problem of gene detection has received a great deal of attention in
high-throughput gene expression analysis research. The goal of gene detection
is to obtain a candidate list of genes that are reliably differentially expressed
between biological populations. For instance, which genes are differentially
expressed between, say, disease and normal populations? Or, which genes are
differentially expressed between disease subtype populations? These are typical
of the questions investigators ask when posing the problem of gene detection
for a microarray gene screening analysis. The hypothesis to test, for each gene
g = 1, . . . , p, is that gene g is not differentially expressed between populations
(H0) against the alternative that gene g is differentially expressed between
populations (H1).

BAYESIAN LINEAR MODELS
31
Gene detection is not always the primary objective in a microarray study,
although it is often considered to be a noteworthy goal serving larger purposes
in microarray experiments. For example, the goal in a study can be to develop
a diagnostic predictor of disease, i.e. supervised classiﬁcation, or to identify
new disease subtypes, i.e. unsupervised classiﬁcation. In studies such as these,
obtaining a reliable list of candidate genes can lead to a better understanding of
the disease, or population of interest.
In gene-screening studies, the way in which a candidate list of genes is
obtained depends on the goals and costs of the study. Some studies are more
liberal, while others focus more attention on controlling error rates. There is a
large body of literature devoted to multiple testing, addressing these questions,
speciﬁcally aimed at gene detection (Efron and Tibshirani, 2002; Muller et al.,
2004; Dudoit et al., 2004; Storey et al., 2004). We will examine these and some
related topics in more detail in Chapter 4.
While there have been major advances in the microarray technologies used
for genome-wide screening studies, there is still much uncertainty in gene detec-
tion. Part of the reason for this uncertainty is related to variation attributed to the
technology. Another important component contributing to uncertainty is incom-
plete knowledge about the genomes we are interested in learning about. Note that
while the above hypotheses are stated for each gene, in principle the hypotheses
can be, and are very often, dependent. We address both of these issues more
fully, the former in Chapter 6 and the latter in Chapter 4.
The earliest notable quantitative research in microarray gene expression anal-
ysis emphasized the importance of properly accounting for variation due to the
technology, the goal being to remove any systematic error introduced by the
technology. A well-accepted assumption that has gained widespread importance
is that the total variation in gene expression experiments may be partitioned into
variation due to the technology and variation due to biology:
Total Variation = Technological Variation + Biological Variation
+ (Technological Variation)*(Biological Variation).
This expression includes a term for the interaction between technology and biol-
ogy. Poor designs, lacking adequate sample replication, proved to be fatal in
many early microarray studies, as the designs did not provide for estimation
of technological biases affecting the results. It is not enough to assume that
technological biases are negligible if they do exist. Microarray construction and
experimentation involve many steps, each of which is important to the ﬁnal prod-
uct. Some examples of systematic technological variation are: (1) spatial variation
on a single chip, (2) variation between replicate chips, (3) variation by day of
experiment, and (4) probe-to-probe variation due to the physics of DNA binding
afﬁnity (Bolstad et al., 2003; Zhang et al., 2003; Scharpf et al., 2006).
It is generally agreed that variation due to the technology should be accounted
for in the data analysis, although how to best achieve this is an open question.
Improperly accounting for systematic variations rooted in the technology could

32
BAYESIAN ANALYSIS OF GENE EXPRESSION DATA
be detrimental to an analysis. Data cleaning steps are considered fundamental
to learning from microarray experiments. Heuristic approaches have grown in
popularity, that is, performance of data cleaning and data modeling in separate
stages. As a result, methodologies have been proposed for either data cleaning
or gene detection, but seldom both. We discuss methods proposed for modeling
biological variation and technological variation jointly in Section 3.7.
The hypotheses above are stated for each gene separately. A common con-
cern in microarray analysis is that on a gene-by-gene basis, these hypotheses
can be, and very frequently are, dependent. Many of the methods proposed for
gene detection begin with the prior assumption of gene-wise independence. This
is an assumption made largely for mathematical convenience, as it is often the
case that the problem of estimating all gene-wise interactions, given a sample
of size M < N, is intractable. Incomplete knowledge of the genomes of many
species does indeed present challenges for microarray gene detection analysis.
Even if gene-wise independence cannot be justiﬁed on biological grounds, other
justiﬁcations are that variation attributable to gene-gene correlation is typically
overwhelmed by the measurement error due to the technology (Gold et al., 2005;
Reverter et al., 2005), and that historical studies relying on the independence
assumption, i.e. a univariate analysis, have produced useful results (Spurgers
et al., 2006). Multivariate methods have been proposed for microarray gene detec-
tion, largely concerned with statistical rather than biological properties, such as
controlling error rates or improving the power for estimating interactions. We
relax the assumption of gene-wise independence in Chapters 5–8. For now we
take the assumption of gene-wise independence as given, with the understanding
that our conclusions will be limited to our understanding of the biology.
3.3.1
Relevant Work
We discuss some of the more prominent historical research in gene detection
analysis in this section. Some of these models have a Bayesian ﬂavor although
not implemented as fully Bayesian. Note that the theoretical development in
many of these models began with a limited understanding of the technology.
Newton et al. (2001) proposed a gamma-gamma model for modeling gene
expression in two-channel microarray experiments. For a given probe, let R and
G be the observed levels of ﬂuorescence measured in the red and green channels
respectively, on one microarray. Following Newton et al.’s notation, T = R/G
is the red to green ratio, or fold change. The quantity of interest is ρ = µr/µg,
or the true relative fold change, where µr and µg are the respective mean levels
of ﬂuorescence in both the red and green channels. R and G are assumed to
follow gamma distributions, with scale parameters θr, θg and a common shape
parameter a. Newton et al. further conditioned the distribution of T on the product
S = R × G. The distribution is
P (t|s, θ, a) ∝1
t exp

−θs−1/2 
t1/2 + t−1/2
(3.9)

BAYESIAN LINEAR MODELS
33
where θ is the common value of θr and θg, leading to the commonly observed
relationship between the mean and variance in gene expression, i.e. that the
variance declines exponentially as the mean signal strengthens. The means are
assigned priors µr ∝1/θr and µg ∝1/θg. The posterior for ρ is derived as
P (ρ|R, G, a, a0, ν) ∝ρ(a+a0+1)
 1
ρ + (G + ν)
(R + ν)
2(a+a0)
,
(3.10)
and the Bayes estimator of ρ is
ˆρB = R + ν
G + ν ,
(3.11)
a shrinkage estimator, shrinking the red-green ratio to 1. Newton et al. used
empirical Bayes procedures to ﬁt their model. Differential expression is inferred
by the Bayes factor
odds = pA(r, g)
p0(r, g)
ˆp
1 −ˆp
(3.12)
or posterior odds, with predictive null and alternative densities p0(r, g) and
pA(r, g). Prior parameters are estimated empirically, by modal values with an
EM algorithm. Extensions of Newton’s method have been proposed in Lo and
Gottardo (2006).
While Newton et al. offered an important advance in modeling gene expres-
sion, serious challenges remained for gene detection. Microarray data exhibits
a high level of noise relative to signal, and as a consequence the results are
often poorly reproduced. As a result, attention turned to robust methods. Kerr
et al. (2000) focused on microarray study designs and linear models for parti-
tioning total probe variation. This was largely in response to concerns about the
technology–biology interaction. Kerr et al.’s model, for two-channel experiments,
included an interaction term for dye and effect:
log(ygijk) = µ + Ai + Dj + Vk + Gg + (AG)gi + (V G)gk + ϵgijk
(3.13)
where ygijk is the expression for the gth gene in the ith array with the jth
dye in the kth experimental group. Here µ is mean gene expression, Ai is the
ith array effect, Dj is the jth dye effect, Vk is the overall effect of the kth
experimental group, Gg is gene g’s effect, (AG)gi is the array i by gene g inter-
action term, (V G)gk is the experimental group by gene interaction term, and
ϵgijk is noise assumed to be independently and identically distributed (i.i.d.)
with zero mean. Statistical inference, on, say, the experimental group–gene
interaction term, H0 : (V G)g1 −(V G)g2 = 0, is performed by the bootstrap, by
resampling with replacement the ﬁtted residuals, ˆϵgijk, and comparing the ﬁtted
differences estimated from the data, (
V G)g1 −(
V G)g2, with the bootstrap dis-
tribution (V G)∗
g1 −(V G)∗
g2. Bootstrap approaches offer advantages with noisy

34
BAYESIAN ANALYSIS OF GENE EXPRESSION DATA
array measurements, although they can be computationally inefﬁcient, depending
on the objectives of the study, requiring many repeated ﬁts of the data.
An alternative robust method requiring less computation involves permutation
testing. One such method proposed for gene detection is signiﬁcance analysis of
microarrays (SAM). The SAM statistic is deﬁned as
d(g) = ¯xI(g) −¯xU(g)
s(g) + s0
(3.14)
for ¯xI(g) and ¯xU(g), the sample mean gene expression levels in states I and U.
In the denominator, s(g) is given by
s(g) =



a

m
[xm(g) −¯xI(g)] +

n
[xn(g) −¯xU(g)]

(3.15)
for a = (1/n1 + 1/n2)/(n1 + n2 −2), and s0 > 0 is a user-deﬁned constant
included in the denominator to protect against underinﬂation of the scale. To
detect signiﬁcant changes in gene expression, balanced permutations of the
samples are performed, by relabeling the states I and U, and recalculating
(3.14), as dp(g) for p = 1, . . . , P permutations. One rationale for permutation
testing is that only a moderate number of sample replicates are required to
obtain a reasonable number of permutations. In Tusher et al.’s (2001) original
analysis with SAM, only four sample replicates were available, allowing 36
permutations in all. Unlike univariate methods, the authors claim that SAM is
robust to the independence assumption between genes. Another approach, via a
mixture model formulation and fully Bayesian method, is explored by Lewin
et al. (2007).
3.4
Bayesian ANOVA for Gene Selection
This section deals with Bayesian analysis of variance models and extensions to
analyze gene expression data. ANOVA models are extremely popular and pow-
erful, while being conceptually simple and intuitive. The past few years have
seen a host of ANOVA models and extensions applied to microarray data3 espe-
cially in the Bayesian paradigm. A key feature of these models is that they
lend themselves nicely to a hierarchical Bayesian modeling framework in which
the variability in the gene expression data can be modeled at various levels.
This framework refers to a generic model building strategy, in which data (and
unobserved variables) are organized into a small number of discrete levels with
logically distinct and scientiﬁcally interpretable functions and probabilistic rela-
tionships between them that capture the inherent features of the data. Appendix
A deals extensively with hierarchical models, and the reader is referred to that
3We will use the terms microarray data and gene expression data interchangeably throughout
this book, while referring to the same data structure.

BAYESIAN LINEAR MODELS
35
chapter for the fundamentals of the model building processes. The two main
advantages of this framework are sharing of information across parallel units
and propogation of uncertainity through the model, which we will illustrate in
the subsequent sections of this chapter. For basic concepts and examples of hier-
archical modeling and borrowing strength across units we refer to the readers to
Section A.4.
As already mentioned, there is a huge literature on the use ANOVA models
and their variations/extensions for gene expression data, depending on the basic
scientiﬁc question one wishes to address – normalization, differential expression,
feature selection, etc. The basic models extend from the log-linear model for gene
expression and its extensions to more complicated settings. An outline of all the
possible models is outside the scope of this chapter, but we attempt to cover a few
of the most popular basic ones while referring the reader to appropriate sources
for additional models. For a brief overview of ANOVA models for microarrays
(more from a frequentist perspective), see Lee (2004).
ANOVA modeling is particularly attractive for high-dimensional data such as
microarrays for a number of reasons. ANOVA models can be easily reparame-
terized as linear regression models and can borrow on the extensive machinery
already in place to analyze such models such as mixed effect models and semi-
or nonparametric regression tools. ANOVA modeling also has a strong basis in
normal theory and thus can be applied in a variety of settings, making it a very
powerful technique for applications. These features make these models extend-
able to complicated settings and this, combined with the hierarchical Bayesian
modeling machinery, gives rise to a large class of appealing models, depending
on the basic scientiﬁc questions being addressed. We will describe some of these
models below.
3.4.1
The Basic Bayesian ANOVA Model
We will start with a basic one-way ANOVA model for microarray data. Let yg
be the response (expression) for single gene g, given by
yg = XT βg + ϵg
(3.16)
where XT is an (often) ﬁxed design matrix of covariates. For example, XT could
be a matrix of indicator variables for j = 1, . . . , K treatments (often K = 2 in
marker studies). The errors ϵg account for all other sources of variation and are
assumed to be normally distributed as N(0, σ 2
g ). As in classical ANOVA models,
we assume σ 2
g = σ 2 for all g. Of primary interest is the vector of regression coef-
ﬁcients, βg = (β1g, . . . , βKg), characterizing the behavior of a particular gene,
g, across all treatments. Depending on the problem at a hand, a wide class of
priors, both informative and noninformative, can be adopted for βg and σ 2 as
we described in Section 3.2.

36
BAYESIAN ANALYSIS OF GENE EXPRESSION DATA
Although very simple, the above model is admittedly naive, especially for
such complex data structures as microarrays. First, the genes are assumed inde-
pendent of each other, which we know is biologically implausible, since genes
in same regulatory pathway interact with each other. Second, the assumption of
constant variance of genes is very restrictive since gene expressions vary con-
siderably. Both of these shortcomings can be easily met by the extending the
model in (3.16) to more realistic settings. This can be done in a simple and ele-
gant manner via the hierarchical Bayesian modeling machinery, as we will show
below.
3.4.2
Differential Expression via Model Selection
In this section we discuss the Bayesian ANOVA for microarrays (BAM) method
of Ishwaran and Rao (2003, 2005). The authors propose an extension of the
ANOVA model to detect differential expression in genes within a model selec-
tion framework. The BAM approach uses a special inferential regularization
known as spike-and-slab shrinkage that provides an optimal balance between
total false detections and total false non-detections. The method provides an efﬁ-
cient way to control the false discovery rate (FDR) while ﬁnding a larger number
of differentially expressed genes. See Chapter 4 for details on multiple testing
and controlling the FDR. The problem of ﬁnding differentially expressed genes
is recast as determining which factors are signiﬁcant in a Bayesian ANOVA
model. This is done using a parametric stochastic variable selection procedure
ﬁrst proposed by Mitchell and Beauchamp (1988), via the hierarchical model
Yi|Xi, β, σ 2 ∼N(XT
i β, σ 2),
i = 1, . . . , n,
βg|γg, τ 2
g ) ∼N(0, γgτ 2
g ),
g = 1, . . . , G,
γg|λg ∼(1 −λg)δγ ∗(·) + λgδ1(·),
λg ∼U(0, 1),
τ −2
g |a1, a2 ∼Gamma(a1, a2),
σ −2|b1, b2 ∼Gamma(b1, b2).
where Yi is the response/gene expression, Xi is the G-dimensional covariate
with β as the associated regression coefﬁcients and σ 2 the measurement error.
The key feature in this model is that the prior variance ν2
g = γgτ 2
g on a given
coefﬁcient βg has a bimodal distribution, which is calibrated via the choice of
priors on τ 2
g and γg. For example, a large value of ν2
g occurs when γg = 1 and
τ 2
g is large, thus inducing a large values for βg, indicating the covariate could be
potentially informative. Similarly, small values of ν2
g occur when γg = γ ∗(ﬁxed
to a pre-speciﬁed small value), which leads to shrinkage of βg.

BAYESIAN LINEAR MODELS
37
Under the above model formulation, the conditional posterior mean of β is
E(β|ν2, σ 2, Y) = (σ 2
−1 + XT X)−1XT Y,
where 
 = diag(ν2
1, . . . , ν2
G), τ 2 = (τ 2
1 , . . . , τ 2
G) and Y = (Y1, . . . , Yn). This is
the (generalized) ridge regression estimate of Y on X with weights σ 2
−1.
Shrinkage is induced via the small diagonal elements of 
, which are determined
by the posteriors of γ , τ 2 and λ.
This variable selection framework is then extended to microarray data via
an ANOVA model and its corresponding representation as a linear regression
model. The two-group setting is discussed in Ishwaran and Rao (2003) and the
multigroup extension is proposed in Ishwaran and Rao (2005). For purposes
of illustration, we present the the two-group setting here. For a group l = 1, 2,
let Ygil denote the gene expression from array/individual i = 1, . . . , ngl of gene
g = 1, . . . , G. We are then interested in identifying differentially expressed genes
between two groups, say, control(l = 1) versus treatment (l = 2). To this end,
the ANOVA model can then be written as
Ygil = θg0 + µg0I{l = 2} + ϵgil,
(3.17)
where the errors ϵgil are assumed i.i.d. N(0, σ 2). θg0 models the mean of the
gth gene in the control group. In this model those genes that are differentially
expressed correspond to µg0 ̸= 0, i.e. turned on or off depending on the sign
on µg0.
A series of transformations of the data are required before the model in (3.17)
can be ﬁtted. There are two primary transformations: centering and rescaling the
data. They transformed data used for downstream analysis are
Ygil = (Ygil −¯Yg1)

n/σ 2n,
(3.18)
where
σ 2
n = (n −p)−1 
gil
(Ygil −¯Yg2I{l = 2} −¯Yg1I{l = 1})2
is the usual unbiased (pooled) estimator of σ 2
0 , n = p
g=1 nj is the total number
of observations, ¯Xgl is mean of group l. The effect of centering is twofold: it
reduces the number of parameters, hence the effective dimension, in (3.17) from
2p to p and also reduces the correlation between the model parameters θg and µg.
The effect of rescaling is to force the variance σ 2 to be approximately equal to
n, and to rescale the posterior mean values so that they can be directly compared
with a limiting normal distribution. Finally, the transformed model that is ﬁtted
to the data is
Y = 
XT 
β0 +ϵ,

38
BAYESIAN ANALYSIS OF GENE EXPRESSION DATA
where Y is a vector of expression values obtained by concatenating the values
Ygil in (3.18) in a vector, β0 are the new vector regression coefﬁcients containing
(θg, µg) at alternate positions, and ϵ is the vector of measurement errors. 
X is
the rescaled n × 2p design matrix such that the second moments are equal to 1.
It is deﬁned in such a manner that its 2g −1 columns consist of 0s everywhere
except for values n/ng placed along the ng rows corresponding the gene g,
while column 2g consists of of 0s everywhere except for values n/ng1 placed
along the ng2 rows corresponding the gene g for group 2.
As a consequence of the simple construction of the design matrix 
X, the
conditional distribution of β can be explicitly obtained. The conditional mean
for µg is then (approximately) equal to (√ng2/σ 2
n)( ˆYg2 −ˆYg1). with variance
approximately equal to 1. Testing whether µg is nonzero is then conducted by
comparing its values to a N(0, ng/ng1) distribution called the Zcut procedure
for differential expression.
3.5
Robust ANOVA model with Mixtures of Singular
Distributions
We have seen the use of Bayesian ANOVA models to identify differentially
expressed genes under different conditions using gene expression microarrays.
However, there are many complex steps involved in the experimental process,
from hybridization to image analysis, hence microarray data often contain out-
liers. These outlying data values can occur due to imperfections in the experiment
or experimental equipment. Gottardo et al. (2006) developed Bayesian robust
inference for differential gene expression (BRIDGE) using novel hierarchical
models.
The error distribution of the ANOVA models for gene expression data may
not follow a normal distribution and ﬂatter-tailed distributions are required to
capture the outliers. BRIDGE follows a robust approach using a t error distri-
bution rather than a normal distribution. Consider a simple situation where we
are comparing two independent groups (control and treatment, say, or cancer and
healthy conditions) and there are J replicated observations for each gene within
each group. A basic ANOVA model can be developed for such data, following
the notation of the previous sections, as
ygij = µgi +
ϵgij
√wgij
,
for g = 1, . . . , G; i = 1, 2; j = 1, . . . , J,
where g indexes the genes, i the group (there are two groups), and j the replica-
tions, µgi is the mean expression for the gth gene for the ith group, and ϵgij is
the random error. The t distribution for the error has been introduced by assum-
ing ϵgij|σgi2 ∼N(0, σgi2) and wgij|νj ∼Gamma(νj/2, νj/2) where wgij and ϵgij
are independent. In additional to the basic ANOVA assumptions, it has been also
assume that the error variances are different for the genes and groups but the same

BAYESIAN LINEAR MODELS
39
for the replications, whereas the number of degrees of freedom for the t distribu-
tion is same for the all the genes and groups but may change among replicates.
An inverse-gamma prior is assigned for the error variance parameter σ 2
gi.
µgi is the effect of group i on gene g, and the gth gene is not differen-
tially expressed between the two groups if these effects are equal (µg1 = µg2).
Gottardo et al. (2006) developed a mixture prior for µgi to use this fact to iden-
tify signiﬁcant genes. Deﬁning µg = (µg1, µg2) and λ = (λ1, λ2, λ12), then the
mixture prior is
µg|λ, π ∼(1 −π)N(µg1|0, λ−1
12 )I[µg1 = µg2]
+ πN(µg1|0, λ−1
1 )N(µg2|0, λ−1
2 )I(µg1 ̸= µg2),
where N(µg1|0, λ−1
12 ) means that µg1 follows a zero-mean normal distribution
with variance λ−1
12 . The ﬁrst component corresponds to the genes that are not
differentially expressed so µg1 = µg2 so for the particular gene g, and the two
groups share the same variance (that way it can borrow strength). Likewise,
the second component corresponds to the genes that are differentially expressed
(µg1 ̸= µg2) so we assume independent normal priors for these two compo-
nents with separate variances. Conjugate gamma priors are assigned for λs. The
hierarchical model is
ygij|µgi, σ 2
gj, wgij ∼N(µgi, σ 2
gj/wgij),
σ 2
gj ∼IG(ajo, bjo),
wgij ∼Gamma(νj/2, νj/2),
µg|λ, p ∼MixNorm(0, λ, p),
λ ∼Gamma(·)
π ∼U[0, 1].
(3.19)
Gibbs sampling is required to obtain realizations from the posterior distribution.
Due to assignment of conjugate priors, all the complete conditionals except for
µ are in explicit form using linear model theory (see Section 3.2). To obtain the
complete conditional distribution for µ, we need to use the results for the mixture
distribution from the example A.3 in the Appendix A. If the prior is a mixture
distribution then the posterior will again be a mixture distribution with updated
components. In this situation it is easy to show using our previous results that
the complete conditional distribution (conditioned on all the parameters and the
vector of responses y) for µg is
µg|y, λ∗, π ∝(1 −π)N(µg1|θg, λ∗
g
−1)I[µg1 = µg2]
+ πN(µg1|θg1, λ∗
g1
−1)N(µg2|θg2, λ∗
g2
−1)I(µg1 ̸= µg2).

40
BAYESIAN ANALYSIS OF GENE EXPRESSION DATA
The conditional posterior precisions (inverse of variance) λ∗
g = 
g,i,j wgij
τgj + λ12 and λ∗
gi = τgi

j wgij + λi where τ is the precision (1/σ 2) of the error
distribution. The conditional posterior means are θg = λ∗
g
−1 
ij wgijτgjygij and
θgi = λ∗
gi
−1 
i wgijτgjygij. To simulate µg from this conditional distribution, we
can use a MH algorithm or direct Gibbs sampler as discussed in Appendix B.
After convergence of the MCMC chains we obtain the posterior samples of
the parameters. From the posterior output we can compute the marginal posterior
probability of differential expression of gene g, namely P (µg1 ̸= µg2|y). For
each gene g, This probability is computed for each gene g, and Monte Carlo
samples of µ are used to estimate it. With B posterior samples, we use the
relative frequency formula
1
B
 I[µg1(l) ̸= µg2(l)], where µg1(l), µg2(l) are the
values generated at the lth iteration of the MCMC and I(·) is the indicator
function which is 1 when µg1(l) ̸= µg2(l).
The method can be extended for paired samples (where the conditions are
related) and the details are given in Gottardo et al. (2006).
3.6
Case Study
We demonstrate the BAM methodology on the lung cancer Affymetrix microar-
ray data set of Wachi et al. (2005). The data consist of expression values for
22 283 genes collected from ten patients, ﬁve of whom had squamous cell carci-
noma (SCC) of the lung and ﬁve were normal patients. The data set is available
for download at http://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc = GSE3268.
The microarray data are normalized using the RMA method (see Section 2.2.3)
in R using affy. We refer the reader to Wachi et al. (2005) for further details
on the pre-processing of the data.
We used the BAMarray 2.0 academic edition software available for download
at http://www.bamarray.com/ (see Ishwaran et al., 2006, for further details on
BAMarray). We used the default options built into the software for the entire
analysis. The BAM method assumes constant variance across the groups and an
automatic CART based clustering approach to variance stabilization built within
the software. The adequacy of the transformation can be visualized via a V-plot
(Ishwaran and Rao, 2005) as shown in Figure 3.1. If the variances have stabilized
to values near 1, then plotting the group mean difference for a gene versus the
corresponding absolute value of the t-statistic should give a plot with a line
having constant slope. These theoretical lines are the ones shown as dotted black
lines on the V-plot and the clustering of differences along these lines represents
the appropriateness of the transformation.
Figure 3.2 shows the shrinkage plot for the lung cancer data set. Plotted are the
posterior variances on the vertical axis and the Zcut values on the horizontal axis.
As demonstrated in Ishwaran and Rao (2005), genes that are truly differentially
expressed will have posterior variances converge to 1 on the far right and left of
the plot. The cutoff values are determined in a data-adaptive manner by balancing

BAYESIAN LINEAR MODELS
41
24.0
20.0
16.0
12.0
8.0
4.0
0.0
Abs Value for t-test
−10
0
10
20
Group Mean Difference
V-Plot Diagnostic
Tumor vs Normal
Turned On (2043)
Turned Off (2213)
Not Sig (18027)
Figure 3.1
V-plot of the tumor versus normal comparison for the lung cancer
data.
10
−10
−20
0
0.20 0.60 1.00 1.40 1.80 2.20 2.60 3.00
20
30
Zcut
Turned On (2043)
Turned Off (2213)
Not Sig (18027)
Posterior Variance
Tumor vs Normal
Shrinkage Plot
Figure 3.2
Shrinkage plot for determining differentially expressed genes for
tumor groups relative to the normal group for the lung cancer data.

42
BAYESIAN ANALYSIS OF GENE EXPRESSION DATA
the total false detections against total false non-detections. It can be seen that
out of the 22 383 genes: 2043 genes are turned on (up-regulated), 2213 genes
are turned off (down regulated) and 18 027 genes are found not to be signiﬁcant.
There are thus 4256 differentially expressed genes in total.
Figure 3.3 shows the posterior probabilities from the BRIDGE method using
t-distributed errors. Plotted are the posterior probabilities versus the log-posterior
differences, log(γ1 −γ2), between the two groups. Notice how most of the
log-ratios are shrunk towards zero and hence have very low posterior probability
of differential expression. The BRIDGE analysis found 1982 genes differen-
tially expressed at the 0.5 threshold for the posterior probability, which was the
value the authors used in the paper. In addition, we found 1259 at 0.9, 2507
genes at 0.25, 3873 at 0.05 thresholds, as the number of differentially expressed
genes. Comparing with the BAM method, we found 1741 (41%), 1191 (28%),
2101 (49%) and 2925 (69%) overlap between the number of genes that were
differentially expressed. Using t-tests we found more than 10 000 genes that
were differentially expressed, thus the Bayesian methods found a substantial
reduction in the number of differentially expressed genes. This demonstrates the
Posterior Probability
1.0
0.8
0.6
0.4
0.2
0.0
Posterior probability
−0.5
0.0
0.5
log (g1–g2)
Figure 3.3
Posterior probabilities from the BRIDGE method. Plotted are the
posterior probabilities on the vertical axis versus the log-posterior differences,
log(γ1 −γ2), between the two groups.

BAYESIAN LINEAR MODELS
43
effectiveness of the model-based Bayesian shrinkage methods, in which the pos-
terior means are shrunk towards zero, while maintaining nominal Type I error
rates. The corresponding gene-lists can be obtained from the companion website
for this book. See Chapter 4 for a detailed analysis using frequentist and Bayesian
FDR techniques.
3.7
Accounting for Nuisance Effects
There are many sources of signals in gene expression measurements. The signal
reﬂection from the background of the array can contaminate the spot signal. A
spot with signal below background haze is considered for practical purposes to be
a measurement of noise. Background subtraction methods have been proposed,
although the sensitivity to background correction is not completely understood
(Scharpf et al., 2006). Low signal-to-noise ratios are commonly observed on a
probe-by-probe basis in microarray analyses. If one partitions total signal into
two sources,
Observed Spot Signal = True Spot Signal + Background Spot Noise,
then the goal of signal detection is to classify spots as measurements of signal
from gene expression or measurements of noise, and estimate and remove the
signal attributed to noise. The signal attributed to background is in this case
considered to be a nuisance parameter, since what we want to learn about is
biology. An approach similar to hard thresholding compares signal relative to
an estimate of the noise in (3.7) with a user-deﬁned threshold. A gene is called
‘expressed’ if the measured signal/noise is above the threshold, and unexpressed
otherwise. According to this approach, genes that are consistently unexpressed
are removed from the analysis, as are genes exhibiting consistently weak sig-
nal. Affymetrix produces a detection probe-wise detection p-value with their
Microarray Suite software, see their White Paper (www.affymetrix.com).
Ibrahim et al. (2002) proposed a truncated model for gene expression with a
component for signal detection,
xg =

co,
with probability πg,
co + yg,
with probability 1 −πg,
where co > 0 is the threshold below which xg is unexpressed. The constant co
is a user-deﬁned minimum signal threshold. The prior probability that gene g is
expressed is πg = P (xg > co). If gene g is expressed, then xg has a truncated
distribution, where co is the lower bound and yg is the continuous part, assumed
to follow a lognormal distribution.

44
BAYESIAN ANALYSIS OF GENE EXPRESSION DATA
The likelihood of gene expression for gene g, ygij, conditional upon individual
i and tissue type k, is
P (ygik|µgk, σ 2
gk) = (2π)−1/2y−1
gikσ −1
gk
× exp

−1
2σ 2
gk
(log(ygik) −µgk)2

.
The mean µgi is assigned a normal prior
µgk|mk0, σ 2
gk ∼N(mk0, τoσ 2
gk/¯nk),
(3.20)
where ¯nk is interpreted as the total number of expressed genes across all samples
in tissue type k, divided by the total number of gene expression measurements
across all probes and samples. Independent inverse-gamma priors are assigned
to the σ 2
gk. Ibrahim et al. derived the correlation between µgk and µg′k from the
joint posterior
(µgk, µg′k) ∼N2(µ∗, ∗)
with µ∗= (mko, mko)′ and
∗=


τoσ 2
gk
¯nk
+ ν2
ko
ν2
ko
ν2
ko
τoσ 2
g′k
¯nk
+ ν2
ko

,
implying that corr(µgk, µg′k|σ 2
gk, σ 2
g′k, νok) →1 as ¯nk →∞or ν2
ko →∞.
Their method of gene detection is based on the posterior distribution of ξg =
gk/gk′, where
gk = copgk + (1 −pgk)(co + E(ygik|µ, σ 2)).
The details of gene selection are rather involved, using what the authors call an
L measure. The interested reader is referred to Appendix A.1.
In Chapter 1 we discussed the problem of array-to-array variation in detail.
Methods for array normalization are largely heuristic (Bolstad), although joint
modeling has been proposed. Some of the statistical methods for normalization
differ depending on the technology, i.e. based on different assumptions about the
underlying process that led to array-to-array disparity. A basic extension of the
linear model for array normalization in single channel experiments is
ygik = αg + βgk + fik(αg) + ϵgik,
(3.21)
where αg is the mean level of gene expression for gene i, βgk is the experimental
effect of condition k on gene g, and ϵgik is residual noise, which has mean zero
and variance σ 2
g . The term fik(αg) is assumed to be a smooth function of αg, a
nuisance effect due to technological variation between the arrays. In one-channel
experiments there is disagreement as to how array-to-array differences arise,

BAYESIAN LINEAR MODELS
45
although a general observation is that differences between the distributions of
signal between arrays is inevitable.
A similar model may be adopted for two-channel experiments, where con-
dition k indexes the channel and fik(αg) is a channel-array dependent trend. In
two-channel experiments, fik(αg) is generally assumed to be the result of dispar-
ities in the measurements between the channels unrelated to biology, e.g. differ-
ences in the dynamic ranges of the ﬂuorescent dyes, or related to differences in the
manner in which the respective channels were processed, e.g. laser gain setting.
In some cases fik(αg) is simply assumed to be linear. A very general assump-
tion is that fik(αg) belongs to a class of smooth functions, including both linear
and nonlinear functions. The heuristic approach is to ﬁrst estimate the func-
tion fik(αg) on each array, 
fik(αg), and then analyze the transformed response,
ygik −
fik(αg). A valid criticism of the heuristic approach is that real differences
in the βgj can distort estimation of fik(αg), and therefore real experimental effects
can be smoothed out in estimation and removal of array effects, fik(αg). It is
argued that joint estimation of βgk and fik(αg) does not suffer from this prob-
lem, at least not to the same degree as the heuristic approach. Iterative methods
have been proposed (e.g. Huang et al., 2005; Fan et al., 2006) for two-channel
experiments. In principle, these methods could be applied to one-channel array
experiments.
Lewin et al. (2006) offer a fully Bayesian solution to the problem of estimating
(3.21). The function fik(αg) is assumed to be a linear function of second-order
spline basis functions, depending on the intercept terms αg, g = 1, . . . , N:
fik(αg) = b(0)
ik + b(1)
ik (αg −a0) + b(2)
ik (αg −a0)
+S
s=1b(2)
iks(αg −aiks)2I[αg ≥aiks].
(3.22)
Here the aiks are unknown knot points, while the bs are unknown polynomial
spline coefﬁcients, for an assumed ﬁxed number of knots S. Model (3.22) as
speciﬁed is not identiﬁable. Lewin et al. normalize within each experimental
condition, such that K
k=1 fik(αg) = 0 for all g, i, k. The authors place con-
strained uniform priors on the knot points and intercept terms, αg. Vague normal
priors are assigned to the coefﬁcients. The authors considered different priors
for the error variance, σ 2, a lognormal prior and a gamma prior, and compared
modeling assumption (3.22) versus a piecewise constant model
fik(αg) =
S

s=1
biks × I(as < αg ≤aa+1),
(3.23)
for as < aa+1. Note that several very important assumptions are required in order
for the model to be identiﬁable, and to eliminate confounding between differential
gene effects, δg, and channel-array effects, fik(αg). The channel-array effects
must sum to zero, 
ik fik(αg) = 0, across replicates and conditions, for each
gene g, and control genes (often referred to as housekeeping genes), for which

46
BAYESIAN ANALYSIS OF GENE EXPRESSION DATA
δg = 0, must be known over a range of expression values. Care must be taken,
as failure to achieve both of these requirements can lead to confounding between
the differential gene and channel-array effects. With this in mind, the example
below demonstrates use of Lewin et al.’s should be at their supplementary website
algorithm.
Example 3.1
Two-channel microarray data were simulated, for g = 1, . . . , 300 genes and k =
1, . . . , 5 replicates. Channel-array effects, fa(αg) and fb(αg), were maintained
the same for all replicates, between channels a and b respectively; see Figure 3.4.
Notice that for increasing mean intensity αg, array effects are concave increas-
ing functions, by channel, similar to the intensity-dependent saturation docu-
mented in two-channel experiments. Two simulations were performed, with and
without true differential gene effects. Figure 3.5 illustrates the posterior sum-
maries from the simulation without true differential gene effects. Differences in
the channel-array effects are largely captured in the posterior means. Following
the criterion, P (δ < −1 ∪δ > 1|Y) ≥0.80, no genes show strong evidence of
differential expression.
Figure 3.6 illustrates the posterior summaries from the simulation with true
differential gene effects. Gene with intensity 5 ≤α ≤10, excluding the control
15
10
5
0
0
5
10
15
a
(a)
fa(a)
15
10
5
0
0
5
10
15
a
(b)
fb(a)
15
10
5
0
0
5
10
15
fa(a)
(c)
fb(a)
0.2
0.0
−0.2
−0.4
0
5
10
15
a
(d)
fa(a)–fb(a)
Figure 3.4
Array effects. (a) Channel a effect by mean α. (b) Channel b effect
by mean α, (c) Channel a effect versus channel b effect. (d) Difference in channel
effects, fa(α) −fb(α) by mean α.

BAYESIAN LINEAR MODELS
47
3
2
1
0
−1
−2
−3
ya−yb
0.5 (ya+yb)
(a)
0
5
10
15
3
2
1
0
−1
−2
−3
(c)
0
5
10
15
1.0
0.5
0.0
−0.5
−1.0
fa(a)–fb(a)
a
(b)
0
5
10
15
ya−yb−(fa−fb)
^
^
3
2
1
0
−1
−2
−3
(d)
0
5
10
15
d^
a^
a^
Figure 3.5
Array effects. (a) MA plot for one representative sample. Superimposed is the true difference in array effects,
fa(α) −fb(α), and in gray the posterior mean difference in array effect ˆfa(ˆα) −fb(ˆα) by posterior mean gene effect ˆαg.
(b) The true difference in array effects, fa(α) −fb(α), is shown in black, and the posterior mean differences in array effects
ˆfa(ˆα) −fb(ˆα) are shown in gray for all ﬁve replicates. (c) MA plot for one representative sample, with the posterior mean
difference in array effect, ˆfa(α) −fb(α), removed. (d) Posterior mean and 90% conﬁdence interval for differential gene
effects δg by posterior mean gene effect ˆαg.

48
BAYESIAN ANALYSIS OF GENE EXPRESSION DATA
3
2
1
0
−1
−2
−3
ya−yb
0.5 (ya+yb)
(a)
0
5
10
15
3
2
1
0
−1
−2
−3
(c)
0
5
10
15
1.0
0.5
0.0
−0.5
−1.0
fa(a)–fb(a)
a
(b)
0
5
10
15
ya−yb−(fa−fb)
^
^
3
2
1
0
−1
−2
−3
(d)
0
5
10
15
d^
a^
a^
Figure 3.6
Array effects. (a) MA plot for one representative sample. Superimposed is the true difference in array effects,
fa(α) −fb(α), and in gray the posterior mean difference in array effect ˆfa(ˆα) −fb(ˆα) by posterior mean gene effect ˆαg.
(b) The true difference in array effects, fa(α) −fb(α), is shown in black, and the posterior mean differences in array effects
ˆfa(ˆα) −fb(ˆα) are shown in gray for all ﬁve replicates. (c) MA plot for one representative sample, with the posterior mean
difference in array effect ˆfa(α) −fb(α) removed. Genes discovered for change are “crossed”. (d) Posterior mean and 90%
conﬁdence interval for differential gene effects δg by posterior mean gene effect ˆαg.

BAYESIAN LINEAR MODELS
49
genes, were given random differential gene effects following a standard nor-
mal δg ∼N(0, 1). Of the 89 simulated genes with true differential effects, 20
(22.47%) were discovered, following the criterion P (δ < −1 ∪δ > 1|Y) ≥0.80.
Their genes are circled in red in Figure 3.6. Note that several of the differentially
expressed genes have point estimates of mean intensity above 10.
Another important nuisance effect discussed in the literature is the
within-array
spatial
variation.
Background
subtraction
is
one
approach,
discussed above, for accounting for spatial variation, although systematic trends
in spatial effects have been attributed to certain technologies, e.g. print tip
technology (Geller et al., 2003). A generalization of (3.22) is to include an
additional function g(s(g)) in
ygik = αg + βgk + fik(αg) + g(s(g)) + ϵgik
(3.24)
to model spatial effects, given the location map s(g) of probe or gene g on the
array. The function g(s(g)) can be very general, or in the special case of an
associations with print tips, can be regarded as linear,
g(s(·)) = Zθ
(3.25)
where Z is an N × P association matrix, with 1s in the respective rows of
column p for genes printed with print tip p and 0s otherwise. The vector θ
is assumed to be multivariate normal, with covariation matrix . In this way,
random associations are modeled between gene measurements by print tip blocks.
3.8
Summary and Further Reading
Our aim here is to demonstrate how simple ANOVA linear models can be
extended in a straightforward manner to deal with gene expression data via hier-
archical modeling. The resulting class of models is rich and leads to a coherent
inferential framework for the basic scientiﬁc question being addressed. We have
outlined just some of a host of other methods available (from both a Bayesian and
a frequentist perspective) for gene selection and differential expression. Recent
Bayesian developments include the probability of expression (POE) methods of
Parmigiani et al. (2002) and stochastic regularization (West et al., 2001). Parimi-
giani et al. (2003) also provide a review of some recent methods, both Bayesian
and frequentist, along with the associated software. For a review of some recent
Bayesian advancements in microarray literature, see Do et al. (2006).


4
Bayesian Multiple Testing and
False Discovery Rate Analysis
4.1
Introduction to Multiple Testing
Univariate gene detection in microarray experiments involves choosing a list
of gene candidates exhibiting differential expression across known experimental
conditions, e.g. over time following treatment or between normal and disease
patients, see Chapter 3 for details. Consider a gene expression microarray exper-
iment with, for the sake of simplicity, K = 2 experimental conditions. The
univariate hypotheses are speciﬁed as
H (g)
0
: Gene g is differentially expressed,
H (g)
1
: Gene g is not differentially expressed,
(4.1)
across factors k = 1, . . . , K for g = 1, . . . , G genes, where G is typically quite
large. Suppose also that the arrays are suitably preprocessed to account for any
technological variation and that each gene is represented by one and only one
probe on the array. While many microarray technologies do include multiple
probes to represent at least some genes, we adopt this simplifying assumption
for the present. Test statistics tg are computed for genes g = 1, . . . , G from
expression data, to infer differential expression between factors. Embodied in the
statistic tg is a measure of the likelihood or in a Bayesian analysis the posterior
belief that gene g is differentially expressed. A rule D : tg →ag is speciﬁed to
determine for each gene g the action ag chosen, to accept or reject H (g)
0 . The
actions (a1, a2, a3, . . .) have consequences that include further experimental and
Bayesian Analysis of Gene Expression Data
B. Mallick, D. Gold, and V. Baladandayuthapani
2009 John Wiley & Sons, Ltd

52
BAYESIAN ANALYSIS OF GENE EXPRESSION DATA
ﬁnancial costs. The enormity of the number G of univariate hypothesis tests to
be performed requires attention to the practical and relevant costs and consider-
ations for assembling a reliable collection of gene candidates in gene screening
studies. This leads to us to the very important statistical consideration of multiple
testing.
Example 4.1
Suppose that a microarray experiment is performed, with an expression array
including G = 20 000 genes. Suppose that independent univariate tests are per-
formed to infer differential expression across conditions, resulting in independent
frequentist p-values (p1, . . . , pG). For each gene one of four possible events
shown in Table 4.1 occurs.
Table 4.1
Inference for differential expression
No differential expression
Differential expression
Fail to reject H0
Correct decision
Type II error
Reject H0
Type I error
Correct decision
Setting a p-value cutoff of α = 0.05 is expected to produce 0.05 × 20 000 =
1000 Type I errors, or false positive rejections. In this special case, the stan-
dard deviation of the false positive count is √20 000 × 0.05 × (1 −0.05) ≈31.
That is, even if no genes actually exhibit differential expression, and the test
assumptions are valid, the 0.05 p-value cutoff is likely to yield between approx-
imately 938 and 1062 false positives. In such a situation as this, further time and
resources would have to be allocated to follow-up study on all of the important
gene candidates, to better understand their biological roles in the experiment at
hand. Suppose that after considering the time and cost, the principal investigator
decides to allocate resources to follow up on only 50 gene candidates. Moreover,
the investigator asks you to determine which gene candidates you recommend
for further follow-up.
In the above example, the time spent on follow-up study of 1000 false positive
gene candidates is prohibitive. Not only would this be an inefﬁcient expense in
terms of time and money, but also it would divert resources away from other
potential experimental research. Conservative tests, on the other hand, might miss
important gene bio-markers. This begs the question: what cutoff should be used to
decide which gene candidates are important? In general, the goals of experiments
can vary depending on the objectives and costs. Balancing the relative costs of
false and missed discoveries is an important consideration that can be included
in a decision-theoretic framework. Many approaches have been proposed, with
very different theoretical properties. In Section 4.2 we discuss false discovery
rate analysis, and extensions from a frequentist point of view. In Sections 4.3
and 4.4 we turn to Bayesian FDR analysis for control of FDR in gene expression

BAYESIAN MULTIPLE TESTING AND FALSE DISCOVERY RATE ANALYSIS
53
studies. A discussion of the decision-theoretic framework and of posterior and
predictive inferences is given in Appendix A.1.6
4.2
False Discovery Rate Analysis
4.2.1
Theoretical Developments
In gene expression studies, a popular decision rule is based in principle on con-
trolling the false discovery rate. The FDR is generally deﬁned to be the expected
percentage of positive tests that are incorrectly rejected. In gene detection anal-
ysis FDR is the expected proportion of genes that are detected for change, that
are in fact not differentially expressed. An important complication with this def-
inition is that the true number of differentially expressed genes is unknown,
requiring innovative theoretical work. Before we review the relevant frequentist
work on FDR estimation, let us begin with the earliest work in multiple compar-
ison, dating back to Bonferroni. Bonferroni proposed that, rather than controlling
the probability of a Type I error for each individual test, as in Example 4.1, one
control the family-wise error rate (FWER) deﬁned as
FWER = P (‘at least one false positive’).
This is equal to 1 −P (‘no false positives’). If the tests are independent, and
controlled at the same level α, then the P (‘no false positives’) = (1 −α)G and
FWER = 1 −(1 −α)G,
(4.2)
where G is the number of genes tested. The Bonferroni method controls the
FWER by setting the level of signiﬁcance for each test equal to
˜α = 1 −(1 −α)1/G ≈α
G.
The Bonferroni method ensures that the probability of at least one Type I error is
less than α. For example, if G = 20 000 genes are tested for differential expres-
sion, and the desired signiﬁcance level is α = 0.05, then tests with a p-value less
than or equal to ˜α = 0.05/20, 000 = 2.5 × 10−6 are rejected. This rule is con-
sidered to be conservative, and in some cases too stringent. Many genes that are
differentially expressed at low to moderate levels can be missed with this small
threshold, sacriﬁcing power. There are several important advantages to working
with the FDR rather than the FWER. Frequentist p-value thresholds convey the
probability that a test will be rejected given that the null is true. The FDR is a
better-understood and more intuitive concept for multiple testing, conveying the
probability that a test is incorrectly rejected. A 10% FDR is consistent with the
expectation that for every 10 gene detected, only one will be a false positive,
a 20% FDR with 2 in 10 false positive genes, etc. Rather than controlling the

54
BAYESIAN ANALYSIS OF GENE EXPRESSION DATA
FWER, which can greatly impede detection, an investigator may be willing to
accept some false positives, in order to discover important biomarkers. In theory,
FDR analysis provides the investigator with a clear intuition for choosing the
FDR threshold.
Consider the 2 × 2 table of counts of outcomes for m tests, shown in
Table 4.2.1. For each test, a test positive is a test for which the null hypothesis
is rejected, a test negative is a test for which the null hypothesis is not rejected,
and a false positive is test for which the null is rejected when in fact the null
is true, i.e. a Type I error. The number of false positives in Table 4.2.1 is V
and the proportion of false positives is V/R, for R > 0, and 0 if R = 0. The
FDR is the expected value of this ratio given R > 0. In practice V is unknown.
In the special case of a binomial action, either action a0 or a1 is chosen. The
frequentist decision rule minimizes the risk of making a wrong decision. In the
context of FDR, there are g = 1, . . . , G actions sought, and the decision rule
minimizes the risk or, in terms of actions, expected number of false positive
and false negative decisions.
Table 4.2
Multiple comparison counts, test outcomes by truth
Accept null
Reject null
Total
Null true
U
V
m0
Alternative true
T
S
m1
Total
W
R
m
Benjamini and Hochberg (1995) deﬁned the FDR as
FDR = E

V
R |R > 0

P (R > 0),
(4.3)
the expected value of the ratio of false positives to the number of positive tests,
multiplied by the probability that a test is rejected. For m tests, with m ordered
p-values, p(1) ≤p(2) ≤. . . ≤p(m), they reject all hypotheses with a p-value
p ≤p( ˆj), where
ˆj = argmax1≤j≤mk : p(j) ≤α · j/m.
This yields
FDR = m0
m · α ≤α.
Benjamini and Hochberg show that choosing the p-value threshold in this way
is asymptotically equivalent to choosing the threshold at the point p∗where the

BAYESIAN MULTIPLE TESTING AND FALSE DISCOVERY RATE ANALYSIS
55
lines β · p and F(p) cross, for
β = 1/α −π0
1 −π0
,
where π0 is the probability that H0 is true and F(p) is the CDF of all p-values.
Among all ‘last crossing’ rules, Benjamini and Hochberg (1995) show that their
rule is asymptotically optimal, in the sense of minimizing the risk of incorrectly
classifying the genes as differentially or nondifferentially expressed (Genovese
and Wasserman, 2002). Benjamini and Hochberg’s (1995) FDR is not the optimal
Bayes rule, although it does approach the Bayes rule for large G and large true
differences.
Like Bonferroni, this approach can suffer from a loss of power. Benjamini
and Hochberg (1995) also offered a less conservative estimator of the FDR by
replacing k/m with k/ ˆm, where ˆm = m ˆπ0 is an estimator of mπ0, the number
of true negatives.
Storey (2003) discusses the relative advantages of what he calls the positive
FDR (pFDR), deﬁned as
pFDR = E

V
R |R > 0

,
omitting P (R > 0) in (4.3), arguing that the case where R = 0 is generally not
of interest. Storey (2002) proposed the FDR estimator

FDR =
ˆπ0α
max {R(α), 1},
(4.4)
where ˆπ0 is an estimator of π0, the proportion of true negatives, and R(α) equals
the number of p-values less than or equal to α. Storey related the FDR to the
posterior probability of H0. Consider rejecting H0 if the t-test statistic t is in the
rejection region 
. The pFDR may be expressed as
pFDR(
) = P (H0 is True|t ∈
)
=
π0 · P (Type I error of 
)
π0 · P (Type I error of 
) + π1 · P (Power of 
)
(4.5)
for the test statistic t and test signiﬁcance region 
, where π0 and π1 are the
prior probabilities of accepting/rejecting the null respectively (see Storey, 2003),
assuming the tests are identical. Storey (2003) deﬁnes what he calls the q-value,
which he argues is the Bayesian analog of a p-value, as
q(t) =
inf

α:t∈
α pFDR(
α),
the smallest value of the pFDR consistent with a test statistic in the rejection
region 
α. This relation holds for independent and weakly dependent hypothesis
tests. The q-value is not the pFDR, but rather the posterior probability of H0,

56
BAYESIAN ANALYSIS OF GENE EXPRESSION DATA
given that the data t is in the minimal rejection region which includes t. While
Storey offers a Bayesian interpretation of the q-value, it does not have an analog
in Bayesian inference. The concept of power for detecting t in 
 is not a natural
one in a Bayesian setting, where by deﬁnition inference is deﬁned on the param-
eter space given the data. Chi (2007) considered the importance of sample size
in determining power with Storey’s pFDR.
Bickel (2004) introduced the decisive FDR (dFDR), deﬁned as
dFDR(
) = E
m
i=1 Vi

E
m
i=1 Ri
,
(4.6)
considering its useful decision-theoretic properties. The desirability function,
given the beneﬁt, bg, and cost, cg, of rejecting the gth hypothesis, is
d(b, c) =
m

g=1
(bi(Ri −Vi) −ciVi) =
m

g=1
biRi −
m

g=1
(bi + ci)Vi.
(4.7)
the expected desirability, given bg and cg, is
Ed(b, c, 
) = b1 (1 −(1 + c1/b1)dFDR(
)) E


m

g=1
Ri

.
(4.8)
A rejection region can be chosen such that a given expected desirability is
achieved. Moreover, Bickel (2004) explained that the dFDR more directly
approximates the posterior probability of the null hypothesis than the pFDR.
For those interested in advanced topics, Genovese and Wasserman (2004)
developed a theoretical framework for construction of frequentist false discovery
proportion (FDP) conﬁdence envelopes. Muller et al. (2004) developed optimal
sample size criteria for bivariate loss functions, including a criterion for both
the FDR and the false negative rate (FNR), i.e. the probability of missing a
true positive. They showed that the FNR decays at rate O

log n/n where n
is the number of samples, i.e. subjects, in the study. Yekutieli and Benjamini
(2001) propose a resampling-based approach to FDR analysis with correlated
tests. Storey (2002) discusses weak and positive dependence. Bickel (2004)
argues that the dFDR holds regardless of assumptions about independence or
weak dependence between the tests. The assumption of weak dependence is dif-
ﬁcult to justify in practice. It is generally believed that gene-wise expression
is dependent. Much of the development in univariate gene detection includes
the assumption of gene-wise independence, for practical reasons: the uncertainty
in gene expression, the uncertainty in gene networks, as well as the intractable
mathematics involved in imposing the assumption of gene-wise dependence.

BAYESIAN MULTIPLE TESTING AND FALSE DISCOVERY RATE ANALYSIS
57
4.2.2
FDR Analysis with Gene Expression Arrays
Methodological developments in frequentist FDR analysis for gene detection
extend the theoretical testing framework by pooling information between test
statistics with a broad array of non-parametric and bootstrap resampling tech-
niques aimed at robustness (see Westfall and Young, 1993). Efron and Tibshirani
(2002) proposed what they called an empirical Bayes method for gene detection.
Efron discussed the relevance of properly modeling both the null and alternative
distributions of the test statistics, using robust methods. He cited, among oth-
ers, the importance of properly accounting for technological variation possibly
due to gene-wise correlation or unexplained technological variability inducing
gene-wise dependence, and the consequences for FDR analyses that pool infor-
mation from univariate tests. Suppose that the change in gene expression, e.g.
between cancer and normal, is summarized by a z-score, {zg; g = 1, . . . , G}, the
difference in group averages divided by an estimate of the standard deviation.
Efron applies an empirical quantile transformation to the t-statistics, in order to
obtain standard normal z-scores
zg = −1(EDIFY (tg))
where EDIFY is the empirical CDF of the test statistics. Efron models the z-scores
as a mixture of random variables,
P (zg) = π0P0(zg) + π1P1(zg),
where P0(z) is assumed to be the distribution of z if H0 is true, and P1(z) if
H1 is true. Efron deliberated on the problem of robust estimation of the nonnull
distribution of the tests statistics. While P (z) is estimated directly from the data,
P0(z) is estimated on a permuted version of the data, with sign permutations on
differences between observations within each respective group. The probability
of no change, π0, is estimated given ˆP (z) and ˆP0(z). Efron’s local false discovery
rate (lFDR) is
lFDR(z) = π0P0(z)/P (z).
(4.9)
Efron relates the lFDR to the FDR as follows:
FDR(
) = P (null|z ∈
) =
	

lFDR(z)P (z)dz.
(4.10)
Pounds and Morris (2003) also model the p-value distribution as a mixture.
In their original work, the p-values were modeled as a beta-uniform mixture
(BUM),
P (p) = π0P0(p) + (1 −π0)P1(p),
with P0(p) the uniform part and P1(p) the nonuniform part. The nonuniform
part P1 is assumed to be a beta distribution with parameters a and b estimated

58
BAYESIAN ANALYSIS OF GENE EXPRESSION DATA
from the data. The motivation for this method is that, under strict conditions,
the theoretical distribution of p-values for which the null hypothesis is true is a
uniform distribution, and for tests for which the alternative is true, the theoretical
distribution is beta with expectation less than 0.5. In practice, it has been well
documented that the nonuniform part of the p-value distribution does not follow
a beta distribution, and can exhibit signs of multimodality. Pounds and Cheng
(2006) extended the BUM to model the nonuniform component of the p-value
distribution P1(p) by nonparametric regression. Another method is provided by
Dahl and Newton (2007) who conduct multiple hypothesis testing by clustering
treatment effects.
Example 4.2
We return to the Wachi et al. (2005) data set consisting of 22 283 expression
probe values from ten patients, ﬁve with SCC and ﬁve normal lung patients (see
Section 3.6). For each gene, the hypothesis test was performed comparing the
mean log-ratio δg in gene expression between the groups: H (g)
0
: δg = 0 versus
H (g)
1
: δg ̸= 0, assuming independent lognormal residuals. The BUM results are
plotted in Figure 4.1. While a 10% FDR cutoff is usually considered conservative,
the present analysis results in 6714 probe candidates with approximately 670
expected false positives!
Allison et al. (2002) extended the p-value mixture model in a purely para-
metric framework, treating the distribution of the p-values as mixtures of beta
random variables
P (p) =
J

j=1
πjBeta(p; aj, bj)
(4.11)
with parameters a1 = 1 and b1 = 1 in the ﬁrst component. Given a total of J
components, sensible order constraints can be placed on the aj and bj, in order
to assure identiﬁability. The log-likelihood with J∗components is
Lj∗(p) =
G

i=1
log


J∗

j=1
πjBeta(pg; aj, bj)

.
Allison applies a bootstrap approach to choosing the number of components with
the statistic Q = 2(Lj∗(p) −Lj∗−1(p)). Consider the mixture model of Allison
(4.11) with FDR estimator (4.12)
P (H0|p ≤α) = P (H0 ∩p ≤α)
P (p ≤α)
where
P (p ≤α) = τ1α +
J

j=2
	 α
0
πjBeta(p; aj, bj)dp

BAYESIAN MULTIPLE TESTING AND FALSE DISCOVERY RATE ANALYSIS
59
15
10
5
0
Density
0.25
0.15
0.20
0.10
0.05
Desired False Discovery Rate
0.0
0.2
0.4
0.6
0.8
1.0
P Values
(a)
0.00
0.05
0.15
0.10
0.20
0.25
0.30
Significant P Values
(b)
p* = 0.061
Figure 4.1
BUM analysis. (a) p-value distribution with ﬁtted Beta mixture density. (b) BUM FDR estimates: a 10% FDR
cutoff corresponds to a 0.061 p-value cutoff, resulting in 6714 probe candidates.

60
BAYESIAN ANALYSIS OF GENE EXPRESSION DATA
and
P (H0 ∩p ≤α) = π1α,
so that
P (H0|p ≤α) =
π1α
π1α + J
j=2
 α
0 τjBeta(p; aj, bj)dp
.
(4.12)
This is Allison’s estimator of FDR. They propose their bootstrap approach for
calculating conﬁdence intervals around 
FDR.
4.3
Bayesian False Discovery Rate Analysis
4.3.1
Theoretical Developments
Suppose that gene expression measurements are conditionally independent, xg ∼
P (θg), for genes g = 1, . . . , G, with the prior for θg denoted by P (θg). Suppose
further that the parameter space of θg ∈ can partitioned into two sets  =
(0, 1) with the null hypothesis for gene g deﬁned as H (g)
0
: θg ∈0 and the
alternative H (g)
1
: θg ∈1. The Bayesian decision rule rejects H (g)
0
if
P (θg ∈1 | xg) >(1 −α),
(4.13)
for some user-speciﬁed α. In a Bayesian framework the FDR is a measure of
the posterior belief of the event θg ∈0 given that H (g)
1
was chosen, i.e. gene g
was discovered to be biologically important. Unlike frequentist testing, H (g)
0
is
assumed unknown according to a prior belief that is updated from data. Let the
variable rα
1(xg) = 1 if H (g)
1
is chosen and rα
1(xg) = 0 otherwise. The Bayesian
FDR (bFDR) for a single gene g is deﬁned as P (H (g)
0 |rα
1(xg), xg). In an exper-
iment with G genes, the bFDR is deﬁned as
bFDR(rα
1) =

g P (θg ∈0 | rα
1(xg), xg) · rα
1(xg)

g rα
1(xg)
,
(4.14)
(Muller et al., 2004; Whittemore, 2007). In like manner, the Bayesian true neg-
ative rate (bTNR) is deﬁned as
bTNR(rα
1) =

g P (θg ∈0 | rα
1(xg) = 0, xg) · (1 −rα
1(xg))

g(1 −rα
1(xg))
.
(4.15)
Notice that, like Storey’s FDR, the bFDR depends on the rejection decision rule
and consequently on (1, α). In practice, the set 1 is ﬁxed, and the decision

BAYESIAN MULTIPLE TESTING AND FALSE DISCOVERY RATE ANALYSIS
61
rule is allowed to vary with the choice of α. The local bFDR for gene g, lbFDRg,
is deﬁned as
lbFDRg = bFDR(rα∗
1),
for α∗= argminαrα
1(xg) = 1,
(4.16)
which may be reported for each gene.
Multivariate generalizations of the bFDR follow from the deﬁnition. The null
hypothesis for gene set G = {g1, g2, g3, . . .} is deﬁned as H (G)
0
: θG ∈(G)
0
and
the alternative as H (G)
1
: θG ∈(G)
1 . The posterior rejection region is
P (θG ∈G
1
| x) >(1 −α(|G|)),
(4.17)
for user-speciﬁed α(|G|), a function of the cardinality of the set G. Let the vari-
able rα(|G|)
(G)
1
(x) = 1 if H (G)
1
is chosen and rα(|G|)
G
1
(x) = 0 otherwise. In order to
estimate the multivariate bFDR(r(α(|G|))
(G)
1
(x)), a measure is needed of the posterior
probability P (θG ∈(G)
0 |x), in the case where H (G)
0
is rejected. The Bayesian mul-
tivariate false discovery rate (bMVFDR) is deﬁned as this conditional posterior
probability,
bMV FDR(r(α(|G|))
(G)
1
(x)) = P (H (G)
0 |r(α(|G|))
(G)
1
(x) = 1, x).
(4.18)
A practical consequence of this deﬁnition is a framework for the Bayesian esti-
mation of the FDR of gene or probe combinations. For instance, consider a
microarray design with replicate probes for at least some of the genes. Prior
information about replicate probes, e.g. dependence, relative binding efﬁciency
or signal strength, can be speciﬁed ﬂexibly through the prior in order to form
bFDR estimates pooling across the probes mapping to each gene.
4.4
Bayesian Estimation of FDR
Bayesian extensions of frequentist parametric FDR inference are straightforward.
The essential difference is that in a Bayesian hypothesis test, rather than estimat-
ing FDR with plug-in estimators for model parameters, a Bayesian FDR analysis
is conducted by integrating over all of the posterior uncertainty in the parameter
space. Consider the mixture model of Allison (4.12) with FDR estimator (4.13).
The Bayesian FDR is estimated by
bFDR(p < α) =
	
· · ·
	
ψ(p < α|π, a, b)P (π, a, b|p)dπdadb (4.19)
with
ψ(p < α|π, a, b) =
π1α
π1α + J
j=2
 α
0 πjBeta(p; aj, bj)dp
(4.20)

62
BAYESIAN ANALYSIS OF GENE EXPRESSION DATA
integrating over the posterior uncertainty in π, a, b. Bayesian posterior credi-
ble envelopes for the quantity ψ(p < α|π, a, b) over the range (0, 1) can be
computed in a straightforward manner as well following the method outlined for
determining credible envelopes in Chapter 3.
Next, we demonstrate the ﬂexibility of the Bayesian decision-theoretic
approach, not to be confused with Bayesian hypothesis testing, generalizing the
FDR analysis in Example 13.1 with a continuous null hypothesis region.
Example 4.3
Returning to the analysis in Example 13.1, suppose that we pose the hypotheses
as H (g)
0
: |δg| ≤1 versus H (g)
1
: |δg| > 1, where δg is the mean log-ratio of gene
expression between cancer and normal in the Wachi experiment for gene g.
Following the assumptions in Example 13.1, the likelihood of
¯xg1 −¯xg2 −δ
ˆσg
√(1/5 + 1/5) ∼t8,
(4.21)
for g = 1, . . . , G, difference in group means ¯xg1 −¯xg2, and standard deviation
estimate ˆσg, follows a t distribution with 8 degrees of freedom. Specifying an
improper ﬂat prior for δg results in the posterior distribution
δg|xg ∼t8(¯xg1 −¯xg2, ˆσ 2
g (1/5 + 1/5))
(4.22)
a t with 8 degrees of freedom, location parameter ¯xg1 −¯xg2, and dispersion
ˆσ 2
g
1
5 + 1
5

.
(4.23)
The Bayesian decision rule rejects H (g)
0
if
P (δg < −1|xg) + P (δg > 1|xg) >(1 −α)
(4.24)
for user-speciﬁed α. Different choices of α lead to different decisions, more or
less rejected hypotheses, and hence different bFDRs. The bFDR in this example is
bFDR(rα) =

g P (−1 < δg < 1|xg)rα(xg)

g rα(xg)
,
(4.25)
depending on α, where rα(xg) = 1 if H (g)
0
is rejected and 0 otherwise. The
corresponding likelihood ratio frequentist test of H (g)
0
: |δg| ≤1 versus H (g)
1
:
|δg| > 1 yields 21 431 genes with p-values of 1, and few p-values between 0
and 1, i.e. strong evidence against a continuous p-value distribution. Recall also
that Efron’s approach to modeling change in gene expression relies on permuted
versions of the data, i.e. differences between and within the normal and cancer

BAYESIAN MULTIPLE TESTING AND FALSE DISCOVERY RATE ANALYSIS
63
groups respectively, and therefore does not permit inference against a continuous
null set such as H (g)
0
: |δg| ≤1.
Robust innovations for FDR estimation combined with Bayesian methods
have been proposed for multiple testing with gene expression data analysis. Ish-
waran and Rao (see BAM in Appendix A), propose the strategy of simulating
data Ynew from the posterior predictive distribution assuming H0 is true to esti-
mate a null distribution, and consequently the FDR. Recall that the transformed
response in BAM is
˜Ygkl = θg0 + µglI(l = 2) + ϵgkl,
(4.26)
with ϵjkl assumed to be residual noise. Due to the centering Ygkl −¯Ygl, θg0 is
expected to be near 0. The posterior distributions are known in the case where
µgl = 0 and gene g is selected for change, i.e. ν2
2g is large. The statistic
√n2
ˆσn
( ¯Yg1 −¯Ygx2)
(4.27)
is normally distributed with variance (n1 + n2)/n1. Ishwaran and Rao ﬁt a
two-point mixture model to data simulated from their model, what they call
FDRmix, constraining µjl = 0. The simulated data provides estimation of a null
distribution for each gene, and a p-value for change. Their FDR is controlled as
proposed by Benjamini and Hochberg. This is a two-stage analysis, making use
of the posterior estimates in the ﬁrst stage, to model the FDRs for the parameters
of change in the second stage.
Do et al. (2005) applied robust mixture modeling to permuted versions of
the data to estimate FDR. In the case where microarray data are collected from
two groups, disease and normal phenotypes, a matrix D is generated with, for
each gene (in the rows), all possible pairs of differences between the tumor
and normal samples (in the columns). A matrix d is constructed similarly, with
differences between all possible pairs of samples within the tumor and normal
groups respectively. For genes g = 1, . . . , G, two z statistics are generated
zmix
g
= ¯d′
g/(a′
0 + ˆσ ′
g),
znull
g
= ¯dg/(a′
0 + ˆσg),
where zmix
g
and znull
g
are standardized Z statistics computed across the columns
of d′ and d, with sample standard deviations ˆσ ′
g and ˆσg. The constants a′
0 and
a0 are added for robustness. Extending Efron’s work, Do et al. model zmix as a
mixture
P (Z) = π0P0(z) + π1P1(z)

64
BAYESIAN ANALYSIS OF GENE EXPRESSION DATA
0.5
0.20
0.15
0.10
0.05
0.00
0.4
0.3
0.2
0.1
0.0
bFDR
bFDR
0.5
0.6
0.7
0.8
0.9
1.0
0
100
200
300
400
500
a
Count of Rejected Hypotheses
Figure 4.2
Bayesian FDR analysis in Example 13.2 (a) bFDR by decision rule
cutoff α, (b) bFDR by count of rejected hypotheses, choosing bFDR = 10% results
in 33 rejected hypotheses.
with likelihood for znull assumed to be P0(z). For j = 0, 1,
Pj(z) =
	
N(z; µ, σ 2)d
j(µ),

j ∼DP (a, 
∗
j ).
The means are assumed to follow a Dirichlet process prior DP(a, 
∗
j ), with scale
a and base measure 
∗
j . Do et al. deﬁne FDR by (14), applying the rejection rule
EP1(z∗) = E(1 −π0)P1(z∗)/P(z∗) > 1 −α, with the expectation taken over the
posterior distribution of the parameters and α speciﬁed as the smallest cutoff
achieving a desired FDR.
Example 4.4
We conclude our discussion of Bayesian FDR analysis with application of Do
et al.’s robust Dirichlet process mixture model to the Wachi et al. (2005) data
set. Information about obtaining the BayesMix software is available from the
authors’ supplementary materials. BayesMix was run with correction factors a0 =
a′
0 = 0.50. Figure 4.3 shows the densities of the empirical scores znull and zmix.
The tails of the density of the latter are considerably heavier than the former.
Output from BayesMix is displayed in Figure 4.4. At the 0.10 FDR 12 664 probes
were discovered for change, at the 0.05 FDR 8487, and at the 0.01 FDR 5404
probes. The DP mixture model is quite robust, relying on very little in the way of
assumptions about the underlying distributions. Unlike other software, BayesMix
is quick and easy to apply, and relies on few user-controlled parameters.

BAYESIAN MULTIPLE TESTING AND FALSE DISCOVERY RATE ANALYSIS
65
znull
zmix
−2
−1
0
z
1
2
0.0
0.5
1.0
1.5
2.0
2.5
Density
Figure 4.3
Kernel density plots znull and zmix with Wachi et al. (2005) experi-
mental gene expression data.
4.5
FDR and Decision Theory
The multiple testing problem was posed in a decision-theoretic framework by
Duncan (1965), who laid the groundwork for Bayesian analysis of multiple com-
parisons. His early work surpassed even much later thinking, that the problem
of choosing a rejection rule can be driven by competing goals. Conventional
thinking in multiple testing recognizes the importance of the decision-theoretic
framework; see; for example; Berry and Hochberg (2001) and Chen and Sarkar
(2005). Working with the posterior of the FDR function offers theoretical simplic-
ity for Bayesian analysis. Analogously, the Bayesian false negative rate (bFNR)
function can be deﬁned as
bFNR(rα
1) =

g P (θg ∈1 | rα
1(xg), xg)) · (1 −rα
1(xg))

g(1 −rα
1(xg))
,
(4.28)
In a decision-theoretic setting, the choice of the best threshold is guided by the
relative costs of misspeciﬁcation. Some popular loss functions include linear loss
FDR + ωFNR and bivariate loss FNR|FDR < α. Posterior evaluation of the
loss yields probability measures over the full posterior uncertainty in decision
making. The posterior uncertainty in decisions can be explored in silico, to inform
choices of study design; see Muller et al. (2004) for extensive discussion.
4.6
FDR and bFDR Summary
Interest in FDR analysis has grown considerably with the advent of new array
technologies, and the accumulation of ever increasing computing storage capac-
ities. These advances have provoked researchers to expand the deﬁnitions of

66
BAYESIAN ANALYSIS OF GENE EXPRESSION DATA
z* = 0.001165961
GAMMA
0.65 0.70 0.75 0.80 0.85 0.90 0.95 1.00
FDR
0.00 0.05 0.10 0.15 0.20
z* = 0.001165961
GAMMA
0.65 0.70 0.75 0.80 0.85 0.90 0.95 1.00
FDR
0.00 0.05 0.10 0.15 0.20
z* = 0.001165961
GAMMA
0.65 0.70 0.75 0.80 0.85 0.90 0.95 1.00
FDR
0.00 0.05 0.10 0.15 0.20
z* = 0.09951558
GAMMA
0.65 0.70 0.75 0.80 0.85 0.90 0.95 1.00
FDR
0.00 0.05 0.10 0.15 0.20
z* = 0.1724676
GAMMA
0.65 0.70 0.75 0.80 0.85 0.90 0.95 1.00
FDR
0.00 0.05 0.10 0.15 0.20
z* = 0.3056199
GAMMA
0.65 0.70 0.75 0.80 0.85 0.90 0.95 1.00
FDR
0.00 0.05 0.10 0.15 0.20
z* = 0.4763746
GAMMA
0.65 0.70 0.75 0.80 0.85 0.90 0.95 1.00
FDR
0.00 0.05 0.10 0.15 0.20
z* = 0.7154709
GAMMA
0.65 0.70 0.75 0.80 0.85 0.90 0.95 1.00
FDR
0.00 0.05 0.10 0.15 0.20
z* = 720982
GAMMA
0.65 0.70 0.75 0.80 0.85 0.90 0.95 1.00
FDR
0.00 0.05 0.10 0.15 0.20
Figure 4.4
BayesMix, applied to Wachi et al. (2005) experimental gene expression data. z-value cutoffs are shown for
different desired FDRs.

BAYESIAN MULTIPLE TESTING AND FALSE DISCOVERY RATE ANALYSIS
67
what it means to infer that a set of variables are important for discovery. While
there is little consensus on how to estimate or even deﬁne the FDR, FDR analyses
provide a practical rule for the problem of multiple testing with microarray data.
Further statistical considerations include addressing more complicated experi-
mental designs, and combining multiple per gene hypotheses, of many different
characteristics measured across a variety of sources. Other important topics, for
example FDR conﬁdent envelop estimation (Genovese and Wasserman, 2004;
Allison et al., 2002) or sample size requirements (Muller et al., 2004; Chi, 2007),
are still very much active areas of research.
Much of the underlying theory of FDR analysis can be posed in a
decision-theory framework, and this framework allows contributions from both
frequentist and Bayesian schools of thought. Ad hoc frequentist and Bayesian
methods for FDR analysis are growing more plentiful, for a list of some of the
most important advances, see Pounds and Cheng (2007). We have presented
some of the ground-breaking ideas in frequentist and Bayesian FDR estimation.
There is still much to be discovered. As we continue to expand our vocabulary
in new ways that allow us to think about multiple testing with ever large data
sets to consider, these important new ideas will ﬁlter into other arenas, providing
concepts of inference to the realms medical and biological informatics.


5
Bayesian Classiﬁcation
for Microarray Data
5.1
Introduction
One of the fundamental goals of microarray data analysis is classiﬁcation of bio-
logical samples. The problem of classiﬁcation or supervised learning has received
a great deal of attention, especially in the context of cancer studies. Precise clas-
siﬁcation of tumors is often of critical importance to the diagnosis and treatment
of cancer. Targeting speciﬁc therapies to pathogenetically distinct types of tumor
is important for the treatment of cancer because it maximizes efﬁcacy and min-
imizes toxicity (Golub et al., 1999). Initial studies relied on macroscopic and
microscopic histology and tumor morphology as the basis for the classiﬁcation
of tumors. However, within current frameworks, one cannot discriminate between
tumors with similar histopathologic features, which vary in clinical course and
in response to treatment.
The use of DNA microarrays allows simultaneous monitoring of the expres-
sions of thousands of genes (Schena et al., 1995; DeRisi et al., 1997; Duggan
et al., 1999), and has emerged as a tool for disease diagnosis based on molecular
signatures. Several studies using microarrays to proﬁle colon, breast and other
tumors have demonstrated the potential power of expression proﬁling for clas-
siﬁcation (Alon et al., 1999; Golub et al., 1999; Hedenfalk et al., 2001). Gene
expression proﬁles may offer more information than and provide an alternative
to morphology-based tumor classiﬁcation systems. In this chapter we focus on
classiﬁcation using microarray data. In principle, gene expression proﬁles might
serve as molecular ﬁngerprints that would allow for accurate classiﬁcation of
Bayesian Analysis of Gene Expression Data
B. Mallick, D. Gold, and V. Baladandayuthapani
2009 John Wiley & Sons, Ltd

70
BAYESIAN ANALYSIS OF GENE EXPRESSION DATA
diseases. The underlying assumption is that samples from the same class share
expression proﬁle patterns unique to their class (Yeang et al., 2001). In addition,
these molecular ﬁngerprints might reveal newer taxonomies that previously have
not been readily appreciated.
In general, the problem of class detection can be categorized based on two
main aspects: clustering or supervised learning, and classiﬁcation or unsuper-
vised learning. In unsupervised learning (other nomenclatures include cluster
analysis, class discovery, and unsupervised pattern recognition), the classes are
unknown a priori and are estimated from the observed data. Note that the cluster-
ing mechanism can be applied to both the genes and the microarrays (samples).
In contrast, in supervised learning or classiﬁcation, the classes are predeﬁned
and the goal is build a classiﬁer using the attributes of the data and predict
the class of future unlabeled observations. Although there are inherent connec-
tions between the two, we shall focus on supervised learning/classiﬁcation for
the remainder of the chapter. See Chapter 7 for clustering techniques applied to
microarrays.
In classical multivariate statistics, classiﬁcation is a prediction or learning
problem in which the response variable Y to be predicted assumes one of C pre-
deﬁned and unordered values arbitrarily coded as dummy variables (1, 2, . . . , C).
For K = 2 this reduces to a binary classiﬁcation problem with responses usually
coded as Y ∈(0, 1), where 0 might be normal tissue and 1 tumor tissue. Also
observed are a set of p measurements that form the feature/predictor vector,
X = (X1, . . . , Xp) belonging the a feature space X (e.g. span of real numbers
Rp). The task of classiﬁcation is to predict Y from X on the basis of the observed
measurements X = x. The classiﬁer for the C classes is a mapping, T, from X
onto (1, 2, . . . , C) as T : X →(1, 2, . . . , C), where T(x) is the predicted class
for an observed feature matrix X.
In microarrays, the features correspond to the expression values of differ-
ent genes, and classes correspond (usually) to the different tumor types (nor-
mal/cancer, etc.). Gene expression data for p genes for n samples are summarized
in an n × p matrix, X, where each element Xij denotes the expression level (gene
expression value) of the jth gene, j = 1, . . . , p, in the ith sample:


Gene 1
Gene 2
· · ·
Gene p
Y1
X11
X12
· · ·
X1p
Y2
X21
X22
· · ·
X2p
...
...
...
...
...
Yn
Xn1
Xn2
· · ·
Xnp


.
The exact meaning of expression values may be different for different matrices,
representing absolute or comparative measurements (see Brazma et al., 2001).
In this chapter we review some Bayesian linear and nonlinear approaches to
classiﬁcation – where the linearity/nonlinearity assumption is made on the way

BAYESIAN CLASSIFICATION FOR MICROARRAY DATA
71
the feature space is modeled. We show how the classical linear discriminant anal-
ysis can be extended in a Bayesian framework in order to account for estimation
uncertainty. At the heart of our discussion of the methods lies classiﬁcation
via regression-based approaches. Regression methods are extremely ﬂexible and
these models lend themselves nicely to the hierarchical Bayesian modeling frame-
work in which the variability in the gene expression data can be modeled at
various levels.
One of the key issues in microarray classiﬁcation as opposed to other appli-
cations is the dimension of the feature space, typically of the order of thousands
of genes, coupled with a low number of arrays (usually in hundreds). This is
typically known as the large p, small n problem (West, 2003). In this situation,
in most classiﬁcation procedures a dimension reduction step is needed to reduce
the high-dimensional gene space. There are two main reasons for this: ﬁrst, most
of the genes are likely to be uninformative for prediction purposes; and sec-
ond, the performance of the classiﬁer depends highly on the number of features
used (West et al., 2001; Ambroise and Mclachlan, 2002). This is termed feature
selection and is a topic of extensive research in both the statistical and computer
science literature. Feature selection in the context of microarrays can be done
in two ways: explicitly and implicitly. In the former genes are ﬁrst selected one
at a time using univariate test statistics such as t- or F-statistics (Dudoit et al.,
2003); ad hoc signal-to-noise statistics (Golub et al., 1999; Pomeroy et al., 2002);
nonparametric Wilcoxon statistics (Park et al., 2001); and p-values (Dudoit et al,.
2002). The cutoff thresholds are then selected using some criteria such as false
discovery rates. The classiﬁer is then built upon this feature set. Such methods
have a potential disadvantage since the uncertainty in estimation of the feature
set is not taken into account while building and estimating the error rates of the
classiﬁer. In the latter implicit approach, the selection is embedded within the
classiﬁcation rule itself. This scenario can easily handled within the Bayesian
hierarchical modeling framework, by specifying an extra layer for feature selec-
tion set uncertainty. Not only is the uncertainty in estimation propagated through
the modeling process, but also we obtain an honest assessment of this uncer-
tainty via posterior sampling, as we shall describe in the subsequent sections.
Section 5.2 starts by exploring the classical classiﬁcation and discriminant rules,
and their Bayesian extensions are discussed in Section 5.3. Section 5.4 delves
into regression-based approaches to classiﬁcation and Section 5.5 deals with non-
linear extensions. We talk about some aspects of evaluating the performance of
a classiﬁer in Section 5.6 and then round matters off with a real data example
(Section 5.7) and a discussion (Section 5.8).
5.2
Classiﬁcation and Discriminant Rules
Classical approaches to classiﬁcation have been mainly via discriminant rules
which are inherently Bayesian in some sense. For example, consider the Bayes
rule under a symmetric loss function. Given class conditional densities Pc(X) =

72
BAYESIAN ANALYSIS OF GENE EXPRESSION DATA
P (X|Y = c) of the features X in class c and class priors πc, the posterior prob-
ability P (Y = c|X) of class c given feature vector X is
P (Y = c|X) =
πcPc(X)

k πkPk(X).
The Bayes rule predicts the class of an observation X by that with higher posterior
probability TB(X) = argmaxcP (Y = c|X). Under a symmetric loss function this
is just the Bayes risk. We refer the reader to Mardia et al. (1979) for further
discussion on classiﬁcation within a decision-theoretic framework.
Many classiﬁers can be viewed as variations of this general rule, with para-
metric or nonparametric estimators of P (Y|X) (Dudoit and Fridlyand, 2002).
There are two general paradigms that are used to estimate the class posterior
probabilities P (Y|X): density estimation and direct function estimation (Fried-
man, 1997). In the density estimation approach, the class conditional densities
Pc(X) = P (X|Y = c), along with the class priors πc, are estimated separately
for each class and Bayes’ theorem is then used to obtain P (Y|X). Classiﬁcation
methods employing this principle include maximum likelihood discriminant rules
and discriminant analysis. In the next section we discuss a Bayesian extension of
these rules. In the direct function estimation approach, the class posterior prob-
abilities P (Y|X) are estimated directly based on function estimation approaches
such as regression. This general framework includes most of the popular clas-
siﬁcation approaches: logistic regression, neural networks (Ripley, 1996) and
classiﬁcation trees (Breiman et al., 1984). We focus on this approach for most
of our discussion of Bayesian regression-based methods for linear and nonlinear
classiﬁcation primarily due to their ﬂexibility and popularity.
5.3
Bayesian Discriminant Analysis
Bayesian discriminant rules for classiﬁcation usually proceed by extending the
frequentist classiﬁcation rules and assuming a prior on the parameters. To for-
malize, suppose that independent random variables, features or genes (possibly
vectors) Xi1, . . . , Xini are observed from populations (classes) i = 1, . . . , C, each
with probability distribution Pi(θi), where ni is the number of genes in class i.
The likelihood can then be written as (C
i=1
(ni
j=1 Pi(Xij|θi) where the θi are the
unobserved population parameters that need to estimated.
In classical frequentist classiﬁcation, a new observation Xnew is classiﬁed
by estimating θi from the training observations, θi, and plugging back into the
likelihood to form the prediction rules. The new observation Xnew is then assigned
to the class i for which Pi(Xnew|θi) > P ′
i (Xnew|θ′
i) for all i′, and in case of ties
assigned randomly. In the Bayesian paradigm, a prior is assumed over θ, in order
to carry forward the uncertainty in estimation of θ while making predictions. We
refer the reader to Friedman (1997) for a discussion of the bias–variance tradeoff
in classiﬁcation.

BAYESIAN CLASSIFICATION FOR MICROARRAY DATA
73
In Bayesian parametric classiﬁcation, a new observation Xnew is classiﬁed
by assigning a prior distribution to the θ, P (θ1, . . . , θC), and using the posterior
distribution for inference. The (joint) posterior, P (θ1, . . . , θC|X), is then pro-
portional to (C
i=1
(ni
j=1 Pi(Xij|θi)P (θ1, . . . , θC). The predictive distribution can
then be obtained by marginalizing over θ. For the ith population the predictive
distribution for Xnew can then be written as
Pi(Xnew|X) =
	
i
Pi(Xnew|θi)π(θi|X)dθi,
(5.1)
for all i and where i is the support of the θi. The Bayesian prediction rule
then assigns Xnew to population i via the rule Pi(Xnew|X) > P ′
i (Xnew|X) for all
(i, i′), and in case of ties assigned randomly. This then leads to the Bayesian
classiﬁcation rule based on the posterior predictive distribution,
P (Ynew = i) =
πiPi(Xnew|X)

c πcPc(Xnew|X).
Frequentist classiﬁcation methods usually rely on large-sample theory or resam-
pling mechanisms in order the determine the uncertainty in the predictions. In
the Bayesian classiﬁcation rule presented above, uncertainty is quantiﬁed once
the posterior π(θi|X) is obtained. This is discussed in some detail in Section 5.6
below.
In principle, one can assume any distribution for the feature space Pi(X|θ)
depending on the application at hand. Assuming a Gaussian distribution gives
rise to the most common class of Bayesian linear classiﬁers, which are linear
on the feature space. Suppose the feature vectors Xi1, . . . , Xini are indepen-
dently observed from populations i = 1, . . . , C, each with probability distribution
N(µi, i), where θi = (µi, i) are the unknown mean and covariance of Xij.
The likelihood can then be written analogously as
P (X|µ1, . . . , µC, 1, . . . , C) =
C

i=1
Ni

j=1
(Xij|µi, i),
where (µ, ) is the probability density function (pdf) of a normal distribution
with mean µ and variance–covariance matrix .
This then corresponds to the maximum likelihood discriminant rules in the
frequentist sense (also known as linear discriminant analysis). In the context of
microarray experiments, the feature vector Xij denotes the vector of gene expres-
sion intensity values for individual j in population i. To complete the hierarchical
formulation we need to specify priors on both (µi, i). One natural choice of
conjugate priors is a Gaussian prior on µi and a corresponding inverse-Wishart
prior on i. Another convenient, noninformative joint prior for (µi, i) is the
Jeffreys prior (Jeffreys, 1946). It can be shown that the Jeffreys prior for this par-
ticular case corresponds to P (µ1, . . . , µC, 1, . . . , C) ∝(C
i=1 |i|(p+1)/2 (the

74
BAYESIAN ANALYSIS OF GENE EXPRESSION DATA
reader might like to try this as an exercise). As already mentioned, inferences
using the Jeffreys prior have strong connections to maximum likelihood infer-
ence. The posterior predictive density (5.1) for this class of priors can then be
shown to be a multivariate t density (Press, 2003),
P (Ynew| ¯Xi, Si, πi) ∝

1 + Ni(Xnew −¯Xi)′Si(Xnew −¯Xi)
(Ni + 1)(N −C)
−(N−C+1)/2
where N = C
i=1 Ni,
¯Xi = N−1
i
Ni
j=1 Xij, and (Ni −1)Si = 
j(Xij −¯Xi)
(Xij −¯Xi)′.
For simplicity of exposition, consider a binary classiﬁcation situation with
two populations. Assuming 1 = 2, a frequentist rule is to assign Xnew to
class 1 if P = [1 + (π1/π2) exp(−L)]−1 is greater than 1, class 2 is P < 1, and
at random if P = 1, where L = log[f1(X)/f2(X)] is the log-density ratio and
πi is the prior class probability. In the Bayesian context, with Jeffrey’s prior,
the classiﬁcation rule can be shown to be PB = [1 + (π1/π2) exp(−LB)]−1,
where
LB = 1
2{(ν + 1) log[(ν + r2 ˆδ2)/(ν + r1 ˆδ1)] + p log(r1/r2)},
ˆδi = (X−
¯Xi)′−1(X −¯Xi),
 = [(N1 −1)S1 + (N2 −1)S2]/(N1 + N2 −2),
ri = Ni/
(Ni + 1)
and
ν = N1 + N2 −2.
From
a
fully
Bayesian
viewpoint,
PB = P (Xnew ∈πi|X) = E(P|X), the posterior expected value of P given X
(Rigby, 1997). Proof of this is left as an exercise.
Note that until now, we have been working with the full n × p expression
matrix X. This might be unwieldy from both a computational and modeling
standpoint. One can incorporate feature selection into this framework in straight-
forward manner via hierarchical modeling. We sketch out a possible method via
the introduction of latent indicator variables, which are a popular tool in Bayesian
variable selection (See Chapter 3, Section 3.2.2 on feature selection). Let the indi-
cator variable γg be deﬁned such that γg = 1 if gene g is included in the model
and γg = 0 if gene g is excluded. Thus the number of genes included in the
model is given by 
g γg = K, which is assumed less than p via specifying a
prior on K to control the number of genes included in the model. Choices for
K include any discrete prior such as Poisson, discrete uniform or a geometric
distribution. To complete the hierarchical formulation we need to assume a prior
on the γg. A convenient choice is accorded by a Bernoulli distribution.
5.4
Bayesian Regression Based Approaches
to Classiﬁcation
We now turn our attention to regression approaches to classiﬁcation which can
be treated within a regression framework via generalized linear model (GLM)
methodology (McCullagh and Nelder, 1989). The GLMs are a natural generaliza-
tion of the classical linear models and include as special cases linear regression,
analysis of variance, logit and probit models, and multinomial response models.

BAYESIAN CLASSIFICATION FOR MICROARRAY DATA
75
The GLMs have been used in a variety of applications, including microarrays.
In a Bayesian context, classiﬁcation via GLMs has many potential advantages,
especially in the microarray context. Using hierarchical modeling approaches,
one can embed the basic GLM into a more complex setting in order to address
the scientiﬁc question at hand. The model building process is thus elucidated via
a series of hierarchical steps as described below.
5.4.1
Bayesian Analysis of Generalized Linear Models
As already mentioned, GLMs are extensions of the linear regression model
described in the previous chapter. Speciﬁcally, they avoid the selection of a
single transformation of the data that must achieve the possibly conﬂicting goals
of normality and linearity imposed by the linear regression model which in some
cases is unrealistic ,e.g. for binary or count data. The class of GLMs uniﬁes the
approaches needed to analyze such data for which either the assumption of a
linear relation between X (covariates) and y (responses) or the assumption of
normal variation is not appropriate. The name GLM stems from the fact that the
dependence of y on X is partly linear in the sense that the conditional distribution
of y given X is deﬁned in terms of a linear combination XT β as
y|X, β ∼P (y|XT β).
A GLM is usually speciﬁed in two stages:
1. The linear predictor, η = XT β.
2. The link function g(·) that relates the linear predictor to the mean of
response variable: µ(X) = g−1(η) = g−1(XT β).
Additionally, a random component characterizing the distribution of the
response variable can be speciﬁed along with a dispersion parameter. Thus,
essentially the mean of the distribution of y, given X, is determined by XT β as
E(y|X) = g−1(XT β) which for identiﬁability reasons is a one-to-one function.
The sampling model for the data can then be written as
P (y|X, β) =
n

i=1
P (yi|XT
i β).
5.4.2
Link Functions
Ordinary linear regression is obviously a special case of the GLM where g(x) = x
such that y|X, β, σ 2 ∼N(XT β, σ 2). For continuous data that are all positive, the
normal model on the logarithmic scale can be used. Note that the regression
coefﬁcients β have a distinct interpretation since they are directly related to the
response variable, but this interpretation becomes harder for other families due
the presence of the link function.

76
BAYESIAN ANALYSIS OF GENE EXPRESSION DATA
Perhaps the most widely used of the GLMs are those for binary or binomial
data which shall be our focus in this chapter. Suppose that yi ∼Binomial(ni, µi)
with ni known. The most common approach is to specify the model in terms of the
mean of the proportions yi/ni rather than the mean of yi. The logit transformation
of the probability of success, g(µi) = log{µi/(1 −µi)}, as the link function leads
to the ubiquitous logistic regression model. The sampling model then turns out
to be
P (y|β) =
n

i=1

ni
µi
 
eηi
1 + eηi
yi 
1
1 + eηi
ni−yi
.
An alternative link function often used for binary data is the probit link function,
where g(µ) = −1(µ), which leads to the sampling model
P (y|β) =
n

i=1

ni
µi

{(ηi)}yi {1 −(ηi)}ni−yi .
Although in practice the probit and logit links are very similar, they differ only
in the extremes of the tails. Other robust link choices include the complementary
log-log link: g(µ) = log(−log(µ)).
5.4.3
GLM using Latent Processes
A key idea, both in computing and interpretation of discrete GLMs, is via latent
(unobserved) continuous processes. For example, the probit model for binary
data, P (yi = 1) = (XT
i βi), is equivalent to the following model on latent scale
zi where zi ∼N(XT
i βi, 1) and
yi =
 1,
if zi ≥0,
0,
if zi < 0.
There are two advantages of viewing the GLM via latent processes. First, the
latent process zi can be given a useful interpretation in terms of the underlying
process that generates the discrete response. Second, the latent parameterization
admits a computationally efﬁcient estimation procedure via MCMC techniques,
especially the Gibbs sampler. We shall use this idea in many of our examples
in the subsequent sections. The latent process framework can also be used for
logistic regression using a residual component, as we will demonstrate.
5.4.4
Priors and Computation
The main quantity of interest are the β parameters which quantify the strength of
the relationship between the response (classes) and predictors (genes). Estimation
and inference in the frequentist context usually proceed via maximum likelihood

BAYESIAN CLASSIFICATION FOR MICROARRAY DATA
77
estimation, which does not have a closed form. In Bayesian inference, prior
(informative/noninformative) distributions are elicited for β; however, in most
cases conjugate priors do not exist and the posterior distribution is not of closed
form. Hence, posterior sampling is usually carried out using adaptive rejection
Metropolis sampling (Gilks and Wild, 1992) since in most cases the posterior
distribution is log-concave. Another attractive sampling approach that we shall
explain here is via the use of latent/auxiliary variables (Albert and Chib, 1993;
Holmes and Held, 2006) which reduces the sampling procedure to a series of
Gibbs steps which we now explain.
5.4.5
Bayesian Probit Regression using Auxiliary Variables
For simplicity we consider a binary classiﬁcation problem where the response is
usually coded as Yi = 1 for class 1 (diseased tissue) and Yi = 0 (normal tissue)
for the other class. Our objective is to use the training data Y = (Y1, . . . , Yn)
to estimate πi(X) = P (Yi = 1|Xi), the probability that a sample is diseased.
Assume that Yi are independent Bernoulli distributed so that Yi = 1 with prob-
ability πi(X), independent of other Yj, ∼j ̸= i. The gene expression levels of
the genes can then be related to the response using a probit regression model,
P (Yi = 1|β) = (X′
iβ),
where Xi is the ith row of the matrix X (vector of gene expression levels of the ith
sample), β is the vector of regression coefﬁcients and  is the normal cumulative
distribution function. Following Albert and Chib (1993), n independent latent
variables Z1, . . . , Zn are introduced, with each Zi ∼N(X′
iβ, σ 2) and where
Yi =

1,
if Zi > 0,
0,
if Zi < 0.
Hence, we assume the existence of a continuous unobserved or latent variable
underlying the observed categorical variable. When the latent variable crosses a
threshold (assumed to be 0 here), the observed categorical variable takes on a
different value. The key to this data augmentation approach is to transform the
model into a normal linear model conditional on the latent responses, as now
Zi = X′
iβ + ϵi where ϵi ∼N(0, σ 2): the residual random effects. The residual
random effects account for the unexplained sources of variation in the data, most
probably due to explanatory variables/genes not included in the study. We ﬁx
σ 2 = 1 as it is unidentiﬁed in the model. It can be seen with this formulation
the marginal distribution of Yi is (X′
iβ). The full hierarchical model is now
speciﬁed as
Yi|Zi, β ∼I(Zi > 0)δ1,
Zi ∼N(Xiβ, 1),

78
BAYESIAN ANALYSIS OF GENE EXPRESSION DATA
β ∼N(µ, σ 2V ),
σ 2 ∼IG(a, d),
where δ1 is the indicator variable. The prior mean µ on β is usually set to 0,
in order to shrink the (effects of) nonimportant genes to 0. V is the prior
variance–covariance matrix of the genes, which can modeled in a variety of
ways. It can be taken to be diagonal, assuming a priori independence between
genes (which is unrealistic in some sense), or can be structured to all correla-
tions/interactions between genes. We will revisit this issue later in the chapter.
The MCMC sampling proceeds by sampling all parameters of the model condi-
tional on the Zi and then sampling Zi conditional on Yi from a truncated normal
distribution,
P (Zi|Yi = 1, β) ∝P (Yi = 1|Zi, β)P (Zi|β)
= I(Zi > 0)N(X′
iβ, 1).
Analogously, P (Zi|Yi = 0, β) = I(Zi < 0)N(X′
iβ, 1). Efﬁcient sampling algo-
rithms exist to sample from a truncated normal distribution (Robert, 1995).
Conditional on Zi, all the other model parameters (β, σ 2) have closed-form
posteriors (NIG) and can be sampled via the Gibbs sampler.
Future samples can be classiﬁed once we have obtained the posterior samples
for all the model parameters and random variables M = (Z, β, σ 2). Suppose our
training data arise (without loss of generality) from the ﬁrst M samples. Given
the sampled parameters from the posterior distribution based on our training data,
we sample ZM+1. If the estimated ZM+1 > 0 then YM+1 = 1, and 0 otherwise.
The error rates can be estimated via standard methods: misclassiﬁcation error
rates and cross-validation. We delve into this issue in Section 5.6.
In the context of microarrays, as with most classiﬁers, it is imperative to
reduce the dimensionality of the model space before ﬁtting the classiﬁers. Once
can pre-select a certain number of genes based on some criteria (e.g. two-sample
t-tests and selecting genes that are above a threshold deﬁned by FDR-based cut-
offs) before ﬁtting the above classiﬁer. The disadvantage of this approach, as
already mentioned, is that the uncertainty in the selection is not carried for-
ward in the classiﬁcation mechanism. Bayesian hierarchical machinery gives
a coherent way to do this by adding another (gene) selection layer as a part
of the general classiﬁer building process. Following Lee et al. (2003) a Gaus-
sian mixture prior for β enables us to perform a variable selection procedure.
Deﬁne γ = (γ 1, . . . , γp), such that γi = 1 or 0 indicates whether the ith gene
is selected or not (βi = 1 or 0). Given γ , let βγ = {βi ∈β : βi ̸= 0} and Xγ be
the columns of X corresponding to βγ . The prior distribution on βγ is taken as
N(0, c(XT
γ Xγ )−1) with c as a positive scalar (usually between 10 and 100 (see
Smith and Kohn, 1996). The indicators γi are assumed to be a priori independent
with P (γi = 1) = πi, which are chosen to be small to restrict the number of

BAYESIAN CLASSIFICATION FOR MICROARRAY DATA
79
genes. The Bayesian hierarchical model for gene selection can then be summa-
rized as
Z|βγ , γ ∼N(Xγ βγ , σ 2),
βγ |γ ∼N(0, c(XT
γ Xγ )−1),
γi ∼Bernoulli(πi).
The relative importance of each gene can be assessed by the frequency of its
appearance in the MCMC samples, i.e. the number of times the corresponding
component of γ is 1. This gives us an estimate of the posterior probability of
inclusion of a single gene as a measure of the relative importance of the the gene
for classiﬁcation purposes.
5.5
Bayesian Nonlinear Classiﬁcation
Although conceptually simple and easily implementable, the methods outlined in
the previous sections assume a linear relationship between the response and pre-
dictor, i.e. Zi = X′
iβ + ϵi. There are two potential disadvantages of this approach,
especially for microarray data: ﬁrst, they fail to capture nonlinear relationships,
if any, present in the data; and second, the genes are assumed independent (addi-
tive) in the classiﬁcation scheme. As with many biological data and especially
with gene expression data, this might be a potentially restrictive assumption. We
review two methods here that relax each of these assumptions via a nonlinear
modeling framework.
5.5.1
Classiﬁcation using Interactions
We shall use the same notation as above, where Y represents the categorical
response (class labels) and X is the gene expression matrix. Our objective is
to use the training data Y = (Y1, . . . , Yn)T to estimate P (X) = P (Y = 1|X) or
alternatively (say) the logit function f (X) = log[P (X)/(1 −P (X))]. One could
potentially use any link function as mentioned before, but we use the logistic
link function to aid interpretability, as we shall see later in the section.
As before, we assume that the Yi are independent Bernoulli random vari-
ables with P (Yi = 1) = πi, so that P (Yi|πi) = πYi
i (1 −πi)1−Yi. We construct a
hierarchical Bayesian model for classiﬁcation, again exploiting the latent variable
mechanism explained in the previous section. Write πi = exp(Zi)/[1 + exp(Zi)],
where the Zi are the latent variables introduced in the model to make the Yi
conditionally independent given the Zi. Zi is related to f (Xi) by
Zi = f (Xi) + ϵi,
(5.2)
where Xi is the ith row of the gene expression data matrix X (vector of gene
expression levels of the ith sample) and the ϵi are residual random effects. The

80
BAYESIAN ANALYSIS OF GENE EXPRESSION DATA
residual random effects account for the unexplained sources of variation in the
data, most probably due to explanatory variables (genes) not included in the
study.
The key here is modeling the unknown function f (X) since it is of potentially
very high dimension. We would want f to ﬂexible enough to aid accurate pre-
diction and at the same time have a parsimonious structure to aid interpretation.
A ﬂexible class of models is accorded via nonparametric regression techniques
using basis function expansions,
f (Xi) =
k

i=1
βjB(Xi, θj),
where β are the regression coefﬁcients for the bases B(Xi, θj), which are nonlin-
ear functions of Xi and θ. There are various choices of basis functions depending
on the scientiﬁc question at hand. Examples include regression splines, wavelets,
artiﬁcial neural networks, and radial bases; see Denison et al. (2002) for a
detailed exposition of choices of basis function. An attractive alternative for
high-dimensional data is provided by the multivariate adaptive regression spline
(MARS) basis function proposed by Friedman (1991). MARS is a popular method
for ﬂexible regression modeling of high-dimensional data. Although originally
proposed for continuous responses, it has been extended to deal with categorical
responses by Kooperberg et al. (1997) and in the Bayesian framework by Holmes
and Denison (2003).
The function form of the basis function f (X) is given by
f (Xi) = β0 +
k

j=1
βj
zj

l=1
(Xidjl −θjl)qjl,
(5.3)
where k is the number of spline basis, β = {β1, . . . , βk} are the set of spline
coefﬁcients (or output weights), zj is the interaction level (or order) of the jth
spline, θjl is a spline knot point, djl indicates which of the p predictors (genes)
enters into the lth interaction of the jth spline, djl∼∈∼{1, . . . , p}, and qjl
determines the orientation of the spline components, qjl ∈{+, −} where (a)+ =
max(a, 0), and (a)−= min(a, 0).
Although apparently complex, the MARS basis function has several advan-
tages in a microarray context:
1. The term zj determines the order of interaction for a given gene set. Hence
genes can enter the MARS basis as a main effect (linear term, zj = 1 ) or
as a bivariate interaction (product term, zj = 2). Higher-order interactions
are also possible (zj = 3, 4, . . .) but are not usually considered due to
computational issues and lack of clear interpretability. Thus, depending
on k, the number of genes (sets) the MARS model is an additive function
of main effect and bivariate interaction terms. This feature models not

BAYESIAN CLASSIFICATION FOR MICROARRAY DATA
81
only the interactions between the genes but also could could capture the
potential nonlinear relationship between the predictor and response.
2. Since the MARS model clearly delineates the relationship between the pre-
dictors and the response, this results in much more interpretable models as
compared to, say, black box techniques such as artiﬁcial neural networks.
Such rule-based methods for classiﬁcation are very useful when determin-
ing the effect of individual genes on the response, which in microarrays
are usually the odds of having cancer or not.
3. Since the number of splines (genes) is not ﬁxed a priori, variable (gene)
selection is built within this framework, thus discerning relevant genes for
classiﬁcation.
4. The MARS model not only captures an interaction between the genes (if
any) but also the functional form of the interaction in a ﬂexible manner
by positing a spline structure on the product(s) of genes.
Although the MARS model comes with many advantages, MCMC sampling
is not trivial: ﬁrst, the model has a lot of parameters; and second, standard MCMC
algorithms such as the Gibbs and Metropolis–Hastings samplers are not appli-
cable here since the model dimension is not ﬁxed: the number of genes/splines
entering the model changes at each iteration of the MCMC chain. The ﬁrst
drawback can easily the handled via the latent variable framework, as in (5.2)
that reduces most of the conditional distributions, especially of the MARS basis
parameters, to a standard form. For the latter one needs a transdimensional algo-
rithm such as the reversible jump MCMC method (Green 1995) for sampling
from the posterior distribution.
The priors on the model parameters are as follows. A Gaussian prior is
assumed for β with mean 0 and variance . The form of  is usually taken
to be diagonal to aid parsimony in an already overparameterized model. Specif-
ically  = σ 2D−1, where D ≡diag(λ1, λ, . . . , λ) is an (n + 1) × (n + 1) diag-
onal matrix. We ﬁx λ1 to a small value, amounting to a large variance for the
intercept term, but keep λ unknown. We assign a inverse-gamma prior to σ 2 and
a gamma prior to λ with parameters (γ1, γ2) and (τ1, τ2), respectively. The prior
structure on the MARS model parameters is as follows. The prior on the individ-
ual knot selections θjl is taken to be uniform over the n data points P (θjl|djl) =
U(x1djl, x2djl, . . . , xndjl), where djl indicates which of the genes enter our model
and p(djl) is uniform over the p genes, P (djl) = U(1, . . . , p). The prior on the
orientation of the spline is again uniform, P (qjl = +) = P (qjl = −) = 0.5. The
interaction level in each spline has a prior, P (zj) = U(1, . . . , zmax), where zmax
is the maximum level of interaction set by the user. Finally, the prior on k, the
number of splines, is taken to an improper one, P (k) = U(1, . . . , ∞), which
indicates no a priori knowledge on the number of splines. Hence, the model now
has only one user deﬁned parameter, zmax, the maximum level of interaction,
which is usually set to 2, as mentioned earlier in this section.

82
BAYESIAN ANALYSIS OF GENE EXPRESSION DATA
As already suggested, conventional MCMC methods such as the Metropolis–
Hastings algorithm (Metropolis et al., 1953; Hastings, 1970) are not applicable
here since the parameter (model) space is variable: we do not know the number
of splines (genes) a priori. Hence, we use the variable dimension reversible jump
algorithm outlined in Green (1995) and modiﬁed for our framework by Denison
et al. (2002). Speciﬁcally the chain is updated using the following proposals with
equal probability: (1) add a new spline basis to the model; (2) remove one of the
k existing spline bases from the model; (3) alter an existing spline basis in the
model (by changing the knot points). Conditional on k, efﬁcient sampling can
then be achieved by the use of latent variables. For more details of the procedure,
see Baladandayuthapani et al. (2006).
5.5.2
Classiﬁcation using Kernel Methods
Another closely related nonlinear classiﬁer is based on the Bayesian reproducing
kernel Hilbert spaces (RKHS) approach (Aronszajn, 1950; Parzen, 1970) pro-
posed by Mallick et al. (2005). RKHS are especially useful for microarray data
since they project the feature space into a space of dimension much less than p,
the number of genes. Classical RKHS use decision-theoretic framework with no
explicit probability model, hence it is not possible to assess the uncertainty of
both the classiﬁer and the predictions based on it. Mallick et al. (2005) propose a
full probabilistic model-based approach to RKHS classiﬁcation which has several
advantages, as explained below.
Consider the binary classiﬁcation problem with two classes. The data con-
struct contains (Y, X), where Y ∈{0, 1} codes the class labels from n training
samples and X is the n × p matrix of gene expression values. Classiﬁcation pro-
ceeds by specifying a probability model on P (Y|X). As before, introduce latent
variables Z = (Z1, . . . , Zn), conditional on which the P (Y|Z) = (n
i=1 P (Yi|Zi),
i.e. the Yi are conditionally independent given the Zi. The latent variables are
then linked to covariates X by Zi = f (Xi) + ϵi for i = 1, . . . , n, where Xi is
the gene expression vector associated with the ith array. To complete the model
speciﬁcation one needs to specify P (Y|Z) and f .
To model P (Y|Z), one can exploit the duality between loss and likelihood,
whereby the loss can be viewed as the negative of log-likelhood. If the loss
function is deﬁned as l(y, Z), then minimizing this loss function corresponds to
maximizing exp{−l(y, z)}, which is proportional to the likelihood function. There
are various choices of loss and corresponding likelihood functions available.
As in the Bayesian MARS (BMARS) classiﬁer, the key here again is modeling
the high-dimensional function f (X), to keep it ﬂexible and at the same time
admit parsimony. This can achieved by modeling f via an RKHS approach. A
Hilbert space H is a collection of functions on a set T with an associated inner
product ⟨g1, g2⟩and the norm ∥g1∥= ⟨g1, g2⟩for g1.g2 ∈H. An RKHS H with
reproducing kernel K is a Hilbert space having an associated function K on
T × T with the properties

BAYESIAN CLASSIFICATION FOR MICROARRAY DATA
83
(a) K(·, X) ∈H,
(b) ⟨K(·, X), g(·)⟩= g(X) for all X ∈T and for every g in H,
where K(·, X) and g(·) are functions that are deﬁned on T with values X∗∈T
equal to K(X∗, X) and g(X∗), respectively. The reproducing kernel function
provides a basic building block for H via the following lemma (shown here
without proof) from Parzen (1970).
Lemma 5.1. If K is a reproducing kernel for the Hilbert space H, then {K(, ·, X)}
spans H.
There are two important consequences of this lemma. First, any g ∈H with
reproducing kernel K can approximated to any desired level of accuracy via
a dense function in H given by gN(·) = N
j=1 βjK(·, x)j), where xj ∈T for
each j = 1. . . . , N. Second and more importantly, for gene expression data, we
achieve dimension reduction as follows. Given the expression vectors X1, . . . , Xn
and Z the latent variables, let f ∈H with choices of K discussed below. The
optimal f is based on minimizing n
i=1{zi −f (Xi)}2 + ∥f ∥2 with respect to f
which must admit the representation (Wahba, 1990)
f (·) = β0 +
n

j=1
βjK(·, Xj).
(5.4)
This reduces the dimensionality of the optimization to n, which is far less
than p, the number of genes. Inference on f
is made via inference on
β = (β0, . . . , βn)T . The choice of kernels can be ﬂexible by allowing them to
depend on unknown parameters. Classical choices include the Gaussian kernel
K(Xi, Xj) = exp{−∥Xi −Xj∥2/θ} and the polynomial kernel K(Xi, Xj) =
{(Xi.Xj) + 1}θ, where X.Y denotes the inner product of two vectors X and Y.
We now show how we can exploit the above framework to achieve clas-
siﬁcation via Bayesian suport vector machines (SVMs). We refer the reader to
Cristianini and Shawe-Taylor (2000), Sch¨olkopf and Smola (2002) and Herbrich
(2002) for the basics of SVMs. To follow notation, suppose the class labels
are coded as Yi = 1 and Yi = −1 for the two classes. The basic idea behind
SVMs is to ﬁnd a linear hyperplane that best separates the observations with
Yi = 1 from those with Yi = 1 called the margin. As shown by Wahba (1999)
or Pontil et al. (2000), this optimization problem amounts to ﬁnding β so as to
minimize ∥β∥2 + C n
i=1[1 −Yif (Xi)]+ where [a]+ = a if a > 0 and [a]+ = 0
otherwise. C ≥0 is a penalty term and f is given by (5.4). The problem can be
solved by using nonlinear programming methods. Fast algorithms for comput-
ing SVM classiﬁers can be found in Chapter 7 of Cristianini and Shawe-Taylor
(2000).
In the Bayesian paradigm, this optimization problem is equivalent to ﬁnd-
ing the posterior mode of β, where the likelihood is given by exp[−n
i=1[1 −
Yif (Xi)]+ with a N(0, CIn+1) prior on β where we exploit the loss–likelihood

84
BAYESIAN ANALYSIS OF GENE EXPRESSION DATA
duality mentioned above. With the formulation based on latent variables Z, the
density can be written as
P (Y|Z) ∝exp
)
−
n

i=1
[1 −YiZi]+
*
,
(5.5)
with the prior on Zi ∼N{f (Xi), σ 2}. Writw (5.4) in matrix notation as f (Xi) =
K′
iβ, where K is a positive deﬁnite function of the covariates/genes X and some
unknown parameter θ. A Gaussian prior is assumed over β with mean 0 and
variance σ 2D, where D ≡diag(λ1, λ, . . . , λ) is of size n × n. The precision λ1
of the intercept term is set to small number, corresponding to a large variance. A
proper uniform prior is assumed over θ, a conjugate inverse-gamma prior over
σ 2 and λ−1.
Note, however, that the density in (5.5) may involve Z. Following Sollich
(2001), one may bypass this problem by assuming a distribution for Z such that
the normalizing constant cancels out. If the normalized likelihood is is written
as exp

−n
i=1[1 −YiZi]+

/c(Z), where c(·) is the normalizing constant, then,
choosing P (Z) ∝Q(Z)c(Z), the joint distribution turns out to be
P (Y|Z) ∝exp
)
−
n

i=1
[1 −YiZi]+
*
Q(Z),
as c(·) cancels from the expression. A simple choice of Q(Z) is a product of
independent normal probability density functions with means f (Xi) and common
variance σ 2. Mallick et al. (2005) refer to this method as Bayesian support vector
machine (BSVM) classiﬁcation. We refer the reader to this article for further
details.
5.6
Prediction and Model Choice
The main goal of classiﬁcation is the prediction of new samples. In the Bayesian
context this can be achieved via the posterior predictive densities. For a new
sample with gene expression matrix Xnew, the marginal posterior distribution of
the new sample Ynew is given by
P (Ynew = 1|Xnew) =
	
M
P (Ynew = 1|Xnew, M, Y)P (M|Y)dM,
(5.6)
where M contains all the unknowns in the model, P (M|Y) is the posterior prob-
ability of the unknown random variables and parameters in the model. Assuming
conditional independence of the responses, the integral in (5.6) reduces to
	
M
P (Ynew = 1|Xnew, M)P (M|Y)dM,
(5.7)

BAYESIAN CLASSIFICATION FOR MICROARRAY DATA
85
with
the
associated
measure
uncertainty
being
P (Ynew = 1|Xnew, Y){1 −
P (Ynew = 1|Xnew, Y)}. The integral given in (5.8) is computationally and
analytically intractable and needs approximation procedures. We approximate it
by its Monte Carlo estimate,
P (Ynew = 1|Xnew) = 1
m
m

j=1
P (Ynew = 1|Xnew, M(j)),
(5.8)
where M(j) ∼for ∼j = 1, . . . , m are the m MCMC posterior samples of the
model unknowns M. The approximation (5.8) converges to the true value (5.7)
as m →∞.
In order to select from different models/classiﬁers, we generally use mis-
classiﬁcation error. When a test set is provided, we ﬁrst obtain the posterior
distribution of the parameters based on training data, Ytrn (train the model), and
use them to classify the test samples. For a new observation from the test set,
Yi,test we will obtain the probability P (Yi,test = 1|Ytrn, Xi,test) by using the approx-
imation to (5.8) given by (5.7). If there is no test set available, we will use a
hold-one-out cross-validation approach. We follow the technique described in
Gelfand (1996) to simplify the computation. For the cross-validation predictive
density, in general, let Y−i be the vector of Yjs without the ith observation Yi,
P (Yi|Y−i) =
P (Y)
P (Y−i) =

	
{P (yi|Y−i, M)}−1P (M|Y)dM
−1
.
The MCMC approximation to this is

P (Yi|Y−i,trn) = m−1
m

j=1

P (yi|Y−i,trn, M(j))
−1,
where M(j) ∼for ∼j = 1, . . . , m are the m MCMC posterior samples of the
model unknowns M. This simple expression is due to the fact that the Yi are
conditionally independent given the model parameters M. If we wish to make
draws from p(Yi|Yi,trn), then we need to use importance sampling.
5.7
Examples
We illustrate some of the methods discussed above with some real experi-
mental data. We use the microarray data set used in Hedenfalk et al. (2001),
on breast tumors from patients carrying mutations in the predisposing genes,
BRCA1 or BRCA2, or from patients not expected to carry a hereditary predis-
posing mutation. Pathological and genetic differences appear to imply different
but overlapping functions for BRCA1 and BRCA2. They examined 22 breast
tumor samples from 21 breast cancer patients, and all patients except one were
women. Fifteen women had hereditary breast cancer, 7 tumors with BRCA1 and

86
BAYESIAN ANALYSIS OF GENE EXPRESSION DATA
8 tumors with BRCA2. For each breast tumor sample 3226 genes were used. We
use our method to classify BRCA1 versus the others (BRCA2 and sporadic). As
a test data set is not available, we have used a full leave-one-out cross-validation
test and use the number of misclassiﬁcations to compare the various approaches.
Table 5.1 summarizes our results. BLDA stands to Bayesian linear discriminant
analysis, and BPR for Bayesian probit regression.
Table 5.1
Model misclassiﬁcation errors using hold-one-out
cross-validation for breast cancer data
Model
Number of
misclassiﬁes samples
BLDA
1
BPR
0
BMARS
0
BSVM
0
Probabilistic neural network (r = 0.01)
3
k nearest neighbors (k = 1)
4
SVM linear
4
Perceptron
5
We compare the cross-validation results with other popular classiﬁcation algo-
rithms including feed-forward neural networks (Williams and Barber, 1998), k
nearest neighbours (Fix and Hodges, 1951), classical SVMs (Vapnik, 2000), per-
ceptrons (Rosenblatt, 1962) and probabilistic neural networks (Specht, 1990) in
Table 5.1 All these methods have used expression values of only 51 genes as used
in Hedenfalk et al. (2001), thus pre-selecting the genes in a two-step approach.
Note that the Bayesian methods perform better than the other methods, at least
for this example. Moreover, the regression-based approaches achieve almost per-
fect classiﬁcation for this data set, while BLDA misclassiﬁes one sample. When
we used BLDA without a variable selection procedure it performed poorly, mis-
classifying four samples, thus emphasizing the importance of feature selection
before classiﬁcation for microarray data. As an example we present a list of top
25 genes that were selected using the BMARS model in Table 5.2. The list is
sorted by the number of times the gene appears in the MARS basis function in
the MCMC sampler – which gives some indication of the relative importance of
each gene. The asterisk (*) corresponds to the common genes that are selected
using a BPR. Another nice feature of Bayesian approaches is that as a byprod-
uct we also get an (honest) estimate of the uncertainty in classiﬁcation. For
example, with BSVM procedure the 95% credible interval for misclassiﬁcation
was (0, 3).

BAYESIAN CLASSIFICATION FOR MICROARRAY DATA
87
Table 5.2
Breast cancer data: top 25 genes from the BMARS model. The
asterisk(*) corresponds to the common genes that appear by using a linear
probit classiﬁer
Image
Clone ID
Gene description
767817
polymerase (RNA) II (DNA directed) polypeptide F
307843
ESTs (*)
81331
‘FATTY ACID-BINDING PROTEIN, EPIDERMAL’
843076
signal transducing adaptor molecule (SH3 domain and ITAM
motif) 1
825478
zinc ﬁnger protein 146
28012
O-linked N-acetylglucosamine (GlcNAc) transferase
812227
‘solute carrier family 9 (sodium/hydrogen exchanger), isoform 1’
566887
heterochromatin-like protein 1 (*)
841617
ornithine decarboxylase antizyme 1 (*)
788721
KIAA0090 protein
811930
KIAA0020 gene product
32790
‘mutS (E. coli) homolog 2 (colon cancer, nonpolyposis type 1)’
784830
D123 gene product (*)
949932
nuclease sensitive element binding protein 1 (*)
26184
‘phosphofructokinase, platelet’ (*)
810899
CDC28 protein kinase 1
46019
minichromosome maintenance deﬁcient (S. cerevisiae) 7 (*)
897781
keratin 8 (*)
32231
KIAA0246 protein (*)
293104
phytanoyl-CoA hydroxylase (Refsum disease) (*)
180298
protein tyrosine kinase 2 beta
47884
macrophage migration inhibitory factor (glycosylation-inhibiting
factor)
137638
ESTs (*)
246749
‘ESTs, weakly similar to trg [R. norvegicus]’
5.8
Discussion
Since the explosion of microarrays in the past decade there have been many
approaches to classiﬁcation using gene expression proﬁling. We have reviewed
some of recent developments in Bayesian classiﬁcation of gene expression
data, both from a linear and a nonlinear perspective, with special emphasis on
regression-based methods for classiﬁcation. We also hasten to point out that
there is a growing literature on classiﬁcation methods from both a Bayesian
and frequentist standpoint for microarray data. Some additional methods not
covered in this book include the classiﬁcation methods based on Bayesian tree
models (Ji et al., 2006), factor regression models (West, 2003), and Bayesian
nonparametric approaches based on Dirichlet process priors.

88
BAYESIAN ANALYSIS OF GENE EXPRESSION DATA
We have treated the binary classiﬁcation in detail. When the response is
not binary, such that the number of classes (C) is greater than two, then the
problem becomes a multiclass classiﬁcation problem. This can be handled in
a manner similar to the binary classiﬁcation approach, as follows. Let Yi =
(Yi1, . . . , YiC) denote the multinomial indicator vector with elements Yiq = 1 if
the qth sample belongs to the qth class and Yij = 0 otherwise. Let Y denote
the n × C matrix of these indicators. The likelihood of the data given the model
parameters (1, . . . , C) is given by
P (Yi = 1|Xi) = πyi1
1 πyi2
2
. . . πyiC
C ,
where πq is the probability that the sample came from class q. This is modeled
in a similar manner to the binary class case as discussed above.

6
Bayesian Hypothesis Inference
for Gene Classes
6.1
Interpreting Microarray Results
In Chapter 3, we discussed the many challenges of detecting differentially
expressed genes. The Bayesian approach provides very powerful and often
superior
methods
for
inferring
differential
expression.
Gene
expression
experiments generate an overwhelming quantity of expression measurements on
typically thousands of genes. Producing a interesting list of genes associated
with a phenotype or outcome is an important accomplishment, but by no means
ﬁnal. The results must be interpreted in such a way that biological conclusions
can be drawn from the experiment. Here we adopt the strategy of integrating
biological knowledge and experimental design in a powerful but ﬂexible way.
Making sense of the biological roles of genes from large gene expression
experiments requires insight into the biological regulation of genes at the molec-
ular level. Drawing biological conclusions from gene expression arrays requires
more than a cursory understanding of the relevant biology. A familiarity with cell
biology, organic chemistry, and biochemistry is helpful. Suppose that a particu-
lar expression pattern is observed in a series of bio-markers in a known relevant
pathway. Genes showing similar experimental proﬁles, it has been hypothesized,
perform related functions in the cell. In practice this hypothesis may or may not
be true, on a case by case basis. In other words, genes can show high association
in expression, and be physically unrelated, or so dissociated through a chain of
molecular events as to negate any meaningful biological associations. The con-
verse can also be true. Gene expression can be highly physically dependent, but
Bayesian Analysis of Gene Expression Data
B. Mallick, D. Gold, and V. Baladandayuthapani
2009 John Wiley & Sons, Ltd

90
BAYESIAN ANALYSIS OF GENE EXPRESSION DATA
in ways so nontrivial that it is unlikely that a naive co-expression analysis will
infer an association. While these facts present difﬁculties, there are nevertheless
beneﬁts to making use of historical prior information.
Commonly available information of regulatory relationships or shared
pathways, i.e. the larger biological context, is used in gene expression analyses
in order to interpret results in a biologically meaningful way. Gene–gene
co-expression, for example, for genes known to be involved in, say, photo-
synthesis or metabolite regulation, is conceptually helpful, more so than, say,
the collection of a few hundred or even thousand gene-wise events, where
little is known about the genes. This implies that working with, albeit limited,
biological information can shift the emphasis of an analysis from a collection
of gene-wise variables to a class of documented biological events, associated
with a particular cellular task or function, potentially leading to discovery of an
important class of epistatic events.
The aim of gene class detection is to perform inference on a collections of
genes, deﬁned by an historical source. Experimental design and biological knowl-
edge are the keys to drawing biologically meaningful and logical conclusions.
We must condition any conclusions on the given biology, as our understand-
ing is incomplete. In this chapter we adopt the strategy of integrating biological
knowledge and experimental design in a powerful but ﬂexible way.
6.2
Gene Classes
Historical information concerning genes and their related biological roles is read-
ily available in the form of on-line databases that can be mined with sophisticated
queries. Much of this information is publicly accessible, following academic
and government initiatives to promote accessibility as well as high standards for
analysis of high-throughput biology. There are many ways to utilize this informa-
tion. In this chapter we consider using such information by deﬁning collections
of genes, or biological clusters of genes, based on biological characteristics.
A gene class is here deﬁned as a collection of genes deﬁned to be biologically
associated given a biological reference such as gene ontology, historical path-
way databases, scientiﬁc literature, transcription factor databases, expert opinion,
empirical evidence (e.g. past experiments), and theoretical evidence.
The Gene Ontology (GO) Consortium offers one of the most detailed, com-
plete, and up-to-date sets of gene-annotations available. It consists of 17 members,
each responsible for annotating and curating the gene annotations of a class of
model organisms, ranging from protozoa to Homo sapiens. The GO database is
publicly available at http://www.geneontology.org. The GO annotations consist
of a dense hierarchically structured vocabulary describing three components of
gene products: the biological processes (what), molecular functions (how), and
cellular components (where). The very hierarchical structure of each of the three

BAYESIAN HYPOTHESIS INFERENCE FOR GENE CLASSES
91
components of the language can be visualized as a separate directed acyclic
graph (DAG). The nodes along a single branch of a graph provide multiple
levels of description of each gene products. For example, on the biological pro-
cess DAG, the child nodes under transcription include positive regulation of
transcription and negative regulation of transcription. The annotations are com-
pleted by a mapping of the known genes to membership on the nodes of the GO
DAGs.
In order to facilitate visualization of GO, there are many publicly available
browsers, such as the Cancer Genome Anatomy Project (CGAP) browser at
http://cgap.nci.nih.gov/Genes/GOBrowser. A user can explore the dense vocab-
ulary rooted within the three GO directed acyclic graphs. In practice, analysis of
GO gene classes is performed on pruned versions of the GO hierarchical DAGs.
The vocabulary of the GO categories provides ﬁner detail on the genes, as one
moves down the DAG, while the density of the nodes, i.e. the number of genes
mapped to them, becomes sparser. The highly resolved level of detail, combined
with the sparsity of genes, near terminal nodes, makes their inclusion typically
impractical for analysis. New terminal nodes can be derived by determining the
maximum depth for each branch along the DAG, annotating gene mapping to
nodes beneath the maximum depth to the newly deﬁned terminal leaf nodes.
For example, consider proliferation. If one wishes to deﬁne proliferation as a
terminal node, genes mapped to child nodes of proliferation are assigned to the
proliferation parent class.
Historical pathway databases largely portray the relationships of genes, tran-
scripts and gene products in the form of diagrams and static graphs. The sources
for historical pathways vary from the literature to panels of biological experts.
In contrast to GO, the pathway databases tend to have much lower coverage of
the genome, with much more detailed information about the genes and how they
interact. There are many different kinds of pathways and, as such, many ways
in which genes can be associated in a myriad of processes. Genes in a path-
way can perform very different roles triggered at very different times. Genes,
for example, can be involved in a metabolic pathway, a signaling pathway, or a
cascade response to external stimuli in the cell. The nature of the way genes can
interact, e.g. direct protein phosphorilation, or DNA binding, makes using such
information complicated, as gene expression arrays measure relative transcrip-
tion levels. It would appear reasonable to build gene classes from transcriptional
regulation as gene regulatory factors can explain variation on arrays (Tamada et
al., 2003; Allocco et al., 2004; Nagaraj et al., 2004; Bhardwaj and Lu, 2005).
Working with gene classes is intuitively a supervised dimension reduction
step, reducing thousands of gene-wise variables to a more sophisticated
and informed source of covariates, reﬂecting historical pathways, biological
processes, or regulatory roles. This reduction might be quite useful, although in
some cases it can be quite awkward. This information may or may not be useful
for microarray analysis, as pathways and processes can be dynamic processes,

92
BAYESIAN ANALYSIS OF GENE EXPRESSION DATA
while microarrays provide a one-time snapshot of relative gene expression.
Incorporating extra network structure in an analysis is a nontrivial tasks
requiring sensible assumptions. Despite the enormity of past data, and the prior
information on gene classes and historical pathways, there are limitations to
using this information. While complex, the annotations are incomplete, the
implication being that any conclusions must be conditioned upon the current
state of progress in annotating the genome. In some organisms, such as yeast
and E. coli, the coverage of the genomes is high, while in others, such as
H. sapiens, the coverage is far poorer.
6.2.1
Enrichment Analysis
We begin our discussion of gene class detection, taking as given that gene classes
are well deﬁned. Membership in a gene class does not imply co-regulation, or
necessarily suggest likely co-expression. We understand that some genes within
a gene class can show differential expression, conditional upon the experimen-
tal factors, while others do not. What makes gene classes useful is the degree
of evidence provided among the genes assigned to the class, that a particular
regulatory process or function is activated/deactivated across experimental fac-
tors. Each gene provides a vote of evidence in favor of or not in favor of the
hypothesis that the process or pathway represented by the gene class is biologi-
cally somehow affected by or part of a response to the experimental stimulus or
conditions.
Genes can be members of more than one gene class, and therefore statistical
measures of differential expression among gene classes can be dependent. Gene
class statistics can be dependent for other reasons, such as biology. Another
very important consideration is that gene class membership is incomplete. The
conclusions of enrichment analysis must be conditioned upon on the annotations,
lacking further knowledge.
Suppose a gene expression microarray experiment is conducted to compare
gene expression in two groups, experimental and control. Let b be a known class
of genes, such as a speciﬁc GO biological function. Of the the G unique genes
on the array, some proportion show signiﬁcant differential expression. Table 6.1
shows the counts of genes that are members of gene class b by differential
expression.
Table 6.1
Enrichment analysis table, gene class b
Differentially
Not differentially
expressed
expressed
Total
Member of b
G11
G12
G1·
Not a member of b
G21
G22
G2·
Total
G·1
G·2
G

BAYESIAN HYPOTHESIS INFERENCE FOR GENE CLASSES
93
Enrichment analysis seeks to test the hypotheses
H (b)
0 : The differentially expressed genes were sampled i.i.d. uniformly at
random without replacement from the G genes on the array,
H (b)
1 : The differentially expressed genes were sampled from gene class b in
greater proportion than G11/G,
i.e. the the set of differentially expressed genes is ‘enriched’ with genes from class
b (Mootha et al., 2003). Following the assumption under H0 that the differentially
expressed genes were sampled i.i.d. uniformly at random without replacement
from the G genes on the array, the probability of observing G11 genes from
class b changing is given by the hypergeometric distribution. The p-value for
testing H0 is calculated by the probability of observing a count greater than or
equal to G11. This is the procedure that R. A. Fisher (1922) proposed for testing
one-way independence of counts in 2×2 contingency tables, the one-way Fisher
exact test.
There are many programs available that perform enrichment analysis, among
them GO Miner and GOsurfer (for a review, see Curtis et al., 2005). These
applications are equipped with sophisticated graphical user interfaces, allowing
one to view the results of an enrichment analysis displayed on directly on a GO
DAG, with color codes to indicate up or down regulation.
Gene set enrichment analysis (GSEA; Subramanian et al., 2005), extends
the ideas of enrichment analysis, incorporating information about the empiri-
cal distribution of the gene-wise test statistics. Suppose that an experiment is
conducted to compare expression between two groups. In GSEA, test statistics
such as t-statistics are computed for each gene. The distribution of t-statistics
of genes in each gene class are compared to the distribution of t-statistics in
the gene class compliment, by what is essentially a Kolmogorov–Smirnov score
statistic. A measure of signiﬁcance is derived by permuting the subject (array)
labels in each experimental group. Gene-wise randomizations have also been sug-
gested by Kim and Volsky (2005), leading to a parametric computational shortcut,
although they can perturb gene–gene correlations. Efron and Tibshirani (2006)
proposed a more powerful alternative to the Kolmogorov–Smirnov score statistic
for inferring gene set enrichment, the maxmean statistic, which is computed as
the larger of the absolute value of the average of positive t-statistics or negative
t-statistics, for genes in a gene class. As for GSEA, Efron and Tibshirani use
a permutation-based approach to derive the null. In order to appropriately account
for variation in the null, both the gene class maxmean statistics, and permuted
gene class statistics, are normalized, subtracting the respective means and divid-
ing the respective standard deviations, before comparison to derive p-values, a
procedure Efron and Tibshirani call restandardization. Another recent Bayesian
alternative is provided by Newton et al. (2007).
A multivariate generalization of enrichment analysis (MEA) was proposed by
Klebanov (2007). Modifying the enrichment hypotheses extensively, Klebanov

94
BAYESIAN ANALYSIS OF GENE EXPRESSION DATA
considers the correlation structure of genes, comparing the joint distribution of
gene proﬁles between phenotypes, or experimental factors, for genes in each a
gene class with what is called the N-statistic. The test is performed, given a set of
genes in a gene class, by comparing the joint multivariate distance of gene proﬁles
between phenotype groups with the average multivariate distance of the gene
proﬁles within each phenotype, in each gene. The N-statistic null distribution is
derived by permuting subject (array) labels. Unlike enrichment analysis, GSEA
and MEA are not performed conditionally upon a set of discovered candidate
genes.
6.3
Bayesian Enrichment Analysis
Historical pathways help facilitate inference on collections of genes that may act
in concert. Utilizing such information can be a complex task for which Bayesian
formalism helps, making prior beliefs explicit as well as challenging them. Sup-
pose that one wishes to draw biological evidence of an association between an
entire pathway of genes and an experimental outcome of interest. Inference is
drawn from synthesizing what we know about the genes involved in the par-
ticular regulatory process or function, i.e. the pathway, with data, observed or
experimental.
In a Bayesian enrichment analysis, the parameters indicating differential
expression are accounted for by posterior uncertainty. Differential gene expres-
sion is inferred with posterior uncertainty, contributing a weight of probability in
favor of enrichment, rather than casting a single vote. Let the set GD be the col-
lection of genes selected for change, Gb be the collection of genes with function
b, and ||G|| be the cardinality of the set G. The variable
ϒb = I(||GD ∪Gb||/||GD|| ≥α)
(6.1)
measures the event that the proportion of differentially expressed genes, GD,
enriched for gene class b, Gb, exceeds a user-speciﬁed α ∈(0, 1) for indica-
tor function I(A), where I(A) = 1 if the event A is true and 0 otherwise. In
Bayesian enrichment analysis, we summarize the posterior distribution of, and
make probability statements about, enrichment through ϒb, e.g. E(ϒb|X).
Consider deﬁning ϒb as
ϒb = I(||GD ∪Gb||/||D|| ≥||Gc
D ∪Gb||/||Gc
D||),
(6.2)
to indicate that the proportion of genes with function b that are selected exceeds
the proportion not selected. This is the deﬁnition adopted by Bhattachajee et al.
(2001), in a study to infer differential gene expression between organ tissues.
One could alternatively deﬁne
ϒb = I(||GD ∪Gb||/||Gb|| ≥||GD||/(||GD|| + ||Gc
D||)).
(6.3)
These deﬁnitions of ϒ obviously address different hypotheses, with (6.2) most
closely resembling the test of enrichment. Accounting for the uncertainty in gene

BAYESIAN HYPOTHESIS INFERENCE FOR GENE CLASSES
95
detection and enrichment of gene classes in probabilistic fashion is quite natural
in a Bayesian setting.
Example 6.1 SCC STUDY
We return to the Wachi et al. (2005) data set consisting of 22 283 expression probe
values from ten patients, ﬁve with SCC (group 1) and ﬁve normal lung patients
(group 2). The likelihood for gene g is deﬁned for the difference in sample
averages ¯xg1 and ¯xg2 for the respective groups, and the appropriate standard
deviation estimate ˆσg, as
¯xg1 −¯xg2 −δg
ˆσg
√(1/5 + 1/5)|δg ∼t8,
(6.4)
a Student’s t-distribution with 8 degrees of freedom, given the unknown mean
log-ratio δg. Suppose that an improper mixture prior is speciﬁed for δg
P (δg|λg) ∝λg1{δg<−1∪δg > 1} + (1 −λg)1{−1≤δg≤1},
P (λg) = Bernoulli
1
2

,
(6.5)
conditional upon the discrete variable λg = 0 indicating the event δg ∈(−1, 1),
i.e. less than twofold differential expression on the original scale, and λg =
1 indicating that δg ∈(−∞, −1) ∪(1, ∞), or greater than twofold differential
expression.
Integrating over δg, the marginal posterior of λg = 0 is Bernoulli, with prob-
ability
πg|xg =
	 1
−1
t8(s; ¯xg1 −¯xg2, ˆσ 2
g (1/5 + 1/5))ds,
(6.6)
the integral of a t random variable from −1 to 1, with location ¯xg1 −¯xg2 and
dispersion ˆσ 2
g (1/5 + 1/5). Deﬁning ϒb as in (6.2), we obtain a the posterior
probability of enrichment, sampling λg for g = 1, . . . , G and at each iteration,
and deriving counts ||Gb||, ||GD||, ||Gc
D|| and ||Gc
b||, and subsequently ϒb. The
gene classes were obtained from GO biological process annotations. The top 25
gene classes, ranked by the posterior expectation of ϒb are listed in Table 6.2.
6.4
Multivariate Gene Class Detection
Multivariate gene class detection seeks to infer differential expression among
genes and gene classes. The collection of methods available for multivariate gene
detection and gene class inference is quite diverse. Some methods incorporate
gene class information in the modeling step, others derive measures of enrichment
from modeling results. The proposed methods can also vary to the degree to

96
BAYESIAN ANALYSIS OF GENE EXPRESSION DATA
Table 6.2
Bayesian enrichment analysis, Wachi data
No. of
Posterior
Gene class
genes
expectation ϒb (6.2)
chloride channel activity
68
1
cytokine activity
126
1
endopeptidase inhibitor activity
122
1
extracellular matrix structural
140
1
constituent
GPI anchor binding
147
1
heme binding
161
1
heparin binding
129
1
integrin binding
85
1
NAD binding
56
1
oxygen binding
59
1
protein binding, bridging
52
1
serine-type endopeptidase inhibitor
89
1
activity
structural constituent of cytoskeleton
155
1
transmembrane receptor protein tyrosine
51
1
kinase activity
chemokine activity
53
0.9999
electron carrier activity
132
0.9999
sugar binding
178
0.9994
G-protein coupled receptor activity
88
0.9992
growth factor binding
65
0.9989
actin ﬁlament binding
79
0.9961
protein kinase inhibitor activity
50
0.994
protein tyrosine phosphatase activity
146
0.9911
kinase inhibitor activity
52
0.9792
ATPase activity, coupled to transmembrane
51
0.9764
movement of substances
lipid transporter activity
68
0.9764
which dependence is assumed between the genes in a gene class, and between
gene classes, inferring differential expression among genes and gene classes in a
cogent fashion.
Barry et al. (2005) were the ﬁrst to apply permutation testing to infer
enrichment of gene classes. A statistic is computed for each gene class, deﬁned
abstractly, that depends on the differential expression of genes assigned to each
gene class. The sample labels are permuted to derive a null distribution for the
gene class statistics, producing a false discovery rate of enrichment for each
class. Van Der Laan and Bryan (2001) proposed the parametric bootstrap, to
perform gene subset selection accounting for covariance between the genes. The
gene subsets can be deﬁned in a general way, using gene class annotations. Lu
et al. (2005) provide an in-depth discussion of multivariate gene detection with

BAYESIAN HYPOTHESIS INFERENCE FOR GENE CLASSES
97
Hotelling’s T 2 statistic. Lu’s approach is not designed speciﬁcally for inference
of gene classes, although it generalizes well for the analysis of gene classes.
Pan (2006) performed stratiﬁed gene detection given GO biological process
annotations. Geoman et al. (2004) derive a score test for inferring the association
of a gene expression in groups of genes with a clinical outcome, where the
outcome is assumed to be a linear function of gene expressions. Wei and Li
(2007) derived a nonparametric regression model for pathway associations with
gene expression data. Liao et al. (2003) developed network component analysis
for learning about gene pathway structure in multivariate data.
More advanced modeling includes group gene detection, given known feature
groups G1, G2, . . . , GK. In contrast to the lasso, group norm penalties ||βGk|| =
(βT
GkβGk)1/2 are speciﬁed over the coefﬁcients for differential gene expression
in group Gk, βGk = (βg)g∈Gk, k = 1, . . . , K (Yuan and Lin, 2006; Zhao and Yu,
2006). The group lasso penalized loss function L(β) with K groups is
L(β) = (β −ˆβOLS)′X′X(β −ˆβOLS) + λk||βGk||.
(6.7)
The group norm can be generalized to include group penalty, with general penal-
ized loss function,
L(β) = (β −ˆβOLS)′X′X(β −ˆβOLS) + λ
K

k=1
+
||ˆβGk||γk
γk
,γ0 ,
(6.8)
for ||βGk||γk
γk = (g∈Gk|βg|γk)1/γk. Different values of γk and γ0 furnish different
degrees of sparsity, either shrinking the coefﬁcients of all variables in a group
simultaneously to zero, or allowing some nonzero coefﬁcients, as well as con-
trolling the ordering by which genes are detected. Ma et al. (2007) applied the
group lasso to gene expression array data.
From a Bayesian point of view, the group lasso solution can be expressed as
the Bayes rule with zero–one loss, Gaussian likelihood, and prior
P (β) ∝Cλ,γ0,γ exp

λ
K

k=1
+
||βGk||γk
γk
,γ0

(6.9)
on β.
Example 6.2 Breast cancer case study
The breast cancer array data set of Farmer et al. (2005) was downloaded from
http://www.ncbi.org/GEO. The R package GSA (Gene Set Analysis), which can
be installed using the R graphical user interface in the usual way, was applied to
detect important GO biological processes, contrasting gene expression in a subset
of the subjects, with basal versus luminal breast cancers. GO gene classes with
at least 50 and no more than 300 genes were included in the analysis, resulting
in 194 gene classes, while reducing the number of genes to 10 825.

98
BAYESIAN ANALYSIS OF GENE EXPRESSION DATA
GSA was applied with a two-class unpaired comparison, using the maxmean
gene class statistic and restandardization, for 200 permutation iterations. The
package reports two permutation p-values for each gene class, for up and down
regulation. For our purposes the GSA p-value pGSA was evaluated as 2 times
the minimum of the p-values for up and down regulation. Additionally, for
comparison, the statistic λmax was computed as the largest value of the penalty
parameter λ in (6.7), such that the gene-wise parameters for change are not
simultaneously zero, using the grplasso package in R. The λmax statistic was
derived for each gene class separately, with the responses for each gene weighted
by an estimate of the respective gene-wise pooled scale. Larger values of λmax
indicate a greater degree of differential gene expression in the class, although they
can also inﬂate in a locally linear fashion with number of genes in a gene class.
The permutation distribution of the λ∗
maxs was derived with 300 permutations
of the subject labels. The λmaxs and permuted λ∗
maxs were detrended separately
by GO gene class gene counts, with ordinary least squares linear regression.
Group lasso permuted p-values were obtained, after standardizing the detrended
λmaxs and λ∗
maxs, respectively, by mean and standard deviation, following the
restandardization procedure of Efron and Tibshirani.
The empirical p-value distributions for both the GSA and group lasso applica-
tions are shown in Figure 6.1. Neither distribution shows any obvious departure
from the uniform null distribution. At a liberal 0.05 p-value level, GO gene
classes were selected: 13 GO classes by GSA and 10 by group lasso, with four
GO classes discovered by both, printed in bold face in Table 6.3.
ECDF
200
150
100
50
0
ECDF
200
150
100
50
0
0.0
0.2
0.4
0.6
0.8
1.0
P–values Group LASSO
0.0
0.2
0.4
0.6
0.8
1.0
P–values GSA
Figure 6.1
Gene set detection, Farmer breast cancer data, empirical p-value
distributions: group lasso (left) and GSA (right).
6.4.1
Extending the Bayesian ANOVA Model
Enrichment analysis can be a very powerful method for inferring biological vari-
ation among gene classes, although it is a heuristic approach, ﬁrst selecting a
list of candidate genes given some experimental factors of interest, and then,

BAYESIAN HYPOTHESIS INFERENCE FOR GENE CLASSES
99
Table 6.3
Gene set detection: Farmer breast cancer data
Group lasso
GSA
1. androgen receptor signaling
pathway
2. chromatin assembly or disassem-
bly
3. chromatin modiﬁcation
4. epidermis development
5. fatty acid metabolic process
6. positive regulation of transcrip-
tion, DNA-dependent
7. positive regulation of transcription
from RNA polymerase II promoter
8. protein amino acid dephosphoryla-
tion
9. regulation of Rho protein signal
transduction
10. Wnt receptor signaling pathway
1. androgen receptor signaling
pathway
2. calcium ion homeostasis
3. chromatin assembly or disassem-
bly
4. chromatin modiﬁcation
5. glycolysis
6. negative regulation of apoptosis
7. nucleotide biosynthetic process
8. nucleotide metabolic process
9. one-carbon compound metabolic
process
10. positive regulation of transcrip-
tion, DNA-dependent
11. protein metabolic process
12. regulation of Rab GTPase activity
13. regulation of small GTPase medi-
ated signal transduction
conditional upon that list of genes, choosing a set of gene classes. Information
about GO biological processes or historical pathways is ignored during gene
detection. While there is uncertainty in prior knowledge, integrating historical
pathways with array data analysis is something that investigators do naturally. In
this section, we discuss a Bayesian multistage linear model for use with microar-
ray data, that incorporates prior information into the analysis. Unlike the ordinary
linear model described in Chapter 3, an additional layer is included in the model,
essentially to borrow strength between genes.
The multistage linear model, with two stages, can be expressed as
y = Xβ + ϵ,
β = Zθ + w,
θ = θ0 + u,
(6.10)

100
BAYESIAN ANALYSIS OF GENE EXPRESSION DATA
where y is a response of gene expression assumed to depend linearly on the
unknown parameters for change β through the known matrix X, an experimental
design matrix, of for example zero–one variables, as in ANOVA. The residual
vector ϵ is distributed as multivariate Gaussian noise with mean 0 and variance
covariance matrix . The vector of unknown coefﬁcients β determines the mag-
nitude of the experimental effects explaining variation in the response y. These
variables are linearly related to the vector θ through the (partially) known asso-
ciation matrix Z. The matrix Z is deﬁned by additional knowledge in order to
form associations between genes (rows) and gene classes (columns), discussed
more fully below. The vector of residuals w is assumed to follow multivariate
Gaussian noise with mean 0 and variance covariance matrix w. A multivariate
normal prior is speciﬁed for θ as MVN (θ0, θ). Information from the top stage
concerning y ﬁlters down to the next stage, accounted for by variation in β, which
in turn is ﬁltered down to the deepest stage, accounted for by variation in θ.
The multistage linear model can be made identiﬁable by including prior infor-
mation about the association matrix Z, and sign constraints on . Suppose that
we truncate  below by δ > 0, a reasonable prior guess at a moderate degree
of differential expression considered biologically meaningful. Assume that the
(g, c)th element of Z is a priori distributed as
zgc =



−ag,
with probability π1,
0,
with probability π2,
ag,
with probability π3,
(6.11)
for agc = 0 if gene g is not a member of class b, and agc = 1/κg if gene g is a
member of gene class b, where κg equals the number of gene classes of which
gene g is a member. Genes that are associated, as depicted by Z, are not necessar-
ily expected to show co-expression. In a related context, this is accomplished by
Bayesian mixture modeling, where the elements of Z identify the mixture compo-
nents. Distinguishing the effects of multiple pathways on the gene expression of
a single gene g is not feasible, without strong prior information. During Markov
Chain Monte Carlo (MCMC) updating, at iteration c, if gene g is updated to be
in a state of differential expression, all of the respective elements of Z associated
with pathways, i.e. gene classes, for which gene g is a member, are updated with
non-zero values. See Appendix B for an introduction to Bayesian Computational
Tools.
The motivation for the group lasso, and group feature selection algorithms
like it, lies in the strictly sparse solutions that these algorithms provide. Consid-
ering the risk, these algorithms furnish a shrinking effect, shrinking the unknown
parameter estimates that are close to zero to zero, with the possibility of shrink-
ing coefﬁcients of features simultaneously to zero in the same group. While the
multistage model does not enforce sparse solutions, it does shrink coefﬁcient
vectors in a convenient way. Genes that are not differentially expressed tend to
have expression coefﬁcients shrunk to zero, while differentially expressed genes
tend to have coefﬁcients for change that are shrunk to Zθ. The bth component of

BAYESIAN HYPOTHESIS INFERENCE FOR GENE CLASSES
101
the random variable θ, θb, represents the degree of differential expression in gene
class b. If genes in gene class b exhibit moderate and consistent changes, for
example, then θb will tend to be moderate, albeit with high precision. Unlike the
previous models discussed, both a shrinking effect to the null, and a borrowing
effect between differentially expressed genes, are possible with the multistage
linear model.
Example 6.3 SCC Study
The Wachi data from case study 3.6 were analyzed with the multistage model to
infer enrichment of GO biological processes. The goal is to learn if the regulation
of any important biological functions are associated with SSC GO biological
process classes with at least 50 and no more than 200 genes on the array were
included in the analysis, resulting in 116 gene classes and 7748 genes. Log gene
expression for each gene was mean centered, ˜yg = yg −1n ¯yg, modeled as
˜yg = Xβg + ϵg,
ϵg ∼N(0, σ 2
g In),
σ −2
g
∼Gamma(a0/2, b0/2),
(6.12)
for each gene, with X = 15 ⊗(−1, 1)T . A priori, the vector β = (β1, β2, . . .) was
modeled as
β ∼N(Zθ, W)
(6.13)
with Z the 7748 × 116 stochastic association matrix, deﬁned as above, and  =
diag(σ 2
1 , . . . , σ 2
G) for user-deﬁned W. The prior distributions for the zgc were
deﬁned as in (6), with π1 = π2 = π3 = 1/3. The parameter θ is distributed a
priori as
θ ∼N(1116θ0, )I(θ > δ),
(6.14)
with δ > 0.5 reﬂecting the desire to detect changes in gene expression of mag-
nitude at least 2 on the original scale. The diagonal hyperparameter matrix
 = τdiag(κ−1
1 , κ−1
2 , . . .) is deﬁned by a user constant τ divided by the number
of genes in each gene class, along the main diagonal. Several combinations of
hyperparameters, W = 1, 3, 5, 7 and τ = 0.05, 0.1, were ﬁtted to the data, index-
ing different models m1, . . . , m8. The best combination of W and τ was chosen
by the Bayesian model selection criterion, maximizing log P (Y|mj) (Figures 6.2
and 6.3).
Having reduced the number of gene classes to 116 and the number of genes
to 7,748, the remaining genes were used to determine the hyperparameters a0, b0
and θ0. The median sample variance among the remaining genes was 0.0357;
hyperparameters were set to a0 = 5 and b0 = 0.15, reﬂecting moderately strong
beliefs. The sample estimate of the absolute mean change |β| in gene expression,
given that |β| > 0.5, was 0.77, and therefore we set θ0 = 0.77.

102
BAYESIAN ANALYSIS OF GENE EXPRESSION DATA
2
1.5
1
0.5
0
−0.5
−1
−1.5
−2
EDATA(b)
1.0
0.8
0.6
0.4
0.0
0.2
EDATA(s2)
Θ
−2
−1.5
−1
−0.5
0
0.5
1
1.5
2
0.0
0.2
0.4
0.6
0.8
1.0
bOLS
∧
1.0
0.5
0.0
−0.5
−1.0
EDATA(Z)
−2
−1
0
1
2
3
EDATA(b)
S2
pooled
0.8
0.7
0.6
0.5
10 20 30 40 50 60 70 80 90 100 110
Gene Class
Figure 6.2
Enrichment Analysis of the Wachi Experiment with Bayesian Multistage Linear Model Analysis, W = 7 and
τ = 0.05.

BAYESIAN HYPOTHESIS INFERENCE FOR GENE CLASSES
103
2
1.5
1
0.5
0
−0.5
−1
−1.5
−2
EDATA(b)
1.0
0.8
0.6
0.4
0.0
0.2
EDATA(s2)
Θ
−2
−1.5
−1
−0.5
0
0.5
1
1.5
2
0.0
0.2
0.4
0.6
0.8
1.0
bOLS
∧
1.0
0.5
0.0
−0.5
−1.0
EDATA(Z)
−1
0
1
2
EDATA(b)
S2
pooled
0.9
0.8
0.7
0.6
0.5
10
20
30
40
50
60
70
80
90 100 110
Gene Class
Figure 6.3
Enrichment Analysis of the Wachi Experiment with Bayesian Multistage Linear Model Analysis, W = 1 and
τ = 0.1.

104
BAYESIAN ANALYSIS OF GENE EXPRESSION DATA
Table 6.4
Bayesian multistage enrichment analysis, Wachi data
No. of
Posterior
Gene class
genes.
expectation ϒb (2)
transmembrane receptor activity
154
0.998
sugar binding
178
0.998
structural constituent of cytoskeleton
155
0.998
serine-type endopeptidase inhibitor activity
89
0.998
ribonuclease activity
71
0.998
protein N-terminus binding
62
0.998
protein kinase inhibitor activity
50
0.998
protein kinase binding
80
0.998
protein heterodimerization activity
170
0.998
protein binding, bridging
52
0.998
phosphoric monoester hydrolase activity
63
0.998
oxygen binding
59
0.998
nuclease activity
149
0.998
NAD binding
56
0.998
monooxygenase activity
117
0.998
kinase inhibitor activity
52
0.998
kinase binding
114
0.998
integrin binding
85
0.998
heparin binding
129
0.998
heme binding
161
0.998
GPI anchor binding
147
0.998
endopeptidase inhibitor activity
122
0.998
endonuclease activity
69
0.998
electron carrier activity
132
0.998
copper ion binding
81
0.998
The combination W = 1 and τ = 0.05 was chosen to maximize log P (Y|mj),
found by Monte Carlo integration. These hyperparameters reﬂect a relatively
informative prior for θ and moderate to weak dependence between β coefﬁcients
for the genes (Figure 6.4). Contrast these results with the combination W = 7
and τ = 0.1, allowing a weaker prior for β, while inducing more dependence
between the genes (Figure 6.4). Notice that in both cases some shrinkage of the
parameter estimates from the OLS estimates is induced, with more shrinkage
at W = 7, while the larger gene-wise parameters for variance are shrunk down
from their pooled estimates.
The top 25 most highly enriched GO classes, ordered by E(ϒ) (6.2) are
listed in Table 6.4. Out of the top 25 functions, 14 functions are shared with the
univariate analysis in Example 11.1.

BAYESIAN HYPOTHESIS INFERENCE FOR GENE CLASSES
105
Pattern 8
Pattern 12
Pattern 18
0.040
0.035
0.030
0.025
0.020
P.mean [8]
0
5
10
15
20
25
30
Index
Figure 6.4
BD analysis of Klevecz yeast time course data: three enriched pat-
terns.
6.4.2
Bayesian Decomposition
Enrichment analysis can be a very useful application for learning about the
association of important pathways with phenotypes in designed controlled
experiments. In some cases, the design of an experiment or case–control study
does not lend itself well to or permit direct inference on a factor of interest. For
example, changes in gene expression over time in a time course gene expression
study can be very difﬁcult to relate to a particular pathway or function since
there are so many possible combinations of ways genes can vary in expression
over multiple time points. In contrast to controlled factorial designs, associating
pathways with signiﬁcant temporal patterns is a tenuous task. We discuss
Bayesian methods for analysis of time course gene expression studies in Chapter
9, and here demonstrate the use of a novel Bayesian pattern recognition method
proposed for exploring pathway associations with temporal gene expression
patterns.
Moloshok et al. (2002) proposed Bayesian decomposition (BD), a pattern
recognition algorithm that allow genes to cluster, and borrow strength, in multiple
expression patterns. The fundamental idea is that an G × n expression matrix X,

106
BAYESIAN ANALYSIS OF GENE EXPRESSION DATA
X = A


x11
x12
· · ·
...
xGn


=


a11
· · ·
...
anp




γ11
γ12
· · ·
...
γpn

, (6.15)
can be decomposed into a p × n matrix of expression patterns 
, and a matrix
of G × p amplitudes A. Each row of 
 corresponds to a ‘common’ expression
pattern found among genes in X, while each row g in A provides the amplitudes,
or weights, of association between gene expression proﬁle g and the patterns in 
.
For example, the largest amplitude in the gth row of A, max(Ag1, Ag2, . . . , Agp),
corresponds to the dominant pattern found in 
 associated with the expression
proﬁle of gene g. The second largest amplitude corresponds to the next most
dominant pattern in 
 associated with expression proﬁle of gene g, and so on.
Moloshok et al. (2002) put prior distributions on the matrices 
 and A, with the
the number of patterns p deﬁned explicitly. One may assign informative priors
for the patterns in 
, as well as dependent joint priors linking the rows of A,
based on prior pathway information. The prior distribution on A can specify the
joint probability that any collection of genes will co-express across one or more
of the patterns:
P (A, 
|X) ∝P (X|A, 
)P(A, 
).
(6.16)
Multiple solutions of the decomposition (6.15), can exist that ﬁt the data well. For
this, a search is conducted to compare multiple solutions, to sample the ones that
are more probable. Note that while assignment of informative priors is feasible
with Bayesian decomposition, in practice it can be cumbersome and awkward to
translate prior information into the expression domain.
An advantage of Bayesian decomposition is the utility to associate the poste-
rior expression patterns discovered with genes among gene classes. Enrichment
analysis is performed, updating the random variable
ϒp
b = I(||Gp ∪Gb||/||Gp|| ≥||Gc
p ∪Gb||/||Gc
p||),
(6.17)
contrasting each pattern p, by class b, via a selection criterion cutoff applied to
the weights in A.
Example 6.4 Yeast time course study
In the ﬁnal example of this chapter, we explore associations between complex
temporal gene expression proﬁles and known gene transcription regulators in a

BAYESIAN HYPOTHESIS INFERENCE FOR GENE CLASSES
107
yeast time course study. The software BD (http://bioinformatics.fccc.edu) imple-
ments Bayesian decomposition, allowing for user speciﬁcation of the standard
deviation of the noise in the data, σ, assumed to be the same for all genes, the
number of patterns, or rows in 
, and the number of iterations for convergence.
The algorithm provides the posterior expectation and posterior standard deviation
of the data predictions, pattern matrix P , and amplitude matrix A. It also provides
the log of the marginal predictive distribution of the data log P (X) integrating
over 
 and A.
The Klevecz et al. (2004) Affymetrix S98 yeast arrays were down-
loaded, including 32 time points over three cycles of dissolved oxygen.
Yeast transcription factors and their downstream targets were obtained from
http://www.yeastextract.org, deﬁning 17 gene classes, including 703 targets.
BD was run for combinations of p = 5, 10, 15, 20, 25 patterns and uncertainty
σ = 0.4, 0.3, 0.2, 0.1, 0.05 for 1000 iterations each. Utilizing the Bayesian model
selection criterion, the model with p∗= 20 patterns and σ∗= 0.4 was selected,
i.e. maximizing the log predictive distribution log P (X).
Three transcription factors (classes) showed a large portion of respective
downstream targets (gene members), following the analysis of Moloshok et al.
(2002), with at least 70% of variation explained by the posterior expectation of
a pattern in P |X. These were HAA1, with 40% of genes associated with pattern
12, MET31 with 41% of genes associated with pattern 18, and OPI1, with 50%
of genes associated with pattern 8, displayed in Figure 6.4.
6.5
Summary
Inferring differential expression among a priori gene classes, associated with a
phenotype or experimental factor of interest, remains an open research question,
and will remain so, as no one approach is necessarily correct for every analysis.
There are many ways to deﬁne the hypothesis of change for a gene class, with
enrichment being the convention. The heuristic enrichment analysis approach
considers gene and gene class detection separately. This is a reasonable approach
to take, and in practice is a sensible ﬁrst step. More advanced frequentist methods,
such as GSA, show promise, providing for statistical considerations.
Incorporating gene dependence as in the multistage model or through prior
speciﬁcation with Bayesian decomposition should not be regarded as replacing
but rather as supplementing a one-at-a-time gene analysis. The reason for this is
that there is still much uncertainty surrounding gene annotations, and potential
uses of the prior information. The goals of gene expression analyses and and
ongoing efforts to genotype diseases do not necessarily have to be independent
aims, as efforts to integrate learning between a spectrum of studies provide new
incentives and directions. The Bayesian learning paradigm is a natural one to
adopt, although any analysis aimed at using prior information should largely
reﬂect the fact that, in the state of post-genomic research, the devil, so to speak,
is in the detail.

108
BAYESIAN ANALYSIS OF GENE EXPRESSION DATA
Ever more exciting and challenging statistical research will no doubt improve
the way we can pool information between genes, and infer important patterns
in known pathways, to give researchers focus and direction toward historical
pathways and gene targets. Despite the uncertainty, there can be more to learn
from high-throughput experiments, if we are careful to consider the functional
roles of genes and their course in evolution.

7
Unsupervised Classiﬁcation
and Bayesian Clustering
7.1
Introduction to Bayesian Clustering for Gene
Expression Data
In Chapter 5 we introduced the problem of Bayesian supervised classiﬁcation
for gene expression data, for example, to predict patient disease status, given
gene expression proﬁles and clinical covariates. For pairs of observed responses
and gene expression proﬁles (yi, xi) for individuals i = 1, 2, 3, . . ., a classiﬁer,
or prediction, rule is constructed with the goal of predicting the response of
a new individual given gene expression proﬁles xnew. The goals of supervised
classiﬁcation can be several-fold, although the utility of the predictor is measured
by how well it can discriminate, for example, pathological status or response to
therapy, given observed gene expression proﬁles.
Some diseases such as cancer are moxlecularly heterogeneous, appearing
to produce similar symptoms and cellular irregularities, although very differ-
ent responses to therapy. Many of the important differences in gene expression
between diseases have yet to be discovered. It is for this reason that we turn to
a different although related problem: unsupervised classiﬁcation, also commonly
referred to as cluster analysis (we use the two terms interchangeably through-
out). Suppose that we observe gene expression xi for individuals i = 1, 2, 3, . . .,
originating from k = 1, . . . , K biological classes where the class membership yi
for each individual as well as the number of classes C are unknown – hence
the name unsupervised classiﬁcation, as there is no a priori class knowledge
to guide learning. In principle, individuals with similar expression proﬁles are
Bayesian Analysis of Gene Expression Data
B. Mallick, D. Gold, and V. Baladandayuthapani
2009 John Wiley & Sons, Ltd

110
BAYESIAN ANALYSIS OF GENE EXPRESSION DATA
more likely to belong to the same cluster, while individuals with very differ-
ent gene expression should belong to different clusters. Clustering algorithms
based on this principle assign individuals to clusters by a measure of similarity
in observed gene expression.
In this chapter, our aim is to predict class membership of individuals, where
the individuals can be the subjects (e.g. patient disease status) or the genes
(e.g. to discover clusters of highly associated genes), or perhaps to cluster both
the subjects and the genes. Some of the methodologies that we present in this
chapter lend themselves more conveniently to clustering just the subjects or genes,
requiring constraints or modiﬁcation to cluster both. In order not to confuse these
two important, though not necessarily competing, aims, we make distinctions
throughout this chapter where applicable between clustering the subjects and
clustering the genes.
Example 7.1 Bi-way clustering of lung cancer cell lines
We illustrate the results of a common clustering task, bi-way clustering, of
gene expression proﬁles with the Zhou et al. (2006) study of expression
in 79 lung cell line samples from 13 origins, including adenocarcinomas,
squamous cell carcinoma, mesothelioma and normal lung tissue. The Affymetrix
U133A data were downloaded from the NCBI/GEO data retrieval system
(http://www.ncbi.nlm.nih.gov/)
and transformed by the natural logarithm,
truncating values less than 0. Probes were ﬁltered, including only probes with
at least 50% of values above the overall data set median, resulting in 10 881
probes. The cell lines and genes were both clustered, each independently, with
the techniques described in Section 15.3. Notice in Figure 7.1 the yellow (low)
and red (high) ‘expression patterns’ that appear by cell line (rows) and genes
(columns). These patterns can be indicative of pathological subtypes and gene
clusters associated with disease.
Figure 7.1
Heatmap: Bi-way clustering of lung cancer cell lines, 79 samples
(rows) and 10881 probes (columns). High (red) and low (yellow) probe expression
patterns distinguish the samples.
The results of the lung cancer study of Example 7.1 suggest that cancer-speciﬁc
molecular signatures can be discovered and used to discriminate subjects. Keep
in mind that the results and conclusions of applied cluster analysis with gene
expression data can be hotly debated. Part of the reason for this is that classi-
ﬁcation, as we saw in Chapter 14, involves potential risks to classify subjects
incorrectly, and the cost of making a wrong decision can be high. It is widely

UNSUPERVISED CLASSIFICATION AND BAYESIAN CLUSTERING
111
believed that the results from these studies should be validated with more data, or
external means. The nature of this problem is further compounded when we do
not know the true classiﬁcation labels, i.e. the potential to read too much into the
analysis or infer from apparent patterns – false signatures. With this in mind we
proceed with clustering, and list the advantages that Bayesian clustering methods
can deliver in a variety of situations.
Among the many popular approaches for unsupervised classiﬁcation that we
will discuss in this chapter are hierarchical clustering, principal component clus-
tering, model-based clustering, K-means clustering, mixture model clustering,
and Dirichlet (Chinese restaurant) process clustering. Bayesian generalizations
of many of these clustering methods are well known and can provide advantages
for learning. An important distinction though, from frequentist clustering meth-
ods, is that to a Bayesian there is uncertainty in cluster membership, both before
seeing the data, and after. This fundamental distinction implies that membership
in a cluster is only known up to a measure of uncertainty. We begin discussion
of Bayesian clustering by ﬁrst introducing the fundamentals from classical work
in the ﬁeld.
7.2
Hierarchical Clustering
Hierarchical clustering algorithms come in many forms. In this section we discuss
one of the most popular forms, recursive binary clustering. No distinction is
required in order to apply the algorithm to cluster the subjects or the genes, and
the algorithm may be applied to cluster both, independently.
In order to perform the hierarchical clustering algorithm, rules are speci-
ﬁed for measuring distances between individuals and between clusters, and the
direction of agglomeration:
1. Deﬁne a measure of pairwise distance between individuals i and j ̸= i.
2. Adopt a linkage rule.
3. Choose the direction of aggregation (top-down or bottom-up).
As in other clustering methods, measures of dissimilarity or distance between
individuals are important. Some common measures of distance between multi-
variate vectors are deﬁned in Table 7.1.
Table 7.1
Distance Metrics for hierarchical clustering
Name
Formula
Euclidean
DEuclid(xj, xk) =

(x1j −x1k)2 + . . . + (xpj −xpk)2
Correlation
DCorr(xj, xk) = 2 × (1 −rXj ,Xk)
Manhattan
DBlock(xj, xk) = |x1j −x1k| + . . . + |xpj −xpk|

112
BAYESIAN ANALYSIS OF GENE EXPRESSION DATA
Linkage deﬁnes how distance is measured between clusters, and between
individuals and clusters. The three commonly used linkage methods are nearest
neighbor, average and complete linkage. Consider two clusters c = 1, 2,
consisting of n1 and n2 subjects. Let D(Xxi, xk) be the distance between
the ith individual in cluster 1, and the kth individual in cluster 2. Nearest
neighbor linkage deﬁnes the distance between any two clusters as the minimum
of the D(xi, xk), i.e. the shortest distance between between all pairs of
individuals from the respective clusters. Average linkage deﬁnes the distance
between two clusters as the sample average of the D(xi, xk) between all
pairs of individuals from the respective clusters. Complete linkage deﬁnes
the distance between two clusters as the maximum of the D(xi, xk), i.e.
the farthest distance between all pairs of individuals from the respective
clusters.
Methods for agglomeration can either be bottom-up, or top-down. In
bottom-up agglomeration all points are initially in their own own cluster. The
ﬁrst cluster is formed by combining the pair of individuals that are closest
in terms of distance. At the next step, the individual(s) and or cluster with
the shortest distance are merged into a cluster. The process continues until
all individuals are combined into one cluster. In top-down agglomeration all
individuals are initially assumed to belong to one cluster. The cluster is split
into two groups so that the measure of distance between the clusters is greatest.
At the next step, one of the respective clusters is split, so that the two new
clusters that are formed are the farthest apart in terms of distance. This process
continues until all individuals belong to only one cluster.
A graphic commonly used for visualizing hierarchical cluster results
is the dendrogram. On the horizontal axis all individuals are displayed,
ordered by their respective cluster assignments. Above and between each
individual/cluster is a single branching node, connected to two branches
below and one above. Nodes are positioned at the height equal to the
distance of the merged clusters. Figure 7.2 shows a dendrogram for the
lung cell line data of Example 7.1. In order to assign cluster membership,
a cutoff is chosen along the vertical axis. Individuals that are merged by a
branching node immediately above the cutoff height are assigned to the same
cluster.
7.3
K-Means Clustering
Consider a population with K p-dimensional sub-populations or clusters, of genes
or subjects, as clustering of either is possible with K-means. Each cluster is
assumed to have its own unique density with mean µk = (µ1k, . . . , µkp). More
generally, µk is considered to be the ‘center of mass’ or ‘center of gravity’ to
which points aggregate in the kth cluster. The essential feature of the algorithm
is to choose { ˆµ1, . . . , ˆµK} and {A1, . . . , AK} in order to minimize the score

UNSUPERVISED CLASSIFICATION AND BAYESIAN CLUSTERING
113
200
180
160
140
120
100
80
Height
NSCLC
NSCLC
NSCLC
NSCLC
NSCLC
NSCLC
NSCLC
NSCLC
NSCLC
SCLC
SCLC
SCLC
SCLC
SCLC
SCLC
SCLC
SCLC
SCLC
SCLC
SCLC
SCLC
SCLC
SCLC
SCLC
SCLC
SCLC
SCLC
SCLC
SCLC
SCLC
SCLC
large cell carcinoma
large cell carcinoma
large cell carcinoma
squamous cell carcinoma
adenocarcinoma
adenocarcinoma
adenocarcinoma
adenocarcinoma
adenocarcinoma
adenocarcinoma
adenocarcinoma
adenocarcinoma
adenocarcinoma
adenocarcinoma
adenocarcinoma
adenocarcinoma
immortalized normal
immortalized norm
squamous cell carcinoma
Bronchoalveolar
Bronchoalveolar
adenocarcinoma
large cell carcinoma
large cell carcinoma
immortalized normal
large cell carcinoma
adenocarcinoma
squamous cell carcinoma
squamous cell carcinoma
adenocarcinoma
adenocarcinoma
adenosquamous
adenocarcinoma
adenocarcinoma
adenocarcinoma
adenocarcinoma
adenocarcinoma
adenocarcinoma
adenocarcinoma
adenocarcinoma
adenocarcinoma (mixed)
adenocarcinoma (BAC features)
Bronchoalveolar
Neuroendocrine
mesothelioma
mesothelioma
Bronchoalveolar
bronchoalveolar
Figure 7.2
Dendrogram for lung cell line data.

114
BAYESIAN ANALYSIS OF GENE EXPRESSION DATA
function
S(µ; X) =
K

k=1

i∈Ak
p

g=1

xgi −µgk
2 ,
(7.1)
where the set Ak = {i : subject i is a member of cluster k}. Maximizing the
K-means score function is synonymous with maximizing the product of K
p-dimensional Gaussian likelihoods, with diagonal covariance matrix of equal
dispersion along the main diagonal.
In practice, choosing K is a difﬁcult problem, as it deﬁnes how many clusters
are in the data. Frequentist methods deal with the problem of choosing K by
assessing the ﬁtness of different values of K with resampling-based methods,
such as cross-validation and the bootstrap. For a ﬁxed K, the Bayesian extension
of the K-means algorithm is straightforward, as described in the next section
on model-based clustering. In addition, the Bayesian can specify a prior on K,
the number of clusters, allowing integration over all of the uncertainty in K in
determining cluster membership.
7.4
Model-Based Clustering
Consider a set of K clusters, in p-dimensional space, each distributed accord-
ing to parametric densities Pk(θk), for k = 1, . . . , K. If the densities Pk were
known then the individuals, subjects or genes, could be assigned to the cluster
maximizing the respective likelihood
k(i) = argmaxkPk(xi; θk),
(7.2)
for k = 1, . . . , K. In practice, the number and distributions of the cluster pop-
ulations are unknown. Suppose that the K clusters are distributed according to
the multivariate Gaussian distribution. Conditioning upon cluster label yi = k,
the likelihood of the p-dimensional variable xi is
xi|yi = k ∼Np(µk, ),
(7.3)
indexed by unknown mean vector µk and unknown variance–covariance
matrix . Here the aim is to determine the number of cluster populations K,
estimate the unknown means and covariance, and assign cluster membership.
This can be achieved by choosing ˆµk, ˆ, ˆyi and ˆK to maximize the likelihood
P (y|µ, , X) =
n

i=1
ˆK

k=1
N(xi; ˆµk, ˆ)I(yi=k).
(7.4)
A heavy-tailed alternative to the multivariate Gaussian distribution is the mul-
tivariate t distribution: see the R package mclust for details and model ﬁtting.

UNSUPERVISED CLASSIFICATION AND BAYESIAN CLUSTERING
115
Note that like principal component clustering discussed in Section 7.7, cluster-
ing genes with full off-diagonal covariance matrix  is typically not possible
without simplifying assumptions, due to the number of subject samples required.
Model-based clustering of genes is made possible by, for instance, constraining
the off-diagonal elements of  to be 0.
Extending the model-based clustering framework to Bayesian clustering is
straightforward. Posterior inference proceeds by specifying prior distributions for
unknown parameters. Suppose, for example, that xi is distributed conditionally as
xi|yi = k ∼N(µk, ),
(7.5)
with
p-dimensional
mean
µk
and
p × p
diagonal
covariance
matrix
 = diag(σ 2
1 , . . . , σ 2
p). We specify prior distributions
µk ∼Np(mk, V ),
σ 2
g ∼IG(ag, bg),
yi ∼Multinomial(1, π)
(7.6)
Integrating out the parameters, the posterior probability of yi = k|xi is
P (yi = k|xi) ∝πk
		
N(xi; µk, )N(µk; mc, ν2)
p

g=1
IG(σ 2
g ; ag, bg)dσ 2
g dµk
∝πkS(xi; mk, Q),
(7.7)
for p < G, i.e. proportional to a multivariate t distribution with location mk
and dispersion matrix Q. While the frequentist and Bayesian criteria appear
identical, recall that only Bayesians can make probability statements about the
parameters, i.e. yi, given the data.
7.5
Model-Based Agglomerative Hierarchical
Clustering
A disadvantage of model-based clustering is the computation time required to
evaluate the likelihood for all possible clusters. Combining the computational efﬁ-
ciency of hierarchical clustering with the model-based approach of model-based
clustering leads to model-based agglomerative hierarchical clustering. The idea
is to consider the likelihood as a score function to be optimized. The likelihood
is given by
P (y|θ1, . . . θK; l1, . . . , ln) =
N

i=1
Pli(yi|θli),
(7.8)

116
BAYESIAN ANALYSIS OF GENE EXPRESSION DATA
where the li are the group labels. The likelihood is maximized in stages, merging
the pair of clusters achieving the greatest increase at each stage. A disadvantage
of model-based agglomerative hierarchical clustering is that at each stage the
results are conditional upon the previous stage. The algorithm does run much
faster than model-based clustering, and can lead to similar results.
7.6
Bayesian Clustering
The landmark paper in Bayesian clustering is that of Binder (1978) who ﬁrst
introduced the theoretical innovations for Bayesian clustering. Let X denote an
n × p matrix of p measured characteristics for each of n individuals, i.e. xi =
{xi1, . . . , xip} denotes the measured characteristics for the ith individual. Let
y = (y1, . . . , yn) denote the (at least partially) unobserved cluster membership
of each individual i, i.e. yi = k denotes that the ith individual is a member of
the kth cluster, with K the (unknown) total number of clusters. Without loss
of generality, Binder’s method can be applied to cluster subjects, in which case
the xi are the measured expression proﬁles of each subject across a collection
of genes on the array, or to cluster the genes given the subjects proﬁled across
the study, in which case the response is indexed by g = 1, . . . , G. We omit
any distinction between clustering subjects and genes for the present discussion,
considering individuals to be appropriately deﬁned for the aim of the analysis.
Individuals are assumed conditionally independent, given y,
xi|yi = k, θ
i.i.d. Pk(xi|θk),
(7.9)
with density Pk(xi|θk). The prior for the unknown variables (K, y, θ) is
P (K, y, θ) = P (K)P (y|K)P (θ|K, y).
(7.10)
Notice that the prior density for θ is deﬁned conditionally upon y. In the case
where P (yi = k|K) = πk, introducing the unobserved prior weights for cluster
membership, the prior is
P (K, y, π, θ) = P (K)P (π|K)
K

k=1
πnk
k Pk(θk|y),
(7.11)
where nk is the number of members in the kth cluster.
Suppose further that θ does not depend on y, in practice a reasonable and
convenient assumption to make. Then the likelihood is
P (X|K, π, θ) =
K

k=1
πkPk(X|θk),
(7.12)

UNSUPERVISED CLASSIFICATION AND BAYESIAN CLUSTERING
117
i.e. a mixture over Pk with mixture weights πk for k = 1, . . . , K. The marginal
posterior y|X is given by
p(y|X) ∝

K
p(K)p(y|K) ×
K

k=1
	
c
P (θk|K, y) ·

i∈Ak(y)
Pk(xi|θk)dθk
(7.13)
where the set Ak(y) = {i : yi = k} includes the indices of samples assigned to
cluster k, and the integral is over θk where P (θk|K, Ak(y)) > 0. An advantage
of Bayesian clustering is that it provides a probability measure of uncertainty
in class membership. The marginal prior probability P (yi = k) is updated in a
straightforward way to yield P (yi = k|X), a measure of posterior precision. In
contrast to frequentist clustering, yi is not assumed ﬁxed. In the event that a
point prediction is desired, Bayesian clustering rules include criteria for:
1. maximizing the joint posterior probability;
2. for each individual i, choosing the group k = argmax P(yi = k|X), i.e. the
marginal posterior probability;
3. performing an available clustering algorithm, deﬁning pairwise similarity
as P (yi = yj|X);
4. minimizing a posterior risk function.
Evaluating P (y|X) for all possible permutations of the cluster labels y =
{1, 2, 3, . . .} is computationally cumbersome. For K clusters and n individuals
there are Kn possible ways to cluster individuals, allowing for empty clusters.
An approximation to the posterior mode can be found by choosing ˆyi, for i =
1, . . . , n, maximizing
S∗=
K

k=1

i∈Ak

l∈Ak
P (ˆyk = ˆyl|X) −1
2

i
ˆn2
k
(7.14)
for the set Ak of members in the kth cluster and ˆnk = |Ak (Binder, 1981). This
sum is maximized in two stages:
I Obtain a list of local maxima, by a hill climbing procedure, based on an
approximation of S∗.
II Search the neighborhoods of local maxima and choose the partition with
the largest value.
7.7
Principal Components
Consider the gene expression matrix X, with gene expression values in the rows
and subject samples in the columns. The estimated covariance matrix between

118
BAYESIAN ANALYSIS OF GENE EXPRESSION DATA
subject samples is
V = (N −1)−1S = (N −1)−1
N

i=1
(xi −¯X)(xi −¯X)T .
(7.15)
The singular value decomposition (SVD) of V is
V = EDET ,
(7.16)
where D is a diagonal matrix of eigenvalues (d1, d2, . . . , dp) ordered from largest
to smallest down the main diagonal, and E is the corresponding orthogonal matrix
of eigenvectors, i.e. ET E = I. The ﬁrst principal component (PC) is deﬁned as
P1 = XT E1, the second as P2 = XT E2 etc., where Ej is the jth eigenvector cor-
responding to the jth largest eigenvalue, dj = var(Pj). Intuitively, E1 captures
the dominant proﬁle in gene expression values, across subjects, accounting for
the largest PC, E2 the second largest PC, etc. PC clustering has been applied to
explore the dominant sources of variation between array samples in gene expres-
sion data that may be attributable to biology or experimental variation. Unlike
the previous methods, PC clustering requires a full-rank positive semi-deﬁnite
sample covariance matrix V , and therefore it is ‘typically’ not possible to per-
form PC clustering on all of the genes on the array, given the subjects, without
a substantial number of samples.
A scatter plot of the ﬁrst two eigenvectors, E2 versus E1, provides a visual
display of clusters discriminated the ﬁrst two PC reductions of their expression
proﬁles. Further scatter plots can be useful, of the ﬁrst and third eigenvectors,
the second and third, etc., to discover subject clusters.
How many PCs are needed to explain the major sources of variation in the
data? Scree plots provide a visual approach to choosing the number of mean-
ingful PCs. The eigenvalues are plotted from largest to smallest. An obvious
break in the plot (Figure 7.3(d)) between the eigenvalues yields evidence of the
maximum number of PCs needed. In practice, inference is desirable for choos-
ing the number of components accounting for the major variation in the data. It
has been shown that the frequentist asymptotic distribution of the eigenvalues is
multivariate normal (Wigner, 1955; Dhesi and Jones, 1990). In a Bayesian PC
analysis, one assumes that the true covariance matrix is unknown, and speciﬁes a
prior distribution for it. For a general variance–covariance matrix with nonzero
off-diagonal terms, suppose we adopt an inverse Wishart prior on ,
 ∼InverseWishart(α, R),
(7.17)
with parameters α, the effective prior sample size, and matrix R, a prior guess
at . Suppose that the SVD of  is EET . The posterior is of  is inverse

UNSUPERVISED CLASSIFICATION AND BAYESIAN CLUSTERING
119
Wishart
|X ∼InverseWishart(α∗, R∗),
α∗= α + n,
R∗= R + S,
(7.18)
with posterior parameters α∗and R∗and posterior expectation E(|X) = (α +
N −p −1)−1(R + S). One can generate samples from the posterior of , trans-
form them by the SVD, and derive posterior probability statements about the θj.
Example 7.2 PC clustering of lung cancer cell lines
PC plots of the Lung Cell Line data (see Example 7.1) are shown in Figure 7.3.
A strong contrast is observed in the second component. The scree plot in
Figure 7.3(d) suggests that more than two factors separate the cell lines. A
Bayesian PC analysis was applied (Figure 7.4). The 95% Bayesian posterior
credible envelope ﬁts very tightly around the eigenvalues. As many as six
signiﬁcant components can be inferred.
7.8
Mixture Modeling
Mixture modeling is an attractive way to identify clusters in the data. In this
setup we assume that the multivariate gene expression proﬁles x1, . . . , xn, genes
or subjects, are distributed as a mixture with likelihood
P (X|θ1, . . . θK; π1, . . . , πK, ) =
n

i=1
K

k=1
πkPk(xi|θk),
(7.19)
with Pk the multivariate Gaussian distribution, 0 ≤πk ≤1 mixture weights, and
θc = (µc, c). The space of possible parameterizations of  is rather large, and
must be appropriately constrained in the event that the number of dimensions p
exceeds the number of individuals. Trouble arises, for example, if one desires to
form gene clusters, where the count of genes per cluster can exceed the number
of subjects. In the present discussion, we move forward based on the under-
standing that the appropriate sample size considerations are taken into account in
estimation of c. Consider constraining the space of possible (feasible) models
through , by the decomposition of c into
k = λkEkAkET
k ,
(7.20)
where Ek are orthogonal eigenvectors, Ak is a diagonal matrix with elements pro-
portional to the eigenvalues, and λk is a constant of proportionality, for cluster k.

120
BAYESIAN ANALYSIS OF GENE EXPRESSION DATA
−0.12
−0.11
−0.10
−0.09
−0.1
0.0
0.1
0.2
Component II
Component I
(a)
20
0
40
60
80
0.12
−0.11 −0.10 −0.09
Component I
Dimension
(b)
−0.1
0.0
0.1
0.2
Component II
20
0
40
60
80
Dimension
(c)
20
0
40
60
80
Dimension
(d)
2.5
2.0
0.5
1.0
0.5
0.0
Eigen Values (Log Base 10)
Figure 7.3
PC analysis: (a) scatter plot of ﬁrst and second eigenvectors; (b) eigenvector 1; (c) eigenvector 2; (d) scree plot.

UNSUPERVISED CLASSIFICATION AND BAYESIAN CLUSTERING
121
1
2
3
4
5
6
7
8
9
10
Component
2.5
2.0
1.5
1.0
0.5
0.0
Eigen Values (Log Base 10)
Figure 7.4
Posterior scree plot, with 95% posterior credible envelope.
Some speciﬁcations for k include the following:
Model
Speciﬁcation
1
k = λI
2
k = λkI
3
k = λkAk
4
k = λkEAkET
5
k = λkEkAkET
k
Model 1 treats restricts each cluster to an identical diagonal covariance matrix,
with variance λ. The least restrictive, Model 5, is the most highly parameter-
ized, allowing each cluster to have its rotation and scale. For each model, the
EM algorithm is employed to maximize the likelihood. Convergence of the EM
algorithm may be speeded up by considering good starting values. A sensible
place to start is with the results form model-based hierarchical clustering.
Deﬁne models m1, m2, m3, . . . , each with a different speciﬁcation on k, for
K = 1, 2, . . . the total number of clusters. A strategy for model selection, based
on the Bayesian information criterion (BIC), is as follows:
1. Determine the maximum number of cluster K to consider, and ﬁx K.

122
BAYESIAN ANALYSIS OF GENE EXPRESSION DATA
2. Obtain starting values for each model speciﬁcation and number of clus-
ters k = 1, . . . , K from model based agglomerative hierarchical clustering,
using the multivariate normal densities.
3. Apply the EM algorithm to ﬁt each mixture model.
4. Compute the BIC for each mixture model, with the optimal EM ﬁtted
parameters.
This is the strategy proposed by Fraley and Raftery (2002).
The Bayesian mixture model is completed by specifying prior distributions
for all unknown parameters, including model choice and the mixture weights
π1, . . . , πK. The conditional conjugate prior for π1, . . . , πK is the Dirichlet dis-
tribution,
P (π1, . . . , πK|α1, . . . , αK) =

+K
k=1 αk
,
(K
k=1 
(αk)
τ α1−1
1
τ α2−1
2
. . . τ αK−1
K
,
(7.21)
with mean and variance
E(πk) =
αk

l αl
,
var(πk) =
αk(
l̸=k αl)
(
l αl)2(
l αl −1).
(7.22)
Since there is model uncertainty, i.e. as to the speciﬁcation of k, the prior for
model choice M is denoted P (M = m). The full model is:
xi|M, π, θ i.i.d.
K(m)

k=1
π(m)
k
N(xi|µ(m)
k
, (m)
k
),
π(m)
k
|M
∼
Dir(α, K(m)),
µ(m)
k
, (m)
k
|M
∼
NIG(µ(m)
k
, (m)
k
),
M
∼
P (M).
(7.23)
A convenient alternative speciﬁcation introduces latent variables, hik, that indi-
cate cluster membership, hik = 1 identiﬁes subject i as a member of cluster k,
and 0 otherwise. The hik are constrained so that khik = 1 for all i. With this

UNSUPERVISED CLASSIFICATION AND BAYESIAN CLUSTERING
123
prior speciﬁcation, the full model is:
xi|π, θ ∼
K

k=1
Pk(xi|θk)hik,
hik ∼Multinomial(1, π),
πk ∼Dir(α, R),
θck ∼P (θk),
M ∼P (M).
(7.24)
The full conditionals for Gibbs sampling at iteration t, given M = m, are:
1. P (h(t)
ik = 1|·) ∝π(t−1)
k
Pk(yi|θ(t−1)
k
);
2. P (π(t)
k |·) = Dir(α1 + ˆn(t)
1 , . . . , αK + ˆn(t)
K ), where ˆn(t)
k = 
i h(t)
ik ; and
3. P (θ(t)
k |·) ∝P (θ(t)
k ) × (
i Pk(yi|θ(t)
k )λ(t)
ik
One could ﬁt all possible models, and compare them using, for example, the
Bayes factor, although consider allowing model choice to be random, updated
in the posterior sampling scheme by the reversible jump algorithm (Richardson
and Green, 1997). The general idea for reversible jump, given current dimension
model M, is as follows:
1. From a staring state (m, θm), propose a new model with probability qm,m∗.
2. If m∗has a different dimensionality than m, generate an augmenting ran-
dom u from the proposal q(u|m, m∗, θm), otherwise
3. determine the proposed models parameters θm∗= fm,m∗(θm, u).
4. Accept the new model with probability min(r,1) where
r = P (X|θm∗)P (θm∗)πm∗qm,m∗q(u∗|m∗, m, θm∗)
P (X|θK)P (θK)πKqK∗,KP (u|K, K∗, θK)
3333
▽fK,K∗(θK, u)
▽(θK, u)
3333 .
(7.25)
In the special case of the normal ﬁnite mixture model, at each step consider:
1. splitting one component into two, or combining two into one;
2. the birth or death of an empty component.
For (1) make a random choice between splitting and combining, according
to predeﬁned proposal probabilities, depending, of course, on K. If combined,
choose two adjacent components, (k1, k2), at random based on the distance of

124
BAYESIAN ANALYSIS OF GENE EXPRESSION DATA
their means. The merge is completed by setting
πk∗= πk1 + πk2,
πk∗µk∗= πk1µk1 + πk2µk2,
πk∗(µ2
k∗+ σ 2
k∗) = πk1(µ2
k1 + σ 2
k1) + πk1(µ2
k1 + σ 2
k1),
(7.26)
matching ﬁrst and second moments. The reverse split begins by choosing a group
k∗at random and generating uh ∼Beta(2, 2), h = 1, 2, 3:
πk1 = πk∗u1,
(7.27)
πk2 = πk∗(1 −u1),
(7.28)
µk1 = µk∗−u2σk∗
πk1/πk2,
(7.29)
µk2 = µk∗+ u2σk∗
πk2/πk1,
(7.30)
σ 2
k1 = u3(1 −u2
2)σk∗πk∗/πk1,
(7.31)
σ 2
k2 = (1 −u3)(1 −u2
2)σk∗πk∗/πk2.
(7.32)
For a birth or death the same proposal probabilities are used. If the draw is a
birth, the parameters are drawn from
πk∗∼Beta(1, π0),
(7.33)
µk∗∼N(ξ, κ−1),
(7.34)
σ −2
k∗∼Gamma(a, b),
(7.35)
and the existing weights are rescaled. For a death, a component is deleted, and
the existing weights are rescaled.
7.8.1
Label Switching
Mixture type models in general suffer from a lack of cluster label identiﬁability,
as the cluster labels are not necessarily ordered without further constraints, and
as such are purely semantic. Label switching occurs when, during the course of
posterior updating, the labels of respective components are switched. This may
not seem like a crucial problem, at least as long as the component integrity is
maintained, although it can pose serious challenges for interpreting the results of
an MCMC algorithm. Since the labels are arbitrary, note that the likelihood
X|P, θ ∼

i

k
πP(k)P (xi; θP(k))
(7.36)
is invariant to any random permutation P(k) of k = 1, . . . , K. Some authors argue
that MCMC convergence is not achieved until all possible label permutations

UNSUPERVISED CLASSIFICATION AND BAYESIAN CLUSTERING
125
have been realized on one’s Monte Carlo chains. Efﬁcient algorithms have been
proposed. Some possible solutions are based on:(1) relabeling algorithms, (2)
identiﬁability constraints, (3) strong priors, (4) integrating out θk, and (5) label
invariant loss functions. One proposal for relabeling is to randomly permute
the labels (Fr¨uhwirth-Schnatter, 2001). Strong priors can also be helpful, or for
example imposing an order constraint µ1 < µ2 < . . . on the means. For more on
the label switching problem, see Jasra et al. (2005).
Example 7.3
We analyzed the yeast galactose data of Ideker et al. (2001) where four replicate
hybridizations were performed for each cDNA array experiment. We used a
subset of 205 genes that are reproducibly measured, whose expression patterns
reﬂect four functional categories in the GO listings and that we expect to cluster
together. For these data, our goal is to cluster the genes using mclust and the
four functional categories are used as our external knowledge.
Figure 7.5 shows the available options in mclust for hierarchical clus-
tering (HC) and expectation maximization (EM). The model identiﬁers code
geometric characteristics of the model. For example, EFV denotes a model in
which volumes of all clusters are equal (E), shapes of all clusters are ﬁxed (F),
and the orientation of the clusters are allowed to vary (V). Parameters associ-
ated with characteristics designated by either E or V are determined from the
data.
ID
Model
HC
EM
Distribution
Volume
Shape
Orientation
EI
VI
EEE
VVV
EEV
EFV2
VFV2
VEV
lI
Spherical
Spherical
Ellipsoidal
Ellipsoidal
Ellipsoidal
Ellipsoidal
Ellipsoidal
Ellipsoidal
equal
variable
equal
variable
equal
equal
variable
variable
equal
equal
equal
variable
fixed
equal
fixed
equal
NA
NA
equal
variable
variable
variable
variable
variable
lkI
lDADT
lk Dk Ak Dk
T
lDk ÂDk
T
lDk A Dk
T
lkDk A Dk
T
lDk Â Dk
T
Figure 7.5
Model descriptions in mclust.
We can see from the BIC in Figure 7.6 that BIC is maximized for the
VEV model at two parameters. So mclust is saying that there are two
clusters in the data. VEV denotes a model in which volumes of all clusters
vary, shapes of all clusters are equal, and the orientation of the clusters are
allowed to vary. The two clusters can be seen in Figure 7.7 when the data
are projected into two dimensions. The red squares are one cluster and the
blue triangles the other. The ellipses show the orientation of the cluster in two
dimensions.

126
BAYESIAN ANALYSIS OF GENE EXPRESSION DATA
2
4
6
8
number of components
8000
6000
4000
2000
0
BIC
EII
VII
EEI
VEI
EVI
VVI
EEE
EEV
VEV
VVV
Figure 7.6
BIC values for each of the possible models in mclust.
1,2 Coordinate Projection showing Classification
0.2
0.1
0.0
0.1
−0.2
V3
0.2
0.4
0.6
0.8
1.0
V2
Figure 7.7
Clustering proﬁle in mclust.
7.9
Clustering Using Dirichlet Process Prior
The alternative Bayesian clustering method is the use of inﬁnite mixture model
via the Dirichlet process prior (Ferguson, 1973; Antoniak, 1974; Neal, 2000). In
contrast to the ﬁnite mixture approach, this model does not require the number of
mixture components to be speciﬁed. Following our notation, xi is the multivariate

UNSUPERVISED CLASSIFICATION AND BAYESIAN CLUSTERING
127
gene expression proﬁle for the ith tissue sample, which is assumed to be drawn
from a distribution P (θ) (where θ could be multidimensional as in Section 7.8),
with the prior (mixing) distribution over θ being G. In the usual parametric
Bayesian modeling setup, we assign some parametric distribution for G and use
further prior distributions for those unknown hyperparameters corresponding to
that parametric distribution. In a nonparametric problem we assume that G is
completely unknown and assign stochastic process based prior distributions over
the class of distribution functions. The most popular nonparametric priors are
Dirichlet processes. They were introduced by Ferguson (1973) with additional
clariﬁcation provided by Blackwell and MacQueen (1973), Blackwell (1973),
Antoniak (1974), Sethuraman and Tiwari (1982) and Sethuraman (1994).
A cumulative distribution function or, equivalently, a probability measure
G on  is said to follow Dirichlet process, i.e. G ∼DP (αG0), if, for
any measurable partition of , B1, B2, . . . , Bm, the joint distribution of the
random variable [G(B1), G(B2), . . . , G(Bm) follows a Dirichlet distribution
Dir(αG0(B1), . . . , αG0(Bm)). Here G0 is a speciﬁed probability measure
interpreted as the prior mean or baseline distribution of G or E(G) = G0. α
is the precision parameter showing the a priori strength of belief in G0, so the
larger α is the closer we expect G to be to G0 a priori.
Our main aim in this section is to use Dirichlet processes as a basis for
model-based clustering. An early criticism which has been leveled at Dirichlet
process models is that they assign mass 1 to the subset of all discrete distribution
on T. However, currently this inherent discreteness property has been utilized suc-
cessfully for clustering. In particular, if we generate two independent sequences
of i.i.d. random variables, θi and zi, such that θi ∼G0 and zi ∼Beta(1, α) and
let wi = zi
(
j<i(1 −zj). Then the random function G = ∞
i=1 wiδ(θi) where
δ(θi) is a degenerate distribution at θi and it is clear that  wi = 1. This char-
acterizes DP (αG0) as G ∼DP (αG0) if and only if almost every realization is
of this form. This is a surely discrete distribution hence the realizations from
this distribution will have positive probability of a tie which is the basis of the
cluster formation. Furthermore, the realizations wi determine an inﬁnite breaking
of a stick of unit length. We ﬁrst break off a piece of length w1, then a piece
of length w2, etc., hence it is also known as a stick breaking prior (Freedman,
1963).
The conjugacy result states that given a set of n realizations θ = (θ1, . . . , θn)
from G ∼DP (αG0), the posterior distribution of G given θ is also a Dirich-
let process. More precisely, G|θ ∼DP (α∗, G∗
0) where α∗= α + n and G∗
0 =
(α + n)−1[αG0 +  δ(θi)]. Though θ1, . . . , θn are independent draws from G,
if we marginalize over G then the θi are no longer independent since they
shared a common random G. This marginal distribution is built sequentially
using appropriate G∗
0s. The sequential scheme is, we draw θ1 from G0, we draw
θ2|θ1 from the distribution (α + 1)−1[αG0 + δ(θ1)], . . ., up to θn|θ1, . . . , θn−1 ∼
(α + (n −1))−1[αG0 +  δ(θi)].

128
BAYESIAN ANALYSIS OF GENE EXPRESSION DATA
In general, we can write the conditional distribution for the ith θ as
θi|θ1, . . . , θi−1 ∼
1
i −1 + α
i−1

j=1
δ(θj) +
α
i −1 + α G0.
(7.37)
It is clear that this conditional distribution is a mixture of a continuous distribution
and a point mass. This conditional distribution relates the Dirichlet process prior
with clustering idea. It says that a priori for the ith realization there is a probability
of (i −1 + α)−1 that it will be equal to (or cluster with) any of the previous
i −1 realizations, θ1, . . . , θi−1, whereas the probability of a new draw from the
baseline distribution G0 is α/(i −1 + α).
A convenient way to obtain these results is to consider the equivalent ﬁnite
mixture models as in Section 7.8 and taking the limit as K goes to inﬁnity (Neal,
2000). The hierarchical model we shall consider is
xi|ci, θ ∼P (θci),
ci|p ∼Discrete(p1, . . . , pK),
ci ∈{1, . . . , K},
θc ∼G0,
p ∼Dir(α/K, . . . , α/K),
(7.38)
where ci
indicates the cluster membership of xi
and θc
indicates the
cluster-speciﬁc parameter corresponding to the cth group. The mixing propor-
tions for the classes, p = (p1, . . . , pK) have been assigned a Dirichlet prior,
with concentration parameter α/K which approaches 0 as K goes to inﬁnity.
By integrating out p we can write the prior for the ci as the product of the
conditional probabilities as shown below:
P (ci = c|c1, . . . , ci−1) = P (c1, . . . , ci−1, ci = c)
P (c1, . . . , ci−1)
=

p P (c1, . . . , ci−1, c|p)f (p)dp

p P (c1, . . . , ci−1|p)f (p)dp
=

p P (c1|p) . . . P (ci−1|p)P (c|p) (K
i=1 p
α
K −1
i
dp

p P (c1|p) . . . P (ci−1|p) (K
i=1 p
α
K −1
i
dp
=

p p
ni,1+ α
K −1
1
. . . p
ni,c+ α
K −1+1
c
. . . p
ni,K+ α
K −1
K
dp

p p
ni,1+ α
K −1
1
. . . p
ni,c+ α
K −1
c
. . . p
ni,K+ α
K −1
K
dp
where ni,l is the number of cl for l < i that are equal to l ∈{0, K}. The above
integration yields the normalizing constant of a Dirichlet distribution, hence we

UNSUPERVISED CLASSIFICATION AND BAYESIAN CLUSTERING
129
obtain
P (ci = c|c1, . . . , ci−1) =

(ni,1+ α
K )...
(ni,c+ α
K +1)
(ni,K+ α
K )

(i+α)

(ni,1+ α
K )...
(ni,c+ α
K )
(ni,K+ α
K )

(i−1+α)
= 
(ni,c + α
K + 1)
(i −1 + α)

(ni,c + α
K )
(i + α)
= ni,c + α
K
i −1 + α .
(7.39)
Now letting K →∞, the prior of ci is
P (ci = c|c1, . . . , ci−1) →
ni,c
i −1 + α
P (ci ̸= cj|c1, . . . , ci−1) →1 −
K→∞

c=1
nc
i −1 + α →
α
i −1 + α
(7.40)
Note: Even though P (ci ̸= cj|c1, . . . , ci−1) = 0 for ﬁnite K it is > 0 when K →
∞.
It is clear that the limit of this model as C goes to inﬁnity is equivalent to the
Dirichlet process mixture model due to the correspondence between the condi-
tional probability of θi in equation (7.37) and equation (7.40). Moreover, we can
represent this joint prior distribution of θ as product of conditional distributions
after marginalizing G. We combine this prior with the data to make posterior
inferences about the clustering process.
A colorful description of this clustering procedure and its corresponding clus-
ter indicator distribution is given by the Chinese restaurant process presented in
Arratia
et al. (1992). Here, customers enter a restaurant sequentially and sit
one after another. Initially, all tables are folded up, so that one table is opened
when the ﬁrst customer enters. After customers 1, . . . , i −1 are seated, the ith
customer will either choose an empty table with probability α/(i −1 + α) for
some α > 0 or an occupied table with probability proportional to the number
of occupants at the given table. This scheme is same as the representation in
equation (7.40).
Posterior Inference The hierarchical model in this setup is
xi|θi ∼P (θi),
θi|G ∼G,
(7.41)
G ∼DP (G0, α).

130
BAYESIAN ANALYSIS OF GENE EXPRESSION DATA
Now assigning a Dirichlet process prior for G, a priori the full conditional dis-
tribution (conditioned on all the θs) for θi will be
θi|θ−i ∼
1
n −1 + α

j̸=i
δ(θj) +
α
n −1 + α G0.
(7.42)
This conditional prior can be derived from equation (7.37) by imagining that i is
the last of the n observations, as we may, since the observations are exchange-
able so the ordering does not matter. When combined with the likelihood, the
conditional distribution we obtain is
θi|θ−i, xi ∼

j̸=i
qi,jδ(θj) + riHi.
(7.43)
Here Hi is the posterior distribution for θ based on the baseline prior G0 and the
single observation xi, with likelihood P (xi|θ). qi,j and ri are the mixing weights
whose values are deﬁned as qi,j = bP (xi, θj) and ri = bα

P (xi|θ)dG0(θ)
where b is such that 
j̸=i qi,j + ri = 1. We only consider the conjugate case
where the baseline prior G0 is conjugate to the likelihood F so that computing
the integral deﬁning ri and sampling from Hi are feasible operations. In this
conjugate case we can use equation (7.43) as the conditional distribution in our
Gibbs sampling framework (Escobar 1994; Escobar and West, 1995). To obtain
a more suitable algorithm we can draw the cluster indicators directly as has
been shown in prior modeling in equation (7.40). The argument is similar to the
prior modeling situation where we consider a ﬁnite mixture model as in (7.38),
then integrate out the mixing proportion p. When K is ﬁnite, within each Gibbs
iteration, a new value will be picked for each clustering indicator ci given the
data xi, cluster speciﬁc parameter θc, and all other cj for j ̸= i (written as
c−i). Then we draw a new value for each θc conditioned on the the data xi and
ci = c. The conditional probabilities for ci will be
P (ci|c−i, xi, θ) = bP (xi|θc)n−i,c + α/K
n −1 + α ,
(7.44)
where n−i,c is the number of cj for j ̸= i that are equal to c, and b is the appro-
priate normalizing constant. The calculation of this probability is very similar to
equation (7.39) except that the conditional prior (as in equation (7.42)) has been
multiplied by the likelihood P (xi|θc) and except for using the exchangeability
argument so that we can consider i as the last observation. That way we calculate
the full conditional probability rather than sequential conditional probability as
in equation (7.40).

UNSUPERVISED CLASSIFICATION AND BAYESIAN CLUSTERING
131
When C goes to inﬁnity, we consider only those θc that are currently asso-
ciated with some observations. The conditional distributions for the cluster indi-
cators will be
P (ci = c|ci, xi, θ) = b
n−i,c
n −1 + α P (xi|θc),
P (ci ̸= cj∀j ̸= i|ci, xi, θ) = b
α
n −1 + α
	
P (xi|θ)dG0(θ),
(7.45)
where θ is the set of θc currently associated with one of the clusters and b is the
normalizing constant that makes the above probability sum to one.
The numerical values of the ci are arbitrary and they only represent whether
or not ci = cj. They determine the conﬁguration in which the data items are
grouped in accordance with shared values for θ. The numerical values of ci are
usually chosen for programming convenience. In a Gibbs iteration, if ci chooses
a value not equal to any other cj, a value for θci is chosen from Hi, the posterior
distribution based on the prior G0 and the observation xi.
The Gibbs sampling algorithm works as follows (Bush and MacEachern,
1996; West et al.,1994). In each iteration of the Gibbs sampling we conditionally
draw from the states c = (c1, . . . , cn) and θ = (θc : c ∈{c1, . . . , cn}):
(i) For i = 1, . . . , n, if the present value of ci is not associated with any other
observation (n−i,ci = 0) then remove the corresponding θci from the state. Draw
a new value for ci from ci|c−i, xi, θ using equation (7.45). If the new ci is not
associated with any other observation, draw a value for θci from Hi and add it
to the state.
(ii) For all c ∈{c1, . . . , cn}, draw a new value from θc conditioned on all the
data X for which ci = c. that is, from the posterior distribution based on the
prior G0 and all the data points currently associated with latent class c.
This algorithm can be made more efﬁcient by further marginalization of the
parameters θ in a conjugate setup (MacEachern, 1994; Neal, 1992). In this case
the cis are updated using the conditional probabilities:
P (ci = c|ci, xi) = b n−i,c
n−1+α

P (xi|θ)dH−i,c(θ),
P (ci ̸= cj∀j ̸= i|ci, xi, θ)
= b
α
n−1+α

P (xi|θ)dG0(θ),
(7.46)
where H−i,c is the posterior distribution of φ based on the prior G0 and all the
observations xj for which j ̸= i and cj = c. In this situation the Gibbs sampling
contains one step to draw ci from ci|c−i, xi as deﬁned in equation (7.46).
7.9.1
Inﬁnite Mixture of Gaussian Distributions
Medvedovic and Sivaganesan (2002) used an inﬁnite mixture of Gaussian dis-
tributions to cluster gene expression data using the methodology described in
the previous section. They use the Gaussian likelihood for P (x|θ) as well as an

132
BAYESIAN ANALYSIS OF GENE EXPRESSION DATA
assigned Gaussian distribution for the baseline prior G0. Hence the hierarchical
model in (7.41) adopted to
xi|θi ∼N(µi, σ 2
i I),
µi, σ 2
i |G ∼G,
G ∼DP (G0, α),
(7.47)
where
G0
is
NIG
distributed.
Hence
µi|λ, r ∼N(λ, r−1I)
and
σ 2
i ∼
Gamma(a/2, ab/2). They have further hierarchical structure given by
λ|µx, σ 2
x ∼N(µx, σ 2
x I),
a ∼Gamma(0.5, 0.5),
b|σ 2
x ∼Gamma

0.5,
1
2σ 2x

,
G ∼DP (G0, α),
where µx is the average of all proﬁles analyzed and σ 2
x is the average of
gene-speciﬁc sample variances based on all proﬁles analyzed. They assigned
the precision parameter α = 1 though further priors could be assigned on α. This
is not exactly a conjugate setup but the corresponding Gibbs sampling algorithm
had been described in that paper. The conditional distribution of the cluster can
be obtained easily using equation (7.45) with speciﬁc Gaussian likelihood and
the above mentioned prior structure.
We can use the conditional distributions in a Gibbs sampling framework to
generate clusters from the posterior distribution. To identify the best cluster (or
some good ones) out of these samples itself is a challenging problem. The max-
imum a posteriori (MAP) approach usually fails in this situation as the state
space of possible clustering combinations can be very large and different clus-
tering occurs frequently with very small posterior probabilities. Quintana and
Iglesias (2003) provide a search algorithm to approach the best model by min-
imizing a penalized risk, but for large data sets the computational constraints
can be prohibitive. Medvedovic and Sivaganesan used the sequence of clustering
generated by the Gibbs sampler after the burn-in cycles to calculate pairwise
probabilities for two genes to be generated by the same pattern. This pairwise
probability is estimated by the relative frequency with which these two genes are
in the same cluster among the MCMC samples. This probability measure can be
treated as a distance measure to calculate pairwise distances between the genes.
Based on these distances, clusters of similar expression proﬁles are created by
the complete linkage approach (Everitt, 1993).
Example 7.4
We have used the Gaussian inﬁnite mixture model (GIMM) code provided by
Medavedovic to reanalyze the yeast galactose data using 205 genes. To run this

UNSUPERVISED CLASSIFICATION AND BAYESIAN CLUSTERING
133
Figure 7.8
Starting up WINGIMM.
code, the ﬁrst step is to start the GIMM program (see Figure 7.8). Then input
all the required ﬁelds and press Compute. The program will compute the full
clustering conﬁguration. After completion press TreeView and you will get a tree
view window from which you can interpret the clustering conﬁguration of the
genes as in Figure 7.9. The ﬁnal clustering conﬁguration for the data is shown
in Figure 7.10.
Figure 7.9
Tree in WINGIMM.

134
BAYESIAN ANALYSIS OF GENE EXPRESSION DATA
Figure 7.10
Clustering tree in GIMM.
We identify the number of clusters using the adjusted Rand index (ARI:
Hubert and Arabie, 1985) which is derived from the Rand index (Rand, 1971).
Suppose n objects have been classiﬁed by two separate partitions A and B. Let
nij be the number of elements classiﬁed by both Ai ∈A and Bj ∈B. The Rand
index is evaluated as
ARI =
Data −Expected index
Maximum value −Expected index,
which is a number bounded between 0 and 1, 1 meaning perfect correspondence.
Assuming a generalized hypergeometric distribution, the expected value for the
index is
E


i,j
 nij
2

=


i

ni.
2
 
j

n.j
2


n
2

.

UNSUPERVISED CLASSIFICATION AND BAYESIAN CLUSTERING
135
The corresponding adjusted Rand index (ARI) is then derived (Hubert and Arabie,
1985) as
ARI =

i,j
)
nij
2
*
−
4

i
)
ni.
2
*

j
)
n.j
2
*5
6
)
n
2
*
1
2
4

i
)
ni.
2
*
+ 
j
)
n.j
2
*5
−
4

i
)
ni.
2
*

j
)
n.j
2
*5
6
)
n
2
*.
(7.48)
The expression patterns of these genes reﬂect four functional categories in the
GO Consortium (Ashburner et al., 2000), hence these classes have been used
as the external criterion to assess clustering results. We tried several different
cluster sizes and patterns in GIMM and calculated corresponding ARIs. The best
result was obtained with an ARI of 0.95. The mclust package, which uses the
BIC to determine the number of clusters to be 2, produces an ARI value of 0.78.
We observe that genes in the same GO category may not be co-expressed and we
need to be careful to select the external criterion for assessing clustering results.
More formal Bayesian model choice procedures should be utilized to compare
clustering results (Ray and Mallick, 2006).


8
Bayesian Graphical Models
8.1
Introduction
In Chapter 7 we used cluster analysis to identify co-regulated genes. Recently
there has been growing interest in the underlying relationships between genes.
For example, we may wish to obtain a network of dependencies between the dif-
ferentially expressed genes. In this chapter we do further investigation to develop
probability models to relate genes based on microarray data. In this way we can
extract more information from the data to develop biologically meaningful rela-
tionships between genes. We describe Bayesian graphical models to represent
these biological processes and we make posterior inference about these models
based on available gene expression data.
Recently several methods have been developed for modeling regulatory and
cellular networks based on genome-wide high-throughout data, including both
Bayesian network modeling (Friedman, 2004; Segal et al., 2003) and Gaussian
graphical modeling (Schaffer and Strimmer, 2005; Dobra et al., 2004). The goal
of such probability models is to investigate the patterns of association in order
to generate biological insights plausibly related to underlying biological and reg-
ulatory pathways. The interaction between two genes in a gene network deﬁned
by such graphical models does not necessarily imply a physical interaction, but
rather may refer to an indirect regulation via proteins, metabolites and noncoding
RNA (Bansal et al., 2007).
Bayesian Analysis of Gene Expression Data
B. Mallick, D. Gold, and V. Baladandayuthapani
2009 John Wiley & Sons, Ltd

138
BAYESIAN ANALYSIS OF GENE EXPRESSION DATA
8.2
Probabilistic Graphical Models
A central goal of all of our studies in this section is to construct a model for
genetic networks such that the model class incorporates dependencies between
genes. To understand how cells function, it is necessary to study the behavior
of genes in a holistic rather than in an individual manner because the expressions
and activities of genes are not isolated nor independent of each other.
Probabilistic graphical models are the basic tool to represent these dependence
structures. They provide a simple way to visualize the structure of a probability
model as well as providing insights into the properties of the model, including
conditional independence structures. A graph compromises vertices (nodes) con-
nected by edges (links or arcs). In a probabilistic graphical model, each vertex
represents a random variable (scalar or vector) and the edges express probabilis-
tic relationship between these variables. The graph deﬁnes the way in which the
joint distribution over all the random variables can be decomposed into a product
of factors contacting subset of the variables. There are two types of probabilistic
graphical models: directed graphical models (DAGs) or Bayesian networks where
the edges of the graphs have a particular directionality which expresses causal
relationships between random variables; and (ii) undirected graphical models in
which the edges do not carry the directional information. In this book we concen-
trate on Bayesian networks; however, Gaussian undirected graphical models are
also promising tools to explore relationships between genes. For further infor-
mation about undirected Gaussian graphical models, see Schaffer and Strimmer
(2005), Dobra et al. (2004), Wei and Li (2007), and references therein. In addi-
tion, there have been a number of alternative ways to model gene regulatory
networks, including neural networks (Weaver et al., 1999), differential equations
(Mestl et al., 1995), and Boolean networks (Huang, 1999).
8.3
Bayesian Networks
Bayesian networks are graphical models that explicitly represent probability rela-
tionships between variables x1, x2, . . . , xn (Pearl, 1988; Jensen, 1996). Suppose
that xi represents the mRNA expression level of a gene. xi could be continuous
for original expression data, or discrete when the expression levels are discretized
into two or more categories (e.g. expressed/not expressed, high/medium/low),
using appropriate thresholds. The model structure embeds conditional dependen-
cies and independencies and efﬁciently speciﬁes the joint probability distribution
of all the variables. Bayesian networks describe joint processes processes com-
posed of locally interacting components; that is, the value of each component
directly depends on the value of a relatively small number of parent components.
Through this Bayesian network we obtain the graphical model of joint probability
distributions that capture properties of conditional independence between vari-
ables. This local nature of the Bayesian network is attractive for computational
efﬁciency.

BAYESIAN GRAPHICAL MODELS
139
By way of illustration, we consider a situation with ﬁve genes A, B, C, D, E
(8.1). Here xi, i = A, . . . , E, represents the gene expression correspond-
ing to the ith gene and the joint distribution of xs is speciﬁed by a
conditional
distribution
deﬁned
by
the
graphical
model.
Suppose
that
gene B is a transcription factor of gene C. Gene A does not directly
affect C, and once we ﬁx the expression level of B we will observe
that A and C are independent. In other words, the effect of A on C is
mediated through B. In this case we say that A and C are conditionally
independent given B. Similarly, B and D are regulated by A. Thus, B and D
are conditionally independent once we know the expression level of A. Gene
E inhibits the transcription of A and in our graphical model we represent it
by placing an arc form E to B. The expression of B is regulated by two genes
A and E which are known as B’s parents, denoted as Pa(B). Hence the joint
probability distribution can be speciﬁed based on the graphical structure as
P (xA, xB, xC, xD, xE) = P (xC|xB)P (xB|xA, xE)P (xD|xA)P (xA)P (xE).
The
procedure for writing the the joint distribution is based on sequential decompo-
sition of the joint probability distribution through conditional distributions and
use of conditional independence arguments based on the graph. For example,
P (xA, xB, xC, xD, xE) = P (xC|xA, xB, xD, xE)P (xA, xB, xD, xE).
Now
from
the graph it is clear that xC only depends on xB and, conditional on xB (i.e.
if we know the value of xB), it is independent of the other three random
variables. Hence we can express P (xC|xA, xB, xD, xE) as P (xC|xB). Next we
can further decompose P (xA, xB, xD, xE) as P (xB|xA, xD, xE)P (xA, xD, xE).
In the same way, using the dependence structure of the graph and using
the conditional independence argument, P (xB|xA, xD, xE) = P (xB|xA, xE).
Similarly, decompose P (xA, xD, xE) = P (xD|xA, xE)P (xA)P (xE). Due to
conditional independence, P (xD|xA, xE) = P (xD|xA). Also xA and xE are
marginally independent hence P (xA, xE) = P (xA)P (xE). After decomposition
of these conditional distributions and multiplying them properly we obtain the
joint distribution. Hence the whole network model can presented in terms of the
relationship between parents and children, and local models can be composed
through conditional dependence assumptions.
In general, in our application the variables are expression levels of genes
and the relationships between these variables are represented by a directed graph
in which vertices correspond to variables and directed edges between vertices
represent their dependencies. The speciﬁcation contains two components. The
ﬁrst component consists of a directed acyclic graph G(V, E) with a node set V
corresponding to the random variables x1, . . . , xn, and an edge set E on these
nodes. The edges capture the conditional independence structure in the graph-
ical model. The existence of an edge between two nodes implies conditional
dependence between them. A node is conditionally independent of all other
nodes given its parents in the networks. The second component is a set of con-
ditional probability distributions for each node in the graph G. The probability
distributions are locally speciﬁed by the conditional distribution P {xi|P a(xi)}.

140
BAYESIAN ANALYSIS OF GENE EXPRESSION DATA
The overall list of marginal and conditional independencies represented
by the directed acyclic graph is summarized by the local and global Markov
properties. The local Markov property states that each node is independent of
its nondescendants given the parent nodes and leads to a direct factorization
of the joint distribution of the network variables into the product of the
conditional distribution of each variable xi given its parents P a(xi). For
example, in Figure 8.1 we have a network of ﬁve variables. As shown
above, P (xA, xB, xC, xD, xE) = P (xC|xB)P (xB|xA, xE)P (xD|xA)P (xA)P (xE),
where we ﬁrst replaced P (xC|xA, xB, xD, xE) by P (xC|xB) due to the
conditional independence assumption. Now xB is the parent of xC, hence
P (xC|xB) = P (xC|P a(xC)). A similar argument works for all other conditional
distributions in the expression.
E
A
D
B
C
Figure 8.1
Example of a simple network structure.
Hence, in Bayesian networks, the conditional probability of xi given all its pre-
decessors in the graph is equal to the conditional probability of xi given only the
Markovian parents of xi, denoted by P a(xi). We can express the joint probability
of p network variables as a product of conditional probabilities as
P (x1, . . . , xp) =

i
P{xi|P a(xi)}.
(8.1)
This fact facilitates the economical representation of joint distributions in
Bayesian networks, since the entire joint distribution table does not need to

BAYESIAN GRAPHICAL MODELS
141
be stored. This property is the core of many search algorithms for learning
Bayesian networks from data.
On the other hand, the global Markov property summarizes all conditional
independencies embedded by the directed acyclic graph by identifying the
Markov blanket of each node. The Markov blanket of a node is given by
the parents, the children, and the parents of the children of that node which
are required to specify the complete conditional distribution of that node.
For example, the members of the Markov blanket of xE are xB (child) and
xA (parent of the child). Similarly, the Markov blanket of xB contains xE,
xA (parents) and xC (child). The important result is that x is conditionally
independent of other variables (not included in the Markov blanket) conditioned
on the variables within the Markov blanket. So xE is independent of all other
variables conditioned on xB and xA. This property is the foundation of many
algorithms such as Gibbs sampling. Gibbs sampling uses the global Markov
property to express the complete conditional distribution of each node xi given
the current values of the other nodes. The general expression for the complete
conditional for xi is
P (xi|all other xj, j ̸= i) ∝P (xi|P a(xi))

l
P (ch(xi)l|P a(c(xi)l)),
(8.2)
where P a(xi) and ch(xi) are values of the parents and children of xi and ch(xi)l
are values of the parents of the lth child of xi.
8.4
Inference for Network Models
To draw inferences about a Bayesian network based on variables x1, . . . , xp,
we
start
with
n
measurements
of
these
p
variables
in
a
data
set
D = [{x1(1), . . . , xp(1)}, . . . {x1(n), . . . , xp(n)}].
The
data
for
our
appli-
cations are gene expression microarrays that simultaneously measure the
level of mRNA transcription for the genes. The network model contains two
different components for inference based on the data: the graphical structure
of conditional dependencies, which is more of a model identiﬁcation or model
selection problem; and estimation of the parameters corresponding to the
conditional distributions to quantify the dependence structure. Thus we need
to identify the structure of the graphical model and estimate the parameters
corresponding to this model. Conditioned on a network model, the posterior
inference of the parameters can be done using standard approaches. The more
challenging task is to identify the network model which will best ﬁt the data.
This problem can be expressed as a model selection problem where we have a
model space with several network models M = {N1, . . . , Ng}, where Ni is the
ith network model describing a particular dependency structure of x1, . . . , xp.
Criterion-based methods using the BIC and minimum description length (mdl)
are popular due to fast computation algorithms (Spirtes et al., 1993; Lauritzen,
1996; Whittaker, 1990), but methods based on the marginal likelihood or

142
BAYESIAN ANALYSIS OF GENE EXPRESSION DATA
the Bayes factor are more coherent model selection methods in a Bayesian
framework (Sebastiani et al., 2003, 2005).
If P (Nh) is the prior probability of the hth network, then using Bayes’
theorem we can ﬁnd the posterior probability of the hth network as
P (Nh|D) ∝P (Nh)P (D|Nh),
(8.3)
which is the evidence of support for the hth network from the data. The difﬁ-
cult part is the computation of the marginal likelihood P (D|Nh) as it contains
high-dimensional integration as P (D|Nh) =

θh P (D|θh, Nh)P (θh|Nh)dθh where
θh is the vector of parameters corresponding to the jth network model which
is possibly of very high dimension. The local Markov property and conjugate
structures can be used to reduce the complexity of this computation. The local
Markov properties encoded by the network Nh imply that the joint distribution
of the nodes for the lth sample can be written as
P (Dl|θh) =
p

i=1
P (xi(l)|P a(xi(l)), θh),
(8.4)
where Dl = {x1(l), . . . , xp(l)} is the data from the lth sample, P a(xi(l)) is the
conﬁguration of the parents of xi, and θh is the set of parameters for the network
Nh. By assuming exchangeability of the data, that is, samples are conditionally
independent given the model parameters, the likelihood is given by
P (D|θh) =
n

l=1

i=1
P (xi(l)|P a(xi(l)), θh).
(8.5)
Choice of convenient local priors for θh can make the procedure compu-
tationally efﬁcient, though it may depend on the structure of the likelihood.
One approach is the use of priors following the directed hyper-Markov law
(Dawid and Lauritzen, 1995). Under this assumption, the prior density P (θh)
admits the same factorization of the likelihood function, hence can be writ-
ten as P (θh) = (p
i=1 P (θhi), where θhi is the subset of parameters used to
describe the dependency of xi on its parents. Thus the marginal likelihood can be
expressed as
P (D|Nh) =
n

l=1
p

i=1
	
θhi
P (xi(l)|P a(xi(l)), θhi)P (θhi)dθhi =

i
P (D|Nhi),
(8.6)
where P (D|Nhi)) = (
l

θhi P (xi(l)|P a(xi(l)), θhi)P (θhi)dθhi, the likelihood
contribution of the ith node. To complete the hierarchical prior speciﬁcations, we
need to specify priors for the networks. We may obtain a further simpliﬁcation
by assuming decomposable network prior probabilities which can be expressed
as P (Nh) = (p
i=1 P (Nhi) (Heckerman et al., 1995). Using this prior, the

BAYESIAN GRAPHICAL MODELS
143
posterior can be expressed in a product form as P (Nh|D) = (p
i=1 P (Nhi|D)
where P (Nhi|D) is the posterior probability weighting the dependency of xi on
the set of of parents speciﬁed by the model Nhi. The basic assumption here is
that the prior probability of a local structure Nhi is independent of the other
local dependencies Nhk, where i ̸= k. One choice of this local prior probability
is P (Nhi) = (a + 1)−1/b where a + 1 is the cardinality of the model space and
b is the cardinality of the set of variables.
Simpler assumptions of these types have a huge impact in the computational
efﬁciency and induce local computation. To compare networks that differ for the
parent structure of xi, we only need to compute the local marginal likelihood cor-
responding to xi. Therefore, two local network structures Nh and Nt that specify
different parents for the variable xi can be compared by evaluation of local pos-
terior odds as P (Nhi/D)/P (Nti|D) = P (D|Nhi)/P (D|Nti).P (Nh)/P (Nt). Fur-
thermore, with the assumption of uniform priors, maximization of the marginal
likelihood at individual nodes can provide information about the local model.
Parametric inference given a network can be done using the posterior distri-
bution of the parameter:
P (θh|D) = P (D|θh)P (θh)
P (D|Nh)
=
p

i=1
P (D|θhi)P (θhi)
P (D|Nhi)
.
It is clear that conjugate choices of prior P (θh) with respect to the likelihood
P (D|θh) allow us to obtain the local marginal likelihoods explicitly and thus
produce computationally efﬁcient algorithms. The next two examples will be
based on these conjugate models.
8.4.1
Multinomial-Dirichlet Model
In this situation we use gene expression values discretized into two categories, 0
and 1, depending on whether the genes are similar to (not expressed) or different
(expressed) than the respective control. One possible ﬁner discretization involves
three categories, −1, 0, and 1, depending on whether the expression level is
signiﬁcantly lower than (negatively expressed), similar to (not expressed), or
greater than the respective control. The control expression level of a gene can be
either determined experimentally (as in the method of DeRisi et al., 1997) or set
as the average expression level of the gene across the experiment. The meaning of
‘signiﬁcantly’ is determined by setting a threshold on the ratio between measured
expression and control.
In general, suppose the variables x1, . . . , xp are all discrete and can take val-
ues from ai categories. The multinomial distribution will be a natural local model
for the conditional distributions. Suppose that, for the ith node, the conditional
probability that xi is in the kth category conditioned on the event that the parent
of xi, P a(xi), is in the jth setup is P (xi = k|P a(xi) = j) = θijk.
Figure 8.2 describes a simple network where x3 depends on x1 and x2.
Each xi is binary (so ai = 2) taking values 1 or 2 (true/false, expressed/not

144
BAYESIAN ANALYSIS OF GENE EXPRESSION DATA
x1
1
2
q11
q12
x2
1
2
q21
q22
x1
x2
x3
x3
Pa(x3)
Pa(x3)
x1
x2
True
False
Pa(x3)1
1
1
q311
q312
Pa(x3)2
1
2
q321
q322
Pa(x3)3
2
1
q331
q332
Pa(x3)4
Pa(x3)1
Pa(x3)2
Pa(x3)3
Pa(x3)4
2
2
q341
q342
Database of 10 cases
case k
x1
x2
x3
1
1
1
2
2
1
2
1
3
2
1
1
4
1
2
2
5
1
1
1
6
1
2
1
7
2
2
1
8
1
2
2
9
1
1
2
10
2
2
1
⇓
x3
x1
x2
1
2
1
1
1
2
1
2
2
2
2
1
1
0
2
2
2
0
Figure 8.2
A simple Bayesian network describing the dependency of x3 on x1
and x2, which are marginally independent. The table on the left describes the
parameters θ3jk(j = 1, . . . , 4 and k = 1, 2) used to deﬁne the conditional distri-
butions of x3 = x3k|P a(x3)j, assuming all variables are binary. The two tables
on the right describe a simple database of seven cases, and frequencies n3jk. The
full joint distribution is deﬁned by the parameters θ3jk, and the parameters θ1k
and θ2k that specify the marginal distributions of x1 and x2.
expressed). The table clariﬁes the parameters corresponding to the model. For
example, θ11 represents the probability that x1 is in the ﬁrst category while θ12
is the probability that x1 is in the second category. θ312 denotes that x3 is in the
second category given that x1 and x2 (the parents) are in the ﬁrst conﬁguration,
which is x1 = 1 and x2 = 1.
Now the data set contains nijk which is the sample frequency of the joint
occurrence of xi = k and P a(xi = j). For example, n312 denotes the frequency
that x3 is in category 2 given x1 and x2 are in the ﬁrst conﬁguration (i.e. x1 = 1
and x2 = 1), and the value is n312 = 2. We can calculate the marginal frequen-
cies as nij = 
k nijk, which is the marginal frequency of P a(xi = j). From
the table we can calculate n11, which is the number of cases where x1 is in
the 1st category and n11 = 7. Similarly, n21 = 4. We can calculate all other fre-
quencies in similar fashion by counting the cases. Using these frequencies we

BAYESIAN GRAPHICAL MODELS
145
can write the multinomial likelihood function as
P (D|N) ∝{θ7
11 × θ3
12}{θ4
21 × θ6
22}{θ1
311θ2
312 × θ2
321θ2
322 × θ1
331θ0
332 × θ2
341θ0
342}.
In the general setup the likelihood function will be
P (D|θh) =

ijk
θijknijk.
The hyper-Dirichlet distribution with parameters αijk is the conjugate
hyper-Markov law and the density function is P (θijk) ∝(
ijk θ
αijk−1
ijk
. The
hyperparameters αijk usually satisfy the consistency rule 
j αij = α (Geiger
and Heckerman, 1997) for all i, so that the parameters θ satisfy global and local
parameter independence (Spiegelhalter and Lauritzen, 1990). One of the simple
choices is αijk = α/(cili) where li is the number of states of the parent xi which
is known to obey the symmetric Dirichlet distribution. With this choice, as we
have a multinomial-Dirichlet conjugate form, we can easily derive the marginal
likelihoods for the network N as
p

i=1
P (D|Nhi) =

ij

(αij)

(αij + nij)

k

(αijk + nijk)

(αijk)
,
where 
 denotes the gamma function.
8.4.2
Gaussian Model
Discretization of the expression measurements may lead to loss of informa-
tion. Another approach is to exploit the continuous expression data to build
the network model. Now the variables x1, . . . , xp are all continuous and it is
assumed that the conditional distribution of each variable xi given its parent
P a(xi) = {xi1, . . . , xip(i)} follows a Gaussian distribution with mean µi and vari-
ance σ 2
i . The local dependence structure has been created through the conditional
mean structure as a linear function of the parent variables
µi = βi0 +
pi

j=1
βijxij.
(8.7)
In this way, the dependency xi on the parent xij is equivalent to having the
regression coefﬁcient βij ̸= 0.
Suppose we have n i.i.d. samples of observations and xi = {xi1, . . . , xin}.
The design matrix Zi is of dimension n × p(i) + 1, where the lth row (corre-
sponding to the lth sample) is given by (1, xi1l, xi2l, . . . , xip(i)l) and β denotes

146
BAYESIAN ANALYSIS OF GENE EXPRESSION DATA
the (P a(i) + 1) × 1 vector of regression coefﬁcients (βi0, βi1, . . . , βiP a(i)). The
linear model can be expressed as
xi = Ziβ + ϵi,
where ϵ is random error with mean 0 and variance σ 2In. Based on this model,
the likelihood can be written as
P (D|θ) ∝
p

i=1
τ n/2
i
exp{−τi(xi −Ziβi)′(xi −Ziβi)/2}.
The next step is to assign conjugate priors for βi and τi to obtain the marginal
likelihood explicitly.
The conjugate prior for βi, τi is NIG, where we assume βi|τi ∼N(βi0,
(τiR)−1) and τ ∼Gamma(αi1, αi2). We assume R to be an identity matrix.
Under this speciﬁcation the marginal likelihood is available explicitly as
P (D|N) ∝

i
det R1/2
i0 
(νi∗/2)(νi0σ 2
i0)
νi0/2
det R1/2
i∗
(νi0/2)(νi∗σ 2
i∗)
νi∗/2 ,
(8.8)
where the updated parameters after observing the data are given by
Ri∗= Ri0 + Zi′Zi,
βi∗= R−1i∗(Ri0βi0 + Zi′xi),
αi1∗= νi0/2 + n/2,
αi2∗= {(−β′
i∗Ri∗βi∗+ xi′xi + βi0′Ri0βi0) + 1/αi2}−1,
νi∗= νi0 + n,
σin = 2/(νi∗αi2∗).
8.4.3
Model Search
The conjugate prior and local modeling allow us to calculate the marginal like-
lihood values explicitly. However, the space of possible sets of parents for each
variable grows exponentially with the number of candidate parents, and efﬁ-
cient search procedures are needed to develop a network model successfully.
One simple way is to deﬁne a set of candidate parents for each variable xi,
then implement an independent model selection for each variable xi, and link
together the local models selected for each variable xi. The K2 algorithm (Cooper
and Herskovitz, 1992) can further reduce the computation complexity by means
of a bottom-up strategy. This strategy starts with a null model (say, one in
which xi has no parents) and evaluates its marginal likelihood. In the next step,

BAYESIAN GRAPHICAL MODELS
147
a single parent is allowed for each node and the marginal likelihood of each
model is evaluated. If the maximal marginal likelihood of these models is larger
than the marginal likelihood of the independence model, the parent that increases
the likelihood most is accepted and the algorithm proceeds to evaluate models
with two parents. If none of the models has marginal likelihood that exceeds that
of the independence model, the search stops. The K2 algorithm is implemented
in Bayesware Discoverer and the R package deal (Boettcher and Dethlefsen,
2003). The algorithm is based on a greedy search strategy and therefore can
be trapped in local maxima and induce spurious dependency. However, it often
performs as well as other more complex algorithms (Cooper and Herskovitz,
1992).
Bayesware Discoverer is a knowledge discovery system based on the enabling
technology of Bayesian networks. It deploys a uniﬁed framework which regards
the knowledge discovery process as the automated generation of Bayesian net-
works from data. The core of Bayesware Discoverer implements a novel method-
ology to discover Bayesian networks from possibly incomplete databases, a
generalization of the well-known Bayesian methodology for learning Bayesian
networks from data. Bayesware Discoverer implements several search strategies,
and in particular the K2 algorithm. The K2 algorithm works by selecting the,
a posteriori, most probable Bayesian network from a subset of all the possible
Bayesian networks. The subset of models is selected by the user, who is asked to
identify the order in which the variables in the data set are evaluated. The rank of
each variable deﬁnes the set of variables that will be tested as possible parents:
the higher the order of a variable, the larger the number of variables that will be
tested as its possible parents. If the user does not specify an order, Bayesware
Discoverer uses the order of appearance of the variables in the database to build
an initial network that can be further explored to select other dependencies to
be tested. The implementation of the K2 algorithm in Bayesware Discoverer
starts from the highest ranked variable, say x1, and computes, ﬁrst, the marginal
likelihood of the model that assumes no links pointing to x1 from the other
variables in the list. The next step is the computation of the marginal likelihood
of the models with one link only pointing to x1 from the other variables in the
list. If none of these dependencies has a marginal likelihood larger than that
of the model without links pointing to x1, the latter is taken as most proba-
ble model and the next variable in the list is evaluated. If at least one of these
models has a marginal likelihood larger than that of the model without links
pointing to x1, the corresponding link is accepted and the search continues by
trying adding two links pointing to x1, and so on, until the marginal likelihood
does not increase any more. Once the evaluation of one variable is terminated,
the algorithm removes the variable x1 from the list by replacing it with the
second variable in the original list and repeats the same search. The fact that
data are complete, as no entries are reported as unknown in the data set, is a
key feature in keeping the induction of Bayesian networks computationally fea-
sible. When data are incomplete under an ignorable missing data mechanism,

148
BAYESIAN ANALYSIS OF GENE EXPRESSION DATA
the marginal likelihood of a Bayesian network becomes a mixture of marginal
likelihood induced from the possible completions of the data, with the conse-
quent loss of decomposability properties. Bayesware Discoverer implements a
method based on bound and collapse to compute a ﬁrst-order approximation of
the posterior probability of a Bayesian network for the missing data case. As the
current version of Bayesware Discoverer handles discrete variables only, contin-
uous variables are discretized into a number of bins that can be chosen by the
user and there are two discretization methods available. Continuous variables can
be discretized either into a number of equal-length bins, or into a number of bins
having approximately the same frequency of cases.
The alternative search procedure is to impose some restrictions on the
search space and use the deomposibility of the posterior probability of the
network. One way is to create some ordering on the variables such as xi > xj
if xi cannot be a parent of xj. In this way the search space is reduced to
a subset of networks in which only a subset of directed associations exist.
The ordering information can be induced using known pathway information
(Segal et al., 2003). The ordering operation can be largely automated by using
programs such as MAPPFinder or GenMAPP. Other more extensive search
strategies are based on genetic algorithms (Larranaga et al., 1996), stochastic
methods (Singh and Valtorta, 1995), or MCMC methods (Friedman and Koller,
2003).
It is very common in practice to interpret Bayesian networks to represent
causal relationships. Actually parent–child relationships in a derived network
need not be causal. Furthermore, an edge between variables in the network does
not imply a direct biological mechanism. There may exist several intermediate
variables between the linked variables that may not have been included in the
model. Any network developed from the gene expression data should be bio-
logically validated. Derivation of a biological explanation for the links may not
be possible, but, based on the learning about the biological interactions between
these genes and their products, we can interpret the signiﬁcance of the edges that
occur in this network. New insights into the types of biological inference can
thus be gained from the network.
8.4.4
Example
The gene expression data used in this study result from a study of 31 melanoma
samples. For that study, total mRNA was isolated directly from melanoma biop-
sies, and ﬂuorescent cDNA from the message was prepared and hybridized
to a microarray containing probes for 8150 cDNAs (representing 6971 unique
genes). Several analytical methods were applied to the expression probes from
well-measured genes to visualize the overall expression pattern relationships
among the 31 cutaneous melanoma tumor samples. A statistical measure was
employed to generate a gene list weighted according to the gene’s impact on
minimizing the volume occupied by the groups and maximizing center-to-center

BAYESIAN GRAPHICAL MODELS
149
inter-group distance. The ten most important genes were selected for the]break
analysis:
G1 PHO-C
G2 WNT5A
G3 PIRIN
G4 MART-1
G5 S100
G6 HAHDB
G7 STC2
G8 RET-1
G9 SYNUCLEIN
G10 MMP3
Figure 8.3
Loading the data.
Figure 8.4
Screen with unconnected network.

150
BAYESIAN ANALYSIS OF GENE EXPRESSION DATA
Figure 8.5
Generated Network.
These ten genes were selected for analysis on the basis of either their known
or likely roles in the WNT5A-driven induction of an invasive phenotype in
melanoma cells, or their close predictive relationships between the genes.
To use Bayes Discoverer to analyze these data, the ﬁrst step is to load data
with a header column containing node names and place data in the columns,
as in Figure 8.3. After loading the data, a screen appears with all the nodes
unconnected, as in Figures 8.4. Then select Network and click on Generate
Network option which produces the network based on these data as shown in
Figure 8.5.

9
Advanced Topics
9.1
Introduction
In previous chapters, we concentrated on modeling aspects of microarray data
which conform to some standard form of data collection, in order to answer
relevant biological and scientiﬁc questions such as differential expression of
genes, classiﬁcation using gene expression proﬁles, and identiﬁcation of new
subtypes of diseases using clustering approaches. While most scientiﬁc questions
are addressed by the toolkit we have already described in detail, sometimes we
observe data from microarray experiments that require new classes of methods to
analyze them. Two important examples are time course experiments and studies
where the response is time-to-event. We brieﬂy discuss the Bayesian analytic
tools available to model such data.
9.2
Analysis of Time Course Gene Expression Data
Monitoring gene expression patterns over time provides key information about
the multidimensional dynamics of complex biological systems. In this time series
paradigm we are able to observe the emergence of coherent temporal responses
of many interacting components. For example, one of the most important ways
in which cells regulate gene expression is by using a feedback loop. Some of the
proteins regulate the expression of other genes by either initiating or repressing
transcription, and they are known as transcription factors (TFs). Cells exposed
to a new condition (e.g. infection, stress, starvation) react by activating a new
expression program. This program starts by activating a few TFs, which in turn
activate many other genes that act in response to the new condition. Snapshot
Bayesian Analysis of Gene Expression Data
B. Mallick, D. Gold, and V. Baladandayuthapani
2009 John Wiley & Sons, Ltd

152
BAYESIAN ANALYSIS OF GENE EXPRESSION DATA
data can only provide information about the expressed genes at that time point.
However, to identify the complete set of expressed genes as well as to determine
the interactions between these genes, it is necessary to measure gene expression
data over time. These time series expression experiments can answer several
questions about biological systems such as circadian rhythms (Panda et al., 2002;
Storch et al., 2002), genetic interaction and knockouts (Gasch et al., 2000; Zhu
et al., 2000), or progression of infectious disease (Nau et al., 2002; Whitﬁeld et
al., 2002; Xu et al., 2002).
Time course microarray data differ from static microarray data in that gene
expression levels at different time points can be correlated. Additionally, there
are very few replicates and time points, but a relatively large number of genes.
Hence, standard time series methods may be neither appropriate nor feasible,
and to analyze such dependent data we need to develop more complex statisti-
cal methods (Bar-Joseph, 2004; Androulakis et al., 2007). One big challenge is
to extract the continuous representation (over time) of all genes throughout the
course of the experiment. Furthermore, many experiments contain a large num-
ber of missing time points in a series for any given gene, making gene-speciﬁc
interpolation infeasible. Naive imputation for imbalanced data may lead to mis-
leading results. Also, the timing of the biological process may be variable so
the rate at which similar underlying processes (such as the cell cycle) unfold
can be expected to differ across organisms, genetic variants, and environmental
conditions. For these reasons, it is hard to combine time series expression data.
The static analysis of microarrays, proposed by Tusher et al. (2001), has recently
included an ad hoc approach to deal with time course microarray data where
slopes or signal areas from each time course curve are compared.
9.2.1
Gene Selection
Gene selection with time course microarray data is a relatively new area of
research. Bar-Joseph et al. (2003) considered a likelihood ratio statistic for com-
parison of the two B-spline curves represented for the genes. Guo et al. (2003)
applied estimating equation techniques to construct a variant of the robust Wald
statistic. Park et al. (2003) proposed a two-way ANOVA model with a non-
parametric permutation test to identify genes that have different gene expression
proﬁles among experimental groups. Storey et al. (2005) presented a signiﬁ-
cance method to identify differentially expressed genes under both independent
and longitudinal sampling schemes. Tai and Speed (2006) derived a one-sample
multivariate empirical Bayes statistic to rank genes in the order of differen-
tial expression from replicated microarray time course experiments. Luan and
Li (2004) proposed a B-spline model-based method for identifying periodic
expressed genes. Yuan et al. (2006) suggested a hidden Markov approach to
classify genes based on their temporal expression patterns.
Recently, Chi et al. (2007) proposed a Bayesian hierarchical model for time
course gene expression data. They denoted by ygij the vector of log-transformed
gene expression data for the gth gene, ith biological condition, and jth subject.

ADVANCED TOPICS
153
The vector ygij contains data across time, and its dimension mgij is determined by
the number of observed time points and replication over time, and is allowed to
differ across genes, biological conditions, and subjects to accommodate possible
missing expression data over time. The model is expressed as
ygij = xgijβgi + γijImgij + ϵgij,
(9.1)
where xgij denotes the mgij × p design matrix for the jth subject, gth gene, and
in the ith biological condition, which may include all the covariates, Imgij is an
mgij × 1 vector of ones, βgi is a p × 1 vector of regression coefﬁcients speciﬁc
to the gth gene and ith biological condition, and ϵgij is an mgij| × 1 vector of
measurement errors. The subsequent prior distributional assumption for βgi will
induce correlation structure between the expression data (over subjects, time, and
replicates) for the gth gene in the ith biological condition. γij is a random effect
which accounts for the subject-speciﬁc deviation from the mean expression of
the gene in the ith biological condition.
The prior for β induces correlation within and between genes over temporal
replicates. The prior distribution is speciﬁed as
βgi|µi, i ∼Np(µi, i),
which is conditionally independent (conditioned on µi and i). Further, it is
assumed that
µi ∼Np(mi, i),
so that marginally (integrating out µi) the βgi will be correlated. It can be shown
that, integrating out µi, cov(βgi, βhi) = i, for g ̸= h. The hyperprior i thus
induces a priori correlation between βgi and βhi for the ith biological condi-
tion, which in turn induces correlation between expression levels over different
genes. The prior hierarchy for βgi not only accounts for correlated gene expres-
sion over time but also allows for dependence in expression over different genes,
both within and between subjects in the same biological condition. Chi et al.
(2007) proposed an empirical Bayesian method of eliciting i. Conjugate pri-
ors are assumed for all other parameters. Based on this model, they proposed
gene selection criteria to identify genes that show changes in temporal expres-
sion patterns among biological conditions. This is a ﬂexible method which can
accommodate data generated from complex designs.
9.2.2
Functional Clustering
Clustering is another useful technique where the aim is to gain insight into those
genes that behave similarly over the course of the experiment. By comparing
genes of unknown function with proﬁles that are similar to genes of known
function, clues as to function may be obtained. Hence, co-expression of genes
is of interest. Most clustering algorithms are based on independent data. While

154
BAYESIAN ANALYSIS OF GENE EXPRESSION DATA
clustering is important, most clustering algorithms treat their input as a vector
of independent samples, and do not take into account the temporal relationship
between the time points. Thus, these algorithms cannot beneﬁt from the known
dependencies among consecutive points.
Recently there have been developments in clustering algorithms for time
series expression data. Schliep et al. (2003) present a hidden Markov model
based clustering algorithm for time series expression data. Holter et al. (2001)
used a dynamic version of the singular value decomposition (SVD) to model
the temporal dependence. Qian et al. (2001) used local alignment algorithms
to study time-shifted and inverted gene expression proﬁles and to infer causality
from time series proﬁles. Bar-Joseph et al. (2003) developed an optimally ordered
hierarchical clustering tree to identify clusters.
Bayesian clustering methods are attractive due to their ability to identify the
number of clusters automatically as well as to account for all sources of uncer-
tainty. Most Bayesian clustering methods start from the linear model structure
ygt = f (βg, t) + ϵgt,
where ygt is the log-transformed gene expression value for gth gene at the tth
time point (experiment), f is some potentially nonlinear function of β and ϵ is
some error process usually modeled as independent and Gaussian. The function
f can be written as a linear model using basis function expansion,
ygt = xtβg + ϵgt
(9.2)
where xt is the design matrix, which is usually a function of t. This is a general
framework and most of the existing Bayesian methods follow it by choosing xt
appropriately.
Ramoni et al. (2002) used an autoregressive model, their choice for xt being
the past responses. For example, to use the AR(p) model, the rows of X will be
{1, yg(t−1), yg(t−2), . . . , yg(t−p)}. They developed a probabilistic scoring metric
based on marginal likelihood and the use of conjugate priors to obtain explicit
expression for this likelihood.
Wakeﬁeld et al. (2003) used periodic basis functions in xt and exploited
model-based clustering methods to cluster these periodic functions. Heard et al.
(2005) used spline basis functions in xt and proposed a novel Bayesian version
of the hierarchical clustering algorithm based on marginal likelihood.
Ray and Mallick (2006) used wavelets as basis functions, integrating Dirich-
let process priors with these wavelets to obtain automatic clustering. Wavelet
representations are sparse and can be helpful in limiting the number of regres-
sors. Dimension reduction is inherent in their approach. Furthermore, wavelets
have the ability to capture functions of different degrees of smoothness. As an
example, the gene expression proﬁles of yeast cell cycles may occasionally depart
from the usual cyclic behavior and these shifts will be overlooked, in general, by
the periodic basis model. Similarly, due to induction of some speciﬁc hormones
or chemicals, gene expression data can show sudden changes, such as temporal

ADVANCED TOPICS
155
singularities. Both spline and periodic basis models will fail to identify them and
thus misclassify the genes in the clustering structure. Wavelet-based methods will
successfully cluster the time course data in these complex situations.
9.2.3
Dynamic Bayesian Networks
Dynamic Bayesian network (DBN) models are currently the focus of a growing
number of researchers concerned with discovering novel gene interactions and
regulatory relationships from expression data. DBNs are an extension of Bayesian
networks (Chapter 8). The main advantage of DBNs for gene expression data is
that, unlike Bayesian networks, which are acyclic, DBNs allow for cycles, which
are common in many biological systems. Additionally, DBNs can improve the
ability to learn causal relationships by relying on the temporal nature of the data.
Murphy and Mian (1999) ﬁrst proposed the use of DBNs to model time-course
microarray data, and Ong et al. (2002) were among the ﬁrst to implement DBNs
for this purpose. Their method used prior biological knowledge for grouping
together genes which are co-regulated. This allowed them to reduce the dimension
of the problem. Perrin et al. (2003) developed a DBN model containing hidden
variables to overcome both biological and measurement noise. Their method is a
direct extension of the linear regression model. Kim et al. (2003) demonstrated
the usefulness of DBNs. Husmeirer (2003) performed a simulation-based analysis
to determine the accuracy of DBNs in analyzing gene expression data. They
concluded that while the global network recovered by DBNs is not useful, local
structures can be recovered to a certain extent. Beal et al. (2005) used variational
Bayesian methods to approximate the posterior quantities in DBNs. It is clear that
DBNs are a promising direction for modeling temporal gene expression systems.
9.3
Survival Prediction Using Gene Expression Data
In the past few years, several microarray studies with time-to-event outcomes
have been collected. The main goal is to predict the time to event (also called
survival time) using gene expressions as explanatory variables. Survival times of
patients in clinical studies may shed light on the genetic variation. Sometimes it
is possible to relate the expression levels of a subset of genes with the patient
survival time. Apart from this, it is interesting to estimate the patient survival
probabilities as a function of covariates such as levels of clinical risk. Because
the event is not observed for the samples/array, standard regression techniques
cannot be applied since they do not (properly) take censoring into account. In
particular, in the case of microarrays, analysis is further complicated by the high
dimensionality.
A wide variety of methods have been proposed to handle time-to-event
microarray data. Perhaps the simplest of these are the univariate Cox proportional
hazards (PH) model for each gene and selecting those genes that pass a threshold
of signiﬁcance (Rosenwald et al., 2002). The problem with this approach is that
it does not account for correlation between the genes and consequently many

156
BAYESIAN ANALYSIS OF GENE EXPRESSION DATA
highly correlated genes are selected. Another approach ﬁrst clusters the genes
and then ﬁts a Cox model using the average expression level of each cluster
as covariate (Hastie et al., 2001). However, this method can be sensitive to the
choice of clustering algorithm. Other dimension reduction approaches are based
on partial least squares (Nguyen and Rocke, 2002) or principal component analy-
sis (Li and Gui, 2004). These methods are based on linear combinations of genes
rather than the original variables to achieve dimension reduction. Most of these
methods are done as via a two-step approach where the gene selection is done
ﬁrst and then the selected genes are used for survival prediction, thus ignoring
the uncertainty in the gene selection step. In this section we summarize some
Bayesian approaches that circumvent this problem.
9.3.1
Gene Selection for Time-to-Event Outcomes
Lee and Mallick (2004) perform gene selection by extending the work of George
and McCulloch (1993) to a non-Gaussian mixture prior framework. A ‘small n,
large p’ situation is considered for selection and survival analysis with Weibull
regression or the PH model. They use of a random residual component in the
model consistent with the belief that there may be unexplained sources of vari-
ation in the data, perhaps due to explanatory variables that were not recorded in
the original study. Sha et al. (2006) consider the accelerated failure time (AFT)
model as an alternative to the Cox model for modeling the survival data.
9.3.2
Weibull Regression Model
First we consider the parametric model where the independently distributed sur-
vival times for n individuals, t1, t2, . . . , tn follow a Weibull distribution with
parameters α and γ ∗
i . The data for the jth individual in the experiment, in addi-
tion to his survival time tj, consists of an observation vector (yj, Xj1, . . . , Xjp)
where yj indicates the binary or multicategory phenotype covariate and the p
Xj1, . . . , Xjp are gene expression vectors. A reparameterized model is more
convenient (λ = log γ ∗), whereby
P (t|α, λ) = αtα−1 exp(λ −exp(λ)tα),
t, α > 0.
The hazard and survival function follow as h(t|α, λ) = αtα−1 exp λ and
S(t|α, λ) = exp(−tα exp λ). For the n observations at times t = (t1, . . . , tn),
let the censoring variables be δ = (δ1, . . . , δn)′, where δi = 1 indicates right
censoring and δi = 0. The joint likelihood function for all the n observations is
L(α, λ|θ) =
n

i=1
P (yi|α, λi)δiS(yi|α, λi)(1−δi)
(9.3)
= αd exp
 n

i=1
δiλi + δi(αi) log yi −yα
i exp λi

,

ADVANCED TOPICS
157
where λ = (λ1, . . . , λn) and θ = (n, t, δ). In a hierarchical step, a conjugate prior
can be induced as λi = X′
iβ + ϵi, where Xi are gene expression covariates, β =
(β1, . . . , βn) is a vector of regression parameters for the p genes, and ϵi ∼
N(0, σ 2).
A Gaussian mixture prior for β enables us to perform a variable selection
procedure. Deﬁne γ = (γ 1, . . . , γp), such that γi = 1 or 0 indicates whether the
gene is selected or not (βi = 1 or 0). Given γ , let βγ = {βi ∈β : βi ̸= 0} and
Xγ be the columns of X corresponding to βγ . Then the βγ have distribution
N(0, c(X′
γ Xγ )−1), where c is a positive scalar. The indicators γi are assumed to
be a priori independent with P (γi = 1) = πi, which are chosen to be small to
restrict the number of genes.
The hierarchical model for variable selection can be summarized as
(Ti|α, λi) ∼Weibull(α, λi).
α ∼Gamma(α0, κ0),
(λi|β, σ) ∼N(X′
iβ, σ 2), (βi|γi) ∼N(0, γicσ 2),
σ 2 ∼IG(a0, b0),
γi ∼Bernoulli(πi).
As the posterior distributions of the parameters are not of explicit form, we need
to use MCMC-based approaches, speciﬁcally Gibbs sampling alongside Metropo-
lis algorithms to generate posterior samples. The unknowns are (λ, α, β, γ, σ 2)
and separate Metropolis–Hastings steps are used to sample λi, the indicators
γi (one at a time), ϕ = log α and ﬁnally (β, σ 2) is drawn as (βγ , σ 2|γ, λ) ∼
NIG(Vγ X′
γ λ, Vγ ; a1, b1) where a1, b1 are deﬁned as in O’Hagan and Forster
(2004). For the details of the conditional distributions, the reader is referred to
Lee and Mallick (2004).
9.3.3
Proportional Hazards Model
The Cox PH model (Cox, 1972) represents the hazard function h(t) as
h(t|x) = h0(t) exp W,
where h0(t) is the baseline hazard function and W = x′β, where β is a vector
of regression coefﬁcients. The Weibull model in the previous section is a special
case of the PH model with h0(t) = αtα−1. Kalbﬂeisch (1978) suggests the non-
parametric Bayesian method for the PH model. Lee and Mallick (2004) apply a
Bayesian variable selection approach to this model. As before, they assume that
Wi = x′
iβ + ϵi, where ϵi ∼N(0, σ 2), in which xi forms the ith column of the
design matrix X.
Let Ti be an independent random variable with conditional survival function
P (Ti ≥ti|Wi, ) = exp {−(ti) exp Wi} ,
i = 1, . . . , n.
Kalbﬂeisch (1978) suggested a gamma process prior for h0(t). The assumption
is  ∼GP(a∗, a), where ∗is the mean process and a is a weight parameter
about the mean. If a ≈0, the likelihood is approximately proportional to the

158
BAYESIAN ANALYSIS OF GENE EXPRESSION DATA
partial likelihood, and if a ↑∞, the limit of the likelihood is the same as the
likelihood when the gamma process is replaced by ∗. The joint unconditional
marginal survival function is directly derivable as
P (T1 ≥t1, . . . , Tn ≥tn|W) = exp{−

i
(ti)eWi},
where W = (W1, . . . , Wn). The likelihood with right censoring is then
L(W|θ) = exp

−

i
aBi∗(ti)

n

i=1
{aλ∗(ti)Bi}δi,
where Ai = 
l∈R(ti) exp Wl, i = 1, . . . , n, R(ti) is the set of individuals at risk
at time ti −0, Bi = −log{1 −exp Wi/(a + Ai)}, and θ = n, t, δ) as before. The
hierarchical structure is as follows:
W|βγ ∼N(Xγ βγ , σ 2I),
βγ ∼N(0, cσ 2(X′
γ Xγ )−1),
σ 2 ∼IG(a0, b0),
γi ∼Bernoulli(πi)
The MCMC simulation that follows is similar to the Weibull regression model.
9.3.4
Accelerated Failure Time Model
Sha et al. (2006) consider the accelerated failure time model as an alternative
to the Cox model for modeling survival data. Rather than assuming a multi-
plicative effect on the hazard functions as in the Cox regression, AFT models
assume a multiplicative effect on the survival times. The general form of an AFT
model is
log(Ti) = α + Xiβ + ϵi,
i = 1, . . . , n,
where log(Ti) is the log-survival time, α is the intercept term, Xi is the
p-dimensional vector of gene expression proﬁles, and the ϵi are i.i.d. errors.
Kalbﬂeisch and Prentice (1980) give a comprehensive treatment of parametric
AFT models, and Wei (1992) reviews inference procedures for nonparametric
models.
Sha et al. (2006) work with parametric AFT models under normal and
t-distribution assumptions for ϵi. The censoring is handled via the data
augmentation procedure of Tanner and Wong (1987) to impute censored values.
Speciﬁcally, let w = (w1, . . . , wn)T , where wi = log(ti) is the augmented
data with the censored data imputed. Under Gaussian assumptions for the
errors, the Ti follow a lognormal distribution or, equivalently, wi follows a
normal distribution with mean α + Xiβ. Gene selection is then achieved via a
mixture prior on β as discussed in Chapter 3 and Section 9.3.3. Note that this
representation affords convenient marginalization of the regression parameters
which results in a much faster and more efﬁcient MCMC sampler. See Sha
et al. (2006) for technical details of the method.

Appendix A
Basics of Bayesian Modeling
A.1
Basics
This appendix introduces Bayesian methods and basic parametric models. As this
is an applied text, we avoid going into the theoretical properties of subjective
probability in great depth. There is a large literature discussing rational decision
making through a set of reasonable axioms which would naturally motivate us to
use the Bayesian paradigm For more details, see de Finetti (1937, 1963, 1964),
DeGroot (1970), Berger (1985), Savage (1972) and Bernardo and Smith (1994).
First we introduce de Finetti’s representation theorem (de Finneti, 1930) which
provides a link between subjective probability and parametric models. It also
introduces the predictive interpretation of parameters and provides a way to
make predictive inference.
Let us recall the gene expression example. We wish to determine how the gene
expression measurements on a patient were related to the presence or absence
of cancer. Say, xi represents the vector of gene expression measurements for the
ith patient and yi is the response deﬁned as 1 if the patient has cancer and 0
otherwise, for n patients.
For simplicity, let us start with the situation with no measurement on
the predictor for any individual (absence of xi) and consider the problem of
making a prediction for a new response yn+1. Thus the data set is given by
D = {y1, . . . , yn} and we wish to determine P (yn+1|D), the density of the new
response yn+1 conditioned on the observed responses D. For gene expression,
this involves predicting the probability that a new patient has cancer before
making gene expression measurements and given only the binary responses of
Bayesian Analysis of Gene Expression Data
B. Mallick, D. Gold, and V. Baladandayuthapani
2009 John Wiley & Sons, Ltd

160
BAYESIAN ANALYSIS OF GENE EXPRESSION DATA
other patients in the study. We use conditional probability to evaluate it as
P (yn+1|y1, . . . , yn) = P (y1, . . . , yn, yn+1)
P (y1, . . . , yn)
.
(A.1)
To evaluate this conditional probability we require the joint density functions in
the numerator and the denominator of equation (A.1). The general representation
theorem of de Finetti (1930) advocated us for these choices of joint density
function.
A.1.1
The General Representation Theorem
The concept of exchangeability is a key idea in the representation theorem which
assumes that the subscripts or labels for identifying the individual observable
are noninformative. A sequence of random variables y1, . . . , yn is said to be
exchangeable if the joint density,
P (y1, y2, . . . , yn) = P (yπ(1), yπ(2), . . . , yπ(n)),
for all permutations π deﬁned on the subscripts {1, . . . , n}. In our example this
means that whichever individual we label at the ﬁrst, the second, etc., does
not alter the joint density of all the responses P (y1, . . . , yn). This concept of
exchangeability can be straightforwardly extended to inﬁnite sequences of ran-
dom variables and is used in the following theorem.
General Representation Theorem: If y1, y2, . . . is an inﬁnitely exchange-
able sequence of real-valued random quantities, then there exista a probability
distribution function F over , the space of all distribution functions, such that
the joint distribution P (y1, . . . , yn) for y1, . . . , yn has the form
P (y1, . . . , yn) =
	

n

i=1
P (yi|F)dQ(F),
where P (yi|F) is the density of yi given that it is distributed according to F,
and Q(F) = limn→∞P (Fn), where P (Fn) is a distribution function evaluated at
the empirical distribution function deﬁned by Fn(y) = 1
n
n
i=1 I(yi ≤y).
This theorem tells us that we can decompose the joint density of the yi by
conditioning on a distribution F and then integrating over the range of F,  the
space of all distribution functions. Note that the distribution Q (known as the
prior distribution) encodes our beliefs about the empirical distribution Fn.
When F is completely unspeciﬁed, we need to integrate over the space of all
distribution functions to obtain the joint density of the responses. This is known as
nonparametric modeling (Walker et al., 1999; Dey et al., 1998). In this situation
we wish to draw some inference about the unknown F (see Chapter 7). In
contrast to nonparametric modeling, we assume that F follows some parametric
distribution with ﬁnite vector of parameters θ ∈. As F is completely speciﬁed

BASICS OF BAYESIAN MODELING
161
by θ, the general representation theorem can be used to express the joint density
of the responses as
P (y1, . . . , yn) =
	
θ
n

i=1
{P (yi|θ)}P (θ)dθ.
(A.2)
It is clear that to completely specify a Bayesian model we need to specify P (yi|θ)
(the data distribution which is the basis of the likelihood function) and P (θ) (the
prior distribution of the parameters). Throughout the book we mainly concen-
trate on parametric models following (A.2), though in Chapter 7 we discuss
nonparametric models precisely in the context of clustering distributions.
A.1.2
Bayes’ Theorem
In the parametric modeling framework using equation (A.2) we obtain the pre-
dictive density of a new response as
P (yn+1|D) =
	

P (yn+1|θ)P (θ|D)dθ,
(A.3)
where D = (y1, . . . , yn) and
P (θ|D) = P (D|θ)P (θ)
P (D)
(A.4)
=
(n
i=1 P (yi|θ)P (θ)


(n
i=1 P (yi|θ)P (θ).
(A.5)
P(yn+1|D) is known as the posterior predictive distribution, and to evaluate it we
need P (θ|D) which is known as the posterior distribution of θ. Equation (A.4)
is obtained by using Bayes’ theorem and can be simpliﬁed as
P (θ|D) ∝P (D|θ)P (θ)
(A.6)
or
Posterior ∝Likelihood × Prior.
Hence, the posterior distribution is found by combining the prior distribution
P (θ) with the probability of observing the data given the parameters or the
likelihood P (D|θ).

162
BAYESIAN ANALYSIS OF GENE EXPRESSION DATA
A.1.3
Models Based on Partial Exchangeability
In most complicated applications it is hard to justify the assumption of exchange-
ability. A simple example is when a set of cancer patients under study are being
treated with two different types of drugs. Responses from the patients under
the same drug can be assumed exchangeable, but not patients under both drugs.
Therefore, we assume that responses from the patients treated by same drug
are exchangeable, and they are independent otherwise. This is known as a par-
tially exchangeable structure. The situation can be generalized for more than one
group, assuming within-group observations are exchangeable and between-group
observations are independent.
Suppose there are m such groups and yi = yi1, . . . , yini, i = 1 . . . , m, are the
observations from the ith group. Under the assumption of partial exchangeability
the observations yi1, . . . yini are exchangeable so the representation theorem can
be used and the yi are independent of the yj for i ̸= j. Using the assumption of
within-group exchangeability and between-group independence, the representa-
tion theorem for the joint distribution of the observations will be
P (y1, . . . , ym) =
	
θ
m

i=1
ni

j=1
P (yij|θi)P (θ),
(A.7)
where θi = (θ1, . . . , θp). The proof for the representation theorem for partial
exchangeable sequences is given in Bernardo and Smith (1994).
A class of models based on the partial exchangeable structure is the clustering
or partition models. Brieﬂy, the method can be thought of as partitioning the
sample space,to give d subsets S1, . . . , Sd. We assume exchangeability only for
the data within the same partition Si, while data associated within different subsets
are taken to be independent. Typically there will be several partitions, g out of
all possible partitions G, whose relative plausibilities are described by a prior
probability mass function P (g). These types of models are described in Chapter 7
in the context of cluster modeling.
A.1.4
Modeling with Predictors
So far we have concentrated on a simple modeling situation without using any
predictors or covariates. However, we have additional data such as gene expres-
sion levels for each patient, and we need to model the dependence between the
responses y1, . . . , yn and their corresponding measurements on the predictor vari-
ables x1, . . . , xn. In our example, x are the gene expression measurements which
can be used for cancer prediction. We express the predictive distribution as
P (y1, . . . , yn|x1, . . . , xn) =
	

 n

i=1
P (yi|θ(xi))

P (θ(x))dθ(x).
(A.8)

BASICS OF BAYESIAN MODELING
163
A simple example is the linear regression problem where we assume
P (yi|θ(xi)) = N(θ(xi), σ 2) and θ(xi) = β0 + β1xi. This relationship can be
extended in general linear and nonlinear frameworks and is described in
Chapter 2.
In our example, Yi
is a binary variable and the binary classiﬁca-
tion model is suitable for this situation. We use the binary distribution
P (yi|θ(xi) = Binary(θ(xi)) , where θ(xi) is the canonical parameter and
θ(xi) = β0 + β1xi. In this binary model, θ(xi) is related to the mean parameter
µ(xi) by
θ(xi) = log[
µ(xi)
(1 −µ(xi))].
Linear and nonlinear classiﬁcation models are discussed in Chapter 5.
A.1.5
Prior Distributions
The basic philosophical difference between classical and Bayesian statistical
inference concerns the use of prior information or beliefs. Bayesian statisticians
contend that the investigator’s prior information and beliefs are themselves rele-
vant data and should be considered, along with more objective data, in making
inference. This difference regarding the use of prior information is related to
a disagreement concerning the deﬁnition and interpretation of the concept of
probability. Bayesian statisticians redeﬁne the classical concept of probability as
long-range relative frequency in terms of probability as a degree of conﬁrma-
tion or belief. For axiomatic formulation of subjective probabilities see DeGroot
(1970), de Finetti (1964), and Bernardo and Smith (1994).
Typically prior distributions are speciﬁed based on information gathered from
past studies or from the opinions of experts. There is a growing literature on
methodology for the elicitation of prior information (Craig et al., 1998; Kadane
and Wolfson, 1998; O’Hagan, 1998), which brings together ideas from statistics
and perceptual psychology.
In order to streamline the elicitation process and lessen the burden of com-
putation, we restrict our prior distribution to familiar families of distributions.
Further, to obtain closed-form answers for posterior distributions people assign
conjugate priors which are conjugate to the likelihood, that is, which lead to a
posterior distribution belonging to the same family as the prior.
The prior distribution P (θ) of θ should be constructed based on our prior
information about θ, but objective priors are also popular among practitioners.
The most familiar objective priors are noninformative or default prior distribu-
tions. One of the natural choices in this group is the uniform prior. For the
univariate parameter θ, for example, when the parameter space is bounded this
becomes the uniform distribution over the space. When we have an unbounded
parameter space, say (−∞, ∞), we deﬁne the uniform prior as constant over the

164
BAYESIAN ANALYSIS OF GENE EXPRESSION DATA
space as
P (θ) = c,
−∞< θ < ∞,
where c is any positive constant. This prior distribution is improper as
 ∞
−∞P (θ)dθ = ∞. Still, the posterior distribution could be proper as long as
the likelihood has a ﬁnite integral with respect to θ as
P (θ|y) =
P (y|θ) · c
 ∞
−∞P (y|θ) · cdθ ,
and the integral of P (θ|y) is 1 under the integrability condition over the likeli-
hood.
Another limitation of the uniform prior is that it is not invariant under repa-
rameterization. The famous Jeffreys prior (Jeffreys, 1946) handles this problem.
For the univariate case with parameter θ the Jeffreys prior is
P (θ) = {I(θ}1/2
where I(θ) is the expected Fisher information in the model, namely
I(θ) = −Ey|θ

 δ2
δθ2 log P (y|θ)

.
In the multiparameter case Jeffreys prior is deﬁned as
P (θ) = {|I(θ1/2|},
where | · | denotes the determinant and I(θ) is the expected Fisher information
matrix, with (i, j)th element
I(θ) = −Ey|θ

δ2
δθiδθj
log P (y|θ)

.
The other convenient choice of priors is conjugate priors (conjugate to the like-
lihood) where the posterior distribution could be obtained in closed form. In this
case the posterior distribution P (θ|y) belongs to the same family as the priors
P (θ). Morris (1983) showed that exponential families, a large family containing
most of our known likelihood functions, do actually have conjugate priors, so
this is a useful approach in practice.
There are several other methods for constructing priors, both informative and
noninformative; see Berger (1985) for an overview.
There is controversy regarding the use of prior distribution P (θ) based on sub-
jective probability in Bayesian analysis. Bayesian inference corresponds essen-
tially to the type of inductive reasoning on which the scientiﬁc method is based.
The development of any area of science consists of successive revisions of
hypotheses, which are often at ﬁrst stated vaguely. It is characteristic of such

BASICS OF BAYESIAN MODELING
165
a development that in the beginning many diverse hypotheses are proposed but,
as evidence accumulates, these hypotheses are revised until relatively few remain
and these tend to agree, at least in some major respects. The revision of vague
diverse theories as evidence accumulates seems to be the only reasonable way
for science to proceed; in a sense, criticism of the Bayesian use of subjective
probabilities is no more justiﬁed than criticism of initial vague personal theories
in science.
A.1.6
Decision Theory and Posterior and Predictive Inferences
Statistical decision theory is a general mathematical framework concerned with
optimal actions that one may take when faced with uncertainty. All sorts of
statistical inference procedures are encompassed by the theory. Central to the
theory is the notion of risk, and this forms the basis of comparison of decision
rules. Before discussing this framework we need some basic deﬁnitions.
(a) States of nature: Unknown states of the world ω ∈. Examples are the
parameter θ or a future value of the data ynew.
(b) Actions: a in the action space A. Actions correspond to particular decisions
that the investigator may take. The action space may be somewhat general.
In an estimation problem, the action is to report an estimate of θ, so A = .
In conﬁdence interval construction, the action space is the set of intervals
within the parameter space, and in hypothesis testing the action space is
the set of hypotheses under consideration. In a prediction problem, it is
the predictive space.
(c) Utility function: U(ω, a) is a function assigning numerical reward to
the investigator if he/she takes action a when ω is the true state of
nature. Technically it is a mapping from the product space  × A into
the real line R, and we usually restrict attention to nonnegative util-
ities. Instead of working with U(a, ω), people work with a so-called
loss function L(a, ω) = −U(a, ω). Examples of popular loss functions
are squared-error loss (ω −a)2, absolute error loss |ω −a|, and zero–one
loss where L(ω, a) = 1 if ω = a and L(ω, a) = 0 otherwise.
(d) Decision rules: δ(y), which is a mapping from the sample space Y into the
action space A. In other words, it is a way of associating an action with
any data; i.e., it is a statistic. Depending on the inference problem, δ(y)
may be an estimator, a conﬁdence interval, a test statistic or a predictive
value.
(e) Belief distribution: P (ω), a speciﬁcation, in the form of a probability distri-
bution, of current beliefs about the possible states of the world. Examples
are P (θ) = initial beliefs about a parameter vector θ, P (θ|y) = beliefs
about θ, given data y, and P (ynew|y) = beliefs about future data ynew,
given data y.

166
BAYESIAN ANALYSIS OF GENE EXPRESSION DATA
(f) Risk: The risk R(ω, δ) is the expected loss incurred by the investigator
who uses decision rule δ when ω is the state of nature. Suppose we are
concerned with the inference problem where ω = θ. Now we get different
types of risks depending on how we are computing the expectation. If
we compute the expectation relative to the sampling model, this is known
as frequentist risk,

Y L(θ, δ(y))P (y|θ)dy. The expectation can be taken
with respect to the prior distribution of θ as

θ L(θ, δ(y))P (θ)dθ, which
is known as prior risk. In our context, the inference occurs after the data
y have been observed so the required distribution of θ is the posterior
distribution P (θ|y). If we take the expectation with respect to this posterior
distribution we get the posterior risk

θ L(θ, δ(y))P (θ|y)dθ. Similarly, for
prediction problems the expectation is taken over the marginal distribution
of the data.
(g) Bayes risk: The Bayes risk is the marginal expected loss, averaging over
both data and parameter space, which is

θ

y|θ L(θ, δ(y))P (y|θ)dydθ. Now
we can choose a rule minimizing the Bayes risk which is known as the
Bayes rule. The name is confusing as a proper Bayesian would not aver-
age over the data to choose the decision rule, but instead minimize the
posterior risk. However, it turns out that these two operations are virtually
equivalent. Under very general conditions, minimizing the Bayes risk is
equivalent to minimizing the posterior risk (Berger, 1985).
(h) Point estimation: In the problem of point estimation the decision consists of
asserting a single value as an estimate of θ. For parametric point estimation
A = , the parameter space, and L(θ, a) are speciﬁed.
Under squared-error loss the Bayes rule can be obtained by minimizing
the posterior risk,
R(θ, a) =
	
(θ −a)2P (θ|y)dθ.
To minimize the loss function, we need to take the derivative with respect
to a:
δ
δa [R(θ, a)] =
	
2(θ −a)(−1)P (θ|y)dθ.
Setting this expression equal to zero and solving for a, we obtain
a =
	
θP (θ|y)dθ = E(θ|y),
the posterior mean as solution. To check the other condition of positivity
of the second derivative is straightforward. Similarly, it can be shown that
under absolute error loss the Bayes estimate is the posterior median, while
under zero–one loss the Bayes estimate is the posterior mode.

BASICS OF BAYESIAN MODELING
167
The posterior mean of h(θ) (where h is any known function of θ) can be
obtained as
E{h(θ|y)} =
	
h(θ)P (θ|y)dθ
=

h(θ)P (y|θ)P (θ)

P (θ|y)P (θ)dθ .
(A.9)
Similarly, we can perform predictive inference constructing proper loss
functions and the point estimates will be the predictive mean, median or
mode depending on the choice of the loss function.
(i) Credible region: Rather than point estimation (a single value of θ) we
may be interested about a range of values of θ, the Bayesian analog of the
frequentist conﬁdence interval. Therefore, the decision set A is the set of
subsets of . The loss function is L(θ, a) = I(θ ̸∈a) + k × volume(a),
where I is the indicator function. In this situation the Bayes rule will be
the region or regions for θ having highest posterior density and k will
control the tradeoff between the volume of a and the posterior probability
of coverage.
Now to construct a 100(1 −α)% credible set for θ we choose a set A
such that P (θ ∈A|y) = 1 −α (which says that the probability that θ lies
in A given the observed data y is at least α). The set is called a highest
posterior density credible region if P (θ|y) ≥P (Φ|y) for all θ ∈A and Φ ̸∈
A which is our Bayes rule under the previously mentioned loss function.
Such a credible set is very appealing since it covers the most likely θ
values. Similarly, we can construct predictive intervals by replacing the
posterior distribution by the corresponding predictive distribution.
(j) Hypothesis testing: Now consider a null hypothesis H0 : θ ∈θ0 versus
an alternative hypothesis H1 : θ ∈θc
0 = θ1. We can describe this as a deci-
sion problem with  = (ω0, ω1) where ω0 = [H0 : θ ∈θ0] and ω1 = [H1 :
θ ∈θ1] with θ ∈ = θ0 ∪θc
0. We assign the believe distribution P (ω)
as p0 = P (H0) and p1 = P (H1), the prior probabilities of the hypothe-
ses. The action space A = {a0, a1}, where ai is the action correspond-
ing to rejection of the hypothesis Hi for i = 0, 1 and the loss function
L(ai, ωj) = L(ai, Hj) = lij, i, j ∈{0, 1}, with lij reﬂecting the relative
seriousness of the four possible consequences, and typically, l00 = l11 = 0.
Suppose our loss function is lij = 1 if i = j and lij = 0 if i ̸= j for i, j ∈
{0, 1}. Now the expected loss of the decision ai (choosing the ith hypothesis Hi
for i = 0, 1), given y, is thus
	
L(ai, ω)P (ω|y) =
	
L(ai, H)P (H|y) = P (Hi|y).
The optimal decision is therefore to choose the hypothesis which has the highest
posterior probability. Now to compare two hypotheses, a simple measure is the

168
BAYESIAN ANALYSIS OF GENE EXPRESSION DATA
posterior odds, the ratio of the posterior probabilities corresponding to the two
hypotheses, which is
P (H0|y)
P (H1|y) = P (y|H0)
P (y|H1) × P (H0)
P (H1)
where P (y|Hi) =

pi(y|θi)pi(θi)dθi is the integrated likelihood (or the marginal
likelihood).
Another measure is the Bayes factor,
B = Posterior odds ratio
Prior odds ratio
= P (H0|y)/P (H1|y)
P (H0)/P (H1)
which measures whether the data y have increased or decreased the odds of
H0 relative to H1. Thus B > 1 signiﬁes that H0 is more relatively plausible
considering y. Good (1988) has suggested using the logarithm of this Bayes
factor as the weights of evidence in favor of the hypothesis. We discuss the
Bayes factor in detail in Section A.2 in the model choice context.
A.1.7
Predictive Distributions
Often the goal of the statistical analysis is to predict future observations, which is
straightforward in this Bayesian setup. Suppose that ynew are the future observa-
tions which are independent of y given the underlying θ. Then the prior predictive
is the marginal distribution (with respect to the prior distribution) f (ynew) =

f (ynew|θ)P (θ)dθ, and the posterior predictive is the marginal distribution with
respect to the posterior distribution f (ynew|θ) =

f (ynew|θ)P (θ|y)dθ.
A.1.8
Examples
Example A.1
Suppose that Y1, . . . , Yn are binary outcomes coded as Yi = 1 for diseased tissues
and Yi = 0 for normal tissues as described in Section 5.4.5. Now Y = n
i=1 Yi
conditional on θ is assumed to have a binomial likelihood
P (y|θ) =
n
y

θy(1 −θ)n−y,
0 < θ < 1.
The corresponding conjugate prior for this binomial likelihood is a beta dis-
tribution with parameters (α, β):
P (θ) = θα(1 −θ)β
B(α, β)
,
0 < α, β < ∞.

BASICS OF BAYESIAN MODELING
169
We can also ﬁnd out the noninformative prior using the binomial likelihood.
The posterior density will be proportional to the product of the prior and the
likelihood,
P (θ|y) ∝θα+y−1(1 −θ)β+n−y−1.
This is a beta density (so the posterior has the same distribution as that of
the prior) with updated parameters α∗= α + y and β∗= β + n −y. Using the
standard properties of the beta distribution,
P (θ|y) = θα+y−1(1 −θ)β+n−y−1
Beta(α + y, β + n −y).
Thew prior expectation of θ is
E(θ) =
α
α + β ,
while the posterior expectation is
E(θ) =
α∗
α∗+ β∗=
α + y
α + β + n.
We can write
E(θ|y) =
ny/n + (α + β)(
α
α+β )
n + (α + β)
.
Now the sample estimate of θ is the sample proportion ˆp = y/n, and λ = α + β
is known as the prior sample size. The posterior expectation is
E(θ|Y) = nˆp + λE(θ)
n + λ
,
which is a weighted average of sample proportion ˆp and the prior mean of θ.
The weights are the actual sample size n and the prior sample size λ = α + β.
Now, after observing y, we wish to predict a new observation ynew, which
is independent of y, given θ, and possesses a binomial distribution, with
probability θ and sample size m. Then the prior predictive probability mass
function of ynew is
P (ynew) =
	 1
0
P (ynew|θ)P (θ)dθ
=
 m
ynew

B(α, β)
	 1
0
θα−1(1 −θ)m−ynew+β−1dθ
=
 m
ynew

B(ynew + α, m −ynew + β)
B(α + β)
,
ynew = 0, 1, . . . , m.

170
BAYESIAN ANALYSIS OF GENE EXPRESSION DATA
Similarly, the posterior predictive distribution will be
P (ynew|y) =
	 1
0
P (ynew|θ)P (θ|y)dθ
=
 m
ynew

B(α∗, β∗)
	 1
0
θα∗−1(1 −θ)m−ynew+β∗−1dθ
=
 m
ynew

B(ynew + α∗, m −ynew + β∗)
B(α∗+ β∗)
,
ynew = 0, 1, . . . , m.
Finding the posterior predictive mean is simple:
E(ynew|y) = Eθ|y[E(ynew|θ)]
= Eθ|y(mθ)
= m
α∗
α∗+ β∗.
The Jeffreys prior form binomial model is again beta with parameters (0.5, 0.5),
which is conjugate, too.
Example A.2
Y1, Y2, . . . , Yn are an independent sample of size n from the normal distribution
with mean µ and variance σ 2. Now in this situation the sample mean ¯y is
sufﬁcient as P (θ|y) = P (θ| ¯y). We know that ¯y ∼N(µ, σ 2/n) so the likelihood
function is
P (¯y|θ) =
√n
σ
√
2π
exp

−n(¯y −µ)2
2σ 2

,
y ∈, R
where θ = (µ, σ), µ ∈, σ 2 ∈+. Looking at the likelihood as a function of
µ alone it is clear that the conjugate is again a normal distribution. We can
assume that P (µ) = N(µ|µ0, τ 2), where µ0 and τ 2 are known hyperparameters.
Now looking at the likelihood function as a function of σ 2 alone, the conjugate
prior will be proportional to (σ 2)ae−bσ 2, which is similar to the inverse gamma
distribution. If 1/σ 2 follows a gamma distribution with parameters (a, b) then
σ 2 follows an inverse gamma IG(a, b) distribution, with density function
f (σ 2|a, b) =
e−1/bσ 2

(a)baσ 2a+1 ,
a > 0, b > 0.
Sometimes we reparameterize the model in terms of 1/σ 2 which is known as the
precision and follows a gamma distribution according to this choice of prior for
σ 2. Finally, we assume that µ and σ 2 are a priori independent so that P (θ) =
P (µ)P (σ 2).

BASICS OF BAYESIAN MODELING
171
The conditional posterior distribution of mean µ turns out to be
P (µ|σ 2, ¯y) ∼N
(σ 2/n)µ0 + τ 2 ¯y
σ 2/n + τ 2
,
(σ 2/n)τ 2
(σ 2/n) + τ 2

= N
σ 2µ0 + nτ 2 ¯y
σ 2 + nτ 2
,
σ 2τ 2
σ 2 + nτ 2

,
(A.10)
so we see that this posterior distribution has mean which is a weighted average of
the prior mean and the sample value, with weights that are inversely proportional
to the corresponding variances (proportional to the corresponding precisions).
The conditional posterior distribution of σ 2 turns out to be
P (σ 2|µ, ¯y) = IG
)
n
2 + a,

n
2(¯y −µ)2 + 1
b
−1*
.
(A.11)
The marginal distributions of µ and σ 2 are explicitly available in this conjugate
framework.
Furthermore, we can use noninformative priors in this problem; a popular
one assumes that µ and log σ are locally uniform or, equivalently,
P (µ, σ) ∝σ −1.
Using the prior and normal likelihood, the joint posterior distribution is
P (µ, σ|y) ∝σ n exp
)
−n
i=1 (yi −µ)2
2σ 2
*
.
Now

i
(yi −µ)2 =

i
(yi −¯y)2 + n(¯y −µ)2
= (n −1)s2 + n(¯y −µ)2
where the sample variance is
s2 =

i
(yi −¯y)2
n −1
.
Therefore, clearly the conditional distribution P (µ|σ, y) ∼N(¯y, σ 2/n).
The marginal distribution of σ is
P (σ|y) ∝σ n exp

−(n −1)s2
2σ 2

,
so σ is distributed a posteriori as a chi distribution,
√
n −1sχ−1
n .

172
BAYESIAN ANALYSIS OF GENE EXPRESSION DATA
The marginal distribution of µ is
P (µ|y) ∝
4
1 + n(µ −¯y)2
(n −1)s2
5−n/2
,
(A.12)
which will be recognized as
µ −¯y
s/√n ∼tn−1.
Example A.3 Mixture prior
Suppose that the likelihood is P (y|θ) for a univariate parameter θ, and assign
two-component mixture prior P (θ) = πP1(θ) + (1 −π)P (2(θ), 0 < π < 1,
which is a mixture distribution of pθ and p2(θ). Then
P (θ|y) =
P (y|θ)p1(θ)π + P (y|θ)p2(θ)(1 −π)

[P (y|θ)p1(θ)π + P (y|θ)p2(θ)(1 −π)]
=
P (y|θ)p1(θ)
m1(y)
m1(y)α + P (y|θ)p2(θ)
m2(y)
m2(y)(1 −α)
myα + m2(y)(1 −α)
= P (y|θ)p1(θ)
m1(y)
w1 + P (y|θ)p2(θ)
m2(y)
(1 −w1)
= p1(θ|y)w1 + p2(θ|y)(1 −w1),
(A.13)
where mi(y) is the marginal distribution with prior pi, i = 1, 2, and the posterior
weight is
w1 =
m1(y)π
m1(y)α + m2(y)(1 −α),
which is updated from the prior weight π. Therefore, the posterior is also a mix-
ture distribution. If we use p1(θ) and p2(θ) as conjugate priors for the likelihood,
we obtain the posterior as a mixture of conjugate posteriors in explicit form.
A.2
Bayesian Model Choice
We have deﬁned a Bayes model and representation theorem for an exchangeable
sequence of observations. The predictive model typically has a mixture represen-
tation which may not be unique. Now the question is, if there IS more than one
model, how to choose the best one. Usually Bayesian inference is done using
the posterior distributions, whereas model selection is done using the predictive
distribution (Box, 1980).

BASICS OF BAYESIAN MODELING
173
The model choice could be seen as a decision problem whose solution
involves model choice or comparison among the alternatives in M = {Mi, i ∈I}.
If we assume that the true model is a member of M and our main intention is to
choose the right model (not inference or prediction), then we can think that the
unknown state of nature is the true model Mi ∈M so  = M. Now the action
space in this problem is A = M with a = Mi, Mi ∈M. The belief distribution
P (ω) will be assigned over the model space by P (Mi), Mi ∈M. The next step
will be to construct a loss function L(Mi, ω) (or the utility function U(Mi, ω))
and obtain the true model (best model) M∗by minimizing the loss function.
Model choice could be seen as a special case of hypothesis testing, and we
can obtain similar results just by switching the notation from hypotheses H0 and
H1 to models Mi, i = 1, 2. As there is no limit on the number of hypotheses that
may be simultaneously considered, extension of these concepts to more than two
models are straightforward. For simplicity, suppose that we have two candidate
models M1 and M2 with respective parameter vectors θ1 and θ2. Suppose that the
likelihood and the prior corresponding to ith model (i = 1, 2) are pi(y|θi, Mi)
(we express the dependence of the likelihood on the model by conditioning on
the model Mi) and pi(θi|Mi). Then the marginal distribution of y is found by
integrating out the parameters,
P (y|Mi) =
	
P (y|θi, Mi)pi(θi|Mi)dθi,
i = 1, 2.
(A.14)
Furthermore, P (M2|y) = 1 −P (M1|y).
Now, similarly to Section A.1.6, if we construct a zero–one loss function we
obtain a similar type of results. The loss function L(Mi, ω) is 0 if ω = Mi ,and
1 otherwise. The optimal decision is again to choose the model which has the
highest posterior probability. Therefore, we can use the Bayes factor to compare
these models:
B = P (M1|y)/P(M2|y)
P (M1/P (M2)
= P (y|M1)
P (y|M2),
(A.15)
the ratio of the observed marginals for the two models, which provides the relative
weight of evidence for model M1 compared to model M2.
We expect model choice criteria to follow the principle of Ockham’s razor,
that is, the chosen model should be no more complicated than necessary. The
Bayes factor has an automatic razor built in so that it picks up simpler models.
When we compare models of different dimensions we usually use criteria based
on penalizing the dimension of the model. Use of a Bayes factor is convenient
because it has a natural razor within it to penalize overparameterized models.
One criterion based on a penalty for model dimension is Akaike’s (1974)
information criterion (AIC). Here we choose the model that minimizes
AIC = −2(log maximized likelihood) + 2(number of parameters).

174
BAYESIAN ANALYSIS OF GENE EXPRESSION DATA
The argument for the AIC is based on asymptotic considerations, and it is known
that AIC tends to overestimate the number of parameters needed, even asymp-
totically. This is mainly due to the small penalty term. Therefore, this criterion
was modiﬁed by Schwarz (1978) to give the Bayesian information criterion,
BIC = −2(log maximized likelihood)
+ log(sample size)(number of parameters).
(A.16)
and for large sample sizes it approximates −2 times the log of the Bayes factor.
A serious problem with the Bayes factor is that, if we use vague priors so
that P (θ) is improper, then the marginal distribution f (Y) is improper (though
not necessarily the posterior P (θ|y) which makes it impossible to calibrate the
Bayes factor, and it loses its interpretation in this situation).
Another problem with Bayes factor occurs in the case of nested models, when
it may exhibit ‘Lindley’s paradox’ (Lindley, 1957). Suppose we are comparing
the reduced model with a full model and data points support rejection of the
reduced model. Now if the prior is proper but sufﬁciently diffused, it can place
a little mass on alternatives so that the denominator of the Bayes factor is much
smaller than the numerator, resulting in support for the reduced model in spite
of the data. Therefore, the Bayes factor tends to place too much weight on
parsimonious selection.
Example A.4
Suppose that for y = (y1, y2, . . . , yn) we have two alternative models M1 and
M2 with P (Mi) > 0, i = 1, 2 for a normal model with unknown mean µ and
known variance σ 2. Now the predictive models are
M1 :p1(y) =
n

i=1
N(yi|µ0, σ 2),
µ0 known,
M2 :p2(y) =
	
n

i=1
N(yi|µ, σ 2)N(µ|µ1, λ2)dµ,
µ1, λ2 known,
where N(yi|µ, σ 2) is the normal model with mean µ and variance σ 2. M1 is
the simple normal distribution model with mean µ0 and variance σ 2. M2 is the
complicated model where the mean µ is unknown. We assign a conjugate prior
for this unknown µ which is again a normal distribution with known mean µ1
and variance λ2. If the sample mean ¯y = (n
i=1 yi)/n then the Bayes factor B is
B =
N( ¯y|µ0, σ 2/n)

N(¯y|µ, σ 2/n)N(µ|µ1, λ2)dµ
=
1/λ2 + n/σ 2
1/λ2
1/2 exp{ 1
2(λ2 + σ 2/n)
−1( ¯y −µ1)2}
exp{ 1
2n(¯y −µ1)
2/σ 2}
.

BASICS OF BAYESIAN MODELING
175
Now for ﬁxed ¯y, the Bayes factor B →∞as λ2 →∞so that evidence in
favour of M1 (the simple model) is overwhelming as the prior variance becomes
very large (so prior precision is vanishingly small, creating a larger and more
complex model).
A.3
Hierarchical Modeling
Bayesian hierarchical models allow strength to be borrowed among units across
a whole data set in order to improve inference. For example, gene expression
experiments used by biologists to study fundamental processes of activation /sup-
pression frequently involve genetically modiﬁed animals or speciﬁc cell lines,
and such experiments are typically carried out only with a small number of
biological samples. It is clear that this amount of replication makes standard esti-
mates of gene variability unstable. By assuming exchangeability across the genes,
inference is strengthened by borrowing information from comparable units.
When the data are exchangeable the representation theorem is given in
equation (A.2). However, we said nothing about the joint prior speciﬁcation
P (θ) = P (θ1, . . . , θm). Subjective beliefs for speciﬁc applications are used to
specify the joint prior distribution, but some assumption of additional structures
may be useful to model the prior. We may assume that the parameters θ1, . . . , θm
are exchangeable themselves and modify the representation theorem as
P (y1, . . . , ym) =
	
θ
m

i=1
ni

j=1
P (yij|θi)P (θi|φ)P (φ).
(A.17)
The complete model structure has the hierarchical form
P (y1, . . . , ym|θ1, . . . , θm) = πm
i=1πni
j=1P (yij|θi)
P (θ1, . . . , θm|φ) = πm
i=1P (θi|φ)
P (φ).
Therefore, the stage-wise assumptions of exchangeability create random samples
of observations and parameters from different distributions in different hierarchies
(stages). At the ﬁrst stage we get i.i.d. yij conditioned on θi and at the second
stage i.i.d. θi conditioned on the hyperparameter φ. At the ﬁnal stage the prior
distribution of φ speciﬁes the belief about it.
This enterprise of specifying a model over several levels is called hierarchical
modeling. The proper number of levels varies with the problem, but since we are
continually adding randomness as we move down the hierarchy, subtle changes
to levels near the top are not likely to have much impact on the bottom or data
level.

176
BAYESIAN ANALYSIS OF GENE EXPRESSION DATA
Shrinkage arises naturally in Bayesian analysis especially in these hierarchical
models, as the effect of the prior distribution in general is to pull the likelihood
towards the prior and the Bayes estimate will be a combination of the sample
estimate and the prior (pulled estimate) and so have more stable and smaller vari-
ances than the other competitors. Whenever we have an assumption of closeness
or equality of parameter values a priori it will cause shrinkage automatically.
Consider a simple example with observations yij, i = 1, . . . , N, j =
1, . . . , M, with i indexing the units and j indexing the subjects within the
units. We wish to estimate gene-speciﬁc means µi. Let us assume that the
observations from a unit are exchangeable and the observations between units
are independent. Due to the assumption of exchangeability, the observations
within a unit will be conditionally independent and we further assume that
yij|µj ∼N(µi, σ 2),
where σ 2 is known.
The maximum likelihood estimate (MLE) of µi is the unit-speciﬁc sample
mean
¯yi. =
M
j=1 Yij
M
.
Among the advantages of this estimator are that it is simple to compute and it
is an unbiased and consistent estimator of µi. However, when ni is small, the
estimates are highly variable. In such a situation, one needs to borrow information
from other units if they have a latent characteristic common to them, to obtain a
more accurate estimate. One such estimator is the pooled mean,
¯y =
N
i=1
M
j=1 yij
NM
,
where one pretends that all units are identical. The pooled mean is a biased esti-
mate for any particular µi, but is preferred over the unbiased estimate because of
its smaller variance. In either case we determine how much pooling is permitted:
no pooling or complete pooling. However, we can let the data determine how
much pooling is required by using a Bayesian hierarchical model. We can then
obtain an estimator of µi that has smaller total mean squared error across the N
estimates of µi. This is an example of hierarchical models where the units and
the subjects within the units form the hierarchy.
We further assume that the µj are exchangeable and develop a hierarchical
model
yij|µi ∼N(µi, σ 2)
µi ∼N(µ, τ 2).

BASICS OF BAYESIAN MODELING
177
Due to the conjugate structure of the model, the posterior distribution of µi
conditional on the data is given by
µi|y ∼N( ˆµi, N/σ 2 + 1/τ 2),
where
ˆµi = (1 −w) ¯yi + wµ.
The term w is called the shrinkage factor. Thus the posterior mean (shrinkage
estimator) of µi is the weighted average of the population pooled mean (µ)
and sample unit mean ( ¯yi). We stated earlier that exchangeability does not mean
independence. Below, we provide a brief but important proof concerned with
independence in hierarchical models.
We wish to obtain the marginal distribution of yij,
f (Yij|µ) ∝
	
f (Yij|µi)f (µi|µ)dµi
=
	
exp

−(yij −µj)2
σ 2

exp

−(µj −µ)2
τ 2

dµj.
It can be easily seen that the marginal density is
yij|µ ∼N(µ, σ 2 + τ 2).
To show that conditional independence does not imply marginal independence,
we derive the marginal correlation between Yij and Ykj using the law of iterated
expectation as follows. We have
E(yij) = E(E(yij|µi)) = µ,
and the marginal variance is given by
var(yij) = var(E(yij|µi)) + E(var(yij|µi)) = σ 2 + τ 2.
The covariance between yij and Yik is
cov(yij, yik) = E(yijyik) −E(yij)E(ykj)
= E(E(yijyik|µi)) −µ2
= E(µ2
i ) −µ2
= τ 2.
Then the marginal correlation can be obtained as
corr(Yij, Ykj) =
τ 2
σ 2 + τ 2 .

178
BAYESIAN ANALYSIS OF GENE EXPRESSION DATA
Thus, the observations within a unit are exchangeable but not independent. This
is crucial in analysis of gene expression data where the units could be the state of
the disease, for example control and cancer (or cancer of different kinds), and the
observations with the units are from different genes or signatures. Assumptions of
independence may not be valid here as the dependence among genes is a known
biological fact. Assumption of exchangeability and development of a hierarchi-
cal model can facilitate dependence through simple conditional independence
structures.
The usage of the pooled mean allows us to borrow information across all the
units. It can be easily shown that, under certain conditions, the mean squared
error of ˆµj is lower than that of ¯Yj. The shrinkage estimator is biased, but
makes better use of the total information available to yield superior performance
in terms of total mean squared error. If σ 2 is small relative to the total variation,
we do not shrink our estimate of µj towards the pooled mean µ But if σ 2 is
large relative to τ 2, the resulting estimate will shrink towards µ. We demonstrate
this idea using two examples.
Example A.5
Here we consider the example given in Goldstein et al. (1993), where the aim
is to differentiate between good and bad schools. A sample of 1978 students
(subjects) is chosen from 38 schools (units) and their test scores are measured
along with several covariates such as gender. We wish to calculate each school’s
observed average score, and the overall average for all schools. Since some
schools have as few as three students, it might be a good idea to model the
students as exchangeable given the school information. This allows us to borrow
strength across schools to obtain improved estimates of school averages. By way
of illustration, we consider a sample without any covariates. The model is
yij|µi ∼N(µi, σ 2
y ),
µi ∼N(µ, σ 2
µ),
where yij is the test score of the jth student from the ith school. We elicit
conjugate priors for the unknown parameters (random variables):
σ 2
y ∼IG(uy, vy),
µ ∼N(µ0, σ 2
µ),
σ 2
µ ∼IG(uµ, vµ).
Here, IG(u, v) is the notation for inverse gamma distribution with mean
v/(u −1) and variance v2/(u −1)2(u −2), u > 2, and N(µ, σ 2) denotes a
normal distribution with mean µ and variance σ 2. Figure A.1 shows the 95%
conﬁdence and 95% credible intervals of scores for individual schools using
the direct sample estimator and the Bayes shrinkage estimator. Note that in

BASICS OF BAYESIAN MODELING
179
0
5
10
15
20
25
30
35
40
−3
−2
−1
0
1
2
3
95% CIs for estimated school effects 
school ids
OLE
estimates
Shrinkage
estimates
Grand Mean
Shrinkage
Figure A.1
School effects using maximum likelihood and Bayes estimates.
most cases the shrinkage estimator yields a narrower interval, indicating better
precision, as a consequence of borrowing information across schools.
Example A.6
Gelfand et al. (1990) considered modeling rat growth. Weekly measurements of
rat weights were taken for ﬁve weeks. The response variable, yij, is the weight of
rat i at week j, i = 1, . . . , 30, j = 1, . . . , 5, and the explanatory variable is Xj,
the days on which the weights were taken. Each rat has its own expected growth
line, but we wish to borrow information across rats to tell us about individual
rat growth. The individual expected growth line of rat i is given by
E(yij) = αi + βiXj.
The average population growth line is given by
E[E(Yij)] = αc + βcXj.
Information across the rats is borrowed using the following hierarchical structure:
E(Yij|αi, βi) = αi + βiXj
αi ∼N(αc, τα)
βi ∼N(βc, τβ).
(A.18)

180
BAYESIAN ANALYSIS OF GENE EXPRESSION DATA
The model described above can be interpreted as saying that each rat has its
individual growth line but these lines themselves share a common distribution.
In other words, the individual growth line models the additional effect of the
covariates affecting individual rats over the general effect. The average population
growth line essentially models the general effect of the covariates that affect all
the rats. The marginal expected growth line is given by E(Yij|αc, βc). Using the
law of iterated expectation, we get the marginal expected growth line as follows:
E(Yij|αc, βc) = E(E(Yij|αi, βi))
= E(αi + βiXj)
= αc + βcXj.
Note that the marginal model follows the average population growth model. This
phenomenon can be interpreted as saying that if we integrate out the additional
effect of the covariates affecting individual rats, we are left with the general
effect of the covariates affecting all the rats. Now a hierarchal model can be
written as:
yi ≡(yi1, yi2, . . . , yini)T
θi ≡(αi, βi)T
yi|θi, σ 2
y ∼N(Xiθi, σ 2Ini).
Here X is the appropriate design matrix and IJ is the identity matrix of size J.
The priors for the unknown parameters (random variables) are elicited as
θi|µc, c ∼N(µc, c),
σ 2 ∼IG(u, v, )
µc ∼N(η, C),
−1
c
∼W((ρR)−1, ρ),
where W(V, n) is the Wishart distribution with mean nV . In Figure A.2(a) the
individual growth lines obtained using the MLEs of the regression parameters
along with the population average growth line are shown. In Figure A.2(b) Bayes
shrinkage estimates of the individual growth lines obtained using the regression
parameters along with the population average growth line are shown. Note that
the Bayes shrunk growth line shrinks the individual growth lines toward the
overall population average.
In summary, using hierarchical models, we can (i) borrow strength from
other observations, (ii) shrink estimates toward overall averages, (iii) update the
model in the light of available data, and (iv) incorporate prior/other information
in estimates and account for other sources of uncertainty.

BASICS OF BAYESIAN MODELING
181
A.4
Bayesian Mixture Modeling
The general problem of inferring a hypothesis, a model choice, or a complex
decision has received much attention in the subjective probability framework.
Bayesian mixture modeling offers subtle advantages over many alternatives for
decision making, providing a framework capable of incorporating the full prob-
lem uncertainty. Bayesian mixture modeling has its roots in frequentist theory,
extending the basic mixture modeling, allowing much greater ﬂexibility and con-
trol with prior information.
Suppose that observations y = (y1, . . . , yn) are distributed according to the
likelihood
P (y|θ, π) =
n

i=1
K

k=1
πkPk(yi|θk),
(A.19)
a weighted sum of likelihoods P1, . . . , Pk, indexed by θ1, . . . , θK with mixture
weights π =(π1, . . . , πK). This is an example of a ﬁnite mixture model. Inﬁnite
mixture models are discussed in Chapter 7. In practice, the parameters θ and π
are unobserved, although it is often the case that prior information is available
about one or more components. In the case of no prior information, the data
informs decisions about the mixture components. Usually when a mixture prior
is speciﬁed, it results in a posterior that is also a mixture, with weights naturally
updated according to the data. Often the mixture is expressed introducing latent
variables, zik, i = 1, . . . , n and k = 1, . . . , K,
P (y|θ, π) =
n

i=1
K

k=1
Pk(yi|θk)zik,
(A.20)
where
the
zik
follow
a
multinomial
distribution
with
parameters
π =
(π1, . . . , πK), naturally imposing the constraints 
k zik = 1 and 
k πk = 1.
The posterior predictive distribution of a new observation yn+1 is also a mixture,
derived by integrating over the full posterior uncertainty,
P (yn+1|y) =
	
Θ
	
Π

k
πkPk(yn+1|θk)P (θk, πk|y)dθdπ
=
K

k=1
	
k
	
k
πkPk(yn+1|θk)P (θk, πk|y)dθkdπk
=
K

k=1
αkPk(yn+1|y),
(A.21)

182
BAYESIAN ANALYSIS OF GENE EXPRESSION DATA
5
10
15
20
25
30
35
40
100
150
200
250
300
350
400
MLE growth lines
Days
(a) MLE
5
10
15
20
25
30
35
40
100
150
200
250
300
350
400
Bayesian growth lines
Days
(b) Bayes shrinkage
Figure A.2
Growth lines using maximum likelihood and Bayes estimates.

BASICS OF BAYESIAN MODELING
183
a mixuture of predictive densities, where we let
	
k
	
k
πkPk(yn+1|θk)P (θk, πk|y)dθkdπk = αk
	
k
Pk(yn+1|y, θk)P (θk|y)dθk,
= αkPk(yn+1|y)
(A.22)
for all k = 1, . . . , K. A widely speciﬁed prior for π is the Dir(α) density, for
its conditional conjugacy; in the special case where K = 2 this is the beta
distribution.
Example A.7
Let yi, i = 1, . . . , n, be i.i.d. according to the mixture model
P (yi) = π10 + (1 −π)N(0, σ 2),
(A.23)
an additive mixture of a point mass at 0 and a Gaussian density centered at 0
with unknown scale σ, with probabilities π and 1 −π, respectively. The priors
of the unobserved parameters π and σ 2 are speciﬁed as
π ∼Beta(a, b),
σ 2 ∼IG(ν/2, νσ 2
0 /2).
(A.24)
Reexpressing the posterior with the use of latent variables,
P (π, σ 2|y) ∝
n

i=1
(π10)zi((1 −π)N(yi; 0, σ 2))1−ziπzi(1 −π)1−zi
×πa−1(1 −π)b−1{σ 2}−ν/2−1 exp

−νσ 2
0
2σ 2

,
(A.25)
the expression does not admit a recognizable form. In Chapter 7 we discuss
Bayesian mixture models in the context of unsupervised classiﬁcation for gene
expression proﬁles, as well as computational methods for ﬁnite mixtures where
K is known, and where K is unknown.
A.5
Bayesian Model Averaging
Model choice is a popular approach where we select one model from some class
of models and assume that as the true model for the generation of the data.
By choosing a single model as correct we ignore model uncertainty and create
overconﬁdent inference. As ‘none of the models are true but some of them are
useful’, assumption of a single true model may not be a reasonable approach and

184
BAYESIAN ANALYSIS OF GENE EXPRESSION DATA
a mixture model may capture the unknown better than a single model. In fact
this mixing over models also improves the predictive performance.
Suppose that M1, M2, . . . , Mk is our collection of candidate models and
Mi = {P (y|θi, Mi), P (θi|Mi)} the likelihood and prior for model i. Let φ be
the quantity of interest, assumed to be well deﬁned for each model. If the prior
model probabilities are {P (M1), . . . , P (Mk)} then the posterior distribution of φ
is given by
P (φ|y) =
k

i=1
P (φ|Mi, y)P (Mi|y),
(A.26)
where P (φ|Mi, y) is the posterior for φ under the ith model and P (Mi|y) is the
posterior probability of the ith model given by
P (Mi|y) =
P (y|Mi)P (Mi)
k
j=1 P (y|Mj)P (Mj)
.
(A.27)
P(y|Mi) =

P (y|θi, Mi)P (θi|Mi)dθi is the marginal distribution of the data
under the ith model. Madigan and Raftery (1994) note that averaging over all
possible models in this fashion provides better average predictive ability, as mea-
sured by a logarithmic scoring rule, than using a single model. The main difﬁculty
of this method is that the number of potential models k could be extremely large;
for example, in a variable selection problem with 15 predictors there will be 215
models. Madigan and Raftery (1994) have used the Ockham’s window method
to average over parsimonious, data-supported models. An alternative would be
to search the entire sample space using MCMC, locating the high-probability
models. The problem will be more difﬁcult if we need to search the parameter
space simultaneously with the model space, when the marginalization of θ is not
analytically possible.

Appendix B
Bayesian Computation Tools
B.1
Overview
Bayesian computation tools are at the heart of Bayesian methods. As the science
of statistics has advanced, so have Bayesian methods and the computation tools
needed to apply them. These tools, while not unique to science, are generally
much more widely employed by Bayesian rather than frequentist statisticians,
and arguably have shaped Bayesian thinking as much as Bayesian thinking has
shaped science.
While, on the one hand, Bayesian methods offer ﬂexible modeling strategies,
there are computational hurdles to consider. Complex dependencies between vari-
ables, as well as intricate levels of variation, can be dealt with quite naturally
in a Bayesian paradigm, if one is willing to work with and develop the required
sophisticated computation algorithms. Many factors need to be considered when
choosing a Bayesian computation algorithm, such as the complexity of the pos-
terior distribution, the degree of difﬁculty of computing posterior probabilities of
events of interest, and the probability statements desired, to name but a few.
In this appendix we consider the computation problems of posterior estimation
in the spotlight of Bayesian application, and take as given that the likelihood
f (x|θ) and prior p(θ) distributions are adequate and fully reﬂect the modeler’s
beliefs about the experimental design and all sources of variation. The posterior
distribution of the parameters of interest is not required to have an analytical
closed form, i.e., the normalizing constant can be unknown.
Recall that the normalizing constant is given by p(x) =

 p(x|θ)π(θ)dθ.
Even in the event that p(θ|x) does have a closed form, i.e. p(x) is known, the
posterior probability distribution might still not admit a familiar form that is
convenient to work with, to make for example statements such as p(θ ∈R|x)
Bayesian Analysis of Gene Expression Data
B. Mallick, D. Gold, and V. Baladandayuthapani
2009 John Wiley & Sons, Ltd

186
BAYESIAN ANALYSIS OF GENE EXPRESSION DATA
for some set R contained in the parameter space, using conventional software.
In bioinformatics applications this can very often be the case; we must resort
to methods for approximating integrals over the posterior distribution, especially
when p(x) is unknown and cannot be evaluated analytically. In high dimensions
we can apply strategies based on Monte Carlo sampling or simulation.
B.2
Large-Sample Posterior Approximations
Where they exist, large-sample approximations to posterior distributions, or inte-
grals involving posterior measures, can offer a great deal of convenience. The
theoretical elegance of these methods attracts many, owing to a vast body of fre-
quentist work in asymptotics. While posterior approximations can be time-saving
devices, their use is limited to applications where an approximation is suitable,
to an appropriate order of magnitude. We begin with the fundamental and most
straightforward of posterior approximations, provided by the Bayesian central
limit theorem.
B.2.1
The Bayesian Central Limit Theorem
Like its frequentist counterpart this is a large-sample approximation, requiring
large samples for the approximation to work well. Let x1, . . . , xn be independent
observations from f (x|θ), and suppose that π(θ) is the prior for θ. The prior
may be improper, as long as the posterior distribution is proper and its mode
exists. Then as n →∞,
θ|x →Np
7 ˆθm, H −1( ˆθm)
8
with posterior mode ˆθm and posterior covariance H −1( ˆθm). The p × p matrix
H(θ) = −∂2 log p∗(θ|x)
∂θi∂θj
is the negative of the Hessian matrix over p∗(θ|x) = p(x|θ)π(θ), the kernel of
the (unnormalized) posterior density. The estimator of the asymptotic covariance
matrix is minus the inverse of the Hessian matrix, evaluated at ˆθm:
H −1( ˆθm) = H −1(θ)|θ= ˆθm.
The Bayesian central limit theorem tells us that once the appropriate normal dis-
tribution is identiﬁed, for a large enough sample size n, the posterior probability
of an event E can be approximated by
P (E|x) ≈
	
θ∈E
Np
θ; ˆθm, H −1( ˆθm) dθ.

BAYESIAN COMPUTATION TOOLS
187
Of course, the approximation depends on how closely the true posterior dis-
tribution resembles a normal distribution, and also on n, being a ﬁrst-order
approximation. The proof of the Bayesian central limit theorem has been dis-
cussed extensively, and can be found elsewhere. We direct the interested reader
to Carlin and Louis (2000).
Example B.1
Suppose that a cohort of n women is studied to link the development of benign
ovarian cysts with gene expression. We assume that the subjects are not exposed
to environmental risks of disease that would increase their susceptibility, and
are categorized as otherwise healthy individuals. Gene expression measurements
x′
i = (xi1, . . . , xiG) for G genes are obtained at time t0, the beginning of the
study. The binary response variables y1, . . . , yn are recorded for n subjects and
represent the presence or absence of a benign ovarian cyst during the follow-up
period with regular six-monthly visits to the doctor. Responses are assumed
independent Binomial(1,θi) random variables, with success (in this case presence
of a cyst) probability θi. The logistic function links the θi to gene expression,
θi =
exp(x′
iβ)
1 + exp(x′
iβ),
with unknown β, a G × 1 vector of regression coefﬁcients. The likelihood as a
function of β is given by
p(y|β) =
n

i=1
θyi
i (1 −θi)1−yi
=
n

i=1

exp(x′
iβ)
1 + exp(x′
iβ)
yi 
1
1 + exp(x′
iβ)
1−yi
= exp
 n

i=1

yix′
iβ −log
+
1 + ex′
iβ,
.
Setting a a uniform (improper) prior for β, i.e. π(β) ∝1, it follows that the
posterior mode of β is the maximum likelihood estimate, ˆβ, since the posterior
is proportional to the likelihood
p(β|y) ∝exp
 n

i=1

yix′
iβ −log
+
1 + ex′
iβ,
.

188
BAYESIAN ANALYSIS OF GENE EXPRESSION DATA
The variance–covariance matrix is derived via the Hessian,
∂
∂βj
log p∗(β|y) =
n

i=1

yi −
exp(x′
iβ)
1 + exp(x′
iβ)

xij
−
∂2
∂βj∂βk
log p∗(β|y) =
n

i=1
xijxik
exp(x′
iβ)

1 + exp(x′
iβ)
2
H(β) = −∂2 log p∗(β|x)
∂β∂β′
= X′V X,
where V is an n × n diagonal matrix with ith diagonal element
vii =
exp(x′
iβ)

1 + exp(x′
iβ)
2 ,
for i = 1, . . . , n. Asymptotically,
β|y →Np
 ˆβ, (X′ ˆV X)−1 ,
where ˆV = V |β= ˆβ evaluated at ˆβ is


exp(x′
1 ˆβ)
(1+exp(x′
1 ˆβ))
2
0
...
0
exp(x′n ˆβ)
(1+exp(x′n ˆβ))
2


n×n
.
B.2.2
Laplace’s Method
Another class of posterior approximations are provided by Laplace’s method
of approximating integrals, which dates from 1774 (Carlin and Lewis, 2000).
Laplace’s method has been studied extensively, and recent extensions, discussed
below, provide second-order convergence, without strong dependence on the
Gaussian assumption.
We are interested in approximating integrals (one-dimensional for the time
being) of the form
	
f (θ)e−nq(θ)dθ,

BAYESIAN COMPUTATION TOOLS
189
for smooth functions f and q of θ, assuming that q attains a unique minimum
at ˆθ. It is straightforward to see, expanding the functions f and q as Taylor’s
series around ˆθ, that
	
f (θ)e−nq(θ)dθ ≈
	 
f ( ˆθ) + f ′( ˆθ)
1!
(θ −ˆθ) + f ′′( ˆθ)
2!
(θ −ˆθ)2

× exp

−nq( ˆθ) −nq′( ˆθ)(θ −ˆθ) −nq′′( ˆθ)(θ −ˆθ)2
2!

dθ
=
	
e−nq( ˆθ)

f ( ˆθ) + f ′( ˆθ)(θ −ˆθ) + f ′′( ˆθ)
2
(θ −ˆθ)2

× exp

−(θ −ˆθ)2
2(nq′′( ˆθ)−1

dθ.
Notice that the quadratic term in the exponent resembles a normal kernel, forcing
the second term in brackets to vanish under integration. The expectation can be
approximated by
	
f (θ)e−nq(θ)dθ ≈e−nh( ˆθ)f ( ˆθ)
9
2π
nh′′( ˆθ)

1 + f ′′( ˆθ)
2f ( ˆθ) var(θ)

,
for var(θ) = 7nh′′( ˆθ)8−1 = O(n−1), i.e. a ﬁrst-order approximation. In p dimen-
sions
	
f (θ)e−nq(θ)dθ ≈f ( ˆθ)
2π
n
p/2
| ˜|1/2 exp
7
−nh( ˆθ)
8
,
for the p × p matrix
˜ =

 ∂q(θ)
∂θj∂θk
−1
|θ= ˆθ.
A very useful second-order approximation to Laplace’s method can be achieved
by means of the following ingenious substitution (Tierny and Kadane, 1986).
Suppose that we wish to compute the posterior expectation of a function g(θ) > 0.
We can let −nq(θ) equal the unnormalized posterior density,
−nq(θ) = log[p(x|θ)π(θ)].
The expectation of g(θ) is
E[g(θ)] =

g(θ)e−nq(θ)dθ

e−nq(θ)dθ
=

e−nq∗(θ)dθ

e−nq(θ)dθ ,

190
BAYESIAN ANALYSIS OF GENE EXPRESSION DATA
with q∗(θ) = −1
n log(g(θ)) + q(θ). We can now apply Laplace’s method as
before with f = 1 in both the numerator and denominator. The integral is
E[g(θ)] = | ˜∗|1/2 exp
7
−nq∗( ˆθ∗)
8
| ˜|1/2 exp
7
−nq( ˆθ)
8
7
1 + O(n−2)
8
,
where θ∗maximizes −q∗and
∗=

∂q∗(θ)
∂θj∂θk
−1
|θ=θ∗,
approximated by
E[g(θ)] ≈|∗|1/2 exp
7
−nq∗( ˆθ∗)
8
||1/2 exp
7
−nq( ˆθ)
8 .
The reason for the improved convergence is that the error terms of O(n−1) in
the numerator and denominator are identical, canceling to leave the remaining
higher-order term of O(n−2). The proof is left as an exercise. For further discus-
sion of asymptotic methods for integral approximation, see Tierny and Kadane
(1986) and Kass et al. (1988, 1989).
Example B.2
Suppose that the parameter space can be partitioned as  = 1 × 2, where
θ2 ∈2 is not of interest. We are interested in approximating the integral
E(g(θ1)|x) =
	
1
	
2
g(θ1)p(θ1, θ2|x)dθ2dθ1
=

1

2 exp {log[g(θ1)p(x|θ1, θ2)π(θ1, θ2)]} dθ1dθ2

1

2 exp {log[p(x|θ1, θ2)π(θ1, θ2)]} dθ1dθ2
,
=

 exp {log[−nq∗(θ)]} dθ

 exp {log[−nq(θ)]} dθ .
The Laplace approximation is
E[g(θ)] ≈| ˜∗|1/2 exp
7
−nq∗( ˆθ∗)
8
| ˜|1/2 exp 7−nq( ˆθ)8 ,
where
ˆθ = ( ˆθ1, ˆθ2)
maximizes
p(x|θ)p(θ),
and
ˆθ∗= ( ˆθ∗
1 , ˆθ∗
2 )
maximizes
g(θ1)p(x|θ)p(θ).

BAYESIAN COMPUTATION TOOLS
191
B.3
Monte Carlo Integration
A common criticism of Laplace’s method is that it assumes ‘smooth’ func-
tions. Suppose that we are interested in p(θ > 0|x) or p(δ1 < g(θ) < δ2|x), for a
discrete parameter or a discontinuous transformation, respectively. Further com-
plications can arise as the number of dimensions grows, or if the sample size is
small. Obviously, these cases present problems, although they represent a large
class of typical problems in bioinformatics. However, if we can obtain a sequence
of values θ1, θ2, θ3, . . . sampled from the target distribution p(θ|x) then we can
form approximations to integrals of interest to a desired degree of error. The
following result provides a more general class of computational approximations
that are very useful in a wide set of circumstances. Suppose that we are interested
in evaluating
E
7
g(θ)|x
8
=
	
g(θ)p(θ|x)dθ.
Then if θ1, . . . , θn are i.i.d. samples from p(θ|x),
1
n
n

j=1
g(θj) →E
7
g(θ)|x
8
as n →∞with probability 1 by the strong law of large numbers. This very
general result does not require that g(θ) or p(θ|x) be smooth, or that p(θ|x)
be unimodal. All we need is a large enough sample from p(θ|x), to achieve the
desired accuracy. In practice, obtaining a sample from the posterior distribution
can be direct or indirect, depending on whether or not a suitable scheme exists
to perform sampling. If we can sample from p(θ|x) directly, then we are done,
as we can rely on the strong law of large numbers to achieve the desired result.
If not, a mechanism is needed to carry out indirect sampling, that is, sampling
from a source other than p(θ|x), to yield a sequence from p(θ|x), or in more
difﬁcult situations, a sequence converging in distribution to p(θ|x).
Monte Carlo simulation is a widely and fundamentally accepted tool among
Bayesians for estimation, inference, and prediction. Increasing the Monte Carlo
size n of the sequence can lead to improved approximations, although many
Bayesians make use of kernel methods to make approximations to posterior
statements, once a reasonable sample from the posterior distribution is obtained.
For example, the kernel estimate of p(θ|x) is
ˆp(θ|x) =
1
nwn
n

j=1
K
θ −θj
wn

,
for the kernel function K(·), usually a normal or a symmetric unimodal distri-
bution. The window width wn controls the smoothness of the estimate; smaller
windows provide better resolution, requiring a larger n. A useful result is that

192
BAYESIAN ANALYSIS OF GENE EXPRESSION DATA
the window can be treated as a function of the Monte Carlo size n, satisfying
wn →0 and nwn →∞as n →∞(Bernard and Silverman, 1986).
Example B.3
Suppose that x1, . . . , xn are i.i.d. N(µ, σ 2), π(µ, σ) ∝σ −1, and τ = σ −2, and
we are interested in making posterior statements about the coefﬁcient of variation,
deﬁned as ψ(µ, σ) = σ/µ. Recall that the posterior distribution of µ and τ =
σ −2 is
µ|τ, x ∼N

¯x, 1
nτ

,
τ|x ∼Gamma
n −1
2
, (n −1)s2
2

,
for s2 = (n −1)−1 n
i=1(xi −¯x)2. We generate samples in a sequence as follows:
1. Sample τj ∼Gamma
+
n−1
2 , (n−1)s2
2
,
.
2. Sample µj|τj ∼N
+
¯x,
1
nτj
,
.
3. Repeat steps 1 and 2 until a desired sequence (µ1, τ1), . . . , (µn, τn) of n
observations is obtained.
We approximate E(ψ|x) by
ˆE(ψ|x) = 1
n
n

j=1
ψj = 1
n
n

j=1
τj/µj.
Alternatively, for a (1 −α) highest posterior credible set for ψ, we can use the
empirical α/2 and 1 −α/2 quantiles of the sampled ψj. In this example, a sample
from the posterior distribution can be obtained directly. We now turn to indirect
sampling methods, which require much more consideration. These methods open
up a richer set of computational tools, putting at our disposal a wide variety of
Bayesian methods.
B.4
Importance Sampling
Suppose that we are interested in the posterior expectation of the transformation
g(θ),
E
7
g(θ)|x
8
=

g(θ)f (x|θ)p(θ)dθ

f (x|θ)p(θ)dθ
,

BAYESIAN COMPUTATION TOOLS
193
and, unlike the example above, no mechanism exists to sample directly from
the posterior p(θ|x). If a density q(θ) exists that is similar to the posterior, and
that one can sample directly, then Monte Carlo integration can be used, with the
following modiﬁcation. Since the sample θj, j = 1, . . . , n, is from q(θ) rather
than p(θ|x), then the average must be modiﬁed, with the appropriate weights.
Consider the importance weight function
ω(θ) = f (x|θ)p(θ)
q(θ)
,
the ratio of the unnormalized posterior to our density q(θ). Then
E
7
g(θ)|x
8
=

g(θ)ω(θ)q(θ)dθ

ω(θ)q(θ)dθ
≈
1
N
N
j=1 g(θj)ω(θj)
1
N
N
j=1 ω(θj)
,
where the θj, j = 1, . . . , N, are sampled i.i.d. directly from q(θ). The density
q(θ) is called the importance function.
There are many ways to choose the importance function q(θ) in practice. For
all sets A ∈θ such that the posterior P (θ ∈A|x) > 0, the envelope function must
be greater than 0, q(θ ∈A) > 0, as well. That is to say, the more the importance
function is similar to the posterior, the better the precision of the approximation.
Not to be confused with prior speciﬁcation, during posterior computation one
may look at the data.
Importance sampling is appealing in its simplicity; all that is required to
approximate integrals of (possibly) complex variable transformation with Monte
Carlo integration is a sample from the importance function q(θ), and formation
of the appropriate weights.
B.5
Rejection Sampling
For high-dimensional posterior parameter spaces, choosing an importance func-
tion that agrees well with the posterior can be a formidable task. Imagine that
the posterior density is only known up to a normalizing constant,
p(θ|x) ∝f (x|θ)p(θ),
in such a way that the normalizing constant cannot be evaluated. Posterior sam-
pling can still be achieved, albeit with more sophisticated methods that we have
examined so far.
Rejection sampling is a well-established technique used by Bayesians for its
robustness and simplicity. Like importance sampling, we must specify a distribu-
tion q(θ) that is similar to the posterior that we can easily sample from, called the

194
BAYESIAN ANALYSIS OF GENE EXPRESSION DATA
envelope function. Think of the envelope function as ‘blanketing’ the posterior,
that is, we want a function q(θ) that is similar to and somewhat more dispersed
that the posterior.
The steps of the rejection method are as follows. First, suppose that we have
an envelope function q(θ) and a constant M such that
f (x|θ)p(θ) < Mq(θ),
(B.1)
for all θ ∈, the parameter space. For each iteration j, we sample a candidate
θj ∼q(θ), and a uniform random variate uj ∼U(0, 1). If
uj < f (x|θj)p(θj)
Mq(θj)
,
(B.2)
then we accept the candidate θj, and include it in our sample, and reject the
candidate otherwise. The iterations are completed when the desired number of
samples N are obtained. The accepted θj in our sample are distributed accord-
ing to p(θ|x), although the number R of required iterations can vary from one
sequence to the next. One measure of the efﬁciency of the algorithm is the
acceptance ratio, the percentage of accepted candidates. In practice, we desire to
generate a minimum expected number of iterations in order to obtain a sample
of size N. Since the functions f (x|θ) and p(θ) are ﬁxed, we must identify an
envelope function q(θ) such that M is as small as possible, so that the maxi-
mum acceptance ratio is achieved. The random count of rejections proceeding
a candidate’s acceptance follows the well-known geometric distribution, with
acceptance probability
P (accept candidate) =

 f (x|θ)p(θ)dθ
M
(B.3)
and expected value proportional to M. Hence, choosing M small furnishes a high
acceptance rate; see Carlin and Luis (2000), Ripley (1987), and Devroye (1986).
Example B.4
In this example, the posterior is bimodal, a mixture of two Gaussian densities.
The envelope function is speciﬁed as N(0, 22), with M = 3, and achieved an
acceptance ratio of 0.333, displayed in Figure B.1.
The following code was run in R to generate the sample:
f.star <- function(theta) dnorm(theta,-2,1)/3+2*dnorm(theta,1,.5)/3
f.env <- function(theta) dnorm(theta,0,2)
theta.seq <- seq(-8,8,length=1000)
N = 100000
M = 3
theta <- rnorm(N,0,2)

BAYESIAN COMPUTATION TOOLS
195
0
q
q
(b)
(a)
5
−5
0
5
−5
f(q)
frequency
1.0
0.8
0.6
0.4
0.2
0.0
1.0
0.8
0.6
0.4
0.2
0.0
Figure B.1
(a) Target density (solid line) and envelope q(·) · M (dashed line).
(b) Histogram of sampled values.
u <- runif(N)
accept <- u < f.star(theta)/(M*f.env(theta))
theta <- theta[accept]
B.6
Gibbs Sampling
The Gibbs sampling algorithm is central to Bayesian applications and research.
One attractive feature of the algorithm is its implementation of the mathemat-
ical principle of ‘divide and conquer’. High-dimensional posterior parameter
spaces can easily be sampled with Gibbs sampling, provided thatcertain robust
conditions are met. Let us begin by deﬁning a multivariate k-dimensional param-
eter space θ1, . . . , θk ∈1 × . . . × k, with joint posterior density denoted by
pk(θ1, . . . , θk|x). We introduce the posterior full conditional distribution of θi,
which is simply
p(θi|θ1, . . . , θi−1, θi+1, . . . , θk, x) ∝p(θ1, . . . , θi, . . . , θk|x),
(B.4)
proportional to the posterior, holding all other parameter values as given, i.e.
ﬁxed. The posterior full conditional of θi is just the marginal posterior distribution
of θi conditional on, or taking as given, θ1, . . . , θi−1, θi+1, . . . , θk. The algorithm
proceeds as follows:
1. Choose starting values {θ(0)
1 , . . . , θ(0)
k }.
2. Draw θ(1)
1
from its full conditional p(θ1|θ(0)
2 , . . . , θ(0)
k ).
3. Draw θ(1)
2
from its full conditional p(θ2|θ(1)
1 , θ(0)
3 , . . . , θ(0)
k ) given θ(1)
1
drawn in the last step.
4. Draw θ(1)
3
from its full conditional p(θ3|θ(1)
1 , θ(1)
2 , . . . , θ(0)
k ) given θ(1)
1
and
θ(1)
2 .

196
BAYESIAN ANALYSIS OF GENE EXPRESSION DATA
5. . . .
6. Draw θ(1)
k
from its full conditional p(θk|θ(1)
1 , . . . , θ(1)
k−1), conditional upon
all parameter values drawn thus far.
One iteration of the Gibbs sampler is completed after we obtain one new sample
for all k parameters, {θ(1)
1 , . . . , θ(1)
k }. The process is repeated R times to obtain
a sample:
θ(0)
1 ,
θ(0)
2 ,
· · ·
θ(0)
k
θ(1)
1 ,
θ(1)
2 ,
· · ·
θ(1)
k
θ(2)
1 ,
θ(2)
2 ,
· · ·
θ(2)
k
...
θ(R)
1
,
θ(R)
2
,
· · ·
θ(R)
k
(B.5)
from the joint posterior distribution of size R. Geman and Geman (1984) showed
that the probability distribution of the sequence {θ(t)
1 , . . . , θ(t)
k } converges to
p(θ1, . . . , θk|x) as t →∞, exponentially. Gibbs sampling was ﬁrst introduced
in the statistical literature by Gelfand and Smith (1990), and further discussed in
Schervish and Carlin (1992).
If one is interested in the marginal density of a variable transformation of
a subset of θis, the respective columns of (B.5) provide the sub-joint sample,
i.e. ignoring the remaining columns. The marginal posterior of, for example, a
function of ψ(θi) can be derived by extracting the ith column of (B.5), with
marginal posterior estimated by
ˆp(ψ(θi)|x) = 1
R
R

r=1
p(ψ(θi)|θ(r)
1 , . . . , θ(r)
i−1, θ(r)
i+1, . . . , θ(r)
k ).
Example B.5
Suppose x1, . . . , xn are i.i.d. N(µ, σ 2), τ = σ −2, and π(µ, τ) ∝τ −1. From pre-
vious examples, we obtain the full posterior conditionals as
π(µ|τ, x) = N

¯x, 1
nτ

,
π(τ|µ, x) = Gamma
n
2,
(xi −µ)2
2

,
The Gibbs sampler is as follows:
1. Generate starting values (µ(0), τ (0)).

BAYESIAN COMPUTATION TOOLS
197
2. Generate µ(1) from π(µ|τ (0), x).
3. Generate τ (1) from π(τ|µ(1), x).
4. Repeat steps 2 −3 until we have R samples (µ(1), τ (1)), . . . , (µ(R), τ (R)).
While Gibbs sampling provides great computational convenience for sam-
pling high-dimensional posterior distributions, its utility is of course limited to
situations where the full conditional distributions exist in known form and can
easily be sampled. If the full conditionals are not easily sampled then we must
resort to advanced methods. Notice that the full conditional expressions provide
insight into the parameter dependence, and thus, the ﬂow of information between
the parameters, while updating at each Gibbs stage. The analytical convenience
of working with these expressions cannot be overemphasized, as among other
things, it can aid in choosing priors!
Example B.6
Suppose that (xi1, xi2)T ∼N(µ, ) are bivariate normal with µ unknown and
 =

1
0.90
0.90
1

.
(B.6)
Let the prior for µ be ﬂat, i.e. improper, and suppose that n observations
are obtained. The Gibbs updating procedure can be performed by ﬁrst specifying
starting values, in this case (0, 0). Next we can update the chain one dimension
at a time for r = 1, . . . , 1000 iterations:
1. µ(r)
1
∼N(¯x1 + 0.90(µ(r−1)
2
−¯x2), 1).
2. µ(r)
2
∼N(¯x2 + 0.90(µ(r)
1 −¯x1), 1).
Figure B.2 displays one realization of the resulting chains. The following code
was run in R to generate the sample:
rho = .9
N = 1000
mu1 = mu2 = rep(0,N)
xbar1 = 1
xbar2 = 2
n = 25
sigma = sqrt((1-rho*rho)/n)
for(r in 2:N)–
mu1[r] = rnorm(1, xbar1+rho*(mu2[r-1]-xbar2), sigma)
mu2[r] = rnorm(1, xbar2+rho*(mu1[r]-xbar1), sigma)
˝

198
BAYESIAN ANALYSIS OF GENE EXPRESSION DATA
−3
−1.0
−1.0
0.0
0.5
1.0
0.0
1.0
2.0
−0.5
0.0
0.5
1.0
−3
−1
m2
m2
m2
m
m1
1
2
3
−3
−1
1
2
3
−2
−1
0
(b)
m1
(a)
−0.5
0.0
0.5
1.0
1.5
m1
(c)
0
200
400
600
800
Iteration
(d)
1
1
1
2
2
2
3
3
3
4 56 00
Figure B.2
(a) The ﬁrst 3 observations in the chain. (b) The ﬁrst 9 observations
in the chain. (c) 1000 observations. (d) Trace plots, µ1 (black open circles) and
µ2 (gray open circles).
Burn-in appears to be achieved after 10 or so observations, as the observations in
the chain migrate away from (0,0). Note that choosing the starting values closer
to regions of high posterior mass (i.e. in this case the sample mean ¯x1, ¯x2) works
ﬁne, and will speed up burn-in dramatically.
B.7
The Metropolis Algorithm and
Metropolis–Hastings
Of the many difﬁculties in posterior computation, the most challenging hurdles
are frequently those encountered in practice, calling for robust methods that can
be applied in a variety of situations. Each indirect sampling scheme in the pro-
ceeding sections offers a solution, provided certain conditions are met, e.g. a
suitable importance function exists for importance sampling, a convenient enve-
lope function can be found for rejection sampling, or full conditionals are readily
available for sampling with the Gibbs algorithm. As is often the case in practice,

BAYESIAN COMPUTATION TOOLS
199
mild conditions can preclude the use of such methods, as even a simple tweak
to a prior can have major implications for the efﬁciency of a computational
tool. The Metropolis algorithm (Metropolis et al., 1953) is a general computa-
tional tool that is very popular among Bayesian for the ﬂexibility it furnishes for
indirect sampling of a posterior distribution. Hastings (1970) later proposed the
algorithm for problems in statistics. The Metropolis and Metropolis–Hastings
algorithms, discussed below, are Markov chain Monte Carlo methods, meaning
that the methods generate a sequence of values moving from one state (i.e. the
domain of the posterior in our case) to the next in a series of decisions, that
in theory provide a chain of observations converging in distribution to the true
target posterior.
Suppose that we desire a sample from the joint posterior distribution of θ =
(θ1, . . . , θk), denoting the joint posterior density as pk(θ). Let us introduce the
proposal density q(u, v), symmetric in its arguments, i.e. q(u, v) = q(v, u). The
proposal density provides candidate proposals to move between states, i.e. from
v, the current state of the chain, to u, an update in the chain. In a single iteration
t of the Metropolis algorithm, a candidate θ∗is generated as θ∗∼q(·, θ(t−1)),
conditional upon the current state of the chain, θ(t−1). A decision is made to
either accept the candidate θ∗and set θt = θ∗or to reject the candidate, in
which case θt = θt−1 and the chain does not ‘move’. The candidate is accepted
with probability equal to min(r, 1), where
r = p(v)
p(u) =
f (x|θ∗)π(θ∗)
L(x|θ(t−1))π(θ(t−1)),
and rejected with probability 1 −min(r, 1). A full implementation of the algo-
rithm proceeds as with Gibbs sampling from a starting point θ(0), e.g. the posterior
mode or in some cases a perturbed estimate thereof, and is followed by the
above iterations, accepting/rejecting candidates from the proposal, until the algo-
rithm converges to the target posterior distribution, i.e., after the burn-in stage.
Once it is determined that burn-in is achieved, many iterations are repeated until
the desired number of samples are obtained. Unlike the rejection algorithm, the
sequence generated by the Metropolis algorithm is a Markov chain and, as such,
yields observations in a sequence that are dependent. A procedure called thinning
is often employed to reduce the covariance between observations, for example,
culling the sequence to retain only observations d iterations, or lags, apart.
While it may appear that the Metropolis algorithm is expensive to run,
in high-dimensional settings, for example, remember that there is information
embodied in the decision to move from the last state, which can inform the next
decision, etc. Bayesians use all of the information! Under mild conditions the ran-
dom sequence generated by the Metropolis algorithm converges in distribution to
the target density, in our case the posterior, as t →∞(Robert and Casella, 1999).

200
BAYESIAN ANALYSIS OF GENE EXPRESSION DATA
Example B.7
Suppose that (xi1, xi2)T are i.i.d. bivariate S(d, µ, ), for i = 1, . . . , n, with 
known and µ = (µ1, µ2)T unknown. We specify an improper ﬂat prior for µ,
f (x|µ)p(µ) ∝
n

i=1

1 + 1
d (xi −µ)T −1(xi −µ)
−2
.
For many problems with a continuous parameter space, a convenient choice
for the proposal density is to generate a candidate,
µ∗∼N

µ(t−1), R

,
where µ(t−1) is the current value of the chain. The proposal covariance R con-
trols the variability in the ‘jumps’ of the proposal from the current state. Large
jumps will tend to be accepted less often, using little information about the
current state, whereas small jumps, while accepted at a higher rate, can yield
chains with much more autocorrelation and take longer to explore the parameter
space.
At iteration t, the odds ratio is computed,
r =
(n
i=1(1 + 1
d (xi −µ∗)T −1(xi −µ∗))−2
(n
i=1(1 + 1
d (xi −µ(t−1))T −1(xi −µ(t−1)))−2 ,
and compared with a uniform random variable u ∼U(0, 1). If u <min(r,1) then
the candidate is accepted, i.e., µ(t) = µ∗, otherwise we set µ(t) = µ(t−1).
For continuous parameter settings, the most convenient choice for the pro-
posal q is N

θ(t−1), ˜

, where
˜ = −

∂2 log[p∗(θ|x)]
∂θ∂θ′
−1
|θ=θ(t−1),
with p∗(θ|x) = f (x|θ)p(θ). This choice of q is easily sampled and clearly sym-
metric in v and θ(t−1). In practice, choosing a good proposal variance is an
acquired skill, requiring trial and error.
An important enhancement to the Metropolis algorithm was made by Hastings
(1970), allowing for an asymmetric proposal density. Hastings generalized the
odds ratio as
r = f (x|v)p(v)q(u, v)
f (x|u)p(u)q(v, u).
For more discussion of MCMC methods, see Chib and Greenburg (1998) and
Robert and Casella (1999).

BAYESIAN COMPUTATION TOOLS
201
Example B.8
Suppose that (xi1, xi2)T ∼N((1, 2)T ), ), with covariance matrix
 =
 1
ρ
ρ
1

(B.7)
and ρ unknown. Placing a ﬂat prior on ρ, the log-posterior is
log(p(ρ)) ∝−n
2 log || −1
2
n

i=1
 xi1
xi2 −1
2
T
−1
 xi1
xi2 −1
2

.(B.8)
Metropolis–Hastings updating is performed at iteration t, with beta proposal, let-
ting α = (c −2) ∗ρ(t−1) + 1 and β = c −α, for c = 25. Why? The acceptance
ratio in this example is
r = exp
7
log p(ρ∗|x) + log q(ρ(t−1)|x) −log p(ρ(t−1)|x) −log q(ρ(t−1)|x)
8
.
Trace plots and histograms for 10 000 iterations are shown in Figure B.3. The
acceptance rate, i.e. the rate at which the chain is updated to a new state, is 25%.
The following code was run in R to generate the sample:
0.0
0.2
0.4
0.6
0.8
1.0
0
0.50
0.60
0.70
0.80
200
600
Frequency
1000
r
r
(b)
(a)
0
2000 4000 6000 8000
Iteration
Figure B.3
(a) Trace plot for ρ. (b) Histogram of sampled ρs.
logpost <- function(rho)–
out = -(n/2) * log(1-rho*rho)
out = out - 1/(2*(1-rho*rho))*(sum(x1*x1) -2*rho*sum(x1*x2)
+ sum(x2*x2))
out
˝
# generate bivariate normal data
N = 10000

202
BAYESIAN ANALYSIS OF GENE EXPRESSION DATA
R = 5000
true.rho = 0.70
V = matrix(c(1,true.rho,true.rho,1),2,2)
decomp = svd(V)
C = diag(sqrt(decomp$d))%*%decomp$u
n = 100
X <- matrix(rnorm(n*2),n,2)
Y <- X rho = rep(0,R)
rho[1] = cor(Y)[1,2]
x1=Y[,1]
x2=Y[,2]
# *beta proposal
# candidate a* + b* = const
# a* = (const-2)*rho.old+1
# b* = const - a*
consts = c(1,4,10)
const = 3
for(r in 2:N)–
rho.old = rho[r-1]
a.star = (const-2)*rho.old+1
b.star = const - a.star
rho.star = rbeta(1,a.star,b.star)
a.star2 = (const-2)*rho.star+1
b.star2 = const - a.star
acc = logpost(rho.star) + log(dbeta(rho.old,a.star2,b.star2))
acc = acc - logpost(rho.old) - log(dbeta(rho.star,a.star,b.star))
if(log(runif(1)) < acc) rho[r] = rho.star else rho[r] = rho.old
˝
B.8
Advanced Computational Methods
In practice, there are occasions when the methods outlined above do not provide
the necessary tools to perform posterior sampling as desired. Much analytical and
methodological effort has been put into overcoming such obstacles and providing
improved efﬁciency for Monte Carlo applications. Fortunately, many advances
have been made to combine the methods that we already discussed, or to apply
them with simple modiﬁcations that can substantially improve performance. For
example, recall that with Gibbs sampling an important condition is that the full
conditionals can be sampled conveniently. If the full conditionals cannot be easily
sampled, say because some collection of full conditionals have unknown form,
one option is to perform the rejection algorithm within Gibbs sampling. Each
full conditional is sampled in turn, until a full conditional of unknown form
is encountered, p(θi|θ[−i], x). The appropriate envelope function and constant
M must be identiﬁed, and the rejection algorithm applied, until an observa-
tion is obtained from the full conditional. Rejection sampling within Gibbs was

BAYESIAN COMPUTATION TOOLS
203
discussed in Gelfand and Smith (1990), Gilks and Wild (1992), Gelfand et al.
(1990), and Wakeﬁeld et al. (1994). Metropolis within Gibbs has also been dis-
cussed (Gelfand, 2000).
B.8.1
Block MCMC
Thus far, we have discussed iterative schemes to update the sequence of parameter
observations one dimension at a time. In some cases, the parameters can naturally
be partitioned into blocks, or multivariate sets, that can be conveniently sampled
in batches. Suppose, for example, that a multistage linear model is ﬁtted, of the
form
Y ∼Nn(µ, ),
µ ∼Np(, ),
 ∼Nv(0, 0),
where at the ﬁrst stage the data Y are of dimension n, at the next stage µ is of
dimension p, and at the deepest stage in the hierarchy  is of dimension v. For
simplicity, let us take 0, ,  and 0 as known, and let parameter vectors µ
and  be unknown. The model can be expressed as a system of linear equations
Y = µ + E,
µ =  + W,
 = 0 + W0,
where E ∼Nn(0, ), W ∼Nn(0, ), and W0 ∼Nn(0, 0). This system of
equations can conveniently be reexpressed as
Y = µ + 0 + E,
0 = −µ +  + W,
−0 = 0 +  + W0,
so that a new multivariate normal response can be deﬁned as
˜Y =


Y
0
−0


with mean
˜µ =


µ
−µ + 


=


1
0
−1
1
0
1



µ



204
BAYESIAN ANALYSIS OF GENE EXPRESSION DATA
and covariance ˜, of block diagonal structure, with , , and 0 along the
diagonal. Sampling techniques, discussed in Hodges (1998) can be used to obtain
posterior samples of the unknown parameter vectors. The above reexpression
can be applied to the case where there are many more stages. When p and v
are very large, or if there are many stages to consider, it may be more practical
to sample the parameters in blocks, i.e. sample the vectors µ|, · and |µ, ·
in turn. This is called block sampling, and can be applied with the methods
discussed above (Wilkinson and Yeung, 2002). It is advantageous, since, if the
vector of parameters θ has known full conditional distribution, it can be sampled
simultaneously, rather than sampling each individual dimension θ1, followed by
θ2, etc.
B.8.2
Truncated Posterior Spaces
Suppose, for example, that, rather than the full parameter space, it is known that
the p-dimensional mean of the multivariate Gaussian response Y lies in the con-
tinuous line between vectors a and b, i.e. the posterior parameter space lies in
a hypercube with hyperplane manifolds speciﬁed by the elements in the vectors
a and b. A normal prior would appear advantageous, if the constraints were not
imposed. Suppose that a normal prior is speciﬁed as p(µ) ∝N(0, R) × I(a, b)
where the identity function is one if µ ∈(a, b) and zero otherwise. Conveniently,
we sample the elements of µ one at a time, as µ1, µ2|µ1, µ3|µ2, µ1, etc., where at
each stage, the density of the parameter µj is speciﬁed as the appropriate condi-
tional normal truncated to be in (aj, bj). This algorithm will produce multivariate
observations, within the appropriate bounds.
A more challenging situation arises when the parameters lie in an amorphous
set of nonlinear manifolds. The above algorithm can still be applied, taking
care to evaluate the appropriate bounds at each stage, e.g. for µj given the
parameter values µ1, . . . , µj−1 already sampled. For example, suppose that the
parameter space is two-dimensional and bounded within the unit sphere. Then
µ1 is bounded to lie in (−1, 1). Once µ1 is sampled then µ2 is bounded to be
between ±

1 −µ2
1.
B.8.3
Latent Variables and the Auto-Probit Model
Suppose that random binary responses Y1, Y2, . . . , Yn exhibit dependence, deﬁned
through the n × n adjacency matrix A, i.e. the (i, j)th entry of A equals 1 if
Yi and Yj are dependent, and zero otherwise. There are many ways to deﬁne
dependence, but for now, let us take the dependence between pairwise neighbors
to be deﬁned globally. In the Bayesian probit modeling framework, the marginal
binary responses can be ﬁtted by introducing a latent normal variable Z such

BAYESIAN COMPUTATION TOOLS
205
that
P (Yi = 1|Zi > 0) = 1,
Zi ∼N(θ, σ 2),
that is, the conditional distribution of p(Zi|Yi = 1) ∝N(θ, σ 2) × I(Zi > 0) and
p(Zi|Yi = 0) ∝N(θ, σ 2) × I(Zi < > 0) (Chib and Greenberg, 1998). Posterior
updating for θ and σ 2 are made conditional upon the true labels, i.e. conditional
upon the latent Zs during updating, while the Zs are updated, given Y, i.e.
truncated, conditional upon sampled θ and σ 2.
For multivariate binary response data, the prior density of the vector Z is
speciﬁed as
Z ∼N(θ, (I −ρA)−1),
multivariate Gaussian, for the unknown mean θ, scalar ρ, and known adjacency
matrix A. The full conditional prior, setting θ = 0 without loss of generality,
for Zi is N(ρ 
i∼j zj, 1), with the notational convention that the ith and jth
entries are neighbors if i ∼j. The full conditional posterior for Zi is updated
with sign restriction, given Yi, and ρ. Placing a ﬂat prior on ρ, the Metropolis
algorithm proceeds subject to the condition that (I −ρA) is positive deﬁnite; see
Weir and Pettitt (2000) for details on model ﬁtting and imputation of missing
data. Intuitively, this is a nearest-neighbor model, with the vote of neighbors
depending on ρ, through the conditional mean. Thus, the unknown parameter ρ
tells us the strength of borrowing between neighbors.
B.8.4
Bayesian Simultaneous Credible Envelopes
A serious challenge facing statisticians is estimation of variation and credible
(or conﬁdence) bounds. For a frequentist, this entails derivation of the appro-
priate asymptotic frequency conﬁdence bounds, if the exact bounds cannot be
found, for each new ﬁtted model. This challenge is compounded by many orders
of magnitude for multivariate estimates, e.g. function data analysis, or spatial
analysis. In functional data analysis one may desire a set of plausible outcomes,
given speciﬁed covariate values, or a simultaneous set of outcomes across the
functional range, given the complete domain of covariate values. The theoreti-
cal challenges for deriving simultaneous functional data conﬁdence sets are well
known. For example, with nonlinear regression, the asymptotic approximations
depend on the smoothness of the function, and are known to exhibit problems at
discontinuities and loci with sharp second derivative.
For a Bayesian, there are no obstacles to credible set estimation, once a col-
lection of observations are obtained from the posterior. Suppose that the response
y is not a single observation, but rather a function of t for t = 1, . . . , T . Then the
response is a curve, and predictions nearby, say for t and t + 1, can be dependent.

206
BAYESIAN ANALYSIS OF GENE EXPRESSION DATA
Let us assume that the response is
y(t; θ) = f (t; θ) + ϵ(t),
for t = 1, . . . , T , where f (t; θ) is a function, not necessarily smooth, of t and
θ, with ϵ(t) i.i.d. N(0, σ 2). Suppose further that a proper prior is speciﬁed on θ
and σ 2 as p(θ, σ 2), and given observations y(1), . . . , y(T ), the posterior density
is obtained.
Generating r = 1, . . . , R observations from the posterior, a (1 −α)100%
simultaneous credible envelope for the function f (θ) is found numerically, by
sorting the posterior evaluated at each θ(r), i.e. p(θ(r)|y), ranking these val-
ues from smallest to largest, and choosing curves f (1, . . . , T ; θ(r)) : p(θr|y) ∈
(p(θr|y)α/2, p(θr|y)1−α/2), i.e., such that the posterior evaluated at θ(r) is within
its numerical (1 −α)100% bounds. We call this a Bayesian credible envelope
(BCE) for the function f (θ). As the number of MCMC samples increases,
the BCE bounds will resolve with increasing clarity. Determining simultane-
ous theoretical conﬁdence envelopes for frequentist estimators of θ is much
more involved, requiring assumptions about f and the underlying random error
distribution for ϵ. Note that the above technique for deriving BCEs is robust
to normality and independence. Readers interested in frequentist function data
estimation are referred to Genovese and Wasserman (2004).
B.8.5
Proposal Updating
One of the most controversial and exciting areas of MCMC thinking involves
continuous updating of the proposal q(u, v). Recall that a good proposal density
can make a big difference, improving the efﬁciency and time to generate samples
from the posterior. Here we address the topic of proposal updating. Can one learn
to improve a proposal continuously? It would appear that information in the
Monte Carlo chain should help in generating better candidate proposals, based
on acceptance ratios, and other criteria. In general, there is no general formula
for updating proposal densities, and in practice many would doubt that such a
routine would ever converge. There are exceptions, and we turn to an important
one for multivariate posterior generation.
Consider multivariate observations Yi|θ, of dimension p, and suppose that the
joint prior for the v-dimensional parameter θ is speciﬁed as p(θ) ̸= v
j=1p(θj),
i.e. dependence. Ignoring posterior parameter dependence while updating can be
inefﬁcient. Imagine we are trying to ﬁll a mass stretched over an afﬁne hyper-
plane, in some set of subdimension v. Choosing a joint proposal that takes into
account dependence can avoid jumps outside of the afﬁne plane, and save time,
i.e. improve acceptance rates. In practice, though, determining the dependence can
be tricky. The following algorithm has been suggested for just such a situation.
Suppose that the parameter space is continuous.

BAYESIAN COMPUTATION TOOLS
207
1. Transform variables to lie on the real line,  →ϒ, and to stabilize vari-
ance.
2. Run Metropolis–Hastings and derive a preliminary estimate of the covari-
ance of υ ∈ϒ.
3. Apply MCMC with a normal proposal, using the covariance estimated
from the sample from step 2.
Comparing the acceptance rates between steps 2 and 3, there can be remark-
able improvements. See (Carlin and Louis, 2000).
B.9
Posterior Convergence Diagnostics
Recall that the Gibbs and Metropolis algorithms create Markov chains, which
under suitable conditions converge to the appropriate target (in our case posterior)
distribution. In MCMC theory, the ultimate goal is to establish that the chain will
reach its appropriate stationary distribution. Unfortunately, there are no general
results to guarantee the number of iterations required for convergence, or even
suitable evidence that by iteration N in the sequence, to within some tolerance,
the MCMC chain converges.
There are many diagnostic methods available for monitoring MCMC conver-
gence, many of which follow from two basic strategies:
1. Run one long chain, and compare sample statistics of interest, e.g. the
mean or range of quantiles, along ﬁxed windows.
2. Run many chains, from different starting values, and compare sample
statistics of interest between them, i.e. infer signiﬁcant variation between
the chains, relative to the variation within each chain.
There is no consensus on how to run MCMC chains or monitor convergence.
Nor is it likely that a general consensus will develop. There are many difﬁcul-
ties to consider in achieving convergence, such as the length of the chains, the
modality and covariance of the posterior, and regions of the support over which
much of the posterior is relatively ﬂat. In the case where the posterior distribu-
tion lies largely along an afﬁne hyperplane of the full dimension space, or the
parameter space is distributed nonuniformly, i.e. with discrete jumps, between
the dimensions, one can expect to encounter chains that mix slowly with con-
ventional updating tools. In some cases, a one-to-one transformation, if it exists,
among the parameters, can reduce drag, although in practice these problems can
be very difﬁcult to deal with, and to monitor convergence. Other issues to con-
sider with MCMC convergence include models that are overparameterized, or
full conditionals that do not admit a legitimate joint probability distribution. For
instance, it is trivial to assign conditional distributions (Zi|Zj) speciﬁed in such
as way that the covariance is asymmetric, a serious problem for statisticians and

208
BAYESIAN ANALYSIS OF GENE EXPRESSION DATA
probabilists, although less so for others using MCMC methods. We can disregard
such cases here, as we are interested in legitimate joint probability distribution,
but beware that there are many who insist that such models can perform well.
In theory, methods for monitoring convergence, including both graphical and
numeric, can be shown to work reasonably well for low-dimensional parameter
spaces. In practice, especially with high-dimensional parameter spaces, the reality
is that true convergence can never really be known. In fact, many approaches
to inferring convergence serve more to ease the mind of the analyst, providing
psychological comfort, rather than direct proof, which cannot be had, that a chain
has converged.
Suppose that one is interested in generating a sequence θ1, θ2, . . . from a
distribution p(θ|x). In Monte Carlo estimation the error can be partitioned into
two sources, that which arises from random moves between states, and that
which arises during the initial period during which the chain is updating, and
not yet converged. Suppose that

|pt(θ|x) −pt−1(θ|x)|dθ →0 as t →∞, and
also that we can approximate pt(θ|x) by ˆpt(θ|x), i.e. with noise. By the triangle
inequality,
	
|pt(θ|x) −pt−1(θ|x)|dθ ≤
	
| ˆpt(θ) −pt(θ)|dθ
+
	
|pt(θ|x) −p(θ|x)|dθ.
Both terms on the right must go to zero as t →∞for the sequence ˆpt(x)
to converge to p(x). Diagnostic methods cannot ‘prove’ overall convergence.
Diagnostic monitoring and checking are concerned with divergence between
ˆpt(θ|x) and ˆpt+k(θ|x) within a chain, or divergence between m different chains
ˆp1
t (θ|x), ˆp2
t (θ|x), . . . , ˆpm
t (θ|x), as a function of N, the number of iterations.
Unusual behavior in the chains is certainly a sign of problems, but lack of such
clues is not evidence to conclude convergence, especially when a posterior is
high-dimensional, or only known up to a normalizing constant. The chains may
be near convergence, although if little is known about the full posterior, it may
be that a seemingly well-behaved chain or collection of chains, for example, have
not toured the entire parameter space.
These issues, and many like them, prevent us from ever determining with
complete certainty whether or not a Monte Carlo chain has converged. In the
sections that follow, we turn to traditional approaches for monitoring MCMC
convergence, illustrating the affect of the proposal and offer some guidance for
Bayesian applications with high-throughput data.
B.10
MCMC Convergence and the Proposal
Thus far, we learned that the proposal density selection is very important to
achieve adequate acceptance rates. The proposal density is of course subjective.

BAYESIAN COMPUTATION TOOLS
209
Admittedly, choosing a good proposal is a black art to some, but we consider it
an acquired skill that Bayesians must master. There is much practical advice in
the literature, and we do not attempt to review it all here. Many good examples
can be found in other texts that offer a complete introduction to training MCMC
chains to achieve convergence.
Recall that, in the Metropolis algorithm, a candidate θ∗proposal is accepted
with probability equal to min(r, 1), where
r =
f (x|θ∗)π(θ∗)
L(x|θ(t−1))π(θ(t−1)),
setting θ(t) = θ∗, and rejected with probability 1 −min(r, 1), in which case we set
θ(t) = θ(t−1). Consider indexing a symmetric proposal qν(θ) by the variance ν of
the proposal density. Large values of ν will tend to proposal candidates θ∗further
from the current state of the chain θ(t−1), which can result in lower acceptance
rates. The last candidate to be accepted equals θ(t−1), i.e. offering information
about the relative probability of moving near this state. Moves that are too far
from accepted candidates can suggest transitions to regions of minimal posterior
mass. On the other hand, small values of ν tend to yield higher acceptance rates,
while providing very short moves between states. Such chains can take a very
long time to explore the entire parameter space, while suffering from severe
autocorrelation.
As a general rule of thumb, acceptance rates between 30% and 40% are
considered reasonable. Acceptance rates lower than this tell us that the chain is not
mixing well, an inefﬁcient use of our resources. Acceptance rates higher than 40%
might suggest high autocorrelation in the chains, in which case the observation
in the sequence cannot be taken as a random sample from the posterior.
Example B.9
We return to Example B.8, with bivariate normal data and unknown correlation
ρ. In this example, no mechanism exists to sample ρ directly. We are interested
in the properties of the Monte Carlo chains, as a function of starting values and
the variance of the proposal.
In Figure B.4 we display 100 iterations of four chains of the Metropolis
algorithm constraining the proposal α∗+ β∗= 100, corresponding to a relatively
moderate proposal variance. While the chains began at different starting values,
these tend to be converging in distribution over similar support of the parameter
space. Figure B.5 shows the results of four different chains, each with different
proposal variance, as measured by α∗+ β∗= c, with larger values of c corre-
sponding to smaller proposal variances. Figure B.5(a) has very high acceptance
ratios, although the chain appears to be wandering about, not really depicting a
random sample from the posterior. Figure B.5(d) shows a more moderate tem-
poral trend and less autocorrelation. The autocorrelation functions for each chain
in Figure B.5 are shown in Figure B.6. High autocorrelation can be reduced by

210
BAYESIAN ANALYSIS OF GENE EXPRESSION DATA
0
20
40
60
80
100
0.0
0.2
0.4
0.6
0.8
1.0
Iteration
r
Chain 1
Chain 2
Chain 3
Chain 4
Figure B.4
MCMC chain initiated from four different starting values: 0.20, 0.50,
0.70, and 0.90.
0
1000
2000
3000
4000
5000
0.0 0.2 0.4 0.6 0.8 1.0
0.0 0.2 0.4 0.6 0.8 1.0
(a)
Iteration
r
r
0.0 0.2 0.4 0.6 0.8 1.0
r
0.0 0.2 0.4 0.6 0.8 1.0
r
0
1000
2000
3000
4000
5000
(b)
Iteration
0
1000
2000
3000
4000
5000
(c)
Iteration
0
1000
2000
3000
4000
5000
(d)
Iteration
Figure B.5
MCMC chains each with different proposals for α∗+ β∗: (a) 700,
(b) 500, (c) 300, and (d) 100.

BAYESIAN COMPUTATION TOOLS
211
0
5
10
15
20
25
30
35
0.0 0.2 0.4 0.6 0.8 1.0
Lag
ACF
(a)
0
5
10
15
20
25
30
35
0.0 0.2 0.4 0.6 0.8 1.0
Lag
ACF
(b)
0
5
10
15
20
25
30
35
0.0 0.2 0.4 0.6 0.8 1.0
Lag
ACF
(c)
0
5
10
15
20
25
30
35
0.0 0.2 0.4 0.6 0.8 1.0
Lag
ACF
(d)
Figure B.6
Autocorrelation functions for the chains in Figure B.5.
a process called thinning, where only observations d lags apart in the sequence
are retained.
The results in the above example beg the question, has the chain converged?
Imagine if we had 10 000 such parameters. How could we monitor the con-
vergence of all of them? How many chains should we run, and for how many
iterations? How should we choose the starting values? In many situations, the
answers to these questions are limited by available resources, although there are
not necessarily right or wrong ways to perform MCMC. We turn to some general
diagnostic results in the next section.
B.10.1
Graphical Checks for MCMC Methods
Ironically, most MCMC diagnostic inference involves tools from our fundamental
frequentist arsenal to compare the functions of MCMC chains, along blocks of
the sequence, or across chains. Diagnostic plots can be very helpful for getting
a rough estimate of where burn-in is achieved. The trace plots shown in Figures
B.4–B.5 reveal that much longer chains must be realized before one should begin
to accept the observations as coming from the target, or stationary, distribution.
These plots reveal trends in the chain that are vitally important, such as drift, and
how well the chains are mixing, that is, exploring the parameter space, something
that statistics cannot tell us. For example, the analyst hardly ever wishes to

212
BAYESIAN ANALYSIS OF GENE EXPRESSION DATA
observe a scale parameter σ (t) for t = 1, . . . drifting off to inﬁnity, or zero,
although diagnosing such a problem early on can avoid countless headaches later,
as a single drift in one parameter can set off a chain reaction (no pun intended!).
Typically, trace plots display the observations in the chain, although it can
also be useful, speciﬁcally for functions of the sequence, to plot a running
statistic of the sequence, for a speciﬁed window width, such as the moving
average, or moving percentiles, e.g. the moving 5th and 95th percentiles. Much
thought has gone into considerations of chain length, but in practice this is a
function of the starting values, the proposal, and many other factors, that in gen-
eral complicate the choice of chain length. One proposal is to monitor the ratio
of the mean of a running statistic across chains, relative to the pooled running
statistic from all chains. This is the approach implemented in the WinBugs soft-
ware (http://www.mrc.bsu.cam.ac.uk/bugs) to monitor chains graphically using
the range of the posterior 80th central percentiles. Obviously this method has
limitations, as it is a graphical tool, and a univariate one at that. Other helpful
ﬁgures include kernel density plots and boxplots which can help to detect outliers
and other signs that a chain has drifted. Starting MCMC chains from different
starting values is a good idea, to check that the chains are mixing properly, and
moving to the same support.
B.10.2
Convergence Statistics
When has a Monte Carlo Chain converged to the stationary distribution? One of
the most cited and widely available statistics for comparing chains was proposed
by Gelman and Rubin (1992). The method is simple and quite intuitive. Suppose
that we are interested in a parameter θ. Many chains can be run, and we can
compare the overall variance between chains to the variance within the chains,
much like the standard approaches to frequentist model selection and inference,
i.e. the F-test procedure. The steps are as follows:
1. Choose K, the number of parallel chains to run, each from different starting
values. It is important that the chains be initially overdispersed with respect
to the true posterior, in order, inter alia, to minimize the chance of a chain
getting stuck in a region of high mass, that it cannot get out of early on.
2. Each chain is run for 2N iterations.
3. We compute the scaled reduction factor,

ˆR =
9
N −1
N
+ m + 1
mN
 B
W
 
df
df −2

,
where B/N is variance between the means from the m parallel chains, W
is the average of the m within-chain variances, and df is the degrees of
freedom of an approximating t density to the posterior distribution. As the
length of the chains N →∞then ˆR →1 evidence of good convergence.

BAYESIAN COMPUTATION TOOLS
213
In multivariate settings monitoring convergence is much more difﬁcult.
Rather than monitor each parameter, one could monitor, for instance, the
logarithm of the posterior evaluated over the full parameter set.
Geyer (1992) proposed a different approach, running one long chain to mon-
itor the variance of
ˆθn = 1
N
n

b=1
f (θ(b))
for N iterations. By the central limit theorem, under certain regularity conditions,
we have that
√n( ˆθn −Eθ)
σ
→N(0, 1).
This suggests that as the chain length increases, we should observe the sample
average of the sequence converging in distribution to normality. A complication
with this, of course, is that the unbiased estimation of σ 2 is unlikely in the
presence of autocorrelation. Let us consider dividing the chain into m chains of
equal size, that are approximately uncorrelated (this should be veriﬁed). We can
estimate σ 2/n by
ˆσ 2 =
1
m(m −1)
m

h=1
( ˆθh
n/m −ˆθn)2
where ˆθh
n/m, for h = 1, . . . , m, is the sample mean in the hth chain. A 95%
conﬁdence interval for E(θ) is
ˆθ ± tm−1,.025
√
ˆσ 2
Raftery and Lewis propose estimating σ 2 using the technique of thinning,
keeping every dth sample after burn-in. We agree with Raftery and Lewis that a
very sensible suggestion is to choose N and d large enough to estimate quantities
such as P (θ < c|y) to within tolerable limits, of course taking the goals of the
analysis into account.
B.10.3
MCMC in High-throughput Analysis
The illustrations that we have provided thus far serve to illustrate the complexity
of monitoring MCMC convergence. As demonstrated in Example B.9, monitor-
ing one-dimensional MCMC is complicated enough. Now imagine repeating the
process several thousand times. This is absurd and will not be done in prac-
tice. Nevertheless, we provide some reasonable advice for Bayesians performing
high-throughput data analysis. First of all, if you are considering a Bayesian anal-
ysis, try to avoid MCMC methods such as Metropolis altogether. If you insist

214
BAYESIAN ANALYSIS OF GENE EXPRESSION DATA
on a sophisticated analysis, consider Monte Carlo integration of other forms,
such as importance sampling. Or if feasible, attempt to rely on the Bayesian cen-
tral limit theorem; treating the likelihood as approximately normal-gamma can
greatly reduce the computational complexity. If it is not possible to further reduce
the computational sophistication, at least attempt to pose the computation algo-
rithm in a Gibbs framework. Other examples of simpliﬁcations that can greatly
ease computation costs include reasonable approximations to full conditionals,
approximations of one-to-one parameter transformation, and marginalizing over
nuisance parameters that are difﬁcult to sample indirectly. These devices can
greatly alleviate the computational difﬁculty, freeing precious time for other
stages of the analysis.
Parallel processing is making it possible to perform Monte Carlo sampling
at rates that were once unimaginable. As a practical step, the output of such
procedures must be carefully followed for signs of that the chains are healthy,
such as trends in dispersion parameters or mixture weights. If the dimensionality
is to high to monitor the chains one by one, consider spot checking, moving
averages, or Gelman-Rubin statistic or Raftery–Lewis statistics can be helpful.
Remember, the goal is to diagnose problems with convergence early on, since
proving convergence is unlikely. Readers interested in further exploration should
consult Cowles and Carlin (1996), Schervish and Carlin (1992), and Robert and
Casella (1999).
B.11
Summary
There is no right or wrong way to utilize the Bayesian computational tools
discussed in this appendix, although some methods are less efﬁcient than others,
or lead to poor approximations of posterior probabilities of events of interest. The
methods outlined are very powerful, but should be used thoughtfully, considering
such matters as acceptance ratios, the dimensionality of the data, and alternate
methods. For example, the Laplace method, and extensions, provide a useful
tool for making posterior statements, in situations where the posterior can be
approximated well and the parameter space is continuous. In many situations,
such as mixture modeling or hierarchical modeling, better alternative methods
involve updating procedures with indirect sampling such as Gibbs sampling and
Metropolis–Hastings, see Chapter 7. The basic outline provided in this chapter
is an introduction and a guide, rather than a comprehensive overview. Posterior
computation is an art, and we advise those interested to practice, starting with
simple examples.
There are no easy solutions or recipes for monitoring chains, and it is best in
practice to keep the process under control. Recall that if the goal is to generate
samples from p(θ|y) with Monte Carlo methods then we must at least consider:
(a) the dimensionality of the parameter space;

BAYESIAN COMPUTATION TOOLS
215
(b) whether the full posterior conditionals lead to a legitimate joint posterior
distribution;
(c) the length of the chains;
(d) when burn-in is achieved;
(e) autocorrelation in the chains;
(f) drift in the chains, i.e. to local modes.
For the novice, we suggest practicing with simulated examples before attempting
advanced modeling. For those considering advanced modeling, keep in mind
that monitoring convergence is a difﬁcult, and at some times frustrating, task,
although the rewards of mastering the art of performing MCMC sampling are
well worth the effort.


References
Akaike, H. (1974) A new look at statistical model identiﬁcation, IEEE Transactions on
Automatic Control, 19, 716–723.
Albert, J. and Chib, S. (1993) Bayesian analysis of binary and polychotomous response
data. Journal of the American Statistical Association, 88, 669–679.
Alizadeh, A., Eisen, M., Davis, R.E., Ma, C., Sabet, H., Tran, T., Powell, J.I., Yang, L.,
Marti, G.E., Moore, D.T., Hudson, J.R. Jr, Chan, W.C., Greiner, T., Weisenburger,
D., Armitage, J.O., Lossos, I., Levy, R., Botstein, D., Brown, P.O., and Staudt, L.M.
(1999) The lymphochip: a specialized cDNA microarray for the genomic-scale analysis
of gene expression in normal and malignant lymphocytes. Cold Spring Harbor Symp.
Quant. Biol., 64, 71–78.
Alizadeh, A.A., Ross, D.T., Perou, C.M., and van de Rijn M. (2001) Towards a novel
classiﬁcation of human malignancies based on gene expression patterns. J. Pathol.,
195(1), 41–52.
Allison, D.B., Gadbury, G.L., Heo, M., Fern´andez, J.R., Lee, C.-K., Prolla, T.A., and
Weindruch, R. (2002) A mixture model approach for the analysis of microarray gene
expression data. Computational Statistics & Data Analysis, 39(1), 1–20.
Allocco, D.J., Kohane, I.S., and Butte, A.J. (2004) Quantifying the relationship between
co-expression, co-regulation and gene function, BMC Bioinformatics, 5.
Alon, U., Barkai N., Notterman, D.A., Gish, K., Ybarra, S., Mack, D., and Levine, A.J.
(1999) Broad patterns of gene expression revealed by clustering analysis of tumor and
normal colon tissues probed by oligonucleotide arrays. Proc. Natl. Acad. Sci. USA,
96(12), 6745–6750.
Ambroise, C., and McLachlan G.J. (2002) Selection bias in gene extraction on the basis
of microarray gene-expression data. Proc. Natl. Acad. Sci. USA, 99(10), 6562–6566.
Androulakis, I., Yang, E., and Almon, R. (2007) Analysis of time-series gene expression
data: methods, challenges and opportunities. Annual Review of Biomedical Engineering,
9, 205–228.
Antoniak, C.E. (1974). Mixtures of Dirichlet processes with applications to Bayesian
nonparametric problems. Annals of Statistics, 2, 1152–1174.
Arratia, R., Barbour, A.D. and Tavar´e, S. (1992) Poisson process approximations for the
Ewens sampling formula. Annals of Applied Probability, 2, 519–535.
Arteaga-Salas, J.M., Zuzan, H., Langdon, W.B., Upton, G.J., and Harrison, A.P. (2008)
An overview of image-processing methods for Affymetrix GeneChips. Brieﬁngs in
Bioinformatics, 9(1), 25–33.
Bayesian Analysis of Gene Expression Data
B. Mallick, D. Gold, and V. Baladandayuthapani
2009 John Wiley & Sons, Ltd

218
REFERENCES
Aronszajn, N. (1950) Theory of reproducing kernels. Transactions of the American Math-
ematical Society, 68, 337–404.
Ashburner, M., Ball, C.A., Blake, J.A., Botstein, D., Butler, H., Cherry, J.M., Davis,
A.P., Dolinski, K., Dwight, S.S., Eppig, J.T., Harris, M.A., Hill, D.P., Issel-Tarver, L.,
Kasarskis, A., Lewis, S., Matese, J.C., Richardson, J.E., Ringwald, M., Rubin, G.M.,
and Sherlock, G. (2000) Gene ontology: tool for the uniﬁcation of biology. The Gene
Ontology Consortium. Nat. Genet., 25, 25–29.
Bae, K., and Mallick, B.M. (2004) Gene selection using a two-level hierarchical Bayesian
model. Bioinformatics. (18): 3423–3430.
Baggerly, K.A., Coombes, K.R., Hess, K.R., Stivers, D.N., Abruzzo, L.V., and Zhang,
W. (2001) Identifying differentially expressed genes in cDNA microarray experiments.
J. Comput. Biol., 8(6), 639–659.
Baladandayuthapani, V., Holmes, C.C., Mallick, B.K., and Carroll, R.J. (2006). Modeling
nonlinear gene interactions using Bayesian MARS. In K.A. Do, P. Mueller, and M.
Vannucci (eds), Bayesian Inference for Gene Expression and Proteomics. Cambridge:
Cambridge University Press.
Bansal, M. (2007) How to infer gene networks from protein proﬁles. Molecular System
Biology, 3, 1–10.
Bar-Joseph, Z. (2004) Analyzing time series gene expression data. Bioinformatics, 20,
2493–2503.
Bar-Joseph, Z., Demaine, E., Gifford, D., Hamel, A., Srebro, N., and Jaakkola, T. (2003)
k-ary clustering with optimal leaf ordering for gene expression data. Bioinformatics,
19, 1070–1078.
Bar-Joseph, Z., Farkash, S., Gifford, D., Simon, I., and Rosenfeld, R. (2004) Decon-
volving cell cycle expression data with complementary information. Bioinformatics
(Proceedings of ISMB) 20(Suppl. 1), I23–I30.
Barry, W.T., Nobel, A.B., and Wright, F.A (2005) Signiﬁcance analysis of functional cat-
egories in geneexpression studies: a structured permutation approach. Bioinformatics,
21, 1943–1949.
Beal, M.J., Falciani, F., Gharamani, Z., Rangel, C., and Wild, D.L. (2005). A Bayesian
approach to reconstructing genetic regulatory networks with hidden factors. Bioinfor-
matics, 21, 349–356.
Beaumont, M. and Rannala, B. (2004) The Bayesian revolution in genetics. Nat. Rev.
Genet., 2004, 251–261.
Benjamini, Y., and Hochberg, Y. (1995) Controlling the false discovery rate: a practical
and powerful approach to multiple testing. Journal of the Royal Statistical Society Series
B, 57(1), 289–300.
Berger, J. (1985) Statistical Decision Theory and Bayesian Analysis. New York:
Springer-Verlag.
Bernardo, J.M., and Smith, A.F.M. (1994) Bayesian Theory. New York: John Wiley &
Sons, Ltd.
Berry, D.A., and Hochberg, Y. (2001) Bayesian perspectives on multiple com-
parisons.
Journal of Statistical Planning and Inference,
82,
215–227,
DOI:
10.1016/S0378-3758(99)00044-0.
Bhardwaj, N. and Lu, H. (2005) Correlation between gene expression proﬁles
and protein–protein interactions within and across genomes. Bioinformatics, 21,
2730–2738.

REFERENCES
219
Bhattacharjee, A., Richards, W.G., Staunton, J., Li, C., Monti, S., Vasa, P., Ladd, C.,
Beheshti, J., Bueno, R., Gillette, M., Loda, M., Weber, G., Mark, E.J., Lander, E.S.,
Wong, W., Johnson, B.E., Golub, T.R., Sugarbaker, D.J., and Meyerson, M. (2001)
Classiﬁcation of human lung carcinomas by mRNA expression proﬁling reveals distinct
adenocarcinoma subclasses. Proc. Natl. Acad. Sci. USA, 98(24), 13790–13795.
Bickel, D.R. (2004) Error-rate and decision-theoretic methods of multiple testing: Which
genes have high objective probabilities of differential expression? Statistical Applica-
tions in Genetics and Molecular Biology, 3(1), 8.
Binder, D.A. (1978) Bayesian cluster analysis. Biometrika, 65(1), 31–38.
Binder, D.A. (1981) Approximations to Bayesian clustering rules I. Biometrika, 68(1),
275–285.
Blackwell, D. (1973) Discreteness of Ferguson selection. Annals of Statistics, 1, 356–
358.
Blackwell, D., and MacQueen, J.B. (1973). Ferguson distributions via Polya urn schemes.
Annals of Statistics, 1, 353–355.
Boettcher, S. and Dethlefsen, C. (2003) Deal: A package for learning Bayesian networks.
Journal of Statistical Software, 8(20).
Bolstad, B.M. (2006) Pre-processing Microarray Data in Fundamentals of Data Mining
for Genomics and Proteomics. Dubitzky W, Granzow M, Berrar DP (Eds.), Springer,
2006
Bolstad, B.M., Irizarry, R.A., Astrand, M., and Speed, T.P. (2003) A comparison of
normalization methods for high density oligonucleotide array data based on bias and
variance. Bioinformatics, 19(2), 185–193.
Box, G.E.P. (1980) Sampling inference, Bayes inference, and robustness in the advance-
ment of learning. In J.M. Bernardo, M.H. DeGroot, D.V. Lindley, and A.F.M. Smith
(eds), Bayesian Statistics, pp. 366–381. Oxford: Oxford University Press.
Brazma, A., Hingamp, P., Quakenbush, J. et al. (2001) Minimum Information about
Micorarray Experiment (MAIME) – towards standards for microarray data. Nature
Genetics, 29, 365–371.
Breiman L., Friedman, J.H., Olshen, R., and Stone, C.J. (1984). Classiﬁcation and Regres-
sion Trees. Belmont, CA: Wadsworth.
Brettschneider, J., Collin, F., Bolstad, B.M., and Speed, T.P. (2008) Quality assessment
for short oligonucleotide microarray data. Technometrics 50(3), 241–264.
Broet, P., Richardson, S., and Radvanyi, F. (2002) Bayesian hierarchical model for
identifying changes in gene expression from microarray experiments. Journal of Com-
putational Biology, 9, 671–683.
Bueno-de-Mesquita, J.M., van Harten, W.H., Retel, V.P., Van’t Veer, L.J., van Dam, F.S.,
Karsenberg, K., Douma, K.F., van Tinteren, H., Peterse, J.L., Wesseling, J., Wu, T.S.,
Atsma, D., Rutgers, E.J., Brink, G., Floore, A.N., Glas, A.M., Roumen, R.M., Bellot,
F.E., van Krimpen, C., Rodenhuis, S., van de Vijver, M.J., and Linn, S.C. (2007)
Use of 70-gene signature to predict prognosis of patients with node-negative breast
cancer: a prospective community-based feasibility study (RASTER). Lancet Oncol.,
8(12), 1079–1087.
Bush, C. and MacEachern, S. (1996) A semiparametric Bayesian model for randomised
block designs. Biometrika, 83, 275–285.

220
REFERENCES
Cardoso, F., Van’t Veer, L., Rutgers, E., Loi, S., Mook, S., and Piccart-Gebhart, M.J.
(2008) Clinical application of the 70-gene proﬁle: the MINDACT trial. J Clin Oncol.,
26(5), 729–735.
Carlin, B.P., and Louis, T.A. (2000) Bayes and Empirical Bayes Methods for Data Anal-
ysis, 2nd edition Boca Raton, FL: Chapman & Hall/CRC Press.
Chen, Y., Dougherty, E.R., and Bittner, M.L. (1997). Ratio-based decisions and the quan-
titative analysis of cDNA microarray images. Journ. Biomedical Optics, 4, 364–374.
Chen, J., and Sarkar, S.K. (2005) A bayesian determination of threshold for identifying
differentially expressed genes in microarray experiments. Statistics in Medicine, 25,
3174–3189.
Chib, S., and Greenberg, E. (1998). Analysis of multivariate probit models. Biometrika,
85, 347–361.
Chi, Y., Ibrahim, J., Bissahoyo, A., and Threadgill, D. (2007) Bayesian hierarchical mod-
eling for time course microarray experiments. Biometrics, 63, 496–504.
Chi, Z. (2007) Sample size and positive false discovery rate control for multiple testing,
Elec. Journ. of Stat., 1, 77–118.
Chipman, H., George, E.I., and McCulloch, R.E. (2001) The practical implementation of
Bayesian model selection (Pkg: p65-134) Model selection [Institute of Mathematical
Statistics lecture notes-monograph series 38, 65–116. Fountain Hills, AZ: IMS Press.
Churchill GA (2002) Fundamentals of experimental design for cDNA microarrays. Nature
Genetics, 32, 490–495.
Clyde, M., DeSimone, H., and Parmigiani, G. (1996) Prediction via orthogonalized model
mixing. Journal of the American Statistical Association, 91, 1197–1208.
Cooper, G., and Herskovitz, E. (1992) A Bayesian method for the induction of proba-
bilistic networks from data. Machine Learning, 9, 309–347.
Cowles, M.K., and Carlin, B.P. (1996) Markov chain Monte Carlo convergence diag-
nostics: a comparative review. Journal of the American Statistical Association, 91,
883–904.
Cox, D.R. (1972) Regression models and life tables. Journal of the Royal Statistical
Society Series B, 34, 187–220.
Craig, P., Goldstein, M., Seheult, A., and Smith, J. (1998) Constructing partial prior
speciﬁcations for models of complex physical systems. The Statistician, 47, 37–53.
Cristianini, N., and Shawe-Taylor, J. (2000) An Introduction to Support Vector Machines.
Cambridge: Cambridge University Press.
Curtis, R.K., Oresic, M., and Vidal-Puig, A. (2005) Pathways to the analysis of microarray
data. Trends in Biotechnology, 23, 429–435.
Dahl, D.B., and Newton, M.A. (2007). Multiple hypothesis testing by clustering treatment
effects. Journal of the American Statistical Association, 102, 517–526.
Dawid, A.P., and Lauritzen, S. (1995) Hyper Markov laws in the statistical analysis of
decomposable graphical models. Annals of Statistics, 21, 1272–1317.
de Finetti, B. (1930) Funzione caratteristica di un fenomeno aleatoria. Men. Acad. Naz.
Lincei., 4, 86–133.
de Finetti, B. (1937) La pr´evision: ses lois logiques, ses source subjectives. Annales de
l’Institut Henri Poincar´e, 7(1), 1–68.
Reprinted (1964) in H.E. Kyburg and H.E.
Smokler (eds), Studies in Subjective Probability. New York: John Wiley & Sons, Inc.

REFERENCES
221
de Finetti, B. (1963) La d´ecision et les probabilit´es. Rev. Roumine Math. Pures Appl., 7,
405–413.
de Finetti, B. (1964) Probabilit`a. Reprinted (1972) as: Conditional probabilities and deci-
sion theory. In Probability, Induction and Statistics: The Art of Guessing. New York:
John Wiley & Sons, Inc.
DeGroot M.H. (1970) Optimal statistical decisions. New York: McGraw-Hill.
Denison, D.G.T., Holmes, C.C., Mallick, B.K., and Smith, A.F.M. (2002) Bayesian Meth-
ods for Nonlinear Classiﬁcation and Regression. Chichester: John Wiley & Sons, Ltd.
DeRisi, J., Penland, L., Brown, P.O., Bittner, M.L., Meltzer, P.S., Ray, M., Chen, Y., Su,
Y.A., and Trent, J.M. (1996) Use of a cDNA microarray to analyse gene expression
patterns in human cancer. Nature Genetics, 14(4), 457–460.
DeRisi, J. L., Iyer, V. R., and Brown, P. O. (1997) Exploring the metabolic and genetic
control of gene expression on a genomic scale. Science, 278, 680–685.
Desmedt, C., Haibe-Kains, B., Wirapati, P., Buyse, M., Larsimont, D., Bontempi, G.,
Delorenzi, M., Piccart, M., and Sotiriou, C. (2008) Biological processes associated
with breast cancer clinical outcome depend on the molecular subtypes. Clin Cancer
Res., 14(16), 5158–5165.
Devroye
L.
(1986)
Non-Uniform
Random
Variate
Generation.
New
York:
Springer-Verlag.
Dey, D., Muller, P., and Sinha, D. (1998) Practical Nonparametric and Semiparametric
Bayesian Statistics. Berlin: Springer-Verlag.
Dey D. K. Ghosh S. and Mallick B. K. (2000). Generalized Linear Models: a Bayesian
Perspective. New York: Marcel Dekker.
Dhesi, G.S., and Jones, R.C. (1990) Asymptotic corrections to the Wigner semicircular
eigenvalue spectrum of a large real symmetric random matrix using the replica method.
J. Phys. A: Math. Gen, 23, 5577–5599. Printed in the UK.
Do, K.A., Mueller, P., and Tang, F. (2005) A nonparametric Bayesian mixture model for
gene expression. Applied Statistics, 54(3), 1–18.
Do K.A., Mueller P., and Vannucci M. (eds) (2006) Bayesian Inference for Gene Expres-
sion and Proteomics. Cambridge: Cambridge University Press.
Dobbin, K.K., Beer, D.G., Meyerson, M., Yeatman, T.J., Gerald, W.L., Jacobson, J.W.,
Conley, B., Buetow, K.H., Heiskanen, M., Simon, R.M., Minna, J.D., Girard, L., Misek,
D.E., Taylor, J.M., Hanash, S., Naoki, K., Hayes, D.N., Ladd-Acosta, C., Enkemann,
S.A., Viale, A., and Giordano, T.J. (2005) Interlaboratory comparability study of can-
cer gene expression analysis using oligonucleotide microarrays. Clin Cancer Res., 11,
565–572.
Dobra, A., Jones, B., Hans, C., Nevis, J. and west, M. (2004) Sparse graphical models
for exploring gene expression data. Journal of Multivariate Analysis, 90, 196–212.
Dougherty ER (2001) Small sample issues for microarray-based classiﬁcation. Comp
Funct Genomics., 2(1): 28–34.
Dudoit, S. and Fridlyand, J. Classiﬁcation in microarray experiments. (Analysis of
microarray experiments, Chapman & Hall/CRC, 2003, edited by T. P. Speed).
Dudoit S., Y. H. Yang, T. P. Speed, and M. J. Callow (2002). Statistical methods for
identifying differentially expressed genes in replicated cDNA microarray experiments.
Statistica Sinica, Vol. 12, No. 1, p. 111–139.

222
REFERENCES
Dudoit S., Shaffer, J.P. and Boldrick, J.C. (2003). Multiple hypothesis testing in microar-
ray experiments. STATSCI , Vol. 18, No. 1, p. 71–103.
Dudoit S., van der Laan M.J., and Pollard, K.S. (2004). Multiple testing. Part I. Single-step
procedures for control of general Type I error rates. Statistical Applications in Genetics
and Molecular Biology, Vol. 3, No. 1, Article 13.
Duggan, D.J., Bittner, M.L., Chen, Y., Meltzer, P.S., and Trent, J.M. (1999). Expression
proﬁling using cDNA microarrays. Nature Genetics, 21, 10–14.
Duncan, D.B. (1965) A bayesian approach to multiple comparisons. Technometrics, 7,
171–222.
Efron, B. and Tibshirani, R. (2002) Empirical Bayes methods and false discovery rates
for microarrays. Genetic Epidemiology, 23, 70–86.
Efron, B. and Tibshirani, R. (2006) On testing the signiﬁcance of sets of genes. Technical
Report, Standford University.
Efron, B., Tibshirani, R., Storey, J.D., and Tusher, V. (2001) Empirical Bayes analy-
sis of a microarray experiment. Journal of the American Statistical Association, 96,
1151–1160.
Eisen, M.B., Spellman, P.T., Brown, P.O., and Botstein, D. (1998) Cluster analysis
and display of genome-wide expression patterns. Proc. Natl. Acad. Sci. USA, 95(25),
14863–14868.
Escobar, M.D. (1994) Estimating normal means with a Dirichlet process prior. Journal
of the American Statistical Association, 89, 268–277.
Escobar, M.D. and West, M. (1995) Bayesian density estimation and inference using
mixtures. Journal of the American Statistical Association, 90, 577–588.
Everitt, B. (1993) Cluster Analysis. London: Edward Arnold.
Fan, C., Oh, D.S., Wessels, L., Weigelt, B., Nuyten, D.S., Nobel, A.B., Van’t Veer,
L.J., and Perou, C.M. (2006) Concordance among gene-expression-based predictors
for breast cancer. New England Journal of Medicine, 355(6), 560–569.
Fan, J., Tam, P., Woude, G.V., and Ren, Y. (2004) Normalization and analysis of cDNA
microarrays using within-array replications applied to neuroblastoma cell response to
a cytokine. Proc. Natl. Acad. Sci. USA, 101(5), 1135–1140.
Fare, T.L., Coffey, E.M., Dai, H., He, Y.D., Kessler, D.A., Kilian, K.A., Koch, J.E.,
LeProust, E., Marton, M.J., Meyer, M.R., Stoughton, R.B., Tokiwa, G.Y., and Wang,
Y. (2003) Effects of atmospheric ozone on microarray data quality. Anal. Chem., 75,
4672–4675.
Farmer, P., Bonnefoi, H., Becette, V., Tubiana-Hulin, M., Fumoleau, P., Larsimont, D.,
MacGrogan, G., Bergh, J., Cameron, D., Goldstein, D., Duss, S., Nicoulaz, A.-L.,
Brisken, C., Fiche, M., Delorenzi1, M., and Iggo R. (2005) Identiﬁcation of molecular
apocrine breast tumours by microarray analysis. Oncogene, 24, 4660–4671.
Ferguson, T.S. (1973) A Bayesian analysis of some nonparametric problems. Annals of
Statistics, 1, 209–230.
Fisher, R.A. (1922) On the interpretation of ξ 2 from contingency tables, and the calculation
of P. Journal of the Royal Statistical Society, 85(1), 87–94.
Fix, E., and Hodges, J.L. (1951) Discriminatory analysis-nonparametric discrimination:
consistency properties. US Air Force School of Aviation Medicine, Randolph Field,
TX.

REFERENCES
223
Fraley, C., and Raftery, A.E. (2002) Model-based clustering, discriminant analysis, and
density estimation. Journal of the American Statistical Association, 97(458), 611–631.
Freedman, D.A. (1963) On the asymptotic behavior of Bayes estimate in the discrete case.
Annals of Mathematical Statistics, 34, 1386–1403.
Friedman, J.H. (1991). Multivariate adaptive regression splines (with discussion). Annals
of Mathematical Statistics, 19, 1–141.
Friedman, J. H. (1997) On bias, variance, 0/1 – loss, and the curse-of-dimensionality.
Data Mining and Knowledge Discovery, 1(1), 55–77.
Friedman, N. (2004) Inferring cellular networks using probabilistic graphical models.
Science, 30, 799–805.
Friedman, N., and Koller, D. (2003) Being Bayesian about network structure: a Bayesian
approach to structure discovery in ayesian networks. Machine Learning, 50, 95–125.
Fr¨uhwirth-Schnatter, S. (2001) MCMC estimation of classical and dynamic switching and
mixture models. Journal of the American Statistical Association, 96, 194–209.
Fu, J., Jeffrey, S.S. (2007) Transcriptomic signatures in breast cancer. Mol. Biosyst., 3(7),
466–472.
Garthwaite, P.H., and Dickey, J.M. (1996) Quantifying and using expert opinion for
variable-selection problems in regression. Chemometrics and Intelligent Laboratory
Systems, 35, 1–26; discussion 27–43.
Gasch, A., Spellman, P., Kao, C., Carmel-Harel, O., Eisen, M., Storz, G., Botstein, D.
and Brown, P. (2000) Genomic expression programs in the response of yeast cells to
environmental changes. Mol. Biol. Cell, 11, 4241–4257.
Geiger, D. and Heckerman, D. (1997) A characterization of Dirichlet distributions through
local and global independence. Annals of Statistics, 25, 1344–1368.
Gelfand A. (2000) Gibbs sampling. Journal of the American Statistical Association, 95,
1300–1304.
Gelfand, A. (1996). Model determination using sampling-based methods. In W.R. Gilks,
S. Richardson, and D.J. Spiegelhalter (eds), Markov Chain Monte Carlo in Practice.
London: Chapman & Hall.
Gelfand A.E. and Smith A.F.M. (1990) Sampling-based approaches to calculating
marginal densities. Journal of the American Statistical Association, 85(410), 398–409.
Gelfand, A., Hills, S.E., Racine-Poon, A., and Smith, A.F.M. (1990) Illustration of
Bayesian inference in normal data models using Gibbs sampling. Journal of the Amer-
ican Statistical Association, 85, 972–985.
Geller, S.C., Gregg, J.P., Hagerman, P., and Rocke, D.M. (2003) Transformation and
normalization of oligonucleotide microarra data. Bioinformatics, 19(14), 1817–1823.
Gelman, A., and Rubin, D.B. (1992) Inference from iterative simulation using multiple
sequences. Stat. Sci., 7, 457–511.
Geman, S., and Geman, D. (1984) Stochastic relaxation, Gibbs distributions, and the
Bayesian restoration of images. IEEE Transactions on Pattern Analysis and Machine
Intelligence, 6, 721–741.
Genovese, C., and Wasserman, L. (2002) Operating characteristics and extensions of the
false discovery rate procedure. Journal of the Royal Statistical Society, Series B, 64(3),
499–517.
Genovese, C., and Wasserman, L. (2004) A stochastic process approach to false discovery
control. Annnals of Statistics, 32, 1035–1061.

224
REFERENCES
Gentleman, R.C., Carey, V.J., Bates, D.M., Bolstad, B., Dettling, M., Dudoit, S., Ellis, B.,
Gautier, L., Ge, Y., Gentry, J., Hornik, K., Hothorn, T., Huber, W., Iacus, S., Irizarry,
R., Leisch, F., Li, C., Maechler, M., Rossini, A.J., Sawitzki, G., Smith, C., Smyth, G.,
Tierney, L., Yang, J.Y., and Zhang, J. (2004) Bioconductor: open software development
for computational biology and bioinformatics. Genome Biol., 5(10), R80.
Geoman, J.J., van de Geer, S., de Kort, F., and van Houwelingen, H.C. (2004) A global
test for groups of genes: testing association with a clinical outcome. Bioinformatics,
20, 93–99.
George, E.I., and McCulloch, R.E. (1993) Variable selection via Gibbs sampling. Journal
of the American Statistical Association 88, 881–889.
George, E.I., and McCulloch, R.E. (1997) Approaches for Bayesian variable selection.
Statistica Sinica, 7, 339.
Geyer, C.J. (1992) Practical Markov chain Monte Carlo. Statistical Science, 7(4),
473–483.
Gilks, W.R., and Wild, P. (1992) Adaptive rejection sampling for Gibbs sampling. Applied
Statistics, 41, 337–348.
Gold, D.L., Wang, J., and Coombes, K.R. (2005) Inter-gene correlation on oligonucleotide
arrays: how much does normalization matter? American Journal of Pharmacogenomics,
5, 271–279.
Gold, D.L., Coombes, K.R., Wang, J. and Mallick, B. (2007) Enrichment analysis in
high-throughput genomics – accounting for dependency in the NULL. Brieﬁngs in
Bioinformatics, 8, 71–77.
Goldstein, H., Rasbash, J., Yang, M., Woodhouse, G., Pan, H., Nuttall, D., and Thomas, S.
(1993) A multilevel analysis of school examination results. Oxford Review of Education,
19(4), 425–433.
Golub, T.R., Slonim, D.K., Tamayo, P., Huard, C., Gaasenbeek, M., Mesirov, J.P., Coller,
H., Loh, M.L., Downing, J.R., Caligiuri, Bloomfoiel, C.D., and Lander E.S. (1999)
Molecular classiﬁcation of cancer: class discovery and class prediction by gene expres-
sion monitoring. Science, 286, 531–537.
Good, I.J. (1988) The interface between statistics and philosophy of science (with discus-
sion). Statistical Science, 3, 386–398.
Gottardo, R., Raftery, A.E., Yee Y.K., and Bumgarner, R.E. (2006) Bayesian robust infer-
ence for differential gene expression in microarrays with multiple samples. Biometrics,
62, 10–18.
Green, P.J. (1995). Reversible jump Markov chain Monte Carlo computation and Bayesian
model determination. Biometrika, 82, 711–732.
Guo, X., Qi, H., Verfaillie, C., and Pan, W. (2003). Statistical signiﬁcance analysis of
longitudinal gene expression data. Bioinformatics, 19, 1628–1635.
Haab, B.B., Dunham, M.J., and Brown, P.O. (2001) Protein microarrays for highly parallel
detection and quantitation of speciﬁc proteins and antibodies in complex solutions.
Genome Biology, 2(2), research 0004.1–0004.13.
Hastie, T., Tibshirani, R., Botstein, D., and Brown, P. (2001) Supervised harvesting of
expression trees. Genome Biology, 2, research 0003.1–0003.12.
Hastings W. (1970) Monte Carlo sampling methods using Markov chains and their appli-
cations. Biometrika, 57, 97–110.

REFERENCES
225
Heard, N., Holmes, C., Stephens, D., Hand, D. and Dimopoulos, G. (2005) A Bayesian
coclustering of Anopheles gene expression time series response to multiple immune
challenges. Proc. Natl. Acad. Sci. USA, 102, 16939–16944.
Heckerman, D., Geiger, D., and Chickering, D. (1995) Learning Bayesian networks: the
combination of knowledge and statistical data. Machine Learning, 20, 197–243.
Hedenfalk, I., Duggan, D., Chen, Y., Radmacher, M., Bittner, M., Simon, R., Meltzer,
P., Gusterson, B., Esteller, M., Kallioniemi, O.P., Wilfond, B., Borg, A., and Trent J.
(2001). Gene expression proﬁles in hereditary breast cancer. New England Journal of
Medicine, 344, 539–548.
Herbrich, R. (2002) Learning Kernel Classiﬁers. Cambridge, MA: MIT Press.
Hess, K.R., Zhang, W., Baggerly, K.A., Stivers, D.N., and Coombes, K.R. (2001) Microar-
rays: handling the deluge of data and extracting reliable information. Trends Biotech-
nol., 19(11), 463–468.
Hodges, J.S. (1998) Some algebra and geometry for hierarchical models, applied to diag-
nostics. Journal of the Royal Statistical Society, Series B, 60, 497–536.
Holmes, C.C., and Denison, D.G.T. (2003). Classiﬁcation with Bayesian MARS. Machine
Learning, 50, 159–173.
Holmes, C.C. and Held, K. (2006) Bayesian auxiliary variable models for binary and
polychotomous regression. Bayesian Analysis, 1(1), 145–168.
Holter, N.S., Maritan, A., Cieplak, M., Fedoroff, N., and Banavar, J. (2001) Dynamic
modeling of gene expression data. Proc. Natl. Acad. Sci. USA, 98, 1693–1698.
Huang, S. (1999). Gene expression proﬁling, genetic networks and cellular states; an
integrating concept for tumorigenesis and drug discovery. J. Mol. Med., 77, 469–480.
Huang, J., Wang, D., and Zhang, C. (2005) A two-way semilinear model for normalization
and analysis of cDNA microarray data. Journal of the American Statistical Association,
471, 814–829.
Hubert, L., and Arabie, P. (1985) Comparing partitions. Journal of Classﬁcation, 2,
193–218.
Husmeier, D. (2003) Sensitivity and speciﬁcity of inferring genetic regulatory interactions
from microarray experiments with dynamic bayesian networks. Bioinformatics, 19,
2271–2282.
Ibrahim, J.G., Chen, M.-H., and Gray, R.J. (2002) Bayesian models for gene expression
with DNA microarray data, Journal of the American Statistical Association, 97, 88–99.
Ideker,
T.,
Thorsson,
V.,
Siegel,
A.F.,
and
Hood,
L.R.
(2000).
Testing
for
differentially-expressed genes by maximum-likelihood analysis of microarray data.
Journal of Computational Biology, 6, 805–817.
Ideker, T., Thorsson, V., Ranish, J.A., Christmas, R., Buhler, J., Eng, J.K., Bumgarner,
R., Goodlett, D.R., Aebersold, R., and Hood, L. (2001) Integrated genomic and pro-
teomic analyses of a systematically perturbed metabolic network. Science, 292(5518),
929–934.
Irizarry, R.A., Bolstad, B.M., Collin, F., Cope, L.M., Hobbs, B., and Speed, T.P. (2003)
Summaries of Affymetrix GeneChip probe level data. Nucleic Acids Research, 31(4).
Irizarry, R.A., Warren, D., Spencer, F., Kim, I.F., Biswal, S., Frank, B.C., Gabrielson,
E., Garcia, J.G., Geoghegan, J., Germino, G., Grifﬁn, C., Hilmer, S.C., Hoffman, E.,
Jedlicka, A.E., Kawasaki, E., Mart´ınez-Murillo, F., Morsberger, L., Lee, H., Petersen,

226
REFERENCES
D., Quackenbush, J., Scott, A., Wilson, M., Yang, Y., Ye, S.Q., and Yu, W. (2005)
Multiple-laboratory comparison of microarray platforms. Nature Methods, 2, 345–349.
Ishwaran, H., and Rao, J.S. (2003). Detecting differentially expressed genes in microarrays
using Bayesian model selection. Journal of the American Statistical Association, 98,
438–455.
Ishwaran, H., and Rao, J.S. (2005). Spike and slab gene selection for multigroup microar-
ray data. Journal of the American Statistical Association, 100, 764–780.
Ishwaran H., Rao, J.S., and Kogalur, U.B. (2006). BAMarray: Java software for
Bayesian analysis of variance for microarray data. BMC Bioinformatics, 7: 59.
Jasra, A., Holmes, C.C., and Stephens, D.A. (2005) Markov chain Monte Carlo methods
and the label switching problem in Bayesian mixture modeling. Statist. Sci., 20(1),
50–67.
Jeffreys, H. (1946) An invariant form for the prior probability in estimation problems.
Proceedings of the Royal Society of London, Series A: Mathematical, Physical and
Engineering Sciences, 186, 453–461.
Jensen, F.V. (1996) An Introduction to Bayesian Networks. London: UCL Press.
Ji, Y., Tsui, K.-W., and Kim, K.M. (2006) A two-stage empirical Bayes method for
identifying differentially expressed genes. Computational Statistics & Data Analysis,
50, 3592–3604.
Jiang, Z., and Gentleman, R. (2007) Extensions to gene set enrichment. Bioinformatics,
23(3), 306–313.
Kadane, J. and Wolfson, L. (1998) Experience in elicitation. The Statistician, 47, 3–19.
Kalbﬂeisch, J. (1978) Non-parametric Bayesian analysis of survival time data. Journal of
the Royal Statistical Society, Series B, 40, 214–221.
Kalbﬂeisch, J., and Prentice, R. (1980) The Statistical Analysis of Failure Time Data. New
York: John Wiley & Sons, Inc.
Kass, R., Tierney, L., and Kadane J. (1988) Asymptotics in Bayesian computation. In J.M.
Bernardo, M.H. DeGroot, D.V. Lindley, and A.F.M. Smith (eds), Bayesian Statistics
3, pp. 263–278. Oxford: Oxford University Press.
Kass, R., Tierney, L., and Kadane J. (1989). Fully exponential Laplace approximations to
expectations and variances of nonpositive functions. Journal of the American Statistical
Association, 84, 710–716.
Kerr, M.K., and Churchill, G.A. (2001a) Experimental design for gene expression microar-
rays. Biostatistics, 2(2), 183–201.
Kerr, M.K., and Chruchill, G.A. (2001b) Statistical design and the analysis of gene expres-
sion microarray data. Genetics Research, 77, 123–128.
Kerr, M.K., Martin, M., and Churchill, G.A. (2000) Analysis of variance for gene expres-
sion microarray data. Journal of Computational Biology, 7(6), 819–837.
Kim, S., Imoto, S., and Miyano, S. (2003) Inferring gene networks from time series
microarray data using dynamic Bayesian networks. Brieﬁngs in Bioinformatics, 4,
228–235.
Kim, S.-Y., and Volsky, D.J. (2005) PAGE: Parametric Analysis of Gene Set Enrichment.
BMC Bioinformatics, 6, 144.
Klebanov, L., Glazko, G., Salzman, P., Yakovlev, A., and Xiao, Y. (2007) A multivari-
ate extension of the gene set enrichment analysis. J. Bioinform. Comput. Biol., 5(5),
1139–1153.

REFERENCES
227
Klevecz, R.R., Bolen, J., Forrest, G., and Murray D.B. (2004). A genomewide oscillation
in transcription gates DNA replication and cell cycle. Proc. Natl. Acad. Sci. USA, 101,
1200–1205.
Koscielny S (2008) Critical review of microarray-based prognostic tests and trials in breast
cancer. Curr. Opin. Obstet. Gynecol., 20(1), 47–50.
Kooperberg, C., Bose, S., and Stone, C. J. (1997). Polychotomous regression. Journal of
the American Statistical Association, 93, 117–127.
Lander, E.S., and Weinberg, R.A. (2000). Journey to the center of biology. Science, 287,
1777–1782.
Larranaga, P., Kuijpers, C., Murga, R., and Yurramendi, Y. (1996) Learning Bayesian
network structures by searching for the best ordering with genetic algorithms. IEEE
Transactions on Pattern Analysis and Machine Intelligence, 26, 487–493.
Lauritzen, S. L. (1996). Graphical Models. Oxford: Oxford University Press.
Lee, K., and Mallick, B. (2004) Bayesian methods for variable selection in survival models
with application to DNA microarray data. Sankhya, 66, 756–778.
Lee, K.Y., Sha N., Doughetry, E.R., Vannucci, M., and Mallick, B.K. (2003). Gene
selection: a Bayesian variable selection approach. Bioinformatics, 19, 90–97.
Lee, M.-L.T. (2004) Analysis of Microarray Gene Expression Data. Boston: Kluwer Aca-
demic.
Lewin, A., Bochkina, N., and Richardson, S. (2007). Fully Bayesian mixture model for
differential gene expression: Simulations and model checks. Statistical Applications in
Genetics and Molecular Biology, 6.
Li, C., and Wong, W.H. (2001) Model-based analysis of oligonucleotide arrays: model
validation, design issues and standard error application. Genome Biology. 2(8), research
0032.I–0032.II
Li, H., and Gui, J. (2004) Partial Cox regression analysis for high-dimensional micro-
array gene expression data. Bioinformatics, 20, 208–215.
Li, Y., Campbell, C., and Tipping, M. (2002). Bayesian automatic relevance determination
algorithms for classifying gene expression data. Bioinformatics, 18, 1332–1339.
Liao, J.C., Boscolo, R., Yang, Y.-L., Tran, L.M., Sabatti, C., and Roychowdhury V.P.
(2003) Network component analysis: Reconstruction of regulatory signals in biological
systems. Proc. Natl. Acad. Sci. USA, 100, 15522–15527.
Lindley, D.V. (1957) A statistical paradox. Biometrika, 45, 533–534.
Lindley, D.V., and Smith, A.F.M. (1972) Bayes estimates for the linear model (with
discussion). Journal of the Royal Statistical Society, Series B, 34, 1–41.
Lu, Y., Liu, P.Y., Xiao, P., and Deng, H.W. (2005) Hotelling’s T 2 multivariate pro-
ﬁling for detecting differential expression in microarrays. Bioinformatics, 21(14),
3105–3113.
Luan, Y., and Li, H. (2004). Model-based methods for identifying periodically expressed
genes based on time course microarraygene expression data. Bioinformatics, 20, 332–
339.
MacEachern, S. (1994) Estimating normal means with a conjugate style Dirichlet process
prior. Communications in Statistics, B, 23, 727–741.
Mallick, B.K., Ghosh, D., and Ghosh, M. (2005) Bayesian classiﬁcation of tumors using
gene expression data. Journal of the Royal Statistical Society, Series B, 67, 219–234.

228
REFERENCES
Ma, S., Song, X., and Huang, J. (2007) Supervised group lasso with applications to
microarray data analysis. BMC Bioinformatics, 8, 60.
Madigan, D., and Raftery, A. (1994) Model selection and accounting for model uncer-
tainty in graphical models using Occam’s window. Journal of the American Statistical
Association, 89, 1535–1546.
Mardia, K.V., Kent, J.T., and Bibby, J.M. (1979) Multivariate Analysis. London: Aca-
demic Press.
MAQC Consortium (2006) The MicroArray Quality Control (MAQC) project shows inter-
and intraplatform reproducibility of gene expression measurements. Nature Biotech.,
24, 1151–1161.
McCall, M.N., and Irizarry, R.A. (2008) Consolidated strategy for the analysis of microar-
ray spike-in data. Nucleic Acids Research, 36(17), e108.
McCullagh, P., and Nelder, J.A. (1989) Generalized Linear Models, 2nd edition. London:
Chapman & Hall.
Medvedovic, M., and Sivaganesan, S. (2002) Bayesian inﬁnite mixture model-based clus-
tering of gene expression proﬁles. Bioinformatics, 18, 1194–1206.
Mestl, T., Plahte, E., and Omholt, S. (1995). A mathematical framework for describing
and analyzing gene regulatory network. Journal of Theoretical Biology, 176, 291–300.
Metropolis, N., Rosenbluth, A., Rosenbluth, M., Teller, A.H., and Teller, E. (1953)
Equations of state calculations by fast computing machines. Journal of Chemical
Physics, 21, 1087–1091.
Millikan, R.C., Newman, B., Tse, C.K., Moorman, P.G., Conway, K., Dressler, L.G.,
Smith, L.V., Labbok, M.H., Geradts, J., Bensen, J.T., Jackson, S., Nyante, S., Livasy,
C., Carey, L., Earp, H.S., Perou, C.M. (2007) Epidemiology of basal-like breast cancer.
Breast Cancer Res Treat., 109(1), 123–139.
Mitchell, T.J., and Beauchamp, J.J. (1988). Bayesian variable selection in linear regres-
sion. Journal of the American Statistical Association, 83, 1023–1036.
Moloshok, T.D., Klevecz, R.R., Grant, J.D., Manion, F.J., Speier IV, W.F., and Ochs,
M.F. (2002) Application of Bayesian decomposition for analyzing microarray data.
Bioinformatics, 18, 566–575.
Mootha, V.K., Lindgren, C.M., Eriksson, K.F., Subramanian, A., Sihag, S., Lehar, J.,
Puigserver, P., Carlsson, E., Ridderstrale, M., Laurila, E., Houstis, N., Daly, M.J.,
Patterson, N., Mesirov, J.P., Golub, T.R., Tamayo, P., Spiegelman, B., Lander, E.S.,
Hirschhorn, J.N., Altshuler, D., and Groop, L.C. (2003) PGC-1alpha-responsive genes
involved in oxidative phosphorylation are coordinately downregulated in human dia-
betes. Nature Genetics, 34, 267–273.
Morris, C.N. (1983) Natural exponential families with quadratic variance functions: sta-
tistical theory. Annals of Statistics, 11, 515–529.
Muller, P., Parmigiani, G., Robert, C., and Rousseau, J. (2004) Optimal sample size
for multiple testing: the case of gene expression microarray. Journal of the American
Statistical Association, 99, 990–1001.
Murphy, K., and Mian, S. (1999). Modelling gene expression data using dynamic Bayesian
networks. Technical report. Computer Science Division, University of California,
Berkeley.
Nagaraj, V.H., O’Flanagan, R.A., Bruning, A.R., Mathias, J.R., Vershon, A.K., and Sen-
gupta, A.M. (2004) Combined analysis of expression data and transcription factor
binding sites in the yeast genome. BMC Genomics, 5(1), 59.

REFERENCES
229
Nau, G., Richmond, J., Schlesinger, A., Jennings, E., Lander, E., and Young, R. (2002)
Human macrophage activation programs induced by bacterial pathogens. Proc. Natl.
Acad. Sci. USA, 99, 1503–1508.
Neal, R. (1992) Bayesian mixture modelling. In Proceedings of the Workshop on Maximum
Entropy and Bayesian Methods of Statistical Analysis, 11, 197–211.
Neal, R. (2000) Markov chain methods for Dirichlet process mixture models. Journal of
Computational and Graphical Statistics, 9, 249–265.
Newton M.A., Kendziorski C.M., Richmond, C.S., Blattner, F.R., and Tsui, K.W. (2001).
On differential variability of expression ratios: Improving statistical inference about
gene expression changes from microarray data. Journal of Computational Biology, 8,
37–52.
Newton, M.A., Quintana, F.A., den Boon, J.A., Sengupta, S., and Ahlquist, P. (2007).
Random-set methods identify distinct aspects of the enrichment signal in gene-set
analysis. Annals of Applied Statistics, 1, 85–106.
Nguyen, D., and Rocke, D. (2002) Partial least squares proportional hazard regression for
application to DNA microarray survival data. Bioinformatics, 18, 1625–1632.
Nguyen, D., Arpat, A., Wang, N., and Carroll, R. (2002) DNA microarray experiments:
biological and technological aspects. Biometrics, 58, 701–717.
O’Hagan, A. (1998) Eliciting expert beliefs in substantial practical applications, The Statis-
tician, 47, 21–35.
O’Hagan, A. and Forster, J.J. (2004) Bayesian Inference, 2nd edition. London: Arnold.
Olson, J. (2004) Application of microarray proﬁling to clinical trials in cancer. Surgery,
136(3), 519–523.
Ong, I., Glasner, J., and Page, D. (2002) Modelling regulatory pathways in E. coli from
time series expression proﬁles. Bioinformatics, 18(Suppl. 1), 241–248.
Pan, W. (2006) Incorporating gene functional annotations in detecting differential gene
expression. Journal of the Royal Statistical Society, Series C - Applied Statistics, 55,
301–316.
Panda, S., Antoch, M., Miller, B., Su, A., Schook, A., Straume, M., Schultz, P., Kay, S.,
Takahashi, J., and Hogenesch, J. (2002) Coordinated transcription of key pathways in
the mouse by the circadian clock. Cell, 109, 307–320.
Park, T., Yi, A.G., Lee, S. Yoo, D.H., Ahn, J.I., and Lee, Y.S. (2003). Statistical tests
for identifying differentially expressed genes in time-course microarray experiments.
Bioinformatics, 19, 694–703.
Park, P.J., Pagano, M., and Bonetti, M. (2001) A nonparametric scoring algorithm for iden-
tifying informative genes from microarray data. Paciﬁc Symposium on Biocomputing,
6, 52–63.
Parmigiani, G., Garrett, E.S., Anbazhagan, R., and Gabrielson, E. (2002). A statistical
framework for expression-based molecular classiﬁcation in cancer (with discussion).
Journal of the Royal Statistical Society, Series B, 64, 717–736.
Parmigiani, G., Garrett, E.S., Irizarry, R.A., and Zeger S. L. (2003). The Analysis of Gene
Expression Data: Methods and Software. New York: Springer-Verlag.
Parzen, E. (1970) Statistical inference on time series by rkhs methods. In R. Pyke (ed.),
Proc. 12th Bienn. Sem., pp. 1–37. Montreal: Canadian Mathematical Congress.
Pearl, J. (1988) Probabilistic Reasoning in Intelligent Systems: Networks of Plausible
Inference. San Francisco: Morgan Kaufmann.

230
REFERENCES
Perrin, B.E., Ralaivola, L., Mazurie, A., Bottani, S., Mallet, J., and d’Alch´e-Buc, F.
(2003) Gene networks inference using dynamic Bayesian networks. Bioinformatics,
19, II138–II148.
Pomeroy, S.L., Tamayo, P., Gaasenbeek, M. et al. (2002). Prediction of central ner-
vous system embryonal tumor outcome based on gene expression. Nature, 415(24),
436–442.
Pontil, M., Evgeniou, T., and Poggio, T. (2000) Regularization networks and support
vector machines. Adv. Comput. Math., 13, 1–50.
Pounds, S., and Morris, S.W. (2003) Estimating the occurrence of false positives and
false negatives in microarray studies by approximating and partitioning the empirical
distribution of p-values. Bioinformatics, 19, 1236–1242.
Pounds, S. and Cheng, C. (2006) Robust estimation of the false discovery rate. Bioinfor-
matics, 22(16), 1979–1987.
Press,
S.J.
(2003)
Subjective and Objective Bayesian Statistics.
Hoboken,
NJ:
Wiley-Interscience.
Pusztai, L., and Hess, K.R. (2004) Clinical trial design for microarray predictive marker
discovery and assessment. Ann. Oncol., 15(12), 1731–1737.
Qian, J., Dolled-Filhart, M., Lin, J., Yu, H., and Gerstein, M. (2001) Beyond synexpression
relationships: local clustering of time-shifted and inverted gene expression proﬁles
identiﬁes new, biologically relevant interactions. Journal of Molecular Biology, 314(5),
1053–1066.
Quintana, F.A., and Iglesias, P.L. (2003) Bayesian clustering and product partition models.
Journal of the Royal Statistical Society, Series B, 65, 557–574.
Raftery, A.E., Madigan, D., and Hoeting, J. A. (1997) Bayesian model averaging for linear
regression models. Journal of the American Statistical Association, 92, 179–191.
Ramaswamy, S. and Golub, T.R. (2002) DNA microarrays in clinical oncology. Journal
of Clinical Oncology, 20(7), 1932–1941.
Ramaswamy, S., Tamayo, P., Rifkin, R., Mukherjee, S., Yeang, C.H., Angelo, M., Ladd,
C., Reich, M., Latulippe, E., Mesirov, J.P., Poggio, T., Gerald, W., Loda, M., Lander,
E.S., and Golub, T.R. (2001) Multiclass cancer diagnosis using tumor gene expression
signatures. Proc. Natl. Acad. Sci. USA, 98(26), 15149–15154.
Ramaswamy, S., Ross, K.N., Lander, E.S., and Golub, T.R. (2003) A molecular signature
of metastasis in primary solid tumors. Nature Genetics, 33(1), 49–54.
Ramoni, M., Sebastiani, P., and Kohane, I. (2002) Cluster analysis of gene expression
dynamics. Proc. Natl. Acad. Sci. USA, 99, 9121–9126.
Ramsay, G. (1998) DNA chips: state-of-the art. Nature Biotechnology, 16(1), 40–44.
Rand, W.M. (1971) Objective criteria for the evaluation of clustering methods. Journal
of the American Statistical Association, 66, 846–850.
Ray, S., and Mallick, B.K. (2006) Functional clustering by Bayesian wavelet methods.
Journal of the Royal Statistical Society, Series B, 68, 305–332.
Reverter, A., Barris, W., McWilliam, S, Byrne, K.A., Wang, Y.H., Tan, S.H., Hudson, N.,
and Dalrymple, B.P. (2005) Validation of alternative methods of data normalization in
gnee co-expression studies. Bioinformatics, 21(7), 1112–1120.
Richardson, A.M., Woodson, K., Wang, Y., Rodriguez-Canales, J., Erickson, H.S., Tan-
grea, M.A., Novakovic, K., Gonzalez, S., Velasco, A., Kawasaki, E.S., Emmert-Buck,

REFERENCES
231
M.R., Chuaqui, R.F., and Player, A. (2007) Global expression analysis of prostate
cancer-associated stroma and epithelia. Diagn Mol Pathol., 16(4), 189–197.
Richardson, S., and Green, P.J. (1997). On Bayesian analysis of mixtures with an unknown
number of components (with discussion). Journal of the Royal Statistical Society, Series
B, 59, 731–792.
Rigby, R.A. (1997) Bayesian discrimination between two multivariate normal popula-
tions with equal covariance matrices. Journal of the American Statistical Association,
92(439), 1151–1154.
Ripley, B.D. (1987) Stochastic Simulation. New York: John Wiley & Sons, Inc.
Ripley, B.D. (1996). Pattern Recognition and Neural Networks. New York: Cambridge
University Press.
Robert, C.P. (1995) Simulation of truncated normal variables. Statistics and Computing,
5, 121–125.
Robert, C.P. and Casella, G. (1999). Monte Carlo Statistical Methods. New York:
Springer-Verlag.
Rosenblatt F. (1962) Principles of Neurodynamics. New York: Spartan.
Rosenwald, A. et al. (2002) The use of molecular proﬁling to predict survival after
chemotherapy for diffuse large B-cell lymphoma. New England Journal of Medicine,
346, 1937–1946.
Savage, L.J. (1972) The Foundations of Statistics. New York: Dover.
Schadt, E.E., Li, Ch., Ellis, B., and Wong, W.H. (2001) Feature extraction and normal-
ization algorithms for high-density oligonucleotide gene expression array data. Journal
of Cellular Biochemistry, Supplement 37, 120–125.
Schaffer, J., and Strimmer, K. (2005) An emepirical Bayes approach to inferring
large-scale gene association networks. Bioinformatics, 21, 754–764.
Scharpf, R.B., Lacobuzio-Donahue, C.A., Sneddon, J.B., and Parmigiani, G. (2006) When
should one subtract background ﬂuorescence in 2-color microarrays? Biostatistcs, 8(4),
697–707.
Schena, M., Shalon, D., Davis, R., and Brown, P. (1995) Quantitative monitoring of gene
expression patterns with a complementary DNA microarray. Science, 270, 467–470.
Schena, M., Heller, R.A., Theriault, T.P., Konrad, K., Lachenmeier, E., and Davis,
R.W. (1998) Microarrays: biotechnology’s discovery platform for functional genomics.
Trends Biotechnol., 16(7), 301–306.
Schervish, M.J., and Carlin, B.P. (1992) On the convergence of successive substitution
sampling. Journal of Computational and Graphical Statistics, 1, 111–127.
Schliep, A., Schonhuth, A., and Steinhoff, C. (2003) Using hidden Markov models to
analyze gene expression time course data. Bioinformatics, 19, I264–I272.
Sch¨olkopf, B., and Smola, A. (2002) Learning with Kernels. Cambridge, MA: MIT Press.
Schwarz, G. (1978) Estimating the dimension of a model. Annals of Statistics, 6,
461–464.
Sebastiani, P., Gussoni, E., Kohane, I., and Ramoni, M. (2003) Statistical challenges in
functional genomics (with discussion). Statistical Science, 18, 33–70.
Sebastiani, P., Abad-Grau, M., and Ramoni, M. (2005) Bayesian networks. In O. Mai-
mon and L. Rokach (eds), The Data Mining and Knowledge Discovery Handbook, pp.
193–230. New York: Springer-Verlag.

232
REFERENCES
Segal, E., Shapira, M., Regev, A., Koller, D., and Friedman, N. (2003) Module net-
works: identifying regulatory modules and their condition-speciﬁc regulators from gene
expression data. Nature Genetics, 34, 166–176.
Sethuraman, J. (1994) A constructive deﬁnition of Dirichlet priors. Statistica Sinica, 4,
639–650.
Sethuraman, J., and Tiwari, R. (1982) Convergence of Dirichlet measures and the inter-
pretation of their paramegters. In S.S. Gupta and J.O. Berger (eds), Statistical Decision
theory and related topics, III , pp. 305–315. New York: Academic Press.
Sha, N., Tadesse, M.G., and Vannucci, M. (2006). Bayesian variable selection for the anal-
ysis of microarray data with consored outcome. Bioinformatics, 22(18), 2262–2268.
Shalon, D., Smith, S.J., and Brown, P.O. (1996) A DNA microarray system for analyz-
ing complex DNA samples using two-color ﬂuorescent probe hybridization. Genome
Research, 6(7), 639–645.
Shipp, M.A., Ross, K.N., Tamayo, P., Weng, A.P., Kutok, J.L., Aguiar, R.C., Gaasenbeek,
M., Angelo, M., Reich, M., Pinkus, G.S., Ray, T.S., Koval, M.A., Last, K.W., Norton,
A., Lister, T.A., Mesirov, J., Neuberg, D.S., Lander, E.S., Aster, J.C., and Golub, T.R.
(2002) Diffuse large B-cell lymphoma outcome prediction by gene-expression proﬁling
and supervised machine learning. Nat. Med., 8(1), 68–74.
Shoemaker, J., Painter, I., and Weir, B. (1999) Bayesian statistics in genetics, a guide for
the uninitiated. Trends in Genetics, 15, 354–358.
Silverman, B.W. (1986) Density Estimation for Statistics and Data Analysis (Monographs
on Statistics and Applied Probability), 1st edition. Chapman & Hall/CRC.
Singh, M., and Valtorta, M. (1995) Construction of Bayesian network structures from data:
A brief survey and efﬁcient algorithm. International Journal of Approximate Reasoning,
12, 111–131.
Smith, M., and Kohn, R. (1996) Nonparametric regression using Bayesian variable selec-
tion. Journal of Econometrics, 75, 317–344.
Sollich, P. (2001) Bayesian methods for support vector machines: evidence and predictive
class probabilities. Machine Learning, 46, 21–52.
Specht, D. F. (1990). Probabilistic neural networks. Neur. Networks, 3, 109–118.
Speed, T. (ed.) (2003) Statistical Analysis of Gene Expression Microarray Data. Boca
Raton, FL: Chapman & Hall/CRC.
Spiegelhalter, D., and Lauritzen, S. (1990) Sequential updating of conditional probabilities
on directed graphical structures. Networks, 20, 157–224.
Spirtes, P., Glymour, C., and Scheines, R. (1993) Causation, Prediction and Search. New
York: Springer-Verlag.
Spurgers, K.B., Gold, D.L., Coombes, K.R., Bohnenstiehl, N.L., Mullins, B., Meyn, R.E.,
Logothetis, C.J., and McDonnell, T.J. (2006) Identiﬁcation of cell cycle regulatory
genes as principal targets of p53-mediated transcriptional repression. J. Biol. Chem.,
281(35), 25134–25142.
Storey, J.D. (2002) A direct approach to false discovery rates. Journal of the Royal Sta-
tistical Society, Series B, 64, 479–498.

REFERENCES
233
Storey, J.D. (2003) The positive false discovery rate: a Bayesian interpretation and the
q-value. Annals of Statistics, 31(6), 2013–2035.
Storey J.D., Taylor J.E., and Siegmund D. (2004) Strong control, conservative point esti-
mation, and simultaneous conservative consistency of false discovery rates: A uniﬁed
approach. Journal of the Royal Statistical Society, Series B, 66, 187–205.
Storey, J., Xiao, W., Leek, J.T., Dai, J.Y., Tompkins, R.G., and Davis, R.W. (2005).
Signiﬁcance analysis of time course microarrayexp eriments. Proc. Natl. Acad. Sci.
USA, 102, 12837–12842.
Storch, K., Lipan, O., Leykin, I., Viswanathan, N., Davis, F., Wong, W. and Weitz, C.
(2002) Extensive and divergent circadian gene expression in liver and heart. Nature,
418, 78–83.
Stuart, R.O., Wachsman, W., Berry, C.C., Wang-Rodriguez, J., Wasserman, L., Kla-
cansky, I., Masys, D., Arden, K., Goodison, S., McClelland, M., Wang, Y.,
Sawyers, A., Kalcheva, I., Tarin, D., and Mercola, D. (2004) In silico dissection of
cell-type-associated patterns of gene expression in prostate cancer. Proc. Natl. Acad.
Sci. USA, 101(2), 615–620.
Subramanian, A., Kuehn, H., Gould, J., Tamayo, P., and Mesirov, J.P. (2007) GSEA-P:
a desktop application for gene set enrichment analysis. Bioinformatics, 23(23),
3251–3253.
Subramanian, A., Tamayo, P., Mootha, V.K., Mukherjee, S., Ebert, B.L., Gillette, M.A.,
Paulovich, A., Pomeroy, S.L., Golub, T.R., Lander, E.S., and Mesirov, J.P. (2005) Gene
set enrichment analysis: a knowledge-based approach for interpreting genome-wide
expression proﬁles. Proc. Natl. Acad. Sci. USA, 102, 15545–15550.
Tai, Y.C., and Speed, T.P. (2006). A multivariate empirical Bayes statistic for replicated
microarray time course data. Annals of Statistics, 34(5), 2387–2412.
Tamada, Y., Kim, S.Y., Bannai, H., Imoto, S., Tashiro, K., Kuhara, S., and Miyano, S.
(2003) Estimating gene networks from gene expression data by combining Bayesian
network model with promoter element detection. Bioinformatics, 19, 227–236.
Tanner, M. and Wong, W. (1987) The calculation of posterior distributions by data aug-
mentation, Journal of the American Statistical Association, 82, 528–540.
Thompson, K.L., and Pine, P,S. (2009) Comparison of the diagnostic performance of
human whole genome microarrays using mixed-tissue RNA reference samples. Toxi-
cology Letters, 186(1), 58–61.
Tibshirani, R. (1996) Regression shrinkage and selection via the lasso. Journal of the
Royal Statistical Society, Series B, 58, 267–288.
Tierny L and Kadane JB (1986) Accurate approximations for posterior moments and
marginal densities. Journal of the American Statistical Association, 81, 82–86.
T¨ozeren, A. and Byers, S. (2004) New Biology for Engineers and Computer Scientists.
Upper Saddle River, NJ: Pearson Prentice Hall.
Tusher, V.G., Tibshirani, R., and Chu, G. (2001) Signiﬁcance analysis of microar-
rays applied to the ionizing radiation response. Proc. Natl. Acad. Sci. USA, 98(9),
5116–5121.
van der Laan, M., and Bryan, J. (2001) Gene expression analysis with the parametric
bootstrap. Biostatistics, 2, 445–461.
Vapnik, V.N. (2000) The Nature of Statistical Learning Theory, 2nd edition. New York:
Springer.

234
REFERENCES
Wachi S., Yoneda K., and Wu, R. (2005) Interactome-transcriptome analysis reveals the
high centrality of genes differentially expressed in lung cancer tissues. Bioinformatics
21: 4205–4208.
Wahba, G. (1990). Spline Models for Observational Data. Philadelphia: Society for Indus-
trial and Applied Mathematics.
Wahba, G. (1999) Support vector machines, reproducing kernel Hilbert spaces and the
randomized GACV. In B. Sch¨olkopf, C. Burges and A. Smola (eds), Advances in
Kernel Methods, pp. 69–88. Cambridge, MA: MIT Press.
Wakeﬁeld J.C., Smith, A., Racine-Poon, A., and Gelfand A. (1994) Bayesian analysis of
linear and non-linear population models by using the Gibbs sampler. Applied Statistics,
43, 201–221.
Wakeﬁeld, J., Zhou, C., and Self, S. (2003) Modeling gene expression over time: curve
clustering with informative prior distributions. In J.M. Bernardo, M. Bayarri, A.P.
Dawid, J.O. Berger, D. Heckerman, A.F.M. Smith and M. West (eds), Bayesian Statis-
tics 7. Oxford: Oxford University Press.
Walker, S., Damien, P., Laud, P., and Smith, A.F.M. (1999) Bayesian nonparametric
inference for random distributions and realted functions. Journal of the Royal Statistical
Society, Series B, 61, 485–527.
Wang, Y., Miao, Z.H., Pommier, Y., Kawasaki, E.S., and Player, A. (2007) Charac-
terization of mismatch and high-signal intensity probes associated with Affymetrix
genechips. Bioinformatics, 23(16), 2088–2095.
Watson, J., and Crick, F. (1953) Genetical implications of the structure of deoxyribonucleic
acid. Nature, 171, 964–967.
Watson, J., Baker, T., Bell, S., Gann, A., Levine, M., and Losick, R. (2007) Molecular
Biology of the Gene, 6th edition. Menlo Park, CA: Benjamin/Cummings.
Weaver, D.C., Workman, C.T., and Stormo, G. D. (1999) Modeling regulatory networks
with weight matrices. Proceedings of the Speciﬁc Symposium on Biocomputing, pp.
340–359.
Wei, L. (1992) The accelerated failure time model: a useful alternative to the Cox regres-
sion model in survival analysis. Statistics in Medicine, 11, 1871–1879.
Wei, Z. and Li, H. (2007) Nonparametric pathway-based regression models for analysis
of genomic data. Biostatistics, 8, 265–284.
Weigelt, B., Horlings, H.M., Kreike, B., Hayes, M.M., Hauptmann, M., Wessels, L.F., de
Jong, D., van de Vijver, M.J., Van’t Veer, L.J., and Peterse, J.L. (2008) Reﬁnement of
breast cancer classiﬁcation by molecular characterization of histological special types.
J. Pathol., 216(6), 141–150.
Weir, I.S., and Pettitt, A.N. (2000) Binary probability maps using a hidden conditional
autoregressive Gaussian process with an application to Finnish common toad data.
Applied Statistics, 49, 473–484.
West, M. (2003) Bayesian factor regression models in the ‘large p, small n’ paradigm. In
J.M. Bernardo, M. Bayarri, A.P. Dawid, J.O. Berger, D. Heckerman, A.F.M. Smith and
M. West (eds), Bayesian Statistics 7, pp. 723–732. Oxford: Oxford University Press.
West, M., Muller, P., and Escobar, M. (1994) Hierarchical priors and mixture models with
application in regression and density estimation. In A.F.M. Smith and P.R. Freeman
(eds), Aspects of Uncertainty: a tribute to D.V. Lindley, pp. 363–386. Chichester: John
Wiley & Sons, Ltd.

REFERENCES
235
West, M., Blanchette, C., Dressman, H., Huang, E., Ishida, S., Spang, R., Zuzan, H.,
Olson, J.A. Jr, Marks, J.R., and Nevins, J.R. (2001) Predicting the clinical status of
human breast cancer by using gene expression proﬁles. Proc. Natl. Acad. Sci. USA,
98(20), 11462–11467.
Westfall, P.H. and Young, S.S. (1993) Resampling Based Multiple Testing. New York:
John Wiley & Sons, Inc.
Whitﬁeld, M., Sherlock, G., Saldanha, A., Murray, J., Ball, C., Alexander, K., Matese,
J., Perou, C., Hurt, M., Brown, P., and Botstein, D. (2002) Identiﬁcation of genes
periodically expressed in the human cell cycle and their expression in tumors. Mol.
Biol. Cell, 13, 1977–2000.
Whittaker, J. (1990) Graphical Models in Applied Multivariate Statistics. Chichester: John
Wiley & Sons, Ltd.
Whittemore, A.S. (2007) A Bayesian false discovery rate for multiple testing. Journal of
Applied Statistics, 34, 1–9.
Wigner, E.P. (1955) Characteristic vectors of bordered matrices with inﬁnite dimensions
characteristic vectors of bordered matrices with inﬁnite dimensions. Ann. Math. 62,
548–564.
Wilkinson, D.J. (2007) Bayesian methods in bioinformatics and computational systems
biology. Brieﬁngs in Bioinformatics, 8, 109–116.
Wilkinson, D.J. and Yeung, S.K.H. (2002) Conditional simulation from highly structured
Gaussian systems, with application to blocking-MCMC for the Bayesian analysis of
very large linear models. Statistics and Computing, 12, 287–300.
Williams, C., and Barber, D. (1998) Bayesian classiﬁcation with Gaussian priors. IEEE
Transactions on Pattern Analysis and Machine Intelligence, 20, 1342–1351.
Wilson, D.L., Buckley, M.J., Helliwell, C.A., and Wilson, I.W. (2003) New normalization
methods for cDNA microarray data. Bioinformatics, 19(11), 1325–1332.
Winter, S.S., Jiang, Z., Khawaja, H.M., Grifﬁn, T., Devidas, M., Asselin, B.L., Larson,
R.S., and Children’s Oncology Group (2007) Identiﬁcation of genomic classiﬁers that
distinguish induction failure in T-lineage acute lymphoblastic leukemia: a report from
the Children’s Oncology Group. Blood, 110(5), 1429–1438.
Wu, Z., Irizarry, R.A., Gentleman, R., Murillo F.M., and Spencer, F. (2004)
A Model Based Background Adjustment for Oligonucleotide Expression Arrays.
Johns Hopkins University, Dept. of Biostatistics Working Papers. Working Paper 1.
http://www.bepress.com/jhubiostat/paper1.
Xu, X., Olson, J., and Zhao, L. (2002) A regression-based method to identify differentially
expressed genes in time course studies and its application to inducible Huntington’s
disease. Hum. Mol. Genet., 11, 1977–1985.
Yang, Y.H., Buckley, M.J., and Speed, T.P. (2001) Analysis of cDNA microarray images.
Brieﬁngs in Bioinformatics, 2(4), 341–349.
Yang, Y.H., Dudoit, S, Luu, P., Lin, D.M., Peng, V., Ngai, J., and Speed, T.P. (2002a)
Normalization for cDNA microarray data: a robust composite method addressing single
and multiple slide systematic variation. Nucleic Acids Research, 30(4), e15.
Yang, Y.H., Buckley, M.J., Dudoit, S., and Speed, T.P. (2002b). Comparison of methods
for image analysis on cDNA microarray data. Journal of Computational and Graphical
Statistics, 11(1), 108–136.

236
REFERENCES
Yauk, C.L., Berndt, M.L., Williams, A., and Douglas, G.R. (2004) Comprehensive com-
parison of six microarray technologies. Nucleic Acids Research, 32, e124.
Yeang, C.H., Ramaswamy, S., Tamayo P., Mukherjee, S., Rifkin, R.M., Angelo, M.,
Reich, M., Lander, E., Mesirov, J., and Golub, T. (2001) Molecular classiﬁcation of
multiple tumor types. Bioinformatics, 17(Supplement 1), S316–322.
Yekutieli, D., and Benjamini, Y. (2001) The control of the false discovery rate in multiple
testing under dependency. Annals of Statistics, 29(4), 1165–1188.
Yuan, M., and Lin, Y. (2006) Model selection and estimation in regression with grouped
variables. Journal of the Royal Statistical Society, Series B, 68, 49–67.
Yuan, M., Kendziorski, C., Park, F., Porter, J.R., Hayes, K., and Bardﬁeld, C.A. (2006).
Hidden Markov models for microarray time course data in multiple biological condi-
tions. Journal of the American Statistical Association, 101, 1323–1332.
Zellner, A. (1986) On assessing prior distributions and Bayesian regression analysis
with g-prior distributions. In P.K. Goel and A. Zellner (eds), Bayesian Inference and
Decision Techniques: Essays in Honor of Bruno de Finetti, pp. 233–243. New York:
Elsevier.
Zhang, K., and Zhao, H. (2000) Assessing reliability of gene clusters from gene expression
data. Funct. Integr. Genomics, 1(3), 156–173.
Zhang, L., Miles, M.F., and Aldape, K.D. (2003) A model of molecular interactions on
short oligonucleotide microarrays. Nature Biotechnology, 21(7), 818–821.
Zhang, M., Yao, C., Guo, Z., Zou, J., Zhang, L., Xiao, H., Wang, D., Yang, D., Gong, X.,
Zhu, J., Li, Y., and Li, X. (2008) Apparently low reproducibility of true differential
expression discoveries in microarray studies. Bioinformatics, 24(18), 2057–2063.
Zhao, P., and and Yu, B. (2006) On model selection consistency of lasso. Journal of
Machine Learning Research, 7, 2541–2563.
Zhou, B.B., Peyton, M., He, B., Liu, C., Girard, L., Caudler, E., Lo, Y., Baribaud, F.,
Mikami, I., Reguart, N., Yang, G., Li, Y., Yao, W., Vaddi, K., Gazdar, A.F., Friedman,
S.M., Jablons, D.M., Newton, R.C., Fridman, J.S., Minna, J.D., and Scherle, P.A. (2006)
Targeting ADAM- mediated ligand cleavage to inhibit HER3 and EGFR pathways in
non-small cell lung cancer. Cancer Cell., 10(1), 39–50.
Zhou, X., Liu, S., Kim, E.S., Herbst, R.S., and Lee, J.J. (2008) Bayesian adaptive design
for targeted therapy development in lung cancer – a step toward personalized medicine.
Clin. Trials, 5(3), 181–193.
Zhu, G., Spellman, P.T., Volpe, T., Brown, P.D., Botstein, D., Davis, T.N., and Futcher,
B. (2000) Two yeast forkhead genes regulate the cell cycle and pseudohyphal growth.
Nature, 406, 90–94.

Index
Accelerated failure time model 158
Adenine 6
Adjusted Rand index 134
AIC 173
Amino acids 9
ANOVA
Bayesian ANOVA model 34
Models for differential
expression 36
Robust ANOVA model 38
Autoregressive model 154
Auxiliary variables 77
baseline distribution 127
B-spline 152
Bayes estimate 176
Bayes factor 168, 174
Bayes risk 166, 173
Bayes Theorem 161
Bayesian Analysis of Linear
Model 22
Bayesian Decomposition 107, 112
Bayesian hypothesis testing 66
Bayesware Discoverer 147
BIC 141
BIC 174
Binary distribution 163
Binary outcomes 168
Bioconductor 21
Bonferroni 58
Celera Corporation 4
Chinese restaurant process 129
Bayesian Analysis of Gene Expression Data
B. Mallick, D. Gold, and V. Baladandayuthapani
2009 John Wiley & Sons, Ltd
Classiﬁcation 69
Linear 71
Nonlinear 79
Cluster indicator 130
Co-regulated genes 137
Codons 9
Complementary base 6
Complementary hybridization
15
Complimentary pair 7
Conjugate posteriors 172
Correlation 177
covariance 177
Cox proportional hazards model
157
Craig Venter 4
Credible region 167
Cytosine 6
DAG 138
dChip 12, 21, 22
Decision rule 165
Decision Theory 55, 165
Differential Expression 36
Differential gene expression
analysis 32
Dirichlet distribution 127
Dirichlet process mixture model
129
Dirichlet Process Prior 126
Discriminant analysis 71
DNA 5
DNA structure 6

238
INDEX
Double Helix 6
Dynamic Bayesian networks
155
edges 138
Empirical distribution 160
Enrichment Analysis (EA) 96
Bayesian Enrichment Analysis
98
Bayesian Multistage Linear
Model 105
Gene Set Analysis (GSA) 102
Gene Set Enrichment Analysis
(GSEA) 97
Multivariate generalization of
enrichment analysis
(MEA) 97
European Bioinformatics Institute
4
Exchangeability 130
Exchangeable 160, 175
Exons 9
False Discovery Rate 53
BAM FDR analysis 67
Bayesian FDR 64
Bayesian Multivariate FDR 65
BayesMix Software 68
BUM 62
decisive FDR 60
local FDR 62
positive FDR 60
Family-wise Error Rate 58
Fisher information 164
Functional clustering 153
gamma-gamma model 34
GCRMA 12
gene 5, 7
Gene expression 6
Gene Ontology 95
GIMM 133
Graphical Models 137
Guanine 6
Helical Chain 6
Hidden Markov model 154
Hierarchical Modeling 175
Human Genome Project 4
Hyper-Dirichlet distribution 145
Hyper-Markov law 142
Hypothesis testing 167, 173
Improper prior 164
Inﬁnite mixture model 126
Inﬁnite mixture of Gaussian
distributions 131
Interactions 79
Introns 9
Jeffreys prior 164
K2 algorithm 146
Kernel methods 82
Latent processes 76
Lindley’s paradox 174
Linear Models for Differential
Expression 30
MA plot 20
MAP 132
Marginal Likelihood 142
Markov blanket 141
Markov properties 140
MARS 80
MAS 5.0 21
Microarray experiments 11
Affymetrix 12, 15–22, 41–42,
46, 112, 116
Bayesian Normalization 48
Cross hybridization 20
Designs 12
Hybridization 15–17,
Image processing 13, 15, 18
Normalization 20, 21, 47
Print Tip effects 49, 51–52
reproducibility in 12
Scanning 15, 20

INDEX
239
Spot/feature identiﬁcation
13–15, 17–18, 20
technical artifacts 18
two-channel 20, 34, 35
Mismatch 13
mixing proportion 128
Mixture Model 125
Bayesian 127–128, 180–181
Classiﬁcation 82
Clustering 122, 125, 130
FDR 62–63, 66, 68
Frequentist 127
Gaussian Inﬁnite Mixture Model
137–138
Gene Class Enrichment 99,
105
Inﬁnite, Dirichlet Process
130–135
Label Switching 129
MCMC updating 128
Prior 177
Mixture modeling 181
Mixture prior 172
MLE 176
Model averaging 183
Model choice 172, 183
Model Selection Priors 26
Multinomial 181
Multinomial Dirichlet model 143
Multiple testing 53
National Center for Biotechnology
Information 4
network 137
nonparametric: 127
Nucleotides 6
Nuisance effects 42, 43
Ocham’s window 184
Ockham’s razor 173
Pairwise probability 132
Partial exchangeability 162
Pathways, gene pathways 94
Perfect Match 13
Periodic basis function 154
PLIER 21
Point Estimation 165
Posterior distribution 163
Posterior predictive distribution 161
Posterior probability 184
precision parameter 127
Predictive Distribution 168, 172
Principal Component Analysis
123
Prior distribution 163
Promoter region 7
protein 5
Rand index 134
Regression
Linear 21
Nonlinear 70
Generalized 75
Regression 163
regulatory 137
Representation Theorem 159, 160
Reproducing kernel Hilbert spaces
82
Reversible Jump 128
RNA 5
RNA splicing 8, 9
Shrinkage 176, 177
Signiﬁcance Analysis of
Microarrays (SAM)
35–36, 158
Singular distributions 38
Sparsity Priors 29
Splines 80
Stick breaking prior 127
Strands 6
Subjective probability 163
Support Vector Machines 82
Survival Prediction 155
Thyamine 6
Time course data 151

240
INDEX
Time course gene expression data
151
Transcription process 6
transcription factor 139
translation process 6
tRNA 9
Undirected graphical models
138
Uniform prior 163
Utility function 165
Variable selection 25
vertices 138
Wavelets 154
Weibull 156

