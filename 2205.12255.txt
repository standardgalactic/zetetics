TALM: Tool Augmented Language Models
Aaron Parisi
Yao Zhao
Noah Fiedel
{aarontp,yaozhaoyz,nfiedel}
@google.com
Abstract
Transformer based language models (LMs)
demonstrate
increasing
performance
with
scale across a wide variety of tasks.
Scale
alone however cannot enable models to solve
tasks that require access to ephemeral, chang-
ing, or private data that was unavailable at
training time. Many useful tasks may also ben-
eﬁt from LMs being able to access APIs that
read or modify state. In this work, we present
Tool Augmented Language Models (TALM),
combining a text-only approach to augment
language models with non-differentiable tools,
and an iterative “self-play” technique to boot-
strap performance starting from few tool
demonstrations.
TALM exhibits strong per-
formance on both a knowledge-heavy QA task
and a reasoning oriented math task with sim-
ple tools. At a given model scale, TALM sig-
niﬁcantly outperforms non-augmented LMs.
We further demonstrate that TALM success-
fully performs out-of-distribution inferences
on both QA and math tasks, where non-
augmented LMs fail.
Our results suggest
that Tool Augmented Language Models are a
promising direction to enrich LMs’ capabili-
ties, with less dependence on scale.
1
Introduction
Language models using the Transformer architec-
ture [Vaswani et al., 2017] demonstrate increas-
ing performance at larger scales, e.g.
T5 [Raf-
fel et al., 2019], GPT-3 [Brown et al., 2020], and
PaLM [Chowdhery et al., 2022]. Scale related per-
formance gains are observed on a variety of bench-
marks, e.g. SuperGLUE [Wang et al., 2019] and
BIG-bench [BIG-bench collaboration, 2021].
Scaling up has practical downsides. Large scale
models are unwieldy to store, transfer, and deploy.
Their costs to train or perform inference can be
prohibitively high for many researchers and orga-
nizations.
0
1
2
3
Self-play Round
15
20
25
30
35
40
Natural Questions (F1)
Knowledge
TALM (xl)
TALM (large)
TALM (base)
LM (xl)
LM (large)
LM (base)
0
1
2
3
Self-play Round
20
25
30
35
40
45
50
55
MathQA (solver-match)
Reasoning
TALM (xl)
TALM (large)
TALM (base)
LM (xl)
LM (large)
LM (base)
Figure 1: Baseline LM and TALM performance on two
tasks, with increasing rounds of self-play.
Larger models memorize more world knowl-
edge [Roberts et al., 2020]. While good for many
benchmark tasks, relying on memorization alone
poses several problems. First, models sometimes
generate incorrect outputs that are problematic for
some applications. Second, world knowledge is
constantly changing. The knowledge from yester-
day’s training data might be invalid today. Third,
large models can memorize parts of their training
data with undesirable consequences [Carlini et al.,
2022].
Retrieval based approaches to enhancing LMs
can lower the dependence on scale. REALM [Guu
et al., 2020] learns retrieval via backpropagation
from a ﬁxed corpus.
RETRO [Borgeaud et al.,
2021] adds an ”internet scale” retrieval mecha-
nism. RAG [Lewis et al., 2020] uses a dense vec-
tor index of Wikipedia, and retrieves either once
per token or once per query. Other works demon-
strated that LMs can be enhanced on math reason-
ing with access to a calculator [Andor et al., 2019].
Looking towards the future utility of language
models, it is clear that scale and retrieval cannot
solve all useful problems. Many knowledge tasks
and desirable applications require access to read
live or private data (e.g. weather or a person’s cal-
arXiv:2205.12255v1  [cs.CL]  24 May 2022

endar), or to invoke APIs that modify state. Recent
works such as Say Can [Ahn et al., 2022] connect
languages models to an environment, though with
the model as a recipient of queries. In contrast,
TALM’s approach enables models to invoke arbi-
trary tools with model-generated output, and to at-
tend to tool output to generate task outputs.
In summary, our contributions are:
• Demonstrating that language models can be
augmented with tools via a text-to-text API.
• Demonstrating an iterative self-play tech-
nique to bootstrap tool-augmented datasets
and subsequent tool-augmented model per-
formance, from few labeled examples.
2
Methods
We use pretrained T5 models [Raffel et al., 2019,
Roberts et al., 2022] for ﬁnetuning, inference and
evaluation. To measure the effects of model scal-
ing, we use the base, large, and XL sizes.
2.1
Tool Augmented Language Models
input
output
tool input
tool result
Language Model
Tool Augmented Language Model
input
output
call
external
tool
append
tool
result
Figure 2: LM and Tool Augmented LMs.
We use a Text-to-Text tool interface given its
broad applicability and simplicity, as shown in
Fig. 3. TALM ﬁrst generates a tool input condi-
tioned on the task input text and invokes a tool’s
API by generating a delimiter, such as ”|result”.
Whenever this delimiter is detected, the tool API is
called and its result appended to the text sequence.
TALM then continues to generate the ﬁnal task
output.
An abstract task:
task input text |tool-call tool input text |result tool out-
put text |output task output text
A weather task:
how hot will it get in NYC today? |weather lookup re-
gion=NYC |result precipitation chance: 10, high temp:
20c, low-temp: 12c |output today’s high will be 20C
Figure 3: TALM text-to-text interface example.
TALM learns two subtasks at the same time:
calling a tool and generating an answer based on
tool results. TALM is architecturally agnostic and
can be implemented as Seq2Seq, left-to-right LM
or preﬁx LM. We chose the Seq2Seq family for its
high ﬁnetuning performance at modest scale [Raf-
fel et al., 2019].
2.2
Iterative self-play
When introducing new tools to solve existing
tasks, there are often a limited number of demon-
strations of tool interactions. However, there is
typically plenty of supervised task data consist-
ing of input and target pairs, and automated met-
rics for evaluating the correctness of a generated
output. Inspired by Decision Transformer [Chen
et al., 2021], we use a self-play approach to iter-
atively bootstrap examples of tool-use with pro-
gressively higher quality. In this work, we refer
to a model interacting with a tool API as self-play
rather than adversarial play among models.
Algorithm 1 Iterative Self-Play Algorithm.
x: task input, y: task output, t: tool input, r: tool output
1: T = {xi, yi}T
# task set
2: D = {xj, tj, rj, yj}D
# tool-use set
3: Pθ ←pretrained LM
4: for t ∈[0, 1, ..., R] do
# self-play rounds
5:
# ﬁnetune LM
6:
θ ←argmax
θ
Q
D Pθ(yj|xj, tj, rj)Pθ(tj|xj)
7:
for xi, yi ∈T do
# iterate task set
8:
for n ∈[0, 1, ..., N] do
9:
tn ←Pθ(t|xi)
# sample tool query
10:
rn ←Tool(tn)
# call tool API
11:
yn ←Pθ(y|xi, tn, rn)
# get task output
12:
if |yn −yi| < th then
# ﬁlter wrong output
13:
D ←D ∪{xi, tn, rn, yn}1
14:
# update tool-use set
The iterative self-play pipeline starts with a
small tool-use bootstrapping set {xj, tj, rj, yj}D.
In each round of self-play, the TALM is ﬁne-tuned
on the tool-use set D. Next, for every example in
the task set T, the TALM samples tool inputs, calls
a tool API, and samples task outputs based on the
tool results. If the TALM generated task output
matches the target within some threshold th, the
tool-use sequence led to the result is added to the
tool-use set D for the next round of self-play.
To explore diverse tool API invocations and an-
swers during self-play, the TALM decodes using
random sampling with temperature=1.0, and top-

k=40. To grow the dataset during self-play, the
TALM generates up to N=600 tool-use sequences
per example. At evaluation time, the model uses
beam decoding with 4 beams to generate a single
output.
We note that this iterative self-play pipeline rep-
resents a special case of a policy-gradient RL al-
gorithm, where the LM is the policy network and
is trained by policy gradient with a binary reward
signal. Iterative self-play is related to expert it-
eration [Anthony et al., 2017], which has been
demonstrated to work well in tasks with extremely
weak supervision [Christiano et al., 2018]. While
our tasks are currently single-hop, this formulation
can be extended further into RL: modelling multi-
hop tool-use tasks as markov decision processes
(MDPs), or integrating algorithms like Decision
Transformer [Chen et al., 2021].
3
Results
We evaluate TALM on two domains. The ﬁrst is
the knowledge-oriented Natural Questions (NQ)
[Kwiatkowski et al., 2019], a diverse QA task. The
second is MathQA [Amini et al., 2019], selected to
measure general reasoning capability rather than
knowledge.
3.1
Natural Questions
Natural Questions (NQ) is a large (≈300k train-
ing examples) QA dataset collected from real user
queries.
NQ contains both long and short an-
swer tasks.
We selected the short answer task
as it is both more challenging as measured with
lower baseline performance, and closer to practi-
cal use-cases such as assistants. In addition to a
question and short-answer pair, examples in the
NQ dataset include an ”oracle” context (span) of
a Wikipedia document containing the answer. We
remove boolean questions to avoid inﬂated perfor-
mance due to random-chance guesses. We com-
pare TALM against closed-book LM benchmarks.
For TALM experiments, we do not feed the or-
acle contexts directly to the model, instead using
them to populate an index that TALM can access
as a retrieval tool. The retrieval system is imple-
mented using a BM25-based index over the union
of all NQ oracle contexts.
In Fig. 5, even the 220M base TALM outper-
forms 3B XL LM. There is also a smaller perfor-
mance gap between base and XL sized TALMs
than between TALM and LM, suggesting that
Question: when are hops added in brewing process?
Short Answer: The boiling process.
|question when are hops added in brewing process?
|search brewing process |result The boiling process is
where chemical reactions take place...including |output
The boiling process.
Figure 4: Example from Natural Questions, as a stan-
dard NQ task and the corresponding tool-augmented
sequence.
220M
770M
3000M
Parameters
32
34
36
38
40
42
Natural Questions (F1)
Knowledge
TALM
LM
Figure 5:
Performance of TALM compared with
TALM of different model sizes on Natural Questions.
The TALM is bootstrapped from 150 tool demonstra-
tions and undergoes two rounds of self-plays.
We
hypothesize that the noise in the performance-scale
curves is due to ﬁnetuning in a low-data regime.
smaller models beneﬁt more from retrieval tools
for knowledge intensive tasks.
3.2
MathQA
MathQA [Amini et al., 2019] is a large scale
dataset of math word problems (≈30k training
examples). Each example includes the word prob-
lem, a formula generated by crowd-source work-
ers to calculate the answer, and the correct text-
form answer among multiple choices.
Question: If Lily’s test scores are 85 , 88 and 95 out
of 100 in 3 different subjects , what will be her average
score?
Formula: Divide(Add(85, Add(88, 95)), 3)
Answer: 89.33
|question If Lily’s test scores are 85 , 88 and 95 out
of 100 in 3 different subjects , what will be her aver-
age score? |formula Divide(Add(85, Add(88, 95)), 3)
|result 89.3333333333 |output 89.33
Figure 6:
Example from MathQA, as a standard
MathQA task and the corresponding tool-augmented
sequence..
We implemented a simple solver tool to exe-

cute formulas and check their results’ correctness
against their associated text-form answers.
Ac-
cording to our solver tool, approximately 70%
of the formulas in MathQA produce results that
match their corresponding answers, similar to the
ﬁndings in [Hendrycks et al., 2021]. Our man-
ual inspections show that mismatched results are
due to either wrong formulas or invalid answers.
The bootstrap tool-use dataset consists of a ran-
dom sample of 10% of the training corpus where
the formula is valid (≈2k examples). The TALM
signiﬁcantly outperforms a non-augmented LM as
shown in Fig. 7.
220M
770M
3000M
Parameters
20
25
30
35
40
45
50
55
MathQA (solver-match)
Reasoning
TALM
LM
Figure 7: Performance of TALM compared with LM of
different model sizes on MathQA.
3.3
Self-Play Ablations
We ﬁnd that TALMs perform signiﬁcantly better
after a single round of self-play than after training
only on the limited bootstrap tool-use training ex-
amples, as shown in Fig. 1. Their performance
continues to increase over three rounds of self-
play. This trend holds across model sizes ranging
from 220M to 3B.
3.4
Out-of-distribution Examples
One beneﬁt of TALM is its capability to general-
ize to input text that is out-of-distribution to the
model’s training data, yet solvable with access to
tools.
On the knowledge-heavy QA task, we replace
the BM-25 Wiki retriever with a public search
engine, and show that TALM handles changing
world knowledge well (see Fig. 8).
Question: What is wordle?
LM: a word generator
TALM: a simple online word game that challenges
people to ﬁnd a ﬁve-letter word in six guesses
Figure 8: LM vs TALM on changing knowledge.
On the math task, we test large number han-
dling, an area where training data is lacking and
non-augmented LMs are known to perform poorly
[Brown et al., 2020]. See Fig. 9 demonstrating
that TALM can handle large numbers, where a LM
does not.
Question: A car is driving 535 miles per hour, how
many hours does it take to travel 2450 miles?
LM: 8.5
TALM: 4.58
Figure 9: LM vs TALM on a large number operation.
4
Conclusion
In this paper we present TALM, a framework for
augmenting language models with arbitrary tools.
TALM has two key ideas. First, we model tool-
use via a text-to-text interface. Second, we apply
an iterative self-play technique to bootstrap high
performance on tasks with few tool-use labelled
examples. Taken together, this interface and tech-
nique make exploring additional tools and tasks
possible, without requiring expensive data label-
ing efforts.
TALM
consistently
outperforms
a
non-
augmented LM on both a knowledge task (NQ)
and reasoning task (MathQA). Ablations show
that self-play is key to good performance, and that
iterative self-play yields further gains. We con-
clude that the combination of tool augmentation
and iterative self-play enables smaller models to
outperform larger non-augmented LMs.
We hope that this work enables further research
into tool augmented language models, a promising
direction to enhance model capabilities with less
dependency on scale than many contemporary ap-
proaches.
References
Michael Ahn, Anthony Brohan, Noah Brown, Yev-
gen Chebotar, Omar Cortes, Byron David, Chelsea
Finn, Keerthana Gopalakrishnan, Karol Hausman,
Alex Herzog, Daniel Ho, Jasmine Hsu, Julian Ibarz,
Brian Ichter, Alex Irpan, Eric Jang, Rosario Jau-
regui Ruano, Kyle Jeffrey, Sally Jesmonth, Nikhil J
Joshi, Ryan Julian, Dmitry Kalashnikov, Yuheng
Kuang, Kuang-Huei Lee, Sergey Levine, Yao Lu,
Linda Luu, Carolina Parada, Peter Pastor, Jor-
nell Quiambao, Kanishka Rao, Jarek Rettinghouse,
Diego Reyes, Pierre Sermanet, Nicolas Sievers,
Clayton Tan,
Alexander Toshev,
Vincent Van-
houcke, Fei Xia, Ted Xiao, Peng Xu, Sichun Xu, and

Mengyuan Yan. Do as i can, not as i say: Ground-
ing language in robotic affordances, 2022.
URL
https://arxiv.org/abs/2204.01691.
Aida
Amini,
Saadia
Gabriel,
Peter
Lin,
Rik
Koncel-Kedziorski,
Yejin
Choi,
and
Han-
naneh
Hajishirzi.
Mathqa:
Towards
inter-
pretable
math
word
problem
solving
with
operation-based
formalisms,
2019.
URL
https://arxiv.org/abs/1905.13319.
Daniel Andor, Luheng He, Kenton Lee, and Emily
Pitler. Giving bert a calculator: Finding operations
and arguments with reading comprehension, 2019.
Thomas Anthony, Zheng Tian, and David Barber.
Thinking fast and slow with deep learning and tree
search. CoRR, abs/1705.08439, 2017. URL http:
//arxiv.org/abs/1705.08439.
BIG-bench collaboration. Beyond the imitation game:
Measuring and extrapolating the capabilities of lan-
guage models. In preparation, 2021. URL https:
//github.com/google/BIG-bench/.
Sebastian Borgeaud, Arthur Mensch, Jordan Hoff-
mann, Trevor Cai, Eliza Rutherford, Katie Millican,
George van den Driessche, Jean-Baptiste Lespiau,
Bogdan Damoc, Aidan Clark, Diego de Las Casas,
Aurelia Guy, Jacob Menick, Roman Ring, Tom Hen-
nigan, Saffron Huang, Loren Maggiore, Chris Jones,
Albin Cassirer, Andy Brock, Michela Paganini, Ge-
offrey Irving, Oriol Vinyals, Simon Osindero, Karen
Simonyan, Jack W. Rae, Erich Elsen, and Laurent
Sifre.
Improving language models by retrieving
from trillions of tokens, 2021.
Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie
Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell,
Sandhini Agarwal,
Ariel Herbert-Voss,
Gretchen Krueger, Tom Henighan, Rewon Child,
Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,
Clemens Winter, Christopher Hesse, Mark Chen,
Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin
Chess, Jack Clark, Christopher Berner, Sam Mc-
Candlish, Alec Radford, Ilya Sutskever, and Dario
Amodei.
Language models are few-shot learn-
ers, 2020.
URL https://arxiv.org/abs/
2005.14165.
Nicholas Carlini, Daphne Ippolito, Matthew Jagiel-
ski, Katherine Lee, Florian Tramer, and Chiyuan
Zhang.
Quantifying memorization across neural
language models, 2022. URL https://arxiv.
org/abs/2202.07646.
Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee,
Aditya Grover, Michael Laskin, Pieter Abbeel, Ar-
avind Srinivas, and Igor Mordatch. Decision trans-
former: Reinforcement learning via sequence mod-
eling, 2021. URL https://arxiv.org/abs/
2106.01345.
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin,
Maarten Bosma, Gaurav Mishra, Adam Roberts,
Paul Barham, Hyung Won Chung, Charles Sutton,
Sebastian Gehrmann, Parker Schuh, Kensen Shi,
Sasha Tsvyashchenko, Joshua Maynez, Abhishek
Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vin-
odkumar Prabhakaran, Emily Reif, Nan Du, Ben
Hutchinson, Reiner Pope, James Bradbury, Jacob
Austin, Michael Isard, Guy Gur-Ari, Pengcheng
Yin, Toju Duke, Anselm Levskaya, Sanjay Ghe-
mawat, Sunipa Dev, Henryk Michalewski, Xavier
Garcia, Vedant Misra, Kevin Robinson, Liam Fedus,
Denny Zhou, Daphne Ippolito, David Luan, Hyeon-
taek Lim, Barret Zoph, Alexander Spiridonov, Ryan
Sepassi, David Dohan, Shivani Agrawal, Mark
Omernick, Andrew M. Dai, Thanumalayan Sankara-
narayana Pillai, Marie Pellat, Aitor Lewkowycz,
Erica Moreira,
Rewon Child,
Oleksandr Polo-
zov, Katherine Lee, Zongwei Zhou, Xuezhi Wang,
Brennan Saeta, Mark Diaz, Orhan Firat, Michele
Catasta, Jason Wei, Kathy Meier-Hellstern, Dou-
glas Eck,
Jeff Dean,
Slav Petrov,
and Noah
Fiedel. Palm: Scaling language modeling with path-
ways, 2022. URL https://arxiv.org/abs/
2204.02311.
Paul F. Christiano, Buck Shlegeris, and Dario Amodei.
Supervising strong learners by amplifying weak ex-
perts. CoRR, abs/1810.08575, 2018. URL http:
//arxiv.org/abs/1810.08575.
Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pa-
supat, and Ming-Wei Chang.
Realm: Retrieval-
augmented language model pre-training, 2020.
Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul
Arora, Steven Basart, Eric Tang, Dawn Song, and
Jacob Steinhardt.
Measuring mathematical prob-
lem solving with the MATH dataset.
CoRR,
abs/2103.03874, 2021. URL https://arxiv.
org/abs/2103.03874.
Tom Kwiatkowski, Jennimaria Palomaki, Olivia Red-
ﬁeld, Michael Collins, Ankur Parikh, Chris Alberti,
Danielle Epstein, Illia Polosukhin, Matthew Kelcey,
Jacob Devlin, Kenton Lee, Kristina N. Toutanova,
Llion Jones, Ming-Wei Chang, Andrew Dai, Jakob
Uszkoreit, Quoc Le, and Slav Petrov. Natural ques-
tions: a benchmark for question answering research.
Transactions of the Association of Computational
Linguistics, 2019.
Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio
Petroni, Vladimir Karpukhin, Naman Goyal, Hein-
rich K¨uttler,
Mike Lewis,
Wen-tau Yih,
Tim
Rockt¨aschel, Sebastian Riedel, and Douwe Kiela.
Retrieval-augmented generation for knowledge-
intensive nlp tasks,
2020.
URL https://
arxiv.org/abs/2005.11401.
Colin Raffel, Noam Shazeer, Adam Roberts, Kather-
ine Lee, Sharan Narang, Michael Matena, Yanqi
Zhou, Wei Li, and Peter J. Liu. Exploring the lim-
its of transfer learning with a uniﬁed text-to-text

transformer, 2019. URL https://arxiv.org/
abs/1910.10683.
Adam Roberts, Colin Raffel, and Noam Shazeer. How
much knowledge can you pack into the parame-
ters of a language model?, 2020.
URL https:
//arxiv.org/abs/2002.08910.
Adam Roberts, Hyung Won Chung, Anselm Levskaya,
Gaurav Mishra, James Bradbury, Daniel Andor,
Sharan Narang, Brian Lester, Colin Gaffney, Afroz
Mohiuddin, et al. Scaling up models and data with
t5x and seqio.
arXiv preprint arXiv:2203.17189,
2022.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
Kaiser, and Illia Polosukhin.
Attention is all you
need, 2017.
Alex Wang, Yada Pruksachatkun, Nikita Nangia,
Amanpreet Singh, Julian Michael, Felix Hill, Omer
Levy, and Samuel R. Bowman. Superglue: A stick-
ier benchmark for general-purpose language under-
standing systems, 2019. URL https://arxiv.
org/abs/1905.00537.
5
Appendix
5.1
Acknowledgements
The authors would like to thank Noam Shazeer
for early brainstorming on the path towards this
work. We also thank Igor Mordatch for discus-
sions and feedback. Finally we thank Mohammad
Saleh for his helpful review and feedback improv-
ing this manuscript.
5.2
Author Contributions
This section lists the author contributions of each
author.
• Aaron Parisi designed and implemented tool-
augmentation and self-play pipelines. Aaron
ran the vast majority of experiments, and par-
ticipated in brainstorming and paper writing.
• Yao Zhao participated in brainstorming, ex-
perimental setup discussion and paper writ-
ing. Yao implemented NQ/mathQA baselines
and mathQA solvers.
• Noah Fiedel conceived of the project, par-
ticipated in brainstorming, led the research
group and writing the paper.

