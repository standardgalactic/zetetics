POSTER PRESENTATION
Open Access
Partial information decomposition as a unified
approach to the characterization and design of
neural goal functions
Michael Wibral1,2*, William A Phillips3, Joseph T Lizier4, Viola Priesemann5,6
From 24th Annual Computational Neuroscience Meeting: CNS*2015
Prague, Czech Republic. 18-23 July 2015
In many neural systems anatomical motifs are found
repeatedly in different places. Despite this repetition
these motifs often seem to serve a perplexing variety of
functions. A prime example is the canonical microcir-
cuit, which is repeated across multiple cortical areas,
but supports a variety of functions from sensory proces-
sing and memory to executive functions and motor con-
trol. The multiplicity of functions served by a single
anatomical motif suggests a common, but more abstract,
information processing goal underlying all the different
functions. Identifying this goal from neural recordings is
a key challenge in understanding the general principles
of neural information processing. The apparent diversity
of functions makes it clear that this common goal can-
not be described using function-specific language (e.g.
“edge filters”), but calls for an abstract framework. Here,
information theory is the obvious candidate. Notable
past approaches using information theoretic descriptions
of neural goal functions suggested to maximize the mutual
information between input and output [1], maximize the
coherent mutual information that all the inputs share
about the output [2], or, very generally, to minimize the
free energy [3]. To facilitate these efforts, and to better dis-
sect the implications of existing neural goal functions, we
suggest to build on a recent progress in information the-
ory, termed partial information decomposition (PID). PID
allows to measure which of a set of inputs contributes
either uniquely, redundantly or synergistically to the out-
put of a (neural) processing unit [4-7], and which fraction
of the output’s entropy remains unexplained by the input
set. We show how these measures can be used to identify
an information theoretic footprint of a neural goal
function. Most importantly, these measures can quan-
tify how much of the information is modified rather
than merely relayed when passing through the neural
processor [8]. This shifts the focus from information
transmission to more complex processing and allows a
much better understanding of the (theoretical?) cap-
abilities of a neuron or neural circuit. Using this
approach we show how to better understand existing
neural goal functions using PID measures and provide
an information theoretic framework for the design of
novel goal functions for artificial neural networks.
Authors’ details
1MEG Unit, Brain Imaging Center, Goethe University, Frankfurt 60528,
Germany. 2Ernst Strüngmann Institute for Neuroscience, Frankfurt 60528,
Germany. 3School of Natural Sciences, University of Stirling, Stirling FK9 4LA,
UK. 4School of Civil Engineering, The University of Sydney, Sydney, NSW
2006, Australia. 5Department of Nonlinear Dynamics, Max Planck Institute for
Dynamics and Self-Organization, 37077 Göttingen, Germany. 6Bernstein
Center for Computational Neuroscience, 37077 Göttingen, Germany.
Published: 18 December 2015
References
1.
Linsker R: Self-organization in a perceptual network. Computer 1988,
21(3):105-117.
2.
Kay JW, Phillips WA: Coherent Infomax as a computational goal for
neural systems. Bull Math Biol 2011, 73(2):344-372.
3.
Friston K, Kilner J, Harrison L: A free energy principle for the brain.
J Physiol Paris 2006, 100(1-3):70-87.
4.
Williams PL, Beer RD: Nonnegative Decomposition of Multivariate
Information. ArXiv10042515 Math-Ph Physicsphysics Q-Bio 2010.
5.
Bertschinger N, Rauh J, Olbrich E, Jost J, Ay N: Quantifying Unique
Information. Entropy 2014, 16(4):2161-2183.
6.
Griffith V, Koch C: Quantifying Synergistic Mutual Information. In Guided
Self-Organization: Inception. Springer Berlin Heidelberg;Prokopenko M
2014:159-190, [Emergence, Complexity and Computation, vol. 9].
7.
Wibral M, Lizier JT, Priesemann V: Bits from Brains for Biologically-Inspired
Computing. Frontiers in Robotics and AI 2015.
* Correspondence: Wibral@em.uni-frankfurt.de
1MEG Unit, Brain Imaging Center, Goethe University, Frankfurt 60528,
Germany
Full list of author information is available at the end of the article
Wibral et al. BMC Neuroscience 2015, 16(Suppl 1):P199
http://www.biomedcentral.com/1471-2202/16/S1/P199
© 2015 Wibral et al. This is an Open Access article distributed under the terms of the Creative Commons Attribution License (http://
creativecommons.org/licenses/by/4.0), which permits unrestricted use, distribution, and reproduction in any medium, provided the
original work is properly cited. The Creative Commons Public Domain Dedication waiver (http://creativecommons.org/publicdomain/
zero/1.0/) applies to the data made available in this article, unless otherwise stated.

8.
Lizier JT, Flecker B, Williams PL: Towards a synergy-based approach to
measuring information modification. Artificial Life (ALIFE), 2013 IEEE
Symposium on. IEEE 2013, S43-S51.
doi:10.1186/1471-2202-16-S1-P199
Cite this article as: Wibral et al.: Partial information decomposition as a
unified approach to the characterization and design of neural goal
functions. BMC Neuroscience 2015 16(Suppl 1):P199.
Submit your next manuscript to BioMed Central
and take full advantage of: 
• Convenient online submission
• Thorough peer review
• No space constraints or color ﬁgure charges
• Immediate publication on acceptance
• Inclusion in PubMed, CAS, Scopus and Google Scholar
• Research which is freely available for redistribution
Submit your manuscript at 
www.biomedcentral.com/submit
Wibral et al. BMC Neuroscience 2015, 16(Suppl 1):P199
http://www.biomedcentral.com/1471-2202/16/S1/P199
Page 2 of 2

