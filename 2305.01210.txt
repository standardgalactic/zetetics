Is Your Code Generated by ChatGPT Really Correct?
Rigorous Evaluation of Large Language Models for
Code Generation
Jiawei Liu
∗
Chunqiu Steven Xia
∗
Yuyao Wang
Lingming Zhang
University of Illinois Urbana-Champaign
Nanjing University
{jiawei6, chunqiu2, lingming}@illinois.edu
yuyao6@outlook.com
Abstract
Program synthesis has been long studied with recent approaches focused on directly
using the power of Large Language Models (LLMs) to generate code according
to user intent written in natural language. Code evaluation datasets, containing
curated synthesis problems with input/output test-cases, are used to measure the
performance of various LLMs on code synthesis. However, test-cases in these
datasets can be limited in both quantity and quality for fully assessing the functional
correctness of the generated code. Such limitation in the existing benchmarks begs
the following question: In the era of LLMs, is the code generated really correct?
To answer this, we propose EvalPlus – a code synthesis benchmarking framework
to rigorously evaluate the functional correctness of LLM-synthesized code. In
short, EvalPlus takes in the base evaluation dataset and uses an automatic input
generation step to produce and diversify large amounts of new test inputs using both
LLM-based and mutation-based input generators to further validate the synthesized
code. While EvalPlus is general, we extend the popular HUMANEVAL benchmark
and build HUMANEVAL+ with 81× additionally generated tests. Our extensive
evaluation across 14 popular LLMs (including GPT-4 and ChatGPT) demonstrates
that HUMANEVAL+ is able to catch signiﬁcant amounts of previously undetected
wrong code synthesized by LLMs, reducing the pass@k by 15.1% on average! For
example, the pass@k of widely studied open-source models like CODEGEN-16B
can drop by over 18.0%, while the performance of state-of-the-art commercial
models like ChatGPT and GPT-4 can also drop by at least 13.0%, largely affect the
result analysis for almost all recent work on LLM-based code generation. Moreover,
we even found several incorrect ground-truth implementations in HUMANEVAL.
Our work not only indicates that prior popular code synthesis evaluation results
do not accurately reﬂect the true performance of LLMs for code synthesis but also
opens up a new direction to improve programming benchmarks through automated
test input generation. We have open-sourced our tools, enhanced datasets as well
as all LLM-generated code at https://github.com/evalplus/evalplus to
facilitate and accelerate future LLM-for-code research.
1
Introduction
Automatically generating programs that accurately correspond to user intents is a long-standing
challenge in computer science known as program synthesis [19]. In the past few decades, classical
program synthesis techniques have been developed, including deductive synthesis [17, 31, 54],
∗Equal contribution. Author ordering decided by Nigiri.
arXiv:2305.01210v1  [cs.SE]  2 May 2023

def common(l1: list, l2: list):
    """Return sorted unique common elements for two lists"""
    common_elements = list(set(l1).intersection(set(l2)))
    common_elements.sort()
    return list(set(common_elements))
[5,3,2,8], [3,2]
[4,3,2,8], [3,2,4]
[4,3,2,8], []
[]
[2,3]
[2,3,4]
[6,8,1], [6,8,1]
[8,1,6]
HUMANEVAL inputs
HUMANEVAL+ input
not sorted!
ChatGPT synthesized code
correct
Figure 1: Exemplary wrong code synthesized by ChatGPT for HUMANEVAL #58. With new tests
provided by EvalPlus, the overall pass rate on this problem drops by 28.5%.
programming by examples [18, 49] and neural-guided synthesis [23]. More recently, with the advent
of Large Language Models [53, 6] (LLMs) and the abundance of code datasets, researchers have been
focusing on applying LLMs for direct code generation. LLMs like CODEX [10] and CODEGEN [38]
perform code generation by autoregressively predicting the next token given previous context, in
the form of function signature and docstring that denote the desired program functionality. The
generated code snippet is then combined with the context to form a complete function that aligns
with the user intent. Leveraging both natural language understanding and generative power, LLMs
have demonstrated impressive performance in code synthesis [3, 10].
The primary concern when it comes to the code generated by LLMs is its correctness. In contrast to
Natural Lanugage Processing (NLP), two syntactically different code snippets can be semantically
equivalent, rendering classic NLP metrics like BLEU score [42] unreliable. Ideally, we would like
to formally verify the correctness of LLM-provided solutions for any input, but verifying domain-
speciﬁc problems through methods such as translation validation [28, 36, 4] is already challenging
enough, let alone building a general veriﬁer with absolute certainty to prove arbitrary problems,
including those in code benchmarks. As such, the functional correctness of LLM-generated code
has been commonly assessed by testing. Speciﬁcally, popular code benchmarks for LLMs such as
HUMANEVAL [10] rely on hand-crafted test-cases, where LLMs’ solutions are considered correct if
they can pass all tests. Unfortunately, these unit-tests can be insufﬁcient, as manually constructing
high-quality tests is a laborious task, especially for complicated programs. Therefore, we argue
that existing code synthesis benchmarkings are inadequate for precisely assessing the functional
correctness of LLM-generated code, leading to false conﬁdence in the results. In summary, we
observed the common limitations in existing LLM-for-code benchmarks:
• Insufﬁcient testing. Current popular code evaluation datasets such as HUMANEVAL only
include on average less than 10 tests for each coding problem. Furthermore, these tests tend
to use relatively simple inputs and outputs that do not fully explore the functionality of the
code or corner cases. Figure 1 shows an example program synthesized by ChatGPT [40]
to return the sorted unique common elements from two lists. At ﬁrst glance, the function
looks correct and computes the desired output when using the base test inputs from the
HUMANEVAL dataset. However, notice that in the return statement, the function incorrectly
converts the intermediate list to a set (which is not order-preserving) before returning the
ﬁnal list, meaning that the returned list may not be sorted. Exposed by the additional input
from our improved HUMANEVAL+ benchmark, we can see that the output of the synthesized
function is no longer correct. This example shows that a logically ﬂawed solution may still
pass all tests, misconsidered correct due to testing inadequacy.
• Imprecise problem description. The input for code generation includes the function sig-
nature along with natural language descriptions. However, unlike code where constraints
can be formally deﬁned, natural language can be interpreted differently without precise and
careful formulation. This is especially true for code synthesis benchmark where inputs are
either hand-written [10, 3] or scraped from simple programming problems [27] without
special consideration for problem clarity. For example, the input docstring may not specify
the expected input domain (e.g., only positive integers) or how the function should handle
error conditions. As such, these imprecise problem descriptions can lead to different LLMs
synthesizing completely different code based on different interpretation of the problem.
• Incorrect ground-truth. To avoid data leakage of solutions in code synthesis benchmarks
and LLM training corpus, both problems and ground-truth solutions in these benchmarks are
usually hand-crafted. However, not only can the LLM-synthesized code be error-prone, but
even the ground-truth solutions provided within the evaluation dataset could contain errors.
2

This means these ground-truth solutions are not reliable oracles that can provide desired
outputs according to user speciﬁcations, leading to potentially generating incorrect outputs
when given additional test inputs outside of the base test cases.
These limitations described above are common across many popular code generation benchmarks [10,
3, 27]. This not only questions the validity of the impressive performance in code generation achieved
by LLMs but also sets a challenge on how we should properly evaluate the code generated by
LLMs. In this paper, we aim to address this fundamental evaluation challenge and ask the following
introspective question: Is the code generated by LLMs really correct?
Our proposal. In this work, we set out to answer the important question and evaluate the evaluation
dataset. Consequently, we construct EvalPlus – an evaluation framework to improve the evaluation
of the code generated by LLMs for functional correctness. At the heart of EvalPlus is an automatic
test input generation technique which augments the inadequate tests used in existing code synthesis
benchmarks by generating complex test inputs to fully exercise the desired behaviors of the code
solution. EvalPlus takes in an evaluation dataset (e.g., HUMANEVAL) and ﬁrst clariﬁes any imprecise
code descriptions with program contracts, i.e., preconditions over the function arguments. Such
preconditions (e.g., expected range of an index) for well-formedness constrain the inputs of the
function and complement the natural language description (e.g., docstring) to improve problem
clarity. Next, EvalPlus adopts both LLM- and mutation-based test input generation [48, 66, 39]
methods to generate new test inputs on top of the original evaluation inputs. More speciﬁcally, we
focus on generating and diversifying test inputs to fully practice program behaviors. EvalPlus ﬁrst
uses ChatGPT [40] to generate a set of high-quality seed inputs that not only aims to test difﬁcult
corner cases and functionalities of the program, but also conforms to the expected syntactic and
semantic structure of the program input space. Using these high-quality seed inputs, EvalPlus then
performs type-aware mutation to efﬁciently generate a large number of additional test inputs. These
newly generated test inputs are then used to evaluate the LLM-generated code through differential
testing [32] against the ground-truth implementation.
While EvalPlus is generalizable to all code synthesis benchmarks, in this work, we focus on HU-
MANEVAL [10], one of the most widely used datasets for LLM program synthesis evaluation. To this
end, we build on top of HUMANEVAL and construct a new dataset – HUMANEVAL+ using EvalPlus
to automatically generate diverse and complex inputs which supplement the test inadequacy problem
in HUMANEVAL. We evaluate multiple recent and popular state-of-the-art LLMs (GPT-4 [41],
ChatGPT [40], CODEGEN [38], GPT-J [55], GPT-NEO [5] INCODER [16], SANTACODER [2], Poly-
Coder [61], VICUNA [11] and StableLM-α [52]) and demonstrate that our improved HUMANEVAL+
can catch a signiﬁcant amount of incorrect LLM-synthesized code that base HUMANEVAL cannot,
showing EvalPlus-enhanced benchmarks can more rigorously and accurately evaluate the functional
correctness of LLM-generated code. Furthermore, we carefully wrote our own ground-truth solutions
to each task in HUMANEVAL and compare against the original ground-truth. By comparing the
outputs between our and original ground-truth, we discovered several bugs in the ground-truth of base
HUMANEVAL! Our results not only conﬁrm our hypothesis that current code synthesis evaluation
benchmarks lack both quantity and quality of test inputs which are needed to properly evaluate
LLM-generated code but also open up a new direction to design better benchmarks for code synthesis.
Contribution. Our work revisited and proposed to improve programming benchmarks for LLMs:
• Study: We are the ﬁrst to study the existing code-generation benchmarks for LLMs. Our
study demonstrates the inadequacy of existing handcrafted test-cases for evaluating func-
tional correctness and efﬁciency. Our work opens up a new dimension for future research to
closely examine and improve benchmarks for better evaluation of code synthesis.
• Approach: We propose EvalPlus – an evaluation system and framework containing a
fully automated test input generation mechanism to rigorously and extensively evaluate
LLM-synthesized code. Our input generation uses an LLM-based strategy to ﬁrst generate
high-quality seed inputs to target difﬁcult or corner-case functionalities and then further
efﬁciently extend large amounts of test inputs using a type-aware mutation strategy from
the seed inputs. In addition, we propose to use program contracts (e.g., preconditions) to
not only improve our input generation (ﬁlter out invalid inputs) but can act as a ﬁrst step to
complement the natural language problem description and improve problem clarity.
3

def sample_56(input):
... 
def sample_2(input):
... 
def sample_1(input):
... 
input
def groundtruth(input):
... 
input
input
groundtruth function
base inputs
original dataset
seed pool
  ChatGPT
seed inputs
type-aware 
mutation
def sample_0(input):
... 
LLM samples
differential
testing
def sample_11(input):
... 
Rigorously-validated 
LLM samples
pass
generate corner inputs cases
generate difficult inputs
generate complex inputs
error-inducing 
inputs
test inputs
new inputs
def groundtruth(input):
... 
groundtruth function
EvalPlus dataset
new input 
Figure 2: Overview of EvalPlus
• Results and Findings: Using EvalPlus, we build on top of the popular HUMANEVAL eval-
uation dataset and produce HUMANEVAL+ which adds additional diverse and effective test
inputs to fully evaluate the functionality of LLM-synthesized code. We use HUMANEVAL+
and evaluate 14 popular state-of-the-art LLMs (e.g., GPT-4, ChatGPT and CODEGEN),
across different model types and sizes, and ﬁnd that surprisingly the pass@k on the new
dataset is on average 15.1% lower than the base HUMANEVAL. For example, the pass@k
of widely studied open-source models like CODEGEN-16B can drop by over 18.0%, while
the performance of state-of-the-art commercial models like ChatGPT and GPT-4 can also
drop by at least 13.0%, largely affect the result analysis for almost all recent work on
LLM-based code generation. This demonstrates that the prior program synthesis evaluation
dataset is not complete and sufﬁcient to evaluate the performance of LLM-generated code.
Additionally, we found that even the ground-truth implementations of HUMANEVAL contain
errors, further calling into question the validity of program synthesis benchmarks.
2
Approach
We propose EvalPlus, an automatic test generation approach to improve evaluation correctness for
LLM code generation by combining ChatGPT-based [40] test generation with a traditional mutation
method to generate a large set of diverse and complex inputs. Researchers have shown that LLMs can
be effective in test generation [26, 12, 13, ?, 37, 64] to produce both syntactically and semantically
meaningful inputs using knowledge gained during pre-training. In this work, we leverage the powerful
code and natural language understanding ability of LLMs through prompting to initialize complex
corner-case inputs and further augment with traditional mutation-based test generation [66, 48, 39] to
produce a large set of valid test inputs for rigorous code-synthesis evaluation.
Figure 2 shows the overview of EvalPlus. We ﬁrst take in as input the original dataset containing
the ground-truth implementation as well as the base inputs used for evaluation. EvalPlus then
constructs a prompt using the original ground-truth, several exemplary test inputs as demonstration
and a specialized instruction to query ChatGPT and generate high-quality seed inputs. ChatGPT, by
following base input formats and inspecting the ground-truth implementation, can serve as a vehicle
to generate valid yet rigorous test inputs to trigger interesting program behaviors. These seed inputs
then act as a starting point for our type-aware mutation to quickly generate a large number of new
test inputs. Both the ChatGPT seed inputs and mutated inputs are combined together to form the ﬁnal
test inputs used to extensive evaluate the functional correctness of LLM-generated code.
Using the complex and diverse test inputs generated, EvalPlus then begins to evaluate the functional
correctness of LLM-synthesized code. We use differential testing [32] as the oracle to compare the
output of the ground-truth implementation with each LLM-generated solution and ﬁlter out any
4

error-prone code synthesized by LLMs. The set of LLM samples which produce the same output
as the ground-truth across all generated test inputs are considered rigorously tested to perform
the desired functionality of the user intent. As the ﬁnal output, EvalPlus obtains a new dataset
using the generated high-quality test inputs which can fully evaluate the functional correctness of
LLM-synthesized code. We now describe each of the sub-approaches in detail.
2.1
Automated Test Input Generation
Seed initialization via ChatGPT. EvalPlus ﬁrst uses ChatGPT to generate a set of high-quality
seed inputs for later mutation. Following Figure 2, we construct a prompt using (i) the ground-truth
solution of the problem for ChatGPT to inspect; (ii) the set of test inputs as demonstration; and (iii) an
instruction to encourage ChatGPT to come up with diverse and complex inputs. Speciﬁcally, during
each ChatGPT generation step, we start with the ground-truth implementation and then randomly
sample a set of test inputs from the existing dataset to construct the prompt. We then ﬁnalize the
prompt with our instruction (we use a collection of different instructions for ChatGPT, see Figure 2)
and query ChatGPT to produce new inputs. EvalPlus aims to leverage the powerful understanding
ability of ChatGPT to learn both the valid input formats (e.g., variable types) as well as the desired
functionality of the ground-truth implementation in order to come up with additional inputs to reveal
bugs in wrong code produced by LLMs. Programs can have their own expected input domains, where
out-of-domain inputs (i.e., invalid inputs) should not be passed into the function as it could lead to
undeﬁned behaviors which raise undesired false-positives in differential testing. As such, we ﬁlter out
any invalid inputs which violate the input precondition required by the ground-truth implementation.
By using ChatGPT as an automated generation engine, we can generate inputs that are valid even
under semantic constraints. For example, a programming problem may require the input to conform
to a speciﬁc structure (e.g., a palindrome). Without manual effort, such semantic constraints can
be extremely difﬁcult for traditional input generators to satisfy. By providing ChatGPT with the
original ground-truth implementation, it can capture the desired input structure by understanding the
semantics of the function to generate valid and complex test inputs. However, ChatGPT is unsuitable
for large amounts of automated test generation due to undesired speed and cost of querying such a
large model. To address this issue and still create a rich set of test inputs, we perform type-aware
input mutation starting from high-quality seed inputs generated by ChatGPT.
Type-aware input mutation. We adopt a typical mutation-based workﬂow [66, 48] to continuously
create more inputs: (i) an initial corpus of seed inputs from ChatGPT are used to initialize the seed
pool and bootstrap the generation pipeline; (ii) each time an input (i.e., seed) from the seed pool is
randomly selected to be mutated to a new input (i.e., mutant); and (iii) new inputs that comply with
the input constraints (§2.2) of the ground-truth function are added to the seed pool and we start over
from (ii) to continue the generation process.
To efﬁciently create more valid inputs, we leverage type-aware mutation [57] in step (ii) which
inspects the data types of the incoming valid seeds and generates new inputs that are structurally
similar to the seeds. We illustrate our approach by ﬁrst assuming the input types following the
grammar below. The exemplary grammar includes both primitive types (e.g.,int) and compound
types (e.g.,List[T] denotes a list type where all elements are typed as T). Table 1 illustrates the basic
mutators used for different (templated) types of inputs. For simple primitive types such as int and
float, the mutation is as simple as incrementing/decrementing the value. For compound types and
the string type (i.e., str), besides generally removing or repeating existing elements (or sub-strings
for str), the elements and sub-strings can be mutated recursively according to their inner types. Such
sub-mutants can then be used to replace existing items or add new items in a ﬁner-grain manner.
⟨T⟩
::=
List[T] | Tuple[T] | Set[T] | Dict[Prim, T] | Prim
⟨Prim⟩
::=
float | int | bool | str | NoneType
In addition, programming tasks may require semantical constraints in addition to type constraints,
e.g., the inputs could be required to be strings representing numbers. To alleviate generating invalid
inputs that violate such semantic constraints, following [20], we additionally apply an ingredient
mechanism to collect appeared data fragments and reuse them during mutation. More speciﬁcally,
5

Table 1: List of basic type-aware mutations over input x.
Type
Mutation
int | float
Returns x ± 1
bool
Returns True or False randomly
NoneType
Returns None
str

Remove/repeat a sub-string s
Replace a sub-string s with Mutate(s)
List[T]

Remove/repeat a random item x[i]
Insert/replace x[i] with Mutate(x[i])
Tuple[T]
Returns Tuple(Mutate(List(x)))
Set[T]
Returns Set(Mutate(List(x)))
Dict[K,V]
( Remove a key-value pair k →v
Update k →v to k →Mutate(v)
Insert Mutate(k) →Mutate(v) from k →v
for any new and valid (i.e., conforming the contracts) mutants and seeds, we recursively visit its
sub-elements to catch primitive values typed in {int, float, str}. The collected primitives will
be used as return values of mutating corresponding types at a certain probability. We also include
sub-strings tokenized by splitting visited strings over whitespaces. In short, type-aware input mutation
builds on the high-quality seed inputs produced by ChatGPT to generate large amounts of test inputs
which we use as the ﬁnal set of extensive test inputs used to evaluate LLM-synthesized code.
2.2
Program Input Contracts
The goal of evaluating code synthesis is to check whether the synthesized code accurately reﬂects
the desired user intent and is done through using several test inputs and comparing the output of
the generated code against the ground-truth solution. The prior sections demonstrated how we can
improve the test inputs used to evaluate the synthesized code for more rigorous evaluation. However,
these user intents (expressed as natural language docstring) can be unclear and vague. As such, LLMs
might allow for different interpretations of the desired functionality, unclear input formats as well as
how to handle corner cases. Therefore, to properly evaluate code synthesis, we require a dataset that
clearly illustrates each task and the desired input and output behaviors.
To accomplish this, we adopt a programming by contract [33] philosophy by systematically adding
code assertions as contracts (e.g., assert n > 0) to ensure the test inputs for the function are well-
formed. The beneﬁts of the code contracts are two-fold: (i) they serve as orthogonal descriptors
together with the natural language description to further clarify the task to be solved; and (ii)
they can complement the automatic input generation steps to ﬁlter out any generated inputs (e.g.,
through random generation) which violate the contracts. Such ill-formed inputs can trigger undeﬁned
behaviors and thus can be unreasonable to use for evaluation of LLM-synthesized code.
3
Experimental Setup
Metrics. To evaluate the functional correctness of code generated by LLM, we focus on the widely
used and reliable pass@k metric [10]. Our implementation uses the unbiased estimator version [10]
to determine the proportion of tasks in a dataset that can be solved by passing all tests within k tries.
Dataset. The evaluation is conducted over HUMANEVAL [10], one of the most popular datasets for
code generation2 and has been used to evaluate many recent code-based LLMs such as CODEX [10],
CODEGEN [38], PolyCoder [61], and INCODER [16]. HUMANEVAL consists of 164 human-written
programming tasks, each of which provides a Python function signature and a docstring as the input
to the LLM. Based on this input, LLMs complete a solution whose functional correctness is judged
by a handful of manually-curated test-cases (Table 2). Speciﬁcally, we evaluate LLM-generated code
over the base HUMANEVAL, as well as the new HUMANEVAL+ dataset created by EvalPlus.
2Top-1 HuggingFace downloads on March, 2023. https://hf.co/datasets?other=code-generation
6

Table 2: Overview of HUMANEVAL and EvalPlus-improved versions.
#Tests
#Task
Avg.
Medium
Min.
Max.
HUMANEVAL
9.6
7
1
105*
164
HUMANEVAL+
774.8
986
12
1,105
* Only four HUMANEVAL tasks (e.g., add two numbers) have over 100
tests, as random inputs are generated to cross-check with the ground-
truth. Except these, the maximum number is 26 and the average is 7.3.
Table 3: Overview of evaluated models. “Mon. DLs” in the Popularity column refers to the number
of model downloads on HuggingFace for the last month (Apr. 2023).
Model Name
Sizes
Release Date
Open-source
Popularity
Coding
CODEGEN [38]
{2B, 6B, 16B}
2022

3.2k stars
SANTACODER [2]
{1.1B}
2023

206k Mon. DLs
INCODER [16]
{1.1B, 6.7B}
2022

2k Mon. DLs
PolyCoder [61]
{2.7B}
2022

2k stars
General
GPT-4 [41]
N/A
Mar. 2023
ChatGPT [40]
N/A
Nov. 2022
100M+ users
VICUNA [11]
{7B, 13B}
Mar. 2023

16k stars
StableLM-α [52]
{7B}
Apr. 2023

11k stars
GPT-J [55]
{6B}
2021

5.8k stars
GPT-NEO [5]
{2.7B}
2021

7.7k stars
Creating HUMANEVAL+. EvalPlus transforms HUMANEVAL to HUMANEVAL+ by adding 81×
unique test-cases and ﬁxing incorrect ground-truth solutions from HUMANEVAL. More speciﬁcally,
for each task, based on around 30 ChatGPT-generated seed inputs (produced using 3 separate Chat-
GPT prompts), we run type-aware mutation to generate new inputs until 103 test inputs are generated
or the one-hour timeout is reached. To avoid undeﬁned behaviors, we only keep new inputs that
comply with the corresponding program contracts. In HUMANEVAL+, 83 out of the 164 program-
ming tasks are annotated with hand-crafted contracts. EvalPlus requires ground-truth solutions for
cross-checking with LLM-generated code. Although ground-truth (i.e., “canonical_solution”) is
provided by HUMANEVAL, over 10% of them are spotted to be ﬂawed (detailed in §4.2). Therefore,
as another contribution, we carefully re-implemented and tested the ground-truths for HUMANEVAL+.
Evaluation of LLMs. Our goal is to comprehensively evaluate recent and widely used LLMs, both
specialized for code generation [38, 61, 16, 2] and general-purpose tasks [41, 40, 11, 52, 55, 5].
Table 3 presents an overview of the studied models, with column Sizes reﬂecting the model sizes
in billions of parameters, Release Date showing when the LLM is released, Open-Source marking
the models whose weights are publicly available, and Popularity indicating the number of GitHub
stars, users, or downloads of the LLM. In total, we evaluate 14 of the most representative and popular
LLMs with a broad range of conﬁgurations to fully demonstrate the generalizability of our results.
Our hyper-parameter conﬁgurations follow prior work [10, 38]. For each model we randomly sample
200 programs and repeat the experiments over a group of softmax temperature settings, namely
{0.2, 0.4, 0.6, 0.8} where the best-performing one is selected as the representative result. Additionally,
we also evaluate by performing greedy decoding with temperature of 0 as another evaluation setting.
We use nucleus sampling [21] with top p = 0.95 if conﬁgurable (i.e., HuggingFace APIs). By default,
we let each model generate at most 512 new tokens and truncate the produced code with end-of-ﬁle
(EOF) identiﬁers suggested in HUMANEVAL [10], as well as those favoured by certain models (e.g.,
“<|endoftext|>” and “\n```”). For conversational models (i.e., ChatGPT and GPT-4), we obtain
the code fragments by parsing the code blocks (i.e., within “```”) in the output. We found ChatGPT
tends to repeat problem description with detailed explanation, which can consume more than 512 new
tokens to complete a solution for around 11% of problems. To align ChatGPT with other models, for
tasks with very long problem descriptions, we extend the token limit from 512 to 1024. Furthermore,
due to the time and cost of querying ChatGPT API, we only evaluate it using greedy decoding and
7

Table 4: Evaluating LLMs on HUMANEVAL and HUMANEVAL+. All models, except for INCODER
and SANTACODER which perform inﬁlling, perform auto-regressive generation. k=1⋆marks pass@1
done with greedy decoding with temperature of 0. T ∗
k denotes the optimal pass@k temperature.
Size
pass@k [%]
k=1⋆
k=1
k=10
k=100
T ∗
1
T ∗
10
T ∗
100
GPT-4
N/A
before
88.4
after
76.2
ChatGPT
N/A
before
73.2
69.4
88.6
94.0
after
63.4
61.7
81.2
89.8
CODEGEN
2B
before
24.4
18.4
39.8
66.8
.2
.8
.8
after
20.7
15.1
34.8
55.8
.2
.2
.8
6B
before
29.3
27.7
46.9
72.7
.2
.6
.8
after
25.6
23.6
41.0
64.6
.2
.6
.8
16B
before
32.9
32.2
56.0
81.5
.2
.6
.8
after
26.8
27.2
48.4
71.4
.2
.6
.8
VICUNA
7B
before
11.6
11.0
24.2
42.9
.2
.6
.6
after
10.4
9.9
20.0
34.7
.2
.6
.6
13B
before
17.1
15.5
30.5
55.0
.2
.8
.8
after
15.2
13.7
25.9
45.8
.2
.8
.8
SANTACODER
1.1B
before
14.6
16.6
29.3
45.9
.4
.6
.8
after
12.8
14.1
26.3
40.6
.4
.6
.8
INCODER
1.1B
before
12.2
10.0
15.9
25.2
.2
.6
.6
after
10.4
7.9
13.5
20.7
.2
.6
.4
6.7B
before
15.9
15.6
27.7
45.0
.2
.4
.6
after
12.2
12.4
22.2
38.9
.2
.6
.6
GPT-J
6B
before
12.2
11.3
17.7
31.8
.2
.6
.6
after
10.4
9.5
15.2
25.9
.2
.6
.6
GPT-NEO
2.7B
before
7.9
6.5
11.8
20.7
.2
.6
.6
after
6.7
6.0
9.0
16.8
.2
.6
.6
PolyCoder
2.7B
before
6.1
5.9
10.2
17.1
.2
.4
.6
after
5.5
5.3
7.9
13.6
.2
.6
.6
StableLM-α
7B
before
2.4
2.7
7.5
15.8
.2
.6
.6
after
2.4
2.6
6.2
11.9
.2
.6
.6
0.8 temperature. Additionally, GPT-4 is even more costly than ChatGPT (∼10×), as such, we
only evaluate it over greedy decoding. For model implementation, we run ChatGPT and GPT-4
via OpenAI APIs, and accelerate CODEGEN-6B and -16B with NVIDIA FasterTransformer via
FauxPilot [14]. All other LLMs are based on the HuggingFace transformers library. Due to the
discontinuation of CODEX APIs [10], unfortunately it is not included in the benchmark.
Test oracles. An LLM-produced solution is regarded to be correct if for all test inputs it returns
values that match the expected outputs within a reasonable run time. We perform exact matching by
default. For ﬂoating-point comparisons, we tolerate absolute differences to the degrees annotated in
HUMANEVAL or 10−6 if not annotated. In original HUMANEVAL, the default timeout is set to three
seconds to run the whole test-suite (i.e., all test-cases) for each programming problem. Such a setting
is neither suitable when having more test-cases nor reasonable as each problem could have its own
run time characteristics. Consequently, we let the timeout for each test-case to be max(50ms, 2 × tgt)
where tgt refers to the execution time of the corresponding ground-truth solution. In other words, we
expect the LLM-provided solution to be no slower than the ground-truth by two times or use a base
50-millisecond timeout when 2 × tgt < 50ms to avoid variance caused by performance randomness.
4
Evaluation
4.1
Functional Correctness Modulo Rigorous Inputs
Table 4 shows the pass@k metric when evaluating LLMs using both the base HUMANEVAL and
HUMANEVAL+. We ﬁrst observe that across all LLMs, models sizes and k values, when evaluating
8

Table 5: Misimplemented ground-truth solutions in HUMANEVAL
Unhandled edge-case
Bad logic
Performance issue
Total
# ground-truths
5
10
3
18
using HUMANEVAL+, all pass@k results consistently drop compared to evaluating using the base
HUMANEVAL dataset. Notably, the performance drop is signiﬁcant with on average 15.1% reduction
in pass@k across all models and k values. Such performance decrease is not only seen in popular
open-source LLMs (e.g., CODEGEN) but also observed in state-of-the-art commercial ChatGPT
(13.4% reduction) and GPT-4 (13.8% reduction) models. Not only are these LLMs widely used
by downstream users to aid in daily programming but they are also commonly used as reference
approaches to evaluate new code synthesis techniques, leading to potential signiﬁcant impact on
claimed relative improvement when evaluated on our more robust benchmark. This conﬁrms our
hypothesis that the prior evaluation on HUMANEVAL is not robust enough to detect LLM-synthesized
wrong code. By using HUMANEVAL+, we perform a more rigorous evaluation with additional and
automatically generated test-cases focused on functionality of the generated programs, leading to the
more accurate pass@k results that align with the real performance of LLMs for code synthesis.
Table 4 also shows that smaller LLMs speciﬁcally trained on code (e.g., CODEGEN-6B, INCODER-
6.7B and SANTACODER) are found to match or even largely outperform the general-purpose LLMs
(e.g., VICUNA-7B, GPT-J-6B and StableLM-α-7B). Meanwhile, CODEGEN-16B seems to match
around 78% of performance compared with ChatGPT which is deemed to use a much larger parameter
setting (i.e., GPT-3.53). We observed that a more rigorous evaluation could yield totally different or
even contradictory results. For example, on HUMANEVAL VICUNA-13B was considered similar to
INCODER-6B on pass@1, whereas HUMANEVAL+ draws the opposite conclusion, with VICUNA-13B
outperforming the other by 10%. Similarly, the pass@1⋆of SANTACODER-1.1B on HUMANEVAL
was 9% wrose than that of INCODER-6B, but actually outperforms the other by 5% on HUMANEVAL+.
Table 4 further illustrates the distribution of best-performing temperature values that achieve the
highest pass@k over different k. Our results conforms with prior ﬁndings [10] that a lower temperature
tends to perform better for smaller k, while a higher temperature works better for larger k. We also
observe that the best-performing temperatures seem to stay fairly consistent before and after using
HUMANEVAL+; however, we do observe slight differences, e.g., CODEGEN-2B’s best temperature
for pass@10 becomes 0.2 from 0.8 after using HUMANEVAL+. Nonetheless, this slight difference
motivates future research to look more closely on the effect of temperature with respect to the
robustness of the evaluation tests, esp. those edge-cases.
Figure 3 illustrates for each programming task the difference of pass rate between HUMANEVAL
and HUMANEVAL+ tests. By comparing the gap between HUMANEVAL and HUMANEVAL+ pass
rates (i.e., height difference for each bar), it shows overall HUMANEVAL+ is able to detect signiﬁcant
proportions of HUMANEVAL-misidentiﬁed solutions for problems of all levels of difﬁculties. We also
observe that not all problems in HUMANEVAL are equal, not only in terms of problem difﬁculty but
the difﬁculty of generating counter-examples and edge-cases to fully exercise LLM-generated code.
For simple problems such as “adding two numbers” and “length of a string” (i.e., the top-2 solvable
problems), it is trivial to solve for LLMs (as demonstrated by the high pass rate on the right-hand
side of Figure 3) and easy to test with manual test-cases. While problems dealing with multiple
conditions (e.g., “word splitting”), efﬁciency requirements (e.g., “n-th prime Fibonacci number”)
and reasoning ability (e.g., “Tribonacci sequence”) are the most challenging (i.e., unsolvable) to the
LLMs being evaluated, positioning future research directions to improve LLMs for conquering such
coding skills. By using EvalPlus, we create HUMANEVAL+ with the goal to ﬁll in these weak tests
within HUMANEVAL through automated test input generation to more precisely assess the functional
correctness of LLM-generated code.
4.2
Incorrect “Ground-truth” in HUMANEVAL
In addition to detecting more incorrect LLM-generated code snippets using EvalPlus, we also found
multiple defects even in the original ground-truth in HUMANEVAL. Such defects are detected also
3GPT-3.5’s model size is not ofﬁcially disclosed, but is unlikely to be smaller than GPT-3 (175B).
9

Problems (Sorted by HumanEval pass rate)
10−2
10−1
100
101
102
Average Pass Rate (%)
HumanEval
HumanEval+
Figure 3: Pass rate distribution for each programming problem, with and without extra tests introduced
by HUMANEVAL+. X-axis spans bars for all 164 problems, sorted the HUMANEVAL pass rate (i.e.,
difﬁculty). Y-axis shows the log-scale pass rates averaged by all LLM-generated samples in Table 4.
def valid_date(date):
  ...
  if month in [1,3,5,7,8,10,12] and day < 1 or day > 31:
    return False
  if month in [4,6,9,11] and day < 1 or day > 30:
    return False
  ...
12-31-1999
False
HUMANEVAL+ input  
12/31/1999 
is a valid date!
A bracket is needed!
Figure 4: Exemplary incorrect-logic ground-truth solution in HUMANEVAL (#124)
through differential testing but between our own carefully constructed ground-truth and the original
ground-truth in HUMANEVAL. Table 5 shows the number of incorrect ground-truth implementations
in the original dataset classiﬁed as 1) Unhandled edge-case: failing to/incorrectly handles corner-case
inputs (e.g., empty list or string) 2) Bad logic: incorrectly implements the desired functionality
3) Performance issue: inefﬁcient implementations leading to slow performance on reasonably-
sized inputs. In total, we detected 18 (11% of problems) incorrect ground-truth implementations in
HUMANEVAL. Among those, bad logic (10) is the most serious as the original “ground-truth” does
not accurately reﬂect the user intent.
Figure 4 shows an incorrect ground-truth implementation (validate_date) from HUMANEVAL
classiﬁed as having bad logic. The desired task is to check if the input date format is correct to
the speciﬁcation (e.g., less than 31/30 days for certain months). We see that in the core logic,
the conditions attempt to ﬁrst check the month condition and then handle the corresponding day
conditions. However, this is implemented incorrectly as “and” in Python4 has higher precedence than
“or”, leading to the ground-truth function to check if either conditions satisﬁes instead of the desired
both conditions must satisfy. This is exposed via our automatically generated test input of 12-31-1999
where the ground-truth implementation incorrectly labels this as not a valid date. Surprisingly this
egregious error is not exposed by any of the base test inputs in HUMANEVAL, further demonstrating
the weakness and limited evaluation power of the original test inputs.
5
Related Work
LLMs for code. The use of LLMs for code has gained traction in recent years, owing to the abundance
of publicly available code on platforms like GitHub and the need for improving developer efﬁciency.
LLMs have demonstrated state-of-the-art performance on various code-related tasks, including code
generation/synthesis [10, 27, 22], program repair [59, 60, 58], code translation [25, 44] and code
summarization [1, 29]. In particular, prominent LLMs including CODEX [10], CODEGEN [38], IN-
CODER [16] and PolyCoder [61], have been developed and extensively evaluated for code generation
(widely recognized as the holy grail for computer science research since the inception of AI in the
1950s [19]), where the model generates code snippets based on natural language descriptions (e.g.,
docstring) of the desired functionality.
4https://docs.python.org/3/reference/expressions.html#operator-precedence
10

In contrast to natural language, code carries well-deﬁned semantic meanings, where two syntactically
different code segments can be semantically equivalent. As such, LLM-based code synthesis is
largely evaluated based on functional correctness, which is typically assessed by running test-cases
to check the desired outputs. HUMANEVAL [10] is one of the pioneering and most widely studied
human-written benchmarks for LLM-based code synthesis, consisting of 164 pairs of Python function
signature with docstring and the associated test-cases for correctness evaluation. Additionally, each
HUMANEVAL problem is also equipped with a reference solution. Another Python-focused dataset,
MBPP [3], is created by crowd-sourcing participants to write in summation 974 programming prob-
lems, each of which is comprised of the problem statement (i.e., docstring), the function signature,
as well as three test-cases for evaluation. Of these 974 problems, 427 hand-veriﬁed problems form
MBPP-sanitized, a well-formed subset of MBPP problems with more precise problem descriptions,
normalized function signatures and better-designed test-cases. Beyond Python, there are other bench-
marks targeting additional languages such as Spider [65] (SQL generation), HUMANEVAL-X [68]
(C++, Javascript and Go), CodeContests [27] (C++ and Java) and MultiPL-E [8] (extending HU-
MANEVAL and MBPP to 18 programming languages). Our work EvalPlus shows for the ﬁrst time the
test inadequacy problem of prior popular benchmarks for assessing the correctness of code generated
by LLMs and addresses the issue via automatic test generation to complement existing benchmarks.
Automated test generation. Automated test generation is a widely used technique to spot software
defects with automatically generated tests. Black-box test generation such as fuzz testing [35, 62, 46,
20] feeds random test inputs (e.g., random bytes) to the system under test (SUT), without looking
into its source code. Traditional black-box techniques can mainly be categorized into generation-
based [62, 20, 46] and mutation-based [57, 9, 39] ones. While generation-based techniques aim to
directly generate test inputs from scratch, mutation-based techniques apply systematic mutation
changes on seed inputs to generate more diverse tests. As a fundamental limitation of “blindness”,
black-box methods may hardly produce well-designed test-cases to exhaustively explore deep code
paths. White-box approaches provide better-quality test-cases by analyzing the source code of SUT.
For instance, symbolic execution [24, 7] breaks the coverage plateaus by solving symbolic path
constraints to generate tests targeting deep paths. Because constraint solving is costly, both algorithmic
(e.g., concolic testing [30, 47]) and implementation (e.g., compilation [43]) solutions have been
proposed to alleviate the scalability issue. As a mid-point of black- and white-box testing, coverage-
guided fuzzing [66, 48] (i.e., grey-box) uses the coverage information of SUT as feedback to adjust
the input generation and mutation. Unfortunately, traditional white-, black- and grey-box methods are
inapplicable to generating semantically meaningful inputs for arbitrary problems programmed in a
dynamic-type language. To address this, we propose a mixed approach to use ChatGPT to inspect the
ground-truth implementation (i.e., white-box) for initializing interesting seed inputs, based on which
type-aware mutation (i.e., black-box) can efﬁciently scale the test inputs to a large amount.
More recently, researchers have also directly applied modern LLMs for test generation due to their
ability to synthesize both syntactically and semantically valid inputs. TeCo [37] and TestPilot [?] both
use LLMs (CodeT5 [56] and CODEX [10] respectively) for unit test generation through prompting
with target functions and example usages. CodeMosa [26] is a code coverage-based technique
that equips traditional search-based software testing (SBST [15]) with CODEX for better unit test
generation. In addition, TitanFuzz [12] demonstrates for the ﬁrst time that modern LLMs can be
directly leveraged for fuzzing real-world systems and discover critical bugs/vulnerabilities. More
speciﬁcally, TitanFuzz applies both inﬁlling (INCODER) and auto-regressive (CODEX) LLMs to
generate large sets of inputs to fuzz [35] Deep Learning libraries, demonstrating that LLMs can
perform both traditional generation-based [63] and mutation-based [20] fuzzing studied for decades.
More recently, FuzzGPT [13] builds on top of the idea of TitanFuzz to apply additional ﬁne-tuning
or prompting with historical bug reports to produce more unusual program (i.e., edge-cases) for
more powerful fuzzing. In our work, we built on the insights of prior work and ﬁrst use the powerful
understanding and generation ability of LLMs (e.g., ChatGPT) to produce high-quality seed inputs
that conform to input constraints. We then further generate using traditional mutation-based strategies
to efﬁciently obtain a large set of diverse and complex test inputs.
6
Conclusion & Future Work
In this work, we present EvalPlus – a rigorous evaluation framework for program synthesis, driven
by automated test generation. We propose to combine both LLM-based test input generation (through
11

prompting with ChatGPT) and traditional mutation-based input generation to obtain a large set of
diverse test inputs, in order to accurately evaluate the functional correctness of LLM-generated
code. EvalPlus creates HUMANEVAL+, built on top of the popular HUMANEVAL with additional
high-quality and automatically generated test inputs. We extensively evaluate the new benchmark on
several state-of-the-art and widely-used LLMs and show that HUMANEVAL+ can identify a signiﬁcant
amount of previously undetected incorrect solutions proposed by LLMs, demonstrating that EvalPlus
is effective to augment synthesis evaluation datasets for more accurate and rigorous evaluation.
One on-going work of EvalPlus is to bring better-quality testing to more and more code benchmarks
such as MBPP. In the future, one can also explore more and better test generation techniques
to keep improving the benchmark quality. Meanwhile, high-quality test suites may include large
numbers of tests, unnecessarily slowing down the code generation evaluation. Therefore, we are
also leveraging test suite reduction techniques [67, 50] widely studied in the software engineering
community to further systematically reduce redundant tests while maintaining the test effectiveness.
Furthermore, formal veriﬁers can be used to prove functional correctness but are oftentimes used for
domain-speciﬁc programs, e.g., SMT solvers for arithmetic problems [51]. Future work can look
into how to integrate EvalPlus with more formal veriﬁcation tools (e.g., translation validation [28])
to provide stronger guarantees of the evaluation results when applicable. Additionally, the core test
generation technique behind can be even used to remind developers of potential ﬂaws of the accepted
LLM-generated code snippets when doing AI pair-programming (e.g., Copilot [34]).
References
[1] T. Ahmed and P. Devanbu. Few-shot training llms for project-speciﬁc code-summarization.
In 37th IEEE/ACM International Conference on Automated Software Engineering, pages 1–5,
2022.
[2] L. B. Allal, R. Li, D. Kocetkov, C. Mou, C. Akiki, C. M. Ferrandis, N. Muennighoff, M. Mishra,
A. Gu, M. Dey, et al. Santacoder: don’t reach for the stars! arXiv preprint arXiv:2301.03988,
2023.
[3] J. Austin, A. Odena, M. Nye, M. Bosma, H. Michalewski, D. Dohan, E. Jiang, C. Cai, M. Terry,
Q. Le, and C. Sutton. Program synthesis with large language models, 2021.
[4] S. Bang, S. Nam, I. Chun, H. Y. Jhoo, and J. Lee. Smt-based translation validation for machine
learning compiler. In Computer Aided Veriﬁcation: 34th International Conference, CAV 2022,
Haifa, Israel, August 7–10, 2022, Proceedings, Part II, pages 386–407. Springer, 2022.
[5] S. Black, L. Gao, P. Wang, C. Leahy, and S. Biderman. GPT-Neo: Large Scale Autoregressive
Language Modeling with Mesh-Tensorﬂow, Mar. 2021. If you use this software, please cite it
using these metadata.
[6] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam,
G. Sastry, A. Askell, et al. Language models are few-shot learners. Advances in neural
information processing systems, 33:1877–1901, 2020.
[7] C. Cadar, D. Dunbar, D. R. Engler, et al. Klee: unassisted and automatic generation of high-
coverage tests for complex systems programs. In OSDI, volume 8, pages 209–224, 2008.
[8] F. Cassano, J. Gouwar, D. Nguyen, S. Nguyen, L. Phipps-Costin, D. Pinckney, M.-H. Yee,
Y. Zi, C. J. Anderson, M. Q. Feldman, et al. Multipl-e: A scalable and polyglot approach to
benchmarking neural code generation. IEEE Transactions on Software Engineering, 2023.
[9] S. K. Cha, M. Woo, and D. Brumley. Program-adaptive mutational fuzzing. In 2015 IEEE
Symposium on Security and Privacy, pages 725–741. IEEE, 2015.
[10] M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. d. O. Pinto, J. Kaplan, H. Edwards, Y. Burda,
N. Joseph, G. Brockman, et al. Evaluating large language models trained on code. arXiv
preprint arXiv:2107.03374, 2021.
[11] W.-L. Chiang, Z. Li, Z. Lin, Y. Sheng, Z. Wu, H. Zhang, L. Zheng, S. Zhuang, Y. Zhuang, J. E.
Gonzalez, I. Stoica, and E. P. Xing. Vicuna: An open-source chatbot impressing gpt-4 with
90%* chatgpt quality, March 2023.
[12] Y. Deng, C. S. Xia, H. Peng, C. Yang, and L. Zhang. Large language models are zero-shot
fuzzers: Fuzzing deep-learning libraries via large language models, 2023.
12

[13] Y. Deng, C. S. Xia, C. Yang, S. D. Zhang, S. Yang, and L. Zhang. Large language models are
edge-case fuzzers: Testing deep learning libraries via fuzzgpt. arXiv preprint arXiv:2304.02014,
2023.
[14] fauxpilot. Fauxpilot: an open-source alternative to github copilot server. https://github.
com/fauxpilot/fauxpilot, 2022.
[15] G. Fraser and A. Arcuri. Evosuite: automatic test suite generation for object-oriented software.
In Proceedings of the 19th ACM SIGSOFT symposium and the 13th European conference on
Foundations of software engineering, pages 416–419, 2011.
[16] D. Fried, A. Aghajanyan, J. Lin, S. Wang, E. Wallace, F. Shi, R. Zhong, S. Yih, L. Zettlemoyer,
and M. Lewis. Incoder: A generative model for code inﬁlling and synthesis. In The Eleventh
International Conference on Learning Representations, 2023.
[17] C. Green. Application of theorem proving to problem solving. In Readings in Artiﬁcial
Intelligence, pages 202–222. Elsevier, 1981.
[18] S. Gulwani. Automating string processing in spreadsheets using input-output examples. SIG-
PLAN Not., 46(1):317–330, jan 2011.
[19] S. Gulwani, O. Polozov, and R. Singh. Program synthesis. Foundations and Trends® in
Programming Languages, 4(1-2):1–119, 2017.
[20] C. Holler, K. Herzig, and A. Zeller. Fuzzing with code fragments. In 21st USENIX Secu-
rity Symposium (USENIX Security 12), pages 445–458, Bellevue, WA, Aug. 2012. USENIX
Association.
[21] A. Holtzman, J. Buys, L. Du, M. Forbes, and Y. Choi. The curious case of neural text degenera-
tion. In International Conference on Learning Representations, 2020.
[22] S. Iyer, I. Konstas, A. Cheung, and L. Zettlemoyer. Mapping language to code in programmatic
context. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language
Processing, pages 1643–1652, 2018.
[23] A. Kalyan, A. Mohta, O. Polozov, D. Batra, P. Jain, and S. Gulwani. Neural-guided deductive
search for real-time program synthesis from examples. In International Conference on Learning
Representations, 2018.
[24] J. C. King. Symbolic execution and program testing. Communications of the ACM, 19(7):385–
394, 1976.
[25] M.-A. Lachaux, B. Roziere, L. Chanussot, and G. Lample. Unsupervised translation of pro-
gramming languages. arXiv preprint arXiv:2006.03511, 2020.
[26] C. Lemieux, J. P. Inala, S. K. Lahiri, and S. Sen. Codamosa: Escaping coverage plateaus in
test generation with pre-trained large language models. In 45th International Conference on
Software Engineering (ICSE), 2023.
[27] Y. Li, D. Choi, J. Chung, N. Kushman, J. Schrittwieser, R. Leblond, T. Eccles, J. Keeling,
F. Gimeno, A. Dal Lago, et al. Competition-level code generation with alphacode. Science,
378(6624):1092–1097, 2022.
[28] N. P. Lopes, J. Lee, C.-K. Hur, Z. Liu, and J. Regehr. Alive2: bounded translation validation for
llvm. In Proceedings of the 42nd ACM SIGPLAN International Conference on Programming
Language Design and Implementation, pages 65–79, 2021.
[29] S. Lu, D. Guo, S. Ren, J. Huang, A. Svyatkovskiy, A. Blanco, C. Clement, D. Drain, D. Jiang,
D. Tang, et al. Codexglue: A machine learning benchmark dataset for code understanding and
generation. arXiv preprint arXiv:2102.04664, 2021.
[30] R. Majumdar and K. Sen. Hybrid concolic testing. In 29th International Conference on Software
Engineering (ICSE’07), pages 416–426. IEEE, 2007.
[31] Z. Manna and R. J. Waldinger. Toward automatic program synthesis. Communications of the
ACM, 14(3):151–165, 1971.
[32] W. M. McKeeman. Differential testing for software. Digital Technical Journal, 10(1):100–107,
1998.
[33] B. Meyer. Applying’design by contract’. Computer, 25(10):40–51, 1992.
13

[34] Microsoft. GitHub Copilot – Your AI pair programmer. https://github.com/features/
copilot, 2023.
[35] B. P. Miller, L. Fredriksen, and B. So. An empirical study of the reliability of unix utilities.
Communications of the ACM, 33(12):32–44, 1990.
[36] G. C. Necula. Translation validation for an optimizing compiler. In Proceedings of the ACM
SIGPLAN 2000 conference on Programming language design and implementation, pages 83–94,
2000.
[37] P. Nie, R. Banerjee, J. J. Li, R. J. Mooney, and M. Gligoric. Learning deep semantics for test
completion, 2023.
[38] E. Nijkamp, B. Pang, H. Hayashi, L. Tu, H. Wang, Y. Zhou, S. Savarese, and C. Xiong. Codegen:
An open large language model for code with multi-turn program synthesis. In The Eleventh
International Conference on Learning Representations, 2023.
[39] P. Oehlert. Violating assumptions with fuzzing. IEEE Security & Privacy, 3(2):58–62, 2005.
[40] OpenAI. Chatgpt: Optimizing language models for dialogue. https://openai.com/blog/
chatgpt/, 2022.
[41] OpenAI. Gpt-4 technical report. ArXiv, abs/2303.08774, 2023.
[42] K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. Bleu: a method for automatic evaluation
of machine translation. In Proceedings of the 40th annual meeting of the Association for
Computational Linguistics, pages 311–318, 2002.
[43] S. Poeplau and A. Francillon. Symbolic execution with symcc: Don’t interpret, compile! In
Proceedings of the 29th USENIX Conference on Security Symposium, pages 181–198, 2020.
[44] B. Roziere, J. M. Zhang, F. Charton, M. Harman, G. Synnaeve, and G. Lample. Leveraging
automated unit tests for unsupervised code translation. arXiv preprint arXiv:2110.06773, 2021.
[45] M. Schäfer, S. Nadi, A. Eghbali, and F. Tip. Adaptive test generation using a large language
model, 2023.
[46] M. Security. jsfunfuzz. https://github.com/MozillaSecurity/funfuzz, 2007.
[47] K. Sen, D. Marinov, and G. Agha. Cute: A concolic unit testing engine for c. ACM SIGSOFT
Software Engineering Notes, 30(5):263–272, 2005.
[48] K. Serebryany. Continuous fuzzing with libfuzzer and addresssanitizer. In 2016 IEEE Cyberse-
curity Development (SecDev), pages 157–157. IEEE, 2016.
[49] D. E. Shaw, W. R. Swartout, and C. C. Green. Inferring lisp programs from examples. In IJCAI,
volume 75, pages 260–267, 1975.
[50] A. Shi, A. Gyori, M. Gligoric, A. Zaytsev, and D. Marinov. Balancing trade-offs in test-suite
reduction. In Proceedings of the 22nd ACM SIGSOFT international symposium on foundations
of software engineering, pages 246–256, 2014.
[51] A. Shypula, P. Yin, J. Lacomis, C. L. Goues, E. Schwartz, and G. Neubig.
Learning to
superoptimize real-world programs. arXiv preprint arXiv:2109.13498, 2021.
[52] Stability-AI. Stablelm: Stability ai language models. https://github.com/Stability-AI/
StableLM, 2023.
[53] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and
I. Polosukhin. Attention is all you need. Advances in neural information processing systems,
30, 2017.
[54] R. J. Waldinger and R. C. Lee. Prow: A step toward automatic program writing. In Proceedings
of the 1st international joint conference on Artiﬁcial intelligence, pages 241–252, 1969.
[55] B. Wang and A. Komatsuzaki. GPT-J-6B: A 6 Billion Parameter Autoregressive Language
Model. https://github.com/kingoflolz/mesh-transformer-jax, May 2021.
[56] Y. Wang, W. Wang, S. Joty, and S. C. Hoi. Codet5: Identiﬁer-aware uniﬁed pre-trained encoder-
decoder models for code understanding and generation. In Proceedings of the 2021 Conference
on Empirical Methods in Natural Language Processing, pages 8696–8708, 2021.
[57] D. Winterer, C. Zhang, and Z. Su. On the unusual effectiveness of type-aware operator mutations
for testing smt solvers. Proceedings of the ACM on Programming Languages, 4(OOPSLA):1–25,
2020.
14

[58] C. S. Xia, Y. Wei, and L. Zhang. Automated program repair in the era of large pre-trained
language models. In Proceedings of the 45th International Conference on Software Engineering
(ICSE 2023). Association for Computing Machinery, 2023.
[59] C. S. Xia and L. Zhang. Less training, more repairing please: revisiting automated program repair
via zero-shot learning. In Proceedings of the 30th ACM Joint European Software Engineering
Conference and Symposium on the Foundations of Software Engineering, pages 959–971, 2022.
[60] C. S. Xia and L. Zhang. Keep the conversation going: Fixing 162 out of 337 bugs for $0.42
each using chatgpt. arXiv preprint arXiv:2304.00385, 2023.
[61] F. F. Xu, U. Alon, G. Neubig, and V. J. Hellendoorn. A systematic evaluation of large language
models of code. In Proceedings of the 6th ACM SIGPLAN International Symposium on Machine
Programming, pages 1–10, 2022.
[62] X. Yang, Y. Chen, E. Eide, and J. Regehr. Finding and understanding bugs in c compilers.
In Proceedings of the 32nd ACM SIGPLAN Conference on Programming Language Design
and Implementation, PLDI ’11, page 283–294, New York, NY, USA, 2011. Association for
Computing Machinery.
[63] X. Yang, Y. Chen, E. Eide, and J. Regehr. Finding and understanding bugs in c compilers. In
Proceedings of the 32nd ACM SIGPLAN conference on Programming language design and
implementation, pages 283–294, 2011.
[64] G. Ye, Z. Tang, S. H. Tan, S. Huang, D. Fang, X. Sun, L. Bian, H. Wang, and Z. Wang.
Automated conformance testing for javascript engines via deep compiler fuzzing. In Proceedings
of the 42nd ACM SIGPLAN International Conference on Programming Language Design and
Implementation, PLDI 2021, page 435–450.
[65] T. Yu, R. Zhang, K. Yang, M. Yasunaga, D. Wang, Z. Li, J. Ma, I. Li, Q. Yao, S. Roman, et al.
Spider: A large-scale human-labeled dataset for complex and cross-domain semantic parsing
and text-to-sql task. In Proceedings of the 2018 Conference on Empirical Methods in Natural
Language Processing, 2018.
[66] M. Zalewski. American fuzzing lop (aﬂ). https://lcamtuf.coredump.cx/afl/, 2018.
[67] L. Zhang, D. Marinov, L. Zhang, and S. Khurshid. An empirical study of junit test-suite
reduction. In 2011 IEEE 22nd International Symposium on Software Reliability Engineering,
pages 170–179. IEEE, 2011.
[68] Q. Zheng, X. Xia, X. Zou, Y. Dong, S. Wang, Y. Xue, Z. Wang, L. Shen, A. Wang, Y. Li,
et al. Codegeex: A pre-trained model for code generation with multilingual evaluations on
humaneval-x. arXiv preprint arXiv:2303.17568, 2023.
15

