Image Clustering via the Principle of Rate Reduction
in the Age of Pretrained Models
Tianzhe Chu1,2‚àó
Shengbang Tong1‚àó
Tianjiao Ding3‚àó
Xili Dai4
Benjamin D. Haeffele3
Ren√© Vidal5
Yi Ma1
Abstract
The advent of large pre-trained models has brought about a paradigm shift in
both visual representation learning and natural language processing. However,
clustering unlabeled images, as a fundamental and classic machine learning prob-
lem, still lacks effective solution, particularly for large-scale datasets. In this
paper, we propose a novel image clustering pipeline that leverages the powerful
feature representation of large pre-trained models such as CLIP and cluster im-
ages effectively and efficiently at scale. We show that the pre-trained features
are significantly more structured by further optimizing the rate reduction objec-
tive. The resulting features may significantly improve the clustering accuracy,
e.g., from 57% to 66% on ImageNet-1k. Furthermore, by leveraging CLIP‚Äôs
image-text binding, we show how the new clustering method leads to a simple
yet effective self-labeling algorithm that successfully works on unlabeled large
datasets such as MS-COCO and LAION-Aesthetics. We will release the code in
https://github.com/LeslieTrue/CPP.
1
Motivation
Clustering is a fundamental problem in pattern recognition and machine learning, with many common
methods emerging as early as the 1950s [1‚Äì4] along with numerous modern developments. Never-
theless, there is a significant discrepancy separating the recent advance of large-scale methods from
that of clustering performance. Namely, there are datasets with millions [5] or even billions [6] of
images and thousands of classes and classification methods with close to 100% accuracy2, yet existing
clustering approaches typically either fail on natural images [7‚Äì18], or have been tested only with
datasets of a small number of clusters (‚àº102) and images (‚àº105) [19‚Äì27] with a few exceptions
[28, 29]. Thus far few methods have achieved above 50% clustering accuracy on ImageNet-1k or
TinyImageNet-200: e.g., [28, 20, 23, 27, 30] all achieve smaller than 50% accuracy.
Classic methods often build on assumptions about the geometry of data from each cluster, such as
modeling each cluster as a centroid [1‚Äì4, 7‚Äì9], a low-dimensional linear or affine subspace [14‚Äì18],
a manifold from known families [12], sampled densely [10], or locally approximated by affine
subspaces [11]. Despite being effective on relatively simple datasets such as COIL [31] or MNIST
[32], these methods become either not accurate (the assumptions are drastically violated) or not
scalable (computing a neighborhood graph is expensive) when confronted with more complicated or
large-scale datasets such as CIFAR [33] or even ImageNet [5].
Key to the recent advance in clustering is teaching an old dog new tricks, i.e., learning features via
deep networks which can in turn be used for clustering. Modern clustering pipelines [34, 25, 30, 35]
proceed by i) learning an initial representation by self-supervised learning, such as the seminal
joint-embedding approaches [36‚Äì39] and ii) gradually refining the representation and clustering
membership by minimizing some objective function. One family of methods [25, 30] based on
the principle of Maximal Coding Rate Reduction (MCR2 [40]) aim to learn a representation such
‚àómeans equal contribution, 1University of California, Berkeley, 2ShanghaiTech University, 3Johns Hopkins
University, 4Hong Kong University of Science and Technology (Guangzhou), 5University of Pennsylvania
2See for example https://paperswithcode.com/task/image-classification.
Preprint. Under review.
arXiv:2306.05272v2  [cs.CV]  9 Jun 2023

Target
Our Method
CLIP
Image to Image Search
Figure 1: An example of image-to-image search on ImageNet, using features provided by CLIP
(Right) and refined by our method (Left). Given a target image from the validation split, we find the
images from the train split that are closest to the target image, measured by the Euclidean distance
between extracted features. The improvement is rather significant and common.
that features within the same cluster tend to span a low-dimensional subspace (i.e., within-cluster
diverse), and subspaces from different clusters tend to be orthogonal (between-cluster discriminative).
Learning such a representation appears to not only help clustering [25, 30] but also allows effective
image generation [41, 42], incremental learning [43, 44], and retrieval. Promising as it may seem,
clustering based on the MCR2 principle [25, 30] highly depends on the initial representation, and thus
clustering performance is far from the supervised classification baselines on CIFAR100 or ImageNet.
In parallel with these developments, the advent of large scale pre-trained models like CLIP [45]
and DINO [46, 47] have showcased an impressive capacity to learn rich representations for a wide
variety of data. In particular, CLIP (Contrastive Language-Image Pre-Training) trains on a diverse set
of images paired with natural language descriptions and has demonstrated its ability to serve as a
foundation model that truly scales up to larger neural network and training data, making it highly
suitable for tasks that require a nuanced understanding of visual information.
In this work, we leverage the advancements in both pre-trained models like CLIP and principled
approaches to clustering to develop a novel framework which can effectively address the challenges
inherent in clustering large-scale data. In particular, our paper makes the following key contributions:
1. We propose a novel image clustering pipeline that leverages the powerful feature representa-
tion of large pre-trained models such as CLIP, providing a highly effective image clustering
method in the challenging regime of large-scale and complex datasets.
2. When the number of clusters is unknown, we give a mechanism for our pipeline that
estimates the optimal number of clusters, without any costly retraining of deep networks.
3. We provide comprehensive experimental results on various datasets, including CIFAR
[33], ImageNet [5], MS-COCO [48] and LAION-Aesthetics[6], demonstrating the superior
performance of our method both in terms of scalability and clustering performance.
4. With the benefit of the vision-text binding provided by CLIP, we also propose a simple
yet effective self-labelling algorithm that automatically labels the clusters with textual
descriptions that can be comprehended by a human.
2
Related Work
In this section, we broadly review some of the work that has inspired our research. We begin with
the recent progress in pre-trained models, and then discuss the recent trend of image clustering via
pre-trained models.
Vision Pretrained Model. In recent years, we have witnessed the rapid development of vision pre-
trained models in the field. Pure vision models have benefited from advancements in joint-embedding
self-supervised learning [36‚Äì38, 46, 49] and masked self-supervised learning [50‚Äì52]. These models
have shown great promise in learning semantically meaningful features on large scale of datasets.
Vision-Language Pretrained Model. Another line of work focuses on learning meaningful represen-
tation from images with the guidance of text information. These Vision and Language Models (VLM)
bridge the gap between textual and visual information. Some early work like ICMLM [53] and Virtex
[54] demonstrated the potential of language supervision on the COCO dataset [48]. Inspired by the
2

progress in contrastive learning [36, 37], CLIP proposes contrastive language-image pre-training.
The work has gained significant attention because of the simplicity, incredible scalability and very
strong performance. Followup reproduction openCLIP [55] has verified that the method‚Äôs scalability
with the largest vision transformer model [56] and a dataset containing 5 billion text-image pairs
[6]. As a result, in this work, we adopt CLIP as our pre-trained model to design truly scalable and
effective clustering learning algorithm.
Image Clustering via Pretrained Model. The advance in vision pre-trained models have led to major
breakthroughs in image clustering. For instance, SCAN [34] proposes a three step image clustering
pipeline, starting with a self-supervised SimCLR [36] model. The method then train a preliminary
cluster head based on the extracted representation and finally fine-tune the trained cluster head via
confidence-based pseudo labeling. Subsequent research such as RUC [19], TSP[57] and SPICE [35]
has further enriched the field by exploring robust training, alternative network architecture, diverse
self-supervised pre-trained and different finetune methods. A few works [58‚Äì60] also explored this
paradigm in generative models, showcasing some promising downstream applications.
These pioneering methods have undoubtedly made remarkable progress and have served as a strong
foundation for the field. However, they often involve a degree of intricate engineering, performance
optimization, and parameter tuning. While these complexities are inherent to their design and have
enabled them to achieve their goals, they may present some challenges when implementing and
scaling these methods to larger datasets. Recently, approaches like NMCE [25] and MLC [30] have
connected manifold clustering and deep learning via the MCR2 principle [40, 61]. In particular,
MLC [30] has demonstrated that this principled approach has shown promise in terms of efficiency,
scalability and capability to handle imbalances present in larger unlabeled datasets [6, 48] and real-life
data. Consequently, we draw inspiration from MLC to develop a truly scalable and effective image
clustering pipeline capable of handling the scale and natural imbalances present in real-world data.
3
Our Method
In this section, we first briefly review Manifold Linearizing and Clustering [30] in ¬ß3.1. Based on
this, we provide a simple yet effective pipeline for performing clustering and representation learning
using MLC with CLIP pre-training in ¬ß3.2. Finally, we leverage the proposed pipeline to identify the
optimal number of clusters in ¬ß3.3 and provide self-labelling textual cluster labels in ¬ß3.4.
3.1
Review of Manifold Linearizing and Clustering (MLC)
Given a dataset X = [x1, . . . , xn] ‚ààRD√ón of n points lying on a union of k unknown manifolds,
how to i) cluster the points in X, as well as ii) learn a representation for X that has desireable
geometric properties?
Diverse and Discriminative Representation. A fruitful line of research [62, 63, 43, 64‚Äì66, 25, 44],
including MLC [30], considers learning a representation by using the principle of Maximal Coding
Rate Reduction (MCR2). Roughly speaking, an ideal representation pursued by MCR2 should have
the following properties.
‚Ä¢ Within-cluster diversity: Features from each cluster lie in a low-dimensional linear subspace. This
naturally leads to a notion of (non-trivial) distance within a cluster, allowing downstream tasks such
as image retrieval, generation, or interpolation.
‚Ä¢ Between-cluster discrimination: Features (or subspaces) from different clusters are orthogonal.
In particular, MLC seeks to find 1) an embedding of the data Z ‚ààRd√ón = [fŒ∑(x1), . . . , fŒ∑(xn)]
where fŒ∑ : RD ‚ÜíRd denotes a neural network with parameters Œ∑, and 2) a cluster membership
Œ† ‚ààRn√ón produced by a network parameterized by œâ and a doubly stochastic projection (detailed
below) where each entry Œ†i,j ‚â•0 measures the similarity between the i-th and j-th points to
maximize the following objective function.
max
Œ∑,œâ
R(Z(Œ∑); Œµ) ‚àíRc(Z(Œ∑), Œ†(œâ); Œµ),
(MLC)
where
R(Z; Œµ) := log det

I + d
Œµ2 ¬∑ 1
nZZ‚ä§

,
(1)
Rc(Z, Œ†; Œµ) := 1
n
n
X
j=1
log det

I + d
Œµ2 Z diag(Œ†j)Z‚ä§

.
(2)
3

Backbone
ùìß
CLIP Image Encoder
ùúÇ
ùúî
Feature head
Cluster head
Pre-feature layer
Images
Figure 2: Overall training pipeline of our method, named CPP (Clustering via the Principle of rate
reduction and Pretrained models). Given n input images, their D dimensional representation X
is given by the image encoder of CLIP with parameters frozen, i.e., the backbone. Z(Œ∑) ‚ààRd√ón
denotes the d-dimensional feature where Œ∑ represents feature head parameters. The doubly stochastic
membership matrix Œ†(œâ) ‚ààRn√ón is given by taking an inner product of the outputs from cluster
head followed by a sinkhorn projection.
Broadly speaking, R(Z; Œµ) measures the volume of points in Z up to a Œµ > 0 rate distortion coding
precision. Likewise Rc(Z, Œ†; Œµ) measures the sum of volumes of the clusters encoded by Œ†, and
minimizing it would push points from each cluster to be close. Thus, MLC proposes to maximize
the difference between the volumes (i.e., expand the volume of the embedded data globally, while
compressing the volume of the data within a cluster). This has been shown in [40] to provably give
data representations where each class/cluster (for a fixed Œ†) are low-rank linear subspaces which are
orthogonal to each other.
Doubly Stochastic Membership. To learn a membership of the data for clustering, MLC draws
inspiration from the success of doubly stochastic clustering [67, 68] and computes a doubly stochastic
membership from some latent codes of data. Specifically, MLC adopts a neural network gœâ : RD ‚Üí
Rd with parameters œâ to first obtain latent codes C = [gœâ(x1), . . . , gœâ(xn)] for each datapoint.
Then, Œ† is given as a regularized projection proj‚Ñ¶,Œ≥(C‚ä§C) onto ‚Ñ¶, where ‚Ñ¶denotes the set of doubly
stochastic matrices
‚Ñ¶:= {Œ† ‚ààRn√ón : Œ†1 = Œ†‚ä§1 = 1;
Œ†ij ‚â•0,
‚àÄi, j}.
(3)
Here, proj‚Ñ¶,Œ≥(¬∑) is defined as argminŒ†‚àà‚Ñ¶‚àí‚ü®C‚ä§C, Œ†‚ü©+ Œ≥ Pn
ij Œ†ij log Œ†ij which can be efficiently
computed [69, 70], and Œ≥ is a regularization strength that controls the entropy of Œ†; in short, the
larger Œ≥ is, the more uniform Œ† is. Note that roughly speaking less uniform Œ† solutions result in
fewer false connections between different clusters at the expense of less inter-cluster connectivity.
We highlight here the dependency of Z and Œ† on their respective network parameters Œ∑, œâ, which
we will typically omit for simplicity of notation.
3.2
Refining CLIP Features with MLC
Structured Feature and Membership Initialization via CLIP. Since (MLC) is non-convex, its
initialization and optimization dynamics play a vital role in performance. Prior work [25, 30] used
self-supervised contrastive learning to initialize the features, which often fail to capture nuanced
semantics, and as a result, they only reach, e.g., 33.5% clustering accuracy on Tiny-ImageNet [30].
We describe in the sequel how to initialize the representation and membership leveraging a pre-trained
CLIP [45] model. Recall that CLIP has an image encoder and a text encoder, which maps input
image and text respectively to a joint feature space. Ideally, two features coming from text and image
data are expected to be close only when the text and the image manifest similar semantics. These
encoders are trained utilizing image-caption pairs widely available on the internet. Motivated by the
remarkable performance of CLIP in doing zero-shot tasks on unseen datasets, we take its pre-trained
image encoder as our backbone (or feature extractor). The parameters of the backbone are henceforth
fixed. Equivalently, we are taking the input data X to be CLIP features rather than raw images.
Refining CLIP features via MLC. To allow fine-tuning of both Z and Œ†, it is natural to add extra
trainable layers after the backbone, as seen by the feature head and cluster head in Figure 2. However,
4

Algorithm 1: Clustering without knowing the number of clusters
Input: Learned features Z ‚ààRd√ón and membership Œ† ‚ààRn√ón, max. # of clusters K ‚ààZ+
For k ‚Üê1, . . . , K:
Z[1], . . . , Z[k] ‚ÜêSpectral clustering on Œ† to get k clusters for Z;
(6)
Lk ‚Üê
k
X
i=1
L(Z[i]) + |Z[i]|

‚àílog
|Z[i]|
n

.
(7)
Output: Optimal number of clusters argmink Lk
these added layers could be arbitrary due to random initialization, undermining the quality of Z and
Œ†. Moreover, as seen in Figure 4, the pre-trained features from CLIP (i.e., the output of the backbone)
often have moderate pair-wise similarity even for those from very different classes. Toward this end,
we propose to diversify the features Z simply via
max
Œ∑
R(Z(Œ∑); Œµ),
(4)
which is precisely (1); similar ideas have been explored also in [25, 39]. In practice, we find that
updating (4) only 1-2 epochs suffices to diversify Z, making it a lightweight initialization step. To
initialize Œ†, we copy Œ∑ to œâ (equivalently, assigning C = Z) without extra training effort, which is
a benefit of using doubly stochastic membership. With both Z and Œ† initialized, we proceed and
optimize the full model in (MLC). Once the training process is done, one can use the feature head
and cluster head to obtain Z, C for any set of images, seen or unseen; C in turn gives a Œ† following
¬ß3.1. When the number k of clusters is given, one can simply run spectral clustering [71] on Œ† to get
k clusters.
3.3
Clustering without Knowing Number of Clusters
In many scenarios, it is impossible for one to know the number of clusters. Two issues arise: i) one
must have a mechanism to guess the number of clusters, and ii) to obtain an accurate estimate, one
typically runs the entire deep clustering pipeline multiple times, which is computationally costly.
In this regard, we propose to estimate the number of clusters without expensive retraining. This
flexibility which alleviates ii) is attributed to the following fact: Note that the membership Œ† ‚ààRn√ón
merely signals pairwise similarity (recall ¬ß3.1), so the number k of clusters is not part of the training
pipeline of (MLC) whatsoever in contrast to the more common way of using a n √ó k Œ† matrix which
encodes k explicitly [25]. That said, given a Œ† ‚ààRn√ón, how can one know what is a reasonable
number of clusters?
Towards this end, we again leverage the minimum coding length (or rate) criteria [72], this time
including not only the cost of features but also that of the labels; in short, with more clusters, the cost
of the labels increases. Recall the definition of coding length from [72]
L(Z) = (n + d)R(Z; Œµ).
(5)
We now present Algorithm 1 to estimate the number of clusters. Assume that the training is done
and one already has Z and Œ†. For each k ‚àà{1, . . . , K} where K is the max possible number of
clusters, we do spectral clustering on Œ† to obtain k clusters. Let Z[i] denote the features from the i-th
cluster and |Z[i]| denote the number of features in i-th cluster. Then, (7) gives the cost Lk of coding
k clusters, which includes the cost of the features of each cluster as well as the labels, which are the
first and second terms in the summation. Finally, one can choose the optimal number of cluster that
gives the lowest cost Lk. We will show in Figure 5 some examples.
3.4
Labeling the Learned Clusters and Image2Image Search
We end the section by noting that since MLC refines the CLIP features as well as performs clustering,
several interesting applications could be done, such as labelling the learned clusters, as well as
image2image search. For the interest of space, we showcase these applications in ¬ß4 and detail the
procedures in the Appendix E.1 and D.1.
5

4
Experiments
In this section, we empirically verify the effectiveness of CPP. We introduce the experiment setup in
¬ß4.1, then qualitatively show that our superior clustering performance on standard and large-scale
clustering datasets in ¬ß4.2. We finish this section discussing advantages of CPP learned representation
in ¬ß4.3. We then show how to use our method to label clusters of large unlabelled datasets such as
MS-COCO and LAION-Aesthetics in ¬ß4.4.
4.1
Datasets and Experiment Setting
Datasets. CIFAR contains 50, 000 training and 10, 000 test images, which are divided evenly into 10,
20 or 100 ground-truth classes, which we refer to as CIFAR-10, -20 or -100; note that the classes of
CIFAR-20 are given by merging those of CIFAR-100. ImageNet incorporates around 1.2 million
training images and 100, 000 test images, spread across 1, 000 classes, called ImageNet-1k. We
process data in a manner identical to that used in CLIP [45], which involves resizing and center
cropping images to dimensions of 224 √ó 224.
Network Architecture. We use a ViT L/14 model [56] pre-trained via CLIP [45], with checkpoint
from OpenAI3. As shown in Figure 2, we freeze the backbone during training and add a pre-feature
layer composed of Linear-BatchNorm-ReLU-Linear-ReLU after the backbone. For feature head and
cluster head, we use a Linear layer with mapping from the hidden dimension to feature dimension d
respectively. Unified architecture is applied on all the experiments across different datasets except
adjusted hidden dimension and feature dimension d. Finally, to learn a ideal representation that spans
a union of orthogonal subspaces as in ¬ß3.1, note that the feature dimension d should be larger than or
equal to the expected number of clusters. We leave more details in the Appendix A.
Optimization. As describe in ¬ß3.2, we first warmup our network by 1-2 epochs by training R(Z; Œµ)
alone, then simultaneously optimize both feature head and cluster head using (MLC). For both the
feature head and cluster head, we train with SGD optimizer, learning rate set to 0.0001, momentum
set to 0.9 and weight decay set to 0.0001 and 0.005 respectively.
4.2
Performance on Standard Clustering Datasets
Metrics. Once clustering has been done (i.e., after spectral clustering has been run on Œ†), one
can compute the standard clustering accuracy (ACC), normalized mutual information (NMI) on the
obtained clusters and the ground-truth ones.
Improved Clustering via CPP. As demonstrated in prior literature, CLIP [45] has successfully
learned a discriminative representation that can be employed for clustering. In order to assess the
quality of these clusters without the application of CPP, and to validate the necessity of CPP, we
apply KMeans, subspace clustering methods (EnSC [18] and SSC-OMP [73]), and spectral clustering
on the representation learned through CLIP, comparing their results with CPP. For EnSC and SSC-
OMP, we conduct a grid search of parameters and report the clustering accuracy at its highest point.
More details regarding this process can be found in Appendix B. We present the results in Table 1.
From the table, it can be observed that in comparison to other clustering methods, CPP has achieved
a significant improvement in cluster accuracy across all four datasets. This implies that CPP cluster
heads has learned a better representation comparing to representation from the CLIP pre-trained
model, making it more suited for image clustering.
CIFAR-10
CIFAR-20
CIFAR-100
ImageNet-1k
Method
Backbone
ACC
NMI
ACC
NMI
ACC
NMI
ACC
NMI
KMeans
ViT L/14
83.5
84.0
47.3
51.3
52.3
67.7
49.2
81.3
EnSC
ViT L/14
85.8
89.2
61.6
69.3
66.6
77.1
56.8
83.7
SSC-OMP
ViT L/14
85.4
84.6
60.9
65.3
64.6
72.8
49.6
80.5
Spectral Clustering
ViT L/14
73.6
75.2
52.4
56.2
67.2
76.1
55.8
83.4
CPP
ViT L/14
97.4
93.6
64.2
72.5
74.0
81.8
65.7
86.4
Table 1: Clustering Accuracy and Normalized Mutual Information of classic clustering methods
applied to CLIP features (Top) and the proposed CPP using CLIP as pre-trained features (Bottom).
3https://github.com/openai/CLIP
6

Comparison with Deep Clustering Methods. We proceed by contrasting CPP with the state-of-the-
art (SOTA) stage-wise deep clustering methods. These approaches utilize a pre-trained self-supervised
or language-supervised network and propose clustering methods based on this pre-trained model.
From Table 2, it is evident that our method has outperformed others in terms of clustering accuracy
and normalized mutual information across all datasets.
It is notable that methods incorporating pre-training from external sources have displayed significant
improvement compared to their predecessors. This underlines the importance of integrating pre-
trained models for image clustering. When compared with TEMI [29], which also employs external
pre-training data, our method not only attains superior training accuracy but also drastically reduces
training epochs. CPP converges within a maximum of 50 epochs, whereas previous methods typically
require more than 100. This endows CPP with the capacity to efficiently and effectively scale up to
even larger datasets in this era of pre-trained models. We will delve deeper into this in ¬ß4.4.
CIFAR-10
CIFAR-20
CIFAR-100
ImageNet-1k
Method
Backbone
ACC
NMI
ACC
NMI
ACC
NMI
ACC
NMI
MLC [30]
ResNet-18
86.3
76.3
52.2
54.6
49.4
68.3
-
-
SCAN [34]
ResNet-18
88.3
79.7
50.7
48.6
34.3
55.7
39.9 4
-
IDFD [21]
ResNet-18
81.5
71.1
42.5
42.6
-
-
-
-
IMC-SWAV [24]
ResNet-18
89.7
81.8
51.9
52.7
45.1
67.5
-
-
RUC+SCAN [19]
ResNet-18
90.3
-
54.3
-
-
-
-
-
SPICE [23]
ResNet-34
91.7
85.8
58.4
58.3
-
-
-
-
NMCE [25]
ResNet-34
88.7
81.9
53.1
52.4
-
-
-
-
TCL [26]
ResNet-34
88.7
81.9
53.1
52.9
-
-
-
-
C3 [27]
ResNet-34
83.8
74.8
45.1
43.4
-
-
-
-
CC [20]
ResNet-34
79.0
70.5
42.9
43.1
-
-
-
-
ConCURL [22]
ResNet-50
84.6
76.2
47.9
46.8
-
-
-
-
Single-Noun Prior [74]
ViT-B/32
93.4
85.9
48.4
51.5
-
-
-
-
TEMI* [29]
ViT L/14
96.9
92.6
61.8
64.5
73.7
79.9
64.0
-
CPP*
ViT L/14
97.4
93.6
64.2
72.5
74.0
81.8
65.7
86.4
Table 2: Comparison with state-of-the-art deep clustering models. Methods marked with an asterisk
(*) uses pre-training from CLIP.
4.3
Advantages of CPP Learned Representation
Structure within Each Cluster. CPP aims at learning a union-of-orthogonal-subspace representation
for the given dataset. We visualize the cosine similarity (|Z‚ä§Z|) of learned features by CPP and
CLIP in Figure 4. From plot, it is evident that CPP learned representation has a clear block-diagonal
structure even for ImageNet-1k, implies that the representations are well-clustered. In other words,
the feature vectors within the same cluster (block) are relatively similar (have high cosine similarity),
while feature vectors from different clusters are relatively dissimilar (have low cosine similarity).
PCA Components of CPP Learned Representation. Here, we plot normalized singular values
of CLIP representation on CIFAR-10 and CPP representation on CIFAR-10. We observe that CPP
representation has a higher rank than CLIP representation after training, manifesting that the learned
representation is more structured.
Using CPP Learned Representation: Better Image-to-Image Search. We can facilitate down-
stream tasks with the more structured representation learned by CPP. We demonstrate this by the task
of image-to-image search, which is widely studied and used in real-life. In this work, we approach
image-to-image search as follows: We first establish an image repository comprised of a set of images
from the dataset, and then compute their respective representations, yielding our feature set. To search
for a target image, we compute its representation and identify the 64 images from our feature set that
have the closest cosine similarity to the target. A visualization of image-to-image search pipeline can
be found in Appendix D.2.
For this experiment, we use training images from ImageNet-1k to form our image repository, while
randomly selected images from the ImageNet-1k validation set serve as our target images. We
compare the outcomes of image-to-image search using both the CLIP pre-trained network and CPP,
trained on ImageNet-1k.
7

(a) CIFAR-10 (CLIP)
(b) CIFAR-100 (CLIP)
(c) ImageNet-1k (CLIP)
(d) CIFAR-10 (CPP)
(e) CIFAR-100 (CPP)
(f) ImageNet-1k (CPP)
Figure 3: Cosine Similarity |Z‚ä§Z| Visualization. (Top): Strong sample-wise correlation for CLIP‚Äôs
features is reflected in top 3 figures; (Bottom): Clear block-diagonal structure for CPP learned
features is reflected in bottom 3 figures.
0
5
10
15
20
25
30
Components
0.0
0.2
0.4
0.6
0.8
1.0
Nomarlized Value
Singular Value of Ours vs CLIP
CLIP
Ours
(a) All Samples
0
5
10
15
20
25
30
Components
0.0
0.2
0.4
0.6
0.8
1.0
Nomarlized Value
Singular Value within Cluster of CLIP
(b) Within Cluster (CLIP)
0
5
10
15
20
25
30
Components
0.0
0.2
0.4
0.6
0.8
1.0
Nomarlized Value
Singular Value within Cluster of Ours
(c) Within Cluster (CPP)
Figure 4: Principal Components on CIFAR-100 features. (Left): Principal components on all features;
(Middle): Principal components within each cluster for CLIP‚Äôs feature, membership given by KMeans;
(Right): Principal components within each cluster for CPP‚Äôs feature, membership given by spectral
clustering upon membership matrix.
An example of these results is displayed in Figure 1, with additional results provided in Appendix
D.2. From these figures, we can observe that the CPP representation fosters a more comprehensive
understanding, as demonstrated by the shampoo bottle example. The image-to-image search based
on CPP recognizes the semantic meaning of a shampoo bottle as a daily hygiene product, while the
search based on CLIP predominantly returns images of cylindrical objects. This indicates that CPP
provides a better representation for the clustered dataset.
4.4
Clustering and Labeling Large Unlabeled Image Datasets
We venture beyond the realm of standard dataset clustering to explore clustering of large-scale
unlabeled datasets: MS-COCO [48] and LAION-Aesthetics [6]. This expansion is made feasible due
to the scalability, efficiency, and effectiveness demonstrated by CPP in the previous section. Here, we
tackle two difficulties in labelling large-scale data (1) identifying the optimal number of clusters in
the dataset. (2) assigning semantically meaningful names to the identified clusters, enabling humans
to understand the results more intuitively and enhance the practical utility of these labelled outputs.
Datasets. In this section, we cluster on large scale datasets - MS-COCO-2017 with 118000 training
images and LAION-Aesthetics with around 2.7 million training images. We process the data with
same resizing and center cropping as is described in ¬ß4.1.
8

10
20
30
40
50
Number of Clusters
1.05
1.10
1.15
1.20
Coding Bits
1e6
(a) CIFAR-10 (10)
25
50
75
100
125
150
175
200
Number of Clusters
1.2
1.3
1.4
1.5
1.6
Coding Bits
1e6
(b) CIFAR-100 (20)
100
200
300
400
500
Number of Clusters
0.8
1.0
1.2
1.4
1.6
1.8
Coding Bits
1e6
(c) MS-COCO (150)
0
200
400
600
800
1000
Number of Clusters
3.0
3.5
4.0
4.5
5.0
5.5
6.0
6.5
Coding Bits
1e7
(d) LAION-Aes. (300)
Figure 5: Model selection for clustering without knowing the number of clusters on 4 datasets using
Algorithm 1; see also ¬ß3.3. For each dataset, the elbow point of the curve indicates the optimal
number (in parenthesis) of clusters. The model selection is done efficiently without any retraining.
seashore
sports car
xmas tree
bicycle
valley
comic
vase
joy stick
church
desktop 
computer
windsor tie
surfboard
Figure 6: Images automatically clustered and labeled by our algorithm on MS-COCO (Left) and
LAION-Aesthetics (Right). We refer interested readers to the Appendix E.2 for more visualization.
Finding Optimal Number of Clusters. We empirically verify Algorithm 1 for finding an optimal
number of clusters. Figure 5 left shows the results on CIFAR-10 and CIFAR-100. Our finding
suggests that the optimal numbers of clusters are 10 and 20 respectively; this echoes the finding
in prior works [34, 35] Figure 5 shows that the optimal number cluster for LAION-Aesthetics and
MS-COCO are 300 and 150 respectively. We will use these two numbers to cluster LAION-Aesthetics
and MS-COCO in the next paragraph. More results can be found in Appendix C.
Labeling Cluster with Text. We present the results of our clustering conducted on the LAION-
Aesthetics and MS-COCO datasets. The scale and diversity of data in these collections surpass those
of previous datasets, leaving us without a quantitative basis for comparison. A visualization of the
learned clusters and the corresponding semantic labels can be found in Figure 6, with additional results
included in Appendix E.2. From these depictions, it‚Äôs clear that our model, CPP, has successfully
identified meaningful clusters and assigned appropriate semantic labels to these groupings on large
scale data.
5
Conclusion and Discussion
This paper proposes a pipeline to do representation learning and clustering on large-scale datasets.
The pipeline, dubbed CPP, takes advantage of CLIP pre-trained model. CPP achieves state-of-the-art
clustering performance on CIFAR-10, -20, -100, and ImageNet-1k. Further, when the number of
clusters is unknown, we give a mechanism for CPP that estimates the optimal number of clusters,
without any costly retraining of deep networks. Finally, CPP refines the CLIP model, by giving
more accurate clustering, as well as more diverse and discriminative representation, allowing better
image2image search.
As for future work, we find it fascinating to explore the continual learning setting since real-world big
data come only in a streaming fashion with new modes continuously showing up. It is also of interest
to learn a diverse and discriminative representation and to cluster data with both text and image input.
9

References
[1] Stuart Lloyd. Least squares quantization in PCM. Technical report, Bell Laboratories, 1957.
[2] Edward Forgey. Cluster analysis of multivariate data: Efficiency vs. interpretability of classification.
Biometrics, 1965.
[3] R C Jancey. Multidimensional group analysis. Australian Journal of Botany, 14:127‚Äì130, 1966.
[4] James B McQueen. Some methods for classification and analysis of multivariate observations. In Fifth
Berkeley Symposium on Mathematical Statistics and Probability, pages 281‚Äì297, 1967.
[5] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang,
Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C Berg, and Li Fei-Fei. ImageNet large
scale visual recognition challenge. Int. J. Comput. Vis., 115(3):211‚Äì252, December 2015.
[6] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti,
Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An open large-scale
dataset for training next generation image-text models. arXiv preprint arXiv:2210.08402, 2022.
[7] Paul Bradley, Olvi Mangasarian, and W Street. Clustering via concave minimization. In Advances in
neural information processing systems, 1996.
[8] David Arthur and Sergei Vassilvitskii. k-means++: The advantages of careful seeding. In the Eighteenth
Annual ACM-SIAM Symposium on Discrete Algorithms. Society for Industrial and Applied Mathematics,
June 2006.
[9] Bahman Bahmani, Benjamin Moseley, Andrea Vattani, Ravi Kumar, and Sergei Vassilvitskii. Scalable
K-Means++. Proceedings VLDB Endowment, 5(7), March 2012.
[10] R Souvenir and R Pless. Manifold clustering. In Tenth IEEE International Conference on Computer Vision
(ICCV‚Äô05) Volume 1, volume 1, pages 648‚Äì653 Vol. 1, October 2005.
[11] Ehsan Elhamifar and Ren√© Vidal. Sparse manifold clustering and embedding. Advances in neural
information processing systems, 24, 2011.
[12] Vishal M Patel and Rene Vidal. Kernel sparse subspace clustering. In 2014 IEEE International Conference
on Image Processing (ICIP). IEEE, October 2014.
[13] Ehsan Elhamifar and Rene Vidal. Sparse subspace clustering. In IEEE Conference on Computer Vision
and Pattern Recognition, pages 2790‚Äì2797, June 2009.
[14] Ehsan Elhamifar and Rene Vidal. Sparse subspace clustering: Algorithm, theory, and applications. IEEE
transactions on pattern analysis and machine intelligence, 35(11):2765‚Äì2781, 2013.
[15] Can-Yi Lu, Hai Min, Zhong-Qiu Zhao, Lin Zhu, De-Shuang Huang, and Shuicheng Yan. Robust and
efficient subspace segmentation via least squares regression. In European conference on computer vision,
pages 347‚Äì360. Springer, 2012.
[16] Guangcan Liu, Zhouchen Lin, Shuicheng Yan, Ju Sun, Yong Yu, and Yi Ma. Robust recovery of subspace
structures by low-rank representation. IEEE transactions on pattern analysis and machine intelligence,
35(1):171‚Äì184, January 2013.
[17] Reinhard Heckel and Helmut B√∂lcskei. Robust subspace clustering via thresholding. IEEE transactions on
information theory, 61(11):6320‚Äì6342, 2015.
[18] Chong You, Chun Guang Li, Daniel P Robinson, and Rene Vidal. Oracle based active set algorithm for
scalable elastic net subspace clustering. In the IEEE conference on computer vision and pattern recognition,
pages 3928‚Äì3937, 2016.
[19] Sungwon Park, Sungwon Han, Sundong Kim, Danu Kim, Sungkyu Park, Seunghoon Hong, and Meeyoung
Cha. Improving unsupervised image clustering with robust learning. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition, pages 12278‚Äì12287, 2021.
[20] Yunfan Li, Peng Hu, Zitao Liu, Dezhong Peng, Joey Tianyi Zhou, and Xi Peng. Contrastive clustering. In
Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pages 8547‚Äì8555, 2021.
[21] Kouta Nakata Yaling Tao, Kentaro Takagi.
Clustering-friendly representation learning via instance
discrimination and feature decorrelation. Proceedings of ICLR 2021, 2021.
10

[22] Aniket Anand Deshmukh, Jayanth Reddy Regatti, Eren Manavoglu, and Urun Dogan. Representation
learning for clustering via building consensus.
Springer Machine Learning Journal arXiv preprint
arXiv:2105.01289, 2021.
[23] Chuang Niu and Ge Wang. Spice: Semantic pseudo-labeling for image clustering, 2021.
[24] Foivos Ntelemis, Yaochu Jin, and Spencer A Thomas. Information maximization clustering via multi-view
self-labelling. Knowledge-Based Systems, 250:109042, 2022.
[25] Zengyi Li, Yubei Chen, Yann LeCun, and Friedrich T Sommer. Neural manifold clustering and embedding.
arXiv preprint arXiv:2201.10000, 2022.
[26] Li Yunfan, Yang Mouxing, Peng Dezhong, Li Taihao, Huang Jiantao, and Peng Xi. Twin contrastive
learning for online clustering. International Journal of Computer Vision, 2022.
[27] Mohammadreza Sadeghi, Hadi Hojjati, and Narges Armanfard. C3: Cross-instance guided contrastive
clustering. arXiv preprint arXiv:2211.07136, 2022.
[28] Wouter Van Gansbeke, Simon Vandenhende, Stamatios Georgoulis, Marc Proesmans, and Luc Van Gool.
SCAN: Learning to classify images without labels. In European conference on computer vision. Springer,
2020.
[29] Nikolas Adaloglou, Felix Michels, Hamza Kalisch, and Markus Kollmann. Exploring the limits of deep
image clustering using pretrained models. March 2023.
[30] Tianjiao Ding, Shengbang Tong, Kwan Ho Ryan Chan, Xili Dai, Yi Ma, and Benjamin D Haeffele.
Unsupervised manifold linearizing and clustering. arXiv preprint arXiv:2301.01805, 2023.
[31] Sameer A Nene, Shree K Nayar, Hiroshi Murase, et al. Columbia object image library (coil-20). 1996.
[32] Yann LeCun. The mnist database of handwritten digits. http://yann. lecun. com/exdb/mnist/, 1998.
[33] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.
[34] Wouter Van Gansbeke, Simon Vandenhende, Stamatios Georgoulis, Marc Proesmans, and Luc Van Gool.
Scan: Learning to classify images without labels. In Computer Vision‚ÄìECCV 2020: 16th European
Conference, Glasgow, UK, August 23‚Äì28, 2020, Proceedings, Part X, pages 268‚Äì285. Springer, 2020.
[35] Chuang Niu, Hongming Shan, and Ge Wang. Spice: Semantic pseudo-labeling for image clustering. IEEE
Transactions on Image Processing, 31:7264‚Äì7278, 2022.
[36] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for
contrastive learning of visual representations. In International conference on machine learning, pages
1597‚Äì1607. PMLR, 2020.
[37] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised
visual representation learning. In Proceedings of the IEEE/CVF conference on computer vision and pattern
recognition, pages 9729‚Äì9738, 2020.
[38] Adrien Bardes, Jean Ponce, and Yann LeCun. Vicreg: Variance-invariance-covariance regularization for
self-supervised learning. arXiv preprint arXiv:2105.04906, 2021.
[39] Shengbang Tong, Yubei Chen, Yi Ma, and Yann Lecun. Emp-ssl: Towards self-supervised learning in one
training epoch. arXiv preprint arXiv:2304.03977, 2023.
[40] Yaodong Yu, Kwan Ho Ryan Chan, Chong You, Chaobing Song, and Yi Ma. Learning diverse and
discriminative representations via the principle of maximal coding rate reduction. Advances in Neural
Information Processing Systems, 33:9422‚Äì9434, 2020.
[41] Xili Dai, Shengbang Tong, Mingyang Li, Ziyang Wu, Michael Psenka, Kwan Ho Ryan Chan, Pengyuan
Zhai, Yaodong Yu, Xiaojun Yuan, Heung-Yeung Shum, et al. Ctrl: Closed-loop transcription to an ldr via
minimaxing rate reduction. Entropy, 24(4):456, 2022.
[42] Xili Dai, Ke Chen, Shengbang Tong, Jingyuan Zhang, Xingjian Gao, Mingyang Li, Druv Pai, Yuexiang
Zhai, XIaojun Yuan, Heung-Yeung Shum, et al. Closed-loop transcription via convolutional sparse coding.
arXiv preprint arXiv:2302.09347, 2023.
[43] Ziyang Wu, Christina Baek, Chong You, and Yi Ma. Incremental learning via rate reduction. arXiv, 2020.
11

[44] Shengbang Tong, Xili Dai, Ziyang Wu, Mingyang Li, Brent Yi, and Yi Ma. Incremental learning of
structured memory via closed-loop transcription. arXiv preprint arXiv:2202.05411, 2022.
[45] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish
Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from
natural language supervision. In International conference on machine learning, pages 8748‚Äì8763. PMLR,
2021.
[46] Mathilde Caron, Hugo Touvron, Ishan Misra, Herv√© J√©gou, Julien Mairal, Piotr Bojanowski, and Armand
Joulin. Emerging properties in self-supervised vision transformers. In Proceedings of the IEEE/CVF
international conference on computer vision, pages 9650‚Äì9660, 2021.
[47] Maxime Oquab, Timoth√©e Darcet, Th√©o Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre
Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visual
features without supervision. arXiv preprint arXiv:2304.07193, 2023.
[48] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll√°r,
and C Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer Vision‚ÄìECCV 2014:
13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13, pages
740‚Äì755. Springer, 2014.
[49] Jean-Bastien Grill, Florian Strub, Florent Altch√©, Corentin Tallec, Pierre Richemond, Elena Buchatskaya,
Carl Doersch, Bernardo Avila Pires, Zhaohan Guo, Mohammad Gheshlaghi Azar, et al. Bootstrap your
own latent-a new approach to self-supervised learning. Advances in neural information processing systems,
33:21271‚Äì21284, 2020.
[50] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Doll√°r, and Ross Girshick. Masked autoencoders
are scalable vision learners. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition, pages 16000‚Äì16009, 2022.
[51] Jinghao Zhou, Chen Wei, Huiyu Wang, Wei Shen, Cihang Xie, Alan Yuille, and Tao Kong. ibot: Image
bert pre-training with online tokenizer. arXiv preprint arXiv:2111.07832, 2021.
[52] Hangbo Bao, Li Dong, Songhao Piao, and Furu Wei. Beit: Bert pre-training of image transformers. arXiv
preprint arXiv:2106.08254, 2021.
[53] Mert Bulent Sariyildiz, Julien Perez, and Diane Larlus. Learning visual representations with caption
annotations. In Computer Vision‚ÄìECCV 2020: 16th European Conference, Glasgow, UK, August 23‚Äì28,
2020, Proceedings, Part VIII 16, pages 153‚Äì170. Springer, 2020.
[54] Karan Desai and Justin Johnson. Virtex: Learning visual representations from textual annotations. In
Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 11162‚Äì11173,
2021.
[55] Gabriel Ilharco, Mitchell Wortsman, Ross Wightman, Cade Gordon, Nicholas Carlini, Rohan Taori, Achal
Dave, Vaishaal Shankar, Hongseok Namkoong, John Miller, Hannaneh Hajishirzi, Ali Farhadi, and Ludwig
Schmidt. Openclip, July 2021.
[56] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas
Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth
16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020.
[57] Xingzhi Zhou and Nevin L Zhang. Deep clustering with features from self-supervised pretraining. arXiv
preprint arXiv:2207.13364, 2022.
[58] Markos Georgopoulos, James Oldfield, Grigorios G Chrysos, and Yannis Panagakis. Cluster-guided image
synthesis with unconditional models. In Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition, pages 11543‚Äì11552, 2022.
[59] Shengbang Tong, Xili Dai, Yubei Chen, Mingyang Li, Zengyi Li, Brent Yi, Yann LeCun, and Yi Ma.
Unsupervised learning of structured representations via closed-loop transcription.
arXiv preprint
arXiv:2210.16782, 2022.
[60] Yunji Kim and Jung-Woo Ha. Contrastive fine-grained class clustering via generative adversarial networks.
arXiv preprint arXiv:2112.14971, 2021.
[61] Yi Ma, Harm Derksen, Wei Hong, and John Wright. Segmentation of multivariate mixed data via
lossy data coding and compression. IEEE transactions on pattern analysis and machine intelligence,
29(9):1546‚Äì1562, 2007.
12

[62] Yaodong Yu, Kwan Ho Ryan Chan, Chong You, Chaobing Song, and Yi Ma. Learning diverse and
discriminative representations via the principle of maximal coding rate reduction. In Neural Information
Processing Systems, June 2020.
[63] Kwan Ho Ryan Chan, Yaodong Yu, Chong You, Haozhi Qi, John Wright, and Yi Ma. Deep networks from
the principle of rate reduction. October 2020.
[64] Xili Dai, Shengbang Tong, Mingyang Li, Ziyang Wu, Michael Psenka, Kwan Ho Ryan Chan, Pengyuan
Zhai, Yaodong Yu, Xiaojun Yuan, Heung Yeung Shum, and Yi Ma. Closed-Loop data transcription to an
LDR via minimaxing rate reduction. November 2021.
[65] Christina Baek, Ziyang Wu, Kwan Ho Ryan Chan, Tianjiao Ding, Yi Ma, and Benjamin D Haeffele.
Efficient maximal coding rate reduction by variational forms. In IEEE/CVF Conference on Computer
Vision and Pattern Recognition, pages 500‚Äì508, 2022.
[66] Xiaotian Han, Zhimeng Jiang, Ninghao Liu, Qingquan Song, Jundong Li, and Xia Hu. Geometric graph
representation learning via maximizing rate reduction. February 2022.
[67] Derek Lim, Ren√© Vidal, and Benjamin D Haeffele. Doubly stochastic subspace clustering. arXiv [cs.LG],
November 2020.
[68] Tianjiao Ding, Derek Lim, Rene Vidal, and Benjamin D Haeffele. Understanding doubly stochastic
clustering. In the 39th International Conference on Machine Learning, volume 162 of Proceedings of
Machine Learning Research, pages 5153‚Äì5165. PMLR, 2022.
[69] Marvin Eisenberger, Aysim Toker, Laura Leal-Taix√©, Florian Bernard, and Daniel Cremers. A unified
framework for implicit sinkhorn differentiation. In IEEE/CVF Conference on Computer Vision and Pattern
Recognition, pages 509‚Äì518, 2022.
[70] Michael E Sander, Pierre Ablin, Mathieu Blondel, and Gabriel Peyr√©. Sinkformers: Transformers with
doubly stochastic attention. In International Conference on Artificial Intelligence and Statistics, October
2021.
[71] Ulrike von Luxburg. A tutorial on spectral clustering. Statistics and computing, 17(4):395‚Äì416, 2007.
[72] Yi Ma, Harm Derksen, Wei Hong, and John Wright. Segmentation of multivariate mixed data via
lossy data coding and compression. IEEE transactions on pattern analysis and machine intelligence,
29(9):1546‚Äì1562, 2007.
[73] Chong You, Daniel P Robinson, and Ren√© Vidal. Scalable sparse subspace clustering by orthogonal
matching pursuit. In IEEE Conference on Computer Vision and Pattern Recognition, June 2016.
[74] Niv Cohen and Yedid Hoshen.
Dataset summarization by k principal concepts.
arXiv preprint
arXiv:2104.03952, 2021.
13

A
Training Details
This section provides the training details - network architecture and hyper parameters.
Network Architecture Details. For all datasets, we utilize a simple architecture composed of
three parts: pre-feature layer, feature head and cluster head. Pre-feature layer has a structure with
Linear-BatchNorm-ReLU-Linear-ReLU, with detailed setting in Table 3a. For feature head and
cluster head, we use a linear layer respectively as is described in Table 3b.
Table 3: Network Architecture
(a) Pre-feature layer
Linear: R768 ‚àí‚ÜíRdhidden
BatchNorm1d(dhidden)
ReLU
Linear: Rdhidden ‚àí‚ÜíRdhidden
ReLU
(b) Feature head and cluster head
Feature head
Linear: Rdhidden ‚àí‚ÜíRd
Cluster head
Linear: Rdhidden ‚àí‚ÜíRd
Dimensions. Dimension d and dhidden for each dataset are provided in Table 4a, note that d should be
larger than or equal to the expected number of clusters to satisfy the orthogonal subspace assumption.
Optimizers. We specify two independent optimizers to simultaneously optimize the MLC objective
with detailed parameters in Table 4b.
Sinkhorn Distance. The doubly stochastic membership matrix Œ† is computed by a sinkhorn distance
projection on C‚ä§C, where the parameters Œ≥ regulate the sparsity of the membership matrix as is
described in Section 3.1. Details with this parameter are recorded in Table 4c.
Initialization and Training Epochs. Details in initialization(simply optimize R(Z; Œµ)) epochs and
total training epochs are recorded in Table 4d.
14

Table 4: Hyperparameters
(a) Model Parameters. We adjust the dimension of
learned features for the different expected numbers
of clusters.
Datasets/Parameters
d
dhidden
CIFAR-10
128
4096
CIFAR-20
128
4096
CIFAR-100
128
4096
ImageNet-1k
1024
2048
MS-COCO
128
4096
LAION-Aesthetics
1024
2048
(b) Optimizers. We optimize the objective function
using SGD optimizer with unified parameters as
below:
Optimizers
Type
lr
wd
momentum
Feature
SGD
0.0001
0.0001
0.9
Cluster
SGD
0.0001
0.005
0.9
(c) Sinkhorn Distance Parameters while Training
Datasets
Œ≥
Iter
CIFAR-10
0.175
5
CIFAR-20
0.13
5
CIFAR-100
0.1
5
ImageNet-1k
0.12
5
COCO
0.12
5
LAION
0.09
5
(d) Initialization epoch, total training epoch, batch
size. Batch size doesn‚Äôt affect too much on the
performance. All experiments can be conducted
on a single A100.
Datasets
epochinit
epochtotal
bs
CIFAR-10
1
5
1024
CIFAR-20
1
15
1024
CIFAR-100
1
50
1500
ImageNet-1k
2
20
1024
COCO
1
20
1200
LAION
2
20
1024
B
Subspace Clustering Parameters
We conduct subspace clustering methods on CLIP features and report the highest accuracy after
searching for optimal parameters.
EnSC. Both EnSC and SSC-OMP5 estimate a membership matrix via solving some convex optimiza-
tions that depend only on CLIP features, and then run spectral clustering on the resulting membership.
For EnSC, we use the efficient active-set solvers from [18] to solve the convex optimization. EnSC
has two parameters Œ≥, œÑ. Roughly speaking, œÑ ‚àà[0, 1] balances between an ‚Ñì1 and an ‚Ñì2 penalty on
the membership, with larger œÑ giving sparser affinity; Œ≥ > 0 is the weight of the data fidelity error,
aside from the regularizing term.
SSC-OMP. (kmax, œµ) We use the OMP solver for SSC [73]. kmax is the maximum number of
non-zero entries of each row of the membership, while œµ controls the allowed data fidelity error.
Spectral Clustering. Œ≥ denotes the parameter for sink horn distance projection, which is the same
as the one mentioned in previous sections. For a given batch of CLIP‚Äôs feature C‚Ä≤ ‚ààRd√ón, we first
normalize each feature vector and then do inner production plus sink horn distance projection, i.e.
Œ†CLIP = proj‚Ñ¶,Œ≥(C‚Ä≤‚ä§C‚Ä≤). We then do spectral clustering on this membership matrix Œ†CLIP .
Table 5: Parameter search with the following parameters for EnSC, SSC-OMP and spectral clustering.
We report the highest performance on Table 1.
Datasets
Parameters
EnSC
Œ≥ ‚àà[1, 5, 10, 50, 100], œÑ ‚àà[0.9, 0.95, 1.0]
SSC-OMP
kmax ‚àà[3, 5, 10], œµ ‚àà[1e ‚àí4, 1e ‚àí5, 1e ‚àí6, 1e ‚àí7]
Spectral Clustering
Œ≥ ‚àà[0.2, 0.18, 0.16, 0.1, 0.09, 0.08, 0.07, 0.06]
5The
implementations
are
provided
by
the
authors
at
https://github.com/ChongYou/
subspace-clustering.
15

C
More Results on Optimal Number of Clusters
We additionally measure the coding length for ImageNet as is shown in Figure 7. For all datasets,
we compute the coding length with œµ2 = 0.1, which is consistent with the one in MLC objective
function.
0
200
400
600
800
1000
Number of Clusters
1.05
1.10
1.15
1.20
1.25
1.30
1.35
1.40
1.45
Coding Bits
1e7
Figure 7: ImageNet (200)
D
Image To Image Search
D.1
Pipeline
Figure 8 demonstrates the pipeline of image-to-image search. In practice, the image repository is
composed of 1.2M images from ImageNet‚Äôs training split while the target image is randomly picked
from ImageNet‚Äôs validation split. We search the images in the repository via measuring the Euclidean
distance and plot the 64 most similar images.
Image Repository
Target Image
Target Feature
Feature Set
Euclidean 
Distance
Top 64
Backbone
ùúÇ
Pre-feature layer
Feature
head
Figure 8: Image-to-Image Search Pipeline.
D.2
More Results
Here, we provide 10 more image-to-image search results in Figure 9. We observe from these results
that CPP learned better representation that facilitates image-to-image search.
16

Figure 9: Searching for similar images in ImageNet‚Äôs training split. Left:Target image from
validation split in ImageNet.Middle: searched images via CPP‚Äôs representation; Right: searched
images via CLIP‚Äôs representation.
E
More Results on Clustering and Labelling with Text
E.1
Algorithm
We introduce a cluster-labeling algorithm after we obtain a well-trained CPP. First, we do spectral
clustering upon the membership matrix given by CPP and get clusters of images. Then, for images in
each cluster, we conduct weighted voting for the common labels. The voting algorithm is described
in Algorithm 2.
Algorithm 2: Labeling one cluster
Input: Images from one learned cluster X ‚ààRN√ó3√ó224√ó224, M text candidates, 0 initialized
voting result vector V ‚ààRM
Zimg ‚ÜêCLIP: encode images(X)
Ztxt ‚ÜêCLIP: encode texts(text candidates)
For i ‚Üê1, . . . , N:
Scores4labels ‚ÜêCosine Similarity for(Zi
img, Ztxt)
(8)
Valid Score ‚ÜêScore4labels[top5]
(9)
V ‚ÜêV + Valid Score
(10)
Output: Label for this cluster: text candidates[argmax V ]
17

E.2
More Results
In this section, we visualize more labeling-clusters results for datasets including CIFAR-100,
ImageNet-1k, COCO and LAION-Aesthetics. We also follow the optimal number of clusters
measured in Section 4.4 for each dataset.
tricycle, trike, velocipede: 27.37; 
(a) tricycle.
brown bear, bruin, Ursus arctos: 55.89; 
(b) brown bear.
goblet: 63.35; 
(c) goblet
steel arch bridge: 18.14; 
(d) steel arch bridge
crib, cot: 75.10; 
(e) crib
baby: 33.45; 
(f) baby
streetcar, tram, tramcar, trolley, trolley car: 95.31; 
(g) streetcar.
poppy: 36.12; 
(h) poppy
sweet_pepper: 83.25; 
(i) sweet pepper
agaric: 53.12; 
(j) agaric
r whale, killer, orca, grampus, sea wolf, Orcinus orca: 5
(k) whale.
Arabian camel, dromedary, Camelus dromedarius: 68.92  
(l) Arabian camel.
lion, king of beasts, Panthera leo: 50.23; 
(m) lion.
y tank, armored combat vehicle, armoured combat vehi
(n) armored vehicle.
chimpanzee, chimp, Pan troglodytes: 53.34; 
(o) chimp.
maple_tree: 101.27; 
(p) maple tree
dial telephone, dial phone: 57.36; 
(q) dial telephone.
lawn_mower: 41.23; 
(r) lawn mower
sunflower: 30.26; 
(s) sunflower
bee: 24.04; 
(t) bee
Figure 10: Clustering CIFAR-100 into 20 clusters and relabeling them using our pipeline.
18

head cabbage: 10.25; 
(a) head cabbage
parking meter: 15.03; 
(b) parking meter
manhole cover: 15.77; 
(c) manhole cover
gibbon, Hylobates lar: 15.49; 
(d) gibbon
Rhodesian ridgeback: 13.10; 
(e) Rhodesian ridgeback
snorkel: 14.18; 
(f) snorkel
lorikeet: 19.46; 
(g) lorikeet
keeshond: 14.00; 
(h) keeshond
weevil: 13.41; 
(i) weevil
buckeye, horse chestnut, conker: 15.56; 
(j) buckeye
guenon, guenon monkey: 18.28; 
(k) guenon
geyser: 20.95; 
(l) geyser
Urutu (a type of viper): 22.19; 
(m) Urutu
guacamole: 16.84; 
(n) guacamole
pill bottle: 12.45; 
(o) pill bottle
French horn, horn: 11.01; 
(p) French horn
entertainment center: 19.16; 
(q) entertainment center
African grey, African gray, Psittacus erithacus: 13.55; 
(r) African grey
EntleBucher: 12.64; 
(s) EntleBucher
hip, rose hip, rosehip: 16.39; 
(t) rose hip
Figure 11: Clustering ImageNet(15k random samples from train split) into 200 clusters and relabeling
them using our pipeline. (Randomly selected 20 clusters) Non-square figures represent that images
within that cluster are not enough to fulfill the 8 √ó 8 grid.
19

(a) ballplayer
(b) joystick
(c) stove
(d) Giraffe
(e) motor scooter
(f) trailer truck
(g) groom
(h) church
(i) cellular phone
(j) Police car
(k) Windsor tie
(l) Vegetable garden
(m) desktop computer
(n) surfboard
(o) Tennis racket
(p) Vase
(q) Fire extinguisher
(r) Sun umbrella
(s) soccer ball
(t) dock
Figure 12: Clustering COCO (30k random samples) into 150 clusters and labeling them using our
pipeline. (Randomly selected 20 clusters)
20

Dollhouse kit: 19.42; 
(a) Dollhouse kit
web site, website, internet site, site: 1.67; 
(b) website
seashore, coast, seacoast, sea-coast: 11.84; 
(c) seashore
cocktail shaker: 36.42; 
(d) cocktail
Loaf pan: 21.27; 
(e) Loaf pan
Vegetable garden: 5.36; 
(f) Vegetable garden
Needle felting kit: 18.09; 
(g) needle fitting kit
fountain: 3.89; 
(h) fountain
Dollhouse kit: 29.57; 
(i) Dollhouse kit
Quilted tablecloth: 8.10; 
(j) Quiltered tablecloth
suit, suit of clothes: 5.13; 
(k) suit
gown: 41.42; 
(l) gown
accordion, piano accordion, squeeze box: 6.75; 
(m) accordion
suit, suit of clothes: 4.74; 
(n) suit
gown: 60.27; 
(o) gown
Quilted tablecloth: 8.59; 
(p) Quiltered tablecloth
Egg: 14.74; 
(q) Egg
Model T: 23.56; 
(r) Model T
mosque: 4.83; 
(s) mosque
pickup, pickup truck: 99.93; 
(t) pickup truck
Figure 13: Clustering LAION-Aesthetic into 300 clusters and labeling them using our pipeline.
(Randomly selected 20 clusters)
21

