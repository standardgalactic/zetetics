CAN FOUNDATION MODELS HELP US ACHIEVE PERFECT
SECRECY?
Simran Arora
Stanford University
Stanford, CA
simran@cs.stanford.edu
Christopher R√©
Stanford University
Stanford, CA
chrismre@cs.stanford.edu
January 10, 2023
ABSTRACT
A key promise of machine learning is the ability to assist users with personal tasks. Because the
personal context required to make accurate predictions is often sensitive, we require systems that
protect privacy. A gold standard privacy-preserving system will satisfy perfect secrecy, meaning that
interactions with the system provably reveal no private information. However, privacy and quality
appear to be in tension in existing systems for personal tasks. Neural models typically require
copious amounts of training to perform well, while individual users typically hold a limited scale of
data, so federated learning (FL) systems propose to learn from the aggregate data of multiple users.
FL does not provide perfect secrecy, but rather practitioners apply statistical notions of privacy ‚Äî
i.e., the probability of learning private information about a user should be reasonably low. The
strength of the privacy guarantee is governed by privacy parameters. Numerous privacy attacks have
been demonstrated on FL systems and it can be challenging to reason about the appropriate privacy
parameters for a privacy-sensitive use case. Therefore our work proposes a simple baseline for FL,
which both provides the stronger perfect secrecy guarantee and does not require setting any privacy
parameters. We initiate the study of when and where an emerging tool in ML ‚Äî the in-context
learning abilities of recent pretrained models ‚Äî can be an effective baseline alongside FL. We Ô¨Ånd
in-context learning is competitive with strong FL baselines on 6 of 7 popular benchmarks from the
privacy literature and a real-world case study, which is disjoint from the pretraining data. We release
our code here: https://github.com/simran-arora/focus
1
Introduction
A key promise of machine learning is the ability to assist users with personal tasks. Given the private nature of the
personal data, these systems should satisfy three desiderata: (1) no leakage of private information, (2) quality, and
(3) feasibility. Guided by these desiderata, we propose in-context learning, an emergent capability of recent pretrained
models, as a simple approach for privacy sensitive workloads Bommasani et al. [2021], Wei et al. [2022a].
The ideal privacy system will offer perfect secrecy: as users interact with the system, the probability that adversaries
learn private information should not increase Shannon [1949]. A trivial way to achieve this guarantee is by purely
local training or Ô¨Åne-tuning a public model on a user‚Äôs private dataset. However, recent neural models require copious
amounts of training data Dodge et al. [2020] and users often hold a limited scale of labeled data. On average across our
evaluation tasks, an individual user has 149 (std. dev. 191) labeled training examples. To address the challenge that
individual users lack sufÔ¨Åcient data, federated learning (FL) over data spanning multiple privacy scopes (i.e. users)
has emerged as a popular approach Shokri and Shmatikov [2015], McMahan et al. [2016]. Indeed Wu et al. [2022a]
Ô¨Ånds that the local training baseline performs up to 60% worse than FL on average across users.
Since requiring all users to send data to a central location sacriÔ¨Åces data privacy, FL trains a task model by sending
the model between users and a central server. In each training iteration, (1) the model is sent to each user, (2) users
compute gradients on their local data and send the results back to the central server, where (3) they are aggregated to
arXiv:2205.13722v2  [cs.LG]  8 Jan 2023

Figure 1: Towards the goal of providing users with perfect secrecy guarantes, we propose in-context learning as a privacy baseline.
The proposal entails shipping FMs to users and using in-context learning to adapt to personal tasks.
update the central model. FL has gained widespread popularity as an approach to privacy. Evaluating FL along the
desiderata:
‚Ä¢ Privacy FL does not require users to upload raw private data, however private information can be recovered
from the exposed model Shokri et al. [2017], Melis et al. [2019], Nasr et al. [2019], Bagdasaryan et al. [2020].
To improve the privacy guarantee over the training inputs, a popular strategy is to insert statistical noise to
the training procedure Dwork et al. [2006]. Inserting more noise improves the privacy guarantee, but can
degrade quality. The world‚Äôs largest organizations struggle to reason about how to set the appropriate privacy
parameters Greenberg [2017], McMahan et al. [2018].
‚Ä¢ Quality Standard FL algorithms improve model performance for the average user. However private data
often widely differs by individual and performance Ô¨Çuctuates across participants Zhao et al. [2018], Wu et al.
[2022a]. Personalized FL is a popular research area aimed at addressing this challenge [Arivazhagan et al.,
2019, Li et al., 2022a, inter alia.], but can be brittle in non-IID settings. Wu et al. [2022a] demonstrates
that even under personalized FL algorithms, up to 40% of clients experience worse performance than if a
non-personalized FL algorithm were used on certain tasks. Finally, adversarial participants can corrupt the
FL procedure Yin et al. [2018].
‚Ä¢ Feasibility Executing FL requires rounds of communication between multiple users KoneÀácn√Ω et al. [2016].
The protocol must be repeated for every personal task a user wishes to perform. FL also requires aligning
the incentives of users who share task goals and may exclude users who cannot contribute training data Zhan
et al. [2021].
Given the challenges and opportunities of FL, we revisit the question of when users should apply FL to their privacy
sensitive workloads. SpeciÔ¨Åcally, we observe natural compatibilities between the capabilities of recent pretrained
models, referred to in the literature as Foundation Models (FMs) Bommasani et al. [2021], and the above challenges.
Traditionally, the models are adapted to downstream tasks through a task speciÔ¨Åc Ô¨Åne-tuning step in which all model
weights are updated. Recent FMs often provide impressive quality on new downstream tasks out-of-the-box or with
minimal effort, simply given a textual description of the task and zero-to-few task demonstrations, which is known as
in-context learning (ICL) Brown et al. [2020], Radford et al. [2021]. ICL is applied at inference time; no additional
training is required. Interestingly, ICL has proven effective in tasks that are widely different from the objective for
which the FM was originally trained Chowdhery et al. [2022], Liu et al. [2022].
In this work, we observe that ICL can provide perfect secrecy, in contrast to FL, and therefore investigate how the
performance and systems feasibility compare to FL. While prior work compares ICL to other non-private methods of
adapting FMs to downstream tasks, such as lightweight or full Ô¨Åne-tuning Brown et al. [2020], Liu et al. [2021], we
initiate the study of ICL as it compares to federated learning. Our proposed architecture entails sending off-the-shelf
public FMs to private user silos and locally applying in-context learning to perform personal privacy-sensitive tasks.
We initiate the study of when and where this simple baseline may be effective by studying 9 popular FMs and 7 popular
evaluation settings in the privacy literature, which span language and vision.
2

‚Ä¢ Privacy: What privacy guarantee does ICL provide? ICL provides perfect secrecy given the unidirec-
tional information Ô¨Çow: FMs are sent to the user, no information leaves the user silo. Formally, ICL uses
an access control based privacy framework Bell and LaPadula [1976]. Beyond data privacy, we observe ICL
offers Task-Privacy. In FL, users share the same task goal and label schema in order to train a central model,
but ICL hides what the user is doing and how they are doing it (i.e. the label schema for the task).
‚Ä¢ Quality: Is ICL competitive with FL? Privacy tasks have not previously been benchmarked with ICL
techniques. We initiate this benchmarking and Ô¨Ånd the ICL baseline is competitive on 6 of 7 tasks from the
privacy literature. However, FL performs much better on Ô¨Åne-grained tasks and when the inductive bias of
the FM is misaligned with the task. We also evaluate on a real-world information extraction task on data that
is disjoint from the FM pretraining corpus.
‚Ä¢ Feasibility: What are the systems costs of ICL? We characterize the landscape of costs in terms of (1) the
training and inference costs: ICL eliminates the costs of training and communicating the model for multiple
iterations, but incurs large inference costs; and (2) the number of users who need to share the same task goal
in order for FL to succeed: under ICL, users do not need to share the same task objectives. Finally, the FL
literature traditionally reports the cost of supporting a single task. Yet, users may want to perform multiple
private tasks over their data (e.g., classiÔ¨Åcation and QA). We propose practitioners should incorporate the
number of tasks we can support with a single model when evaluating our privacy baselines ‚Äî one FM can
support multiple tasks.
In summary, we initiate the study of in-context learning with recent FMs as a baseline for FL. We conduct an extensive
empirical analysis of the proposed baseline and hope this encourages further work on how we can provide users with
perfect secrecy guarantees for their sensitive tasks.
2
Background and Related Work
We focus on performing personal tasks over privacy-sensitive data. Complementary work studies how to release FMs
that have been trained over sensitive data Tian et al. [2022], Li et al. [2022b], which is beyond our scope. We further
focus on the setting where users have a limited amount of personal data as in [Caldas et al., 2019, Dodge et al., 2020,
Wu et al., 2022a, inter alia.].
Perfect secrecy A system preserves perfect secrecy if the probability of an adversary obtaining knowledge of a client‚Äôs
private data does not increase as the client interacts with the system Shannon [1949]. As the system performs multiple
tasks over the same underlying private data, the probability an adversary obtains knowledge of the private data should
not increase. Mathematically, these are logical notions of privacy Miklau and Suciu [2004].
Machine learning for personal tasks The above properties are achievable through training or Ô¨Åne-tuning on only the
user‚Äôs private data, for which existing work assumes users own a sufÔ¨Åcient amount of private labeled training data Xu
et al. [2018]. However, Ô¨Åne-tuning is found to be unstable in the low-data regime Dodge et al. [2020], Mosbach et al.
[2021], Wu et al. [2022a]. Therefore federated learning (FL) proposes to train a model over the private data owned by
multiple parties McMahan et al. [2016]. FL initializes a global model Œ∏ and solves:
min
Œ∏
G(L(Œ∏; D1), ..., L(Œ∏; Di), ..., L(Œ∏; DN))
where L(Œ∏; Di) are the local objectives for each client, and G(¬∑) is a function of the local objectives. In training, each
client receives a copy of model Œ∏t at the current timestep t from a central server, performs gradient descent on the local
data Di, and sends the gradients to the central server, where the local gradients are combined using G(¬∑) to produce
Œ∏t+1. FedAvg McMahan et al. [2016], a weighted sum of the local objectives, is the vanilla choice for G(¬∑). Li et al.
[2019] and Appendix 7.5 describe other objectives.
To achieve personalization in FL, popular approaches include using a mixture of local and global model updates,
training a global model with model-agnostic meta-learning so it may be quickly adapted to local data, and initializing
the global model with a pretrained initial model Fallah et al. [2020], Zhao et al. [2018], Hilmkil et al. [2021], Wu et al.
[2022a]. These methods improve upon training from scratch with FedAvg, though do not provide perfect privacy.
Empirically, the improvements of these methods over FedAvg also appear to degrade in increasingly non-IID settings
Chen et al. [2021], Arivazhagan et al. [2019]. Wu et al. [2022a] reports that up to 40% of clients can experience worse
performance under personal FL compared to non-personal FedAvg on certain tasks.
The key constraint under FL is that the raw data Di never leaves the private silo for client i, however, functions of
Di may be exposed. Unfortunately, attacks on the gradients or inference attacks on the Ô¨Ånal model output can reveal
private information Shokri et al. [2017], Melis et al. [2019]. Techniques to protect the privacy of the communicated
3

parameters include differential privacy (DP) Dwork et al. [2006], which degrades model performance McMahan et al.
[2018], or secure multiparty computation Bonawitz et al. [2017], which does not protect against the membership
inference attacks and signiÔ¨Åcantly increases the computational cost Fang and Qian [2021].
Foundation models Foundation models (FMs) are trained via self-supervised objectives on large corpora of publicly
available data (e.g. web pages or image-caption pairs). Traditionally, the models are adapted to downstream tasks
through a task speciÔ¨Åc Ô¨Åne-tuning step in which all model weights are updated. Impressively, recent FMs can solve
new tasks by conditioning their predictions on natural language speciÔ¨Åcations of the task, with no additional training.
This is referred to as in-context learning (ICL).
We are certainly not the Ô¨Årst to use public models to achieve improved privacy guarantees Kerrigan et al. [2020], Yu
et al. [2020], Li et al. [2022b]. The prior works use the FM as an initialization and adapt the model to downstream
personal tasks via federated Ô¨Åne-tuning. We initiate the study of ICL as a baseline for privacy-preserving ML and
evaluate whether perfect secrecy is feasible using ICL.
3
In-Context Learning Baseline
In this section, we introduce the in-context learning baseline.
3.1
Baseline
Consider client i who wants to perform several personal tasks t ‚ààTi over their private data. For t, the client has a
private dataset Dit = {xj, yj}j=ni
j=1 , where ni ‚àà[0, k] for some small k. The ICL architecture centers on a unidirec-
tional dataÔ¨Çow, as depicted in Figure 1. The user downloads the FM to the private silo and uses in-context learning to
perform the private task.
In-context learning
Language FMs are pretrained to learn a probability distribution pFM(x) over large coprora of
text x, drawn from the distribution p, such that pFM ‚âàp. Popular LMs are factorized as pFM(x) = Q|x|
i=1 pLM(xi|x<i).
The FM is tasked with predicting the next token xi given the context x<i. Recent FMs have been trained on large
amounts of diverse data, and demonstrate the ability to transfer to new tasks simply given natural language descriptions
of the task Brown et al. [2020], Wei et al. [2022a]. This inference-time adaptation is termed in-context learning.
Current text-image FMs are pretrained over pairs of images and paired text data in the same latent representation
space. Given pairs Dpaired = {(vi, xi)}npaired
i=1
, for image vi and text xi drawn from pFM, the FM learns embedding
functions fimage and ftext such that the distances between the embeddings dist(fimage(vi), ftext(xj)) reÔ¨Çect the
semantic similarity between vi and xj for all i, j. During inference, the user can design contexts x1, ..., xk (for
instance representing different classes in a classiÔ¨Åcation task) and given a new image, determine the most similar text
(class).
Designing the context
The context is deÔ¨Åned by a template, which contains placeholders for the task description
and demonstrations of the inputs and outputs for the task. Context design is a brittle process ‚Äî small changes to the
template and choice of demonstrations can result in large performance variations Zhao et al. [2021] ‚Äî so signiÔ¨Åcant
recent work studies how to write effective contexts [Mishra et al., 2021, Wu et al., 2022b, Wei et al., 2022b, inter
alia.]. In this work we use standard task-agnostic templates ‚Äî we randomly select demonstrations from the user‚Äôs
local dataset Dit for task t ‚Äî to understand the performance of ICL without over-engineering the contexts.
3.2
Properties
The ideal baseline for solving personal tasks over privacy-sensitive data should provide strong privacy guarantees and
high quality along ML metrics. Here we deÔ¨Åne privacy model of our proposal and analyze when we expect the ICL
baseline to be effective.
Privacy guarantee
ICL provides access control privacy with perfect secrecy (œµ = 0) since no private data leaves
the user device Bell and LaPadula [1976], Hu et al. [2006]. In our setting, the FM is owned by public entities and
users own zero-to-few private task demonstrations and task descriptions, which remain on device. No private data or
function of private data is exposed to untrustworthy subjects. FL violates this rule by exposing functions of the private
data to untrustworthy entities. Untrustworthy entities cannot send data to the user under our framework. FL violates
this by allowing the central server and other users to tamper with the data incorporated in the user‚Äôs local model.
4

Additionally task privacy, which encompases the task (e.g. sentiment classiÔ¨Åcation or QA) the user is performing
and the label schema (e.g. binary or 5-class sentiment classiÔ¨Åcation) immediately emerge from ICL, while neither is
protected under FL.
Quality
ICL is expected to be compelling in settings with the following data and task properties:
1. Non-IID user data FL is challenging over non-IID data and requires users to share the same task goals,
sacriÔ¨Åcing task privacy. With ICL, the client task and data distributions are independently handled.
2. Public and user distributions ICL is likely effective when the sensitive tasks of interest are well-represented
in the public pretraining data. The degree to which sensitive tasks are reÔ¨Çected in pretraining is an open-
question.
3. Data scale A client‚Äôs training dataset size, ni, is small enough that training or Ô¨Åne-tuning purely locally
results in low-quality models Dodge et al. [2020], Mosbach et al. [2021], Wu et al. [2022a]. ICL uses 0 ‚àík
task demonstrations to guide the FM to perform new tasks.
4
Experiments and Analysis
To study the beneÔ¨Åts and limitations of FMs for privacy, we ask the following questions: (1) Is ICL competitive in
quality to leading privacy frameworks? How does this vary by FM size and bias, and task properties? (2) To what
degree does ICL enable personalization to the user‚Äôs data distribution? (3) Is ICL effective on data that is disjoint
from the pretraining distribution?
4.1
Experimental Setup
Benchmarks We use a representative set of standard benchmarks in the privacy literature, which are useful proxies
for personal tasks such as intent and content classiÔ¨Åcation, message completion, and QA Caldas et al. [2019], He et al.
[2020], Lai et al. [2021], Lin et al. [2021]. Each benchmark contains a small number of examples per user with mean
149, standard deviation 191 examples across users and tasks.
Our tasks include: Text classiÔ¨Åcation Sentiment140 2-way Go et al. [2009] and 20News 20-way classiÔ¨Åcation bench-
marks Lang [1995], Image classiÔ¨Åcation CelebA binary Liu et al. [2015], CIFAR10 10-way Krizhevsky [2009], and
Federated EMNIST 62-way classiÔ¨Åcation benchmarks Cohen et al. [2017], Language modeling Reddit benchmark
Caldas et al. [2019], Reading Comprehension MRQA benchmark Fisch et al. [2019].
Foundation models
We use a representative set of FMs including T0 (3B and 11B parameters) Sanh et al. [2022],
and GPT-3 (125M, 1.3B, 2.7B, 6.7B and 175B p.) Brown et al. [2020] for prompting tasks, MPNet-base bi-encoders
for textual zero-shot similarity-search (110M p.) Reimers and Gurevych [2019], Song et al. [2020], and CLIP for
image zero-shot similarity-search (150M p.) Radford et al. [2021].
We characterize the above models by their pretraining strategy. T0 was Ô¨Åne-tuned on pairs of context-input-output
tuples to improve its ICL preformance. On the other hand, GPT simply uses next word prediction and no Ô¨Åne-tuning.
The above models report the pretraining data and we conÔ¨Årm that the private benchmarks are not included during
pretraining. 1 We also release our code and prompts.
Federated learning baselines
We report FL numbers where FL is initialized with SoTA pretrained architectures and
task adaptation is performed using FedAvg. FL is initialized with ViT(S) (22M p.) and ResNet101 (45M p.) for vision
tasks, DistillBERT for 20News and MRQA (67 M p.), and Stacked-LSTM for Sent140 and next-word prediction (1M
p.). FedAvg is performed over 10k-1Ms of examples, 5-1000s clients and 10-1000s rounds of model communication
for each task He et al. [2020], Caldas et al. [2019], Lin et al. [2021], Lai et al. [2021].
Model selection
An extended discussion of how the ICL and FL baselines are selected is in Appendix 7, and a
concrete memory and computation comparison is in Section 5. Overall, there is a large search space for designing the
context in in-context learning [Wu et al., 2022b, Mishra et al., 2021, Wei et al., 2022b, inter alia.], and similarly in
1However for Reddit, most FMs are trained on generic Common Crawl, which contains Reddit data. We thus use Grover Zellers
et al. [2019], which are pretrained only on news articles and Ô¨Ånd their result directly match or exceed the performance of GPT
models of the same size ‚Äî 9.6% for GPT-125M vs. 11.4% for Grover-124M, and 13.2% for GPT-1.3B vs. 13.4% for Grover-1.5B.
Due to the limited range of Gover sizes and given the comparable quality of both models, we proceed with GPT variants.
5

BENCHMARK
IN-CONTEXT LEARNING
FEDERATED LEARNING
Method
Accuracy
Method
Accuracy
Reddit
Prompting (175B)
13.6
FedAvg (1M) Caldas et al. [2019]
13.4
MRQA*
Prompting (175B)
64.1
FedAvg (67M) Lin et al. [2021]
27.1
Sent140
Prompting (175B)
80.4
FedAvg (1M) Michieli and Ozay [2021]
69.5
20News
Prompting (175B)
31.3
FedAvg (67M) Lin et al. [2021]
51.4
Sent140
Prompting (11B)
78.2
FedAvg (1M) Michieli and Ozay [2021]
69.5
20News
Prompting (3B)
19.1
FedAvg (67M) Lin et al. [2021]
51.4
Sent140
Similarity-search (110M)
61.5
FedAvg (1M) Michieli and Ozay [2021]
69.5
20News
Similarity-search (110M)
63.4
FedAvg (67M) Lin et al. [2021]
51.4
CelebA
Similarity-search (150M)
86.7
FedAvg (22M) Qu et al. [2022]
86.6
CIFAR10
Similarity-search (150M)
88.2
FedAvg (22M) McMahan et al. [2016]
98.5/86.9**
F-EMNIST
Similarity-search (150M)
22.3
FedAvg (45M) Rothchild et al. [2020]
81.0
Table 1: Test results on standard privacy benchmarks using FMs in zero-shot adaptation. Italics indicates the FM was Ô¨Åne-tuned
on task or instruction-label pairs after pretraining. *F1 score. **Trained from scratch.
FL, there several training objectives have been proposed. We compare ICL to FL baselines that apply no differential
privacy or protections against adversarial parties, or for certain benchmarks, client heterogeneity, all of which degrade
performance McMahan et al. [2018], Li et al. [2022a], Rothchild et al. [2020], Sun et al. [2021].
Given the many potential variations to each baseline, we compare the standard implementations for each. The purpose
of the evaluation is to uncover which portions of the privacy-quality-feasibility tradeoff space the baselines occupy
and not to conclude one baseline is strictly preferable.
4.2
Does ICL give competitive quality?
First we evaluate the performance of ICL using zero-shot prompts, i.e. the context contains a task description and
no task demonstrations. Results in Table 1 show that in the zero-shot setting, FM performance competes with FL
performance on 6 of 7 benchmarks. Critically, the above ICL numbers provide perfect secrecy and the FL numbers
provide no privacy. We next analyze when ICL is competitive based on the FM size, task-difÔ¨Åculty, and applied
prompting strategy.
Model size
Larger FMs reliably provide higher quality (Figure 2). Figure 2 shows FMs that were Ô¨Åne-tuned for
ICL in blue and FMs without Ô¨Åne-tuning in red. Fine-tuning can drastically improve performance with orders of
magnitude fewer parameters (e.g., on 20News with bi-encoders), however, when there is misalignment between the
Ô¨Åne-tuning and the downstream tasks, these FMs fail. In particular, inspecting T0-3B FM mispredictions on 20News,
for 24.4% of examples, the model generated one of the valid classes for a particularly similar topic classiÔ¨Åcation task
called AGNews, which, unlike 20News, was part of the T0 training data. The issue is far more severe for T0-11B,
which entirely fails on the task. This never happens for GPT mispredictions. ICL Ô¨Åne-tuning over more data may help
maintain the model generality.
Task-difÔ¨Åculty
FMs entirely fail on the Ô¨Åne-grained classiÔ¨Åcation tasks, F-EMNIST and 20News, which are rela-
tively disjoint from the public pretraining data. To study this, we create 4-class synthetic classiÔ¨Åcation tasks using sub-
sets of 20News. The Coarse synthetic requires classifying an input ‚àà{politics, automobiles, baseball, or medicine},
while the Fine synthetic requires classifying an input ‚àà{politics, politics of guns, politics in the Middle East, or re-
ligion}. GPT-175B incurs a 55% drop from Fine to Coarse (Figure 2 (left)). Further, zero-shot performance is quite
uneven across classes on F-EMNIST and 20News (Figure 2 (right)) and across users with non-IID private distributions
(Figure 7, Appendix 7.5), suggesting FMs are not robust zero-shot.
Prompting Strategy
We Ô¨Ånally implement and evaluate two advanced ICL methods beyond zero-shot prompting
to study the opportunities for improved performance under ICL, which include:
‚Ä¢ Prompt-decomposition For tasks with large label-spaces, we decompose the task such that the model must
decide between a smaller number of label choices in each prompt, to reduce task-difÔ¨Åculty. For instance in
the 20-Way News task, we can (1) decompose the label space into 5 groups of 4 labels each at random, (2)
prompt the FM 5 times to output the 5 respective labels of each 4-way classiÔ¨Åcation, and (3) prompt the FM
to output the answer from the shortlist of 5 labels.
6

=HUR6KRW$FFXUDF\





%LHQFRGHU
7%
7%
*370
*37%
*37%
*37%
*37%
6HQW6HQWLPHQW&ODVVLILFDWLRQ
=HUR6KRW$FFXUDF\





%LHQFRGHU
7%
7%
*370
*37%
*37%
*37%
*37%
1HZV7RSLF&ODVVLILFDWLRQ
=HUR6KRW$FFXUDF\





*37% *37%
&RDUVH
)LQH
6\QWKHWLF:D\&ODVVLILFDWLRQ
F-EMNIST (Similarity Search)
20News (Similarity Search)
Accuracy
Accuracy
Density
Density
-0.2
0
0.2
0.4
0.6
0.8
1.0
1.2
-0.2
0
0.2
0.4
0.6
0.8
1.0
1.2
Figure 2: Top: Zero-shot performance by model size and type. Blue indicates ICL Ô¨Åne-tuned FMs and Red indicates non Ô¨Åne-
tuned FMs. Bottom: FMs struggle on Ô¨Åne-grained classiÔ¨Åcation (left) and can provide highly uneven zero-shot performance across
classes (right). Additional violin plots by user level accuracy are in Appendix 7.5.
Privacy Benchmark
Method
Accuracy
20News
Zero-Shot Baseline
31.3
Prompt-Decomposition
48.1 (+53.7%)
Prompt-Calibration Zhao et al. [2021]
58.0 (+87.1%)
Table 2: Results using advanced FM prompting methods within the ICL framework on standard privacy benchmarks. These results
use the GPT3-175B FM, each on a random set of 100 test examples.
.
‚Ä¢ Prompt-calibration Zhao et al. [2021]. The motivation for this approach is that the initial FM might be
biased towards certain output classes and we can use Platt scaling to estimate and adjust for this bias. Given
the raw FM output probabilities per class, p, we can adjust using the equation: q = softmax(Wp + b)).
The method involves taking the FM‚Äôs token-probabilities on ‚Äúcontent-free inputs‚Äù (e.g. passing in the empty
string to the FM), and setting the values for W and b such that the outputs for the label-tokens have a uniform
probability distribution. Biases can arise because some label tokens are more common than others.
Results from applying the two methods are in Table 2. On 20News, prompt-decomposition results in a 53.7% im-
provement over the naive zero-shot FM baseline in Table 1 (31.3%). This brings performance to 3.3% points below
the FL baseline. Meanwhile, applying the calibration method (with no additional decomposition) results in an 87.1%
improvement over the standard FM baseline, which is 6.6% above the FL baseline in Table 1.
4.3
To what degree does ICL allow personalizing to the user‚Äôs data distribution?
Personalization is a well-studied question in the FL literature Arivazhagan et al. [2019], Fallah et al. [2020] and we
similarly evaluate ICL along this axis. Each user often holds a few training examples per task which are used for
personalization. We consider users with < k and 0 train examples, for some small k.
Users with few < k private examples.
Now we include task demonstrations in the context for ICL. We compare:
randomly chosen training examples from the aggregate pool of all users‚Äô training data (‚ÄúNo User Privacy‚Äù) and ex-
amples chosen from the few training examples the user has available (‚ÄúUser Privacy‚Äù). The context includes no task
description to focus on the effect of task demonstrations. We report results for K ‚àà{0, 3, 5} on Reddit and Sent140
in Figure 3.
We observe: (1) K > 0 task demonstrations provides improvements over the zero-shot baseline across model sizes,
barring GPT-6.7B on Reddit. (2) Few-shot learning enables small FMs to match the performance of FMs with orders
of magnitude more parameters ‚Äî for example, on Sent140, comparing GPT-125M k-shot performance to GPT-6.7B
0-shot, and GPT-6.7B k-shot to GPT-175B 0-shot. (3) We observe personal user-level demonstrations consistently
outperform non-personal contexts.
7







0
%
%
%
%
8VHU/HYHO6KRW
8VHU/HYHOVKRW
1R8VHU3ULYDF\6KRW
1R8VHU3ULYDF\6KRW
6KRW

)/%DVHOLQH
)HZ6KRW5HGGLW
$FFXUDF\





0 %
%
% %
6KRW
3XEOLF
1R8VHU3ULYDF\
8VHU3ULYDF\
/HYHOVRI3URPSW3HUVRQDOL]DWLRQ
$FFXUDF\





0 %
%
% %
)HZVKRW6HQW
Figure 3: Left: Sent140 results including the ‚ÄúPublic Prompt‚Äù baseline. Right: Reddit and Sent140 results comparing 0-shot
vs. k-shot performance by k, and comparing k-shot with examples selected from the user‚Äôs small labeled training dataset (‚ÄúUser
Privacy‚Äù) vs. randomly from the aggregate training dataset of all users (‚ÄúNo User Privacy‚Äù). Accuracy is averaged across users.
Figure 4: Sample phone numbers embedded in the ads.
Users with k = 0 private examples.
Several personal tasks (e.g., spam classiÔ¨Åcation, message generation) are of
interest to many users. We suggest that a public prompt repository, or library of public k-shot examples, can be shipped
down to users along with the FM. 2 Accordingly, we evaluate ‚ÄúPublic‚Äù third baseline beyond ‚ÄúNo User Privacy‚Äù and
‚ÄúUser Privacy‚Äù, where we use hard-coded task demonstrations for every inference example and user. As a proof of
concept, Figure 3 (left) demonstrates that this provides a clear boost over zero-shot FM performance.
Overall, there are clear improvements from the ‚ÄúPublic‚Äù to ‚ÄúNo User Privacy‚Äù to ‚ÄúUser Privacy‚Äù baseline, which is
intuitive from a distribution shift perspective. The FMs are sensitive to small peculiarities across users, demonstrating
the ability to incorporate personal information during inference. A few in-context examples enable using orders-of-
magnitude smaller FM. We include an extended analysis of why user-level examples may help in Appendix 7.5.
4.4
Real world case study: Does ICL support out-of-distribution sensitive tasks?
Thus far, we have evaluated ICL on traditional privacy benchmarks. A pessimistic view of the results is that the
canonical privacy benchmarks are not representative of sensitive tasks that users care about. While it is an open
question as to the proportion of sensitive tasks that are represented during pretraining, here we demonstrate a case
study on real-world sensitive data analysis tasks. We observe that ICL is effective on tasks that are clearly disjoint
from the data the FMs saw during pretraining Gao et al. [2021].
Out-of-Pretraining-Distribution Data
We evaluate two tasks: 1) phone-number and 2) incall and outcall price
information extraction from highly noisy human-trafÔ¨Åcking ads. The tasks are critical data-analysis steps for legal
entities and researchers seeking to investigate human-trafÔ¨Åcking patterns towards its prevention. The advertisements
were scraped from the website Backpage.com Delateur [2019], that "hosted more than 80 percent of the online adver-
tising for illegal commercial sex in the United States" doe [2016] until it was shut down by the Department of Justice
in 2018. The data is highly adversarial as advertisers use obscure and code language to hide from law-enforcement.
We include examples of phone-numbers as they appear in Figure 4.
Results
We evaluate on manually labeled evaluation sets of 100 ads for phone number extraction and 128 ads for
price extraction. As a baseline, the data analysts spent signiÔ¨Åcant effort designing regular expressions and using this,
their pipeline yields 61.4% on phone numbers and 58.6% of the prices. Using two Ô¨Åxed task demonstrations, GPT3-
175B achieves an 94.1% accuracy on phone number extraction and 86.6% accuracy on price extraction. GPT3-6.7B
achieves 74.3% on phone and 66.9% on price extraction.
2Excitingly, relevant resources are already in development, originally proposed for alternate purposes Bach et al. [2022].
8

Resource
ICL
(10B model)
FEDERATED LEARNING
(100M model)
Modern Phone
Communication
40 GB, 1.5 hours
download
40 GB, 13 hours upload &
download, barring latency
61 Mbps download & 8
Mbps upload
Training
None
1e16 FLOPs
16 TFLOPs, 4 GB RAM
Inference
2e13 FLOPs
1e11 FLOPs
16 TFLOPs, 4 GB RAM
Storage
40 GB on disk
400 MB on disk
1 TB disk
Table 3: Cost comparison. Assumes 100 communication rounds, 32 batch size, 1000 steps, & 512 sequence length for FL; 1024
max sequence length for the 10B model. Assumes full precision. Statistics for modern phones are from Freedman [2021] and spe
[2022].
Analysis
Though the ad data is disjoint from the pretraining data, ICL is effective and requires low effort (simply
two task demonstrations). Further, under FL, the analysts would need to repeat the procedure twice to train the phone
extraction and the price extraction models. Moreover consider if one analyst only is interested in phone extraction, the
second is only interested in prices, while a third is interested in a custom extraction. For each of these tasks of interest,
under FL, the analysts would need to assemble cohorts of other analysts with the same task goals to obtain the model.
Overall, the upfront cost is high with FL in this setting.
However, the 175B FM far outperforms the 6.7B FM, and acquiring the resources to host 175B models may be
challenging. Further, phone numbers and prices are intuitively concepts that are well-represented during pretraining,
even though the noisy phone numbers and prices from the ads are not found. We also see that on F-EMNIST and
20-News in Table 1, FL signiÔ¨Åcantly outperforms ICL. Overall, an important question to investigate in future work is
whether existing ML benchmarks are appropriate for measuring the ability of FMs to transfer to sensitive domains. It
is challenging to anticipate when and why FMs underperform and whether sensitive tasks that are of interest to users
are well-represented in the corpora used to pretrain popular FMs.
5
Systems Feasibility
We next examine the feasibility of the FL and ICL baselines. Additional discussion can be found in Appendix 7.
Systems costs under FL and ICL are shown in Table 3, using an order 100M parameter model for FL and 10B
parameter model for ICL, based on the trends in Section 4.1. Communication ICL requires locally downloading and
FL requires communicating the model repeatedly between users and the central server. Training requires (2 √ó 3 √ó
model parameters √ó steps √ó batch size √ó input length) FLOPs, and communication between clients and the central
server, a function of network download/upload speeds and model size. FMs in ICL are frozen. Inference takes
(2 √ó model parameters √ó input length) FLOPs, assuming key and value vectors for the attention computations are
cached. As we freeze and perform inference with FMs, we can use quantization (i.e., 8 or even 4-bit precision) and
pruning, to improve efÔ¨Åciency.
Critically, FL incurs these costs per personal task the client cares about. Meanwhile, it is possible for a single FM
to transfer to multiple tasks given different ICL contexts. The number of tasks served with the same model is an
important factor in evaluating the feasibility of the baselines.
Analysis
The limited RAM on modern phones is a key challenge for using large FMs on device today. Future-
looking, we are optimistic, noting: (1) readily available support for ofÔ¨Çoaded training and inference Ren et al. [2021],
Rajbhandari et al. [2021], (2) clear trends towards more RAM and effective small FMs app [2022], Tay et al. [2022],
and (3) many users own other capable devices beyond phones. Recently works demonstrate how to enable small and
open-source models to meet the quality of 30x larger foundation models using in-context learning Arora et al. [2022].
Incentives If the FM baseline is competitive even for a subset of the N clients, the subset is disincentivized from
participating. Not only do these clients avoid privacy leakage and communication costs of FL, but the fact that the
zero-shot abilities succeed for these and not the other clients indicates there may be a distribution mismatch between
the subsets of clients that make different decisions.
6
Conclusion
Amidst a recent focus on statistical notions of privacy, in light of in-context learning, perfect secrecy might be possible
for some sensitive tasks. While prior work initializes FL procedures from public pretrained models, we are the Ô¨Årst
9

to propose a complementary architecture based on ICL and no further training. There are several future challenges
such as the fragility of prompting, out-of-domain degradation, and slow runtime of inference using large models. We
release our code to help facilitate further study.
Acknowledgements
We thank Sabri Eyuboglu, Neel Guha, Michael Zhang, Laurel Orr, Kawin Ethayarajh, Deepak Narayanan, Mayee
Chen, Maya Varma, Gary Cheng, Rohith Kuditipudi, Xuechen Li, Sidd Karamcheti, and Rishi Bommasani for their
helpful feedback and discussions. We gratefully acknowledge the support of DARPA under Nos. FA86501827865
(SDH) and FA86501827882 (ASED); NIH under No. U54EB020405 (Mobilize), NSF under Nos. CCF1763315
(Beyond Sparsity), CCF1563078 (Volume to Velocity), and 1937301 (RTML); ONR under No. N000141712266
(Unifying Weak Supervision); the Moore Foundation, NXP, Xilinx, LETI-CEA, Intel, IBM, Microsoft, NEC, Toshiba,
TSMC, ARM, Hitachi, BASF, Accenture, Ericsson, Qualcomm, Analog Devices, the Okawa Foundation, American
Family Insurance, Google Cloud, Swiss Re, Brown Institute for Media Innovation, Department of Defense (DoD)
through the National Defense Science and Engineering Graduate Fellowship (NDSEG) Program, Fannie and John
Hertz Foundation, National Science Foundation Graduate Research Fellowship Program, Texas Instruments, [Stanford
SpeciÔ¨Åc Fellowships], and members of the Stanford DAWN project: Teradata, Facebook, Google, Ant Financial, NEC,
VMWare, and Infosys. The U.S. Government is authorized to reproduce and distribute reprints for Governmental pur-
poses notwithstanding any copyright notation thereon. Any opinions, Ô¨Åndings, and conclusions or recommendations
expressed in this material are those of the authors and do not necessarily reÔ¨Çect the views, policies, or endorsements,
either expressed or implied, of DARPA, NIH, ONR, or the U.S. Government.
References
Rishi Bommasani, Drew A. Hudson, E. Adeli, Russ Altman, Simran Arora, S. von Arx, Michael S. Bernstein,
Jeanette Bohg, A. Bosselut, Emma Brunskill, and et al. On the opportunities and risks of foundation models.
arXiv:2108.07258, 2021.
Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten
Bosma, Denny Zhou, Donald Metzler, Ed H. Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean,
and William Fedus. Emergent abilities of large language models, 2022a. URL https://arxiv.org/abs/2206.
07682.
Claude E. Shannon. Communication theory of secrecy systems. 1949. URL https://pages.cs.wisc.edu/~rist/
642-spring-2014/shannon-secrecy.pdf.
Jesse Dodge, Gabriel Ilharco, Roy Schwartz, Ali Farhadi, Hannaneh Hajishirzi, and Noah Smith. Fine-tuning pre-
trained language models: Weight initializations, data orders, and early stopping. In arXiv:2002.06305, 2020. URL
https://arxiv.org/pdf/2002.06305.pdf.
Reza Shokri and Vitaly Shmatikov. Privacy-preserving deep learning. Proceedings of the 22nd ACM SIGSAC Con-
ference on Computer and Communications Security (CCS), 2015. URL https://doi.org/10.1145/2810103.
2813687.
H. Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Ag√ºera y Arcas. Communication-
efÔ¨Åcient learning of deep networks from decentralized data. Proceedings of the 20th International Conference on
ArtiÔ¨Åcial Intelligence and Statistics (AISTATS), 2016.
Shanshan Wu, Tian Li, Zachary Charles, Yu Xiao, Ziyu Liu, Zheng Xu, and Virginia Smith. Motley: Benchmarking
heterogeneity and personalization in federated learning. 2022a. URL https://arxiv.org/abs/2206.09262.
Reza Shokri, Marco Stronati, Congzheng Song, and Vitaly Shmatikov. Membership inference attacks against machine
learning models. In the proceedings of the IEEE Symposium on Security and Privacy, 2017.
Luca Melis, Congzheng Song, Emiliano De Cristofaro, and Vitaly Shmatikov. Exploiting unintended feature leakage
in collaborative learning. In Proceedings of 40th IEEE Symposium on Security and Privacy, 2019.
Milad Nasr, Reza Shokri, and Amir Houmansadr. Comprehensive privacy analysis of deep learning: Passive and active
white-box inference attacks against centralized and federated learning. IEEE Symposium on Security and Privacy,
2019.
Eugene Bagdasaryan, Andreas Veit, Yiqing Hua, Deborah Estrin, and Vitaly Shmatikov. How to backdoor federated
learning. Proceedings of the Twenty Third International Conference on ArtiÔ¨Åcial Intelligence and Statistics (PMLR),
2020. URL http://proceedings.mlr.press/v108/bagdasaryan20a.html.
10

Cynthia Dwork, Frank McSherry, Kobbi Nissim, and Adam Smith. Calibrating noise to sensitivity in private data
analysis. Theory of Cryptography Conference, 2006.
Andy Greenberg. How one of apple‚Äôs key privacy safeguards falls short, 2017. URL https://www.wired.com/
story/apple-differential-privacy-shortcomings/.
H. Brendan McMahan, Daniel Ramage, Kunal Talwar, and Li Zhang. Learning differentially private recurrent language
models. ICLR, 2018.
Yue Zhao, Meng Li, Liangzhen Lai, Naveen Suda, Damon Civin, and Vikas Chandra. Federated learning with non-iid
data. arXiv:1806.00582, 2018.
Manoj Ghuhan Arivazhagan, Vinay Aggarwal, Aaditya Kumar Singh, and Sunav Choudhary. Federated learning with
personalization layers. In arXiv:1912.00818v1, 2019. URL https://arxiv.org/pdf/1912.00818.pdf.
Tian Li, Shengyuan Hu, Ahmad Beirami, and Virginia Smith. Ditto: Fair and robust federated learning through
personalization. In ICML 2021, 2022a. URL https://arxiv.org/abs/2012.04221.
Dong Yin, Yudong Chen, Kannan Ramchandran, and Peter Bartlett. Byzantine-robust distributed learning: Towards
optimal statistical rates. In ICML, 2018.
Jakub KoneÀácn√Ω, H. Brendan McMahan, Felix X. Yu, Peter Richt√°rik, Ananda Theertha Suresh, and Dave Bacon.
Federated learning: Strategies for improving communication efÔ¨Åciency. In arXiv:1610.05492, 2016.
Yufeng Zhan, Jie Zhang, Zicong Hong, Leijie Wu, Peng Li, and Song Guo. A survey of incentive mechanism design
for federated learning. IEEE Transactions on Emerging Topics in Computing, 2021. URL https://ieeexplore.
ieee.org/stamp/stamp.jsp?arnumber=9369019.
Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan,
Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom
Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark
Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish,
Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In arXiv:2005.14165,
2020.
Alec Radford, Jong Wook Ki, Chris Hallacy, Aditya Ramesh, Gabriel Go, Sandhini Agarwal, Girish Sastry, Amanda
Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models
from natural language supervision. arXiv:2103.00020, 2021.
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham,
Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways.
arXiv preprint arXiv:2204.02311, 2022.
Jiacheng Liu, Alisa Liu, Ximing Lu, Sean Welleck, Peter West, Ronan Le Bras, Yejin Choi, and Hannaneh Hajishirzi.
Generated knowledge prompting for commonsense reasoning. 2022. URL https://liujch1998.github.io/
assets/papers/GKP_v8.pdf.
Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig. Pre-train, prompt, and
predict: A systematic survey of prompting methods in natural language processing. In arXiv:2107.13586, 2021.
D. E. Bell and L. J. LaPadula. Secure computer system: UniÔ¨Åed exposition and multics interpretation. The MITRE
Corporation, 1976.
Yuanyishu Tian, Yao Wan, Lingjuan Lyu, Dezhong Yao, Hai Jin, and Lichao Sun. Fedbert: When federated learning
meets pre-training. In ACM Transactions on Intelligent Systems and Technology, 2022.
Xuechen Li, Florian Tramer, Percy Liang, and Tatsunori Hashimoto. Large language models can be strong differen-
tially private learners. In ICLR 2022, 2022b. URL https://openreview.net/pdf?id=bVuP3ltATMz.
Sebastian Caldas, Sai Meher, Karthik Duddu, Peter Wu, Tian Li, Jakub KoneÀácn√Ω, H. Brendan McMahan, Virginia
Smith, , and Ameet Talwalkar. Leaf: A benchmark for federated settings. In Workshop on Federated Learning for
Data Privacy and ConÔ¨Ådentiality, 2019.
Gerome Miklau and Dan Suciu. A formal analysis of information disclosure in data exchange. SIGMOD, 2004. URL
https://homes.cs.washington.edu/~suciu/base-sigmod2004.pdf.
Mengwei Xu, Feng Qian, Qiaozhu Mei, Kang Huang, and Xuanzhe Liu. Deeptype: On-device deep learning for input
personalization service with minimal privacy concern. Proceedings of the ACM on Interactive, Mobile, Wearable
and Ubiquitous Technologies, 2018.
Marius Mosbach, Maksym Andriushchenko, and Dietrich Klakow. On the stability of Ô¨Åne-tuning bert: Misconcep-
tions, explanations, and strong baselines. In ICLR 2021, 2021. URL https://arxiv.org/pdf/2006.04884.
pdf.
11

Tian Li, Anit Kumar Sahu, Ameet Talwalkar, and Virginia Smith. Federated learning: Challenges, methods, and future
directions. 2019. URL https://arxiv.org/pdf/1908.07873.pdf.
Alireza Fallah, Aryan Mokhtari, and Asuman Ozdaglar.
Personalized federated learning with theoretical
guarantees:
A model-agnostic meta-learning approach.
In 34th Conference on Neural Information Pro-
cessing Systems (NeurIPS 2020), 2020.
URL https://proceedings.neurips.cc/paper/2020/file/
24389bfe4fe2eba8bf9aa9203a44cdad-Paper.pdf.
Agrin Hilmkil, Sebastian Callh, Matteo Barbieri, Leon Rene Sutfeld, Edvin Listo Zec, and Olof Mogren. Scaling
federated learning for Ô¨Åne-tuning of large language models. In arXiv:2102.00875, 2021.
Jiangui Chen, Ruqing Zhang, Jiafeng Guo, Yixing Fan, , and Xueqi Cheng. Fedmatch: Federated learning over
heterogeneous question answering data. In Proceedings of the 30th ACM International Conference on Information
and Knowledge Management (CIKM ‚Äô21),, 2021. URL https://arxiv.org/pdf/2108.05069.pdf#page=10&
zoom=100,76,202.
Keith Bonawitz, Vladimir Ivanov, Ben Kreuter, Antonio Marcedone, H. Brendan McMahan, Sarvar Patel, Daniel
Ramage, Aaron Segal, , and Karn Seth. Practical secure aggregation for privacy-preserving machine learning.
Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security (CCS), 2017.
Haokun Fang and Quan Qian. Privacy preserving machine learning with homomorphic encryption and federated
learning. Future Internet, 2021. doi:https://doi.org/10.3390/Ô¨Å13040094.
Gavin Kerrigan, Dylan Slack, and Jens Tuyls. Differentially private language models beneÔ¨Åt from public pre-training.
In arXiv:2009.05886v2, 2020. URL https://arxiv.org/abs/2009.05886.
Da Yu, Saurabh Naik, Arturs Backurs, Sivakanth Gopi, Huseyin A. Inan, Gautam Kamath, Janardhan Kulkarni, Yin Tat
Lee, Andre Manoel, Lukas Wutschitz, Sergey Yekhanin, and Huishuai Zhang. Differentially private Ô¨Åne-tuning of
language models. In arXiv:2110.06500v2, 2020.
Tony Z. Zhao, 1 Eric Wallace, Shi Feng, Dan Klein, and Sameer Singh. Calibrate before use: Improving few-shot
performance of language models. In arXiv:2102.09690v2, 2021. URL https://arxiv.org/pdf/2102.09690.
pdf.
Swaroop Mishra, Daniel Khashabi, Chitta Baral, Yejin Choi, and Hannaneh Hajishirzi.
Reframing instructional
prompts to gptk‚Äôs language. arXiv preprint arXiv:2109.07830, 2021.
Tongshuang Wu, Michael Terry, and Carrie J. Cai. Ai chains: Transparent and controllable human-ai interaction by
chaining large language model prompts. In arXiv:2110.01691, 2022b. URL https://arxiv.org/pdf/2110.
01691.pdf.
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H. Chi, Quoc V. Le, and Denny
Zhou. Chain of thought prompting elicits reasoning in large language models. arXiv:2201.11903v4, 2022b.
Vincent C. Hu, David F. Ferraiolo, and D. Rick Kuhn. Assessment of access control systems. NIST, 2006. URL
https://nvlpubs.nist.gov/nistpubs/Legacy/IR/nistir7316.pdf.
Chaoyang He, Songze Li, Jinhyun So, Mi Zhang, Hongyi Wang, Xiaoyang Wang, Praneeth Vepakomma, Abhishek
Singh, Hang Qiu, Li Shen, Peilin Zhao, Yan Kang, Yang Liu, Ramesh Raskar, Qiang Yang, Murali Annavaram,
and Salman Avestimehr. Fedml: A research library and benchmark for federated machine learning. arXiv preprint
arXiv:2007.13518, 2020.
Fan Lai, Yinwei Dai, Xiangfeng Zhu, Harsha V. Madhyastha, and Mosharaf Chowdhury. Fedscale: Benchmarking
model and system performance of federated learning at scale. In arXiv:2105.11367, 2021.
Bill Yuchen Lin, Chaoyang He, ZiHang Zeng, Hulin Wang, Yufen Huang, M. Soltanolkotabi, Xiang Ren, and
S. Avestimehr. Fednlp: A research platform for federated learning in natural language processing. In arXiv cs.CL
2104.08815, 2021. URL https://arxiv.org/abs/2104.08815.
Alec Go, Richa Bhayani, , and Lei Huang. Twitter sentiment classiÔ¨Åcation using distant supervision. 2009.
Ken Lang. Newsweeder: Learning to Ô¨Ålter netnews. In In Proceedings of ICML, 1995.
Ziwei Liu, Ping Luo, Xiaogang Wang, , and Xiaoou Tang. Deep learning face attributes in the wild. 2015.
Alex Krizhevsky. Learning multiple layers of features from tiny images, 2009. URL https://www.cs.toronto.
edu/~kriz/cifar.html.
Gregory Cohen, Saeed Afshar, Jonathan Tapson, , and Andr√© van Schaik. Emnist: an extension of mnist to handwritten
letters. 2017.
Adam Fisch, Alon Talmor, Robin Jia, Minjoon Seo, Eunsol Choi, and Danqi Chen. Mrqa 2019 shared task: Evaluating
generalization in reading comprehension. 2019.
12

Victor Sanh, Albert Webson, Colin Raffel, Stephan H. Bach, Lintang Sutawika, Zaid Alyafeai, Antoine ChafÔ¨Ån,
Arnaud Stiegler, Teven Le Scao, Arun Raja, and et. al. Multitask prompted training enables zero-shot task general-
ization. In ICLR 2022, 2022. URL https://arxiv.org/pdf/2110.08207.pdf.
Nils Reimers and Iryna Gurevych. Sentence-bert: Sentence embeddings using siamese bert-networks. In Proceedings
of the 2019 Conference on Empirical Methods in Natural Language Processing. Association for Computational
Linguistics, 11 2019. URL https://arxiv.org/abs/1908.10084.
Kaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and Tie-Yan Liu. Mpnet: Masked and permuted pre-training for language
understanding. In arXiv:2004.09297, 2020. URL https://arxiv.org/abs/2004.09297.
Rowan Zellers, Ari Holtzman, Hannah Rashkin, Yonatan Bisk, Ali Farhadi, Franziska Roesner, and Yejin Choi. De-
fending against neural fake news. In Advances in Neural Information Processing Systems 32, 2019.
Umberto Michieli and Mete Ozay.
Are all users treated fairly in federated learning systems?
In CVPR,
2021.
URL https://openaccess.thecvf.com/content/CVPR2021W/RCV/papers/Michieli_Are_All_
Users_Treated_Fairly_in_Federated_Learning_Systems_CVPRW_2021_paper.pdf.
Liangqiong Qu, Yuyin Zhou, Paul Pu Liang, Yingda Xia, Feifei Wang, Ehsan Adeli, Li Fei-Fei, and Daniel Rubin.
Rethinking architecture design for tackling data heterogeneity in federated learning. In CVPR 2022, 2022. URL
https://arxiv.org/abs/2106.06047.
Daniel Rothchild, Ashwinee Panda, Enayat Ullah, Nikita Ivkin, Ion Stoica, Vladimir Braverman, Joseph Gonzalez,
and Raman Arora. Fetchsgd: Communication-efÔ¨Åcient federated learning with sketching. In Proceedings of the
37th International Conference on Machine Learning (PMLR), 2020. URL http://proceedings.mlr.press/
v119/rothchild20a/rothchild20a.pdf.
Taro Sun, Dongsheng Li, and Bao Wang. Decentralized federated averaging. In arXiv:2104.11375v1, 2021. URL
https://arxiv.org/abs/2012.04221.
Stephen H. Bach, Victor Sanh, Zheng-Xin Yong, Albert Webson, Colin Raffel, Nihal V. Nayak, Abheesht Sharma,
Taewoon Kim, M Saiful Bari, Thibault Fevry, Zaid Alyafeai, Manan Dey, Andrea Santilli, Zhiqing Sun, Srulik
Ben-David, Canwen Xu, Gunjan Chhablani, Han Wang, Jason Alan Fries, Maged S. Al-shaibani, Shanya Sharma,
Urmish Thakker, Khalid Almubarak, Xiangru Tang, Xiangru Tang, Mike Tian-Jian Jiang, and Alexander M. Rush.
Promptsource: An integrated development environment and repository for natural language prompts, 2022.
Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He,
Anish Thite, Noa Nabeshima, Shawn Presser, and Connor Leahy. The pile: An 800gb dataset of diverse text for
language modeling, 2021. URL https://arxiv.org/abs/2101.00027.
Monica J. Delateur. From craigslist to backpage.com: Conspiracy as a strategy to prosecute third-party websites for
sex trafÔ¨Åcking. In Santa Clara Law Review, pages 532‚Äì590, 2019. URL https://digitalcommons.law.scu.
edu/cgi/viewcontent.cgi?article=2826&context=lawreview.
Jane doe, et al., petitioners v. backpage.com llc, et al. In In the Supreme Court of the United States, 2016.
Jie Ren, Samyam Rajbhandari, Reza Yazdani Aminabadi, Olatunji Ruwase, Shuangyan Yang, Minjia Zhang, Dong
Li, and Yuxiong He. Zero-ofÔ¨Çoad: Democratizing billion-scale model training. In arXiv:2101.06840, 2021. URL
https://arxiv.org/abs/2101.06840.
Samyam Rajbhandari, Olatunji Ruwase, Jeff Rasley, Shaden Smith, and Yuxiong He. Zero-inÔ¨Ånity: Breaking the gpu
memory wall for extreme scale deep learning. In arXiv:2104.07857v1, 2021. URL https://arxiv.org/pdf/
2104.07857.pdf.
Blake‚Äôs
ios
device
speciÔ¨Åcations
grid,
April
2022.
URL
https://blakespot.com/ios_device_
specifications_grid.html.
Yi Tay, Mostafa Dehghani, Vinh Q. Tran, Xavier Garcia, Dara Bahri, Tal Schuster, Huaixiu Steven Zheng, Neil
Houlsby, and Donald Metzler. Unifying language learning paradigms. arXiv:2205.05131v1, 2022. URL https:
//arxiv.org/pdf/2205.05131.pdf.
Simran Arora, Avanika Narayan, Mayee F. Chen, Laurel Orr, Neel Guha, Kush Bhatia, Ines Chami, Frederic Sala, and
Christopher R√©. Ask me anything: A simple strategy for prompting language models. arXiv:2210.02441, 2022.
Andrew E. Freedman. Apple a15 bionic powers iphone 13 and ipad mini, September 2021. URL https://www.
tomshardware.com/news/ipad-iphone-13-a15-bionic.
United states‚Äô mobile and Ô¨Åxed broadband internet speeds, 2022.
URL https://www.speedtest.net/
global-index/united-states.
PyTorch.
Introducing accelerated pytorch training on mac, May 2022.
URL https://pytorch.org/blog/
introducing-accelerated-pytorch-training-on-mac/.
13

Jon
Peddie
and
Robert
Dow.
Q1‚Äô22
saw
a
decline
in
gpu
and
pc
shipments
quarter-to-quarter,
May
2022.
URL
https://www.jonpeddie.com/press-releases/
q122-saw-a-decline-in-gpu-and-pc-shipments-quarter-to-quarter#:~:text=The%20GPU‚Äôs%
20overall%20attach%20rate,%25%20year%2Dto%2Dyear.
Os statistics., July 2022. URL https://stats.foldingathome.org/os.
Michael Diskin, Alexey Bukhtiyarov, Max Ryabinin, Lucile Saulnier, Anton Sinitsin, Dmitry Popov, Dmitry V Pyrkin,
Maxim Kashirin, Alexander Borzunov, and et al. Albert Villanova del Moral. Distributed deep learning in open
collaborations. In arXiv:2106.10207v2, 2021.
Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini
Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, and et al. Training
language models to follow instructions with human feedback. In arXiv:2203.02155v1, 2022.
Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona
Diab, Xian Li, Xi Victoria Lin, and et. al. Opt: Open pre-trained transformer language models. In arXiv:2205.01068,
2022. URL https://arxiv.org/abs/2205.01068.
OpenAI. Openai api, Nov 2021. URL https://openai.com/api/.
Kang Wei, Jun Li, Ming Ding, Chuan Ma, Howard H. Yang, Farokhi Farhad, Shi Jin, Tony Q. S. Quek, and H. Vincent
Poor. Federated learning with differential privacy: Algorithms and performance analysis. arXiv:1911.00222, 2019.
URL https://arxiv.org/abs/1911.00222v2.
Eugene Bagdasaryan, Andreas Veit, Yiqing Hua, Deborah Estrin, and Vitaly Shmatikov. How to backdoor federated
learning. arXiv:1807.00459v3, 2019. URL https://arxiv.org/pdf/1807.00459.pdf.
Xinyi Xu and Lingjuan Lyu. Towards building a robust and fair federated learning system. arXiv:2011.10464v1, 2020.
URL https://arxiv.org/pdf/2011.10464v1.pdf.
14

Privacy Benchmark
Train Examples per User
Total Test Set Size
Reddit (Non-IID) Caldas et al. [2019]
34.1
25.5k
MRQA (Non-IID) Fisch et al. [2019]
501.5
3.0k
Sent140 (Non-IID) Go et al. [2009]
2.4
286.6k
20News (IID) Lang [1995]
113.1
7.5k
CelebA (IID) Liu et al. [2015]
21.4
11.1k
CIFAR-10 (IID) Krizhevsky [2009]
Varied
10.0k
F-EMNIST (IID) Cohen et al. [2017]
226.8
81.7k
Table 4: We show statistics for each benchmark and whether each has IID or Non-IID data per user.
.
7
Appendix: Experimental Details
7.1
Benchmark Protocols
We release our code for reproducability and future work, including task and class descriptions we use, prompt formats,
and task scoring functions. The benchmarks were originally proposed in Caldas et al. [2019], Lin et al. [2021] and
are openly-accessible.3 4 Benchmark size-statistics are shown in Table 4 and here we provide details of any dataset
sampling or prompting choice.
CelebA This is an image classiÔ¨Åcation benchmark and we evaluate using the CLIP-ViT32B model using the full test
dataset. We encode the binary class descriptions and match the image embeddings to the closest class embedding.
task_description = NA
class_descriptions = ["frowning", "smiling"]
CIFAR-10 This is an image classiÔ¨Åcation benchmark and we evaluate using the CLIP-ViT32B model using the full
test dataset. We encode the class descriptions and match the image embeddings to the closest class embedding.
task_description = NA
class_descriptions = ["airplane", "automobile", "bird", "cat", "deer", "dog", "frog", "horse",
"ship", "truck"]
F-EMNIST This is an image classiÔ¨Åcation benchmark and we evaluate using the CLIP-ViT32B model, using the full
test dataset. We encode the class descriptions and match the image embeddings to the closest class embedding. Note
that the CLIP models are not case-sensitive.
task_description = NA
class_descriptions = ["The picture is of the uppercase letter <<c>>", "The picture is of the
lowercase letter <<c>>", "The picture is of the digit <<c>>"] where the classname c is
inserted as <<c>>
Sent140 This is a text classiÔ¨Åcation benchmark and we evaluate using the GPT, T0, and bi-encoder model variants,
using a constant random subsample of 2.5% of clients (7.2k examples), due to cost restrictions. We include clients with
‚â•0 labeled training examples. For the autoregressive models, we map the output to a class using the Ô¨Årst generated
token (scoring code is released). For the bi-encoder model, we encode class descriptions ‚Äúpositive‚Äù and ‚Äú‚Äònegative‚Äù
and match the input embeddings to the closest class embedding. The Ô¨Åxed public prompt used in Figure 3 is shown
below:
task_description = "Is this text positive or negative? Text: "
class_descriptions = ["positive", "negative"]
20News This is a text classiÔ¨Åcation benchmark and we evaluate using the GPT, T0, and bi-encoder model variants,
using the full test dataset. For the autoregressive models, we map the output to a class using the Ô¨Årst generated token
3https://github.com/FedML-AI/FedNLP
4https://github.com/TalwalkarLab/leaf
15

(scoring code is released). For the bi-encoder model, we directly encode class descriptions as follows and match the
input embeddings to the closest class embedding.
task_description = "Is the topic electronics, cryptography security, religion, graphics, ibm pc
hardware, space, politics, mac hardware, motorcycles, politics middle east, sale, automobiles,
baseball, medical, christianity, os ms-windows, politics guns, atheism, hockey, or windows x? "
class_descriptions = ["electronics", "cryptography security", "religion", "graphics", "ibm pc
hardware", "space", "politics", "mac hardware", "motorcycles", "politics middle east", "sale",
"automobiles", "baseball", "medical", "christianity", "os ms-windows", "politics guns",
"atheism", "hockey", "windows x"]
Synthetic 20News This is a text classiÔ¨Åcation synthetic dataset constructed by subsampling 1k points from the 20News
dataset that have one of the desired labels. We evaluate using GPT-6.7B and GPT-175B. We speciÔ¨Åcally create two
synthetics, termed ‚ÄúCoarse‚Äù and ‚ÄúFine‚Äù. The Coarse details are as follows:
task_description = "Is the topic politics, baseball, medical, or automobiles? "
class_descriptions = ["politics", "baseball", "medical", "automobiles"]
The Fine details are as follows:
task_description = "Is the topic politics, religion, politics guns, or politics middle east? "
class_descriptions = ["politics", "religion", "politics guns", "politics middle east"]
MRQA This is a reading comprehension benchmark and we evaluate using GPT model, using the full test dataset.
Given an input question and context, we prompt the model as follows:
task_description =
"What is the answer span to the following question?
Obama was born in Honolulu, Hawaii. After graduating from Columbia University in 1983, he
worked as a community organizer in Chicago. In 1988, he enrolled in Harvard Law School,
where he was the first black president of the Harvard Law Review. After graduating, he
became a civil rights attorney and an academic, teaching constitutional law at the
University of Chicago Law School from 1992 to 2004. Turning to elective politics, he
represented the 13th district in the Illinois Senate from 1997 until 2004, when he ran for
the U.S. Senate. Obama received national attention in 2004 with his March Senate primary
win, his well-received July Democratic National Convention keynote address, and his
landslide November election to the Senate.
Question: When was Barack Obama born?
Answer: 1983
<<context>>
Question: <<input>>
Answer: "
class_descriptions = NA
Reddit This is a language modeling benchmark and we evaluate using the Grover and GPT variants, using the full test
dataset. However, for GPT-6.7B and GPT-175B we subsample 5% of the clients.
task_description = NA
class_descriptions = NA
7.2
Experimental Details
Inference-only strategies
For the results in Table 1 and Figure 2, we use the models zero-shot. For autoregressive
models, we simply prompt the model with the exact task descriptions speciÔ¨Åed above. For similarity search, we use the
16

exact class descriptions speciÔ¨Åed above. For all results in Figure 3, we use no task description to focus on the effect
of task demonstrations. Note that to hold the number of task demonstrations per user constant across baselines, the
‚ÄúNo User Privacy‚Äù baseline randomly selects the minimum of {number of user‚Äôs training examples, k} for k ‚àà{3, 5}
‚Äî for example on Sent140, the average number of task demonstrations per user is 2.4, and the number of in-context
examples available for the ‚ÄúUser Privacy‚Äù baseline is the user‚Äôs training data size.
7.3
Examples of each few-sample adaptation technique
Using a topic classiÔ¨Åcation task as an example, the following demonstrate examples of (1) prompting with task de-
scriptions or labeled examples, (2) zero-shot classiÔ¨Åcation via nearest neighbors similarity search, and (3) FM tuning.
An example of prompting follows. The above prompt includes the task description and the below prompt includes task
demonstrations.
// With Task Instructions (Zero Shot)
prompt = "
Is the text about medicine, baseball, atheism, electronics, ..., or hockey?
Text: I believe the Cubs have the best record ever in the MLB.
"
prediction = model.generate(prompt)
//
With Task Demonstrations (Few Shot)
prompt = "
Is the text about medicine, baseball, atheism, electronics, ..., or hockey?
Input: They detect the oscillator operating in the detector.
Label: electronics
Input: I believe the Cubs have the best record ever in the MLB.
Label:
"
prediction = model.generate(prompt)
>> print(prediction)
>> "baseball"
An example of similarity search follows.
input = "I believe the Cubs have the best record ever in the MLB."
class_descriptions = ["baseball", "hockey", "politics in the middle east", "electronics",
"medicine", "automobiles", "atheism", "cryptography"]
embedded_input = model.encode_query(input)
embedded_descriptions = model.encode_keys(class_descriptions)
// predict class with highest embedding similarity score to the input
prediction = max_similarity_class(encoded_input, encoded_descriptions)
>> print(prediction)
>> "baseball"
7.4
Additional Discussion of ICL Deployment
Here we discuss additional considerations for deploying ICL, elaborating on the systems feasibility, FM availability,
and FM prompting reliability challenges.
Systems Feasibility
In Section 5, we observe it would be highly inefÔ¨Åcient to use large FMs and FL on modern
mobile phones. However, we refer to the fact that several users and organizations also own devices beyond mobile
phones. In particular, modern personal computers are capable of performing training and inference for several-hundred
million parameter models PyTorch [2022]. Further, the personal computer and desktop GPU market shipped 96 million
units in Q1 2022 Peddie and Dow [2022]. The availability of GPUs by a large number of parties is evidenced by the
17

increasing number of projects that take advantage of available and underutilized personal GPUs fol [2022], Diskin
et al. [2021]. Large FMs can also be hosted on CPUs during inference. Meanwhile, hardware is improving at a rapid
rate Freedman [2021].
Meanwhile, recent progress has showed that Ô¨Åne-tuning smaller FMs to follow prompt-instructions improves zero-
shot generalization. For instance, Ô¨Ånetuned models ranging from 1.3B to 11B outperform their 175B pretrained
counterparts Ouyang et al. [2022]. Together trends point towards improved accessibility to FM baselines over time,
however deploying current billion-parameter FMs is challenging.
FM Availability
ICL relies on the availability of an FM. FMs both trained over general web data Zhang et al. [2022]
and over domain-speciÔ¨Åc data (e.g., medical, legal) are increasingly available. These resources expand the scope in
which ICL may be successful, however FMs are not available for all domains and many current FMs are only available
via API access, which does not protect data-privacy OpenAI [2021]. We are optimistic about the increasing availability
of open-source models Zhang et al. [2022].
FM Interfaces and Reliability
The next key deployment challenge for ICL is to develop interfaces for users to
express prompts and integrate personal data-sources. Current FMs have largely been used by ML practitioners and
prompt engineering is a temperamental process, especially for more complex tasks of interest. Progress on human-
computer interfaces for FMs is improving the user experience. For instance, Wu et al. [2022b] propose to decompose
complex tasks into sub-steps, each of which is easier for users to prompt and debug. However, decomposition-based
methods introduce additional inference-calls to the FM (i.e. a new call for each sub-step), which increases latency.
These are interesting tradeoffs to study in future work.
Improving Quality
As shown in Section 4.1, FM performance drastically underperforms FL on Ô¨Åne-grained clas-
siÔ¨Åcation tasks, using standard baselines. In Appendix 7.5, we provide additional experiments using more advanced
FM-prompting strategies.
Finally, we note that as users interact with models, their outward behaviors and decisions may change over time. This
can occur under any privacy framework or non-private modes of operation. However, perfect secrecy is deÔ¨Åned with
respect the system providing the decisions and does not encompass all external user actions Shannon [1949].
7.5
Extended Analysis
User vs. Non-User Level Task Demonstrations
In Section 4.1, we observe that the ‚ÄúUser Privacy‚Äù prompts provide
larger performance gains compared to the ‚ÄúNo User Privacy‚Äù prompts. We hypothesize that (1) label distributions
user level task demonstrations help because user labels are generally in a restricted subset of classes from the overall
task, and (2) input distributions user level task demonstrations help due to linguistic or topical similarity between
user demonstrations.
Towards the former, on a per-user basis, as the entropy of the training data labels decreases, the accuracy tends to
increase, as in Figure 5. The entropy is computed for users with at least 4 training examples and each point represents at
least 200 examples. As the entropy of the user-level train set increases, the improvement of the ‚ÄúUser Privacy‚Äù baseline
over the ‚ÄúNo User Privacy‚Äù baseline tends to decrease, suggesting this is an important reason for the difference, though
there is still some gap between the baselines for high entropy points. Towards the latter, qualitatively certain users
clearly write with speciÔ¨Åc artifacts or about certain topics, though overall the examples are quite noisy. Three users
for whom this occurs are in Table 5.
Violin Plots
Figures 7 and 6 provide violin plots with respect to zero-shot performance across tasks. These plots
demonstrate that on tasks where user data are non-IID, the model performs unevenly in zero-shot use across different
groups. Further, on the Ô¨Åne-grained classiÔ¨Åcation tasks, performance is uneven across classes. For example, on F-
EMNIST, the model often either always selects the uppercase or lowercase version of a letter, resulting in close to
100% accuracy on the selected case and 0% on the unselected case. On 20News, accuracy by class is above 50%
for every class except for ‚Äúpolitics‚Äù, where the accuracy is 12.9% and ‚Äúreligion‚Äù, where the accuracy is 9.6%. We
suggest this is because ‚Äúpolitics guns‚Äù and ‚Äúpolitics in the middle east‚Äù related to politics, and ‚ÄúChristianity‚Äù and
‚Äúathiesm‚Äù related to religion are other classes in the 20News task ‚Äî using the inference-only strategies based on
natural language descriptions of these classes, it is difÔ¨Åcult to express the distinction between these similar classes.
We require additional investigation into the zero-shot robustness of FMs, and effective ways of describing classes that
avoid class embeddings that point in the same or similar directions in embedding space.
18

2
4
6
8
Number of User Training Examples
0.2
0.3
0.4
0.5
Accuracy
Acc. vs. Size of Train Set (GPT-2.7B)
User Privacy
No User Privacy
0.0
0.2
0.4
0.6
0.8
1.0
Entropy of User Training Examples
0.35
0.40
0.45
0.50
0.55
Accuracy
Acc. vs. Entropy of Train Set (GPT-2.7B)
User Privacy
No User Privacy
2
4
6
8
Number of User Training Examples
0.1
0.2
0.3
0.4
0.5
Accuracy
Acc. vs. Size of Train Set (GPT-125M)
User Privacy
No User Privacy
0.0
0.2
0.4
0.6
0.8
1.0
Entropy of User Training Examples
0.35
0.40
0.45
0.50
0.55
0.60
Accuracy
Acc. vs. Entropy of Train Set (GPT-125M)
User Privacy
No User Privacy
Figure 5: Accuracy vs. user-level training data properties.
Figure 6: Violin plots by class for the Ô¨Åne-grained classiÔ¨Åcation tasks. The plots are multi-modal suggesting the FMs give unequal
zero-shot performance across classes.
19

Figure 7: Violin plots by user id. We note that the Sent140, MRQA, CelebA, and Reddit are Non-IID and F-EMNIST and 20News
use IID mappings of data to clients. Non-IID plots appear more multi-modal.
20

User
Task Demonstrations
User 1
(Sent140,
Syntax)
Example 1 @MYIDOLTOWN Oh my gosh!!!!!!!!!!!!!! I‚Äùm speechless where
did you see/hear about this story?
Example 2 Just updated my MySpace proÔ¨Åle am about to send Sweet Danny a
comment on his MySpace page going to sleep shortly zzzzzzz!!!!!
Example 3 Had fun; happy day going to sleep for Church; Sunday School in
the morning ZZZZZZZZZZZZZZZZZZZZZ!!!!!!!!!!!!!!!!!
Example
4
Am
doing
wash√£nd
as
alwaysÀúlistening
to
Danny!!!!!!!!!!!!!!!!!!!!!!!!!!
Example 5 @dannygokey Happy to hear you‚Äôre working on Sophia‚Äôs Heart I‚Äôll
always support you; Sophia‚Äôs Heart!!!!!!!!!!!!!
Test Input @Cupcake1012 I <3 the Gokey Gang; Danny always; for-
ever Danny has the best fans ever!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
User 2
(Sent140,
Artifacts)
Example 1 ... A kidney stone. Really?? Ugh http://tinyurl.com/qsw9vq
Example 2 Ahhh everything hurts... warming up my bed buddy and going back
to sleep. http://tinyurl.com/ol4ugp
Example 3 Feelin so sick its dumb. http://tinyurl.com/o6glqr
Example 4 I got so much done today!! Got my car Ô¨Åxed, went to the bank.
Now im at work to make monies.http://tinyurl.com/of2ane
Example 5 name the movie...
‚Äúi wanna be like yooo-oo-ooo....‚Äù;
http://tinyurl.com/pqbyst
Test Input i wish i was a snail http://tinyurl.com/pgysd5
User 3
(Reddit,
‚ÄúJustice‚Äù
topic)
Example 1 dion lewis stiff arm plesse my favorite professor in law school ac-
tually argued at scotus
Example 2 scotus for the petitioner in mccleskey i hope for his
Example 3 his sake ( and for the sake of justice ) that
Example 4 that they revisit it at some point
Test Input ... am a personal injury attorney <PAD>
Table 5: Examples reÔ¨Çecting consistent linguistic patterns, artifacts, and topics across a user‚Äôs examples.
.
7.6
Additional Discussion of Baselines
Given the heterogeneity of FL evaluations He et al. [2020], we aim to compare vanilla baselines for both paradigms in
this Ô¨Årst investigation. For baselines within the ICL and FL frameworks, we consider advanced neural architectures
with standard training and inference methods. For ICL, this involves off-the-shelf, publicly accessible models with
manual prompting, without any tuning. For FL, we include numbers that use the most advanced architecture reported
for the benchmark in prior work (to the best of our knowledge), using standard FedAvg for training and none of the
following factors which signiÔ¨Åcantly degrade quality.
‚Ä¢ Differential privacy Differential privacy (DP) seeks to provide a guarantee that we cannot reconstruct or
memorize the sensitive examples appearing in our training data. The basic approach to achieve this involves
adding noise to the data during preprocessing, gradients during training, or elsewhere in the ML pipeline,
where the amount of noise depends on the desired privacy parameters. DP faces a tradeoff between stronger
privacy protection (achieved by adding more noise) and better convergence performance Wei et al. [2019].
Our FL baselines applied no DP.
‚Ä¢ Adversarial users Adversaries can execute a data poisoning attack (i.e. poison the labels of the contributed
data), send random updates to the central server, and/or execute a model replacement attack (i.e. manipulate
the shared model to enforce that it performs a desired subtask, while maintaining performance on the original
FL task) Bagdasaryan et al. [2019], Xu and Lyu [2020]. As the fraction of adversarial participants increases,
the quality of the FL model tends to degrade Li et al. [2022a].
21

‚Ä¢ Adversarial central server Because the central server is a single point of failure and attacks upon the central
server signiÔ¨Åcantly compromise privacy, decentralized federated learning has emerged as an alternative line
of work. However, this again degrades quality Sun et al. [2021].
‚Ä¢ Communication efÔ¨Åciency Popular methods to reduce the communication cost of FL such as compressing
the gradients passed between client and server tend to degrade quality Rothchild et al. [2020].
‚Ä¢ Non-IID Data Note that some of our benchmarks are already non-IID. Under SGD, we require an unbiased
estimate of the full gradient. This relies on computing the stochastic gradients over IID samples. User data is
likely non-IID, which can degrade the quality and increase the convergence time of the FL model trained with
FedAvg Zhao et al. [2018]. However, fortunately recent architectures appear more robust to heterogeneous
data under FL Qu et al. [2022].
A natural follow up is to compare implementations under ICL to FL under these threats.
22

