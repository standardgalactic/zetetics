©          Nature Publishing Group
1971
526 
NATURE VOL. 234 DECEMBER 31 1971 
A New Approach to Pattern Recognition 
N. JARDINE 
King's College, Cambridge 
Automatic clustering methods are shown 
to provide a basis for adaptive pattern 
recognition when neither the number 
nor the specifications of classes are 
known in advance. 
SOME terminology is needed to begin with. 
Descriptions of 
objects such as "red" and "2 cm long" are called attribute 
states and sets of attribute states such as "colour" and "length" 
are called attributes. 
The term classification is used for 
formation of classes of objects although many articles on 
pattern recognition use the term for assignment of objects to 
existing classes. 
Present approaches to pattern recognition share the assump-
tion that each of a set of classes to which new objects are to 
be assigned can be wholly or partially specified. 
In one 
approach it is assumed that each class is defined in terms_ of 
some list of attributes or relations. If, for example, a matchmg 
coefficient based on some set of attributes is defined on pairs 
of objects, each class may be defined as a neighbourhood of 
some typical object or template. 
A new object is assigned 
to a class if it matches the template closely enough. In the 
various structural methods of pattern recognition, images of 
objects are described by rules for assembling them from such 
components as line segments, curve segments, and corners (see, 
for example, refs. 1-3). Class definitiof1:S are given in terms of 
these structural descriptions. 
Similarly the methods for 
identifying parts of objects, devised by Jardine and Jardine4 • 5 , 
depend on the definition of classes of parts of different _objects 
in terms of a set of relations that hold between parts m each 
object. 
. . 
. 
The classical statistical model of pattern recogmt1on 1s 
applicable when classes cannot be defined in terms of some 
list of attributes relations. 
It is assumed that each of a 
set of classes ro 1 
• • • ron = Q has an a priori probability of 
occurrence P(ro1) and is specified by a conditional probability 
density function p(Xsro ), i= I, ... , n, over attribute states 
x 1 , .•• , xm = X. If the costs of misidentification are equal, and 
an object described by Xis observed, the quantities 
P (rodX) = 
n 
!: p(Xlc.o1)P(<.01) 
j=I 
are computed and the object is assigned to the class ro, for 
which P(c.oi\X) is greatest. This is the maximum likelihood 
assignment. Equivalently, the discriminant functions 
may be computed and the class selected for which the highest 
value is achieved. When costs of misidentification are unequal, 
Bayesian or minimax assignment rules may be used. 
In practice the probabilities of occurrence and the condi-
tional density functions which specify the classes may not be 
fully known in advance and various strategies are used to 
learn the class specifications. In supervised learning, samples 
of objects correctly assigned to each class are provided. In 
unsupervised learning, it is generally assumed only that the 
objects presented are drawn at random from the classes ro , . .. , 
c.o. with probabilities P(ro1) respectively. In both supervised and 
unsupervised learning, choice of a strategy depends on what 
is initially known about the classes. Thus if the form of the 
conditional density functions is known but their parameters 
are unknown, these may be estimated. Or if the discriminant 
functions are assumed to be linear, their coefficients may be 
learned. 
Statistical pattern recognition methods are reviewed 
in refs. 6-8. 
The approach to pattern recognition which I suggest is 
nearest in spirit to statistical pattern recognition with unsuper-
vised learning but it differs from all forms of statistical pattern 
recognition in two ways. First, no initial assumption is made 
about the number of classes and their specifications; second, 
it is not assumed that the objects encountered are drawn at 
random from some mixture of classes. These assumptions are 
inappropriate in many practical applications of pattern recog-
nition. They may also be inappropriate in models for pattern 
recognition by higher animals. 
The approach is based on the concept of seeking among 
objects systems of clusters in which members of each cluster 
are relatively isolated from members of other clusters. Methods 
for forming systems of clusters may be regarded as formal-
izations of the notion that formation of empirical class con-
cepts depends on recognition of the overall resemblances of 
objects. The relevance of cluster methods to pattern recog-
nition has been suggested by Sebestyen 9 , Ball 1 ° and Duda 8 . 
Cluster methods have been widely applied to problems of 
classification in biology, psychology, sociology and so on11 • 12 
-the usual starting point is a dissimilarity (or similarity) 
matrix on the entire set of objects to be classified, which is 
based on the states of each of some set of attributes shown by 
each of the objects. The final product is a clustering of the 
objects which represents as accurately as possible their relative 
dissimilarities. In applying cluster methods in pattern recog-
nition, objects are considered sequentially. The dissimilarities 
of each new object to the objects already clustered are used to 
place the new object in the system of clusters and, if necessary, 
to modify the system of clusters. 
In other words, cluster 
methods are carried out in such a way that cluster formation, 
diagnosis of new objects and modification of the system of 
clusters in the light of new information are performed by a 
single algorithm. 
This approach differs from statistical 
pattern recognition in that it is deterministic: each new object 

©          Nature Publishing Group
1971
NATURE VOL. 234 DECEMBER 31 1971 
is definitely assigned to or excluded from each of the clusters 
already formed. 
It differs from the deterministic template 
matching and structural methods because it does not involve 
fixed definitions of each class in terms of some list of attributes 
or relations. 
Systems of Clusters 
We define a pairwise dissimilarity matrix d on a set of 
objects P as a real-valued function on pairs of elements of P, 
such that 
d(a,b)"2.0 
d(a,b)=d(b,a) 
d(a,a)=O 
for all a,b in P 
for all a,b in P 
for all a in P 
In order to describe the systems of clusters to be considered, 
it is convenient to represent a dissimilarity matrix by a sequence 
of graphs, one for each of its values. The graph for value 8 
has vertices representing objects and edges joining pairs of 
Fig. 1 A, Single-link clusters obtained when five 
untidily handwritten capital As and five Bs are clus-
tered; B, single-link clusters obtained when five 
more As and five more Bs are considered. Vertices 
represent specimen letters and edges join pairs of 
vertices which represent specimens with dissimilarity 
8 or less, where 8 is the cluster level. 
A 
B 
vertices which represent objects which differ by 8 or Jess. The 
single-link clusters specified by a dissimilarity matrix are the 
connected components of the graphs for each value of the 
dissimilarity coefficient (Fig. 1). The set of single-link clusters 
determined by a dissimilarity matrix on a set of objects consti-
tutes a hierarchic stratified clustering of the objects. The level 
of a cluster is the least value of the dissimilarity matrix at 
which it appears as a connected component in the representa-
tive graph. Clusters at each level are disjoint and are nested 
in clusters at higher levels. 
The single-link cluster criterion has obvious intuitive appeal. 
Suppose, for example, that the objects considered are hand-
written Roman capital As and Bs. It is plausible that from 
any one capital A it is possible to reach any other capital A 
through a chain of intermediate symbols each of which differs 
only to some small degree from the next, and each of which 
could definitely be identified as a capital A; it is also plausible 
that we cannot thus transform a capital A into a capital B 
without passing through a sequence of symbols each of which 
could not definitely be identified either as a capital A or as a 
capital B. 
If this speculation is correct, then the class of 
527 
symbols of As and the class of symbols of Bs are single-link 
clusters specified by a dissimilarity matrix on the symbols of 
As and Bs. A more concrete example is taken from the analysis 
of pictures digitized by grey levels. If dissimilarity between 
cells in a picture is defined as a function of number of inter-
vening cells and difference of grey level, then single-link 
clusters at some threshold of the dissimilarity matrix may 
represent images of objects or parts of objects in a picture 
(compare ref. 13). 
The appropriateness of single-link clustering in pattern 
recognition rests, however, on the properties of the single-link 
cluster method as a method of data simplification, rather than 
on the intuitive appeal of single-link clusters. 
Jardine and 
Sibson12 •14 showed that the single-link method is the only 
hierarchic cluster method which satisfies certain invariance 
and optimality conditions. It is also stable in the sense that 
small changes in the values of a dissimilarity matrix can lead 
only to small changes in the resultant clustering, and it involves 
only order operations on the values of a dissimilarity matrix. 
----
......... 
A 
---- - --
.......... 
B '\ 
' ' 
.\ 
\ 
• I 
The single-link method may be inappropriate when objects 
intermediate between classes occur, because this can lead to 
amalgamation of otherwise well marked clusters; this can be 
avoided by generalizing the method to allow overlap between 
clusters. The k-link cluster methods 14 , which satisfy the same 
invariance and optimality conditions as the single-link method, 
allow clusters to overlap by, at most, k-1 objects. To see how 
the methods can be used in pattern recognition it is necessary 
to consider the way in which the successive clusterings obtained 
by a k-link cluster method fit together as the set of objects 
clustered is extended. 
Sequential Diagnosis 
A diagnostic rule is said to be consistent with a method of 
classification if sequential application of the rule to all the 
members of a set of objects, in whatever order, yields the same 
system of classes as application of the method of classification 
to the whole set of objects. 
The diagnostic rule consistent with single-link clustering is 
as follows. 
A new object is assigned to a cluster at level 8 

©          Nature Publishing Group
1971
528 
if it differs by 0 or less from at least one of its members. If a 
new object is assigned to more than one cluster at level 0 then 
the clusters are amalgamated. If a new object differs by more 
than 0 from all objects already clustered, it constitutes a new 
cluster at level 8. The diagnostic rule consistent with single-
link is related to a rule, widely used in pattern recognition, 
which assigns each new object to the class of its nearest 
neighbour15• 
Use of the nearest-neighbour assignment rule 
presupposes, however, that a fixed set of classes is determined 
initially. The generalization of the single-link diagnostic rule 
to a rule consistent with k-link clustering is more complex 
(see ref. 14). It suffices to note that all the k-link cluster methods 
have an important stability property. 
Addition of a new 
object cannot alter the extension of a cluster at level 8 if it 
is not linked directly, or indirectly through a chain of inter-
mediate objects at level 8, to any member of the cluster. 
The various average-link cluster methods which have been 
proposed as heuristics in pattern recognition do not have this 
property, so addition of a new object may alter the entire 
system of clusters. This property makes feasible algorithms 
which carry out k-link clustering by sequential diagnosis and 
hence perform simultaneously the operations of cluster forma-
tion, diagnosis and updating of a system of clusters in the 
light of successive diagnoses. 
k-Link clustering by sequential diagnosis involves only 
order operations on the values of a dissimilarity matrix. In 
this it differs from most cluster methods and pattern recog-
nition methods which involve such essentially algebraic opera-
tions as addition and multiplication. 
Pattern recognition 
methods which involve only order operations and which are 
invariant under order of presentation of objects may be of 
interest as models for pattern recognition by higher animals. 
Such methods could be carried out by simple threshold devices 
and could exploit redundancy in sensory input arising from 
repetition of kinds and arrangements of matter in the environ-
ment (compare ref. 16). 
The construction of efficient computer algorithms to carry 
out k-link clustering by sequential diagnosis rests on the 
observation that once an object is assigned to a cluster C at 
level 8, its assignment to any cluster which includes C is 
guaranteed. It follows that usually only a small subset of the 
dissimilarity values between a new object and objects already 
clustered need be used to determine the position of the new 
object in the clustering. Van Rijsbergen17 has described an 
algorithm which exploits this redundancy in carrying out 
single-link clustering by sequential diagnosis of objects or 
batches of objects. 
The algorithm can handle as many as 
- 1,000 objects. 
Applications 
I shall consider first an application in which the classes are 
known in advance. 
The objects are untidily handwritten 
Roman capital letters. 
Each is placed on a grid with its 
centroid at the centre of a system of concentric equally spaced 
rings, scaled so that the point furthest from the centroid lies 
on the outer edge of the outermost ring. Each specimen is 
described by the proportions of the occupied squares of the 
grid which lie within each ring. (The description is invariant 
under magnification, rotation, and translation of a specimen.) 
The dissimilarity between two specimen letters is the sum of 
the modular differences between proportions in corresponding 
rings. Other measures of dissimilarity are described in refs. 11, 
12 and 18. Initially, when single-link clustering by sequential 
diagnosis is applied, the fit between the clusters obtained and 
the letter classes is poor but as the number of specimens is 
increased so the fit between clusters and letter classes improves. 
Fig. 1 A, for example, shows clusters obtained after five As 
and five Bs have been considered; Fig. 1 B shows clusters 
obtained after five more specimens of each have been considered. 
In applications in which the classes are known in advance, 
clustering by sequential diagnosis is continued until a good 
NATURE VOL. 234 DECEMBER 31 1971 
or perfect fit between clusters and classes is obtained, or until 
addition of further objects produces no further improvement 
in the fit. Once a good or perfect fit is achieved, the clustering 
may be "frozen" by ceasing to allow amalgamation of clusters 
and creation of new clusters. 
Diagnostic rules which are 
much faster than the single-link rule may then be used. New 
objects may, for example, be matched with a typical object 
from each cluster and assigned to the cluster whose typical 
object they match best (see ref. 17). 
The classes are not known in advance in such applications 
as the classification of documents to facilitate retrieval of 
documents relevant to requests. Each document is described 
by presence or absence of each of a list of keywords. Single-
link clustering is applied to a dissimilarity matrix calculated 
from these descriptions. In ref. 19 it was shown that retrieval 
of documents based on matching of requests with document 
clusters is as effective as the usual strategies, which are based 
on matching of each request with each document, and is much 
faster. 
When the classes are not known in advance it is more 
difficult to decide at what stage a clustering can be frozen. 
One approach is to define a measure of the extent to which a 
clustering is modified by diagnosis of a new object. 
As 
diagnosis of new objects proceeds, it generally happens that 
the average modification by each new object is initially high 
because new clusters are being created and existing clusters 
amalgamated. 
As the number of objects considered is 
increased, the average modification by each new object falls 
because creation of new clusters and amalgamation of existing 
clusters become less frequent. The clustering is frozen when 
the average modification falls below a chosen threshold. 
With a collection of 200 aeronautics documents, for example, 
it was found that the clustering ceased to be modified substan-
tially after about fifty documents had been considered19• 
I conclude that clustering by sequential diagnosis is a useful 
approach to pattern recognition, especially when no initial 
assumptions can be made about the number and specifications 
of classes. 
I thank Mr C. J. van Rijsbergen and Professor H. C. Longuet-
Higgins for useful comments. 
The work was carried out 
during the tenure of a Royal Society scientific information 
research fellowship and an SRC research grant. 
Received April 20; revised July 15, 1971. 
1 Narasimhan, R., Comm. ACM, 9, 166 (1966). 
2 Kirsch, R. A., IEEE Trans. Elect. Comput., 13, 363 (1964). 
3 Clowes, M. B., in Machine Intelligence (edit. by Melzer, B., Michie, 
D., and Swann, M.), 4, 361 (Edinburgh University Press, 1969). 
4 Jardine, N., and Jardine, C. J., Nature, 216, 301 {1967). 
5 Jardine, C. J., and Jardine, N., Studia Logica, 27, 213 (1971). 
6 Methodologies of Pattern Recognition (edit. by Watanabe, S.) 
(Academic Press, New York and London, 1969). 
7 Kovalevsky, V. A., in Advances in Information Systems Science, 
3, 1 (Plenum Press, New York, 1970). 
8 Duda, R. 0., in Adaptive, Learning, and Pattern Recognitio_n 
Systems (edit. by Mendel, J. M., and Fu, K. S.), 3 (Academic 
Press, New York and London, 1970). 
9 Sebestyen, G. S., Decision-making Processes in Pattern Recogni-
tion (Macmillan, New York, 1962). 
10 Ball, G. H., Proc. Fall Jt Comput. Conf, 27, 533 (1966). 
11 Sokal, R. R., and Sneath, P. H. A., Principles of Numerical 
Taxonomy (Freeman, San Francisco and London, 1963). 
12 Jardine, N., and Sibson, R., Mathematical Taxonomy (Wiley, 
London and New York, 1971). 
13 Rosenfeld, A., Picture Processing by Computer (Academic Press, 
New York and London, 1969). 
14 Jardine, N., and Sibson, R., Math. Biosci., 2,465 (1968). 
1s Cover, T. M., and Hart, P. E., IEEE Trans. Inf Theory, 13, 21 
(1967). 
1 6 Marr, D., Proc. Roy. Soc. Land., B, 176, 161 (1970). 
17 Van Rijsbergen, C. J., Comput. J. (in the press). 
18 Cormack, R. M., J. Roy. Stat. Soc., B (in the press). 
19 Jardine, N., and van Rijsbergen, C. J., Inf Stor. Retr., 7, 217 
(1971). 

