Qwen-Audio: Advancing Universal Audio Understanding
via Unified Large-Scale Audio-Language Models
Yunfei Chu∗
Jin Xu∗
Xiaohuan Zhou∗
Qian Yang
Shiliang Zhang
Zhijie Yan
Chang Zhou†
Jingren Zhou
Alibaba Group
Code & Demo & Models:
https://github.com/QwenLM/Qwen-Audio
Abstract
Recently, instruction-following audio-language models have received broad attention
for audio interaction with humans. However, the absence of pre-trained audio models
capable of handling diverse audio types and tasks has hindered progress in this field.
Consequently, most existing works have only been able to support a limited range of inter-
action capabilities. In this paper, we develop the Qwen-Audio model and address this
limitation by scaling up audio-language pre-training to cover over 30 tasks and various
audio types, such as human speech, natural sounds, music, and songs, to facilitate uni-
versal audio understanding abilities. However, directly co-training all tasks and datasets
can lead to interference issues, as the textual labels associated with different datasets
exhibit considerable variations due to differences in task focus, language, granularity of
annotation, and text structure. To overcome the one-to-many interference, we carefully
design a multi-task training framework by conditioning on a sequence of hierarchical tags
to the decoder for encouraging knowledge sharing and avoiding interference through
shared and specified tags respectively. Remarkably, Qwen-Audio achieves impressive per-
formance across diverse benchmark tasks without requiring any task-specific fine-tuning,
surpassing its counterparts. Building upon the capabilities of Qwen-Audio, we further
develop Qwen-Audio-Chat, which allows for input from various audios and text inputs,
enabling multi-turn dialogues and supporting various audio-central scenarios.
1
Introduction
Large language models (LLMs) (Brown et al., 2020; OpenAI, 2022, 2023; Chowdhery et al., 2022; Anil et al.,
2023; Touvron et al., 2023a,c; Qwen, 2023) have greatly propelled advancements in the field of general
artificial intelligence (AGI) due to their strong knowledge retention, complex reasoning and problem-solving
capabilities. However, language models lack the capability to perceive non-textual modalities like images and
audio in the same manner as humans do. Speech, as an important modality, provides diverse and complex
signals beyond texts such as emotions, tones, and intentions in human voice, train whistle, clock chime and
thunder in natural sounds, and melody in music. Enabling LLMs to perceive and comprehend rich audio
signals for audio interaction has received broad attention (Huang et al., 2023; Shen et al., 2023; Wang et al.,
2023a; Lyu et al., 2023; Wu et al., 2023b; Gong et al., 2023b; Wang et al., 2023c; Shu et al., 2023).
Prior works for instruction following mainly inherit the capabilities from large (multimodal) LLMs and adopt
light-weight supervised fine-tuning to activate the abilities of the model to align with user intent (Ouyang
et al., 2022; Wang et al., 2023a; Gong et al., 2023b). However, most works have been constrained in terms
∗Equal contribution, †Corresponding author
1
arXiv:2311.07919v1  [eess.AS]  14 Nov 2023

Librispeech
Aishell1
Aishell2
CoVoST2
Clotho
CochlScene
TUT2017
Meld
ClothoAQA
VocalSound
NS.Qualities
NS.Instrucment
91.5
93.0
94.5
97.44
97.88
98.31
91.75
93.5
95.25
8.75
17.5
26.25
11.5
23.0
34.5
65.0
70.0
75.0
17.5
35.0
52.5
37.0
44.0
51.0
20.0
40.0
60.0
46.25
62.5
78.75
12.5
25.0
37.5
20.0
40.0
60.0
N/A
Previous Top-tiers
Qwen-Audio
Figure 1: Performance of Qwen-Audio and previous top-tiers from multi-task audio-text learning mod-
els such as SpeechT5 (Ao et al., 2021), SpeechNet (Chen et al., 2021), SpeechLLaMA (Wu et al., 2023a),
SALMONN (Anonymous, 2023) and Pengi (Deshmukh et al., 2023). We demonstrate the test set results
across the 12 datasets covering Automatic Speech Recognition (ASR), Speech-to-Text Translation (S2TT), Au-
tomatic Audio Captioning (AAC), Acoustic Scene Classification (ASC), Speech Emotion Recognition (SER),
Audio Question and Answering (AQA), Vocal Sound Classification (VSC), and Music Note Analysis (MNA).
The results of ASR datasets, such as Librispeech, Aishell1 and Aishell2 refer to 1 - WER%. The results of
CoVoST2 is the average BLUE score of seven translation directions (en-de, de-en, en-zh, zh-en, es-en, fr-en
and it-en). Qwen-Audio achieves remarkable performance without requiring any task-specific fine-tuning,
surpassing its counterparts.
of audio interaction capabilities due to the lack of pre-trained audio-language models that can handle
diverse audio types and tasks. Existing representative audio-language multi-tasks language models, such as
SpeechNet (Chen et al., 2021), SpeechT5 (Ao et al., 2021), VIOLA (Wang et al., 2023d), Whisper (Radford
et al., 2023) and Pengi (Deshmukh et al., 2023) are limited to processing specific audio types, such as human
speech or natural sounds.
To promote the growth and development of the audio-text multimodal community, we introduce Qwen-
Audio, a large-scale audio-language model. Qwen-Audio is a multi-task language model conditioning on
audio and text inputs, that extends the Qwen-7B (Qwen, 2023) language model to effectively perceive audio
signals by the connection of a single audio encoder. Different from previous works that primarily cater to a
single type of audio such as human speech, or focus on specific tasks like speech recognition and captioning,
or limit models to a single language (Wang et al., 2023a; Lyu et al., 2023; Wu et al., 2023b; Gong et al., 2023b;
Shu et al., 2023), we scale up the training to dozens of datasets covering over 30 tasks, eight languages and
various types of audio for advancing universal audio understanding abilities. A significant challenge of multi-
2

(a) Multilingual ASR and Translation
(f) Speech Editing with SWRT
(d) Audio-Motivated Creative Writing
(e) Music Appreciation
(c) Sound Understanding and Reasoning
(b) Multiple Audio Analysis
OK. The sentence “⼤家好我们来讨论⼀下公司周末团建的地点吧⼤家
有什么意⻅呢” translated into English is “Hello, let’s 
discuss the location for our company’s weekend team 
building. What are your opinions?”
recognize the speech, and translate it into English
recognize the speech, and translate it into English
OK. The sentence “I think we can go to the countryside 
and find a beautiful natural area.” translated into 
English is “I think we can go to the countryside and 
find a beautiful natural area.
recognize the speech, and translate it into English
OK. The sentence “わかりました。⽥舎に⾏くことに同意します。⽥
舎暮らしを体験してみるのもいいかもしれません。” translated 
into English is “I understand. I agree to go to the 
countryside. It’s also good to experience country 
life.”
What are the emotions in audio 1 
and audio 2, respectively?
The tone of the woman in audio 1 is 
cheerful, possibly talking about 
something pleasant. The tone of the 
woman in audio 2 is fearful, 
possibly discussing something 
terrifying.
How to respond to the questions 
in audio 1 and audio 2?
In audio 1, when the woman asks if 
you're okay, you can respond with, 
"Thank you for your concern, I'm fine 
now, just felt a bit uncomfortable 
all of a sudden."
In audio 2, considering the woman's 
fearful tone and discussing something 
terrifying, you can respond to her 
with either, "I'm scared, I don't 
know what to do" or "I'm feeling 
really upset, I don't know how to 
face this situation."
Recognize the sound information and assume 
that the sound is happening around the 
user. Please provide the user with some 
handling suggestions.
The sound is of breaking glass. Here are 
some suggestions for handling the situation:
1. Turn off the source of the sound: If the 
sound is coming from a broken glass item, 
turn off the source of the sound as soon as 
possible to prevent further damage.
2. Clean up the broken glass: Use gloves and 
a broom or vacuum cleaner to clean up the 
broken glass. Be careful not to cut 
yourself.
3. Assess the damage: If the broken glass is 
in a location that is not easily accessible 
or if it poses a safety hazard, contact a 
professional for assistance.
4. Cover the area: Cover the area where the 
broken glass is located to prevent injury or 
further damage.
5. Avoid walking on broken glass: If the 
broken glass is on a floor, do not walk on 
it. Instead, use a broom or vacuum cleaner 
to clean it up or cover it with a piece of 
cardboard or tape.  
what is it
Describe this music in detail
This is a solo piano piece. It is fast 
tempo with a playful and humorous mood. 
The piece is in the key of C major and 
is in 4/4 time signature. The melody is 
played on the right hand with the left 
hand playing chords. The piece is a 
classic ragtime style.
A solo piano piece.
Give me some recommendations for similar music.
some similar music to “Solo Piano Ragtime” could 
be:
1. “The Entertainer” by Scott Joplin
2. “Maple Leaf Rag” by Scott Joplin
3. “Ave Maria” by Franz Schubert
4. “Für Elise” by Ludwig van Beethoven
5. “The Blue Danube” by Johann Strauss II
Write a poem about the 
above audio
 Birds are singing in the trees, 
 A gentle breeze is blowing free,
 The rustling leaves are music, 
 As nature’s symphony goes on. 
 The sun is shining bright and warm,   
 A peaceful atmosphere is sown, 
 In the midst of nature’s beauty, 
 We can find serenity and duty.  
 The birds take flight with ease, 
 Their chirping voices fill the trees, 
 A symphony of nature’s grace, 
 A moment to pause and embrace.
 The wind whispers through the branches, 
 As the leaves dance in the branches, 
 A peaceful moment in time, 
 A chance to let go and unwind.  
 So let us take a moment here, 
 To appreciate the beauty so dear, 
 In the midst of nature’s wonder, 
 We can find a sense of wonder.
change “joyful” to “interesting”
ok, here is the edited audio
Edit out the word "what", so I can 
turn it into a meme
The word “what” starts at 3.04 
seconds and ends at 3.23 seconds.
change the person name to “John”
ok, here is the edited audio
Figure 2: Examples of Qwen-Audio showcasing its proficiency in perceiving and comprehending various
types of audio. Qwen-Audio supports multiple-audio analysis, sound understanding and reasoning, music
appreciation, and tool usage for speech editing. Demos are available at https://qwen-audio.github.io/
Qwen-Audio/.
task and multi-dataset co-training arises from the considerable variation in textual labels associated with
different datasets. This variation stems from differences in task objectives, languages, annotation granularity,
and text structure (structured or unstructured). To address this one-to-many challenge, we have carefully
designed a multi-task training framework that conditions the decoder on a sequence of hierarchical tags. This
design encourages knowledge sharing and helps mitigate interference through shared and specified tags,
respectively. Furthermore, we incorporate the speech recognition with the word-level time-stamp prediction
(SRWT) task for training, which is usually ignored in previous multi-task learning research. We find the task
not only improves the grounding and grounding-based QA tasks beyond speech signals such as sound and
music, but also improves the performance of ASR. As shown in Figure 1, extensive evaluation demonstrates
that Qwen-Audio, without any task-specific fine-tuning, outperforms previous multi-task training models
across a diverse range of tasks. A notable achievement of Qwen-Audio is its state-of-the-art performance on
the test set of Aishell1, cochlscene, ClothoAQA, and VocalSound. Leveraging the capabilities of Qwen-Audio,
we introduce Qwen-Audio-Chat via supervised instruction fine-tuning, which facilitates flexible input from
both audio and text modalities in multi-turn dialogues, enabling effective interaction following human
instructions. The contribution of the paper is summarized below:
• We introduce Qwen-Audio, a fundamental multi-task audio-language model that supports various
3

tasks, languages, and audio types, serving as a universal audio understanding model. Building upon
Qwen-Audio, we develop Qwen-Audio-Chat through instruction fine-tuning, enabling multi-turn
dialogues and supporting diverse audio-oriented scenarios. Both Qwen-Audio and Qwen-Audio-
Chat models are open-source, promoting the growth and development of the audio-text multimodal
community.
• To scale up audio-language pre-training, we address the challenge of variation in textual labels associ-
ated with different datasets by proposing a multi-task training framework, enabling knowledge sharing
and avoiding one-to-many interference. Our model incorporates more than 30 tasks and extensive
experiments show the model achieves strong performance.
• To promote audio-language pre-training, we demonstrate that incorporating the SRWT task, which is
often overlooked in the audio multimodal research community, improves grounding and grounding-
based question answering tasks beyond speech signals, as well as ASR performance.
• Experimental results show that Qwen-Audio achieves impressive performance across diverse bench-
mark tasks without requiring any task-specific fine-tuning, surpassing its counterparts. Specifically,
Qwen-Audio achieves state-of-the-art results on the test set of Aishell1, cochlscene, ClothoAQA, and
VocalSound.
2
Related Work
Multi-task Audio-Text Learning
The goal of multi-task training is to transfer knowledge between different
tasks with unified model architectures and data format (Raffel et al., 2020; Ao et al., 2021; Chen et al., 2021).
In audio processing domains, it is challenging to unify all audio processing tasks since there are various
audio signals, such as human speech, natural sounds, music, and songs, and their labeling format differs a
lot. SpeechNet (Chen et al., 2021) and SpeechT5 (Ao et al., 2021) treat human speech tasks into a speech/text
input and speech/text output format, and leverage a shared encoder-decoder framework for pretraining.
Many works (Wang et al., 2023d; Maiti et al., 2023; Rubenstein et al.; Wang et al., 2023e; Nachmani et al.,
2023) unify data format and tasks by directly feeding speech representation (Nachmani et al., 2023) or
encoding continuous speech signals as discrete codes (Défossez et al., 2022; Zeghidour et al., 2022; Zhang
et al., 2023c), and treating different human speech tasks as conditional generative tasks. For training, they
directly adopt a decoder-only Transformer model (Vaswani et al., 2017). VoiceBox (Le et al., 2023) employs a
non-autoregressive continuous normalizing flow model for human speech synthesis and speech editing tasks.
Whisper (Radford et al., 2023) proposes a template for multi-task training, considering the granularity of
dataset annotations (with or without sentence-level timestamps) and task types (human speech recognition
and translation) for unified training. Previous works mostly focus only on human speech processing tasks
such as speech recognition and translation and ignore other types of audio such as natural sounds and music.
Pengi (Deshmukh et al., 2023) focuses on natural sound understanding tasks and treats these tasks as text
generation tasks. Specifically, Pengi unifies data format with text templates and then trains all tasks within
a Transformer decoder model. In this work, Qwen-Audio integrates diverse audio types, such as human
speech, natural sounds, music, and songs, and facilitates co-training on datasets sourced from heterogeneous
data and featured disparate labeling granularities. This is achieved through introducing a unified learning
framework. Upon completion of the co-training process, the model demonstrates comprehensive capabilities
in speech perception, comprehension, and recognition tasks, eliminating the need for additional task-specific
architectural extensions.
Interact with LLMs through Multiple Modality
Recently, large language models such as ChatGPT (Ope-
nAI, 2022) have demonstrated impressive capabilities for knowledge retention, reasoning, and coding
followed by human instructions. To extend to application scope of LLMs beyond pure text tasks, many LLM-
based multimodal models have been developed. For visual modality, GPT4 (OpenAI, 2023), Flamingo (Alayrac
et al., 2022), Kosmos (Peng et al., 2023), BLIP (Li et al., 2022), Shikra (Chen et al., 2023), Emu (Sun et al., 2023)
4

and Qwen-VL (Bai et al., 2023) have proposed different integration method to enable image understanding
or generation capabilities for LLMs.
For the audio modality, there have been attempts to utilize well-trained audio foundation models as tools,
such as AudioGPT (Huang et al., 2023) and HuggingGPT (Shen et al., 2023), while leveraging LLMs as a
versatile interface. These endeavors involve instructing LLMs to generate commands for controlling external
tools or transcribing human speech to text before inputting into the LLMs. However, these approaches lack
the inclusion of crucial information like prosody and sentiment in human speech, and in certain cases, they
fail to convert non-textual audio, such as natural sound. Consequently, the transfer of knowledge from LLMs
to the speech modality encounters obstacles, and the LLMs lack the necessary capabilities to perceive and
comprehend audio signals. Recent efforts explore training end-to-end audio-text LLMs for direct speech
interaction. SpeechGPT (Zhang et al., 2023a) first converts human speech into discrete HuBERT tokens (Hsu
et al., 2021), and then designs a three-stage training pipeline on paired speech data, speech instruction
data and chain-of-modality instruction data accordingly. BLSP (Wang et al., 2023a) aligns representation
by requiring the LLM to generate the same text continuation given the human speech and corresponding
transcripts. LLaSM (Shu et al., 2023) creates large speech instruction datasets by generating speech questions
using Microsoft TTS API, and then conducts training to enable end-to-end interaction between human speech
and text.
LTU (Gong et al., 2023b) creates a 5M audio QA dataset, and conducts supervised finetuning (SFT) on
the audio module and LoRA adapters (Hu et al., 2021) of LLaMA (Touvron et al., 2023b) to enhance the
alignment between sound perception and reasoning. SALMMON (Anonymous, 2023) utilizes both a text
encoder and a speech encoder to extract the representation from different kinds of audio and text input,
and then connects the inputs to a well-train LLM with Q-former (Li et al., 2023) style attention to generate
response. In this work, Qwen-Audio aims at training a unified audio-text multi-task multilingual LLMs
capable of perceiving and understanding audio inputs while preserving the textual conversational abilities.
Qwen-Audio employs a single encoder for all audios, and bridges the gap of audio and text modality by large-
scale end-to-end training to support various tasks such as natural sound detection, human speech recognition
and grounding, and audio captions tasks. The resulting model demonstrates superior performance than
previous works across a diverse style of tasks.
3
Methodology
This section provides details of Qwen-Audio and Qwen-Audio-Chat, which are designed for universal audio
understanding and flexible interaction based on human instructions. The model structure of Qwen-Audio and
Qwen-Audio-Chat is first presented in Section 3.1. The training process of our models consists of two stages:
multitask pretraining and supervised fine-tuning. We describe the training of Qwen-Audio via multitask
learning in Section 3.2. Then, we describe Qwen-Audio-Chat with supervised fine-tuning in Section 3.3 ,
which enables flexible human interaction.
3.1
Model Architecture
The architecture of Qwen-Audio models is depicted in Figure 3. Qwen-Audio contains an audio encoder and
a large language model. Given the paired data (a, x), where the a and x denote the audio sequences and
text sequences, the training objective is to maximize the next text token probability as
Pθ(xt|x<t, Encoderϕ(a)),
(1)
conditioning on audio representations and previous text sequences x<t, where θ and ϕ denote the trainable
parameters of the LLM and audio encoder respectively.
Audio Encoder
Qwen-Audio employs a single audio encoder to process various types of audio. The
initialization of the audio encoder is based on the Whisper-large-v2 model (Radford et al., 2023), which is
5

QwenLM
Next Token Prediction
0.32
SOT
…
ZH
TRANS
-CRIBE
ZH
TIMES-
TAMPS
WLT
My
0.15
cat
Audio Encoder
Input Training Data
                       Multi-language transcription
“What work are you doing now?”
<|0.00|>What<|0.23|>work<|0.43|>are<|0.45|><|SIL|><|0.51|>…
                     Any-to-any translation
“我记得我当时队友的反应，跟我差不多同时间登顶的队友”
I remember the reaction of my teammates, who reached …
                      Audio caption analysis
(Background birds chirping and music playing)
A guitar tune is played with birds chirping in the background.
                      Audio keyword analysis
(A man speaks Chinese)
Chinese, male speech
                       Audio question and answer
(Sound of a car)
Q:Are the sounds outside? A: Yes. 
START OF
TRANSCRIPTION
START OF 
ANALYSIS
Audio Language Tag
Task Tag
CAPTION
KEYWORD
TRANSCRIBE
TRANSLATE
QUESTION
Text Language Tag
Output Instruction
UNKNOWN
ZH
EN
FR
DE
JA
IT
ES
KO
ZH
EN
FR
DE
JA
IT
ES
KO
WORD LEVEL TRANSCRIPTION
TRANSCRIPTION WITH ENTITY
question
…
Or
Timestamps Tag
Multi-Task Training Format Framework
Transcription Tag
TIMESTAMPS
NO TIMESTAMPS
EMOTION
SCENE
INVERSE TEXT NORMALIZATION
SPEAKER INFO
SONG INFO
Or
VOCAL
…
ZH
TRANS
-CRIBE
ZH
TIMES-
TAMPS
WLT
My
0.15
cat
0.00
0.00
Figure 3: The overview of Qwen-Audio architecture and multitask-pretraining.
a 32-layer Transformer model that includes two convolution down-sampling layers as a stem. The audio
encoder is composed of 640M parameters. Although Whisper is supervised trained for speech recognition
and translation, its encoded representation still contains rich information, such as background noise (Gong
et al., 2023a), and can even be used for recovering the original speech (Zhang et al., 2023b). To preprocess the
audio data, Whisper resamples it to a frequency of 16kHz and converts the raw waveform into 80-channel mel-
spectrogram using a window size of 25ms and a hop size of 10ms. Additionally, a pooling layer with a stride
of two is incorporated to reduce the length of the audio representation. As a result, each frame of the encoder
output approximately corresponds to a 40ms segment of the original audio signal. SpecAugment (Park et al.)
is applied at the training time as data augmentation.
Large Language Model
Qwen-Audio incorporates a large language model as its foundational component.
The model is initialized using pre-trained weights derived from Qwen-7B (Qwen, 2023). Qwen-7B is a
32-layer Transformer decoder model with a hidden size of 4096, encompassing a total of 7.7B parameters.
3.2
Multitask Pretraining
In the domain of audio processing, diverse audio datasets have been developed to address specific tasks,
as shown in Table 1. Qwen-Audio aims to perform co-training using a wide range of audio datasets. The
objective is to train a unified model capable of supporting all audio tasks, eliminating the need for laborious
model switching when dealing with different tasks. More importantly, during co-training, tasks can benefit
from each other since 1) similar tasks can benefit from knowledge sharing and collaborative learning, as
they share a common focus on the fundamental information embedded within the audio signal; 2) tasks that
rely on lower-level perceptual abilities can assist tasks that require higher-level understanding or reasoning
capabilities.
However, different datasets exhibit significant variations in textual labels due to differences in task focus,
6

Table 1: Multi-task pre-training dataset.
Types
Task
Description
Hours
Speech
ASR
Automatic speech recognition (multiple languages)
30k
S2TT
Speech-to-text translation
3.7k
OSR
Overlapped speech recognition
<1k
Dialect ASR
Automatic dialect speech recognition
2k
SRWT
English speech recognition with word-level timestamps
10k
Mandarin speech recognition with word-level timestamps
11k
DID
Dialect identification
2k
LID
Spoken language identification
11.7k
SGC
Speaker gender recognition (biologically)
4.8k
ER
Emotion recognition
<1k
SV
Speaker verification
1.2k
SD
Speaker diarization
<1k
SER
Speech entity recognition
<1k
KS
Keyword spotting
<1k
IC
Intent classification
<1k
SF
Slot filling
<1k
SAP
Speaker age prediction
4.8k
Sound
AAC
Automatic audio caption
8.4k
SEC
Sound event classification
5.4k
ASC
Acoustic scene classification
<1k
SED
Sound event detection with timestamps
<1k
AQA
Audio question answering
<1k
Music&Song
SID
Singer identification
<1k
SMER
Singer and music emotion recognition
<1k
MC
Music caption
25k
MIC
Music instruments classification
<1k
MNA
Music note analysis such as pitch, velocity
<1k
MGR
Music genre recognition
9.5k
MR
Music recognition
<1k
MQA
Music question answering
<1k
Others
VSC
Vocal sound classification
<1k
language, granularity of annotation, and text structure (e.g., some data is structured while others are
unstructured). To train a network for different tasks, simply mixing these diverse datasets cannot lead
to mutual enhancement; instead, it introduces interference. Most existing multi-task training approaches
have either grouped similar tasks (e.g., audio captioning, transcription) or assigned a dataset ID to each
dataset (Wang et al., 2023a; Lyu et al., 2023; Wu et al., 2023b; Gong et al., 2023b; Shu et al., 2023) to avoid
interference. Although these approaches have achieved certain effectiveness, there is still considerable
room for improvement. Whisper proposes a multitask training format by specifying tasks and condition
information as a sequence of input special tokens to the language decoder, such as voice activity detection,
language identification, and sentence-level timestamp tags. However, Whisper focuses on speech translation
and recognition tasks only.
Multi-task Training Format Framework
Motivated by Whisper (Radford et al., 2023), to incorporate
different kinds of audio, we propose a multitask training format framework as follows:
• Transcription Tag: The initiation of prediction is denoted using a transcription tag. The <|startof-
transcripts|> is employed to indicate the tasks involve accurately transcribing the spoken words and
7

capturing the linguistic content of a speech recording, such as speech recognition and speech translation
tasks. For other tasks, the <|startofanalysis|> tag is utilized.
• Audio Language Tag: Then, we incorporate a language tag that indicates the spoken language in
the audio. This tag uses a unique token assigned to each language present in our training set, eight
languages in totally. In the case where an audio segment does not contain any speech, such as natural
sounds and music, the model is trained to predict a <|unknown|> token.
• Task Tag: The subsequent tokens specify the task. We categorize the collected audio tasks into five
categories: <|transcribe|>, <|translate|>, <|caption|>, <|analysis|>, and <|question-answer|> tasks.
For question-answer (QA) tasks, we append the corresponding questions after the tag.
• Text Language Tag: The tag token specifies the language of output text sequences.
• Timestamps Tag: The presence of a <|timestamps|> or <|notimestamps|> token determines whether
the model needs to predict timestamps or not. Different from the sentence-level timestamps used in
Whisper, the inclusion of the <|timestamps|> tag requires the model to perform fine-grained word-level
timestamp prediction, abbreviated as SRWT (Speech Recognition with Word-level Timestamps). The
prediction of these timestamps is interleaved with the transcription words: the start time token is
predicted before each transcription token, while the end time token is predicted after. According to our
experiments, SRWT improves the ability of the model to align audio signals with timestamps. This
improved alignment contributes to a comprehensive understanding of speech signals by the model,
resulting in notable advancements across many tasks such as speech recognition and audio QA tasks.
• Output Instruction: Lastly, we provide output instruction to further specify the task and desired format
for different subtasks, and then the text output begins.
The guiding principle behind our framework is to maximize the sharing of knowledge among similar tasks
through shared tags, thereby improving their performance. Meanwhile, we ensure that different tasks and
output formats can be distinguished to avoid the one-to-many mapping problem for the model. Please see
Figure 3 for an overview of our multitask format of Qwen-Audio.
3.3
Supervised Fine-tuning
The extensive pretraining of multitask models has equipped them with a broad understanding of audio.
Building upon this, we employ instruction-based fine-tuning techniques to improve the ability of the model to
align with human intent, resulting in an interactive chat model, termed Qwen-Audio-Chat. To accomplish this,
we manually create demonstrations for each task. These demonstrations consist of raw text labels, questions,
and answers. We then utilize GPT-3.5 (OpenAI, 2022) to generate further questions and answers based on the
provided raw text labels. Additionally, we also create a dataset of audio-dialogue data by employing manual
annotation, model generation, and strategy concatenation. This dataset helps us incorporate reasoning, story
generation, and multi-image comprehension abilities into our model.
To handle multi-audio dialogue and multiple audio inputs effectively, we introduce the convention of labeling
different audios with "Audio id:", where id corresponds to the order of the audio input dialogue. In terms of
dialogue format, we construct our instruction tuning dataset using the ChatML (Openai) format. In this
format, each interaction’s statement is marked with two special tokens (<im_start> and <im_end>) to
facilitate dialogue termination.
8

Table 2: Summary of the evaluation benchmarks for Qwen-Audio.
Task
Description
Dataset
Split
Metric
ASR
Automatic Speech Recognition
Aishell1 (Bu et al., 2017)
dev | test
WER
Aishell2 (Du et al., 2018)
test
Librispeech (Panayotov et al., 2015)
dev | test
S2TT
Speech-to-text translation
CoVoST2 (Wang et al., 2020)
test
BLEU1(Papineni et al., 2002)
SRWT
Speech Recognition with Word-level Timestamp
Industrial Data (Gao et al., 2023)
test
AAS (Shi et al., 2023)
AAC
Automatic Audio Caption
Clotho (Drossos et al., 2020)
test
CIDEr | SPICE | SPIDEr
(Vedantam et al., 2015; Anderson et al., 2016)
ASC
Acoustic Scene Classfication
CochlScene (Jeong and Park, 2022)
test
ACC
TUT2017 (Mesaros et al., 2017)
eval
SER
Speech Emotion Recognition
Meld (Poria et al., 2019)
test
ACC
AQA
Audio Question & Answer
ClothoAQA (Lipping et al., 2022)
test
ACC
VSC
Vocal Sound Classification
VocalSound (Gong et al., 2022)
test
ACC
MNA
Music Note Analysis
NSynth (Engel et al., 2017)
test
ACC / MAP
The Data Format Example of Supervised Fine-Tuning.
<im_start>user
Audio 1: <audio>emov-db/141-168-0155.wav</audio>what does the speaker say?<im_end>
<im_start>assistant
The speaker says in English, "Won’t you draw up, gentlemen.".<im_end>
<im_start>user
What’s the mood of the person?<im_end>
<im_start>assistant
Based on the voice, the mood of the person is disgusted.<im_end>
In order to facilitate versatile input from both audio and pure text modalities within multi-turn dialogues, we
use a combination of audio-centric instruction data mentioned above and pure text instruction data during
this training process. This approach allows the model to handle diverse forms of input seamlessly. The total
amount of instruction tuning data is 20k.
4
Experiments
4.1
Setup
For multi-task pre-training, we freeze the weights of LLM and only optimize the audio encoder. This trained
model is referred to as Qwen-Audio. In the subsequent supervised fine-tuning stage, we fix the weights of
the audio encoder and only optimize the LLM. The resulting model is denoted as Qwen-Audio-Chat. The
detailed training configurations of both stages are listed in Table 6.
4.2
Evaluation
In order to assess the universal understanding capabilities of Qwen-Audio, as shown in Table 2, we perform
a comprehensive evaluation that encompasses various tasks, namely Automatic Speech Recognition (ASR),
Speech-to-Text Translation (S2TT), Automatic Audio Captioning (AAC), Acoustic Scene Classification (ASC),
Speech Emotion Recognition (SER), Audio Question and Answering (AQA), Vocal Sound Classification
(VSC), and Music Note Analysis (MNA). This evaluation is conducted across 12 datasets. The evalua-
tion datasets are rigorously excluded from the training data to avoid data leakage. The detailed training
configurations of both stages are listed in Table 6.
1https://github.com/mjpost/sacrebleu
9

Table 3: The results of Automatic Speech Recognition (ASR), Speech-to-Text Translation (S2TT), Auto-
matic Audio Captioning (AAC), Speech Recognition with Word-level Timestamps (SRWT), Acoustic Scene
Classification (ASC), Speech Emotion Recognition (SER), Audio Question and Answering (AQA), Vocal
Sound Classification (VSC), and Music Note Analysis (MNA) tasks. For SRWT task, the results of Forced-
aligner (McAuliffe et al., 2017) are to predict the timestamps given the ground-truth transcripts, while
Paraformer-large-TP (Gao et al., 2023) and Qwen-audio tackle a more challenging scenario by directly gener-
ating sequences containing both transcriptions and timestamps.
Task
Dataset
Model
Performance
Metrics
Results
ASR
Librispeech
dev-clean | dev-other |
test-clean | test-other
SpeechT5 (Ao et al., 2021)
WER ↓
2.1 | 5.5 | 2.4 | 5.8
SpeechNet (Chen et al., 2021)
- | - | 30.7 | -
SLM-FT (Wang et al., 2023b)
- | - | 2.6 | 5.0
SALMONN (Anonymous, 2023)
- | - | 2.1 | 4.9
Qwen-Audio
1.8 | 4.0 | 2.0 | 4.2
Aishell1
dev | test
MMSpeech-base (Zhou et al., 2022)
WER ↓
2.0 | 2.1
MMSpeech-large (Zhou et al., 2022)
1.6 | 1.9
Paraformer-large (Gao et al., 2023)
- | 2.0
Qwen-Audio
1.2 | 1.3
Aishell2
Mic | iOS | Android
MMSpeech-base (Zhou et al., 2022)
WER ↓
4.5 | 3.9 | 4.0
Paraformer-large (Gao et al., 2023)
- | 2.9 | -
Qwen-Audio
3.3 | 3.1 | 3.3
S2TT
CoVoST2
en-de | de-en |
en-zh | zh-en
SALMONN (Anonymous, 2023)
BLEU ↑
18.6 | - | 33.1 | -
SpeechLLaMA (Wu et al., 2023a)
- | 27.1 | - | 12.3
BLSP (Wang et al., 2023a)
14.1 | - | - | -
Qwen-Audio
25.1 | 33.9 | 41.5 | 15.7
CoVoST2
SpeechLLaMA (Wu et al., 2023a)
BLEU ↑
27.9 | 25.2 | 25.9
es-en | fr-en | it-en
Qwen-Audio
39.7 | 38.5 | 36.0
AAC
Clotho
Pengi (Deshmukh et al., 2023)
CIDEr |
SPICE | SPIDEr ↑
0.416 | 0.126 | 0.271
Qwen-Audio
0.441 | 0.136 | 0.288
SRWT
Industrial Data
Force-aligner (McAuliffe et al., 2017)
AAS (ms) ↓
60.3
Paraformer-large-TP (Gao et al., 2023)
65.3
Qwen-Audio
51.5
ASC
CochlScene
CochlScene (Jeong and Park, 2022)
ACC ↑
0.669
Qwen-Audio
0.795
TUT2017
Pengi (Deshmukh et al., 2023)
ACC ↑
0.353
Qwen-Audio
0.649
SER
Meld
WavLM-large (Chen et al., 2022)
ACC ↑
0.542
Qwen-Audio
0.557
AQA
ClothoAQA
ClothoAQA (Lipping et al., 2022)
ACC | ACC (binary) ↑
0.542 | 0.627
Pengi (Deshmukh et al., 2023)
- | 0.645
Qwen-Audio
0.579 | 0.749
VSC
VocalSound
CLAP (Elizalde et al., 2022)
ACC ↑
0.4945
Pengi (Deshmukh et al., 2023)
0.6035
Qwen-Audio
0.9289
MNA
NS. Qualities
Pengi (Deshmukh et al., 2023)
MAP ↑
0.3860
Qwen-Audio
0.4742
NS. Instrument
Pengi (Deshmukh et al., 2023)
ACC ↑
0.5007
Qwen-Audio
0.7882
4.3
Main Results
In this section, we present a comprehensive evaluation of the Qwen-Audio model, assessing its performance
across various tasks without any task-specific fine-tuning. We begin by examining its English Automatic
10

Table 4: Results of ASR tasks with or without training word-level timestamps tasks.
Method
LibriSpeech
AISHELL1
dev-clean
dev-other
test-clean
test-other
dev
test
w/o SRWT
1.93
4.18
2.22
4.21
1.54
1.71
Qwen-Audio
1.79
4.00
2.04
4.19
1.22
1.29
Table 5: Results of AQA tasks with or without training word-level timestamps tasks.
Method
ClothoAQA
MusicAVQA
test
test-binary
audio question
w/o SRWT
0.5648
0.7418
0.7027
Qwen-Audio
0.5795
0.7491
0.7211
Speech Recognition (ASR) results, as depicted in Table 3, where Qwen-Audio exhibits superior perfor-
mance compared to previous multi-task learning models. Specifically, it achieves a 2.0% and 4.2% WER
on the librispeech test-clean and test-other datasets, respectively. Similarly, the Chinese Mandarin ASR
results demonstrate Qwen-Audio’s competitive performance against previous approaches. To the best of
our knowledge, Qwen-Audio achieves state-of-the-art results on the Aishell1 dev and test sets. Furthermore,
we evaluate Qwen-Audio’s speech translation performance on the CoVoST2 dataset. The results reveal that
Qwen-Audio outperforms the baselines by a substantial margin across all seven translation directions.
Lastly, we analyze the performance of Qwen-Audio on various audio analysis tasks, including AAC, SWRT
ASC, SER, AQA, VSC, and MNA, as summarized in Table 3. Across these tasks, Qwen-Audio consistently
outperforms the baselines by a significant margin. Notably, it achieves state-of-the-art results on CochlScene,
ClothoAQA, and VocalSound, thereby demonstrating the model’s robust audio understanding capabilities.
4.4
Results of Interactive Chat
We showcase the conversational capabilities of Qwen-Audio-Chat through illustrative cases depicted in
Figure 2. Furthermore, we intend to provide public access to the trained models for online chat interactions.
4.5
The Analysis of Word-level Timestamps Prediction
We propose the task of speech recognition with word-level timestamps (SRWT) by training Qwen-Audio to
not only recognize speech transcripts but also predict the timestamps for each word. The purpose of SRWT is
twofold: firstly, to improve the model’s ability to align audio signals with fine-grained timestamps; secondly,
to support grounding of speech and audio, and grounding-based QA tasks in Qwen-Audio-Chat, such as
finding the starting and ending time of an audio segment mentioning a person’s name or identifying whether
a sound occurs in the given audio 2.
In this section, we exclude the training of SRWT tasks from multi-task pretraining while maintaining the other
tasks unchanged. Notably, the removal of SRWT does not impact the coverage of audio datasets for training
since SRWT tasks share the same audio dataset as automatic speech recognition (ASR) tasks. The results are
shown in Table 4 and Table 5: models trained with SRWT achieve superior performance in automatic speech
recognition and audio question-answering tasks, including natural sounds QA and Music QA. These results
highlight the efficacy of incorporating fine-grained word-level timestamps to enhance the general audio
signal grounding ability and subsequently improve the performance of sound and music signal QA tasks.
2Audio event detection can be considered as a subtask of event timestamp prediction as the absence of an event’s timestamp implies
its non-occurrence in the audio.
11

5
Conclusion
In this paper, we present the Qwen-Audio series, a set of large-scale audio-language models with universal
audio understanding abilities. To incorporate different kinds of audios for co-training, we propose a unified
multi-task learning framework that facilitates the sharing of knowledge among similar tasks and avoids one-to-
many mapping problem caused by different text formats. Without any task-specific fine-tuning, the resulting
Qwen-Audio models outperform previous works across diverse benchmarks, demonstrating its universal
audio understanding abilities. Through supervised instruction finetuning, Qwen-Audio-Chat showcases
robust capabilities in aligning with human intent, supporting multilingual and multi-turn dialogues from
both audio and text inputs.
6
Acknowledgements
We express our gratitude to Jinze Bai, Shuai Bai, Peng Wang, Sinan Tan, Shijie Wang for their insightful
discussion. We would like to thank Juan Zhu, Junyang Lin, Siqi Zheng, Jiaming Wang and Zhihao Du for
their support of this project.
References
Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur
Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: a visual language model for few-shot
learning. NeurIPS, 2022.
Peter Anderson, Basura Fernando, Mark Johnson, and Stephen Gould. Spice: Semantic propositional image
caption evaluation. In Computer Vision–ECCV 2016: 14th European Conference, Amsterdam, The Netherlands,
October 11-14, 2016, Proceedings, Part V 14. Springer, 2016.
Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak
Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et al. PaLM 2 technical report. arXiv:2305.10403,
2023.
Anonymous. SALMONN: Towards generic hearing abilities for large language models. In Submitted to The
Twelfth International Conference on Learning Representations, 2023. under review.
Junyi Ao, Rui Wang, Long Zhou, Chengyi Wang, Shuo Ren, Yu Wu, Shujie Liu, Tom Ko, Qing Li, Yu Zhang, et al.
Speecht5: Unified-modal encoder-decoder pre-training for spoken language processing. arXiv:2110.07205,
2021.
Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jin-
gren Zhou. Qwen-VL: A frontier large vision-language model with versatile abilities. CoRR, abs/2308.12966,
2023.
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners.
NeurIPS, 2020.
Hui Bu, Jiayu Du, Xingyu Na, Bengu Wu, and Hao Zheng. AISHELL-1: an open-source mandarin speech
corpus and a speech recognition baseline. In 20th Conference of the Oriental Chapter of the International
Coordinating Committee on Speech Databases and Speech I/O Systems and Assessment, O-COCOSDA 2017, Seoul,
South Korea, November 1-3, 2017. IEEE, 2017.
Keqin Chen, Zhao Zhang, Weili Zeng, Richong Zhang, Feng Zhu, and Rui Zhao. Shikra: Unleashing
multimodal llm’s referential dialogue magic. arXiv:2306.15195, 2023.
12

Sanyuan Chen, Chengyi Wang, Zhengyang Chen, Yu Wu, Shujie Liu, Zhuo Chen, Jinyu Li, Naoyuki Kanda,
Takuya Yoshioka, Xiong Xiao, Jian Wu, Long Zhou, Shuo Ren, Yanmin Qian, Yao Qian, Jian Wu, Michael
Zeng, Xiangzhan Yu, and Furu Wei. Wavlm: Large-scale self-supervised pre-training for full stack speech
processing. IEEE J. Sel. Top. Signal Process., 2022.
Yi-Chen Chen, Po-Han Chi, Shu-wen Yang, Kai-Wei Chang, Jheng-hao Lin, Sung-Feng Huang, Da-Rong
Liu, Chi-Liang Liu, Cheng-Kuang Lee, and Hung-yi Lee. Speechnet: A universal modularized model for
speech processing tasks. arXiv:2105.03070, 2021.
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul
Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. PaLM: Scaling language modeling
with pathways. arXiv:2204.02311, 2022.
Alexandre Défossez, Jade Copet, Gabriel Synnaeve, and Yossi Adi. High fidelity neural audio compression.
arXiv:2210.13438, 2022.
Soham Deshmukh, Benjamin Elizalde, Rita Singh, and Huaming Wang. Pengi: An audio language model for
audio tasks. CoRR, 2023.
Konstantinos Drossos, Samuel Lipping, and Tuomas Virtanen. Clotho: an audio captioning dataset. In 2020
IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP 2020, Barcelona, Spain, May
4-8, 2020. IEEE, 2020.
Jiayu Du, Xingyu Na, Xuechen Liu, and Hui Bu. AISHELL-2: transforming mandarin ASR research into
industrial scale. abs/1808.10583, 2018.
Benjamin Elizalde, Soham Deshmukh, Mahmoud Al Ismail, and Huaming Wang. CLAP: learning audio
concepts from natural language supervision. abs/2206.04769, 2022.
Jesse H. Engel, Cinjon Resnick, Adam Roberts, Sander Dieleman, Mohammad Norouzi, Douglas Eck, and
Karen Simonyan. Neural audio synthesis of musical notes with wavenet autoencoders. In Proceedings of
the 34th International Conference on Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-11 August 2017,
Proceedings of Machine Learning Research. PMLR, 2017.
Zhifu Gao, Zerui Li, Jiaming Wang, Haoneng Luo, Xian Shi, Mengzhe Chen, Yabin Li, Lingyun Zuo, Zhihao
Du, Zhangyu Xiao, and Shiliang Zhang. Funasr: A fundamental end-to-end speech recognition toolkit.
CoRR, abs/2305.11013, 2023.
Yuan Gong, Jin Yu, and James R. Glass. Vocalsound: A dataset for improving human vocal sounds recognition.
In IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP 2022, Virtual and Singapore,
23-27 May 2022, pages 151–155. IEEE, 2022. doi: 10.1109/ICASSP43922.2022.9746828. URL https://doi.
org/10.1109/ICASSP43922.2022.9746828.
Yuan Gong, Sameer Khurana, Leonid Karlinsky, and James R. Glass. Whisper-at: Noise-robust automatic
speech recognizers are also strong general audio event taggers. CoRR, abs/2307.03183, 2023a.
Yuan Gong, Hongyin Luo, Alexander H. Liu, Leonid Karlinsky, and James R. Glass. Listen, think, and
understand. CoRR, abs/2305.10790, 2023b.
Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai, Kushal Lakhotia, Ruslan Salakhutdinov, and Ab-
delrahman Mohamed. Hubert: Self-supervised speech representation learning by masked prediction of
hidden units. IEEE ACM Trans. Audio Speech Lang. Process., 2021.
Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu
Chen. Lora: Low-rank adaptation of large language models. arXiv:2106.09685, 2021.
Rongjie Huang, Mingze Li, Dongchao Yang, Jiatong Shi, Xuankai Chang, Zhenhui Ye, Yuning Wu, Zhiqing
Hong, Jiawei Huang, Jinglin Liu, Yi Ren, Zhou Zhao, and Shinji Watanabe. Audiogpt: Understanding and
generating speech, music, sound, and talking head. CoRR, abs/2304.12995, 2023.
13

Il-Young Jeong and Jeongsoo Park. Cochlscene: Acquisition of acoustic scene data using crowdsourcing.
abs/2211.02289, 2022.
Matthew Le, Apoorv Vyas, Bowen Shi, Brian Karrer, Leda Sari, Rashel Moritz, Mary Williamson, Vimal
Manohar, Yossi Adi, Jay Mahadeokar, and Wei-Ning Hsu. Voicebox: Text-guided multilingual universal
speech generation at scale. CoRR, 2023.
Junnan Li, Dongxu Li, Caiming Xiong, and Steven C. H. Hoi. Blip: Bootstrapping language-image pre-training
for unified vision-language understanding and generation. In ICML, 2022.
Junnan Li, Dongxu Li, Silvio Savarese, and Steven C. H. Hoi. BLIP-2: bootstrapping language-image pre-
training with frozen image encoders and large language models. In International Conference on Machine
Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA, Proceedings of Machine Learning Research.
PMLR, 2023.
Samuel Lipping, Parthasaarathy Sudarsanam, Konstantinos Drossos, and Tuomas Virtanen. Clotho-aqa: A
crowdsourced dataset for audio question answering. In 30th European Signal Processing Conference, EUSIPCO
2022, Belgrade, Serbia, August 29 - Sept. 2, 2022. IEEE, 2022.
Chenyang Lyu, Minghao Wu, Longyue Wang, Xinting Huang, Bingshuai Liu, Zefeng Du, Shuming Shi, and
Zhaopeng Tu. Macaw-llm: Multi-modal language modeling with image, audio, video, and text integration.
CoRR, abs/2306.09093, 2023.
Soumi Maiti, Yifan Peng, Shukjae Choi, Jee-weon Jung, Xuankai Chang, and Shinji Watanabe. Voxtlm: unified
decoder-only models for consolidating speech recognition/synthesis and speech/text continuation tasks.
arXiv:2309.07937, 2023.
Michael McAuliffe, Michaela Socolof, Sarah Mihuc, Michael Wagner, and Morgan Sonderegger. Montreal
forced aligner: Trainable text-speech alignment using kaldi. In Interspeech 2017, 18th Annual Conference of
the International Speech Communication Association, Stockholm, Sweden, August 20-24, 2017, 2017.
Annamaria Mesaros, Toni Heittola, Aleksandr Diment, Benjamin Elizalde, Ankit Shah, Emmanuel Vincent,
Bhiksha Raj, and Tuomas Virtanen. DCASE2017 challenge setup: Tasks, datasets and baseline system. In
Proceedings of the Workshop on Detection and Classification of Acoustic Scenes and Events, DCASE 2017, Munich,
Germany, November 16-17, 2017, 2017.
Eliya Nachmani, Alon Levkovitch, Julian Salazar, Chulayuth Asawaroengchai, Soroosh Mariooryad, R. J.
Skerry-Ryan, and Michelle Tadmor Ramanovich. Lms with a voice: Spoken language modeling beyond
speech tokens. CoRR, 2023.
Openai. Chatml documents. URL https://github.com/openai/openai-python/blob/main/chatml.md.
OpenAI. Introducing ChatGPT, 2022. URL https://openai.com/blog/chatgpt.
OpenAI. Gpt-4 technical report, 2023.
Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang,
Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with
human feedback. NeurIPS, 2022.
Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur. Librispeech: An ASR corpus based on
public domain audio books. In 2015 IEEE International Conference on Acoustics, Speech and Signal Processing,
ICASSP 2015, South Brisbane, Queensland, Australia, April 19-24, 2015. IEEE, 2015.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic evaluation of
machine translation. In Proceedings of the 40th annual meeting of the Association for Computational Linguistics,
2002.
14

Daniel S. Park, William Chan, Yu Zhang, Chung-Cheng Chiu, Barret Zoph, Ekin D. Cubuk, and Quoc V. Le.
Specaugment: A simple data augmentation method for automatic speech recognition. In Interspeech 2019,
20th Annual Conference of the International Speech Communication Association, Graz, Austria, 15-19 September
2019.
Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan Huang, Shuming Ma, and Furu Wei. Kosmos-2:
Grounding multimodal large language models to the world. arXiv:2306.14824, 2023.
Soujanya Poria, Devamanyu Hazarika, Navonil Majumder, Gautam Naik, Erik Cambria, and Rada Mihalcea.
MELD: A multimodal multi-party dataset for emotion recognition in conversations. In Proceedings of the
57th Conference of the Association for Computational Linguistics, ACL 2019, Florence, Italy, July 28- August 2,
2019, Volume 1: Long Papers. Association for Computational Linguistics, 2019.
Qwen. Introducing qwen-7b: Open foundation and human-aligned models (of the state-of-the-arts), 2023.
URL https://github.com/QwenLM/Qwen-7B.
Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and Ilya Sutskever. Robust
speech recognition via large-scale weak supervision. In International Conference on Machine Learning, ICML
2023, 23-29 July 2023, Honolulu, Hawaii, USA, 2023.
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou,
Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. The
Journal of Machine Learning Research, 2020.
Paul K. Rubenstein, Chulayuth Asawaroengchai, Duc Dung Nguyen, Ankur Bapna, Zalán Borsos, Félix
de Chaumont Quitry, Peter Chen, Dalia El Badawy, Wei Han, Eugene Kharitonov, Hannah Mucken-
hirn, Dirk Padfield, James Qin, Danny Rozenberg, Tara N. Sainath, Johan Schalkwyk, Matthew Sharifi,
Michelle Tadmor Ramanovich, Marco Tagliasacchi, Alexandru Tudor, Mihajlo Velimirovic, Damien Vincent,
Jiahui Yu, Yongqiang Wang, Vicky Zayats, Neil Zeghidour, Yu Zhang, Zhishuai Zhang, Lukas Zilka, and
Christian Havnø Frank. Audiopalm: A large language model that can speak and listen. CoRR.
Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yueting Zhuang. Hugginggpt: Solving
AI tasks with chatgpt and its friends in huggingface. CoRR, abs/2303.17580, 2023.
Xian Shi, Yanni Chen, Shiliang Zhang, and Zhijie Yan. Achieving timestamp prediction while recognizing
with non-autoregressive end-to-end asr model. In National Conference on Man-Machine Speech Communication.
Springer, 2023.
Yu Shu, Siwei Dong, Guangyao Chen, Wenhao Huang, Ruihua Zhang, Daochen Shi, Qiqi Xiang, and Yemin
Shi. Llasm: Large language and speech model. arXiv:2308.15930, 2023.
Quan Sun, Qiying Yu, Yufeng Cui, Fan Zhang, Xiaosong Zhang, Yueze Wang, Hongcheng Gao, Jingjing Liu,
Tiejun Huang, and Xinlong Wang. Generative pretraining in multimodality. arXiv:2307.05222, 2023.
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix,
Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. LLaMA: Open and efficient foundation
language models. arXiv:2302.13971, 2023a.
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix,
Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation
language models. arXiv:2302.13971, 2023b.
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bash-
lykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton-Ferrer,
Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia
Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin
Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne
Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor
Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta,
15

Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan,
Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang,
Angela Fan, Melanie Kambadur, Sharan Narang, Aurélien Rodriguez, Robert Stojnic, Sergey Edunov, and
Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models. CoRR, abs/2307.09288, 2023c.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser,
and Illia Polosukhin. Attention is all you need. In Isabelle Guyon, Ulrike von Luxburg, Samy Bengio,
Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett, editors, Advances in Neural
Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, 2017.
Ramakrishna Vedantam, C Lawrence Zitnick, and Devi Parikh. Cider: Consensus-based image description
evaluation. In CVPR, 2015.
Changhan Wang, Anne Wu, and Juan Miguel Pino. Covost 2: A massively multilingual speech-to-text
translation corpus. abs/2007.10310, 2020. URL https://arxiv.org/abs/2007.10310.
Chen Wang, Minpeng Liao, Zhongqiang Huang, Jinliang Lu, Junhong Wu, Yuchen Liu, Chengqing Zong, and
Jiajun Zhang. Blsp: Bootstrapping language-speech pre-training via behavior alignment of continuation
writing. arXiv:2309.00916, 2023a.
Mingqiu Wang, Wei Han, Izhak Shafran, Zelin Wu, Chung-Cheng Chiu, Yuan Cao, Yongqiang Wang, Nanxin
Chen, Yu Zhang, Hagen Soltau, Paul K. Rubenstein, Lukas Zilka, Dian Yu, Zhong Meng, Golan Pundak,
Nikhil Siddhartha, Johan Schalkwyk, and Yonghui Wu. SLM: bridge the thin gap between speech and text
foundation models. abs/2310.00230, 2023b.
Mingqiu Wang, Wei Han, Izhak Shafran, Zelin Wu, Chung-Cheng Chiu, Yuan Cao, Yongqiang Wang, Nanxin
Chen, Yu Zhang, Hagen Soltau, et al. Slm: Bridge the thin gap between speech and text foundation models.
arXiv:2310.00230, 2023c.
Tianrui Wang, Long Zhou, Ziqiang Zhang, Yu Wu, Shujie Liu, Yashesh Gaur, Zhuo Chen, Jinyu Li, and Furu
Wei. Viola: Unified codec language models for speech recognition, synthesis, and translation. CoRR, 2023d.
Xiaofei Wang, Manthan Thakker, Zhuo Chen, Naoyuki Kanda, Sefik Emre Eskimez, Sanyuan Chen, Min
Tang, Shujie Liu, Jinyu Li, and Takuya Yoshioka. Speechx: Neural codec language model as a versatile
speech transformer. CoRR, 2023e.
Jian Wu, Yashesh Gaur, Zhuo Chen, Long Zhou, Yimeng Zhu, Tianrui Wang, Jinyu Li, Shujie Liu, Bo Ren,
Linquan Liu, and Yu Wu. On decoder-only architecture for speech-to-text and large language model
integration. abs/2307.03917, 2023a.
Shengqiong Wu, Hao Fei, Leigang Qu, Wei Ji, and Tat-Seng Chua. Next-gpt: Any-to-any multimodal LLM.
CoRR, abs/2309.05519, 2023b.
Neil Zeghidour, Alejandro Luebs, Ahmed Omran, Jan Skoglund, and Marco Tagliasacchi. Soundstream: An
end-to-end neural audio codec. IEEE ACM Trans. Audio Speech Lang. Process., 2022.
Dong Zhang, Shimin Li, Xin Zhang, Jun Zhan, Pengyu Wang, Yaqian Zhou, and Xipeng Qiu. Speechgpt: Em-
powering large language models with intrinsic cross-modal conversational abilities. CoRR, abs/2305.11000,
2023a.
Xin Zhang, Dong Zhang, Shimin Li, Yaqian Zhou, and Xipeng Qiu. Speechtokenizer: Unified speech tokenizer
for speech large language models. CoRR, abs/2308.16692, 2023b.
Yu Zhang, Wei Han, James Qin, Yongqiang Wang, Ankur Bapna, Zhehuai Chen, Nanxin Chen, Bo Li, Vera
Axelrod, Gary Wang, Zhong Meng, Ke Hu, Andrew Rosenberg, Rohit Prabhavalkar, Daniel S. Park, Parisa
Haghani, Jason Riesa, Ginger Perng, Hagen Soltau, Trevor Strohman, Bhuvana Ramabhadran, Tara N.
Sainath, Pedro J. Moreno, Chung-Cheng Chiu, Johan Schalkwyk, Françoise Beaufays, and Yonghui Wu.
Google usm: Scaling automatic speech recognition beyond 100 languages. CoRR, 2023c.
16

Xiaohuan Zhou, Jiaming Wang, Zeyu Cui, Shiliang Zhang, Zhijie Yan, Jingren Zhou, and Chang Zhou.
Mmspeech: Multi-modal multi-task encoder-decoder pre-training for speech recognition. abs/2212.00500,
2022.
17

A
Hyperparameters
We report the detailed training hyperparameter settings of Qwen-Audio in Table 6.
Table 6: Training hyperparameters of Qwen-Audio
Configuration
Multi-task Pre-training
Supervised Fine-tuning
Audio encoder init.
Whisper-large-v2
Qwen-audio 1st-stage
LLM init.
Qwen-7B
Qwen-7B
SpecAugment Policy
LibriSpeech Basic
LibriSpeech Basic
Optimizer
AdamW
AdamW
Optimizer hyperparameter
β1=0.9, β2=0.98, eps = 1e−6
Peak learning rate
5e−5
1e−5
Minimum learning rate
1e−5
1e−6
Audio encoder learning rate decay
0.95
0
Learning rate schedule
cosine decay
cosine decay
Weight decay
0.05
0.05
Gradient clip
1.0
1.0
Training steps
500k
8k
Warm-up steps
2000
3k
Global batch size
120
128
Gradient Acc.
1
8
Numerical precision
bfloat16
bfloat16
Optimizer sharding
✓
✓
Activation checkpointing
✗
✗
Model parallelism
✗
2
Pipeline parallelism
✗
✗
18

