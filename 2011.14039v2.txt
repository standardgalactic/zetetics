An Investigation of Language Model Interpretability via Sentence Editing
Samuel Stevens
The Ohio State University
stevens.994@osu.edu
Yu Su
The Ohio State University
su.809@osu.edu
Abstract
Pre-trained language models (PLMs) like
BERT are being used for almost all language-
related tasks, but interpreting their behavior
still remains a signiﬁcant challenge and
many important questions remain largely
unanswered.
In this work, we re-purpose a
sentence editing dataset, where faithful high-
quality human rationales can be automatically
extracted and compared with extracted model
rationales, as a new testbed for interpretability.
This enables us to conduct a systematic inves-
tigation on an array of questions regarding
PLMs’ interpretability,
including the role
of pre-training procedure,
comparison of
rationale extraction methods, and different
layers in the PLM. The investigation generates
new insights, for example, contrary to the
common understanding, we ﬁnd that attention
weights correlate well with human rationales
and work better than gradient-based saliency
in extracting model rationales.
Both the
dataset and code are available at https:
//github.com/samuelstevens/
sentence-editing-interpretability
to facilitate future interpretability research.
1
Introduction
Pre-trained language models (PLMs) (Devlin et al.,
2019; Liu et al., 2019; Beltagy et al., 2019) are
pervasively used in language-related tasks, but
interpreting their predictions is notoriously difﬁ-
cult because of their parameters’ complex inter-
dependencies. Given a speciﬁc prediction, we want
to know why a model made that decision, both to
further improve performance and to use the model
in high-stakes scenarios such as healthcare or bank
loan approvals where interpretability is important.
This has motivated efforts in extracting model ex-
planations, typically in the form of rationales, i.e.,
subsets of the original input that support a decision
(Zaidan et al., 2007). Attention heatmaps (Xu et al.,
2015) and gradient-based saliency maps (Simonyan
et al., 2014) are common extraction methods.
The algorithm <del>descripted</del>
<ins>described</ins> in the previous
sections has several advantages.
The algorithm descripted→described in the previous sec-
tions has several advantages.
However, we <del>must note that
we </del>still have no means of
deciding which documents out of _MATH_
deserve to be in _MATH_ and _MATH_,
respectively.
However, we must note that we still have no means of
deciding which documents out of _MATH_ deserve to be
in _MATH_ and _MATH_, respectively.
Figure 1: Two “need edit” examples from AESW in the
original data format and a human-readable format. The
ﬁrst example (a) has a spelling error “descripted” and
the second (b) is edited for concision.
There have been efforts on developing datasets
for interpretability research, for example, the re-
cent ERASER benchmark (DeYoung et al., 2020).
However, the majority of ERASER tasks use hu-
man rationales highlighted by a different annotator
after the original labeling process. Such rationales
are not necessarily faithful; a rationale highlighted
by the second annotator may not have been ac-
tually used by the ﬁrst annotator while labeling.
Manual rationale labeling is also difﬁcult and time-
consuming; of the six datasets in the ERASER
benchmark, only one has more than 200 examples.
Our ﬁrst contribution is the realization that
AESW (Automatic Evaluation of Scientiﬁc Writ-
ing; Daudaravicius et al., 2016), a sentence editing
dataset, contains thousands of faithful human ra-
tionales that can be automatically re-purposed for
interpretability research. See Figure 1 for exam-
ples. This provides a new, large-scale dataset with
truly faithful human rationales for interpretability
questions surrounding model rationales.
Our second contribution is investigating multi-
ple factors in PLM rationale plausibility. More
plausible rationales are valuable in human-in-the-
arXiv:2011.14039v2  [cs.CL]  26 Sep 2021

loop systems where humans use model rationales
to make a ﬁnal decision. We compare (1) pre-
training procedures, (2) attention weight- and in-
put gradient-based methods of extracting model
rationales, (3) correlation between model rationale
plausibility and model conﬁdence, and (4) differ-
ences in transformer layers. While previous work
(Jain and Wallace, 2019; Serrano and Smith, 2019)
has shown that attention weights are not always
faithful, we ﬁnd that they correlate with human
rationales better than gradient-based methods.
2
Related Work
Human rationales (as deﬁned by Zaidan et al.,
2007) are subsets of input highlighted by human
annotators as evidence to support a decision. The
same annotator labeling an example might also
highlight their rationale (Khashabi et al., 2018;
Thorne et al., 2018). In other cases, rationales are
collected for an existing dataset by different anno-
tators (Zaidan et al., 2008; Camburu et al., 2018;
Rajani et al., 2019). As previously stated, such
rationales may not be faithful. Rationale length can
vary from sub-sentence spans (Talmor et al., 2019)
to multiple sentences (Lehman et al., 2019).
Model rationales can be produced as an explicit
training objective (Zaidan et al., 2008) or extracted
as a post-hoc explanation. Post-hoc methods typi-
cally assign token-level importance scores: atten-
tion weights are often used in attention-based mod-
els (Bahdanau et al., 2016), gradient-based expla-
nations are typical for differentiable models (Denil
et al., 2015; Shrikumar et al., 2017), and LIME is a
model-agnostic method (Ribeiro et al., 2016). We
follow work using BERT’s attention (Clark et al.,
2019; Kovaleva et al., 2019) to extract rationales.
A model rationale is evaluated on faithfulness
(if it is actually used to make a decision) and
plausibility (if it is easily understood by humans).
Faithfulness can be measured by perturbing inputs
marked as evidence and measuring change in out-
puts (Jain and Wallace, 2019; Serrano and Smith,
2019). Plausibility can be measured through user
studies, wherein users are given a model rationale
and asked either to predict the model’s decision
(Kim et al., 2016) or to rate rationale understand-
ability (Nguyen, 2018; Ehsan et al., 2018, 2019;
Strout et al., 2019).
Rationale plausibility can
also be measured by similarity to human rationales
(DeYoung et al., 2020), but this requires faithful
human rationales. We use similarity to evaluate
rationale plausibility because we gather faithful hu-
man rationales from sentence editing annotations.
3
Proposed Task
We propose re-purposing the AESW classiﬁca-
tion task for measuring model interpretability. We
gather examples from AESW from which we can
automatically extract faithful and sufﬁcient human
rationales, and then use said rationales to investi-
gate factors in PLM rationale plausibility, specif-
ically BERT (Devlin et al., 2019) and its variants.
It is worth noting that our rationale dataset can be
used for other interpretability topics such as train-
ing with rationales or evaluating rationale faithful-
ness; we will focus on plausibility considering the
scope of this paper.
3.1
Human Rationales
Human rationales are substrings used as evidence
for a decision (Zaidan et al., 2007). Faithful and
sufﬁcient (enough evidence to justify a decision)
human rationales can be used as gold labels for
evaluating model rationale plausibility.
The original AESW task is to predict if a sen-
tence from a scientiﬁc paper needs editing. Daudar-
avicius et al. extract spans of a sentence before and
after professional editing1 as deleted (<del>) or
inserted (<ins>) and provide 1.1M training, 140K
validation and 140K testing examples. Sentences
without changes are assumed to not require editing.
We exploit the data format to automatically
extract faithful and sufﬁcient human rationales.
Delete text (text between <del> tags) is always
a faithful rationale (such text is a source of the er-
ror). For edits with arbitrary <del> text alone
and <ins> text, the <del> alone is not always a
sufﬁcient rationale to justify “need edit”. Consider
a sentence where a verb is incorrectly conjugated
and replaced with the correctly conjugated verb.
The incorrectly conjugated verb is not sufﬁcient
to decide that the sentence needs editing; the sur-
rounding context is required. To ﬁnd edits where
<del> text is always a sufﬁcient rationale, we use
two criteria:
1. A spelling error is corrected. (spelling error)
2. Text is only deleted, not added. (deleted text)
Spelling errors are always a sufﬁcient rationale
to justify editing a sentence (see Figure 1a). In
edits with no insertions, removing the <del> text
leads to an error-free sentence, so the <del> text
1Native English speaking editors working at VTeX.

is sufﬁcient explanation for editing (see Figure 1b).
These criteria lead to both a simple, lexical task
(spelling error) and a more challenging semantic
task (deleted text) and constitute a wider range of
challenges for future interpretability research. We
extract faithful and sufﬁcient human rationales for
1,321 spelling error edits and 6,741 deleted text
edits from the validation set of AESW.2
3.2
Model Rationales
Model rationales are substrings provided by a
model as evidence for a decision. Given a model,
an example xi and a prediction yi, we extract three
model rationales.
First, we use attention maps (Xu et al., 2015;
Kovaleva et al., 2019) to rank word relevance. We
measure the total attention weight from BERT’s
ﬁnal layer’s initial [CLS] token to each token tj
across H attention heads. Then we add those totals
together for each token tj in a word w:
score(w) =
X
tj∈w
H
X
h=1
Attnhj ([CLS] →tj) . (1)
Second, we use gradient-based saliency (speciﬁ-
cally gradient×input; Denil et al., 2015; Shrikumar
et al., 2017) to rank word relevance. We calculate a
saliency score for each token tj in xi. First, change
in model output with respect to tj’s input embed-
ding ∇e(tj)fyi(xi) captures the sensitivity to token
t. The dot product with e(tj) is then a scalar mea-
sure of each token’s marginal impact on the model
prediction (Han et al., 2020). Finally, we again
compute a word-level score by summing over each
token tj in word w:
score(w) =
X
tj∈w
∇e(tj)fyi(xi) · e(tj)
(2)
In contrast to attention, gradient×input faithfully
measures the marginal effect of each input token
on the prediction (Bastings and Filippova, 2020).
Finally, we extract a third set of rankings using
gradient×input’s magnitude to rank words.
3.3
Evaluation
We evaluate extracted rationale plausibility using
similarity to human rationales (DeYoung et al.,
2More details on extracting human rationales, as well as
additional examples, can be found in Appendix C
2020). We use the continuous word scores gener-
ated in the previous section to rank relevance, then
use mean reciprocal rank as an evaluation metric.3
4
Experiments
To demonstrate the utility of the AESW task for
interpretability research, we present four experi-
ments, each with the goal of understanding factors
in PLM rationale plausibility.
For our experiments, we add a linear layer and
sigmoid activation function on top of the [CLS]
token representation, ﬁne-tune BERT-base end-to-
end on the AESW training set using the original
training objective (classify a sentence as “need edit”
or “no edit”) and use validation loss to tune hyper-
parameters. We do not add any interpretability or
rationale-related objectives. The classiﬁcation F1
for the models on spelling error and deleted text
edits is 82.0 and 74.0, respectively.4 We extract
and evaluate model rationales on all spelling error
and deleted text edits, even edits that a model does
not correctly predict as “need edit.”
Does pre-training procedure affect rationale
plausibility?
We are interested in how pre-
training affects plausibility after ﬁne-tuning (to the
best of our knowledge, this is a previously unex-
plored topic). We compare BERT and two variants,
RoBERTa (Liu et al., 2019) and SciBERT (Belt-
agy et al., 2019). These models have the same
architecture but differ in pre-training: RoBERTa is
pre-trained for 5x longer with 10x more data than
BERT, and SciBERT is pre-trained on a corpus of
academic papers. We extract model rationales us-
ing attention weights and evaluate their plausibility.
As seen in Figure 2, RoBERTa and BERT gen-
erate nearly equally plausible rationales despite
differences in pre-training corpus size. We hypoth-
esize that SciBERT generates less plausible ratio-
nales because it encodes “need edit” representa-
tions in earlier layers (rather than in the ﬁnal layer),
then attends to [SEP] as a no-op in later layers, as
proposed in Clark et al. (2019) and Kobayashi et al.
(2020) and further conﬁrmed in the subsequent ex-
periments.
3We performed our analyses with additional, similar met-
rics; our main observations remain consistent independent of
the choice of metric. Appendix D contains additional details
and complete results.
4Appendix B contains more details on ﬁne-tuning proce-
dure and results.

BERT
RoBERTa SciBERT
0.0
0.2
0.4
0.6
0.8
Mean Reciprocal Rank
Pre-Training Scheme
Attention
Grad.
(dir.)
Grad.
(mag.)
Extracting Method
Spelling Errors
Deleted Text
Figure 2: Left: BERT, RoBERTa and SciBERT’s at-
tention weight rationale plausibility. Right: BERT’s
attention weight, gradient×input and |gradient×input|
rationale plausibility (see Section 3.2).
Do attention weights or input gradients pro-
duce better rationales?
In contrast to attention
weights, gradient×input scores are naturally faith-
ful with respect to individual feature importance
(Jain and Wallace, 2019; Bastings and Filippova,
2020). However, attention weights can represent
word relevance in context, potentially leading to
more plausible rationales. We extract and evalu-
ate rationales using attention weights and the two
gradient×input methods described in Section 3.2.
Figure 2 shows that attention-based rationales
are more plausible and that the difference is more
pronounced on deleted text edits. Using |gradient×
input| (right-most) also shows improvements over
directional gradient×input (middle-right), in con-
trast to Han et al. (2020).
Are plausibility and conﬁdence correlated?
We are curious if BERT is more conﬁdent in its
classiﬁcation decisions when it attends to the origi-
nal evidence used by editors. We quantify BERT’s
classiﬁcation conﬁdence by treating the sigmoid
activation function’s value as the probability of a
“need edit” decision. We only consider examples
that BERT correctly classiﬁes as “need edit” and
calculate BERT’s mean conﬁdence. We ﬁnd that
when human rationales are BERT’s most attended-
to words, it is 11.7% and 11.9% more conﬁdent in
its predictions for spelling error and deleted text
edits, respectively.
One possible explanation for such a correlation
is that easy-to-classify examples also have easy-to-
identify rationales. Because agreement with human
rationales is not the training objective, however, we
believe this correlation suggests that BERT learns
to classify edits similarly to humans.
1
2
3
4
5
6
7
8
9
10
11
12
Layer
0.0
0.2
0.4
0.6
0.8
Mean Reciprocal Rank
Spelling Errors
BERT
RoBERTa
SciBERT
1
2
3
4
5
6
7
8
9
10
11
12
Layer
0.0
0.2
0.4
0.6
0.8
Mean Reciprocal Rank
Deleted Text
BERT
RoBERTa
SciBERT
Figure 3: Mean reciprocal rank for each layer (using
the mean strategy) for each model for spelling error
and deleted text edits.4
How does transformer layer affect plausibility?
It is widely agreed that BERT’s early layers encode
more lexical and phrasal information than other
layers (Jawahar et al., 2019; Rogers et al., 2020).
We hypothesize that rationales extracted from early
layers will be more plausible for spelling error than
deleted text edits because it is a more lexical task.
We extract rationales from each layer’s attention
weights and measure their plausibility in Figure 3.
The results conﬁrm the hypothesis and show
that early layers in BERT models are indeed more
lexically-oriented.
We also ﬁnd that SciBERT
strongly attends to spelling errors in earlier lay-
ers. We believe it is because that, pre-trained on
well-formed academic text, SciBERT is not as well
exposed to spelling errors as BERT and RoBERTa
so it learns to attend to spelling errors in earlier
layers during ﬁne-tuning.
4BERT shows a decline in plausibility at layer 11 in both
edit types because it attends heavily to periods.

5
Conclusion
We re-purpose the AESW task to gather thousands
of inherently faithful human rationales and inves-
tigate an array of questions regarding PLM inter-
pretability. We ﬁnd, among other new insights, that
attention weights correlate well with human ratio-
nales and produce more plausible rationales than
input gradients, which is different from existing
understanding. Furthermore, we ﬁnd that BERT is
more conﬁdent in its predictions when it attends
to the same words that a human did, supporting
the idea that while attention is not inherently faith-
ful, attention-based models might rely on the same
information as humans when making a prediction.
Future work might expand the subset of exam-
ples for which human rationales can be automat-
ically extracted, include human rationales during
training or evaluate faithful-by-design model ratio-
nales on this dataset.
6
Acknowledgements
The authors would like to extend their thanks to the
anonymous reviewers for their insightful feedback.
References
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
gio. 2016.
Neural machine translation by jointly
learning to align and translate.
ArXiv preprint
arXiv:1409.0473.
Jasmijn Bastings and Katja Filippova. 2020. The ele-
phant in the interpretability room: Why use attention
as explanation when we have saliency methods? In
Proceedings of the Third BlackboxNLP Workshop
on Analyzing and Interpreting Neural Networks for
NLP, pages 149–155, Online. Association for Com-
putational Linguistics.
Iz Beltagy, Kyle Lo, and Arman Cohan. 2019. SciB-
ERT: A pretrained language model for scientiﬁc
text.
In Proceedings of the 2019 Conference on
Empirical Methods in Natural Language Processing
and the 9th International Joint Conference on
Natural Language Processing (EMNLP-IJCNLP),
pages 3615–3620, Hong Kong, China. Association
for Computational Linguistics.
Oana-Maria Camburu,
Tim Rocktäschel,
Thomas
Lukasiewicz, and Phil Blunsom. 2018. e-snli: Nat-
ural language inference with natural language ex-
planations.
In Advances in Neural Information
Processing Systems, volume 31, pages 9539–9549.
Curran Associates, Inc.
Kevin Clark, Urvashi Khandelwal, Omer Levy, and
Christopher D. Manning. 2019.
What does
BERT look at?
an analysis of BERT’s atten-
tion.
In Proceedings of the 2019 ACL Workshop
BlackboxNLP: Analyzing and Interpreting Neural
Networks for NLP, pages 276–286, Florence, Italy.
Association for Computational Linguistics.
Vidas Daudaravicius, Rafael E. Banchs, Elena Volod-
ina, and Courtney Napoles. 2016. A report on the au-
tomatic evaluation of scientiﬁc writing shared task.
In Proceedings of the 11th Workshop on Innovative
Use of NLP for Building Educational Applications,
pages 53–62, San Diego, CA. Association for Com-
putational Linguistics.
Misha Denil, Alban Demiraj, and Nando de Freitas.
2015. Extraction of salient sentences from labelled
documents. ArXiv preprint arXiv:1412.6815.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2019.
BERT: Pre-training of
deep bidirectional transformers for language under-
standing.
In Proceedings of the 2019 Conference
of the North American Chapter of the Association
for Computational Linguistics: Human Language
Technologies, Volume 1 (Long and Short Papers),
pages 4171–4186, Minneapolis, Minnesota. Associ-
ation for Computational Linguistics.
Jay DeYoung, Sarthak Jain, Nazneen Fatema Rajani,
Eric Lehman, Caiming Xiong, Richard Socher, and
Byron C. Wallace. 2020. ERASER: A benchmark to
evaluate rationalized NLP models. In Proceedings
of the 58th Annual Meeting of the Association for
Computational Linguistics, pages 4443–4458, On-
line. Association for Computational Linguistics.
Upol Ehsan, Brent Harrison, Larry Chan, and Mark O.
Riedl. 2018.
Rationalization:
A neural ma-
chine translation approach to generating natural lan-
guage explanations.
In Proceedings of the 2018
AAAI/ACM Conference on AI, Ethics, and Society,
AIES ’18, page 81–87, New York, NY, USA. Asso-
ciation for Computing Machinery.
Upol Ehsan, Pradyumna Tambwekar, Larry Chan,
Brent Harrison, and Mark O. Riedl. 2019.
Auto-
mated rationale generation: A technique for explain-
able ai and its effects on human perceptions.
In
Proceedings of the 24th International Conference on
Intelligent User Interfaces, IUI ’19, page 263–274,
New York, NY, USA. Association for Computing
Machinery.
Xiaochuang Han,
Byron C. Wallace,
and Yulia
Tsvetkov. 2020. Explaining black box predictions
and unveiling data artifacts through inﬂuence func-
tions. In Proceedings of the 58th Annual Meeting
of the Association for Computational Linguistics,
pages 5553–5563, Online. Association for Compu-
tational Linguistics.
Sarthak Jain and Byron C. Wallace. 2019. Attention
is not Explanation.
In Proceedings of the 2019
Conference of the North American Chapter of the
Association for Computational Linguistics: Human

Language Technologies, Volume 1 (Long and Short
Papers), pages 3543–3556, Minneapolis, Minnesota.
Association for Computational Linguistics.
Ganesh Jawahar, Benoît Sagot, and Djamé Seddah.
2019.
What does BERT learn about the structure
of language?
In Proceedings of the 57th Annual
Meeting of the Association for Computational
Linguistics, pages 3651–3657, Florence, Italy. As-
sociation for Computational Linguistics.
Daniel Khashabi, Snigdha Chaturvedi, Michael Roth,
Shyam Upadhyay, and Dan Roth. 2018.
Look-
ing beyond the surface:
A challenge set for
reading comprehension over multiple sentences.
In Proceedings of the 2018 Conference of the
North American Chapter of the Association for
Computational
Linguistics:
Human
Language
Technologies, Volume 1 (Long Papers), pages 252–
262, New Orleans, Louisiana. Association for Com-
putational Linguistics.
Been Kim, Rajiv Khanna, and Oluwasanmi Koyejo.
2016.
Examples are not enough, learn to criti-
cize! criticism for interpretability. In Proceedings
of the 30th International Conference on Neural
Information Processing Systems, NIPS’16, page
2288–2296, Red Hook, NY, USA. Curran Asso-
ciates Inc.
Goro Kobayashi, Tatsuki Kuribayashi, Sho Yokoi, and
Kentaro Inui. 2020. Attention is not only a weight:
Analyzing transformers with vector norms.
In
Proceedings of the 2020 Conference on Empirical
Methods in Natural Language Processing (EMNLP),
pages 7057–7075, Online. Association for Computa-
tional Linguistics.
Olga Kovaleva, Alexey Romanov, Anna Rogers, and
Anna Rumshisky. 2019. Revealing the dark secrets
of BERT. In Proceedings of the 2019 Conference on
Empirical Methods in Natural Language Processing
and the 9th International Joint Conference on
Natural Language Processing (EMNLP-IJCNLP),
pages 4365–4374, Hong Kong, China. Association
for Computational Linguistics.
Eric Lehman, Jay DeYoung, Regina Barzilay, and
Byron C. Wallace. 2019.
Inferring which med-
ical treatments work from reports of clinical tri-
als.
In Proceedings of the 2019 Conference of
the North American Chapter of the Association
for Computational Linguistics: Human Language
Technologies, Volume 1 (Long and Short Papers),
pages 3705–3717, Minneapolis, Minnesota. Associ-
ation for Computational Linguistics.
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-
dar Joshi, Danqi Chen, Omer Levy, Mike Lewis,
Luke Zettlemoyer, and Veselin Stoyanov. 2019.
Roberta: A robustly optimized bert pretraining ap-
proach. ArXiv preprint arXiv:1907.11692.
Dong Nguyen. 2018.
Comparing automatic and hu-
man evaluation of local explanations for text clas-
siﬁcation. In Proceedings of the 2018 Conference
of the North American Chapter of the Association
for Computational Linguistics: Human Language
Technologies, Volume 1 (Long Papers), pages 1069–
1078, New Orleans, Louisiana. Association for Com-
putational Linguistics.
Nazneen Fatema Rajani, Bryan McCann, Caiming
Xiong, and Richard Socher. 2019. Explain yourself!
leveraging language models for commonsense rea-
soning. In Proceedings of the 57th Annual Meeting
of the Association for Computational Linguistics,
pages 4932–4942, Florence, Italy. Association for
Computational Linguistics.
Marco Ribeiro, Sameer Singh, and Carlos Guestrin.
2016.
“why should I trust you?”: Explaining the
predictions of any classiﬁer. In Proceedings of the
2016 Conference of the North American Chapter
of the Association for Computational Linguistics:
Demonstrations, pages 97–101, San Diego, Califor-
nia. Association for Computational Linguistics.
Anna Rogers, Olga Kovaleva, and Anna Rumshisky.
2020. A primer in bertology: What we know about
how bert works. Transactions of the Association for
Computational Linguistics, 8:842–866.
Soﬁa Serrano and Noah A. Smith. 2019. Is attention
interpretable?
In Proceedings of the 57th Annual
Meeting of the Association for Computational
Linguistics, pages 2931–2951, Florence, Italy. As-
sociation for Computational Linguistics.
Avanti Shrikumar, Peyton Greenside, and Anshul Kun-
daje. 2017.
Learning important features through
propagating activation differences. In Proceedings
of the 34th International Conference on Machine
Learning, volume 70 of Proceedings of Machine
Learning Research, pages 3145–3153, International
Convention Centre, Sydney, Australia. PMLR.
Karen Simonyan, Andrea Vedaldi, and Andrew Zisser-
man. 2014. Deep inside convolutional networks: Vi-
sualising image classiﬁcation models and saliency
maps. ArXiv preprint arXiv:1312.6034.
Julia Strout, Ye Zhang, and Raymond Mooney. 2019.
Do human rationales improve machine explana-
tions? In Proceedings of the 2019 ACL Workshop
BlackboxNLP: Analyzing and Interpreting Neural
Networks for NLP, pages 56–62, Florence, Italy. As-
sociation for Computational Linguistics.
Alon Talmor, Jonathan Herzig, Nicholas Lourie, and
Jonathan Berant. 2019. CommonsenseQA: A ques-
tion answering challenge targeting commonsense
knowledge. In Proceedings of the 2019 Conference
of the North American Chapter of the Association
for Computational Linguistics: Human Language
Technologies, Volume 1 (Long and Short Papers),
pages 4149–4158, Minneapolis, Minnesota. Associ-
ation for Computational Linguistics.
James
Thorne,
Andreas
Vlachos,
Christos
Christodoulopoulos,
and
Arpit
Mittal.
2018.

FEVER: a large-scale dataset for fact extraction
and VERiﬁcation.
In Proceedings of the 2018
Conference of the North American Chapter of
the Association for Computational Linguistics:
Human Language Technologies, Volume 1 (Long
Papers), pages 809–819, New Orleans, Louisiana.
Association for Computational Linguistics.
Thomas Wolf, Lysandre Debut, Victor Sanh, Julien
Chaumond, Clement Delangue, Anthony Moi, Pier-
ric Cistac, Tim Rault, Rémi Louf, Morgan Fun-
towicz, Joe Davison, Sam Shleifer, Patrick von
Platen, Clara Ma, Yacine Jernite, Julien Plu, Can-
wen Xu, Teven Le Scao, Sylvain Gugger, Mariama
Drame, Quentin Lhoest, and Alexander M. Rush.
2020.
Huggingface’s transformers:
State-of-the-
art natural language processing.
ArXiv preprint
arXiv:1910.03771.
Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho,
Aaron Courville, Ruslan Salakhudinov, Rich Zemel,
and Yoshua Bengio. 2015.
Show, attend and tell:
Neural image caption generation with visual atten-
tion.
In Proceedings of the 32nd International
Conference on Machine Learning, volume 37 of
Proceedings of Machine Learning Research, pages
2048–2057, Lille, France. PMLR.
Omar Zaidan, Jason Eisner, and Christine Piatko.
2007. Using “annotator rationales” to improve ma-
chine learning for text categorization.
In Human
Language Technologies 2007: The Conference of
the North American Chapter of the Association for
Computational Linguistics; Proceedings of the Main
Conference, pages 260–267, Rochester, New York.
Association for Computational Linguistics.
Omar F. Zaidan, Jason Eisner, and Christine Piatko.
2008. Machine learning with annotator rationales
to reduce annotation cost.
In Proceedings of the
NIPS*2008 Workshop on Cost Sensitive Learning.

A
AESW Details
The original AESW task is to classify a sentence
from an academic paper as “need edit” or “no edit”.
There is no special markup attached to the sentence
when it is given as input to a model. Task partici-
pants receive the training and validation sets with
<del> and <ins> tokens for model development,
and Daudaravicius et al. provided an automatic test
set evaluation through an online portal. Daudar-
avicius et al. collected these sentences from 9,919
journal articles published by Springer Publishing
Company and edited at VTeX. Sentences before
and after editing were automatically aligned using a
modiﬁed diff algorithm.5 After the challenge, Dau-
daravicius et al. released the dataset in its entirety
(all three datasets with all tokens/spans included).
B
Training Details
The AESW task uses scientiﬁc papers written in La-
TeX, which contains markup characters that impact
sentence meaning. The original authors (Daudar-
avicius et al.) replace these characters with special
tokens, as seen in Table 1. We add these four spe-
cial tokens (_MATH_, _MATHDISP_, _CITE_ and
_REF_) to the model vocabulary, ﬁne-tuning the
word representations during training.
LaTeX Example
Special Token
$\beta_{2}$
_MATH_
$$2 + 3$$
_MATHDISP_
\cite{google2018}
_CITE_
\ref{tab:results}
_REF_
Table 1: Special tokens found in the original AESW
data that should not be split further into bytes/tokens.
We train all models for a maximum of 30 epochs
with a patience of 5 on a single Tesla P100 GPU.
All models (BERT6, SciBERT7, RoBERTA8) are
based on their HuggingFace (Wolf et al., 2020)
implementations.
We list all the key hyperparameters and tuning
bounds for reproducibility in Table 3. Our ﬁnal
results for all three BERT-based models along with
5More details can be found in (Daudaravicius et al., 2016)
6https://huggingface.co/transformers/
v3.0.2/model_doc/bert.html#
bertforsequenceclassification
7https://github.com/allenai/scibert#
pytorch-huggingface-models
8https://huggingface.co/transformers/
v3.0.2/model_doc/roberta.html#
robertaforsequenceclassification
the top three models from (Daudaravicius et al.,
2016) can be found in Table 2. Additionally, we
will release code and instructions for reproducing
our results.
Model
Dev F1
Test F1
CNN+LSTM
–
0.628
CNN
–
0.611
SVM
–
0.555
BERTbase
0.654
0.666
RoBERTabase
0.661
0.670
SciBERTscivocab
0.658
0.668
Table 2: Performance on the original AESW sentence
classiﬁcation task. Dev set results are not available for
models reported in Daudaravicius et al. (2016).
C
Gathering Human Rationales
To ﬁnd spelling error edits, we look for sentences
with a deleted, misspelled word followed by an
inserted, correctly spelled word. The ﬁrst two ex-
amples in Figure 4 are examples of spelling error
edits, while the third is not.
To ﬁnd deleted text edits, we look for sentences
where text is removed but not added. The fourth
and ﬁfth examples in Figure 4 are examples of
deleted text edits.
D
Evaluating Model Rationales
Although the main text presents our analyses using
mean reciprocal rank, we performed our analyses
with multiple metrics. Here we provide speciﬁc
deﬁnitions for our metrics and our complete results
for all models with every metric.
Mean reciprocal rank
Because human ratio-
nales can be made up of multiple words, we need
to modify mean reciprocal ranking. For a single
sentence, a model’s ordered ranking of words M
and an unordered human rationale H:
1. Find the top ranked word w in M from H and
record the rank.
2. Remove w from M and H.
3. Repeat until H is empty.
4. Use the reciprocal of the mean rank.
If we did not remove words from M, a perfect
score for a rationale with multiple words would be
impossible. Consider the sentence “The boy ate
the found his ball.” If the rationale ranked ‘ate’
and ‘the’ as most important, the mean rationale
rank would be (1 + 2)/2 = 1.5 and the rationale’s
reciprocal rank would be 1/1.5 = 0.66, despite a
perfect rationale.

Model
Hyperparameters
Hyperparameter bounds
BERTbase
learning rate: 1 × 10−6
batch size: 32
model: bert-base-uncased
vocab size: 30526 (normally 30522)
learning rate: (2 × 10−7,
1 × 10−6, 2 × 10−5,
1 × 10−4)
RoBERTabase
learning rate: 1 × 10−6
batch size: 32
model: roberta-base
vocab size: 50269 (normally 50265)
learning rate: (1 × 10−6)
SciBERT
learning rate: 1 × 10−6
batch size: 32
model: allenai/scibert_scivocab_uncased
vocab size: 31094 (normally 31090)
learning rate: (1 × 10−6)
Table 3: Hyperparameter options for each model. Note that each model had 4 special tokens added to the vocab-
ulary. BERT was ﬁne-tuned ﬁrst. Because of compute limitations, RobERTa and SciBERT were both ﬁne-tuned
using the same hyperparameters as the optimal BERT conﬁguration (learning rate of (1 × 10−6)).
We take the mean reciprocal ranking across all
examples to evaluate rationales.
Mean area under precision-recall curve
Using
the model relevance scores for words in a sentence,
we adjust the threshold for classifying a word as
part of the rationale, calculate a precision-recall
curve and measure the area underneath. We take
the mean AUPRC across all examples.
Mean top 1 match
We score a rationale as 1 if
the rationale’s top ranked word is the human ratio-
nale and 0 otherwise. This means that all multiple-
word rationales are automatically not a match and
scored as 0. We take the mean of these scores
across all examples.
Our main observations are consistent across met-
rics: Table 4 contains all results for models on
spelling error edits and Table 5 contains all results
for models on deleted text edits.

Model
F1
Method
Classiﬁcation
Mean Recip.
AUPRC
Top 1 Match
BERT
82.1
Attention
All
0.841
0.115
0.756
BERT
100.0
Attention
Correct
0.895
0.081
0.824
BERT
0.0
Attention
Wrong
0.717
0.194
0.602
BERT
82.1
gradient×input
All
0.407
0.332
0.325
BERT
100.0
gradient×input
Correct
0.401
0.335
0.322
BERT
0.0
gradient×input
Wrong
0.422
0.325
0.331
BERT
82.1
|gradient × input|
All
0.701
0.201
0.579
BERT
100.0
|gradient × input|
Correct
0.730
0.184
0.615
BERT
0.0
|gradient × input|
Wrong
0.635
0.241
0.498
RoBERTa
81.4
Attention
All
0.834
0.118
0.739
RoBERTa
100.0
Attention
Correct
0.886
0.085
0.811
RoBERTa
0.0
Attention
Wrong
0.721
0.192
0.581
RoBERTa
81.4
gradient×input
All
0.310
0.380
0.231
RoBERTa
100.0
gradient×input
Correct
0.303
0.383
0.229
RoBERTa
0.0
gradient×input
Wrong
0.324
0.375
0.236
RoBERTa
81.4
|gradient × input|
All
0.675
0.219
0.546
RoBERTa
100.0
|gradient × input|
Correct
0.707
0.201
0.583
RoBERTa
0.0
|gradient × input|
Wrong
0.605
0.259
0.463
SciBERT
86.8
Attention
All
0.739
0.203
0.577
SciBERT
100.0
Attention
Correct
0.830
0.138
0.707
SciBERT
0.0
Attention
Wrong
0.442
0.415
0.152
SciBERT
86.8
gradient×input
All
0.475
0.296
0.399
SciBERT
100.0
gradient×input
Correct
0.467
0.296
0.401
SciBERT
0.0
gradient×input
Wrong
0.501
0.295
0.392
SciBERT
86.8
|gradient × input|
All
0.670
0.214
0.557
SciBERT
100.0
|gradient × input|
Correct
0.698
0.197
0.591
SciBERT
0.0
|gradient × input|
Wrong
0.575
0.268
0.447
Table 4: Results for all three models for spelling error edits across all methods of selecting rationales (attention,
gradient×input and |gradient×input|) and all metrics to evaluate rationales (mean reciprocal ranking, AUPRC and
mean top 1 match).

Model
F1
Method
Classiﬁcation
Mean Recip.
AUPRC
Top 1 Match
BERT
74.0
Attention
All
0.561
0.272
0.411
BERT
100.0
Attention
Correct
0.680
0.211
0.528
BERT
0.0
Attention
Wrong
0.391
0.359
0.244
BERT
74.0
gradient×input
All
0.189
0.447
0.074
BERT
100.0
gradient×input
Correct
0.195
0.442
0.089
BERT
0.0
gradient×input
Wrong
0.181
0.454
0.054
BERT
74.0
|gradient × input|
All
0.306
0.416
0.130
BERT
100.0
|gradient × input|
Correct
0.354
0.399
0.166
BERT
0.0
|gradient × input|
Wrong
0.238
0.440
0.078
RoBERTa
74.8
Attention
All
0.598
0.263
0.426
RoBERTa
100.0
Attention
Correct
0.662
0.230
0.487
RoBERTa
0.0
Attention
Wrong
0.503
0.312
0.335
RoBERTa
74.8
gradient×input
All
0.168
0.456
0.058
RoBERTa
100.0
gradient×input
Correct
0.156
0.458
0.054
RoBERTa
0.0
gradient×input
Wrong
0.187
0.452
0.063
RoBERTa
74.8
|gradient × input|
All
0.321
0.404
0.154
RoBERTa
100.0
|gradient × input|
Correct
0.367
0.384
0.194
RoBERTa
0.0
|gradient × input|
Wrong
0.252
0.434
0.094
SciBERT
73.9
Attention
All
0.501
0.327
0.304
SciBERT
100.0
Attention
Correct
0.605
0.270
0.414
SciBERT
0.0
Attention
Wrong
0.353
0.406
0.148
SciBERT
73.9
gradient×input
All
0.226
0.426
0.111
SciBERT
100.0
gradient×input
Correct
0.260
0.410
0.145
SciBERT
0.0
gradient×input
Wrong
0.178
0.450
0.062
SciBERT
73.9
|gradient × input|
All
0.322
0.400
0.162
SciBERT
100.0
|gradient × input|
Correct
0.356
0.387
0.189
SciBERT
0.0
|gradient × input|
Wrong
0.275
0.418
0.123
Table 5: Results for all three models for deleted text edits across all methods of selecting rationales (attention,
gradient×input and |gradient×input|) and all metrics to evaluate rationales (mean reciprocal ranking, AUPRC and
mean top 1 match).

The algorithm <del>descripted</del><ins>described</ins> in the
previous sections has several advantages.
A spelling error edit; “descripted” is a spelling error and is corrected.
For each energy point, the thereby obtained cross-section values
and errors from different experiments have been further averaged
according to the <del>weigthed</del><ins>weighted</ins> average
method used by the Particle Data Group _CITE_, including error
rescaling by _MATH_ in case of large discrepancy.
A spelling error edit; “weigthed” is a spelling error and is corrected.
And the short notations for the denominators are _MATHDISP_,
Furthermore, the following relations are useful to
<del>short</del><ins>shortcut</ins> the expressions:
_MATHDISP_.
Not a spelling error edit; “short” is incorrect in this context, but it is not a spelling error.
However, we <del>must note that we </del>still have no means of
deciding which documents out of _MATH_ deserve to be in _MATH_ and
_MATH_, respectively.
A deleted text edit; text is only removed, while no text is inserted.
Let _MATH_ be the conjugate Holder<del>’s</del> function of a
Holder<del>’s</del> function _MATH_.
A deleted text edit; text is only removed, while no text is inserted.
Figure 4: Additional examples of the two types of edits extracted from the original AESW dataset.

