Image2Point: 3D Point-Cloud Understanding with 2D
Image Pretrained Models
Chenfeng Xu 1∗
Shijia Yang 1∗
Tomer Galanti 2
Bichen Wu 3†
Xiangyu Yue 1
Bohan Zhai 1
Wei Zhan 1
Peter Vajda 3
Kurt Keutzer 1
Masayoshi Tomizuka 1
1 University of California, Berkeley, 2 Massachusetts Institute of Technology, 3 Meta Reality Labs
{xuchenfeng, shijiayang, xyyue, zhaibohan, wzhan, keutzer, tomizuka}@berkeley.edu,
galanti@mit.edu, {wbc, vajdap}@fb.com
Abstract
3D point-clouds and 2D images are different visual representations of the physical
world. While human vision can understand both representations, computer vision
models designed for 2D image and 3D point-cloud understanding are quite different.
Our paper explores the potential of transferring 2D model architectures and weights
to understand 3D point-clouds, by empirically investigating the feasibility of the
transfer, the benefits of the transfer, and shedding light on why the transfer works.
We discover that we can indeed use the same architecture and pretrained weights
of a neural net model to understand both images and point-clouds. Specifically,
we transfer the image-pretrained model to a point-cloud model by copying or
inflating the weights. We find that finetuning the transformed image-pretrained
models (FIP) with minimal efforts — only on input, output, and normalization
layers — can achieve competitive performance on 3D point-cloud classification,
beating a wide range of point-cloud models that adopt task-specific architectures
and use a variety of tricks. When finetuning the whole model, the performance
improves even further. Meanwhile, FIP improves data efficiency, reaching up
to 10.0 top-1 accuracy percent on few-shot classification. It also speeds up the
training of point-cloud models by up to 11.1x for a target accuracy (e.g., 90
% accuracy). Lastly, we provide an explanation of the image to point-cloud
transfer from the aspect of neural collapse. The code is available at: https:
//github.com/chenfengxu714/image2point.
1
Introduction
Point-cloud is an important visual representation for 3D computer vision. It is widely used in a
variety of applications, including autonomous driving [1, 2, 3], robotics [4, 5, 6], augmented and
virtual reality [7, 8, 9], etc. However, a point-cloud represents visual information in a significantly
different way from a 2D image. Specifically, a point-cloud consists of a set of unordered points lying
on the object’s surface, with each point encoding its spatial x, y, z coordinates and potentially other
∗Equal contribution
†Corresponding author
Preprint. Under review.

features such as intensity. In contrast, a 2D image organizes visual features as a dense 2D RGB
pixel array. Due to the representation differences, 2D image and 3D point-cloud understanding are
treated as two separate problems. 2D image models and point-cloud models are designed to have
different architectures and are trained on different types of data. No efforts have tried to directly
transfer models from images to point-clouds.
Intuitively, both 3D point-clouds and 2D images are visual representations of the physical world.
Their low-level representations are drastically different, but they can represent the same underlying
visual concept. Furthermore, human vision has no problem understanding both representations.
To connect images and point-clouds, previous works attempted to generate pseudo point-clouds
by estimating the depth of mono/stereo images [10, 11, 12]. However, depth estimation from
a single image is a challenging problem in computer vision, which requires large-scale dense
depth labels [13]. Estimating depth from stereo images is easier but requires strict calibrated and
synchronized stereo cameras, which limits the data scale. Therefore, it is interesting to ask whether
we could use large-scale image models that were pretrained using supervised classification datasets
(e.g., ImageNet1K/ImageNet21K classification) for point-cloud understanding.
Remarkably, the answer to the question above is positive. As we show in this work, 2D image models
trained on image datasets can be transferred to understand 3D point-clouds with minimal effort. As
illustrated in Figure 1, given the commonly-used image-pretrained models, such as 2D ConvNets
[14] and vision transformers [15], we can easily convert them into various kinds of point-cloud
models. In particular, a pretrained 2D ConvNet and vision transformer can be easily extended into
projection-based, voxel-based, and transformer-based point-cloud models via copying weights or
inflating weights [16].
In this paper, we primarily focus on 3D ConvNets inflated from 2D pre-trained models. With the
transformed point-cloud model (e.g., inflated 3D ConvNets), we add linear input and output layers to
the network; and on a target point-cloud dataset, we only finetune the input and output layers, and
batch normalization layers, while keeping the pretrained model weights untouched. We call such
partially-finetuned-image-pretrained models as FIP-IO+BN (finetuning input, output, and BN layers).
As we show, FIP-IO+BN can achieve competitive performance up to 90.8% top-1 accuracy on the
ModelNet 3D Warehouse dataset, on top of ResNet50, outperforming previous point-cloud models
that adopt task-specific model architectures and tricks.
Even though incorporating pretrained models is useful for tackling downstream tasks, point-cloud
models are typically trained from scratch. Based on our discovery, we further investigate fully-
finetuned-image-pretrained models (termed as FIP-ALL). We observe that FIP-ALL brings significant
improvement on top of different kinds of point-cloud models transformed from image-pretrained
models. Besides, we also find that it generalizes to PointNet++ [17] which is pre-trained on images
by ourselves. Specifically, FIP-ALL outperforms the training-from-scratch by a large margin on top
of PointNet++, SimpleView, ViT-B-16, and ViT-L-16, respectively. In addition to the performance
gain, FIP-ALL exhibits superior data efficiency with up to 10.0% accuracy improvement in few-shot
classification on the ModelNet 3D Warehouse dataset. Compared with training-from-scratch, FIP-
ALL also dramatically speeds up the training by using 11.1 times fewer epochs to reach the same
validation accuracy (e.g., 90% accuracy).
Finally, we theoretically explore the relationship between transferring knowledge between tasks of
different modalities and neural collapse to shed light on why the transfer works. The analysis is based
on extending the framework proposed in [18] and is provided in Appendix C.
2
Related Work
2.1
Point-Cloud Processing Models
In this section, we list the most prominent approaches for processing point-clouds.
The 3D convolution-based method is one of the mainstream point-cloud processing approaches
which efficiently processes point-clouds based on voxelization. In this approach, voxelization is used
to rasterize point-clouds into regular grids (called voxels). Then, we can apply 3D convolutions to
the processed point-cloud. However, enamors empty voxels make lots of unnecessary computations.
Sparse convolution is proposed to apply on the non-empty voxels [19, 20, 21, 22, 23, 24], largely
improving the efficiency of 3D convolutions.
2

Classifier
Input Layer
Input Layer
Input Layer
Input Layer
Classifier
Decoder
Decoder
Car
Chair
Copy
Conv2D
Norm
Conv2D
Norm
Copy
Inflation
Transformer
2D ConvNets
Projection-based
Point-cloud model
Conv2D
Norm
Conv3D
Norm
Copy
2D ConvNets
Voxel-based
Point-cloud model
Transformer
Copy
Transformer
Point-cloud model
Vision Transformers
Figure 1: We investigate the feasibility of converting pretrained 2D image models to 3D point-cloud
models. For example, with filter inflation and finetuning the input, output (classifier for classification
task and decoder for semantic segmentation task), and normalization layers, the transformed 2D
ConvNets are able to deal with point-cloud classification, indoor, and driving scene segmentation.
The projection-based method attempts to project a 3D point-cloud to a 2D plane and uses 2D
convolution to extract features [25, 26, 27, 28, 29, 30, 31]. Specifically, bird-eye-view projection [32,
33] and spherical projection [26, 27, 28, 34] have made great progress in outdoor point-cloud tasks.
Another approach is the point-based method, which directly processes the point-cloud data. The
most classic methods, PointNet [35] and PointNet++ [17], consume points by sampling the center
points, group the nearby points, and aggregate the local features. Many works further develop
advanced local-feature aggregation operators that mimic the 3D convolution operation to structured
data [6, 36, 37, 38, 39, 40, 41, 42].
2.2
Pretraining in 2D and 3D Computer Vision
Pretraining in 2D computer vision is an effective approach using supervised [15, 43], self-
supervised [44, 45], and contrastive learning [46, 47, 48, 49, 50, 51]. After pretraining on a
large amount of data, a 2D model requires less computational and data resources for finetuning in
order to obtain competitive performance on downstream tasks [52, 53, 54, 55].
Pretraining in 3D computer vision has been studied similarly as pretraining in 2D vision: both
self-supervised and contrastive pretraining [56, 57, 58] show promising results. 3D point-clouds are
difficult to annotate, and there is no large-scale annotated dataset available. To address this, previous
works have tried to use model pretraining to improve data efficiency [59]. Recent works [60, 61]
explored using contrastive learning on point-clouds. Our work does not rely on long-time pretraining.
Instead, we can directly take large amounts of open-sourced image-pretrained models for a variety of
point-cloud tasks.
2.3
Cross-Modal Transfer Learning
Cross-modal transfer learning takes advantage of data from various modalities [62, 63]. For
example, [64] proposed pixel-to-point knowledge transfer (PPKT) from 2D to 3D which uses aligned
RGB and RGB-D images during pretraining. Our work does not rely on joint image-point-cloud
pretraining. Instead, we directly transfer an image-pretrained model to a point-cloud model with the
simplest pretraining-finetuning scheme.
Some of the previous works for video and medical images [16, 65] have adopted the method of
simply extending a pretrained 2D convolutional filter along time or depth direction for transferring to
3D models. However, the domain gaps between point-clouds and images are much more than that of
videos/medical images and images. Between language and image modalities, transfer learning with
minimal finetuning also shows a competitive performance [66, 67].
3

2.4
Neural Collapse
Neural collapse (NC) [68, 69] is a recently discovered phenomenon in deep learning. It has been
observed that during the training of deep overparameterized neural networks for standard classification
tasks, the penultimate layer’s features associated with training samples belonging to the same class
concentrate around their class means. Essentially, [68] observed that the ratio of the within-class
variances and the distances between the class means converge to zero. In addition to that, it has
also been observed that asymptotically the class means (centered at their global mean) are not only
linearly separable, but are also maximally distant and located on a sphere centered at the origin up
to scaling, and furthermore, that the behavior of the last-layer classifier (operating on the features)
converges to that of the nearest-class-mean decision rule.
Recently, [18] studied the relationship between neural collapse and transfer learning. They studied
a transfer learning setting, where we intend to solve a target (classification) task, where only a limited
amount of samples is available, so a model is pretrained and transferred from a source (classification)
task. They showed that neural collapse extends beyond training and generalizes also to unseen test
samples and new classes. In addition, it was shown that in the presence of neural collapse in the
new classes, training a linear classifier on top of the learned penultimate layer requires only a few
samples to generalize well. However, their empirical and theoretical analysis assumes that the source
and target classes are i.i.d. samples (e.g., a random split of the classes in ImageNet). This implies
that the two tasks share the same modality. Therefore, we suggest training an adaptor (e.g., a linear
layer) along with retraining the normalization parameters as part of the transfer process. Intuitively,
the adaptor takes samples of the second modality and translates them to representations that are
interpretable by the pretrained model, such that it produces feature embeddings that are clustered
into classes. In Appendix C, we extend the framework in [18] to the case where the source and target
tasks are of different modalities and theoretically analyze it.
3
Converting a 2D Image Model to a 3D Point-Cloud Model
In this paper, we primarily focus on the 3D sparse-convolution-based method to process point-clouds,
since it can be extended to a wide range of point-cloud tasks. The other point-cloud models we use in
this paper are byproducts of copying the weights of 2D image models, for example, 2D ConvNets [14]
or vision transformers [15]. In this section, we provide an in-depth introduction to how we transform
the 2D ConvNets into 3D sparse ConvNets by inflation [16].
Inflating a 2D ConvNet into a 3D sparse ConvNet.
As discussed in Section 2.1, we consider a
set of points, where each point is represented by its 3D coordinates and additional features such as its
intensity and RGB. We then voxelize/quantize these points into voxels according to their 3D space
coordinates, following [20]. A voxel’s feature is inherited from the point that lies within the voxel. If
there are multiple points associated with the same voxel, we average all points’ features and assign
the mean to the voxel. If there is no point in the voxel, then we simply set the voxel’s feature to 0.
With sparse convolution, the computation on empty voxels can be skipped.
Given a pretrained 2D ConvNet, we convert it to a 3D ConvNet that takes 3D voxels as input. The
key element of this procedure is to convert 2D convolution filters to 3D, i.e., constructing 3D filters
with the weights directly inherited from 2D filters. A 2D convolutional filter can be represented
with a 4D tensor of shape [M, N, K, K], representing output dimension, input dimension, and two
spatial kernel sizes, respectively. A 3D convolutional filter has an extra dimension, and its shape is
[M, N, K, K, K]. To better illustrate, we ignore the output and input dimensions and only consider a
spatial slice of the 2D filter with shape [K, K]. The simplest way to convert this 2D filter to 3D is
to repeat the 2D filter K times along a third dimension. This operation is the same as the inflation
technique used by [16] to initialize a video model with a pretrained 2D ConvNet.
Besides convolution, other operations such as downsampling, BN, and nonlinear activation can be
easily migrated to 3D. Our 3D model inherits the architecture of the original 2D ConvNet, but we also
add a linear layer as the input layer and an output layer depending on the target task. For classification,
we use a global average pooling layer followed by one fully connected layer to get the final prediction.
For semantic segmentation, the output layer is a U-Net style decoder [70]. The architecture of the
input/output layers is described in more detail in Appendix B.6.
4

A note on image-to-video transfer.
It is noteworthy to mention that although inflation is commonly
used in video domains, image-to-point-cloud transfer is fundamentally different from image-to-video
transfer. Even though videos and point-clouds are both 3D data, they are represented with completely
different visual modalities with different distributions. Intrinsically, 3D point-clouds are represented
as a sparse set of points lying on object surfaces and parameterized by xyz-coordinates, while videos
are dense RGB arrays, where the two spatial arrays represent RGB images and the temporal array
reflects how images evolve through time. Point-clouds are translation and rotation invariant or
equi-variant, while for videos, the spatial and temporal dimensions are not interchangeable. In this
paper, we surprisingly find that with simple operations such as inflation, the image-pretrained models
can be directly used for point-cloud understanding under the situation that image and point-cloud are
drastically different. The detailed experiments showing the feasibility and utility, and the discussion of
why it works from the aspect of neural collapse are illustrated in Section 4 and Section 5, respectively.
4
Empirical Evaluation
To explore the image to point-cloud transfer, we study three settings: , (1) finetuning input, output,
and batch normalization layers (FIP-IO+BN), (2) finetuning the whole pretrained network (FIP-ALL),
and optionally (3) partially-finetuned-image-pretrained model, only finetuning input and output layers
(FIP-IO). Under the three settings, we extensively explore the feasibility of transferring the image-
pretrained model for point-cloud understanding and its benefits. The entire empirical evaluation
is organized as four questions: (1) Can we transfer pretrained-image models to recognize point-
clouds? (Section 4.1) (2) Can image-pretraining benefit the performance of point-cloud recognition?
(Section 4.2) (3) Can image-pretrained models improve the data efficiency on point-cloud recognition?
(Section 4.3) (4) Can image-pretrained models accelerate training point-cloud models? (Section 4.4)
Datasets.
We evaluate the transferred models on ModelNet 3D Warehouse classification [8], S3DIS
indoor segmentation [4], and SemanticKITTI outdoor segmentation [1] tasks. ModelNet 3D Ware-
house is a CAD model classification dataset that consists of point-clouds with 40 categories. CAD
models in this benchmark come from 3D Warehouse [7]. In this benchmark, we only utilize x, y, z
coordinates as features. S3DIS is a dataset collected from real-world indoor scenes and includes 3D
scans of Matterport Scanners from 6 areas. It provides point-wise annotations for indoor objects like
chair, table, and bookshelf, etc. SemanticKITTI dataset from KITTI Vision Odometry [71] is a driving
scene dataset. It provides dense point-wise annotations for the complete 360 degrees field-of-view of
the deployed automotive lidar, which is currently one of the most challenging datasets.
ResNet [14] series is used mostly throughout our experiments. Depending on the experiments,
ResNets are pretrained on Tiny-ImageNet, ImageNet-1K, ImageNet-21K [72], and Fractal database
(FractalDB) [52]. Our pretrained models are directly downloaded from various sources, with detailed
links provided in the Appendix A. To study the benefits of using pretrained image models, we also
utilize PointNet++ [17], ViT [15], and SimpleView [73] as our baselines.
4.1
Can we transfer pretrained-image models to recognize point-clouds?
To evaluate the feasibility of transferring pretrained 2D image models to 3D point-cloud tasks, we
conduct experiments on top of the ResNet series since there are abundant open-source pretrained
ResNet available. In particular, we convert 2D ConvNets into 3D ConvNets using the procedure
described in Section 3. We hypothesize that, if a pretrained 2D image model is capable of understand-
ing point-clouds directly, we can see a non-trivial performance by only finetuning input and output
layers of the transferred model. Further, as we gradually relax the frozen parameters, finetuning
BN parameters as well, the transferred model can achieve better performance, even surpassing
training-from-scratch.
We conduct two groups of experiments with FIP-IO and FIP-IO+BN, with the results shown in
Figure 2. The first is to evaluate the performance as the trainable parameters gradually increase. As
shown in Figure 2 (a), training no more than 0.3 % (345.5x fewer) of the whole parameters, the
image pretraining even beats the training-from-scratch (100 % trainable parameters). Specifically,
ResNet152 FIP-IO+BN with ImageNet1K pretraining improves training-from-scratch by 0.16 points,
and ResNet50 FIP-IO+BN with ImageNet21K pretraining improves 0.48 points. Meanwhile, FIP-IO
reaches a non-trivial performance. ResNet50 FIP-IO pretrained on ImageNet1K achieves 81.20 %
5

//
FIP-IO (71.03)
FIP-IO+BN (88.75)
FIP-IO (81.20)
FIP-IO (73.74)
FIP-IO+BN (90.44)
FIP-IO+BN (90.80)
FIP-IO (64.63)
FIP-IO+BN (89.87)
60
65
70
75
80
85
90
95
0
0.001
0.002
0.003
0.004
ResNet18 90.39
ResNet50 90.32
ResNet152 90.28
1
345.5 x 
ResNet18 on ImageNet1K
ResNet50 on ImageNet1K
ResNet50 on ImageNet21K
ResNet152 on ImageNet1K
Train from scratch
Trainable parameters/Total parameters
Top 1 Accuracy
77
77.6
89.2
90.7
90.7
90.1
90.6
90.316
90.8
83.35
65
70
75
80
85
90
3DShapeNets
DeepPano
PointNet
PointNet++
DGCNN
MVCNN
KDNet
ResNet50 (baseline)
ResNet50 FIP-IO
ResNet50 FIP-IO+BN
Train from scratch
Tiny-ImageNet pretrain
ImageNet1K pretrain
ImageNet21K pretrain
FractalDB1K pretrain
FractalDB10K pretrain
(a) Trainable Parameters vs. Accuracy
(b) Pretrained Dataset 
Figure 2: a) the left figure shows the trainable parameters ratio w.r.t top-1 accuracy on ModelNet 3D
Warehouse dataset. b) the right figure shows the performance of FIP-IO and FIP-IO+BN on top of
ResNet50 pretrained on different datasets.
Table 1: ModelNet 3D Warehouse classification results (top-1 accuracy %) of fully-finetuned-image-
pretrained models (FIP-ALL) based on different pretrained models. We include 2021 SOTAs, such
as RSMix [75], Point Transformer (Point-Trans) [76], DRNet [77], and PointCutMix [78], for
comparison.
Method
ResNet18
ResNet50
ResNet152
ResNet101×2
From Scratch
90.39
90.32
90.28
90.03
FIP-ALL on ImageNet1K
90.52 (+0.13)
90.92 (+0.60)
91.09 (+0.81)
90.52 (+0.49)
FIP-ALL on ImageNet21K
-
91.05 (+0.73)
-
-
Method
PointNet++(SSG)
ViT-B-16
ViT-L-16
SimpleView
From Scratch
90.34
84.27
83.48
93.3
FIP-ALL on ImageNet1K
91.22 (+0.88)
-
-
93.8 (+0.50)
FIP-ALL on ImageNet21K
-
87.77 (+3.50)
87.66 (+4.18)
-
Method
RSMix
Point-Trans
DRNet
PointCutMix
From Scratch
93.5
93.7
93.1
93.4
top-1 accuracy, only 9.12 points worse than training-from-scratch with approximately 0.1 % trainable
parameters.
Furthermore, to investigate the effect of different datasets, as shown in the right figure of Figure 2, we
inflate ResNet50 pretrained from different image datasets, including Tiny-ImageNet, ImageNet1K,
ImageNet21K, FractalDB1K, and FractalDB10K, then evaluate on the ModelNet 3D Warehouse.
We discover that, even if we only finetune the input and output layers while keeping the image-
pretrained weights frozen, the FIP-IO pretrained from ImageNet1K, FractalDB1K, and FractalDB10K
achieves competitive performance. Specifically, ResNet50 FIP-IO with ImageNet1K pretraining
outperforms 3D ShapeNet [8] and DeepPano [9], which were the state-of-the-arts in 2015, by 4.2
and 3.6 points respectively in top-1 accuracy on ModelNet 3D Warehouse. More importantly,
with ImageNet21K pretrained model, ResNet50 FIP-IO+BN surpasses training-from-scratch by
0.48 points, even beating a variety of well-known methods including PointNet [35], MVCNN [29],
DGCNN [74], etc.
Notably, we find out the answer to "Can we transfer pretrained-image models to recognize point-
clouds?": Yes. The pretrained 2D image models can be directly used for recognizing point-clouds.
Surprisingly, the pretraining dataset is not restricted to natural but also synthetic images like those in
FractalDB1K/10K.
6

Table 2: Indoor scene and outdoor scene segmentation results (mIoU %) of fully-finetuned-image-
pretrained Model (FIP-ALL). In this table, all image-pretrained models are pretrained on ImageNet1K.
Method
S3DIS (mIoU %)
SemanticKITTI (mIoU %)
PointNet++(SSG)
ResNet18
HRNetV2-W48
ResNet18
From Scratch
52.45
55.09
44.12
64.75
FIP-ALL on ImageNet1K
55.01 (+2.56)
56.62 (+1.53)
47.53 (+3.41)
65.57 (+0.82)
Table 3: Comparison with PointContrast [56] on the ModelNet 3D Warehouse. PointContrast
provides two different pretrained models with using PointInfoNCE loss and Hardest Contrastive loss,
respectively.
From scratch
PointInfoNCE
Hardest Contrastive
ImageNet1K pretrain (Ours)
89.95
90.24 (+0.29)
90.15 (+0.20)
90.88 (+0.93)
4.2
Can image-pretraining benefit point-cloud recognition?
From the previous subsection, we find unexpectedly that the image-pretrained model can be directly
used for point-cloud understanding. In this subsection, we investigate whether the image-pretrained
model is helpful to improve the performance of point-cloud tasks. We use different baselines, includ-
ing voxelization-based method (simply ResNet), point-based method (PointNet++ [17]), projection-
based method (SimpleView [73]), and current popular transformer-based method (ViT-B-16 and
ViT-L-16 [15]), and fully finetune them on three point-cloud datasets: classification on ModelNet 3D
Warehouse, indoor scene segmentation on S3DIS, and outdoor scene segmentation on SemanticKITTI,
as shown in Table 1 and Table 2.
For PointNet++, we use ImageNet1K to pretrain: we break each image into pixels and regard it as a
point-cloud. For ViT, we directly use the open-source pretrained model and finetune it on ModelNet
3D Warehouse. All the implementation details are illustrated in Appendix A.
Table 1 presents performance on ModelNet 3D Warehouse dataset. We observe that FIP-ALL
improves all baselines steadily and significantly. Besides, pretraining brings more improvements
to deeper models. For example, ResNet18 can only be improved by 0.13% top-1 accuracy, but
pretraining on ImageNet1K leads to 0.81 points top-1 accuracy improvement on top of ResNet152.
Moreover, larger pretrained datasets also lead to better performance. Specifically, ResNet50 FIP-ALL
from ImageNet21K can reach 91.05% top-1 acc, with 0.73 points improvement over training-from-
scratch. Such FIP-ALL significantly outperforms a series of well-known methods such as [35, 17, 79,
74, 29, 41].
We also explore FIP-ALL on different architectures, as shown in the second group of Table 1.
In particular, FIP-ALL on top of PointNet++, ViT-B-16, ViT-L-16, and SimpleView with image
dataset pretraining improve the training-from-scratch by 0.88, 3.50, 4.18, 0.50 points, respectively.
Especially for the current superior baseline in image recognition, ViT-B-16 and ViT-L-16, the
improved performance is quite significant, revealing the huge potential of using image-pretrained
models for point cloud recognition.
For the challenging indoor and outdoor scene segmentation, using ImageNet1K pretrained models
(FIP-ALL on ImageNet1K) also improve the training-from-scratch consistently, as shown in Table 2.
PointNet++ (resp. ResNet18) pretrained on ImageNet1K outperforms the training-from-scratch by
2.56 points (resp. 1.53 points) mIoU on S3DIS dataset. For SemanticKITTI, we utilize the commonly
used projection-based method with 2D ConvNet HRNet. With ImageNet1K pretraining, we observe
3.41 points mIoU improvement, a large margin in such a challenging task. Since HRNetV2-W48 has
rich pretrained models, we finetune Cityscapes pretrained HRNetV2-W48 and observe this enhances
more (5.25% mIoU improvement over training from scratch). Even for the ResNet18 with a high
from-scratch performance of 64.75% mIoU, the ImageNet1K pretraining can also bring 0.82 points
mIoU improvement.
Finally, we compare the performance gain with the well-known point-cloud self-supervised method
PointContrast [56], as presented in Table 3. We use the same model architecture and finetuning
7

Table 4: Few-shot experiments on top of different ResNets on the ModelNet 3D Warehouse dataset.
We conduct 3 trials for each setting and results are as mean ± std.
Few-shot
ResNet18
ResNet50
ResNet152
(from scratch/FIP-ALL)
10-shot
72.2±0.8/73.2±0.6 (+1.0)
71.7±0.7/74.1±0.8 (+2.4)
69.8±1.1/73.9±0.4 (+4.1)
5-shot
63.7±1.6/66.6±0.8 (+2.9)
62.4±1.1/66.0±2.2 (+3.6)
59.4±0.8/66.5±0.9 (+7.1)
1-shot
26.8±4.4/36.8±0.6 (+10.0)
28.1±0.4/34.1±0.2 (+6.0)
23.3±4.3/33.2±1.3 (+9.9)
Table 5: Semi-supervised distillation experiments on top of ResNet34 on the ModelNet 3D Warehouse
dataset.
Few-shot
From scratch
PointInfoNCE
Hardest
Contrastive
ImageNet1K
pretrain (Ours)
10-shot
72.2
74.6 (+2.4)
74.6 (+2.4)
74.9 (+2.7)
5-shot
61.9
65.1 (+3.2)
65.9 (+4.0)
66.0 (+4.1)
1-shot
29.2
39.0 (+9.8)
37.2 (+8.0)
41.1 (+11.9)
recipe, and the only difference is the pretraining weights. Note that the model architecture used in
PointContrast does not have corresponding open-sourced image-pretrained weights, so we pretrain it
by ourselves on ImageNet1K, with the standard ImageNet training recipe provided by Pytorch. We
can observe that image-pretraining on ImageNet1K significantly boosts the training-from-scratch by
0.93 points, surpassing the PointContrast by at least 0.64 points.
Therefore, the answer to "Can image-pretraining benefit point-cloud recognition" is: Yes. Image-
pretraining can indeed improve point-cloud recognition, which can generalize to a wide range of
backbones and benefit a variety of challenging tasks.
4.3
Can image-pretrained models improve the data efficiency on point-cloud recognition?
Data efficiency is extremely important in point-cloud understanding due to the huge labor of collecting
and annotating point-cloud data. In this subsection, we investigate whether the image-pretrained
model can help to improve the data efficiency by conducting few-shot setting experiments, including
1-shot, 5-shot, and 10-shot.
In detail, for each class (ModelNet 3D Warehouse involves 40 classes), we randomly choose a few
point-clouds as training data and still evaluate on the whole test set. We compare the results between
training-from-scratch and FIP-ALL pretrained on the ImageNet1K dataset. The experimental results
are shown in Table 4. We observe that FIP-ALL dramatically surpasses training-from-scratch on the
low data regime (1-shot): pretraining on ImageNet1K brings 10.0, 6.0, and 9.9 points top-1 accuracy
improvement for ResNet18, ResNet50, and ResNet152, respectively. For 5-shot and 10-shot settings,
using ImageNet1K pretraining can still consistently improve the performance.
Furthermore, inspired by previous work [54] which proposed big self-supervised models are strong
semi-supervised learners in 2D image recognition, we borrow the idea and propose an image-
pretrained model is also a strong semi-supervised learner in point-cloud recognition. We also
compared the image-pretrained model with the self-supervised pretrained model in this experiment.
Specifically, we first take pretrained models from the previous self-supervised pretraining method
PointContrast [56]. PointContrast provides two ScanNet [80] pretrained models of architecture
ResNet34 trained with hardest-contrastive loss and PointInfoNCE loss. Then, we finetune PointCon-
trast on 1/5/10 shot of the labeled ModelNet 3D Warehouse dataset and regard it as a teacher model.
Finally, we distill the teacher model to a randomly initialized student model. In detail, we pass in
the rest of unlabeled ModelNet 3D Warehouse dataset and 1/5/10 shot of the labeled dataset into
the teacher model to generate pseudo labels. We use softmax MSE loss as consistency loss between
student model outputs and pseudo labels. When the data instance is labeled, we add an additional
cross entropy loss as a class criterion between student output and the label.
To show the effectiveness of the image-pretrained model, we repeat the above experiment, only
replacing self-supervised pretrained models with ResNet34 ImageNet1K pretrained models. Results
8

70
75
80
85
90
95
1
11
21
31
41
51
61
71
81
91 101 111 121
Validation (top 1 accuracy) vs. Epoch
ResNet 152 train from scratch
ResNet 152 FIP-ALL on ImageNet1K
70
75
80
85
90
95
1
11
21
31
41
51
61
71
81
91 101 111 121
Validation (top 1 accuracy) vs. Epoch
ResNet 18 train from scratch
ResNet 18 FIP-ALL on ImageNet1K
19 epoch
56 epoch
1th epoch: 79.34
1th epoch: 13.94
70
75
80
85
90
95
1
11
21
31
41
51
61
71
81
91 101 111 121
Validation (top 1 accuracy) vs. Epoch
ResNet 50 train from scratch
ResNet 50 FIP-ALL on ImageNet1K
1th epoch: 81.65
1th epoch: 70.22
28 epoch
60 epoch
1th epoch: 80.11
1th epoch: 28.48
11 epoch
122 epoch
Epoch
Epoch
Epoch
Figure 3: Comparing the validation accuracy of training-from-scratch and FIP-ALL on Mod-
elNet 3D. We report the validation accuracy during training. We compare the results between
training-from-scratch and fine-tuning the whole network (FIP-ALL) with pretraining ResNet18,
ResNet50, and ResNet152, on the ImageNet1K dataset.
are reported in Table 5. We observe that image-pretrained ResNet34 consistently outperforms
PointContrast, and improves the baseline by a large margin with 11.9, 4.1, and 2.7 points on 1-shot,
5-shot, and 10-shot, respectively. The results in Table 5 show that an image-pretrained model is
indeed a strong semi-supervised learner in point-cloud recognition.
However, in both Table 4 and Table 5, we observe that as the amount of training data increases,
the performance gain becomes saturated. Therefore, our answer to "Can image-pretrained models
improve the data efficiency on point-cloud recognition?" is: Yes. Image-pretrained models can
improve the data efficiency on point-cloud recognition, especially on low data regime. When the
training data increases, performance still improves, but the gain becomes marginal.
4.4
Can image-pretrained models accelerate point-cloud training?
We also investigate whether the image-pretrained model can accelerate training on the point-cloud
domains. The results are shown in Fig. 3.
We discover that, after training only one epoch on ModelNet 3D Warehouse dataset, FIP-ALL
pretrained on ImageNet1K achieves very impressive performance, yet the performance of training-
from-scratch is still very low. For instance, after the first epoch, ResNet50 (resp. ResNet152)
with training from scratch achieves 28.48% (resp. 13.94%) top-1 accuracy while ResNet50 (resp.
ResNet152) with ImageNet1K pretraining reaches 80.11% (resp. 79.34%) top-1 accuracy. Moreover,
to reach 90% top-1 accuracy, a non-trivial performance, FIP-ALL significantly accelerates the
training by 2.14x (28 vs. 60 epoch), 11.1x (11 vs. 122 epoch), 2.95x (19 vs. 56 epoch) over
training-from-scratch, on top of ResNet18, ResNet50, and ResNet152, respectively.
Therefore, our answer to “Can image-pretrained models accelerate point-cloud training?” is still
positive. The image-pretrained models can significantly accelerate the training speed of point-cloud
tasks.
5
Neural Collapse in Cross-Modal Transfer
In this section, we provide an explanation of why the image to point-cloud transfer works based on the
recently observed phenomenon called neural collapse [69, 68]. [18] in depth studied the relationship
between neural collapse and transfer learning between two classification tasks of the same modality
(image domain). Similar to this work, we focus on transferring pretrained models between domains
of different modalities, i.e., from images to point-clouds.
As illustrated in Section 4, we can transfer models that were pretrained on images to the point-cloud
domain. This motivates us to question whether the phenomenon of neural collapse generalization [18]
(see Section 2.4) is also evident in our case. Following [18], we explore the relationships between
neural collapse and image-to-point transfer by calculating the class-distance normalized variance
(CDNV). Informally, the CDNV measures the ratio between the within-class variances of the embed-
dings and the squared distance of their means (see Appendix B.5 for details). We measure the CDNV
of the fine-tuned model on both train and test data of the point-cloud domain. Since neural collapse
9

From scratch
Top-1 Acc: 90.32  
Training CDNV: 0.37
Validation CDNV: 0.43
FIO-IO+BN on ImageNet1K
Top-1 Acc: 89.90 
Training CDNV: 0.71
Validation CDNV: 0.68
FIO-IO+BN on ImageNet21K
Top-1 Acc: 90.80 
Training CDNV: 0.47
Validation CDNV: 0.60
Figure 4: tSNE visualization and class-distance normalized variance on ModelNet 3D Wharehouse
dataset. FIP-IO+BN on ImageNet1K/21K are the same models in Fig. 2. CDNV is computed for the
fine-tuned model on the train and test data of the point-cloud domain.
is essentially a clustering property of features learned by neural networks, we further examine the
neural collapse using tSNE visualizations. The results are summarized in Fig. 4.
We observe that with finetuning much fewer (345.5x fewer) parameters in ResNet50 pretrained on
ImageNet1K, both class-distance-normalized-variance and the clustering of tSNE are worse than
training-from scratch, but still show relatively obvious clustering phenomenon. However, when we
use the ResNet50 pretrained on ImageNet21K, the top-1 accuracy, and CDNV are significantly im-
proved. More importantly, CDNV of ImageNet1K pretrained ResNet50 and ImageNet21K pretrained
ResNet50 is lower than 1. This observation indicates although the image domain and point-cloud
domain are quite different, the phenomenon of neural collapse generalization [18] still exists in their
transfer. More results and analysis are illustrated in Appendix B.5.
Moreover, the interesting discovery pushes us to think about the reason of cross-modal transfer
having neural collapse. Inspired by [18], we briefly explain below. More detailed theoretical proof is
presented in Appendix C.
Theoretical idea.
In this work we focused on the problem of transferring knowledge between two
tasks (source and target) consisting of two different modalities with different classes. Therefore, in
the theoretical analysis, we have to deal with two separate modes of generalization: between classes
and between modalities. In order to model this problem, we assume that the target and source tasks
are decomposed of i.i.d. classes that are samples of two different distributions D1 and D2 (each stands
for a different domain/modality). Each class is defined by a distribution over samples (e.g., samples of
dog images). Given a target task (consisting of a set of randomly selected classes P1, . . . , Pk ∼D1),
the pretrained model is evaluated after training an adaptor and a linear classifier on top of it. Its
overall performance is measured in expectation over the selection of target tasks.
To capture the similarity between the two domains, we assume there exists an invertible mapping F
between the classes that preserves the density of the two distributions, namely, ˆPc = F(Pc) ∼D2
for Pc ∼D1. To characterize the similarity between the classes coming from D1 and D2, we further
assume that the classes Pc and ˆPc share a ‘mutual representation space’ from which the class label
can be recovered. The shared space is given by two simple functions g∗and ˜g∗for which the distance
between g∗◦Pc and ˜g∗◦ˆPc is small (in expectation over Pc ∼D1). By utilizing tools from the theory
of Unsupervised Domain Adaptation [81, 82, 83], we translate the performance of a pretrained model
on randomly selected target tasks into its expected error on randomly selected tasks with classes from
D2. Then, in order to bound this error, we use Proposition 5 in [18] that relates the error and the
degree of neural collapse of the pretrained model on randomly selected classes from D2. Finally,
according to Propositions 1 and 2 in [18], this quantity can be upper bounded by the degree of neural
collapse of the pretrained model on the source train data.
10

6
Conclusions
In this work, we use finetuned-image-pretrained models (FIP) to explore the feasibility of transferring
image-pretrained models for point-cloud understanding and the benefits of using image-pretrained
models on point-cloud tasks. We surprisingly discover that, with simply transforming a 2D pretrained
ConvNet and minimal finetuning — input, output, and batch normalization layer (FIP-IO or FIP-
IO+BN), FIP can achieve very competitive performance on 3D point-cloud classification, beating
a wide range of point-cloud models that adopt a variety of tricks. Moreover, we find that when
finetuning all the parameters of the pretrained models (FIP-ALL), the performance can be significantly
improved on point-cloud classification, indoor and outdoor scene segmentation. Fully finetuned
models generalize to most of the popular point-cloud methods. We also find that FIP-ALL can
improve the data efficiency on few-shot learning and accelerate the training speed by a large margin.
Additionally, we explore the relationships between neural collapse and cross modal transferring for
our case, and shed light on why it works based on neural collapse. Compared with previous works
that seek improvements from designing architectures and pretraining only on point-cloud modality,
our work is not limited by the architecture design and the small-scale point-cloud dataset. We believe
that image pretraining is one of the solutions to the bottleneck of point-cloud understanding and hope
this direction can inspire the research community.
Acknowledgements
Co-authors from UC Berkeley were sponsored by Berkeley Deep Drive (BDD). Tomer Galanti’s
contribution was supported by the Center for Minds, Brains and Machines (CBMM), funded by NSF
STC award CCF-1231216.
References
[1] J. Behley, M. Garbade, A. Milioto, J. Quenzel, S. Behnke, C. Stachniss, and J. Gall. Se-
manticKITTI: A Dataset for Semantic Scene Understanding of LiDAR Sequences. In Proc. of
the IEEE/CVF International Conf. on Computer Vision (ICCV), 2019.
[2] Holger Caesar, Varun Bankiti, Alex H Lang, Sourabh Vora, Venice Erin Liong, Qiang Xu,
Anush Krishnan, Yu Pan, Giancarlo Baldan, and Oscar Beijbom. nuscenes: A multimodal
dataset for autonomous driving. In Proceedings of the IEEE/CVF conference on computer
vision and pattern recognition, pages 11621–11631, 2020.
[3] Xiangyu Yue, Bichen Wu, Sanjit A Seshia, Kurt Keutzer, and Alberto L Sangiovanni-Vincentelli.
A lidar point cloud generator: from a virtual world to autonomous driving. In Proceedings of
the 2018 ACM on International Conference on Multimedia Retrieval, pages 458–464, 2018.
[4] Iro Armeni, Sasha Sax, Amir R Zamir, and Silvio Savarese. Joint 2d-3d-semantic data for
indoor scene understanding. arXiv preprint arXiv:1702.01105, 2017.
[5] François Pomerleau, Francis Colas, and Roland Siegwart. A review of point cloud registration
algorithms for mobile robotics. Foundations and Trends in Robotics, 4(1):1–104, 2015.
[6] Chenfeng Xu, Bohan Zhai, Bichen Wu, Tian Li, Wei Zhan, Peter Vajda, Kurt Keutzer, and
Masayoshi Tomizuka. You only group once: Efficient point-cloud processing with token
representation and relation inference module. arXiv preprint arXiv:2103.09975, 2021.
[7] Sketchup. 3d modeling online free|3d warehouse models. https://3dwarehouse.sketchup.
com, 2021.
[8] Zhirong Wu, Shuran Song, Aditya Khosla, Fisher Yu, Linguang Zhang, Xiaoou Tang, and
Jianxiong Xiao. 3d shapenets: A deep representation for volumetric shapes. In Proceedings of
the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2015.
[9] Baoguang Shi, Song Bai, Zhichao Zhou, and Xiang Bai. Deeppano: Deep panoramic rep-
resentation for 3-d shape recognition. IEEE Signal Processing Letters, 22(12):2339–2343,
2015.
[10] Yan Wang, Wei-Lun Chao, Divyansh Garg, Bharath Hariharan, Mark Campbell, and Kilian
Weinberger. Pseudo-lidar from visual depth estimation: Bridging the gap in 3d object detection
for autonomous driving. In CVPR, 2019.
11

[11] Shir Gur and Lior Wolf. Single image depth estimation trained via depth from defocus cues. In
Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages
7683–7692, 2019.
[12] Wei Yin, Yifan Liu, and Chunhua Shen. Virtual normal: Enforcing geometric constraints for
accurate and robust depth prediction. IEEE Transactions on Pattern Analysis and Machine
Intelligence, 2021.
[13] René Ranftl, Katrin Lasinger, David Hafner, Konrad Schindler, and Vladlen Koltun. Towards
robust monocular depth estimation: Mixing datasets for zero-shot cross-dataset transfer. IEEE
Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2020.
[14] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition,
pages 770–778, 2016.
[15] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai,
Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al.
An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint
arXiv:2010.11929, 2020.
[16] Joao Carreira and Andrew Zisserman. Quo vadis, action recognition? a new model and the
kinetics dataset. In proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition, pages 6299–6308, 2017.
[17] Charles R Qi, Li Yi, Hao Su, and Leonidas J Guibas. Pointnet++: Deep hierarchical feature
learning on point sets in a metric space. arXiv preprint arXiv:1706.02413, 2017.
[18] Tomer Galanti, András György, and Marcus Hutter. On the role of neural collapse in transfer
learning. In International Conference on Learning Representations, 2022.
[19] Baoyuan Liu, Min Wang, Hassan Foroosh, Marshall Tappen, and Marianna Pensky. Sparse
convolutional neural networks. In Proceedings of the IEEE conference on computer vision and
pattern recognition, pages 806–814, 2015.
[20] Christopher Choy, JunYoung Gwak, and Silvio Savarese.
4d spatio-temporal convnets:
Minkowski convolutional neural networks. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, pages 3075–3084, 2019.
[21] Haotian* Tang, Zhijian* Liu, Shengyu Zhao, Yujun Lin, Ji Lin, Hanrui Wang, and Song
Han. Searching efficient 3d architectures with sparse point-voxel convolution. In European
Conference on Computer Vision, 2020.
[22] Hui Zhou, Xinge Zhu, Xiao Song, Yuexin Ma, Zhe Wang, Hongsheng Li, and Dahua Lin.
Cylinder3d: An effective 3d framework for driving-scene lidar semantic segmentation. arXiv
preprint arXiv:2008.01550, 2020.
[23] Yan Yan, Yuxing Mao, and Bo Li. Second: Sparsely embedded convolutional detection. Sensors,
18(10):3337, 2018.
[24] Di Feng, Yiyang Zhou, Chenfeng Xu, Masayoshi Tomizuka, and Wei Zhan. A simple and
efficient multi-task network for 3d object detection and road understanding. arXiv preprint
arXiv:2103.04056, 2021.
[25] Zining Wang, Wei Zhan, and Masayoshi Tomizuka. Fusing bird’s eye view lidar point cloud and
front view camera image for 3d object detection. In 2018 IEEE Intelligent Vehicles Symposium
(IV), pages 1–6. IEEE, 2018.
[26] Bichen Wu, Alvin Wan, Xiangyu Yue, and Kurt Keutzer. Squeezeseg: Convolutional neural
nets with recurrent crf for real-time road-object segmentation from 3d lidar point cloud. In
ICRA, 2018.
[27] Bichen Wu, Xuanyu Zhou, Sicheng Zhao, Xiangyu Yue, and Kurt Keutzer. Squeezesegv2:
Improved model structure and unsupervised domain adaptation for road-object segmentation
from a lidar point cloud. In ICRA, 2019.
[28] Chenfeng Xu, Bichen Wu, Zining Wang, Wei Zhan, Peter Vajda, Kurt Keutzer, and Masayoshi
Tomizuka. Squeezesegv3: Spatially-adaptive convolution for efficient point-cloud segmentation.
In European Conference on Computer Vision, pages 1–19. Springer, 2020.
12

[29] Hang Su, Subhransu Maji, Evangelos Kalogerakis, and Erik Learned-Miller. Multi-view convo-
lutional neural networks for 3d shape recognition. In Proceedings of the IEEE international
conference on computer vision, pages 945–953, 2015.
[30] Felix Järemo Lawin, Martin Danelljan, Patrik Tosteberg, Goutam Bhat, Fahad Shahbaz Khan,
and Michael Felsberg. Deep projective 3d semantic segmentation. In International Conference
on Computer Analysis of Images and Patterns, pages 95–107. Springer, 2017.
[31] Alexandre Boulch, Bertrand Le Saux, and Nicolas Audebert. Unstructured point cloud semantic
labeling using deep segmentation networks. 3DOR, 2:7, 2017.
[32] Bin Yang, Wenjie Luo, and Raquel Urtasun. Pixor: Real-time 3d object detection from point
clouds. In Proceedings of the IEEE conference on Computer Vision and Pattern Recognition,
pages 7652–7660, 2018.
[33] Alex H Lang, Sourabh Vora, Holger Caesar, Lubing Zhou, Jiong Yang, and Oscar Beijbom.
Pointpillars: Fast encoders for object detection from point clouds. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 12697–12705,
2019.
[34] Andres Milioto, Ignacio Vizzo, Jens Behley, and Cyrill Stachniss. Rangenet++: Fast and
accurate lidar semantic segmentation. In 2019 IEEE/RSJ International Conference on Intelligent
Robots and Systems (IROS), pages 4213–4220. IEEE, 2019.
[35] Charles R. Qi, Hao Su, Kaichun Mo, and Leonidas J. Guibas. Pointnet: Deep learning on point
sets for 3d classification and segmentation, 2016. cite arxiv:1612.00593.
[36] Yangyan Li, Rui Bu, Mingchao Sun, Wei Wu, Xinhan Di, and Baoquan Chen. Pointcnn:
Convolution on χ-transformed points. In Proceedings of the 32nd International Conference on
Neural Information Processing Systems, pages 828–838, 2018.
[37] Binh-Son Hua, Minh-Khoi Tran, and Sai-Kit Yeung. Pointwise convolutional neural networks.
In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages
984–993, 2018.
[38] Yongcheng Liu, Bin Fan, Gaofeng Meng, Jiwen Lu, Shiming Xiang, and Chunhong Pan.
Densepoint: Learning densely contextual representation for efficient point cloud processing. In
Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 5239–5248,
2019.
[39] Ze Liu, Han Hu, Yue Cao, Zheng Zhang, and Xin Tong. A closer look at local aggregation
operators in point cloud analysis. In European Conference on Computer Vision, pages 326–342.
Springer, 2020.
[40] Peng-Shuai Wang, Yang Liu, Yu-Xiao Guo, Chun-Yu Sun, and Xin Tong. O-cnn: Octree-based
convolutional neural networks for 3d shape analysis. ACM Transactions on Graphics (TOG),
36(4):1–11, 2017.
[41] Jiaxin Li, Ben M Chen, and Gim Hee Lee. So-net: Self-organizing network for point cloud
analysis. In Proceedings of the IEEE conference on computer vision and pattern recognition,
pages 9397–9406, 2018.
[42] Artem Komarichev, Zichun Zhong, and Jing Hua. A-cnn: Annularly convolutional neural
networks on point clouds. In Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition, pages 7421–7430, 2019.
[43] Ross Girshick, Jeff Donahue, Trevor Darrell, and Jitendra Malik. Rich feature hierarchies for
accurate object detection and semantic segmentation. In Proceedings of the IEEE conference on
computer vision and pattern recognition, pages 580–587, 2014.
[44] Longlong Jing and Yingli Tian. Self-supervised visual feature learning with deep neural
networks: A survey. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2020.
[45] Priya Goyal, Mathilde Caron, Benjamin Lefaudeux, Min Xu, Pengchao Wang, Vivek Pai, Man-
nat Singh, Vitaliy Liptchinsky, Ishan Misra, Armand Joulin, et al. Self-supervised pretraining
of visual features in the wild. arXiv preprint arXiv:2103.01988, 2021.
[46] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for
unsupervised visual representation learning. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition, pages 9729–9738, 2020.
13

[47] Philip Bachman, R Devon Hjelm, and William Buchwalter. Learning representations by
maximizing mutual information across views. arXiv preprint arXiv:1906.00910, 2019.
[48] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework
for contrastive learning of visual representations. In International conference on machine
learning, pages 1597–1607. PMLR, 2020.
[49] Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand Joulin.
Unsupervised learning of visual features by contrasting cluster assignments. arXiv preprint
arXiv:2006.09882, 2020.
[50] Xinlei Chen, Haoqi Fan, Ross Girshick, and Kaiming He. Improved baselines with momentum
contrastive learning. arXiv preprint arXiv:2003.04297, 2020.
[51] R Devon Hjelm, Alex Fedorov, Samuel Lavoie-Marchildon, Karan Grewal, Phil Bachman,
Adam Trischler, and Yoshua Bengio. Learning deep representations by mutual information
estimation and maximization. arXiv preprint arXiv:1808.06670, 2018.
[52] Hirokatsu Kataoka, Kazushige Okayasu, Asato Matsumoto, Eisuke Yamagata, Ryosuke Yamada,
Nakamasa Inoue, Akio Nakamura, and Yutaka Satoh. Pre-training without natural images. In
Proceedings of the Asian Conference on Computer Vision, 2020.
[53] Mathilde Caron, Piotr Bojanowski, Julien Mairal, and Armand Joulin. Unsupervised pre-
training of image features on non-curated data. In Proceedings of the IEEE/CVF International
Conference on Computer Vision, pages 2959–2968, 2019.
[54] Ting Chen, Simon Kornblith, Kevin Swersky, Mohammad Norouzi, and Geoffrey Hinton. Big
self-supervised models are strong semi-supervised learners. arXiv preprint arXiv:2006.10029,
2020.
[55] Olivier Henaff. Data-efficient image recognition with contrastive predictive coding. In Interna-
tional Conference on Machine Learning, pages 4182–4192. PMLR, 2020.
[56] Saining Xie, Jiatao Gu, Demi Guo, Charles R Qi, Leonidas Guibas, and Or Litany. Pointcontrast:
Unsupervised pre-training for 3d point cloud understanding. In European Conference on
Computer Vision, pages 574–591. Springer, 2020.
[57] Ji Hou, Benjamin Graham, Matthias Nießner, and Saining Xie. Exploring data-efficient 3d scene
understanding with contrastive scene contexts. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition, pages 15587–15597, 2021.
[58] Hanchen Wang, Qi Liu, Xiangyu Yue, Joan Lasenby, and Matthew J Kusner. Unsupervised
point cloud pre-training via view-point occlusion, completion. arXiv preprint arXiv:2010.01089,
2020.
[59] Xun Xu and Gim Hee Lee. Weakly supervised semantic point cloud segmentation: Towards
10x fewer labels. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition, pages 13706–13715, 2020.
[60] Ji Hou, Benjamin Graham, Matthias Nießner, and Saining Xie. Exploring data-efficient 3d
scene understanding with contrastive scene contexts. arXiv preprint arXiv:2012.09165, 2020.
[61] Zaiwei Zhang, Rohit Girdhar, Armand Joulin, and Ishan Misra. Self-supervised pretraining of
3d features on any point-cloud. arXiv preprint arXiv:2101.02691, 2021.
[62] Angela Dai and Matthias Nießner. 3dmv: Joint 3d-multi-view prediction for 3d semantic scene
segmentation. In Proceedings of the European Conference on Computer Vision (ECCV), pages
452–468, 2018.
[63] Zhengzhe Liu, Xiaojuan Qi, and Chi-Wing Fu. 3d-to-2d distillation for indoor scene parsing.
arXiv preprint arXiv:2104.02243, 2021.
[64] Yueh-Cheng Liu, Yu-Kai Huang, Hung-Yueh Chiang, Hung-Ting Su, Zhe-Yu Liu, Chin-Tang
Chen, Ching-Yu Tseng, and Winston H Hsu. Learning from 2d: Pixel-to-point knowledge
transfer for 3d pretraining. arXiv preprint arXiv:2104.04687, 2021.
[65] Hongming Shan, Yi Zhang, Qingsong Yang, Uwe Kruger, Mannudeep K Kalra, Ling Sun,
Wenxiang Cong, and Ge Wang. 3-d convolutional encoder-decoder network for low-dose
ct via transfer learning from a 2-d trained network. IEEE transactions on medical imaging,
37(6):1522–1534, 2018.
14

[66] Kevin Lu, Aditya Grover, Pieter Abbeel, and Igor Mordatch. Pretrained transformers as universal
computation engines. arXiv preprint arXiv:2103.05247, 2021.
[67] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,
Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual
models from natural language supervision. In International Conference on Machine Learning,
pages 8748–8763. PMLR, 2021.
[68] Vardan Papyan, X. Y. Han, and David L. Donoho. Prevalence of neural collapse during the
terminal phase of deep learning training. Proceedings of the National Academy of Sciences,
117(40):24652–24663, 2020.
[69] X. Y. Han, Vardan Papyan, and David L. Donoho. Neural collapse under mse loss: Proximity to
and dynamics on the central path, 2021.
[70] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for
biomedical image segmentation. In International Conference on Medical image computing and
computer-assisted intervention, pages 234–241. Springer, 2015.
[71] A. Geiger, P. Lenz, and R. Urtasun. Are we ready for Autonomous Driving? The KITTI Vision
Benchmark Suite. In Proc. of the IEEE Conf. on Computer Vision and Pattern Recognition
(CVPR), pages 3354–3361, 2012.
[72] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-
scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern
recognition, pages 248–255. Ieee, 2009.
[73] Ankit Goyal, Hei Law, Bowei Liu, Alejandro Newell, and Jia Deng. Revisiting point cloud
shape classification with a simple and effective baseline. arXiv preprint arXiv:2106.05304,
2021.
[74] Yue Wang, Yongbin Sun, Ziwei Liu, Sanjay E Sarma, Michael M Bronstein, and Justin M
Solomon. Dynamic graph cnn for learning on point clouds. Acm Transactions On Graphics
(tog), 38(5):1–12, 2019.
[75] Dogyoon Lee, Jaeha Lee, Junhyeop Lee, Hyeongmin Lee, Minhyeok Lee, Sungmin Woo, and
Sangyoun Lee. Regularization strategy for point cloud via rigidly mixed sample. In Proceedings
of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 15900–15909,
2021.
[76] Hengshuang Zhao, Li Jiang, Jiaya Jia, Philip HS Torr, and Vladlen Koltun. Point transformer.
In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 16259–
16268, 2021.
[77] Shi Qiu, Saeed Anwar, and Nick Barnes. Dense-resolution network for point cloud classification
and segmentation. In Proceedings of the IEEE/CVF Winter Conference on Applications of
Computer Vision, pages 3813–3822, 2021.
[78] Jinlai Zhang, Lyujie Chen, Bo Ouyang, Binbin Liu, Jihong Zhu, Yujing Chen, Yanmei Meng,
and Danfeng Wu. Pointcutmix: Regularization strategy for point cloud classification. arXiv
preprint arXiv:2101.01461, 2021.
[79] Roman Klokov and Victor Lempitsky. Escape from cells: Deep kd-networks for the recognition
of 3d point cloud models. In Proceedings of the IEEE International Conference on Computer
Vision, pages 863–872, 2017.
[80] Angela Dai, Angel X. Chang, Manolis Savva, Maciej Halber, Thomas Funkhouser, and Matthias
Nießner. Scannet: Richly-annotated 3d reconstructions of indoor scenes. In Proc. Computer
Vision and Pattern Recognition (CVPR), IEEE, 2017.
[81] Shai Ben-david, John Blitzer, Koby Crammer, and Fernando Pereira. Analysis of representations
for domain adaptation. In Advances in Neural Information Processing Systems 19, pages 137–
144. Curran Associates, Inc., 2006.
[82] Yishay Mansour. Learning and domain adaptation. In Algorithmic Learning Theory, 20th
International Conference, ALT, pages 4–6, 2009.
[83] Yishay Mansour, Mehryar Mohri, and Afshin Rostamizadeh. Domain adaptation: Learning
bounds and algorithms. In COLT - The 22nd Conference on Learning Theory, 2009.
15

[84] Tal Ridnik, Emanuel Ben-Baruch, Asaf Noy, and Lihi Zelnik-Manor. Imagenet-21k pretraining
for the masses, 2021.
[85] Ke Sun, Bin Xiao, Dong Liu, and Jingdong Wang. Deep high-resolution representation learning
for human pose estimation. In CVPR, 2019.
[86] Mehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar. Foundations of Machine Learning.
MIT Press, Cambridge, MA, 2 edition, 2018.
[87] Vladimir Koltchinskii and Dmitry Panchenko. Rademacher processes and bounding the risk of
function learning, 2004.
[88] Scott Reed, Zeynep Akata, Xinchen Yan, Lajanugen Logeswaran, Bernt Schiele, and Honglak
Lee. Generative adversarial text to image synthesis. In Maria Florina Balcan and Kilian Q.
Weinberger, editors, Proceedings of The 33rd International Conference on Machine Learning,
volume 48 of Proceedings of Machine Learning Research, pages 1060–1069, New York, New
York, USA, 20–22 Jun 2016. PMLR.
[89] Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan Salakhudinov,
Rich Zemel, and Yoshua Bengio. Show, attend and tell: Neural image caption generation with
visual attention. In Francis Bach and David Blei, editors, Proceedings of the 32nd International
Conference on Machine Learning, volume 37 of Proceedings of Machine Learning Research,
pages 2048–2057, Lille, France, 07–09 Jul 2015. PMLR.
[90] Katrin Lasinger, René Ranftl, Konrad Schindler, and Vladlen Koltun. Towards robust monocular
depth estimation: Mixing datasets for zero-shot cross-dataset transfer. CoRR, abs/1907.01341,
2019.
[91] Xun Huang, Ming-Yu Liu, Serge Belongie, and Jan Kautz. Multimodal unsupervised image-to-
image translation. In ECCV, 2018.
[92] Sagie Benaim, Michael Khaitov, Tomer Galanti, and Lior Wolf. Domain intersection and
domain difference. In ICCV, 2019.
[93] Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A. Efros. Unpaired image-to-image
translation using cycle-consistent adversarial networks. 2017 IEEE International Conference
on Computer Vision (ICCV), pages 2242–2251, 2017.
[94] Yunze Liu, Qingnan Fan, Shanghang Zhang, Hao Dong, Thomas A. Funkhouser, and Li Yi.
Contrastive multimodal fusion with tupleinfonce. 2021 IEEE/CVF International Conference on
Computer Vision (ICCV), pages 734–743, 2021.
[95] Junnan Li, Ramprasaath R. Selvaraju, Akhilesh Deepak Gotmare, Shafiq Joty, Caiming Xiong,
and Steven Hoi. Align before fuse: Vision and language representation learning with momentum
distillation. In NeurIPS, 2021.
[96] Bernard Chazelle. The discrepancy method - randomness and complexity. Cambridge University
Press, 2000.
[97] Alfred Müller. Integral probability metrics and their generating classes of functions advances in
applied probability. In Advances in Applied Probability, pages 429––443, 1997.
16

A
Implementation Details
Datasets.
Our experiments are conducted on ModelNet 3D Warehouse, S3DIS, and SemanticKITTI
datasets. For the ModelNet 3D Warehouse dataset, we train all models on the train set and evaluate
on the validation set. For the S3DIS, we train all models on area 1, 2, 3, 4, 6 and evaluate on area
5. For the SemanticKITTI dataset, we train all models on splits 00-10 except 08 which is used for
evaluation. For each of the datasets, all ResNet series models use the same training scheme, and all
experiments are implemented with PyTorch.
Training on ModelNet 3D Warehouse dataset.
In this case, coordinates of point-clouds are
randomly scaled, translated, and jittered. We employ the SGD optimizer with momentum 0.9, weight-
decay 10−4, and initial learning rate 0.1 with cosine learning rate scheduler. Each mini batch is
set to 32, and models are trained for 300 epochs. For both training and inference phase, we only
utilize x, y, z coordinates without other features and set the voxel size to be 0.05. The experiments
for ModelNet 3D Warehouse are all conducted on a Titan RTX GPU.
Training on the S3DIS dataset.
In this case, we concatenate all subparts of an indoor scene to train
and validate on. Along x, y directions, scenes are applied horizontal flip randomly. RGB features are
randomly jittered, translated, and auto contrasted. Finally, we normalize and clip point-clouds. We
set voxel size to 0.05, use SGD optimizer with momentum 0.9, weight-decay 10−4, and initialize
learning rate to 0.1 with polynomial learning rate scheduler. Each mini batch is set to 3, and models
are trained for 400 epochs on 2 Titan RTX GPUs.
Training on the SemanticKITTI dataset.
In this case, coordinates of each point-cloud are ran-
domly scaled and rotated. We use SGD optimizer with momentum 0.9, weight-decay 10−4, and
initial learning rate 0.24 with cosine warmup learning rate scheduler. Each mini batch is set to 2, and
models are trained for 15 epochs on 4 Titan RTX GPUs. For both training and inference phases, we
utilize x, y, z coordinates as well as intensity feature and set voxel size to 0.05.
Most of our pretrained models were taken from open-sources 345678, so we do not need to take time
and computational resources for pretraining. We use torchsparse9 to produce sparse 3D convolutions.
Details on Section 4.1
In this section, we take the ResNet architecture, inflate the pretrained models
of different image datasets, and add linear input and output layers as shown in Section B.6. The
ResNet50 was pretrained on ImageNet1K and is taken from the original PyTorch example. We use the
same training recipe provided by PyTorch to train the ResNet50 on Tiny-ImageNet. The pretrained
ResNet50 on ImageNet21K was taken from [84].
Details on Sections 4.2, 4.3, and 4.4.
In these sections, the pretrained ResNet models are taken
from the same sources as those in Section 4.1.
For pretraining PointNet++ on ImageNet1K, we utilize the PointNet++ SSG version [17]. We break
the image into pixels and regard the group of pixels as a point-cloud with coordinates of x, y positions
in the original image and appending z = 1 to all pixels. Then, we set center sampling number to
1024 and 256 for first and second stage, and the radius is set into 8 and 64, respectively. For each
center point, we query 64 neighboring points. The training recipe is also provided by PyTorch.
For ViT models, we directly take the pretrained weights from [15]. To apply it on ModelNet 3D
Warehouse, we sample 256 centers and group 64 nearby points, regarding these as “point-cloud
patches”. Then, we use a linear embedding to project the point-cloud patches into a sequence, and
ViT processes them the same as image patches. Except for the linear embedding and the final output
classifier, all the models are kept the same as the original version. For the experiments on S3DIS and
SemanticKITTI, the architectural detail of ResNet18 is shown in A.4 listing 2.
3https://pytorch.org/vision/stable/models.html
4https://github.com/Alibaba-MIIL/ImageNet21K
5https://github.com/hirokatsukataoka16/FractalDB-Pretrained-ResNet-PyTorch
6https://github.com/HRNet/HRNet-Semantic-Segmentation/tree/pytorch-v1.1
7https://github.com/rgeirhos/Stylized-ImageNet
8https://github.com/wielandbrendel/bag-of-local-features-models
9https://github.com/mit-han-lab/torchsparse
17

Table 6: ResNet50 results (evaluated on ModelNet 3D Warehouse) of finetuning the mean and
variance in batch normalization layers (BN) on different image-pretrained-models. IO (FIP-IO)
indicates finetuning input and output layer. IOms indicates updating input and output layer, BN mean
and variance. IOmsWb (FIP-IO+BN) indicates finetuning input, output layer, and the whole BN.
Layers
Tiny-
ImageNet
ImageNet1K
ImageNet21K
FractalDB1K
FractalDB10K
IO
67.67
81.20
73.74
83.35
80.11
IOms
83.79
82.94
84.08
72.33
79.66
IOmsWb
89.99
89.87
90.80
89.26
89.34
From Scratch
90.32
Table 7: ResNet18, 50, 152 results (evaluated on ModelNet 3D Warehouse) of finetuing the mean
and variance in batch normalization layers.
Layers
ResNet18
ResNet50
ResNet152
IO
71.03
81.20
64.63
IOms
81.89
82.94
82.66
IOmsWb
88.75
89.87
90.44
From Scratch
90.39
90.32
90.28
For SimpleView model, all the experiment settings are the same as [73]. The only difference is
whether to use the pretrained ResNet18. For HRNetV2-W48, we directly use the ImageNet1K and
Cityscape pretrained models from [85].
We conduct three trials on the few-shot experiments. For each trial, we change the random seed but
keep all the other settings the same. To plot the training speed curve, we directly use the training log
without any other changes, such as smoothing.
B
Additional Experiments
B.1
Finetuning the mean and variance of batch normalization.
For the first group of experiments, ResNet50 FIP either has IO or IO+BN finetuned. In addition
to these two experimental settings, we also investigate finetuning input, output layers, and mean,
variance of normalization layers, while fixing the convolution layer weights, normalization layer
weights, and bias. The full experiment results with this extra setting are reported in Table 6 and 7. We
can observe that compared with only finetuning input and output layers, updating mean and variance
can also largely improve the performance of point-cloud recognition. As suggested in Section 2.4,
we train batch normalizations to enhance the adaption between modalities.
B.2
Ablation study of inflating towards different directions.
We conduct experiments of inflating filters along different directions with the illustration figure shown
in Figure 5 and the results shown in Table 8. We find that the performance is different when using
different inflation methods. In particular, with ResNet50 pretrained on ImageNet1K, inflating along
the x axis and the y axis leads to better performance compared with inflating along z axis for both
FIP-IO and FIP-IO+BN. More importantly, the minimally finetuned FIP-IO+BN with inflating along
the x and y axis even surpasses the training-from-scratch.
B.3
Ablation study of loading different stages of the image-pretrained model.
We investigate the effect of loading different subsets of stages. The results are shown in Table 9. In
detail, we load the pretrained weights partially while keeping the other weights randomly initialized.
We observe that excluding the weights of the first stage achieves the best performance, bringing 0.77
points improvement.
18

2D Filter
3D Filter
Z axis
Y axis
X axis
Figure 5: Visualization of filter inflation along different axis.
Table 8: ModelNet 3D Warehouse results (Top-1 accuracy) of partially finetuning ResNet50 pretrained
on ImageNet1K with inflation along the x, y, z axis.
Method
x axis
y axis
z axis
FIP-IO
82.17
81.73
81.20
FIP-IO+BN
90.44
90.84
89.87
From Scratch
90.32
Table 9: ModelNet 3D Warehouse results (Top-1 accuracy) of finetuning ResNet50 pretrained on
ImageNet1K with only subset of stages loaded.
Loaded Stages
1
1 2
1 2 3
1 2 3 4
2 3 4
3 4
4
FIP-ALL
89.91
90.36
90.64
90.92
91.09
90.19
89.99
From Scratch
90.32
B.4
Stability analysis of the semi-supervised experiment.
For the semi-supervised experiment, we change the random seed and calculated the mean and standard
deviation of three trials for each setting as shown in Table 10.
B.5
Neural Collapse in the Embedding Layer.
[68] characterized neural collapse as training dynamics of overparameterized neural networks in
which the feature embeddings of samples from the same class tend to concentrate around their class
means. In this section, we briefly define neural collapse and evaluate it in our current setting. We
refer the reader for additional details in [68, 18].
Suppose we have a classification problem, in which we are provided with a training dataset S =
∪C
c=1Sc = ∪C
c=1{(xci, c)}m0
i=1 split into classes. We would like to train a neural network h = e ◦q,
with q : Rn →Rp and e : Rp →RC is a linear layer. The neural network is trained by minimizing
cross-entropy loss between the one-hot encodings of its labels of samples in S and its logits. For
additional details, see Section C.1.
Several definitions of neural collapse have been proposed in the literature. In this paper we work with
a relatively simple definition that has been proposed in [18]. We start by defining the class-distance
normalized variance (CDNV), which is a measure of clusterability of the feature embeddings. For a
given feature map q : Rn →Rp and two distributions Q1, Q2 (of samples from two different classes)
over X, the 10 is defined in the following manner
  V_q( Q_1 , Q_2) = \ f rac {\Va
r _q(Q_1 )  + \Var _q(Q_2)}{2\|\mu _q(Q_1)-\mu _q(Q_2)\|^2}. 
(1)
Essentially, this quantity measures to what extent the deviations of the embeddings q(x) of samples
coming from Q1 and Q2 are smaller than the distance between their means. Intuitively, if the
deviations are very small in comparison with the distances, then we expect the embeddings to be
clustered with respect to their class labels. Note, that this quantity is also scale-invariant, i.e., if we
multiply q by α ̸= 0, then, the CDNV would not change for any pair Q1, Q2.
10The CDNV can be extended to finite sets S1, S2 ⊂X by defining Vq(S1, S2) = Vq(U[S1], U[S2]).
19

Table 10: Stability analysis of semi-supervised distillation experiments on top of ResNet34 on the
ModelNet 3D Warehouse dataset.
Few-shot
From scratch
PointInfoNCE
Hardest
Contrastive
ImageNet1K
pretrain (Ours)
10-shot
72.7±1.2
74.3±1.3 (+1.6)
73.9±1.1 (+1.2)
74.6±1.1 (+1.9)
5-shot
62.2±0.7
64.5±1.4 (+2.3)
65.0±2.1 (+2.8)
65.4±1.4 (+3.2)
1-shot
30.8±2.4
36.5±1.8 (+5.7)
35.9±1.0 (+5.1)
38.3±2.0 (+7.5)
Table 11: CDNV of the pretrained models on ImageNet1K Training/validation set, and CDNV of
training from scratch and FIP-IO+BN on ModelNet 3D Warehouse training/validation set
Models
ImageNet1K
From scratch
FIP-IO+BN
ImageNet1K
FIP-IO+BN
ImageNet21K
Training CDNV
0.63
0.37
0.71
0.47
Validation CDNV
0.66
0.43
0.68
0.60
According to the definition in [18], neural collapse is defined in the following manner
  \
lim  _{t\to \infty }\Av g _{ i \neq j \in [l]}[V_{q_t}(S_i,S_j)] = 0, 
(2)
where qt is the embedding function after t epochs of training e ◦q. Intuitively, during train time, the
feature embeddings of samples of the same class tend to concentrate around their class-means in
comparison with their distance from the other classes.
Evaluating neural collapse.
In this experiment we measure the clusterability of the feature embed-
dings of the penultimate layer of the pretrained model into classes, on both the source/pretraining task
(i.e., ImageNet1K) and the target task (e.g., ModelNet40). In this experiment, we train a classifier of
the form ˜h = ˜e ◦q = ˜e ◦f ◦˜g on ImageNet1K. We used ResNet50 as the architecture of h, where ˜g
is the first convolutional layer of the model and ˜e is the top linear layer of the model. As a second
step, we replace ˜e and ˜g with neural networks e (a linear layer) and g (a linear layer) and train them,
along with retraining the batch normalization parameters of f on ModelNet40, resulting in a function
e ◦f ′ ◦g. We denote by ˜Str = ∪k
c=1 ˜Str
c the source training dataset and by Str = ∪l
c=1Str
c the target
training dataset. Here, ˜Str
c and Str
c are the samples associated with the cth class. We also denote by
˜Sval = ∪k
c=1 ˜Sval
c
and Sval = ∪k
c=1Sval
c
the corresponding validation datasets.
To measure the degree of clusterability of the feature embeddings, we consider multiple applications
of the averaged CDNV. For measuring the clusterability of the features on the source task, we
consider the CDNV of f ◦˜g on the source train and validation datasets: Avgi̸=j∈[k][Vf◦˜g( ˜Str
i , ˜Str
j )]
and Avgi̸=j∈[k][Vf◦˜g( ˜Sval
i
, ˜Sval
j
)]. These results are reported in Table 11. Similarly, we also measure
the CDNV of f ′ ◦g on the source train and validation datasets: Avgi̸=j∈[k][Vf ′◦g( ˜Str
i , ˜Str
j )] and
Avgi̸=j∈[k][Vf ′◦g( ˜Sval
i
, ˜Sval
j
)]. In the main text, we report the CDNV on the validation set for
e ◦f ′ ◦g which reflects the property of the embedding’s clusterability in a low-dimensional space.
As can be seen in Table 11, across all of the experiments, the values of the CDNV are lower than 1,
meaning that the standard deviations of the embeddings per class are smaller in comparison with the
distances between class means. Therefore, we encounter a scenario where the embeddings of samples
are fairly separated into classes. In addition, we observe that the degree of collapse generalizes well
to new samples, as the CDNV on the train and validation data are relatively similar.
B.6
Details of used architectures.
1 Class 3DRes_cls(nn.Module):
2
def
__init__(self , res_block):
3
# res_block
means the
residual
block as same as the
conventional
ResNet.
4
super ().__init__ ()
5
20

6
self.input_layer = nn.Sequential(
7
sparse_conv3d(input_dim , layer1_Idim , k=3, s=1),
8
sparse_bn(layer1_Idim))
9
10
self.layer1 = inflated_resnet_layer1 (
11
res_block , layer1_Idim , layer1_Odim)
12
self.layer2 = inflated_resnet_layer2 (
13
res_block , layer2_Idim , layer2_Odim)
14
self.layer3 = inflated_resnet_layer3 (
15
res_block , layer3_Idim , layer3_Odim)
16
self.layer4 = inflated_resnet_layer4 (
17
res_block , layer4_Idim , layer4_Odim)
18
19
self.output_layer = nn.Sequential(
20
global_average_pooling ,
21
nn.Linear(layer4_Odim , class_num),
22
nn.bn(class_num))
23
24
def
forward(self , x):
25
x = self.input_layer(x)
26
x = self.layer1(x)
27
x = self.layer2(x)
28
x = self.layer3(x)
29
x = self.layer4(x)
30
return
self.output_layer(x)
Listing 1: Pseudo code of inflated ResNet with linear input and output layers for classification.
1 Class 3DRes_seg(nn.Module):
2
def
__init__(self , res_block):
3
# res_block
means the
residual
block as same as the
conventional
ResNet.
4
super ().__init__ ()
5
6
self.input_layer = nn.Sequential(
7
sparse_conv3d(
8
input_dim , layer1_Idim , k=3, s=1),
9
sparse_bn(layer1_Idim),
10
sparse_ReLU(True),
11
sparse_conv3d(
12
layer1_Idim , layer1_Idim , k=3, s=1),
13
sparse_bn(layer1_Idim),
14
sparse_ReLU(True),
15
sparse_conv3d(
16
layer1_Idim , layer1_Idim , k=3, s=2),
17
sparse_bn(layer1_Idim),
18
sparse_ReLU(True))
19
20
self.layer1 = inflated_resnet_layer1 (
21
res_block , layer1_Idim , layer1_Odim)
22
self.layer2 = inflated_resnet_layer2 (
23
res_block , layer2_Idim , layer2_Odim)
24
self.layer3 = inflated_resnet_layer3 (
25
res_block , layer3_Idim , layer3_Odim)
26
self.layer4 = inflated_resnet_layer4 (
27
res_block , layer4_Idim , layer4_Odim)
28
29
self.up1 = sparse_deconv (
30
layer4_Odim , layer4_Odim , k=2, s=2)
31
self.decode1 = self.Sequential(
32
res_block(layer4_Odim+layer3_Odim , layer3_Odim),
33
res_block(layer3_Odim , layer3_Odim))
34
35
self.up2 = sparse_deconv (
36
layer3_Odim , layer3_Odim , k=2, s=2)
21

37
self.decode2 = self.Sequential(
38
res_block(layer3_Odim+layer2_Odim , layer2_Odim),
39
res_block(layer2_Odim , layer2_Odim))
40
41
self.up3 = sparse_deconv (
42
layer2_Odim , layer2_Odim , k=2, s=2)
43
self.decode3 = self.Sequential(
44
res_block(layer2_Odim+layer1_Odim , layer1_Odim),
45
res_block(layer1_Odim , layer1_Odim))
46
47
self.up4 = sparse_deconv (
48
layer1_Odim , layer1_Odim , k=2, s=2)
49
self.decode4 = self.Sequential(
50
res_block(layer1_Odim+layer1_Odim , layer1_Odim),
51
res_block(layer1_Odim , layer1_Odim))
52
53
self.output_layer = nn.Sequential(
54
nn.Linear(layer1_Odim , class_num))
55
56
def
forward(self , x):
57
x_i = self.input_layer(x)
58
x1 = self.layer1(x_i)
59
x2 = self.layer2(x1)
60
x3 = self.layer3(x2)
61
x4 = self.layer4(x3)
62
63
x3_ = self.decoder1(cat(x3 , self.up1(x4)))
64
x2_ = self.decoder2(cat(x2 , self.up2(x3_)))
65
x1_ = self.decoder3(cat(x1 , self.up3(x2_)))
66
xi_ = self.decoder4(cat(x_i , self.up4(x1_)))
67
return
self.output_layer(xi_)
Listing 2: Pseudo code of inflated ResNet for segmentation.
C
Theoretical Analysis
To motivate our approach, in this section we provide some theoretical support for the transfer of
image domain to point-cloud domain described in the main text. Instead of specifying image and
point-cloud in the analysis, we begin by introducing a formal framework for analyzing transfer
learning between different modalities and classes. Note that the modalities should have grounded
“relationships”, or as illustrated below, have meaningful task similarity. For example, image and
point-cloud are both visual representations of the real world that share mutual characteristics, such as
content and shape that could be captured using neural networks encoders.
We begin by introducing a formal framework for analyzing transfer learning between different
modalities and classes. Then, we analyze a simple toy example, in which it is possible to perfectly
translate one modality to the other using a linear mapping. Finally, we consider a more realistic case,
in which we assume that the two domains share a ‘mutual semantic space’ that encodes the content
within samples from the two domains.
C.1
Problem Setup
We extend the transfer learning setting in [18]. We consider the problem of training a generic feature
representation on a source classification task and transferring it to a target task. The two classification
problems correspond to different modalities (i.e., two different kinds of data representations) and
consist of different sets of classes.
To model this problem, we assume that the target task is a k-class classification problem and the
source task where the feature representation is learned on an l-class classification problem. Formally,
the target task T = (P, y) is defined by a distribution P over samples x ∈X, where X ⊂Rd1 is
the instance space, along with a function y : X →Yk, where Yk is a label space with cardinality
k. To simplify the presentation, we use one-hot encoding for the label space, that is, the labels are
22

represented by the unit vectors in Rk, and Yk = {ec : c = 1, . . . , k} where ec ∈Rk is the cth
standard unit vector in Rk; with a slight abuse of notation, sometimes we will also write y(x) = c
instead of y(x) = ec. For a sample x with distribution P, we denote by Pc(·) = P[x ∈· | y(x) = c]
the class distribution of x given y(x) = c. We consider balanced classes, i.e., P[y(x) = c] = 1/k.
The target task.
A classifier h ∈H is a mapping h : X →Rk that assigns a soft label to an input
point x ∈X, and its performance on the target task is measured by the risk
  \la be l  {eq :loss} L _Pxh,y)=\E _{x\sim P}[\ell (h(x),y(x))], 
(3)
where ℓ: Rk × Yk →[0, ∞) is a loss function, defined as follows ℓ(u, v) = I[arg max(u) = v].
Our goal is to learn a classifier h from some training data S = ∪k
c=1Sc = ∪k
c=1{(xci, c)}n
i=1 of n
independent and identically distributed (i.i.d.) samples drawn from each class Pc of P. However,
when encountering a complicated classification problem and n is small, this is likely to be a hard
task. To facilitate finding a good solution, we aim to find a classifier of the form h = e ◦f ◦g, where
f : Rp1 →Rp2 is a feature map from a family of functions F ⊂{f ′ : Rp1 →Rp2}, e ∈E ⊂{e′ :
Rp2 →Rk} is an affine function and g is a function from a family G1 ⊂{g′ : Rd1 →Rp1}. Namely,
the feature map f is trained on a source problem, potentially of a different modality, where much
more data is available, and then g and e are trained on S while freezing f. Intuitively, g and e are
relatively ‘simple’ functions (e.g., linear layers) that are task-specific. Concretely, e (the classifier) is
responsible for translating f into a classifier between the classes in P and g (the adaptor) adapts f to
the specific modality of P.
The source task.
We assume that the source task helping to find f is an l-class classification
problem over a different sample space X ′ ⊂Rd2. For example, X could be a set of 3D point-clouds,
while X ′ is a set of 2D natural images. The source task B = ( ˜P, ˜y) is defined by a distribution ˜P
and function ˜y : X ′ →Yl, and here we are interested in finding a classifier ˜h : X ′ →Rl of the
form ˜h = ˜e ◦f ◦˜g, where ˜e ∈˜E ⊂{e′ : Rp →Rl} is an affine function over the feature space
f(˜g(X)) = {f(˜g(x)) : x ∈X} and ˜g ∈G2 ⊂{g′ : Rd2 →Rp1} is an adaptor. Given a training
dataset ˜S = ∪l
c=1 ˜Sc = ∪l
c=1{(˜xci, c)}m
i=1, all components of the classifier, denoted by ˜g, f and ˜e are
trained on ˜S, with the goal of minimizing the cross-entropy loss in the source task. Following [18]
we assume that ˜Sc are drawn i.i.d. from ˜Pc and that P[˜y(x) = c] = 1/l.
Tasks similarity.
In general, transferring between the source and target tasks is meaningless if
the two tasks are extremely unrelated to each other. For instance, we should not expect to have
any guarantee to transfer knowledge from very different tasks, such as voice separation and image
segmentation.
Intuitively, we think of the classes of the target (source) task as being of ‘similar character’. To
formalize this intuition, we simply assume that the target (source) classes are i.i.d. samples of a
distribution D1 over C1 (D2 over C2). In [18] they assumed that the source and target classes differ,
but share the same underlying distribution, i.e., D1 = D2. Since in this work we intend to study
transfer between different modalities, D1 need not be the same as D2. To formally define notions of
similarity between domains, we first assume the existence of an invertible mapping F : C1 →C2,
such that, ˆPc := F(Pc) ∼D2 for Pc ∼D1. In a sense, D1 and D2 share the same set of categories,
encoded with different kinds of modalities. The mapping F takes a certain class Pc ∈C1 and maps
it to its analogous class ˆPc ∈C2 in the second domain. For a given target task T = (P, y) with
distribution P, classes {Pc}k
c=1 and function y : X →Rk, we denote A = ( ˆP, ˆy) the corresponding
analogous task within the second domain, where ˆPc = F(Pc) (not to be confused with the source
task B = ( ˜P, ˜y)). In general, F could be an arbitrary mapping between the classes. Therefore,
to concretely relate between D1 and D2 we will have to make additional assumptions about the
relationship between P and ˆP. The specific relationship between Pc and ˆPc will be explicitly defined
near the presentation of each result.
Evaluation process.
As a next step, we would like to evaluate the performance of the pretrained
feature map f on the target task. To do so, we evaluate its expected performance over the distribution
of binary classification target tasks
 \
label  {eq:error} L^k_{\cD _1}(f)=\ E _ { P _ 1\n eq \dots \neq P_k \sim \cD _1}\E _{S_1,\dots ,S_k} [L_P(e_S \circ f \circ g_S, y)], 
(4)
23

where Sc ∼P n
c and eS, gS are the outputs of a learning algorithm that trains e ∈E and g ∈
G1 to fit e ◦f ◦g to the dataset S while freezing f. For simplicity, in this work we focus on
k = 2 and denote LD1(f) = L2
D1(f), even though the analysis could be readily extended to
k > 2. Note that several implementations of mappings S 7→eS, gS are possible. For simplicity,
in this work, we choose arg max ◦eS to be the ‘nearest empirical mean classifier’ and gS to be an
empirical risk minimizer. Formally, for a given embedding function h, we consider the linear function
eS
h(z) = (⟨z, 2µh(Sc)⟩−∥µh(Sc)∥2)c=1,2. In this case, arg max ◦eS
h(z) = minc=1,2 ∥z −µh(Sc)∥
forms a nearest empirical mean classification rule that classifies a vector z as 1 if it is closer to
the empirical embeddings mean µh(S1) = Avgx∼S1[h(x)] than to µh(S2) = Avgx∼S2[h(x)]. In
addition, gS = arg ming∈G LX(eS
f◦g ◦f ◦g, y), where X = ∪k
c=1{xci}n
i=1 and eS = ef◦gS is the
corresponding nearest empirical mean classifier. Notice that while the feature map f is evaluated on
the distribution of target tasks determined by classes taken from D1, the training of f, as described
above, is fully agnostic of this target.
To summarize, in the proposed setting we train a feature map f as part of a classification model ˜e ◦f
to fit some source data corresponding to a set of source classes ˜P1, . . . , ˜Pl using dataset ˜S = ∪l
c=1 ˜Sc.
At the second stage, f’s performance is evaluated against a randomly selected set of target classes
P1, . . . , Pk the differ by content (i.e., different categories) and modality (e.g., 2D and 3D point
clouds) by training an adaptor g and a linear classifier e based on the available data S = ∪k
c=1Sc.
Therefore, in this work we deal with two modes of transfer. First, we would like to train f to be a
generic feature map that can be used to distinguish between many different categories. The second
deals with the ability to adapt f from one modality to another using minimal efforts. Intuitively, if
the pretrained feature map f enjoys both qualities, we expect LD1(f) to be small. This is what we
show in Theorem 1.
Notation.
Throughout the analysis, we use the following notations.
For an integer k ≥1,
[k] = {1, . . . , k}. For any real vector z, ∥z∥denotes its Euclidean norm. For a given set A =
{a1, . . . , an} ⊂B and a function u ∈U ⊂{u′ : B →R}, we define u(A) = {u(a1), . . . , u(an)}
and U(A) = {u(A) : u ∈U}. Let Q be a distribution over X ⊂Rd and u : X →Rp. We
denote by µu(Q) = Ex∼Q[u(x)] and by Varu(Q) = Ex∼Q[∥u(x) −µu(Q)∥2] the mean and vari-
ance of u(x) for x ∼Q. For A above, we denote by Avgn
i=1[ai] = Avg A =
1
n
Pn
i=1 ai the
average of A. For a finite set A, we denote by U[A] the uniform distribution over A. We denote by
I : {True, False} →{0, 1} the indicator function. For a given distribution P over X and a measurable
function f : X →X ′, we denote the distribution of f(x) by f ◦P. For two classes of functions
G = {g′ : Rd1 →Rd2} and F = {f ′ : Rd2 →Rd3}, we denote F ◦G = {f ◦g | f ∈F, g ∈G}.
D
Theoretical Results
D.1
Case 1
As a toy example, we first consider the case where the first domain (target) can be translated into
the second domain (source), using some simple function g∗. For this purpose, we assume that G1
is decomposed into G1 = G′′
1 ◦G′
1, where G′
1 ⊂{g′ : Rd1 →Rd2} and G′′
1 ⊂{g′ : Rd2 →Rp1}.
Intuitively, the functions g ∈G1 are decomposed into sub-architectures g′
1 : Rd1 →Rd2 and
g′′
1 : Rd2 →Rp1. In addition, we assume there exists a function g∗∈G′
1, such that g∗◦P ∼D2 for
P ∼D1 (where g ◦P is the distribution of g(x) for x ∼P). In addition, we assume that G2 ⊂G′′
1 .
Hence, for each candidate function ˜g ∈G2 that could have been selected when training the classifier
h = ˜e ◦f ◦˜g, there exists a function g = ˜g ◦g∗∈G1 that maps Pc into g ◦Pc = ˜g ◦˜Pc. Intuitively,
any representation ˜g ◦˜P of the distribution ˜P could be implemented as g ◦P for some g ∈G1. While
g∗is unknown to the learning algorithm, as we will see in Theorem 1, this simplifying assumption
makes it possible to easily transfer between the two domains.
The following theorem provides an upper bound on the expected error of a pretrained feature map
f in terms of the expected CDNV between pairs of classes ˜Pi and ˜Pj from D2 and the empirical
Rademacher complexity of the class Hf = {arg max ◦e ◦f ◦g | (e, g) ∈E × G1}. The Rademacher
complexity is a measure of generalization that quantifies the ability of a class of functions to fit noise.
Formally, for a given set X = {xi}n
i=1 ⊂Rd and set of functions H ⊂{h′ | h′ : Rd →R}, the
24

empirical Rademacher complexity (see [86]) of H is defined as follows
  \Rad _ X(
\
cH 
) =
 
\
E
 
_{\
si g ma }\
l
eft [\sup _{h \in \cH } \frac {1}{n}\sum ^{n}_{i=1} \sigma _i \cdot f(x_i) \right ], 
(5)
where σ = (σ1, . . . , σn) are i.i.d. uniformly distributed over {±1}. The empirical Rademacher
complexity can lead to tighter bounds than those based on other measures of complexity such as the
VC-dimension [87]. It also has the added advantage that it is data-dependent and can be measured
from finite samples.
Theorem 1. In the setting above. For any tuple (f, ˜g) ∈F × G, we have:
  \beg i n { a
lign e
d} L_
{
\cD _1 }(f)  &~\
leq  ~ 1 6\ma thbb
 
{ E
}
_{\tP _
1 
\ n
e
q  \tP _2 \sim  
\ mathcal {D} _2} \left [\frac {V_{f\circ \tig }(\tP _1,\tP _2)}{s(f\circ \tig ,\tP _1,\tP _2)}\right ] + 6\sqrt {\frac {\log (4n)}{2n}} + \frac {2}{n} \\ &~~\quad +2\E _{P_c\sim \cD _1}\E _{X_c\sim P^{n}_c}\left [\Rad _{X_c}(\cH _f)\right ], \end {aligned} 
where s(f, P1, P2) = p2 if f ◦P1 and f ◦P2 are spherically symmetric and s(f, P1, P2) = 1
otherwise.
The theorem above provides an upper bound on the expected error LD1(f) of a pretrained feature
map f against classification tasks generated using classes from D1. The bound holds uniformly for
any pair (f, ˜g) ∈F × G. Hence, we think of f and ˜g as the pretrained functions that were obtained
by training the classifier ˜e ◦f ◦˜g to fit the source data ˜S = ∪l
c=1 ˜Sc. The bound is decomposed into
several parts. The first term is (approximately) proportional to the CDNV between pairs of randomly
selected classes from D2. Namely, it measures the extent of neural collapse we encounter between
randomly selected pairs of source classes ˜P1 ̸= ˜P2 ∼D2.
As mentioned in Section B.5 in the regime of neural collapse, we expect Avgi̸=j∈[l]
h
Vf◦˜g( ˜Si, ˜Sj)
i
to be small. In addition, by Propositions 1 in [18], under certain circumstances if the number of
samples per source class m is large enough, we also expect Avgi̸=j∈[l]
h
Vf◦˜g( ˜Pi, ˜Pj)
i
to be small.
Finally, by Propositions 2 in [18], if the number of source classes l is also large, we should also expect
E ˜
Pi̸= ˜
Pj∼D2[Vf◦˜g( ˜Pi, ˜Pj)] to be small. As a side note, a smaller bound is obtained when f ◦˜g projects
the classes ˜Pc ∼D2 to spherically symmetric distributions as s(f ◦˜g, ˜P1, ˜P2) = p2 if f ◦˜g ◦˜Pc are
spherically symmetric. For estimations of Avgi̸=j∈[l]
h
Vf◦˜g( ˜Si, ˜Sj)
i
and Avgi̸=j∈[l]
h
Vf◦˜g( ˜Pi, ˜Pj)
i
,
see Section B.5, in which ˜Sc is denoted by ˜Str
c and the class distributions ˜Pc are approximated with
validation sets ˜Sval
c
.
The second group of terms includes the (expected) Rademacher complexity of the class Hf and
additional terms that scale as O
p
log(n)/n

. The Rademacher complexity RadS(H) of a class H
of neural network classifiers h : Rd →{0, 1} typically scales as O
p
N/n

, where N polynomially
depends on the number of trainable parameters and n is the number of samples. Therefore, in
standard settings, we expect RadS(Hf) to scale as O
p
N/n

, where N polynomially depends
on the number of parameters in e and g. On the other hand, a standard Rademacher complexity
generalization bound would yield a dependence on the number of parameters existing in the full
network e◦f ◦g (including the ones in f). Since typically f contains most of the trainable parameters
in the neural network, this allows us to significantly reduce the sample complexity of the target task.
Proof. To prove this theorem, we fix a target task T = (P, y) with a pair of classes P1, P2 and a
pretrained feature map f ◦˜g. By (cf. [86], Theorem 3.5), for any c = 1, 2, with probability at least
1 −
1
4n over the selection of Sc, for any pair (e, g) ∈E × G1, we have
| \beg i n  {a li g ned} \ v e rt  L_ { P _c}(e \ci r c
 
f \circ
 g
, y) - L_{X_c}(e\circ f \circ g, y) \vert \leq 2\Rad _{X_c}(\cH _f) + 3\sqrt {\frac {\log (4n)}{2n}}, \end {aligned} 
(6)
where Xc consists of the samples in Sc excluding their labels and X = X1 ∪X2. By union bound
over c = 1, 2, with probability at least 1 −
1
2n over the selection of S, the following inequality holds
25

uniformly for all (e, g) ∈E × G1,
| \ be g i n  { al i gned }  \ ve rt| L
_
{P}(e
 \circ f \ c
i
rc g, y
) 
- L_{X}(e \circ f \circ g, y) \vert ~\leq ~ \sum _{c=1,2}\Rad _{X_c}(\cH _f) + 3\sqrt {\frac {\log (4n)}{2n}}. \end {aligned} 
(7)
Hence, with probability at least 1 −
1
2n over the selection of S,
  \be g i n  {a li g ned} L _ P (e_ S 
\
c
irc f
 \circ g_ S ,
 
y) &~\l
eq
 ~ L_X(e_S \circ f \circ g_S, y)\\ &~~\quad + \sum _{c=1,2}\Rad _{X_c}(\cH _f) + 3\sqrt {\frac {\log (4n)}{2n}}. \\ \end {aligned} 
(8)
Let E′ =

e(z) = (−∥z −µc∥2)c=1,2 = (⟨z, 2µc⟩−∥µc∥2)c=1,2 | µ1, µ2 ∈Rp2	
⊂E. In addi-
tion, we let eP (z) = (−∥z −µf◦˜g◦g∗(Pc)∥2)c=1,2. Since ˜g ◦g∗∈G2 ◦G′
1 ∈G′′
1 ◦G′
1 = G1, we have
  \be g i n  {a li g
ned
} L_ X(e
_S \ circ  f  \c ir
c
 g_
S, y ) &~ = ~  \i n f _ {g
 \in \ c G  _1 }  \i nf _{e \in \cE '} L_X(e \circ f \circ g, y) \\ &~\leq ~ \inf _{e \in \cE '} L_X(e \circ f \circ \tig \circ g^*, y) \\ &~\leq ~ L_X(e_{P} \circ f \circ \tig \circ g^*, y). \end {aligned} 
(9)
In particular, since the loss function is bounded in [0, 1] and the above event holds with probability at
least 1 −
1
2n, by taking expectation over S, we obtain the following inequality
  \be gin  { a lig ned
}  & \mathb b  { E} _ {S} \le f t
 
[L_P(e_
S 
\ c
ir c
 
f \ci
rc g_S, y
) \r ight ]  \ \  & ~ \le q ~  \
m
athbb {
E}
_ {
S} \
l
eft [
L_S(e_P \circ f \circ \tig \circ g^*, y)\right ] + 3\sqrt {\frac {\log (4n)}{2n}} + \frac {1}{2n} + \sum _{c=1,2}\Rad _{X_c}(\cH _f) \\ &~=~ \mathbb {E}_{S}\left [L_S(e_P \circ f \circ \tig \circ g^*, y)\right ] + 3\sqrt {\frac {\log (4n)}{2n}} + \frac {1}{2n} + \sum _{c=1,2}\Rad _{X_c}(\cH _f). \end {aligned} 
(10)
Finally, we can take expectation over the selection of P1, P2 on both sides of the inequality to obtain
that
  \beg i n {aligned} L_{ \cD  _ 1 }(f ) &
~ =~ \mat hbb  {E } _ { P_ 1 \ne q P
_ 2 \sim \cD _1} \mathbb {E} _ {
S
}\left 
[L
_ P
(e_S \circ f \circ g_S, y)\right ] \\ &~\leq ~ \mathbb {E}_{P_1\neq P_2}\left [L_P(e_P \circ f \circ \tig \circ g^*,y)\right ] \\ &~~\quad + 2\mathbb {E}_{P_1\neq P_2\sim \cD _1}\E _{X_c}\left [\Rad _{X_c}(\cH _f)\right ] + 3\sqrt {\frac {\log (4n)}{2n}} + \frac {1}{2n}. \end {aligned} 
(11)
Finally, since for any given distribution P, we have g∗◦P ∼D1 for P ∼D2, by Proposition 5
in [18], we obtain that
  \begi n { ali g n e d}  \ma thb b  {
E}_{ P
_1
\
n e
q  P_ 2 } \ lef t [
L
_ P(e_P \c irc f \c
irc  \ti g \c irc 
g^*,y)\right ] &~=~ \mathbb {E}_{\hP _1\neq \hP _2}\left [L_{\hP }(e_P \circ f \circ \tig , \hiy )\right ] \\ &~\leq ~ \frac {16 V_{f\circ \tig }(\hP _1,\hP _2) }{s(f\circ \tig ,\hP _1,\hP _2)}. \end {aligned} 
(12)
D.2
Case 2
In the previous section, we assumed existence of a mapping g∗∈G′
1, such that g∗◦Pc ∼D2
for Pc ∼D1. However, this assumption is typically violated in practice [88, 89, 90]. Therefore,
following the Unsupervised Domain Adaptation literature [81, 82], we use a relaxed assumption that
there is a ‘shared representation space’ for both domains. Informally, the two domains can be mapped
to a shared space, in which classification into classes is possible. Variations of this assumption are
algorithmically and theoretically used in multiple areas of computer vision [91, 92, 93, 67, 94, 95].
Formally, we assume the existence of two mappings g∗∈G1 and ˜g∗∈G2 that satisfy g∗◦Pc ≈˜g∗◦ˆPc
in expectation over Pc ∼D1. To formalize this assumption, we make use of the notion of discrep-
ancy [81, 96]. Namely, for a given set V of functions v : X →R, we define the discrepancy between
two distributions P1 and P2 over X, as discV(P1, P2) = suph∈V |Ex∼P1[h(x)] −Ex∼P2[h(x)]|. The
26

discrepancy, or Integral Probability Metric (IPM) [97], is a pseudo-metric between distributions.
Namely, we (implicitly) assume that EPc∼D1
h
discV(g∗◦Pc, ˜g∗◦ˆPc)
i
is small, where V is some
class of binary functions (to be defined in the proof).
To capture the intuition that one can classify the samples into classes from the representation space
g∗(X) ≈˜g∗(X ′), we assume that the following term
  \E _{
P
_1\
neq P_2} \l ef t  [ \in f _ { (
e , f )  \in \cE
 \times \cF } L_P(e \circ f \circ g^*, y) + L_{\hP }(e \circ f \circ \tig ^*, \hiy ) \right ] 
(13)
is small. In words, in expectation over the selection of P and ˆP, the correct labels y(x) and ˆy(x) can
be simultaneously recovered using classifiers e ◦f ◦g∗and e ◦f ◦˜g∗, for some e ◦f ∈E ◦F.
Finally, as a technicality, we assume that the pretrained adaptor ˜g can be represented as ˜g = u ◦˜g∗,
for some function u ∈U, where U ⊂{u′ : Rp1 →Rp1}, such that, U ◦G1 ⊂G1. For example,
if G1 is a class of neural networks, ending with a linear layer and U is the set of linear mappings
u : Rd1 →Rd1, then indeed we have U ◦G1 ⊂G1.
Theorem 2. In the setting above. For any pair (f, ˜g) ∈F × G, such that ˜g = u ◦˜g∗, we have:
  \beg i n { a
lign e
d} L_
{
\cD _1 }(f)  &\l
eq 1 6\E  _{\ tP _
1
 \neq \tP _
2  \sim \math
c al {D}_2}\le f t [ \fr a c {V_ { f
 
\circ \
ti
g  
}(\tP _i,\tP _j)}{s(f \circ \tig , \tP _1, \tP _2)}\right ] + 2\E _{P_c}\E _{X_c \sim P^{n}_c}[\Rad _{X_c}(\cH _f)] \\ &\quad + \E _{P_c}[\textnormal {disc}_{\cV }(g^* \circ P_c, \tig ^* \circ \hP _c)] + 3\sqrt {\frac {\log (4n)}{2n}} + \frac {1}{2m} \\ &\quad + \E _{P_1\neq P_2} \left [\inf _{(e, f) \in \cE \times \cF } L_P(e \circ f \circ g^*, y) + L_{\hP }(e \circ f \circ \tig ^*, \hiy ) \right ], \end {aligned} m
where s(f, P1, P2) = p2 if f ◦P1 and f ◦P2 are spherically symmetric and s(f, P1, P2) = 1
otherwise.
The above theorem provides an upper bound on the expected error of the pretrained feature map f
against binary classification target tasks T = (P, y). In this theorem, we assume that f has been
trained along with an adaptor ˜g that can be represented as u ◦˜g∗, where u is a linear mapping.
The upper bound is decomposed into multiple parts. Similar to the previous bound it sums the
expected CDNV between pairs of classes ˜P1, ˜P2, the Rademacher complexity of Hf and additional
terms scaling as O(
p
log(n)/n). As discussed in Section D.1, we expect these terms to be small.
In addition, the bound also includes the expected discrepancy between g∗◦Pc and ˜g∗◦ˆPc that
measures to what extent the two adaptors g∗and ˜g∗can map the distributions Pc and ˆPc to the same
space. Finally, the last term measures to what extent we can actually recover the class label from
representations taken from the shared space g∗◦Pc ≈˜g∗◦ˆPc. As mentioned above, we explicitly
assume that these terms are small. Intuitively, these terms are small when the two domains share
a mutual semantic space g∗◦Pc ≈˜g∗◦ˆPc that encodes the content within samples from the two
domains.
The proof of this theorem is based on the analysis of [18], the theory of Unsupervised Domain
Adaptation [81, 82, 83] and Rademacher complexities [86].
Proof. Let (ˆe, ˆf) ∈E × F be any pair of functions. Let u be a linear mapping, such that, ˜g = u ◦˜g∗.
To prove this theorem, we start by considering a pair of target classes P1, P2. Since the loss function
is bounded in [0, 1], for any c = 1, 2, by (cf. [86], Theorem 3.3) with probability at least 1 −
1
4m over
the selection of Sc, for any pair (e, g) ∈E × G, we have
  \be g i n  { al i gned}  L _ {P _c } (
e
 \circ f \circ g, y) &~\leq ~ L_{X_c}(e\circ f \circ g, y) + 3\sqrt {\frac {\log (4m)}{2m}} + 2\Rad _{X_c}(\cH _f). \end {aligned} m
(14)
By union bound over c = 1, 2, with probability at least 1 −
1
2m over the selection of S = S1 ∪S2,
for any pair (e, g) ∈E × G, we have
  \b e g i n {a l igne d }  L_ {P } (
e
 \circ f \circ g, y) &~\leq ~ L_{S}(e\circ f \circ g, y) + 3\sqrt {\frac {\log (4m)}{2m}} + \sum _{c=1,2}\Rad _{S_c}(\cH _f). \end {aligned} m
(15)
27

Hence, with probability at least 1 −
1
2m over the selection of S,
  \be g i n  {a li g ned} L _ P (e_ S \ c
i
rc f \circ g_S, y) &~\leq ~ L_S(e_S \circ f \circ g_S, y) + 3\sqrt {\frac {\log (4m)}{2m}} \\ &~~\quad + \sum _{c=1,2}\Rad _{S_c}(\cH _f). \end {aligned} m
(16)
Let E′ =

e(z) = (−∥z −µc∥2)c=1,2 = (⟨z, 2µc⟩−∥µc∥2)c=1,2 | µ1, µ2 ∈Rp2	
⊂E. In addi-
tion, we let e ˆ
P (z) = (−∥z −µf◦˜g( ˆPc)∥2)c=1,2. We notice that
  \be g i n  {a li g
ned
} L_ S(e
_S \ circ  f  \c ir
c
 g_
S, y ) &~ = ~  \ i nf _{
g  \in  
\ c G  _ 1 } \ inf _{e \in \cE '} L_S(e \circ f \circ g, y) \\ &~\leq ~ \inf _{e \in \cE '} L_S(e \circ f \circ u \circ g^*, y) \\ &~\leq ~ L_S(e_{\hP } \circ f \circ u \circ g^*, y). \end {aligned} 
(17)
Since the loss function is bounded in [0, 1] and the above event holds with probability at least 1 −
1
2m,
by taking expectation over S, we have the following
  \be gin  { a lig ned }  \ E _
{ S } \ l e ft [L _ P
(
e_S \circ f \circ g_S, y)\right ] &\leq L_P(e_{\hP } \circ f \circ u \circ g^*, y) + 3\sqrt {\frac {\log (4m)}{2m}} + \frac {1}{2m} \\ &\quad + \sum _{c=1,2}\E _{S_c}\left [\Rad _{S_c}(\cH _f)\right ] \\ \end {aligned} m
(18)
In addition,
  \b e
g i n  { a lig ne d } L_ P
( e _ { \ h P }  \ c ir c  f 
\ ci rc u  \ c irc  g
^ * ,
 y)  
& ~ \ l e q  ~ L _P ( e_ { \hP 
}  \ cir c  f  \ci rc
 u \circ g ^* , \ h at 
{ e }
 \c i
r c  \ h a t {f } \
c i r
c  g^ * ) \ \ &~ ~\q u ad  + L _P ( \ha t 
{ e} \circ  \h at { f} \circ g^*, y) \\ &~\leq ~ L_{\hP }(e_{\hP } \circ f \circ u \circ \tig ^*, \hat {e} \circ \hat {f} \circ \tig ^*) \\ &~~\quad + L_P(\hat {e} \circ \hat {f} \circ g^*, y) \\ &~~\quad + \textnormal {disc}_{\cV }(g^* \circ P, \tig ^* \circ \hP ) \\ &\leq L_{\hP }(e_{\hP } \circ f \circ u \circ \tig ^*, \hiy ) \\ &~~\quad + L_{\hP }(\hat {e} \circ \hat {f} \circ \tig ^*, \hiy ) + L_P(\hat {e} \circ \hat {f} \circ g^*, y) \\ &~~\quad + \textnormal {disc}_{\cV }(g^* \circ P, \tig ^* \circ \hP ), \end {aligned} 
(19)
where V = {I[e1 ◦f1 ◦u1 ̸= e2 ◦f2] | e1, e2 ∈E, f1, f2 ∈F, u1 ∈U}. In particular, we can take
infimum over (ˆe, ˆf) ∈E × F to obtain
  \b e
g i n  { a lig ne d }  
L _P (
e _ { \ h P  } \ cir
c  f 
\ci
r
c  
u  \ c i r c g^ *, y ) &~ \ l e q ~  L
_
{ \hP }(e_ { \h P }  \ci
r c  
f  \ c
i r c  u \ cir
c  \t
ig 
^
* ,
 \h i y  ) \\  &~ ~ \q ua d  +  \i nf
 
_ {e,f} \l e ft  [L _ {\hP }(e \circ f \circ \tig ^*, \hiy ) + L_P(e \circ f \circ g^*, y)\right ] \\ &~~\quad + \textnormal {disc}_{\cV }(g^* \circ P, \tig ^* \circ \hP ) \\ &~=~ L_{\hP }(e_{\hP } \circ f \circ \tig , \hiy ) \\ &~~\quad + \inf _{e,f} \left [L_{\hP }(e \circ f \circ \tig ^*, \hiy ) + L_P(e \circ f \circ g^*, y)\right ] \\ &~~\quad + \textnormal {disc}_{\cV }(g^* \circ P, \tig ^* \circ \hP ). \end {aligned} 
(20)
28

Next, we can take expectation over the selection of P on both sides of the inequality to obtain that
  \begi
n
 { al i
g n e d }  &\E  _
{
P _1\neq 
P
_ 2
}  \ l
e f t  [L_ P(e
_
{ \hP } \
c
irc
 f 
\
c i
r c u  \ circ  g^ * , y) \ r i ght  ]
 \
\  &~\leq
 
~ \E _{P _ 1\ neq  P_2
}
 \ l
eft [
L_
{
\ h
P  } (
e _ { \ hP } \
c
i rc f \c
i
rc 
\ti
g
 ,
 \h i y  )\ri ght  ] \\  & ~ ~\q ua
d 
+  \E _{P
_
1\neq P_ 2 } \le f t [
\inf _{e,f} \left [L_{\hP }(e \circ f \circ \tig ^*, \hiy ) + L_P(e \circ f \circ g^*, y)\right ]\right ] \\ &~~\quad + \E _{P_1\neq P_2} \left [\textnormal {disc}_{\cV }(g^* \circ P, \tig ^* \circ \hP )\right ] \\ &~=~ \E _{\hP _1\neq \hP _2} \left [L_{\hP }(e_{\hP } \circ f \circ \tig , \hiy )\right ] \\ &~~\quad + \E _{P_1\neq P_2} \left [\inf _{e,f} \left [L_{\hP }(e \circ f \circ \tig ^*, \hiy ) + L_P(e \circ f \circ g^*, y)\right ]\right ] \\ &~~\quad + \E _{P_1\neq P_2} \left [\textnormal {disc}_{\cV }(g^* \circ P, \tig ^* \circ \hP )\right ] \end {aligned} 
(21)
We note that
  \begin  {a lig n ed}
 &\t
ext
normal {di sc}_{\ c V }(g^*  
\ circ P
, \
t ig 
^* 
\circ
 
\
h
P ) \
\ &~=~ \sup _{ v  
\
i
n \cV
 } \lef t
 \vert \
E _{z
 \
s
i
m g^*
 \c
irc
 P}[v(z)] - \E _{ z  \sim \ t
ig ^* \c
irc
 \
h
P
 }[v(
z)] \rig h t \ ver t  \\ &~=~ \sup _{v \in \cV } \left \vert \frac {1}{2}\sum _{c=1,2}\E _{z \sim g^* \circ P_c}[v(z)] - \frac {1}{2}\sum _{c=1,2}\E _{z \sim \tig ^* \circ \hP _c}[v(z)] \right \vert \\ &~\leq ~ \frac {1}{2}\sum _{c=1,2}\sup _{v \in \cV } \left \vert \E _{z \sim g^* \circ P_c}[v(z)] - \E _{z \sim \tig ^* \circ \hP _c}[v(z)] \right \vert \\ &~\leq ~ \frac {1}{2}\sum _{c=1,2}\textnormal {disc}_{\cV }(g^* \circ P_c, \tig ^* \circ \hP _c). \end {aligned} 
(22)
Hence,
  \E _{
P
_1\neq P _ 2}  \l e ft 
[
\ textno
r
mal {dis c }_{ \cV  }(g^
*
 \circ P, \tig ^* \circ \hP )\right ] ~\leq ~ \E _{P_c \sim \cD _1} \left [\textnormal {disc}_{\cV }(g^* \circ P_c, \tig ^* \circ \hP _c)\right ]. 
(23)
Finally, by Proposition 5 in [18] we obtain that
  
\ be g
i n  { ali gne d } L_{\hP  }(e _{\h
P }  \ci rc f  \ci
rc \tig , \hiy ) ~\leq ~ \frac {16V_{f \circ \tig }(\hP _1, \hP _2)}{s(f \circ \tig , \hP _1,\hP _2)}. \end {aligned} 
(24)
29

