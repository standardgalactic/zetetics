Similarity is Not All You Need: Endowing Retrieval-Augmented
Generation with Multi–layered Thoughts
Chunjing Gan
Dan Yang
Binbin Hu
Hanxiao Zhang
Siyuan Li
Ziqi Liu
Yue Shen
Lin Ju
Zhiqiang Zhang
Jinjie Gu
Lei Liang
Jun Zhou†
Ant Group
jun.zhoujun@antgroup.com
Abstract
In recent years, large language models (LLMs)
have made remarkable achievements in var-
ious domains.
However, the untimeliness
and cost of knowledge updates coupled with
hallucination issues of LLMs have curtailed
their applications in knowledge-intensive tasks,
where retrieval-augmented generation (RAG)
can be of help. Nevertheless, existing retrieval-
augmented models typically use similarity
as a bridge between queries and documents
and follow a retrieve-then-read procedure.
In this work, we argue that similarity is
not always the “panacea” and totally relying
on similarity would sometimes degrade the
performance of retrieval-augmented genera-
tion. To this end, we propose METRAG, a
Multi–layEred Thoughts enhanced Retrieval-
Augmented Generation framework.
To be-
gin with, beyond existing similarity-oriented
thought, we embrace a small-scale utility
model that draws supervision from an LLM
for utility-oriented thought and further come
up with a “smarter” model by comprehen-
sively combining the similarity- and utility-
oriented thoughts.
Furthermore, given the
fact that the retrieved document set tends to
be huge and using them in isolation makes
it difficult to capture the commonalities and
characteristics among them, we propose to
make an LLM as a task-adaptive summa-
rizer to endow retrieval-augmented generation
with compactness-oriented thought. Finally,
with multi-layered thoughts from the prece-
dent stages, an LLM is called for knowledge-
augmented generation. Extensive experiments
on knowledge-intensive tasks have demon-
strated the superiority of METRAG.
1
Introduction
In recent years, large language models (LLMs)
such as ChatGPT (OpenAI, 2023a), GPT4 (Ope-
nAI, 2023b), Llama (Touvron et al., 2023) have
made remarkable achievements in a variety of
tasks due to their marvelous capability in language
comprehension and generation (Zhou et al., 2023).
However, the untimeliness and cost of knowledge
updates (Kasai et al., 2022) together with halluci-
nations issues of LLMs (Rawte et al., 2023) have
curtailed their applications in knowledge-intensive
tasks to a large extent (Mallen et al., 2023), where
retrieval-augmented generation (RAG) approaches
that prepending documents to the query without
updating the underlying language models would
come in handy (Ram et al., 2023; Asai et al., 2023a).
Nevertheless, existing retrieval-augmented gen-
eration approaches are typically similarity-based
(Jiang et al., 2023), i.e., they retrieve documents
from external corpus based on similarity. Then,
the retrieved documents are prepended as con-
text for LLMs at once or independently for gen-
eration augmentation (Shi et al., 2023). In total,
these approaches have been found to outperform
purely parametric LLMs (Jiang et al., 2023), es-
pecially in some knowledge-intensive generation
tasks (Kwiatkowski et al., 2019).
In this work, we argue that similarity is not al-
ways the “panacea” for retrieval-augmented gener-
ation and totally relying on similarity would some-
times degrade the performance. As is shown in the
upper part of Figure 1, when a user types in the
query “Tell me about author George RR Martin”,
a similarity-driven retrieval system would rank the
documents in a given corpus according to simi-
larity metrics, i.e., the semantic relevance or TF-
IDF based metric (Robertson and Zaragoza, 2009).
Even though the retrieved documents barely pro-
vide useful information, e.g., “George RR Martin
is an author”, it would rank higher due to high
similarity score and the document that states the
publications “The Song of Ice and Fire” of George
RR Martin with higher information gain would
rank lower due to inadequate low similarity score.
Besides, given the fact that the retrieved documents
are often more than one, using them in isolation due
to the context limitation of LLMs (Shi et al., 2023)
arXiv:2405.19893v1  [cs.LG]  30 May 2024

or simply aggregating the Top-k document without
considering the relationships between them makes
it difficult to capture the commonalities and char-
acteristics among them and even confuse LLMs
due to excessive text length thus incurring infor-
mation loss and probably performance degradation
(Mallen et al., 2023).
A Similarity Model
A Utility Model
Tell me about author
George RR Martin.
1. George RR Martin is an 
author. [0.71]
2. The book set The Song 
of Ice and Fire, is one of 
George RR Martin's most 
famous books. [0.63]
1. The book set The Song 
of Ice and Fire, is one of 
George RR Martin's most 
famous books. [0.85]
2. George RR Martin is an 
author. [0.61]
A Smarter Model
Figure 1: A toy example illustrating the difference be-
tween similarity and utility, where the score of similarity
model is given by BGE1. Can we reunite the virtues of
both worlds and come up with a better model?
Given the above limitations in current ap-
proaches, beyond similarity we aim to endow
retrieval-augmented generation with multi-layered
thoughts (i.e., utility- and compactness-oriented
thoughts) for performance boosting. However, the
solution is quite non-trivial, which needs to tackle
the following essential challenges: (C1) To train a
model that is capable of perceiving utility-oriented
thoughts rather than solely similarity, external la-
beled data is required. However, it is hard to obtain
external labeled data for guiding the learning pro-
cess with explicit supervision. Though LLMs can
serve as data annotators and come up with high-
quality corpus for model training and have demon-
strated their tremendous capability in many cir-
cumstances, the innate uncontrollable and instabil-
ity characteristics would sometimes deteriorate the
performance. (C2) With high-quality retrieved doc-
uments, to reduce the burden that dozens of docu-
ments impose on LLMs and better capture the com-
monalities and characteristics between retrieved
documents, document summarization is plausible.
However, simple summarization cannot guarantee
that the most important information w.r.t. the input
query can be retained, hence there is need to train a
summarization model that aligns with the task itself
thus possessing compactness-oriented thoughts.
To this end, we propose METRAG. In particular,
with an LLM serves as supervision on document
utility w.r.t. the input query, we come up with a
utility model that aligns itself with the LLM’s feed-
1https://github.com/FlagOpen/FlagEmbedding
back such that beyond similarity it is also equipped
with utility-oriented thoughts. Considering that an
LLM is strong most of time but sometimes would
go out of control, yet dense retrievers trained on
labelled corpus remain stable with relevance guar-
antee, though sometimes useless, we further com-
bine similarity- and utility-oriented thoughts and
takes the output of a similarity model and a utility
model into consideration (C1), as depicted in Fig-
ure 1. Furthermore, to endow the summarization
model with compactness-oriented thoughts, we first
distill the summary ability from a strong teacher
model (e.g., GPT4). Afterwards with multiple gen-
erated summaries, a subsequent reward model is uti-
lized to further constrain the summarization model
to align with the end task (C2). With the multi-
layered thoughts derived from the precedent stages,
an LLM is called for knowledge-augmented gener-
ation. At last, we evaluate the proposed METRAG
on multiple knowledge-extensive tasks, extensive
experiments and analysis have demonstrated the
superiority of the proposed method.
2
Related work
2.1
Retrieval-Augmented Generation
Retrieval-Augmented
Generation
(RAG)
ap-
proaches (Gao et al., 2023; Asai et al., 2023a; Ram
et al., 2023) tend to enhance LLMs following a
retrieve-then-read pipeline when given the input
query, which first retrieve a set of documents
from the external corpus, and then utilize the
retrieved documents as side information for an
LLM to make the final prediction. This pipeline
has demonstrated their superior performance, espe-
cially in knowledge-intensive tasks by fine-tuning
or directly prepending to LLMs (Hu et al., 2022;
Izacard et al., 2023). However, the essence of
RAG is the quality of retrieved passages, yet plenty
of existing approaches rely on similarity-based
retrieval (Xu et al., 2023; Mallen et al., 2023) that
tend to ignore the underlying utility of associated
passages.
Though Self-RAG, REPLUG (Asai
et al., 2023b; Shi et al., 2023) have made progress
in introducing the power of LLMs in augmenting
retrieval ability and even achieve adaptive retrieval,
purely depending on the capability of LLMs can
be dangerous since the innate “uncontrollable”
characteristics of LLMs would sometimes degrade
the performance. Furthermore, they tend to ignore
the potential inner relationships among retrieved
passages and utilize them in isolation, which

deteriorate the performance.
2.2
Task-Oriented Summarization
Large language models (LLMs) (Zhao et al., 2023;
OpenAI, 2023a,b) have made remarkable achieve-
ments in a variety of domains such as question
answering, and summarization. However, calling
large-scale commercial LLMs are of high cost and
may induce data leakage issue, hence many ap-
proaches have been devoted to distilling the abil-
ity of large-scale LLMs (e.g., ChatGPT (OpenAI,
2023a)) to small-scale LLMs (e.g., Llama2 (Tou-
vron et al., 2023)) and enhance their capabilities
for downstream tasks, e.g., generate high-quality
summary through distillation (Jung et al., 2023).
However, for knowledge-intensive tasks, simple
summarization is far from optimized because it
cannot ensure that the most important information
relevant to the input query is retained, hence it is
necessary to generate the summary associated with
downstream tasks. Though RECOMP (Xu et al.,
2023) has made its step to train the summarization
model for enhancing performance in tasks such as
question answer and language modeling, it purely
designs intricate samples and performs distillation
without further aligning strategy for performance
boosting, which downgrades its performance in
applications.
3
The proposed approach
3.1
Overview
In this work, we propose METRAG, as shown in
Figure 2. METRAG endows retrieval-augmented
generation with multi-layered thoughts by firstly
embracing LLM’s supervision for utility-oriented
thoughts and combining similarity and utility of
documents for performance boosting (I, detailed
in Section 3.2) and further pursuing compactness-
oriented thoughts via a task-adaptive summarizer
(II, detailed in Section 3.3), finally incorporating
the derived multi-layered thoughts for answer gen-
eration (III, detailed in Section 3.4).
3.2
A Tale of Two “Models”
3.2.1
Similarity Model as an Off-the-shelf
Retriever
Given an input query q, an off-the-shelf dense re-
triever R is incorporated to map the input to a
low-dimensional embedding E(·) such that we can
efficiently retrieve relevant documents w.r.t. query
q from a given corpus D = {d1, d2, ..., dn} via a
predefined similarity metric M (which could be
cosine similarity and so on), where the similarity
score between the query and document can be com-
puted as follows:
s(q, d) = M(E(q), E(d))
(1)
With the derived similarity scores, the Top-k doc-
uments D∗that have the highest similarity scores
w.r.t. input query q are retrieved for enhancing
follow-up tasks.
3.2.2
LLM’s Supervision Empowered Utility
Model
In this work, we argue that similarity is not always
the “panacea” for information retrieval and totally
relying on similarity would sometimes degrade the
performance of retrieval-augmented generation ap-
proaches. As is illustrated in Figure 1, document
with higher information gain to the input query
would rank lower due to inadequate low similarity
score. Inspired by the great success of LLMs in a
variety of tasks, we aim to incorporate an LLM for
supervision on document utility (In this paper, we
define the utility of one document w.r.t. a question
by its usefulness in assisting an LLM to answer this
question, which is modelled by the normalization
of the probability of generating correct answers
with a specific LLM.) w.r.t. the input query to en-
dow the retriever R with utility-oriented thoughts.
To start with, given the Top-n documents set D•
(D• ⊆D∗) that a retriever R considers most simi-
lar to the input query q, the approximated similarity
likelihood of each document d can be formalized
as follows:
PR(d|q) =
eM(q,d)/τ
P
d∈D• eM(q,d)/τ ,
(2)
where τ is the temperature hyperparameter that
controls the smoothness of probability distribution,
with a higher τ produces a softer probability dis-
tribution while a lower τ results in a "harder" max
operation and D• denotes the documents pool for
model training. To endow the retriever R with
LLM’s insights on retrieval utility, we further in-
corporate an LLM as the supervision signal on doc-
ument utility of d w.r.t. the input query q such that
beyond similarity the trained utility model can take
the utility a document provides into consideration,
which we can formalize as follows:
PU(d|q, y) =
ePLLM(y|q,d)/τ
P
d∈D• ePLLM(y|q,d)/τ .
(3)

Answer
Generation
(SFT) LLM
III: Knowledge-augmented 
Generation
User 
Query
Knowledge
(from former stages)
II: Pursuit of Compactness-oriented Thoughts
I: Reunion of Similarity- and Utility-oriented Thoughts
Task-adaptive 
Summarizer
(Trainable)
Smarter Model
Utility Model 
(Trainable)
Similarity Model 
(Frozen)
User Query
User 
Query
Collected 
Info
+
+
Summary
Teacher Model
Distill!
SFT 
Model
User 
Query
+
Summary
SFT Model
Reward 
Model
Reward
Align!
External 
Corpus
Search
Info
Call for Supervision
KL Optimization!
LLM
(Frozen)
Utility Model 
Similarity Model 
Relevant, 
sometimes useless
Strong, sometimes 
unreliable
Combine them!
Figure 2: The proposed METRAG framework, where we endow retrieval-augmented generation with multi-layered
thoughts from Stage I and II, and utilize the derived knowledge in Stage III for answer generation.
Finally, we break down the barriers of both sides
and push the similarity distribution toward LLM-
supervision enhanced utility distribution by min-
imizing the KL-divergence of these two distribu-
tions as follows:
LU =
1
|Q|
X
q∈Q
KL(PR(·)d∈D•||PU(·)d∈D•),
(4)
where Q is a set of queries. Due to the reason that
the LLM is served as the external supervision, dur-
ing training the parameter of the LLM is frozen and
we only update the parameters of retriever R and
finally come up with the utility model U, thus the
overall optimization process remains lightweight.
Besides, we further add an empty string es to D•
in the training process to enable the utility model
with the ability to judge whether introducing a doc-
ument for a query can promote the utility or not
thus achieving selective retrieval.
3.2.3
Reunion of Similarity- and Utility-
oriented Thoughts
Considering that an LLM would sometimes go out
of control and produces unreliable supervision sig-
nals which degrade the model performance some-
times yet dense retrievers trained on accurately la-
belled corpus remain stable with relevance guaran-
tee, though sometimes useless, we further combine
similarity- and utility-oriented thoughts and take
the output of a similarity model into consideration
via an integration strategy I(·). Therefore, the final
score between document d and the input query q
can be defined as follows:
f(q, d) = I(R(q, d), U(q, d))
=





1.0,
R(q, d) ≥ζR(kR)
1.0,
U(q, d) ≥ζU(kU)
0.0,
otherwise
(5)
where ζR(kR) and ζU(kU) find value of the kR-
largest similarity score and kU-largest utility score
among retrieved documents through similarity and
utility, respectively. Finally, only documents with
f(q, d) = 1.0 are permitted to proceed to the sub-
sequent stages. With the derived final score, we can
easily obtain the current documents set D∆w.r.t.
input query q for follow-up tasks.
3.3
Pursuit of Compactness-oriented
Thoughts
Given the fact that the retrieved documents are of-
ten more than one, using them in isolation due to
the context limitation of LLMs or simply aggre-
gating the Top-k document without considering
the relationships between them makes it difficult
to capture the commonalities and characteristics
among them and even confuse LLMs thus incur-
ring information loss and probably performance

degradation, where text summarization can be of
help. However, simple summarization cannot en-
sure that the most important information relevant
to the input query is retained, therefore it is nec-
essary to train a summarization model that aligns
with the task itself. Hence, we propose the Task-
adaptive Summarizer, which not only reduces the
computational costs in end tasks but also relieves
the burden of LLMs to identify relevant informa-
tion in a large chunk of retrieved documents to
obtain compactness-oriented thoughts.
3.3.1
Distilling from Strong Teacher Models
To initiate the summarization process, we design
instructions via randomly sampled queries and re-
trieved documents. To start with, given query q,
we harness the expertise of a sophisticated teacher
model T , e.g., GPT4, to extract summarization
proficiencies that facilitate the creation of a prelim-
inary summary Sq and compile an initial corpus CI
consisting of Instruction-Summary pairs

Is
q, Sq

,
where Is
q and Sq are defined as follows:
Is
q = TEMPLATE(Is, q, [d1
q, d2
q, · · · , dn
q ]),
(6)
Is
q
T−→Sq,
(7)
where
Is
is
the
summarization
instruction,
TEMPLATE is the summarization prompt template
(detailed in Section A.1) and di
q is the i-th docu-
ment retrieved for query q from former stage. Next,
we apply Lora tuning to meticulously refine an
open-source student model, e.g., Llama2, which re-
sults in an initial summarizer model πSFT tailored
for the end task.
3.3.2
Alignment via End-task Feedback
To ensure the faithfulness of the summarizer to the
end task, inspired by the principles of the DPO
(Rafailov et al., 2023), we incorporate the LLM’s
performance on the end task as reward for the
summarizer. With model πSFT, given query q we
adeptly produce concise summaries of the retrieved
documents as Sq and generate prompt Xq to obtain
end task response Yq and the label Zq indicating
whether the response is correct or not, which ac-
cords with the following distribution:
p(Zq = 1|Xq, Yq) =
1
1 + e−r∗(Xq,Yq) .
(8)
Formally, we define the training corpus of this
aligning process as CA consisting of triplets
(Xq, Yq, Zq) for each query q. With a reward model
rϕ parameterized by ϕ to estimate r∗, the binary
classification loss can be defined as follows:
LRM(ϕ) =
X
(Xq,Yq,Zq)∈CA
−Zqlogσ(rϕ(Xq, Yq))−
(1 −Zq)logσ(1 −rϕ(Xq, Yq)),
(9)
where σ is the sigmoid function. Furthermore, we
follow the concept of DPO that eschews the need to
explicitly estimate the reward model by solving rϕ
as a function of language model policy πθ, which
is formalized as follows:
rϕ(Xq, Yq) = βlog πθ(Yq|Xq)
πSFT(Yq|Xq).
(10)
3.4
Knowledge-augmented Generation
With the input query q and the external knowledge
K derived from the former stages, we can directly
call an LLM (can be fine-tuned in a supervised
manner using question answering datasets), where
its knowledge-augmented generation of answer a
can be formalized as follows:
a∗= arg max
a
P(a|q, K),
(11)
where P(a|q, K) is the probability of the answer
a given the query q and the external knowledge
K, and arg max denotes the argument of the max-
imum, i.e., the answer a for which P(a|q, K) is
maximized.
4
Experiments
4.1
Experimental Setup
4.1.1
Tasks and Datasets
We evaluate our proposed METRAG and multiple
baselines on a variety of knowledge-intensive pub-
lic datasets (including general Open-Domain QA:
NQ, TriviaQA-unfiltered2, HotpotQA and entity-
centric QA datasets: PopQA3) and evaluate the
performance via metrics EM (following (Mallen
et al., 2023), we evaluate the performance based
on whether the gold answers are included in the
model generations rather than strictly exact match-
ing) and F1. All experiments are conducted in a
zero-shot manner, where we provide instructions
and retrieved information about tasks without few-
shot demonstrations (Wei et al., 2022).
2As the test set is not publicly available, we follow the split
setting of (Guu et al., 2020) and use 11,313 queries for testing.
3We test on the long-tail subset that includes 1399 queries
with less than 100 Wikipedia page views.

4.1.2
Baselines
Baselines
without
retrievals.
We
evalu-
ate strong publicly available pre-trained LLMs,
ChatGLM26B (Du et al., 2022), Llama27B,13B (Tou-
vron et al., 2023), Baichuan7B,13B (Baichuan, 2023)
and Qwen7B,14B (Bai et al., 2023); and models
trained and reinforced using private data, Chat-
GPT4 (OpenAI, 2023a).
Baselines with retrievals. We evaluate models
augmented with retrieval only at test time or dur-
ing training. The first category includes standard
RAG baselines, where an LM (e.g., Llama27B,13B)
generates output given the query prepended with
the top retrieved documents. The latter category
includes approaches that are trained with retrieved
passages, including Self-RAG (Asai et al., 2023b),
RECOMP (Xu et al., 2023)5.
4.1.3
Settings
Training details.
We split all train, validation
and test sets of datasets following (Izacard et al.,
2023; Asai et al., 2023b). Our training data in-
cludes randomly sampled instruction-following
input-output pairs.
We randomly sample 50k
pairs from training sets of NQ, TriviaQA and Hot-
potQA to train our model in different stages with
Llama213B as base LLM. All experiments are con-
ducted using 4 NVIDIA A100 GPUs. We train the
utility model for 5 epochs with a learning rate of
1e-5, a batch size of 16 for each device, a warm-up
ratio of 0.2, the passage window size of 50 and
the temperature parameter τ set to 0.05. For task-
adaptive summarizer and generation model, we uti-
lize open-source Llama Factory 6 and adopt Lora
tuning for 1 epoch with a learning rate of 5e-5, a
batch size of 4 and a cosine learning rate scheduler.
Retrieval details.
For NQ, TriviaQA and Hot-
potQA, we use 2018 English Wikipedia as the ex-
ternal retrieval source while for PopQA, since the
2018 Wikipedia sometimes lacks articles about en-
tities that have been added to Wikipedia recently,
we use December 2020 preprocessed Wikipedia
corpus as the external retrieval source. We prepro-
cess the external corpus following (Izacard et al.,
2023) 7. As for retriever R, we incorporate BGE
owing to its superior performance in a variety of
benchmark leaderboards and retrieve up to 5 docu-
4We use GPT-3.5-turbo-16K in our experiment.
5We report results reported in the original paper.
6https://github.com/hiyouga/LLaMA-Factory
7https://github.com/facebookresearch/atlas
ments for each input query for testing, where M(·)
is defined as cosine similarity.
4.2
Results and Analysis
4.2.1
Main Results
From
the
empirical
results
across
multiple
knowledge-intensive datasets, the major findings
can be summarized as follows:
• Retrieval
largely
improves
performance.
When compared against approaches without
retrieval, the retrieval-augmented approaches
demonstrated the superiority in EM metric even
when compared to the strong baseline ChatGPT,
showing the tremendous power and potential of
retrieval-augmented generation. Besides, with
retrieval-augmentation, even small-scale LLMs
(e.g., 7B LLMs) can achieve comparable perfor-
mance w.r.t. large-scale LMs (e.g., 13B LLMs)
in terms of EM metric, showing their power in
pursuit of true knowledge.
• Long-tail queries benefit more from retrieval.
As depicted in Table 1, retrieval-augmented
approaches achieve most performance gain in
PopQA dataset with long-tail queries while the
large-scale competitive LLM ChatGPT performs
worst in this dataset due to the untimely updated
knowledge, which illustrates that performance
degradation due to knowledge updating issues
can be alleviated to a great extent by retrieval.
• Supervised Fine-tuning improves instruction
following. With regard to F1 metric, we find that
approaches trained with retrieved passages per-
form better, showcasing their ability in instruc-
tion following for abstracting concise answers
(which is the case of ChatGPT with remarkable
instruction following ability for zero-shot tasks).
However, for approaches without supervised fine-
tuning, there exists a seesaw effect between EM
and F1 with one focuses on answer accuracy
while other focuses on the balance between ex-
actness and conciseness (which is stated in input
prompts).
• The way of incorporating external informa-
tion matters! Different from approaches that
directly incorporate the retrieved passages for an-
swer generation, our proposed METRAG endows
multi-layered thoughts to “take the essence and
discard the dross” so that the most useful infor-
mation of retrieved passages can be abstracted

Table 1: Overall performance evaluation across 4 public datasets. The best results are highlighted in boldface.
Methods
NQ
TriviaQA
HotpotQA
PopQA
EM
F1
EM
F1
EM
F1
EM
F1
Baselines without retrieval
ChatGLM26B
11.1
8.80
22.5
20.0
13.5
14.6
18.8
14.0
Llama27B
21.5
12.3
43.0
29.0
17.0
10.8
19.6
3.36
Baichuan27B
18.0
14.0
39.3
35.0
18.8
15.9
25.1
17.8
Qwen7B
20.7
17.9
47.3
42.1
21.7
19.6
21.6
15.4
Llama213B
32.4
8.60
58.9
14.6
24.1
8.53
19.0
2.91
Baichuan213B
20.8
20.6
46.2
45.4
17.7
20.5
23.8
16.4
Qwen14B
25.9
26.9
53.4
53.7
24.1
25.6
19.7
18.7
ChatGPT
37.8
42.9
71.4
72.3
27.9
34.2
25.2
25.7
Baselines with retrieval
ChatGLM26B
42.8
28.2
59.1
47.2
30.3
27.1
60.3
43.7
Llama27B
42.2
40.6
65.4
62.5
30.3
33.0
60.3
57.8
Baichuan27B
43.4
34.3
65.0
54.7
33.5
29.1
62.9
47.0
Qwen7B
43.7
31.2
62.1
48.3
30.9
24.8
40.4
27.3
Llama213B
50.3
21.2
70.2
25.7
38.2
17.3
62.9
15.1
Baichuan213B
44.4
41.7
65.1
62.6
30.2
32.4
61.4
53.1
Qwen14B
50.3
47.5
69.4
67.1
39.2
39.9
64.1
57.6
SELF-RAG*7B
-
-
66.4
-
-
-
54.9
-
SELF-RAG*13B
-
-
69.3
-
-
-
55.8
-
RECOMP*EXT
36.6
44.2
59.0
65.3
30.4
40.1
-
-
RECOMP*ABS
37.0
45.5
58.7
66.3
28.2
37.9
-
-
METRAG
49.6
53.9
74.8
78.7
42.2
51.9
58.8
61.5
and distraction information can be dropped for
end tasks. Experimental results in four different
datasets illustrate and verify the rationale of our
proposed METRAG.
4.2.2
Analysis
Ablation study.
We examine the effectiveness
of each component in METRAG by preparing the
following variants: i) METRAG W/O COMB, which
removes the combination of similarity model and
utility model and degrades to the original similar-
ity based retriever; ii) METRAG W/O AS, which
removes the task-adaptive summarization for in-
formation integration. We plot the performance
comparison in Figure 3, from which we can ob-
serve that the overall performance would drop a
lot when either component is discarded, thus the
effectiveness of our dedicate design is verified. In
particular, we find that the METRAG W/O COMB
performs worst among all variants, showcasing that
the augmented information, which serves as the
cornerstone in retrieval-augmented generation, de-
serves more attention.
NQ
TriviaQA
HotpotQA
PopQA
40
50
60
70
80
MetRag
MetRag w/o Comb
MetRag w/o AS
(a) EM
NQ
TriviaQA
HotpotQA
PopQA
40
50
60
70
80
MetRag
MetRag w/o Comb
MetRag w/o AS
(b) F1
Figure 3: Ablation study.
Passage window size matters in utility model
performance!
Since the utility model plays a
key role in bring in the supervision of LLMs into
passage selection process, we take a closer look
at how the passage window size ∥D•∥8 would
influence the final performance by directly incor-
porating the utility model trained under different
settings to the final task. Due to the space limit, we
average the metrics in four datasets and present the
result in Figure 4 and find that the model perfor-
8Here, since the empty string es is added by default, we
only count the passages involved in training.

Table 2: The Impact of LLMs on Utility Modeling.
LLMs
NQ
TriviaQA
HotpotQA
PopQA
EM
F1
EM
F1
EM
F1
EM
F1
Baichuan27B
32.1
36.8
63.5
67.8
25.3
33.2
23.6
25.1
Llama27B
42.1
46.5
69.1
73.5
35.7
45.2
55.7
56.8
Llama213B
45.2
49.8
71.9
76.0
38.3
47.9
55.1
56.4
10
20
30
40
50
45
50
55
60
65
EM
F1
Figure 4: The influence of passage window size.
mance improves as the window size grows, which
demonstrates that the growing passage window size
endows more LLMs’ powers for distinguishing pas-
sage significance among diverse inputs to the utility
model thus improving performance on downstream
tasks. However, due to the computational burden
(the model training time grows linearly with the
passage window size) that a large passage window
size imposes during training, there is need to bal-
ance the trade-off between performance and cost.
Impact of selective retrieval.
When optimizing
the utility model, we add an empty string es in
the training process to leverage the knowledge of
LLMs to achieve selective retrieval. Here, we con-
duct some case studies to examine this mechanism.
As we can see from Table 3, there are example
queries that the utility model deems no retrieval
and in total there are 20.9% queries which the util-
ity model ranks the empty string es higher than
other documents. As we can see from Table 3,
many of the listed queries are commonsense knowl-
edge that has been memorized in LLMs’ parame-
ters and we can easily call an LLM for the required
answer instead of retrieval, which illustrates that in
retrieval-augmented generation, despite the further
knowledge external corpus introduces, the inher-
ent knowledge of LLMs when deploying retrieval-
based techniques is worth investigation and we be-
lieve that our work has provided a straightforward
and plausible solution. However, this design still
has certain limitations, i.e., the LLM used for end
task needs to be the same or stronger than the one
utilized during utility model training, otherwise
this mechanism may not work.
Table 3: Queries Deemed No Retrieval by Utility Model.
Is Cartagena or Chess more popular around the world?
In what country is Chalhuacocha?
What was Ian Fleming’s first Bond book?
When was catch me if you can made?
Cathay is a poetic name for which country?
Who sings sugar sugar you are my candy girl?
Who used to present I'm a celebrity now?
What county is Icknield Walk First School located in?
Where does the cell spend most of its time in the cell cycle?
What country lies to north of the Republic of Chad?
Are Vintage Life and InStyle both US-based magazines?
Who is the first president to be impeached?
Are Andrew Stevens and Charles Burnett both American?
Which Genre of Television did Charles Quinton Murphy act for?
How was President Kennedy assassinated?
Which animal is the carrier of the h1n1 virus?
What part of the body produces insulin?
The impact of LLMs on utility modeling.
To
further analyse the impact of LLMs on utility mod-
eling, beside Llama213B we also add Baichuan7B
and Llama27B as the base LLMs for comparison
by directly incorporating the utility model trained
under different settings to the final task and present
the metrics in four datasets in Table 2, from which
we have two main conclusions: 1) Usually, a large-
scale LLM can outperforms an LLM of smaller-
scale, e.g., Llama213B vs Llama27B. Furthermore,
we also find an interesting phenomenon that some-
times (e.g., PopQA dataset) the score of Llama27B
is higher than that of Llama213B, which is con-
sistent of the findings reported in Self-RAG(Asai
et al., 2023b). 2) Baichuan7B underperforms the
Llama series in our experiments, one possible rea-
son is that it places greater emphasis on its appli-
cation in specific languages such as Chinese, as is
stated in its original paper(Baichuan, 2023).
Investigation into task-adaptive summarizer.
To start with, we introduce a case study to illus-
trate the virtues of our task-adaptive summarizer
in Table 4, as we can see from the table, since the
original information is quite long and with a lot of
distracting information, an LLM tend to get lost in
a large chunk of words. However, with our task-

adaptive summarizer, the most relevant information
for this query is extracted thus an LLM can easily
answer the question given the extracted knowledge.
Table 4: Case Study: Task-adaptive Summarizer Output.
Query
What is Walter de la Pole's occupation?
Original information
Walter de la Pole: Family
Walter was the son and heir of the MP
Sir Edmund de la Pole and his second wife.
Walter de la Pole
Sir Walter de la Pole (November 1371 – 1434), of
Dernford in Sawston, Cambridgeshire, was an English politician.
Richard de la Pole
Richard de la Pole (1480 – 24 February 1525)
was a pretender to the English crown. Commonly nicknamed "White
Rose", he was the last Yorkist claimant to actively and openly seek
the crown of England. He lived in exile...(4203 characters)
Task-adaptive summary
Walter de la Pole was an English politician. (44 characters)
In sum, the virtues of task-adaptive summarizer
are two-fold. On the one hand, by using a small-
scale LLM (i.e., a 13B Llama model) as the summa-
rizer it could speed up the inference for downstream
tasks. The summary ratio across the four datasets
are 8.25%, 7.35%, 6.77% and 7.32% respectively,
which might increase a small amount of inference
cost by adding the summary stage, nevertheness,
since the inference cost of LLMs is linearly cor-
related with token number, with the knowledge
derived from the summarizer, it can potentially de-
crease the inference cost for high-cost commercial
LLMs such as GPT4. On the other hand, it further
enhances the performance by extracting the most
relevant information for a given task so as to alle-
viate distracting information, which improves the
EM and F1 metrics across the four datasets on aver-
age by 1.4% and 1.7% respectively, which further
justifies our model design.
5
Conclusion
In
this
work,
we
propose
METRAG,
a
multi–layered
thoughts
enhanced
retrieval-
augmented generation framework, which first
embraces LLM’s supervision to obtain utility-
oriented thoughts and combines the similarity and
utility of documents for performance boosting, and
further pursuits compactness-oriented thoughts
via a task-adaptive summarizer.
Finally, with
multi-layered thoughts from the precedent stages,
an LLM is called for knowledge-augmented
generation. Extensive experiments on knowledge-
intensive tasks have demonstrated the superiority
of the proposed METRAG.
6
Limitations
One of the limitation of our work is that the ef-
fectiveness of utility model is highly hinges on a
strong LLM’s supervision, although we have find
that an LLM like LLama 7B or 13B is enough for
training a satisfactory utility model. In sum, our
work opens up a fresh perspective to reconsider
retrieval-augmented generation, but more complex
situation that require reading a large amount of ma-
terial to answer (e.g., legal or medical documents)
is still unresolved. Hence, extending our frame-
work in the super-long contexts is one of the future
work.

References
Akari Asai, Sewon Min, Zexuan Zhong, and Danqi
Chen. 2023a. Retrieval-based language models and
applications. In ACL (tutorial), pages 41–46.
Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and
Hannaneh Hajishirzi. 2023b. Self-rag: Learning to
retrieve, generate, and critique through self-reflection.
CoRR, abs/2310.11511.
Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang,
Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei
Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin,
Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu,
Keming Lu, Jianxin Ma, Rui Men, Xingzhang Ren,
Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong
Tu, Peng Wang, Shijie Wang, Wei Wang, Sheng-
guang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang,
Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu,
Hongyi Yuan, Zheng Yuan, Jianwei Zhang, Xingx-
uan Zhang, Yichang Zhang, Zhenru Zhang, Chang
Zhou, Jingren Zhou, Xiaohuan Zhou, and Tianhang
Zhu. 2023. Qwen technical report. arXiv preprint
arXiv:2309.16609.
Baichuan. 2023. Baichuan 2: Open large-scale lan-
guage models. arXiv preprint arXiv:2309.10305.
Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding,
Jiezhong Qiu, Zhilin Yang, and Jie Tang. 2022. Glm:
General language model pretraining with autoregres-
sive blank infilling. In ACL, pages 320–335.
Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia,
Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, Qianyu Guo,
Meng Wang, and Haofen Wang. 2023. Retrieval-
augmented generation for large language models: A
survey. CoRR, abs/2312.10997.
Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat,
and Ming-Wei Chang. 2020. Retrieval augmented
language model pre-training. In ICML, pages 3929–
3938.
Yushi Hu, Hang Hua, Zhengyuan Yang, Weijia Shi,
Noah A. Smith, and Jiebo Luo. 2022. Promptcap:
Prompt-guided task-aware image captioning. CoRR,
abs/2211.09699.
Gautier Izacard, Patrick S. H. Lewis, Maria Lomeli,
Lucas Hosseini, Fabio Petroni, Timo Schick, Jane
Dwivedi-Yu, Armand Joulin, Sebastian Riedel, and
Edouard Grave. 2023.
Atlas: Few-shot learning
with retrieval augmented language models. J. Mach.
Learn. Res., 24:251:1–251:43.
Zhengbao Jiang, Frank F. Xu, Luyu Gao, Zhiqing Sun,
Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie
Callan, and Graham Neubig. 2023. Active retrieval
augmented generation.
In EMNLP, pages 7969–
7992.
Jaehun Jung, Peter West, Liwei Jiang, Faeze Brahman,
Ximing Lu, Jillian Fisher, Taylor Sorensen, and Yejin
Choi. 2023. Impossible distillation: from low-quality
model to high-quality dataset & model for summa-
rization and paraphrasing. CoRR, abs/2305.16635.
Jungo Kasai, Keisuke Sakaguchi, Yoichi Takahashi, Ro-
nan Le Bras, Akari Asai, Xinyan Yu, Dragomir R.
Radev, Noah A. Smith, Yejin Choi, and Kentaro Inui.
2022. Realtime QA: what’s the answer right now?
CoRR, abs/2207.13332.
Tom Kwiatkowski, Jennimaria Palomaki, Olivia Red-
field, Michael Collins, Ankur P. Parikh, Chris Alberti,
Danielle Epstein, Illia Polosukhin, Jacob Devlin, Ken-
ton Lee, Kristina Toutanova, Llion Jones, Matthew
Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob
Uszkoreit, Quoc Le, and Slav Petrov. 2019. Natu-
ral questions: a benchmark for question answering
research. Trans. Assoc. Comput. Linguistics, 7:452–
466.
Alex Mallen, Akari Asai, Victor Zhong, Rajarshi Das,
Daniel Khashabi, and Hannaneh Hajishirzi. 2023.
When not to trust language models: Investigating
effectiveness of parametric and non-parametric mem-
ories. In ACL, pages 9802–9822.
OpenAI. 2023a. Chatgpt: Optimizing language models
for dialogue.
OpenAI. 2023b. GPT-4 technical report. arXiv preprint
arXiv:2303.08774.
Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano
Ermon, Christopher D Manning, and Chelsea Finn.
2023. Direct preference optimization: Your language
model is secretly a reward model. arXiv preprint
arXiv:2305.18290.
Ori Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay,
Amnon Shashua, Kevin Leyton-Brown, and Yoav
Shoham. 2023. In-context retrieval-augmented lan-
guage models. CoRR, abs/2302.00083.
Vipula Rawte, Amit P. Sheth, and Amitava Das. 2023.
A survey of hallucination in large foundation models.
CoRR.
Stephen E. Robertson and Hugo Zaragoza. 2009. The
probabilistic relevance framework: BM25 and be-
yond. Found. Trends Inf. Retr., 3(4):333–389.
Weijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon
Seo, Rich James, Mike Lewis, Luke Zettlemoyer, and
Wen-tau Yih. 2023. REPLUG: retrieval-augmented
black-box language models. CoRR, abs/2301.12652.
Hugo Touvron, Louis Martin, Kevin R. Stone, Peter
Albert, Amjad Almahairi, Yasmine Babaei, Nikolay
Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti
Bhosale, D. Bikel, Lukas Blecher, Cristian Cantón
Ferrer, Moya Chen, Guillem Cucurull, David Esiobu,
Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,
Cynthia Gao, Vedanuj Goswami, Naman Goyal,
A. Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan,
Marcin Kardas, Viktor Kerkez, Madian Khabsa, Is-
abel M. Kloumann, A. Korenev, Punit Singh Koura,

Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Di-
ana Liskovich, Yinghai Lu, Yuning Mao, Xavier Mar-
tinet, Todor Mihaylov, Pushkar Mishra, Igor Moly-
bog, Yixin Nie, Andrew Poulton, Jeremy Reizen-
stein, Rashi Rungta, Kalyan Saladi, Alan Schelten,
Ruan Silva, Eric Michael Smith, R. Subramanian,
Xia Tan, Binh Tang, Ross Taylor, Adina Williams,
Jian Xiang Kuan, Puxin Xu, Zhengxu Yan, Iliyan
Zarov, Yuchen Zhang, Angela Fan, Melanie Kam-
badur, Sharan Narang, Aurelien Rodriguez, Robert
Stojnic, Sergey Edunov, and Thomas Scialom. 2023.
Llama: Open and efficient foundation language mod-
els. arXiv preprint arXiv:2307.09288.
Jason Wei, Maarten Bosma, Vincent Y. Zhao, Kelvin
Guu, Adams Wei Yu, Brian Lester, Nan Du, An-
drew M. Dai, and Quoc V. Le. 2022.
Finetuned
language models are zero-shot learners. In ICLR.
Fangyuan Xu, Weijia Shi, and Eunsol Choi. 2023.
RECOMP: improving retrieval-augmented lms with
compression and selective augmentation.
CoRR,
abs/2310.04408.
Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang,
Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen
Zhang, Junjie Zhang, Zican Dong, Yifan Du, Chen
Yang, Yushuo Chen, Zhipeng Chen, Jinhao Jiang,
Ruiyang Ren, Yifan Li, Xinyu Tang, Zikang Liu,
Peiyu Liu, Jian-Yun Nie, and Ji-Rong Wen. 2023.
A survey of large language models. arXiv preprint
arXiv:2303.18223.
Wenxuan Zhou, Sheng Zhang, Hoifung Poon, and
Muhao Chen. 2023. Context-faithful prompting for
large language models. In EMNLP Findings, pages
14544–14556.

A
Appendix
A.1
Prompt Template
A.1.1
Utility Model Training Prompt
When training the utility model, we design differ-
ent prompts so that an LLM can output its perplex-
ity when i) answering the question with retrieved
document; ii) answering the question directly. We
present the prompts in Table 5.
Table 5: Utility Model Training Prompt.
w/ retrieved document
Please answer the question based on the given context. Question:
[question] The context related to the question is as follows:
[retrieved document]. Answer: [answer]
w/o retrieved document
Please answer the question. Question: [question] Answer: [answer]
A.1.2
Task-adaptive Summarizer Training
and Inference Prompt
To endow retrieval-augmented generation with
compactness-oriented thought, we carefully train a
task-adaptive summarizer for information integra-
tion and extraction, where we show the prompt in
Table 6.
Table 6: Task-adaptive Summarizer Prompt.
Instruction:
You are an excellent summary generation robot. Given the following
question (Question) and texts (Docs), you need to summarize these
texts (Docs) into a concise abstract to adequately address the
corresponding question.
Question:
[question], [options if it is multiple-choice question]
Docs:
[retrieved documents]
Summary:
A.1.3
Knowledge-augmented Generation
Prompt
We show the prompt for generating the final answer
in Table 7.
Table 7: Knowledge-augmented Generation Prompt.
Instruction:
You are an AI assistant for answering questions. Based on the given
question (Question) and the corresponding information (Info), please
provide the correct answer as concise as possible according to the
info and your commonsense.
Info:
[retrieved documents / summary]
Question:
[question]
Answer:
Please answer the question in the form of 2 or 3 words.

