arXiv:1710.09118v3  [q-bio.NC]  20 Jan 2018
Recognition Dynamics in the Brain under the Free
Energy Principle
Chang Sub Kim
Department of Physics, Chonnam National University, Gwangju 61186, Republic of
Korea
E-mail: cskim@jnu.ac.kr
Abstract.
We formulate the computational processes of perception in the framework
of the principle of least action by postulating the theoretical action as a time integral
of the free energy in the brain sciences.
The free energy principle is accordingly
rephrased as that for autopoietic grounds all viable organisms attempt to minimize
the sensory uncertainty about the unpredictable environment over a temporal horizon.
By varying the informational action, we derive the brain’s recognition dynamics
(RD) which conducts Bayesian ﬁltering of the external causes from noisy sensory
inputs. Consequently, we eﬀectively cast the gradient-descent scheme of minimizing
the free energy into Hamiltonian mechanics by addressing only positions and momenta
of the organisms’ representations of the causal environment.
To manifest the
utility of our theory, we show how the RD may be implemented in a neuronally
based biophysical model at a single-cell level and subsequently in a coarse-grained,
hierarchical architecture of the brain. We also present formal solutions to the RD for a
model brain in linear regime and analyze the perceptual trajectories around attractors
in neural state space.
Keywords: recognition dynamics, Bayesian ﬁltering, perception, free energy principle,
sensory uncertainty, informational action, principle of least action

Recognition Dynamics in the Brain under the Free Energy Principle
2
1. Introduction
The quest for a universal principle that may explain the cognitive and behavioral
operation of the brain is of great scientiﬁc interest at present. The apparent diﬃculty in
addressing the quest is the gap between the information processing and the biophysics
that governs neurophysiology in the brain.
However, it is evident that the matter,
which is the ground-stuﬀupon which the brain functions emerge, comprises neurons
obeying the laws guided by physics principles.
Thus, any biological principles that
attempt to explain the brain’s large-scale workings must cope with our accepted physical
reality [1]. It appears that on the current approaches still prevails the classical, eﬀective
epistemology of regarding perceptions as constructing hypotheses which may hit upon
truth by producing symbolic structures matching physical reality [2, 3, 4].
One inﬂuential candidate at present that seeks for such a rubric in neuroscience
is the free energy principle (FEP) [5, 6, 7]. For a technical appraisal of the FEP, we
refer to [8] where the theoretical assumptions and the mathematical structure involved
in the FEP are reviewed in great detail. We have noticed [9] suggesting ‘variational
neuroethology’ which explains, integrating the FEP with evolutionary systems theory,
how living systems appear to resist the second law of thermodynamics.
To state
compactly, the FEP oﬀers that all viable organisms perceive and act on the external
world by instantiating a probabilistic causal model embodied in their brain in a way to
ensure their adaptive ﬁtness or autopoiesis [10]. The biological mechanism that endows
the organism’s brain with the operation is theoretically framed into an information-
theoretic measure, which we call ‘informational free energy (IFE)’. According to the
FEP, a living system tries to minimize the sensory surprisal when it faces an external
cause that perturbs its spontaneous equilibrium within its physiological boundary by
pursuing perceptive as well as active inferences. However, the brain does not preside
over instreaming sensory distribution; accordingly, the brain cannot directly minimize
the sensory surprisal but, instead, minimizes its upper bound, the IFE. The probabilistic
rationale of the FEP argues that the brain’s representations of the uncertain environment
are the suﬃcient statistics, e.g., means or variances, of a probability density encoded in
the brain. The variational parameters are supposed to be encoded as physical variables
in the brain.
The brain statistically infers the external causes of sensory input by
Bayesian ﬁltering, using its internal top-down model about predicting, or generating,
the sensory data. Filtering is a probabilistic approach to determining the external states
from noisy measurements of sensory data [11]. There is growing experimental support for
the brain’s maintaining internal models of the environment to predict sensory inputs and
to prepare actions, see for instance [12]. The computational operation of the abductive
inference is subserved by the brain variables and the resulting perceptual mechanics is
termed as the ‘recognition dynamics (RD)’.
Although the suggestion of the FEP has been promising to account for the brain’s
inferring mechanism of and acting upon sensory causes, we ﬁnd certain theoretical
subtleties in the conventional formulation:

Recognition Dynamics in the Brain under the Free Energy Principle
3
First, the FEP minimizes the IFE at each point in time for successive sensory
inputs [13].
However, precisely the objective function to be minimized is the
continuously accumulated IFE over a ﬁnite time.;
The minimization must be
performed concerning trajectories over a temporal horizon across which an organism
encounters with atypical events to its natural habitat and biology.
Second, the FEP employs the gradient-descent method in practically executing
minimization of the IFE [15], which is widely used in machine learning theory to
solve engineering optimization problems eﬃciently.
The engaged scheme allows
formulation to ﬁnd heuristically optimal solutions in the FE landscape, but it not
derived from a scientiﬁc principle.
Third, the FEP introduces the notion of the ‘generalized coordinates’ of an inﬁnite
number of the so-called ‘generalized motions’ to account for the dynamical nature
of the environment [16]. The ensuing theoretical construct is a generalization of the
standard Newtonian mechanics.§ With the hired theory, however, it is obscure to
decide the number of independent dynamical variables for a complete description.
In practice, typically dynamic truncation is made at a ﬁnite embedding order by
assuming that the precision of random ﬂuctuations on higher orders of motion
disappears very quickly.
Fourth, the FEP introduces the hydrodynamics-like concepts of the ‘path of a
mode (motion of expectation)’ and the ‘mode of a path (expected motion)’ by
distinguishing the dynamic update from the temporal update of a time-dependent
state [17]. Because the distinction is essential to ensure an equilibrium solution
to the RD in employing the dynamical generative models, further theoretical
exploration seems worthwhile.
Fifth, the FEP considers the states of the environment ‘hidden’ because what the
brain faces is only a probabilistic sensory mapping. Subsequently, a distinction
is made between the hidden-state representations, responsible for intra-level
dynamics, and causal-state representations, responsible for inter-level dynamics, in
the hierarchical brain [18]. Such a distinction must emerge as neuronal dynamics in
the brain on diﬀerent timescales. Accordingly, a biophysically grounded formulation
that supports the top-down idea is required.
In this paper, we present a mechanical formulation of the RD in the brain in the
; According to the FEP, the updating or learning of the generative model takes places in the brain
on a longer time scale than that associated with perceptual inference. To derive the RD of the slow
variables for synaptic eﬃcacy and gain, the time-integral of the IFE is taken as an objective function;
however, again the gradient descent method is executed in a pointwise way in time [14].
§ The mechanical state of a particle is speciﬁed only by position and velocity in the Newtonian
mechanics, and no physical observables are assigned to the dynamical orders beyond the second-order.
In some literature [19], the concept of ‘jerk’ is assigned to the third-order time-derivative of position as
a physical reality. From the mathematical perspective, such a generalization is not forbidden. However,
not only higher-orders are diﬃcult to measure, but more seriously it raises the question of what the
corresponding cause to jerk as the force to acceleration. And, the same impasse in all next orders.

Recognition Dynamics in the Brain under the Free Energy Principle
4
framework of Hamilton’s principle of least action [20]. Motivated by the aforementioned
theoretical observations, we try to resolve some of the technical complexities in the
FEP framework. To be speciﬁc, the goal is to recast the gradient-descent strategy of
minimizing the IFE into the mathematical framework that appeals to the normative
physics rules.
We do this by hypothesizing the IFE as a Lagrangian of the brain
which enters the theoretical action, being the fundamental objective function to be
minimized in continuous time under the principle of least action.
Consequently, we
reformulate the RD regarding only the canonical, physical realities to eschew the
generalized coordinates of inﬁnitely recursive time-derivatives of the continuous states
of the organism’s environment and brain. In the canonical description, the dynamical
state of a system is speciﬁed only by positions and their ﬁrst-order derivatives.
In this work, supported by the present day evidence [22, 23], we admit the bi-
directional facet in informational ﬂow in the brain. The environment begets sensory
data at the brain-environment interface such as sensory receptors or interoceptors within
an organism.
The incited electro-opto-chemical interaction in sensory neurons must
transduce forward in the anatomical structure of the brain. Whereas complying with
the idea of perception as constructing hypotheses, there must be backward pathway as
well in information processing in the functional hierarchy of the brain. To understand
how such bidirectional functional architecture is emergent from the electrophysiology
of biophysics and anatomical organization of the brain is a forefront research interest
(see, for instance, [24] and references therein). We shall consider a simple model that
eﬀectively incorporates the functional hierarchy while focusing our attention on the
brain’s perceptual mechanics of inferring of the external world, given sensory data. The
problem of learning of the environment via updating the internal model of the world
and of active inference of changing sensory input via action on the external world, see
for instance [21], is deferred for an upcoming paper.
Here, we shall outline how in this work we cast Bayesian ﬁltering in the FEP using a
variational principle of least action and how we articulate the minimization of the sensory
uncertainty in terms of an associated Lagrangian and Hamiltonian. Furthermore, given
a particular form for the diﬀerential equations, aﬀorded by computational neuroscience,
one can see relatively easily how neuronal dynamics could implement the Bayesian
ﬁltering: (i) According to the FEP, the brain represents the environmental features
statistically eﬃciently, using the suﬃcient statistics µ. We assume that µ stands for
the basic computational unit of the neural attributes of perception in the brain. Such
a constituent is considered a ‘perceptual particle’ which may be a single neuron or
physically coarse-grained multiple neurons forming a small particle; (ii) We postulate
that the Laplace-encoded IFE in the FEP, denoted as F (Sec. 2.1), serves as an eﬀective,
informational Lagrangian (IL) of the brain, written as L. Accordingly, the informational
action (IA), which we denote by S, is deﬁned to be time-integral of the approximate
IFE (Sec. 3.1); (iii) Then, conforming to the Hamilton principle of least action, the
equations of motion of the perceptual particles are derived mathematically by varying
the IA with respect to both µ and 9µ.
The resulting Lagrange equations constitute

Recognition Dynamics in the Brain under the Free Energy Principle
5
the perceptual mechanics, i.e., the RD of the brain’s inferring of the external causes
of the sensory stimuli (Sec. 3.1); (iv) In turn, we obtain the brain’s informational
Hamiltonian H from the Lagrangian via a Legendre transformation.
Consequently,
we derive a set of coupled, ﬁrst-order diﬀerential equations for µ and its conjugate pµ,
which are equivalent to the perceptual mechanics derived from the Lagrange formalism.
The resulting perceptual mechanics is our derived RD in the brain. Accordingly, the
brain performs the RD in the state space spanned by the position µ and momentum
pµ variables of the constituting neural particles. (Sec. 3.2); (v) We adopt the Hodgkin-
Huxley (H-H) neurons as biophysical neural correlates which form the basic perceptual
units in the brain. We ﬁrst derive the RD of sensory perception at a single-neuron level
where the membrane potential, ionic transport, and gating are the relevant physical
attributes. Subsequently, we scale up the cellular formulation to furnish a functional
hierarchical-architecture of the brain. On this coarse-grained scale, the perceptual states
are the averaged properties of many interacting neurons. We simplify the hierarchical
picture with two averaged, activation and connection variables, mediating the intra- and
inter-level dynamics, respectively. According to our formulation of the hierarchical RD
in the brain, as sensory perturbation comes in at the lowest level, i.e., sensory interface,
the brain carries out the RD in its functional network and ﬁnds an optimal trajectory
which minimizes the IA.
To summarize, we have adopted the IFE as an informational Lagrangian of
the brain and subsequently employed the principle of least action to construct the
Hamiltonian mechanics of cognition. In doing so, only positions and momenta of the
neural particles have been addressed as dynamical variables; positions and momenta
are the metaphorical terms for the perceptual states and the brain’s prediction errors,
respectively. We do not distinguish the causal and hidden states, both of which must
emerge as biophysical neuronal activities on diﬀerent timescales.
The resulting RD
is statistically deterministic, arising from unpredictable motions of the environmental
states and noisy sensory mapping. Furthermore, the derived RD describes not only the
temporal development of the brain variables but also the prediction errors. The rate
of the prediction errors is not incorporated in the conventional formulation of the FEP.
The successful solutions of the RD are stable equilibrium trajectories in the neural state
space, specifying the tightest upper bound of the sensory uncertainty, conforming to the
rephrased FEP. Our formulation allows solutions in an analytical form in linear regimes
near ﬁxed points, expanded in terms of the eigenvectors of the Jacobian and, thus,
provides with tractability of real-time analysis. We hope that our theory will motivate
further investigations of some model brains with numerical simulations and also of the
active inference and learning problems.
This paper is organized as follows. We ﬁrst recapitulate the FEP in Sec. 2 to support
our motivation for casting the gradient descent scheme into the standard mechanical
formulation. In the followed Sec. 3 we present the RD reformulated in the Lagrangian
and Hamiltonian formalisms. Then, in Sec. 4 biophysical implementations of our theory
at the cellular level and in the scaled-up hierarchical brain are formulated, where

Recognition Dynamics in the Brain under the Free Energy Principle
6
nonlinear as well as linear dynamical analyses are carried out. Finally, a discussion
is provided in Sec. 5.
2. The free energy principle
To unveil our motivation for this paper, we shall compactly digest here the IFE and
the FEP, that are currently exercised in the brain sciences. The RD is an organism’s
organization of executing minimization of the IFE in the brain under the FEP. In
practice, there are various ways of IFE-minimizing schemes, for instance, variational
message passing and belief propagation, which do not lend themselves to treatment
regarding generalized coordinates of motion.
Our treatment in this paper is more
relevant to Bayesian ﬁltering and predictive coding schemes that have become a popular
metaphor for message passing in the brain. Filtering is the problem of determining the
state of a system from noisy measurements [11]. For a detailed technical appraisal of
the FEP, we refer to [8] from which we borrow the mathematical notations.
2.1. The informational free energy
A living organism occupies a ﬁnite space and time in the unbounded, changing world
while interacting with the rest of the world, comprising its environment. The states of
the environment are denoted as ϑ collectively, which are ‘hidden’ from the organism’s
perspective.
The signals from the environment are registered biophysically at the
organism’s sensory interface as sensory data ϕ.
The organism’s brain faces uncertainty when it tries to predict the sensory inputs,
the amount of which is quantiﬁed as the sensory uncertainty H. The sensory uncertainty
is deﬁned to be an average of the self-information, ´ ln ppϕq, over the probability density
ppϕq encoded at the interface,
H ”
ż
dϕ t´ ln ppϕqu ppϕq.
(1)
The self-information, which is also termed as the sensory ‘surprise’ or ‘surprisal’
in information theory, quantiﬁes the survival tendency of living organisms in the
unpredictable environment.
Assuming that the sensory density describes an ergodic
ensemble of sensory streaming}, one may convert the sensory uncertainty into a time-
average as
ż
dϕ t´ ln ppϕqu ppϕq “ 1
T
ż T
0
dt t´ ln ppϕptqqu ,
where T is the temporal window over which an environmental event takes places, i.e., a
temporal horizon. Here, one may manipulate the right-hand side (RHS) of the preceding
} This ergodicity assumption is an essential ingredient of the FEP, which hypothesizes that the ensemble
average of the surprisal is equal to the time-average of that, regarding the surprisal as a statistical,
dynamical quantity.

Recognition Dynamics in the Brain under the Free Energy Principle
7
equation by adding a Kullback-Leibler divergence to the integrand to get
´ ln ppϕq `
ż
dϑqpϑq ln qpϑq
ppϑ|ϕq Ñ
ż
dϑqpϑq ln
qpϑq
ppϑ, ϕq.
The outcome brings about the mathematical deﬁnition of the IFE,
Frqpϑq, ppϑ, ϕqs ”
ż
dϑqpϑq ln
qpϑq
ppϑ, ϕq,
(2)
which is expressed as a functional of the two probability densities, qpϑq and ppϑ, ϕq;
where qpϑq and ppϑ, ϕq are termed the recognition density (R-density) and the generative
density (G-density), respectively.
The R-density is the organism’s probabilistic
representation of the external world, which the organism’s brain uses in approximately
inferring the causes ϑ of inputs ϕ. The G-density, a joint probability between ϑ and ϕ,
underlies the top-down model about how the sensory data are biophysically generated
by interaction between the brain and the environment. By construction, the surprisal is
smaller than the IFE by the added positive-amount, accordingly the sensory uncertainty
is bounded from above in accordance with
ż
dt r´ ln ppϕqs ď
ż
dtFrqpϑq, ppϑ, ϕqs.
(3)
Note that the sensory uncertainty on the left-hand side (LHS) of Eq. (3) speciﬁes the
accumulated surprisal over a temporal horizon involved in an environmental event.
Equation (3) constitutes a mathematical statement of the FEP: “All viable
organisms try to avoid being placed in an atypical situation in their environmental
habitats for existence by minimizing the sensory uncertainty. However, organisms do not
possess direct control over the sensory distribution ppϕq; accordingly they, instead, are
to minimize the upper bound of Eq. (3),
ş
dtF as a proxy for the sensory uncertainty.”
The brain conducts the minimization probabilistically by updating the R-density to
approximate the posterior density ppϑ|ϕq, namely carrying out Bayesian inference of
the causes ϑ of the sensory data ϕ. In the conventional application of the FEP the
following approximate inequality is usually exercised [25, 26],
´ ln ppϕq ď F.
(4)
However, note that the inequality, Eq. (4) is not equivalent to Eq. (3), in general. It is
only a point approximation piecewise in time.
Here, a diﬃculty arises because the functional shape of the R-density is not given
a priori. It may be ﬁxed after knowing all orders of its moments of the external states,
which is not possible. To circumvent the diﬃculty, usually one invokes variational Bayes
by assuming a Gaussian ﬁxed-form for the R-density, the ‘Laplace approximation’,
qpϑq “
1
?2πζ exp
␣
´pϑ ´ µq2{p2ζq
(
” N pϑ; µ, ζq,
(5)
which is fully characterized simply by its means µ and variances ζ, namely ﬁrst and
second order suﬃcient statistics, respectively. Then, by substituting Eq. (5) into Eq. (2)

Recognition Dynamics in the Brain under the Free Energy Principle
8
and after some technical approximations, see [8] for the details, one can convert the IFE
functional F into
Frqpϑq, ppϑ, ϕqs Ñ ´ ln ppµ, ϕq ” Fpµ, ϕq.
(6)
At the end of manipulation the outcome becomes a function of only the means µ, given
the sensory input ϕ; where the dependence of variances has been removed by their
optimal values. The resulting IFE function F in Eq. (6) is termed the ‘Laplace-encoded’
IFE in which the parameters µ, specifying the organism’s belief or expectation of the
environmental states, are the organism’s probabilistic representation of the external
world. In turn, it is argued that the variational parameters µ are encoded in the brain
as biophysical variables.
To proceed with minimization of the IFE in the ﬁltering scheme, a model for noisy-
data measurement and also the equations of motion of the states must be supplied. The
FEP assumes a formal homology between the external dynamics and the organism’s
top-down belief: The former describes, according to physics laws, the equations of
motion of environmental states and the sensory-data registering process. And, the latter
prescribes the internal dynamics of the representations of the environmental states and
the generative model of the sensory data in the organism’s brain [13]. Following this
idea, we hypothesize that the registered data ϕ are predicted by the organism according
to a linear or nonlinear morphism,
ϕ “ gpµq ` z,
(7)
where gpµq is a map from µ onto ϕ and z is the involved random ﬂuctuation. Also, the
brain’s representations µ of the causes are assumed to obey the stochastic equation of
motion,
dµ
dt “ fpµq ` w
(8)
where fpµq is a linear or nonlinear function of the organism’s expectation of
environmental dynamics and w is the associated random ﬂuctuation.
Assuming mutually uncorrelated Gaussian ﬂuctuations, w and z, of the organism’s
beliefs, one may furnish the models for the likelihood ppϕ|µq and the empirical prior
ppµq, which jointly enter the Laplace-encoded IFE in Eq. (6) in the factorized form,
ppϕ, µq “ ppϕ|µqppµq.
(9)
Using the notation introduced in Eq. (5), they are given explicitly as
ppϕ|µq “ N pϕ ´ gpµq; 0, σzq,
(10)
ppµq “ N p 9µ ´ fpµq; 0, σwq,
(11)
where we have set 9µ “ dµ{dt, and the normal densities are assumed to possess zero
means with variances σz and σw, respectively. When the ﬂuctuations are statistically
stationary, the variances are handled as constant.; however nonstationarity can also be
taken into account by assuming an explicit time-dependence in the variances. Finally,

Recognition Dynamics in the Brain under the Free Energy Principle
9
by substituting Eqs. (10) and (11) into Eq. (6), one can convert the Laplace-encoded
IFE , up to a constant, into
Fpµ, ϕq “ 1
2σ´1
z ε2
z ` 1
2σ´1
w ε2
w ` 1
2 ln pσzσwq ,
(12)
where the new variables have been deﬁned as
εz ” ϕ ´ gpµq
and
εw ” 9µ ´ fpµq.
The auxiliary variable εz speciﬁes the discrepancy between the sensory data ϕ and the
brain’s prediction gpµq. Similarly, εw speciﬁes the discrepancy between the change of
the environmental representations 9µ and the organism’s belief fpµq.
It is straightforward to extend the formulation to the multiple, correlated noisy
inputs. However, for simplicity, we shall continually work in the single-variable picture
and will promote it to the general situation later.
2.2. Gradient descent scheme of the RD
With the Laplace-encoded IFE as an instrumental tool, the organism’s brain
searches for the tightest bound for the surprisal, conforming to Eq. (4), by varying
its internal states µ. The critical question is what machinery the brain hires for the
minimization procedure. Typically the gradient descent method in machine learning
theory is employed in the conventional approach.
To give an idea of the gradient-descent scheme, here we set up a simple gradient-
descent equation, in the usual manner, by regarding the IFE function F as an objective
function as
9µ “ ´κ∇µF.
(13)
In the above 9µ implies a temporal or sequential update of the brain variable µ and ∇µ is
the gradient operator with respect to µ, and κ is the learning rate that controls the speed
of optimization. In steady state, deﬁned by 9µ ” 0, the solution µp0q to the relaxation
equation, Eq. (13) must satisfy ∇µF “ 0. Subsequently, it may be interpreted that
such a solution corresponds to an equilibrium (or ﬁxed) point of the IFE function F,
specifying a local minimum in the IFE landscape.
By inspection, however, we ﬁnd that the gradient-descent construct in the preceding
way bears an ambiguity when it is applied to dynamic causal models such as Eq. (8).
This is because imposing the condition, 9µ ” 0 on the LHS of Eq. (13), does not guarantee
a desired equilibrium point in the state space spanned by µ. The reason is that 9µ also
appears on the RHS of Eq. (13) via F: The gradient operation on the RHS of Eq. (13)
can be performed explicitly for given F, Eq. (12) to give
ˆµ ¨ ∇µF “ ´σ´1
z pϕ ´ gqBg
Bµ ´ σ´1
w p 9µ ´ fq Bf
Bµ.
This subtlety does not appear in the conventional theory which incorporates the
nonstationarity, i.e., the aspect of continually changing external states, into formulation
using the mathematical construct of unbounded, higher-order motion of the generalized

Recognition Dynamics in the Brain under the Free Energy Principle
10
coordinates.¶
It is an attempt to allow a more precise speciﬁcation of a system’s
dynamical state. The generalized coordinates are deﬁned to be a row vector in the
state space spanned by all orders of time-derivatives of bare state µ,
˜µ “ pµ, µ1, µ2, ¨ ¨ ¨q ” pµr0s, µr1s, µr2s, ¨ ¨ ¨q
(14)
where vector components are deﬁned, with understanding µr0s ” µ, as
µrn`1s “ µ1
rns ” Dµrns.
Note that the notation Dµrns ” µ1
rns has been introduced to denote the dynamical update
of the component µrns, which is in contrast to the notation 9µrns for the sequential update.
Also, two components of a vector at diﬀerent dynamical orders in the generalized
coordinates are mutually independent variables.
Similarly, the sensory-data ˜ϕ are
expressed in the generalized coordinates as a row vector,
˜ϕ “ pϕ, ϕ1, ϕ2, ¨ ¨ ¨q ” pϕr0s, ϕr1s, ϕr2s, ¨ ¨ ¨q.
(15)
Each component in the vectors, ˜µ and ˜ϕ, is to be considered as a dynamically-
independent variable.
Also, assuming that the random ﬂuctuations, z and w, are
analytic, they have been written in the generalized coordinates as ˜z and ˜w, respectively.
Then, the generalization of Eqs. (7) and (8) follows after some technical approximations
as (for details, see [8])
˜ϕ “ ˜g ` ˜z,
(16)
D˜µ “ ˜f ` ˜w;
(17)
where D˜µ “ pµ1, µ2, µ3, ¨ ¨ ¨q. For reference, we explicitly speel out n, Eqs. (16) and (17)
at dynamical orders as
ϕrns
“ Bg
Bµµrns ` zrns,
Dµrns “ Bf
Bµµrns ` wrns.
Note that diﬀerent dynamical orders of the noises ˜z and
˜w may be considered
to be statistically correlated in general.
Then, the Laplace-encoded IFE can be
mathematically constructed from multivariate correlated Gaussian noises with zero
means and covariance matrices Σw and Σz,
Fp˜µ, ˜ϕq “ 1
2t 9˜µ ´ ˜fuΣ´1
w t 9˜µ ´ ˜fuT ` 1
2 ln |Σw|
` 1
2t ˜ϕ ´ ˜guΣ´1
z t ˜ϕ ´ ˜guT ` 1
2 ln |Σz|,
(18)
¶ The terminology of the generalized coordinates in generalized ﬁltering is dissimilar from its common
usage in physics. In classical mechanics, the generalized coordinates refer the independent coordinate
variables which are required to completely specify the conﬁguration of a system with a holonomic
constraint, not including their temporal derivatives. The number of generalized coordinates determines
the degree of freedom in the system [20]. Accordingly, the term, ‘generalized states’ seems better suit
generalized ﬁltering.

Recognition Dynamics in the Brain under the Free Energy Principle
11
where t 9˜µ´ ˜fuT is the transpose of row vector t 9˜µ´ ˜fu.; |Σw| and Σ´1
w are the determinant
and the inverse of the covariance matrix Σw, respectively, etc.
In many practical
exercises, however, usually, the conditional independence among diﬀerent dynamical
orders is imposed.
Consequently, the noise distribution at each dynamical order is
assumed to be an uncorrelated Gaussian density about zero means. This simpliﬁcation
corresponds to the Wiener process or Markovian approximation [11]. Here, we recall
that the generalized states ˜µ are the means of the brain’s probabilistic model of the
dynamical world, the R-density Eq. (5) after rewritten in the generalized coordinates.
Note Eq. (18) is a direct generalization of Eq. (12).
Furnished with the extra theoretical constructs, the IFE becomes a function of
the generalized coordinates ˜µ, given sensory data ˜ϕ, F “ Fp˜µ, ˜ϕq. Accordingly, the
gradient-descent scheme must be extended to incorporate the generalized motions in its
formulation. This is done by the theoretical prescription that the dynamical update D˜µ
is distinctive from the sequential update 9˜µ. Consequently, one recasts Eq. (13) into the
form,
9˜µ ´ D˜µ “ ´κ∇˜µFp˜µ, ˜ϕq.
(19)
With the revised gradient-descent equation, the conventional FEP claims that the IFE is
minimized by reaching a desired ﬁxed point ˜µ˚ ” ˜µpt Ñ 8q in the generalized state space
spanned by ˜µ. This corresponds to the situation when the two rates of the generalized
brain variables, 9˜µ and D˜µ become coincident, namely 9µrns “ Dµrns at every dynamic
order n. To support the idea it is argued that the purpose of Eq. (19) is to place the
gradient descent in the frame of reference that moves with the mean ˜µ , see [15]. The
entire minimization procedure is compactly expressed in the literature as
˜µ˚ “ arg min
˜µ Fp˜µ, ˜ϕ|mq,
where we have inserted m in F to indicate explicitly that the minimization is conditioned
on the generative model of an organism.
In brief, the brain performs the RD of perceptual inference by biophysically
implementing Eq. (19) in the gray matter.
The steady-state solution ˜µ˚ speciﬁes
minimum value of the IFE, say Fmin “ Fp˜µ˚, ˜ϕq, giving the tightest bound of the
surprisal [see Eq. (4)] associated with a given sensory experience ˜ϕ. Despite its frequent
employment in practicing the FEP, we have disclosed some subtleties involved in the
conventional formulation of the gradient descent scheme, which has motivated our
reformulation.
3. The informational action principle
The RD condensed in Sec. 2.2 is based on the mathematical statement Eq. (4) of
the FEP, which is a point approximation of Eq. (3). Here we reformulate the RD by
complying with the full mathematical statement of FEP given in Eq. (3). Accordingly,
we need a formalism that allows minimization of the time-integral of the IFE, not at each

Recognition Dynamics in the Brain under the Free Energy Principle
12
point in time. We have come to assimilate that the theoretical ‘action’ in the principle of
least action neatly serves the goal [20]. This formalism allows us to eschew introduction
of the generalized coordinates of a dynamical state comprising an inﬁnite number of
time-derivatives of the brain state µ.
Consequently, not required is the distinctive
classiﬁcation of time-derivative of the parametric update ( 9µ) and the dynamical update
(Dµ) of the state variable. In what follows, we shall consistently use the dot symbol to
denote time-derivative of a dynamical variable.
3.1. Lagrangian formalism
To formulate the RD from the principle of least action, the ‘Lagrangian’ of the
system must be supplied. We deﬁne the informational Lagrangian (IL) of the brain,
denoted by L, as the Laplace-encoded IFE function,
Lpµ, 9µ; ϕq ” Fpµ, 9µ; ϕq,
where we have placed the semicolon in L to indicate that µ and 9µ are the two brain’s
dynamical variables, given sensory input ϕ.
The sensory inputs are stochastic and
time-dependent, in general; ϕ “ ϕptq, reﬂecting the changing external states, of which
generative processes are to be supplied by physics laws.
The proposed IL is not a
physical quantity but an information-theoretic object. When we take Eq. (12) as an
explicit expression for F, the IL is written up as
Lpµ, 9µ; ϕq “ 1
2σ´1
w p 9µ ´ fpµqq2 ` 1
2σ´1
z pϕ ´ gpµqq2.
(20)
Note that we have dropped out the term, 1
2 ln pσzσwq in writing Eq. (20) by assuming
it as a constant, which then does not aﬀect the dynamics of µ and 9µ. This assumption
of statistical nonstationarity may be lifted by introducing time-dependence in the
variances,
σw “ σwptq
and
σz “ σzptq.
Still, however, the dropped-out term does not aﬀect the dynamics because a term that
can be expressed as a total time-derivative in the Lagrangian will not do anything [20].
Next, we postulate that the perceptual dynamics of the neural particles conforms
to the principle of least action [20]. Accordingly, we suppose that the brain’s perceptual
operation corresponds to searching for an optimal dynamical path that minimizes the
informational action (IA), denoted by S,
S ”
ż tf
ti
dt Lpµ, 9µ; ϕq;
(21)
where tf ´ ti ” T is the temporal horizon over which a living organism encounters an
environmental event. When functional derivative of S is carried out with respect to µ
and 9µ, it gives
δS “
„BL
Bµδµ
tf
ti
´
ż tf
ti
dt
ˆ d
dt
BL
B 9µ ´ BL
Bµ
˙
δµ.

Recognition Dynamics in the Brain under the Free Energy Principle
13
By imposing δS ” 0 under the condition that initial and ﬁnal states are ﬁxed,
δµptiq “ 0 “ δµptfq,
we derive the Lagrangian equation as
d
dt
BL
B 9µ ´ BL
Bµ “ 0.
(22)
Using the speciﬁed Lagrangian, Eq. (20), in Eq. (22), we obtain a Newtonian equation
of motion for the brain variable µ,
σ´1
w 9v “ ¯Λ1 ` ¯Λ2,
(23)
where we have deﬁned the kinematic velocity to be
v ” 9µ
and the additional notations on the RHS as
¯Λ1 ” σ´1
w f Bf
Bµ
and
¯Λ2 ” ´σ´1
z pϕ ´ gqBg
Bµ.
(24)
Equation (23) entails the RD of the brain in the Lagrangian formulation. We interpret
that the inverse of the variance σ´1
w
plays, as a metaphor, a role of inertial mass of
the neural particles. Accordingly, the LHS of Eq. (23) represents an inertial force, i.e.
the product of ‘inertial mass’ and ‘acceleration’, :µ. Note that the inverse of variance
is interpreted as precision in the Friston formulation [8], which gives a measure for the
accuracy of the brain’s expectation or prediction of sensory data. So, the precision is the
‘informational mass’ of the neural particle metaphorically. Also, the terms ¯Λi, i “ 1, 2,
on the RHS are interpreted as the ‘forces’ that drive the internal ¯Λ1 as well as sensory
¯Λ2 excitations in the brain. The acceleration can be evaluated from :µ “ ř ¯Λi{σ´1
w when
the net force is known.
While the organism’s brain integrates the RD for incoming sensory input, an
optimal trajectory µ˚ptq is continuously achieved in the neural-conﬁguration space. And,
the steady-state condition in the long-time limit t Ñ 8 is given by
9µ˚ “ v˚ “ const,
(25)
where the net force vanishes. Note that equation (25) deﬁning an attractor, µeq “ µ˚p8q,
is more general than the simple guess, 9µ˚ “ 0. The optimal trajectory µ˚ptq minimizes
the IA, which, in turn, provides the organism with the tightest estimate of the sensory
uncertainty, see equation (3).
3.2. Hamiltonian formalism
The mechanical formulation can be made more modish in terms of Hamiltonian
language which admits position and momentum as independent brain variables, instead
of position and velocity in the Lagrangian formulation. The positions and the momenta
span phase space of a physical system, which deﬁnes the neural state space of the
organism’s brain.

Recognition Dynamics in the Brain under the Free Energy Principle
14
The ‘canonical’ momentum p, which is conjugate to the position µ, is deﬁned via
Lagrangian L as [20]
p ” BL
B 9µ “ σ´1
w p 9µ ´ fq ,
(26)
which evidently diﬀers from the ‘kinematic’ momentum σ´1
w v
“
σ´1
w 9µ.
Then,
the informational ‘Hamiltonian’ H may be constructed from the Lagrangian using
Legendre’s transformation [20],
Hpµ, p; ϕq “
ÿ BL
B 9µ 9µ ´ Lpµ, 9µ; ϕq.
(27)
The ﬁrst term on the RHS of Eq. (27) can be further manipulated to give
ÿ BL
B 9µ 9µ “ σ´1
w 9µ2 ´ σ´1
w 9µf.
By plugging the outcome and also the Lagrangian L given in Eq. (20) into Eq. (27), we
obtain the Hamiltonian as a function of µ and p as desired, given ϕ,
Hpµ, p; ϕq “ T ppq ` Vpµ, p; ϕq,
(28)
where Eq. (26) has been used to replace 9µ with p. The ﬁrst term on the RHS of Eq. (28)
is the ‘kinetic energy’ which depends only on momentum,
T ppq “
p2
2σ´1
w
.
(29)
Also, the second term on the RHS of Eq. (28) is the ‘potential energy’ which depends
on both position and momentum,
Vpµ, p; ϕq “ V pµ; ϕq ` pfpµq,
(30)
where we have deﬁned the momentum-independent term separately as V ,
V pµ; ϕq “ ´1
2σ´1
z pϕ ´ gq2.
(31)
We remark that the sensory stimuli ϕ enter the Hamiltonian only through the potential-
energy part V which becomes ‘conservative’ when ϕ is static. Here, we shall assume
that the variances associated with the noisy data are constant.
For time-varying
sensory inputs, in general, the Hamiltonian is nonautonomous.
In Fig. 1 we depict
the conservative potential energy, using three-term approximations for the generative
function,
gpµq « b1 ` b2µ ` b2µ2.
For convenience, we have assumed a ﬁxed sensory input, ϕ “ 15, and set parameters as
pb1, b2, b3q “ p0, 1, 0.01q. We have observed numerically that the static sensory signal ϕ
changes the distance between two unstable ﬁxed points, but do not aﬀect the location
of the stable equilibrium point. Also, the depth of the stable equilibrium valley gets
deeper as the magnitude of ϕ increases.

Recognition Dynamics in the Brain under the Free Energy Principle
15
-100
-50
50 µ
-30
-25
-20
-15
-10
-5
V(µ; )
Figure 1. The potential energy, given in Eq. (31), in arbitrary units; where the dashed
and solid curves are for the variance σz “ 100 and 30, respectively. Both cases exhibit
a stable minimum in the central well and two unstable maxima on the side hills, which
contribute to determining the FE landscape.
Next, we take the total derivative of the Hamiltonian given in Eq. (27) with respect
to µ and 9µ to get
dHpµ, p; ϕq “
ÿ
d pp 9µq ´ dLpµ, 9µ; ϕq
“ 9µdp ` pd 9µ ´
ˆBL
Bµdµ ` BL
B 9µd 9µ
˙
“ ´ 9pµdµ ` 9µdp.
By comparing the last expression with the formal expansion,
dH “ BH
Bµ dµ ` BH
Bp dp,
we identify the Hamilton equations of motion for independent variables µ and p of a
neural particle,
9µ “ BH
Bp
(32)
9p “ ´ BH
Bµ .
(33)
For given H in Eq. (28), we spell out the RHS of Eq. (32) to get
9µ “
1
σ´1
w
p ` f
(34)
which is identical to Eq. (26). Similarly, the second equation, Eq. (33) is spelled out
9p “ ´BV
Bµ ´ Bf
Bµp.
(35)
The ﬁrst term on the RHS of Eq. (35) speciﬁes the conservative force,
´BV
Bµ Ñ ´σ´1
z pϕ ´ gqBg
Bµ.

Recognition Dynamics in the Brain under the Free Energy Principle
16
Whereas, the second term on the RHS of Eq. (35) speciﬁes the dissipative force, where
Bf{Bµ plays the role of damping coeﬃcient.
The derived set of coupled equations for the variables µ and p furnish the RD of the
brain in phase space spanned by µ and p, which involve only ﬁrst order time-derivatives.
When time-derivative is taken once more for both sides of Eq. (34) with followed
substitution of Eq. (35) for 9p, the outcome becomes identical to the Lagrangian equation
of motion, Eq. (23). This observation conﬁrms that two mechanical formulations, one
from the Lagrangian and the other from the Hamiltonian, are in fact equivalent.
In the Hamiltonian formulation, the brain’s fulﬁlling of the RD is equivalent to
ﬁnding an optimal trajectory pµ˚ptq, p˚ptqq in phase space. For a static sensory input,
the dynamics governed by Eqs. (34) and (35) is autonomous, and for the time-dependent
sensory input it becomes non-autonomous. The RD can be integrated by providing
appropriate models for the generative functions f and g. The attractor pµ˚p8q, p˚p8qq
would be a focus or center in phase space, which can be calculated by simultaneously
imposing the conditions on LHSs of Eqs. (34) and (35) ,
9µ˚ “ 0
and
9p˚ “ 0.
(36)
One can readily conﬁrm that these ﬁxed-point conditions match with the Newtonian
equilibrium condition, ř
i ¯Λi “ 0 in the Lagrangian formulation, see section 3.1. The
situation corresponds to the brain’s resting state at a local minimum on the energy
landscape deﬁned by the Hamiltonian function.
3.3. Multivariate formulation
Having established the Hamiltonian dynamics for a single brain variable µ, we now
extend our formulation to the general case of the multivariate brain. We denote tµu as
a row vector of N brain states as done in Sec. 2.1,
tµu “ pµ1, µ2, ¨ ¨ ¨ , µNq,
that respond to the multiple of sensory inputs in a general way,
tϕu “ pϕ1, ϕ2, ¨ ¨ ¨ , ϕNq.
For simplicity, we neglect the statistical correlation of the ﬂuctuations associated with
environmental variables and also with sensory inputs. Then, within the independent-
particle approximation of uncorrelated brain variables, the Laplace-encoded IFE
Eq. (18) furnishes the multivariate Lagrangian,
Lptµu, t 9µu; tϕuq “ 1
2
N
ÿ
α“1
“
σ´1
wα p 9µα ´ fαptµuqq2 ` σ´1
zα pϕα ´ gαptµuqq2‰
,(37)
where we have dropped out the terms which contain only the variances, σzα and σwα,
assuming that the noises are statistically nonstationary. One may extend Eq. (37) to
interacting neural nodes in terms of covariance matrix formulation [8], which is not

Recognition Dynamics in the Brain under the Free Energy Principle
17
our concern here, either. Subsequently, the conjugate momentum to the generalized
coordinate µα is determined by an explicit evaluation of
pα “ BL
B 9µα
“ σ´1
wα p 9µα ´ fαq .
(38)
Note the momentum pα gives a measure of the discrepancy, weighted by the inverse
variance σwα, between the change of the probabilistic representation of the environment
9µα and the organism’s belief of it fα. The weighting factor σ´1
wα is called the precision
in the FEP. In turn, the Hamiltonian of the multivariate brain can be constructed from
Eq. (28) as
Hptµu, tpu; tϕuq “ T ptµu, tpu; tϕuq ` Vptµu, tpu; tϕuq
(39)
where ﬁrst term on the RHS is the kinetic energy,
T ptpu; tϕuq ”
ÿ
α
pα2
2σ´1
wα
(40)
and the potential energy V is identiﬁed as
Vptµu, tpu; tϕuq ”
ÿ
α
„
´1
2σ´1
zα pϕα ´ gαq2 ` pαfα

.
(41)
Then, it is straightforward to derive the RD of the variables µα and pα, given sensory
data ϕα, as
9µα “ BH
Bpα
“
1
σ´1
wα
pα ` fα,
(42)
and for their conjugate momenta,
9pα “ ´ BH
Bµα
“ ´ Bgα
Bµα
pϕα ´ Bfα
Bµα
pα.
(43)
In writing equation (43), for notational convenience we have introduced an auxiliary
quantity pϕα,
pϕα ” σ´1
zα pϕα ´ gαq.
Equations (42) and (43) are a coupled set of equations for the computational units,
µα and pα, describing the brain states and their conjugate momenta, respectively, given
the sensory discrepancy pϕα between the observed data ϕα and their predictions gαpµαq.
With some working models for fα and gα, they shape the RD in the brain’s multi-
dimensional phase space in the Hamiltonian prescription.
In Fig. 2 we present a schematic illustration of the perceptual circuitry implied by
the RD at a neural node. The classiﬁcation of excitatory and inhibitory activation of the
computational units is not absolute because the overall sign depends on the generative
function fα and map gα, which are not speciﬁed. It is admissible to assume that the brain
is, at the outset, in a resting state. As the sensory inputs ϕα come in, the organism’s
brain performs the RD online, by integrating Eqs. (42) and (43), to attain an optimal
trajectory in neural phase space,
µα “ µ˚
αptq
and
pα “ p˚
αptq,

Recognition Dynamics in the Brain under the Free Energy Principle
18
f(μ)
∂μf(μ)
∂μg(μ)
pφα
pα
μα
g(μ)
φα
Figure 2. The perceptual circuitry at neural node α in which sensory data ϕα stream;
where it is depicted that the computational units µα and pα are positively activated
by arrows and negatively by lines ended with ﬁlled dots. The conjugate momenta pα,
deﬁned in Eq. (38), to the brain variables µα mimic the precision-weighted prediction
errors in the language of predictive coding [27].
which minimize the IA, see Eq. (21). The entire minimization procedure may be stated
abstractly as
pµ˚
α, p˚
αq “ arg min
µα,pα Spµα, pα; ϕ|mq,
(44)
where S is the IA and m has been inserted to indicate explicitly that minimization is
conditioned on the organism as a model of the world.
Note that in our revised RD is involved not only the organism’s prediction of the
environmental change via its representation µα but also the dynamics of its prediction-
error pα.
4. Biophysical implementation
We know that the anatomy and entire functions of an organism’s brain develop from
single cells. In order to provide empirical Bayes in the FEP with a solid biophysical
basis, we must start with known biophysical substrates and then introduce probabilities
to describe a neuron, neurons, and a network.
Until now, however, most work has
taken the reverse direction: Theory prescribes ﬁrst a conjectural model and then tries
to allocate possible neural correlates. At present, our knowledge remains limited on how
biophysical mechanisms of neurons implicate predictions and model aspects about the
environment, while a ‘neurocentric’ approach to the inference problem seems suggestive
to bridge the gap [28, 29].
Here, we regard coarse-grained Hodgkin-Huxley (H-H) neurons as the generic, basic
building-blocks of encoding and transmitting a perceptual message in the brain. The
famous H-H model continues to be used to this day in computational neuroscience
studies of neuronal dynamics [30, 31]. In extracellular electrical recordings, the local
ﬁeld potential and multi-unit activity result in as combined signals from a population

Recognition Dynamics in the Brain under the Free Energy Principle
19
of neurons [32]. Such averaged neuronal variables must subserve the perceptual states
and conduct the cognitive computation in the brain. We shall call them ‘small’ neural
particles and envisage that a small neural particle enacts a node that collectively forms
the whole neural network on a large scale. Before proceeding, we shall mention that there
are many biophysical eﬀorts to describe such averaged neuronal properties; for instance,
the neural mass models and neural ﬁeld theories are a few examples [33, 34, 35, 36, 37].
Also, we note the bottom-up eﬀort of trying to understand the large-scale brain function
at the cortical microcircuit level based on the averaged, spikes and synaptic inputs over
a coarse-grained time interval [38, 39].
4.1. Single cell description
We ﬁrst present how our formulation may be implemented at a single-cell level by
hypothesizing that each neuron reﬂects the fundamentals of the perceptual computation
of the whole system.
A typical neuron receives current information about its
surroundings from the sensory periphery via glutamate, which excites or inhibits the
membrane potential V with regulating the gating variables γl and ionic concentrations
nl; where l is the ion channel index. We assume that pV, tnlu, tγluq specify the neural
states of a neuron as a neural observer in the neural conﬁgurational space [29]. We
encapsulate the neural states as components in a multi-dimensional row vector,
tµu “ pV, tnlu, tγluq “ pµ1, µ2, µ3, ¨ ¨ ¨q.
The H-H equation for excitation of the membrane voltage V
in a spatially
homogeneous cell is given by
C dV
dt “
ÿ
l
γlGlpEl ´ V q ` Iexptq
(45)
where C is the membrane capacitance, Gl is the maximal conductance of ion channel l,
γl is the probability factor associated with opening or closing channel l which in general
a product of activation and inactivation gating variables, and Iex is the external driving
current. For simplicity, contributions from leakage current as well as synaptic input are
assumed to be included in the external currents. The reverse potential El of l-th ion
channel is given, allowing its time-dependence via nonequilibrium ion-concentrations, in
general, as
Elptq “ kBT
ql
ln nliptq
nloptq,
(46)
where kB is the Boltzmann constant, T is the metabolic temperature of an organism,
ql is the ionic charge of channel l, and nliptq and nloptq are the instantaneous ion
concentrations inside and outside the membrane, respectively.
In the steady state
without external current, Iex “ 0, V tends to the resting (Nernst) potential V pt Ñ 8q
with retaining ionic concentrations in electro-chemical equilibrium. The gating variable

Recognition Dynamics in the Brain under the Free Energy Principle
20
γl of ion channels is assumed to obey the kinetics, a diﬀerent model of that may be
preferable,
dγl
dt “ ´ 1
τl
pγl ´ γleqq ` ηl,
(47)
where the relaxation time τl and steady-state gating variable γleq depend on the
membrane potential, in general,
τl “ τlpV q
and
γleq “ γleqpV q,
and ηl is the noise involved in the process.
For ionic concentration dynamics, we suppose that ion concentrations tnlu
vary slowly compared to the membrane potential and gating-channel kinetics, and
consequently treat them statically in our work.
This restriction can be lifted when
a more detailed description is required for ion concentration dynamics. Accordingly, the
reverse potentials El are also treated statically in below.
Then, the state equations for the multivariate neural vaiable tµu neatly map onto
the standard form suggested in the FEP,
dµα
dt “ fαpV, tγlu, tnluq ` wαptq,
(48)
where α runs 1, 2, ¨ ¨ ¨ with implying µ1 “ V , µ2 “ γ1, and µ3 “ γ2, etc. The driving
functions fα, that are speciﬁed to be
fV pV, tγlu; tnluq “ 1
C
ÿ
l
γlGlpEl ´ V q ` 1
C Iex,
(49)
fγlpV, tγlu; tnluq “ ´ 1
τl
pγl ´ γleqq.
(50)
The terms wα in Eq. (48) describe the noisy synaptic and/or leakage current wV ﬂowing
into the neural cell, not the deterministic contribution Iex which is included in fV ,
and the noise wγl “ ηl associated with the activation and inactivation of ion channels,
respectively. For both noises, we assume the Gaussian distributions N p 9µα ´ fα; 0, σwαq
with variances σwα about zero means.
Regarding neuronal response to the sensory stimulus ϕα, we adopt the usual
generative map in the FEP [see Eq. (7)] as
ϕα “ gαpV, tγlu, tnluq ` zα,
(51)
where gα is the generative map that is unknown but must be supplied for practical
application and zα characterizes the stochastic nature of the sensory reading which
we assume the normal distribution N pϕα ´ gα; 0, σzαq. With the present model, we
admit that the neural observer responds to the sensory data instantly by means of the
neuronal states. Currently, we do not possess a ﬁrm ground on biophysical processes of
the sensory prediction.
As a working example, here we consider a H-H neuron which allows fast relaxation,
i.e. τl ! 1, of gating variables to their steady-states, γlptq Ñ γlp8q “ γleqpV q. In this
case our neural particle is fully characterized by a single dynamical variable of V . Note

Recognition Dynamics in the Brain under the Free Energy Principle
21
the time-dependence of the gating variables occurs only implicitly through the long-
time membrane voltages in Eq. (49). Then, the RD of our neural particle is fulﬁlled
in a two-dimensional state space spanned by tµu “ pV, pV q ” pµ, pq, prescribed by the
Hamiltonian function, equation (28),
Hpµ, pq “
p2
2σ´1
w
´ 1
2σ´1
z pϕ ´ gq2 ` pf.
While the ‘dissipative’ function f is explicitly given in the H-H model as
fpµq “ 1
C
ÿ
l
γleqpµqGlpEl ´ µq ` Iex{C,
(52)
the ‘conservative’ function g must be additionally supplied.
Also, one needs to
make the voltage-dependence of γleq available in practice.
Note the Hamiltonian is
nonautonomous, in general, because it explicitly depends on time through both the
sensory input ϕptq and the driving current Iex in f, and also through σwptq, σzptq when
the noisy data are statistically nonstationary.
Figure 3.
Hamiltonian function Hpµ, pq in arbitrary units for the chosen set of
parameters given in the main text; where the black curve on the energy landscape
is the trajectory which is calculated by solving the Hamilton equations of motion for
an initial condition at pµ, pq “ p2.5, ´5.0q. [For interpretation of the references to color
in this ﬁgure, the reader is referred to the web version of this article.]
In Fig. 3 we present the energy landscape described by the Hamiltonian function,
assuming static sensory data, constant driving currents, and statistical stationarity.
Since our knowledge is limited to the functional form of gpµq and fpµq, we have taken
the algebraic approximations [40],
gpµq « a0 ` a1µ ` a2µ2,

Recognition Dynamics in the Brain under the Free Energy Principle
22
fpµq « b0 ` b1µ ` b2µ2 ` b3µ3.
For numerical purposes, we have speciﬁed pa0, a1, a2q “ p0, 1, 1q and pb0, b1, b2, b3q “
p0, 0.1, 1, 1q, and also assumed a ﬁxed sensory input with equal masses (precisions ) on
the brain’s internal model and belief of sensory prediction as
ϕ “ 1.0
and
σ´1
w “ 0.1 “ σ´1
z .
The Hamilton equations of motion, Eqs. (42) and (43), bring about the nonlinear
RD as
9µ “ Λ1pµ, p; tq,
(53)
9p “ Λ2pµ, p; tq,
(54)
where the ‘force’ functions Λ1 and Λ2 are speciﬁed as
Λ1 “ fpµq `
1
σ´1
w
p,
(55)
Λ2 “ ´σ´1
z pϕ ´ gqBg
Bµ ´ Bf
Bµp.
(56)
We have chosen an initial state and solved the equations of motion, for the same
parameters used in Fig. 3, to obtain a trajectory in phase space. The outcome is drawn
on the energy surface in Fig. 3. According to the present model, the neural observer
performs the RD, given the sensory input ϕ and, consequently, obtains the optimal
trajectory pµ˚, p˚q conforming to Eq. (44). In the long-time limit the brain will reach a
ﬁxed (equilibrium) point pµ˚
eq, p˚
eqq in the state space, that is speciﬁed by intersections
of two isoclines,
Λipµ, p; 8q “ 0,
i “ 1, 2.
We have determined the ﬁxed points numerically. It turns out that there exist three
real solutions for the speciﬁed system parameters, p´1.23, 0.05q, p´0.50, ´0.01q, and
p0.07, ´0.04q, which are depicted as the blue dots in Fig. 4. By further analysis, we
have found that only the middle point is a stable equilibrium solution and the other
two specify saddle points. Figure 4 shows a ﬂow of trajectories, obtained from arbitrary
initial points on the red-colored circle of radius µ2 ` p2 “ 1.6, in phase space.
To gain an insight into how the system approaches to a steady state, we inspect
the optimal trajectories near an equilibrium point,
µ˚ « µ˚
eq ` δµ˚
and
p˚ « p˚
eq ` δp˚.
We expand Eqs. (53) and (54) to the linear order in the deviations δµ˚ and δp˚ and,
after rearrangement, obtain the normal form,
d
dt
˜
δµ˚
δp˚
¸
`
˜
R11
R12
R21
R22
¸ ˜
δµ˚
δp˚
¸
“ 0.
(57)

Recognition Dynamics in the Brain under the Free Energy Principle
23
-3
-1.5
1.5
3 µ
-2
-1
1
2p
Figure 4. Optimal trajectories in phase space which are obtained by integrating the
RD, equations (53) and (53), from the initial conditions arbitrarily chosen on the red-
colored circle; where the blue dots are the equilibrium points among which only the
middle dot at p´0.50, ´0.01q is a stable ﬁxed point, and other two points are saddle
points. The stable ﬁxed point turns out to be a center, which we have conﬁrmed by
linear stability analysis and also numerically. [For interpretation of the references to
color in this ﬁgure, the reader is referred to the web version of this article.]
In Eq. (57) the elements of the relaxation (Jacobian) matrix R are speciﬁed to be
R11 “ ´
„Bf
Bµ

eq
,
R12 “ ´ 1
σ´1
w
R21 “ σ´1
z
«
´
ˆBg
Bµ
˙2
` pϕ ´ gqB2g
Bµ2 ´ B2f
Bµ2p
ﬀ
eq
,
R22 “
„Bf
Bµ

eq
;
where the partial derivatives are to be evaluated at the equilibrium points. Here, for
notational convention we denote the column vector as
δψ ”
˜
δµ˚
δp˚
¸
.
Then, the formal solution to Eq. (57) is written as
δψptq “ e´Rtδψp0q.
One may expand the initial state ψp0q in terms of the eigenvectors of R as
δψp0q “
ÿ
cαφα,

Recognition Dynamics in the Brain under the Free Energy Principle
24
where the eigenvalues λα and eigenvectors φα are determined by the secular equation,
Rφα “ λαφα.
Consequently, the solutions to the linear RD at a single node level is completed as
δψptq “
2ÿ
α“1
cαe´λαtφα,
(58)
where the expansion coeﬃcients cα are ﬁxed by the initial condition.
In the linear regime, a geometrical interpretation of the equilibrium solutions is
possible by inspecting the eigenvalues of the Jacobian matrix R. Considering that the
matrix R is not symmetric, we anticipate that the eigenvalues are not real. Furthermore,
because the trace of the relaxation matrix equals zero, the sum of the two eigenvalues
must be zero. Thus, when the determinant of R is positive, the two eigenvalues λ1 and
λ2 would be pure imaginary with opposite sign. Consequently, in the present particular
model, the resulting equilibrium point is likely to be a center.
We have conﬁrmed
numerically that the eigenvalues of the Jacobian corresponding to the stable equilibrium
point in ﬁgure 4 meet the condition for a center.
4.2. The hierarchical neural network
Here, we suppose that there are a ﬁnite number of levels in the perceptual hierarchy
of the whole system and that for simplicity each level is characterized eﬃciently as a
single neural node. Further, we assume that the neural node at hierarchical level i is
described by the coarse-grained, activation and connection variables, denoted as V piq
and Spiq, respectively. The activation variable describes action potential at a node, and
the connection variable describes inter-level synaptic input and output variables. Both
variables are derived from a population of neurons and thus vary on a coarse-grained
space and time scale. The technical details of how one may derive such a coarse-graining
description are not our scope, for a reference see [37]. They form the coordinates in
brain’s conﬁgurational space,
µpiq “ pV piq, Spiqq,
where the superscript runs i “ 1, 2, ¨ ¨ ¨, M, with M denoting the highest level.
We assume that the activation variables V piq obey the eﬀective dynamics with noise
wpiq within each hierarchical level i,
dV piq
dt
“ f piqpV piq, Spiqq ` wpiq,
(59)
which is a direct generalization of Eq. (48) with incorporating the hierarchical
dependence via Spiq. For inter-level dynamics, we propose that the connection variables
are updated by one-level higher connetion as well as activation variables, subjected to
the stochastic equations,
dSpiq
dt
“ gpi`1qpV pi`1q, Spi`1qq ` zpiq,
(60)

Recognition Dynamics in the Brain under the Free Energy Principle
25
where zpiq represents the noise associated with the process.
The brain’s top-down
prediction functions f piq and gpiq must be supplied in practical implementation. Note
there is only spontaneous ﬂuctuation at the top cortical level, i “ M, accordingly
gpM`1q “ 0.
(61)
Also, we constrain that the sensory data ϕ enter the interface (or boundary between)
of the brain and the environment speciﬁed as the lowest hierarchicl level, i “ 1.
Subsequently we assume that the brain’s prediction of the sensory inputs is performed
by way of an instantaneous mapping,
Sp0q “ gp1qpV p1q, Sp1qq ` zp0q,
(62)
where, for notational convenience, we have set
Sp0q ” ϕptq.
We remark that the hierarchical equations Eq. (60) we propose is dissimilar to the
conventional formulation which assumes the static model in the entire hierarchy like the
one Eq. (62) at the sensory interface, see [8]. We treat here the connection variables
dynamically not statically to treat lateral and hierarchical dynamics symmetrically.
The rates of the activation and connection variables may be subjected to diﬀerent time-
scales, that can be incorporated, for instance, by introducing distinctive relaxation-times
in their generative functions. It turns out that our equations suit the formalism of the
Hamilton action principle neatly.
Having speciﬁed our hierarchical model, we write the informational Lagrangian for
the constructed neural network by generalizing Eq. (37) with a single sensory input for
now, as
LpV, 9V ; S, 9S; ϕq “ 1
2
M
ÿ
i“1
mpiq
w
`
εpiq
w
˘2 ` 1
2
M
ÿ
i“0
mpiq
z
`
εpiq
z
˘2 ,
(63)
where mpiq
w and mpiq
z
are the inertia masses, associated with the Gaussian noises, wpiq
and zpiq, respectively, deﬁned to be
mpiq
w ” 1{σpiq
w
and
mpiq
z ” 1{σpiq
z .
(64)
The auxiliary variables in the Lagrangian are deﬁned to be (i ě 1)
εpiq
w ” 9V piq ´ f piq `
V piq, Spiq˘
,
(65)
εpiq
z ” 9Spiq ´ gpi`1q `
V pi`1q, Spi`1q˘
.
(66)
We interpret that εpiq
w
speciﬁes the discrepancy between the change in the present
lateral state and the brain’s on-level prediction, which may be considered as the lateral
prediction-error. On the other hand, εpiq
z
measures the prediction error between the
change in the present hierarchical state and its prediction from one higher level via the
generative map g, which may be viewed as the hierarchical prediction-error. Note εp0q
z
in the second term on the RHS of Eq. (63) is deﬁned separately as
εp0q
z
” Sp0q ´ gp1q `
V p1q, Sp1q˘
,

Recognition Dynamics in the Brain under the Free Energy Principle
26
which speciﬁes an error estimation in sensory prediction at the lowest hierarchical level.
The generalized momenta, conjugate to V piq and Spiq, are readily calculated for
i ě 1, respectively, as
ppiq
V ”
BL
B 9V piq “ mpiq
w εpiq
w ,
(67)
ppiq
S ” BL
B 9Spiq “ mpiq
z εpiq
z ,
(68)
Note that the inverse variances mpiq
w , mpiq
z
have been termed the informational masses,
see discussion below equation (24). The role of the inertial masses is to modulate the
discrepancy between the change of the perceptual states and their prediction. Thus, in
our theory, momentum ppiq
V is a measure of lateral prediction-error modulated by inertial
mass mpiq
w , and momentum ppiq
S is a measure of hierarchical prediction-error modulated
by inertial mass mpiq
z . And, the heavier the mass is, the bigger the precision becomes.
Given the Lagrangian Eq. (63), we can formulate the informational Hamiltonian
by performing a Legendre transformation,
H “
ÿ
i
´
9V piqppiq
V ` 9Spiqppiq
S
¯
´ L.
After some manipulation, we obtain the outcome as
HpV, pV ; S, pS; ϕq “
M
ÿ
i“1
`
T piq ` Vpiq˘
,
(69)
where the infromational kinetic energy is deﬁned to be (i ě 1)
T piqppV , pSq “
1
2mpiq
w
´
ppiq
V
¯2
`
1
2mpiq
z
´
ppiq
S
¯2
,
(70)
and the potential energy to be (i ě 2)
VpiqpV, pV ; S, pS; ϕq ” ppiq
V f piq ` ppiq
S gpi`1q.
(71)
Note the potential energy at the lowest level is speciﬁed separately to be
Vp1q “ pp1q
V f p1q ´
1
2mp0q
z
´
pp0q
S
¯2
;
(72)
where, for notational convention, we have written the weighted prediction-error
associated with the sensory measurement as
pp0q
S ” mp0q
z εp0q
z
“ mp0q
z pϕ ´ gp1qq,
which, unlike ppiq
S for i ě 1, is not a canonical momentum. Consequently, the multi-level
Hamiltonian Eq. (69) has been prescribed via the perceptual states in the hierarchical
chain, i “ 1, 2, ¨ ¨ ¨, M, denoted as a four dimensional column vector ψpiq at each level,
ψpiq “ pV piq, ppiq
V , Spiq, ppiq
S qT ” pψpiq
1 , ψpiq
2 , ψpiq
3 , ψpiq
4 qT,
where T means the transverse operation.
Next, it is straightforward to generate the Hamiltonian equations of motion for the
brain’s perceptual states ψpiq. The results are the coupled diﬀerential equations for the

Recognition Dynamics in the Brain under the Free Energy Principle
27
four computational components at each level (i ě 1), which are, in turn, hierarchically-
connected among adjacent levels:
9V piq “ BH
Bppiq
V
“
1
mpiq
w
ppiq
V ` f piq,
(73)
9Spiq “ BH
Bppiq
S
“
1
mpiq
z
ppiq
S ` gpi`1q,
(74)
9ppiq
V “ ´ BH
BV piq “ ´ Bf piq
BV piqppiq
V ´ Bgpiq
BV piqppi´1q
S
;
(75)
9ppiq
S “ ´ BH
BSpiq “ ´Bf piq
BSpiqppiq
V ´ Bgpiq
BSpiqppi´1q
S
.
(76)
According to the derived RD, the sensory inputs ϕ enter the brain-environment
interface at the level j “ 1, which are instantly predicted by the organism’s lowest-
level generative model gp1qpV p1q, Sp1qq. Subsequently, the resulting prediction-error pp0q
S
acts as a source to update the prediction-errors, pp1q
V
and pp1q
S .
The change of on-
level perceptual states V p1q and Sp1q are predicted by the generative models f p1q and
gp2q with additional modulations from the perceptual momenta pp1q
V
and pp1q
S , being the
lateral and hierarchical prediction-errors, respectively. At higher levels i ě 2, the intra-
level dynamics of the activation state V piq is updated, through Eq. (73), by the on-
level generative function f piq and prediction error ppiq
V , while the change of the current
hierarchical state Spiq is determined, through Eq. (74), by the inter-level prediction gpi`1q
and the on-level prediction error ppiq
S . The organism’s top-down message ﬂow is mediated
by the connection state Spiq via Eq. (74) as pSpi`1q, V pi`1qq Ñ Spiq. And, Eqs. (75), and
(76) govern the coupled, bottom-up propagation of the prediction errors, mediated by
ppiq
S , ppiq
S Ñ pppi`1q
S
, ppi`1q
V
q. In Fig. 5 we draw a diagram that schematically illustrates
the perceptual architecture of the hierarchical network, at lowest two levels, implied by
Eqs. (73)-(76).
Here, we emphasize that the dynamics of precision-weighted prediction errors,
encapsulated in canonical momenta in which mass takes over the role of precision, are
taken into account in our Hamiltonian formulation on an equal footing with the dynamics
of prediction of the state variables. This aspect is also in contrast to the conventional
minimization algorithm which entails diﬀerential equations only for the update of the
brain states without carrying parallel ones for the prediction errors. Consequently, the
message passing in our model shows diﬀerent features in the details compared with the
neural circuitry from the conventional RD [41]. However, the general message ﬂow, in
terms of the computational units, of feedforward, feedback, and lateral connections hold
the same in the hierarchical brain network. We recognize an attempt to incorporate the
brain’s computation of prediction errors in the FEP can be found in a recent tutorial
model [42].
Here, for mathematical compactness, we rewrite the ﬁltering algorithm, Eqs. (73)-
(76), as
dψpiq
α
dt
“ Λpiq
α ptψpiq
α uq,
(77)

Recognition Dynamics in the Brain under the Free Energy Principle
28
S(1)
V(1)
pS
(1)
pV
(1)
S(2)
V(2)
pS
(2)
pV
(2)
φ
pS
(0)
Figure 5.
A schematic of the neural circuitry which conducts the RD Eqs. (73)-
(76) in the hierarchical network of the brain;
where the computational units
pSpiq, V piq, ppiq
S , ppiq
V q, i “ 1, 2, ¨ ¨ ¨ , M, are connected by arrows for excitatory (positive)
inputs and by lines ended with ﬁlled dots for inhibitory (negative) inputs. Note that
the prediction error pp0q
S
of incoming sensory data ϕ, at the lowest level, induces an
inhibitory change in the perceptual momenta ppp1q
S , pp1q
V q. Subsequently, the prediction
error propagates up in the hierarchy, pp1q
S
Ñ ppp2q
S , pp2q
V q, etc. On the other hand, the
top-down message passing is mediated by means of the connection states Spiq. For
instance, the connection state Sp1q is top-down predicted by both units pSp2q, V p2qq
from one higher-level.
where the hierarchical index i runs from 1 to M, α runs from 1 to 4, and the force
function Λpiq
α is the corresponding RHS to each vector component ψpiq
α at cortical level
i. The obtained hierarchical equations are the highlight of our theory, prescribing the
RD of the brain’s sensory inference under the FEP framework.
To apply our formulation to an empirical brain, one needs to supply the generating
function of lateral dynamics f piq and the hierarchical connecting function gpiq, that
enter the force functions Λpiq
α in the perceptual mechanics, Eq. (77). For the generating
function we once again use the H-H model Eq. (49) to write
f piqpV piq, Spiqq “
ÿ
l
γleq ˜Gl
`
El ´ V piq˘
` ˜GSSpiq `
ES ´ V piq˘
,
(78)
where ˜Gl are the channel conductances normalized by the capacitance C. And, the
second term on the RHS accounts for other deterministic driving sources such as leakage
and/or lateral synaptic currents with ˜GS being the normalized synaptic conductance.
The hierarchical connection function, for which we have limited biophysical knowledge,
shall be taken in a simple form here as
gpiqpV piq, Spiqq “ ΓpV piqqSpiq,
(79)

Recognition Dynamics in the Brain under the Free Energy Principle
29
where the function Γ specify the voltage-dependent synaptic plasticity from hierarchical
level i to level i´1. In addition, as in the single-node case, one must supply approximate
models for voltage-dependence of the gating variables γleq and the connection strength
Γ. For instance, one may take the quadratic approximations [40],
γleqpV piqq « bl0 ` bl1V piq ` bl2V piqV piq,
ΓpV piqq « a0 ` a1V piq ` a2V piqV piq.
Having laid down the lateral and hierarchical generative models, the organism’s
brain can now perform the RD given a streaming of noisy inputs. While conducting the
ﬁltering, an optimal trajectory is obtained in multi-dimensional phase space,
ψ˚piq
α
“ ψ˚piq
α
ptq,
which, in the end, tends to a ﬁxed point, ψpiq
α,eq “ ψ˚piq
α
pt Ñ 8q.
The necessary
equilibrium condition to Eq. (77) is speciﬁed by
Λpiq
α ptψpiq
α uq “ 0.
(80)
Although the full time-dependent solutions must be invoked numerically, one may
inspect the perceptual trajectories near a ﬁxed point by linear analysis. To this end, we
consider a small deviation of αth component of the perceptual state vector ψ˚piq
α
at the
cortical level i, δψpiq
α , from the ﬁxed point ψpiq
α,eq,
ψ˚piq
α
« ψpiq
α0 ` δψpiq
α .
Then, we expand Eq. (77) about the ﬁxed point to linear order in the small deviation,
and after some manipulation we get the hierarchical equations for δψpiq
α ,
dδψpiq
α
dt
`
4ÿ
β“1
Rpiq
αβδψpiq
β “
4ÿ
β“1
M
ÿ
j‰i
Φpijq
αβ δψpjq
β ;
(81)
where the αβ component of the 4 ˆ 4 Jacobian matrix at cortical level i is speciﬁed by
Rpiq
αβ “
«
BΛpiq
α
Bψpiq
β
ﬀ
eq
,
and the inter-level connection between level i and level j in the hierarchical pathway is
speciﬁed by
Φpijq
αβ “
«
BΛpiq
α
Bψpjq
β
ﬀ
eq
;
where the subscript eq indicates that the matrix elements are to be evaluated at the
equilibrium points. To cast the inhomogeneous term into a more suggestive form we
further inspect it in detail within the models speciﬁed: We observe ﬁrst that the matrix
elements Φpijq
αβ do not vanish only for α “ 3 because only the force function Λpiq
3 possesses
ψpjq
β
for j ‰ i as variables via gpi`1q [see Eq. (74)]. Second, because gpi`1q depends solely
on the hierarchical level-index i ` 1, only matrix elements with the hierarchical index
j “ i ` 1 survives. Combining these two observations, the source term on the RHS of

Recognition Dynamics in the Brain under the Free Energy Principle
30
Eq. (81) is converted into a vector at level i ` 1 with only a single nonvanishing α “ 3
component,
4ÿ
β“1
M
ÿ
j‰i
Φpijq
αβ δψpjq
β
” δζpi`1q
α
which to be complete we spell out explicitly as
δζpi`1q
α
“ δα3
$
&
%
«
Bgpi`1q
Bψpi`1q
1
ﬀ
eq
δψpi`1q
1
`
«
Bgpi`1q
Bψpi`1q
3
ﬀ
eq
δψpi`1q
3
,
.
- ,
(82)
where δα3 is the Kronecker delta.
Finally, we shall present a formal solution to the linearized perceptual mechanics
Eq. (81), that can be obtained by a direct integration with respect to time. The result
takes the form
δψpiqptq “ e´Rpiqtδψpiqp0q `
ż t
0
dt1e´Rpiqpt´t1qδζpi`1qpt1q.
(83)
We next solve the eigenvalue problem at each hierarchical level, which is deﬁned to be
Rpiqφpiq
α “ λpiq
α φpiq
α ,
(84)
where λpiq
α
and φpiq
α
are the eigenvalues and corresponding eigenvectors at level i,
respectively.
Then, we expand the initial state δψpiqp0q in terms of the complete
eigenvectors:
δψpiqp0q “
ÿ
apiq
α φpiq
α .
(85)
Similarly, we may expand the inhomogeneous vector δζpi`1q as
δζpi`1qpt1q “
ÿ
bpi`1q
α
pt1qφpiq
α ,
(86)
where note the expansion coeﬃcients bpi`1q
α
are time-dependent. By substituting the
expansions Eqs. (85) and (86) into Eq. (83) we obtain the desired formal solution near
equilibrium points,
δψpiqptq “
4ÿ
α“1
aαe´λpiq
α tφpiq
α `
4ÿ
α“1
φpiq
α
ż t
0
dt1e´λpiq
α pt´t1qbpi`1q
α
pt1q.
(87)
The geometrical approach to a ﬁxed point is again determined by the eigenvalues λpiq
α ;
however, the details are driven by the time-dependent generative sources bpi`1q
α
ptq from
one-level higher in the hierarchy.
To sum, responding to sensory streaming ϕ “ Sp0q, at the lowest hierarchical level
(i “ 1), the brain in an initial resting state performs the hierarchical RD by integrating
Eq. (77) to infer the external causes.
The ensuing brain’s computation corresponds
to minimizing the IA, which is an upper bound of the sensory uncertainty, whose
mathematical statement, equation (3), is repeated compactly as
Hrppϕqs ď SrF; ϕs,

Recognition Dynamics in the Brain under the Free Energy Principle
31
where the sensory uncertainty H was deﬁned in Eq. (1) and the IA on the RSH
is expressed here in terms of the hierarchical states as SrF; ϕs “
ş
dtFptψpiq
α u; ϕq.
Conforming to the FEP, the minimum value of IA speciﬁes the tightest bound of the
sensory uncertainty over a relevant biological time-scale, which preserves the organism’s
current model of the environment.
5. Discussion
We have recast the FEP following the principles of mechanics, which articulates
that all living organisms are evolutionally self-organized to tend to minimize the sensory
uncertainty about uninhabitable environmental encounters. The sensory uncertainty is
an average of the surprisal over the sensory density registered on the brain-environment
interface, the sensory surprisal being the self-information of the sensory probability
density. The FEP suggests that the organisms implement the minimization by calling
forth the IFE in the brain.
The time-integral of the IFE gives an estimate of the
upper bound of the sensory uncertainty. We have enunciated that the minimization of
the IFE must continually take place over a ﬁnite temporal horizon of an organism’s
unfolding environmental event.
Our scheme is a generalization of the conventional
theory which approximates minimization of the IFE at each point in time when it
performs the gradient descent. The sensory uncertainty is an information-theoretical
Shannon entropy [43]; however, in this work, we have circumvented the term, ‘sensory
entropy’ to call the sensory uncertainty.
The reason is that ‘minimization of the
sensory entropy’ is reminiscent of Erwin Schr¨odinger’s word, ‘negative entropy’ which
carries a disputable connotation in implying how the living organism avoids decay. He
subsequently suggested FE instead as a more appropriate notion in the context [1].
Conforming to the second law of thermodynamics, the organism’s adaptive ﬁtness of
minimizing the sensory uncertainty must contribute to increasing the total entropy of
the brain and its environment.
We have adopted the Laplace-encoded IFE as an informational Lagrangian in
implementing the FEP under the principle of least action.
And, by subscribing to
the standard Newtonian dynamics, we have considered the IFE a function of position
and velocity as the metaphors for the organism’s brain variable and their ﬁrst-order time
derivative, respectively, in the continuous-time picture. The brain variable maps onto
the ﬁrst-order suﬃcient statistics of the recognition density launched in the organism’s
brain to perform the RD, Bayesian ﬁltering of the noisy sensory data. In the following
Hamiltonian formulation, the RD prescribes momentum, conjugate to a position, as a
mechanical measure of prediction error weighted by mass, the precision. The theoretical
construct of generalized coordinates introduced in the prevailing theory to specify the
extended states of higher-orders of motion has been eschewed in our formulation. The
features of changing world enter our theory via the sensory inputs in continuous time,
and the statistical nonstationarity of the noise is embedded in time-dependence of the
variances of the Gaussian ﬂuctuations in the organism’s belief of the changing state

Recognition Dynamics in the Brain under the Free Energy Principle
32
and sensory generation.
The temporal correlations of the dynamical states may be
incorporated as time-dependent covariances, but not explored in this work. Also, in
our theory all the parameters in the RD are speciﬁed in the Hamiltonian; thus, there
require no extra parameters like learning rates in the gradient descent scheme to control
the speed of convergence to a steady state. According to our formulation, the brain’s
Helmholtzian perception corresponds to ﬁnding an optimal trajectory in the hierarchical
functional network by minimizing the IA. When the brain completes the RD by reaching
a desired ﬁxed-point or a limit cycle, it remains resting, i.e., being spontaneous, until
another sensory stimulus will come in.
We have applied our formalism to a biophysically grounded model for hierarchical
neural dynamics by suggesting that the large-scale architecture of the brain be an
emergent coarse-grained description of the interacting single neurons. We have admitted
the asymmetric top-down rationale of sensory inference in our formalism, which is an
essential facet of the FEP: The sensory inputs at the interface, the lowest hierarchical
level, were assumed to be instantaneously mapped onto the organism’s belief, encoded
in the brain variables, about environmental causes with associated noise. Diﬀerently,
however, from the instant model at the lowest level, we have generalized that the
inter-level ﬁltering in the brain’s functional hierarchy obeys the stochastic dynamics,
supplied with the organism’s dynamical generative model of environmental states. The
resulting RD is deterministic and hierarchical, which notably incorporates dynamics
of both predictions and prediction errors of the perceptual states on an equal footing.
Consequently, the details of the ensuing neural circuitry from our formulation diﬀers
from the one supported by the gradient-descent scheme which generates only dynamics of
the causal and hidden states, not their prediction errors. However, the general structure
of message passing, namely descending predictions and ascending prediction-errors in
the hierarchical network, shows the same. Also, our obtained RD tenably underpins the
causality: For a speciﬁed set of the perceptual positions and corresponding momenta,
at the outset, responding to sensory inputs that may be slow or fast time-dependent,
the RD can be integrated online. The arbitrariness involved in deciding the number of
generalized coordinates for a complete description and also the ambiguity in specifying
unknowable initial conditions can be averted.
In short, it is still a long way to understanding how the Bayesian FEP in
neurosciences may be made congruous with the biophysical reality of the brain.
It
is far from clear how the organism embodies the generative model of the environment
in the physical brain. Our theory only delivers a hybrid of the biologically plausible
information-theoretic framework of the FEP and the mechanical formulation of the
RD under the principle of least action. To borrow what Hopﬁeld puts in words, “it
lies somewhere between a model of neurobiology and a metaphor for how the brain
computes” [44]. We hope that our eﬀort will guide a step forward to unraveling the
challenging problem.

Recognition Dynamics in the Brain under the Free Energy Principle
33
References
[1] Schr¨odinger, E. (1967). What is Life? Mind and Matter. Cambridge: Cambridge University Press.
[2] von Helmholtz, H. (1962). Trietise on physiological optics. Vol. III. Mineola: Dover Publication,
Inc.
[3] Gregory, R. L. (1980). Perceptions as hypoheses. Philosophical Transactions of the Royal Society
of London B. 290: 181-197.
[4] Dayan, P., Hinton, G. E., Neal, R. M., & Zemel, R. S. (1995). The Helmholtz machine. Neural
Computation. 7: 889–904.
[5] Friston, K. (2009). The free-energy principle: a rough guide to the brain? Trends in Cognitive
Science. 13: 293–301.
[6] Friston, K. (2010). The free-energy principle:
a uniﬁed brain theory?
Nature Reviews.
Neuroscience. 11: 127-138.
[7] Friston, K. (2013). Life as we know it. Journal of Royal Society Interface. 10: 20130475.
[8] Buckley, C. L., Kim, C. S., McGregor, S., & Seth, A. K. (2017). The free energy principle for
action and perception: A mathematical review, Journal of Mathematical Psychology. 81: 55-79.
http://dx.doi.org/10.1016/j.jmp.2017.09.004.
[9] Ramstead,
M.
J.
D.,
Badcock,
P.
B.,
&
Friston,
K.
J.
(2017).
https://doi.org/10.1016/j.plrev.2017.09.001.
[10] Maturana, H., & Varela, F. (1980). Autopoiesis and Cognition: The Realization of the Living.
Boston: D. Reidel.
[11] Jazwinski, A. H. (1970). Stochastic Process and Filtering Theory. New York: Academic Press.
[12] Berkes, P., Orban, G., Lengyel, M., & Fiser, J. (2011). Spontaneous cortical activity reveals
hallmakrs of an optimal internal model of the environment. Science. 331: 83-87.
[13] Friston, K. J., Daunizeau, J., Kilner, J., & Kiebel, S. J. (2010). Action and behavior: a free energy
formulation. Biological Cybernetics. 102(3): 227-260.
[14] Friston, K. J., & Stephan, K. E. (2007). Free-energy and the brain. Synthese. 159: 417-458.
[15] Friston. K., Stephan, K., Li, B., & Daunnizeau, J. (2010). Generalized Filtering. Mathematical
Problems in Engineering. 261670.
[16] Friston, K. J. (2008). Variational Filtering. NeuroImage. 41: 747-766.
[17] Friston, K. (2008). Hierarchical models in the brain. PLoS Computational Biology. 4(11): e1000211.
[18] Friston, K. (2006). A free energy principle for the brain. Journal of Physiology-Paris. 100: 70-87.
[19] Sprott, J. C. (1997). Some simple chaotic jerk funcitons. Americal Journal of Physics. 65: 537-543.
[20] Landau, L. P. (1998) Classical Mechanics. (2nd ed.). New York: Springer-Verlag.
[21] Friston, K. J., Daunizeau, J., & Kiebel, S. J. (2009). Reinforcement learning or active inference?.
PLoS One. 4(7): e6421.
[22] Markov, N. T., Vezoli, J., Chameau, P., Falchier, A., Quilodran, R., Huissoud, C., Lamy, C.,
Misery, P., Giroud, P., Ullman, S., Barone, P., Dehay, C., Knoblauch, K., & Kennedy, H.
(2014). Anatomy of hierarchy: Feedforward and feedback pathways in Macaque visual cortx.
Journal of Comparative Neurology. 522:225-259.
[23] Michalareas, G., Vezoli, J., van Pelt, S., Schoﬀelen, J.-M. & Kennedy, H. (2016). Alpha-beta
and gamma rhythms subserve feedback and feedforward inﬂuneces among human visual cortical
areas. Neuron. 89:384-397.
[24] Markov, N. T., & Kennedy, H. (2013). Current Opinion in Neurobiology. 23:187-194.
[25] Friston, K. & Kiebel, S. (2009). Cortical circuits for perceptual inference. Neural Networks. 22:
1093-1104.
[26] Friston, K., Adams, R. A., Perrinet, L., & Breakspear, M. (2012). Frontiers in Psychology. 3: 151.
[27] Rao, R. P. N. & Ballard, D. H. (1999). Predictive coding in the visual cortex: a functional
interpretation of some extra-classical receptive-ﬁeld eﬀects. Natue neuroscience 2(1): 79-87.
[28] Fiorillo, C. D. (2008). Towards a general theory of neural computation based on prediction by
single neurons. PLoS One. 3(10).

Recognition Dynamics in the Brain under the Free Energy Principle
34
[29] Fiorillo, C. D., Kim, J. K., & Hong, S. Z. (2014). The meaning of spikes from the neuron’s
point of view: predivtive homeostatis generates the appearance of randomness. Frontiers in
Computational Neuroscience. 8:49.
[30] Hodgkin, A., & Huxley, A. (1952). A quantitative description of membrane current and its
application to conduction and excitation in nerve. Journla of Physiology,. 117:500-544.
[31] Hille, B. (2001). Ion Channels of Excitable Membranes. (3rd ed.). Sunderland: Sinauer Associates,
Inc.
[32] Einevoll, G. T., Kayser, C., Logothetis, N. K., & Panzeri, S. (2013). Nature Reviews. Neuroscience.
14:770.
[33] Jansen, B. H., Zouridakis, G., & Brandt, E. (1993). A neurophysiologically-based mathematical
model of ﬂash visual potentials. Biological Cybernetics. 68: 275-283.
[34] Jirsa, V. K., & Haken, H. (1996). Field theory of electromagnetic brain activity. Physical Review
Letter. 77: 960-963.
[35] Robinson, P. A., Rennie, C. J., & Wright, J. J. (1997). Propagation and stability of waves of
electrical activity in the cerebral cortex. Physical Review E. 56: 826-840.
[36] David, O., Friston, K. J. (2003). A neural mass model for MEG/EEG: coupling and neuronal
dynamics. Neuroimage. 20: 1743-1755.
[37] Deco, G., Jirsa, V. K., Robinson, P. A., Breakspear, M., & Friston, K. (2008). The dyamic brain:
From spiking neurons to neural masses and cortical ﬁelds. PLoS Compt. Biol. 4: e1000092.
[38] Potjans, T. C., & Diesmann, M. (2014). The Cell-type speciﬁc cortical microcircuit: Relating
structure and activity in a full-scale spiking network model. Cerebral Cortex. 24(3):785-806.
[39] Steyn-Ross, M. L., & Steyn-Ross, D. A. (2016). From individual spiking neurons to population
behavior: Systematic elimination of short-wavelength spatial models. Physical Review E. 93:
022402.
[40] Wilson, H. R. (1999). Simpliﬁed dynamics of human and mammaian neocortical neurons. J. Theor.
Biol. 200: 375-388.
[41] Bastos, A. M., Usrey, W. M., Adams, R. A., Mangun, G. R., Fries P.,& Friston, K. J. (2012).
Canonical microcircuits for predictive coding. Neuron 76: 695-711.
[42] Bogacz, R. (2017). A tutorial on the free-energy framework for modelling perception and learning.
Journal of mathematical psychology, 76 (B): 198–211.
[43] Shannon, C. E. (1948). A mathematical theory of communication. Bell System Technical Journal.
27: 379-423, 623-656.
[44] Hopﬁeld, J. J. (1999). Brain, neural network, and computation. Review of modeern physics. 71:
S431-S437.

