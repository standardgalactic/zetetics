CS 329P : Practical Machine Learning (2021 Fall)
Qingqing Huang, Mu Li, Alex Smola
https://c.d2l.ai/stanford-cs329p
11.4 Prompt-based Learning

Stanford CS 329P (2021 Fall) - https://c.d2l.ai/stanford-cs329p
Prompt-based Learning
• BERT has different pre-training tasks and downstream tasks 
• Usually requires thousands of examples to fine tune for a task 
• Prompting tries to convert downstream tasks to the same 
format of pre-training, i.e. language model 
• GPT made prompt-based learning popular 
• Sentiment analysis: I like this movie. It was great 
• Machine translation: Hello world! => Bonjour le monde!
Prompt
Pos: great  
Neg: terrible

Stanford CS 329P (2021 Fall) - https://c.d2l.ai/stanford-cs329p
Prompt with GPT-3
• A giant transformer decoder with 173B parameters 
• Trained on >500B tokens from CommonCrawl, WebText, books.. 
• OpenAI spent ~$12M to train the model 
• OpenAI provides paid API access (~$0.06 per 1K tokens)…with a waitlist as of 
now 
• General purpose LM with impressive text generation capabilities and 
zero-shot / few-shot learning 
• The LM also understands task specification 
• Prompting: Provide a task description with  
a few (~10) examples, and a prompt

Stanford CS 329P (2021 Fall) - https://c.d2l.ai/stanford-cs329p
GPT-3 Applications
• Write code: a table of the 
richest countries in the 
worlds with column 
name and gdp
• Generate thought 
experiment with classic 
examples as input 
• Check gpt3demo.com for hundreds 
demos such as search engine, NPC 
dialogues, and writing poems

Stanford CS 329P (2021 Fall) - https://c.d2l.ai/stanford-cs329p
Prompt-based Fine-tuning
• Goal: Fine-tune the weights of medium sized LM (e.g. <1B) 
• Design task-specific prompts VS train a new output layer 
• Prompt-based FT is more example efficient than standard FT (100x) 
• Automatic prompt search 
• Label words and template selection
Gao et.al. 2021
The impact of templates and label words

Stanford CS 329P (2021 Fall) - https://c.d2l.ai/stanford-cs329p
Summary
• Prompt-based learning uses prompt to present downstream 
tasks in a language model format 
• GPT-3 directly uses the pre-trained models for downstream tasks 
without updating parameters 
• Using it in fine-tuning leads to better example efficiency

