Behind the Scenes: Density Fields for Single View Reconstruction
Felix Wimbauer1,2
Nan Yang1
Christian Rupprecht3
Daniel Cremers1,2,3
1Technical University of Munich
2MCML
3University of Oxford
{felix.wimbauer, nan.yang, cremers}@tum.de
chrisr@robots.ox.ac.uk
Single Input Image
Density Field
Self-Supervised Loss based on
Multiple Views
Training
Inference
Figure 1. Predicting a Density Field from a Single Image. Through a novel “density ﬁeld” formulation, which decouples geometry
from color, architectural improvements, and a novel self-supervised training scheme, our method learns to predict a volumetric scene
representation from a single image in challenging conditions. The voxel occupancy view shows that our method predicts accurate density
even in occluded regions, which is not possible in traditional depth prediction. Please check out our project page at fwmb.github.io/bts/.
Abstract
Inferring a meaningful geometric scene representation
from a single image is a fundamental problem in computer
vision. Approaches based on traditional depth map predic-
tion can only reason about areas that are visible in the im-
age. Currently, neural radiance ﬁelds (NeRFs) can capture
true 3D including color, but are too complex to be generated
from a single image. As an alternative, we propose to pre-
dict implicit density ﬁelds. A density ﬁeld maps every loca-
tion in the frustum of the input image to volumetric density.
By directly sampling color from the available views instead
of storing color in the density ﬁeld, our scene representation
becomes signiﬁcantly less complex compared to NeRFs, and
a neural network can predict it in a single forward pass. The
prediction network is trained through self-supervision from
only video data. Our formulation allows volume rendering
to perform both depth prediction and novel view synthesis.
Through experiments, we show that our method is able to
predict meaningful geometry for regions that are occluded
in the input image. Additionally, we demonstrate the poten-
tial of our approach on three datasets for depth prediction
and novel-view synthesis.
1. Introduction
The ability to infer information about the geometric
structure of a scene from a single image is of high impor-
tance for a wide range of applications from robotics to aug-
mented reality. While traditional computer vision mainly
focused on reconstruction from multiple images, in the deep
learning age the challenge of inferring a 3D scene from
merely a single image has received renewed attention.
Traditionally, this problem has been formulated as the
task of predicting per-pixel depth values (i.e. depth maps).
One of the most inﬂuential lines of work showed that it is
possible to train neural networks for accurate single-image
depth prediction in a self-supervised way only from video
sequences. [14–16, 29, 44, 51, 58, 59, 61] Despite these ad-
vances, depth prediction methods are not modeling the true
3D of the scene: they model only a single depth value per
pixel. As a result, it is not directly possible to obtain depth
values from views other than the input view without con-
sidering interpolation and occlusion. Further, the predicted
geometric representation of the scenes does not allow rea-
soning about areas that lie behind another object in the im-
age (e.g. a house behind a tree), inhibiting the applicability
of monocular depth estimation to 3D understanding.
Due to the recent advance of 3D neural ﬁelds, the related
task of novel view synthesis has also seen a lot of progress.
Instead of directly reasoning about the scene geometry, the
goal here is to infer a representation that allows rendering
views of the scene from novel viewpoints. While geomet-
ric properties can often be inferred from the representation,
they are usually only a side product and lack visual quality.
Even though neural radiance ﬁeld [32] based methods
achieve impressive results, they require many training im-
1
arXiv:2301.07668v3  [cs.CV]  19 Apr 2023

ages per scene and do not generalize to new scenes. To en-
able generalization, efforts have been made to condition the
neural network on global or local scene features. However,
this has only been shown to work well on simple scenes,
for example, scenes containing an object from a single cat-
egory [43, 57]. Nevertheless, obtaining a neural radiance
ﬁeld from a single image has not been achieved before.
In this work, we tackle the problem of inferring a ge-
ometric representation from a single image by generaliz-
ing the depth prediction formulation to a continuous den-
sity ﬁeld. Concretely, our architecture contains an encoder-
decoder network that predicts a dense feature map from the
input image. This feature map locally conditions a density
ﬁeld inside the camera frustum, which can be evaluated at
any spatial point through a multi-layer perceptron (MLP).
The MLP is fed with the coordinates of the point and the
feature sampled from the predicted feature map by repro-
jecting points into the camera view. To train our method,
we rely on simple image reconstruction losses.
Our method achieves robust generalization and accu-
rate geometry prediction even in very challenging outdoor
scenes through three key novelties:
1. Color sampling. When performing volume render-
ing, we sample color values directly from the input frames
through reprojection instead of using the MLP to predict
color values. We ﬁnd that only predicting density drasti-
cally reduces the complexity of the function the network
has to learn. Further, it forces the model to adhere to the
multi-view consistency assumption during training, leading
to more accurate geometry predictions.
2. Shifting capacity to the feature extractor. In many
previous works, an encoder extracts image features to con-
dition local appearance, while a high-capacity MLP is ex-
pected to generalize to multiple scenes. However, on com-
plex and diverse datasets, the training signal is too noisy for
the MLP to learn meaningful priors. To enable robust train-
ing, we signiﬁcantly reduce the capacity of the MLP and
use a more powerful encoder-decoder that can capture the
entire scene in the extracted features. The MLP then only
evaluates those features locally.
3. Behind the Scenes loss formulation. The continuous
nature of density ﬁelds and color sampling allow us to re-
construct a novel view from the colors of any frame, not just
the input frame. By applying a reconstruction loss between
two frames that both observe areas occluded in the input
frame, we train our model to predict meaningful geometry
everywhere in the camera frustum, not just the visible areas.
We demonstrate the potential of our new approach in
a number of experiments on different datasets regarding
the aspects of capturing true 3D, depth estimation, and
novel view synthesis.
On KITTI [12] and KITTI-360
[26], we show both qualitatively and quantitatively that
our model can indeed capture true 3D, and that our model
achieves state-of-the-art depth estimation accuracy.
On
RealEstate10K [45] and KITTI, we achieve competitive
novel view synthesis results, even though our method is
purely geometry-based. Further, we perform thorough ab-
lation studies to highlight the impact of our design choices.
2. Related Work
In the following, we review the most relevant works that
are related to our proposed method.
2.1. Single-Image Depth Prediction
One of the predominant formulations to capture the ge-
ometric structure of a scene from a single image is predict-
ing a per-pixel depth map. Learning-based methods have
proven able to overcome the inherent ambiguities of this
task by correlating contextual cues extracted from the im-
age with certain depth values. One of the most common
ways to train a method for single-image depth prediction
is to immediately regress the per-pixel ground-truth depth
values [10, 27]. Later approaches supplemented the fully-
supervised training with reconstruction losses [21, 56], or
specialise the architecture and loss formulation [1, 11, 22,
24, 25, 54]. To overcome the need for ground-truth depth
annotations, several papers focused on relying exclusively
on reconstruction losses to train prediction networks. Both
temporal video frames [61] and stereo frames [13], as well
as combinations of both [14, 59] can be used as the recon-
struction target. Different follow-up works reﬁne the archi-
tecture and loss [15, 16, 29, 44, 51, 58]. [60] ﬁrst predicts a
discrete density volume as an intermediate step, from which
depth maps can be rendered from different views. While
they use this density volume for regularization, their focus
is on improving depth prediction and their method does not
demonstrate the ability to learn true 3D.
2.2. Neural Radiance Fields
Many works have investigated alternative approaches to
representing scenes captured from a single or multiple im-
ages, oftentimes with the goal of novel view synthesis. Re-
cently, [32] proposed to represent scenes as neural radiance
ﬁelds (NeRFs). In NeRFs, a multi-layer perceptron (MLP)
is optimized per scene to map spatial coordinates to color
(appearance) and density (geometry) values. By evaluat-
ing the optimized MLP along rays and then integrating the
color over the densities, novel views can be rendered un-
der the volume rendering formulation [30]. Training data
consists of a large number of images of the same scene
from different viewpoints with poses computed from tra-
ditional SFM and SLAM methods [4, 40, 41]. The training
goal is to reconstruct these images as accurately as possi-
ble. NeRF’s impressive performance inspired many follow-
up works, which improve different parts of the architec-
ture [2,3,7,18,20,35,38].
2

In the traditional NeRF formulation, an entire scene is
captured in a single, large MLP. Thus, the trained net-
work cannot be adapted to different settings or used for
other scenes.
Further, the MLP has to have a high ca-
pacity, resulting in slow inference. Several methods pro-
pose to condition such MLPs on feature grids or voxels
[6, 28, 31, 34, 37, 43, 47, 57]. Through this, the MLP needs
to store less information and can be simpliﬁed, speeding up
inference [6, 28, 34, 47]. Additionally, this allows for some
generalization to new scenes [33,43,57]. However, general-
ization is mostly limited to a single object category, or sim-
ple synthetic data, where the scenes differ in local details.
In contrast, our proposed method can generalize to highly
complex outdoor scenes. [5] also improves generalization
through depth supervision and improved ray sampling.
2.3. Single Image Novel View Synthesis
While traditional NeRF-based methods achieve impres-
sive performance when provided with enough images per
scene, they do not work with only a single image of a scene
available. In recent years, a number of methods for novel-
view synthesis (NVS) from a single image emerged.
Several methods [8, 9, 49] predict layered depth images
(LDI) [42] for rendering. Later approaches [46,48] directly
produce a multiplane image (MPI) [62]. [23] predicts a gen-
eralized multiplane image. Instead of directly outputting the
discrete layers, the architecture’s decoder receives a vari-
able depth value, for which it outputs the layer. In [52],
a network predicts both a per-pixel depth and feature map,
which are then used in a neural rendering framework. Other
works [53, 55] perform image decomposition, followed by
classical rendering. While these methods achieve impres-
sive NVS results, the quality of predicted geometry usually
falls short. Some methods even predict novel views without
any geometric representation [39,63].
3. Method
In the following, we describe a neural network architec-
ture that predicts the geometric structure of a scene from a
single image II, as shown in Fig. 2. We ﬁrst cover how we
represent a scene as a continuous density ﬁeld, and then pro-
pose a training scheme that allows our architecture to learn
geometry even in occluded areas.
3.1. Notation
Let II
∈[0, 1]3×H×W
= (R3)Ωbe the input im-
age, deﬁned on a lattice Ω= {1, . . . , H} × {1, . . . , W}.
TI ∈R4×4 and KI ∈R3×4 are the corresponding world-
to-camera pose matrix and projection matrix, respectively.
During training, we have available an additional set of N =
{1, 2, . . . , n} frames Ik, k ∈N with corresponding world-
to-camera pose and projection matrices Tk, Kk, k ∈N.
When assuming homogeneous coordinates, a point x ∈R3
in world coordinates can be projected onto the image plane
of frame k with the following operation: πk(x) = KkTkx
3.2. Predicting a Density Field
We represent the geometric structure of a scene as a func-
tion, which maps scene coordinates x to volume density σ.
We term this function ”density ﬁeld”. Inference happens in
two steps. From the input image II, an encoder-decoder net-
work ﬁrst predicts a pixel-aligned feature map F ∈(RC)Ω.
The idea behind this is that every feature fu = F(u) at pixel
location u ∈Ωcaptures the distribution of local geometry
along the ray from the camera origin through the pixel at u.
It also means that the density ﬁeld is designed to lie inside
the camera frustum. For points outside of this frustum, we
extrapolate features from within the frustum.
To obtain a density value at a 3D coordinate x, we ﬁrst
project x onto the input image u′
I = πI(x) and bilinearly
sample the feature fu′ = F(u′) at that position. This feature
fu′, along with the positional encoding [32] γ(d) of the dis-
tance d between x and the camera origin, and the positional
encoding γ(u′
I) of the pixel, is then passed to a multi-layer
perceptron (MLP) φ. During training, φ and F learn to de-
scribe the density of the scene given the input view. We can
interpret the feature representation fu′ as a descriptor of the
density along a ray through the camera center and pixel u′.
In turn, φ acts as a decoder, that given fu′ and a distance to
the camera, predicts the density at the 3D location x.
σx = φ(fu′
I , γ(d), γ(u′
I))
(1)
Unlike most current works on neural ﬁelds, we do not use φ
to also predict color. This drastically reduces the complex-
ity of the distribution along a ray as density distributions
tend to be simple, while color often contains complex high-
frequency components. In our experiments, this makes cap-
turing such a distribution in a single feature, so that it can
be evaluated by an MLP, much more tractable.
3.3. Volume Rendering with Color Sampling
When rendering the scene from a novel viewpoint, we
do not retrieve color from our scene representation directly.
Instead, we sample the color for a point in 3D space from
the available images. Concretely, we ﬁrst project a point x
into a frame k and then bilinearly sample the color cx,k =
Ik(πk(x)).
By combining σx and cx,k, we can perform volume ren-
dering [19,30] to synthesize novel views. We follow the dis-
cretization strategy of other radiance ﬁeld-based methods,
e.g. [32]. To obtain the color ˆck for a pixel in a novel view,
we emit a ray from the camera and integrate the color along
the ray over the probability of the ray ending at a certain
distance. To approximate this integral, density and color are
evaluated at S discrete steps xi along the ray. Let δi be the
distance between xi and xi+1, and αi be the probability of
3

Encoder-Decoder
Feature Map 
 
Feature 
Density
MLP
Positional encoding
Sampling operation
Projection into frame k
Sampling position after reprojection
Sample color from image
Sample feature from feature map
f
Legend:
c
k
a) Inferring a density field from 
b) Reconstructing a novel view 
 from 
 and 
Implicit density distribution 
 described by 
Feature 
Figure 2. Overview. a) Our method ﬁrst predicts a pixel-aligned feature map F, which describes a density ﬁeld, from the input image II.
For every pixel u′, the feature fu′ implicitly describes the density distribution along the ray from the camera origin through u′. Crucially,
this distribution can model density even in occluded regions (e.g. the house). b) To render novel views, we perform volume rendering. For
any point x, we project x into F and sample fu′. This feature is combined with positional encoding and fed into an MLP to obtain density
σ. We obtain the color c by projecting x into one of the views, in this case, I1, and directly sampling the image.
a ray ending between xi and xi+1. From the previous αjs,
we can compute the probability Ti that the ray does not ter-
minate before xi, i.e. the probability that xi is not occluded.
αi = exp(1 −σxiδi)
Ti =
i−1
Y
j=1
(1 −αj)
(2)
ˆck =
S
X
i=1
Tiαicxi,k
(3)
Similarly, we can also retrieve the expected ray termination
depth, which corresponds to the depth in a depth map. Let
di be the distance between xi and the ray origin.
ˆd =
S
X
i=1
Tiαidi
(4)
This rendering formulation is very ﬂexible. We can sam-
ple the color values from any frame, and, crucially, it can be
a different frame from the input frame. It is even possible
to obtain multiple colors from multiple different frames for
a single ray, which enables reasoning about occluded areas
during training. Note that even though different frames can
be used, the density is always based on features from the
input image and does not change. During inference, color
sampling from different frames is not necessary, everything
can be done based on a single input image.
3.4. Behind the Scenes Loss Formulation
Our training goal is to optimize both the encoder-decoder
network and φ to predict a density ﬁeld only from the input
image, such that it allows the reconstruction of other views.
Similar to radiance ﬁelds and self-supervised depth pre-
diction methods, we rely on an image reconstruction loss.
For a single sample, we ﬁrst compute the feature map F
from II and randomly partition all frames ˆN = {II} ∪N
into two sets Nloss, Nrender. Note that the input image can
end up in any of the two sets. We reconstruct the frames
in Nloss by sampling colors from Nrender using the camera
poses and the predicted densities. The photometric consis-
tency between the reconstructed frames and the frames in
Nloss serves as supervision for the density ﬁeld. In practice,
we randomly sample p patches Pi to use patch-wise photo-
metric measurement. For every patch Pi in Nloss, we obtain
a reconstructed patch ˆPi,k from every frame k ∈Nrender.
We aggregate the costs between Pi and every ˆPi,k by taking
the per-pixel minimum across the different frames k, simi-
lar to [14]. The intuition behind this is that for every patch,
there is a frame in Nrender, which “sees” the same surface.
Therefore, if the predicted density is correct, then it results
in a very good reconstruction and a low error.
For the ﬁnal loss formula, we use a combination of L1
and SSIM [50] to compute the photometric discrepancy, as
well as an edge-aware smoothness term. Let d⋆
i denote the
inverse, mean-normalized expected ray termination depth
of patch Pi. Both Lph and Leas are computed per (x, y)
element of the patch, thus resulting in 2D loss maps. They
are then aggregated when computing L.
Lph =
min
k∈Nrender

λL1L1(Pi, ˆPi,k) + λSSIMSSIM(Pi, ˆPi,k)

(5)
Leas = |δxd⋆
i | e−|δxPi| + |δyd⋆
i | e−|δyPi|
(6)
4

Legend:
Object 2
Object 1
Patch 
Figure 3. Loss in Occluded Regions. Patch P on Object 2 is
occluded by Object 1 in the input frame II and I1. In order to
correctly reconstruct P in I2 from I3, the network needs to predict
density for Object 2 behind Object 1.
L =
p
X
i=1
X
x,y∈P
(Lph + λeasLeas) (x, y)
(7)
Learning true 3D.
Our loss formula Eq. (7) is the same as
for self-supervised depth prediction methods, like [14]. The
key difference, however, is that depth prediction methods
can only densely reconstruct the input image, for which the
per-pixel depth was predicted.
In contrast, our density ﬁeld formulation allows us to re-
construct any frame from any other frame. Consider an area
of the scene, which is occluded in the input II, but visible
in two other frames Ik, Ik+1, as depicted in Fig. 3: During
training, we aim to reconstruct this area in Ik. The recon-
struction based on colors sampled from Ik+1 will give a clear
training signal to correctly predict the geometric structure of
this area, even though it is occluded in II. Note, that in order
to learn geometry about occluded areas, we require at least
two additional views besides the input during training, i.e.
to look behind the scenes.
Handling invalid samples.
While the frustums of the dif-
ferent views overlap for the most part, there is still a chance
of a ray leaving the frustums, thus sampling invalid features,
or sampling invalid colors. Such invalid rays lead to noise
and instability in the training process. Therefore, we pro-
pose a policy to detect and remove invalid rays. Our in-
tuition is that when the amount of contribution to the ﬁ-
nal aggregated color, that comes from invalidly sampled
colors or features, exceeds a certain threshold τ, the ray
should be discarded. Consider a ray that is evaluated at po-
sitions xi, i ∈[1, 2, . . . , S] and reconstructed from frames
K: Oi,k, k ∈{I} ∪K denotes the indicator function that
xi is outside the camera frustum of frame k. Note that we
always sample features from the input frame. We deﬁne
IV(k) to be the function indicating that the rendered color
based on frame k is invalid as:
IV(k) =
S
X
i=1
Tiαi (Oi,I ∨Oi,k) > τ
(8)
Only if IV(k) is true for all frames the ray was reconstructed
from, we ignore the ray when computing the loss value. The
reasoning behind this is that non-invalid rays will still lead
to the lowest error. Therefore, the min operation in Eq. (5)
will ignore the invalid rays.
3.5. Implementation Details
We implement our model in PyTorch [36] on a single
Nvidia RTX A40 GPU with 48GB memory. The encoder-
decoder network follows [14] using a ResNet encoder [17]
and predicts feature maps with 64 channels. The MLP φ is
made lightweight with only 2 fully connected layers and 64
hidden nodes each. We use a batch size of 16 and sample
32 patches of size 8 × 8 from the images for which we want
to compute the reconstruction loss. Every ray is sampled at
64 locations, based on linear spacing in inverse depth. For
more details, e.g. exact network architecture and further hy-
perparameters, please refer to the supplementary material.
4. Experiments
To demonstrate the abilities and advantages of our pro-
posed method, we conduct a wide range of experiments.
First, we demonstrate that our method is uniquely able to
capture a holistic geometry representation of the scene, even
in areas that are occluded in the input image. Additionally,
we also show the effect of different data setups on the pre-
diction quality. Second, we show that our method, even
though depth maps are only a side product of our scene
representation, achieves depth accuracy on par with other
state-of-the-art self-supervised methods, that are speciﬁ-
cally designed for depth prediction. Third, we demonstrate
that, even though our representation is geometry-only, our
method can be used to perform high-quality novel view syn-
thesis from a single image. Finally, we conduct thorough
ablation studies based on occupancy estimation and depth
prediction to justify our design choices.
4.1. Data
For our experiments, we use three different datasets:
KITTI [12], KITTI-360 [26] (autonomous driving), and
RealEstate10K [62] (indoor).
RealEstate10K only has
monocular sequences, while KITTI and KITTI-360 pro-
vide stereo. KITTI-360 also contains ﬁsheye camera frames
facing left and right.
For monocular data, we use three
timesteps per sample, for stereo sequences (possibly with
ﬁsheye frames), we use two timesteps. The ﬁsheye frames
are offset by one second to increase the overlap of the differ-
ent camera frustums.1 Training is performed for 50 epochs
on KITTI (approx. 125k steps), 25 epochs on KITTI-360
(approx. 150k steps), and 360k iterations on RealEstate10K.
1More details on offsets, pose data, and data splits in the supp. mat.
5

No S, F
Ours
Full
No F
MonoDepth2
MINE
Input & Predicted Depth
PixelNeRF
Figure 4. Occupancy Estimation. Top-down visualization of predicted occupancy volumes. We show an area of x = [−9m, 9m], z =
[3m, 21m] and y = [0m, 1m] (just above the road). Our method produces an accurate volumetric reconstruction, even for occluded
regions. Training with more views improves the quality. Depth prediction methods like MonoDepth2 [14] do not predict a full 3D volume.
Thus, objects cast “occupancy shadows” behind them. Volumetric methods like PixelNeRF [57] and MINE [23] produce noisy predictions.
Inference is from a single image. Legend: S: Stereo, F: Fisheye, ⋆: ofﬁcial checkpoint, †: trained in same setup as our Full variant.
Method
Oacc ↑
IEacc ↑
IErec ↑
Depth† [14]
0.94
n/a
n/a
Depth† + 4m [14]
0.91
0.63
0.22
PixelNeRF† [57]
0.92
0.63
0.43
Ours (No S, F)
0.94
0.70
0.06
Ours (No F)
0.94
0.71
0.09
Ours
0.94
0.77
0.43
Table 1. 3D Scene Occupancy Accuracy on KITTI-360. We
evaluate the capability of the model to predict occupancy behind
objects in the image. Ground truth occupancy maps are computed
from 20 consecutive Lidar scans per frame. Depth prediction [14]
naturally has no ability to predict behind occlusions. PixelNeRF
[57] can predict free space in occluded regions, but produces poor
overall geometry. Our method improves when training with more
views. Inference from a single image. Samples are evenly spaced
in a cuboid w = [−4m, 4m], h = [−1m, 0m], d = [3m, 20m]
relative to the camera. Legend: ref. Fig. 4.
We use a resolution of 640 × 192 for KITTI and KITTI-
360, and follow [23] in using a resolution of 384 × 256 for
RealEstate10K.
4.2. Capturing true 3D
Evaluation of fully geometric 3D representations like
density ﬁelds is difﬁcult. Real-world datasets usually only
provide ground truth data captured from a single viewpoint,
e.g. RGB-D frames and Lidar measurements. Nevertheless,
we aim to evaluate and compare this key advantage of our
method both qualitatively and quantitatively. Through our
proposed training scheme, our networks are able to learn to
also predict meaningful geometry in occluded areas.
To overcome the lack of volumetric ground truth, we ac-
cumulate Lidar scans to build reference occupancy maps
for KITTI-360. Consider a single input frame for which
we want to evaluate an occupancy prediction: As KITTI-
360 is a driving dataset with a forward-moving camera, the
consecutive Lidar scans captured a short time later measure
different areas within the camera frustum. Note that these
Lidar measurements can reach areas that are occluded in the
input image. To determine whether a point is occupied, we
check whether it is in front of the measured surface for any
of the Lidar scans. Intuitively, every Lidar measurement
“carves out” unoccupied areas in 3D space. By accumu-
lating enough Lidar scans, we obtain a reliable occupancy
measurement of the entire camera frustum. Whether a point
is visible in the input frame can be checked using the Lidar
scan corresponding to the input frame.2
For every frame, we sample points in a cuboid area in
the camera frustum and compute the following metrics: 1.
Occupancy accuracy (OAcc), 2. Invisible and empty accu-
racy (IEAcc), and 3. Invisible and empty recall (IERec). OAcc
evaluates the occupancy predictions across the whole scene
volume. IEAcc and IERec speciﬁcally evaluate invisible re-
gions, evaluating performance beyond depth prediction.
We train a MonoDepth2 [14] model to serve as a base-
line representing ordinary depth prediction methods. Here,
we consider all points behind the predicted depth to be oc-
cupied. Additionally, we evaluate a version in which we
consider points only up to 4 meters (average car length) be-
hind the predicted depth as occupied. As a second baseline,
we train a PixelNeRF [57] model, one of the most promi-
nent NeRF variants that also has the ability to generalize.
To demonstrate that our loss formulation generates
2More details on the exact procedure and examples in the supp. mat.
6

Model
Volum.
Split
Abs Rel ↓
RMSE ↓
α < 1.25 ↑
PixelNeRF [57]

Eigen [10]
0.130
5.134
0.845
EPC++ [29]

0.128
5.585
0.831
MonoDepth2 [14]

0.106
4.750
0.874
PackNet [16]

0.111
4.601
0.878
DepthHint [51]

0.105
4.627
0.875
FeatDepth [44]

0.099
4.427
0.889
DevNet [60]
()
0.095
4.365
0.895
Ours

0.102
4.407
0.882
MINE [23]

Tuls. [49]
0.137
6.592
0.839
Ours

0.132
6.104
0.873
Table 2. Depth Prediction on KITTI. While our goal is full vol-
umetric scene understanding, we compare to state-of-the-art self-
supervised depth estimation method. Our approach achieves com-
petitive performance while clearly improving over other volumet-
ric approaches like PixelNeRF [57] and MINE [23]. DevNet [60]
performs better, but does not show any results of their volume.
strong training signals for occluded regions, given the right
data, we train our model in several different data conﬁgura-
tions. By removing the ﬁsheye, respectively ﬁsheye, and
stereo frames, the training signal for occluded areas be-
comes much weaker. Tab. 1 reports the obtained results.
The depth prediction baselines achieve a strong over-
all accuracy, but are, by design, not able to predict mean-
ingful free space in occluded areas. PixelNeRF can pre-
dict free space in occluded areas but produces poor over-
all geometry. Our model achieves strong overall accuracy,
while it is also able to recover the geometry of the occluded
parts of the scene. Importantly, our model becomes bet-
ter at predicting free space in occluded areas when train-
ing with more views, naturally providing a better training
signal for occluded areas. To qualitatively visualize these
results we sample the camera frustum in horizontal slices
from the center of the image downwards and aggregate the
density in Fig. 4. This shows the layout of the scene, sim-
ilar to the birds-eye perspective but for density. In the Full
variant, the strong signal lets our model learn sharp object
boundaries, as can be seen for several cars in the examples.
For depth prediction, all objects cast occupancy shadows
along the viewing direction. PixelNeRF predicts a volu-
metric representation with free space in occluded regions.
However, the results are noisy and the geometry is inaccu-
rate. MINE [23] also specializes in predicting a volumetric
representation from a single image. However, it does not
produce meaningful density prediction behind objects. In-
stead, similar to depth prediction, all objects cast occupancy
shadows along the viewing direction.
4.3. Depth Prediction
While our method does not predict depth maps directly,
they can be synthesized as a side product from our repre-
sentation through the expected ray termination depth ˆd. To
MonoDepth2
FeatDepth
DevNet
Ours
Eigen Split
MINE
Ours
Tulsiani Split
PixelNeRF
Figure 5. Depth Prediction on KITTI. Expected ray termination
depth compared with depth prediction results of other state-of-the-
art methods [14, 23, 44, 57, 60] on both the Eigen [10] and [49]
split. Our predictions are very detailed and sharp, and capture the
structure of the scene, even when trained on a smaller split such
as Tulsiani. Visualizations for DevNet and FeatDepth are taken
from [60]. Legend: ref. Fig. 4.
demonstrate that our predicted representation achieves high
accuracy, we train our model on KITTI sequences and com-
pare to both self-supervised depth prediction methods and
volume reconstruction methods.
As can be seen in Tab. 2 and Fig. 5, our method performs
on par with the current state-of-the-art methods for self-
supervised depth prediction. Our synthesized depth maps
capture ﬁner details and contain fewer artifacts, as often
seen with depth maps obtained from neural radiance ﬁeld-
based methods, like PixelNeRF [57] and MINE [23]. Over-
all, we achieve competitive performance, even though depth
prediction is not the main objective of our approach.
4.4. Novel View Synthesis from a Single Image
As we obtain a volumetric representation of a scene from
a single image, we are able to synthesize images from novel
viewpoints by sampling color from the input image. Thus,
we also evaluate novel view synthesis from a single image.
To demonstrate the variability of our approach, we train two
models, one on RealEstate10K [62], and one on the KITTI
(Tulsiani split [49]). As Tab. 4 shows, our method achieves
strong performance on both datasets, despite the fact, that
we only predict geometry and obtain color by sampling the
input image. Our results are comparable with many recent
methods, that were speciﬁcally designed for this task, and of
which some even use sparse depth supervision during train-
ing for RealEstate10K (MPI, MINE). MINE [23] achieves
7

Conﬁguration
Occupancy Estimation
Depth Prediction
Method
Features
MLP
Predicts
Oacc ↑
IEacc ↑
IErec ↑
Abs Rel ↓
RMSE ↓α < 1.25 ↑
Baselines
Enc
Big
σ + c
0.92
0.63
0.41
0.130
5.134
0.845
E+D
Big
σ + c
0.93
0.62
0.43
0.149
5.441
0.800
Enc
Small
σ + c
0.92
0.69
0.31
0.112
4.897
0.860
E+D
Small
σ + c
0.93
0.69
0.15
0.109
4.758
0.864
Enc
Small
σ
0.94
0.77
0.39
0.105
4.590
0.872
Ours
E+D
Small
σ
0.94
0.77
0.43
0.102
4.407
0.882
Ours
Keep invalid rays
0.94
0.77
0.41
0.108
4.493
0.875
Table 3. Ablation Studies. Evaluation of variants with different contributions (predicting only density σ and sampling color, shifting
capacity from the MLP to the feature extractor, discarding invalid rays) turned on / off. Occupancy estimation results on KITTI-360
and depth prediction results on KITTI. The variant using only an encoder, big MLP, and color prediction corresponds exactly to the
PixelNeRF [57] architecture, but with our training scheme. Legend: Enc Encoder, E+D Encoder-Decoder, σ density, c color.
KITTI
RealEstate10K
Model
LPIPS ↓
SSIM ↑PSNR ↑LPIPS ↓
SSIM ↑PSNR ↑
SynSin [52]
n/a
n/a
n/a
1.180
0.740
22.3
Tulsiani [49]
n/a
0.572
16.5
0.176
0.785
23.5
MPI [48]
n/a
0.733
19.5
n/a
n/a
n/a
MINE [23]
0.112
0.828
21.9
0.156
0.822
24.5
PixelNeRF [57]
0.175
0.761
20.1
n/a
n/a
n/a
Ours
0.144
0.764
20.1
0.194
0.755
24.0
Table 4.
Novel View Synthesis.
We test the NVS ability on
KITTI (Tulsiani split [49]) and RealEstate10K (MINE split [23],
target frame randomly sampled within 30 frames). Even though
our method does not predict color, we still achieve strong results.
slightly better accuracy. This can be attributed to them be-
ing able to predict color and thereby circumventing issues
arising from imperfect geometry.
4.5. Ablation Studies
Our architectural design choices are critically important
for the strong performance of our method. To quantify the
impact of the different contributions, we conduct ablation
studies based on occupancy estimation on KITTI-360 and
depth prediction on KITTI. PixelNeRF [57] can be seen as a
basis, which we modify step-by-step to reach our proposed
model. Namely, we 1. shift capacity from the MLP to the
feature extractor and 2. introduce color sampling as an al-
ternative to predicting the color alongside density.
As Tab. 3 shows, reducing the MLP capacity and us-
ing a more powerful encoder-decoder rather than encoder
as a feature extractor allows the model to learn signiﬁcantly
more precise overall geometry. We conjecture that a power-
ful feature extractor is more suited to generalize to unseen
scenes based on a single input image than a high-capacity
MLP. The feature extractor outputs a geometry representa-
tion (i.e. the feature map) of the full scene in a single for-
ward pass. During training, it receives gradient information
from all points sampled in the camera frustum, conditioned
on the input image. Thus, potential noise from small visual
details gets averaged out. On the other hand, the MLP out-
puts density based on the coordinates and is conditioned on
a local feature. The coordinates and feature are different for
every sampled point, rather than per scene. Consequently,
noise will affect the MLP training signiﬁcantly more.
Introducing the sampling of color from the input frames
further boosts accuracy, especially for occupancy estima-
tion in occluded areas. We hypothesize that only predicting
density simpliﬁes the training task signiﬁcantly. Crucially,
the network does not have to hallucinate colors in occluded
regions. Additionally, color sampling enforces strict multi-
view consistency. The network cannot compensate for im-
perfect geometry by predicting the correct color.
Finally, the results show that our policy of discarding in-
valid rays during training improves accuracy by reducing
noise in the training signal. This mainly affects the border
regions of the frustum.
5. Conclusion
In this paper, we introduced a new approach to learn-
ing to estimate the 3D geometric structure of a scene from
a single image. Our method predicts a continuous density
ﬁeld, which can be evaluated at any point in the camera
frustum. The key contributions in our paper are 1. color
sampling, 2. architecture improvements, and 3. a new self-
supervised loss formulation. This enables us to train a net-
work on large in-the-wild datasets with challenging scenes,
such as KITTI, KITTI-360, and RealEstate10K. We show
that our method is able to capture geometry in occluded ar-
eas. We evaluate depth maps synthesized from the predicted
representation achieving comparable results to state-of-the-
art methods. Despite only predicting geometry, our model
even achieves high accuracy for novel view synthesis from
a single image. Finally, we justify all of our design choices
through detailed ablation studies.
Acknowledgements.
This work was supported by the ERC Ad-
vanced Grant SIMULACRON, by the Munich Center for Machine Learn-
ing and by the EPSRC Programme Grant VisualAI EP/T028572/1. C.
R. is supported by VisualAI EP/T028572/1 and ERC-UNION-CoG-
101001212.
8

References
[1] Shubhra Aich, Jean Marie Uwabeza Vianney, Md Amirul Is-
lam, and Mannat Kaur Bingbing Liu. Bidirectional attention
network for monocular depth estimation. In 2021 IEEE In-
ternational Conference on Robotics and Automation (ICRA),
pages 11746–11752. IEEE, 2021. 2
[2] Jonathan T Barron, Ben Mildenhall, Matthew Tancik, Peter
Hedman, Ricardo Martin-Brualla, and Pratul P Srinivasan.
Mip-nerf: A multiscale representation for anti-aliasing neu-
ral radiance ﬁelds. In Proceedings of the IEEE/CVF Inter-
national Conference on Computer Vision, pages 5855–5864,
2021. 2
[3] Jonathan T Barron, Ben Mildenhall, Dor Verbin, Pratul P
Srinivasan, and Peter Hedman. Mip-nerf 360: Unbounded
anti-aliased neural radiance ﬁelds.
In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern
Recognition, pages 5470–5479, 2022. 2
[4] Carlos Campos, Richard Elvira, Juan J G´omez Rodr´ıguez,
Jos´e MM Montiel, and Juan D Tard´os. Orb-slam3: An accu-
rate open-source library for visual, visual–inertial, and mul-
timap slam. IEEE Transactions on Robotics, 37(6):1874–
1890, 2021. 2, 13
[5] Anh-Quan Cao and Raoul de Charette.
Scenerf:
Self-
supervised monocular 3d scene reconstruction with radiance
ﬁelds. arxiv, 2022. 3
[6] Anpei Chen, Zexiang Xu, Andreas Geiger, Jingyi Yu, and
Hao Su. Tensorf: Tensorial radiance ﬁelds. arXiv preprint
arXiv:2203.09517, 2022. 3
[7] Kangle Deng, Andrew Liu, Jun-Yan Zhu, and Deva Ra-
manan. Depth-supervised nerf: Fewer views and faster train-
ing for free. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition, pages 12882–
12891, 2022. 2
[8] Helisa Dhamo, Nassir Navab, and Federico Tombari. Object-
driven multi-layer scene decomposition from a single image.
In Proceedings of the IEEE/CVF International Conference
on Computer Vision, pages 5369–5378, 2019. 3
[9] Helisa Dhamo, Keisuke Tateno, Iro Laina, Nassir Navab, and
Federico Tombari. Peeking behind objects: Layered depth
prediction from a single image. Pattern Recognition Letters,
125:333–340, 2019. 3
[10] David Eigen, Christian Puhrsch, and Rob Fergus. Depth map
prediction from a single image using a multi-scale deep net-
work. Advances in neural information processing systems,
27, 2014. 2, 7, 13, 15, 18
[11] Huan Fu, Mingming Gong, Chaohui Wang, Kayhan Bat-
manghelich, and Dacheng Tao. Deep ordinal regression net-
work for monocular depth estimation. In Proceedings of the
IEEE conference on computer vision and pattern recogni-
tion, pages 2002–2011, 2018. 2
[12] Andreas Geiger, Philip Lenz, Christoph Stiller, and Raquel
Urtasun. Vision meets robotics: The kitti dataset. The Inter-
national Journal of Robotics Research, 32(11):1231–1237,
2013. 2, 5, 12, 13
[13] Cl´ement Godard, Oisin Mac Aodha, and Gabriel J Bros-
tow.
Unsupervised monocular depth estimation with left-
right consistency. In Proceedings of the IEEE conference
on computer vision and pattern recognition, pages 270–279,
2017. 2
[14] Cl´ement Godard, Oisin Mac Aodha, Michael Firman, and
Gabriel J Brostow. Digging into self-supervised monocular
depth estimation. In Proceedings of the IEEE/CVF Inter-
national Conference on Computer Vision, pages 3828–3838,
2019. 1, 2, 4, 5, 6, 7, 12, 13, 15, 18
[15] Juan Luis GonzalezBello and Munchurl Kim. Forget about
the lidar: Self-supervised depth estimators with med proba-
bility volumes. Advances in Neural Information Processing
Systems, 33:12626–12637, 2020. 1, 2
[16] Vitor Guizilini, Rares Ambrus, Sudeep Pillai, Allan Raven-
tos, and Adrien Gaidon.
3d packing for self-supervised
monocular depth estimation.
In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern
Recognition, pages 2485–2494, 2020. 1, 2, 7, 15
[17] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Deep residual learning for image recognition. In Proceed-
ings of the IEEE conference on computer vision and pattern
recognition, pages 770–778, 2016. 5, 12
[18] Ajay Jain, Matthew Tancik, and Pieter Abbeel. Putting nerf
on a diet: Semantically consistent few-shot view synthesis.
In Proceedings of the IEEE/CVF International Conference
on Computer Vision, pages 5885–5894, 2021. 2
[19] James T Kajiya and Brian P Von Herzen. Ray tracing volume
densities. ACM SIGGRAPH computer graphics, 18(3):165–
174, 1984. 3
[20] Mijeong Kim, Seonguk Seo, and Bohyung Han. Infonerf:
Ray entropy minimization for few-shot neural volume ren-
dering.
In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition, pages 12912–
12921, 2022. 2
[21] Yevhen Kuznietsov, Jorg Stuckler, and Bastian Leibe. Semi-
supervised deep learning for monocular depth map predic-
tion. In Proceedings of the IEEE conference on computer
vision and pattern recognition, pages 6647–6655, 2017. 2
[22] Sihaeng Lee, Janghyeon Lee, Byungju Kim, Eojindl Yi, and
Junmo Kim.
Patch-wise attention network for monocular
depth estimation. In Proceedings of the AAAI Conference on
Artiﬁcial Intelligence, volume 35, pages 1873–1881, 2021. 2
[23] Jiaxin Li, Zijian Feng, Qi She, Henghui Ding, Changhu
Wang, and Gim Hee Lee. Mine: Towards continuous depth
mpi with nerf for novel view synthesis. In Proceedings of
the IEEE/CVF International Conference on Computer Vi-
sion, pages 12578–12588, 2021. 3, 6, 7, 8, 12, 13, 15, 18
[24] Zhenyu Li, Zehui Chen, Xianming Liu, and Junjun Jiang.
Depthformer: Exploiting long-range correlation and local in-
formation for accurate monocular depth estimation. arXiv
preprint arXiv:2203.14211, 2022. 2
[25] Zhenyu Li, Xuyang Wang, Xianming Liu, and Junjun Jiang.
Binsformer: Revisiting adaptive bins for monocular depth
estimation. arXiv preprint arXiv:2204.00987, 2022. 2
[26] Yiyi Liao, Jun Xie, and Andreas Geiger. Kitti-360: A novel
dataset and benchmarks for urban scene understanding in 2d
and 3d. IEEE Transactions on Pattern Analysis and Machine
Intelligence, 2022. 2, 5, 12, 13
[27] Fayao Liu, Chunhua Shen, Guosheng Lin, and Ian Reid.
Learning depth from single monocular images using deep
9

convolutional neural ﬁelds.
IEEE transactions on pattern
analysis and machine intelligence, 38(10):2024–2039, 2015.
2
[28] Lingjie Liu, Jiatao Gu, Kyaw Zaw Lin, Tat-Seng Chua, and
Christian Theobalt. Neural sparse voxel ﬁelds. Advances
in Neural Information Processing Systems, 33:15651–15663,
2020. 3
[29] Chenxu Luo, Zhenheng Yang, Peng Wang, Yang Wang, Wei
Xu, Ram Nevatia, and Alan Yuille. Every pixel counts++:
Joint learning of geometry and motion with 3d holistic un-
derstanding. IEEE transactions on pattern analysis and ma-
chine intelligence, 42(10):2624–2641, 2019. 1, 2, 7, 15
[30] Nelson Max. Optical models for direct volume rendering.
IEEE Transactions on Visualization and Computer Graphics,
1(2):99–108, 1995. 2, 3
[31] Lars Mescheder, Michael Oechsle, Michael Niemeyer, Se-
bastian Nowozin, and Andreas Geiger. Occupancy networks:
Learning 3d reconstruction in function space. In Proceedings
of the IEEE/CVF conference on computer vision and pattern
recognition, pages 4460–4470, 2019. 3
[32] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik,
Jonathan T Barron, Ravi Ramamoorthi, and Ren Ng. Nerf:
Representing scenes as neural radiance ﬁelds for view syn-
thesis. Communications of the ACM, 65(1):99–106, 2021. 1,
2, 3, 12
[33] Norman
M¨uller,
Andrea
Simonelli,
Lorenzo
Porzi,
Samuel
Rota
Bul`o,
Matthias
Nießner,
and
Peter
Kontschieder.
Autorf:
Learning 3d object radiance
ﬁelds from single view observations.
In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition, pages 3971–3980, 2022. 3
[34] Thomas M¨uller, Alex Evans, Christoph Schied, and Alexan-
der Keller. Instant neural graphics primitives with a multires-
olution hash encoding.
arXiv preprint arXiv:2201.05989,
2022. 3
[35] Michael Niemeyer, Jonathan T Barron, Ben Mildenhall,
Mehdi SM Sajjadi, Andreas Geiger, and Noha Radwan. Reg-
nerf: Regularizing neural radiance ﬁelds for view synthesis
from sparse inputs. In Proceedings of the IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition, pages
5480–5490, 2022. 2
[36] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer,
James Bradbury, Gregory Chanan, Trevor Killeen, Zeming
Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison,
Andreas Kopf, Edward Yang, Zachary DeVito, Martin Rai-
son, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner,
Lu Fang, Junjie Bai, and Soumith Chintala. PyTorch: An
Imperative Style, High-Performance Deep Learning Library.
In Advances in Neural Information Processing Systems 32,
pages 8024–8035. Curran Associates, Inc., 2019. 5, 12
[37] Songyou Peng, Michael Niemeyer, Lars Mescheder, Marc
Pollefeys, and Andreas Geiger.
Convolutional occupancy
networks.
In European Conference on Computer Vision,
pages 523–540. Springer, 2020. 3
[38] Barbara Roessle, Jonathan T Barron, Ben Mildenhall,
Pratul P Srinivasan, and Matthias Nießner. Dense depth pri-
ors for neural radiance ﬁelds from sparse input views. In
Proceedings of the IEEE/CVF Conference on Computer Vi-
sion and Pattern Recognition, pages 12892–12901, 2022. 2
[39] Mehdi SM Sajjadi, Henning Meyer, Etienne Pot, Urs
Bergmann, Klaus Greff, Noha Radwan, Suhani Vora, Mario
Luˇci´c, Daniel Duckworth, Alexey Dosovitskiy, et al. Scene
representation transformer: Geometry-free novel view syn-
thesis through set-latent scene representations. In Proceed-
ings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition, pages 6229–6238, 2022. 3
[40] Johannes
Lutz
Sch¨onberger
and
Jan-Michael
Frahm.
Structure-from-motion revisited.
In Conference on Com-
puter Vision and Pattern Recognition (CVPR), 2016. 2
[41] Johannes Lutz Sch¨onberger, Enliang Zheng, Marc Pollefeys,
and Jan-Michael Frahm. Pixelwise view selection for un-
structured multi-view stereo.
In European Conference on
Computer Vision (ECCV), 2016. 2
[42] Jonathan Shade, Steven Gortler, Li-wei He, and Richard
Szeliski.
Layered depth images.
In Proceedings of the
25th annual conference on Computer graphics and interac-
tive techniques, pages 231–242, 1998. 3
[43] Prafull Sharma, Ayush Tewari, Yilun Du, Sergey Zakharov,
Rares Ambrus, Adrien Gaidon, William T Freeman, Fredo
Durand, Joshua B Tenenbaum, and Vincent Sitzmann. See-
ing 3d objects in a single image via self-supervised static-
dynamic disentanglement. arXiv preprint arXiv:2207.11232,
2022. 2, 3
[44] Chang Shu, Kun Yu, Zhixiang Duan, and Kuiyuan Yang.
Feature-metric loss for self-supervised learning of depth and
egomotion. In European Conference on Computer Vision,
pages 572–588. Springer, 2020. 1, 2, 7, 15, 18
[45] Nathan Silberman, Derek Hoiem, Pushmeet Kohli, and Rob
Fergus.
Indoor segmentation and support inference from
rgbd images. In European conference on computer vision,
pages 746–760. Springer, 2012. 2
[46] Pratul P Srinivasan, Richard Tucker, Jonathan T Barron,
Ravi Ramamoorthi, Ren Ng, and Noah Snavely. Pushing the
boundaries of view extrapolation with multiplane images. In
Proceedings of the IEEE/CVF Conference on Computer Vi-
sion and Pattern Recognition, pages 175–184, 2019. 3
[47] Towaki Takikawa, Joey Litalien, Kangxue Yin, Karsten
Kreis, Charles Loop, Derek Nowrouzezahrai, Alec Jacobson,
Morgan McGuire, and Sanja Fidler. Neural geometric level
of detail: Real-time rendering with implicit 3d shapes. In
Proceedings of the IEEE/CVF Conference on Computer Vi-
sion and Pattern Recognition, pages 11358–11367, 2021. 3
[48] Richard Tucker and Noah Snavely. Single-view view synthe-
sis with multiplane images. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition,
pages 551–560, 2020. 3, 8
[49] Shubham Tulsiani, Richard Tucker, and Noah Snavely.
Layer-structured 3d scene inference via view synthesis. In
Proceedings of the European Conference on Computer Vi-
sion (ECCV), pages 302–317, 2018. 3, 7, 8, 12, 13, 15, 18
[50] Zhou Wang, Alan C Bovik, Hamid R Sheikh, and Eero P Si-
moncelli. Image quality assessment: from error visibility to
structural similarity. IEEE transactions on image processing,
13(4):600–612, 2004. 4
10

[51] Jamie Watson, Michael Firman, Gabriel J Brostow, and
Daniyar Turmukhambetov. Self-supervised monocular depth
hints. In Proceedings of the IEEE/CVF International Con-
ference on Computer Vision, pages 2162–2171, 2019. 1, 2,
7, 15
[52] Olivia Wiles, Georgia Gkioxari, Richard Szeliski, and Justin
Johnson.
Synsin: End-to-end view synthesis from a sin-
gle image.
In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition, pages 7467–
7477, 2020. 3, 8
[53] Felix Wimbauer, Shangzhe Wu, and Christian Rupprecht.
De-rendering 3d objects in the wild.
In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition, pages 18490–18499, 2022. 3
[54] Felix Wimbauer, Nan Yang, Lukas Von Stumberg, Niclas
Zeller, and Daniel Cremers.
Monorec: Semi-supervised
dense reconstruction in dynamic environments from a sin-
gle moving camera. In Proceedings of the IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition, pages
6112–6122, 2021. 2
[55] Shangzhe Wu, Christian Rupprecht, and Andrea Vedaldi.
Unsupervised learning of probably symmetric deformable
3d objects from images in the wild.
In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition, pages 1–10, 2020. 3
[56] Nan Yang, Rui Wang, Jorg Stuckler, and Daniel Cremers.
Deep virtual stereo odometry: Leveraging deep depth predic-
tion for monocular direct sparse odometry. In Proceedings
of the European Conference on Computer Vision (ECCV),
pages 817–833, 2018. 2
[57] Alex Yu, Vickie Ye, Matthew Tancik, and Angjoo Kanazawa.
pixelnerf: Neural radiance ﬁelds from one or few images.
In Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition, pages 4578–4587, 2021. 2,
3, 6, 7, 8, 12, 14, 15
[58] Weihao Yuan, Xiaodong Gu, Zuozhuo Dai, Siyu Zhu, and
Ping Tan.
New crfs:
Neural window fully-connected
crfs for monocular depth estimation.
arXiv preprint
arXiv:2203.01502, 2022. 1, 2
[59] Huangying Zhan, Ravi Garg, Chamara Saroj Weerasekera,
Kejie Li, Harsh Agarwal, and Ian Reid. Unsupervised learn-
ing of monocular depth estimation and visual odometry with
deep feature reconstruction. In Proceedings of the IEEE con-
ference on computer vision and pattern recognition, pages
340–349, 2018. 1, 2
[60] Kaichen Zhou, Lanqing Hong, Changhao Chen, Hang Xu,
Chaoqiang Ye, Qingyong Hu, and Zhenguo Li.
Devnet:
Self-supervised monocular depth learning via density vol-
ume construction. arXiv preprint arXiv:2209.06351, 2022.
2, 7, 15, 18
[61] Tinghui Zhou, Matthew Brown, Noah Snavely, and David G
Lowe. Unsupervised learning of depth and ego-motion from
video. In Proceedings of the IEEE conference on computer
vision and pattern recognition, pages 1851–1858, 2017. 1, 2
[62] Tinghui Zhou, Richard Tucker, John Flynn, Graham Fyffe,
and Noah Snavely.
Stereo magniﬁcation:
Learning
view synthesis using multiplane images.
arXiv preprint
arXiv:1805.09817, 2018. 3, 5, 7, 12, 13
[63] Tinghui Zhou, Shubham Tulsiani, Weilun Sun, Jitendra Ma-
lik, and Alexei A Efros. View synthesis by appearance ﬂow.
In European conference on computer vision, pages 286–301.
Springer, 2016. 3
11

A. Ethics
This research uses datasets (KITTI [12], KITTI-360 [26], and
RealEstate10K [62]) to develop and benchmark computer vision models.
The datasets are used in a manner compatible with their terms of usage.
Some datasets the images can contain visible faces and other personal
data collected without consent, however there is no processing of bio-
metric information. Images are CC-BY or used in a manner compatible
with the Data Analysis Permission. We do not process biometric informa-
tion. Please see https://www.robots.ox.ac.uk/˜vedaldi/
research/union/ethics.html for further information on ethics
and data protection rights and mitigation.
B. Limitations
As the model makes predictions from a single frame, it can only rely
on priors to predict visible and invisible parts of the scene. Naturally, it
should thus not be used in safety-critical applications, nor outside of a
research setting.
As the model is sampling colors instead of predicting them, it cannot
predict plausible colors for unseen objects. Thus, in the setting of novel
view synthesis, extreme camera pose changes tend to lead to visible arti-
facts in the images.
Similar to self-supervised depth prediction methods, our loss formu-
lation relies on photometric consistency. View-dependent effects are not
modeled explicitly and could therefore introduce noise in the training pro-
cess.
Further, our loss formulation relies on a static scene assumption and
dynamic objects are not modeled explicitly. While this has the potential to
reduce accuracy, there are several reasons why it only has marginal effect
in our case. 1. In most cases, we have stereo frames available, which give
accurate training signals, even for moving objects. 2. The time difference
between the different views is very small (usually in the order of 0.1 sec-
onds. Therefore, even if an object is moving, the introduced noise is rather
small. Nonetheless, it would be an interesting direction for future work to
investigate explicit modelling of dynamic objects in a loss formulation like
ours.
C. Additional Results
In the following, we show additional results for novel view synthesis,
capturing true 3D, and depth prediction. Please also see the video for ad-
ditional qualitative results and explanations. Figures can be found after the
text of the supplementary material.
C.1. Novel View Synthesis
Fig. 9 shows qualitative results from the Tulsiani [49] test split
for KITTI [12].
Fig. 10 shows qualitative results for the test set of
RealEstate10K [62] proposed by MINE [23].
C.2. Capturing True 3D
Fig. 11 shows further visualizations of the predicted density ﬁeld, in
which you can clearly make out the different objects in the scene in the
top-down view.
C.3. Depth Prediction
Fig. 12 shows further results comparing our expected ray termination
depth with results from other depth prediction methods. Tab. 6 reports
additional metrics to the table in the main paper.
D. Technical Details
In the following, we discuss the exact implementation details, network
conﬁgurations, and training setup, so that our results can be reproduced
easily. Further, we provide further details regarding the computation of the
occupancy metrics.
D.1. Implementation Details
We base our implementation on the ofﬁcial code repository published
by [57]. Further, we are inspired by the repository of [14] regarding the
implementation of the image reconstruction loss functions.
Networks.
For encoder, we use a ResNet-50 [17] backbone pretrained
on ImageNet. We rely on the ofﬁcial weights provided by PyTorch [36] /
Torchvision. As decoder, we follow the architecture of MonoDepth2 [14]
with a minor modiﬁcation. Since we output feature maps with C channels
at the same resolution of the input, we do not reduce the features during
upconvolutions below C to prevent information loss. Concretely for every
layer, we have an output channel dimension of ˆCout = max(C, Cout),
where Cout is the output channel dimension of the MonoDepth2 model.
We found C = 64 to give best results.
For the decoding MLP, we use two fully-connected layers with hid-
den dimension C (same as the feature dimension) and ReLU activation
function. We found that more layers do not improve the quality of the re-
construction. Our hypothesis is that the decoding is a simple task that does
not require a network with high capacity.
Rendering.
To obtain a color / expected ray termination depth for a
given ray, we sample S points between znear and zfar. As we deal with
potentially unbounded scenes with many different scales, we use inverse
depth to obtain the ranges for the different parts. For every range, we
uniformly draw one sample. Let di be the depth step for the i-th point
(i ∈[0, S −1]) and r ∼U[0, 1] a random sample from the uniform
distribution between 0 and 1.
di = 1/
 1 −si
znear
+ si
zfar

,
si = i + r
S
(9)
We also experimented with coarse and ﬁne sampling as used in many
NeRF papers (e.g. [32,57]). Here, after sampling the entire range of depths
as above (coarse sampling), we perform importance sampling based on the
returned weights and sampling around the expected ray termination depth
(ﬁne sampling). Further, we also duplicate the MLP: one for coarse and
one for ﬁne sampling. While the outputs of both networks are used for two
seperate reconstructions with separate losses, only the ﬁne reconstruction
results are used for evaluation. This technique is particularly helpful in
NeRFs to increase the visual quality. While the coarse MLP has to model
the density and color distribution for a big range of coordinates, the ﬁne
MLP only has to learn the relevant area around surfaces. In our experi-
ments, we found that we do not get any beneﬁt from adding ﬁne sampling,
both when using two seperate MLPs or one for coarse and ﬁne sampling.
We suspect that our single MLP already has enough capacity to model the
density distribution (we do not model color with the MLP) described by
the feature at sufﬁcient accuracy.
Positional Encoding.
As described in the main paper, we pass di
and u′
I (pixel coordinate) values through a positional encoding function,
before feeding them to the network along side the sampled feature fu′
I .
This positional encoding functions maps the input to sin and cos func-
tions with different frequencies. This is an established practice in methods
where networks have to reason about the spatial location of points in 2D or
3D [32,57]. As we deal with real-world scale of scenes, we ﬁrst normalize
the depth to [−1, 1]. This ensures that the data-range perfectly matches the
used frequencies. u′
I uses normalized pixel coordinates with u′
I ∈[−1, 1]2
already. For a vector, we compute the positional encoding per element as:
γ(x) = [x, sin(xπ20), cos(xπ20), sin(xπ21), cos(xπ21),
. . . , sin(xπ26), cos(xπ26)]
(10)
D.2. Training Conﬁguration
Through preliminary experiments, we found that the following training
conﬁguration yields the best results:
12

Dataset
Split
#Train
#Val.
#Test
KITTI [12]
Eigen [10]
39.810
4.424
697
Tulsiani [49]
11.987
1.243
1.079
KITTI-360 [26]
Ours
98.008
11.451
446
RealEstate10K [62]
MINE [23]
8.954.743
245
3270
Table 5. Dataset Overview. Different datasets used in this work
with information on data split. Our KITTI-360 split is a modiﬁed
version of the split for the image segmentation task.
RealEstate10K
KITTI
KITTI-360
Figure 6. Frame Arrangement per Sample. RealEstate10K only
has monocular sequences. KITTI and KITTI-360 provide stereo.
KITTI-360 also contains ﬁsheye camera frames facing left and
right. Legend: ref. Fig. 3.
In all of our experiments, we use a batch size of 16. In total, we sample
2048 rays for each item in a batch. These rays are grouped in 8 × 8 sized
patches randomly sampled from any of the frames in Nloss From this, the
loss is computed. Following [14], we set λSSIM = 0.85, λL1 = 0.15
and λEAS = 0.001 ∗2. The default learning rate is λ = 10−4 and we
decrease it to λ = 10−5 for the last 20% of the training. For all train-
ings, we use color augmentation (same parameters for all views of an item
in the batch) and ﬂip augmentation (randomly horizontally ﬂip the image
that gets fed into the encoder-decoder and then ﬂip the resulting feature
maps back to avoid changing the geometry of the scene). Tab. 5 shows an
overview of the different datasets and the used split. Fig. 6 visualizes the
frame arrangement and a possible partitioning into Nloss and Nrender for
the different datasets.
KITTI.
We rely on poses computed from ORB-SLAM 3 [4], which
uses the given stereo cameras, intrinsics, and baseline length. To be con-
sistent with popular depth prediction methods, we perform all trainings and
experiments at a resolution of 640 × 192 and rely on the Eigen split [10].
For evaluation, like in most other works, the cut off distance is set to
80m. The training runs for 50 epochs and we reduce the learning rate
after 100.000 iterations. We report depth prediction results for our model
which was trained with two timesteps (input + following) and stereo, i.e.
four frames in total. For occupancy estimation, we also train a model with
three timesteps (previous + input + following) and stereo. Depth prediction
results for this model are on par, but not better than the model trained with
two timesteps. A depth range of znear = 3m and zfar = 80m proved to
work best.
On the Tulsiani split [49], we use the same settings, except that we
train for 150 epochs, as the split contains around 3× fewer samples.
Training in both cases takes around four days.
KITTI-360.
As the setting is very similar to KITTI, we use the same
parameters. Because the dataset is signiﬁcantly larger, we only train for
25 epochs. Training again takes around four days. Additionally to the
two stereo frames, we also have access to ﬁsheye camera pointing left and
right. In order to be able to use them within our implementation, we resam-
ple them based on a virtual perspective camera with the same parameters
as the forward-facing perspective cameras. Note that the ﬁsheye cameras
Figure 7. Fisheye Resampling. KITTI-360 provides frames from
two ﬁsheye cameras, one facing to the left and one facing to the
right. We resample them based on a virtual perspective camera that
has the same camera intrinsics as the perspective forward-facing
cameras. We rotate the virtual camera 15◦downward to maximize
the overlap of the frustums with the forward-facing cameras.
seem to be mounted higher up than the perspective cameras. Therefore,
we rotate the virtual cameras 15◦downwards along the x-axis during the
resampling process. Further, ﬁsheye and forward-facing cameras of the
same timeframe have barely any overlapping visible areas. Therefore, we
offset ﬁsheye cameras by 10 timesteps. Fig. 7 shows examples from the
ﬁsheye cameras and the resampled images.
RealEstate10K
As RealEstate10K contains magnitudes more im-
ages than KITTI or KITTI-360, approximately 8 mio. for training. There-
fore, we deﬁne the training length by the number of iterations, in this
case 360k. We follow [23] and perform all experiments at a resolution of
384 × 256. We train with three frames (previous + input + following) per
item in the batch. As the framerate is very high, we randomly draw an off-
set from the range [1, 30] between the frames. [62] states that all sequences
are normalized to ﬁt a depth range of znear = 1m and zfar = 100m with
inverse depth spacing. We use the poses provided by the dataset.
D.3. Occupancy Metric
We rely on Lidar scans provided by KITTI-360 to build ground-truth
occupancy and visibility maps, which we then use to evaluate our predic-
tion quality. Our evaluation protocol relies on 2D slices parallel to the
ground between the street and the height of the car. This allows to focus
on the interesting regions of the scene, that also contain other objects like
cars and pedestrians, and ignore areas that are not interesting, like the area
below the street or the sky.
Consider now a single input frame, for which we would like to
build our ground-truth occupancy and visibility. As KITTI-360 is an au-
tonomous driving dataset, the vehicle is generally moving forward at a
steady pace. Thus, consecutive Lidar scans captured a short time later
measure different areas within the camera frustum. Note that these Lidar
measurements can reach areas that are occluded in the input image. To
determine whether a point is occupied, we check whether it is in front of
the measured surface for any of the Lidar scans. Intuitively, every Lidar
measurement “carves out” unoccupied areas in 3D space.
Let Li =

xj ∈R3|j ∈[M]
	
be the Lidar scan i timesteps after the
input frame with M measurement points in the world coordinate system.
L0 denotes the Lidar scan captured synchronously to the input frame. Let
Pi denote the vehicle-to-world transformation (for ease of notation we as-
sume that the Lidar scanner is centered at the vehicle pose and that the
13

input frame has the same pose as the the 0-th Lidar scan).
3D interpolation with sparse Lidar point clouds is difﬁcult. Therefore,
we extract slices from the scan pointclouds and project them onto a 2D
plane. Additionally, we convert every point from Cartesian coordinates to
polar coordinates, centered around the origin of the respective Lidar scan.
This makes the measurements much more dense and evaluation of whether
a point is in front or behind a surface easier.
Let ymin, ymax describe the min and max y-coordinate for our slice.
polxz(x) →(α, d) denotes a function to convert a Cartesian coordinate to
a polar coordinate after projecting it onto the xz-plane.
Si =
n
polxz(xj)|xj ∈Li ∧ymin ≤xy
j ≤ymax
o
(11)
For a given Lidar scan, we can now check whether a point x is in front
or behind the measured surface by transforming it into the scan’s coor-
dinate system, converting it to polar coordinates and then comparing the
distance. However, experiments showed that the Lidar scans in KITTI-360
can be fairly noisy, especially for objects like cars, that have translucent or
reﬂective materials. Oftentimes, single outlier points are measured to be at
a much bigger distance. As we rely on the “carving out” idea, such points
would carve out a lot of free space and lead to inaccurate occupancy maps.
To ﬁlter out these outliers, we split the 360 degree range of the polar coor-
dinates into b = 360 equally sized bins and assign every measured point
to the corresponding bin. For every bin Si[α], we then choose the minimal
measured distance.
Using the Si, we now deﬁne a function that allows to check whether a
point is occupied or not. Note that we can obtain the two closest bins for a
given angle α through the ﬂoor and ceil functions.
α, d = polxz(P −1
i
x)
αl, αr = ⌊α⌋, ⌈α⌉
δ = α −αl
αr −αl
is freei(x) = d < ((1 −δ)Si[αl] + δSi[αr])
(12)
We then accumulate several timesteps i ∈[0, N −1] to build a ground-
truth occupancy map. In practice, we consider N = 20 timesteps.
occ(x) = ¬


_
i∈[0,N]
is freei(x)


(13)
Similarly, we determine visibility by only considering the Lidar scan cor-
responding to the input frame:
vis(x) = ¬is free0(x)
(14)
Based on these functions, we can compute the ﬁnal metric results. We
consider a point x to be occupied, if the predicted density is over a thresh-
old: σx > 0.5. Let xi, i ∈[1, Npts] be points we sample from the camera
frustum. Let X¬vis = {i ∈[1, Npts]|¬vis(xi)} be the subset of points
that are invisible, and X¬vis∧¬occ = {i ∈[1, Npts]|¬vis(xi)∧¬occ(xi)}
be the subset of points that are invisible and empty.
Oacc =
1
Npts
Npts
X
i=1
(occ(x) == (σx > 0.5))
(15)
IEacc =
1
|X¬vis|
X
i∈X¬vis
(occ(x) == (σx > 0.5))
(16)
IErec =
1
|X¬vis∧¬occ|
X
i∈X¬vis∧¬occ
(σx < 0.5)
(17)
We sample 2720 points in total, uniformly spaced from a cuboid with
dimensions x = [−4m, 4m], y = [0m, 1m], z = [3m, 20m] (y-axis
facing downward). This means that all points are just above the surface
of the street. Fig. 8 shows examples of the evaluation for two samples.
Evaluation code will be included in the code release.
Lidar GT
Ours
PixelNeRF
Lidar GT
Ours
PixelNeRF
Figure 8. Occupancy Metric on KITTI-360. Visualization of
the 1. occupancy ground-truth accumulated from 20 Lidar scans,
2. predicted occupancy map by our model, and 3. predicted occu-
pancy map by PixelNeRF [57].
E. Additional Considerations
In this section, we discuss hypotheses on the working principles of
our proposed approach, for which we do not have immediate experimental
results.
Training Stability.
Contrary to many NeRF-based methods, we
ﬁnd that our proposed approach offers stable training and that it is not
overly sensitive to changes in hyperparameters. We hypothesize, that this
is due to the nature of color sampling, which shares similarities with clas-
sical stereo matching. When casting a ray and sampling the color from a
frame, the sampling positions will lie on the epipolar line. The best match
on this epipolar line, which would be the desired correspondence point in
stereo matching, will give the smallest loss and a clear training signal. This
is even the case when sampling color from a single or very few frames. In
contrast, with a NeRF formulation, the color gets learned when multiple
rays with the same color go through the same area in space. Therefore,
here we require many views to give a meaningful signal.
Reconstruction Quality.
One of the key advantages of NeRF-
based methods is that they offer a great way to aggregate the information
from many frames that see the same areas of a scene. In our formulation,
color is only aggregated through the min operation in the loss term. In a
setting with many views, NeRF would clearly provide better reconstruction
quality than a density ﬁeld with color sampling.
However, in settings, where there are only few view scenes per scene
available, most areas in the scene have very limited view coverage. This
means, that the aggregation aspect of NeRFs becomes much less relevant
and the ”visual expressiveness” of NeRFs and density ﬁelds with color
sampling converge.
F. Visualizations
Assets for Fig. 2 were taken from Blendswap34 under the CC-BY li-
cense.
3https://blendswap.com/blend/18686
4https://blendswap.com/blend/13698
14

Input Image
Predicted Novel View
Target View
Figure 9. Novel View Synthesis on KITTI. Rendering the right stereo frame based on the density ﬁeld predicted from the left stereo
frame. Colors are also sampled from the same frame we make the prediction from. Areas of the image that are not occluded in both the
input and target frame are reconstructed very accurately.
Model
Volumetric
Split
Abs Rel
Sq Rel
RMSE
RMSElog
α < 1.25
α < 1.252
α < 1.253
PixelNeRF [57]

Eigen [10]
0.130
1.241
5.134
0.220
0.845
0.943
0.974
EPC++ [29]

0.128
1.132
5.585
0.209
0.831
0.945
0.979
MonoDepth 2 [14]

0.106
0.818
4.750
0.196
0.874
0.957
0.975
PackNet [16]

0.111
0.785
4.601
0.189
0.878
0.960
0.982
DepthHint [51]

0.105
0.769
4.627
0.189
0.875
0.959
0.982
FeatDepth [44]

0.099
0.697
4.427
0.184
0.889
0.963
0.982
DevNet [60]
()
0.095
0.671
4.365
0.174
0.895
0.970
0.988
Ours

0.102
0.751
4.407
0.188
0.882
0.961
0.982
MINE [23]

Tulsiani [49]
0.137
1.993
6.592
0.250
0.839
0.940
0.971
Ours

0.132
1.936
6.104
0.235
0.873
0.951
0.974
Table 6. Depth Prediction on KITTI. While our goal is full volumetric scene understanding, we compare to state-of-the-art self-supervised
depth estimation method. Our approach achieves competitive performance while clearly improving over other volumetric approaches like
PixelNeRF [57] and MINE [23]. DevNet [60] performs better, but does not show any results of their volume.
15

Input Image
Predicted Novel View
Target View
Predicted Novel View
Target View
Offset: 5 Frames
Offset: 30 Frames
Figure 10. Novel View Synthesis on RealEstate10K. Rendering a later frame based on the density ﬁeld predicted from the input frame.
Colors are also sampled from the same frame we make the prediction from.
16

Figure 11. Occupancy Estimation. More qualitative top-down visualization of the occupancy map predicted by different methods. We
show an area of x = [−15m, 15m], z = [5m, 30m] and aggregate density from the y-coordinate of the camera 1m downward.
17

MonoDepth 2
FeatDepth
R-MSMF 3
R-MSMF 6
DevNet
Ours
Eigen Split
MINE
Ours
Tulsiani Split
Figure 12. Depth Prediction. Additional visualizations of the expected ray termination depth compared with depth prediction results of
other state-of-the-art methods [14, 23, 44, 60] on both the Eigen [10] and [49] split. Visualizations for DevNet and FeatDepth are taken
from [60].
18

