On Multi-Cause Causal Inference with Unobserved Confounding:
Counterexamples, Impossibility, and Alternatives
Alexander D‚ÄôAmour
Google AI
Abstract
Unobserved confounding is a central barrier
to drawing causal inferences from observa-
tional data.
Several authors have recently
proposed that this barrier can be overcome
in the case where one attempts to infer the
eÔ¨Äects of several variables simultaneously. In
this paper, we present two simple, analytical
counterexamples that challenge the general
claims that are central to these approaches.
In addition, we show that nonparametric
identiÔ¨Åcation is impossible in this setting.
We discuss practical implications, and sug-
gest alternatives to the methods that have
been proposed so far in this line of work: us-
ing proxy variables and shifting focus to sen-
sitivity analysis.
1
INTRODUCTION
Estimating causal eÔ¨Äects in the presence of unobserved
confounding is one of the fundamental challenges of ca-
sual inference from observational data, and is known
to be infeasible in general (Pearl, 2009). This is be-
cause, in the presence of unobserved confounding, the
observed data distribution is compatible with many
potentially contradictory causal explanations, leaving
the investigator with no way to distinguish between
them on the basis of data. When this is the case, we
say that the causal quantity of interest, or estimand, is
not identiÔ¨Åed. Conversely, when the causal estimand
can be written entirely in terms of observable proba-
bility distributions, we say the query is identiÔ¨Åed.
A recent string of work has suggested that progress
can be made with unobserved confounding in the spe-
cial case where one is estimating the eÔ¨Äects of mul-
Proceedings of the 22nd International Conference on Ar-
tiÔ¨Åcial Intelligence and Statistics (AISTATS) 2019, Naha,
Okinawa, Japan. PMLR: Volume 89. Copyright 2019 by
the author(s).
tiple interventions (causes) simultaneously, and these
causes are conditionally independent in the observa-
tional data given the latent confounder (Wang and
Blei, 2018; Tran and Blei, 2017; Ranganath and Per-
otte, 2018).
The structure of this solution is com-
pelling because it admits model checking, is compat-
ible with modern machine learning methods, models
real-world settings where the space of potential inter-
ventions is high-dimensional, and leverages this dimen-
sionality to extract causal conclusions. Unfortunately,
this work does not establish general suÔ¨Écient condi-
tions for identiÔ¨Åcation.
In this paper, we explore some of these gaps, making
use of two simple counterexamples. We focus on the
central question of how much information about the
unobserved confounder can be recovered from the ob-
served data alone, considering settings where progres-
sively more information is available. In each setting,
we show that the information gained about the unob-
served confounder is insuÔ¨Écient to pinpoint a single
causal conclusion from the observed data. In the end,
we show that parametric assumptions are necessary
to identify causal quantities of interest in this setting.
This suggests caution when drawing causal inferences
in this setting, whether one is using Ô¨Çexible modeling
and machine learning methods or parametric models.
Despite these negative results, we discuss how it is still
possible to make progress in this setting under minor
modiÔ¨Åcations to either the data collection or estima-
tion objective. We highlight two alternatives. First,
we discuss estimation with proxy variables, which can
be used to identify causal estimands without paramet-
ric assumptions by adding a small number of variables
to the multi-cause setting (Miao et al., 2016; Louizos
et al., 2017). Secondly, we discuss sensitivity analy-
sis, which gives a principled approach to exploring the
set of causal conclusions that are compatible with the
distribution of observed data.
arXiv:1902.10286v4  [stat.ML]  19 Mar 2019

Multi-Cause Causal Inference: Counterexamples, Impossibility, Alternatives
2
RELATED WORK
This paper primarily engages with the young litera-
ture on multi-cause causal inference whose primary
audience has been the machine learning community.
This line of work is motivated by several applications,
including genome-wide association studies (GWAS)
(Tran and Blei, 2017), recommender systems (Wang
et al., 2018), and medicine (Ranganath and Perotte,
2018). Wang and Blei (2018) include a thorough re-
view of this line of work and application areas.
These papers can be seen as an extension of factor
models to causal settings. IdentiÔ¨Åcation in factor mod-
els is an old topic. The foundational results in this area
are due to Kruskal (1989) and were extended to a wide
variety of settings by Allman et al. (2009). For more
elementary results similar to those in our Ô¨Årst coun-
terexample, see Bollen (1989) or similar introductory
texts on factor analysis.
The approach taken in this paper is an example of sen-
sitivity analysis, which is a central technique for assess-
ing the robustness of conclusions in causal inference.
One prominent approach, due to Rosenbaum and Ru-
bin (1983) posits the existence of a latent confounder,
and maps out the causal conclusions that result when
unidentiÔ¨Åed parameters in this model are assumed to
take certain values. Our second counterexample takes
inspiration from the model suggested in this paper.
3
NOTATION AND
PRELIMINARIES
Consider a problem where one hopes to learn how
multiple inputs aÔ¨Äect an outcome.
Let A
=
(A(1), . . . , A(m)) be a vector of m variables (causes)
whose causal eÔ¨Äects we wish to infer, and let Y be
the scalar outcome variable of interest. We write the
supports of A and Y as A and Y, repsectively. For
example, suppose that A corresponds a set of genes
that a scientist could, in principle, knock out by gene
editing, where A(k) = 1 if the gene remains active and
A(k) = 0 if it is knocked out. In this case, the scien-
tist may be interested in predicting a measure of cell
growth Y if various interventions were applied. For-
mally, we represent this quantity of interest using the
do-operator:
P(Y | do(A)),
which represents the family of distributions of the out-
come Y when the causes A are set to arbitrary values
in A (Pearl, 2009).
In general, it is diÔ¨Écult to infer P(Y | do(A)) from ob-
servational, or non-experimental, data because there
may be background factors, or confounders, that drive
both the outcome Y and the observed causes A. We
represent these confounders with the variable U, with
support U. In the presence of such confounders, the
conditional distribution P(Y | A) may be diÔ¨Äerent
from the intervention distribution P(Y | do(A)).
If U is observed, the following assumptions are suÔ¨É-
cient to identify the intervention distribution:
‚Ä¢ Unconfoundedness:
U
blocks all backdoor
paths between A and Y , and
‚Ä¢ Positivity: P(A = a | U) > 0 almost surely for
each a ‚ààA.
Importantly, under these conditions, no parametric as-
sumptions are necessary to identify the intervention
distribution. We say that the intervention distribution
is nonparametrically identiÔ¨Åed under these conditions.
The unconfoundedness assumption ensures that the
following relation holds between the unobservable in-
tervention distribution and the observable distribu-
tions:
P(Y | do(A = a)) = E[P(Y | do(A = a), U)]
= E[P(Y | A = a, U)]
=
Z
U
P(Y | A = a, U = u)P(U = u)du.
(1)
We will call P(Y | A, U) the conditional outcome dis-
tribution, and P(U) the integrating measure. Mean-
while, the postivity assumption ensures that all pairs
(A, U) are observable in the support of U, so that the
integrand in (1) can be evaluated along each point on
the path of the integral. The intervention distribution
P(Y | do(A = a)) is identiÔ¨Åed under these conditions
because (1) can be written completely in terms of ob-
servable distributions.
4
UNOBSERVED CONFOUNDING
AND MULTIPLE CAUSES
When the confounder U is unobserved, the uncon-
foundedness and positivity assumptions are no longer
suÔ¨Écient for P(Y | do(A = a)) to be identiÔ¨Åed. In this
case, additional assumptions are necessary because (1)
is no longer a function of observable distributions.
The multi-cause approach attempts to infer (1) from
the observable data (A, Y ) alone under assumptions
about the conditional independence structure of this
distribution. SpeciÔ¨Åcally, this approach incorporates
the assumption that the observed distribution of
causes P(A) admits a factorization by the unobserved
confounder U. We group these central assumptions in
Assumption 1 below, and illustrate them in Figure 1.

Alexander D‚ÄôAmour
U
Y
A(m)
¬∑ ¬∑ ¬∑
A(1)
Figure 1: DAG representation of the multi-cause factor
model setting (Assumption 1). The causes A obey a
factor model, and are conditionally independent given
U.
In addition, the latent confounder U blocks all
backdoor paths between causes A = (A(1) ¬∑ ¬∑ ¬∑ A(m))
and outcome Y .
DeÔ¨Ånition 1. We say a variable U factorizes the dis-
tribution of a set of variables A iÔ¨Ä
P(A) =
Z
U
Ô£Æ
Ô£∞
m
Y
j=1
P(A(j) | U = u)
Ô£π
Ô£ªP(U = u)du,
(2)
Assumption 1. There exists an unobserved variable
U such that (i) U blocks all backdoor paths from A to
Y and (ii) U factorizes the distribution of A.
Under this assumption, the most general form of multi-
cause causal inference rests on the following identiÔ¨Åca-
tion claim.
Claim 1. Under Assumption 1, for any variable V
that factorizes A, the following relation (‚àó) holds
Z
V
P(Y | A = a, V = v)P(V = v)dv
(3)
(‚àó)
=
Z
U
P(Y | A = a, U = u)P(U = u)du
= P(Y | do(A = a)).
If this claim were true, one could obtain an arbitrary
factor model for the observed causes satisfying (2) and
calculate P(Y | do(A)).
In Section 5, we present a simple counterexample that
shows that this claim does not hold in general. The
crux of the counterexample is that factorizations of
P(A) are not always unique, and diÔ¨Äerences between
these factorizations can induce diÔ¨Äerent values for (3).
In light of this counterexample, it is natural to ask
whether identiÔ¨Åcation by (1) is feasible in the special
case that the factorization of P(A) is unique. In this
case, we say the factorization is identiÔ¨Åed. Depend-
ing on the speciÔ¨Åcation, a factor model may be identi-
Ô¨Åed under fairly weak conditions, especially when the
latent factor U is categorical; Allman et al. (2009)
present a broad set of suÔ¨Écient conditions.
Claim 2. Under Assumption 1, if the factorization of
P(A) is identiÔ¨Åed, then the intervention distribution
P(Y | do(A = a)) is identiÔ¨Åed by (1).
Ranganath and Perotte (2018) and Tran and Blei
(2017) make a variation of this claim by supposing
that U can be consistently estimated as a function of
A. In this case, the factorization is identiÔ¨Åed in the
limit where the number of causes m grows large.
Claim 3. Under Assumption 1, if there exists an es-
timator of U that is a function of A, ÀÜU(A), such that
ÀÜU(A)
a.s.
‚àí‚ÜíU,
then the intervention distribution is identiÔ¨Åed by
P(Y | do(A = a)) =
Z
U
P(Y | A = a, ÀÜU(A) = u)P( ÀÜU(A) = u)du. (4)
In Section 6, we give a counterexample and a theorem
showing that Claim 2 is false except in the trivial case
that the observational and intervention distributions
coincide; that is, P(Y | do(A = a)) = P(Y | A = a).
In a supporting proposition for this result, we show
speciÔ¨Åcally that Claim 3 is false because the consis-
tency premise implies that the positivity assumption
is violated.
5
FACTORIZATION EXISTENCE IS
INSUFFICIENT
5.1
Setup
In this section, we show that Claim 1 is false by a thor-
ough exploration of a counterexample. SpeciÔ¨Åcally, we
show that, even under Assumption 1, it is possible that
the observed data is compatible with many distinct in-
tevention distributions P(Y | do(A)).
Consider a simple setting where all variables are lin-
early related, and all independent errors are Gaussian.
Letting œµw ‚àºN(0, œÉ2
w) for each w ‚àà{A, Y, U}, the
structural equations for this setting are
U := œµU
A := Œ±U + œµA
Y := Œ≤‚ä§A + Œ≥U + œµY
Here, Œ±, Œ≤ are m √ó 1 column vectors, and Œ≥ is a scalar;
œµA is a m √ó 1 random column vector, and œµY , œµU are
random scalars. This data-generating process satisÔ¨Åes
Assumption 1.
Under this model, the intervention distribution has the
following form:
P(Y | do(A = a)) = N(Œ≤‚ä§a, Œ≥2œÉ2
U + œÉ2
Y ).
We will focus speciÔ¨Åcally on estimating the conditional
E[Y | do(A = a)], which is fully parameterized by Œ≤.

Multi-Cause Causal Inference: Counterexamples, Impossibility, Alternatives
Thus, our goal is to recover Œ≤ from the distribution of
observed data.
The covariance matrix can be written as
Œ£AY U =
Ô£´
Ô£≠
Œ£UU
Œ£UA
Œ£UY
Œ£AU
Œ£AA
Œ£AY
Œ£Y U
Œ£Y A
Œ£Y Y
Ô£∂
Ô£∏
where Œ£AA is m √ó m, Œ£AY = Œ£‚ä§
Y A is m √ó 1, and Œ£Y Y
is 1 √ó 1.
The marginal covariance matrix of the observable vari-
ables (A, Y ) is the bottom-right 2√ó2 sub-matrix of this
matrix. Its entries are deÔ¨Åned by:
Œ£AA = Œ±Œ±‚ä§œÉ2
U + diag(œÉ2
A)
Œ£AY = Œ£AAŒ≤ + Œ≥œÉ2
UŒ±
Œ£Y Y = (Œ≤‚ä§Œ± + Œ≥)2œÉ2
U + Œ≤‚ä§diag(œÉ2
A)Œ≤ + œÉ2
Y
In these equations, the quantity on the LHS is observ-
able, while the structural parameters on the RHS are
unobservable. The goal is to invert these equations to
obtain a unique value for Œ≤.
5.2
Equivalence Class Construction
When m ‚â•3, the number of equations in this model
exceeds the number of unknowns, but there still ex-
ists an equivalence class of structural equations with
parameters
(Œ±1, Œ≤1, Œ≥1, œÉ2
U,1, œÉ2
A,1, œÉ2
Y,1) Ã∏= (Œ±, Œ≤, Œ≥, œÉ2
U, œÉ2
A, œÉ2
Y )
that induce the same observable covariance matrix,
and for which Œ≤1 Ã∏= Œ≤. These parmeterizations can-
not be distinguished by observed data. In this section,
we show how to construct such a class.
The key to this argument is that the scale of U is not
identiÔ¨Åed given A, regardless of the number of causes
m. This is a well-known non-identiÔ¨Åcation result in
conÔ¨Årmatory factor analysis (e.g., Bollen, 1989, Chap-
ter 7). In our example, the expression for Œ£AA does
not change when Œ± and œÉ2
U are replaced with the struc-
tural parameters Œ±1 and œÉ2
U,1:
Œ±1 := c ¬∑ Œ±
œÉ2
U,1 := œÉ2
U/c2.
In the following proposition, we state how the remain-
ing structural variables can be adjusted to maintain
the same observable covariance matrix when the scale
c is changed.
Proposition 1. For any Ô¨Åxed vector of parameters
(Œ±, Œ≤, Œ≥, œÉ2
U, œÉ2
A, œÉ2
Y ) and a valid scaling factor c (de-
Ô¨Åned below), there exists a vector of parameters that
Figure 2: An illustration of the ignorance region for
Œ≤ in an example where the ignorance region can be
represented as a scalar eÔ¨Äect multiplier.
All values
of Œ≤ on the black line are equally supported by the
data, and include both positive and negative eÔ¨Äects of
varying magnitudes.
induces the same observable data distribution.
Œ±1(c) = c ¬∑ Œ±
Œ≤1(c) = Œ≤ + Œ£‚àí1
AAŒ± ¬∑ Œ≥œÉ2
U

1 ‚àí1
c

(5)
Œ≥1(c) = Œ≥
œÉ2
U,1(c) = œÉ2
U/c2
œÉ2
A,1(c) = œÉ2
A
œÉ2
Y,1(c) = Œ£Y Y ‚àí(Œ≤‚ä§
1 Œ±1 + Œ≥1)2œÉ2
U,1
‚àíŒ≤‚ä§
1 diag(œÉ2
A,1)Œ≤1
The factor c is valid if it implies positive œÉ2
Y,1(c).
We call the set of all parameter vectors that correspond
to valid values of c the ignorance region in the param-
eter space. Parameters in the ignorance region cannot
be distinguished on the basis of observed data because
they all imply the same observed data distribution.
We plot an illustration of the ignorance region for Œ≤
from a numerical example in Figure 2.
In this ex-
ample, we set Œ≤ = b ¬∑ 1m√ó1 and Œ± = a ¬∑ 1m√ó1 to be
constant vectors for some constants a and b. In this
case Œ≤1(c) = s(c) ¬∑ b ¬∑ 1m√ó1 is a simple scaling of Œ≤, so
the ignorance region can be represented by the value
of this scalar s(c). In this example, the data cannot
distinguish between eÔ¨Äect vectors that have the oppo-
site sign of the true eÔ¨Äect vector Œ≤, and those those
that overstate the eÔ¨Äect of A by nearly a factor of 2.

Alexander D‚ÄôAmour
5.3
Large-m Asymptotics
The ignorance region does not in general disappear
in the large treatment number (large-m) limit. Here,
we extend our example to an asymptotic frame where
the ignorance region maintains the same (multiplica-
tive) size even as m goes to inÔ¨Ånity. Consider a se-
quence of problems where the number of treatments
analyzed in each problem is increasing in the se-
quence.
Each problem has its own data generating
process, with some structural parameters indexed by
m: (Œ±m, Œ≤m, Œ≥, œÉ2
U, œÉ2
A,m, œÉ2
Y ). We keep the scalar pa-
rameters not indexed by m Ô¨Åxed.
We consider the marginal variance of each A to be
Ô¨Åxed, so for some Ô¨Åxed scalar s2
0, for each problem m,
œÉ2
A,m = 1m√ó1s2
0.
Likewise, we expect the marginal variance of Y to be
relatively stable, no matter how many treatments we
choose to analyze. Given our setup, this means that if
the number of treatments is large, the eÔ¨Äect of each in-
dividual treatment on average needs to become smaller
as m grows large, or else the variance of Y would
increase in m (this is clear from the speciÔ¨Åcation of
Œ£Y Y ). To handle this, we Ô¨Åx some constant scalars a0
and b0 and assume that, for problem m,
Œ±m = 1m√ó1 ¬∑ a0/‚àöm;
Œ≤m = 1m√ó1 ¬∑ b0/‚àöm.
Thus, as m ‚Üí‚àû, the norms of Œ±m and Œ≤m, as well
as their inner product Œ±‚ä§
mŒ≤m, which appears in the
expression for Œ£Y Y , remain Ô¨Åxed. 1
Under this setup, the interval of valid values for the
latent scaling factor c remains Ô¨Åxed for any value of
m.For a Ô¨Åxed c in this interval, we examine how the
corresponding shift vector ‚àÜŒ≤,m(c) = Œ≤1(c)‚àíŒ≤ behaves
as m grows large. The components of the shift ‚àÜŒ≤,m(c)
scale as m‚àí1/2.
SpeciÔ¨Åcally, applying the Sherman-
Morrison formula,
‚àÜŒ≤,m(c) = Œ£‚àí1
AAŒ±m ¬∑ Œ≥œÉ2
U

1 ‚àí1
c

= m‚àí1/2 ¬∑ 1m√ó1 ¬∑
a0
s2
0 + œÉ2
Ua2
0
¬∑ Œ≥œÉ2
U

1 ‚àí1
c

.
Thus, for each k, the ratio of the kth component of
the shift vector relative to the kth component of the
1 The asymptotic frame in this section is not the only
way to maintain stable variance in Y as m increases. In
particular, one could specify the sequence of problems so
that they are projective, and simulate an investigator incre-
mentally adding causes to a Ô¨Åxed analysis. One could then
deÔ¨Åne a sequence of coeÔ¨Écients Œ±(k) for each cause added
to the analysis, putting some conditions on the growth of
the inner product Œ±‚ä§
mŒ≤m and norm of Œ≤m, such as a sparsity
constraint on Œ≤m. Our setup here is simpler.
true parameters remains Ô¨Åxed in m:
‚àÜ(k)
Œ≤,m(c)
Œ≤(k)
m
=
a0
b0(s2
0 + œÉ2
Ua2
0) ¬∑ Œ≥œÉ2
U

1 ‚àí1
c

.
Thus, even asymptotically as m ‚Üí‚àûthere is no iden-
tiÔ¨Åcation.
6
FACTORIZATION UNIQUENESS
IS INSUFFICIENT
6.1
Impossibility of Nonparametric
IdentiÔ¨Åcation
In this section, we consider identiÔ¨Åcation in the multi-
cause setting in the special case where the factoriza-
tion of P(A) by U is unique; that is, we consider the
case where P(A) can be decomposed uniquely into a
mixing measure P(U) and a conditional treatment dis-
tribution P(A | U). In this setting, Claims 2 and 3
assert that P(Y | do(A)) is identiÔ¨Åed by (1).
This
claim arises as a natural response to the counterexam-
ple in the last section, where the non-uniqueness of the
factorization contributes to non-identiÔ¨Åcation.
As in the last section, we show via counterexample
that the conditions in these claims are insuÔ¨Écient for
identiÔ¨Åcation of P(Y | do(A)). In addition, we show
that, in general, parametric assumptions about the
conditional outcome distribution P(Y | U, A) are nec-
essary to identify P(Y | do(A)), except in the case
where there is no confounding, i.e., the intervention
distribution P(Y
| do(A)) and the observed condi-
tional distribution P(Y | A) are equal almost every-
where. We summarize this statement in the following
theorem.
Theorem 1. Suppose that Assumption 1 holds, that
P(U, A) is identiÔ¨Åed, and that the model for P(Y |
U, A) is not subject to parametric restrictions.
Then either P(Y | do(A)) = P(Y | A) almost every-
where, or P(Y | do(A)) is not identiÔ¨Åed.
Put another way, our theorem states that nonparamet-
ric identiÔ¨Åcation is impossible in this setting, except
in the trivial case. In this section, we prove two sup-
porting propositions for this theorem and demonstrate
them in the context of our running example. The proof
of the theorem, which follows almost immediately from
these propositions, appears at the end of the section.
6.2
Counterexample Setup
Let U
be a binary latent variable,
and A
:=
(A(1), ¬∑ ¬∑ ¬∑ , A(m)) a vector of m binary causes. In the
structural model, we assume that the individual causes

Multi-Cause Causal Inference: Counterexamples, Impossibility, Alternatives
A(k) are generated independently and identically as a
function of U. Let the outcome Y be binary, and gen-
erated as a function of U and A.
U := Bern(œÄU)
A(k) := Bern(pA(U))
k = 1, ¬∑ ¬∑ ¬∑ , m
Y := Bern(pY (U, A))
In addition to this structural model, we assume that
m ‚â•3 and that pA(U) is a non-trivial function of U.
These assumptions are suÔ¨Écient for the factorization
of P(A) by U to be unique (Kruskal, 1989; Allman
et al., 2009). Thus, this example satisÔ¨Åes the premise
of Claim 2.
Our goal is to estimate the intervention distribution
for each a ‚ààA using the identity in (1). Here, the
intervention distribution can be summarized by the
following causal parameter:
œÄY |do(a) := P(Y = 1 | do(A = a))
= (1 ‚àíœÄU)pY (0, a) + œÄUpY (1, a).
(6)
Because the factorization of P(A) is identiÔ¨Åed, œÄU and
pA(U) are identiÔ¨Åable. Thus, to calculate (6), it re-
mains to recover pY (U, A) = P(Y = 1 | U, A).
We will show that this conditional probability cannot
be recovered from the observed data. Our approach
will be to analyze the residual distribution P(Y, U |
A = a). For each value A = a, we can characterize
P(U, Y | A = a) by 4 probabilities in a 2√ó2 table (see
Figure 3). We use shorthand notation puy|a := P(U =
u, Y = y | A = a) to denote the value in each cell of
this table. The values in this table are unobservable,
but they are subject to several constraints. The entries
of this table are constrained to be positive and sum
to 1. In addition, because P(Y, A) and P(U, A) are
identiÔ¨Åed, the margins of the table are constrained to
be equal to probabilities given by P(Y | A = a) and
P(U | A = a). We refer to these probabilities with
the shorthand œÄY |a := P(Y = 1 | A = a) and œÄU|a :=
P(U = 1 | A = a). Using this notation, we can rewrite
(1) as:
œÄY |do(a) = (1 ‚àíœÄU)
p01|a
1 ‚àíœÄU|a
+ œÄU
p11|a
œÄU|a
.
(7)
We consider identiÔ¨Åcation in two separate cases, de-
pending on the amount of information about U is con-
tained in the event A = a. We Ô¨Årst consider the case
where P(U | A = a) is non-degenerate, so that there
is residual uncertainty about U after A is observed.
We then consider the degenerate case, where U can be
deterministically reconstructed as a function of A (the
premise of Claim 3).
p00|a
U = 0
Y = 0
p01|a
Y = 1
1 ‚àíœÄU|a
P(U | A = a)
p10|a
U = 1
p11|a
œÄU|a
1 ‚àíœÄY |a
P(Y | A = a)
œÄY |a
Figure 3: 2√ó2 table representation of P(U, Y | A), the
joint distribution of confounder U and outcome Y con-
ditional on causes A, in the all-binary example. When
the factor model for P(U) is identiÔ¨Åed, the observed
data Ô¨Åx the margins of the table, but leave one degree
of freedom unidentiÔ¨Åed.
6.3
Non-Degenerate Case
When P(U | A = a) is not degenerate, the table in
Figure 3 is underdetermined by the data, and has one
remaining degree of freedom. This degree of freedom
determines the dependence between Y and U con-
ditional on A.
We parameterize this dependence in
terms of p11|a := P(U = 1, Y = 1 | A = a). For pur-
poses of interpretation, p11|a is linearly related to the
cor(U, Y, | A = a) (Joe, 1997, Sec 7.1.1):
cor(U, Y | A = a) =
p11|a ‚àíœÄU|aœÄY |a
œÄU|a(1 ‚àíœÄU|a)œÄY |a(1 ‚àíœÄY |a).
The constraints on the table constrain p11|a to lie in
the following range (Joe, 1997):
max{0, œÄU|a + œÄY |a ‚àí1} ‚â§p11|a ‚â§min{œÄU|a, œÄY |a}.
(8)
Fixing a value for p11|a in this range determines the
values of all four cells in the table which, in turn, de-
termine the causal parameter œÄY |do(a) by (7).
By varying p11|a in the valid range (8), we can generate
the ignorance region of œÄY |do(a) values that are equiva-
lently compatible with the observed data. To demon-
strate, we instantiate the model with m = 6 causes.
To simplify plotting, we specify P(Y | U, A) so that it
only depends on S(A) = Pm
k=1 A(k). This ensures that
inferences about œÄY |do(a) only depend on S(a). In Fig-
ure 4, we plot ignorance regions for œÄY |do(a) indexed
by S(a).
The Ô¨Ågure demonstrates that in this simple example,
P(Y | do(A)) is not identiÔ¨Åed in general. SpeciÔ¨Åcally,
for all vectors a where S(a) Ã∏= 3, the ignorance region

Alexander D‚ÄôAmour
0.0
0.2
0.4
0.6
0.8
1.0
Ignorance Region for Binary (U, A, Y)
S(a)
P(Y = 1 | do(A = a))
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
G
0
1
2
3
4
5
6
G
G
G
True
Observed P(Y | A)
Minimum cor(Y, U | A)
Maximum cor(Y, U | A)
Ignorance Region
‚àí4
‚àí2
0
2
4
0.0
0.2
0.4
0.6
0.8
1.0
Influence of Weak Regularizer
Log Odds Ratio Œ≥ Targeted by Penalty
Estimated  P(Y = 1 | do( A = (1,1,1,1,1,0) ))
Standard Multi‚àíCause
With Proxy
Truth
Figure 4: (Left) Ignorance regions corresponding to the causal parameters œÄY |do(a), indexed by S(a) = Pm
k=1 a(k).
For convenience, this example was instantiated so that these ignorance regions only depend on S(a).
The
ignorance regions are generated by allowing the unidentiÔ¨Åed parameter p11|a to vary in its valid range. (Right)
Results of estimation experiment, attempting to recover P(Y | do(A = (1, 1, 1, 1, 1, 0)) using maximum likelihood
estimation. In the standard multi-cause setting, a weak regularizer on Œ≥ := log
 p11|a
p10|a
. p01|a
p00|a

can push the
estimate of œÄY |do(a) to arbitrary locations in the ignorance region. When two proxy variables are added, the
weak regularizer has little eÔ¨Äect. Vertical bars show ¬±1 sd from 20 simulations.
is non-trivial. For each S(a), the extreme points of the
ignorance region correspond to the extreme values of
p11|a compatible with the margins given by P(Y | A)
and P(U | A). The true causal parameters P(Y = 1 |
do(A = a)) and the observational parameters P(Y =
1 | A = a) are always contained in this region. In some
cases, the ignorance region is large relative to the [0, 1]
interval.
Figure 4 also shows a trivial case, where P(Y | do(A =
a)) = P(Y | A = a), and thus the intervention dis-
tribution is identiÔ¨Åed despite p11|a being underdeter-
mined. This case arises because we deÔ¨Åned the struc-
tural parameters such that pA(1) = 1 ‚àípA(0), which
ensures that when S(a) = m/2 = 3, P(U | A = a) =
P(U), and thus P(Y | do(A = a)) = P(Y | A = a).
6.4
Copula Non-IdentiÔ¨Åcation in General
The non-identiÔ¨Åcation in the above example occurs
generally when P(Y | U, A) is nonparametric.
We
state this in the following supporting proposition for
Theorem 1. For the general case, we represent the un-
derdetermined degree of freedom by the copula den-
sity:
c(Y, U | A) :=
P(Y, U | A)
P(Y | A)P(U | A),
which speciÔ¨Åes the dependence between Y and U given
A, independently of the margins P(Y | A) and P(U |
A) (see, e.g., Nelsen, 2007). In the following proposi-
tion, we show in a very general setting that this copula
is not identiÔ¨Åed, and that this non-identiÔ¨Åcation pre-
cludes identiÔ¨Åcation of P(Y | do(A)).
Proposition 2. In the setting of Theorem 1, suppose
that P(U | A) is almost surely non-degenerate. Then,
the following are true
1. The copula density c(Y, U | A) is not identiÔ¨Åed.
2. Either P(Y | do(A)) = P(Y | A), or P(Y | do(A))
is not identiÔ¨Åed.
Proof. For the Ô¨Årst statement, the joint distribution
P(U, Y, A) can be written
P(U, A, Y ) = P(A)P(Y | A)P(U | A)c(Y, U | A),
By assumption, P(Y, A) and P(U, A) are identiÔ¨Åed,
but the copula density c(Y, U | A) remains unspeciÔ¨Åed
because there are no restrictions on P(Y | U, A).
For the second statement, note that the independence
copula c(Y, U | A) = 1 is compatible with the ob-
served data, as a result of the Ô¨Årst statement. Under
the independence copula, P(Y | do(A)) = P(Y | A).
If this causal hypothesis is not true, then the true
P(Y | do(A)) is also compatible with the observed
data, so multiple causal hypotheses are compatible

Multi-Cause Causal Inference: Counterexamples, Impossibility, Alternatives
with the observed data, and P(Y | do(A = a)) is not
identiÔ¨Åed.
6.5
Degenerate Case
We now continue our example and consider the case,
invoked in Claim 3, where P(U | A) is degenerate al-
most everywhere, i.e., where for any observable unit,
U can be written as a deterministic function of A. In
this case, the copula P(U, Y | A) is trivial, so the non-
identiÔ¨Åcation in Proposition 2 is not an issue. How-
ever, P(Y | do(A)) remains unidentiÔ¨Åed in this setting
because the degeneracy of P(U | A) implies that the
positivity assumption is violated.
This case is invoked asymptotically, so we analyze our
example as the number of causes grows large. First,
we show that U can indeed be consistently estimated
from A. As m grows large, the unobserved confounder
U can be estimated perfectly for any unit, up to label-
switching. SpeciÔ¨Åcally, by the strong law of large num-
bers, for any unit with U = u, as m grows large,
ÀÜp(A) := S(A)/m
a.s.
‚ÜípA(u).
From this fact, and our assumption that pA(u) is a
non-trivial function of u, we can construct consistent
estimators ÀÜU(A) for U. For example, letting I{¬∑} be
an indicator function,
ÀÜU(A) := I

ÀÜp(A) > pA(1) + pA(0)
2

is consistent as m grows large.
However, as m grows large, the causes A begin to
occupy disjoint regions of the cause space A, induc-
ing a violation of the positivity assumption. This is
the same phenomenon that drives the consistency of
ÀÜU(A). We illustrate in Figure 5, where we plot sam-
ples of the causes A at various values of m, projected
into two dimensions and scaled. The Ô¨Årst dimension
is obtained by calculating ÀÜp(A), expressed as a lin-
ear operator: ÀÜp(A) = A‚ä§(m‚àí1 ¬∑ 1m√ó1). The second
dimension is obtained by projecting A onto a vector
orthogonal to 1m√ó1. In this case, we choose the vec-
tor v2 = m‚àí1/2 ¬∑ (1‚ä§
(m/2)√ó1, ‚àí1‚ä§
(m/2)√ó1)‚ä§and calcu-
late A‚ä§v2. When m is small, A has the same support
whether U = 0 or U = 1, but as m grows, the causes
A concentrate on either side of the decision bound-
ary of our estimator ÀÜU(A), ensuring consistency of the
estimator, but violating positivity.
Because of this phenomenon, the conditional proba-
bilities P(Y = 1 | U = 0, ÀÜU(A) = 1) and P(Y = 1 |
ÀÜU(A) = 0, U = 1) are inestimable from the data in the
large-m limit, and we cannot evaluate (6). At best, we
can bound (6) by substituting extreme values 0 and 1
for the inestimable probabilities.
This problem is not merely asymptotic; it also mani-
fests for Ô¨Ånite m, when the residual uncertainty about
U after observing A is small.
Consider the 2 √ó 2
table in Figure 3 corresponding to a cause vector a
for which œÄU|a = œµ for some small epsilon satisfying
œµ < min{œÄY |a, 1 ‚àíœÄY |a}.
The bounds in (8) imply
that p11|a ‚àà[0, œµ], such that
P(Y = 1 | U = 1, A = a) ‚àà[0, 1],
i.e., the data provide no information about this condi-
tional probability. In the limit, as œµ ‚Üí0, the ignorance
region has the form
œÄY |do(a) ‚àà(1 ‚àíœÄU)P(Y = 1 | A = a) + [0, œÄU].
Similarly, for cases where œÄU|a approaches 1, the data
provide no information about P(Y | U = 0, A = a),
and the ignorance region has width equal to 1 ‚àíœÄU.
This regime appears in Figure 4 for small and large
values of S(a), where the ignorance region widths ap-
proach œÄU = 0.3 and 1 ‚àíœÄU = 0.7, respectively. As
m grows large, the probability that a sampled cause
vector A falls in this regime approaches 1.
6.6
Consistency and Positivity in General
The above positivity violation occurs in any case where
the latent confounder U can be reconstructed consis-
tently from the causes A. We summarize this fact in
the following supporting proposition for Theorem 1.
The central idea of this result is as follows: when ÀÜU(A)
is consistent in the large m limit, the event U = u
implies that A takes a value a such that ÀÜU(a) = u.
Thus, for distinct values of the latent variable U, the
observed causes A must lie in disjoint regions of the
cause space, violating positivity.
Proposition 3. Suppose that Assumption 1 holds,
that P(U) is not degenerate, and that there exists a
consistent estimator ÀÜU(Am) of U as m grows large.
Then positivity is violated as m grows large.
Proof. Because P(U) is non-degenerate, U takes on
multiple values with positive probability. In this case,
we establish that Am concentrates in diÔ¨Äerent sets of
Am depending on the value of U. For each m and each
latent variable value u in the support of U, deÔ¨Åne the
set
Em(u) = {am : ÀÜU(am) Ã∏= u}.
Em(u) is the set of cause vector values am ‚ààAm that
ÀÜU(¬∑) would map to a value other than u.
Because
ÀÜU(Am) is consistent, for each u in the support of U,
as m grows large,
P(Am ‚ààEm(u) | U = u) = P( ÀÜU(Am) Ã∏= u | U = u) ‚Üí0.

Alexander D‚ÄôAmour
Figure 5: Linear projections of sampled treatments A, with color and shape indicating the value of the latent
confounder U. Here, v1 = m‚àí1 ¬∑ 1m√ó1, such that A‚ä§v1 = S(A)/m = ÀÜp(A), and v2 is a vector orthogonal to v1,
scaled by m‚àí1/2. These values are generated according to the model in Section 6. As m, the dimension of A,
increases, the treatment vectors A generated under each value of U become more distinct. We take advantage of
this distinctness to estimate U consistently; as m grows large, the black boundary encodes an estimator ÀÜU(A)
that calls U = 0 if a point lands to the left of the line, and U = 1 if it lands to the right. However, this separation
between red squares and blue circles also indicates a positivity violation.
Likewise, for any u‚Ä≤ Ã∏= u in the support of U,
P(A ‚ààEm(u) | U = u‚Ä≤) = P( ÀÜU(Am) Ã∏= u | U = u‚Ä≤) ‚Üí1.
Thus, positivity is violated.
6.7
Proof of Theorem 1
We conclude with a proof of Theorem 1.
Proof. One of two cases must hold:
P(U | A) is
either degenerate almost everywhere, or not.
In
the non-degenerate case, Proposition 2 proves non-
identiÔ¨Åcation, except in the trivial case.
In the degnerate case, there are again two cases: either
P(U) is degenerate, or not. If P(U) is not degener-
ate, Proposition 3 shows that the positivity assump-
tion fails, and because P(Y | U, A) is nonparametric
by assumption, P(Y | U, A) is inestimable for some
(u, a) ‚ààU √ó A, and (1) is not identiÔ¨Åed. If P(U) is
degenerate, then the latent variable does not induce
any confounding, and P(Y | do(A)) = P(Y | A).
7
DISCUSSION
7.1
Practical Implications
In this section, we discuss practical implications of our
negative results, then end on a more hopeful note, and
suggest some constructive ways forward.
We put the bad news Ô¨Årst. Theorem 1 suggests that
one should generally be cautious about drawing causal
inferences in the multi-cause setting. SpeciÔ¨Åcally, the
existence of a non-trivial ignorance region can make
conclusions highly dependent on modeling choices that
in other contexts would be innocuous. On one hand,
Ô¨Çexible, nonparametric models are likely to behave un-
predictably when they are used to estimate causal ef-
fects in this setting.
In particular, within the igno-
rance region, point estimates will be driven primarily
by choices of regularization. To demonstrate this, we
plot the results of an estimation experiment on the
right of Figure 4.
In this experiment, we estimate
œÄy|do(a) for a given vector a from 15,000 datapoints
drawn from the binary example‚Äôs data generating pro-
cess. We perform estimation by maximum likelihood,
but we add a weak L2 penalty term on the log-odds ra-
tio Œ≥ := log
 p11|a
p10|a
. p01|a
p00|a

, which pushes estimates of
p11|a to take speciÔ¨Åc values. As expected, the penalty
determines where in the ignorance region the estimate
appears. This behavior can be unpredictable in com-
plex problems, especially when applying estimation
methods that involve tuning parameters or stochastic
training.
On the other hand, one may be able to obtain nu-
merically stable estimates of causal eÔ¨Äects with para-
metric assumptions in this setting, e.g., assuming that
E[Y
| U, A] is linear, but our results still suggest
caution, unless one has strong prior knowledge that
the parametric speciÔ¨Åcation is correct. Parametric as-
sumptions can induce identiÔ¨Åcation in this setting, but
this can be diÔ¨Écult to conÔ¨Årm without formal analysis
of the model, as our Ô¨Årst counterexample suggests. In

Multi-Cause Causal Inference: Counterexamples, Impossibility, Alternatives
addition, because of the lack of nonparametric identiÔ¨Å-
cation, if one does obtain identiÔ¨Åcation via parametric
assumptions, the parts of these assumptions that de-
termine result of the analysis cannot be checked: the
data will be indiÔ¨Äerent to diÔ¨Äerent parametric speciÔ¨Å-
cations that yield diÔ¨Äerent estimates in the ignorance
region. Finally, uncertainty estimates obtained under
parametric speciÔ¨Åcations may not faithfully represent
this sensitivity to functional form.
7.2
Proxy Variables
Despite these negative results, there are straightfor-
ward modiÔ¨Åcations to the multi-cause setting that can
yield nonparametric identiÔ¨Åcation. One alternative is
to incorporate proxy variables (a.k.a.
negative con-
trols) into the estimation procedure. A proxy variable
Z is a supplementary variable that is downstream of
the latent confounder U, but is causally unrelated to
either the outcome Y or the cause variables A. Several
authors have given suÔ¨Écient conditions for causal iden-
tiÔ¨Åcation when using proxy variables in the presence of
unobserved confounding (see, e.g., Kuroki and Pearl,
2014; Miao et al., 2016; Shi et al., 2018). Recently,
Louizos et al. (2017) applied these results to estimate
causal eÔ¨Äects by calculating (1) from the posterior dis-
tribution of a variational autoencoder model.
Adding proxy variables to an analysis requires collect-
ing a slightly larger dataset, but it is plausible that in
many multi-cause settings collecting the appropriate
variables would not be onerous. For example, the suf-
Ô¨Åcient conditions for nonparametric identiÔ¨Åcation in
Miao et al. (2016) require the addition of two proxy
variables, one conditionally independent of A given U,
and the other conditionally independent of Y given
U.
In the multi-cause setting, it is not diÔ¨Écult to
Ô¨Ånd a proxy of the Ô¨Årst type; each cause variable A(k)
satisÔ¨Åes this condition. Similarly, it is plausible that
proxies of the second type would be readily available
in many contexts that could be framed as multi-cause
causal inference problems, e.g., GWAS (Tran and Blei,
2017) or recommender systems (Wang et al., 2018).
For example, if the number of causes m is large because
A includes causes that span a large set of functions,
then it may be plausible to designate some loosely re-
lated causes as proxies of this second type. Likewise,
in cases where the latent confounder is shared by two
outcomes Y (1) and Y (2) that are conditionally inde-
pendent given U, each outcome can serve as a proxy
in estimating the intervention distribution of the other.
To demonstrate that the proxy approach addresses
some of the pathologies highlighted here, we repeat
the estimation experiment on the right side of Fig-
ure 4, adding two proxy variables to the data.
As
in Section 7.1, we estimate the model parameters by
maximum likelihood estimation for varying settings of
a weak regularizer. In this case, the regularizer has
negligible eÔ¨Äect on the estimates of œÄY |do(a), although
the estimates show signiÔ¨Åcant variability.
7.3
Sensitivity Analysis
To close, we note that the multi-cause setting could
present a fruitful use case for sensitivity analysis as an
alternative to point estimation of causal estimands.
In such a case, rather than focus on point-identifying
a causal estimand under strong assumptions, it can
be useful to relax our requirements, and attempt to
identify an ignorance region under weaker assumptions
(see, e.g., Manski, 2009). The multi-cause setting is
particularly amenable to sensitivity analysis, because
the assumptions about factorizations of P(A) repre-
sent a reasonable middle ground between the strong
assumption of no unobserved confounding at one ex-
treme and the weak assumption of unstructured con-
founding on the other. For example, although we pre-
sented it as a negative result before, Assumption 1 in-
duced ignorance regions obtained in Figure 4 that are
relatively narrow for some values of a; for some appli-
cations, analyses that explicitly map these sensitivity
regions could acceptably informative.
As a bonus, sensitivity analyses can often be applied
post hoc, so that an investigator is Ô¨Årst free to Ô¨Ånd
a best-Ô¨Åtting model for the observed data, and then
reason about the causal implications of the model sep-
arately (Franks et al., 2018).
We leave adaptation
of such methods to the multi-cause setting for future
work.
Acknowledgements
Thanks to Yixin Wang, Rajesh Ranganath, Dustin
Tran, and David Blei for open conversations about
their work.
Thanks to Alexander Franks, Andrew
Miller, Avi Feller, Jasper Snoek, and D. Sculley for
their feedback on this project.
References
Allman, E. S., C. Matias, J. A. Rhodes, et al. (2009).
IdentiÔ¨Åability of parameters in latent structure mod-
els with many observed variables.
The Annals of
Statistics 37(6A), 3099‚Äì3132.
Bollen, K. A. (1989). Structural Equation Models with
Latent Variables. New York: John Wiley and Sons.
Franks, A., A. D‚ÄôAmour, and A. Feller (2018). Flex-
ible sensitivity analysis for observational studies
without observable implications.
arXiv preprint
arXiv:1809.00399.

Alexander D‚ÄôAmour
Joe, H. (1997). Multivariate models and multivariate
dependence concepts. Chapman and Hall/CRC.
Kruskal, J. (1989). Rank, decomposition, and unique-
ness for three-way and n-way arrays. Multiway data
analysis. Elsevier Science Publisher BV (North-
Holland).
Kuroki,
M. and J. Pearl (2014).
Measurement
bias and eÔ¨Äect restoration in causal inference.
Biometrika 101(2), 423‚Äì437.
Louizos, C., U. Shalit, J. M. Mooij, D. Sontag,
R. Zemel, and M. Welling (2017).
Causal eÔ¨Äect
inference with deep latent-variable models. In Ad-
vances in Neural Information Processing Systems,
pp. 6446‚Äì6456.
Manski, C. F. (2009). IdentiÔ¨Åcation for prediction and
decision. Harvard University Press.
Miao, W., Z. Geng, and E. Tchetgen (2016). Identify-
ing causal eÔ¨Äects with proxy variables of an unmea-
sured confounder.
Nelsen, R. B. (2007).
An introduction to copulas.
Springer Science & Business Media.
Pearl, J. (2009).
Causality.
Cambridge university
press.
Ranganath, R. and A. Perotte (2018). Multiple causal
inference with latent confounding.
arXiv preprint
arXiv:1805.08273.
Rosenbaum, P. R. and D. B. Rubin (1983). Assessing
sensitivity to an unobserved binary covariate in an
observational study with binary outcome. Journal
of the Royal Statistical Society. Series B (Method-
ological), 212‚Äì218.
Shi, X., W. Miao, and E. T. Tchetgen (2018). Multiply
robust causal inference with double negative con-
trol adjustment for unmeasured confounding. arXiv
preprint arXiv:1808.04906.
Tran, D. and D. M. Blei (2017). Implicit causal models
for genome-wide association studies. arXiv preprint
arXiv:1710.10742.
Wang, Y. and D. M. Blei (2018).
The blessings of
multiple causes. arXiv preprint arXiv:1805.06826.
Wang, Y., D. Liang, L. Charlin, and D. M. Blei (2018).
The deconfounded recommender:
A causal infer-
ence approach to recommendation. arXiv preprint
arXiv:1808.06581.

