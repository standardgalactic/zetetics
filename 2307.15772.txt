arXiv:2307.15772v1  [stat.ML]  28 Jul 2023
Weighted variation spaces and approximation by shallow ReLU
networks∗
Ronald DeVore, Robert D. Nowak, Rahul Parhi, and Jonathan W. Siegel
Abstract
We investigate the approximation of functions f on a bounded domain Ω⊂Rd by the
outputs of single-hidden-layer ReLU neural networks of width n. This form of nonlinear n-
term dictionary approximation has been intensely studied since it is the simplest case of neural
network approximation (NNA). There are several celebrated approximation results for this form
of NNA that introduce novel model classes of functions on Ωwhose approximation rates avoid
the curse of dimensionality. These novel classes include Barron classes [2], and classes based on
sparsity or variation [20, 21] such as the Radon-domain BV classes.
The present paper is concerned with the deﬁnition of these novel model classes on domains Ω.
The current deﬁnition of these model classes does not depend on the domain Ω. A new and more
proper deﬁnition of model classes on domains is given by introducing the concept of weighted
variation spaces. These new model classes are intrinsic to the domain itself. The importance of
these new model classes is that they are strictly larger than the classical (domain-independent)
classes. Yet, it is shown that they maintain the same NNA rates.
Keywords—Neural networks, approximation rates, variation spaces, curse of dimensionality.
1
Introduction
Neural networks (NNs) are now the numerical method of choice for the development of learning
algorithms in regression and classiﬁcation, especially when dealing with functions of d variables
with d large. It is therefore important to understand, through mathematical theory, the reasons for
this success. In learning, we are tasked with approximating an unknown function f on a domain
Ω⊂Rd from some ﬁnite set of data observations of f. Thus, at least part of the success in using
NNs for such learning problems, must lie in their ability to eﬀectively approximate the functions
of interest. This has led to the study of approximation by NNs and, in particular, to understand
exactly which functions are well approximated by NNs and what is their approximation rate in
terms of the number of neurons employed (so-called n-term approximation).
A large number of papers have been written in recent years that give quantitative bounds on
the approximation rates of various model classes of functions when using neural networks. General
accountings of such results can be found in [3, 7, 10, 25].
Two types of results have emerged.
The ﬁrst is to show that deep NNs with ReLU activation functions (see, e.g., [15, 28, 29, 33]) are
surprisingly eﬀective in approximating functions from classical model classes such as ﬁnite balls
∗This research was supported by the NSF grants DMS-2134077 and DMS-2134140 of the MoDL program as well
as the MURI ONR grant N00014-20-1-2787 (RD and RN). RP was supported by the European Research Council
(ERC Project FunLearn) under grant 101020573. JS was supported by NSF grants DMS-2111387 and CCF-2205004.
1

in a Sobolev or Besov space when the approximation error is measured in an Lp(Ω) norm with
1 ≤p ≤∞. While such results are deep and interesting, they do not match the most common
setting of learning in high dimensions (d large) because these model classes necessarily suﬀer the
curse of dimensionality. Indeed, the approximation rates for such smoothness classes is of the form
O(n−s/d) with s related to the smoothness assumption on f. Here and later n always refers to
the number of neurons used in the approximation. Thus, for large values of d, membership in
such a model class is not a realistic assumption to make on the target function f to be learned.
This negativity for classical smoothness as a model class asssumption for f can be ameliorated by
assuming that the input variable to f (and hence the data as well) is restricted by a probability
measure µ on Ωsupported on a low-dimensional submanifold.
The second type of approximation result introduces novel high-dimensional model classes for
which neural network approximation (NNA) does not suﬀer this curse of dimensionality. Thus,
membership in these new model classes can be a realistic model class assumption for learning a
function of many variables. The most celebrated examples of such new model classes are the Barron
class Bs, s > 0, introduced in [2]. The set Bs = Bs(Rd) consists of all functions f deﬁned on Rd
whose Fourier transform ˆf satisﬁes
∥f∥Bs :=
Z
Rd
(1 + |ω|)s| ˆf(ω)| dω < +∞.
(1.1)
The original result of Barron showed that on a bounded domain Ω⊂Rd, any function f on Ω
which is the restriction of a function from B1(Rd) can be approximated in the L2(Ω) norm by single-
hidden-layer sigmoidal networks with n neurons to an accuracy of CΩ∥f∥B1n−1/2, n ≥1, where
the constant CΩonly depends upon the measure of Ω. Notice that this approximation rate does
not deteriorate with increasing d in contrast with classical smoothness model classes. However, one
must note that the above deﬁnition of Barron spaces depend on d and indeed get more demanding
as d increases.
Barron’s result spurred a lot of study and generalizations over the last decades. In particular,
new model classes of functions which have sparse representation of as linear combinations of neural
atoms were introduced. In the case of ReLU neurons, the sparsity class is larger than the (second-
order) Barron class and yet preserves the rate of approximation of n-term approximation [16].
These spaces based on sparsity are called variation spaces [1, 14, 18, 30, 31]. We summarize these
activities for ReLU neurons in the following two sections. For the moment, we only wish to focus
on the existing theory for these model classes and their approximation rates on domains Ω⊂Rd.
This is the typical setting in applications. The existing theory deﬁnes the corresponding model
classes on Rd and then extends the deﬁnition to domains as the restriction of functions deﬁned on
Rd. As such, the theory and corresponding results are in a strong sense independent of the domain
Ω. While this leads to a simple approximation theory on domains, these results never take into
consideration the nature of Ω, e.g., its geometry.
The purpose of the present paper is to show there is a more satisfactory deﬁnition of these novel
model classes on domains Ωthat leads to domain-dependent results that are stronger than that
provided by the existing theory. We call these new model classes weighted variation spaces since
they generalize the classical variation space for ReLU neurons by introducing a domain-dependent
weighting of the ReLU atoms. These new model classes are strictly larger than the existing variation
spaces while still maintaining the same rate of approximation of n-term approximation. We develop
this domain-dependent theory primarily in the case when Ω= Bd is the Euclidean unit ball in
2

Rd. To indicate how the theory would depend on the domain Ω, we also consider the domain
Qd := [−1, 1]d and contrast the diﬀerence in this case with that of Bd.
While we develop our results only for the case of ReLU neurons, it will be clear how they
generalize to the case of ReLUk neurons, k > 1. So, for the remainder of this paper the activation
function is
σ(t) = t+ = max{0, t}.
(1.2)
This paper is organized as follows. In the next two sections, we review some of the existing
results on ReLU neural network approximation. This will serve to frame the new results proved in
this paper. In §4 we introduce our new (domain-dependent) model classes. In §5, we prove our new
approximation results for Ω= B2 the unit Euclidean ball in R2. We separate out this case since
it is the simplest setting to understand. The remaining sections of this paper formulate and prove
our results for Ω= Bd which is the Euclidean unit ball in Rd. We also contrast how the results
change when Ω= Qd. Finally, we discuss the possible signiﬁcance of these new model classes for
the problem of learning from data.
2
Approximation by shallow ReLU networks
In this paper, we concentrate on a very speciﬁc case of NNA, namely approximation by single-
hidden-layer ReLU NNs, i.e., the activation function σ is given by (1.2). We study neural network
approximation on a given bounded domain (the closure of an open connected set) Ωof Rd. The most
natural choices for Ωare the unit Euclidean ball Bd of Rd or the d-dimensional cube Qd := [−1, 1]d.
The case Ω= Bd will be the primary example considered in this paper. In going further, we let
∥· ∥denote the Euclidean norm on Rd.
We deﬁne the ReLU atoms
φ(x; ξ, t) := σ(ξ · x −t) = (ξ · x −t)+,
ξ ∈Rd, ∥ξ∥= 1, t ∈R.
(2.1)
Given the atom φ, we let
Hφ := {x ∈Ω: ξ · x = t}
(2.2)
be its hyperplane cut. Hφ divides Ωinto two regions H±
φ . The function φ is identically zero on
the region H−
φ := {x ∈Ω:
ξ · x ≤t} and the linear function = ξ · x −t on the second region
H+
φ := {x ∈Ω: ξ · x > t}. Notice that for some values of t, the atom φ is identically zero on Ωso
that H−
φ = Ω.
For each Ω, there is a smallest interval T = T(Ω) such that for t /∈T, the dictionary element
φ(·; ξ, t) is either identically zero on Ωor a linear function on Ω. Let D = D(Ω) := {φ(·; ξ, t)} be
the dictionary of all atoms φ for which t ∈T = T(Ω). We are interested in n-term approximation
from the dictionary D. For n = 1, 2, . . . , let Σn := Σn(D) be the set of functions of the form
S(x) =
n
X
j=1
ajφj(x),
x ∈Ω,
(2.3)
where the φj are chosen arbitrarily from D and a1, . . . , an are real numbers. When n = 0, we deﬁne
Σ0 := {0}. The functions S ∈Σn are precisely the functions on Ωproduced by a single-hidden-layer
ReLU network with n neurons, i.e., width n. The set Σn is thus a (d + 2)n dimensional parametric
3

nonlinear manifold parameterized by the ξj ∈Bd, j = 1, . . . , n, the tj ∈R, j = 1, . . . , n, and the
coeﬃcients a1, . . . , an ∈R. Note that a given S ∈Σn has in general many representations of the
form (2.3). In other words, the dictionary D(Ω) is redundant.
The above paragraph tells us that there are two ways to view shallow network approxima-
tion with ReLU activation. One view is that it is a special case of n-term approximation from a
dictionary of functions. Another view is that it is a special case of manifold approximation. There-
fore, a proper assessment of this form of NN approximation would be to compare it with other
approximation methods of either one of these forms.
Approximation by Σn is one of the simplest examples of neural network approximation (NNA).
It is therefore a fundamental problem to completely understand the approximation properties of
Σn, n ≥1, i.e., what are the properties of a function f that determine how well f is approximated
by the elements of Σn. In the case d = 1 and Ωis an interval, the set Σn is the space of piecewise
linear function with n breakpoints. In this special case, approximation by Σn is well understood
(see e.g. [7, 8]). So, we restrict ourselves to the case d ≥2 in going further in this paper.
For a function f in Lp(Ω), 1 ≤p ≤∞, we deﬁne
En(f)p := En(f)Lp(Ω) := inf
S∈Σn ∥f −S∥Lp(Ω).
(2.4)
This is a form of nonlinear approximation since the set Σn is not a linear space but rather a
nonlinear manifold. Rightfully, we often put this form of approximation in competition with other
examples of manifold approximation (see e.g. [6, 7]).
From the viewpoint of approximation theory, an understanding of the approximation properties
of Σn would seek to precisely characterize the approximation classes for Σn approximation. An ap-
proximation class is the collection of all functions whose approximation error decays at a prescribed
decay rate. For example, for a given α > 0, we seek a characterization of the set
Aα := Aα((Σn)n≥0, Lp(Ω))
(2.5)
of functions f ∈Lp(Ω) for which
En(f)p ≤M(n + 1)−α,
n = 0, 1, 2, . . . .
(2.6)
Note that by deﬁnition, Σ0 = {0} and hence E0(f)p = ∥f∥Lp(Ω) The smallest value of M for which
(2.6) holds is deﬁned as ∥f∥Aα. Notice that Aα is a quasi-normed linear space. While for most
classical methods of linear and nonlinear approximation, e.g. polynomials, splines, n-term wavelets,
there is a characterization of the spaces Aα (at least for a certain range of α), the case for neural
network approximation is much diﬀerent. There is at present no known characterization of Aα for
any value of α > 0. There are however many suﬃcient conditions that guarantee membership in
Aα (see [7]).
Another (less ambitious) viewpoint of approximation by Σn is to propose model classes K, i.e.,
compact subsets K ⊂Lp(Ω), and study how well the elements of K can be approximated by the
elements of Σn. This leads to the study of
En(K)p := sup
f∈K
En(f)p,
n ≥0.
(2.7)
If one comes up with a set K for which En(K) ≤Cn−α, n ≥1, then clearly K ⊂Aα and we
gain some information about Aα. Many interesting approximation results have been proven for
4

various classical model classes K such as Sobolev and Besov balls, however, the best approximation
rates are not known in all cases [7]. These results show no gain in approximation eﬃciency when
compared with more classical methods of approximation such as those that use splines or wavelets.
Moreover, these classical model classes all suﬀer the curse of dimensionality: smoothness of order
s gives rate decay En(K)p ≥Cn−s/d, n ≥0.
One of the celebrated accomplishments in the study of NNA was the introduction of new model
classes K which are known to not suﬀer the curse of dimensionality and also give us information
on Aα. We discuss these model classes in the next two sections. In going further in this paper we
only treat the case of approximation in L2(Ω). However, the case of Lp(Ω) approximation has also
been well studied (see [30]).
3
Novel (non-classical) model classes
While the classical model classes based on smoothness all suﬀer the curse of dimensionality, certain
novel model classes K have been introduced that avoid this curse. The discovery of these novel
model classes begin with the celebrated work of Barron [2]. We have already deﬁned the Barron
spaces Bs(Rd) in the introduction.
Barron’s original results on NNA were for sigmoidal activation and the Barron class B1(Rd)
where he showed that functions in this class, when restricted to a domain Ω⊂Rd, had an L2(Ω)
approximation rate n−1/2, n ≥1. It was rather straightforward to extend his approach to proving
that functions in B2 had the same approximation rate when using ReLU activation. Several follow
up papers signiﬁcantly improved on these original results as we now describe.
Notice that the Barron classes are formulated for functions which are deﬁned on all of Rd. Given
a bounded domain Ω, it is not obvious how these classes should be deﬁned on Ω. The deﬁnition
employed in the literature is that the space Bs(Ω) is the set of function f deﬁned on Ωwhich are
the restriction of a function F ∈Bs(Rd) with norm given by
∥f∥Bs(Ω) :=
inf
F |Ω=f ∥F∥Bs(Rd),
s > 0.
(3.1)
With this deﬁnition, we have
En(U(B2(Ω))L2(Ω) ≤Cn−1/2,
n ≥1,
(3.2)
where C depends only on the diameter and measure of Ω. Here and later we use the notation
U(Y ) to denote the unit ball of a normed space Y . This approximation rate was improved over
the years starting with Makovoz [17] and continuing on with the results of [1, 12, 30]. The current
best known approximation rate for n-term ReLU NNA is
En(U(B2(Ω))L2(Ω) ≤Cn−1
2−3
2d ,
n ≥1,
(3.3)
where again C depends only on d. We refer the reader to [30] for a more detailed discussion of
these approximation results. It is still not known if this rate can be improved for the Barron class
B2.
We turn next to a second family of novel model classes for NNA referred to as variation spaces.
Let D = D(Ω) be the dictionary of ReLU atoms whose hyperplane cut intersects Ω. Consider any
function S = Pn
j=1 ajφj, i.e., S ∈Σn. Recall that this representation is not unique. We deﬁne
5

V (S) := inf



n
X
j=1
|aj| : S =
n
X
j=1
ajφj


,
(3.4)
which is called the variation of S with respect to the dictionary D.
With this notation in hand, we can deﬁne a new space V := V(Ω) = V(Ω, D) as the set of all f
in L2(Ω) for which there is a sequence Sn ∈Σn, n ≥1, such that ∥f −Sn∥L2(Ω) →0, n →∞, and
V (Sn) ≤M, n ≥1. Throughout the paper, we will use V when the domain Ωand dictionary D
are clear from the context, and use V(Ω), V(D), or V(Ω, D) when we want to call attention to the
domain and/or dictionary. The smallest M for which this is true is deﬁned as ∥f∥V(Ω). This space
is called the variation space of the dictionary D. The space V(Ω) is a Banach space with respect to
this norm (see [31] for properties of variation spaces). A fundamental relation between the Barron
and variation space is the embedding
∥f∥V(Ω) ≤CΩ∥f∥B2(Ω),
f ∈B2(Ω),
(3.5)
with CΩthe embedding constant (which depends only on the diameter of Ω). The space V(Ω) is
strictly larger than B2(Ω).
The variation space V(Ω) has been carefully studied and in particular it has been proven that
that (see [30])
En(U(V(Ω)))L2(Ω) ≤Cn−1
2−3
2d ,
n ≥1,
(3.6)
where C depends only on Ωand d. This approximation rate also matches the decay rate of the metric
entropy of U(V(Ω)) [30]. Notice that this gives the bound (3.3) and is in fact how approximation
rates for the Barron class are proved. The important thing to note here is that V is a larger space
than B2 but the current best known approximation rates (with shallow ReLU NNs) for both of
these classes is the same, namely O(n−1
2−3
2d ), n ≥1.
A major breakthrough in the understanding of V(Ω) was made by characterizing membership
of a function f in V(Ω) through the smoothness of its Radon transform. Namely, it was originally
proved in [20] that a function f is in V(Ω) if and only if f has an extension F to all of Rd such that
the Radon transform R(F; ξ, t) is in a certain smoothness space. Properties and generalizations
of this notion of smoothness were extensively studied in [21, 22, 24], giving rise to a new family
of Banach spaces, now referred to as the Radon BV spaces. These spaces are denoted by RBVk,
k ∈N.
The key result of [21] is the following representer theorem for these spaces.
Let xi ∈Rd,
i = 1, . . . , m, and yi ∈R, i = 1, . . . , m. Then, there always exists a solution to the data-ﬁtting
problem
min
f∈RBVk
m
X
i=1
L(yi, f(xi)) + λ|f|RBVk
(3.7)
that takes the form of a function S which is the output of a single-hidden-layer neural network with
≤m neurons and ReLUk−1 activation functions. Here, L is any loss function which is lower-semi-
continuous in its second argument and |f|RBVk is the semi-norm which deﬁnes the RBVk spaces,
which measures smoothness in the Radon domain. The Radon BV spaces are deﬁned on domains
6

Ω⊂Rd via restrictions. For the case k = 2 (which corresponds to shallow ReLU NNs) it has been
shown in [24, Theorem 6] (see also [31, Theorem 2 and Corollary 1]) that
RBV2(Ω) = V(Ω),
(3.8)
with equivalent norms. It has also been shown that there exists a solution S to (3.7) which is in
Σm(D) on any bounded domain Ω⊂Rd [24, Theorem 5].
4
Weighted variation model classes
One of the main points of the present paper is that one can derive improved results on approximation
by shallow ReLU networks if one considers new model classes that generalize the standard variation
space by including weights on the atoms. In this section, we introduce these new model classes for
the case when we want the error of approximation to be taken in the L2(Ω) norm with Ωa bounded
domain in Rd. We begin with the general principle of weighted variation spaces.
Let D be the dictionary of ReLU atoms. Let Sd−1 be the boundary of the unit Euclidean ball
Bd of Rd. That is, Sd−1 := {ξ ∈Rd : ∥ξ∥= 1}. Any atom φ in D is of the form φ(x) = (ξ · x −t)+
where t ∈R. We are interested in the atoms φ whose hyperplane cut intersects Ω(since otherwise
the atom is identically an aﬃne function). Accordingly, we deﬁne
Z(Ω) := {(ξ, t) : ξ ∈Sd−1, t ∈R such that Hφ(·;ξ,t) ∩Ω̸= ∅}
(4.1)
and ¯Z(Ω) its closure in the Euclidean norm. Note that whenever φ(x) = φ(x; ξ, t) is positive for
some x ∈Ω, it is positive in a neighborhood of x and hence ∥φ∥L2(Ω) > 0. Given the domain Ωwe
deﬁne the dictionary
D(Ω) := {φ(·, ξ, t) : (ξ, t) ∈¯Z(Ω)}.
(4.2)
The set ¯Z(Ω) is a compact subset of Sd−1 ×R. If we equip ¯Z(Ω) with the Euclidean norm topology
then the mapping (ξ, t) 7→φ(·; ξ, t) is a continuous mapping from ¯Z(Ω) into L2(Ω).
Here is an important observation about the atoms in this dictionary which underlies the im-
proved approximation results of this paper. While each atom φ ∈D(Ω) is in L2(Ω) whenever Ωis a
bounded domain, the L2(Ω) norm of φ will depend heavily on φ and Ω. Namely, if the support of φ
lies near the boundary of Ωthen this norm will be small and we expect that φ has a less important
role in approximating a given target function f ∈L2(Ω).
As an example, consider the case when Ω= Bd is the d-dimensional Euclidean ball. It is easy
to see that the atom φ(x) = (ξ · x −t)+, has L2(Ω)-norm satisfying
∥φ∥L2(Ω) ≈(1 −t)
3
2+ d−1
4 ,
−1 ≤t ≤1,
(4.3)
with constants of equivalence depending only on d. Indeed, the L∞(Ω) norm of φ is 1 −t and the
measure of its support ≈(1 −t)[√1 −t]d−1. It follows that the norms of atoms get smaller as t
approaches one.
The compactness of ¯Z(Ω) implies that the dictionary D(Ω) is a compact subset of L2(Ω).
Thus, there is another useful characterization of the functions in V(Ω). Consider the space M :=
M( ¯Z(Ω)) of all ﬁnite (signed) Radon measures on ¯Z(Ω), equipped with the variation norm ∥µ∥M :=
R
¯Z(Ω)
d|µ|. For µ ∈M, we introduce the function
fµ :=
Z
¯Z(Ω)
φ(·; ξ, t) dµ(ξ, t),
(4.4)
7

where the integral in (4.4) can be understood as a Bochner integral (see [31, Lemma 3] for more
details). Then, any f ∈V(Ω) has a representation
f = fµ,
for some µ ∈M.
(4.5)
This representation is not unique in the sense that diﬀerent measures µ can give rise to the same
f. It then follows (see [31]) that the V-norm can be alternatively speciﬁed by
∥f∥V = inf{∥µ∥M : f = fµ, µ ∈M}.
(4.6)
In order to simplify the geometry, in going further in this section, we assume that Ωis a convex
subset of Rd and D := D(Ω). We say that
w(ξ, t),
(ξ, t) ∈¯Z(Ω),
(4.7)
is a weight function if w is a non-negative continuous function on ¯Z(Ω). Given an atom φ(·; ξ, t)
we will abuse notation and also write w(φ) or w(φ(·; ξ, t)) for w(ξ, t).
Admissible Weights: Given a weight function w deﬁned on ¯Z(Ω), we deﬁne
˜φ(·; ξ, t) := φ(·; ξ, t)
w(ξ, t) ,
(ξ, t) ∈¯Z(Ω),
(4.8)
where ˜φ(·; ξ, t) is deﬁned to be the zero function whenever w(ξ, t) = 0. We say that the weight
function w is admissible for Ω, if the mapping (ξ, t) →˜φ(·; ξ, t) is continuous as a mapping from
¯Z(Ω) into L2(Ω). It follows that
∥˜φ(·; ξ, t)∥L2(Ω) ≤Cw,
(4.9)
with Cw an absolute constant. Notice that if a weight function w is admissible, then any larger
weight function ˜w is also admissible.
When given an admissible weight w, the set of functions
Dw := Dw(Ω) = {˜φ(·; ξ, t) : (ξ, t) ∈¯Z(Ω)}.
(4.10)
is a new dictionary contained in L2(Ω). Furthermore, this dictionary is compact in L2(Ω). We
deﬁne the weighted variation space Vw := Vw(Ω) to be variation space of this new dictionary Dw.
Since the admissibility conditions ensure that the dictionary Dw is compact in L2(Ω), we have,
from the discussion above, that, for every f ∈Vw(Ω), there exists a signed Radon measure µ = µf
on ¯Z(Ω) such that
f = ˜fµ :=
Z
¯Z(Ω)
˜φ(·; ξ, t) dµ(ξ, t)
with
∥f∥Vw(Ω) = ∥˜fµ∥Vw(Ω) = ∥µf∥M.
(4.11)
We also clearly have
∥f∥L2(Ω) ≤Cw∥f∥Vw(Ω),
f ∈Vw(Ω),
(4.12)
where Cw is the constant in (4.9).
We also have that, if ˜w ≥w, then V ˜w(Ω) ⊂Vw(Ω) and
∥f∥V ˜
w ≤∥f∥Vw(Ω) which implies that
En(V ˜w(Ω)) ≤En(Vw(Ω)),
n ≥0.
(4.13)
8

While Vw(Ω) is deﬁned for any nonnegative weight w which is admissible, there is a particular
choice of w which we will consider in this paper. Speciﬁcally, we show that the approximation rates
derived for shallow ReLU neural networks on the unweighted space V(Ω) actually hold on the larger
space Vw(Ω) for a certain collection of admissible weights w. As we will later see, the smallest
admissible weight with this property will depend upon the domain Ω. This domain-dependent
smallest weight is related to the measure of the intersection of the hyperplane of φ restricted to
Ω. To describe this particular weight w and our new approximation results, we start with the case
d = 2 where the proofs of approximation rates are simplest to understand. We consider the two
domains Ω= B2 and Ω= Q2. Later, we treat the general cases Ω= Bd, d ≥2. We then explain
how the same theory carries over to Ω= Qd (see Remark (6.8)).
Variation spaces V(D0) are deﬁned as above for any dictionary D0 in any Hilbert space H
provided that the dictionary elements ψ ∈D0 satisfy ∥ψ∥H ≤δ for a ﬁxed value of δ > 0. Given
such a dictionary D0, we deﬁne Σn := Σn(D0) as the set of all functions S ∈H that are a linear
combination of at most n terms of D0. For any f ∈H, we deﬁne the error of n term approximation
to be
E(f, Σn)H := inf
S∈Σn ∥f −S∥H.
(4.14)
This n-term approximation error from a dictionary is well studied. A fundamental result for such
n-term approximation is the theorem of Maurey [26] (see also [2, 11]). Maurey’s theorem says that
for each n ≥0 and f ∈V(D0) we have
inf
Sn∈Σn ∥f −Sn∥H ≤∥f∥V(D0)δn−1/2,
n ≥1.
(4.15)
In fact, Maurey’s theorem can be generalized beyond the setting of a Hilbert space to the class of
type-2 Banach spaces (see [30] for the application to non-linear dictionary approximation). This
introduces an extra constant factor which depends upon the type-2 constant of the space. We shall
use this theorem going forward, but restrict ourselves to the Hilbert space setting.
5
Approximation in Ω= B2
In this section, we develop our results in the case Ω= B2 where B2 is the unit Euclidean ball in
R2. Here, ¯Z(Ω) = S1 × [−1, 1]. This will illustrate, in their simplest form, all of the principles
needed to treat the more general case Ω= Bd, d ≥2. The treatment of Bd is given in §6 but with
a signiﬁcant increase in the level of technicality.
In this section, we let D = D(Ω) be the ReLU dictionary of atoms φ = φ(·; ξ, t), ξ ∈S1 and
t ∈[−1, 1].
Note that since d = 2, the hyperplane Hφ associated to the atom φ is a line and
Lφ := Hφ ∩Ωis a line segment whose length is |Lφ| = (1 −t2)1/2. We deﬁne the weight of this
atom by
w(φ) = w(φ(·; ξ, t)) := 1 −t,
t ∈[−1, 1].
(5.1)
It is easy to check that this weight is admissible since ∥φ(·; ξ, t)∥L2(Ω) ≈(1 −t)7/4 (see (4.3)).
We ﬁrst want to prove results on the linear approximation of the atoms φ. Namely, for each
n = 1, 2, . . . , we want to construct an n dimensional linear space Xn which is good at approxating
all of the atoms φ ∈D(Ω).
The linear space Xn will be the span of n well chosen atoms φj,
j = 1, . . . , n, from D(Ω). The construction we give for Xn is a modiﬁcation of ideas from [30]. Our
analysis of the approximation error in approximating φ by the elements of Xn is new in that it
gives an improved error estimate when the support of φ is near the boundary of Ω.
9

To deﬁne the space Xn := span{φ1, . . . , φn}, we want to choose the atoms φj, j = 1, . . . , n,
to have as a special discrete distribution from D. In the case d = 2, these atoms are rather easy
to describe geometrically as is given in the next paragraph.
When d > 2, we will need more
sophisticated arguments (see §6).
We ﬁx m ≥4 and let P = Pm be the set of points
µj = µj(m) := (cos θj, sin θj),
θj = θj,m := 2πj
m , j ∈Z.
(5.2)
There are m distinct points and µj = µj′ if j and j′ are congruent modulo m, i.e., if j ≡j′. These
points are equally spaced on the circle.
Let Xn be the linear space spanned by the dictionary elements φ whose line segment Lφ has end
points µi and µj, 1 ≤i < j ≤m. Notice that for each pair i, j there are two such atoms. Hence,
the dimension of Xn is n := m(m −1). We also note that Xn contains all linear functions on Ω.
Given i, j ∈Z, we deﬁne the distance between i and j by
d(i, j) := min{|i′ −j′| : i ≡i′, j ≡j′},
i.e. to be the periodic distance between the indices i and j.
Let Li,j = Li,j(m) be the set of all line segments L whose end points a, b are the points
(cos θ, sin θ) where θ ∈[θi, θi+1] in the case of a and θ ∈[θj, θj+1] in the case of b. We denote by
Si,j = Si,j(m) the union of all the line segments Lφ in Li,j.
Note that the length Li,j and width Wi,j of Si,j satisfy
|Lij| ≈d(i, j) + 1
m
,
|Wij| ≈d(i, j) + 1
m2
,
1 ≤i ≤j ≤m.
(5.3)
Here and later in this section, all constants of equivalence are absolute. It follows that the measure
of Si,j satisﬁes
|Si,j| ≲(d(i, j) + 1)2
m3
,
1 ≤i ≤j ≤m.
(5.4)
Lemma 5.1. Suppose that m ≥4 is an even integer, n = m(m −1), and φ = σ(·; ξ, t) is any
dictionary element whose line segment Lφ is in Li,j = Li,j(m) with µi ̸= µj. Then there is a
function g ∈Xn such that
(i) φ(x) = g(x), x /∈Si,j,
(ii) ∥φ −g∥L∞(Ω) ≤C d(i,j)
m2 , with C an absolute constant.
(iii) ∥φ −g∥L2(Ω) ≤Cw(φ)n−3/4, with C an absolute constant.
If φ ∈Li,i for some i, then there is a g ∈Xn such that statement (iii) holds.
Proof: We ﬁrst assume that 0 ≤i < j ≤m. Also, by reversing the roles of i and j if necessary, we
can also assume that j < m/2. Because of rotational symmetry we can assume that i = 0, i+1 = 1,
0 < j < m/2. Consider the linear function ℓ(x) := ξ · x −t. Let the line segment Lφ = Hφ ∩Ω
associated with φ be in Li,j. Let µi = µi(m), i ∈Z. We use the following three functions φ1, φ2, φ3
in Xn each of whose line segments Lφi are contained in Li,j. Here, Lφ1 has endpoints µi, µj+1,
the second segment Lφ2 has end points µi, µj, and the third function φ3 has line segment Lφ3 with
endpoints µi+1, µj+1. The orientation of these three atoms matches that of φ. By this we mean
that whenever x ∈Ωis strictly outside Si,j and φ(x) > 0 then each of the functions φi, i = 1, 2, 3,
10

will likewise be positive. Similarly, if x is strictly outside this strip and φ(x) = 0 the three functions
φi, i = 1, 2, 3, will likewise vanish.
Consider the three linear functions ℓj, j = 1, 2, 3, corresponding to these line segments. That
is, we have ℓi(x) = ξ′
i · x −t′
i and φi(x) = ℓi(x)+ with ξ′
i ∈S1 and t′
i ∈[−1, 1]. Since these three
linear functions are linearly independent, we can write
ℓ= c1ℓ1 + c2ℓ2 + c3ℓ3.
(5.5)
Speciﬁcally, let ζ be the point where ℓ2(ζ) = ℓ3(ζ) = 0. Then,
c1 = ℓ(ζ)
ℓ1(ζ),
c2 = ℓ(µj+1)
ℓ2(µj+1),
c3 = ℓ(µi)
ℓ3(µi).
(5.6)
This follows by noting that with this choice (5.5) holds at the aﬃnely independent set of points ζ,
µj+1 and µi.
We claim that
|ci| ≤1,
i = 1, 2, 3.
(5.7)
Indeed, since the ξ, ξi lie on the sphere it is clear that |ℓi(x)| = d(x, Lφi) and |ℓ(x)| = d(x, Lφ) for
any x ∈R2 (here d(x, L) denotes the distance from the point x to the line L). We will show that
d(µi, Lφ) ≤d(µi, Lφ3),
(5.8)
which implies |c3| ≤1. A completely analogous argument shows that |c2| ≤1.
For the proof of (5.8), we assume that j > i + 1. If j = i + 1, a similar argument applies
(which we leave to the reader). Consider the trapezoid whose vertices are µi, µi+1, µj, µj+1 and let
T denote its interior. Let ¯µi denote the orthogonal projection of µi onto the line Lφ3. The angle
formed by the vertices µj+1, µi+1, µi is larger than or equal to π/2. This means that ¯µi lies either
on or outside of the circle. By the deﬁning property of Lφ this line must intersect the segment
[µi, ¯µi]. Therefore, d(µi, Lφ) ≤d(µi, Lφ3) which proves (5.8) as desired.
Next, we consider bounding |c1|. The line segments [µi, µj+1] and [µi+1, µj] are parallel, and
the intersection point ζ lies on the perpendicular line Lp connecting the midpoints of these two line
segments. Moreover, the lengths of these segments satisfy l([µi, µj+1]) > l([µi+1, µj]). This means
that the distance from ζ to Lφ1 is greater than the distance to the parallel line segment [µi+1, µj].
Finally, since Lφ ∈Li,j, Lφ must intersect Lp, which implies that d(ζ, Lφ) ≤d(ζ, Lφ1). This means
that |c1| ≤1 as claimed.
Now, consider the function
g := c1φ1 + c2φ2 + c3φ3,
(5.9)
which is in the linear space Xn. This function agrees with φ outside Si,j so that (i) is satisﬁed.
Each of the functions φ and g have L∞(Si,j) norm not exceeding the width Wij ≤C d(i,j)
m2
(they are
1-Lipschitz and vanish on one edge) and so the upper bound in (ii) follows. The function φ −g is
supported on Si,j and we have
∥φ −g∥L2(Ω) ≤∥φ −g∥L∞(Ω)|Si,j|1/2 ≤Cd(i, j)2m−2−3/2 ≤C|Lij|2m−1−1/2.
(5.10)
Note that in this calculation we have use that d(i, j) ≈(d(i, j) + 1). Since d(i, j) > 1, we easily see
that |Lij| ≈|Lφ| = w(φ), which veriﬁes (iii).
Finally, if Lφ is in Li,j with d(i, j) ≤1 then the conclusion follows in the same way we proved
(5.10) by taking either v = 0 or v = w · x + b to be linear function which matches the linear part
of φ.
✷
11

5.1
The approximation theorem
Throughout this section En(f) := En(f)L2(Ω), n ≥1 for any f ∈L2(Ω). We can now state the
main theorem to be proved in this section.
Theorem 5.2. Let Ω= B2 and w(φ), φ ∈D(Ω), be deﬁned by (5.1). Then for any f ∈Vw, we
have
En(f) ≤C∥f∥Vw(Ω)n−5
4,
n ≥1,
(5.11)
where C is an absolute constant.
Proof: Since Σn ⊂Σn+1, n ≥0, it is enough to prove the theorem for any n = m(m −1) with
m ≥4 an even integer. This means that we can apply Lemma 5.1. It is enough to prove the
theorem for any function f from U(Vw(Ω)). According to the deﬁnition of Vw(Ω), for N suﬃciently
large, there is an S ∈ΣN with S = PN
j=1 ajφj such that
∥f −S∥L2(Ω) ≤n−5/4
and
N
X
j=1
w(φj)|aj| ≤1.
(5.12)
For each j, let gj ∈Xn approximate the function φj appearing in the representation of S according
to (iii) of Lemma 5.1. That is, we have
∥φj −gj∥L2(Ω) ≤C0w(φj)n−3/4,
(5.13)
with C0 an absolute constant. The function g := PN
j=1 ajgj is in Xn and hence in Σn. We write
f = f −S + h + g,
h := S −g.
(5.14)
Therefore,
E3n(f) ≤n−5/4 + E2n(h).
(5.15)
We want to bound E2n(h). We have h = PN
j=1 aj[φj −vj]. We consider the dictionary D′ = {ψj}N
j=1
with ψj := w(φj)−1(φj −gj). According to (5.12) and (5.13), each ψj has L2(Ω) norm at most
C0n−3/4 and h = PN
j=1 c′
jψj with PN
j=1 |c′
j| ≤1. It follows from Maurey’s theorem (see (4.15)) that
h can be approximated by a sum T of n terms from the dictionary D′ with error
∥h −T∥L2(Ω) ≤Cn−3/4n−1/2 = Cn−5/4,
(5.16)
with C an absolute constant.
The function T is a sum of at most 2n terms from the original
dictionary D. Hence,
E2n(h) ≤Cn−5/4.
(5.17)
If we place this inequality back into (5.15), we obtain
E3n(f) ≤[1 + C]n−5/4
(5.18)
and the theorem follows.
✷
We close this section by again emphasizing that Theorem 5.2 is an improvement on the known
theorem that any f ∈V(D) satisﬁes En(f) ≤Cn−5/4 because the weighted variation space Vw is
strictly larger than the standard variation space V.
12

5.2
Weighted variation spaces for Ω= Q2
Although we do not formulate a general result, it will be clear that the techniques of this paper can
be generalized to any convex domain Ω. In this section, we want to point out what such a result is
for Q2 := [−1, 1]2 since this will allow us to see the eﬀect of the geometry of Ω. So, in going further
in this section, we take Ω= Q2.
If φ is a ReLU atom, then the line segment Lφ relative to Ωis Hφ ∩Ω. The length |Lφ| can
now be large even if Lφ is close to the boundary of Ω, for example when Lφ is parallel to one of
the sides of Ω. In other words, many fewer atoms φ will have small |Lφ|.
Let us sketch how the results and analysis for approximating general atoms φ given in §5.1 for
B2, changes in this case. We now take a set of m ∼√n equally spaced points on the boundary
of Q2. We can associate each φ to a Li,j similar to the case of B2 and create a linear space Xn
of dimension m2 ∼n as before. Now the analogue of Lemma 5.1 says that any dictionary element
φ can be approximated by an element of g ∈Xn to an accuracy (corresponding to (iii) in that
lemma)
∥φ −g∥L2(Ω) ≤C0m−1[|Lφ|m−1]1/2 = C0m−3/2|Lφ|1/2.
(5.19)
Here the factor m−1 reﬂects the L∞error and the bracketed factor is the measure of the support
where φ and g diﬀer.
Given the above calculations, we deﬁne w(φ) := |Lφ|1/2 as the weight of the atom φ and use
this weight to deﬁne Vw(Q2). The proof of Theorem 5.2 now gives
Theorem 5.3. Let d = 2 and Ω= Q2 and deﬁne w(φ) := |Lφ|1/2. Then for any f ∈Vw, we have
En(f) ≤C∥f∥Vw(Q2)n−5
4 ,
n ≥1,
(5.20)
where C is an absolute constant.
6
Approximation in L2(Bd)
We turn now to the case of approximation on the domain Ω= Bd, d > 2, i.e., Ωis the unit
Euclidean ball of Rd. Recall that each atom φ = φ(·; ξ, t), satisﬁes (ξ, t) ∈¯Z(Ω) = Sd−1 × [−1, 1].
To each atom, we assign the special weight
w(ξ, t) := w∗(ξ, t) := (1 −t)
1
2 + d
4 .
(6.1)
From (4.3), we see that this weight is admissible for Ω. Thus, w is taken to be given by (6.1)
throughout this section.
We recall the variation space Vw introduced and studied in §4. The main result of this paper is
the following theorem
Theorem 6.1. If f ∈Vw = Vw(Ω) then
En(f) := En(f)L2(Ω) ≤C∥f∥Vwn−1
2 −3
2d ,
n ≥1,
(6.2)
where C depends only on d.
13

Notice that this theorem gives a stronger result than the previously known results on approxima-
tion by shallow neural networks with ReLU activation. Indeed, although the approximation rate
O(n−1
2−3
2d ) is the same as known whenever f ∈V, the assumption of membership in Vw is a strictly
weaker assumption than the membership in the traditional variation space V.
The remainder of this section is devoted to proving Theorem 6.1. The proof is similar, in spirit,
to the case d = 2 which was given in Theorem 5.2, but it is quite a bit more technical. Our ﬁrst goal
is to construct certain linear spaces Xn of dimension at most n, which can be used to eﬀectively
approximate general ReLU atoms. The space Xn will be the span of at most n well chosen atoms
from D(Ω). The choice of the atoms used to deﬁne Xn would intuitively be gotten by discretizing
the unit Euclidean sphere Sd−1 with md−1 uniformly spaced vectors and then and to discretize
the oﬀsets in T = [−1, 1] with m points. Here m is chosen so that n ≈md. The discretization
of T will not be uniform but instead will be be done in such a way that atoms whose support is
small, i.e., atoms whose associated hyperplane lies near the boundary Sd−1 of Bd will be very well
approximated.
Since there is no natural discretization of Sd−1, when d > 2, we proceed as follows.
Let
Q := Qd := [−1, 1]d and F be a face of Q. Each face F is gotten by setting one of the coordinates,
say, coordinate i, equal to either +1 or −1. Given one of these faces F, we shall use dyadic partitions
of F into d −1 dimensional cubes of side length 2−k. We let Vk(F) be the set of all vertices of this
partition. Thus, the cardinality of Vk(F) is (2k + 1)d−1. We deﬁne Vk to be the union of all of the
sets Vk(F) as F runs through the 2d faces of Q. This gives a discrete set of points on the boundary
of Q with ℓ∞spacing 2−k. To obtain our discrete set of points on Sd−1, we simply renormalize.
Namely,
Wk :=

ξ =
¯ξ
∥¯ξ∥: ¯ξ ∈Vk

(6.3)
gives a set of points on the boundary of Bd that are quasi-uniformly spaced in the sense that
c02−k ≤dist(ξi, Wk \ {ξi}) ≤C02−k,
(6.4)
where the constants1 depend only on d. After adjusting for redundancy, we see that the cardinality
of Wk is 2d(2k)d−1. It is important to note that
Vk ⊂Vk+1
and
Wk ⊂Wk+1,
k ≥1.
(6.5)
We also want to discretize the oﬀsets t. For this, we take
Tm := {−1 < t1 < · · · < t2m = 1}.
(6.6)
We take the ﬁrst m of these to be equally spaced in [−1, 0], i.e., tj := −1 + j/m, j = 1, . . . , m. For
the remainder of these points, we take
tj+m := cos π(m −j)
2m
=: cos θj,m,
j = 1, . . . , m.
(6.7)
Notice that the points in Tm have a ﬁner spacing near one. Concerning this spacing, in going
further we will use the fact that for each m < j < 2m −1 and t ∈[tj, tj+1] we have
π
q
1 −t2
j+1
2m
≤|tj+1 −tj| ≤
π
q
1 −t2
j
2m
and
q
1 −t2
j ≤2
q
1 −t2
j+1 ≤2
p
1 −t2.
(6.8)
1In this paper, all constants depend only on d and may change from line to line. We use c for small constants and
C for large constants, sometimes with subscripts.
14

To prove this, we note that for ﬁxed j = i + m, 1 ≤i < m, we have
|tj+1 −tj| = π
2m sin ζ = π
2m
p
1 −cos2 ζ
where ζ ∈[θi+1,m, θi,m].
This gives the ﬁrst inequalities in (6.8).
The second inequalities are
proved similarly. The inequalities in (6.8) show that for any given t ∈[tj, tj+1], j ≤2m −2, we
have |tj+1 −tj| ≈
√
1 −t2/m with absolute constants in this comparison. We shall use this fact
repeatedly.
We now want to deﬁne the linear space Xn. Consider the set of atoms given by
Φk,m := {φ(·; ξ, t), ξ ∈Wk, t ∈Tm}.
(6.9)
This is a set of at most 4dm(2k)d−1 ReLU atoms. We choose k as the largest integer such that
4d2k(2k)d−1 ≤n and then take m = 2k. Then, Φn := Φk,m is a set of at most n atoms. We deﬁne
Xn as the linear space
Xn := span(Φn),
(6.10)
Then, Xn is a linear space of dimension at most n.
We caution the reader that for the remainder of this paper, the integer n is always taken of the
form n = 4dmd, where m = 2k. It is enough to prove our approximation results for these n.
We now proceed to show that any atom φ := φ(·; ξ, t) from D(Ω) can be well approximated by
an element of the linear space Xn. We ﬁx ξ, t and n. The approximation result we prove is given
in the following theorem.
Theorem 6.2. For any (ξ, t) ∈¯Z(Ω) = Sd−1 × [−1, 1], there is an element g = gφ ∈Xn such that
∥φ −g∥L2(Ω) ≤Cw(φ)n−3
2d ,
(6.11)
with the constant C depending only on d.
The proof of this theorem is a bit technical and given in the next subsection. After proving this
theorem we prove Theorem 6.1. In the constructions given below there are two important constants
A and L which depend only on d. It will be useful to the reader if we explain their role and their
deﬁnition. To prove Theorem 6.2, we are presented with an atom φ(x) = (ξ · x −t)+ and need to
construct an element g ∈Xn that approximates φ to the given accuracy. From the deﬁnition of
Xn, the function g will take the form
g =
n
X
j=1
ajφj,
(6.12)
where the φj are the atoms φj(x) = (ξj · x −τj)+ used to deﬁne Xn, where τj = ti(j). The function
g that we construct to provide the approximation will agree with φ except on a certain set of small
measure. The only atoms active in the deﬁnition of g, i.e., which have nonzero coeﬃcients, will
satisfy ∥ξ −ξj∥≤A
m, with A ≥2 a ﬁxed integer constant depending only on d. The size of A is
determined by the proof of Lemma 6.6 which is formulated later in this section and then proved
in the Appendix. Hence, in going forward, we can consider d arbitrary but ﬁxed and A depending
only on d to be ﬁxed as well.
The constant L is an integer which is chosen in the proof of Lemma 6.5. We will only have
to consider values of t such that t ≤t2m−L. This restriction can be applied on t because of the
following remark.
15

Remark 6.3. Let us note and record the following:
(i) If t ≥t2m or even t ≥tm′ with m′ = 2m −L with L a ﬁxed integer, then for any atom φ(·; ξ, t)
the statement (6.11) holds by simply taking g = 0 and using the estimate (4.3) for the norm of the
atom.
(ii) If t ≤C < 1 with C ﬁxed then the weight w(ξ, t) ≥c. In this case, the existence of a space
Xn spanned by n atoms that provides the estimate (6.11) was given in [30]. While our space Xn is
deﬁned diﬀerently (we use a diﬀerent discretization of the oﬀsets t), the proof in this case is simpler
and we exclude this case going forward.
(iii) If ξ is one of the discrete vectors from Wk, then the proof of the existence of a g for which
(6.11) is quite simple. Indeed, one can take g = aφ(·; ξ, ti)+(1−a)φ(·; ξ, ti+1) where ti is the closest
discrete oﬀset to t and a is chosen so that ati + (1 −a)ti+1 = t.
In the proof of (6.11) we only need to provide a proof in the case that none of the special cases
(i-iii) holds.
6.1
The proof of Theorem 6.2
Obviously, we only need to prove the theorem for m suﬃciently large, say m ≥m∗where m∗
depends on d. The integer m∗will be speciﬁed as we go along. Because of Remark 6.3 we only
need to prove the theorem in the case 1/2 < t ≤tm−L, where L is a ﬁxed integer depending only
on d. Again, we shall specify L as we proceed in the proof. Similarly, we can assume ξ is not in
Wk. We ﬁx such an ξ ∈Sd−1 and such a t throughout this subsection.
We deﬁne
H+ := {x : ξ · x ≥t}
and
H−:= {x : ξ · x < t}.
(6.13)
So φ is identically zero on H−and the linear function ξ · x −t on H+. For any one of the vectors
ξi appearing in the set Wk and any given a tj ∈Tm, we similarly deﬁne
H+
j := Hj(ξi) := {x ∈Ω: ξi · x ≥tj},
H−
j := H−
j (ξi) := {x ∈Ω: ξi · x < tj}.
(6.14)
Given ξi, we want to choose a tj with j ≤2m −1 (depending on i) that is close to t and so that
H+
j is a subset of H+. This is always possible whenever ∥ξ −ξi∥≤A/m and t ≤tm−L and L is
suﬃciently large (depending only on A). One such choice for tj is to take
t+
i := t+(ξi) := min{tj ∈Tm : H+
j ⊂H+}.
(6.15)
If t+
i = tj, we let
˜ti := tj+1.
(6.16)
Then, we will also have H+
j+1 ⊂H+.
We now proceed to proving Theorem 6.2. We begin by recalling the following fact.
Lemma 6.4. If ξ, ξ′ ∈Sd−1 with ∥ξ −ξ′∥= δ, then we have
ξ · ξ′ = 1 −δ2/2.
(6.17)
Proof: By rotation, we can assume ξ = e1 = (1, 0, . . . , 0) and ξ′ = αe1 + η where η is orthogonal
to e1 and ∥η∥2 = 1 −α2. Therefore,
δ2 = (1 −α)2 + ∥η∥2 = 2 −2α = 2 −2ξ · ξ′
16

and so (6.17) follows.
✷
The last lemma allows us to compare t+
i with t.
Lemma 6.5. Given the integer A, we deﬁne
L := (A + 1)2 = A2 + 2A + 1.
(6.18)
If m∗is suﬃciently large, depending only on d and m ≥m∗, then whenever t ∈[1/2, t2m−L] and
∥ξ −ξi∥≤A
m, then t+
i and ˜ti are well deﬁned, and we have
t ≤t+
i ≤˜ti ≤t + C1
m
p
1 −t2,
(6.19)
where C1 depends only on d.
Proof: Consider ﬁrst the existence of t+
i , ˜ti. It is enough to show that if t ≤t2m−L, and ξi satisﬁes
∥ξ −ξi∥≤
A
m, then there is a j ≤2m −2 such that H+
j
⊂H+. Suppose that j is such that
tj ≥t but H+
j is not contained in H+. Then, there is an x = tjξi + η with η orthogonal to ξi and
∥η∥≤
q
1 −t2
j and
x · ξ = tjξ · ξi + η · (ξ −ξi) < t.
From Lemma 6.4, we know that ξ · ξj ≥1 −A2
m2 and so we must have

1 −A2
m2

tj ≤t + ∥ξ −ξi∥
q
1 −t2
j ≤t + A
m
q
1 −t2
j.
(6.20)
That is, we must have
tj ≤t + A
m
q
1 −t2
j + A2
m2 ≤t2m−L + A
m
q
1 −t2
j + A2
m2 .
(6.21)
If we write j = 2m −k, and use the deﬁnition of the tj (see (6.7)) we can rewrite (6.21) as
cos πk
2m −cos πL
2m ≤A
m sin πk
2m + A2
m2 ≤πAk
2m2 + A2
m2 ≤A2 + 2Ak
m2
.
(6.22)
The left side of (6.22) is larger than k(L−k)
m2
and so we see with the above deﬁnition of L, (6.22) is
violated when k = 2. This proves that t+
i and ˜ti are well deﬁned.
We turn now to proving (6.19).
First note that if there is j ∈{m + 1, . . . , 2m} such that
H+
j ⊂H+, then we must have tjξi · ξ ≥t which gives the left inequality in (6.19). To prove the
right inequality in (6.19), we let t+
i = tj, ˜ti = tj+1. It follows from the minimality in the deﬁnition
of t+
i that we must have H+
j−1 is not contained in H+. Thus, the inequality (6.20) holds with j
replaced by j −1. This gives

1 −A2
m2

tj−1 ≤t + ∥ξ −ξi∥
q
1 −t2
j−1 ≤t + 2A
m
q
1 −t+
i
2,
(6.23)
where the last inequality uses (6.8). From (6.8), we also have
tj ≤tj−1 + (tj −tj−1) ≤tj−1 + π
m
q
1 −t2
j.
17

If we multiply both sides of this last inequality by (1 −A2
m2 ) and use (6.23) we obtain

1 −A2
m2

t+
i ≤t + 2(A + 2)
m
p
1 −t2,
where we used that t+
i ≥t . We also have ˜ti ≤t+
i +
π
2m
√
1 −t2 because of (6.8). When these facts
are used in the last inequality we obtain the right inequality in (6.19).
✷
Now consider any ξ ∈Sd−1 and deﬁne
Wk(ξ) :=

ξi ∈Wk : ∥ξ −ξi∥≤A
m

and
t+ := t+(ξ) :=
max
ξi∈Wk(ξ) t+
i .
(6.24)
We can write t+
i = tj for some j ≤2m −1 and deﬁne ˜t := ˜t(ξ) := tj+1. From the previous lemma,
we know that
t ≤t+ ≤˜t ≤t + C1
m
p
1 −t2,
(6.25)
where C1 depends only on d.
For the construction of g, we use the following lemma.
Lemma 6.6. There is a constant m∗depending only on d such that the following holds. Given any
m ≥m∗and any ξ ∈Sd−1 and any 1/2 ≤t ≤t2m−L, there exists (a+
i ), (˜ai) such that
(i)
ξ = P
ξi∈Wk(ξ) a+
i ξi + P
ξi∈Wk(ξ) ˜aiξi,
(ii)
t+ P
ξi∈Wk(ξ) a+
i + ˜t P
ξi∈Wk(ξ) ˜ai = t,
(iii)
P
ξi∈Wk(ξ) |a+
i | + P
ξi∈Wk(ξ) |˜ai| ≤C1, where C1 depends only on d.
The proof of this lemma is technical and so we place it in the Appendix so as not to interrupt the
ﬂow of the proof of Theorem 6.2.
We deﬁne the following function g which will be used to approximate φ = φ(·; ξ, t) in the case
1/2 ≤t ≤t2m−L
g(x) :=
X
ξi∈Wk(ξ)
[a+
i (ξi · x −t+)+ + ˜ai(ξi · x −˜t)+]
(6.26)
where the coeﬃcients come from Lemma 6.6. The functions appearing in the representation of g
are all in Xn and therefore g is also in Xn. From Lemma 6.6, we obtain
φ(x) −g(x) =


X
ξi∈Wk(ξ)
a+
i (ξi · x −t+) + ˜ai(ξi · x −˜t)


+
−
X
ξi∈Wk(ξ)
[a+
i (ξi · x −t+)+ + ˜ai(ξi · x −˜t)+]
(6.27)
Before bounding the error in approximating φ by g, let us make some remarks to motivate the
idea of how to estimate this error. Notice that if x ∈Ωis such that ξi · x ≥˜t for all ξi ∈Wk(ξ),
then x is also in H+ and so g(x) = φ(x). Similarly, if x ∈H−then φ(x) = g(x) = 0. This means
that the only points x ∈Ωwhere φ(x) ̸= g(x) must be in one of the sets
˜Ωi := {x ∈Ω: ξ · x > t, ξi · x ≤˜t}.
(6.28)
We will now proceed to bound the measure of each of these sets and also bound the error |φ(x)−g(x)|
on each of these sets.
18

Lemma 6.7. There are constants C and m∗depending only on d such that the following holds for
m ≥m∗and any 1/2 ≤t ≤t2m−L:
(i) If x ∈˜Ω:= S
ξi∈Wk(ξ) ˜Ωi, then
|φ(x) −g(x)| ≤C
p
1 −t2/m.
(6.29)
(ii) The measure of ˜Ωis bounded by
|˜Ω|d ≤C(1 −t2)d/2/m.
(6.30)
Proof: From Lemma 6.5 we have that
t ≤t+ ≤˜t ≤t + C1
√
1 −t2
m
,
i = 1, . . . , M
(6.31)
where C1 depends only on d. Now, for any ﬁxed i consider any x ∈˜Ωi. Our goal is to estimate
the distance from x to the hyperplane H := {z : z · ξ = t}. From the deﬁnition of Wk(ξ), we have
∥ξ −ξi∥≤A/m. Since α := x · ξ > t, we can write
x = αξ + η,
(6.32)
where η is orthogonal to ξ and
∥η∥≤
p
1 −α2 ≤
p
1 −t2.
(6.33)
We want to show that α cannot be too large. Since x ∈˜Ωi, we know that
(x · ξi) = αξ · ξi + η · ξi ≤˜t,
(6.34)
and
|η · ξi| = |η · (ξi −ξ)| ≤A
√
1 −t2
m
.
(6.35)
Using the inequality ξ · ξi ≥1 −A2/m2 (see Lemma 6.4) and (6.35) back in (6.34), we obtain
(1 −A2m−2)α ≤˜t + A
√
1 −t2
m
≤t + C1
√
1 −t2
m
+ A
√
1 −t2
m
,
(6.36)
where the last inequality used (6.31). Going further, we note that since by assumption t ≤t2m−L,
we have
(1 −A2m−2)−1 ≤1 + 2Am−2 ≤1 + 2A
√
1 −t2
m
.
Using this estimate back in (6.36), we arrive at
α ≤t + C2
√
1 −t2
m
,
(6.37)
with C2 depending only on d. It is important to note that this bound is independent of i. Therefore
any point x in ˜Ωcan be written as x = αξ + η where α satisﬁes (6.37) and ∥η∥≤
√
1 −t2. The
measure of the set of such η is ≤C3[
√
1 −t2]d−1. Hence, we have proven (ii).
19

The inequality (6.37) also shows that
φ(x) ≤C2
√
1 −t2
m
,
x ∈˜Ω.
(6.38)
Since the functions appearing in the sum for g are all smaller than φ from (iii) of Lemma 6.6 we
conclude that
|g(x)| ≤C3
√
1 −t2
m
,
x ∈˜Ω.
(6.39)
This proves (i) and concludes the proof of the lemma.
✷
Proof of Theorem 6.2 According to Remark 6.3, we only need to consider the case φ = φ(·; ξ, t)
when 1/2 ≤t < t2m−L. We return to our representation (6.27). As already mentioned φ(x) = g(x)
outside the set ˜Ω. From Lemma 6.7 it follows that
∥φ −g∥L2(Ω) ≤C|˜Ω|1/2
d
√
1 −t2
m
≤Cw(φ)m−3/2.
(6.40)
Since md ≥cdn, we have completed the proof of Theorem 6.2.
✷
6.2
Proof of Theorem 6.1
We can now prove Theorem 6.1 in the same way we proved the case d = 2. Let f be any ﬁxed
function from Vw(Ω). According to the deﬁnition of Vw(Ω), for N suﬃciently large, there is an
S ∈ΣN with S = PN
j=1 ajφj such that
∥f −S∥L2(Ω) ≤Mn−1
2−3
2d
and
N
X
j=1
w(φj)|aj| ≤∥f∥Vw =: M.
(6.41)
For each j, let gj ∈Xn approximate the function φj appearing in the representation of S according
to Theorem 6.2. That is, we have
∥φj −gj∥L2(Ω) ≤Cw(φj)n−3
2d ,
(6.42)
with C depending only on d. The function g := PN
j=1 ajgj is in Xn and hence in Σn. We write
f = f −S + h + g,
h := S −g =
N
X
j=1
aj[φj −gj].
(6.43)
Therefore,
E3n(f) ≤Mn−1
2 −3
2d + E2n(h).
(6.44)
To bound E2n(h), we consider the dictionary D′ = {ψj}N
j=1 with ψj := w(φj)−1(φj −gj).
According to Theorem 6.2 each ψj has L2(Ω) norm at most Cn−3
3d and h = PN
j=1 cjψj with
PN
j=1 |cj| ≤M. It follows from Maurey’s Theorem (see (4.15)) that h can be approximated by a
sum T of n terms from the dictionary D′ with error
∥h −T∥L2(Ω) ≤CMn−3
2d n−1/2 = CMn
1
2−3
2d .
(6.45)
20

The function T is a sum of at most 2n terms from the original dictionary D. Hence,
E2n(h) ≤CMn−1
2−3
2d .
(6.46)
If we place this inequality back into (6.44), we obtain
E3n(f) ≤CMn−1
2−3
2d ,
(6.47)
and the theorem follows.
✷
Remark 6.8. If we consider Ω= Qd in place of Bd then for the weight w(φ(·; ξ, t)) = |Lφ|1/2
where |Lφ| is the (d −1)-dimensional measure of the intersection Qd ∩Lφ, we obtain
En(f)L2(Qd) ≤Cn−1
2−3
2d ∥f∥Vw,
f ∈Vw.
(6.48)
We do not give the proof which follows along the lines of the case d = 2 given in §5.2.
7
Concluding remarks and implications to neural network training
The main theme of this paper was to introduce, for a bounded domain Ω⊂Rd, new model classes
Vw := Vw(D(Ω)), called weighted variation spaces, and to prove bounds on how well functions in
these classes can be approximated by a linear combination of n terms of the ReLU dictionary D(Ω).
That is, we provided bounds on the error En(f)L2(Ω) in approximating f ∈Vw(Ω) in the L2(Ω)
norm by the elements of Σn := Σn(D(Ω)) where Σn is the nonlinear parameterized manifold of
functions g that are a linear combination of n elements of the ReLU dictionary. We showed that
for certain choices of the weight w (dependent on Ω) the functions in these new model classes have
the same approximation rate as those in the classical variation spaces V(Ω). Since Vw is strictly
larger than the classical variation spaces V := V(Ω), this gives stronger results on n-term ReLU
approximation than those in the literature. Thus, these new model classes Vw are important in
trying to understand which functions are well approximated by Σn.
These approximation theoretic results may also have implications for training and regularization
when using shallow neural networks for learning an unknown function from data observations. We
outline these implications in the remainder of this section. We assume that d ≥2 and Ω= Bd
throughout this section. Therefore, ¯Z(Ω) = Sd−1×[−1, 1]. Let w be any admissible weight function
in the sense of (4.9) and let Dw be the weighted dictionary deﬁned in (4.10). We use the results
and notation of §4 in going forward. In particular, the functions in Vw := Vw(Ω) all take the form
(see (4.11))
˜fµ :=
Z
¯Z(Ω)
˜φ(·; ξ, t) dµ
and
∥˜fµ∥Vw(Ω) = ∥µ∥M.
(7.1)
We consider the following data-ﬁtting problem. Suppose that xi, i = 1, . . . , m, are points from
the interior of Ωand yi, i = 1, . . . , m, are real numbers. The data-ﬁtting problem
inf
f∈Vw(Ω)
m
X
i=1
|yi −f(xi)|2 + λ ∥f∥Vw ,
(7.2)
21

with λ > 0 is equivalent to the data-ﬁtting problem
inf
µ∈M( ¯Z(Ω))
m
X
i=1
|yi −fµ(xi)|2 + λ ∥µ∥M ,
(7.3)
in the sense that their inﬁmal values are the same and if µ⋆is a minimizer of (7.3), then fµ∗is a
minimizer of (7.2). Note that the minimization problem (7.2) does not depend on the ambient space
L2(Ω) in which we measure error of performance for the approximation problem. An important
property of the weighted variation spaces is the following representer theorem.
Theorem 7.1. Suppose that w is an admissible weight function and the {xi}m
i=1 lie in the interior
of Ω. Then, there exists a solution f ∗to (7.2) that takes the form of a shallow ReLU network
f ⋆(x) =
n
X
j=1
ajφ(x; ξj, tj) =
n
X
j=1
aj(ξj · x −tj)+,
(7.4)
where the number of atoms satisﬁes n ≤m, {aj}n
j=1 ⊂R \ {0}, and {(ξj, tj)}n
j=1 ⊂¯Z(Ω) are data-
dependent and not known a priori. Furthermore, the regularization cost is ∥f ⋆∥Vw = Pn
j=1 w(ξj, tj) |aj|.
Proof: Let C( ¯Z(Ω)) denote the space of real valued functions on ¯Z(Ω). This is a Banach space
when equipped with the L∞-norm. By the Riesz–Markov–Kakutani representation theorem [9,
Chapter 7], the dual of C( ¯Z(Ω)) can be identiﬁed with the space of signed Radon measures M :=
M( ¯Z(Ω)). It is well-known that the extreme points of the unit ball
{µ ∈M : ∥µ∥M ≤1}
(7.5)
are the Dirac measures ±δ(ξ,t), (ξ, t) ∈¯Z(Ω) (see, e.g., [5, Proposition 4.1]).
Next, for i = 1, . . . , m, we introduce the functions
hi(ξ, t) := ˜φ(xi; ξ, t) = φ(xi; ξ, t)
w(ξ, t) ,
(ξ, t) ∈¯Z(Ω).
(7.6)
We can rewrite (7.3) as
inf
µ∈M
m
X
i=1
|yi −⟨µ, hi⟩|2 + λ ∥µ∥M ,
(7.7)
where ⟨·, ·⟩denotes the duality pairing between C( ¯Z(Ω)) and M.
Since the functions hi, i =
1, . . . , m, are in C( ¯Z(Ω)), the mappings µ 7→⟨µ, hi⟩are weak∗continuous [27, Theorem IV.20, p.
114]. This shows that the hypothesis of the abstract representer theorem of [32, Theorems 2 and 3]
(see also [4, 5]) are satisﬁed. That theorem shows that there exists a solution to (7.7) that takes
the form of a linear combination of the extreme points of the unit regularization ball. Thus, there
exists a solution that takes the form
µ⋆=
n
X
j=1
cjδ(ξj,tj),
(7.8)
where the number of atoms satisﬁes n ≤m, {cj}n
j=1 ⊂R \ {0}, and {(ξj, tj)}n
j=1 ⊂¯Z(Ω) are
distinct, data-dependent, and not known a priori. Clearly ∥µ⋆∥M = Pn
j=1 |cj|.
22

From the equivalence between (7.2) and (7.7), we see that the function
fµ⋆=
Z
¯Z(Ω)
˜φ(·; ξ, t)dµ⋆(ξ, t) =
n
X
j=1
cj
w(ξj, tj)φ(x; ξj, tj)
(7.9)
is a minimizer of (7.2). The theorem follows by the substitution aj := cj/w(ξj, tj).
✷
We have not indicated the fact that the solution (7.9) to the data ﬁtting problem depends on
λ. If we let λ tend to zero then the solutions converge to a minimum-norm interpolant f # of the
data
f # ∈argmin{∥f∥Vw : f(xi) = yi, i = 1, . . . , m}.
(7.10)
In which case, there always exists an f # that has a representation
f # =
n
X
j=1
a#
j φ(x; ξ#
j , t#
j ),
(7.11)
with n ≤m.
The theorem statement also holds when the ﬁrst term in the objective in (7.2) is replaced by
any loss function L(·, ·) which is lower semi-continuous (see [22, Proof of Theorem 3.2]). In neural
network parlance, the ξj are referred to as the input weights, the aj are referred to as the output
weights and the tj are referred to as the biases.
This representer theorem provides insight into new forms of regularization for neural networks.
Indeed, ﬁrst notice that the norm of a single neuron φ(·; ξ, t), where ξ ∈Rd and t ∈R, takes the
form
∥φ(·; ξ, t)∥Vw = ∥ξ∥w
 ξ
∥ξ∥,
t
∥ξ∥

,
(7.12)
where we took advantage of the fact that the ReLU is positively homogeneous of degree 1. In this
form, the input weights are not restricted to be unit norm. The representer theorem (Theorem 7.1)
implies that a solution to the variational problem in (7.2) can be found by training a suﬃciently
wide (ﬁxed width n ≥m) neural network to a global minimizer. In particular, by ﬁnding a solution
to the neural network training problem
min
θ
m
X
i=1
|yi −fθ(xi)|2 + λ
n
X
j=1
|aj| ∥ξj∥w
 ξj
∥ξj∥,
tj
∥ξj∥

,
(7.13)
where
fθ(x) =
n
X
j=1
ajφ(x; ξj, tj) =
n
X
j=1
aj(ξj · x −tj)+
(7.14)
is a shallow ReLU neural network and θ = (aj, ξj, tj)n
j=1 denotes the neural network parameters.
When w is the weight speciﬁed in (6.1) (which satisﬁes the hypotheses of Theorem 7.1), the
resulting regularizer takes the form
n
X
j=1
|aj| ∥ξj∥

1 −
tj
∥ξj∥
 1
2+ d
4
(7.15)
23

This is a new regularizer for training neural networks which directly penalizes the biases. If we
assume the data sites {xi}m
i=1 are drawn i.i.d. uniformly on Bd, then this penalization reﬂects the
volume of the subset of Bd where the neuron is “active” (nonzero output). This suggests a new,
data-adaptive regularization scheme in which the the penalty on a given neuron is proportional to
the number of data in its support. This regularizer should be contrasted with the unweighted case
in which the regularizer takes the form
n
X
j=1
|aj| ∥ξj∥,
(7.16)
which is sometimes referred to as the path-norm [19] of the neural network. Remarkably, path-
norm regularization is equivalent to the common procedure of training a neural network with weight
decay [13] which corresponds to a regularizer of the form
1
2
n
X
j=1
|aj|2 + ∥ξj∥2 .
(7.17)
We refer the reader to [23] for more details about this equivalence. The new regularizer in (7.15)
requires further study in both theory and practice.
7.1
Open Problems
The results presented in this paper open the door to several new research directions.
(i) We have shown that the classical variation space V(Ω) is not the approximation space Aα =
Aα(L2(Ω)), α = 1
2 + 3
2d, since the (strictly larger) weighted variation space Vw(Ω) admits the
same n-term approximation rate with shallow ReLU networks. Thus, the results of this paper
bring us one step closer to characterizing the approximation space Aα, α = 1
2 + 3
2d. Future
work will be devoted to ﬁnding a characterization of this approximation space.
(ii) The results of this paper only consider L2-approximation.
We conjecture that the same
rates hold for weighted variation spaces for all Lp, 1 ≤p ≤∞, where now the admissibility
condition on the weights will depend on p. That is to say, for each 1 ≤p ≤∞, there exists a
weight function w∗
p such that the the optimal rate n−1
2−3
2d is achieved.
(iii) The weighted variation spaces lead to a new form of data-adaptive regularization for neu-
ral networks. Theoretical and experimental comparisons of this new form of regularization
compared with more conventional regularization techniques is a direction of future work. Fur-
thermore, extensions of this regularizer to deep neural networks is also a direction of future
work.
8
Appendix
In this appendix, we prove Lemma 6.6. We let ξ be arbitrary but ﬁxed throughout this section.
We begin by recalling some well known results on the representation of points x in a cube R ⊂Rd
in terms of the vertices of R. Given any cube R ⊂Rd, we let V (R) denote its set of vertices. Let
24

us ﬁrst consider the case R = U where U := U d := [0, 1]d. We denote the vertices in V (U) by e.
So e is a vector with d components e = (e1, . . . , ed) with each ej ∈{0, 1}. There are 2d such e.
Let
ℓ0(s) := (1 −s)
ℓ1(s) = s,
s ∈R.
(8.1)
For each e ∈V , we deﬁne
ℓe(x) :=
d
Y
j=1
ℓej(xj),
x = (x1, . . . , xd) ∈U.
(8.2)
Then, ℓe(e′) = 0, e′ ̸= e and ℓe(e) = 1. Any x ∈U is represented as
x =
X
e∈V
ℓe(x)e.
(8.3)
This is a convex representation in that the coeﬃcients ℓe(x) ∈[0, 1], e ∈V (U), and they sum to
one.
Now consider an arbitrary cube R ⊂Rd. We can write R = v + α[0, 1]d = v + αU with α > 0.
This cube has vertices v + αe, e ∈V . Any point x = v + αy, y ∈U, from this cube, has the
representation
x = v + α
X
e∈V
ℓe(y)e =
X
e∈V
ℓe(y)[v + αe],
(8.4)
because P
e∈V ℓe(y) = 1. Again this is the representation of x as a convex combination of the
vertices V (R) of R. Let us note that here we are taking v as the smallest vertex of R. We can
derive a similar decomposition by starting with any other vertex of R.
We use the above to ﬁnd a variety of representations of any x on the boundary of the cube
Qd := [−1, 1]d. Later, we shall apply these representations to x = ¯ξ and subsequently to ξ ∈Sd−1.
Let x be in the face F of Qd. We assume that the d −1 dimensional face F of Qd corresponds to
x1 = 1. We will derive representations for points x ∈F. Similar representations hold for any of
the other faces of Qd.
Any x ∈F takes the form x = (1, ˜x) with ˜x ∈[−1, 1]d−1. Suppose now that R is any d −1
dimensional cube on F, i.e., R consist of points (1, ˜x) where ˜x is in a d −1 dimensional cube ˜R.
From the above, we can write ˜x = ˜v +αy, y ∈U d−1, where ˜v is the smallest vertex of R. Therefore,
we have
˜x = ˜v + αy = ˜v + α
X
e∈V (Ud−1)
ℓe(y)e =
X
e∈V (Ud−1)
ℓe(y)[˜v + αe] =
X
ν∈V ( ˜R)
γνν,
(8.5)
where the ν are the vertices of ˜R and
γν = ℓe(y),
when ν = ˜v + αe.
(8.6)
This is a representation of ˜x as a convex combination of the vertices V ( ˜R).
We turn now to representations of ξ ∈Sd−1. We write ξ =
¯ξ
∥¯ξ∥where ¯ξ lies on the boundary
of Qd = [−1, 1]d.
We assume ¯ξ lies on the face F corresponding to ﬁrst coordinate equal to
one. All other cases are handled similarly. We write ¯ξ = (1, ˜x) with ˜x ∈[−1, 1]d−1 and use the
25

representations of ˜x given above. Recall the discrete set of points Fk, with m = 2k. If k′ < k then
Fk′ ⊂Fk. We ﬁx such a k′ to be chosen in a moment.
We let A ≥1 be an integer whose value will be chosen below.
We place ourselves in the
following situation where ˜x ∈˜v + δU d−1 =: ˜R ⊂˜R′ := ˜v + δ′U d−1 where ˜v ∈Fk, δ = 2−k = 1/m
and δ′ = 2−k′ = Aδ with A = 2k−k′. The assumption that ˜x ∈˜R for which there is such a ˜R
and ˜R′ is a restriction on the position of ˜x in [−1, 1]d−1. When this is not the case, the argument
below needs to be adjusted by changing the choice of the initial vertex and the direction for the
representation. Since the adjustment is purely notational, we leave it to the reader.
We will give two representations of ˜x, respectively ¯ξ; the one in terms of the vertices of ˜R and
the second in terms of the vertices of ˜R′. For the ﬁrst representation, we use (8.5) with α = δ to
write
¯ξ =
X
ν∈V ( ˜R)
γν(1, ν) =
X
ν∈V ( ˜R)
γν
p
(1 + ∥ν∥2)ξν,
ξν =
(1, ν)
p
1 + ∥ν∥2 ,
(8.7)
with the coeﬃcients γν given by (8.6). Notice that the ξν are all in Wk. This gives the representation
ξ =
X
ν∈V ( ˜R)
aνξν,
aν := γν
p
(1 + ∥ν∥2)
∥¯ξ∥
.
(8.8)
We obtain a second representation as follows. We again write ˜x = ˜v + δy with y ∈U d−1. Then,
˜x = ˜v +δ
X
e∈V (Ud−1)
ℓe(y)e = ˜v +
X
e∈V (Ud−1)
ℓe(y)
A Aδe =

1 −1
A

˜v +
X
e∈V (Ud−1)
ℓe(y)
A
[˜v +Aδe]. (8.9)
This gives the representation
˜x =
X
ν∈V ( ˜R′)
γ′
νν,
(8.10)
where
γ′
ν = ℓe(y)
A
,
when ν = ˜v + Aδe with e ̸= 0 and γ′
0 = 1 −1
A + ℓ0(y)
A
.
(8.11)
Notice that this representation of ˜x is again a convex combination of the vertices of ˜R′. It follows
that
ξ =
X
ν∈V ( ˜R′)
a′
νξ′
ν,
a′
ν := γ′
ν
p
(1 + ∥ν∥2)
∥¯ξ∥
.
ξ′
ν =
(1, ν)
p
1 + ∥ν∥2
(8.12)
We now want to estimate the sums
S :=
X
ν∈V ( ˜R)
aν,
S′ =
X
ν∈V ( ˜R′)
a′
ν,
(8.13)
Lemma 8.1. There is an m∗= m∗(d), depending only on d, such that whenever m ≥m∗and A
is suﬃciently large (depending only on d), the following hold. Whenever ξ is not a vertex in Wk,
i.e., ¯ξ = (1, ˜x) where ˜x = ˜v + y where y ̸= 0, we have
S = 1 + ǫ
and
S′ = 1 + ǫ′,
where
0 < 2|ǫ| < |ǫ′| ≤σδ2,
(8.14)
26

and
σ := σ(y) =
X
e̸=0
ℓe(y) > 0,
(8.15)
where the strict inequality holds because y ̸= 0.
Proof: Let B2 := 1 + ∥˜v∥2 and recall that δ := 1/m.
Observe that when ν = ˜v + aδe,
e ∈V (U d−1), we have
1 + ∥ν∥2 = B2 + 2aδ⟨˜v, e⟩+ a2δ2∥e∥2 = B2 + sν(a),
(8.16)
where
sν(a) := 2aδ⟨˜v, e⟩+ a2δ2∥e∥2.
(8.17)
We are interested in the cases, a = 1, A. Notice that sν(a) = 0 when e = 0, i.e., ν = ˜v, and
also |sν(a)| ≤1/2 for these two values of a provided m∗is large enough. These facts will be used
without further mention in what follows.
We will use the Taylor expansion of the function F(s) :=
√
B2 + s. We have
F(s) = B + 1
2B−1s −1
4B−3s2 + O(s3),
|s| < 1.
(8.18)
This gives that
p
1 + ∥ν∥2 = F(sν(a)) = B + 1
2B−1sν(a) −1
4B−3sν(a)2 + O(sν(a)3)
(8.19)
From the above observations, we can write
∥¯ξ∥S′
=
X
ν∈V ( ˜R′)
γ′
νF(sν(A))
=
(1 −1/A)B +
X
e∈V (Ud−1)
ℓe(y)
A

B + 1
2B−1sν(A) −1
4B−3sν(A)2 + O(sν(A)3)

=
B + B−1
2
X
e∈V (Ud−1)
ℓe(y)sν(A)
A
−B−3
4
X
e∈V (Ud−1)
ℓe(y)sν(A)2
A
+ O(A2σδ3).
(8.20)
Let us analyze the ﬁrst sum Σ1 in (8.20). Using the deﬁnition of the sν, we see that this sum
equals
Σ1 = C1δ + C2Aδ2,
where
C1 = B−1 X
e̸=0
ℓe(y)⟨˜v, e⟩and C2 = B−1
2
X
e̸=0
ℓe(y)∥e∥2.
(8.21)
A similar analysis of the second sum Σ2 gives
Σ2 = C3Aδ2 + C4A2δ3 + C5A3δ4,
where
C3 = B−3 X
e̸=0
ℓe(y)⟨˜v, e⟩2,
and |C4|, |C5| ≤C0σ,
(8.22)
where C0 depends at most on d. In total, this gives
∥¯ξ∥S′ = B + C1δ + ˜CσAδ2 + O(σA2δ3),
(8.23)
27

where
σ ˜C = C2 + C3,
(8.24)
and where the constants in the ”O” term depend only on d. It is important to notice that
˜C ≥σ−1C2 ≥1/4.
(8.25)
Replacing A by one, we get
∥¯ξ∥S = B + C1δ + ˜Cσδ2 + O(σδ3).
(8.26)
Notice that these constants are the same as those in (8.23) and again the constants in the ”O”
term depend only on d.
Next, we want to compute ∥¯ξ∥and compare this number with B + C1δ. We have ¯ξ = (1, ˜x)
where
˜x = ˜v + δy = ˜v + δ
X
e∈V (Ud−1)
ℓe(y)e.
Therefore,
∥˜ξ∥2 = 1 + ∥˜v∥2 + 2δ
X
e∈V (Ud−1)
ℓe(y)⟨˜v, e⟩+ δ2∥y∥2 = B2 + s.
(8.27)
If we now use (8.18), we obtain
∥¯ξ∥= F(s) = B + B−1
2 s −B−3
4 s2 + O(s3) = B + C1δ + ˜C′σδ2 + O(σδ3).
where
σ ˜C′ = B−1
2 δ2∥y∥2 + B−3

X
e̸=0
ℓe(y)⟨˜v, e⟩


2
δ2.
(8.28)
Here, we have also used the fact that ∥y∥≤σ. If we use this expression for ∥¯ξ∥in (8.26), we obtain
S = 1 + C∗σδ2 + O(σδ3) =: 1 + ε,
C∗= ˜C −˜C′.
(8.29)
Similarly
S′ = 1 + C∗∗σδ2 + O(σA2δ3) =: 1 + ε′,
C∗∗= ˜CA2 −˜C′.
(8.30)
If we choose m suﬃciently large (m ≥m∗with m∗depending only on d and A as a suﬃciently
large integer depending only on d we will have 0 < 2ε < ε′ (see (8.25)). This completes the proof
of the Lemma.
✷
Note that the constant A of this lemma serves to deﬁne A for this paper and then L = (A + 1)2
is deﬁned as in Lemma 6.5.
Proof of Lemma 6.6:
Case ξ ∈Wk: Let ξ = ξi ∈Wk. Given t ∈[1/2, t2m−L], we have t+
i = tj and we take ˜ti := tj+1.
We deﬁne α by the requirement
αt+
i + (1 −α)˜ti = t,
i.e.
α = t −˜ti
t+
i −˜ti
.
(8.31)
28

Then, ξ = αξ + (1 −α)ξ, which is the decomposition for ξ required in Lemma 6.6. Indeed, |α| ≤C
with C depending only on d because of Lemma 6.5 and (6.8).
Case ξ is not in Wk: We will use the constructions given above. We take A to be an integer as
given in Lemma 8.1. We have given two ways of representing ξ as given in (8.8) and (8.12). The
ξν and ξ′
ν appearing in these representations are all from Wk. We take Wk(ξ) as the collection of
all these points. Property (ii) of Lemma 6.6 is satisﬁed since ∥ξ −ξi∥≤A/m for each i. We deﬁne
α by the requirement
αǫ + (1 −α)ǫ′ = 0,
i.e.
α =
ǫ′
ǫ′ −ǫ.
(8.32)
It follows that
ξ = α
X
ν∈V (R)
aνξν + (1 −α)
X
ν∈V (R′)
a′
νξ′
ν, =
M
X
j=1
bjξj,
M
X
j=1
bj = 1,
(8.33)
where all of the ξj are in W(ξ). The key here is that the coeﬃcients in this representation sum to
one.
Now, given t ∈[1/2, tm−L], we deﬁne
t+ := max{t+
i : ξi ∈Wk(ξ)} = tj,
˜t := tj+1.
(8.34)
Similar to the above, we deﬁne β by requiring that
βt+ + (1 −β)˜t = t,
i.e.
β = t −t+
t+ −˜t.
(8.35)
It follows that
ξ · x −t =
M
X
j=1
βbj(ξj · x −t+) +
M
X
j=1
(1 −β)bj(ξj · x −˜t).
(8.36)
This is the decomposition promised in Lemma 6.6 and thereby completes the proof of the lemma.
✷
References
[1] Francis Bach. Breaking the curse of dimensionality with convex neural networks. Journal of
Machine Learning Research, 18(1):629–681, 2017.
[2] Andrew R. Barron. Universal approximation bounds for superpositions of a sigmoidal function.
IEEE Transactions on Information Theory, 39(3):930–945, 1993.
[3] Helmut Bolcskei, Philipp Grohs, Gitta Kutyniok, and Philipp Petersen. Optimal approxima-
tion with sparsely connected deep neural networks. SIAM Journal on Mathematics of Data
Science, 1(1):8–45, 2019.
[4] Claire Boyer, Antonin Chambolle, Yohann De Castro, Vincent Duval, Fr´ed´eric de Gournay,
and Pierre Weiss.
On representer theorems and convex regularization.
SIAM Journal on
Optimization, 29(2):1260–1281, 2019.
29

[5] Kristian Bredies and Marcello Carioni.
Sparsity of solutions for variational inverse prob-
lems with ﬁnite-dimensional data. Calculus of Variations and Partial Diﬀerential Equations,
59(1):Paper No. 14, 26, 2020.
[6] Albert Cohen, Ronald DeVore, Guergana Petrova, and Przemyslaw Wojtaszczyk.
Optimal
stable nonlinear approximation. Foundations of Computational Mathematics, 22(3):607–648,
2022.
[7] Ronald DeVore, Boris Hanin, and Guergana Petrova. Neural network approximation. Acta
Numerica, 30:327–444, 2021.
[8] Ronald A. DeVore. Nonlinear approximation. Acta Numerica, 7:51–150, 1998.
[9] Gerald B. Folland. Real analysis: Modern techniques and their applications. Pure and Applied
Mathematics (New York). John Wiley & Sons, Inc., New York, second edition, 1999.
[10] R´emi Gribonval, Gitta Kutyniok, Morten Nielsen, and Felix Voigtlaender.
Approximation
spaces of deep neural networks. Constructive Approximation, 55(1):259–367, 2022.
[11] Lee K. Jones. A simple lemma on greedy approximation in Hilbert space and convergence
rates for projection pursuit regression and neural network training. The Annals of Statistics,
pages 608–613, 1992.
[12] Jason M. Klusowski and Andrew R. Barron. Approximation by combinations of ReLU and
squared ReLU ridge functions with ℓ1 and ℓ0 controls. IEEE Transactions on Information
Theory, 64(12):7649–7656, 2018.
[13] Anders Krogh and John Hertz. A simple weight decay can improve generalization. Advances
in neural information processing systems, 4, 1991.
[14] Vera Kurkov´a and Marcello Sanguineti. Bounds on rates of variable-basis and neural-network
approximation. IEEE Transactions on Information Theory, 47(6):2659–2665, 2001.
[15] Jianfeng Lu, Zuowei Shen, Haizhao Yang, and Shijun Zhang. Deep network approximation for
smooth functions. SIAM Journal on Mathematical Analysis, 53(5):5465–5506, 2021.
[16] Chao Ma, Lei Wu, et al. The barron space and the ﬂow-induced function spaces for neural
network models. Constructive Approximation, 55(1):369–406, 2022.
[17] Yuly Makovoz. Uniform approximation by neural networks. Journal of Approximation Theory,
95(2):215–228, 1998.
[18] Hrushikesh Narhar Mhaskar. On the tractability of multivariate integration and approximation
by neural networks. Journal of Complexity, 20(4):561–590, 2004.
[19] Behnam Neyshabur, Russ R. Salakhutdinov, and Nati Srebro. Path-SGD: Path-normalized
optimization in deep neural networks. Advances in neural information processing systems, 28,
2015.
[20] Greg Ongie, Rebecca Willett, Daniel Soudry, and Nathan Srebro. A function space view of
bounded norm inﬁnite width ReLU nets: The multivariate case. In International Conference
on Learning Representations, 2020.
30

[21] Rahul Parhi and Robert D. Nowak. Banach space representer theorems for neural networks
and ridge splines. Journal of Machine Learning Research, 22(43):1–40, 2021.
[22] Rahul Parhi and Robert D. Nowak. What kinds of functions do deep neural networks learn? In-
sights from variational spline theory. SIAM Journal on Mathematics of Data Science, 4(2):464–
489, 2022.
[23] Rahul Parhi and Robert D. Nowak.
Deep learning meets sparse regularization: A signal
processing perspective. arXiv preprint arXiv:2301.09554, 2023.
[24] Rahul Parhi and Robert D. Nowak. Near-minimax optimal estimation with shallow ReLU
neural networks. IEEE Transactions on Information Theory, 69(2):1125–1140, 2023.
[25] Allan Pinkus. Approximation theory of the MLP model in neural networks. Acta Numerica,
8:143–195, 1999.
[26] Gilles Pisier.
Remarques sur un r´esultat non publi´e de B. Maurey.
S´eminaire d’Analyse
Fonctionnelle (dit “Maurey-Schwartz”), pages 1–12, April 1981.
[27] Michael Reed and Barry Simon. Methods of Modern Mathematical Physics I: Functional anal-
ysis. Academic Press, 1972.
[28] Zuowei Shen, Haizhao Yang, and Shijun Zhang. Optimal approximation rate of relu networks
in terms of width and depth. Journal de Math´ematiques Pures et Appliqu´ees, 157:101–135,
2022.
[29] Jonathan W. Siegel. Optimal approximation rates for deep ReLU neural networks on Sobolev
spaces. arXiv preprint arXiv:2211.14400, 2022.
[30] Jonathan W. Siegel and Jinchao Xu. Sharp bounds on the approximation rates, metric entropy,
and n-widths of shallow neural networks. Foundations of Computational Mathematics, pages
1–57, 2022.
[31] Jonathan W. Siegel and Jinchao Xu. Characterization of the variation spaces corresponding
to shallow neural networks. Constructive Approximation, pages 1–24, 2023.
[32] Michael Unser. A unifying representer theorem for inverse problems and machine learning.
Foundations of Computational Mathematics, 21(4):941–960, 2021.
[33] Dmitry Yarotsky. Error bounds for approximations with deep ReLU networks. Neural Net-
works, 94:103–114, 2017.
Ronald DeVore, Department of Mathematics, Texas A&M University, College Station, TX 77843
Robert D. Nowak, Department of Electrical and Computer Engineering, University of Wisconsin–
Madison, Madison, WI 53706
Rahul Parhi, Biomedical Imaging Group, ´Ecole polytechnique f´ed´erale de Lausanne, CH-1015 Lau-
sanne, Switzerland
Jonathan W. Siegel, Department of Mathematics, Texas A&M University, College Station, TX
77843
31

