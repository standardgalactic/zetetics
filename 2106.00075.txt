Variational Combinatorial Sequential Monte Carlo Methods for Bayesian
Phylogenetic Inference
Antonio Khalil Moretti1,*
Liyi Zhang2,*
Christian A. Naesseth2
Hadiah Venner1
David Blei1,2
Itsik Pe’er1
1 Computer Science Department, Columbia University, USA
2 Data Science Institute, Columbia University, USA
1{amoretti,itsik}@cs.columbia.edu,
{lz2574,christian.a.naesseth,hkv2001,david.blei}@columbia.edu
Abstract
Bayesian phylogenetic inference is often con-
ducted via local or sequential search over topolo-
gies and branch lengths using algorithms such as
random-walk Markov chain Monte Carlo (MCMC)
or Combinatorial Sequential Monte Carlo (CSMC).
However, when MCMC is used for evolutionary pa-
rameter learning, convergence requires long runs
with inefﬁcient exploration of the state space. We
introduce Variational Combinatorial Sequential
Monte Carlo (VCSMC), a powerful framework that
establishes variational sequential search to learn
distributions over intricate combinatorial struc-
tures. We then develop nested CSMC, an efﬁcient
proposal distribution for CSMC and prove that
nested CSMC is an exact approximation to the (in-
tractable) locally optimal proposal. We use nested
CSMC to deﬁne a second objective, VNCSMC
which yields tighter lower bounds than VCSMC.
We show that VCSMC and VNCSMC are compu-
tationally efﬁcient and explore higher probability
spaces than existing methods on a range of tasks.
1
INTRODUCTION
What is the origin of SARS-COV-II and how can we an-
alyze the progression of its genetic variants? How do an-
tibodies evolve and develop in response to infection and
vaccination? Bayesian phylogenetic inference is a power-
ful statistical tool to address these and other questions of
central importance in molecular evolutionary biology and
epidemiology [Dhar et al., 2020, Boni et al., 2020]. Given an
evolutionary model and an alignment of observed molecular
sequences (DNA, RNA, PROTEIN), Bayesian methods sam-
ple latent bifurcating trees to uncover genetic history, quan-
tify uncertainty and incorporate prior information [Huelsen-
* = Authors contributed equally
beck and Ronquist, 2001]. Phylogenetic modeling involves
three distinct challenges: (i) sampling from a discrete distri-
bution to approximate an intractable summation over tree
topologies, (ii) for each tree, integrating over the continuous
branch lengths that govern the stochastic process for genetic
mutations, and (iii) performing parameter optimization or
model learning. The marginalization of tree topologies and
branch lengths is typically accomplished via local search
algorithms such as random-walk Markov chain Monte Carlo
(MCMC) [Huelsenbeck and Ronquist, 2001] or sequential
search algorithms such as Combinatorial Sequential Monte
Carlo (CSMC) [Bouchard-Côté et al., 2012]. Sophisticated
proposal methods based on Hamiltonian Monte Carlo or par-
ticle MCMC have been suggested to simultaneously sample
from composite spaces and optimize evolutionary parame-
ters [Dinh et al., 2017a, Wang et al., 2015, Wang and Wang,
2020]. However, these methods are often difﬁcult to imple-
ment, slow to converge requiring days or weeks of CPU
time, and heavily dependent upon heuristics.
Variational Inference (VI) is a computationally efﬁcient al-
ternative to MCMC. VI posits an approximate posterior and
then recovers parameters of both the model and approximate
posterior by maximizing a lower bound to the log-marginal
likelihood. One approach to learning variational distribu-
tions on phylogenetic trees is to parameterize the tree as a
sequence of subsplits, or ordered partitions on clades, and
to recast the problem as a Bayesian network [Zhang and
Matsen IV, 2018]. The drawback of this setup is that the
support of the conditional probability tables scales exponen-
tially with the number of taxa [Zhang and Matsen IV, 2019].
A body of recent work has established connections between
VI and sequential search by deﬁning a variational family of
distributions on hidden Markov models, where Sequential
Monte Carlo (SMC) is used as the marginal likelihood esti-
mator [Maddison et al., 2017, Le et al., 2018, Naesseth et al.,
2018, Lawson et al., 2018, Moretti et al., 2019a,b, Naesseth
et al., 2020, Moretti et al., 2020a,b, Moretti, 2021]. We
extend these approaches by developing variational sequen-
tial search methods that learn distributions over complex
Accepted for the 37th Conference on Uncertainty in Artiﬁcial Intelligence (UAI 2021).
arXiv:2106.00075v1  [stat.ML]  31 May 2021

combinatorial structures. Our contributions are as follows:
• We develop Variational Combinatorial Sequential
Monte Carlo (VCSMC), a novel variational objective
and structured approximate posterior deﬁned on the
space of phylogenetic trees. VCSMC blends CSMC and
VI, providing the user with a ﬂexible and powerful
approximate inference algorithm.
• We further extend CSMC with nested SMC [Naesseth
et al., 2015, 2019a], introducing a new efﬁcient pro-
posal distribution for CSMC. We prove that this pro-
posal is an exact approximation to the (intractable)
locally optimal proposal for CSMC. We use NCSMC
to deﬁne a second objective, VNCSMC which yields
tighter lower bounds than VCSMC.
• In empirical studies, we demonstrate the advantage
of VCSMC and VNCSMC. First, we analyze a stan-
dard dataset of primate mitochondrial DNA, then the
complete genomes of 17 Betacoronavirus species over
36,889 sites, and ﬁnally 7 benchmark datasets (DS1-
DS7) ranging from 27 to 64 taxa. VCSMC and VNC-
SMC are compared to existing benchmarks and shown
to perform favorably across a range of tasks.
Related Work.
Bayesian phylogenetics is often approxi-
mated using local search algorithms such as random-walk
MCMC [Huelsenbeck and Ronquist, 2001] or sequential
search algorithms such as CSMC [Bouchard-Côté et al.,
2012]. MCMC methods can also be used for model learning,
jointly estimating the phylogenetic trees and evolutionary
parameters. Probabilistic path Hamiltonian Monte Carlo
(ppHMC) [Dinh et al., 2017a] is one such method that ex-
tends Hamiltonian Monte Carlo by deﬁning a Markov chain
on the orthant complex of phylogenetic tree space. It is often
the case that the likelihood term in the MCMC acceptance
ratio is difﬁcult to evaluate. The idea of Particle MCMC al-
gorithms (PMCMC) is to use SMC as an unbiased estimate of
the marginal likelihood to deﬁne a proposal for MCMC [An-
drieu et al., 2010]. A PMCMC algorithm for evolutionary
parameter learning was introduced in [Wang et al., 2015],
and improved upon using a particle Gibbs sampler in [Wang
and Wang, 2020]. In contrast to these methods, the proposed
approach leverages VI for inference and introduces a new
efﬁcient proposal distribution for CSMC.
One approach to VI for phylogenetic trees is to parame-
terize a tree as a sequence of subsplits, or ordered parti-
tions on clades and to recast the problem as a Bayesian
network [Zhang and Matsen IV, 2018]. A drawback of this
setup is that the support of the conditional probability ta-
bles scales exponentially with the number of taxa [Zhang
and Matsen IV, 2019]. In subsequent work, the authors in-
troduce two Variational Bayesian Phylogenetic Inference
frameworks (VBPI and VBPI-NF) by using pre-computed
topologies to deﬁne the support of the conditional proba-
bility tables for the approximation [Zhang and Matsen IV,
2019, Zhang, 2020]. In contrast, VCSMC does not restrict the
support of the tree topologies and instead leverages CSMC
to compute a lower bound.
2
BACKGROUND
Phylogenetic Trees.
We wish to infer a latent bifurcating
tree that describes the evolutionary relationships among a
set of observed molecular sequences. A phylogeny is de-
ﬁned by a tree topology τ and a set of branch lengths B.
A tree topology is deﬁned as a connected acyclic graph
(V, E) where V is a set of vertices and E is a set of edges.
Leaf nodes denote vertices of degree 1 and correspond to
observed taxa. Internal nodes designate vertices of degree
3 (one parent and two children) and represent unobserved
taxa (e.g. DNA bases of ancestral species). The root node
is of degree 2 (two children) and represents the common
evolutionary ancestor of all taxa.
For each edge e ∈E, we associate a branch length, denoted
b(e) ∈R>0, and B = {b(e)}e∈E. The branch length cap-
tures the intensity of the evolutionary changes between two
vertices. An ultrametric tree is one with constant evolution-
ary rate along all paths from v to its descendants. Nonclock
trees are general trees that do not require ultrametric as-
sumptions. In this work we focus on phylogenetic inference
methods for nonclock trees as these are most pertinent to
biologists.
Bayesian Phylogenetic Inference.
Let the matrix Y =
{Y1, · · · , YS} ∈ΩNxS denote the observed molecular se-
quences with characters in Ωof length S over N species.
Bayesian inference requires specifying the prior density
and likelihood function over tree topology τ, branch length
set B and generative model parameters θ to write the joint
posterior,
Pθ(B, τ|Y) = Pθ(Y|τ, B)Pθ(τ, B)
Pθ(Y)
.
(1)
The prior is uniform over topologies and a product of inde-
pendent exponential distributions over branch lengths with
rate λbl. The evolution of each site is modeled independently
using a continuous time Markov chain with rate matrix Q.
Let ζv,s denote the state of genome for species v at site s
and deﬁne the evolutionary model along branch b(v →v′):
Pθ(ζv′,s = j|ζv,s = i) = exp (b(e)Qi,j) .
(2)
The likelihood of a given phylogeny Pθ(Y|τ, B)
=
SQ
i=1
Pθ(Yi|τ, B) can be evaluated in linear time using the
sum-product or Felsenstein’s pruning algorithm [Felsen-
stein, 1981] via the formula:
Pθ(Y|τ, B) :=
S
Y
i=1
X
ai
η(ai
ρ)
Y
(u,v)∈E(τ)
exp
 −bu,vQaiu,ai
v

,

RESAMPLE
A
B
C
D
A
B
C
D
A
B
C
D
PROPOSE
A
B
C
D
A
B
C
D
A
B
C
D
A
B
C
D
A
B
C
D
A
B
C
D
WEIGHTING
A
B
C
D
A
B
C
D
A
B
C
D
Figure 1: Overview of the CSMC framework. K partial states are maintained as forests over the set of taxa. Each iteration of
Algorithm 2 involves three steps: (1) resample partial states according to their importance weights, (2) propose an extension
of each partial state to a new partial state by connecting two trees in the forest, and (3) compute weights for each new
partial state by using Felsenstein’s pruning algorithm. In the example above, three partial states are shown over four taxa
A, B, C, D.
where ρ is the root node, ai
u is the assigned character of
node u, E(τ) represents the set of edges in τ and η is the
prior or stationary distribution of the Markov chain. The
normalization constant Pθ(Y) requires marginalizing the
(2N −3)!! distinct topologies which is intractable [Semple
and Steel, 2003].
Computational Challenges.
We distinguish the two com-
putational tasks required for phylogenetic inference. First,
inference involves computing the normalization constant
Pθ(Y) by marginalizing the (2N −3)!! distinct topologies:
Pθ(Y) =
X
τ∈T
Z
pθ(Y|τ, B)pθ(τ, B)dB .
(3)
A common approach used for approximating Eq. 3 is to
sample tree topologies τ and branch lengths B via Monte
Carlo methods, such as CSMC, given that θ is known.
Second, learning (optimization) refers to ﬁnding the set of
parameters θ = (Q, {λi}|E|
i=1 ∈Θ) that maximize the data
log-likelihood obtained by marginalizing Eq. 3:
θ⋆= arg max
Q,{λi}|E|
i=1
log Pθ(Y) .
(4)
Sampling algorithms can also be used by assigning a prior
to θ, then performing a local search for the parameters via
MCMC methods, given that the data likelihood is available.
Variational Inference.
VI is a technique for approximat-
ing the posterior Pθ(B, τ|Y) when marginalization of la-
tent variables is not analytically feasible. By introducing a
tractable distribution Qφ(B, τ|Y) it is possible to form a
lower bound to the log-likelihood:
log Pθ(Y) ≥LELBO(θ, φ, Y) := E
Q
"
log Pθ(Y, B, τ)
Qφ(B, τ|Y)
#
.
(5)
Auto Encoding Variational Bayes [Kingma and Welling,
2013] (AEVB) simultaneously trains Qφ(B, τ|Y) and
Pθ(Y, B, τ). The expectation in Eq. 5 is approximated by
averaging Monte Carlo samples from Qφ(B, τ|Y) which
are reparameterized by evaluating a deterministic func-
tion of a φ-independent random variable. When the ratio
Pθ(Y, B, τ)/Qφ(B, τ|Y) is concentrated around its mean,
Jensen’s inequality produces a tighter bound.
Deriving a tractable approximation Qφ(B, τ|Y) for the phy-
logenetic tree model can be challenging so we turn to CSMC.
Combinatorial Sequential Monte Carlo.
CSMC is de-
signed for inference in phylogenetic tree models. CSMC
approximates a sequence of target distributions ¯πr on in-
creasing probability spaces such that the ﬁnal target coin-
cides with Eq. 1 [Wang et al., 2015]. The (unnormalized)
target distribution π and its normalization constant ∥π∥cor-
responding to the numerator and denominator in Eq. 1 are
approximated by sequential importance resampling in R
steps. Unlike standard SMC methods, the target π is deﬁned
on a combinatorial set (the space of tree topologies) and
the continuous branch lengths. This requires deﬁning an
intermediate object referred to as a partial state.
Deﬁnition 1 (Partial State). A partial state of rank r denoted
sr = {(ti, Xi)} is a collection of rooted trees that satisﬁes

S0
S1
S2
S3
O0
O1
O2
O3
(a) State space model representation of CSMC. Latent
variables consist of internal nodes and branch lengths
with Markovian dependencies whereas observations are
the recorded molecular sequences.
PAB
A
B
C
D
π(s) =
Q
(ti,Xi)∈s
πYi(xi)(ti)
= PAB · η × C · η × D · η
(b) Overview of the natural forest extension of the target
measure. Partial state s1,1 = {PAB, C, D} is deﬁned as a
forest over leaves {A, B, C, D}.
Figure 2: Illustration of the sequence of probability spaces along with the natural forest extension used by CSMC. Partial
states are deﬁned as forests over leaves. The partial state s1
1 = {PAB, C, D} corresponding to Fig. 2 (a) is illustrated in Fig.
2 (b) as a set of disjoint components over the four taxa {A, B, C, D}. Felsenstein’s pruning algorithm is used to obtain a
marginal likelihood estimate for each tree by passing messages from left and right child nodes and taking the inner product
with η, the stationary state of Q. Each distinct likelihood is then multiplied to assign probability π(sr,k) to partial state sr,k.
the following three conditions: (i) the set of partial states
of different ranks are disjoint, ∀r ̸= s, Sr ∩Ss = ∅; (ii)
the set of partial states of smallest rank has a single element
S0 = {⊥}; and (ii) the set of partial states at the ﬁnal rank
R corresponds to the target space X.
CSMC operates by sampling K partial states (or particles)
{sk
r}K
k=1 ∈Sr at each rank r which are used to form a
distribution,
bπr = ∥bπr−1∥1
K
K
X
k=1
wk
rδskr (s)
∀s ∈S,
(6)
where δs is the Dirac measure and wk
r are the importance
weights. Resampling ensures that particles remain in areas
of high probability mass. Each resampled state s
ak
r−1
r−1 , where
ak
r−1 ∈{1, . . . , K} is the resampled index, of rank r −1
is then extended to a state of rank r, sk
r, by simulating
from a proposal distribution q(·|s
ak
r−1
r−1 ) : S →[0, 1]. The
importance weights are computed as follows:
wk
r = w(s
ak
r−1
r−1 , sk
r) =
π(sk
r)
π(s
ak
r−1
r−1 )
· ν−(s
ak
r−1
r−1 )
q(skr|s
ak
r−1
r−1 )
,
(7)
where ν−is a probability density over S correcting an over-
counting problem [Wang et al., 2015]. An overview of the
procedure is given in Fig. 1. An unbiased estimate for the
marginal likelihood can be constructed from the weights
which converges in L2 norm,
b
ZCSMC := ∥bπR∥=
R
Y
r=1
 
1
K
K
X
k=1
wk
r
!
→∥π∥.
(8)
VCSMC melds VI and CSMC to approximate the posterior
as well as the model parameters.
3
VARIATIONAL COMBINATORIAL
SEQUENTIAL MONTE CARLO
Variational Objective.
The idea of VCSMC is to simul-
taneously learn the model parameters and proposal param-
eters by maximizing a lower bound to the data marginal
log-likelihood, using CSMC as an unbiased estimator of the
marginal likelihood.
We begin by deﬁning a structured approximate posterior
which factorizes over rank events. Each state (or rank event)
sr is speciﬁed by a topology, a forest of trees, and their corre-
sponding set of branch lengths. The proposal qφ,ψ(sk
r|s
ak
r−1
r−1 )
is the probability of state sk
r given the resampled state at
the previous rank s
ak
r−1
r−1 . Subscripts φ and ψ denote dis-
crete and continuous proposal parameters respectively. The
approximate posterior is (written explicitly in Eq. 15):
Qφ,ψ
 s1:K
1:R , a1:K
1:R−1
 :=
(9)
K
Y
k=1
qφ,ψ(sk
1) ×
R
Y
r=2
K
Y
k=1


w
ak
r−1
r−1
PK
l=1 wl
r−1
· qφ,ψ

sk
r|s
ak
r−1
r−1

.
At the ﬁnal rank event R = N −1, an unbiased approxi-
mation to the likelihood is formed by averaging over im-
portance weights, which, in turn represent the sample phy-
logenies that are constructed iteratively. A multi-sample
variational objective is formed via the lower bound:
LCSMC := E
Q
h
log b
ZCSMC
i
, b
ZCSMC :=
R
Y
r=1
 
1
K
K
X
k=1
wk
r
!
.
The presence of the discrete distribution over partial states
presents a challenge for variational reparameterization. Un-
like standard variational SMC methods [Naesseth et al.,
2018], states are formed by sampling from a large combi-
natorial set. We take two approaches, the ﬁrst is to drop

discrete terms from the gradient estimates. The second is
to reparameterize these terms as Gumbel-Softmax random
variables forming a differentiable approximation through a
convex relaxation over the simplex. Continuous proposal
terms are drawn by evaluating a deterministic function of a
ψ-independent random variable.
Implementation Details.
Constructing the objective
LCSMC is done iteratively in three steps. The proposal
procedure, q(sr|sr−1), requires selecting two trees to co-
alesce by sampling without replacement. This is accom-
plished by deﬁning Gumbel-Softmax random variables.
The uniform log-probability for each index is perturbed by
adding independent Gumbel distributed noise, after which
the largest two elements are returned. For example let
U ∼UNIFORM(0, 1), we then form G = γ −log(−log U)
so that G can be reparameterized as G′ = G + γ. The RE-
SAMPLE procedure can also be reparameterized similarly
by deﬁning Gumbel-Softmax random variables.
Extending the Target Measure.
The WEIGHTING step
requires some care. In order to compute importance weights,
the likelihood of a partial state must be evaluated using
Felsenstein’s pruning algorithm, however the likelihood of
Eq. 3 and the probability measure π are deﬁned on the target
space of trees SR, and not the larger sample space of partial
states Sr<R, which are deﬁned on forests (trees disjoint
from each other). The pruning algorithm yields a maximum
likelihood estimate for an evolutionary tree, but partial states
are deﬁned as collections of disjoint trees or leaf nodes. One
extension of the target measure π into a measure on S is to
treat all elements of the jump chain as trees [Wang et al.,
2015]. The contribution of each of the trees to the likelihood
is multiplied by taking the inner product of each distribution
over characters with η.
Deﬁnition 2 (Natural Forest Extension). The natural forest
assigns an extension of π to forests by taking a product over
the trees in the forest:
π(s) =
Y
(ti,Xi)
πYi(xi)(ti) .
(10)
The natural forest extension (NFE) has the advantage of
passing information from the non-coalescing elements to
the local weight update. Fig. 2 provides an illustration of
the NFE applied to the state consisting of PA(A, B) and
non-coalescing singletons {C} and {D}.
4
NESTED COMBINATORIAL
SEQUENTIAL MONTE CARLO
A potential drawback of the CSMC method is that partial
states are sampled to coalesce uniformly, when many of the
resulting topologies correspond to areas of low probabil-
ity mass. It seems natural to incorporate information from
future iterations within the proposal distribution to subse-
quently guide the exploration of partial states. Adapting the
proposal requires marginalizing the intermediate target over
future topologies and branch lengths.
Locally Optimal Combinatorial SMC.
Choosing a good
proposal distribution is key for the effectiveness of SMC
methods. The locally optimal SMC [Doucet et al., 2000,
Naesseth et al., 2019b] chooses the proposal in such a way
that all particles have equal weights. This can signiﬁcantly
improve the performance over the standard proposal used
in CSMC. The locally optimal proposal based on the natural
forest extension is
q(sr|sr−1) ∝π(sr)ν−(sr−1)
π(sr−1)
.
(11)
This locally optimal proposal for the CSMC algorithm
is computationally intractable, it requires us to exactly
marginalize the branch lengths. We use the nested SMC
[Naesseth et al., 2015, 2019a] method to overcome this
problem.
Nested Combinatorial SMC.
We provide an overview of
Nested Combinatorial Sequential Monte Carlo before pre-
senting a detailed description in Algorithm 1 (we have anno-
tated the overview with steps from the algorithm). NCSMC
iterates over rank events (line 2) to perform a standard RE-
SAMPLE step also used in CSMC methods (line 4). For each
sample, NCSMC enumerates all
 N−r
2

possible one-step
ahead topologies and samples corresponding M sub-branch
lengths (line 7). We evaluate importance sub-weights or
potential functions for each of these sk,m
r
[i] sampled look-
ahead states (line 8). Then, we extend our ancestral partial
state s
ak
r−1
r−1 to the new partial state sk
r (line 11) by selecting
one of the topologies and a corresponding branch length
according to its weight. Finally, for each sample (line 12),
we compute its weight by averaging over all the potential
functions. An illustration of the procedure is given in Fig. 5
of the Appendix.
Variational Nested CSMC Objective.
The nested CSMC
method described in Algorithm 1 can also be used to con-
struct a variational objective:
LNCSMC := E
Q
h
log ˆZNCSMC
i
,
(12)
b
ZNCSMC :=
R
Y
r=1
 
1
K
K
X
k=1
wk
r
!
.
(13)
We refer to the resulting VI framework as VNCSMC.
Theoretical Justiﬁcation.
Nested CSMC is an SMC algo-
rithm on the extended space of all random variables gener-
ated by Algorithm 1. This means it keeps keeps the favorable
properties of CSMC, such as unbiasedness of the normaliza-
tion constant estimate and asymptotic consistency. The key

property that ensures this for NCSMC is proper weighting
[Naesseth et al., 2015, 2019a].
Deﬁnition 3 (Proper Weighting). We say that the random
pair (sr, wr) are properly weighted for the unnormalized
distribution π(sr)ν−(sr−1)
π(sr−1)
if wr ≥0 almost surely, and for
all measurable functions h,
E[wrh(sr)] =
Z
h(sr)π(sr)ν−(sr−1)
π(sr−1)
dsr.
(14)
We formalize the result for NCSMC, Algorithm 1, in Theo-
rem 1. We say that nested CSMC is an exact approximation
[Naesseth et al., 2019b] of CSMC with the locally optimal
proposal.
Theorem 1. The particles sk
r and weights wk
r generated by
Algorithm 1 are properly weighted for π(sr)ν−(sr−1)
π(sr−1)
.
Proof.
E[wk
rh(sk
r)] = E

wk
r · h(sk,J
r
[I])

= E


L
X
i=1
M
X
j=1
wk
r
wk,j
r [i]
P
l
P
m wk,m
r
[l]
h(sk,j
r [i])


=
1
ML
L
X
i=1
M
X
j=1
E

wk,j
r [i] · h(sk,j
r [i])

= E

wk,j
r [i] · h(sk,j
r [i])

=
Z
h(sr)π(sr)ν−(sr−1)
π(sr−1)
dsr
5
EXPERIMENTS
We evaluate VCSMC and VNCSMC on three tasks: (i) a stan-
dard dataset of primate mitochondrial DNA, (ii) on the com-
plete 36 kilobase genomes of 17 species of Betacoronavirus,
and (iii) on 7 large taxa benchmarks datasets ranging from
27 to 64 taxa. For experiments using the same initialization
of likelihood and prior, the proposed methods converge to
higher log-marginal likelihood values than existing methods.
Additionally, they can be more easily adopted to a variety of
models, with arbitrary settings of parameters θ. VCSMC and
VNCSMC also scale well with the number of sites in input
sequences.
Primate Mitochondrial DNA.
We evaluate VCSMC on a
benchmark dataset of nucleotide sequences of homologous
fragments of primate mitochondrial DNA [Hayasaka et al.,
1988]. The dataset consists of 12 taxa {S0, · · · , S11} over
898 sites admitting 13,749,310,575 distinct tree topologies.
The set of taxa includes ﬁve species of homonoids, four
species of old world monkeys, one species of new world
Algorithm 1 Nested Combinatorial Sequential Monte Carlo
Input: Y
=
{Y1, · · · , YM}
∈
ΩNxM, θ
=
(Q, {λi}|E|
i=1)
1: Initialization. ∀k, sk
0 ←⊥, wk
0 ←1/K.
2: for r = 1 to R = N −1 do
3:
for k = 1 to K do
4:
RESAMPLE P(ak
r−1 = i) =
wi
r−1
PK
l=1 wl
r−1
5:
for i = 1 to L =
 N−r
2

do
6:
for m = 1 to M do
7:
FORM LOOK-AHEAD PARTIAL STATE
sk,m
r
[i] ∼q(·|s
ak
r−1
r−1 )
8:
COMPUTE POTENTIALS
wk,m
r
[i] = π(sk,m
r
[i])
π(s
ak
r−1
r−1 )
·
ν−(s
ak
r−1
r−1 )
q(sk,m
r
[i]|s
ak
r−1
r−1 )
9:
end for
10:
end for
11:
EXTEND PARTIAL STATE
sk
r = sk,J
r
[I],
P(I = i, J = j) =
wk,j
r [i]
PL
l=1
PM
m=1 wk,m
r
[i]
12:
COMPUTE WEIGHTS
wk
r =
1
ML
L
X
i=1
M
X
m=1
wk,m
r
[i]
13:
end for
14: end for
Output: s1:K
R
, w1:K
1:R
monkey and two species of prosimians. VCSMC is run with
K = {4, 8, 16, 32, 64, 128} particles, whereas VNCSMC is
run with K = {4, 8, 16, 32, 64, 128} and M = 1 particles,
each averaged over 5 random seeds. Fig. 3 shows higher
values of K produce larger log-marginal likelihood values
(tighter ELBO values) with lower stochastic gradient noise.
VCSMC (blue) with K ≥16 outperforms probabilistic path
Hamiltonain Monte Carlo (ppHMC) shown (green trace)
for comparison. VNCSMC (red) requires fewer epochs than
VCSMC to converge and produces tighter ELBO / larger log-
marginal likelihood values with lower stochastic gadient
noise. VNCSMC with (K, M) = (4, 1) (top left) outper-
forms both ppHMC and VCSMC with K = 128 (bottom
right).
Fig. 4 provides a single maximum likelihood phylogeny
selected from a run of VNCSMC using K, M = (256, 1)
particles, along with a phylogeny from Mr Bayes on the

Figure 3: Log likelihood values for VCSMC (blue) with K = {4, 8, 16, 32, 64, 128} samples and VNCSMC (red) with
K = {4, 8, 16, 32, 64, 128} and M = 1 samples on the primates data averaged across 5 random seeds. Higher values of K
produce tighter ELBO / larger log likelihood values with lower stochastic gadient noise. VCSMC with K ≥16 outperforms
probabilistic path Hamiltonain Monte Carlo (ppHMC) which is shown (green trace) for comparison. VNCSMC requires
fewer epochs than VCSMC to converge and produces tighter ELBO / larger log likelihood values with lower stochastic
gadient noise. VNCSMC with (K, M) = (4, 1) (top left) outperforms both ppHMC and VCSMC with K = 128 (bottom
right).
Samiri Sciure (S11)
M Sylvanus (S10)
M Fascicularis (S9)
M Mulatta (S8)
Macaca Fuscata (S7)
Hylobates (S6)
Pongo (S5)
Gorilla (S4)
Pan (S3)
Homo Sapiens (S2)
Lemur Catta (S1)
Tarsius Syrichta (S0)
(a) MrBayes phylogeny
Samiri Sciure (S11)
M Sylvanus (S10)
M Fascicularis (S9)
M Mulatta (S8)
Macaca Fuscata (S7)
Hylobates (S6)
Pongo (S5)
Gorilla (S4)
Pan (S3)
Homo Sapiens (S2)
Lemur Catta (S1)
Tarsius Syrichta (S0)
(b) VNCSMC phylogeny
Figure 4: MrBayes vs VNCSMC phylogeny on the primate mitochondrial DNA dataset. The data consists of 12 taxa
{S0, · · · , S11} over 898 sites on the genome. The maximum likelihood topology returned by VNCSMC corresponds to that
of Mr Bayes. The bottom clade partitions monkeys, while the central and top clades partition hominids and prosimians.
MrBayes uses 20,000 iterations of MCMC in contrast to VNCSMC which uses 256 samples.
same dataset. The topology returned by VNCSMC corre-
sponds to that of Mr Bayes. The bottom clade partitions
monkeys, while central and top clades partition hominids
and prosimians respectively.
Betacoronavirus Data.
The evolutionary origin of SARS-
COV-II and the development of its genetic variants is an
open question of paramount importance in both virology
and in public health. At a high level, the species SARS-COV-

Table 1: Log-marginal likelihood estimates of different variational inference techniques across 7 benchmark datasets for
Bayesian phylogenetic inference. Results reported by VBPI [Zhang and Matsen IV, 2019] and VBPI-NF [Zhang, 2020] were
obtained by (i) using 10 replicates of 10,000 maximum likelihood bootstrap trees [Minh et al., 2013] to obtain topologies
deﬁning the support of the conditional probability tables and (ii) performing 400,000 parameter updates. VCSMC does not
require bootstrapped or MCMC tree topologies in order to learn parameters. We give VCSMC 2048 particles and evaluate the
likelihood after 100 parameter updates. Results for VCSMC and VCSMC (JC) are averaged over three random seeds. VCSMC
consistently explores higher probability phylogenies than VBPI and VBPI-NF without the use of preloaded topologies.
Marginal Likelihood
Dataset
Reference
(#Taxa, #Sites)
VBPI
VBPI-NF
VCSMC
VCSMC (JC)
DS1
Hedges et al. [1990]
(27, 1949)
-7108.4
-7108.39
-5929.8
-6906.65
DS2
Garey et al. [1996]
(29, 2520)
-26367.7
-26367.7
-14160.4
-23252.6
DS3
Yang and Yoder [2003]
(36, 1812)
-33735.1
-33735.09
-17460.5
-33177.8
DS4
Henk et al. [2003]
(41, 1137)
-13329.9
-13329.92
-11251.9
-12232.6
DS5
Lakner et al. [2008]
(50, 378)
-8214.5
-8214.51
-5797.1
-7921.2
DS6
Zhang and Blackwell [2001]
(50, 1133)
-6724.3
-6724.25
-5216.5
-6575.51
DS7
Rossman et al. [2001]
(64, 1008)
-8650.6
-8650.42
-5847.5
-6781.54
II belongs to the genera of betacoronaviruses, which include
OC43 and HKU1 (which cause the common cold) of lineage
A, SARS-COV and SARS-COV-II (which causes the disease
COVID-19) of lineage B, and MERS-COV-II (which causes
the disease MERS) of lineage C [Boni et al., 2020]. The ex-
act origin of SARS-COV-II however is unknown; different
approaches to phylogenetic inference produce statistically
incompatible results [Pipes et al., 2020]. Coronaviruses have
relatively large genomes ranging from 26-32 kilobases, and
performing analyses on the full genomes is often a challenge.
Recently, it has been argued that viral recombination in be-
tacoronaviruses often encompasses the receptor binding do-
main (RBD) of the spike gene [Patiño-Galindo et al., 2020].
This process is thought to have produced a recombination
event at least 11 years ago in an ancestor of SARS-COV-
II [Patiño-Galindo et al., 2020]. We use VNCSMC to analyze
the complete genomes for 17 species of Betacoronavirus
downloaded from the NCBI Viral Genomes Resource [Bris-
ter et al., 2014]. Multiple Sequence Aligmnent using Clustal
was performed and each nucleotide was one-hot encoded
as a vector, producing input sequences with 36,889 sites.
Fig. 6 of the Appendix provides the maximum likelihood
phylogeny from a VNCSMC run using K, M = (256, 1) par-
ticles. The result shows that the phylogeny partitions four
lineages into clades: Embecovirus (lineage A), Sarbecovirus
(lineage B including SARS-COV and SARS-COV-II), Mer-
becovirus (lineage C), and Nobecovirus (lineage D) [Cotten
et al., 2013, Woo et al., 2010, Geldenhuys et al., 2018].
Large Taxa Benchmarks.
We evaluate VCSMC on 7
large benchmark datasets for Bayesian phylogenetic infer-
ence [Hedges et al., 1990, Garey et al., 1996, Yang and
Yoder, 2003, Henk et al., 2003, Lakner et al., 2008, Zhang
and Blackwell, 2001, Rossman et al., 2001]. Each dataset
ranges from 27 to 64 eukaryote species with 378 to 2520
sites. Table 1 provides the marginal likelihood values for
various methods. VBPI [Zhang and Matsen IV, 2019] and
VBPI-NF [Zhang, 2020] both learn a simpliﬁed model of
molecular evolution referred to as Jukes-Cantor (JC), which
ﬁxes the transition matrix [Jukes and Cantor, 1969]. For
a fair comparison, we report VCSMC (JC) results in addi-
tion to the harder task of also learning the transition matrix.
Results reported by VBPI and VBPI-NF were obtained by
(i) using 10 replicates of 10,000 maximum likelihood boot-
strap trees [Minh et al., 2013] to obtain topologies deﬁning
the support of the conditional probability tables and (ii)
performing 400,000 parameter updates. Without bootstrap
trees, the conditional probability tables for VBPI and VBPI-
NF scale exponentially with the number of taxa [Zhang and
Matsen IV, 2019]. VCSMC does not restrict the support of
the tree topologies and instead leverages CSMC to compute
a lower bound. We give VCSMC 2048 particles and evaluate
the likelihood after 100 parameter updates, averaged over
three random seeds. Both VCSMC and VCSMC (JC) explore
higher probability spaces than VBPI and VBPI-NF.
Empirical Running Times.
We report the empirical run-
ning times of VCSMC and VNCSMC on the primates dataset
and highlight the results in Table 2 of the Appendix. Experi-
ments were performed on a 2.4GHz 8-core Intel i9 processor
Macbook pro with 64 GB memory and no GPU utilization.
We note that alternative methods are designed for solving
simpler problems in both inference and learning making
any runtime comparisons indirect. For instance, VBPI and
VBPI-NF use precomputed topologies, while ppHMC sup-
port Jukes-Cantor models. VCSMC runs on the primates
dataset at an average speed of 19.34 iterations per second
(it/s) with K = 4 and an average of 2.25 seconds per itera-
tion (s/it) with K = 256. VNCSMC runs in 3.89 seconds per
iteration with K = 4 and 21.77 seconds per iteration with
K = 256. These numbers can be improved by leveraging
GPU utilization. In contrast, MrBayes in Figure 4 takes 12

seconds with 20,000 iterations, and the minimum it would
take to converge on the primates dataset is about 2,000 it-
erations, implying a runtime of ∼1.2 seconds. We observe
that the ﬁrst epoch of VCSMC and VNCSMC is equivalent to
the inference task and runs faster than MrBayes.
6
DISCUSSION
Computational Complexity.
The locally optimal pro-
posal in NCSMC requires additional computational com-
plexity to marginalize the intermediate target densities in
exchange for a more informed exploration of partial states.
NCSMC costs O(KN 3M) in contrast to O(KNM) for
CSMC. Empirically, NCSMC with small K, M produces a
more accurate posterior approximation than CSMC with
larger K (see Fig. 3). NCSMC can accommodate a large
number of particles with low memory overhead, however
maintaining the computational graph and applying the sum-
product algorithm symbolically for each of the K samples
and M sub-samples, along with evaluating gradients for
each of these terms places a practical restriction on the
values of K, M and N used with VNCSMC without GPU
utilization. Alternative implementations of Bayesian phylo-
genetic inference are computationally intensive. While the
process of enumerating the
 N−r
2

topologies across rank
events cannot be avoided, we ﬁnd that choosing K as large
as possible and M = 1 is a useful heuristic for producing
good results. For example, K, M = (256, 1) can be run
on the betacoronavirus data with N = 17 and 36,889 sites
(see Fig. 6) without GPU utilization. One advantage of VC-
SMC and VNCSMC is the ability to use minibatch iteration
to speed up training. The experiments were trained using
ADAM with a batch size B = S/4. Opportunities exist to
parallelize VCSMC and leverage GPU optimization which
we expect would produce signiﬁcant performance gains on
DS1-DS7 as K increases.
Effective Sample Size.
One pertinent theoretical question
concerns the relationship between the effective sample size
(ESS), the number of samples K and the number of taxa
N. The ESS measures the diversity among samples and
is deﬁned as ESS = (P
i wi)2/ P
i w2
i where wi are the
unnormalized weights. We report ESS values on the primates
data in Table 2 of the Appendix. While an ESS close to
K is not sufﬁcient to ensure a good approximation, it is
a necessary condition. We ﬁnd near optimal ESS values
across all choices of K for both VCSMC and VNCSMC. The
theoretical foundations for developing lower bounds on ESS
for a given value of K have only been developed in the
context of online inference, where the posterior distribution
is updated as new sequence data becomes available [Dinh
et al., 2017b]. We leave theoretical questions of ESS and
online extensions of VCSMC for future work.
Contacts Outside of Phylogenetic Inference.
VCSMC
and VNCSMC may be adapted to a wide class of problems
outside of phylogenetic inference. In principle, any gener-
ative model of data simulated by a Markov tree can be ﬁt
using VCSMC and VNCSMC. Coalescent models for heirar-
chical Bayesian clustering and diffusion trees [Teh et al.,
2009, Boyles and Welling, 2012, Knowles and Ghahramani,
2011] are examples of alternative probabilistic approaches
involving distributions over latent bifurcating trees that may
be suited for VCSMC. The nested CSMC algorithm may also
be used to simulate approximate solutions to other combina-
torial optimization tasks. Combinatorial Monte Carlo meth-
ods are used to approximate the number of self-avoiding ran-
dom walks on the lattice [Sokal, 1996, Shirai and Kikuchi,
2013]. Another point of contact is the reconstruction of jet
structures in particle physics for the analysis of data from
experiments at the Large Hadron Collider at CERN. Jet
reconstruction algorithms are typically based on greedy ap-
proximation methods [Cacciari et al., 2008, Dokshitzer et al.,
1997], however VCSMC and VNCSMC may be particularly
suited for these tasks. The aforementioned extensions are
open directions for further development.
7
CONCLUSION
We have introduced VCSMC, a powerful framework for
model inference and parameter learning in Bayesian phy-
logenetics. VCSMC is the ﬁrst method to establish the use
of variational sequential search to learn distributions over
intricate combinatorial structures, uncovering connections
between VI and SMC. We have introduced NCSMC, and
proved that it provides an exact approximation to the locally
optimal proposal for CSMC. We have used NCSMC to deﬁne
a second objective, VNCSMC which yields tighter lower
bounds than VCSMC. VCSMC and VNCSMC outperform
existing methods on a range of tasks. A TensorFlow imple-
mentation of both VCSMC and VNCSMC is available online
at https://github.com/amoretti86/phylo.
Acknowledgements
We thank the reviewers for their helpful feedback. We ac-
knowledge funding from NIH/NCI grant U54CA209997
and two NIH shared instrumentation grants, S10 OD012351
and S10 OD021764. This work is also supported by ONR
N00014-17-1-2131, ONR N00014-15-1-2209, DARPA SD2
FA8750-18-C-0130, Amazon, Sloan Foundation, and the
Simons Foundation.
References
Christophe Andrieu, Arnaud Doucet, and Roman Holen-
stein. Particle Markov chain Monte Carlo methods. Jour-

nal of the Royal Statistical Society: Series B (Statistical
Methodology), 72(3):269–342, 2010.
Maciej F. Boni, Philippe Lemey, Xiaowei Jiang, Tommy
Tsan Yuk Lam, Blair W. Perry, Todd A. Castoe, Andrew
Rambaut, and David L. Robertson. Evolutionary origins
of the SARS-CoV-2 sarbecovirus lineage responsible for
the COVID-19 pandemic. Nature Microbiology, 5(11):
1408–1417, November 2020. ISSN 2058-5276.
Alexandre Bouchard-Côté, Sriram Sankararaman, and
Michael Jordan. Phylogenetic inference via sequential
Monte Carlo. Systematic biology, 61:579–93, 01 2012.
Levi Boyles and Max Welling. The time-marginalized co-
alescent prior for hierarchical clustering. In F. Pereira,
C. J. C. Burges, L. Bottou, and K. Q. Weinberger, edi-
tors, Advances in Neural Information Processing Systems,
volume 25. Curran Associates, Inc., 2012.
J. Rodney Brister, Danso Ako-adjei, Yiming Bao, and Olga
Blinkova. NCBI Viral Genomes Resource. Nucleic Acids
Research, 43(D1):D571–D577, 11 2014. ISSN 0305-
1048.
Matteo Cacciari, Gavin P Salam, and Gregory Soyez. The
anti-ktjet clustering algorithm. Journal of High Energy
Physics, 2008(04):063–063, Apr 2008. ISSN 1029-8479.
doi: 10.1088/1126-6708/2008/04/063.
M. Cotten, T. T. Lam, S. J. Watson, A. L. Palser, V. Petrova,
P. Grant, O. G. Pybus, A. Rambaut, Y. Guan, D. Pillay,
P. Kellam, and E. Nastouli. Full-genome deep sequenc-
ing and phylogenetic analysis of novel human betacoro-
navirus. Emerging infectious diseases, 19(5):736–42,
2013.
Amrit Dhar, Duncan K. Ralph, Vladimir N. Minin, and Fred-
erick A. Matsen. A bayesian phylogenetic hidden Markov
model for B cell receptor sequence analysis. PLOS Com-
putational Biology, 16(8):e1008030, Aug 2020. ISSN
1553-7358.
Vu Dinh, Arman Bilge, Cheng Zhang, and Frederick A.
Matsen, IV. Probabilistic path Hamiltonian Monte Carlo.
volume 70 of Proceedings of Machine Learning Research,
pages 1009–1018, International Convention Centre, Syd-
ney, Australia, 06–11 Aug 2017a. PMLR.
Vu Dinh, Aaron E Darling, and Frederick A Matsen IV. On-
line Bayesian Phylogenetic Inference: Theoretical Foun-
dations via Sequential Monte Carlo. Systematic Biology,
67(3):503–517, 12 2017b. ISSN 1063-5157.
Yu.L Dokshitzer, G.D Leder, S Moretti, and B.R Webber.
Better jet clustering algorithms. Journal of High Energy
Physics, 1997(08):001–001, Aug 1997. ISSN 1029-8479.
doi: 10.1088/1126-6708/1997/08/001.
Arnaud Doucet, Simon Godsill, and Christophe Andrieu. On
sequential Monte Carlo sampling methods for Bayesian
ﬁltering. Statistics and computing, 10(3):197–208, 2000.
J Felsenstein. Evolutionary trees from DNA sequences: a
maximum likelihood approach. Journal of Molecular
Evolution, 17(6):368–376, 1981.
JR Garey, TJ Near, MR Nonnemacher, and SA Nadler.
Molecular evidence for Acanthocephala as a subtaxon of
Rotifera. Journal of molecular evolution, 43(3):287–292,
1996.
Marike Geldenhuys, Marinda Mortlock, Jacqueline Weyer,
Oliver Bezuidt, Ernest C. J. Seamark, Teresa Kearney,
Cheryl Gleasner, Tracy H. Erkkila, Helen Cui, and Wanda
Markotter. A metagenomic viral discovery approach iden-
tiﬁes potential zoonotic and novel mammalian viruses in
Neoromicia bats within South Africa. PLOS ONE, 13(3):
1–27, 03 2018.
K Hayasaka, T Gojobori, and S Horai. Molecular phylogeny
and evolution of primate mitochondrial DNA. Molecular
Biology and Evolution, 5(6):626–644, 11 1988. ISSN
0737-4038.
S Hedges, K Moberg, and LR Maxson. Tetrapod phylogeny
inferred from 18S and 28S ribosomal RNA sequences
and a review of the evidence for amniote relationships.
Molecular Biology and Evolution, 7(6):607–633, 11 1990.
ISSN 0737-4038.
Daniel A. Henk, Alex Weir, and Meredith Blackwell.
Laboulbeniopsis termitarius, an ectoparasite of ter-
mites newly recognized as a member of the Laboulbe-
niomycetes. Mycologia, 95(4):561–564, 2003.
Daniel Hernandez, Antonio Moretti, Ziqiang Wei, S. Saxena,
John Cunningham, and Liam Paninski. A novel varia-
tional family for hidden nonlinear markov models. CoRR,
abs/1811.02459, 2018a.
Daniel Hernandez, Antonio Khalil Moretti, Ziqiang Wei,
Shreya Saxena, John Cunningham, and Liam Paninski.
Nonlinear evolution via spatially-dependent linear dy-
namics for electrophysiology and calcium data. Neurons,
Behavior, Data analysis and Theory, 2018b.
John P. Huelsenbeck and Fredrik Ronquist. MRBAYES:
Bayesian inference of phylogenetic trees . Bioinformatics,
17(8):754–755, 08 2001. ISSN 1367-4803.
Thomas H. Jukes and Charles R. Cantor. Chapter 24 - evolu-
tion of protein molecules. In H.N. MUNRO, editor, Mam-
malian Protein Metabolism, pages 21–132. Academic
Press, 1969. ISBN 978-1-4832-3211-9.
Diederik P Kingma and Max Welling. Auto-encoding varia-
tional Bayes, 2013.

David Knowles and Zoubin Ghahramani. Nonparametric
Bayesian sparse factor models with application to gene
expression modeling. The Annals of Applied Statistics, 5
(2B):1534 – 1552, 2011. doi: 10.1214/10-AOAS435.
Clemens Lakner, Paul van der Mark, John P. Huelsenbeck,
Bret Larget, and Fredrik Ronquist. Efﬁciency of Markov
Chain Monte Carlo Tree Proposals in Bayesian Phyloge-
netics. Systematic Biology, 57(1):86–103, 02 2008. ISSN
1063-5157.
Dieterich Lawson, George Tucker, Christian A Naesseth,
Chris Maddison, Ryan P Adams, and Yee Whye Teh.
Twisted variational sequential Monte Carlo. Third work-
shop on Bayesian Deep Learning (NeurIPS), 2018.
Tuan Anh Le, Maximilian Igl, Tom Rainforth, Tom Jin, and
Frank Wood. Auto-encoding sequential Monte Carlo. In
International Conference on Learning Representations,
2018.
Chris J. Maddison, Dieterich Lawson, George Tucker, Nico-
las Heess, Mohammad Norouzi, Andriy Mnih, Arnaud
Doucet, and Yee Whye Teh. Filtering variational objec-
tives. 2017.
Bui Quang Minh, Minh Anh Thi Nguyen, and Arndt von
Haeseler. Ultrafast Approximation for Phylogenetic Boot-
strap. Molecular Biology and Evolution, 30(5):1188–
1195, 02 2013. ISSN 0737-4038.
Antonio Moretti, Liyi Zhang, and Itsik Pe’er. Variational
combinatorial sequential monte carlo for bayesian phylo-
genetic inference. Machine Learning in Computational
Biology, Nov 2020a.
Antonio K Moretti, Zizhao Wang, Luhuan Wu, and Itsik
Pe’er. Smoothing nonlinear variational objectives with
sequential Monte Carlo. ICLR Workshops, 2019a.
Antonio Khalil Moretti. Variational Bayesian Methods for
Inferring Spatial Statistics and Nonlinear Dynamics. PhD
thesis, 2021.
Antonio Khalil Moretti, Zizhao Wang, Luhuan Wu, Iddo
Drori, and Itsik Pe’er. Particle smoothing variational
objectives. CoRR, abs/1909.09734, 2019b.
Antonio Khalil Moretti, Zizhao Wang, Luhuan Wu, Iddo
Drori, and Itsik Pe’er. Variational objectives for Marko-
vian dynamics with backward simulation. European Con-
ference on Artiﬁcial Intelligence, 2020b.
C. A. Naesseth, F. Lindsten, and D. Blei.
Markovian
score climbing: Variational inference with KL(p||q). In
Advances in Neural Information Processing Systems
(NeurIPS) 33, Vancouver, Canada, 2020.
Christian Naesseth, Scott Linderman, Rajesh Ranganath,
and David Blei. Variational sequential Monte Carlo. vol-
ume 84 of Proceedings of Machine Learning Research,
pages 968–977, Playa Blanca, Lanzarote, Canary Islands,
09–11 Apr 2018. PMLR.
Christian A. Naesseth, Fredrik Lindsten, and Thomas B.
Schön.
Nested sequential Monte Carlo methods.
In
International Conference on Machine Learning (ICML),
pages 1292–1301, 2015.
Christian A. Naesseth, Fredrik Lindsten, and Thomas B.
Schön. High-dimensional ﬁltering using nested sequential
Monte Carlo. IEEE Transactions on Signal Processing,
67(16):4177–4188, 2019a.
Christian A. Naesseth, Fredrik Lindsten, and Thomas B.
Schön. Elements of sequential Monte Carlo. Founda-
tions and Trends® in Machine Learning, 12(3):307–392,
2019b.
Juan Ángel Patiño-Galindo, Ioan Filip, Mohammed
AlQuraishi, and Raul Rabadan.
Recombination and
lineage-speciﬁc mutations led to the emergence of SARS-
CoV-2. 2020.
Lenore Pipes, Hongru Wang, John P Huelsenbeck, and Ras-
mus Nielsen. Assessing Uncertainty in the Rooting of
the SARS-CoV-2 Phylogeny. Molecular Biology and
Evolution, 12 2020. ISSN 0737-4038.
Amy Y. Rossman, John M. McKemy, Rebecca A. Pardo-
Schultheiss, and Hans-Josef Schroers. Molecular stud-
ies of the Bionectriaceae using large subunit rDNA se-
quences. Mycologia, 93(1):100–110, 2001.
Charles Semple and Mike Steel. Phylogenetics. 2003.
Nobu C. Shirai and Macoto Kikuchi.
How to estimate
the number of self-avoiding walks over 10100? use ran-
dom walks. Interdisciplinary Information Sciences, 19(1):
79–83, 2013. ISSN 1347-6157. doi: 10.4036/iis.2013.79.
Alan D. Sokal. Monte carlo methods for the self-avoiding
walk. Nuclear Physics B - Proceedings Supplements, 47
(1):172–179, 1996. ISSN 0920-5632.
Yee Whye Teh, Hal Daumé III au2, and Daniel Roy.
Bayesian agglomerative clustering with coalescents,
2009.
Liangliang Wang, Alexandre Bouchard-Côté, and Arnaud
Doucet. Bayesian phylogenetic inference using a combi-
natorial sequential Monte Carlo method. Journal of the
American Statistical Association, 01 2015.
Shijia Wang and Liangliang Wang. Particle Gibbs sampling
for Bayesian phylogenetic inference, 2020.

P. C. Woo, Y. Huang, S. K. Lau, and K. Y. Yuen. Coronavirus
genomics and bioinformatics analysis. Viruses, 2(8):1804–
1820, 2010.
Ziheng Yang and Anne Yoder.
Comparison of Likeli-
hood and Bayesian Methods for Estimating Divergence
Times Using Multiple Gene Loci and Calibration Points,
with Application to a Radiation of Cute-Looking Mouse
Lemur Species. Systematic Biology, 52(5):705–716, 10
2003. ISSN 1063-5157.
Cheng Zhang. Improved variational Bayesian phylogenetic
inference with normalizing ﬂows, 2020.
Cheng Zhang and Frederick A Matsen IV. Generalizing tree
probability estimation via Bayesian networks. In S. Ben-
gio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-
Bianchi, and R. Garnett, editors, Advances in Neural
Information Processing Systems 31, pages 1444–1453.
Curran Associates, Inc., 2018.
Cheng Zhang and Frederick A Matsen IV.
Variational
Bayesian phylogenetic inference. In International Con-
ference on Learning Representations, 2019.
Ning Zhang and Meredith Blackwell. Molecular phylogeny
of dogwood anthracnose fungus (Discula destructiva) and
the Diaporthales. Mycologia, 93(2):355–365, 2001.

APPENDIX
Algorithm 2 Combinatorial Sequential Monte Carlo
Input: Y = {Y1, · · · , YM} ∈ΩNxM, θ = (Q, {λi}|E|
i=1)
1: Initialization. ∀k, sk
0 ←⊥, wk
0 ←1/K.
2: for r = 0 to R = N −1 do
3:
for k = 1 to K do
4:
RESAMPLE
P(ak
r−1 = i) =
wi
r−1
PK
l=1 wl
r−1
5:
EXTEND PARTIAL STATE
sk
r ∼q(·|s
ak
r−1
r−1 )
6:
COMPUTE WEIGHTS
wk
r = w(s
ak
r−1
r−1 , sk
r) =
π(sk
r)
π(s
ak
r−1
r−1 )
· ν−(s
ak
r−1
r−1 )
q(skr|s
ak
r−1
r−1 )
7:
end for
8: end for
9: Output: s1:K
R
, w1:K
1:R
The proposal distribution for CSMC and approximate posterior for VCSMC can be written explicitly as follows:
Qφ,ψ
 T 1:K
1:R , B1:K
1:R , a1:K
1:R−1
 :=
 K
Y
k=1
qφ(T k
1 ) · qψ(Bk
1)
!
·
R
Y
r=2
K
Y
k=1


w
ak
r−1
r−1
PK
l=1 wl
r−1
· qφ

T k
r |T
ak
r−1
r−1

· qψ

Bk
r |B
ak
r−1
r−1 , T
ak
r−1
r−1

.
(15)
State sk
r = (T k
r , Bk
r ) is sampled by proposing forest T k
r ∼qφ(·|T
ak
r−1
r−1 ) and branch lengths Bk
r ∼qψ(·|B
ak
r−1
r−1 , T
ak
r−1
r−1 ) from
UNIFORM and EXPONENTIAL distributions corresponding to Eq. 1 with φ and ψ denoting discrete and continuous terms.
ENUMERATE TOPOLOGIES
A
B
C
D
SUBSAMPLE BRANCH LENGTHS
A
B
C
D
A
B
C
D
B
A
C
D
A
B
C
D
A
B
C
D
A
B
C
D
COMPUTE POTENTIALS
A
B
C
D
A
B
C
D
A
B
C
D
Figure 5: Overview of the NCSMC framework. The enumerated topologies for state {A, B, {C, D}} are (top):
{A, {B, {C, D}}}, (center): {{A, B}, {C, D}} and (bottom): {B, {A, {C, D}}} . M = 1 sub-branch lengths are sampled
for each edge. Sub-weights or potentials are computed (right). A single candidate is sampled to form the new partial state.

BOVINE CORONAVIRUS: NC_003045.1
HUMAN CORONAVIRUS OC43: NC_006213.1
RABBIT CORONAVIRUS HKU14: NC_017083.1
BETACORONAVIRUS HKU24: NC_026011.1
RAT CORONAVIRUS PARKER: NC_012936.1
HUMAN CORONAVIRUS HKU-11: NC_006577.2
SARS-COV : NC_004718.3
SARS-COV-II: NC_045512.2
BAT CORONAVIRUS BM48-31/BGR/2008: NC_014470.1
BAT HP-BETACORONAVIRUS/ZHEJIANG2013: NC_025217.1
ROUSETTUS BAT CORONAVIRUS HKU9-1: NC_009021.1
ROUSETTUS BAT CORONAVIRUS ISOLATE GCCDC1 356: NC_030886.1
TYLONYCTERIS BAT CORONAVIRUS HKU4-1: NC_009019.1
PIPISTRELLUS BAT CORONAVIRUS HKU5-1: NC_009020.1
MERS CORONAVIRUS EMC/2012: NC_019843.3
BETACORONAVIRUS ENGLAND 1: NC_038294.1
BETACORONAVIRUS ERINACEUS/VMC/DEU/2012: NC_039207.1
Figure 6: Overview of the betacoronavirus results. The data consists of 17 species of betacoronavirus across 36,889 sites.
VNCSMC is run using K, M = (256, 1). A single nonclock phylogeny is chosen based on maximum likelihood and
displayed. Colors denote species from the four varying viral lineages: Embecovirus (orange lineage A); Nobecovirus (blue
lineage D); Sarbecovirus (red lineage B including SARS-COV and SARS-COV-II); Merbecovirus (grey lineage C) and
Hibecovirus (black not classiﬁed into the four lineages) are each partitioned in clades.

Figure 7: VNCSMC on the primates data with K, M = (128, 1). The full distribution of log likelihood values for all particles
across epochs is plotted in black. The average likelihood across samples is plotted in blue.
A
B
C
D
A
B
C
D
A
B
D
C
Figure 8: Overview of the dual representation of a partial state. The partial state s1
1 = {PAB, C, D} for four taxa correspond-
ing to Fig. 2 is illustrated using its dual representation D(s). The dual state D(s) ⊆T corresponds to the three complete
tree topologies. (left): {{A, B}, {C, D}} (center): {{A, B}, {A, B, C}} and (right): {{A, B}, {A, B, D}}.
VCSMC
VNCSMC
K
s/it
s/mit
time (minutes)
ESS
s/it
s/mit
time (minutes)
ESS
4
5.17e-2
1.31e-2
0:22
3.98
4.01
1.17
6:32
3.99
8
5.58e-2
1.42e-2
0:28
7.96
4.27
1.24
7:09
7.88
16
3.11e-2
7.76e-2
0:30
15.79
4.83
1.53
8:15
15.62
32
5.78e-2
2.17e-1
0:49
31.72
5.98
1.59
10:17
31.00
64
9.80e-2
2.66e-1
1:23
62.92
8.33
2.09
14:33
62.59
128
1.35
3.48e-1
2:16
122.79
11.88
2.89
20:02
124.23
256
2.25
5.95e-1
3:52
252.02
21.77
4.98
36:51
252.43
Table 2: Empirical running times of VCSMC and VNCSMC. The Primates data consists of 12 taxa over 898 sites admitting
13,749,310,575 distinct tree topologies. Experiments were performed on a 2.4GHz 8-core intel i9 processor Macbook Pro
with 64 GB memory and no GPU utilization. We proﬁle using K = {4, 8, 16, 32, 64, 128} and M = 1. The left column
provides seconds per iteration (s/it), the left center column provides seconds per minibatch (s/mit), the center right column
provides total running time (minutes) across 100 epochs. The effective sample size is provided in the right columns.

Figure 9: VNCSMC on the 9-taxa subset of primates data with K, M = (128, 1). The full distribution of log likelihood
values for all VNCSMC particles across epochs is plotted in black. The average likelihood across samples is plotted in red.
Particle Gibbs [Wang and Wang, 2020] is run for 5000 iterations 10 times independently. The last 100 iterations for the
10 independent runs of Particle Gibbs are averaged and plotted in green. VNCSMC using 100 epochs outperforms Particle
Gibbs using 5000 iterations.

