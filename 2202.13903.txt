Bayesian Structure Learning with Generative Flow Networks
Tristan Deleu1
António Góis1
Chris Emezue2,*
Mansi Rankawat1
Simon Lacoste-Julien1,4
Stefan Bauer3,5
Yoshua Bengio1,4,6
1Mila, Université de Montréal
2Technical University of Munich
3KTH Stockholm
4CIFAR AI Chair
5CIFAR Azrieli Global Scholar
6CIFAR Senior Fellow
Abstract
In Bayesian structure learning, we are interested
in inferring a distribution over the directed acyclic
graph (DAG) structure of Bayesian networks, from
data. Deﬁning such a distribution is very chal-
lenging, due to the combinatorially large sample
space, and approximations based on MCMC are
often required. Recently, a novel class of proba-
bilistic models, called Generative Flow Networks
(GFlowNets), have been introduced as a general
framework for generative modeling of discrete and
composite objects, such as graphs. In this work,
we propose to use a GFlowNet as an alternative
to MCMC for approximating the posterior distri-
bution over the structure of Bayesian networks,
given a dataset of observations. Generating a sam-
ple DAG from this approximate distribution is
viewed as a sequential decision problem, where
the graph is constructed one edge at a time, based
on learned transition probabilities. Through evalua-
tion on both simulated and real data, we show that
our approach, called DAG-GFlowNet, provides
an accurate approximation of the posterior over
DAGs, and it compares favorably against other
methods based on MCMC or variational inference.
1
INTRODUCTION
Bayesian networks (Pearl, 1988) are a popular framework of
choice for representing uncertainty about the world in mul-
tiple scientiﬁc domains, including medical diagnosis (Lau-
ritzen and Spiegelhalter, 1988; Heckerman and Nathwani,
1992), molecular biology (Friedman, 2004; Sebastiani et al.,
2005), and ecological modeling (Varis and Kuikka, 1999;
Marcot et al., 2006). For many applications, the structure
Correspondence to: Tristan Deleu <deleutri@mila.quebec>
*Work done during an internship at Mila
of the Bayesian network, represented as a directed acyclic
graph (DAG) and encoding the statistical dependencies be-
tween the variables of interest, is assumed to be known
based on knowledge from domain experts. However, when
this graph is unknown, we can learn the DAG structure of
the Bayesian network from data alone in order to discover
these statistical (or possibly causal) relationships. This may
form the basis of novel scientiﬁc theories.
Given a dataset of observations, most of the existing algo-
rithms for structure learning return a single DAG (or a single
equivalence class; Chickering, 2002), and in practice those
may lead to poorly calibrated predictions (Madigan et al.,
1994), especially in cases where data is limited. Instead of
learning a single graph candidate, we can view the problem
of structure learning from a Bayesian perspective and infer
the posterior over graphs P(G | D), given a dataset of ob-
servations D, to account for the epistemic uncertainty over
models. Except in limited settings (Koivisto, 2006; Meil˘a
and Jaakkola, 2006), characterizing a whole distribution
over DAGs remains intractable because of its combinato-
rially large sample space and the complex acyclicity con-
straint. Therefore, we must often resort to approximations
of this posterior distribution, e.g., based on MCMC or, more
recently, variational inference.
In this paper, we propose to use a novel class of probabilis-
tic models called Generative Flow Networks (GFlowNets;
Bengio et al., 2021a,b) to approximate this posterior distri-
bution over DAGs. A GFlowNet is a generative model over
discrete and composite objects that treats the generation of a
sample as a sequential decision problem. This makes it par-
ticularly appealing for modeling a distribution over graphs,
where sample graphs are constructed sequentially, starting
from the empty graph, by adding one edge at a time. In the
context of Bayesian structure learning, we also introduce
improvements over the original GFlowNet framework, in-
cluding a novel ﬂow-matching condition and corresponding
loss function, a hierarchical probabilistic model for forward
transitions, and using additional tools from the literature on
Reinforcement Learning (RL). We call our method DAG-
Accepted for the 38th Conference on Uncertainty in Artiﬁcial Intelligence (UAI 2022).
arXiv:2202.13903v2  [cs.LG]  28 Jun 2022

GFlowNet, to emphasize that the support of the distribution
induced by the GFlowNet is exactly the space of DAGs,
unlike some variational approaches that may sample cyclic
graphs (Annadani et al., 2021; Lorch et al., 2021). Compared
to MCMC, which operates through local moves in the sam-
ple space (here, adding or removing edges of a graph) and
is therefore subject to slow mixing (Friedman and Koller,
2003), DAG-GFlowNet yields a sampling process that sam-
ples iid. DAGs, each of them constructed from scratch.
We evaluate DAG-GFlowNet on various problems with sim-
ulated and real data, on both discrete and linear-Gaussian
Bayesian networks. Furthermore, we show that DAG-
GFlowNet can be applied on both observational and in-
terventional data, by modifying standard Bayesian scores
(Cooper and Yoo, 1999). On smaller graphs, we also show
that it is capable of learning an accurate approximation
of the exact posterior distribution. The code is available
online.1
2
RELATED WORK
Markov chain Monte Carlo
Methods based on MCMC
have been particularly popular in Bayesian structure learn-
ing to approximate the posterior distribution. Structure
MCMC (MC3; Madigan et al., 1995) simulates a Markov
chain in the space of DAGs, through local moves (e.g.
adding or removing an edge). Working directly with DAGs
leads to slow mixing though; to improve mixing, Friedman
and Koller (2003) proposed a sampler in the space of node
orders, that introduced a bias (Ellis and Wong, 2008). This
was further reﬁned by either modifying the underlying space
of the Markov chain (Kuipers and Moffa, 2017; Niinimäki
et al., 2016), or its local moves (Mansinghka et al., 2006;
Eaton and Murphy, 2007a; Kuipers et al., 2021). Recently,
Viinikka et al. (2020) incorporated many of these advances
into an efﬁcient MCMC sampler called Gadget.
Variational Inference
In the context of structure learn-
ing, applying the recent advances in approximate inference
based on gradient methods can be difﬁcult due to the dis-
crete nature of the problem (Lorch et al., 2021). Cundy et al.
(2021) decomposed the adjacency matrix of a DAG into a
triangular matrix and a permutation, and used a continuous
relaxation to parametrize a distribution over permutations.
Other methods (Annadani et al., 2021; Lorch et al., 2021) en-
code the acyclicity constraint into a soft prior P(G), based
on continuous characterizations of acyclicity (Zheng et al.,
2018). While the effect of this prior can be made arbitrar-
ily strong, this does not guarantee that the graphs sampled
from the resulting distribution are acyclic. By contrast, our
approach guarantees by construction that the support of the
posterior approximation is exactly the space of DAGs.
1https://github.com/tristandeleu/jax-dag-gflownet
Sequential decisions
In this work, we treat the construc-
tion of a sample graph from the posterior as a sequential
decision problem, starting from the empty graph and adding
one edge at a time. Li et al. (2018) use a similar process for
creating a generative model over graphs with a ﬁxed order-
ing over nodes. Similarly, although they do not consider a
distribution over graphs, Buesing et al. (2020) use a variant
of Monte Carlo Tree Search to approximate a distribution
over a pre-speciﬁed ordering of discrete random variables.
Our method, based on Generative Flow Networks, does not
make any assumption on the order in which the edges are
added, and multiple edge insertion sequences may lead to
the same DAG. Zhu et al. (2020) learn a single high-scoring
structure using RL; however, unlike our approach, the cre-
ation of this graph does not involve sequential decisions.
3
BACKGROUND
A Bayesian network is a probabilistic model over d random
variables {X1, . . . , Xd}, whose joint distribution factorizes
according to a DAG G as
P(X1, . . . , Xd) =
d
Y
k=1
P
 Xk | PaG(Xk)

,
(1)
where PaG(X) is the set of parents of node X in G. Sim-
ilarly, we denote by ChG(X) the children of X; when the
context is clear, we may drop the explicit dependency on G.
3.1
GENERATIVE FLOW NETWORKS
Originally introduced to encourage the discovery of di-
verse modes of an unnormalized distribution (Bengio et al.,
2021a), Generative Flow Networks (GFlowNets; Bengio
et al., 2021b) are a class of generative models over a dis-
crete and structured sample space X. The structure of a
GFlowNet is deﬁned by a DAG over some states s ∈S;
in general, the sample space over which we wish to deﬁne
a distribution is only a subset of the overall state space of
the GFlowNet: X ⊆S. Samples s ∈X are constructed
sequentially by following the edges of the DAG, starting
from a ﬁxed initial state s0. We also deﬁne a special absorb-
ing state sf, called the terminal state, indicating when the
sequential construction terminates; some of the states s ∈X
are connected to sf, and we call them complete states.2 For
example, Bengio et al. (2021a) use a GFlowNet to deﬁne a
distribution over molecules, where X would correspond to
the space of all (complete) molecules, which are constructed
piece by piece by attaching a new block to an atom in a pos-
sibly partially constructed molecule (i.e. a state in S\X).
2“Complete” here means that the state is a valid sample from
the distribution induced by the GFlowNet. This must not be con-
fused with a “complete graph”, where all the nodes are connected
to one another, when the states are DAGs (see Section 4).

Another example of a GFlowNet structure is given in Fig. 1,
illustrating the sequential process of constructing a DAG.
A GFlowNet is structurally equivalent to a Markov Deci-
sion Process (MDP; Puterman, 1994) with deterministic
dynamics, or a Markov Reward Process (Howard, 1971).
In addition to the DAG structure over states, every com-
plete state s ∈X is associated with a reward R(s) ≥0,
indicating a notion of “preference” for certain states. By
convention, R(s) = 0 for any incomplete state s ∈S\X.
The goal of a GFlowNet is to ﬁnd a ﬂow that satisﬁes, for
all states s′ ∈S, the following ﬂow-matching condition:
X
s∈Pa(s′)
Fθ(s →s′) −
X
s′′∈Ch(s′)
Fθ(s′ →s′′) = R(s′),
(2)
where Fθ(s →s′) ≥0 is a scalar representing the ﬂow from
state s to s′, typically parametrized by a neural network.
Putting it in words, the overall ﬂow going into s′ is equal to
the ﬂow going out of s′, plus some residual R(s′). To learn
the parameters θ of the ﬂow with SGD, we can turn (2) into
a regression problem, e.g. using a least squares objective
over sampled states.
If the conditions in (2) are satisﬁed for all states s′, a
GFlowNet induces a generative process to sample complete
states s ∈X with probability proportional to R(s). Starting
from the initial state s0, if we sample a complete trajec-
tory (s0, s1, . . . , sT , s, sf) using the transition probability
deﬁned as the normalized outgoing ﬂow
P(st+1 | st) ∝Fθ(st →st+1),
(3)
with the conventions sT +1 = s and sT +2 = sf, then s is
sampled with probability P(s) ∝R(s). Note that the linear
system in (2) is in general underdetermined, and therefore
it may admit many solutions Fθ(s →s′) that all induce
the same distribution ∝R(s). Unlike MCMC, each sample
s ∈X is constructed from scratch, starting at the initial
state s0, instead of traversing X from sample to sample.
Therefore, the underlying Markov process of the GFlowNet
does not have to be irreducible, which is typically necessary
in MCMC, but merely requires all the complete states to be
reachable from the initial state. Finally, although GFlowNets
borrow terminology from RL and control theory (e.g. MDPs,
rewards, trajectories), their objective is different from the
typical RL training objective: the latter seeks to maximize a
function of the rewards, while the goal of GFlowNets is to
model the whole distribution proportional to the rewards.
3.2
DETAILED-BALANCE CONDITION
Since the ﬂows are added together, one of the downsides of
the ﬂow-matching condition is that ﬂows tend to be orders
of magnitude larger the closer we are of the initial state
(Bengio et al., 2021a), making it challenging to parametrize
Fθ. Bengio et al. (2021b) proposed an alternative charac-
terization of GFlowNets inspired by the detailed-balance
equations from the literature on Markov chains (Grimmett
and Stirzaker, 2020). Instead of working with ﬂows, this
condition uses a parametrization of the forward transition
probability Pθ(st+1 | st) directly, together with a backward
transition probability PB(st | st+1) to enforce reversibility.
As opposed to Pθ(st+1 | st), which is a distribution over the
children of st, PB(st | st+1) is a distribution over the par-
ents of st+1 in the structure of the GFlowNet. If all the states
of the GFlowNet are complete (except the terminal state sf),
which will be the case here for generating DAGs, then we
show in Appendix B that we can write the detailed-balance
condition for all transitions s →s′ as follows:
R(s′)PB(s | s′)Pθ(sf | s) = R(s)Pθ(s′ | s)Pθ(sf | s′).
Similar to Section 3.1, ﬁnding Pθ and PB that satisfy this
condition for all the transitions s →s′ of the GFlowNet also
yields a sampling process of complete states s with proba-
bility proportional to R(s), based on the forward transition
probability Pθ(st+1 | st). Because this system of equations
also admits many solutions, similar to (2), we can set the
backward transition probability PB to some ﬁxed distribu-
tion (e.g. the uniform distribution over the parent states) to
reduce the search space, making Pθ the only quantity to
learn and, with enough capacity (to satisfy the constraints),
there is a unique solution Pθ (Bengio et al., 2021b).
To ﬁt the parameters θ of the forward transition probability,
we can minimize the following non-linear least squares
objective for all the transitions s →s′ of the GFlowNet,
called the detailed-balance loss:
L(θ) =
X
s→s′

log R(s′)PB(s | s′)Pθ(sf | s)
R(s)Pθ(s′ | s)Pθ(sf | s′)
2
.
(4)
Alternatively, we can minimize this loss in expectation, us-
ing a distribution π(s →s′) with full support over transi-
tions (i.e. for all transitions s →s′ in the GFlowNet, we
have π(s →s′) > 0; see Section 5.2).
4
GFLOWNET OVER DIRECTED
ACYCLIC GRAPHS
Our objective in this paper is to construct a distribution
over DAGs. This is a challenging problem in general, as the
space of DAGs is discrete and combinatorially large. We
propose to use a GFlowNet to model such a distribution; this
is particularly appropriate here since graphs are composite
objects, and the acyclicity constraint can be obtained by con-
straining the allowed actions in each state (as in Figure 1).
Note that the DAGs in this section and thereafter represent
the states of the GFlowNet, and they must not be confused
with the DAG structure of the GFlowNet itself.

A
B
C
G0
A
B
C
G1
A
B
C
G2
A
B
C
G3
A
B
C
G4
A
B
C
Invalid state
A
B
C
G5
Add A →B
Pθ(G1 | G0)
Add B →C
Pθ(G4 | G1)
. . .
. . .
. . .
. . .
. . .
R(G0)
R(G1)
R(G2)
R(G3)
R(G4)
R(G5)
Figure 1: Structure of a GFlowNet over DAGs. The states
of the GFlowNet correspond to DAGs, with the initial state
G0 being the completely disconnected graph. Each state G
is complete (i.e. connected to the terminal state sf, repre-
sented by blue arrows for brevity) and associated to a reward
R(G). Transitioning from one state to another corresponds
to adding an edge to the graph. The state in red is invalid
since the graph includes a cycle.
4.1
STRUCTURE OF THE GFLOWNET
We consider a GFlowNet where the states are DAGs over d
(labeled) nodes. Since the states of the GFlowNet are graphs,
we will use the notation G to denote a state, in favour of s as
in Section 3.1, except for the terminal state sf. A transition
G →G′ in this GFlowNet corresponds to adding an edge
to G to obtain the graph G′; in other words, the graphs are
constructed one edge at a time, starting from the initial state
G0, which is the fully disconnected graph over d nodes.
Since we assume that all the states G of the GFlowNet
are valid DAGs, they are all complete (i.e. connected to
the terminal state sf) with a corresponding reward R(G).
Figure 1 shows an illustration of the structure of such a
GFlowNet, where the states are DAGs over d = 3 nodes.
This application to graphs highlights the importance of the
DAG structure of the GFlowNet, since there can be multiple
paths leading to the same state: for any graph G with k
edges, there are k! possible paths from G0 leading to G,
because the edges of G may have been added in any order.
To guarantee the integrity of the GFlowNet, we have to en-
sure that adding a new edge to some state G also yields a
valid DAG, meaning that this edge (1) must not be already
present in G, and (2) must not introduce a cycle. Fortu-
nately, we can ﬁlter out invalid actions using some mask m
associated to the graph, built from the adjacency matrix of
G and the transitive closure of its transpose, and that can
be updated efﬁciently after the addition an edge (Giudici
and Castelo, 2003). A description of this update is given in
Appendix C for completeness.
4.2
FORWARD TRANSITION PROBABILITIES
Following Section 3.2, the GFlowNet may be parametrized
only by the forward transition probabilities Pθ(Gt+1 | Gt);
here, Gt+1 might be the terminal state sf by abuse of no-
tation. To make sure that the detailed-balance conditions
can be satisﬁed, we need to deﬁne these transition proba-
bilities using a sufﬁciently expressive function, such as a
neural network. We use a hierarchical model, where the
forward transition probabilities are deﬁned using two neu-
ral networks: (1) a network modeling the probability of
terminating Pθ(sf | G), and (2) another giving the proba-
bility Pθ(G′ | G, ¬sf) of transitioning to a new graph G′,
given that we do not terminate. The probability of taking a
transition G →G′ is then given by
Pθ(G′ | G) =
 1 −Pθ(sf | G)

Pθ(G′ | G, ¬sf).
(5)
In practice, as G′ is the result of adding an edge to the
DAG G, we can model Pθ(G′ | G, ¬sf) as a probability
distribution over the d2 possible edges one could add to G—
this includes self-loops, for simplicity, even though these
actions are guaranteed to be invalid. We can use the mask
m introduced in Section 4.1 to ﬁlter out actions that would
not lead to a valid DAG G′ and set Pθ(G′ | G, ¬sf) = 0
for any invalid action (as well as normalize Pθ accordingly).
4.3
PARAMETRIZATION WITH LINEAR
TRANSFORMERS
Beyond having enough capacity to satisfy as well as possible
the detailed-balance condition at all states, we choose to
parametrize the forward transition probabilities with neural
networks to beneﬁt from their capacity to generalize to
states not encountered during training. In practice, instead
of deﬁning two separate networks to parametrize Pθ(sf | G)
and Pθ(G′ | G, ¬sf), we use a single neural network with a
common backbone and two separate heads, to beneﬁt from
parameter sharing. The full architecture is given in Figure 2.
Our choice of neural network architecture is motivated by
multiple factors: we want an architecture (1) that is invariant
to the order of the inputs, since G is represented as a set of
edges, (2) that transforms a set of input edges into a set of
output probabilities for each edge to be added, in order to
deﬁne Pθ(G′ | G, ¬sf), and (3) whose parameters θ do not
scale too much with d. A natural option would be to use a
Transformer (Vaswani et al., 2017); however, because the
size of our inputs is d2, the self-attention layers would scale
as d4, and this would severely limit our ability to apply our
method to model a distribution over larger DAGs.
We opted for a Linear Transformer (Katharopoulos et al.,
2020) instead, which has the advantage to not suffer from
this quadratic scaling in the input size. This architecture

Norm
Linearized
Attention
Norm
MLP
A
B
C
G
m
×L
Embeddings
Linear Transformer
Linear
Transformer
MLP
Mask
Linear
Transformer
Pool
MLP
Softmax
σ
Pθ(G′ | G, ¬sf)
Pθ(sf | G)
Figure 2: Neural network architecture of the forward transition probabilities Pθ(Gt+1 | Gt). The input graph G is encoded
as a set of d2 possible edges (including self-loops). Each directed edge is embedded using the embeddings of its source
and target, with an additional vector indicating whether the edge is present in G. These embeddings are fed into a Linear
Transformer (Katharopoulos et al., 2020), with two separate output heads. The ﬁrst head (above) gives the probability to add
a new edge Pθ(G′ | G, ¬sf), using the mask m associated to G to ﬁlter out invalid actions; here, the only valid actions are
either adding B →C, or C →B. The second head (below) gives the probability to terminate the trajectory Pθ(sf | G).
relies on a linearized attention mechanism, deﬁned as
Q = xWQ
K = xWK
V = xWV
LinAttnk(x) =
PJ
j=1
 φ(Qk)⊤φ(Kj)

Vj
PJ
j=1 φ(Qk)⊤φ(Kj)
,
(6)
where x is the input of the linearized attention layer, φ(·)
is a non-linear feature map, J is the size of the input x (in
our case, J = d2), and Q, K, and V are linear transforma-
tions of x corresponding to the queries, keys, and values
respectively, as is standard with Transformers.
5
APPLICATION TO BAYESIAN
STRUCTURE LEARNING
We are given a dataset D = {x(1), . . . , x(N)} of N obser-
vations x(j), each consisting of d elements. We consider the
task of characterizing the posterior distribution P(G | D)
over Bayesian networks that model these observations. We
assume that the samples in D are iid. and fully-observed.
As an alternative to MCMC (Madigan et al., 1995) or vari-
ational inference (Lorch et al., 2021), we approximate the
posterior distribution over DAGs using a GFlowNet, as de-
scribed in the previous section. For any DAG G, we will
deﬁne its reward as the joint probability
R(G) = P(G)P(D | G),
(7)
where P(G) is a prior over DAGs (Eggeling et al., 2019),
and P(D | G) is the marginal likelihood. In Sec. 3.2, we saw
that if the detailed-balance conditions are satisﬁed for all the
states of the GFlowNet, then this yields a sampling process
with probability proportional to R(G). Therefore, by Bayes’
theorem, a GFlowNet with the speciﬁc reward function in (7)
approximates the posterior distribution P(G | D) ∝R(G).
We call our method DAG-GFlowNet.
5.1
MODULARITY & COMPUTATIONAL
EFFICIENCY
Following prior works on Bayesian structure learning, we
assume that both the priors over parameters P(φ | G) of the
Bayesian network (required to compute the marginal likeli-
hood) and over structures P(G) are modular (Heckerman
et al., 1995; Chickering et al., 1995). As a consequence the
reward R(G) is also modular, and its logarithm can be writ-
ten as a sum of local scores that only depend on individual
variables and their parents in G:
log R(G) =
d
X
j=1
LocalScore
 Xj | PaG(Xj)

.
(8)
Note that with our choice of reward, log R(G) corresponds
to the Bayesian score (Koller and Friedman, 2009). Exam-
ples of modular scores include the BDe score (Heckerman
et al., 1995) and the BGe score (Geiger and Heckerman,
1994; Kuipers et al., 2014). In order to ﬁt the parameters θ
of the GFlowNet, we will use the detailed-balance loss in
(4). We can observe that this loss function only involves the
difference in log-rewards log R(G′) −log R(G) between
two consecutive states, where G′ is the result of adding
some edge Xi →Xj to the DAG G. Using our assump-
tion of modularity, we can therefore compute this difference
efﬁciently, as the terms in (8) remain unchanged for j′ ̸= j:
log R(G′) −log R(G) = LocalScore
 Xj | PaG(Xj) ∪{Xi}

−LocalScore
 Xj | PaG(Xj)

.
(9)
This difference in local scores is sometimes called the delta
score, or the incremental value (Friedman and Koller, 2003),
and has been employed in the literature to improve the
efﬁciency of search algorithms (Chickering, 2002; Koller
and Friedman, 2009).

0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
Exact posterior
DAG-GFlowNet
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
Exact posterior
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
Exact posterior
(a) Edge features
(b) Path features
(c) Markov features
r = 0.9992
r = 0.9989
r = 0.9997
Figure 3: Comparison between the exact posterior distribution and the posterior approximation from DAG-GFlowNet, for
different structural features: (a) edge features Xi →Xj, (b) path features Xi ⇝Xj, (c) Markov features Xi ∼M Xj. Each
point corresponds to a feature computed for speciﬁc variables Xi and Xj in a graph over d = 5 nodes, either based on the
exact posterior (x-axis), or the posterior approximation found with the GFlowNet (y-axis). We repeated this experiment with
20 different (ground-truth) DAGs. The Pearson correlation coefﬁcient r is included in the bottom-right corner of each plot.
5.2
OFF-POLICY LEARNING
As the number of states in DAG-GFlowNet is super-
exponential in d, the number of nodes in each DAG G,
it would be impractical to minimize the detailed-balance
loss for all possible transitions G →G′. Alternatively, we
can minimize this loss in expectation using a distribution
π(G →G′) with full support over transitions:
L(θ) = Eπ
"
log R(G′)PB(G | G′)Pθ(sf | G)
R(G)Pθ(G′ | G)Pθ(sf | G′)
2#
.
(10)
This distribution π(G →G′) can be arbitrary; for example,
we can use Pθ(G′ | G) directly and learn it on-policy (Rum-
mery and Niranjan, 1994), as long as it assigns non-zero
probability to any next state G′.
Taking inspiration from Deep Q-learning (Mnih et al.,
2015), we instead learn Pθ using off-policy data. Transi-
tions G →G′ are collected based on Pθ(G′ | G), along
with their corresponding delta score (9), and they are stored
in a replay buffer. We can also sample some transitions
uniformly at random, with probability ε, to encourage ex-
ploration. To estimate L(θ) and update the parameters θ,
we can then sample a mini-batch of transitions randomly
from the replay buffer. Moreover, again inspired by Deep
Q-learning (Van Hasselt et al., 2018), we found it advanta-
geous to evaluate P¯θ(sf | G′) in (10) with a separate target
network—where the parameters ¯θ are updated periodically.
6
EXPERIMENTAL RESULTS
We compared DAG-GFlowNet against 3 broad classes
of Bayesian structure learning algorithms: MCMC, non-
parametric DAG Bootstrapping (Friedman et al., 1999),
and variational inference. We used Structure MCMC (MC3;
Madigan et al., 1995) and the recent Gadget (Viinikka et al.,
2020) samplers as two representative methods based on
MCMC. Following Lorch et al. (2021), we used two vari-
ants of Bootstrapping based on the score-based algorithm
GES (Bootstrap GES; Chickering, 2002), and the constraint-
based algorithm PC (Bootstrap PC; Spirtes et al., 2000), as
the internal structure learning routines. Finally for methods
based on variational inference, we used DiBS (Lorch et al.,
2021) and BCD Nets (Cundy et al., 2021). Throughout this
section, we used the BGe score for continuous data, and the
BDe score for discrete data, to compute log p(D | G).
6.1
COMPARISON WITH THE EXACT
POSTERIOR
In order to measure the quality of the posterior approxima-
tion returned by DAG-GFlowNet, we want to compare it
with the exact posterior distribution P(G | D). However,
the latter requires an exhaustive enumeration of all possible
DAGs, which is only feasible for graphs with no more than
5 nodes. Therefore, we sampled N = 100 datapoints from
a randomly generated (under an Erd˝os-Rényi model; Erd˝os
and Rényi, 1960) linear-Gaussian Bayesian network over
d = 5 variables. We used the BGe score to compute the
reward R(G) = P(G)P(D | G). The exact posterior dis-
tribution P(G | D) is obtained by enumerating all 29,281
possible DAGs over 5 nodes and computing their respective
rewards R(G) (normalized to sum to 1).
We evaluated the quality of the approximation based on the
probability of various structural features. For example, using
samples {G1, G2, . . . , Gn} from the posterior approxima-

MC3
Gadget
B-PC
B-GES
DiBS
BCD
GFN
20
40
60
80
100
E-SHD
MC3
Gadget
B-PC
B-GES
DiBS
BCD
GFN
0.4
0.6
0.8
1
AUROC
MC3
Gadget
B-PC
B-GES
DiBS
BCD
GFN
700
750
800
850
900
log P(G, D′ | D)
Figure 4: Bayesian structure learning of linear-Gaussian Bayesian networks with d = 20 nodes. Results for E-SHD &
AUROC are aggregated over 25 randomly generated datasets D, sampled from different (ground-truth) Bayesian networks.
Results for log P(G, D′ | D) are given for a single dataset D; the dashed line corresponds to the log-likelihood of the
ground truth graph G⋆. For E-SHD lower is better, and for AUROC and log P(G, D′ | D) higher is better. Labels: B-PC =
Bootstrap-PC, B-GES = Bootstrap-GES, BCD = BCD Nets, GFN = DAG-GFlowNet.
tion, the marginal probability of an edge feature Xi →Xj
can be estimated with
Pθ(Xi →Xj | D) ≈1
n
n
X
k=1
1(Xi →Xj ∈Gk),
(11)
where 1(·) is the indicator function. For the exact posterior,
we can obtain the posterior probability of the edge feature
by simply marginalizing over P(G | D). Similarly, we
compute (or estimate) the marginal probability of a path
feature Xi ⇝Xj, i.e. of a (directed) path existing from Xi
to Xj, and the probability of a Markov feature Xi ∼M Xj,
i.e. of Xi being in the Markov blanket of Xj (Friedman and
Koller, 2003). These features are computed for all variables
Xi and Xj in the Bayesian network.
In Figure 3, we compare the probabilities of these features
for both the exact posterior and the distribution induced by
DAG-GFlowNet, where we repeated the experiment above
with 20 different (ground-truth) Bayesian networks. We
observe that the probabilities of all structural features esti-
mated by the GFlowNet are strongly correlated with the ex-
act marginal probabilities. This shows that DAG-GFlowNet
is capable of learning a very accurate approximation of the
posterior distribution over graphs P(G | D).
6.2
SIMULATED DATA
We follow the experimental setup of Zheng et al. (2018) &
Lorch et al. (2021), and sample synthetic data from linear-
Gaussian Bayesian networks with randomly generated struc-
tures; details about this data generation process are given
in Appendix D.2. To show that DAG-GFlowNet can ef-
fectively approximate the posterior distribution over larger
graphs, we experimented with Bayesian networks of size
d = 20 (and d = 50, see Appendix D.2). Similar to Sec-
tion 6.1, the ground-truth graphs are sampled according to
an Erd˝os-Rényi model, with 2d edges in expectation—a
setting sometimes referred to as ER2 (Cundy et al., 2021).
For each experiment, we sampled a dataset D of N = 100
observations, and we used the BGe score to compute R(G).
Since we have access to the ground-truth graph G⋆that
generated D, we evaluate the performance of each algorithm
with the expected structural Hamming distance (E-SHD) to
G⋆over the posterior approximation; a detailed deﬁnition is
available in Appendix D.1. We also compute the area under
the ROC curve (AUROC; Husmeier, 2003) for the edge
features as deﬁned in (11), compared to the edges of G⋆.
Finally, we compute the joint log-likelihood log P(G, D′ |
D) on a held-out dataset D′; we chose this metric over
the log-predictive likelihood log P(D′ | D), as proposed
by Eaton and Murphy (2007a), to study the effect of the
posterior approximation P(G | D).
The results on graphs with d = 20 nodes are shown in Fig-
ure 4. We observe that both in terms of E-SHD & AUROC,
DAG-GFlowNet, is competitive against all other methods, in
particular those based on MCMC, and this does not come at
a cost in terms of its predictive capacity on held-out data. In
particular, we can see that the distribution induced by DAG-
GFlowNet yields a predictive log-likelihood concentrated
near the log-likelihood of the ground-truth DAG G⋆.
6.3
APPLICATION: FLOW CYTOMETRY DATA
We also evaluated DAG-GFlowNet on real-world ﬂow cy-
tometry data (Sachs et al., 2005) to learn protein signaling
pathways. The data consists of continuous measurements
of d = 11 phosphoproteins in individual T-cells. Out of all
the measurements, we selected the N = 853 observations
corresponding to the ﬁrst experimental condition of Sachs
et al. (2005) as our dataset D. Following prior work on
structure learning, we used the DAG inferred by Sachs et al.
(2005), containing d = 11 nodes and 17 edges, as our graph
of reference (ground-truth). However, it should be noted

that this “consensus graph” may not represent a realistic
and complete description of the system being modeled here
(Mooij et al., 2020). We standardized the data, and used the
BGe score to compute R(G).
Table 1: Learning protein signaling pathways from ﬂow
cytometry data (Sachs et al., 2005). All results include a 95%
conﬁdence interval estimated with bootstrap resampling.
E-# Edges
E-SHD
AUROC
MC3
10.96 ± 0.09
22.66 ± 0.11
0.508
Gadget
10.59 ± 0.09
21.77 ± 0.10
0.479
Bootstrap GES
11.11 ± 0.09
23.07 ± 0.11
0.548
Bootstrap PC
7.83 ± 0.04
20.65 ± 0.06
0.520
DiBS
12.62 ± 0.16
23.32 ± 0.14
0.518
BCD Nets
4.14 ± 0.09
18.14 ± 0.09
0.510
DAG-GFlowNet
11.25 ± 0.09
22.88 ± 0.10
0.541
In Table 1, we compare the expected SHD and the AUROC
obtained with DAG-GFlowNet and other approaches. While
BCD Nets and Bootstrap PC have a smaller E-SHD, suggest-
ing that the distribution is concentrated closer to the consen-
sus graph, in reality they tend to be more conservative and
sample graphs with fewer edges. Overall, DAG-GFlowNet
offers a good trade-off between performance (as measured
by the E-SHD and the AUROC), and getting a distribution
that assigns higher probability to DAGs with more edges.
We also observed that 1.50% of the graphs sampled with
DiBS contained a cycle.
Beyond these metrics, we would like to test if the advan-
tages of Bayesian structure learning are also reﬂected in the
distribution induced by DAG-GFlowNet. In particular, we
want to study (1) if this distribution covers multiple high-
scoring DAGs, instead of being peaked at a single most
likely graph, and (2) if the GFlowNet can sample a variety
of DAGs from the same Markov equivalence class (MEC),
showing the inherent uncertainty over equivalent graphs. In
Figure 5, we visualize the MECs of the graphs sampled with
DAG-GFlowNet, and two methods based on MCMC (MC3
and Gadget); other baselines were excluded for clarity. The
size of each point represents the number of unique DAGs in
the corresponding MEC. We observe that DAG-GFlowNet
largely follows the behavior of MCMC: the distribution
does not collapse to a single most-likely DAG, and covers
multiple MECs. Moreover, the GFlowNet is also capable
of sampling different equivalent DAGs (corresponding to
larger points), showing again that the distribution does not
collapse to a single representative of the MECs with higher
marginal probability. We also observe that the maximum a
posteriori MEC found by DAG-GFlowNet reaches a higher
score than the one found with Gadget, but a lower score
than MC3; as a point of reference, the score of the best MEC
obtained with GES (Chickering, 2002) is −10,716.12.
−10,760
−10,750
−10,740
−10,730
−10,720
0
0.01
0.02
0.03
0.04
BGe score
Marginal probability of MEC
MC3
Gadget
DAG-GFlowNet
Figure 5: Coverage of the posterior approximations learned
on ﬂow cytometry data (Sachs et al., 2005). Each point cor-
responds to a sampled Markov equivalence class, and its
size represents the number of different DAGs (in the equiva-
lence class) sampled from the posterior approximation. See
Figure 8 in Appendix D.3 for an additional comparison with
methods based on Variational Inference.
6.4
APPLICATION: INTERVENTIONAL DATA
In addition to the observational data we used in Section 6.3,
Sachs et al. (2005) also provided ﬂow cytometry data under
different experimental conditions, where the T-cells were
perturbed with some reagents; this effectively corresponds
to interventional data (Pearl, 2009). Although a molecular
intervention may be imperfect and affect multiple proteins
(Eaton and Murphy, 2007b), we assume here that these inter-
ventions are perfect, and the intervention targets are known.
We used a discretized dataset of N = 5,400 samples from
9 experimental conditions—of which 6 are interventions.
We modiﬁed the BDe score to handle this mixture of ob-
servational and interventional data (Cooper and Yoo, 1999).
Table 2: Combining discrete interventional and observa-
tional ﬂow cytometry data (Sachs et al., 2005). ⋆Result
reported in Eaton and Murphy (2007b).
E-# Edges
E-SHD
AUROC
Exact posterior⋆
—
—
0.816
MC3
25.97 ± 0.01
25.08 ± 0.02
0.665
DAG-GFlowNet
30.66 ± 0.04
27.77 ± 0.03
0.700
In Table 2, we compare with Eaton and Murphy (2007b),
which compute the AUROC of the exact posterior us-
ing dynamic programming, therefore working as an up-
per bound for what a posterior approximation can achieve.
They achieve this at the expense of computing only edge
marginals, without providing access to a distribution over
DAGs. We also use the modiﬁed BDe score with MC3,
which predicts sparser graphs with higher SHD than DAG-

GFlowNet, but lower AUROC. Note that this setup is differ-
ent from previous works which use continuous data instead
(Brouillard et al., 2020; Faria et al., 2022).
7
CONCLUSION
We have proposed a new method for Bayesian structure
learning, based on a novel class of probabilistic models
called GFlowNets, where the generation of a sample graph
is treated as a sequential decision problem. We introduced
a number of enhancements to the standard framework of
GFlowNets, speciﬁcally designed for approximating a dis-
tribution over DAGs. In cases where the data is limited
and measuring the epistemic uncertainty is critical, DAG-
GFlowNet offers an effective solution to approximate the
posterior distribution over DAGs P(G | D). However, we
also observed that in its current state, DAG-GFlowNet may
suffer from some limitations, notably as the size of the
dataset D increases; see Appendix A for a discussion.
While DAG-GFlownet operates on the space of DAGs di-
rectly, the structure of the GFlowNet may eventually be
adapted to work with alternative representations of statisti-
cal dependencies in Bayesian networks, such as essential
graphs for MECs (Chickering, 2002). Moreover, although
we have already shown that DAG-GFlowNet can approx-
imate the posterior using a mixture of observational and
interventional data, we will continue to study in future work
its applications to causal discovery, especially in the context
of learning the structure of models with latent variables.
Acknowledgements
We would like to thank Emmanuel Bengio, Paul Bertin,
and Valentin Thomas for the useful discussions about the
project, and Dinghuai Zhang, Kolya Malkin, and Xu Ji for
their valuable feedback on the paper. This research was
partially supported by the Canada CIFAR AI Chair Program
and by Samsung Electronics Co., Ldt. Simon Lacoste-Julien
is a CIFAR Associate Fellow in the Learning in Machines &
Brains program, Yoshua Bengio is a CIFAR Senior Fellow
and Stefan Bauer is a CIFAR Azrieli Global Scholar.
References
Yashas Annadani, Jonas Rothfuss, Alexandre Lacoste,
Nino Scherrer, Anirudh Goyal, Yoshua Bengio, and
Stefan Bauer. Variational Causal Networks: Approxi-
mate Bayesian Inference over Causal Structures. arXiv
preprint, 2021.
Emmanuel Bengio, Moksh Jain, Maksym Korablyov, Doina
Precup, and Yoshua Bengio. Flow Network based Gener-
ative Models for Non-Iterative Diverse Candidate Gener-
ation. Neural Information Processing Systems, 2021a.
Yoshua Bengio, Tristan Deleu, Edward J Hu, Salem Lahlou,
Mo Tiwari, and Emmanuel Bengio. GFlowNet Founda-
tions. arXiv preprint, 2021b.
Philippe Brouillard, Sébastien Lachapelle, Alexandre La-
coste, Simon Lacoste-Julien, and Alexandre Drouin. Dif-
ferentiable Causal Discovery from Interventional Data.
Advances in Neural Information Processing Systems,
2020.
Lars Buesing, Nicolas Heess, and Theophane Weber. Ap-
proximate Inference in Discrete Distributions with Monte
Carlo Tree Search and Value Functions.
In Interna-
tional Conference on Artiﬁcial Intelligence and Statistics.
PMLR, 2020.
David Chickering, Dan Geiger, and David Heckerman.
Learning Bayesian Networks: Search Methods and Ex-
perimental Results. In Proceedings of Fifth Conference
on Artiﬁcial Intelligence and Statistics, 1995.
David Maxwell Chickering. Optimal Structure Identiﬁca-
tion With Greedy Search. Journal of Machine Learning
Research, 2002.
Gregory F Cooper and Changwon Yoo. Causal Discovery
from a Mixture of Experimental and Observational Data.
Proceedings of the Fifteenth conference on Uncertainty
in Artiﬁcial Intelligence, 1999.
Chris Cundy, Aditya Grover, and Stefano Ermon. BCD Nets:
Scalable Variational Approaches for Bayesian Causal
Discovery. Advances in Neural Information Processing
Systems, 2021.
Daniel Eaton and Kevin Murphy. Bayesian structure learn-
ing using dynamic programming and MCMC. Conference
on Uncertainty in Artiﬁcial Intelligence, 2007a.
Daniel Eaton and Kevin Murphy. Belief net structure learn-
ing from uncertain interventions. 2007b.
Ralf Eggeling, Jussi Viinikka, Aleksis Vuoksenmaa, and
Mikko Koivisto.
On Structure Priors for Learning
Bayesian Networks. In The 22nd International Confer-
ence on Artiﬁcial Intelligence and Statistics, 2019.
Byron Ellis and Wing Hung Wong.
Learning Causal
Bayesian Network Structures from Experimental Data.
Journal of the American Statistical Association, 2008.
Paul Erd˝os and Alfréd Rényi. On the evolution of random
graphs. Publications of the Mathematical Institute of the
Hungarian Academy of Sciences, 1960.
Gonçalo Rui Alves Faria, Andre Martins, and Mario A. T.
Figueiredo. Differentiable Causal Discovery Under La-
tent Interventions. In First Conference on Causal Learn-
ing and Reasoning, 2022.

Nir Friedman. Inferring cellular networks using probabilis-
tic graphical models. Science, 2004.
Nir Friedman and Daphne Koller. Being Bayesian About
Network Structure. A Bayesian Approach to Structure
Discovery in Bayesian Networks. Machine Learning,
2003.
Nir Friedman, Moises Goldszmidt, and Abraham Wyner.
Data Analysis with Bayesian Networks: A Bootstrap Ap-
proach. Proceedings of the Fifteenth conference on Un-
certainty in Artiﬁcial Intelligence, 1999.
Dan Geiger and David Heckerman. Learning Gaussian
Networks. In Uncertainty in Artiﬁcial Intelligence. 1994.
Paolo Giudici and Robert Castelo. Improving Markov chain
Monte Carlo model search for data mining. Machine
learning, 2003.
Geoffrey Grimmett and David Stirzaker. Probability and
Random Processes. Oxford University Press, 2020.
David Heckerman, Dan Geiger, and David M Chickering.
Learning bayesian networks: The combination of knowl-
edge and statistical data. Machine learning, 1995.
ED Heckerman and NB Nathwani. Toward Normative Ex-
pert Systems: Part II Probability-Based Representations
for Efﬁcient Knowledge Acquisition and Inference. Meth-
ods of Information in Medicine, 1992.
Ronald A Howard. Dynamic Probabilistic Systems, Volume
II: Semi-Markov and Decision Processes. Wiley, 1971.
Dirk Husmeier. Sensitivity and speciﬁcity of inferring ge-
netic regulatory interactions from microarray experiments
with dynamic Bayesian networks. Bioinformatics, 2003.
Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas,
and François Fleuret. Transformers are RNNs: Fast Au-
toregressive Transformers with Linear Attention. In In-
ternational Conference on Machine Learning, 2020.
Mikko Koivisto. Advances in Exact Bayesian Structure
Discovery in Bayesian Networks. Proceedings of the
Twenty-Second Conference on Uncertainty in Artiﬁcial
Intelligence, 2006.
Daphne Koller and Nir Friedman. Probabilistic Graphical
Models: Principles and Techniques. MIT press, 2009.
Jack Kuipers and Giusi Moffa. Partition MCMC for In-
ference on Acyclic Digraphs. Journal of the American
Statistical Association, 2017.
Jack Kuipers, Giusi Moffa, and David Heckerman. Adden-
dum on the scoring of gaussian directed acyclic graphical
models. The Annals of Statistics, 2014.
Jack Kuipers, Polina Suter, and Giusi Moffa. Efﬁcient Sam-
pling and Structure Learning of Bayesian Networks. Jour-
nal of Computational and Graphical Statistics, 2021.
Steffen L Lauritzen and David J Spiegelhalter. Local Com-
putations with Probabilities on Graphical Structures and
their Application to Expert Systems. Journal of the Royal
Statistical Society: Series B (Methodological), 1988.
Yujia Li, Oriol Vinyals, Chris Dyer, Razvan Pascanu, and
Peter Battaglia. Learning Deep Generative Models of
Graphs. Proceedings of the 35th International Conference
on Machine Learning, 2018.
Lars Lorch, Jonas Rothfuss, Bernhard Schölkopf, and An-
dreas Krause. DiBS: Differentiable Bayesian Structure
Learning. Advances in Neural Information Processing
Systems, 2021.
David Madigan, Jonathan Gavrin, and Adrian E Raftery. En-
hancing the Predictive Performance of Bayesian Graphi-
cal Models. 1994.
David Madigan, Jeremy York, and Denis Allard. Bayesian
Graphical Models for Discrete Data. International Statis-
tical Review, 1995.
Vikash Mansinghka, Charles Kemp, Thomas Grifﬁths, and
Joshua Tenenbaum. Structured Priors for Structure Learn-
ing. Conference on Uncertainty in Artiﬁcial Intelligence,
2006.
Bruce G Marcot, J Douglas Steventon, Glenn D Sutherland,
and Robert K McCann. Guidelines for developing and
updating Bayesian belief networks applied to ecological
modeling and conservation. Canadian Journal of Forest
Research, 2006.
Marina Meil˘a and Tommi Jaakkola. Tractable Bayesian
learning of tree belief networks. Statistics and Computing,
2006.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, An-
drei A Rusu, Joel Veness, Marc G Bellemare, Alex
Graves, Martin Riedmiller, Andreas K Fidjeland, Georg
Ostrovski, et al. Human-level control through deep rein-
forcement learning. Nature, 2015.
Joris M Mooij, Sara Magliacane, and Tom Claassen. Joint
Causal Inference from Multiple Contexts. Journal of
Machine Learning Research, 2020.
Teppo Niinimäki, Pekka Parviainen, and Mikko Koivisto.
Structure Discovery in Bayesian Networks by Sampling
Partial Orders. The Journal of Machine Learning Re-
search, 2016.
Judea Pearl. Probabilistic Reasoning in Intelligent Systems.
Morgan Kaufmann, 1988.

Judea Pearl. Causality: Models, Reasoning, and Inference.
Cambridge University Press, 2009.
Martin L Puterman. Markov Decision Processes: Discrete
Stochastic Dynamic Programming. John Wiley & Sons,
1994.
Gavin A Rummery and Mahesan Niranjan.
On-line Q-
learning using Connectionist Systems. 1994.
Karen Sachs, Omar Perez, Dana Pe’er, Douglas A Lauf-
fenburger, and Garry P Nolan. Causal protein-signaling
networks derived from multiparameter single-cell data.
Science, 2005.
Paola Sebastiani, Marco F Ramoni, Vikki Nolan, Clinton T
Baldwin, and Martin H Steinberg. Genetic dissection and
prognostic modeling of overt stroke in sickle cell anemia.
Nature genetics, 2005.
Peter Spirtes, Clark N Glymour, Richard Scheines, and
David Heckerman. Causation, Prediction, and Search.
MIT press, 2000.
Hado Van Hasselt, Yotam Doron, Florian Strub, Matteo
Hessel, Nicolas Sonnerat, and Joseph Modayil. Deep
Reinforcement Learning and the Deadly Triad. arXiv
preprint, 2018.
Olli Varis and Sakari Kuikka. Learning Bayesian decision
analysis by doing: lessons from environmental and natural
resources management. Ecological Modelling, 1999.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-
reit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and
Illia Polosukhin. Attention Is All You Need. Advances in
Neural Information Processing Systems, 2017.
Jussi Viinikka, Antti Hyttinen, Johan Pensar, and Mikko
Koivisto. Towards Scalable Bayesian Learning of Causal
DAGs. Advances in Neural Information Processing Sys-
tems, 2020.
Xun Zheng, Bryon Aragam, Pradeep Ravikumar, and Eric P.
Xing. DAGs with NO TEARS: Continuous Optimization
for Structure Learning. In Advances in Neural Informa-
tion Processing Systems, 2018.
Shengyu Zhu, Ignavier Ng, and Zhitang Chen. Causal Dis-
covery with Reinforcement Learning. International Con-
ference on Learning Representations, 2020.

Bayesian Structure Learning with Generative Flow Networks
(Supplementary material)
Tristan Deleu1
António Góis1
Chris Emezue2
Mansi Rankawat1
Simon Lacoste-Julien1,4
Stefan Bauer3,5
Yoshua Bengio1,4,6
1Mila, Université de Montréal
2Technical University of Munich
3KTH Stockholm
4CIFAR AI Chair
5CIFAR Azrieli Global Scholar
6CIFAR Senior Fellow
A
LIMITATIONS OF DAG-GFLOWNET
Although we have shown in the main paper that DAG-GFlowNet is capable of learning an accurate approximation of the
posterior distribution P(G | D) when the size of the dataset D is moderate (a situation where the beneﬁts of a Bayesian
treatment of structure learning are larger), we observed that as the size of the dataset increases, ﬁtting the detailed-balance
loss in (10) was more challenging. This can be explained by the fact that with a larger amount of data, the posterior
distribution becomes very peaky (Koller and Friedman, 2009). As a consequence, in this situation, the delta-score in (9),
which is required to calculate the loss, can take a wide range of values: adding an edge to a graph can drastically increase or
decrease its score. In turn, the neural network parametrizing Pθ(Gt+1 | Gt) needs to compensate for these large ﬂuctuations,
making it harder to train.
Unfortunately, some of the standard techniques used in Machine Learning to tackle this issue, such as normalization of
the inputs, cannot be applied here. Normalizing the delta-score is equivalent to normalizing the rewards R(G) and R(G′)
themselves, and as a consequence it would change the distribution that is being approximated: instead of approximating the
posterior distribution P(G | D), we would approximate a distribution P(G | D)τ under some temperature τ. Solutions to
this problem include a schedule of temperature, similar to simulated annealing, or a reparametrization of Pθ(Gt+1 | Gt) to
better handle large ﬂuctuations of delta-scores; this exploration is left as future work.
B
DETAILED-BALANCE CONDITION WITH ALL COMPLETE STATES
In this section, we will prove a special case of the detailed-balance condition introduced by Bengio et al. (2021b) applied to
the case where all the states of the GFlowNet are complete (except the terminal state sf). To simplify the presentation, we
will follow the notations of Bengio et al. (2021b), and denote the forward transition probability by PF (st+1 | st)—instead
of Pθ(st+1 | st) in the main paper. Recall that the detailed-balance condition (Bengio et al., 2021b, Def. 17) is given by
F(st)PF (st+1 | st) = F(st+1)PB(st | st+1).
(B.1)
In the case where all the states are complete, we also know that (Bengio et al., 2021b, Def. 16)
PF (sf | st) :=
F(st →sf)
P
s′∈Ch(st) F(st →s′) = R(st)
F(st)
⇔
F(st) =
R(st)
PF (sf | st),
where F(s →s′) represents the ﬂow from state s to s′, as described in Section 3.1, F(s) is the total ﬂow through state s,
and we used Proposition 4 & Equation 34 of Bengio et al. (2021b) to introduce F(st) and R(st) respectively. Replacing
F(·) in (B.1) yields the expected condition:
R(st)PF (st+1 | st)PF (sf | st+1) = R(st+1)PB(st | st+1)PF (sf | st).
(B.2)
The original formulation in (B.1) would require us to parametrize both PF (st+1 | st) and F(s). On the other hand, using
this alternative condition, we only have to parametrize PF (st+1 | st) (including when st+1 = sf is the terminal state).
Accepted for the 38th Conference on Uncertainty in Artiﬁcial Intelligence (UAI 2022).

A
B
C
Gt
mt
A
B
C
A
B
C
Gt+1
mt+1
⊗
=
Adjacency
matrix of Gt
Transitive
closure of G⊤
t
Figure 6: Online update of the mask m. The mask mt associated with Gt represents (in black) the edges that cannot be
added to Gt to obtain a valid DAG. mt is decomposed in two parts: the adjacency matrix of Gt (top), and the transitive
closure of G⊤
t (bottom). To update the mask and obtain mt+1 associated with Gt+1, the result of adding the edge C →A
to Gt, each component must be updated separately, and then recombined. The diagonal elements of mt, corresponding to
self-loops (which are always invalid actions to take) are integrated into the transitive closure of G⊤
t by convention.
C
DEFINITION AND UPDATE OF THE MASK OVER ACTIONS
In Section 4.1, we introduced a mask m associated with a DAG G to indicate which edges could be legally added to G to
obtain a new valid DAG G′. This mask must ignore (1) the edges already present in G (which cannot be added further), and
(2) any edge whose addition leads to the introduction of a cycle. The mask m is constructed using (1) the adjacency matrix
of G, and (2) the adjacency matrix of the transitive closure of G⊤, the transpose graph of G; recall that G⊤is obtained from
G by inverting the direction of its edges.
Giudici and Castelo (2003) use a similar construction to efﬁciently obtain the legal actions their MCMC sampler may
take. In particular, they show that this mask m can be updated very efﬁciently online as edges are added one by one. In
practice, this allows us to circumvent an expensive check for cycles at every stage of the construction of a sample DAG in
the GFlowNet. Since the mask can be composed in 2 parts (as explained above), we can simply update each part anytime a
new edge is added to a DAG G.
In Figure 6, we show how the mask mt associated with a graph Gt can be updated after adding a new edge C →A to
obtain the mask mt+1. The mask is decomposed in 2 parts: the adjacency matrix of Gt, and the transitive closure of G⊤
t .
After adding C →A, each component is updated separately:
1. Adjacency matrix: To update the adjacency matrix, the entry in the adjacency matrix must be set (here, the entry
corresponding to the edge C →A).
2. Transitive closure: To update the transitive closure of the transpose, we need to compute the outer product of the
column corresponding to the target of the edge (here A, in blue) with the row corresponding to the source of the edge
(here C, in red). The outer product is added (more precisely, this is a binary OR) to the initial transitive closure.
These two operations can be done very efﬁciently in O(d2), where d is the number of nodes in the DAG.
D
ADDITIONAL EXPERIMENTAL RESULTS
D.1
DETAILS ABOUT THE METRICS
Throughout this paper, we used mainly two metrics to compare the performance of DAG-GFlowNet over alternative
Bayesian structure learning algorithms: the expected SHD (E−SHD), and the area under the ROC curve (AUROC). Let
{G1, . . . , Gn} be samples from the posterior approximation to be evaluated, and G⋆be the ground truth graph. The E-SHD

MC3
Gadget
B-PC
B-GES
DiBS
BCD
GFN
50
100
150
200
E-SHD
MC3
Gadget
B-PC
B-GES
DiBS
BCD
GFN
0.6
0.8
1
AUROC
MC3
Gadget
B-PC
B-GES
DiBS
BCD
GFN
500
1,000
1,500
2,000
log P(G, D′ | D)
Figure 7: Bayesian structure learning of linear-Gaussian Bayesian networks with d = 50 nodes. Results for E-SHD &
AUROC are aggregated over 10 randomly generated datasets D, sampled from different (ground-truth) Bayesian networks.
Results for log P(G, D′ | D) are given for a single dataset D; the dashed line corresponds to the log-likelihood of the ground
truth graph. Labels: B-PC = Bootstrap-PC, B-GES = Bootstrap-GES, BCD = BCD Nets, GFN = DAG-GFlowNet.
to G⋆can be estimated as
E−SHD ≈1
n
n
X
k=1
SHD(Gk, G⋆),
(D.3)
where SHD(G, G⋆) counts the number of edges changes (adding, removing, reversing an edge) necessary to move from G
to G⋆.
D.2
SIMULATED DATA
In addition to the experiments on simulated data with graphs over d = 20 nodes, we also compared DAG-GFlowNet with
other methods on graphs with d = 50 nodes. The experimental setup described in Section 6.2 remains unchanged, and
the data generation process is detailed below. We show this comparison in Figure 7, in terms of E-SHD, AUROC, and
the joint log-likelihood P(D′, G | D) on some held-out dataset D′. We observe that DAG-GFlowNet is still competitive
compared to the other algorithms, even though it suffers from a higher variance. This can be partly explained by the neural
network parametrizing the forward transition probability Pθ(Gt+1 | Gt) (see Sections 4.2 and 4.3) underﬁtting the data,
and therefore not accurately matching the detailed-balance conditions, necessary for a close approximation of the posterior
distribution P(G | D). Similar to our observations in Section 6.3, we also noticed that algorithms that tend to perform better
in terms of E-SHD (e.g. BCD Nets, Bootstrap-PC) tend to have an order of magnitude fewer edges in the sampled DAGs.
Data generation
For our experiments on simulated data, we followed the generation process described in (Lorch et al.,
2021). The data was generated in the following way:
1. We sampled a DAG using an Erd˝os-Rényi model (Erd˝os and Rényi, 1960), with 2d edges on average; the value of the
probability of creating an edge between two nodes was scaled accordingly.
2. Once the structure of the graph is known, we sampled the parameters of the linear-Gaussian model randomly from a
standard Normal distribution N(0, 1). The linear-Gaussian model is therefore deﬁned as, ∀j ∈[1, d]
Xj =
X
Xi∈PaG(Xj)
βijXi + ε,
where βij ∼N(0, 1), and ε ∼N(0, 0.01). The deﬁnes all the conditional probability distribution of the generative
model.
3. Once the full Bayesian Network is known, we used ancestral sampling to generate N = 100 datapoints to ﬁll our
dataset D.
D.3
FLOW CYTOMETRY DATA
In Section 6.3, we described an application of DAG-GFlowNet to real-world ﬂow cytometry data. In particular, we showed
in Figure 5 that DAG-GFlowNet was capable of modeling a distribution that was not only capable of capturing the mode of

−12,600
−12,400
−12,200
−12,000
−11,800
−11,600
−11,400
−11,200
−11,000
−10,800
−10,600
10−3
10−2
10−1
100
BGe score
log-marginal probability of MEC
MC3
Gadget
DiBS
BCD Nets
DAG-GFlowNet
Figure 8: Coverage of the posterior approximations learned on ﬂow cytometry data (Sachs et al., 2005). Each point
corresponds to a sampled Markov equivalence class, and its size represents the number of different DAGs (in the equivalence
class) sampled from the posterior approximation.
the posterior distribution (i.e., graphs with a high score), but also had diversity in the graphs sampled, both in terms of the
different Markov Equivalence Classes (MECs) those graphs belong to, but also multiple unique DAG instances of the same
MEC (depicted by the size of each point in Figure 5).
However for clarity, we only compared DAG-GFlowNet to methods based on MCMC in Figure 5. In Figure 8, we also added
a comparison to BCD Nets (Cundy et al., 2021) and DiBS (Lorch et al., 2021), two methods based on Variational Inference.
Although we saw in Table 1 that those two methods were comparing favorably against other algorithms in terms of E−SHD
and AUROC, including against DAG-GFlowNet, we can assess more precisely the quality of the posterior approximation
returned by BCD Nets and DiBS:
• Out of 1,000 graphs sampled with BCD Nets, those graphs belonged to one of only two MECs (with a BGe score
around −10,950). Furthermore, as shown by the size of each point, those MECs happen to only contain a single unique
DAG. Overall, this means that BCD Nets only returned 2 unique DAGs (out of the 1,000 samples), showing the lack of
diversity of the posterior approximation learned with BCD Nets.
• DiBS sampled a signiﬁcant number of very low scoring DAGs, with BGe scores as low as −12,600 (whereas the best
MEC obtained with GES (Chickering, 2002) had a score of −10,716.12).
• With our choice of the BGe score, the true posterior distribution would assign the same probability to graphs in the
same MEC. However, we can see that DiBS only returned graphs belonging to unique MECs, as opposed to having
multiple unique DAGs from the same MEC. This shows that while DiBS has a high diversity in terms of MECs (mainly
due to covering low-scoring DAGs), DiBS suffers from a lack of diversity with a single MEC, which would be expected
from a faithful approximation of the posterior distribution.

