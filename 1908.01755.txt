On the Existence of Simpler Machine Learning Models
Lesia Semenova, Cynthia Rudin, and Ronald Parr
{lesia, cynthia, parr}@cs.duke.edu
Department of Computer Science, Duke University
It is almost always easier to ﬁnd an accurate-but-complex model than an accurate-yet-simple model.
Finding optimal, sparse, accurate models of various forms (linear models with integer coeﬃcients, decision
sets, rule lists, decision trees) is generally NP-hard. We often do not know whether the search for a simpler
model will be worthwhile, and thus we do not go to the trouble of searching for one. In this work, we ask
an important practical question: can accurate-yet-simple models be proven to exist, or shown likely to exist,
before explicitly searching for them? We hypothesize that there is an important reason that simple-yet-
accurate models often do exist. This hypothesis is that the size of the Rashomon set is often large, where
the Rashomon set is the set of almost-equally-accurate models from a function class. If the Rashomon set is
large, it contains numerous accurate models, and perhaps at least one of them is the simple model we desire.
In this work, we formally present the Rashomon ratio as a new gauge of simplicity for a learning problem,
depending on a function class and a data set. The Rashomon ratio is the ratio of the volume of the set of
accurate models to the volume of the hypothesis space, and it is diﬀerent from standard complexity measures
from statistical learning theory. Insight from studying the Rashomon ratio provides an easy way to check
whether a simpler model might exist for a problem before ﬁnding it, namely whether several diﬀerent machine
learning methods achieve similar performance on the data. In that sense, the Rashomon ratio is a powerful
tool for understanding why and when an accurate-yet-simple model might exist. If, as we hypothesize in
this work, many real-world data sets admit large Rashomon sets, the implications are vast: it means that
simple or interpretable models may often be used for high-stakes decisions without losing accuracy.
1
Introduction
Following the principle of Occam’s Razor, one should use the simplest model that explains the data well.
However, ﬁnding the simplest model, let alone any simple-yet-accurate model, is hard. As soon as simplicity
constraints such as sparsity are introduced, the optimization problem for ﬁnding a simpler model typically
becomes NP-hard. Thus, practitioners – who have no assurance of ﬁnding a simpler model that achieves the
performance level of a black box – may not see a reason to attempt such potentially diﬃcult optimization
problems. Thus, sadly, what was once the holy grail of ﬁnding simpler models, has been, for the most part,
abandoned in modern machine learning. In this work, we ask a question that is essential, and potentially
game-changing, for this discussion: what if we knew, before attempting a computationally expensive search
for a simpler-yet-accurate model, that one was likely to exist? Perhaps knowing this would allow us to justify
the time and expense of searching for such a model. If it is true that many data sets have large enough
Rashomon sets to admit simple models, then there are important implications for society – it means we may
be able to use simpler or interpretable models for many high-stakes problems without losing accuracy.
Proving the existence of simpler models before aiming to ﬁnd them diﬀers from the current approach
to machine learning in practice. We generally do not think about going from more complicated spaces to
simpler ones; in fact, the reverse is true, where typical statistical learning theory and algorithms allowed us
to maintain generalization when handling more complicated model classes (e.g., large margins for support
1
arXiv:1908.01755v4  [cs.LG]  12 May 2022

vector machines with complex kernels or large margins for boosted trees) Cortes and Vapnik (1995); Schapire
et al. (1998). We even build neural networks that are so complex that they can achieve zero training error,
and try afterwards to determine why they generalize Belkin et al. (2019); Nakkiran et al. (2021). However,
because simple models are essential for many high-stakes decisions (Rudin, 2019), perhaps we should return
to the goal of aiming directly for simpler models. We will need new ideas in order to do this.
Decades of study about generalization in machine learning have provided many diﬀerent mathematical
theories. Many of them measure the complexity of classes of functions without considering the data (e.g., VC
theory, Vapnik, 1995), or measure properties of speciﬁc algorithms (e.g., algorithmic stability, see Bousquet
and Elisseeﬀ, 2002). However, none of these theories seems to capture directly a phenomenon that occurs
throughout practical machine learning. In particular, there are a vast number of data sets for which many
standard machine learning algorithms perform similarly. In these cases, the machine learning models tend
to generalize well. Furthermore, in these same cases, there is often a simpler model that performs similarly
and also generalizes well.
We hypothesize that these three observations can all be explained by the same phenomenon:
the
“Rashomon eﬀect,” which is the existence of many almost-equally-accurate models (Breiman et al., 2001).
Firstly, following a key argument in our work, if there is a large Rashomon set of almost-equally-accurate
models, a simple model may also be contained in it. Secondly, if the Rashomon set is large, many diﬀerent
machine learning algorithms may ﬁnd diﬀerent but approximately-equally-well-performing models inside it.
An experimenter could then observe similar performance for diﬀerent types of algorithms that produce very
diﬀerent functions. Thirdly, if the Rashomon set is large enough to contain simpler models, those models are
guaranteed to generalize well. As we will show in Section 5.2, there are mathematical assumptions that allow
us to prove existence of simpler models within the Rashomon set. If the assumptions are satisﬁed, a model
from a simpler class is approximately as accurate as the most accurate model within the hypothesis space,
which consequently leads to better generalization guarantees. The assumptions are based in approximation
theory, which models how one class of functions can approximate another.
We quantify the magnitude of the Rashomon eﬀect through the Rashomon ratio, which is the ratio of
the Rashomon set’s volume to the volume of the hypothesis space. An illustration of the Rashomon set is
shown in Figure 1; it does not need to be a connected or convex set. The Rashomon ratio can serve as a
gauge of simplicity for a learning problem.1 As a property of both a data set and a hypothesis space, it
diﬀers from the VC dimension (Vapnik and Chervonenkis, 1971) (because the Rashomon ratio is speciﬁc to
a data set), it diﬀers from algorithmic stability (see Rogers and Wagner, 1978; Kearns and Ron, 1999) (as
the Rashomon ratio does not rely on robustness of an algorithm with respect to changes in the data), it
diﬀers from local Rademacher complexity (Bartlett et al., 2005) (as the Rashomon ratio does not measure
the ability of the hypothesis space to handle random changes in targets and actually beneﬁts from multiple
similar models), and it diﬀers from geometric margins (Vapnik, 1995) (as the maximum margin classiﬁer can
have a small minimum margin yet the Rashomon ratio can be large, and margins are measured with respect
to one model, whereas the Rashomon ratio considers the existence of many). We provide theorems that show
simple cases when the Rashomon ratio disagrees with these complexity measures in Section 3 and Appendix
B. The Rashomon set is not just functions within a ﬂat minimum; it could consist of functions from many
non-ﬂat local minima as illustrated in Figure 4 in Appendix A, and it applies to discrete hypothesis spaces
where gradients, and thus “sharpness” (Dinh et al., 2017) do not exist. For linear regression, we derive a
closed form solution for the volume of the Rashomon set in parameter space in Theorem 10 in Appendix
B.1.
Our theory and empirical results have implications beyond cases where the size of the Rashomon set can
be estimated in practice: they suggest computationally inexpensive ways to gauge whether the Rashomon set
is large without directly measuring it. In particular, our results indicate that when many machine learning
methods perform similarly on the same data set (without overﬁtting), it could be because the Rashomon set
of the functions these algorithms consider is large. Thus, after running diﬀerent machine learning methods
and observing similar performance, our results indicate that it may be worthwhile to optimize directly for
simpler models within the Rashomon set.
1Such measures are typically called “complexity” measures, but the Rashomon ratio measures simplicity, not complexity.
2

Empirical risk
Hypothesis space
(a) From the side 
   (b) From above 
(c) From underneath
Figure 1: An illustration of a possible Rashomon set in two dimensional hypothesis space F. Models below
the gray plane belong to the Rashomon set ˆRset(F, θ), where the height of the gray plane is adjusted by the
Rashomon parameter θ deﬁned in Section 3.
We summarize the contributions of this work as follows: (i) We deﬁne the Rashomon ratio as an important
characteristic of the Rashomon set. (ii) We provide generalization bounds for models from the Rashomon set,
and show that the size of the Rashomon set serves as a barometer for the existence of accurate-yet-simpler
models that generalize well. These are diﬀerent from standard learning theory bounds that consider the
distance between the true and empirical risks for the same function. (iii) We provide several approaches for
estimating the size of the Rashomon set. (iv) We show empirically that when a large Rashomon set occurs,
most machine learning methods tend to perform similarly, and also in these cases, simple or sparse (yet
accurate) models exist. (v) We demonstrate that the Rashomon ratio, as a gauge of simplicity of a machine
learning problem, is diﬀerent from other known complexity measures such as VC-dimension, algorithmic
stability, geometric margin, and Rademacher complexity. (vi) We show that larger Rashomon sets might
occur in the presence of label or feature noise.
2
Related Work
There are several bodies of relevant literature as discussed below.
Rashomon sets:
Rashomon sets have been used for various purposes (Breiman et al., 2001; Srebro et al.,
2010; Fisher et al., 2019; Coker et al., 2021; Tulabandhula and Rudin, 2014b; Meinshausen and Bühlmann,
2010; Letham et al., 2016; Nevo and Ritov, 2017). For instance, Srebro et al. (2010) consider a loss-restricted
class of close-to-optimal models, and with an assumption of H-smoothness of a loss function, they obtain
a tighter excess risk bound through local Rademacher complexity (Bartlett et al., 2005). Our bounds do
not work the same way and aim to prove a diﬀerent type of result. Other works aim to search through the
Rashomon set to ﬁnd the most extreme models within it, rather than looking at the size of the Rashomon
set, as we do in this work. Fisher et al. (2019) leverages the Rashomon set in order to understand the
spectrum of variable importance and other statistics across the set of good models. Our work considers the
existence of models from simpler classes rather than exploring the Rashomon set to ﬁnd a range of variable
importance or other statistics. The work of Tulabandhula and Rudin (2013, 2014a,b) uses the Rashomon
set to assist with decision making, by ﬁnding the range of downstream operational costs associated with the
Rashomon set. Rashomon sets are related to p-hacking and robustness of estimation, because the Rashomon
set is a set over which one might conduct a sensitivity analysis to choices made by an analyst (Coker et al.,
2021). Large Rashomon sets can occur when the machine learning pipeline is underspeciﬁed. D’Amour et al.
(2020) provides multiple examples of underspeciﬁcation in computer vision, natural language processing, and
3

healthcare domains; their work builds oﬀof (an earlier version of) our work. Madras et al. (2019) proposed
a post-hoc local-ensemble method that measures underspeciﬁcation for a given test datum. Marx et al.
(2020) studies conﬂicting predictions between models within the Rashomon set, while Coston et al. (2021)
investigates predictive disparities for algorithmic fairness.
Flat minima or wide valleys:
The concept of ﬂat minima (wide valleys) has been explored in the deep
learning literature as a possible way to understand convergence properties of the complicated, non-convex loss
functions that deep networks traverse during training (Hochreiter and Schmidhuber, 1997; Dinh et al., 2017;
Keskar et al., 2016; Chaudhari et al., 2019). Based on a minimum-message-length argument (Wallace and
Boulton, 1968), several works claim that ﬂat loss functions lead to better generalization due to a robustness to
noise around the minimum (Hochreiter and Schmidhuber, 1997; Keskar et al., 2016; Chaudhari et al., 2019).
Following Hochreiter and Schmidhuber (1997), Dinh et al. (2017) deﬁne volume ϵ-ﬂatness, which constitutes
a special case of our Rashomon sets, as shown in Figure 4 in Appendix A. In particular, the Rashomon set
is deﬁned over the hypothesis (functional) space, while the volume ϵ-ﬂatness is deﬁned in a parameter space
(though sometimes we use parameter space for ease of computation), and the Rashomon set is not necessarily
a single connected component (although it might be in the case of a convex loss over a continuous domain),
while volume ϵ-ﬂatness pertains only to a connected set. This means that the Rashomon set can contain
models from diﬀerent local minima, or can be deﬁned on discrete spaces, while volume ϵ-ﬂatness is relevant
only for continuous loss functions. Another way of quantifying ﬂatness is σ-sharpness (Keskar et al., 2016;
Dinh et al., 2017), which measures the change of the loss function inside a σ-ball in a parameter space. In
the case of a connected Rashomon set, this loss diﬀerence corresponds to the Rashomon parameter θ.
Statistical learning theory:
Numerous works provide generalization bounds based on diﬀerent com-
plexity measures, and under diﬀerent assumptions. Some discuss Rademacher (Srebro et al., 2010; Kakade
et al., 2008) and Gaussian complexities (Kakade et al., 2008), PAC-Bayes theorems (Langford and Shawe-
Taylor, 2002), covering numbers bounds (Zhou, 2002), and margin bounds (Vapnik and Chervonenkis, 1971;
Schapire et al., 1998; Koltchinskii and Panchenko, 2002). In contrast, under assumptions elaborated in Sec-
tion 5, the Rashomon ratio provides a certiﬁcate of the existence of a simpler model that generalizes. The use
of approximating sets, as used extensively in this paper, is used throughout the literature on learning theory
(Lecué, 2011; Lugosi and Wegkamp, 2004; Schapire et al., 1998; Mendelson, 2003). An example of this is the
classical generalization bound for boosting and margins (Schapire et al., 1998), which uses combinations of
several random draws of base classiﬁers to represent combinations of base classiﬁers. This is an instance of
the so-called “Maurey’s lemma,” which provides an approximating set for linear model classes.
3
Rashomon Set Deﬁnitions and Notation
Consider a training set of n data points S = {z1, z2, ..., zn}, zi = (xi, yi) drawn i.i.d. from an unknown
distribution D on a bounded set Z = X × Y, where X ⊂Rp and Y ⊂R are an input and an output space
respectively. Our hypothesis space is F = {f : X →Y}. We limit the hypothesis space F to contain only
models that vary within the bounded domain Z where the data reside. We will assume that the hypothesis
space is bounded and that there is a prior distribution ρ over functions in F. To measure the quality of a
prediction made by a hypothesis, we use a loss function φ : Y × Y →R+. Speciﬁcally, for each given point
z = (x, y) and a hypothesis f, the loss function is φ(f(x), y). For a given f we will also overload notation
by writing l : F × Z →R+ that takes f explicitly as an argument: l(f, z) = φ(f(x), y). We are interested
in learning a model f that minimizes the true risk L(f) = Ez∼D[φ(f(x), y)], which depends on unknown
distribution D and therefore is estimated with an empirical risk: ˆL(f) = 1
n
Pn
i=1 φ(f(xi), yi).
The empirical Rashomon set (or simply Rashomon set) is a subset of models of the hypothesis space F
that have training performance close to the best model in the class, according to a loss function (Breiman
et al., 2001; Srebro et al., 2010; Fisher et al., 2019; Coker et al., 2021; Tulabandhula and Rudin, 2014b).
More precisely:
4

Deﬁnition 1 (Rashomon set). Given θ ≥0, a data set S, a hypothesis space F, and a loss function φ, the
Rashomon set ˆRset(F, θ) is the subspace of the hypothesis space deﬁned as follows:
ˆRset(F, θ) := {f ∈F : ˆL(f) ≤ˆL( ˆf) + θ},
where ˆf is an empirical risk minimizer for the training data S with respect to loss function φ:
ˆf ∈
argminf∈F ˆL(f).
If we want to specify the data set S that is used to compute the Rashomon set, we indicate the data set
in the subscript, as ˆRsetS(F, θ). Fisher et al. (2019)’s deﬁnition of Rashomon set is distinct from ours in that
we typically use an empirical risk minimizer to deﬁne the Rashomon set instead of a prespeciﬁed reference
model which is independent of the sample.
The hypothesis space F can be a well-deﬁned hypothesis space, such as the space of decision trees of
depth D or neural nets with D hidden layers, or it can be a more general space (a meta-hypothesis space)
that contains models from diﬀerent hypothesis spaces (e.g., linear functions, polynomials up to degree D,
and piecewise constant functions).
We call θ the Rashomon parameter. Since hypothesis spaces can vary from one problem to another, we
will often normalize the size of the Rashomom set via the Rashomon ratio ˆRratio( ˆRset(F, θ)) which takes
the Rashomon set as input and outputs a value between 0 and 1.
Given a prior, ρ, on the hypothesis
space, the Rashomon ratio measures the fraction of the hypothesis space contained in the Rashomon set.
Unless explicitly speciﬁed, ρ is assumed to be uniform. For simplicity, we will denote the Rashomon ratio
as ˆRratio(F, θ). In general, the Rashomon ratio is ˆRratio(F, θ) =
R
f∈F 1f∈ˆ
Rset(F,θ)ρ(f)dρ. If the hypothesis
space has a uniform prior, then the Rashomon ratio is the volume of the Rashomon set divided by the
volume of the hypothesis space ˆRratio(F, θ) = V( ˆ
Rset(F,θ))
V(F)
, where V(·) : F →R+ is the volume function. If
the hypothesis space is discrete with a uniform prior, the Rashomon ratio can be computed as ˆRratio(F, θ) =
| ˆ
Rset(F,θ)|
|F|
, where |A| = P
f∈F 1f∈A. The Rashomon ratio represents the fraction of models that are good
(the fraction of models that ﬁt the data about equally well). A larger Rashomon ratio implies that more
models perform about equally well. The data set S is denoted in the subscript, as ˆRratioS(F, θ).
We consider true Rashomon sets that contain models with low true risk, relative to the optimal true risk
value, with parameter γ > 0:
Rset(F, γ) = {f ∈F : L(f) ≤L(f ∗) + γ},
where f ∗∈F minimizes the true risk. Rratio(F, θ) denotes the Rashomon ratio for the true Rashomon set.
A large true Rashomon set, as it turns out, can be a certiﬁcate of the existence of a simpler model.
Though, since we can never actually explore the true Rashomon set, we would never know whether it will
be (or has been) useful for a particular problem. We explain this in Section 5, and spend most of our eﬀort
considering empirical Rashomon sets, which are easier to work with in practice.
When the hypothesis space has a parameterized representation (denote FΩ), we assume that we can
parameterize each model f ∈FΩwith a unique parameter vector ω ∈Ωof ﬁnite length and denote f(z) =
fω(z). In the next section, we discuss properties of the Rashomon ratio as a complexity measure.
4
Rashomon ratio as a simplicity measure
The Rashomon ratio, as a property of a data set and a hypothesis space, serves as gauge of simplicity of the
learning problem. If the Rashomon set is large, many diﬀerent reasonable optimization procedures could lead
to a model from the Rashomon set. Therefore, for large Rashomon sets, accurate models tend to be easier
to ﬁnd (since optimization procedures can ﬁnd them). In other words, if the Rashomon ratio is large, the
Rashomon set could contain many accurate and simple models, and the learning problem becomes simpler.
On the other hand, smaller Rashomon ratios might imply a harder learning problem, especially in the case
of few deep and narrow local minima.
5

Table 1: Comparison of Rashomon ratio and other complexity measures. The Rashomon ratio considers the
fact that there are multiple good models and is a property both of the hypothesis space and data.
Complexity measure
Property of
Depends on data
Considers set of
good models
VC Dimension
hypothesis space
no
no
Algorithmic stability (Hypothesis sta-
bility Bousquet and Elisseeﬀ(2002))
algorithm, hypothe-
sis space
no
no
Empirical algorithmic stability (Algo-
rithmic hypothesis stability Bousquet
and Elisseeﬀ(2002))
algorithm, hypothe-
sis space
yes
no
Geometric margins
one function
yes
no
Empirical
Local
Rademacher
Com-
plexity (Bartlett et al., 2005)
hypothesis space
depends
on
features,
not on labels
no
Rashomon ratio
hypothesis space
yes, but not always on
labels (see Theorem 10)
yes
The Rashomon ratio can give insight into the simplicity of a learning problem, though it was designed
for a fundamentally diﬀerent goal than well-known complexity measures from learning theory (see Table 1).
While those complexity measures were designed to help us understand generalization, the Rashomon ratio
(with additional assumptions) helps us understand whether simpler functions might exist with the same level
of accuracy as complex functions. The Rashomon ratio depends on a loss function, the hypothesis space,
and a data set, while the majority of other measures are either data-agnostic or focus on properties of a
speciﬁc model in the space.
The Rashomon ratio is diﬀerent from VC dimension.
The VC dimension (Vapnik and Chervonenkis,
1971) shows the expressive power of a hypothesis space for any data set including an extreme arrangement
of data points and labels. On the contrary, the Rashomon set depends on an empirical risk minimizer that
we compute directly for a speciﬁc data set, which may not be extreme.
The Rashomon ratio is diﬀerent from algorithmic stability.
Algorithmic stability (Bousquet and
Elisseeﬀ, 2002) (see Deﬁnition B.2) depends on a change to a data set, whereas Rashomon Ratio uses a ﬁxed
data set. As we will show in Theorem 10 in Appendix B.1, in the case of linear least squares regression, the
Rashomon ratio depends on features (X) only, and does not depend on regression targets Y . In contrast,
hypothesis stability depends heavily on Y . In fact, if we can control how we change the set of targets,
hypothesis stability (a form of algorithmic stability) can be made to change by an arbitrarily large amount.
This is formalized in Theorem 2 with proof in Appendix B.2.
Theorem 2 (Rashomon ratio is not algorithmic stability). Consider a distribution PX over a discrete
domain X = {x1, ...xN} and a learning algorithm A that minimizes the sum of squares loss : ∥Xω −Y ∥2
2.
for a linear hypothesis space FΩ. For any λ > 0, there exist joint distributions PX,Y1 and PX,Y2 where for X
drawn i.i.d. from PX, Y1 drawn from PY1|X over Y | X, and Y2 drawn from PY2|X over Y | X, the expected
Rashomon ratios are the same:
EPX,Y1[ ˆRratioS1(FΩ, θ)] = EPX,Y2[ ˆRratioS2(FΩ, θ)],
yet hypothesis stability constants are diﬀerent by our arbitrarily chosen value of λ: ˜β2 −˜β1 ≥λ, where S1
and S2 denote data sets S1 = [X, Y1] and S2 = [X, Y2], ˜β1 is the hypothesis stability coeﬃcient of algorithm
A for distribution PX,Y1 and ˜β2 is the hypothesis stability coeﬃcient for distribution PX,Y2.
The Rashomon ratio is diﬀerent from geometric margins.
The margin (i.e., the minimum margin of
the maximum margin classiﬁer) (Schapire et al., 1998; Burges, 1998) depends on points closest to the decision
6

boundary (support vectors), while the Rashomon set does not necessarily rely on the support vectors and
may depend on the full data set. Theorem 3 shows this with proof in Appendix B.3.
Theorem 3 (Rashomon ratio is not the geometric margin). For any ﬁxed 0 < λ < 1, there exists a ﬁxed
hypothesis space FΩ, a Rashomon parameter θ, and there exist two data sets S1 and S2 with the same
empirical risk minimizer ˆf ∈FΩsuch that the geometric margin d is the same for both data sets, yet the
Rashomon ratios are diﬀerent:
| ˆRratioS1(FΩ, θ) −ˆRratioS2(FΩ, θ)| > λ.
The Rashomon ratio is diﬀerent from empirical local Rademacher complexity.
Empirical Rademacher
complexity (Bartlett et al., 2005) (see Deﬁnitions B.4) measures how well the hypothesis space can ﬁt ran-
dom assignments of the labels. The Rashomon ratio uses ﬁxed labels. It measures the number of models
that are close to optimal. In other words, the Rashomon set beneﬁts from having multiple similar models,
while Rademacher complexity treats them as equivalent. Please see Theorem 4 with deﬁnition and proof in
Appendix B.4.
Theorem 4 (Rashomon ratio is not local Rademacher complexity). For 0 < λ < 1, there exist two data sets
S1 and S2, a hypothesis space FΩ, and a Rashomon parameter θ such that the local Rademacher complexities
deﬁned on the Rashomon sets for S1 and S2 are the same: ˆRS1
n

ˆRset(FΩ, θ)

= ˆRS2
n

ˆRset(FΩ, θ)

, yet the
Rashomon ratios are diﬀerent:
 ˆRratioS1(FΩ, θ) −ˆRratioS2(FΩ, θ)
 > λ.
Now that we have established that the Rashomon ratio is not the same as other simplicity measures, we
can now shift our focus to proving simplicity and generalization properties of models in the Rashomon set.
This is critical to our thesis that simple-yet-accurate models exist.
5
Rashomon Set Models: Simplicity and Generalization
Consider two hypothesis (functional) spaces with diﬀerent levels of complexity, where the lower-complexity
space serves as a good approximating set (i.e., a good cover) for the higher-complexity space. The hypothesis
spaces are called F1, for the simpler space, and F2, for the more complex space, where F1 ⊂F2. Here,
to determine the complexity of a hypothesis space, we use traditional notions of complexity (conversely,
simplicity) such as covering numbers or VC dimension. For a useful example of a simple and a more complex
space, consider F2 to be the space of linear models with real-valued coeﬃcients in a space of d dimensions,
and consider F1 to be the space of scoring systems (Ustun and Rudin, 2016), which are sparse linear models,
with at most d′ nonzero integer coeﬃcients, d′ ≪d. Another example is if the more complex space F2
consists of boosted decision trees, and F1 consists of single trees. Generalization bounds would be tighter if
we could use the lower complexity space F1, but as we are considering functions from F2, learning theory
often has us include the complexity of F2 in the bound. Given this setup, we have several questions to
answer:
1. What if the higher-complexity hypothesis space we chose were more complex than necessary for mod-
eling the data? In that case, if we had instead used the simpler model class F1, would we still get
a model that is (almost) as good as we could have obtained using the more complex class F2? If so,
perhaps we can leverage the complexity of the simpler model class F1 for generalization bounds on
our model rather than the more complex class F2. We answer this question in Section 5.1, where a
property on the complex space that will help us is that the true Rashomon set of F2 is large
enough to admit a simpler model. We do not need to know what this model is and we may never
discover it (we would likely discover a diﬀerent model using data).
2. Under what conditions on the complex and simpler model classes does the property we mentioned
above (that the Rashomon set includes simpler models) hold? Does it hold often? As it turns out,
7

under natural conditions on the function class and loss function, a large Rashomon set in the complex
class does imply the existence of simple-yet-accurate models. We identify these conditions in Section
5.2, namely that the loss function is smooth, and that F1 serves as a cover for F2. Thus, under these
natural conditions that occur in practice, a large Rashomon set for a complex class of
functions implies the existence of a simple-yet-accurate model.
The bounds we present in Section 5.1 do not serve the same purpose as standard statistical learning
theoretic bounds, as they do not aim to bound generalization error for a single function (that is, the diﬀerence
between training and test loss for a function). Rather, we are interested in bounding train loss of one function
(a simpler function) with test loss of another (the optimal model in a more complex function class). Standard
learning theory analysis handles the single function case nicely; we are concerned with other questions here.
5.1
The True Rashomon Set Can Be Very Helpful... But You Might Not Know
When
As in classic Occam’s razor bounds, we start with ﬁnite hypothesis spaces. Consider ﬁnite hypothesis spaces
F1 and F2, where F1 ⊂F2. Consider the ﬁrst question discussed above: Given F1 and F2, can we have a
guarantee that a model we produce using a simpler function class F1 on our data could be approximately
as good as the test performance of the best model from F2? In the following theorem, we will make a
key assumption that allows us to do this: we assume that the Rashomon set of F2 includes a member of
the simpler class of functions, F1, even if we do not know which function it is. Later, in Section 5.2, we
show conditions under which simple models from F1 are proven to exist in the Rashomon set of F2, which
depends on the size of F2’s Rashomon set. Here, |F| denotes the cardinality of the ﬁnite space F. These
bounds can be generalized to inﬁnite hypothesis spaces with a simple extension to covering numbers, but
they are designed for intuition, which works nicely with ﬁnite hypothesis spaces. Again, this is diﬀerent from
a regular learning theory bound as it does not consider generalization of just one function.
Theorem 5 (The advantage of a true Rashomon set). Consider ﬁnite hypothesis spaces F1 and F2, such
that F1 ⊂F2. Let the loss l be bounded by b, l(f2, z) ∈[0, b] ∀f2 ∈F2, ∀z ∈Z. Deﬁne an optimal function
f ∗
2 ∈argminf2∈F2L(f2). Assume that the true Rashomon set includes a function from F1, so there exists a
model ˜f1 ∈F1 such that ˜f1 ∈Rset(F2, γ). (Note that we do not know ˜f1.) In that case, for any ϵ > 0 with
probability at least 1 −ϵ with respect to the random draw of data:
L(f ∗
2 ) −b
r
log | F1 | + log 2/ϵ
2n
≤ˆL( ˆf1) ≤L(f ∗
2 ) + γ + b
r
log 1/ϵ
2n
,
(1)
where ˆf1 ∈argminf1∈F1 ˆL(f1). (Unlike ˜f1, we do know ˆf1 because we can calculate it.)
That is, we can bound the best empirical model from F1 with the true risk of the best model within F2.
Thus, if the Rashomon set is large enough to include a single model from F1, we can work with the simpler
class F1 in practice and achieve strong performance guarantees.
The main assumption in Theorem 5 is about the population, and does not rely on the sample. It relies
only on the existence of one special function in the true Rashomon set. There are no smoothness assumptions
on the loss function. If the main assumption of this theorem holds, then we gain the beneﬁt of guarantees on
F2 from looking only at F1 empirically. We cannot check whether the assumption holds since it involves the
true risk, but practitioners can reap the beneﬁts of it anyway: The possibility of a large Rashomon set may
embolden the practitioner to minimize over F1, achieving test error close to the best of F2 if the conditions
of Theorem 5 are indeed satisﬁed.
To make the connection of this result to Rashomon sets more explicit, we will choose a speciﬁc relationship
between F1 and F2, speciﬁcally, F1 will be a random sample of F2 that is chosen prior to, and separately
from, learning. This is an artiﬁcial example in that F1 would never actually be chosen as a random sample
from F2 in reality. However, the random sampling assumption permits F1 to be distributed fairly evenly
8

Table 2: Examples of the possible usage of Theorem 6.
If |F1| = 100000 then to get the bound (1) to hold with probability at least 99%
the Rashomon ratio should be Rratio(F2, γ) ≥0.0053%.
If |F1| = 10000 then to get the bound (1) to hold with probability at least 99%
the Rashomon ratio should be Rratio(F2, γ) ≥0.053%.
If |F1| = 1000 then to get the bound (1) to hold with probability at least 99%
the Rashomon ratio should be Rratio(F2, γ) ≥0.53%.
within F2, which, arguably, could approximate the way some simpler spaces are embedded in more complex
spaces.
If F1 is a random sample of functions from F2, and if F2 has a large true Rashomon set, then the true
Rashomon set is likely to include at least one model from F1. In that case, Theorem 5 applies. This is
formalized below.
Theorem 6 (Example of the advantage of a large true Rashomon set). Consider ﬁnite hypothesis spaces F1
and F2, such that F1 ⊂F2 and F1 is uniformly drawn from F2 without replacement. For loss l bounded by
b, if the Rashomon ratio is at least
Rratio(F2, γ) ≥1 −ϵ
1
| F1 |
then for any ϵ > 0, with probability at least (1 −ϵ)2 with respect to the random draw of functions from F2
to form F1 and with respect to the random draw of data, the assumptions of Theorem 5 hold and thus the
bound (1) holds.
Table 2 shows possible values of the lower bound on the Rashomon ratio, given | F1 | and ϵ. For example,
the ﬁrst line of the table states that if at least a tiny fraction (0.0053%) of the complex function space F2
consists of good models, and there exists at least 100,000 simple functions in F1, then the chance that we
will ﬁnd an accurate-but-simple model on our data set is over 99%.
The intuition for Theorem 6 holds beyond the case when F1 is randomly sampled from F2, it holds
whenever F1 covers F2 suﬃciently well. This intuition is that as the true Rashomon ratio increases, it is
more likely that the empirical risk minimum of F1 will be close to the minimum of the true risk of F2.
5.2
Proving the Existence of Simple-yet-Accurate Models with Good General-
ization
Theorems 5 and 6 do not take advantage of the fact that we can investigate F2 empirically, and more easily
than we can investigate F1; these theorems instead only discuss exploration of F1. Thus, the next analysis
makes two improvements: (1) it studies empirical Rashomon sets instead of true Rashomon sets, (2) it
substitutes the unrealistic random draw assumption for a realistic smoothness assumption. We now assume
smoothness of the loss over the function space.
The ﬁeld of Approximation Theory provides general conditions under which classes of functions can
approximate each other. Given a target function from one class, we want to know whether a sequence of
functions from another class can converge to the target. Table 3 in Appendix C.4 shows classes of functions
F2 that can be approximated by classes F1. For instance, piecewise constant functions, such as decision
trees, can approximate smooth functions.
For a hypothesis space F and some f ′ ∈F, deﬁne the δ-ball of functions centered at f ′ as Bδ(f ′) = {f ∈
F : ∥f ′ −f∥p ≤δ}. A loss l : F × X →Y is said to be K-Lipschitz, K ≥0, if for all f1, f2 ∈F and for all
z ∈Z: |l(f1, z)−l(f2, z)| ≤K∥f1 −f2∥p. The p-norm can be deﬁned, for example, as ∥f∥p =
 R
X |f|pdµ
1/p,
where µ is a measure on X. Deﬁne a δ-packing as a ﬁnite set Ξ = {ξ1, ..., ξk|ξi ∈F} such that ∥ξi −ξj∥p > δ,
meaning that Bδ/2(ξi) ∩Bδ/2(ξj) = ∅for all i ̸= j. The packing number B(F, δ) is the largest δ-packing.
Theorem 7 below uses the approximating set argument from the previous subsection, but now requires
the Rashomon set to be large enough to include balls of functions rather than using the random draw
9

assumption. As long as the set of simpler functions is distributed well among the full hypothesis space, each
ball contains at least one function from the simpler class.
Theorem 7 (Existence of multiple simpler models). For K-Lipschitz loss l bounded by b, consider hypothesis
spaces F1 and F2, F1 ⊂F2. With probability greater than 1 −ϵ w.r.t. the random draw of training data,
if for every model f2 ∈ˆRset(F2, θ) there exists f1 ∈F1 such that ∥f2 −f1∥p ≤δ, then there exists at least
B = B( ˆRset(F2, θ), 2δ) functions ¯f 1
1 , ¯f 2
1 ..., ¯
f B
1 ∈ˆRset(F, θ) such that:
1. They are from the simpler space: ¯f 1
1 , ¯f 2
1 ..., ¯
f B
1 ∈F1.
2.
L( ¯f i
1) −ˆL( ¯f i
1)
 ≤2KRn(F1) + b
q
log(2/ϵ)
2n
, for all i ∈[1, .., B], where Rn(F) is the Rademacher
complexity of a hypothesis space F. (This is from standard learning theory.)
From Theorem 7, we see that since larger Rashomon sets have larger packing numbers, they contain more
simpler models with good generalization guarantees. Note that in Theorem 7, other complexity measures
from learning theory could be used. We chose Rademacher complexity as it provides the tightest bound
among standard complexity measures.
Theorem 7 has practical implications. If the Rashomon set is large, and the smoothness conditions are
obeyed, Theorem 7 shows that many simple-yet-accurate models would exist, prior to actually ﬁnding them.
Knowledge that simple models exist implies it will be worthwhile to actually solve the diﬃcult optimization
problem to ﬁnd a simple model.
Thus, if the Rashomon set is large, we have a guarantee. But how will we know when is the Rashomon
set large? This is what we answer in the next section.
6
Larger Rashomon Ratios Correlate with Similar Performance of
Machine Learning Algorithms, and Good Generalization
We expect that in many real-world applications of machine learning, properties similar to the assumptions
behind our theorems hold, i.e., that large enough Rashomon sets intersect simpler hypothesis spaces in ways
that lead to or explain good performance. This conjecture is diﬃcult to verify theoretically because it is not
a mathematical conjecture about the structure of two speciﬁc function spaces, but a statement about many
function spaces, and how they interact with commonly occurring data sets. Thus, we consider this question
empirically.
Our experiments will demonstrate that, in the case where Rashomon sets are large, two conclusions follow
that are consistent with our theoretical development. First, training performance in simpler hypothesis
spaces is correlated with test performance in the more complex hypothesis spaces (Theorem 5), and second,
that good training performance in a simpler space F1 correlates with good generalization performance of
other models in the more complex space F2.
Most importantly, our experiments suggest an intriguing
alternative to the often diﬃcult computational problem of directly estimating the size of the Rashomon set,
namely that similar performance across a range of algorithms with diﬀerent hypothesis spaces is strongly
correlated with a large Rashomon set.
Now we will describe our experimental setup for arriving at these conclusions.
6.1
Experimental Design
Data sets.
We used 38 machine learning classiﬁcation data sets from the UCI Machine Learning Repository
(Dua and Graﬀ, 2019), among which 16 have categorical features and 22 have real-valued features. The
majority of the data sets are binary classiﬁcation data sets and we adapted the rest to binary classiﬁcation
(as shown in Table 4 in Appendix D) to make importance sampling easier (as discussed in Appendix E).
The number of features varies from 3 to 784, with the majority of the data sets being in the 15–25 feature
range. Appendix D contains a description of the data sets we considered.
10

Deﬁnition of complex hypothesis space.
For these experiments, we will consider F2 to be the union
(Funion) of the hypothesis spaces of ﬁve popular machine learning algorithms: logistic regression (LR),
CART, random forests (RF), gradient boosted trees (GBT), and support vector machines with RBF kernels
(SVM). CART, RF and GBT were regularized by varying the tree depth, the minimum number of samples
required to split a node, the minimum number of samples required to create a leaf node, and the number of
trees in the ensemble. SVMs were tuned by varying the regularization parameter and the kernel coeﬃcient
and LR by varying the regularization parameter. Appendix E discusses the eﬀect of regularization on the
model class. We chose algorithms that search hypothesis spaces of diﬀerent complexity to ensure that these
algorithms produce diverse models. The notion of F2 as a union of hypothesis spaces may seem surprising
at ﬁrst, but it is consistent with how many machine learning practitioners approach problems by running
a collection of machine learning techniques in parallel and comparing the results, creating a de facto union
space. Our experiment has three steps, as follows.
Step 1: Run all machine learning algorithms.
We obtain training and generalization performance
from all algorithms (logistic regression, CART, random forests, gradient boosted trees, and SVM with RBF
kernels) on all data sets.
Step 2: Estimate the size of the Rashomon set.
It is not possible to measure the Rashomon set of such
a complex model space, so we will estimate its size by sampling from an approximating set, which is decision
trees of bounded depth. Decision trees are easy to sample and can reﬁne an input space arbitrarily ﬁnely
as tree depth increases. With suﬃcient depth they can approximate many other types of hypothesis spaces,
including those used by other machine learning methods. Thus, we will measure the size of the Rashomon
set and Rashomon ratio in decision trees of depth seven as a surrogate for measuring these quantities in
F2. The suitability of these trees for this role is an empirical observation about the data sets we have used;
they may not be a suitable surrogate for some other data sets, e.g., imagery data. We measure the size
of the empirical Rashomon ratio as a surrogate for the true Rashomon ratio when referring to Theorem 5.
To estimate the Rashomon ratio of depth seven decision trees, we used importance sampling. The proposal
distribution assigns the correct labels to the leaves of the tree based on the training data. Since the data are
populated on a bounded domain, to grow a tree up to a depth D fully, we make 2D−1 splits. For each data
set and each depth, we average our results over ten folds for data sets with less than 200 points and over
ﬁve folds for data sets with more than 200 points, and we sample 250,000 decision trees per fold. We choose
the Rashomon parameter θ to be 5%, and, therefore, all the models in the Rashomon set have empirical
risk not more than ˆL( ˆf) + 0.05, where ˆL( ˆf) is the lowest achievable empirical risk across all algorithms we
considered. We further discuss experimental setup in Appendix E.
Step 3: See if a large Rashomon Set in Step 2 correlates with performance diﬀerences in
Step 1.
By construction, the hypothesis spaces of each of the machine learning algorithms we consider are
embedded in F2. RF and GBT both enjoy extremely rich hypothesis spaces that are likely close in size to
F2 itself. LR and CART are less expressive than these others, so we will view LR and CART as simpler,
F1 type, hypothesis spaces. Our question to answer is whether a large Rashomon set measured in Step 2
correlates with the functions from F1 (CART, LR) having performance as good as that of F2 (GBT, RF,
SVM) as our theory predicts it will.
6.2
Experimental Results
Figure 2(a) shows the performance of the ﬁve machine learning algorithms on data sets for which the
Rashomon ratio was largest, as measured in the space of decision trees of depth 7. Performance for all data
sets is shown in Figures 14 and 15 in Appendix E. Across the 38 data sets considered, we observe larger
Rashomon ratios led to approximately similar training results across all algorithms (within
∼5% diﬀerence between algorithms). Here, large Rashomon ratios are on the order of 10−37% or 10−38%,
11

LR
CART
RF
GBT
SVM
50
60
70
80
90
100
Accuracy, %
train
test
Credit Card
RRatio
 2.9387E-37 %
LR
CART
RF
GBT
SVM
50
60
70
80
90
100
Accuracy, %
train
test
Voting
RRatio
 2.2168E-37 %
LR
CART
RF
GBT
SVM
50
60
70
80
90
100
Accuracy, %
train
test
Mammographic Masses
RRatio
 1.3667E-37 %
LR
CART
RF
GBT
SVM
50
60
70
80
90
100
Accuracy, %
train
test
HTRU_2
RRatio
 1.7479E-37 %
(a)
LR
CART
RF
GBT
SVM
50
60
70
80
90
100
Accuracy, %
train
test
Banknote
RRatio
 8.9455E-41 %
LR
CART
RF
GBT
SVM
50
60
70
80
90
100
Accuracy, %
train
test
MNIST 4-9
RRatio < 1.1755E-42 %
(b)
LR
CART
RF
GBT
SVM
50
60
70
80
90
100
Accuracy, %
train
test
Monks-2
RRatio < 1.1755E-42 %
LR
CART
RF
GBT
SVM
50
60
70
80
90
100
Accuracy, %
train
test
Eye State
RRatio < 1.1755E-42 %
(c)
Figure 2: (a) Examples of experiments on four data sets showing that larger Rashomon ratios lead to similar
performance of ﬁve machine learning algorithms with regularization. All the algorithms generalize well and
have similar test accuracy. (b)-(c): Examples showing that smaller Rashomon ratios do not necessarily imply
a performance diﬀerence between machine learning algorithms. Even with low Rashomon ratios, algorithms
can be highly accurate and generalize well, as shown in Figure (b). On the other hand, when the Rashomon
ratio is small, sometimes algorithms can perform diﬀerently or fail to generalize, as shown in Figure (c). In
the ﬁgure, test accuracies, training accuracies and the Rashomon ratio are averaged over ten folds. We show
all 38 data sets in the Appendix E.
whereas small Rashomon ratios are 10−40% or less2. Moreover, all of the models chosen by the algorithms,
including simpler F1 type models, generalized well (the diﬀerences between training and test errors are
within ∼5%). These results are consistent with our thesis that larger Rashomon sets lead to the existence
of accurate-yet-simpler models (in agreement with the theory in Section 5.1), and that larger Rashomon sets
lead to better generalization. The results also imply that large Rashomon sets do occur in many data sets,
with the Rashomon eﬀect being large enough to include simpler models in practice (in agreement with Section
5.2).
Interestingly, the converse statement, that similar performance across diﬀerent algorithms should lead to
large Rashomon sets, does not always hold; sometimes, generalization occurs with small Rashomon ratios
(see Figure 2(b)). This observation could be explained in several diﬀerent ways. Mainly, the Rashomon ratio
is not the only driver of good generalization performance. The amount of data is one obvious additional
driver. Appendix F discusses this further. Quality of features is another driver, as discussed in Appendix G.
Our second main result is that in all cases where large Rashomon ratios were observed,
test perfor-
mance was consistent with training performance across algorithms of varying complexity. This
correlation between the size of the Rashomon ratio and consistent generalization performance suggests an
indirect means of assessing the size of the Rashomon ratio as an alternative to the computationally intensive
approach of sampling. When consistent training and test performance across algorithms is observed, this
may indicate a large Rashomon ratio.
One thing we notably did not observe were cases where algorithms did not generalize, performance diﬀered
across algorithms, and the Rashomon set was large. Across all 38 data sets, we did not observe cases where
the Rashomon set was large and performance diﬀered among algorithms.
Figure 2(c) shows small Rashomon sets, where we observe wildly diﬀerent performance across algorithms,
where sometimes the models generalize and sometimes they do not. We show one example of each of these
cases in Figure 2(c). Our theory does not apply to the case of small Rashomon sets, and thus there is no
guarantee for such data sets.
2For other data sets and other metrics of measuring the Rashomon set, the results might be diﬀerent.
12

7
Rashomon Sets in the Presence of Noise
We have seen that we can empirically determine whether a data set is likely to have a large Rashomon set: as
we showed, we simply run many algorithms, and if they all perform similarly and generalize, there could be
a large Rashomon set. But what about before examining the data? Could we know, just from understanding
what kind of data set it is, whether it is likely to have a large Rashomon set? We aim to answer this now.
A typical reason given for “underspeciﬁcation” D’Amour et al. (2020) (i.e., a large number of approximately-
equally good models, a large Rashomon set) is the presence of substantial noise in the data. Intuitively, for
data whose outcomes are essentially a random guess, it makes sense that no model would perform well,
and many models would be equally poor. But what about more interesting cases? Does this intuition still
hold? First, as we have shown above, large Rashomon sets exists for non-noisy data sets as well (see Figure
2 Voting and HTRU 2 data sets); it is not just noise that determines the Rashomon set. Second, it is not
true that the Rashomon set always gets much larger under noise. In fact, if we add random classiﬁcation
noise Angluin and Laird (1988); Natarajan et al. (2013) (each label is ﬂipped independently with some small
probability), it is possible that the Rashomon set does not change at all. This is because the error rate of
all models in the Rashomon set (assuming they are all better than random guessing) increases by the same
amount in expectation. At least, as we show in Theorem 8, the size of the true Rashomon set does not
decrease if we add random classiﬁcation noise.
Theorem 8 (Expected size of the true Rashomon set cannot decrease under random classiﬁcation noise).
Consider hypothesis space F, data distribution D = X × Y, where, as before, X ∈Rp, and Y ∈{−1, 1}. Let
ρ ∈(0, 1
2) be a probability with which each label yi is ﬂipped independently, and Dρ denotes the noisy version
of D. If the loss function is φ(f(x), y) = 1[f(x)̸=y], then in expectation, the true Rashomon set over D is a
subset of the true Rashomon set over Dρ, RsetD(F, γ) ⊆RsetDρ (F, γ).
In Theorem 8 we have shown that the size of the true Rashomon set does not decrease when adding
random classiﬁcation noise, but to prove that RsetD(F, γ) ⊂RsetDρ (F, γ), we would need at least one model
¯f such that ¯f ̸∈RsetD(F, γ), yet ¯f ∈RsetDρ (F, γ), and such a model may not actually exist; in fact, if all
models have increased error rates when noise is added, it does not.
We still are left with ﬁnding a scenario where noise does impact the size of the Rashomon set in order to
provide some proof to the intuition. Let us consider the setting of linear (Gaussian) discriminant analysis,
where the data arise from two Gaussians, one with positive labels and one with negative labels. Instead of
increasing label noise, we will increase feature noise by increasing the variances or changing the means of the
Gaussians, making the two distributions overlap. In this case, will the Rashomon set increase in size? The
answer to this question is conjectured in Conjecture 9.
Conjecture 9 (The Rashomon set can increase with feature noise). Consider data distribution D = X × Y,
where, X ∈R, Y ∈{−1, 1}, and classes are balanced P(Y = −1) = P(Y = 1) and generated by Gaussian
distributions P(X|Y = −1) = N(µ1, σ2), P(X|Y = 1) = N(µ2, σ2), where 0 ≤µ1 < µ2. For the hypothesis
space F = {f : f ∈(β1, β2)}, where (µ1, µ2) ⊂(β1, β2), β1 ≪µ1, and µ2 ≪β2, and the Rashomon parameter
γ > 0:
(I) The volume of the Rashomon set is V(Rsetσ(F, γ)) = |f e1
σ −f e2
σ |, where f e1
σ
and f e2
σ
are the two
solutions to Eqn. (2), where Φ is the CDF of the standard normal:
2Φ
µ2 −µ1
2σ

−Φ
µ2 −f
σ

−Φ
f −µ1
σ

= γ.
(2)
(II) We conjecture that3 for F = {f : f ∈(µ1, µ2)}, as we add feature noise to the data set by increasing
the standard deviation σ, for all σ such that σ > ˜σ = µ2−µ1
2
√
2 , the volume of the Rashomon set increases
as a function of σ.
3The hypothesis space for Part II conservatively includes all reasonable candidates for the empirical risk minimizer. In other
words, we assume that decision boundary can be anywhere between the means of the two distributions.
13

Small Rashomon Ratio
Large Rashomon ratio
Higher Accuracy
Large Rashomon ratio 
Lower Accuracy
(a)
   (b)
Figure 3: (a) Dependence of the Rashomon ratio on noise σ for the two Gaussians example in Conjecture 9.
When σ > ˜σ, as we add more noise, the size of the Rashomon set increases. (2). (b) Scatter plot of the log
of Rashomon ratio versus the maximum accuracy across ﬁve diﬀerent algorithms for 38 data sets in Section
6. We observe larger Rashomon ratios in both noisy and non-noisy data sets.
(III) Consider the setting where σ = 1 for both Gaussians, and we add or remove noise by moving the
means µ1 and µ2 of the Gaussians towards or away from each other. For any γ > 0, the volume of
the Rashomon set is minimized when µ2 ≈µ1 + 2. Moving the Gaussians either away from or towards
each other increases the volume of the Rashomon set.
This conjecture is not a theorem because there is no analytical solution to the minimizer of the volume
of the Rashomon set; the calculations are quite complex, involving diﬀerences of the CDF values of diﬀerent
Gaussians. However, all parts of the conjecture have been fully checked numerically. In part (II), we use an
analytical derivation and exhaustive numerical computations to show that the derivatives of the left side of
Eqn. (2) are either positive or negative sign. For part (III), we transpose the left Gaussian to N(0, 1) to form
a canonical problem in which all possible solutions can be computed numerically. We exhaustively search
over the range of γ, ﬁnding the optimal µ2 and volume of the Rashomon set for each γ. We ﬁnd that µ2 is
very close to 2 for all γ. We discuss this in Appendix I.
This conjecture suggests that data that are approximately distributed according to two normal
distributions, where the positive and negative normal distributions substantially overlap, will
have a large Rashomon set. Figure 3(a) shows the dependence of the Rashomon set on the noise level
σ for µ1 = 1, µ2 = 6 and σ ∈[0.2, 4]. Figure 3(b) plots maximum accuracy versus Rashomon ratio for 38
data sets considered in Section 6. These ﬁgures indicate that large Rashomon sets occur both in noisy and
non-noisy data.
There can be many data sets with characteristics as in Conjecture 9.
For example, let us consider
criminal recidivism data, whose Rashomon sets have been studied Fisher et al. (2019); Dong and Rudin
(2020) and that admit simple-yet-accurate models Zeng et al. (2017); Rudin et al. (2020). Each data point
is generated based on a set of random events happening in the world; whether someone enters a job training
program, whether someone associates with criminal associates after release, and whether someone commits
a crime each day are all random variables whose random eﬀects are cumulative over time, and thus could be
modeled by Gaussians by the central limit theorem. By this logic, we would expect many criminal recidivism
prediction problems to admit large Rashomon sets. Other high-stakes predictions such as loan defaults may
have similar characteristics.
In a sense, this full analysis paints a much clearer picture as to why such problems admit simple yet
similarly accurate models: their distributions are approximately Gaussian with signiﬁcant overlap, such
overlap leads to large Rashomon sets, and large Rashomon sets lead to the existence of simple yet similarly
accurate models.
14

8
Conclusion and Implications
We have proposed Rashomon sets and ratios as another perspective on the relationship between hypothesis
spaces and data sets, and we have provided initial theoretical and experimental results showing that this is a
unique perspective that may help explain some phenomena observed in practice. More speciﬁcally, the main
conclusions include: (1) Large Rashomon sets can embed models from simpler hypothesis spaces (Section
5); (2) Similar performance across diﬀerent machine learning algorithms may correlate with large Rashomon
sets (Section 6); (3) Large Rashomon sets correlate with existence of models that have good generalization
performance (Section 6); (4) The Rashomon ratio is a measure of a learning problem’s complexity (Section 4),
and that data that approximately arise from overlapping Gaussian distributions tend to have large Rashomon
sets (Section 7).
Consider a researcher conducting a standard set of machine learning experiments in which the performance
of several diﬀerent algorithms are compared, and generalization is assessed. In the possible scenario where all
algorithms perform similarly, and when their models tend to generalize well on validation data, the learning
problem is likely to have a large Rashomon set. Based on the result in Section 5, simpler models are likely to
exist in a large Rashomon set. If the researcher is interested in simpler models, they can search the simpler
function class F1, a subset of the larger class F2, to locate simpler models within it. While optimizing
for simplicity or interpretability constraints is usually much more computationally expensive than running
standard machine learning algorithms, our thesis is that this search would be likely to succeed in the presence
of a large Rashomon set. In the converse case, if the researcher’s algorithms perform diﬀerently from each
other, the researcher might then select a more complex model class that achieves better performance yet
does not overﬁt. Further, if the researcher knows that the data are likely to have arisen from overlapping
Gaussian distributions, the researcher could assume that it is worthwhile to search for a simple model that
performs well.
References
Dana Angluin and Philip Laird. 1988. Learning from noisy examples. Machine Learning 2, 4 (1988), 343–370.
Peter L Bartlett, Olivier Bousquet, Shahar Mendelson, et al. 2005. Local Rademacher complexities. The
Annals of Statistics 33, 4 (2005), 1497–1537.
Peter L Bartlett and Shahar Mendelson. 2002. Rademacher and Gaussian complexities: Risk bounds and
structural results. Journal of Machine Learning Research 3, Nov (2002), 463–482.
Mikhail Belkin, Daniel Hsu, Siyuan Ma, and Soumik Mandal. 2019. Reconciling modern machine-learning
practice and the classical bias–variance trade-oﬀ. Proceedings of the National Academy of Sciences 116,
32 (2019), 15849–15854.
Olivier Bousquet and André Elisseeﬀ. 2002.
Stability and generalization.
Journal of Machine Learning
Research 2, Mar (2002), 499–526.
Leo Breiman et al. 2001. Statistical modeling: The two cultures (with comments and a rejoinder by the
author). Statist. Sci. 16, 3 (2001), 199–231.
Christopher JC Burges. 1998. A tutorial on support vector machines for pattern recognition. Data Mining
and Knowledge Discovery 2, 2 (1998), 121–167.
Feilong Cao, Tingfan Xie, and Zongben Xu. 2008. The estimate for approximation error of neural networks:
A constructive approach. Neurocomputing 71, 4-6 (2008), 626–630.
Pratik Chaudhari, Anna Choromanska, Stefano Soatto, Yann LeCun, Carlo Baldassi, Christian Borgs, Jen-
nifer Chayes, Levent Sagun, and Riccardo Zecchina. 2019. Entropy-SGD: Biasing gradient descent into
wide valleys. Journal of Statistical Mechanics: Theory and Experiment 2019, 12 (2019), 124018.
15

Beau Coker, Cynthia Rudin, and Gary King. 2021. A theory of statistical inference for ensuring the robustness
of scientiﬁc results. Management Science 67 (2021), 5969–6627. Issue 10.
Corinna Cortes and Vladimir Vapnik. 1995. Support-vector networks. Machine learning 20, 3 (1995), 273–
297.
Amanda Coston, Ashesh Rambachan, and Alexandra Chouldechova. 2021. Characterizing fairness over the
set of good models under selective labels. In Proceedings of the 38th International Conference on Machine
Learning (Proceedings of Machine Learning Research).
Alexander D’Amour, Katherine Heller, Dan Moldovan, Ben Adlam, Babak Alipanahi, Alex Beutel, Christina
Chen, Jonathan Deaton, Jacob Eisenstein, Matthew D Hoﬀman, et al. 2020. Underspeciﬁcation presents
challenges for credibility in modern machine learning. arXiv preprint arXiv:2011.03395 (2020).
Oleg Davydov. 2011. Algorithms and error bounds for multivariate piecewise constant approximation. In
Approximation Algorithms for Complex Systems. Vol. 3. Springer, Berlin, Heidelberg, 27–45.
Ronald A DeVore. 1998. Nonlinear approximation. Acta Numerica 7 (1998), 51–150.
Laurent Dinh, Razvan Pascanu, Samy Bengio, and Yoshua Bengio. 2017.
Sharp minima can generalize
for deep nets. In Proceedings of the 34th International Conference on Machine Learning (Proceedings of
Machine Learning Research, Vol. 70). 1019–1028.
Jiayun Dong and Cynthia Rudin. 2020. Exploring the cloud of variable importance for the set of all good
models. Nature Machine Intelligence 2, 12 (2020), 810–824.
Dheeru Dua and Casey Graﬀ. 2019. UCI Machine Learning Repository. University of California, Irvine,
School of Information and Computer Sciences.
Aaron Fisher, Cynthia Rudin, and Francesca Dominici. 2019. All models are wrong, but many are useful:
Learning a variable’s importance by studying an entire class of prediction models simultaneously. Journal
of Machine Learning Research 20, 177 (2019), 1–81.
Sepp Hochreiter and Jürgen Schmidhuber. 1997. Flat minima. Neural Computation 9, 1 (1997), 1–42.
Sham M. Kakade, Karthik Sridharan, and Ambuj Tewari. 2008. On the complexity of linear prediction: risk
bounds, margin bounds, and regularization. In Proceedings of the 21st International Conference on Neural
Information Processing Systems (Vancouver, British Columbia, Canada) (NIPS’08). Curran Associates
Inc., Red Hook, NY, USA, 793–800.
Michael Kearns and Dana Ron. 1999. Algorithmic stability and sanity-check bounds for leave-one-out cross-
validation. Neural Computation 11, 6 (1999), 1427–1453.
Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, and Ping Tak Peter Tang.
2016. On large-batch training for deep learning: Generalization gap and sharp minima. arXiv preprint
arXiv:1609.04836 (appeared at ICLR 2017) (2016).
Vladimir Koltchinskii and Dmitry Panchenko. 2002. Empirical margin distributions and bounding the gen-
eralization error of combined classiﬁers. The Annals of Statistics 30, 1 (2002), 1–50.
John Langford and John Shawe-Taylor. 2002. PAC-Bayes & margins. In Proceedings of the 15th International
Conference on Neural Information Processing Systems (NIPS’02). MIT Press, Cambridge, MA, USA,
439–446.
Guillaume Lecué. 2011. Interplay between concentration, complexity and geometry in learning theory with
applications to high dimensional data analysis. Ph.D. Dissertation. Université Paris-Est.
16

Benjamin Letham, Portia A. Letham, Cynthia Rudin, and Edward Browne. 2016. Prediction uncertainty
and optimal experimental design for learning dynamical systems. Chaos 26, 6 (2016).
Gábor Lugosi and Marten Wegkamp. 2004. Complexity regularization via localized random penalties. The
Annals of Statistics 32, 4 (2004), 1679–1697.
David Madras, James Atwood, and Alex D’Amour. 2019. Detecting underspeciﬁcation with local ensembles.
arXiv preprint arXiv:1910.09573 (appeared at ICLR 2020 under the title “Detecting Extrapolation with
Local Ensembles”) (2019).
Naresh Manwani and PS Sastry. 2013. Noise tolerance under risk minimization. IEEE Transactions on
Cybernetics 43, 3 (2013), 1146–1151.
Charles Marx, Flavio Calmon, and Berk Ustun. 2020. Predictive Multiplicity in Classiﬁcation. In Proceedings
of the 37th International Conference on Machine Learning (Proceedings of Machine Learning Research,
Vol. 119). PMLR, 6765–6774.
Nicolai Meinshausen and Peter Bühlmann. 2010. Stability selection. Journal of the Royal Statistical Society:
Series B (Statistical Methodology) 72, 4 (2010), 417–473.
Shahar Mendelson. 2003.
A few notes on statistical learning theory. In Advanced Lectures on Machine
Learning. Springer, 40 pages.
Preetum Nakkiran, Gal Kaplun, Yamini Bansal, Tristan Yang, Boaz Barak, and Ilya Sutskever. 2021. Deep
double descent: Where bigger models and more data hurt. Journal of Statistical Mechanics: Theory and
Experiment 2021, 12 (2021), 124003.
Nagarajan Natarajan, Inderjit S Dhillon, Pradeep K Ravikumar, and Ambuj Tewari. 2013.
Learning
with noisy labels. In Advances in Neural Information Processing Systems, C. J. C. Burges, L. Bottou,
M. Welling, Z. Ghahramani, and K. Q. Weinberger (Eds.), Vol. 26. Curran Associates, Inc.
Daniel Nevo and Ya’acov Ritov. 2017. Identifying a minimal class of models for high-dimensional data. The
Journal of Machine Learning Research 18, 1 (2017), 797–825.
DJ Newman and TJ Rivlin. 1976. Approximation of monomials by lower degree polynomials. Aequationes
Mathematicae 14, 3 (1976), 451–455.
Ramamohan Paturi. 1992. On the degree of polynomials that approximate symmetric boolean functions
(preliminary version). In Proceedings of the Twenty-Fourth Annual ACM Symposium on Theory of Com-
puting (Victoria, British Columbia, Canada) (STOC ’92). Association for Computing Machinery, New
York, NY, USA, 468–474.
William H Rogers and Terry J Wagner. 1978. A ﬁnite sample distribution-free performance bound for local
discrimination rules. The Annals of Statistics 6, 3 (1978), 506–514.
Cynthia Rudin. 2019. Stop explaining black box machine learning models for high stakes decisions and use
interpretable models instead. Nature Machine Intelligence 1, 5 (2019), 206–215.
Cynthia Rudin, Caroline Wang, and Beau Coker. 2020. The Age of Secrecy and Unfairness in Recidivism
Prediction. Harvard Data Science Review 2, 1 (31 1 2020).
Robert E Schapire, Yoav Freund, Peter Bartlett, Wee Sun Lee, et al. 1998. Boosting the margin: A new
explanation for the eﬀectiveness of voting methods. The Annals of Statistics 26, 5 (1998), 1651–1686.
Nathan Srebro, Karthik Sridharan, and Ambuj Tewari. 2010. Smoothness, Low Noise and Fast Rates. In
Advances in Neural Information Processing Systems, Vol. 23. Curran Associates, Inc., 2199–2207.
17

Theja Tulabandhula and Cynthia Rudin. 2013. Machine learning with operational costs. The Journal of
Machine Learning Research 14, 1 (2013), 1989–2028.
Theja Tulabandhula and Cynthia Rudin. 2014a.
On combining machine learning with decision making.
Machine Learning (ECML-PKDD journal track) 97, 1-2 (2014), 33–64.
Theja Tulabandhula and Cynthia Rudin. 2014b. Robust optimization using machine learning for uncertainty
sets. arXiv preprint arXiv:1407.1097 (2014).
Berk Ustun and Cynthia Rudin. 2016.
Supersparse linear integer models for optimized medical scoring
systems. Machine Learning 102, 3 (2016), 349–391.
VN Vapnik and A Ya Chervonenkis. 1971. On the Uniform Convergence of Relative Frequencies of Events
to Their Probabilities. Theory of Probability and its Applications 16, 2 (1971), 264.
Vladimir N Vapnik. 1995. The Nature of Statistical Learning Theory. Springer.
Christopher S Wallace and David M Boulton. 1968. An information measure for classiﬁcation. Comput. J.
11, 2 (1968), 185–194.
Jiaming Zeng, Berk Ustun, and Cynthia Rudin. 2017.
Interpretable classiﬁcation models for recidivism
prediction. Journal of the Royal Statistical Society: Series A (Statistics in Society) 180, 3 (2017), 689–
722.
Ding-Xuan Zhou. 2002.
The covering number in learning theory.
Journal of Complexity 18, 3 (2002),
739–767.
18

A
Diﬀerence between ﬂat minima and Rashomon set
Figure 4 illustrates diﬀerences between volume ϵ-ﬂatness and the Rashomon set.
Parameter space
Loss
(a) Volume ϵ-ﬂatness
Hypothesis space
Loss
(b) The Rashomon set
Figure 4: Diﬀerence between volume ϵ-ﬂatness as deﬁned in Dinh et al. (2017) and the Rashomon set. The
red line represents the volume ϵ-ﬂatness in (a), and the Rashomon set in (b). The volume of the Rashomon
set is the sum of lengths of red lines in (b). The height of the shaded area represents (a) the parameter
ϵ or the 2σ-sharpness, and (b) the Rashomon parameter θ. Volume ϵ-ﬂatness is deﬁned by a connected
component in a parameter space for a given local minimum, while the Rashomon set is deﬁned with respect
to an empirical risk minimizer over the full hypothesis space F and may contain models from multiple local
minima. Rashomon sets are also deﬁned for discrete spaces.
B
Connection to Simplicity Measures
We will use demonstrations to show the diﬀerences between the Rashomon ratio and other complexity
measures mentioned in Section 4. However, ﬁrst we discuss how to analytically compute the Rashomon ratio
for ridge regression since the demonstration of the diﬀerence between algorithmic stability and Rashomon
ratio relies on it.
B.1
Analytical Calculation of Rashomon Ratio for Ridge Regression
A special case of when the Rashomon ratio can be computed in closed form in a parameter space is ridge
regression. For a space of linear models FΩ= {ωT x, ω ∈Rp}, ridge regression chooses a parameter vector
by minimizing the penalized sum of squared errors for a training data set S = [X, Y ]:
min
ω
ˆL(ω) = min
ω (Xω −Y )T (Xω −Y ) + CωT ω,
(3)
where the optimal solution of the ridge regression estimator is ˆω = (XT X + CIp)−1XT Y.
Geometrically, the optimal solution to ridge regression will be a parameter vector that corresponds to the
intersection of ellipsoidal isosurfaces of the sum of squares term and a hypersphere centered at the origin,
with the regularization parameter C determining the trade oﬀbetween the loss and the radius of the sphere.
More generally, isosurfaces of the ridge regression loss function are ellipsoids, and the volume of such an
ellipsoid corresponds to the volume of the Rashomon set. For a hypothesis space with uniform prior and
volume function V, the Rashomon ratio is V( ˆ
Rset(FΩ,θ))
V(FΩ)
. Using the geometric intuition above, we compute
the Rashomon ratio in the parameter space by the following theorem:
Theorem 10 (Rashomon ratio for ridge regression). For a parametric hypothesis space of linear models
FΩ= {fω(x) = ωT x, ω ∈Rp} with uniform prior and a data set S = X × Y , the Rashomon set ˆRset(FΩ, θ)
of ridge regression is an ellipsoid, containing vectors ω such that:
(ω −ˆω)T XT X + CIp
θ
(ω −ˆω) ≤1,
19

and the Rashomon ratio can be computed as:
ˆRratio(FΩ, θ) = J(θ, p)
V(FΩ)
p
Y
i=1
1
p
σ2
i + C
,
(4)
where σi are singular values of matrix X, J(θ, p) = πp/2θp/2
Γ(p/2+1) and Γ(·) is the gamma function.
Proof. Consider all models fω ∈FΩfrom the Rashomon set ˆRset(FΩ, θ). Then by Deﬁnition 1 we get:
ˆL(X, Y, ω) ≤ˆL(X, Y, ˆω) + θ.
(5)
Using XT Y = (XT X + CIp)ˆω from the optimal solution of the ridge regression estimator ˆω = (XT X +
CIp)−1XT Y , and expanding the diﬀerence between empirical risks we have:
θ ≥ˆL(X, Y, ω) −ˆL(X, Y, ˆω)
= (Xω −Y )T (Xω −Y ) + CωT ω −(X ˆω −Y )T (X ˆω −Y ) −C ˆωT ˆω
= ωT XT Xω −2ωT XT Y + CωT ω −ˆωT XT X ˆω + 2ˆωT XT Y −C ˆωT ˆω
= ωT XT Xω −2ωT (XT X + CIp)ˆω + CωT ω −ˆωT XT X ˆω
+ 2ˆωT (XT X + CIp)ˆω −C ˆωT ˆω
= ωT XT Xω + CωT ω −2ωT (XT X + CIp)ˆω + ˆωT XT X ˆω + C ˆωT ˆω
= ωT (XT X + CIp)ω −2ωT (XT X + CIp)ˆω + ˆωT (XT X + CIp)ˆω
= (ω −ˆω)T (XT X + CIp)(ω −ˆω).
Therefore the Rashomon set is an ellipsoid centered at ˆω:
(ω −ˆω)T XT X + CIp
θ
(ω −ˆω) ≤1.
By the formula of the volume of a p-dimensional ellipsoid, the volume of the Rashomon set can be computed
as:
V( ˆRset(FΩ, θ)) =
πp/2θp/2
Γ(p/2 + 1)
p
Y
i=1
1
p
σ2
i + C
,
where σi are singular values of X.
Since we assume a uniform prior on FΩ, V(FΩ) is the volume of a box (or other closed region) con-
taining the plausible values of Ω.
Therefore, the Rashomon ratio is ˆRratio(FΩ, θ) =
V( ˆ
Rset(FΩ,θ))
V(FΩ)
=
J(θ,p)
V(FΩ)
Qp
i=1
1
√
σ2
i +C , where J(θ, p) = πp/2θp/2
Γ(p/2+1).
■
Interestingly, from Theorem 10, it follows that for ridge regression, the Rashomon ratio depends on the
feature space only and does not depend on the regression targets Y . Indeed, assume that every parameter
vector ω such that fω ∈ˆRset(FΩ, θ) can be represented as ω = ˆω + δ. By a simple transformation, we
have that ˆL(fω) −ˆL(fˆω) = δT XT Xδ, meaning that if we take a step in parameter space, the empirical risk
diﬀerence will depend only on the feature space and the step itself, and not on the targets of the problem.
This observation can help us choose the parameter θ as θ = δT XT Xδ if we want to ensure some dependence
between the optimal model ˆω and a model of interest ω. Then, by choosing the direction as δ = ω −ˆω, we
can compute the Rashomon parameter θ.
For other algorithms, the Rashomon ratio generally depends on the targets; in that sense, ridge regression
is unusual.
20

B.2
Algorithmic Stability
The main motivation for algorithmic stability theory is to ensure robustness of a learning algorithm. Fol-
lowing Bousquet and Elisseeﬀ(2002), we deﬁne the hypothesis stability of a learning algorithm as follows.
Deﬁnition 11 (Hypothesis stability). A learning algorithm A has β hypothesis stability with respect to the
loss l if for all i ∈{1, ..., n},
ES,z[|l(fS, z) −l(fS\i, z)|] ≤β,
where β ∈R+, hypothesis fS is learned by an algorithm A on a data set S, loss l(fS, z) = φ(fS(x), y) for
z = (x, y), data set S = {z1, ...zn}, and S\i is modiﬁed from the training data by removing the ith element
of the data set: S\i = {z1, ..., zi−1, zi+1, ...zn}.
The Rashomon ratio is fundamentally diﬀerent from hypothesis stability, in case of linear least squares
regression (which is discussed in Section B.1). This is formalized in Theorem 2.
Theorem 2 (Rashomon ratio is not algorithmic stability). Consider a distribution PX over a discrete
domain X = {x1, ...xN} and a learning algorithm A that minimizes the sum of squares loss : ∥Xω −Y ∥2
2.
for a linear hypothesis space FΩ. For any λ > 0, there exist joint distributions PX,Y1 and PX,Y2 where for X
drawn i.i.d. from PX, Y1 drawn from PY1|X over Y | X, and Y2 drawn from PY2|X over Y | X, the expected
Rashomon ratios are the same:
EPX,Y1[ ˆRratioS1(FΩ, θ)] = EPX,Y2[ ˆRratioS2(FΩ, θ)],
yet hypothesis stability constants are diﬀerent by our arbitrarily chosen value of λ: ˜β2 −˜β1 ≥λ, where S1
and S2 denote data sets S1 = [X, Y1] and S2 = [X, Y2], ˜β1 is the hypothesis stability coeﬃcient of algorithm
A for distribution PX,Y1 and ˜β2 is the hypothesis stability coeﬃcient for distribution PX,Y2.
Proof. Let us create our distribution.
Consider the least squares regression minω
Pn
i=1 l(ω, zi)2, where
ω ∈Rp, and loss l(ω, z) = φ(ωT x, y) for z = (x, y). For the marginal distribution PX and X = [x1, ..., xn]
drawn i.i.d. from PX, we design distributions PY1|X and PY2|X as:
PY1|X(y = 0|x) = 1 ∀x ∈X,
PY2|X(y = 0|x ̸= x0) = 1, PY2|X(y = 0|x = x0) = 0.5,
PY2|X(y = H|x = x0) = 0.5,
where x0 ∈{x1, ..., xN} is some ﬁxed point with a positive probability PX(x0) and we deﬁne H ∈R later.
That is, the two conditional distributions have y = 0 except when x = x0 for Y2, when it is H with probability
1/2.
As a ﬁrst part of the proof, we show that the algorithmic stability constants are diﬀerent. According to
the deﬁnition of algorithmic stability, for PX,Y1 we have:
ES1,z[|l(fS1, z) −l(fS\i
1 , z)|] = 0 = ˜β1,
and for distribution PX,Y2:
ES2,z
hl(fS2, z) −l(fS\i
2 , z)

i
=
X
S2,z∼PX,Y2
PX,Y2(S2)PX,Y2(z)
×
l(fS2, z) −l(fS\i
2 , z)

≥PX,Y2(Ss
2)PX,Y2(zs)
l(f s
S2, zs) −l(fSs,\i
2
, zs)
 ,
21

where Ss
2, zs is a special draw such that zs = (x0, H), and where Ss
2 includes one point at (x0, H), one point
at (x0, 0), and the rest at other values (x, 0). Since the domain X is discrete, the probabilities of a special
draw are:
PX,Y2(zs) = 1
2Bin(1, n, PX(x0)),
PX,Y2(Ss
2) = 1
4Bin(1, n, PX(x0))2Bin(n −2, n, 1 −PX(x0)),
where Bin(k, n, pk) =
 n
k

pk
k(1 −pk)(n−k) is a binomial coeﬃcient, namely the probability of getting exactly
k successes from n trials, where each trial has a probability of success pk. Denote P(Ss
2,zs) as the probability
of getting a special draw, then P(Ss
2,zs) = PX,Y2(Ss
2)PX,Y2(zs).
If Ss
2 contains only two points z1 = (x0, H) and z2 = (x0, 0), the loss diﬀerence |l(fSs
2, zs) −l(fSs,\i
2
, zs)|
evaluated at zs for all i will be at least
H2
4 .
To see this, note that the optimal function’s value at x0
is: fSs
2(x0) =
H
2 , the optimal function’s value at x0 after we remove the ﬁrst point is fSs,\1
2
(x0) = 0,
and the optimal function’s value at x0 after removing the second point is fSs,\2
2
(x0) = H.
Therefore,
l(fSs
2, zs) =
H2
4 , l(fSs,\1
2
, zs) = H2, l(fSs,\2
2
, zs) = 0. And we get that |l(fSs
2, zs) −l(fSs,\1
2
, zs)| =
3H2
4 ,
|l(fSs
2, zs) −l(fSs,\2
2
, zs)| = H2
4 . As we add the rest of the points (xi, 0) to the data set Ss
2, the loss diﬀerence
(from changing fSs
2(zs) to fSs,\i
2
(zs)) in the special draw case will only increase. Therefore for all i:
|l(fSs
2, zs) −l(fSs,\i
2
, zs)| ≥H2
4 .
If we choose H such that H > 2
√
λ
 P(Ss
2,zs)
−1/2, then from the deﬁnition of algorithmic stability we have:
˜β2 ≥ES2,z
hl(fS2, z) −l(fS\i
2 , z)

i
≥P(Ss
2,zs)
H2
4
> λ.
Therefore for any given λ we get that
 ˜β1 −˜β2
 > λ. This proves that the hypothesis stability constants are
diﬀerent and completes the ﬁrst part of the proof.
We now need to prove that the expected Rashomon ratios are the same, which will constitute the second
part of the proof. The Rashomon ratio for the hypothesis space FΩof linear models does not depend on
targets and can be calculated as in (4) for both S1 and S2. Therefore the expected Rashomon ratios are the
same:
EPX,Y1[ ˆRratioS1(FΩ, θ)] = EPX,Y2[ ˆRratioS2(FΩ, θ)].
Thus, both halves of our proof are complete.
■
B.3
Geometric Margin
For the parametric hypothesis space of linear models FΩ= {f : f(x) = ωT x, ω ∈Rp} and binary classiﬁca-
tion, denote d+ and d−as the shortest distances from a decision boundary to the closest points with targets
y = 1 and y = −1 respectively. Then the margin d is a sum of these distances d = d+ + d−(Burges, 1998).
Moreover, for the model fˆω that maximizes the margin, the margin width is
2
∥ˆω∥2 .
Intuitively both the Rashomon ratio and the width of the geometric margin are data-dependent and show
how expressive the hypothesis space is with respect to a given data set. However, the margin depends on
support vectors while the Rashomon set depends on the full data set. Theorem 3 summarizes this idea.
Theorem 3 (Rashomon ratio is not the geometric margin). For any ﬁxed 0 < λ < 1, there exists a ﬁxed
hypothesis space FΩ, a Rashomon parameter θ, and there exist two data sets S1 and S2 with the same
empirical risk minimizer ˆf ∈FΩsuch that the geometric margin d is the same for both data sets, yet the
Rashomon ratios are diﬀerent:
| ˆRratioS1(FΩ, θ) −ˆRratioS2(FΩ, θ)| > λ.
22

0.0
0.2
0.4
0.6
0.8
1.0
x1
0.0
0.2
0.4
0.6
0.8
1.0
x2
d
a
a
A
B
(a) Structure of S1
0.0
0.2
0.4
0.6
0.8
1.0
x1
0.0
0.2
0.4
0.6
0.8
1.0
x2
d
a
a
C
D
(b) Structure of S2
0.0
0.2
0.4
0.6
0.8
1.0
x1
0.0
0.2
0.4
0.6
0.8
1.0
x2
(c) ˆRratio(F Ω, θ) = α
β
0.0
0.2
0.4
0.6
0.8
1.0
1.2
x1
0.0
0.2
0.4
0.6
0.8
1.0
1.2
x2
d
1
(d) ˆRratioS1 (F Ω, 0) =
α1
π/2
0.0
0.2
0.4
0.6
0.8
1.0
1.2
x1
0.0
0.2
0.4
0.6
0.8
1.0
1.2
x2
d
2
(e) ˆRratioS2 (F Ω, 0) =
α2
π/2
Figure 5: An illustration of diﬀerent Rashomon ratios with identical geometric margins. (a) and (b) show
the data sets S1 and S2 with identical margin d. The black line in (d) and (e) shows the optimal model, the
shaded region in (c), (d), and (e) indicates the Rashomon set ˆRset(FΩ, 0) with its boundaries represented
by green lines. The hypothesis space consists of all origin-centered linear models that intersect the zero-one
hypercube, where data reside. (c) shows that the Rashomon ratio can be computed as a ratio of angles
α (represents the Rashomon set) and β (represents the hypothesis space). (d) and (e) illustrate that data
sets S1 and S2 are represented by diﬀerent angles α1 and α2 and therefore have diﬀerent Rashomon ratios.
Figure is best seen in color.
Proof. Consider two-dimensional separable data, X ∈[0, 1]2, and a parametrized hypothesis space of origin-
centered linear models:
F = {ωT x, ω = (k, −1), x ∈R2, k ∈R}.
Consider also 0-1 loss φω(x, y) =
1[y=sign(ωT x)] and an empirical risk minimizer ˆf = fˆω that maximizes the geometric margin. Since the
data are populated in a [0, 1]2 hypercube, as a hypothesis space we will consider all models that intersect
the unit-hypercube.
For some positive constant a ∈(0, 1) that we choose later, consider the following regions of the feature
space:
A = {x1 ∈[0, 1 −a), x2 > x1 + (1 −2a)},
B = {x1 ∈(a, 1], x2 < x1 −(1 −2a)},
C = {x1 ∈[0, a), x2 ∈(1 −a, 1]},
D = {x1 ∈(1 −a, 1], x2 ∈[0, a)}.
Construct data set S1, such that S1 = {(xA, 1) ∪(xB, −1) ∪(xs1
S1, 1) ∪(xs2
S1, −1)}, where xA ∈A is any
sample from the region A, xB ∈B is any sample from the region B, xs1
S1 and ss2
S1 are special points for the
data set S1 such that xs1
S1 = [1 −2a, 1] and xs2
S1 = [1, 1 −2a]. Please see Figure 5a for details.
23

Construct data set S2, such that S2 = {(xC, 1) ∪(xD, −1) ∪(xs1
S2, 1) ∪(xs2
S2, −1)}, where xC ∈C is any
sample from the region C, xD ∈D is any sample from the region D, xs1
S2 and xs2
S2 are special points for the
data set S2 such that xs1
S2 = [a, 1 −a] and xs2
S2 = [1 −a, a]. Please see Figure 5b for details.
Note that the data sets we considered have the same width for the geometrical margin d =
√
2(2a −1)
(see Figures 5a, 5b). Now, we are left to show that the Rashomon ratios are diﬀerent.
For the hypothesis space of origin-centered lines we have a unique parameterization and a one-to-one
correspondence between an actual model and its parameterization. Therefore, if the Rashomon set is a
single connected component, an angle α between the two most distant models in the Rashomon set gives us
some information about the size of the Rashomon set. In particular, we can compute the Rashomon ratio as
a ratio of the angle α that represents the Rashomon set and the angle β that corresponds to the hypothesis
space as shown on Figure 5c. Since the hypothesis space is deﬁned on the unit-hypercube, β = π/2 and for
the Rashomon parameter θ = 0 the Rashomon ratio is:
ˆRratio(F, 0)) = α
β =
2 maxf∈ˆ
Rset(FΩ,0) |arctan(fˆω) −arctan(fω)|
π/2
.
For data sets S1 and S2 Figures 5d and 5e show the Rashomon set and angles α1 and α2 that represent the
volume of the Rashomon set. Given the special points in the data sets we can compute α1 and α2 exactly:
α1 = 2 (arctan(1) −arctan(1 −2a)) =
π
2 −2 arctan(1 −2a) and α2 = 2

arctan(1) −arctan

a
1−a

=
π
2 −2 arctan

a
1−a

. Then the Rashomon ratios diﬀerence is:
| ˆRratioS1(F, 0) −ˆRratioS2(F, 0)| =

α1 −α2
π/2

=

4
π

arctan(1 −2a) −arctan

a
1 −a

=

4
π arctan

1 −4a −2
2a2 −1
 .
Now if we choose a ∈(0, 1) and such that
 4
π arctan

1 −4a−2
2a2−1
 > λ, then the Rashomon ratio diﬀerence
| ˆRratioS1(F, 0) −ˆRratioS2(F, 0)| is at least λ.
■
B.4
Empirical Local Rademacher Complexity
The empirical Rademacher complexity is another complexity measure of the hypothesis space. Following
Bartlett et al. (2005), for binary classiﬁcation we deﬁne it as follows.
Deﬁnition 12 (Empirical Rademacher complexity). Given a data set S, and a hypothesis space F of real-
valued functions, the empirical Rademacher complexity of F is deﬁned as:
ˆRS
n(F) = 1
nEσ
"
sup
f∈F
n
X
i=1
σif(zi)
#
,
where σ1, σ2, . . . , σn are independent random variables drawn from the Rademacher distribution i.e. P(σi =
+1) = P(σi = −1) = 1/2 for i = 1, 2, . . . , n.
Since we are interested only in models that are inside the Rashomon set, we will consider local empirical
Rademacher complexity (Bartlett et al., 2005), which is deﬁned using the Rashomon set ˆRset(F, θ). In the
following theorem, we provide a simple example to show the discrepancy between the two measures.
24

0.2
0.0
0.2
0.4
0.6
0.8
1.0
1.2
x1
0.0
0.2
0.4
0.6
0.8
1.0
1.2
x2
d
(a) ˆRratio(F Ω, θ) = d
γ
0.2
0.0
0.2
0.4
0.6
0.8
1.0
1.2
x1
0.2
0.0
0.2
0.4
0.6
0.8
1.0
1.2
x2
±1
±1
(b) Toy data set
0.2
0.0
0.2
0.4
0.6
0.8
1.0
1.2
x1
0.0
0.2
0.4
0.6
0.8
1.0
1.2
x2
d1
(c) ˆRratioS1 (F Ω, 0) = d1
0.2
0.0
0.2
0.4
0.6
0.8
1.0
1.2
x1
0.0
0.2
0.4
0.6
0.8
1.0
1.2
x2
d2
(d) ˆRratioS2 (F Ω, 0) = d2
Figure 6: An illustration of diﬀerent Rashomon ratios with equivalent empirical local Rademacher complex-
ities. Black line shows the optimal model, shaded region indicates the Rashomon set ˆRset(FΩ, 0) with its
models represented by green lines, the magenta color indicates boundaries of the hypothesis space. (a) The
projected minimal distance d is equivalent to the volume of the Rashomon set. (b) A toy data set that
illustrates that the empirical local Rademacher complexity is zero for models in the Rashomon set. (c) data
set S1, and (d) data set S2 illustrate symmetric separable data sets with diﬀerent Rashomon ratios. Best
seen in color.
Theorem 4 (Rashomon ratio is not the local Rademacher complexity). For 0 < λ < 1, there exist two data
sets S1 and S2, a hypothesis space FΩ, and a Rashomon parameter θ such that the local Rademacher com-
plexities deﬁned on the Rashomon sets for S1 and S2 are the same: ˆRS1
n

ˆRset(FΩ, θ)

= ˆRS2
n

ˆRset(FΩ, θ)

,
yet the Rashomon ratios are diﬀerent:
 ˆRratioS1(FΩ, θ) −ˆRratioS2(FΩ, θ)
 > λ.
Proof. Consider two-dimensional separable symmetric data, X ∈[0, 1]2, Y = {0, 1}, 0-1 loss φf(x, y) =
1[y=signf(x)] with empirical risk minimizer ˆf, and a hypothesis space FΩof decision stumps based on the
ﬁrst feature, where for f ∈FΩ: f = 1 if x1 > ω, ω ∈R, f = 0 otherwise.
We have a one-to-one
correspondence between a function and its threshold parameter ω. Therefore, if the Rashomon set is a single
connected component, we can compute the volume of the Rashomon set in parameter space by computing
the diﬀerence between the largest and smallest threshold values of models within the Rashomon set, as
illustrated in Figure 6a. For θ = 0, the diﬀerence between the largest and the smallest threshold values
will be equivalent to the minimal distance between points of opposite classes projected onto the ﬁrst feature
d = minxi,xj:yi̸=yj |PR1(xi) −PR1(xj)|, where PR1 is the projection of point x onto ﬁrst feature.
For the hypothesis space, we consider all decision stumps in the ﬁrst dimension that are in the segment
[0, 1], where data are populated. The diﬀerence in thresholds for the hypothesis space is β = 1 and therefore
V(FΩ) = 1. For θ = 0, the volume of the Rashomon set will be equivalent to d—the projected minimal
distance between points of opposite classes, and have that V( ˆRset(FΩ, 0)) = d and ˆRratio( ˆRset(FΩ, 0)) = d
1 =
d. Now consider any two separable symmetric data sets S1, S2 with diﬀerent projected minimal distances
25

d1 and d2, such that |d1 −d2| > λ. (Please see Figure 6c and 6d for details of the data sets S1 and S2.)
Consequently we get that:
 ˆRratioS1(FΩ, 0) −ˆRratioS2(FΩ, 0)
 = |d1 −d2| > λ.
For a separable symmetric data S and 0-1 loss function, the Rashomon set ˆRset(FΩ, 0) contains all
models that separate data in the same way. Therefore the Rademacher complexity of the Rashomon set is
ˆRS
n

ˆRset(FΩ

is:
ˆRS
n

ˆRset(FΩ, 0)

= 1
nEσ
"
sup
f∈ˆ
Rset(FΩ,0)
n
X
i=1
σif(xi)
#
= 1
nEσ
" n
X
i=1
σi ˆf(xi)
#
= 0,
where in the penultimate equality we have used the fact that, in the case of separable data and θ = 0, all
models in the Rashomon set will perform identically on any permutation of the labels.
Equality of the empirical Rademacher complexity of the optimal model to zero follows from the symmetric
data considered and symmetrical patterns of all possible target assignments. For example, for the toy data
set in Figure 6b: ˆRS
n

ˆRset(FΩ, 0)

= 1
2
1
4

ˆf(x1) + ˆf(x2)

+

ˆf(x1) −ˆf(x2)

+

−ˆf(x1) + ˆf(x2)

+

−
ˆf(x1) −ˆf(x2)

= 0.
Since both S1 and S2 are separable and symmetric we get that:
ˆRS1
n

ˆRset(FΩ, 0)

= 0 = ˆRS2
n

ˆRset(FΩ, 0)

.
■
C
Proofs for Generalization Results
C.1
Proof of Theorem 5
We recall and provide the proof of Theorem 5 after Proposition 13 that we use to prove Theorem 5.
Given a parameter η ≥0, we call the Rashomon set with restricted empirical risk an anchored Rashomon
set:
ˆRanc
set (F, η) := {f ∈F : ˆL(f) ≤η}.
We deﬁne also the true anchored Rashomon set based on the true risk as follows:
Ranc
set (F, η) := {f ∈F : L(f) ≤η}.
Proposition 13 (True anchored Rashomon set is close to empirical). For a loss l bounded by b and for any
ϵ > 0, and for a ﬁxed f, if f ∈Ranc
set (F, η) then with probability at least 1 −e−2n(ϵ/b)2 with respect to the
random draw of training data,
f ∈ˆRanc
set (F, η + ϵ).
Proof. For a ﬁxed f ∈Ranc
set (F, η) by Hoeﬀding’s inequality:
P
h
ˆL(f) −L(f) > ϵ
i
= P
"
1
n
n
X
i=1
l(f, zi) −E [l(f, z)] > ϵ
#
≤e−2n(ϵ/b)2.
26

Therefore, with probability at least 1−e−2n(ϵ/b)2 with respect to the random draw of data, ˆL(f)−L(f) ≤ϵ.
Since f ∈Ranc
set (F, η), then by deﬁnition of the Rashomon set, L(f) ≤η. Combining this with Hoeﬀding’s
inequality, we get that with probability at least 1 −e−2n(ϵ/b)2:
ˆL(f) ≤L(f) + ϵ ≤η + ϵ,
therefore f ∈ˆRanc
set (F, η + ϵ).
■
Proposition 13 is based on the same intuition as Lemma 23 in the work of Fisher et al. (2019), which is
used to bound the probability with which a given model is not in the empirical Rashomon set; this is used
in a proof of a bound for model class reliance. We use the proposition to indicate the probability with which
the empirical anchored Rashomon set is as close as possible to the true anchored Rashomon set for a given
model.
Theorem 5 (The advantage of a true Rashomon set). Consider ﬁnite hypothesis spaces F1 and F2, such
that F1 ⊂F2. Let the loss l be bounded by b, l(f2, z) ∈[0, b] ∀f2 ∈F2, ∀z ∈Z. Deﬁne an optimal function
f ∗
2 ∈argminf2∈F2L(f2). Assume that the true Rashomon set includes a function from F1, so there exists a
model ˜f1 ∈F1 such that ˜f1 ∈Rset(F2, γ). (Note that we do not know ˜f1.) In that case, for any ϵ > 0 with
probability at least 1 −ϵ with respect to the random draw of data:
L(f ∗
2 ) −b
r
log | F1 | + log 2/ϵ
2n
≤ˆL( ˆf1) ≤L(f ∗
2 ) + γ + b
r
log 1/ϵ
2n
,
where ˆf1 ∈argminf1∈F1 ˆL(f1). (Unlike ˜f1, we do know ˆf1 because we can calculate it.)
Proof. Lower bound.
We apply the union bound and Hoeﬀding’s inequality.
The result is that with
probability at least 1 −ϵ for every f1 ∈F1 we have, for ﬁnite hypothesis space F1:
L(f1) ≤ˆL(f1) + b
r
log | F1 | + log 2/ϵ
2n
.
(6)
Combining this Occam’s razor bound with the deﬁnition of f ∗
2 ∈arg minf∈F2 L(f) we get that, under the
same conditions:
L(f ∗
2 ) ≤L( ˆf1) ≤ˆL( ˆf1) + b
r
log | F1 | + log 2/ϵ
2n
.
Upper bound. By the assumption of the theorem, we have that L( ˜f1) ≤L(f ∗
2 ) + γ. Also, by the
deﬁnition of an optimal model f ∗
1 , L(f ∗
1 ) ≤L( ˜f1). Combining these, we get that L(f ∗
1 ) ≤L( ˜f1) ≤L(f ∗
2 )+γ.
Thus f ∗
1 is in the true Rashomon set of F2 with parameter γ. Alternatively, f ∗
1 is in the true anchored
Rashomon set of F2 with parameter η = L(f ∗
2 ) + γ, f ∗
1 ∈Ranc
set (F2, η). Following Proposition 13, we have
that for any ϵ1 > 0 with probability at least 1 −e−2n(ϵ1/b)2 with respect to the random draw of data,
f ∗
1 is in the slightly larger anchored Rashomon set ˆRanc
set (F2, η + ϵ1), and therefore, with high probability,
ˆL(f ∗
1 ) ≤η + ϵ1. Or alternatively, by setting ϵ = e−2n(ϵ1/b)2 we get that for any ϵ > 0 with probability at
least 1 −ϵ, we have ˆL(f ∗
1 ) ≤η + b
q
log 1/ϵ
2n
. Further, by deﬁnition of the empirical risk minimizer and given
that η = L(f ∗
2 ) + γ we get:
ˆL( ˆf1) ≤ˆL(f ∗
1 ) ≤L(f ∗
2 ) + γ + b
r
log 1/ϵ
2n
.
Combining the previous two equations yields the statement of the theorem.
■
27

C.2
Proof of Theorem 6 via Lemma 14
Theorem 6 follows directly from Lemma 14 below and Theorem 5, which guarantees that with high proba-
bility, the sampled space F1 will contain at least one model from the true anchored Rashomon set.
Lemma 14. For a ﬁnite hypothesis space F2 of size | F2 |, we will draw | F1 | functions uniformly without
replacement from F2 to form F1. If the true Rashomon ratio of the hypothesis space F2 is at least
Rratio(F2, γ) ≥1 −ϵ
1
| F1 |
then with probability at least 1 −ϵ with respect to the random draw of functions from F2 to form F1, the
Rashomon set contains at least one model ˜f1 from F1.
Proof. The probability of an individual sample from F2 missing the true Rashomon set is 1 −Rratio(F2, γ).
The probability if this happening | F1 | times independently is (1 −Rratio(F2, γ))| F1 |. Thus, for any ϵ > 0,
if the Rashomon ratio is at least Rratio(F2, γ) ≥1−ϵ
1
| F1 | , the probability pw of sampling, with replacement,
at least one hypothesis from Rratio(F2, γ) is:
pw = 1 −(1 −Rratio(F2, γ))| F1 | ≥1 −ϵ.
Let pi be the probability, under sampling without replacement, that samples 1 . . . i have missed Rratio(F2, γ).
p1 = 1 −Rratio(F2, γ), and pi ≤(1 −Rratio(F2, γ))i. The probability, under sampling without replacement,
that at least one hypothesis from Rratio(F2, γ) in F1 is therefore 1 −p| F1 | ≥pw. Thus the statement of the
lemma holds with probability at least 1 −ϵ.
■
Let us recall Theorem 6:
Theorem 6 (Example of the advantage of a large true Rashomon set). Consider ﬁnite hypothesis spaces F1
and F2, such that F1 ⊂F2 and F1 is uniformly drawn from F2 without replacement. For loss l bounded by
b, if the Rashomon ratio is at least
Rratio(F2, γ) ≥1 −ϵ
1
| F1 |
then for any ϵ > 0, with probability at least (1 −ϵ)2 with respect to the random draw of functions from F2
to form F1 and with respect to the random draw of data, the assumptions of Theorem 5 hold and thus the
bound (1) holds.
Proof. According to the Lemma 14, for any ϵ > 0 with probability at least 1 −ϵ with respect to the random
draw of functions, if the Rashomon set it at least Rratio(F2, γ) ≥1 −ϵ
1
| F1 | , then the Rashomon set contains
at least one model ˜f from F1. In that case, according to Theorem 5 with probability at least 1 −ϵ with
respect to the random draw of data, the bound (1) holds. Therefore with probability at least (1 −ϵ)2 we get
the statement of the theorem.
■
C.3
Proof of Theorem 7
Theorem 7 (Existence of multiple simpler models). For K-Lipschitz loss l bounded by b, consider hypothesis
spaces F1 and F2, F1 ⊂F2. With probability greater than 1 −ϵ w.r.t. the random draw of training data,
if for every model f2 ∈ˆRset(F2, θ) there exists f1 ∈F1 such that ∥f2 −f1∥p ≤δ, then there exists at least
B = B( ˆRset(F2, θ), 2δ) functions ¯f 1
1 , ¯f 2
1 ..., ¯
f B
1 ∈ˆRset(F, θ) such that:
1. They are from the simpler space: ¯f 1
1 , ¯f 2
1 ..., ¯
f B
1 ∈F1.
2.
L( ¯f i
1) −ˆL( ¯f i
1)
 ≤2KRn(F1) + b
q
log(2/ϵ)
2n
, for all i ∈[1, .., B], where Rn(F) is the Rademacher
complexity of a hypothesis space F. (This is from standard learning theory.)
28

Table 3: Examples of function approximation in diﬀerent hypothesis spaces: a function from space F1
approximates a function in space F2 with given guarantee δ.
F 2
F 1
δ
(depends
on
parameters
in
bounds below)
Source
f ∈L∞(Ω),
∥f∥∞∈[m, M]
sN ∈S(Ω),
sN—piecewise constant,
N—number of constants
∥f −sN∥∞≤M−m
2N
DeVore
(1998);
Davydov
(2011)
f ∈W 1
p (Ω), 1 ≤p ≤∞,
where W 1
p is a Sobolev space
s∆(f) ∈S(Ω),
s∆—piecewise constant,
∆—ﬁxed partition,
Ω= (0, 1)d,
N—number of constants
∥f −s∆(f)∥p ≤CN −1/d|f|W 1
p Ω
Davydov
(2011)
f ∈{xk, k ∈N}
P(n)—polynomials of degree
at most n ∈N
∥f
−
P(n)∥∞
≤
1
2k−1
P
j>(n+k)/2
 k
j

Newman
and
Rivlin
(1976)
f ∈C[0, 1] is a non-constant
symmetric boolean function
on x1,..,xn
P(d)—algebraic polynomials
of
degree d
∥f −P(d)∥∞≤O(
p
n(n −Γ(f)))
Paturi
(1992)
f ∈LipM(α), f is Lipschitz
continuous with constant M
Nn : [a, b] →R is a feedfor-
ward neural network with one
layer and bounded, monotone
and odd
deﬁned activation function,
n ∈N
supx∈[a,b] |f(x)
−
Nn(x)|
≤
5M
2
  b−a
n
α
Cao et al.
(2008)
f ∈Lp(I), where I ⊂Rd is
a cube in Rd, ∥· ∥W r(Lp(I))—
Sobolev semi norm
Pr—space of polynomials of
order r in d, constant C de-
pends on r
infp∈Pr ∥f
−
p∥Lp(I)
≤
C|I|r/d|f|W r(Lp(I))
DeVore
(1998)
Proof. Starting from the packing number of the Rashomon set B( ˆRset(F2, θ), 2δ), there exists a 2δ-packing
Ξ = {ξ1, ..., ξk|ξi ∈ˆRset(F2, θ)} such that ∥ξi −ξj∥p > 2δ for all i ̸= j.
On the other hand, for each
ξi ∈ˆRset(F2, θ) there exists ¯f i
1 ∈F1 such that ∥ξi −¯f i
1∥p ≤δ (this is the assumption that F1 serves as a
good cover for F2). Therefore for each ball center ξi in the packing there is a distinct model ¯f i
1 from the
simpler hypothesis space F1. Thus, the Rashomon set contains at least B = B( ˆRset(F2, θ), 2δ) models from
F1.
The generalization bound follows Bartlett and Mendelson (2002).
■
C.4
Examples of function approximation in diﬀerent hypothesis spaces
Table 3 shows examples of function classes where good approximating sets occur. More speciﬁcally, Table 3
describes classes of functions F2 that can be approximated with functions from classes F1 within δ using a
speciﬁed norm.
D
Data Set Descriptions
We provide a description of the data sets used in our experiments in Table 4. All of them were downloaded
from the UCI Machine Learning Repository (Dua and Graﬀ, 2019). We show the number of features in each
data set, sizes of the data set and any preprocessing steps that we used mainly to convert data to binary
classiﬁcation. For each data set, we performed cross-validation over ten folds for data sets with more than
200 points and over ﬁve folds for data sets with less than 200 points. We reserve one fold for testing, one
for validation (e.g., hyper-parameter optimization) and the rest for training. All of the real-valued data
29

0.0
0.2
0.4
0.6
0.8
1.0
Feature 1
0.0
0.2
0.4
0.6
0.8
1.0
Feature 2
Class 1
Class 2
Toy 1
0.0
0.2
0.4
0.6
0.8
1.0
Feature 1
0.0
0.2
0.4
0.6
0.8
1.0
Feature 2
Class 1
Class 2
Toy 2
0.0
0.2
0.4
0.6
0.8
1.0
Feature 1
0.0
0.2
0.4
0.6
0.8
1.0
Feature 2
Class 1
Class 2
Toy 3
0.0
0.2
0.4
0.6
0.8
1.0
Feature 1
0.0
0.2
0.4
0.6
0.8
1.0
Feature 2
Class 1
Class 2
Toy 4
0.0
0.2
0.4
0.6
0.8
1.0
Feature 1
0.0
0.2
0.4
0.6
0.8
1.0
Feature 2
Class 1
Class 2
Toy 5
0.0
0.2
0.4
0.6
0.8
1.0
Feature 1
0.0
0.2
0.4
0.6
0.8
1.0
Feature 2
Class 1
Class 2
Toy 6
0.0
0.2
0.4
0.6
0.8
1.0
Feature 1
0.0
0.2
0.4
0.6
0.8
1.0
Feature 2
Class 1
Class 2
Toy 7
0.0
0.2
0.4
0.6
0.8
1.0
Feature 1
0.0
0.2
0.4
0.6
0.8
1.0
Feature 2
Class 1
Class 2
Toy 8
0.0
0.2
0.4
0.6
0.8
1.0
Feature 1
0.0
0.2
0.4
0.6
0.8
1.0
Feature 2
Class 1
Class 2
Toy 9
0.0
0.2
0.4
0.6
0.8
1.0
Feature 1
0.0
0.2
0.4
0.6
0.8
1.0
Feature 2
Class 1
Class 2
Toy 10
0.0
0.2
0.4
0.6
0.8
1.0
Feature 1
0.0
0.2
0.4
0.6
0.8
1.0
Feature 2
Class 1
Class 2
Toy 11
0.0
0.2
0.4
0.6
0.8
1.0
Feature 1
0.0
0.2
0.4
0.6
0.8
1.0
Feature 2
Class 1
Class 2
Toy 12
Figure 7: Synthetic two-dimensional data sets that we used in the experiments.
sets were normalized to ﬁt the unit-cube, and we did not standardize the data. During data processing, we
omitted data records with missing values. We also omitted non-numerical features (e.g., date or text) when
there was not a natural way to convert them to categorical features.
Additionally, we performed experiments on twelve synthetic binary classiﬁcation data sets. These data
sets have two real features and represent diﬀerent geometrical concepts for two-dimensional classiﬁcation
(e.g., large and small margins, concentric circles, half moons, etc.) as in Figure 7. Results and implications
for synthetic data sets are consistent with those on the UCI data sets.
E
Performance of Diﬀerent Machine Learning Algorithms and Rashomon
Ratio
Figure 14 and Figure 15 show a performance comparison of diﬀerent machine learning algorithms with
regularization for the categorical and real-valued data sets. Data sets shown in Figures 14 and 15 are shown
in decreasing order of the Rashomon ratio, from the highest in Figure 14 to the Rashomon ratios that were
so small that we were not able to measure them.
Regularization limits the hypothesis space and thus changes the nature of the Rashomon set’s measure-
ments. Each value of the regularization parameter corresponds to a soft constraint on the hypothesis space,
which in turn can be realized as a hard constraint on this space. The Rashomon ratio in the regularized case
30

will typically be larger or equal to the Rashomon ratio in the unregularized case. There are two reasons for
this, explained below.
First, regularization reduces the hypothesis space. Hypotheses that were available when learning without
regularization may be excluded when learning with regularization. As a result, the size of the hypothesis
space decreases, which increases the Rashomon ratio.
Second, the empirical risk minimizer changes between the regularized and unregularized hypothesis sets,
which means the criterion for falling into the Rashomon set changes as well. Recall that the Rashomon
set is deﬁned based on the best performing model on the training set. The regularized hypothesis space is
less likely to contain overﬁtted models than the unregularized space. This means the regularized hypothesis
space’s empirical risk minimizer typically has higher empirical risk than that of the unregularized hypothesis
space. Then, if the Rashomon parameter θ is ﬁxed when comparing the two hypothesis spaces, there may
be more models in the Rashomon set for the regularized case. Thus, in the regularized case, the size of the
Rashomon set would be larger, and, therefore, the Rashomon ratio would be larger too.
Figure 17 and Figure 18 show a comparison of the performance of diﬀerent machine learning algorithms
without regularization for the categorical and real-valued data sets.
Figure 16 and Figure 19 shows a
performance comparison of diﬀerent machine learning algorithms for the synthetic data sets with and without
regularization.
As we mentioned before, we estimate the Rashomon ratio with importance sampling. For the proposal
distribution, we generate a tree of depth D by randomly splitting on features. We assign labels to all 2D
leaves using the training data. If a leaf contains no training points, it acquires its label from the nearest
ancestor that any training data pass through.
The probability of sampling any tree from the proposal
distribution is pp = pf × Q2D
i=1 1, where pf is the probability of randomly sampling all of the features that
comprise the splits of the tree. Our target distribution is a randomly sampled decision tree (both features
and leaves) of depth D. Therefore, the probability of sampling a given tree from the target distribution is
pt = pf ×Q2D
i=1
1
2, since we have two classiﬁcation classes, where, as before, pf is the probability of randomly
sampling all features used within splits of the tree. Thus, for one tree of depth seven, its importance weight
will be
pt
pp =
  1
2
27
≈3 × 10−39. The importance weight clearly dictates the order of magnitude of the
Rashomon ratio in our experiments. The smallest possible non-zero Rashomon ratio (≈1.175 × 10−42%)
arises when we sample one model that is in the Rashomon set among 250,000 total models that were sampled.
Therefore, we consider the Rashomon ratios of order 10−37% and 10−38% to be large, and Rashomon ratios
of order 10−40%, 10−41%, etc., to be small. Note that if there are more than two classes in the data set,
the importance weight will be even smaller, as the probability of sampling a random tree from a target
distribution decreases. Thus, trees built on a data set with three classes will have lower probability than
trees built on a data set with two classes. That is why we considered binary classiﬁcation only and modiﬁed
data as described in Table 4, as it is essential for us to compare ratios over diﬀerent data sets.
If we choose another importance sampling method (for example with data assignment for only half of the
leaves or with the guidance of both features and leaves) the Rashomon ratio may have diﬀerent importance
weights and therefore might have a diﬀerent estimated size as well. This issue would be resolved if we sample
a huge number of trees, which is hard to do in practice. Therefore, since our goal is to compare the Rashomon
ratios across data sets and feature spaces, we use a consistent method of leaf-based importance sampling
across all data sets and sample a manageable number of trees (250,000 in our case).
F
Large Rashomon Ratios May Appear Artiﬁcially Small
Even when the Rashomon ratio is a good driver of generalization performance, it may appear artiﬁcially
small because of a poor representation of data or poor choice of hypothesis space. For instance, if the features
are highly correlated, this artiﬁcially deﬂates the size of the Rashomon ratio as discussed in Appendix G.
Moreover, if the hypothesis space is poorly designed to include an overly large number of models, then
the Rashomon ratio may appear artiﬁcially small.
The issues with measuring the Rashomon ratio may
be a possible explanation for some of the results in Figure 2(b), which includes some data sets with high-
31

0
5
10
15
20
25
30
Number of features considered in 
2 test
0
50
100
150
200
Rashomon ratio  ×1040, %
(a) Reducing features
0
2
4
6
8
10
Number of noise features added to BCW6
0
50
100
150
200
Rashomon ratio  ×1040, %
(b) Adding noise
0
5
10
15
Number of good features added to BCW
0
50
100
150
200
Rashomon ratio  ×1040, %
(c) Adding good features
Figure 8: An illustration of the inﬂuence of feature quality on the Rashomon ratio for the Breast Cancer
Wisconsin data set (BCW). (a) shows the Rashomon ratio for the data set with diﬀerent numbers of signiﬁcant
features according to a χ2 test. Denote the BCW with the six most signiﬁcant features as BCW6. (b) depicts
the correspondence between the Rashomon ratio and diﬀerent numbers of noisy features added to the BCW6
data set. The noise features are sampled from normal distribution N(0, 1) and then standardized to be in a
hypercube of volume one. (c) shows the change in the Rashomon ratio as we add more redundant features to
the BCW data set. We iteratively add one out of six features from the BCW6 data set at a time. Rashomon
ratios in (a)–(c) are averaged over ten folds. The Rashomon parameter θ is set to 0.05. Rashomon ratios
are computed with respect to the best sampled model across all variations of the data set.
performing algorithms, yet (by the way we measured it) a small Rashomon ratio. In any case, small Rashomon
ratios are not our main interest; here we are interested in what we would observe under large Rashomon
ratios.
G
Quality of the Features
In our experiments, we observed a connection between the quality of the features and Rashomon ratios.
The Rashomon ratio in its simplest form, under uniform prior on the hypothesis space, is the fraction of
models that are inside the Rashomon set compared to the models in the hypothesis space. When a data
set is augmented with additional features, the size of the hypothesis space grows. If the added features
are completely irrelevant (consisting, for instance, of noise) then adding these features increases the size of
the hypothesis space but does not increase the size of the Rashomon set. Thus, we might predict that the
Rashomon ratio could decrease as irrelevant features are added to a data set.
Additionally, if we augment a data set with features that are highly correlated or identical to features
that improve performance, then not only is the size of the hypothesis space increased, but also the size of the
Rashomon set is likely to increase, as there exist more relevant models (even if the set becomes redundant
with models that predict equivalently). Thus, we might predict that the Rashomon ratio increases as we
add copies of relevant features.
In general, these two examples of irrelevant and redundant features are corner cases, however, they do
occur to a lesser degree in real world data sets, and we are interested in whether these cases have potentially
inﬂuenced our experimental results in Section 6 in our observed Rashomon ratios. To investigate this, we
augmented a data set with noise features, and separately, augmented the same data set with copies of useful
features to see whether irrelevant or correlated features may have inﬂuenced our ﬁndings on the measurement
of the Rashomon ratio. We used the Breast Cancer Wisconsin (Diagnostic) data set (shortly, BCW), which
has approximately six important features. The results are shown in Figure 8. As before, our hypothesis
space is decision trees of depth seven.
Irrelevant features
If the data set contains a lot of irrelevant or noisy features, we expect the Rashomon
set to be relatively small compared to the hypothesis space. Figure 8(a) shows how the Rashomon ratio
changes as we iteratively decrease the number of features in the Wine data set, eliminating the least relevant
32

features ﬁrst, leaving the most signiﬁcant ones (where relevance is determined according to a χ2 test with
the label). The Rashomon ratio grows as we ﬁrst remove non-signiﬁcant features, and after reaching a peak
at around six features, it starts to decrease as we remove relevant features, and as models lose accuracy.
Similarly, Figure 8(b) shows the inﬂuence of noisy features on the Rashomon ratio. Particularly, as we add
more noisy irrelevant features, the Rashomon ratio starts to decrease. This is due to the same fact, that we
artiﬁcially enlarge the hypothesis space while keeping the Rashomon set approximately the same. The noise
features do not help improve the empirical risk, they only increase the size of the reasonable set.
Redundant features
As a contrast to how we increased the hypothesis space in the previous experiment,
we can increase the Rashomon set by adding more redundant, good features. Figure 8(c) shows how the
Rashomon ratio changes for the BCW data set as we add more copies of the six most signiﬁcant features. We
observe that the Rashomon ratio increases. By adding copies of relevant features, we increased the number
of trees at a given depth that could be good enough to be in the Rashomon set.
Our ﬁndings show a possible connection between the Rashomon ratio and feature analysis. In particular,
in the case where diﬀerent algorithms perform similarly, but the Rashomon ratio is observed to be small,
it could be due to the reason that the data set contains noisy or irrelevant features. In that case, it may
be possible to iteratively remove features to ﬁnd those that produce the largest Rashomon ratio without
changes to the empirical risk. The other extreme is less likely to be observed in practice, which is when the
Rashomon ratio is extremely large due to redundant features. In that case, one could remove redundant
(highly correlated) features before measuring the Rashomon ratio. The data sets with smaller numbers of
features induce easier learning/optimization problems in general. As we discussed earlier, the Rashomon
ratio would generally not be measured in practice, and would be inferred in other ways. Thus, these results
mainly pertain to an understanding of the experiments we did in Section 6 to provide a possible explanation
for cases of small observed Rashomon ratios but where all methods perform the same and all functions
generalize.
H
Proof of Theorem 8
Theorem 8 (Expected size of the true Rashomon set cannot decrease under random classiﬁcation noise).
Consider hypothesis space F, data distribution D = X × Y, where, as before, X ∈Rp, and Y ∈{−1, 1}. Let
ρ ∈(0, 1
2) be a probability with which each label yi is ﬂipped independently, and Dρ denotes the noisy version
of D. If the loss function is φ(f(x), y) = 1[f(x)̸=y], then in expectation, the true Rashomon set over D is a
subset of the true Rashomon set over Dρ, RsetD(F, γ) ⊆RsetDρ (F, γ).
Proof. We are given that the probability with which each label is ﬂipped is ρ, P(˜yi ̸= yi) = ρ, where ˜yi
is a ﬂipped label of yi. Recall that the true risk LD(f) = E(x,y)∼D[φ(f(x), y)] = E(x,y)∼D[1[f(x)̸=y]], and
f ∗is an optimal function, meaning that f ∗∈argminf∈FLD(f). Given the noisy distribution Dρ, denote
f ∗
ρ ∈argminf∈FLDρ(f). For any model from the true Rashomon set f ∈RsetD(F, γ) = {f : LD(f) ≤
33

Figure 9: The setup for Conjecture 9. We show two Gaussians N(µ1, σ) and N(µ2, σ), with optimal model
f ∗= µ1+µ2
2
(shown in red). Models f e1
σ
and f e2
σ
(shown in blue) correspond to the left and right edges of
the Rashomon set (shown in purple). The loss of model f e2
σ
is computed as the sum of two Gaussian tails
and is equal to the area of the green region. The objective G(f e2
σ , σ) = L(f e2
σ ) −L(f ∗) corresponds to the
area of the shaded region to the left of function f e2
σ .
LD(f ∗) + γ}, we have:
LDρ(f) = E(x,˜y)∼D

1[f(x)̸=˜y]

= E(x,˜y)∼D

1[f(x)̸=˜y]|˜y = y

P(˜y = y)
+ E(x,˜y)∼D

1[f(x)̸=˜y]|˜y ̸= y

P(˜y ̸= y)
= E(x,y)∼D

1[f(x)̸=y]

P(˜y = y)
+ E(x,y)∼D

1[f(x)=y]

P(˜y ̸= y)
= E(x,y)∼D

1[f(x)̸=y]

(1 −P(˜y ̸= y))
+
 1 −E(x,y)∼D

1[f(x)̸=y]

P(˜y ̸= y)
= E(x,y)∼D

1[f(x)̸=y]

(1 −2P(˜y ̸= y)) + P(˜y ̸= y)
= E(x,y)∼D

1[f(x)̸=y]

(1 −2ρ) + ρ
= LD(f)(1 −2ρ) + ρ.
Since LD(f ∗) ≤LD(f ∗
ρ ) as f ∗is an optimal model over D, 0 < ρ < 0.5 by assumption, and, again, f is
in the true Rashomon set RsetD(F, γ), we have:
LDρ(f) −LDρ(f ∗
ρ ) = LD(f)(1 −2ρ) + ρ −LD(f ∗
ρ )(1 −2ρ) −ρ
= (LD(f) −LD(f ∗
ρ ))(1 −2ρ)
≤(LD(f) −LD(f ∗))(1 −2ρ)
≤γ(1 −2ρ) ≤γ.
(7)
Therefore, f is in the true Rashomon set RsetDρ (F, γ). As this calculation holds for every model f from the
true Rashomon set RsetD(F, γ), then in expectation RsetD(F, γ) ⊆RsetDρ (F, γ).
This proof uses similar technique as Theorem 1 in the work of (Manwani and Sastry, 2013), where the
purpose is to show noise tolerance of the optimal model.
■
I
Evidence for Conjecture 9
While there is not yet a proof for the conjecture, there is substantial evidence, which we present in this
section. Please see Figure 9 for illustration and details that will help with the intuition for these conjectures.
34

Conjecture 9 (The Rashomon set can increase with feature noise). Consider data distribution D = X × Y,
where, X ∈R, Y ∈{−1, 1}, and classes are balanced P(Y = −1) = P(Y = 1) and generated by Gaussian
distributions P(X|Y = −1) = N(µ1, σ2), P(X|Y = 1) = N(µ2, σ2), where 0 ≤µ1 < µ2. For the hypothesis
space F = {f : f ∈(β1, β2)}, where (µ1, µ2) ⊂(β1, β2), β1 ≪µ1, and µ2 ≪β2, and the Rashomon parameter
γ > 0:
(I) The volume of the Rashomon set is V(Rsetσ(F, γ)) = |f e1
σ −f e2
σ |, where f e1
σ
and f e2
σ
are the two
solutions to Eqn. (2), where Φ is the CDF of the standard normal:
2Φ
µ2 −µ1
2σ

−Φ
µ2 −f
σ

−Φ
f −µ1
σ

= γ.
(2)
(II) We conjecture that for F = {f : f ∈(µ1, µ2)}, as we add feature noise to the data set by increasing the
standard deviation σ, for all σ such that σ > ˜σ = µ2−µ1
2
√
2 , the volume of the Rashomon set increases as
a function of σ.
(III) Consider the setting where σ = 1 for both Gaussians, and we add or remove noise by moving the
means µ1 and µ2 of the Gaussians towards or away from each other. For any γ > 0, the volume of
the Rashomon set is minimized when µ2 ≈µ1 + 2. Moving the Gaussians either away from or towards
each other increases the volume of the Rashomon set.
Proof. Without loss of generality, we will assume that µ1 = 0 and µ2 = µ > 0. To get results for the original
values µ1 and µ2, we can simply add µ1 to µ and f.
Let us show the ﬁrst point of the conjecture and show how to compute the volume of the Rashomon set.
Evidence for Part (I)
Denote φ1 and φ2 as probability density functions (PDF) and Φ1 and Φ2 as
cumulative distribution functions (CDF) of classes Y = −1 and Y = 1 correspondingly. For a given model
f ∈F, the loss can be computed as the sum of areas under the PDF corresponding to misclassiﬁcation
errors:
L(f) = P(X > f|Y = −1) + P(X ≤f|Y = 1)
=
Z ∞
f
φ1(t)dt +
Z f
−∞
φ2(t)dt = 1 −Φ1(f) + Φ2(f)
= 1 −Φ
f −0
σ

+ Φ
f −µ
σ

= 2 −Φ
f
σ

−Φ
µ −f
σ

,
(8)
where Φ is the CDF of the normal distribution N(0, 1).
The optimal model, f ∗, can be obtained when P(X|Y = −1) = P(X|Y = 1), meaning that
1
σ
√
2π exp

−(x−0)2
2σ2

=
1
σ
√
2π exp

−(x−µ)2
2σ2

, which leads to f ∗= µ
2 . The loss of the optimal model is:
L(f ∗) = 2 −2Φ
 µ
2σ

.
Denote G(f, σ) as the diﬀerence in loss between a model f from the Rashomon set and optimal model
f ∗,
G(f, σ) := L(f) −L(f ∗) = 2Φ
 µ
2σ

−Φ
µ −f
σ

−Φ
f
σ

.
To ﬁnd a model on the edge of the true Rashomon set, we set G(f, σ) = γ and obtain Equation (2). As
the problem is symmetric with respect to f ∗and γ > 0, solutions f e1
σ
and f e2
σ
to Equation (2) correspond
35

to the left and right edges of the true Rashomon set. Thus, for a given γ and σ, we can estimate the volume
of the Rashomon set as V(Rsetσ(F, γ)) = |f e1
σ −f e2
σ |.
Now we will show evidence for the second point of the conjecture that the volume of the Rashomon set
increases when σ increases.
Evidence for Part (II)
Let ˜σ =
µ
2
√
2. Let’s focus on the right edge of the true Rashomon set where
f ∈(f ∗, µ). We will show that:
(i) For any ﬁxed σ, and for f ∈(f ∗, µ), G(f, σ) is monotonically increasing in f.
(ii) For any ﬁxed f ∈(f ∗, µ) and σ > ˜σ, G(f, σ) is monotonically decreasing in σ.
(iii) If (i) and (ii) hold, then for any σ1 and σ2 such that ˜σ < σ1 < σ2, we have that V(Rsetσ1(F, γ)) <
V(Rsetσ2(F, γ)).
We will prove (i) and (iii), and conjecture that (ii) is true, based on numerical experiments.
Let’s focus on (iii) ﬁrst. Consider σ1 and σ2 such that ˜σ < σ1 < σ2. Let f e
σ1, f e
σ2 ∈(f ∗, µ) be the right
edge models of the corresponding true Rashomon sets G(f e
σ1, σ1) = γ and G(f e
σ2, σ2) = γ. The volume of
the Rashomon set is V(Rsetσ(F, γ)) = |f e1
σ −f e2
σ | = 2|f e2
σ −f ∗| = 2|f e
σ −f ∗|. Using monotonicity of G(f, σ)
with respect to σ given ﬁxed f (the result of part (ii)), we get that:
G(f e
σ1, σ2) < G(f e
σ1, σ1) = γ = G(f e
σ2, σ2),
which means that f e
σ1 < f e
σ2 due to monotonicity of G(f, σ) with respect to f given ﬁxed σ (the result of
part (i)). Therefore, as we increase the standard deviation from σ1 to σ2, the Rashomon set increases as
well:
V(Rsetσ1(F, γ)) = 2|f e
σ1 −f ∗| < 2|f e
σ2 −f ∗| = V(Rsetσ2(F, γ)).
Thus, we have proved part (iii).
Now, let us prove (i). Given ﬁxed σ, the derivative of the objective G(f, σ) with respect to f is:
G′
f(f, σ) = 1
σ Φ′
f
µ −f
σ

−1
σ Φ′
f
f
σ

=
1
√
2πσ

e−(µ−f)2
2σ2
−e−(f)2
2σ2

> 0,
since µ −f < f since f > f ∗= µ
2 . Since the derivative G′
f(f, σ) > 0, the objective G(f, σ) monotonically
increases in f.
Finally, let’s show (ii). Given ﬁxed f ∈(f ∗, µ) and σ > ˜σ, the derivative of the objective with respect to
σ is:
G′
σ(f, σ) =
−2(µ)
2
√
2πσ2 Φ′
σ
 µ
2σ

+ µ −f
√
2πσ2 Φ′
σ
µ −f
σ

+
f
√
2πσ2 Φ′
σ
f
σ

=
1
√
2πσ2

−µe−µ2
8σ2 + (µ −f)e−(µ−f)2
2σ2
+ fe−f2
2σ2

=
f
√
2πσ2

−µ
f

e
f2
2σ2
 −(µ/f)2
4
+
µ
f −1
 
e
f2
2σ2
−((µ/f)−1)2
+

e
f2
2σ2
−1#
.
Denote u = µ
f . Since, µ
2 < f < µ, then u ∈(1, 2). Denote a = e
f2
2σ2 , then σ2 =
f 2
2 log(a). Note that a > 1,
and since σ > ˜σ =
µ
2
√
2, a < e
4f2
µ2 = e
4
u2 = s(u). Then G′
σ(f, σ) can be expressed in terms of parameter u
and variable a:
36

G′
σ(u, a) = 2 log(a)
√
2πf
h
−ua
−u2
4
+ (u −1) a−(u−1)2 + a−1i
= 2 log(a)
√
2πf D(u, a).
As a > 1 and f > µ
2 > 0, 2 log(a)
√
2πf
> 0. To show that D(u, a) < 0 for any u ∈(1, 2) and any a ∈(1, s(u)),
we perform exhaustive numerical calculations spanning the possible values of u ∈(1, 2) and a ∈(1, s(u))
in Figure 10. Indeed, D(u, a) = 0 when u = 2 or a = 1, and for all other values of u and a, D(u, a) < 0.
Therefore, G′
σ(f, σ) = G′
σ(u, a) < 0. Since the derivative is negative, when σ > ˜σ the objective G(f, σ)
monotonically decreases in σ, which concludes our evidence for (ii), and thus part (II), of the conjecture.
Evidence for Part (III)
Now we add or remove noise from the data set by moving the two means closer
together or further apart (see Figure 11), for a ﬁxed σ = 1. Recall that without a loss of generality, we take
µ1 = 0 and denote µ = µ2. The optimal model f ∗is f ∗= µ
2 . Given σ = 1 and γ > 0, we are interested in
ﬁnding µ > 0 and edge model f ∈(0, µ/2) such that the volume of the Rashomon set is minimal. Therefore
we have the following optimization problem:
min
µ
µ
2 −fµ s.t. fµ is deﬁned by
2Φ
µ
2

−Φ(µ −fµ) −Φ(fµ) = γ.
(9)
We cannot solve optimization problem (9) directly for the best µ for each γ, but we provide numerical
solutions in Figure 12 for a range of values of γ. We observe that, regardless of the value of γ > 0, as
we minimize the volume of the Rashomon set to ﬁnd µ, the µ corresponding to the optimal solution is
always approximately equal to 2. The edge model f e ∈(0, 1) is then the one that satisﬁes the constraint:
2Φ(1) −Φ(2 −f e) −Φ(f e) = γ. (The edge model must vary with γ since the optimal µ does not.)
The value µ = 2 might seem surprising as a solution for any γ > 0. However, when µ1 = 0 and µ2 = 2,
f ∗= 1, which is one standard deviation away from each mean and therefore corresponds to a value where
the inﬂection points of the two Gaussians coincide (normal distribution N(µ, σ) has two inﬂection points
µ±σ). Given ﬁxed γ, as we move either of the means of the Gaussians so that the inﬂection points no longer
coincide, f e moves outward to compensate so that the constraint in optimization problem (9) is satisﬁed.
Therefore, the volume of the Rashomon set grows as we increase or decrease feature noise by moving µ away
from 1 (in either direction) as shown in Figure 13.
■
37

Table 4: Classiﬁcation data sets description and processing notes.
Data Set Name
Type
of
Features
Number
of
Features
Number
of
Data Points
Processing notes
Monks-1
Binary
15
556
Monks-2
Binary
15
601
Monks-3
Binary
15
554
Voting
Binary
16
232
SPECT
Binary
22
267
Tic-tac-toe
Binary
27
958
Hayes-Roth
Binary
12
160
Considered class 1 versus classes 2 and 3
Nursery-1
Binary
27
8586
Considered classes not_recom and priority
Nursery-2
Binary
27
8310
Considered classes priority and spec_prior
Mushroom
Binary
117
8124
Breast Cancer
Binary
43
286
Car Evaluation
Binary
21
1728
Converted to one vs all problem: class 1 versus
all others
Primary Tumor
Binary
31
336
Converted to binary classiﬁcation by consider-
ing classes 1, 2, 3, 4, 22, 10 versus all others
Mammographic
Masses
Binary
25
830
Phishing
Binary
23
1353
Considered classes 0 and 1 versus class -1
Balance
Binary
20
576
Considered classes L and R
Wine
Real
13
130
Considered classes 0 and 1
Iris
Real
4
100
Considered classes versicolour and viginica
Breast
Cancer
Wis-
consin
Real
30
569
Breast Cancer Coim-
bra
Real
9
116
Digits 0-4
Real
64
363
Classes 0 and 4 considered only
Digits 6-8
Real
64
355
Classes 6 and 8 considered only
Student
Real
3
400
Banknote
Real
4
1372
Mapping
Real
28
10545
Converted to one vs all problem: class forest
versus all others
WiﬁLocalization
Real
7
1000
Considered classes that represent rooms 2 and
3
Column 2C
Real
6
310
Credit Card
Real
23
30000
Planing Relax
Real
12
182
Diabetic Retinopathy
Real
19
1151
Survival
Real
3
306
Skin Segmentation
Real
3
245057
HTRU_2
Real
8
17898
Magic
Real
10
19020
Seeds
Real
7
140
Considered classes 1 and 2
Eye State
Real
14
14980
MNIST 0-1
Real
784
13738
Considered classes 0 and 1
MNIST 4-9
Real
784
12752
Considered classes 4 and 9
38

Figure 10: Numerically showing that D(u, a) < 0 for any u ∈(1, 2), and a ∈(1, e
4
u2 ). Red plane corresponds
to the value 0.
39

Figure 11: In part (III), we add or remove noise from the data by moving the mean of the right Gaussian.
Both Gaussians have standard deviation 1.
0.02
0.04
0.06
0.08
Rashomon parameter, 
0.0
0.5
1.0
1.5
2.0
2.5
Optimal mean, 
0.5
0.6
0.7
0.8
0.9
1.0
1.1
1.2
0.02
0.04
0.06
0.08
Rashomon parameter, 
0.4
0.5
0.6
0.7
0.8
Optimal left edge model, fe1
0.5
0.6
0.7
0.8
0.9
1.0
1.1
1.2
Figure 12: Numerical solution to the optimization problem (9). For each ﬁxed γ, we plot f e1
σ = f e ∈(0, µ/2),
such that the volume of the Rashomon set is minimized. The color of the scatter plot points correspond to
the value of the volume of the Rashomon set, where more intense color means higher value.
Figure 13: An example that shows that in part (III) as we move the right Gaussian away from a mean of 2
in either direction, the volume of the Rashomon set increases.
40

LR
CART
RF
GBT
SVM
50
60
70
80
90
100
Accuracy, %
train
test
Credit Card
RRatio
 2.9387E-37 %
LR
CART
RF
GBT
SVM
50
60
70
80
90
100
Accuracy, %
train
test
Student
RRatio
 2.9387E-37 %
LR
CART
RF
GBT
SVM
50
60
70
80
90
100
Accuracy, %
train
test
Survival
RRatio
 2.9387E-37 %
LR
CART
RF
GBT
SVM
50
60
70
80
90
100
Accuracy, %
train
test
Voting
RRatio
 2.2168E-37 %
LR
CART
RF
GBT
SVM
50
60
70
80
90
100
Accuracy, %
train
test
SPECT
RRatio
 1.8801E-37 %
LR
CART
RF
GBT
SVM
50
60
70
80
90
100
Accuracy, %
train
test
HTRU_2
RRatio
 1.7479E-37 %
LR
CART
RF
GBT
SVM
50
60
70
80
90
100
Accuracy, %
train
test
Planing Relax
RRatio
 1.4694E-37 %
LR
CART
RF
GBT
SVM
50
60
70
80
90
100
Accuracy, %
train
test
Mammographic Masses
RRatio
 1.3667E-37 %
LR
CART
RF
GBT
SVM
50
60
70
80
90
100
Accuracy, %
train
test
Seeds
RRatio
 1.8107E-38 %
LR
CART
RF
GBT
SVM
50
60
70
80
90
100
Accuracy, %
train
test
Nursery-1
RRatio
 1.7237E-38 %
LR
CART
RF
GBT
SVM
50
60
70
80
90
100
Accuracy, %
train
test
Phishing
RRatio
 1.5832E-38 %
LR
CART
RF
GBT
SVM
50
60
70
80
90
100
Accuracy, %
train
test
Hayes-Roth
RRatio
 1.5647E-38 %
LR
CART
RF
GBT
SVM
50
60
70
80
90
100
Accuracy, %
train
test
Primary Tumor
RRatio
 1.5105E-38 %
LR
CART
RF
GBT
SVM
50
60
70
80
90
100
Accuracy, %
train
test
MNIST 0-1
RRatio
 1.0710E-38 %
LR
CART
RF
GBT
SVM
50
60
70
80
90
100
Accuracy, %
train
test
Skin Segmentation
RRatio
 1.0056E-38 %
LR
CART
RF
GBT
SVM
50
60
70
80
90
100
Accuracy, %
train
test
Monks-3
RRatio
 9.0477E-39 %
LR
CART
RF
GBT
SVM
50
60
70
80
90
100
Accuracy, %
train
test
Iris
RRatio
 8.3939E-39 %
LR
CART
RF
GBT
SVM
50
60
70
80
90
100
Accuracy, %
train
test
Digits 6-8
RRatio
 5.5902E-39 %
LR
CART
RF
GBT
SVM
50
60
70
80
90
100
Accuracy, %
train
test
Wine
RRatio
 4.2271E-39 %
LR
CART
RF
GBT
SVM
50
60
70
80
90
100
Accuracy, %
train
test
Breast Cancer Wisconsin
RRatio
 2.4526E-39 %
Figure 14: Performance of ﬁve machine learning algorithms with regularization for the UCI classiﬁcation
data sets. Data sets are listed in decreasing order of Rashomon ratio. Rashomon ratios, train and test
accuracies are averaged over ten folds for data sets with more than 200 points and over ﬁve folds for data
sets with less than 200 points. These plots continue in Figure 15. In these cases, test performance seems
to be similar across algorithms. This will not be true in all cases as the Rashomon set becomes smaller, in
Figure 15.
41

LR
CART
RF
GBT
SVM
50
60
70
80
90
100
Accuracy, %
train
test
Wifi Localization
RRatio
 2.0755E-39 %
LR
CART
RF
GBT
SVM
50
60
70
80
90
100
Accuracy, %
train
test
Mushroom
RRatio
 1.6610E-39 %
LR
CART
RF
GBT
SVM
50
60
70
80
90
100
Accuracy, %
train
test
Digits 0-4
RRatio
 8.2755E-40 %
LR
CART
RF
GBT
SVM
50
60
70
80
90
100
Accuracy, %
train
test
Monks-1
RRatio
 1.2437E-40 %
LR
CART
RF
GBT
SVM
50
60
70
80
90
100
Accuracy, %
train
test
Banknote
RRatio
 8.9455E-41 %
LR
CART
RF
GBT
SVM
50
60
70
80
90
100
Accuracy, %
train
test
Car Evaluation
RRatio < 1.1755E-42 %
LR
CART
RF
GBT
SVM
50
60
70
80
90
100
Accuracy, %
train
test
Monks-2
RRatio < 1.1755E-42 %
LR
CART
RF
GBT
SVM
50
60
70
80
90
100
Accuracy, %
train
test
Balance
RRatio < 1.1755E-42 %
LR
CART
RF
GBT
SVM
50
60
70
80
90
100
Accuracy, %
train
test
Tic-tac-toe
RRatio < 1.1755E-42 %
LR
CART
RF
GBT
SVM
50
60
70
80
90
100
Accuracy, %
train
test
Nursery-2
RRatio < 1.1755E-42 %
LR
CART
RF
GBT
SVM
50
60
70
80
90
100
Accuracy, %
train
test
Breast Cancer
RRatio < 1.1755E-42 %
LR
CART
RF
GBT
SVM
50
60
70
80
90
100
Accuracy, %
train
test
Breast Cancer Coimbra
RRatio < 1.1755E-42 %
LR
CART
RF
GBT
SVM
50
60
70
80
90
100
Accuracy, %
train
test
Column 2C
RRatio < 1.1755E-42 %
LR
CART
RF
GBT
SVM
50
60
70
80
90
100
Accuracy, %
train
test
Mapping
RRatio < 1.1755E-42 %
LR
CART
RF
GBT
SVM
50
60
70
80
90
100
Accuracy, %
train
test
Diabetic Retinopathy
RRatio < 1.1755E-42 %
LR
CART
RF
GBT
SVM
50
60
70
80
90
100
Accuracy, %
train
test
Eye State
RRatio < 1.1755E-42 %
LR
CART
RF
GBT
SVM
50
60
70
80
90
100
Accuracy, %
train
test
Magic
RRatio < 1.1755E-42 %
LR
CART
RF
GBT
SVM
50
60
70
80
90
100
Accuracy, %
train
test
MNIST 4-9
RRatio < 1.1755E-42 %
Figure 15: Performance of ﬁve machine learning algorithms with regularization for the UCI classiﬁcation data
sets. Data sets are listed in decreasing order of the Rashomon ratio, continued from Figure 14. Rashomon
ratios, training accuracies, and test accuracies are averaged over ten folds for data sets with more than 200
points and over ﬁve folds for data sets with less than 200 points. Test performance sometimes varies across
algorithms.
42

LR
CART
RF
GBT
SVM
50
60
70
80
90
100
Accuracy, %
train
test
Toy 5
RRatio
 2.5615E-37 %
LR
CART
RF
GBT
SVM
50
60
70
80
90
100
Accuracy, %
train
test
Toy 6
RRatio
 1.6471E-37 %
LR
CART
RF
GBT
SVM
50
60
70
80
90
100
Accuracy, %
train
test
Toy 1
RRatio
 3.7623E-38 %
LR
CART
RF
GBT
SVM
50
60
70
80
90
100
Accuracy, %
train
test
Toy 2
RRatio
 2.9953E-38 %
LR
CART
RF
GBT
SVM
50
60
70
80
90
100
Accuracy, %
train
test
Toy 3
RRatio
 5.7341E-39 %
LR
CART
RF
GBT
SVM
50
60
70
80
90
100
Accuracy, %
train
test
Toy 9
RRatio
 4.7248E-39 %
LR
CART
RF
GBT
SVM
50
60
70
80
90
100
Accuracy, %
train
test
Toy 4
RRatio
 4.4229E-39 %
LR
CART
RF
GBT
SVM
50
60
70
80
90
100
Accuracy, %
train
test
Toy 8
RRatio
 1.7820E-39 %
LR
CART
RF
GBT
SVM
50
60
70
80
90
100
Accuracy, %
train
test
Toy 11
RRatio
 5.7329E-40 %
LR
CART
RF
GBT
SVM
50
60
70
80
90
100
Accuracy, %
train
test
Toy 10
RRatio
 1.6986E-40 %
LR
CART
RF
GBT
SVM
50
60
70
80
90
100
Accuracy, %
train
test
Toy 7
RRatio
 5.3720E-41 %
LR
CART
RF
GBT
SVM
50
60
70
80
90
100
Accuracy, %
train
test
Toy 12
RRatio
 2.5861E-42 %
Figure 16: Performance of ﬁve machine learning algorithms with regularization for the synthetic data sets
with real-valued features. Data sets are listed in decreasing order of the Rashomon ratio. Rashomon ratios,
train and test accuracies are averaged over ten folds.
43

LR
CART
RF
GBT
SVM
50
60
70
80
90
100
Accuracy, %
train
test
HTRU_2
RRatio
 1.0659E-37 %
LR
CART
RF
GBT
SVM
50
60
70
80
90
100
Accuracy, %
train
test
Voting
RRatio
 1.0265E-37 %
LR
CART
RF
GBT
SVM
50
60
70
80
90
100
Accuracy, %
train
test
Mammographic Masses
RRatio
 6.6004E-38 %
LR
CART
RF
GBT
SVM
50
60
70
80
90
100
Accuracy, %
train
test
Seeds
RRatio
 2.6155E-38 %
LR
CART
RF
GBT
SVM
50
60
70
80
90
100
Accuracy, %
train
test
Nursery-1
RRatio
 1.7237E-38 %
LR
CART
RF
GBT
SVM
50
60
70
80
90
100
Accuracy, %
train
test
Skin Segmentation
RRatio
 1.0056E-38 %
LR
CART
RF
GBT
SVM
50
60
70
80
90
100
Accuracy, %
train
test
Iris
RRatio
 8.9258E-39 %
LR
CART
RF
GBT
SVM
50
60
70
80
90
100
Accuracy, %
train
test
Wine
RRatio
 8.4542E-39 %
LR
CART
RF
GBT
SVM
50
60
70
80
90
100
Accuracy, %
train
test
MNIST 0-1
RRatio
 7.6348E-39 %
LR
CART
RF
GBT
SVM
50
60
70
80
90
100
Accuracy, %
train
test
Monks-3
RRatio
 6.9747E-39 %
LR
CART
RF
GBT
SVM
50
60
70
80
90
100
Accuracy, %
train
test
Hayes-Roth
RRatio
 8.0146E-39 %
LR
CART
RF
GBT
SVM
50
60
70
80
90
100
Accuracy, %
train
test
Digits 6-8
RRatio
 5.5902E-39 %
LR
CART
RF
GBT
SVM
50
60
70
80
90
100
Accuracy, %
train
test
Mushroom
RRatio
 1.6610E-39 %
LR
CART
RF
GBT
SVM
50
60
70
80
90
100
Accuracy, %
train
test
Digits 0-4
RRatio
 4.1413E-40 %
LR
CART
RF
GBT
SVM
50
60
70
80
90
100
Accuracy, %
train
test
Wifi Localization
RRatio
 3.6734E-40 %
LR
CART
RF
GBT
SVM
50
60
70
80
90
100
Accuracy, %
train
test
Monks-1
RRatio
 1.2437E-40 %
LR
CART
RF
GBT
SVM
50
60
70
80
90
100
Accuracy, %
train
test
Phishing
RRatio
 9.8154E-41 %
LR
CART
RF
GBT
SVM
50
60
70
80
90
100
Accuracy, %
train
test
SPECT
RRatio
 7.8641E-41 %
LR
CART
RF
GBT
SVM
50
60
70
80
90
100
Accuracy, %
train
test
Breast Cancer Wisconsin
RRatio
 7.4644E-41 %
LR
CART
RF
GBT
SVM
50
60
70
80
90
100
Accuracy, %
train
test
Banknote
RRatio
 2.6684E-41 %
Figure 17: Performance of ﬁve machine learning algorithms without regularization for the UCI classiﬁcation
data sets. Data sets are listed in decreasing order of Rashomon ratio. Rashomon ratios, train and test
accuracies are averaged over ten folds for data sets with more than 200 points and over ﬁve folds for data
sets with less than 200 points. These plots continue in Figure 18. The data sets with larger Rashomon ratios
correlate with similar performance of machine learning algorithms and good generalization.
.
44

LR
CART
RF
GBT
SVM
50
60
70
80
90
100
Accuracy, %
train
test
Monks-2
RRatio < 1.1755E-42 %
LR
CART
RF
GBT
SVM
50
60
70
80
90
100
Accuracy, %
train
test
Primary Tumor
RRatio < 1.1755E-42 %
LR
CART
RF
GBT
SVM
50
60
70
80
90
100
Accuracy, %
train
test
Balance
RRatio < 1.1755E-42 %
LR
CART
RF
GBT
SVM
50
60
70
80
90
100
Accuracy, %
train
test
Breast Cancer
RRatio < 1.1755E-42 %
LR
CART
RF
GBT
SVM
50
60
70
80
90
100
Accuracy, %
train
test
Tic-tac-toe
RRatio < 1.1755E-42 %
LR
CART
RF
GBT
SVM
50
60
70
80
90
100
Accuracy, %
train
test
Nursery-2
RRatio < 1.1755E-42 %
LR
CART
RF
GBT
SVM
50
60
70
80
90
100
Accuracy, %
train
test
Car Evaluation
RRatio < 1.1755E-42 %
LR
CART
RF
GBT
SVM
50
60
70
80
90
100
Accuracy, %
train
test
Breast Cancer Coimbra
RRatio < 1.1755E-42 %
LR
CART
RF
GBT
SVM
50
60
70
80
90
100
Accuracy, %
train
test
Student
RRatio < 1.1755E-42 %
LR
CART
RF
GBT
SVM
50
60
70
80
90
100
Accuracy, %
train
test
Column 2C
RRatio < 1.1755E-42 %
LR
CART
RF
GBT
SVM
50
60
70
80
90
100
Accuracy, %
train
test
Mapping
RRatio < 1.1755E-42 %
LR
CART
RF
GBT
SVM
50
60
70
80
90
100
Accuracy, %
train
test
Planing Relax
RRatio < 1.1755E-42 %
LR
CART
RF
GBT
SVM
50
60
70
80
90
100
Accuracy, %
train
test
Diabetic Retinopathy
RRatio < 1.1755E-42 %
LR
CART
RF
GBT
SVM
50
60
70
80
90
100
Accuracy, %
train
test
Survival
RRatio < 1.1755E-42 %
LR
CART
RF
GBT
SVM
50
60
70
80
90
100
Accuracy, %
train
test
Eye State
RRatio < 1.1755E-42 %
LR
CART
RF
GBT
SVM
50
60
70
80
90
100
Accuracy, %
train
test
Magic
RRatio < 1.1755E-42 %
LR
CART
RF
GBT
SVM
50
60
70
80
90
100
Accuracy, %
train
test
Credit Card
RRatio < 1.1755E-42 %
LR
CART
RF
GBT
SVM
50
60
70
80
90
100
Accuracy, %
train
test
MNIST 4-9
RRatio < 1.1755E-42 %
Figure 18: Performance of ﬁve machine learning algorithms without regularization for the UCI classiﬁca-
tion data sets. Data sets are listed in decreasing order of the Rashomon ratio continuing from Figure 17.
Rashomon ratios, train and test accuracies are averaged over ten folds for data sets with more than 200
points and over ﬁve folds for data sets with less than 200 points
.
45

LR
CART
RF
GBT
SVM
50
60
70
80
90
100
Accuracy, %
train
test
Toy 5
RRatio
 2.5615E-37 %
LR
CART
RF
GBT
SVM
50
60
70
80
90
100
Accuracy, %
train
test
Toy 6
RRatio
 1.6471E-37 %
LR
CART
RF
GBT
SVM
50
60
70
80
90
100
Accuracy, %
train
test
Toy 9
RRatio
 3.0499E-39 %
LR
CART
RF
GBT
SVM
50
60
70
80
90
100
Accuracy, %
train
test
Toy 8
RRatio
 1.7820E-39 %
LR
CART
RF
GBT
SVM
50
60
70
80
90
100
Accuracy, %
train
test
Toy 3
RRatio
 1.0715E-39 %
LR
CART
RF
GBT
SVM
50
60
70
80
90
100
Accuracy, %
train
test
Toy 4
RRatio
 3.4289E-40 %
LR
CART
RF
GBT
SVM
50
60
70
80
90
100
Accuracy, %
train
test
Toy 7
RRatio
 8.3460E-42 %
LR
CART
RF
GBT
SVM
50
60
70
80
90
100
Accuracy, %
train
test
Toy 1
RRatio < 1.1755E-42 %
LR
CART
RF
GBT
SVM
50
60
70
80
90
100
Accuracy, %
train
test
Toy 2
RRatio < 1.1755E-42 %
LR
CART
RF
GBT
SVM
50
60
70
80
90
100
Accuracy, %
train
test
Toy 10
RRatio < 1.1755E-42 %
LR
CART
RF
GBT
SVM
50
60
70
80
90
100
Accuracy, %
train
test
Toy 11
RRatio < 1.1755E-42 %
LR
CART
RF
GBT
SVM
50
60
70
80
90
100
Accuracy, %
train
test
Toy 12
RRatio < 1.1755E-42 %
Figure 19: Performance of ﬁve machine learning algorithms without regularization for the synthetic data
sets with real-valued features. Data sets are listed in decreasing order of the Rashomon ratio. Rashomon
ratios, train and test accuracies are averaged over ten folds.
46

