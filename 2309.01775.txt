Preprint
GATED RECURRENT NEURAL NETWORKS
DISCOVER ATTENTION
Nicolas Zucchet∗, Seijin Kobayashi∗, Yassir Akram∗, Johannes von Oswald,
Maxime Larcher, Angelika Steger†, Jo˜ao Sacramento†
Department of Computer Science, ETH Z¨urich
{nzucchet, seijink, yakram, voswaldj, larcherm, asteger, rjoao}@ethz.ch
ABSTRACT
Recent architectural developments have enabled recurrent neural networks
(RNNs) to reach and even surpass the performance of Transformers on certain se-
quence modeling tasks. These modern RNNs feature a prominent design pattern:
linear recurrent layers interconnected by feedforward paths with multiplicative
gating. Here, we show how RNNs equipped with these two design elements can
exactly implement (linear) self-attention, the main building block of Transform-
ers. By reverse-engineering a set of trained RNNs, we find that gradient descent
in practice discovers our construction. In particular, we examine RNNs trained
to solve simple in-context learning tasks on which Transformers are known to ex-
cel and find that gradient descent instills in our RNNs the same attention-based
in-context learning algorithm used by Transformers. Our findings highlight the
importance of multiplicative interactions in neural networks and suggest that cer-
tain RNNs might be unexpectedly implementing attention under the hood.
Attention-based neural networks, most notably Transformers (Vaswani et al., 2017), have rapidly
become the state-of-the-art deep learning architecture, replacing traditional models such as multi-
layer perceptrons, convolutional neural networks, and recurrent neural networks (RNNs). This is
particularly true in the realm of sequence modeling, where once-dominating RNNs such as the long
short-term memory (LSTM; Hochreiter & Schmidhuber, 1997) model and the related gated recurrent
unit (GRU; Cho et al., 2014) have been mostly replaced by Transformers.
Nevertheless, RNNs remain actively researched for various reasons, such as their value as models in
neuroscience (Dayan & Abbott, 2001), or simply out of genuine interest in their rich properties as a
dynamical system and unconventional computer (Jaeger et al., 2023). Perhaps most importantly for
applications, RNNs are able to perform inference for arbitrarily long sequences at a constant memory
cost, unlike models based on conventional softmax-attention layers (Bahdanau et al., 2015). This
ongoing research has led to a wave of recent developments. On the one hand, new deep linear RNN
architectures (Gu et al., 2022; Orvieto et al., 2023b) have been shown to significantly outperform
Transformers on challenging long-sequence tasks (e.g., Tay et al., 2020). On the other hand, efficient
linearized attention models have been developed, whose forward pass can be executed in an RNN-
like fashion at a constant inference memory cost (Tsai et al., 2019; Katharopoulos et al., 2020;
Choromanski et al., 2021; Schlag et al., 2021).
We present a unifying perspective on these two seemingly unrelated lines of work by providing a set
of parameters under which gated RNNs become equivalent to any linearized self-attention, without
requiring infinite number of neurons or invoking a universality argument. Crucially, our construc-
tion makes use of gated linear units (GLUs; Dauphin et al., 2017), which are ostensibly featured in
recent deep linear RNN models. Turning to LSTMs and GRUs, which also include multiplicative
gating interactions, we find somewhat surprisingly that our results extend only to LSTMs. More-
over, the LSTM construction we provide hints that the inductive bias towards attention-compatible
configurations might be weaker for this architecture than for deep gated linear RNNs.
We then demonstrate that GLU-equipped RNNs, but not LSTMs and GRUs, can effectively imple-
ment our construction once trained, thus behaving as attention layers. Moreover, we find that such
∗Equal contribution. †Shared senior authorship.
1
arXiv:2309.01775v1  [cs.LG]  4 Sep 2023

Preprint
GLU-equipped RNNs trained to solve linear regression tasks acquire an attention-based in-context
learning algorithm. Incidentally, it has been shown that the very same algorithm is typically used
by Transformers trained on this problem class (von Oswald et al., 2023; Mahankali et al., 2023;
Ahn et al., 2023; Zhang et al., 2023). Our results thus challenge the standard view of RNNs and
Transformers as two mutually exclusive model classes and suggest that, through learning, RNNs
with multiplicative interactions may end up encoding attention-based algorithms disguised in their
weights.
1
BACKGROUND
1.1
LINEAR SELF-ATTENTION
We study causally-masked linear self-attention layers that process input sequences (xt)t with xt ∈
Rd as follows:
yt =

X
t′≤t
(WV xt′)(WKxt′)⊤

(WQxt)
(1)
In the previous equation, WV ∈Rd×d is the value matrix, WK ∈Rd×d the key matrix and WQ ∈
Rd×d the query matrix. Note that we use square matrices throughout our paper for simplicity, but
our findings hold for rectangular ones. As usually done, we call vt := WV xt, kt := WKxt and
qt := WQxt the values, keys and queries. The output vector yt has the same dimension as the
input, that is d. Such linear self-attention layers can be understood as a linearized version of the
softmax attention mechanism (Bahdanau et al., 2015) in use within Transformers (Vaswani et al.,
2017). Attention layers commonly combine different attention heads; we focus on a single one here
for simplicity.
In a linear self-attention layer, information about the past is stored in an effective weight matrix
W ff
t
:= P
t′ vt′k⊤
t′ that will later be used to process the current query qt through yt = W ff
t qt.
At every timestep, W ff
t is updated through the rule W ff
t
= W ff
t−1 + vtk⊤
t , which is reminiscent
of Hebbian learning (Schmidhuber, 1992; Schlag et al., 2021) and leads to faster inference time
(Katharopoulos et al., 2020; Choromanski et al., 2021; Shen et al., 2021; Peng et al., 2021) than
softmax self-attention.
1.2
GATED RECURRENT NEURAL NETWORKS
In this paper, we focus our analysis on a simplified class of gated diagonal linear recurrent neural
networks. They implement linear input and output gating that multiplies a linear transformation
Wxxt of the input with a linear gate Wmxt: g(xt) = Wmxt ⊙Wxxt. Here, ⊙is the element-wise
product. The class of gated networks we consider satisfies
ht+1 = λ ⊙ht + gin(xt),
yt = Dgout(ht).
(2)
In the previous equation, λ is a real vector, xt is the input to the recurrent layer, ht the hidden state,
gin the input gating, gout the output gating, and D a linear readout. This simplified class makes
connecting to attention easier while employing similar computational mechanisms as standard gated
RNNs architectures.
This class is tightly linked to recent deep linear RNN architectures and shares most of its compu-
tational mechanisms with them. While linear diagonal recurrence might be seen as a very strong
inductive bias, many of the recent powerful deep linear RNN models adopt a similar bias (Gupta
et al., 2022; Smith et al., 2023), and it has been shown to facilitate gradient-based learning (Orvieto
et al., 2023b; Zucchet et al., 2023b). Those architectures use complex-valued hidden states in the
recurrence; we only use its real part here. Some of those works employ a GLU (Dauphin et al.,
2017) after each recurrent layer, GLU(x) = σ(Wmxt) ⊙Wxxt with σ the sigmoid function. The
gating mechanism we consider can thus be interpreted as a linearized GLU. Finally, we can recover
(2) by stacking two layers: the GLU in the first layer acts as our input gating, and the one in the
second as output gating. We include a more detailed comparison in Appendix A. In the rest of the
paper, we will use the LRU layer (Orvieto et al., 2023b) as the representative of the deep linear RNN
architectures because of its proximity with (2).
2

Preprint
matrix (KV) - vector (Q)
multiplication
accumulate key-values
+ send current query
compute key-value
+ compute query
Figure 1: An example of a diagonal linear gated recurrent neural network that implements the same
function as a linear self-attention layer with parameters (WV , WK, WQ) and input dimension d.
Inputs are processed from the bottom to the top. We do not use biases so we append 1 to the input
vector xt to be able to send queries to the recurrent neurons. We use repeat(A, n) to denote that
the matrix A is repeated n times on the row axis and WV,i is the i-th row of the WV matrix. The
bars in the matrices highlight the different kinds of inputs/outputs. Digits in matrices denote column
vectors appropriately sized. The readout matrix D appropriately sums the element-wise products
between key-values and queries computed after the output gating gout.
LSTMs can operate in the regime of Equation 2, but this requires more adaptation. First, the recur-
rent processing of these units is nonlinear and is more involved than a simple matrix multiplication
followed by a nonlinearity. Second, gating occurs in different parts of the computation and depends
on additional variables. We compare in more details this architecture and the one of Equation 2 in
Appendix A, showing that LSTMs can implement (2) when stacking two layers on top of each other.
We additionally show that GRUs cannot do so.
2
THEORETICAL CONSTRUCTION
As highlighted in the previous section, our class of gated RNNs and linear self-attention have very
different ways of storing past information and using it to modify the feedforward processing of the
current input. The previous state ht acts through a bias term λ ⊙ht that is added to the current input
gin(xt) in gated RNNs, whereas the linear self-attention recurrent state W ff
t modifies the weights of
the feedforward pathway. We reconcile these two mismatched views of neural computation in the
following by showing that gated RNNs can implement linear self-attention.
We demonstrate in this section how a gated recurrent layer followed by a linear readout as in Equa-
tion 2 can implement any linear self-attention layer through a constructive proof. In particular, our
construction only requires a finite number of neurons, therefore providing a much stronger equiv-
alence result than more general universality of linear recurrent networks theorems (Grigoryeva &
Ortega, 2018; Orvieto et al., 2023a).
2.1
KEY IDEAS
Our construction comprises three main components: Firstly, the input gating gin is responsible for
generating the element-wise products between the keys and values, as well as the queries. Then,
recurrent units associated with key-values accumulate their inputs with λ = 1, whereas those re-
ceiving queries as inputs return the current value of the query, hence λ = 0. Lastly, the output
gating gout and the final readout layer D are in charge of multiplying the flattened key-value matrix
3

Preprint
Loss
Score KV
Score Q
Polynomial distance
4.97 × 10−8
4.52 × 10−8
2.06 × 10−10
3.73 × 10−4
Table 1: Gated RNNs implement attention. The KV and Q scores are equal to one minus the R2
score of the linear regression that predicts key-values and queries from resp. the perfect memory
neurons and perfect forget neurons. The polynomial distance is the L2 distance between the coeffi-
cients of the degree-4 polynomial that describes the instantaneous processing of the (optimal) linear
self-attention layer and the trained RNN.
with the query vector. We visually illustrate our construction and provide a set of weights for which
the functional equivalence holds in Figure 1. Crucially, the key-values in a linear self-attention layer
are the sum of degree two polynomials of each previous input. Input gating mechanism and perfect
memory units (λ = 1) are needed to replicate this behavior within a gated recurrent layer. Similarly,
output gating is required to multiply key-values with the queries.
2.2
ON THE NUMBER OF NEURONS REQUIRED BY THE CONSTRUCTION
The simple construction we illustrate in Figure 1 requires d2 hidden neurons for the key-value ele-
ments and d additional ones to represent the query. It is, however, possible to make the construction
more compact by leveraging two additional insights: First, when the key and value matrices are
equal, the key-value matrix is symmetric and, therefore, requires d(d + 1)/2 elements to be repre-
sented. Second, any combination of key and value matrices for which (W ⊤
KWQ) is fixed leads to the
same function in the linear self-attention layer. This implies that, when the value matrix is invertible,
we can reduce the number of hidden neurons storing key-values to d(d + 1)/2 by making the key
matrix to be equal to WV , and changing the query matrix to be W −⊤
V
W ⊤
KWQ.
Overall, the output gating requires O(d2) input and output entries for the gated RNN to match a
linear self-attention layer. The RNN thus requires O(d4) parameters in total, with a lot of redun-
dancy, significantly more than the 3d2 parameters of the linear self-attention layer. It comes as
no surprise that numerous equivalent configurations exist within the gated RNN we study. For in-
stance, linear gating is invariant under permutations of rows between its two matrices and under
multiplication-division of these two rows by a constant. Left-multiplying WQ in the input gating
by any invertible matrix P, and subsequently reading out the hidden neurons with λ = 0 through
repeat(P −1, d), also does not alter the network’s output. Several other invariances exist, making
exact weight retrieval nearly impossible.
2.3
IMPLICATIONS FOR OTHER CLASSES OF GATED RNNS
We conclude this section by commenting on whether similar insights hold for other gated RNNs
architectures. The LRU architecture is close to (2) but only has output gating. Stacking two LRU
layers on top of each other enables the output gating of the first layer to act as the input gating for
the second layer, and, therefore, implement linear self-attention. As noted in Section 1.2, LSTMs
and GRUs are further away from our simplified gated RNN model. However, one single LSTM
layer can implement linear self-attention, but stacked GRU layers cannot do so. We detailed those
considerations in Appendix A.
3
GATED RNNS LEARN TO MIMIC ATTENTION
We now demonstrate that gated RNNs learn to implement linear self-attention and comprehend
how they do so. In this section, a student RNN is tasked to reproduce the output of a linear self-
attention layer. Appendix B contains detailed descriptions of all experiments performed in this
section. Importantly, each sequence is only presented once to the network.
4

Preprint
A
B
C
Figure 2: Teacher-student experiment (d = 4). (A) Weights of the student RNN once post-processed.
The predictions yielded by those weights closely match the ones of the linear self-attention teacher.
(B) Value WV , key WK and query WV weight matrices of the teacher considered in this example;
all are invertible. (C) For each output coordinate i of the network, the kernels generated by the last
3 lines (11, 12 and 13) of the output gating are linearly combined through the decoding matrix D
are all proportional to the same kernel, which can be generated in a way that is coherent with our
construction. In all the weight matrices displayed here, zero entries are shown in grey, blue denotes
positive entries, and red negative ones.
3.1
TEACHER IDENTIFICATION
In our first experiment, we train a student RNN (|x| = 4, |h| = 100 and |y| = 4) to emulate the
behavior of a linear self-attention layer with weights sampled from a normal distribution. The low
training loss, reported in Table 1, highlights that the student’s in-distribution behavior aligns with
the teacher’s. However, this is insufficient to establish that the student implements the same function
as the teacher. The strategy we adopt to show functional equivalence is as follows: First, we observe
that only perfect memory neurons (λ = 1) and perfect forget neurons (λ = 0) influence the network
output. Additionally, each of these groups of neurons receives all the information needed to linearly
reconstruct resp. the key-values and the queries from the input. Finally, we show that the output
gating and the decoder matrix accurately multiply accumulated key-values with current queries.
After the learning process, a significant proportion of the weights in the input and output gating and
the readout becomes zeros. Consequently, we can eliminate neurons with input or output weights
that are entirely zeros, thereby preserving the network’s function. By doing so, we can remove 86
out of the 100 hidden neurons and 87 out of the 100 pre-readout neurons. After having permuted
rows in the two gating mechanisms and reordered hidden neurons, we plot the resulting weights
on Figure 2.A. Consistently with our construction, only recurrent neurons with λ = 0 or λ = 1
contribute to the network’s output. The key-values neurons receive a polynomial of degree 2 without
any term of degree 1 as the last column of W in
m and W in
x
is equal to zero for those units, and
the query neurons a polynomial of degree 1. The learning process discovers that it can only use
d(d + 1)/2 = 10 neurons to store key-values, similar to our compact construction. We show in
Table 1 that it is possible to linearly reconstruct the key-values from those 10 neurons perfectly, as
well as the queries from the 4 query neurons. By combining this information with the fact that the λs
are zeros and ones, we deduce that the cumulative key-values P
t′≤t vt′k⊤
t′ can be obtained linearly
from the key-values’ hidden neurons, and the instantaneous queries qt from the query neurons.
Additionally, the output gating combined with the linear readout can multiply the key-values with
the queries. Since we have already confirmed that the temporal processing correctly accumulates
5

Preprint
A
B
Figure 3: Gated RNNs learn compressed representation. (A) In the teacher-student experiment of
Section 3 in which there is no clear structure within the attention mechanism that the RNN can
extract, slight overparametrization is needed in order to identify the teacher. (B) In the linear re-
gression task of Section 4, the linear attention mechanism needed to solve the task optimally has a
sparse structure that the RNN leverages. Identification is thus possible with much smaller networks.
The quantity reported in B is the difference between the prediction loss of the RNN and the optimal
loss obtained after one step of gradient descent. We use the same input dimension d = 6 to make
the two plots comparable.
key-values, our focus shifts to proving that the instantaneous processing of the gated RNN matches
the one of the attention layer across the entire input domain. Given that both architectures solely
employ linear combinations and multiplications, their instantaneous processing can be expressed as
a polynomial of their input. The one of linear self-attention, (WV x)(WKx)⊤(WQx), corresponds
to a polynomial of degree 3, whereas the one of the gated RNN, gout(gin(x)), corresponds to one of
degree 4. By comparing these two polynomials, we can compare their functions beyond the training
domain. For every one of the four network outputs, we compute the coefficients of terms of degree
4 or lower of their respective polynomials and store this information into a vector. We then calculate
the normalized Euclidean distance between these coefficient vectors of the linear self-attention layer
and the gated RNN, and report the average over all 4 output units in Table 1. The evidence presented
thus far enables us to conclude that the student network has correctly identified the function of the
teacher.
While the majority of the weights depicted in Figure 2.A conform to the block structure characteris-
tic of our construction, the final three rows within the output gating matrices deviate from this trend.
As shown in Figure 2, these three rows can be combined into a single row matching the desired
structure. More details about this manipulation can be found in Appendix B.2.
3.2
IDENTIFICATION REQUIRES MILD OVERPARAMETRIZATION
The previous experiment shows that only a few neurons in a network of 100 hidden neurons are
needed to replicate the behavior of a self-attention layer whose input size is d. We therefore won-
der if identification remains possible when decreasing the number of hidden and pre-output gating
neurons the student has. We observe that mild overparametrization, around twice as many neurons
as the actual number of neurons required, is needed to reach identification. We report the results in
Figure 3.A.
3.3
NONLINEARITY MAKES IDENTIFICATION HARDER
We now move from our simplified class of gated RNNs and seek to understand how our findings
apply to LSTMs, GRUs, and LRUs. We use the following architecture for those three layers: a
linear embedding layer projects the input to a latent representation, we then repeat the recurrent
layer once or twice, and finally apply a linear readout. While those layers are often combined with
layer normalization, dropout, or skip connections in modern deep learning experiments, we do not
include any of those here to stay as close as possible to the teacher’s specifications. In an LRU
layer, the input/output dimension differs from the number of different neurons; we here set all those
dimensions to the same value for a fair comparison with LSTMs and GRUs. We compare these
6

Preprint
A
B
Figure 4: Comparison of the validation loss obtained by different gated recurrent networks archi-
tectures in (A) the teacher-student task of Section 1.2 and (B) the in-context linear regression of
Section 4. The construction baseline corresponds to the gated RNN of Eq. 2, with diagonal or dense
connectivity. We use the default implementation of LSTMs and GRUs, and slightly modify the LRU
architecture to reflect our construction better.
methods to the performance of our simplified gated RNNs, with both diagonal (as in Equation 2)
and dense linear recurrent connectivity.
We report the results in Figure 4 for d = 6. While diagonal connectivity provides a useful inductive
bias to learn how to mimic linear self-attention, it is not absolutely needed as the performance of the
dense counterpart of the construction highlights. It is theoretically possible to identify the teacher
with one LSTM layer. However, gradient descent does not find such a solution and the performance
of LSTMs is close to that of GRUs that cannot implement attention. Motivated by our construction,
we slightly change the LRU architecture and add a nonlinear input gating to each layer. We find that
it significantly improves its ability to mimic attention. We extensively compare different LRU layers
in Appendix B.
4
ATTENTION-BASED IN-CONTEXT LEARNING EMERGES IN TRAINED RNNS
The previous section shows that gated RNNs learn to replicate a given linear self-attention teacher.
We now demonstrate that they can find the same solution as linear self-attention when both are
learned. To that end, we study an in-context regression task in which the network is shown a few
input-output pairs and later has to predict the output value corresponding to an unseen input. Linear
self-attention is a particularly beneficial inductive bias for solving this task. When the input-output
mapping is linear, von Oswald et al. (2023) have shown that linear self-attention implement one step
of gradient descent.
4.1
IN-CONTEXT LINEAR REGRESSION
Linear regression consists in estimating the parameters W ∗∈Rdy×dx of a linear model y = W ∗x
from a set of observations {(xt, yt)}T
t=1 that satisfy yt = W ∗xt. The objective consists in finding a
parameter ˆW which minimizes the squared error loss L(W) =
1
2T
PT
t=1 ∥yt −Wxt∥2. Given an
initial estimate of the parameter W0, one step of gradient descent on L with learning rate Tη yields
the weight change
∆W0 = η
T
X
t=1
(yt −W0xt)x⊤
t .
(3)
In the in-context version of the task, the observations (xt, yt)1≤t≤T are provided one after the other
to the network, and later, at time T +1, the network is queried with (xT +1, 0) and its output regressed
against yT +1. Under this setting, von Oswald et al. (2023) showed that if all bias terms are zero,
a linear self-attention layer learns to implement one step of gradient descent starting from W0 = 0
and predict through
ˆyT +1 = (W0 + ∆W0)xT +1 = η
T
X
t=1
ytx⊤
t xT +1.
(4)
7

Preprint
Term
RNN
GD
x2
1y1
6.81 × 10−2 ± 8.52 × 10−5
6.76 × 10−2
x2
2y1
6.82 × 10−2 ± 6.40 × 10−5
6.76 × 10−2
x2
3y1
6.82 × 10−2 ± 5.56 × 10−5
6.76 × 10−2
residual norm
1.35 × 10−3 ± 1.97 × 10−4
0
Table 2: The coefficients of the instantaneous polynomial implemented by the first output unit of
a trained RNN on the in-context linear regression task match one optimal step of gradient descent,
averaged over 4 seeds. The ”residual norm” measures the norm of the polynomial coefficients,
excluding the ones appearing in the table. Those coefficients are all vanishingly small. The optimal
GD learning rate is obtained analytically (η∗= (T + dx −1/5)−1), c.f. Appendix C.2.
In the following, we show that gated RNNs also learn to implement the same algorithm and leverage
the sparse structure of the different attention matrices corresponding to gradient descent to learn a
more compressed representation than the construction one.
4.2
GATED RNNS LEARN TO IMPLEMENT GRADIENT DESCENT
We now train gated RNNs as in Equation 2 to solve the in-context linear regression task, see Ap-
pendix C.1 for more details. We set the number of observations to T = 12 and set the input and
output dimensions to 3 so that d = 6. Once learned, the RNN implements one step of gradient de-
scent with optimal learning rate, which is also the optimal solution one layer of linear self-attention
can find (Mahankali et al., 2023). Several pieces of evidence back up this claim: the training loss
of RNN after training (0.0945) is almost equal to the one of an optimal step of gradient descent
(0.0947) and the trained RNN implements the same instantaneous function, as the polynomial anal-
ysis of Table 2 reveals.
Linear self-attention weights implementing gradient descent have a very specific sparse struc-
ture (von Oswald et al., 2023). In particular, many key-values entries are always 0, so the con-
struction contains many dead neurons. This leads us to wonder whether gated RNNs would pick up
this additional structure and learn compressed representations. To test that, we vary the gated RNN
size and report in Figure 3.B the difference between the final training loss and the loss obtained after
one optimal gradient descent step. We observe a similar phase transition than in the teacher-student
experiment, this time happening for a much smaller number of neurons than our construction spec-
ifies. Gated RNNs thus learn a more compressed representation than the one naively mimicking
self-attention. This result provides some hope regarding the poor O(d4) scaling underlying our con-
struction: in situations that require an attention mechanism with sparse (WV , WK, WQ) matrices,
gated RNNs can implement attention with far fewer neurons. A precise understanding of how much
compression is possible in practical scenarios requires further investigation.
4.3
NONLINEAR GATED RNNS ARE BETTER IN-CONTEXT LEARNERS THAN ONE STEP
GRADIENT DESCENT
Finally, as a side question we compare the ability to learn in context of the nonlinear gated RNN
architectures that are LSTMs, GRUs and LRUs. Although not the main focus of our paper, this
allows putting our previous results in perspective. In particular, we are interested in understanding
if similarity with attention correlates with in-context learning performance, as attention has been
hypothesized to be a key mechanism for in-context learning (Olsson et al., 2022; Garg et al., 2022;
von Oswald et al., 2023). We report our comparison results in Figure 4, measuring the loss on
weights W ∗drawn from a distribution with double the variance of the one used to train the model.
Overall, we find that nonlinearity greatly helps and enables nonlinear gated RNN architectures to
outperform one gradient descent step when given enough parameters, suggesting that they imple-
ment some more sophisticated mechanism than vanilla gradient descent. Surprisingly, while the
GRU is the architecture that is the furthest away from attention, it performs the best in the task.
Within the different LRU layers we compare, we find a high correlation between in-context learning
abilities and closeness to attention, c.f. Figure 5 in the appendix. In particular, we observe a mas-
8

Preprint
sive performance improvement from the vanilla LRU architecture to the ones additionally including
input gating to match our construction more closely.
5
DISCUSSION
Our study reveals a closer conceptual relationship between RNNs and Transformers than commonly
assumed. We demonstrate that gated RNNs can theoretically and practically implement linear self-
attention, bridging the gap between these two architectures. Moreover, while Transformers have
been shown to be powerful in-context learners (Brown et al., 2020; Chan et al., 2022), we find that
RNNs excel in toy in-context learning tasks and that this performance is partly uncorrelated with the
architecture inductive bias toward attention. This highlights the needed for further investigations on
the differences between RNNs and Transformers in controlled settings, as also advocated by Garg
et al. (2022).
Our results partly serve as a negative result: implementation of attention is possible but requires
squaring the number of parameters attention has. We have shown that gated RNNs can leverage
possible compression, but understanding whether real-world attention mechanisms lie in this regime
remains an open question. Yet, our work is of current practical relevance as it provides a framework
that can guide future algorithmic developments. Bridging the gap between Transformers’ computa-
tional power and RNNs’ inference efficiency is a thriving research area (Fournier et al., 2023), and
the link we made facilitates interpolation between those two model classes.
Finally, our work carries implications beyond deep learning. Inspired by evidence from neuroscience
supporting the existence of synaptic plasticity at different timescales, previous work (Schmidhuber,
1992; Ba et al., 2016; Miconi et al., 2018) added a fast Hebbian learning rule, akin to linear self-
attention, to slow synaptic plasticity with RNNs. We show that, to some extent, this mechanism
already exists within the neural dynamics, provided that the response of neurons can be multiplica-
tively amplified or shut-off in an input-dependent manner. Interestingly, several single-neuron and
circuit level mechanisms have been experimentally identified which could support this operation in
biological neural networks (Silver, 2010). We speculate that such multiplicative mechanisms could
be involved in implementing self-attention-like computations in biological circuitry.
ACKNOWLEDGEMENTS
The authors thank Asier Mujika and Razvan Pascanu for invaluable discussions. This study was
supported by an Ambizione grant (PZ00P3 186027) from the Swiss National Science Foundation
and an ETH Research Grant (ETH-23 21-1).
9

Preprint
REFERENCES
Kwangjun Ahn, Xiang Cheng, Hadi Daneshmand, and Suvrit Sra. Transformers learn to implement
preconditioned gradient descent for in-context learning. arXiv preprint arXiv:2306.00297, 2023.
Jimmy Ba, Geoffrey E Hinton, Volodymyr Mnih, Joel Z Leibo, and Catalin Ionescu. Using fast
weights to attend to the recent past. In Advances in neural information processing systems, 2016.
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly
learning to align and translate. In International Conference on Learning Representations, 2015.
James Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal
Maclaurin, George Necula, Adam Paszke, Jake VanderPlas, Skye Wanderman-Milne, and Qiao
Zhang. JAX: composable transformations of Python+NumPy programs, 2018. URL http:
//github.com/google/jax.
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,
Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, and others. Language models
are few-shot learners. In Advances in neural information processing systems, 2020.
Stephanie Chan, Adam Santoro, Andrew Lampinen, Jane Wang, Aaditya Singh, Pierre Richemond,
James McClelland, and Felix Hill. Data distributional properties drive emergent in-context learn-
ing in transformers. In Advances in Neural Information Processing Systems, 2022.
Kyunghyun Cho, Bart van Merrienboer, Dzmitry Bahdanau, and Yoshua Bengio. On the properties
of neural machine translation: encoder-decoder approaches. In Proceedings of SSST-8, Eighth
Workshop on Syntax, Semantics and Structure in Statistical Translation, 2014.
Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas
Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, David Belanger, Lucy
Colwell, and Adrian Weller. Rethinking attention with Performers. In International Conference
on Learning Representations, 2021.
Yann N. Dauphin, Angela Fan, Michael Auli, and David Grangier. Language modeling with gated
convolutional networks. In International Conference on Machine Learning, 2017.
Peter Dayan and Laurence F Abbott. Theoretical neuroscience: computational and mathematical
modeling of neural systems. MIT Press, 2001.
Quentin Fournier, Ga´etan Marceau Caron, and Daniel Aloise. A practical survey on faster and
lighter transformers. ACM Computing Surveys, 55(14s):1–40, 2023. Publisher: ACM New York,
NY.
Shivam Garg, Dimitris Tsipras, Percy S Liang, and Gregory Valiant. What can transformers learn in-
context? a case study of simple function classes. In Advances in Neural Information Processing
Systems, 2022.
Lyudmila Grigoryeva and Juan-Pablo Ortega.
Universal discrete-time reservoir computers with
stochastic inputs and linear readouts using non-homogeneous state-affine systems. Journal of
Machine Learning Research, 19, 2018.
Albert Gu, Karan Goel, and Christopher R´e. Efficiently modeling long sequences with structured
state spaces. In International Conference on Learning Representations, 2022.
Ankit Gupta, Albert Gu, and Jonathan Berant. Diagonal state spaces are as effective as structured
states spaces. In Advances in Neural Information Processing Systems, 2022.
Charles R. Harris, K. Jarrod Millman, St´efan J. van der Walt, Ralf Gommers, Pauli Virtanen,
David Cournapeau, Eric Wieser, Julian Taylor, Sebastian Berg, Nathaniel J. Smith, Robert Kern,
Matti Picus, Stephan Hoyer, Marten H. van Kerkwijk, Matthew Brett, Allan Haldane, Jaime
Fern´andez del R´ıo, Mark Wiebe, Pearu Peterson, Pierre G´erard-Marchant, Kevin Sheppard, Tyler
Reddy, Warren Weckesser, Hameer Abbasi, Christoph Gohlke, and Travis E. Oliphant. Array
programming with NumPy. Nature, 585(7825):357–362, 2020.
10

Preprint
Jonathan Heek, Anselm Levskaya, Avital Oliver, Marvin Ritter, Bertrand Rondepierre, Andreas
Steiner, and Marc van Zee. Flax: A neural network library and ecosystem for JAX, 2023. URL
http://github.com/google/flax.
Sepp Hochreiter and J¨urgen Schmidhuber. Long short-term memory. Neural Computation, 9(8):
1735–1780, 1997.
J. D. Hunter. Matplotlib: A 2D graphics environment. Computing in Science & Engineering, 9(3):
90–95, 2007.
Herbert Jaeger, Beatriz Noheda, and Wilfred G. Van Der Wiel. Toward a formal theory for comput-
ing machines made out of whatever physics offers. Nature Communications, 14(1):4911, 2023.
Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Franc¸ois Fleuret. Transformers are
RNNs: fast autoregressive Transformers with linear attention. In International Conference on
Machine Learning, 2020.
Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International Confer-
ence on Learning Representations, 2019.
Arvind Mahankali, Tatsunori B Hashimoto, and Tengyu Ma.
One step of gradient descent is
provably the optimal in-context learner with one layer of linear self-attention. arXiv preprint
arXiv:2307.03576, 2023.
Flavio Martinelli, Berfin Simsek, Johanni Brea, and Wulfram Gerstner. Expand-and-cluster: exact
parameter recovery of neural networks. arXiv preprint arXiv:2304.12794, 2023.
Thomas Miconi, Jeff Clune, and Kenneth O Stanley. Differentiable plasticity: training plastic neural
networks with backpropagation. In International Conference on Machine Learning, 2018.
Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma, Tom Henighan,
Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Dawn Drain, Deep Ganguli,
Zac Hatfield-Dodds, Danny Hernandez, Scott Johnston, Andy Jones, Jackson Kernion, Liane
Lovitt, Kamal Ndousse, Dario Amodei, Tom Brown, Jack Clark, Jared Kaplan, Sam McCandlish,
and Chris Olah. In-context learning and induction heads. Transformer Circuits Thread, 2022.
Antonio Orvieto, Soham De, Caglar Gulcehre, Razvan Pascanu, and Samuel L. Smith. On the
universality of linear recurrences followed by nonlinear projections. In ICML 2023: 1st Workshop
on High-dimensional Learning Dynamics, 2023a.
Antonio Orvieto, Samuel L. Smith, Albert Gu, Anushan Fernando, Caglar Gulcehre, Razvan Pas-
canu, and Soham De. Resurrecting recurrent neural networks for long sequences. In International
Conference on Machine Learning, 2023b.
Fabian Pedregosa, Ga¨el Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion, Olivier
Grisel, Mathieu Blondel, Peter Prettenhofer, Ron Weiss, Vincent Dubourg, and others. Scikit-
learn: Machine learning in Python. the Journal of machine Learning research, 12:2825–2830,
2011. Publisher: JMLR. org.
Hao Peng, Nikolaos Pappas, Dani Yogatama, Roy Schwartz, Noah A. Smith, and Lingpeng Kong.
Random feature attention. In International Conference on Learning Representations, 2021.
Imanol Schlag, Kazuki Irie, and J¨urgen Schmidhuber. Linear Transformers are secretly fast weight
programmers. In International Conference on Machine Learning, 2021.
J¨urgen Schmidhuber. Learning to control fast-weight memories: an alternative to dynamic recurrent
networks. Neural Computation, 4(1):131–139, January 1992.
Zhuoran Shen, Mingyuan Zhang, Haiyu Zhao, Shuai Yi, and Hongsheng Li. Efficient attention:
attention with linear complexities. In Proceedings of the IEEE/CVF Winter Conference on Appli-
cations of Computer Vision (WACV), 2021. arXiv:1812.01243 [cs].
R. Angus Silver. Neuronal arithmetic. Nature Reviews Neuroscience, 11(7):474–489, 2010.
11

Preprint
Jimmy T.H. Smith, Andrew Warrington, and Scott W. Linderman. Simplified state space layers for
sequence modeling. In International Conference on Learning Representations, 2023.
Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao,
Liu Yang, Sebastian Ruder, and Donald Metzler. Long range arena: A benchmark for efficient
transformers. arXiv preprint arXiv:2011.04006, 2020.
Yao-Hung Hubert Tsai, Shaojie Bai, Makoto Yamada, Louis-Philippe Morency, and Ruslan
Salakhutdinov. Transformer dissection: a unified understanding of transformer’s attention via
the lens of kernel. In Proceedings of the 2019 Conference on Empirical Methods in Natural Lan-
guage Processing and the 9th International Joint Conference on Natural Language Processing,
2019.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez,
Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Infor-
mation Processing Systems, 2017.
Johannes von Oswald, Eyvind Niklasson, Ettore Randazzo, Jo˜ao Sacramento, Alexander Mordv-
intsev, Andrey Zhmoginov, and Max Vladymyrov. Transformers learn in-context by gradient
descent. In International Conference on Machine Learning, 2023.
Ruiqi Zhang, Spencer Frei, and Peter L Bartlett. Trained transformers learn linear models in-context.
arXiv preprint arXiv:2306.09927, 2023.
Nicolas Zucchet, Robert Meier, and Simon Schug.
Minimal LRU, 2023a.
URL https://
github.com/NicolasZucchet/minimal-LRU.
Nicolas Zucchet, Robert Meier, Simon Schug, Asier Mujika, and Jo˜ao Sacramento. Online learning
of long-range dependencies. arXiv preprint arXiv:2305.15947, 2023b.
12

Preprint
A
GATED RNNS AND LINEAR SELF-ATTENTION
In this section, we compare our simplified gated RNN model, linear self-attention, and nonlinear
gated RNN models (LSTMs, GRUs and LRUs). We recall that the key ingredients of our simplified
gated RNNs defined as
ht+1 = λ ⊙ht + gin(xt),
yt = Dgout(ht),
(5)
are the diagonal linear recurrence and the input and output gating. The input gating serves as a way
to generate the key-values of linear self-attention, which will then be accumulated in the hidden
recurrent units and combined with queries within the output gating.
Table 3 summarizes how many layers of LRUs, LSTMs and GRUs are needed to exactly implement
our simplified class of gated RNNs and linear self-attention. We provide more details below.
Simplified gated RNN
Linear self-attention
LRU
2
2
LRU In-Out
1
1
LRU In-Skip
2
1
LSTM
2
1
GRU
–
–
Table 3: Number of layers needed for different RNN layers to exactly implement our simplified
class and linear self-attention.
A.1
LRU
An LRU layer (Orvieto et al., 2023b) consists of a recurrent state ht and some instantaneous post-
processing. Its recurrent state is updated as
ht+1 = λ ⊙ht + γ ⊙(Bxt+1)
(6)
and its output yt is computed with
˜yt+1 = Re[Cht] + Dxt+1
(7)
yt+1 = σ(Wm˜yt+1) ⊙(Wx˜yt+1).
(8)
In the equations above, ht+1, B and C are complex-valued, Re denotes the real part of a complex
number, and σ is the sigmoid function. The transformation nonlinear transformation between yt+1
and ˜yt+1 is called a gated linear unit (GLU) and was introduced in Dauphin et al. (2017). Addition-
ally, λ and γ are parametrized exponentially:
λ = exp(−exp(νlog) + i exp(θlog)) and γ = exp(γlog).
(9)
The LRU layer detailed above comprises two central computational mechanisms: a linear recurrence
coupled with a GLU serving as nonlinear output gating. The recurrence is here complex-valued, but
we only need the real part of it for our purposes. Assuming that the sigmoid can be linearized,
our class of gated RNNs can be implemented using two layers by letting the output gating of the
first layer serve as input gating. We are now left with linearizing the sigmoid. To achieve this,
we double the number of output neurons of the GLU and require small weights in Wm, that can for
example, be compensated by large weights in Wm. Under this regime, we have σ(Wmx)⊙(Wxx) ≈
(1/2 + Wmx) ⊙(Wxx). Half of the neurons require identical weights as the target linear gating (up
to a proportional factor), half should have Wm = 0 and the same Wx as target linear gating. The
1/2Wxx term that comes from the second half of the neurons can be subtracted from the first half
of the neurons in a subsequent linear transformation, thereby yielding the desired result.
In our experiments, we consider two additional variations of the LRU layer that can implement our
class of gated RNNs and/or linear self-attention using only one layer. The LRU In+Out variation
has an additional nonlinear input gating mechanism compared to the original version (LRU Out) that
modifies the input before the recurrent part of the layer. The LRU In+Skip version includes the input
13

Preprint
gating mechanism, but the m branch of the output gating mechanism takes xt+1 as input instead of
˜yt+1. LRU In-Out can implement both linear self-attention and our class of gated RNNs in one
layer, whereas LRU In-Skip requires two layers for RNNs as described in Eq.2. In the later, the skip
part of the output gating computes the queries, which will then be multiplied with the key-values
extracted from the recurrent state.
A.2
LSTM
An LSTM cell (Hochreiter & Schmidhuber, 1997) has two recurrent states: the hidden state ht and
the cell state ct. They are updated as follows.
ft+1 = σ(Ufxt+1 + Vfht + bf)
(10)
˜ct+1 = tanh(Ucxt+1 + Vcht + bc)
(11)
gt+1 = σ(Ugxt+1 + Vght + bg)
(12)
ct+1 = ft+1 ⊙ct + gt+1 ⊙˜ct+1
(13)
ot+1 = σ(Uoxt+1 + Voht + bo)
(14)
ht+1 = ot+1 ⊙tanh(ct+1).
(15)
Here, ft is the cell state forget gate, ˜ct the cell state update candidate, gt the cell state update
candidate gate, ot the output gate, and σ the sigmoid function applied element-wise.
First, we show that one single LSTM layer can implement linear self-attention, by using gt+1 ⊙˜ct+1
as a way to compute key-values and c to aggregate them, ft+1 and use ot+1 for the query. We
provide the corresponding weights in the table below, ignoring all the nonlinearities except σ in
the f computation. Note that, compared to our simplified gated RNN class, we do not need to
include neurons that forget their last state (λ = 0) here as the output gate directly provides the
query to the output. Finally, linearizing the tanh function requires small Uc weights that can later
be compensated by large decoder weights, and ways to linearize the sigmoid were discussed in the
previous section.
Implementing a gated RNN as in Equation 2 can be done by using two layers: in the first layer
gt+1 ⊙˜ct+1 serves as input gating, ft+1 corresponds to λ, and, in the second layer, gt+1 ⊙˜ct+1
serves as output gating. Table 4 provides one set of such weights. This ignores the linearization
trick for the tanh in ˜c and the sigmoid in gt+1.
Layer 1
U
V
b
f
0
0
+∞
˜c
˜WK
0
0
g
˜WV
0
0
o
˜WQ
0
0
Layer 1
Layer 2
U
V
b
U
V
b
f
0
0
σ−1(λ)
0
0
−∞
c
W in
m
0
0
W out
m
0
0
g
W in
x
0
0
W out
x
0
0
o
0
0
+∞
0
0
+∞
Table 4: LSTM weight configuration that matches a linear self-attention layer (left) and a gated
RNN as in Equation 2 (right). This presumes that the activation functions in ˜c, g and o are linear.
We use ˜W to denote the value, key and query matrices transformed in a similar way to what we did
in Figure 1.
A.3
GRU
A GRU cell (Cho et al., 2014) has a hidden state ht, updated through
rt+1 = σ(Urxt+1 + Vrht + br)
(16)
˜ht+1 = tanh(Uhxt+1 + Vh(rt+1 ⊙ht) + bh)
(17)
zt+1 = σ(Uzxt+1 + Vzht + bz)
(18)
ht+1 = (1 −zt+1) ⊙ht + zt+1 ⊙˜ht+1
(19)
14

Preprint
where rt is the reset gate, zt is the update gate, ˜ht the update candidate, and σ is the sigmoid
function.
Here, stacking multiple GRUs on top of each other does not enable the implementation of any
network from our class of gated RNNs nor linear self-attention layers. One layer can implement
diagonal linear recurrence by linearizing the tanh, having zt+1 = 1 and rt+1 = λ. However,
implementing a gating mechanism of the form g(x) = (Wmx ⊙Wxx) is not possible1: we would
need to use zt+1 to implement one branch of the gating and ˜ht+1 the other but, given that zt+1 ̸= 0,
the previous hidden state ht influence the result.
B
TEACHER-STUDENT
B.1
EXPERIMENTAL DETAILS
For all experiments in Section 3, we train the student for almost one million training iterations on
sequences of length 32 and a batch size of 64 (50000 training examples per epoch, 1000 epochs).
We use the AdamW (Loshchilov & Hutter, 2019) optimizer with a cosine annealing learning rate
scheduler. The initial learning rate is set at 10−3, scheduled to anneal down to 10−6 by the end
of training and a weight decay of 10−4 is applied to all parameters except the recurrent ones λ in
the experiment of Section 3.1. To ensure that the hidden states do not explode, we ensure that λ
stays within [0, 1] by employing the exponential parametrization described in Appendix A.1 (we
only keep the ν part as λ takes real values here).
In Figure 5, we add more results to the architecture comparison we did 3.3. In particular, we compare
the three different types of LRU we mentioned in Appendix A.1, and observe that adding a GLU
improves LRUs ability to mimic linear self-attention within one layer, but also with several layers.
B.2
COMPRESSION OF THE LEARNED OUTPUT GATING WEIGHTS
In Figure 2, we show that the gating weight matrices have a structure that is close to the one of our
construction, except for three different rows (11, 12, and 13). We claim they can be reduced to a
single row; we now provide details justifying it.
Therefore, our objective is to demonstrate that these three rows are functionally equivalent to a
single row with the expected structure and to gain insights into the invariances inherent to the gating
mechanism we study in this paper along the way. The initial step toward achieving this entails
examining the influence of these three rows on the i-th coordinate of the network’s output:
13
X
j=11
Di,jgout(h)j =
13
X
j=11
Di,j(W out
m,jx)(W out
x,j x) = x⊤


13
X
j=11
Di,jW out
m,j W out
x,j
⊤

x.
(20)
This contribution can be interpreted as a quadratic form whose kernel is a weighted sum of rank-
1 kernels defined by the rows of the output gating matrices. In Figure 2.C, we plot the obtained
kernel for one of the output components. Crucially, the resulting kernel for the four output units
are all proportional to one another and is of rank-1. We can thus reduce the three neurons (11, 12
and 13) to one. Furthermore, the two vectors whose outer product yields the resulting kernel now
mirror the construction’s structure. One of these two vectors exclusively accesses query neurons
while the other reads key-value neurons, as seen in Figure 2.C. As usually occurs with this kind of
manipulation (Martinelli et al., 2023), merging the neurons slightly increases the loss, but original
loss levels can be recovered after fine-tuning.
C
IN-CONTEXT LINEAR REGRESSION
C.1
EXPERIMENTAL DETAILS
In the in-context linear regression, each sequence is a task characterized by a unique W ∗. The
weight matrix W ∗entries are sampled i.i.d. from a normal distribution N(0, 1
3). Each element of
1When the tanh is replaced by Id, it is possible to achieve so by having ht ≪˜ht+1 and correcting for the
exponential growth in the next layer.
15

Preprint
Figure 5: Extensive comparison between the different architectures. Compared to Figure 5, we
consider different versions of the LRU here, plot the loss as the function of the number of parameters,
and include both training and validation losses. Those two losses are almost (up to some sampling
noise) for the teacher-student task but are different for the in-context linear regression task because
we change the W ∗distribution in the validation set.
16

Preprint
the sequence is of the form (xt, W ∗xt). The entries of the inputs (xt)T +1
t=1 are sampled i.i.d. from
the uniform distribution U(−
√
3,
√
3). During the validation phase, we draw tasks from a different
distribution, W ∗
ij ∼N(0, 2
3) to highlight the generalization abilities of the learned models. We train
the model with the same optimization scheme described in Appendix B.1, except that we use a
smaller number of training iterations, totaling 300, 000. By default, we use gated RNNs with 80
hidden neurons.
C.2
OPTIMAL LEARNING RATE FOR ONE-STEP GRADIENT DESCENT
Let X ∈Rdx×n, W ∈Rdy×dx random variables such that all entries of X are sampled i.i.d. from
a centered uniform distribution with variance σ2
x, and those of W i.i.d. from some centered distri-
bution with finite variance σ2
W . We set Y = WX. Let x ∈Rdy a column vector, whose entries are
sampled from the same distribution as those of X, and y = Wx.
The goal of this section is to analytically derive the optimal learning rate for the in-context linear
regression task, that is to find η which minimizes
L(η) = 1
2EX,W,Y,x,y
h
∥y −ˆW(η, X, Y )x∥2i
(21)
where ˆW(X, Y ) is the result of one gradient descent step starting from 0 with learning rate η on the
loss W 7→1
2∥Y −WX∥2. The calculation is presented in a more general form in Mahankali et al.
(2023). We include it here as we additionally provide a simple formula for exact optimal learning
rate value.
Plugging in the analytical expressions for y and ˆW, we get
L(η) = 1
2EX,W,Y,x,y

∥y −ηY X⊤x∥2
(22)
= 1
2EX,W,x

∥Wx −ηWXX⊤x∥2
(23)
= 1
2EX,W,x

∥W(I −ηXX⊤)x∥2
(24)
We want to minimize L, i.e. look for η∗that satisfies ∂ηL(η∗) = 0. We have
∂ηL(η) = EX,W,x
h W(I −ηXX⊤)x
⊤WXX⊤x
i
(25)
= Tr EX,W,x

(I −ηXX⊤)W ⊤WXX⊤xx⊤
(26)
= σ2
x Tr EX,W

(I −ηXX⊤)W ⊤WXX⊤
(27)
= σ2
x Tr EX,W

XX⊤(I −ηXX⊤)W ⊤W

(28)
= σ2
xσ2
W Tr EX

XX⊤(I −ηXX⊤)

(29)
In the first equation, we use that E[a⊤b] = Tr E[ba⊤]. Third and fifth ones make use of Ex[xx⊤] =
σ2
xId and EW [WW ⊤] = σ2
W Id. Having ∂ηL(η∗) = 0 is then equivalent to
η⋆:=
Tr EX[XX⊤]
Tr EX[XX⊤XX⊤].
(30)
This result shows that only the distribution of the learning data matters. Let’s compute precisely this
quantity. We have EX[XX⊤] = nσ2
xId so we are left with computing Ex[XX⊤XX⊤]. Using that
17

Preprint
entries of X are i.i.d., we get
Tr EX[XX⊤XX⊤] = dxEX

X
i
 X
t
xi,tx1,t
!2

(31)
= dxEX


 X
t
x2
1,t
!2
+ dx(dx −1)EX


 X
t
x1,tx2,t
!2

(32)
= dxEX

X
t
x4
1,t +
X
t̸=t′
x2
1,tx2
1,t′

+ dx(dx −1)EX
"X
t
x2
2,tx2
1,t
#
(33)
= 9
5ndxσ4
x + n(n −1)dxσ4
x + n(dx −1)σ4
x
(34)
= ndxσ4
x

n + dx −1
5

(35)
because the fourth moment of a centered uniform distribution is 9
5σ4
x. Putting everything together,
we finally have
η∗=
1
σ2x(n + dx −1
5).
(36)
D
SOFTWARE
We run our experiments using the Jax (Bradbury et al., 2018) Python framework, using the Flax
(Heek et al., 2023) library for neural networks. We base our code base on the Minimal-LRU (Zucchet
et al., 2023a) repository. Data analysis and visualization were done using Numpy (Harris et al.,
2020), Scikit-learn (Pedregosa et al., 2011) and Matplotlib (Hunter, 2007).
18

