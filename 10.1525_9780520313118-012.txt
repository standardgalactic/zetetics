NINE 
Metaphors for Mind, Theories of Mind: 
Should the Humanities Mind? 
Allen Newell 
The outer frame of this essay is the computer revolution. The computer 
threatens to infiltrate the humanities and, indeed, all intellectual disci-
plines. There is much to the world besides disciplines, of course, and the 
computer has its effects on the larger stage of economics and war. But the 
disciplines provide a more than adequate frame for us. The computer— 
by which I mean the hardware, the software, and the algorithms, along 
with their underlying theory—is leading to a technical understanding of 
all intellectual operations. By taking intellectual operations to be the tech-
nology for processing information as represented in data structures, the 
computer is proceeding to transform them all. From word processors, to 
communication networks, to data bases, to data analysis, to theoretical 
calculations, to simulations, to automatic running of experiments, to 
preparation of scholarly papers, to scheduling appointments—all across 
the board, it transforms first this intellectual operation and then that one. 
The transformations are piecemeal, mostly commercially driven, and of-
ten attack the intellectually peripheral and the socially mundane. But, as 
with the locusts, one can hear the munch, munch, munch across the land. 
Since we did not previously understand intellectual operations all that 
well, the sweep of this revolution is unclear and contentious. It is possible 
to believe that only certain intellectual operations will be transformed 
and that others will remain within a separate and sheltered realm. This is 
one reason the computer revolution is only a threat; the reality has only 
partly arrived, and it is unclear that the rest will ever come. Contentious-
ness arises because the lack of clarity extends even to whether it is a 
threat. Perhaps the metaphor of the locusts is false. Perhaps the arrival 
should be greeted with joy and hosannas. For it will bring with it not only 
the capability for intellectuals to engage in more intellectual operations 
158 

METAPHORS FOR MIND, THEORIES OF MIND 
159 
per day but to do so with an immeasurably deepened understanding of 
what the intellectual operations themselves mean. The situation is yet 
more complex. One of the insidious aspects of the computer revolution 
is that it arrives on wings of benefice—cost/benefice, it is true, but since 
the disciplines value funds as much as the world at large, and also find 
them in short supply, the infiltration of the computer is usually wel-
comed in the small, even while in the large, it is discussed in symposia, 
papers, and books. 
So much for the outer frame. T h e theme of this volume is the location 
of humanity, and its method is to work along the boundaries between 
humanity and other things, such as beasts and machines. This connects it 
with the computer revolution, although in overlapping and partial ways. 
For the computer as machine touches this theme at more than one place: 
as an exemplar of the modern technological age, as a machine that deals 
in the root operations of intellectuals, and as something that mimics the 
human mind. 
Which introduces the inner frame—computers and the mind. This is 
the aspect of the computer revolution that concerns us. Whenever the 
issue explicitly arises of how the computer locates our humanity, the 
answer invariably centers on what computers tell us about our mind. It is 
not clear that we are right in always moving to this focus. Other aspects 
of the revolution may in the long run be more important. But, as in 
much that is human, there is recursive self-fulfillment: we inquire about 
what is important, but our criteria for importance is largely how much 
we observe ourselves inquiring. 
I accept this framing. Let us inquire once more about what the com-
puter tells us about ourselves, as reflected in our minds. However, this 
inner frame is still too large. We must pick out a single thread within it. 
Nevertheless, at last we are ready to begin, although the framing is not 
quite ready to end. 
METAPHORS FOR MIND 
T h e computer is a metaphor for the mind. That is how it so often gets said. 
It does not even seem strange any more. Western society has always used 
the technologies of the time as metaphors for the human mind. For all I 
know, all other civilizations do the same. In any event, it is enough that 
we do. 
We often trace the course of technological metaphors for mind from 
Descartes and the hydraulic fountains of his time, through the telephone 
switchboard, and on up to the computer. Rising above the sequence, by 
the sort of modest inductive capability natural to us all, we have often 
been assured that the leading metaphor for mind will move on with the 

160 
H U M A N S A N D MACHINES 
next technological advance (holograms were once a favorite). Some truth 
resides in this, no doubt. Metaphor is a particularly disarming way of 
arriving at truth. It invites the listener to find within the metaphor those 
aspects that apply, leaving the rest as the false residual, necessary to the 
essence of metaphor. And since within the metaphor is always, by the lin-
guistic turn (Toews 1987), within the listener, the invitation is to find within 
oneself some truth that fits. Having done that, it is difficult to argue 
against the truth that the metaphor reveals. Indeed, to fail to find any 
truth that applies seems to say more about the listener than about the 
metaphor; perhaps he is one of those who does not get jokes either. So 
there is truth in the computer as a metaphor for mind, though perhaps 
as much social truth as scientific truth. 
This brings us to the final framing, which is personal. This pervasive 
use of computer as metaphor for mind has always somehow rankled. 
Always I experience a sense of discomfort in using it, or in hearing it, or 
in taking it as the right way of putting it. Maybe I do not get the joke. 
Maybe I do and do not like it. Maybe I like it but do not know how to do 
it. Maybe I think it is no metaphoring matter. 
In any event, let us explore the issue. I shall expand on my William 
James Lectures at Harvard University (Newell 1987). Although those 
were a perfect place for a wider view, in fact, I focused strongly on 
matters internal to the science of cognition and did not address such 
issues as the computer metaphor. 
I now believe, however, that I can answer the question of what dis-
turbs me about the use of computer as metaphor for mind. It contrasts 
with a theory of mind. It distances the object, so to speak. If the computer is 
a metaphor for mind, it is by that same token no more about mind than a 
metaphor. Metaphors being clearly in the eye of the beholder, they leave 
the objects themselves untouched, or touched in only a literary way that 
does not derange the topic itself, because it is only commentary. Thus, to 
view the computer as a metaphor for mind is to keep the mind safe from 
analysis. 
Science is not like that. At least science does not take itself as meta-
phorical. Acknowledging always the necessity of approximation and the 
inevitability of error, it still sees itself as attempting to describe its subject 
matter directly and not metaphorically. It has indeed become fashionable 
both to deconstruct science (Latour and Woolgar 1979) and to take all 
science as metaphorical—to take metaphor as a metaphor for science. 
This can be done, of course. As already noted, metaphor has a certain 
inevitability about it with respect to squeezing out some truth. A boa 
constrictor is not a bad metaphor for metaphor. But sometimes the 
bones crack. In particular, it is clearly wrong to treat science as metaphor, 
for the more metaphorical, the less scientific. In contrast to metaphors 

METAPHORS FOR MIND, THEORIES OF MIND 
161 
for mind, to understand the m i n d — w h a t it is, how it can be, why it 
works, how far it reaches, whence it arises, and where it is located— 
requires a scientific theory of mind. 
T h e metaphor for mind that the computer makes available for our 
use is available to us all already. It is unclear that it can or needs to be 
sharpened. It is already richer than might be imagined. It indicates mind 
as something mechanical, atomistic, context independent, slavishly rule 
following, repetitive, stereotypical, jerky, single minded [sic], quantita-
tive, unchanging, rigid, unadaptive, powerful, compulsive, exhaustive, 
obsessed with detail, noncaring, a spindler/mutilator, passionless, formal, 
logical, errorless . . . Pull the plug! I am being too compulsive. 
In fact, the computer as metaphor mixes many sources and changes 
before our eyes. T h e computer as computer continues to become differ-
ent things at a rapid rate. O u r exposure to it, in all its forms, increases 
correspondingly, both directly by personal experience and indirectly 
through both the advertising and public media and the technical and 
intellectual literature. O n e source goes back all the way to the computer 
as machine, in which the computer is all gears. But we have all seen 
enough to realize that the computer is clearly not a rolling mill or tick-
tock clock. So the machine view gets overlaid with others that arise from 
the nature of programming and from the specialized nature of applica-
tions, such as the lean, clean, saturated color panels of computer anima-
tion or the whiz and abstract thrill and terror of interactive computer 
games. These different sources bear family resemblances, but they blur 
the metaphor and allow it to convey different things on different occa-
sions. Good thing, in fact, for a metaphor, where the richer, the better. 
T h e computer metaphor can take care of itself very well, without any 
help from me. My purpose is to probe whether the way to treat what the 
computer can tell us about mind is as metaphor or as science. 
T H E O R I E S OF MIND 
More effort needs to be spent explicating the nature of a scientific theory 
of mind. I prefer to do that at a specific level, not at the level of science in 
general. Still, something general needs to be said, if only because so 
much has been written and mused about the nature of science as it might 
pertain to humans and their societies and cultures, or whether in fact it 
could pertain at all. Again, the focus here is not to straighten out the 
notion of science per se. Rather, I want to be sure the notion of science 
that I use is clear. 
By a theory of mind, I mean just what I would in talking about any 
scientific theory. I mean it in the same sense as in the theory of plate 
tectonics in geology, the theory of organic chemistry, the astronomical 

162 
HUMANS AND MACHINES 
theory of the planetary orbits, the theory of the atom, and on and on. 
These examples are themselves not quite the same, but they all contain a 
solid common kernel. Society, in the body of the attending scientists, 
attempts to organize its knowledge of some body of phenomena. Its goal 
is to use the knowledge for prediction, explanation, design, control, or 
whatever. Theory is the symbolic organization of this knowledge. 
Sciences all grow to look alike in many ways—the natural and biologi-
cal sciences very much so, as well as bits and parts of the human sciences. 
They all develop bodies of solid fact and regularities and surround them 
with an explicit, if somewhat conventional, apparatus of evidential sup-
port. They all develop theories, which tend to mathematical and formal 
symbolic form. These theories tend to be mechanistic. That is, they posit 
a system or collection of mechanisms, whose operation and interaction 
produce the regularities. The theories of all the sciences all fit, more or 
less well, into a single theoretical fabric that is a stitched-together coher-
ent picture of a single universe. An article of faith for a long time, this 
has become increasingly evident with the amazing emergence of biology 
to match in power and elegance the older physical sciences—and to be 
one with them in a seamless scientific web. 
Some aspects of the sciences reflect the nature of the world, others the 
nature of the enterprise itself. That scientific theories are cast in terms of 
underlying mechanisms seems to reflect the nature of the world. Theo-
ries could be different and sometimes are. That theories have a formal 
and calculational character reflects the scientific enterprise. This quality 
makes the knowledge that is the science derivable from the theory and 
not from the theorist. To use a scientific theory requires both knowledge 
and skill, especially the latter. Hence, not everybody can take a theory 
and produce its results (or even reproduce them). But the results, when 
produced, are the results of the theory, not of the theorist—a matter of 
no small import, since humans themselves have (or contain, depending 
on your metaphor) bodies of knowledge. Humans can predict, explain, 
design, and control, all without benefit of science, much less theory. 
Perhaps what best characterizes science methodologically is its ability to 
get these activities into external symbolic artifacts, available to all who are 
"skilled in the art." 
If science stayed outside the house of man, there would be nothing to 
consider in contrasting metaphors and theories of mind. But, of course, 
a scientific psychology does exist, and it is recognizably a family member 
of the science kin group. The contrast requires more than just a scientific 
psychology, however. The computer must underlie both the metaphor of 
mind (which it avowedly does) and the theory of mind. If this were not 
the case, the contrast would still amount only to the proverbial one hand 
clapping. 

METAPHORS FOR MIND, THEORIES OF MIND 
163 
But the cognitive revolution has occurred (Gardner 1985). It is thirty 
years old. It has come to dominate individual psychology. New scientific 
upstarts now rail against it instead of against behaviorism. And this 
revolution has been dominated by the computer—or more correctly, by 
the abstract notions of computation and information processing that 
have emerged as the theoretical counterpart to the technological ad-
vance. Even the philosophers say so (Dennett 1988, Fodor 1983). The 
acceptance has moved to the creation of an umbrella interdiscipline 
called cognitive science. In some quarters, we can actually hear the clap-
ping. There is indeed a contrast to consider. 
Unified Theories of Cognition 
My William James Lectures give some indication of what it might mean 
for there to be a theory of mind, in the sense we have been discussing. 
Psychology has arrived at the possibility of unified theories of c o g n i t i o n -
theories that gain their power by having a single system of mechanisms 
that operate together to produce the full range of human cognition. 
I did not say they are here yet, but I argued they are within reach and 
that we should strive to attain them. Nor did I claim there was a single 
such unified theory. Indeed, in my lectures I argued that in our current 
state of knowledge, there would be several theories. I did claim that 
enough was known to attempt unified theories and that they had im-
mense benefits for cognitive science—bringing into one theoretical struc-
ture the constraints from the great store of empirical regularities that 
cognitive psychology has amassed, along with what we now understand 
about the mechanisms of cognition. 
The lectures were built around the presentation of an exemplar uni-
fied theory, embodied in a system called Soar, developed by John Laird 
of the University of Michigan, Paul Rosenbloom of the Information 
Sciences Institute at the University of Southern California, and me 
(Laird, Newell, and Rosenbloom 1987). Soar provides an appreciation of 
what is required of a unified theory, what its yield might be, and how 
ready the field is to develop them. Soar is only an exemplar; there are 
others as well (Anderson 1983). 
Figure 9.1 presents the elements of the theory of human cognition 
embodied in Soar. So far, I have taken care to say theory embodied in Soar. 
As we shall see, Soar is a specific kind of system—an architecture or 
machine organization.1 We usually take a theory of some domain, here a 
theory of the mind, as being the assertions about the nature of that 
domain—here assertions about how the mind is structured, how it oper-
ates, how it is situated, and so on. So Soar, as a system, cannot literally be 
a theory. But the theory asserts that the central structure in mind is the 

164 
H U M A N S A N D M A C H I N E S 
1. Controller-Perception-Cognition-Motor 
2. Knowledge and Goals 
3. Representation, Computation, Symbols 
4. An Architecture plus Content 
5. Recognition Memory (about 10 ms) 
6. Decision Cycles—Automatic (about 100 ms) 
7. Problem Spaces and Operators (about 1 sec.) 
8. Impasses and Subgoals 
9. Chunking (about 10 sec.) 
X - T —Y 
je je îd 
x -> t -» y 
T T T 
li 
i l R i i l 
oo§® J
àlOO 
•© 
10. Intended Rationality (100 sec. and up) 
Fig. 9.1. Soar as a unified theory of cognition 
cognitive architecture, that humans have one and that its nature deter-
mines the nature of mind. The theory then specifies a lot about that 
architecture. Soar is a system that embodies these particular specifics. 
Because the architecture is so central and determines so much about the 
mind, it is convenient to slip language a bit and identify Soar with the 
theory of cognition it embodies. 
Figure 9.1 enumerates the main mechanisms in Soar. The top four 
items are shared by all comprehensive cognitive-science theories of hu-
man cognition. Soar operates as a controller of the human organism, 
hence it is a complete system with perception, cognition, and motor 
components. This already takes mind in essentially functional terms—as 
the system that arose to control the gross movements of a mammal in a 
mammalian world. Soar is goal oriented with knowledge of the world, 
which it uses to attain its goal. That knowledge is represented by a 
symbol system, which means that computation is used to encode repre-

METAPHORS FOR MIND, THEORIES OF MIND 
165 
sentations, extract their implications for action, and decode specific de-
sired actions. Thus, Soar is an architecture—a structure that makes possi-
ble a hardware-software distinction. Most of the knowledge in such a 
system is embodied in the content that the architecture makes meaning-
ful and accessible. 
The rest of the items describe Soar from the bottom up, temporally 
speaking. Soar comprises a large recognition memory. This is realized by an 
Ops5-like production system (Brownston et al. 1985). A production sys-
tem consists of a set of productions, each consisting of a set of conditions 
and a set of actions. At each moment, the conditions of all productions 
are matched against the elements of a temporary working memory, and 
those productions that are satisfied then execute, putting new elements 
into working memory. Human long-term memory comprises many pro-
ductions, in the millions perhaps. A cycle of production execution also 
occurs very rapidly, around 10 milliseconds (ms).2 Although in artificial 
intelligence (AI) and cognitive science, productions are usually taken to 
correspond to operators (deliberately deployed actions), in Soar they 
correspond to an associational memory. Thus, production actions behave 
like a memory retrieval: they only enter new elements into working 
memory and cannot modify or delete what is there. Also, there is no 
conflict resolution (of the kind familiar from Ops5); instead, each pro-
duction executes independently, just like an isolated memory access and 
retrieval. 
The next level of organization, which occurs within about 100 ms, 
consists of the decision cycle. This comprises a sequence of retrievals from 
long-term memory (i.e., a sequence of production firings) which assem-
ble from memory what is immediately accessible and relevant to the 
current decision context. This sequence ultimately terminates when no 
more knowledge is forthcoming (in practice, it quiesces quickly). Then a 
decision procedure makes a choice of the next step to be taken. This 
changes the decision context, so that the cycle can repeat to make the 
next decision. At the 100 ms level, cognitive life is an endless sequence of 
assembling the available knowledge and using it to make the next deliber-
ate choice. 
The decisions taken at the 100 ms level implement search in problem 
spaces, which comprise the next level of organization, at the 1 second 
(sec.) level. Soar organizes all its goal-oriented activity in problem spaces, 
from the most problematical to the most routine. It performs a task by 
creating a space within which the attainment of the task can be defined as 
reaching some state and where the moves in the space are the operations 
that are appropriate to performing the task. The problem then becomes 
which operators to apply and in what order to reach a desired state. The 
search in the problem space is governed by the knowledge in the recogni-

166 
H U M A N S A N D MACHINES 
don memory. If Soar has the appropriate knowledge and if it can be 
brought to bear when needed, then Soar can put one operator in front of 
another, to step its way directly to task attainment. If the memory con-
tains little relevant knowledge or it cannot be accessed, then Soar must 
search the problem space, leading to the combinatorial explosion famil-
iar to AI research. 
Given that the problem-space organization is built into the architec-
ture, the decisions to be made at any point are always the same—what 
problem space to work in; what state to use (if more than one is avail-
able); and what operator to apply to this state to get a new state, on the 
way to a desired state. Making these choices is the continual business of 
the decision cycle. Operators must actually be applied, of course; life is 
not all decision making. But applying operators is merely another task, 
which occurs by going into another problem space to accomplish the 
implementation. The recursion bottoms out when an operator becomes 
simple enough to be accomplished within a single decision cycle, by a few 
memory retrievals. 
The decision procedure that actually makes the choice at each point is 
a simple, uniform process that can only use whatever knowledge has 
accumulated via the repeated memory searches. Some of this knowledge 
is in the form of preferences about what to choose—that one operator is 
preferred to another, that a state is acceptable, that another state is to be 
rejected. The decision procedure takes whatever preferences are avail-
able and extracts from them the decision. It adds no knowledge of its 
own. 
There is no magic in the decision cycle. It can extract from the mem-
ory only what knowledge is there, and it may not even get it all. And the 
decision procedure can select only from the options thereby produced 
and by using the preferences thereby obtained. Sometimes this is suffi-
cient, and Soar proceeds to move through its given space. Sometimes— 
often, as it turns out—the knowledge is insufficient or conflicting. Then 
the architecture is unable to continue: it arrives at an impasse. This is like 
a standard computer trying to divide by zero. Except that, instead of 
aborting, the architecture sets up a subgoal to resolve the impasse. For 
example, if several operators have been proposed but there is insuffi-
cient information to select one, then a tie impasse occurs, and Soar sets up 
a subgoal to obtain the knowledge to resolve the tie, so it can then 
continue. 
Impasses are the dynamo of Soar; they drive all its problem solving. 
Soar simply attempts to execute its top-level operators. If this can be 
done, Soar has attained what it wanted. Failures imply impasses. Resolv-
ing these impasses, which occurs in other problem spaces, can lead to 
other impasses, hence to subproblem spaces, and so on. The entire 

METAPHORS FOR MIND, THEORIES OF MIND 
167 
subgoal hierarchy is generated by Soar itself, in response to its inability to 
attain its objectives. The different types of impasses generate the full 
variety of goal-driven behavior familiar in AI systems—operator imple-
mentation, operator instantiation, operator selection, precondition satis-
faction, state rejection, and so on. 
In addition to problem solving, Soar learns continuously from its 
experiences. The mechanism is called chunking. Every time Soar encoun-
ters and resolves an impasse, it creates a new production (a chunk) to 
capture and retain that experience. If the situation ever recurs, the 
chunk will fire, making available the information that was missing on the 
first occasion. Thus, Soar will not encounter an impasse on a second 
pass. 
The little diagram at the right of chunking in figure 9.1 sketches how 
this happens. The view is looking down on working memory, with time 
running from left to right. Each little circle is a data element that encodes 
some information about the task. Starting at the left, Soar is chugging 
along, with productions putting in new elements and the decision proce-
dure determining which next steps to take. At the left vertical line, an 
impasse occurs. The architecture adds some elements to record the im-
passe, hence setting a new context, and then behavior continues. Finally, 
Soar produces an element that resolves the impasse (the element c at the 
right vertical line). Behavior then continues in the original context, be-
cause operationally resolving an impasse just is behavior continuing. The 
chunk is built at this point, with an action corresponding to the element 
that resolved the impasse and with conditions corresponding to the ele-
ments prior to the impasse that led to the resolution (the elements a and 
b ). This captures the result of the problem solving to resolve the impasse 
and does so in a way that permits it to be evoked again to avoid that 
particular impasse. 
Chunking operates as an automatic mechanism that continually caches 
all of Soar's goal-oriented experience, without detailed interpretation or 
analysis. As described, it appears to be simply a practice mechanism, a way 
to avoid redoing the problem solving to resolve prior impasses, thus speed-
ing up Soar's performance. However, the conditions of the productions 
reflect only a few of the elements in working memory at the time of the 
impasse. Thus, chunks abstract from the situation of occurrence and can 
apply in different situations, as long as the specific conditions apply. This 
provides a form of transfer of learning. Although far from obvious, this 
mechanism in fact generates a wide variety of learning (Steier et al. 1987), 
enough to conjecture that chunking might be the only learning mecha-
nism Soar needs. 
Chunks get built in response to solving problems (i.e., resolving im-
passes). Hence, they correspond to activities at about the 1 sec. level and 

168 
H U M A N S A N D MACHINES 
above. T h e chunk itself, of course, is a production, which is an entity 
down at the memory-access level at about 10 ms. 
The higher organization of cognitive activity arises from top-level 
operators not being implementable immediately with the information at 
hand. They must be implemented in subspaces with their own operators, 
which themselves may require further subspaces. Each descent into an-
other layer of subspaces means that the top-level operators take longer to 
complete, that is, are higher level. Thus, the time scale of organized 
cognitive activity climbs above what can be called the region of cognitive 
mechanism and toward the region of intendedly rational behavior. Here, 
enough time is available for the system to do substantial problem solving 
and use more and more of its knowledge. T h e organization of cognition 
becomes increasingly dictated by the nature of the task and the knowl-
edge available, rather than by the structure of the architecture. 
This rapid-fire tour through the mechanisms of Soar serves primarily 
to box its compass, to see the mechanisms that are involved. It is an 
architecture that spans an extremely wide range of psychological func-
tions. Some limits of the range should be noted. Perception and motor 
behavior currently exist in the theory only in nascent form. Perhaps as 
important, the impasse-driven means-ends structure that builds up in a 
given situation is ephemeral. Long-term stable organization of behavior 
could hardly be held in place by the momentary piled-up impasse 
subgoal hierarchy. Soar does not yet incorporate a theory of what hap-
pens as the hours grow, disparate activities punctuate one another, and 
sleep intervenes to let the world of cognition start afresh each morning. 
All these aspects must eventually be within the scope of a unified theory 
of cognition. Soar's failure to include them shows it to be like any scien-
tific theory, always in a state of becoming. 
Our description of Soar contains a strong emphasis on temporal level. 
Soar models behavior from about 10 ms on up to about 1,000 sec. (30 
min.). Soar, as a theory of human cognition, is tied strongly to the world 
of real time. Figure 9.2 provides a useful view of the time scale of human 
action. T h e characteristic time taken by processes fractionates our world 
into realms of distinct character. Neural systems take times of the order 
of 100 microseconds (¡usee) to 10 ms to produce significant effects. Cogni-
tive systems take times of the order of 100 ms to 10 sec. to produce 
significant effects. Beyond that, in the minutes to hours range, is some-
thing labeled the rational band. And up above that stretch time scales 
that are primarily social and historical, left blank because theories of 
unified cognition are initially situated in the lower bands, focused on the 
architecture. 
These bands correspond to realms of scientific law. The neural band 
is within the realm of physical law, as we have come to understand it in 

METAPHORS FOR MIND, THEORIES OF MIND 
169 
TIME SCALE OF HUMAN ACTION 
Scale 
World 
(sec.) 
Time Units 
System 
(theory) 
107 
months 
106 
weeks 
SOCIAL 
BAND 
105 
days 
IO4 
hours 
Task 
IO3 
10 minutes 
Task 
RATIONAL 
BAND 
IO2 
minutes 
Task 
IO1 
10 seconds 
Unit task 
10° 
1 second 
Operations 
COGNITIVE 
BAND 
IO-1 
100 milliseconds 
Deliberate act 
IO"2 
10 milliseconds 
Neural circuit 
IO"3 
1 millisecond 
Neuron 
BIOLOGICAL 
BAND 
IO"4 
100 microsec-
Organelle 
onds 
Fig. 9.2. Time scale of human action 
natural science. And it is physical law on down, although with a twist as it 
enters the realm of the very small and quantum indeterminacy. But the 
cognitive band, which is the structuring into a cognitive architecture, is 
the realm of what can be called representational law. By appropriate 
computational structuring, internal happenings represent external hap-
penings. The computations obey physical laws; they are physical systems 
after all. But they also obey the laws of what they represent. From the 
internal tokens that represent two numbers, an addition algorithm fash-
ions another internal token for a sum of the two numbers. To discover an 
addition algorithm is precisely to discover a tiny physical system that, 
while doing its physical thing, also produces situations that obey the laws 
of addition (given further encoding and decoding processes). 
As computations operate in the service of the system's goals, the sys-

170 
H U M A N S A N D MACHINES 
tem itself begins to behave as a function of the environment to attain its 
goals. This is the realm of reason. No rigid laws hold here, because goal-
oriented computation is precisely a device to circumvent whatever is in 
the way of goal attainment. In Aristotelian terms, this is the realm of 
final causes, whereas the neural band is the realm of efficient causes, and 
there was nothing in the Aristotelian scheme that corresponded to com-
putation, which is the apparatus for moving between the two. A key 
point in this is that it takes time to move away from the mechanics (the 
architecture) and up into rational behavior. And, indeed, it never fully 
happens, so that a longer but better term would be intendedly rational 
band. 
With the picture of figure 9.2, one can see that a unified theory of 
cognition is primarily a theory of the cognitive band. It provides a 
frame within which to consider the other great determiners of human 
behavior—the structures of the task environments people work in and 
the knowledge people have accumulated through their social worlds— 
but it does not determine this. Rather, it describes how these determin-
ers can be possible and what limits their expression. 
Fragments of the Theory 
Let me provide a few quick illustrations of the theory. These will be like 
strobe-light exposures—a fragment here, a flash there. Still, I hope they 
can bring home two critical points. First, Soar is a theory, in the same 
mold as theories in the other sciences, a collection of mechanisms that 
combine together to predict and explain empirical phenomena. The 
predictions come from the theory, not the theorist. Second, as a unified 
theory of cognition, Soar has a wide scope, both in types of behavior 
covered and in terms of time scale. Though never as great as wishes 
would have it, Soar can still stand for the possibility that unified theories 
of cognition might be in the offing. Let us begin with immediate reactive 
behavior, which occurs at a time scale of about 1 sec., and work up the 
time scale of human action. 
Stimulus-response compatibility 
Stimulus-response compatibility is a phe-
nomenon known to everyone, though perhaps not by that name. Anyone 
who has arrived at an elevator to find the Up button located physically 
below the Down button would recognize the phenomena. The Up button 
should map into the direction of travel—up on top. This human sense of 
should, in fact, translates into longer times to hit the button and greater 
chances to hit the wrong button. Stimulus-response compatibility effects 
are everywhere. Figure 9.3 shows another example, perhaps less obvi-
ous. A person at a computer editor wants to delete some word. The editor 
uses abbreviations, in this case dropping the vowels to get dlt. Thus, the 

METAPHORS FOR MIND, THEORIES OF MIND 
171 
Task: Intend to delete —> Type dlt (vowel deletion) 
ms 
Perceive. 
Minimum 
- 4 0 
Encode. 
1 production/letter (6 letters) 
- 1 2 0 
Attend. 
1 operator 
- 6 0 
Comprehend. 
1 operator to verify 
- 6 0 
Perception^ = 314 
- 2 8 0 
Intend. 
Get each syllable: 2 operators 
- 1 2 0 
Get spelling of syllable: 2 operators 
- 1 2 0 
Get each letter: 2 + 4 = 6 operators 
- 3 6 0 
Identify each letter: 2 + 4 = 6 operators 
- 3 6 0 
If consonant link to save: 3 operators 
- 1 8 0 
Issue command: Type letter: 3 operators 
- 1 8 0 
Mappingbj = 66 x 25 = 1650 
- 1 , 3 2 0 
Decode & move. Keystroke (can't split D&M): 3 x 180 
- 5 4 0 
Motorbj = 203 x 3 = 609 
- 5 4 0 
Totals: Totalbj = 2,573, Obs avg = 2,400 
- 2 , 1 4 0 
Fig. 9.3. 
SRC example: Recall command abbreviation 
person needs to get from delete to dlt to command the editor appropri-
ately. Stimulus-response compatibility occurs here. On the more compati-
ble side, the designer of the editor might have chosen delete itself, al-
though it would have required more typing. On the less compatible side, 
the designer might have chosen gro, thinking of get rid of. 
Figure 9.3 shows an accounting of how Soar would predict the time it 
takes a person to type dlt. First is the processing that acquires the word 
and obtains its internal symbol: perceive the sensory stimulus (in the 
experimental situation, the word was presented on a computer display); 
encode it (automatically) to obtain its internal symbol; attend to the new 
input; and comprehend it to be the task word. Second is the cognitive 
processing that develops the intended answer: getting each syllable; ex-
tracting each letter; determining if it is consonant; and, if so, creating the 
command to the motor system that constitutes the internal intention. 
Third is the motor processing: decode the command, and move the finger 
to hit the key (successively d, I, and t). The entire response is predicted to 
take about 2.1 sec. (2,140 ms), whereas it actually took 2.4 sec. 
Soar is operating here as a detailed chronometric model of what the 
human does in responding immediately in a speeded situation. This does 
not fit the usual view of an Al-like system, which is usually focused on 
higher-level activities. But a theory of cognition must cover the full tem-

172 
H U M A N S AND MACHINES 
poral range of human activity. In particular, if the theory of the architec-
ture is right, then it must apply at this level of immediate behavior. 
These operators and productions are occurring within the architec-
tural frame indicated in figure 9.1. But Soar is not an original theory here. 
Lots of psychological research has been done on such immediate-
response tasks, both theoretical and experimental. It has been a hallmark 
of modern cognitive psychology. In this case, the experimental work goes 
back many years (Fitts and Seeger 1953), and there is an extant theory, 
developed primarily by Bonnie John (1987), which makes predictions of 
stimulus-response compatibility. What is being demonstrated is that Soar 
incorporates the essential characteristics of this theory to produce roughly 
the same results (the numbers subscripted with bj are the predictions from 
John's theory). 
Acquiring a task 
Figure 9.4 shows a sequence of situations. At the top 
is a variant of a well-known experiment in psycholinguistics from the 
early 1970s (Clark and Chase 1972). In the top panel, a person faces a 
display, a warning light turns on, then a sentence appears in the left-
hand panel and a picture of a vertical pair of symbols in the right-hand 
panel. The person is to read the sentence, then examine the picture and 
say whether the sentence is true or not. This is another immediate-
response chronometric experiment, not too different in some ways from 
the stimulus-response compatibility experiment above. In this case, one 
can reliably predict how long it takes to do this task, depending on 
whether the sentence is in affirmative or negative mode, uses above or 
below, and is actually true or false. This experiment, along with many 
others, has shed light on how humans comprehend language (Clark and 
Clark 1977). 
Our interest in this example does not rest with the experiment itself 
but with the next panel down in the figure. This is a set of trial-specific 
instructions for doing the task. A cognitive theory should not only pre-
dict the performance in the experiment but also how the person reads 
the instructions and becomes organized to do the task. The second panel 
gives the procedure for doing the task. Actually, there were two variants 
of the experiment, the one shown, and one where 4 reads "Examine the 
picture" and 5 reads "Then read the sentence." These are not the only 
instructions needed for doing the task. The bottom two panels indicate 
increasingly wider contexts within which a person does this task. These 
panels, written in simple language, are an overly homogeneous and 
systematic way of indicating these layers of context. In an actual experi-
ment, the person would gather part of this information by observation, 
part by the gestures and behavior of the experimenter, and part by 
interaction directly with the experimental apparatus. 

METAPHORS FOR MIND, THEORIES OF MIND 
173 
+ 
Plus is above star 
* 
The experiment occurs 
1. Light turns on. 
2. Display shows. 
3. Subject reads, examines, and presses a button. 
Prior trial-specific instructions 
4. "Read the sentence." 
5. "Then examine the picture." 
6. "Press the T-button if the sentence is true of the picture.' 
7. "Push the F-button if the sentence is false of the picture.' 
8. "Then the task is done." 
Prior general instructions 
9. "At some moment the light will come on." 
10. "After the light comes on, a display will occur." 
11. "The left side of the display shows a sentence." 
12. "The right side of the display shows a picture." 
Introduction 
13. "Hello." 
14. "This morning we will run an experiment." 
15. "Here is the experimental apparatus." 
16. . . . 
Fig. 9.4. Acquiring a task 
Soar does both the top two panels (but not the bottom two). Focusing 
on the second panel, as the interesting one for our purposes, Soar takes 
in each simple sentence and comprehends it. This comprehension re-
sults in a data structure in the working memory. Soar then remembers 
these specifications for how to behave by chunking them away, that is, by 
performing a task whose objective is to be able to recall this information, 
in the context of being asked to perform the actual task. On recalling the 
instructions at performance time, Soar performs the task initially by 
following the recalled instructions interpretively, essentially by following 
them as rules. Doing this leads to building additional chunks (since Soar 
builds chunks to capture all its experiences). On subsequent occasions, 
these chunks fire and perform the task without reference to the explic-
itly expressed rule. Soar has now internalized this task and performs it 
directly thereafter. 
The point is that Soar combines performance and task acquisition in a 
single theory, as required of a unified theory of cognition. It shows one 
advantage of having unified theories. The theory of the performance 

174 
H U M A N S AND M A C H I N E S 
task is not simply stipulated by the theorist (as Clark and Chase had to 
do) but flows, in part, from the theory of how the task instructions 
organize the person to do that performance. 
Problem solving 
Let us move up the time scale. Figure 9.5 shows a little 
arithmetical puzzle called cryptarithmetic. The words DONALD, GER-
ALD, and ROBERT represent three 6-digit numbers. Each letter is to be 
replaced by a distinct digit (e.g., D and T must each be a digit, say D = 5 
and T = 0, but they cannot be the same digit). This replacement must lead 
to a correct sum, that is, DONALD + GERALD = ROBERT. The figure 
shows the behavior of a subject solving the puzzle (Newell and Simon 
1972). Humans can be given cryptarithmetic tasks and protocols obtained 
from transcripts of their verbalizations while they work. The subject pro-
ceeds by searching in a problem space; the figure shows the search explic-
itly, starting in the initial state (the upper left dot). Each short horizontal 
segment is an operator application, yielding a new state. When the search 
line ends at the right of a horizontal line, the subject has stopped searching 
deeper and returns to some prior state already generated (as indicated by 
the vertical line, so that all vertically connected dots represent the same 
state on successive returns). The subject often reapplies an earlier opera-
tor, as indicated by the double lines, so the same path is retrod repeatedly. 
It takes the subject about 2,000 sec. (30 min.) to traverse the 238 states 
of this search, averaging some 7 sec. per state. Although a puzzle, it is still 
genuinely free cognitive behavior, constrained only by the demands of 
the task. This particular data is from 1960, being part of the analysis of 
problem solving by Herb Simon and me (Newell and Simon 1972). A 
unified theory of cognition should explain such cognitive behavior, and 
Soar has been organized to do so, providing detailed simulations of two 
stretches, lines 1—4 and 8—12. Figure 9.6 shows the more complex behav-
ior fragment (lines 8—12), where the subject has trouble with column 5 of 
the sum (E + O = 0 ) and thus goes over the material several times, a 
behavior pattern called progressive deepening. These two stretches are far 
from the whole protocol, but they still amount to some 200 sec. worth. 
The reason for reaching back to old data is the same as with the 
stimulus-response compatibility and the sentence-comprehension cases. 
Initially, the most important element in a proposed unified theory of 
cognition is coverage—that it can explain what existing theories can do. 
One attempts to go further, of course. In the sentence case, it is getting 
the theory to cover the acquisition of the task by instruction. In the 
cryptarithmetic case, it is attaining completeness and detail. 
Development 
Finally, consider an attempt to understand how the de-
velopment of cognitive functions might occur. This territory has been 

METAPHORS FOR MIND, THEORIES OF MIND 
175 
1 2 3 « 9 • 7 t • 10 11 12 13 14 15 It 17 It It 20 21 22 23 M 25 2t Z7 2t 2t 30 31 32 DMK3t 37 3t 31 40 41 42 43 
1 Q M r 
tZ 
^ L4-1,n«3o 
> 
ROBERT 
V R a7vt, Rt-7. la3.G* 1 v2:G*t£ 
J 
0-5 
\ 
" E I 
y Deabe E » Oor E = 9 
E«_9o (A = 4 artc3 = 0) 
" t r 
V Start over: R «-9, L = 4. G 
V E * Oor E e 9: resolved E • 
" E 
( Exp lets E = 0 ) 
Go bat* lo Irsi board 
Try lor c3» 1. tai 
DeadeE:E«0.E = Y,c6«2 
Resolve: E = 9 necessary (twrefcxe R «- 9 o) 
I«- 8 (from get c3 = 1 tar A) 
DONALD 
• GERALD 
X L 
Fig. 9.5. 
Behavior of a person on the cryptarithmetic task 
mapped out by Piaget, who gave us an elaborate, but imperfect and 
incomplete, theoretical story of stages of development, with general pro-
cesses of assimilation and accommodation, oscillating through repeated 
equilibrations. Piaget also mapped the territory by means of a large and 
varied collection of tasks that seem to capture the varying capabilities of 

176 
HUMANS AND MACHINES 
Fig. 9.6. 
Soar simulation of the cryptarithmetic task 
children as they grow up. Some are widely known, such as the conserva-
tion tasks, but there are many others as well. 
This exploration with Soar uses the Piagetian task of predicting 
whether a simple balance beam (like a seesaw with weights at various 
distances on each side) will balance, tilt right, or tilt left with various 
placements of weights. As they grow up, children show striking differ-
ences in their ability to predict, only taking total weight into account 
(around 5 years), to considering both weights and distance, providing 
they are separable, to (sometimes) effectively computing the torque (by 
late adolescence). Developmental psychologists have good information-
processing models of each of these stages (Siegler 1976), models that are 
consonant with cognitive architectures such as Soar. What is still missing— 
here and throughout developmental psychology—is what the transition 
mechanisms could be (Sternberg 1984). That, of course, is the crux of the 
developmental process. It will finally settle, for instance, whether there 
really are stages or whether cognitive growth is effectively continuous. 
Soar provides a possible transition mechanism. It learns to move 

METAPHORS FOR MIND, THEORIES OF MIND 
177 
through the first two transitions: from level 1 (just weights) to level 2 
(weights and distance if the weights are the same) to level 3 (weights, and 
distance if they do not conflict). It does not learn the final transition to 
level 4 (computing torques).3 Soar predicts how the beam will tilt by 
encoding the balance beam into a description, then using that descrip-
tion to compare the two sides, and finally linking these comparisons to 
the three possible movements (balance, tilt-left, tilt-right). Soar has to 
learn both new encodings and new comparings to accomplish the transi-
tions, and it does both through chunking. Figure 9.7 provides a high-
level view of the transition from level 1 to level 2. It shows the different 
problem spaces involved and only indicates schematically the behavior 
within problem spaces. My purpose, however, is not to show these learn-
ings in detail. In fact, both types of learning are substantially less rich 
than needed to account for the sorts of explorations and tribulations that 
children go through. 
The above provides the context for noting a critical aspect of this 
effort to explore development with Soar. Soar must learn new knowl-
edge and skill in the face of existing learned knowledge and skill, which 
is now wrong. In this developmental sequence, the child has stable ways 
of predicting the balance beam; they are just wrong. Development im-
plies replacing these wrong ways with correct ways (and doing so repeat-
edly). That seems obvious enough, except that Soar does not forget its 
old ways. Chunking is a process that adds recognitional capability, not 
one that deletes or modifies existing capability. Furthermore, the essence 
of the decision cycle is to remain open to whatever memory can provide. 
Soar, as a theory of human cognition, predicts that humans face this 
problem, too, and there is good reason and some evidence on this score. 
Humans do not simply forget and destroy their past, even when proved 
wrong. 
The solution within Soar is to create cascades of problem spaces. If an 
existing problem space becomes contaminated with bad learning, a new 
clean space is created to be used in its stead. That is, whenever the old 
space is to be used, the new one is chosen instead. Of course, when first 
created, this new space is empty. Any attempt to use it leads to impasses. 
These impasses are resolved by going back into the old space, which is 
still around, since nothing ever gets destroyed. This old space contains 
the knowledge necessary to resolve the impasse. Of course, it also has in 
it the bad learning. But this aspect can be rejected, even though it cannot 
be made to go away. The knowledge for this must come from a higher 
context, which ultimately derives from experimental feedback. Once an 
impasse has been resolved by appropriate problem solving in the old 
space, chunks are automatically formed (as always). These chunks trans-
fer this knowledge into the new space. Thus, on subsequent occurrences 

178 
H U M A N S A N D MACHINES 
TRIAL «1 ( W «, D > ) 
BEFORE FEEDBACK 
G - GENERATE 
P - PREDICT 
D - DISTINGUISH 
C - COMPARE 
E - ENCODE 
CU - USE-COMPARISON 
N - GEN-NEW-PS 
S - SELECT 
NN - GENERATE NAME 
TRIAL #2 ( W 
D * ) 
AFTER FEEDBACK 
Fig. 9.7. Problem spaces used in learning about the balance beam 
of using the new space, it will not have to return to the old space. It may 
do so for some other aspect, but then that too is transferred into the new 
space. Gradually, with continued experience, the new space is built up 
and the old space entered less and less often. But it always remains, 
because Soar never knows all the information that was encoded in the 
old space, nor could it evaluate its quality in the abstract. Only in the 
context of an appropriate task does such knowledge emerge. 
The Scope of Soar 
Soar addresses a significant range of other phenomena that surround 
central cognition. Figure 9.8 provides a summary list. Heading the list is 
the demonstration that Soar accounts for the ability to be intelligent. One 
of the reasons AI is closely related to cognitive psychology is that func-
tionality is so important. A theory of cognition must explain how humans 
can be intelligent. But there seems no way to demonstrate this without 
constructing something that exhibits intelligent behavior according to 
the theory. Soar demonstrates this by being a state-of-the-art AI system 
(Laird, Newell, and Rosenbloom 1987). 
Soar exhibits the qualitative shape of human cognition in many global 

METAPHORS FOR MIND, THEORIES OF MIND 
179 
1. The ability to exhibit intelligence 
2. Global properties of cognitive behavior 
3. Immediate-response behavior 
4. Simple discrete motor-perceptual skills 
5. Acquisition of cognitive skills 
6. Recognition and recall of verbal material 
7. Short-term memory 
8. Logical reasoning 
9. Problem solving 
10. Instructions and self-organization for tasks 
11. Natural language comprehension 
12. Developmental transitions 
Fig. 9.8. 
Cognitive aspects addressed by Soar 
ways. For instance, it is serial in the midst of parallel activity and it is 
interrupt driven. Next, Soar provides a theory of immediate responses, 
those that take only about 1 sec. Stimulus-response compatibility was an 
example. Soar also provides a theory of simple discrete motor-perceptual 
skills, namely, transcription typing. In general, however, Soar is still defi-
cient in its coverage of perceptual and motor behavior, with typing as close 
as we have gotten to these critical aspects. 
Soar provides a plausible theory of acquisition of cognitive skills 
through practice. It also offers the main elements of a theory of recogni-
tion and recall of verbal material—the classical learning domain of 
stimulus/response psychology. Soar also provides some aspects of a 
theory of short-term memory, including a notion on how the plethora of 
different short-term memories (whose existence has been revealed ex-
perimentally) might arise. It provides a theory of logical reasoning, of 
problem solving, and of how instructions are converted into the self-
organization for doing new immediate-response tasks. In addition, it 
implies a specific but still undeveloped theory of natural language com-
prehension. Finally, it has demonstrated an idea for a transition mecha-
nism in developmental psychology. 
Figure 9.8 presents the range of things that one candidate for a uni-
fied theory of cognition has addressed. It has done this with varying 
degrees of success, depth, and coverage. No claim is made for its superi-
ority over existing cognitive theories. Indeed, Soar contains mechanisms 
that make it a variant of existing successful theories. The issue is not to 
demonstrate novelty but to show that a single unified theory can cover all 
these phenomena—that it is one architecture that does all of these tasks 
and does them in fair accord with human behavior on the same tasks. 
Finally, to come back to our main point, Soar is a genuine theory. It is not 
just a broad framework or a simulation language that provides a medium 

180 
HUMANS AND MACHINES 
to express specific microtheories. One does calculations and simulations 
with Soar and reasons from the structure of the architecture to behavior. 
Above all, Soar is not a metaphor for mind. 
SHOULD THE HUMANITIES MIND? 
We have taken a brief tour of Soar. Should the humanities care? They 
could be fascinated, of course, or repelled, or bored, or mildly pleased at 
being informed about events across the intellectual river. But should 
they care? 
Recall the contrast: metaphors for mind or theories of mind. T h e 
computer provides a metaphor for mind, to be used at will, for all that 
metaphor is good for. Soar represents the other side of the contrast. It is 
also the computer—a living representative of artificial intelligence and 
computer science, a state-of-the-art AI system realized as a computer 
program, employing many mechanisms historically central to AI. But 
Soar is also a theory of human cognition, a theory of mind. It is put forth 
not as metaphor but as a standard sort of scientific theory, analogous to, 
say, the valence theory of chemical compounds or the kinetic theory of 
gases in thermodynamics. Soar embodies much that has gone into the 
computer-derived theories of mind, which have played a central role in 
the cognitive sciences. Soar is not pure AI, whatever that might be, but 
the melding of cognitive psychology and AI. 
What follows for the humanities if it turns out that the metaphorical 
stance cannot prevail and that, with greater or lesser speed, a theory of 
mind emerges? Is the mind likely to clank, if the computational view is 
sustained? Or will it make no difference, because it will simply become a 
new psychophysics that occupies some obscure scientific corner, while 
the great questions march on always as before? Or will it turn the humani-
ties into a part of science? If such a transformation were to occur, would 
it be a good thing for the humanities? Would it finally give them power 
to deal with questions they have found it difficult to grapple with? Would 
they pose new questions? Would they start to accumulate knowledge as 
opposed to history? Or would it destroy them, by grinding them down 
into laboratory drudges focused only on small questions? Even if it could 
be maintained that the true, good, and beautiful still lay over the far 
horizon, would it raise so much havoc in the transition to not be worth 
the candle? And if it is not worth the candle, can it then be avoided? Or 
must it be suffered like the plague? 
Such questions cannot be answered any more than history can be lived 
in advance. One way to approach them is by speculating from Soar as a 
unified theory of cognition. Suppose this is how a cognitive theory will 
look. Suppose, further, this theory is as right about humans as, say, the 

METAPHORS FOR MIND, THEORIES OF MIND 
181 
classical valence theory is about chemical reactions (which is not all that 
right, by the way, and much better approximations exist in orbital 
theory). What kinds of things might follow for the humanities? What 
would such a science tell them about the nature of mind, hence the 
nature of man, that they might find useful, interesting, or provocative? 
I offer several such speculations. They must be taken as such. They can 
be appreciated, criticized, and used for further speculation. But they are 
not solid scientific claims. They attempt too much. Treat them as precur-
sors, perhaps. But none of this makes them metaphorical—approximate, 
irrelevant, or downright wrong, perhaps, but not metaphorical. 
Speculation 1. On being Logical and the Nature of Insight 
The issue is familiar and easily stated. Are people logical? Why not? Do 
people follow rules? If so, do they do so on everything and always? Why 
is insight opposed to reason? What is insight anyway? These are not all 
the same question, but they all cluster in the same corner of our concern 
with our nature. Indeed, the computer metaphor for mind is itself part 
of this cluster, though here we use the computational theory of mind to 
explore the nature of the corner. 
What does the theory say? Let us start with representation—the form 
in which knowledge about the world or task is encoded into internal data 
structures. Consider a situation with an apple, ball, and cap on a table, 
viewed by someone in passing (fig. 9.9). A moment later, when asked 
where the ball is, the person promptly answers that it is between the 
apple and the cap. The situation is simple, but prototypical, permitting 
the inference that humans have memory, containing knowledge of the 
external world, organized to permit the performance of tasks (in this 
case, answering a simple question about an observed situation). 
Two broad classes of representations can be distinguished (among 
others). On the one hand are propositions. The statement, the apple is to the 
left of the ball, can serve as prototype, although it is necessary to abstract 
away from its specifically linguistic form to admit various graphlike and 
Lisp-like symbolic structures (and PI of the figure is abstracted a little). 
On the other hand are models. The list (A B) in the figure at the right side 
(Ml) can serve as prototype. An item is to the left of another if it occurs 
before it in a list. 
Both representations are symbolic: they refer to something external, 
to (in this case) apples, balls, and their spatial relation, and they are 
realized in symbolic structures of a familiar kind. But they represent in 
different ways. Each requires different processes to encode knowledge 
into the representation and make use of it. Propositions involve the 
familiar apparatus of connectives, variables, predicates, functions, quan-
tifiers, modalities, and rules of inference. Their processing is easily de-

182 
HUMANS AND MACHINES 
Situation 
Proposition 
PI. 
P2. 
P3. 
P4. 
P5. 
A is to the left of B 
C is to the right of B 
If X is to the left of Y 
and Y is to the left of Z 
then Y is between X and Z 
If X is to the left of Y 
then Y is to the right of Z 
B is between A and C 
Model 
Ml. 
M2. 
(A B) 
(A B C) 
A 
-B is between A 
and C 
Fig. 9.9. 
Propositions and models 
limited and realized in many kinds of computer programs, for exam-
ple, theorem provers. Models are also familiar. Their structures can be 
put in part-to-part correspondence with what they represent. Their 
processing is not quite so easily delineated, but it can be described as 
matchlike. 
The difference is easily illustrated in figure 9.9 by what is required to 
answer the question of the whereabouts of the ball. On the propositional 
side, PI and P2 encode the information about the table as acquired in 
passing. P3 and P4 are two general rules of inference, and by their 
appropriate application, the answer P5 is produced. On the model side, 
M2 encodes the information acquired in passing (being an augmentation 
of Ml, the initial encoding), and then inspection of this representation 
(by the eye at the bottom) simply reads off the fact that the ball is 
between the apple and the cap. T h e reading-off process must, of course, 
embody the semantics of the encoding, which means it must have been 
formed especially for this encoding scheme (though not especially for 
this particular question). 
Soar posits that humans always represent by models.4 The eye of 
figure 9.9 is the recognition memory of figure 9.1. The construction of 
special recognizers for various classes of tasks is accomplished by chunk-
ing in response to initially working it through deliberately. There is a fair 
amount of evidence generally in the cognitive literature that humans do 

METAPHORS FOR MIND, THEORIES OF MIND 
183 
use models (Johnson-Laird 1983), although the case is not conclusive. It 
is difficult to be conclusive about representations since alternative repre-
sentations can mimic one another in many ways. 
But we are interested in what our theory says. Why might humans 
employ models? A prime reason is functional. A model is a representa-
tion that can be processed rapidly and with assurance by recognition 
(matchlike) processes. Two models of something can be seen to be the 
same (or different) just by putting them into part-to-part correspon-
dence and checking each part locally. Not only are models fast but they 
can be relied on to be fast. Propositions, however, require general infer-
ence techniques—essentially the proving of theorems and the derivation 
of results. These are processes that can take an indefinite amount of time 
and that lead to combinatorial explosions. For a system built to operate in 
real time, under the necessity of rapid response, models are substantially 
more satisfactory than propositions. It might also be observed that mod-
els are a natural representation for recognition-memories such as Soar's; 
after all, the production system is the eye that surveys the model and sees 
its properties. But that is not a coincidence. Model and recognition are as 
yin and yang. 
Models satisfy another function, namely, the relation of being the inter-
nal representation of the external situation with which the organism is in 
close-coupled sensory contact. To track and fixate a moving visual target 
requires an internal representational structure. Such a structure func-
tions as a pointer-structure to permit the coordination of eye movements, 
occlusion, object permanence when the object disappears behind a tree, 
and so on. It has intense real time constraints, for continuous updating 
and for coordinated action. Phenomenally, it would be transparent— 
pipes through which attention reaches out to contact the moving scene. 
Models with their part-to-part correspondences are clearly fitted for this 
task—built like an optic fiber, with its multiplicity of channels, to stay with 
the pipe metaphor. It is a small (but real) leap to take the representations 
for all thought as being grounded structurally in the representations that 
are available for perceptual-motor purposes. But it is another reason to 
believe that human representations are models, to wit, they grew out of 
the operational models used in perceiving and acting. 
As might be expected, a trade-off is involved. Models are not nearly as 
powerful a representation as propositions. In their pure form, they can-
not deal with notions of disjunction, negation, or quantification. Even if 
augmented by various notations, such as a tag that a given model part is 
not in the real situation (which thus adds a local pro positional element), 
they remain extremely limited. However, propositional representations, 
especially forms that are equivalent to quantified logics, have essentially 
unlimited representational power. 

184 
HUMANS AND MACHINES 
In terms of the theory, humans are not logical because models do not 
lend themselves to working with general situations. Models work well for 
situations such as that of figure 9.9. But human failings in reasoning are 
typically revealed by tasks such as syllogisms: 
All bakers are golfers. 
Some bakers are not 
cardplayers. 
What follows necessarily about golfers and cardplayers? 
It is difficult to represent such quantified situations with models. The 
expressions can represent a diversity of possible situations, which cannot 
be captured in any single model. To reason correctly requires generating 
all possible models and accepting as valid only what holds in all of them. 
But this requires elaborate and controlled processing, which humans 
find difficult to do in their heads. 
That humans represent by models does not rule out their employing 
propositions mentally. They certainly hold internal dialogues with them-
selves and, no doubt, more complex reasonings as well. Within the 
theory, this occurs by working with models of propositions. The internal 
representation is a model of the propositional form, and the operators 
are rules of inference. They could be rules such as P3 and P4 in the 
figure, or even more basic ones such as modus ponens and substitution, 
which apply encoded forms of rules such as P3 and P4 to other proposi-
tions such as PI and P2. This might seem to abolish the distinction 
between models and propositions. On the contrary, it reveals how differ-
ent they are. For the cognitive system, working with a model is working 
directly in terms of what the model refers to. Thus, when it is working 
with propositions, it is working directly with propositions, all right, but 
therefore only indirectly—that is, at one remove—from what the propo-
sitions refer to. This is the basis, within the theory, for the fact that 
following rules is always cognitively remote, compared to directly grasp-
ing or seeing the facts themselves. 
Insight is bound up with the modellike structure we have described. 
Seeing that the ball is between the apple and the cap in M2 of figure 9.9 
is as close to direct appreciation as the organism comes. No application of 
rules mediates the process.3 The recognition of the memory system is 
inner sight, so to speak. It is not a homunculus: it does not reason, and it 
does not reflect (that happens in the higher levels of decision cycles and 
problem spaces). It responds to patterns. The patterns it sees are not 
fixed in advance, say, by nature. Chunking adds continually new pat-
terns. These can be highly specific, for example, involving the relation-
ship of betweenness, as expressed in a given modellike representation. 
Suppose all the above were true? Should the humanities care? It 
would seem so. The central point would be to make evident exactly why 

M E T A P H O R S FOR MIND, T H E O R I E S OF MIND 
185 
it is that humans fail to be logical or arrive at conclusions by applying 
rules versus seeing the matter directly. It would also make clear that this 
depends on specific features of the human architecture and why these 
features are part of the architecture. This would lead on to understand-
ing that there could be intelligent systems with different architectural 
properties, which would be quite different—never showing insight, or 
always being logical on small reasoning tasks, or never being able to 
apply rules and only working by insight. Such a zoo of intelligences 
would let us understand our own humanity, with the liberation that 
comparative study always brings. 
Speculation 2. On being Embedded in the Natural Order 
The issue is the place of man in the universe. The background can start 
with the special version of this in our Western heritage—Greek and 
Hebrew roots, through Rome and up through the emergence of Euro-
pean civilization. A strand of this heritage is the uniqueness of man and 
his separateness from the other creatures of the world, even his hege-
mony over them. Not all cultural heritages have this particular homo-
centric focus, but ours certainly does. The scientific contribution to this 
heritage, at least in the received version of cultural history, is a succession 
of defeats for uniqueness and special position and a gradual realization 
that man is part of nature, not to be distinguished in kind and maybe 
only modestly in degree. Some of the major scientific revolutions are 
associated with such defeats (or victories, if you are rooting for the other 
side). Think of the Copernican revolution, which decentered man and 
his earth, the Darwinian revolution, which joined man firmly with the 
animals, and the Freudian revolution, which removed man's action from 
the control of his own conscious will. 
One current battleground over human uniqueness seems to be the 
human mind—its intelligence, ability to use language, and so on.6 The 
computer, itself, is one of the skirmish lines. For if humans were like a 
computer—whatever that might mean, given the diverse and multiple 
notions comprising the computer metaphor for mind—then humankind 
would no longer be unique. Or at least its mind would not, so that some 
other source of uniqueness would have to be found. And the supply is 
becoming a little lean. 
What does the theory say? There is the obvious statement that a scien-
tific psychology makes the human mind a part of the natural order. 
Indeed, a scientific biology does so almost as well. However, both could 
still leave the central claim of uniqueness essentially untouched. Instead, 
let us focus on the attempt to admit computers as capable of solving 
difficult tasks but to take them as doing so in an entirely different way 
than human intelligence. Thus, it is said, the computer solves problems 

186 
HUMANS AND MACHINES 
by brute force. In the context of the grand battle above, such a move 
yields on functionality—solving difficult problems is not unique to 
humans—but preserves style and type. In the words of Monty Python's 
Flying Circus, any discussion of computer intelligence should be pref-
aced by, "And now for something completely different." 
Figure 9.10 provides an analysis of this issue. It shows a space within 
which intelligent systems are located by the means they use to perform 
their task.7 T h e vertical axis is preparation, the extent to which a system 
draws on what it has prepared in advance of the task. The horizontal axis 
is deliberation, the extent to which a system engages in processing once the 
task has begun. T h e curves represent equal-performance isobars. Differ-
ent choices of how much to draw on prepared material and how much to 
compute once the task is set can yield the same performance—more 
preparation and less deliberation versus less preparation and more delib-
eration. Traveling out along the arrow moves from lower performance 
isobars to higher performance ones. Both axes represent knowledge— 
knowledge that comes from stored memory structures (what has been 
prepared) and knowledge that comes from computation during the deci-
sion process. T h e axes, however, are not measured in knowledge ab-
stractly. Stored knowledge is measured in structures (number of rules or 
number of memory bits) and acquired knowledge is measured in situa-
tions searched or processed. 
This trade-off is fundamental to information-processing systems. T h e 
structure of a system places it in a local region of this space, for it tends to 
treat all tasks similarly. T h e rough locations of various types of AI sys-
tems are shown in figure 9.10. T h e early AI systems were search ori-
ented, with little knowledge (equivalent to a dozen rules) and modest 
search (hundreds of situations). Expert systems use more knowledge (up 
to about 104 rules currently) but do much less search. Indeed, they can 
be seen as an exploration into what can be gained by immediately accessi-
ble knowledge without appreciable search. Hitech, Hans Berliner's high-
master chess machine (Berliner and Ebeling 1988) is way out at the 
extreme of deliberation, with about 107 situations examined per external 
move and with only about 102 rules. Hitech would certainly qualify as 
one of those systems that attains its results by brute force, that is, by 
massive search. 
It is also possible to locate humans on this curve at least approxi-
mately. Taking chess as an example, humans can search only about 102 
situations in deciding on a move. This rough estimate can be derived 
from Soar and is attested empirically. But humans have about 103 rules, 
as inferred from a Soar-like theory (Simon and Gilmartin 1973) to ex-
plain the empirical results of the well-known chess perceptions experi-
ments (Chase and Simon 1973). Chess is only one area of expertise, of 

METAPHORS FOR MIND, THEORIES OF MIND 
187 
Immediate Knowledge 
(prepare) 
Search Knowledge 
(deliberate) 
Fig. 9.10. 
T h e preparation vs. deliberation trade-off 
course. The total knowledge that a human has available over all tasks 
would amount to many times this, although perhaps not by more than 
two orders of magnitude. 
This space provides a fundamental view of how information-
processing systems can differ and yet be related to one another. It tells 
us that systems, such as Hitech, are not completely different—not an 
unrelated way of attaining task performance (labeled say as brute force) 
but rather a different part of the total space of information-processing 
systems. The same considerations enter into systems such as Hitech as 
into other systems in the space. For instance, once the architecture of a 
system is fixed, whether for a human or Hitech, the amount of delibera-
tion in a given time becomes essentially fixed. Then the system can 
improve only by vertical movement in the space (see the small vertical 
arrows in fig. 9.10). Indeed, Hitech has moved from a low expert to a 
high master entirely by adding recognition knowledge. For another 
instance, it is possible to have systems that move back and up along an 
isobar, that is, decreasing deliberation and increasing preparation. Soar, 
with its chunking, is such a system. It appears much more difficult to 
move in the other direction. In fact, I do not know any artificial systems 

188 
H U M A N S AND MACHINES 
that do so in any substantial way. To understand intelligence requires 
understanding the nature of this entire space. 
Having located humans and brute force intelligent systems in the same 
space, can we push farther? Why, for instance, are humans located where 
they are?8 The theory we are assuming (Soar) offers a clue and echoes the 
prior example. Human minds are controllers of real time systems. Their 
form as recognition-intensive rather than search-intensive systems is func-
tional in that respect. Furthermore, their architecture is structured to 
learn continuously from experience, which continuously moves the sys-
tem away from deliberation toward recognition.9 
Suppose all the above were true? Should the humanities care? This 
analysis denies the separateness of machinelike, brute force intelligence 
from human recognition-intensive intelligence, which relates only to one 
aspect of whether the human mind is unique and what assures its unique 
role in the universe. Nevertheless, this is reason enough to care, given 
the long multistage battle on this topic, which has occupied such a secure 
place in Western thought. 
There is, perhaps, another small yield. Such a view emphasizes that 
our minds are a technology—like other technologies in being a set of 
mechanisms for the routine solution of a class of problems. Technologies 
differ with respect to the features of the natural order they exploit and 
to the class of tasks they solve. But all technologies have much in com-
mon. This might join with our growing appreciation of other natural 
biological technologies—the biochemical technologies of metabolism, 
the macromolecular (DNA/RNA) technologies of replication and protein 
manufacture, and the cellular technologies of the immune system. We 
might come to realize the essential role of technologies in the scheme of 
all organized complex systems, whether natural or artificial. This might 
even lead us to accept technology as an essential part of what is human 
and therefore an essential part of a liberal and humanistic education. 
That also would be reason enough for the humanities to care. 
Speculation 3. On Linking the Mundane and the Sublime 
The issue is the nature of creativity, which occupies a special place in the 
humanities. There is confounding between the role of social worth, 
which introduces an unsettling relativity, and the role of individual tal-
ent, which is where we Westerners seem to want to ground creative 
action. However, too much of what the humanities celebrate—from 
great composers to great artists to great writers—is polarized by the 
matter of creativity. The topic remains special. 
What does the theory say? There is a long history of attempts within 
cognitive psychology to describe creativity as just problem solving (Newell, 
Shaw, and Simon 1962). More specifically, the activity of the creative agent 

METAPHORS FOR MIND, THEORIES OF MIND 
189 
within the total social and historical creative process is asserted to be 
intellectual activity as predicted by Soar-like theories of cognition. It con-
sists of searches in problem spaces, under various degrees and amounts of 
knowledge, as produced by the experiences of the agent. This does not 
provide a full accounting of what happens in creative activity, of why 
certain creative events occur on certain occasions with certain individuals, 
and so on. Much is buried in the social and historical contexts. The theory 
does say that no special processes are involved, beyond those that we 
understand from more mundane cognitive activities.10 
Such a claim does not follow from the theory. Creativity is a phenome-
non in the world, not a defined notion within the theory. Rather, such a 
claim obtains support (or not), as the cognitive theory of the mind is 
successful (or not) in explaining empirically labeled creative activities. 
We need to examine attempts to describe and explain creative activities, 
and clearly the more creative, the better. 
A useful case is the research over the years by my colleague Herb Simon 
to understand scientific discovery in information-processing terms. This 
work has recently been summarized at book length (Langley et al. 1987), 
emphasizing that scientific discovery is problem solving as formulated in 
current cognitive theories. The theory that the work draws on is not a 
detailed cognitive architecture, such as Soar, but a more generalized for-
mulation that is consonant with the Soar model, to wit, symbol manipula-
tion, goal hierarchies, problem spaces, and recognition memory. The tem-
poral grain of scientific discovery is too long to require sharp assumptions 
about the details of the architecture. By the same coin, this work extends 
the reach of a theory of the architecture, such as Soar. 
The methodology employed in this research is both interesting 
and important. It explains historically significant events of scientific 
discovery—Boyle's law or Kepler's third law. These satisfy the criteria 
of being highly creative events. It is not possible to experiment with 
these events directly; they are history. Therefore, the goal is to reproduce 
these discoveries by a program. The actual data that the discoverer 
used is presented to a discovery program, and the program should 
make the actual discovery. It should produce Boyle's law, for instance. 
The context of what the discoverer knew is set as closely as possible 
according to the historical evidence. The record is only partial, espe-
cially the record of immediate knowledge and skill. But, in a positive 
confluence, creative scientific discoveries are exactly those on which 
historians of science lavish special attention, so that often much can be 
said. This approach is quite analogous to that of the paleoarchaeolo-
gists, who endeavor to chip flints by the methods available to early man, 
or to Thor Hyderdahl, with his sailing of the Ra from Ecuador to the 
Polynesian islands. Care must be taken, both in setting the stage and in 

190 
H U M A N S A N D MACHINES 
interpreting the results. But the yield is new data of a particularly 
intriguing and relevant sort. 
Figure 9.11 summarizes the yield of this research effort. The cases are 
taken from Langley et al. (1987), plus one more recent effort, Kekada 
(Kulkarni and Simon 1988), which reproduces the synthesis of urea. 
There have been several different programs, each of which addresses a 
different type of scientific discovery problem. The varieties of the Bacon 
program perform data-driven induction from quantitative data, Glauber 
and Stahl discover qualitative laws, and so on. Some of the programs 
provide alternatives or more complete versions of the same discovery. 
The exact number of cases depends on how to count, but essentially a 
score of examples has now been worked through. It is important to 
realize that these programs (Bacon, etc.) actually discover the laws. The 
stipulation is the context of discovery, and the program then engages in a 
search in the relevant problem spaces. 
This research raises many questions in addition to providing support 
for the general proposition that creativity is continuous with problem 
solving on mundane tasks. Such programs require very modest search: 
they discover the laws quickly. Scientific discovery, as we well know, takes 
long efforts. Thus, there is dissonance between theory and common 
understanding. There are several possibilities to be explored in explain-
ing this. Perhaps the greats, like the rest of us, never find the time 
actually to do science, so when they finally get a few hours, interesting 
things happen. Perhaps there is lots of scientific detailed work to be 
done, such as setting up experiments and tabulating data, activities not 
accounted for by these programs. Perhaps the problem spaces they work 
in are really much bigger and the problem much more difficult, but that 
explanation contains a disturbing reversal. Perhaps, finally, the creative 
activities are really located elsewhere, and the actual discovery from the 
data of, say, Boyle's law, is just a minor bit of routine activity. Then the 
programs are working on the wrong aspect. However, one should not 
embrace this last explanation too quickly. The events dealt with by the 
programs have certainly been considered central by scientific philoso-
phers and historians and by the scientists themselves—as attested by 
Archimedes's cry of Eureka! as he leapt from a bath that could not have 
lasted more than a few hours. 
Suppose all the above were true? Should the humanities care? The 
research certainly has an impact on history of science and philosophy of 
science. But that is to be expected, because the research speaks to them 
directly with the details of its findings. If, however, the research were 
taken to be a conclusive demonstration about the nature of creativity, it 
would affect all future discourse on the topic. But the research has 

METAPHORS FOR MIND, THEORIES OF MIND 
191 
Bacon 1 
Boyle's law, Galileo's law of uniform acceleration, Ohm's law, 
Kepler's third law 
Bacon 2 
(Variant of Bacon. 1 with different heuristics) 
Bacon 3 
Ideal gas law, Kepler's third law, Coulomb's law, Ohm's law 
Bacon 4 
Archimedes' law of displacement, Snell's law of refraction, Black's 
law of specific heat, law of gravitation, Ohm's law, law of conserva-
tion of momentum, chemical law of combination 
Bacon 5 
Snell's law of refraction, conservation of momentum, Black's specific 
heat, Joule's law 
Glauber 
Acids and alkalis (bases) 
Stahl 
Phlogiston theory, Black on Magnesium alba, Lavoisier's caloric 
theory and the discovery of oxygen 
Dalton 
Hydrogen and oxygen to water, plausible extension to particle phys-
ics reactions, Mendelian genetics 
Kekada 
Orinthine cycle (synthesis of urea) 
Fig. 9.11. Scientific discoveries reproduced by programs of Langley et al. (1987) 
and Kulkarni and Simon (1988) 
focused only on scientific creativity. Many other activities are included 
under the rubric o f creativity—novel and impressive performances in 
the fine arts, the performing arts, literature, and poetry—to list only 
those in the heartland of the humanities. T h e r e is much evidence about 
the separability o f these various areas, each involving unique technolo-
gies to be manipulated and skills to do the manipulations. Thus, it is 
possible to treat this evidence from creative scientific discovery as frag-
mentary. However, it would seem to raise strong enough signals to en-
courage seeking analogous investigations in related areas where creativ-
ity plays a similarly important role. 
Speculation 4. On Knowing and Not Articulating 
T h e issue is that humans know things they cannot articulate. This is 
often said with an overtone of amazement or disbelief. O n e scientist cum 
philosopher, Michael Polyani, even turned this observation into the cen-
tral theme o f his philosophic oeuvre; he called it tacit knowledge (Polyani 
1958). A favorite example was riding a bicycle, where it seemed clear for 
purposes o f philosophy (i.e., without gathering empirical data) that bicy-
cle riders could not articulate how they rode a bicycle, nor would telling 
nonbicyclists how to ride, that is, what physical principles to use, enable 
them to ride bicycles. T h e central significance of tacit knowledge has 
been widely acknowledged throughout the humanities (Grene 
1966), 
though not always tied to Polyani. 

192 
HUMANS AND MACHINES 
What does the theory say? It yields a straightforward answer to why 
human cognition shows the phenomena of tacit knowledge. Actually, it 
provides two separate answers: one for extended behavior and one for 
immediate behavior. The extended case comes from the nature of long-
term memory, which is a recognition memory, in the sense we described 
above. In such a memory, knowledge is accessed by means of the re-
trieval cues that occur in short-term working memory. Thus, knowledge 
is accessible only if the right cues exist. But the cues are right only if they 
pull out the right knowledge, and the system cannot know what the right 
knowledge is. That is what it is trying to find out. Thus, the system in 
effect must guess at the retrieval cues (although the guessing can be 
intelligent). This situation holds for any associative or content-addressed 
memory, not just the specific recognition memory system posited for 
Soar (fig. 9.1). 
In such a memory, it is no wonder that what the system knows for 
some purposes is not generally available to it for another. Couple this 
with the fact, in Soar, that procedures are also stored this way, that is, 
available by recognition when the behavior is to be produced. It follows 
that people in general cannot articulate the knowledge of how they 
behave. When they do, they only construct an articulated view on the 
basis of the internal and external observation of their own performance. 
The immediate-behavior case arises from the structure of the decision 
cycle (fig. 9.1). The decision cycle is composite, consisting of a sequence 
of production firings that accumulate the available knowledge prior to a 
decision actually being made. Simple reasoning occurs within a cycle as 
productions trigger succeeding productions. However, the decision cycle 
is the shortest time scale at which deliberate action can be taken. It is not 
possible for a person to articulate what goes on within the decision cycle. 
They can report various products that end up in working memory, but 
there is no way they can monitor the detailed processing within the cycle 
whereby these products originated. Soar provides an explicit picture of 
the lower limits of articulation. This is not, in general, what Polyani was 
referring to by tacit knowledge, but both types of limitations easily merge 
together, if there is no theoretical reason for separating them. 
Suppose all the above were true? Should the humanities care? The 
main effect might be just debunking—to make obvious the limits on 
articulation, so that it would cease to play a role in larger, more profound 
discussions. The consequences of knowing without articulation remain, 
however. It remains an important ingredient in discussing artistic and 
literary skills and how they relate to the thoughts that artists and writers 
have of their own art and craft. From this standpoint, having a theory of 
the mechanisms that lead to the separation of knowing from articulating 
would seem to be extremely worthwhile. 

METAPHORS FOR MIND, THEORIES OF MIND 
193 
Speculation 5. On Repression 
The issue is emotion and cognition. One striking feature of modern 
cognitive theories is their failure to incorporate emotional behavior with 
the cognitive mechanisms. Soar is no exception. There have been a few 
efforts (Colby 1975; Mueller 1987; Oatley and Johnson-Laird 1987; Si-
mon 1967), which mostly converge on the role of emotions in the inter-
ruption of behavior. However, they do not change the overall picture. 
This situation reflects the problematic relation of emotion to reason in 
our society as a whole. The emotion side of the scientific literature shows 
the same tension. For instance, although some theories actually compute a 
trajectory of a person's affective state during an interaction, they have no 
cognitive or task dimension (Heise 1977). Yet, it seems almost undeniable 
that emotion and affect exist within, around, among, and throughout the 
mechanisms of the cognitive architecture. One potential approach is to 
identify cognitive mechanisms that might enter into emotional behaviors 
in an interesting way, as preparations for understanding how cognition 
and emotion might fit together into a single system. 
What might the theory suggest? The snapshot on developmental 
mechanisms in Soar provides an interesting example. Recall (fig. 9.7) 
that the major system problem for Soar in the balance beam task was how 
to relearn in the face of prior learning that is wrong. This problem has a 
specific character in Soar, because chunking is the permanent acquisition 
of productions. Soar's solution was to create new problem spaces, with-
out the bad learning. They did not necessarily contain all the good 
learning, but these new spaces were linked with the original ones, so that 
Soar would revert into the originals. By operating momentarily in the 
old space, with additional considerations from the new one to filter out 
the wrong behavior, it could operate successfully. Chunking in the old 
space gradually transfers filtered behaviors into the new space. In this 
respect at least, Soar would not have to return to the old space on 
subsequent occasions. This entire mechanism is built within Soar, using 
chunking, and does not involve any architectural modifications. 
The purpose in reviewing this mechanism is to note what has been 
produced. Problem spaces that contain undesirable effects are being 
gradually insulated from access by the new problem spaces built in front 
of them. Gradually, it will become more and more difficult to access these 
insulated spaces. However, if on some occasion, perhaps much later, a 
way of working with the new space occurs which has not previously been 
attempted, the old space will be reentered. It could be like opening the 
door to a long-locked room. The mind is populated by such old spaces, 
since new ones get overlaid with additional new ones ad infinitum, de-
pending on the learning history of the system. 
This looks like a mechanism for repression, as Freud has taught us to 

194 
H U M A N S AND MACHINES 
understand it, namely, as central in the emotional life of the individual. 
The Soar mechanism was not deliberately designed for repression; this is 
a side effect. It was designed to let new learning become effective and 
not get mixed up with old learning that had been rejected. It is a cogni-
tive mechanism for cognitive purposes. When we consider what a fully 
emotional mind might be like, we can imagine that the phenomenon we 
(as good neo-Freudians) think of as repression would involve this mecha-
nism. This does not say that Freudian repression (more generally, affec-
tively grounded repression) is nothing but this mechanism. We know too 
little about emotion within the cognitive realm to guess at that. Rather, it 
opens the possibility that emotional repression will inhabit and use such a 
mechanism, perhaps rather like the hermit crab, a mechanism defined 
for entirely cognitive reasons, and become an integral part of the emo-
tional apparatus. 
Suppose all the above were true? Should the humanities care? I 
would think so. The attempt to understand the realms of behavior of 
major significance to the humanities—represented here by emotion 
and affect—will be enriched by the use of theories of cognition. These 
will be a source of ideas about the character of emotion and how it is 
manifest. In short, such a theory of mind might end up becoming a 
central theoretical resource for the humanities. 
CONCLUSIONS 
We have had five speculations to provide some indication of what might 
follow if there were to emerge from the coming of the computer a theory 
of mind. They represent a potential yield from Soar, but Soar, in turn, 
represents the possibility of a unified theory of cognition, and such a 
theory, in its turn, represents the general and continued development of 
cognitive science. 
These speculations are fragile, but I hope they are useful. A greater 
concern, perhaps, is their being something of a congeries, rather than a 
coherence. This arises from the method we chose, namely, to move from 
Soar toward the humanities. It was not within my ken to pick a central 
issue in the humanities and build the case for what a theory of mind 
might say. Scattershot seemed the only way to have a chance at contact. 
The point of all the speculations is to illustrate that a theory of mind 
has a wealth of detail that allows sustained argument and investigation 
about why humans are the way they are. There are mechanisms of 
particular shapes and kinds behind the capacities and limits of our abili-
ties to articulate, be logical, have insight, or repress. In this web of 
mechanisms lies many of the clues to our humanity—what it is like and 
what it can be like. The details of these speculations are not critical for 

METAPHORS FOR MIND, THEORIES OF MIND 
195 
this point, though to some extent their plausibility is. But most important 
is that they arise f r o m the operations o f a theory, not from reflections on 
a metaphor. 
Finally, does it m a k e any difference to the humanities which o n e o f 
these two ways o f viewing the matter o f mind, m e t a p h o r or theory, turns 
out to be sustained? T h e two views are not symmetric. Sustain the meta-
phorical view and a nonevent has happened. T h e c o m p u t e r as m e t a p h o r 
enriches a little o u r total view o f ourselves, allowing us to see facets that 
we might not otherwise have glimpsed. But we have been enriched by 
metaphors before, and on the whole, they provide just a few m o r e 
threads in the fabric o f life, nothing more. T h e c o m p u t e r as generator o f 
a theory o f mind is a n o t h e r thing entirely. It is an event. Not because o f 
the c o m p u t e r but because finally we would have obtained a theory o f 
mind. For a theory o f mind, in the same sense as a theory o f genetics o r 
o f plate tectonics, will entrain an indefinite sequence o f shocks t h r o u g h 
all o u r dealings with m i n d — w h i c h is to say, through all o u r dealings with 
ourselves. A n d the humanities might just be caught in the maelstrom. O r 
so it would seem. 
N O T E S 
1. Actually, of course, Soar is realized as a set of programs in various program-
ming languages (Lisp and C) that run on a wide range of standard computers. 
2. This and other times are asserted of human cognition; they are not the 
time for running Soar on a computer work station. 
3. There is good reason that it should not: the upper reaches of capability 
are not yet well charted empirically. 
4. Actually, the current Soar has a more general attribute-value representa-
tion (Laird, Newell, and Rosenbloom 1987), but we posit that humans represent 
external situations by forming models in this more basic representation. 
5. Production systems are often referred to as rule-based 
systems. In many 
systems, they function as rules with a rule interpreter. In Soar, productions 
function as an associative recognition memory and not as rules, as that term is 
used in philosophical discussions of rule following. 
6. There need not be a unique battleground; consider sociobiology. 
7. This graph is a variant of the store-versus-compute trade-off familiar in 
computer science. This particular version is adapted from Berliner and Ebeling 
1988. 
8. T h e analogous inquiry for Hitech-like systems is not useful yet, because 
few exist and because they arise from a research community that values explora-
tion for its own sake. 
9. T h e slow neural technology (which runs at ms speeds) might be conjec-
tured to be relevant. However, animals (including man) mostly must respond to 
other organisms, as predator, prey, and rival. Since all are constructed in the 
same technology, its absolute speed is probably not strongly relevant. 

196 
H U M A N S A N D MACHINES 
10. It does not, by the way, claim completeness of current cognitive theory, 
only that what is missing is as relevant to mundane as to creative cognition. 
REFERENCES 
Anderson, J. R. The Architecture of Cognition. Cambridge: Harvard University 
Press, 1983. 
Berliner, H., and C. Ebeling. "Pattern Knowledge and Search: The SUPREM 
Architecture," Artificial Intelligence 38 (1988): 161-198. 
Brownston, L., R. Farrell, E. Kant, and N. Martin. Programming Expert Systems in 
OPS5. Reading, Mass.: Addison-Wesley, 1985. 
Chase, W. G., and H. A. Simon. "Perception in Chess," Cognitive Psychology 4 
(1973): 55-81. 
Clark, H. H., and W. G. Chase. "On the Process of Comparing Sentences against 
Pictures," Cognitive Psychology 3 (1972): 472-517. 
Clark, H., and E. Clark. The Psychology of Language: An Introduction to Psy-
cholinguistics. New York: Harcourt Brace Jovanovich, 1977. 
Colby, K. M. Artificial Paranoia. Elmsford, N.Y.: Pergamon, 1975. 
Dennett, D. C. The Intentional Stance. Cambridge: Bradford Books/MIT Press, 
1988. 
Fitts, P. M., and C. M. Seeger. "S-R Compatibility: Spatial Characteristics of 
Stimulus and Response Codes "Journal of Experimental Psychology 46 (1953): 
199-210. 
Fodor, J. A. The Modularity of Mind. Cambridge: Bradford Books/MIT Press, 
1983. 
Gardner, H. The Mind's New Science: A History of the Cognitive Revolution. New 
York: Basic Books, 1985. 
Grene, M. The Knower and the Known. London: Faber and Faber, 1966. 
Heise, D. R. "Social Action as the Control of Affect," Behavioral Science 22 (1977): 
163-177. 
John, B. E., and A. Newell. "Predicting the Time to Recall Computer Command 
Abbreviations," in Proceedings of CHI'87 Human Factors in Computing Systems, 
April 1987. New York: Association for Computing Machinery. 
Johnson-Laird, P. Mental Models. Cambridge: Harvard University Press, 1983. 
Kulkarni, D., and H. A. Simon. "The Processes of Scientific Discovery: The 
Strategy of Experimentation," Cognitive Science 12 (1988): 139—175. 
Laird, J. E., A. Newell, and P. S. Rosenbloom. "Soar: An Architecture for General 
Intelligence," Artificial Intelligence 33 (1987): 1-64. 
Langley, P., H. A. Simon, G. L. Bradshaw, and J. M. Zytkow. Scientific Discovery: 
Computational Explorations of the Creative Processes. Cambridge: MIT Press, 
1987. 
Latour, B., and S. Woolgar. Laboratory Life: The Social Construction of Scientific 
Facts. Beverly Hills: Sage, 1979. 
Mueller, E. T. "Daydreaming and Computation: A Computer Model of Everyday 
Creativity, Learning, and Emotions in the Human Stream of Thought." Ph.D. 

METAPHORS FOR MIND, THEORIES OF MIND 
197 
dissertation, Computer Science Department, University of California, Los An-
geles, 1987. 
Newell, A. Unified Theories of Cognition. Cambridge: Harvard University Press (in 
press). (The William James Lectures, Harvard University, Spring 1987). 
Newell, A., and H. A. Simon. Human Problem Solving. Englewood Cliffs, N.J.: 
Prentice-Hall, 1972. 
Newell, A., J. C. Shaw, and H. A. Simon. "The Processes of Creative Thinking," 
in H. E. Gruber, G. Terrell, and J. Wertheimer, eds., Contemporary Approaches to 
Creative Thinking. New York: Atherton, 1962. 
Oatley, K., and P. N.Johnson-Laird. "Towards a Cognitive Theory of Emotions," 
Cognition and Emotion 1 (1987): 29—50. 
Polyani, M. Personal Knowledge: Toward a Post-Critical Philosophy. Chicago: Univer-
sity of Chicago Press, 1958. 
Siegler, R. S. "Three Aspects of Cognitive Development," Cognitive Psychology 8 
(1976): 481-520. 
Simon, H. A. "Motivational and Emotional Controls of Cognition," Psychological 
Review 74 (1967): 29-39. 
Simon, H. A., and K. Gilmartin. "A Simulation of Memory for Chess Positions," 
Cognitive Psychology 5 (1973): 29-46. 
Steier, D. E., J. E. Laird, A. Newell, P. S. Rosenbloom, R. A. Flynn, A. Golding, 
T. A. Polk, O. G. Shivers, A. Unruh, and G. R. Yost. "Varieties of Learning in 
Soar: 1987," in Proceedings of the Fourth International Workshop on Machine Learn-
ing, June 1987. Los Altos, Calif.: Morgan Kaufman. 
Sternberg, R. J., ed. Mechanisms of Cognitive Development. New York: Freeman, 
1984. 
Toews, J. E. "Intellectual History after the Linguistic Turn: The Autonomy of 
Meaning and the Irreducibility of Experience," The American Historical Review 
92 (1987): 879-907. 

