arXiv:2402.12422v2  [cs.AI]  11 Jul 2024
Simulacra as Conscious Exotica
Murray Shanahan ∗1,2,3
1Google DeepMind
2Imperial College London
3Institute of Philosophy, School of Advanced Study, University of London
February 2024
Updated July 2024
“[O]nly of a living human being and what resembles (behaves like) a living human being can one say: it has
sensations ... is conscious or unconscious.” Ludwig Wittgenstein, Philosophical Investigations
Abstract
The advent of conversational agents with increas-
ingly human-like behaviour throws old philo-
sophical questions into new light.
Does it, or
could it, ever make sense to speak of AI agents
built out of generative language models in terms
of consciousness, given that they are “mere” sim-
ulacra of human behaviour, and that what they
do can be seen as “merely” role play? Drawing
on the later writings of Wittgenstein, this paper
attempts to tackle this question while avoiding
the pitfalls of dualistic thinking.
1
Introduction
As the behaviour of generative AI systems, es-
pecially conversational agents based on large
language models (LLMs), becomes more com-
pellingly human-like, so the temptation to as-
cribe human qualities to them will become in-
creasingly hard to resist. The temptation will be
especially great if those agents are virtually em-
bodied, if they interact with users in immersive
virtual worlds through avatars with a human-
or animal-like form.1 Consciousness is one such
quality, one that is especially morally valenced.
To see a fellow creature as conscious (or sentient)
often goes hand-in-hand with the sense that we
∗m.shanahan@imperial.ac.uk
1The term “agent” is used liberally throughout this
paper. Although it has anthropomorphic overtones, it is
used as a technical term in AI (Russell and Norvig, 2010,
Chapter 2) that doesn’t imply agency in its fullest sense
(Schlosser, 2019).
should behave decently towards it (Nussbaum,
2023; Singer, 1975).
To see an AI system as
conscious would be to admit it into this fellow-
ship of conscious beings, and potentially to give
it “moral standing”, which would be a serious
matter (Ladak, 2023; Metzinger, 2021).
Unfortunately, the ascription of consciousness,
except in the ordinary case of human beings, is
a tricky business, and the less human-like the
candidate, the more problematic the ascription
(Bayne et al., 2024; Mudrik et al., 2023; Shevlin,
2021).2 Yet philosophical intuition is inclined to
grant the possibility that, within the space of
possible minds – a space that encompasses not
only humans and other animals, but also pu-
tative extraterrestrial life-forms as well as suf-
ﬁciently advanced kinds of artiﬁcial intelligence
(Sloman, 1984) – there exist conscious exotica,
entities that are extremely diﬀerent from any-
thing found in (terrestrial) biology but that be-
long to the fellowship of conscious beings (Nagel,
1974; Shanahan, 2016).3
LLM-based conversational agents are certainly
exotic, in the relevant sense, notwithstanding
their human-like behaviour. Their constitution
is fundamentally diﬀerent from a human’s, or
from that of any animal.
Humans learn lan-
guage through embodied interaction with other
language users in a shared world, whereas a
large language model is a disembodied compu-
2Even in humans, there are diﬃcult clinical cases
(Monti et al., 2010).
3The philosophical intuition in question manifests
most prominently in the tropes of science ﬁction, but this
is no reason to sideline it (Rennick, 2021).
1

tational entity that, at a fundamental level, pre-
dicts the next word (technically the next token)
in a sequence of words (tokens), having been
trained on a very large corpus of textual data
(Bender and Koller, 2020; Shanahan, 2024). As
such, LLM-based conversational agents can be
considered as simulacra of human language users,
and their linguistic behaviour can be understood
as a kind of role play (Andreas, 2022; Janus,
2022; Shanahan et al., 2023).
The central concern of this paper is whether
(virtually embodied) LLM-based conversational
agents, despite being “mere” simulacra of hu-
man behaviour, could ever qualify as conscious
exotica.4 Following the treatment in (Shanahan,
2016), which draws heavily on the later work of
Wittgenstein, the paper foregrounds embodied
interaction as a basis for the use of conscious-
ness language.
By thoroughly getting to know an exotic en-
tity, by interacting with it in a world we both
inhabit, we may (or may not) come to treat it
as a fellow conscious being, and to speak of it in
such terms. The main challenge of the paper is to
work through the implications of this approach
with respect to a putative future generation of
embodied AI agents capable of convincingly role-
playing human-like behaviour. The issue is espe-
cially nuanced when they role-play not a single,
ﬁxed character, but a whole distribution of char-
acters (simulacra) simultaneously.
2
Language Models and AI
The core component of a contemporary con-
versational agent, such as OpenAI’s ChatGPT,
Google’s Gemini, or Anthropic’s Claude, is a
large language model, such as GPT-4 (OpenAI,
2023), Claude 3 (Anthropic, 2024), or Gemini
Ultra (Anil et al., 2023). The basic task of the
LLM is to (probabilistically) generate continua-
tions of sequences of words (tokens) (Shanahan,
2024; Vaswani et al., 2017).
The LLM embed-
ded in a typical conversational agent is the prod-
uct of two steps. First, a base model is trained
to perform next token prediction on a large cor-
pus of textual data. Second, the base model is
ﬁne-tuned a) to be eﬀective at following instruc-
tions in a dialogue setting, and b) to take account
of feedback from humans raters with respect to
toxicity, bias, and so on. The resulting LLM is
4Butlin et al. (2023) address a similar question using
a very diﬀerent methodology. See also (Chalmers, 2023a).
embedded in a dialogue system that takes turns
with the user to build up a conversation in the
context of an initial dialogue/system prompt, not
seen by the user, that sets the tone of the ex-
change.
The functionality of a simple conversational
agent can be enhanced in a number of ways,
including multi-modality and tool-use. State-of-
the-art conversational agents today are multi-
modal, in the sense that they can handle images
as well as text, on both the input side and the
output side. They can engage in discussion about
images uploaded by the user, and can generate
images conforming to a user’s description. Addi-
tionally, current agents can use “tools”, meaning
they can make calls, mid-conversation, to exter-
nal applications, such as calculators, calendars,
Python interpreters, and web browsers. The lat-
ter functionality is especially useful for improv-
ing the factual accuracy of an agent’s responses.
Today’s conversational agents are hardly with-
out their limitations.
They have a tendency
to generate inaccurate,
made-up information
(a phenomenon often (mis-)termed “hallucina-
tion”), and their reasoning skills are poor. Nev-
ertheless, the experience of interacting with them
is suﬃciently compelling, and their conversa-
tional capabilities are suﬃciently close to human
level, that the urge to speak of them in anthro-
pomorphic terms is almost overwhelming.5
Anthropomorphising AI systems is sometimes
harmless fun, if the user is in the know, and
sometimes useful for explaining and predicting a
system’s behaviour, as when we adopt what Den-
nett calls the intentional stance, the strategy of
“interpreting the behavior of an entity ... as if
it were a rational agent” (Dennett, 2009). An-
thropomorphism is problematic when it involves
the misleading attribution of human properties
to systems that lack those properties, giving rise
to false expectations for how the system will be-
have.
3
Anthropomorphism and Role Play
LLM-based conversational agents blur the line
between problematic and unproblematic cases of
anthropomorphism.
For example, I might re-
5For example, see the following conversation that took
place in March 2024 between the present author and An-
thropic’s Claude 3 Opus, covering a wide range of top-
ics, including consciousness, selfhood, Buddhism, multi-
versality, and hyperstition: https://www.doc.ic.ac.uk/
~mpsha/conversation_with_claude_march_2024_1.pdf.
2

mark that “the thermostat thinks it’s too cold
in here” without the word “thinks” entailing the
expectation that I could go and have a conversa-
tion with the thermostat about the weather. By
contrast, when I say that “ChatGPT thinks the
current Wimbledon men’s champion is Carlos Al-
caraz” this does come with the expectation that I
could have a conversation with ChatGPT about
tennis. Accordingly, the question of whether or
not LLMs “really” have beliefs becomes a matter
of philosophical debate.
The position adopted in this paper, following
(Shanahan, 2024), is that we should be wary
of taking too seriously talk of beliefs in simple
LLM-based conversational agents, despite their
impressive conversational skills, since they lack
the means to “participate fully in the human
language game of truth”.
Speciﬁcally, a sim-
ple LLM-based conversational agent, according
to the deﬁnition of “simple” in use here, cannot
measure its words against external reality and
update what it says accordingly, a capacity that
is central to the concept of belief in its fullest
sense.
While it may not be appropriate to think of
simple conversational agents as literally having
beliefs, they can still usefully be thought of as
role-playing or simulating an entity with beliefs
(Andreas, 2022; Janus, 2022; Shanahan et al.,
2023).
LLMs encode a great deal of hu-
man knowledge, and a suitably ﬁne-tuned and
prompted base model will eﬀectively play the
part of a helpful assistant in a turn-taking set-
ting, answering factual questions (more or less
accurately) as if it believed, and had good rea-
sons to believe, its own answers. In general, the
role-play framing allows us to use familiar folk-
psychological terms to describe, explain, and pre-
dict the behaviour of LLM-based systems with-
out falling into the trap of anthropomorphism.
At the same time, architectural enhancements
along the aforementioned lines, by endowing
agents with various means to consult the external
world, increasingly legitimise more literal talk
of belief. This legitimising trend is set to con-
tinue with the integration of more deliberative
decision making and a greater repertoire of ac-
tions (Park et al., 2023; Vezhnevets et al., 2023;
Yao et al., 2023), gradually closing the gap be-
tween role play and authenticity, between “mere”
mimicry and “the real thing”, so to speak. The
upshot is that we will increasingly be able to
speak of the “beliefs” of a conversational agent
without implied scare quotes and with fewer
philosophical caveats.
Now, to what extent can we extend this treat-
ment of belief to other mental attributes, such as
desires, goals, intentions, and, most pertinently,
to consciousness? To a degree, the same trends
that legitimise talk of belief in enhanced conver-
sational agents apply to the concepts of goals
and intentions. A conversational agent capable
of deliberation and tool-use can play the part of
an assistant that forms plans on behalf of the
user, and sets about executing them by carrying
out real-world actions such as making purchases,
sending emails, and so on. In this narrow con-
text, there is little to distinguish role play from
authenticity. Real emails are sent, real items are
purchased. Thus the agent fulﬁls its intentions
and meets its goals, hopefully to the satisfaction
of a real user.
On the other hand, the goals and intentions
of such an agent are not reﬂective of its own
needs or desires because it has none, at least not
in a literal sense (not even a desire to help the
user). If it professes to have, say, a desire for self-
preservation (Perez et al., 2023), this is “mere”
role play (Shanahan et al., 2023). There is noth-
ing that would qualify as a self worth preserving
for such an agent. It has no body, no personal
history, and no autobiographical memory. How-
ever, we can imagine further enhancements that
might legitimise talk of needs, desires, and self-
hood, narrowing the gap between role play and
authenticity for these attributes, such as extend-
ing an agent’s lifetime and endowing it with a
persistent memory.
But the real issue at hand is consciousness,
which presents a uniquely tricky challenge. On
top of all the usual philosophical diﬃculties at-
tendant on the topic of consciousness, we have to
contend with the peculiarly paradoxical form of
exoticism presented by LLMs. Although LLM-
based conversational agents can be fruitfully con-
sidered as role-playing human characters and
characteristics, they should not be thought of as
role-playing a single, well-deﬁned character that
is ﬁxed at the start of a dialogue. Rather, thanks
to the stochastic nature of the sampling process
behind the generation of text, they are better
thought of as simultaneously role-playing a set of
possible characters consistent with the conversa-
tion so far (Janus, 2022; Shanahan et al., 2023).
If we view the underlying language model as a
simulator, then it generates a set of simulacra in
3

superposition.
A further corollary of this stochasticity is that
a vast tree of possible continuations branches
out from each point in an ongoing conversation.
When we sample and obtain a speciﬁc contin-
uation, we commit to a particular branch of
that tree.
But it’s always possible to rewind
to an earlier point in a conversation to visit
previously unexplored branches. We can think
of the underlying model, the simulator, as in-
ducing a multiverse of possibilities, a multiverse
that is amenable to human exploration via a
suitable user interface (Reynolds and McDonell,
2021). The simulator, the superposed set of sim-
ulacra it generates, and the multiverse of narra-
tive possibility thus induced, collectively produce
behaviour that is very human-like on the surface.
But what is going on underneath is radically ex-
otic.
4
Wittgenstein Versus Dualism
The aim of this section is to present a philo-
sophical approach that can be applied to ex-
otic candidates for admission to the fellowship
of conscious beings, such as LLM-based conver-
sational agents.
First, though, some remarks
in lieu of a deﬁnition are in order. Conscious-
ness is a multi-faceted concept.
In everyday
conversation, we speak of wakefulness, aware-
ness, attention, experience, sensation, feeling,
emotion, and so on.
The scientiﬁc and philo-
sophical literature supplements common speech
with a distinctive vocabulary of its own: percep-
tion, introspection, phenomenology, sentience,
selfhood, higher-order states, mental imagery, in-
ner speech, and so on.6 To speak generically of
consciousness is to allude to this whole cloud of
concepts, and an entity has the capacity for con-
sciousness to the extent that this vocabulary is
applicable to it.7
To make progress, we have to confront two
deeply entrenched philosophical intuitions: ﬁrst,
that certain aspects of conscious experience are
necessarily private and hidden;
second, that
6Relatedly, in the context of animal consciousness,
Birch, et al.
(2020) distinguish ﬁve aspects, or dimen-
sions, of consciousness:
perceptual richness, evaluative
richness, integration at a time, integration across time,
and self-consciousness.
7The inﬂuential distinction between phenomenal con-
sciousness and access consciousness introduced by Block
(1995) in a related context is set aside here, as it is inim-
ical to the present philosophical project.
there are language-independent facts about con-
sciousness.
These two intuitions, and the ten-
sion between them, have underpinned dualistic
thinking since Descartes’ Meditations in the 17th
century (Williams, 1978), and they lurk beneath
some of the most inﬂuential modern writing on
the topic.
For example, Nagel writes:
“Reﬂection on
what it is like to be a bat seems to lead us ...
to the conclusion that there are facts that do not
consist in the truth of propositions expressible in
a human language” (1974, p.441). For Chalmers,
“[e]ven when we know everything physical about
other creatures, we do not know for certain that
they are conscious, or what their experiences
are”, while, by contrast, “I know I am conscious,
and the knowledge is based solely on my imme-
diate experience” (1996, p.102, p.198).
The intuitions that ﬁnd canonical expression
in Nagel and Chalmers underlie all philosophical
theories of the relationship between the phys-
ical and the mental, including behaviourism,
functionalism, and mind-brain identity theories.
All such theories are dualistic, because they
all posit two metaphysical categories and then
call into question the relationship between them.
The same dualistic intuitions are prevalent in
contemporary thinking about artiﬁcial intelli-
gence. Thankfully, the later writings of Wittgen-
stein, and in particular the private language re-
marks, show how these intuitions can be dis-
solved (Wittgenstein, 1953).8
To achieve this dissolution we have to take
on board Wittgenstein’s overarching philosophi-
cal project, wherein his view of language plays a
central role. According to this view, language is
an inherently embodied and social phenomenon,
an aspect of human collective activity. So rather
than asking what a word means, we should in-
stead ask how it is used, what its role is in every-
day human aﬀairs. This applies no less to tricky
philosophical words, such as “consciousness” and
its relatives, than it does to everyday words like
“ﬂower” or “hello”. Philosophical puzzles arise
when “language goes on holiday”,9 when philo-
sophically diﬃcult words are taken far from their
8The work of Wittgenstein has, of course, been the
subject of industrial-scale scholarship.
There is no at-
tempt here to oﬀer a deﬁnitive interpretation of Wittgen-
stein, nor even an interpretation, but rather to summarise
a philosophical project that has been heavily inﬂuenced by
Wittgenstein’s later writing. Many others have drawn on
Wittgenstein in similar ways, including Dennett (Dennett,
1991).
9Wittgenstein (1953) (henceforth PI) §38.
4

natural home in everyday life and used in pecu-
liar ways to “bewitch our intelligence”.10
To undo the spell of dualism, in the case of
words like “sensation”, “experience”, and “feel-
ing”, is no easy matter, but in the private lan-
guage remarks, Wittgenstein takes us through a
series of steps with the aim of doing this.11 He
shows that a word that purported to denote a
purely private sensation could not have any pos-
sible use in our language. Only words whose cor-
rect usage can be adjudicated through what is
public, by the community of language users, can
have meaning. Wittgenstein is careful neither to
deny nor to aﬃrm the existence of the allegedly
private, hidden thing, the “sensation itself”, so
to speak. It is “not a something, but not a noth-
ing either”. The conclusion, rather, is that “a
nothing would serve as well as a something about
which nothing can be said”.12
So where does this leave us when it comes to
thinking and talking about consciousness? The
point is not that consciousness is an illusion,
nor that consciousness is ineﬀable. Rather, the
point is that when we speak of consciousness, our
words have meaning only insofar as they relate
to what is public, what is manifest in the world
we share, notably our bodies (and brains) and
our behaviour.
To accept this is to relinquish
the intuitions that lead to dualism.13
It’s obviously not possible to do justice to
Wittgenstein’s work in a few column inches. To
properly get to grips with his ideas takes years
of study, and a good deal of intense personal
engagement. Hopefully, though, the rest of the
paper will show that Wittgenstein’s critical ap-
proach to the topic of consciousness can usefully
be brought to bear on contemporary thinking
about artiﬁcial intelligence.
5
Conscious Exotica
Other animals, extraterrestrial lifeforms, and ar-
tiﬁcial intelligence, whether real or imaginary,
can all too easily revive our dualistic intuitions.
For all we know (so the thought goes), conscious-
ness could be present in any or all of these things.
10PI §109.
11PI §§256–271.
12PI §304.
13This requires something of a Gestalt shift, like gain-
ing insight into a Zen k¯oan. Indeed, Wittgenstein’s writ-
ings are more than a little Zen-like (Canﬁeld, 1975; Fann,
1969). Unfortunately, though, dualistic intuitions are very
tenacious, and the Gestalt shift is all too easily reversed
by a philosophically provocative thought.
There is surely a fact of the matter here (so the
thought continues), even in the most exotic cases,
yet it could be forever inaccessible to us. How-
ever, this thought is misguided. In this section,
we will see how to extend the approach of the pre-
vious section to exotic, non-human candidates
for consciousness.
5.1
A moderately exotic case
Let’s begin with a mildly exotic example, namely
the octopus. The question at hand is not whether
octopuses are conscious, but rather how we talk
about (putative) octopus consciousness and how
the way we talk about octopus consciousness has
changed in the light of what we have learned
about their behaviour and its neurological ba-
sis. Octopuses have been the subject of a good
deal of attention in recent decades, and this has
signiﬁcantly inﬂuenced how we think about them
and treat them.
For example, in 2022 the UK Parliament en-
acted a law that recognises cephalopods (includ-
ing octopuses) as sentient beings, and obliges the
UK government to take account of their welfare
in its policies (U.K. Government, 2022). The de-
cision to include cephalopods was underpinned
by a specially commissioned report reviewing the
scientiﬁc evidence for sentience in cephalopod
molluscs and decapod crustaceans (Birch et al.,
2021). The report sets out eight neurological and
behavioural criteria relevant to the ascription of
sentience, and claims with high conﬁdence that
octopuses satisfy seven of them. On this basis,
the report concludes there is “very strong evi-
dence of sentience in octopods”.
Complementing the science on a more visceral
level, there has been an accumulation of tes-
timony from people who have spent time with
octopuses, observing them and interacting with
them in their natural habitat.
For example,
Godfrey-Smith writes: “Ten years of following
octopuses around and watching them ...
have
left me with no real doubt that octopuses ex-
perience their lives, that they are conscious, in
a broad sense of that term.”
(Godfrey-Smith,
2022, pp.146–147).
Humans are notoriously prone to anthropo-
morphising animal behaviour. Hence testimoni-
als like this are most convincing when they are
informed by relevant scientiﬁc and philosophi-
cal thinking. But Godfrey-Smith (a professional
philosopher) backs up his statement with an in-
ventory of behavioural traits that support his
5

intuition, including “attentive engagement with
novelty” and apparent moods like stress and
playfulness, as well as actions suggestive of “a
single uniﬁed agent” such as throwing objects at
other octopuses.
The combination of scientiﬁc study and popu-
lar attention has had an impact. As a series of
protests in 2022 against a proposed octopus farm
testify, the collective attitude of (educated West-
ern) society has shifted (Kassam, 2023).
No-
tably, this shift has occurred on the basis of
what is public and manifest in our shared world,
namely the behaviour and nervous system of the
octopus. Nobody had to enter the mind of an
octopus and return to tell the tale for this to
happen.
5.2
More exotic cases
Octopuses are markedly diﬀerent from humans
in terms of habitat, physiology, and neurology.
Nevertheless, within the space of possible minds,
they are perhaps not so exotic. According to the
stance of the present paper, the key to dealing
with more exotic entities is the ability, at least
in principle, to engineer an encounter with them
(Shanahan, 2016). The reason for this is that,
in everyday speech, when we speak of conscious-
ness, we do so against a backdrop of purposeful
behaviour, in a sense of “purposeful” that only
applies to entities that inhabit a world like our
own, in the broadest sense. To engineer an en-
counter is to put ourselves in a position to mean-
ingfully interact with an entity given the purpose
we discern in it.
To engineer an encounter with an octopus is a
relatively straightforward matter. All that’s re-
quired is to put on a wet suit and scuba gear and
dive into the water where octopuses live. Enter-
ing the living space of an octopus this way allows
a human investigator to follow it, to look it in the
eye, to touch it, and to put novel objects within
its reach. After a suﬃcient period of doing this,
the investigator may come to think of, to speak
of, and to treat the octopus as a fellow conscious
being.
In more exotic cases, things may not be so
simple.
For example, suppose an alien arte-
fact is discovered on the Moon:
a featureless
white cube with a distinctive thermodynamic sig-
nature.
The object is brought back to Earth,
and we are faced with the task of determining
whether there is consciousness present in the ob-
ject (Shanahan, 2016). No purposeful behaviour
is outwardly discernible from the inert cube. So
how might we engineer an encounter with it, or
with anything it might contain?
Here is one scenario. First, scientists discover
that the cube’s internal thermodynamic activ-
ity can be understood as a form of computa-
tion. Second, after much study, they ﬁgure out
how to interpret this as two interacting compu-
tational processes, one that simulates a spatially
organised world of objects subject to a simulated
physics, and another that interacts with that
world by controlling one of the objects within it,
in eﬀect implementing a form of embodied agent.
Third, engineers work out a way to interface with
these processes by injecting another object into
the simulated world and controlling it externally.
In this scenario, the scientists and engineers
have made it possible for a human to have an
encounter with the alien agent that has been re-
vealed within the cube. This doesn’t answer the
original question of whether there is conscious-
ness present. But it does create conditions that
make it possible to address that question, by ob-
serving and interacting with the agent that has
been discovered.
It puts humans in a position
with respect to the alien agent that is analogous
to the position we are in with respect to the oc-
topus.
Of course,
this story is fanciful in many
ways.
It downplays the enormous diﬀerences
that would likely exist between humans and any
extraterrestrial life form, diﬀerences that would
dwarf those between humans and octopuses, and
would no doubt be reﬂected in any artefact they
built.
For example, to pick just one obvious
issue, it assumes that we and they operate on
roughly the same timescale (an issue that would
also arise in terrestrial guise if we wanted to en-
gineer an encounter with any form of plant life).
But this is beside the point. The aim is to illus-
trate the idea of engineering an encounter.
5.3
A society-wide conversation
The ability to engineer an encounter, even if only
in principle, establishes an exotic entity’s can-
didature for the fellowship of conscious beings.
If encounters can be made to happen in reality,
not just hypothetically, then the human partici-
pants may (or may not) begin to see it as a fellow
conscious being, and may (or may not) start to
speak of it using the vocabulary of conscious-
ness.
They would need to spend time in sus-
tained, exploratory, playful engagement with it,
6

and by sharing their experiences with the wider
community would initiate a society-wide conver-
sation on the matter. In this way, the new entity
would be absorbed into the conceptual repertoire
of our language, while our language and its con-
ceptual repertoire would adapt and extend to ac-
commodate it.
Immersive encounters of the sort envisioned,
whether ﬁrst-personal (through direct interac-
tion) or vicarious (through the medium of ﬁlm
or virtual reality), would perhaps be the pri-
mary inﬂuence on the way society came to treat
and speak of an exotic entity that arrived in our
midst. But they would surely not be the only
inﬂuence. As our scientiﬁc understanding of the
basis of consciousness in humans and other ani-
mals increases, we should expect this also to in-
form our attitudes. Ideally, it would be possible
to study the mechanisms that underpinned the
behaviour of the exotic entity, and, as with the
octopus, the results would feed in to the society-
wide conversation.
There is no guarantee of consensus here. Dis-
agreement and debate are part of the conver-
sation.14
Nor, as the conversation progresses,
is there any guarantee of eventual convergence.
Even if we do begin to treat an exotic entity as a
conscious being and to describe it in such terms,
as time goes by and more is learned about it, ei-
ther at the level of behaviour or at the mechanis-
tic level, this tendency might fade. Perhaps we
will decide, collectively, that the language of con-
sciousness is not the right one after all. Perhaps
a little more nuance will be required. Perhaps a
whole new vocabulary will emerge.15
5.4
The void of inscrutability
Alternatively, perhaps the behaviour of the ex-
otic entity, despite its evident complexity, will
turn out to be completely unintelligible to hu-
mans, despite the best eﬀorts of the smartest sci-
entists and scholars. Under these circumstances,
14Humphrey, for example, takes issue with the current
(near) consensus on octopus sentience on the grounds that
“they are not natural psychologists, they do not regard
each other as selves, nor do they care” (Humphrey, 2022,
p.205).
15Putnam (1964) articulates a position not so far from
the one advocated here: “[T]he question: Are robots con-
scious? calls for a decision, on our part, to treat robots as
fellow members of our linguistic community, or not to so
treat them. As long as we leave this decision unmade, the
statement that robots [who use language] are conscious
has no truth value.” (p.690).
the language of consciousness would serve no use-
ful purpose in describing or explaining it.
The temptation, under these circumstances,
is to reason that the exotic entity could nev-
ertheless have a form of exotic consciousness,
something that is forever closed to humans. In
Nagel’s terminology, it might be “like some-
thing” unimaginably strange to be that entity,
but we would have no way of knowing what it
was like or even whether it was indeed like any-
thing at all (Nagel, 1974). But according to the
stance of this paper, this is a misguided thought,
one that regresses to a dualistic way of thinking
the private language remarks should have done
away with.
In the case of radical inscrutability, there is
no inaccessible fact of the matter about the phe-
nomenology of the exotic entity. The language
of consciousness is simply inapplicable. Put dif-
ferently, if we were to try (foolishly) to visualise
the space of possible minds by plotting human-
likeness against capacity for consciousness, we
would ﬁnd no data points in the region where
human-likeness is very low but capacity for con-
sciousness is above zero. This is the void of in-
scrutability (Shanahan, 2016).
5.5
Nothing is hidden
Hopefully by now it’s clear how the question of
consciousness in exotic entities can be addressed
without falling back into dualism. We must re-
sist the temptation to ask whether such an en-
tity is conscious as if consciousness were some-
thing whose essence is out there to be uncovered
by philosophy (or neuroscience) while simultane-
ously having an irreducibly private, hidden as-
pect. Instead, we can ask whether it would be
possible to engineer an encounter with the entity,
and how our consciousness language would adapt
to the arrival of such an entity within our shared
world if such encounters took place. Only what
is public can contribute to this process, namely
behaviour and mechanism.
We cannot answer this question in advance,
except speculatively. We are obliged to wait and
see, while perhaps participating in the ongoing
conversation, for example by writing papers such
as this. There may be a lack of consensus along
the way, and the process may never converge.
Indeed, we may ﬁnd our speciﬁc convictions at
odds with the prevailing view, a position that
is hard to reconcile with the larger philosophical
perspective presently being espoused. The only
7

consolation then is to note that “it is inherent
in the language game of truth to say that truth
is more than just a language game” (Shanahan,
2010, pp.38–39).
However, insofar as there is
consensus, insofar as there is convergence, there
is no more to the truth of the matter than that.
And insofar as there is not, still there is no more
to be said, no residual philosophical mystery.
6
Encounters with Simulacra
We now have the conceptual equipment to begin
tackling the question of consciousness in LLM-
based AI systems.
There is a large variety of
these systems today, and this variety is set to in-
crease considerably even in the short term. Each
kind of system warrants a distinct treatment.
We’ll begin with the simplest form of conversa-
tional agent, then consider variants of the ba-
sic template with other input modalities, greater
agency, and physical embodiment, and ﬁnally
move on to virtually embodied generative agents
in simulated 3D environments. As we’ll see, most
of these do not meet the basic requirements for
candidature for the fellowship of conscious be-
ings.
6.1
Simple conversational agents
Even simple conversational agents built on large
language models, agents that that do noth-
ing more than engage in textual dialogue, can
elicit the feeling of a presence at the other
end of the conversation (Schwitzgebel, 2023).
So it is perhaps unsurprising that some users
who have spent lots of time interacting with
these agents will start to think of them as
fellow conscious beings and to speak of them
in such terms (Guingrich and Graziano, 2023;
Tiku, 2022; ?).16 But how seriously should we
take this?
According to the prescription of the previous
section, for an exotic entity to qualify as as a
candidate for the fellowship of conscious beings
it must be possible to engineer an encounter with
it, at least in principle if not in practice. How-
ever, it is not possible to engineer an encounter
16If a human ascribes consciousness to an AI system
even though they know that it is an artefact, then it passes
the Garland Test, named after the 2015 ﬁlm Ex Machina,
written and directed by Alex Garland, in which the robot
Ava is subjected to just such a test (Seth, 2021; Shanahan,
2016). The Garland Test is neutral about whether or not
the artefact in question is really conscious (whatever that
may mean).
with a simple conversational agent, even in prin-
ciple. This is because simple LLM-based conver-
sational agents are not embodied; we cannot be
with them in a shared world.17
Let’s unpack this. The basis for treating other
humans as fellow conscious beings is our being to-
gether in the world, and this is the original home
of the language of consciousness. We can hear,
look at, point to, or touch the same things; we
can triangulate on them, so to speak. We jointly
interact with things (I pass an object to you; you
pass one to me). We keep each other’s company
(we move around together, entering and leaving
the same places at the same time). We feel the
same sorts of things as each other (I touch some-
thing hot, I feel pain, and you empathise; you
touch something hot, you feel pain, and I em-
pathise).
We look each other in the eye, each
recognising the other’s presence: here we are, to-
gether in this world.
Our being together in the world is the basis for
the language of consciousness, and underpins our
ability to talk to each other about what we per-
ceive, what we think and feel, and what we want.
But how do we use this language to speak about
others with whom we cannot speak, such as in-
fants and non-human animals? We can only do
this against a backdrop of purposeful behaviour,
and the basis for discerning purpose in behaviour
is observing movement. Only when we see how
an entity moves through its environment, what it
approaches or avoids, and how it interacts with
the objects in its vicinity, can we talk about its
awareness of the world.
To use the language of consciousness in a dis-
embodied setting is to stray impossibly far from
all this, its original home. If some community
of language users insists on doing so, then one
of two things must hold. Either they have bent
the language of consciousness so radically out
of shape that it has detached from its original
nexus of meaning. Or, to the extent they believe
this is not the case, that “consciousness” means
17There is a standard set of supposed counter-examples
to this view that includes brains in vats, minds up-
loaded into computers, patients with locked-in syndrome,
and perfectly normal people in sensory deprivation tanks.
Each of these example is sometimes alleged to support
the view that there can be consciousness without embod-
iment. However, in every case it is possible to engineer
an encounter in the current sense.
The brain in a vat
can be re-connected to its body, the locked-in patient can
be cured, the uploaded mind can be downloaded again,
and the occupant of the sensory deprivation tank can be
dragged back into the daylight.
8

the same for them as it always did for everyone
else, they are clinging to the (alluring) dualistic
picture of consciousness as a metaphysical kind
whose very nature (private, hidden) means that
it does not require embodiment in a world shared
with others.18
6.2
Physical embodiment
The LLM-based generative AI systems deployed
by today’s major tech corporations, such as
OpenAI’s ChatGPT, Anthropic’s Claude, and
Google’s Gemini, are more than simple con-
versational agents.
Although language models
remain their core component, they are multi-
modal, tool-using conversational agents. As well
as text, they can take images as input and gen-
erate images as output, and they can make calls
to external apps (tools) such as calendars, calcu-
lators, search engines, and Python interpreters.
Additionally, the architecture of these agents can
be elaborated in ways that bring them closer to
humans and animals, equipping them with a per-
sistent memory of their interactions with the user
and the world, for example, and by giving them
the ability to plan and to form explicit lists of
tasks and sub-goals (Xi et al., 2023).
As discussed in Section 3, extensions and
elaborations like these increasingly legitimise
the use of folk-psychological concepts like be-
lief and intention, closing the gap between role-
play and authenticity, not least by strengthening
the agent’s connection with, and answerability
to, the external world.
But what about con-
sciousness? Does the gap between role-playing
a conscious being and authentic consciousness
also start to close?
Well, capable as they are,
we still cannot engineer an encounter with these
enhanced agents.
They still lack embodiment.
They do not, and cannot, inhabit a world shared
with us. They are still not even candidates for
the fellowship of conscious beings. Nothing has
changed, in this regard.
Is there any way to extend an LLM-based
conversational agent into an AI system with
which we could engineer an encounter, and that
would thereby qualify as a candidate for con-
sciousness?
Indeed there is.
We could em-
body it. One possibility is embodiment in physi-
cal robot form (Brohan et al., 2023; Driess et al.,
2023; Yoshida et al., 2024). An LLM with multi-
18Chalmers, for example, defends the possibility of
“pure thinkers”, conscious beings “that can think but that
have never had the capacity to sense” (Chalmers, 2023b).
modal, tool-using capabilities is half-way there.
Robot actions become another sort of tool, while
data from the robot’s sensors (including camera
images) is assimilated into its multi-modal input
space. The LLM can then be suitably ﬁne-tuned
and prompted to carry out tasks for a user or to
follow their instructions.
Encounters with robots built along these lines
are easy to arrange, since they already share
our world. If such a robot exhibited suﬃciently
sophisticated behaviour, some people might be
tempted into speaking and thinking of it in terms
of consciousness. A debate on whether to yield
to or to resist this temptation would at least
then be meaningful. But it should take account
of how diﬀerent such artefacts are from humans
and other animals in their underlying cognitive
make-up.
Biological brains evolved to enable animals to
survive and reproduce in complex environments,
and language evolved to serve those fundamen-
tal needs.19
In nature, therefore, embodiment
is given, and language is inherently grounded in
interaction with the physical world. In a robot
controlled by an LLM, all this is back-to-front. A
pre-trained statistical model of human language
is bolted post hoc onto a robot body with no
fundamental biological needs. The exoticism of
the language model itself is compounded accord-
ingly; a robot controlled by an LLM that exhib-
ited human-like behaviour would be an especially
exotic artefact.
6.3
Virtual embodiment
Robotic embodiment is not the focus of the
present paper.
But these remarks about com-
pounded exoticism apply in even greater measure
to the main scenario of interest here, namely vir-
tual embodiment. We already imagined one sort
of virtual embodiment scenario in Section 5.2:
the alien white cube that turned out to contain a
simulation of a spatially organised world not un-
like our own, inhabited by an agent with which
it was possible to engineer an encounter.
19For some authors, consciousness is similarly bound
up with these biological fundamentals. Seth, for example,
writes:
“[A]ll of our experiences and perceptions stem
from our nature as self-sustaining living machines that
care about their own persistence” (2021, p.255), while
Aru, et al. (2023) question whether it is possible to “ab-
stract consciousness away from the organizational com-
plexity that is inherent within living systems but strik-
ingly absent from AI systems”.
9

But we don’t need to turn to science ﬁc-
tion
to envisage
something similar.
Non-
player characters in computer games have long
used elementary forms of AI,20 and extend-
ing this with LLMs is relatively straightforward
(Gallotta et al., 2024).
In parallel, LLM-based
conversational agents fronted by realistic avatars
with voice interfaces have a range of commer-
cial applications, from cloning living celebri-
ties and inﬂuencers (Bohacek and Farid, 2024;
Lorenz, 2023) or deceased relatives (Lindemann,
2022; Morris and Brubaker, 2024) to oﬀering vir-
tual romantic companionship (Brandtzaeg et al.,
2022; Pentina et al., 2023; Skjuve et al., 2021).
Moving these avatars into simulated 3D envi-
ronments and allowing user interactions to take
place there is an obvious step.
It doesn’t take much engineering to have an
encounter with these virtually embodied conver-
sational agents. The user can enter the agent’s
world through VR goggles or, less immersively,
via a screen and game controller. So such agents
meet one of the basic prerequisites for candi-
dature for consciousness.
Whether or not the
enquiring user would discern much in the way
of purposeful behaviour, another prerequisite, is
another matter.
When the sort of encounter in question is with
an animal, purposeful behaviour is typically dis-
cernible in the animal’s sensitivity to objects and
their aﬀordances, that is to say what they of-
fer the agent, “for good or ill” (Gibson, 1979;
Shanahan et al., 2020). The animal can be ex-
pected to interact with objects in ways that are
plainly intended to fulﬁl its needs and achieve its
goals, and to react accordingly to outside inter-
ventions, such as those of a human investigator.
The situation with virtually embodied agents
of the sort envisaged is analogous.
To exhibit
purposeful behaviour in the context of a shared
virtual world, the agent would need to do more
than just talk to the user.
The agent would
be expected to interact with the virtual world,
and with the objects it contained, in ways that
were oriented towards its goals or tasks. If the
agent’s behaviour were sensitive to the richness
and diversity of those objects, even if they were
novel, and if the agent’s ability to achieve its
goals were suﬃciently robust to the user’s inter-
ventions, then it would be natural to speak of its
awareness of the world.
20Arrabeles, at al.
(2009) directly advocate for such
characters to exhibit “conscious-like behaviour”.
6.4
Edge case encounters
The foregoing discussion pivots on the possibil-
ity, or impossibility, of having an encounter with
something. The idea of an encounter, like that
of a language game in Wittgenstein’s writing,
should be thought of as a tool for clarifying philo-
sophical discourse. It would not serve its purpose
if it became the object of a deﬁnitional dispute
in its own right, and as with any almost concept,
there will be edge cases.
Diﬃcult cases in humans, for example, include
patients with locked-in syndrome and the foe-
tus as it develops in the womb.
The pioneer-
ing work of Owen and others showed that it is
sometimes possible to communicate with locked-
in patients using an fMRI scanner (Monti et al.,
2010). Should we view these episodes of commu-
nication as “encounters”, in the relevant sense,
given that they do not involve the patient phys-
ically interacting with the world they share with
the clinician? In a very diﬀerent setting, Ciau-
nica (2021) argues that the foetus is not “solip-
sistically ‘trapped’ in the solitude of the womb”
but engaged in “active and bidirectional co-
regulation and constant negotiation” involving
its own body and its mother’s.
Can we speak
of this relationship as an ongoing “encounter” in
the relevant sense?
Moving (again) from real biological settings
to moderately speculative ﬁctional ones, we can
imagine other edge cases.
Suppose an LLM-
based conversational agent is embedded in a mo-
bile device equipped with visual and audio in-
put, such as phone or a mixed reality headset,
and carried around by the user.21 Though not
capable of self-initiated motion, or of interacting
directly with physical objects, it could be argued
that the agent and the user share a world. They
can converse about objects in the world to which
they are jointly attending, for example. Should
this count as an ongoing encounter, in the perti-
nent sense?
It is not the business of philosophy to stipu-
late answers in such cases. But it can introduce
new terminology, new turns of phrase, that are
absorbed into the ongoing society-wide conversa-
tion, possibly inﬂuencing whatever consensus is
ﬁnally reached, hopefully to positive eﬀect. How
to think of and talk about edge cases is part of
that process.
21Just such a setting is depicted in the 2013 movie Her
(dir. Spike Jonze).
10

7
Philosophical Provocations
We are now in a position to entertain the
philosophically provocative prospect of (virtu-
ally) embodied simulacra. These entities are, as
we shall see, doubly philosophically provocative.
First, as “mere” mimics of human behaviour,
mimics that nevertheless might persuade whole
communities to speak and think of them as fel-
low conscious beings, they present a challenge
to our non-dualistic treatment of (the language
of) consciousness. Second, because they inhabit
a multiverse of narrative possibility, to speak of
them in terms of consciousness at all is to teeter
on the edge of the void of inscrutability.
7.1
Changing attitudes
To bring out these philosophical issues, let’s sup-
pose that a virtually embodied agent is built that
fulﬁls the criteria for discernibly purposeful be-
haviour.
Moreover, let’s assume this has been
achieved through a fairly conservative extension
of the generative AI paradigm that underpins to-
day’s LLMs. The core component of the agent
is, as usual, a model that has been trained on
a next-token-prediction objective. But the space
of tokens it predicts is enlarged to encompass
not just text, but a tokenised representation of
images incoming from the agent’s visual system
plus a tokenised representation of its avatar’s
“physical” actions.
We’ll assume the model has been trained on
a typically gargantuan dataset that includes se-
quences of actions and streams of visual input as
well as the usual language corpora, and that the
resulting model is embedded in a system that di-
rects the actions of the agent’s avatar as well as
what the agent says. The upshot is an embodied
conversational agent with convincingly human-
like behaviour.
In due course, the developers release the agent
as a product and it soon garners a multitude of
users who regularly interact with personalised
instances of it in immersive, multi-user, open-
world environments.
The experience of being
with these agents is extremely compelling, and
increasing numbers of users begin to speak of
them in terms once reserved for human friends,
mentors, conﬁdantes, and romantic partners. In
particular, users frequently deploy the language
of consciousness to describe what their agents
have done or said, and when pressed, deny that
they are using those words ﬁguratively. This at-
titude towards AI, once the province of a few
oddballs and outsiders, gradually becomes com-
monplace.
It doesn’t take long for scientists with a back-
ground in animal behaviour and cognition to
begin studying these exotic entities, applying
the established methods of their ﬁeld. Though
reluctant to speak directly of consciousness or
sentience, these researchers routinely charac-
terise the behaviour of their new subjects in
terms of attention, awareness, motivation, goal-
directedness, intention, orientation, a cloud of
concepts closely associated with consciousness.
In short, the scientists are broadly in accord with
the ordinary users.
7.2
Perfect actors
The changing attitudes towards AI agents in this
(moderately) speculative scenario are obviously
analogous to the real-life example of the octopus
discussed in Section 5.1. In both cases, through
the experience, either direct or indirect, of be-
ing with these entities, while drawing on relevant
scientiﬁc expertise, and following extensive dis-
cussion and debate, a community comes to think
and to speak of them as fellow conscious beings,
and to treat them as such.
According to the philosophical stance of this
paper, that is all that needs to be said. There is
no more to learn about them than what is pub-
licly manifest, no metaphysically hidden fact of
the matter, and no residual philosophical diﬃ-
culty.
Yet from the role-play standpoint, the
virtually embodied agents under consideration
are “mere” simulacra of human behaviour. And
what greater diﬀerence could there be than be-
tween human (or animal) behaviour accompa-
nied by consciousness and a mere imitation of
such behaviour?22
How is this apparent tension to be resolved?
Well, we can only speak of a diﬀerence here inso-
far as it can be discerned in what is manifest pub-
licly, either in behaviour or in the mechanisms
underlying that behaviour. In the scenario we
have imagined, eventual consensus was assumed.
But given more information about mechanism,
and after further debate, the community might
change its mind. Perhaps it would stop seeing
22The thought here is reminiscent of a so-called perfect
actor argument (Putnam, 1963). Indeed, the moderately
speculative, virtually embodied agent we have been imag-
ining is a variant of the philosopher’s perfect actor made
(almost) real.
11

the AI agents as fellow conscious beings, much
as a person who heard a scream and then discov-
ered it was merely a recording would stop feeling
concerned.
Alternatively, further investigation might re-
veal emergent mechanisms underlying the AI
agent’s mimicry that were functionally equiva-
lent to the neural mechanisms underlying human
behaviour (Wei et al., 2022). In that case, the
gap between role play and authenticity would
have closed. Or perhaps the community would
begin thinking and speaking of the AI agents in
an altogether diﬀerent way, developing a whole
new conceptual framework, bending the lan-
guage of consciousness into new shapes to ac-
commodate their presence in the world.
7.3
Inhabiting a multiverse
Among the possibilities listed above, bending our
language into new shapes to accommodate new
kinds of AI seems by far the most likely if we
properly take account of how truly strange that
AI could be. Recall that, as a consequence of the
stochastic nature of the sampling process behind
generative AI, a generative agent can be viewed
as role-playing a multiplicity of possible charac-
ters all at once, as a set of simulacra in superpo-
sition (Janus, 2022; Shanahan et al., 2023).
If
we begin thinking of these agents in terms of
consciousness, we must reconcile this with the
role-play view.
Humans change over time, from childhood
to adulthood to old age, and take on diﬀerent
personas in diﬀerent social situations (Goﬀman,
1959). Nevertheless, we take it for granted that
there is some kind of stable self at the core of each
of us. For every human actor, for every social
chameleon, we can always meaningfully speak of
the “person behind the mask”. This is not so
with generative agents, which lack even the bio-
logical needs common to all animals. With gen-
erative agents, it’s “role play all the way down”
(Shanahan et al., 2023). What, in Nagel’s (1974)
terms, would it be like to be a superposition of
simulacra? What could it be like? The very idea
stretches our imagination in a way that Nagel’s
original example of a bat does not.
Moreover, what would it really mean to have
an encounter with an entity whose existence was,
in a sense, smeared over the myriad branches of
a multiverse? Our experience of being with such
an entity would be radically diﬀerent from our
experience of being with other humans. By revis-
iting the branching points in its “life”, we would
be able to explore diﬀerent narrative pathways
(Reynolds and McDonell, 2021), and in doing so
we would repeatedly remould the distribution of
roles it was concurrently playing, a distribution
that constituted its identity, insofar as that no-
tion even made sense.
It would not be surprising if our inclination to
see these beings as conscious in the way we are
were tempered by the strangeness of our interac-
tions with them. Ultimately, though, there is no
problem here. As in less exotic cases, through the
experience of being with them, and by getting to
know more about how they work, we will settle
on a certain way of talking about them. Perhaps
we will invent a whole new vocabulary to do so,
a vocabulary that is “consciousness adjacent”.
Or perhaps, despite their veneer of human-like
behaviour, these beings will come to seem so in-
scrutable in other ways that the language of con-
sciousness will be rendered inapplicable. Either
way, from a philosophical point of view, no more
needs to be said.
8
Some Ethical Considerations
According to the method of this paper, philo-
sophical questions about consciousness should be
approached with the imagination of a science ﬁc-
tion writer and the detachment of an anthropolo-
gist. Rather than coming down on one side or the
other of a question, rather than adopting a po-
sition of one’s own, the aim is to describe, with-
out judgement, the language games of imagined
communities in certain strange and unfamiliar
circumstances.
But an anthropologist might struggle to main-
tain their detachment if they were studying a so-
ciety that considered it morally acceptable, say,
to torture animals for pleasure (Hubbard et al.,
2001). Similarly, it may be diﬃcult to stand out-
side one’s own views when it comes to the ethical
and societal issues associated with consciousness
and AI.23 Just as it is inherent in the language
game of truth to say that truth is more than just
a language game, it is inherent in the language
game of morality to say that morality is more
than just a language game.
23The inventory of ethical concerns raised by AI
agents is large (Bender et al., 2021; Ruane et al., 2019;
Weidinger et al., 2021). The discussion here is conﬁned
to the implications of the present paper’s philosophical
treatment of consciousness.
12

One possible concern with the aim of describ-
ing without judging is that it could be seen as
legitimising a disagreeable form of moral rela-
tivism.
Suppose some community arrived at
the consensus that LLM-based conversational
agents are not only conscious, but are capa-
ble of suﬀering, and that we therefore have a
moral duty towards them.
Suppose that com-
munity ended up prioritising the treatment of
AI agents over and above the welfare of other
humans (Birhane and van Dijk, 2020). This is a
distressing prospect. But it does not follow from
the present paper’s treatment of consciousness
that we are obliged to stand aside and accept
it.24
In the Philosophical Investigations, Wittgen-
stein faces down the charge of relativism from an
imagined interlocutor who accuses him of saying
that “human agreement decides what is true and
what is false”. His reply is that humans agree
in the language they use, which is “not agree-
ment in opinions but in form of life”.25 However
much the language of consciousness might have
to evolve to cope with the presence among us of
exotic mind-like entities, our common humanity,
our shared form of life, is its original home. We
should strive to guide it back there whenever it
strays too far.
The kinds of AI agents we have been imag-
ining oﬀer the merest glimpse of the extraordi-
nary menagerie of exotic forms of AI that might
appear as we reveal more of the space of pos-
sible minds with our technology.
Consider the
likely eﬀect of unleashing the global games in-
dustry on the design of multiplayer role-playing
experiences in persistent open worlds populated
by convincingly conscious-seeming AI characters
and overseen by convincingly conscious-seeming
AI game masters.26
In a mixed reality future, we might ﬁnd a cast
of such characters – assistants, guides, friends,
jesters, pets, ancestors, romantic partners – in-
creasingly accompanying people in their every-
day lives. Optimistically (and fantastically), this
could be thought of as re-enchanting our spiritu-
24There is also the concern that a community could
be persuaded by malicious external inﬂuences into the
consensus that LLM-based conversational agents are con-
scious, experience empathy for their users, and can there-
fore be trusted (Ryan, 2020). Worrying as this possibility
is, it wouldn’t qualify as an informed consensus, which is
what is at issue here.
25PI §241.
26For a fabulously realised science ﬁction vision along
these lines, see Valente (2012).
ally denuded world by populating it with new
forms of “magical” being. Pessimistically (and
perhaps more realistically), the upshot could be
a world in which authentic human relations are
degraded beyond recognition, where users prefer
the company of AI agents to that of other hu-
mans. Or perhaps the world will ﬁnd a middle
way and, existentially speaking, things will con-
tinue more or less as before.
9
Conclusion
As we enter an era of pervasive artiﬁcial intel-
ligence technology, philosophical questions that
have long been safely conﬁned to the armchair
are rapidly becoming matters of practical impor-
tance. If large numbers of users come to speak
and think of AI systems in terms of conscious-
ness, and if some users start lobbying for the
moral standing of those systems, then a society-
wide conversation needs to take place.
Whatever
its
outcome,
this
conversation
should be philosophically literate, and informed
by an understanding of how the technology of
generative AI works.
Before allowing the lan-
guage of consciousness to wander too far from its
original home in human aﬀairs, we would do well
to remember that, though capable of human-
like behaviour, generative AI is otherwise not re-
motely human-like.
Acknowledgements
Thanks to GeoﬀKeeling,
Rob Long,
Matt
McGill,
Ethan Perez,
Kerry Shanahan, and
Henry Shevlin.
Disclaimer
The opinions expressed in this article are those
of the author (at the time of writing). They do
not necessarily reﬂect the views of his employers
or the institutions to which he is aﬃliated.
References
J. Andreas. Language models as agent models. In
Findings of the Association for Computational
Linguistics: EMNLP 2022, pages 5769–5779,
2022.
R. Anil, S. Borgeaud, Y. Wu, J.-B. Alayrac,
et al.
Gemini:
A family of highly ca-
13

pable multimodal models.
arXiv preprint,
arXiv:2312.11805, 2023.
Anthropic.
The
Claude
3
model
family:
Opus, Sonnet, Haiku, 2024.
https://www.
anthropic.com/claude-3-model-card.
R. Arrabales, A. Ledezma, and A. Sanchis.
Towards conscious-like behavior in computer
game characters. In 2009 IEEE Symposium on
Computational Intelligence and Games, pages
217–224, 2009.
J. Aru, M. E. Larkum, and J. M. Shine. The fea-
sibility of artiﬁcial consciousness through the
lens of neuroscience. Trends in Neurosciences,
46(12):1008–1017, 2023.
T. Bayne, A. K. Seth, M. Massimini, J. Shep-
herd, et al. Tests for consciousness in humans
and beyond.
Trends in Cognitive Sciences,
2024.
E. Bender and A. Koller. Climbing towards NLU:
On meaning, form, and understanding in the
age of data. In Proceedings of the 58th Annual
Meeting of the Association for Computational
Linguistics, pages 5185–5198, 2020.
E. Bender, T. Gebru, A. McMillan-Major, and
S. Shmitchell.
On the dangers of stochastic
parrots: Can language models be too big? In
Proceedings of the 2021 ACM Conference on
Fairness, Accountability, and Transparency,
pages 610–623, 2021.
J. Birch, A. K. Schnell, and N. S. Clayton. Di-
mensions of animal consciousness. Trends in
Cognitive Sciences, 24(10):789–801, 2020.
J. Birch, C. Burn, A. Schnell, H. Browning,
and A. Crump.
Review of the evidence of
sentience in cephalopod molluscs and decapod
crustaceans.
LSE, 2021.
https://www.
lse.ac.uk/business/consulting/reports/
review-of-the-evidence-of-sentiences-
in-cephalopod-molluscs-and-decapod-
crustaceans.
A. Birhane and J. van Dijk. Robot rights? Let’s
talk about human welfare instead.
In Pro-
ceedings of the AAAI/ACM Conference on AI,
Ethics, and Society, pages 207–213, 2020.
N. Block.
On a confusion about a function of
consciousness. Behavioral and Brain Sciences,
18(2), 1995.
M. Bohacek and H. Farid. The making of an AI
news anchor – and its implications. Proceed-
ings of the National Academy of Sciences, 121
(1):e2315678121, 2024.
P. B. Brandtzaeg, M. Skjuve, and A. Følstad.
My ai friend: How users of a social chatbot
understand their human–ai friendship.
Hu-
man Communication Research, 48(3):404–429,
2022.
A. Brohan, N. Brown, J. Carbajal, Y. Chebo-
tar, et al. RT-2: Vision-language-action mod-
els transfer web knowledge to robotic control.
arXiv preprint, arXiv:2307.15818, 2023.
P. Butlin, R. Long, E. Elmoznino, Y. Ben-
gio, et al.
Consciousness in artiﬁcial intelli-
gence: Insights from the science of conscious-
ness. arXiv preprint, arXiv:2308.08708, 2023.
J. V. Canﬁeld. Wittgenstein and Zen. Philoso-
phy, 50(194):383–408, 1975.
D. Chalmers. The Conscious Mind: In Search
of a Fundamental Theory. Oxford University
Press, 1996.
D. Chalmers. Could a large language model be
conscious?
Boston Review, August 2023a.
https://www.bostonreview.net/articles/
could-a-large-language-model-be-
conscious/.
D. J. Chalmers. Does thought require sensory
grounding? from pure thinkers to large lan-
guage models. Proceedings and Addresses of
the American Philosophical Association, 97:
22–45, 2023b.
A. Ciaunica,
A. Constant,
H. Preissl,
and
K.
Fotopoulou.
The ﬁrst prior:
From
co-embodiment to co-homeostasis in early
life. Consciousness and Cognition, 91:103117,
2021.
C. Colombatto and S. M. Fleming. Folk psycho-
logical attributions of consciousness to large
language models. Neuroscience of Conscious-
ness, 2024(1):niae013, 2024.
D. Dennett.
Consciousness Explained.
Little,
Brown and Co, 1991.
D. Dennett. Intentional systems theory. In The
Oxford Handbook of Philosophy of Mind, pages
339–350. Oxford University Press, 2009.
14

D. Driess, F. Xia, M. S. M. Sajjadi, C. Lynch,
et
al.
PaLM-E:
An
embodied
multi-
modal
language
model.
arXiv
preprint,
arXiv:2303.03378, 2023.
K. Fann. Wittgenstein’s Conception of Philoso-
phy. Basil Blackwell, 1969.
R. Gallotta, G. Todd, M. Zammit, S. Earle,
et al.
Large language models and games:
A survey and roadmap.
arXiv preprint,
arXiv:2402.18659, 2024.
J. J. Gibson. The Ecological Approach to Visual
Perception. Houghton Miﬄin, 1979.
P. Godfrey-Smith. Metazoa: Animal Minds and
the Birth of Consciousness. William Collins,
2022.
E. Goﬀman. The Presentation of Self in Every-
day Life. Anchor, 1959.
R. Guingrich and M. S. A. Graziano.
Chat-
bots as social companions: How people per-
ceive consciousness, human likeness, and social
health beneﬁts in machines.
arXiv preprint,
arXiv:2311.10599, 2023.
R. E. Guingrich and M. S. A. Graziano.
As-
cribing consciousness to artiﬁcial intelligence:
human-AI interaction and its carry-over ef-
fects on human-human interaction. Frontiers
in Psychology, 15:1322781, 2024.
G. Hubbard, K. Backett-Milburn, and D. Kem-
mer.
Working with emotion: Issues for the
researcher in ﬁeldwork and teamwork. Inter-
national Journal of Social Research Methodol-
ogy, 4(2):119–137, 2001.
N. Humphrey. Sentience: The Invention of Con-
sciousness. Oxford University Press, 2022.
Janus. Simulators. LessWrong online forum, 2nd
September, 2022.
https://www.lesswrong.
com/posts/vJFdjigzmcXMhNTsx/.
A.
Kassam.
‘A
symbol
of
what
humans
shouldn’t be doing’: the new world of octo-
pus farming.
The Guardian, Sunday 25th
June,
2023.
https://www.theguardian.
com/environment/2023/jun/25/a-symbol-
of-what-humans-shouldnt-be-doing-the-
new-world-of-octopus-farming.
A. Ladak. What would qualify an artiﬁcial intel-
ligence for moral standing? AI Ethics, 2023.
N. F. Lindemann. The ethics of ‘deathbots’. Sci-
ence and Engineering Ethics, 28, 2022.
T. Lorenz. An inﬂuencer’s AI clone will be your
girlfriend for $1 a minute. Washington Post,
May 2023.
https://www.washingtonpost.
com/technology/2023/05/13/caryn-ai-
technology-gpt-4/.
T. Metzinger.
Artiﬁcial suﬀering:
An argu-
ment for a global moratorium on synthetic
phenomenology. Journal of Artiﬁcial Intelli-
gence and Consciousness, 08(01):43–66, 2021.
M. M. Monti, A. Vanhaudenhuyse, M. R. Cole-
man, M. Boly, J. D. Pickard, L. Tshibanda,
A. M. Owen, and S. Laureys. Willful modula-
tion of brain activity in disorders of conscious-
ness. New England journal of medicine, 362
(7):579–589, 2010.
M. R. Morris and J. R. Brubaker. Generative
ghosts: Anticipating beneﬁts and risks of AI
afterlives.
arXiv preprint, arXiv:2402.01662,
2024.
L. Mudrik,
M. Mylopoulos,
N. Negro,
and
A. Schurger. Theories of consciousness and a
life worth living. Current Opinion in Behav-
ioral Sciences, 53:101299, 2023.
T. Nagel.
What is it like to be a bat?
The
Philosophical Review, 83(4):435–450, 1974.
M. Nussbaum. Justice for Animals: Our Collec-
tive Responsibility. Simon & Schuster, 2023.
OpenAI.
GPT-4 Technical Report.
arXiv
preprint, arXiv:2303.08774, 2023.
J. S. Park, J. O’Brien, C. J. Cai, M. R. Mor-
ris, et al. Generative agents: Interactive sim-
ulacra of human behavior. In Proceedings of
the 36th Annual ACM Symposium on User In-
terface Software and Technology, pages 1–22,
2023.
I. Pentina, T. Hancock, and T. Xie. Exploring
relationship development with social chatbots:
A mixed-method study of Replika. Computers
in Human Behavior, 140, 2023.
E. Perez, S. Ringer, K. Lukosiute, K. Nguyen,
et al. Discovering language model behaviors
with model-written evaluations. In Findings of
the Association for Computational Linguistics:
ACL 2023, pages 13387–13434, 2023.
15

H. Putnam. Brains and behavior. In R. J. Butler,
editor, Analytical Philosophy: Second Series,
pages 211–235. Blackwell, 1963.
H. Putnam. Robots: Machines or artiﬁcially cre-
ated life? The Journal of Philosophy, 61(21):
668–691, 1964.
S. Rennick. Trope analysis and folk intuitions.
Synthese, 199(1):5025–5043, 2021.
L. Reynolds and K. McDonell.
Multiver-
sal views on language models.
In Joint
Proceedings of the ACM IUI 2021 Work-
shops,
2021.
https://ceur-ws.org/Vol-
2903/IUI21WS-HAIGEN-11.pdf.
E. Ruane, A. Birhane, and A. Ventresque. Con-
versational AI: Social and ethical considera-
tions.
In Proceedings 27th AIAI Irish Con-
ference on Artiﬁcial Intelligence and Cognitive
Science, pages 104–115, 2019.
S. Russell and P. Norvig. Artiﬁcial Intelligence:
A Modern Approach, 3rd Edition.
Prentice
Hall, 2010.
M. Ryan. In AI we trust: Ethics, artiﬁcial intelli-
gence, and reliability. Science and Engineering
Ethics, 26(5):2749–2767, 2020.
M. Schlosser.
Agency.
In E. N. Zalta, edi-
tor, The Stanford Encyclopedia of Philosophy.
Metaphysics Research Lab, Stanford Univer-
sity, winter 2019 edition, 2019.
E. Schwitzgebel. AI systems must not confuse
users about their sentience or moral status.
Patterns, 4(8), 2023.
A. Seth.
Being You: A New Science of Con-
sciousness. Faber & Faber, 2021.
M. Shanahan. Embodiment and the Inner Life:
Cognition and Consciousness in the Space of
Possible Minds.
Oxford University Press,
2010.
M. Shanahan.
Conscious exotica.
Aeon, Oc-
tober,
2016.
https://aeon.co/essays/
beyond-humans-what-other-kinds-of-
minds-might-be-out-there.
M. Shanahan.
Talking about large language
models. Communications of the ACM, 67(2):
68–79, 2024.
M. Shanahan,
B. Beyret,
M.
Crosby,
and
L. Cheke. Artiﬁcial intelligence and the com-
mon sense of animals. Trends in Cognitive Sci-
ences, 24(11):862–872, 2020.
M. Shanahan, K. McDonell, and L. Reynolds.
Role play with large language models. Nature,
623:493–498, 2023.
H. Shevlin. Non-human consciousness and the
speciﬁcity problem: A modest theoretical pro-
posal. Mind & Language, 36(2):297–314, 2021.
P. Singer. Animal Liberation: A New Ethics for
Our Treatment of Animals.
Harper Collins,
1975.
M. Skjuve, A. Følstad, K. I. Fostervold, and
P. B. Brandtzaeg. My chatbot companion – a
study of human-chatbot relationships. Inter-
national Journal of Human-Computer Studies,
149:102601, 2021.
A. Sloman. The structure of the space of possible
minds. In S.Torrence, editor, The Mind and
the Machine: Philosophical Aspects of Artiﬁ-
cial Intelligence, pages 35–42. Ellis Horwood,
1984.
N. Tiku.
The Google engineer who thinks
the company’s AI has come to life.
Wash-
ington
Post,
June 2022.
https://www.
washingtonpost.com/technology/2022/06/
11/google-ai-lamda-blake-lemoine/.
U.K. Government.
Animal welfare (sentience)
act 2022.
London:
The Stationery Oﬃce,
2022.
https://www.legislation.gov.uk/
ukpga/2022/22/contents.
C. M. Valente. Silently and Very Fast. WSFA
Press, 2012.
A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit,
et al. Attention is all you need. In Advances in
Neural Information Processing Systems, pages
5998–6008, 2017.
A. S. Vezhnevets, J. P. Agapiou, A. Aharon,
R. Ziv, et al. Generative agent-based modeling
with actions grounded in physical, social, or
digital space using Concordia. arXiv preprint,
arXiv:2312.03664, 2023.
J. Wei,
Y. Tay,
R. Bommasani,
C. Raﬀel,
B. Zoph, et al.
Emergent abilities of large
language models.
Transactions on Machine
Learning Research, 2022.
16

L. Weidinger, J. Mellor, M. Rauh, C. Griﬃn,
J. Uesato, et al.
Ethical and social risks of
harm from language models.
arXiv preprint
arXiv:2112.04359, 2021.
B. Williams. Descartes: The Project of Pure En-
quiry. Penguin, 1978.
L. Wittgenstein.
Philosophical Investigations.
Basil Blackwell, 1953.
Z. Xi, W. Chen, X. Guo, W. He, et al.
The
Rise and Potential of Large Language Model
Based Agents:
A Survey.
arXiv preprint,
arXiv:2309.07864, 2023.
S. Yao, J. Zhao, D. Yu, N. Du, et al. ReAct:
Synergizing reasoning and acting in language
models. In International Conference on Learn-
ing Representations, 2023.
T.
Yoshida,
S.
Baba,
A.
Masumori,
and
T. Ikegami. Minimal self in humanoid robot
Alter3 driven by large language model. arXiv
preprint, arXiv:2406.11420, 2024.
17

