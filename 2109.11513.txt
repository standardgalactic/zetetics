arXiv:2109.11513v1  [cs.AI]  23 Sep 2021
Temporal Inference with Finite Factored Sets
Scott Garrabrant
Machine Intelligence Research Institute
scott@intelligence.org
Abstract
We propose a new approach to temporal inference, inspired by the Pearlian
causal inference paradigm—though quite diﬀerent from Pearl’s approach for-
mally. Rather than using directed acyclic graphs, we make use of factored
sets, which are sets expressed as Cartesian products. We show that ﬁnite fac-
tored sets are powerful tools for inferring temporal relations. We introduce
an analog of d-separation for factored sets, conditional orthogonality, and we
demonstrate that this notion is equivalent to conditional independence in all
probability distributions on a ﬁnite factored set.
Contents
1
Introduction
2
1.1
Pearlian Causal Inference
. . . . . . . . . . . . . . . . . . . . . . . .
2
1.2
Overview
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3
2
Factorization
3
2.1
Partitions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3
2.2
Factorizations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
4
2.3
Chimera Functions . . . . . . . . . . . . . . . . . . . . . . . . . . . .
5
2.4
Trivial Factorizations . . . . . . . . . . . . . . . . . . . . . . . . . . .
6
2.5
Finite Factored Sets
. . . . . . . . . . . . . . . . . . . . . . . . . . .
7
3
Orthogonality and Time
8
3.1
Generating a Partition with Factors
. . . . . . . . . . . . . . . . . .
8
3.2
History
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
10
3.3
Orthogonality . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
10
3.4
Time . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
11
4
Subpartitions and Conditional Orthogonality
12
4.1
Generating a Subpartition . . . . . . . . . . . . . . . . . . . . . . . .
12
4.2
History of a Subpartition
. . . . . . . . . . . . . . . . . . . . . . . .
14
4.3
Conditional Orthogonality . . . . . . . . . . . . . . . . . . . . . . . .
16
5
Polynomials and Probability
18
5.1
Characteristic Polynomials . . . . . . . . . . . . . . . . . . . . . . . .
18
5.2
Factoring Characteristic Polynomials . . . . . . . . . . . . . . . . . .
20
5.3
Characteristic Polynomials and Orthogonality . . . . . . . . . . . . .
21
5.4
Probability Distributions on Finite Factored Sets . . . . . . . . . . .
23
5.5
The Fundamental Theorem of Finite Factored Sets . . . . . . . . . .
23
Research supported by the Machine Intelligence Research Institute (intelligence.org).
1

6
Inferring Time
24
6.1
Factored Set Models . . . . . . . . . . . . . . . . . . . . . . . . . . .
25
6.2
Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
25
7
Applications, Future Work, and Speculation
28
7.1
Inference . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
28
7.2
Inﬁnity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
30
7.3
Embedded Agency . . . . . . . . . . . . . . . . . . . . . . . . . . . .
31
1
Introduction
1.1
Pearlian Causal Inference
Judea Pearl’s theory of inferred causation (e.g., as presented in chapter 2 of Causal-
ity: Models, Reasoning, and Inference) was a deep advance in our understanding of
the nature of time. The Pearlian paradigm allows us to infer causal relationships
between variables using statistical data, and thereby infer temporal sequence—in
deﬁance of the old adage that correlation does not imply causation.
In particular, given a collection of variables and a joint probability distribution
over those variables, the Pearlian paradigm can often infer temporal relationships
between the variables.
The joint probability distribution is usually what gets emphasized in discus-
sions of Pearl’s approach.
Quite a bit of work is being done, however, by the
assumption that we are handed “a collection of variables” to reason about. The
Pearlian paradigm is not inferring temporal relationships from purely statistical
data, but rather inferring temporal relationships from statistical data together with
data about how to factorize the world into variables.1
A doctor who misdiagnoses their patient or misidentiﬁes a symptom may base
their subsequent reasoning on a wrong factorization of the situation into causally
relevant variables. We would ideally like to build fewer assumptions like this into our
model of inference, and instead allow the reasoner to ﬁgure such facts out, consider
the merits of diﬀerent factorizations into variables, etc.
Instead of beginning with a collection of variables and a joint probability dis-
tribution over those variables, one could imagine starting with just a ﬁnite sample
space and a probability distribution on that sample space. In this way, we might
hope to do temporal inference purely using statistical data, without relying on a
priori knowledge of a canonical way of factoring the situation into variables.
How might one do temporal inference without an existing factorization? One
way might be to just consider all possible variables that can be deﬁned on the
sample space. This gives us one variable for each partition of the set.
However, when one tries to apply Pearl’s methods to this collection of variables,
one quickly runs into a problem: many of the variables deﬁnable on a ﬁxed set
are deterministic functions of each other. The Pearlian paradigm, as presented in
the early chapters of Causality, lacks tools for performing temporal inference on
variables that are highly deterministically related.2
1. Although I say “factorize” here, note that this will not be the kind of factorization
that shows up in ﬁnite factored sets, because (as we will see) disjoint factors must be
independent in a ﬁnite factored set. I appeal to the same concept in both contexts because
factorization is just a very general and useful concept, rather than to indicate a direct
connection.
2. At least, it lacks such causal inference tools unless we assume access to interventional
data.
2

We will introduce a new approach to temporal inference instead—one which is
heavily inspired by the Pearlian paradigm, but approaches the problem with a very
diﬀerent formal apparatus, and does not make use of graphical models.
1.2
Overview
We’ll begin by introducing the concept of a ﬁnite factored set, in Section 2. This
will be our analogue of the directed acyclic graphs in Pearl’s framework.
In Section 3, we will introduce the concepts of time and orthogonality, which
can be read oﬀof a ﬁnite factored set. In Pearl’s framework, “time” corresponds to
directed paths between nodes, and “orthogonality” corresponds to nodes that have
no common ancestor.
In Section 4, we will introduce conditional orthogonality, which is our analogue
of d-separation. We show that conditional orthogonality satisﬁes (a modiﬁed ver-
sion of) the compositional semigraphoid axioms. We then (in Section 5) prove the
fundamental theorem of ﬁnite factored sets, which states that conditional orthogo-
nality is equivalent to conditional independence in all probability distributions on
the ﬁnite factored set.
In Section 6, we discuss how to do temporal inference using ﬁnite factored sets,
and give two examples. Finally, in Section 7 we discuss applications and future
work, with an emphasis on temporal and conceptual inference, generalizing ﬁnite
factored sets to the inﬁnite case, and applications to embedded agency (Demski and
Garrabrant 2019).
And here, we take our leave of Pearl. We’ve highlighted this approach’s relation-
ship to the Pearlian paradigm in order to motivate ﬁnite factored sets and explain
how we’ll be using them in this paper. Technically, however, our approach is quite
unlike Pearl’s, and the rest of the paper will stand alone.
2
Factorization
Before giving a deﬁnition of ﬁnite factored sets, we will recall the deﬁnition of a
partition, and give some basic notation related to partitions.
We do this for two reasons. First, we will use partitions in the deﬁnition of a
factored set; and second, we want to draw attention to a duality between the notion
of a partition, and the notion of a factorization.
2.1
Partitions
We begin with a deﬁnition of disjoint union.
Deﬁnition 1 (disjoint union). Given a set S of sets, let F(S) denote the set of all
ordered pairs (T, t), where T ∈S and t ∈T .3
Deﬁnition 2 (partition). A partition of a set S is a set X ⊆P(S) of nonempty
subsets of S such that the function ι : F(X) →S given by ι(x, s) = s is a bijection.4
Let Part(S) denote the set of all partitions of S. The elements of a partition
are called parts.
An equivalent deﬁnition of partition is often given: a partition is a set X of
nonempty subsets of S that are pairwise disjoint and union to S. We choose the
above deﬁnition because it will make the symmetry between partitions and factor-
izations more obvious.
Deﬁnition 3 (trivial partition). A partition X of a set S is called trivial if |X| = 1.
Deﬁnition 4. Given a partition X of a set S, and an element s ∈S, let [s]X
denote the unique x ∈X such that s ∈x.
3. Note that this deﬁnition and Deﬁnition 9 could have been made more general by
taking S to be a multiset.
4. P(S) denotes the power set of S.
3

Deﬁnition 5. Given a partition X of a set S, and elements s0, s1 ∈S, we say
s0 ∼X s1 if [s0]X = [s1]X.
Proposition 1. Given a partition X of a set S, ∼X is an equivalence relation on
S.
Proof. Trivial.
Deﬁnition 6 (ﬁner and coarser). We say that a partition X of S is ﬁner than
another partition Y of S, if for all s0, s1 ∈S, if s0 ∼X s1, then s0 ∼Y s1.
If X is ﬁner than Y , we also say Y is coarser than X, and we write X ≥S Y
and Y ≤S X.
Deﬁnition 7 (discrete and indiscrete partitions). Given a set S, let DisS = {{s} |
s ∈S}.
If S is empty, let IndS = {}, and if S is nonempty, let IndS = {S}.
DisS is called the discrete partition, and IndS is called the indiscrete partition.
Proposition 2. For any set S, ≥S is a partial order on Part(S). Further, for all
X ∈Part(S), DisS ≥S X and X ≥S IndS.
Proof. Trivial.
While both notations are sometimes used, it is more standard to draw the symbol
in the opposite direction and have X ≤Y when X is ﬁner than Y . We choose to go
against that standard because we want to think of partitions in part as the ability
to distinguish between elements, and ﬁner partitions correspond to greater ability
to distinguish.5
Deﬁnition 8 (common reﬁnement). Given a set C of partitions of a ﬁxed set S, let
W
S(C) denote the partition X ∈Part(S) satisfying s0 ∼X s1 if and only if s0 ∼c s1
for all c ∈C. Given X, Y ∈Part(S), we let X ∨S Y = W
S({X, Y }).
2.2
Factorizations
We start with a deﬁnition of Cartesian product.
Deﬁnition 9 (Cartesian product). Given a set S of sets, let d(S) denote the set
of all functions f : S →F(S) such that for all T ∈S, f(T ) is of the form (T, t),
for some t ∈T .
We can now give the deﬁnition of a factorization of a set.
Deﬁnition 10 (factorization). A factorization of a set S is a set B ⊆Part(S) of
nontrivial partitions of S such that the function π : S →d(B), given by π(s) =
(b 7→(b, [s]b)), is a bijection.
Let Fact(S) denote the set of all factorizations of S. The elements of a factor-
ization are called factors.
In other words, a set of nontrivial partitions is a factorization of S if for each
way of choosing one part from each factor, there exists a unique element of S in the
intersection of those parts.
Notice the duality between the deﬁnitions of partition and factorization. We
replace subsets with partitions, nonempty with nontrivial, and disjoint union with
Cartesian product, and we reverse the direction of the function. We can think of a
factorization of S as a way to view S as a product, in the same way that a partition
was a way to view S as a disjoint union.
A factored set is just a set together with a factorization of that set.
5. In our view, “Y ≥X” is also a more natural way to visually represent a mapping
between a three-part partition Y that is ﬁner than a two-part partition X.
4

Deﬁnition 11 (factored set). A factored set F is an ordered pair (S, B), such that
B is a factorization of S.
If F = (S, B) is a factored set, we let set(F) = S, and let basis(F) = B.
Proposition 3. Given a factored set F = (S, B), and elements s0, s1 ∈S, if
s0 ∼b s1 for all b ∈B, then s0 = s1.
Proof. Let F = (S, B) be a ﬁnite factored set, and let s0, s1 ∈S satisfy s0 ∼b s1
for all b ∈B.
Let π : S →d(B) be given by π(s) = (b 7→(b, [s]b)), as in the deﬁnition of
factorization. Then π(s0) = (b 7→(b, [s0]b)) = (b 7→(b, [s1]b)) = π(s1). Since π is
bijective, this means s0 = s1.
2.3
Chimera Functions
The following theorem can be viewed as an alternate characterization of factoriza-
tion. We will use this alternate characterization to deﬁne chimera functions, which
will be useful tools for manipulating elements of factored sets.
Theorem 1. Given a set S, a set B of nontrivial partitions of S is a factorization
of S if and only if for every function g : B →S, there exists a unique s ∈S such
that for all b ∈B, s ∼b g(b).
Proof. First, we let B be a factorization of S, and let g : B →S be any function. We
want to show that there exists a unique s ∈S such that for all b ∈B, s ∼b g(b). Let
π : S →d(B) be given by π(s) = (b 7→(b, [s]b)), as in the deﬁnition of factorization.
Note that π is bijective, and thus has an inverse.
Let s = π−1(b 7→(b, [g(b)]b)). Observe that this is well-deﬁned, because (b 7→
(b, [g(b)]b)) is in fact in d(B). We will show that s ∼b g(b) for all b ∈B, and the
uniqueness of this s will then follow directly from Proposition 3.
We have π(s) = (b 7→[s]b) by the deﬁnition of π.
However, we also have
π(s) = (b 7→[g(b)]b) by the deﬁnition of s. Thus, b 7→[s]b and b 7→[g(b)]b are the
same function, so [s]b = [g(b)]b for all b ∈B, so s ∼b g(b) for all b ∈B.
Conversely, let S be any set, and let B be any set of nontrivial partitions of S.
Assume that for all g : B →S, there exists a unique s ∈S satisfying s ∼b g(b)
for b ∈B. Again, let π : S →d(B) be given by π(s) = (b 7→(b, [s]b)), as in the
deﬁnition of factorization. We want to show that π is invertible.
First, we show that π is injective. Take an arbitrary s0 ∈S, and let g : B →S
be the constant function satisfying g(b) = s0 for all b ∈B. Given another s1 ∈S, if
π(s0) = π(s1), then (b 7→[s0]b) = (b 7→[s1]b), so [s1]b = [s0]b = [g(b)]b for all b ∈B,
so s0 ∼b s1 ∼b g(b) for all b ∈B. Since there is a unique s ∈S satisfying s ∼b g(b)
for all b ∈B, this means s0 = s1. Thus π is injective.
To see that π is surjective, consider some arbitrary h ∈d(B). We want to show
that there exists an s ∈S with h = π(s).
For all b ∈B, let Hb ∈b be given by h(b) = (b, Hb), which is well-deﬁned since
h ∈d(B). Note that Hb is a nonempty subset of S, so there exists a function
g : B →S with g(b) ∈Hb for all b ∈B. Fix any such g, and let s satisfy s ∼b g(b)
for all b ∈B.
We thus have that for all b ∈B, h(b) = (b, Hb) = (b, [g(b)]b) = (b, [s]b) = π(s)(b),
so h = π(s). Thus π is surjective.
Since π is bijective, we have that B is a factorization of S.
This also gives us that factors are disjoint from each other.
Corollary 1. Given a factored set F = (S, B) and distinct factors b0, b1 ∈B,
b0 ∩b1 = {}.
Proof. Assume by way of contradiction that T ∈b0∩b1. Since b0 is nontrivial, there
must be some other T ′ ∈b0 with T ∩T ′ = {}. Let g : B →S be any function such
that g(b0) ∈T ′ and g(b1) ∈T . Then there can be no s such that s ∼b0 g(b0) and
s ∼b1 g(b1), since then s would be in both T and T ′. This contradicts Theorem
1.
5

We are now ready to deﬁne the chimera function of a factored set.
Deﬁnition 12 (chimera function). Given a factored set F = (S, B), the chimera
function (of F) is the function χF : (B →S) →S deﬁned by χF (g) ∼b g(b) for all
g : B →S and b ∈B.
The name “chimera function” comes from the fact that χF can be viewed as
building an element of S by fusing together the properties of various diﬀerent ele-
ments. Since we will often apply the chimera function to functions g that only take
on two values, we will give notation for this special case.
Deﬁnition 13. Given a factored set F = (S, B), and a subset C ⊆B, let χF
C :
S × S →S be given by χF
C(s, t) = χF (g), where g : B →S is given by g(b) = s if
b ∈C, and g(b) = t otherwise.
For T, R ⊆S, we will write χF
C(T, R) for {χF
C(t, r) | t ∈T, r ∈R}.
The following is a list of properties of χF
C, which will be useful in later proofs.
All of these properties follow directly from the deﬁnition of χF
C.
Proposition 4. Fix F = (S, B), a factored set, C, D ⊆B, and s, t, r ∈S.
1. χF
C(s, t) ∼c s for all c ∈C.
2. χF
C(s, t) ∼b t for all b ∈B \ C.
3. χF
C(s, s) = s.
4. χF
B\C(s, t) = χF
C(t, s).
5. χF
C∪D(s, t) = χF
C(s, χF
D(s, t)).
6. χF
C∩D(s, t) = χF
C(χF
D(s, t), t).
7. χF
C(χF
C(s, t), r) = χF
C(s, χF
C(t, r)) = χF
C(s, r).
8. χF
C(s, χF
D(t, r)) = χF
D(χF
C(s, t), χF
C(s, r)).
9. χF
C(χF
D(s, t), r) = χF
D(χF
C(s, r), χF
C(t, r)).
10. χF
B(s, t) = s.
11. χF
{}(s, t) = t.
Proof. Trivial.
2.4
Trivial Factorizations
We now deﬁne a notion of a trivial factorization of a set, and show that every set
has a unique trivial factorization.
Deﬁnition 14 (trivial factorization). A factorization B of a set S is called trivial
if |B| ≤1. A factored set (S, B) is called trivial if B is trivial.
Proposition 5. For every set S, there exists a unique trivial factorization B of S.
If |S| ̸= 1, this trivial factorization is given by B = {DisS}, and if |S| = 1, it is
given by B = {}.
Proof. We start with the case where |S| = 0. The only partition of S is {}, so we
only need to consider the sets of partitions {{}} and {} as potential factorizations.
{{}} is vacuously a factorization of S by Theorem 1, since there are no functions
from {{}} to S. {} is not a factorization by Theorem 1, since there is a function
from {} to S, but there is no element of S. Thus, when |S| = 0, {{}} = {DisS} is
the unique trivial factorization of S.
6

Next, consider the case where |S| = 1. First, observe that the unique s ∈S
vacuously satisﬁes s ∼b g(b) for all g : {} →S and b ∈{}, since there is no b ∈{}.
Thus, by Theorem 1, {} is a factorization of S. Further, {} is the only factorization
of S, since there are no nontrivial partitions of S. Thus, when |S| = 1, {} is the
unique trivial factorization of S.
Next, we consider the case where |S| ≥2. Observe that DisS is a nontrivial
partition of S. Let B = {DisS}. We want to show that B is a factorization of S. By
Theorem 1, it suﬃces to show that for all g : B →S, there exists a unique s ∈S with
s ∼DisS g(DisS). We can take s = g(DisS), which clearly satisﬁes s ∼DisS g(DisS).
This s is unique, since if s′ ∼DisS g(DisS), then s′ ∈[g(DisS)]DisS = [s]b = {s}, so
s′ = s. Thus B is a factorization of S.
On the other hand, if |S| ≥2, {} is not a factorization of S, since if it were,
Proposition 3 would imply that all elements of S are equal. Further, for any partition
b of S, with b ̸= {DisS}, there must exist s0, s1 ∈S, with s0 ∼b s1, but s0 ̸= s1.
Thus {b} cannot be a factorization of S by Proposition 3. Thus when |S| ≥2, DisS
is the unique trivial factorization of S.
2.5
Finite Factored Sets
This paper will primarily be about ﬁnite factored sets.
Deﬁnition 15. If F = (S, B) is a factored set, the size of F, written size(F), is
the cardinality of S. The dimension of F, written dim(F), is the cardinality of B.
F is called ﬁnite if its size is ﬁnite, and ﬁnite-dimensional if its dimension is ﬁnite.
We suspect that the theory of inﬁnite factored sets is both interesting and impor-
tant. However, it is outside of the scope of this paper, which will require ﬁniteness
for many of its key results.
Some of the deﬁnitions and results in this paper will be given for ﬁnite factored
sets, in spite of the fact that they could easily be extended to ﬁnite-dimensional or
arbitrary factored sets. This is because they can often be extended in more than one
way, and determining which extension is most natural requires further developing
the theory of arbitrary factored sets.
Proposition 6. Every ﬁnite factored set is also ﬁnite-dimensional.
Proof. If F = (S, B) is a factored set, B is a set of sets of subsets of S. Thus,
|B| ≤22|S|.
This bound is horrible and will be improved in Proposition 9. First, however,
we will take a look at the number of factorizations of a ﬁxed ﬁnite set.
Proposition 7. Let F = (S, B) be a ﬁnite factored set. Then |S| = Q
b∈B |b|.
Proof. Trivial.
Proposition 8. If |S| is equal to 0, 1, or a prime, the trivial factorization of S is
the only factorization of S.
Proof. If |S| = 0 or |S| = 1, then |Part(S)| = 1, so B ⊆Part(S) can have cardinality
at most 1.
If |S| = p, a prime, then by Proposition 7, |b| must divide p for all b ∈B.
Since factorizations cannot contain trivial partitions, this means |b| = p for all
b ∈B. However, {{s} | s ∈S} is the only element of Part(S) of cardinality p, so
|B| ≤1.
On the other hand, in the case where |S| is ﬁnite and composite, the
number of factorizations of S grows very quickly, as seen in Table 1.
Given the naturalness of the notion of factorization, we were surprised to discover
that this sequence did not exist on the On-Line Encyclopedia of Integer Sequences
(OEIS). We added the sequence, A338681, on April 30, 2021.
To give one concrete example, the four factorizations of the set {0, 1, 2, 3} are:
7

|S|
|Fact(S)|
|S|
|Fact(S)|
0
1
13
1
1
1
14
8648641
2
1
15
1816214401
3
1
16
181880899201
4
4
17
1
5
1
18
45951781075201
6
61
19
1
7
1
20
3379365788198401
8
1681
21
1689515283456001
9
5041
22
14079294028801
10
15121
23
1
11
1
24
4454857103544668620801
12
13638241
25
538583682060103680001
Table 1: The number of factorizations of a set S with cardinality up to 25.
• {{{0}, {1}, {2}, {3}}},
• {{{0, 1}, {2, 3}}, {{0, 2}, {1, 3}}},
• {{{0, 1}, {2, 3}}, {{0, 3}, {1, 2}}}, and
• {{{0, 2}, {1, 3}}, {{0, 3}, {1, 2}}}.
Proposition 9. Let F be a ﬁnite factored set.
1. If size(F) = 0, then dim(F) = 1.
2. If size(F) = 1, then dim(F) = 0.
3. If size(F) = p is prime, then dim(F) = 1.
4. If size(F) = p0 . . . pk−1 is a product of k ≥2 primes, then 1 ≤dim(F) ≤k.
Proof. The ﬁrst three parts follow directly from Proposition 5 and Proposition 8.
For the fourth part, let F = (S, B), and let |S| = p0 . . . pk−1 be a product of k ≥2
primes.
By Proposition 7, |S| = Q
b∈B |b|. Consider an arbitrary b ∈B. Since b is a
nontrivial partition of a ﬁnite set S, |b| is ﬁnite and |b| ̸= 1. If |b| were 0, then |S|
would be 0. Thus |b| is a natural number greater than or equal to 2. B cannot be
empty, since |S| ̸= 1. If |B| were greater than k, then we would be able to express
|S| as a product of more than k natural numbers greater than or equal to 2, which
is clearly not possible since |S| is a product of k primes. Thus 1 ≤dim(F) ≤k.
3
Orthogonality and Time
The main way we’ll be using factored sets is as a foundation for talking about
concepts like orthogonality and time. Finite factored sets will play a role that’s
analogous to that of directed acyclic graphs in Pearlian causal inference.
To utilize factored sets in this way, we will ﬁrst want to introduce the concept
of generating a partition with factors.
3.1
Generating a Partition with Factors
Deﬁnition 16 (generating a partition). Given a ﬁnite factored set F = (S, B),
a partition X ∈Part(S), and a C ⊆B, we say C generates X (in F), written
C ⊢F X, if χF
C(x, S) = x for all x ∈X.
The following proposition gives many equivalent deﬁnitions of ⊢F .
8

Proposition 10. Let F = (S, B) be a ﬁnite factored set, let X ∈Part(S) be a
partition of S, and let C be a subset of B. The following are equivalent:
1. C ⊢F X.
2. χF
C(x, S) = x for all x ∈X.
3. χF
C(x, S) ⊆x for all x ∈X.
4. χF
C(x, y) ⊆x for all x, y ∈X.
5. χF
C(s, t) ∈[s]X for all s, t ∈S.
6. χF
C(s, t) ∼X s for all s, t ∈S.
7. X ≤S
W
S(C).
Proof. The equivalence of conditions 1 and 2 is by deﬁnition.
The equivalence of conditions 2 and 3 follows directly from the fact that
χF
C(s, s) = s for all s ∈x, so χF
C(x, S) ⊇χF
C(x, x) ⊇x.
To see that conditions 3 and 4 are equivalent, observe that since S = S
y∈X y,
χF
C(x, S) = S
y∈X χF
C(x, y). Thus, if χF
C(x, S) ⊆x, χF
C(x, y) ⊆x for all y ∈X, and
conversely if χF
C(x, y) ⊆x for all y ∈X, then χF
C(x, S) ⊆x.
To see that condition 3 is equivalent to condition 5, observe that if condition 5
holds, then for all x ∈X, we have χF
C(s, t) ∈[s]X = x for all s ∈x and t ∈S. Thus
χF
C(x, S) ⊆x. Conversely, if condition 3 holds, χF
C(s, t) ∈χF
C([s]X, S) ⊆[s]X for all
s, t ∈S.
Condition 6 is clearly a trivial restatement of condition 5.
To see that conditions 6 and 7 are equivalent, observe that if condition 6 holds,
and s, t ∈S satisfy s ∼W
S(C) t, then χF
C(s, t) = t, so t = χF
C(s, t) ∼X s. Thus
X ≤S
W
S(C). Conversely, if condition 7 holds, then since χF
C(s, t) ∼W
S(C) s for all
s, t ∈S, we have χF
C(s, t) ∼X s.
Here are some basic properties of ⊢F .
Proposition 11. Let F = (S, B) be a ﬁnite factored set, let C and D be subsets of
B, and let X, Y ∈Part(S) be partitions of S.
1. If X ≤S Y and C ⊢F Y , then C ⊢F X.
2. If C ⊢F X and C ⊢F Y , then C ⊢F X ∨S Y .
3. B ⊢F X.
4. {} ⊢F X if and only if X = IndS.
5. If C ⊆D and C ⊢F X, then D ⊢F X.
6. If C ⊢F X and D ⊢F X, then C ∩D ⊢F X.
Proof. For the ﬁrst 5 parts, we will use the equivalent deﬁnition from Proposition
10 that C ⊢F X if and only if X ≤S
W
S(C).
Then 1 follows directly from the transitivity of ≤S.
2 follows directly from the fact that any partition Z satisﬁes X ∨S Y ≤Z if and
only if X ≤Z and Y ≤Z.
3 follows directly from the fact that W
S(B) = DisS by Proposition 3.
4 follows directly from the fact that W
S({}) = IndS, together with the fact that
X ≤S IndS if and only if X = IndS.
5 follows directly from the fact that if C ⊆D, then W
S(C) ≤W
S(D).
Finally, we need to prove part 6. For this, we will use the equivalent deﬁnition
from Proposition 10 that C ⊢F X if and only if χF
C(s, t) ∼X s for all s, t ∈S.
Assume that for all s, t ∈S, χF
C(s, t) ∼X s and χF
D(s, t) ∼X s. Thus, for all s, t ∈S,
χF
C∩D(s, t) = χF
C(χF
D(s, t), t) ∼X χF
D(s, t) ∼X s. Thus C ∩D ⊢F X.
Our main use of ⊢F will be in the deﬁnition of the history of a partition.
9

3.2
History
Deﬁnition 17 (history of a partition). Given a ﬁnite factored set F = (S, B) and
a partition X ∈Part(S), let hF (X) denote the smallest (according to the subset
ordering) subset of B such that hF (X) ⊢F X.
The history of X, then, is the smallest set of factors C ⊆B such that if you’re
trying to ﬁgure out which part in X any given s ∈S is in, it suﬃces to know what
part s is in within each of the factors in C. We can informally think of hF (X) as
the smallest amount of information needed to compute X.
Proposition 12. Given a ﬁnite factored set F = (S, B), and a partition X ∈
Part(S), hF (X) is well-deﬁned.
Proof. Fix a ﬁnite factored set F = (S, B) and a partition X ∈Part(S), and let
hF (X) be the intersection of all C ⊆B such that C ⊢F X. It suﬃces to show
that hF (X) ⊢F X; then hF (X) will clearly be the unique smallest (according to
the subset ordering) subset of B such that hF (X) ⊢F X.
Note that hF (X) is a ﬁnite intersection, since there are only ﬁnitely many subsets
of B, and that hF (X) is an intersection of a nonempty collection of sets since
B ⊢F X. Thus, we can express hF (X) as a composition of ﬁnitely many binary
intersections.
By part 6 of Proposition 11, the intersection of two subsets that
generate X also generates X. Thus hF (X) ⊢F X.
Here are some basic properties of history.
Proposition 13. Let F = (S, B) be a ﬁnite factored set, and let X, Y ∈Part(S)
be partitions of S.
1. If X ≤S Y , then hF (X) ⊆hF (Y ).
2. hF (X ∨S Y ) = hF (X) ∪hF (Y ).
3. hF (X) = {} if and only if X = IndS.
4. If S is nonempty, then hF (b) = {b} for all b ∈B.
Proof. The ﬁrst 3 parts are trivial consequences of history’s deﬁnition and Proposi-
tion 11.
For the fourth part, observe that {b} ⊢F b by condition 7 of Proposition 10, b is
nontrivial, and since S is nonempty b is nonempty, so we have ¬({} ⊢F b) by part
4 of Proposition 11. Thus {b} is the smallest subset of B that generates b.
3.3
Orthogonality
We are now ready to deﬁne the notion of orthogonality between two partitions of
S.
Deﬁnition 18 (orthogonality). Given a ﬁnite factored set F = (S, B) and parti-
tions X, Y ∈Part(S), we say X is orthogonal to Y (in F), written X ⊥F Y , if
hF (X) ∩hF (Y ) = {}.
If ¬(X ⊥F Y ), we say X is entangled with Y (in F).
We could also unpack this deﬁnition to not mention history or chimera functions.
Proposition 14. Given a ﬁnite factored set F = (S, B), and partitions X, Y ∈
Part(S), X ⊥F Y if and only if there exists a C ⊆B such that X ≤S
W
S(C) and
Y ≤S
W
S(B \ C).
Proof. If there exists a C ⊆B such that X ≤S
W
S(C) and Y ≤S
W
S(B \ C),
then C ⊢F X and B \ C ⊢F Y .
Thus, hF (X) ⊆C and hF (Y ) ⊆B \ C, so
hF (X) ∩hF (Y ) = {}.
Conversely, if hF (X) ∩hF (Y ) = {}, let C = hF (X). Then C ⊢F X, so X ≤S
W
S(C), and B \ C ⊇hF (Y ), so B \ C ⊢F Y , so Y ≤S
W
S(B \ C).
10

Here are some basic properties of orthogonality.
Proposition 15. Let F = (S, B) be a ﬁnite factored set, and let X, Y, Z ∈Part(S)
be partitions of S.
1. If X ⊥F Y , then Y ⊥F X.
2. If X ⊥F Z and Y ≤S X, then Y ⊥F Z.
3. If X ⊥F Z and Y ⊥F Z, then (X ∨S Y ) ⊥F Z.
4. X ⊥F X if and only if X = IndS.
Proof. Part 1 is trivial from the symmetry in the deﬁnition.
Parts 2, 3, and 4 follow directly from Proposition 13.
3.4
Time
Finally, we can deﬁne our notion of time in a factored set.
Deﬁnition 19 ((strictly) before). Given a ﬁnite factored set F = (S, B), and
partitions X, Y ∈Part(S), we say X is before Y (in F), written X ≤F Y , if
hF (X) ⊆hF (Y ).
We say X is strictly before Y (in F), written X <F Y , if hF (X) ⊂hF (Y ).
Again, we could also unpack this deﬁnition to not mention history or chimera
functions.
Proposition 16. Given a ﬁnite factored set F = (S, B), and partitions X, Y ∈
Part(S), X ≤F Y if and only if every C ⊆B satisfying Y ≤S
W
S(C) also satisﬁes
X ≤S
W
S(C).
Proof. Note that by part 7 of Proposition 10, part 5 of Proposition 11, and the
deﬁnition of history, C satisﬁes Y ≤S
W
S(C) if and only if C ⊇hF (Y ), and
similarly for X.
Clearly, if hF (Y ) ⊇hF (X), every C ⊇hF (Y ) satisﬁes C ⊇hF (X). Conversely,
if hF (X) is not a subset of hF (Y ), then we can take C = hF (Y ), and observe that
C ⊇hF (Y ) but not C ⊇hF (X).
Interestingly, we can also deﬁne time entirely as a closure property of orthogo-
nality. We hold that the philosophical interpretation of time as a closure property
on orthogonality is natural and transcends the ontology set up in this paper.
Proposition 17. Given a ﬁnite factored set F = (S, B), and partitions X, Y ∈
Part(S), X ≤F Y if and only if every Z ∈Part(S) satisfying Y ⊥F Z also satisﬁes
X ⊥F Z.
Proof. Clearly if hF (X) ⊆hF (Y ), then every Z satisfying hF (Y )∩hF (Z) = {} also
satisﬁes hF (X) ∩hF (Z) = {}.
Conversely, if hF (X) is not a subset of hF (Y ), let b ∈B be an element of
hF (X) that is not in hF (Y ). Assuming S is nonempty, b is nonempty, so we have
hF (b) = {b}, so Y ⊥F b, but not X ⊥F b. On the other hand, if S is empty, then
X = Y = {}, so clearly X ≤F Y .
Here are some basic properties of time.
Proposition 18. Let F = (S, B) be a ﬁnite factored set, and let X, Y, Z ∈Part(S)
be partitions of S.
1. X ≤F X.
2. If X ≤F Y and Y ≤F Z, then X ≤F Z.
3. If X ≤S Y , then X ≤F Y .
11

4. If X ≤F Z and Y ≤F Z, then (X ∨S Y ) ≤F Z.
Proof. Part 1 is trivial from the deﬁnition.
Part 2 is trivial by transitivity of the subset relation.
Part 3 follows directly from part 1 of Proposition 13.
Part 4 follows directly from part 2 of Proposition 13.
Finally, note that we can (circularly) redeﬁne history in terms of time, thus
partially justifying the names.
Proposition 19. Given a nonempty ﬁnite factored set F = (S, B) and a partition
X ∈Part(S), hF (X) = {b ∈B | b ≤F X}.
Proof. Since S is nonempty, part 4 of Proposition 13 says that hF (b) = {b} for all
b ∈B. Thus {b ∈B | b ≤F X} = {b ∈B | {b} ⊆hF (X)} = {b ∈B | b ∈hF (X)} =
hF (X).
4
Subpartitions and Conditional Orthogonality
We now want to extend our notion of orthogonality to conditional orthogonality.
This will take a bit of work. In particular, we will have to ﬁrst extend our notions
of partition generation and history to be deﬁned on partitions of subsets of S.
4.1
Generating a Subpartition
Deﬁnition 20 (subpartition). A subpartition of a set S is a partition of a subset
of S. Let SubPart(S) = S
E⊆S Part(E) denote the set of all subpartitions of S.
Deﬁnition 21 (domain). The domain of a subpartition X of S, written dom(X),
is the unique E ⊆S such that X ∈Part(E).
Deﬁnition 22 (restricted partitions). Given sets S and E and a partition X of S,
let X|E denote the partition of S ∩E given by X|E = {[e]X ∩E | e ∈E}.
Deﬁnition 23 (generating a subpartition). Given a ﬁnite factored set F = (S, B),
and X ∈SubPart(S), and a C ⊆B, we say C generates X (in F), written C ⊢F X,
if χF
C(x, dom(X)) = x for all x ∈X.
Note that this deﬁnition clearly coincides with Deﬁnition 16, when X has domain
S. Despite the similarity of the deﬁnitions, the idea of generating a subpartition is
a bit more complicated than the idea of generating a partition of S.
To see this, consider the following list of equivalent deﬁnitions. Notice that while
the ﬁrst ﬁve directly mirror their counterparts in Proposition 10, the last two (and
especially the last one) require an extra condition.
Proposition 20. Let F = (S, B) be a ﬁnite factored set, let X ∈SubPart(S) be a
subpartition of S, let E = dom(X) be the domain of X, and let C be a subset of B.
The following are equivalent.
1. C ⊢F X.
2. χF
C(x, E) = x for all x ∈X.
3. χF
C(x, E) ⊆x for all x ∈X.
4. χF
C(x, y) ⊆x for all x, y ∈X.
5. χF
C(s, t) ∈[s]X for all s, t ∈E.
6. χF
C(s, t) ∈E and χF
C(s, t) ∼X s for all s, t ∈E.
7. X ≤E (W
S(C)|E) and χF
C(E, E) = E.
12

Proof. The equivalence of conditions 1 and 2 is by deﬁnition.
The equivalence of conditions 2 and 3 follows directly from the fact that
χF
C(s, s) = s for all s ∈x, so χF
C(x, E) ⊇χF
C(x, x) ⊇x.
To see that conditions 3 and 4 are equivalent, observe that since E = S
y∈X y,
χF
C(x, E) = S
y∈X χF
C(x, y). Thus, if χF
C(x, E) ⊆x, χF
C(x, y) ⊆x for all y ∈X, and
conversely if χF
C(x, y) ⊆x for all y ∈X, then χF
C(x, E) ⊆x.
To see that condition 3 is equivalent to condition 5, observe that if condition 5
holds, then for all x ∈X, we have χF
C(s, t) ∈[s]X = x for all s ∈x and t ∈E. Thus
χF
C(x, E) ⊆x. Conversely, if condition 3 holds, χF
C(s, t) ∈χF
C([s]X, E) ⊆[s]X for
all s, t ∈E.
Condition 6 is clearly a trivial restatement of condition 5.
To see that conditions 6 and 7 are equivalent, observe that if condition 6 holds,
then χF
C(s, t) ∈E for all s, t ∈E, so χF
C(E, E) ⊆E, so χF
C(E, E) = E. Further,
if s, t ∈E satisfy s ∼W
S(C)|E t, then s ∼c t for all c ∈C, so χF
C(s, t) = t, so
t = χF
C(s, t) ∼X s. Thus X ≤E
W
S(C)|E.
Conversely, if condition 7 holds, then for all s, t ∈E, we have χF
C(s, t) ∼W
S(C) s,
so χF
C(s, t) ∼W
S(C)|E s, and thus χF
C(s, t) ∼X s. Further, clearly χF
C(E, E) = E
implies χF
C(s, t) ∈E for all s, t ∈E.
The ﬁrst half of condition 7 in the above proposition can be thought of as saying
that the values of factors in C are suﬃcient to distinguish between the parts of X.
The second half can be thought of as saying that no factors in C become en-
tangled with any factors outside of C when conditioning on E. This second half
is actually necessary (for example) to ensure that the set of all C that generate X
is closed under intersection. As such, we will need this fact in order to extend our
notion of history to arbitrary subpartitions.
Proposition 21. Let F = (S, B) be a ﬁnite factored set, let C and D be subsets of
B, let X, Y, Z ∈SubPart(S) be subpartitions of S, and let dom(X) = dom(Y ) = E.
1. If X ≤E Y and C ⊢F Y , then C ⊢F X.
2. If C ⊢F X and C ⊢F Y , then C ⊢F X ∨E Y .
3. B ⊢F X.
4. {} ⊢F X if and only if X = IndE.
5. If C ⊢F X and D ⊢F X, then C ∩D ⊢F X and C ∪D ⊢F X.
6. If X ⊆Z, and C ⊢F Z, then C ⊢F X.
Proof. The ﬁrst 4 parts will use the equivalent deﬁnition from Proposition 20 that
C ⊢F X if and only if X ≤S
W
S(C). 1 and 2 are immediate from this deﬁnition.
3 follows directly from Deﬁnition 23.
4 follows directly from the fact that W
S({}) = IndS, and IndS|E = IndE so
X ≤E
W
S(C)|E if and only if X = IndE.
For part 5, we will use the equivalent deﬁnition from Proposition 20 that C ⊢F X
if and only if χF
C(s, t) ∈[s]X for all s, t ∈E. Assume that for all s, t ∈E, χF
C(s, t) ∈
[s]X and χF
D(s, t) ∈[s]X. Thus, for all s, t ∈E, χF
C∩D(s, t) = χF
C(χF
D(s, t), t) ∈
[χF
D(s, t)]X = [s]X. Similarly, for all s, t ∈E, χF
C∪D(s, t) = χF
C(s, χF
D(s, t)) ∈[s]X.
Thus C ∩D ⊢F X and C ∪D ⊢F X.
For part 6, we use the deﬁnition that C ⊢F X if and only if χF
C(x, y) ∈x for all
x, y ∈X. Clearly if X ⊆Z, and χF
C(x, y) ∈x for all x, y ∈Z, then χF
C(x, y) ∈x for
all x, y ∈X.
Note that while the set of C that generate an X ∈Part(S) is closed under
supersets, the set of C that generate an X ∈SubPart(S) is merely closed under
union.
Further note that part 6 of Proposition 21 uses the subset relation on
subpartitions, which is a slightly unnatural relation.
13

4.2
History of a Subpartition
Deﬁnition 24 (history of a subpartition). Given a ﬁnite factored set F = (S, B)
and a subpartition X ∈SubPart(S), let hF (X) denote the smallest (according to
the subset ordering) subset of B such that hF (X) ⊢F X.
Proposition 22. Given a ﬁnite factored set F = (S, B), hF : SubPart(S) →P(B)
is well-deﬁned, and if X is a partition of S, this deﬁnition coincides with Deﬁnition
17.
Proof. Fix a ﬁnite factored set F = (S, B) and a subpartition X ∈SubPart(S), and
let hF (X) be the intersection of all C ⊆B such that C ⊢F X. It suﬃces to show
that hF (X) ⊢F X.
Then hF (X) will clearly be the unique smallest (according
to the subset ordering) subset of B such that hF (X) ⊢F X. The fact that this
deﬁnition coincides with Deﬁnition 17 if X ∈Part(S) is clear.
Note that hF (X) is a ﬁnite intersection, since there are only ﬁnitely many sub-
sets of B, and that hF (X) is a nonempty intersection since B ⊢F X. Thus, we can
express hF (X) as a (possibly empty) composition of ﬁnitely many binary intersec-
tions. By part 5 of Proposition 21, the intersection of two subsets that generate X
also generates X. Thus hF (X) ⊢F X.
We will now give ﬁve basic properties of the history of subpartitions, followed
by two more properties that are less basic.
Proposition 23. Let F = (S, B) be a ﬁnite factored set, let X, Y, Z ∈SubPart(S)
be subpartitions of S, and let dom(X) = dom(Y ) = E.
1. If X ≤E Y , then hF (X) ⊆hY (Y ).
2. hF (X ∨E Y ) = hF (X) ∪hF (Y ).
3. If X ⊆Z, then hF (X) ⊆hF (Z).
4. hF (X) = {} if and only if X = IndE.
5. If S is nonempty, then hF (b) = {b} for all b ∈B.
Proof. Parts 1, 3, and 4 are trivial consequences of Proposition 21, and part 5 is
just a restatement of part 4 of Proposition 13.
For part 2, ﬁrst observe that hF (X ∨E Y ) ⊇hF (X) ∪hF (Y ), by part 1 of
Proposition 21. Thus it suﬃces to show that hF (X) ∪hF (Y ) ⊇hF (X ∨E Y ), by
showing that hF (X) ∪hF (Y ) ⊢F X ∨E Y .
We will use condition 7 in Proposition 20. Clearly
X ≤E (
_
E(hF (X))|E)
≤E (
_
S(hF (X) ∪hF (Y ))|E),
(1)
and similarly,
Y ≤E (
_
E(hF (Y ))|E)
≤E (
_
S(hF (X) ∪hF (Y ))|E).
(2)
Thus, X ∨E Y ≤E (W
S(hF (X) ∪hF (Y ))|E).
Next, we need to show that χF
hF (X)∪hF (Y )(E, E)
=
E.
Clearly E
⊆
χF
hF (X)∪hF (Y )(E, E).
Let s and t be elements of E, and observe that χF
hF (X)∪hF (Y )(s, t)
=
χF
hF (X)(s, χF
hF (Y )(s, t)). We have that χF
hF (Y )(s, t) ∈E, since χF
hF (Y )(E, E) = E.
Thus, we also have that χF
hF (X)(s, χF
hF (Y )(s, t)) ∈E, since χF
hF (X)(E, E) = E. Thus,
χF
hF (X)∪hF (Y )(E, E) ⊆E.
14

Thus
we
have
that
X ∨E Y
≤E
(W
S(hF (X) ∪hF (Y ))|E)
and
χF
hF (X)∪hF (Y )(E, E) = E.
Thus, by condition 7 in Proposition 20, hF (X) ∪
hF (Y ) ⊢F X ∨E Y , so hF (X ∨E Y ) = hF (X) ∪hF (Y ).
Lemma 1. Let F = (S, B) be a ﬁnite factored set, and let X, Y ∈Part(E) be
subpartitions of S with the same domain. If hF (X) ∩hF (Y ) = {}, then hF (X) =
hF (X|y) for all y ∈Y .
Proof. Let F = (S, B) be a ﬁnite factored set, let E ⊆S, and let X, Y ∈Part(E).
We start by showing that (B\hF (X)) ⊢F Y and (B\hF (Y )) ⊢F X. Observe that
χB\hF (X)(E, E) = χhF (X)(E, E) = E. Further observe that B\hF(X) ⊇hF (Y ), so
W
S(B \ hF (X)) ≥S
W
S(hF (Y )), so (W
S(B \ hF (X))|E) ≥E (W
S(hF (Y ))|E) ≥E Y .
Thus, (B \ hF (X)) ⊢F Y . Symmetrically, (B \ hF (Y )) ⊢F X.
Fix some y ∈Y . We start by showing that hF (X) ⊇hF (X|y).
We have that χF
B\hF (X)(y, E) ⊆y, so χF
hF (X)(E, y) ⊆y, so for all x ∈X, we
have χF
hF (X)(x∩y, y) ⊆y. We also have χF
hF (X)(x∩y, y) ⊆χF
hF (X)(x, E) ⊆x. Thus
χF
hF (X)(x ∩y, y) ⊆x ∩y. Every element of X|y is of the form x ∩y for some x ∈X,
so we have hF (X) ⊢F (X|y), so hF (X) ⊇hF (X|y).
Next, we need to show that hF (X) ⊆hF (X|y). For this, it suﬃces to show
that hF (X|y) ⊢F X. Let s, t be arbitrary elements of E. It suﬃces to show that
χF
hF (X|y)(s, t) ∈[s]X.
First, observe that since (B \ hF (Y )) ⊇hF (X) ⊇hF (X|y), we have that
χF
hF (X|y)(s, t) = χF
B\hF (Y )(χF
hF (X|y)(s, t), t).
Let r be an arbitrary element of y. We thus have:
χF
hF (X|y)(s, t) = χF
B\hF (Y )(χF
hF (X|y)(s, t), t)
= χF
B\hF (Y )(χF
hF (Y )(r, χF
hF (X|y)(s, t)), t)
= χF
B\hF (Y )(χF
hF (X|y)(χF
hF (Y )(r, s), χF
hF (Y )(r, t)), t).
(3)
Let s′ = χF
hF (X|y)(χF
hF (Y )(r, s), χF
hF (Y )(r, t)). Note that χF
hF (Y )(r, t) and χF
hF (Y )(r, s)
are both in y. Thus we have that s′ ∈[χF
hF (Y )(r, s)](X|y). Since (B \ hF (Y )) ⊢F X,
χF
hF (Y )(r, s) = χF
B\hF (Y )(s, r) ∈[s]X. Thus [χF
hF (Y )(r, s)](X|y) ⊆[χF
hF (Y )(r, s)]X =
[s]X, so s′ ∈[s]X.
We have that χF
hF (X|y)(s, t) = χF
B\hF (Y )(s′, t). However, since B \ hF (Y ) ⊢F X,
we have χF
B\hF (Y )(s′, t) ∈[s′]X = [s]X. Thus, hF (X) ⊆hF (X|y), so hF (X) =
hF (X|y).
Lemma 2. Let F = (S, B) be a ﬁnite factored set. Let E ⊆S and let X, Y ∈
Part(E) be subpartitions of S with the same domain. Then hF (X ∨E Y ) = hF (X)∪
S
x∈X hF (Y |x).
Proof. Since X ≤E X ∨E Y , we have hF (X) ⊆hF (X ∨E Y ). Similarly, for all x ∈X,
since Y |x ⊆X∨EY , we have hF (Y |x) ⊆hF (X∨EY ). Thus, hF (X∨EY ) ⊇hF (X)∪
S
x∈X hF (Y |x). We still need to show that hF (X ∨E Y ) ⊆hF (X) ∪S
x∈X hF (Y |x).
We start with the special case where |X| = 2. Let X = {x0, x1}. In this case, we
want to show that hF (X ∨E Y ) = hF (X) ∪hF (Y |x0) ∪hF (Y |x0). Let C = hF (X),
let C0 = hF (Y |x0), and let C1 = hF (Y |x1).
Consider arbitrary s, t ∈E. Without loss of generality, assume that s ∈x0, and
let y = [s]Y . It suﬃces to show that χF
C∪C0∪C1(s, t) ∈x0 ∩y. Fix some r ∈x1.
χF
C∪C0∪C1(s, t) = χF
C0(s, χF
C(s, χF
C1(s, t)))
= χF
C0(s, χF
C(s, χF
C(r, χF
C1(s, t))))
= χF
C0(s, χF
C(s, χF
C1(χF
C(r, s), χF
C(r, t)))).
(4)
15

Observe that χF
C(r, s) and χF
C(r, t) are both in x1, so χF
C1(χF
C(r, s), χF
C(r, t)) ∈x1,
and thus is in E.
Combining this with the fact that s ∈x0 gives us that
χF
C(s, χF
C1(χF
C(r, s), χF
C(r, t))) ∈x0. Thus, since s ∈x0 ∩y, χF
C∪C0∪C1(s, t) =
χF
C0(s, χF
C(s, χF
C1(χF
C(r, s), χF
C(r, t)))) ∈x0 ∩y.
Now, consider the case where |X| ̸= 2. If |X| = 0, then E = {}, so all subparti-
tions involved are empty, and thus have the same (empty) history. If |X| = 1, let
X = {E}. Then
hF (X ∨E Y ) = hF (Y )
= hF (Y |E) ⊆hF (X) ∪hF (Y |E)
= hF (X) ∪
[
x∈X
hF (Y |x).
(5)
Thus, we can restrict our attention to the case where |X| ≥3.
Observe that X ∨E Y = W
E({(Y |x) ∪{E \ x} | x ∈X}). Thus hF (X ∨E Y ) =
S
x∈X hF ((Y |x) ∪{E \ x}). However, from the case where |X| = 2, we have
hF ((Y |x) ∪{E \ x}) = hF ({x, E \ x} ∨E ((Y |x) ∪{E \ x}))
= hF ({x, E \ x}) ∪hF ({E \ x}) ∪hF (Y |x).
(6)
hF ({E\x}) is empty, so this gives us that hF (X∨EY ) = S
x∈X(hF (Y |x)∪hF ({x, E\
x})). Since W
E({{x, E \ x} | x ∈X}) = X, S
x∈X hF ({x, E \ x}) = hF (X), so we
have hF (X ∨E Y ) = hF (X) ∪S
x∈X hF (Y |x).
4.3
Conditional Orthogonality
We can also extend our notions of orthogonality and time to subpartitions.
Deﬁnition 25. Let F = (S, B) be a ﬁnite factored set. Let X, Y ∈SubPart(S) be
subpartitions of S. We write X ⊥F Y if hF (X) ∩hF (Y ) = {}, we write X ≤F Y if
hF (X) ⊆hF (Y ), and we write X <F Y if hF (X) ⊂hF (Y ).
We give this deﬁnition in general, but it is not clear whether orthogonality
and time should be considered philosophically meaningful when the domains of
the inputs diﬀer from each other. Further, the temporal structure of subpartitions
will mostly be outside the scope of this paper, and the orthogonality structure on
subpartitions will mostly just be used for the following pair of deﬁnitions.
Deﬁnition 26 (conditional orthogonality given a subset). Given a ﬁnite factored
set F = (S, B), partitions X, Y ∈Part(S), and E ⊆S, we say X and Y are
orthogonal given E (in F), written X ⊥F Y | E, if (X|E) ⊥F (Y |E).
Deﬁnition 27 (conditional orthogonality). Given a ﬁnite factored set F = (S, B),
and partitions X, Y, Z ∈Part(S), if X ⊥F Y | z for all z ∈Z, then we say X and
Y are orthogonal given Z (in F), written X ⊥F Y | Z.
Unconditioned orthogonality can be thought of as a special case of conditional
orthogonality, where you condition on the indiscrete partition.
Proposition 24. Given a ﬁnite factored set F = (S, B) and partitions X, Y ∈
Part(S), X ⊥F Y if and only if X ⊥F Y | IndS.
Proof. If S = {}, then there is only one partition X = {}, and X ⊥F X holds. Also,
since IndS is empty, X ⊥F X | IndS holds vacuously.
If S ̸= {}, then IndS = {S}, so X ⊥F Y | IndS if and only if X ⊥F Y | S if and
only if X|S ⊥F Y |S if and only if X ⊥F Y .
The primary combinatorial structure of ﬁnite factored sets that we will be in-
terested in is the structure of orthogonality (X ⊥F Y ), conditional orthogonality
(X ⊥F Y | Z), and time (X ≤F Y and X <F Y ) on inputs that are partitions.
We now will show that conditional orthogonality satisﬁes (a slight modiﬁcation
of) the axioms for a compositional semigraphoid.
16

Theorem 2. Let F = (S, B) be a ﬁnite factored set, and let X, Y, Z, W ∈Part(S)
be partitions of S.
1. If X ⊥F Y | Z, then Y ⊥F X | Z. (symmetry)
2. If X ⊥F (Y ∨S W) | Z, then X ⊥F Y | Z and X ⊥F W | Z. (decomposition)
3. If X ⊥F (Y ∨S W) | Z, then X ⊥F Y | (Z ∨S W). (weak union)
4. If X ⊥F Y | Z and X ⊥F W | (Z ∨S Y ), then X ⊥F (Y ∨S W) | Z. (contraction)
5. If X ⊥F Y | Z and X ⊥F W | Z, then X ⊥F (Y ∨S W) | Z. (composition)
Proof. Symmetry is clear from the deﬁnition.
Decomposition and composition both follow directly from the fact that for all
z ∈Z, hF ((Y ∨S W)|z) = hF ((Y |z) ∨z (W|z)) = hF (Y |z) ∪hF (W|z).
For weak union, assume that X ⊥F (Y ∨S W) | Z. Thus, for all z ∈Z, hF (X|z)∩
hF ((Y ∨S W)|z) = {}.
In particular, this means that hF (X|z) ∩hF (W|z) = {}, so by Lemma 1, for all
w ∈W, hF (X|z) = hF (X|w ∩z).
Further, we have that for all w ∈W, hF (Y |w ∩z) ⊆hF (Y ∨S W|z). Thus, for
all w ∈W, hF (X|w ∩z) ∩hF (Y |w ∩z) = {}, which since every element of W ∨S Z
is of the form w ∩z for some w ∈W and z ∈Z, means that X ⊥F Y | (Z ∨S W).
Finally, for contraction, assume that X ⊥F Y | Z and X ⊥F W | Z ∨S Y .
Fix some z ∈Z. We want to show that hF (X|z)∩hF ((Y ∨SW)|z) = {}. We have
that hF ((Y ∨SW)|z) = hF ((Y |z)∨z(W|z)), and by Lemma 2, hF ((Y |z)∨z(W|z)) =
hF (Y |z)∪S
y∈Y hF (W|(y ∩z)). Thus, it suﬃces to show that hF (X|z)∩hF(Y |z) =
{} and hF (X|z) ∩hF (W|(y ∩z)) = {} for all y ∈Y .
The fact that hF (X|z) ∩hF (Y |z) = {} follows directly from X ⊥F Y | Z.
Fix a y ∈Y . If y∩z = {}, then hF (W|(y∩z)) = {}, so hF (X|z)∩hF(W|(y∩z)) =
{}.
Otherwise, we have hF (X|z) = hF (X|(y ∩z)) by Lemma 1, and we have that
hF (X|(y ∩z)) ∩hF(W|(y ∩z)) = {}, since X ⊥F W | Z ∨S Y , so we have hF (X|z) ∩
hF (W|(y ∩z)) = {}.
Thus, X ⊥F (Y ∨S W) | Z.
The ﬁrst four parts of Theorem 2 are essentially the semigraphoid axioms. The
diﬀerence is that the semigraphoid axioms are normally deﬁned as a ternary rela-
tion on disjoint sets of variables. We use partitions instead of sets of variables, use
common reﬁnement instead of union, and have no need for the disjointness condi-
tion. The ﬁfth part (composition) is a converse to the decomposition axiom that is
sometimes added to deﬁne a compositional semigraphoid.
The results in this paper will not depend on the theory of compositional semi-
graphoids, so we will not need to make the analogy any more explicit, but it is nice
to note the similarity to existing well-studied structures.
We also get a nice relationship between conditional orthogonality and the reﬁne-
ment order.
Proposition 25. Let F = (S, B) be a ﬁnite factored set, and let X, Y ∈Part(S)
be partitions of S. X ⊥F X | Y if and only if X ≤S Y .
Proof. If X ⊥F X | Y , then for all y ∈Y , hF (X|y) = {}, so X|y = indy, so for all
s, t ∈y, we have s ∼X|y t, and thus s ∼X t. Thus, for all s, t ∈S, if s ∼Y t, then
s ∼X t. Thus X ≤S Y .
Conversely, if X ≤S Y , observe that for all y ∈Y , X|y = indy, so hF (X|y) = {}.
Thus, X ⊥F X | Y .
17

5
Polynomials and Probability
In this section, given a ﬁnite factored set F = (S, B), we will show how to associate
each E ⊆S with a characteristic polynomial, QF
E. We will discuss how to factor
these characteristic polynomials, and use these characteristic polynomials to build
up to the fundamental theorem of ﬁnite factored sets, which associates conditional
orthogonality with conditional independence in probability distributions.
5.1
Characteristic Polynomials
Deﬁnition 28. Given a ﬁnite factored set F = (S, B), let PolyF denote the ring
of polynomials with coeﬃcients in R and variables in P(S).
Deﬁnition 29. Given a ﬁnite factored set F = (S, B), a p ∈PolyF , and an
f : P(S) →R, we write p(f) ∈R for the evaluation of p at f, computed by
replacing each E ⊆S with f(E).
Deﬁnition 30. Given a ﬁnite factored set F = (S, B) and a polynomial p ∈PolyF ,
supp(p) ⊆P(S) denotes the set of all variables v ∈P(S) that appear in p. supp(p)
is called the support of p.
Deﬁnition 31. Given a ﬁnite factored set F = (S, B), and an E ⊆S, let QF
E ∈
PolyF be given by QF
E = P
s∈E
Q
b∈B[s]b. QF
E is called the characteristic polynomial
of E (in F).
We will be building up to an understanding of how to factor QF
E into irreducibles.
For that, we will ﬁrst need to give some basic notation for manipulating polynomials
in PolyF .
Deﬁnition 32. Given a ﬁnite factored set F = (S, B), an s ∈S, and a C ⊆B, let
monoF
C(s) ∈PolyF be given by monoF
C(s) = Q
b∈C[s]b.
Deﬁnition 33. Given a ﬁnite factored set F = (S, B), an E ⊆S, and a C ⊆B,
let monosF
C(E) ∈P(PolyF ) be given by monosF
C(E) = {monoF
C(s) | s ∈E}.
Deﬁnition 34. Given a ﬁnite factored set F = (S, B), an E ⊆S, and a C ⊆B,
let polyF
C(E) ∈PolyF be given by polyF
C(E) = P
m∈monosF
C(E) m.
Proposition 26. Let F = (S, B) be a ﬁnite factored set, and let E ⊆S. Then
QF
E = polyF
B(E).
Proof. We start by showing that for all s ̸= t ∈S, monoF
B(s) ̸= monoF
B(t).
Let s ̸= t ∈S be arbitrary. By Proposition 3, if s ̸= t, there must be some b ∈B
such that [s]b ̸= [t]b. Then, note that [s]b ∈supp(monoF
B(s)). If [s]b were also in
supp(monoF
B(t)), then t would be in both [s]b and [t]b, contradicting the fact that
these two sets are disjoint. Therefore monoF
B(s) ̸= monoF
B(t).
Thus monosF
B(E) has exactly one element for each element of E, so we have that
P
m∈monosF
B(E) m = P
s∈E monoF
B(s) = QF
E.
Proposition 27. Let F = (S, B) be a ﬁnite factored set, and let E0, E1 ⊆S be
subsets of S. Let C0, C1 ⊆B be disjoint subsets of B. Let E2 = χF
C0(E0, E1), and
let C2 = C0 ∪C1. Then polyF
C2(E2) = polyF
C0(E0) · polyF
C1(E1).
Proof. For i ∈{0, 1, 2}, let Mi = monosF
Ci(Ei).
We will start by showing that
f : M0 × M1 →M2, given by f(m0, m1) = m0m1, is a well-deﬁned function and a
bijection.
First, observe that it follows immediately from the deﬁnition that for all s0, s1 ∈
S, if s2 = χF
C0(s0, s1) we have that monoF
C0(s0) = monoF
C0(s2), monoF
C1(s1) =
18

monoF
C1(s2), and monoF
C0(s2) · monoF
C1(s2) = monoF
C2(s2). Combining these, we get
that monoF
C0(s0) · monoF
C1(s1) = monoF
C2(χF
C0(s0, s1)).
For all (m0, m1) ∈M0 × M1, there exists some s0 ∈E0 such that m0 =
monoF
C0(s0), and some s1 ∈E1 such that m1 = monoF
C1(s1), and this gives us
that m0m1 = monoF
C0(s0)monoF
C1(s1) = monoF
C2(χF
C(s0, s1)) ∈M2.
Thus, f is
well-deﬁned.
To see that f is surjective, observe that for all m2 ∈M2, there exists an s2 ∈E2
such that m2 = monoF
C2(s2), and there exist s0 ∈E0 and s1 ∈E1 such that
s2 = χF
C(s0, s1), and we have f(monoF
C0(s0), monoF
C1(s1)) = m2.
To see that f is injective, observe that for i ∈{0, 1}, for all mi ∈Mi, supp(mi) ⊆
S
b∈Ci b. Further, S
b∈C0 b and S
b∈C1 b are disjoint. Thus, for all m0 ∈M0 and
m1 ∈M1, supp(mi) = supp(m0m1) ∩S
b∈Ci b.
This means that for all m0, m′
0 ∈M0 and m1, m′
1 ∈M1, if m0m1 = m′
0m′
1,
then supp(m0) = supp(m′
0) and supp(m1) = supp(m′
1). However, every monomial
in M0 or M1 is just equal to the product of all variables in its support.
Thus
m0 = Q
v∈supp(m0) v = m′
0 and m1 = Q
v∈supp(m1) v = m′
1. Thus f is injective, and
thus a bijection between M0 × M1 and M2.
Now, we have that
polyF
C0(E0) · polyF
C1(E1) =
 X
m0∈M0
m0
!  X
m1∈M1
m1
!
=
X
m0∈M0
X
m1∈M1
m0m1
=
X
(m0,m1)∈M0×M1
m0m1
=
X
(m0,m1)∈M0×M1
f(m0, m1)
=
X
m2∈M2
m2
= polyF
C2(E2).
(7)
Proposition 28. Let F = (S, B) be a ﬁnite factored set, and let E be a nonempty
subset of S. If p divides QF
E, then p = r · polyF
C(E), for some r ∈R and C ⊆B.
Proof. Let F = (S, B) be a ﬁnite factored set, and let E be a nonempty subset
of S. Let p, q ∈PolyF satisfy pq = QF
E. We thus must have supp(p) ∪supp(q) =
supp(QF
E).
If there were some T ∈supp(p) ∩supp(q), then the degree of T in QF
E would
be at least 2, contradicting the deﬁnition of QF
E and Corollary 1. Thus, supp(p) ∩
supp(q) = {}.
There can be no combining like terms, then, in the product pq. The monomial
terms in QF
E are in bijective correspondence to the pairs of monomial terms in p
and monomial terms in q.
In particular, this means that since all the coeﬃcients in pq are equal to 1, all
the coeﬃcients in p must be equal to some r ∈R, and all of the coeﬃcients in q
must be equal to 1/r.
Further, for all b ∈B, if b ∩supp(p) is nonempty, b ∩supp(q) must be empty,
since otherwise QF
E would contain a term with two factors in b, which clearly never
happens according to the deﬁnition of QF
E.
Since E is nonempty, for each b ∈B there must be some T ∈b ∩supp(QF
E).
Thus at least one of b ∩supp(p) and b ∩supp(q) must be nonempty, so exactly one
of b ∩supp(p) and b ∩supp(q) must be nonempty.
19

Let C be the set of all b ∈B such that b ∩supp(p) is nonempty.
For every b ∈C, every term of QF
E has exactly one factor in b. Thus, every term
in p has exactly one factor in b. These cover all variables in the support of p, so
each term in p must have total degree |C|.
For each m ∈monosF
C(E), m divides a term in QF
E.
Since m has no common support with q, m must also divide a term in p. Thus
r · m must be a term in p. Conversely, every term in p divides a term in QF
E, and
thus must be in monosF
C(E). Thus every term in p is of the form r · m for some
m ∈monosF
C(E). Thus p = P
m∈monosF
C(E) r · m = r · polyF
C(E).
5.2
Factoring Characteristic Polynomials
We will now show how to factor characteristic polynomials into irreducibles.
Deﬁnition 35. Given a ﬁnite factored set F = (S, B), and a nonempty subset
E ⊆S, let IrrF (E) ⊆P(B) denote the set of all C ⊆B such that:
1. C is nonempty,
2. χF
C(E, E) = E, and
3. there is no nonempty strict subset D ⊂C such that χF
D(E, E) = E.
Proposition 29. Let F = (S, B) be a ﬁnite factored set, and let E be a nonempty
subset of S. Then IrrF (E) ∈Part(B).
Proof. Let F = (S, B) be a ﬁnite factored set, and let E be a nonempty subset of
S. It suﬃces to show that the sets in IrrF (E) are pairwise disjoint and cover B.
We start by showing that the set of all C ⊆B satisfying χF
C(E, E) = E is
closed under intersection.
Indeed, if χF
C0(E, E) = E and χF
C1(E, E) = E, then
χF
C0∩C1(E, E) = χF
C0(E, χF
C1(E, E)) = χF
C0(E, E) = E.
Next, observe that χF
B(E, E) = E. Thus, for all b ∈B, we can consider Cb =
T
C⊆B,b∈C,χF
C(E,E)=E C. Since Cb is an intersection of a ﬁnite nonempty collection
of sets C satisfying χF
C(E, E) = E, we have that χF
Cb(E, E) = E. Further, b ∈Cb,
so Cb is nonempty.
Assume for the purpose of contradiction that there is some nonempty strict
subset D ⊂Cb such that χF
D(E, E) = E. If b ∈D, then we have a contradiction by
the deﬁnition of Cb. If b /∈D, then note that χF
B\D(E, E) = E, so χF
Cb\D(E, E) = E,
and Cb \ D is a nonempty strict subset of Cb that contains b, contradicting the
deﬁnition of Cb.
Thus Cb ∈IrrF (E) for all b ∈B, and since b ∈Cb, this means that the sets in
IrrF (E) cover B.
Next, we need to show that the sets in IrrF (E) are pairwise disjoint. Let C0, C1 ∈
IrrF (E) be arbitrary distinct elements.
We have that χF
C0∩C1(E, E) = E, and
C0 ∩C1 is a subset of C0 and C1, and thus a strict subset of at least one of them.
Thus C0 ∩C1 is empty.
Thus IrrF (E) ∈Part(B).
The following two propositions constitute a factorization of QF
E into irreducibles.
Proposition 30. Let F = (S, B) be a ﬁnite factored set, and let E be a nonempty
subset of S. Then QF
E = Q
C∈IrrF (E) polyF
C(E).
Proof. Let F = (S, B) be a ﬁnite factored set, and let E be a nonempty subset
of S. Let n = |IrrF (E)|, and let IrrF (E) = {C0, . . . , Cn−1}. For 0 ≤k < n, let
C≤k = Sk
i=0 Ci.
We will show by induction on k that Qk
i=0 polyF
Ci(E) = polyF
C≤k(E) for all
0 ≤k < n.
20

If k = 0, the result is trivial, as Q0
i=0 polyF
Ci(E) = polyF
C0(E) = polyF
C≤0(E).
For k > 0, observe that Ck and C≤k−1 are disjoint, and that E = χF
Ck(E, E).
Thus by Proposition 27, we have polyF
Ck(E) · polyF
C≤k−1(E) = polyF
C≤k(E). Thus,
by induction, we get Qk
i=0 polyF
Ci(E) = polyF
C≤k(E).
In the case where k = n−1, this gives that Q
C∈IrrF (E) polyF
C(E) = polyF
B(E) =
QF
E.
Proposition 31. Let F = (S, B) be a ﬁnite factored set, and let E be a nonempty
subset of S. Then polyF
C(E) is irreducible for all C ∈IrrF (E).
Proof. Let F = (S, B) be a ﬁnite factored set, let E be a nonempty subset of S,
and let C ∈IrrF (E).
Assume for the purpose of contradiction that p0 · p1 = polyF
C(E), and that both
p0 and p1 have nonempty support.
By Proposition 28, we have that pi = ri · polyF
Ci(E), for some r0, r1 ∈R, and
C0, C1 ⊆B.
We will ﬁrst need to show that C0 and C1 are nonempty and disjoint. They
must be nonempty, because p0 and p1 have nonempty support. Assume for the
purpose of contradiction that b ∈C0 ∩C1. Let s be an element of E, and note that
for i ∈{0, 1}, we have [s]b ∈supp polyF
Ci(E). Thus [s]b must be degree at least 2 in
polyF
C(E), which contradicts the fact that every variable clearly has degree at most
1 in polyF
C(E).
Next, we need to show that C0 ∪C1 = C. We already know that
supp(polyF
C(E)) = supp(r0r1polyF
C0(E)polyF
C1(E))
= supp(polyF
C0(E)) ∪supp(polyF
C1(E)).
(8)
Let s be an element of E. Given an arbitrary b ∈B, we have that b ∈C if and only
if [s]b ∈supp(polyF
C(E)) if and only if [s]b ∈supp(polyF
Ci(E)) for some i ∈{0, 1} if
and only if b ∈C0 ∪C1.
We now have that C0 and C1 are disjoint and that C = C0 ∪C1. Thus, by
Proposition 27, we have that polyF
C0(E) · polyF
C1(E) = polyF
C(χF
C0(E, E)). Thus
polyF
C(E) = r0r1polyF
C(χF
C0(E, E)), so monosF
C(E) = monosF
C(χF
C0(E, E)).
Let s0, s1 ∈E be arbitrary, and let s2 = χF
C0(s0, s1). Note that monoF
C(s2) ∈
monosF
C(χF
C0(E, E)) = monosF
C(E), so there is some s3 ∈E such that monoF
C(s2) =
monoF
C(s3). Thus s2 ∼b s3 for all b ∈C. However, we also have that s2 ∼b s1
for all b ∈B \ C, so s2 = χF
C(s3, s1).
Since C ∈IrrF (E), χF
C(E, E) = E, so
s2 = χF
C0(s0, s1) ∈E. Since s0 and s1 were arbitrary elements of E, we have that
χF
C0(E, E) = E. Since C0 is a nonempty strict subset of C, this contradicts the fact
that C ∈IrrF (E).
Thus, polyF
C(E) is irreducible for all C ∈IrrF (E).
5.3
Characteristic Polynomials and Orthogonality
We can now give an alternate characterization of conditional orthogonality in terms
of divisibility of characteristic polynomials.
Lemma 3. Let F = (S, B) be a ﬁnite factored set, and let X, Y, Z ∈Part(S) be
partitions of S. The following are equivalent.
1. X ⊥F Y | Z.
2. QF
z divides QF
x∩z · QF
y∩z for all x ∈X, y ∈Y , and z ∈Z.
3. QF
z · QF
x∩y∩z = QF
x∩z · QF
y∩z for all x ∈X, y ∈Y , and z ∈Z.
21

Proof. Clearly condition 3 implies condition 2. We will ﬁrst show that condition 1
implies condition 3, and then show that condition 2 implies condition 1.
Let F = (S, B), and let X, Y, Z ∈Part(S) satisfy X ⊥F Y | Z. Consider an
arbitrary x ∈X, y ∈Y , and z ∈Z.
We want to show that QF
z · QF
x∩y∩z =
QF
x∩z · QF
y∩z.
Let C = hF (X|z). Clearly C ⊢F X|z. We thus have that χF
C(z, z) = z, so
χF
B\C(z, z) = z. We also have that hF (Y |z) ⊆B \ C, so Y |z ≤z (W
S(B \ C))|z.
These two together give that B \ C ⊢F Y |z.
Since C ⊢F X|z, we have that χF
C(x ∩z, z) = x ∩z. Thus, by Proposition 27,
we have that polyF
C(x ∩z) · polyF
B\C(z) = QF
x∩z. Similarly, since B \ C ⊢F Y |z, we
have that polyF
C(z) · polyF
B\C(y ∩z) = QF
y∩z.
Since χF
C(x∩z, y∩z) ⊆χF
C(x∩z, z) = x∩z, and χF
C(x∩z, y∩z) ⊆χF
C(z, y∩z) =
y ∩z, we have χF
C(x ∩z, y ∩z) ⊆x ∩y ∩z. We also have that
χF
C(x ∩z, y ∩z) ⊇χF
C(x ∩y ∩z, x ∩y ∩z)
⊇x ∩y ∩z.
(9)
Thus χF
C(x ∩z, y ∩z) = x ∩y ∩z.
By Proposition 27, this gives that polyF
C(x ∩z) · polyF
B\C(y ∩z) = QF
x∩y∩z.
Finally, since χF
C(z, z) = z, we have that polyF
C(z) · polyF
B\C(z) = QF
z .
Thus, QF
z · QF
x∩y∩z and QF
x∩z · QF
y∩z are both equal to polyF
C(x ∩z) · polyF
B\C(y ∩
z) · polyF
C(z) · polyF
B\C(z).
Thus, condition 1 implies condition 3.
It remains to show that condition 2
implies condition 1.
Fix F = (S, B), and X, Y, Z ∈Part(S), and let QF
z divide QF
x∩z · QF
y∩z for
all x ∈X, y ∈Y , and z ∈Z.
Assume for the purpose of contradiction that
it is not the case that X ⊥F Y | Z.
Thus, there exists some z ∈Z such that
hF (X|z) ∩hF (Y |z) ̸= {}. Let z ∈Z and b ∈B satisfy b ∈hF (X|z) ∩hF (Y |z).
Let C ⊆B be such that b ∈C and C ∈IrrF (z), and let p = polyF
C(z). Thus, p
is an irreducible factor of QF
z .
Either p divides QF
x∩z for all x ∈X or p divides QF
y∩z for all y ∈I, since
otherwise there would exist an x ∈X and a y ∈Y such that p divides neither QF
x∩z
nor QF
x∩z, but does divide their product, contradicting the fact that p is irreducible,
and thus prime.
Assume without loss of generality that p divides QF
x∩z for all x ∈X. Fix an
x ∈X. Let us ﬁrst restrict attention to the case where x ∩z is nonempty.
Let QF
x∩z = p·q. By Proposition 28, p = r0·polyF
C0(x∩z) and q = r1·polyF
C1(x∩z)
for some r0, r1 ∈R and C0, C1 ⊆B. We will show that C0 = C, C1 = B \ C, and
r0 = r1 = 1.
Let s be an element of x∩z. Then for all b ∈B, b ∈C if and only if [s]b ∈supp(p)
if and only if [s]b ∈supp(polyF
C0(x ∩z)) if and only if b ∈C0. Thus C0 = C.
For all b ∈B\C, we have [s]b ∈supp(QF
x∩z) and [s]b /∈supp(p), so [s]b ∈supp(q),
so b ∈C1. Similarly, for all b ∈C1, [s]b ∈supp(q), so [s]b /∈supp(p), so b ∈B \ C.
Thus C1 = B \ C.
Since p and polyF
C0(x ∩z) both have all coeﬃcients equal to 1, we have r0 = 1.
Thus, p = polyF
C(x ∩z).
Similarly, since all the coeﬃcients of p are 1 and all the coeﬃcients of QF
x∩z are
1, all the coeﬃcients of q are 1, so r1 = 1. Thus, q = polyF
B\C(x ∩z).
We thus have that QF
x∩z = polyF
C(z) · polyF
B\C(x ∩z).
In the case where x ∩z is empty, we also have QF
x∩z = polyF
C(z) · polyF
B\C(x ∩z),
since both sides are 0.
By Proposition 27, QF
x∩z = polyF
B(χF
C(z, x ∩z)).
Thus, monosF
B(x ∩z) =
monosF
B(χF
C(z, x ∩z)), so x ∩z = χF
C(z, x ∩z) = χF
B\C(x ∩z, z).
22

Since x∩z = χF
B\C(x∩z, z) for all x ∈X, we have that B \C ⊢F X|z. However,
this contradicts the fact that b /∈B \ C, and b ∈hF (X|z).
Thus, condition 2 implies condition 1.
5.4
Probability Distributions on Finite Factored Sets
The primary purpose of all this discussion of characteristic polynomials has been to
build up to thinking about the relationship between orthogonality and probabilistic
independence. We will now discuss probability distributions on ﬁnite factored sets.
Recall the deﬁnition of a probability distribution.
Deﬁnition 36. Given a ﬁnite set S, a probability distribution on S is a function
P : P(S) →R such that
1. P(E) ≥0 for all E ⊆S,
2. P({}) = 0,
3. P(S) = 1, and
4. P(E0 ∪E1) = P(E0) + P(E1) whenever E0, E1 ⊆S satisfy E0 ∩E1 = {}.
A probability distribution on a ﬁnite factored set F is a probability distribution
on its underlying set that also satisﬁes another condition, which represents the
probability distribution coming from a product of distributions on the underlying
factors.
Deﬁnition 37. Given a ﬁnite factored set F = (S, B), a probability distribution on
F is a probability distribution P on S such that for all s ∈S, we have P({s}) =
Q
b∈B P([s]b).
Proposition 32. Given a ﬁnite factored set F = (S, B), a probability distribution
on S is a probability distribution P on F if and only if P(E) = QF
E(P) for all
E ⊆S.
Proof. If P(E) = QF
E(P) for all E ⊆S, in particular this means that P({s}) =
QF
{s}(P) = (Q
b∈B[s]b)(P) = Q
b∈B P([s]b) for all s ∈S.
Conversely, if P({s}) = Q
b∈B P([s]b) for all s ∈S, then for all E ⊆S, P(E) =
P
s∈E
Q
b∈B P([s]b) = (P
s∈E
Q
b∈B[s]b)(P) = QF
E(P).
5.5
The Fundamental Theorem of Finite Factored Sets
We are now ready to state and prove the fundamental theorem of ﬁnite factored
sets.
Theorem 3. Let F = (S, B) be a ﬁnite factored set, and let X, Y, Z ∈Part(S) be
partitions of S. Then X ⊥F Y | Z if and only if for all probability distributions P on
F and all x ∈X, y ∈Y , and z ∈Z, we have P(x∩z)·P(y ∩z) = P(x∩y ∩z)·P(z).
Proof. We already have by Lemma 3 that if X ⊥F Y | Z, then for all x ∈X, y ∈Y ,
and z ∈Z, QF
z · QF
x∩y∩z = QF
x∩z · QF
y∩z. Thus for any probability distribution P on
F, we have
P(z) · P(x ∩y ∩z) = QF
z (P) · QF
x∩y∩z(P)
= QF
x∩z(P) · QF
y∩z(P)
= P(x ∩z) · P(y ∩z).
(10)
Conversely, assume that for all probability distributions P on F, and all x ∈X,
y ∈Y , and z ∈Z, we have P(x ∩z) · P(y ∩z) = P(x ∩y ∩z) · P(z).
If S is empty, then {} is the unique partition of S, and we have {} ⊥F {} | {}.
Thus, we can restrict our attention to the case where S is nonempty.
23

Fix an arbitrary x ∈X, y ∈Y , and z ∈Z. Let q = QF
x∩z · QF
y∩z −QF
x∩y∩z · QF
z .
We will ﬁrst show that q(f) = 0 for all f : P(S) →R>0.
Given an arbitrary f : P(S) →R>0, we can deﬁne Pf : P(S) →R by Pf(E) =
QF
E(f)/QF
S (f), and we will show that Pf is a distribution on F.
Pf is well-deﬁned because QF
S (f) is a nonempty sum of products of positive real
numbers, and thus positive. Further, since QF
E(f) is a sum of products of positive
real numbers, Pf(E) ≥0 for all E ⊆S. Since QF
{} = 0, we also have Pf({}) = 0.
Clearly Pf(S) = 1. Finally, for all E0, E1 ⊆S with E0 ∩E1 = {}, we have
Pf(E0 ∪E1) = QF
E0∪E1(f)/QF
S (f)
= (QF
E0(f) + QF
E1(f))/QF
S (f)
= Pf(E0) + Pf(E1).
(11)
Therefore Pf is a distribution on S. We still need to show that Pf is a distribution
on F.
Observe that for all s ∈S and b ∈B, since χF
{b}([s]b, S) = [s]b, we have that
QF
[s]b(f) = polyF
{b}([s]b) · polyF
B\{b}(S), and since χF
{b}(S, S) = S, we have that
QF
S (f) = polyF
{b}(S) · polyF
B\{b}(S). Thus, we have that
Pf([s]b) = polyF
{b}([s]b)(f)/polyF
{b}(S)(f)
= f([s]b)/polyF
{b}(S)(f).
(12)
Thus, for all s ∈S,
Y
b∈B
Pf([s]b) = (
Y
b∈B
f([s]b))/(
Y
b∈B
polyF
{b}(S)(f))
= QF
{s}(f)/QF
S (f)
= Pf({s}).
(13)
Thus Pf is a distribution on F.
It follows that Pf(x ∩z) · Pf(y ∩z) = Pf(x ∩y ∩z) · Pf(z). We therefore have
that
q(f) = QF
x∩z(f) · QF
y∩z(f) −QF
x∩y∩z(f) · QF
z (f)
= (Pf(x ∩z) · Pf(y ∩z) −Pf(x ∩y ∩z) · Pf(z)) · QF
S (f)2
= 0 · QF
S (f)2
= 0.
(14)
Thus, q is a polynomial that is zero on an open subset of inputs, so q is the zero
polynomial. Thus QF
x∩z·QF
y∩z−QF
z ·QF
x∩y∩z = 0, so QF
z ·QF
x∩y∩z = QF
x∩z·QF
y∩z. Since
x ∈X, y ∈Y , and z ∈Z were arbitrary, by Lemma 3, we have X ⊥F Y | Z.
6
Inferring Time
The fundamental theorem tells us that (conditional) orthogonality data can be
inferred from probabilistic data. Thus, if we can infer temporal data from orthog-
onality data, we will be able to combine these to infer temporal data purely from
probabilistic data.
In this section, we will discuss the problem of inferring temporal data from
orthogonality data, mostly by going through a couple of examples.
24

6.1
Factored Set Models
We’ll begin with a sample space, Ω.
Naively, one might except that temporal inference in this paradigm involves
inferring a factorization of Ω. What we’ll actually be doing, however, is inferring
a factored set model of Ω. This will allow for the possibility that some situations
are distinct without being distinct in Ω—that there can be latent structure not
represented in Ω.
Deﬁnition 38 (model). Given a set Ω, a model of Ωis a pair M = (F, f), where
F is a ﬁnite factored set and f : set(F) →Ωis a function from the set of F to Ω.
Deﬁnition 39. Let S and Ωbe sets, and let f : S →Ωbe a function from S to Ω.
Given a ω ∈Ω, we let f −1(ω) = {s ∈S | f(s) = ω}.
Given an E ⊆Ω, we let f −1(E) = {s ∈S | f(s) ∈E}.
Given an X ∈Part(Ω), we let f −1(X) ∈Part(S) be given by f −1(X) =
{f −1(x)|x ∈X, f −1(x) ̸= {}}.
Deﬁnition 40 (orthogonality database). Given a set Ω, an orthogonality database
on Ωis a pair D = (O, N), where O and N are both subsets of Part(Ω)×Part(Ω)×
Part(Ω).
Deﬁnition 41. Given an orthogonality database D = (O, N) on a set Ω, and
partitions X, Y, Z ∈Part(Ω), we write X ⊥D Y | Z if (X, Y, Z) ∈O, and we write
X ⇋D Y | Z if (X, Y, Z) ∈N.
Deﬁnition 42. Given a set Ω, a model M = (F, f) of Ω, and an orthogonality
database D = (O, N) on Ω, we say M models D if for all X, Y, Z ∈Part(Ω),
1. if X ⊥D Y | Z then f −1(X) ⊥F f −1(Y ) | f −1(Z), and
2. if X ⇋D Y | Z then ¬(f −1(X) ⊥F f −1(Y ) | f −1(Z)).
Deﬁnition 43. An orthogonality database D on a set Ωis called consistent if there
exists a model M of Ωsuch that M models D.
Deﬁnition 44. An orthogonality database D on a set Ωis called complete if for
all X, Y, Z ∈Part(Ω), either X ⊥D Y | Z or X ⇋D Y | Z.
Deﬁnition 45. Given a set Ω, an orthogonality database D on Ω, and X, Y ∈
Part(Ω), we say X <D Y if for all models (F, f) of Ωthat model D, we have
f −1(X) <F f −1(Y ).
6.2
Examples
Example 1. Let Ω= {00, 01, 10, 11} be the set of all bit strings of length 2. For
i ∈{0, 1}, let xi = {i0, i1} be the event that the ﬁrst bit is i, and let yi = {0i, 1i}
be the event that the second bit is i. Let X = {x0, x1} and let Y = {y0, y1}.
Let v0 = {00, 11} be the event that the two bits are equal, let v1 = {01, 10} be
the event that the two bits are unequal, and let V = {v0, v1}.
Let D = (O, N), where O = {(X, V, {Ω})} and N = {(V, V, {Ω})}.
Proposition 33. In Example 1, D is consistent.
Proof. First observe that F = (Ω, {X, V }) is a factored set, and so M = (F, f) is a
model of Ω, where f is the identity on Ω. It suﬃces to show that M models D.
Indeed hF (X) = {X}, and hF (V ) = {V }, so X ⊥F V , so f −1(X) ⊥F f −1(V ) |
f −1({Ω}).
Further, it is not the case that V ⊥F V , since V ̸= IndΩ. Thus it is not the case
that f −1(V ) ⊥F f −1(V ) | f −1({Ω}).
Thus M satisﬁes all of the conditions to model D, so D is consistent.
Proposition 34. In Example 1, X <D Y .
25

Proof. Let (F, f) be any model of Ωthat models D. Let F = (S, B). For any
A ∈Part(Ω), let HA = hF (f −1(A)). Our goal is to show that HX is a strict subset
of HY .
First observe that X ≤ΩY ∨ΩV , so for any s, t ∈S, if s ∼f −1(Y ) t and
s ∼f −1(V ) t, then f(s) ∼Y f(t) and f(s) ∼V f(t), so f(s) ∼X f(t), so s ∼f −1(X) t.
Thus f −1(X) ≤S f −1(Y ) ∨S f −1(V ).
It follows that HX ⊆hF (f −1(Y ) ∨S f −1(V )) = HY ∩HV .
However, since
X ⊥D V | {Ω}, we have that HX ∩HV = {}, so HX ⊆HY .
By swapping X and V in the argument above, we also get that HV ⊆HY . Since
V ⇋D V | {Ω}, we have that HV ̸= {}. Thus HV contains some element b. Observe
that b /∈HX, but b ∈HY . Thus HX is a strict subset of HY , so f −1(X) <F f −1(Y ).
Since (F, f) was an arbitrary model of Ωthat models D, this implies that X <D
Y .
Example 2. Let Ω= {000, 001, 010, 011, 100, 101, 110, 111} be the set of all bit
strings of length 3. For i ∈{0, 1}, let xi = {i00, i01, i10, i11} be the event that the
ﬁrst bit is i, let yi = {0i0, 0i1, 1i0, 1i1} be the event that the second bit is i, and
let zi = {00i, 01i, 10i, 11i} be the event that the third bit is i. Let X = {x0, x1}, let
Y = {y0, y1}, and let Z = {z0, z1}.
Let v0 = {000, 001, 110, 111} be the event that the ﬁrst two bits are equal, let
v1 = {010, 011, 100, 101} be the event that the ﬁrst two bits are unequal, and let
V = {v0, v1}.
Let D = (O, N), where O = {(X, V, {Ω}), (X, Z, Y ), (V, Z, Y )} and N =
{(X, Z, {Ω}), (V, Z, {Ω}), (Z, Z, Y )}.
Proposition 35. In Example 2, D is consistent.
Proof. Let S = Ω∪{00, 01, 10, 11} be the set of all bit strings of length either 2 or
3.
For i ∈{0, 1}, let x′
i = {i00, i01, i10, i11, i0, i1} be the event that the ﬁrst bit is
i, and let X′ = {x′
0, x′
1}.
For i ∈{0, 1}, let y′
i = {0i0, 0i1, 1i0, 1i1, 0i, 1i} be the event that the second bit
is i, and let Y ′ = {y′
0, y′
1}.
Let v′
0 = {000, 001, 110, 111, 00, 11} be the event that the ﬁrst two bits are equal,
let v′
1 = {010, 011, 100, 101, 01, 10} be the event that the ﬁrst two bits are unequal,
and let V ′ = {v′
0, v′
1}.
For i ∈{0, 1}, let z′
i = {00i, 01i, 10i, 11i} be the event that the third bit exists
and is i, let z′
2 = {00, 01, 10, 11} be the event that there are only two bits, and let
Z′ = {z′
0, z′
1, z′
2}.
Let B = {X′, V ′, Z′}. Clearly, (S, B) is a ﬁnite factored set.
Let f : S →Ωbe given by f(s) = s if s ∈Ω, f(00) = 000, f(01) = 011,
f(10) = 100, and f(11) = 111, so f copies the last bit on inputs of length 2, and
otherwise leaves the bit string alone. We will show that (F, f) models D.
First, observe that f −1(X) = X′, f −1(Y ) = Y ′, f −1(V ) = V ′, and f −1(Z) =
{{000, 010, 100, 110, 00, 10}, {001, 011, 101, 111, 01, 11}}.
It is easy to verify that hF (X′) = {X′}, hF (V ′) = {V ′}, hF (Y ′) = {X′, V ′},
and hF (f −1(Z)) = B. From this, we get that X′ ⊥F V ′ holds, but X′ ⊥F f −1(Z)
and V ′ ⊥F f −1(Z) do not hold.
Next, observe that for i ∈{0, 1}, X′|yi = V ′|yi = {{0i0, 0i1, 0i}, {1i0, 1i1, 1i}}.
It is easy to verify that hF (X′|yi) = hF (V ′|yi) = {X′, V ′}.
Also, observe that f −1(Z)|y0 = {{000, 100, 00, 10}, {001, 101}}, and observe
that f −1(Z)|y1
=
{{010, 110}, {011, 111, 01, 11}}.
It is easy to verify that
hF (f −1(Z)|y0) = hF (f −1(Z)|y1) = {Z′}.
From this, we get that X′ ⊥F f −1(Z) | Y ′ and V ′ ⊥F f −1(Z) | Y ′ hold, and
f −1(Z) ⊥F f −1(Z) | Y ′ does not hold.
Thus, (F, f) models D, so D is consistent.
Proposition 36. In Example 2, X <D Y <D Z.
26

Proof. Let (F, f) be any model of Ωthat models D. Let F = (S, B). For any
A ∈Part(Ω), let HA = hF (f −1(A)). Our goal is to show that HX is a strict subset
of HY and that HY is a strict subset of HZ.
First observe that X ≤ΩY ∨ΩV , so f −1(X) ≤S f −1(Y ) ∨f −1(V ), so HX ⊆
HY ∪HV . Since X ⊥D Y | {Ω}, HX ∩HV = {}, so HX ⊆HY . Symmetrically,
HV ⊆HY , so HX ∪HV ⊆HY .
Similarly, Y ≤ΩX ∨ΩV , so HY ⊆HX ∪HV . Thus HY = HX ∪HV .
We also know that HX and HV are nonempty, because X ⇋D Z | {Ω} and
Y ⇋D Z | {Ω}.
Thus HX is a strict subset of HY , so X <D Y .
Let C ⊆B be arbitrary such that HX ∩C and HV ∩(B \ C) are both nonempty.
Fix some bX ∈HX ∩C and bV ∈HV ∩(B \ C).
Since bX ∈HX, there must exist s0, s1 ∈S such that s0 ∼b s1 for all b ∈B\{bX},
but not s0 ∼f −1(X) s1. Thus it is not the case that f(s0) ∼X f(s1). Without loss
of generality, assume that f(s0) ∈x0 and f(s1) ∈x1.
Similarly, since bV ∈HV , there must exist t0, t1 ∈S such that t0 ∼b t1 for all
b ∈B \ {bV }, but not t0 ∼f −1(V ) t1. Again, without loss of generality, assume that
f(t0) ∈v0 and f(t1) ∈v1.
For i, j ∈{0, 1}, let rij = χF
HX(si, tj).
Next, observe that rij ∼f −1(X) si, so f(rij) ∼X f(si) ∈xi, so f(rij) ∈xi.
Similarly, f(rij) ∈vj, so f(rij) ∈xi ∩vj. Thus, if i = j, f(rij) ∈y0, and if i ̸= j,
f(rij) ∈y1.
Further, observe that χF
C(r00, r11) = r01, since r00 and r11 agree on all factors
other than bX and bV .
In particular, this means that χF
C(f −1(y0), f −1(y0)) ̸=
f −1(y0). Similarly, since χF
C(r01, r10) = r00, we have that χF
C(f −1(y1), f −1(y1)) ̸=
f −1(y1).
We will use this to show that for any y ∈f −1(Y ) and A ∈Part(y), either
hF (A) ∩HY = {}, or HY ⊆hF (A). This is because hF (A) ⊢F A, so χF
hF (A)(y, y) =
y, so by the above argument, if hF (A) ∩HX is nonempty, then HV ⊆hF (A),
which since HV is nonempty means hF (A) ∩HV is nonempty, so HX ⊆hF (A),
so HY ⊆hF (A). Symmetrically, we also have that if hF (A) ∩HV is nonempty,
then HY ⊆hF (A). Thus, if hF (A) ∩HY is nonempty, then either hF (A) ∩HX or
hF (A) ∩HV is nonempty, so HY ⊆hF (A).
Note that for any y ∈f −1(Y ), two of the elements among the four rij de-
ﬁned above are in y, and those two elements are in diﬀerent parts in f −1(X),
so f −1(X)|y has at least two parts, so hF (f −1(X)|y) is nonempty.
However,
hF (f −1(X)|y) ⊆hF (f −1(X) ∨S f −1(Y )) = HY . Thus, hF (f −1(X)|y) ∩HY ̸= {},
so HY ⊆hF (f −1(X)|y), so hF (f −1(X)|y) = HY . Symmetrically, hF (f −1(V )|y) =
HY .
In particular, this means that hF (f −1(Z)|y) ∩HY = {}, since X ⊥D Z | Y .
Since X ⇋D Z | {Ω}, there exists some bZ ∈HX ∩HZ. Since bZ ∈HZ, there
exist u0, u1 ∈S such that u0 ∼b u1 for all b ∈B \ {bZ}, but it is not the case that
u0 ∼f −1(Z) u1. Without loss of generality, assume that f(u0) ∈z0 and f(u1) ∈z1.
Let y = [u0]f −1(Y ).
Let by be an arbitrary element of HY . Since bY ∈HY , there exist q0, q1 ∈S
such that q0 ∼b q1 for all b ∈B \ {bY }, but it is not the case that q0 ∼f −1(Y ) q1.
Without loss of generality, assume that q0 ∈y and q1 /∈y.
Consider p0 = χF
HY (q0, u0) = χF
HY (q0, u1). Since q0 ∈y, p0 ∈y. Since u0 is also
in y, χF
hF (f −1(Z)|y)(p0, u0) ∼f −1(Z) p0. However, since hF (f −1(Z)|y) ∩HY = {}, we
have χF
hF (f −1(Z)|y)(p0, u0) = u0, so u0 ∼f −1(Z) p0.
If u1 were in y, we would similarly have u1 ∼f −1(Z) p0, which would contradict
the fact that it is not the case that u0 ∼f −1(Z) u1. Thus u1 /∈y.
Next, consider p1 = χF
HY (q1, u0) = χF
HY (q1, u1). Since q1 /∈y, p1 /∈y. Since u1 is
also not in y, χF
hF (f −1(Z)|(S\y))(p1, u1) ∼f −1(Z) p1. However, since hF (f −1(Z)|(S \
y)) ∩HY = {}, we have χF
hF (f −1(Z)|(S\y))(p1, u1) = u1, so u1 ∼f −1(Z) p1.
27

Thus, it is not the case that p0 ∼f −1(Z) p1. However, we constructed p0 and p1
such that p0 ∼b p1 for all b ̸= bY . Thus bY ∈HZ. Since bY was arbitrary in HY ,
we have that HY ⊆HZ. Finally, we need to show that this subset relation is strict.
Since Z ⇋D Z | Y , there is some y such that hF (f −1(Z)|y) ̸= {}. Let b be
any element of hF (f −1(Z)|y). Since hF (f −1(Z)|y) ∩HY = {}, b /∈HY . However,
b ∈hF (f −1(Z)|y) ⊆hF (f −1(Z) ∨S f −1(Y )) = hZ ∪HY . Therefore b ∈HZ. Thus
HY is a strict subset of HZ, so Y <D Z.
7
Applications, Future Work, and Speculation
We will now discuss several diﬀerent applications and directions for future work.
We will divide these research directions into three categories: ‘Inference,’ ‘Inﬁnity,’
and ‘Embedded Agency.’
This section will be much more speculative than the rest of the paper. It is very
likely that some of these avenues for research will turn out to be dead ends, and
some of the claims made here may not hold up to further investigation.
7.1
Inference
Decidability of Temporal Inference
In Section 6, we described a combinatorial problem of inferring temporal relations
from an orthogonality database. However, it is not clear whether the question “Does
a given temporal relation follow from a given orthogonality database?” is decidable.
However, it is not clear whether or not it is decidable whether a given temporal
relation follows from a given orthogonality database.
One way we could hope to decide whether a temporal relation follows from some
orthogonality database D over Ωwould be to simply check all factored set models
of Ωthat model D up to a given size, and see whether the temporal relation always
holds. For this to work, we would need an upper bound on the size of factored sets
that we need to consider, as a function of the size of Ω. (Note that the existence
of such a bound would not mean that there are no models larger than this upper
bound. Rather, it would mean that every model larger than this will have all of the
same temporal relations as some smaller model.)
Eﬃcient Temporal Inference
Assuming temporal inference is computable, we would further like to be able to
infer temporal relations from an orthogonality database quickly.
The naive way to get negative results in temporal inference (i.e., to show that
certain temporal relations need not hold) would be to search over the space of
models. Without the upper bound discussed above, however, this method would
only ever yield negative results.
The naive way to get positive results would be to formalize the kind of reasoning
used to prove Propositions 34 and 36, and search over proofs of this form. It is
unclear whether this method can be made eﬃcient.
Alternatively, we could hope to develop some new results and reﬁne our under-
standing of temporal inference to the point where an alternative method can be
made eﬃcient.
Temporal Inference from Raw Data and Fewer Ontological Assumptions
In the Pearlian causal inference paradigm, we can infer temporal relationships from
joint distributions on a collection of variables.
In Pearl’s paradigm, however, this data is already factored into a collection of
variables at the outset. Further, the Pearlian paradigm does not make explicit the
assumptions that go into this factorization.
Our paradigm instead starts from a distribution on some set of observably dis-
tinct worlds. This approach allows us to make fewer ontological assumptions; we
28

don’t need to take for granted a particular way the world should be factored into
variables. Thus, one might hope that the factored sets paradigm could be used to
infer time or causality more directly from raw probabilistic data.
Causality, Determinism, and Abstraction
Another issue with the Pearlian causal inference paradigm is that it does not work
well in cases where some of the variables are (partially) deterministic functions of
each other. Our paradigm has determinism and abstraction built in, so it can be
used to infer time in situations where the Pearlian paradigm might not apply.
Conceptual Inference
In Example 1, we can infer that X <D Y . We can think of this fact as being about
time. However, we can also think of it as being about which concepts are more
natural or fundamental. In that example, X and V were more primitive variables,
while Y was a more derived variable that was computed from X and V .
Suppose we had a symbol that was either 0 or 1, chosen according to some prob-
ability, and was also colored either blue or green, chosen independently according to
some other probability. We can reason about this symbol using concepts like color
or number. Alternatively, we could deﬁne a new concept bleen meaning “the symbol
is either blue and 0, or green and 1,” and grue, meaning “the symbol is either green
and 0, or blue and 1,” and use these two concepts instead (cf. Goodman 1955).
We want to say that color and number are in some sense better or more useful
concepts, while bleen and grue are less useful. Finite factored sets help give formal
content to the idea that color and number are more primitive, while bleen and grue
are more derived; and this primitiveness seems to point at part of what it means to
be a good concept for the purpose of thinking about the world.
Inferring Time without Orthogonality
In this paper, we have focused on inferring time from an orthogonality database.
Such a database may have been inferred in turn from observed independence and
dependence facts drawn from a probability distribution.
We could instead consider inferring time directly from a probability distribution.
Cutting out the orthogonality database in this way could even allow us to infer time
from a probability distribution that has no nontrivial conditional independencies at
all.
To see why it might be possible to infer time without any orthogonality, consider
a set Ω, and a model of Ω, (F, f), where F = (S, B) has n binary factors, and
|Ω| > n + 1.
There are n degrees of freedom in an arbitrary probability distribution on F,
and thus at most n degrees of freedom in a probability distribution P on Ωthat
comes from a probability distribution on F. However, there are |Ω−1| degrees of
freedom in an arbitrary distribution on Ω.
As such, the probability distribution on Ωwill lie on some surface without full
dimension in the space of probability distributions on Ω, which could be used to
infer some of the properties of F.
However, if |Ω| is much smaller than |S|, and f is chosen at random, it is
unlikely that there will be any conditional orthogonality relations on partitions of
Ωat all (other than the trivial conditional orthogonality relations that come from
one partition being ﬁner than another).
Inferring Conditioned Finite Factored Sets
If we modify the temporal inference deﬁnition to instead allow for f to be a partial
function from S to Ω, we get a new, weaker model of temporal inference. This
can be thought of as allowing for the possibility that our distribution on Ωpasses
through some ﬁlter that only shows us some of the observably distinct worlds.
29

7.2
Inﬁnity
The Fundamental Theorem of Finitely Generated Factored Sets
Throughout this paper, we have assumed ﬁniteness fairly gratuitously. It is likely
that many of the results can be extended to arbitrary factored sets. However, this
generalization will not be immediate. Indeed, even history is not well-deﬁned on
arbitrary factored sets.
One intermediate possibility is to consider ﬁnite-dimensional factored sets. In
this case, history would be well-deﬁned, but our proof of the fundamental theorem
would not directly generalize. However, we conjecture that the ﬁnite-dimensional
analogue of the fundamental theorem would in fact hold.
Conjecture 1. Theorem 3 can be generalized to ﬁnite-dimensional factored sets.
On the other hand, we do not expect the fundamental theorem to generalize to
arbitrary factored sets. To see why, consider the following example.
Example 3. Let F = (S, B), where S = P(N), bn = {{s ∈S | n ∈s}, {s ∈S | n /∈
s}}, and B = {bn | n ∈N}. Let X = {{{}}, S \ {{}}}, and let Y = {{N}, S \ {N}}.
In this example, it seems that in the correct generalization of orthogonality
to arbitrary factored sets, we likely want to say that X is not orthogonal to Y .
However, it also seems like we want to say that in every distribution on F, at least
one of {{}} and {N} has probability zero, so this should give a counterexample to
the fundamental theorem. Even without the fundamental theorem, we believe that
orthogonality and time in arbitrary-dimensional factored sets will be important and
interesting.
Orthogonality and Time in Arbitrary Factored Sets
In the inﬁnite-dimensional case, it is not even clear how we should deﬁne orthogo-
nality, time, and conditional orthogonality. There are three main contenders.
First, we could say that (sub)partitions X and Y are orthogonal if there exist
disjoint CX, CY ⊆B such that CX ⊢F X and CY ⊢F Y . We could then deﬁne time
as a closure property on orthogonality.
Second, we could just deﬁne the history of a (sub)partition X to be the inter-
section of all C ⊆B such that C ⊢F X, and leave the deﬁnitions of orthogonality
and time alone. This second option has some unintuitive behavior. Consider the
following example.
Example 4. Let F = (S, B), where S = P(N), bn = {{s ∈S | n ∈s}, {s ∈S | n /∈
s}}, and B = {bn | n ∈N}. Let Z = {{s ∈S | |s| < ∞}, {s ∈S | |s| = ∞}}.
In this example, Z is orthogonal to itself according to the second option, in
spite of having more than one part. However, it is possible that this is a feature,
rather than a bug, since it seems to interact nicely with Kolmogorov’s zero–one law
(Kolmogorov 1956).
Third, we could deﬁne a way to ﬂatten factored sets by merging some of the
factors into their common reﬁnement, and we could say X and Y are orthogonal
given Z in F if X and Y are orthogonal given Z in some ﬁnite-dimensional ﬂattening
of F.
The main diﬀerence between the ﬁrst and third options comes from the case
where Z has inﬁnitely many parts. In the third option, we must ﬁx a single ﬁnite-
dimensional ﬂattening such that X|z and Y |z have disjoint histories for all z ∈Z.
We are most optimistic about the third option, because we conjecture that it can
satisfy the compositional semigraphoid axioms, while the other two options cannot.
It is also possible that other options give the compositional semigraphoid axioms
for partitions with ﬁnitely many parts, but not general partitions.
30

Continuity and Physics
A major reason why we are interested in exploring arbitrary-dimensional factored
sets is because it could allow us to talk about continuous time.
The Pearlian paradigm takes advantage of the parenthood relationship between
nodes to make inferences. E.g., the nodes are thought of as probabilistic functions of
their parents, and the existence of edges between nodes is a central part of temporal
inference.
In the factored set paradigm, there is no mention of parenthood; instead, ≤F is
both reﬂexive and transitive, and so can be thought of as an ancestry relation. Fur-
ther, by working with arbitrary partitions rather than a ﬁxed collection of variables,
we allow for “zooming in” on our variables.
These two properties together suggest that the factored set paradigm is much
closer to being able to talk about continuous time, if the theory can be extended
naturally to inﬁnite dimensions.
As pointed out by Yudkowsky (2012), physics looks an awful lot like a continuous
analogue of Pearlian causal diagrams. We are thus hopeful that when extended to
arbitrary dimensions, factored sets could provide a useful new way of looking at
physics.
7.3
Embedded Agency
Embedded Observations
We can use ﬁnite factored sets to build a new way of thinking about observations.
Deﬁnition 46 (observes an event). Let F = (S, B) be a ﬁnite factored set. Let A
and W be partitions of S, and let E be a subset of S. Let XE be the partition of S
given by XE = {S} if E = {} or E = S, and XE = {E, S \ E} otherwise. We say
A observes E with respect to W (in F) if the following two conditions hold.
1. A ⊥F XE.
2. A ⊥F W | S \ E.
A can be thought of as an agent, with the diﬀerent parts in A representing
options available to A. E represents some fact about the world. W can be thought
of as some high-level world model. We will especially think of W as a world model
that captures all of the information about the world that the agent cares about.
When we say that A observes E, this does not necessarily mean that E holds.
Rather, we are saying that A can safely assume that E holds. A can safely make
this assumption if it is the case that A’s choice can’t eﬀect whether E holds, and
if, when E does not hold, A’s choice can have no eﬀect on any part of the world
that A cares about. This is exactly what is represented by the two conditions in
Deﬁnition 46.
In Drescher’s (2006) transparent Newcomb thought experiment, the agent can-
not be said to observe the contents of the box, because the ﬁrst condition in Deﬁni-
tion 46 is violated. In Nesov’s (2009) counterfactual mugging thought experiment,
the agent cannot be said to observe the result of the coin ﬂip, because the second
condition is violated.
We can extend this deﬁnition to give a notion of an agent observing a partition
rather than an event.
Deﬁnition 47 (observes a partition). Let F = (S, B) be a ﬁnite factored set. Let
A, W, and X be partitions of S. Let X = {x0, . . . , xn−1}. We say A observes
X with respect to W (in F) if A ⊥F X and there exist partitions of S, Ai for
i ∈{0, . . . , n −1} such that
1. A = W
S({Ai | i ∈{0, . . ., n −1}}).
2. Ai ⊥F W | S \ xi.
Saying that A observes X is roughly saying that A can be divided into subagents,
where each subagent observes a diﬀerent part in X.
31

Counterfactability
The factored set paradigm also has some interesting things to say about counter-
factuals. The chimera functions can be thought of representing a way of taking
counterfactuals.
Given a ﬁnite factored set F = (S, B), C ⊆B, and s, t ∈S, let XC = W
S(C).
We can think of χF
C(s, t) as the result of starting with t, then performing a
counterfactual surgery that changes the value of XC to match its value in s.
Unfortunately, while we can tell this story for XC, we cannot tell the same story
for an arbitrary partition of S.
Deﬁnition 48 (counterfactability). Given a ﬁnite factored set F = (S, B), a par-
tition X ∈Part(S) is called counterfactable (in F) if X = W
S(hF (X)).
When a partition X is counterfactable, the chimera function gives a well-deﬁned
way to start with an element of S, and change it by changing what part in X it is
in.
Being counterfactable is rather strong, but we have a weaker notion of relative
counterfactability.
Deﬁnition 49 (relative counterfactability). Given a ﬁnite factored set F = (S, B),
a partition X ∈Part(S) is called counterfactable relative to another partition W ∈
Part(S) (in F) if W
S(hF (X)) ⊥F W | X.
X is counterfactable relative to W if X screens oﬀthe history of X from W. This
means that if we want to counterfact on the value of X, we can safely counterfact
on the ﬁner partition W
S(hF (X)). As long as we only care about what part in W
the result is in, choices about which subpart in W
S(hF (X)) to counterfact will not
matter, so we can think of counterfacting on the value of X as well-deﬁned up to
the partition W.
This notion of counterfactability explains why counterfactuals sometimes seem
clear, and other times they do not seem well-deﬁned. In the factored set ontology,
sometimes partitions are not counterfactable because they are not ﬁne enough to
fully specify all the eﬀects of the counterfactual.
Cartesian Frames
The factored set paradigm can be seen as capturing many of the beneﬁts of the
Cartesian frame paradigm (Garrabrant 2020). We have already seen this in part in
our discussion of embedded observations. We feel that the factored set paradigm suc-
cessfully captures a meaningful notion of time, while the Cartesian frame paradigm
mostly fails at this goal.
The connection between factored sets and Cartesian frames is rather strong. For
example, a 2-dimensional factored set model of a set W is in eﬀect a Cartesian frame
over W. The only diﬀerence is that the factored set model forgets which factor is the
agent, and which factor is the environment. When one Cartesian frame over W is
a multiplicative subagent of another, we can construct a 3-dimensional factored set
model of W, with the subagent represented by one of the factors, and the superagent
represented by a pair of the factors.
Unraveling Causal Loops
Whenever an agent makes a decision, there is a temptation to think of the eﬀects of
the decision as causally “before” the decision being made. This is because the agent
uses its model of the eﬀects as an input when making the decision. This causes a
problem, because the eﬀects of the decision can of course also be seen as causally
after the decision being made.
On our view, part of what is going on is that there is a distinction between the
agent’s model of the eﬀects, and the eﬀects themselves. The problem is that the
agent’s model of the eﬀects is highly entangled with the actual eﬀects, which is why
we feel tempted to combine them in the ﬁrst place.
32

One way to model this situation is by thinking of the agent’s model of the eﬀects
as being a coarser version of the actual world state after the decision. It is thus
possible for the model of the eﬀects to be before the decision, which is before the
eﬀects themselves.
By allowing for some variables to be coarsenings or reﬁnements of other variables,
the factored set paradigm possibly gives us the tools to be able to straighten out
these causal loops.
Conditional Time
We can deﬁne conditional time similarly to how we deﬁne conditional orthogonality.
Deﬁnition 50 (conditional time). Given a ﬁnite factored set F = (S, B), partitions
X, Y ∈Part(S), and E ⊆S, we say that X is before Y given E (in F), written
X ≤F Y | E, if hF (X|E) ⊆hF (Y |E).
It is not clear if this notion has any important philosophical meaning, but it
seems plausible that it does. In particular, this notion could be useful for reasoning
about situations where time appears to ﬂow in multiple directions at diﬀerent levels
of description, or under diﬀerent assumptions. Incorporating conditional time could
then be used to ﬂatten some causal loops.
Logical Causality
Upon discovering logical induction, one of the ﬁrst things we considered was the
possibility of inferring logical causality using our probabilities on logical sentences
(Garrabrant et al. 2016). We considered doing this using the Pearlian paradigm,
but it now seems like that approach was doomed to fail, because we had many
deterministic relationships between our variables.
The factored set paradigm seems much closer to allowing us to correctly infer
logical causality from logical probabilities, but it is still far from ready.
One major obstacle is that the factored set paradigm does not have a reasonable
way to think about the uniform distribution on a four-element set. The indepen-
dence structure of the uniform distribution on a four-element set is not a composi-
tional semigraphoid, because if we take X, Y , and Z to be the three partitions that
partition the four-element set into two parts of size two, then X is independent of
Y and of Z, but not independent of the common reﬁnement of Y and Z.
Since the uniform distribution on a four-element set will likely (approximately)
show up many times in logical induction, it is not clear how to do the causal infer-
ence.
Orthogonality as Simplifying Assumptions for Decisions
While we largely have been thinking of orthogonality as a property of the world,
one could also think of orthogonality as something that an agent assumes to make
decisions.
For example, when an agent is looking at a coin that came up heads, the agent
might make the assumption that its decision has no eﬀect on the worlds in which
the coin came up tails. This assumption might only be approximately true, but
part of being an embedded agent is working with approximations. Orthogonality
seems like a useful language for some of the simplifying assumptions agents might
make.
Conditional Orthogonality and Abstractions
Given some complicated structure X, one might want to know when a simpler
structure Y is a good abstraction for X. One desirable property of an abstraction
is that Y screens oﬀX from all of the properties of the world that an agent cares
about, W. In this way, by thinking in terms of Y , the agent does not risk missing
any important information.
33

We could also consider weaker notions than this, by taking W to just be that
which the agent cares about within a certain context in which the agent is using
the abstraction.
This is all very vague and rough, but the point is that conditional orthogonal-
ity seems related to what makes a good abstraction, so being able to talk about
conditional orthogonality and abstractions together seems like it could prove useful.
Acknowledgments: My thanks to Alex Appel, Ramana Kumar, Xiaoyu He, Tsvi
Benson-Tilsen, Andrew Critch, Sam Eisenstat, Rob Bensinger, and Claire Wang for
discussion and feedback on this paper.
34

References
Demski, Abram, and Scott Garrabrant. 2019. “Embedded Agency,” arXiv: 1902.09469
[cs.AI].
Drescher, Gary L. 2006. Good and Real: Demystifying Paradoxes from Physics to Ethics.
Cambridge, MA: MIT Press.
Garrabrant, Scott. 2020. “Introduction to Cartesian Frames.” Less Wrong (blog). https
://www.lesswrong.com/s/2A7rrZ4ySx6R8mfoT/p/BSpdshJWGAW6TuNzZ.
Garrabrant, Scott, Tsvi Benson-Tilsen, Andrew Critch, Nate Soares, and Jessica Taylor.
2016. “Logical Induction,” arXiv: 1609.03543 [cs.AI].
Goodman, Nelson. 1955. Fact, Fiction, and Forecast. Harvard University Press.
Kolmogorov, Andrey. 1956. Foundations of the Theory of Probability. 2nd ed. New York:
Chelsea Publishing Company.
Nesov, Vladimir. 2009. “Counterfactual Mugging.” Less Wrong (blog). https://www.lessw
rong.com/posts/mg6jDEuQEjBGtibX7/counterfactual-mugging.
OEIS Foundation Inc.. 2021. The On-Line Encyclopedia of Integer Sequences. Accessed
May 26, 2021. https://oeis.org/A338681.
Pearl, Judea. 2000. Causality: Models, Reasoning, and Inference. 1st ed. New York: Cam-
bridge University Press.
Yudkowsky, Eliezer. 2012. “Causal Universes.” Less Wrong (blog). https://www.lesswron
g.com/posts/o5F2p3krzT4JgzqQc/causal-universes.
35

