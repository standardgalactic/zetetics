entropy
Article
On a Variational Deï¬nition for the Jensen-Shannon
Symmetrization of Distances Based on the Information Radius
Frank Nielsen


Citation: Nielsen, F. On a Variational
Deï¬nition for the Jensen-Shannon
Symmetrization of Distances Based
on the Information Radius. Entropy
2021, 23, 464. https://doi.org/
10.3390/e23040464
Academic Editor: Yong Deng
Received: 12 March 2021
Accepted: 9 April 2021
Published: 14 April 2021
Publisherâ€™s Note: MDPI stays neutral
with regard to jurisdictional claims in
published maps and institutional afï¬l-
iations.
Copyright: Â© 2021 by the author.
Licensee MDPI, Basel, Switzerland.
This article is an open access article
distributed
under
the
terms
and
conditions of the Creative Commons
Attribution (CC BY) license (https://
creativecommons.org/licenses/by/
4.0/).
Sony Computer Science Laboratories, Tokyo 141-0022, Japan; Frank.Nielsen@acm.org
Abstract: We generalize the Jensen-Shannon divergence and the Jensen-Shannon diversity index by
considering a variational deï¬nition with respect to a generic mean, thereby extending the notion
of Sibsonâ€™s information radius. The variational deï¬nition applies to any arbitrary distance and
yields a new way to deï¬ne a Jensen-Shannon symmetrization of distances. When the variational
optimization is further constrained to belong to prescribed families of probability measures, we
get relative Jensen-Shannon divergences and their equivalent Jensen-Shannon symmetrizations of
distances that generalize the concept of information projections. Finally, we touch upon applications
of these variational Jensen-Shannon divergences and diversity indices to clustering and quantization
tasks of probability measures, including statistical mixtures.
Keywords: Jensen-Shannon divergence; diversity index; RÃ©nyi entropy; information radius; information
projection; exponential family; Bregman divergence; Fenchelâ€“Young divergence; Bregman information;
q-exponential family; q-divergence; Bhattacharyya distance; centroid; clustering
1. Introduction: Background and Motivations
The goal of the author is to methodologically contribute to an extension of the Sib-
sonâ€™s information radius [1] and also concentrate on analysis of the speciï¬ed families of
distributions called exponential families [2].
Let (X , F) denote a measurable space [3] with sample space X and Ïƒ-algebra F on
the set X . The Jensen-Shannon divergence [4] (JSD) between two probability measures P
and Q (or probability distributions) on (X , F) is deï¬ned by:
DJS[P, Q] := 1
2

DKL

P : P + Q
2

+ DKL

Q : P + Q
2

,
(1)
where DKL denotes the Kullbackâ€“Leibler divergence [5,6] (KLD):
DKL[P : Q] :=
( R
X log
 dP(x)
dQ(x)

dP,
P â‰ªQ
+âˆ,
P Ì¸â‰ªQ
(2)
where P â‰ªQ means that P is absolutely continuous with respect to Q [3], and dP
dQ is the
Radonâ€“Nikodym derivative of P with respect to Q. Equation (2) can be rewritten using the
chain rule as:
DKL[P : Q] :=
( R
X
dP(x)
dQ(x) log
 dP(x)
dQ(x)

dQ,
P â‰ªQ
+âˆ,
P Ì¸â‰ªQ
(3)
Consider a measure Âµ for which both the Radonâ€“Nikodym derivatives p :=
dP
dÂµ
and q := dP
dÂµ exist (e.g., Âµ = P+Q
2 ). Subsequently the Kullbackâ€“Leibler divergence can
be rewritten as (see Equation (2.5) page 5 of [5] and page 251 of the Cover & Thomasâ€™
textbook [6]):
Entropy 2021, 23, 464. https://doi.org/10.3390/e23040464
https://www.mdpi.com/journal/entropy

Entropy 2021, 23, 464
2 of 28
DKL[p : q] :=
Z
X p(x) log
 p(x)
q(x)

dÂµ(x).
(4)
Denote by D = D(X ) the set of all densities with full support X (Radonâ€“Nikodym
derivatives of probability measures with respect to Âµ):
D(X ) :=

p : X â†’R : p(x) > 0 Âµ-almost everywhere,
Z
X p(x)dÂµ(x) = 1

.
Subsequently, the Jensen-Shannon divergence [4] between two densities p and q of D is
deï¬ned by:
DJS[p, q] := 1
2

DKL

p : p + q
2

+ DKL

q : p + q
2

.
(5)
Often, one considers the Lebesgue measure [3] Âµ = ÂµL on (Rd, B(Rd)), where B(Rd) is
the Borel Ïƒ-algebra, or the counting measure [3] Âµ = Âµ# on (X , 2X ) where X is a countable
set, for deï¬ning the measure space (X , F, Âµ).
The JSD belongs to the class of f-divergences [7â€“9] which are known as the invariant
decomposable divergences of information geometry (see [10], pp. 52â€“57). Although the
KLD is asymmetric (i.e., DKL[p : q] Ì¸= DKL[q : p]), the JSD is symmetric (i.e., DJS[p, q] =
DJS[q, p]). The notation â€˜:â€™ is used as a parameter separator to indicate that the parameters
are not permutation invariant, and that the order of parameters is important.
In this work, a distance D(O1 : O2) is a measure of dissimilarity between two objects
O1 and O2, which do not need to be symmetric or satisfy the triangle inequality of metric
distances. A distance only satisï¬es the identity of indiscernibles: D(O1 : O2) = 0 if and
only if O1 = O2. When the objects O1 and O2 are probability densities with respect to
Âµ, we call this distance a statistical distance, use the brackets to enclose the arguments
of the statistical distance (i.e., D[O1 : O2]), and we have D[O1 : O2] = 0 if and only if
O1(x) = O2(x) Âµ-almost everywhere.
The 2-point JSD of Equation (4) can be extended to a weighted set of n densities P :=
{(w1, p1), . . . , (wn, pn)} (with positive wiâ€™s normalized to sum up to unity, i.e., âˆ‘n
i=1 wi = 1)
thus providing a diversity index, i.e., a n-point JSD for P:
DJS(P) :=
n
âˆ‘
i=1
wiDKL[pi : Â¯p],
(6)
where Â¯p := âˆ‘n
i=1 wipi denotes the statistical mixture [11] of the densities of P. We have
DJS[p : q] = DJS({( 1
2, p), ( 1
2, q)}). We call DJS(P) the Jensen-Shannon diversity index.
The KLD is also called the relative entropy since it can be expressed as the difference
between the cross entropy h[p : q] and the entropy h[p]:
DKL[p : q]
:=
Z
X p(x) log
 p(x)
q(x)

dÂµ(x)
(7)
=
Z
X p(x) log p(x)dÂµ(x) âˆ’
Z
X p(x) log q(x)dÂµ(x),
(8)
=
h[p : q] âˆ’h[p],
(9)
with the cross-entropy and entropy deï¬ned, respectively, by
h[p : q]
:=
âˆ’
Z
X p(x) log q(x)dÂµ(x),
(10)
h[p]
:=
âˆ’
Z
X p(x) log p(x)dÂµ(x).
(11)
Because h[p] = h[p : p], we may say that the entropy is the self-cross-entropy.

Entropy 2021, 23, 464
3 of 28
When Âµ is the Lebesgue measure, the Shannon entropy is also called the differential
entropy [6]. Although the discrete entropy H[p] = âˆ’âˆ‘i pi log pi (i.e., entropy with respect
to the counting measure) is always positive and bounded by log |X |, the differential entropy
may be negative (e.g., entropy of a Gaussian distribution with small variance).
The Jensen-Shannon divergence of Equation (6) can be rewritten as:
DJS[p, q] = h[ Â¯p] âˆ’
n
âˆ‘
i=1
wih[pi] := Jâˆ’h[p, q].
(12)
The JSD representation of Equation (12) is a Jensen divergence [12] for the strictly convex
negentropy F(p) = âˆ’h[p], since the entropy function h[.] is strictly concave. Therefore, it is
appropriate to call this divergence the Jensen-Shannon divergence.
Because pi(x)
Â¯p(x) â‰¤
pi(x)
wipi(x) =
1
wi , it can be shown that the Jensen-Shannon diversity
index is upper bounded by H(w) := âˆ’âˆ‘n
i=1 wi log wi, the discrete Shannon entropy. Thus,
the Jensen-Shannon diversity index is bounded by log n, and the 2-point JSD is bounded by
log 2, although the KLD is unbounded and it may even be equal to +âˆwhen the deï¬nite
integral diverges (e.g., KLD between the standard Cauchy distribution and the standard
Gaussian distribution). Another nice property of the JSD is that its square root yields a
metric distance [13,14]. This property further holds for the quantum JSD [15]. The JSD
has gained interest in machine learning. See, for example, the Generative Adversarial
Networks [16] (GANs) in deep learning [17], where it was proven that minimizing the
GAN objective function by adversarial training is equivalent to minimizing a JSD.
To delineate the different roles that are played by the factor 1
2 in the ordinary Jensen-
Shannon divergence (i.e., in weighting the two KLDs and in weighting the two densities), let
us introduce two scalars Î±, Î² âˆˆ(0, 1), and deï¬ne a generic (Î±, Î²)-skewed Jensen-Shannon
divergence, as follows:
DJS,Î±,Î²[p : q]
:=
(1 âˆ’Î²)DKL[p : mÎ±] + Î²DKL[q : mÎ±],
(13)
=
(1 âˆ’Î²)h[p : mÎ±] + Î²h[q : mÎ±] âˆ’(1 âˆ’Î²)h[p] âˆ’Î²h[q],
(14)
=
h[mÎ² : mÎ±] âˆ’((1 âˆ’Î²)h[p] + Î²h[q]),
(15)
where mÎ± := (1 âˆ’Î±)p + Î±q and mÎ² := (1 âˆ’Î²)p + Î²q. This identity holds, because DJS,Î±,Î²
is bounded by (1 âˆ’Î²) log
1
1âˆ’Î± + Î² log 1
Î±, see [18]. Thus, when Î² = Î±, we have DJS,Î±[p, q] =
DJS,Î±,Î±[p, q] = h[mÎ±] âˆ’((1 âˆ’Î±)h[p] + Î±h[q]), since the self-cross entropy corresponds to the
entropy: h[mÎ± : mÎ±] = h[mÎ±].
A f-divergence [9,19,20] is deï¬ned for a convex generator f, which is strictly convex
at 1 (to satisfy the identity of the indiscernibles) and that satisï¬es f (1) = 0, by
If [p : q] :=
Z
p(x) f
 q(x)
p(x)

dÂµ(x) â‰¥f (1) = 0,
(16)
where the right-hand-side follows from Jensenâ€™s inequality [20]. For example, the total
variation distance DTV[p : q] = 1
2
R
X |p(x) âˆ’q(x)|dÂµ(x) is a f-divergence for the generator
fTV(u) = |u âˆ’1|: DTV[p : q] = IfTV[p : q]. The generator fTV(u) is convex on R, strictly
convex at 1, and it satisï¬es f (u) = 1.
The DJS,Î±,Î² divergence is a f-divergence
DJS,Î±,Î²[p : q] = IfJS,Î±,Î²[p : q],
(17)
for the generator:
fJS,Î±,Î²(u) = âˆ’

(1 âˆ’Î²) log(Î±u + (1 âˆ’Î±)) + Î²u log
1 âˆ’Î±
u
+ Î±

.
(18)

Entropy 2021, 23, 464
4 of 28
We check that the generator fJS,Î±,Î² is strictly convex, since, for any a âˆˆ(0, 1) and b âˆˆ(0, 1),
we have
f â€²â€²
JS,Î±,Î²(u) =
a2(1 âˆ’b)u + (a âˆ’1)2b
a2u3 + 2a(1 âˆ’a)u2 + (a âˆ’1)2u > 0,
(19)
when u > 0.
The Jensen-Shannon principle of taking the average of the (Kullbackâ€“Leibler) di-
vergences between the source parameters to the mid-parameter can be applied to other
distances. For example, the Jensenâ€“Bregman divergence is a Jensen-Shannon symmetriza-
tion of the Bregman divergence BF [12]:
BJS
F (Î¸1 : Î¸2) := 1
2

BF

Î¸1 : Î¸1 + Î¸2
2

+ BF

Î¸2 : Î¸1 + Î¸2
2

,
(20)
where the Bregman divergence [21] BF is deï¬ned by
BF(Î¸ : Î¸â€²) := F(Î¸) âˆ’F(Î¸â€²) âˆ’(Î¸ âˆ’Î¸â€²)âŠ¤âˆ‡F(Î¸â€²).
(21)
The Jensenâ€“Bregman divergence BJS
F can also be written as an equivalent Jensen
divergence JF:
BJS
F (Î¸1 : Î¸2) = JF(Î¸1 : Î¸2) := F(Î¸1) + F(Î¸2)
2
âˆ’F
Î¸1 + Î¸2
2

,
(22)
where F is a strictly convex function ensuring JF(Î¸1 : Î¸2) â‰¥0 with equality if Î¸1 = Î¸2.
Because of its use in various ï¬elds of information sciences [22], various generalizations
of the JSD have been proposed: These generalizations are either based on Equation (5) [23]
or Equation (12) [18,24,25].
For example, the (arithmetic) mixture Â¯p = âˆ‘i wipi in
Equation (6) was replaced by an abstract statistical mixture with respect to a generic mean
M in [23] (e.g., the geometric mixture induced by the geometric mean), and the two KLDS
deï¬ning the JSD in Equation (5) was further averaged using another abstract mean N,
thus yielding the following generic (M, N)-Jensen-Shannon divergence [23] (abbreviated as
(M, N)-JSD):
DM,N
JS
[p : q] := N

DKL
h
p : (pq)M
1
2
i
, DKL
h
q : (pq)M
1
2
i
,
(23)
where (pq)M
Î± denotes the statistical weighted M-mixture:
(pq)M
Î± :=
MÎ±(p(x), q(x))
R
X MÎ±(p(x), q(x))dÂµ(x).
(24)
Notice that, when M = N = A (the arithmetic mean), Equation (23) of the (A, A)-JSD
reduces to the ordinary JSD of Equation (5). When the means M and N are symmetric,
the (M, N)-JSD is symmetric.
In general, a weighted mean MÎ±(a, b) for any Î± âˆˆ[0, 1] shall satisfy the in-betweeness
property [26] (i.e., a mean should be contained inside its extrema):
min{a, b} â‰¤MÎ±(a, b) â‰¤max{a, b}.
(25)
The three Pythagorean means deï¬ned for positive scalars a > 0 and b > 0 are classic
examples of means:
â€¢
The arithmetic mean A(a, b) = a+b
2 ,
â€¢
the geometric mean G(a, b) =
âˆš
ab, and
â€¢
the harmonic mean H(a, b) = 2ab
a+b.
These Pythagorean means may be interpreted as special instances of another paramet-
ric family of means: The power means

Entropy 2021, 23, 464
5 of 28
PÎ±(a, b) :=
 aÎ± + bÎ±
2
 1
Î±
,
(26)
deï¬ned for Î± âˆˆR\{0} (also called HÃ¶lder means). The power means can be extended
to the full range Î± âˆˆR by using the property that limÎ±â†’0 PÎ±(a, b) = G(a, b). The power
means are homogeneous means: PÎ±(Î»a, Î»b) = Î»PÎ±(a, b) for any Î» > 0. We refer to the
handbook of means [27] to obtain deï¬nitions and principles of other means beyond these
power means.
A weighted mean (also called barycenter) can be built from a non-weighted mean
M(a, b) (i.e., Î± = 1
2) by using the dyadic expansion of the real weight Î± âˆˆ[0, 1], see [28]. That
is, we can deï¬ne the weighted mean M(p, q; w, 1 âˆ’w) for w =
i
2k with i âˆˆ{0, . . . , 2k} and
k an integer. For example, consider a symmetric mean M(p, q) = M(q, p). Subsequently,
we get the following weighted means when k = 3:
M

p, q; 0
8 = 0, 8
8 = 1

=
q
M

p, q; 1
8, 7
8

=
M(M(M(p, q), q), q)
M

p, q; 2
8 = 1
4, 6
8 = 3
4

=
M(M(p, q), q)
M

p, q; 3
8, 5
8

=
M(M(M(p, q), p), q)
M

p, q; 4
8 = 1
2, 4
8 = 1
2

=
M(p, q)
M

p, q; 5
8, 3
8

=
M(M(M(p, q), q), p)
M

p, q; 6
8 = 3
4, 2
8 = 1
4

=
M(M(p, q), p)
M

p, q; 7
8, 1
8

=
M(M(M(p, q), p), p)
M

p, q; 8
8 = 1, 0
8 = 0

=
p
Let w = âˆ‘âˆ
i=1
di
2i be the unique dyadic expansion of the real number w âˆˆ(0, 1), where
the diâ€™s are binary digits (i.e., di âˆˆ{0, 1}). We deï¬ne the weighted mean M(x, y; w, 1 âˆ’w)
of two positive reals p and q for a real weight w âˆˆ(0, 1) as
M(x, y; w, 1 âˆ’w) := lim
nâ†’âˆM
 
x, y;
n
âˆ‘
i=1
di
2i , 1 âˆ’
n
âˆ‘
i=1
di
2i
!
.
(27)
Choosing the abstract mean M in accordance with the family R = {pÎ¸ : Î¸ âˆˆÎ˜} of the
densities allows one to obtain closed-form formula for the (M, N)-JSDs that rely on deï¬nite
integral calculations [23]. For example, the JSD between two Gaussian densities does not
admit a closed-form formula because of the log-sum integral, but the (G, N)-JSD admits
a closed-form formula when using geometric statistical mixtures (i.e., when M = G).
The calculus trick is to ï¬nd a weighted mean MÎ±, such that, for two densities pÎ¸1 and
pÎ¸2, the weighted mean distribution MÎ±(pÎ¸1(x), pÎ¸2(x)) =
pÎ¸1,2,Î±(x)
ZMÎ±(Î¸1,Î¸2), where ZMÎ±(Î¸1, Î¸2)
is the normalizing coefï¬cient and pÎ¸1,2,Î± âˆˆR. Thus, the integral calculation can be sim-
ply calculated as R
MÎ±(pÎ¸1(x), pÎ¸2(x))dÂµ(x) =
1
ZMÎ±(Î¸1,Î¸2) since pÎ¸1,2,Î±(x), and, therefore,
R
pÎ¸1,2,Î±(x)dÂµ(x) = 1. This trick has also been used in Bayesian hypothesis testing for
upper bounding the probability of error between two densities of a parametric family of

Entropy 2021, 23, 464
6 of 28
distributions by replacing the usual geometric mean (Section 11.7 of [6], page 375) by a
more general quasi-arithmetic mean [29]. For example, the harmonic mean is well-suited
to Cauchy distributions, and the power means to Student t-distributions [29].
As an application of these generalized JSDs, Deasy et al. [30] used the skewed ge-
ometric JSD (namely, the (GÎ±, A1âˆ’Î±)-JSD for Î± âˆˆ(0, 1)), which admits a closed-form
formula between normal densities [23], and showed how regularizing an optimization
task with this G-JSD divergence improved reconstruction and generation of Variational
AutoEncoders (VAEs).
More generally, instead of using the KLD, one can also use any arbitrary distance D to
deï¬ne its JS-symmetrization, as follows:
DJS
M,N[p : q] := N

D
h
p : (pq)M
1
2
i
, D
h
q : (pq)M
1
2
i
.
(28)
These symmetrizations may further be skewed by using MÎ± and/or NÎ² for Î± âˆˆ(0, 1) and
Î² âˆˆ(0, 1), yielding the deï¬nition [23]:
DJS
MÎ±,NÎ²[p : q] := NÎ²

D
h
p : (pq)M
Î±
i
, D
h
q : (pq)M
Î±
i
.
(29)
With these notations, the ordinary JSD is DJS = DKL
JS
A,A, the (A, A) JS-symmetrization of
the KLD with respect to the arithmetic means M = A and N = A.
The JS-symmetrization can be interpreted as the NÎ²-Jeffreysâ€™ symmetrization of a
generalization of Linâ€™s Î±-skewed K-divergence [4] DK
MÎ±[p : q]:
DJS
MÎ±,NÎ²[p : q]
=
NÎ²(DK
MÎ±[p : q], DK
MÎ±[p : q]),
(30)
DK
MÎ±[p : q]
:=
D
h
p : (pq)MÎ±
Î±
i
.
(31)
In this work, we consider symmetrizing an arbitrary distance D (including the KLD),
generalizing the Jensen-Shannon divergence by using a variational formula for the JSD.
Namely, we observe that the Jensen-Shannon divergence can also be deï¬ned as the follow-
ing minimization problem:
DJS[p, q] := min
câˆˆD
1
2(DKL[p : c] + DKL[q : c]),
(32)
since the optimal density c is proven unique using the calculus of variation [1,31,32] and it
corresponds to the mid density p+q
2 , a statistical (arithmetic) mixture.
Proof. Let S(c) = DKL[p : c] + DKL[q : c] â‰¥0. We use the method of the Lagrange multi-
pliers for the constrained optimization problem minc S(c) such that R
c(x)dÂµ(x) = 1. Let
us minimize S(c) + Î»(
R
c(x)dÂµ(x) âˆ’1). The density c realizing the minimum S(c) satisï¬es
the Eulerâ€“Lagrange equation âˆ‚L
âˆ‚c = 0, where L(c) := p log p
c + q log q
c + Î»c is the Lagrangian.
That is, âˆ’p
c âˆ’q
c + Î» = 0 or, equivalently, c = 1
Î»(p + q). Parameter Î» is then evaluated from
the constraint R
X c(x)dÂµ(x) = 1: we get Î» = 2 since R
X (p(x) + q(x))dÂµ(x) = 2. Therefore,
we ï¬nd that c(x) = p(x)+q(x)
2
, the mid density of p(x) and q(x).
Considering Equation (32) instead of Equation (5) for deï¬ning the Jensen-Shannon
divergence is interesting, because it allows one to consider a novel approach for general-
izing the Jensen-Shannon divergence. This variational approach was ï¬rst considered by
Sibson [1] to deï¬ne the Î±-information radius of a set of weighted distributions while using
RÃ©nyi Î±-entropies that are based on RÃ©nyi principled Î±-means [33]. The Î±-information
radius includes the Jensen-Shannon diversity index when Î± = 1. Sibsonâ€™s work is our
point of departure for generalizing the Jensen-Shannon divergence and proposing the
Jensen-Shannon symmetrizations of arbitrary distances.

Entropy 2021, 23, 464
7 of 28
The paper is organized, as follows: in Section 2, we recall the rationale and deï¬nitions
of the RÃ©nyi Î±-entropy and the RÃ©nyi Î±-divergence [33], and explain the information radius
of Sibson [1], which includes, as a special case, the ordinary Jensen-Shannon divergence
and that can be interpreted as generalized skew Bhattacharyya distances. We report,
in Theorem 2, a closed-form formula for calculating the information radius of order Î±
between two densities of an exponential family when 1
Î± is an integer. It is noteworthy
to point out that Sibsonâ€™s work (1969) includes, as a particular case of the information
radius, a deï¬nition of the JSD, prior to the well-known reference paper of Lin [4] (1991).
In Section 3, we present the JS-symmetrization variational deï¬nition that is based on a
generalization of the information radius with a generic mean (Equation (88) and Deï¬ni-
tion 3). In Section 4, we constrain the mixture density to belong to a prescribed class of
(parametric) probability densities, like an exponential family [2], and obtain a relative
information radius generalizing information radius and related to the concept of informa-
tion projections. Our Deï¬nition 5 generalizes the (relative) normal information radius of
Sibson [1], who considered the multivariate normal family (Proposition 4). We illustrate
this notion of relative information radius by calculating the density of an exponential family
minimizing the reverse Kullbackâ€“Leibler divergence between a mixture of densities of that
exponential family (Proposition 6). Moreover, we get a semi-closed-form formula for the
Kullbackâ€“Leibler divergence between the densities of two different exponential families
(Proposition 5), generalizing the Fenchelâ€“Young divergence [34]. As an application of these
relative variational JSDs, we touch upon the problems of clustering and quantization of
probability densities in Section 4.2. Finally, we conclude by summarizing our contributions
and discussing related works in Section 5.
2. RÃ©nyi Entropy and Divergence, and Sibson Information Radius
RÃ©nyi [33] investigated a generalization of the four axioms of Fadeev [35], yielding
the unique Shannon entropy [20]. In doing so, RÃ©nyi replaced the ordinary weighted
arithmetic mean by a more general class of averaging schemes. Namely, RÃ©nyi considered
the weighted quasi-arithmetic means [36]. A weighted quasi-arithmetic mean can be
induced by a strictly monotonous and continuous function g, as follows:
Mg(x1, . . . , xn; w1, . . . , wn) := gâˆ’1
 
n
âˆ‘
i=1
wig(xi)
!
,
(33)
where the xiâ€™s and the wiâ€™s are positive (the weights are normalized, so that âˆ‘n
i=1 wi = 1).
Because Mg = Mâˆ’g, we may assume without loss of generality that g is a strictly increasing
and continuous function. The quasi-arithmetic means were investigated independently by
Kolmogorov [36], Nagumo [37], and de Finetti [38].
For example, the power means PÎ±(a, b) =

aÎ±+bÎ±
2
 1
Î± introduced earlier are quasi-
arithmetic means for the generator gP
Î± (u) := uÎ±:
PÎ±(a, b) = MgPÎ±

a, b; 1
2, 1
2

.
(34)
RÃ©nyi proved that, among the class of weighted quasi-arithmetic means, only the
means induced by the family of functions
gÎ±(u)
:=
2(Î±âˆ’1)u,
(35)
gâˆ’1
Î± (v)
:=
1
Î± âˆ’1 log2 v,
(36)

Entropy 2021, 23, 464
8 of 28
for Î± > 0 and Î± Ì¸= 1 yield a proper generalization of Shannon entropy, nowadays called the
RÃ©nyi Î±-entropy. The RÃ©nyi Î±-mean is
MR
Î± (x1, . . . , xn; w1, . . . , wn)
=
MgÎ±(x1, . . . , xn; w1, . . . , wn),
(37)
=
1
Î± âˆ’1 log2
 
n
âˆ‘
i=1
wi2(Î±âˆ’1)xi
!
.
(38)
The RÃ©nyi Î±-means MR
Î± are not power means: They are not homogeneous means [31].
Let MR
Î± (p, q) = MR
Î±

p, q; 1
2, 1
2

=
1
Î±âˆ’1 log2
2(Î±âˆ’1)p+2(Î±âˆ’1)q
2
. Subsequently, we have limÎ±â†’âˆ
MR
Î± (p, q) = max{p, q} and limÎ±â†’1 MR
Î± (p, q) = A(p, q) = p+q
2 . Indeed, we have
MR
Î± (p, q)
=
1
Î± âˆ’1 log2
2(Î±âˆ’1)p + 2(Î±âˆ’1)q
2
,
=
1
Î± âˆ’1 log2
e(Î±âˆ’1)p log 2 + e(Î±âˆ’1)q log 2
2
,
â‰ˆÎ±â†’1
1
Î± âˆ’1 log2

1 + (Î± âˆ’1) p + q
2
log 2

,
â‰ˆÎ±â†’1
1
Î± âˆ’1
1
log 2(Î± âˆ’1) p + q
2
log 2,
â‰ˆÎ±â†’1
p + q
2
= A(p, q),
using the following ï¬rst-order approximations: ex â‰ˆxâ†’0= 1 + x and log(1 + x) â‰ˆxâ†’0= x.
To obtain an intuition of the RÃ©nyi entropy, we may consider generalized entropies
derived from quasi-arithmetic means, as follows:
hg[p] := âˆ’Mg(log2 p1, . . . , log2 pn; p1, . . . , pn).
(39)
When g(u) = u, we recover Shannon entropy. When g2(u) = 2u, we get hg2[p] =
âˆ’log2 âˆ‘i p2
i , called the collision entropy, since âˆ’log Pr[X1 = X2] = hg2[p], when X1
and X2 are independent and identically distributed random variables with X1 âˆ¼p and
X2 âˆ¼p. When g(u) = gÎ±(u) = 2(Î±âˆ’1)u, we get
hgÎ±[p]
=
âˆ’
1
Î± âˆ’1 log2
 
âˆ‘
i
pi2(Î±âˆ’1) log2 pi
!
,
(40)
=
1
1 âˆ’Î± log2 âˆ‘
i
pipÎ±âˆ’1
i
=
1
1 âˆ’Î± log2 âˆ‘
i
pÎ±
i .
(41)
The formula of Equation (41) is the discrete RÃ©nyi Î±-entropy [33], which can be deï¬ned
more generally on a measure space (X , F, Âµ), as follows:
hR
Î± [p] :=
1
1 âˆ’Î± log
Z
X pÎ±(x)dÂµ(x)

,
Î± âˆˆ(0, 1) âˆª(1, âˆ).
(42)
In the limit case Î± â†’1, the RÃ©nyi Î±-entropy converges to Shannon entropy: limÎ±â†’1 hR
Î± [p] =
h[p]. RÃ©nyi Î±-entropies are non-increasing with respect to increasing Î±: hR
Î± [p] â‰¥hR
Î±â€²[p] for
Î± < Î±â€². In the discrete case (i.e., counting measure Âµ on a ï¬nite alphabet X ), we can further
deï¬ne h0[p] = log |X | for Î± = 0 (also called max-entropy or Hartley entropy). The RÃ©nyi
+âˆ-entropy
h+âˆ[p] = âˆ’log max
xâˆˆX p(x)
is also called the min-entropy, since the sequence hÎ± is non-increasing with respect to
increasing Î±.

Entropy 2021, 23, 464
9 of 28
Similarly, RÃ©nyi obtained the Î±-divergences for Î± > 0 and Î± Ì¸= 1 (originally called
information gain of order Î±):
DR
Î± [p : q] :=
1
Î± âˆ’1 log2
Z
X p(x)Î±q(x)1âˆ’Î±dÂµ(x)

,
(43)
generalizing the Kullbackâ€“Leibler divergence, since limÎ±â†’1 DR
Î± [p : q] = DKL[p : q]. RÃ©nyi
Î±-divergences are non-decreasing with respect to increasing Î± [39]: DR
Î± [p : q] â‰¤DR
Î±â€²[p : q]
for Î±â€² â‰¥Î±.
Sibson (Robin Sibson (1944â€“2017) is also renown for inventing the natural neighbour
interpolation [40]) [1] considered both the RÃ©nyi Î±-divergence [33] DR
Î± and the RÃ©nyi Î±-
weighted mean MR
Î± := MgÎ± to deï¬ne the information radius RÎ± of order Î± of a weighted
set P = {(wi, pi)}n
i=1 of densities piâ€™s as the following minimization problem:
RÎ±(P) := min
câˆˆD RÎ±(P, c),
(44)
where
RÎ±(P, c) := MR
Î±

DR
Î± [p1 : c], . . . , DR
Î± [pn : c]; w1, . . . , wn

.
(45)
The RÃ©nyi Î±-weighted mean MR
Î± can be rewritten as
MR
Î± (x1, . . . , xn; w1, . . . , wn)
=
1
Î± âˆ’1LSE((Î± âˆ’1)x1 log 2 + log w1, . . . , (Î± âˆ’1)xi log 2 + log wi),
(46)
where function LSE(a1, . . . , an) := log(âˆ‘n
i=1 eai) denotes the log-sum-exp (convex) func-
tion [41,42].
Notice that 2(Î±âˆ’1)DRÎ± [p:q] = R
X p(x)Î±q(x)1âˆ’Î±dÂµ(x), the Bhattacharyya Î±-coefï¬cient [12]
(also called Chernoff Î±-coefï¬cient [43,44]):
CBhat,Î±[p : q] :=
Z
X p(x)Î±q(x)1âˆ’Î±dÂµ(x).
(47)
Thus, we have
RÎ±(P, c) =
1
Î± âˆ’1 log2
 âˆ‘wiCBhat,Î±[pi : c]

.
(48)
The ordinary Bhattacharyya coefï¬cient is obtained for Î± = 1
2: CBhat[p : q] := R
X
p
p(x)
p
q(x)dÂµ(x).
Sibson [1] also considered the limit case Î± â†’âˆwhen deï¬ning the information radius:
DR
âˆ[p : q] := log2 sup
xâˆˆX
p(x)
q(x) .
(49)
Sibson reported the following theorem in his information radius study [1]:
Theorem 1 (Theorem 2.2 and Corollary 2.3 of [1]). The optimal density câˆ—
Î± = arg mincâˆˆD RÎ±
(P, c) is unique, and we have:
câˆ—
1(x) = âˆ‘i wipi(x),
R1(P) = R1(P, câˆ—
1) = R
X âˆ‘i wipi log2
pi
âˆ‘j wjpj(x)dÂµ(x),
câˆ—Î±(x) =
(âˆ‘i wi pi(x)Î±)
1Î±
R
X (âˆ‘i wi pi(x)Î±)
1Î± dÂµ(x),
RÎ±(P) = RÎ±(P, câˆ—Î±) =
1
Î±âˆ’1 log2
R
X (âˆ‘i wipi(x)Î±)
1
Î± dÂµ(x)
Î±
,
Î± âˆˆ(0, 1) âˆª(1, âˆ)
câˆ—âˆ(x) =
maxi pi(x)
R
X (maxi pi(x))dÂµ(x),
Râˆ(P) = Râˆ(P, câˆ—âˆ) = log2
R
X (maxi pi(x))dÂµ(x),
Observe that Râˆ(P) does not depend on the (positive) weights.
The proof follows from the following decomposition of the information radius:

Entropy 2021, 23, 464
10 of 28
Proposition 1. We have:
RÎ±(P, c) âˆ’RÎ±(P, câˆ—
Î±) = DR
Î± (câˆ—
Î±, c) â‰¥0.
(50)
Because the proof is omitted in [1], we report it here:
Proof. Let âˆ†(c, câˆ—
Î±) := RÎ±(P, c) âˆ’RÎ±(P, câˆ—
Î±). We handle the three cases, depending on the
Î± values:
â€¢
Case Î± âˆˆ(0, 1) âˆª(1, âˆ): Let PÎ±(P)(x) := (âˆ‘i wipi(x)Î±)
1
Î± . We have (câˆ—
Î±(x))Î± =
âˆ‘i wipi(x)Î±
(
R
PÎ±(P)(x)dÂµ(x))Î± . We obtain
âˆ†(c, câˆ—
Î±)
=
1
Î± âˆ’1 log2
 
âˆ‘
i
wi
Z
pi(x)Î±c(x)1âˆ’Î±dÂµ(x)
!
âˆ’
1
Î± âˆ’1 log2
Z
PÎ±(P)(x)dÂµ(x)
Î±
, (51)
=
1
Î± âˆ’1 log2
âˆ‘i wi
R
pi(x)Î±c(x)1âˆ’Î±dÂµ
(R
PÎ±(P)(x)dÂµ(x))Î±
,
(52)
=
1
Î± âˆ’1 log2
R (âˆ‘i wipi(x)Î±)c(x)1âˆ’Î±
(R
PÎ±(P)(x)dÂµ(x))Î± dÂµ(x),
(53)
=
1
Î± âˆ’1 log2
Z
(câˆ—
Î±(x))Î±c(x)1âˆ’Î±dÂµ(x),
(54)
:=
DR
Î± (câˆ—
Î±, c).
(55)
â€¢
Case Î± = 1: we have âˆ†(c, câˆ—
1) := R1(P, c) âˆ’R1(P, câˆ—
1) with câˆ—
1 = âˆ‘i wipi. Because
R1(P, c) = âˆ‘i wiDKL[pi : c], we have
R1(P, c)
= âˆ‘
i
wih[pi : c] âˆ’wih[pi],
(56)
=
h[âˆ‘
i
wipi : c] âˆ’âˆ‘
i
wih[pi],
(57)
=
h[câˆ—
1 : c] âˆ’âˆ‘
i
wih[pi].
(58)
It follows that
âˆ†(c, câˆ—
1)
=
h[câˆ—
1 : c] âˆ’âˆ‘
i
wih[pi] âˆ’
 
h[câˆ—
1 : câˆ—
1] âˆ’âˆ‘
i
wih[pi]
!
,
(59)
=
h[câˆ—
1 : c] âˆ’h[câˆ—
1],
(60)
=
DKL[câˆ—
1 : c] = DR
1 [câˆ—
1 : c].
(61)
â€¢
Case Î± = âˆ: we have câˆ—
âˆ=
maxi pi(x)
R (maxi pi(x))dÂµ(x), Râˆ(P, câˆ—
âˆ) = log2
R (maxi pi(x))dÂµ(x),
and DR
âˆ[p : q] = log2 supx
p(x)
q(x) . We have Râˆ(P, c) = log2 supx
pi(x)
c(x) Thus, âˆ†(c, câˆ—
Î±) :=
Râˆ(P, c) âˆ’Râˆ(P, câˆ—
âˆ) = log2 supx
câˆ—âˆ(x)
c(x) = DR
âˆ[câˆ—
âˆ: c].
It follows that
min
c
RÎ±(P, c) = min
c
RÎ±(P, câˆ—
Î±) + DR
Î± (câˆ—
Î±, c) â‰¡min
c
DR
Î± (câˆ—
Î±, c) â‰¥0.
Thus we have c = câˆ—
Î± since DR
Î± (câˆ—
Î±, c) is minimized for c = câˆ—
Î±.
Notice that câˆ—
âˆ(x) = max{p1(x),...,pn(x)}
R
X (maxi pi(x))dÂµ(x) is the upper envelope of the densities pi(x)â€™s
normalized to be a density. Provided that the densities piâ€™s intersect pairwise in at most
s locations (i.e., |{pi(x) âˆ©pj(x)}| â‰¤s for i Ì¸= j), we can efï¬ciently compute this upper
envelope using an output-sensitive algorithm [45] of computational geometry.
When the point set is P =
n
1
2, p

,

1
2, q
o
with w1 = w2 = 1
2, the information radius
deï¬nes a (2-point) symmetric distance, as follows:

Entropy 2021, 23, 464
11 of 28
R1(p, q) = 1
2
R
X p(x) log2
2p
p(x)+q(x)dÂµ(x) + 1
2
R
X q(x) log2
2q(x)
p(x)+q(x)dÂµ(x),
Î± = 1
RÎ±(p, q) =
Î±
Î±âˆ’1 log2
R
X
 p(x)Î±+q(x)Î±
2
 1
Î± dÂµ(x) =
Î±
Î±âˆ’1 log2
R
X PÎ±(p(x), q(x))dÂµ(x),
Î± âˆˆ(0, 1) âˆª(1, âˆ)
Râˆ(p, q) = log2
R
X max{p(x), q(x)}dÂµ(x),
Î± = âˆ.
This family of symmetric divergences may be called the Sibsonâ€™s Î±-divergences,
and the Jensen-Shannon divergence is interpreted as a limit case when Î± â†’1. No-
tice that, since we have limÎ±â†’âˆPÎ±(p, q) = max{p, q} and limÎ±â†’âˆ
Î±
Î±âˆ’1 = 1, we have
limÎ±â†’âˆRÎ±(p, q) = Râˆ(p, q). Notice that, for Î± = 1, the integral and logarithm operations
are swapped as compared to RÎ± for Î± âˆˆ(0, 1) âˆª(1, âˆ).
Theorem 2. When Î± = 1
k for an integer k â‰¥2, the Sibson Î±-divergences between two densities
pÎ¸1 and pÎ¸2 of an exponential family {pÎ¸ : Î¸ âˆˆÎ˜} with cumulant function F(Î¸) is available in
closed form:
RÎ±(pÎ¸1, pÎ¸2) = âˆ’
1
k âˆ’1 log2
 
1
2k
k
âˆ‘
i=0
k
i

exp

F
 i
k Î¸1 +

1 âˆ’i
k

Î¸2

âˆ’
 i
k F(Î¸1) +

1 âˆ’i
k

F(Î¸2)
!
.
Proof. Let p = pÎ¸1 and q = pÎ¸2 be two densities of an exponential family [2] with cumulant
function F(Î¸) and natural parameter space Î˜. Without a loss of generality, we may consider
a natural exponential family [2] with densities written canonically as pÎ¸(x) = exp(xâŠ¤Î¸ âˆ’
F(Î¸)) for Î¸ âˆˆÎ˜. It can be shown that the cumulant function F(Î¸) = log R
X exp(xâŠ¤Î¸)dÂµ(x)
is strictly convex and analytic on the open convex natural parameter space Î˜ [2].
When Î± = 1
2 (i.e., k = 2), we have:
R 1
2 (p, q)
=
âˆ’log2
Z
X
 p
p(x) +
p
q(x)
2
!2
dÂµ(x),
(62)
=
âˆ’log2
1
2 + 1
2
Z
X
q
p(x)
q
q(x)dÂµ(x)

,
(63)
=
âˆ’log2
1
2 + 1
2CBhat[p : q]

â‰¥0,
(64)
where CBhat[p : q] := R
X
p
p(x)
p
q(x)dÂµ(x) is the Bhattacharyya coefï¬cient (with 0 â‰¤
CBhat[p : q] â‰¤1). Using Theorem 3 of [12], we have
CBhat[pÎ¸1, pÎ¸2] = exp

F
Î¸p + Î¸q
2

âˆ’F(Î¸p) + F(Î¸q)
2

,
so that we obtain the following closed-form formula:
R 1
2 (pÎ¸1, pÎ¸2) = âˆ’log2
1
2 + 1
2 exp

F
Î¸p + Î¸q
2

âˆ’F(Î¸p) + F(Î¸q)
2

â‰¥0,
Now, assume that k = 1
Î± â‰¥2 is an arbitrary integer, and let us apply the binomial
expansion for PÎ±(pÎ¸1, pÎ¸2) in the spirit of [46,47]:
Z
X PÎ±(pÎ¸1(x), pÎ¸2(x))dÂµ(x)
=
Z
X
 
pÎ¸1(x)
1
k + pÎ¸2(x)
1
k
2
!k
dÂµ(x),
(65)
=
1
2k
k
âˆ‘
i=0
k
i
 Z
X

pÎ¸1(x)
1
k
i
pÎ¸2(x)
1
k
kâˆ’i
dÂµ(x). (66)

Entropy 2021, 23, 464
12 of 28
Let Ik,i(Î¸1, Î¸2) := R
X

pÎ¸1(x)
1
k
i
pÎ¸2(x)
1
k
kâˆ’i
dÂµ(x).
Because
i
kÎ¸1 + kâˆ’i
k Î¸2 = Î¸2 +
i
k(Î¸1 âˆ’Î¸2) âˆˆÎ˜ for i âˆˆ{0, . . . , k}, we get by following the calculation steps in [12]:
Ik,i(Î¸1, Î¸2) := exp

F
 i
k Î¸1 +

1 âˆ’i
k

Î¸2

âˆ’
 i
k F(Î¸1) +

1 âˆ’i
k

F(Î¸2)

< âˆ.
Notice that I2,1 = CBhat[pÎ¸1, pÎ¸2], and Ik,0 = Ik,k = 1.
Thus, we get the following closed-form formula:
RÎ±(pÎ¸1, pÎ¸2)
=
âˆ’
1
k âˆ’1 log2
 
1
2k
k
âˆ‘
i=0
k
i

exp

F
 i
k Î¸1 +

1 âˆ’i
k

Î¸2

âˆ’
 i
k F(Î¸1) +

1 âˆ’i
k

F(Î¸2)
!
. (67)
This closed-form formula applies, in particular, to the family {N (Âµ, Î£)} of (multivari-
ate) normal distributions: In this case, the natural parameters Î¸ are expressed using both a
vector parameter component v and a matrix parameter component M:
Î¸ = (v, M) =

Î£âˆ’1m, âˆ’1
2Î£âˆ’1

,
(68)
and the cumulant function is:
FN (Î¸) = d
2 log Ï€ âˆ’1
2 log | âˆ’2M| âˆ’1
4vâŠ¤Mâˆ’1v,
(69)
where | Â· | denotes the matrix determinant.
In general, the optimal density câˆ—
Î± = arg mincâˆˆD RÎ±(P, c) yielding the information
radius RÎ±(P) can be interpreted as a generalized centroid (extending the notion of FrÃ©chet
means [48]) with respect to (MR
Î± , DR
Î± ), where a (M, D)-centroid is deï¬ned by:
Deï¬nition 1 ((M, D)-centroid). Let P = {(w1, p1), . . . , (wn, pn)} be a normalized weighted
parameter set, M a mean, and D a distance. Subsequently, the (M, D)-centroid is deï¬ned as
cM,D(P) = arg min
c
M(D(p1 : c), . . . , D(pn : c); w1, . . . , wn).
Here, we give a general deï¬nition of the (M, D)-centroid for an arbitrary distance (not
necessarily a symmetric nor metric distance). The parameter set can either be probability
measures having densities with respect to a given measure Âµ or a set of vectors. In the
ï¬rst case, the distance D is called a statistical distance. When the densities belong to a
parametric family of densities P = {pÎ¸
: Î¸ âˆˆÎ˜}, the statistical distance D[pÎ¸1 : pÎ¸2]
amounts to a parameter distance: DP(Î¸1 : Î¸2) := D[pÎ¸1 : pÎ¸2]. For example, when all of the
densities piâ€™s belong to a same natural exponential family [2]
P = {pÎ¸(x) = exp(Î¸âŠ¤t(x) âˆ’F(Î¸)) : Î¸ âˆˆÎ˜}
with cumulant function F(Î¸) = log R
exp(Î¸âŠ¤t(x))dÂµ(x) (i.e., pi = pÎ¸i) and sufï¬cient
statistic vector t(x), we have DKL[pÎ¸ : pÎ¸i] = Bâˆ—
F(Î¸ : Î¸i) := BF(Î¸i : Î¸), where Bâˆ—
F denotes the
reverse Bregman divergence (by parameter order swapping) the Bregman divergence [21]
BF deï¬ned by
BF(Î¸ : Î¸â€²) := F(Î¸) âˆ’F(Î¸â€²) âˆ’(Î¸ âˆ’Î¸â€²)âŠ¤âˆ‡F(Î¸â€²).
(70)
Thus, we have DP(Î¸1 : Î¸2) := Bâˆ—
F(Î¸1 : Î¸2) = DKL[pÎ¸1 : pÎ¸2].
Let V = {(w1, Î¸1), . . . , (wn, Î¸n)} be the parameter set corresponding to P. Deï¬ne
RF(V, Î¸) :=
n
âˆ‘
i=1
wiBF(Î¸i : Î¸).
(71)

Entropy 2021, 23, 464
13 of 28
Subsequently, we have the equivalent decomposition of Proposition 1:
RF(V, Î¸) âˆ’RF(V, Î¸âˆ—) = BF(Î¸âˆ—: Î¸),
(72)
with Î¸âˆ—= Â¯Î¸ := âˆ‘n
i=1 wiÎ¸i. (this decomposition is used to prove Proposition 1 of [21]). The
quantity RF(V) = RF(V, Î¸âˆ—) was termed the Bregman information [21,49]. The Bregman
information generalizes the variance that was obtained when the Bregman divergence is
the squared Euclidean distance. RF(V) could also be called Bregman information radius
according to Sibson. Because RF(V) = âˆ‘n
i=1 wiDKL[p Â¯Î¸ : pÎ¸i], we can interpret the Bregman
information as a Sibsonâ€™s information radius for densities of an exponential family with
respect to the arithmetic mean MR
1 = A and the reverse Kullbackâ€“Leibler divergence:
Dâˆ—
KL[p : q] := DKL[q : p]. This observation yields us the JS-symmetrization of distances
based on generalized information radii in Section 3.
More generally, we may consider the densities belonging to a deformed q-exponential
family (see [10], page 85â€“89 and the monograph [50]). Deformed q-exponential families
generalize the exponential families, and include the q-Gaussians [10]. A common way
to measure the statistical distance between two densities of a q-exponential family is the
q-divergence [10], which is related to Tsallisâ€™ entropy [51]. We may also deï¬ne another
statistical divergence between two densities of a q-exponential family which amounts to
Bregman divergence. For example, we refer to [52] for details concerning the family of
Cauchy distributions, which are q-Gaussians for q = 2.
Sibson proved that the information radii of any order are all upper bounded (Theo-
rem 2.8 and Theorem 2.9 of [1]) as follows:
R1(P)
â‰¤âˆ‘
i
wi log2
1
wj
â‰¤log2 n < âˆ,
(73)
RÎ±(P)
â‰¤
Î±
Î± âˆ’1 log2
 
âˆ‘
i
w
1
Î±
i
!
â‰¤log2 n < âˆ,
Î± âˆˆ(0, 1) âˆª(1, âˆ)
(74)
Râˆ(P)
â‰¤
log2 n < âˆ.
(75)
We interpret Sibsonâ€™s upper bounds of Equations (73)â€“(75), as follows:
Proposition 2 (Information radius upper bound). The information radius of order Î± of a
weighted set of distributions is upper bounded by the discrete RÃ©nyi entropy of order 1
Î± of the weight
distribution: RÎ±(P) â‰¤HR
1
Î± [w], where HR
Î± [w] :=
1
1âˆ’Î± log
 âˆ‘i wÎ±
i

.
3. JS-Symmetrization of Distances Based on Generalized Information Radius
Let us give the following deï¬nitions generalizing the information radius (i.e., Jensen-
Shannon symmetrization of the distance when |P| = 2) and the ordinary Jensen-Shannon
divergence:
Deï¬nition 2 ((M, D)-information radius). Let M be a weighted mean and D a distance. Subse-
quently, the generalized information radius for a weighted set of points (e.g., vectors or densities)
(w1, p1), . . . , (wn, pn) is:
RM,D(P) := min
câˆˆD M(D(p1 : c), . . . , D(pn : c); w1, . . . , wn).
Recall that we also deï¬ned the (M, D)-centroid in Deï¬nition 1 as follows:
cM,D(P) := arg min
câˆˆD M(D(p1 : c), . . . , D(pn : c); w1, . . . , wn).
When M = A, we recover the notion of FrÃ©chet mean [48]. Notice that, although the
minimum RM,D(P) is unique, several generalized centroids cM,D(P) may potentially
exist, depending on (M, D). In particular, Deï¬nition 2 and Deï¬nition 1 apply when D is

Entropy 2021, 23, 464
14 of 28
a statistical distance, i.e., a distance between densities (Radonâ€“Nikodym derivatives of
corresponding probability measures with respect to a dominating measure Âµ).
The generalized information radius can be interpreted as a diversity index or an n-
point distance. When n = 2, we get the following (2-point) distances, which are considered
as a generalization of the Jensen-Shannon divergence or Jensen-Shannon symmetrization:
Deï¬nition 3 (M-vJS symmetrization of D). Let M be a mean and D a statistical distance.
Subsequently, the variational Jensen-Shannon symmetrization of D is deï¬ned by the formula of a
generalized information radius:
DvJS
M [p : q] := min
câˆˆD M(D[p : c], D[q : c]).
We use the acronym vJS to distinguish it with the JS-symmetrization reported in [23]:
DJS
M[p : q] = DJS
M,A[p : q] := 1
2

D
h
p : (pq)M
1
2
i
+ D
h
q : (pq)M
1
2
i
.
We recover Sibsonâ€™s information radius RÎ±[p : q] induced by two densities p and q
from Deï¬nition 3 as the MR
Î± -vJS symmetrization of the RÃ©nyi divergence DR
Î± . We have BF
vJS
A ,
which is the Bregman information [21]. Notice that we may skew these generalized JSDs
by taking weighted mean MÎ² instead of M for Î² âˆˆ(0, 1), yielding the general deï¬nition:
Deï¬nition 4 (Skew MÎ²-vJS symmetrization of D). Let MÎ² be a weighted mean and D a
statistical distance. Subsequently, the variational skewed Jensen-Shannon symmetrization of D is
deï¬ned by the formula of a generalized information radius:
DvJS
MÎ²[p : q] := min
câˆˆD MÎ²(D[p : c], D[q : c])
Example 1. For example, the skewed Jensenâ€“Bregman divergence of Equation (20) can be inter-
preted as a Jensen-Shannon symmetrization of the Bregman divergence BF [12] since we have:
BF
vJS
AÎ² (Î¸1 : Î¸2)
=
min
Î¸âˆˆÎ˜ AÎ²(BF(Î¸1 : Î¸), BF(Î¸2 : Î¸)),
(76)
=
min
Î¸âˆˆÎ˜(1 âˆ’Î²)BF(Î¸1 : Î¸) + Î²BF(Î¸2 : Î¸),
(77)
=
(1 âˆ’Î²)BF(Î¸1 : (1 âˆ’Î²)Î¸1 + Î²Î¸2) + Î²BF(Î¸2 : (1 âˆ’Î²)Î¸1 + Î²Î¸2), (78)
=:
JBF,Î²(Î¸1 : Î¸2).
(79)
Indeed, the Bregman barycenter arg minÎ¸âˆˆÎ˜(1 âˆ’Î²)BF(Î¸1 : Î¸) + BF(Î¸2 : Î¸) is unique and it
corresponds to Î¸ = (1 âˆ’Î²)Î¸1 + Î²Î¸2, see [21]. The skewed Jensenâ€“Bregman divergence JBF,Î²(Î¸1 :
Î¸2) can also be rewritten as an equivalent skewed Jensen divergence (see Equation (22)):
JBF,Î²(Î¸1 : Î¸2)
=
(1 âˆ’Î²)BF(Î¸1 : (1 âˆ’Î²)Î¸1 + Î²Î¸2) + Î²BF(Î¸2 : (1 âˆ’Î²)Î¸1 + Î²Î¸2), (80)
=
(1 âˆ’Î²)F(Î¸1) + Î²F(Î¸2) âˆ’F((1 âˆ’Î²)Î¸1 + Î²Î¸2),
(81)
=:
JF,Î²(Î¸1 : Î¸2).
(82)
Example 2. Consider a conformal Bregman divergence [53] that is deï¬ned by
BF,Ï(Î¸1 : Î¸2) = Ï(Î¸1)BF(Î¸1 : Î¸2),
(83)

Entropy 2021, 23, 464
15 of 28
where Ï(Î¸) > 0 is a conformal factor. Subsequently, we have
BF,Ï
vJS
AÎ² (Î¸1 : Î¸2)
=
min
Î¸âˆˆÎ˜ AÎ²
 BF,Ï(Î¸1 : Î¸), BF,Ï(Î¸2 : Î¸)

,
(84)
=
min
Î¸âˆˆÎ˜(1 âˆ’Î²)BF,Ï(Î¸1 : Î¸) + BF,Ï(Î¸2 : Î¸),
(85)
=
(1 âˆ’Î²)BF(Î¸1 : Î³1Î¸1 + Î³2Î¸2) + Î²BF(Î¸2 : Î³1Î¸1 + Î³2Î¸2),
(86)
where Î³1 =
(1âˆ’Î²)Ï(Î¸1)
(1âˆ’Î²)Ï(Î¸1)+Î²Ï(Î¸2) and Î³2 =
Î²Ï(Î¸2)
(1âˆ’Î²)Ï(Î¸1)+Î²Ï(Î¸2) = 1 âˆ’Î³1.
Notice that this deï¬nition is implicit and it can be made explicit when the centroid
câˆ—(p, q) is unique:
DvJS
MÎ²[p : q] = MÎ²(D[p : câˆ—(p, q)], D[q : câˆ—(p, q)]
(87)
In particular, when D = DKL, the KLD, we obtain generalized skewed Jensen-Shannon
divergences for MÎ² a weighted mean with Î² âˆˆ(0, 1):
D
MÎ²
vJS [p : q] := min
câˆˆD MÎ²(DKL[p : c], DKL[q : c]).
(88)
Example 3. Amari [31] obtained the (A, DÎ±)-information radius and its corresponding unique
centroid for DÎ±, the Î±-divergence of information geometry [10] (page 67).
Example 4. Brekelmans et al. [54] studied the geometric path (p1p2)G
Î² (x) âˆp1âˆ’Î²
1
(x)pÎ²
2(x)
between two distributions p1 and p2 of D, where GÎ²(a, b) = a1âˆ’Î²bÎ² (with a, b > 0) is the
weighted geometric mean. They proved the variational formula:
(p1p2)G
Î² = min
câˆˆD (1 âˆ’Î²)DKL[c : p1] + Î²DKL[c : p2].
(89)
That is, (p1p2)G
Î² is a GÎ²-Dâˆ—
KL centroid, where Dâˆ—
KL is the reverse KLD. The corresponding
(GÎ², Dâˆ—
KL)-vJSD is studied is [23] and it is used in deep learning in [30].
It is interesting to study the link between (MÎ², D)-variational Jensen-Shannon symmetriza-
tion of D and the (Mâ€²
Î±, Nâ€²
Î²)-JS symmetrization of [23]. In particular, the link between MÎ² for
averaging in the minimization and Mâ€²
Î± the mean for generating abstract mixtures.
More generally, Brekelmans et al. [55] considered the Î±-divergences extended to positive
measures (i.e., a separable divergence built as the different between a weighted arithmetic mean and
a geometric mean [56]):
De
Î±[p : q] :=
4
1 âˆ’Î±2
Z
X
1 âˆ’Î±
2
p(x) + 1 + Î±
2
q(x) âˆ’p
1âˆ’Î±
2 (x)q
1+Î±
2 (x)

dÂµ(x)
(90)
and proved that
câˆ—
Î² = arg min
câˆˆD {(1 âˆ’Î²)De
Î±[p1 : c] + Î²De
Î±[p2 : c]}
(91)
is a density of a likelihood ratio q-exponential family: câˆ—
Î² = p1(x)
ZÎ²,q expq(Î² logq
p2(x)
p1(x)) for q = 1+Î±
2 .
That is, câˆ—
Î² is the (AÎ², De
Î±)-generalized centroid, and the corresponding information radius is the
variational JS symmetrization:
De
Î±
vJS[p1 : p2] = (1 âˆ’Î²)De
Î±[p1 : câˆ—
Î²] + Î²De
Î±[p2 : câˆ—
Î²]
(92)
Example 5. The q-divergence [57] Dq between two densities of a q-exponential family amounts
to a Bregman divergence [10,57]. Thus, DvJS
q
for M = A is a generalized information radius that
amounts to a Bregman information.

Entropy 2021, 23, 464
16 of 28
For the case Î± = âˆin Sibsonâ€™s information radius, we ï¬nd that the information radius
is related to the total variation:
Proposition 3 (Lemma 2.4 [1]). :
DvJS,R
âˆ
[p : q] = log2(1 + DTV[p : q]),
(93)
where DTV denotes the total variation
DTV[p : q] = 1
2
Z
X |p(x) âˆ’q(x)|dÂµ(x).
(94)
Proof. Because max{p(x), q(x)} = p(x)+q(x)
2
+ 1
2|q(x) âˆ’p(x)|, it follows that we have:
Z
X max{p(x), q(x)}dÂµ(x) = 1 + DTV[p : q].
From Theorem 1, we have Râˆ({( 1
2, p), ( 1
2, q)) = log2
R
X max{p(x), q(x)}dÂµ(x) and, there-
fore, Râˆ({( 1
2, p), ( 1
2, q)) = log2(1 + DTV[p : q]).
Notice that, when M = Mg is a quasi-arithmetic mean, we may consider the diver-
gence Dg[p : q] = gâˆ’1(D[p : q)), so that the centroid of the (Mg, Dg)-JS symmetrization is:
arg min
c
gâˆ’1
 
n
âˆ‘
i=1
wiD[pi : c]
!
â‰¡arg min
c
n
âˆ‘
i=1
wiD[pi : c].
(95)
The generalized Î±-skewed Bhattacharyya divergence [29] can also be considered with
respect to a weighted mean MÎ±:
DBhat,MÎ±[p : q] = âˆ’log
Z
X MÎ±(p(x), q(x))dÂµ(x).
In particular, when MÎ± is a quasi-arithmetic weighted mean that is induced by a strictly
continuous and monotone function g, we have
DBhat,g,Î±[p : q] := âˆ’log
Z
X Mg(p(x), q(x); Î±)dÂµ(x) =: DBhat,(Mg)Î±[p : q].
Because min{p(x), q(x)} â‰¤Mg(p(x), q(x); Î±) â‰¤max{p(x), q(x)}, min{a, b} =
a+b
2
âˆ’
|bâˆ’a|
2
and max{a, b} = a+b
2 + |bâˆ’a|
2
, we deduce that we have:
0 â‰¤1 âˆ’DTV[p, q] â‰¤
Z
X Mg(p(x), q(x); Î±)dÂµ(x) â‰¤1 + DTV[p, q] â‰¤2.
(96)
The information radius of Sibson for Î± âˆˆ(0, 1) âˆª(1, âˆ) may be interpreted as gener-
alized scaled Î±-skewed Bhattacharyya divergences with respect to the power means PÎ±,
since we have RÎ±(p, q) =
Î±
Î±âˆ’1 log2
R
X PÎ±(p(x), q(x); Î±)dÂµ(x) =
Î±
1âˆ’Î± DBhat,PÎ±[p : q].
4. Relative Information Radius and Relative Jensen-Shannon Symmetrizations
of Distances
4.1. Relative Information Radius
In this section, instead of considering the full space of densities D on (X , F, Âµ) for
performing the variational optimization of the information radius, we rather consider
a subfamily of (parametric) densities R âŠ‚D. Subsequently, we deï¬ne accordingly the
R-relative Jensen-Shannon divergence (R-JSD for short) as
DR
vJS[p : q] := min
câˆˆR
1
2 DKL[p : c] + 1
2 DKL[q : c]

.
(97)

Entropy 2021, 23, 464
17 of 28
In particular, Sibson [1] considered the normal information radius, i.e., the R-relative
Jensen-Shannon divergence with R = {N (Âµ, Î£)
:
(Âµ, Î£) âˆˆRd Ã— Pd++}, where Pd++
denotes the open cone of d Ã— d positive-deï¬nite matrices (positive-deï¬nite covariance
matrices of Gaussian distributions). More generally, we may consider any exponential
family E [2].
Deï¬nition 5 (Relative (R, M)-JS symmetrization of D). Let M be a mean and D a statistical
distance. Subsequently, the relative (R, M)-JS symmetrization of D is:
DvJS
M,R[p : q] := min
câˆˆR M(D[p : c], D[q : c]).
We obtain the relative Jensen-Shannon divergences when D = DKL.
Example 6. Grosse et al. [58] considered geometric and moment average paths for annealing. They
proved that, when p1 = pÎ¸1 and p2 = pÎ¸2 belong to an exponential family [2] EF with cumulant
function F, we have
(p1p2)G
Î² =
p1(x)1âˆ’Î²p2(x)Î²
R
p1(x)1âˆ’Î²p2(x)Î²dÂµ(x) = arg min
câˆˆEF
{(1 âˆ’Î²)DKL[c : p1] + Î²DKL[c : p2]}, (98)
and
p Â¯Î· = arg min
câˆˆEF
{(1 âˆ’Î²)DKL[p1 : c] + Î²DKL[c : p2]},
(99)
where Â¯Î· = (1 âˆ’Î²)Î·1 + Î²Î·2, Î·i = EpÎ¸i [t(x)] (this is not an arithmetic mixture, but an exponential
family density moment parameter that is a mixture of the parameters).
The corresponding minima can be interpreted as relative skewed Jensen-Shannon symmetriza-
tion for the reverse KLD Dâˆ—
KL (Equation (98)) and the relative skewed Jensen-Shannon divergence
(Equation (99)):
Dâˆ—
KL
vJS
AÎ²,EF[p1 : p2]
=
min
câˆˆEF
{(1 âˆ’Î²)Dâˆ—
KL[p1 : c] + Î²Dâˆ—
KL[p2 : c]},
(100)
DvJS
AÎ²,EF[p1 : p2]
=
min
câˆˆEF
{(1 âˆ’Î²)DKL[c : p1] + Î²DKL[c : p2]},
(101)
where AÎ²(a, b) := (1 âˆ’Î²)a + Î²b is the weighted arithmetic mean for Î² âˆˆ(0, 1).
Notice that, when p = q, we have DvJS
M,R[p : p] = mincâˆˆR D[p : c], which is the
information projection [59] with respect to D of density p to the submanifold R. Thus,
when p Ì¸âˆˆR, we have DvJS
M,R[p : p] > 0, i.e., the relative JSDs are not proper divergences,
since a proper divergence ensures that D[p : q] â‰¥0 with equality if p = q. Figure 1
illustrates the main cases of the relative Jensen-Shannon divergenc between p and q: Either
p and q are both inside or outside R, or one point is inside R, while the other point is
outside R. When p = q, we get an information projection when both of the points are
outside R, and DR
vJS[p : p] = 0 when p âˆˆR. When p, q âˆˆR with p Ì¸= q, the value
DR
vJS[p : q] corresponds to the information radius (and the arg min to the right-sided
Kullbackâ€“Leibler centroid).

Entropy 2021, 23, 464
18 of 28
DR
JS[p : q] := mincâˆˆR 1
2DKL[p : c] + 1
2DKL[q : c]
R
D
p
q
câˆ—
R(p, q)
câˆ—
R(p, q) := arg mincâˆˆR 1
2DKL[p : c] + 1
2DKL[q : c]
R
D
p = q
DR
JS[p : p] := mincâˆˆR DKL[p : c]
câˆ—
R(p) := câˆ—
R(p, q) := arg mincâˆˆR DKL[p : c]
2-point information projection
Information projection
câˆ—
R(p)
R
D
p
q
câˆ—
R(p, q)
Right-sided KL centroid
R
D
p
q
Traversing
câˆ—
R(p, q)
Figure 1. Illustrating several cases of the relative Jensen-Shannon divergence based on whether p âˆˆR and q âˆˆR or not.
4.2. Relative Jensen-Shannon Divergences: Applications to Density Clustering and Quantization
Let DKL[p : qÎ¸] be the Kullbackâ€“Leibler divergence between an arbitrary density p and
a density qÎ¸ of an exponential family Q = {qÎ¸ : Î¸ âˆˆÎ˜}. Let us canonically express [2,60]
the density qÎ¸(x), as
qÎ¸(x) = exp

Î¸âŠ¤tQ(x) âˆ’FQ(Î¸) + kQ(x)

,
where tQ(x) denotes the sufï¬cient statistics, kQ(x) is an auxiliary carrier measure term (e.g.,
k(x) = 0 for the Gaussian family and k(x) = log(x) for the Rayleigh family [60]), and FQ(Î¸)
the cumulant function. Assume that we know in closed-form the following quantities:
â€¢
mp := Ep[tQ(x)] = R
p(x)tQ(x)dÂµ(x) and
â€¢
the Shannon entropy h[p] = âˆ’R
p(x) log p(x)dÂµ(x) of p.
Subsequently, we can express the KLD using a semi-closed-form formula.
Proposition 4. Let qÎ¸ âˆˆQ be a density of an exponential family and p an arbitrary density with
mp = Ep[tQ(x)]. Subsequently, the Kullbackâ€“Leibler divergence between p and qÎ¸ is expressed as:
DKL[p : qÎ¸] = FQ(Î¸) âˆ’mâŠ¤
p Î¸ âˆ’Ep[kQ(x)] âˆ’h[p],
(102)
where h[p : qÎ¸] = FQ(Î¸) âˆ’mâŠ¤
p Î¸ âˆ’Ep[kQ(x)] is the cross-entropy between p and qÎ¸.

Entropy 2021, 23, 464
19 of 28
Proof. The proof is straightforward since log qÎ¸(x) = Î¸âŠ¤tQ(x) âˆ’FQ(Î¸) + kQ(x). Therefore,
we have:
DKL[p : qÎ¸]
=
h[p : qÎ¸] âˆ’h[p],
(103)
=
âˆ’
Z
X p(x) log qÎ¸(x)dÂµ(x) âˆ’h[p],
(104)
=
FQ(Î¸) âˆ’mâŠ¤
p Î¸ âˆ’Ep[kQ(x)] âˆ’h[p].
(105)
Example 7. For example, when qÎ¸ = qÂµ,Î£ is the density of a multivariate Gaussian distribution
N (Âµ, Î£) (with kN (x) = 0), we have
DKL[p : qÂµ,Î£] = 1
2

log |2Ï€Î£| + (Âµ âˆ’m)âŠ¤Î£âˆ’1(Âµ âˆ’m) + tr(Î£âˆ’1S)

âˆ’h[p],
(106)
where m = Âµ(p) = Ep[X] and S = Cov(p) := Ep

XXâŠ¤ âˆ’Ep[X]Ep[X]âŠ¤.
The formula of Proposition 4 is said in semi-closed-form, because it relies on knowing
both the entropy h of p and the sufï¬cient statistic moments Ep[tQ(x)]. Yet, this semi-closed
formula may prove to be useful in practice: For example, we can answer the compari-
son predicate
â€œIs DKL[p : qÎ¸1] â‰¥DKL[p : qÎ¸2] or not?â€
by checking whether FQ(Î¸1) âˆ’FQ(Î¸2) âˆ’mâŠ¤
p (Î¸1 âˆ’Î¸2) â‰¥0 or not (i.e., the terms âˆ’Ep[kQ(x)] âˆ’
h[p] in Equation (102) cancel out). Thus, we get a closed-form predicate, although DKL is
only known in semi-closed-form. This KLD comparison predicate shall be used later on
when clustering densities with respect to centroids in Section 4.2.
Remark 1. Note that when Y = f (X) for an invertible and differentiable transformation f then
we have h[Y] = h[X] + EX[log |Jf (X)|] where Jf denotes the Jacobian matrix. For example, when
Y = f (X) = AX, we have h[Y] = h[X] + log |A|.
When p belongs to an exponential family P (P may be different from Q) with cu-
mulant function FP, sufï¬cient statistics tP(x), auxiliary carrier term kP(x), and natural
parameter Î¸, we have the entropy [61] expressed, as follows:
h[p]
=
FP(Î¸) âˆ’Î¸âŠ¤âˆ‡FP(Î¸) âˆ’Ep[kP(x)],
(107)
=
âˆ’Fâˆ—
P(Î·) âˆ’Ep[kP(x)],
(108)
where Fâˆ—
P(Î·) = Î¸âŠ¤âˆ‡F(Î¸) âˆ’F(Î¸) is the Legendre transform of F(Î¸) and Î· = Î·(Î¸) = âˆ‡F(Î¸)
is called the moment parameter since we have Î·(Î¸) = Ep[tP(x)] [2,60].
It follows the following proposition reï¬ning Proposition 4 when p = pÎ¸ âˆˆP:
Proposition 5. Let pÎ¸ be a density of an exponential family P and qÎ¸â€² be a density of an exponential
family Q. Subsequently, the Kullbackâ€“Leibler divergence between pÎ¸ and qÎ¸â€² is expressed as:
DKL[pÎ¸ : qÎ¸â€²] = FQ(Î¸â€²) + Fâˆ—
P(Î·) âˆ’EpÎ¸[tQ(x)]âŠ¤Î¸â€² + EpÎ¸[kP(x) âˆ’kQ(x)].
(109)
Proof. We have
DKL[pÎ¸ : qÎ¸â€²]
=
h[pÎ¸ : qÎ¸â€²] âˆ’h[pÎ¸],
(110)
=
FQ(Î¸â€²) âˆ’mâŠ¤
pÎ¸Î¸â€² âˆ’EpÎ¸[kQ(x)] + Fâˆ—
P(Î·) + EpÎ¸[kP(x)],
(111)
=
FQ(Î¸â€²) + Fâˆ—
P(Î·) âˆ’EpÎ¸[tQ(x)]âŠ¤Î¸â€² + EpÎ¸[kP(x) âˆ’kQ(x)].
(112)

Entropy 2021, 23, 464
20 of 28
In particular, when p and q belong both to the same exponential family (i.e., P = Q
with kP(x) = kQ(x)), we have F(Î¸) := FP(Î¸) := FQ(Î¸) and EpÎ¸[tQ(x)] = âˆ‡F(Î¸) =: Î·, and
DKL[pÎ¸ : qÎ¸â€²] = F(Î¸â€²) + Fâˆ—(Î·) âˆ’Î¸â€²âŠ¤Î·.
This last equation is the Fenchelâ€“Young divergence in Bregman manifolds [34,62] (called
dually ï¬‚at spaces in information geometry [10]). Thus the divergence can be rewritten as
equivalent dual Bregman divergences:
DKL[pÎ¸ : qÎ¸â€²]
=
F(Î¸â€²) + Fâˆ—(Î·) âˆ’Î·âŠ¤Î¸â€²,
(113)
=
BF(Î¸â€² : Î¸),
(114)
=
BFâˆ—(Î· : Î·â€²),
(115)
where Î·â€² = âˆ‡F(Î¸â€²).
Notice that DKL[pÎ¸ : Q] := minÎ¸â€²âˆˆÎ˜â€² DKL[pÎ¸ : qÎ¸â€²] is unique and can be calculated as
Î·â€² = âˆ‡FQ(Î¸â€²) = EpÎ¸[tQ(x)].
Let us report two examples of calculations of the KLD between two densities of two
exponential families.
Example 8. For the ï¬rst exponential family, consider the family of Laplacian distributions:
P = L =

pÏƒ(x) := 1
2Ïƒ exp

âˆ’|x|
Ïƒ

: Ïƒ > 0

.
The canonical decomposition of the density yields tL(x) = |x|, Î¸ = âˆ’1
Ïƒ, kL(x) = 0, and FL(Î¸) =
log 2
âˆ’Î¸ . (i.e., FL(Î¸(Ïƒ)) = log 2Ïƒ). It follows that Î·(Î¸) = Fâ€²
L(Î¸) = âˆ’1
Î¸ (Î·(Ïƒ) = Ïƒ = E[|x|]),
Î¸(Î·) = âˆ’1
Î· , and Fâˆ—
L(Î·) = âˆ’1 âˆ’log(2Î·) and, therefore, Fâˆ—
L(Î·(Ïƒ)) = âˆ’1 âˆ’log(2Ïƒ).
For the second family, consider the exponential family of zero-centered Gaussian distributions:
Q = N0 =
(
qÏƒâ€²(x) =
1
p
2Ï€(Ïƒâ€²)2 exp

âˆ’
x2
2(Ïƒâ€²)2
)
.
We have tN0(x) = x2, kN0(x) = 0, Î¸â€² = âˆ’
1
2(Ïƒâ€²)2 , and FN0(Ïƒâ€²) = 1
2 log(2Ï€(Ïƒâ€²)2).
Moreover, let us calculate EpÏƒ[tN0(x)] = EpÏƒ[x2] = 2Ïƒ2. Subsequently, we can calculate the
Kullbackâ€“Leibler divergence between pÏƒ âˆ¼L(Ïƒ) and qÏƒâ€² âˆ¼N0(Ïƒâ€²), as follows:
DKL[pÏƒ : qÏƒâ€²]
=
FQ(Î¸â€²(Ïƒâ€²)) + Fâˆ—
P(Î·(Ïƒ)) âˆ’EpÏƒ[tQ(x)]âŠ¤Î¸â€²(Ïƒâ€²) + EpÏƒ[kP(x) âˆ’kQ(x)], (116)
=
1
2 log(2Ï€(Ïƒâ€²)2) âˆ’1 âˆ’log(2Ïƒ) âˆ’2Ïƒ2

âˆ’
1
2(Ïƒâ€²)2

,
(117)
=
log
 Ïƒâ€²
Ïƒ

+
 Ïƒ
Ïƒâ€²
2
+ 1
2 log
 Ï€
2

âˆ’1.
(118)
Notice that DKL[pÏƒ : qÏƒâ€²] â‰¥0, but never 0 since the P âˆ©Q = âˆ….
Let us now compute the reverse Kullbackâ€“Leibler divergence DKL[qÏƒâ€² : pÏƒ]. We ï¬rst calculate
EqÏƒâ€² [tL(x)] = EqÏƒâ€²(Ïƒâ€²)[|x|] =
q
2
Ï€ Ïƒâ€². Since FQ(Î¸â€²) = 1
2 log( Ï€
âˆ’Î¸â€² ), we have Î·â€²(Î¸â€²) = Fâ€²
Q(Î¸â€²) =
âˆ’1
2Î¸â€² . Thus Î·â€²(Ïƒâ€²) = (Ïƒâ€²)2 and Fâˆ—
Q(Î·â€²) = âˆ’1
2 âˆ’1
2 log(2Ï€Î·). Therefore, we get Fâˆ—
Q(Î·â€²(Ïƒâ€²)) =
âˆ’h[qÏƒâ€²] = âˆ’1
2 log(2Ï€e(Ïƒâ€²)2).
It follows that
DKL[qÏƒâ€² : pÏƒ]
=
FP(Î¸(Ïƒ)) + Fâˆ—
Q(Î·â€²(Ïƒâ€²)) âˆ’EqÎ¸â€² [tP(x)]âŠ¤Î¸(Ïƒ) + EqÎ¸â€² [kP(x) âˆ’kQ(x)], (119)
=
log(2Ïƒ) âˆ’1
2 log(2Ï€e(Ïƒâ€²)2) âˆ’
r
2
Ï€ Ïƒâ€² Ã—

âˆ’1
Ïƒ

,
(120)
=
r
2
Ï€
Ïƒâ€²
Ïƒ + log
 Ïƒ
Ïƒâ€²

âˆ’1
2 log( Ï€
2 e).
(121)

Entropy 2021, 23, 464
21 of 28
Again, we have DKL[qÏƒâ€² : pÏƒ] â‰¥0, but never 0, because P âˆ©Q = âˆ….
Example 9. Let us use the formula of Equation (109) to calculate the KLD between two Weibull
distributions [63]. A Weibull distribution of shape Îº > 0 and scale Ïƒ > 0 has a density deï¬ned on
X = [0, âˆ), as follows:
pWei
Îº,Ïƒ (x) := Îº
Ïƒ
 x
Ïƒ
Îºâˆ’1
exp

âˆ’
 x
Ïƒ
Îº
.
For a ï¬xed shape Îº, the set of Weibull distributions WÎº := {pWei
Îº,Ïƒ
:
Ïƒ > 0} form an
exponential family with natural parameter Î¸ = âˆ’1
ÏƒÎº , sufï¬cient statistic tÎº(x) = xÎº, auxiliary
carrier term kÎº(x) = (Îº âˆ’1) log x + log Îº, and cumulant function FÎº(Î¸) = âˆ’log(âˆ’Î¸) (so that
FÎº(Î¸(Ïƒ)) = FÎº(Ïƒ) = Îº log Ïƒ):
pWei
Îº,Ïƒ (x) := exp

âˆ’1
ÏƒÎº xk + log 1
ÏƒÎº + k(x)

.
We recover the exponential family of exponential distributions of rate parameter Î» = 1
Ïƒ when
Îº = 1:
pExp
Î»
(x)
=
pWei
1,Ïƒ (x) = 1
Ïƒ exp

âˆ’x
Ïƒ

,
=
Î» exp(âˆ’Î»x),
and the exponential family of Rayleigh distributions when Îº = 2 with scale parameter ÏƒRay =
Ïƒ
âˆš
2:
pRay
ÏƒRay(x)
=
pWei
2,Ïƒ (x) = 2x
Ïƒ2 exp

âˆ’x2
Ïƒ2

,
=
x
Ïƒ2
Ray
exp
 
âˆ’
x2
2Ïƒ2
Ray
!
.
Now, assume that we are given the differential entropy of the Weibull distributions [64]
(pp. 155â€“156):
h
h
pWei
Îº1,Ïƒ1
i
= Î³

1 âˆ’1
Îº1

+ log Ïƒ1
Îº1
+ 1,
where Î³ â‰ˆ0.5772156649 is the Eulerâ€“Mascheroni constant, and the Weibull raw moments [64]
(p. 155):
m = EpWei
Îº1,Ïƒ[xÎº2] = ÏƒÎº2
1 Î“

1 + Îº2
Îº1

,
where Î“(x) = R âˆ
0 txâˆ’1eâˆ’tdt is the gamma function (with Î“(n) = (n âˆ’1)! for integers n). Because
h[pWei
Îº,Ïƒ ] = FÎº(Î¸) âˆ’Î¸âŠ¤âˆ‡FÎº(Î¸) âˆ’EpWei
Îº,Ïƒ [kÎº(x)] = âˆ’Fâˆ—
Îº (Î·) âˆ’EpWei
Îº,Ïƒ [kÎº(x)], we deduce that
EpWei
Îº,Ïƒ [kÎº(x)] = âˆ’Fâˆ—
Îº (Î·) âˆ’h
h
pWei
Îº,Ïƒ
i
,
where Fâˆ—
Îº (Î·) is the Legendre transform of FÎº(Î¸) and Î·(Î¸) = âˆ‡FÎº(Î¸) = âˆ’1
Î¸ = E[t(x)] = E[xÎº].
We have Î¸(Î·) = âˆ‡Fâˆ—
Îº (Î·) = âˆ’1
Î· and Fâˆ—
Îº (Î·) = Î·âŠ¤âˆ‡Fâˆ—
Îº (Î·) âˆ’FÎº(âˆ‡Fâˆ—
Îº (Î·)) = âˆ’1 âˆ’log Î·. It
follows that
EpWei
Îº,Ïƒ [kÎº(x)] = 1 + log

ÏƒÎ“

1 + 1
Îº

âˆ’Î³

1 âˆ’1
Îº

âˆ’log Ïƒ
Îº + 1.
Therefore, we deduce that the logarithmic moment of pWei
Îº1,Ïƒ is:
EpWei
Îº1,Ïƒ[log x] = âˆ’Î³
Îº1
+ log Ïƒ1.

Entropy 2021, 23, 464
22 of 28
This coincides with the explicit deï¬nite integral calculation reported in [63].
Subsequently, we calculate the KLD between two Weibull distributions using Equation (109),
as follows:
DKL
h
pWei
Îº1,Ïƒ1 : pWei
Îº2,Ïƒ2
i
=
FÎº2(Î¸â€²) + Fâˆ—
Îº1(Î·) âˆ’EpÎº1,Ïƒ1 [xÎº2]âŠ¤Î¸â€² + EpÎº1,Ïƒ1 [kÎº1(x) âˆ’kÎº2(x)]
(122)
=
log Îº1
ÏƒÎº1
1
âˆ’log Îº2
ÏƒÎº2
2
+ (Îº1 âˆ’Îº2)

log Ïƒ1 âˆ’Î³
Îº1

+
 Ïƒ1
Ïƒ2
Îº2
Î“
 Îº2
Îº1
+ 1

âˆ’1 (123)
since we have the following terms:
FÎº2(Î¸â€²)
=
log ÏƒÎº2
2 ,
Fâˆ—
Îº1(Î·)
=
âˆ’1 âˆ’log ÏƒÎº1
1 ,
âˆ’EpÎº1,Ïƒ1 [xÎº2]âŠ¤Î¸â€²
=
1
ÏƒÎº2
2
ÏƒÎº2
1 Î“

1 + Îº2
Îº1

EpÎº1,Ïƒ1 [kÎº1(x) âˆ’kÎº2(x)]
=
(Îº1 âˆ’Îº2)EpÎº1,Ïƒ1 [log x] + log Îº1
Îº2
,
=
log Îº1
Îº2
+ (Îº1 âˆ’Îº2)

log Ïƒ1 âˆ’Î³
Îº1

.
This formula matches the formula reported in [63].
When Îº1 = Îº2 = 1, we recover the ordinary KLD formula between two exponential distribu-
tions [60] with Î»i = 1
Ïƒi since Î“(2) = (2 âˆ’1)! = 1:
DKL
h
pWei
1,Ïƒ1 : pWei
1,Ïƒ2
i
=
log Ïƒ2
Ïƒ1
+ Ïƒ1
Ïƒ2
âˆ’1,
(124)
=
Î»2
Î»1
âˆ’log Î»2
Î»1
âˆ’1.
(125)
When Îº1 = Îº2 = 2, we recover the ordinary KLD formula between two Rayleigh distribu-
tions [60], with ÏƒRay =
Ïƒ
âˆš
2:
DKL
h
pWei
2,Ïƒ1 : pWei
2,Ïƒ2
i
=
log
 
Ïƒ2
2
Ïƒ2
1
!
+ Ïƒ2
1
Ïƒ2
2
âˆ’1,
(126)
=
log
 
ÏƒRay2
2
ÏƒRay2
1
!
+
ÏƒRay2
1
ÏƒRay2
2
âˆ’1.
(127)
The formulae of Equations (125) and (127) are linked by the fact that if X âˆ¼Exp(Î») and
Y =
âˆš
X then Y âˆ¼Ray

1
âˆš
2Î»

, and f-divergences [65], including the Kullbackâ€“Leibler divergence
are invariant by a differentiable transformation [66].
Jeffreysâ€™ divergence symmetrizes the KLD divergence, as follows:
DJ[p : q] := DKL[p : q] + DKL[q : p] = 2A(DKL[p : q], DKL[q : p]).
(128)
The Jeffreys divergence between two densities of different exponential families P and Q is
DJ[pÎ¸ : qÎ¸â€²] = Î¸â€²âŠ¤(Î·â€² âˆ’EpÎ¸[tQ(x)]) + Î¸âŠ¤(Î· âˆ’EqÎ¸â€² [tP(x)]) + EpÎ¸[kP(x) âˆ’kQ(x)] + EqÎ¸â€² [kQ(x) âˆ’kP(x)].
(129)
When P = Q, we have EpÎ¸[tQ(x)] = Î· and EqÎ¸â€² [tP(x)]) = Î·â€², so that we ï¬nd the
usual expression of the Jeffreys divergence between two densities of an exponential family:
DJ[pÎ¸ : pÎ¸â€²] = (Î¸â€² âˆ’Î¸)âŠ¤(Î·â€² âˆ’Î·).
(130)

Entropy 2021, 23, 464
23 of 28
To ï¬nd the best density qÎ¸ approximating p by minimizing minÎ¸ DKL[p : qÎ¸], we solve
âˆ‡F(Î¸) = Î· = m and, therefore, Î¸ = âˆ‡Fâˆ—(m) = (âˆ‡F)âˆ’1(m), where Fâˆ—(Î·) = EqÎ·[log qÎ·(m)],
with Fâˆ—denoting the Legendreâ€“Fenchel convex conjugate [2]. In particular, when p =
âˆ‘wipÎ¸i is a mixture of EFs (with m = Ep[t(x)] = âˆ‘wiÎ·i with Î·i = EpÎ¸i [t(x)] thanks to the
linearity of the expectation), then the best density of the EF simplifying p is
min
Î¸
DKL[p : qÎ¸]
=
min
Î¸
F(Î¸) âˆ’mâŠ¤Î¸,
(131)
=
min
Î¸
F(Î¸) âˆ’âˆ‘wiÎ·âŠ¤
i Î¸.
(132)
Taking the gradient with respect to Î¸, we have âˆ‡F(Î¸) = Î· = âˆ‘wiÎ·i. This yields
another proof without the Pythagoras theorem [67,68].
Proposition 6. Let m(x) = âˆ‘wipÎ¸i(x) be a mixture with components that belong to an ex-
ponential family with cumulant function F. Subsequently, Î¸âˆ—= argÎ¸ minÎ¸ DKL[p : qÎ¸] is
âˆ‡Fâˆ—(âˆ‘n
i=1 wiÎ·i), where the Î·i = âˆ‡F(Î¸i) are the moment parameters of the mixture components.
Consider the following two problems:
Problem 1 (Density clustering). Given a set of n weighted densities (w1, p1), . . . , (wn, pn),
partition them into k clusters C1, . . . , Ck in order to minimize the k-centroid objective function with
respect to a statistical divergence D: âˆ‘n
i=1 wi minlâˆˆ{1,...,k} D[pi : cl], where cl denotes the centroid
of cluster Cl for l âˆˆ{1, . . . , k}.
For example, when all the densities piâ€™s are isotropic Gaussians, we recover the
k-means objective function [69].
Problem 2 (Mixture component quantization). Given a statistical mixture m(x) = âˆ‘n
i=1 wi
pi(x), quantize the mixture components into k densities q1, . . . , qk in order to minimize âˆ‘i wi
minlâˆˆ{1,...,k} D[pi : ql].
Notice that, in Problem 1, the input densities piâ€™s may be mixtures, i.e., pi(x) =
âˆ‘ni
j=1 wi,jpi,j(x). Using the relative information radius, we can cluster a set of distributions
(potentially mixtures) into an exponential family mixture, or quantize an exponential
family mixture. Indeed, we can implement an extension of k-means [69] with k-centers qÎ¸i,
to assign density pi to cluster Cj (with center qj), we need to perform basic comparison
tests DKL[pi : qÎ¸l] â‰¥DKL[pi : qÎ¸j]. Provided that the cumulant F of the exponential family
is in closed-form, we do not need formula for the entropies h(pi).
Clustering and quantization of densities/mixtures have been widely studied in the
literature, see, for example, [70â€“76].
5. Conclusions
To summarize, the ordinary Jensen-Shannon divergence has been deï¬ned in three
equivalent ways in the literature:
DJS[p, q]
:=
min
câˆˆD
1
2(DKL[p : c] + DKL[q : c]),
(133)
=
1
2

DKL

p : p + q
2

+ DKL

q : p + q
2

,
(134)
=
h
 p + q
2

âˆ’h[p] + h[q]
2
.
(135)
The JSD Equation (133) was studied by Sibson in 1969 within the wider scope of infor-
mation radius [1]: Sibson relied on the RÃ©nyi Î±-divergences (relative RÃ©nyi Î±-entropies [77])
and recovered the ordinary Jensen-Shannon divergence as a particular case of the Î±-

Entropy 2021, 23, 464
24 of 28
information radius when Î± = 1 and n = 2 points. The Î±-information radii are related to
generalized Bhattacharyya distances with respect to power means and the total variation
distance in the limit case of Î± = âˆ.
Lin [4] investigated the JSD Equation (134) in 1991 with its connection to the JSD
deï¬ned in Equation (134)). In Lin [4], the JSD is interpreted as the arithmetic symmetriza-
tion of the K-divergence [24]. Generalizations of the JSD based on Equation (134) were
proposed in [23] using a generic mean instead of the arithmetic mean. One motivation
was to obtain a closed-form formula for the geometric JSD between multivariate Gaussian
distributions, which relies on the geometric mixture (see [30] for a use case of that formula
in deep learning). Indeed, the ordinary JSD between Gaussians is not available in closed-
form (not analytic). However, the JSD between Cauchy distributions admit a closed-form
formula [78], despite the calculation of a deï¬nite integral of a log-sum term. Instead of
using an abstract mean to deï¬ne a mid-distribution of two densities, one may also consider
the mid-point of a geodesic linking these two densities (the arithmetic means p+q
2
is inter-
preted as a geodesic midpoint). Recently, Li [79] investigated the transport Jensen-Shannon
divergence as a symmetrization of the Kullbackâ€“Leibler divergence in the L2-Wasserstein
space. See Section 5.4 of [79] and the closed-form formula of Equation (18) obtained for the
transport Jensen-Shannon divergence between two multivariate Gaussian distributions.
The generalization of the identity between the JSD of Equation (134) and the JSD of
Equation (135) was studied while using a skewing vector in [18]. Although the JSD is
a f-divergence [8,18], the Sibson-M Jensen-Shannon symmetrization of a distance does
not belong, in general, to the class of f-divergences. The variational JSD deï¬nition of
Equation (133) is implicit, while the deï¬nitions of Equations (134) and (135) are explicit
because the unique optimal centroid câˆ—= p+q
2
has been plugged into the objective function
that was minimized by Equation (133).
In this paper, we proposed a generalization of the Jensen-Shannon divergence based
on the variational deï¬nition of the ordinary Jensen-Shannon divergence based on the
variational JSD deï¬nition of Equation (133): DvJS[p : q] = minc 1
2(DKL[p : c] + DKL[q :
c]). We introduced the Jensen-Shannon symmetrization of an arbitrary divergence D by
considering a generalization of the information radius with respect to an abstract weighted
mean MÎ²: DvJS
M [p : q] := minc MÎ²(D[p : c], D[q : c]). Notice that, in the variational JSD,
the mean MÎ² is used for averaging divergence values, while the mean MÎ± in the (MÎ±, NÎ²)
JSD is used to deï¬ne generic statistical mixtures. We also consider relative variational JS
symmetrization when the centroid has to belong to a prescribed family of densities. For
the case of exponential family, we showed how to compute the relative centroid in closed
form, thus extending the pioneering work of Sibson, who considered the relative normal
centroid used to calculate the relative normal information radius. Figure 2 illustrates the
three generalizations of the ordinary skewed Jensen-Shannon divergence. Notice that,
in general, the (M, N)-JSDs and the variational JDSs are not f-divergences (except in the
ordinary case).

Entropy 2021, 23, 464
25 of 28
ordinary skewed Jensen-Shannon divergence DÎ±,Î²
JS
(MÎ±, NÎ²)-Jensen-Shannon divergence DMÎ±,NÎ²
JS
Ì¸= f-divergence
f-divergence If[p : q] :=
R
p(x)f

q(x)
p(x)

dÂµ(x)
Vector-skew DÎ±,w
JS
Variational MÎ² Jensen-Shannon divergence DMÎ²
vJS
f-divergence
Ì¸= f-divergence
Generalized Jensen-Shannon divergences
DÎ±,Î²
JS [p : q] := (1 âˆ’Î²)DKL[p : mÎ±] + Î²DKL[q : mÎ±] =: If Î±,Î²
JS [p : q]
f Î±,Î²
JS (u) = âˆ’
 (1 âˆ’Î²) log (Î±u + (1 âˆ’Î±)) + Î²u log
  1âˆ’Î±
u
+ Î±

DJS[p, q] := mincâˆˆD 1
2 (DKL[p : c] + DKL[q : c])
DJS[p, q] := h[ p+q
2 ] âˆ’h[p]+h[q]
2
DJS[p, q] := 1
2
 DKL

p : p+q
2

+ DKL

q : p+q
2

DÎ±,w
JS (p : q) := Pk
i=1 wiDKL[(1 âˆ’Î±i)p + Î±iq : (1 âˆ’Â¯Î±)p + Â¯Î±q]
DÎ±,w
JS (p : q) = h [(1 âˆ’Â¯Î±)p + Â¯Î±q] âˆ’Pk
i=1 wih [(1 âˆ’Î±i)p + Î±i]
Â¯Î± = Pk
i=1 wiÎ±i
fÎ±,w(u) = Pk
i=1 wi(Î±iu + (1 âˆ’Î±i)) log (1âˆ’Î±i)+Î±iu
(1âˆ’Â¯Î±)+Â¯Î±u
DMÎ±,NÎ²
JS
(p : q) := NÎ²(DKL[p, (pq)M
Î± ], DKL[q :, (pq)M
Î± ])
DMÎ²
vJS[p : q] := mincâˆˆD MÎ² (D[p : c], D[q : c])
(pq)M
Î± (x) :=
MÎ±(p(x),q(x))
R
MÎ±(p(x),q(x))dÂµ(x)
Figure 2. Three equivalent expressions of the ordinary (skewed) Jensen-Shannon divergence which yield three differ-
ent generalizations.
In a similar vein, Chen et al. [80] considered the following minimax symmetrization
of the scalar Bregman divergence [81]:
Bminmax
f
(p, q)
:=
min
c
max
Î»âˆˆ[0,1] Î»Bf (p : c) + (1 âˆ’Î»)Bf (q : c),
(136)
=
max
Î»âˆˆ[0,1] Î»Bf (p : Î»p + (1 âˆ’Î»)q) + (1 âˆ’Î»)Bf (q : Î»p + (1 âˆ’Î»)),
(137)
=
Î» f (p) + (1 âˆ’Î») f (q) âˆ’f (Î»p + (1 âˆ’Î»))
(138)
where Bf denotes the scalar Bregman divergence induced by a strictly convex and smooth
function f:
Bf (p : q) = f (p) âˆ’f (q) âˆ’(p âˆ’q) f â€²(q).
(139)
They proved that
q
Bminmax
f
(p, q) yields a metric when 3(log f â€²â€²)â€²â€² â‰¥((log f â€²â€²)â€²)2, and ex-
tend the deï¬nition to the vector case and conjecture that the square-root metrization still
holds in the multivariate case. In a sense, this deï¬nition geometrically highlights the
notion of radius, since the minmax optimization amount to ï¬nd a smallest enclosing
ball enclosing [82] the source distributions. The circumcenter, also called the Chebyshev
center [83], is then the mid-distribution instead of the centroid for the information ra-
dius. The term "information radiusâ€ is well-suited to measure the distance between two
points for an arbitrary distance D. Indeed, the JS-symmetrization of D is deï¬ned by
DJS[p : q] := minc{ 1
2 D[p : c] + 1
2 D[q : c]}. When D[p : q] = DE[p : q] = âˆ¥p âˆ’qâˆ¥is the
Euclidean distance, we have c =
p+q
2 , and D[p : c] = D[q : c] = 1
2âˆ¥p âˆ’qâˆ¥=: r (i.e.,
the radius being half of the diameter âˆ¥p âˆ’qâˆ¥). Thus, DJS
E [p : q] = r; hence, the term chosen
by Sibson [1] for DJS: information radius. Besides providing another viewpoint, varia-
tional deï¬nitions of divergences have proven to be useful in practice (e.g., for estimation).
For example, a variational deï¬nition of the RÃ©nyi divergence generalizing the Donskerâ€“

Entropy 2021, 23, 464
26 of 28
Varadhan variational formula of the KLD is given in [84], which is used to estimate the
RÃ©nyi Divergences.
Funding: This research received no external funding.
Informed Consent Statement: Not applicable.
Data Availability Statement: Data sharing not applicable.
Acknowledgments: We warmly thank Rob Brekelmans (Information Sciences Institute, University
of Southern California, USA) for discussions and feedback related to the contents of this work.
The author thanks the reviewers for valuable feedback, comments, and suggestions, and GaÃ«tan
Hadjeres (Sony CSL Paris) for his careful reading of the manuscript.
Conï¬‚icts of Interest: The authors declare no conï¬‚ict of interest.
References
1.
Sibson, R. Information radius. Z. Wahrscheinlichkeitstheorie Verwandte Geb. 1969, 14, 149â€“160. [CrossRef]
2.
Barndorff-Nielsen, O. Information and Exponential Families: In Statistical Theory; John Wiley & Sons: Hoboken, NJ, USA, 2014.
3.
Billingsley, P. Probability and Measure; John Wiley & Sons: Hoboken, NJ, USA, 2008.
4.
Lin, J. Divergence measures based on the Shannon entropy. IEEE Trans. Inf. Theory 1991, 37, 145â€“151. [CrossRef]
5.
Kullback, S. Information Theory and Statistics; Courier Corporation: Chelmsford, MA, USA, 1997.
6.
Cover, T.M.; Thomas, J.A. Elements of Information Theory; John Wiley & Sons: Hoboken, NJ, USA, 2012.
7.
Morimoto, T. Markov processes and the H-theorem. J. Phys. Soc. Jpn. 1963, 18, 328â€“331. [CrossRef]
8.
CsiszÃ¡r, I. Eine informationstheoretische ungleichung und ihre anwendung auf beweis der ergodizitaet von markoffschen ketten.
Magyer Tud. Akad. Mat. Kut. Int. Koezl. 1964, 8, 85â€“108.
9.
Ali, S.M.; Silvey, S.D. A general class of coefï¬cients of divergence of one distribution from another. J. R. Stat. Soc. Ser. B
(Methodological) 1966, 28, 131â€“142. [CrossRef]
10.
Amari, S.i. Information Geometry and Its Applications; Applied Mathematical Sciences; Springer: Tokyo, Japan, 2016.
11.
McLachlan, G.J.; Peel, D. Finite Mixture Models; John Wiley & Sons: Hoboken, NJ, USA, 2004.
12.
Nielsen, F.; Boltz, S. The Burbea-Rao and Bhattacharyya centroids. IEEE Trans. Inf. Theory 2011, 57, 5455â€“5466. [CrossRef]
13.
Endres, D.M.; Schindelin, J.E. A new metric for probability distributions. IEEE Trans. Inf. Theory 2003, 49, 1858â€“1860. [CrossRef]
14.
Fuglede, B.; Topsoe, F. Jensen-Shannon divergence and Hilbert space embedding. In Proceedings of the International Symposium
onInformation Theory, 2004. ISIT 2004. Proceedings, Chicago, IL, USA, 27 Juneâ€“2 July 2004; IEEE: Piscataway, NJ, USA, 2004; p. 31.
15.
Virosztek, D. The metric property of the quantum Jensen-Shannon divergence. Adv. Math. 2021, 380, 107595. [CrossRef]
16.
Goodfellow, I.J.; Pouget-Abadie, J.; Mirza, M.; Xu, B.; Warde-Farley, D.; Ozair, S.; Courville, A.; Bengio, Y. Generative adversarial
networks. arXiv 2014, arXiv:1406.2661.
17.
Goodfellow, I.; Bengio, Y.; Courville, A.; Bengio, Y. Deep Learning; MIT Press: Cambridge, MA, USA, 2016.
18.
Nielsen, F. On a generalization of the Jensen-Shannon divergence and the Jensen-Shannon centroid. Entropy 2020, 22, 221.
[CrossRef]
19.
CsiszÃ¡r, I. Information-type measures of difference of probability distributions and indirect observation. Stud. Sci. Math. Hung.
1967, 2, 229â€“318.
20.
CsiszÃ¡r, I. Axiomatic characterizations of information measures. Entropy 2008, 10, 261â€“273. [CrossRef]
21.
Banerjee, A.; Merugu, S.; Dhillon, I.S.; Ghosh, J. Clustering with Bregman divergences. J. Mach. Learn. Res. 2005, 6, 1705â€“1749.
22.
AntolÃ­n, J.; Angulo, J.; LÃ³pez-Rosa, S. Fisher and Jensen-Shannon divergences: Quantitative comparisons among distributions.
application to position and momentum atomic densities. J. Chem. Phys. 2009, 130, 074110. [CrossRef] [PubMed]
23.
Nielsen, F. On the Jensen-Shannon symmetrization of distances relying on abstract means. Entropy 2019, 21, 485. [CrossRef]
24.
Nielsen, F. A family of statistical symmetric divergences based on Jensenâ€™s inequality. arXiv 2010, arXiv:1009.4004.
25.
Nielsen, F.; Nock, R. Generalizing skew Jensen divergences and Bregman divergences with comparative convexity. IEEE Signal
Process. Lett. 2017, 24, 1123â€“1127. [CrossRef]
26.
de Carvalho, M. Mean, what do you Mean? Am. Stat. 2016, 70, 270â€“274. [CrossRef]
27.
Bullen, P.S. Handbook of Means and Their Inequalities; Springer Science & Business Media: Berlin/Heidelberg, Germany, 2013;
Volume 560.
28.
Niculescu, C.P.; Persson, L.E. Convex Functions and Their Applications: A Contemporary Approach; Springer: Berlin/Heidelberg,
Germany, 2018.
29.
Nielsen, F. Generalized Bhattacharyya and Chernoff upper bounds on Bayes error using quasi-arithmetic means. Pattern Recognit.
Lett. 2014, 42, 25â€“34. [CrossRef]
30.
Deasy, J.; Simidjievski, N.; LiÃ², P. Constraining Variational Inference with Geometric Jensen-Shannon Divergence. In Proceedings
of the 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, BC, Canada, 6â€“12 December 2020.
31.
Amari, S.I. Integration of stochastic models by minimizing Î±-divergence. Neural Comput. 2007, 19, 2780â€“2796. [CrossRef]

Entropy 2021, 23, 464
27 of 28
32.
Calin, O.; Udriste, C. Geometric Modeling in Probability and Statistics; Mathematics and Statistics; Springer International Publishing:
Berlin/Heidelberg, Germany, 2014.
33.
RÃ©nyi, A. On measures of entropy and information. In Proceedings of the Fourth Berkeley Symposium on Mathematical Statistics
and Probability, Berkeley, CA, USA, 20 Juneâ€“30 July 1961; Volume 1: Contributions to the Theory of Statistics; The Regents of the
University of California: Oakland, CA, USA, 1961.
34.
Blondel, M.; Martins, A.F.; Niculae, V. Learning with Fenchel-Young losses. J. Mach. Learn. Res. 2020, 21, 1â€“69.
35.
Faddeev, D.K. Zum Begriff der Entropie einer endlichen Wahrscheinlichkeitsschemas. In Arbeiten zur Informationstheorie I;
Deutscher Verlag der Wissenschaften: Berlin, Germany, 1957; pp. 85â€“90.
36.
Kolmogorov, A.N.; Castelnuovo, G. Sur la Notion de la Moyenne; Bardi, G., Ed.; Atti della Academia Nazionale dei Lincei: Rome,
Italy, 1930; Volume 12, pp. 323â€“343.
37.
Nagumo, M. Ãœber eine klasse der mittelwerte. In Japanese Journal of Mathematics: Transactions and Abstracts; The Mathematical
Society of Japan: Tokyo, Japan, 1930; Volume 7, pp. 71â€“79.
38.
De Finetti, B. Sul Concetto di Media; Istituto Italiano Degli Attuari: Roma, Italy, 1931.
39.
Van Erven, T.; Harremos, P. RÃ©nyi divergence and Kullback-Leibler divergence. IEEE Trans. Inf. Theory 2014, 60, 3797â€“3820.
[CrossRef]
40.
Sibson, R. A brief description of natural neighbour interpolation. In Interpreting Multivariate Data; Barnett, V., Ed.; John Wiley &
Sons: Hoboken, NJ, USA, 1981; pp. 21â€“36. .
41.
Boyd, S.; Boyd, S.P.; Vandenberghe, L. Convex Optimization; Cambridge University Press: Cambridge, UK, 2004.
42.
Nielsen, F.; Sun, K. Guaranteed bounds on information-theoretic measures of univariate mixtures using piecewise log-sum-exp
inequalities. Entropy 2016, 18, 442. [CrossRef]
43.
Nielsen, F. Chernoff information of exponential families. arXiv 2011, arXiv:1102.2684.
44.
Nielsen, F. An information-geometric characterization of Chernoff information. IEEE Signal Process. Lett. 2013, 20, 269â€“272.
[CrossRef]
45.
Nielsen, F.; Yvinec, M. An output-sensitive convex hull algorithm for planar objects. Int. J. Comput. Geom. Appl. 1998, 8, 39â€“65.
[CrossRef]
46.
Nielsen, F.; Nock, R. On the chi square and higher-order chi distances for approximating f-divergences. IEEE Signal Process. Lett.
2013, 21, 10â€“13. [CrossRef]
47.
Nielsen, F. The statistical Minkowski distances: Closed-form formula for Gaussian mixture models. In International Conference on
Geometric Science of Information; Springer: Berlin/Heidelberg, Germany, 2019; pp. 359â€“367.
48.
FrÃ©chet, M. Les Ã©lÃ©ments alÃ©atoires de nature quelconque dans un espace distanciÃ©.
Ann. Lâ€™Institut Henri PoincarÃ© 1948,
10, 215â€“310.
49.
Nielsen, F.; Nock, R. Sided and symmetrized Bregman centroids. IEEE Trans. Inf. Theory 2009, 55, 2882â€“2904. [CrossRef]
50.
Naudts, J. Generalised Thermostatistics; Springer Science & Business Media: Berlin/Heidelberg, Germany, 2011.
51.
Tsallis, C. Possible generalization of Boltzmann-Gibbs statistics. J. Stat. Phys. 1988, 52, 479â€“487. [CrossRef]
52.
Nielsen, F. On Voronoi diagrams on the information-geometric Cauchy manifolds. Entropy 2020, 22, 713. [CrossRef] [PubMed]
53.
Nock, R.; Nielsen, F.; Amari, S.i. On conformal divergences and their population minimizers. IEEE Trans. Inf. Theory 2015,
62, 527â€“538. [CrossRef]
54.
Brekelmans, R.; Nielsen, F.; Makhzani, A.; Galstyan, A.; Steeg, G.V. Likelihood Ratio Exponential Families.
arXiv 2020,
arXiv:2012.15480.
55.
Brekelmans, R.; Masrani, V.; Bui, T.; Wood, F.; Galstyan, A.; Steeg, G.V.; Nielsen, F. Annealed Importance Sampling with q-Paths.
arXiv 2020, arXiv:2012.07823.
56.
Nielsen, F. A generalization of the Î±-divergences based on comparable and distinct weighted means. arXiv 2020, arXiv:2001.09660.
57.
Amari, S.i.; Ohara, A. Geometry of q-exponential family of probability distributions. Entropy 2011, 13, 1170â€“1185. [CrossRef]
58.
Grosse, R.; Maddison, C.J.; Salakhutdinov, R. Annealing between distributions by averaging moments. In Proceedings of the 26th
International Conference on Neural Information Processing Systems, Lake Tahoe, NV, USA, 5â€“8 December 2013; pp. 2769â€“2777.
59.
Nielsen, F. What is an information projection? Not. AMS 2018, 65, 321â€“324. [CrossRef]
60.
Nielsen, F.; Garcia, V. Statistical exponential families: A digest with ï¬‚ash cards. arXiv 2009, arXiv:0911.4863.
61.
Nielsen, F.; Nock, R. Entropies and cross-entropies of exponential families. In Proceedings of the 2010 IEEE International
Conference on Image Processing, Hong Kong, China, 26â€“29 September 2010; IEEE: Piscataway, NJ, USA, 2010; pp. 3621â€“3624.
62.
Nielsen, F. On Geodesic Triangles with Right Angles in a Dually Flat Space. In Progress in Information Geometry: Theory and
Applications; Springer: Berlin/Heidelberg, Germany, 2021; pp. 153â€“190.
63.
Bauckhage, C. Computing the Kullback-Leibler divergence between two Weibull distributions. arXiv 2013, arXiv:1310.3713.
64.
Michalowicz, J.V.; Nichols, J.M.; Bucholtz, F. Handbook of Differential Entropy; CRC Press: Boca Raton, FL, USA, 2013.
65.
CsiszÃ¡r, I. On topological properties of f-divergences. Stud. Math. Hungar. 1967, 2, 329â€“339.
66.
Nielsen, F. On information projections between multivariate elliptical and location-scale families. arXiv 2021, arXiv:2101.03839.
67.
Pelletier, B. Informative barycentres in statistics. Ann. Inst. Stat. Math. 2005, 57, 767â€“780. [CrossRef]
68.
Schwander, O.; Nielsen, F. Learning mixtures by simplifying kernel density estimators. In Matrix Information Geometry; Springer:
Berlin/Heidelberg, Germany, 2013; pp. 403â€“426.
69.
Lloyd, S. Least squares quantization in PCM. IEEE Trans. Inf. Theory 1982, 28, 129â€“137. [CrossRef]

Entropy 2021, 23, 464
28 of 28
70.
Davis, J.V.; Dhillon, I. Differential entropic clustering of multivariate Gaussians.
In Proceedings of the 19th International
Conference on Neural Information Processing Systems, Vancouver, BC, Canada, 4â€“7 December 2006; pp. 337â€“344.
71.
Nielsen, F.; Nock, R.
Clustering multivariate normal distributions.
In Emerging Trends in Visual Computing; Springer:
Berlin/Heidelberg, Germany, 2008; pp. 164â€“174.
72.
Fischer, A. Quantization and clustering with Bregman divergences. J. Multivar. Anal. 2010, 101, 2207â€“2221. [CrossRef]
73.
Zhang, K.; Kwok, J.T. Simplifying mixture models through function approximation. IEEE Trans. Neural Netw. 2010, 21, 644â€“658.
[CrossRef] [PubMed]
74.
Duan, J.; Wang, Y. Information-Theoretic Clustering for Gaussian Mixture Model via Divergence Factorization. In Proceedings
of the 2013 Chinese Intelligent Automation Conference, Yangzhou, China, 23â€“25 August 2013; Springer: Berlin/Heidelberg,
Germany, 2013; pp. 565â€“573.
75.
Wang, J.C.; Yang, Y.H.; Wang, H.M.; Jeng, S.K. Modeling the affective content of music with a Gaussian mixture model. IEEE
Trans. Affect. Comput. 2015, 6, 56â€“68. [CrossRef]
76.
Spurek, P.; PaÅ‚ka, W. Clustering of Gaussian distributions. In Proceedings of the 2016 International Joint Conference on Neural
Networks (IJCNN), Vancouver, BC, USA, 24â€“29 July 2016; IEEE: Piscataway, NJ, USA, 2016; pp. 3346â€“3353.
77.
Esteban, M.D.; Morales, D. A summary on entropy statistics. Kybernetika 1995, 31, 337â€“346.
78.
Nielsen, F.; Okamura, K. On f-divergences between Cauchy distributions. arXiv 2021, arXiv:2101.12459.
79.
Li, W. Transport information Bregman divergences. arXiv 2021, arXiv:2101.01162.
80.
Chen, P.; Chen, Y.; Rao, M. Metrics deï¬ned by Bregman divergences: Part 2. Commun. Math. Sci. 2008, 6, 927â€“948. [CrossRef]
81.
Bregman, L.M. The relaxation method of ï¬nding the common point of convex sets and its application to the solution of problems
in convex programming. USSR Comput. Math. Math. Phys. 1967, 7, 200â€“217. [CrossRef]
82.
Arnaudon, M.; Nielsen, F. On approximating the Riemannian 1-center. Comput. Geom. 2013, 46, 93â€“104. [CrossRef]
83.
Candan, Ã‡. Chebyshev Center Computation on Probability Simplex With Î±-Divergence Measure. IEEE Signal Process. Lett. 2020,
27, 1515â€“1519. [CrossRef]
84.
Birrell, J.; Dupuis, P.; Katsoulakis, M.A.; Rey-Bellet, L.; Wang, J. Variational Representations and Neural Network Estimation for
RÃ©nyi Divergences. arXiv 2020, arXiv:2007.03814.

